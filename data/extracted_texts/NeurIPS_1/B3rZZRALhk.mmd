# On Improved Conditioning Mechanisms and Pre-training Strategies for Diffusion Models

Tariq Berrada\({}^{1,2}\)  Pietro Astolfi\({}^{1}\)  Melissa Hall\({}^{1}\)  Reyhane Askari-Hemmat\({}^{1}\)

Yohann Benchetrit\({}^{1}\)  Marton Havasi\({}^{1}\)  Matthew Muckley\({}^{1}\)  Karteek Alahari\({}^{2}\)

Adriana Romero-Soriano\({}^{1,3,4,5}\)  Jakob Verbeek\({}^{1}\)  Michal Drozdzal\({}^{1}\)

\({}^{1}\)FAIR at Meta \({}^{2}\)Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, France

\({}^{3}\)McGill University \({}^{4}\)Mila, Quebec AI institute \({}^{5}\)Canada CIFAR AI chair

###### Abstract

Large-scale training of latent diffusion models (LDMs) has enabled unprecedented quality in image generation. However, the key components of the best performing LDM training recipes are oftentimes not available to the research community, preventing apple-to-apple comparisons and hindering the validation of progress in the field. In this work, we perform an in-depth study of LDM training recipes focusing on the performance of models and their training efficiency. To ensure apple-to-apple comparisons, we re-implement five previously published models with their corresponding recipes. Through our study, we explore the effects of (i) the mechanisms used to condition the generative model on semantic information (_e.g._, text prompt) and control metadata (_e.g._, crop size, random flip flag, _etc._) on the model performance, and (ii) the transfer of the representations learned on smaller and lower-resolution datasets to larger ones on the training efficiency and model performance. We then propose a novel conditioning mechanism that disentangles semantic and control metadata conditionings and sets a new state-of-the-art in class-conditional generation on the ImageNet-1k dataset - with FID improvements of 7% on 256 and 8% on 512 resolutions - as well as text-to-image generation on the CC12M dataset - with FID improvements of 8% on 256 and 23% on 512 resolution.

## 1 Introduction

Diffusion models have emerged as a powerful class of generative models and demonstrated unprecedented ability at generating high-quality and realistic images. Their superior performance is evident across a spectrum of applications, encompassing image  and video synthesis , denoising , super-resolution  and layout-to-image synthesis . The fundamental principle underpinning diffusion models is the iterative denoising of an initial sample from a trivial prior distribution, that progressively transforms it to a sample from the target distribution. The popularity of diffusion models can be attributed to several factors. First, they offer a simple yet effective approach for generative modeling, often outperforming traditional approaches such as Generative Adversarial Networks (GANs)  and Variational Autoencoders (VAEs)  in terms of visual fidelity and sample diversity. Second, diffusion models are generally more stable and less prone to mode collapse compared to GANs, which are notoriously difficult to stabilize without careful tuning of hyperparameters and training procedures .

Despite the success of diffusion models, training such models at scale remains computationally challenging, leading to a lack of insights on the most effective training strategies. Training recipes of large-scale models are often closed (_e.g._, DALL-E, Imagen, Midjourney), and only a few studies have analyzed training dynamics in detail . Moreover, evaluation often involves human studies which are easily biased and hard to replicate . Due to the high computational costs, the research community mostly focused on the finetuning of large text-to-image models for different downstream tasks  and efficient sampling techniques . However, there has been less focus on ablating different mechanisms to condition on user inputs such as text prompts, and strategies to pre-train using datasets of smaller resolution and/or data size. The benefits of conditioning mechanisms are two-fold: allowing users to have better control over the content that is being generated, and unlocking training on augmented or lower quality data by for example conditioning on the original image size  and other metadata of the data augmentation. Improving pre-training strategies, on the other hand, can allow for big cuts in the training cost of diffusion models by significantly reducing the number of iterations necessary for convergence.

Our work aims to disambiguate some of these design choices, and provide a set of guidelines that enable the scaling of the training of diffusion models in an efficient and effective manner. Beyond the main architectural choices (_e.g._, Unet _vs._ ViT), we focus on two other important aspects for generative performance and efficiency of training. First, we enhance conditioning by decoupling different conditionings based on their type: control metadata conditioning (_e.g._, crop size, random flip, _etc._), semantic-level conditioning based on class names or text-prompts. In this manner, we disentangle the contribution of each conditioning and avoid undesired interference among them. Second, we optimize the scaling strategy to larger dataset sizes and higher resolution by studying the influence of the initialization of the model with weights from models pre-trained on smaller datasets and resolutions. Here, we propose three improvements needed to seamlessly transition across resolutions: interpolation of the positional embeddings, scaling of the noise schedule, and using a more aggressive data augmentation strategy.

In our experiments we evaluate models at 256 and 512 resolution on ImageNet-1k and Conceptual Captions (CC12M), and also present results for ImageNet-22k at 256 resolution. We study the following five architectures: _Unet/LDM-G4_, _DiT-XL2 w/ LN_, _mDT-v2-XL2 w/ LN_, _PixArt-\(\)-XL2/_, and _mmDiT-XL2 (SD3)_. We find that among the studied base architectures, _mmDiT-XL2 (SD3)_ performs the best. Our improved conditioning approach further boosts the performance of the best model consistently across metrics, resolutions, and datasets. In particular, we improve the previous state-of-the-art DiT result of 3.04 FID on ImageNet-1k at 512 resolution to 2.76. For CC12M at 512 resolution, we improve FID of 11.24 to 8.64 when using our improved conditioning, while also obtaining a (small) improvement in CLIPscore from 26.01 to 26.17. See Fig. 1 for qualitative examples of our model trained on CC12M.

Figure 1: **Qualitative examples. Images generated using our model trained on CC12M at 512 resolution.**

In summary, our contributions are the following:

* We present a systematic study of five different diffusion architectures, which we train from scratch using face-blurred ImageNet and CC12M datasets at 256 and 512 resolutions.
* We introduce a conditioning mechanism that disentangles different control conditionings and semantic-level conditioning, improving generation and avoiding interference between conditions.
* To transfer weights from pre-trained models we propose to interpolate positional embeddings, scale the noise schedule, and use stronger data augmentation, leading to improved performance.
* We obtain state-of-the-art results at 256 and 512 resolution for class-conditional generation on ImageNet-1k and text-to-image generation and CC12M.

## 2 Conditioning and pre-training strategies for diffusion models

In this section, we review and analyze the conditioning mechanisms and pre-training strategies used in prior work (see more detailed discussion of related work in App. A), and propose improved approaches based on the analysis.

### Conditioning mechanisms

**Background.** To control the generated content, diffusion models are usually conditioned on class labels or text prompts. Adaptive layer norm is a lightweight solution to condition on class labels, used for both UNets [21; 39; 41] and DiT models . Cross-attention is used to allow more fine-grained conditioning on textual prompts, where particular regions of the sampled image are affected only by part of the prompt, see _e.g._. More recently, another attention based conditioning was proposed in SD3  within a transformer-based architecture that evolves both the visual and textual tokens across layers. It concatenates the image and text tokens across the sequence dimension, and then performs a self-attention operation on the combined sequence. Because of the difference between the two modalities, the keys and queries are normalized using RMSNorm , which stabilizes training. This enables complex interactions between the two modalities in one attention block instead of using both self-attention and cross-attention blocks.

Moreover, since generative models aim to learn the distribution of the training data, data quality is important when training generative models. Having low quality training samples, such as the ones that are poorly cropped or have unnatural aspect ratios, can result in low quality generations. Previous work tackles this problem by careful data curation and fine-tuning on high quality data, see _e.g._[7; 9]. However, strictly filtering the training data may deprive the model from large portions of the available data , and collecting high-quality data is not trivial. Rather than treating them as nuisance factors, SDXL  proposes an alternative solution where a UNet-based model is conditioned on parameters corresponding to image size and crop parameters during training. In this manner, the model is aware of these parameters and can account for them during training, while also offering users control over these parameters during inference. These _control conditions_ are transformed and additively combined with the timestep embedding before feeding them to the diffusion model.

**Disentangled control conditions.** Straightforward implementation of control conditions in DiT may cause interference between the time-step, class-level and control conditions if their corresponding embeddings are additively combined in the adaptive layer norm conditioning, _e.g._ causing changes in high-level content of the generated image when modifying its resolution, see Fig. 2. To disentangle the different conditions, we propose two modifications. First, we move the class embedding to be fed through the attention layers present in the DiT blocks. Second, to ensure that the control embedding

Figure 2: **Influence of control conditions. Images generated using the same latent sample. Top: Model trained with constant weighting of the size conditioning as used in SDXL , introducing undesirable correlations between image content and size condition. Bottom: Model trained using our cosine weighting of low-level conditioning, disentangling the size condition from the image content.**

does not overpower the timestep embedding when additively combined in the adaptive layer norm, we zero out the control embedding in early denoising steps, and gradually increase its strength.

Control conditions can be used for different types of data augmentations: (i) _high-level augmentations (\(_{h}\))_ that affect the image composition - _e.g._ flipping, cropping and aspect ratio -, and (ii) _low-level augmentations (\(_{l}\))_ that affect low-level details - _e.g._ image resolution and color. Intuitively, high-level augmentations should impact the image formation process early on, while low-level augmentations should enter the process only once sufficient image details are present. We achieve this by scaling the contribution of the low-level augmentations, \(_{l}\), to the control embedding using a cosine schedule that downweights earlier contributions:

\[c_{}(_{h},_{l},t)=E_{h}(_{h})+_{c}(t) E_{l}( _{l}), \]

where the embedding functions \(E_{h},E_{l}\) are made of sinusoidal embeddings followed by a 2-layer MLP with SiLU activation, and where \(_{c}\) is the cosine schedule illustrated in Fig. 3.

**Improved text conditioning.** Most commonly used text encoders, like CLIP , output a constant number of tokens \(T\) that are fed to the denoising model (usually \(T=77\)). Consequently, when the prompt has less than \(T\) tokens, the remaining positions are filled by zero-padding, but remain accessible via cross-attention to the denoising network. To make better use of the conditioning vector, we propose a _noisy replicate_ padding mechanism where the padding tokens are replaced with copies of the text tokens, thereby pushing the subsequent cross-attention layers to attend to all the tokens in its inputs. As this might lead to redundant token embeddings, we improve the diversity of the feature representation across the sequence dimension, by perturbing the embeddings with additive Gaussian noise with a small variance \(_{}\). To ensure enough diversity in the token embeddings, we scale the additive noise by \((_{})\), where \(m\) is the number of token replications needed for padding, and \((_{})\) is the per-channel standard deviation in the token embeddings.

**Integrating classifier-free guidance.** Classifier-free guidance (CFG)  allows for training conditional models by combining the output of the unconditional generation with the output of the conditional generation. Formally, given a latent diffusion model trained to predict the noise \(\), CFG reads as: \(^{}=_{s}+(1-)_{}\), where \(_{}\) is the unconditional noise prediction, \(_{s}\) is the noise prediction conditioned on the semantic conditioning \(s\) (_e.g._, text prompt), and \(\) is the hyper-parameter, known as _guidance scale_, which regulates the strength of the conditioning. Importantly, during training \(\) is set alternatively to 0 or 1, while at inference time it is arbitrarily changed in order to steer the generation to be more or less consistent with the conditioning. In our case, we propose the control conditioning to be an auxiliary guidance term, in order to separately regulate the strength of the conditioning on the control variables \(c\) and semantic conditioning \(s\) at inference time. In particular, we define the guided noise estimate as:

\[^{,}=[_{c,s}+(1-) _{,s}]+(1-)_{,}, \]

where \(\) sets the strength of the control guidance, and \(\) sets the strength of the semantic guidance.

### On transferring models pre-trained on different datasets and resolutions

Background.Transfer learning has been a pillar of the deep learning community, enabling generalization to different domains and the emergence of foundational models such as DINO  and CLIP . Large pre-trained text-to-image diffusion models have also been re-purposed for different tasks, including image compression  and spatially-guided image generation . Here, we are interested in understanding to which extent pre-training on other datasets and resolutions can be leveraged to achieve a more efficient training of large text-to-image models. Indeed, training diffusion models directly to generate high resolution images is computationally demanding, therefore, it is common to either couple them with super-resolution models, see _e.g._, or fine-tune them with high resolution data, see _e.g._. Although most models can directly operate at a higher resolution than the one used for training, fine-tuning is important to adjust the model to the different statistics of high-resolution images. In particular, we find that the different statistics influence the positional embedding of patches, the noise schedule, and the optimal guidance scale. Therefore, we focus on improving the transferability of these components.

Figure 3: Weighting of low-level control conditions. The weight is zeroed out early on when image semantics are defined, and increased later when adding details.

**Positional Embedding.** Adapting to a higher resolution can be done in different ways. _Interpolation_ scales the - most often learnable - embeddings according to the new resolution . _Extrapolation_ simply replicates the embeddings of the original resolution to higher resolutions as illustrated in Fig. 4, resulting in a mismatch between the positional embeddings and the image features when switching to different resolutions. Most methods that use interpolation of learnable positional embeddings, _e.g_. , adopt either bicubic or bilinear interpolation to avoid the norm reduction associated with the interpolation. In our case, we take advantage of the fact that our embeddings are sinusoidal and simply adjust the sampling grid to have constants limit under every resolution, see App. C.

**Scaling the noise schedule.** At higher resolution, the amount of noise necessary to mask objects at the same rate changes . If we observe a spatial patch at low resolution under a given uncertainty, upscaling the image by a factor \(s\) creates \(s^{2}\) observations of this patch of the form \(y_{t}^{(i)}=x_{t}+_{t}^{(i)}-\) assuming the value of the patch is constant across the patch. This increase in the number of observations reduces the uncertainty around the value of that token, resulting in a higher signal-to-noise (SNR) ratio than expected. This issue gets further accentuated when the scheduler does not reach a terminal state with pure noise during training, _i.e_., a zero SNR , as the mismatch between the non-zero SNR seen during training and the purely Gaussian initial state of the sampling phase becomes significant. To resolve this, we scale the noise scheduler in order to recover the same uncertainty for the same timestep.

**Proposition 1**.: _When going from a scale of \(s\) to a scale \(s^{}\), we update the \(\) scheduler according to the following rule_

\[_{t^{}}=_{t}}{s^{ 2}+_{t}(s^{2}-s^{ 2})} \]

This increases the noise amplitude during intermediate denoising steps as illustrated in Fig. A1. The final equation obtained is similar to the one obtained in  with the accompanying change of variable \(t=}{1+^{2}}\).

**Pre-training cropping strategies.** When pre-training and finetuning at different resolutions, we can either first crop and then resize the crops according to the training resolution, or directly take differently sized crops from the training images. Using a different resizing during pre-training and finetuning may introduce some distribution shift, while using crops of different sizes may be detrimental to low-resolution training as the model will learn the distribution of smaller crops rather than full images, see Fig. 5. We experimentally investigate which strategy is more effective for low-resolution pre-training of high-resolution models.

**Guidance scale.** We discover that the optimal guidance scale for both FID and CLIPScore varies with the resolution of images. In App. D, we present a proof revealing that under certain conditions, the optimal guidance scale adheres to a scaling law with respect to the resolution, as

\[^{}(s)=1+s(-1). \]

## 3 Experimental evaluation

### Experimental setup

**Datasets.** In our study, we train models on three datasets. To train class-conditional models, we use _ImageNet-1k_, which has 1.3M images spanning 1,000 classes, as well as _ImageNet-22k_, which contains 14.2M images spanning 21,841 classes. Additionally, we train text-to-image models using _Conceptual 12M_ (CC12M) , which contains 12M images with accompanying manually generated textual descriptions. We pre-process both datasets by blurring all faces. Differently from , we use the original captions for the CC12M dataset.

**Evaluation.** For image quality, we evaluate our models using the common FID  metric. We follow the standard evaluation protocol on ImageNet to have a fair comparison with the relevant

Figure 4: Interpolation and extrapolation of positional embeddings.

Figure 5: Low-resolution pre-training. Crop size used for pre-training impacts finetuning.

literature . Specifically, we compute the FID between the full training set and 50k synthetic samples generated using 250 DDIM sampling steps. For image-text alignment, we compute the CLIP  score similarly to . We measure conditional diversity, either using class-level or text prompt conditioning, using LPIPS . LPIPS is measured pairwise and averaged among ten generations obtained with the same random seed, prompt, and initial noise, but different size conditioning (we exclude sizes smaller than the target resolution); then we report the average over 10k prompts. In addition to ImageNet and CC12M evaluations, we provide FID and CLIPScore on the COCO  validation set, which contains approximately 40k images with associated captions. For COCO evaluation , we follow the same setting as  for computing the CLIP score, using \(25\) sampling steps and a guidance scale of \(5.0\).

**Training.** To train our models we use the Adam  optimizer, with a learning rate of \(10^{-4}\) and \(_{1},_{2}=0.9,0.999\). When training at \(256 256\) resolution, we use a batch size of \(2,048\) images, a constant learning rate of \(10 10^{-4}\), train our models on two machines with eight A100 GPUs each. In preliminary experiments with the DiT architecture we found that the FID metric on ImageNet-1k at 256 resolution consistently improved with larger batches and learning rate, but that increasing the learning rate by another factor of two led to diverging runs. We report these results in supplementary. When training models at \(512 512\) resolution, we use the same approach but with a batch size of \(384\) distributed over 16 A100 GPUs. We train our ImageNet-1k models for 500k to 1M iterations and for 300k to 500k iterations for CC12M.

**Model architectures.** We train different diffusion architectures under the same setting to provide a fair comparison between model architectures. Specifically, we re-implement a UNet-based architecture following Stable Diffusion XL (SDXL) 1 and several transformer-based architectures: vanilla DiT , masked DiT (mDiT-v2) , PixArt DiT (PixArt-\(\)) , and multimodal DiT (mmDiT) as in Stable Diffusion 3 . For vanilla DiT, which only supports class-conditional generation, we explore two variants one incorporating the class conditioning within LayerNorm and another one within the attention layer. Also, for text-conditioned models, we use the text encoder and tokenizer of CLIP (ViT-L/14)  having a maximum sequence length of \(T=77\). The final models share similar number parameters, _e.g._ for DiTs we inspect the XL/2 variant , for UNet (SDXL) we adopt similar size to the original LDM . Similar to , we found the training of DiT with

    &  &  \\   & 256 & 512 &  &  \\   & FID\({}_{}\)\(\) & FID\({}_{}\)\(\) & FID\({}_{}\)\(\) & CLIP\({}_{}\)\(\) & FID\({}_{}\)\(\) & FID\({}_{}\)\(\) & CLIP\({}_{}\)\(\) \\   _Results taken from references_ & _UNet (SDLM-G4)_ & \(3.60\) & — & \(17.01\) & \(24\) & — & \(9.62\) & — \\ _DiT-XL2 w/LN_ & \(2.27\) & \(3.04\) & — & — & — & — & — \\ _mDT-v2-XL2 w/ LN_ & \(1.79\) & — & — & — & — & — & — \\ _PixArt-\(\)-XL2_ & — & — & — & — & — & \(10.65\) & — \\ _mml-XL2 (SD3)_ & — & — & — & — & \(22.4\) & — & \({}^{*}\) & — \\   _Our re-implementation of existing architectures_ & _UNet_ (SDXL) & \(2.05\) & \(4.81\) & \(8.53\) & \(\) & \(12.56\) & \(7.26\) & \(24.79\) \\ _DiT-XL2 w/ LN_ & \(1.95\) & \(\) & — & — & — & — & — \\ _DiT-XL2 w/ Nat_ & \(\) & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ _mDT-v2-XL2 w/ LN_ & \(2.51\) & \(3.75\) & — & — & — & — & — \\ _PixArt-\(\)-XL2_ & \(2.06\) & \(3.05\) & ✗ & ✗ & ✗ & ✗ & ✗ \\ _mml-XL2 (SD3)_ & \(3.02\) & \(\) & \(24.78\) & \(\) & \(\) & \(\) \\   _Our improved architecture and training_ & _UNet-XL2 (ours)_ & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 1: **Comparison between different model architectures.** We compare results reported in the literature (top, reporting available numbers) with our reimplementations of existing architectures (middle), and to our best results obtained using architectural refinements and improved training. For 512 resolution, we trained models by fine-tuning models pre-trained at 256 resolution. In each column, we bold the best results among those in the first two blocks, and also those in the last row when they are equivalent or superior. ‘—’ denotes that numbers are unavailable in the original papers, or architectures are incompatible with text-to-image generation in our experiments. ‘✗’ indicates diverged runs. ‘*’ is used for Esser et al.  pre-trained on CC12M to denote that FID is computed differently and some details about their evaluation are unclear.

cross-attention to be unstable and had to resort to using RMSNorm  to normalize the key and query in the attention layers. We detail the models sizes and computational footprints in Tab. A1.

### Evaluation of model architectures and comparison with the state of the art

In Tab. 1, we report results for models with different architectures trained at both 256 and 512 resolutions for ImageNet and CC12M, and compare our results (2nd block of rows) with those reported in the literature, where available (1st block of rows). Where direct comparison is possible, we notice that our re-implementation outperforms the one of existing references. Overall, we found the mmDiT  architecture to perform best or second best in all settings compared to other alternatives. For this reason, we apply our conditioning improvements on top of this architecture (last row of the table), boosting the results as measured with FID and CLIPScore in all settings. Below, we analyse the improvements due to our conditioning mechanisms and pre-training strategies.

### Control conditioning

**Scheduling rate of control conditioning.** In Tab. 1(a) we consider the effect of controlling the conditioning on low-level augmentations via a cosine schedule for different decay rates \(\). We compare to baselines (first two rows) with constant weighting (as in SDXL ) and without control conditioning. We find that our cosine weighting schedule significantly reduces the dependence between size control and image semantics as it drastically improves the instance specific LPIPS (0.33 _vs._ 0.04) in comparison to uniform weighting. In terms of FID, we observe a small gap with the baseline (3.04 _vs._ 3.08), which increases (3.80 _vs._ 5.04) when computing FID by randomly sampling the size conditioning in the range , see Tab. 1(b). Finally, the improved disentangling between semantics and low-level conditioning is clearly visible in the qualitative samples in Fig. 2.

**Crop and random-flip control conditioning.** A potential issue of horizontal flip data augmentations is that it can create misalignment between the text prompt and corresponding image. For example the prompt _"A teddy bear holding a baseball bat in their **right arm"_ will no longer be accurate when an image is flipped - showing a teddy bear holding the bat in their **left** arm. Similarly, cropping images can remove details mentioned in the corresponding caption. In Tab. 1(c) we evaluate models trained on CC12M@256 with and without horizontal flip conditioning, and find that adding this conditioning leads to slight improvements in both FID and CLIP as compared to using only crop conditioing. We depict qualitative comparison in Fig. 6, where we observe that flip conditioning improves prompt-layout consistency.

**Inference-time control conditioning of image size.** High-level augmentations (\(_{h}\)) may affect the image semantics. As a result they influence the learned distribution and modify the generation diversity. For example, aspect ratio conditioning can harm the quality of generated images, when images of a particular class or text prompt are unlikely to appear with a given aspect ratio. In Tab. 1(b) we compare of different image size conditionings for inference. We find that conditioning on the same size distribution as encountered during the training of the model yields a significant boost in FID

   _Init._ & \(t\) _weighting_ & _FID_ (\(\)) & _LPIPS_ (\(\)) & _LPIPS_/HR (\(\)) \\  _sem_ & _sem_ & \(3.29\) & \(-\) & \(-\) \\ _zero._ & _unif._ & \(\) & \(0.33\) & \(0.210\) \\  _zero._ & _cos_(\(=1.0\)) & \(3.08\) & \(0.23\) & \(0.076\) \\ _zero._ & _cos_(\(=2.0\)) & \(3.09\) & \(0.18\) & \(0.045\) \\ _zero._ & _cos_(\(=4.0\)) & \(3.05\) & \(0.13\) & \(0.025\) \\ _zero._ & _cos_(\(=8.0\)) & \(\) & \(\) & \(\) \\   

Table 2: Control conditioning. We study different facets of control conditioning and their impact on the model performance. (a-b) We report FID\({}_{}\) on ImageNet-1k@256 using 250 sampling steps. 120k training iterations.

as compared to generating all images with constant size conditioning or using uniformly randomly sampled sizes. Note that in all cases images are generated at 256 resolution.

**Control conditioning and guidance.** To understand how control condition impacts the generation process, we investigate the influence of control guidance \(\) (introduced in Sec. 2.1 ) on FID and report the results in Fig. 7. We find that a higher control guidance scale results in improved FID scores. However, note that this improvement comes at the cost of compute due to the additional control term \(_{c,s}\).

**Replication text padding.** We compare our noisy replication padding to the baseline zero-padding in Tab. 3. We observe that using a replication padding improves both FID and CLIP score, and that adding scaled perturbations further improves the results - \(0.35\) point improvement in CLIP score and \(0.4\) point improvement in FID.

### Transferring weights between datasets and resolutions

**Dataset shift.** We evaluate the effect of pre-training on ImageNet-1k (at 256 resolution) when training the models on CC12M or ImageNet-22k (at 512 resolution) by the time needed to achieve the same performance as a model trained from scratch. In Tab. 4a, when comparing models trained from scratch to ImageNet-1k pre-trained models (600k iterations) we observe two benefits: improved training convergence and performance boosts. For CC12M, we find that after only 100k iterations, both FID and CLIP scores improve over the baseline model trained with more than six times the

  _Pre-train_ & _Finetune_ & _FID\({}_{}\)_ & _CLIP_ \\  _IN22k (375k)_ & — & \(5.80\) & — \\ _IN1k_ & _IN22k (80k)_ & \(5.29\) (\(+8.67\%\)) & — \\ _IN1k_ & _IN22k (10k)_ & \(4.67\) (\(+17.82\%\)) & — \\  _CC12M (600k)_ & — & \(7.54\) & \(24.78\) \\ _IN1k_ & _CC12M (600k)_ & \(7.59\) (\(-0.66\%\)) & \(25.09\) (\(+1.24\%\)) \\ _IN1k_ & _CC12M (100k)_ & \(7.27\) (\(+3.71\%\)) & \(25.62\) (\(+3.43\%\)) \\ _IN1k_ & _CC12M (120k)_ & \(7.25\) (\(+3.85\%\)) & \(25.69\) (\(+3.71\%\)) \\  

Table 4: Effect of pre-training across datasets and resolutions. Number of (pre-)training iterations given in thousands (k) per row. Relative improvements in FID and CLIP score given as percentage in parenthesis.

  _Padding_ & \(_{}\) & _FID_ & _CLIP_ \\  _zero_ & — & \(7.19\) & \(26.25\) \\ _replicate_ & \(0\) & \(6.93\) & \(26.47\) \\ _replicate_ & \(0.02\) & \(\) & \(\) \\ _replicate_ & \(0.05\) & \(6.82\) & \(26.58\) \\ _replicate_ & \(0.1\) & \(7.01\) & \(26.47\) \\ _replicate_ & \(0.2\) & \(7.02\) & \(26.41\) \\  

Table 3: Text padding. Our noisy replication embedding _vs_. baseline zero-padding. Models trained on CC12M@256.

Figure 6: Illustration of the impact of flip conditioning. Without the flip conditioning, the model may confuse left-right specifications. Including flipping as a control condition enables the model to properly follow left-right instructions.

Figure 7: Guidance scales. Left + center: The optimal guidance scale varies with the image resolution. Right: Decoupling the control guidance improves FID, the best reported performance is obtained with \(=1.375\).

amount of training iterations. For ImageNet-22k, which is closer in distribution to ImageNet-1k than CC12M, the gains are even more significant, the finetuned model achieves an FID lower by \(0.5\) point after only 80k training iterations. In Tab. 3(b), we study the relative importance of pre-training vs finetuning when the datasets have dissimilar distributions but similar sample sizes. We fix a training budget in terms of number of training iterations \(N\), we first train our model on ImageNet-22k for \(K\) iterations before continuing the training on CC12M for the remaining \(N-K\) iterations. We see that the model pretrained for 200k iterations and finetuned for 150k performs better than the one spending the bulk of the training during pretraining phase. This validates the importance of domain specific training and demonstrates that the bulk of the gains from the pretrained checkpoint come from the representation learned during earlier stages.

**Resolution change.** We compare the performance boost obtained from training from scratch at \(512\) resolution _vs._ resuming from a \(256\)-resolution trained model. According to our results in Tab. 3(c), pretraining on low resolution significantly boosts the performance at higher resolutions, both for UNet and mmDiT, we find that higher resolution finetuning for short periods of time outperforms high resolution training from scratch by a large margin (\( 25\%\)). These performance gains might in part be due to the increased batch size when pre-training the 256 resolution model, which allows the model to "see" more images as compared to training from-scratch at 512 resolution.

**Positional Embedding.** In Fig. 7(a), we compare the influence of the adjustment mechanism for the positional embedding. We find that our grid resampling approach outperforms the default extrapolation approach, resulting in 0.2 point difference in FID after 130k training iterations.

**Scaling the noise schedule.** We conducted an evaluation to ascertain the influence of the noise schedule by refining our mmDiT model post its low resolution training and report the results in Fig. 7(b). Remarkably, the application of the rectified schedule, for 40k iterations, resulted in an improvement of 0.7 FID points demonstrating its efficacy at higher resolutions.

**Pre-training cropping strategies.** During pretraining, the model sees objects that are smaller than what it sees during fine tuning, see Fig. 5. We aim to reduce this discrepancy by adopting more aggressive cropping during the pretraining phase. We experiment with three cropping ratios for training: \(0.9-1\) (global), \(0.4-0.6\) (local), \(0.4-1\) (mix). We report the results in Fig. 7(c). On ImageNet1K@256, the pretraining FID scores are \(2.36\), \(245.55\) and \(2.21\) for the local, global and mixed strategies respectively. During training at \(512\) resolution, we observe that the global and mix cropping strategies both outperform the local strategy. However, as reported in Fig. 7(c), the local strategy provides benefits at higher resolutions. Overall, training with the global strategy performs the best at \(256\) resolution but lags behind for higher resolution adaptation. While local cropping underperforms at lower resolutions, because it does not see any images in their totality, it outperforms the other methods at higher resolutions - an improvement of almost \(0.2\) FID points is consistent after the first \(50k\) training steps at higher resolution.

## 4 Conclusion

In this paper, we explored various approaches to enhance the conditional training of diffusion models. Our empirical findings revealed significant improvements in the quality and control over generated images when incorporating different coditioning mechanisms. Moreover, we conducted a

Figure 8: **Resolution shift. Experiments are conducted on ImageNet-1k at 512 resolution, FID is reported using 50 DDIM steps with respect to the ImageNet-1k validation set.**

comprehensive study on the transferability of these models across diverse datasets and resolutions. Our results demonstrated that leveraging pretrained representations is a powerful tool to improve the model performance while also cutting down the training costs. Furthermore, we provided valuable insights into efficiently scaling up the training process for these models without compromising performance. By adapting the schedulers and positional embeddings when scaling up the resolution, we achieved substantial reductions in training time while boosting the quality of the generated images. Additional experiments unveil the expected gains from different transfer strategies, making it easier for researchers to explore new ideas and applications in this domain. In Appendix B we discuss societal impact and limitations of our work.