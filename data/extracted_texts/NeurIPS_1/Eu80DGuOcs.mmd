# Understanding and Improving Training-free

Loss-based Diffusion Guidance

Yifei Shen\({}^{1}\)1 Xinyang Jiang\({}^{1}\) Yifan Yang\({}^{1}\) Yezhen Wang\({}^{2}\) Dongqi Han\({}^{1}\) Dongsheng Li\({}^{1}\)

\({}^{1}\)Microsoft Research Asia \({}^{2}\)National University of Singapore

###### Abstract

Adding additional guidance to pretrained diffusion models has become an increasingly popular research area, with extensive applications in computer vision, reinforcement learning, and AI for science. Recently, several studies have proposed training-free loss-based guidance by using off-the-shelf networks pretrained on clean images. This approach enables zero-shot conditional generation for universal control formats, which appears to offer a free lunch in diffusion guidance. In this paper, we aim to develop a deeper understanding of training-free guidance, as well as overcome its limitations. We offer a theoretical analysis that supports training-free guidance from the perspective of optimization, distinguishing it from classifier-based (or classifier-free) guidance. To elucidate their drawbacks, we theoretically demonstrate that training-free guidance is more susceptible to misaligned gradients and exhibits slower convergence rates compared to classifier guidance. We then introduce a collection of techniques designed to overcome the limitations, accompanied by theoretical rationale and empirical evidence. Our experiments in image and motion generation confirm the efficacy of these techniques.

## 1 Introduction

Diffusion models represent a class of powerful deep generative models that have recently broken the long-standing dominance of generative adversarial networks (GANs) . These models have demonstrated remarkable success in a variety of domains, including the generation of images and videos in computer vision , the synthesis of molecules and proteins in computational biology , as well as the creation of trajectories and actions in the field of reinforcement learning (RL) .

One critical area of research in the field of diffusion models involves enhancing controllability, such as pose manipulation in image diffusion , modulation of quantum properties in molecule diffusion , and direction of goal-oriented actions in RL diffusion . The predominant techniques for exerting control over diffusion models include classifier guidance and classifier-free guidance. Classifier guidance involves training a time-dependent classifier to map a noisy image, denoted as \(_{t}\), to a specific condition \(\), and then employing the classifier's gradient to influence each step of the diffusion process . Conversely, classifier-free guidance bypasses the need for a classifier by training an additional diffusion model conditioned on \(\). However, both approaches necessitate extra training to integrate the conditions. Moreover, their efficacy is often constrained when the data-condition pairs are limited and typically lack the zero-shot generalization capability.

Recently, several studies  have introduced training-free guidance that builds upon the concept of classifier guidance. These models eschew the need for training a classifier on noisy images; instead, they estimate the clean image from its noisy counterpart using Tweedie's formula and then employ pretrained networks, designed for clean images, to guide the diffusion process. Given thatcheckpoints for these networks pretrained on clean images are widely accessible online, this form of guidance can be executed in a zero-shot manner. A unique advantage of training-free guidance is that it can be applied to universal control formats, such as style, layout, and FaceID  without any additional training efforts. Furthermore, these algorithms have been successfully applied to offline reinforcement learning, enabling agents to achieve novel goals not previously encountered during training. In contrast to classifier guidance and classifier-free guidance, it is proved in Appendix E of  that training-free guidance does not offer an approximation to the exact conditional energy. Therefore, from a theoretical perspective, it is intriguing to understand how and when these methods succeed or fail. From an empirical standpoint, it is crucial to develop algorithms that can address and overcome these limitations.

This paper seeks to deepen the understanding of training-free guidance by examining its mechanisms and inherent limitations, as well as overcoming these limitations. Specifically, our major contributions can be summarized as follows:

* **How does training-free guidance work?** Although exact conditional energy is difficult to approximate in a training-free manner, from the optimization standpoint, we show that training-free guidance can effectively decrease the guidance loss function. The optimization perspective clarifies the mystery of why the guidance weights should be meticulously designed in relation to the guidance function and diffusion time, as observed in .
* **When does training-free guidance not work?** We theoretically identify the susceptibility of training-free guidance to misaligned gradient issues and slower convergence rates. We attribute these challenges to a decrease in the smoothness of the guidance network in contrast to the classifier guidance.
* **Improving training-free guidance:** We introduce random augmentation to alleviate the misaligned gradient and Polyak step size scheduling to improve convergence. The efficacy of these methods is empirically confirmed across various diffusion models (i.e., image diffusion and motion diffusion) and under multiple conditions (i.e., segmentation, sketch, text, object avoidance, and targeting)2. 
## 2 Preliminaries

### Diffusion Models

Diffusion models are characterized by forward and reverse processes. The forward process, occurring over a time interval from \(0\) to \(T\), incrementally transforms an image into Gaussian noise. On the contrary, the reverse process, from \(T\) back to \(0\), reconstructs the image from the noise. Let \(_{t}\) represent the state of the data point at time \(t\); the forward process systematically introduces noise to the data by following a predefined noise schedule given by \(_{t}=}_{0}+_{t}_{t}\), where \(_{t}\) is monotonically decreasing with \(t\), \(_{t}=}\), and \(_{t}(0,)\) is random noise. Diffusion models use a neural network to learn the noise at each step:

\[_{}_{_{t},,t}[\|_{}( _{t},t)-_{t}\|^{2}_{2}]=_{}_{_{t},,t}[\|_{}(_{t},t)+_{t}_{ {x}_{t}} p_{t}(_{t})\|^{2}_{2}],\]

where \(p_{t}(_{t})\) is the distribution of \(_{t}\). The reverse process is obtained by the following ODE:

\[_{t}}{t}=f(t)_{t}-(t)}{2} _{_{t}} p_{t}(_{t})=f(t)_{t}+(t)}{2 _{t}}_{}(_{t},t), \]

where \(f(t)=}}{t}\), \(g^{2}(t)=_{t}^{2}}{t}-2 }}{t}_{t}^{2}\). The reverse process enables generation as it converts a Gaussian noise into the image.

### Diffusion Guidance

In diffusion control, we aim to sample \(_{0}\) given a condition \(\). The conditional score function is expressed as follows:

\[_{_{t}} p_{t}(_{t}|)=_{_{t}} p_{t }(_{t})+_{_{t}} p_{t}(|_{t}). \]The conditions are specified by the output of a neural network, and the energy is quantified by the corresponding loss function. If \((f_{}(),)\) represents the loss function as computed by neural networks, then the distribution of the clean data is expected to follow the following formula [2; 49; 37]:

\[p_{0}(_{0}|) p_{0}(_{0})(-(f_{}(_{0} ),)). \]

For instance, consider a scenario where the condition is the object location. In this case, \(f_{}\) represents a fastRCNN architecture, and \(\) denotes the classification loss and bounding box loss. By following the computations outlined in , we can derive the exact formula for the second term in the RHS of (2) as:

\[_{_{t}} p_{t}(|_{t})=_{_{t}}_{p(_{0}|_{t})}[(-(f_{}(_{0}),)]. \]

**Classifier guidance** involves initially training a time-dependent classifier to predict the output of the clean image \(_{0}\) based on noisy intermediate representations \(_{t}\) during the diffusion process, i.e., to train a time-dependent classifier \(f_{}(_{t},t)\) such that \(f_{}(_{t},t) f_{}(_{0})\). Then the gradient of the time-dependent classifier is used for guidance, given by \(_{_{t}} p_{t}(|_{t}):=-_{_{t}}(f_ {}(_{t},t),)\). This term equals (4) if the loss is cross-entropy.

**Training-free loss-based guidance**[2; 49; 6] puts the expectation in (4) inside the loss function:

\[_{_{t}} p_{t}(|_{t}):= _{_{t}}[(-(f_{}(_{p(_{0}|_{t})}(_{0})),))]}{{=}}- _{_{t}}[f_{}(_{t}-_{t}_{ }(_{t},t)}{}}),], \]

where (a) uses Tweedie's formula \(_{p(_{0}|_{t})}(_{0})=_{t}-_{t} _{}(_{t},t)}{}}\). Leveraging this formula permits the use of a pretrained off-the-shelf network designed for processing clean data. The gradient of the last term in the energy function is obtained via backpropagation through both the guidance network and the diffusion backbone.

## 3 Analysis of Training-Free Guidance

### How does Training-free Guidance Work?

**On the difficulty of approximating \(_{_{t}} p_{t}(|_{t})\) in high-dimensional space.** Despite being intuitive,  has shown that training-free guidance in (5) does not offer an approximation to the true energy in (4). The authors of  consider to directly approximate (4) with a Gaussian distribution:

\[_{_{t}}_{p(_{0}|_{t})}[ (-(f_{}(_{0}),)]}{{}} _{_{t}}_{q(_{0}|_{t})}[(-(f_{ }(_{0}),)] \] \[_{_{t}}_{i=1}^{n}(- (f_{}(_{0}^{i}),)),_{0}^{i} q(_{0}| _{t}),\]

where \(q(_{0}|_{t})\) is chosen as \((_{p(_{0}|_{t})}(_{0}),r_{t}^{2})\) and \(r_{t}\) is a tunable parameter. As demonstrated in , the approximation is effective for one-dimensional distribution. However, we find that the approximation denoted by (a) does not extend to high-dimensional data (e.g., images) if the surrogate distribution \(q\) is sub-Gaussian. This is due to the well-known high-dimensional probability phenomenon  that if \(q\) has sub-Gaussian coordinates (e.g., iid and bounded), then \(q(_{0}|_{t})\) tends to concentrate on a spherical shell centered at \(_{p(_{0}|_{t})}(_{0})\) with radius \(r_{t}\) (details are in Appendix C.1). Since the spherical shell represents a low-dimensional manifold with zero measure in the high-dimensional space, there is a significant likelihood that the supports of \(p(_{0}|_{t})\) and \(q(_{0}|_{t})\) do not overlap, rendering the approximation (a) ineffective.

**Understanding training-free guidance from an optimization perspective.** We instead analyze the training-free guidance from the optimization perspective. Intuitively, in each step, the gradient is taken and the loss of the guidance network decreases. At the initial stage of the diffusion (\(t\) is large), the diffusion trajectory can exhibit substantial deviations between adjacent steps and may increase the objective value. So the objective value will oscillate at the beginning. When \(t\) is smaller, the change to the sample is more fine-grained, leading to a bounded change in the objective value. Therefore, the objective value is guaranteed to decrease when \(t\) is small, as showing in the next proposition.

**Proposition 3.1**.: _Assume that the guidance loss function \((f_{}(_{0}),)\) is \(\)-PL (defined in Definition D.2 in appendix) and \(L_{f}\)-Lipschitz with respect to clean images \(_{0}\), and the score function \( p_{t}(_{t})\) is \(L_{p}\)-Lipschitz (defined in Definition D.1 in appendix) with respect to noisy image \(_{t}\). Denote \(_{}\) as the minimum eigenvalue of the (semi)-definite matrix \([_{0}|_{t}]\). Then the following conditions hold: (1) Consider the loss function \(_{t}(_{t})=(f_{}(_{t}+_{t}^{2}  p_{t}(_{t})}{}}),)\) and denote \(_{1}=^{2}}{L_{f}(1+L_{p})_{t }^{3}}}\). After one gradient step \(}_{t}=_{t}-_{_{t}}_{t}(_{t}),= }}{L_{f}(1+L_{p})}\), we have \(_{t}(}_{t})(1-_{1})_{t}(_{t})\); (2) Consider a diffusion process that adheres to a bounded change in the objective function such that for any diffusion step, i.e., \(_{t-1}(_{t-1})(}_{t})}{(1-_{2})}\) for some \(_{2}<_{1}\), then the objective function converges at a linear rate, i.e., \(_{t-1}(_{t-1})}{1-_{2}}_{t}(_{t})\).

The proof is given in Appendix D.2. The Lipschitz continuity and PL conditions are basic assumptions in optimization, and it has been shown that neural networks can locally satisfy these conditions . These assumptions, while not essential, simplify the proof and results for clarity, similar to the assumptions taken in . The optimization perspective clarifies the mystery of why the guidance weights (i.e., the step size in optimization) should be carefully selected with respect to the guidance function and time \(t\). For example, in , most guidance weights \(\) are chosen to be proportional to \(}\) and dependent on guidance network, which differs from the weights used in classifier guidance and aligns with our step size in Proposition 3.1.

Then we empirically verify Proposition 3.1 via experiments in Figure 1. We use ResNet-50 trained on clean images to guide ImageNet pretrained diffusion models. The loss value at each diffusion step is plotted. As a reference, we choose \(100\) images from the class "indigo bird" in ImageNet training set and compute the loss value, which is referred to as "Correct Image Loss" in the figure. The objective value oscillates when \(t\) is large, followed by a swift decrease, which verifies our analysis. More convergence figures are given in Figure 7 in Appendix.

An intriguing aspect of the theory is that the loss remains low regardless of the success of the guidance, akin to the loss associated with correct images. Figure 0(b) demonstrates this phenomenon: despite the absence of an indigo bird in the image, the loss is still minimal. This phenomenon can be attributed to the effect of misaligned gradients, which is explored in detail in the following subsection.

### Limitations of Training-free Guidance

In this subsection, we examine the disadvantages of employing training-free guidance networks as opposed to training-based classifier guidance.

**Training-free guidance is more sensitive to the misaligned gradient.** Adversarial gradient is a significant challenge for neural networks, which refers to minimal perturbations deliberately applied to inputs that can induce disproportionate alterations in the model's output . The resilience of a model to adversarial gradients is often analyzed through the lens of its Lipschitz constant . If the model has a lower Lipschitz constant, then the output is less sensitive to the input perturbations and thus is more robust.

In the classifier or training-free guidance, the gradient of the guidance network is added to the image. In contrast to yielding a direction that meaningfully minimizes the loss, the adversarial gradient primarily serves to minimize the loss in a manner that is not necessarily aligned with the intended guidance direction. As a result, we refer the adversarial gradient of guidance network as the _misaligned gradient_ in diffusion guidance.

Figure 1: The classifier loss of a successful and a failure guidance example. The target class is “indigo bird”.

Compared with the off-the-shelve guidance network used in training-free guidance, time-dependent classifiers are trained on noise-augmented images. Our finding is that adding Gaussian noise improves the Lipschitzness of the guidance network. This transition mitigates the misaligned gradient challenge by inherently enhancing the model's robustness to such perturbations, as shown in the next proposition.

**Proposition 3.2**.: _(Time-dependent network is more robust and smooth) Given a bounded loss function \(() C\), the loss \(()=_{(0,)}[( +_{t})]\) is \(C}{_{t}^{2}}}\)-Lipschitz and \(\) is \(}\)-Lipschitz._

The proof is given in Appendix D.3. We then support Proposition 3.2 with both qualitative and quantitative experiments. For qualitative experiments, we present visualizations of the accumulated gradients for both the time-dependent and off-the-shelf time-independent classifiers corresponding to different classes in Figure 1(b) and Figure 1(c), respectively. These visualizations are generated by initializing an image with a random background and computing \(1000\) gradient steps for each classifier. For the time-dependent classifier, the input time for the \(t\)-th gradient step is \(1000-t\). The images are generated purely by the classifier gradients without diffusion involved. For comparative analysis, we include the accumulated gradient of an adversarially robust classifier , as shown in Figure 1(a), which has been specifically trained to resist misaligned (adversarial) gradients. The resulting plots reveal a stark contrast: the gradient of the time-dependent classifier visually resembles the target image, whereas the gradient of the time-independent classifier does not exhibit such recognizability. This observation suggests that off-the-shelf time-independent classifiers are prone to generating misaligned gradients for guidance compared to the time-dependent classifier used in classifier guidance. The quantitative experiments are given in Table 4 in Appendix.

Figure 2 provides a more intuitive visual explanation of diffusion guidance compared to the existing formula-based approaches as shown in (7). The gradient produced by the guidance network represents a valid image. When these gradients are incorporated into the images, the diffusion model is able to identify the object and enhance it into a clearer and more vivid representation.

**Training-free guidance slows down the convergence of reverse ODE.** The efficiency of an algorithm in solving reverse ordinary differential equations is often gauged by the number of non-linear function estimations (NFEs) required to achieve convergence. This metric is vital for algorithmic design, as it directly relates to computational cost and time efficiency . In light of this, we explore the convergence rates associated with various guidance paradigms, beginning our analysis with a reverse ODE framework that incorporates a generic gradient guidance term. The formula is expressed as

\[_{t}}{t}=f(t)_{t}+(t)}{2}( {}_{}(_{t},t)+_{_{t}}v(_{t},t)), \]

where \(h(,)\) can be either a time-dependent classifier or a time-independent classifier with Tweedie's formula. The subsequent proposition elucidates the relationship between the discretization error and the smoothness of the guidance function.

**Proposition 3.3**.: _Let \(u(_{t},t)=_{}(_{t},t)+_{_{t}}v(_{t},t)\) in (7), \(h_{}=_{t}[(}{1-_{t}})-( }{1-_{t-1}})]\). Assume we run DDIM solver for \(M\) steps and \(M=O(1/h_{})\). Then the error is bounded by \(O((1+L^{M})/M)\)._

Figure 2: Gradients of different classifiers on random backgrounds. The images in the first row correspond to the target class “cock”, and the second row to “goldfinch”.

The proof is given in Appendix D.4. Proposition 3.2 establishes that time-dependent classifiers exhibit superior gradient Lipschitz constants compared to their off-the-shelf time-independent counterparts. This disparity in smoothness slows down the convergence for training-free guidance methods, necessitating a greater number of NFEs to achieve the desired level of accuracy when compared to classifier guidance. To provide quantitative support, we compare the convergence speed of training-based PPAP  and training-free FreeDoM in Table 5 in Appendix.

## 4 Improving Training-free Guidance

In this section, we propose to adopt random augmentation to mitigate the misaligned gradient issue, and Polyak step size  to mitigate the convergence issue. In addition to these two techniques, our method and baselines will also incorporate a trick named time travel, often referred to as "restart sampling", and its theoretical framework is detailed in .

```
for\(t=T,,0\)do \(_{t-1}=(_{t})\) \(}_{0}=_{t}-_{t}_{}( _{t},t)}{}}\)\(\) Tweedie's formula \(_{t}=|}_{T}_{_{t}} (f_{}(T(}_{0})),)\) \(_{t-1}=_{t-1}-_{t}\) endfor
```

**Algorithm 1** Random Augmentation

### Random Augmentation

As established by Proposition 3.2, the introduction of Gaussian perturbations enhances the Lipschitz property of a neural network. A direct application of this principle involves creating multiple noisy instances of an estimated clean image and passing them into the guidance network, a method analogous to the one described in (6). However, given the high-dimensional nature of image data, achieving a satisfactory approximation of the expected value necessitates an impractically large number of noisy copies. To circumvent this issue, we propose an alternative strategy that employs a diverse set of data augmentations in place of solely adding Gaussian noise. This approach effectively introduces perturbations within a lower-dimensional latent space, thus requiring fewer samples. The suite of data augmentations utilized, denoted by \(\), is derived from the differentiable data augmentation techniques outlined in , which encompasses transformations such as translation, resizing, color adjustments, and cutout operations. The details are shown in Algorithm 1 and the rationale is shown in the following proposition.

**Proposition 4.1**.: _(Random augmentation improves smoothness) Given a bounded non-Lipschitz loss function \(()\), the loss \(()=_{ p()}[( +)]\) is \(C_{^{n}}\| p()\|_{2}\)-Lipschitz and its gradient is \(C_{^{n}}\|^{2}p()\|_{}\)-Lipschitz._

The proof is shown in Appendix D.5. Echoing the experimental methodology delineated in Section 3.2, we present an analysis of the accumulated gradient effects when applying random augmentation to a ResNet-50 model. Specifically, we utilize a set of \(||=10\) diverse transformations as our augmentation strategy. The results of this experiment are visualized in Figure 1(d), where the target object's color and shape emerge in the gradient profile. This observation suggests that the implementation of random augmentation can alleviate the misaligned gradient issue. The quantitative effect of random augmentation is given in Table 4 in Appendix. The computational efficiency of random augmentation is further discussed in Appendix C.4.

### Polyak Step Size

In Section 3.1, we analyzed training-free guidance from the optimization perspective. To accelerate the convergence, gradient step size should be adaptive to the gradient landscape. We adopt Polyak step size, which has near-optimal convergence rates under various conditions . The algorithm is shown in Algorithm 2 and the term \(\|_{}(_{t},t)\|\) is used to both estimate the gap to optimal values and balance the magnitude of diffusion term and guidance term.

We implement Polyak step size within the context of a training-free guidance framework called FreeDoM  and benchmark the performance of this implementation using the DDIM sampler with \(50\) steps. As shown in Figure 3, FreeDoM is unable to effectively guide the generation process when faced with a significant discrepancy between the unconditional generation and the specified condition. An illustrative example is the difficulty in guiding the model to generate faces oriented to the left when the unconditionally generated faces predominantly orient to the right, as shown in Figure 2(b). This challenge, which arises due to the insufficiency of \(50\) steps for convergence under the condition, is ameliorated by substituting gradient descent with adaptive step size, thereby illustrating the benefits of employing a better step size in the guidance process. The quantitative experiments are given in Table 5 in Appendix.

## 5 Experiments

In this section, we evaluate the efficacy of our proposed techniques across various diffusion models and guidance conditions. We compare our methods with established baselines: Universal Guidance (UG) , Loss-Guided Diffusion with Monte Carlo (LGD-MC) , Training-Free Energy-Guided Diffusion Models (FreeDoM) , and Manifold Preserving Guided Diffusion (MPGD) . LGD-MC utilizes (6) while UG and FreeDoM are built on (5). MPGD utilizes an auto-encoder to ensure the manifold constraints. Furthermore, time travel trick (Algorithm 3) is adopted in UG, FreeDoM, and MPGD to improve the performance. Please refer to Appendix B for details of baselines. For the sampling method, DDIM with \(100\) steps is adopted as in [49; 37]. The method "Ours" is built on FreeDoM, with Polyak step size and random augmentation.

### Guidance to CelebA-HQ Diffusion

In this subsection, we adopt the experimental setup from . Specifically, we utilize the CelebA-HQ diffusion model  to generate high-quality facial images. We explore three guidance conditions: segmentation, sketch, and text. For segmentation guidance, BiSeNet  generates the facial segmentation maps, with an \(_{2}\)-loss applied between the estimated map of the synthesized image and the provided map. Sketch guidance involves using the method from  to produce facial sketches, where the loss function is the \(_{2}\)-loss between the estimated sketch of \(}_{0}\) and the given sketch. For text guidance, we employ CLIP  as both the image and text encoders, setting the loss to be the \(_{2}\) distance between the image and text embeddings.

We randomly select \(1000\) samples each of segmentation maps, sketches, and text descriptions. The comparative results are presented in Table 1. Consistent with , the time-travel number for all methods is set to \(s=1\). Figure 4 displays a random selection of the generated images. More image samples are provided in the supplementary materials. We find that the baselines failed to guide if the condition differs from unconditionally generated images significantly, as discussed in Section 4.2.

### Guidance to ImageNet Diffusion

For the unconditional ImageNet diffusion, we employ text guidance in line with the approach used in FreeDoM and UG [2; 49]. We utilize CLIP-B/16 as the image and text encoder, with cosine similarity

Figure 3: The effects of step size.

serving as the loss function to measure the congruence between the image and text embeddings. To evaluate performance and mitigate the potential for high-scoring adversarial images, we use CLIP-L/14 for computing the CLIP score. In FreeDoM and MPGD-Z, resampling is conducted for time steps ranging from \(800\) to \(300\), with the time-travel number fixed at \(10\), as described in . Given that UG resamples at every step, we adjust its time-travel number \(s=5\) to align the execution time with that of FreeDoM. The textual prompts for our experiments are sourced from . The comparison of different methods is depicted in Table 2. The corresponding randomly selected images are illustrated in Figure 5. The table indicates that our method achieves the highest consistency with the provided prompts. As shown in Figure 5, LGD-MC and MPGD tend to overlook elements of the prompts. Both UG and FreeDoM occasionally produce poorly shaped objects, likely influenced by misaligned gradients. Our approach addresses this issue through the implementation of random augmentation. Additionally, none of the methods successfully generate images that accurately adhere to positional prompts such as "left to" or "below". This limitation is inherent to CLIP and extends to all text-to-image generative models . More image samples are provided in the supplementary materials.

### Guidance to Human Motion Diffusion

In this subsection, we extend our evaluation to human motion generation using the Motion Diffusion Model (MDM) , which represents motion through a sequence of joint coordinates and is trained on a large corpus of text-motion pairs with classifier-free guidance. We apply the targeting guidance and object avoidance guidance as described in . Let \(_{0}(t)\) denote the joint coordinates at time \(t\), \(_{t}\) the target location, \(_{}\) the obstacle location, \(r\) the radius of the objects, and \(T\) the total number of frames. The loss function is defined as follows:

\[=\|_{t}-_{0}(T)\|_{2}^{2}+_{i}(-(\|_{ 0}(i)-_{}\|-r) 50) 100. \]

Our experimental configuration adheres to the guidelines set forth in . We assess the methods using the targeting loss (the first term in (8)), the object avoidance loss (the second term in (8)), and the CLIP score calculated by MotionCLIP . In this application, MPGD-Z cannot be applied as there are no auto-encoder. MPGD w/o proj suffers from the shortcut and cannot achieve good performance, as discussed in Appendix B.2. In our method, random augmentation is omitted because the guidance is not computed by neural networks so the adversarial gradient issues are not obvious. The quantitative results of our investigation are summarized in Table 3, while Figure 6 showcases randomly selected samples. Our methods exhibit enhanced control quality over the generated motion. The videos are provided in the supplementary materials.

    &  &  &  \\   & Distance\(\) & FID\(\) & Distance\(\) & FID\(\) & Distance\(\) & FID\(\) \\  UG  & 2247.2 & 39.91 & 52.15 & 47.20 & 12.08 & 44.27 \\ LGD-MC  & 2088.5 & 38.99 & 49.46 & 54.47 & 11.84 & 41.74 \\ FreeDoM  & 1657.0 & 38.65 & 34.21 & 52.18 & 11.17 & 46.13 \\ MPGD-Z  & 1976.0 & 39.81 & 37.23 & 54.18 & 10.78 & 42.45 \\ Ours & **1575.7** & **33.31** & **30.41** & **41.26** & **10.72** & **41.25** \\   

Table 1: The performance comparison of various methods on CelebA-HQ with different types of zero-shot guidance. The experimental settings adhere to Table 1 of .

   Methods & LGD-MC  & UG  & FreeDoM  & MPGD-Z  & Ours \\  CLIP Score\(\) & 24.3 & 25.7 & 25.9 & 25.1 & **27.7** \\   

Table 2: The performance comparison of various methods on unconditional ImageNet with zero-shot text guidance. We compare various methods using ImageNet pretrained diffusion models with CLIP-B/16 guidance. For evaluating performance, the CLIP score is computed using CLIP-L/14.

### Related Work on Training-Free Guidance

Due to space limitation, we only introduce the related work on training-free guidance while leaving more related work in Appendix A. The current training-free guidance strategies for diffusion models can be divided into two primary categories. The first category is the loss-based guidance in this paper, which is universally applicable to universal control formats and diffusion models. These methods predict a clean image, subsequently leveraging pretrained networks to guide the diffusion process. Central to this approach are the algorithms based on (5), which have been augmented through techniques like time-travel [2; 49] and the introduction of Gaussian noise . The adjoint sensitivity method  and spherical Gaussian constraint  have been adopted to estimate a more accurate guidance gradient. Extensions of these algorithms have found utility in domains with constrained data-condition pairs, such as molecule generation , and in scenarios necessitating zero-shot guidance, like open-ended goals in offline reinforcement learning . In molecular generation and offline reinforcement learning, they outperform training-based alternatives as additional training presents challenges. This paper delves deeper into the mechanics of this paradigm and introduces a suite of enhancements to bolster its performance. The efficacy of our proposed modifications is demonstrated across image and motion generation, with promising potential for generalization to molecular modeling and reinforcement learning tasks.

The second category of training-free guidance is tailored to text-to-image or text-to-video diffusion models, which is based on insights into their internal backbone architecture. For instance, object layout and shape have been linked to the cross-attention mechanisms , while network activations have been shown to preserve object appearance . These understandings facilitate targeted editing of object layout and appearance (Diffusion Self-Guidance ) and enable the imposition of conditions in ControlNet through training-free means (FreeControl ). Analyzing these methodologies is challenging due to their reliance on emergent representations during training. Nonetheless, certain principles from this paper remain relevant; for example, as noted in Proposition 3.3, these methods often necessitate extensive diffusion steps, with instances such as [30; 11] employing \(1000\) steps. A thorough examination and refinement of these techniques remain an avenue for future research.

## 6 Conclusions

In this paper, we conducted a comprehensive investigation into training-free guidance, which employs pretrained diffusion models and guides them using the off-the-shelf trained on clean images. Our exploration delved into the underlying mechanisms and fundamental limits of these models. Moreover, we proposed a set of enhancement techniques and verified their effectiveness both theoretically and empirically.

**Limitations.** Despite our efforts to mitigate the shortcomings of training-free methods and enhance their performance, certain limitations remain. Notably, the refined training-free guidance still necessitates a higher number of NFEs when compared with extensive training methods such as classifier-free guidance. This is because misaligned gradient cannot be fully eliminated without training.

**Ethical Consideration.** Similar to other models designed for image creation, our model also has the unfortunate potential to be used for creating deceitful or damaging material. We pledge to restrict the usage of our model exclusively to the realm of research to prevent such misuse.

   &  &  &  &  \\  & Loss\(\) & CLIP\(\) & Loss\(\) & CLIP\(\) & Loss\(\) & CLIP\(\) & Loss\(\) & CLIP\(\) \\  Unconditional  & \(3.55+9.66\) & 65.6 & \(47.92+0\) & **70.8** & \(48.88+0\) & 37.6 & \(144.84+0\) & **61.72** \\ FreeDoM  & \(1.09+6.63\) & 67.23 & \(9.83+4.48\) & 62.65 & \(1.64+7.55\) & 40.12 & \(34.95+7.83\) & 58.74 \\ LGD-MC  & \(0.98+6.48\) & 67.31 & \(4.42+0.02\) & 63.13 & \(1.30+0.39\) & 38.82 & \(6.12+2.38\) & 57.89 \\ Ours & **0.68+1.32** & **67.50** & **1.13+0.30** & 63.02 & **0.43+0.31** & **40.40** & **2.93+1.15** & 60.03 \\  

Table 3: Comparison of various methods on MDM with zero-shot targeting and object avoidance guidance. Loss is reported as a two-component metric: the first part is the MSE between the target and the actual final position of the individual; the second part measures the object avoidance loss.

Figure 4: Qualitative results of CelebA-HQ with zero-shot segmentation, sketch, and text guidance. The images are randomly selected.

Figure 5: Qualitative results of ImageNet model with zero-shot text guidance. The images are randomly selected.

Figure 6: Qualitative results of human motion diffusion with zero-shot object avoidance and targeting guidance. Instances of intersection with obstacles are highlighted by marking the person in red. The trajectories are randomly selected.