# Deep Support Vectors

Junhoo Lee

Hyunho Lee

Kyomin Hwang

Nojun Kwak

Seoul National University

{mrjunoo, hhlee822, kyomin98, nojunk}@snu.ac.kr

Corresponding Author

###### Abstract

Deep learning has achieved tremendous success. However, unlike SVMs, which provide direct decision criteria and can be trained with a small dataset, it still has significant weaknesses due to its requirement for massive datasets during training and the black-box characteristics on decision criteria. This paper addresses these issues by identifying support vectors in deep learning models. To this end, we propose the DeepKKT condition, an adaptation of the traditional Karush-Kuhn-Tucker (KKT) condition for deep learning models, and confirm that generated Deep Support Vectors (DSVs) using this condition exhibit properties similar to traditional support vectors. This allows us to apply our method to few-shot dataset distillation problems and alleviate the black-box characteristics of deep learning models. Additionally, we demonstrate that the DeepKKT condition can transform conventional classification models into generative models with high fidelity, particularly as latent generative models using class labels as latent variables. We validate the effectiveness of DSVs using common datasets (ImageNet, CIFAR10 and CIFAR100) on the general architectures (ResNet and ConvNet), proving their practical applicability. (See Fig. 1)

## 1 Introduction

Although deep learning has gained enormous success, it requires huge amounts of data for training, and its black-box characteristics regarding decision criteria result in a lack of reliability. For example, CLIP  needs 400 million image pairs for training and Stable Diffusion XL (SDXL)  requires

Figure 1: Generated images using a model trained with the ImageNet, CIFAR10, and CIFAR100 datasets, respectively. **Each image was generated without referencing the original training data.**5 billion images. This implies only a small number of groups can train foundation models from scratch. Also, the black-box nature makes it hard to anticipate the model's performance in different environments. For example, suppose we are to classify pictures of deer and most training deer images contain antlers. For the test images taken in winter, the performance will be worse as deer shed their antlers in winter. As modern deep learning models do not provide any decision criterion _i.e_., black box, we cannot determine whether the domain of the model has shifted, or if the model is biased in advance, thus cannot anticipate the performance drop in this case.

Interestingly, these problems do not occur in previous state-of-the-art, support vector machines (SVMs), which require substantially less data, enabling almost anyone to train a model from scratch. Also, as it encodes the decision boundary explicitly, SVM can reconstruct the support vectors from the training dataset using the KKT condition. Since it is a white box, one can anticipate the test's performance in advance. In the deer classification example, if the model's support vectors of deer have prominent antlers, using that SVM is not appropriate for photos taken in winter.

In this paper, we tackle the natural limitations of deep learning - the need for large data and black-box characteristics - by extracting SVM features in deep learning models. In doing so, we introduce the DeepKKT condition for deep models, which corresponds to the KKT condition in traditional SVMs. By either selecting deep support vectors (DSVs) from training data or generating them from already trained deep learning models, we show DSVs can play a similar role to conventional support vectors. Like support vectors can reconstruct SVM, we can reconstruct the deep models from scratch only with DSVs. Also, we show that DSVs encode the decision criterion visually, providing a global explanation for the trained deep model. Expanding beyond conventional support vectors, DSVs suggest that a trained deep classification model can also function as a latent generative model by utilizing logits as latent variables and applying DeepKKT conditions.

To this end, we generalize the KKT condition and define the DeepKKT condition considering that the data handled by a deep model is high-dimensional and multi-class. We demonstrate that the selected data points (selected DSVs) among the training data satisfying the DeepKKT condition are closer to the decision boundary than other training data, as evidenced by comparing entropy values. Also, we show that the calculated Lagrangian multiplier can reveal the level of uncertainty of the model for the sample in question. Additionally, we demonstrate that the DSVs outperform existing algorithms in the few-shot dataset distillation setting, where only a portion of the training set is used, indicating that DSVs exhibit characteristics similar to SVMs. Moreover, we confirm that modifying existing images using information obtained from DSVs allows us to change their class at will, verifying that DSVs can meaningfully explain the decision boundary. Finally, by using soft labels as latent variables in ImageNet, we generate unseen images with high fidelity.

Our contributions are as follows:

* By generalizing the KKT condition for deep models, we propose the DeepKKT condition to extract Deep Support Vectors, which are applicable to general architectures such as ConvNet and ResNet.
* We verify that DeepKKT can be used to extract and generate DSVs for common image datasets such as CIFAR10, CIFAR100, SVHN, and ImageNet.
* DSVs are better than existing algorithms in few-shot dataset distillation problems.
* DeepKKT acts as a novel model inversion algorithm that can be applied in practical situations.
* By using the DeepKKT condition, we not only show the trained deep models can reconstruct data, but also can serve as latent generative models by using logits as latent.

## 2 Related Works

### SVM in Deep Learning

Numerous studies have endeavored to establish a connection between deep learning and conventional SVMs. In the theoretical side,  demonstrated that, in an overparameterized setting with linearly separable data, the learning dynamics of a model possessing a logistic tail are equivalent to those of a support vector machine, with the model's normalized weights converging towards a finite value. Following this,  extended this equivalence to feedforward networks. This line of research relies on strong assumptions such as full-batch training and non-residual architecture without data augmentation. There also exists a body of work on integrating SVM principles into deep learning, often referred to as DeepSVM, aiming to leverage SVM's desirable properties [30; 29; 27; 23; 21]. DeepSVM integrates SVM components, specifically using them as feature extractors to derive meaningful, human-crafted features.

In contrast, our work does not modify or incorporate SVM architectures. Instead, we focus on identifying support vectors directly within deep learning models, thereby bridging the gap between deep learning and support vector machines in a more fundamental manner. Despite these advancements, there remains a lack of research that directly connects support vectors through a theoretical lens of equivalence. In this study, we address this gap by introducing the DeepKKT condition, a KKT condition tailored for deep learning, allowing us to apply the concept of support vectors in a practical deep learning context.

We show that reconstructing support vectors in deep models is indeed feasible, and obtaining high-quality support vectors is achievable under much less restrictive conditions compared to prior work.

### Model Inversion Through the Lens of Maximum Margian

There is a line of research utilizing the stationarity condition, a part of the KKT condition, for model inversion.  firstly exploited the KKT condition for model generation, adopting SVM-like architectures. They normally conducted experiments with binary classification, a 2-layer MLP, and full-batch gradient descent.  extended these experiments to multi-label classification by adapting their existing architecture to a multi-class SVM structure. To ensure the generated samples lie on the data manifold, they initialized with the dataset's mean, implying the adoption of some prior knowledge of the data. Similarly,  generated images through the stationarity condition, also adopting the mean-initialization and conducting experiments on the CIFAR10 , MNIST  and downsampled CelebA  datasets. Their work generally focused on low-dimensional, labeled datasets with a small number of classes such as CIFAR10 and MNIST, consistent with the traditional SVM setting of binary-labeled, low-dimensional datasets. In contrast, we extended our experiments to high-dimensional datasets with many classes, an area traditionally dominated by deep learning. Specifically, we conducted experiments on ImageNet  using a pretrained ResNet50 model following the settings described in the original paper .

Furthermore, previous works have concentrated on **reconstructing the training** dataset. In contrast, similar to generative models, our work focuses on **generating unseen** data from noise using a classification model. Additionally, we emphasize the original meaning of'support vectors'. Unlike other approaches, our Deep Support Vectors (DSVs) adhere to the traditional role of support vectors: they explain the decision criteria, and a small number of DSVs can effectively reconstruct the model.

### Dataset Distillation

Dataset distillation [2; 31; 13] fundamentally aims to reduce the size of the original dataset while maintaining model performance. The achievement also involves addressing privacy concerns and alleviating communication issues between the server and client nodes. The dataset distillation problem is typically addressed under the following conditions: 1) Access to the entire dataset for gradient matching, 2) Possession of snapshots from all stages of the model's training phase, which are impractical settings for practical usage . Furthermore, these algorithms typically require Hessian computation, which imposes a heavy computational burden.

In SVM, the model can be reconstructed using support vectors. This reconstruction is more practical compared to previous dataset distillation methods, as it does not require any of the restrictive conditions. Likewise, because Deep Support Vectors (DSVs) also do not require these conditions and are Hessian-free, they can play the role of distillation under practical conditions.

## 3 Preliminaries

### Notation

In the SVM formulation, \((:=[w;b])\) represents the concatenated weight vector \(w\) and bias \(b\). Each data instance, expanded to include the bias term, is denoted by \(_{i}(:=[x_{i};1])\), while the corresponding binary label is represented by \(y_{i}\{ 1\}\). The Lagrange multipliers are denoted by \(_{i}\)'s.

Transitioning to the context of deep learning, we denote the parameter vector of a neural network model by \(\), which, upon optimization, yields \(^{*}\) as the set of learned weights. The mapping function \((x_{i};)\) represents the transformation of input data into a \(C\)-dimensional logit in a \(C\)-class classification problem in a manner dictated by the parameters \(\), _i.e._, \((x_{i};)=[_{1}(x_{i};),,_{C}(x_{i};)]^{T} ^{C}\). We define the **score** as the logit of a target class, _i.e._, \(_{y_{i}}(x_{i};)\). If the score is the largest among logits, _i.e._, \(*{arg\,max}_{c}~{}_{c}(x_{i};)=y_{i}\), then it correctly classifies the sample. The Lagrange multipliers adapted to the optimization in deep learning are represented by \(_{i}\)'s. \((x,y)\) denotes a pair of input and output and \(\) is the index set with \(||=n\).

### Support Vector Machines

The fundamental concept of Support Vector Machines (SVMs) is to find the optimal hyperplane that classifies the given data. This hyperplane is defined by the closest data points to itself known as support vectors, and the distance between the support vectors and the hyperplane is termed the margin. The hyperplane must classify the classes correctly while maximizing the margin. This leads to the following KKT conditions that an SVM must satisfy: **(1) Primal feasibility:**\( i,~{}y_{i}^{T}_{i} 1\), **(2) Dual feasibility:**\( i,~{}_{i} 0\), **(3) Complementary slackness:**\(_{i}(y_{i}^{T}_{i}-1)=0\) and **(4) Stationarity:**\(=_{i=1}^{n}_{i}y_{i}_{i}\).

The primal and dual conditions ensure these critical values correctly classify the data while being outside the margin. The complementary slackness condition mandates that support vectors lie on the decision boundary. The stationarity condition ensures that the derivative of the Lagrangian is zero.

The final condition, stationarity, offers profound insights into SVMs. It underscores that support vectors encode the decision boundary \(\). Consequently, identifying the decision hyperplane in SVMs is tantamount to pinpointing the corresponding support vectors. This implies that with a trained model at our disposal, we can reconstruct the SVM in two distinct ways:

1. **Support Vector Selection:** From the trained model, we can extract support vectors among the training data that inherently encode the decision hyperplane.

2. **Support Vector Synthesis:** Alternatively, it is feasible to generate or synthesize support vectors, even in the absence of a training set, which can effectively represent the decision hyperplane by generating samples that satisfy \(|^{T}|=1\).

## 4 Deep Support Vector

This section presents the specific conditions that DSVs (Deep Support Vectors) must satisfy and discusses how to get an optimization loss to meet these conditions.

### DeepKKT

SVM's Relationship with Hinge lossWe start our discussion by focusing on the _hinge loss_, a continuous surrogate loss for the primal feasibility, and its gradient:

\[& L_{h}(x_{i},y_{i} ;)=_{i=1}^{n}(0,1-y_{i}(^{T}_{i} )),\\ &_{}L_{h}=_{i=1}^{n}0,& y_{i}(^{T}_{i}) 1\\ -y_{i}_{i},&. \]

With Eq. (1), the stationarity condition of SVM becomes

\[w^{*}:=-_{i=1}^{n}_{i}_{w}L_{h}(x_{i},y_{i};w^{*}),_{i} 0. \]

Generalization of conventional KKT conditionsIn this paper, we extend the KKT conditions to deep learning. In doing so, the two main hurdles of a deep network different from a linear binary SVM are 1) the nonlinearity of \((x_{i};)\) taking the role of \(^{T}_{i}\) and 2) multi-class nature of a deep learning model.

Considering that the role of the primal feasibility condition is to correctly classify \(x_{i}\) into \(y_{i}\), we can enforce the score \(_{y_{i}}(x_{i};)\) for the correct class \(y_{i}\) to take the maximum value among all the logits with some margin \(\), _i.e._,

\[_{y_{i}}(x_{i};^{*})-_{c y_{i},c[C]}_{c}(x_{i};^ {*}), \]

We can relax this discontinuity with a continuous surrogate function \(-L((x_{i};^{*}),y_{i})\), which is the negative loss function to maximize. Note that if we take the cross-entropy loss for \(L\), it becomes

\[-L_{ce}=_{y_{i}}(x_{i};^{*})-_{c=1}^{C}(_{c}(x_{i}; ^{*})), \]

which takes a similar form as Eq. (3) and the negative loss can be maximized to meet the condition.

Now that we found the analogy between \(y_{i}^{T}_{i}\) and \(-L((x_{i};^{*}),y_{i})\), \(y_{i}_{i}(=_{}(y_{i}^{T}_{i}))\) corresponds to \(-_{^{*}}L((x_{i};^{*}),y_{i})\). Thus, the stationary condition \(=_{i=1}^{n}_{i}y_{i}_{i}\) in SVMs can be translated into that of deep networks such that

\[^{*}=-_{i=1}^{n}_{i}_{^{*}}L((x_{i};^{ *}),y_{i}). \]

This condition is a generalized formulation of Eq. (2), where we substitute the linear model \(^{T}\) with a nonlinear model \((x;)\), and the binary classification hinge loss \(L_{h}\) with multi-class classification loss \(L\). Furthermore, the stationarity condition reflects the dynamics of overparameterized deep learning models. We provide an analogy with respect to  in Appendix C.

However, these conditions are not enough for deep learning. As mentioned before, we are interested in dealing with high dimensional manifolds. Compared to the problems dealt with in the classical SVMs, the input dimensions of deep learning problems are typically much higher. In this case, the data are likely to lie along a low-dimensional latent manifold \(\) inside the high-dimensional space. To make a generated sample be a plausible DSV, it not only should satisfy the generalized KKT condition but also should lie in an appropriate data manifold, _i.e._, \(x\).

Finally, we can rewrite the new **DeepKKT condition** as follows:

\[& \,i,*{arg\,max}_{c}\;_{c}(x_{i};^{ *})=y_{i}\\ &\,i ,_{i} 0,\\ &^{*}=-_{i=1}^{n} _{i}_{}((x_{i};^{*}),y_{i}),\\ &\,i,  x_{i}. \]

### Deep Support Vectors, From Dust to Diamonds

Sec. 4.1 explored the KKT conditions in the context of deep learning and how these conditions can be used to formulate a loss function. It is important to note that our goal is not to construct the model \(\), but rather to generate **support vectors of an already-trained model \(\) with its parameter \(^{*}\)**.

To reconstruct support vectors from a trained deep learning model, sampling or synthesizing support vectors is essential. By replacing the optimization variable \(\) with \(x\), we shift our focus to utilizing the DeepKKT conditions for generating or evaluating input \(x\) rather than \(\). This adjustment necessitates a consideration of the data's inherent characteristics, specifically its multiclass nature and the tendency of the data to reside on a lower-dimensional manifold in an ambient space.

  img/cls & ratio (\%) & Random & selected DSVs \\ 
50 & 1 & \(46.16 1.93\) & \(48.91 0.90\) \\
10 & 0.2 & \(30.08 1.96\) & \(33.69 2.05\) \\
1 & 0.02 & \(14.26 0.99\) & \(16.83 0.29\) \\  

Table 1: In coreset selection benchmarks using the CIFAR-10 dataset, the DeepKKT condition is used as the selection criterion. Images with the highest \(\) for each class were chosen to train a network.

Pimal FeasibilityFirstly, the primal feasibility condition in Eq. (6) mandates support vectors be correctly classified by the trained model \(^{*}\). As presented in Sec. 4.1, instead of Eq. (3), we use a surrogate function for the loss:

\[L_{}=_{i=1}^{n}L_{i}, L_{i}= 0&\;_{c}_{c}(x_{i};^{*})=y_{i},\\ L((x_{i};^{*}),y_{i})&. \]

This is designed to match the primal condition by mimicking the Hinge loss. When each DSV is correctly classified, no loss is incurred. Otherwise, we adjust the DSVs to align them with the correct target, effectively optimizing the support vectors. This approach also implicitly enforces the complementary slackness condition, as \(L_{i}\) decreases confidence in the incorrect classification.

Here, \(L\) can be any loss function and we have employed the cross-entropy loss in our experiments.

StationaritySecondly, the stationarity condition can be used directly as a loss function. Since we are extracting DSVs from the trained model \((;^{*})\), we construct this loss as follows:

\[L_{}=D(^{*},-_{i=1}^{n}_{i}_{}L((x _{i};^{*}),y_{i})). \]

For the distance measure \(D\), any metric can be used; we have chosen to use the \(l_{1}\) distance to suppress the effect of outliers. It is crucial to remember that our objective is to find DSVs and the optimization is done for the primal and dual variables \(x_{i}\) and \(_{i}\) and not for the parameter \(\). For this, we require one forward pass and two backward passes; one for \(_{}L\) and the other for \(_{x_{i}}D\). The overall computational cost is quite low, as we optimize only a small number of samples.

Moreover, as shown in Algorithm 1 (Appendix H), we satisfy the dual condition by ensuring the Lagrange multipliers \(_{i}\)'s are greater than zero and disqualify any \(x_{i}\)'s from being a support vector candidate if during optimization \(_{i}\) becomes less than zero. The condition that is not explicitly satisfied is the complementary slackness. To directly fulfill the functional boundary for support vectors as specified in Sec. 3.2, we would need to be able to calculate the distance between functions, which is not only abstract but also requires a second-order computation cost. Therefore, we adopted a relaxed version of the KKT conditions that excludes this requirement. Furthermore, as demonstrated in Sec. 5.1, we have shown that DSVs implicitly satisfy the complementary slackness condition. This implies that DSVs meet every condition introduced in conventional SVM.

Manifold Condition: Reflecting High-Dimensional Dynamics of Deep LearningAs modern deep learning deals with extremely high-dimensional spaces, imposing additional constraints other than the primal feasibility and stationarity conditions is needed so that DSVs reside in the desired data manifold. To achieve this, we add a manifold condition, which enforces that the DSVs lie on the data manifold. By selecting DSVs that are in the intersection of the solution subspace and the data manifold, we can properly represent both the model and the training dataset.

To extract DSVs from the manifold, we assume that the model is well-trained, meaning it maintains consistent decisions despite data augmentation. In other words, the model should classify DSVs invariantly even after augmentation. To ensure this, we enforce that the augmented DSVs (\((x)\) where \(\) denotes augmentation function) also meet the primary and stationarity conditions.

Figure 2: Characteristics of DSVs; (Left) Entropy change of DSV candidates over time, (Right) Correlation between classwise mean test accuracy and the sum of \(\)’s

Also, we exploit traditional image prior [33; 16], total variance \(L_{}\) and size of the norm \(L_{norm}\) to make DSVs lie in the data manifold. \(L_{}\) is calculated by summing the differences in brightness between neighboring pixels, reducing unnecessary noise in an image, and maintaining a natural appearance. \(L_{norm}\), taking a similar role, penalizes the outlier and preserves important pixels.

Finally our DSV is obtained as follows where \(_{}\) represents expectation over augmentations:

\[=*{arg\,min}_{x}_{}[L_{}((x))+_{1}L_{}((x))+_{2}L_{ }(x)+_{3}L_{}(x)]. \]

One might wonder if there is a better sampling strategy than using DSVs, such as sampling far from the decision boundary instead of near it. We argue that in a high-dimensional data manifold, most data points are located close to the decision boundary because, in a data-scarce, high-dimensional space, every sample matters and thus would serve as a DSV.

## 5 Experiments

### DSVs: Revival of Support Vectors in Deep Learning

DSVs meet SVM characteristicsAs discussed in Section 3.2, the principle of complementary slackness within the KKT conditions suggests that support vectors should be situated on the decision boundary, implying that support vectors typically exhibit high uncertainty from a probabilistic perspective, _i.e._, they possess high entropy. While DeepKKT does not explicitly incorporate the complementary slackness condition due to computational costs and ambiguity, Fig. 2a suggests that DSVs implicitly fulfill this condition; During the training process, we observe an increase in the entropy of DSV candidates, hinting that the generated DSVs are close to the decision boundary.

In addition, we can infer the importance of a sample in the decision process by utilizing the DeepKKT condition. We trained the Lagrangian multiplier \(\) for each test image. Figure 2b shows a strong correlation between the sum of \(\) values for each class and its test accuracy. This finding is intriguing because, despite the model achieving nearly 100% accuracy during training due to overparameterization, DSVs provide insights into categorical generalization in the test phase. Not only does measuring its credibility indicate that a large \(\) refers to an 'important' image for training, but \(\) could also serve as a natural core-set selection measure. Table 1 shows this to be true. On the CIFAR-10  dataset, we selected images with high \(\) values and retrained the network with the selected images. In this case, the selected DSVs show higher test acccuracies compared to random selection. This characteristic resembles that of support vectors, as the model can be reconstructed with support vectors.

Finally, Fig. 1 demonstrates the high fidelity of the generated DSVs, providing practical evidence that these DSVs lie on the data manifold. Similar to how support vectors are reconstructed in an SVM, the DeepKKT condition enables the reconstruction of these vectors without referencing training data. This shows the effectiveness and adaptability of our DeepKKT in capturing key data features.

DSVs for Few Shot Dataset DistillationDeepKKT emerges as a pioneering algorithm tailored for practical dataset distillation. DSVs addresses two critical concerns: 1) Protecting private information through data synthesis, and 2) Reducing the communication load by minimizing the size of data transmission. Traditional distillation algorithms encounter a fundamental paradox; as data predominantly originate from edge devices like smartphones [18; 17; 10], the requirement to access the entire dataset introduces significant communication overhead and heightens privacy concerns.

Figure 3: Model predictions for original versus DSV-informed edited images. (Top) Images were altered manually based on decision criteria derived from DSVs, influencing the model’s prediction. (Bottom) Images were altered based on DeepKKT loss.

Our DeepKKT relies solely on a pre-trained model without relying on the training dataset. This unique approach eliminates the need for edge devices to store or process large volumes of private data. As shown in Table 2, while traditional methods suffer significant performance drops under these scenarios and are incapable of implementing zero-shot scenarios. Conversely DeepKKT remains effective, requiring only minimal data: a single image per sample (_i.e_., initialization with real data), or in some cases, no images at all. For the zero-image setting, we initialized the images with data from other datasets to ensure diversity.

DSVs Encode the Decision Criteria VisuallyOur findings suggest that DSVs not only satisfy the conditions of classical support vectors but also offers a global explanation of visual information.

Fig. 3 experimentally verifies our claims and illustrates the practical use of DSVs, _e.g_., analysis of Fig. 1-Cifar10 reveals the decision criteria for classifying deer, cats, and dogs: 1) DSVs highlight antlers in deer, signifying them as a distinctive characteristic. 2) Pointed triangular ears are a recurring feature in DSVs of cats. 3) For dogs, a trio of facial dots holds significant importance. Using these observations, we altered a deer's image by erasing its antlers and reshaping its ears to a pointed contour, which reduced the model's confidence in classifying it as a deer, and caused the model to misclassify it as a cat. Similarly, by smoothing the ears of a cat image to diminish its classification confidence and then adding antlers or three facial dots, we influenced the model to reclassify the image as a deer or a dog, respectively. Additionally, the DeepKKT-altering case in Fig. 3-Bottom supports our assertions. Altering a flower image to resemble a deer class by changing the target class in the primal and dual feasibility loss, antlers grew similar to our manipulation.

This discovery holds significant implications about making models responsible; it introduces a qualitative aspect to assessing model performance. Consider a deer classification problem again. The model in our study would be less suitable, as evidenced by Fig. 1-Cifar10-deer, which indicates the model's reliance on antlers for identifying deer - a feature not present in winter. This shows that DSVs enable us to conduct causal predictions by qualitatively analyzing models, as SVM does.

### Unlocking the potential of classifier as generator with DeepKKT

Practical Model Inversion with DeepKKTIn cloud environments or APIs, models are deployed with the belief that although they are sometimes trained with sensitive information, their black-box nature prevents users from inferring the data. This belief makes it possible to deploy sensitive models. However, as demonstrated in Fig 1, this belief is no longer valid. Fig. 4 further illustrates that model inversion remains feasible even in practical scenarios such as transfer learning scenarios, where only specific layers of a foundation model are fine-tuned. Remarkably, DeepKKT condi

 img/cls & shot/class & ratio (\%) & DC  & DSA  & DM  & DSVs \\   & 0 & 0 & - & - & - & 21.68 \(\) 0.80 \\  & 1 & 0.02 & 16.48\(\)0.81 & 15.41\(\)1.91 & 13.03\(\)0.15 & **22.69 \(\)** 0.38 \\  & 10 & 0.2 & 19.66\(\)0.78 & 21.15\(\)0.58 & 22.42\(\)0.43 & - \\  & 50 & 1 & 25.90\(\)0.62 & 26.01\(\)0.70 & 24.42\(\)0.29 & - \\  & 500 & 10 & 28.06\(\)0.61 & 28.20\(\)0.63 & 25.06\(\)1.20 & - \\   & 0 & 0 & - & - & - & 30.35 \(\) 0.99 \\  & 10 & 0.2 & 25.06\(\)1.20 & 26.67\(\)1.04 & 29.77\(\)0.66 & **37.90 \(\)** 1.69 \\  & 50 & 1 & 36.44\(\)0.52 & 36.63\(\)0.52 & 36.63\(\)0.52 & - \\  & 500 &tions enable model inversion even in these challenging environments, suggesting that they can be applied to a subset of the parameter space rather than the entire parameter space _i.e_., a more relaxed condition.

Classifier as Latent generative modelConsidering the impressive capabilities of DSVs in the image generation domain, and the geometric interpretation that DSVs are samples near the decision boundaries, it is noted that enforcing the DeepKKT condition resembles the diffusion process. In each iteration, the DSV develops through the DeepKKT condition as follows:

\[x_{t+1}=x_{t}-([_{x}L_{}((x_{t}))+ _{2}L_{}(x_{t})+_{3}L_{}(x_{t})]+_{1} _{x}L_{}((x_{t}))). \]

This is similar to the generalized form of the score-based diffusion process:

\[x_{t+1}=x_{t}+_{t}(_{x} p(x_{t})+_{x}  p(y|x_{t})). \]

The first three loss terms in Eq. (10) aim to maximize the score (\(_{x} p(x_{t})\)), while the last term, the primal feasibility term, corresponds to the guidance term (\(_{x} p(y|x_{t})\)). As shown in Fig. 5, when only the primal loss term is used, meaningful DSV samples are not generated. This indicates that the other losses (stationarity and manifold terms) function update the image towards manifold _i.e_., score function. From this perspective, an arbitrarily assigned label \(y\) can be used as a latent variable for guidance.

To experimentally verify this, we performed a latent interpolation task and image editing, which is common

Figure 5: Results showing DeepKKT images created solely by the primal condition or by the stationary condition. A sole usage of the primal condition shows low fidelity.

Figure 6: (Top) Generated DSVs using soft labels: \(\) was set 0.6, _i.e_., soft label \(y=0.4y_{}+0.6y_{}\). (Middle) Examples of latent (soft-label) interpolation. (Bottom) Image Editing through latent.

in generative models [8; 3] By mixing different labels (\(y_{i}=(1-)y_{a}+ y_{b}\), where \(y_{a} y_{b}\)), we generated DSVs as depicted in Fig. 6. When generating DSVs with these mixed soft labels, the generated DSVs semantically represent the midpoint between the two classes. A generated DSV either simply contains both images (the case of "balloon" and "spaniel") or semantically 'fuse' objects (the case of "lobster" and "harp", producing an image of a harp made out of lobster claws). For the image editing task, we assigned the latent variable to the desired class and then aligned the image using DeepKKT loss. The result was quite surprising: the method successfully transferred the image to the desired class while maintaining the original structure. For example, the sail of a yacht was seamlessly transformed into the shape of a scarf. This task was impossible with other methods; in diffusion models, for instance, a mask would be needed to edit the image seamlessly.

The fact that the generated images correctly merge the semantics of the classes suggests a couple of significant implications: 1) 1) **New Generative Model**: This approach offers a new type of generative model as an alternative to GANs and diffusion models. It can handle the same task without the need for training a specific model, as it leverages existing classification models for generative purposes. Furthermore, it is lightweight compared to diffusion models. For example, as the model size of a pretrained ResNet50 for ImageNet is only one-twentieth of that for SDXL , DSVs show a potential to leverage existing classification models for generative purposes. 2) **Exploration of Classification Model Generalization**: Unlike other generative models, classification models are trained simply to predict the label of an image. Yet, in latent interpolation and editing tasks, they demonstrate an understanding of semantics. This implies that, despite being trained to memorize class labels, the models grasp the overall semantics of the dataset. As they can generate seemingly unseen samples by interpolation and editing.

## 6 Conclusion

In this paper, we redefined support vectors in nonlinear deep learning models through the introduction of Deep Support Vectors (DSVs). We demonstrated the feasibility of generating DSVs using only a pretrained model, without accessing to the training dataset. To achieve this, we extended the KKT (Karush-Kuhn-Tucker) conditions to DeepKKT conditions and the proposed method can be applied to any deep learning models.

Akin to SVMs, the DeepKKT condition effectively encodes the decision boundary into DSVs. DSVs can reconstruct the model, making them useful for dataset distillation. Additionally, their visual encoding of the decision criteria can serve as a global explanation, helping to understand the model's overall behavior and decisions. Furthermore, the DeepKKT condition transforms a classification model into a generative model with high fidelity. Not only can it sample data, but it also generalizes well, allowing the use of labels as latent variables.