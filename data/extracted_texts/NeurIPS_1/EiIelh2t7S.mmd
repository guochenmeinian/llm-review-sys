# Base of RoPE Bounds Context Length

Mingyu Xu\({}^{1}\), Xin Men\({}^{1}\), Bingning Wang\({}^{1}\), Qingyu Zhang\({}^{2}\),

**Hongyu Lin\({}^{2}\), Yaojie Lu\({}^{2}\), Xianpei Han\({}^{2}\) and Weipeng Chen \({}^{1}\)**

\({}^{1}\) Baichuan Inc.

\({}^{2}\) Chinese Information Processing Laboratory

Institute of Software, Chinese Academy of Sciences

{menxin,xumingyu,daniel}@baichuan-inc.com

{zhangqingyu2024,hongyu,yaojie,xianpei}@iscas.ac.cn

Equal contribution. Order determined by swapping the one in 

\({}^{1}\) Baichuan Inc.

\({}^{2}\) Chinese Information Processing Laboratory

Institute of Software, Chinese Academy of Sciences

{menxin,xumingyu,daniel}@baichuan-inc.com

{zhangqingyu2024,hongyu,yaojie,xianpei}@iscas.ac.cn

###### Abstract

Position embedding is a core component of current Large Language Models (LLMs). Rotary position embedding (RoPE), a technique that encodes the position information with a rotation matrix, has been the de facto choice for position embedding in many LLMs, such as the Llama series. RoPE has been further utilized to extend long context capability, which is roughly based on adjusting the _base_ parameter of RoPE to mitigate out-of-distribution (OOD) problems in position embedding. However, in this paper, we find that LLMs may obtain a superficial long-context ability based on the OOD theory. We revisit the role of RoPE in LLMs and propose a novel property of long-term decay, deriving that the _base of RoPE bounds context length_: there is an absolute lower bound for the base value to obtain certain context length capability. Our work reveals the relationship between context length and RoPE base both theoretically and empirically, which may shed light on future long context training.

Figure 1: Context length and its corresponding lower bound of RoPE’s base value.

Introduction

In the past few years, large language models have demonstrated surprising capabilities and undergone rapid development. By now, LLMs have been widely applied across various domains, including chatbots, intelligent agents, and code assistants [2; 3]. The Transformer , based on the attention mechanism, has been the most popular backbone of LLMs due to its good performance and scaling properties . One of the key component modules in the Transformer is position embedding, which is introduced to embed positional information that is vital for processing sequential data. Rotary position embedding (RoPE), which encodes relative distance information in the form of absolute position embedding , has been a popular choice and applied in many LLMs [7; 8; 9].

RoPE introduces no training parameters and shows improvement in language modeling and many other tasks [6; 10]. One reason that RoPE is widely used is its ability for context length extrapolation [11; 12], which extends the context length of a trained LLM without expensive retraining. In practice, many works [7; 13; 14] have successfully extended the window length by simply increasing base value, the only one hyper-parameter in RoPE, and fine-tuning on long texts.

The reasons behind the success of these long context extensions are often explained as avoiding out-of-distribution (OOD) rotation angles [15; 16] in RoPE, meaning the extended context length (OOD) can be mapped to the in-distribution context length that has been properly trained. Based on the OOD theory, a recent study  finds that a smaller base can mitigate OOD and is beneficial for the model's ability to process long contexts, which inspires us to further study the relationship between the base of RoPE and the length of context the model can process.

In this paper, we find that the model may show superficial long context capability with an inappropriate RoPE base value, in which case the model can only preserve low perplexity but loses the ability to retrieve long context information. We also show that the out-of-distribution (OOD) theory in position embedding, which motivates most length extrapolation works [11; 12; 15], is insufficient to fully reflect the model's ability to process long contexts. Therefore, we revisit the role of RoPE in LLMs and derive a novel property of long-term decay in RoPE: the ability to pay more attention to similar tokens than random tokens decays as the relative distance increases. While previous long context works often focus on the relative scale of the RoPE base, based on our theory, we derive an absolute lower bound for the base value of RoPE to obtain a certain context length ability, as shown in Figure 1. To verify our theory, we conducted thorough experiments on various LLMs such as Llama2-7B , Baichuan2-7B  and a 2-billion model we trained from scratch, demonstrating that this lower bound holds not only in the fine-tuning stage but also in the pre-training stage.

We summarize the contributions of the paper as follows:

* **Theoretical perspective**: we derive a novel property of long-term decay in RoPE, indicating the model's ability to attend more to similar tokens than random tokens, which is a new perspective to study the long context capability of the LLMs.
* **Lower Bound of RoPE's Base**: to achieve the expected context length capability, we derive an absolute lower bound for RoPE's base according to our theory. In short, the base of RoPE bounds context length.
* **Superficial Capability**: we reveal that if the RoPE's base is smaller than a lower bound, the model may obtain superficial long context capability, which can preserve low perplexity but lose the ability to retrieve information from long context.

## 2 Background

In this section, we first introduce the Transformer and RoPE, which are most commonly used in current LLMs. Then we discuss long context methods based on the OOD of rotation angle theory.

### Attention and RoPE

The LLMs in current are primarily based on the Transformer . The core component of it is the calculation of the attention mechanism. The naive attention can be written as:

\[A_{ij}=q_{i}^{T}k_{j} \]

\[(X)=(A/)\ v, \]

where \(A R^{L L}\ q,k,v R^{d}\). Position embedding is introduced to use the order of the sequence in attention.

RoPE  implements relative position embedding through absolute position embedding, which applies rotation matrix into the calculation of the attention score in Eq. 1, which can be written as:

\[A_{ij}=(R_{i,}q_{i})^{T}(R_{j,}k_{i})=q_{i}^{T}R_{j-i,}k_{j} =q_{i}^{T}R_{m,}k_{j}, \]

where \(m=j-i\) is the relative distance of \(i\) and \(j\), \(R_{m,}\) is a rotation matrix denoted as:

\[[cos(m_{0})&-sin(m_{0})&0&0& &0&0\\ sin(m_{0})&cos(m_{0})&0&0&&0&0\\ 0&0&cos(m_{1})&-sin(m_{1})&&0&0\\ 0&0&sin(m_{1})&cos(m_{1})&&0&0\\ &&&&&&\\ 0&0&0&0&&cos(m_{d/2-1})&-sin(m_{d/2-1})\\ 0&0&0&0&&sin(m_{d/2-1})&cos(m_{d/2-1})] \]

Generally, the selection of rotation angles satisfies \(_{i}=base^{-2i/d}\), the typical base value for current LLMs is 10,000.

### OOD theory of relative rotation angle

Based on RoPE, researchers have proposed various methods to extend the long context ability of LLMs, among which representatives are PI  and NTK-series (NTK-aware , YaRN , and Dynamical-NTK ). Those methods depend on the relative scale \(s=T_{}/T_{}\), where \(T_{}\) is the training length of the original pre-trained model and \(T_{}\) is the training length in long-context fine-tuning.

PiPI directly interpolates the position embedding, and the calculation of \(A_{ij}\) becomes:

\[A_{ij}=(R_{i/s}q_{i})^{T}(R_{j/s}k_{i})=q_{i}^{T}R_{(j-i)/s}k_{j}=q_{i}^{T}R_{ m/s}k_{j}, \]

In other words, the position embedding of the token at position \(i\) in pre-training becomes \(i/s\) in fine-tuning, ensuring the position embedding range of the longer context remains the same as before.

Figure 2: An illustration of OOD in RoPE when we extend context length from 4k to 32k, and two solutions to avoid the OOD. We show the last dimension as it is the lowest frequency part of RoPE, which suffers OOD mostly in extrapolation. (a) For a 4k context-length model with base value as 1e4, when we extend the context length to 32k without changing the base value, the context length from 4k to 32k is OOD for RoPE (red area in the figure). (b) OOD can be avoided with a small base value like 500 , since the full period has been fitted during fine-tuning stage. (c) We set base as \(b s^{}\) from NTK .The blue line denotes the pre-training stage (base=1e4) and the red dashed line denotes the fine-tuning stage (base=\(b s^{}\)), we can observe that the RoPE’s rotation angle of extended positions is in-distribution.

NTK-seriesThe idea is that neural networks are difficult to learn high-frequency features, and direct interpolation can affect the high-frequency parts. Therefore, the NTK-aware method achieves high-frequency extrapolation and low-frequency interpolation by modifying the base value of RoPE. Specifically, it modifies the base \(b\) of the RoPE to:

\[b_{}=b\ s^{}. \]

The derivation of this expression is derived from \(T_{}b_{}^{-}=T_{}b^{-}\) to ensure that the lowest frequency part being interpolated.

A recent study  proposes to set a much smaller base (e.g. 500), in which case \(_{i}=base^{-}\) is small enough and typical training length (say 4,096) fully covers the period of \((t-s)_{i}\), so the model can obtain longer context capabilities.

One perspective to explain current extrapolation methods is the OOD of rotation angle [15; 16]. If all possible values of \((t-s)_{i}\) have been fitted during the pre-training stage, OOD would be avoided when processing longer context. Figure 2 demonstrates how these methods avoid OOD of RoPE.

## 3 Motivation

Recent advancements in long-context language models have seen widespread adoption of NTK-based methods [7; 13; 14]. However, a curious trend has emerged: practitioners often employ significantly larger base values than those originally suggested by NTK-aware approaches. This discrepancy raises critical questions about the efficacy of current theoretical frameworks. Why do practitioners deviate from the recommendations of NTK-based methods? Is the out-of-distribution (OOD) theory underlying these methods insufficient to unlock long-context capabilities fully?

On the other hand, recent research , driven by OOD theory, proposes using a much smaller base for RoPE to extend context length. However, our findings, as illustrated in Figure 3, suggest that this approach may only provide superficial long-context capability. While achieving low perplexity even at 128k context length (explicable by OOD theory), the model fails to retrieve relevant information for context lengths as short as 1kwell below its pre-trained length. The observation suggests that the small base determined by OOD theory can't unlock true long-context capability.

These phenomena motivate us to delve deeper into the relationship between RoPE's base and context length. To address the gap between OOD theory and our observations, we conduct a theoretical exploration in the next section, aiming to uncover the underlying mechanisms of effective long-context modeling.

## 4 Theory Perspective

For attention mechanism in language modeling, we have the following desiderata:

**Desiderata 1**: _The closer token gets more attention: the current token tends to pay more attention to the token that has a smaller relative distance._

Figure 3: The superficial long context capability of avoiding OOD by the smaller base. Following the recent work , we fine-tune Llama2-7B with a small base (500) to a context length of 32k.

**Desiderata 2**: _The similar token gets more attention: the token tends to pay more attention to the token whose key value is more similar to the query value of the current token._

Then we examine the desiderata when we apply RoPE to the attention mechanism in LLMs.

### Long-term Decay of Upper Bound of Attention Score

For Desiderata 1, the property of RoPE makes the model attend more to closer tokens. This kind of long-term decay has been thoroughly discussed in previous work [6; 23]. It comes from the upper bound of attention score calculation, which can be written as:

\[|A_{ij}|=|q_{i}^{T}R_{m}k_{j}| _{l}(|h_{l}-h_{l+1}|)_{n=1}^{d/2}|S_{n}|\] \[=_{l}(|h_{l}-h_{l+1}|)_{n=1}^{d/2}|_{l=0}^{n-1}e^{(j- i)_{l}}|, \]

where \(h_{l}=q_{i}^{T}[2l:l2+1]k_{j}[2l:2l+1]\). Equation 7 indicates that the upper bound of the attention score \(|A_{ij}|\) decays as the relative distance increases. Figure 5 shows the long-term decay curve of this upper bound, which is in accordance with previous findings [6; 23].

### Long-term Decay of the Ability to Attend More to Similar Tokens than Random Tokens

In addition to the attention score's upper bound, we also find there exists another long-term decay property in RoPE: the ability to attend more to similar tokens than random tokens decays as the relative distance increases. We define the ability to attend more to similar tokens than random tokens as:

\[_{q,k^{*}}[q^{T}R_{m,}k^{*}]-_{q,k}[ q^{T}R_{m,}k], \]

where \(q R^{d}\) is the query vector for the current token, \(k^{*}=q+\) is the key value of a similar token, where \(\) is a small random variable, \(k R^{d}\) is the key vector of a random token, \(R_{m,}\) is the rotation matrix in RoPE. The first term in Eq. 8 is the attention score of \(q\) and a similar token \(k^{*}\), the second term in Eq. 8 is the attention score of \(q\) and random token \(k\). Then we derive the following theorem:

**Theorem 1**: _Assuming that the components of query \(q R^{d}\) and key \(k R^{d}\) are independent and identically distributed, their standard deviations are denoted as \( R\). The key \(k^{*}=q+\) is a token similar to the query, where \(\) is a random variable with a mean of 0. Then we have:_

\[}(_{q,k^{*}}[q^{T}R_{m,}k^{*}]- _{q,k}[q^{T}R_{m,}k])=_{i=0}^{d/2-1}(m _{i}) \]

The proof is shown in Appendix A. We denote \(_{i=0}^{d/2-1}(m_{i})\) as \(B_{m,}\), and according to Theorem 1, \(B_{m,}\) measures the ability to give more attention to similar tokens than random tokens, which decreases as the relative distance \(m\) increases, as shown in Figure 5. For a very small base value, we can observe that the \(B_{m,}\) is even below zero at a certain distance, meaning the random tokens have larger attention scores than the similar tokens, which may be problematic for long context modeling.

### Base of RoPE Bounds the Context Length

To satisfy the Desiderata 2, we will get \(_{q,k^{*}}[q^{T}R_{m,}k^{*}]_{q,k}[q ^{T}R_{m,}k]\). According to Theorem 1, \(B_{m,}\) needs to be larger than zero. Given the \(\) in RoPE, the context length \(L_{}\) that can be truly obtained satisfies:

\[L_{}=\{L|B_{m,} 0, m[0,1,...,L]\} \]

In other word, if we follow the setting that \(_{i}=base^{-2i/d}\), in order to get the expected context length \(L\), there is a lower bound of the base value \(base_{L}\):

\[base_{L}=\{base|B_{m,} 0, m[0,1,...,L]\} \]

In summary, the RoPE's base determines the upper bound of context length the model can truly obtain. Although there exists the absolute lower bound, Eq. 9 and Eq. 11 are hard to get the closed-form solution since \(B_{m,}\) is a summation of many cosine functions. Therefore, in this paper, we get the numerical solution. Table 1 shows this lower bound for context length ranging from 1,000 to one million. In Figure 1, we plot the context length and corresponding lower bound, we can observe that as the context length increases, the required base also increases.

_Note: this boundary is not very strict because the stacking of layers in LLMs allows the model to extract information beyond the single layers' range, which may increase the context length in Eq. 10 and decrease the base in Eq. 11._ Notwithstanding, in Section 5 we find that the derived bound approximates the real context length in practice.

**Long-term decay from different perspectives.** The long-term decay in section 4.1 and section 4.2 are from different perspectives. The former refers to the long-term decay of the attention score as the relative distance increases. This ensures that current tokens tend to pay more attention to the tokens closer to them. The latter indicates that with the introduction of the rotation matrix in attention, the ability to discriminate the relevant tokens from irrelevant tokens decreases as the relative distance increases. Therefore, a large \(B_{m,}\), corresponding to a large base value, is important to keep the model's discrimination ability in long context modeling.

## 5 Experiment

In this section, we conduct thorough experiments. The empirical result can be summarized in Table 2, the details are in the following sections.

### Experiments Setup

For fine-tuning, we utilized Llama2-7B  and Baichuan2-7B , both of which are popular open-source models employing RoPE with a base of \(1e4\). We utilized a fixed learning rate of 2e-5 and a

   Questions & Answers \\  Q: Does RoPE’s base bounds the context & Yes. When the base is small, it is difficult to get extrapolation \\ length during the fine-tuning stage? & for specific context length. \\   & Yes. Our proposed lower bound for RoPE’s base also applies to pre-training. If we train a model from scratch with a small base but the context length is large (larger than the bounded length), the resulting model has very limited context length capabilities, meaning some of the context in pre-training is wasted. \\   & 
 The model will get the superficial long context capability. \\ The model can keep perplexity low, but can’t retrieve useful information from long context. \\  \\   

Table 2: In Section 5, we aim to answer the following questions.

   Context Len. & 1k & 2k & 4k & 8k & 16k & 32k & 64k & 128k & 256k & 512k & 1M \\  Lower Bound & 4.3e3 & 1.6e4 & 2.7e4 & 8.4e4 & 3.1e5 & 6.4e5 & 2.1e6 & 7.8e6 & 3.6e7 & 6.4e7 & 5.1e8 \\   

Table 1: Context length and its corresponding lower bound of RoPE’s base.

global batch size of 128 and fine-tuning for 1000 steps. For pre-training, we trained a Llama-like 2B model from scratch for a total of 1 trillion tokens. We set the learning rate to 1e-4 and adopted a cosine decay schedule, with models trained on a total of 1T tokens. The dataset we used is a subset of RedPajama . More details of the experimental setup are provided in Appendix B.

Our evaluation focused on two aspects: (1) **Perplexity**: we use PG19 dataset  which are often used in long context evaluation; (2) **Retrieval**: in addition to perplexity, we also adopt retrieval since it represents the real long-context understanding ability of LLMs. We choose a) Long-eval benchmark from  and b) Needle in a haystack (NIH) . The Long-eval benchmark generates numerous random similar sentences and asks the model to answer questions based on a specific sentence within the context, while the NIH requires the model to retrieve information from various positions in the long context.

### Base of RoPE bounds context length in fine-tuning stages

According to Eq. 11, there is a lower bound of RoPE's base determined by expected context length. We fine-tune Llama2-7b-Base on 32k context with varying bases. As depicted in Figure 6, although the difference in perplexity between different bases is negligible, the accuracy of Long-eval varies significantly. In Figure 5(b), the dotted line denotes the lower bound derived from Eq. 11, below which the Long-eval accuracy declines significantly. Additional results are provided in Appendix C. Notably, this empirically observed lower bound closely aligns with our theoretical derivation. On the other hand, we can see that \(base=2e5\) achieves the best perplexity, but the accuracy of Long-eval is very low, which indicates the limitations of perplexity in evaluating long context capabilities. We also provide the more comprehensive RULER benchmark results in Appendix G.

### The Base of RoPE bounds context length in pre-training stages

According to **Theorem 1** and **Eq. 11**, these constraints could also apply to the pre-training stage. To validate this, we trained a 2B model from scratch with RoPE base=100. The results, depicted in the first row of Figure 7, indicate that even though the model was trained with a context length of 4,096 tokens, it was capable of retrieving information from only the most recent approximately 500 tokens. This demonstrates that the base parameter bounds the context length during the pre-training stage as well. We define the context length from which the model can effectively retrieve information as the effective context length.

According to our theory, the effective context length can be extended as the RoPE's base increases. To validate this, we further fine-tune this 2B model on 32k context length, with RoPE's base set to 1e4, as shown in the second row of Figure 7. While the effective context length increased, it remains significantly below 32k since the effective context length bounded by base=1e4 is much smaller

Figure 6: Fine-tuning Llama2-7B-Base on 32k context length with varying RoPE’s base. Although the perplexity remains low with varying bases, the Long-eval accuracy reveals a discernible bound for the base value, below which the Long-eval accuracy declines significantly. The dotted line denotes the lower bound derived from Eq. 11 and code is provided in Appendix E

than 32k. Further, when we increase the base to 1e6 and fine-tune the base 2B model on 32K (the third row in Figure 7), the model could obtain a larger context length than base=1e4, which is in accordance with our theory.

To further remove the influence of model size, we also fine-tuned a larger 7B model on a 32k context length with a RoPE base set to 1e4 and observed an effective context length nearly identical to that of the 2B model with the same RoPE base (see Appendix D). This is empirical proof that the effective context length is determined by RoPE's base.

### Interpretation for the superficial long context capability for small base

Based on our theory and empirical observations, it is easy to explain what happens in Figure 3.

**Better Extrapolation (Perplexity)?** Due to the small base, \(B_{m,}\) can be smaller than zero as \(m\) increases, which is shown in Figure 5. The model can't attend more to similar tokens than random tokens with a large relative distance, so the model tends to focus more on nearby tokens, this will lead to a smaller empirical receptive field, even smaller than the training length. In this case, the model has a strong ability to maintain perplexity stability .

**Worse Ability (Long-eval and NIH)!** According to our previous analysis, RoPE's base bounds the context length, and the context length bounded by 500 is much lower than that bound by 10,000. Therefore, when the base is set to 500, the effective context length drops sharply, even after training on 32k context length.

### OOD theory is insufficient to reveal long context capability

Section 3 mentions that methods based on the OOD theory of rotation angles may not fully reflect the long context capability. In this section, we conduct further experiments to substantiate and explain this observation. We present two methods to extend the context length of Llama2 from 4k to 32k. Both of them are devoid of OOD angles. These methods are delineated mathematically as follows:

* Method 1: \(_{i}=(5e6)^{-2i/d}\),

Figure 7: The first row: the results of a 2B model training from scratch with base=1e2. The second row: The results of fine-tuning the 2B model with base=1e4. The third row: The results of fine-tuning the 2B model with base=1e6.

* Method 2: \(_{i}=(1e4)^{-2i/128}/8,&i 44\\ (1e4*8^{128/88})^{-2i/128},&i<44.\)

We can see from Table 3 that these two methods exhibit significantly different long context capabilities. Under the perspective of OOD rotation angle, both methods avoid OOD rotation angle, suggesting effective extrapolation. However, despite being trained on a context length of 32k, "method 2" struggles in completing the retrieval task at a context length of 32k. This phenomenon is beyond the scope which the OOD theory can explain.

Under our perspective, "method 2" is severely violating \(B_{m,} 0\) when \(m[15k,30k]\), thereby impeding its ability to achieve long-context discrimination. We speculate that the model may achieve better extrapolation in the fine-tuning stage if the base is sufficiently large to surpass a lower bound and avoid OOD of rotation angles.

## 6 Related Work

Position embedding.Since its introduction, Transformer  has achieved remarkable results in the field of natural language processing. To make full use of the order of sequence, researchers have introduced position embedding. The earliest position embedding was based on sinusoidal functions  for absolute positions, learnable absolute position embedding  and many variants [29; 30] were proposed. Nevertheless, absolute position embedding has difficulties in extending directly to texts longer than the training length. Subsequently, researchers proposed relative position embedding methods [31; 32]. With the development of large language models, rotary position embedding and its variants [6; 23] has become widely used, such as Llama2 , Baichuan2 , Mistral-7B-. A recent study reveals that no position embedding is also potential .

Long context learning.Implementing models with longer or even infinitely long contexts has always been an important goal in the field of natural language processing. Due to the squared complexity of the transformer model over time, a significant portion of the work focuses on improving the model structure [35; 35; 36; 37]. However, most of the work is still based on the transformer architecture. The other part of the work is aimed at reducing the computational complexity of attention itself, such as sparse attention  and group query attention . In addition, there are also some optimizations in engineering efficiency, such as flash attention  and ring attention . In the model inference stage, to save time and space, there are also some methods for accelerating long context, such as KV cache compression , etc. And the position embedding is important in extrapolation. In the process of fine-tuning, methods such as PI , NTK, and YARN  are used to change the original position embedding information. FoT  assigns the position information of the tokens outside the local context as the first token in the local context.

## 7 Limitation

In this work, we investigate the relationship between the base of RoPE and context length. Although we have derived that there exists a lower bound for the base of RoPE determined by context length, the existence of the upper bound for RoPE's base remains an open question that warrants further exploration. In addition, because of the lack of effective benchmarks for assessing long-context capabilities, the scope of long-context capabilities discussed in this paper may be limited.

    &  &  &  0\)} \\  & & 15k & 30k & 15k & 30k \\  Method 1 & ✖ & 0.33 & 0.27 & 0 & 0 \\ Method 2 & ✖ & 0.40 & 0.00 & 97 & 2554 \\   

Table 3: The comparison of “Method 1” and “Method 2”. These methods are designed carefully. They both are no OOD, but they are very different under our theory.

Conclusion

Our work presents a comprehensive study on the role of RoPE in LLMs for effectively modeling long context. Our main contribution lies in uncovering a novel property of RoPE through theoretical analysis, demonstrating that as the relative distance between tokens increases, the model's ability to attend more to similar tokens decreases. According to our theory, we derive a lower bound for RoPE's base in accommodating to expected context lengths. Our experimental results validate that the base of RoPE bounds context length for not only fine-tuning but also the pre-training stage. Our theory offers a new perspective on understanding the functionality of RoPE in long-context modeling. By shedding light on the relationship between context length and position embedding, we hope our work could provide insights for enhancing the long context capability of LLMs.