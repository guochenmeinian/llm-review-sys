# Identifying General Mechanism Shifts in Linear

Causal Representations

 Tianyu Chen\({}^{*}\) Kevin Bello\({}^{}\) Francesco Locatello\({}^{}\) Bryon Aragam\({}^{}\) Pradeep Ravikumar\({}^{}\)

\({}^{*}\)Department of Statistics and Data Sciences, University of Texas at Austin

\({}^{}\)Booth School of Business, University of Chicago

\({}^{}\)Machine Learning Department, Carnegie Mellon University

\({}^{}\) Institute of Science and Technology Austria

###### Abstract

We consider the linear causal representation learning setting where we observe a linear mixing of \(d\) unknown latent factors, which follow a linear structural causal model. Recent work has shown that it is possible to recover the latent factors as well as the underlying structural causal model over them, up to permutation and scaling, provided that we have at least \(d\) environments, each of which corresponds to perfect interventions on a single latent node (factor). After this powerful result, a key open problem faced by the community has been to relax these conditions: allow for coarser than perfect single-node interventions, and allow for fewer than \(d\) of them, since the number of latent factors \(d\) could be very large. In this work, we consider precisely such a setting, where we allow a smaller than \(d\) number of environments, and also allow for very coarse interventions that can very coarsely _change the entire causal graph over the latent factors_. On the flip side, we relax what we wish to extract to simply the _list of nodes that have shifted between one or more environments_. We provide a surprising identifiability result that it is indeed possible, under some very mild standard assumptions, to identify the set of shifted nodes. Our identifiability proof moreover is a constructive one: we explicitly provide necessary and sufficient conditions for a node to be a shifted node, and show that we can check these conditions given observed data. Our algorithm lends itself very naturally to the sample setting where instead of just interventional distributions, we are provided datasets of samples from each of these distributions. We corroborate our results on both synthetic experiments as well as an interesting psychometric dataset. The code can be found at [https://github.com/TianyuCodings/iLCS](https://github.com/TianyuCodings/iLCS).

## 1 Introduction

The objective of learning disentangled representations is to separate the different factors that contribute to the variation in the observed data, resulting in a representation that is easier to understand and manipulate . Traditional methods for disentanglement [e.g., 19, 20, 7, 9, 26] aim to make the latent variables independent of each other.

Consider the setting of linear independent component analysis (ICA) , that is, the observed variables \(X^{p}\) are generated through the process \(X=GZ\), where \(Z^{d}\) are _latent_ factors, and \(G^{p d}\) is an _unknown_ "mixing" matrix. Under the key assumption that \(Z\) has statistically independent components, and under some additional mild assumptions, landmark results in linear ICA show that it is possible to recover the latent variables \(Z\) up to permutation and scaling .

However, what if instead of independent sources \(Z\) we have a _structural causal model_ (SCM, [37; 38]) over them? For instance, if the latent factors correspond to biomarkers in a biology context, or root causes in a root cause analysis context, then we expect there to be rich associations between them. Indeed, this question is central in the burgeoning field of causal representation learning (CRL) [39; 51], where we are interested in extracting the latent factors and causal associations between them given raw data.

Let us look at the simplest CRL setting where the latent variables \(Z\) follow a _linear_ SCM, that is, \(Z=AZ+^{}{{2}}}\), where \(A^{d d}\) encodes a directed acyclic graph (DAG), \(\) is a diagonal matrix that controls the scale of noise variances, and \(\) is some noise vector with zero-mean and unit-variance independent components. In such a case, \(Z\) is a linear mixing of independent components \(\), that is, \(Z=B^{-1}\), where \(B=^{-}{{2}}}(I_{d}-A)\) succinctly encodes the SCM and \(I_{d}\) is the identity matrix of dimension \(^{d d}\). We then have \(X=GB^{-1}\) so that ICA can only recover \(BG^{}\) up to permutation and scaling, which does not suffice to recover the SCM \(B\) since the mixing function \(G\) is unknown.

Recently, Seigal et al.  showed that given the interventional distributions arising from _perfect interventions_ on _each_ latent variable in \(Z\), we can recover the SCM over \(Z\) up to permutation. But there are two caveats to this: (a) it is difficult to obtain perfect single-node interventions that only intervene on a single factor in \(Z\); and (b) it is difficult to obtain \(d\) number of such perfect interventional distributions or environments.

We are interested in the setting where we do not have perfect interventions: we allow for far more general interventions that can quite coarsely change the SCM, namely, _soft_ and _hard_ interventions, interventions targeting _single_ or _multiple_ nodes, as well as interventions capable of _adding_ or _removing_ parent nodes and _reversing_ edges. Moreover, we do not need as many as \(d\) of these.

Our goal, however, is not to recover the entire SCM over \(Z\) but simply to recover those nodes \(Z\) that have incurred shifts or changes between the different interventional distributions. This is closely related to root cause analysis [5; 6; 21; 33], which aims to identify the origins of the observed changes in a joint distribution. In addition, understanding the sources of distribution shifts--that is, localizing invariant/shifted conditional distributions--can benefit downstream tasks such as domain adaptation , and domain generalization [36; 55].

Contributions.Our work sits at the intersection of linear CRL [40; 23] and _direct estimation_ of causal mechanism shifts [52; 14]. The key contribution of this work is to show that it is possible to identify the _latent_ sources of distribution shifts in multiple datasets while _bypassing_ the estimation of the mixing function \(G\) and the SCM \(B\) over the latent variables, under very general types of interventions. More concretely, we make the following set of contributions:

1. **Identifiability:** We show that we can identify the shifted latent factors even under more general types of interventions. (Section 4.1).
2. **Algorithm:** We also provide an scalable algorithm that implements our identifiability result to infer such shifted latent factors even in the practical scenarios where we are not given the entire coarse interventional distributions but merely finite samples from each (Section 4.2).
3. **Experiments:** We corroborate our results on both synthetic experiments (Section 5.1) as well as an interesting psychometric dataset (Section 5.2).

## 2 Related Work

Causal representation learning.In contrast to our setting, which focuses on identifying shifted nodes in the latent representation, existing methods in CRL aim to recover _both_ the latent causal graph and the mixing function. Previous works have studied identifiability in various settings, such as latent linear SEMs with linear mixing , and with nonlinear mixing ; latent nonlinear SEMs with finite degree polynomial mixing , and with linear mixing ; and nonlinear SEMs with nonlinear mixing [50; 49; 23; 22]. Although these studies ensure the identifiability of causal graphs (up to permutation and scaling ambiguities), they generally rely on the assumption that _each latent variable_ is intervened upon in at least one environment, necessitating access to at least \(d\) interventional distributions. Moreover, the aforementioned works assume specific types of interventions, such as hard/soft interventions and single-node interventions, and restrict changes in interventional distributions, disallowing edge reversals or the addition of new edges. The most recent work  enables causal representation learning under general interventions in latent linear SEMs with linear mixing. However, this approach still requires the assumption that the number of environments \(K\) is at least equal to the number of latent nodes \(d\) and that there are at least \((d^{2})\) interventions. If the objective is to detect variables with general mechanism changes across multiple environments--environments that may lack a consistent topological order and sufficient interventions or environments--using existing CRL methods to recover each latent graph becomes overly restrictive or even infeasible. In contrast, we present a more flexible approach, enabling the identification of shifted variables without assuming restrictive interventions per environment or a consistent topological order of the latent graphs.

Direct estimation of mechanism shifts.The problem of directly estimating causal mechanism changes _without_ estimating the causal graphs has also been explored in various settings in the regime in which the causal variables are observable. Wang et al.  and Ghoshal et al.  have focused on identifying structural differences, assuming linear SEMs as environments, and proposing methods that take advantage of variations in the precision matrices. More recently, Chen et al.  studied this problem for nonlinear additive noise models, assuming that the environments originate from soft/hard interventions and leverage recent work in causal discovery via score matching. Finally, the concept of detecting/localizing feature shifts between two distributions has also been discussed in , although from a non-causal perspective. To our knowledge, there is a gap in the literature regarding the study of these objectives when considering latent causal variables. We address this gap by proposing a novel approach for directly detecting mechanism shifts within the latent SCMs.

Independent component analysis.The application of independent component analysis (ICA)  in the realm of causal discovery has seen significant developments. Linear ICA  and its nonlinear counterpart  have been instrumental in causal discovery  and more recently in causal latent discovery . Beyond these established applications, our work uncovers a novel use of ICA, namely, identifying shifted nodes within the latent linear SEMs.

Given the relevance of ICA for our approach, we briefly recap it next. ICA considers the following setting: \(X=W\) where \(X^{p}\), \(^{d}\), \(p d\). A key assumption in ICA is that each component of \(\) is independent. Given only observations of \(X\), the goal of ICA is to estimate both \(W\) and \(\). The objective function typically aims to maximize negentropy or non-Gaussianity, with further details given in . The identifiability results of ICA can be summarized as follows.

**Theorem 1** (Theorems 3,4 in ).: _If every component of \(\) is independent and at most one component is Gaussian distributed, with \(W\) being full column rank, then ICA can estimate \(W\) up to a permutation and scaling of each column, and \(\) can be recovered for some permutation up to scaling for each component. Furthermore, as noted in , if \([_{i}^{2}]=1, i[d]\), the estimated \(W\) and \(\) will have ambiguities only in permutation and sign. Formally, this means_

\[X=W=(WP^{T}D)(DP),\]

_where \(P\) is a permutation matrix and \(D\) is a diagonal matrix with diagonal entries \( 1\). Then, the best estimate given by ICA is \(WP^{T}D\) and \(DP\)._

## 3 Problem Setting

Consider a random vector \(X\) in \(^{p}\) that is a linear mixing of \(d\) latent variables \(Z=(Z_{1},,Z_{d})\):

\[X=GZ.\]

Here the latent variables in \(Z\) follows a linear SCM , that is,

\[Z=AZ+^{}{{2}}}\]

where \(A^{d d}\) corresponds to a DAG \(\) such that \(A_{jk} 0\) iff there exists an edge \(j k\) in the DAG \(\); \(^{d d}\) is a diagonal matrix with positive entries, and \(^{d}\) is a random vector with independent components with mean zero and variance one, i.e., that \(()=I_{d}\). Denoting \(B=^{-}{{2}}}(I_{d}-A)\), we have that:

\[Z=B^{-1}.\]We assume that we observe \(K 2\) generalized interventional distributions that keep the mixing map \(G\) fixed but allow for generalized interventions to \(Z\). That is, for environment \(k[K]\) we have,

\[X^{(k)}=GZ^{(k)},\]

where \(Z^{(k)}=A^{(k)}Z^{(k)}+(^{(k)})^{}{{2}}}^{(k)}\). Similarly, we have \(Z^{(k)}=(B^{(k)})^{-1}^{(k)}\), where \(B^{(k)}=(^{(k)})^{-}{{2}}}(I_{d}-A^{(k)})\).

Notably, we allow generalized interventions that allow for \(A^{(k)}\) to be arbitrary, which includes _soft_ and _hard_ interventions, interventions targeting _single_ or _multiple_ nodes, as well as interventions capable of _adding_ or _removing_ parent nodes and _reversing_ edges. This contrasts with the existing literature on CRL, where single-node soft/hard interventions are the standard assumption . See Figure 1, for a toy example of what we aim to estimate.

**Remark 1**.: _Since we allow for general types of interventions, we can take any of the given environments as the canonical "observational" distribution with respect to which we observe interventions, or simply that we observe \(k\) interventions of an unknown observational distribution. This is a clear distinction from the standard setting in CRL  which requires to know which environment is a suitable observational distribution._

To develop our identifiability result and algorithm, we will make additional assumptions on the noise distributions of the linear SEMs.

**Assumption A** (Noise Assumptions).: _For any environment \(k[K]\), let \(^{(k)}=(^{(k)}_{1},,^{(k)}_{d})\) be the vector of \(d\) independent noises with \((^{(k)})=I_{d}\). We have:_

1. _Identically distributed across environments:_ \((^{(k)})=(^{(k^{})})\)_, for all_ \(k^{} k\)_._
2. _Non-Gaussianity: At most one noise component_ \(^{(k)}_{i}\) _is Gaussian distributed._
3. _Pairwise differences: For any_ \(i j\)_, we have_ \((^{(k)}_{i})(^{(k)}_{j})\) _and_ \((^{(k)}_{i})(-^{(k)}_{j})\)_._

Assumption A.1 is usually assumed for learning causal models from multiple environments . Assumption A.2 is typically made in causal discovery methods, as detailed in seminal works such as  and is considered a more realistic assumption . Assumption A.3 is generally satisfied in a generic sense; that is, when probability distributions on the real line are randomly selected, they are pairwise different with probability one. This assumption is also adopted in .

**Assumption B** (Test Function).: _We assume access to a test function \(\) that maps each noise r.v. to \(\) s.t. \((^{(k)}_{i})=(-^{(k)}_{i})\), and \((^{(k)}_{i})(^{(k)}_{j})\) if \(^{(k)}_{i}\) and \(^{(k)}_{j}\) are not identically distributed._

Figure 1: We have 5 _latent_ variables \(Z\) which in this case relate to personality concepts, and the observations \(X\) represent the scores of 50 questions from a psychometric personality test. The latent variables \(Z\) follow a linear SCM, while the _unknown_ shared linear mixing is a full-rank matrix \(G^{50 5}\). Then, for environment \(k=\{,,\}\), the observables are generated through \(X^{(k)}=GZ^{(k)}\). Here, \(^{()}\) is taken as the “observational” (reference) distribution, and the distribution shifts in \(^{()}\) and \(^{()}\) are due to changes in the causal mechanisms of \(\{Z_{1}\}\) and \(\{Z_{2},Z_{3},Z_{5}\}\), respectively. Finally, the types of interventions are general; for UK, the edge \(Z_{4} Z_{1}\) is removed and the dashed red lines indicate changes in the edge weights to \(Z_{1}\); for AU, \(Z_{2}\) was intervened by removing \(Z_{5} Z_{2}\) and _adding_\(Z_{3} Z_{2}\), while the edge \(Z_{5} Z_{3}\) was _reversed_, thus changing the mechanisms of \(Z_{3}\) and \(Z_{5}\). Thus, we aim to identify \(\{Z_{1}\}\) and \(\{Z_{2},Z_{3},Z_{5}\}\).

This assumption states that we can access a test function that can help differentiate the noise components. One coarse example is \((y)=(|y| 1)\). This assumption is introduced to better understand our method workflow in Section 4, but it is not completely necessary. We discuss how to relax this assumption in Appendix C. Next, we formally define a mechanism shift.

**Definition 1** (Latent Mechanism Shifts).: _Let \((Z_{i}^{(k)})\) denote the set of parents of \(Z_{i}^{(k)}\). A latent variable \(Z_{i}\) is called a latent shifted node within environments \(k\) and \(k^{}\), if and only if:_

\[(Z_{i}^{(k)}(Z_{i}^{(k)}))(Z_{i}^ {(k^{})}(Z_{i}^{(k^{})})).\]

**Remark 2**.: _Following Definition 1, \(Z_{i}\) is a latent shifted node between environments \(k\) and \(k^{}\) if: (1) The \(i\)-th rows of \(A^{(k)}\) and \(A^{(k^{})}\) are different; (2) \(_{ii}^{(k)}_{ii}^{(k^{})}\); or (3) both._

Definition 1 aligns with those previously discussed in [52; 14; 10], with the key difference that we consider changes in the causal mechanisms of the latent causal variables. However, note that our results also contribute to the setting in which causal variables are observable considering that the mixing function is the identity matrix, that is, \(G=I_{d}\).

## 4 Identifying Shifts in Latent Causal Mechanisms

Following the setup outlined in the previous section, our focus now turns to developing an algorithm to identify latent shifted nodes, given data from multiple environments. First, note that we can write the overall model as a linear ICA problem, where, for any environment \(k\), the observation \(X^{(k)}\) is a linear combination of independent components \(^{(k)}\). Specifically, we have

\[X^{(k)}=GZ^{(k)}=G(B^{(k)})^{-1}^{(k)}\]

Under the mild conditions given in Assumption A, from classical ICA identifiability results stated in Theorem 1, we can identify \(G(B^{(k)})^{-1}\) up to permutation and sign flip. Let \(M^{(k)}=B^{(k)}H\) where \(H=G^{}\). Then, we can only identify \(M^{(k)}\) up to permutation and sign flip, which does not suffice to identify the latent SCM encoded in \(B^{(k)}\). In sum, what we can only obtain from ICA is

\[^{(k)}=P^{(k)}D^{(k)}B^{(k)}H\]

where \(P^{(k)}\) is a permutation matrix, and \(D^{(k)}\) is a diagonal matrix with \(-1\) or \(+1\) on its diagonal. As Seigal et al.  points out, it is not possible to identify \(B^{(k)}\) further given _generalized interventions_. Our first result is that our present mild assumptions suffice to infer shifted nodes.

**Theorem 2** (Identifiability).: _Given access to \(K 2\) environments, assume that A and B hold for all environments. Then, all latent shifted nodes are identifiable._

An interesting facet of our identifiability result is that it is _constructive_. In the next subsection we will provide an explicit algorithm to infer the shifted nodes and prove the main theorem above.

### Constructive identifiability

Consider \(^{(k)}=B^{(k)}HX^{(k)}\) and \(^{(k)}=^{(k)}X^{(k)}=P^{(k)}D^{(k)}B^{(k)}HX^{ (k)}=P^{(k)}D^{(k)}^{(k)}\), where \(^{(k)}\) and \(^{(k)}\) are the output of ICA, which contain the permutation and sign flip ambiguities given by \(P^{(k)}D^{(k)}\).

Obtaining a consistent ordering of the noise components across all environments is equivalent to finding \(P^{(k)}\). Under Assumption B, and without loss of generality, we consider that \((_{1}^{(k)},,_{d}^{(k)})\) are in increasing order with respect to their \(\) values. Since \(\) is invariant to sign flip, we can calculate \((_{i}^{(k)})\) for all \(i[d]\) and sort the calculated \(\) values in increasing order. Let \(^{(k)}\) denote the sorting permutation with respect to \(\), so that post-sorting, we get \(^{(k)}^{(k)}\).

**Remark 3**.: _In Appendix C, we discuss how to relax the assumption on the test function \(\)._

**Proposition 1**.: \(^{(k)}=(P^{(k)})^{T}\)_, i.e., \(^{(k)}\) is the inverse permutation of the ICA scrambling._

From Proposition 1, we thus find that we can unscramble the permutation \(P^{(k)}\) by sorting with respect to \(\). We get \(^{(k)}^{(k)}=^{k}P^{(k)}D^{(k)} ^{(k)}=D^{(k)}^{(k)}\) from the above proposition. In other words, we can extract \(^{(k)}=D^{(k)}^{(k)}\) via \(^{(k)}=^{(k)}^{(k)}=D^{(k)}B^{(k)}H=D^{(k )}M^{(k)}\) after ICA and sorting by \(\).

**Proposition 2**.: _Given access to \(K 2\) environments, assume that A holds. Then, \(Z_{i}\) is identified as a latent non-shifted node between environments \(k\) and \(k^{}\) if and only if \(M_{i}^{(k)}=M_{i}^{(k^{})}\), where \(M_{i}^{(k)}\) represents the \(i\)-th row of \(M^{(k)}\), and \(M^{(k)}=B^{(k)}H\)._

All formal proofs are given in Appendix E. Our next result shows the identifiability of shifted nodes in the unscrambled matrix \(^{(k)}\).

**Theorem 3**.: \(Z_{i}\) _is identified as a non-shifted node if and only if \(_{i}^{(k)}=_{i}^{(k^{})}\) or \(_{i}^{(k)}=-_{i}^{(k^{})}\)._

We can summarize this in the following algorithm, which proves Theorem 2:

* Perform ICA to obtain \(^{(k)}\) and \(^{(k)}\) with input \(X^{(k)}\).
* Sort by \(\) to get the permutation \(^{(k)}\) and compute \(^{(k)}=^{(k)}^{(k)}\) and \(^{(k)}=^{(k)}^{(k)}\).
* Check the condition on \(\{_{i}^{(k)}:k[K]\}\) to detect if \(Z_{i}\) is a shifted node, as prescribed by Theorem 3.

### Finite-sample algorithm

Thus far, we have considered the population setting where we are given the entire interventional distributions. In practice, we are given samples from each of these interventional distributions, so that we have \(K\) datasets, one for each of the interventional distributions. The overall algorithm is given next in Alg. 1 (see illustration in Appendix B) with detailed explanations following the algorithm.

```
0: Datasets \(\{^{(k)}\}_{k=1}^{K}\) and threshold \(\) (e.g., 0.5)  Calculate covariance matrix \(^{(k)}\) from \(^{(k)}\) for all k \(d=_{k=1,,K}(^{(k)})\) for\(k=1,,K\)do //Step 1: \(^{(k)}\) is samples from \(^{(k)}\) \(}^{(k)},^{(k)}( ^{(k)},d)\)  Calculate \((}^{(k)})=[(}_{1}^{(k)}),(}_{2}^{(k)}), ,(}_{d}^{(k)})]\) //Step 2  sorted_idx \(\) argsort(\((}^{(k)})\)) \(^{(k)}^{(k)}[,:]\)  Initialize \(S^{(k,k^{})}=\), for all \(k k^{}\) for\(i=1,,d\)do for\(k k^{}\)do  Calculate \(L_{i}^{k,k^{}}\) //Step 3 if\(L_{i}^{k,k^{}}>\)then \(S^{(k,k^{})} S^{(k,k^{})}\{i\}\)
0: All latent shifted nodes \(S=(S^{(k,k^{})})_{k,k^{}}\)
```

**Algorithm 1**iLCS: Identifying Latent Causal Mechanisms **S**hifts

Step 1:We perform ICA with samples from \(X^{(k)}\) to extract \(^{(k)}\) and samples from \(^{(k)}\).

**Remark 4** (Estimation of \(d\).).: _One missing component in using ICA in practice is that, along with samples from \(X^{(k)}\), we need to input the number of latent nodes \(d\), which need to be estimated from samples. Define \(^{(k)}=[X^{(k)}X^{(k)}T]=G(B^{(k)})^{-1}(B^{(k)})^{-T}G^{T}\). Since all matrices are full rank, it follows that \(d=(^{(k)})\), where \(^{(k)}\) can be estimated by the sample covariance matrix. Thus, \(d\) can also be estimated by the rank of the sample covariance matrix._

Step 2:We compute the empirical expectation of \(\) on samples from \(^{(k)}\), which by law of large number arguments, converges to its population expectation, which is \((^{(k)})\). We use the sorted order of the empirical expectations to sort the noise components, unscrambling the noise components as earlier, to get \(^{(k)}\) and samples from \(^{(k)}\).

Step 3:Here, we explicitly construct a test statistic to check the condition on \(\{_{i}^{(k)}:k[K]\}\) to detect if \(Z_{i}\) is a shifted node. Note that from our Theorem 3, there is a non-shift node \(Z_{i}\) between environments \(k\) and \(k^{}\) if and only if \(_{i}^{(k)}=_{i}^{(k^{})}\). Accordingly, we define a test statistic:

\[L_{i}^{k,k^{}}=_{i}^{(k)}_{i}^ {(k^{})}\|_{1}}}{\|_{i}^{(k)}\|_{1}+\|_{i}^{( k^{})}\|_{1}}\]

It can be seen that \(L_{i}^{k,k^{}}=0\) if and only if \(_{i}^{(k)}=_{i}^{(k^{})}\), which implies node \(Z_{i}\) is not shifted between environments \(k\) and \(k^{}\). Thus, in step three of the algorithm above, for each coordinate \(i[d]\), we check if there exists \(k k^{}\) such that \(L_{i}^{k,k^{}}>\) for a given threshold \(\). If such a \(k k^{}\) exists, we include \(i\) in the list of shifted nodes.

Algorithm 1 is consistent with the ground truth set of shifted nodes as \(n\) approaches infinity. Empirical evidence supporting this claim is presented in Figure 2, which shows that with a sufficiently large sample size, all shifted nodes are correctly identified, and the F1 score reaches 1. Further theoretical discussion on the sample complexity of our method can be found in Appendix D.

## 5 Experiments

In this section, we investigate the performance of our method in synthetic and real-world data.

### Synthetic Data

In our setup, each noise component \(_{i}\) is sampled from a generalized normal distribution with the probability density function given by \(p(_{i})\{-|_{i}|^{i}\}\), where \(i=1,2,,d\). In this noise generation process, the noise vector \(\) adheres to the condition \((_{i})<(_{j})\) for all \(i<j\) if we choose \((y)=(|y| 1)\). Following the methodology similar to that in , we start by sampling either an Erdos-Renyi (ER) or Scale-Free (SF) graph with \(d\) nodes and an expected edge count of \(md\), where \(m\{2,4,6\}\), denoted as \(ERm\) or \(SFm\). The observed space dimension \(p\) is set to \(2d\). For each graph, the weights are independently sampled from Unif \(\)\([0.25,1]\) and the diagonal entries of \(\) from Unif\(\). In each environment \(k\), \(15\%\) of the nodes are randomly selected for shifting. The new weights \(A_{i}^{(k)}\) for the shifted node \(i\), and the new entries of \(^{(k)}\), specifically \(_{ii}^{(k)}\), are independently sampled from Unif\(\). The mixing function \(G\) is independently generated from Unif\([-0.25,0.25]\).

Empirically, we have observed that the following formulation of \(L_{i}^{k,k^{}}\) leads to improved results:

\[L_{i}^{k,k^{}}=_{i}^{(k)}|-|_{i}^{(k^ {})}|\|_{1}}{\|_{i}^{(k)}\|_{1}+\|_{i}^{(k^{ })}\|_{1}},\]

Figure 2: Illustration of the efficacy of our method in accurately identifying latent shifted nodes as the sample size increases, for ER2 graphs. In the first subplot, for a latent graph with \(d=5\) nodes, we examine scenarios with observed dimensions \(p=10,20,40\) and plot their corresponding F1 scores against the number of samples \(n\). It is observed that the F1 score approaches 1 with a sufficiently large sample size. Detailed experimental procedures and results are discussed in Section 5.

[MISSING_PAGE_FAIL:8]

normalization of data to fit within the \(\) range, achieved by adjusting according to the maximum and minimum values observed. The research question we have formalized in this study is not derived from any data competition. It aligns with interests explored in existing psychological literature , yet our investigation is distinguished by a unique analytical framework.

Labeling latent nodes.Prior to detecting shifted nodes, it is essential to assign semantics to each node. This process involves conducting interventions on each component of the noise vector to aid in labeling the latent nodes. Given that the noise components are distinct for each latent node, labeling the noise effectively equates to labeling the latent nodes.

Initially, we apply ICA to the data for males, followed by getting post-sorting \(^{male}\) and \(}^{male}\) as outlined in our methodology. Subsequently, we perform interventions on each noise component, setting each to \(0\) sequentially, and then re-mixing the intervened noise vector using \((^{male})^{}\). By examining the impact of these interventions on the observation space -- specifically, identifying which question scores undergo significant changes -- we can assign appropriate semantic labels to each latent node index. For instance, nullifying the first column of \(}\) and remixing the intervened noise with \((^{male})^{}\) alters the score distribution in a manner that reveals the semantic domain affected by the first noise component. An example of assigning the label _Agreeableness_ to a latent node is depicted in Figure 3. By applying the same process to all noise components, we are able to assign semantic labels _Openness, Conscientiousness, Extraversion_, and _Neuroticism_ to the remaining latent nodes. More detailed experiment results are shown in Section G.

Shifted nodes detection.To identify shifted personality dimensions across gender, we computed \(L_{i}^{male,female}\) for each latent node, obtaining values of \(\{0.074,0.0497,0.078,0.638,0.633\}\). Setting a tolerance threshold \(=0.5\) to accommodate real data estimation variances, we observed that the last two nodes exhibit significantly higher \(L_{i}^{male,female}\) scores, surpassing \(\), and thus are considered shifted. These nodes correspond to the labels _Neuroticism_ and _Extraversion_. Consistent with existing psychological literature, women have been found to score higher in _Neuroticism_ than men , while men scored higher in the Activity subcomponent of _Extraversion_. This discovery aligns with the findings in psychology literature. To further validate our method's effectiveness, a similar analysis was conducted across countries, comparing the UK and the US, which have the most observations in our dataset. The computed \(L_{i}^{US,UK}\) for each latent node was \(\{0.302,0.258,0.109,0.189,0.088\}\). All values fell below \(\), indicating no latent node shifts between

Figure 3: We apply an intervention to the first column of \(\) and then use \((^{male})^{}\) for remixing. The first row of the resulting histograms represents scores for 5 out of the 10 questions related to the Extraversion personality dimension. Subsequent rows display histograms for 5 questions from each of the other four personality dimensions, as indicated at the right end of each row. The red distribution represents the scores before the intervention on the noise, while the blue distribution corresponds to scores after the intervention. Overlapping areas are shown in purple. Notably, the intervention on the first column of \(\) alters the distribution in the observed space, specifically affecting the scores for questions related to the _Agreeableness_ personality dimension, whereas distributions for other dimensions remain unchanged. Consequently, we can label the first noise component as corresponding to _Agreeableness_.

these two countries. This finding is also in agreement with existing studies that personality exhibits stability across countries and cultures .

## 6 Concluding Remarks

In this study, we demonstrated that latent mechanism shifts are identifiable, up to a permutation, within the framework of linear latent causal structures and linear mixing functions. Furthermore, we introduced an algorithm, grounded in ICA, designed to detect these shifts. Our method offers a broader applicability to various types of interventions compared to CRL framework. Unlike shift detection methods where node variables are directly observable, our approach extends to scenarios where latent variables remain unobserved. A promising future direction consists of adapting our methodology to nonlinear transformations, which could address more complex, practical challenges, such as identifying latent mechanism shifts in real-world image data.