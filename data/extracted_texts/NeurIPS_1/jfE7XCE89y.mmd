# FuseMoE: Mixture-of-Experts Transformers for

Fleximodal Fusion

 Xing Han

Department of Computer Science

Johns Hopkins University

xhan56@jhu.edu

&Huy Nguyen

Department of Statistics and Data Sciences

The University of Texas at Austin

huynm@utexas.edu

&Carl Harris

Department of Biomedical Engineering

Johns Hopkins University

charr165@jhu.edu

&Nhat Ho

Department of Statistics and Data Sciences

The University of Texas at Austin

minhnhat@utexas.edu

&Suchi Saria

Department of Computer Science

Johns Hopkins University

ssaria@cs.jhu.edu

Equal Contribution, \({}^{+}\)Equal Advising.

###### Abstract

As machine learning models in critical fields increasingly grapple with multimodal data, they face the dual challenges of handling a wide array of modalities, often incomplete due to missing elements, and the temporal irregularity and sparsity of collected samples. Successfully leveraging this complex data, while overcoming the scarcity of high-quality training samples, is key to improving these models' predictive performance. We introduce "FuseMoE", a mixture-of-experts framework incorporated with an innovative gating function. Designed to integrate a diverse number of modalities, FuseMoE is effective in managing scenarios with missing modalities and irregularly sampled data trajectories. Theoretically, our unique gating function contributes to enhanced convergence rates, leading to better performance in multiple downstream tasks. The practical utility of FuseMoE in the real world is validated by a diverse set of challenging prediction tasks.

## 1 Introduction

Multimodal fusion is a critical and extensively studied problem in many significant domains , such as sentiment analysis , image and video captioning , and medical prediction . Previous research has shown that embracing multimodality can improve predictive performance by capturing complementary information across modalities, outperforming single-modality approaches in similar tasks . However, an ongoing challenge lies in the creation of scalable frameworks for fusing multimodal data under a variety of conditions, and in creating reliable models that consistently surpass their single-modal counterparts.

Handling a variable number of input modalities remains an open challenge in multimodal fusion, due to challenges with scalability and lack of unified approaches for addressing missing modalities. Many existing multimodal fusion methods are designed for only two modalities , rely on costly pairwise comparisons between modalities , or employ simple concatenation approaches , rendering them unable to scale to settings with a large number of input modalities or adequately capture inter-modal interactions. Similarly, existing works are either unable to handle missing modalities entirely  or use imputation approaches  of varying sophistication. The former methods restrict usage to cases where all modalities are completely observed, significantly diminishing their utility in settings where this is often not the case (such as in clinical applications); the latter can lead to suboptimal performance due to the inherent limitations of imputed data. In addition, the complex and irregular temporal dynamics present in multimodal data have often been overlooked , with existing methods often ignoring irregularity entirely  or relying on positional embedding schemes  that may not be appropriate when modalities display a varying degree of temporal irregularity. Consequently, there is a pressing need for more advanced and scalable multimodal fusion techniques that can efficiently handle a broader set of modalities, effectively manage missing and irregular data, and capture the nuanced inter-modal relationships necessary for robust and accurate prediction. We use the term **FlexiModal Data** to capture several of these key aspects, which haven't been well-addressed by prior works:

_"Flexi" suggests flexibility, indicating the possibility of having any combination of modalities, even with arbitrary missingness or irregularity._

FlexiModal data is most evident in clinical scenarios, where extensive monitoring results in the accumulation of comprehensive electronic health records (EHRs) for each patient. A typical EHR encompasses diverse data types, including tabular (e.g., age, demographics, gender), images (X-rays, magnetic resonance imaging, and photographs), clinical notes, physiological time series (ECG and EEG), and vital signs (blood chemistry, heart rate). In this setting, we observe a variety of modalities, sampled with varying irregularity and a high degree of missingness and sparsity.

ContributionsIn this paper, we introduce a novel mixture-of-experts (MoE) framework, which we call FuseMoE, specifically designed to enhance the multimodal fusion of FlexiModal data. FuseMoE incorporates sparsely gated MoE layers in its fusion component, which are adept at managing distinct tasks and learning optimal modality partitioning. In addition, FuseMoE surpasses previous cross-attention-based methods in scalability, accommodating an unlimited array of input modalities. Furthermore, FuseMoE routes each modality to designated experts that specialize in those specific data types. This allows FuseMoE to effectively handle scenarios with missing modalities by dynamically adjusting the influence of experts primarily responsible for the absent data, while still utilizing the available modalities. Lastly, FuseMoE integrates a novel Laplace gating function, which is theoretically proven to ensure better convergence rates compared to traditional Softmax functions, thereby enhancing predictive performance. We have conducted comprehensive empirical evaluations of FuseMoE across a range of application scenarios to validate its effectiveness.

   Method & Type & Irregularity & Missingness & Num of & Theory & FlexiModal Adaptive? \\  & & & & Mods & & \\  Soenksen et al.  & Data Pipeline & ✗ & ✗ & \(\)4 & ✗ & ✓ \\ Zhang et al.  & Modality Fusion & ✓ & ✗ & 2 & ✗ & ✗ \\ Zadeh et al.  & Modality Fusion & ✗ & ✗ & 3 & ✗ & ✗ \\ Mustafa et al.  & Multimodal MoE & ✗ & ✗ & 2 & ✗ & ✗ \\ FuseMoE & This Paper & ✓ & ✓ & \(\)4 & ✓ & Adapted \\   

Table 1: We evaluated the characteristics of FuseMoE against various benchmarks. The pipeline approach  relies on a simple feature extraction scheme for each modality, followed by concatenation and classification. It doesn’t incorporate irregularities or missingness in its process, but its use of concatenation and zero-imputation for missing modalities allows it to be adapted to FlexiModal settings. Both  and  tackle multi-modality fusion, but as modalities increase, their method demands exponentially more cross-modal computations and significant model architecture modifications. Finally,  presents MoE for language-image alignment, yet it also requires substantial adjustments for the more intricate and universal FlexiModal context we explore.

## 2 FuseMoE: Enhance Predictive Performance for FlexiModal Data

In this section, we delve into the fundamental components of FuseMoE, illustrated in Figure 1. We focus on two critical elements: the modality and irregularity encoder, and the MoE fusion layer.

### Sparse MoE Backbone

The main components of a sparse MoE layer are a network \(G\) as a sparse gate and an expert network \(E\).  proposed a Top-\(K\) gating function that takes as an input a token representation \(x^{D}\) and then routes it to the Top-\(K\) experts out of the set \(\{E_{i}\}_{i=1}^{S}\). The gating network parameter \(W^{D N}\) produces logits \(h_{s}(x)=(x W)\), which are normalized via Softmax:

\[G(x)_{i}=(x)_{i})}{_{j}^{K}(h_{s}(x)_{j})}. \]

Each expert network (\(E_{i}:^{D}^{D}\)) contains a feed-forward layer (FFN) and its parameters are independent of other models. The final output of the expert network \(y\) is the linearly weighted combination of each expert's output on the token by the gate's output: \(y=_{i=1}^{S}G(x)_{i}E_{i}(x)\).

**Gating Network Design** The gating network's advantage lies in its capacity to be concurrently trained with FFNs, facilitating the learning of an optimal sparse combination of experts. Essentially, by evaluating the similarity between the input token and the experts, the gating network/router optimally matches the input partition with the most suitable experts. In many cases, variations in the routing mechanism can greatly influence performance across diverse applications . The Softmax gating is the most widely adopted across domains [77; 79]. We introduce a novel Laplace gating function that offers enhanced convergence guarantees and delivers superior predictive performance, particularly in FlexiModal applications. The function is formulated as follows:

\[h_{l}(x)=(-\|W-x\|_{2}). \]

The Laplace gating function, characterized by its Euclidean term \((-\|W-x\|_{2})\), is less prone to converge towards extreme weight distributions due to the bounded nature of this term. In subsequent sections, we will illustrate how this gating function facilitates faster parameter estimation rates compared to Softmax gating. Moreover, our empirical findings indicate that the Laplace gating exhibits enhanced performance in managing FlexiModal data.

### Modality and Irregularity Encoder

To encode the irregularity of sampling in each modality, we utilize a discretized multi-time attention (mTAND) module , which leverages a time attention mechanism [43; 92] to discretize irregularly sampled observations into discrete intervals. Specifically, given a set of \(l_{k}\) continuous time points,

Figure 1: An example of addressing the challenge of FlexiModal Data: patients in ICUs often have extensive and irregular health status measurements over time; patients with milder conditions only require monitoring across fewer categories. FuseMoE is adept at handling inputs featuring any combination of modalities, including those with missing elements. It starts by encoding inputs using modality-specific feature extractors, followed by employing a multi-time attention mechanism  to address temporal irregularities. The core of FuseMoE lies the MoE Fusion Layer, where a routing mechanism is trained to categorize multimodal inputs and direct them to the appropriate combinations of MLPs. The outputs from these MLPs are weighted through a gating function, resulting in fused embeddings, which are subsequently utilized for further processing.

\(t^{l_{k}}\), corresponding to the \(k^{}\) dimensionality of a given modality, we employ \(H\) embedding functions \(_{h}()\) to embed each \(_{k} t_{k}\) in a \(d_{h}\) dimensional vector space (detailed definition and examples can be found in Appendix B and E). The \(i^{}\) dimension of the \(h^{}\) embedding is defined as

\[_{h}()[i]=w_{i}_{k},&i=1\\ (w_{i}_{k}+_{i}),&1<i d_{h},\]

where \(\{w_{i},_{i}\}_{i=1}^{d_{h}}\) are learnable parameters. By performing this for each continuous time point in \(t_{k}\), we create a \(d_{h}\) dimensional representation of each time point in \(H\) different embedding spaces. We then leverage these embeddings to discretize the irregularly sampled observations into discretized bins. Specifically, we seek to discretize \(x_{k}\) (with \(l_{k}\) corresponding observation times \(t_{k}\)) into \(\) regularly sampled intervals \(\). We do this via an attention mechanism, which, for each embedding function \(_{h}()\), takes \(\) as queries, \(t_{k}\) as keys, and \(x_{k}\) as values and produces \(_{k,h}^{}\) embeddings for each sequence. Formally,

\[_{k,h}=(()_{h} _{h}^{}_{h}(t_{k})^{}}{}})x_{k},\]

where \(_{h}\) and \(_{h}\) are learnable parameters. This formulation allows us to discretize univariate observations \(x_{k}\) into \(\) regularly-sampled bins. To model irregularity across a multivariate set of observations for a given modality with \(d_{m}\) dimensions, we repeat this process for each dimension of the input. This allows us to obtain an interpolation matrix \(_{h}=[_{1,h},_{2,h},...,_{d_{m},h}]^{  d_{m}}\) for each of the \(h\) embedding functions. We then concatenate the interpolation matrices across all \(H\) embedding functions (i.e., \(I=[_{1},_{2},...,_{h}]^{(H.d_{m})}\)) and employ a linear projection to achieve a final, discretized embedding for each modality, \(Z^{ d_{e}}\), where \(d_{e}\) denotes the desired dimensionality of each modality's representation. The discretization procedure offers a standardized approach to managing irregularly sampled time series across various input types; however, it can inevitably result in information loss. On the other hand, relying solely on the mTAND module may yield suboptimal performance due to the potentially varying sampling rates of different variables , especially in scenarios where the sample sizes are small. To mitigate this, we combine discretized outputs with continuous representations learned through the mTAND module.

**Encoding Multiple Modalities** The process described above allows us to discretize an arbitrarily long irregular, multivariate sequence into a regularly sampled, discretized embedding with length \(\) and dimensionality \(d_{e}\). We repeat this for each of the \(M\) modalities, to create \(M\) embeddings, \(\{Z_{j}\}_{j=1}^{M}\), which are then combined to generate predictions.

### MoE Fusion Layer

Router Design StudyUpon obtaining embeddings from each of the \(j\) modalities, we propose multiple complementary approaches for processing multimodal inputs. Figure 2 illustrates a range of router design options. The most straightforward strategy involves employing a common router that handles the concatenated embeddings of all \(j\) modalities, without imposing any gating constraints.

Figure 2: We present three exemplary designs of the Top-\(K\) router for effective multimodal fusion, considering an input scenario with three modalities: Time-Series (TS), Text (TXT), and images (IMG). (a) The joint router design utilizes a concatenated embedding of all modalities, directing this combined input to selected experts. (b) In the modality-specific router design, each modality’s embedding is independently assigned to a shared pool of experts. (c) The third design variant also uses modality-specific routers but assigns each modality’s embedding to separate pools of experts, each pool uniquely tailored to process a specific modality type.

As the complexity increases with additional modalities, we consider more sophisticated alternatives: deploying separate routers for each modality's embedding and assigning these embeddings to a shared pool of experts. This allows for distinct processing while maintaining a unified expert framework. Additionally, we further segregate these common expert pools, allowing each router to direct its respective embedding to dedicated experts skilled in handling such specific inputs. These varied router design choices offer users enhanced flexibility, enabling more fine-grained control of both inter-modal and intra-modal relationships. Details of the respective advantages and challenges of these router design mechanisms can be found in Appendix C.

We implement an entropy regularization loss to ensure balanced and stable expert utilization, a concept supported by various previous studies [58; 57; 22]. It maximizes the mutual information between modalities and experts and serves as an auxiliary loss function in addition to task-specific loss. Given a total of \(M\) modalities, and denoting \(\) as the entropy, we define the loss function \(\) as

\[(x)=_{j=1}^{M}(_{m_{j}}(E))- (_{j=1}^{M}_{m_{j}}(E)), \]

where \(_{m_{j}}(E)\) is the distribution over the experts \(\{E_{i}\}_{i=1}^{S}\) for the \(j^{}\) modality. This distribution can be approximated by \(_{m_{j}}(E)=_{i=1}^{l^{j}}p_{m_{j}}(E x_{i}^{m_{j}})\), where \(l^{j}\) is the number of observations of the \(j^{}\) modality. Intuitively, we actively encourage the input embeddings to diminish the uncertainty in selecting experts. By incorporating the loss \(\), we aim to stabilize the experts' preferences within each modality, while promoting a diverse range of expert selections across different modalities.

Missing ModalitiesIn scenarios where certain modalities are missing throughout the data trajectories, we substitute the original embedding \(Z_{}\) with a learnable embedding \(\), acting as a generic "missing indicator". This strategy is facilitated by employing per-modality routers, which, in conjunction with entropy regularization, guide \(\) predominantly toward a specific group of less-utilized experts. The new embeddings \(\) are dynamically adjusted throughout the model training process to minimize the task-specific loss and the entropy regularization loss. As a result, the router will assign lower weights to the experts responsible for processing these embeddings.

## 3 Theoretical Contribution

In this section, we provide a theoretical guarantee of the benefits of the Laplace gating over the standard Softmax gating in MoE. In particular, we conduct a convergence analysis for maximum likelihood estimation (MLE) under the Lapace gating Gaussian MoE, and demonstrate that the MLE under this model has better convergence behaviors than that under the softmax gating Gaussian MoE.

**Problem Setup.** Since the convergence analysis of MLE under the Top-K sparse gating MoE has been studied in , we will focus on examining the Laplace gating solely in the sequel. Assume that \((X_{1},Y_{1}),,(X_{n},Y_{n})^{d}\) are i.i.d. samples drawn from the Laplace gating Gaussian MoE of order \(k_{*}\) whose conditional density function \(p_{G_{*}}(Y|X)\) is

\[p_{G_{*}}(Y|X)=_{i=1}^{k_{*}}(-\|W_{i}^{*}-X\|+_{i}^{ *}) f(Y|(a_{i}^{*})^{}X+b_{i}^{*},_{i}^{*}), \]

where we define for any vectors \(v=(v_{i})_{i=1}^{k_{*}}\) that \((v_{i}):=)}{_{j=1}^{k_{*}}(v_{j})}\). Above, \(f(|,)\) denotes a univariate Gaussian density function with mean \(\) and variance \(\). For ease of the presentation, we denote \(G_{*}:=_{i=1}^{k_{*}}(_{i}^{*})_{(W_{*}^{*},a_{i}^{*},b_{i} ^{*},_{i}^{*})}\) as a true but unknown _mixing measure_ associated with unknown parameters \((_{i}^{*},W_{i}^{*},a_{i}^{*},b_{i}^{*},_{i}^{*})\) for \(i\{1,2,,k_{*}\}\). In the paper, we specifically consider two settings of the true number of experts \(k_{*}\): (i) _Exact-specified setting_: when \(k_{*}\) is known; (ii) _Over-specified setting_: when \(k_{*}\) is unknown, and we over-specify the model in equation 4 by a Laplace gating MoE model with \(k>k_{*}\) experts. However, due to the space limit, we present only the latter setting, and defer the former setting to Appendix J.

**Maximum Likelihood Estimation.** We use the maximum likelihood method to estimate the unknown mixing measure \(G_{*}\). In particular, the MLE is given by

\[_{n}*{arg\,max}_{G_{k}()} {n}_{i=1}^{n}(p_{G}(Y_{i}|X_{i})), \]

where \(_{k}():=\{G=_{i=1}^{k^{}}(_{i})_{(W_ {i},a_{i},b_{i},_{i})}:1 k^{} k,\;(W_{i},a_{i},b_{i},_{i}) \}\) denotes the set of all mixing measures with at most \(k\) components. Given the MLE defined in equation 5, we are ready to present the main results. Before that, let us introduce some necessary notations for our analysis.

**Notations.** We denote \([n]:=\{1,2,,n\}\) for any \(n\). For any vector \(v^{d}\), \(\|v\|\) stands for its \(2\)-norm value. Additionally, the notation \(|S|\) indicates the cardinality of a given set \(S\), while \(\) denotes the Dirac delta measure. Finally, for any two probability densities \(p,q\) dominated by the Lebesgue measure \(\), we denote \(V(p,q)=|p-q|d\) as their Total Variation distance.

Firstly, we demonstrate in Theorem 3.1 that the convergence rate of density estimation under the Laplace gating Gaussian MoE is parametric on the sample size \(n\).

**Theorem 3.1** (Density estimation).: _The density estimation \(p_{_{n}}(Y|X)\) converges to the true density \(p_{G_{*}}(Y|X)\) under the Total Variation distance at the following rate:_

\[_{X}[V(p_{_{n}}(|X),p_{G_{*}}(|X))]= ().\]

Proof of Theorem 3.1 is in Appendix K.2. The parametric rate \(()\) of the conditional density function \(p_{_{n}}\) indicates that if there exists a loss function among parameters \(\) such that \(_{X}[V(p_{_{n}}(|X),p_{G_{*}}(|X))] (_{n},G_{*})\), then we will achieve the parameter and expert estimation rates via the bound \((_{n},G_{*})=()\).

**Voronoi Loss.** Following the above implication, we now define a loss function among parameters based on a notion of Voronoi cells as in . Given some mixing measure \(G\), we distribute its components \(_{i}:=(W_{i},a_{i},b_{i},_{i})\) to the following Voronoi cells, which are generated by the components \(_{j}^{*}:=(W_{j}^{*},a_{j}^{*},b_{j}^{*},_{j}^{*})\) of the true mixing measure \(G_{*}\):

\[_{j}_{j}(G):=\{i[k]:\|_{i}-_{j}^{*} \|\|_{i}-_{}^{*}\|,\; j\}, \]

for any \(1 j k_{*}\). Note that, the cardinality of the Voronoi cell \(_{j}\) is exactly the number of fitted components approximating \(_{j}^{*}\). For ease of the presentation, let us denote \(_{ij}(_{1},_{2},_{3},_{4}):=\|W_{i}-W_{j}^{*}\|^{_{1}} +\|a_{i}-a_{j}^{*}\|^{_{2}}+|b_{i}-b_{j}^{*}|^{_{3}}+|_{i}-_{j}^ {*}|^{_{4}}\), for any \((_{1},_{2},_{3},_{4})^{4}\). Then, the Voronoi loss function \(_{2}G,G_{*}\) used for our analysis under the over-specified setting is given by:

\[_{2}(G,G_{*}):=_{j=1}^{k_{*}}_{i _{j}}(_{i})-(_{j}^{*})+_{j[k_{*}]:| _{j}|=1}_{i_{j}}(_{i})_{ij}(1,1,1,1)\] \[+_{j[k_{*}]:|_{j}|>1}_{i_{j }}(_{i})_{ij}2,2,(|_{j}|),( |_{j}|)}{2}. \]

The notation \((|_{j}|)\) stands for the minimum value of \(r\) such that the following system of polynomial equations does not have any non-trivial solutions for the unknown variables \(\{(q_{1i},q_{2i},q_{3i})\}_{i=1}^{|_{j}|}\):

\[_{i=1}^{|_{j}|}_{m_{1}+2m_{2}=s,\\ 1 m_{1}+m_{2} r}^{2}q_{1i}^{m_{1}}q_{2i}^{m _{2}}}{m_{1}!m_{2}!}=0,s=1,2,,r, \]

A solution to the above system is regarded as non-trivial if at least among variables \(q_{1i}\) is different from zero, whereas all the variables \(q_{3i}\) are non-zero. It is worth noting that the function \(()\) was previously studied in  to characterize the convergence behavior of parameter estimation under the location-scale Gaussian mixture models.  also gave some specific values of that function, namely \((2)=4\) and \((3)=6\). Meanwhile, they claimed that it was non-trivial to determine thevalue of \((m)\) when \(m 4\), and further techniques should be developed for that purpose. Since Gaussian MoE models are generalization of the Gaussian mixture models, we also involve the function \(()\) in our convergence analysis. Now, we provide in the following theorem the convergence rate of parameter estimation under the over-specified setting of the Laplace gating Gaussian MoE model (see also Figure 3 for the empirical convergence rates justifying the theoretical rates in Theorem 3.2).

**Theorem 3.2** (Parameter Estimation).: _When \(k>k_{*}\) becomes unknown, the following Total Variation bound holds true for any mixing measure \(G_{k}()\):_

\[_{X}[V(p_{G}(|X),p_{G_{*}}(|X))]_{2}(G, G_{*}).\]

_Consequently, we obtain that \(_{2}(_{n},G_{*})=()\)._

Proof of Theorem 3.2 is in Appendix K.3. The results of Theorem 3.2 together with the formulation of the loss function \(_{2}\) in equation 7 reveal that (see also Table 2):

**(i)** The parameters \(W_{i}^{*},a_{i}^{*},b_{i}^{*},_{i}^{*}\) which are fitted by exactly one component, i.e. \(|_{i}^{n}|:=|_{i}(_{n})|=1\), enjoy the same estimation rate of order \((n^{-1/2})\) (up to some logarithmic factor), which match those in .

**(ii)** The rates for estimating the parameters \(W_{i}^{*},a_{i}^{*},b_{i}^{*},_{i}^{*}\) which are fitted by more than one component, i.e. \(|_{i}^{n}|>1\), are no longer homogeneous. On the one hand, the estimation rates for the parameters \(b_{i}^{*}\) and \(_{i}^{*}\) are of orders \((n^{-1/2(|_{i}^{n}|)})\) and \((n^{-1/(|_{i}^{n}|)})\), respectively, both of which are determined by the function \(()\) and vary with the number of fitted components \(|_{i}^{n}|\). Those rates are comparable to their counterparts in . On the other hand, the estimation rates for the gating parameters \(W_{i}^{*}\) and the expert parameters \(a_{i}^{*}\) are all of order \((n^{-1/4})\), which remains constant with respect to the number of fitted components. Meanwhile, those rates in  depend on a different system of polynomial equations from that in equation 8, which are significantly slower.

**Advantage of Laplace Gating on FlexiModal Setting** In the standard Softmax gating , the similarity score is computed as the inner product of a token's hidden representation and an expert

 
**Gates** & \((_{j}^{*})\) & \(W_{j}^{*}\) & \(a_{j}^{*}\) & \(b_{j}^{*}\) & \(_{j}^{*}\) \\  Softmax  & \((n^{-1/2})\) & \((n^{-1/2(|_{j}^{n}|)})\) & \((n^{-1/(|_{j}^{n}|)})\) & \((n^{-1/2(|_{j}^{n}|)})\) & \((n^{-1/(|_{j}^{n}|)})\) \\  Laplace (Ours) & \((n^{-1/2})\) & \((n^{-1/4})\) & \((n^{-1/4})\) & \((n^{-1/2(|_{j}^{n}|)})\) & \((n^{-1/(|_{j}^{n}|)})\) \\  

Table 2: Parameter estimation rates under the Softmax and Laplace gating Gaussian MoE models. The function \(()\) represents the solvability of a system of polynomial equations considered in  while \(()()\) and \((2)=4\), \((3)=6\). Additionally, \(_{j}^{n}:=_{j}(_{n})\) denotes a Voronoi cell defined in equation 6.

Figure 3: Log-log scaled plots illustrating simulation results under the exact-specified (left) and the over-specified settings (right). The orange curves depict the mean discrepancy between the MLE \(_{n}\) and the true mixing measure \(G_{*}\), accompanied by error bars signifying two empirical standard deviations. Additionally, the gray dash-dotted line represents the least-squares fitted linear regression line for these data points. Finally, the loss functions \(_{1}\) and \(_{2}\) are defined in equations equation 9 and equation 7, respectively. See Appendix I for the experimental details.

embedding. However, this approach can lead to _representation collapse_[13; 69], where a subset of experts dominates the decision-making process, resulting in the redundancy of other experts. This issue likely contributes to the slow rates of estimating expert parameters \(a_{i}^{*}\) in this setting (see Table 2). By contrast, the Laplace gating function partially alleviates this problem by computing the similarity score as the \(L_{2}\)-distance between token representations and expert embeddings. This approach does not inherently favor any expert based on magnitude, unlike inner product which can be biased towards experts with larger norms. The Laplace gating ensures that all experts have a more balanced opportunity to be selected based on how close they are to the token representation. Therefore, Laplace gating is beneficial when dealing with heterogeneous inputs, such as multimodal data, where its feature distributions can be very different across modalities. This is because it can handle these differences without being overly sensitive to the scale and variance of the input features. In addition, it can gracefully degrade in the presence of missing data, rather than causing abrupt changes in gating probabilities that might occur with inner product-based measures. The improved estimation rates for expert parameters \(a_{i}^{*}\) under the Laplace gating Gaussian MoE, along with our empirical results on multiple large-scale datasets, substantiate these insights.

## 4 Experiments

OverviewWe demonstrate that FuseMoE can provide accurate and efficient predictions when applied to the FlexiModal setting. We tested FuseMoE on a diverse set of benchmarks, including MIMIC-III  and MIMIC-IV , CMU-MOSI and MOSEI , the Physical Activity Monitoring (PAM) dataset , and CIFAR-10 . Compared to CMU-MOSI and MOSEI, the MIMIC ecosystem exhibits irregular and missing modality patterns and includes distinct modalities unlike PAM and CIFAR-10. Evaluating FuseMoE across these diverse datasets provides various empirical insights into critical aspects of our model's performance. Comprehensive details on the datasets, metrics, parameters, and additional results are thoroughly presented in the Appendices.

### Main Results

CMU-MOSI and MOSEI DatasetsWe first apply our method to the CMU-MOSI and MOSEI datasets , which utilize visual, acoustic, and textual data for sentiment analysis and emotion recognition tasks. Our methodology employs pre-trained T5  for text encoding, librosa  for audio feature extraction, and EfficientNet  for video feature encoding. Table 3 details the per

Figure 4: (a) The Laplace gating mechanism enhances CIFAR-10 classification when integrated into VisionMoE . We employed Vision Transformer (ViT)  and ViT-small as the backbone models and selectively replaced their FFN layers with MoE layers; (b) FuseMoE improves prediction on PAM dataset over baseline time series models; (c) Per-modality routers and the entropy loss \(\) mitigate the impact of missing modalities.

    &  &  \\   & MAE\({}_{}\) & Acc-2\(\) & Corr\(\) & F\(\) & MAE\({}_{}\) & Acc-2\(\) & Corr\(\) & F\(\) \\   ITN & 0.90 \(\) 0.02 & 80.81 \(\) 0.34 & 0.70 \(\) 0.04 & 80.70 \(\) 0.18 & 0.59 \(\) 0.03 & 82.50 \(\) 0.58 & 0.68 \(\) 0.02 & 82.10 \(\) 0.41 \\ MuIT & 0.86 \(\) 0.01 & 84.10 \(\) 0.21 & 0.71 \(\) 0.02 & 83.90 \(\) 0.27 & 0.58 \(\) 0.02 & 82.51 \(\) 0.41 & 0.71 \(\) 0.04 & 82.31 \(\) 0.27 \\ MAG & 0.71 \(\) 0.04 & 86.10 \(\) 0.44 & 0.80 \(\) 0.03 & 86.00 \(\) 0.09 & 0.57 \(\) 0.07 & 85.56 \(\) 0.22 & 0.79 \(\) 0.02 & 84.50 \(\) 0.18 \\ Softmax-MoF & 0.69 \(\) 0.01 & 87.09 \(\) 0.18 & 0.82 \(\) 0.02 & 87.29 \(\) 0.22 & 0.55 \(\) 0.03 & 86.34 \(\) 0.23 & 0.76 \(\) 0.05 & 84.97 \(\) 0.32 \\  Joint experts-krouter & 0.67 \(\) 0.02 & 87.28 \(\) 0.35 & 0.82 \(\) 0.03 & 87.35 \(\) 0.24 & **0.54 \(\) 0.01** & **86.41 \(\) 0.36** & 0.81 \(\) 0.05 & **85.43 \(\) 0.25** \\ Per-mod router & **0.65 \(\) 0.04** & **88.23 \(\) 0.57** & **0.84 \(\) 0.01** & **87.39 \(\) 0.13** & 0.56 \(\) 0.02 & 86.12 \(\) 0.19 & 0.78 \(\) 0.02 & 85.07 \(\) 0.14 \\ Disjoint muner & 0.73 \(\) 0.02 & 86.37 \(\) 0.33 & 0.81 \(\) 0.04 & 86.89 \(\) 0.21 & 0.55 \(\) 0.02 & 85.67 \(\) 0.31 & **0.81 \(\) 0.01** & 85.21 \(\) 0.22 \\   

Table 3: MoE demonstrates improved performance averaged over 5 random experiments on the CMU-MOSI and MOSEI datasets; the best results are highlighted in **bold font** and the second best results are underlined.

formance of various router design mechanisms within our MoE architecture, utilizing the Laplace gating function, compared against representative baselines. The baselines include (1) the early fusion method, Tensor Fusion Network (TFN) ; (2) the Multimodal Transformer (MuIT), which fuses modalities by modeling their interactions ; (3) the Multimodal Adaptation Gate (MAG), which focuses on the consistency and differences across modalities ; and (4) multimodal fusion using standard MoE with the Softmax gating function. Results indicate that employing an MoE backbone--regardless of the gating function chosen or whether utilizing per-modality routers or a joint experts & router configuration--significantly enhances performance on the multimodal task. This improvement is attributed to the MoE's ability to effectively allocate specific components to handle distinct input modalities, thus better addressing both inter- and intra-modal relationships.

CIFAR-10 DatasetSubsequently, we evaluate our method using the Vision-MoE framework  on the CIFAR-10 classification task , with results illustrated in Figure 4(a). In this experiment, we selectively replace the FFN layers with an even number in the Vision Transformer (ViT) models with MoE layers. These results, along with Table 3 on the CMU-MOSI and MOSEI datasets comparing Softmax-gating MoE, indicate that the Laplace gating function surpasses the standard Softmax gating function in performance. This outcome is consistent with our theoretical claims.

MIMIC-IV and PAM DatasetsWe then conduct comprehensive evaluations of FuseMoE on MIMIC-IV , and the Physical Activity Monitoring (PAM) dataset . These datasets feature multiple input modalities, each _characterized by varying degrees of irregular sampling or significant levels of missingness_. Our tasks of interest for MIMIC datasets include the 48-hour in-hospital mortality prediction (48-IHM), 25-type phenotype classification (25-PHE), and length-of-stay (LOS) prediction. In addition to the previously mentioned baselines, we have incorporated the HAIM method , a data pipeline specifically designed for integrating multimodal data from the MIMIC-IV dataset. We also include the cross-attention combined with irregular sequences modeling approach (MISTS) . Table 4 shows the outcomes of combining irregular vital signs and clinical notes from the MIMIC-IV dataset. In addition to the commonly used Softmax gating function, we also evaluated the Gaussian gating function  as a comparative benchmark. The FuseMoE-based methods surpass baselines in most scenarios, often by a non-trivial margin. Furthermore, we observe that HAIM shows considerable efficacy in extracting features from time series, resulting in a strong performance in the 48-IHM and LOS tasks, which are heavily reliant on such data. However, its performance appears more moderate on the 25-PHE task. The PAM dataset captures daily living activities through 17 sensors, with data from each sensor treated as a separate modality. These modalities are individually processed through time-series and irregularity encoders before being integrated into the FuseMoE framework. Our baselines include the Transformer , GRU-D , SeFT , a mTAND-only configuration, and IP-Net . We use the Laplace gating and its joint experts & router structure in these experiments. The results in Figure 4(b) have again shown the efficacy of integrating the irregularity encoder with the MoE fusion layer.

### Ablation Studies

Scalability of FuseMoE with Increasing ModalitiesTable 5 presents the revised outcomes of the MIMIC-IV dataset after integrating CXR and ECG of corresponding patients, employing the per-modality router and the entropy loss \(\) within FuseMoE. This setup was chosen as it slightly outperformed the joint router with an increase in modalities. Relative to their two-modality versions, FuseMoE has effectively harnessed additional information (notably from CXR), resulting in

    &  &  &  &  &  &  &  &  \\   & AUROC & 75.06 ± 1.03 & 75.95 ± 0.84 & 75.82 ± 0.73 & 78.76 ± 0.79 & 79.65 ± 0.00 & 79.49 ± 0.83 & 80.76 ± 0.56 & **81.03 ± 0.25** \\  & F1 & 45.61 ± 0.34 & 38.81 ± 0.22 & 42.55 ± 0.82 & 40.61 ± 0.41 & 39.79 ± 0.00 & 42.86 ± 0.44 & **46.68 ± 0.24** & **46.53 ± 0.57** \\   & AUROC & 80.56 ± 0.33 & 81.36 ± 1.32 & 81.13 ± 0.66 & 80.71 ± 0.45 & 82.58 ± 0.00 & 82.11 ± 0.39 & 81.92 ± 0.73 & **82.91 ± 1.02** \\  & F1 & 73.01 ± 0.52 & 73.45 ± 0.59 & 72.51 ± 0.27 & 73.84 ± 0.61 & 73.18 ± 0.00 & 74.43 ± 0.88 & 74.46 ± 0.52 & **74.58 ± 0.63** \\   & AUROC & 69.45 ± 0.72 & 66.58 ± 0.41 & 69.55 ± 0.67 & 69.18 ± 0.32 & 63.39 ± 0.00 & 70.25 ± 0.47 & 70.42 ± 0.26 & **71.23 ± 0.53** \\  & F1 & 28.59 ± 0.46 & 28.55 ± 0.31 & 27.86 ± 0.29 & 28.52 ± 0.22 & **42.13 ± 0.00** & 31.25 ± 0.18 & 3

[MISSING_PAGE_FAIL:10]

phenotypes using noisy labeled training data. _Journal of the American Medical Informatics Association_, 23(6):1166-1173, 2016.
*  Emily Alsentzer, John R Murphy, Willie Boag, Wei-Hung Weng, Di Jin, Tristan Naumann, and Matthew McDermott. Publicly available clinical bert embeddings. _arXiv preprint arXiv:1904.03323_, 2019.
*  Aryan Arbabi, David R Adams, Sanja Fidler, Michael Brudno, et al. Identifying clinical terms in medical text using ontology-guided machine learning. _JMIR medical informatics_, 7(2):e12596, 2019.
*  Zachi I Attia, Suraj Kapa, Francisco Lopez-Jimenez, Paul M McKie, Dorothy J Ladewig, Gaurav Satam, Patricia A Pellikka, Maurice Enriquez-Sarano, Peter A Noseworthy, Thomas M Munger, et al. Screening for cardiac contractile dysfunction using an artificial intelligence-enabled electrocardiogram. _Nature medicine_, 25(1):70-74, 2019.
*  Aya Awad, Mohamed Bader-El-Den, James McNicholas, and Jim Briggs. Early hospital mortality prediction of intensive care unit patients using an ensemble learning approach. _International journal of medical informatics_, 108:185-195, 2017.
*  Dimitris Bertsimas, Jean Pauphilet, Jennifer Stevens, and Manu Tandon. Predicting inpatient flow at a major hospital using interpretable analytics. _Manufacturing & Service Operations Management_, 24(6):2809-2824, 2022.
*  Serhat S Bucak, Rong Jin, and Anil K Jain. Multiple kernel learning for visual object recognition: A review. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 36(7):1354-1369, 2013.
*  Rhonda R Butler. Icd-10 general equivalence mappings: Bridging the translation gap from icd-9. _Journal of AHIMA_, 78(9):84-86, 2007.
*  Bing Cao, Yiming Sun, Pengfei Zhu, and Qinghua Hu. Multi-modal gated mixture of local-to-global experts for dynamic image fusion. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 23555-23564, 2023.
*  Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan Liu. Recurrent neural networks for multivariate time series with missing values. _Scientific reports_, 8(1):6085, 2018.
*  JunKai Chen, Zenghai Chen, Zheru Chi, and Hong Fu. Emotion recognition in the wild with feature fusion and multiple kernel learning. In _Proceedings of the 16th International Conference on Multimodal Interaction_, pages 508-513, 2014.
*  Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In _Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining_, pages 785-794, 2016.
*  Zewen Chi, Li Dong, Shaohan Huang, Damai Dai, Shuming Ma, Barun Patra, Saksham Singhal, Payal Bajaj, Xia Song, Xian-Ling Mao, Heyan Huang, and Furu Wei. On the representation collapse of sparse mixture of experts. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
*  Joseph Paul Cohen, Joseph D Viviano, Paul Bertin, Paul Morrison, Parsa Torabian, Matteo Guarrera, Matthew P Lungren, Akshay Chaudhari, Rupert Brooks, Mohammad Hashir, et al. Torchxrayvision: A library of chest x-ray datasets and models. In _International Conference on Medical Imaging with Deep Learning_, pages 231-249. PMLR, 2022.
*  A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum Likelihood from Incomplete Data Via the EM Algorithm. _Journal of the Royal Statistical Society: Series B (Methodological)_, 39(1):1-22, September 1977.
*  Iman Deznabi, Mohit Iyyer, and Madalina Fiterau. Predicting in-hospital mortality by combining clinical notes with time-series data. In _Findings of the association for computational linguistics: ACL-IJCNLP 2021_, pages 4026-4031, 2021.

*  Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020.
*  A Elixhauser. Clinical classifications software (ccs) 2009. _http://www. hcug-us. ahrq. gov/toolssoft-ware/ccs. jsp_, 2009.
*  William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. _The Journal of Machine Learning Research_, 23(1):5232-5270, 2022.
*  Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, and Wei Xu. Are you talking to a machine? dataset and methods for multilingual image question. _Advances in neural information processing systems_, 28, 2015.
*  Ashutosh Garg, Vladimir Pavlovic, and James M Rehg. Boosted learning in dynamic bayesian networks for multimodal speaker detection. _Proceedings of the IEEE_, 91(9):1355-1369, 2003.
*  Aude Genevay. _Entropy-regularized optimal transport for machine learning_. PhD thesis, Paris Sciences et Lettres (ComUE), 2019.
*  Ary L Goldberger, Luis AN Amaral, Leon Glass, Jeffrey M Hausdorff, Plamen Ch Ivanov, Roger G Mark, Joseph E Mietus, George B Moody, Chung-Kang Peng, and H Eugene Stanley. Physiobank, physiotoolkit, and physionet: components of a new research resource for complex physiologic signals. _circulation_, 101(23):e215-e220, 2000.
*  Brian Gow, Tom Pollard, Larry A Nathanson, Alistair Johnson, Benjamin Moody, Chrystinne Fernandes, Nathaniel Greenbaum, Seth Berkowitz, Dana Moukheiber, Parastou Eslami, et al. Mimic-iv-ecg-diagnostic electrocardiogram matched subset. 2022.
*  Wei Han, Hui Chen, Alexander Gelbukh, Amir Zadeh, Louis-philippe Morency, and Soujanya Poria. Bi-bimodal modality fusion for correlation-controlled multimodal sentiment analysis. In _Proceedings of the 2021 International Conference on Multimodal Interaction_, pages 6-15, 2021.
*  Wei Han, Hui Chen, and Soujanya Poria. Improving multimodal fusion with hierarchical mutual information maximization for multimodal sentiment analysis. _arXiv preprint arXiv:2109.00412_, 2021.
*  Hrayr Harutyunyan, Hrant Khachatrian, David C Kale, Greg Ver Steeg, and Aram Galstyan. Multitask learning and benchmarking with clinical time series data. _Scientific data_, 6(1):96, 2019.
*  N. Ho and X. Nguyen. Convergence rates of parameter estimation for some weakly identifiable finite mixtures. _Annals of Statistics_, 44:2726-2755, 2016.
*  Nhat Ho, Chiao-Yu Yang, and Michael I. Jordan. Convergence rates for Gaussian mixtures of experts. _Journal of Machine Learning Research_, 23(323):1-81, 2022.
*  Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. _Neural computation_, 9(8):1735-1780, 1997.
*  Max Horn, Michael Moor, Christian Bock, Bastian Rieck, and Karsten Borgwardt. Set functions for time series. In _International Conference on Machine Learning_, pages 4353-4363. PMLR, 2020.
*  Shih-Cheng Huang, Anuj Pareek, Saeed Seyyedi, Imon Banerjee, and Matthew P Lungren. Fusion of medical imaging and electronic health records using deep learning: a systematic review and implementation guidelines. _NPJ digital medicine_, 3(1):136, 2020.
*  Shih-Cheng Huang, Anuj Pareek, Roham Zamanian, Imon Banerjee, and Matthew P Lungren. Multimodal fusion with deep neural networks for leveraging ct imaging and electronic health record: a case-study in pulmonary embolism detection. _Scientific reports_, 10(1):22147, 2020.

*  Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison. In _Proceedings of the AAAI conference on artificial intelligence_, volume 33, pages 590-597, 2019.
*  Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of local experts. _Neural computation_, 3(1):79-87, 1991.
*  Alistair Johnson, Lucas Bulgarelli, Tom Pollard, Steven Horng, Leo Anthony Celi, and Roger Mark. Mimic-iv. _PhysioNet. Available online at: https://physionet. org/content/mimiciv/1.0/(accessed August 23, 2021)_, 2020.
*  Alistair Johnson, Matt Lungren, Yifan Peng, Zhiyong Lu, Roger Mark, Seth Berkowitz, and Steven Horng. Mimic-cxr-jpg-chest radiographs with structured labels. _PhysioNet_, 2019.
*  Alistair Johnson, Tom Pollard, Steven Horng, Leo Anthony Celi, and Roger Mark. Mimic-iv-note: Deidentified free-text clinical notes, 2023.
*  Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz, Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng, Roger G Mark, and Steven Horng. Mimic-cxr, a de-identified publicly available database of chest radiographs with free-text reports. _Scientific data_, 6(1):317, 2019.
*  Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii, a freely accessible critical care database. _Scientific data_, 3(1):1-9, 2016.
*  Justin Johnson, Andrej Karpathy, and Li Fei-Fei. Densecap: Fully convolutional localization networks for dense captioning. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4565-4574, 2016.
*  Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 3128-3137, 2015.
*  Seyed Mehran Kazemi, Rishab Goel, Sepehr Eghbali, Janahan Ramanan, Jaspreet Sahota, Sanjay Thakur, Stella Wu, Cathal Smyth, Pascal Poupart, and Marcus Brubaker. Time2vec: Learning a vector representation of time. _arXiv preprint arXiv:1907.05321_, 2019.
*  Britt E Keuning, Thomas Kaufmann, Renske Wiersema, Anders Granholm, Ville Pettila, Morten Hylander Moller, Christian Fynbo Christiansen, Jose Castela Forte, Harold Snieder, Frederik Keus, et al. Mortality prediction models in the adult critically ill: A scoping review. _Acta Anaesthesiologica Scandinavica_, 64(4):424-442, 2020.
*  Swaraj Khadanga, Karan Aggarwal, Shafiq Joty, and Jaideep Srivastava. Using clinical notes with time series data for icu management. _arXiv preprint arXiv:1909.09702_, 2019.
*  Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
*  Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. _Proceedings of the IEEE_, 86(11):2278-2324, 1998.
*  Bo Li, Yifei Shen, Jingkang Yang, Yezhen Wang, Jiawei Ren, Tong Che, Jun Zhang, and Ziwei Liu. Sparse mixture-of-experts are domain generalizable learners. _arXiv preprint arXiv:2206.04046_, 2022.
*  Yikuan Li, Ramsey M Wehbe, Faraz S Ahmad, Hanyin Wang, and Yuan Luo. Clinical-longformer and clinical-bigbird: Transformers for long clinical sequences. _arXiv preprint arXiv:2201.11838_, 2022.
*  Ke Lin, Yonghua Hu, and Guilan Kong. Predicting in-hospital mortality of patients with acute kidney injury in the icu using random forest model. _International journal of medical informatics_, 125:55-61, 2019.

*  Jinghui Liu, Daniel Capurro, Anthony Nguyen, and Karin Verspoor. Attention-based multimodal fusion with contrast for robust clinical prediction in the face of missing modalities. _Journal of Biomedical Informatics_, 145:104466, 2023.
*  Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshminarasimhan, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Efficient low-rank multimodal fusion with modality-specific factors. _arXiv preprint arXiv:1806.00064_, 2018.
*  Karla R Lovaasen and Jennifer Schwerdtfeger. _ICD-9-CM Coding: Theory and Practice with ICD-10, 2013/2014 Edition-E-Book_. Elsevier Health Sciences, 2012.
*  Navonil Majumder, Devamanyu Hazarika, Alexander Gelbukh, Erik Cambria, and Soujanya Poria. Multimodal sentiment analysis using hierarchical fusion with context modeling. _Knowledge-based systems_, 161:124-133, 2018.
*  T. Manole and N. Ho. Refined convergence rates for maximum likelihood estimation under finite mixture models. In _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 14979-15006. PMLR, 17-23 Jul 2022.
*  Brian McFee, Colin Raffel, Dawen Liang, Daniel PW Ellis, Matt McVicar, Eric Battenberg, and Oriol Nieto. librosa: Audio and music signal analysis in python. In _SciPy_, pages 18-24, 2015.
*  Clara Meister, Elizabeth Salesky, and Ryan Cotterell. Generalized entropy regularization or: There's nothing special about label smoothing. _arXiv preprint arXiv:2005.00820_, 2020.
*  Basil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe Jenatton, and Neil Houlsby. Multimodal contrastive learning with limoe: the language-image mixture of experts. _Advances in Neural Information Processing Systems_, 35:9564-9576, 2022.
*  Ara V Nefian, Luhong Liang, Xiaobo Pi, Liu Xiaoxiang, Crusoe Mao, and Kevin Murphy. A coupled hmm for audio-visual speech recognition. In _2002 IEEE International Conference on Acoustics, Speech, and Signal Processing_, volume 2, pages II-2013. IEEE, 2002.
*  Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew Y Ng. Multimodal deep learning. In _Proceedings of the 28th international conference on machine learning (ICML-11)_, pages 689-696, 2011.
*  Huy Nguyen, Pedram Akbarian, and Nhat Ho. Is temperature sample efficient for softmax Gaussian mixture of experts? In _Proceedings of the ICML_, 2024.
*  Huy Nguyen, Pedram Akbarian, TrungTin Nguyen, and Nhat Ho. A general theory for softmax gating multinomial logistic mixture of experts. In _Proceedings of the ICML_, 2024.
*  Huy Nguyen, Pedram Akbarian, Fanqi Yan, and Nhat Ho. Statistical perspective of top-k sparse softmax gating mixture of experts. In _International Conference on Learning Representations_, 2024.
*  Huy Nguyen, TrungTin Nguyen, and Nhat Ho. Demystifying softmax gating function in Gaussian mixture of experts. In _Advances in Neural Information Processing Systems_, 2023.
*  Huy Nguyen, TrungTin Nguyen, Khai Nguyen, and Nhat Ho. Towards convergence rates for parameter estimation in Gaussian-gated mixture of experts. In _Proceedings of The 27th International Conference on Artificial Intelligence and Statistics_, 2024.
*  Xiaonan Nie, Xupeng Miao, Shijie Cao, Lingxiao Ma, Qibin Liu, Jilong Xue, Youshan Miao, Yi Liu, Zhi Yang, and Bin Cui. Evomoe: An evolutional mixture-of-experts training framework via dense-to-sparse gate. _arXiv preprint arXiv:2112.14397_, 2021.
*  Behnaz Nojavanasghari, Deepak Gopinath, Jayanth Koushik, Tadas Baltrusaitis, and Louis-Philippe Morency. Deep multimodal fusion for persuasiveness prediction. In _Proceedings of the 18th ACM International Conference on Multimodal Interaction_, pages 284-288, 2016.

*  Raghavendra Pappagari, Piotr Zelasko, Jesus Villalba, Yishay Carmiel, and Najim Dehak. Hierarchical transformers for long document classification. In _2019 IEEE automatic speech recognition and understanding workshop (ASRU)_, pages 838-844. IEEE, 2019.
* effective training of sparse mixture of experts via competition. _arXiv preprint arXiv:2402.02526_, 2024.
*  Soujanya Poria, Erik Cambria, and Alexander Gelbukh. Deep convolutional neural network textual features and multiple kernel learning for utterance-level multimodal sentiment analysis. In _Proceedings of the 2015 conference on empirical methods in natural language processing_, pages 2539-2544, 2015.
*  Gerasimos Potamianos, Chalapathy Neti, Guillaume Gravier, Ashutosh Garg, and Andrew W Senior. Recent advances in the automatic recognition of audiovisual speech. _Proceedings of the IEEE_, 91(9):1306-1326, 2003.
*  Joan Puigcerver, Carlos Riquelme, Basil Mustafa, and Neil Houlsby. From sparse to soft mixtures of experts. _arXiv preprint arXiv:2308.00951_, 2023.
*  Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of Machine Learning Research_, 21(140):1-67, 2020.
*  Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, Amir Zadeh, Chengfeng Mao, Louis-Philippe Morency, and Ehsan Hoque. Integrating multimodal information in large pretrained transformers. In _Proceedings of the conference. Association for Computational Linguistics. Meeting_, volume 2020, page 2359. NIH Public Access, 2020.
*  Attila Reiss and Didier Stricker. Introducing a new benchmarked dataset for activity monitoring. In _2012 16th international symposium on wearable computers_, pages 108-109. IEEE, 2012.
*  Stephan Reiter, Bjorn Schuller, and Gerhard Rigoll. Hidden conditional random fields for meeting segmentation. In _2007 IEEE International Conference on Multimedia and Expo_, pages 639-642. IEEE, 2007.
*  Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andre Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts. _Advances in Neural Information Processing Systems_, 34:8583-8595, 2021.
*  Thanveer Shaik, Xiaohui Tao, Lin Li, Haoran Xie, and Juan D Velasquez. A survey of multimodal information fusion for smart healthcare: Mapping the journey from data to wisdom. _Information Fusion_, page 102040, 2023.
*  Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. _arXiv preprint arXiv:1701.06538_, 2017.
*  Sheng Shen, Zhewei Yao, Chunyuan Li, Trevor Darrell, Kurt Keutzer, and Yuxiong He. Scaling vision-language models with sparse mixture of experts. _arXiv preprint arXiv:2303.07226_, 2023.
*  Satya Narayan Shukla and Benjamin M Marlin. Interpolation-prediction networks for irregularly sampled time series. _arXiv preprint arXiv:1909.07782_, 2019.
*  Satya Narayan Shukla and Benjamin M Marlin. Multi-time attention networks for irregularly sampled time series. _arXiv preprint arXiv:2101.10318_, 2021.
*  Luis R Soenksen, Yu Ma, Cynthia Zeng, Leonard Boussioux, Kimberly Villalobos Carballo, Liangyuan Na, Holly M Wiberg, Michael L Li, Ignacio Fuentes, and Dimitris Bertsimas. Integrated multimodal artificial intelligence framework for healthcare applications. _NPJ digital medicine_, 5(1):149, 2022.

*  Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In _International conference on machine learning_, pages 6105-6114. PMLR, 2019.
*  H. Teicher. On the mixture of distributions. _Annals of Statistics_, 31:55-73, 1960.
*  H. Teicher. Identifiability of mixtures. _Annals of Statistics_, 32:244-248, 1961.
*  H. Teicher. Identifiability of finite mixtures. _Ann. Math. Statist._, 32:1265-1269, 1963.
*  Sindhu Tipirneni and Chandan K Reddy. Self-supervised transformer for sparse and irregularly sampled multivariate clinical time-series. _ACM Transactions on Knowledge Discovery from Data (TKDD)_, 16(6):1-17, 2022.
*  Luan Tran, Xiaoming Liu, Jiayu Zhou, and Rong Jin. Missing modalities imputation via cascaded residual autoencoder. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1405-1414, 2017.
*  Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, J Zico Kolter, Louis-Philippe Morency, and Ruslan Salakhutdinov. Multimodal transformer for unaligned multimodal language sequences. In _Proceedings of the conference. Association for Computational Linguistics. Meeting_, volume 2019, page 6558. NIH Public Access, 2019.
*  S. van de Geer. _Empirical Processes in M-estimation_. Cambridge University Press, 2000.
*  Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
*  Lei Xu, Michael Jordan, and Geoffrey E Hinton. An alternative model for mixtures of experts. _Advances in neural information processing systems_, 7, 1994.
*  Bo Yang, Tao Mei, Xian-Sheng Hua, Linjun Yang, Shi-Qiang Yang, and Mingjing Li. Online video recommendation based on multimodal fusion and relevance feedback. In _Proceedings of the 6th ACM international conference on Image and video retrieval_, pages 73-80, 2007.
*  Bo Yang and Lijun Wu. How to leverage multimodal ehr data for better medical predictions? _arXiv preprint arXiv:2110.15763_, 2021.
*  Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. Tensor fusion network for multimodal sentiment analysis. _arXiv preprint arXiv:1707.07250_, 2017.
*  Amir Zadeh, Paul Pu Liang, Soujanya Poria, Prateek Vij, Erik Cambria, and Louis-Philippe Morency. Multi-attention recurrent network for human communication comprehension. In _Thirty-Second AAAI Conference on Artificial Intelligence_, 2018.
*  Xunlin Zhan, Yangxin Wu, Xiao Dong, Yunchao Wei, Minlong Lu, Yichi Zhang, Hang Xu, and Xiaodan Liang. Product1m: Towards weakly supervised instance-level product retrieval via cross-modal pretraining. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 11782-11791, 2021.
*  Dongyu Zhang, Jidapa Thadajarassiri, Cansu Sen, and Elke Rundensteiner. Time-aware transformer-based network for clinical notes series prediction. In _Machine learning for healthcare conference_, pages 566-588. PMLR, 2020.
*  Xiang Zhang, Marko Zeman, Theodoros Tsiligkaridis, and Marinka Zitnik. Graph-guided network for irregularly sampled multivariate time series. _arXiv preprint arXiv:2110.05357_, 2021.
*  Xinlu Zhang, Shiyang Li, Zhiyu Chen, Xifeng Yan, and Linda Ruth Petzold. Improving medical predictions by irregular multimodal electronic health records modeling. In _International Conference on Machine Learning_, pages 41300-41313. PMLR, 2023.
*  Tongxue Zhou, Su Ruan, and Stephane Canu. A review: Deep learning for medical image segmentation using multi-modality fusion. _Array_, 3:100004, 2019.

*  Yanqi Zhou, Nan Du, Yanping Huang, Daiyi Peng, Chang Lan, Da Huang, Siamak Shakeri, David So, Andrew M Dai, Yifeng Lu, et al. Brainformers: Trading simplicity for efficiency. In _International Conference on Machine Learning_, pages 42531-42542. PMLR, 2023.
*  Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai, Quoc V Le, James Laudon, et al. Mixture-of-experts with expert choice routing. _Advances in Neural Information Processing Systems_, 35:7103-7114, 2022.

**Appendix for**

**"FuseMoE: Mixture-of-Experts Transformers for**

**Fleximodal Fusion"**

###### Contents

* 1 Introduction
* 2 FuseMoE: Enhance Predictive Performance for FlexiModal Data
	* 2.1 Sparse MoE Backbone
	* 2.2 Modality and Irregularity Encoder
	* 2.3 MoE Fusion Layer
* 3 Theoretical Contribution
* 4 Experiments
	* 4.1 Main Results
	* 4.2 Ablation Studies
* 5 Discussions and Limitations
* A Related Works
* B FlexiModal Data and Tasks of Interest
* B.1 MIMIC-IV and MIMIC-III Datasets
* B.2 MOSI and MOSEI Datasets
* B.3 PAM Dataset
* B.4 CIFAR-10 Dataset
* C Mechanisms of Different Router Designs
* C.1 Joint Experts & Routers
* C.2 Modality-Specific Router
* C.3 Disjoint Experts & Routers
* D Data Preprocessing
* D.1 MIMIC-IV
* D.2 PAM Dataset
* E Modeling Irregularity
* E.1 Unified Temporal Discretization Embeddings
* E.2 Unifying imputation and mTAND embeddings
* F Baseline Comparison
* F.1 MISTS
*  MufT F.3 MAG F.4 TFN F.5 HAIM F.6 Implementation
*  Computational Resources and Hyper-Parameters G.1 Computational Resources G.2 Hyper-Parameters
*  Additional Results H.1 F1exiModal Experiments H.2 Ablation Study on FuseMoE Building Blocks H.3 Ablation Study on MoE Architecture
*  Details on Numerical Experiments I.1 Experimental Setup I.2 Exact-specified Setting I.3 Over-specified Setting
*  Exact-Specified Setting
*  Proof of Theoretical Results K.1 Proof of Theorem J.2 K.2 Proof of Theorem 3.1 K.2.1 Main Proof K.2.2 Proof of Lemma K.1 K.3 Proof of Theorem 3.2
*  Identifiability of the Laplace Gating Gaussian MoE
*  Broader ImpactRelated Works

Multimodal FusionInitial approaches to multimodal fusion incorporated techniques such as kernel-based methods , graphical models , and neural networks . With the diverse evolution of deep learning models, numerous advanced methods have now been employed in the fusion of multimodal data. In the realm of sentiment analysis,  employ a low-rank Tensor Fusion method that leverages both language and video content. Attention-gating mechanisms are used by  to generate displacement vectors through cross-modal self-attention, which are then added to the input vectors from the primary modality.  takes an alternative approach by integrating multiple layers of cross-modal attention blocks in a word-level vision/language/audio alignment task.

In the context of clinical prediction,  adopt a late fusion approach to combining vital sign and text data by concatenating embeddings from pre-trained feature extractors.  developed a generalizable data preprocessing and modeling pipeline for EHR encompassing four data modalities, albeit through a direct concatenation of existing feature embeddings for each modality followed by an XGBoost classifier . Recently,  expanded on the work of  by introducing a discretized multi-time attention (mTAND) module  to encode temporal irregularities in time series and text data. Their fusion approach involves layering sets of self- and cross-modal attention blocks. However, this approach is limited to just two modalities and is not easily extendable to include additional modal components or handle missing modalities. To the best of our knowledge, existing works are tailored to application-specific settings that necessitate the computation of pairwise cross-modal relationships, which are not scalable to more general settings with arbitrary modalities. Moreover, these studies typically do not account for scenarios where modalities are missing, or rely on imputation approaches based on observed data.

Mixture-of-ExpertsMoE  has gained significant popularity for managing complex tasks since its introduction three decades ago. Unlike traditional models that reuse the same parameters for all inputs, MoE selects distinct parameters for each specific input. This results in a sparsely activated layer, enabling a substantial scaling of model capacity without a corresponding increase in computational cost. Recent studies have demonstrated the effectiveness of integrating MoE with cutting-edge models across a diverse range of tasks . These works have also tackled key challenges such as accuracy and training instability . Given its ability to assign input partitions to specialized experts, MoE naturally lends itself to multimodal applications. This approach has been explored in fields such as vision-language modeling  and dynamic image fusion . However, the application of MoE in complex real-world settings, such as those involving FlexiModal Data, remains largely unexplored. This gap presents an opportunity to leverage MoE's potential in handling its intricate and multifaceted nature such as multimodal EHR, where reliable multimodal integration is crucial.

MoE TheoryWhile MoE has been widely employed to scale up large models, its theoretical foundations have remained nascent. Recently,  provided convergence rates for both density and parameter estimation of Softmax gating Gaussian MoE. They connected these rates to the solvability of systems of polynomial equations under Voronoi-based loss functions. Later,  extended these theories to the top-K sparse softmax gating Gaussian MoE. Their theories further characterize the effect of the sparsity of gating functions on the behaviors of parameter estimation and verify the benefits of using top-1 sparse softmax gating MoE in practice. Other theoretical results for MoE include estimation rates of parameters and experts for multinomial logistic MoE , for dense-to-sparse gating MoE , for Gaussian gating MoE , and for input-independent gating MoE .

## Appendix B FlexiModal Data and Tasks of Interest

Definition of FlexiModal DataWe provide a generic definition for the FlexiModal Data as we used throughout the paper. Let \(=\{(x_{i}^{m_{1}},t_{i}^{_{1}}),(x_{i}^{m_{2}},t_{i}^{ m_{2}}),,(x_{i}^{m_{j}},t_{i}^{m_{j}}),y_{i}\}_{i=1}^{N}\) to be the FlexiModal dataset with \(N\) units, where \(x_{i}^{m_{j}}\) represents the input sequence from the \(i^{}\) unit of the \(j^{}\) modality, \(t_{i}^{m_{j}}\) denotes the corresponding time points, and \(y_{i}\) is the task-specific outcome. Take multimodal EHR as an example, each \(j^{}\) modality, which may vary from time-series data like heart rate, blood pressure, and glucose levels to high-dimensional inputs such as clinical notes and X-rays, contains \(l^{_{}}\) observations. Figure 5 is a more specific illustration of the FlexiModal example.

### MIMIC-IV and MIMIC-III Datasets

TasksIn the ICU, where rapid and informed decisions are crucial, accurate mortality prediction is essential to provide clinicians with advanced warnings of patient deterioration, aiding in critical decision-making processes . Similarly, the prediction of patient length-of-stay is indispensable for optimizing treatment plans, resource allocation, and discharge processes . Further, phenotyping of critical care conditions is highly relevant to comorbidity detection and risk adjustment and presents a more challenging task than binary classification, due to the heterogeneous presentation of conditions and the larger number of prediction tasks . We concentrate on three critical care tasks as highlighted in , performing extensive empirical analysis on each building block of the proposed framework.

* **48-IHM** In this binary classification task, we predict in-hospital mortality based on the first 48 of the ICU stay for patients who stayed in the ICU for at least 48 hours.
* **LOS** We formulate our length-of-stay task similar to that of 48-IHM: for patients who spent at least 48 hours in the ICU, we predict ICU discharge without expiration within the following 48 hours.
* **25-PHE** In this multilabel classification problem, we attempt to predict one of 25 acute care conditions  (e.g., congestive heart failure, pneumonia, shock, etc.) at the _end_ each each patient's ICU stay. Because the original task was designed for diagnoses based on ICD-9 codes, but MIMIC-IV includes both ICD-9 and ICD-10 codes, we map patients with diagnoses coded using ICD-10 using the conversion database provided by .

We implement an in-hospital mortality prediction (**48-IHM**) task to evaluate our method's ability to predict short-term patient deterioration. Similarly, an accurate determination of patient discharge times is crucial for optimizing patient outcomes and hospital resource allocation , which motivates our length-of-stay (**LOS**) task. We frame 48-IHM and LOS as binary classification problems and use a 48-hour observation window (for patients who spent at least 48 hours in the ICU) to predict in-hospital mortality (48-IHM) and discharge (without expiration) within the 48 hours following the observation window (LOS). Lastly, identifying the presence of specific acute care conditions in patient records is essential for various clinical objectives, including the construction of cohorts for clinical studies and the detection of comorbidities . Traditional methods, often reliant on manual chart reviews or simple billing code-based definitions, are increasingly being supplemented by machine learning techniques ; automating this process requires high-fidelity classifications, motivating our 25-type phenotype classification (**25-PHE**) task. In this multilabel classification problem, we attempt to predict one of 25 acute care conditions using data from the entire ICU stay.

Figure 5: **Schematic of tasks of interest.** Plotted are example vitals/labs, radiological notes, X-rays, and ECGs sampled over the course of a patient’s ICU stay. The first three rows represent example observations from a single modality consisting of three irregularly sampled vital signs (HR, BP), and lab values (Glucose). The following three rows represent irregularly sampled radiological notes, X-rays, and ECGs. Opaque shapes denote observations falling within the observation window (i.e., observations that are used to generate predictions), while translucent shapes are not used to generate predictions. For the **48-IHM** task, we use the first 48 hours of observations to predict death at any time during the ICU stay. For the **LOS** task, we use the first 48 hours of observations to predict whether the patient will be discharged (alive) during the following 48 hours. And in the phenotyping task (**PHE**), we use all observations to predict one of 25 critical care conditions.

EvaluationIn our initial analysis, we focused on patients with no missing modalities, resulting in a dataset comprised of 8,770 ICU stays for the 48-IHM and LOS tasks, and 14,541 stays for the 25-PHE task. For our analyses _with_ missing observations, we include a total of 35,129 stays for 48-IHM and LOS, and 71.173 for 25-PHE. To evaluate the single-label tasks, 48-IHM and LOS, we employ the F1-score and AUROC as our primary metrics. In line with previous studies , we use macro-averaged F1-score and AUROC to assess the 25-PHE task.

Dataset InformationWe leveraged data from MIMIC-IV , a comprehensive database with records from nearly \(300k\) patients admitted to a medical center from 2008 to 2019, focusing on the subset of 73,181 ICU stays. We were able to link core ICU records (containing lab results and vital signs) to corresponding chest X-rays , radiological notes , and electrocardiogram (ECG) data  taking place during a given ICU stay. We allocated 70 percent of the data for model training, with the remaining 30 percent evenly split between validation and testing.

Missingness RatesThe total number of samples for each of our three tasks (i.e., those in which _at least one_ vital sign was recorded in the specified observation window), along with the total number of observations per-modality, are shown in Table 6.

### MOSI and MOSEI Datasets

TaskWe focus on the multimodal sentiment analysis (MSA) task which aims to predict sentiment polarity \(\) {positive, negative, and neutral} and sentiment intensity, which is a real number ranging from -3 to +3 under a multimodal setting.

EvaluationFollowing previous work such as , we adopt mean absolute error (MAE), Pearson correlation (Corr), binary classification accuracy, F1 score computed for non-negative/negative class as evaluation metrics.

Dataset InformationThe CMU-MOSI dataset contains 1284/229/686 train/validation/test samples, and the CMU-MOSEI dataset contains 16326/1871/4659 train/validation/test samples. They are the largest dataset of multimodal sentiment analysis and emotion recognition to date. The datasets contain utterance videos from numerous online YouTube speakers, which are transcribed and properly punctuated, leading to multimodal input consisting of video frames, text, and audio signals.

### PAM Dataset

TaskPhysical Activity Monitoring (PAM) dataset measures the daily living activities of 9 subjects with 3 inertial measurement units. PAM is labeled into 8 classes where each class represents an activity of daily living.

EvaluationWe choose common classification accuracy as the evaluation metric for this task.

Dataset InformationThe processed PAM dataset contains 5,333 segments (samples) of sensory signals. Each sample is measured by 17 sensors and contains 600 continuous observations with the sampling frequency 100 Hz. PAM does not include static attributes and the samples are approximately balanced across all 8 categories.

  
**Task(s)** & **Total** & **Text** & **CXR** & **ECG** \\ 
48-IHM \& LOS & 35,129 & 32,038 & 8,781 & 18,271 \\
25-PHE & 73,173 & 56,824 & 14,568 & 35,925 \\   

Table 6: We present the total number of ICU stays in each task, taking into account observations with missing modalities. The total number of stays with _at least one_ observation of the corresponding modality are shown in the three right-most columns.

### CIFAR-10 Dataset

CIFAR-10  is an established computer-vision dataset used for object recognition. It consists of 60,000 32x32 color images containing one of 10 object classes ("plane", "car", "bird", "cat", "deer", "dog", "frog", "horse", "ship", "truck"), with 6000 images per class.

## Appendix C Mechanisms of Different Router Designs

### Joint Experts & Routers

In this approach, a concatenated embedding of all modalities is created, and this combined input is directed to selected experts by the router. This method allows the model to capture interactions between modalities at the input level, as the concatenated embedding provides a unified representation that includes all modalities. The router and experts work with this comprehensive view, enabling the model to learn correlations and interactions directly from the fused data. However, this approach might not fully capture modality-specific nuances since the characteristics of each modality are blended into a single representation.

**Advantages**

1. Captures inter-modal relationships by considering all modalities together.
2. Simplifies the routing mechanism by treating the concatenated embedding as a single input.

**Challenges**

1. May overlook modality-specific features due to the blending of all modalities into one representation.
2. Could be less efficient if some modalities are irrelevant for certain tasks or experts.

### Modality-Specific Router

Each modality's embedding is independently assigned to a shared pool of experts by modality-specific routers. This setup allows the model to maintain the distinctiveness of each modality while still leveraging a common pool of expertise. By doing so, it can better capture modality-specific nuances and how they contribute independently to the overall task. However, this approach might be less effective in capturing complex inter-modal interactions since the initial routing is done independently for each modality.

**Advantages**

1. Preserves modality-specific information by routing each modality independently.
2. Flexible in directing modalities to the most relevant experts, potentially improving efficiency.
3. Captures interactions between modalities to some extent, but may not be as effective as joint routing approaches.

**Challenge** needs additional coordination between independent routes to leverage cross-modal insights.

   Dataset & Research Area & Modalities & Sample Size & Tasks \\  MIMIC-III & Healthcare & Time-Series, Text & 36,212 & Mortality, length-of-stay, phenotyping \\ MIMIC-IV & Healthcare & Time-Series, Text, Images, ECG & 73,173 & Mortality, length-of-stay, phenotyping \\ CMU-MOSI & Affective Computing & Text, Video, Audio & 2,199 & Sentiment \\ CMU-MOSEI & Affective Computing & Text, Video, Audio & 22,777 & Sentiment, emotions \\ PAM & Healthcare & Time-Series & 5,333 & Activity recognition \\ CIFAR-10 & Multimedia & Images & 60,000 & Image classification \\   

Table 7: Dataset Summary

### Disjoint Experts & Routers

In this configuration, modality-specific routers assign each modality's embedding to separate pools of experts, with each pool uniquely tailored to process a specific modality type. This method maximizes the ability of the model to capture and exploit modality-specific features and relationships, as each pool of experts is optimized for a particular type of data. However, this setup might limit the model's ability to learn from the interactions between modalities, as each is processed in isolation.

#### Advantages

1. Allows for highly specialized processing of each modality, potentially improving performance on modality-specific tasks.
2. Modality-specific experts can develop deeper insights into the characteristics and patterns within their designated data type.

#### Challenges

1. Inter-modal relationships might be underutilized due to the segregated processing of each modality.
2. Requires additional coordination or subsequent integration stages to combine insights from different modality-specific experts.

Each router type offers unique benefits and faces specific challenges in capturing the subtle relationships between modalities. The choice among them depends on the specific requirements of the application, including the importance of preserving modality-specific information versus capturing inter-modal interactions, and the computational efficiency of managing multiple experts and routers. For example, we found that modality-specific routers are more effective in ameliorating the effect of missing modality in our experiments.

## Appendix D Data Preprocessing

### Mimic-Iv

In the preprocessing stage, we focused on 30 pertinent lab and chart events from each patient's ICU record for vital sign measurements. For chest X-rays, we utilized a pre-trained DenseNet-121 model , which was fine-tuned on the CheXpert dataset , to extract 1024-dimensional image embeddings. For radiological notes, we obtained 768-dimensional embeddings using the BioClinicalBERT model . ECG signals were processed using a convolutional autoencoder, adapted from , to generate a 256-dimensional embedding for each ECG.

Time seriesWe selected 30 time series events for inclusion, following . Nine of these were vital signs: heart rate, mean/systolic/diastolic blood pressure, respiratory rate, oxygen saturation, and Glascow Coma Scale (GCS) verbal, eye, and motor response. We also included 21 lab values: potassium, sodium, chloride, creatinine, urea nitrogram, bicarbonate, anion gap, hemoglobin, hematocrit, magnesium, platlet count, phosphate, white blood cell count, total calcium, MCH, red blood cell count, MCHC, MCV, RDW, platlet count, neutrophil count, and vancomycin. We standard scale each time series value to have mean \(0\) and standard deviation \(1\), based on the values in the training set.

Chest X-raysTo incorporate a medical imaging modality into our analyses, we use the MIMIC-CXR-JPG  module available from Physionet , which includes 377,110 JPG format images derived from the DICOM-based MIMIC-CXR database . Following , for each image, we resize each JPG image to 224 \(\) 224 pixels and then extract embeddings from the last layer of the Densenet121 model. We identify X-rays taken while the patient was in the ICU by first matching subject IDs in MIMIC-CXR-JPG with the core MIMIC-IV database, then limiting these matched X-rays to those with a chart time occuring between an ICU admission and discharge.

Radiological notesTo incorporate text data, we use the MIMIC-IV-Note module , which contains 2,321,355 deidentified radiology reports for 237,427 patients that can be matched with patients in the main MIMIC-IV via a similar approach to chest X-rays. We note that we were unable to obtain _intermediate_ clinical notes (i.e., notes made by clinicians throughout a patient stay), as those have not yet been publicly released. We extract note embeddings using Bio-Clinical BERT .

Electrocardiograms (ECGs)To include ECGs as an additional modality in our models, we utilize the MIMIC-IV-ECG  module, which includes approximately 800,000 ECGs (10 seconds, sampled at 500 Hz) collected from nearly 160,000 unique patients. To transform the ECGs so that they are suitable for input to our model, we adopt a convolutional autoencoder approach, adapted from , that compresses each ECG into a 256-dimensional vector. Specifically, each diagnostic ECG contains a \(5000 12\) dimensional vector (5000 time points \(\) 12 ECG leads). To prepare the ECG for input to the autoencoder, we only include the first \(4096\) time points. We then train the autoencoder to compress the ECG into a 256-dimensional latent vector, and then reconstruct the original ECG using upsampling layers, using mean squared error as our loss function. The architecture is shown in Figure 6. We train the autoencoder with 90% of the ECGs available in the MIMIC-IV-ECG projection and use the rest for validation. We selected a batch size of 2048, and reduced the learning rate by a factor of \(0.5\) if the validation loss had plateaued for \(3\) epochs. Training stopped if the validation loss had not decreased for \(6\) epochs. For our encoder, we use filter numbers of \(\), kernel widths of \(\) and a dropout rate of \(0.1\). For the decoder, we use the same filter numbers and kernel widths in reverse, and maintain a dropout rate of \(0.1\).

### PAM Dataset

We follow the preprocessing procedure from  as published from their official GitHub repository1.

Figure 6: **CNN Autoencoder Architecture** The encoder consists of 6 convolutional blocks (temporal convolution, batch normalization, dropout, and max pooling layers), followed by a dense layer that reduces the dimensionality of the representation of 256. The decoder reconstructs the input ECG (dimensionality \(4096 12\)) from this latent vector via a dense layer, followed by 6 upsampling convolutional blocks (upsampling, convolutional, batch normalization, and dropout layers).

Modeling Irregularity

### Unified Temporal Discretization Embeddings

Unlike the embeddings in chest X-rays, clinical notes, and ECGs, vitals/lab/time-series values present temporal irregularity _across dimensions_. That is, for the former three modalities, each dimension of the corresponding is observed at each irregular time point \(\). By contrast, the sampling for vitals/labs is irregular in both _within_ and _across_ dimensions. For example, we might observe heart rate values sampled at times \(_{}=\{0,0.2,0.8,1.2,2.8\}\) and glucose values sampled at time \(_{}=\{0.1,0.7,3.4\}\). Given this unique challenge present in vitals/labs, we adapt the Unified Temporal Discretization Embedding (UTDE) approach described in , which combines the mTAND approach described in Section 2.2 with a simpler imputation-based discretization scheme. Specifically, given a set of \(t\) observations \(^{t}\) observed at irregular times \(^{t}\), we a simple imputation scheme to discretize \(\) into target bins \(\) (e.g., \(=\{0,1,2,...,\}\). Specifically, given bin value \(_{i}\), we apply the following rules:

* If there exists a previously observed value of \(\) (i.e., \(\,st.\,_{i}\)), we set the imputed value of \(\) at time \(\), \(_{_{i}}\), to the closest previously observed value.
* If no previously observed value exists, we set the value of \(_{_{i}}\) to the global mean of \(\).

We do this for each possible vitals/lab, to generate a matrix of imputation embeddings \(^{ d_{}}\), were \(d_{}\) is the number of vitals/labs. We then input this embedding into a 1D causal convolutional layer with stride 1 to obtain our final imputation embeddings with hidden dimension \(d_{h}\), \(_{}^{ d_{h}}\).

### Unifying imputation and mTAND embeddings

We combined simple imputation and mTAND embeddings via a gating function \(\). Following , we let \(_{}^{ d_{h}}\) denote the mTAND embeddings for vitals/labs derived from the process described in Section 2.2 and let \(_{}^{ d_{h}}\) denote the simple imputations from the process described above. We use each of these discretization embeddings to derive a final set of embeddings for vitals/labs \(_{}\) via a one-layer MLP gating function \(f\). Specifically, we let \(=f(_{}_{})\), where \(\) denotes the concatenation operator. We then calculate \(_{}\) as

\[_{}=_{}+(1-)_{}^{ d_{h}},\]

where \(\) denotes point-wise multiplication.

## Appendix F Baseline Comparison

### Mists

This approach, from , casts time series and clinical notes as multivariate, irregularly-sampled time series (MISTS) and uses layers of self- and cross-attention to fuse modalities. The method uses a Time2Vec  encoding scheme to represent the irregularity of observation times. We use the same hyperparameters as in the original paper (e.g., \(3\) self- and cross-attention blocks, \(128\)-dimensional time embedding, etc.).

### MuIT

This model from  relies on multiple stacks of pairwise and bidirectional cross-modal attention blocks (without a self-attention mechanism) to attend to low-level features. The results of cross-modal attention are then sent to modality-specific transformers, concatenated, and used to make predictions.

### Mag

This method introduces the Multimodal Adaptation Gate (MAG) as an extension to BERT and XLNet, allowing these pre-trained models to incorporate visual and acoustic data during fine-tuning.

By generating a modality-conditioned shift in their internal representations, MAG enables enhanced sentiment analysis performance on multimodal datasets, achieving human-level accuracy in the field .

### Tfn

The proposed Tensor Fusion Network approach (TFN) integrates three core components: Modality Embedding Subnetworks for generating rich embeddings from unimodal inputs, a Tensor Fusion Layer for capturing all levels of modality interactions through a 3-fold Cartesian product, and a Sentiment Inference Subnetwork tailored to perform sentiment analysis based on the fusion layer's output .

### Haim

The multimodal fusion approach detailed by  extracts a single set of features for each ICU stay, and uses this to predict the outcome of interest (in-hospital mortality, etc.). For vitals/lab values, the authors extract a set of 11 generic time series features: signal length, maximum, minimum, mean, median, SD, variance, number of peaks, and average time-series slope and piece-wise change over time of these metrics. This is done independently for each of the 30 events, leading to \(30 11=330\) vital/lab features per ICU stay. To provide a fair comparison with our method, we only include the most recent five notes and \(128\) vitals measurements in calculating embeddings. We only include entries for which all modalities are observed. For note/X-ray/ECG embeddings, we compute the mean embedding across all observations occurring during the specified time frame (i.e., the first 48 hours of 48-IHM and LOS, the entire stay for PHE). As with our method, we standardize scale values based on the training set.  uses an XGBoost  classifier to predict the outcomes of interest. We follow the hyperparameter optimization approach described in the paper. Specifically, we conduct a grid search across the following sets of hyperparameters: max depth \(=\{5,6,7,8\}\), number of estimators \(=\{200,300\}\), learning rate \(=\{0.3,0.1,0.05\}\). Hyperparameters are selected based on the maximum AU-ROC from five-fold cross-validation.

### Implementation

We integrate F.1 through F.4 into our workflow using the implementation provided by . For F.5, we adapt the time series (e.g., series variance, mean, etc.) feature extraction and model fitting code from the repository released by the corresponding paper. The original paper doesn't use ECG waveforms, so we adopt a similar approach to ECG embeddings as with image and note embeddings, and take the mean value of the latent vector across all included observations.

## Appendix G Computational Resources and Hyper-Parameters

### Computational Resources

We train models using a Lambda Workstation with four A550 GPUs with 24 GB of memory. We are able to train models using a single GPU. An analysis of computation time and memory requirements is shown in Figure 12.

### Hyper-Parameters

The set of parameters we used for experiments can be found in Table 8.

## Appendix H Additional Results

### FlexiModal Experiments

We present additional results comparing FuseMoE to baselines using the MIMIC-III dataset, which includes only vital signs and clinical notes (Table 9), and the MIMIC-IV dataset, featuring vital signs and CXR (Table 10). All experiments utilize the "joint experts and router" configuration. In these settings, FuseMoE demonstrates noticeable advantages.

### Ablation Study on FuseMoE Building Blocks

In Figure 7, we evaluate the impact of various irregularity encoders on the performance of the FuseMoE framework. Our baseline approaches include the following methods:

1. employing only the imputation (discretization) module from the time-series irregularity encoder, as detailed in Appendix E
2. utilizing solely the mTAND module  within the time-series irregularity encoder
3. implementing the SeFT method  as an irregularity encoder
4. adopting the RAINDROP method  as an irregularity encoder

   Task \(\) Method &  &  &  &  &  &  &  &  \\   & AUROC & 81.36 \(\) 0.24 & 77.70 \(\) 0.44 & 81.19 \(\) 1.25 & 76.92 \(\) 0.65 & 80.87 \(\) 0.00 & 82.08 \(\) 0.26 & 81.26 \(\) 0.18 & **82.97 \(\) 0.49** \\   & F1 & 43.35 \(\) 0.39 & 28.40 \(\) 0.75 & 39.59 \(\) 0.43 & 46.59 \(\) 0.33 & 40.88 \(\) 0.00 & 38.14 \(\) 0.31 & 44.59 \(\) 0.24 & **47.48 \(\) 0.23** \\   & AUROC & 82.07 \(\) 0.82 & 81.94 \(\) 0.26 & 81.86 \(\) 0.76 & 81.47 \(\) 0.89 & 81.69 \(\) 0.00 & 82.96 \(\) 0.47 & 82.74 \(\) 0.85 & **83.22 \(\) 0.68** \\  & F1 & 74.07 \(\) 0.18 & 74.46 \(\) 0.17 & 73.89 \(\) 0.93 & 73.39 \(\) 0.14 & 72.93 \(\) 0.00 & **75.67 \(\) 0.59** & 75.16 \(\) 0.42 & 75.43 \(\) 0.19 \\   & AUROC & **71.50 \(\) 0.22** & 71.20 \(\) 0.76 & 70.89 \(\) 0.47 & 70.55 \(\) 0.29 & 63.43 \(\) 0.00 & 71.38 \(\) 0.31 & 70.87 \(\) 0.67 & 71.44 \(\) 0.24 \\  & F1 & 33.52 \(\) 0.39 & 32.80 \(\) 0.18 & 33In Figure 8, we evaluate the impact of various time-series encoders on the performance of the FuseMoE framework. The original FuseMoE framework feeds time-series embeddings obtained from the irregularity encoder into the Transformer  and extracts the last hidden states of the Transformer output to pass through fully connected layers to make predictions. Our baseline approaches include CNN  and LSTM  to encode time-series embeddings from the irregularity encoder.

In Figure 9, we assess the effect of different CXR encoders on the FuseMoE framework. Currently, the FuseMoE framework incorporates DenseNet-121 as the feature extractor for CXR images before their integration into the mTAND module. This setup is compared with the application of the state-of-the-art vision transformer (ViT-B)  as an alternative CXR encoder.

In Figure 10, we evaluate the influence of text encoders on the FuseMoE framework. Currently, FuseMoE incorporates Clinical-Longformer  as the text encoder before integrating it into the mTAND module. This setup is compared with other state-of-the-art text encoders: GRU-D , FT-LSTM , and HierTrans .

Finally, in Figure 11, we investigate the effect of the mTAND module on each modality, while we removed mTAND for a particular modality, the rest of FuseMoE's components remained constant.

### Ablation Study on MoE Architecture

We then conducted ablation studies to explore the efficiency and effectiveness of MoE architecture on model performance. We mainly use MIMIC-IV as our test bed. Figure 12(a) examines the computational efficiency and resource utilization, positioning FuseMoE approximately in the middle of

Figure 8: Transformer is more effective in acting as the time-series encoder than CNN and LSTM. The performance outcomes of these approaches are derived from averages over 5 random runs. We utilized the vital signs and clinical notes components of the MIMIC-IV dataset.

Figure 7: The irregularity encoder employed by FuseMoE achieves the best average results compared with 4 baseline irregularity encoders. The performance of these approaches is averaged over 5 random runs. We utilized the vital signs and clinical notes components of the MIMIC-IV dataset.

Figure 11: Encoding irregularity using the mTAND module improves the overall performance. The positive effect of the irregularity encoder is most evident in vital signs and clinical notes. The performance outcomes of these approaches are averaged over 5 random runs. We utilized all 4 modalities (vital signs, clinical notes, CXR, and ECG) components of the MIMIC-IV dataset.

Figure 10: Clinical-Longformer as the text encoder achieves the best performance compared with baselines. The performance of these approaches is derived from averages over 5 random runs. We utilized all 4 modalities (vital signs, clinical notes, CXR, and ECG) of the MIMIC-IV dataset, while we varied the text encoders, the rest of our framework’s components remained constant.

Figure 9: ViT could further improve predictive results in some tasks by providing better CXR embeddings. The performance of these approaches is averaged over 5 random runs. We utilized all 4 modalities (vital signs, clinical notes, CXR, and ECG) of MIMIC-IV. Note that while we vary the CXR encoders, the rest of our framework’s components remain constant.

the comparison. Despite the increase in model parameters due to the incorporation of the MoE layer, its sparse nature does not significantly escalate the computational load. Figure 12(b) illustrates the correlation between the number of experts and task performance across different modalities. Generally, performance improves with the addition of more experts, plateauing once the count exceeds 16. To achieve a compromise between performance and computational expense, we opted to utilize the top 4 experts out of 16 in our experiments. Figure 12(c) and Figure 13 study the influence of each modality on the top-\(k\) chosen experts. For every expert selected, we calculate the number of samples that include a specific modality, weighted by corresponding weight factors from the gating functions. The outcomes are subsequently normalized across modalities. The analysis of Figure 12(c) reveals that predictions across all tasks heavily depend on vital signs and clinical notes. This reliance is attributed to the abundant samples in these two modalities. Despite the notably smaller quantity of CXR, they play more significant roles in the 25-PHE and 48-IHM tasks, which aligns with our findings in Table 5. The results in Figure 13 demonstrate that the modality weight distribution in the MOSI and MOSEI datasets is more "spread out", with the audio component carrying a greater weight in the MOSEI dataset.

## Appendix I Details on Numerical Experiments

We conduct multiple numerical experiments to illustrate the theoretical convergence rates of the MLE \(_{n}\) to the true mixing measure \(G_{*}\) under both exact-specified and over-specified settings.

### Experimental Setup

**Synthetic Data.** Assume that the true mixing measure \(G_{*}=_{i=1}^{k_{*}}(_{i}^{*})_{(W_{i}^{*},a_{i}^{*},b_{i}^ {*},_{i}^{*})}\) is of order \(k_{*}=2\). The true parameters for the router, \((W_{i}^{*},_{i}^{*})^{d}\), are drawn independently from an isotropic Gaussian distribution with zero mean and variance \(_{r}^{2}=0.01/d\) for \(1 i 6\), and otherwise are set to zero. Similarly, the true parameters of the experts, \((a_{i}^{*},b_{i}^{*})^{d}\), are drawn independently of an isotropic Gaussian distribution with zero mean and variance \(_{e}^{2}=1/d\)

Figure 12: Results of ablation studies on MoE architecture: (a) The computational efficiency and resource utilization of each method when applied to vital signs and clinical notes from the MIMIC-IV dataset; (b) The relationship between the number of experts and task performance across different modalities, including vital signs, clinical notes, and CXR; (c) The impact of each modality on the top-\(k\) experts within a disjoint router structure.

Figure 13: Modality weight composition of CMU-MOSI and CMU-MOSEI datasets.

for all experts. For the variances \(_{i}^{*}\), we also sample from the Gaussian distribution \((0,_{e}^{2})\), and then take the absolute value of the sample. These parameters remain unchanged for all experiments.

Then, we generate i.i.d samples \(\{(X_{i},Y_{i})\}_{i=1}^{n}\) by first sampling \(X_{i}\)'s from the uniform distribution \(\) and then sampling \(Y_{i}\)'s from the true conditional density \(g_{G_{*}}(Y|X)\) of the Laplace gating Gaussian mixture of experts (MoE) given in equation 4.

**Maximum Likelihood Estimation (MLE).** A popular approach to determining the MLE \(_{n}\) for each set of samples is to use the EM algorithm . However, since there are not any closed-form expressions for updating the gating parameters \(_{0i},_{1i}\) in the maximization steps, we have to leverage an EM-based numerical scheme, which was previously used in . In particular, we utilize a simple coordinate gradient descent algorithm in the maximization steps. Additionally, we select the convergence criterion of \(=10^{-6}\) and run a maximum of 2000 EM iterations.

**Initialization.** For each \(k\{k_{*},k_{*}+1\}\), we randomly distribute elements of the set \(\{1,2,...,k\}\) into \(k_{*}\) different Voronoi cells \(_{1},_{2},,_{k_{*}}\), each contains at least one element. Moreover, we repeat this process for each replication. Subsequently, for each \(j[k_{*}]\), we initialize parameters \(W_{i}\) by sampling from a Gaussian distribution centered around its true counterpart \(W_{j}^{*}\) with a small variance, where \(i_{j}\). Other parameters \(_{i},a_{i},b_{i},_{i}\) are also initialized in a similar fashion.

### Exact-specified Setting

Under the exact-specified settings, we conduct 5 sample generations for each configuration, across a spectrum of 10 different sample sizes \(n\) ranging from \(10^{3}\) to \(10^{5}\). It can be seen from Figure 3 (left) that the MLE \(_{n}\) empirically converges to the true mixing measure \(G_{*}\) under the Voronoi metric \(_{1}\) at the rate of order \((n^{-0.49})\), which matches the theoretical parametric convergence rate established in Theorem J.2.

### Over-specified Setting

Under the over-specified settings, we continue to generate 5 samples of size \(n\) for each setting, given 10 different choices of sample size \(n[10^{3},10^{5}]\). From Figure 3 (right), we observe that the MLE \(_{n}\) empirically converges to \(G_{*}\) under the Voronoi metric \(_{2}\) at the rate of order \((n^{-0.47})\), which aligns with the theoretical parametric convergence rate established in Theorem 3.2.

## Appendix J Exact-Specified Setting

In this appendix, we study the theoretical behaviors of the MLE under the exact-specified setting, i.e., \(k=k_{*}\), of the Laplace gating Gaussian MoE. We demonstrate that under the exact-specified setting, the rate of estimated conditional density function \(p_{_{n}}\) to \(p_{G_{*}}\) is parametric \(n^{-1/2})\) (up to some logarithmic factor).

**Theorem J.1**.: _The density estimation \(p_{_{n}}(Y|X)\) converges to the true density \(p_{G_{*}}(Y|X)\) under the Total Variation distance \(V\) at the following rate:_

\[_{X}[V(p_{_{n}}(|X),p_{G_{*}}(|X))]=().\]

The proof of Theorem J.1 can be done similarly to that of Theorem 3.1 in Appendix K.2. The result of Theorem J.1 indicates that as long as we can establish the lower bound of the total variation distance between \(p_{_{n}}\) and \(p_{G_{*}}\) based on certain loss function between the MLE \(_{n}\) and the true mixing measure \(G_{*}\), we directly achieve the rate of the MLE under that loss function.

**Voronoi Loss** We now define that loss function between the MLE and the true mixing measure for the exact-specified setting:

\[_{1}(G,G_{*}):=_{j=1}^{k_{*}}_{i_{j} }(_{i})-(_{j}^{*})+_{j[k_{*}]:|_{j }|=1}_{i_{j}}(_{i})_{ij}(1,1,1,1). \]

Above, for any \((_{1},_{2},_{3},_{4})^{4}\), we define \(_{ij}(_{1},_{2},_{3},_{4})=\|W_{i}-W_{j}^{*}\|^{_{1}}+ \|a_{i}-a_{j}^{*}\|^{_{2}}+|b_{i}-b_{j}^{*}|^{_{3}}+|_{i}-_{j}^{* }|^{_{4}}\) for any \(i_{j}\) and \(j[k_{*}]\). We demonstrate in the following theorem that the rate of MLE to the true mixing measure under the Voronoi loss function \(_{1}\) is \((n^{-1/2})\) (up to some logarithmic factor).

**Theorem J.2** (Exact-specified setting).: _When \(k=k_{*}\) is known, the following Total Variation bound holds guarantertrue for any \(G_{k}()\):_

\[_{X}[V(p_{G}(|X),p_{G_{*}}(|X))]_{1}(G, G_{*}).\]

_Therefore, we have \(_{1}(_{n},G_{*})=()\)._

Proof of Theorem J.2 is in Appendix K.1. The convergence rate of MLE under the Voronoi loss function \(_{1}\) implies that the rates of estimating the true parameters \(W_{i}^{*},a_{i}^{*},b_{i}^{*},_{i}^{*}\) are also \((n^{-1/2})\) (up to logarithmic factors). These rates are comparable to those under the exact-specified setting of softmax gating Gaussian MoE (cf. Theorem 1 in ).

## Appendix K Proof of Theoretical Results

In this appendix, we provide proofs for all theoretical results in the paper. Throughout this appendix, for any vector \(v^{d}\) and \(:=(_{1},_{2},,_{d})^{d}\), we denote \(v^{}=v_{1}^{_{1}}v_{2}^{_{2}} v_{d}^{_{d}}\), \(|v|:=v_{1}+v_{2}++v_{d}\) and \(!:=_{1}!_{2}!_{d}!\).

### Proof of Theorem J.2

First of all, we need to establish the following bound:

\[_{X}[V(p_{G}(|X),p_{G_{*}}(|X))]_{1}(G,G _{*}).\]

For that sake, it is sufficient to demonstrate two following inequalities:

* **Inequality \(\).**\(_{G_{k_{*}}():_{1}(G,G_{*})^{ }}_{X}[V(p_{G}(|X),p_{G_{*}}(|X))]}{ _{1}(G,G_{*})}>0\);
* **Inequality \(\).**\(_{G_{k_{*}}():_{1}(G,G_{*})>^{ }}_{X}[V(p_{G}(|X),p_{G_{*}}(|X))]}{_{1}(G,G_{*})}>0\),

for some constant \(^{}>0\).

**Proof of inequality \(\)**: The inequality \(\) is equivalent to

\[_{ 0}_{G_{k_{*}}():_{1}(G,G _{*})}_{X}[V(p_{G}(|X),p_{G_{*}}(|X) )]}{_{1}(G,G_{*})}>0.\]

Assume that the above inequality is not true, then, there exists a sequence of mixing measure \(G_{n}:=_{i=1}^{k_{*}}(_{i}^{n})_{(W_{i}^{n},a_{i}^{n},b_{i} ^{n},_{i}^{n})}_{k_{*}}()\) such that both \(_{1}(G_{n},G_{*})\) and \(_{X}[V(p_{G_{n}}(|X),p_{G_{*}}(|X))]/_{1}(G_{n},G_{*})\) go to zero as \(n\). Now, we define

\[_{j}^{n}=_{j}(G_{n}):=\{i[k_{*}]:\|_{i}^{n}- _{j}^{*}\|\|_{i}^{n}-_{}^{*}\|,\; j\},\]

for any \(j[k_{*}]\) as Voronoi cells with respect to the mixing measure \(G_{n}\), where we denote \(_{i}^{n}:=(W_{i}^{n},a_{i}^{n},b_{i}^{n},_{i}^{n})\) and \(_{j}^{*}:=(W_{j}^{*},a_{j}^{*},b_{j}^{*},_{j}^{*})\). In this proof, since our arguments are assymptotic, we can assume without loss of generality (WLOG) that these Voronoi cells does not depend on \(n\), that is, \(_{j}=_{j}^{n}\). Next, it follows from the hypothesis \(_{1n}:=_{1}(G_{n},G_{*}) 0\) as \(n\) that each Voronoi cell contains only one element. Therefore, we may assume WLOG that \(_{j}=\{j\}\) for any \(j[k_{*}]\), which implies that \((W_{j}^{n},a_{j}^{n},b_{j}^{n},_{j}^{n})(W_{j}^{*},a_{j}^{*},b_{j}^{*}, _{j}^{*})\) and \((_{j}^{n})(_{j}^{*})\) as \(n\). Then, the loss function between \(G_{n}\) and \(G_{*}\) is given by

\[_{1}(G_{n},G_{*})=_{i=1}^{k_{*}}(_{i}^{n}) \| W_{i}^{n}\|+\| a_{i}^{n}\|+\| b_{i}^{n}\|+\|_{i}^ {n}\|+(_{i}^{n})-(_{i}^{*}),\]

where we denote \(_{1i}^{n}:=_{1i}^{n}-_{1i}^{*}\), \( a_{i}^{n}:=a_{i}^{n}-a_{i}^{*}\), \( b_{i}^{n}:=b_{i}^{n}-b_{i}^{*}\) and \(_{i}^{n}:=_{i}^{n}-_{i}^{*}\).

Now, we break the rest of our arguments into three steps:

#### Stage I - Density decomposition

In this step, we aim to decompose the term \(Q_{n}:=_{i=1}^{k_{*}}(-\|W_{i}^{*}-X\|+_{i}^{*})[p_{G_ {n}}(Y|X)-p_{G_{*}}(Y|X)]\), which can be represented as follows:

\[Q_{n} =_{i=1}^{k_{*}}(_{i}^{n})F(Y|X;W_{i}^{n},a_{i} ^{n},b_{i}^{n},_{i}^{n})-F(Y|X;W_{i}^{*},a_{i}^{*},b_{i}^{*},_{i}^{*}) \] \[-_{i=1}^{k_{*}}(_{i}^{n})H(Y|X;W_{i}^{n})-H(Y| X;W_{i}^{*})\] \[+_{i=1}^{k_{*}}(_{i}^{n})-(_{i}^{*}) F(Y|X;W_{i}^{*},a_{i}^{*},b_{i}^{*},_{i}^{*})-H(Y|X,W_{i}^{*} )\] \[:=A_{n}-B_{n}+E_{n}, \]

where we denote \(F(Y|X;W,a,b,):=(-\|W-X\|)f(Y|a^{}X+b,)\) and \(H(Y|X;W)=(-\|W-X\|)p_{G_{n}}(Y|X)\). By applying the first-order Taylor expansion, we can rewrite \(A_{n}\) as

\[A_{n} =_{i=1}^{k_{*}}_{||=1}^{n})}{ !}( W_{i}^{n})^{_{1}}( a_{i}^{n})^{_{2}}(  b_{i}^{n})^{_{3}}(_{i}^{n})^{_{4}}\] \[|+|_{2}|+_{3}+_{4}}F}{ W^{_{1}} a^{ _{2}} b^{_{3}}^{_{4}}}(Y|X;W_{i}^{*},a_{i} ^{*},b_{i}^{*},_{i}^{*})+R_{1}(X,Y)\] \[=_{i=1}^{k_{*}}_{||=1}^{n})}{ !}( W_{i}^{n})^{_{1}}( a_{i}^{n})^{_{2}}(  b_{i}^{n})^{_{3}}(_{i}^{n})^{_{4}}\] \[|} g}{ W^{_{1}}}(X;W_{i}^{*})|+_{3}+ _{4}}f}{ a^{_{2}} b^{_{3}}^{ _{4}}}(Y|(a_{i}^{*})^{}X+b_{i}^{*},_{i}^{*})+R_{1}(X,Y),\]

where \(R_{1}(X,Y)\) is a Taylor remainder that satisfies \(R_{1}(X,Y)/_{1}(X,Y) 0\) as \(n\) and \(g(X,W):=(\|W-X\|)\). Recall that \(f\) is the univariate Gaussian density, then by denoting \(h_{1}(X;a,b):=a^{}X+b\), we can verify that

\[}f}{^{_{4}}}(Y|(a_{i}^{*})^{}X +b_{i}^{*},_{i}^{*})=}}}f}{ h_{1}^{2_{4}}}(Y|(a_{i}^{*})^{}X+b_{i}^{*},_{i}^ {*}).\]

Consequently, we get

\[A_{n} =_{i=1}^{k_{*}}_{||=1}^{n})}{2^ {_{4}}!}( W_{i}^{n})^{_{1}}( a_{i}^{n})^{ _{2}}( b_{i}^{n})^{_{3}}(_{i}^{n})^{_{4}}\] \[ X^{_{2}}|}g}{ W^{_{1}}}(X;W_{i}^{*})|+_{3}+2_{4}}f}{ h_{1}^{|_{2}|+ _{3}+2_{4}}}(Y|(a_{i}^{*})^{}X+b_{i}^{*},_{i}^{*})+R_{1}( X,Y)\] \[=_{i=1}^{k_{*}}_{|_{1}|=0}^{1-|_{1}|}_{| _{2}|=0}^{2(1-|_{1}|-|_{2}|)}_{=0}^{ +2_{4}=}{2^{_{4}}!}}^{n})}{2^{ _{4}}!}( W_{i}^{n})^{_{1}}( a_{i}^{n})^{ _{2}}( b_{i}^{n})^{_{3}}(_{i}^{n})^{_{4}}\] \[ X^{_{2}} |}g}{ W^{_{1}}}(X;W_{i}^{*})|+}f}{ h_{1}^{|_{2}|+}}(Y|(a_{i}^{ *})^{}X+b_{i}^{*},_{i}^{*})+R_{1}(X,Y), \]

where we denote \(=_{3}+2_{4}\).

Subsequently, we also apply the first-order Taylor expansion to the term \(B_{n}\) defined in equation 10 and get that

\[B_{n} =_{i=1}^{k_{*}}_{||=1}^{n})}{ !}( W_{i}^{n})^{}H}{ W^{ }}(Y|X;W_{i}^{*})+R_{2}(X,Y)\] \[=_{i=1}^{k_{*}}_{||=1}^{n})}{ !}( W_{i}^{n})^{}g}{ W ^{}}(X;W_{i}^{*})p_{G_{n}}(Y|X)+R_{2}(X,Y), \]

where \(R_{2}(X,Y)\) is a Taylor remainder such that \(R_{2}(X,Y)/_{1}(G_{n},G_{*}) 0\) as \(n\).

From the above results, the term \(Q_{n}\) can be rewritten as

\[Q_{n} =_{i=1}^{k_{*}}_{|_{1}|=0}^{1}_{|_{2}|=0} ^{1-|_{1}|}_{=0}^{2(1-|_{1}|-|_{2}|)}S_{i,_{1},_{2},}^{n} X^{_{2}}|}g}{  W^{_{1}}}(X;W_{i}^{*})|+}f} { h_{1}^{|_{2}|+}}(Y|(a_{i}^{*})^{}X+b_{i}^{*},_{i}^{*})\] \[+_{i=1}^{k_{*}}_{||=0}^{1}T_{i,}^{n} g}{ W^{}}(X;W_{i}^{*})p_{G_{n}}(Y|X)+R_ {1}(X,Y)+R_{2}(X,Y), \]

in which we respectively define for each \(i[k_{*}]\) that

\[S_{i,_{1},_{2},}^{n} :=_{_{3}+2_{4}=,\\ 0_{3}+_{4} 1-|_{1}|-|_{2}|} ^{n})}{2^{_{4}}!}( W_{i}^{n})^{ _{1}}( a_{i}^{n})^{_{2}}( b_{i}^{n})^{_{3}}( _{i}^{n})^{_{4}},\] \[T_{i,}^{n} :=^{n})}{!}( W_{i}^{n})^{},\]

for any \((_{1},_{2},)(_{d},_{d},0)\) and \(_{d}\). Otherwise, \(S_{i,_{d},_{d},0}^{n}=T_{i,_{d}}^{n}:=( _{i}^{n})-(_{i}^{*})\).

**Stage 2 - Non-vanishing coefficients**:

Moving to the second step, we will show that not all the ratios \(S_{i,_{1},_{2},}^{n}/_{1}(G_{n},G_{*})\) and \(T_{i,}^{n}/_{1}(G_{n},G_{*})\) tend to zero as \(n\). Assume by contrary that all of them approach zero when \(n\), then for \((_{1},_{2},)=(_{d},_{d},0)\), it follows that

\[_{1}(G_{n},G_{*})}_{i=1}^{k_{*}}( _{i}^{n})-(_{i}^{*})=_{i=1}^{k_{*}}, _{2},}^{n}|}{_{1}(G_{n},G_{*})} 0. \]

Additionally, for tuples \((_{1},_{2},)\) where \(_{1}\{e_{1},e_{2},,e_{d}\}\) with \(e_{j}:=(0,,0,_{j-th},0,,0)\), \(_{2}=_{d}\) and \(=0\), we get

\[_{1}(G_{n},G_{*})}_{i=1}^{k_{*}}(_{i}^{n })\| W_{i}^{n}\|_{1}=_{i=1}^{k_{*}},_{2},}^{n}|}{_{1}(G_{n},G_{*})} 0.\]

For \((_{1},_{2},)\) where \(_{1}=_{d}\), \(_{2}\{e_{1},e_{2},,e_{d}\}\) and \(=0\), we have

\[_{1}(G_{n},G_{*})}_{i=1}^{k_{*}}(_{i}^{n })\| a_{i}^{n}\|_{1}=_{i=1}^{k_{*}},_{2},}^{n}|}{_{1}(G_{n},G_{*})} 0.\]

For \((_{1},_{2},)\) where \(_{1}=_{2}=_{d}\) and \(=1\), we have

\[_{1}(G_{n},G_{*})}_{i=1}^{k_{*}}(_{i}^{n })\| b_{i}^{n}\|_{1}=_{i=1}^{k_{*}},_{2},}^{n}|}{_{1}(G_{n},G_{*})} 0.\]

For \((_{1},_{2},)\) where \(_{1}=_{2}=_{d}\) and \(=2\), we have

\[_{1}(G_{n},G_{*})}_{i=1}^{k_{*}}(_{i}^{n })\|_{i}^{n}\|_{1}=_{i=1}^{k_{*}},_{2},}^{n}|}{_{1}(G_{n},G_{*})} 0.\]As a result, we achieve that

\[_{1}(G_{n},G_{*})}_{i=1}^{k_{*}}(_{i}^{n}) \| W_{i}^{n}\|_{1}+\| a_{i}^{n}\|_{1}+| b_{i}^{n}|+| _{i}^{n}| 0.\]

Due to the topological equivalence between norm-1 and norm-2, the above limit implies that

\[_{1}(G_{n},G_{*})}_{i=1}^{k_{*}}(_{i}^{n} )\| W_{i}^{n}\|+\| a_{i}^{n}\|+| b_{i}^{n}|+| _{i}^{n}| 0. \]

Combine equation 14 with equation 15, we deduce that \(_{1}(G_{n},G_{*})_{1}(G_{n},G_{*}) 0\), which is a contradiction. Consequently, at least one among the ratios \(S_{i,_{1},_{2},}^{n}/_{1}(G_{n},G_{*})\) and \(T_{i,}^{n}/_{1}(G_{n},G_{*})\) does not vanish as \(n\) tends to infinity.

**Stage 3 - Fatou's contradiction**:

In this step, we use the Fatou's lemma to point out a contradiction to the results achieved in Step 2. In particular, we denote by \(m_{n}\) the maximum of the absolute values of \(S_{i,_{1},_{2},}^{n}/_{1}(G_{n},G_{*})\) and \(T_{i,}^{n}/_{1}(G_{n},G_{*})\). Since at least one of the previous ratios does not converge to zero, we deduce that \(1/m_{n}\).

Recall from the hypothesis that \(_{X}[V(p_{G_{n}}(|X),p_{G_{*}}(|X))]/_{1}(G_{n},G_{*}) 0\) as \(n\). According to the Fatou's lemma, we have

\[0=_{n}_{X}[V(p_{G_{n}}(|X),p_{G_{*}}(|X ))]}{_{1}(G_{n},G_{*})}_{n} }(Y|X)-p_{G_{*}}(Y|X)|}{_{1}(G_{n},G_{*})}( X,Y) 0.\]

This result indicates that \(|p_{G_{n}}(Y|X)-p_{G_{*}}(Y|X)|/_{1}(G_{n},G_{*})\) tends to zero as \(n\) goes to infinity for almost surely \((X,Y)\). As a result, it follows that

\[_{n}}{m_{n}_{1}(G_{n},G_{*})}=_{n }}(Y|X)-p_{G_{*}}(Y|X)|}{m_{n}_{1}(G_{n},G_{*} )}=0.\]

Next, let us denote \(S_{i,_{1},_{2},}^{n}/[m_{n}_{1}(G_{n},G_{*})] _{i,_{1},_{2},}\) and \(T_{i,}^{n}/[m_{n}_{1}(G_{n},G_{*})]_{i,}\) with a note that at least one among them is non-zero. From the formulation of \(Q_{n}\) in equation 13, we deduce that

\[_{i=1}^{k_{*}}_{|_{1}|=0}^{1}_{|_{2}|=0} ^{1-|_{1}|\,2(1-|_{1}|-|_{2}|)}_{i,_{1},_{2}, } X^{_{2}}|}g}{ W^{ _{1}}}(X;W_{i}^{*})|+}f}{ h_ {1}^{|_{2}|+}}(Y|(a_{i}^{*})^{}X+b_{i}^{*},_{i}^{*})\\ +_{i=1}^{k_{*}}_{||=0}^{1}_{i,} g}{ W^{}}(X;W_{i}^{*})p_{G_{n}}(Y|X)=0, \]

for almost surely \((X,Y)\). The above equation is equivalent to

\[_{i=1}^{k_{*}}_{|_{1}|=0}^{1}[_{|_{2}|=0}^{1-| _{1}|\,2(1-|_{1}|-|_{2}|)}_{=0}^{2(1-|_{1}|-| _{2}|)}_{i,_{1},_{2},} X^{_{2}}+}f}{ h_{1}^{_{2}+}}(Y|(a_{i}^{*})^{ }X+b_{i}^{*},_{i}^{*})+_{i,_{1}}p_{G_{*}}(Y|X)]\\ |}g}{ W^{_{1}}}(X;W _{i}^{*})=0,\]

for almost surely \((X,Y)\). It is worth noting that parameters \(W_{1}^{*},,W_{K}^{*}\) are pair-wise distinct, thus, the set \(|}g}{ W^{_{1}}}(X;W_{i}^{*}): i[k_{*}],\ 0|_{1}| 1}\) is a linearly independent, which implies that

\[_{|_{2}|=0}^{1-|_{1}|\,2(1-|_{1}|-|_{2}|)}_{i, _{1},_{2},} X^{_{2}}+} f}{ h_{1}^{_{2}+}}(Y|(a_{i}^{*})^{}X+b_{i}^{*},_{i}^{*})+ _{i,_{1}}p_{G_{*}}(Y|X)=0,\]for any \(i[k_{*}]\), \(0|_{1}| 1\) for almost surely \((X,Y)\). Moreover, since \((a_{1}^{*},b_{1}^{*},_{1}^{*}),,(a_{k_{*}}^{*},b_{k_{*}}^{*},_{k_{*}}^ {*})\) have pair-wise distinct values, those of \(((a_{1}^{*})^{}X+b_{1}^{*},_{1}^{*}),,((a_{k_{*}}^{*})^{}X+b_{k _{*}}^{*},_{k_{*}}^{*})\) are also pair-wise different. Therefore, the set

\[X^{_{2}}+}f}{ h _{1}^{_{2}+}}(Y|(a_{i}^{*})^{}X+b_{i}^{*},_{i}^{*}),\;p_{G_{* }}(Y|X):\] \[0|_{2}| 1-|_{1}|,\;0 2(1-| _{1}|-|_{2}|)}\]

is also linearly independent. Consequently, we obtain that \(_{i,_{1},_{2},}=_{i,}=0\) for any \(i[k_{*}]\), \(0|_{1}|+_{2} 1\), \(0 2(1-|_{1}|-|_{2}|)\) and \(0|| 1\), which contradicts the fact that at least one among those terms is different from zero.

Hence, we can find some constant \(^{}>0\) such that

\[_{G_{k_{*}}():_{1}(G,G_{*}) ^{}}_{X}[V(p_{G}(|X),p_{G_{*}}(| X))]}{_{1}(G,G_{*})}>0.\]

**Proof of inequality B**: Assume by contrary that the inequality B does not hold, then there exists a sequence of mixing measures \(G_{n}^{}_{k_{*}}()\) such that \(_{1}(G_{n}^{},G_{*})>^{}\) and

\[_{n}_{X}[V(p_{G_{n}^{}}(|X),p_{G_{*}}( |X))]}{_{1}(G_{n}^{},G_{*})}=0.\]

This result leads to \(_{X}[V(p_{G_{n}^{}}(|X),p_{G_{*}}(|X))] 0\) as \(n\). Recall that \(\) is a compact set, therefore, we can replace the sequence \(G_{n}^{}\) by one of its subsequences that converges to a mixing measure \(G^{}_{k_{*}}()\). Since \(_{1}(G_{n}^{},G_{*})>^{}\), this result induces that \(_{1}(G^{},G_{*})>^{}\).

Subsequently, by means of the Fatou's lemma, we achieve that

\[0=_{n}_{X}[2V(p_{G_{n}^{}}(|X),p_{G_{*}}( |X))]_{n}|p_{G_{n}^{}}(Y|X)-p_{G_{*}}( Y|X)|(X,Y).\]

It follows that \(p_{G^{}}(Y|X)=p_{G_{*}}(Y|X)\) for almost surely \((X,Y)\). According to Lemma L.1, the noisy top-K sparse softmax gating Gaussian mixture of experts is identifiable, thus, we obtain that \(G^{} G_{*}\). As a consequence, we obtain that \(_{1}(G^{},G_{*})=0\), which contradicts to the fact that \(_{1}(G^{},G_{*})>^{}>0\).

Hence, the proof is completed.

### Proof of Theorem 3.1

In this appendix, we employ results for M-estimators in  to establish the density estimation rate under the Laplace gating Gaussian mixture of experts (MoE).

Firstly, we introduce some necessary notations and fundamental results. In particular, let \(_{k}():=\{p_{G}(Y|X):G_{k}()\}\) be the set of all conditional density functions w.r.t mixing measures in \(_{k}()\). Next, we denote by \(N(,_{k}(),\|\|_{})\) the covering number of metric space \((_{k}(),\|\|_{})\). Meanwhile, \(H_{B}(,_{k}(),h)\) stands for the bracketing entropy of \(_{k}()\) under the Hellinger distance \(h\) where \(h(p,q):=((-)^{2}d)^{1/2}\) for any probability densities \(p,q\) dominated by the Lebesgue measure \(\). Then, we provide in the following lemma the upper bounds of those terms.

**Lemma K.1**.: _If \(\) is a bounded set, then the following inequalities hold for any \(0<<1/2\):_

1. \( N(,_{k}(),\|\|_{})(1/)\)_;_
2. \(H_{B}(,_{k}(),h)(1/)\)_._

Proof of Lemma K.1 is in Appendix K.2.2. Subsequently, we denote

\[}_{k}() :=\{p_{(G+G_{*})/2}(Y|X):G_{k}()\};\] \[}_{k}^{1/2}() :=\{p_{(G+G_{*})/2}^{1/2}(Y|X):G_{k}()\}.\]In addition, for each \(>0\), we define a Hellinger ball centered around the conditional density function \(p_{G_{*}}(Y|X)\) and intersected with the set \(}_{k}^{1/2}()\) as

\[}_{k}^{1/2}(,):=\{p^{1/2}}_{k}^{1/2}():h(p,p_{G_{*}})\}.\]

To capture the size of the above Hellinger ball,  suggest using the following quantity:

\[_{B}(,}_{k}^{1/2}(,)):=_ {^{2}/2^{13}}^{}H_{B}^{1/2}(t,}_{k}^{1/2}( ,t),\|\|_{2})t, \]

where \(t:=\{t,\}\). Given those notations, let us recall a standard result for density estimation in .

**Lemma K.2** (Theorem 7.4, ).: _Take \(()_{B}(,}_{k}^{1/2}( ,))\) such that \(()/^{2}\) is a non-increasing function of \(\). Then, for some sequence \((_{n})\) and universal constant \(c\) which satisfy \(_{n}^{2} c()\), we obtain that_

\[(_{X}h(p_{_{n}}(|X),p_{G_{*} }(|X))>) c(-n^{2}/c^{2}),\]

_for any \(_{n}\)_

Proof of Lemma K.2 can be found in . Now, we are ready to provide the proof for convergence rate of density estimation in Theorem J.1 in Appendix K.2.1.

#### k.2.1 Main Proof

It is worth noting that for any \(t>0\), we have

\[H_{B}(t,}_{k}^{1/2}(,t),\|\|_{2}) H_{B}(t,_{k}(,t),h).\]

Then, the integral in equation 17 is upper bounded as follows:

\[_{B}(,}_{k}^{1/2}(,)) _{^{2}/2^{13}}^{}H_{B}^{1/2}(t,_{k}(,t),h) t_{^{2}/2^{13}}^{}(1/t) t, \]

where the second inequality follows from part (ii) of Lemma K.1.

As a result, by choosing \(()=\), we can verify that \(()/^{2}\) is a non-increasing function of \(\). Furthermore, the inequality in equation 18 indicates that \(()_{B}(,}_{k}^{1/2}( ,))\). Next, let us consider a sequence \((_{n})\) defined as \(_{n}:=\). This sequence can be validated to satisfy the condition \(_{n}^{2} c()\) for some universal constant \(c\). Therefore, by Lemma K.2, we reach the conclusion of Theorem J.1:

\[_{X}[h(p_{_{n}}(|X),p_{G_{*}}( |X))]>C n^{-c},\]

for some universal constant \(C\) depending only on \(\).

#### k.2.2 Proof of Lemma K.1

**Part (i).** In this part, we will derive the following upper bound for the covering number of metric space \((_{k}(),\|\|_{})\) for any \(0<<1/2\) given the bounded set \(\):

\[ N(,_{k}(),\|\|_{})(1/).\]

To start with, we denote \(:=\{(a,b,)^{d}_{+}:( ,W,a,b,)\}\). As \(\) is a bounded set, the set \(\) is also bounded. Therefore, we can find an \(\)-cover of \(\), denoted by \(_{}\). Additionally, we also define \(:=\{(,W)^{d}:(,W,a,b,)\}\), and \(_{}\) be an \(\)-cover of \(\). Then, it can be validated that

\[|_{}|(^{-(d+2)k}),|_{}|(^{-(d+1)k}).\]

Next, for each mixing measure \(G=_{i=1}^{k}(_{i})_{(W_{i},a_{i},b_{i},_{i})}_{k}()\), we take into account two other mixing measures. The first measure is \(G^{}=_{i=1}^{k}(_{i})_{(W_{i},_{i}, _{i},_{i})}\), where \((_{i},_{i},_{i})_{}\) is the closest points to \((a_{i},b_{i},_{i})\) in this set for all \(i[k]\). The second one is \(:=_{i=1}^{k}(_{i})_{(_{i}, _{i},_{i},_{i})}\) in which \((_{i},_{i})_{}\) for any \(i[k]\). Next, let us define

\[:=\{p_{}_{k}():(_{i },_{i})_{},\;(_{i},_{i},_{i})_{}, i[k]\},\]

then it is obvious that \(p_{^{}}\). Now, we will show that \(\) is an \(\)-cover of metric space \((_{k}(),\|\|_{})\) with a note that it is not necessarily the smallest cover. Indeed, according to the triangle inequality, we have

\[\|p_{G}-p_{}\|_{}\|p_{G}-p_{G^{}}\|_{}+\|p_ {G^{}}-p_{}\|_{}. \]

Since the softmax function is no greater than one, the first term in the right hand side can be upper bounded as follows:

\[\|p_{G}-p_{G^{}}\|_{} _{i=1}^{k}_{X}(-\|W_{i }-X\|+_{i})f(Y|a_{i}^{}X+b_{i},_{i})-f(Y|_ {i}^{}X+_{i},_{i})\] \[_{i=1}^{k}_{X}f(Y|a_{i}^{}X+ b_{i},_{i})-f(Y|_{i}^{}X+_{i}, _{i})\] \[_{i=1}^{k}_{X}\|a_{i}- _{i}\|+\|b_{i}-_{i}\|+\|_{i}-_{i}\| \] \[=_{i=1}^{k}\|a_{i}-_{i}\|+\|b_{i}- {b}_{i}\|+\|_{i}-_{i}\|\] \[. \]

Subsequently, we bound the second term \(\|p_{G^{}}-p_{}\|_{}\) as follows:

\[\|p_{G^{}}-p_{}\|_{} _{i=1}^{k}_{X}(-\|W_{i}-X\|+_{i})-(-\|_{i}-X\|+ _{i})\] \[ f(Y|_{_{i}}^{}X+_{_{i}},_{_{i}})}\] \[_{i=1}^{k}_{X}(- \|W_{i}-X\|+_{i})-(-\|_{i}-X\|+_{i})\] \[_{i=1}^{k}_{X}-\|W_{i}-X\|+ _{i}+\|W_{i}-X\|-_{i}\] \[_{i=1}^{k}_{X}[\|W_{i}-_{i }\|+|_{i}-_{i}|]\] \[, \]

It follows from the results in equation 19, equation 20 and equation 21 that \(\|p_{G}-p_{}\|_{}\). This result indicates that \(\) is an \(\)-cover of the metric space \((_{k}(),\|\|_{})\). As a consequence, we obtain that

\[N(,_{k}(),\|\|_{})|_ {}||_{}|(1/^{(2d+3)k}),\]

which leads to the conclusion of this part: \( N(,_{k}(),\|\|_{})(1/)\).

**Part (ii).** In this part, we provide an upper bound for the bracketing entropy of \(_{k}()\) under the Hellinger distance \(h\):

\[H_{B}(,_{k}(),h)(1/).\]

Since \(\) and \(\) are bounded sets, there exist positive constants \(,,u\) such that \(- a^{}X+b\) and \( u\). Let us define

\[B(Y|X):=}-}{8u} ,&|Y| 2\\ },&|Y|<2\]Then, it can be validated that \(f(Y|a^{}X+b,) B(Y|X)\) for any \((X,Y)\).

Next, let \(\) which will be chosen later and \(\{p_{1},,p_{N}\}\) be an \(\)-cover of metric space \((_{k}(),\|\|_{})\) with the covering number \(N:=N(,_{k}(),\|\|_{})\). Additionally, we also consider brackets of the form \([_{i}^{L}(Y|X),_{i}^{U}(Y|X)]\) where

\[_{i}^{L}(Y|X) :=\{p_{i}(Y|X)-,0\}\] \[_{i}^{U}(Y|X) :=\{p_{i}(Y|X)+,B(Y|X)\}.\]

Then, we can check that \(_{k}()_{i=1}^{N}[_{i}^{L}(Y|X),_{i}^{ U}(Y|X)]\) and \(_{i}^{U}(Y|X)-_{i}^{L}(Y|X)\{2,B(Y|X)\}\).

Let \(S:=\{2,\}(1/)\), we have for any \(i[N]\) that

\[\|_{i}^{U}-_{i}^{L}\|_{1} =_{|Y|<2}[_{i}^{U}(Y|X)-_{i}^{L}(Y|X)]\;(X,Y)+_{|Y| 2}[_{i}^{U}(Y|X)-_{i}^{L}(Y|X)]\;(X,Y)\] \[ S+-}{2u} S^{},\]

where \(S^{}\) is some positive constant. This inequality indicates that

\[H_{B}(S^{},_{k}(),\|\|_{1}) N(, _{k}(),\|\|_{})(1/).\]

By setting \(=/S^{}\), we obtain that \(H_{B}(,_{k}(),\|\|_{1})(1/)\). Finally, due to the inequality \(h^{2}\|\|_{1}\), we reach the conclusion of this part:

\[H_{B}(,_{k}(),h)(1/).\]

Hence, the proof is completed.

### Proof of Theorem 3.2

In order to establish the following Total Variation lower bound under the over-specified settings, i.e. when \(k>k_{*}\) is unknown:

\[_{X}[V(p_{G}(|X),p_{G_{*}}(|X))]_{2}(G,G _{*}),\]

we need to prove two following inequalities:

* **Inequality A.**\(_{G_{k}():_{2}(G,G_{*})^{ }}_{X}[V(p_{G}(|X),p_{G_{*}}(|X))]}{_{2}(G,G_{*})}>0\);
* **Inequality B.**\(_{G_{k}():_{2}(G,G_{*})>^{}}_{X}[V(p_{G}(|X),p_{G_{*}}(|X))]}{_{2}(G,G_{*} )}>0\),

for some constant \(^{}>0\). As the inequality B can be achieved in the same fashion as in Appendix K.1, we concentrate on showing the inequality A in this proof. For that purpose, it suffices to prove that

\[_{ 0}_{G_{k}():_{2}(G,G _{*})}_{X}[V(p_{G}(|X),p_{G_{*}}(|X ))]}{_{2}(G,G_{*})}>0. \]

Assume that the above claim does not hold true, then there exists a sequence of mixing measures \(G_{n}:=_{i=1}^{k_{n}}(_{i}^{n})_{(W_{i}^{n},a_{i}^{n},b_{i} ^{n},a_{i}^{n})}_{k}()\) such that both the terms \(_{2}(G_{n},G_{*})\) and \(_{X}[V(p_{G_{n}}(|X),p_{G_{*}}(|X))]/_{2}(G_{n}, G_{*})\) go to zero as \(n\). Let us recall the formulation of the loss \(_{2}(G_{n},G_{*})\):

\[_{2}(G_{n},G_{*})=_{j[k_{*}],\\ |_{j}|>1}_{i_{j}}(_{i}^{n })\| W_{ij}^{n}\|^{2}+\| a_{ij}^{n}\|^{2}+| b_{ij}^{ n}|^{_{j}}+|_{ij}^{n}|^{}{2}}\] \[+_{j[k_{*}],\\ |_{j}|=1}_{i_{j}}(_{i}^{n })\| W_{ij}^{n}\|+\| a_{ij}^{n}\|+| b_{ij}^{n}|+| _{ij}^{n}|+_{j=1}^{k_{*}}_{i_{j}} (_{i}^{n})-(_{j}^{*}). \]Since \(_{2}(G_{n},G_{*}) 0\), we deduce that \(_{i_{j}}(_{i}^{n})(_{j}^{*})\) and \((W_{i}^{n},a_{i}^{n},b_{i}^{n},_{i}^{n})(W_{j}^{*},a_{j}^{*},b_{j}^{*}, _{j}^{*})\) for all \(i_{j}\) and \(j[k_{*}]\).

Now, we reuse the three-step framework in Appendix K.1.

**Stage 1 - Density decomposition**:

Firstly, by abuse of notations, let us consider the quantity

\[Q_{n}:=_{j=1}^{k_{*}}(-\|W_{j}^{*}-X\|+_{j}^{*}) [p_{G_{n}}(Y|X)-p_{G_{*}}(Y|X)].\]

Similar to Step 1 in Appendix K.1, we can express this term as

\[Q_{n} =_{j=1}^{k_{*}}_{i_{j}}(_{i}^{n}) F(Y|X;W_{i}^{n},a_{i}^{n},b_{i}^{n},_{i}^{n})-F(Y|X;W_{j}^{*},a_{j}^ {*},b_{j}^{*},_{j}^{*})\] \[-_{j=1}^{k_{*}}_{i_{j}}(_{i}^{n}) H(Y|X;W_{i}^{n})-H(Y|X;W_{j}^{*})\] \[+_{j=1}^{k_{*}}_{i_{j}}(_{i }^{n})-(_{j}^{*})F(Y|X;W_{j}^{*},a_{j}^{*},b_{j}^{*}, _{j}^{*})-H(Y|X,W_{j}^{*})\] \[:=A_{n}-B_{n}+E_{n},\]

Next, we proceed to decompose \(A_{n}\) based on the cardinality of the Voronoi cells as follows:

\[A_{n} =_{j:|_{j}|=1}_{i_{j}}( _{i}^{n})F(Y|X;W_{i}^{n},a_{i}^{n},b_{i}^{n},_{i}^{n})-F(Y|X;W_{j}^ {*},a_{j}^{*},b_{j}^{*},_{j}^{*})\] \[+_{j:|_{j}|>1}_{i_{j}}( _{i}^{n})F(Y|X;W_{i}^{n},a_{i}^{n},b_{i}^{n},_{i}^{n})-F(Y|X;W_{j}^ {*},a_{j}^{*},b_{j}^{*},_{j}^{*}).\]

By applying the Taylor expansions of order 1 and \(_{j}\) to the first and second terms of \(A_{n}\), respectively, and following the derivation in equation 11, we get that

\[A_{n} =_{j:|_{j}|=1}_{i_{j}}_{| _{1}|=0}^{1}_{|_{2}|=0}^{1-|_{1}|}_{=0}^{1-| _{1}|-|_{2}|}_{_{3}+2_{4}=,\\ 0_{3}+_{4} 1-|_{1}|-|_{2}|}^{n})}{2^{_{4}}!}( W_{ij}^{n})^{_{1}}(  a_{ij}^{n})^{_{2}}\] \[( b_{ij}^{n})^{_{3}}(_{ij}^{n})^{ _{4}} X^{_{2}}|}g}{ W ^{_{1}}}(X;W_{j}^{*})|+}f}{ h _{1}^{|_{2}|+}}(Y|(a_{j}^{*})^{}X+b_{j}^{*},_{j}^{*})+R_{3}( X,Y)\] \[+_{j:|_{j}|>1}_{i_{j}}_{| _{1}|=0}^{_{j}}_{|_{2}|=0}^{|_{1}|}_{=0}^ {_{j}-|_{1}|}_{_{3}+2_{4}=,\\ 0_{3}+_{4}_{j}=|_{1}|-|_{2}|} ^{n})}{2^{_{4}}!}( W_{ij}^{n})^{ _{1}}( a_{ij}^{n})^{_{2}}\] \[( b_{ij}^{n})^{_{3}}(_{ij}^{n})^{ _{4}} X^{_{2}}|}g}{ W ^{_{1}}}(X;W_{j}^{*})|+}f}{ h _{1}^{|_{2}|+}}(Y|(a_{j}^{*})^{}X+b_{j}^{*},_{j}^{*})+R_{4}( X,Y)\]

where \(R_{i}(X,Y)\) is a Taylor remainder such that \(R_{i}(X,Y)/_{2}(G_{n},G_{s}) 0\) as \(n\) for \(i\{3,4\}\). Next, we apply the Taylor expansions of order 1 and 2 to the first and second terms of \(B_{n}\), respectively, and following the derivation in equation 12, we get that

\[B_{n} =_{j:|_{j}|=1}_{i_{j}}_{| |=1}^{n})}{!}( W_{ij}^{n})^{} g}{ W^{}}(X;W_{j}^{*})p_{G_{n}}(Y|X )+R_{5}(X,Y)\] \[_{j:|_{j}|>1}_{i_{j}}_{| |=1}^{2}^{n})}{!}( W_{ij}^{n})^{} g}{ W^{}}(X;W_{j}^{*})p_{G_{n}}(Y| X)+R_{6}(X,Y),\]where \(R_{5}(X,Y)\) and \(R_{6}(X,Y)\) are Taylor remainders such that their ratios over \(_{2}(G_{n},G_{*})\) approach zero as \(n\). Subsequently, let us define

\[S^{n}_{j,_{1},_{2},} :=_{i_{j}}_{_{3}+ 2_{4}=,\\ 0_{3}+_{4}_{j}-|_{1}|-|_{2}| }^{n})}{2^{_{4}}!}( W^{n}_{ij})^{ _{1}}( a^{n}_{ij})^{_{2}}( b^{n}_{ij})^{_{3}}( ^{n}_{ij})^{_{4}},\] \[T^{n}_{j,} :=_{i_{j}}^{n})}{!}(  W^{n}_{ij})^{},\]

for any \((_{1},_{2},)(_{d},_{d},0)\) and \(_{d}\). Otherwise, \(S^{n}_{j,_{d},_{d},0}=T^{n}_{j,_{d}}:=_{i _{j}}(_{i}^{n})-(_{j}^{*})\). As a consequence, it follows that

\[Q_{n} =_{j=1}^{k_{*}}_{|_{1}|=0}^{_{j}}_{| _{2}|=0}^{_{j}-|_{1}|\,2(_{j}-|_{1}|-|_{2}|)}S^ {n}_{j,_{1},_{2},} X^{_{2}}|}g}{ W^{_{1}}}(X;W^{s}_{j})|+}f}{ h_{1}^{|_{2}|+}}(Y|(a^{*}_{j})^{}X+b ^{*}_{j},^{*}_{j})\] \[+_{j=1}^{k_{*}}_{||=0}^{1+_{(| _{j}|>1)}}T^{n}_{j,}g}{ W^{ }}(X;W^{s}_{j})p_{G_{n}}(Y|X)+R_{3}(X,Y)+R_{4}(X,Y)+R_{5}(X,Y)+R_{6}(X,Y). \]

#### Stage 2 - Non-vanishing coefficients

In this step, we demonstrate that not all the ratios \(S^{n}_{j,_{1},_{2},}/_{2}(G_{n},G_{*})\) and \(T^{n}_{j,}/_{2}(G_{n},G_{*})\) converge to zero as \(n\). Assume by contrary that all these terms go to zero. Then, by employing arguments for deriving equation 14 and equation 15, we get that

\[_{2}(G_{n},G_{*})}_{j=1}^{k_{ *}}_{i_{j}}(_{i}^{n})-(_{j}^{*}) \] \[+_{j:|_{j}|=1}_{i _{j}}(_{i}^{n})\| W^{n}_{ij}\|+\| a^{n }_{ij}\|+| b^{n}_{ij}|+|^{n}_{ij}| 0.\]

Taking the summation of \(_{j:|_{j}|>1}_{j,_{1},_{2},}|}{ _{2}(G_{n},G_{*})}\) for all \((_{1},_{2},)\) where \(_{1}\{2e_{1},2e_{2},,2e_{d}\}\), \(_{2}=_{d}\) and \(=0\), we have

\[_{2}(G_{n},G_{*})}_{j:|_{j}|>1}_{i _{j}}(_{i}^{n})\| W^{n}_{ij}\|^{2} 0.\]

Taking the summation of \(_{j:|_{j}|>1}_{j,_{1},_{2},}|}{ _{2}(G_{n},G_{*})}\) for all \((_{1},_{2},)\) where \(_{1}=_{d}\), \(_{2}\{2e_{1},2e_{2},,2e_{d}\}\) and \(=0\), we have

\[_{2}(G_{n},G_{*})}_{j:|_{j}|>1}_{i _{j}}(_{i}^{n})\| a^{n}_{ij}\|^{2} 0.\]

Combine the above limit with the formulation of \(_{2}(G_{n},G_{*})\) in equation 23, we have that

\[_{2}(G_{n},G_{*})}_{j:|_{j}|>1}_{i _{j}}(_{i}^{n})| b^{n}_{ij}|^{_{j }}+|^{n}_{ij}|^{}{2}} 0.\]

This result implies that we can find some index \(j^{}[k_{*}]:|_{j^{}}|>1\) that satisfies

\[_{2}(G_{n},G_{*})}_{i_{j^{}}} (_{i}^{n})| b^{n}_{ij^{}}|^{_{j^{}}} +|^{n}_{ij^{}}|^{}}{2}} 0.\]

For simplicity, we may assume that \(j^{}=1\). Since \(S^{n}_{1,_{d},_{d},}/_{2}(G_{n},G_{*})\) vanishes as \(n\) for any \(1_{j}\), we divide this term by the left hand side of the above equation and achieve that

\[_{1}}_{_{3}+2_{4}= ,\\ 1_{3}+_{4}_{1}}^{n} )}{2^{_{4}}!}( b^{n}_{i1})^{_{3}}(^{n}_{i1})^{ _{4}}}{_{i_{1}}(_{i}^{n})| b^{n}_{ i1}|^{_{i}}+|^{n}_{i1}|^{}{2}}} 0, \]for any \(1_{1}\).

Subsequently, we define \(M_{n}:=\{| b^{n}_{i1}|,|^{n}_{i1}|^{1/2}:i_{1}\}\) and \(_{n}:=\{(^{n}_{i}):i_{1}\}\). As a result, the sequence \((^{n}_{i})/_{n}\) is bounded, which indicates that we can substitute it with its subsequence that admits a positive limit \(z^{2}_{5i}:=_{n}(^{n}_{i})/_{n}\). Therefore, at least one among the limits \(z^{2}_{5i}\) equals to one. Furthermore, we also denote

\[( b^{n}_{i1})/M_{n} z_{3i},\ (^{n}_{i1})/(2M_{n}) z_{4i}.\]

From the above definition, it follows that at least one among the limits \(z_{3i}\) and \(z_{4i}\) equals to either 1 or \(-1\). By dividing both the numerator and the denominator of the term in equation 25 by \(_{n}M^{n}_{n}\), we arrive at the following system of polynomial equations:

\[_{i_{1}}_{_{3}+2_{4}= ,\\ 1_{3}+_{4}_{1}}_{5i}\,z^{ _{3}}_{3i}\,z^{_{4}}_{4i}}{_{3}!\,_{4}!}=0,\]

for all \(1_{1}\). Nevertheless, from the definition of \(_{1}\), we know that the above system does not admit any non-trivial solutions, which is a contradiction. Consequently, not all the ratios \(S^{n}_{j,_{1},_{2},}/_{2}(G_{n},G_{*})\) and \(T^{n}_{j,}/_{2}(G_{n},G_{*})\) tend to zero as \(n\).

**Stage 3 - Fatou's contradiction**:

Recall that \(_{X}[V(p_{G_{n}}(|X),p_{G_{*}}(|X))]/_{2}(G_{n},G_{*}) 0\) as \(n\). Then, by applying the Fatou's lemma, we get

\[0=_{n}_{X}[V(p_{G_{n}}(|X),p_{G_{*}}(|X ))]}{_{2}(G_{n},G_{*})}_{n} }(Y|X)-p_{G_{*}}(Y|X)|}{_{2}(G_{n},G_{*})} (X,Y),\]

which implies that \(|p_{G_{n}}(Y|X)-p_{G_{*}}(Y|X)|/_{2}(G_{n},G_{*}) 0\) as \(n\) for almost surely \((X,Y)\).

Next, we define \(m_{n}\) as the maximum of the absolute values of \(S^{n}_{j,_{1},_{2},}/_{2}(G_{n},G_{*})\). It follows from Step 2 that \(1/m_{n}\). Moreover, by arguing in the same way as in Step 3 in Appendix K.1, we receive that

\[Q_{n}/[m_{n}_{2}(G_{n},G_{*})] 0 \]

as \(n\). By abuse of notations, let us denote

\[S^{n}_{j,_{1},_{2},}/[m_{n}_{2}(G_{n },G_{*})]_{j,_{1},_{2},},\] \[T^{n}_{j,}/[m_{n}_{2}(G_{n},G_{*})]_{j, }.\]

Here, at least one among \(_{j,_{1},_{2},},_{j,}\) is non-zero. Then, by putting the results in equation 24 and equation 26 together, we get

\[_{j=1}^{k_{*}}_{|_{1}|=0}^{_{j}}_{| _{2}|=0}^{_{j}-|_{1}|\,2(_{j}-_{=0}^{|_{1}| -|_{2}|})}_{j,_{1},_{2},} X^{_{2}} |}g}{ W^{_{1}}}(X;W^{*}_{j}) |+}f}{ h^{|_{2}|+}_{1}}(Y|(a ^{*}_{j})^{}X+b^{*}_{j},^{*}_{j})\\ +_{j=1}^{K}_{||=0}^{1+1\{|_{j}|>1\}} _{j,}g}{ W^{}}(X;W^{* }_{j})p_{G_{n}}(Y|X)=0.\]

Arguing in a similar fashion as in Step 3 of Appendix K.1, we obtain that \(_{j,_{1},_{2},}=_{j,}=0\) for any \(j[k_{*}]\), \(0|_{1}|+|_{2}| 2_{j}\), \(0 2(_{j}-|_{1}|-|_{2}|)\) and \(0|| 1+_{\{|_{j}|>1\}}\). This contradicts the fact that at least one among them is non-zero. Hence, the proof is completed.

## Appendix L Identifiability of the Laplace Gating Gaussian MoE

**Lemma L.1**.: _For any mixing measures \(G\) and \(G_{*}\) in \(_{k}()\) that satisfy \(p_{G}(Y|X)=p_{G_{*}}(Y|X)\) for almost surely \((X,Y)\), we have that \(G G_{*}\)._

Proof of Lemma L.1.: First, we assume that two mixing measures \(G\) and \(G_{*}\) take the following forms: \(G=_{i=1}^{k}(_{i})_{(W_{i},a_{i},b_{i},_{i})}\) and \(G_{*}=_{i=1}^{k_{*}}(^{*}_{i})_{(W^{*}_{i},a^{*}_{i},b^{*}_{i },^{*}_{i})}\). Recall that \(p_{G}(Y|X)=p_{G_{*}}(Y|X)\) for almost surely \((X,Y)\), then we have

\[_{i=1}^{k}( -\|W_{i}-X\|+_{i}) f(Y|a_{i}^{}X+b_{i},_{i})\] \[=_{i=1}^{k_{*}}(-\|W_{i}^{*}-X\|+_{i}^{*} ) f(Y|(a_{i}^{*})^{}+b_{i}^{*},_{i}^{*}). \]

Due to the identifiability of the location-scale Gaussian mixtures , we get that \(k=k_{*}\) and

\[(-\|W_{i}-X\|+_{i}):i[k]} (-\|W_{i}^{*}-X\|+_{i}^{*}):i[k]},\]

for almost surely \(X\). WLOG, we may assume that

\[(-\|W_{i}-X\|+_{i})=(-\|W_{i}^{*}-X\|+ _{i}^{*}), \]

for almost surely \(X\) for any \(i[k]\). Since the \(\) function is invariant to translations, it follows from equation 28 that \(W_{i}=W_{i}^{*}\) and \(_{i}=_{i}^{*}+v_{0}\) for some \(v_{0}\). Notably, from the assumption of the model, we have \(_{k}=_{k}^{*}=0\), which implies that \(v_{0}=0\). As a result, we obtain that \(_{i}=_{i}^{*}\) for any \(i[k_{*}]\). Then, equation 27 can be rewritten as

\[_{i=1}^{k_{*}}(_{i})(-\|W_{i}^{*}-X\|)f(Y|a_{i}^ {}X+b_{i},_{i})\] \[=_{i=1}^{k_{*}}(_{i})(-\|W_{i}^{*}-X\|)f(Y|(a_{i} ^{*})^{}X+b_{i}^{*},_{i}^{*}), \]

for almost surely \((X,Y)\). Next, we denote \(J_{1},J_{2},,J_{m}\) as a partition of the index set \([k_{*}]\), where \(m k_{*}\), such that \((_{i})=(_{i^{}})\) for any \(i,i^{} J_{j}\) and \(j[m]\). On the other hand, when \(i\) and \(i^{}\) do not belong to the same set \(J_{j}\), we let \((_{i})(_{i^{}})\). Thus, we can reformulate equation 29 as

\[_{j=1}^{m}_{i J_{j}}(_{i})(-\|W_{i}^{*}-X \|)f(Y|a_{i}^{}X+b_{i},_{i})\] \[=_{j=1}^{m}_{i J_{j}}(_{i})(-\|W_{i}^{*}- X\|)f(Y|(a_{i}^{*})^{}X+b_{i}^{*},_{i}^{*}),\]

for almost surely \((X,Y)\). This results leads to \(\{((a_{i})^{}X+b_{i},_{i}):i J_{j}\}\{((a_{i}^{*})^{}X+b_{ i}^{*},_{i}^{*}):i J_{j}\}\), for almost surely \(X\) for any \(j[m]\). Therefore, we have

\[\{(a_{i},b_{i},_{i}):i J_{j}\}\{(a_{i}^{*},b_{i}^{*},_{i}^{*}): i J_{j}\},\]

for any \(j[m]\). As a consequence,

\[G=_{j=1}^{m}_{i J_{j}}(_{i})_{(W_{i},a_{i},b_{i}, _{i})}=_{j=1}^{m}_{i J_{j}}(_{i}^{*})_{(W_{i}^{*},a_ {i}^{*},b_{i}^{*},_{i}^{*})}=G_{*}.\]

Hence, we reach the conclusion of this lemma. 

## Appendix M Broader Impact

This paper presents research aimed at propelling advancements in the broad domain of machine learning. The implications of our findings are wide-ranging, with potential applications in sectors including healthcare, autonomous driving, and recommendation systems. Based on our current understanding, this research does not warrant an ethics review, and a detailed discussion of the potential societal impacts is not required at the current stage.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: we have added a separate contribution paragraph in the introduction section (section 1), along with the highly summarized contributions in the abstract. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: see section 5 on discussion on general limitations. Section 3 and 4 have also thoroughly discussed the assumptions and experimental settings. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes]

Justification: see section 3 and Appendix J, K and L for assumptions and proofs.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: we have submitted the implementation of the proposed methods and all baselines in the supplementary material. Appendix B, D, F and G also contain all dataset information and implementation details. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: we have submitted the implementation of the proposed methods and all baselines in the supplementary material. Appendix B, D, F and G also contain comprehensive information to reproduce the experiments. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: dataset information and preprocessing procedure can be found in Appendix B and D. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: all results are averaged over 5 random experiments, as specified by the experiment section 4 and additional results in Appendix H. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ** The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: computational resources used to run the experiments can be found in Appendix G. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: we have reviewed the NeurIPS Code of Ethics and make sure the paper conforms to this. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: we have attempted to discuss the impacts of this work in Appendix M. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: our paper has no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: all the assets used in the paper have been properly credited. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: we have provided documents in the code repository submitted in the supplementary material. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: this paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: this paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.

* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.