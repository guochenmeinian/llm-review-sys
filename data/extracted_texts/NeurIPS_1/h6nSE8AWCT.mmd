# TPC: Test-time Procrustes Calibration for Diffusion-based Human Image Animation

Sunjae Yoon  Gwanhyeong Koo  Younghwan Lee  Chang D. Yoo

Korea Advanced Institute of Science and Technology (KAIST)

{sunjae.yoon,cd_yoo}@kaist.ac.kr

Corresponding author

###### Abstract

Human image animation aims to generate a human motion video from the inputs of a reference human image and a target motion video. Current diffusion-based image animation systems exhibit high precision in transferring human identity into targeted motion, yet they still exhibit irregular quality in their outputs. Their optimal precision is achieved only when the physical compositions (i.e., scale and rotation) of the human shapes in the reference image and target pose frame are aligned. In the absence of such alignment, there is a noticeable decline in fidelity and consistency. Especially, in real-world environments, this compositional misalignment commonly occurs, posing significant challenges to the practical usage of current systems. To this end, we propose Test-time Procrustes Calibration (TPC), which enhances the robustness of diffusion-based image animation systems by maintaining optimal performance even when faced with compositional misalignment, effectively addressing real-world scenarios. The TPC provides a calibrated reference image for the diffusion model, enhancing its capability to understand the correspondence between human shapes in the reference and target images. Our method is simple and can be applied to any diffusion-based image animation system in a model-agnostic manner, improving the effectiveness at test time without additional training.

## 1 Introduction

Denoising diffusion models [5; 27; 26; 10] have transformed the generative landscape of artificial intelligence, leading to groundbreaking achievements [17; 20; 41; 39] in image, speech, and video generation. We explore the application of diffusion model in the specific context of image-to-video generation, focusing on the task of human image animation. The technology of image animation holds great promise, enabling immersive and interactive experiences in entertainment, virtual reality, and digital communication. The human image animation systems [33; 31; 15; 13] are designed to work with referential human image and target motion video, where they transfer the human identity into the target motion, ensuring seamless and unobtrusive integration. This process requires understanding the correspondence between human shapes in the reference image and the target pose frame.

Recent advancements [33; 31] of human image animation systems have demonstrated notable precision in adapting human identity into target motion. Despite advancements, these systems continue to suffer from irregular quality of image animation when the compositions (_i.e._, scale and rotation) of human shapes are not aligned between reference image and target motion. To be specific, Figure 1 (a) presents exploratory experiments on the compositional misalignment of human shapes between the reference and target. As shown in the left experiments, for a given target pose, adjusting the composition of the same human in the reference image (_i.e._, via scaling or rotating) results in inconsistent and low-fidelity output images, especially in terms of clothes and faces of the human.

Furthermore, in the right experiments, when providing motion sequences that display various dynamic movements for a given reference image, the human image animation outputs consistently show low fidelity, especially evident in target poses (_e.g._, bending or approaching close to viewpoint) that cause significant differences in the composition of the human shape. To quantitatively investigate these, Figure 1 (b) presents a sensitivity analysis evaluating how current image animation systems  respond to varying degrees of the compositional misalignment of human shape. The left shows the fidelity (_i.e._, PSNR) of the resulting human according to the relative scale difference of input human shapes between the target and reference. We employ the Intersection of Union (IoU) of bounding boxes of the shapes for relative scale. The right shows fidelity according to the variations of relative angle \(\)2 of human shapes between the target and reference. Current systems demonstrate significant vulnerability to the variations of the compositions, indicating that output fidelity forms a robust region only in the areas with compositionally aligned conditions (_i.e._, \(-<<\), IoU \(>0.7\)).

In fact, diffusion-based systems are inevitably susceptible to this compositional misalignment. The diffusion model uses a reference human image as a condition to generate controlled output from noise based on the target pose. Here, the conditioning is performed through cross-attention based on a visual similarity between patch-wise features of the target pose frame and reference image. To be specific, Figure 2 shows the attention map about a single patch of the target frame (_i.e._, shoulder denoted by blue point) from the reference image during the denoising process. The upper section displays a sample where the human shapes are relatively aligned between the target and the reference, while the lower section shows a case where this alignment is not present. Initially, the attention maps were blurry in both cases. However, as denoising continued, it became clear that samples with aligned human between the target frame and the reference image. (_i.e._, Attention to the shoulder on the target frame is focused on the shoulder on the reference image.) However, when the shape is misaligned, attention to the target frame's shoulder is incorrectly focused on unrelated areas in reference, even up to the

Figure 1: Illustration of compositional misalignment: (a) Results of current human image animation models  on samples in compositional misalignment of human shapes between reference and target. (b) Sensitivity analysis according to variation of compositional misalignment by scaling and rotating (MA: MagicAnimate). Best viewed with zoom.

Figure 2: Attention maps on the reference image corresponding to the target human shape (_e.g._, shoulder at blue point) according to denoising.

last stages of denoising. Consequently, the compositional misalignment between the target and the reference image hinders accurate attention correspondence throughout the denoising process.

To this end, we propose a diffusion guidance referred to as Test-time Procrustes Calibration (TPC). As shown in Figure 3 (a), the existing diffusion-based human image animation system takes inputs of reference image \(x\) and \(i\)-th target pose frame \(p_{i}\), where it generates \(i\)-th output animation frame \(y_{i}\). Here, the \(x\) is encoded into the latent feature \(z\), which serves as a conditioning input for the denoising diffusion model (_i.e._, UNet). As depicted in Figure 3 (b), our proposed TPC incorporates an auxiliary branch into this diffusion conditioning process. This branch is defined as a calibration branch that guides denoising UNet to properly capture visual correspondence between target and reference. To be specific, the TPC provides a calibrated reference image latent \(_{i}\) that aligns with the human shape in the target pose \(p_{i}\) based on statistical shape analysis referred to as Procrustes analysis . Figure 4 offers a qualitative understanding of the influence of this \(_{i}\). Conceptually, in Figure 4 (a), when the humans in reference image \(x\) and the target pose sequence \(p=\{p_{i-1},p_{i},p_{i+1},p_{i+2}\}\) are projected onto the ideal 2D shape-style space, they show distinct locations in terms of style axis, where the system aims to generate image animation frame \(y=\{y_{i-1},y_{i},y_{i+1},y_{i+2}\}\) with the style of \(x\) and the shape of \(p\). However, in cases (_e.g._, \(x\) and \(p_{i+2}\)) where significant gaps exist along the shape axis (_i.e._, compositional misalignment), the current diffusion model struggles to preserve the original style. This leads to low-fidelity outputs (_e.g._, \(y_{i+2}\)) and results in temporal inconsistencies due to unstable fidelity across frames. Thus, as illustrated in Figure 4 (b), we bridge this gap in shapes between reference and target by providing correspondence guidance latent \(\) by our designed TPC. With this guidance condition, diffusion-based animation systems achieve robustness to fidelity variations and maintain temporal consistency among frames. The TPC is simple and works in a model-agnostic manner without additional training and validates its effectiveness on human image animation benchmarks (_i.e._, TiTok, TED-talks ) and even in unseen domain data of real environment scenarios.

## 2 Related Work

### Diffusion-based Human Image Animation

Human image animation aims to provide a video about an animated version of an input human image. As a human-centric application of image-to-video technology, the human image animation systems  have previously been developed based on generative adversarial networks. The recent emergence of denoising diffusion models  has presented a new paradigm for generative models, where image animation has also faced fundamental innovations. The diffusion-based framework generates an image animation from noise using pre-trained denoising capabilities based on input human image and target motion video, where the motion video can be extracted

Figure 4: Conceptual illustration of the effectiveness of TPC in terms of style and shape variation.

Figure 3: Illustration of (a) current diffusion-based human image animation systems and (b) Test-time Procrustes Calibration (TPC) on top of these systems. The TPC can be applied to diffusion-based models in a model-agnostic manner, enhancing the fidelity and consistency of the output video.

from various pose estimation models [4; 8]. Early work of diffusion-based image animation was made in DreamPose , which leveraged the pre-trained text-to-image diffusion model (_e.g._, Stable Diffusion ) by conditioning on human image embeddings instead of text, rendering videos of humans in various outfits performing simple walking motions. AnimateAnyone  introduces a UNet-style reference image encoder that enhances the layered conditioning of the reference image following the encoding-decoding process of UNet. Furthermore, ControlNet  has been a popular choice with the diffusion model by offering more controlled guidance about target motion. To enhance background fidelity, DisCo  segments the background of the reference image and integrates it into ControlNet, along with the target motion. For the temporal consistency of output video, MagicAnimate  introduces temporal attention by inflating the original 2D UNet to 3D temporal UNet. However, current systems still suffer from quality irregularity issues when the human shapes are not aligned between the reference image and the target motion. Such misalignments frequently occur in real-world scenarios, prompting our proposed Procrustes calibration to address this challenge.

## 3 Preliminary

### Procrustes Analysis

Procrustes3 analysis (PA) [6; 7] is a statistical shape analysis technique used to compare the shapes of objects [11; 19]. The PA involves finding the best alignment between two shapes by scaling, translating, and rotating one shape to match the other as closely as possible. To formulate the process of PA, we are given two sets of \(n\) points as \(X=\{x_{1},,x_{n}\}^{n d}\) and \(Y=\{y_{1},,y_{n}\}^{n d}\), where \(d\) is the dimension of the point. We perform Procrustes transformation composed of scaling factor \(s^{1}\), rotation matrix \(r^{d d}\), and translation vector \(t^{1 d}\) as given below:

\[=s Xr+t, \]

where the vector \(t\) is added with broadcasting to all \(n\) points. The objective is to find the optimal transformation parameters \(s,r,t\) to minimize the sum of squared differences between \(\) and the \(Y\).

\[}||-Y||_{F}, \]

where \(||||_{F}\) is Frobenius norm. The optimal value \(r^{*}\) is typically obtained via singular value decomposition and the \(s^{*}\) and \(t^{*}\) are calculated after the optimal rotation \(r^{*}\) is found. Here, we apply this PA to align the human shapes of a reference image and target motion frame.

## 4 Method

Given a human reference image \(R\) and a target pose sequence \(P=[P_{1},,P_{L}]\), a human image animation system generates image animation video \(V=[V_{1},,V_{L}]\) which follows the pose sequence by the human in the reference image, where \(L\) is the number of frames. Figure 5 shows the application of Test-time Procrustes Calibration (TPC) into the general diffusion-based human image animation system. The TPC aims to improve the quality of resulting animation by consistently ensuring the compositional alignment of human shapes between the reference image and the target poses. At each denoising step \(t\), TPC takes an input reference image \(R\) and target poses \(P\), and produces calibrated image \(C\) and its embedded latent \(\) for conditioning in diffusion denoising. The calibrated latent \(\) guides the conditioning module (_i.e._, cross-attention) in denoising UNet with precise correspondence about human shapes between the reference and the target poses. To perform this, the TPC follows the sequential process of \(R C\), and it comprises two main modules: (1) Procrustes Warping (Sec 4.1) and (2) Iterative Propagation (Sec 4.2). Using the input reference image \(R\) and target poses \(P\), Procrustes warping produces a calibrated reference image \(C\) optimized to align the human shape with each target pose. After embedding this calibrated image \(C\) into calibrated latent feature \(\), Iterative Propagation iteratively refines the \(\) during denoising step to enhance temporal consistency among the features by applying our designed feature propagation method. This calibrated latent \(\) is finally given to diffusion model's conditioning module (_i.e._, cross-attention) as a condition by concatenating with the original latent feature \(\) of the reference image.

### Procrustes Warping

Procrustes Warping (PW) aims to align human shapes between reference and target pose. Thus, the PW takes a reference image \(R\) as input and produces a calibrated reference image \(C=[C_{1},,C_{L}]\) to the target human pose \(P=[P_{1},,P_{L}]\). To construct \(C\), we first extract keypoint sets from both the reference and target humans. Then, we apply Procrustes analysis 4 between the two sets, determining transformation parameters: scaling, rotation, and translation. Using these parameters, we transform the reference image to align it with the target. To be specific, as shown in Figure 6 (a), we define 17 keypoints 5 in 2-dimensional space for human body and face using keypoint extractor (_e.g._, OpenPose ). Figure 6 (b) illustrates that we obtain these keypoints from the reference image \(R\) and \(i\)-th target pose frame \(P_{i}\). From these, we filter out commonly visible \(N\) points, defining \(X=\{x_{1},,x_{N}\}\) as reference set and \(Y=\{y_{1},,y_{N}\}\) as target set. Following Procrustes analysis (_i.e._, Eq. (1,2)), we obtain optimal transformation parameters \(s^{*}^{1},r^{*}^{2 2},t^{*}^{1  2}\) and warp all pixels \([u,v]^{n 2}\) (\(n\) is the number of pixels) in the reference image by mapping as below:

\[:[u,v] s^{*}[u,v] r^{*}+t^{*}. \]

Therefore, we obtain the \(i\)-th calibrated image \(C_{i}=f_{}^{P_{i}}(R)\) using Procrustes warping \(f_{}^{P_{i}}\), which aligns \(R\) with human shape in \(P_{i}\) using mapping \(\). However, considering all \(N\) common visible points for warping is unreasonable since the reference and target shapes cannot perfectly overlap due to differing poses (_e.g._, the arms in Figure 6 (b) cannot overlap). To address this, we apply Procrustes warping to subset \( X\) of the common points and select the most effective subset \(^{*}\) based on our defined alignment score function \(h\) as below (subset \(\) is also defined corresponding to the \(\)):

\[^{*}=}{}\ h(P_{i},C_{i}^{}), \]

where \(C_{i}^{}\) denotes calibrated image using keypoints of subset \(\). The \(h\) is alignment score function that computes pixel-wise IoU6 of human shapes between pose \(P_{i}\) and calibrated image \(C_{i}^{}\). Thus, the final \(i\)-th calibrated image is defined as \(C_{i}=C_{i}^{^{*}}\) with optimal subset \(^{*}\).

Figure 5: Illustration of Test-time Procrustes Calibration (TPC) on diffusion-based human image animation. TPC provides calibrated latent feature \(\) to enhance shape correspondence between the reference image and target poses. Procrustes Warping aligns the reference image with the target pose shape, while Iterative Propagation improves temporal consistency among calibrated features.

Figure 6: Illustration of (a) pre-defined 17 keypoints and (b) keypoints in reference and target human, their common point set, and optimal set.

### Iterative Propagation

Conceptually, as shown in Figure 5, our proposed TPC aims to mitigate compositional misalignment by feeding calibrated image latent features \(=[_{1},,_{L}]\) into the conditioning module (_i.e_., cross-attention) of denoising diffusion along with the original reference latent feature \(\).7 Thus, the \(\) is designed to bridge the correspondence of the human shapes between the reference image and target pose. However, the pose variation within the target pose frames affects the degree of calibration with the reference image, which reduces temporal consistency in the output. To address this, we introduce Iterative Propagation (IP) to enhance consistency among calibrated latent features. Figure 7 illustrates the IP process during the diffusion denoising. The IP forms \(M\) groups of sequential features in calibrated features \(\), randomly selects a feature within each group, and updates all features in the group with the selected one. This method enhances temporal consistency among calibrated latent features while maintaining compositional alignment with the target pose due to the continuity of target pose variation. The random selection of features ensures all features have an equal chance of being chosen during the denoising process.

### Plug-and-Play Test-time Procrustes Calibration

We integrate calibrated latent feature \(\) into human image animation systems by applying it into a conditioning module (_i.e_., cross-attention) of video diffusion UNet. For \(i\)-th frame \(m\) patch-wise image feature \(_{i}^{m d}\), reference feature \(^{m d}\), and calibrated latent feature \(_{i}^{m d}\) the cross attention is formulated as \(Q(QK^{T}/d)V\), where it satisfies \(Q=_{i}\) and \(K=V=[,_{i}]^{2m d}\) is concatenated latent condition.

## 5 Experiments

### Experimental Settings

Implementation Details.SAM  is used for screening out the background in calibrated images. VQ-VAE  is used for encoding images of the video. The number of groups in iterative propagation is chosen as \(M=30\) under ablation studies in Table 2. The average number of video frames is about 120. We use Stable Diffusion 1.5  for all baselines on 4 NVIDIA A100 GPUs. We follow the same pose encoders and image encoders of baseline models.

Data and Baselines.We validate human image animation on two popular benchmarks (_i.e_., TikTok , TED-talks ) about a test split. Due to no validation splits, we provide valid sets matching the test set sizes for the ablation study. We further collected 114 samples 8 from TikTok and TED-talks as another test split. These samples contain compositional misalignment about rotation and scaling between human shapes in reference images and motion videos. The criteria for misalignment include relative angle and scaling differences, with samples having a relative angle \(>/6\) or relative scale IoU < 0.7. As shown in Figure 1 (a), the relative angle measures an angle between the straight lines from the pelvis to the nose in humans of reference and target poses using keypoints estimator . The relative scaling measures the IoU (Intersection of Union) between bounding boxes of humans in the reference and the target. Procrustes Calibration is validated on recent diffusion-based human image animation models including MagicAnimate , DisCo , AnimateAnyone9, DreamPose  on their public codes and papers.

Figure 7: Illustration of iterative propagation on calibrated latent features. It shows \(M\) groups on \(L\) frame calibrated features and updates features in each group with randomly selected ones during the denoising process.

### Evaluation Metrics

We evaluate videos in terms of single-frame quality and video quality. For the single-frame, we measure Peak Signal-to-Noise Ratio (PSNR) , Structural Similarity Index Measure (SSIM) , Learned Perceptual Image Patch Similarity (LPIPS) , FID (Frechet Image Distance) , and L1 error between output and ground-truth images. For the video quality, we measure Frechet Video Distance (FVD)  and FID-VID . All automatic metrics are averaged over 10 runs with different seeds. We also analyze human preferences for results from the baselines with and without our TPC.

### Experimental Results

Quantitative Comparisons.Table 1 presents evaluations of image animation on two benchmark datasets (_i.e._, TikTok, TED-talks) using recent diffusion image animation baselines (MagicAnimate, DisCo, AnimateAnyone, DreamPose) with TPC across three assessments (_i.e._, image, video, human). Evaluations are conducted on two test splits: the original set and the compositional misalignment set. Initially, all baselines use a single reference image latent for conditioning all poses. After integrating TPC, they use calibrated latents corresponding to each pose. Consistent improvements in image and video quality are observed in both test sets. All baselines struggle with the compositional misalignment set, but when integrated with TPC, they achieve quality close to the original test set. This demonstrates that morphological similarity affects the current diffusion model's ability to map human shapes from reference to target.

Qualitative Comparisons.To validate our proposed TPC, we applied it to four recent baselines and compared the original results. We used the same types of human pose inputs (e.g., OpenPose, DensePose) for each baseline to prepare target motion videos. Figure 8 shows predictions on four different samples exhibiting compositional misalignment. The top left results show predictions on temporal misalignment between the reference and target due to the target's bending motion. MagicAnimate's fidelity diminishes with increased bending (red box in the last frame), whereas the model with TPC maintains high fidelity. The top right results display predictions on temporal misalignment due to a walking motion towards the front, with TPC similarly enhancing DisCo's performance. The bottom left results show consistent misalignment across all frames. DreamPose struggles with low fidelity, causing unwanted stripes on the pants (red box), which are clearly removed with TPC. The results in the bottom right exhibit consistent misalignment due to a scale difference. In AnimateAnyone, the reference human's yellow pants were incorrectly mapped onto the target human's arms, making them appear yellow. However, the TPC completely mitigates this incorrect mapping. This occurs because the calibrated image filters out the pants and enhances the correspondence of each body part. (Please, refer to the calibrated images also in the Appendix.)

    &  &  \\   & \(L1\)E.04 & PSNR \(\) & SSIM \(\) & LPIPS \(\) & FID \(\) & FID-VID \(\) & FVD \(\) \\   \\  DreamPose & 7.229.64 & 27.31/25.17 & 0.532/0.481 & 0.449/0.529 & 55.487/2.6 & 61.1/193.1 & 568/738 & 0.04 \\ DreamPose + TPC & 5.155/3.31 & 28.47/28.01 & 0.620/0.613 & 0.406/0.412 & 48.44/9.3 & 54.7/56.3 & 426/441 & 0.96 \\  DisCo & 4.09/5.23 & 28.43/24.97 & 0.641/0.512 & 0.312/0.492 & 37.1/71.4 & 58.3/82.1 & 339/522 & 0.28 \\ DisCo + TPC & 3.493/8.28 & 29.02/82.87 & 0.689/0.673 & 0.283/0.294 & 34.3/56.2 & 51.2/25.8 & 281/297 & 0.72 \\  A-Anyone & 3.774/8.82 & 29.06/26.52 & 6.070/0.584 & 0.289/0.392 & 32.96/5.2 & 54.2/59.1 & 296/442 & 0.24 \\ A-Anyone + TPC & 3.323/5.53 & 29.27/29.01 & 0.705/0.688 & 0.264/0.273 & 31.3/03.2 & 48.7/49.3 & 254/269 & 0.76 \\  M-Animate & 3.174/3.46 & 29.11/27.82 & 0.717/0.641 & 0.241/0.321 & 31.84/9.2 & 22.4/52.3 & 182/362 & 0.34 \\ M-Animate + TPC & **2.98**/3.19 & **29.43**/29.21 & **0.753**/0.731 & **0.232**/0.249 & **29.2**/30.4 & **21.0**/21.9 & **158**/164 & 0.66 \\   \\  DreamPose & 6.657/4.41 & 27.63/26.11 & 0.559/0.482 & 0.421/0.521 & 63.48/6.26 & 43.6/64.2 & 411/532 & 0.24 \\ DreamPose + TPC & 6.176/3.61 & 28.11/28.03 & 0.593/0.589 & 0.393/0.404 & 53.25/5.2 & 38.2/38.9 & 369/372 & 0.76 \\  DisCo & 3.526/3.91 & 28.51/26.71 & 0.661/0.511 & 0.309/0.451 & 34.5/71.1 & 28.2/52.6 & 332/471 & 0.24 \\ DisCo + TPC & 3.183/2.3 & 28.93/28.87 & 0.704/0.692 & 0.283/0.294 & 31.83/82.6 & 25.1/25.8 & 289/304 & 0.76 \\  A-Anyone & 3.126/6.61 & 28.93/27.07 & 0.712/0.613 & 0.267/0.361 & 29.3/54.2 & 20.3/39.5 & 192/372 & 0.24 \\ A-Anyone + TPC & 2.81/2.91 & 29.31/29.25 & 0.753/0.742 & 0.254/0.269 & 27.8/28.6 & **18.8**/19.5 & 173/178 & 0.76 \\  M-Animate & 2.92/4.31 & 29.17/27.47 & 0.734/0.661 & 0.239/0.312 & 25.7/47.1 & 20.2/39.1 & 136/331 & 0.24 \\ M-Animate + TPC & **2.77**/2.87 & **29.52**/29.41 & **0.782**/0.764 & **0.230**/249 & **24.3**/25.6 & 19.2/20.4 & **128**/137 & 0.76 \\   

Table 1: Quantitative evaluations of Test-time Procrustes Calibration (TPC) with recent diffusion-based human image animation models. A-Anyone: AnimateAnyone, M-Animate: MagicAnimate. It is reported in a format of (original test set / compositional misalignment test set).

Figure 10 shows the results on reference images from an unseen domain, generated by the T2I model , applied in a compositional misalignment scenario. The left displays a temporal misalignment sample due to bending motions, with improved fidelity in both baselines, especially in DisCo. The right side displays a consistent misalignment sample, where the baselines show low precision in transferring identity when the reference is flipped upside down. However, with the integration of TPC, their performance is significantly improved.

Robustness analysisTo assess the robustness of baselines with TPC, we measured fidelity (PSNR) by varying the scale and rotation of the human reference, as shown in Figure 9. In the case of scaling, the baselines show a drop in performance when the size difference between the reference and target human shapes falls below an IoU of 0.6. However, TPC maintains optimal performance even down to an IoU of 0.4. Notably, TPC enhances baseline robustness to all rotational variations by ensuring the calibrated image aligns well with shape differences caused by rotation.

Figure 8: Qualitative results about applying TPC on diffusion-based human image animation systems (_i.e._, MA: MagicAnimate, DisCo, AA: AnimateAnyone, DP: DreamPose) on cases about compositional misalignment of human shape between a reference image and target motion: Temporal misalignment by motion affecting factor rotation (top left) and scale (top right). Consistent misalignment affecting rotation (bottom left) and scale (bottom right). Calibrated images and predictions of compositional aligned samples are available in Appendix. Please see also video in the supplementary.

Figure 9: Robustness analysis of variations in compositional misalignment. Measurements of scale and rotation are in Figure 1 (b).

Ablation Study.Table 2 presents ablation studies on transformation methods and iterative propagation. Transformations are categorized as shape-preserving (_e.g_., Linear, Procrustes) and shape-distorting (_e.g_., Affine). Linear transformation performs scaling and rotation without translation, based on the object's bounding box coordinates. The Procrustes method is the most effective, transforming by selecting the optimal keypoint. The affine transform is less effective due to shearing, which can cause distortion and information loss by moving the subject out of the frame, as shown in Figure 11. The second section presents an ablation study on iterative propagation, showing significant enhancements in foreground and background, with \(M\)=30 being the most effective. Table 3 provides inference time on baselines with TPC. Our method requires only minimal additional time because it processes all frames batch-wise, generating the calibrated image in a single iteration.

### Broader Impacts and Ethic Statements

Visual generative models present a range of ethical dilemmas, including the creation of unauthorized counterfeit content, potential privacy breaches, and challenges related to fairness. Due to our reliance on the architecture of these models, our work inherently adopts these ethical vulnerabilities. Addressing these concerns is imperative and requires the establishment of comprehensive regulations and technical countermeasures. We are exploring advanced measures such as learning-based digital forensics and digital watermarking to ethically navigate the complexities of visual generative models.

    &  &  \\   & SSIM\(\) & FVD\(\) & SSIM\(\) & FVD\(\) \\  Linear & 0.702 & 191 & 0.751 & 170 \\ Affine & 0.704 & 193 & 0.754 & 171 \\ Processes & **0.734** & **162** & **0.782** & **142** \\  w/o IP & 0.709 & 184 & 0.728 & 162 \\ w/ IP (\(M\)=20) & 0.731 & 164 & 0.776 & 145 \\ w/ IP (\(M\)=30) & **0.734** & **162** & **0.782** & **142** \\ w/ IP (\(M\)=40) & 0.728 & 165 & 0.777 & 145 \\   

Table 2: Ablation studies on transformation methods for reference image calibration and iterative propagation (IP) on TED-talks and TiKTok. (validation splits, average score compositional alignment/misalignment).

Figure 11: Comparisons of calibrated images using an affine transform and Procrustes transform.

Figure 10: Qualitative results about unseen domain reference image. TPC enhances human image animation models’ robustness on samples about compositional misalignment in the unseen domain.

### Limitation and Future work

We provide an overview of the various limitations and potential development directions identified during this study. Human image animation systems transfer reference images to target poses. However, some frames in a target pose video may lack proper specification, leading to flicker or low fidelity in model predictions. Thus, integrating high-quality pose estimation is crucial. Another limitation is that significant differences in body shape between the reference and target poses result in awkward transfers, such as transferring a skinny person to a fat target pose. To achieve natural results, a system or module that aligns body shapes is required. For the future extension of human image animation system, it is essential to ensure robust operations under multiple individuals. The system should ensure consistent identity transfer across multiple individuals and accurately adapt to those appearing at specific times. To achieve this, video technologies about recognition  and perception  can be further incorporated. Currently, conditioning relies solely on images, but it is anticipated that various other modalities, such as text and audio, could also be incorporated. Especially, the integration of the text modality with emerging large language models  is expected to drive innovative industrial advancements, with the introduction of video/image-based conversational systems  serving as a compelling utility. Lastly, integrating technologies focused on speed , resource efficiency , and test-time calibration  is expected to improve human image animation systems' applicability into real-world environments.

## 6 Conclusion

Current diffusion-based human image animation systems are facing challenges on samples of compositional misalignment of human shapes between a reference image and target pose frames. To this end, this paper presents Test-time Procrustes Calibration (TPC) which improves the robustness of image animation models on the compositional misalignment samples in a model-agnostic manner. Extensive experiments demonstrate the effectiveness of TPC.