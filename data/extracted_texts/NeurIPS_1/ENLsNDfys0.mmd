# Novel Object Synthesis via Adaptive

Text-Image Harmony

 Zeren Xiong\({}^{1}\), Zedong Zhang\({}^{1}\), Zikun Chen\({}^{1}\), Shuo Chen\({}^{2}\),

**Xiang Li\({}^{3}\)**, **Gan Sun\({}^{4}\)**, **Jian Yang\({}^{1}\)**, **Jun Li\({}^{1}\)**

\({}^{1}\)School of Computer Science and Engineering,

Nanjing University of Science and Technology, Nanjing, 210094, China

\({}^{2}\)RIKEN, \({}^{3}\)College of Computer Science, Nankai University, Tianjing, 300350, China

\({}^{4}\)College of Automation Science and Engineering,

South China University of Technology, Guangzhou, 510640, China

{zandyz,csjyang,junli}@njust.edu.cn, {xzr3312,zikunchencs,sungan1412}@gmail.com

xiang.li.implus@nankai.edu.cn, shuo.chen.ya@riken.jp

Corresponding author

###### Abstract

In this paper, we study an object synthesis task that combines an object text with an object image to create a new object image. However, most diffusion models struggle with this task, _i.e._, often generating an object that predominantly reflects either the text or the image due to an imbalance between their inputs. To address this issue, we propose a simple yet effective method called Adaptive Text-Image Harmony (ATIH) to generate novel and surprising objects. First, we introduce a scale factor and an injection step to balance text and image features in cross-attention and to preserve image information in self-attention during the text-image inversion diffusion process, respectively. Second, to better integrate object text and image, we design a balanced loss function with a noise parameter, ensuring both

Figure 1: We propose a straightforward yet powerful approach to generate combinational objects from a given object text-image pair for novel object synthesis. Our algorithm produces these combined object images using the central image and its surrounding text inputs, such as _glass jar_ (image) and _porcupine_ (text) in the left picture, and _horse_ (image) and _bald eagle_ (text) in the right picture.

optimal editability and fidelity of the object image. Third, to adaptively adjust these parameters, we present a novel similarity score function that not only maximizes the similarities between the generated object image and the input text/image but also balances these similarities to harmonize text and image integration. Extensive experiments demonstrate the effectiveness of our approach, showcasing remarkable object creations such as _colobus-glass jar_ in Fig. 1. Project Page.

## 1 Introduction

Image synthesis from text or/and image using diffusion models such as Stable Diffusion , SDXL , and DALL-E3  has gained considerable attention due to their impressive generative capabilities and practical applications, including editing [6; 75] and inversion [24; 61]. Many of these methods focus on object-centric diffusion, utilizing textual descriptions to manipulate objects within images through operations like composition , addition [36; 15], removal , replacement , movement , and adjustments in size, shape, action, and pose . In contrast, we study an object synthesis task that creates a new object image by combining an object text with an object image. For instance, combining _kingfisher_ (image) and _terrier_ (text) results in a new and harmonious terrier-like kingfisher object, as shown in the right-side of Fig. 2.

To implement object text-image fusion, most diffusion models, such as SDXL-Turbo , often use cross-attention  to integrate the input text and image. However, the cross-attention frequently results in imbalanced outcomes, as evidenced by the following observations. On the left side of Fig. 2, when inputting an _axolot_ (image) and a _toucan_ (text), SDXL-Turbo only generates an image of a toucan, showing a bias towards the toucan text (green circles). Conversely, when inputting a _rooster_ (image) and an _iron_ (text), it produces an image of a rooster, which closely resembles the original rooster image (orange circles). These observations reveal that the text (or image) feature often suppresses the influence of the image (or text) feature during the diffusion process, leading to a failed fusion. To mitigate the image degeneration, Plug-and-Play  can inject the guidance image features into self-attention. Unfortunately, even with the application of the best inversion editing method, PnPinv , which incorporates the plug-and-play inversion into diffusion-based editing methods for improved performance, we still observe similar imbalances, as shown on the right-side of Fig. 2. This arises an important problem: _how can we balance object text and image integration?_

To address this problem, we propose an **A**daptive **T**ext-**I**mage **H**armony (**ATIH**) method for novel object synthesis, as shown in Fig. 3. First, during the inversion diffusion process, we introduce a scale factor \(\) to balance text and image features in cross-attention, and an injection step \(i\) to preserve image information in self-attention for adaptive adjustment. Second, the inverted noise maps adhere to the statistical properties of uncorrelated Gaussian white noise, which increases editability . However, they are preferable for approximating the feed-forward noise maps, thereby enhancing fidelity. To better integrate object text and image, we treat sampling noise as a parameter in designing

Figure 2: **Imbalances between text and image in diffusion models.** Using SDXL-Turbo  (left) and PnPinv  (right), the top pictures show a tendency for generated objects to align with textual content (green circles), while the bottom pictures tend to align with visual aspects (orange circles). In contrast, our approach achieves a more harmonious integration of both object text and image.

a balanced loss function, which strikes a balance between reconstruction and Gaussian white noise approximation, ensuring both optimal editability and fidelity of the object image. Third, we present a novel similarity loss that considers both \(i\) and \(\). This loss function not only maximizes the similarities between the generated object image and the input text/image but also balances these two similarities to harmonize text and image integration. Furthermore, we employ the _Golden Section Search_ algorithm to quickly find the optimal parameters \(\) and \(i\). Therefore, our ATIH method is capable of generating novel object combinations. For instance, an _iron-like rooster_ is produced by merging the image _rooster_ with the text _iron_, resulting in a rooster image with an iron texture, as shown in Fig. 2.

Overall, our contributions can be summarized as follows: **(1)** To the best of our knowledge, we are the first to propose an adaptive text-image harmony method for generating novel object synthesis. The key idea is to achieve a balanced blend of object text and image by adaptively adjusting a scale factor and an injection step in the inversion diffusion process, ensuring their effective harmony. **(2)** We introduce a novel similarity score function that incorporates the scale factor and injection step. This aims to balance and maximize the similarities between the generated image and the input text/image, achieving a harmonious integration of text and image. **(3)** Experimental results on PIE-bench  and ImageNet  demonstrate the effectiveness of our method. Our approach shows superior performance in creative object combination compared to state-of-the-art image-editing and creative mixing methods. Examples of these creative objects, such as _sea lion-glass jar_, _African chameleon-bird_, and _corgi-cock_ are shown in Figs. 1, 6, and 8.

## 2 Related Work

**Text-to-Image Generation**  The rapid development of generative models based on diffusion processes has advanced the state-of-the-art for tasks [12; 21; 33] like text-to-image synthesis [22; 31], image editing [64; 2], and style transfer [65; 23; 35]. Large-scale models such as Stable Diffusion , Imagen , and DALL-E  have demonstrated remarkable capabilities. Sdklturbo  introduced a distillation method that further enhances efficiency by reducing the steps needed for high-quality image generation. Our method utilizes Sdklurbo for adaptive and innovative object fusion, preserving the original image's layout and details while requiring only the textual description of the target object.

**Text Guided Image Editing.** Diffusion models have garnered significant attention for their success in text-to-image generation and text-driven image editing using natural language descriptions. Early studies [1; 54; 70; 40], such as SDEdit , balanced authenticity and fidelity by adding noise, while Prompt2Prompt  and Plug-and-Play (PNP)  enhanced editing through attention mechanisms. Further research, including MasaCtrl , Instructpix2pix , and InfEdit , explored non-rigid editing, specialized image editing models, and rapid editing via consistency sampling. Advances in image inversion and reconstruction  have focused on diffusion-based denoising process inversion, categorized into deterministic and non-deterministic sampling . Deterministic methods, such as Null-text inversion using DDIM sampling , precisely recover original images but require lengthy optimization; non-deterministic methods, such as DDPM inversion  and CycleDiffusion , achieve precision by storing variance noise. PnPinv  simplifies the process by accurately replacing latent features during denoising, achieving perfect reconstruction but with weaker editability.We propose a framework for creative object synthesis using object textual descriptions for effective fusion and a regularization technique to enhance PnPinv editability.

**Object Composition.** Compositional Text-to-Image synthesis and multi-image subject blending methods [37; 19; 58; 70; 59] aim to create novel images by integrating various concepts, including object interactions, colors, shapes, and attributes. Numerous methodologies [8; 71; 24; 52; 55] have been developed focusing on object combinations, context integration, segmentation, and text descriptions. However, these methods often merely assemble components without effectively melding inter-object relationships, resulting in compositions that, while accurate, lack deeper integration and interaction. This limitation is particularly evident in image editing, where multiple objects in a single image fail to achieve cohesive synthesis. Our method addresses this by harmoniously fusing two objects to create novel entities, thereby enhancing creativity and imagination.

**Semantic Mixing.** The breadth of creativity spans diverse fields, from scientific theories to culinary recipes, driving advancements in AI as highlighted by scholars  and recent researchers . This creativity has led to significant innovations in AI, particularly through generative models. Creative Adversarial Networks  push traditional art boundaries, producing norm-defying works while maintaining artistic connections. Efforts to adapt AI for novel engineering designs  further exemplify this technological creativity. MagicMix  introduced semantic mixing task,unlike traditional style transfer methods [73; 60; 10] which blending two concepts into a photo-realistic object while retaining the original image's layout and geometry, but often resulting in biased images and less harmonious fusion. ConceptLab  uses diffusion models to generate unique concepts, like new types of pets, but requires time-consuming optimization and struggles to semantically blend real images. Our method operates at the attention layer of diffusion models for harmonious semantic fusion and proposes an adaptive fast search to quickly produce balanced, fused images, ensuring novel and cohesive integration of semantic concepts.

## 3 Methodology

Let \(O_{I}\) and \(O_{T}\) be an object image and an object text, respectively, used as inputs for diffusion models. Our goal is to create a novel object image \(O\) by combining \(O_{I}\) with \(O_{T}\) during the diffusion process. To achieve this goal, we develop an adaptive text-image harmony (ATIH) method in our object synthesis framework, as shown in Fig. 3. In subsection 3.1, we introduce a text-image diffusion model with a scale factor \(\), an injection step \(i\) and noise \(_{t}\). In subsection 3.2, we present to optimize the noise \(_{t}\) to balance object editability and fidelity. In subsection 3.3, we propose a simple yet effective ATIH method to adaptively adjust \(\) and \(i\) for harmonizing text and image.

### Text-Image Diffusion Model (TIDM)

Here, we construct a Text-Image Diffusion Model (TIDM) by utilizing the pre-trained SDXL Turbo . The key components include dual denoising branches: inversion for inverting the input object image, and fusion for fusing the object text and image. Following the latent diffusion model , the input latent codes are defined as \(z_{0}=(O_{I})\) for object image \(O_{I}\) and \(=(O_{T})\) for object text \(O_{T}\), using a pre-trained image/text encoder \(()\). \(_{N}=(O_{N})\) denotes as a null-text embedding. The **latent denoising process** is described as follows:

_Inversion Denoising._ The inversion denoising process predicts the latent code at the previous noise level, \(_{t-1}\), based on the current noisy data \(_{t}\). This process is defined as:

\[_{t-1}=_{t}_{t}+_{t}_{}(_{t},t,)+_{t}_{t}, \]

where \(_{t}\), \(_{t}\) and \(_{t}\) are sampler parameters, \(_{t}\) is sampled noise, and \(_{}(_{t},t,)\) is a pre-trained U-Net model  with self-attention and cross-attention layers. The self-attention is implemented as:

\[(^{s}_{t},^{s}_{t},^{s} _{t})=^{s}_{t}^{s}_{t},\ \ \ ^{s}_{t}=(^{s}_{t}(^{s}_{t})^{T}/ ), \]

where \(^{s}_{t}\), \(^{s}_{t}\) and \(^{s}_{t}\) are the query, key and value features derived from the representation \(_{t}\), and \(d\) is the dimension of projected keys and queries. The cross-attention is to control the synthesis process

Figure 3: **Framework of our object synthesis incorporating a scale factor \(\), an injection step \(i\) and noise \(_{t}\) in the diffusion process. We design a balance loss for optimizing the noise \(_{t}\) to balance object editability and fidelity. Using the optimal noise \(_{t}\), we introduce an adaptive harmony mechanism to adjust \(\) and \(i\), balancing text (_Peacock_) and image (_Rabbit_) similarities.**

through the input null-text embedding \(_{N}\), implemented as follows: \((_{t}^{c},_{t}^{c},_{t}^{c })=_{t}^{c}_{t}^{c}\), where \(_{t}^{c}=(_{t}^{c}(_{t}^{c })^{T}/)\), \(_{t}^{c}\) is the query feature derived from the output of the self-attention layer, \(_{t}^{c}\) and \(_{t}^{c}\) are the key and value features derived from \(_{N}\).

_Fusion Denoising._ Similar to the inversion denoising branch, we redefine the self-attention and cross-attention for easily adjusting the balance between the image latent code \(z_{t}\) and the text embedding \(\). The fusion denoising process is redefined as:

\[z_{t-1}=_{t}z_{t}+_{t}_{}(z_{t},t,,,i)+_ {t}_{t}, \]

where \(_{t}\), \(_{t}\), \(_{t}\) and \(_{t}\) are defined as Eq. (1), and \(_{}(z_{t},t,,,i)\) is also the pre-trained U-Net model  with injected self-attention and scale cross-attention layers. The **injected self-attention** with an adjustable injection step \(i(0 i T)\) is implemented as:

\[(M_{t}^{s},V_{t}^{s})=M_{t}^{s} V_{t}^{s}, \;\;\;M_{t}^{s}=_{t}^{s},&\;\;t>i\\ (Q_{t}^{s}(K_{t}^{s})^{T}/),& , \]

where \(Q_{t}^{s}\), \(K_{t}^{s}\) and \(V_{t}^{s}\) are the query, key and value features derived from the representation \(z_{t}\). Unlike the approach of injecting \(_{t}^{s}\) and \(_{t}^{s}\) from Eq. (2) into \(K_{t}^{s}\) and \(V_{t}^{s}\) in MasaCtrl , we focus on adjusting the injection step \(i\) by injecting \(_{t}^{s}\) from Eq. (2) into \(M_{t}^{s}\). The **scale cross-attention** with an adjustable factor \(\) is to control the synthesis process through the input text embedding \(\), implemented as follows:

\[(Q_{t}^{c},K_{t}^{c},V_{t}^{c})=M_{t}^{c}  V_{t}^{c},\;\;\;M_{t}^{c}=(Q_{t}^{c}(K_{t}^{c })^{T}/), \]

where \(Q_{t}^{c}\) is the query feature derived from the output of the self-attention layer, \(K_{t}^{c}\) and \(V_{t}^{c}\) are the key and value features derived from the text embedding \(\). Unlike the non-adjustable scale attention map approach in Prompt-to-Prompt , we introduce a factor, \(\), to adjust the value feature. This allows for better balancing of the text and image features, even though they share the same form. Using this fusion denoising process, the generation of a new object image is denoted as \(O\).

Following the ReNoise inversion technique , based on the denoising Eq. (1) and the approximation \(_{}(_{t},t,)_{}(_ {t-1},t,)\), the **noise addition process** is reformulated as:

\[_{t}^{}=(_{t-1}^{}-_{t}_{ }(_{t}^{},t,)-_{t}_{t})/_{t}. \]

### Balance fidelity and editability by optimizing the noise \(_{t}\) in inversion process

In this subsection, our goal is to achieve better fidelity and editability of the object image during the inversion process. We observe that increasing the Gaussian white noise of the denoising latent code \(_{t-1}\) can enhance editability , while reducing the difference between the denoising latent code \(_{t-1}\) and the standard path noise code \(_{t-1}^{}\) in Eq. (6) can improve fidelity [25; 67]. However, these two objectives are contradictory. To address this, we treat the sampling noise \(_{t}\) in Eq.(1) as a learnable parameter. We define a reconstructed \(_{2}\) loss between \(_{t-1}\) and \(_{t-1}^{}\), \(_{r}(_{t})=\|_{t-1}^{}-(_{t} _{t}+_{t}_{}(_{t},t,)+_{t}_{t })\|\), and a KL divergence loss between \(_{t}\) and a Gaussian distribution, \(_{n}(_{t})=(q(_{t})||p((0,I)))\), to simultaneously handle fidelity and editability. Based on Eqs.(1) and (6), we design a balance loss function as follows:

\[(_{t})=|_{r}(_{t})-_{ n}(_{t})|, \]

where \(\) represents the weight to balance \(_{r}\) and \(_{n}\), and in this paper, we set to \(=_{r}}{_{n}}=125\). Since the parameter \(_{t}\) is sampled from a standard Gaussian distribution during the noise addition process, \(_{n}\) is used solely to balance \(_{r}\) and its gradient is not computed for optimization.

### Text-image harmony by adaptively adjusting injection step \(i\) and scale factor \(\)

Using the optimal noise \(_{t}\), a fused object image \(O(,i)\) can be generated by the TIDM with an initial scale factor \(_{0}=1\) and injection step \(i_{0}= T/2\) from the input object image \(O_{I}\) and object text \(O_{T}\). Here, we adaptively adjust \(\) and \(i\) (\(0 i T\)) by introducing an Adaptive Text-Image Harmony (ATIH) method. We denote the similarity between the image \(O_{I}\) and the fused image \(O(,i)\) as \(I_{}(,i)=d(O_{I},O(,i))\), and the similarity between the text \(O_{T}\) and the fused image \(O(,i)\) as \(T_{}(,i)=d(O_{T},O(,i))\), where \(d(,)\) represents the similarity distance between text/image and image. In this paper, we compute the similarities \(I_{}(,i)\) and \(T_{}(,i)\) using the DINO features  and the CLIP features , respectively, based on a cosine distance \(d\). Our key idea is to balance and maximize both \(I_{}(,i)\) and \(T_{}(,i)\) for optimal text-image fusion.

**Adjust injection step \(i\) to balance fidelity and editability.** Before achieving the idea, we first enable the object image to be smoothly editable by adjusting the injection step \(i\) in the injected self-attention. We denote \(I_{}(i)=I_{}(_{0},i)\) for for convenience. In the inversion process, it is generally observed that more injections lead to less editability. When all injections are applied (\(i=T\)), an ideal fidelity is achieved. We observe that when \(I_{}(i)<I_{}^{}\), the fused image deviates significantly from the input image, resulting in a loss of fidelity. Conversely, when \(I_{}(i)>I_{}^{}\), the fused image is too similar to the input image, resulting in no editability. To balance fidelity and editability, \(I_{}(i)\) must satisfy \(I_{}^{} I_{}(i) I_{}^{}\), in Fig. 5. Therefore, initializing \(i= T/2\), \(i\) is adjusted as follows:

\[i=i-1,&I_{}(i)<I_{}^{}\\ i,&I_{}^{} I_{}(i) I_{}^{}\\ i+1,&I_{}(i)>I_{}^{}, \]

where \(I_{}^{}\) and \(^{}\) are set to \(0.45\) and \(0.85\) in this paper, respectively, based on observations from Fig. 17. After using Eq. (8), this adaptive approach can obtain an injection step \(i^{*}\) to smooth the fusion process while maintaining a balance between fidelity and editability. Fixing the injection step \(i=i^{*}\), next we use abbreviations, \(I_{}()=I_{}(,i^{*})\) and \(T_{}()=T_{}(,i^{*})\).

**Adaptively adjust the scale factor \(\) for harmonizing text and image.** To implement our key idea, we design an exquisite score function with \(\) as:

\[_{}F():=}()+k T_{}()}_{}-}()-k T_{}( )|}_{}, \]

where \(\) is a weighting factor, and the parameter \(k\) is introduced to mitigate inconsistencies in scale between high \(I_{}()\) and low \(T_{}()\) due to differences in text and image modalities, ensuring their scale balance. As shown in Fig. 4, \(I_{}()\) decreases and \(T_{}()\) increases as \(\) increases, and vice versa. Based on these observations, we set \(k=2.3\) and \(=1\) in this paper.

In Eq. (9), the left-hand side represents the sum of the text and image similarities, forming an ellipse, while the right-hand side represents the absolute value of the difference between the text and image similarities, forming a hyperbola. A larger sum value indicates that the generated image integrates more information from the input text and image. Conversely, a smaller absolute value signifies a better balance between the text and image similarities. Additionally, given that \(I_{}()\) and \(T_{}()\), their sum is greater than or equal to the absolute value of their difference, leading to \(F() 0\). Therefore, our objective is to maximize \(F()\) to simultaneously enhance and balance both \(I_{}()\)and and \(T_{}()\). Maximizing \(F()\) is easily implemented by the Golden Section Search  algorithm, and we get the optimal \(^{*}\). Fig. 5 depicts a schematic diagram to adjust both ii and \(\). Overall, our **novel object synthesis**, detailed in **Algorithm 1**, is presented in Appendix F.

## 4 Experiments

### Experimental Settings

**Datasets.** We constructed an object text-image fusion (OTIF) dataset consisting of 1,800 text-image pairs, derived from 60 texts and 30 images in Appendix C. Images, selected from various classes in PIE-bench , include 20 animal and 10 non-animal categories. Texts were chosen from the 1,000 classes in ImageNet , with ChatGPT  filtering out 40 distinct animals and 20 non-animals.

**Details.** We implemented our method on SDXLturbo  only taking _ten seconds_. For image editing, we set the source prompt \(p_{s}\) as an empty string "Null" and the target prompt \(P_{t}\) as the target object class name. During sampling, we used the Ancestral-Euler sampler  with four denoising steps. All input images were uniformly scaled to \(512 512\) pixels to ensure consistent resolution in all the experiments. Our experiments were conducted using two NVIDIA GeForce RTX 4090 GPUs.

**Metrics.** To comprehensively evaluate the performance of our method, we employed four key metrics: aesthetic score (AES) , CLIP text-image similarity (CLIP-T) , Dinov2 image similarity (Dino-I) , and human preference score (HPS) . Following the Eq. (9), \(F\)score and balance similarities (\(B\)sim) with \(k=2.3\) are used to measure the text-image fusion effect.

### Main Results

We conducted a comprehensive comparison of our ATIH model with three image-editing models (_i.e._, MasaCtrl , InfEdit , and InstructPix2pix ), two mixing models (_i.e._, MagicMix  and ConceptLab ), and ControlNet . Notably, MagicMix and ConceptLab share a similar objective with ours to fuse object text/image, while ConceptLab only accepts two text prompts as its inputs. Due to no available code for MagicMix, we utilized its unofficial implementation .

Figure 6: **Comparisons with different image editing methods.** We observe that InfEdit  MasaCtrl  and InstructPix2Pix  struggle to fuse object images and texts, while our method successfully implements new object synthesis, such as _bowling ball-fawn_ in the second row.

**Comparisons with image-editing methods.** For a fair comparison, we uniformly set the editing text prompt in all methods as _a photo of an [image category] creatively fused with a [text category]_ to achieve the fusion of two objects. Fig. 6 visualizes some combinational objects, with additional results available in Appendix H. Our observations are as follows: Firstly, MasaCtrl and InfEdit generally preserve the original image's details better during editing, as seen in examples like _sheep-triceratops_. In contrast, InstructPix2Pix tends to alter the image more significantly, making it closer to the edited text description. Secondly, different methods exhibit varying degrees of distortion when fusing two objects during the image editing process. For instance, in the case of _African chameleon-bird_, our method performs better by minimizing distortions and maintaining the harmony and high quality of the image. Thirdly, our method shows significant advantages in enhancing the editability of images. For the _European fire salamander-glass jar_ example, other methods often result in only color changes and slight deformations, failing to effectively merge the two objects. In contrast, our method harmoniously integrates the colors and shapes of both the glass jar and the European fire salamander, significantly improving the editing effect and operability. Specially, Fig. 7 shows the results of InstructPix2Pix with manually adjusted image strengths (\(1.0,1.5,2.0\)) and text strengths (ranging from \(1.0\) to \(1.5\)).

Figure 8: **Comparisons with different creative mixing methods.** We observe that our results surpass those of MagicMix . For ConceptLab , we exclusively examine its fusion results without making good or bad comparisons, as it is a distinct approach to creative generation.

Figure 7: Comparisons with InstructPix2Pix  using image/text strength variations.

from \(1.5\) to \(7.5\)). At optimal settings of image strength \(1.5\) and text strength \(5.0\), InstructPix2Pix produced its best fusion, though some results were unnatural, like replacing the rabbit's ears with a rooster's head. In contrast, our method created novel and natural combinations of the rabbit and rooster by automatically achieving superior visual synthesis without manual adjustments.

**Comparisons with the mixing methods.** Fig. 8 illustrates the results of text-image object synthesis. We observe that both MagicMix and ConceptLab tend to overly bias towards one class, such as _zucchini-owl_ and _corgi-cock_. Their generated images often lean more towards one category. In contrast, our method achieves a more harmonious balance between the features of the two categories. Moreover, the fusion images produced by MagicMix frequently exhibit insufficiently smooth feature blending. For instance, in the fusion of a rabbit and an emperor penguin, the rabbit's facial features nearly disappear. Conversely, our method seamlessly merges the facial features of both the penguin and the rabbit in the head region, preserving the main characteristics of each.

**Comparisons with ControlNet.** We rigorously compared our method with ControlNet to assess their performance in complex text-image fusion tasks, as shown in Fig. 9. Our results highlight notable differences: ControlNet preserves structure well from depth or edge maps but struggles with semantic integration, especially with complex prompts, often failing to achieve seamless blending. In contrast, our method leverages full RGB features, including color and texture, alongside structural data.

**Quantitative Results.** Table 1 displays the quantitative results, illustrating that our method achieves state-of-the-art performance in AES, HPS, \(F\)score and \(B\)sim, surpassing other methods. These results indicate that our approach excels in enhancing the visual appeal and artistic quality of images, while also aligning more closely with human preferences and understanding in terms of object fusion. Moreover, when dealing with text-image inconsistencies at scale \(k\)=2.3, our method achieves superior text-image similarity and balance, demonstrating superior fusion capability. Despite achieving the best DINO-I and CLIP-T scores under inconsistencies, InfEdit and InstructPix2Pix perform worse than our method in terms of AES, HPS, \(F\)score and \(B\)sim, and their visual results remain sub-optimal. These inconsistencies ultimately lead to the failure of integrating object text and image. In contrast, our approach achieves a better text-image balance similarities. Furthermore, Table 2 presents the \(H\)-statistics  and \(P\)-values  assessing the statistical significance of performance differences between our ATIH and other methods across various metrics. Compared to Instructpix2pix, for

   Models & DINO-I7  & CLIP-T  & AES  & HPS  & \(F\)score\(\) & \(B\)sim \\ 
**Our ATIH** & 0.756 & 0.296 & **6.214** & **0.383** & **1.362** & **0.075** \\ MagicMix  & 0.587 & 0.328 & 5.786 & 0.373 & 1.174 & 0.167 \\ InfEdit  & **0.817** & 0.255 & 6.080 & 0.367 & 1.173 & 0.230 \\ MasaCtrl  & 0.815 & 0.234 & 5.684 & 0.343 & 1.077 & 0.277 \\ InstructPix2Pix  & 0.384 & **0.394** & 5.881 & 0.375 & 0.768 & 0.522 \\   

Table 1: Quantitative comparisons on our TIF dataset.

   Methods & DINO-I7  & CLIP-T  & AES  & HPS  & \(F\)score\(\) & \(B\)sim \\  MagMixMix  & 665.20 (\(1.10^{-116}\)) & 248.15 (\(6.56^{-10}\)) & 433.00 (\(6.16^{-10}\)) & 232.1 (\(1.45^{-10}\)) & 633.89 (\(7.13^{-16}\)) & 792.72 (\(2.06^{-11}\)) \\ InfEdit  & 402.63 (\(1.68^{-10}\)) & 477.31 (\(8.22^{-10}\)) & 3.70 (\(0.45^{-11}\)) & 1140.2 (\(2.26^{-10instance, our method shows significant differences, with \(H\)-statistics of 268.57 for AES and 39.63 for HPS, indicating potential improvements in both aesthetic quality and human preference scoring.

**User Study.** We conducted two user studies to assess intuitive human perception of results presented in Table 3, Table 4, and Appendix G. Each participant evaluated 6 image-editing sets and 6 fusion sets. In total, these studies garnered 570 votes from 95 participants. Our method received the highest ratings in both studies, capturing 74.03% and 79.47% of the total votes, respectively. Among the image-editing methods, InfEdit  garnered 14.7% of votes for its superior editing performance, while InstructPix2Pix  and MasaCtrl  received only 8% and 2.8%, respectively. In the fusion category, ConceptLab  received 12.28% of votes, while MagicMix  received 8%.

### Parameter Analysis and Ablation Study

**Parameter analysis.** Our primary parameters include \(\) in (7), \(I_{}^{}\) and \(I_{}^{}\) in (8), and \(k\) in (9). \(=_{e}}{L_{n}}\) balances the editability and fidelity of our model. We determined the specific value through personal observation combined with the changes in AES, CLIP-T, and Dino-I values at different \(\) settings. Ultimately, we set \(\) to 125. To address the inconsistency between image similarity and text similarity scales, we approximated the scale \(k\). Initially, we measured the variations in image similarity and text similarity with changes in \(\), and identified the balanced similarity regions in the fusion results. As shown in Fig. 4, the optimal range for \(k\) was found to be between [0.21, 0.27]. Based on these observations and experimental experience, we ultimately set \(k\) to 0.23. As shown in Fig. 17 of Appendix D, we observe that when the similarity between the image and the original exceeds 0.85, the images become too similar, making edits with different class texts less effective and necessitating a decrease in \(i\). Conversely, when the similarity is below 0.45, the images overly favor the text, making them excessively editable, requiring an increase in injection steps. Therefore, we set \(I_{}^{}\) to 0.45 and \(I_{}^{}\) to 0.85. More discussions are provided in Appendix D.

**Ablation Study.** In Figs. 10 and 18 in Appendix E, we visualize the results with and without the balance loss in Eq. (7), the adaptive injection ii in Eq. (8), and the adaptive selection \(\) in Eq. (9) within our object synthesis framework. PnPinv, used for direct inversion and prompt editing, resulted in some distortion and blurriness. Compared to PnPinv, the balance loss significantly enhances image fidelity, improving details, textures, and editability. The adaptive injection enables a smooth transition from Corgi to Fire Engine in Fig. 18. Without this injection, the transformation is too abrupt, lacking a seamless fusion process. Finally, the adaptive selection achieves a balanced image that harmoniously integrates the original and target features. _Note that for limitations, please refer to Appendix B._

## 5 Conclusion

In this paper, we explored a novel object synthesis framework that fuses object texts with object images to create unique and surprising objects. We introduced a simple yet effective difference loss to optimize sampling noise, balancing image fidelity and editability. Additionally, we proposed an adaptive text-image harmony module to seamlessly integrate text and image elements. Extensive experiments demonstrate that our framework excels at generating a wide array of impressive object combinations. This capability is particularly advantageous for crafting innovative and captivating animated characters in the entertainment and film industry. Broader impact, please see Appendix A.

  Models & Our ATIH & MasaCtrl & InstructPix2Pix & InfEdit \\  Vote \(\) & **422** & 16 & 48 & 84 \\  

Table 3: User study with image editing methods.

Figure 10: Ablation study of the balance loss, adaptive injection ii and adaptive selection \(\) from the third column to the fifth column.