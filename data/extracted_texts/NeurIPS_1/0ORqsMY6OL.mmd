# Improved Communication Efficiency in Federated Natural Policy Gradient via ADMM-based Gradient Updates

Improved Communication Efficiency in Federated Natural Policy Gradient via ADMM-based Gradient Updates

 Guangchen Lan

Purdue University

West Lafayette, IN 47907

lan44@purdue.edu

Han Wang

Columbia University

New York, NY 10027

hw2786@columbia.edu

James Anderson

Columbia University

New York, NY 10027

anderson@ee.columbia.edu

Christopher Brinton

Purdue University

West Lafayette, IN 47907

cgb@purdue.edu

Vaneet Aggarwal

Purdue University

West Lafayette, IN 47907

vaneet@purdue.edu

###### Abstract

Federated reinforcement learning (FedRL) enables agents to collaboratively train a global policy without sharing their individual data. However, high communication overhead remains a critical bottleneck, particularly for natural policy gradient (NPG) methods, which are second-order. To address this issue, we propose the FedNPG-ADMM framework, which leverages the alternating direction method of multipliers (ADMM) to approximate global NPG directions efficiently. We theoretically demonstrate that using ADMM-based gradient updates reduces communication complexity from \((d^{2})\) to \((d)\) at each iteration, where \(d\) is the number of model parameters. Furthermore, we show that achieving an \(\)-error stationary convergence requires \((})\) iterations for discount factor \(\), demonstrating that FedNPG-ADMM maintains the same convergence rate as the standard FedNPG. Through evaluation of the proposed algorithms in MuJoCo environments, we demonstrate that FedNPG-ADMM maintains the reward performance of standard FedNPG, and that its convergence rate improves when the number of federated agents increases.

## 1 Introduction

Policy gradient methods are commonly used to solve reinforcement learning (RL) problems in various applications, with recent popular examples being InstructGPT  and ChatGPT (GPT-4 ). Although first-order methods, such as Policy Gradient (PG) and Proximal Policy Optimization (PPO) , are favored for their simplicity, second-order methods, such as Natural Policy Gradient (NPG)  and its practical version Trust Region Policy Optimization (TRPO) , are often seen to exhibit superior convergence behavior, e.g., on the Swimmer  and Humanoid  tasks in MuJoCo environments . Furthermore, recent works have demonstrated that the convergence guarantees of NPG with Kullback-Leibler (KL) divergence constraints (second-order methods) are superior to those of PG (first-order methods) , motivating closer inspection for practical use.

Many contemporary RL applications are large-scale and rely on high volumes of data for model training . A conventional approach to enabling training on massive data is transmitting data collected locally by different agents to a central server, which can then be used for policy learning. However, this is not always feasible in real-world systems where communication bandwidthis limited and long delays are not acceptable [19; 28], such as in edge devices , Internet of Things [5; 27], autonomous driving , and vehicle transportation [3; 11]. Moreover, sharing individual data collected by agents raises privacy and legal issues [15; 26].

Federated learning (FL) [12; 25] offers a promising solution to the challenges posed by data centralization, where agents communicate locally trained models rather than raw datasets. However, FL is typically applied to supervised learning problems. Recent work has expanded the scope of FL to federated reinforcement learning (FedRL), where \(N\) agents collaboratively learn a global strategy without sharing the trajectories they collected during agent-environment interaction [14; 17; 50]. Federated reinforcement learning has been studied in tabular RL [2; 14; 17], control tasks [47; 48] and for value-function based algorithms [46; 50], where linear speedup has been demonstrated. For policy-gradient based algorithms, we note that linear speedup is easy to see as the trajectories collected by each agent could be parallelized.

Efficient state-of-the-art guarantees for federated policy gradient based approaches can be achieved by Federated NPG (FedNPG), which is a second-order method. However, sharing second-order information increases the communication complexity, which is one of the fundamental challenges in FL [7; 42; 37]. In supervised FL, works including FedNL , BL , Newton-Star/Learn  and FedNew  have been recently proposed to reduce the communication complexity of second-order methods, typically by approximating Hessian matrices for convex or strongly convex problems. However, federated second-order _reinforcement learning_ has not been investigated, which provides the key motivation for our work. The key question that this paper aims to address is:

_Can we reduce the communication complexity for second-order federated natural policy gradient approach while maintaining performance guarantees?_

We answer this question in the affirmative by introducing FedNPG-ADMM, an algorithm that estimates global NPG directions using alternating direction method of multipliers (ADMM) [8; 9; 45]. This estimation reduces the communication complexity from \((d^{2})\) to \((d)\), where \(d\) is the number of model parameters. However, it is non-trivial to see whether FedNPG-ADMM will maintain similar convergence guarantees as FedNPG. We show in this work that FedNPG-ADMM indeed does maintain these guarantees, and provides a speedup with the number of agents. The key contributions that we make are summarized as follows:

1. We propose a novel federated NPG algorithm, FedNPG-ADMM, where the global NPG directions are estimated through ADMM.
2. Using the ADMM-based global direction estimation, we demonstrated that the communication complexity reduces by \((d)\) as compared to transmitting the second-order information (standard FedNPG).
3. We prove the FedNPG-ADMM method achieves an \(\)-error stationary convergence with \((})\) iterations for discount factor \(\). Thus, it achieves the same convergence rate as the standard FedNPG.
4. Experimental evaluations in MuJoCo environments demonstrate that FedNPG-ADMM maintains the convergence performance of FedNPG. We also show improved performance as more federated agents engage in collecting trajectories.

## 2 Background

Markov Decision Process:We consider the Markov decision process (MDP) as a tuple \(,,,,\), where \(\) is the state space, \(\) is a finite action space, \(:\) is a Markov kernel that determines transition probabilities, \(:\) is a reward function, and \((0,1)\) is a discount factor. At each time step \(t\), the agent executes an action \(a_{t}\) from the current state \(s_{t}\), following a stochastic policy \(\), i.e., \(a_{t}(|s_{t})\). For on policy \(\), a state value function is defined as

\[V_{}(s)=}_{a_{t}(|s_{t}), \\ _{t}+1(|s_{t},a_{t})}[_{t=0}^{} ^{t}r(s_{t},a_{t})|s_{0}=s]. \]Similarly, a state-action value function (Q-function) is defined as

\[Q_{}(s,a)=}_{(|s_{t})} }{{s_{t+1} P|s_{t},a_{t}}}}[_{t=0}^{}^{t}r (s_{t},a_{t})|s_{0}=s,\ a_{0}=a]. \]

An advantage function is then define as \(A_{}(s,a)=Q_{}(s,a)-V_{}(s)\). With continuous states, the policy is parametrized by \(^{d}\), and then the policy is referred as \(_{}\) (Deep RL parametrizes \(_{}\) by deep neural networks). A state-action visitation measure induced by \(_{}\) is given as

\[_{_{}}(s,a)=(1-)}_{s_{0}}[ _{t=0}^{}^{t}P(s_{t}=s,\ a_{t}=a|s_{0},\ _{})], \]

where starting state \(s_{0}\) is drawn from a distribution \(\). The _goal_ of an agent is to maximize the expected discounted return defined as

\[J()=}_{s}[V_{_{}}(s)]. \]

The gradient of \(J()\) can be written as :

\[_{}J()=}_{}[_{t=0}^{} _{}_{}(a_{t}|s_{t})A_{_{}}(s_{ t},a_{t})], \]

where \(=(s_{0},a_{0},s_{1},a_{1},)\) is a trajectory induced by policy \(_{}\). We denote the policy gradient by \(\) for short. In practice, we can sample \((s,a)^{_{^{k}}}\) and obtain the unbiased estimate \(_{_{^{k}}}(s,a)\) using Algorithm 3 in .

Natural Policy Gradient (NPG):At the \(k\)-th iteration, natural policy methods with a trust region  update policy parameters as follows

\[^{k+1}=_{}}_{s,a} [(a|s)}{_{^{k}}(a|s)}A_{_{^{k}}}(s,a)] \] \[\ (\|^{k}).\]

where

\[(\|^{k})=}_{s}[D_{ }(|s)\|_{^{k}}(|s)], \]

\(D()\) is the KL-divergence operation, and \(>0\) is the radius of the trust region. Practically, using the first-order Taylor expansion for the target value and the second-order Taylor expansion for the divergence constraint, (6) is expanded as follows

\[^{k+1}=_{}\ ^{}( -^{k}) \] \[\ (-^{k})^{}( -^{k}),\]

where \(=_{}^{2}(\|^{k})^ {d d}\), and the individual elements are given by \(_{ij}=}}}_{s}[D_{}(|s)\|_{ ^{k}}(|s)]_{=^{k}}\). Using Lagrangian duality, the iterates of NPG ascent are expressed as

\[^{k+1}=^{k}+^{}^{-1 }}}^{-1}. \]

Federated NPG:FedNPG is a paradigm in that \(N\) agents collaboratively train a common global policy with parameters \(\) as illustrated in Figure 1 (a). During the training process, each agent computes gradients (and second-order matrices) using its _local data_; then, the gradients of all agents are transmitted to a central server. In particular, one FedNPG training iteration consists of the following three steps:* Downlink Transmission: The server broadcasts the current global policy parameters \(^{d}\) to all \(N\) agents.
* Uplink Transmission: Collecting its own local data \(_{i}\) based on the common policy \(_{}\), each agent \(i\) computes its local gradient \(_{i}^{d}\) and second-order matrix \(_{i}^{d d}\). Then, it sends \(_{i}\) and \(_{i}\) back to the server.
* Global Update: The server averages local gradients and second-order matrices to get the global gradient and matrix as follows: \[_{i=1}^{N}_{i},\ \ _{i=1}^{N}_{i}.\] (10) The server then updates global policy parameters as \[+^{}^{-1} }}^{-1}.\] (11) We use \(|_{i}|\) to denote the size of collected data set \(_{i}\). Without loss of generality, we take \(|_{1}|==|_{N}|\) for simplicity. As proven in , gradient update methods are _immune_ to whether collected data is i.i.d., or not.

Bottleneck of Federated NPG:As shown in Figure 1, at each iteration, the server collects \(\{_{i}^{d d},\ _{i}^{d}\}_{i=1}^{N}\) from \(N\) agents and updates global policy parameters as follows

\[+^{N}_{i})^{ }(_{i=1}^{N}_{i})^{-1}_{i=1}^{N}_{i}}}(_{i =1}^{N}_{i})^{-1}_{i=1}^{N}_{i}. \]

We call this method a standard average FedNPG. The uplink communication complexity from each agent is \((d^{2})\) in each iteration round. As the uplink is highly limited , applying (12) is not practical when \(d\) is large.

In the next section, we introduce our ADMM-based approach as shown in Figure 1 (b), which reduces the communication complexity to \((d)\) at each iteration and meanwhile keeps convergence performances.

## 3 FedNPG via ADMM

To minimize the communication overhead in each round of communication, we begin by formulating a quadratic problem. The solution to this problem provides the updating direction in (12), as follows:

Figure 1: An illustration of federated learning based on second-order methods with \(N\) agents. (a) FedNPG via standard average. In the uplink, transmitting the matrix \(_{i}\) brings \((d^{2})\) communication complexity. (b) FedNPG-ADMM in this paper with only \((d)\) communication complexity.

\[(_{i=1}^{N}_{i})^{-1}_{i=1}^{N}_{i}=*{ arg\,min}_{}\ ^{}(_{i=1}^{N}_{i})- ^{}_{i=1}^{N}_{i}. \]

This minimization problem is equivalent to

\[_{,\{_{i}\}_{i=1}^{N}}& _{i=1}^{N}_{i}^{}_{i} _{i}-_{i}^{}_{i}+\|_ {i}-\|^{2}\\ &=_{i}, i=1, ,N, \]

where \(>0\) is a penalty constant and \(\|\|\) denotes the Euclidean norm. This equivalence transforms the original quadratic problem into a distributed manner and is the key process of efficient FedNPG.

**Remark**: This approach does not aim to approximate the Hessian from each agent, but approximates the global direction \((_{i=1}^{N}_{i})^{-1}_{i=1}^{N}_{i}\) directly. Note that this differentiates our approach from the work mentioned in the introduction.

The Lagrangian function associated to optimization problem (14) is

\[(,\{_{i}\}_{i=1}^{N},\{_{i}\}_{i=1}^{N}) =_{i=1}^{N}_{i}^{}_{i} _{i}-_{i}^{}_{i}+\|_ {i}-\|^{2}+_{i},_{i}-, \]

where \(\{_{i}^{d}\}_{i=1}^{N}\) are dual variables. Next, we solve the distributed optimization problem through the alternating direction method of multipliers (ADMM) . The policy update of FedNPG via one-step ADMM is given in Algo. 1.

```
0: MDP \(,,,,\); Number of timesteps \(T\); Penalty constant \(\); Step size \(\); Initial \(_{0}^{d}\), \(^{0}^{d}\), \(\{^{0}_{i}=^{0}\}_{i=1}^{N}\), \(\{_{i}^{d}\}_{i=1}^{N}\).
1:for\(k=1,,K\)do
2:\(\) Server broadcast
3:Broadcast \(^{k-1}\) and \(^{k-1}\) to \(N\) agents.
4:\(\) Agent update
5:for each agent \(i\{N\}\)do in parallel
6:\(_{i}_{i}+(_{i}^{k-1}-^{k-1})\)
7:\(_{i}^{k}_{i}|}_{t=_{ i}}_{t=0}^{T}_{^{k-1}}_{^{k-1}}(a_{t}|s_{t}) _{_{^{k-1}}}(s_{t},a_{t})\)
8:\(_{i}^{k}(_{i}^{k}+)^{-1}(_{i}^{k}-_{i}+^{k-1})\)
9: Transmit \(_{i}^{k}^{d}\) and \(_{i}^{k}^{d}\) to the server.
10:endfor
11:\(\) Server update
12:\(^{k}_{i=1}^{N}_{i}^{k}\)
13:\(^{k}^{k-1}+^{N} _{i}^{k})^{}^{k}}}^{k}\)
14:endfor
15:\(^{K}\)
```

**Algorithm 1** FedNPG-ADMM

Agent \(i\) computes \(_{i}\) and \(_{i}\) based on locally collected data. At each step of ADMM, agent \(i\) updates \(_{i}\) in line 8 as follows

\[_{i}&=*{ arg\,min}_{_{i}}_{i}^{}_{i} _{i}-_{i}^{}_{i}+\| _{i}-+}{}\|^{2}\\ &}}}}{{=}}( _{i}+)^{-1}(_{i}-_{i}+), \]

where \(^{d d}\) is the identity matrix. In practical implementation, conjugate gradient methods can be used to compute \(_{i}\) for efficiency (Appendix C in ).

In line 12, after receiving \(_{i}\) and \(_{i}\) from all \(N\) agents, the server updates the global search direction as follows

\[ =*{arg\,min}_{}_{i=1}^{N}\|_{i}-\|^{2}+_{i},_{i}-  \] \[=_{i=1}^{N}(_{i}+}{ })}}}{{=}}_{i=1}^{N} _{i}.\]

Dual variables in line 6 are updated by each agent as follows

\[_{i}_{i}+(_{i}-),  i=1,\ ,\ N. \]

Combining (17) and (18), we have \(_{i=1}^{N}_{i}=0\).

In line 13 of Algo. 1, after the ADMM process, the server updates global policy parameters, where \((0,1)\) is the step size. The server then broadcasts the updated parameters to all \(N\) agents at the next iteration.

In every communication round, agent \(i\) only transmits \(_{i}\) and \(_{i}\), with a communication complexity of \((d)\). In contrast, the standard average approach in (12) requires transmitting \(_{i}\) and \(_{i}\), with a communication complexity of \((d^{2})\). This efficient communication approach allows second-order methods scalable to large-scale systems.

## 4 Convergence Analysis

In this section, we derive the convergence rate of FedNPG based on ADMM. In order to derive the guarantees, we make the following standard assumptions  on policy gradients, second-order matrices, and rewards.

**Assumption 4.1**.:
1. The score function is bounded as \(\|_{}_{}(a s)\| G,\ \ ^{d}\), \(s\), and \(a\).
2. Policy gradient is \(M\)-Lipschitz continuous. In other words, \(_{i},_{j}^{d},\ s,\ \ a\), we have \[_{_{i}}_{_{i}}(a s)-_{_{j}} _{_{j}}(a s) M\|_{i}-_{j}\|.\] (19)
3. The reward function is bounded as \(r(s,a)[0,R],\ \ s\), and \(a\).

**Assumption 4.2**.: For all \(^{}\), the Fisher information matrix induced by policy \(_{}\) and initial state distribution \(\) is positive definite as

\[F()=}_{(s,a)_{_{}}}_{ }_{}(a s)_{}_{}(a s)^{ }_{F} \]

for some constant \(_{F}>0\). For any two symmetric matrices with the same dimension, \(A B\) denotes the eigenvalues of \(A-B\) are greater or equal to zero.

**Theorem 4.3**.: _For a target error \(\) of stationary-point convergence, each agent samples \((N})\) trajectories, and the server obtains the update direction \(^{k}\) at each iteration. Choose \(=^{2}}{4G^{2}(56G^{2}+L_{J})}\) and_

\[K=-J(^{1}))(56G^{2}+L_{J})^{2}16G^{2}+28G^{2}_{F}^{3}}{(5 6G^{2}+L_{J}-56G^{2}_{F})_{F}^{2}}=(}). \]

_We have:_

\[_{k=1}^{K}[ J(^{k} )^{2}]. \]Proof sketch.The main idea in our proof is to show that the approximation error between the updating direction given by FedNPG-ADMM and NPG geometrically decreases up to some additional term, which depends on the statistical error. This term appears since we do not have access to the exact gradient. However, it decays at a rate proportional to sample size. As a result, FedNPG-ADMM achieves the same convergence rate as NPG as shown by constructing an appropriate Lyapunov function. The complete proof of Theorem 4.3 is given in Appendix A.

By the definition of stationary points, we need to find a parameter \(\) such that \(\| J()\|^{2}\), for all \(>0\). The result in (22) achieves the stationary-point convergence for policy gradient methods as provided in . Our approach keeps the sample complexity as that in the NPG method  and thanks to the federated scenario we consider, enjoys a much lower communication complexity.

We summarize the complexity improvement in Table 1. Recall in (5) that the estimated gradient \(\) is an average over collected trajectories. The total trajectories \(_{i=1}^{N}|_{i}|\) are collected by \(N\) agents equally using the common global policy \(_{}\) at each iteration. Each agent \(i\) samples \(|_{i}|=(N})\) at each iteration and enjoys a federated sampling benefit compared to a single agent with \(|_{i}|=(})\). During the whole training process, each agent \(i\) has \(K|_{i}|=(N^{2}})\) sample complexity and \(K 2d=(})\) communication complexity to achieve a stationary convergence.

## 5 Simulations

**Setup**: We consider three MuJoCo tasks  with the MIT License, which have continuous state spaces. Specifically, a Swimmer-v4 task with small state and action spaces, a Hopper-v4 task with middle state and action spaces, and a Humanoid-v4 task with large state and action spaces are considered as described in Table 3. Policies are parameterized by fully connected multi-layer perceptions (MLPs) with settings in Table 4. We follow the practical settings in TRPO with line search , and in stable-baselines3  with generalized advantage estimation (\(0.95\))  and the Adam optimizer  in our implementation. Convergence performances are measured over \(10\) runs with random seeds from \(0\) to \(9\). The solid lines in Figure 2 and 3 are averaged results, and the shadowed areas are confidence intervals with \(95\%\) confidence level. We use PyTorch  to implement deep neural networks and RL algorithms. The tasks are trained on NVIDIA RTX 3080 GPU with \(10\) GB of memory.

**Performance metrics**: We consider performance metrics as follows:

1. Communication overhead: the data size transmitted from each agent;
2. Rewards: the average trajectory rewards across the batch collected at each iteration;
3. Convergence: rewards versus iterations during the training process.

We first evaluate the influence of the number of federated agents. In Figure 2, with different numbers of agents, we test the convergence performances of standard average FedNPG in (12) with \((d^{2})\) communication complexity and FedNPG-ADMM in Algo. 1 with \((d)\) communication complexity at each iteration. The x-axis denotes the number of iterations in federated learning. Both algorithms converge faster, have lower variance, and achieve higher final rewards when more agents are engaged to collect trajectories. Compared to the standard average FedNPG, FedNPG-ADMM does not only reduce communication complexity, but also keeps convergence performances. The hyperparameter and MLP settings are described in Table 4. We summarize the final reward results with standard deviations in Table 2. FedNPG-ADMM achieves similar final rewards compared to the standard average FedNPG. It also works with slightly high variance when only one agent is engaged.

   & NPG  & FedNPG & FedNPG-ADMM \\  Sample complexity & \((^{2}})\) & \((N^{2}})\) & \((N^{2}})\) \\ Communication complexity & - & \((}{(1-)^{2}})\) & \((})\) \\  

Table 1: Complexity comparison in each agent.

In Figure 3, we compare the performances of the standard average FedNPG, FedNPG-ADMM, and first-order FedPPO algorithms. The number of federated agents \(N\) is fixed to \(8\). PPO clipping parameter is set as \(0.2\). FedNPG methods outperform FedPPO in these tasks with faster convergence and higher final rewards. It is noticeable that FedNPG-ADMM has similar convergence rates and achieves slightly higher final rewards than the standard average FedNPG for the Swimmer-v4 task, while it becomes slightly lower for the Humanoid-v4 task. Generally, there is no significant difference in convergence rates after ADMM approximation. The communication overhead is measured by the number of transmitted parameters with double precision in each agent. FedNPG-ADMM keeps the communication overhead as the first-order methods, while FedNPG has much higher costs. The ADMM method reduces the cost by \(4\) orders of magnitude in the Swimmer-v4 task and about \(6\) orders of magnitude in the Humanoid-v4 task.

   &  &  &  &  \\   & FedNPG & \(111.9 5.4\) & \(119.5 3.4\) & \(122.1 3.6\) & \(124.8 2.4\) \\  & FedNPG-ADMM & \(109.4 5.9\) & \(123.6 10.3\) & \(127.2 2.2\) & \(128.5 1.9\) \\   & FedNPG & \(1644 396\) & \(2468 426\) & \(2458 171\) & \(2736 158\) \\  & FedNPG-ADMM & \(1473 383\) & \(2384 371\) & \(2507 230\) & \(2719 173\) \\  

Table 2: Final rewards in federated settings.

Figure 2: Reward performances of standard average FedNPG and FedNPG-ADMM on MuJoCo tasks, where \(N\) is the number of federated agents. **Top:** Swimmer-v4, **Bottom:** Hopper-v4. **Left:** FedNPG with \((d^{2})\) communication complexity, **Right:** FedNPG-ADMM with \((d)\) communication complexity.

  Hyperparameter &  \\  Task & Swimmer-v4 & Hopper-v4 & Humanoid-v4 \\ MLP & \(64 64\) & \(128 128\) & \(512 512 512\) \\ Activation function & ReLU & ReLU & ReLU \\ Output function & Tanh & Tanh & Tanh \\ Penalty (\(\)) & \(0.1\) & \(0.1\) & \(0.01\) \\ Radius (\(\)) & \(0.01\) & \(0.01\) & \(0.01\) \\ Discount (\(\)) & \(0.99\) & \(0.99\) & \(0.99\) \\ Timesteps (\(T\)) & \(2048\) & \(1024\) & \(512\) \\ Iterations (\(K\)) & \(1 10^{3}\) & \(2 10^{3}\) & \(3 10^{3}\) \\ Learning rate & \(3 10^{-4}\) & \(3 10^{-4}\) & \(1 10^{-5}\) \\  

Table 4: Hyperparameter and MLP settings.

Figure 3: Comparisons of FedPPO, standard average FedNPG, and FedNPG-ADMM on MuJoCo tasks, where the number of federated agents \(N\) is \(8\) and the communication overhead is measured by the transmitted bytes in each agent. **Left:** Swimmer-v4 task, **Right:** Humanoid-v4 task, **Top:** Reward performances, **Bottom:** Communication overhead.

  Tasks & Agent & Action Dimension & State Dimension \\  Swimmer-v4 & Three-link swimming robot & \(2\) & \(8\) \\ Hopper-v4 & Two-dimensional one-legged robot & \(3\) & \(11\) \\ Humanoid-v4 & Three-dimensional bipedal robot & \(17\) & \(376\) \\  

Table 3: Description of the MuJoCo environment.

In Figure 4, we test performances with agent selection. In the Swimmer task, we randomly select \(75\%\) and \(50\%\) of agents in each iteration, and the performances only drop slightly (final rewards drop less than \(6\%\)). Thus, our proposed method is robust for agents with disconnection in practice.

## 6 Conclusion & Discussion

In this paper, we proposed a novel federated natural policy gradient (NPG) algorithm that estimates global directions using ADMM (FedNPG-ADMM). Our ADMM-based gradient updates significantly reduce communication complexity from \((d^{2})\) to \((d)\), where \(d\) is the number of model parameters. It thus enables second-order policy gradient methods to be used for large-scale federated reinforcement learning problems. We proved that our FedNPG-ADMM algorithm achieves stationary convergence without requiring more samples as compared to standard FedNPG. Furthermore, we empirically showed the improved performances of FedNPG-ADMM in the MuJoCo environments as compared to FedNPG and the first order methods.

Overall, our proposed FedNPG-ADMM algorithm provides a scalable and efficient solution for large-scale federated reinforcement learning problems. Our contributions include a novel direction on policy-based FedRL, a new algorithm, reduced communication complexity, and convergence analysis. We believe that our approach can have a significant impact on a wide range of real-world applications, where large-scale distributed reinforcement learning with communication and privacy constraints is critical.

**Limitations:** (1) While the communication complexity improvement is loosely tied to the privacy advantage, exploring deep connections with differential privacy improvement is open. (2) Partial agent participation is not studied under this framework, in which communication complexity can be further reduced. (3) In experiments, it would be more persuasive to extend the number of federated agents to a larger scale.