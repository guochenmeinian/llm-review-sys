# Chain of Preference Optimization:

Improving Chain-of-Thought Reasoning in LLMs

Xuan Zhang\({}^{*12}\), Chao Du\({}^{11}\), Tianyu Pang\({}^{1}\), Qian Liu\({}^{1}\), Wei Gao\({}^{2}\), Min Lin\({}^{1}\)

\({}^{1}\)Sea AI Lab, Singapore

\({}^{2}\)School of Computing and Information Systems, Singapore Management University

xuanzhang.2020@phdcs.smu.edu.sg; weigao@smu.edu.sg;

{duchao, liuqian, tianyupang, linmin}@sea.com

Work done during Xuan Zhang's associate membership at Sea AI Lab. \({}^{}\)Correspondence to Chao Du.

###### Abstract

The recent development of chain-of-thought (CoT) decoding has enabled large language models (LLMs) to generate explicit logical reasoning paths for complex problem-solving. However, research indicates that these paths are not always deliberate and optimal. The tree-of-thought (ToT) method employs tree-searching to extensively explore the reasoning space and find better reasoning paths that CoT decoding might overlook. This deliberation, however, comes at the cost of significantly increased inference complexity. In this work, we demonstrate that fine-tuning LLMs leveraging the search tree constructed by ToT allows CoT to achieve similar or better performance, thereby avoiding the substantial inference burden. This is achieved through _Chain of Preference Optimization_ (CPO), where LLMs are fine-tuned to align each step of the CoT reasoning paths with those of ToT using the inherent preference information in the tree-search process. Extensive experimental results show that CPO significantly improves LLM performance in solving a variety of complex problems, including question answering, fact verification, and arithmetic reasoning, demonstrating its effectiveness. Our code is available at [https://github.com/sail-sg/CPO](https://github.com/sail-sg/CPO).

## 1 Introduction

Recent advances in large language models (LLMs) have shown that constructing reasoning chains is critical to improving their problem-solving capabilities . A representative method is chain-of-thought (CoT) , which prompts LLMs to generate intermediate reasoning steps, i.e., thoughts, thereby constructing explicit reasoning paths (as depicted in Figure 1(a)). While straightforward and intuitive, recent research observes that CoT can often overlook optimal reasoning paths and exhibit an unconscious style of answering due to its single-path focus . To foster a more deliberate and conscious reasoning style, Yao et al.  propose tree-of-thought (ToT), which generates multiple branching thoughts at each step of the reasoning process and conducts self-evaluation for pruning and planning to search for reasoning paths (as shown in Figure 1(b)). However, despite improving reasoning quality, ToT significantly increases computational complexity, which limits its practical application. This raises the question: Can the strategic depth of ToT be integrated into CoT to enhance its effectiveness while maintaining efficiency?

Existing research has initially provided a positive answer to the above question . A natural strategy is to treat the reasoning path discovered by ToT for each instance as a target for supervision, and then fine-tune LLMs to improve their CoT reasoning abilities . Several methods have been proposed to improve this approach, including using advanced tree-search techniques like MonteCarlo tree-search (MCTS) and employing external reward models [12; 10] for pruning and planning to gather better reasoning paths as supervision. The effectiveness of these approaches is therefore largely dependent on the quality of the best-discovered reasoning path.

In this paper, we identify a limitation in these approaches: they overlook the non-optimal reasoning thoughts generated during the tree-search process, which naturally provides additional preference information. Specifically, ToT inherently generates multiple alternative thoughts at each reasoning step, and pruning is performed according to their evaluated qualities. This tree-search process constitutes a preference over all _intermediate_ thought candidates--thoughts appearing in the best-discovered reasoning path are preferred over those that do not. Moreover, this could shed even more insights than the final best-discovered reasoning path, as non-optimal reasoning paths (and thus preferences) exist at each step in the tree-search.

Inspired by recently developed reinforcement learning from human feedback (RLHF) techniques like direct preference optimization (DPO) , we propose _Chain-of-Preference Optimization_ (CPO) to fully exploit the inherent preference information. Specifically, we construct paired preference thoughts at each reasoning step according to the search tree of ToT and then train LLMs to align with these preferences using the DPO algorithm (as illustrated in Figure 1(c)). The paired preference thoughts are constructed based on the above intuition: at each reasoning step, we categorize thoughts as preferred or dispreferred based on their inclusion in the final paths chosen by ToT. With such preference data, CPO enables LLMs to generate the path preferred by ToT using CoT decoding at inference time.

We conduct extensive experiments to evaluate the effectiveness of CPO. Experiments on seven datasets using LLaMA  and Mistral  as base models demonstrate that CPO is highly effective in teaching LLMs the preferred thoughts of ToT at each reasoning step, leading to an average accuracy improvement of up to \(4.3\%\) compared to the base models. Additionally, the experiments reveal that CPO can achieve comparable or even superior performance to the ToT method, which on average requires more than \(50\) times longer for inference.

## 2 Related Work

Reasoning with LLMs.LLMs have been shown to perform better when prompted to engage in multi-step reasoning [1; 2; 3]. Many studies have focused on improving the generated reasoning paths

Figure 1: Comparison of CoT, ToT, and CPO methods, where each node illustrates a step in the reasoning process, forming coherent language sequences aimed at solving a problem. The highlighted path indicates the chosen reasoning trajectory. In the CoT method, the LLM generates only one new node at each step, and all generated nodes are used to build the final reasoning path. For ToT, the LLM produces \(k\) new nodes at each step, but only the top n-best nodes are kept, with the rest being pruned. In CPO, nodes marked with a trophy represent preferred thoughts, while those marked with numbers are nodes that can be utilized to create preference data. This method uses the search tree structure from ToT to develop paired preference data, subsequently training LLMs to align with these preferences through DPO.

by post-editing  or accessing external knowledge [3; 17]. A distinct approach, more relevant to our interests, transforms the linear reasoning structure into a non-linear format such as a tree or graph [18; 19; 20; 8; 9; 21], which combines thought evaluation with search algorithms like depth-first search (DFS) . Different from our proposed CPO, these methods require searching during inference, which significantly increases latency.

LLM self-improving.Reinforcement learning (RL) has increasingly been applied to LLMs by treating them as RL agents for alignment with human feedback [23; 24; 25; 26]. Recent advances demonstrate the potential of using LLMs for self-generating data to augment fine-tuning processes [27; 28; 29; 30; 31; 32]. For instance, reinforced self-training methods [33; 34; 35; 36; 37] introduce mechanisms to curate new high-quality examples and iteratively enrich the training dataset for enhancing model performance. Nevertheless, these methods typically rely on either an external reward model [35; 34] or labeled data . In contrast, approaches like self-rewarding [29; 38] utilize LLMs themselves to evaluate the generated content, aligning more closely with our method. However, these strategies still require initial seed data [29; 38], necessitating human annotation. Our work differs from previous methods as it does not rely on any ground-truth data, allowing LLMs to self-learn from their own feedback. Additionally, our approach constructs feedback in a chain fashion, focusing on reasoning steps, an aspect overlooked by prior works.

Monte Carlo tree-search for LLMs.Monte Carlo tree-search (MCTS) is a robust algorithm for navigating complex decision-making environments, commonly employed in strategic board games such as AlphaGo [39; 40; 41; 42; 43]. MCTS methodically constructs a search tree, balancing exploration and exploitation, simulates various outcomes, and updates utility estimates based on these simulations. Recent studies have shown that MCTS can enhance the decoding process in LLMs [11; 44; 45; 21; 12]. However, the primary challenge with MCTS is the high latency during inference, particularly in difficult reasoning tasks [46; 47]. While some approaches have attempted to optimize LLMs by leveraging reasoning paths identified through MCTS [11; 12], these methods still rely on labeled data and require separate policy and value models to explore and evaluate potential moves at the tree's leaves. In contrast, our CPO approach eliminates the need for human annotations and simplifies the tuning of LLMs without the necessity for additional models.

## 3 Background

In this section, we formalize our notation and provide a brief overview of key prior knowledge for our method. We denote language sequences by lowercase letters, e.g., \(x\), \(y\), \(z\), to represent a sequence of tokens. The output distribution of an LLM parameterized by \(\) is denoted by \(_{}\).

### Chain-of-Thought Prompting

Chain-of-thought (CoT)  is a method that prompts LLMs to generate a chain of reasoning steps before the final answer, as shown in Figure 1. It introduces a series of intermediate thoughts, denoted as \(z_{1},,z_{n}\), that link an input \(x\) to an output \(y\), where \(n\) is the total number of reasoning steps. For instance, if \(x\) is a combination of demonstration examples and the input question and \(y\) is the final answer, each intermediate thought \(z_{i}\) forms a coherent language sequence representing a part of the overall reasoning path toward the final answer. The demonstration examples consist of a set of CoT demonstrations, which serve as exemplars in the prompting process. The intermediate reasoning thoughts are sequentially sampled from the distribution \(z_{i}_{}(|x,z_{1},,z_{i-1})\) and the output is then derived from \(y_{}(|x,z_{1},,z_{n})\).

### Tree-of-Thought Prompting

Tree-of-thought (ToT)  enables LLMs to explore multiple reasoning paths before answering a given question, as illustrated in Figure 1. This approach models the LLM reasoning task as a search over a tree, where each node represents a thought step in the reasoning path. ToT comprises two main components, both implemented through prompting LLMs: 1) the _thought generator_ and 2) the _state evaluator_. The _thought generator_ constructs several new thoughts for the next step based on the current state. Subsequently, the _state evaluator_ generates scores for each new thought and selects the n-best thoughts for further search. The final result is determined by the search algorithm (e.g., BFS or DFS) applied over the selected thoughts until the reasoning process reaches a conclusion.

### Direct Preference Optimization

Direct preference optimization (DPO) is a method for directly optimizing an LLM to align with preference data , e.g., human feedback [13; 48; 49]. More specifically, RLHF traditionally frames the application of human feedback to enhance the performance of an LLM within the context of an RL problem. However, DPO reformulates the reward modeling and RL fine-tuning phases in RLHF to a single optimization problem. The objective function of DPO aims to maximize the ratio of probabilities for the preferred responses and optimize the LLM to imitate human preferences.

Given the generations \((_{1},_{2})(|x)\) conditioned on input \(x\), these pairs are evaluated and ranked according to specific criteria. Preference data is then constructed from these ranked pairs, denoted by \(_{w}_{l}|x\), where \(_{w}\) and \(_{l}\) denote the preferred (winning) and dispreferred (losing) completions between \(_{1}\) and \(_{2}\), respectively. The DPO objective is formulated as follows:

\[_{}(_{};_{})=-( (_{w}|x)}{_{}(_{w}|x)} -(_{l}|x)}{_{}(_{l}|x)} ), \]

where \(\) is the logistic function, the hyperparameter \(\) regulates the penalty imposed for the deviations from the base reference model \(_{}\).

## 4 Our Method: Chain of Preference Optimization

Unlike previous methods that train LLMs to learn the complete reasoning paths [10; 50; 11; 12], our approach leverages the preferences over thoughts generated at each reasoning step, which are often discarded in prior works. Our key insight is that non-optimal thoughts generated during the tree-search process in ToT provide valuable preference information that can enhance LLM's reasoning ability. A major advantage of our method is that it utilizes this supervision only during training, thereby avoiding high inference latency. Our approach consists of two components: synthesizing the chain of preference thoughts (i.e., the preference thoughts in a chain fashion) and training with the CPO objective.

### Synthesizing the Chain of Preference Thoughts

Our procedure for synthesizing and collecting preference thought pairs closely follows the inference process of ToT . An overview of our method is shown in Figure 2. Specifically, the detailed process is divided into three parts: 1) _thought generation_, which generates multiple thoughts for each

Figure 2: The framework of our CPO method. The left part illustrates the process of generating, evaluating, and pruning thoughts, while the right part demonstrates the collection of preference thoughts. The shaded path represents the final selected reasoning path. Thoughts marked with a trophy indicate preferred data, while sibling nodes without a trophy are marked as dispreferred.

reasoning step; 2) _state evaluation_, which evaluates each thought; and 3) _search and collection_, which finalizes the preference thoughts.

Thought generation.Given a state \(s_{i-1}=[x,z_{1},,z_{i-1}]\) representing a partial solution with the input \(x\) and the sequence of thoughts \([z_{1},,z_{i-1}]\) so far, we sample \(k\) thoughts for the next reasoning step:

\[z_{i}^{(j)}_{}(z_{i}|s_{i-1})=_{}(z_{i}|x,z_{1},,z _{i-1}),j=1,,k. \]

Conditioned on the initial input \(x\), which contains the demonstration examples and the question to be answered, and the previous thoughts \(z_{1},z_{2},,z_{i-1}\), the LLM generates multiple thoughts for the next reasoning step. Specifically, it follows the format of demonstrations, starting with the prefix "Step \(i\)," and samples \(k\) thoughts \(\{z_{i}^{(j)}\}_{j=1}^{k}\). We control the model to pause at the end of \(z_{i}^{(j)}\) by setting the generation of the string "Step \(i+1\)," as the stop criteria.2 As a result, we obtain \(k\) new states \(s_{i}^{(j)}=[x,z_{1},,z_{i-1},z_{i}^{(j)}]\) for \(j=1,,k\).

State evaluation.Given different states \(\{s_{i}^{(j)}\}_{j=1}^{k}\), we utilize the LLM to reason about the states and evaluate their progress toward solving the problem, eliminating the need for an external reward model or human annotations. To evaluate state \(s_{i}^{(j)}\), the input to the LLM includes specific demonstration examples for the evaluation process, the input question \(x\), and all the thoughts in the state (i.e., \([z_{1},,z_{i-1},z_{i}^{(j)}]\)). The LLM follows the format of demonstrations to generate a verbal justification first, followed by a classification result from two classes: likely and impossible. The classification results are then used to assign a score, with likely\(=10\) and impossible\(=1\).

The prompt template used in our evaluation consists of two parts: (1) the general guidelines, and (2) task-specific demonstration examples. To minimize the effects of randomness and bias, we shuffle the order of demonstration examples  and repeatedly sample the generated justification and evaluation results. We then calculate the average score for the state \(s_{i}^{(j)}\). The general guideline prompt for the evaluation is as follows: Evaluate whether the thought helps in partially or directly answering the original question (likely/impossible).

Search and collection.We use BFS with pruning as the search algorithm to select the reasoning paths. After evaluation, we retain the n-best thoughts with the highest evaluation scores and proceed to the next step of generation. When the LLM generates a thought containing "so the final answer is:", the search algorithm concludes and returns the selected paths.

As shown in the right part of Figure 2, after finalizing the reasoning paths, the thoughts within the selected paths are marked as preferred (i.e., winning) thoughts. For each preferred thought at the \(i\)-th step \(z_{i}^{w}\), we construct corresponding dispreferred (i.e., losing) thoughts. First, we identify the parent state \(s_{i-1}^{w}\), which includes all the previous thoughts leading to \(z_{i}^{w}\). Each child thought of \(s_{i-1}^{w}\) that is not included in the selected path is chosen as a dispreferred thought \(z_{i}^{l}\) compared to \(z_{i}^{w}\). This process results in the preference pair (\(z_{i}^{w}\), \(z_{i}^{l}\)) for the state \(s_{i-1}^{w}\). We highlight that the constructed dataset \(\) includes _preference data at every step of the reasoning chain_. This per-step paired preference supervision is usually overlooked in previous methods [11; 12].

### Training with the CPO Objective

Once we have obtained the chain of preference thoughts \(\), we can proceed with optimization. For the \(i\)-th step, given the previous reasoning thoughts \(s_{i-1}^{w}\), the probabilities of generating \(z_{i}^{w}\) and \(z_{i}^{l}\) are denoted as \(_{}(z_{i}^{w}|x,s_{i-1}^{w})\) and \(_{}(z_{i}^{l}|x,s_{i-1}^{w})\), respectively. To optimize the LLM on this pair of preference thoughts, we can directly substitute it into Equation 1:

\[_{i}(_{};_{})=-( {_{}(z_{i}^{w}|x,s_{i-1}^{w})}{_{}(z_{i}^{w}|x,s_{i-1}^ {w})}-(z_{i}^{l}|x,s_{i-1}^{w})}{_{}(z_ {i}^{l}|x,s_{i-1}^{w})}). \]Thus, the objective function for CPO can be formulated as follows:

\[_{}(_{};_{})= _{(x,z_{i}^{w},z_{i}^{w},z_{i-1}^{t},s_{i-1}^{w}) }[_{i}(_{};_{})]. \]

## 5 Experiments

In this section, we empirically validate that CPO improves the reasoning ability of the base model, and uncover several insightful findings.

### Settings

Datasets and evaluation metrics.We focus our research on three types of reasoning tasks: _Question Answering_ (QA), _Fact Verification_, and _Arithmetic Reasoning_. For QA, we conduct experiments on three widely used datasets: Bamboogle , WikiMultiHopQA , and HotpotQA . For fact verification, we use three datasets: Fever , Feverous , and Vitaminc . For arithmetic reasoning, we test on the SVAMP dataset . Our choice of tasks was driven by the performance of the models using the ToT method, which showed improvements in QA, fact verification, and arithmetic reasoning. We use 4-shot prompting for each dataset, with CoT demonstrations manually constructed by previous works . Detailed experimental configurations can be found in Appendix A. For evaluation metrics, we report the accuracy and the average latency of generating the answer per instance. More metrics can be found in Appendix B.

Baselines.To validate the effectiveness of our proposed CPO, we consider the following baselines: 1) CoT , which prompts the LLM to generate a series of reasoning steps before producing the final answer. In our experiments, we use CoT with greedy decoding to assess the model's reasoning capabilities without any tuning. 2) ToT , which requires the LLM to explore multiple reasoning paths via tree search before generating the final answer. We use ToT to select reasoning paths and construct datasets to improve LLM's reasoning ability in the following TS-SFT baseline and our CPO method. 3) TS-SFT , which finds reasoning paths through tree search (i.e., ToT in our implementation) and then uses these paths during the supervised fine-tuning (SFT) process (referred to as SFT in Section 5.3 and 6).

Implementation DetailsOur experiments are based on widely used LLMs, specifically LLaMA2-7B/13B  and Mistral-7B . For efficient fine-tuning, we use Low-Rank Adaptation (LoRA) adapters . In all experiments, we set the regularization controller \(\) to \(0.1\), generate \(10\) new thoughts for each state, and retain the top \(5\) thoughts after pruning at each step of reasoning. The temperature is set to \(0.9\) for SVAMP and \(0.4\) for the other datasets. The learning rates for DPO and SFT are 5e-6 and 1e-5, respectively. We use a batch size (with accumulation) of 32 and optimize the LLM with AdamW . For LoRA, the rank is set to \(8\), and \(\) is set to \(16\). All experiments are conducted on NVIDIA A100 GPUs. The latency reported in Table 1 is based on a single NVIDIA A100 40GB. Both training and inference are performed using the Accelerate  backend. We train the LLMs for 4 epochs with early stopping based on the performance on a randomly sampled validation set. To mitigate the influence of randomness, all experiments are repeated three times with different random seeds, and the average results are reported.

### Overall Results on Reasoning

Table 1 summarizes the performance across various reasoning tasks. We have the following findings:

CPO improves LLM's reasoning ability.As shown in Table 1, CPO enhances the reasoning ability of the base LLM, achieving an average improvement of \(4.3\%\) and a maximum improvement of \(9.7\%\) across all tasks and LLMs compared to the CoT approach. This indicates that CPO effectively improves the LLM's reasoning capabilities. Notably, CPO achieves these improvements without requiring additional human-annotated data, which is particularly beneficial in resource-constrained settings.

CPO has a lower latency than ToT but comparable performance.Although ToT consistently improves performance over CoT, it incurs high latency due to the need to generate and evaluate multiple thoughts at each reasoning step during inference. This process produces numerous tokens, resulting in significant computational and memory overhead . In contrast, CPO shifts this computational burden to the training phase, maintaining the low latency of CoT (i.e., \(57.5\) faster than ToT on average) during inference while providing comparable or superior performance. This demonstrates that our CPO can deliver enhanced reasoning capabilities without compromising efficiency.

CPO surpasses TS-SFT on average.Despite both CPO and TS-SFT using ToT to generate training data (where our implementation of ToT remains consistent), CPO exhibits an average improvement of \(2.7\%\) and reaches a maximum increase of \(10.3\%\). A key factor behind this performance is the CPO's ability to fully utilize ToT's reasoning process. Specifically, CPO effectively leverages both selected and unselected thoughts at each reasoning step, whereas TS-SFT only uses information from the selected paths, offering CPO with a clear advantage. A detailed discussion of the effectiveness of CPO is presented in Section 5.3.

### Component-wise Evaluations

**Effect of selection methods of dispreferred thoughts.** We analyze the impact of different methods for selecting dispreferred thoughts on model performance. As shown in Figure 4, we experiment with three strategies based on evaluation scores for each thought: 1) _CPO w/ Lowest_: Only the lowest-scoring thoughts in each reasoning step are dispreferred thoughts. 2) _CPO w/ Lower_: Thoughts with evaluation scores lower than the selected paths are dispreferred thoughts. 3) _CPO w/ All_: All thoughts not in the selected paths are considered dispreferred thoughts. We ensured an equal number of training samples for each strategy. Note that the evaluation score at each intermediate reasoning step (apart from the final one) determines whether to create the next reasoning step but not which thoughts are preferred. For example, as shown in the figure, even though the score of 32 is higher than 23, the thought with a score of 23 is preferred since it is part of the selected path.

The results in Figure 3(a) show that the performance differences among these strategies are minimal. This suggests that the distinction between preferred and dispreferred thoughts is better determined in the selected reasoning path rather than intermediate evaluation scores. To obtain a greater number of preferred thoughts for each instance to create paired preference thoughts, we chose the _CPO w/ All_ strategy.

Effect of numbers of training data.To assess the impact of the number of training data used in optimization, we conduct an ablation analysis by varying the number of instances (e.g., questions in the QA task) used to generate paired preference thoughts, ranging from \(0\) to \(200\). As illustrated in

Figure 4: Different strategies for selecting dispreferred thoughts and their impact on model performance. At each reasoning step, three strategies are used to select dispreferred thoughts based on their reasoning scores: 1) _CPO w/ Lowest_: Selects only the thought with the lowest score. 2) _CPO w/ Lower_: Selects all thoughts with scores lower than the preferred thought. 3) _CPO w/ All_: Selects all thoughts as dispreferred as long as they are not the preferred thought.

Figure 3: Component-wise evaluations and analysis on the Bamboo dataset using the LLaMA2-7B as the base model.

Figure 3(c), we observe that with an increase in the number of instances, the model's performance initially declines and then rises. Specifically, when trained with data generated from less than \(80\) instances, the model's performance is even worse than without any training, likely due to overfitting , which leads to performance degradation. However, as the number increases to \(120\), the model's performance consistently improves. Optimizing with paired thoughts from \(120\) instances, the model's performance surpasses that of the base model. When the number exceeds \(120\), the model's performance converges, indicating a balance of data for training.

Sensitivity of data mixture.We explore the performance of the CPO method across diverse data settings to assess its adaptability and learning efficiency from various data types. As shown in Table 2, we specifically examine three different data configurations: 1) single task data, 2) uniform QA data, and 3) mixed-type data. Our findings indicate that CPO demonstrates performance improvements of \(3.2\%\) in both settings 2 and 3, suggesting its robust ability to harness diverse data sources to enhance learning outcomes. In contrast, the SFT method exhibits comparable performance across these settings, indicating a different sensitivity to data diversity. It is worth noting that, to ensure fairness, although we find that mixed data leads to better performance, the experiments in Table 1 are conducted using individual datasets for training, consistent with the baselines.

## 6 Analysis

Do we need dispreferred information?We explore the impact of dispreferred thoughts on model performance by gradually incorporating these thoughts into the training data. Initially, we introduce dispreferred thoughts for their corresponding preferred counterparts and apply CPO to this segment of the data. For preferred thoughts without dispreferred counterparts, we implement SFT on these data. Consequently, the percentage of dispreferred thoughts incorporated can also be viewed as the proportion of data processed using CPO. We adjust the inclusion percentage of dispreferred thoughts from \(0\%\) to \(100\%\). An inclusion of \(0\%\) indicates that we utilize SFT solely on the preferred thoughts, i.e., the baseline TS-SFT. Conversely, an inclusion of \(100\%\) signifies our CPO, where the entire dataset includes paired preferred and dispreferred thoughts.

Why is chain level optimization important?As shown in Figure 3(d), we find that increasing the percentage of dispreferred data inclusion consistently improves model performance. This suggests that dispreferred thoughts are beneficial during the optimization process, highlighting the importance of leveraging both preferred and dispreferred thoughts for enhancing the model's reasoning capabilities.

Unlike our CPO, an alternative approach is to construct preference data using complete reasoning paths, i.e., using the selected full reasoning paths as preferred and other paths as dispreferred data, as shown in Figure 5. This method essentially applies DPO at the full-path level, referred to here as Full-path Preference Optimization (FPO). However, FPO encounters a significant issue where the gradients of the longest common prefix (LCP) tokens in paired data cancel

   Data & Description & **SFT** & **CPO** \\  Single-Task & Training only on specific task (Bamboogle) data. & 30.4 & **32.0** \\ Uniform QA & Training on 3 datasets of the same type (QA) as the test task. & 31.2 & **35.2** \\ Mixed-Type & Training on all 7 different types of data. & 29.6 & **35.2** \\   

Table 2: Effect of different kinds of training data on the Bamboogle dataset using the LLaMA2-7B as the base model.

Figure 5: Illustrations of two different ways to construct paired preference data: 1) CPO: Paired preference data are constructed at each thought step. 2) FPO: Paired preference data are constructed only at the full path level.

out, which we call the _LCP gradient cancellation_ issue. For example, for the preferred path \(_{w}=[5,+,4,=,9,and,9,+,2,=,11]\) and the dispreferred path \(_{l}=[5,+,4,=,9,and,9,+,2,=,15]\), the gradient will only be computed for the last token where the two sequences diverge.

To mathematically illustrate how LCP gradient cancellation happens in FPO, consider \(_{w}=[p_{1:n},w_{n+1}]\) and \(_{l}=[p_{1:n},l_{n+1}]\), where \(p\) is the longest common prefix sequence between \(_{w}\) and \(_{l}\). The gradient of FPO is given by:

\[_{}_{}(_{};_{})=C()_{}(_{}(_{w}|x)- _{}(_{l}|x))\] \[= C()_{}((p_{1: n}|x)}+_{}(w_{n+1}|x,p_{1:n})-(p_{1:n}|x )}-_{}(l_{n+1}|x,p_{1:n})),\]

where \(C()\) is a scalar function that does not affect the direction of the gradient and can be absorbed into the learning rate.

We can clearly see that the gradient terms of the common prefix tokens (highlighted with boxes) cancel each other out. This issue also exists in DPO training , but FPO suffers more frequently and severely due to the longer LCP between paired data constructed by tree search. As an empirical evidence, we observe the LCP length accounts for \(28\%\) of the total length in the Bambooleg dataset. CPO, on the other hand, constructs preference data at every step in the reasoning chain, allowing optimization of the LLM on all steps in the reasoning path. This means the common prefix can be optimized at its own step, ensuring that the gradient still exists for the common prefix.

We also compare FPO to CPO empirically in Figure 3(b), which further substantiates this observation. Switching to FPO led to a relative performance decrease of \(4.6\%\), even worse than the baseline SFT that does not utilize any information from dispreferred data. This underscores the importance of per-step preference thoughts for CPO.

## 7 Conclusion

In this work, we introduce a novel method called Chain of Preference Optimization (CPO), which leverages the supervision generated by the self-reasoning process (i.e., tree-of-thoughts) to enhance the reasoning ability of LLMs. Experiments on three different LLMs across seven different datasets demonstrate that CPO can consistently improve the performance of the base model by \(4.3\%\) on average without sacrificing inference speed. Furthermore, our method also substantially outperforms the strong baseline TS-SFT and even achieves comparable performance to the ToT method, which requires approximately \(57.5\) times more inference time.

In future work, we plan to integrate CPO with other reasoning algorithms, such as Graph-of-Thoughts  and AlphaZero-like tree search . Furthermore, we intend to explore the potential of using a weaker LLM to evaluate a stronger one within the CPO framework, facilitating weak-to-strong alignment .