# Partial Transportability for Domain Generalization

Kasra Jalaldoust

Equal Contribution.

Alexis Bellot1

Elias Bareinboim

Causal Artificial Intelligence Lab

Columbia University

{kasra, eb}@cs.columbia.edu, abellot95@gmail.com

Equal contribution.

###### Abstract

A fundamental task in AI is providing performance guarantees for predictions made in unseen domains. In practice, there can be substantial uncertainty about the distribution of new data, and corresponding variability in the performance of existing predictors. Building on the theory of partial identification and transportability, this paper introduces new results for bounding the value of a functional of the target distribution, such as the generalization error of a classifier, given data from source domains and assumptions about the data generating mechanisms, encoded in causal diagrams. Our contribution is to provide the first general estimation technique for transportability problems, adapting existing parameterization schemes such Neural Causal Models to encode the structural constraints necessary for cross-population inference. We demonstrate the expressiveness and consistency of this procedure and further propose a gradient-based optimization scheme for making scalable inferences in practice. Our results are corroborated with experiments.

## 1 Introduction

In the empirical sciences, the value of scientific theories arguably depends on their ability to make predictions in a domain different from where the theory was initially learned. Understanding when and how a conclusion in one domain, such as a statistical association, can be generalized to a novel, unseen domain has taken a fundamental role in the philosophy of biological and social sciences in the early 21st century. As Campbell and Stanley (2008, p. 17) observed in an early discussion on the interpretation of statistical inferences, "Generalization always turns out to involve generalization into a realm not represented in one's sample" where, in particular, statistical associations and distributions might differ, presenting a fundamental challenge.

As society transitions to become more AI centric, many of the every-day tasks based on predictions are increasingly delegated to automated systems. Such developments make various parts of society more efficient, but also require a notion of performance guarantee that is critical for the safety of AI, in which the problem of generalization appears under different forms. For instance, one critical task in the field is domain generalization, where one tries to learn a model (e.g. classifier, regressor) on data sampled from a distribution that differs in several aspects from that expected when deploying the model in practice. In this context, generalization guarantees must build on knowledge or assumptions on the "relatedness" of different training and testing domains; for instance, if training and testing domains are arbitrarily different, no generalization guarantees can be expected from any predictor . The question becomes how to link the domains of data that are used to train a model (a.k.a., the source domains) to the domain where this model is deployed in practice (a.k.a., the target domain).

To begin to answer this question, a popular type of assumption that relates source and target domains is _statistical_ in nature: invariances in the marginal or conditional distribution of some variables across the source and target distributions. Examples include assumptions of covariate shift and label shift (among others) [35; 34]. Notably, generalization is justified by the stability and invariance of the causal mechanisms shared across the domains [14; 21], since the distributional/statistical invariances across the domains are consequences of mechanistic/structural invariances governing the underlying data generating process. Although the induced statistical invariances, once exploited correctly, can be used as bases for generalizability. Broadly, invariance-based approaches to domain generalization [27; 29; 2; 40; 24; 20; 7; 6; 13] search for predictors that not only achieves small error on the source data but also maintain certain notions of distributional invariance across the source domains. Since these statistical invariances can be viewed as proxies to structural invariances, in certain instances generalization guarantees can be provided through causal reasoning [17; 31; 39]. This idea can be illustrated in Fig. 1. The value of variables \(\{C,Y,W,Z\}\) are determined as a stochastic function of variables pointing to it, while these functions may differ across domains. The challenge is to evaluate the generalization risk of a model, e.g. \(R_{P*}(h):=_{P*}[(Y-h)^{2}]\) for \(h:=h(C,W,Z)=_{P^{1}}[Y C,W,Z]\), without observations from the target \(P^{*}\). General instances of this challenge have been studied under the rubric of the theory of causal transportability, where qualitative assumptions regarding the underlying structural causal models are encoded in a graphical object, and algorithms are designed to leverage these assumptions and compute certain statistical queries in the target domain in terms of the existing source data [26; 4; 5; 19; 11; 17].

Despite these advances, in practice, the combination of source data and graphical assumptions is not always sufficient to identify (uniquely evaluate) the desired statistical query, e.g., the average loss of a given predictor in the target domain. In this case, the query is said to be non-transportable3. For example, given Fig. 1, \(R_{P*}(h)\) is non-transportable for the classifier \(h:=h(C,W,Z)\). In this paper, we study the fundamental task of computing tight upper-bounds for statistical queries in a new unseen domain. This allows us to assess worst-case performance of prediction models for the domain generalization task. Our contributions are as follows:

* **Sections 2 & 3.** We develop the first general estimation technique for bounding the value of queries across multiple domains (e.g., the generalization risk) in non-transportable settings (Def. 4). Specifically, we extend the formulation of canonical models [3; 42] to encode the constraints necessary for solving the transportability task, and demonstrate their expressiveness for generating distributions entailed by the underlying Structural Causal Models (SCMs) (Thm. 1).
* **Section 4.** We adapt Neural Causal Models (NCMs)  for the transportability task via a parameter sharing scheme (Thm. 2), similarly demonstrating their expressiveness and consistency for solving the partial transportability task. We then leverage the theoretical findings in sections 2 & 3 to implement a gradient-based optimization algorithm for making scalable inferences (Alg. 1), as well as a Bayesian inference procedure. Finally, we introduce Causal Robust Optimization (CRO) (Alg. 2), an iterative method to find a predictor with the best worst-case risk.

**Preliminaries.** We use capital letters to denote variables (\(X\)), small letters for their values (\(x\)), bold letters for sets of variables (\(\)) and their values (\(\)), and use \(\) to denote their domains of definition (\(x_{X}\)). A conditional independence statement in distribution \(P\) is written as \((\!\!\!)_{P}\). A \(d\)-separation statement in some graph \(\) is written as \((\!\!\!\!_{d}\!\!)\). To denote \(P(==)\), we use the shorthand \(P()\). The basic semantic framework of our analysis relies on Structural Causal Models (SCMs) [25, Definition 7.1.1], which are defined below.

Figure 1: Illustration of the task of evaluating the generalization error of a model \(h\). The mechanisms for \(C\) and \(W\) vary across domains.

[MISSING_PAGE_FAIL:3]

**Definition 3** (Selection diagram).: The selection diagram \(^{_{i}}\) is constructed from \(^{i}\) (\(i\{1,2,,T\}\)) by adding the selection node \(S_{i}\) to the vertex set, and adding the edge \(S_{i} V\) for every \(V_{i}\). The collection \(^{}=\{^{*}\}\{^{_{i}} \}_{i\{1,2,,T\}}\) encodes the graphical assumptions. Whenever the causal diagram is shared across the domains, a single diagram can depict \(^{}\). \(\)

Selection diagrams extend causal diagrams and provide a parsimonious graphical representation of the commonalities and disparities across a collection of SCMs. The following example illustrates these notions and highlights various subtleties in the generalization error of different predictors.

**Example 2** (Generalization performance of classifiers).: Consider the SCMs \(^{i}\) (\(i\{1,2,*\}\)) over the binary variables \(=\{C_{1},C_{2},,C_{10}\}\{W,Z\}\) and \(Y\), defined as follows:

\[P^{i}(): 1 j 10:U_{C_{j} }(0.1)i=1(0.5)i=2 (0.7)i=*\\ U_{VW}(0.2)\\ U_{W}(0.01)i=1(0.02)i=2 (0.5)i=*\\ U_{Z}(0.9)\]

\(\) denotes the xor operator, _i.e._, \(A B\) evaluates to 1 if \(A B\) and evaluates to 0 if \(A=B\). Notice that the distribution of exogenous noise associated with \(_{1:10}\) and \(\{W\}\) differs across the domains. Consider three baseline classifiers \(h_{1}(,w):=w_{c}c,h_{2}( ):=_{c}c,h_{3}(z):=z\) evaluated on data from \(P^{1},P^{2},P^{*}\) with the symmetric loss function \((Y,h())=1\{Y h()\}\). Their errors are given in Table 1. Notice that \(h_{1}\) has almost perfect accuracy on both source distributions, but does not generalize to \(^{1}\) as it uses the unstable feature \(W\), incurring \(50\%\) loss. This observation indicates that mere minimization of the empirical risk might yield arbitrarily large risk in the unseen target domain. \(h_{2}\) uses the features \(\) that are the direct causes of \(Y\), also known as the causal predictor [27; 2], and yields a stable loss of \(20\%\) across all domains. On the other hand, \(h_{3}\) uses only \(Z\) that is a descendant of \(Y\), yet achieves a small loss across all domains as the mechanism of \(Z\) is assumed to be invariant. This observation is surprising, because \(h_{3}\) is neither a causal predictor nor the minimizer of the empirical risk, yet it performs nearly optimally on all domains. \(\)

Example 2 illustrates potential challenges of the domain generalization problem, particularly regarding the variation of the risk of classifiers across the source and target domains. The following definition introduces the problem of "partial transportability" which is the main conceptual contribution of our paper. The objective is bounding a statistic of the target distribution using the data and assumptions available about related domains.

**Definition 4** (Partial Transportability).: Consider a system of SCMs \(:\{^{1},^{2},,^{K},^{*}\}\) that induces the selection diagram \(^{}\) over the variables \(\) and entails the distributions \(:\{P^{1}(),P^{2}(),,P^{K}( )\}\) and \(P^{*}()\). A functional \(:_{}\) is partially transportable from \(\) given \(^{}\) if,

\[_{P^{}^{}}[()]  q_{},_{0}^{}, \]

where \(q_{}\) is a constant that can be obtained from \(\) given \(^{}\). \(\)

For instance, finding the worst-case performance of a classifier based on the source distributions given the selection diagram is a special case of partial transportability with \((,y):=(y,h())\). In principle, this task is challenging as the exogenous distribution \(P^{*}(_{V})\) and structural assignments \(f^{*}_{V}\) of variables \(V\) that do not match with any of the source domains could be arbitrary. In the following section, we will define tractable parameterization of \(\{P(),\}\) to derive a systematic approach to solving partial transportability tasks.

## 3 Canonical Models for Partial Transportability

We begin with an example to illustrate how one might approach parameterizing a query such as \(_{P*}[()]\), _e.g._, the generalization error, to consistently solve the partial transportability task.

   Classifier & \(R_{P^{1}}\) & \(R_{P^{2}}\) & \(R_{P*}\) \\  \(h_{1}(,w)\) & 1\% & 4\% & 49\% \\ \(h_{2}()\) & 20\% & 20\% & 20\% \\ \(h_{3}(z)\) & 3\% & 5\% & 4\% \\   

Table 1: Classifiers in Example 2.

**Example 3** (The bow model).: Let \(:=\{X\}\) be a single binary variable, and \(Y\) be a binary label. Consider two source domains defined by the following SCMs:

\[^{1}:P^{1}():U_{X} (0.2)\\ U_{Y}(0.05)\\ V_{XY}(0.95)&^{2}:P^ {2}():U_{X}(0.9)\\ U_{Y}(0.05)\\ U_{XY}(0.95)\\ ^{1}:X U_{X} U_{XY}\\ Y(X U_{XY}) U_{Y}&^{2}: P^{2}():U_{X}(0.9)\\ U_{Y}(0.05)\\ U_{XY}(0.95)\\ ^{2}:X U_{X} U_{XY}\\ Y(X U_{XY}) U_{Y}\]

The task is to evaluate the generalization error of the classifier \(h(x)= x\). \(h\) can be shown optimal in both source domains: achieving \(R_{P^{1}}(h) 0.11\) and \(R_{P^{2}}(h) 0.06\). However, it is unclear whether it generalizes well to a target domain \(^{*}\), given the domain discrepancy sets \(_{1}=\{X\},_{2}=\{Y\}\). \(\)

Balke and Pearl  derived a canonical parameterization of SCMs such as \(\{^{1},^{2},^{*}\}\) in Example 3. They showed that it is sufficient to parameterize \(P()\) with correlated discrete latent variables \(R_{X},R_{Y}\), where \(R_{X}\) determines the value of \(X\), and \(R_{Y}\) determines the functional that decides \(Y\) based on \(X\). The causal diagrams are shown in Figure 2. Canonical SCMs entails the same set of distributions as the true underlying SCMs, _i.e._ are equally expressive. In particular, Zhang and Bareinboim  showed that for every SCM \(\), there exists an SCM of the described form specified with only a distribution \(P(r_{X},r_{Y})\), where \(_{R_{X}}=\{0,1\},_{R_{Y}}=\{y=0,y= 1,y=x,y= x\}\). The joint distribution \(P(r_{X},r_{Y})\) can be parameterized by a vector in 8-dimensional simplex, and entails all observational, interventional and counterfactual variables generated by the original SCM.

The following definition by Zhang et al.  provides a general formulation of canonical models.

**Definition 5** (Canonical SCM).: A canonical SCM is an SCM \(=,,},P()\) defined as follows. The set of endogenous variables \(\) is discrete. The set of exogenous variables \(=\{R_{V}:V\}\), where \(_{R_{V}}=\{1,,m_{V}\}\) and \(m_{V}=|\{h_{V}:_{pa_{V}}_{V}\}|\). For each \(V\), \(f_{V}\) is defined as \(f_{V}(pa_{V},r_{V})=h_{V}^{(r_{V})}(pa_{V})\).

**Example 3** (continued).: Consider extending the canonical parameterization to to solve the partial transprobability task by optimization. Each SCM \(^{1},^{2},^{*}\) is associated with a canonical SCM \(^{1},^{2},^{*}\). with exogenous variables \(\{R_{X},R_{Y}\}\) as above. The domain discrepancy sets \(\) indicate that certain causal mechanisms need to match across pairs of the SCMs. For example, \(_{1}=\{X\}\), which does not contain \(Y\), and this means that (1) the function \(f_{Y}\) is the same across \(^{1},^{*}\), and (2) the distribution of unobserved variables that are arguments of \(f_{Y}\), namely, \(U_{XY},U_{Y}\) remains the same across \(^{1},^{*}\). Imposing these equalities on the canonical parameterization is straightforward as (1) the function \(f_{Y}\) is the same across all canonical SCMs by construction, and (2) the only unobserved variable pointing to variable \(V\) is \(R_{V}\) (for \(V\{X,Y\}\)). Following the selection diagram shown in Fig. 2a, \(^{1},^{*}\) agree on the mechanism of \(Y\), which translates to the constraint \(P^{^{1}}(r_{Y})=P^{^{*}}(r_{Y})\). Similarly, \(^{2},^{*}\) agree on the mechanism of \(X\) that translates to the constraint \(P^{^{2}}(r_{X})=P^{^{*}}(r_{X})\). Putting these together, the optimization problem below finds the upper-bound for the risk \(R_{P^{*}}(h)\) for the classifier \(h(x)= x\):

\[_{^{1},^{2},^{*}} P^{^{*}}(Y X)\] (5) s.t. \[P^{^{1}}(r_{Y})=P^{^{*}}(r_{Y}), P^{ ^{2}}(r_{X})=P^{^{*}}(r_{X})(Y_{1} X_{2})\] \[P^{^{1}}(x,y)=P^{1}(x,y), P^{^{2}}(x,y)=P^{2}(x,y)\]

Notably, the above optimization has a linear objective with linear equality constraints. \(\)

This example illustrates a more general strategy, in which probabilities induced by an SCM over discrete endogenous variables \(\) may be generated by a canonical model. What follows is the main result of this section, and provides a systematic approach to partial transportability using the canonical models.

Figure 2: Selection diagram & Canonical param.

**Theorem 1** (Partial-TR with canonical models).: _Consider the tuple of SCMs \(\) that induces the selection diagram \(^{}\) over the variables \(\), and entails the source distributions \(\), and the target distribution \(P^{}\). Let \(:_{}\) be a functional of interest. Consider the following optimization scheme:_

\[_{^{1},^{2},,^{}}_{ P}[()]P^{^{i}}()=P^{i}()  i\{1,2,,K,*\} \]

\[P^{^{i}}(r_{V})=P^{^{j}}(r_{V}), i,j\{1,2,,K,*\} V_{i,j}\]

_where each \(^{i}\) is a canonical model characterized by a joint distribution over \(\{R_{V}\}_{V}\). The value of the above optimization is a tight upper-bound for the quantity \(_{P*}()\) among all tuples of SCMs that induce \(^{}\) and entail \(\). _

In words, this Theorem states that one may tightly bound the value of a target quantity \(_{P*}()\) by optimizing over the space of canonical models subject to the proposed constraints, without any loss of information. An implementation of Thm. 1 approximating the worst-case error, by making inference on the posterior distribution of the target quantity, is provided in Appendix A.

## 4 Neural Causal Models for Partial Transportability

In this section we consider inferences in more general settings by using neural networks as a generative model, acting as a proxy for the underlying SCMs \(\) with the potential to scale to real-world, high-dimensional settings while preserving the validity and tightness of bounds. For this purpose, we consider Neural Causal Models  and adapt them for the partial transportability task. What follows is an instantiation of [41, Definition 7].

**Definition 6** (Neural Causal Model).: A Neural Causal Model (NCM) corresponding to the causal diagram \(\) over the discrete variables \(\) is is an SCM defined by the exogenous variables:

\[=\{U_{}(0,1): A B, A,B \}, \]

and the functional assignments \(V f_{_{V}}(Pa_{V},_{V}),\) where \(_{V}=\{U_{}:V\}\). The function \(f_{_{V}}\) is a feed-forward neural network parameterized with \(_{V}\) that outputs in \(_{V}\). The distribution entailed by an NCM is denoted by \(P(;)\), where \(=\{_{V}\}_{V}\).

To illustrate how one might leverage this parameterization to define an instance of partial transportability task consider the following example.

**Example 4**.: Let SCMs \(^{1},^{2},^{*}\) induce \(^{}\) shown in Fig. 3 over the binary variables \(,Y\), where \(=\{C_{1},C_{2},Z_{1},Z_{2},W_{1},,W_{5}\}\). Let \(^{1},^{2},^{*}\) be the parameters of NCMs constructed based on the causal diagram in Fig. 3 (without the s-nodes). The objective is to constrain these parameters to simulate a compatible tuple of NCMs \(_{^{1}},_{^{2}},_{*}\) that equivalently entail \(P^{1}(,y),P^{2}(,y)\) and induce \(^{}\).

For instance, the fact that \(S_{2}\) is not pointing to \(Y\) suggests the invariance \(f_{Y}^{*}=f_{Y}^{2}\) and \(P^{*}(_{Y})=P^{2}(_{Y})\) for the true underlying SCMs. That same invariance may be enforced in the corresponding NCMs by relating the parameterization of \(_{^{2}},_{*}\), i.e., imposing that \(_{Y}^{*}=_{Y}^{2}\) for the NN generating \(Y\). Similarly, the observed data \(D^{1},D^{2}\) from the source distributions \(P^{1}(,y),P^{2}(,y)\), respectively, impose constraints on the parameterization of NCMs as plausible models must satisfy \(P(,y;^{1})=P^{1}(,y)\) and \(P(,y;^{2})=P^{2}(,y)\). This may be enforced, for instance, by maximizing the likelihood of data w.r.t. the NCM parameters: \(^{i}*{arg\,max}_{}_{,y D^{i }} P(,y;^{i})\), for \(i\{1,2\}\). By extending this intuition for all constraints imposed by the selection diagram and data, we narrow the set of NCMs \(_{^{1}},_{^{2}},_{*}\) to a set that is compatible with our assumptions and data. Maximizing the risk of some prediction function \(R_{P*}(h)\) in this class of constrained NCMs might then achieve an informative upper-bound. _

Motivated by the observation in Example 4, we now show a more formal result (analogous to Thm. 1) that guarantees that the solution to the partial transportability task in the space of constrained NCMs achieves a tight bound on a given target quantity \(_{P*}()\).

Figure 3: Selection diagram for Example 4.

**Theorem 2** (Partial-TR with NCMs).: _Consider a tuple of SCMs \(\) that induces \(^{},\) and \(P^{}\) over the variables \(\). Let \(D^{i} P^{i}(x,y)\) denote the samples drawn from the \(i\)-th source domain. Let \(^{i}\) denote the parameters of NCM corresponding to \(^{i}\). Let \(_{P^{}}[()]\) be the target quantity. The solution to the optimization problem,_

\[*{arg\,max}_{: ^{1},^{2},,^{K},^{}}_{}()_{}P(;^{}) \] \[^{i}_{V}=^{j}_{V}, i,j\{1,2, ,K,\} V_{i,j}\] \[^{i}*{arg\,max}_{}_{ D^{ i}} P(;), i\{1,2,,K\}.\]

_is a tuple of NCMs that induce \(^{}\), entails \(\). In the large sample limit, the solution yields a tight upper-bound for \(_{P^{}}[()]\). \(\)_

Theorem 2 establishes the expressive power of NCMs for solving partial transportability tasks. This formulation is powerful because it enables the use of gradient-based optimization of neural networks for learning and, in principle, might scale to large number of variables.

### Neural-TR: An Efficient Implementation

We could further explore the efficient optimization of parameters by exploiting the separation between variables in the selection diagram. Rahman et al. , for instance, show that the NCM parameterization is modular w.r.t. the c-components of the causal diagram. We can similarly elaborate on this property, and leverage it for more efficient partial transportability.

In the following, we build towards an efficient algorithm for partial transportability using NCMs by first showing an example that describes how a given target quantity \(_{P^{}}[()]\) might be decomposed for learning more efficiently.

**Example 4** (continued).: \(P(,y;^{})\) in the objective in Eq. (8) may be decomposed as follows:

\[P(,y;^{})=P^{}(,c_{2},w_{1},w_{2}; ^{}_{_{1}}}_{_{1}}) P(}_{_{2}}|,w_{2}}_{_{2}};^{}_{_{2}}) P (,z_{2},w_{4},w_{5}}_{_{3}}|}_{_{3}};^{}_{_{3}}),\]

where the subsets \(_{1},_{2},_{3}\) are the \(c\)-components of \(^{}\). Notice, \(S_{2}\) is not pointing to any of the variables \(_{2}\), which means that their mechanism is shared across \(^{2},^{}\), and therefore,

\[P(_{2}_{2};^{}_{_{2}})=P(_{2}_{2};^{2}_{_{2}}) P^{2}(_{2}_{2}). \]

This property is the basis of transportability algorithms , and is known as the s-admissibility criterion , which allows us to deduce distributional invariances from structural invariances. By Eq. (9), we can replace the term \(P(_{2}_{2};^{}_{_{2}})\) in the objective with the probabilistic model \(P(_{2}_{2};^{2})\) that is trained with \(D^{2}\) to approximate \(P^{2}(_{2}_{2})\) and plug it into the objective \(Eq.\) (8) as a constant.

As a consequence, we do not need to optimize over the parameters \(^{1}_{_{2}},^{2}_{_{2}},^{}_{_{2}}\) from the partial transportability optimization problem. Similarly, since \(S_{1}\) does not point to \(_{1}\), we can substitute \(P(_{1};^{})\) with \(P(_{1};^{1})\), and pre-train it with data \(D^{1}\). In the context of Example 4 and the evaluation of \(R_{P^{}}(h)\), the objective in Eq. (8) may be simplified to the substantially lighter optimization task:

\[_{^{1}_{_{3}},^{2}_{_{3}},^{ }_{3}}_{_{1} P(_{1};^{1})}_{_{2} P(_{2}|_{2};^{2})}[_{_{3}}P( _{3}_{3};^{}_{_{3}}) 1\{h(_{1},_{2},_{3} \{y\}) y\}]\] \[^{i}_{_{3}}*{arg\,max}_{ _{_{3}}}_{_{3}, D^{i}} P(_{3} {b}_{3};_{_{3}}),i\{1,2\}. \]

In general, the parameter space of NCMs can be cleverly decoupled and the computational cost of the optimization problem can be significantly improved since only a subset of the conditional distributions need to be parameterized and optimized. This observation motivates Alg. 1 designed to exploit these insights. It proceeds by first, decomposing the query, second, computing the identifiable components, and third, parameterizing the components that are not point identifiable and running the NCM optimization routine. The following proposition demonstrates the correctness of this procedure.

```
0: Source data \(D^{1},D^{2},,D^{K}\); selection diagram \(^{}\); functional \(:_{}\).
0: Upper-bound for \(_{P^{*}}}\)
1:\(\{_{j}\}_{j=1}^{m}\) c-components of \(:=_{^{}}()\) in causal diagram \(^{*}\).
2:\(,_{},_{ } 0\)
3:\(P^{*}():=_{}_{j=1}^{m}P^{*}(_{j} (pa_{_{j}}))\)
4:for\(j=1\) to \(m\)do
5:if\( i\{1,2,,K\}\) such that \(_{j}_{i}=\)then
6:\(^{i}_{_{j}}_{_{_{j}}}_{_{j}, pa_{_{j}} D^{i}} P(_{j}(pa_{_{j}}); _{_{j}})\)
7: In \(P^{*}()\), replace \(P^{*}(_{j}(pa_{_{j}}))\) with \(P(_{j}(pa_{_{j}});^{i}_{_{j}})\).
8:else
9:\(\{^{i}_{_{j}}\}_{i\{1,2,,K,*\}}\)
10: In \(P^{*}()\), replace \(P^{*}(_{j}(pa_{_{j}}))\) with \(P(_{j}(pa_{_{j}});^{*}_{_{j}})\).
11:\(_{}_{}\{^{i }_{V}=^{*}_{V}\}_{V_{j}_{j}} _{i=1}^{K}\)
12:\(_{}_{}+ _{_{j},pa_{_{j}} D^{i}} P(_{j}, (pa_{_{j}});^{i}_{_{j}})\).
13:endif
14:endfor
15:Return\(_{}_{}P^{*}(;) ()+_{}()\) subject to \(_{}\)
```

**Algorithm 1** Neural-TR

This result may be understood as an enhancement of Thm. 2 in which the factors that are readily transportable from source data are taken care of in a pre-processing step. The hybrid approach is especially useful in case researchers have pre-trained probabilistic models with arbitrary architecture that they can use off-the-shelf and avoid unnecessary computation.

### Neural-TR for the Optimization of Classifiers

The Neural-TR algorithm can be viewed as an adversarial domain generator that takes a classifier \(h()\) as the input, and then parameterizes a collection of SCMs to find a plausible target domain that yields the worst-case risk for the given classifier, namely, \(^{*}\). By flipping \(h()\) for some \(\) we can reduce the risk of \(h\) under \(^{*}\).

Interestingly, we can exploit Neural-TR to generate adversarial data for a given classifier and introduce an iterative procedure to progressively train classifiers with with minimum risk upper-bound. Algorithm 2 describes this approach. At each iteration, CRO uses Neural-TR as a subroutine to obtain an adversarially designed NCM \(^{*}\) that yields the worst-case risk for the classifier at hand. Next, it collects data \(D^{*}\) from this NCM and adds it to a collection of datasets \(^{*}\). Finally, it updates the classifier to be robust to the collection \(^{*}\) by minimizing the maximum of the empirical risk \(R_{D}(h):=_{,y D}(y,h(x))\) across all \(D^{*}\). We repeat this process until convergence of the upper-bound for risk. The following result justifies optimality of CRO for domain generalization; more discussion is provided in Appendix C.2.

**Theorem 3** (Domain generalization with CRO).: _Algorithm 2 returns a worst-case optimal solution;_

\[(,^{})*{arg\,min} _{h:_{}_{}}_{$ that entails $$ \& induces $^{}$}}R_{P^{^{*}_{0}}}(h). \]

In words, Thm. 3 states that the classifier returned by CRO, in the large sample limit, minimizes worst-case risk in the target domain subject to the constraints entailed by the available data and induced by the structural assumptions.

## 5 Experiments

This section illustrates Algs. 1 and 2 for the evaluation and optimization of the generalization error on several tasks, ranging from simulated examples to semi-synthetic image datasets. The details of the experimental set-up and examples not fully described below, along with additional experiments, can be found in the Appendix.

### Simulations

Worst-case risk evaluationOur first experiment revisits Examples 2 and 3 for the evaluation of the worst-case risk \(R_{P*}\) of various classifiers with Neural-TR (Alg. 1).

In Example 2 we had made (anecdotal) performance observations for the classifiers \(h_{1}(,w):=w_{c}c,h_{2}():=_{c }c,h_{3}(z):=z\) in a selected target domain \(^{*}\). We now consider providing a worst-case risk _guarantee_ with Neural-TR for _any_ (compatible) target domain. The main panel in Fig. 3(a) shows the convergence of the worst-case risk evaluator over successive training iterations (line 15, Alg. 1), repeated 10 times with different model seeds and solid lines denoting the mean worst-case risk. The source performances \(R_{P_{1}},R_{P_{2}}\) are given in the two right-most panels for reference. We observe that the good source performance of \(h_{2}()\) and \(h_{3}(z)\) generalizes to _all_ possible target domains consistent with our assumptions, while the classifier \(h_{1}(,w)\) diverges, with an error of \(90\%\) in the worst target domain. In Example 3, we consider the evaluation of binary classifiers \(h\{h_{1}(x):=x,h_{2}(x):= x,h_{3}(x):=0,h_{4}(x):=1\}\). \(h_{2}(x)= x\). Our results are given in Fig. 3(b), highlighting the extent to which source performance need not be indicative of target performance. With these results, we are now in a position to confirm the desirable performance profile of \(h_{2}\), even in the worst-case, as hypothesized in Example 3.

Worst-case risk optimizationFor each one of the examples above, we implement CRO (Alg. 2) to uncover the theoretically optimal classifier in the worst-case. The worst-case risks of the classifiers learned by CRO, denoted \(h_{}\), are given by \(0.05\) for Example 2 and \(0.18\) for Example 3. The worst-case risk evaluation results (with Neural-TR, as above) are given in Figs. 3(d) and 3(e). It is interesting to note that these errors coincide with the best performing classifiers considered in the previous experiment, i.e. \(h_{3}(z):=z\) for Example 2 and \(h_{2}(x)= x\) for Example 3. In fact, by comparing the outputs of CRO \(h_{}\) with these classifiers, we can verify that the classifiers learned by CRO in these examples are precisely the mappings \(h_{}(z):=z\) and \(h_{}(x)= x\) which is remarkable. By Thm. 3, \(h_{3}(z):=z\) and \(h_{2}(x)= x\) are the theoretically best worst-case classifiers among all possible functions given the data and assumptions.

### Colored MNIST

Our second experiment considers the colored MNIST (CMNIST) dataset that is used in the literature to highlight the robustness of classifiers to spurious correlations, e.g. see . The goal of the classifier is to predict a binary label \(Y\{0,1\}\) assigned to an image \(^{28 28 3}\) based on whether the digit in the image is greater or equal to five. MNIST images \(^{28 28}\) are grayscale (and latent), and color \(C\{,\}\) correlates with the class label \(Y\).

Following standard implementations, we construct datasets from three domains with varying correlation strength between the color and image label: set to \(90\%\) agreement between the color \(C=\)red and label \(Y=1\) in source domain \(^{1}\), and \(80\%\) in source domain \(^{2}\). We consider performance evaluation and optimization in a target domain \(^{*}\) with potential discrep

Figure 4: (a-c): worst-case risk evaluation results as a function of Neural-TR (Alg. 1) training iterations. (d,e): worst-case risk evaluation of CRO.

ancies in the mechanism for \(C\), rendering the correlation between color and label unstable. The selection diagram is given in Figure 5.

Worst-case risk evaluationConsider a setting in which we are given a classifier \(h:_{}_{Y}\), and the task is to assess its generalizability with a symmetric 0-1 loss function. We use data drawn from \(P^{1,2}(,y)\) to train predictors using Empirical Risk Minimization (ERM) , Invariant Risk Minimization (IRM) , and group Distributionally Robust Optimization (group DRO) , namely \(h_{}(),h_{}()\), and \(h_{}()\) respectively; more detailed discussion about the role of invariance and robustness in domain generalization is available in appendix D. Using Neural-TR, we observe in Fig. 3(c) that the worst-case risk of \(h_{}\) in a target domain with a discrepancy in the color assignment is approximately \(0.95\), \(h_{}\) achieves \(0.90\) worst-case risk, and \(h_{}\) achieves \(0.65\) worst-case risk. Either method perform worse than the baseline, that is random classification with risk \(0.5\). On this task, a classifier trained on gray-scale images \(\) achieves a worst-case error of \(0.25\).

Worst-case risk optimizationWe now ask whether we could learn a theoretically optimal classifier in the worst-case with CRO (Alg. 2). Fig. 6 illustrates the training process over several iterations. Specifically, given a randomly initialized \(h\), we infer the NCM \(^{}\) that entails worst-case performance of \(h\) (in this case, chance performance \(R_{P}*(h)=0.5\)) and generate data \(D^{*}\) from \(^{}\), shown in Fig. 5(a). In a second iteration, a new candidate \(h\) is trained to minimize worst-case risk on \(=D^{*}\). Note that in \(D\)*, we observe an almost perfect association between the color \(C=\)green and label \(Y=1\): \(h\) therefore is encouraged to exploit color for prediction. Its worst-case error (inferred with Neural-TR) is accordingly close to 1, and the corresponding worst-case NCM \(\)* entails a distribution of data in which the correlation between color and label is flipped: with a strong association between the color \(C=\)red and label \(Y=1\), as shown in Fig. 5(b). In a third iteration, a new candidate \(h\) is trained to minimize worst-case risk on the updated \(^{*}\) with data samples from the previous two iterations (exhibiting opposite color-label correlations). By construction, this classifier is trained to ignore the spurious association between color and label, classifying images based on the inferred digit which leads to better behavior in the worst-case: achieving a final error of approximately 0.25, as shown in Fig. 5(c), which is theoretically optimal. Note, however, that the poor performance of the baseline algorithms is not directly comparable to that of CRO, since CRO has access to background information (selection diagrams) that can not be communicated with the baseline algorithms. CRO may thus be interpreted as a meta-algorithm that operates with a broader range of assumptions encoded in a certain format (i.e., the selection diagram) that enable it to find the theoretically optimal classifier for domain generalization, in contrast to the baseline algorithms.

## 6 Conclusion

Guaranteeing the performance of ML algorithms implemented in the wild is a critical ingredient for improving the safety of AI. In practice, evaluating the performance of a given algorithm is non-trivial. Often the performance may vary as a consequence of our uncertainty about the possible target domain, also called a non-transportable setting. In this paper, we provide the first general estimation technique for bounding an arbitrary statistic such as the classification risk across multiple domains. More specifically, we extend the formulation of canonical models and neural causal models for the transportability task, demonstrating that tight bounds may be estimated with both approaches. Building on these theoretical findings, we introduce a Bayesian inference procedure as well as a gradient-based optimization algorithm for scalable inferences in practice. Moreover, we introduce Causal Robust Optimization (CRO), an iterative learning scheme that uses partial transportability as a subroutine to find a predictor with the best worst-case risk given the data and graphical assumptions.

Figure 6: Illustration of the CRO training process (Alg. 2) on the colored MNIST task.

Acknowledgement

This research was supported in part by the NSF, ONR, AFOSR, DARPA, DoE, Amazon, JP Morgan, and The Alfred P. Sloan Foundation.