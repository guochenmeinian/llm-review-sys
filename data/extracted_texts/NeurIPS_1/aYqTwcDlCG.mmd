# Learning World Models for Unconstrained Goal Navigation

Yuanlin Duan

Rutgers University

yuanlin.duan@rutgers.edu

&Wensen Mao

Rutgers University

wm300@cs.rutgers.edu

&He Zhu

Rutgers University

hz375@cs.rutgers.edu

###### Abstract

Learning world models offers a promising avenue for goal-conditioned reinforcement learning with sparse rewards. By allowing agents to plan actions or exploratory goals without direct interaction with the environment, world models enhance exploration efficiency. The quality of a world model hinges on the richness of data stored in the agent's replay buffer, with expectations of reasonable generalization across the state space surrounding recorded trajectories. However, challenges arise in generalizing learned world models to state transitions backward along recorded trajectories or between states across different trajectories, hindering their ability to accurately model real-world dynamics. To address these challenges, we introduce a novel goal-directed exploration algorithm, MUN (short for "World Models for Unconstrained Goal Navigation"). This algorithm is capable of modeling state transitions between arbitrary subgoal states in the replay buffer, thereby facilitating the learning of policies to navigate between any "key" states. Experimental results demonstrate that MUN strengthens the reliability of world models and significantly improves the policy's capacity to generalize across new goal settings.

## 1 Introduction

Goal-conditioned reinforcement learning (GCRL) has emerged as a powerful framework for learning diverse skills within an environment and subsequently solving tasks based on user-specified goal commands, without requiring further training (Mendonca et al., 2021; Andrychowicz et al., 2017). Given that specifying dense task rewards for GCRL requires domain expertise, access to object positions, is time-consuming, and is prone to human errors, rewards in GCRL are typically sparse, signaling success only upon reaching goal states. However, sparse rewards pose a challenge for exploration during training. To address this challenge, several previous methods, e.g., (Hafner et al., 2019; Hansen et al., 2023; Mendonca et al., 2021) have proposed learning a generative world model of the environment using a reconstruction (decoder) objective, an instantiation of Model-based Reinforcement Learning (MBRL), visualized in Fig. 1. This approach is appealing because the world model can provide a rich learning signal (Yu et al., 2020; Georgiev et al., 2024). For example, world models allow agents to plan their actions or exploratory goals without directly interacting with the real environment for more efficient exploration (Hu et al., 2023; Sekar et al., 2020).

Existing MBRL techniques train world models to capture the dynamics of the environment from the agent's past experiences stored in a replay buffer. The richness of the data stored in the agent's replay buffer directly impacts the quality of a World Model. It is expected that the world model generalizes reasonably well to the state space surrounding the trajectories recorded in the replay buffer. However, the world model may not generalize well to state transitions backward along recorded trajectories or to states across different trajectories, which impedes the world model's learning of the real-world dynamics.

To induce a data-rich replay buffer covering a wide range of dynamic transitions, in this paper, we present a novel goal-directed exploration algorithm for effective world modeling and policy learning, MUN (short for "World **M**odels for **U**nconstrained Goal **N**avigation"). MUN facilitates modeling state transitions between any subgoal states in the replay buffer, whether tracing back along recorded trajectories or transitioning between states on separate trajectories. This enhances the reliability of the learned world model and significantly improves the generalizability of the policy derived from the model to real-world environments, thereby boosting the exploration capabilities of the method. Additionally, we introduce a simple and practical strategy for discovering _key_ subgoal states from the replay buffer. The key subgoals precisely mark the milestones necessary for task completion, such as steps like grasping and releasing blocks in the context of block-stacking scenarios. By world modeling and policy learning for unconstrained navigation between these key states, MUN can generalize to new goal settings, such as block unstacking that was not given to the agent at training time.

Our key contributions are as follows. First, we propose a novel goal-directed exploration algorithm MUN for effective world modeling of state transition between arbitrary subgoal states in replay buffers. As the quality of the world model improves, MUN becomes highly effective at learning goal-conditioned policies that excel at exploration in sparse-reward environments. Second, we present a practical strategy for identifying pivotal subgoal states, which serve as milestones in completing sophisticated tasks. By training world models for unconstrained transition between these milestones, our method enables learning policies that can adapt to novel goal scenarios. Finally, we evaluate MUN in challenging robotics environments, such as guiding a multi-legged ant robot through a maze, maneuvering a robot arm amidst cluttered tabletop objects, and rotating items in the grasp of an anthropomorphic robotic hand. Across these environments, MUN exhibits superior efficiency in training generalizable goal-conditioned policies compared to baseline methods and ablations.

## 2 Problem Setup and Background

We consider the problem of goal-conditioned reinforcement learning (GCRL) under a Markov Decision Process (MDP) parameterized by \((S,A,P,G,,R,_{0})\). \(S\) and \(A\) are the state and action spaces, respectively. The probability distribution of the initial states is given by \(_{0}(s)\), and \(P(s^{}|s,a)\) is the transition probability. \(:S G\) is a mapping from the state space to the goal space, which assumes that every state \(s\) can be mapped to a corresponding achieved goal \(g\). The reward function \(R\) is defined as \(R(s,a,s^{},g)=1\{(s^{})=g\}\). We assume that each episode has a fixed horizon \(T\). For ease of presentation, we further assume \(S=G\) and \(\) is an identity function in this paper.

A goal-conditioned policy is a probability distribution \(:S G A^{+}\), which gives rise to trajectory samples of the form \(=\{s_{0},a_{0},g,s_{1},,s_{T}\}\). The purpose of the policy \(\) is to learn how to reach the goals drawn from the goal distribution \(p_{g}\). With a discount factor \((0,1)\), it maximizes \(J()=_{g p_{g},(g)}[_{t=0}^{T-1}^{t}  R(s_{t},a_{t},s_{t+1},g)]\).

In the context of model-based reinforcement learning (MBRL), a world model \(\) is trained over trajectories sampled from the agent's interactions with the real environment, which are stored in a replay buffer, to predict the dynamics of the real environment. Fig. 1 illustrates the general MBRL framework. We use the world model structure \(\) of Dreamer (Hafner et al., 2019, 2020, 2023) to learn real environment dynamics as a recurrent state-space model (RSSM). We provide a detailed explanation of the network architecture and working principles of the RSSM in Appendix A.1. Our study focuses on tackling the world model learning problem in goal-conditioned model based reinforcement learning settings. Particularly, we consider **GC-Dreamer** (goal-conditioned Dreamer) as an important baseline with the following learning components:

\[(s_{t}|s_{t-1},a_{t-1})^{G}(a_{t}|s_{t},g) V(s_{t},g) \]

Figure 1: The general framework of model-based RL.

In GC-Dreamer, the goal-conditioned agent \(^{G}(a|s,g)\) samples goal commands \(g G\) from the given environment goal distribution \(p_{g}\) to collect trajectories in the real world. These trajectories are used to train the world model \(\), and subsequently, \(^{G}\) is trained on imagined rollouts generated by \(\) using the model-based actor-critic algorithm in Dreamer (Hafner et al., 2020), with these two steps run in alternation. The critic estimates the sum of future rewards \(_{t}r_{t}^{G}\), and the actor tries to maximize the predicted values from the critic. The goal-reaching reward \(r^{G}\) is defined by the self-supervised temporal distance network \(D_{t}\)(Mendonca et al., 2021), i.e. \(r^{G}(s,g)=-D_{t}(s,g)\). \(D_{t}\) predicts the anticipated number of action steps needed to transition from \(s\) to \(g\). Essentially, \(^{G}\) is reinforced to minimize the action steps required to transition from the current state \(s\) to a sampled goal state \(g\). The temporal distance estimator \(D_{t}\) is trained by extracting pairs of states \(s_{t}\) and \(s_{t+k}\) from an imagined rollout generated by running the policy over the world model and predicting the distance \(k\) between them as follows:

\[D_{t}(s_{t}),(s_{t+k}) k/H \]

Here, \(\) represents the preprocessing for imagined states, such as transforming them into the world model's latent space (we assume \(S=G\) in the paper). \(H\) represents the total length of the imagined rollout. Further details on the training procedure of \(D_{t}\) can be found in Appendix A.2.

## 3 Training World Models for Unconstrained Goal Navigation

In this section, we introduce MUN, our main approach to addressing the core challenge in GCRL: efficient exploration in long-horizon, sparse-reward environments. Our approach focuses on enhancing the agent's understanding of the real-world environment through improved dynamic (world) modeling and latent space representation. As the quality of the world model improves, the goal-conditioned policy developed from it generalizes more effectively to the real environment. By closing the generalization gap between the policy's behavior in the real environment and the world model, MUN effectively guides the agent's exploration towards the desired goal region in the real environment.

### Training Generalizable World Models

Fig 1 illustrates the general framework of Model-based RL, where world models are trained using agent's experiences stored in a replay buffer populated with observed environment transitions \((s_{t},a_{t},s_{t+1})\) linking the environment's future states \(s_{t+1}\) and past states \(s_{t}\) along with the corresponding control actions \(a_{t}\). The richness of the environment space and dynamic transitions captured by the replay buffer define the extent of what a world model can learn about the real environment. Through supervised learning, the model can generalize reasonably well within the state space moving _forward_ along the trajectories recorded in the replay buffer. However, it may be inaccurate for the state transitions moving _backward_ along the recorded trajectories or _across_ different trajectories. Consider the task of stacking blocks using a robot manipulator in Fig. 2(a). When humans learn to stack blocks, they also understand how to reverse the process to unstack the blocks or return to the initial state. In contrast, a world model trained solely on data from policies under training for stacking

Figure 2: In Fig. 2(a), we illustrate the key states involved in completing the task of 3-block stacking. In Fig. 2(b), we demonstrate the significant advantages of the bidirectional replay buffer used in MUN over traditional methods in learning world models.

is unlikely to accurately model the unstacking process. As a result, the model may yield hallucinated trajectories for training policies, causing a significant discrepancy between the policy's behavior in the model and in the real world, thereby leading to ineffective exploration.

To improve model generalizability, in MUN, we proposed to learn world models capable of characterizing state transitions between any states in the replay buffer, whether by tracing back along recorded trajectories or transitioning between states on separate trajectories. Fig. 2(b) visualizes the comparison between the bidirectional replay buffer for learning world models used in MUN and the unidirectional replay buffer in conventional model-based algorithms. The bidirectional replay buffer not only covers a wider observation space but also captures a richer set of dynamic transitions. As discussed in Sec. 2, due to joint optimization, the richer set of dynamic transitions in MUN allows for a more reliable latent representation of the environmental space and consequently a higher quality reward function (Equation 2) for training policies generalizable to the real environment on top of the learned model.

We depict the learning algorithm in MUN in Algorithm 1. In the algorithm, we maintain \(G_{subgoals}\) as a set of pivot subgoal states sampled from the relay buffer (illustrated in Algorithm 2) and aim to learn world models capable of seamless transitions between these subgoals. At line 6, we periodically update \(G_{subgoals}\) as the training evolves. In the loop starting from line 8, we repeatedly sample \(N_{s}\) subgoals from \(G_{}\) and direct the agent to sequentially reach these subgoals within a time limit of \(T_{s}\) steps for each. In this way, MUN samples a replay buffer that records bidirectional state transitions between the subgoals in \(G_{subgoals}\). Based on our experience, we find that setting \(N_{s}=2\) is sufficient. Further discussion on the setting of \(N_{s}\) is provided in Sec 4. At line 18, we train the world model \(\) using trajectories collected by both the goal commands from \(G_{subgoals}\) (stored in \(D_{DAD}\)) and that sampled from the environment goal distribution \(p_{g}\) (stored in \(D_{egc}\)). Then, we sample imaginary rollouts from the world model for policy training at line 19.

```
1:Input: Policy \(^{G}\), World Model \(\), reward function \(r^{G}\), subgoals transfer number \(N_{s}\), subgoal time limit \(T_{s}\)
2:Initialize buffers \(D,D_{DAD},D_{egc}\)
3:for\(i=1\) to \(N_{train}\)do
4:if Should Plan Subgoals then
5:\(B_{egc}\) A batch of episodes from \(D_{egc}\)
6:\(G_{subgoals}\) DAD(\(B_{egc}\)) with Algorithm 2
7:Initialize empty trajectory \(\)
8:for\(s=1\) to \(N_{s}\)do
9:\(t_{s}=0\)
10:\(g_{s}=\) Sample a subgoal randomly from \(G_{subgoals}\)
11:while agent has not reached \(g_{s}\) and \(t_{s}<T_{s}\)do
12: Append one step in real environment with \(^{G}\) using goal \(g_{s}\) to \(\)
13:\(t_{s} t_{s}+1\)
14:\(D_{DAD}\)\(D_{DAD}\{\}\)
15:\(^{}\) Trajectory of \(^{G}\) sampled using the environment goal distribution \(g p_{g}\)
16:\(D_{egc} D_{egc}^{}\)
17:\(D D_{DAD} D_{egc}\)
18: Update \(\) with \(D\)
19: Update \(^{G}\) in imagination with \(\) to maximize \(r^{G}\)
```

**Algorithm 1** The main training framework of MUN

**Comparison with Go-Explore.** We highlight the key difference between MUN's exploration strategy and the recently popular "Go-Explore" strategy (Ecoffet et al., 2019; Pislar et al., 2021; Tuyls et al., 2022; Hu et al., 2023), designed for exploration-extensive long-term GCRL settings. In Go-Explore, each training episode comprises two phases: the "Go-phase" and the "Explore-phase". During the "Go-phase," the goal-conditioned policy \(_{G}\) directs the agent to an "interesting" goal (e.g., states with low frequency of occurrence in the replay buffer) (Pong et al., 2019; Pitis et al., 2020), resulting in a final state \(s_{T_{g}}\) after \(T_{g}\) steps. Following this, the "Explore-phase" begins, where an undirected exploration policy takes over from \(s_{T_{g}}\) for the remaining \(T_{e}\) timesteps. This exploration policy is trained to maximize an intrinsic exploration reward (e.g., to visit areas of the real world that the World

[MISSING_PAGE_FAIL:5]

hallways within a maze structure. The **Walker** task involves a two-legged robot learning to control its leg joints effectively to achieve stable walking to reach goals along a flat plane forward or backward. In **3-Block Stacking**, a robot arm with a two-fingered gripper operates on a tabletop with three blocks. The goal is to stack the blocks into a tower configuration. The agent needs to learn to push, pick, and stack objects while discovering complex action sequences to complete the task in the environment. Previous solutions have relied on methods like demonstrations, curriculum learning, or extensive simulator data, highlighting the task's difficulty (Ecoffet et al., 2019; Li et al., 2020; Nair et al., 2018; Lanier, 2019). The **Block Rotation** and **Pen Rotation** tasks require the agent to manipulate a block and a pen, respectively, to achieve a randomly specified orientation along all axes. Pen Rotation is particularly challenging due to the pen's thinness, requiring precise control to prevent it from dropping. In **Fetch Slide**, a manipulator slides a puck to a designated goal area on a slippery table. Unlike tasks that involve direct manipulation, Fetch Slide emphasizes the challenge of accurately controlling the force and direction of the push operation, as the puck must slide across the flat surface to the target. See Appendix. **C** for more information about environments.

### Baselines

We compare MUN with the following baselines. The **GC-Dreamer** baseline is discussed in Sec. 2. We include two baselines based on the Go-Explore strategy (Ecoffet et al., 2019) that has been proved efficient in the GCRL setting: MEGA (Pitis et al., 2020) and PEG (Hu et al., 2023). A Go-Explore agent firstly uses its goal-conditioned policy \(^{G}\) to approach a sampled exploration-inducing goal command \(g\), referred to as the Go-phase. In the Explore-phase, it activates an exploration policy \(^{E}\) to explore the environment from the terminal state of the Go-phase. In contrast, MUN improves the generalization of world models to facilitate effective real-world environment exploration. During training, MUN collects trajectories that navigate between two goal states sampled from its candidate subgoal set, essentially replacing the "Explore-phase" in "Go-Explore" with another "Go-phase". MEGA commands the agent to rarely seen states at the frontier by using kernel density estimates (KDE) of state densities and chooses low-density goals from the replay buffer. PEG selects goal commands to guide an agent's goal-conditioned policy toward states with the highest exploration potential given its current level of training. This potential is defined as the expected accumulated exploration reward during the Explore-phase. Similar to MUN, our baseline methods, named **PEG-G** and **MEGA-G**, augment **GC-Dreamer** with the PEG and MEGA Go-Explore strategies, respectively. In these methods, the replay buffer \(D\) contains not only trajectories sampled by the GCRL policy \(_{G}\) commanded by environment goals but also exploratory trajectories sampled using the Go-Explore strategies. The exploration policy \(^{E}\) in **PEG-G** and **MEGA-G** is the Plan2Explore policy from Sekar et al. (2020), which encourages the agent to actively search for states that induce disparities among an ensemble of world models.

We note that MUN and the baselines are all implemented based on the Dreamer framework as realized in **GC-Dreamer1**.

### Results

Fig. 4 shows the evaluation performance of MUN and all baselines across training. MUN demonstrates superior performance compared to the baseline models, excelling in both the final success rate and the speed of learning. MUN outperforms the Go-Explore baselines (MEGA-G and PEG-G) across all tasks, demonstrating the effectiveness of the exploration strategy in MUN over the alternative Go-Explore strategies. In the most challenging tasks--block stacking, block rotation, and pen rotation--MUN shows a significant margin of superiority. For example, MUN achieves over 95%

Figure 3: We evaluate MUN on 6 environments: Ant Maze, Walker, 3-Block Stacking, Block Rotation, Pen Rotation, Fetch Slide.

success rate on 3 block stacking, while all other baselines only manage to achieve around 60% success rate on this task within 2.5M steps. MEGA-G and PEG-G heuristically pick exploration-inducing goals to initiate exploration by a separate policy. Since finding a goal state that is optimally aligned with both the goal-conditioned policy and the exploration policy is challenging, these methods can result in suboptimal goals, thereby slowing down exploration. GC-Dreamer lacks a Go-Explore phase, which limits its exploration potential. Despite this, it can still perform comparably to or even better than MEGA-G and PEG-G in certain contexts. This indicates that the Go-Explore strategy does not always guarantee improved exploration, and suboptimal goal-setting during the "Go-phase" can hinder exploration (see 3 block stacking).

Fetch Slide is a non-prehensile manipulation task. This environment has asymmetric state transitions: when the puck is slid outside the robot's workspace, the manipulator cannot reach the puck's position to slide it backward due to physical constraints. MUN still outperforms the other baselines in this environment. We found MUN, with the DAD strategy, can discover key subgoals for this task, like contacting the puck, placing the manipulator at different angles around the puck, and stabilizing the manipulator upon reaching the goal (these key states result from distinct actions). MUN enables learning state transitions between these key subgoals to discover the high-level task structure. It learns a generalizable world model that handles sliding the puck between any positions within the workspace and predicts low probabilities for infeasible transitions from puck positions outside the workspace. Particularly, it enables the agent to hit the puck multiple times if it is within its workspace, thereby improving task success rates. That said, the current goal selection mechanism in MUN lacks a process to filter out infeasible goals from the current state, which could adversely affect sample efficiency. We left addressing this limitation and implementing a robust filtering mechanism for infeasible goals as a focus for future work.

We studied the prediction error of learned world models in MUN and the baselines. Fig. 5 shows the one-step model prediction error throughout the training steps. The world models trained by MUN

Figure 4: Experiment results comparing MUN with the baselines over 5 random seeds.

Figure 5: The world model prediction error curves throughout the training steps for 3-Block Stacking and Pen Rotation.

show a much smaller generalization gap to the real environment compared to the baselines across the training steps. Consequently, MUN can effectively leverage these higher-quality world models to train policies that generalize better to the real environment. We present a quantitative comparison of the world model prediction quality between MUN and the baselines in terms of model prediction _compounding error_ in Appendix F.3.

### Can DAD find key subgoals?

We visualize several subgoals found by the DAD algorithm during the training process in Fig. 6 for three environments: Ant-Maze, Walker, 3-Block Stacking. In **Walker**, DAD successfully identifies the crucial joint angles and forces of the Walker robot during its forward locomotion, including standing, striding, jumping, landing, and leg support. In **Ant-Maze**, DAD recognizes significant motion variations at corridor corners. In **3-Block Stacking**, DAD successfully identifies crucial state transitions required during the stacking process. These critical subgoals include block grasping, lifting, horizontal movement, vertical movement, and gripper release. For more discussion about subgoals found by the DAD in other environments, please refer to Appendix F.1.

### Can MUN navigate between arbitrary subgoals?

As MUN is capable of identifying pivotal subgoal states necessary for complex tasks and training world models and policies for seamless transitions between these subgoals, we investigate MUN's capacity to generalize to new task settings concerning important subgoals. We set the initial state of the agent at one random subgoal and command it to reach another random subgoal. Such task setting is _not provided to the agent during training_. For the 3 Block Stacking task, we employ a set of 15 manually created subgoals representing various critical states in the block-stacking process, resulting in 225 unique combinations of initial states and test goals for evaluation. Each combination undergoes 10 repeated evaluations, totaling 2250 evaluation trajectories. These evaluations encompass both the forward and reverse processes of stacking and unstacking blocks, assessing the agent's proficiency in both task completion and restoration. For example, in the left portion of Fig. 7, we visualize some subgoals used as initial task state in the upper part and some subgoals used as evaluation test goals in the lower part. The right section of Fig. 7 illustrates MUN's superiority over the other baselines in these evaluation experiments, achieving the highest success rate through its ability to develop

Figure 6: Key subgoals found by DAD (Algorithm 2) in three environments: Ant-Maze, Walker, 3-Block Stacking. They present the important landmarks on the path to the task goal regions.

Figure 7: Experiment setup and results of navigation between any pair of subgoals in the 3-Block Stacking environment. In the left part, the bottom section of each image depicts the ultimate evaluation goal for one evaluation episode, while the top section illustrates the manually set initial state. The right part shows the evaluation success rates.

a robust and adaptable world model that generalizes to novel tasks. Additional results in different environments are provided in Appendix F.2.

### Ablation study

We conducted the following ablation studies to investigate MUN's exploration goal selection mechanism. First, we investigated the effect of the number of subgoal states (\(N_{s}\)) in our algorithm. MUN sequentially traverses \(N_{s}=2\) goal states sampled from the replay buffer to explore the environment during each training episode. We introduced an ablation **MUN-Ns-3** that sets \(N_{s}=3\). This ablation aims to investigate whether increasing \(N_{s}\) leads to improved learning performance. Second, we considered an ablated version of MUN, named **MUN-noDAD**, which replaces the goal sampling strategy DAD (Algorithm 2) with a simple method that chooses goal states with fixed time interval in trajectories sampled from the replay buffer. This ablation investigates the importance of identifying key subgoal states, which represent pivotal milestones necessary to complete a complex task. It seeks to determine whether training world models from state transitions between these key states in MUN is essential, or if using any states from the replay buffer would suffice. Lastly, we explored an alternative key subgoal discovery strategy. MUN identifies key subgoals for exploration as states in the replay buffer that result in distinct actions within the action space. We introduced an ablation, **MUN-KeyObs**, which directly discovers key subgoals from the state space by identifying centroids of (latent) state clusters in the replay buffer, following the strategy in Zhang et al. (2021).

The results are depicted in Fig. 8. MUN outperforms all ablated versions. Setting \(N_{s}=3\) slows down the training performance, supporting our claim it suffices to set \(N_{s}=2\). The performance of MUN-noDAD and MUN-KeyObs does not match MUN, especially in the 3 Block Stacking environment, highlighting that discovering key subgoals in the action space (the DAD strategy) indeed contributes to higher performance and efficiency. It is noteworthy that the ablation methods achieve a relatively small gap in success rates compared to MUN in the challenging Block Rotation and Pen Rotation environments. This suggests that MUN's approach to learning a world model from state transitions between any states in the replay buffer (whether tracing back along recorded trajectories or transitioning across separate trajectories) alone is effective in bridging the generalization gap between the model and the real environment.

## 5 Related Work

Model-based reinforcement learning (MBRL) is a promising approach to reinforcement learning that learns a model of the environment and uses it to plan actions (Sutton, 1991; Deisenroth and Rasmussen, 2011; Oh et al., 2017; Chua et al., 2018). It has achieved remarkable success in numerous control tasks and games, such as chess (Silver et al., 2017; Schrittwieser et al., 2020; Xu et al., 2022), Atari games (Hafner et al., 2020; Schrittwieser et al., 2020; Oh et al., 2017), continuous control tasks (Kurutach et al., 2018; Buckman et al., 2018; Hafner et al., 2019; Janner et al., 2019), and robotic manipulation tasks (Lowrey et al., 2018; Luo et al., 2018). The dynamic model serves as a pivotal component of model-based reinforcement learning, primarily fulfilling two key roles: planning actions (Deisenroth and Rasmussen, 2011; Oh et al., 2017; Chua et al., 2018; Lowrey et al., 2018; Hafner et al., 2019) or generating synthetic data to aid in the training of model-free reinforcement

Figure 8: Experiment results comparing MUN with its ablations over 5 random seeds.

learning algorithms (Janner et al., 2019; Hafner et al., 2020, 2023). The primary drawback of the former lies in the excessive cost associated with long-term planning. To address this issue, the concept of ensemble has been employed to enhance performance (Chua et al., 2018; Kurutach et al., 2018; Buckman et al., 2018). Oh et al. (2017) and Hansen et al. (2022b) integrate the dynamics model with a value prediction network to improve the accuracy of long-term planning. The latter also suffers from the potential bias of the model, which can result in inaccuracies in the generated data, thereby directly impacting policy learning (Luo et al., 2018; Lai et al., 2021).

Multi-goal reinforcement learning (RL) agents (Schaul et al., 2015; Plappert et al., 2018; Ghosh et al., 2019) acquire goal-conditioned behaviors capable of achieving and generalizing across diverse sets of objectives. Researchers have been continuously exploring the integration of Model-based RL and Goal-conditioned RL (Mendonca et al., 2021; Nair et al., 2020; Zhang et al., 2020), leveraging the capabilities of dynamic models in planning and generating synthetic data to enhance the training efficiency and generalization of GCRL. However, compared to traditional RL problems, GCRL faces more severe challenges regarding reward sparsity and exploration difficulties (Ren et al., 2019; Florensa et al., 2018; Trott et al., 2019). These challenges often lead to significant biases in the learned World Model, consequently impairing the performance of goal-conditioned policies (Mendonca et al., 2021; Hu et al., 2023). Pong et al. (2019) propose to learn a maximum-entropy goal distribution, Pitis et al. (2020) encourage the agent to explore goals with low frequency of occurrence in the replay buffer. Sekar et al. (2020) introduce a planning algorithm to pick goals for exploration using World Models.

World Models hold significant promise for GCRL, as they enable fast exploration and support the training of more generalized policies (McCarthy et al., 2021; Shyam et al., 2019; Hu et al., 2023; Sekar et al., 2020). Within this framework, learning a reliable World Model is essential for developing effective policies (Zhang et al., 2024; Young et al., 2022; Wang et al., 2023; Lai et al., 2021). Kauvar et al. (2023) propose a curiosity-driven exploration method, which is focused on replay buffer management. Hansen et al. (2022a) use demonstration data as a supplement to the replay buffer to learn a more reliable World Model. Previous work has often focused on devising more appropriate objectives when sampling real trajectory data from the environment to enrich the diversity of dynamic transitions in the replay buffer (Nair et al., 2020; Charlesworth and Montana, 2020; Trott et al., 2019; Florensa et al., 2018; Campero et al., 2020). However, they overlooked the overall direction of dynamic transitions within the data which extremely affects the richness of dynamic transitions to learn a comprehensive World Model.

## 6 Conclusion

In summary, we introduce MUN, a novel goal-directed exploration algorithm designed for effective world modeling of seamless transitions between arbitrary states in replay buffers, whether retracing along recorded trajectories or transitioning between states on separate trajectories. As the quality of the world model improves, MUN demonstrates high efficacy in learning goal-conditioned policies in sparse-reward environments. Additionally, we present a practical strategy DAD for identifying pivotal subgoal states, which act as critical milestones in completing complex tasks. The experimental results underscored the effectiveness of MUN in strengthening the reliability of world models and learning policies capable of adapting to novel test goals.

## Reproducibility Statement

The code for MUN is available on [https://github.com/RU-Automated-Reasoning-Group/MUN](https://github.com/RU-Automated-Reasoning-Group/MUN). For hyperparameter settings and baseline pseudocode, please refer to Appendix D and Appendix E.3.