# Explaining Predictive Uncertainty with

Information Theoretic Shapley Values

David S. Watson

King's College London

david.watson@kcl.ac.uk

&Joshua O'Hara

King's College London

&Niek Tax

Meta, Central Applied Science &Richard Mudd

Meta, Central Applied Science

&Ido Guy

Meta, Central Applied Science

###### Abstract

Researchers in explainable artificial intelligence have developed numerous methods for helping users understand the predictions of complex supervised learning models. By contrast, explaining the _uncertainty_ of model outputs has received relatively little attention. We adapt the popular Shapley value framework to explain various types of predictive uncertainty, quantifying each feature's contribution to the conditional entropy of individual model outputs. We consider games with modified characteristic functions and find deep connections between the resulting Shapley values and fundamental quantities from information theory and conditional independence testing. We outline inference procedures for finite sample error rate control with provable guarantees, and implement efficient algorithms that perform well in a range of experiments on real and simulated data. Our method has applications to covariate shift detection, active learning, feature selection, and active feature-value acquisition.

## 1 Introduction

Machine learning (ML) algorithms can solve many prediction tasks with greater accuracy than classical methods. However, some of the most popular and successful algorithms, such as deep neural networks, often produce models with millions of parameters and complex nonlinearities. The resulting "black box" is essentially unintelligible to humans. Researchers in explainable artificial intelligence (XAI) have developed numerous methods to help users better understand the inner workings of such models (see Sect. 2).

Despite the rapid proliferation of XAI tools, the goals of the field have thus far been somewhat narrow. The vast majority of methods in use today aim to explain model _predictions_, i.e. point estimates. But these are not necessarily the only model output of interest. Predictive _uncertainty_ can also vary widely across the feature space, in ways that may have a major impact on model performance and human decision making. Such variation makes it risky to rely on the advice of a black box, especially when generalizing to new environments. Discovering the source of uncertainty can be an important first step toward reducing it.

Quantifying predictive uncertainty has many applications in ML. For instance, it is an essential subroutine in any task that involves exploration, e.g. active learning [18; 36], multi-armed bandits [73; 40], and reinforcement learning more generally [55; 76]. Other applications of predictiveuncertainty quantification include detecting covariate shift  and adversarial examples , as well as classification with reject option . Our method aims to expand the scope of XAI to these varied domains by explaining predictive uncertainty via feature attributions.

Knowing the impact of individual features on local uncertainty can help drive data collection and model design. It can be used to detect the source of a suspected covariate shift, select informative features, and test for heteroskedasticity. Our attribution strategy makes use of the Shapley value framework for XAI [83; 44; 74; 49; 9], a popular approach inspired by cooperative game theory, which we adapt by altering the characteristic function and augment with inference procedures for provable error rate control. The approach is fully model-agnostic and therefore not limited to any particular function class.

Our main contributions are threefold: (1) We describe modified variants of the Shapley value algorithm that can explain higher moments of the predictive distribution, thereby extending its explanatory utility beyond mere point estimates. We provide an information theoretic interpretation of the resulting measures and study their properties. (2) We introduce a split conformal inference procedure for Shapley variables with finite sample coverage guarantees. This allows users to test the extent to which attributions for a given feature are concentrated around zero with fixed type I error control. (3) We implement model-specific and model-agnostic variants of our method and illustrate their performance in a range of simulated and real-world experiments, with applications to feature selection, covariate shift detection, and active learning.

## 2 Related Work

XAI has become a major subfield of machine learning in recent years. The focus to date has overwhelmingly been on explaining predictions in supervised learning tasks, most prominently via feature attributions [64; 44; 75], rule lists [65; 38; 71], and counterfactuals [84; 50; 34]. Despite obvious differences between these methods, all arguably share the same goal of identifying minimal conditions sufficient to alter predictions in some pre-specified way . Resulting explanations should be accurate, simple, and relevant for the inquiring agent .

Quantifying inductive uncertainty is a fundamental problem in probability theory and statistics, although machine learning poses new challenges and opportunities in this regard . The classical literature on this topic comes primarily from Bayesian modeling  and information theory , which provide a range of methods for analyzing the distribution of random variables. More recent work on conformal inference [82; 41; 4] has expanded the toolkit for practitioners.

Important application areas for these methods include active learning (AL) and covariate shift detection. In AL, the goal is to selectively query labels for unlabeled instances aimed to maximize classifier improvement under a given query budget. Methods often select instances on which the model has high epistemic (as opposed to aleatoric) uncertainty , as for example in BatchBALD . This is especially valuable when labels are sparse and costly to collect while unlabeled data is widely available. In covariate shift detection, the goal is to identify samples that are abnormal relative to the in-distribution observations that the classifier has seen during training. It is well-known that neural networks can be overconfident , yielding predictions with unjustifiably high levels of certainty on test samples. Addressing this issue is an active area of research, and a variety of articles take a perspective of quantifying epistemic uncertainty [30; 56]. In safety-critical applications, the degree of model uncertainty can be factored into the decision making, for example by abstaining from prediction altogether when confidence is sufficiently low .

Very little work has been done on explaining predictive uncertainty. A notable exception is the CLUE algorithm [3; 42], a model-specific method designed for Bayesian deep learning, which generates counterfactual samples that are maximally similar to some target observation but optimized for minimal conditional variance. This contrasts with our feature attribution approach, which is model-agnostic and thereby the first to make uncertainty explanations available to function classes beyond Bayesian deep learning models.

Predictive uncertainty is often correlated with prediction loss, and therefore explanations of model errors are close relatives of our method. LossSHAP  is an extension of Lundberg and Lee 's SHAP algorithm designed to explain the pointwise loss of a supervised learner (e.g., squared error or cross entropy). Though this could plausibly help identify regions where the model is least certain about predictions, it requires a large labelled test dataset, which may not be available in practice. By contrast, our method only assumes access to some unlabelled dataset of test samples, which is especially valuable when labels are slow or expensive to collect. For instance, LossSHAP is little help in learning environments where covariate shift is detectable before labels are known . This is common, for example, in online advertising , where an impression today may lead to a conversion next week but quick detection (and explanation) of covariate shift is vital.

Previous authors have explored information theoretic interpretations of variable importance measures; see [16, Sect. 8.3] for a summary. These methods often operate at global resolutions--e.g., Sobol' indices  and SAGE --whereas our focus is on local explanations. Alternatives such as INVASE  must be trained alongside the supervised learner itself and are therefore not model-agnostic. L2X , REAL-X , and SHAP-KL  provide post-hoc local explanations, but they require surrogate models to approximate a joint distribution over the full feature space. Chen et al.  propose an information theoretic variant of Shapley values for graph-structured data, which we examine more closely in Sect. 4.

## 3 Background

Notation.We use uppercase letters to denote random variables (e.g., \(X\)) and lowercase for their values (e.g., \(x\)). Matrices and sets of random variables are denoted by uppercase boldface type (e.g., \(\)) and vectors by lowercase boldface (e.g., \(\)). We occasionally use superscripts to denote samples, e.g. \(^{(i)}\) is the \(i^{}\) row of \(\). Subscripts index features or subsets thereof, e.g. \(_{S}=\{X_{j}\}_{j S}\) and \(_{S}^{(i)}=\{x_{j}^{(i)}\}_{j S}\), where \(S[d]=\{1,,d\}\). We define the complementary subset \(=[d] S\).

Information Theory.Let \(p,q\) be two probability distributions over the same \(\)-algebra of events. Further, let \(p,q\) be absolutely continuous with respect to some appropriate measure. We make use of several fundamental quantities from information theory , such as entropy \(H(p)\), cross entropy \(H(p,q)\), KL-divergence \(D_{KL}(p q)\), and mutual information \(I(X;Y)\) (all formally defined in Appx. B.1). We use shorthand for the (conditional) probability mass/density function of the random variable \(Y\), e.g. \(p_{Y}|_{_{S}}:=p(Y_{S}=_{S})\). We speak interchangeably of the entropy of a random variable and the entropy of the associated mass/density function: \(H(Y x)=H(p_{Y|x})\). We call this the _local_ conditional entropy to distinguish it from its global counterpart, \(H(Y X)\), which requires marginalization over the joint space \(\).

Shapley Values.Consider a supervised learning model \(f\) trained on features \(^{d}\) to predict outcomes \(Y\). We assume that data are distributed according to some fixed but unknown distribution \(\). Shapley values are a feature attribution method in which model predictions are decomposed as a sum: \(f()=_{0}+_{j=1}^{d}(j,)\), where \(_{0}\) is the baseline expectation (i.e., \(_{0}=_{}[f()]\)) and \((j,)\) denotes the Shapley value of feature \(j\) at point \(\). To define this quantity, we require a value function \(v:2^{[d]}^{d}\) that quantifies the payoff associated with subsets \(S[d]\) for a particular sample. This characterizes a cooperative game, in which each feature acts as a player. A common choice for defining payoffs in XAI is the following :

\[v_{0}(S,):=_{}f()_{S}=_{S},\]

where we marginalize over the complementary features \(\) in accordance with reference distribution \(\). For any value function \(v\), we may define the following random variable to represent \(j\)'s marginal contribution to coalition \(S\) at point \(\):

\[_{v}(S,j,):=v(S\{j\},)-v(S,).\]

Then \(j\)'s Shapley value is just the weighted mean of this variable over all subsets:

\[_{v}(j,):=_{S[d]\{j\}}\;_{v}(S,j,). \]

It is well known that Eq. 1 is the unique solution to the attribution problem that satisfies certain desirable properties, including efficiency, symmetry, sensitivity, and linearity  (for formal statements of these axioms, see Appx. B.2.)

## 4 Alternative Value Functions

To see how standard Shapley values can fall short, consider a simple data generating process with \(X,Z(0,1)^{2}\) and \(Y(X,Z^{2})\). Since the true conditional expectation of \(Y\) is \(X\), this feature will get 100% of the attributions in a game with payoffs given by \(v_{0}\). However, just because \(Z\) receives zero attribution does not mean that it adds no information to our predictions--on the contrary, we can use \(Z\) to infer the predictive variance of \(Y\) and calibrate confidence intervals accordingly. This sort of higher order information is lost in the vast majority of XAI methods.

We consider information theoretic games that assign nonzero attribution to \(Z\) in the example above, and study the properties of resulting Shapley values. We start in an idealized scenario in which we have: (i) oracle knowledge of the joint distribution \(\); and (ii) unlimited computational budget, thereby allowing complete enumeration of all feature subsets.

INVASE  is a method for learning a relatively small but maximally informative subset of features \(S[d]\) using the loss function \(D_{KL}(p_{Y|} p_{Y|_{S}})+|S|\), where \(\) is a regularization penalty. Jethani et al. 's SHAP-KL adapts this loss function to define a new game:

\[v_{KL}(S,):=-D_{KL}(p_{Y|} p_{Y|_{S}}),\]

which can be interpreted as \(-1\) times the excess number of bits one would need on average to describe samples from \(Y\) given code optimized for \(Y_{S}\).

Chen et al.  make a similar proposal, replacing KL-divergence with cross entropy:

\[v_{CE}(S,):=-H(p_{Y|},p_{Y|_{S}}).\]

This value function is closely related to that of LossSHAP , which for likelihood-based loss functions can be written:

\[v_{L}(S,):=- p(Y=y_{S}),\]

where \(y\) denotes the true value of \(Y\) at the point \(\). As Covert et al.  point out, this is equivalent to the pointwise mutual information \(I(y;_{S})\), up to an additive constant. However, \(v_{L}\) requires true labels for \(Y\), which may not be available when evaluating feature attributions on a test set. By contrast, \(v_{CE}\) averages over \(\), thereby avoiding this issue: \(v_{CE}(S,)=-_{Y}v_{L}(S,) \). We reiterate that in all cases we condition on some fixed value of \(\) and do not marginalize over the feature space \(\). This contrasts with global feature attribution methods like SAGE , which can be characterized by averaging \(v_{L}\) over the complete joint distribution \(p(,Y)\).

It is evident from the definitions that \(v_{KL}\) and \(v_{CE}\) are equivalent up to an additive constant not depending on \(S\), namely \(H(p_{Y|})\). This renders the resulting Shapley values from both games identical (all proofs in Appx. A.)1

**Proposition 4.1**.: _For all features \(j[d]\), coalitions \(S[d]\{j\}\), and samples \(_{X}\):_

\[_{KL}(S,j,) =_{CE}(S,j,)\] \[=_{}p(y) _{S},x_{j})}{p(y_{S})}\;dy.\]

This quantity answers the question: if the target distribution were \(p_{Y|}\), how many more bits of information would we get on average by adding \(x_{j}\) to the conditioning event \(_{S}\)? Resulting Shapley values summarize each feature's contribution in bits to the distance between \(Y\)'s fully specified local posterior distribution \(p(Y)\) and the prior \(p(Y)\).

**Proposition 4.2**.: _With \(v\{v_{KL},v_{CE}\}\), Shapley values satisfy \(_{j=1}^{d}_{v}(j,)=D_{KL}(p_{Y|} p_{Y})\)._

We introduce two novel information theoretic games, characterized by negative and positive local conditional entropies:

\[v_{IG}(S,):=-H(Y_{S}), v_{H}(S,):=H(Y _{S}).\]The former subscript stands for information gain; the latter for entropy. Much like \(v_{CE}\), these value functions can be understood as weighted averages of LossSHAP payoffs over \(\), however this time with expectation over a slightly different distribution: \(v_{IG}(S,)=-v_{H}(S,)=-_{Y|_{S}} v_{L}(S,)\). The marginal contribution of feature \(j\) to coalition \(S\) is measured in bits of local conditional mutual information added or lost, respectively (note that \(_{IG}=-_{H}\)).

**Proposition 4.3**.: _For all features \(j[d]\), coalitions \(S[d]\{j\}\), and samples \(_{X}\):_

\[_{IG}(S,j,)=I(Y;x_{j}_{S}).\]

This represents the decrease in \(Y\)'s uncertainty attributable to the conditioning event \(x_{j}\) when we already know \(_{S}\). This quantity is similar (but not quite equivalent) to the _information gain_, a common optimization objective in tree growing algorithms . The difference again lies in the fact that we do not marginalize over \(\), but instead condition on a single instance. Resulting Shapley values summarize each feature's contribution in bits to the overall local information gain.

**Proposition 4.4**.: _Under \(v_{IG}\), Shapley values satisfy \(_{j=1}^{d}_{IG}(j,)=I(Y;)\)._

In the classic game \(v_{0}\), out-of-coalition features are eliminated by marginalization. However, this will not generally work in our information theoretic games. Consider the modified entropy game, designed to take \(d\)-dimensional input:

\[v_{H^{*}}(S,):=_{}H(p_{Y|}) _{S}=_{S}.\]

This game is not equivalent to \(v_{H}\), as shown in the following proposition.

**Proposition 4.5**.: _For all coalitions \(S[d]\) and samples \(_{X}\):_

\[v_{H}(S,)-v_{H^{*}}(S,)=D_{KL}(p_{Y|_{S}, _{S}} p_{Y|_{S}}).\]

The two value functions will tend to diverge when out-of-coalition features \(_{}\) inform our predictions about \(Y\), given prior knowledge of \(_{S}\). Resulting Shapley values represent the difference in bits between the local and global conditional entropy.

**Proposition 4.6**.: _Under \(v_{H^{*}}\), Shapley values satisfy \(_{j=1}^{d}_{H^{*}}(j,)=H(Y)-H(Y)\)._

In other words, \(_{H^{*}}(j,)\) is \(j\)'s contribution to conditional entropy at a given point, compared to a global baseline that averages over all points.

These games share an important and complex relationship to conditional independence structures. We distinguish here between global claims of conditional independence, e.g. \(Y\!\!\! X Z\), and local or context-specific independence (CSI), e.g. \(Y\!\!\! X z\). The latter occurs when \(X\) adds no information about \(Y\) under the conditioning event \(Z=z\) (see Appx. B.1 for an example).

**Theorem 4.7**.: _For value functions \(v\{v_{KL},v_{CE},v_{IG},v_{H}\}\), we have:_

1. \(Y\!\!\! X_{j}_{S}_{ X }|_{v}(S,j,)|=0\)_._
2. \(Y\!\!\! X_{j}_{S}_{v}(S,j,)=0\)_._
3. _The set of distributions such that_ \(_{v}(S,j,)=0 Y X_{j}_{S}\) _is Lebesgue measure zero._

Item (a) states that \(Y\) is conditionally independent of \(X_{j}\) given \(_{S}\) if and only if \(j\) makes no contribution to \(S\) at any point \(\). Item (b) states that the weaker condition of CSI is sufficient for zero marginal payout. However, while the converse does not hold in general, item (c) states that the set of counterexamples is _small_ in a precise sense--namely, it has Lebesgue measure zero. A similar result holds for so-called _unfaithful_ distributions in causality , in which positive and negative effects cancel out exactly, making it impossible to detect certain graphical structures. Similarly, context-specific dependencies may be obscured when positive and negative log likelihood ratios cancel out as we marginalize over \(\). Measure zero events are not necessarily harmless, especially when working with finite samples. Near violations may in fact be quite common due to statistical noise . Together, these results establish a powerful, somewhat subtle link between conditional independencies and information theoretic Shapley values. Similar results are lacking for the standard value function \(v_{0}\)--with the notable exception that conditional independence implies zero marginal payout --an inevitable byproduct of the failure to account for predictive uncertainty.

Method

The information theoretic quantities described in the previous section are often challenging to calculate, as they require extensive conditioning and marginalization. Computing some \((2^{d})\) such quantities per Shapley value, as Eq. 1 requires, quickly becomes infeasible. (See  for an in-depth analysis of the time complexity of Shapley value algorithms.) Therefore, we make several simplifying assumptions that strike a balance between computational tractability and error rate control.

First, we require some uncertainty estimator \(h:_{ 0}\). Alternatively, we could train new estimators for each coalition ; however, this can be impractical for large datasets and/or complex function classes. In the previous section, we assumed access to the true data generating process. In practice, we must train on finite samples, often using outputs from the base model \(f\). In the regression setting, this may be a conditional variance estimator, as in heteroskedastic error models ; in the classification setting, we assume that \(f\) outputs a pmf over class labels and write \(f_{y}:^{d}\) to denote the predicted probability of class \(y\). Then predictive entropy is estimated via the plug-in formula \(h_{t}():=-_{y}f_{y}() f_{y}()\), where the subscript \(t\) stands for _total_.

In many applications, we must decompose total entropy into epistemic and aleatoric components--i.e., uncertainty arising from the model or the data, respectively. We achieve this via ensemble methods, using a set of \(B\) basis functions, \(\{f^{1},,f^{B}\}\). These may be decision trees, as in a random forest , or subsets of neural network nodes, as in Monte Carlo (MC) dropout . Let \(f^{b}_{y}()\) be the conditional probability estimate for class \(y\) given sample \(\) for the \(b^{}\) basis function. Then aleatoric uncertainty is given by \(h_{a}():=-_{b=1}^{B}_{y}f^{b}_{y}( ) f^{b}_{y}()\). Epistemic uncertainty is simply the difference , \(h_{e}():=h_{t}()-h_{a}()\).2 Alternative methods may be appropriate for specific function classes, e.g. Gaussian processes  or Bayesian deep learning models . We leave the choice of which uncertainty measure to explain up to practitioners. In what follows, we use the generic \(h()\) to signify whichever estimator is of relevance for a given application.

We are similarly ecumenical regarding reference distributions. This has been the subject of much debate in recent years, with authors variously arguing that \(\) should be a simple product of marginals ; or that the joint distribution should be modeled for proper conditioning and marginalization ; or else that structural information should be encoded to quantify causal effects . Each approach makes sense in certain settings , so we leave it up to practitioners to decide which is most appropriate for their use case. We stress that information theoretic games inherit all the advantages and disadvantages of these samplers from the conventional XAI setting, and acknowledge that attributions should be interpreted with caution when models are forced to extrapolate to off-manifold data . Previous work has shown that no single sampling method dominates, with performance varying as a function of data type and function class ; see  for a discussion.

Finally, we adopt standard methods to efficiently sample candidate coalitions. Observe that the distribution on subsets implied by Eq. 1 induces a symmetric pmf on cardinalities \(|S|\{0,,d-1\}\) that places exponentially greater weight at the extrema than it does at the center. Thus while there are over 500 billion coalitions at \(d=40\), we can cover 50% of the total weight by sampling just over 0.1% of these subsets (i.e., those with cardinality \( 9\) or \( 30\)). To reach 90% accuracy requires just over half of all coalitions. In fact, under some reasonable conditions, sampling \((n)\) coalitions is asymptotically optimal, up to a constant factor . We also employ the paired sampling approach of Covert and Lee  to reduce variance and speed up convergence still further.

Several authors have proposed inference procedures for Shapley values . These methods could in principle be extended to our revised games. However, existing algorithms are typically either designed for local inference, in which case they are ill-suited to make global claims about feature relevance, or require global value functions upfront, unlike the local games we consider here. As an alternative, we describe a method for aggregating local statistics for global inference. Specifically, we test whether the random variable \((j,)\) tends to concentrate around zero for a given \(j\). We take a conformal approach  that provides the following finite sample coverage guarantee.

**Theorem 5.1** (Coverage).: _Partition \(n\) training samples \(\{(^{(i)},y^{(i)})\}_{i=1}^{n}\) into two equal-sized subsets \(_{1},_{2}\) where \(_{1}\) is used for model fitting and \(_{2}\) for computing Shapley values. Fixa target level \((0,1)\) and estimate the upper and lower bounds of the Shapley distribution from the empirical quantiles. That is, let \(_{lo}\) be the \(\)th smallest value of \((j,^{(i)}),i_{2}\), for \(=(n/2+1)(/2)\), and let \(_{hi}\) be the \(u\)th smallest value of the same set, for \(u=(n/2+1)(1-/2)\). Then for any test sample \(^{(n+1)}\), we have:_

\[(j,^{(n+1)})[_{lo},_{hi}]  1-.\]

_Moreover, if Shapley values have a continuous joint distribution, then the upper bound on this probability is \(1-+2/(n+2)\)._

Note that this is not a _conditional_ coverage claim, insomuch as the bounds are fixed for a given \(X_{j}\) and do not vary with other feature values. However, Thm. 5.1 provides a PAC-style guarantee that Shapley values do not exceed a given (absolute) threshold with high probability, or that zero falls within the \((1-) 100\%\) confidence interval for a given Shapley value. These results can inform decisions about feature selection, since narrow intervals around zero are necessary (but not sufficient) evidence of uninformative predictors. This result is most relevant for tabular or text data, where features have some consistent meaning across samples; it is less applicable to image data, where individual pixels have no stable interpretation over images.

## 6 Experiments

Full details of all datasets and hyperparameters can be found in Appx. C, along with supplemental experiments that did not fit in the main text. Code for all experiments and figures can be found in our dedicated GitHub repository.3 We use DeepSHAP to sample out-of-coalition feature values in neural network models, and TreeSHAP for boosted ensembles. Alternative samplers are compared in a separate simulation experiment below. Since our goal is to explain predictive _entropy_ rather than _information_, we use the value function \(v_{H^{*}}\) throughout, with plug-in estimators for total, epistemic, and/or aleatoric uncertainty.

### Supervised Learning Examples

First, we perform a simple proof of concept experiment that illustrates the method's performance on image, text, and tabular data.

Image Data.We examine binary classifiers on subsets of the MNIST dataset. Specifically, we train deep convolutional neural nets to distinguish 1 vs. 7, 3 vs. 8, and 4 vs. 9. These digit pairs tend to look similar in many people's handwriting and are often mistaken for one another. We therefore expect relatively high uncertainty in these examples, and use a variant of DeepSHAP to visualize the pixel-wise contributions to predictive entropy, as estimated via MC dropout. We compute attributions for epistemic and aleatoric uncertainty, visually confirming that the former identifies regions of the image that most increase or reduce uncertainty (see Fig. 1A).

Applying our method, we find that epistemic uncertainty is reduced by the upper loop of the 9, as well as by the downward hook on the 7. By contrast, uncertainty is increased by the odd angle of the 8 and its small bottom loop. Aleatoric uncertainty, by contrast, is more mixed across the pixels, reflecting irreducible noise.

Text Data.We apply a transformer network to the IMDB dataset, which contains movie reviews for some 50,000 films. This is a sentiment analysis task, with the goal of identifying positive vs. negative reviews. We visualize the contribution of individual words to the uncertainty of particular predictions as calculated using the modified DeepSHAP pipeline, highlighting how some tokens tend to add or remove predictive information.

We report results for two high-entropy examples in Fig. 1B. In the first review, the model appears confused by the sentence "This is not Great Cinema but I was most entertained," which clearly conveys some ambiguity in the reviewer's sentiment. In the second example, the uncertainty comes from several sources including unexpected juxtapositions such as "laughing and crying", as well as "liar liar...you will love this movie."Tabular Data.We design a simple simulation experiment, loosely inspired by , in which Shapley values under the entropy game have a closed form solution (see Appx. C.1 for details). This allows us to compare various approaches for sampling out-of-coalition feature values. Variables \(\) are multivariate normally distributed with a Toeplitz covariance matrix \(_{ij}=^{|i-j|}\) and \(d=4\) dimensions. The conditional distribution of outcome variable \(Y\) is Gaussian, with mean and variance depending on \(\). We exhaustively enumerate all feature subsets at varying sample sizes and values of the autocorrelation parameter \(\).

For imputation schemes, we compare KernelSHAP, maximum likelihood, copula methods, empirical samplers, and ctree (see  for definitions of each, and a benchmark study of conditional sampling strategies for feature attributions). We note that this task is strictly more difficult than computing Shapley values under the standard \(v_{0}\), since conditional variance must be estimated from the residuals of a preliminary model, itself estimated from the data. No single method dominates throughout, but most converge on the true Shapley value as sample size increases. Predictably, samplers that take conditional relationships into account tend to do better under autocorrelation than those that do not.

### Covariate Shift and Active Learning

To illustrate the utility of our method for explaining covariate shift, we consider several semi-synthetic experiments. We start with four binary classification datasets from the UCI machine learning repository --BreastCancer, Diabetes, Ionosphere, and Sonar--and make a random 80/20 train/test split on each. We use an XGBoost model  with 50 trees to estimate conditional probabilities and the associated uncertainty. We then perturb a random feature from the test set, adding a small amount of Gaussian noise to alter its underlying distribution. Resulting predictions have a large degree of entropy, and would therefore be ranked highly by an AL acquisition function. We compute information theoretic Shapley values for original and perturbed test sets. Results are visualized in Fig. 3.

Our method clearly identifies the source of uncertainty in these datapoints, assigning large positive or negative attributions to perturbed features in the test environment. Note that the distribution shifts

Figure 1: **A.** MNIST examples. We highlight pixels that increase (red) and decrease (blue) predictive uncertainty in digit classification tasks (1 vs. 7, 3 vs. 8, and 4 vs. 9). **B.** Reviews from the IMDB dataset, with tokens colored by their relative contribution to the entropy of sentiment predictions.

Figure 2: **A.** Mean absolute error (MAE) as a function of sample size, with autocorrelation fixed at \(=0.5\). **B.** MAE as a function of autocorrelation with sample size fixed at \(n=2000\). Shading represents standard errors across 50 replicates.

are fairly subtle in each case, rarely falling outside the support of training values for a given feature. Thus we find that information theoretic Shapley values can be used in conjunction with covariate shift detection algorithms to explain the source of the anomaly, or in conjunction with AL algorithms to explain the exploratory selection procedure.

### Feature Selection

Another application of the method is as a feature selection tool when heteroskedasticity is driven by some but not all variables. For this experiment, we modify the classic Friedman benchmark , which was originally proposed to test the performance of nonlinear regression methods under signal sparsity. Outcomes are generated according to:

\[Y=10( X_{1}X_{2})+20(X_{3}-0.5)^{2}+10X_{4}+5X_{5}+_{y},\]

with input features \((0,1)^{10}\) and standard normal residuals \(_{y}(0,1^{2})\). To adapt this DGP to our setting, we scale \(Y\) to the unit interval and define:

\[Z=10( X_{6}X_{7})+20(X_{8}-0.5)^{2}+10X_{9}+5X_{10}+_{z},\]

with \(_{z}(0,^{2})\), where \(\) denotes the rescaled version of \(Y\). Note that \(Z\)'s conditional variance depends exclusively on the first five features, while its conditional mean depends only on the second five. Thus with \(f()=[Z]\) and \(h()=[Z]\), we should expect Shapley values for \(f\) to concentrate around zero for \(\{X_{6},,X_{10}\}\), while Shapley values for \(h\) should do the same for \(\{X_{1},,X_{5}\}\).

We draw \(2000\) training samples and fit \(f\) using XGBoost with 100 trees. This provides estimates of both the conditional mean (via predictions) and the conditional variance (via observed residuals \(_{y}\)). We fit a second XGBoost model \(h\) with the same hyperparameters to predict \((_{y}^{2})\). Results are reported on a test set of size \(1000\). We compute attributions using TreeSHAP  and visualize results in Fig. 4A. We find that Shapley values are clustered around zero for unimportant features in each model, demonstrating the method's promise for discriminating between different modes of predictive information. In a supplemental experiment, we empirically evaluate our conformal coverage guarantee on this same task, achieving nominal coverage at \(=0.1\) for all features (see Appx. C.3).

As an active feature-value acquisition example, we use the same modified Friedman benchmark, but this time increase the training sample size to \(5000\) and randomly delete some proportion of cells in the design matrix for \(\). This simulates the effect of missing data, which may arise due to entry errors or high collection costs. XGBoost has native methods for handling missing data at training and test time, although resulting Shapley values are inevitably noisy. We refit the conditional variance estimator \(h\) and record feature rankings with variable missingness.

The goal in active feature-value acquisition is to prioritize the variables whose values will best inform future predictions subject to budgetary constraints. Fig. 4B shows receiver operating characteristic (ROC) curves for a feature importance ranking task as the frequency of missing data increases from zero to 50%. Importance is estimated via absolute Shapley values. Though performance degrades with increased missing data, as expected, we find that our method reliably ranks important features above unimportant ones in all trials. Even with fully half the data missing, we find an AUC of 0.682, substantially better than random.

Figure 3: Information theoretic Shapley values explain the uncertainty of predictions on original and perturbed test sets. Our method correctly attributes the excess entropy to the perturbed features.

## 7 Discussion

Critics have long complained that Shapley values (using the conventional payoff function \(v_{0}\)) are difficult to interpret in XAI. It is not always clear what it even means to remove features [43; 2], and large/small attributions are neither necessary nor sufficient for important/unimportant predictors, respectively [5; 29]. In an effort to ground these methods in classical statistical notions, several authors have analyzed Shapley values in the context of ANOVA decompositions [24; 6] or conditional independence tests [47; 78], with mixed results. Our information theoretic approach provides another window into this debate. With modified value functions, we show that marginal payoffs \(_{v}(S,j,)\) have an unambiguous interpretation as a local dependence measure. Still, Shapley values muddy the waters somewhat by averaging these payoffs over coalitions.

There has been a great deal of interest in recent years on _functional data analysis_, where the goal is to model not just the conditional mean of the response variable \([Y]\), but rather the entire distribution \(P(Y)\), including higher moments. Distributional regression techniques have been developed for additive models , gradient boosting machines , random forests , and neural density estimators . Few if any XAI methods have been specifically designed to explain such models, perhaps because attributions would be heavily weighted toward features with a significant impact on the conditional expectation, thereby simply reducing to classic measures. Our method provides one possible way to disentangle those attributions and focus attention on higher moments. Future work will explore more explicit connections to the domain of functional data.

One advantage of our approach is its modularity. We consider a range of different information theoretic games, each characterized by a unique value function. We are agnostic about how to estimate the relevant uncertainty measures, fix reference distributions, or sample candidate coalitions. These are all active areas of research in their own right, and practitioners should choose whichever combination of tools works best for their purpose.

However, this flexibility does not come for free. Computing Shapley values for many common function classes is #P-hard, even when features are jointly independent . Modeling dependencies to impute values for out-of-coalition features is a statistical challenge that requires extensive marginalization. Some speedups can be achieved by making convenient assumptions, but these may incur substantial errors in practice. These are familiar problems in feature attribution tasks. Our method inherits the same benefits and drawbacks.

## 8 Conclusion

We introduced a range of methods to explain conditional entropy in ML models, bringing together existing work on uncertainty quantification and feature attributions. We studied the information theoretic properties of several games, and implemented our approach in model-specific and model-agnostic algorithms with numerous applications. Future work will continue to examine how XAI can go beyond its origins in prediction to inform decision making in areas requiring an exploration-exploitation trade-off, such as bandits and reinforcement learning.

Figure 4: **A. Results for the modified Friedman benchmark experiment. The conditional mean depends on \(\{X_{6},,X_{10}\}\), while the conditional variance relies on \(\{X_{1},,X_{5}\}\). B. ROC curves for a feature ranking task with variable levels of missingness. The proposed value function gives informative results for feature-value acquisition.**