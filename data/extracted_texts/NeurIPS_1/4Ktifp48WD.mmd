# Differentially Private Optimization

with Sparse Gradients

 Badih Ghazi

Google Research

badihghazi@google.com

&Cristobal Guzman

Google Research and Pontificia Universidad Catolica de Chile

crguzman@google.com

&Pritish Kamath

Google Research

pritishk@google.com

&Ravi Kumar

Google Research

ravi.k53@gmail.com

&Pasin Manurangsi

Google Research

pasin@google.com

###### Abstract

Motivated by applications of large embedding models, we study differentially private (DP) optimization problems under sparsity of _individual_ gradients. We start with new near-optimal bounds for the classic mean estimation problem but with sparse data, improving upon existing algorithms particularly for the high-dimensional regime. The corresponding lower bounds are based on a novel block-diagonal construction that is combined with existing DP mean estimation lower bounds. Next, we obtain pure- and approximate-DP algorithms with almost optimal rates for stochastic convex optimization with sparse gradients; the former represents the first nearly dimension-independent rates for this problem. Furthermore, by introducing novel analyses of bias reduction in mean estimation and randomly-stopped biased SGD we obtain nearly dimension-independent rates for near-stationary points for the empirical risk in nonconvex settings under approximate-DP.

## 1 Introduction

The pervasiveness of personally sensitive data in machine learning applications (e.g., advertising, public policy, and healthcare) has led to the major concern of protecting users' data from their exposure. When releasing or deploying these trained models, differential privacy (DP) offers a rigorous and quantifiable guarantee on the privacy exposure risk .

Consider neural networks whose inputs have categorical features with large vocabularies. These features can be modeled using embedding tables; namely, for a feature that takes \(K\) distinct values, we create trainable parameters \(w_{1},,w_{K}^{k}\), and use \(w_{a}\) as input to the neural network when the corresponding input feature is \(a\). A natural outcome of such models is that the per-example gradients are guaranteed to be sparse; when the input feature is \(a\), then only the gradient with respect to \(w_{a}\) is non-zero. Given the prevalence of sparse gradients in practical deep learning applications, GPUs/TPUs that are optimized to leverage gradient sparsity are commercially offered and widely used in industry . To leverage gradient sparsity, recent practical work has considered DP stochastic optimization with _sparse gradients_ for large embedding models for different applications including recommendation systems, natural language processing, and ads modeling .

Despite its relevance and promising empirical results, there is limited understanding of the theoretical limits of DP learning under gradient sparsity. This gap motivates our work.

### Our Results

We initiate the study of DP optimization under gradient sparsity. More precisely, we consider a stochastic optimization (SO) problem, \(\{F_{}(x):x\}\), where \(^{d}\) is a convex set, and \(F_{}(x)=_{z}[f(x,z)]\), with \(f(,z)\) enjoying some regularity properties, and \(\) is a probability measure supported on a set \(\). Our main assumption is gradient sparsity: for an integer \(0 s d\),

\[ x,\,z\;:\| f(x,z)\|_{0} s\,,\]

where \(\|y\|_{0}\) denotes the number of nonzero entries of \(y\). We also study empirical risk minimization (ERM), where given a dataset \(S=(z_{1},,z_{n})\) we aim to minimize \(F_{S}(x):=_{i[n]}f(x,z_{i})\).

Our results unearth three regimes of accuracy rates for the above setting: (i) the small dataset size regime where the optimal rate is constant, (ii) the large dataset size where the optimal rates are polynomial in the dimension, and (iii) an intermediate dataset size regime characterized by a new high-dimensional rate1 (see Table 1 and Table 2, for precise rates). These results imply in particular that even for high-dimensional models, this problem is tractable under gradient sparsity. Without sparsity, these polylogarithmic rates is impossible due to known lower bounds .

In Section 3, we start with the fundamental task of \(_{2}\)-mean estimation with sparse data (which reduces to ERM with sparse linear losses ). Here, we obtain new upper bounds (see Table 1). These rates are obtained by adapting the projection mechanism , with a convex relaxation that makes our algorithms efficient. Note that for pure-DP, even our large dataset rate of \(/( n)\) can be substantially smaller than the dense pure-DP rate of \(d/( n)\), whenever \(s d\). For approximate-DP we also obtain a sharper upper bound by solving an \(_{1}\)-regression problem of a noisy projection of the empirical mean over a random subspace. Its analysis combines ideas from compressed sensing  with sparse approximation via the Approximate Caratheodory Theorem .

In Section 4, we prove lower bounds that show the near-optimality of our algorithms. For pure-DP, we obtain a new lower bound of \((s(d/s)/(n))\), which is based on a packing of sparse vectors.

 
**Setting** & **Upper bound** & **Lower bound** \\   \(\)-DP & \(1}}{ n}\) (Thm. 3.2) & \(1}\) (Thm. 4.1) \\  \((,)\)-DP & \(1}{} }{ n}\) (Thm. 8.1) & \(1}{}}{ n}\) (Thm. 4.5) \\  

Table 1: Rates for DP mean estimation with sparse data of unit \(_{2}\)-norm. Bounds stated for constant success/failure probability, resp. We use a \(\)\(b\) to denote \((a,b)\). New results \(\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;While this lower bound looks weaker than the standard \((d/(n))\) lower bound based on dense packings , we design a novel bootstrapping via a block diagonal construction where each block contains a sparse lower bound as above. This, together with a padding argument , yields lower bounds for the three regimes of interest. For approximate-DP, we also use the block diagonal bootstrapping, where this time the blocks use classical fingerprinting codes in dimension \(s\). Our approximate-DP lower bounds, however, have a gap of \((d/s)^{1/4}\) in the high-dimensional regime; we conjecture that the aforementioned compressed sensing-based upper bound is tight.

In Section 5, we study DP-ERM with sparse gradients, under approximate-DP. We propose the use of stochastic gradient (SGD) with a mean estimation gradient oracle based on the results in Section 3. This technique yields nearly-tight bounds in the convex case (similar to first row of Table 2), and for the nonconvex case the stationarity rates are nearly dimension independent (last row of Table 2). The main challenge here is the _bias in mean estimation_, which dramatically deteriorates the rates of SGD. Hence we propose a bias reduction method inspired by the simulation literature . This technique uses a random batch size in an exponentially increasing schedule and a telescopic estimator of the gradient which--used in conjunction with our DP mean estimation methods--provides a stochastic first-order oracle that attains bias similar to the one of a full-batch algorithm, with moderately bounded variance. Note that using the full-batch in this case would lead to polynomially weaker rates; in turn, our method leverages the batch randomization to conduct a more careful privacy accounting based on subsampling and the fully-adaptive properties of DP . The introduction of random batch sizes and the random evolution of the privacy budget leads to various challenges in analyzing the performance of SGD. First, we analyze a _randomly stopped method_, where the stopping time dictated by the privacy budget. Noting that the standard SGD analysis bounds the cumulative regret, which is a submartingale, we carry out this analysis by integrating ideas from submartingales and stopping times . Second, this analysis only yields the desired rates _with constant probability_. Towards high probability results, we leverage a private model selection  based on multiple runs of randomly-stopped SGD that exponentially boosts the success probability (details in Appendix F).

In Section 6, we study further DP-SO and DP-ERM algorithms for the convex case. Our algorithms are based on regularized output perturbation with an \(_{}\) projection post-processing step. While this projection step is rather unusual, its role is clear from the analysis: it leverages the \(_{}\) bounds of noise addition, which in conjunction with convexity provides an error guarantee that also leverages the gradient sparsity. This algorithm is nearly-optimal for approximate-DP. For pure-DP, the previous algorithm requires an additional smoothness assumption, hence we propose a second algorithm based on the exponential mechanism  run over a net of suitably sparse vectors. Neither of the pure-DP algorithms matches the lower bound for mean estimation (the gap in the exponent of the rate is of \(1/6\)), but they attain the first nearly dimension-independent rates for this problem.

### Related Work

DP optimization is an extensively studied topic for over a decade (see , and the references therein). In this field, some works have highlighted the role of _model sparsity_ (e.g., using sparsity-promoting \(_{1}\)-ball constraints) in near-dimension independent excess-risk rates for DP optimization, both for ERM and SCO . These settings are unrelated to ours, as sparse predictors are typically related to dense gradients.

Another proposed assumption to mitigate the impact of dimension in DP learning is that gradients lie (approximately) in a low dimensional subspace  or where dimension is substituted by a bound on the trace of the Hessian of the loss . These useful results are unfortunately not applicable to our setting of interest, as we are interested in arbitrary gradient sparsity patterns for different datapoints.

Substantially less studied is the role of gradient sparsity. Closely related to our work,  studied approximate DP-ERM under gradient sparsity, with some stronger assumptions. Aside from an additional \(_{}\) bound on individual gradients, the following _partitioning sparsity assumption_ is imposed. The dataset \(S\) can be uniformly partitioned into subsets \(S_{1},,S_{m}\) with a uniform gradient sparsity bound: for all \(k[m]\) and \(x\), \(\|_{z S_{k}} f(x,z)\|_{0} c_{1}\). The work shows polylogarithmic in the dimension rates, for both convex and nonconvex settings. Our results only assume individual gradient sparsity, so on top of being more general, they are also faster and provably nearly optimal in the convex case. Another relevant work is , which studies the computational and utility benefits for DP with sparse gradients in neural networks with embedding tables. With the caveat that variable selection on stochastic gradients is performed at the level of _contributing buckets_ (i.e., rows of the embedding table), rather than on gradient coordinates, this work shows substantial improvements on computational efficiency and also on the resulting utility.

In , bias reduction is used to mitigate the regularization bias in SCO. While they also borrow inspiration from , both their techniques and scope are unrelated to ours.

### Future Directions

We present some of the main open questions and future directions of this work. First, we conjecture that for approximate-DP mean estimation--similarly to the pure-DP case--a lower bound \(/[n]\) should exist; such construction could be bootstrapped with a block-diagonal dataset for a tight lower bound (Lemma 4.3). Second, for pure DP-SCO, we believe an algorithm should exist that achieves rates analogous to those for mean estimation. Unfortunately, most of variants of output perturbation (including phasing ) cannot attain such rates. From a practical perspective, the main open question is whether our rates are attainable without prior knowledge of \(s\); note that all our mean estimation algorithms (which carries over to our optimization results) depend crucially on knowledge of this parameter. While we can treat \(s\) as a hyperparameter, it would be highly beneficial to design algorithms that automatically adapt to it.

We believe our bias reduction is of broader interest. For example,  have shown strong negative results about bias in DP mean estimation. While similar lower bounds may hold for sparse estimation, bias reduction allows us to amortize this error within an iterative method, preventing error accumulation.

Finally, there is no evidence of our nonconvex rate being optimal. In this vein, we should remark that even in the dense case the optimal stationarity rates are still open .

## 2 Notation and Preliminaries

In this work, \(\|\|=\|\|_{2}\) is the standard Euclidean norm on \(^{d}\). We will also make use of \(_{p}\)-norms, where \(\|x\|_{p}:=_{j[d]}|x_{j}|^{p}^{1/p}\) for \(1 p\). For \(p=0\), we use the notation \(\|x\|_{0}=|\{j[d]:x_{j} 0\}|\), i.e., the size of the support of \(x\). We denote the \(r\)-radius ball centered at \(x\) of the \(p\)-norm in \(^{d}\) by \(^{d}_{p}(x,r):=\{y^{d}:\|y-x\|_{p} r\}\). Given \(s[d]\) and \(L>0\), the set of \(s\)-_sparse vectors_ is (the scaling factor \(L\) is omitted in the notation for brevity)

\[^{d}_{s}:=\{x^{d}:\,\|x\|_{0} s,\,\|x\|_{2} L\}. \]

Note that Jensen's inequality implies: if \(\|x\|_{0} s\) and \(1 p<q\), then \(\|x\|_{p} s^{1/p-1/q}\|x\|_{q}\).

**Remark 2.1**.: _The upper bound results in this paper hold even if we replace the set \(^{d}_{s}\) of sparse vectors by the strictly larger \(_{1}\)-ball \(^{d}_{1}(0,L)\). Note that while our upper bounds extend to the \(_{1}\) assumption above, our lower bounds work under the original sparsity assumption._

Let \(f:\) be a loss function. The function evaluation \(f(x,z)\) represents the loss incurred by hypothesis \(x\) on datapoint \(z\). In _stochastic optimization_ (SO), we consider a data distribution \(\), and our goal is to minimize the expected loss under this distribution

\[_{x}F_{}(x):=_{z }[f(x,z)]}.\] (SO)

Throughout, we use \(x^{*}()\) to denote an optimal solution to (SO), which we assume exists. In the _empirical risk minimization_ (ERM) problem, we consider sample datapoints \(S=(z_{1},,z_{n})\) and our goal is to minimize the empirical error with respect to the sample

\[_{x}F_{S}(x):=_{i[n]}f(x, z_{i})}.\] (ERM)

We denote by \(x^{*}(S)\) an arbitrary optimal solution to (ERM), which we assume exists. Even when \(S\) is drawn i.i.d. from \(\), solutions (or optimal values) of (SO) and (ERM) do not necessarily coincide.

We present the definition of differential privacy (DP), deferring useful properties and examples to Appendix A. Let \(\) be a sample space, and \(\) an output space. A dataset is a tuple \(S^{n}\), and datasets \(S,S^{}^{n}\) are _neighbors_ (denoted as \(S S^{}\)) if they differ in only one of their entries.

**Definition 2.2** (Differential Privacy).: Let \(:^{n}\). We say that \(\) is _\((,)\)-(approximately) differentially private (DP)_ if for every pair \(S S^{}\), we have for all \(\) that \([(S)]\,e^{}[(S^ {})]+\). When \(=0\), we say that \(\) is \(\)-DP or pure-DP.

## 3 Upper Bounds for DP Mean Estimation with Sparse Data

We first study DP mean estimation with sparse data. Our first result is that the projection mechanism  is nearly optimal, both for pure- and approximate-DP. In our case, we interpret the marginals on each of the \(d\) dimensions as the queries of interest: this way, the \(_{2}\)-error on private query answers corresponds exactly to the \(_{2}\)-norm estimation error. A key difference to the approach in  and related works is that we project the noisy answers onto the set \(:=_{1}^{d}(0,L)\), which is a (coarse) convex relaxation of \((_{s}^{d})\). This is crucial to make our algorithm efficiently implementable. Due to space limitations, proofs from this section have been deferred to Appendix B.

```
Vector \((S)=_{i=1}^{n}z_{i}\) from dataset \(S(_{s}^{d})^{n}\); \(, 0\), privacy parameters \(=(S)+\), with \(()^{ d}&=( }{ne})=0\,,\\ (0,^{2}I)&^{2}=(1.25/ )}{(ne)^{2}}>0\,.\) return\(=\{\|z-\|_{2}:z\}\), where \(:=_{1}^{d}(0,L)\)
```

**Algorithm 1**Projection_Mechanism\((S),,,n\))

**Lemma 3.1**.: _In Algorithm 1, it holds that \(\|-(S)\|_{2}}\), almost surely._

We now provide the privacy and accuracy guarantees of Algorithm 1.

**Theorem 3.2**.: _For \(=0\), Algorithm 1 is \(\)-DP, and with probability \(1-\):_

\[\|-(S)\|_{2} L\{(d/ )}{ne},}\}.\]

**Theorem 3.3**.: _For \(>0\), Algorithm 1 is \((,)\)-DP, and with probability \(1-\):_

\[\|-(S)\|_{2} L\{+)}{ne},}{}\}.\]

Sharper Upper Bound via Compressed SensingIn Appendix B.4 we propose a faster mean estimation approximate-DP algorithm. Its rate nearly matches the lower bound we will prove in Theorem 4.4. We believe that this rate is essentially optimal. This algorithm projects the data average into a low dimensional subspace (via a random projection matrix), and uses compressed sensing to recover a noisy version of this projection: this way, noise provides privacy, which is further boosted by the random projection, and the accuracy follows from an application of the stable and noisy recovery properties of compressed sensing , together with the Approximate Caratheodory Theorem.

## 4 Lower Bounds for DP Mean Estimation with Sparse Data

We provide matching lower bounds to those from Section 3. Moreover, although the stated lower bounds are for mean estimation, known reductions imply analogous lower bounds for DP-ERM and DP-SCO . First, for pure-DP we provide a packing-type construction based on sparse vectors. This is used in a novel block-diagonal construction, which provides the right low/high-dimensional transition. On the other hand, for approximate-DP, a block diagonal reduction with existing fingerprinting codes , suffices to obtain lower bounds that exhibit a nearly tight low/high-dimensional transition. For simplicity, we consider the case of \(L=1\), i.e., \(_{s}^{d}=\{z^{d}:\|z\|_{0} s,\|z\|_{2} 1\}\); it is easy to see that any lower bound scales linearly in \(L\). We defer proofs from this section to Appendix C.

### Lower Bounds for Pure-DP

Our main lower bound for pure-DP mechanisms is as follows.

**Theorem 4.1**.: _Let \(>0\) and \(s<d/2\). Then the empirical mean estimation problem over \(_{s}^{d}\) satisfies_

\[_{\,:\,}_{S(_{s}^{d})^{n}} [\|(S)-(S)\|_{2}\;\{1,},}{  n}\}] 1.\]

The statement above--as well as those which follow--should be read as "for all DP algorithms \(\), there exists a dataset \(S\), such that the mean estimation error is lower bounded by \((n,d,,)\) with probability at least \((n,d,,)\)" (where in this case \(\{1,},}{ n}\}\) and \( 1\)).

We also introduce a strengthening of the worst case lower bound, based on hard distributions.

**Definition 4.2**.: We say that a probability \(\) over \(^{n}\) induces an \((,)\)_-distributional lower bound_ for \((,)\)-DP mean estimation if \(_{\,:\,(,)}_{S, }\|(S)-(S)\|_{2}\).

Note this type of lower bound readily implies a worst case lower bound. On the other hand, while the existence of hard distributions follows by the existence of hard datasets (by Yao's minimax principle), we provide explicit constructions of these distributions, for the sake of clarity.

Theorem 4.1 follows by combining the two results that we provide next. First, and our main technical innovation in the sparse case is a block-diagonal dataset bootstrapping construction, which turns a low-dimensional lower bound into a high-dimensional one.

**Lemma 4.3** (Block-Diagonal Lower Bound Bootstrapping).: _Let \(n_{0},t\). Let \(\) be a distribution over \((_{s}^{t})^{n_{0}}\) that induces an \((_{0},_{0})\)-distributional lower bound for \((,)\)-DP mean estimation. Then, for any \(d t\), \(n n_{0}\) and \(K\{},\}\), there exists \(\) over \((_{s}^{d})^{n}\) that induces an \((,)\)-distributional lower bound for \((,)\)-DP mean estimation, where \(n_{0}}{n}K}\) and \( 1-(-_{0}/8)\)._

Note that the above result needs a base lower bound for which packing-based constructions suffice.

**Theorem 4.4**.: _Let \(>0\) and \(s<d/2\). Then there exists an \((,)\)-distributional lower bound for \(\)-DP mean estimation over \((_{s}^{d})^{n}\) with \(\{1,\}\) and \(=1/2\)._

### Lower Bounds for Approximate-DP

While the lower bound for the approximate-DP case is similarly based on the block-diagonal reduction, its base lower bound follows more directly from the dense case.

**Theorem 4.5**.: _Let \((0,1]\), \(2^{-o(n)}}\). Then the empirical mean estimation problem over \(_{s}^{d}\) satisfies_

\[_{\,:\,(,)}_{S(_{ s}^{d})^{n}}[\|(S)-(S)\|_{2}\{1, }{},}{n}\}] 1.\]

## 5 Bias Reduction Method for DP-ERM with Sparse Gradients

We now start with our study of DP-ERM with sparse gradients. We defer some proofs to Appendix E. In this section and later, we will impose subsets of the following assumptions:

1. Initial distance: For SCO, \(\|x^{0}-x^{*}()\| D\); for ERM, \(\|x^{0}-x^{*}(S)\| D\).
2. Diameter bound: \(\|x-y\| D\), for all \(x,y\).
3. Convexity: \(f(,z)\) is convex, for all \(z\).
4. Loss range: \(f(x,z)-f(y,z) B\), for all \(x,y\), \(z\).
5. Lipschitzness: \(f(,z)\) is \(L\)-Lipschitz, for all \(z\).
6. Smoothness: \( f(,z)\) is \(H\)-Lipschitz, for all \(z\).
7. Individual gradient sparsity: \( f(x,z)\) is \(s\)-sparse, for all \(x\) and \(z\).

The most natural and popular DP optimization algorithms are based on SGD. Here we show how to integrate the mean estimation algorithms from Section 3 to design a stochastic first-order oracle that can be readily used by any stochastic first-order method. The key challenge here is that estimators from Section 3 are inherently biased, which is known to dramatically deteriorate the convergence rates. Hence, we start by introducing a bias reduction method.

```
0: Dataset \(S=(z_{1},,z_{n})^{n}\), \(,>0\) privacy parameters, \(L\)-Lipschitz loss \(f(x,z)\) with \(s\)-sparse gradient, \(x\), batch size parameter \(N(M)\) with \(M=_{2}(n)-1\)  Let \(B}\), \(O,E\) a partition of \(B\) with \(|O|=|E|=2^{N}\), \(I([n])\) \(G^{+}_{N+1}(x,B)=( F_{B}(x),/4, /4,2^{N+1})\) (Algorithm 1) \(G^{-}_{N}(x,O)=( F_{C}(x),/4, /4,2^{N})\) \(G^{-}_{N}(x,E)=( F_{E}(x),/4, /4,2^{N})\) \(G_{0}(x,I)=( f(x,z_{I}),/4, /4,1)\) Return (below \(p_{k}=[(M)=k]\)) \((x)=}G^{+}_{N+1}(x,B)-G^{-} _{N}(x,O)+G^{-}_{N}(x,E)+G_{0}(x,I)\)
```

**Algorithm 2**Subsampled_Bias-Reduced_Gradient_Estimator(\(x,S,N,,\))

### Subsampled Bias-Reduced Gradient Estimator for DP-ERM

We propose Algorithm 2, inspired by a debiasing technique proposed in . The idea is the following: we know that the projection mechanism2 would provide more accurate gradient estimators with larger sample sizes, and we will see that its bias improves analogously. We choose our batch size as a random variable with exponentially increasing range, and given such a realization we subtract the projection mechanism applied to the whole batch minus the same mechanism applied to both halves of this batch.3 This subtraction, together with a multiplicative and additive correction, results in the expected value of the outcome \((x)\) corresponding to the estimator with the largest batch size, leading to its expected accuracy being boosted by such large sample size, without necessarily utilizing such amount of data (in fact, the probability of such batch size being picked is polynomially smaller, compared to the smallest possible one). The caveat with this technique, as we will see, relates to a heavy-tailed distribution of outcomes, and therefore great care is needed for its analysis.

Instrumental to our analysis is the following _truncated geometric distribution_ with parameter \(M\), whose law will be denoted by \((M)\): we say \(N(M)\) if it is supported on \(\{0,,M\}\), and takes value \(k\) with probability \(p_{k}:=C_{M}/2^{k}\), where \(C_{M}=(2(1-2^{-(M+1)}))^{-1}\), is the normalizing constant. Note that \(1/2 C_{M} 1\), thus it is bounded away from \(0\) and \(+\).

We propose Algorithm 3, which interacts with the oracle given in Algorithm 2. For convenience, we will denote the random realization from the truncated geometric distribution used in iteration \(t\) by \(N_{t}\). The idea is that, using the fully adaptive composition property of DP , we can run the method until our privacy budget is exhausted. Due to technical reasons, related to the bias reduction, we need to shift by one the termination condition in the algorithm. In particular, our algorithm goes over the reduced privacy budget of \((/2,/2)\). The additional slack in the privacy budget guarantees that even with the extra oracle call the algorithm respects the privacy constraint.

**Lemma 5.1**.: _Algorithm 3 is \((,)\)-DP._

### Bias and Moment Estimates for the Debiased Gradient Estimator

We provide bias and second moment estimates for our debiased estimator of the empirical gradient. In summary, we show that this estimator has bias matching that of the full-batch gradient estimator, while at the same time its second moment is bounded by a mild function of the problem parameters.

**Lemma 5.2**.: _Let \(d}{}\). Algorithm 2, enjoys bias and second moment bounds_

\[[(x)- F_{S}(x)|x] \ }{} =:b,\] \[[\|(x)\|^{2}|x]\ (n)}{}=:^{2}.\]

Proof.: For simplicity, we assume without loss of generality that \(n\) is a power of 2, so that \(2^{M+1}=n\).

**Bias.** Let, for \(k=0,,M\), \(G_{k+1}^{+}(x)=[G_{N+1}^{+}(x,B) N=k,x]\), and

\[G_{k}^{-}(x)=[G_{N}^{-}(x,E) N=k,x]=[G_{N}^{-}(x,O)  N=k,x],\]

where the last equality follows from the identical distribution of \(O\) and \(E\). Noting further that \(G_{k}^{+}(x)=G_{k}^{-}(x)\) (which follows from the uniform sampling and the cardinality of the used data-points), and using the law of total probability, we have

\[[(x) x] = _{k=0}^{M}(G_{k}^{+}(x)-G_{k-1}^{-}(x))+ [G_{0}(x,I) x]\] \[= G_{M+1}^{+}(x)-G_{0}^{-}(x)+[G_{0}(x,I) x]\] \[= [G_{M+1}^{+}(x)- F_{S}(x)|x]+ F_{S}(x),\]

where we also used that \([G_{0}(x,I) x]=G_{0}^{-}(x)\) (since \(I\) is a singleton). Next, by Theorem B.1

\[\|[(x) x]- F_{S}(x)\|\|[G_{M+1}^{ +}(x)- F_{S}(x)|x]\| L}{ }.\]

**Second moment bound.** Using the law of total probability, and that \(O,E\) are a partition of \(B\):

\[[\|(x)\|^{2} x]=_{k=0}^{M}p_{k} }[G_{N+1}^{+}(x,B)- F_{B}(x)]\] \[-}G_{N}^{-}(x,O)- F_{O}(x)+G_{N}^{-}( x,E)- F_{E}(x)+G_{0}(x,I)^{2}x,N=k\] \[\ 2[\|G_{0}(x,I)\|^{2} x]+4_{k=0}^{M}}G_{N+1}^{+}(x,B)- F_{B}(x)^{2 }x,N=k\] \[+\ _{k=0}^{M}}G_{N}^{- }(x,O)- F_{O}(x)^{2}+G_{N}^{-}(x,E)- F_{E}(x) ^{2}x,N=k.\]

We now use Theorem B.1, to conclude that

\[G_{N+1}^{+}(x,B)- F_{B}(x) ^{2}x,N=k }{2^{k+1}}\] \[_{A\{O,E\}}G_{N}^{-}(x,A) - F_{A}(x)^{2}x,N=k} }{2^{k}}\] \[G_{0}(x,I)^{2} x }{}.\]

Recalling that \(M+1=_{2}n\) and \(p_{k}=2^{-k}\), these bounds readily imply that \(\|(x)\|^{2}^{2}\).

### Accuracy Guarantees for Subsampled Bias-Reduced Sparse SGD

The previous results provide useful information about the privacy, bias, and second-moment of our proposed oracle. Our goal now is to provide excess risk rates for DP-ERM. For this, we need to prove the algorithm runs for long enough, i.e., a lower bound on the stopping time of Algorithm 3,

\[T:=t:<2\!\!_{s=0}^{t}\!\!+1}+1}{16n}^{2} ^{1/2}+}{2}\!_{s=0}^{t}\!\!+ 1}+1}{16n}\ \ \ \ <_{s=0}^{t}\!\!+ 1}+1)}{16n}}. \]

The proof of Theorem 5.2 implies that moments of \(\) increase exponentially in \(M\). This heavy-tailed behavior implies that \(T\) may not concentrate strongly enough to obtain high probability lower bounds for \(T\). What we will do instead is showing that _with constant probability_\(T\) behaves as desired.

To justify the approach, let us provide a simple in-expectation bound on how the privacy budget accumulates in the definition of \(T\): letting \(_{t}=(3 2^{N_{t}+1}+1)/[16n]\), we have that

\[_{s=0}^{t}_{s}^{2}=}{(16n)^{2}}(3 2^{N_{1}+1}+1)^{2} }{(16n)^{2}}9[2^{2(N_{1}+1)}] +1}{n},\]

where in the last step we used that \(2^{2(N_{1}+1)}=C_{M}_{k=1}^{M+1}2^{k} n\). This in-expectation analysis can be used in combination with ideas from stopping times to establish bounds for \(T\).

**Lemma 5.3**.: _Let \(0<<1/n^{2}\). Let \(T\) be the stopping time defined in eqn. (2). Then, there exists \(t=Cn/(2/)\) (with \(C>0\) an absolute constant) such that \([T t] 1/4\). On the other hand,_

\[}{(n+1)(4/)}-1[T].\]

With our bounds on \(T\), further analysis involving regret bounds on randomly stopped SGD yields the following bounds for convex and nonconvex losses. See Theorem E.2 and Theorem E.3 for details.

**Theorem 5.4**.: _Consider a (SO) problem under initial distance (Item (A.1)), Lipschitzness (Item (A.5)) and gradient sparsity (Item (A.7)) assumptions._

* _In the convex case (Item (A.3)), Algorithm_ 3 _satisfies_ \[F_{S}()-F_{S}(x^{*}(S)) LD\, [s(d/s)^{3}(1/)]^{1/4}}{}.\]
* _In the nonconvex case, additionally assuming smoothness (Item (A.6)) and the following_ initial suboptimality assumption_: namely, that given our initialization \(x^{0}^{d}\), there exists \(>0\) such that \(F_{S}(x^{0})-F_{S}(x^{*}(S))\); Algorithm_ 3 _satisfies_ \[\| F_{S}(x^{})\|_{2}^{2}L+L^{2}} {}.\]

Boosting the Confidence of the Bias-Reduced SGDTo conclude, in Appendix F we provide a boosting algorithm that can exponentially amplify the success probability of Algorithm 3. The approach is based on making parallel runs of the method and using private model selection to obtain the best performing model.

## 6 DP Convex Optimization with Sparse Gradients via Regularized Output Perturbation

We conclude our work introducing another class of algorithms that attains nearly optimal rates for approximate-DP ERM and SO in the convex setting. These algorithms are based on solving a regularized ERM problem and privatizing its output by an output perturbation method. The main innovation of this technique is that we reduce the noise error by a \(\|\|_{}\)-projection. This type of projection leverages the concentration of the noise in high-dimensions. We carry out an analysis that also leverages the convexity of the risk and the gradient sparsity to obtain these rates. The full description is included in Algorithm 4. We defer missing proofs from this section, as well as additional results, to Appendix G.

**Theorem 6.1**.: _Consider an ERM problem under assumptions: initial distance (Item (A.1)), convexity (Item (A.3)), Lipchitzness (Item (A.5)) and gradient sparsity (Item (A.7)). Then, Algorithm 4 is \((,)\)-DP, and it satisfies the following excess risk guarantees, for any \(0<<1\):_

* _If_ \(=0\)_, and under the additional assumption of smoothness (A.6) and unconstrained domain,_ \(=^{d}\)_, then selecting_ \(=(H}{D^{2}})^ {1/3}\)_, it holds with probability_ \(1-\) _that_ \[F_{S}()-F_{S}(x^{*}(S)) L^{2/3}H^{1/3}D^{4/3}^{1/3}.\]
* _If_ \(>0\) _then selecting_ \(=}{}\)_, we have with probability_ \(1-\) _that_ \[F_{S}()-F_{S}(x^{*}(S)) LD}{}.\]

**Remark 6.2**.: _For approximate-DP, the theorem above can also be proved if we replace assumption (Item (A.1)) by the diameter assumption (Item (A.2)). On the other hand, for the pure-DP case it is a natural question whether the smoothness assumption is essential. In Appendix G.3, we provide a version of the exponential mechanism that works without the smoothness and unconstrained domain assumptions. This algorithm is inefficient and it does require an structural assumption on the feasible set, but it illustrates the possibilities of more general results in the pure-DP setting._

We note that the proposed output perturbation approach (Algorithm 4) leads to nearly optimal population risk bounds for approximate-DP, by a different tuning of the regularization parameter \(\).

**Theorem 6.3**.: _Consider a problem (SO) under bounded initial distance (Item (A.1)) (or bounded diameter, Item (A.2), if \(>0\)), convexity (Item (A.3)), Lipschitzness (Item (A.5)), bounded range (Item (A.4)), and gradient sparsity (Item (A.7)). Then, Algorithm 4 is \((,)\)-DP, and for \(0<<1\),_

* _If_ \(=0\)_, and under the additional assumption of smoothness (A.6) and unconstrained domain,_ \(=^{d}\)_. Selecting_ \(=(H}{D^{2}}) ^{1/3}\)_, then with probability_ \(1-\)__ \[F_{S}()-F_{S}(x^{*}())\ \ L^{2/3}H^{1/3}D^{4/3} ^{1/3}+B}.\]
* _If_ \(>0\)_. Selecting_ \(=+}{ n}^{1/2}\)_, then with probability_ \(1-\)__ \[F_{}()-F_{}(x^{*}())\ \ LD}{}+( LD+B)}.\]