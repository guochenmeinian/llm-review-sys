# [MISSING_PAGE_EMPTY:1]

[MISSING_PAGE_EMPTY:1]

Fig. 2). We introduce the Parsimonious Latent Space Model, or PLSM for short, a world model where actions have more predictable effects on the inferred latent states of the agent. Dynamics are simplified by minimizing how much the predicted dynamics \(_{t}^{_{t}}\) depend on \(_{t}\). This pushes the world model to represent states in a way that makes actions have coherent and predictable effects on the dynamics. We still allow the dynamics to vary depending on the state, but we penalize the extent of this dependence, resulting in dynamics that are softly state-invariant.

We combine PLSM with two classes of world models: Contrastive World Models (CWM)  for latent state prediction, and with Self Predictive Representations (SPR) for model-free and model-based control (TD-MPC) [6; 5; 7]. Across control experiments and prediction experiments we see improvements in planning, representation learning for control, robustness to noise, world model accuracy, and generalization.

## 2 Latent dynamics

We assume that sequences of states, actions and rewards arise in a Markov Decision Process (MDP). An MDP consists of a state space \(\), an action space \(\), and transition dynamics \(_{t+1} P(_{t+1}|_{t},_{t})\) determining how the state evolves with the actions the agent performs. In RL, we additionally care about the reward function \(r(_{t},_{t})\), which maps state-action pairs to a scalar reward term. Here, the goal is to learn the policy \(_{}(_{t}|_{t})\) that maps states to the actions with the highest possible \(Q\)-values \(Q_{_{}}(_{t},_{t})=_{_{}}[ _{t=1}^{T}^{t}r(_{t},_{t})]\), where \(\) is a discount factor. In this paper, we consider latent dynamics learning both in reward-free and RL settings.

Figure 1: **Overview**: World models are commonly used to predict latent trajectories, predict sequences of pixel observations, and perform planning. We propose an architecture together with an information bottleneck for learning simple and parsimonious world models. Our method relies on a query network that extracts a sparse representation \(_{t}\) for predicting latent transition dynamics. Combining our method with auxiliary loss functions for _i_) contrastive learning _ii_) planning and _iii_) and model-free RL, we see consistent performance improvement in all domains. Lines and bars show mean performance from three sets of RL benchmarks. Error bars represent 95% confidence interval.

To predict environment dynamics, the current state \(_{t}\) is first transformed using an encoder into a latent state \(_{t}\) that compactly represents the agent's sensory observation (2). The world model predicts the _change_ in the latent state, \(_{t}^{_{t}}\) that is induced by the action \(_{t}\) (3) & (4).

\[_{t}=e_{}(_{t}) \] \[_{t}=d_{}(_{t},_{t})\] (3) \[}_{t+1}=_{t}+_{t} \]

We omit the action superscript from \(_{t}\) for simplicity. Here, \(e_{}\) is the encoder network mapping states to latent states, and \(d_{}\) is the dynamics network mapping \(_{t}\) and \(_{t}\) to \(_{t}\).

### Parsimonious latent dynamics

Consider the probability distribution of transitions \(P(_{t}|_{t})\) given the agent's actions, marginalizing across latent states. If actions have predictable effects on the state of the environment, the entropy of \(P(_{t}|_{t})\) will be low - knowing the latent state gives little information about the effect of \(_{t}\). To make the world model handle actions more systematically, we propose to minimize the amount of information the world model needs from \(_{t}\) in order to predict correctly how an action changes the state of the world. This quantity is represented in the mutual information between the latent state \(_{t}\) and the dynamics \(_{t}\):

\[I(_{t};_{t}|_{t})=[_{t}|_{t}]-[_{t}|_{t},_{t}] \]

where \([]\) denotes the Shannon entropy. If this quantity is 0, the latent dynamics \(_{t}\) only depend on the agent's _action_\(_{t}\) and not \(_{t}\). In this extreme, all \(_{t}\) are predicted exclusively by the action. However, it is rarely the case that the action can capture an environment's full dynamics, and making dynamics contingent on states is often necessary to some degree.

To allow only the relevant information from \(_{t}\) to influence the dynamics, we introduce a query network \(f_{}\) which maps latent state-action pairs to a latent code \(_{t}\). We modify the next-step prediction components accordingly

\[_{t} =f_{}(_{t},_{t}) \] \[_{t} =d_{}(_{t},_{t}) \]

We give the query network information about the action that the transition is conditioned on. This allows the network to attend to the relevant bits in \(_{t}\) to output an appropriate \(_{t}\). Finally, to make

Figure 2: The heart (left) can appear on any \(x,y\) coordinate in a two-dimensional latent space with boundaries, on which it can transition in 9 different ways (moving in eight directions and standing still, for instance when moving into a boundary). Encouraging dynamics to be parsimonious recovers these 9 different possible transitions (see right), whereas an unconstrained model (see center) does not.

\(_{t}\) represent only the _minimal_ amount of information needed to predict the next state, provided that \(_{t}\) is known, we penalize the norm of \(_{t}\)[8; 9]. This type of regularization has been used to constrain representations of deterministic Autoencoders in past work [8; 9], with  showing that it is equivalent to minimizing the KL divergence to a constant variance zero mean Gaussian. Other regularizers and stochastic formulations are also possible (see Appendix C). For simplicity we use the deterministic variant and leave stochastic versions for future work.

The strength of the penalization is controlled through a hyperparameter \(\). This regularization minimizes how much \(_{t}\) can vary with \(_{t}\) and hence their mutual information (see Appendix A). We then train our encoder \(e_{}\), query network \(f_{}\), and dynamics \(d_{}\) jointly to minimize the following information regularized loss function.

\[=||e_{}(_{t+1})-(_{t}+d_{}( _{t},_{t}))||_{2}^{2}+||_{t}||_{2}^{2} \]

Our loss function encourages that the mutual information term is kept as low as possible, while still allowing the model to predict the next latent state accurately. In contrast to information bottlenecks imposed on the latent states themselves, we apply an information bottleneck to the dynamics, making \(_{t}\) easier to predict simply given \(_{t}\). Regularizing \(_{t}\) differs from regularizing \(_{t}\) in important ways. In environments where the dynamics \(\) can be predicted perfectly from the actions and independently of the state, regularizing \(_{t}\) will not lead to a loss in information in the latent representation \(_{t}\). This is because the bottleneck on \(_{t}\) only constrains the model in using information from \(_{t}\) to predict \(_{t}\), and not necessarily in predicting \(_{t+1}\). See Appendix B for a comparison of our method against \(L_{1}\) and \(L_{2}\) norm regularization on latent states. Notably, our method can _also_ lead to state compression, in that \(e_{}\) will be encouraged to omit features from \(_{t+1}\) that cannot be predicted easily with little information from \(_{t}\).

Unfortunately, the above loss function has a trivial solution: it can be minimized completely if \(d_{}\) and \(e_{}\) output a constant \(\) vector for all states and state-action pairs . This issue is referred to as _representational collapse_. Representational collapse can be remedied in various ways. To show the generality of our information bottleneck, we combine it with two different approaches for mitigating representational collapse, a self-supervised approach for model-based and model-free RL in Section 3, and a contrastive approach for future state prediction in Section 4.

## 3 Parsimonious dynamics for Reinforcement Learning

### Model-based RL

We evaluated the PLSM's effect on planning algorithms' ability to learn policies in continuous control tasks. To do so, we built upon the TD-MPC algorithm , an algorithm that jointly learns a latent dynamics model and performs policy search by planning in the model's latent space.

TD-MPC makes use of a Task-Oriented Latent Dynamics (TOLD) model. This dynamics model is trained to predict its own future state representations from an initial state and action sequence while making sure that the controller's policy \(_{}\) and \(Q\)-value function are decodable from the latent state (hence the name _task-oriented_ latent dynamics). TOLD falls under the category of Self Predictive Representation (SPR) models, since it uses an exponentially moving target encoder \(e_{}^{-}\) with the stop-gradient operator to learn to predict its own representations. For planning TD-MPC uses the Cross-Entropy Method , searching for actions that maximize \(Q\)-values.

Only minimal adjustments to the TOLD model are necessary to attain parsimonious dynamics. Instead of predicting the next latent directly from the current latent and action \(}_{t+1}=d_{}(_{t},_{t})\), we use a query network \(f_{}\), mapping latent state-action tuples to \(_{t}\) and then minimize

\[_{}=||(e_{}(_{t+1}))-( _{t}+d_{}(_{t},_{t}))||_{2}^{2}+||_{ t}||_{2}^{2}\]

We evaluated the efficacy of parsimonious dynamics for control in five state-based continuous control tasks from the DeepMind Control Suite (DMC) . We chose the following environments: _i_) acrobot-swingup, due to its challenging and chaotic dynamics. _ii_) finger-turn hard, which poses a challenging exploration problem that TD-MPC was found to struggle with. _iii_) quadruped-walk, _iv_) quadruped-run and _v_) humanoid-walk due to the high-dimensional dynamics. These tasks have dynamics that appear complex in the original state-space but could potentially be simplified in an appropriate latent space by introducing a dynamics bottleneck.

We trained the latent dynamics and planning models in the five tasks up to a million environment steps. Scores for TD-MPC are obtained from the original implementation provided by the authors1. Again we used \(=0.1\) for all tasks except for humanoid-walk, where \(=0.001\) was more successful. Otherwise we relied on the standard hyperparameters from . We see clear performance gains due in all tasks except quadruped-run (Fig. 3A). Our results suggest that modeling the world with simple dynamics can be beneficial for RL and trajectory optimization.

### Distracting visual control

Since our regularization compresses away aspects of the environment whose dynamics are unpredictable given the agent's actions, it could be beneficial in control tasks with distractors. We implemented PLSM on top of RePo , relying on the authors' official implementation2. RePo is a model-based RL algorithm based on the Dreamer  architecture. Here the environment dynamics are represented through a GRU network which is updated recurrently with latent state and action variables. To make the dynamics parsimonious, we update the GRU using a compressed query representation \(_{t}\) subject to \(L_{2}\) regularization instead of the full state representation \(_{t}\), resulting in recurrent dynamics that are softy state-invariant. We then trained RePo with and without our regularization on the Distracting Control Suite , a challenging visual control benchmark based on DMC, where the background is replaced with a random, distracting video from the 2017 Davis video dataset. These videos are independent of the agent's actions, irrelevant for rewards, and change from episode to episode.

We trained RePo with and without the PLSM objective in five Distracting Control Suite tasks for one million environment steps across five seeds. We used a regularization coefficient of \(=1\)e-7

Figure 3: PLSM, when incorporated into either the TD-MPC algorithm (**A**), or RePo (**B**), improves planning in continuous control tasks with high-dimensional and complex dynamics, and visual distractions, respectively. Lines show the average return attained across 15 evaluation episodes, averaged over five seeds. The shaded region represents the 95% confidence interval.

for all tasks. This produced a considerable improvement over RePo in more challenging tasks like hopper-stand, walker-run and finger-spin (see Fig. 3B). Our results suggest that encouraging the dynamics model to represent the effects of actions more consistently can improve its generalization ability in environments with distractors.

### Model-free RL

Next we tested whether the learned latent space of PLSM could provide useful for model-free learning. Several methods rely on latent dynamics learning as an auxiliary objective for model-free RL [5; 7; 14; 15]. Since PLSM arranges the latent space in way that makes state transitions more predictable, it may discover useful state features and ignore aspects of the environments that would make the dynamics unpredictable otherwise. We build upon the SPR implementation for Atari3, which uses a latent dynamics model for next latent state prediction. We alter the architecture of this latent dynamics model in the same way we did for TD-MPC, and add the \(_{t}\) norm to the loss function. Setting \(=5\) and leaving all other hyperparameters as per the standard implementation, we train the PLSM augmented SPR algorithm on 100k environment steps across 5 seeds on all 26 Atari games. We use the SPR scores reported in  as our baseline.

Across several games we see substantial improvements to human normalized score (see Fig. 4). Averaging over all games, using PLSM dynamics improves human normalized scores by 5.6 percentage points (61.5 % for SPR vs 67.1% with PLSM). While we see improvements in games such as UpNDown and Kangaroo, there are other games where the regularization impacts performance negatively. Performance could potentially improve by fine-tuning the regularization strength for these domains. See Appendix 1 for the full score table.

## 4 Future state prediction

We found that our regularization improved model-free and model-based performance across several environments. Next we evaluated whether PLSM also generally improves world models' long-horizon prediction accuracy in latent space. Using the evaluation framework and environments from  (see Fig. 13 for example observations), we generated datasets of image, action, next-image triplets from two Atari games (Pong and Space Invaders), and grid worlds with moving 3D cubes and 2D shapes, where each action corresponds to moving an object in one of four cardinal directions. To make the learning tasks more challenging, we increased the number of movable objects from 5 to 9. Additionally we created an environment based on the dSprite dataset , with four sprites

Figure 4: Changing the dynamics model in SPR to PLSM increases score in several Atari games, with little implementation overhead. On average, human normalized scores are higher when using PLSM dynamics. Bars show difference in human normalized score between SPR with and without PLSM dynamics, averaged over five seeds.

traversing latent generative factors on a random walk. The sprites had 6 generative factors: Spatial \(x,y\) coordinates, scale, rotation, color, and shape. The sprites could vary in coordinates, scale, and rotation within episodes, and additionally vary in color across episodes. Lastly, we evaluate PLSM on a dynamic object interaction dataset with realistic textures and physics without actions, MOVi-E , to see if our method can be beneficial in action-free settings.

Following , we pair PLSM with a contrastive loss function to mitigate representational collapse: The contrastive loss encourages that different states are _distinguishable_ in the latent space. Given a latent state and action, the model should minimize the distance between the predicted and true future latent state, while maximizing the distance between the predicted future latent state \(_{t+1}\) and all other latent states in the training batch \(^{-}\), up to a margin \(\).

\[_{}=||_{t+1}-}_{t+1} ||_{2}^{2}+(0,-||^{-}-}_{t+1}||_{2}^{2}) \]

To apply our regularization on the contrastive dynamics model, we simply add the norm of the query representation to the contrastive loss, similarly to equation (8). We fitted the regularization coefficient \(\) with a grid search and found a value of \(0.1\) to work the best. As a baseline we used the unregularized contrastive model from , referred to as CWM (for Contrastive World Model), and its dynamics are defined through Equation (2), (4) and (9). We also combine PLSM with the slot-based version of this model, called C-SWM (see Appendix H for results).

The models were scored based on their ability to correctly predict future states (e.g. latent prediction accuracy). The models were trained and evaluated using the same parameters and metrics as in : Given a state \(_{t}\), a sequence of \(N\) actions \(_{t},...,_{t+N-1}\), and the resulting state \(_{t+N}\), we make the model predict its latent representation of \(_{t+N}\) from the initial state and action sequence. The models were evaluated at several prediction horizons. Finally, we report the Hits at Rank 1 accuracy for transitions in the test set, a common test metric for contrastive models .

### PLSM improves long-horizon prediction accuracy

PLSM could better predict its own representations further into the future in five out of the six datasets (Fig. 5). We see the greatest gains in the cubes and shapes environments. Here, all 9 objects can collide with each other and the grid boundaries depending on their position. Always considering all possible interactions makes it challenging for a model to generalize to novel transitions. A more parsimonious solution is simply to represent whether or not the object in question would collide or not if moved in the direction specified by \(_{t}\). Our results suggest that our regularization can help learn such representations, affording better generalization to left-out transitions.

Figure 5: PLSM improves contrastive world models’ accuracy in long-horizon latent prediction in five out of six environments. In the cubes and shapes dataset, the PLSM is close to perfect even when predicting as far as 10 timesteps in the future. Lines show accuracy on entire test data averaged over five random seeds. The shaded region corresponds to the standard error of the mean.

In one environment, Pong, we do not see an advantage in encouraging parsimonious dynamics. Here, various components are outside of the agent's sphere of influence, for instance, the movement of the opponent's paddle. This makes it challenging for the PLSM to capture all aspects of the environment state in its dynamics. In environments with non-controllable dynamics, we offer a remedy by only enforcing half of the latent space to be governed by parsimonious dynamics, and allowing the other half to be unconstrained. This hybrid model in turn shows the strongest performance in the Atari environments. See Appendix G for details.

### Generalization and robustness

Next, we evaluated the generalization and robustness properties of PLSM. For the cubes and shapes environments, we generated novel datasets where the number of moving objects was lower than in the original training data. In these datasets, the number of moving objects varied from 1 to 7. We also probed the models' robustness to noise. We corrupted test data from the dSprite environment with noise sampled from Gaussian distributions. Models were tested both in a low noise (\(=0.1\)) and a high noise (\(=0.2\)) condition. See Supplementary Fig. 14 for example data.

When tested on scenes with fewer objects than the training data, we see a general decrease in accuracy because of the domain shift. Still, with our information-theoretic dynamics bottleneck the model generalizes significantly better to the out-of-distribution transition data (Fig. 6). Since PLSM seeks to predict the next state using as little information from its latent representations as possible, it is more likely to learn that the blocks move in a way that is generally invariant to the number of other blocks in the scene.

Lastly, PLSM proved more robust to Gaussian noise than the unconstrained dynamics models: In the high noise condition, PLSM dynamics still accurately predict almost 60% of the tran

Figure 6: PLSM improves generalization and robustness in contrastive models: When exposed to scenes with fewer objects than trained on (cubes and shapes environment), or corrupted data (dSprite environment) from the test set, PLSM improves accuracy over the CWM. Lines represent the average of models trained across five seeds. Shaded regions and bars reflect the standard error of the mean.

Figure 7: Latent states \(_{t}\) carry decodable information about the data generating factors, whereas query states \(_{t}\) do not. When conditioning on an action \(_{t}\), query states carry more information about the object that the action changes.

We also investigated the representations learned by the query network (see Fig. 7). To verify that PLSM learns objects' attributes (such as position and orientation), we attempted to decode ground truth object positions, scales and orientations in the dSprite dataset from PLSM latent states. We find that one can decode these ground truth factors with high accuracy from the latents. However, trying to decode these variables from the query states yielded substantially lower accuracy, indicating that they contain less information about the generative factors of the environment. Interestingly, when we condition PLSM on an action that affects only one object, we can decode attributes of that object more accurately from the query state than average. We observe this because the query state is designed to only encode information that is relevant to predict the effect of individual actions.

In sum, parsimonious dynamics regularization improves both generalization and robustness properties in the three environments tested. The mutual information bottleneck on \(_{t}\) therefore not only improves prediction accuracy for data in the training distribution, but may also allow the model to generalize better to out-of-distribution data, and improve its robustness to noisy observations.

## 5 Related work

Our approach introduces a mutual information constraint between the latent states \(_{t}\) and the latent dynamics \(_{t}\) inferred by the model. Several methods focus on state compression for dynamics modeling [1; 22; 5; 6; 23; 24]: The Recurrent State Space Model (RSSM) [22; 2; 25], uses a variational Auto-Encoder in combination with a recurrent model (e.g. a GRU ) to infer compact latent states in partially observable environments. Latent consistency is enforced by minimizing the Kullback-Leibler (KL) divergence between latent states predicted by the model and latent states inferred from pixels. The KL term regularizes the latent state not to contain more information than can be predicted by the dynamics model [2; 27]. Unlike our approach, this information bottleneck is not applied to the dynamics \(_{t}\) themselves, but to the representations \(_{t}\). Expanding on this line of work, RePo  discards image reconstruction from the RSSM, and simply enforces that the model can reconstruct the environment's reward function, leading to stronger compression and improved performance in tasks with unpredictable elements. Similar approaches like Denoised MDP  only model _controllable_ and reward-relevant aspects of the environment. While simplified latents can make transition dynamics more tractable to model, they do not necessarily give rise to the systematic action representations that we are interested in. Lastly, Self Predictive Representation (SPR) models [7; 6; 5] learn dynamics by predicting the future representations of a target encoder. SPR models have been used both for model-free and model-based control.

Mutual information minimization is used in many deep learning frameworks more generally:  and  use variational methods for minimizing the mutual information between the network's inputs \(X\) and latent representations \(Z\) while maximizing the mutual information between representations \(Z\) and outputs \(Y\). Mutual information minimization also has links to generalization ability [31; 32; 33; 34; 35], robustness in RL [36; 37], and exploration [38; 39]. Our information bottleneck differs from previous approaches in that it directly constrains the effect the latent state can have on the residual term in the latent dynamics over and above the agent's actions.

Closest to our regularization method is the _past-future_ information bottleneck [40; 41]. Here the mutual information between sequences of past states and future states is minimized [42; 43]. While this method simplifies dynamics, our approach differs in important ways: Rather than representing the environment's dynamics, say, using a low number of its principal components, we treat the dynamics operator \(_{t}\) itself as a random variable, and minimize its conditional dependence on \(_{t}\). Furthermore, when \(_{t}\) is fully disentangled from \(_{t}\), each action can be seen as a transformation that acts on the latent state space in the same way, invariantly of \(_{t}\)[44; 45]. We model the dynamics as _softly_ state-invariant, allowing us to predict future latents both accurately and parsimoniously.

## 6 Conclusion

We have proposed a world model that tries to represent the effect of actions parsimoniously. Our model predicts future states while minimizing the dependence between the predicted dynamics \(_{t}\) and the latent state representations \(_{t}\). Optimizing this objective makes the effect of the actions on the agent's latent state more predictable. Combining our objective with different model classes - contrastive world models and SPR models - we observed consistent improvements in models' ability to predict their own representations accurately, generalize to novel and noisy environments, and perform planning and model-free control in high-dimensional and pixel-based environments with complex dynamics. Overall, our results suggest that systematic action representations can offer important improvements to the generalization ability and data-efficiency of world models.

**Limitations:** Our model, in its current formulation, assumes that actions have predictable and deterministic effects on the environment. Aspects of the environment that do not behave predictably conditioning on actions are susceptible to be ignored by the model, even if they are relevant for the downstream task. While performance in Atari was improved on average, this aspect led to reduced performance in some tasks. Similarly, the degree of regularization \(\) needs to be tuned to achieve the right level of dynamics compression for different environments.

**Future directions:** A promising future direction is to combine our mutual information regularization with recurrent models. In partially observable, non-Markovian environments, next-state prediction is often done with a recurrent model that uses the agent's history as input. Using our bottleneck here would correspond to the assumption that the effect of the agent's actions are softly _history_-invariant. Another promising avenue of research is to combine PLSM with discrete dynamics. Many successful world modelling approaches assume that the latent state space is categorical [25; 46]. Instead of modelling transitions using affine transformations \(\), transitions could be modelled by predicting transition matrices. Finally, recent recent advances in controllable video-generation like Genie  and UniSim  both incorporate actions into their video models. Genie infers _latent actions_ that explain transitions in videos, and UniSim conditions on actions and language instructions to generate controllable videos. Modelling the effect of actions in a parsimonious way could improve the accuracy and generation capabilities of such systems.