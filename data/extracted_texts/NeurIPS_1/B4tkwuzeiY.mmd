# Grammar Prompting for Domain-Specific Language Generation with Large Language Models

Bailin Wang\({}^{}\) Zi Wang\({}^{}\) Xuezhi Wang\({}^{}\) Yuan Cao\({}^{}\) Rif A. Saurous\({}^{}\) Yoon Kim\({}^{}\)

\({}^{}\)Massachusetts Institute of Technology \({}^{}\)Google DeepMind \({}^{}\)Google Research

{bailinw, yoonkim}@mit.edu, {wangzi, xuezhiw, yuancao, rif}@google.com

###### Abstract

Large language models (LLMs) can learn to perform a wide range of natural language tasks from just a handful of in-context examples. However, for generating strings from highly structured languages (e.g., semantic parsing to complex domain-specific languages), it is challenging for the LLM to generalize from just a few exemplars. We propose _grammar prompting_, a simple approach to enable LLMs to use external knowledge and domain-specific constraints, expressed through a grammar in Backus-Naur Form (BNF), during in-context learning. Grammar prompting augments each demonstration example with a specialized grammar that is minimally sufficient for generating the particular output example, where the specialized grammar is a subset of the full DSL grammar. For inference, the LLM first predicts a BNF grammar given a test input, and then generates the output according to the rules of the grammar. Experiments demonstrate that grammar prompting can enable LLMs to perform competitively on a diverse set of DSL generation tasks, including semantic parsing (SMCalFlow, Overnight, GeoQuery), PDDL planning, and SMILES-based molecule generation.

## 1 Introduction

Prompting large language models (LLMs) with demonstrations optionally combined with natural language instructions has been shown to be an effective approach for surfacing their myriad capabilities acquired through pretraining . This approach is however inadequate for applications where the task specifications cannot be fully delineated through just a handful of exemplars, for example in semantic parsing where an LLM must translate a natural language utterance to an executable program in a domain-specific language (DSL). DSLs often incorporate domain-specific abstractions and semantics that are difficult to characterize via just a few demonstrations. And unlike general-purpose programming languages, DSLs are by definition specialized and thus unlikely to have been encountered often enough (or at all) during pretraining for the LLM to acquire its full syntax.

How can we draw on the few-shot learning capabilities of LLMs to generate structured strings that are substantially different from those seen during pretraining? This work explores _grammar prompting_ as a simple approach for data-efficient generation of structured languages where an output string in the language can be derived through a series of symbolic manipulations. We exploit the fact that constraints over a structured output space can often be succinctly described by a context-free grammar in Backus-Naur Form (BNF), which is commonly used to define the syntax of a language. Grammar prompting augments each in-context example \((,)\) with a _specialized_ BNF grammar \(G[]\) that is minimally sufficient for generating \(\). Given a new input, the LLM first predicts the specialized BNF grammar and then generates the answer conditioned on the grammar.

Grammar prompting follows the recent line of work which enhances the few-shot reasoning capabilities of LLMs by interleaving intermediate "reasoning" steps between each in-context input andoutput [51; 24; 86; 80; 73]. The key difference in our approach is that the intermediate variable is in the form of a formal grammar rather than in natural language, which focuses on eliciting the symbolic manipulation capabilities of LLMs. The use of a formal grammar moreover makes it possible to impose constraints during incremental decoding such that syntactic validity is guaranteed. Finally, unlike chain-of-thought-style prompts  which typically require manual verbalization of the intermediate reasoning steps, in our approach the specialized grammar \(G[]\) can be derived automatically by parsing the output \(\) with the full (unspecialized) DSL grammar.

To summarize,

* [noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt,leftmargin=*]
* We propose grammar prompting as a simple approach for enabling LLMs to generate highly structured languages from just a few exemplars.
* We design a constrained LLM decoding algorithm tailored to grammar prompting, which guarantees syntactic validity while minimizing the number of LLM API calls.
* We apply grammar prompting to various domain specific languages for semantic parsing (SMCalFlow, Overnight, GeoQuery), AI planning (PDDL), and molecule generation (SMILES), and find that it can meaningfully improve upon standard prompting baselines in the few-shot setting.

## 2 Background

In this section, we define our problem and review the few-shot learning method that we build on.

### Problem Formulation: Domain-Specific Language Generation

Let \(^{*}\) be the set of all finite strings over an alphabet \(\), and further let \(D^{*}\) be a domain-specific language (DSL) for an application of interest. Given an input \(\) (e.g., a natural language command) we are interested in generating \( D\) (e.g., a program in a DSL fulfilling the command), as shown by the following calendar assistant example from SMCalFlow :

\[:.\] \[:(\& )\]

DSLs are crafted by experts who use their domain-specific knowledge to incorporate higher-level abstractions than are typically found in general-purpose programming languages. We assume access to an expert-defined grammar \(G\) that fully specifies the DSL's syntax. As is the case with many DSLs, we further assume that \(G\) is a context-free grammar in Backus-Naur Form (BNF). See Figure 1 for a simple example adapted from SMCalFlow . Letting \(L(G)\) be the language generated by \(G\), we have \(D L(G)^{*}\) (not all syntactically valid programs are semantically valid).

### Few-shot Learning with Large Language Models

In-context learning with large language models (LLMs) has been shown to be an effective approach for few-shot learning . Under this approach, a pretrained LLM is conditioned on \(N\) demonstration examples \((^{(i)},^{(i)})_{i=1}^{N}\) followed by a test example \(\), and the output is given by decoding from the prompted LLM, i.e., \(P_{}(,(^{(i)},^{(i)})_{i=1}^{N})\). The demonstration examples can be optionally preceded by natural language instructions to further improve performance or even enable zero-shot learning [85; 62]. Recent work has additionally shown that interleaving natural language verbalizations of intermediate reasoning steps between each \(^{(i)}\) and \(^{(i)}\) can greatly improve few-shot performance on complex reasoning tasks [51; 86; 80; 73; 16].

The effectiveness of few-shot in-context learning depends both on how useful the implicit knowledge acquired through pretraining is for the task, and on how effectively the task specifications can be conveyed through the demonstrations. For DSL, the structured nature of combinatorial output space (i.e., the DSL grammar \(G\)) cannot be adequately captured through just a handful of demonstrations. Thus, few-shot generation of strings of a DSL remains challenging for LLMs.

Figure 1: A simple BNF grammar for a calendar DSL.

## 3 Grammar Prompting

Grammar prompting exploits the fact that while the actual strings of a DSL may not have been encountered frequently enough (or at all) during pretraining for the LLM to implicitly acquire its syntax, the LLM will likely have encountered many instances of _metalanguages_ (languages used to describe other languages). BNF grammars are a standard metalanguage for specifying a language's syntax, and are expected to occur in the LLM training corpus with some frequency (e.g., in computer science textbooks). We thus focus on using BNF grammars for few-shot DSL generation.

Let \(G=_{j=1}^{M}\{r_{j}\}\) be an extended BNF grammar where each rule \(r_{j}\) is of the form

<symbol> ::= <expr1> | <expr2> |...

Here <symbol> is a nonterminal symbol and each <expr1> is a sequence of nonterminal and terminal symbols.1 A straightforward approach for incorporating a BNF grammar during in-context learning is to simply prepend the string representation of the full grammar \(G\) to the demonstration examples, along with an instruction to use the grammar. However in preliminary experiments, we found that this did not yield any improvements.2

### Specialized Grammars

We propose to use _specialized grammars_ to enable better use of domain-specific knowledge and constraints. A specialized grammar \(G^{} G\) is a grammar obtained from taking a subset of the rules of the full grammar \(G\). We further define \(G[]\), a _minimal specialized grammar_ of \(\), to be a BNF

Figure 2: Example of grammar prompting for a calendar DSL. We interleave the minimal specialized grammar \(G[^{(i)}]\) between the demonstrations \(^{(i)}\) and \(^{(i)}\). During decoding, the LLM first predicts the specialized grammar \(\), and then predicts the program \(}\) conditioned on \(\). The blue portion is not part of the actual prompt and only shown for illustrative purposes.

grammar with the following properties: (1) \( L(G[])\), and (2) \( r G[],\  L(G[]\{r\})\).3 We can readily obtain a minimal specialized grammar by using \(G\) to parse \(\) and then taking the union of rules that were used in the derivation of \(\).

Grammar prompting feeds a sequence of \((^{(i)},G[^{(i)}],^{(i)})_{i=1}^{N}\) along with \(\) as a prompt to an LLM. For inference we first obtain the specialized grammar with an (approximate) \(\) decoding

\[=*{arg\,max}_{G^{} G}\ P_{} (G^{}\,|\,,(^{(i)},G[^{(i)}],^{(i)})_{i=1}^{N}).\]

We then obtain the program conditioned on \(\),

\[}=*{arg\,max}_{ L()}\ P_{ }(\,|\,,,(^{(i)},G[^{(i)}],^ {(i)})_{i=1}^{N}).\]

We discuss how to perform constrained decoding with \( G\) and \(} L()\) in the next section. Grammar prompting views DSL program generation as a _grammar specialization_ process where given a natural language specification \(\), a set of production rules, \(\), is selected from \(G\), and then a program \(}\) is deduced according to the selected rules. Grammar prompting can also be viewed as an instance of chain-of-thought prompting  where the intermediate thought is in the form of a formal grammar. However, unlike typical chain-of-thought prompting where the answer is (usually) deterministic given the intermediate reasoning steps, in our case there is still some uncertainty with respect to \(}\) given \(\) (e.g., \(L()\) could still be infinite).

### Constrained Decoding

The use of a formal grammar as an intermediate variable makes it possible to enforce grammatical constraints during autoregressive LLM decoding. We first discuss how we enforce the constraint \( L()\). One approach to constrained decoding is to use \(\) to obtain a left-to-right Earley parser  and only decode from valid continuations at each decoding step. However this simple strategy poses several practical challenges when working with API-only LLMs. For one, a valid terminal continuation in \(\) may consist of multiple BPE tokens. Moreover, while we can sample a valid continuation at each time step by disallowing invalid tokens,4 since the set of valid continuations changes at each time step, this strategy would require calling the LLM API at each time step with the full prompt and prefix along with the disallowed continuations, which is prohibitively expensive.5

While there are many methods for grammar-constrained LM decoding , we present a simple strategy which speculatively decodes from the LLM to look ahead for multiple tokens. The pseudocode is shown in Algorithm 1. At each prediction step, we ask the LLM to speculatively decode the full program conditioned on the current prefix (lines 4-5). If the resulting continuation leads to a valid program, we return it (lines 6-7). Otherwise, we consult an Earley parser to extract the longest valid prefix from the current prediction (\(_{}\)), along with a set of valid terminals that can follow the prefix (\([_{}]\)). Finally, we rely on the LLM's probabilities to decide which terminal to use, with which a new partial program can be constructed (lines 10-11).6 Figure 3 illustrates one prediction step where the predicted program is corrected into a new valid partial program. Note that \(\) can consist of multiple BPE tokens, e.g., "FindManager(" in Figure 3. By scoring over multi-token terminals, the search procedure is implicitly augmented by looking ahead for a few tokens.

We use a similar procedure to operationalize the constraint \(G^{} G\), except that \(\) (used at Algorithm 1, line 9) is constructed with a _metagrammar_ (i.e., the grammar of \(G\)) for grammar prediction. See appendix A.1 for more details. In our ablation study we find that while these constraints are helpful insofar as they guarantee syntactic validity, grammar prompting still meaningfully improves upon standard prompting with even with simple unconstrained decoding.

## 4 Experiments

We apply grammar prompting to diverse domains: DSLs for semantic parsing (SMCalFlow, Overnight, GeoQuery), an action DSL (PDDL planning), and a molecule generation DSL (SMILES). These experiments are not necessarily intended to improve upon the state-of-the-art on these benchmarks but rather intended to assess whether LLMs can improve upon standard prompting for few-shot DSL generation by learning to predict and use grammars during in-context learning.

### Semantic Parsing for Tool Usage

Software tools are typically accompanied by a collection of human-interpretable APIs which provide a platform for developers to interact programmatically with the tools. These APIs constitute a DSL, where each production rule of the grammar specifies the input and output types for a specific API call (see Figure 1 for an example). These tools demonstrate a broad spectrum in terms of DSL complexity, ranging from single-function tools such as Google(user_query), Translate(sentence, language) to more complex tools such as the entirety of Wolfram language.7 Enabling LLMs to use external tools via APIs is an important step towards enhancing their capabilities [63; 56; 53; 72; 47].

We test our approach on standard semantic parsing benchmarks involving complex DSLs: SMCalFlow , which features human-generated utterances about calendar management (see Figure 2); GeoQuery  which features queries against a US Geography database; and Overnight-Blocks , which features queries about blocks in a synthetic block world. See appendix B for examples of input-output pairs along with the specialized grammars. The original benchmarks target the training of conventional semantic parsers and thus contain hundreds/thousands of training examples. Even prompting-based approaches on these benchmark rely on retrieval-based in-context learning which first retrieves \(m\) exemplars from a large training set of \(n\) examples (\(n m\)) based on some similarity measure (e.g., BM-25), and then performs in-context learning with the retrieved exemplars [57; 95; 68; 46]. In contrast, we target the true few-shot setting where we only assume access to 16-32 demonstration examples.

Our baselines here include: (1) standard prompting, (2) standard prompting with constrained decoding based on the full DSL grammar \(G\)[68; 64], and (3) a derivation tree-based prompting baseline which imbues more structural information to the exemplars by feeding the linearized derivation

Figure 3: Illustration of how an predicted program is corrected in our proposed Earley-based constrained decoding. The final partial program will be subsequently fed into the LLM for continuation.

tree instead of the surface form program.8 We use Codex-davinci-002  as the base LLM for these main experiments. Language models trained on code (such as Codex) have shown to be particularly effective on semantic parsing benchmarks . We evaluate according to program accuracy (matching the predicted and reference programs) as well as execution accuracy (same execution in both programs) if possible.

Few-shot results.The main results are shown in Table 1. We find that grammar prompting can meaningfully improve upon the standard prompting baseline even without constrained decoding. Interestingly, grammar prompting outperforms derivation tree prompting which actually provides _more_ information than the minimal specialized grammar \(G[]\) (since the derivation tree explicitly shows how the rules are actually applied to obtain the program). This potentially indicates that having the LLM "plan out" the program by forcing it to predict the specialized grammar \(\) first is an effective strategy. We also analyze the effect of constrained decoding on the number of LLM API calls in Table 7 of appendix A.1, where we observe that constrained decoding requires roughly three times more API calls than unconstrained decoding. However, despite the promising performance of grammar prompting, there is a large gap between using the predicted grammar versus using the oracle grammar (i.e., setting \(=G[]\)), indicating opportunities for further work in this area.

Retrieval-based in-context learning.While our core target application is few-shot semantic parsing, we also apply grammar prompting for retrieval-based in-context learning to test whether it can still improve performance in the data-rich regime and also to compare against prior work on these benchmarks. Results in Table 2 (left) show that grammar prompting can improve results even in this setting, although the improvements are less pronounced than in the few-shot setting.

Out-of-distribution generalization.We experiment to see whether grammar prompting can improve compositional generalization on GeoQuery. Specifically, we test grammar prompting on the compositional splits of GeoQuery split from Shaw et al. . These splits feature structural divergence between training and test examples, e.g., programs have different templates or length. Results in Table 2 (right) shows that grammar prompting can improve upon standard prompting, across all splits (Template, TMCD, Length).

    &  &  \\ Model & GeoQuery & SMCalFlow & Overnight-Blk & Template & TMCD & Length & NewFunc \\ (\#CL examples / retrieval set) & (32/560) & (16/128) & (32/1,436) & (32/441) & (32/440) & (32/440) & (32/453) \\  Previous Work & 86.1\({}^{}\) & 60.7\({}^{}\) & 65.2\({}^{}\) & – & – & – & – \\  Standard Prompting & 96.8 & 60.0 & 69.4 & 93.2 & 77.1 & 86.4 & 63.3 \\ Grammar Prompting & 97.9 & 62.8 & 70.2 & 95.7 & 86.6 & 88.6 & 90.8 \\ _w. oracle grammar_ & 98.6 & 88.9 & 97.2 & 97.9 & 95.0 & 95.7 & 96.2 \\   

Table 2: Results on retrieval-based in-context learning (left) and compositional generalization (right) with Codex. GeoQuery and Overnight-Blk show execution accuracy while SMCalFlow shows program accuracy. The numbers with \({}^{}\), \({}^{}\) and \({}^{}\) are taken from Herzig and Berant , Ye et al.  and Cao et al. , respectively.

    &  &  &  \\ Approach & Prog. & Exec. & Prog. & Prog. & Exec. \\  Standard Prompting (unconstrained decoding) & 60.7 & 81.5 & 46.4 & 29.3 & 54.7 \\ _w. constrained decoding (\(} L(G)\))_ & 61.1 & 81.8 & 49.2 & 29.3 & 54.7 \\ Linearized Derivation Tree Prompting & 58.6 & 77.5 & 50.0 & 27.3 & 56.4 \\  Grammar Prompting (unconstrained decoding) & 67.1 & 87.5 & 50.8 & 34.8 & 57.4 \\ _w. grammar constraint (\( G\))_ & 67.9 & 88.6 & 51.3 & 37.1 & 60.4 \\ _w. grammar and program constraint (\(} L()\))_ & 69.6 & 88.9 & 52.4 & 37.6 & 60.9 \\ _w. oracle grammar (\(=G[]\))_ & 95.7 & 96.1 & 80.0 & 73.9 & 94.2 \\ _w. oracle grammar + program constraint_ & 95.7 & 96.8 & 83.6 & 74.4 & 96.5 \\   

Table 1: Results on few-shot semantic parsing with Codex with various decoding strategies. GeoQuery and Overnight-Blk use 32 in-context examples, and SMCalFlow uses 16 examples. We show both program (Prog.) and execution (Exec.) accuracy when possible.

We next assess whether grammar prompting can enable LLMs to make zero-shot use of _unseen functions_ (NewFunc) that are not even part of the retrieval set. We set aside 8 functions (smallest, shortest, most, highest, sum, population_1, count, major) and remove them from the retrieval set, simulating a scenario where new functions are supported in the backend yet no NL-program paired data is available for adapting a semantic parser. Note that for GeoQuery (and Overnight-Blk), we always prepend the full DSL grammar (\(G\)--which includes the held-out functions--before the in-context exemplars. Table 2 (right-most column) shows that grammar-prompted LLMs achieve significantly better performance than standard prompting. Our results suggest that the explicit prediction of specialized grammars elicits understanding and reasoning at the grammar level, thereby enabling generalization to unseen functions. We also found that without constrained generation, LLMs were often able to guess functions that did not exist but were nonetheless sensible. An interesting direction is to explore whether LLMs can tackle DSL-open benchmarks such as LARC .

Different base LLMs.We finally experiment with grammar prompting across different base LLMs. Since GPT-3.5's 4K token limit is smaller than Codex's (8K) and GPT-4's (8K) limits, we use fewer exemplars in these experiments than before (24/8/16 exemplars for GeoQuery/SMCalFlow/Overnight-B respectively). Due to API cost, we limit our experiments to a smaller subset of 100 test examples instead of the full test set.

Table 3 shows that grammar prompting improves upon standard prompting in the majority of the settings. The exceptions are SMCalFlow with GPT-3.5 where both methods performed poorly, and GeoQuery with PaLM 2-L, where standard prompting already performed well.

### Class-Specific Molecule Generation

We next demonstrate an application of grammar prompting beyond language parsing problems with a molecule generation task. Existing methods for molecule generation typically focus on training specialized neural models using large training sets [45; 37; 15; 2; 79; 61; 19]. We instead follow Guo et al.  and explore a few-shot setting where the task is to generate class-specific molecules given a small number of exemplars of that class. Formally, given a small set of molecules \(\{^{(i)}_{c}\}_{i=1}^{N}\) belonging to a particular molecule class \(c\{,\ \ ,\ \},\)our goal is to generate novel molecules \(_{c}\) of the same class that can be synthesized using existing molecules. Since the in-context examples in this case will only consist of molecules of the same class, the "input" \(^{(i)}_{c}\) is the empty string in this case. The data contains 32 Acrylates, 11 Chain Extenders, and 11 Isocyanates (see appendix G of Guo et al. ).

While molecules can be more faithfully represented with 3D graph structure, the SMILES string representation  remains popular due to its ease of use.9 The specialized grammars \(G[_{c}]\) (which are specialized from the SMILES grammar) encode various structural properties of the molecule that are specific to the molecule class. Figure 4 shows an example of a specialized grammar and the corresponding molecule in SMILES format. In this example, ring_closure ::= "1" specifies the number of rings, and branch ::= "(" smiles ")" specifies whether there is a branch.

We test our approach by generating 100 molecules for each class and assessing the quality of the generated molecules. In addition to the standard prompting baseline, we also run the graph grammar baseline from Guo et al.  which learns a hypergraph grammar  from the given molecules. We use four metrics: _Validity (V)_, the percentage of chemically valid molecules; _Diversity (D)_, average pairwise Tanimoto distance over Morgan fingerprints ; _Retrosynthesis score (R)_, the percentage

  
**Base LM** & **Method** & **GeoQuery** & **SMCalFlow** & **Overnight-Blk** \\  Codex & Standard & 83 & 27 & 63 \\  & Grammar & 95 & 35 & 66 \\  GPT-3.5 & Standard & 75 & 9 & 49 \\  & Grammar & 86 & 5 & 67 \\  GPT-4 & Standard & 85 & 32 & 56 \\  & Grammar & 98 & 36 & 62 \\  PaLM 2-L & Standard & 90 & 14 & 59 \\  & Grammar & 87 & 17 & 62 \\   

Table 3: Results with different base LLMs on a subset of 100 examples sampled from the original test set. GeoQuery and Overnight-Blk show execution accuracy, while SMCalFlow shows program accuracy.

[MISSING_PAGE_FAIL:8]

domains in PDDL planning, including Blocks, Depot and Satellite. For the action space, we use either a set of primitive actions (Prim) or an augmented set with macro actions (Macro). In addition to standard prompting, we add two more baselines: (1) No LLM: planning with the entire set of actions; (2) Min Macro: where we construct a minimal set of macro actions for each domain by selecting actions from existing plans for the training tasks. The Min Macro baseline is a domain-specific method to reduce the action space. By comparing to Min Macro, we can verify the effectiveness of instance-specific v.s. domain-specific action selection. See appendix A.3 for more details.

Results.We evaluate the efficiency of planning in terms of the number of search nodes created/expanded, as well as the success rate. Table 5 shows the promising performance of LLM-guided planning via grammar prompting. In Blocks, grammar prompting significantly improves efficiency while maintaining 100% success rate. In Depot, grammar prompting with macro actions improves the success rate by 20% over the best competing baseline. In Satellite, using primitive actions yields the best performance with 100% success rate and a reduction of 57% expanded nodes comparing to the No LLM baseline. While our experiments are not intended to complete with the state-of-the-art algorithms for fast planning , they indicate the promise of LLMs for improving existing planning algorithms.

## 5 Discussion and Limitations

We discuss several limitations of our approach including some negative results. Grammar prompting did not yield any improvements for DSLs that were likely to have been frequently encountered during pretraining (e.g., regular expressions, SQL). Moreover, constrained generation based on specialized grammars led to increased API calls, and was not always beneficial for tasks beyond semantic parsing. For instance, in molecule generation we discovered that enforcing constraints can sometimes result in lower diversity. Additionally, in PDDL planning we observed that the constraints applied to prune objects can sometimes negatively impact performance, suggesting that relevant object selection is still very challenging for LLMs. It may be interesting to explore whether finetuning of moderately-sized LLMs using specialized grammars can lead to better grammar-based models for DSL generation.

On the positive front, our work demonstrates that LLMs have the capacity to understand and generate metalanguages. Working in this "metalanguage space" can be combined with chain-of-thought-style  prompts by, for example, manually providing natural language comments to the rules of the specialized grammars. We found this to improve results slightly on semantic parsing (see Figure 6

Figure 5: Example of a specialized grammar for PDDL planning in the Blocks domain. Given an input \(=(_{0},_{g})\), the specialized grammar \(G[]\) only includes necessary actions for solving this task.

    &  &  &  \\ Approach & Created & Expanded & Success & Created & Expanded & Success & Created & Expanded & Success \\  GBFS + Prim. (No LLM) & 360 & 188 & 1.0 & 18047 & 3870 & 0.4 & 8205 & 150 & 1.0 \\  Standard + Prim. & 348 & 180 & 1.0 & 17597 & 4039 & 0.4 & 6686 & 78 & 1.0 \\ Grammar + Prim. & 251 & 124 & 1.0 & 15033 & 3641 & 0.4 & 5162 & 64 & 1.0 \\  Standard + Macro. & 850 & 16 & 1.0 & 1460 & 56 & 0.4 & 4003 & 27 & 0.3 \\ Grammar + Macro. & 170 & 9 & 1.0 & 2917 & 127 & 0.8 & 3665 & 46 & 0.9 \\  Standard + Min Macro. & 228 & 8 & 1.0 & 1903 & 65 & 0.6 & 3483 & 35 & 0.8 \\   

Table 5: Results on PDDL planning. Created/Expanded refer to the number of nodes during planning (lower is better). Success refers to success rate (higher is better). Numbers are averaged over three runs using GPT-3.5.

of appendix A.1). Moreover, many scientific problems can be formally approached by representing hypotheses as DSL programs , and DSLs can enable easier encoding of human prior knowledge and scientific principles, providing a foundation for scientific discovery. Recent work shows that state-of-the-art LLMs can follow previously unseen formal systems . Techniques like grammar prompting can widen the scope of scientific problems for which LLMs could be effectively applied by more explicitly accounting for external knowledge and constraints.

## 6 Related Work

Chain-of-thought prompting.Grammar prompting extends a recent line of work on improving reasoning capabilities by requesting explicit reasoning steps as part of the prompt [51; 24; 86; 80; 14; 94]. Our approach is closely related to concurrent work on employing symbolic variables as part of the prompt [30; 50; 33; 97; 52], though we are not aware of any existing work that uses formal grammars as the intermediate reasoning step.

LLMs for program generation and semantic parsing.Generating programs from natural language specifications, a task often referred to as semantic parsing, is a sub-problem of program synthesis; for surveys, see Kamath and Das  and Gulwani et al. . Recent works [8; 89] have explored using LLMs for generating code in general-purpose programming languages (e.g., Python). Our work further extends this line by examining whether LLMs can generate DSL programs, which are intrinsically scarce. There has also been work on using LLMs for tool usage via further training  or prompting [56; 77], investigating how model scales  and retrievers [96; 46] affect in-context learning for semantic parsing, and constrained decoding [64; 68; 55] for program generation.

Neural grammars.Grammar prompting can also been seen as a "fully LLM" instantiation of a line of work on neural parameterizations of symbolic grammars [35; 17; 43; 42; 36; 100; 92; 91; 93]. Indeed, our approach to semantic parsing essentially uses prompt-based learning to define a quasi-synchronous grammar [70; 78] whose rules dynamically depend on the source sentence. Concretely, in contrast to recent works which embed learnable neural components within synchronous grammars [41; 23; 76], grammar prompting relies on the implicit in-context learning capabilities of LLMs for the learning component. (However unlike these works, our conditional grammar does not explicitly align its rules to the subparts of the source sentence).

Grammar-based molecule generation.Grammar-based methods have gained significant interest in the realm of molecule generation, offering advantages in interpretability, data-efficiency, and controllability. One line of research involves integrating generic SMILES grammars with neural networks to generate syntactically correct molecules [45; 15]. Another approach centers on data-driven induction of grammars for generation [29; 38]. Our work aligns with the former, viewing grammar prompting as a straightforward method for integrating grammar into an LLM without the need for additional training.

LLMs for planning.Recently, LLMs have been increasingly studied in the context of planning for autonomous agents. When given goals expressed in natural language in household environments, earlier works [3; 65; 34; 48] directly prompted LLMs to predict executable actions. However, in PDDL domains, recent works [69; 74] showed that LLMs underperform classical planners if the desired action sequences are very long. Grammar prompting represents a promising strategy for augmenting classical planners with LLMs to get the best of both worlds. Other related efforts include translating between problems and PDDL models  and corrective re-prompting . Besides using LLMs, integrating learning and planning has been extensively studied in the past literature, e.g., learning actions [4; 82], skills , macro-actions , rules  and guidance strategies [90; 83; 40] for more efficient planning.

## 7 Conclusion

We propose grammar prompting as a simple approach for improving few-shot DSL generation with large language models. Experiments across a range of structured languages including DSLs for semantic parsing (SMCalFlow, GeoQuery, Overnight), PDDL planning (action DSL), and molecule generation (SMILES), show that grammar prompting can improve upon standard prompting baselines. The encouraging results in semantic parsing indicate its potential to assist LLMs with tool usage, and the promising results in other domains indicate that grammar prompting can enable application of LLMs in domains that intrinsically depend on DSLs.