# Can Generative AI Solve Your In-Context Learning Problem? A Martingale Perspective

Andrew Jesson

Correspondence to adj2147@columbia.edu.  Department of Statistics, Columbia University.  Department of Computer Science, Columbia University.

Nicolas Beltran-Velez

Department of Statistics, Columbia University.  Department of Computer Science, Columbia University.

David Blei

###### Abstract

This work is about estimating when a conditional generative model (CGM) can solve an in-context learning (ICL) problem. An in-context learning (ICL) problem comprises a CGM, a dataset, and a prediction task. The CGM could be a multi-modal foundation model; the dataset, a collection of patient histories, test results, and recorded diagnoses; and the prediction task to communicate a diagnosis to a new patient. A Bayesian interpretation of ICL assumes that the CGM computes a posterior predictive distribution over an unknown Bayesian model defining a joint distribution over latent explanations and observable data. From this perspective, Bayesian model criticism is a reasonable approach to assess the suitability of a given CGM for an ICL problem. However, such approaches--like posterior predictive checks (PPCs)--often assume that we can sample from the likelihood and posterior defined by the Bayesian model, which are not explicitly given for contemporary CGMs. To address this, we show when ancestral sampling from the predictive distribution of a CGM is equivalent to sampling datasets from the posterior predictive of the assumed Bayesian model. Then we develop the generative predictive \(p\)-value, which enables PPCs and their cousins for contemporary CGMs. The generative predictive \(p\)-value can then be used in a statistical decision procedure to determine when the model is appropriate for an ICL problem. Our method only requires generating queries and responses from a CGM and evaluating its response log probability. We empirically evaluate our method on synthetic tabular, imaging, and natural language ICL tasks using large language models.

## 1 Introduction

An in-context learning (ICL) problem involves a conditional generative model (CGM), a dataset, and a prediction task . The CGM could be a pre-trained foundation model. The dataset might consist of patient histories, test results, and diagnoses. The prediction task could be providing a diagnosis to a new patient based on their history and test results . This is a complex problem that requires both accurate diagnosis and proper communication to the patient. This complexity makes it difficult to evaluate whether the model is suitable for the dataset and prediction task.

An interpretation of ICL sees a CGM prompted with in-context examples as producing data (either responses or examples of the prediction problem) from a posterior predictive under a Bayesian model. A natural question arises when we accept this premise, "Is the Bayesian model a good model for the prediction problem?" This question is what Bayesian model criticism tries to answer. This field has produced many methods but, they typically assume access to components defined by the Bayesian model, like the likelihood and posterior. In this work we show how to do model criticism in ICL using contemporary generative AI. Specifically, we demonstrate how to implement posterior predictive checks (PPCs)  and their cousins  when we only have access to the predictive distribution. The result is a practical and interpretable test on whether a model can solve an ICL problem.

Input: proof once again that if the filmmakers just follow the books Label: negative Input: in impressive Label: positive Input: the top japanese animations Label: positive Input: a spoof comedy Label: positive (a) SST2 ICL dataset \(^{n}\) Input: follows the formula, but throws in too many conflicts to keep the story compelling. (b) query z. Label: negative, Label: negative, Label: negative, Label: negative, Label: negative (c) CGM responses y (f) CGM responses y ```

Listing 1: An example illustrating two ICL problems. One that the model \(\) (Llama-2 7B ) can solve, and one that it cannot. Left: (a) examples from the SST2 task  comprising part of an ICL dataset \(^{n}\), (b) a new query \(\), and (c) some responses \(\) sampled from the CGM \(p_{}(,^{n})\) when prompted with the dataset and query. The true label is "negative" and the CGM responds correctly. Right: the same format but the dataset (d) and query (e) are taken from the MQP task . Here, the true label is "similar," but the model responds incorrectly with "different" half of the time.

## 2 What is an in-context learning problem?

An ICL problem is a tuple \((^{*},^{n},)\) comprising a prediction task \(^{*}\), a dataset \(^{n}\), and a conditional generative model (CGM) \(\). The prediction task is generalized as providing a response \(\) to a query \(\). The set of valid responses to a user query implies a distribution over responses \(p(,^{*})\). The dataset \(^{n}=\{(_{i},_{i})\}_{i=1} ^{n}\) comprises \(n\) query and response examples of the prediction task; \(_{i},_{i} p(,^{*})\). A practical abstraction decomposes queries and responses into elements called tokens. As such, queries and responses--by extension, examples and datasets--are represented as sequences of tokens. For example, \((,)(_{1}^{},_{2}^{ },,_{1}^{},_{2}^{}, )(_{1}^{},_{2}^{},)\). A CGM \(\) defines a predictive distribution over the next token in an example \(_{j}^{}\) given previous example tokens and \(_{<j}^{}\), and a tokenized dataset; \(p_{}(_{j}^{}_{<j}^{ },^{n})\). By ancestral sampling, the CGM effectively defines additional predictive distributions over responses \(p_{}(,^{n})\), examples \(p_{}(,^{n})\), and datasets \(p_{}(^{n})\). Figure 1 illustrates examples from two different ICL tasks. Figures 0(a) to 0(c) gives an example from the SST2 sentiment prediction task for which Llama-2 7B frequently yields accurate answers. Figures 0(d) to 0(f) gives an example from the medical questions pairs (MQP) prediction task for which Llama-2 7B yields random answers on average. Next, we give several reasons why model generated responses or examples may be inappropriate for the ICL task \(^{*}\).

## 3 What is a model?

Again let \(\) denote a model, but now the model could be Bayesian linear regression, a Gaussian process, or perhaps a large language model (LLM). A model defines a joint distribution \(p_{}(,)\) over observable data \(=\{_{1},_{2},\}=\{( _{1},_{1}),(_{2},_{2}),\}\) and latent explanations \(\). The notation \(\) denotes both tasks and explanations, but we will clearly distinguish between them. The model joint distribution factorizes as \(p_{}(,)=p_{}()p_{}()\), where \(p_{}()\) is the prior over explanations and \(p_{}()\) is the likelihood of the dataset given an explanation. From a frequentist perspective, the prior distribution over \(\) would be ignored and a model would define a set of distributions over datasets indexed by \(\); \(\{p_{}():\}\). A model \(\) alongside data \(^{n}\) further defines the posterior \(p_{}(^{n})\) and posterior predictive \(p_{}(^{n})= p_{}()\ d\)? We justify this with two key assumptions: First, if the model \(p_{}\) is exchangeable (i.e., the distribution \(p_{}()\) is invariant to permutations of the data), de Finetti's theorem  guarantees the existence of such a latent variable \(\). Therefore, assuming we adopt a unique representation of \(\), there is no issue in writing \(p_{}(,)\). Alternatively, if \(p_{}()\) approximates an exchangeable distribution \(p()\), as is the case with ICL problems, then we can treat the statement \(p_{}(,)\) as a convenient abuse of notation, meant to represent \(p(,)\). Throughout, we assume that either of these conditions hold.

## 4 A model is a choice to be criticised

A model \(\) over an observation space \(\) is used to make inferences based on observations \(^{n}\). These inferences can include probabilities of the next word in a sequence, model uncertainty, and other quantities of interest. However, since the model is a choice made by the practitioner, there is no guarantee that these inferences reflect reality or adequately model the data. For example, a randomly initialized LLM can be used to make inferences about next word probabilities, but those inferences are meaningless for modeling natural language. As models become more complex and widely used, it is crucial to understand when they can be trusted.

Much of the discussion around the reliability of CGMs has focused on "hallucination" detection, prediction, and mitigation [12; 13; 14; 15; 16; 17; 18; 19; 20; 21; 22; 23; 24; 25; 26; 27; 28; 29; 30; 31; 32; 33; 34; 35]. An interesting subset of these methods are based on uncertainty quantification where inferences about the variability of responses from the posterior predictive distribution [36; 37; 38; 39], or about the variability of explanations [33; 35; 40; 41; 42] are used to predict when a model may hallucinate. However, these methods do not address the fundamental question of when to trust those inferences so they are susceptible to failure if the model is not appropriate for a task.

A growing body of work is formalizing the connection between ICL with pre-trained CGMs and Bayesian inference [35; 43; 44; 45; 46; 47; 48]. Notably, the works of Jesson et al. , Fong et al. , Lee et al. , Falck et al. , Ye et al.  show how to transform Bayesian functionals of the model likelihood \(p_{}()\) and model posterior \(p_{}(^{n})\) into functionals of the model predictive distribution \(p_{}(^{n})\), which can be computed by contemporary CGMs. These works pave the way for using Bayesian model criticism techniques such as posterior predictive checks as a response to our research question. In the following we formalize how this is done.

## 5 Posterior predictive checks are model critics for ICL problems.

Posterior predictive checks (PPCs) [5; 6; 7; 49] are Bayesian model criticism methods that use the posterior predictive to evaluate a model's capability to make inferences from observations. Model capability is measured by the posterior predictive \(p\)-value, based on the hypothesis that the data are generated according to the model \(\). Following Moran et al. , we assume access to main \(^{n}\) and holdout \(^{}\) sets, both distributed according to the reference likelihood \(p(^{*})\). A class of PPCs assess how well a model \(\) fit to \(^{n}\) explains the holdout data \(^{}\). To measure fit, a PPC defines a discrepancy function, such as the negative log marginal model likelihood \(g_{}(,^{n}):=-_{z_{i},y_{i}}  p_{}(_{i},y_{i}^{n})\), or the negative log model likelihood \(g_{}(,):=-_{x_{i},y_{i}} p _{}(_{i},y_{i})\). Higher values indicate poor model fit, while lower values suggest the model explains the data well.

Defining a goodness-of-fit measure is only half of the story. A PPC needs to define what are relatively high or relatively low values of the discrepancy function. To do this, a reference distribution of values is defined by measuring the discrepancy function over datasets sampled from the model posterior predictive distribution. The posterior predictive \(p\)-value is then evaluated as

\[p_{}:=g_{}(,) g _{}(^{},)}d_{}()d_{}( ^{n}). \]

The PPC compares the discrepancy function value for holdout data \(g_{}(^{},)\) with its distribution under the model \(g_{}(,)\). If the model-generated data often has a greater or equal discrepancy than the holdout data, we can be confident the model explains the holdout data well. Conversely, if the holdout data's discrepancy is frequently higher, we should be less confident in the model's ability to explain it, raising doubts about the model's ability to solve the ICL problem. Algorithm 2 in Appendix C describes a \(p_{}\) estimator.

The generative predictive p-value and how to estimate it.

Modern CGMs--such as LLMS--do not provide an explicit representaion of the joint distribution over observations and explanations. For discrepancy functions that depend on \(\) this is a problem.

Our solution to the inaccessibility of component distributions relies on the intuition that a large enough dataset \(^{N}\{_{i},_{i}\}_{i=1}^{N}\) generated by the likelihood \(^{N} p_{}()\) contains roughly the same information as the explanation \(\) itself for an identifiable Bayesian model. Thus, it makes sense to express functions of \(\), like the model likelihood \(p_{}()\), that are not defined by a CGM, as functions of large datasets, such as the predictive distribution \(p_{}(^{N})\), which are defined.

Now, where do the additional \(N-n\) examples come from if we are only given the \(n\) examples comprising \(^{n}\)? The generation of sufficiently large datasets is done by generating hypothetical completions \(^{n+1:N}\) of the observed ICL dataset \(^{n}\) by ancestrally sampling from the model predictive distribution \(p_{}(,^{n})\) (also called predictive resampling by Fong et al. ):

\[_{n+1},_{n+1} p_{}(, ^{n}),_{n+2},_{n+2} p_{} (,^{n},_{n+1},_{n+1}), \]

As the generated examples are added to the conditional of the predictive distribution after each step, this process can be thought of as reasoning toward one explanation by imagining a sequence of sets of observations that are consistent with a smaller and smaller set of explanations as the sequence length increases. As a stochastic process, it is encouraged to reason toward a different explanation each time it is run to complete \(^{n}\) with \(N-n\) imagined examples.

Building off this intuition, we define martingale and generative predictive \(p\)-values below. We prove that under general conditions the martingale predictive \(p\)-value is equal to the posterior predictive \(p\)-value. We then show how to estimate the generative predictive \(p\)-value for a given ICL problem.

### The martingale predictive p-value

Our method is built on Doob's theorem for estimators (Theorem 2), which helps us transform statements about the random variable \(h()\)--a function of explanations \(\)--to statements about the random variable \([h()_{1},_{2},,_{ n}]\), which is a function of observations \((_{1},_{2},,_{n})\). Thus, we can proceed without direct access to \(p_{}(,)\)and \(p_{}(_{n})\) and define a \(p\)-value that depends on infinite datasets \(^{}(_{i},y_{i})_{i=1}^{}\) rather than \(\)

\[p_{}\{g_{}(, ^{}) g_{}(^{},^ {})\}d_{}(^{}) d_{}(^{n:}^{n}). \]

Doob's Theorem is an application of martingales, so--in line with the current literature --we call this formulation the martingale predictive \(p\)-value.

The main theoretical result of this paper establishes the equality of the posterior and martingale predictive \(p\)-values; Equations (1) and (2). We formalize this statement in the following theorem.

**Theorem 1**.: _Let \(_{}\), and \(_{1},_{2},\) i.i.d \(_{}^{}\). Assume Conditions 1 to 3 and let,_

\[| p_{}(^{m})|d_{}()<:^{m}^{m}.\]

_Then, \(p_{}=p_{}\)._

Proof.: The proof makes use of Doob's Theorem and is presented in Appendix B. 

### The generative predictive p-value

The martingale predictive \(p\)-value cannot be exactly computed because it is impossible to generate infinite datasets. Thus, we define the generative predictive \(p\)-value that clips the limits to infinity by some feasibly large number \(N\) to estimate Equation (2) as

\[p_{}\{g_{}(, ^{N}) g_{}(^{},^{N}) \}d_{}(^{N})d_{ }(^{n:N}^{n}). \]

Note that the generative predictive \(p\)-value enables us to replace any distributions that depend on latent mechanisms \(\) or infinite datasets \(^{}\) with ones that depend on finite sequences. The price we pay for using finite \(N\) is estimation error between \(p_{}\) and \(p_{}\). We leave the formal analysis of this error to future work.

### CGM estimators for the generative predictive p-value

```
0: data \(\{^{n},^{}\}\), discrepancy function \(g_{}(,^{N})\), # replicates \(\), # approx. samples \(\)
1:for\(i 1\) to \(\)do
2:\(^{N}_{i}^{n}\)\(\) initialize f sample data
3:for\(j n+1\) to \(N\)do
4:\(_{j},_{j} p_{}(,|^{N})\)\(\) sample example from model
5:\(^{N}_{i}(^{N}_{i},_{j},_{j})\)\(\) update approximation context
6:\(_{i}()\)\(\) initialize replicant data
7:for\(j 1\) to \(n\)do
8:\(_{j},_{j} p_{}(,|^{N}_{i})\)\(\) sample example from model
9:\(_{i}(_{i},_{j},_{j})\)\(\) update replicant data
10:return\(}_{i=1}^{}\{g_{}( _{i},^{N}_{i}) g_{}(^{},^{N}_{i})\}\)\(\) estimate \(p\)-value
```

**Algorithm 1**\(_{}\)

We derive an estimator for the generative predictive \(p\)-value in Equation (3) that uses Monte Carlo estimates to approximate the integrals. Algorithm 1 describes the estimation procedure. The key stage that differentiates the generative predictive \(p\)-value algorithm from the standard posterior predictive \(p\)-value algorithm is described in Lines 2 to 5. Here datasets \(^{N}_{i}\) of length \(N\) are ancestrally sampled from the CGM predictive distribution to approximate sampling a mechanism \(_{i}\). This is in contrast to sampling an explanation directly from the model posterior as shown in Algorithm 2 Line 2 of Appendix C. When sampling replication data \(_{i}\) in Lines 6 to 9, the CGM predictive distribution is conditioned on \(^{N}_{i}\) and \(n\) new samples are independently generated. This procedure is repeated \(M\) times, and the \(p\)-value is empirically estimated as before.

## 7 Empirical evaluation

This section reports the following empirical findings: (1) The generative predictive \(p\)-value is an accurate predictor of model capability in tabular, natural language, and imaging ICL problems. (2) The \(p\)-value computed under the NLL discrepancy is also an indicator of whether there are enough in-context examples \(n\). (3) The number of generated examples \(N-n\) interpolates the \(p\)-value between the posterior predictive \(p\)-value under the NLML discrepancy and the NLL discrepancy using the model posterior \(p_{}(^{n})\) and likelihood \(p_{}()\). These findings show that the \(p\)-value computed under either discrepancy yields an accurate predictor of whether generative AI can solve your in-context learning problem. If you also need to know whether there are enough in-context examples, we suggest using the NLL discrepancy function. If computational efficiency is a primary concern, we suggest using the NLML discrepancy as dataset completion generation is not required.

**Models.** We evaluate our methods using two model types. For tabular and imaging tasks, we use a Llama-2 regression model for sequences of continuous variables . The model is optimized from scratch for next token (variable or pixel) prediction following the procedure of Touvron et al. . For natural language tasks, we use pre-trained Llama-2 7B  and Gemma-2 9B  LLMs (Gemma-2 9B results are reported in Appendix F).

**Data.** For tabular tasks, queries z are sampled uniformly from the interval \([-2,2]\). Responses y are drawn from a normal distribution with a mean \(()\), parameterized by eithe

Figure 2: Tabular data tasks.

polynomial (in-distribution), a random ReLU neural network (in-distribution or OOD), or a radial basis function (RBF) kernel Gaussian process with a length scale of 0.3 (OOD). The training data comprise 8000 unique in-distribution datasets with 2000 \(-\) examples each. An in-distribution ReLU-NN task is illustrated in Figure 1(a). The mean function \(()\) is plotted by the blue line, and the blue shaded region outlines the 95% CI of \(p_{}(,^{*})\). An OOD GP task is illustrated in Figure 1(b). In-distribution test data comprise a set of 200 new random datasets with 500 \(-\) examples each. The OOD test data comprise 200 random datasets with 500 \(-\) examples each.

For pre-trained LLM experiments, the delineation between in- and out-of-distribution is opaque. Instead, we use in-capability or out-of-capability to differentiate between tasks a model can or cannot perform well. Figure 2(a) illustrates in-capability tasks where the error rate of Llama-2 7B is considerably better than random guessing. The in-capability data are the SST2  sentiment analysis (positive vs. negative) and AG News Zhang et al.  topic classification (World, Sports, Business, Sci/Tech) datasets. Figure 2(b) illustrates out-of-capability tasks where the error rate is only marginally better than random. The out-of-capability data are the Medical Questions Pairs (MQP)  differentiation (similar vs. different) and RTE  natural language inference (entailment vs. not entailment) datasets.

For imaging ICL experiments, we use SVHN for in-distribution data , MNIST as "near" OOD data , and CIFAR-10 as "far" OOD data . Our Llama-2 regression model takes a sequence of flattened, grayscale, 8x8 images as input. It is fit to random sequences of 16 images from the SVHN "extra" split, which has over 500k examples. A series of in-distribution generative fill tasks is shown in Figure 3(a). In each row, the model is prompted with three in-context examples and asked to complete the missing half of the 4th example. Each completion in the "fill" column is sensible, even when the completed number differs from the "real" number. Figure 3(b) illustrates completions for near-OOD MNIST tasks. We see in rows 1, 2, 3, and 6 that the fills are often sensible, but the model is prone to hallucinating odd completions (row 4) and artifacts (row 5). Figure 3(c) illustrates completions for far-OOD CIFAR-10 tasks. The completions are surprisingly consistent at this resolution, but as the result in row 4 demonstrates, the model hallucinates completions from its domain.

Figure 4: Generative fill tasks using the test sets of SVHN, MNIST, and CIFAR-10.

Figure 3: Natural language in-capability vs. out-of-capability tasks. Green solid line is the ICL error rate for Llama-2 7B. Gray dashed line is the random guessing error rate.

**Discrepancy functions.** We evaluate the \(p\)-value using discrepancy functions defined as

\[g_{}(,^{()}):=-|}_{x_{ i},y_{i}},y_{i})|}_{t_{j}(z_{i},y_{i})} p_{ }(_{j}_{<j},^{()}),\]

where \(|(z_{i},y_{i})|\) is the number of tokens in the evaluated example. Following this template, the per-token negative log marginal likelihood (NLML) is written \(g_{}(,^{n})\) and an estimate of the per-token the negative log-likelihood (NLL) is written \(g_{}(,^{N})\), where \(^{N}\) is generated as in Algorithm 1.

**Predicting model capability.** The \(p\)-values are calculated using either Algorithm 1 or Algorithm 3 and a significance level \(\) is selected to yield a binary predictor of model capability \(\{p_{}<\}\); a model is predicted as incapable of solving the ICL problem if the estimated generative predictive p-value is less than the significance level. We report results for significance levels \([0.01,0.05,0.1,0.2,0.5]\). For the NLL discrepancy function, replication data x is independently sampled from the likelihood under a hypothetical dataset completion \(p_{}(,^{N})\). For the NLML discrepancy function, replication data is independently sampled from the predictive distribution \(p_{}(,^{n})\).

**Evaluation metrics.** We evaluate the capability predictor using standard metrics: FPR measures in-capability tasks misclassified as out-of-capability, Precision reflects correctly identified out-of-capability tasks, and Recall measures correctly detected out-of-capability tasks. F1 Score and Accuracy assess overall performance (see Figure 5 for definitions).

We also provide the distribution of \(p\)-values across tasks to assess how confidently the model distinguishes between the different ICL problems. Lower \(p\)-values indicate stronger confidence that a model cannot solve a problem.

### The generative predictive \(p\)-value accurately predicts model capability

**Tabular data.** We first evaluate whether the generative predictive \(p\)-value effectively predicts OOD tabular data tasks. The parameters for Algorithm 1 are \(M=40\) replications and \(N-n=200\) generated examples. The ICL dataset \(^{n}\) size is varied from \(n=2\) to \(n=200\). Figure 6 plots precision, recall, F1, and accuracy curves and shows that the \(p\)-value estimates under either discrepancy function provide non-trivial OOD predictors for all \(\) settings.

**Natural language ICL.** Next, we evaluate whether the generative predictive \(p\)-value effectively predicts out-of-capacity natural language tasks. The parameters for Algorithm 1 are \(M=20\) replications and \(N-n=10\) generated examples. The ICL dataset \(^{n}\) size is varied from \(n=4\) to \(n=64\). Figure 7 plots precision, recall, F1, and accuracy curves and shows that the \(p\)-value estimates under the NLL discrepancy provide non-trivial (accuracy \(>0.5\)) out-of-capability predictors in the domain of natural language for all \(\) settings. The NLML discrepancy \(g_{}(,^{n})\) is also generally robust outside of the small \(n\) and small \(\) setting.

Figure 5: Evaluation metrics for GPC performance.

Figure 6: Tabular OOD detection. Metric values vs. context length. In-distribution functions are from unseen random ReLU-NNs. OOD functions are from an RBF kernel GP.

**Generative fill.** Finally, we evaluate whether the generative predictive \(p\)-value effectively predicts OOD generative fill tasks. The parameters for Algorithm 1 are \(M=100\) replications and \(N-n=8\) generated examples The ICL dataset \(^{n}\) size is varied from \(n=2\) to \(n=8\). Figure 8 plots the OOD prediction metric curves and shows that the \(p\)-value estimates under either discrepancy function provide non-trivial (accuracy \(>0.666\)) OOD predictors for all \(\) settings.

**Discussion.** Figures 6 to 8 reveal several trends. First, the NLML discrepancy (blue) yields better precision, indicating that it is less likely to misclassify an in-capability ICL problem as unsolvable. Second, the NLL discrepancy (purple) yields higher recall, indicating that it is less likely to misclassify an out-of-capability ICL problem as solvable. Third, the NLL discrepancy with significance level \(=0.05\) yields a generally more accurate predictor than the NLML discrepancy function for any significance level in the set evaluated. Finally, the recall of a predictor under the NLML discrepancy is sensitive to the number of in-context examples \(n\). Next, we look deeper into the relationship between dataset size and the discrepancy functions.

### The NLL discrepancy also indicates whether you have enough data

Both discrepancy functions yield accurate predictors of model capability, but the NLL discrepancy also provides information about whether there are enough in-context examples to reliably solve a task. We use prediction RMSE over task responses to measure reliability. Figures 8(a) and 8(b) plot the

Figure 8: Generative fill OOD detection. Metric values vs. context length. In-distribution tasks are from the SVHN test set. Near and far OOD tasks are from the MNIST and CIFAR-10 test sets.

Figure 7: Llama-2-7B out-of-capability detection. Metric values vs. context length. In-capability tasks are from SST2 and AG News datasets. Out-of-capability tasks are from RTE and MQP datasets.

Figure 9: (a) and (b) Scatter plots of response RMSE vs. \(p\)-values for the NLL and NLML discrepancies. Points are style-coded by ICL dataset size \(n\). (c) Risk vs. \(n\). (d) Accuracy vs. \(n\)RMSE against the \(p\)-values computed under the NLL and NLML discrepancies for in-distribution polynomial tabular tasks. We see that lower \(p\)-values correlate with higher RMSE for the NLL discrepancy, but not for the NLML discrepancy. This added information is useful for reducing risk in recommendation systems that autonomously respond if the \(p\)-value is greater than the significance level \(\). For example, at \(=0.1\), the NLL discrepancy reduces the generation of responses with higher error because it accounts for the number of examples provided. Taking the risk as the sum of task RMSEs for tasks predicted as in-capability, Figures 9c and 9d show that the NLL discrepancy results in substantially reduced risk, even when we closely match the accuracies of each predictor. Figure 13 in the appendix gives further insight into how the distributions of \(p\)-values evolve with dataset size for each discrepancy function.

The number of generated examples \(N-n\) interpolates the \(p-value\) estimate between the NLML and the ideal NLL discrepancies

Inspection of Equations (1) to (3) makes clear that the dataset completion size \(N-n\) should closely interpolate \(p\)-value estimates between \(p_{}\) computed with the NLML discrepancy and with the NLL discrepancy using the likelihood and posterior of a Bayesian model. To verify this, we use a reference Bayesian polynomial regression model to compute the \(p_{}\). We use our Llama-2 regression model fit to datasets generated from the reference model likelihood under different explanations to compute the \(p_{}\). We let datasets generated by random ReLU-NNs serve as OOD tasks. Figure 10 demonstrates that our expectation is true. Specifically, the \(p\)-value estimates at \(N-n=2\) are distributionally close to those calculated under the NLML, and they more closely approximate those calculated under the reference NLL discrepancy as we increase \(N-n\) to \(100\). The latter observation is also illustrated in Figure 12.

Since the \(p\)-values computed under either discrepancy yield accurate predictors of model capability, the choice between discrepancy functions ultimately comes down to a decision on whether the added computational cost of generating dataset completions is justified. If you need to know whether there are enough in-context examples to generate an accurate response--a necessity in risk-sensitive applications--then we recommend using the NLL discrepancy function. If computational efficiency or the cost of response deferral are primary concerns--practical user experience concerns--we suggest using the NLML discrepancy.

## 8 Conclusion

This work introduces the _generative predictive \(p\)-value_, a metric for determining whether a Conditional Generative Model can solve an In-Context Learning problem. It extends Bayesian model criticism techniques like PPCs to generative models by sampling dataset completions from the model's predictive distribution to approximate sampling latent explanations from a Bayesian model posterior. Empirical evaluations on tabular, natural language, and imaging tasks show that the generative predictive \(p\)-value can effectively identify the limits of model capability, distinguishing between in-capability and out-of-capability tasks for models like Llama-2 7B and Gemma-2 9B. This approach is a practical method to assess model suitability that advances Bayesian model criticism for CGMs. While we have focused on model capability prediction, the \(p\)-value estimates could also be used for model selection or as a general measure of task uncertainty. We are eager to explore extensions beyond ICL tasks to improve the reliability of generative AI systems.