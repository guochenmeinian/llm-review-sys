# Model-free Posterior Sampling via Learning Rate Randomization

Daniil Tiapkin\({}^{1,2}\) Denis Belomestny\({}^{3,2}\) Daniele Calandriello\({}^{4}\) Eric Moulines\({}^{1,5}\)

**Remi Munos\({}^{4}\) Alexey Naumov\({}^{2}\) Pierre Perrault\({}^{6}\) Michal Valko\({}^{4}\) Pierre Menard\({}^{7}\)**

\({}^{1}\)CMAP, Ecole Polytechnique \({}^{2}\)HSE University \({}^{3}\)Duisburg-Essen University

\({}^{4}\)Google DeepMind \({}^{5}\)Mohamed Bin Zayed University of AI, UAE \({}^{6}\)IDEMIA \({}^{7}\)ENS Lyon

{daniil.tiapkin,eric.moulines}@polytechnique.edu

denis.belomestny@uni-due.de

{dcalandriello,munos,valkom}@google.com anaumov@hse.ru

pierre.perrault@outlook.com pierre.menard@ens-lyon.fr

###### Abstract

In this paper, we introduce Randomized Q-learning (RandQL), a novel randomized model-free algorithm for regret minimization in episodic Markov Decision Processes (MDPs). To the best of our knowledge, RandQL is the first tractable model-free posterior sampling-based algorithm. We analyze the performance of RandQL in both tabular and non-tabular metric space settings. In tabular MDPs, RandQL achieves a regret bound of order \(}(SAT})\), where \(H\) is the planning horizon, \(S\) is the number of states, \(A\) is the number of actions, and \(T\) is the number of episodes. For a metric state-action space, RandQL enjoys a regret bound of order \(}(H^{5/2}T^{(d_{z}+1)/(d_{z}+2)})\), where \(d_{z}\) denotes the zooming dimension. Notably, RandQL achieves optimistic exploration _without using bonuses_, relying instead on a novel idea of _learning rate randomization_. Our empirical study shows that RandQL outperforms existing approaches on baseline exploration environments.

## 1 Introduction

In reinforcement learning (RL, Sutton and Barto 1998), an agent learns to interact with an unknown environment by acting, observing the next state, and receiving a reward. The agent's goal is to maximize the sum of the collected rewards. To achieve this, the agent can choose to use model-based or model-free algorithms. In model-based algorithms, the agent builds a model of the environment by inferring the reward function and the transition kernel that produces the next state. The agent then plans in this model to find the optimal policy. In contrast, model-free algorithms directly learn the optimal policy, which is the mapping of a state to an optimal action, or equivalently, the optimal Q-values, which are the mapping of a state-action pair to the expected return of an optimal policy starting by taking the given action at the given state.

Although empirical evidence suggests that model-based algorithms are more sample efficient than model-free algorithms (Deisenroth and Rasmussen, 2011; Schulman et al., 2015); model-free approaches offer several advantages. These include smaller time and space complexity, the absence of a need to learn an explicit model, and often simpler algorithms. As a result, most of the recent breakthroughs in deep RL, such as those reported by Mnih et al. (2013); Schulman et al. (2015, 2017); Haarnoja et al. (2018), have been based on model-free algorithms, with a few notable exceptions, such as Schrittwieser et al. (2020); Hessel et al. (2021). Many of these model-free algorithms (Mnih et al., 2013; Van Hasselt et al., 2016; Lillicrap et al., 2016) are rooted in the well-known Q-learning algorithm of Watkins and Dayan (1992). Q-learning is an off-policy learning technique where the agent follows a behavioral policy while simultaneously incrementally learning the optimal Q-valuesby combining asynchronous dynamic programming and stochastic approximation. Until recently, little was known about the sample complexity of Q-learning in the setting where the agent has no access to a simulator allowing to sample an arbitrary state-action pair. In this work, we consider such challenging setting where the environment is modelled by an episodic Markov Decision Process (MDP) of horizon \(H\). After \(T\) episodes, the performance of an agent is measured through regret which is the difference between the cumulative reward the agent could have obtained by acting optimally and what the agent really obtained during the interaction with the MDP.

This framework poses the famous exploration-exploitation dilemma where the agent must balance the need to try new state-action pairs to learn an optimal policy against exploiting the current observations to collect the rewards. One effective approach to resolving this dilemma is to adopt the principle of optimism in the face of uncertainty. In finite MDPs, this principle has been successfully implemented in the model-based algorithm using bonuses (Jaksch et al., 2010; Azar et al., 2017; Fruit et al., 2018; Dann et al., 2017; Zanette and Brunskill, 2019). Specifically, the upper confidence bounds (UCBs) on the optimal Q-value are built by adding bonuses and then used for planning. Building on this approach, Jin et al. (2018) proposed the OptQL algorithm, which applies a similar bonus-based technique to Q-learning, achieving efficient exploration. Recently, Zhang et al. (2020) introduced a simple modification to OptQL that achieves optimal sample complexity, making it competitive with model-based algorithms.

Another class of methods for optimistic exploration is Bayesian-based approaches. An iconic example among this class is the posterior sampling for reinforcement learning (PSRL,Strens 2000, Osband et al. 2013) algorithm. This model-based algorithm maintains a _surrogate Bayesian model_ of the MDP, for instance, a Dirichlet posterior on the transition probability distribution if the rewards are known. At each episode, a new MDP is sampled (i.e., a transition probability for each state-action pair) according to the posterior distribution of the Bayesian model. Then, the agent plans in this sampled MDP and uses the resulting policy to interact with the environment. Notably, an optimistic variant of PSRL, named optimistic posterior sampling for reinforcement learning (OPSRL, Agrawal and Jia, 2017; Tiapkin et al., 2022a) also enjoys an optimal sample complexity (Tiapkin et al., 2022a). The random least square value iteration (RLSVI, Osband et al. (2013)) is another well-known model-based algorithm that leverages a Bayesian-based technique for exploration. Precisely, RLSVI directly sets a Gaussian prior on the optimal Q-values and then updates the associated posterior trough value iteration in a model (Osband et al., 2013; Russo, 2019). A close variant of RLSVI proposed by Xiong et al. (2022), using a more sophisticated prior/posterior couple, is also proven to be near-optimal.

It is noteworthy that Bayesian-based exploration techniques have shown superior empirical performance compared to bonus-based exploration, at least in the tabular setting (Osband et al., 2013; Osband and Van Roy, 2017). Furthermore, these techniques have also been successfully applied to the deep RL setting (Osband et al., 2016; Azizzadenesheli et al., 2018; Fortunato et al., 2018; Li et al., 2022; Sasso et al., 2023). Finally, Bayesian methods allow for the incorporation of apriori information into exploration (e.g. by giving more weight to important states). However, most of the theoretical studies on Bayesian-based exploration have focused on model-based algorithms, raising the natural question of whether the PSRL approach can be extended to a provably efficient model-free algorithm that matches the good empirical performance of its model-based counterparts. Recently, Dann et al. (2021) proposed a model-free posterior sampling algorithm for structured MDPs, however, it is not computationally tractable. Therefore, a provably tractable model-free posterior sampling algorithm has remained a challenge.

In this paper, we aim to resolve this challenge. We propose the randomized Q-learning (RandQL) algorithm that achieves exploration without bonuses, relying instead on a novel idea of learning rate randomization. RandQL is a tractable model-free algorithm that updates an ensemble of Q-values via Q-learning with Beta distributed step-sizes. If tuned appropriately, the noise introduced by the random learning rates is similar to the one obtained by sampling from the posterior of the PSRL algorithm. Thus, one can see the ensemble of Q-values as posterior samples from the same induced posterior on the optimal Q-values as in PSRL. Then, RandQL chooses among these samples in the same optimistic fashion as OPSRL. We prove that for tabular MDPs, a staged version (Zhang et al., 2020) of RandQL, named Staged-RandQL enjoys the same regret bound as the OptQL algorithm, that is, \(}(SAT})\) where \(S\) is the number of states and \(A\) the number of actions. Furthermore, we extend Staged-RandQL beyond the tabular setting into the Net-Staged-RandQL algorithm to deal with metric state-action spaces (Domingues et al., 2021; Sinclair et al., 2019). Net-Staged-RandQL operates similarly to Staged-RandQL but over a fixed discretization of the state-action space and uses a specific prior tuning to handle the effect of discretization. We prove that Net-Staged-RandQL enjoys a regret bound of order \(}(H^{5/2}T^{(d_{c}+1)/(d_{c}+2)})\), where \(d_{c}\) denotes the covering dimension. This rate is of the same order as the one of Adaptive-QL by Sinclair et al. (2019, 2023), an adaptation of OptQL to metric state-action space and has a better dependence on the budget \(T\) than one of the model-based kernel algorithms such that Kernel-UCBVI by Domingues et al. (2021c). We also explain how to adapt Net-Staged-RandQL and its analysis to work with an adaptive discretization as by Sinclair et al. (2019, 2023). Finally, we provide preliminary experiments to illustrate the good performance of RandQL against several baselines in finite and continuous environments.

We highlight our main contributions:

* The RandQL algorithm, a new tractable (provably efficient) model-free Q-learning adaptation of the PSRL algorithm that explores through randomization of the learning rates.
* A regret bound of order \(}(SAT})\) for a staged version of the RandQL algorithm in finite MDPs where \(S\) is the number of states and \(A\) the number of actions, \(H\) the horizon and \(T\) the budget.
* A regret bound of order \(}(H^{5/2}T^{(d_{c}+1)/(d_{c}+2)})\) for an adaptation of RandQL to metric spaces where \(d_{c}\) denotes the covering dimension.
* Adaptive version of metric space extension of RandQL algorithm that achieves a regret bound of order \(}(H^{5/2}T^{(d_{z}+1)/(d_{z}+2)})\), where \(d_{z}\) is a _zooming_ dimension.
* Experiments in finite and continuous MDPs that show that RandQL is competitive with model-based and model-free baselines while keeping a low time-complexity.

## 2 Setting

We consider an episodic MDP \(,,H,\{p_{h}\}_{h[H]},\{r_{h}\}_{h[H]}\), where \(\) is the set of states, \(\) is the set of actions, \(H\) is the number of steps in one episode, \(p_{h}(s^{}|s,a)\) is the probability transition from state \(s\) to state \(s^{}\) upon taking action \(a\) at step \(h\), and \(r_{h}(s,a)\) is the bounded deterministic reward received after taking the action \(a\) in state \(s\) at step \(h\). Note that we consider the general case of rewards and transition functions that are possibly non-stationary, i.e., that are allowed to depend on the decision step \(h\) in the episode.

Policy & value functionsA _deterministic_ policy \(\) is a collection of functions \(_{h}:\) for all \(h[H]\), where every \(_{h}\) maps each state to a _single_ action. The value functions of \(\), denoted by \(V^{}_{h}\), as well as the optimal value functions, denoted by \(V^{}_{h}\) are given by the Bellman and the optimal Bellman equations,

\[Q^{}_{h}(s,a) =r_{h}(s,a)+p_{h}V^{}_{h+1}(s,a) V^{}_{h}(s) =_{h}Q^{}_{h}(s)\] \[Q^{}_{h}(s,a) =r_{h}(s,a)+p_{h}V^{}_{h+1}(s,a) V^{}_{h}(s) =_{a}Q^{}_{h}(s,a),\]

where by definition, \(V^{}_{H+1} V^{}_{H+1} 0\). Furthermore, \(p_{h}f>(s,a)_{s^{} p_{h}( s,a)}[f(s^{ })]\) denotes the expectation operator with respect to the transition probabilities \(p_{h}\) and \(_{h}g(s) g(s,_{h}(s))\) denotes the composition with the policy \(\) at step \(h\).

Learning problemThe agent, to which the transitions are _unknown_ (the rewards are assumed to be known1 for simplicity), interacts with the environment during \(T\) episodes of length \(H\), with a _fixed_ initial state \(s_{1}\).2 Before each episode \(t\) the agent selects a policy \(^{t}\) based only on the past observed transitions up to episode \(t-1\). At each step \(h[H]\) in episode \(t\), the agent observes a state \(s^{t}_{h}\), takes an action \(^{t}_{h}(s^{t}_{h})=a^{t}_{h}\) and makes a transition to a new state \(s^{t}_{h+1}\) according to the probability distribution \(p_{h}(s^{t}_{h},a^{t}_{h})\) and receives a deterministic reward \(r_{h}(s^{t}_{h},a^{t}_{h})\).

RegretThe quality of an agent is measured through its regret, that is the difference between what it could obtain (in expectation) by acting optimally and what it really gets,

\[^{T}_{t=1}^{T}V^{}_{1}(s_{1})-V^{^{t}}_{1}(s _{1})\,.\]Additional notationFor \(N_{++}\), we define the set \([N]\{1,,N\}\). We denote the uniform distribution over this set by \([N]\). We define the beta distribution with parameters \(,\) as \((,)\). Appendix A references all the notation used.

## 3 Randomized Q-learning for Tabular Environments

In this section we assume that the state space \(\) is finite of size \(S\) as well as the action space \(\) of size \(A\). We first provide some intuitions for RandQL algorithm.

### Concept

The main idea of RandQL is to perform the usual Q-learning updates but instead of adding bonuses to the targets as OptQL to drive exploration, RandQL injects noise into the updates of the Q-values through _noisy learning rates_. Precisely, for \(J\), we maintain an ensemble of size \(J\) of Q-values3\((^{n,j})_{j[J]}\) updated with random independent Beta-distributed step-sizes \((w_{n,j})_{j[J]}\) where \(w_{n,j}(H,n)\). Then, policy Q-values \(^{n}\) are obtained by taking the maximum among the Q-values of the ensemble

\[^{n+1,j}_{h}(s,a) =(1-w_{n,j})^{n,j}_{h}(s,a)+w_{n,j}[r_{h}(s,a)+ ^{n}_{h+1}(s^{n}_{h+1})]\] \[^{n+1}_{h}(s,a) =_{j[J]}^{n+1,j}_{h}(s,a),^{ n+1}_{h}(s)=_{a}^{n+1}_{h}(s,a),\]

where \(s^{n}_{h+1}\) stands for the next (in time) state after \(n\)-th visitation of \((s,a)\) at step \(h\).

Note that the policy Q-values \(^{n}\) are designed to be upper confidence bound on the optimal Q-values. The policy used to interact with the environment is greedy with respect to the policy Q-values \(^{n}_{h}(s)_{a}^{n}_{h}(s,a)\). We provide a formal description of RandQL in Appendix B.

Connection with OptQL We observe that the learning rates of RandQL are in expectation of the same order \([w_{n,j}]=H/(n+H)\) as the ones used by the OptQL algorithm. Thus, we can view our randomized Q-learning as a noisy version of the OptQL algorithm that doesn't use bonuses.

Connection with PSRL If we unfold the recursive formula above we can express the Q-values \(^{n+1,j}\) as a weighted sum

\[^{n+1,j}_{h}(s,a)=W^{0}_{n,j}^{1,j}_{h}(s,a)+_{k =1}^{n}W^{k}_{n,j}[r_{h}(s,a)+^{k}_{h+1}(s^{k}_{h+1})],\]

where we define \(W^{0}_{n,j}=_{=0}^{n-1}(1-w_{,j})\) and \(W^{k}_{n,j}=w_{k-1,j}_{=k}^{n-1}(1-w_{,j})\).

To compare, we can unfold the corresponding formula for PSRL algorithm using the aggregation properties of the Dirichlet distribution (see e.g. Section 4 of Tiapkin et al. (2022) or Appendix C)

\[^{n+1}_{h}(s,a)=^{0}_{n}^{1}_{h}(s,a)+ _{k=1}^{n}^{k}_{n}[r_{h}(s,a)+^{n+1}_{h+1}(s^{k }_{h+1})], \]

where weights \((^{0}_{n},,^{n}_{n})\) follows Dirichlet distribution \((n_{0},1,,1)\) and \(n_{0}\) is a weight for the prior distribution. In particular, one can represent these weights as partial products of other weights \(w_{n}(1,n+n_{0})\). If we use (1) to construct a model-free algorithm, this would require recomputing the targets \(r_{h}(s,a)+^{n+1}(s^{k}_{h+1})\) in each iteration. To make algorithm more efficient and model-free, we approximate \(^{n+1}\) by \(^{k}\), and, as a result, obtain RandQL algorithm with weight distribution \(w_{n,j}(1,n+n_{0})\).

Note that in expectation this algorithm is equivalent to OptQL with the uniform step-sizes which are known to be sub-optimal due to a high bias (see discussion in Section 3 of (Jin et al., 2018)). There are two known ways to overcome this sub-optimality for Q-learning: to introduce more aggressive learning rates \(w_{n,j}(H,n+n_{0})\) leading to RandQL algorithm, or to use stage-dependent framework by Bai et al. (2019); Zhang et al. (2020) resulting in Staged-RandQL algorithm.

The aforementioned transition from PSRL to RandQL is similar to the transition from UCBVI(Azar et al., 2017) to Q-learning. To make UCBVI model-free, one has to to keep old targets in Q-values. This, however, introduces a bias that could be eliminated either by more aggressive step-size (Jin et al., 2018) or by splitting on stages (Bai et al., 2019). Our algorithms (RandQL and Staged-RandQL) perform the similar tricks for PSRL and thus could be viewed as model-free versions of it. Additionally, RandQL shares some similarities with the OPSRL algorithm (Agrawal and Jia, 2017; Tiapkin et al., 2022) in the way of introducing optimism (taking maximum over \(J\) independent ensembles of Q-values). Let us also mention a close connection to the theory of Dirichlet processes in the proof of optimism for the case of metric spaces (see Remark 1 in Appendix E.4).

PriorAs remarked above, in expectation, RandQL has a learning rate of the same order as OptQL. In particular, it implies that the first \((1-1/H)\) fraction of the the target will be forgotten exponentially fast in the estimation of the Q-values, see Jin et al. (2018); Menard et al. (2021). Thus we need to re-inject prior targets, as explained in Appendix B, in order to not forget too quickly the prior and thus replicate the same exploration mechanism as in the PSRL algorithm.

### Algorithm

In this section, following Bai et al. (2019); Zhang et al. (2020), we present the Staged-RandQL algorithm a scheduled version of RandQL that is simpler to analyse. The main idea is that instead of using a carefully tuned learning rate to keep only the last \(1/H\) fraction of the targets we split the learning of the Q-values in stages of exponentially increasing size with growth rate of order \(1+1/H\). At a given stage, the estimate of the Q-value relies only on the targets within this stage and resets at the beginning of the next stage. Notice that the two procedures are almost equivalent. A detail description of Staged-RandQL is provided in Algorithm 1.

**Counts and stages** Let \(n^{t}_{h}(s,a)_{i=1}^{t-1}\{(s^{i}_{h},a^{i}_{h})=(s, a)\}\) be the number of visits of state-action pair \((s,a)\) at step \(h\) before episode \(t\). We say that a triple \((s,a,h)\) belongs to the \(k\)-th stage at the beginning of episode \(t\) if \(n^{t}_{h}(s,a)[_{i=0}^{k-1}e_{i},_{i=0}^{k}e_{i})\). Here \(e_{k}=(1+1/H)^{k} H\) is the length of the stage \(k 0\) and, by convention, \(e_{-1}=0\). Let \(^{t}_{h}(s,a) n^{t}_{h}(s,a)-_{i=0}^{k-1}e_{i}\) be the number of visits of state-action pair \((s,a)\) at step \(h\) during the current stage \(k\).

**Temporary Q-values** At the beginning of a stage, let say time \(t\), we initialize \(J\)_temporary_ Q-values as \(^{t,j}_{h}(s,a)=r_{h}(s,a)+r_{0}(H-h-1)\) for \(j[J]\) and \(r_{0}\) some pseudo-reward. Then as long as \((s^{t}_{h},a^{t}_{h},h)\) remains within a stage we update recursively the _temporary_ Q-values

\[^{t+1,j}_{h}(s,a)=(1-w_{j,}) {Q}^{t,j}_{h}(s,a)+w_{j,}[r_{h}(s,a)+^{t}_{h+1}(s^{ t}_{h+1})],&(s,a)=(s^{t}_{h},a^{t}_{h})\\ ^{t,j}_{h}(s,a)&,\]

where \(=^{t}_{h}(s,a)\) is the number of visits, \(w_{j,}\) is a sequence of i.i.d. random variables \(w_{j,}(1/,(+n_{0})/)\) with \(>0\) being some posterior inflation coefficient and \(n_{0}\) a number of pseudo-transitions.

**Policy Q-values** Next we define the policy Q-values that is updated at the end of a stage. Let say for state-action pair \((s,a)\) at step \(h\) an stage ends at time \(t\). This policy Q-values is then given by the maximum of temporary Q-values \(^{t+1}_{h}=_{j[J]}^{t+1,j}_{h}(s,a)\). Then the policy Q-values is constant within a stage. The value used to defined the targets is \(^{t+1}_{h}(s)=_{a}^{t+1}_{h}(s,a)\). The policy used to interact with the environment is greedy with respect to the policy Q-values \(^{t+1}_{h}(s)*{arg\,max}_{a}^{t+1 }_{h}(s,a)\) (we break ties arbitrarily).

### Regret bound

We fix \((0,1)\) and the number of posterior samples \(J c_{J}(2SAHT/)\), where \(c_{J}=1/(2/(1+(1)))\) and \(()\) is the cumulative distribution function (CDF) of a normal distribution. Note that \(J\) has a logarithmic dependence on \(S,A,H,T,\) and \(1/\).

We now state the regret bound of Staged-RandQL with a full proof in Appendix D.

```
1:Input: inflation coefficient \(\), \(J\) ensemble size, number of prior transitions \(n_{0}\), prior reward \(r_{0}\).
2:Initialize:\(_{h}(s)=_{h}(s,a)=_{h}^{j}(s,a)=r(s,a)+r_{0}(H-h -1)\), initialize counters \(_{h}(s,a)=0\) for \(j,h,s,a[J][H]\) and stage \(q_{h}(s,a)=0\).
3:for\(t[T]\)do
4:for\(h[H]\)do
5: Play \(a_{h}_{a}_{h}(s_{h},a)\).
6: Observe reward and next state \(s_{h+1} p_{h}(s_{h},a_{h})\).
7: Sample learning rates \(w_{j}(1/,(+n_{0})/)\) for \(=_{h}(s_{h},a_{h})\).
8: Update temporary \(Q\)-values for all \(j[J]\) \[_{h}^{j}(s_{h},a_{h}):=(1-w_{j})_{h}^{j}(s_{h},a_{h })+w_{j}r_{h}(s_{h},a_{h})+_{h+1}(s_{h+1})\,.\]
9: Update counter \(_{h}(s_{h},a_{h}):=_{h}(s_{h},a_{h})+1\)
10:if\(_{h}(s_{h},a_{h})=(1+1/H)^{q}H\) for \(q=q_{h}(s_{h},a_{h})\) being the current stage then
11: Update policy \(Q\)-values \(_{h}(s_{h},a_{h}):=_{j[J]}_{h}^{j}(s_{h},a_{h})\).
12: Update value function \(_{h}(s_{h}):=_{a}_{h}(s_{h},a)\)
13: Reset temporary \(Q\)-values \(_{h}^{j}(s_{h},a_{h}):=r_{h}(s_{h},a_{h})+r_{0}(H-h-1)\).
14: Reset counter \(_{h}(s_{h},a_{h}):=0\) and change stage \(q_{h}(s_{h},a_{h}):=q_{h}(s_{h},a_{h})+1\).
15:endif
16:endfor
17:endfor
```

**Algorithm 1** Tabular Staged-RandQL

**Theorem 1**.: _Consider a parameter \((0,1)\). Let \( 2((8SAH/)+3(e(2T+1)))\), \(n_{0}(c_{0}+_{17/16}(T))\), \(r_{0} 2\), where \(c_{0}\) is an absolute constant defined in (5); see Appendix D.3. Then for Staged-RandQL, with probability at least \(1-\),_

\[^{T}=}SAT}+H^{3}SA.\]

DiscussionThe regret bound of Theorem 1 coincides (up to a logarithmic factor) with the bound of the OptQL algorithm with Hoeffding-type bonuses from Jin et al. (2018). Up to a \(H\) factor, our regret matches the information-theoretic lower bound \((SAT})\)(Jin et al., 2018; Domingues et al., 2021). This bound could be achieved (up to logarithmic terms) in model-free algorithms by using Bernstein-type bonuses and variance reduction (Zhang et al., 2020). We keep these refinements for future research as the main focus of our paper is on the novel randomization technique and its use to construct computationally tractable model-free algorithms.

Computational complexityStaged-RandQL is a model-free algorithm, and thus gets the \(}(HSA)\) space complexity as OptQL, recall that we set \(J=}(1)\). The per-episode time-complexity is also similar and of order \(}(H)\).

## 4 Randomized Q-learning for Metric Spaces

In this section we present a way to extend RandQL to general state-action spaces. We start from the simplest approach with predefined \(\)-net type discretization of the state-action space \(\)(see Song and Sun, 2019), and then discuss an adaptive version of the algorithm, similar to one presented by Sinclair et al. (2019).

### Assumptions

To pose the first assumption, we start from a general definition of covering numbers.

**Definition 1** (Covering number and covering dimension).: Let \((M,)\) be a metric space. A set \(\) of open balls of radius \(\) is called an \(\)-cover of \(M\) if \(M_{B}B\). The cardinality of the minimal \(\)-cover is called covering number \(N_{}\) of \((M,)\). We denote the corresponding minimal \(\)-covering by \(_{}\). A metric space \((M,)\) has a covering dimension \(d_{c}\) if \(>0:N_{} C_{N}^{-d_{c}}\), where \(C_{N}\) is a constant.

The last definition extends the definition of dimension beyond vector spaces. For example, is case of \(M=^{d}\) the covering dimension of \(M\) is equal to \(d\). For more details and examples see e.g. Vershynin (2018, Section 4.2).

Next we are ready to introduce the first assumption.

**Assumption 1** (Metric Assumption).: Spaces \(\) and \(\) are separable compact metric spaces with the corresponding metrics \(_{}\) and \(_{}\). The joint space \(\) endowed with a product metric \(\) that satisfies \(((s,a),(s^{},a^{}))_{}(s,s^{})+_{ }(a,a^{})\). Moreover, the diameter of \(\) is bounded by \(d_{}\), and \(\) has covering dimension \(d_{c}\) with a constant \(C_{N}\).

This assumption is, for example, satisfied for the finite state and action spaces endowed with discrete metrics \(_{}(s,s^{})=\{s s^{}\},_{}(a,a^{})=\{a a^{}\}\) with \(d_{c}=0,\,C_{N}=SA\) and \(S\) and \(A\) being the cardinalities of the state and action spaces respectively. The above assumption also holds in the case \(^{d_{}}\) and \(^{d_{}}\) with \(d_{c}=d_{}+d_{}\).

The next two assumptions describe the regularity conditions of transition kernel and rewards.

**Assumption 2** (Reparametrization Assumption).: The Markov transition kernel could be represented as an iterated random function. In other words, there exists a measurable space \((,_{})\) and a measurable function \(F_{h}()\), such that \(s_{h+1} p_{h}(s_{h},a_{h}) s_{h+1}=F_{h}(s_{h},a_{h},_{h})\) for a sequence of independent random variables \(\{_{h}\}_{h[H]}\).

This assumption is naturally satisfied for a large family of probabilistic model, see Kingma and Welling (2014). Moreover, it has been utilized by the RL community both in theory (Ye and Zhou, 2015) and practice (Heess et al., 2015; Liu et al., 2018). Essentially, this assumption holds for Markov transition kernels over a separable metric space, see Theorem 1.3.6 by Douc et al. (2018). However, the function \(F_{h}\) could be ill-behaved. To avoid this behaviour, we need the following assumption.

**Assumption 3** (Lipschitz Assumption).: The function \(F_{h}(,_{h})\) is \(L_{F}\)-Lipschitz in the first argument for almost every value of \(_{h}\). Additionally, the reward function \(r_{h}\) is \(L_{r}\)-Lipschitz.

This assumption is commonly used in studies of the Markov processes corresponding to iterated random functions, see Diaconis and Freedman (1999); Ghosh and Marecek (2022). Moreover, this assumption holds for many cases of interest. As main example, it trivially holds in tabular and Lipschitz continuous deterministic MDPs (Ni et al., 2019). Notably, this observation demonstrates that Assumption 3 does not necessitate Lipschitz continuity of the transition kernels in total variation distance, since deterministic Lipschitz MDPs are not continuous in that sense. Additionally, incorporation of an additive noise to deterministic Lipschitz MDPs will lead to Assumption 3 with \(L_{F}=1\).

Furthermore, it is possible to show that Assumption 3 implies other assumptions stated in the literature. For example, it implies that the transition kernel is Lipschitz continuous in \(1\)-Wasserstein metric, and that \(Q^{}\) and \(V^{}\) are both Lipschitz continuous.

**Lemma 1**.: _Let Assumption 1,2,3 hold. Then the transition kernels \(p_{h}(s,a)\) are \(L_{F}\)-Lipschitz continuous in \(1\)-Wasserstein distance_

\[_{1}(p_{h}(s,a),p_{h}(s^{},a^{})) L_{F}((s,a),(s^{},a^{})),\]

_where \(1\)-Wasserstein distance between two probability measures on the metric space \((M,)\) is defined as \(_{1}(,)=_{f1-}_{M}f -_{M}f\)._

**Lemma 2**.: _Let Assumption 1,2,3 hold. Then \(Q^{}_{h}\) and \(V^{}_{h}\) are Lipschitz continuous with Lipschitz constant \(L_{V,h}_{h^{}=h}^{H}L_{F}^{h^{}-h}L_{r}\)._

The proof of these lemmas is postponed to Appendix E. For a more detailed exposition on 1-Wasserstein distance we refer to the book by Peyre and Cuturi (2019). The first assumption was studied by Domingues et al. (2021); Sinclair et al. (2023) in the setting of model-based algorithms in metric spaces. We are not aware of any natural examples of MDPs with a compact state-action space where the transition kernels are Lipschitz in \(_{1}\) but fail to satisfy Assumption 3.

### Algorithms

In this section, following Song and Sun (2019), we present Net-Staged-RandQL algorithm that combines a simple non-adaptive discretization and an idea of stages by Bai et al. (2019); Zhang et al. (2020).

We assume that we have an access to all Lipschitz constants \(L_{r},L_{F},L_{V} L_{V,1}\). Additionally, we have access to the oracle that computes \(\)-cover \(_{}\) of the space \(\) for any predefined \(>0\)4.

**Counts and stages** Let \(n^{t}_{h}(B)_{i=1}^{t-1}\{(s^{t}_{h},a^{i}_{h}) B\}\) be the number of visits of the ball \(B_{}\) at step \(h\) before episode \(t\). Let \(e_{k}=[(1+1/H)^{k} H]\) be length of the stage \(k 0\) and, by convention, \(e_{-1}=0\). We say that \((B,h)\) belongs to the \(k\)-th stage at the beginning of episode \(t\) if \(n^{t}_{h}(B)[_{i=0}^{k-1}e_{i},_{i=0}^{k}e_{i})\). Let \(^{t}_{h}(B) n^{t}_{h}(s,a)-_{i=0}^{k-1}e_{i}\) be the number of visits of the ball \(B\) at step \(h\) during the current stage \(k\).

**Temporary Q-values** At the beginning of a stage, let say time \(t\), we initialize \(J\)_temporary_ Q-values as \(^{t,j}_{h}(B)=r_{0}H\) for \(j[J]\) and \(r_{0}\) some pseudo-reward. Then within a stage \(k\) we update recursively the _temporary_ Q-values

\[^{t+1,j}_{h}(B)=(1-w_{j,})^{t,j}_{h}(B)+w_{j,}[r_{h}(s^{t}_{h},a^{t}_{h})+^{t} _{h+1}(s^{t}_{h+1})],&(s,a)=(s^{t}_{h},a^{t}_{h})\\ ^{1,j}_{h}(B)&\]

where \(=^{t}_{h}(B)\) is the number of visits, \(w_{j,}\) is a sequence of i.i.d random variables \(w_{j,}(1/,(+n_{0}(k))/)\) with \(>0\) some posterior inflation coefficient and \(n_{0}(k)\) a number of pseudo-transitions. The important difference between tabular and metric settings is the dependence on the pseudo-count \(n_{0}(k)\) on \(k\) in the latter case, since here the prior is used to eliminate the approximation error.

**Policy Q-values** Next, we define the policy Q-values that are updated at the end of a stage. Let us fix a ball \(B\) at step \(h\) and suppose that the currents stage ends at time \(t\). Then the policy Q-values are given by the maximum of the temporary Q-values \(^{t+1}_{h}(B)=_{j[J]}^{t+1,j}_{h}(B)\). The policy Q-values are constant within a stage. The value used to define the targets is computed on-flight using the formula \(^{t}_{h}(s)=_{a}^{t}_{h}(_{ }(s,a))\), where \(_{}_{}\) is a quantization map, that assigns each state-action pair \((s,a)\) to a ball \(B(s,a)\). The policy used to interact with the environment is greedy with respect to the policy Q-values and also computed on-flight \(^{t}_{h}(s)_{a}^{t}_{h}(_{ }(s,a))\) (we break ties arbitrarily).

A detail description of Net-Staged-RandQL is provided in Algorithm 4 in Appendix E.2.

### Regret Bound

We fix \((0,1),\) the discretization level \(>0\) and the number of posterior samples

\[J_{J}((2C_{N}HT/)+d_{c}(1/ )),\]

where \(_{J}=1/(4/(3+(1)))\) and \(()\) is the cumulative distribution function (CDF) of a normal distribution. Note that \(J\) has a logarithmic dependence on \(H,T,1/\) and \(1/\). For the regret-optimal discretization level \(=T^{-1/(d_{c}+2)}\), the number \(J\) is almost independent of \(d_{c}\). Let us note that the role of prior in metric spaces is much higher than in the tabular setting. Another important difference is dependence of the prior count on the stage index. In particular, we have

\[n_{0}(k)=_{0}++ (e_{k}+_{0}+),_{0}=(c _{0}+1+_{17/16}(T))\]

where \(c_{0}\) is an absolute constant defined in (5) ( see Appendix D.3), \(\) is the posterior inflation coefficient and \(L=L_{r}+(1+L_{F})L_{V}\) is a constant. We now state the regret bound of Net-Staged-RandQL with a full proof being postponed to Appendix E.

**Theorem 2**.: _Suppose that \(N_{} C_{N}^{-d_{c}}\) for all \(>0\) and some constant \(C_{N}>0\). Consider a parameter \((0,1)\) and take an optimal level of discretization \(=T^{-1/(d_{c}+2)}\). Let \( 2((8HC_{N}/)+d_{c}(1/)+3( (2T+1))),\)\(r_{0} 2\). Then it holds for Net-Staged-RandQL with probability at least \(1-\),_

\[^{T}=}H^{5/2}C_{N}^{1/2}T^{+1}{d_{c}+2}}+H^{3}C_{N}T^{}{d_{c}+2}}+LT^{+1}{d_{c}+2 }}.\]

We can restore the regret bound in the tabular setting by letting \(d_{c}=0\) and \(C_{N}=SA\), where \(S\) is the cardinality of the state-space, and \(A\) is the cardinality of the action-space.

DiscussionFrom the point of view of instance-independent bounds, our algorithm achieves the same result as Net-QL (Song and Sun, 2019) and Adaptive-QL (Sinclair et al., 2019), that matches the lower bound \((HT^{+2}})\) by Sinclair et al. (2023) in dependence on budget \(T\) and covering dimension \(d_{c}\). Notably, as discussed by Sinclair et al. (2023), the model-based algorithm such as Kernel-UCBVI(Domingues et al., 2021) does not achieves optimal dependence in \(T\) due to hardness of the transition estimation problem.

Computational complexityFor a fixed level of discretization \(\), our algorithm has a space complexity of order \(}(H_{})\). Assuming that the computation of a quantization map \(_{}\) has \(}(1)\) time complexity, we achieve a per-episode time complexity of \(}(HA)\) for a finite action space and \((HN_{})\) for an infinite action space in the worst case due to computation of \(_{a}_{h}(_{}(s,a))\). However, this can be improved to \(}(H)\) if we consider adaptive discretization (Sinclair et al., 2019).

Adaptive discretizationAdditionally, we propose a way to combine RandQL with adaptive discretization by Cao and Krishnamurthy (2020), Sinclair et al. (2023). This combination results in two algorithms: Adaptive-RandQL and Adaptive-Staged-RandQL. The second one could achieve the instance-dependent regret bound that scales with a _zooming_ dimension, the instance-dependent measure of dimension. We will follow Sinclair et al. (2023) in the exposition of the required notation.

**Definition 2**.: For any \((s,a)\), the stage-dependent sub-optimality gap is defined as \(_{h}(s,a)=V^{*}_{h}(s)-Q^{*}_{h}(s,a)\).

This quantity is widely used in the theoretical instance-dependent analysis of reinforcement learning and contextual bandit algorithms.

**Definition 3**.: The near-optimal set of \(\) for a given value \(\) defined as \(Z^{}_{h}=\{(s,a)_{h }(s,a)(H+1)\}\).

The main insight of this definition is that essentially we are interested in a detailed discretization of the near-optimal set \(Z^{}_{h}\) for small \(\), whereas all other state-action pairs could be discretized in a more rough manner. Interestingly enough, \(Z^{}_{h}\) could be a lower dimensional manifold, leading to the following definition.

**Definition 4**.: The step-\(h\) zooming dimension \(d_{z,h}\) with a constant \(C_{N,h}\) and a scaling factor \(>0\) is given by

\[d_{z,h}=d>0:>0\;N_{}(Z^{\, }_{h}) C_{N,h}^{-d}}.\]

Under some additional structural assumptions on \(Q^{*}_{h}\), it is possible to show that the zooming dimension could be significantly smaller than the covering dimension, see, e.g., Lemma 2.8 in Sinclair et al. (2023). However, at the same time, it has been shown that \(d_{z,h} d_{}-1\), where \(d_{}\) is a covering dimension of the state space. Thus, the zooming dimension allows adaptation to a rich action space but not a rich state space.

Given this definition, it is possible to define define an adaptive algorithm Adaptive-Staged-RandQL that attains the following regret guarantees

**Theorem 3**.: _Consider a parameter \((0,1)\). For a value \(\) that depends on \(T,d_{c}\) ad \(\), for Adaptive-Staged-RandQL the following holds with probability at least \(1-\),_

\[^{T}=}H^{3}+H^{3/2}_{h=1}^{H} T^{+1}{d_{z,h}+2}},\]

_where \(d_{z,h}\) is the step-\(h\) zooming dimension and we ignore all multiplicative factors in the covering dimension \(d_{c}\)\((C_{N})\), and Lipschitz constants._

We refer to Appendix F to a formal statement and a proof.

## 5 Experiments

In this section we present the experiments we conducted for tabular environments using rlberry library (Domingues et al., 2021). We also provide experiments in non-tabular environment in Appendix I.

EnvironmentWe use a grid-world environment with \(100\) states \((i,j)\) and \(4\) actions (left, right, up and down). The horizon is set to \(H=50\). When taking an action, the agent moves in the corresponding direction with probability \(1-\), and moves to a neighbor state at random with probability \(=0.2\). The agent starts at position \((1,1)\). The reward equals to \(1\) at the state \((10,10)\) and is zero elsewhere.

Variations of randomized Q-learningFor the tabular experiment we use the RandQL algorithm, described in Appendix B as it is the version of randomized Q-learning that is the closest to the baseline OptQL. Note that, we compare the different versions of randomized Q-learning in Appendix B.

BaselinesWe compare RandQL algorithm to the following baselines: (i) OptQL[Jin et al., 2018] (ii) UCBVI[Azar et al., 2017] (iii) Greedy-UCBVI, a version of UCBVI using real-time dynamic programming [Efroni et al., 2019] (iv) PSRL [Osband et al., 2013] and (v) RLSVI[Russo, 2019]. For the hyper-parameters used for these baselines refer to Appendix I.

ResultsFigure 1 shows the result of the experiments. Overall, we see that RandQL outperforms OptQL algorithm on tabular environment, but still degrades in comparison to model-based approaches, that is usual for model-free algorithms in tabular environments. Indeed, using a model and backward induction allows new information to be more quickly propagated. But as counterpart, RandQL has a better time-complexity and space-complexity than model-based algorithm, see Table 2 in Appendix I.

## 6 Conclusion

This paper introduced the RandQL algorithm, a new model-free algorithm that achieves exploration without bonuses. It utilizes a novel idea of learning rate randomization, resulting in provable sample efficiency with regret of order \(}(SAT})\) in the tabular case. We also extend RandQL to the case of metric state-action space by using proper discretization techniques. The proposed algorithms inherit the good empirical performance of model-based Bayesian algorithm such that PSRL while keeping the small space and time complexity of model-free algorithm. Our result rises following interesting open questions for a further research.

Optimal rate for RandQLWe conjecture that RandQL could get optimal regret in the tabular setting if coupled with variance reductions techniques as used by Zhang et al. . However, obtaining such improvements is not straightforward due to the intricate statistical dependencies involved in the analysis of RandQL.

Beyond one-step learningWe observe a large gap in the experiments between Q-learning type algorithm that do one-step planning and e.g. UCBVI algorithm that does full planning or Greedy-UCBVI that does one-step planning with full back-up (expectation under transition of the model) for all actions. Therefore, it would interesting to study also algorithms that range between these two extremes [Efroni et al., 2018, 2019].