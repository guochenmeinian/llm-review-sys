# Two Sides of One Coin: the Limits of Untuned SGD

and the Power of Adaptive Methods

Junchi Yang

Department of Computer Science

ETH Zurich, Switzerland

junchi.yang@inf.ethz.ch

Equal contribution.

&Xiang Li

Department of Computer Science

ETH Zurich, Switzerland

xiang.li@inf.ethz.ch

&Ilyas Fatkhullin

Department of Computer Science

ETH Zurich, Switzerland

ilyas.fatkhullin@ai.ethz.ch

&Niao He

Department of Computer Science

ETH Zurich, Switzerland

niao.he@inf.ethz.ch

###### Abstract

The classical analysis of Stochastic Gradient Descent (SGD) with polynomially decaying stepsize \(_{t}=/\) relies on well-tuned \(\) depending on problem parameters such as Lipschitz smoothness constant, which is often unknown in practice. In this work, we prove that SGD with arbitrary \(>0\), referred to as _untuned SGD_, still attains an order-optimal convergence rate \(}(T^{-1/4})\) in terms of gradient norm for minimizing smooth objectives. Unfortunately, it comes at the expense of a catastrophic exponential dependence on the smoothness constant, which we show is unavoidable for this scheme even in the noiseless setting. We then examine three families of adaptive methods -- Normalized SGD (NSGD), AMSGrad, and AdaGrad -- unveiling their power in preventing such exponential dependency in the absence of information about the smoothness parameter and boundedness of stochastic gradients. Our results provide theoretical justification for the advantage of adaptive methods over untuned SGD in alleviating the issue with large gradients.

## 1 Introduction

In this work, we study the stochastic optimization problem of the form:

\[_{x^{d}}\,f(x)=_{ P}[F(x;)],\]

where \(P\) is an unknown probability distribution, and \(f:^{d}\) is an \(\)-Lipschitz smooth function and can be non-convex. In the context of machine learning, \(\) typically represent an individual training sample drawn from the data distribution \(P\), and \(x\) denotes the weights of the model.

Stochastic Gradient Descent (SGD), originating from the seminal work , performs the following update iteratively:

\[x_{t+1}=x_{t}-_{t} F(x_{t};_{t}),\]

where \(_{t}>0\) presents a positive stepsize, and \( F(x_{t};_{t})\) is an unbiased stochastic gradient. SGD has shown remarkable empirical success in many modern machine learning applications, e.g., . Its efficiency is usually attributed to its cheap per iteration cost and the ability to operate in anonline fashion, making it suitable for large-scale problems. However, empirical evidence also reveals undesirable behaviors of SGD, often linked to challenges in selecting appropriate stepsizes. In particular, a number of works report the _gradient explosion_ effect  during the initial phase of training, which may eventually lead to divergence or prohibitively slow convergence. The phenomenon is also observed in our experiments (see Figure 1(f)) when the stepsize is poorly chosen. Unfortunately, this phenomenon has not been well understood from a theoretical point of view. The classical analysis of SGD in the smooth non-convex case , prescribes to select a non-increasing sequence of stepsizes \(\{_{t}\}_{t 1}\) with \(_{1}<2/\). In particular, the choice \(_{t}=1/()\), guarantees2 to find a point \(x\) with \([\| f(x)\|]\) after \((^{-4})\) stochastic gradient calls, which is also known to be unimprovable in the smooth non-convex setting unless additional assumptions are made .

However, the bound on the smoothness parameter \(\) is usually not readily available for practitioners, and the limited computing power usually prevents them from exhaustive tuning to find the best stepsize. It is therefore important to provide a theoretical understanding for SGD with an arbitrary stepsize (which we refer to as _untuned SGD_) that is agnostic to the problem parameter. The following intriguing question remains elusive in the stochastic optimization literature:

_How does untuned SGD with a decaying stepsize \(_{t}=/\) perform when \(\) is independent of the smoothness parameter? How can we explain the undesirably large gradients encountered in training with SGD?_

Recently, there has been a surge of interest in adaptive gradient methods such as Adam , RMSProp , AdaDelta , AMSGrad , AdaGrad , Normalized SGD , and many others. These methods automatically adjust their stepsizes based on past stochastic gradients, rather than using pre-defined iteration-based schedules. Empirically, they are observed to converge faster than SGD and mitigate the issue of gradient explosion across a range of problems, even without explicit knowledge of problem-specific parameters . Figure 1 provides a basic illustration of performance differences between SGD with \(_{t}=1/\) stepsizes and adaptive schemes such as AdaGrad and Normalized SGD with momentum (NSGD-M) . Notably, when the initial stepsize is too large (relative to \(1/\) value), SGD reaches the region with _large gradients_, while adaptive methods do not

  
**Algorithms** & **Upper bound;** & **Lower bound;** & **Upper bound;** & **Lower bound;** \\  & **deterministic** & **deterministic** & **stochastic** & **stochastic** \\   SGD (Eq. 1) & \(}((4e)^{2()^{2}}^{-4})\) & \(((8e)^{^{2}^{2}/8}^{-4})\) & \(}((4e)^{2()^{2}}^{-4})\) & \(((8e)^{^{2}^{2}/8}^{-4})\) \\ \(}\) & [Thm.

suffer from such effect. However, the theoretical benefits of adaptive methods over SGD remain unclear. A large number of existing analyses of adaptive methods assume bounded gradients, or even stochastic gradients, precluding not only a fair comparison with SGD whose convergence does not need bounded gradient but also the possibility to explain their benefit when facing gradient explosions. While recent developments show that AdaGrad-type methods  can attain a \(}(^{-4})\) sample complexity under the same standard assumptions as for SGD analysis, there is still a lack a good explanation for the substantial performance gap observed in practice despite SGD with well-tuned stepsizes theoretically achieving the lower complexity bound. We will address the following open question:

_Can we justify the theoretical benefits of adaptive methods over untuned SGD for smooth non-convex problems without assuming bounded gradients?_

Consequently, this work is grounded on the premise that gradients may be not bounded and that hyper-parameters are independent of the problem parameters.. The main contributions are as follows:

* We show that untuned SGD with diminishing stepsizes \(_{t}=/\) finds an \(\)-stationary point of an \(\)-smooth function within \(}((^{2}+^{4}^{4}^{4})(4e)^{2^{2} ^{2}}^{-4})\) iterations for any \(>0\). Here, \(^{2}\) corresponds to the variance of the stochastic gradient. Although this classical algorithm converges and has the optimal dependence on \(\), we further show that the disastrous exponential term in \(^{2}^{2}\) is unavoidable even when the algorithm has access to exact gradients. This explains its proneness to gradient explosion when the problem parameter is unknown. Previous analyses have failed to capture this exponential term, since they assume \(\) is well-tuned to be \((1/)\).
* AMSGrad, which was proposed to fix the nonconvergence of Adam, is not yet fully understood, with previous analyses depending on _bounded stochastic gradients_. We show that AMSGrad (norm version) is free from exponential constants in the deterministic setting without tuning, in stark contrast with SGD. Surprisingly, in the stochastic setting when the stochastic gradients are

Figure 1: Comparison of SGD, AdaGrad, and NSGD-M on a quadratic function \(f(x)= x^{2}/2\) and a deep neural network. SGD employs a diminishing stepsize of \(/\), while the stepsizes for AdaGrad and NSGD-M are specified in Propositions 2 and 3, respectively. For experiments on the quadratic function, we set \(=1\) for all methods and test with 3 different values of \(\). For the figures in the second row, We train a 50-layer ResNet  on the CIFAR-10 dataset  using cross-entropy loss. Due to the large randomness observed for this setting, we run each combination of optimizer and stepsize 10 times and plot both the average and variance. Note that the scale of the y-axis varies for each sub-figure and SGD diverges to NaN when \(=7\).

unbounded, we show that AMSGrad may converge at an arbitrarily slow polynomial rate. To the best of our knowledge, these are the first results for AMSGrad without assuming bounded gradients.
* To further illuminate the advantages of adaptive methods, we re-examine the results for Normalized Gradient Descent (NGD), Normalized SGD with momentum (NSGD-M) from  and AdaGrad-norm from , considering stepsize independent of the problem parameters similar to untuned SGD. They all achieve near-optimal complexities while shredding off the exponential factor. As a side result, we provide a strong non-convergence result of NSGD without momentum under any bounded stepsizes, which might be of independent interest.

Our findings contribute a fresh understanding of the performance gap between SGD and adaptive methods. Albeit with a near-optimal rate, untuned SGD is vulnerable to gradient explosion and slow convergence due to a large exponential constant in its complexity, which can be circumvented by several adaptive methods. To the best of our knowledge, this substantial difference is unaddressed in the previous literature, because the majority of analyses for SGD and adaptive methods turn to either well-tuned stepsize based on problem parameters or the assumption of bounded gradients. Part of our results are summarized in Table 1, and full results for a broader range of stepsizes can be found in Table 2 in the appendix.

### Related Work

**SGD in nonconvex optimization.** Stochastic approximation methods and SGD in particular have a long history of development [58; 34; 8; 13; 51; 56]. When the objective is \(\)-smooth and the gradient noise has bounded variance \(^{2}\), Ghadimi and Lan  and Bottou et al.  prove that if stepsize \(_{t}=/\), where \(=(,^{2})\) and \(T\) is the total iteration budget, then SGD can find an \(\)-stationary point within \((^{2}^{-4})\) iterations. Similar complexity (up to a logarithmic term) can also be achieved by decaying stepsizes \(/\)[22; 17; 65]. This result was later shown to be optimal for first-order methods under these assumptions . Several works consider various relaxations of the stochastic oracle model with bounded variance, for instance, biased oracle  and relaxed growth condition . However, these results also heavily rely on sufficiently small \(\), e.g., \( 1/\), and the convergence behavior in the large \(\) regime is rarely discussed. Remarkably, Lei et al.  characterize the convergence of SGD under individual smoothness and unbiased function values. They consider Robbins-Monro stepsize schemes, which includes \(/t^{}\) when \(>1/2\), and derive \((^{})\) sample complexity including an exponential dependence on the smoothness parameter. Unlike , we focus on the standard assumptions and derive better dependency in smoothness constant when \(>1/2\). Importantly, we further justify that the exponential constants are unavoidable with a lower bound.

**Adaptive methods.** We focus on methods directly using gradients to adjust stepsize, rather than other strategies like backtracking line search . Normalized Gradient Descent (NGD) was introduced by  for quasi-convex functions. Hazan et al.  apply NGD and NSGD with minibatch to the class of locally-quasi-convex functions. Later, Cutkosky and Mehta  and Zhao et al.  prove NSGD with momentum or minibatch can find an \(\)-stationary point in smooth nonconvex optimization with sample complexity \((^{-4})\). AdaGrad was introduced in the online convex optimization [18; 48]. In nonconvex optimization, AdaGrad and its scalar version, AdaGrad-norm , achieve competitive convergence rates with SGD [66; 43; 31; 44]. RMSProp  and Adam  use the moving average of past gradients, but may suffer from divergence without hyper-parameter tuning . Recently, it was shown in the finite-sum setting that they converge to a neighborhood, whose size shrinks to 0 by tuning hyper-parameters [60; 72]. However, most of the results on AdaGrad and Adam-type algorithms assume both Lipschitz and bounded gradients [75; 12; 16; 66; 76]. Very recently, Faw et al.  and Yang et al.  independently show that AdaGrad-norm converges without assuming bounded gradients and without the need for tuning, attaining a sample complexity of \(}(^{-4})\).

**SGD v.s. adaptive methods.** Despite similar complexities, adaptive methods typically converge faster than SGD in practice [10; 47] and are widely used to prevent large gradients [55; 23]. Various attempts have been made to theoretically explain these differences. Some suggest that the advantage of adaptive algorithms is their ability to achieve order-optimal rates without knowledge of problem parameters such as smoothness and noise variance [66; 40; 30]. Other studies investigate the faster escape from saddle points by adaptive methods [41; 50; 67]. The importance of coordinate-wise normalization in Adam has also been highlighted [5; 37]. Furthermore, the influence of heavy-tail noise on the performance of adaptive methods is studied . However, most previous works do not provide an explanation for the faster convergence of adaptive methods in terms of sample complexity. Notably, Zhang et al.  and Wang et al.  explain the benefits of gradient clipping and Adam under a relaxed smoothness assumption, a setting where SGD with non-adaptive stepsizes may not converge. In contrast, we analyze SGD and several adaptive methods under standard smoothness and noise assumptions, distinguishing it from the recent work of Wang et al.  that focuses on one variant of Adam for finite-sum problems with individual relaxed smoothness and random shuffling.

## 2 Problem Setting

Throughout this work, we focus on minimizing an \(\)-smooth function \(f:^{d}\). We have access to a stochastic gradient oracle that returns \(g(x;)\) at any point \(x\), and we make the following standard assumptions in nonconvex optimization.

**Assumption 1** (smoothness).: _Function \(f(x)\) is \(\)-smooth with \(>0\), that is, \(\| f(x_{1})- f(x_{2})\|\|x_{1}-x_{2}\|\) for any \(x_{1}\) and \(x_{2}^{d}\)._

**Assumption 2** (stochastic gradients).: _The stochastic gradient \(g(x;)\) is unbiased and has a bounded variance, that is, \(_{}[g(x;)]= f(x)\) and \(_{}[\|g(x;)- f(x)\|^{2}] ^{2}\) for any \(x^{d}\)._

We present the general scheme of SGD with initial point \(x_{0}\) and a stepsize sequence \(\{_{t}\}_{t=0}^{}\):

\[x_{t+1}=x_{t}-_{t}\;g(x_{t};_{t}). \]

Some commonly used stepsizes include polynomially and geometrically decaying stepsize, constant stepsize, cosine stepsize, etc. When the stepsize depends on the instantaneous or past gradients, i.e., \(\{g(x;_{k})\}_{k t}\), we call it adaptive stepsize, namely Normalized SGD , AdaGrad , Adam , AMSGrad , etc. In some adaptive methods, momentum is also considered, replacing \(g(x_{t};_{t})\) in (1) with a moving average \(m_{t+1}\) of the past stochastic gradients (see Section 4 for more details). To set the stage for our analysis, we assume that \(f(x_{0})-_{x^{d}}f(x)\), where \(\) represents the initial gap. Given that the function class of interest is nonconvex, we aim to find an \(\)-stationary point \(x\) with \([\| f(x)\|]\).

## 3 Convergence of Untuned SGD

In this section, we focus on SGD with the decaying stepsize:

\[_{t}=},\]

where \(>0\) is the initial stepsize. Most convergent analysis requires \(<2/\)[22; 9] so that there is "sufficient decrease" in function value after each update, and if \(\) is carefully chosen, it can achieve the near-optimal complexity of \(}(^{-4}^{2})\). Nevertheless, as the smoothness parameter is usually unknown, providing guarantees with optimal \(\) or assuming \(\) to be problem-dependent does not give enough insights into practical training with SGD. Hence we are interested in its convergence behavior in both small and large initial stepsize regimes, i.e., \( 1/\) and \(>1/\).

**Theorem 1**.: _Under Assumptions 1 and 2, if we run SGD with stepsize \(_{t}=}\), where \(>0\),_

\[_{t=0}^{T-1}\| f(x_{t})\|^{2} 2A^{-1}T^{-},& 1/,\\ 4 A(4e)^{}( T)^{-},&>1/, \]

_where \(=^{2}^{2}-1\) and \(A=(+^{2}}{2}(1+ T))\)._

This theorem implies that when the initial stepsize \(>1/\), SGD still converges with a sample complexity of \(}((^{2}+^{4}^{4}^{4})(4e)^{2^{2} ^{2}}^{-4})\). Although the dependency in the target accuracy \(\) is near-optimal, it includes a disastrous exponential term in \(^{2}^{2}\). This is due to polynomially decaying stepsizes: in the first stage before \(=()^{2}-1\) iterations, the function value and gradients may keep increasing in expectation until reaching an exponential term \(^{2}^{2}\), which is in stark contrast with adaptive methods that we will see in Section 4; in the second stage after \(t\), the stepsize is small enough to decrease the function value in expectation at a rate of \(1/\) up to a small term in \(\).

If we arbitrarily select \(=(1)\), untuned SGD may induce large gradients growing exponentially in \(\) in the first stage, as observed in Figure 1. Moreover, deriving the dependence in hyper-parameter \(\) is crucial for assessing the effort required in its tuning: SGD with an \(\) that is \(c>1\) times larger than the optimally tuned value can yield a gradient norm that is \(((c))\) times larger in the convergence guarantee.

To the best of our knowledge, there is limited study for non-asymptotic analysis of untuned SGD under the same assumptions. Moulines and Bach  study untuned SGD under individual smoothness and convexity assumptions, i.e., \(g(x;)\) is Lipschitz continuous and \(F(x;)\) is convex almost surely. They showed a convergence rate of \((1/T^{1/3})\) for the last iterate and a rate arbitrarily close to \((1/T^{1/2})\) for the averaged iterate. Later, Fontaine et al.  provide \(}(1/T^{1/2})\) convergence rate for untuned SGD in the convex setting, albeit without an explicit dependency in \(\) and \(\). In the nonconvex setting, Khaled and Richtarik  showed, using SGD with constant stepsize \(>0\), the squared gradient \(_{t<T} f(x_{t})^{2}\) converges at a rate of \((+(1+^{2}^{2})^{T}/( T))\). When the total number of iterations \(T\) is predetermined, selecting a constant stepsize \(=_{0}/\) results in the rate \((}{}+_{0 }^{2}/T)^{T}}{_{0}})(}{}+_{0}^{2}}{_{0} })\), which exhibits the same dependence on \(T\) as Theorem 1 and also includes an exponential constant. However, our focus is on settings where \(T\) is not known in advance.

**Remark 1**.: _We consider stepsize in the order of \(1/\), because it is known for SGD to achieve the best dependency in \(\) for nonconvex optimization  and easier to compare with adaptive stepsizes. We also present the convergence results for more general polynomially decaying stepsizes, i.e., \(_{t}=}\) with \(0<<1\), in Theorem 6 of the appendix. There exists a trade-off between convergence speed \((1/T^{})\) and the exponential term in \(()^{1/}\) for \([1/2,1)\). Intuitively, larger \(\) leads to a shorter time in adapting to \(1/\) stepsize but a slower convergence rate. We do not consider constant stepsize, i.e., \(=0\), because it is well known to diverge even in the deterministic setting if the stepsize is agnostic to the problem parameter ._

The question arises as to whether the exponential term is necessary. In the following, we provide a lower bound for SGD under this choice of stepsize.

**Theorem 2**.: _Fixing \(T 1,>0,>0\) and \(>0\) that \( 5\), there exists a \(\)-smooth function \(f:\) and an initial point \(x_{0}\) with \(f(x_{0})-f^{*}\) such that if we run Gradient Descent with stepsize \(_{t}=}\), then for \(t t_{0}=^{2}^{2}/16-1\),_

\[| f(x_{t})|}}(8e)^{t/2 }| f(x_{t_{0}})|}(8e )^{^{2}^{2}/32-4};\]

Figure 2: Demonstration of the constructed function used to prove the lower bound. The definitions of the segments comprising the function can be found in Appendix B.2.

_if \(T>t_{0}\), then for \(t_{0}<t T\),_

\[| f(x_{t})|}\{^{1/2},(2 )^{-1/2}T^{-1/4}\},( 8e)^{^{2}^{2}/16-2}.\]

This theorem suggests that Gradient Descent (GD) with decaying stepsize \(/\) needs at least \((^{-4}^{-2}(8e)^{^{7}^{2}/8}^{-4})\) iterations to find an \(\)-stationary point in the large initial stepsize regime. Therefore, it validates that an exponential term in \(^{2}^{2}\)_multiplied by \(^{-4}\)_ is not avoidable even in the deterministic setting. It is crucial to note that our result is limited to untuned (S)GD with this particular stepsize scheme.

It is worth pointing out that the existing lower bounds for first-order methods  and SGD  do not contain any exponential terms. On the other hand, Vaswani et al.  established a lower bound in the strongly convex setting for GD with exponentially decreasing stepsizes \((1/T)^{t/T}\), where \(T\) denotes the total number of iterations and \(>0\). With a quadratic function, they showed that if \(>3/\), the distance to the optimal solution \(\|x_{t}-x^{*}\|\) exhibits exponential growth in \(t\) during the initial \((T/ T)\) iterations. However, this exponential growth is rapidly mitigated afterward, resulting in no exponential term at the final iterate \(T\). This behavior starkly contrasts with our nonconvex setting.

We illustrate our hard instance for Theorem 2 in Figure 2, which is one-dimensional. The algorithm starts from a valley of the function \(f(x)= x^{2}/2\), i.e., Segment 1. Because of the large initial stepsize and steep slope, in the first \(t_{0}\) iterations, Gradient Descent increases the function value as large as \(=((8e)^{^{2}^{2}/16})\). Then the iterate \(x_{t_{0}+1}\) jumps to the top of a very flat valley, i.e., Segment 4, so that Gradient Descent decreases the gradient as slowly as \((T^{-1/4})\).

_Why do not we assume gradients to be bounded?_ The assumption on bounded gradients is not satisfied even for the simple function \(f(x)= x^{2}/2\). When training neural networks, gradient explosion is often observed [55; 59], which directly suggests that this assumption is not satisfied or only satisfied with a numerically large constant. In Proposition 4 in the appendix, we also provide a simple proof for the convergence under the additional assumption of bounded gradient, i.e., \(\| f(x)\| G\) for all \(x\), attaining a sample complexity of \(}(^{2}^{2}G^{4}^{2}^{-4})\) without any information about problem parameters. However, compared with Theorem 1 and 2, constant \(G\) hides the exponential term. In Figure 1, we observe that the gradient bound along the trajectory of non-adaptive stepsize can be much larger than that of adaptive stepsize even if starting from the same initial point, so assuming bounded gradient will obscure the difference between them.

## 4 Power of Adaptive Methods

In this section, we focus on the convergence behaviors of adaptive methods, which adjust their stepsizes based on the observed gradients. In particular, when arriving at a point with a large gradient, adaptive methods automatically decrease their stepsizes to counter the effect of possible gradient increase; to list a few, Normalized SGD , AdaGrad , Adam . Since the analysis for adaptive methods is usually on a case-by-case basis, we will examine three examples - Normalized SGD, AMSGrad-norm, and AdaGrad-norm - to establish a universal observation that they avoid exponential dependency in \(\) without tuning. Although many existing analyses rely on bounded gradients (and function values) or information on problem parameters, we will abandon such assumptions as noted in the previous section. We focus on the norm instead of the coordinate-wise version of adaptive methods, which means each coordinate adopts the same stepsize, because the norm version is usually dimension-independent in the complexity, and is also widely used in both theory and practice [73; 45; 43; 38; 54; 32].

### Family of Normalized SGD

Normalized (Stochastic) Gradient Descent [53; 26], referred to as NGD and NSGD, is one of the simplest adaptive methods. It takes the stepsize in (1) to be normalized by the norm of the current (stochastic) gradient:\[_{t}=}{\|g(x_{t};_{t})\|},\]

where \(\{_{t}\}_{t 0}\) is a sequence of positive learning rate. Cutkosky and Mehta  and Zhao et al.  show that NSGD with \(_{t}=/\) can find an \((1/+)\)-stationary point. In order to compare fairly with untuned SGD with decaying stepsize, we present a modification with decaying \(_{t}=/\) in NSGD.

**Proposition 1**.: _Under Assumption 1 and 2, if we run NSGD with \(_{t}=}\), then for any \(>0\),_

\[_{t=0}^{T-1}\| f(x_{t})\| 3(+(T))T^{-}{{2}}}+24.\]

**NGD.** In the deterministic setting, by Proposition 1, NGD converges to an \(\)-stationary point with a complexity of \(}((^{-2}+^{2}^{2})^{-2})\) for any \(>0\), which importantly does not include any exponential term. Thus, even if the initial stepsize is not small enough, it does not result in a catastrophic gradient explosion.

**NSGD.** In the stochastic setting, Proposition 1 implies that NSGD can find an \(\)-stationary point only when the noise variance is small enough, i.e., \(()\). This is not the consequence of a loose analysis. Hazan et al.  show that NSGD with constant \(_{t}\) does not converge when the mini-batch size is smaller than \((^{-1})\) for a non-smooth convex function. Here we provide a non-convergence result in the gradient norm with a smooth objective for all uniformly bounded stepsizes. The intuition behind this is illustrated in Figure 3 in the appendix, where \(_{}\ g(x;)/\|g(x;)\|\) can easily vanish or be in the opposite direction of \( f(x)\) under certain noises.

**Theorem 3**.: _Fixing \(>0\), \(>0\), \(>0\), \(>0\) and stepsize sequence \(\{_{t}\}_{t=0}^{}\) with \(_{t}_{}\) that \(^{2}<\{^{2},2,2(-)/_{}\}\), there exists an \(\)-smooth convex function \(f\), initial point \(x_{0}\) with \(f(x_{0})-_{x}f(x)\) and zero-mean noises with \(^{2}\) variance such that the output from NSGD satisfies \(\| f(x_{t})\|\) for all \(t\)._

This theorem implies that, given a fixed function class (\(,,\)) and any sequence \(\{_{t}\}_{t}\) uniformly upper bounded by \(_{}\), NSGD cannot converge to an arbitrarily small \(\). Specifically, the expected gradient norm will always stay larger than \(\{,,_{}^{-1}(-++2 _{}})\}\). Most \(\{_{t}\}_{t}\) used in practice is upper bounded, e.g., constant or decreasing sequences. The condition \(^{2}<2\) is necessary by noting that \(\| f(x_{0})\|^{2} 2[f(x_{0})-_{x}f(x)] 2\). Considering \(_{t}=1/\), when \(\) and \(\), it matches with Proposition 1, where NSGD can only converge to a \(()\)-stationary point. Since Sign-SGD and NSGD coincide for one-dimensional objectives, our non-convergent example also applies to Sign-SGD. This sheds light on why increasing batch size improves Normalized and Sign-SGD . However, these methods are generally different in higher dimensions, as Karimireddy et al.  show that sign-SGD may not converge even with a full batch.

**NSGD with momentum.** While NSGD may not always converge, Cutkosky and Mehta  introduced NSGD with momentum (NSGD-M) presented in Algorithm 1 with constant \(_{t}\). We provide the following modification with diminishing \(_{t}\) that eliminates the need to specify the total number of runs beforehand.

**Proposition 2**.: _Under Assumptions 1 and 2, if we run NSGD-M with \(_{t}=}{}\) and \(_{t}=}\), then for any \(>0\),_

\[_{t=0}^{T-1}\| f(x_{t})\| C(+(+)(T))T^{-},\]

_where \(C>0\) is a numerical constant._

It implies that NSGD-M attains a complexity of \(}((^{-4}+^{4}^{4})^{-4})\) for any \(>0\). Compared with Theorem 1 and 2, NSGD-M not only achieves near-optimal dependency in the target accuracy \(\), but also shreds the exponential term when the hyper-parameter is agnostic to smoothness constant.

```
1:Input: initial point \(x_{0}\), stepsize sequence \(\{_{t}\}\), momentum sequence \(\{_{t}\}\), and initial momentum \(g_{0}\).
2:for\(t=0,1,2,...\)do
3:\(x_{t+1}=x_{t}-}{\|g_{t}\|}g_{t}\)
4: sample \(_{t+1}\)
5:\(g_{t+1}=(1-_{t})g_{t}+_{t}g(x_{t+1};_{t+1})\)
6:endfor
```

**Algorithm 1** NSGD-M

### AMSGrad-norm

AMSGrad was introduced by Reddi et al.  to fix the possible non-convergence issue of Adam. Notably, current analyses of AMSGrad in the stochastic setting show a convergence rate of \(}(1/T^{1/4})\), but they rely on the assumption of _bounded stochastic gradients_, which is much stronger than assumptions used for SGD analysis. Here, we examine the simpler norm version of AMSGrad, presented in Algorithm 2. We prove that without assuming bounded stochastic gradients, AMSGrad-norm with default \(_{t}=/\) may converge at an arbitrarily slow polynomial rate. In fact, this holds even if the true gradients are bounded. We believe this result is of independent interest.

**Theorem 4**.: _For any \(>0\), \(>0\), \(>0\) and \(T>1\), there exists a \(\)-smooth function \(f:^{2}^{2}\), \(x_{0}\) with \(f(x_{0})-_{x}f(x)\) and noise distribution \(P\) with variance upper bounded by \(^{2}\), such that if we run AMSGrad-norm with \(0_{1} 1\), \(0_{2}<1\) and \(_{t}=}\), we have with probability \(\), it holds that_

\[_{t\{0,1,...,T-1\}}\| f(x_{t})\|)}}{ ((-1))^{}(1- )}}(T^{1-}-)\}}}\]

_for any \(<<1\), where \(()\) denotes the Gamma function._

The intuition behind this thereom is that since AMSGrad utilizes the maximum norm of past stochastic gradients with momentum in the denominator of stepsizes, some noise distributions enable this maximum norm to increase polynomially, making the stepsizes too small. However, we can still explore its benefit in the deterministic setting. Whether it converges without assuming bounded gradients, to the best of our knowledge, is unknown. Here, for simplicity, we consider AMSGrad-norm without momentum, i.e., \(_{1}=_{2}=0\).

**Theorem 5**.: _Under Assumption 1, if we run AMSGrad-norm with \(_{t}=}\), \(v_{0}>0\) and \(_{1}=_{2}=0\) in the deterministic setting, then for any \(>0\) and \(0<<1\),_

\[_{t=0}^{T-1}\| f(x_{t})\|T^{-} ,\}^{-1}},&\ v_{0}< ,\\ T^{-}^{2}^{2}v_{0}^{-2}+T^{-}\}},&\ v_{0}, \]

_where \(M=^{2}(1+(}))\)._

The theorem implies that AMSGrad-norm achieves a complexity of \(}((^{4}^{4}+^{2}+^{3}^{2}+ ^{-2})^{-4})\) with the default \(_{t}=(t^{-1/2})\). Compared with untuned Gradient Descent, it gets rid of the exponential dependency. In the proof, we show that before the first iteration \(\) when stepsizereduces to \(1/\), the accumulated gradient norms \(_{t=0}^{-1} f(x_{t})^{2}\) are upper bounded polynomially, which is in striking contrast with SGD in Theorem 2. We further provide theoretical guarantees for more general schemes \(}\) with \(0<<1\) in Theorem 7 in the appendix. We also derive matching lower bounds in Theorem 8 for any \(0<<1\), and justify that AMSGrad may fail to converge with constant \(_{t}\) (i.e., \(=0\)) if the problem parameter is unknown.

### AdaGrad-norm

AdaGrad chooses its stepsize to be inversely proportional to the element-wise accumulated past gradients [18; 48]. Its norm-version, AdaGrad-norm [61; 66], picks stepsize in (1) to be

\[_{t}=^{2}+_{k=0}^{t}\|g(x_{k};_{k})\|^{2}}},\]

where \(v_{0}>0\). Very recently, AdaGrad is proven to converge in nonconvex optimization without the assumption on bounded gradients or tuning \(\)[20; 68]. Although the result in  are presented for minimax optimization problems, a similar result follows immediately for minimization problems. We present the following result for the completeness of the paper and to further illustrate the benefits of adaptive methods over SGD.

**Proposition 3**.: _Under Assumptions 1 and 2, if we run AdaGrad-norm, then for any \(>0\) and \(v_{0}>0\),_

\[_{t=0}^{T-1}\| f(x_{t})\| }+A}}{}+}{T^{}},\]

_where \(A=}(++)\)._

The above result implies a complexity of \(}((^{-2}+^{2}+^{2}^{2})( ^{-2}+^{2}^{-4}))\). Notably, if \(\) can be chosen to be \(1/\), it achieves the optimal complexity in both \(\) and \(\) up to logarithmic terms like well-tuned SGD . Even if \(\) is agnostic to \(\), AdaGrad-norm does not suffer from the exponential term present in untuned SGD. One of the intuitions in the deterministic setting, similar to the AMSGrad-norm, is that the accumulated squared gradient norm before the first iteration with stepsize smaller than \(1/\) will be upper bounded by a polynomial term (see Theorem 3.2 in ). Another benefit of AdaGrad over other methods is to achieve optimal convergence rates simultaneously in deterministic and stochastic settings with the same hyper-parameters. This is sometimes referred to as "noise adaptivity", which is out of the scope of this paper.

## 5 Conclusion and Future Directions

We convey a crucial message: SGD with a polynomially decaying stepsize can converge at the order-optimal rate, while remaining agnostic to the problem-specific parameter - a notion that may challenge common belief. However, it is subject to an exponential term, which can be avoided by utilizing an appropriate adaptive scheme. We further reveal the similarity and differences between several adaptive schemes; a detailed exposition of these results can be found in Appendix A. This work opens up several avenues for future research. Firstly, it would be valuable to investigate whether the benefits of adaptive methods extend to high-probability convergence guarantees and apply to a broader range of adaptive optimizers. Secondly, removing the assumption of bounded gradients is crucial for a comprehensive analysis of adaptive algorithms, as it can reveal the true dependence on the smoothness parameter \(\) and highlight their advantage over SGD. Lastly, examining the impact of adaptive algorithms on optimizing non-smooth nonconvex objectives, which are prevalent in training modern machine learning models, presents an interesting research direction.