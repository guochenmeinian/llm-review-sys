# MeGraph: Capturing Long-Range Interactions by Alternating Local and Hierarchical Aggregation on Multi-Scaled Graph Hierarchy

MeGraph: Capturing Long-Range Interactions by Alternating Local and Hierarchical Aggregation on Multi-Scaled Graph Hierarchy

 Honghua Dong\({}^{1,2,3}\)   Jiawei Xu\({}^{1,4}\)   Yu Yang\({}^{1,4}\)   Rui Zhao\({}^{1}\)   Shiwen Wu\({}^{5}\)

Chun Yuan\({}^{4}\)   Xiu Li\({}^{4}\)   Chris Maddison\({}^{2,3}\)   Lei Han\({}^{1}\)

\({}^{1}\)Tencent Robotics X \({}^{2}\)University of Toronto \({}^{3}\)Vector Institute

\({}^{4}\)Tsinghua University \({}^{5}\)Hong Kong University of Science and Technology

Equal Contribution. Work done while HD, JX and YY are interns at Tencent Robotics X.Corresponding authors, contact honghuaad@cs.toronto.edu and lxhan@tencent.com.

###### Abstract

Graph neural networks, which typically exchange information between local neighbors, often struggle to capture long-range interactions (LRIs) within the graph. Building a graph hierarchy via graph pooling methods is a promising approach to address this challenge; however, hierarchical information propagation cannot entirely take over the role of local information aggregation. To balance locality and hierarchy, we integrate the local and hierarchical structures, represented by intra- and inter-graphs respectively, of a multi-scale graph hierarchy into a single mega graph. Our proposed MeGraph model consists of multiple layers alternating between local and hierarchical information aggregation on the mega graph. Each layer first performs local-aware message-passing on graphs of varied scales via the intra-graph edges, then fuses information across the entire hierarchy along the bidirectional pathways formed by inter-graph edges. By repeating this fusion process, local and hierarchical information could intertwine and complement each other. To evaluate our model, we establish a new Graph Theory Benchmark designed to assess LRI capture ability, in which MeGraph demonstrates dominant performance. Furthermore, MeGraph exhibits superior or equivalent performance to state-of-the-art models on the Long Range Graph Benchmark. The experimental results on commonly adopted real-world datasets further demonstrate the broad applicability of MeGraph. 1

## 1 Introduction

Graph-structured data, such as social networks, traffic networks, and biological data, are prevalent across a plethora of real-world applications. Recently, Graph Neural Networks (GNNs) have emerged as a powerful tool for modeling and understanding the intricate relationships and patterns present in such data. Most existing GNNs learn graph representations by iteratively aggregating information from individual nodes' local neighborhoods through the message-passing mechanism. Despite their effectiveness, these GNNs struggle to capture long-range interactions (LRIs) between nodes in the graph. For instance, when employing a 4-layer vanilla GNN on the 9-node (\(A\) to \(I\)) graph (as shown in Fig. 1), the receptive field of node \(A\) is limited to 4-hop neighbors, making the aggregation of information from nodes \(G\), \(H\), and \(I\) into node \(A\) quite challenging. While GNNs could theoretically incorporate information from nodes \(n\)-hops away with \(n\)-layers of message passing, this often leads to over-smoothing and over-squashing issues [17; 3] when \(n\) is large.

One mainstream solution to this problem involves constructing a multi-scale graph hierarchy through graph pooling methods. Previous efforts, such as Graph UNets  and HGNet , have attempted to broaden the receptive field using this strategy. They downsample and upsample the graph, aggregating information along the hierarchy. However, hierarchical information propagation cannot take over the role of local information aggregation. To illustrate, consider the graph hierarchy depicted in Fig. 1. The information propagated along the hierarchy from node \(B\) to nodes \(D\), \(E\), and \(F\) tends to be similar since they share the common path _B-1-X-2_. However, in the original graph, node \(B\) holds different degrees of importance to nodes \(D\), \(E\), and \(F\) as they are 2, 1, and 3 hops away respectively.

To balance the importance of locality and hierarchy, we amalgamate the local and hierarchical structures of a multi-scale graph hierarchy into a single mega graph as depicted in Fig. 1, where we refer to the local structure as intra-graph edges and hierarchical structure as inter-graph edges. Based on this mega graph, we introduce our MeGraph model consisting of \(n\) Mee layers. Each layer first performs local-aware message-passing on graphs of varied scales via the intra-graph edges and then fuses information across the whole hierarchy along the bidirectional pathways formed by inter-graph edges. This method enables hierarchically fused information to circulate within local structures and allows locally fused information to distribute across the hierarchy. By repeating this fusion process, local and hierarchical information could intertwine and complement each other. Moreover, to support flexible graph pooling ratios when constructing the multi-scale graph hierarchy, we propose a new graph pooling method S-EdgePool that improves from EdgePool .

In our experiments, We first evaluate MeGraph's capability to capture Long Range interactions (LRIs). We establish a Graph Theory Benchmark comprising four tasks related to shortest paths and one related to connected components. MeGraph demonstrates superior performance compared with many competitive baselines. MeGraph also achieves comparable or superior performance than the state-of-the-art on the Long Range Graph Benchmark (LRGB) . In addition, we perform extensive experiments on widely-used real-world datasets that are not explicitly tailored for assessing the capacity to capture LRIs. These include the GNN benchmark  and OGB-G datasets . In these tests, MeGraph demonstrates superior or equivalent performance compared to the baseline models, suggesting its broad applicability and effectiveness.

The main contributions of this work are summarized as follows: 1) **Mega graph and novel architecture**: we propose the mega graph, a multi-scale graph formed by intra- and inter-graph edges, where the message-passing over the mega graph naturally balances locality and hierarchy. On this basis, we introduce a novel architecture MeGraph, which alternates information aggregation along the intra- and inter-edges of the mega graph. This fusion process intertwines local and hierarchical information, leading to mutual benefits. 2) **Hierarchical information fusion**: we design a bidirectional pathway to facilitate information fusion among the hierarchies. 3) **S-EdgePool**: we enhance EdgePool into S-EdgePool, allowing an adjustable pooling ratio. 4) **Benchmark and Evaluations**: we establish a new graph theory benchmark to evaluate the ability of models to capture LRIs. In these evaluations, MeGraph exhibits dominant performance. Additionally, MeGraph achieves new SOTA in one task of LRGB and shows better or comparable performance compared with baselines on popular real-world datasets.

Figure 1: Illustration of the graph pooling operation, graph pyramid, and mega graph. **Graph pooling** is a downsampling process comprising SELECT, CONNECT, and REDUCE steps. It begins by selecting subsets for grouping and each subset collapses into a new node in the pooled graph. Next, it forms new edges by merging the original ones, and finally calculates the pooled graphâ€™s features. In this graph, nodes _A B C, D E F_, and _G H I_ are pooled into nodes _1, 2_, and \(3\) respectively, while the edges (_B_, _E_) and (_C_, _D_) are merged into _(1, 2)_. **Graph Pyramid** involves multi-scaled graphs derived from iterative graph pooling, with the height indicating different scales and \(h=1\) symbolizing the original graph. **Mega Graph** is formed by connecting the graph pyramid using inter-graph edges, which are the by-products of graph pooling.

## 2 Notations, Backgrounds and Preliminaries

Let \(=(,)\) be a graph with node set \(\) (of cardinality \(N^{v}\)) and edge set \(\) (of cardinality \(N^{e}\)). The edge set can be represented as \(=\{(s_{k},t_{k})\}_{k=1:N^{e}}\), where \(s_{k}\) and \(t_{k}\) are the indices of the source and target nodes connected by edge \(k\). We define \(^{}\) as features of graph \(\), which is a combination of global (graph-level) features \(^{}\), node features \(^{}\), and edge features \(^{}\). Accordingly, we use \(^{}_{i}\) to represent the features of a specific node \(v_{i}\), and \(^{}_{k}\) denotes the features of a specific edge \((s_{k},t_{k})\). We may abuse the notations by omitting the superscript \(\) when there is no context ambiguity.

### Graph Network (GN) Block

We adopt the Graph Network (GN) block design in accordance with the GN framework . In our notation, a GN block accepts a graph \(\) and features \(=(,,)\) as inputs, and produces new features \(^{}=(^{},^{},^{ })\). A full GN block  includes the following update steps. In each of these steps, \(\) denotes an update function, typically implemented as a neural network:

**Edge features**: \(^{}_{k}=^{e}(_{k},_{s_{k}}, _{t_{k}},), k[1,N^{e}]\).

**Node features**: \(^{}_{i}=^{v}(^{e v}(\{^{}_{k}\}_{k [1,N^{e}],t_{k}=i})\), \(_{i},)\), \( i[1,N^{v}]\), where \(^{e v}\) is an aggregation function taking the features of incoming edges as inputs.

**Global features**: \(^{}=^{u}(^{e u}(^{}),^{v u} (^{}),)\), where \(^{e u}\) and \(^{v u}\) are two global aggregation functions over edge and node features.

Given a fixed graph structure \(\) and the consistent input and output formats outlined above, GN blocks can be seamlessly integrated to construct complex, deep graph networks.

### Graph Pooling

Graph pooling operation downsamples the graph structure and its associated features while ensuring the preservation of structural and semantic information inherent to the graph. Drawing from the SRC framework , we identify graph pooling as a category of functions,PODL, that maps a graph \(=(,)\) with \(N^{v}\) nodes and features \(^{}\) to a reduced graph \(}=(},})\) with \(N^{}\) nodes and new features \(^{}\). Here, \(N^{} N^{v}\) and \((},^{})=(, ^{})\).

The SRC framework deconstructs the POOL operation into SELECT, REDUCE, and CONNECT functions, which encompass most existing graph pooling techniques. We reinterpret these functions in our own notation as follows:

\[(},^{})=(, ^{});\ \ }=(, },^{});\ \ ^{}=(^{},}, ^{}). \]

As shown in Fig. 1, the SELECT establishes \(N^{}\) nodes for the pooled graph, and each node \(\) corresponds to a subset of nodes \(S_{}\) in the input graph. This creates an _undirected_ bipartite graph \(}=(},})\), with \(}=}\) and \((v,)}\) if and only if \(v S_{}\). We refer to this graph \(}\) as the _inter-graph_, a larger graph that links nodes in the input graph \(\) with nodes in the pooled graph \(}\). The SELECT function can be generalized to include inter-graph features \(^{}\). As an example, edge weights can be introduced for some edge \((_{k},_{k})\) in graph \(}\) to gauge the importance of node \(_{k}\) from the input graph contributing to node \(_{k}\) in the pooled graph.

The CONNECT function reconstructs the edge set \(}\) between the nodes in \(}\) of the pooled graph \(}\) based on the original edges in \(\) and the inter-graph edges in \(}\). The REDUCE function calculates the graph features \(^{}\) of graph \(}\) by aggregating input graph features \(^{}\), taking into account both the inter-graph \(}\) and features \(^{}\). In a similar vein to the relationship between graph lifting and coarsening, we define the EXPAND function for graph features, which serves as the inverse of the REDUCE function: \(^{}=(^{},},^{})\).

## 3 Methods

We begin with the introduction of the mega graph (Sec.3.1), which amalgamates the local (intra-edges) and hierarchical (inter-edges) structures of a multi-scale graph hierarchy into a single graph.

Following this, we present the MeGraph model (Sec.3.2), which alternates between the aggregation of local and hierarchical information along the intra- and inter-edges of the mega graph. We then discuss the specific choices made for the core modules of the MeGraph, along with the innovations (Sec.3.3). Finally, we delve into the computational complexity of the MeGraph model (Sec.3.4).

### Connecting Multi-scale Graphs into a Mega Graph

Similar to the concept of an image pyramid , a graph pyramid is constructed by stacking multi-scale graphs, which are obtained through iterative downsampling of the graph using a graph pooling technique. Formally, in alignment with the definition of an image feature pyramid , we define a graph feature pyramid as a set of graphs \(}_{1:h}:=\{_{i}\}_{i=1,,h}\) and their corresponding features \(^{}_{1:h}}:=\{^{}_{i}}\}_{i= 1,,h}\). Here, \(_{1}\) represents the original graph, \(^{}_{1}}\) signifies the initial features, \(h\) stands for the _height_, and \((_{i},^{}_{i}})=( _{i-1},^{}_{i-1}})\) for \(i>1\).

By iteratively applying the POOL function, we can collect the inter-graphs \(}}_{1:h}:=\{}_{i}\}_{i=1,,h-1}\) and their features \(^{}}_{1:h}}:=\{^{}_{i }}\}_{i=1,,h-1}\) (since there are \(h-1\) inter-graphs for \(h\) intra-graphs), where \((}_{i},^{}_{i}})=( _{i},^{}_{i}})\) for \(i<h\). The bipartite inter-graph \(}\) and its features \(^{}}\) essentially depict the relationships between the graphs before and after the pooling process (see Sec. 2.2).

Finally, as illustrated in Fig. 1, we wire the graph pyramid \(}_{1:h}\) using the edges found in the bipartite graphs \(}}_{1:h}\). This results in a mega graph \(=(,)\), where \(=_{i=1}^{h}_{i}\) and \(=_{i=1}^{h}_{i}_{i=1}^{h-1}}_{i}\). The structure of the mega graph would vary as the graph pooling method trains. We denote \(_{}=_{i=1}^{h}_{i}\) as the intra-graph of \(\), and refer to the edges therein as intra-edges. Correspondingly, \(_{}=_{i=1}^{h-1}}_{i}\) is referred to as the inter-graph of \(\), with its corresponding edges termed as inter-edges. The features \(^{}\) of the mega graph \(\) is a combination of intra-graph features \(^{}_{1:h}}\) and inter-graph features \(^{}}_{1:h}}\).

### Mega Graph Message Passing

We introduce the MeGraph architecture, designed to perform local and hierarchical aggregations over the mega graph alternately. As shown in Fig.2, the architecture follows the _encode-process-decode_ design [6; 25] and incorporates GN blocks (refer to Sec. 2.1) as fundamental building blocks.

During the _encode_ stage, initial features are inputted into an intra-graph GN block, which is followed by a sequence of graph pooling operations to construct the mega graph \(\) and its associated features

Figure 2: Illustration of the MeGraph model, where \(n_{-}\) denotes \(n-1\). The blue and green circles represent features of intra- and inter-graphs, respectively. In this figure, the horizontal and vertical directions represent the interaction among the local structure (intra-graph) and graph hierarchy (inter-graph) respectively. The features of intra- and inter-graphs are represented by blue and green circles, respectively. In this figure, the horizontal and vertical directions denote the local structure and graph hierarchy respectively. During the _encode_ stage, the mega graph is constructed using graph pooling. In the _process_ stage, the Mee layer, which features bidirectional pathways across multiple scales, is stacked \(n\) times. In the _decode_ stage, multi-scale features are read out. The golden inter GN blocks form bidirectional pathways across the whole hierarchy.

\((^{0})^{}\). In the _process_ stage, the Mee layer, which performs both local and hierarchical information aggregation within the mega graph, is stacked \(n\) times. The \(i\)-th Mee layer receives \((^{i-1})^{}\) as input and outputs \((^{i})^{}\). Through the stacking of Mee layers, a deeper architecture is created, enabling a more profound fusion of local and hierarchical information. Lastly, in the _decode_ stage, the features \((^{n})^{}\) are transformed into task-specific representations using readout functions.

**Mee Layer.** The Mee layer is designed to aggregate local and hierarchical information within the mega graph. A detailed structure of the Mee layer is depicted in Fig. 3.

For the \(i\)-th Mee layer, we consider inputs denoted by \((^{i-1})^{}=\{(^{i-1})^{}_{1:h}},(^{i-1})^{}_{1:h}}\}\). For simplicity, we omit the superscript and denote the features of intra- and inter-graphs as \(\{^{i-1}_{j}\}_{j=1,,h}:=(^{i-1})^{}_{1:h}}\) and \(\{}^{i-1}_{j}\}_{j=1,,h-1}:=(^{i-1})^{ }_{1:h}}\) respectively.

The first step applies GN blocks on intra-graph edges, performing message passing on the local structure of graphs at each scale: \(^{}_{j}=^{i,j}_{}(_{j}, ^{i-1}_{j})\). Here, \(^{}_{j}\) represents the updated intra-graph \(_{j}\) features.

The second and third steps focus on multi-scale information fusion. The second step applies cross-updates across consecutive heights from \(1\) to \(h\), while the third step reverses the process, forming a bidirectional pathway for the information flow across the hierarchy. The cross-update between consecutive heights \(j\) and \(j+1\) is denoted by a function \((^{}_{j},}^{}_{j},j+1^{} )=(j,_{j},}_{j},j+1)\). The prime notation indicates the updated value, and residual links  are used in practice.

This cross-update can be implemented via an inter-graph convolution with \(^{i,j}_{}\), referred to as _X-Conv_ (detailed in App.C.1). Alternatively, it can be realized using the REDUCE and EXPAND operations of POOL (refer to Sec.2.2) by \(^{}_{j+1}=(}_{j},}^{0}_{j},_{j})\) and \(^{}_{j}=(}_{j},}^{0}_{j},_{j+1})\), where \(}_{j}\) is the \(j\)-th inter-graph. We denote this implementation as _X-Pool_.

The Mee layer outputs features \(\{^{i}_{j}\}_{j=1,,h}\) and \(\{}^{i}_{j}\}_{j=1,,h-1}\). Residual links  can be added from \(^{i-1}_{j}\) to \(^{i}_{j}\) and from \(}^{i-1}_{j}\) to \(}^{i}_{j}\) empirically, creating shortcuts that bypass GN blocks in the Mee layer. It's worth noting that the intra and inter GN blocks can share parameters across all heights \(j\) to accommodate varying heights, or across all Mee layers to handle varying layer numbers.

### Module Choice and Innovation

MeGraph incorporates two fundamental modules: the graph pooling operator and the GN block. This architecture can accommodate any graph pooling method from the POOL function family (refer to Sec. 2.2). Furthermore, the GN block is not strictly confined to the graph convolution layer found in standard GCN, GIN, or GAT.

**Graph Pooling.** There are a number of commonly used graph pooling methods, including DiffPool , TopKPool , EdgePool , etc. We opt for EdgePool due to its simplicity, efficiency,

Figure 3: Illustration of the Mee layer, where \(i_{-}\) denotes \(i-1\) and \(j_{+}\) denotes for \(j+1\). The blue and green circles represent the features of intra- and inter-graphs, respectively. Grey and golden arrows represent the intra and inter GN blocks. The cross-update utilizes inter GN blocks to exchange information between consecutive heights, as elaborated in the main text. The Mee layer first aggregates information locally along inter-graph edges. It then applies cross-updates sequentially from lower to higher levels, accumulating information along the pathway to pass to the higher hierarchy. The process is reversed in the last step.

and ability to naturally preserve the graph's connectivity through edge contraction. However, edge contraction is applied only to the edges in a specific maximal matching of the graph's nodes , thereby setting a lower limit of \(50\%\) to the pooling ratio \(_{v}\). This constraint implies that a minimum of \(_{2}N\) pooling operations is required to reduce a graph of \(N\) nodes to a single node. To address this limitation, we propose the Stridden EdgePool (S-EdgePool), which allows for a variable pooling stride.

The principle behind S-EdgePool involves dynamically tracking the clusters of nodes created by the contraction of selected edges. Similar to EdgePool, edges are processed in descending order based on their scores. When an edge is contracted, if both ends do not belong to the same node cluster, the two clusters containing the endpoints of the edge merge. The current edge can be contracted if the resulting cluster contains no more than \(_{c}\) nodes after this edge's contraction. The iteration stops prematurely once a pooling ratio, \(_{v}\), is achieved. During pooling, each node cluster is pooled as a new node. When \(_{c}=2\), S-EdgePool reverts to the original EdgePool. The algorithm's details and pseudocode are available in App. C.2.

For efficiency, we employ the disjoint-set data structure to dynamically maintain the node clusters, which has a complexity of \(O(E(E))\), where \(E\) is the number of edges and \((E)\) is a function that grows slower than \((E)\). The total time complexity of S-EdgePool is equivalent to EdgePool and is calculated as \(O(ED\)+\(E E)\), where \(D\) is the embedding size, \(O(ED)\) from computing edge scores and \(O(E E)\) from sorting the edges.

**GN block.** The full GN block, introduced in Sec. 2.1, is implemented as a graph full network (GFuN) layer. This layer exhibits a highly configurable within-block structure, enabling it to express a variety of other architectures (see Sec. 4.2 of ), like GCN, GIN, GAT, GatedGCN. Thus, modifying the within-block structure of GFuN is akin to plugging in different GNN cores. Further details can be found in App. C.3.

**Encoder and decoder.** Most preprocessing methods (including positional encodings and graph rewiring), encoding (input embedding) and decoding (readout functions) schemes applicable to GNNs can also be applied to MeGraph. We give implementation details in App. C.4.

### Computational Complexity and Discussion

The overall complexity of the MeGraph model is contingent on the height \(h\), the number of Mee layers \(n\), the chosen modules, and the corresponding hyperparameters. Let \(D\) be the embedding size, \(V\) the number of nodes, and \(E\) the number of edges in the input graph \(\). The time complexity of S-Edgepool is \(O(ED\)+\(E E)\), and that of a GFuN layer is \(O(VD^{2}\)+\(ED)\). Assuming both the pooling ratios of nodes and edges are \(\), the total time complexity to construct the mega graph \(\) becomes \(O((ED+E E)/(1-))\), where \(_{i=0}^{h-1}^{i}<1/(1-)\). Similarly, the total time complexity of an Mee layer is \(O((VD^{2}+ED)/(1-))\). This complexity is equivalent to a typical GNN layer if we consider \(1/(1-)\) as a constant (for instance, it is a constant of 2 when \(=0.5\)).

Theoretically, when using the same number of layers, MeGraph is better at capturing LRIs than standard message-passing GNNs owning to the hierarchical structure (see App. D.1 for details). On the other hand, MeGraph can degenerate into standard message-passing GNNs (see App. D.2 for details), indicating it should not perform worse than them on other tasks.

## 4 Experiments

We conduct extensive experiments to evaluate the MeGraph's ability to capture long-range interactions (LRIs) and its performance in general graph learning tasks.

### Experimental Settings

**Baselines.** We compare MeGraph model to three baselines as follows: 1) MeGraph \(h\)=1 variant does not use the hierarchical structure and falls back to standard GNNs. 2) MeGraph \(n\)=1 variant gives up repeating information exchange over the mega graph. 3) Graph U-Nets  uses a U-shaped design and only traverses the multi-scale graphs once.

Due to page limits, statistics of the datasets are provided in App. B.1, hyper-parameters are reported in Table 9, and the training and implementation details are reported in App. E.

### Perfomance on LRI Tasks

To test MeGraph's ability to capture long-range interactions, we establish a Graph Theory Benchmark, of which four tasks related to shortest path distance, _i.e._, Single Source Shortest Path (SP\({}_{}\)), Single Source Single Destination Shortest Path (SP\({}_{}\)), Graph Diameter (Diameter) and Eccentricity of nodes (ECC); and 1 task related to connected component, _i.e._, Maximum Connected Component of the same color (MCC). To generate diversified undirected and unweighted graphs for each task, we adopt the ten methods used in PNA  and add four new methods: cycle graph, pseudotree, SBM, and geographic threshold graphs. The details of the dataset generation can be found in App. B.2.

As depicted in Table 1, the MeGraph model with \(h\)=5, \(n\)=5 significantly outperforms both the \(h\)=1 and \(n\)=1 baselines in terms of reducing regression error across all tasks. It is worth noting that even

  Category & Model & SP\({}_{}\) & MCC & Diameter & SP\({}_{}\) & ECC & Average \\    & \(n\)=1 & 11.184 & 1.504 & 11.781 & 22.786 & 20.133 & 13.478 \\  & \(n\)=5 & 3.898 & 1.229 & 5.750 & 12.354 & 18.971 & 8.440 \\  & \(n\)=10 & 2.326 & 1.264 & 5.529 & 7.038 & 18.876 & 7.006 \\  MeGraph (\(h\)=5) & \(n\)=1 & 1.758 & 0.907 & 4.591 & 5.554 & 14.030 & 5.368 \\ EdgePool (\(_{c}\)=2) & \(n\)=5 & 0.790 & 0.767 & 2.212 & 0.712 & 6.593 & 2.215 \\  \)=3} & \(_{c}\)=3 & 0.660 & 0.747 & 0.719 & 0.459 & **0.942** & 0.705 \\  & \(_{c}\)=0.3 & 2.225 & 0.778 & 1.061 & 3.591 & 2.009 & 1.933 \\ S-EdgePool & \(_{c}\)=0.3, \(_{c}\)=4 & **0.615** & **0.702** & **0.651** & **0.434** & 0.975 & **0.675** \\ Variants & \(_{c}\)=0.5, \(_{c}\)=4 & 1.075 & 0.769 & 0.945 & 1.204 & 1.992 & 1.197 \\ (\(h\)=5, \(n\)=5) & \(_{c}\)=0.3, \(_{c}\)=4 (X-Pool) & 0.935 & 0.751 & 0.864 & 1.462 & 2.003 & 1.203 \\  & \(_{c}\)=0.3, \(_{c}\)=4 (w/o pw) & 0.632 & 0.730 & 0.864 & 0.765 & 2.334 & 1.065 \\  Graph-UNets & \(h\)=5,\(n\)=9,\(_{c}\)=0.3,\(_{c}\)=4 & 1.118 & 1.008 & 2.031 & 1.166 & 2.584 & 1.581 \\   

Table 1: Results on Graph Theory Benchmark (medium size). For each task, we report the MSE regression loss on test set, averaged over different graph generation methods. Darker blue cells denote better performance and the bold denotes the best one. We provide detailed results on each type of graphs in App. F.7.

   Category & Model & SP\({}_{}\) & MCC & Diameter & SP\({}_{}\) & ECC & Average \\   Baseline & h=1,n=5 & 328.014 & 39.4772 & 189.577 & 324.033 & 219.746 & 220.169 \\  MeGraph & h=5,n=5,\(_{c}\)=0.3,\(_{c}\)=4 & **23.8963** & **16.8321** & **19.2185** & **14.9676** & **44.9234** & **23.9676** \\  Graph-UNets & h=5,n=9,\(_{c}\)=0.3,\(_{c}\)=4 & 101.009 & 30.3711 & 39.8708 & 100.070 & 75.1185 & 69.2879 \\   

Table 2: Results on Graph Theory Benchmark (large size).

   Methods & Use PE & Peptide-func \(\) & Peptide-struct \(\) \\  GCN  & & 59.30 \(\)0.23 & 0.3496 \(\)0.0013 \\ GINE  & & 55.43 \(\)0.78 & 0.3547 \(\)0.0045 \\ GatedGCN  & & 58.64 \(\)0.77 & 0.3420 \(\)0.0013 \\ GatedGCN+RWSE  & âœ“ & 60.69 \(\)0.35 & 0.3357 \(\)0.0006 \\ GatedGCN+RWSE+VN  & âœ“ & 66.85 \(\)0.62 & 0.2529 \(\)0.0009 \\ Transformer+LapPE  & âœ“ & 63.26 \(\)1.26 & 0.2529 \(\)0.0016 \\ SAN+LapPE  & âœ“ & 63.84 \(\)1.21 & 0.2683 \(\)0.0043 \\ SAN+RWSE  & âœ“ & 64.39 \(\)0.75 & 0.2545 \(\)0.0012 \\ GPS  & âœ“ & 65.35 \(\)0.41 & 0.2500 \(\)0.0005 \\ MGT+WavePE  & âœ“ & 68.17 \(\)0.64 & **0.2453**\(\)0.0025 \\ GNN-AK+  & & 64.80 \(\)0.89 & 0.2736 \(\)0.0007 \\ SUN  & & 67.30 \(\)0.78 & 0.2498 \(\)0.0008 \\ GraphTrans+PE  & âœ“ & 63.13 \(\)0.39 & 0.2777 \(\)0.0025 \\ GINE+PE  & âœ“ & 64.05 \(\)0.77 & 0.2780 \(\)0.0021 \\ GINE-MLP-Mixer+PE  & âœ“ & 69.21 \(\)0.54 & 0.2485 \(\)0.0004 \\  MeGraph (h=9,n=1) & & 67.52 \(\)0.78 & 0.2557 \(\)0.0011 \\ MeGraph (h=9,n=4) & & **69.45 \(\)0.77** & 0.2507 \(\)0.0009 \\   

Table 3: Results on LRGB , numbers are taken from corresponding papers. All methods use around 500K parameters for a fair comparison. Message-passing-based models have 5 layers, while the Transformer-based models have 4 layers . PE indicates positional encoding, which makes it easier to distinguish different nodes.

the \(h\)=5, \(n\)=1 baseline outperforms the \(h\)=1, \(n\)=10 baseline, indicating that adopting a multi-scale graph hierarchy is crucial in these tasks. The improvement is also substantial when compared with our reproduced Graph-UNets using S-EdgePool ([MeGraph] 0.675 vs. [Graph UNets] 1.581). The improvements are more significant when the size of graphs becomes larger (as shown in Table 2). These results collectively demonstrate the superior ability of MeGraph to capture LRIs.

Furthermore, we evaluated MeGraph model and compared it with other recent methods on the Long Range Graph Benchmark (LRGB)  that contains real-world tasks that require capturing LRIs. As depicted in Table 3, the \(h\)=9, \(n\)=4 variant of MeGraph achieves superior results on the _Peptide-func_ task, and comparable performance on the _Peptide-struct_ task, relative to state-of-the-art models. It is worth noting that the \(n=1\) variant already surpasses other methods except the recent MLP-Mixer  in the _Peptide-func_ task.

### Generality of MeGraph

To verify the generality of MeGraph model, we evaluate MeGraph on widely adopted GNN Benchmark , Open Graph Benchmark  and TU Dataset . Results on TU Datasets are available in App. F.3. In addition to the standard model that shares hyper-parameters in similar tasks, we also report MeGraph\({}_{}\) with specifically tuned hyper-parameters for each task.

**GNN Benchmark**. We experiment on chemical data (ZINC and AQSOL), image data (MNIST and CIFAR10) and social network data (PATTERN and CLUSTER). As shown in Table 4, MeGraph outperforms the three baselines by a large margin, indicating the effectiveness of repeating both the local and hierarchical information aggregation.

**Open Graph Benchmark (OGB)**. We choose 10 datasets related to molecular graphs from the graph prediction tasks of OGB. The task of all datasets is to predict some properties of molecule graphs

   Model & ZINC \(\) & AQSOL \(\) & MNIST \(\) & CIFAR10 \(\) & PATTERN \(\) & CLUSTER \(\) \\  GCN\({}^{}\) & 0.416 \(\)0.006 & 1.372 \(\)0.002 & 90.120 \(\)0.145 & 54.142 \(\)0.394 & 85.498 \(\)0.015 & 47.828 \(\)1.510 \\ GIN\({}^{}\) & 0.387 \(\)0.015 & 1.894 \(\)0.024 & **96.485 \(\)0.026** & 52.555 \(\)1.527 & 85.590 \(\)0.011 & 58.384 \(\)0.236 \\ GAT\({}^{}\) & 0.475 \(\)0.007 & 1.441 \(\)0.023 & 95.535 \(\)0.205 & 64.223 \(\)0.455 & 75.824 \(\)1.823 & 57.732 \(\)0.323 \\ GatedGCN\({}^{}\) & 0.435 \(\)0.011 & 1.352 \(\)0.034 & 97.340 \(\)0.143 & 67.312 \(\)0.311 & 84.480 \(\)0.122 & 60.404 \(\)0.419 \\  Graph-UNets & 0.332 \(\)0.010 & 1.063 \(\)0.018 & 97.130 \(\)0.227 & 68.567 \(\)0.339 & 86.257 \(\)0.078 & 50.371 \(\)0.243 \\ MeGraph (\(h\)=1) & 0.323 \(\)0.002 & 1.075 \(\)0.007 & 97.570 \(\)0.168 & 69.890 \(\)0.209 & 84.845 \(\)0.021 & 58.178 \(\)0.079 \\ MeGraph (\(n\)=1) & 0.310 \(\)0.005 & 1.038 \(\)0.018 & 96.867 \(\)0.167 & 68.522 \(\)0.239 & 85.507 \(\)0.402 & 50.396 \(\)0.082 \\ MeGraph & 0.260 \(\)0.005 & **1.002** \(\)0.021 & **97.860** \(\)0.098 & **69.925** \(\)0.631 & 86.507 \(\)0.067 & 68.603 \(\)0.101 \\ MeGraph\({}_{}\) & **0.202** \(\)0.007 & **1.002** \(\)0.021 & **97.860** \(\)0.098 & **69.925** \(\)0.631 & **86.732** \(\)0.023 & **68.610** \(\)0.164 \\   

Table 4: Results on GNN benchmark. Regression tasks are colored with _blue_. \(\) indicates that smaller numbers are better. Classification tasks are colored with _green_. \(\) indicates that larger numbers are better. Darker colors indicate better performance. \(\) denotes the results are reported in .

   Model & molliv \(\) & molliv \(\) & mollabc \(\) & mollbbbp \(\) & mollcintox \(\) & mollsider \(\) \\   GCN\({}^{}\) & 76.06 \(\)0.97 & 79.15 \(\)1.44 & 68.87 \(\)1.51 & 91.30 \(\)1.73 & 59.60 \(\)1.77 \\ GIN\({}^{}\) & 75.58 \(\)1.40 & 72.97 \(\)4.00 & 68.17 \(\)1.48 & 88.14 \(\)2.51 & 57.60 \(\)1.40 \\  Graph-UNets & **79.48**\(\)1.06 & 81.09 \(\)1.66 & **71.10**\(\)0.52 & 91.67 \(\)1.69 & 59.38 \(\)0.63 \\ MeGraph (\(h\)=1) & 78.54 \(\)1.14 & 71.77 \(\)2.15 & 67.56 \(\)1.11 & 89.77 \(\)3.48 & 58.28 \(\)0.51 \\ MeGraph (\(n\)=1) & 78.56 \(\)1.02 & 79.72 \(\)1.24 & 67.34 \(\)0.98 & 91.07 \(\)2.21 & 58.08 \(\)0.59 \\ MeGraph & 77.20 \(\)0.88 & 78.52 \(\)2.51 & 69.57 \(\)2.33 & 92.04 \(\)2.19 & 59.01 \(\)1.45 \\ MeGraph\({}_{}\) & **79.20**\(\)1.80 & **83.52**\(\)0.47 & 69.57 \(\)2.33 & **92.06**\(\)1.32 & **63.43**\(\)1.10 \\   Model & moltox21 \(\) & moltoxcast \(\) & molesol \(\) & molfreesolv \(\) & mollipo \(\) \\   GCN\({}^{}\) & 75.29 \(\)0.69 & 63.54 \(\)0.42 & 1.114 \(\)0.03 & 2.640 \(\)0.23 & 0.797 \(\)0.02 \\ GIN\({}^{}\) & 74.91 \(\)0.51 & 63.41 \(\)0.74 & 1.173 \(\)0.05 & 2.755 \(\)0.34 & 0.757 \(\)0.01 \\  Graph-UNets & **77.85**\(\)0.81 & 66.49 \(\)0.45 & 1.002 \(\)0.04 & 1.885 \(\)0.07 & 0.716 \(\)0.01 \\ MeGraph (\(h\)=1) & 75.89 \(\)0.45 & 64.49 \(\)0.46 & 1.079 \(\)0.02 & 2.017 \(\)0.08 & 0.768 \(\)0.00 \\ MeGraph (\(n\)=1) & 77.01 \(\)0.93 & 66.89 \(\)1.21 & 0.896 \(\)0.04 & 1.892 \(\)0.06 & 0.730 \(\)0.01 \\ MeGraph & **78.11**\(\)0.47 & 67.67 \(\)0.53 & 0.886 \(\)0.02 & **1.876**\(\)0.05 & 0.726 \(\)0.00 \\ MeGraph\({}_{}\) & **78.11**\(\)0.47 & **67.90**\(\)0.19 & **0.867**\(\)0.02 & **1.876**\(\)0.05 & **0.688**\(\)0.01 \\   

Table 5: Results on OGB-G. \(\) indicates that the results are reported in .

based on their chemical structures. As shown in Table 5, MeGraph outperforms the \(h\)=1 baseline by a large margin, suggesting that building a graph hierarchy is also essential in molecule graphs. The performance of MeGraph, \(n=1\) baseline, and the reproduced Graph U-Nets are comparable. This observation may be because the information obtained from multi-hop neighbors offers only marginal improvements compared to the information aggregated hierarchically.

### Ablation Study

**Hierarchy vs. Locality.** We study the impact of the height \(h\) and the number of Mee layers \(n\) on four synthetic datasets introduced in , which are BAShape, BACommunity, TreeCycle, and TreeGrid. Each dataset contains one graph formed by attaching multiple motifs to a base graph. The motif can be a 'house'-shaped network (BAShape, BACommunity), six-node cycle (TreeCycle), or 3-by-3 grid (TreeGrid). The task is to identify the nodes of the motifs in the fused graph.

As shown in Fig. 4, we can observe clear improvements in performance as the height \(h\) and the number of layers \(n\) increase when using EdgePool. Although increasing height shows better improvements, both hierarchy and locality are indispensable in TreeCycle and TreeGrid tasks. In App. F.5, we show that the conclusion also holds on BAShape and BACommunity datasets, except that the accuracy is already saturated with height \(h=2\). The significance of integrating locality with hierarchy is also demonstrated in the CLUSTER task, as presented in Table 4. Here, MeGraph reaches 68.6% accuracy, which is markedly higher than the 50.4% accuracy achieved by both the \(n\)=1 baseline and Graph-UNets.

**Varying the Pooling Method.** We varied the graph pooling with two non-learnable pooling methods, Louvain  and Random. On the TreeCycle and TreeGrid datasets, as depicted in Fig. 4, Louvain achieves comparable accuracy to EdgePool, while MeGraph using random pooling matches the performance of the \(h=1\) variant of MeGraph. These observations indicate that: 1) a well-structured hierarchy crafted by suitable graph pooling methods enhances MeGraph's ability to harness hierarchical information; and 2) despite the disruption from a randomly constructed hierarchy, the MeGraph model effectively taps into the local structure of the original graph (also discussed in App. D.2).

We further studied the impact of the node pooling ratio \(_{v}\) and the maximum cluster size \(_{c}\) in S-EdgePool by perturbing these parameters. As indicated in Table 1, the best variant (\(_{v}\)=0.3,\(_{c}\)=4) achieved a regression error about 3x smaller (0.675) compared to the original EdgePool (\(_{c}\)=2 with an error of 2.215). This suggests the benefit of having a flexible pooling stride. Moreover, the mega graph produced by S-EdgePool can vary significantly with different parameters. In App. F.2, we visualized the resulting graph hierarchy to illustrate the difference between different S-EdgePool variants. However, irrespective of the parameter set used, MeGraph consistently outperforms \(h=1\) baselines. This suggests that MeGraph exhibits robustness against different architectures of the mega graph.

**Varying GN Block.** We varied the aggregation function of the GN block as attention (w/ ATT) and gated function (w/ GATE). We observe similar results as in Sec. 4.2 and 4.3, verifying the robustness of MeGraph towards different types of GN blocks. Detailed results can be found in Tables 16, 17 and 18 in App. F.6.

Figure 4: Node classification accuracy (averaged over 10 random repetitions) for MeGraph on TreeCycle (left) and TreeGrid (right) datasets by varying the number of Mee layers \(n\), the height \(h\), and the graph pooling methods (EdgePool, Louvain, Random). Clear gaps can be observed among heights 1, 2, and 3 for EdgePool and Louvain  methods, while the accuracy is almost invariant among different heights for randomized pooling.

**Changing cross update function (X-UPD).** The unpool operation is frequently used by other hierarchical architectures that build upon graph pooling. As illustrated in Table 1, we substituted the _X-Conv_ implementation with the _X-Pool_ implementation of X-UPD, which resulted in a performance decline from 0.675 to 1.203 (smaller is better). This finding suggests that other hierarchical GNNs might also benefit from replacing the unpool operation with a convolution over the inter-graph edges.

**Disabling bidirectional pathway.** We verify the effectiveness of the bidirectional pathway design by replacing steps 2 and 3 of the Mee layer as a standard message-passing along the inter-graph edges (denoted w/o pw). As shown in Table 1, the performance degrades from 0.675 to 1.065 (smaller is better), which indicates the contribution of the bidirectional pathway.

## 5 Related Work

**Long-Range Interactions (LRIs).** Various methods have been proposed to address the issue of LRIs, including making the GNNs deeper . Another way is to utilize attention and gating mechanism, including GAT , jumping knowledge (JK) network , incorporating Transformer structures  and MLP-Mixer . Another line of research focuses on multi-scale graph hierarchy using graph pooling methods , or learning representation based on subgraphs . Recently, Long Range Graph Benchmark  has been proposed to better evaluate models' ability to capture LRIs.

**Feature Pyramids and Multi-Scale Feature Fusion.** Multi-scale feature fusion methods on image feature pyramids have been widely studied in computer vision literature, including the U-Net , FPN , UNet++ , and some recent approaches . HRNet  is a similar method compared to MeGraph. HRNet alternates between multi-resolution convolutions and multi-resolution fusion by stridden convolutions. However, the above methods are developed for image data. The key difference compared to these approaches is that the multi-scale feature fusion in MeGraph is along the inter-graph edges, which is not as well structured as the pooling operation in image data. For graph networks, the GraphFPN  builds a graph feature pyramid according to the image feature pyramid and superpixel hierarchy. It applies GNN layers on the hierarchical graph to exchange information within the graph pyramid. Existing works  have also explored similar ideas in graph-structured data. Our approach aligns with the broader concept of multi-scale information fusion, but it is the first method that builds a mega graph using graph pooling operations and alternates local and hierarchical information aggregation.

**Graph Pooling Methods.** Graph pooling is an important part of hierarchical graph representation learning. There have been some traditional graph pooling methods like METIS  in early literature. Recently, many learning-based graph pooling methods have been proposed, including the DiffPool , TopKPool , SAG pool , EdgePool , MinCutPool , Structpool , and MEWISPool , etc. In this work, we utilize S-EdgePool improved from EdgePool to build the mega graph, while this module can be substituted with any of the above-mentioned pooling methods.

**Graph Neural Network (GNN) Layers.** The GNN layer is the core module of graph representation learning models. Typical GNNs include the GCN , GraphSage , GAT , GIN , PNA . MeGraph adopts the full GN block  by removing part of links in the module as an elementary block, and similarly this can be replaced by any one of the popular GNN blocks.

## 6 Limitations and Future Work

The MeGraph model suffers from some limitations. The introduced mega graph architecture inevitably increases both the number of trainable parameters and tuneable hyper-parameters. The flexible choices of many modules in MeGraph post burdens on tuning the architecture on specific datasets. For future research, MeGraph encourages new graph pooling methods to yield edge features in addition to node features, when mapping the input graph to the pooled graph. It is also possible to improve MeGraph using adaptive computational steps . Another direction is to apply some expressive but computationally expensive models like Transformers  and Neural Logic Machines  (only) over the pooled small-sized graphs.