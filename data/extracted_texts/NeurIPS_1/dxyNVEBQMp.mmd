# Introducing Spectral Attention for

Long-Range Dependency in Time Series Forecasting

Bong Gyun Kang\({}^{1}\)

Dongjun Lee\({}^{1}\)

HyunGi Kim\({}^{2}\)

DoHyun Chung\({}^{3}\)

Sungroh Yoon\({}^{1,2}\)

\({}^{1}\) Interdisciplinary Program in Artificial Intelligence, Seoul National University

\({}^{2}\) Department of Electrical and Computer Engineering, Seoul National University

\({}^{3}\) Department of Future Automotive Mobility, Seoul National University

Denotes equal contribution. {luckypanda, elite1717}@snu.ac.krCorresponding author. sryoon@snu.ac.kr

###### Abstract

Sequence modeling faces challenges in capturing long-range dependencies across diverse tasks. Recent linear and transformer-based forecasters have shown superior performance in time series forecasting. However, they are constrained by their inherent inability to effectively address long-range dependencies in time series data, primarily due to using fixed-size inputs for prediction. Furthermore, they typically sacrifice essential temporal correlation among consecutive training samples by shuffling them into mini-batches. To overcome these limitations, we introduce a fast and effective Spectral Attention mechanism, which preserves temporal correlations among samples and facilitates the handling of long-range information while maintaining the base model structure. Spectral Attention preserves long-period trends through a low-pass filter and facilitates gradient to flow between samples. Spectral Attention can be seamlessly integrated into most sequence models, allowing models with fixed-sized look-back windows to capture long-range dependencies over thousands of steps. Through extensive experiments on 11 real-world time series datasets using 7 recent forecasting models, we consistently demonstrate the efficacy of our Spectral Attention mechanism, achieving state-of-the-art results.

## 1 Introduction

Time series forecasting (TSF) stands as a core task in machine learning, ubiquitous in our lives through applications such as weather forecasting, traffic flow estimation, and financial investment. Over time, TSF techniques have evolved from statistical  and machine learning approaches  to deep learning models like Recurrent Neural Networks  and Convolution based Networks . Following the success of Transformers  in various domains, Transformer-based models have also become mainstream in the time series domain . Recently, methodologies based on Multi-layer Perceptron have received renewed attention . However, despite the advancements, long-term dependency modeling in TSF remains challenging .

Unlike image models, where data are randomly sampled from the image distribution and are thus independent of each other , TSF models sample data from the continuous signal, dependenton the time variable \(t\) as shown in Figure 1a. This leads to a high level of correlation between each training sample, which consists of a fixed-sized look-back window before \(t\) (as input) and the subsequent prediction sequence after \(t\) (as label). Therefore, the conventional approach of shuffling the consecutive samples into mini-batches deprives the model of utilizing the crucial inherent temporal correlation between the samples. This restricts the model's consideration to only a fixed-size look-back window for sequence modeling, limiting the ability to address long-range dependencies (Figure 1b).

Recent studies pointed out that simply increasing the look-back window leads to substantially detrimental effects such as increased model size and longer training and inference times . This is particularly challenging for transformer forecasters, which exhibit quadratic time/memory complexity , and even for efficient models using techniques like Sparse Attention, which have O(nlogn) complexity . Furthermore, if the commonly used look-back window of 96 is extended fivefold, the model can only utilize time steps of less than 500, making it difficult to consider long-range dependencies spanning thousands or the entire dataset. Also, increasing the look-back window may not be beneficial, often leading to decreased performance , highlighting the fact that current models are not sufficient in capturing long-range dependencies.

To address this limitation, we propose Spectral Attention, which can be applied to most TSF models and enables the model to utilize long-range temporal correlations in sequentially obtained training data. With the stream of consecutive training samples (Figure 1c), Spectral Attention stores an exponential moving average (EMA) of the activations with various smoothing factors at each time step. This serves as a low-pass filter, inherently embedding long-range information over a thousand steps. Attention is then applied to the stored EMA activations of various smoothing factors (low-pass filters with different frequencies), enabling the model to learn which periodic trends to consider when predicting the future, thereby enhancing its performance. Spectral Attention is even applicable to models such as iTransformer , which do not preserve the temporal order of time series data internally.

We further extend Spectral Attention, where computations depend on the activation of the previous timesteps, to Batched Spectral Attention, enabling parallel training across multiple timesteps. This extension makes Spectral Attention faster and more practical and allows for the direct utilization of temporal relationships among consecutive data samples within a mini-batch in the training base model. In Batched Spectral Attention, the EMA is unfolded over time to perform Spectral Attention simultaneously across multiple time steps. This unfolding allows gradients at time \(t\) to propagate through the Spectral Attention module to the previous time step within the mini-batch, achieving effects similar to Backpropagation Through Time (BPTT)  and extends the model's effective input window.

**Our approach** preserves the base TSF model architecture and learning objective while enabling the model to leverage long-term trends spanning thousands of steps. By effectively utilizing the temporal correlation of training samples, our method allows gradients to flow back in time beyond the look-back window, extending to the entire mini-batch. Also, our method requires little additional training time and memory. We conducted experiments on 7 recent TSF models and 11 real-world datasets and demonstrated consistent performance enhancement in all architecture, achieving state-of-the-art results. We summarize the main contributions as follows:

Figure 1: (a) Training data are sampled for each time step from continuous sequences, exhibiting high temporal correlations. (b) Conventional approaches simply ignore this temporal information with a shuffled batch. (c) We address the temporal correlation between the samples for the first time, enabling the model to consider long-range dependencies that surpass the look-back window.

* We propose Spectral Attention, which addresses long-range dependencies spanning thousands of steps through frequency filtering and attention mechanisms, leveraging the temporal correlation among the consecutive samples.
* We propose Batched Spectral Attention, which enables parallel training across multiple timesteps and expends the effective input window, allowing the gradient to flow through time within the mini-batch.
* Batched Spectral Attention is applicable to most existing TSF models and practical in real-world scenarios with minimal additional memory and comparable training time. Also, it allows finetuning with a trained TSF model.
* Batched Spectral Attention demonstrates consistent model-agnostic performance improvements, particularly showcasing superior performance on datasets with significant long-term trend variations.

## 2 Related Works

**Classic TSF models.** Statistical TSF methods, such as ARIMA , Holt-Winters , and Gaussian Process , assume that temporal variations adhere to predefined patterns. However, their practical applicability is largely limited by the complex nature of real-world data. Machine learning approaches, such as Support Vector Machines  and Random Forests  have proven to be effective even compared to early artificial neural networks [13; 17; 40]. Convolutional network-based methods leverage convolution kernels to capture temporal variations sliding along the temporal dimension [12; 45]. Recurrent neural network (RNN) grasp changes over time via state transitions across different time steps. [25; 41]. However, RNN-based models exhibit limitations in modeling long-range dependencies due to challenges such as vanishing gradients and error accumulation [30; 43]. Recently, transformer and linear-based models have emerged as alternatives, demonstrating superior performance compared to recurrent models [46; 53].

**Transformer and Linear based models.** Transformer-based models  address temporal relationships between time points using the attention. LogSparseTransformer , Reformer , and Informer  have been proposed to make the Transformer architecture efficient, addressing the quadratic time complexity. The Autoformer  incorporates series decomposition as an inner block of Transformer and aggregates similar sub-series by utilizing the Auto-Correlation mechanism. PatchTST  introduces patching, a channel-independent approach that processes each variable separately and focuses on cross-time attention. Crossformer  utilizes a channel-dependent approach to learn cross-variate dependencies. This is achieved through the use of cross-time and cross-dimension attention. iTransformer  applies attention and FFN in an inverted way, where attention handles correlations between channels and FFN handles the temporal information. Recently, to address Transformers' potential difficulties in capturing long-range dependencies , methodologies based on the linear model and Multi-Layer Perceptron (MLP) structures have emerged. DLinear  utilizes the decomposition method introduced in Autoformer and predicts by adding the output of two linear layers for each seasonal and trend element. TiDE  proposes an architecture based on MLP residual blocks that combines information from dynamic and static covariates with a look-back window for encoding, followed by decoding. TSMixer  performs forecasting by repeatedly mixing time and features using an MLP. RLinear  comprises of a single linear layer with RevIN  for normalization and de-normalization.

**Frequency-utilizing models.** Using the frequency domain for TSF is a well-established approach [3; 42; 19]. Conventional approaches leverage frequency information during the preprocessing stage  or decompose time series based on frequency filtering . In deep TSF models, research has also explored architectural advancements that are aware of the frequency information. SAAM , which is applicable to RNNs, performs FFT and autocorrelation on the input signal. WaveFroM  uses discrete wavelet transform (DWT) to project time series into wavelet domains of various scales and performs forecasting through graph convolution and dilated convolution. FEDformer  adopts a mixture-of-experts strategy to refine the decomposition of seasonal and trend components and introduce sparse attention mechanisms in the frequency domain. TimesNet  transforms 1D time series into 2D tensors utilizing multi-periodicity by identifying dominant frequencies through Fourier Transform, modeling temporal variations effectively. FreTS  leverages frequency-domain MLPs to achieve global signal analysis and compact energy representation, addressing the limitations of point-wise mappings and information bottlenecks in conventional MLP-based methods. FITS  employs interpolation within the complex frequency domain to construct a concise and robust model. While these models leverage frequency information, they are limited in modeling long-range dependencies, as the frequency conversion is confined to the look-back window. On the other hand, our Spectral Attention is the first to achieve long-range dependency modeling beyond the look-back window by incorporating consecutive data streams during model training.

## 3 Methods

Problem Statement.In multivariate time series forecasting, time series data is given \(_{T}:\{x_{1},...,x_{T}\}^{T N}\) at time \(T\) with \(N\) variates. Our goal is, at arbitrary future time \(t>T\), to predict future \(S\) time steps \(Y_{t}=\{x_{t+1},...,x_{t+S}\}^{S N}\). To achieve this goal, TSF model \(f\) utilizes length \(L\) look-back window as input \(X_{t}=\{x_{t-L+1},...,x_{t}\}^{L N}\) making prediction \(P_{t}=f(X_{t})\), \(P^{S N}\).

Model is trained with the training dataset \(D_{T}=\{(X_{t},Y_{t})|L t T-S\}\). While conventional methods typically randomly sample each \(X,Y\) from \(D_{T}\) to constitute the mini-batch, we utilize sequential sampling to incorporate temporal correlations between samples into the learning process.

### Spectral Attention

Spectral Attention (\(SA\)) can be applied to every TSF model that satisfies the aforementioned problem statement. This base TSF model is represented by \(P=f(X)\), and \(SA\) can be applied to arbitrary activation \(F\) within the model. The base model can be reformulated as \(P=f_{2}(F,E)\) and \(F,E=f_{1}(X)\). \(F,E\) are intermediate state and SA module takes an arbitrary subset \(F\) as input and transforms it into \(F^{}\) of the same size; \(F^{}=SA(F)\), \(P^{}=f_{2}(F^{},E)\). The resulting SA plugged model \(f_{SA}\) is depicted in Figure 2a.

With \(X_{t}\) as the base model input, \(SA\) takes \(D\)-dimensional feature vector \(F_{t}^{D}\) as input. \(SA\) updates the exponential moving average (EMA) \(M_{t}^{K D}\) of \(F_{t}\) in its internal memory with the \(K\)_smoothing factors_\(\{_{1},...,_{K}\}^{K}\) (\(_{1}<<_{K}\)) as shown in Figure 2b.

\[M_{t+1}^{k,i}=_{k} M_{t}^{k,i}+(1-_{k}) F_{t}^{i} \]

EMA retains the trend of features over long-range time periods based on the smoothing factor. It operates as a low-pass filter, with the -3db (half) cut-off frequency of \(freq_{cut}=cos^{-1}[1-}{2}]\), effectively preserving the trend over 6,000 period with \(=0.999\).

To represent high-frequency patterns contrasting with the low-pass filtered long-range pattern \(M_{t}\), we generated \(H_{t}^{K D}\) by subtracting \(M_{t}\) from \(F_{t}\).

\[H_{t}^{k,i}=F_{t}^{i}-M_{t}^{K-k-1,i} \]

Figure 2: (a) Plug-in Spectral Attention (SA) module takes a subset of intermediate feature \(F\) and returns \(F^{}\) with long-range information beyond the look-back window. The model is trained end-to-end, and gradients flow through the SA module. (b) To capture the long-range dependency, SA stores momentums of feature \(F\) generated from the sequential inputs. Multiple momentum parameters \(_{i}\) capture dependencies across various ranges. (c) SA module computes \(F^{}\) by attending multiple low-frequency (\(M^{_{i}}\)) and high-frequency (\(F-M^{_{i}}\)) components and feature (\(F\)) using learnable Spectral Attention Matrix (SA-Matrix)

\(SA\) contains learnable parameters: _sa-matrix_\(^{(2K+1) D}\), which learns what frequency the model should attend to for each feature. \(2 H_{t}\), \(F_{t}\), \(2 M_{t}\) are concatenated on dimension 0, resulting in \(^{(2K+1) D}\), which is then weighted summed with _sa-matrix_ on dimension 0, generating output \(F^{}_{t}\) (Figure 2c).

\[F^{}_{t}=sum(softmax(,dim\ 0) concat((2 H_{t},F_{t},2  M_{t}),dim\ 0),dim\ 0) \]

The _sa-matrix_ is initialized so that \(softmax()\) resembles a Gaussian distribution on axis 0. This results in symmetric value on axis 0 (_sa-matrix\({}^{K+1-i}\) = sa-matrix\({}^{K+1+i}\)_) and makes \(SA\) an identity function on initialization (\( H^{k}+M^{K-k-1}=F\)).

\[F=SA_{init}(F) \]

\(SA\) allows the model to attend to multiple frequencies of its feature signal, enabling it to focus on either long-range dependencies or high-frequency patterns as needed and shift the feature \(F\) distribution on the frequency domain. By initializing \(SA\) as an identity function, the model can be fine-tuned with the already trained base model, allowing efficient implementation.

### Batched Spectral Attention

Batched Spectral Attention (\(BSA\)) enables batch training over multiple time steps. The main concept involves unfolding EMA, which facilitates gradients to flow across consecutive samples in a mini-batch, akin to BPTT. This enables efficient parallel training and promotes the model to extract long-range information beneficial for future prediction, extending the effective look-back window. The overall flow of \(BSA\) is depicted in Figure 3.

With mini-batch of size \(B\), consecutive samples \(X_{[t,t+B-1]}=\{X_{t},...X_{t+B-1}\}^{B S N}\) are given as input. Following aforementioned \(SA\) setting, \(BSA\) takes \(F_{[t,t+B-1]}=\{F_{t},...F_{t+B-1}\}^{B D}\) as input. In the next step, \(BSA\) utilizes \(F_{[t,t+B-1]}\) and the stored \(M_{t}^{K D}\) to generate \(M_{t+b}(0 b B)\) by unfolding the Equation 1.

\[M^{k,i}_{t+b}=^{b}_{k} M^{k,i}_{t}+(1-_{k})^{b-1}_{k}  F^{i}_{t}++(1-_{k}) F^{i}_{t+b-1} \]

This equation can be transformed to calculate \(M_{[t,t+B]}^{(B+1) K D}\) in parallel as follows.

\[M^{:,k,i}_{[t,t+B]}=lowertriangle(A^{k})((M^{k,i}_{t},F^{:,i}_{[t,t+B-1]}),dim\ 0) \]

\[A^{K(B+1)(B+1)},\ A^{k,p,q}=(1-_{k})^{I\{q>0\} }^{p-q}_{k} \]

\(A\) refers to unfolding matrix and \(I\) refers to indicator function. \(M_{t+B}\) is stored in \(BSA\) for the next mini-batch input. \(F^{}_{[t,t+B-1]}\) is computed in parallel, similar to Equation 2 and 3, using \(F_{[t,t+B-1]}\), \(B_{[t,t+B-1]}\), and _sa-matrix\(^{(2K+1) D}\)_. The \(lowertriangle\) function prevents gradients from the past timestep from flowing into future models, aligning with the characteristics of time-series data.

Figure 3: BSA module takes a sequentially-sampled mini batch \(\{X_{t},...X_{t+B-1}\}\) and computes the corresponding EMA momentums \(\{M_{t},...M_{t+B-1}\}\) over time. This is done via single matrix multiplication enabling parallelization. We made the momentum parameter \(_{i}\) learnable, allowing the model to directly learn the periodicity of the information essential for the future prediction.

At the beginning of each training epoch, \(M_{0}\) is initialized to \(F_{0}\) for all \(\), enhancing stability for subsequent EMA accumulation. Since \(SA\) is proposed to address long-range dependencies in training, it lacks sufficient information in the early stages when not enough steps have been seen. Therefore, for the stability of training, we linearly warm up the learning rate for the first \(1/(1-max())\) timesteps at the beginning of each training epoch. The overall training of both the base model and \(BSA\) is conducted according to Algorithm 1.

In SA, the _smoothing factors_\(\{_{1},...,_{K}\}\)\(^{K}\) were given as scalar values, whereas in \(BSA\), they are expressed by learnable parameters. This is because \(BSA\) can utilize additional past information in training beyond the look-back window by incorporating a batch-sized time window, allowing it to determine the extent of long-range dependency required for training. To keep the smoothing factors between 0 and 1, we initialized learnable parameters by applying an inverse sigmoid to the initial smoothing factors and then applied a sigmoid function in training.

So far, we assume the feature \(F\) from the base model as a vector. However, the output of the intermediate layers of the model is often represented as a tensor with two or more dimensions. In real practice, we use additional channel dimensions in \(BSA\) to process the activation tensor, which acts as applying multiple \(BSA\) modules simultaneously.

### Consecutive dataset split

In the TSF scenario, the entire time series data \(\{x_{1},...,x_{T_{end}}\}\) is divided into train, validation, and test sets in chronological order. Let \(T_{tr}\) and \(T_{val}\) denote the last time in the train and validation data respectively. Model training occurs using data for t in [1, \(T_{tr}\)], while model selection for the best model can utilize data for t in [\(T_{tr+1}\), \(T_{val}\)]. The test set comprises predicting time steps [\(T_{val+1}\), \(T_{end}\)], which are not accessible during training or validation. However, since each training sample consists of the look-back window of size L and a prediction window of size S, the training input samples are restricted to \(X_{[L,T_{tr}-S]}\). Validation samples and test input samples range from \(X_{[T_{tr},T_{val}-S]}\), and from \(X_{[T_{val},T_{end}-S]}\), respectively. While this approach is plausible for independent data like images, it is unnatural for sequential data, as it leaves unreachable gaps (\(X_{[T_{tr}-S+1,T_{tr}-1]}\), \(X_{[T_{val}-S+1,T_{val}-1]}\)), undermining the consecutive characteristics. We filled the missing gaps, making train, validation, and test sets consecutive so that our \(BSA\) model could update momentum continuously. For fair evaluation, added samples were not used for either model training, validation, or performance assessment. Detailed explanations of model validation and evaluation are provided in Appendix A.3. The full code is available at [https://github.com/DJLee1208/BSA_2024](https://github.com/DJLee1208/BSA_2024).

```
Input: Trained up to (\(E\)-1)th epoch  TSF model \(f_{}\), \(BSA\) with sa-matrix, smoothing factors  Train data: \(_{tr}=\{(X_{0},Y_{0}),...,(X_{tr},Y_{tr})\}\)  Valid data: \(_{val}=\{(X_{tr+1},Y_{tr+1}),...,(X_{val},Y_{val})\}\)  mini-batch size: B Train: \(st=0,ed=B-1\)  initialize \(M_{0}\) in \(BSA\) with \(X_{0}\) and \(f_{}\) (\(:=f_{2} f_{1}\)) for\(X_{[st,ed]},Y_{[st,ed]}\) in \(_{tr}\)do {: train phase \(\}\)\(_{]st,ed} f_{2} BSA f_{1}(X_{[st,ed]})\) \(=_{}(P_{[st,ed]},Y_{[st,ed]})\)  Compute \(\) and adjust learning rate update \(f_{}\), sa-matrix, smoothing factors \(st+=B\), \(ed=min(ed+B,tr)\) endfor \(st=tr+1,ed=tr+B\) for\(X_{[st,ed]},Y_{[st,ed]}\) in \(_{val}\)do {: Validation phase \(\}\)\(P_{[st,ed]} f_{2} BSA f_{1}(X_{[st,ed]})\) \(=_{}(P_{[st,ed]},Y_{[st,ed]})\)  accumulate \(\) and calculate validation loss \(_{val}\) \(st+=B\), \(ed=min(ed+B,val)\) endfor Output: Trained up to \(E\) th epoch \(f_{}\), sa-matrix, \(\{_{1},...,_{K}\}\), \(_{val}\) for \(E\) th epoch
```

**Algorithm 1** Batched Spectral Attention (1 epoch)

## 4 Experiments

We first evaluate BSA using state-of-the-art TSF models and various real-world time series forecasting scenarios in section 4.1. Next, to demonstrate that BSA effectively addresses long-range dependencies and is robust to distribution shift, we perform experiments on synthetic signals of various frequencies in section 4.2. Finally, we analyze the BSA's performance variations depending on the insertion sites within the base model, examine computation and memory costs, and conduct an ablation study in section 4.3.

**Datasets.** We use eleven real-world public datasets: Weather, Traffic, ECL, ETT (4 sub-datasets; h1, h2, m1, m2), Exchange, PEMS03, EnergyData, and Illness . In the Illness dataset,

[MISSING_PAGE_FAIL:7]

7). BSA effectively improves MSE and MAE across all architectures. The average performance gain, in terms of MSE, ranged from as low as 0.96% to as high as 7.2%, with linear-based models demonstrating relatively high performance. Paired t-tests demonstrate statistically significant (p-value < 0.05) improvements in most MSE and all MAE across various models. This result emphasizes the model-agnostic versatility of BSA. Furthermore, BSA exhibited overall performance gain across all datasets, with the enhancement being statistically significant in 82% of cases. BSA's higher gain on ETTm compared to ETTh, both derived from the same signal but with different sampling rates, further indicates BSA's effectiveness in handling long-range dependencies.

To understand how BSA addresses long-range dependencies, we conduct an internal inspection. In Figure 3(a), we present the heat map of the trained SA-Matrix of the DLinear (Temperature(\({}^{}\)C) channel, Weather data). The positive x-axis corresponds to the \(log_{10}\) values of the periods preserved by the low-pass filter, derived with the smoothing factor. Negative values correspond to high-frequency components. The blue graph in Figure 3(b) represents the SA-Matrix averaged over the feature dimension, illustrating the frequencies to which BSA attends overall. The red graph represents the result of applying the Fast Fourier Transform (FFT) and denoising to the raw signal. The blue graph skewed towards the low-frequency side indicates that the BSA effectively captures the long-range trend of the data. Figure 3(c) depicts the graph for the SWDR (Short Wave Downward Radiation per unit area, W/m) channel in the same SA-Matrix. While not identical to Figure 3(b), it also exhibits strong attention towards the low-frequency pattern. In contrast, Figure 3(d), the FFT graph for the HULL (High UseLess Load channel, ETTh1 data), shows that the signal itself lacks long-range trends, resulting in the symmetric SA-Matrix. This result demonstrates that BSA operates as intended, learning the low-frequency components of the signal for future prediction. We provided other graphs and detailed information on the graph plotting method in Appendix B.2.

### Synthetic datasets

To further demonstrate that BSA learns long-range dependencies beyond the look-back window, we add sine waves with periods of 100, 300, and 1000 to the natural data while maintaining the mean and standard deviation (Refer to Appendix C for details on synthetic data generation). Figure 5 illustrates the performance improvement of BSA over the iTransformer model on the ETTh1 and

Figure 4: This figure illustrates the analysis of the SA-matrix of the DLinear model trained on the 720-step prediction task for the Weather and ETTh1 datasets. Panel (a) shows the heatmap of the SA-matrix, and (b)-(d) show the attention and FFT graphs.

Figure 5: Results of the iTransformer model on synthetic (a) ETTh1 and (b) ETTh2 datasets. The x-axis is the prediction length (96, 192, 336, 720), and the y-axis is the performance improvement (%) compared to the base model. Each color represents the different periods of the sine wave added to the natural data. 0 indicates original data and serves as the baseline.

ETTh2 datasets. The x-axis is the prediction length, and each line represents the period of the added sine wave. Performance improvement is calculated as 100\(\)(base MSE - BSA MSE) / base MSE. While the base model with a 96-length look-back window is expected to learn the 100-period trend, BSA outperformed it, especially for 96 and 192-step predictions (green line). The yellow line (period 300) shows nearly a 30% performance improvement across all prediction lengths. While the base model fails to learn the long-range interactions within a period of 300, BSA captures and utilizes the underlying trend. BSA also learns the 1000-period signal (red line) and demonstrates substantial improvements, especially in long prediction-length (336, 720) tasks. These results show that BSA effectively learns long-range patterns beyond the look-back window, essential for future prediction.

Figure 6 is generated from LUFL (Low UseFul Load) channel of ETTh1 data and with added sine waves of periods 100, 300, and 1000. The red arrow shows the added synthetic trend in the FFT graph (red line). As long-range trends are introduced, the attention graph (blue line) shifts from resembling symmetric Gaussian to a low-frequency bias. Longer sine wave periods cause greater shifts, prioritizing long-range information for predictions.

### Analysis and ablation studies

The BSA module offers high flexibility as it can be applied to arbitrary activations of the base model. In Table 2, we analyzed the performance changes by applying BSA at various locations within the model. Each location corresponds to a number in the Transformer architecture depicted in Figure 7. While we uniformly applied BSA to position 1 for the main result Table 1, the results in Table 2 suggest the potential for further performance enhancement by applying BSA at appropriate locations. Additionally, while there is variability depending on the placement, the performance consistently remains higher compared to the baseline, demonstrating the stability of our method. We provided a full result in Appendix D.1.

BSA shows consistent performance improvement across varying look-back window (input) lengths. Table 3 demonstrates BSA's superiority for look-back window lengths of 48, 96, and 192. Notably, while the baseline model's performance drops significantly with shorter inputs, BSA maintains high performance.

    &  &  \\  & MSE & MAE & MSE & MAE \\ 
1 & 0.2357 & 0.2682 & **0.2196** & **0.2815** \\
2 & 0.2352 & 0.2681 & - & - \\
3 & 0.2329 & **0.2654** & - & - \\
4 & 0.2508 & 0.2752 & 0.2327 & 0.2994 \\ Query-5 & **0.2326** & 0.2671 & - & - \\ Key-6 & 0.2538 & 0.2762 & - & - \\ Value-7 & 0.2415 & 0.2702 & - & - \\  baseline & 0.2556 & 0.2766 & 0.2444 & 0.3084 \\   

Table 2: Performance analysis on BSA insertion site. Each number corresponds with the insertion site in Figure 7.

We also demonstrate the impact of using BSA on the training time, peak memory, and the number of model parameters. The extra computation required for BSA is constant with data length and linear with a look-back window length (c.f. quadratic for the base model with transformer architecture). Table 4 demonstrates the computational cost of the BSA is quite small, showing less than a 5% increase even with the large PEMS03 dataset.

We conduct an ablation study on the three key components that constitute BSA (Table 5). "BPTT" refers to whether gradients can flow between samples within the mini-batch. Without BPTT, BSA learns similarly to SA. "SFs" denotes whether to use multiple smoothing factors. Lastly, "Learn SF" indicates whether the smoothing factor is treated as learnable. The results indicate that each component significantly contributes to the performance improvement of BSA.

## 5 Conclusion

Our study addresses the challenges in handling long-range dependencies inherent in time series data by introducing a fast and effective Spectral Attention mechanism. By preserving temporal correlations and enabling the flow of gradients between samples, this mechanism facilitates the model in capturing crucial long-range interactions essential for accurate future predictions. Therefore, our research paves the way for fixed-sized input models to effectively handle long-range dependencies extending far beyond the input window. Through extensive experimentation, we demonstrated that our Spectral Attention mechanism enhances performance across various base architectures, with its ability to grasp long-term dependencies being the key factor behind this improvement. BSA effectively tackles long-term fluctuations, complementing the base model's capacity to manage intricate yet short-term patterns. This integrated model holds promise for improving real-world application performance. For instance, it could boost weather forecast accuracy by simultaneously capturing minute-by-minute weather changes and seasonal variations. Moreover, when predicting deterioration from a patient's real-time data, it can consider medications with lengthy onset times. Our study has limitations: we did not analyze the impact of BSA placement within the base model in detail. Also, BSA's performance gains may be limited when applied to datasets with only high-frequency information within the look-back window. These issues should be addressed in future research.

## 6 Acknowledgement

This research was supported by a grant of the MD-Phd/Medical Scientist Training Program through the Korea Health Industry Development Institute (KHIDI), funded by the Ministry of Health & Welfare, Republic of Korea, National Research Foundation of Korea (NRF) grants funded by the Korea government (Ministry of Science and ICT, MSIT) (2022R1A3B1077720 and 2022R1A5A708390811), Institute of Information & Communications Technology Planning & Evaluation (IITP) grants funded by the Korea government (MSIT) (2021-0-01343: Artificial Intelligence Graduate School Program (Seoul National University), 2022-0-00959 and IITP-2024-RS-2024-00397085: Leading Generative AI Human Resources Development), and the BK21 FOUR program of the Education and Research Program for Future ICT Pioneers, Seoul National University in 2024, AI-Bio Research Grant through Seoul National University, Hyundai Motor Company, HUINNO AIM Company through HA-Rnd-2325-PredictClinicalDeterioration.

  &  &  \\ _Length_ & MSE(\%) & MAE*(\%) & MSE(\%) & MAE*(\%) \\ 
48 & 12.08 & 5.21 & 29.45 & 17.35 \\ 
96 & 7.78 & 3.03 & 24.55 & 13.66 \\
192 & 6.87 & 3.34 & 9.63 & 5.18 \\ 

Table 3: BSAâ€™s performance improvement (%, denoted with *) compared to base model across three input lengths \(\{48,96,192\}\) (iTransformer, average over 4 prediction lengths and 3 seeds). The full result, including other models, is in Appendix D.2.

  &  &  \\ _Length_ & MSE(\%) & MAE*(\%) & MSE(\%) & MAE*(\%) \\ 
48 & 12.08 & 5.21 & 29.45 & 17.35 \\ 
96 & 7.78 & 3.03 & 24.55 & 13.66 \\
192 & 6.87 & 3.34 & 9.63 & 5.18 \\ 

Table 4: Computational cost increase with the BSA (%), averaged over 3 seeds and 4 prediction lengths on the PEMS03 dataset. TN: TimesNet, iTF: iTransformer, CF: Crossformer, PTST: PatchTST. The full result, including other datasets, is in Appendix D.3.