# Paxion: Patching Action Knowledge in

Video-Language Foundation Models

 Zhenhailong Wang\({}^{1}\),  Ansel Blume\({}^{1}\),  Sha Li\({}^{1}\),  Genglin Liu\({}^{1}\),

Jaemin Cho\({}^{2}\),  Zineng Tang\({}^{2}\),  Mohit Bansal\({}^{2}\),  Heng Ji\({}^{1}\)

\({}^{1}\)UIUC \({}^{2}\)UNC

{wangz3,hengji}@illinois.edu

###### Abstract

Action knowledge involves the understanding of textual, visual, and temporal aspects of actions. We introduce the **Action Dynamics Benchmark (ActionBench)** containing two carefully designed probing tasks: Action Antonym and Video Reversal, which targets multimodal alignment capabilities and temporal understanding skills of the model, respectively. Despite recent video-language models' (VidLM) impressive performance on various benchmark tasks, our diagnostic tasks reveal their surprising deficiency (near-random performance) in action knowledge, suggesting that current models rely on object recognition abilities as a shortcut for action understanding. To remedy this, we propose a novel framework, **Paxion**, along with a new **Discriminative Video Dynamics Modeling (DVDM)** objective. The **Paxion** framework utilizes a **Knowledge Patcher** network to encode new action knowledge and a **Knowledge Fuser** component to integrate the Patcher into frozen VidLMs without compromising their existing capabilities. Due to limitations of the widely-used Video-Text Contrastive (VTC) loss for learning action knowledge, we introduce the DVDM objective to train the Knowledge Patcher. DVDM forces the model to encode the correlation between the action text and the correct ordering of video frames. Our extensive analyses show that **Paxion** and DVDM together effectively fill the gap in action knowledge understanding (~50% \(\) 80%), while maintaining or improving performance on a wide spectrum of both object- and action-centric downstream tasks. The code and data will be made publicly available for research purposes at [https://github.com/MikeWangWZHL/Paxion.git](https://github.com/MikeWangWZHL/Paxion.git).

## 1 Introduction

Recent video-language models (VidLMs)  have shown impressive performance on a wide range of video-language tasks. However, such multimodal models are not without deficiencies:  points out that many popular video-language benchmarks  can be solved by looking at a single frame, and  shows that vision-language models struggle to understand compositional and order relations in images, treating images as bags of objects. Such limitations suggest that models' understanding of _actions_, which may require several frames and comprehension of object relationships, may be lacking.

To test this hypothesis, we first define **action knowledge** as an understanding of the cause and effect of actions in textual, visual, and temporal dimensions. To quantify a model's action knowledge, we introduce the **Action Dynamics Benchmark (ActionBench)**. ActionBench contains two probing tasks: distinguishing between (1) a video's caption and the caption with its action verbs replaced by their antonyms; (2) the original and reversed videos. The benchmark also includes a baseline task for controlling the undesired impact from domain mismatch and investigating potential bias towards objects. The baseline task requires the model to differentiate between the original video captionsand altered versions with randomly replaced objects. We find that state-of-the-art video-language foundation models  exhibit near-random performance on our action-oriented probing tasks while excelling on the object-oriented baseline task (Figure 2). This shows that VidLMs lack action knowledge and suggests that their impressive performance on other benchmarks may be attributed to their object recognition ability instead of action understanding.

To address this shortcoming, we propose a novel framework, **Paxion** (Patching Actions), to patch existing VidLMs with action knowledge without compromising their general vision-language (VL) capabilities. **Paxion** comprises two main components, the Knowledge Patcher and the Knowledge Fuser. The **Knowledge Patcher (KP)** is a Perceiver-based  lightweight module attached to a frozen VidLM backbone used to augment the VidLM with action-aware representations. Through our preliminary experiments, one major challenge for patching action knowledge is that the widely-used Video-Text Contrastive (VTC) objective  is insufficient, which echoes the findings of related work . Hence, inspired by dynamics modeling in robotics and reinforcement learning , we introduce the **Discriminative Video Dynamics Modeling (DVDM)** objective that forces the model to learn the correlation between an action's textual signifier, the _action text_ (e.g. the word "falling"), and the action's visual depiction (e.g. a clip of a falling book). DVDM includes two new losses, _Video-Action Contrastive (VAC)_ and _Action-Temporal Matching (ATM)_, which are compatible with VTC without additional parameters. Specifically, we formulate discriminative tasks using action antonyms and reversed videos, with special emphasis on learning from data instances with salient state changes. We demonstrate that the synergy between the Knowledge Patcher and DVDM leads to a dramatic improvement on our ActionBench tasks.

Next, we investigate whether our Knowledge Patcher, which is specialized for action understanding, can be integrated into existing VidLMs for downstream tasks that require both action and object knowledge. To this end, we introduce the **Knowledge Fuser (KF)** component of **Paxion** which fuses the _action-centric representation_ from the Knowledge Patcher with the _object-centric representation_ from the frozen backbone using cross-attention. We show that the fused representation from **Paxion** improves both object and action understanding on a wide spectrum of tasks, including Video-Text Retrieval (SSv2-label ), Video-to-Action Retrieval (SSv2-template , Temporal ), and Causal-Temporal Video Question Answering (NEx-QA ). Moreover, our analysis shows that the Knowledge Fuser is essential to maintain a balance between the models' object-related understanding and improving performance on downstream action and temporal-oriented tasks.

We also test the robustness of **Paxion** by considering a zero-shot cross-domain transfer setting on the Moments-in-Time  and Kinetics  datasets. We find that the Knowledge Fuser is critical for increasing robustness to domain shifts and that positive transfer to unseen domains can be achieved by further ensembling **Paxion** with the backbone model.

To the best of our knowledge, this is the first work to systematically evaluate action knowledge and patch it into video-language foundation models. Our main contributions are threefold:

1. We introduce the Action Dynamics Benchmark (SS 2), which probes action understanding capabilities in video-language models. We evaluate three state-of-the-art video-language foundation models and conclude that they lack a basic grasp of action knowledge.
2. We propose a novel learning framework called **Paxion**, which patches the missing action knowledge into frozen video-language foundation models without hurting their gen

Figure 1: Overview of the **Paxion** framework. The goal is to patch frozen VidLMs with action knowledge without compromising their general vision-language capabilities. The Knowledge Patcher (KP) aims to learn an action-centric representation by leveraging ActionBench data (§ 2) and our newly proposed Discriminative Video Dynamics Modeling (DVDM) training objectives (§ 3.1). The Knowledge Fuser (KF) aims to obtain a balanced representation for general downstream tasks by fusing the KP with the backbone.

eral vision-language capabilities. The key components of Paxion include a Perceiver-based Knowledge Patcher (SS 3) and a cross-attention-based Knowledge Fuser (SS 4).
3. We propose the DVDM objective (SS 3.1), an improvement over the widely-used VTC loss, which forces the model to encode the correlation between the action text and the correct ordering of video frames. Extensive experiments show that Paxion with DVDM improves the joint understanding of objects and actions while being robust to domain shift.

Action Dynamics Benchmark (ActionBench): Do Video-Language Foundation Models Understand Action Knowledge?

To investigate the presence of action knowledge in state-of-the-art video-language foundation models, we propose the **Action Dynamics Benchmark (ActionBench)**. ActionBench comprises the **Action Antonym (AA)** and **Video Reversal (VR)** probing tasks, along with the **Object Replacement (OR)** baseline task. The probing tasks evaluate the _multimodal and temporal correlations between an action text and a video_. The baseline task controls for the potential impact of domain mismatch.

Figure 2: **Top**: Illustration of the probing tasks and baseline task in our proposed **ActionBench**. The bounding boxes in the video frames are purely for visualization. The numbers on the right show the ranking scores from a state-of-the-art VidLM, InternVideo . The model struggles to determine whether a book is “falling” or “rising,” but can confidently identify the object to be a “book” instead of a “cellphone”. **Bottom**: ActionBench results of three recent VidLMs . The column “Avg” indicates averaged results on each task across all three models. Existing VidLMs achieve near-random results on the probing tasks (AA and VR) while excelling on the baseline task (OR). This demonstrates that _existing VidLMs lack fundamental action knowledge and exhibit strong bias to object understanding._

We construct this benchmark by leveraging two existing open-domain video-language datasets, Ego4D  and Something-Something v2 (SSv2) , which provide fine-grained action annotations for each video clip. Compared to a previous verb understanding probing benchmark  based on MSRVTT  and LSMDC , ActionBench is more action-oriented, larger in scale, and contains both ego-centric and third-person videos. Detailed statistics can be found in Appendix B. An illustration of each ActionBench task can be found in Figure 2.

Probing Task: Action Antonym (AA).The Action Antonym task probes the multimodal alignment of the action text and the video representation. We formulate AA as a binary classification task that involves distinguishing the original text annotation from its altered version with the action replaced by its antonym, given the corresponding video clip. For example, if the original text is "Book falling like a rock", the action antonym text would be "Book rising like a rock". We leverage the WordNet  database and manually constructed mappings to automatically construct the antonym texts (details in Appendix B).

Probing Task: Video Reversal (VR).The Video Reversal task probes the temporal understanding of actions. We formulate VR as a binary classification task, where given a video-text pair with at least one action and a reversed version of the video, the goal is to distinguish the original video from the reversed one. Achieving non-trivial performance on the Video Reversal task requires the model to understand the temporal sequence implied by the action. The Video Reversal task also evaluates VLMs' abilities to identify violations of physical knowledge, as some clips defy expectation when reversed (e.g. a falling book becomes one which rises without any discernible cause).

Baseline Task: Object Replacement (OR).Object Replacement is a binary classification task that requires the model to distinguish between the original text annotation and an altered version with objects tokens randomly replaced by other object tokens in the dataset. The Object Replacement task allows us to understand: (1) whether current VidLMs rely on object recognition as a "shortcut" for video-text matching (i.e., if they have an object-biased representation), and (2) whether poor performance on Action Antonym can be attributed to domain mismatch (i.e., not being trained on Ego4D or SSv2) instead of a lack of action knowledge.

### Evaluating Video-Language Models on ActionBench

We evaluate three recent video-language foundation models, **InternVideo**, **CLIP-ViP** and **Singularity-temporal**1, on ActionBench. Despite their impressive improvements on video-language benchmarks, these models struggle to achieve non-trivial performance on Action Antonym and Video Reversal, as depicted in Figure 2. The fact that they achieve significantly better performance on the Object Replacement task indicates a strong bias towards objects over actions, and affirms that the poor performance on AA is not solely a result of domain mismatch. The near-random performance on the VR task indicates a lack of basic temporal reasoning and physical knowledge.

These observations align with previous approaches [17; 42; 59; 37] which show similar limitations in image-language models [43; 28] and earlier video-language models [11; 35]. We find that high performance on video-language benchmarks does not necessarily equate to a stronger understanding of action knowledge.

## 3 Patching Action Knowledge in Frozen Video-Language Models

In SS 2, we showed that current VidLMs exhibit limitations in their understanding of action knowledge, a crucial component for developing a comprehensive understanding of the external world. This raises the important question: _Can we enhance existing VidLMs with this missing knowledge without hurting their general video-language capabilities?_

To this end, we propose a novel learning framework, Paxion, which comprises two main components: the **Knowledge Patcher (KP)** (SS 3) and the **Knowledge Fuser (KF)** (SS 4). An overview of the Paxion framework can be found in Figure 1. Analogous to releasing _patches_ to fix bugs in published software, the Knowledge Patcher is a Perceiver-based [21; 20] lightweight module attached to a frozen VidLM for steering the VidLM towards action-centric representations. As the widely used video-language contrastive (VTC) objective is insufficient for learning action knowledge, we introduce 

**Discriminative Video Dynamics Modeling (DVDM)** (SS 3.1) objectives that force the model to encode the correlation between the actual action text (e.g., "falling") and the correct sequence of visual state-changes (i.e., video frames).

**Knowledge Patching with Perceivers.** Inspired by recent work [2; 27] leveraging Perceivers [21; 20] to extract _language-related_ visual features, we use Perceivers to extract _knowledge-specific_ features. As shown in Figure 3 Knowledge Patcher, we use a lightweight Perceiver which performs cross-attention between a sequence of lower-dimensional, learnable latents \(\) and the higher-dimensional visual embedding \(^{}\) from a frozen, pretrained VidLM backbone. To further investigate the viability of Perceivers as an alternative to Transformers , we include another variant of the KP where we replace the Perceiver with a standard Transformer Encoder. Table 1 shows that the

    &  \\   &  & **Trainable Param\#** & **AA** & **VR** & **AA** & **VR** & **Avg** \\  & & **Param\#** & (Ego4d) & (Ego4d) & (SSv2) & (SSv2) & **Avg** \\   & Backbone & - & 58.8 & 46.2 & 51.8 & 48.3 & 51.3 \\  & KP-Transformer [VTC] & 8.4M (1.8\%) & 68.2 & 62.8 & 65.5 & 60.6 & 64.3 \\  & KP-Perceiver [VTC] & 4.2M (0.9\%) & 66.5 & 63.6 & 69.8 & 71.0 & 67.7 \\  & KP-Perceiver [VTC+**DVDM]** & 4.2M (0.9\%) & **90.1** & **75.5** & **90.7** & **87.4** & **85.9** \\   & Backbone & - & 49.3 & 55.0 & 70.2 & 53.6 & 57.0 \\  & KP-Transformer [VTC] & 3.9M (2.6\%) & 61.9 & 53.4 & 72.2 & 54.3 & 60.5 \\  & KP-Perceiver [VTC] & 2.4M (1.6\%) & 61.9 & 54.6 & 71.5 & 48.8 & 59.2 \\  & KP-Perceiver [VTC+**DVDM]** & 2.4M (1.6\%) & **89.3** & **56.9** & **89.3** & **66.0** & **75.4** \\   & Backbone & - & 47.0 & 50.1 & 48.9 & 49.6 & 48.9 \\  & KP-Transformer [VTC] & 3.9M (1.8\%) & 61.9 & 48.2 & 63.8 & 49.5 & 55.9 \\   & KP-Perceiver [VTC] & 1.3M (0.6\%) & 60.3 & 46.1 & 63.3 & 51.5 & 55.3 \\   & KP-Perceiver [VTC+**DVDM]** & 1.3M (0.6\%) & **83.8** & **58.9** & **82.4** & **68.8** & **73.5** \\   & Human & 92.0 & 78.0 & 96.0 & 90.0 & 89.0 \\   

Table 1: ActionBench results (in accuracy %). _KP-*_ refers to Knowledge Patcher. _AA_ and _VR_ indicate the Action Antonym task and the Video Reversal task. _VTC_ and _DVDM_ stands for Video-Text Contrastive loss and our newly proposed Discriminative Video Dynamics Modeling losses detailed in § 3.1. _Trainable Param#_ indicates the size of the trainable parameters compared to the backbone.

Figure 3: Illustration of the **Knowledge Patcher** component (bottom left) of Paxion and the training objectives (upper left). On the right, we show the comparison of performance on ActionBench before and after adding the Knowledge Patcher.

Perceiver-based KP achieves competitive or better performance compared to the Transformer variant while being 2-3 times smaller in scale. Architecture details of the KPs can be found in Appendix D.1.

Video-Text Contrastive (VTC) is insufficient for learning action knowledge.We initially train both variants of the Knowledge Patcher on the training set of ActionBench with only the Video-Text Contrastive (VTC) loss. VTC loss aligns the visual representation \(V\) from the KP with the pooled textual representation \(t^{}\) from the frozen backbone. Results in Table 1 show that training with the VTC loss alone provides marginal to no improvements on Action Antonym (AA) and Video Reversal (VR), particularly on smaller backbone models. This suggests the need for new training objectives (SS 3.1) for learning action knowledge.

### Learning Action Knowledge with Discriminative Video Dynamics Modeling

To address the limitation of the VTC loss in learning action knowledge, we propose two new losses that draw inspiration from dynamics modeling in Robotics and Reinforcement Learning [1; 4; 15; 23; 39]. Specifically, in a typical Markov Decision Process (MDP) setup, _forward dynamics modeling_ aims to predict the next world state \(_{t+1}\) given the current world state \(x_{t}\) and the action \(u_{t}\). _Inverse dynamics modeling_ aims to predict the current action \(_{t}\) given the current and next world state \(x_{t},x_{t+1}\). Given video frame as a representation of the world states, existing work usually formulates forward dynamics modeling as a generative task [1; 39; 15], directly reconstructing the pixels or the latent embedding of the next frame. For inverse dynamics modeling, the action class is usually predicted using a dedicated classification layer [4; 23; 1]. However, our preliminary experiments show that the existing formulation cannot be directly applied in our setting due to the following **unique challenges**: (1) Real world videos are much more complex than videos in a lab setting, with constantly changing backgrounds and moving camera angles, causing a large portion of visual features to be unrelated to the main objects and actions. Furthermore, without additional annotation, it is difficult to identify the frames corresponding to the "current" and "next" states, as actions may be continuous (e.g., "walking") or repetitive (e.g., "doing push-ups") within a video. Thus, the training signal from a regression loss becomes extremely noisy. (2) Unlike previous work that has a small fixed number of action classes, we model actions as natural language phrases, making direct classification inapplicable.

To address these unique challenges, we propose a novel _"relaxed"_ formulation of dynamics modeling, dubbed **Discriminative Video Dynamics Modeling (DVDM)**, which contains two losses: **Video-Action Contrastive (VAC)** and **Action-Temporal Matching (ATM)**. Both VAC and ATM can be directly incorporated into the Video-Text Contrastive (VTC) loss without any additional parameters. As illustrated in Figure 3 Losses, the VAC loss aims to encourage the model to learn the correlation between the visual observations and the actual actions. We formulate the VAC loss as adding action antonym texts as hard negatives. The ATM loss encourages the model to consider the temporal ordering of the visual observations (video frames). Instead of directly generating the next state frames, we formulate ATM as a discriminative task similar to Video Reversal in SS 2, where the model distinguishes reversed videos from the original videos, alleviating the need for explicit state annotations. In order to make sure that the reversed videos are indeed distinguishable from the original ones, we further introduce a method (Appendix C) for identifying videos with salient state-changes by leveraging image-language foundation models . The idea is to measure the frame-text and frame-frame similarity between the first and second half of a video. We compute ATM loss only on the videos that have salient state-changes between frames. Experimental results, as shown in Table 1 and Figure 3, indicate that **adding the DVDM objectives significantly improves the performance on both probing tasks**, suggesting that the resulting representation from the Knowledge Patcher demonstrates a stronger understanding of action knowledge.

## 4 Leveraging Patched Action Knowledge for Downstream Tasks

In SS 3, we showed that the Knowledge Patcher (KP) and DVDM objectives together effectively learn action knowledge-specific representations. However, these representations are highly specialized to action understanding, which may not be optimal for general downstream tasks that require both object and action understanding. Thus, the remaining challenge is to _retain the general VL capabilities of the backbone while leveraging the newly learned action knowledge._One naive idea is to simply use the backbone embeddings whenever the task is less action-centric. However, it is difficult to decide when to use the backbone without prior knowledge of a given task. Further, using the backbone embeddings alone gives up the patched action knowledge that can be essential for certain downstream tasks, such as action recognition. In this section, we demonstrate that we can get the best of both worlds by fusing the action-centric representation from the Knowledge Patcher with the object-centric representation from the frozen backbone.

For this we introduce the second component of Paxion, the **Knowledge Fuser (KF)**, illustrated in Figure 4. The KF takes the pooled visual feature (\(^{}\)) from the frozen VL backbone as the input query, and performs cross-attention with the extracted visual tokens (\(V\)) from the Knowledge Patcher.

### Experimental Setup

To evaluate the model's ability to retain general visual-linguistic capabilities while leveraging newly learned action knowledge, we consider a spectrum of video-language tasks with different emphases on object and action understanding. Specifically, we consider **Video-Text Retrieval** (SSv2-label ), which is object-centric and biased towards static appearances ; **Causal-Temporal VQA** (NExT-QA ), which requires joint understanding of static objects and dynamic events; and **Video-to-Action Retrieval** (SSv2-template , Temporal-SSv2 ), which is highly action-centric and temporal-intensive. A task is considered to be temporal-intensive if it cannot be solved without correct temporal information , e.g., reversed or shuffled frames. For example, as illustrated in Figure 5, the Video-to-Action Retrieval task obscures object names in the text, making it impossible to align text with a video based solely on objects. Moreover, it is impossible to distinguish "approaching" and "moving away" without considering the temporal ordering of the frames.

For **Paxion**, we finetune the Knowledge Fuser jointly with the Knowledge Patcher on downstream tasks using VTC loss. By default, the KP in Paxion is trained with VTC and DVDM losses (SS 3.1). We include the baselines **KP-Transformer FT** (vtc) and **KP-Perceiver FT** (vtc), which are both obtained by continuing to finetune the VTC-only KPs from Table 1 on downstream tasks. Additionally, we compare Paxion with **Side-Tuning**, a Parameter-Efficient Finetuning (PEFT) method that could serve as an alternative to the KF. For the Side-Tuning variant, we initialize the "side-model" using the same Knowledge Patcher as in Paxion and do alpha blending with the frozen backbone. Implementation and configuration details for each method and task can be found in Appendix D. The results are shown in Tables 2 and 3.

### Analysis

**Paxion improves joint understanding of objects and actions.** Tables 2 and 3 show that Paxion outperforms both the _Backbone_ and the _VTC-only baselines (KP-*)_. This indicates that Paxion not only retains the original VL capabilities of the backbone, but also fills in the gap of the missing action knowledge by fusing the original representations with the patched ones. We corroborate this finding by observing more significant improvements on action-centric and temporal-intensive tasks,

    &  &  \\  &  &  &  \\  & \(R1_{v2t}\) & \(R5_{v2t}\) & \(R1_{t2v}\) & \(R5_{t2v}\) & \(R1\) & \(R5\) & \(R1\) & \(R5\) \\  InternVideo Backbone & 18.8 & 39.9 & 19.9 & 40.0 & 5.6 & 15.9 & 11.2 & 35.8 \\ KP-Transformer FT (vtc) & 24.1 & 50.0 & 21.7 & 46.0 & 21.1 & 55.9 & 41.1 & 88.9 \\ KP-Perceiver FT (vtc) & 27.0 & 57.4 & 27.1 & **56.8** & 24.8 & 59.7 & 42.5 & 91.3 \\ Side-Tuning  (vtc+DVDM) & 30.9 & 59.2 & 26.6 & 53.1 & 22.2 & 55.1 & 50.2 & 90.9 \\
**Paxion** (vtc+dVDM) & **32.3** & **61.2** & **28.0** & 54.3 & **26.9** & **61.5** & **51.2** & **91.9** \\   

Table 2: Video-Text Retrieval and Video-to-Action Retrieval results. R1 and R5 represent Recall@1 and Recall@5 (in %) respectively. Subscripts \({}_{vt2}\) and \({}_{t2v}\) represent video-to-text and text-to-video, respectively.

Figure 4: Illustration of the Knowledge Fuser component.

such as Temporal-SSv2 (+20% @R1), compared to object-centric tasks, such as SSv2-label (+11% @R1)2. The decomposed results on NExT-QA (Table 3) show that Paxion helps more on the Causal (\(C\)) and Temporal (\(T\)) types of questions, and on the harder subset (ATP-hard) where the temporal and action knowledge is emphasized.

Paxion also outperforms Side-Tuning, highlighting the effectiveness of cross-attention for deep fusion. Specifically, the Knowledge Fuser allows us to attend to all extracted visual tokens from the Knowledge Patcher instead of only blending with pooled representations as in Side-Tuning.

Qualitative analysis.Figure 5 shows two qualitative examples on Temporal-SSv2 and NExT-QA. For the Temporal-SSv2 example, we find that the finetuned Knowledge Patcher trained with only VTC fails to distinguish _"Moving away"_ from _"Approaching,"_ while Paxion trained with DVDM successfully correlates the seemingly expanding object with the action _"Approaching"_. For the NExT-QA example, the question asks the model to identify what happens after the action _"approached near the camera"_. The VTC baseline incorrectly selects the action _"turn back to the toy,"_ which happens before approaching the camera. On the other hand, Paxion successfully chooses _"raised his hand to take the camera"_. This indicates a stronger understanding of both action dynamics in words such as _"approach"_ and the temporal ordering implied by words such as _"after"_. Additional qualitative examples and analysis of failure cases can be found in Appendix E.

Disentangling the impact of Knowledge Patching and Fusing.We further investigate the disentangled impact of the Knowledge Fuser (KF) and the Knowledge Patcher (KP) with two ablation settings: (1) **KP+Finetune**, where instead of adding the KF, we directly _finetune_ the KP _trained with DVDM_ on downstream tasks; (2) **KP[VTC]+KF**, where we train the KP _without DVDM_ and then _add the KF_ upon it. The results are shown in Figure 6, where the \(\) score represents the relative difference of downstream task performance between our original Paxion (Row 5 in Tables 2 and 3) and the two ablated settings. The key observations are as follows: (1) **The Knowledge Fuser contributes more to object understanding.** From Figure 6 Left, we find that the KF helps most when the tasks are more object-centric, e.g., SSv2-label. On highly action-centric tasks, e.g., Temporal-SSv2, directly using the action-knowledge-patched representation is preferable to fusing with the backbone representation.

Figure 5: Qualitative examples on Temporal-SSv2  and NExT-QA . _VTC-Finetune_ and Paxion refer to methods in row 3 and row 5 in Tables 2 and 3.

    &  \\  &  &  \\  & C & T & D & all & C & T & all \\  InternVideo Backbone & 43.3 & 38.6 & 52.5 & 43.2 & 27.0 & 27.3 & 27.1 \\ KP-Transformer FT [vtc] & 46.1 & 45.0 & 61.3 & 48.1 & 32.5 & 33.6 & 33.0 \\ KP-Perceiver FT [vtc] & 46.0 & 46.0 & 58.9 & 48.0 & 30.1 & 31.6 & 30.7 \\ Side-Tuning  [VTC+DVDM] & 54.9 & 52.0 & **69.8** & 56.3 & 37.4 & 36.0 & 36.8 \\
**PaxION [vtc+DVDM]** & **56.0** & **53.0** & 68.5 & **57.0** & **38.8** & **38.1** & **38.5** \\   

Table 3: Causal-Temporal VQA (NExT-QA) results (in accuracy %) on the validation set. We consider both the original and ATP-hard  split. We report accuracy for _all_ questions or specific types of questions, including causal (\(C\)), temporal (\(T\)), and descriptive (\(D\)) questions.

(2) **Patching with action knowledge contributes more to action-centric understanding.** From Figure 6 Right, we find that patching with action knowledge, i.e., training with DVDM objectives, contributes to better performance on downstream tasks that are more action-centric. Importantly, this result also indicates that the improvements observed in Tables 2 and 3 do not come solely from adding the KF. However, if the task is more object-centric, such as SSv2-label, VTC training alone is sufficient.

Robustness to domain shift.Learned action knowledge should be generalizable to unseen tasks and domains. However, this goal is difficult to realize with only domain-specific datasets like SSv2 which contains only 174 actions. Therefore, in Appendix A we conduct experiments on zero-shot cross-domain transfer which demonstrate that the Knowledge Fuser in Paxion increases robustness to domain shift and can introduce positive transfer during zero-shot inference.

## 5 Related Work

Limitations of vision-language contrastive pretraining.Since CLIP , multimodal contrastive losses have been the major pretraining objective for almost all recent image-language  and video-language models . Previous work  has revealed the limitation of contrastive pretraining on fine-grained compositional understanding, verb understanding, and temporal reasoning. Concurrent work  proposed mining hard negatives by rule-based heuristics or large-language models  to improve understanding of structured vision-language concepts and verbs. In this work, we focus on general action knowledge which includes verb understanding as well as action temporal understanding. Instead of directly tuning the entire backbone as in , Paxion enables fast action knowledge patching while also achieving improved performance on both object-centric and action-centric downstream tasks. It is worth noting that the hard negative mining method proposed by  can be easily incorporated with our VAC loss and could potentially result in stronger results.

Parameter-efficient fine-tuning (PEFT).The recent surge in the size of large language models  has spurred research on parameter-efficient fine-tuning . Although current video-language models are smaller in scale, we aim to develop Paxion to be applicable to larger models that we anticipate will emerge in the near future. The most similar PEFT-related work to ours is Side-Tuning , which we compare against in SS 4.1. At a high-level, unlike existing PEFT methods that optimize for specific downstream tasks, Paxion is designed to learn a specific type of knowledge that can benefit various downstream tasks (SS 4.2). Furthermore, it is unclear how to aggregate the task-specific parameters, such as those in adapters  or low-rank layers , to perform multiple tasks. The versatility of Paxion allows for its use in learning various types of knowledge, each with its own Knowledge Patcher. Subsequently, the patched knowledge-specific representations can be fused together using one Knowledge Fuser. This work serves a proof-of-concept where we focus on action knowledge. We leave the exploration of other types of knowledge and a more comprehensive comparison with PEFT methods as future work.

Figure 6: **Left**: Impact of the _Knowledge Fuser_. Comparing _finetuning_ or _fusing_ the same Knowledge Patcher trained with VTC+DVDM losses. **Right**: Impact of _action knowledge patching (DVDM)_ on downstream tasks. Comparing fusing with the Knowledge Patcher trained with _VTC loss only_ or _VTC+DVDM losses_. The \(\) score indicates the relative difference in terms of downstream task accuracy between our original Paxion and the ablated settings (detailed in §4.2).

Conclusions and Future Work

In this work we propose the ActionBench benchmark for evaluating models' understanding of action knowledge, and reveal a major deficiency in state-of-the-art video-language foundation models in this area. We then propose Paxion to patch in such action knowledge without compromising models' existing capabilities. We show that Paxion significantly improves the model's action understanding while achieving competitive or superior performance on downstream tasks. One limitation of this work is that we only experimented with patching one type of knowledge. We intend to address this in future work, where we plan to expand Paxion to patch broader aspects of physical knowledge such as object affordances and mental simulation, and to explore fusion with multiple learned Knowledge Patchers.