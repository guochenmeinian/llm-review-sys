# GenAI Arena: An Open Evaluation Platform for Generative Models

Dongfu Jiang\({}^{*}\) Max Ku\({}^{*}\) Tianle Li\({}^{*}\)

Yuansheng Ni Shizhuo Sun Rongqi Fan Wenhu Chen

University of Waterloo

{dongfu.jiang, m3ku, t291i, wenhuchen}@uwaterloo.ca

[https://hf.co/spaces/TIGER-Lab/GenAI-Arena](https://hf.co/spaces/TIGER-Lab/GenAI-Arena)

###### Abstract

Generative AI has made remarkable strides to revolutionize fields such as image and video generation. These advancements are driven by innovative algorithms, architecture, and data. However, the rapid proliferation of generative models has highlighted a critical gap: the absence of trustworthy evaluation metrics. Current automatic assessments such as FID, CLIP, FVD, etc often fail to capture the nuanced quality and user satisfaction associated with generative outputs. This paper proposes an open platform GenAI-Arena to evaluate different image and video generative models, where users can actively participate in evaluating these models. By leveraging collective user feedback and votes, GenAI-Arena aims to provide a more democratic and accurate measure of model performance. It covers three arenas for text-to-image generation, text-to-video generation, and image editing respectively. Currently, we cover a total of 35 open-source generative models. GenAI-Arena has been operating for seven months, amassing over 9000 votes from the community. We describe our platform, analyze the data, and explain the statistical methods for ranking the models. To further promote the research in building model-based evaluation metrics, we release a cleaned version of our preference data for the three tasks, namely GenAI-Bench. We prompt the existing multi-modal models like Gemini, GPT-4o to mimic human voting. We compute the accuracy by comparing the model voting with the human voting to understand their judging abilities. Our results show existing multimodal models are still lagging in assessing the generated visual content, even the best model GPT-4o only achieves an average accuracy of \(49.19\%\) across the three generative tasks. Open-source MLLMs perform even worse due to the lack of instruction-following and reasoning ability in the complex vision scenarios.

Figure 1: GenAI Arena contains three components: (1) text-to-image, text-to-video and image editing arena, which accept community voting to obtain the preference pairs. (2) The leaderboard utilizes the preference pairs to calculate elo ranking for all the evaluated models. (3) We further release GenAI-Bench to judge different multimodal LLM judges.

Introduction

Image generation and manipulation technologies have seen rapid advancements, leading to their widespread application across various domains such as creating stunning artwork , enhancing visual content , and aiding in medical imaging . Despite these advancements, navigating through the multitude of available models and assessing their performance remains a challenging task . Traditional evaluation metrics like PSNR, SSIM , LPIPS , and FID , while valuable, offer very specific insights into precise aspects of visual content generation. However, these metrics often fall short in providing a comprehensive assessment of overall model performance, especially when considering subjective qualities like aesthetics and user satisfaction .

To address these challenges, we introduce GenAI-Arena--a novel platform designed to enable fair evaluation. Inspired by successful implementations in other domains , GenAI-Arena offers a dynamic and interactive platform where users can generate images, compare them side-by-side, and vote for their preferred models. Such a platform not only simplifies the process of comparing different models but also provides a ranking system that reflects human preferences, thereby offering a more holistic evaluation of model capabilities. To our knowledge, GenAI-Arena is the first evaluation platform with comprehensive evaluation capabilities across multiple properties. Unlike other platforms, it supports a wide range of tasks across text-to-image generation, text-guided image editing, and text-to-video generation, along with a public voting process to ensure labeling transparency. The votes are utilized to access the evaluation ability of Multimodal Large Language Model (MLLM) evaluators. Table 1 shows our platform excels in its versatility and transparency.

Since February 11th, 2024, we have collected over 9000 votes for three multimodal generative tasks. We constructed leaderboards for each task with these votes, identifying the state-of-the-art models as PlayGround V2.5, MagicBrush, and StableVideoDiffusion, respectively (until Oct 24th, 2024). Detailed analyses based on the votes are presented. For example, our plotted winning fraction heatmaps reveal that while the Elo rating system is generally effective, it can be biased by imbalances between "easy games" and "hard games". We also performed several case studies for qualitative analysis, demonstrating that users can provide preference votes from multiple evaluation aspects, which help distinguish subtle differences between the outputs and upload high-quality votes for Elo rating computation.

Automatically assessing the quality of generated visual content is a challenging problem for several reasons: (1) images and videos have many different aspects like visual quality, consistency, alignment, artifacts, etc. Such a multi-faceted nature makes the evaluation intrinsically difficult. (2) the supervised data is relatively scarce on the web. In our work, we release the user voting data as GenAI-Bench to enable further development in this field. Specifically, we calculate the accuracy between different image/video auto-raters (i.e. MLLM judges like GPT-4o, Gemini, etc.) with user preference to understand their judging abilities. Our results show that even the best MLLM, GPT-4o achieves at most \(49.19\%\) accuracy compared with human preference.

To summarize, our work's contributions include:

* GenAI-Arena, the first open platform to rank multi-modal generative AI based on user preferences.
* Discussion and case studies of collected user votes, showing the reliability of GenAI-Arena.
* GenAI-Bench, a public benchmark for judging MLLM's evaluation ability for generative tasks.

  
**Platform** &  **Text-To-Image** \\ **Generation** \\  &  **Text-Guided** \\ **Image Editing** \\  &  **Text-To-Video** \\ **Generation** \\  &  **Human Label** \\ **Transparency** \\  &  **Open/Public** \\ **Voting Process** \\  & 
 **Judging** \\ **MLLM judge** \\  \\  T2I-CompBench  & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ \\ HEM  & ✓ & ✗ & ✗ & ✓ & ✗ & ✗ \\ ImagenHub  & ✓ & ✓ & ✗ & ✓ & ✗ & ✗ \\ VBench  & ✗ & ✗ & ✓ & ✓ & ✗ & ✗ \\ EvalCrafter  & ✗ & ✗ & ✓ & ✓ & ✗ & ✗ \\ 
**GenAI-Arena** & ✓ & ✓ & ✓ & ✓ & ✓ \\   

Table 1: Comparison with different evaluation platforms on different properties.

Related Work

### Generative AI Evaluation Metrics

Numerous methods have been proposed to evaluate the performance of multi-modal generative models in various aspects. In the context of image generation, CLIPScore  is proposed to measure the text-alignment of an image and a text through computing the cosine similarity of the two embeddings from CLIP . IS  and FID  measure image fidelity by computing a distance function between real and synthesized data distributions. PSNR, SSIM  assess the image similarity. LPIPS  and the follow-up works [15; 16] measure the perceptual similarity of images. More recent works leverage the Multimodal Large Language Model (MLLM) as a judge. T2I-CompBench  proposed the use of miniGPT4  to evaluate compositional text-to-image generation task. TIFA  further adapted visual question answering to compute scores for the text-to-image generation task. VIEScore  leveraged MLLMs as a unified metric across image generation and editing tasks, reporting that MLLM has great potential in replacing human judges.

Metrics in similar fashions are also proposed for the video domain. For example, FVD  measures the coherence shifts and quality in frames. CLIPSIM  utilizes an image-text similarity model to assess the similarity between video frames and text. VBench  and EvalCrafter  also proposed different metrics for evaluating different aspects of the video generation task. However, these automatic metrics still lag compared with human preferences, achieving low correlation and thus giving doubts to their reliability.

### Generative AI Evaluation Platforms

While auto-metric focuses on evaluating a single model's performance, evaluation platforms aim to systematically rank a group of models. Recently, several benchmark suites have been developed to comprehensively assess generative AI models. For image generation, T2ICompBench  evaluates compositional text-to-image generation tasks, while HEIM  offers a holistic evaluation framework that measures text-to-image tasks across multiple dimensions, including safety and toxicity. Similarly, ImagenHub  evaluates text-to-image, image editing, and other prevalent image generation tasks in a unified benchmark suite. For video generation, VBench  and EvalCrafter  provide structured evaluation approaches ensuring rigorous assessment. Despite their functionality, these benchmarks rely on model-based evaluation metrics, which are less reliable than human evaluation.

To address this issue, variable model arenas have been developed to collect direct human preferences for ranking models. Chatbot Arena by LMsys  is the pioneering platform in this regard, setting the standard for evaluation. Subsequent efforts have led to the creation of arenas for vision-language models , TTS models , and tokenizers . However, there is no existing arena for generative AI models. To fill this gap, we propose GenAI-Arena as a complementary solution in this field.

## 3 GenAI-Arena: Design and Implementation

### Design

GenAI-Arena is designed to offer an intuitive and comprehensive evaluation platform for generative models, facilitating user interaction and participation. The platform is structured around three primary tasks: text-to-image generation, image edition, and text-to-video generation. Each task is supported by a set of features that include an anonymous and a non-anonymous battle playground, a direct generation tab, and a leaderboard as shown in Figure 2. These features are designed to cater to both casual users and researchers, ensuring a democratic and accurate assessment of model performance.

Standardized InferenceTo ensure a fair comparison between different models, we ported the highly dispersed codebase from the existing works and then standardized them into a unified format. During inference, we fixed the hyper-parameters and the prompt format to prevent per-instance prompt or hyper-parameter tuning, which makes the inference of different models fair and reproducible. Following ImagenHub , we build the new library of VideoGenHub (details in subsection A.5), which aims to standardize the inference procedure for different text-to-video and image-to-video models. We find the best hyper-parameters of these models to ensure their highest performance.

Voting RulesThe anonymous battle section is designed to ensure unbiased voting and accurate evaluation of generative models. The rules for this section are as follows:

1. Users input a prompt, which is then used to generate outputs from two anonymous models within the same category of task.
2. The generated outputs from the two anonymous models are presented side-by-side for comparison.
3. Users can vote based on their preference using the options: 1) left is better; 2) right is better; 3) tie; 4) both are bad. These four options are being used to calculate Elo ranking.
4. Once the user has made their decision, they click the Vote button to submit their vote. It is important to ensure that the identity of the models remains anonymous throughout the process. Votes will not be counted if the model identity is revealed during the interaction.

### Model Integration

In GenAI-Arena, we incorporate a diverse array of state-of-the-art generative models, covering a broad range of generative tasks including text-to-image generation, image edition, and text-to-video generation. To ensure comprehensive evaluations, the platform includes models that employ diverse underlying technologies, such as different types of architectures, training paradigms, training data and acceleration techniques. These variations can offer insights to understand these factors rigorously.

Text-to-Image GenerationIn Table 2, we list all the included text-to-image generation models. For example, SDXL, SDXL-Turbo, and SDXL-Lightning are all derived based on SDXL , while SDXL-Turbo  and SDXL-Lightning  adopt different distillation method. We also include diffusion transformer models  like PixArt-\(\) and PixArt-\(\). Playground V2 and Playground V2.5 are based on SDXL architecture, but trained by Playground.ai from scratch with an internal dataset. We have also included the latest released HunyuanDiT , FLUX.1-dev , FLUX.1-schnell .

Text-guided Image EditingIn Table 3, we list all the image editing models and approaches. Some of them are plug-and-play approaches without requiring any training, like Pix2PixZero , InfEdit , SDEdit , etc. These methods can be applied to a broad range of diffusion models. Some of the models like PnP  and Prompt2Prompt  require DDIM inversion, which takes

Figure 2: GenAI Arena User Voting Interface.

much longer time than the other approaches. We also include specialized trained image editing models like InstructP2P , MagicBrush  and CosXLEdit .

Text-to-Video GenerationIn Table 4, we list all the text-to-video generation models. We include different types of models. For example, AnimateDiff , ModelScope , Lavie  are initialized from SD-1.5 and continue trained by injecting a motion layer to capture the temporal relation between frames. In contrast, StableVideoDiffusion  and VideoCrafter2  are initialized from SD-2.1. Besides these models, we also include OpenSora , which utilizes a Sora-like diffusion transformer  architecture for joint space-time attention.

### Elo Rating System

Online Elo RatingThe Elo rating system models the probability of player \(i\) winning against player \(j\), based on their current ratings, \(R_{i}\) and \(R_{j}\) respectively, where \(i,j N\). We define a binary outcome \(Y_{ij}\) for each comparison between player \(i\) and player \(j\), where \(Y_{ij}=1\) if player \(i\) wins and \(Y_{ij}=0\) otherwise. The logistic probability is formulated as:

\[P(Y_{ij}=1)=-R_{i})/}} \]

   Model & Base & Len & FPS & Dataset & Resolution & \#Steps \\  AnimateDiff  & SD-1.5 & 2s & 8 & WebVid10M & 512\(\)512 & 25 \\ AnimateDiff-Turbo  & SD-1.5 & 2s & 8 & WebVid10M & 512\(\)512 & 4 \\ ModelScope  & SD-1.5 & 2s & 8 & WebVid10M & 256\(\)256 & 50 \\ LaVie  & SD-1.5 & 2s & 8 & Vimeo25M & 320\(\)512 & 50 \\ StableVideoDiffusion  & SD-2.1 & 2.5s & 10 & LVD-500M & 576\(\)1024 & 20 \\ VideoCrafter2  & SD-2.1 & 2s & 16 & WebVid10M & 320\(\)512 & 50 \\ T2V-Turbo  & VideoCrafter2 & 2s & 8 & WebVid10M & 320\(\)512 & 4 \\ OpenSora  & Pixat-\(\) & 2s & 16 & WebVid10M & 320\(\)512 & 50 \\ OpenSora v1.2.55 & Pixat-\(\) & 2s & 16 & WebVid10M & 320\(\)512 & 50 \\ CogVideoX-2B  & DiT & 2s & 8 & 35M videos + 2B images & 480\(\)720 & 50 \\   

Table 4: Overview of all text-to-video generation models.

   Model & Size & Method & Resolution & \#Steps \\  OpenJourney  & 1B & SD-2.1 + MidJourney Dataset & 512\(\)512 & 50 \\ LCM  & 1B & SD-2.1 + Consistency Distillation & 512\(\)512 & 4 \\ SDXL  & 3.5B & Latent Diffusion Model (LDM) & 1K\(\)1K & 50 \\ SDXL-Turbo  & 3.5B & LDM + Distillation & 1K\(\)1K & 1 \\ SDXL-Lightning  & 3.5B & LDM + Distillation & 1K\(\)1K & 4 \\ PixArt-\(\) & 0.6B & Diffusion Transformer (DiT) & 1K\(\)1K & 50 \\ PixArt-\(\) & 0.6B & DiT + Weak-to-Strong & 4K\(\)4K & 50 \\ StableCascade  & 1.5B + 3.6B & Wurstchen & 1K\(\)1K & 20+10 \\ Playground V2  & 3.5B & LDM & 1K\(\)1K & 50 \\ Playground V2.5  & 3.5B & LDM & 1K\(\)1K & 50 \\ FLUX1-dev  & 12B & Guidance-distilled DiT + Flow Matching & 1K\(\)1K & 20 \\ FLUX.1-schnell  & 12B & Timestep-distilled DiT + Flow Matching & 1K\(\)1K & 4 \\ Kolors  & 2.6B & LDM + ChatGLM3 & 1K\(\)1K & 50 \\ HumanPMT  & 1.5B & DiT + multilingual text encoder & 1K\(\)1K & 50 \\ Stable Diffusion 3  & 8B & Multimodal DiT & 1K\(\)1K & 50 \\ AuraFlow  & 6.8B & Flow-based Model & 1K\(\)1K & 50 \\   

Table 2: The overview of all text-to-image generation models.

   Model & Trained? & Method & Runtime \\  Pix2PixZero  & Zero-shot & Editing Direction Discovery + Attention Control & 21s \\ SDEdit  & Zero-shot & Iteratively Denoising through SDE & 13s \\ CycleDiffusion  & Zero-shot & Reconstructable Encoder for Stochastic DPMs & 9s \\ Prompt2Prompt  & Zero-shot & Prompt-based Cross-attention Control & 120s \\ PnP  & Zero-shot & Feature and Self-attention Injection & 120s \\ InfEdit  & Zero-shot & Consistent Model + Uni-Attention Control & 5s \\ InstructPix2Pix  & Trained & Instruction-based Fine-tuning with Synthetic Data & 12s \\ MagicBrush  & Trained & Instruction-based Fine-tuning with Annotated Data & 12s \\ CosXLEdit  & Trained & Cosine-Continuous EDM VPred schedule & 50s \\   

Table 3: Overview of all the image editing models.

where \(=400\) for Elo rating computation. After each match, a player's rating is updated using the formula:

\[R^{}_{i}=R_{i}+K(S(i,j)-E(i,j)) \]

where \(S(i,j)\) is the actual match outcome, \(S(i,j)=1\) for a win \(S(i,j)=0.5\) for a tie, and \(S(i,j)=0\) for a loss, and \(E(i,j)=P(Yi,j=1)\).

For example, given a model's Elo rating as 1200 and the other model's elo rating as 1100, then the estimated probability of the first model winning will be \(} 0.64\). In this way, we can have a direct understanding of the elo rating's meaning. This mapping from absolute number to the pairwise winning rate of two models gives a more straightforward understanding of the meaning of elo rating score.

Another design logic behind the Elo rating is that a higher-rated player should gain fewer points if they win a lower-rated player, but lose more if they lose the game, whereas the lower-rated player experiences the opposite. In this way, the order of a specific set of matches will significantly affect the final computed Elo rating, as the player's Elo rating and the rating gain of each match are both changing dynamically. This online Elo rating system might be good for real-world competitions, where players usually have less than 100 competitions a year. However the arena for AI models usually comes with thousands of votes (competitions), and the quality of votes is not ensured. Thus, it's necessary to acquire an order-consistent and more stable elo rating. To do this, we follow Chatbot Arena  to adopt the Bradley-Terry model  for a statistically estimated elo rating.

Bradley-Terry Model EstimationThe Bradley-Terry (BT) model  estimates Elo ratings using logistic regression and maximum likelihood estimation (MLE). Suppose there are \(N\) players and we have a series of pairwise comparisons, where \(W_{ij}\) is the number of times player \(i\) has won against player \(j\). The log-likelihood function for all pairwise comparisons is written as:

\[()=_{i,j N,i j}(W_{ij}Y_{ij} P(Y_{ij} =1)) \]

where \(=\{R_{1},,R_{N}\}\) represents the Elo ratings of each player. The Bradley-Terry model provides a stable statistical estimation of the players' ratings by consistently incorporating all pairwise comparisons, thus overcoming the limitations of direct Elo computation in online settings.

Since the BT model does not account for ties, we first duplicate all the votes, then allocate half of the "tie" votes to the scenario where model \(i\) wins (\(Y_{ij}=1\)) and the other half to the scenario where model \(j\) wins (\(Y_{ij}=0\)) in practice. We model the solver to be a logistic regression model and solve it via the LogisticRegression model from sklearn for the solving.

Confidence IntervalTo further investigate the variance of the estimated Elo rating, we use the "sandwich" standard errors described in Huber et al. . That is, for each round, we record the estimated Elo rating based on the same number of battles sampled from the previous round. This process continues for 100 rounds. We select the lowest sampled elo rating as the lower bound of the confidence interval, and the highest sampled elo rating as the upper bound of the elo rating.

Selection of battle pairWith a limited number of games, choosing which two players to match up is a crucial issue. The simplest approach, which we currently use, is to randomly select two players. However, this can introduce bias, with some models getting significantly more matches than others. A vote-aware selection system that increases the probability of selecting less-played models and lowers it for more-played ones is needed, and we plan to explore this in future Arena improvements.

### GenAI-Museum

Current GenAI-Arena runs the model on the Hugging Face Zero GPU system . As shown in Table 3, the time for a single generative inference usually ranges from 5 to 120 seconds. Unlike the auto-regression language model, where inference acceleration techniques like VLLM , SGLang  generate responses in less than a second, diffusion model community does not have such powerful infrastructure. Therefore, pre-computation becomes a necessary way to mitigate computational overhead and streamline user interaction.

To achieve this, we serve GenAI-Museum as a pre-computed data pool comprising various inputs from existing datasets or user collection, along with each model's output. Based on this, a "RandomSample" button shown in Figure 2 is additionally implemented to facilitate the random generation of prompts and the immediate retrieval of corresponding images or videos. This functionality operates by sending requests to our deployed GenAI-Museum every time "Random Sample" button is hit, receiving input and two random model's pre-computed outputs. In this way, we save the computation time on the GPU, enable users to do instant comparisons and votes on the UI, and balance the votes for each unique input so we gradually collect votes for a full combination of all models. The input prompts were sampled from ImagenHub  and VBench . To prevent the bias in the prompt distribution, we also periodically update the input prompts with the lastest collected real-world human votes. We make sure every prompt is filtered via NSFW detector before adding them.

## 4 Benchmarks and Results Discussion

### Arena Leaderboard

We report our leaderboard at the time of paper publishing in Table 5. For image generation, we collected 6300 votes in total. The currently top-1 model is Playground V2.5, released by Playground.ai, which follows the same architecture as SDXL but is trained with a private dataset. In contrast, SDXL only ranks in the thirteenth position, lagging significantly behind. Such finding highlights the importance of the training dataset. StableCascade is ranked in the sixth place in the leaderboard, which utilizes a highly efficient cascade architecture to lower the training cost. According to Wurstchen , StableCascade only requires a 10% training cost of SD-2.1, yet it can beat SDXL significantly on our leaderboard. This highlights the importance of the diffusion architecture to achieve strong performance. For image editing, a total of 1154 votes have been collected. MagicBrush, InFEdit, CosXLEdit, and InstructPix2Pix ranked higher as they can perform localized editing on images. PNP preserves the structure with feature injections, thus limiting the edit variety. The older methods such as Prompt-to-Prompt, CycleDiffusion, SDEdit, and Pix2PixZero, frequently result in completely different images during editing despite the high-quality images, which explains the lower ranking of these models. For text-to-video, there is a total of 2024 votes. StableVideoDiffusion leads with the highest Elo score, suggesting it is the most effective model. Close behind, CogVideoX-2B ranks second. The following VideoCrafter2 and AnimateDiff have very close elo scores, showing nearly equivalent capabilities. LaVie, OpenSora, ModelScope, and AnimateDiff-Turbo follow with decreasing scores, indicating progressively lower performance.

### Discussion and Insights

Winning Fraction and Elo RatingWe visualize the winning fraction heatmap in Figure 3, where each cell represents the actual winning fraction of Model A over Model B. The models are ordered by their Elo rating in the heatmap. Horizontally across each row, the winning fraction of Model A increases as the Elo rating of Model B decreases, demonstrating the effectiveness of the Elo rating system in ranking different models.

Specific cells in the heatmap reveal notable findings. For instance, although PlayGround 2.5 achieves the state-of-the-art (SOTA) Elo rating in the Text-to-Image task, its winning fraction over PixArt-\(\) is only \(0.58\), which is below 60%. The higher Elo rating of T2V-Turbo might be due to our Arena collecting more votes from "easy games" with low-ranked models and fewer from "harder

Table 5: GenAI-Arena Leaderboards. (Last updated on Oct 24th, 2024)games" with high-ranked models. For example, the number of battles between PlayGround V2.5 and SDXL-Turbo (\(93\)) is way more than PlayGround V2.5 with other models (around \(50\)) in Figure 4.

These anomalies highlight potential drawbacks of the Elo rating system: (1) a reliable and robust Elo rating requires a large amount of voting data, and (2) the estimated Elo rating may be biased by the imbalance between "easy games" and "harder games," as they carry similar weight in the estimation.

As shown in Figure 5, we observe that the average win rates of the top-ranked models are all quite similar, none exceeding 80%. This indicates that there is no dominant, highly powerful model in text-to-image, image editing, or text-to-video generation at this time. The community is still awaiting a "ChatGPT moment"--the release of a breakthrough model with transformative capabilities.

Quality assessment of collected human votesSince our arena users come from different backgrounds and have different preferences, we conduct an expert review on a small set of sampled human vote to ensure there are no severe quality issues of our collected votes. We let different authors review 50 items for each set. A total of 350 items from our GenAI-Bench are evaluated. During the

Figure 4: Battle count heatmap of different models for the three tasks in GenAI-Arena (without Ties)

Figure 5: Average Win Rate Against All Other Models (Assuming Uniform Sampling and No Ties)

Figure 3: Winning fraction heatmap of different models for the three tasks in GenAI-Arena

annotations, we skipped those bad items due to NSFW or technical issues, and we finally collected 303 valid evaluations. For each vote, 3 available labels for provided for annotating:

* Clearly Reasonable Vote: This vote will be clearly agreed by most of the people.
* Vague Vote: The current vote makes sense. But it's also reasonable if other vote is selected.
* Wrong Vote: This vote will be clearly disagreed by most of the people.

We report the distribution of valid votes in Table 5(a), and find that 86.57% of the votes are valid without NSFW issues. Among these valid votes, about 76.24% of the votes are clearly reasonable votes and 93.07% of the votes are either clearly reasonable or vaguely reasonable, as shown in Table 5(b). We believe this shows the reliability of our preference data.

Case StudyWe present case studies in Figure 6, showcasing the votes collected for three generative tasks. These cases demonstrate that GenAI-Arena users can provide high-quality votes, even for the most advanced models. For instance, in the text-to-image task, the image generated by PlayGround V2.5 was preferred over that of SDXL-Lightning for the prompt "a cute dog is playing with a ball," as the latter depicted two dogs instead of one. Users can clearly distinguish and vote based on the quality of the outputs, even when both models complete the task. In the image editing task, the edited image from Prompt2Prompt appeared more natural than the one from InfEdit, leading users to make a definitive vote. Similarly, votes collected for the text-to-video task were also of high quality.

## 5 GenAI-Bench

### Dataset

We applied Llama Guard  as an NSFW filter to ensure that the user input prompt is appropriate for a wide range of audiences and protects users of the benchmark from exposure to potentially harmful or offensive content. In the text-to-image generation task, we collect 4.3k anonymous votes in total and there are 1.7k votes left after filtering for the safe content. We observe a large amount of the prompt is filtered out due to sexual content, which takes up 85.6% of the abandoned data. In the text-guided image editing task, we collect 1.1k votes from users before filtering. After applying Llama Guard,

Table 6: Expert Review for 350 sampled human votes

Figure 6: Example of votes from users on the GenAI-Arena for the three generative tasks

there are 0.9k votes for the image edition being released. In this task, 87.5% of the unsafe inputs contain violent crimes, and the other 12.5% is filtered out resulting from sex-related crimes. For text-to-video generation task, our platform collects 1.2k votes before post-processing. After cleaning it with the NSFW filter, we release the remaining 1.1k votes. All of the unsafe data abandoned in this task is due to the sexual content. We released the current version of GenAI-Bench1 on the HuggingFace Dataset website, with an MIT license to allow the reuse with or without modification.

### GenAI-Bench Leaderboard

To construct the GenAI-Bench leaderboard, we prompt MLLMs to output preference labels of AI generated contents, where templates are defined in subsection A.6. Specifically, We selected MLLMs including GPT-4o , Gemini-1.5-Pro , Idefics2 , etc., and ask them to output 4 labels: "[[A>B]]", "[[B>A]]", "[[A=B=Good]]", and "[[A=B=Bad]]". We then compare them with actual human preference labels collected through the GenAI-Arena using the exact match metric. As shown in Table 7, open-source model still lag behind close-source MLLMs such as GPT-4o and Gemini, indicating a lack of generalization ability in vision reasoning of open-source MLLMs. We also tried models including Fuyu , Kosmos-2 , Otter , Mantis , etc., but found that they cannot follow the instruction well to output reasonable labels.

## 6 Conclusion

In this paper, we introduced GenAI-Arena, an open platform designed to rank generative models across text-to-image, image editing, and text-to-video tasks based on user preference. unlike other platforms, GenAI-Arena is driven by community voting to ensure transparency and sustainable operation. We employed the side-by-side human voting method to evaluate the models and collected over 9000 votes starting from February 11th, 2024. We compiled an Elo leaderboard with the votings and found that PlayGround V2.5, MagicBrush, and StableVideoDiffusion are the current state-of-the-art models in the three tasks (until Oct 24th, 2024). Analysis based on the collected votes shows that while the Elo rating is generally functional, but can biased by the imbalance of the "easy games" and "hard games". Our expert review of 350 sampled human votes confirmed that 93.07% of the votes can be viewed as either clearly reasonable or vaguely reasonable, demonstrating the high quality of our collected votes What's more, we also released the human preference voting as GenAI-Bench. We prompt the existing MLLMs to evaluate the generated images and videos on GenAI-Bench and compute the accuracy with human voting. The experiment showed that the open-source MLLMs achieve very low performance, even the best model GPT-4o can only achieve \(49.19\%\) accuracy. This is mostly because their lack of instruction-following and reasoning ability in complex vision scenarios. In the future, we will continue collecting human votes to update the leaderboard, helping the community to keep track of the research progress. We also plan to develop a more robust MLLM to better approximate human ratings in GenAI-Bench.

   Model & Image Generation & Image Editing & Video Generation & Average \\  Random & 25.36 & 25.90 & 25.16 & 25.47 \\  Idefics1  & 0.81 & 5.66 & 0.19 & 2.22 \\ InstructBLIP  & 3.11 & 19.80 & 3.74 & 8.89 \\ QwenVL  & 26.63 & 14.91 & 2.15 & 14.56 \\ CogVLM  & 29.34 & 0.00 & 24.60 & 17.98 \\ VideoLoLAVA  & 37.75 & 26.66 & 0.00 & 21.47 \\ BLIP-2  & 26.34 & 26.01 & 16.93 & 23.09 \\ MiniCPM-V2.5  & 37.81 & 25.24 & 6.55 & 23.20 \\ LLaVA-1.6-7B  & 22.65 & 25.35 & 21.70 & 23.24 \\ Idefics2  & 42.25 & 27.31 & 16.46 & 28.67 \\ LLaVA-1.5-7B  & 37.00 & 26.12 & 30.40 & 31.17 \\ Gemini-1.5-Pro  & 44.67 & **55.93** & 46.21 & 48.94 \\ GPT-4o  & **45.59** & 53.54 & **48.46** & **49.19** \\   

Table 7: GenAI-Bench leaderboard designed to benchmark MLLMs’s ability in judging the quality of AI generative contents by comparing with human preferences. Numbers are accuracy (%).