# Metric Flow Matching for Smooth Interpolations on the Data Manifold

Kacper Kapusniak\({}^{1}\), Peter Potaptdnik\({}^{1}\), Teodora Reu\({}^{1}\), Leo Zhang\({}^{1}\),

Alexander Tong\({}^{2,3}\), Michael Bronstein\({}^{1,4}\), Avishek Joey Bose\({}^{1,2}\), Francesco Di Giovanni\({}^{1}\)

\({}^{1}\)University of Oxford, \({}^{2}\)Mila, \({}^{3}\)Universite de Montreal, \({}^{4}\)AITHYRA

Corresponding author: kacper.kapusniak@cs.ox.ac.uk

Code is available at [https://github.com/kksniak/metric-flow-matching](https://github.com/kksniak/metric-flow-matching)

###### Abstract

Matching objectives underpin the success of modern generative models and rely on constructing conditional paths that transform a source distribution into a target distribution. Despite being a fundamental building block, conditional paths have been designed principally under the assumption of _Euclidean geometry_, resulting in straight interpolations. However, this can be particularly restrictive for tasks such as trajectory inference, where straight paths might lie outside the data manifold, thus failing to capture the underlying dynamics giving rise to the observed marginals. In this paper, we propose Metric Flow Matching (MFM), a novel simulation-free framework for conditional flow matching where interpolants are approximate geodesics learned by minimizing the kinetic energy of a data-induced Riemannian metric. This way, the generative model matches vector fields on the data manifold, which corresponds to lower uncertainty and more meaningful interpolations. We prescribe general metrics to instantiate MFM, independent of the task, and test it on a suite of challenging problems including LiDAR navigation, unpaired image translation, and modeling cellular dynamics. We observe that MFM outperforms the Euclidean baselines, particularly achieving SOTA on single-cell trajectory prediction.

## 1 Introduction

A central task in many natural and scientific domains entails the inference of system dynamics of an underlying (physical) process from noisy measurements. A core challenge, in these application domains such as biomedical ones--e.g. tracking health metrics (Oeppen and Vaupel, 2002) or diseases (Hay et al., 2021)--is that one typically lacks access to entire time-trajectories and can only leverage cross-sectional samples. An even more poignant example is the case of single-cell RNA sequencing (Macosko et al., 2015; Klein et al., 2015), where measurements are _sparse_ and _static_, due to the procedure being expensive and destructive. Consequently, the nature of these tasks demands the design of frameworks capable of reconstructing the temporal dynamics of a system (e.g. cells) from observed time marginals that contain finite samples. This overarching problem specification is referred to as **trajectory inference**(Hashimoto et al., 2016; Lavenant et al., 2021).

To address this challenge, we rely on matching objectives, a powerful generative modeling paradigm encompassing successful approaches including diffusion models (Sohl-Dickstein et al., 2015; Song et al., 2021), flow matching (Lipman et al., 2023; Liu et al., 2022; Albergo and Vanden-Eijnden, 2022), and finding a Schrodinger Bridge (Schrodinger, 1932; Leonard, 2013). Specifically, to reconstruct the unknown dynamics \(t p_{t}^{*}\) between observed time marginals \(p_{0}\) and \(p_{1}\), we leverage Conditional Flow Matching (CFM) (Tong et al., 2023), a simulation-free framework which constructs a probability path \(p_{t}\) through interpolants \(x_{t}\) connecting samples of \(p_{0}\) to samples of \(p_{1}\). In general, \(x_{t}\) is designed under the assumption of _Euclidean geometry_, resulting in _straighttrajectories. However, in light of the widely accepted "manifold hypothesis" (Tenenbaum et al., 2000; Belkin and Niyogi, 2003), the target time-evolving density \(p_{t}^{*}\) is supported on a _curved_ low-dimensional manifold \(^{d}\)--a condition satisfied by cells in the space of gene expressions (Moon et al., 2018). As such, straight interpolants stray away from the data manifold \(\), leading to reconstructions \(p_{t}\) that fail to model the nonlinear dynamics generated by the underlying process.

**Present work**. We aim to design interpolants \(x_{t}\) that stay on the data manifold \(\) associated with the underlying dynamics. Nonetheless, parameterizing the lower-dimensional manifold \(\) is prone to instabilities (Loaiza-Ganem et al., 2022) and may require multiple coordinate systems (Salmona et al., 2022). Accordingly, we adopt the "metric learning" approach (Xing et al., 2002; Hauberg et al., 2012), where we still work in the ambient space \(^{d}\), but equip it with a _data-dependent Riemannian metric_\(g\) whose shortest-paths (geodesics) stay close to the data points, and hence to \(\)(Arvanitidis et al., 2021). We introduce Metric Flow Matching (MFM), a simulation-free generalization of CFM where interpolants \(x_{t}\) are _learned_ by minimizing a geodesic loss that penalizes the velocity measured by the metric \(g\). As a result, \(x_{t}\) approximates the geodesics of \(g\) and hence tends towards the data, leading to the evaluation of the matching objective in regions of lower uncertainty. Therefore, the resulting probability path \(p_{t}\) is supported on the data manifold for all \(t\), giving rise to a more natural reconstruction of the underlying dynamics in the trajectory inference task, as depicted in Figure 1.

Our **main contributions** are:

1. We prove that given a dataset \(^{d}\), one can always construct a metric \(g\) on \(^{d}\) such that the geodesics connecting \(x_{0}\) sampled from \(p_{0}\) to \(x_{1}\) sampled from \(p_{1}\), always lie close to \(\) (SS3.1).
2. We propose Metric Flow Matching, a novel framework generalizing CFM to the Riemannian manifold associated with any data-dependent metric \(g\). In MFM, before training the matching objective, we learn interpolants that stay close to the data by minimizing a cost induced by \(g\) (SS3). MFM is simulation-free and stays relevant when geodesics lack closed form and hence Riemannian Flow Matching (Chen and Lipman, 2023) is not easily applicable (SS3.2).
3. We prescribe a universal way of designing a data-dependent metric, independent of the specifics of the task, which enforces interpolants \(x_{t}\) to stay supported on the data manifold (SS4.1). Through the proposed metric, \(x_{t}\) depends on the entire data manifold and not just the endpoints \(x_{0}\) and \(x_{1}\) sampled from the marginals. By accounting for the Riemannian geometry induced by the data, MFM generalizes existing approaches that construct \(x_{t}\) by minimizing energies (SS4.2).
4. We propose OT-MFM, an instance of MFM that relies on Optimal Transport to draw samples from the marginals (SS4). Empirically, we show that OT-MFM attains SOTA results for reconstructing single-cell dynamics (SS5). Additionally, we validate the versatility of the framework through tasks such as 3D navigation with LiDAR point clouds and unpaired translation of images.

## 2 Preliminaries and Setting

We review Conditional Flow Matching, which forms the basis of Metric Flow Matching SS3. Next, we recall basic notions of Riemannian geometry, with emphasis on constructing geodesics.

**Notation and convention**. We let \(^{d}\) be the ambient space where data points are embedded. A random variable \(x\) with a distribution \(p\) is denoted as \(x p(x)\). A function \(\) depending on time \(t\), space \(x\) and learnable parameters \(\), is denoted by \(_{t,}(x)\), and its time derivative by \(_{t,}\). We also let \(_{x_{0}}(x)\) be the Dirac function centered at \(x_{0}\) and assume that all distributions are absolutely continuous, which allows

Figure 1: In orange and violet the source and target distributions. On the left, straight interpolations vs interpolations following a data-dependent Riemannian metric. On the right, densities of reconstructed marginals at time \(t=\), using Conditional Flow Matching and Metric Flow Matching (MFM), respectively. MFM provides a more meaningful reconstruction supported on the data manifold.

us to use densities. We denote the space of symmetric, positive definite \(d d\) matrices as \((d)\), and let \((x)(d)\) be the coordinate representation of a Riemannian metric at some point \(x\).

**Conditional Flow Matching**. We consider a source distribution \(p_{0}\) and a target distribution \(p_{1}\) defined on \(^{d}\). We are interested in finding a map \(f\) that pushes forward \(p_{0}\) to \(p_{1}\), i.e. \(f_{\#}p_{0}=p_{1}\). In line with Continuous Normalizing Flows (Chen et al., 2018), we look for a map of the form \(f=_{1}\), where the time-dependent diffeomorphism \(_{t}:^{d}^{d}\) is the flow generated by a vector field \(u_{t}\), i.e. \(d_{t}(x)/dt=u_{t}(_{t}(x))\), with \(_{0}(x)=x\), for all \(x^{d}\). If we define the density path \(p_{t}=[_{t}]_{\#}p_{0}\), then \(p_{t}\) and \(u_{t}\) satisfy the continuity equation and we say that \(p_{t}\) is _generated_ by \(u_{t}\).

If density path \(p_{t}\) and vector field \(u_{t}\) are known, we could regress a vector field \(v_{t,}\), modeled as a neural network, to \(u_{t}\) by minimizing \(_{}()=_{t,p_{t}(x)}\|v_{t,}(x)-u_{ t}(x)\|^{2}\). Since \(p_{t}\) and \(u_{t}\) are typically intractable though, Conditional Flow Matching (CFM) (Lipman et al., 2023; Albergo and Vanden-Eijnden, 2022; Liu et al., 2022) simplifies the problem by assuming that \(p_{t}\) is a mixture of conditional paths: \(p_{t}(x)= p_{t}(x|z)q(z)dz\), where \(z=(x_{0},x_{1})\) is sampled from a joint distribution \(q\) with marginals \(p_{0}\) and \(p_{1}\), and \(p_{t}(x|z)\) satisfy \(p_{0}(x|z)_{x_{0}}(x)\) and \(p_{1}(x|z)_{x_{1}}(x)\). If \(_{t}(x|x_{0},x_{1})\) denotes the flow generating \(p_{t}(x|z)\), then the CFM objective is

\[_{}()=_{t,(x_{0},x_{1}) q}\|v_ {t,}(x_{t})-_{t}\|^{2}, \]

where \(x_{t}:=_{t}(x_{0}|x_{0},x_{1})\) are the _interpolants_ from \(x_{0}\) to \(x_{1}\). Since \(_{}\) and \(_{}\) have same gradients (Lipman et al., 2023; Tong et al., 2023b), we can use the tractable conditional paths to learn \(v_{}\). As in SS3 we design \(x_{t}\) to approximate geodesics, we review key notions from Riemannian geometry.

**Riemannian manifolds**. A Riemannian manifold \((,g)\) is a smooth orientable manifold \(\) equipped with a smooth map \(g\) assigning to each point \(x\) an inner product \(,_{g}\) defined on the tangent space of \(\) at \(x\). We let \((x)(d)\) be the matrix representing \(g\) in coordinates, at any point \(x\), with \(d\) the dimension of \(\). Integration is taken with respect to the volume form \(d\) (see Appendix SSA for details). A continuous, positive function \(p\) is then a probability density on \((,g)\), i.e. \(p()\), if \( p(x)d(x)=1\). Naturally, it is possible to define curves \(_{t}\), indexed by time \(t\). A _geodesic_ is then the curve \(_{t}^{*}\) that minimizes the distance with respect to \(g\). Specifically, the geodesic connecting \(x_{0}\) to \(x_{1}\) in \(\), can be found by minimizing the length functional:

\[_{t}^{*}=*{arg\,min}_{_{t}:\,_{0}=x_{0}, _{1}=x_{1}}_{0}^{1}\|_{t}\|_{g(_{t})}dt, \|_{t}\|_{g(_{t})}:=_{t},(_{t})_{t}}, \]

where \(_{t}\) is velocity. From eq. (2), we see that geodesics tend towards regions where \(\|(x)\|\) is small. In Euclidean geometry (i.e., \(=^{d}\) and \((x)=\)), \(_{t}^{*}\) is a straight line with constant speed.

## 3 Metric Flow Matching

We introduce Metric Flow Matching (MFM), a new simulation-free framework that generalizes CFM by constructing probability paths supported on the data manifold. MFM learns interpolants \(x_{t}\) in eq. (1) whose velocity minimizes a data-dependent Riemannian metric assigning a lower cost to regions with high data concentration. Consequently, the CFM objective is evaluated in areas of low data uncertainty, and the corresponding vector field, \(v_{}\) in eq. (1), learns to pass through these regions.

We structure this section as follows. In SS3.1 we discuss the trajectory inference problem and how straight interpolants \(x_{t}\) in CFM result in undesirable probability paths whose support is not defined on the data manifold. We remedy this problem by choosing to represent the data manifold via a Riemannian metric in \(^{d}\), such that geodesics avoid straying away from the samples in the training set. In SS3.2 we introduce MFM and compare it with Riemannian Flow Matching (Chen and Lipman, 2023).

### Metric learning

Assume that \(p_{0}\) and \(p_{1}\) are empirical distributions and that we have access to a dataset of samples \(=\{x_{i}\}_{i=1}^{N}\)--in practice \(\) is constructed concatenating samples from both the source and target distributions. We are interested in the problem of **trajectory inference**(Lavenant et al., 2021), where we need to reconstruct an unknown dynamics \(t p_{t}^{*}\), with observed time marginals \(p_{0}^{*}=p_{0}\) and \(p_{1}^{*}=p_{1}\)--the extension to multiple timepoints is easy. In many realistic settings, including single-cell RNA sequencing (Macosko et al., 2015), time measurements are sparse, and leveraging biases from the data is hence key to achieving a faithful reconstruction. For this reason, we invoke the "manifold hypothesis" (Bengio et al., 2013), where the data arises from a low-dimensional manifold \(^{d}\)--a property satisfied by cells embedded in the space of gene expressions (Moon et al., 2018):

\[(p_{t}^{*}):=\{x^{d}:p_{t}^{*}(x)>0\} , t 0. \]

Since any regular time dynamics can be described using the continuity equation generated by some vector field \(v_{t}^{*}\)(Ambrosio et al., 2005, Theorem 8.3.1), (Neklyudov et al., 2023a), we rely on CFM, and approximate \(p_{t}^{*}\) via the probability path \(p_{t}\) associated with \(v_{t,}\) in eq.1. From eq.3, it follows that a valid reconstruction entails \((p_{t})\). As the support of \(p_{t}\) is determined by the interpolants \(x_{t}\) in eq.1, we need \(x_{t}\) to be constrained to stay on \(\). However, in the classical CFM setup, this condition is violated since interpolants are often _straight_ lines, _agnostic_ of the data's support: \(x_{t}=tx_{1}+(1-t)x_{0}\)(Tong et al., 2023b; Shaul et al., 2023), with \(x_{0},x_{1}\) sampled from the marginals. In this case, if \(p_{t}(x|x_{0},x_{1})_{x_{t}}(x)\), then the support of \(p_{t}\) satisfies

\[(p_{t})\{y^{d}:\ (x_{0},x_{1}) q:\ y=tx_{1}+(1-t)x_{0}\}.\]

However, the dynamics \(t p_{t}^{*}\) is often _nonlinear_, as for single-cells (Moon et al., 2018), meaning that \((p_{t})\). Straight interpolants are hence too restrictive and should instead be supported on \(\) so to replicate _actual_ trajectories from \(x_{0}\) to \(x_{1}\), which are generated by the underlying process.

Operating on a lower-dimensional \(\) is challenging though, since it requires different coordinate systems (Schonsheck et al., 2019, Salmona et al., 2022) and may incur overfitting (Loaiza-Ganem et al., 2022, 2024). Nonetheless, a key property posited by the "manifold hypothesis" is that \(\) concentrates around the data points \(\)(Arvanitidis et al., 2022, Chadebec and Allassonniere, 2022). As such, interpolants \(x_{t}\) should remain close to \(\). Therefore, instead of changing the dimension, we design \(x_{t}\) to minimize a cost in \(^{d}\) that is lower on regions close to \(\). We achieve this following the "metric learning" approach, (Hauberg et al., 2012) where we equip \(^{d}\) with a suitable Riemannian metric \(g\).

**Definition 1**.: _A data-dependent metric \(g\) on \(^{d}\) is a smooth map \(g:^{d}(d)\) parameterized by the dataset \(=\{x_{i}\}_{i=1}^{N}^{d}\), i.e. \(g(x)=(x;)(d), x^{d}\)._

We describe a specific metric \(g\) in SS4, and note that \(g\) can also enforce constraints from the task (SS7). Naturally, if \((x;)=\), we recover the Euclidean metric. We show that we can always construct \(g\) so that the geodesics \(_{t}^{*}\) stay close to the data \(\). For details, we refer to TheoremB.1 in Appendix SSB.

**Proposition 1** (Informal).: _Given a dataset \(^{d}\), let \(g\) be any metric such that: (i) The eigenvalues of \((x;)\) do not approach zero when \(x\) is distant from \(\); (ii) \(\|(x;)\|\) is sufficiently small if \(x\) is close to \(\). Then for each \((x_{0},x_{1}) q\), the geodesic \(_{t}^{*}\) connecting \(x_{0}\) to \(x_{1}\) stays close to \(\)._

Given \(g\) as in the statement, if \(x_{t}=_{t}^{*}\) and \(p_{t}(x|x_{0},x_{1})_{x_{t}}(x)\), then the probability path \(p_{t}\) generated by \(v_{}\) in eq.1 has support near \(\), i.e. \((p_{t})\) lies close to the data manifold \(\), which is our goal. Unfortunately, for all but the most trivial metrics \(g\) on \(^{d}\), it is not possible to obtain closed-form expressions for the geodesics \(_{t}^{*}\). As such, finding the geodesic \(_{t}^{*}\) necessitates the expensive _simulation_ of second-order nonlinear Euler-Lagrange equations (Hennig and Hauberg, 2014).

### Parameterization and optimization of interpolants

Consider a metric \(g\) on \(^{d}\) as in Definition1 whose geodesics \(_{t}^{*}\) lie close to \(\) as per Proposition1. We propose a _simulation-free_ approximation to paths \(_{t}^{*}\) by introducing interpolants of the form

\[x_{t,}=(1-t)x_{0}+tx_{1}+t(1-t)_{t,}(x_{0},x_{1}), \]

where \(\) are the parameters of a neural network \(_{t,}\) acting as a nonlinear "correction" for straight interpolants. Note that the boundary conditions are met, i.e. that the path \(x_{t,}\) recovers both \(x_{0}\) and \(x_{1}\) at times \(t=0\) and \(t=1\), respectively. In fact, \(x_{t,}\) reduces to the convex combination between \(x_{0}\) and \(x_{1}\) if \(_{t,}=0\), meaning that \(x_{t,}\) strictly generalize the straight paths used in (Lipman et al., 2023; Liu et al., 2022). Towards the goal of learning \(\) so that \(x_{t,}\) approximates the geodesic \(_{t}^{*}\), we note that \(_{t}^{*}\) can be characterized as the path minimizing the convex functional \(_{g}\) below:

\[_{t}^{*}=*{arg\,min}_{_{t}:_{0}=x_{0},_{1} =x_{1}}_{g}(_{t}),_{g}(_{t}):= _{t}[_{t}^{}(_{t};) _{t}]. \]

Since \(_{t}^{*}\) minimizes \(_{g}\) over all paths connecting \(x_{0}\) to \(x_{1}\), and \(x_{t,}\) in eq.4 satisfies these boundary conditions, we can estimate \(\) by simply minimizing the convex functional \(_{g}\) over \(x_{t,}\), which leads to the following geodesic objective (the training procedure is reported in Algorithm1):\[_{g}():=_{(x_{0},x_{1}) q}[_{g}(x_{t, })]=_{(x_{0},x_{1}) q,t}[(_{t,})^{} (x_{t,};)_{t,}]. \]

Given interpolants that approximate \(_{t}^{*}\) and hence stay close to the data manifold, we can then rely on the CFM objective in eq.1 to regress the vector field \(v_{}\). Since \(g\) makes the ambient space \(^{d}\) into a Riemannian manifold \((^{d},g)\), we need to replace the norm \(\) in eq.1 with the one \(_{g}\) induced by the metric, and rescale the marginals \(p_{0},p_{1}\) using the volume form induced by \(g\), so to extend \(p_{0},p_{1}\) to densities in \((^{d},g)\) (see Appendix SSA). Similar arguments work for the joint distribution \(q\). We can finally introduce our framework Metric Flow Matching that generalizes Conditional Flow Matching (1) to leverage _any_ data-dependent metric \(g\), by using interpolants \(x_{t,}\) (4), whose parameters \(\) are obtained from minimizing the geodesic cost \(_{g}\). The MFM objective can be stated as:

\[_{}()=_{t,(x_{0},x_{1}) q}[  v_{t,}(x_{t,^{*}})-_{t,^{*}}_{g(x _{t,^{*}})}^{2}],^{*}=*{arg\,min}_{} _{g}(). \]

A description of Metric Flow Matching is given in Algorithm2. As the interpolants \(x_{t,}\) approximate geodesics of \(g\), in MFM the vector field \(v_{t,}\) is regressed on the data manifold \(\), where the underlying dynamics \(p_{t}^{*}\) is supported, resulting in better reconstructions. Crucially, eq.6 only depends on time derivatives of \(x_{t,}\). Therefore, MFM avoids simulations and simply requires training an additional (smaller) network \(_{t,}\) in eq.4, which can be done _prior_ to training \(v_{t,}\).

**MFM versus Riemannian Flow Matching**. While CFM has already been extended to Riemannian manifolds in the Riemannian Flow Matching (RFM) framework of Chen and Lipman (2023), MFM crucially differs from RFM in two ways. To begin with, MFM relies on the data or task inducing a Riemannian metric on the ambient space which is then accounted for in the matching objective. This is in sharp contrast to RFM, which instead assumes that the metric of the ambient space is _given_ and is _independent_ of the data points. Secondly, RFM does not incorporate conditional paths that are learned. In fact, in the scenario above where \(g\) is a metric whose geodesics \(_{t}^{*}\) stay close to the data support, adopting RFM would entail replacing the MFM objective \(_{}\) in (7) with

\[_{}()=_{t,(x_{0},x_{1}) q} v _{t,}(_{t}^{*})-_{t}^{*}_{g(_{t}^{* })}^{2}. \]

However, as argued above, for almost any metric \(g\) on \(^{d}\), geodesics \(_{t}^{*}\) can only be found via _simulations_, which in high dimensions inhibits the easy application of RFM. Conversely, MFM designs interpolants that minimize a geodesic cost (6) and hence approximate \(_{t}^{*}\) without incurring simulations.

```
0: coupling \(q\), initialized network \(_{t,}\), data-dependent metric \((;)\)
1:while Training do
2: Sample \((x_{0},x_{1}) q\) and \(t(0,1)\)
3:\(x_{t,}(1-t)x_{0}+tx_{1}+t(1-t)_{t,}(x_{0},x_{1})\)\(\)eq.4
4:\(_{t,} x_{1}-x_{0}+t(1-t)_{t,}(x_{0},x_{1})+(1- 2t)_{t,}(x_{0},x_{1})\)
5:\(()(_{t,})^{}(x_{t,}; )_{t,}\)\(\)Estimate of objective \(_{g}()\) from eq.6
6: Update \(\) using gradient \(_{}()\)return (approximate) geodesic interpolants parametrized by \(_{t,}\)
```

**Algorithm 1** Pseudocode for training of geodesic interpolants

## 4 Learning Riemannian Metrics in Ambient Space

In this section, we focus on a concrete choice of \(g\), which can easily be used within MFM (SS4.1). We also introduce OT-MFM, a variant of MFM that leverages Optimal Transport to find a coupling \(q\) between \(p_{0}\) and \(p_{1}\). Next, in SS4.2 we discuss how MFM generalizes recent works that find interpolants that minimize energies by accounting for the Riemannian geometry induced by the data.

### A family of diagonal metrics: Lamb and RBF

We consider a family of metrics \(g_{}\) as in Definition1, independent of specifics of the data type or task. For the ease of exposition, we omit to write the explicit dependence on the dataset \(=\{x_{i}\}_{i=1}^{N}\)Given \(>0\), we let \(x g_{}(x)_{}(x)=(( (x))+)^{-1}\) be the "LAND" metric, where

\[h_{}(x)=_{i=1}^{N}(x_{i}^{}-x^{})^{2}-\|^{2}}{2^{2}}, 1 d, \]

with \(\) the kernel size. We emphasize that \(_{}(x)\) was introduced by Arvanitidis et al. (2016)--from which we borrow the name--but is algorithmic use in MFM is fundamentally different. In line with Proposition 1, we see that \(\|_{}(x)\|\) is larger away from \(\), thus pushing geodesics (2) to stay close to the data support, as desired. While \(g_{}\) is flexible and directly accounts for all the samples in \(\), in high-dimension selecting \(\) in eq. (9) can be challenging. For these reasons, we follow Arvanitidis et al. (2021) and introduce a variation of \(g_{}\) of the form \(_{}(x)=((}(x))+ )^{-1}\), where

\[_{}(x)=_{k=1}^{K}_{,k}(x)-}{2}\|x-_{k}\|^{2}, 1 d, \]

with \(K\) the number of clusters with centers \(_{k}\) and \(_{,k}\) the bandwidth of cluster \(k\) for channel \(\) (see Appendix SSC for details). In particular, \(h_{}\) is realized via a Radial Basis Function (RBF) network (Que and Belkin, 2016), where \(_{,k}>0\) are _learned_ to enforce the behavior \(h_{}(x_{i}) 1\) for each data point \(x_{i}\) so that the resulting metric \(g_{}\) assigns lower cost to regions close to the centers \(_{k}\). In our experiments, we then rely on \(g_{}\) in low dimensions, and instead use \(g_{}\) in high dimensions. We also note that all metrics considered are diagonal, which makes MFM more efficient. Explicitly, given the interpolants in eq. (4), the geometric loss in eq. (6) with respect to \(g_{}\) can be written as:

\[_{g_{}}()=_{(x_{0},x_{1}) q}[\, _{g_{}}(x_{t,})]=_{t,(x_{0},x_{1}) q} [_{=1}^{d}_{t,})_{}^{2}}{_{ }(x_{t,})+}]. \]

We see that the loss acts as a geometric regularization, with the velocity \(_{t,}\) penalized more in regions away from the support of the dataset \(\), i.e. when \(_{,k}\|x_{t,}-_{k}\|\) is large for all centers \(_{k}\) in eq. (10).

**OT-MFM.** In eq. (7), samples \(x_{0},x_{1}\) follow a joint distribution \(q\), with marginals \(p_{0}\) and \(p_{1}\). Since we are interested in the problem of trajectory inference, with emphasis on single-cell applications where the principle of least action holds (Schiebinger, 2021), we focus on a coupling \(q\) that minimizes the distance in probability space between the source and target distributions. Namely, we consider the case where \(q\) is the 2-Wasserstein optimal transport plan \(^{*}\) from \(p_{0}\) to \(p_{1}\)(Villani et al., 2009):

\[^{*}=*{arg\,min}_{}\,_{^{d} ^{d}}c^{2}(x,y)d(x,y), \]

where \(\) are the probability measures on \(^{d}^{d}\) with marginals \(p_{0}\) and \(p_{1}\) and \(c\) is any cost. While we could choose \(c\) based on \(g_{}\), we instead select \(c\) to be the \(L_{2}\) distance in \(^{d}\) to avoid additional computations and so we can study the role played by \(x_{t,}\) even when \(q=^{*}\) is agnostic of the data manifold. We then propose the OT-MFM objective, where \(^{*}\) minimizes the geodesic loss in eq. (11):

\[_{_{}}()=_{t,(x_{0},x_{1}) ^{*}}[\|v_{t,}(x_{t,^{*}})-_{t,^{*}}\|_{g_{ }(x_{t,^{*}})}^{2}]. \]

We note that the case of \(g_{}\) is dealt with similarly. Additionally, different choices of the joint distribution \(q\), beyond Optimal Transport, can be adapted from CFM (Tong et al., 2023) to MFM in eq. (7).

### Understanding MFM through energies

In MFM we learn interpolants \(x_{t}\) that are _optimal_ according to eq. (6). Previous works have studied the "optimality" of interpolants, but have ignored the data manifold. Shaul et al. (2023) proposed to learn interpolants \(x_{t}\) that minimize the **kinetic energy**\((_{t})=_{t,(x_{0},x_{1}) q}\|_{t}\|^{2}\). However, \(\) assigns each point in space the same cost, _independent of the data_, and leads to straight interpolants that may stray away from \(\). Conversely, our objective in eq. (6) can equivalently be written as

\[_{g}()=_{(x_{0},x_{1}) q}\,[\,_{g}(x_{t,})]_{t,(x_{0},x_{1}) q}[\,\|_{t,}\|_{g(x_{ t,})}^{2}].\]

As a result, the geodesic loss \(_{g}()_{g}(_{t,})\) is _precisely_ the kinetic energy of vector fields with respect to \(g\) and hence accounts for the cost induced by the data. In fact, choosing \((x)=\), recovers\(\). We note that the parameterization \(x_{t,}\) in eq. (4) is more expressive than \(x_{t}=a(t)x_{1}+b(t)x_{0}\), which is studied in Albergo and Vanden-Eijnden (2022); Shaul et al. (2023). Besides, \(x_{t,}\) not only depends on the endpoints \(x_{0},x_{1}\) but, implicitly, on all the data points \(\) through metrics such as \(g_{}\).

**Data-dependent potentials.** Energies more general than the kinetic one \(\) have been considered in Neklyudov et al. (2023); Liu et al. (2024); Neklyudov et al. (2023). In particular, adapting GSBM (Liu et al., 2024) from the stochastic Schrodinger bridge setting to CFM, entails designing interpolants \(x_{t}\) that minimize \((x_{t},_{t})=(_{t})+V_{t}(x_{t})\), with \(V_{t}\) a potential enforcing additional constraints. However, GSBM does not prescribe a general recipe for \(V_{t}\) and instead leaves to the modeler the task of constructing \(V_{t}\), based on applications. Conversely, MFM relies on a Riemannian approach to propose an _explicit_ objective, i.e. eq. (11), that holds irrespective of the task and can be learned _prior_ to regressing the vector field \(v_{t,}\). In fact, eq. (11) can be rewritten as

\[_{g_{}}()=_{t,(x_{0},x_{1}) q}[ \|_{t,}\|^{2}+V_{t,}(x_{t,},x_{0},x_{1})]. \]

where \(V_{t,}\) is parametric function depending on the boundary points \(x_{0},x_{1}\) (see Appendix SSC.1 for an expression). In contrast to GSBM, MFM designs interpolants \(x_{t,}\) that jointly minimize \(\) and a potential \(V_{t,}\) that is not fixed but also updated with the same parameters \(\) to bend paths towards the data.

## 5 Experiments

We test Metric Flow Matching on different tasks: artificial dynamic reconstruction and navigation through LiDAR surfaces SS5.1; unpaired translation between classes in images SS5.2; reconstruction of cell dynamics. Further results and experimental details can be found in Appendices SSD, SSE and SF.

**The model.** In all the experiments, we test the OT-MFM method detailed in SS4.1. As argued in SS4.1, for high-dimensional data, we leverage the RBF metric (10) and hence train with the objective (13). We refer to this model as OT-

[MISSING_PAGE_FAIL:8]

### Trajectory inference for single-cell data

We finally test MFM for reconstructing cell dynamics, a central problem in biomedical applications (Lahnemann et al., 2020), which holds great promise thanks to the advancements of single-cell RNA sequencing (scRNA-seq) (Macosko et al., 2015; Klein et al., 2015). Since in scRNA-seq trajectories cannot be tracked, we only assume access to \(K\) unpaired distributions describing cell populations at \(K\) time points. We then apply the matching objective in eq.7 between every consecutive time points, sharing parameters for both the vector field \(v_{t,}\) and the interpolants \(x_{t,}\)--see eq.20 for how to extend \(x_{t,}\) to multiple timepoints. Following the setup of Schiebinger et al. (2019), Tong et al. (2020, 2023) we perform leave-one-out interpolation, where we measure the Wasserstein-1 distance between the \(k\)-th left-out density and the one reconstructed after training on the remaining timepoints. We compare OT-MFM and baselines over Embryoid body (EB) (Moon et al., 2019), and CTTE-seq (Cite) and Multiome (Multi) data from (Lance et al., 2022). In Table4 and Table3 we consider the first 5 and 100 principal components of the data, respectively--results with 50 principal components can be found in AppendixD. We observe that OT-MFM significantly improves upon its Euclidean counterpart OT-CFM, which resonates with the manifold hypothesis for single-cell data (Moon et al., 2018). In fact, OT-MFM surpasses all baselines, including those that add biases such as stochasticity (SF\({}^{2}\)Tong et al. (2023)) or mass teleportation (WLF-UOT Neklyudov et al. (2023)). OT-MFM instead relies on metrics such as LAND and RBF to favor interpolations that remain close to the data.

## 6 Related Work

**Geometry-aware generative models**. The manifold hypothesis (Bengio et al., 2013) has been studied in the context of manifold learning (Tenenbaum et al., 2000; Belkin and Niyogi, 2003) and metric learning (Xing et al., 2002; Weinberger and Saul, 2009; Hauberg et al., 2012). Recently, this has also

   Method & Cite & EB & Multi \\  Reg. CNF (Finlay et al., 2020) & — & 0.825\(\) 0.429 & — \\ TrajectoryNet (Tong et al., 2020) & — & 0.848 & — \\ NLSB (Koshizuka and Sato, 2023) & — & 0.970 & — \\  DSBM (Shi et al., 2023) & 1.705 \(\) 0.160 & 1.775 \(\) 0.429 & 1.873 \(\) 0.631 \\ DSB (De Bortoi et al., 2021) & 0.953 \(\) 0.140 & 0.862 \(\) 0.023 & 1.079 \(\) 0.117 \\ SF\({}^{2}\) M-Sink (Tong et al., 2023) & 1.054 \(\) 0.087 & 1.198 \(\) 0.342 & 1.098 \(\) 0.308 \\ SF\({}^{2}\) M-Geo (Tong et al., 2023) & 1.017 \(\) 0.104 & 0.879 \(\) 0.148 & 1.255 \(\) 0.179 \\ SF\({}^{2}\) M-Exact (Tong et al., 2023) & 0.920 \(\) 0.049 & 0.793 \(\) 0.066 & 0.933 \(\) 0.054 \\  OT-CFM (Tong et al., 2023) & 0.882 \(\) 0.058 & 0.790 \(\) 0.068 & 0.937 \(\) 0.054 \\ I-CFM (Tong et al., 2023) & 0.965 \(\) 0.111 & 0.872 \(\) 0.087 & 1.085 \(\) 0.099 \\ SB-CFM (Tong et al., 2023) & 1.067 \(\) 0.107 & 1.221 \(\) 0.380 & 1.129 \(\) 0.363 \\ WLF-UOT (Neklyudov et al., 2023) & 0.733 \(\) 0.063 & 0.738 \(\) 0.014 & 0.911 \(\) 0.147 \\ WLF-SB (Neklyudov et al., 2023) & 0.797 \(\) 0.022 & 0.746 \(\) 0.016 & 0.950 \(\) 0.205 \\ WLF-OT (Neklyudov et al., 2023) & 0.802 \(\) 0.029 & 0.742 \(\) 0.012 & 0.949 \(\) 0.211 \\  I-MFM\({}_{}\) & 0.916 \(\) 0.124 & 0.822 \(\) 0.042 & 1.053 \(\) 0.095 \\ OT-MFM\({}_{}\) & 0.724 \(\) 0.070 & 0.713 \(\) 0.039 & 0.890 \(\) 0.123 \\   

Table 4: Wasserstein-1 distance (\(\)) averaged over left-out marginals for 5-dim PCA representation of single-cell data for corresponding datasets. Results are averaged over 5 independent runs.

   Method & Cite (100D) & Multi (100D) \\  SF\({}^{2}\) M-Geo & 44.498 \(\) 0.416 & 52.203 \(\) 1.957 \\ SF\({}^{2}\) M-Exact & 46.530 \(\) 0.426 & 52.888 \(\) 1.986 \\  OT-CFM & 45.393 \(\) 0.416 & 54.814 \(\) 5.858 \\ I-CFM & 48.276 \(\) 3.281 & 57.262 \(\) 3.855 \\ WLF-OT & 44.821 \(\) 0.126 & 55.416 \(\) 6.097 \\ WLF-UOT & 43.731 \(\) 1.375 & 54.222 \(\) 5.827 \\ WLF-SB & 46.131 \(\) 0.083 & 55.065 \(\) 5.499 \\  I-MFM\({}_{}\) & 45.987 \(\) 4.014 & 54.197 \(\) 1.408 \\ OT-MFM\({}_{}\) & 41.784 \(\) 1.020 & 50.906 \(\) 4.627 \\   

Table 3: Wasserstein-1 distance averaged over left-out marginals for 100-dim PCA single-cell data for corresponding datasets. Results averaged over 5 runs.

been analyzed in relation to generative models for obtaining meaningful interpolations (Arvanitidis et al., 2017, 2021, Chadebec and Allassonniere, 2022), diagnosing model instability (Cornish et al., 2020, Loaiza-Ganem et al., 2022, 2024), assessing the ability to perform dimensionality reduction (Stanczuk et al., 2022, Pidstrigrach, 2022) and for the improved learning and representation of curved, low-dimensional data manifolds (Dupont et al., 2019, Schonsheck et al., 2019, Horvat and Pfister, 2021, Yonghyeon et al., 2021, De Bortoli, 2022, Jang et al., 2022, Lee et al., 2022, Lee and Park, 2023, Nazari et al., 2023). Closely related is the extension of generative models to settings where the ambient space itself is a Riemannian manifold (Mathieu and Nickel, 2020, Lou et al., 2020, Falorsi, 2021, De Bortoli et al., 2022, Huang et al., 2022, Rozen et al., 2021, Ben-Hamu et al., 2022, Chen and Lipman, 2023, Jo and Hwang, 2023). Particularly, Maoutsa (2023) utilized a data-dependent geodesic solver (Arvanitidis et al., 2019) to refine drift estimation in stochastic differential equations. Several studies extended beyond the standard linear matching process to meet specific task requirements, but have not accounted for the geometry that the data naturally forms (Liu et al., 2024, Bartosh et al., 2024, Neklyudov et al., 2023b).

**Trajectory inference**. Reconstructing dynamics from cross-sectional distributions (Hashimoto et al., 2016, Lavenant et al., 2021) is an important problem within the natural sciences, especially in the context of single-cell analysis (Macosko et al., 2015, Moon et al., 2018, Schiebinger et al., 2019). Recently, diffusion and Continuous Normalizing Flow (CNFs) based methods have been proposed (Tong et al., 2020, Bunne et al., 2022, 2023, Huguet et al., 2022, Koshizuka and Sato, 2023) but require simulations, whereas Tong et al. (2023a), Neklyudov et al. (2023a), Palma et al. (2023) allow for simulation-free training. In particular, Huguet et al. (2022), Palma et al. (2023) regularize CNFs in a latent space to enforce that straight paths correspond to interpolations on the original data manifold. Finally, Scarvelis and Solomon (2023) propose a solution to the trajectory inference problem for cellular data, which depends on a regularization of vector fields with respect to a learned metric similar to eq. (6). Crucially though, their framework requires simulations in training and is not immediately extended to more general matching objectives and applications as for MFM. In this regard, we observe that one can also adopt the metric-learning scheme of Scarvelis and Solomon (2023) in MFM, replacing \(g_{}\) and \(g_{}\) introduced in SS4.

## 7 Conclusions and Limitations

We have presented Metric Flow Matching, a simulation-free framework that generalizes Conditional Flow Matching to design probability paths whose support lies on the data manifold. In MFM, this is achieved via interpolants that minimize the geodesic cost of a data-dependent Riemannian metric. We have empirically shown that instances of MFM using prescribed task-agnostic metrics, surpass Euclidean baselines, with emphasis on single-cell dynamics reconstruction. While the universality of the metrics proposed in SS4 is a benefit, we have not investigated how to further encode biases into the metric that are specific to the downstream task--a topic reserved for future work. Additionally, the principle of learning interpolants that minimize a geodesic cost can also be adapted to score-based generative models such as diffusion models, beyond CFM. When relying on the OT coupling, standard limitations of using OT for high-dimensional problems with large datasets may arise. Lastly, our approach requires the data to be embedded in Euclidean space for the interpolants to be defined; it is an interesting direction to explore how one can learn interpolants that minimize a data-dependent metric even when the ambient space itself is not Euclidean.