# Rethinking The Training And Evaluation of Rich-Context Layout-to-Image Generation

Jiaxin Cheng\({}^{2}\)1  Zixu Zhao\({}^{1}\)  Tong He\({}^{1}\)  Tianjun Xiao\({}^{1}\)  Zheng Zhang\({}^{1}\)  Yicong Zhou\({}^{2}\)

{yc47434,yicongzhou}@um.edu.mo {zhaozixu,tianjux,htong,zhaz}@amazon.com

\({}^{1}\)Amazon Web Services Shanghai AI Lab \({}^{2}\)University of Macau

###### Abstract

Recent advancements in generative models have significantly enhanced their capacity for image generation, enabling a wide range of applications such as image editing, completion and video editing. A specialized area within generative modeling is layout-to-image (L2I) generation, where predefined layouts of objects guide the generative process. In this study, we introduce a novel regional cross-attention module tailored to enrich layout-to-image generation. This module notably improves the representation of layout regions, particularly in scenarios where existing methods struggle with highly complex and detailed textual descriptions. Moreover, while current open-vocabulary L2I methods are trained in an open-set setting, their evaluations often occur in closed-set environments. To bridge this gap, we propose two metrics to assess L2I performance in open-vocabulary scenarios. Additionally, we conduct a comprehensive user study to validate the consistency of these metrics with human preferences. [https://github.com/cplusx/rich_context_L2I/tree/main](https://github.com/cplusx/rich_context_L2I/tree/main)

Figure 1: The proposed method demonstrates the ability to accurately generate objects with complex descriptions in the correct locations while faithfully preserving the details specified in the text. In contrast, existing methods such as BoxDiff , R&B , GLIGEN , and InstDiff  struggle with the complex object descriptions, leading to errors in the generated objects.

Introduction

Recent years have witnessed significant advancements in image generation, with the diffusion model [21; 46] emerging as a leading method. This model has shown scalability with billion-scale web training data and has achieved remarkable quality in text-to-image generation tasks [5; 32; 35; 37; 39]. However, text-to-image models that rely solely on textual descriptions face limitations, particularly in scenarios requiring precise location control.

As the diversity and complexity of model training tasks increase, there is a growing demand for both accuracy and precision in generated data. Precision involves more accurate object positioning, while accuracy ensures that generated objects closely match intricate descriptions, even in highly complex scenarios. Recent approaches [25; 54] have addressed this by incorporating precise location control into the diffusion model, enabling open-vocabulary layout-to-image (L2I) generation. Among various layout types, bounding box based layouts offer intuitive and convenient control compared to masks or keypoints . Additionally, bounding box layouts provide greater flexibility for diverse and detailed descriptions. In this work, we systematically investigate layout-to-image generation from bounding box-based layouts with rich context, where the descriptions for each instance to be generated can be complex, lengthy, and diverse, aiming to produce highly accurate objects with intricate and detailed descriptions.

Revisiting existing diffusion-based layout-to-image generation methods reveals that many rely on an extended self-attention mechanism [25; 54], which applies self-attention to the combined features of visual and textual tokens. This approach condenses textual descriptions for individual objects into single vectors and aligns them with image features through a dense connected layer.

However, a closer examination of how diffusion models achieve text-to-image generation [10; 32; 35; 37; 39] shows that text conditions are typically integrated via cross-attention layers rather than self-attention layers. Adopting cross-attention preserves text features as sequences of token embeddings instead of consolidating them into a single vector. Recent diffusion models have demonstrated improved generation results by utilizing larger [9; 10] or multiple  text encoders and more detailed image captions . This underscores the significance of the cross-attention mechanism in enhancing generation quality through richer text representations and more comprehensive textual information.

Drawing an analogy between the generation of individual objects and the entire image, it is natural to consider applying similar cross-attention mechanisms to each object. Therefore, we propose introducing Regional Cross-Attention modules for layout-to-image generation, enabling each object to undergo a generation process akin to that of the entire image.

In addition to the proposed training scheme for L2I generation, we have identified a lack of reliable evaluation metrics for open-vocabulary L2I generation. While models [25; 54] can perform open-vocabulary L2I generation, evaluations are typically conducted on closed-set datasets such as COCO  or LVIS . However, such closed-set evaluations may not accurately reflect the capabilities of open-vocabulary L2I models, as the text descriptions in these datasets are often limited to just a few words. It remains unclear whether these models can perform effectively when presented with complex and detailed object descriptions.

To address this gap, we propose two metrics that consider object-text alignment and layout fidelity that works for rich-context descriptions. Additionally, we conduct a user study to assess the reliability of these metrics and identify the circumstances under which these metrics may fail to reflect human preferences accurately.

Our contributions can be summarized as follows: 1) We revisit the training of L2I generative models and propose regional cross-attention module to enhance rich-context L2I generation, outperforming existing self-attention-based approaches. 2) To effectively evaluate the performance of open-set L2I models, we introduce two metrics that assess the models' capabilities with rich-context object descriptions and validate their reliability through a user study. 3) Our experimental results demonstrate that our proposed solution improves generation performance, especially with rich-context prompts, while reducing computational cost in each layout-conditioning layer thanks to the use of cross-attention.

## 2 Related Works

**Diffusion-based Generative Models** The emergence of diffusion models [21; 46] has significantly advanced the field of image generation. Within just a few years, diffusion models have made remarkable progress across various domains, including super-resolution , colorization , novel view synthesis , 3D generation [36; 51; 14], image editing [29; 6; 24], image completion  and video editing [45; 12]. This progress can be attributed to several factors. Enhancements in network architectures [32; 34; 35; 37; 39; 43] have played a pivotal role. Additionally, improvements in training paradigms [47; 48; 15; 31] have contributed to this advancement. Moreover, the ability to incorporate various conditions during image generation has broadened the impact and applications of diffusion models. These conditions include elements such as segmentation [1; 2; 4; 59], using an image as a reference [40; 16; 30], and layout [54; 11; 25], the latter of which will be the main focus of our discussion in this work.

**Layout-to-image generation**: Early works [19; 23; 26; 33; 38; 49; 50; 60] often utilized GANs  or transformers  for L2I generation. For instance, GAN-based LAMA , LostGANs , and Context L2I  encode layouts as style features fed into adaptive normalization layers, while Taming  and TwFA  use transformers to predict latent visual codes from pretrained VQ-VAE . Recent diffusion models [54; 56; 57; 58; 56; 55; 54] have shown promising results, extending L2I generation to be open-set. LayoutDiffuse  injects objects into the image features through learning per-class embeddings. LayoutDiffusion  fine-tunes pre-trained diffusion models by mapping object labels and layout coordinates into cross-attendable embeddings for attention layers. FreestyleL2I , BoxDiff  and R&B , which are training-free methods, leverage pre-trained diffusion models to inject objects into specified regions by imposing spatial constraints. GLIGEN  and InstDiff  explore open-set L2I generation using grounded bounding boxes, which encodes layout locations and object descriptions into features attended by self-attention layers.

## 3 Methodology

### Challenges in Rich-Context Layout-to-Image Generation

The layout-to-image (L2I) generation task can be formally defined as follows: given a set of description tuples \(S:=\{s_{i}|s_{i}=(b_{i},t_{i})\}\), where \(b_{i}\) represents the bounding box coordinates of an object, and \(t_{i}\) denotes the corresponding text description, the objective is to generate an image that accurately aligns objects with their respective descriptions while maintaining fidelity to the specified layouts. In the closed-set setting, the number of text descriptions is limited to a fixed number \(N\)

Figure 2: An example of regional cross-attention with two overlapping objects. Cross-attention is applied to each pair of regional visual and grounded textual tokens. The overlapping region cross-attends with the textual tokens containing both objects, while the non-object region attends to a learnable “\(null\)” token.

_i.e._, \(N=|\{t_{i}\}|\), where \(N\) is the total number of classes. However, in the open-set and rich-context settings, the number of descriptions is unlimited, with descriptions in the rich-context setting being more diverse, complex, and lengthy.

Rich-context L2I encounters several challenges: 1) The rich-context descriptions for each object can be lengthy and complex, requiring the model to correctly understand the descriptions without overlooking details. Existing open-set layout-to-image solutions [25; 54] typically condense and map text embeddings into a single vector, which is then mapped to the image space for layout conditioning. However, this condensation process can result in significant information loss, particularly for lengthy descriptions. 2) Fitting various text descriptions into their designated layout boxes while maintaining global consistency is challenging. Unlike simpler text-to-image generation with a single description, L2I generation deals with multiple objects, requiring precise matching of each description to its specific layout area without causing global inconsistency. 3) L2I involves objects with intersecting bounding boxes, unlike segmentation-mask-to-image tasks where object areas do not overlap and can be efficiently handled by pixel-conditioned methods such as ControlNet  and Palette . L2I models must determine the appropriate order and occlusion of overlapping objects autonomously, ensuring the proper representation and interaction of each object within the image.

### Regional Cross-Attention

We propose using a regional cross-attention layer as a solution to rich-context layout-to-image generation, addressing the aforementioned challenges. The desired properties for an effective rich-context layout-conditioning module are as follows: 1) _Flexibility_: The model must accurately understand rich-context descriptions, regardless of their length or complexity, ensuring that no details are overlooked. 2) _Locality_: Each textual token should only attend to the visual tokens within its corresponding layout region, without influencing regions beyond the layout. 3) _Completeness_: All visual features, including those in the background, should be properly attended by certain description to maintain consistency in the output feature distribution. 4) _Collectiveness_: In cases where a visual token overlaps with multiple objects, it should consider all descriptions related to those intersecting objects.

Our approach differentiates itself from previous methods [25; 54] by employing cross-attention layers, rather than self-attention layers, to condition objects within the image. This design is inspired by the architecture of modern text-to-image diffusion models, which achieve fine-grained text control by incorporating pre-pooled textual features in the cross-attention layers. Analogously, one can apply cross-attention repeatedly between pre-pooled object description tokens and visual tokens within the corresponding regions for all objects. However, this straightforward method, though satisfies flexibility and locality, does not fully meet the criteria of completeness and collectiveness, as it may inadequately address non-object regions and overlapping objects. This limitation can result in inconsistent global appearances and challenges in managing overlapping objects effectively.

**Region Reorganization.** We propose region reorganization to satisfy locality, completeness and collectiveness, by creating a spatial partition of the image based on the layout. Each region is classified into one of three types: single-object region, overlapping region among objects, and background. This partitioning ensures that regions are mutually exclusive (_i.e._, non-overlapping). Figure 2 illustrates a simple case with two overlapping objects. The overlapping area becomes a new, distinct region, while the non-overlapping parts of the original regions and remaining background are also treated as separate, new regions, thus ensuring completeness.

Formally, in the general case with multiple objects, the reorganized regions \(R:=\{r_{i}\}\) satisfy that the union of these regions will form a complete mask covering the entire visual feature space, while ensuring no intersection between any two reorganized regions:

\[_{i=1}^{|R|}r_{i} =; r_{i}  r_{j} =i ji,j[1,|R|] \]

Our regional cross-attention operates within each reorganized region. We define a selection operation \(f(,r_{i})\) to identify the appropriate regions for cross-attention. For visual tokens \(V:=\{v_{j}\}\) it finds the tokens whose locations \((v_{j})\) lie within the \(i\)-th reorganized region. For description tuples \(S\), it filters the instances that overlap with the \(i\)-th reorganized region. This selection operation ensures that the text description is applied exclusively to the visual tokens within its corresponding region, thus maintaining locality. For regions with multiple objects, \(f\) also ensures that all overlapping descriptions are included to satisfy collectiveness.

\[f(V,r_{i}) :=\{v_{j}|(v_{j}) r_{i}\} \] \[f(S,r_{i}) :=\{s_{j}|b_{j} r_{i}\} \]

The final attention result \(A\) is the aggregation of all regional attention outputs. The selected descriptions \(f(S,r_{i})\) are encoded using Sequenced Grounding Encoding (SGE) in Figure 3 and serve as the key and value during cross-attention. For non-object regions where \(f(S,r_{i})=\), a "\(null\)" embedding is learned as a substitute for the description.

\[a_{i} =(f(V,r_{i}),[f(S,r_{i})]); A =_{i=1}^{|R|}a_{i} \]

**Sequenced Grounding Encoding with Box Indicator.** The selected object descriptions in each reorganized region are encoded into textual tokens using Sequenced Grounding Encoding in Figure 3. When multiple objects are present in a region, their descriptions are concatenated with a separator token. However, if two objects share the same description, their encoded textual embeddings will be identical. This identical embedding makes it impossible for the cross-attention module to distinguish between distinct objects. To address this issue, we incorporate bounding box coordinates as additional indicators. During encoding, we concatenate the bounding box coordinates with the textual tokens. The bounding box coordinates are initially encoded using sinusoidal positional encoding  and then repeated to match the length of the textual tokens before concatenation. For separator tokens and special tokens such as [bos] and [eos], we use an all -1 vector for their box coordinates.

## 4 Evaluation for Rich-Context L2I Generation

### Rethinking L2I Evaluation

For evaluating layout-to-image generation, what are mainly considered are two aspects: 1) Object-label alignment, which checks whether the generated object matches the corresponding descriptions. 2) Layout fidelity, which examines how well the generated object aligns with the given bounding box.

In closed-set scenarios, it is common to use an off-the-shelf detector to evaluate L2I generation performance [11; 25; 54]. Object-label alignment is assessed by classifying image crops extracted from the generated image using a pre-trained classifier. Similarly, layout fidelity is measured by comparing the bounding boxes detected in the generated image with the provided layouts, using a pre-trained object detector.

However, in the open-set scenario, it is impossible to list all the classes. Moreover, even the state-of-the-art open-set object detectors [13; 61; 62] are set up to handle inputs at the word or phrase level, which falls short for the sentence-level descriptions required in rich-context L2I generation. we introduce two metrics to bridge the gap in evaluating open-vocabulary L2I models.

Figure 3: Sequenced Grounding Encoding with box coordinates as indicators.

### Metrics For Rich-Context L2I

We leverage the powerful visual-textual model CLIP for measuring object-label alignment, and the Segment Anything Model (SAM) for evaluating layout fidelity of the generated objects.

**Crop CLIP Similarity**: In rich-context L2I, object descriptions can be diverse and complex. The CLIP model, known for its robustness in image-text alignment, is thus suitable for this evaluation. To ensure accuracy and mitigate interference from surrounding objects, we compute the CLIP score after cropping the object as per the layout specifications.

**SAMIoU**: An accurately generated object should closely align with its designated layout. Given the potential diversity in object shapes, we employ the SAM model, which can highlight an object's region in mask format within a given box region, to identify the actual region of the generated object. We then determine the generated object's circumscribed rectangle as its bounding box. The layout fidelity of the generated object with the ground-truth layout is quantified by the intersection-over-union (IoU) between the provided layout box and the generated object's circumscribed box.

## 5 Experiments

### Model and Dataset

**Model** We leverage powerful pre-trained diffusion models as the foundation for our generative approach. Our best model is fine-tuned from Stable Diffusion XL (SDXL) . We also provide the benchmarks using Stable Diffusion 1.5 (SD1.5) , which is a widely used backbone in existing methods . The proposed regional cross-attention layer is inserted into the original diffusion model right after each self-attention layer. The weights of the output linear layer are initialized to zero, ensuring that the model equals to the foundational model at the very beginning. More implementation details is shown in Appendix B

**Rich-Context Dataset** To equip the model with the capability to be conditioned on complex and detailed layout descriptions, a rich-context dataset is required. While obtaining large-scale real-world datasets through human tagging is labor-intensive and expensive, synthetic training data can be more readily acquired by leveraging recent advancements in large visual-language models. Similar to GLIGEN  and InstDiff , we generate synthetic data to train our model.

We adopt a locating-and-labeling strategy during pseudo-label generation. At the first step, we use the Recognize Anything Model (RAM)  and GroundingDINO  to identify and locate salient objects in the image. Next, we use the visual-language model QWen  to produce the synthetic label for each object by asking it to generate detailed description of the object (see Appendix B for the prompt we used). We utilize CC3M  and COCO Stuff  as the image source. For COCO, we directly use the ground-truth bounding boxes rather than relying on RAM and GroundingDINO to generate synthetic labels. The final training dataset contains two million images, with 10,000 images from CC3M set aside and the 5,000-image validation set of COCO used for evaluation. We

Figure 4: Statistical comparisons between the synthetic object descriptions generated by GLIGEN , InstDiff , and our method. We measure the 1) average caption length, 2) the Gunning Fog Score, which estimates the text complexity from the education level required to understand the text, 3) the number of unique words per sample which indicates the text diversity, and 4) the object-label CLIP Alignment Score to measure object-label alignment. The results show that the pseudo-labels generated for our dataset are more complex, diverse, lengthier, and align better with objects, compared to those generated by GLIGEN and InstDiff.

denote the generated dataset Rich-Context CC3M (RC CC3M) and Rich-Context COCO (RC COCO). Compared to the synthetic training data used in GLIGEN  and InstDiff , our rich-context dataset provides more diverse, complex, lengthy, and accurate descriptions, as shown in Figure 4.

### Reliability Analysis of Automatic L2I Evaluation

Our proposed evaluation metrics in Section 4.2 serve as a substitute for the lack of precise ground-truth in open-set scenarios. Whether the set is closed or open, the goal of evaluation is to ensure that the measurement results align with human perception. To validate the reliability of our automatic evaluation metrics, we conducted a user study on the RC CC3M dataset, randomly selecting 1000 samples. Each synthetic sample may contain multiple objects, but only one object was randomly selected for each question. Users were asked to answer two questions, each rated on a scale from 0 (bad) to 5 (good). For object-label alignment, users responded to the question: "Can the cropped object in the image be recognized as [label]?" with the label being the automatically generated object description from RC CC3M. For layout fidelity, users answered: "How well (tight) does the object align with the bounding box?" referring to the synthetic bounding box in RC CC3M.

In total, we collected 300 answers for each question. We used the Pearson correlation coefficient to analyze how well the automatic evaluation metrics align with human perception. The Pearson correlation coefficient measures the correlation between two distributions with a value ranging from -1 to 1, where 0 indicates no relation and 1 indicates a strong correlation. Empirically, we found that the automatic evaluation metrics sometimes failed to reflect human perception when the object size was very small or very large. We note that for small objects, the clarity of the object can be hampered, while large objects may overlap with many other objects, making the automatic measurements inaccurate. Therefore, we filtered out objects smaller than 5% or larger than 50% of the image, resulting in an improved Pearson correlation between automatic metrics and user scores from 0.33 to 0.59 for CropCLIP and from 0.15 to 0.52 for SAMIoU. We applied the same filtering rule in the remaining evaluations.

Figure 5: Qualitative comparison of rich-context L2I generation, showcasing our method alongside open-set L2I approaches GLIGEN  and InstDiff , based on detailed object descriptions. Our method consistently generates more accurate representations of objects, particularly in terms of specific attributes such as colors and shapes. Strikethrough text indicates missing content in the generated objects from the descriptions. More qualitative results available in Appendix H

### Rich-Context Layout-to-Image Generation

**Evaluation Metrics.** In addition to the two dedicated metrics discussed in Section 4.2, we also consider image quality by measuring the FID scores , which reflects how real or natural the generated images look compared to real images. While we did not observe significant changes in FID scores across different variations of our methods, we did notice change in image quality among different baseline methods.

**Baseline Methods.** We compare our approach with GLIGEN , a popular open-vocabulary L2I generative model, and InstDiff , a recent method that achieves state-of-the-art open-set L2I performance. Besides, two training-free L2I methods BoxDiff  and R&B  are also considered for comparison. Although these methods can accept open-set words, their inputs are limited to single words or simple phrases which are not truly rich-context descriptions. Therefore, we denote them as constrained L2I methods and only evaluate them on COCO using category names.

Table 1 benchmarks the performance of L2I methods at an image sampling resolution of 512. Our model with SD1.5 achieves similar performance to InstDiff while reducing the computation cost in the layout conditioning layer by half, as illustrated in Figure 6. Additionally, our model with SDXL achieves the best performance, even though the 512 resolution is sub-optimal for it. Further experiments in Section 5.5 demonstrate that higher sampling resolutions can further enhance performance. Figure 5 shows that, as the complexity and length of object descriptions increase, existing open-set L2I methods tend to overlook details especially when objects are specified with colors or shapes. In contrast, our method consistently generates objects that accurately represent the given descriptions.

### Performance Across Various Complexity of Object Descriptions

By adopting pre-pooling textual features in the layout conditioning layer, our method maximizes the retention of textual information during generation. We observe that this design significantly enhances performance when dealing with complex and lengthy object descriptions. In Figure 6(a), we categorize object description complexity using the Gunning Fog score into three levels: easy (scores 0-4), medium (5-8), and hard (\(>\)8). Additionally, we classify descriptions by length into phrases (\(\)8 words), short sentences (\(\)15 words), and long sentences (\(\)16 words). Our results indicate that for simple and short descriptions, the performance difference between our method and state-of-the-art open-set L2I methods is close. However, as the complexity and length of the descriptions increase, our method consistently outperforms existing approaches.

### Ablation Study

We investigate the effectiveness of the proposed region reorganization and the use of bounding box indicators on object-label alignment and layout fidelity using RC CC3M dataset. In experiments without region reorganization, we use straightforward averaging features for overlapping objects. Empirically, we observe that without region reorganization, our model struggles to generate the

    &  &  \\ 
**Method** & CropCLIP\(\) & SAMIoU\(\) & FID\(\) & CropCLIP\(\) & SAMIoU\(\) & FID\(\) \\  _Constrained_ & & & & & \\ BoxDiff  & 22.61 & 58.75 & 29.99 & - & - & - \\ R\&B  & 23.68 & 64.68 & 31.70 & - & - & - \\  _Open-Set_ & & & & & & \\ GLIGEN  & 25.20 & 78.66 & 27.62 & 25.27 & 83.64 & 15.81 \\ InstDiff  & 27.46 & 80.78 & 30.00 & 28.46 & 85.59 & 17.96 \\  _Ours_ & & & & & \\ SD1.5 & 27.36 & 80.78 & **25.81** & 28.45 & 86.04 & 16.49 \\ SDXL & **28.15** & **80.84** & 27.41 & **29.42** & **86.56** & **11.02** \\   

Table 1: Quantitative comparison of different L2I approaches under image resolution at 512x512. ‘†’ means that the higher the better, ‘†’ means that the lower the better.

correct object when there is an overlap of objects with complex descriptions, leading to a significant drop in both object-label alignment and layout fidelity as shown in the Table 2.

Unlike self-attention-based solutions that use box indicators to implicitly indicate object locations, our method explicitly cross-attends visual tokens with corresponding textual tokens. This approach allows the model to recognize the correct location for object conditioning even without a box indicator. However, the reorganized mask in the regional cross-attention layer has a lower resolution than the original image, causing misalignment near the borders of generated objects. Adding a bounding box indicator not only helps the model distinguish objects with similar descriptions and but also improves layout fidelity, as validated by the improvement in SAMIoU.

Additionally, we notice that sampling at a higher image resolution (768x768) improves model performance, although it demands greater computational resources. It's important to note that generalization to higher resolution is not a universal capability of L2I models. Existing self-attention-based L2I methods like GLIGEN  experience performance declines when sampling at resolutions different from the training resolution. Another self-attention-based method, InstDiff , uses absolute coordinates for conditioning, requiring the sampling resolution to match the training resolution exactly. In Figure 6(b), we compare the performance-computation trade-off2 of open-set L2I approaches. Since InstDiff does not support flexible resolution sampling, we utilize Multi-Instance Sampling (MIS)  instead. MIS was proposed to enhance InstDiff's performance by sampling each instance separately, albeit with increased inference times. We demonstrate the simplest case of MIS, which requires two inferences, but its computational cost scales linearly with the number of objects.

## 6 Conclusion

In this study, we introduced a novel approach to enhance layout-to-image generation by proposing Regional Cross-Attention module. This module improve the representation of layout regions, particularly in complex scenarios where existing methods struggle. Our method reorganizes object-region correspondence by treating overlapping regions as distinct standalone regions, allowing for more accurate and context-aware generation. Additionally, we addressed the gap in evaluating open-vocabulary

  
**Region Reorg.** & **Box Indicator** & **High Reso.** & CropCLIP\(\) & SAMIoU\(\) \\  ✗ & ✗ & ✗ & 25.32 & 76.92 \\ ✓ & ✗ & ✗ & 28.93 & 85.02 \\ ✓ & ✓ & ✗ & 29.42 & 86.56 \\ ✓ & ✓ & ✓ & 29.79 & 88.10 \\   

Table 2: Ablation study of the proposed methods on RC CC3M dataset with SDXL backbone. The results suggest that region reorganization plays an important role for rich-context L2I generation, while using box indicator and sample at higher-resolution can further enhance performance.

Figure 6: (a) Object-text alignment scores across varying description complexities and lengths on RC CC3M. Our method shows significant advantages for complex and lengthy descriptions. (b) Object-text alignment and layout fidelity relative to computational cost in each layout-conditioning attention layer. Given that the number of textual tokens is much smaller than visual tokens, applying cross-attention can substantially reduce computational costs.

L2I models by proposing two novel metrics to assess their performance in open-set scenarios. Our comprehensive user study validated the consistency of these metrics with human preferences. Overall, our approach improves the quality of generated images, offering precise location control and rich, detailed object descriptions, thus advancing the capabilities of generative models in various potential applications.