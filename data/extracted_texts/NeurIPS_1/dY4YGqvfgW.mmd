# On Weak Regret Analysis for Dueling Bandits

El Mehdi Saad

KAUST

mehdi.saad@kaust.edu.sa &Alexandra Carpentier

Institut fur Mathematik

Universitat Potsdam

carpentier@uni-potsdam.de

Tomas Kocak

Institut fur Mathematik

Universitat Potsdam

kocak@uni-potsdam.de &Nicolas Verzelen

INRAE, MISTEA, Univ. Montpellier

nicolas.verzelen@inrae.fr

Work done while at CentraleSupelec.

###### Abstract

We consider the problem of \(K\)-armed dueling bandits in the stochastic setting, under the sole assumption of the existence of a Condorcet winner. We study the objective of weak regret minimization, where the learner doesn't incur any loss if one of the selected arms is a Condorcet winner--unlike strong regret minimization, where the learner has to select the Condorcet winner twice to incur no loss. This study is particularly motivated by practical scenarios such as content recommendation and online advertising, where frequently only one optimal choice out of the two presented options is necessary to achieve user satisfaction or engagement. This necessitates the development of strategies with more exploration. While existing literature introduces strategies for weak regret with constant bounds (that do not depend on the time horizon), the optimality of these strategies remains an unresolved question. This problem turns out to be really challenging as the optimal regret should heavily depend on the full structure of the dueling problem at hand, and in particular on whether the Condorcet winner has a large minimal optimality gap with the other arms. Our contribution is threefold: first, when said optimality gap is not negligible compared to other properties of the gap matrix, we characterize the optimal budget as a function of \(K\) and the optimality gap. Second, we propose a new strategy called WR-TINF that achieves this optimal regret and improves over the state-of-the-art both in \(K\) and the optimality gap. When the optimality gap is negligible, we propose another algorithm that outperforms our first algorithm, highlighting the subtlety of this dueling bandit problem. Finally, we provide numerical simulations to assess our theoretical findings.

## 1 Introduction

We consider an instance of the problem of sequential and active learning from comparisons - namely dueling bandits. It can be modeled as a sequential game where, at each time, a learner presents to a user a pair of two items and collects feedback, which is a noisy indication of the user's preference between the two items. If neither of the presented items aligns with the user's top choice, the learner incurs a loss. Preference-based learning has gained importance recently as it reflects human decision-making processes, which often rely on relative rather than absolute evaluations. This approach is notably effective in systems that involve human interaction, where feedback is provided in a qualitative form . In the dueling bandit setting, initially presented by , the compared itemsare called "arms" and there are \(K\) of them. This setting is structured as an ongoing sequential game with time horizon \(T\), where in each round \(t T\), the learner selects two arms (items) indexed by \(i,j[K]\), and receives the result of a duel between these arms as feedback. The result of each duel is encoded by \(1\) if the first arm - in our case arm \(i\) - beats the second one - in our case arm \(j\) - and \(0\) otherwise. This result follows a Bernoulli distribution with unknown parameter \(q_{i,j}\). Specifically, this paper focuses on the stochastic scenario where the parameters governing the duels are assumed to be constant throughout the game, albeit unknown to the learner. The literature on the stochastic dueling bandits is very rich and contains various settings that differ from our paper either in optimal arm characterization or the definition of rewards.

In contrast to the classic multi-armed bandit problem, defining the optimal arm in dueling bandits is not straightforward. This led to the introduction of various definitions of winners within the literature as detailed in the surveys [2; 16; 17]. Our study focuses on situations where there is an arm \(k^{*}[K]\) that, on average, defeats all other arms: formally, \(q_{k^{*},j}>1/2\) for any \(j[K]\{k^{*}\}\). This arm is termed the Condorcet winner while we refer to it as optimal in the rest of the paper. In the context of dueling bandits, particularly the cumulative regret minimization problem, most of prior works either made the assumption of the existence of a Condorcet winner [24; 23; 10; 8; 4; 14; 15] or the stronger assumption of the existence of a total order between arms [18; 20; 4].

Once the concept of the optimal arm is established, the next step is to define the objective. Rather than identifying the best arm, our goal is to minimize the cumulative loss. To this end, we must determine the loss incurred each round based on the two arms selected by the learner. The dueling bandit literature distinguishes between two primary types of losses: strong loss and weak loss, as described by . With strong loss, the learner must select the Condorcet winner twice to avoid any loss (noting that the feedback in this case is equivalent to a fair coin flip). In contrast, weak loss requires only one of the selected arms to be the Condorcet winner. Formal definitions of weak and strong regrets are provided in Section 2. In many practical scenarios, such as recommendation systems and online advertising [6; 3], minimizing weak regret aligns more closely with the learner's objectives than strong regret minimization. For example, consider a situation where the learner operates as a service provider, presenting two options to a client who then chooses their preferred option. In this framework, the learner should incur a loss only if neither option matches the client's preference, encouraging exploration and maximizing information gain. While previous research in dueling bandits has primarily focused on minimizing strong regret, developing optimal strategies for minimizing weak regret remains an unresolved issue despite prior works [4; 12], as highlighted in the survey . Further details on the technical distinctions between these two objectives will be discussed in the next sections.

In this paper, we focus on minimising the weak regret. We provide a lower bound for this problem in a specific regime where the Condorcet winner beats largely the other arms. We provide an algorithm that matches it. Nevertheless, it is not optimal in all regimes, and we highlight this by providing another algorithm that performs better in some interesting regimes.

## 2 Problem setting

We consider \(K\) arms. Let \(Q=(q_{i,j})_{1 i,j K}^{K K}\) be the matrix of preference probabilities where the probability of arm \(i\) beating arm \(j\) in a duel corresponds to \(q_{i,j}\). We assume that \(q_{j,i}=1-q_{i,j}\) and \(q_{i,i}=1/2\) for all \(i,j[K]\). Define \(_{i,j}:=q_{i,j}-\). Notably, the sign of \(_{i,j}\) indicates the relative preference between arms \(i\) and \(j\) (specifically, \(i\) is preferred over \(j\) if \(_{i,j}>0\)). The quantity \(_{i,j}\) characterizes the hardness of distinguishing which of the arms \((i,j)\) is preferred to the other. We denote \(:=(_{i,j})\) the gap matrix. The only assumption made in this paper is regarding the existence of a Condorcet winner, which we denote \(k^{*}\) for the remainder of this paper:

**Assumption 2.1**.: _Existence of a Condorcet winner: There exists an arm \(k^{*}[K]\) such that:_

\[ i[K]\{k^{*}\}:q_{k^{*},i}>1/2.\]

We consider that at each time \(t=1,2,\), the learner chooses two arms \((I_{t},J_{t})\) based on past information and receives the output of a duel between the chosen arms. More formally, the output is a sample from a Bernoulli distribution with parameter \(q_{I_{t},J_{t}}\), independent of everything else after conditioning on \((I_{t},J_{t})\). We consider that after each round \(t\) the learner incurs a loss given by:

\[_{t}^{(w)}:=\{_{k^{*},J_{t}},_{k^{*},J_{t}}\},\]which we term the weak instantaneous loss, following . Another concept of instantaneous loss that is often considered in the literature is the strong instantaneous loss [18; 1; 24], where at round \(t\) the learner incurs the loss \(_{t}^{(s)}:=(_{k^{*},I_{t}}+_{k^{*},J_{t}})/2\). Note that when it comes to the weak instantaneous loss, in contrast to the strong instantaneous loss, the learner does not incur any loss if at least one of the two chosen arms is the Condorcet winner, \(I_{t}=k^{*}\) or \(J_{t}=k^{*}\), while for the strong instantaneous loss, both arms need to be the Condorcet winner in order for the learner to not incur a loss. We finally define the weak expected cumulative regret up to time \(T\) by for the weak instantaneous loss by \(R_{T}^{(w)}:=_{t=1}^{T}[_{t}^{(w)}]\), which we term weak regret. Similarly, we define the strong regret as \(R_{T}^{(s)}:=_{t=1}^{T}[_{t}^{(s)}]\).

## 3 Literature review and our contributions

### Related work

When it comes to the stochastic dueling bandit literature, much of the prior work has been devoted to the goal of minimizing the strong regret, under the assumption of a total order between the arms [19; 18] or only under the assumption of the existence of a Condorcet winner [24; 23; 10; 8; 12]. We detail nevertheless those results here as, since the strong regret upper bounds the weak regret, all algorithms and upper bounds that are available for the strong regret also hold for the weak regret.

In , an instance-dependent lower bound for strong regret was established:

\[_{T}[R_{T}^{(s)}]}{(T)} _{k k^{*}}_{i_{k}},k}+_ {k^{*},i}}{2_{i,k}^{2}}, \]

where \(_{k}=\{i[K] q_{i,k}>1/2\}\). This work also introduces an algorithm that asymptotically matches this lower bound as \(T+\). However, in finite-horizon, their regret bound has a quadratic dependence on the number of arms \(K\). Deriving bounds that scale linearly with \(K\) has been the subject of several works [23; 10], In particular,  devised a reduction to a standard (but adversarial) multi-armed bandits problem. They obtained guarantees on the strong regret which are of the order \(_{k k^{*}}(T)/_{k^{*},k}\). This regret bound turns out to match (1) in scenarios where the Condorcet winner is also the arm that is best for eliminating all other sub-optimal arms, namely where \(_{k^{*},k}=_{i}_{i,k}\). In more general cases, the last upper bound of  does not match the lower bound given in (1).

Weak regret itself was introduced in  to model in a more refined way some recommender systems applications. As mentioned, it is upper bounded by the strong regret so that all described algorithms and associated regret upper bounds would also hold for the weak regret. However, a distinction was made in  regarding the fundamental nature of these two problems. While the problem-dependent optimal order of the strong regret scales as \( T\) (see above) - which is aligned with classical results in stochastic bandits - there exist some algorithms whose problem-dependent weak regret is upper bounded by a quantity that does not depend on \(T\) - which is in sharp contrast with classical results on stochastic bandits. Specifically,  introduced an algorithm called WS-W, which, under the sole assumption of the existence of a Condorcet winner, achieves an upper bound on weak regret of the order \(K^{2}/_{i j}_{i,j}^{2}\). More recently, in , the Beat The Winner (BTW) algorithm was introduced. BTW adopts a round-based approach where the best arm so far keeps being challenged through batches of duels by candidate arms. Assuming only the presence of a Condorcet winner, this algorithm achieves an upper bound on weak regret of the order \(K^{2}+K/_{i k^{*}}_{k^{*},i}^{4}\). Finally, under the additional and arguably the strong assumption of the existence of a total order between arms, the upper bound can be proven to be of order \((K K)/_{i j}_{i,j}^{5}\). In summary, the dependency on the optimal regret on both \(K\) and on the matrix \(\) still remains largely unknown.

From a technical standpoint, developing optimal strategies in the weak regret framework underlies different challenges than the ones for the strong regret. This complexity arises because losses in the strong regret framework are linear in the problem parameters (the gaps matrix entries \((_{i,j})_{1 i,j K}\)): \(_{t}^{(s)}=(_{k^{*},I_{t}}+_{k^{*},J_{t}})/2\), whereas in weak regret, the loss is determined as the minimum gap with the client's preference: \(_{t}^{(w)}=\{_{k^{*},I_{t}},_{k^{*},J_{t}}\}\), which breaks linearity. As a result, classical reduction methods as the one used in [1; 14; 15] are not directly applicable for weak losses.

### Main contributions

In this paper, we address the following fundamental question:

1. What is the best possible weak regret one can achieve in terms of \(K\) and the gaps \((_{k^{*},i})_{i k^{*}}\) to the Condorcet winner?
2. Beyond that, is it possible to improve the regret by leveraging over the _unknown_ entries of the matrix \(\)? As a simple toy example, assume that the gaps \(_{k^{*},k}\) are small and that, some gaps (\(_{i,j}\)) for \(i,j k^{*}\) are much higher. In that regime, it is perhaps more beneficial to explore the \((K^{2})\) duels between all arms to better discard sub-optimal arms than simply to directly look for the Condorcet winner. This informal argument suggests the optimal guarantees depend in an intricate way on the number of arms \(K\) and the gaps \((_{i,j})\) and that there is a complex trade-off between directly aiming for the Condorcet winner and further exploration for better elimination.

To address the first question, we provide a lower bound on the weak regret, which, to the best of our knowledge, is the first of its kind for this problem. We demonstrate that in certain cases, where in particular the gaps between the Condorcet winner and the sub-optimal arms are larger than the gaps between sub-optimal arms, the bound \(K/_{i k^{*}}_{k^{*},i}\) is not improvable (see Section 5).

We introduce and analyze two new procedures. First, we provide in Section 4.1 an algorithm WR-TINF (Weak Regret-Tsallis INF) whose weak regret is bounded by

\[}_{k^{*},i}}}} ,i}}}. \]

This can be upper-bounded by \(K/(_{i k^{*}}_{k^{*},i})\). This improves over the state-of-the art  (where bounds are respectively of the order of \(K^{2}/_{i j}_{i,j}^{2}\) and \(K^{2}+K/_{i k^{*}}_{k^{*},i}^{4}\)) both in the dependency with respect to \(K\) and the gaps. Also, we do not require the strong stochastic transitivity assumption, required in 2. Conversely, the bound \(K/_{i k^{*}}_{k^{*},i}\) turns out to be impossible to improve in general -see Section 5.

Second, we introduce in Section 4.2 the algorithm WR-EXP3-IX (Weak Regret EXP3-IX), which, from an heuristic viewpoint aims at eliminating sub-optimal arms by looking at duels between sub-optimal arms. For any \(\), its weak regret is at most of the order of

\[_{i k^{*}})_{k^{*},i}}{_{j^{*}(i),i}^{2}}, \]

where \(j^{*}(i)_{j}_{j,i}\) and \(_{*}=_{k k^{*}}_{k^{*},k}\). In the case, where the gaps \(_{j^{*}(i),i}\) are larger (up to log-terms) than \(_{k^{*},i}\), the regret guarantee (3) for WR-EXP3-IX becomes smaller than (2) for WR-TINF. Up to our knowledge, WR-EXP3-IX is the first algorithm in weak regret minimization that builds upon the complete structure of the gaps matrix \(\) to lower the regret.

To further discuss the difference between the performances of both procedures, let us consider a toy model where, for some positive constants, \(_{}\) and \(_{}\), we have, for any \(i k^{*}\), \(_{k^{*},i}=_{}\), that is the gap between the Condorcet winner and the sub-optimal arms. Besides, for any \(i k^{*}\), there exists \(j^{*}(i)\) such that \(_{j^{*}(i),i}=_{}\). We distinguish two main regimes: (a) If \(_{}/_{} 1/\), then the weak regret of WR-TINF is the better one and is of the order of \(K/_{cw}\). (b) If \(_{}/_{} 1/\), then a transition occurs. To show that an arm \(k\) is not the Condorcet winner, then it now becomes beneficial to identify arms that provide the most evidence for the suboptimality of \(k\). Here, WR-EXP3-IX achieves the better guarantee which is (up to \(\) terms) of the order of \(K^{2}(_{cw}/_{}^{2})\).

The presented algorithms use different techniques: we develop WR-TINF using an adaptation of the standard reduction technique (discussed in Section 4.1). We extend the idea of using a best-of-both worlds procedure as a base algorithm to sample each of the two arms \(I_{t}\) and \(J_{t}\). However, since only one of the sampled arms should be optimal, we modify the sampling distribution prescribed by thebase algorithm to induce more exploration. The second procedure, WR-EXP3-IX, uses a different approach. Given the value of the left arm \(I_{t}\) (selected in a round-robin manner), we use the EXP3-IX algorithm  to select the right arm \(J_{t}\). Then after a fixed number of rounds, the choice of \(J_{t}\) (given the value of \(I_{t}\)) concentrates around the arm with highest probability of defeating it. We leverage the fact that when \(I_{t}\) is the Condorcet winner, the gaps are positive, while for sub-optimal arms the minimal gap is negative.

## 4 Upper Bounds

This section presents two algorithms with guarantees on weak regret. Recall that we present two strategies since we identified two regimes as discussed in Section 3.2. Each of the algorithms we present is optimal in one of the regimes and none of them require prior knowledge on the problem parameters.

The first algorithm, WR-TINF, is built upon an adaptation of the reduction technique to a standard multi-armed bandit problem. Its upper bounds depend on the gaps between sub-optimal arms and the Condorcet winner \((_{k^{*}},k)_{k[K]}\). As demonstrated in the results of Section 5, this algorithm is optimal for some regimes.The second procedure, WR-EXP3-IX, aims for the task of identifying, for each arm, the arm that can eliminate it most rapidly (i.e., the arm with the largest gap). While this strategy results in a quadratic dependence on \(K\), we argue that it outperforms WR-TINF for some instances.

### Algorithm 1: Weak Regret Tsallis-INF

We adopt a previously explored approach , where the dueling bandit problem is converted into two separate multi-armed bandit problems - one for each arm pulled. This reduction was originally applied in the context of strong regret. However, adapting this approach to weak regret requires a more nuanced approach.

The idea of reducing a dueling bandit problem to a standard one was first introduced in  where it was termed _Sparring_ in the context of minimizing strong regret. The high-level idea of this technique is to view the problem of selecting the the arm pair \((I_{t},J_{t})\) as two individual multi-armed bandit (MAB) problems. The choice of \(I_{t}\) (resp. \(J_{t}\)) can be performed by the first (resp. second) player, following which they incur a loss denoted \(_{-1,t}(I_{t}):=X_{t}(I_{t},J_{t})\) (resp. \(_{+1,t}(J_{t}):=1-X_{t}(I_{t},J_{t})\)), where \(X_{t}(I_{t},J_{t})(q_{I_{t},J_{t}})\). Here the subscript \(-1\) (resp. \(+1\)) refers to the first (resp. second) player. It is easy to show that the regret of each player \(R_{ 1,T}\) satisfies the following identity, where \(R_{T}^{(s)}\) is the strong regret of the dueling bandits problem:

\[[R_{T}^{(s)}]=[R_{-1,T}+R_{+1,T}]. \]

The last identity reveals that the dueling bandits problem can be addressed using a 'black-box' strategy, where each player is allowed to use a standard Multi-Armed Bandit (MAB) algorithm. In , the authors selected the EXP3 algorithm, which provides guarantees suitable for worst-case scenarios. It's important to note that achieving problem-dependent bounds is not possible when the players use stochastic MAB procedures such as Upper Confidence Bounds algorithms, as the losses experienced by the first player, for example, are not stationary. In a later work,  implemented a best-of-both-worlds MAB algorithm, specifically the online mirror descent with the Tsallis-INF regularizer . This approach is effective because, from the perspective of the first player, the loss distribution, although variable, is not entirely arbitrary. This is due to the second player's strategy of minimizing their own regret, which involves concentrating on sampling distributions that approximate those associated with the optimal choice, corresponding to the Condorcet winner.

Adopting the reduction above to solve the weak regret dueling bandit problem seems however insufficient due to several reasons. First, Equation (4) shows that minimizing the strong regret, and minimizing the regrets of individual players is equivalent. Second, the weak regret can be significantly smaller than the strong regret. This is because selecting the Condorcet winner just once is sufficient to suffer zero instantaneous weak regret while leaving the second arm free to explore and gain information about the problem. This is not the case in the strong regret minimization where both selected arms have to be Condorcet winners to incur zero instantaneous strong regret. This suggests that the algorithms that are optimal for strong regret cannot be expected to be optimal for weak regret.

[MISSING_PAGE_FAIL:6]

**Theorem 4.2**.: _Consider Algorithm 1 with \(=2/3\) and \(_{t}=2K^{-1/6}/\). For any \(T 1\), the weak regret satisfies:_

\[[R_{T}^{(w)}] c}}},k}}},\]

_where \(c\) is a numerical constant and \(_{*}=_{k k^{*}}_{k^{*},k}\)._

We obtain an upper-bound on weak regret of the order of \((}}1/_{k^{*},k}})\). In the setting where all the gaps for the Condorcet winner are constant, the upper bound above translates to \((K/_{}})\). The last optimality result is shown in Theorem 5.1.

### Algorithm 2: Weak Regret-EXP3-IX

This algorithm uses the Implicit Exploration strategy (EXP3-IX) , which is adapted specifically for the dueling bandits problem. At its core, the algorithm selects one arm (left arm \(I_{t}\)) for exploitation and another (right arm \(J_{t}\)) for exploration. Given \(I_{t}\), \(J_{t}\) is selected following the EXP3-IX procedure. We chose the EXP3-IX algorithm (restated in Section D.2 of the Appendix), particularly the version without a fixed horizon for technical reasons, namely its bounds on cumulative loss that hold with high probability. To clarify the notation used: in each round, we observe the result of the duel between \(I_{t}\) and \(J_{t}\), denoted by \(X_{t}(I_{t},J_{t})\). The variable \(X_{t}(i,j)\) represents the duel outcome between arm \(i\) and arm \(j\) in round \(t\).

The algorithm operates across multiple stages, where each stage \(n 1\) is defined by a threshold value \(B=2^{n-1}\), updated through a doubling technique. At every stage, given \(B\), we consistently select the left arm as \(I_{t}=i\), and consider a standard Multi-Armed Bandit problem where the choices are the duels between arm \(i\) and the other arms in \([K] i\). Specifically, these choices relate to the variables \(X_{t}(i,j)-:j[K] i\). Recall that \([X_{t}(i,j)-]=_{i,j}\), and the optimal arm, which minimizes cumulative loss, is \(j^{*}(i)=_{j[K] i}_{i,j}\). The cumulative loss after executing EXP3-IX for this specified problem over \(u\) rounds is denoted by \(S(i,n,u)\).

\[S(i,n,u):=_{s=r+1}^{u+}(X_{s}(i,J_{s})-),\]

where \(\) represents the round at which the procedure starts. We continue the procedure until the value of \(S(i,n,u)\) reaches the threshold \(-B\). When this threshold is met, we transition to the next arm, \(i+1\), and address the duels involving this new arm. A stage is completed once all arms have met this stopping criterion, allowing the algorithm to advance to the next stage, \(n+1\).

The underlying rationale of the algorithm is as follows: consider stage \(n\), by design of the algorithm, if \(i\) is a sub-optimal arm, then after a constant number of rounds, the process \(S(i,n,u)\) mimics a random walk characterized by a negative drift of \(_{i,j^{*}(i)}<0\). We demonstrate that \(S(i,n,u)\) typically reaches the threshold \(-B\) when \(u\) is approximately of the order \(\{K,B\}/_{j^{*}(i),i}^{2}\). In contrast, the process \(S(k^{*},n,u)\), which is linked to the Condorcet winner, has a positive drift. We show that the probability of the last process never meeting the threshold \(-B\) for some \(u 1\) is less than \(\{1,(-B^{2})(1/_{*})\}\), where \(_{*}=_{k k^{*}}_{k^{*},k}\). Consequently, there is a high probability that, at some stage, the algorithm will be trapped in a loop where the left arm is the Condorcet winner leading to zero regret when considering weak regret.

**Theorem 4.3**.: _Under the assumption of the existence of a Condorcet winner, the weak regret of Algorithm 2 satisfies:_

\[[R_{T}^{(w)}] c(K/_{*})_{k k^{*}} ,k}}{_{j^{*}(k),k}^{2}},\]

_where for each \(k k^{*}\): \(j^{*}(k)_{j}_{j,k}\), \(_{*}=_{k k^{*}}_{k^{*},k}\) and \(c=c^{}\{1,(K 16)\}\) with \(c^{}

this case, as argued in Section 3.2, the algorithm should explore the \(K^{2}\) duals to detect sub-optimal arms. More rigorously, consider an example where the gaps satisfy: For all \(i k^{*}\): \(_{k^{*},i}=_{}\) and \(_{j k}\). \(_{j,i}=_{}\), where \(_{}\) and \(_{}\) are positive constants. Then the upper bound in Theorem 4.3 is of order \(K^{2}_{}/_{}^{2}\). The last bound is sharper than the bound for WR-TINF (which is of order \(K/_{}\)), when we have \(_{}/_{}>1/\).

## 5 Lower Bound

In this section, we provide a lower bound on the largest weak regret of any algorithm, when confronted with a given set of dueling bandit problems, which we will discuss below.

Let \(_{}(0,1/4)\) denote a positive number. For a dueling bandits problem, define the class of problems \((_{})\) by the set of matrices \(M\) representing the gaps \((_{i,j})_{ij}\) such that \(M\) is skew-symmetric and there exists some \(k^{*}[K]\) (representing the Condorcet winner) such that:

\[ i k^{*}:M_{k^{*},i}=_{}\;\;\;\; i,j k^{*}:|M_{i,j}|_{}.\]

The introduced class of matrices \((_{})\) includes many natural instances, such as when the gaps satisfy the general identifiability assumption. This assumption states that for each sub-optimal arm \(j\), the arm with the highest probability to beat \(j\) is the Condorcet winner \(k^{*}\): i.e., \(k^{*}_{i[K]}_{j,i}\). It has been considered in prior works such as  and more specifically it is implied by strong stochastic transitivity assumption (Section 3.1 of ).

**Theorem 5.1**.: _Fix \(K 6\), \(_{}(0,1/4)\). The weak regret of an algorithm \(\) satisfies:_

\[_{M(_{})}_{M,}[R_{T} ] c}},\]

_when \(T c^{}K/_{}^{2}\). Here \(c\) and \(c^{}\) are numerical constants._

The result in Theorem 5.1 proves that Algorithm 1 is optimal for the considered instance, particularly highlighting that linear scaling with \(K\) is optimal in this case. In the lower bound, we assumed uniform gaps between the Condorcet winner and the sub-optimal arms (equal to \(_{}\)). A potential improvement would be to develop a lower bound that depends on all the gaps with the Condorcet winner \((_{k^{*},i})_{i[K]}\). Additionally, a more general lower bound should discard the general identifiability assumption. As previously argued, if the gaps between sub-optimal arms are large compared to the gaps with the Condorcet winner, it becomes easier to explore the \(K^{2}\) duals to detect the sub-optimality of the arms and focus decision-making on the Condorcet winner.

## 6 Experiments

In this section, we perform a numerical evaluation of WR-EXP3-IX and WR-TINF algorithms in three different scenarios that favor different algorithms according to the prior theoretical results. As a benchmark for our experiments, we utilize the state-of-the-art algorithm for weak regret, WS-W . Additionally, we include one of the best-performing algorithms for strong regret, Versatile-DB, to demonstrate that optimizing for strong regret does not necessarily translate into optimal weak regret performance. For each of the experiments, we plot the mean regret over 20 iterations together with \(0.2\) and \(0.8\) quantiles. All the experiments in this section use theoretical values of parameters for the algorithms. The runtime of each algorithm and iteration is in terms of minutes on a personal computer.

Scenario 1: weak regret under SST (Figure 0(a)).We consider here: \(K=30\), \(T=10000\), \(q_{i,j}=1-q_{j,k^{*}}=0.8\) for \(i,j[K]\) such that \(i<j\). In this scenario, we have a small number of arms and the SST holds - the arm with the lower index always wins with probability \(0.8\). This favors WS-W and WR-EXP3-IX algorithms. On the other hand, WR-TINF is a explores less, this results in larger regret for small \(K\) while the algorithm shines as \(K\) grows.

Scenario 2: Strong and weak regret comparison without SST (Figures 0(b) and 0(c)).We consider here: \(K=150\), \(T=100000\), \(q_{k^{*},i}=0.9\) for every \(i[K]\{k^{*}\}\), \(q_{i,j}=0.9\) (resp. \(q_{i,j}=0.1\)) for \(i,j[K]\{k^{*}\}\) such that \(i<j\) and \((i+j) 0 2\) (resp. \((i+j) 1 2\)). In this scenario, we have a moderately large number of arms and SST does not hold - each arm, except for the Condorcet winner, wins against approximately \(K/2\) (every other index) other arms with probability \(0.9\) and loses to the other arms with probability \(0.1\). This should still favor WR-EXP3-IX algorithm but lack of ordering makes WS-W algorithm perform slightly worse. Algorithm WR-TINF slightly closes the gap in weak regret thanks to the increased number of arms. This can be seen in Figure 0(b). For completeness of comparison, we also plot strong regret of the algorithms, see Figure 0(c). Naturally, algorithms WS-W and WR-EXP3-IX suffer linear strong regret since they never play the same arm twice. However, WR-TINF performs well even with extra exploration, needed for weak regret, compared to Versatile-DB.

Scenario 3: large number of arms, no SST (Figure 0(d)).We consider: \(K=400\), \(T=50000\), \(q_{k^{*},i}=0.9\) for every \(i[K]\{k^{*}\}\), \(q_{i,j}=0.9\) (resp. \(q_{i,j}=0.1\)) for \(i,j[K]\{k^{*}\}\) such that \(i<j\) and \((i+j) 0 2\) (resp. \((i+j) 1 2\)). The same setup without SST as in Scenario 2 but with a larger \(K\). Better scaling with \(K\) gives algorithm WR-TINF an edge over algorithms WS-W and WR-EXP3-IX while WS-W still suffers from the lack of SST.

Figure 1: Performance of algorithms in different scenarios

**Remark 6.1**.: _On the variance of the WS-W algorithm: WS-W is a round-based procedure where the selected arms, "winner and challenger," duel in batches of iterations. The length of each batch increases with the number of duels won by the selected arms so far. When an arm loses, it is replaced by a contender chosen from the remaining arms. Once the set of candidate arms is exhausted, the process is repeated. In numerical experiments, particularly with a large number of arms (Scenario 3 in the simulations section), we observe that in some unfortunate cases, especially in the early stages, the CW may lose its duels. This results in a large number of iterations before it is picked again as a contender, leading to very high weak regret for the procedure. Although such outcomes are infrequent, they significantly impact the empirical variance of the weak regret of WS-W._

## 7 Conclusion and limitations

In this work, we addressed the problem of weak regret analysis under the assumption of a Condorcet winner. We showed that, it is impossible in general to achieve a weak regret smaller than \(K/(_{i k^{*}}_{k^{*}i})\) and we introduced the procedure WR-TINF which achieves this bound. The second algorithm, WR-EXP3-IX, employs a more aggressive exploration strategy by querying the \(K^{2}\) duels. We show that in some cases, this approach, despite inducing a quadratic dependence on \(K\) can outperform WR-TINF, because it better adapts to the gaps between suboptimal arms. This is the first work in duelling bandit with weak regret that establishes how that the full matrix \(\) can be leveraged in the regret.

This work gives rise to several open questions. While WR-TINF is optimal in certain instances, developing algorithms that fully adapt to the underlying problem parameters remains a significant challenge.