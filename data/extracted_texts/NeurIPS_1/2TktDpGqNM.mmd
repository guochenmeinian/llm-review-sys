# Overcoming Common Flaws in the Evaluation of Selective Classification Systems

Jeremias Traub\({}^{1,2}\) Till J. Bungert\({}^{1,2,6}\) Carsten T. Luth\({}^{1,2}\) Michael Baumgartner\({}^{2,3,6}\) Klaus H. Maier-Hein\({}^{2,3,5,6,7}\) Lena Maier-Hein\({}^{2,4,6,7}\) Paul F. Jager\({}^{1,2}\)

\({}^{1}\)German Cancer Research Center (DKFZ) Heidelberg, Interactive Machine Learning Group, Germany

\({}^{2}\)Helmholtz Imaging, DKFZ Heidelberg, Germany

\({}^{3}\)DKFZ Heidelberg, Division of Medical Image Computing (MIC), Germany

\({}^{4}\)DKFZ Heidelberg, Division of Intelligent Medical Systems (IMSY), Germany

\({}^{5}\)Pattern Analysis and Learning Group, Department of Radiation Oncology, Heidelberg University Hospital, 69120 Heidelberg, Germany

\({}^{6}\)Faculty of Mathematics and Computer Science, University of Heidelberg, Germany

\({}^{7}\)National Center for Tumor Diseases (NCT) Heidelberg

jeremias.traub@dkfz-heidelberg.de

###### Abstract

Selective Classification, wherein models can reject low-confidence predictions, promises reliable translation of machine-learning based classification systems to real-world scenarios such as clinical diagnostics. While current evaluation of these systems typically assumes fixed working points based on pre-defined rejection thresholds, methodological progress requires benchmarking the general performance of systems akin to the \(\) in standard classification. In this work, we define 5 requirements for multi-threshold metrics in selective classification regarding task alignment, interpretability, and flexibility, and show how current approaches fail to meet them. We propose the Area under the Generalized Risk Coverage curve (\(\)), which meets all requirements and can be directly interpreted as the average risk of undetected failures. We empirically demonstrate the relevance of \(\) on a comprehensive benchmark spanning 6 data sets and 13 confidence scoring functions. We find that the proposed metric substantially changes metric rankings on 5 out of the 6 data sets.

## 1 Introduction

Selective Classification (SC) is increasingly recognized as a crucial component for the reliable deployment of machine learning-based classification systems in real-world scenarios, such as clinical diagnostics . The core idea of SC is to equip models with the ability to reject low-confidence predictions, thereby enhancing the overall reliability and safety of the system . This is typically achieved through three key components: a classifier that makes the predictions, a confidence scoring function (CSF) that assesses the reliability of these predictions, and a rejection threshold \(\) that determines when to reject a prediction based on its confidence score.

In evaluating SC systems, two primary concepts are essential: risk and coverage. Risk expresses the classifier's error potential and is typically measured by its misclassification rate. Coverage, on the other hand, indicates the proportion of instances where the model makes a prediction rather than rejecting it. An effective SC system aims to minimize risk while maintaining high coverage, ensuring that the model provides accurate predictions for as many instances as possible.

Current evaluation of SC systems often focuses on fixed working points defined by pre-set rejection thresholds[Geifman and El-Yaniv, 2017, Liu et al., 2019, Geifman and El-Yaniv, 2019]. For instance, a common evaluation metric might be the selective risk at a given coverage of \(70\%\)[Geifman and El-Yaniv, 2019], which communicates the potential risk associated with a specific confidence score c and selection threshold \(\) to a patient or end-user. While this method is useful for communicating risk in specific instances, it does not provide a comprehensive evaluation of the system's overall performance. In standard classification, this is analogous to the need for the Area Under the Receiver Operating Characteristic (AUROC) curve rather than evaluating sensitivity at a specific specificity, which can be highly misleading when assessing general classifier capabilities [Maier-Hein et al., 2022]. The AUROC provides a holistic view of the classifier's performance across all possible thresholds, thereby driving methodological progress. Similarly, SC requires a multi-threshold metric that aggregates performance across all rejection thresholds to fully benchmark the system's capabilities.

Several current approaches attempt to address this need for multi-threshold metrics in SC. The Area Under the Risk Coverage curve (\(\)) is the most prevalent of these Geifman et al. . However, we demonstrate in this work that \(\) has significant limitations as it fails to adequately translate the risk from specific working points into a meaningful aggregated evaluation score. This shortcoming hampers the ability to benchmark and improve SC methodologies effectively.

In our work, we aim to bridge this gap by providing a robust evaluation framework for SC. Our contributions are as follows:

**Refined Task Formulation:** We are the first to provide a comprehensive SC evaluation framework, explicitly deriving meaningful task formulations for different evaluation and application scenarios such as working point versus multi-threshold evaluation.

**Formulation of Requirements:** We define five critical requirements for multi-threshold metrics in SC, focusing on task suitability, interpretability, and flexibility. We assess current multi-threshold metrics against our five requirements and demonstrate their shortcomings.

**Proposal of AUGRC:** We introduce the Area Under the Generalized Risk Coverage (\(\)) curve, a new metric designed to overcome the flaws of current multi-threshold metrics for SC. \(\) meets all five requirements, providing a comprehensive and interpretable measure of SC system performance.

**Empirical Validation:** We empirically demonstrate the relevance and effectiveness of \(\) through a comprehensive benchmark spanning 6 datasets and 13 confidence scoring functions.

In summary, our work presents a significant advancement in the evaluation of SC systems, offering a more reliable and interpretable metric that can drive further methodological progress in the field.

## 2 Refined Task Formulation

Selective Classification (SC) systems consist of a classifier \(m:\), which outputs a prediction and a confidence scoring function (CSF) \(g:\), which outputs a confidence score associated with the prediction. Assuming a supervised training setup, let \(\{(x_{i},y_{i})\}_{i=1}^{N}\) be a dataset containing \(N\) independent samples from the source distribution \(P(X,Y)\) over \(\). Given a rejection threshold \(\), the model prediction is accepted only if the corresponding score is larger than \(\):

\[(m,g)(x)m(x),&g(x)\\ ,& \]

Given an error function \(:^{+}\), the overall risk of the classifier \(m\) is given by \(R(m)_{i=1}^{N}(m(x),y)\). The error function thereby contains the measure of classification performance suitable for the task at hand. Commonly, SC literature assumes a 0/1 error corresponding to the failure indicator variable \(Y_{}\) with \(y_{,i}(x_{i},y_{i},m)[y_{i} m(x_{i})]\). In this case, the overall risk represents the probability of misclassification \(P(Y_{}=1)\). By rejecting low-confidence predictions, the selective classifier can decrease the risk of misclassifications at the cost of letting the classifier make predictions on only a fraction of the dataset. This fraction is denoted as the coverage \( P(g(x))\), with the respective empirical estimator \(_{i=1}^{N}(g(x_{i}))\). Evaluation of SCsystems often focuses on preventing "silent", i.e. undetected failures for which both \(y_{,i}=1\) and \(g(x_{i})\).

Deploying a SC system in practice requires the selection of a fixed rejection threshold \(\). In the following refined evaluation protocol, we distinguish between the well-established application-specific task formulation where SC models are evaluated at individual working points (Section 2.1) and SC method development where evaluation is independent of individual working points (Section 2.2).

### Evaluating SC systems in applied settings

Current Selective Classification evaluation commonly reports the selective risk of the system at a given coverage ("risk@coverage"), which implies a pre-determined cutoff on the risk or coverage (e.g. "maximum risk of 5%") leading to a working point of the system defined by a rejection threshold \(\). The selective risk is defined as:

\[_{(m,g)}()^{N}(m(x_{i }),y_{i})(g(x_{i}))}{_{i=1}^{N}(g(x_{i}) )} \]

This formulation only considers accepted predictions with \(c>\). For a binary failure error, this risk is an empirical estimator for the conditional probability \(P(Y_{}=1|g(x))\). Thus, this metric effectively communicates the risk of a "silent" failure for a specific prediction of classifier \(m\) that has been accepted (\(c>\)) at a pre-defined threshold \(\). This information may be useful in applied medical settings, e.g. to communicate the risk of misclassification for a patient p whose associated

Figure 1: **The AUGRC metric based on Generalized Risk overcomes common flaws in current evaluation of Selective classification (SC). a) Refined task definition for SC. Analogously to standard classification, we distinguish between holistic evaluation for method development and benchmarking using multi-threshold metrics versus evaluation of specific application scenarios at pre-determined working points. The current most prevalent multi-threshold metric in SC, \(\), is based on Selective Risk, a concept for working point evaluation that is not suitable for aggregation over rejection thresholds (red arrow). To fill this gap, we formulate the new concept of Generalized Risk and a corresponding metric, \(\) (green arrow). b) We formalize our perspective on SC evaluation by identifying five key requirements for multi-threshold metrics and analyze how previous metrics fail to fulfill them. Abbreviations, CSF: Confidence Scoring Function, AU(G)RC: Area Under the (Generalized) Risk Coverage curve.**

prediction has been accepted by a given SC system (\(c_{p}>\)). Aside from the selective risk, other performance measurements for working point selection include for example Classification quality and Rejection quality (Condessa et al., 2017).

### Evaluating SC systems for method development and benchmarking

Evaluating the general performance of a Selective Classification system requires a multi-threshold metric for a more comprehensive view compared to a working point analysis based on a fixed singular rejection threshold, analogous to binary classification tasks, where the Area Under the Receiver Operating Characteristic (AUROC) is used to assess the entire system performance across multiple classifier thresholds. However, the working point analysis can not be simply translated to multi-threshold aggregation, as it is based on Selective Risk, which only considers the risk w.r.t accepted predictions, assuming that a specific selection has already occurred (\(P(Y_{}=1 g(x))\)). This evaluation ignores rejected cases and thus contradicts the holistic assessment, where we are interested in the general risk for _any_ prediction causing a silent failure independent of the future selection decision of individual predictions. This holistic assessment is reflected by the joint probability \(P(Y_{}=1,g(x))\), which reflects the risk of silent failure for any prediction processed by the system including cases that are potentially rejected in the future.

To address the discrepancy between the need for holistic assessment versus the selective risk's focus on already selected cases, we formulate the generalized risk:

\[(m,g)()_{i=1}^{N}(m(x _{i}),y_{i})[g(x_{i})]. \]

This metric reflects the joint probability of misclassification and acceptance by the confidence score threshold. By aggregating this risk over multiple rejection thresholds, we can evaluate the overall performance of the SC system.

### Requirements for Selective Classification multi-threshold metrics

Based on the refined task definition given above, we can formulate concrete five requirements R1-R5 for multi-threshold metrics in Selective Classification:

**R1: Task Alignment.** The general goal in SC is to prevent silent failures either by preventing failures in the first place (via classifier performance), or by detecting the remaining failures (via CSF ranking quality). As argued in Jager et al. (2023), it is crucial to jointly evaluate both aspects of an SC system, since the choice of CSF generally affects the underlying classifier. **R2: Monotonicity.** The metric should be monotonic w.r.t both evaluated factors stated in R1, i.e. improving on one of the two factors while keeping the other one fixed results in an improved metric value. Note that R2 does not make assumptions about how the two factors are combined, but it represents a minimum requirement for meaningful comparison and optimization of the metric. **R3: Ranking Interpretability.** Interpretability is a crucial component of a metric (Maier-Hein et al., 2022). AUROC is the de facto standard ranking metric for binary classification tasks, providing an intuitive assessment of ranking quality which is proportional to the number of permutations needed for establishing an optimal ranking. This can also be interpreted as the "probability of a positive sample having a higher score than a negative one". Ideally, the SC metric should follow this intuitive assessment of rankings. **R4: CSF Flexibility.** The metric should be applicable to arbitrary choices of CSFs. This includes external CSFs, i.e. scores that are not based directly on the classifier output. **R5: Error Flexibility.** Current SC literature largely focuses on 0/1 error (i.e. \(1-\) accuracy) in their risk computation. However, in many real-world scenarios, accuracy is not an adequate classification metric, such as in the presence of class imbalance and for pixel-level classification. Thus, the SC metric should be flexible w.r.t the choice of error function.

### Current multi-threshold metrics in SC do not fulfill requirements R1-R5

**AURC:** Geifman et al. (2018) derive the \(\) as the Area under the Selective Risk-Coverage curve. This metric is the most prevalent multi-threshold metric in SC (Geifman et al., 2018; Jager et al., 2023; Bungert et al., 2023; Cheng et al., 2023; Zhu et al., 2023; Varshney et al., 2020; Naushad and Voiculescu, 2024; Van Landeghem et al., 2024; 2023; Zhu et al., 2022; Xin et al.,2021, Yoshikawa and Okazaki, 2023, Ding et al., 2020, Zhu et al., 2023b, Galli and El-Yaniv, 2021, Franc et al., 2023, Cen et al., 2023, Xia and Bouganis, 2022, Cattelan and Silva, 2023, Tran et al., 2022, Kim et al., 2023, Ashukha et al., 2020, Xia et al., 2024]. For the 0/1-error, \(\) can be expressed through the following integral:

\[=_{0}^{1}P(Y_{}=1|g(x))\,P(g(x) ) \]

This integral effectively aggregates the Selective Risk (Equation 2) over all fractions of accepted predictions (i.e. coverage). However, as discussed in Section 2.2, the Selective Risk is not suitable for aggregation over rejection thresholds to holistically assess a SC system. This inadequacy effectively leads to an excessive over-weighting of high-confidence failures in \(\) and in return to breaching the requirements of monotonicity (R2) and ranking interpretability (R3). We provide empirical evidence for these shortcomings on toy data (Section 3) and real-world data (Section 4.2). Further, current SC literature usually employs the 0/1 error function, even though the related Accuracy metric is well known to be an unsuitable classification performance measure for many applications. For example, Balanced Accuracy is used to address class imbalance and metrics such as the Dice Score are employed for segmentation. Moving beyond the 0/1 error yields a higher variance in distinct error values and may thus amplify the shortcoming of over-weighting high-confidence failures. As more sophisticated error functions being are an important direction of future SC research, it is crucial to ensure the compatibility of metrics in SC evaluation.

**e-AURC:**Geifman et al.  further introduce the excess-\(\) as the difference to the \(\) of an optimal confidence scoring function (denoted as \(^{*}\))

\[=-^{*} \]

It relies on the intuition that subtracting the optimal \(\) eliminates the contribution of the overall classification performance and collapses it to a pure ranking metric [Geifman et al., 2018, Gali et al., 2023, Jager et al., 2023]. However, several works demonstrated that this intuition does not hold and that the \(\) is still sensitive to the overall classification performance [Gali et al., 2023, Cattelan and Silva, 2023]. We attribute this deviation from the intended behavior to the shortcomings of the underlying \(\), as we find the desired isolation of classifier performance in our improved metric formulation in Section 3. e-AURC further inherits the shortcomings of \(\) w.r.t monotonicity (R2), ranking interpretability (R3), and error flexibility (R5).

**NLL / Brier Score:** Importantly, proper scoring rules such as the Negative-Log-Likelihood and the Brier Score are technically not multi-threshold metrics. Yet, we include them here as they also aim for a holistic performance assessment, i.e. assessment beyond individual working points, by evaluating a general meaningfulness of scores based on the softmax-output of the classifier [Ovadia et al., 2019]. Thereby, they jointly assess ranking and calibration of scores, which dilutes the focus on ranking quality in the context of SC [Jager et al., 2023]. In our formulation, the calibration aspect in these metrics breaks the required monotonicity w.r.t SC evaluation (R2). Further, they are not applicable to CSFs beyond those that are directly based on the classifier output (R4). **AUROC:** The "failure version" of the standard AUROC assesses the correctness of predictions with a binary failure label [Jager et al., 2023]. Based on the AUROC, it provides an intuitive ranking quality assessment. However, it ignores the underlying classifier performance (R1, R2), and is restricted to binary error functions (R5). **OC-AUC:**Kivlichan et al.  introduce the Oracle-Model Collaborative AUC, where first a fixed threshold is applied on the confidence scores, above which error values are set to zero. Then, the \(_{}\) (or the Average Precision) are evaluated. This metric is also reported in [Dehghani et al., 2023, Tran et al., 2022], with a review fraction of 0.5%. OC-AUC is subject to the same pitfalls as \(_{}\) (R1, R2, R5). **AUROC-AUC:**Pugnana and Ruggieri  propose the AUROC-AURC where the Selective Risk is defined as the classification (not failure) AUROC computed over the set of accepted predictions. However, it is only applicable to binary classification models with binary error functions (R5). Further, in employing Selective Risk it inherits the pitfalls described for \(\) regarding monotonicity (R2) and ranking interpretability (R3). **NAURC:**Cattelan and Silva  propose the Normalized-\(\), a min-max scaled version of the \(\), and claim that this modification eliminates its lack of interpretability. However, given the linear relationship with the \(\), it does not fulfill the requirements R2 and R3. **F1-AUC:**Malinin and Gales , Malinin et al.  introduce the notion of Error-Retention Curves, which corresponds to that of Generalized Risk Coverage Curves. However, in Malinin et al.  the authors propose an F1-AUC metric which is only applicable to binary error functions (R5) and breaks monotonicity (R2), as it decreases with increasing accuracy for accuracy values above \( 0.56\) (see Appendix A.1.3 for a detailed explanation.) **ARC:** Accuracy-Rejection-Curves [Nadeem et al., 2009, Ashukha et al., 2020, Condessa et al., 2017] and the associated AUC directly correspond to the \(\) and are therefore subject to the same pitfalls (R2, R3).

## 3 Area under the Generalized Risk Coverage Curve

In Section 2.2, we illustrate that aggregating SC performance across working points requires to shift the perspective from the Selective Risk to the Generalized Risk (Equation 3) as an _holistic_ assessment of the risk of silent failures for all predictions, before the rejection decision is made. We propose to evaluate SC methods via the Area under the Generalized Risk Coverage Curve (\(\)). For the binary failure error it becomes an empirical estimator of the following expression:

\[=_{0}^{1}P(Y_{}=1,\,g(x))\,P(g(x )) \]

This metric evaluates SC models in terms of the rate of silent failures averaged over working points and thus provides a practical measurement that is directly interpretable. It is bounded to the \([0,}{{2}}]\) interval, whereby lower \(\) corresponds to better SC performance. The \(\) is not subject to the shortcomings of the \(\), and we can derive a direct relationship to \(_{}\) and \(\) (the derivation and visualizations are shown in Equation 8 and Figure 5):

\[=(1-_{})(1- )+(1-)^{2} \]

To illustrate, when drawing two random samples, it corresponds to the probability that either both are failure cases or that one is a failure and has a higher confidence score than the non-failure. This

Figure 2: **The proposed AUGRC metric resolves shortcomings of AURC.** All figures are based on rankings of predictions according to descending associated confidence scores induced by a CSF. All \(\), \(\), and \(\) values are scaled by \( 1000\). **a)** shows the contribution of an individual failure case on the \(\) and \(\) metrics depending on its ranking position (for technical details, see Section A.1.1). While \(\) reflects the intuitive behavior of weighing the failure cases proportional to their ranking positions, the \(\) puts excessive weight on high-confidence failures. **b-d)** Toy example of three CSFs ranking 20 predictions to show how \(\) resolves the broken monotonicity requirement (R2) of \(\). Despite equal \(_{}\) and equal \(\) in CSF-1 and CSF-2, the \(\) improves. And \(\) even improves in CSF-3, which features lower \(_{}\) and lower \(\) compared to CSF-1. **e-f)** The corresponding risk-coverage curves reveal that the non-intuitive behavior of \(\) is due to the excessive effect of the high-confidence failure of CSF-1 on the Selective Risk, which is resolved in the Generalized Risk.

is reflected by the second and first term of Equation 7, respectively. As an example, a CSF that outputs random scores yields \(}=\), and hence \(=(1-)\). The \(\) for an optimal CSF (\(}=1\)) is given by the second term in Equation 7, hence subtracting the optimal SC performance yields a pure ranking measure re-scaled by the probability of finding a positive/negative pair. This overcomes the lack of interpretability of \(\) and \(\)-\(\) (R3). Monotonicity (R2) is ensured since the partial gradients w.r.t. both \(}\) and \(\) are always negative. Further, it can accommodate arbitrary classification metrics via the error function \(\) (R4) as well as arbitrary CSF (R5).

Figure 1(a) depicts the metric contribution of individual failure cases depending on their ranking position. This shows empirically how the excessive over-weighting of high-confidence failures in \(\) is resolved by \(\), and how the intuitive ranking assessment of \(}\) is established (see Section A.1.1 for a detailed derivation). We further showcase how \(\) resolves the broken monotonicity requirement (R2) of \(\) in Figures 2 b-d. Despite equal \(}\) and equal \(\) in CSF-1 and CSF-2, the \(\) improves. And \(\) even improves in CSF-3, which features lower \(}\) and lower \(\) compared to CSF-1. Figures 2 e-f depict the associated risk-coverage curves and reveal that the non-intuitive behavior of \(\) is due to the excessive effect of the high-confidence failure of CSF-1 on the Selective Risk, which is resolved in the Generalized Risk. In Section 4.2 we demonstrate implications of \(\)'s shortcomings on real-world data.

## 4 Empirical Study

We conduct an empirical study based on the existing _FD-Shifts_ benchmarking framework (Jager et al., 2023), which compares various CSFs across a broad range of datasets. The focus of our study is not on the performance of individual methods (CSFs) but rather on the ranking of methods based on \(\) evaluation as compared to \(\). We utilize the same experimental setup as in Jager et al. (2023) with the addition of CSF scores based on temperature-scaled classifier logits. Extending the benchmark by recent SC methods, e.g. Feng et al. (2022), is an interesting direction of future work. A detailed overview of the datasets and methods used can be found in Appendix A.2. The code for reproducing our results and a PyTorch implementation of the \(\) are available at: [https://github.com/IML-DKFZ/fd-shifts](https://github.com/IML-DKFZ/fd-shifts).

### Comparing Method Rankings of AUGRC and AURC

To study the relevance of \(\), we investigate the changes induced by \(\) on rankings of CSFs compared to \(\) method rankings.

**CSF Rankings with AUGRC substantially deviate from AURC rankings**. Figure 3 illustrates the method ranking differences for all i.i.d. test datasets, showing changes in ranks across all CSFs. Notably, \(\) induces changes in the top-3 methods (out of 13) on 5 out of 6 datasets. To ensure that these ranking changes are not due to variability in the test data, we evaluate the metrics on \(500\) bootstrap samples from the test dataset and derive the compared method rankings based on the average rank across these samples ("rank-then-aggregate"). The size of each bootstrap sample corresponds to the size of the test dataset. Metric values for each bootstrap sample are averaged over 5 training initializations (2 for BREEDS and 10 for CAMELYON-17-Wilds). We analyze the robustness of the resulting method rankings for \(\) and \(\) separately, based on statistical significance. To that end, we perform pairwise CSF comparisons in form of a one-sided Wilcoxon signed-rank test (Wilcoxon, 1992) on the bootstrap samples. To control the family-wise error rate at a 5% significance level, we apply the Holm correction for multiple testing per metric and dataset, following (Wiesenfarth et al., 2021) (see Figure 7 for the results without correction for multiple testing). The resulting significance maps displayed in Figure 3 indicate stable method rankings for both \(\) and \(\). This suggests that the observed ranking differences are induced by the conceptual difference of \(\) compared to \(\) and, as a consequence, that the shortcomings of current metrics and respective solutions discussed in Section 2, are not only conceptually sound, but also highly relevant in practice. For the results in Figure 3, we select the DeepGamblers reward parameter and whether to use dropout for both metrics separately on the validation dataset (details in Table 1 and Table 2).

in Table 4, which also includes results for equal hyperparameters for \(\) and \(\). Results for the bootstrap-based method ranking analysis for distribution shifts are displayed in Figure 6. The substantial differences in method rankings across all datasets and distribution shifts underline the relevance of \(\) for SC evaluation. The complete \(\) results on the _FD-Shifts_ benchmark are shown in Table 3.

In few cases, as shown in Figure 3, we observe that a CSF A is ranked worse than a neighboring CSF B even though CSF A's ranks are significantly superior to those of CSF B based on the Wilcoxon statistic over the bootstrap samples. This discrepancy can occur if the rank variability in CSF A is larger than in CSF B. While this does not affect our conclusions regarding the relevance of the \(\), it indicates that selecting a single best CSF for application based solely on method rankings should be approached with caution. We recommend closer inspection of the individual CSF performance in such cases, although this is not the focus of our study.

Figure 3: **Substantial differences in method rankings for AUGRC and AUGRC.** On 5 out of 6 datasets, the top-3 CSFs (out of 13 compared methods) change when employing the proposed \(\) instead of \(\). This demonstrates the practical relevance of the \(\) metric for Selective Classification evaluation. CSFs are color-coded and sorted from top (best) to bottom (worst) by average rank based on \(500\) bootstrap samples from the test dataset to ensure ranking stability. Ranking changes are reflected in changes in the color sequence and highlighted by red arrows. We assess the stability of the method rankings for each metric individually using one-sided Wilcoxon signed-rank tests based on the bootstrap samples at \(5\%\) significance level with adjustment for multiple testing according to Holm. Adjacent to each ranking, we present the resulting significance maps for the pairwise CSF comparisons. These maps can be interpreted as follows: At each grid position \((x,y)\), filled entries indicate that metric values of CSF \(y\) are ranked significantly better than those from CSF \(x\) (across bootstrap samples), cross-marks indicate no significant superiority. An ideal ranking exhibits only filled entries above the diagonal.

### Showcasing Implications of AURC Shortcomings on Real-world Data

Figure 4 gives an in-depth look at how the conceptual shortcomings of \(\) affect method assessment on real-world data. The example uses the confidence like reservation score of the DeepGamblers method (DG-Res) (Liu et al., 2019) and the Monte Carlo Dropout-based predictive entropy (MCD-PE) (Gal and Ghahramani, 2016) as CSFs on the CIFAR-10 test dataset. Despite DG-Res having higher classification performance and ranking quality than MCD-PE, the \(\) erroneously favors MCD-PE over DG-Res. This violates the monotonicity requirement (R2). This can be attributed to the excessive contributions of only few high-confidence failures, which aligns with the theoretical findings on failure contributions shown in Figure 2 (R3). In this example, the high-confidence failures are associated with high label ambiguity or incorrect labeling, suggesting that the \(\) may exacerbate the influence of label noise in practice.

## 5 Conclusion

Despite the increasing relevance of SC for reliable translation of machine learning systems to real-world application, we find that the current metrics have limitations in providing the comprehensive assessment needed to guide the methodological progress of SC systems.

In this work, we establish a systematic SC evaluation framework, thereby promoting the adoption of more comprehensive, interpretable, and task-aligned metrics for comparative benchmarking of SC systems.

We find that none of the existing multi-threshold metrics, particularly the \(\), meet the key requirements we identified for comprehensive SC evaluation, leading to deviations from intended and intuitive performance assessment behaviors. To address this, we introduce the \(\) as a suitable metric for comprehensive SC method evaluation. Substantial differences in method rankings

Figure 4: **The conceptual shortcomings of AURC affects method assessment in practice.** We illustrate the practical effects of excessive weight high-confidence failures in \(\) by comparing the performance of two CSFs, DG-Res and MCD-PE, on the CIFAR-10 test dataset. (a) shows the coverage curves based on Selective Risk and Generalized Risk for both CSFs. The \(\) violates the monotonicity requirement (R2) in practice, favoring MCD-PE despite a lower classification performance and ranking quality compared to DG-Res. (b) displays the images associated with the top-\(k\) high-confidence failures. For DG-Res, the four failures correspond to the first four peaks in the Selective Risk curve, up to coverage \( 0.27\) (the total number of failures is 446). Only a few high-confidence failures significantly increase the \(\). For both CSFs, the images associated with high-confidence failures exhibit high label ambiguity or are incorrectly labeled, indicating that the \(\) may amplify the influence of label noise in practice. AURC and AUGRC values are scaled by \( 1000\).

between \(\) and \(\), demonstrated through extensive empirical studies, highlight the importance of selecting the right SC metric. Thus, we propose the adoption of \(\) for meaningful SC performance evaluation.

When evaluating specific applications for which certain risk or coverage intervals are known to be irrelevant, adaptations such as a partial AUGRC (analogous to the partial AUROC) may be considered. Further, investigating the relation between confidence ranking and calibration and on evaluating SC in settings where calibrated scores are of interest is an interesting direction of future work. Overall, our proposed evaluation framework provides a solid basis for future work in Selective Classification, including developing novel methods as well as analyzing the properties of individual methods.