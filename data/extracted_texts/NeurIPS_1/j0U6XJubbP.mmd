# Versatile Energy-Based Probabilistic Models for High Energy Physics

Taoli Cheng

Mila

University of Montreal

chengtaoli.1990@gmail.com &Aaron Courville

Mila

University of Montreal

aaron.courville@umontreal.ca

###### Abstract

As a classical generative modeling approach, energy-based models have the natural advantage of flexibility in the form of the energy function. Recently, energy-based models have achieved great success in modeling high-dimensional data in computer vision and natural language processing. In line with these advancements, we build a multi-purpose energy-based probabilistic model for High Energy Physics events at the Large Hadron Collider. This framework builds on a powerful generative model and describes higher-order inter-particle interactions. It suits different encoding architectures and builds on implicit generation. As for applicative aspects, it can serve as a powerful parameterized event generator for physics simulation, a generic anomalous signal detector free from spurious correlations, and an augmented event classifier for particle identification.

## 1 Introduction

The Large Hadron Collider (LHC) , the most energetic particle collider in human history, collides highly energetic protons to examine the underlying physics of subatomic particles and extend our current understanding of the fundamental forces of nature, summarized by the Standard Model of particle physics. After the great success in observing the Higgs boson , the most critical task of searching for new physics signals remains challenging. High Energy Physics (HEP) events produced at the LHC have the properties of high dimensionality, high complexity, and enormous data size. To detect rare signals from enormous background events, physics observables describing these patterns have been used to identify different types of radiation patterns. However, it's not possible to utilize all the information recorded by the detectors with a few expert-designed features. Thus deep neural classifiers and generative models, which can easily process high-dimensional data meet the needs for more precise data analysis and signal detection.

Energy-based Models (EBMs) , as a classical generative framework, leverage the energy function for learning dependencies between input variables. With an energy function \(E()\) and constructing the un-normalized probabilities through the exponential \(()=(-E())\), the energy model naturally yields a probability distribution. Despite the flexibility in the modeling, the training of EBMs has been cumbersome and unstable due to the intractable partition function and the corresponding Monte Carlo sampling involved. More recently, EBMs have been succeeding in high-dimensional modeling  for computer vision and natural language processing. At the same time, it has been revealed that neural classifiers are naturally connected with EBMs , combining the discriminative and generative learning processes in a common learning regime. More interestingly, compositionality  can be easily incorporated within the framework of EBMs by simply summing up the energy functions . On the other hand, statistical physics originally inspired the invention of EBMs. This natural connection in formalism makes EBMs appealing in modeling physical systems. In physical sciences, EBMs have been used tosimulate condensed-matter systems and protein molecules . They have also been shown great potential in structure biology , in a use case of protein conformation.

Motivated by the flexibility in the architecture and the compatibility with different tasks, we explore the potential of EBMs in modeling radiation patterns of elementary particles at high energy. The energy function is flexible enough to incorporate sophisticated architectures. Thus EBMs provide a convenient mechanism to simulate complex correlations in input features or high-order interactions between particles. Aside from image generation, applications for point cloud data , graph neural networks  for molecule generation are also explored. In particle physics, we leverage the self-attention mechanism , to mimic the complex interactions between elementary particles.

As one important practical application, neural net-based unsupervised learning of physics events  have been explored in the usual generative modeling methods including Generative Adversarial Networks (GANs)  and Variational Autoencoders (VAEs) . However, GANs employ separate networks, which need to be carefully tuned, for the generation process. They usually suffer from unstable training, high computation demands, and mode collapse. In comparison, VAEs need a well-designed reconstruction loss, which could be difficult for sophisticated network architectures and complex input features. EBMs thus provide a strong alternative generative modeling framework of LHC events, by easily incorporating sophisticated physics-inspired neural net architectures.

At the same time, EBMs can serve as generic signal detectors, since out-of-distribution (OOD) detection comes naturally in the form of energy comparison. More importantly, EBMs incur fewer spurious correlations in OOD detection. This plays a slightly different role in the context of signal searches at the LHC. There are correlations that are real and useful but at the same time hinder effective signal detection. As we will see in Section 4.2, the EBMs are free from the notorious correlation observed in many anomaly detection methods in HEP, in both the generative and the discriminative approaches.

As summarized in Table 1, we build a multi-tasking framework for High Energy Physics. To that end, we construct an energy-based model of the fundamental interactions of elementary particles to simulate the resulting radiation patterns. We especially employ the short-run Markov Chain Monte Carlo for the EBM training. We show that EBMs are able to generate realistic event patterns and can be used as generic anomaly detectors free from spurious correlations. We also explore EBM-based hybrid modeling combining generative and discriminative models for HEP events. This unified learning framework paves for future-generation event simulators and automated new physics search strategies. It opens a door for combining different methods and components towards a powerful multi-tasking engine for scientific discovery at the Large Hadron Collider.

## 2 Problem Statement

Describing HEP EventsMost particle interactions happening at the LHC are governed by Quantum Chromodynamics (QCD), due to the hadronic nature of the proton-proton collision. Thus jets are enormously produced by these interactions. A jet is formed by collimated radiations originating from highly energetic elementary particles (e.g., quarks, gluons, and sometimes highly-boosted electro-weak bosons). The tracks and energy deposits of the jets left in the particle detectors reveal the underlying physics in complex patterns. A jet may have hundreds of jet constituents, resulting in high-dimensional and complex radiation patterns and bringing difficulties in encoding all the information with expert features. Reconstructing, identifying, and classifying these elementary particles manifest in raw data is thus critical for ongoing physics analysis at the LHC.

Specifically, each constituent particle within a jet has \(( p_{T},,)\) as the descriptive coordinates in the detector's reference frame. Here, \(p_{T}\) represents the transverse momentum perpendicular to the

   Topic & Practice \\  Generative modeling & Parameterized event generation \\ Out-of-distribution detection & Model-independent new physics search \\ Hybrid modeling & Classifier combined with EBMs \\   

Table 1: Application aspects of Energy-based Models for High Energy Physics.

beam axis, and (\(,\)) denotes the spatial coordinates within the cylindrical detector. (More details about the datasets can be found in Appendix A.) Thus a jet can be described by the high-dimensional vector \(=\{( p_{T},,)_{i}\}_{i}^{N}\), supposing the jet has N constituents. Our goal is thus to model the data distribution \(p()\) precisely, and \(p(y|)\) in the case of supervised classification with y denoting the corresponding jet type.

Parameterized Generative ModelingAt the LHC, event simulation serves as an important handle for background estimation and data analysis. For many years, physics event simulators  have been built on Monte Carlo methods based on physics rules. These generators are slow and need to be tuned to the data frequently. Deep neural networks provide us with an efficient parameterized generative approach to event simulation for the coming decades. Generative modeling in LHC physics has been experimented with GANs (image-based [58; 27] and point-cloud-based ) and VAEs . There are also GANs  working on high-level features for event selection. However, as mentioned in the introduction, these models all require explicit generators, which brings obstacles to situations with complex data formats and sophisticated neural architectures.

OOD Detection for New Physics SearchesDespite the great efforts in searching for new physics signals at the LHC, there is no hint of beyond-Standard-Model physics. Given the large amount of data produced at the LHC, it has been increasingly challenging to cover all the possible search channels. Experiments at the LHC over the past decades have been focused on model-oriented searches, as in searching for the Higgs boson [26; 36]. The null results up to now from model-driven searches call for novel solutions. We thus shift to model-independent and data-driven anomaly detection strategies, which are data-oriented rather than theory-guided, for new physics searches.

Neural nets-based anomaly detection in HEP takes different formats. Especially, unsupervised generative models trained on background events can be used to detect potential unseen signals. More specifically, Variational Autoencoders [15; 41; 5] have been employed to detect novel signals, directly based on detector-level features. High-level observables-based VAEs [12; 39] have also been explored. However, naively trained VAEs (and Autoencoders) are usually subject to spurious correlations which result in failure modes in anomaly detection, partially due to the format of the reconstruction loss involved. Usually, one needs to explicitly mitigate these spurious correlations with auxiliary tasks such as outlier exposure [35; 15], or modified model components such as autoencoders with smeared pixel intensities  and autoencoders augmented with energy-based density estimation [69; 20].

At the same time, EBM naturally has the handle for discriminating between in-distribution and out-of-distribution examples and accommodates tolerance for spurious correlations. While in-distribution data points are trained to have lower energy, energies of OOD examples are pushed up in the learning process. This property has been used for OOD detection in computer vision [25; 33; 23; 71]. This thus indicates great potential for EBM-based new physics detection at the LHC.

## 3 Methods

### Energy Based Models

Energy-based models are constructed to model the unnormalized data probabilities. They leverage the property that any exponential term \((-E())\) is non-negative and thus can represent unnormalized probabilities naturally. The data distribution is modeled through the Boltzmann distribution: \(p_{}()=(-E_{}())/Z()\) with the energy model \(E_{}()\)1 : \(\) mapping \(\) to a scalar. And the partition function \(Z=()d=(-E_{}())d \) integrates over all the possible states.

EBMs can be learned through maximum likelihood \(_{p_{D}}( p_{}())\). However, the training of EBMs can pose challenges due to the intractable partition function in \( p_{}()=-E_{}()- Z()\). Though the partition function is intractable, the gradients of the log-likelihood do not involve the partition function directly. Thus when taking gradients w.r.t. the model parameters \(\), the partition function is canceled out. The gradient of the maximum likelihood loss function can be written as:

\[_{}() =-_{p_{D}()}[_{} p_{}( )] \] \[=_{^{+} p_{D}()}[_{} E_{}(^{+})]-_{^{-} p_{}()}[ _{}E_{}(^{-})]\,, \]

where \(p_{D}()\) is the data distribution and \(p_{}()\) is the model distribution. The training objective is thus composed of two terms, corresponding to two different learning phases (i.e., the _positive phase_ to fit the data \(^{+}\), and the _negative phase_ to fit the model distribution \(^{-}\)). When parameterizing the energy function with feed-forward neural networks , the positive phase is straightforward. However, the negative phase requires sampling over the model distribution. This leads to various Monte Carlo sampling strategies for estimating the maximum likelihood.

Contrasting the energies of the data and the model samples as proposed _Contrastive Divergence_ (CD) [37; 11] leads to an effective strategy to train EBMs with the following CD objective:

\[D_{}(p_{D}()\|p_{}())-D_{}(Tp_ {D}()\|p_{}())\,, \]

where \(T\) denotes the one-step Monte Carlo Markov Chain (MCMC) kernel imposed on the data distribution. In more recent approaches for high-dimensional modeling [56; 25], we can directly initialize from random noises to generate MCMC samples instead of initializing from the data distribution as in the original CD approach. This strategy also helps with exploring and mixing between modes.

Negative SamplingThe most critical component in the negative phase is the sampling to estimate the model distribution. We employ gradient-based MCMC generation for the negative sampling, which is handled by Langevin Dynamics [52; 66]. As written in Eq. 4, Langevin dynamics uses gradients w.r.t. the input dimensions, associated with a diffusion term to inject stochasticity, to generate a sequence of negative samples \(\{^{-}_{k}\}_{k=1}^{K}\).

\[^{-}_{k+1}=^{-}_{k}-}{2}_{}E_{}(^{-}_{k})+,\,\, (0,1) \]

MC ConvergenceThe training anatomy [55; 56] for short-run non-convergent MCMC and long-run convergent MCMC shows that short-run (\(\) 5-100 steps) MCMC initialized from random distributions

Figure 1: Schematic of the EBM model. The energy function \(E(,y)\) is estimated with a transformer. The training procedure is governed by _Contrastive Divergence_ (the vertical dimension), for which the model distribution estimation \(q_{}()\) is obtained with Langevin Dynamics (the horizontal dimension), evolving samples from random noises \(^{-}_{0}\).

is able to generate realistic samples, while long-run MCMC might be oversaturated with lower-energy states.

To improve mode coverage, we use random noise to initialize MCMC chains. To accelerate training, we employ a relatively small number of MCMC steps. In practice, we can reuse the generated samples as initial samples of the following MCMC chains to accelerate mixing, similar to _Persistent Contrastive Divergence_. Following the procedure in , we use a randomly initialized buffer that is consistently updated from previous MCMC runs as the initial samples. (As empirically shown , a Metropolis-Hastings step is not necessary. So we ignore this rejection update in our experiments.)

Energy FunctionSince there is no explicit generator in EBMs, we have much freedom in designing the architectures of the energy function. This also connects with the fast-paced development of supervised neural classifier architectures for particle physics. We can directly reuse the architectures from supervised classifiers in the generative modeling of EBMs. We use a self-attention-based transformer to parameterize the energy function \(E_{}()\). We defer the detailed description to Sec. 3.3.

The full algorithm for training EBMs is described in Algorithm 1.

```
Input: training samples \(\{_{i}^{+}\}_{i=1}^{N}\) from \(p_{}()\), parameterized energy function \(E_{}()\), initial buffer \(\), Langevin dynamics step size \(_{}\), number of MCMC steps K, model parameter learning rate \(_{}\), regularization strength \(\) for Gradient descent step 1 = 0...L-1 do \(_{i}^{+} p_{}()\) \(_{i,0}^{-} 0.95*+0.05*\)\(\) Reinitialize the samples in the buffer with random noise in the probability of 0.05 for Langevin dynamics step k = 0...K-1 do \(_{i,k+1}^{-}=_{i,k}^{-}-_{}_{ }E_{}(_{i,k}^{-})+0.005_{k}\), \(_{k}(0,1)\) Langevin Dynamics taking gradients w.r.t. input dimensions endfor \(_{i}^{-}_{i,K}^{-}\) \(_{}=_{i}(E_{}(_{i}^{+})-E _{}(_{i}^{-}))\) \(_{}=_{i}(E_{}(_{i}^{+})^{ 2}+E_{}(_{i}^{-}))^{2}\)\(\)\(L_{2}\) Regularization \(-_{}_{}(_{} +_{})\)\(\) Update model parameters with gradient descent \(_{i,K}^{-}\)\(\) Update the buffer with generated samples endfor
```

**Algorithm 1** EBM training with MCMC by Langevin Dynamics

### Hybrid Modeling

Neural Classifier as an EBMA classical classifier can be re-interpreted in the framework of EBMs , with the logits \(()\) corresponding to negative energies of the joint distribution \(p(,y)=()_{y})}{Z}\), where \(()_{y}\) denotes the logit corresponding to the label y. Thus the probability marginalized over y can be written as \(p()=(()_{y})}{Z}\), with the energy of \(\) as \(-_{y}(()_{y})\). We are then brought back to the classical softmax probability \(()_{y})}{_{y}(()_{ y})}\) when calculating \(p(y|)\).

This viewpoint provides a novel method for jointly training a supervised classifier and an unsupervised generative model. Specifically,  successfully incorporated EBM-based generative modeling into a classifier in image generation and classification. We follow their proposal to train the hybrid model as follows to ensure the classifier \(p(y|)\) is unbiased. The joint log-likelihood is decomposed into two terms:

\[ p(,y)= p()+ p(y|)\:. \]

Thus one can maximize \( p()\) with the contrastive divergence of the EBM with the energy function \(E()=-_{y}(()_{y})\), and maximize \( p(y|)\) with the usual cross-entropy of the classification.

### Energy-based Models for Elementary Particles

We would like to construct an energy-based model for describing jets and their inner structures. In conceiving the energy function for these elementary particles, we consider the following constraints and characteristics: 1) permutation invariance - the energy function should be invariant to jet constituent permutations, and 2) higher-order interactions - we would like the energy function to be powerful enough to simulate the complex inter-particle interactions.

Thus, we leverage the self-attention-based transformer  to approximate the energy function, which takes into account the _higher-order_ interactions between the component particles. As indicated in Eq. 6b, the encoding vector of each constituent \(W\) is connected with all other constituents through the self-attention weights \(A\) in Eq. 6a, which are already products of particle representations \(Q\) and \(K\).

\[A=(Q K^{T}/}}) \] \[W=A V \]

Moreover, we can easily incorporate particle permutation invariance  in the transformer, by summing up the encodings of each jet constituent. The transformer architecture is shown in Fig. 1. The coordinates \(( p_{T},,)\) of each jet constituent are first embedded into a \(d_{}\)-dimensional space through a linear layer, then fed into \(N_{L}\) self-attention blocks sequentially. After that, a sum-pooling layer is used to sum up the features of the jet constituents. Finally, a multi-layer-perceptron projector maps the features into the energy score. Model parameters are recorded in Table 4 of Appendix A.

## 4 Experiments

Training DetailsThe training set consists of 300,000 QCD jets. We have 10,000 samples in the buffer and reinitialize the random samples with a probability of 0.05 in each iteration. We use a relatively small number of steps (e.g., 24) for the MCMC chains. The step size \(_{}\) is set to 0.1 according to standard deviations of the input features. The diffusion magnitude within the Langevin dynamics is set to 0.005. The number of steps used in validation steps is set to 128 for better mixing.

We use Adam  for optimization, with the momenta \(_{1}=0.0\) and \(_{2}=0.999\). The initial learning rate is set to 1e-4, with a decay rate of 0.98 for each epoch. We use a batch size of 128, and train the model for 50 epochs. More details can be found in Appendix A.

Model ValidationIn monitoring the likelihood, the partition function can be estimated with Annealed Importance Sampling (AIS) . However, these estimates can be erroneous and consume a lot of computing resources. Fortunately for physics events, we have well-designed high-level features as a handle for monitoring the generation quality. Especially, we employ the boost-invariant jet transverse momentum \(p_{T}=_{i=1}^{N}p_{Ti}\) and the Lorentz-invariant jet mass \(M=^{N}E_{i})^{2}-(_{i=1}^{N}_{i})^{2}}\) as the validation observables. And we calculate the Jensen-Shannon Divergence (JSD), between these high-level observable distributions of the data and the model generation, as the metric. In contrast to the short-run MCMC in the training steps, we instead use longer MCMC chains for generating the validation samples.

When we focus on downstream tasks such as OOD detection, it's reasonable to employ the Area Under the Receiver Operating Characteristic (ROC) Curve (AUC) as the validation metric, with Standard Model top jets serving as the benchmark signal.

GenerationTest-time generation is achieved in MCMC transition steps from the proposal random (Gaussian) distribution. We use a colder model and a smaller step size at test time which is annealed from 0.1 and decreases by a factor of 0.8 in every 40 steps to encourage stable generation. The diffusion noise magnitude is set to 0.001. Longer MCMC chains with more steps (e.g., 200) are taken to achieve realistic generation. 2 A consistent observation across various considered methods is that the step size stands out as the crucial parameter predominantly influencing the quality of generation.

Anomaly DetectionIn contrast to prevalent practices in computer vision, EBM-based OOD detection in LHC physics exhibits specificity. The straightforward approach of comparing two distinct datasets, such as CIFAR10 and SVHN, overlooks the intricate real-world application environments. Adapting OOD detection to scientific discovery at the LHC, we reformulate and tailor the decision process as follows: if we train on multiple known Standard Model jet classes, we focus on class-conditional model evaluation for discriminating between the unseen test signals and the most copious background events (i.e., QCD jets rather than all the Standard Model jets).

### Generative Modeling - Energy-based Event Generator

We present the generated jets transformed from initial random noises with the Langevin dynamics MCMC. Due to the non-human-readable nature of physics events (e.g., low-level raw records at the particle detectors), we are not able to examine the generation quality through formats such as images directly. However, it has a long history that expert-designed high-level observables can serve as strong discriminating features. In the first row of Fig. 2, we first show the distributions of input features for the data and the model generation. Meanwhile, in the second row, we plot the distributions of high-level expert observables including the jet transverse momentum \(p_{}\) and the jet mass M. Through modeling low-level features in the detector space, we achieve precise recovery of the high-level physics observables in the theoretical framework. For better visualization, we can also map the jets onto the \((,)\) plane, with pixel intensities associated with the corresponding energy deposits. We show the average generated jet images in Fig. 7 of Appendix B, comparing to the real jet images (_right-most_) in the \((,)\) plane. In Table 2, we present the Jensen-Shannon Divergence of the high-level observables \(p_{T}\) and \(M\) distributions between real data and model generation, as the quantitative measure of the generation performance.

   Model & JSD (\(p_{T}\)) / \(10^{-4}\) & JSD (\(M\))/ \(10^{-4}\) & JSD (\(M/p_{T}\))/ \(10^{-4}\) \\  \(\)-VAE  & 3.7 & 11.0 & – \\ EBM & \(2.4 1.9\) & \(3.2 2.0\) & \(6.3 4.6\) \\   

Table 2: Model comparison in terms of generation quality measured in Jensen-Shannon Divergence of high-level observables \(p_{T}\) and \(M\). For EBMs, we present the means and standard deviations for the JSDs obtained from 5 independent runs. We also present a \(\)-VAE  model as a baseline.

Figure 2: **Top:** Input feature distributions of jet constituents for the data and the model generation. **Bottom:** High-level feature distributions for the data and the model generation.

### Anomaly Detection - Anomalous Jet Tagging

Since EBMs naturally provide an energy score for each jet, for which the in-distribution samples should have lower scores while OOD samples are expected to incur higher energies. Furthermore, a classifier, when interpreted as an energy-based model, the transformed energy score can also serve as an OOD identifier .

EbmIn HEP, the _in-situ_ energy score can be used to identify potential new physics signals. With an energy-based model, which is trained on the QCD background events or directly on the slightly signal-contaminated data, we expect unseen signals (i.e., non-QCD jets) to have higher energies and correspondingly lower likelihoods.

In Fig. 3, we compare the energy distributions of in-distribution QCD samples, out-of-distribution signal examples (hypothesized heavy Higgs boson  which decays into four QCD sub-jets), and random samples drawn from the proposal Gaussian distribution. We observe that random samples unusually have the highest energies. Signal jets have relatively higher energies compared with the QCD background jets, making model-independent new physics searches possible.

Spurious CorrelationA more intriguing property of EBMs is that spurious correlations can be better handled. Spurious correlations in jet tagging might result in distorted background distributions and obscure effective signal detection. For instance, VAEs in OOD detection can be highly correlated with the jet masses , similar to the spurious correlation with image pixel numbers in computer vision . In the right panel of Fig. 3, we plot the correlation between the energy score and the jet mass. Unlike other generative strategies for model-independent anomaly detection, EBMs are largely free from the spurious correlation between the energy \(E()\) and the jet mass M. _The underlying reason for EBMs not presenting mass correlation could be the negative sampling involved in the training process. Larger mass modes are already covered during the negative sampling process._ This makes EBMs a promising candidate for model-independent new physics searches.

Ebm-ClefThe task of classifying/identifying different Standard Model jet types and the task of searching for beyond the Standard Model signals actually can be unified in a single approach with neural classifiers distinguishing different Standard Model particle types . Compared with naive generative model based OOD, training neural classifiers on different known Standard Model jets helps the model with learning more meaningful and robust representations for effectively detecting new signals. Further combining generative models and discriminative classifiers can be achieved in the EBM framework elegantly. We employ the hybrid learning scheme  combining the discriminate and the generative approaches. It links with many OOD detection techniques and observations. For instance, the maximum logit for OOD detection  comes in the format of the lowest energy \(_{y}E(,y)\) in this framework.

Figure 3: **Left: Energy distributions for random samples, background QCD jets, and novel signals. Right: Correlation between the jet mass \(M_{J}\) and the energy \(E\).**We train an EBM-based multi-class classifier (EBM-CLF) according to Eq. 5, for both discriminating different Standard Model jets (QCD jets, boosted W jets, and boosted top jets) and generating real-like jet samples. The results of generative sampling are shown in Appendix B. The jointly trained EBM and jet classifier maintain the classification accuracy (see Appendix B). The associated EBM is augmented by the discriminative task, and thus assisted with better inductive biases and domain knowledge contained in the in-distribution classes. _EBM-CLF is an example of how we unify different physics tasks (jet classification, anomaly detection, and generative modeling) in a unified framework._

We measure the OOD detection performance in the ROC curve and the AUC of the binary classification between the background QCD samples and the signal jets. Table 3 records the AUCs of different models (and different anomaly scoring functions) in tagging Standard Model Top jets and hypothesized Higgs bosons (OOD \(H\)). The jointly trained model generally has better anomaly tagging performance compared with the naive EBM. We also explore the norm of the _score function_ of \(p_{}()\): \(\|_{} p_{}()\|=\|_{}E( )\|\) serving as the anomaly score (similar to the _approximate mass_ in ). Constructed from the energy function, the _score function_ approximately inherits the nice property of mass decorrelation. However, they have slightly worse performance compared with \(E()\). The corresponding ROC curves are shown in the left panel of Fig. 4, in terms of the signal efficiency \(_{S}\) (i.e., _true positive rate_) and the background rejection rate \(1/_{B}\) (i.e., 1_/false positive rate_). In the right panel, we plot the background mass distributions under different cuts on the energy scores. We observe excellent jet mass decorrelation/invariance for energy score-based anomalous jet tagging. Additionally for EBM-CLF, we also record the AUCs for anomaly scores of the class-conditional softmax probability \(p(y|)\) and the logit \(()_{y}=-E(,y)\) corresponding to the background QCD class, in Table 3. However, without further decorrelation strategies (auxiliary tasks or assistant training strategies), these two anomaly scores are usually strongly correlated with the masses of the in-distribution classes and distort the background distributions. Thus we list the results here only for reference.

## 5 Conclusion

We present a novel energy-based generative framework for modeling the behavior of elementary particles. By mimicking the inter-particle interactions with a self-attention-based transformer, we map the correlations in the detector space to a probabilistic space with an energy function. The energy model is used for the implicit generation of physics events. Despite the difficulty in training EBMs, we adapted the training strategies to balance learning efficiency and training stability. We adopt short-run MCMC sampling at training, while at test time we instead use dynamic generation to obtain prominent generative quality. Additionally, the framework supports the construction of an augmented classifier with integrated generative modeling. This unified approach provides us with flexible tools for high-performance parameterized physics event simulation and spurious-correlation-free model-independent signal detection. It marks a significant stride in next-generation multitasking machine learning models for high-energy physics.

   Model & AUC (Top) & AUC (OOD \(H\)) \\  DisCo-VAE (\(=1000\))  & 0.593 & 0.481 \\ KL-OE-VAE  & 0.744 & 0.625 \\   \\ EBM (\(E()\)) & \(0.679 0.009\) & \(0.794 0.032\) \\ EBM (\(\|_{} p_{}()\|\)) & \(0.628 0.017\) & \(0.738 0.033\) \\ EBM-CLF (\(E()\)) & \(0.703 0.020\) & \(0.815 0.018\) \\ EBM-CLF (\(\|_{} p_{}()\|\)) & \(0.679 0.040\) & \(0.722 0.028\) \\   \\ EBM-CLF (\(()_{y}\)) & \(0.920 0.002\) & \(0.877 0.008\) \\ EBM-CLF (\(p(y|)\)) & \(0.940 0.002\) & \(0.865 0.012\) \\   

Table 3: Anomaly detection performance for different models (anomaly scores in parentheses) measured in AUCs. We present the averaged AUCs over 5 independent models with different random seeds and the associated standard deviations. We list a few baseline models with mass decorrelation achieved either by a regularization term (DisCo-VAE (\(=1000\))) or an auxiliary classification task contrasting in-distribution data and outlier data (KL-OE-VAE).