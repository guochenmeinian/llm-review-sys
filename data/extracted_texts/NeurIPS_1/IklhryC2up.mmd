# Computational Complexity of Detecting Proximity to Losslessly Compressible Neural Network Parameters

Computational Complexity of Detecting Proximity to Losslessly Compressible Neural Network Parameters

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

To better understand complexity in neural networks, we theoretically investigate the idealised phenomenon of lossless network compressibility, whereby an identical function can be implemented with a smaller network. We give an efficient formal algorithm for optimal lossless compression in the setting of single-hidden-layer hyperbolic tangent networks. To measure lossless compressibility, we define the _rank_ of a parameter as the minimum number of hidden units required to implement the same function. Losslessly compressible parameters are atypical, but their existence has implications for nearby parameters. We define the _proximate rank_ of a parameter as the rank of the most compressible parameter within a small \(L^{}\) neighbourhood. Unfortunately, detecting nearby losslessly compressible parameters is not so easy: we show that bounding the proximate rank is an \(\)-complete problem, using a reduction from Boolean satisfiability via a novel abstract clustering problem involving covering points with small squares. These results underscore the computational complexity of measuring neural network complexity, laying a foundation for future theoretical and empirical work in this direction.

## 1 Introduction

Learned neural networks often generalise well, deplete the excessive expressive capacity of their architectures (Zhang et al., 2017, 2021). Moreover, learned neural networks are often _approximately compressible_, in that smaller networks can be found implementing similar functions (via, e.g., model distillation, Bucilua et al., 2006; Hinton et al., 2014; see, e.g., Sanh et al., 2019 for a large-scale example). In other words, learned neural networks are often simpler than they might seem. Advancing our understanding of neural network complexity is key to understanding deep learning.

We propose studying the idealised phenomenon of _lossless compressibility_, whereby an _identical_ function can be implemented with a smaller network.1 Classical functional equivalence results imply that, in many architectures, almost all parameters are incompressible in this lossless, unit-based sense (e.g., Sussmann, 1992; Chen et al., 1993; Fefferman, 1994; Phuong and Lampert, 2020). However, these results specifically exclude measure zero sets of parameters with more complex functional equivalence classes (Anonymous, 2023), some of which are losslessly compressible.

We argue that, despite their atypicality, losslessly compressible parameters may be highly relevant to deep learning. The learning process exerts a non-random selection pressure on parameters, and losslessly compressible parameters are appealing solutions due to parsimony. Moreover, losslessly compressible parameters are a source of information singularities (cf. Fukumizu, 1996), highly relevant to statistical theories of deep learning (Watanabe, 2009; Wei et al., 2022).

Even if losslessly compressible parameters themselves are rare, their aggregate parametric neighbourhoods have nonzero measure. These neighbourhoods have a rich structure that reaches throughout the parameter space (Anonymous, 2023). The parameters in these neighbourhoods implement similar functions to their losslessly compressible neighbours, so they are necessarily approximately compressible. Their proximity to information singularities also has implications for local learning dynamics (Amari et al., 2006; Wei et al., 2008; Cousseau et al., 2008; Amari et al., 2018).

In this paper, we study losslessly compressible parameters and their neighbours in the setting of single-hidden-layer hyperbolic tangent networks. While this architecture is not immediately relevant to modern deep learning, parts of the theory are generic to feed-forward architecture components. A comprehensive investigation of this simple and concrete case is a first step towards studying more modern architectures. To this end, we offer the following theoretical contributions.

1. In Section 4, we give efficient formal algorithms for optimal lossless compression of single-hidden-layer hyperbolic tangent networks, and for computing the _rank_ of a parameters--the minimum number of hidden units required to implement the same function.
2. In Section 5, we define the _proximate rank_--the rank of the most compressible parameter within a small \(L^{}\) neighbourhood. We give a greedy algorithm for bounding this value.
3. In Section 6, we show that bounding the proximate rank below a given value (that is, detecting proximity to parameters with a given maximum rank), is an \(\)-complete decision problem. The proof involves a reduction from Boolean satisfiability via a novel abstract decision problem involving clustering points in the plane into small squares.

These results underscore the computational complexity of measuring neural network complexity: we show that while lossless network compression is easy, detecting highly-compressible networks near a given parameter can be very hard indeed (embedding any computational problem in \(\)). Our contributions lay a foundation for future theoretical and empirical work detecting proximity to losslessly compressible parameters in learned networks using modern architectures. In Section 7, we discuss these research directions, and limitations of the lossless compressibility framework.

## 2 Related work2

Two neural network parameters are _functionally equivalent_ if they implement the same function. In single-hidden-layer hyperbolic tangent networks, Sussmann (1992) showed that, for almost all parameters, two parameters are functionally equivalent if and only if they are related by simple operations of exchanging and negating the weights of hidden units. Similar operations have been found for various architectures, including different nonlinearities (e.g., Albertini et al., 1993; Karkova and Kainen, 1994), multiple hidden layers (e.g., Fefferman and Markel, 1993; Fefferman, 1994; Phuong and Lampert, 2020), and more complex connection graphs (Vlacic and Bolcskei, 2021, 2022).

Lossless compressibility requires functionally equivalent parameters in smaller architectures. In all architectures where functional equivalence has been studied (cf. above), the simple operations identified do not change the number of units. However, all of these studies explicitly exclude from consideration certain measure zero subsets of parameters with richer functional equivalence classes. The clearest example of this crucial assumption comes from Sussmann (1992), whose result holds exactly for "minimal networks" (in our parlance, losslessly incompressible networks).

Anonymous (2023) relaxes this assumption, studying functional equivalence for non-minimal single-hidden-layer hyperbolic tangent networks. Anonymous (2023) gives an algorithm for finding canonical equivalent parameters using various opportunities for eliminating or merging redundant units.3 This algorithm implements optimal lossless compression as a side-effect. We give a more direct and efficient lossless compression algorithm using similar techniques.

Beyond _lossless_ compression, there is a significant empirical literature on approximate compressibility and compression techniques in neural networks, including via network pruning, weight quantisation, and student-teacher learning (or model distillation). Approximate compressibility has also been proposed as a learning objective (see, e.g., Hinton and van Camp, 1993; Aytekin et al., 2019) and used as a basis for generalisation bounds (Suzuki et al., 2020, 2020; Suzuki et al., 2020). For an overview, see Cheng et al. (2018, 2020) or Choudhary et al. (2020). Of particular interest is a recent empirical study of network pruning from Casper et al. (2021), who, while investigating the structure of learned neural networks, found many instances of units with weak or correlated outputs. Casper et al. (2021) found that these units could be removed without a large effect on performance, using elimination and merging operations bearing a striking resemblance to those discussed by Anonymous (2023).

## 3 Preliminaries

We consider a family of fully-connected, feed-forward neural network architectures with one input unit, one biased output unit, and one hidden layer of \(h\) biased hidden units with the hyperbolic tangent nonlinearity \((z)=(e^{z}-e^{-z})/(e^{z}+e^{-z})\). The weights and biases of the network are encoded in a parameter vector in the format \(w=(a_{1},b_{1},c_{1},,a_{h},b_{h},c_{h},d)_{h}=^ {3h+1}\), where for each hidden unit \(i=1,,h\) there is an _outgoing weight_\(a_{i}\), an _incoming weight_\(b_{i}\), and a _bias_\(c_{i}\); and \(d\) is the _output unit bias_. Thus each parameter \(w_{h}\) indexes a mathematical function \(f_{w}:\) such that \(f_{w}(x)=d+_{i=1}^{h}a_{i}(b_{i}x+c_{i})\). All of our results generalise to networks with multi-dimensional inputs and outputs.

Two parameters \(w_{h},w^{}_{h^{}}\) are _functionally equivalent_ if \(f_{w}=f_{w^{}}\) as functions on \(\) (\( x,f_{w}(x)=f_{w^{}}(x)\)). A parameter \(w_{h}\) is _(losslessly) compressible_ (or _non-minimal_) if and only if \(w\) is functionally equivalent to some \(w^{}_{h^{}}\) with fewer hidden units \(h^{}<h\) (otherwise, \(w\) is _incompressible_ or _minimal_). Sussmann (1992) showed that a simple condition, _reducibility_, is necessary and sufficient for lossless compressibility. A parameter \((a_{1},b_{1},c_{1},,a_{h},b_{h},c_{h},d)_{h}\) is _reducible_ if and only if it satisfies any of the following _reducibility conditions_:

1. \(a_{i}=0\) for some \(i\), or
2. \(b_{i}=0\) for some \(i\), or
3. \((b_{i},c_{i})=(b_{j},c_{j})\) for some \(i j\), or
4. \((b_{i},c_{i})=(-b_{j},-c_{j})\) for some \(i j\).

Each reducibility condition suggests a simple operation to remove a hidden unit while preserving the function (Sussmann, 1992; Anonymous, 2023): (i) units with zero outgoing weight do not contribute to the function; (ii) units with zero incoming weight contribute a constant that can be incorporated into the output bias; and (iii), (iv) unit pairs with identical (negative) incoming weight and bias contribute in proportion (since the hyperbolic tangent is odd), and can be merged into a single unit with the sum (difference) of their outgoing weights.

Define the _uniform norm_ (or \(L^{}\)_norm_) of a vector \(v^{p}\) as \(\|v\|_{}=_{i=1}^{p}(v_{i})\), the largest absolute component of \(v\). Define the _uniform distance_ between \(v\) and \(u^{p}\) as \(\|u-v\|_{}\). Given a positive scalar \(^{+}\), define the _closed uniform neighbourhood of \(v\) with radius \(\), \(_{}(v;)\)_, as the set of vectors of distance at most \(\) from \(v\): \(_{}(v;)=\{\,u^{p}\,:\,\|u -v\|_{}\,\}\).

A _decision problem4_ is a tuple \((I,J)\) where \(I\) is a set of _instances_ and \(J I\) is a subset of _affirmative instances_. A _solution_ is a deterministic algorithm that determines if any given instance \(i I\) is affirmative (\(i J\)). A _reduction_ from one decision problem \(X=(I,J)\) to another \(Y=(I^{},J^{})\) is a deterministic polytime algorithm implementing a mapping \(:I I^{}\) such that \((i) J^{} i J\). If such a reduction exists, say \(X\)_is reducible5 to \(Y\)_ and write \(X Y\). Reducibility is transitive.

\(\) is the class of decision problems with polytime solutions (polynomial in the instance size). \(\) is the class of decision problems for which a deterministic polytime algorithm can verify affirmative instances given a certificate. A decision problem \(Y\) is \(\)_-hard_ if all problems in \(\) are reducible to \(Y\) (\( X,X Y\)). \(Y\) is \(\)_-complete_ if \(Y\) and \(Y\) is \(\)-hard. Boolean satisfiability is a well-known \(\)-complete decision problem (Cook, 1971; Levin, 1973; see also Garey and Johnson, 1979). \(\)-complete decision problems have no known polytime exact solutions.

Lossless compression and rank

We consider the problem of lossless neural network compression: finding, given a compressible parameter, a functionally equivalent but incompressible parameter. The following algorithm solves this problem by eliminating units meeting reducibility conditions (i) and (ii), and merging unit pairs meeting reducibility conditions (iii) and (iv) in ways preserving functional equivalence.

**Algorithm 4.1** (Lossless neural network compression).: Given \(h\), proceed:

```
1:procedureCompress(\(w=(a_{1},b_{1},c_{1},,a_{h},b_{h},c_{h},d)_{h}\))
2:\(\)Stage 1: Eliminate units with incoming weight zero (incorporate into new output bias \(\)) \(\)
3:\(I\{i\{1,,h\}\,:\,b_{i} 0\,\}\)
4:\( d+_{i I}(c_{i}) a_{i}\)
5:\(\)Stage 2: Partition and merge remaining units by incoming weight and bias \(\)
6:\(_{1},,_{r}\) - partition \(I\) by the value of \((b_{i})(b_{i},c_{i})\)
7:for\(j 1,,J\)do
8:\(_{j}_{i_{j}}(b_{i}) a_{i}\)
9:\(_{j},_{j}(b_{_{j}})(b_{ _{j}},c_{_{j}})\)
10:endfor
11:\(\)Stage 3: Eliminate merged units with outgoing weight zero \(\)
12:\(k_{1},,k_{r}\{j\{1,,J\}\,:\,_{j} 0\,\}\)
13:\(\)Construct a new parameter with the remaining merged units \(\)
14:return\((_{k_{1}},_{k_{1}},_{k_{1}},,_{k_{r}},_{k_{r}}, _{k_{r}},)_{r}\)
15:endprocedure
```

**Theorem 4.1** (Algorithm 4.1 correctness).: _Given \(w_{h}\), compute \(w^{}=(w)_{r}\). (i) \(f_{w^{}}=f_{w}\), and (ii) \(w^{}\) is incompressible._

Proof sketch (Full proof in Appendix A).: For (i), note that units eliminated in Stage 1 contribute a constant \(a_{i}(c_{i})\), units merged in Stage 2 have proportional contributions (\(\) is odd), and merged units eliminated in Stage 3 do not contribute. For (ii), by construction, \(w^{}\) satisfies no reducibility conditions, so \(w^{}\) is not reducible and thus incompressible by Sussmann (1992). \(\)

We define the _rank6_ of a neural network parameter \(w_{h}\), denoted \((w)\), as the minimum number of hidden units required to implement \(f_{w}\): \((w)=\{\,h^{}\,:\, w^{} _{h^{}};\,f_{w}=f_{w^{}}\,\}\). The rank is also the number of hidden units in \((w)\), since Algorithm 4.1 produces an incompressible parameter, which is minimal by definition. Computing the rank is therefore a trivial matter of counting the units, after performing lossless compression. The following is a streamlined algorithm, following Algorithm 4.1 but removing steps that don't influence the final count.

**Algorithm 4.2** (Rank of a neural network parameter).: Given \(h\), proceed:

```
1:procedureRank(\(w=(a_{1},b_{1},c_{1},,a_{h},b_{h},c_{h},d)_{h}\))
2:\(\)Stage 1: Identify units with incoming weight nonzero \(\)
3:\(I\{i\{1,,h\}\,:\,b_{i} 0\,\}\)
4:\(\)Stage 2: Partition and compute outgoing weights for merged units \(\)
5:\(_{1},,_{J}\) partition \(I\) by the value of \((b_{i})(b_{i},c_{i})\)
6:\(_{j}_{i_{j}}(b_{i}) a_{i}\) for\(j 1,,J\)
7:\(\)Stage 3: Count merged units with outgoing weight nonzero \(\)
8:return\(|\{j\{1,,J\}\,:\,_{j} 0\,\}|\)\(\)\(|S|\) denotes set cardinality
9:endprocedure
```

**Theorem 4.2** (Algorithm 4.2 correctness).: _Given \(w_{h}\), \((w)=(w)\)._

Proof.: Let \(r\) be the number of hidden units in \((w)\). Then \(r=(w)\) by Theorem 4.1. Moreover, comparing Algorithms 4.1 and 4.2, observe \((w)=r\). 

**Remark 4.3**.: Both Algorithms 4.1 and 4.2 require \((h h)\) time if the partitioning step is performed by first sorting the units by lexicographically non-decreasing \((b_{i})(b_{i},c_{i})\).

## 5 Proximity to low-rank parameters

Given a neural network parameter \(w_{h}\) and a positive radius \(^{+}\), we define the _proximate rank_ of \(w\) at radius \(\), denoted \(}_{}(w)\), as the rank of the lowest-rank parameter within a closed uniform (\(L^{}\)) neighbourhood of \(w\) with radius \(\). That is,

\[}_{}(w)=}(u)\,:\,u_{}(w;)\,}.\]

The proximate rank measures the proximity of \(w\) to the set of parameters with a given rank bound, that is, sufficiently losslessly compressible parameters.

The following greedy algorithm computes an upper bound on the proximate rank. The algorithm replaces each of the three stages of Algorithm 4.2 with a relaxed version, as follows.

1. Instead of eliminating units with zero incoming weight, eliminate units with _near_ zero incoming weight (there is a nearby parameter where these are zero).
2. Instead of partitioning the remaining units by \(}(b_{i})(b_{i},c_{i})\), _cluster_ them by _nearby_\(}(b_{i})(b_{i},c_{i})\) (there is a nearby parameter where they have the same \(}(b_{i})(b_{i},c_{i})\)).
3. Instead of eliminating merged units with zero outgoing weight, eliminate merged units with _near_ zero outgoing weight (there is a nearby parameter where these are zero).

Step (2) is non-trivial, we use a greedy approach, described separately as Algorithm 5.2.

**Algorithm 5.1** (Greedy bound for proximate rank).: Given \(h\), proceed:

```
1:procedureBound(\(^{+}\), \(w=(a_{1},b_{1},c_{1},,a_{h},b_{h},c_{h},d)_{h}\))
2:\(\)Stage 1: Identify units with incoming weight not near zero
3:\(I\{i\{1,,h\}\,:\,}(b_{i})> \}\)
4:\(\)Stage 2: Compute outgoing weights for nearly-mergeable units
5:\(_{1},,_{J}(, }(b_{i})(b_{i},c_{i})i I)\)\(\)Algorithm 5.2
6:\(_{j}_{i_{j}}}(b_{i}) a_{i} j 1,,J\)
7:\(\)Stage 3: Count nearly-mergeable units with outgoing weight not near zero
8:return\(|\{j\{1,,J\}\,:\,}(_{j})> |_{j}|\,\}|\)\(\)\(|S|\) denotes set cardinality
9:endprocedure
```

**Algorithm 5.2** (Greedy approximate partition).: Given \(h\), proceed:

**Theorem 5.1** (Algorithm 5.1 correctness).: _For \(w_{h}\) and \(^{+}\), \(}_{}(w)(,w)\)._

Proof sketch.: (Full proof in Appendix A). Trace the algorithm to construct a parameter \(u_{}(w;)\) with \(}(u)=(,w)\). During Stage 1, set the nearly-eliminable incoming weights to zero. Use the group-starting vectors \(v_{1},,v_{J}\) from Algorithm 5.2 to construct mergeable incoming weights and biases during Stage 2. During Stage 3, subtract or add a fraction of the merged unit outgoing weight from the outgoing weights of the original units. 

**Remark 5.2**.: Both Algorithms 5.1 and 5.2 have worst-case runtime complexity \((h^{2})\).

**Remark 5.3**.: Algorithm 5.1 does _not_ compute the proximate rank--merely an upper bound. There may exist a more efficient approximate partition than the one found by Algorithm 5.2. It turns out that this suboptimality is fundamental--computing a smallest approximate partition is \(\)-hard, and can be reduced to computing the proximate rank. We formally prove this observation below.

Computational complexity of proximate rank

Remark 5.3 alludes to an essential difficulty in computing the proximate rank: grouping units with similar (up to sign) incoming weight and bias pairs for merging. The following abstract decision problem, Problem UPC, captures the related task of clustering points in the plane into groups with a fixed maximum uniform radius.7

Given \(h\)_source points_\(x_{1},,x_{h}^{2}\), define an _\((r,)\)-cover_, a collection of _\(r\) covering points_\(y_{1},,y_{r}^{2}\) such that the uniform distance between each source point and its nearest covering point is at most \(\) (that is, \( i\{1,,h\}\, j\{1,,r\}\,\|x_{i}-y_{j} \|_{}\)).

**Problem UPC.** Uniform point cover, or UPC, is a decision problem. The instances are tuples of the form \((h,r,,X)\) where \(h,r\); \(^{+}\); and \(X\) is a list of \(h\) source points in \(^{2}\). The affirmative instances are all tuples \((h,r,,X)\) for which there exists an \((r,)\)-cover of the \(h\) points in \(X\).

**Theorem 6.1**.: _Problem UPC is \(\)-complete._

_Proof sketch_ (Full proof in Appendix C). The main task is to show that UPC is \(\)-hard (\( X\), \(X\)). Since reducibility is transitive, it suffices to give a reduction from the well-known \(\)-complete problem Boolean satisfiability (Cook, 1971; Levin, 1973). Actually, to simplify the proof, we consider an \(\)-complete variant of Boolean satisfiability, restricted to formulas with (i) two or three literals per clause, (ii) one negative occurrence and one or two positive occurrences per literal, and (iii) a planar bipartite clause-variable incidence graph.

From such a formula we must construct a UPC instance, affirmative if and only if the formula is satisfiable. Due to the restrictions, the bipartite clause-variable is planar with maximum degree 3, and can be embedded onto an integer grid (Valiant, 1981, SSIV). We divide the embedded graph into unit-width tiles of finitely many types, and we replace each tile with an arrangement of source points based on its type. The aggregate collection of source points mirrors the structure of the original formula. The variable tile arrangements can be covered essentially in either of two ways, corresponding to "true" and "false" in a satisfying assignment. The edge tile arrangements transfer these assignments to the clause tiles, where the cover can only be completed if all clauses have at least one true positive literal or false negative literal. Figure 1 shows one example of this construction.

Figure 1: Example of reduction from restricted Boolean satisfiability to Problem UPC. **(a)** A satisfiable restricted Boolean formula. **(b)** The formula’s planar bipartite variable–clause invidence graph (circles: variables, squares: clauses, edges: \(\) literals). **(c)** The graph embedded onto an integer grid. **(d)** The embedding divided into unit tiles of various types. **(e)** The \(h=68\) source points aggregated from each of the tiles. **(f)** Existence of a \((34,1/8)\)-cover of the source points (coloured points are covering points, with uniform neighbourhoods of radius \(1/8\) shown). General case in Appendix C.

The following decision problem formalises the task of bounding the proximate rank, or equivalently, detecting nearby low-rank parameters. It is \(\)-complete by reduction from Problem \(\).

**Problem PR.** Bounding proximate rank, or \(\), is a decision problem. Each instance comprises a number of hidden units \(h\), a parameter \(w_{h}\), a uniform radius \(^{+}\), and a maximum rank \(r\). The affirmative instances are those instances where \(}_{}(w) r\).

**Theorem 6.2**.: _Problem PR is \(\)-complete._

Proof.: Since \(\) is \(\)-complete (Theorem 6.1), it suffices to show \(\) and \(\).

(\(\), the reduction): Given an instance of Problem \(\), allocate one hidden unit per source point, and construct a parameter using the source point coordinates as incoming weights and biases. Actually, to avoid issues with zeros and signs, first translate the source points well into the positive quadrant. Likewise, set the outgoing weights to a positive value. Figure 2 gives an example.

Formally, let \(h,r\), \(^{+}\), and \(x_{1},,x_{h}^{2}\). In linear time construct a \(\) instance with \(h\) hidden units, uniform radius \(\), maximum rank \(r\), and parameter \(w_{h}\) as follows.

1. Define \(x_{}=(_{i=1}^{h}x_{i,1},_{i=1}^{h}x_{i,2})^{2}\), containing the minimum first and second coordinates among all source points (minimising over each dimension independently).
2. Define a translation \(T:^{2}^{2}\) such that \(T(x)=x-x_{}+(2,2)\).
3. Translate the source points \(x_{1},,x_{h}\) to \(x^{}_{1},,x^{}_{h}\) where \(x^{}_{i}=T(x_{i})\). Note (for later) that all components of the translated source points are at least \(2\) by step (1).
4. Construct the neural network parameter \(w=(2,x^{}_{1,1},x^{}_{1,2},,2,x^{ }_{h,1},x^{}_{h,2},0)_{h}\). In other words, for \(i=1,,h\), set \(a_{i}=2\), \(b_{i}=x^{}_{i,1}\), and \(c_{i}=x^{}_{i,2}\); and set \(d=0\).

(\(\), equivalence): It remains to show that the constructed instance of \(\) is affirmative if and only if the given instance of \(\) is affirmative, that is, there exists an \((r,)\)-cover of the source points if and only if the constructed parameter has \(}_{}(w) r\).

(\(\)): If there is a small cover of the source points, then the hidden units can be perturbed so that they match up with the (translated) covering points. Since there are few covering points, many units can now be merged, so the original parameter has low proximate rank.

Formally, suppose there exists an \((r,)\)-cover \(y_{1},,y_{r}\). Define \(:\{1,,h\}\{1,,r\}\) such that the nearest covering point to each source point \(x_{i}\) is \(y_{(i)}\) (breaking ties arbitrarily). Then for \(j=1,,r\), define \(y^{}_{j}=T(y_{j})\) where \(T\) is the translation defined in step (2) of the construction. Finally, define a parameter \(w^{}=(2,y^{}_{(1),1},y^{}_{(1),2},,2,y^{}_{(h),1},y^{}_{(h),2},0) _{h}\) (in other words, for \(i=1,,h\), \(a^{}_{i}=2\), \(b^{}_{i}=y^{}_{(i),1}\), and \(c^{}_{i}=y^{}_{(i),2}\); and \(d^{}=0\)).

Then \(}(w^{}) r\), since there are at most \(r\) distinct incoming weight and bias pairs (namely \(y^{}_{1},,y^{}_{r}\)). Moreover, \( w-w^{}_{}\), since both parameters have the same output bias and outgoing weights, and, by the defining property of the cover, for \(i=1,,h\),

\[(b_{i},c_{i})-(b^{}_{i},c^{}_{i})_{}=  x^{}_{i}-y^{}_{(i)}_{}=  T(x_{i})-T(y_{(i)})_{}= x_{i}-y_{ (i)}_{}.\]

Therefore \(}_{}(w)}(w^{}) r\).

Figure 2: Illustrative example of the parameter construction. **(a)** A set of source points \(x_{1},,x_{9}\). **(b)** Transformation \(T\) translates all points into the positive quadrant by a margin of \(2\). **(c,d)** The coordinates of the transformed points become the incoming weights and biases of the parameter.

(\(\)): Conversely, since all of the weights and biases are at least \(2\), any nearby low-rank parameter implies the approximate mergeability of some units. Therefore, if the parameter has low proximate rank, there is a small cover of the translated points, and, in turn, of the original points.

Formally, suppose \(}_{}(w) r\), with \(w^{}_{}(w;)\) such that \(}(w^{})=r^{} r\). In general, the only ways that \(w^{}\) could have reduced rank compared to \(w\) are the following (cf. Algorithm 4.1):

1. Some incoming weight \(b_{i}\) could be perturbed to zero, allowing its unit to be eliminated.
2. Two units \(i,j\) with \((b_{i},c_{i})\) and \((b_{j},c_{j})\) within \(2\) could be perturbed to have identical incoming weight and bias, allowing them to be merged.
3. Two units \(i,j\) with \((b_{i},c_{i})\) and \(-(b_{j},c_{j})\) within \(2\) could be perturbed to have identically negative weight and bias, again allowing them to be merged.
4. Some group of \(m 1\) units, merged through the above options, with total outgoing weight within \(m\) of zero, could have their outgoing weights perturbed to make the total zero.

By construction, all \(a_{i},b_{i},c_{i} 2>0\), immediately ruling out (1) and (3). Option (4) is also ruled out because any such total outgoing weight is \(2m>m\). This leaves option (2) alone responsible. Thus, there are exactly \(r^{}\) distinct incoming weight and bias pairs among the units of \(w^{}\). Denote these pairs \(y^{}_{1},,y^{}_{r^{}}\)--they constitute an \((r^{},)\)-cover of the incoming weight and bias vectors of \(w\), \(x^{}_{1},,x^{}_{h}\) (as \(w^{}_{}(w;)\)). Finally, invert \(T\) to produce an \((r^{},)\)-cover of \(x_{1},,x_{h}\), and add \(r-r^{}\) arbitrary covering points to extend this to the desired \((r,)\)-cover.

(\(\)): We must show that an affirmative instance of \(\) can be verified in polynomial time, given a certificate. Consider an instance \(h,r\), \(^{+}\), and \(w=(a_{1},b_{1},c_{1},,a_{h},b_{h},c_{h},d)_{h}\). Use as a certificate a partition8\(_{1},,_{J}\) of \(\{i\{1,,h\}:(b_{i})>\}\), such that (1) for each \(_{j}\), for each \(i,k_{j}\), \(\|(b_{i})(b_{i},c_{i})-(b_{k} )(b_{k},c_{k})\|_{} 2\); and (2) at most \(r\) of the \(_{j}\) satisfy \(_{i_{j}}(b_{i}) a_{i}>| _{j}|\). The validity of such a certificate can be verified in polynomial time by checking each of these conditions directly.

It remains to show that such a certificate exists if and only if the instance is affirmative. If \(}_{}(w) r\), then there exists a parameter \(w^{}_{}(w;)\) with \(}(w^{}) r\). The partition computed from Stage 2 of \((w^{})\) satisfies the required properties for \(w_{}(w^{};)\).

Conversely, given such a partition, for each \(_{j}\), define \(v_{j}^{2}\) as the centroid of the bounding rectangle of the set of points \(\{\,(b_{i})(b_{i},c_{i})\,:\,i_{j}\,\}\), that is,

\[v_{j}=(_{i_{j}}(b_{i})+_{i _{j}}(b_{i}),\,_{i_{j}}(b_ {i}) c_{i}+_{i_{j}}(b_{i}) c_{i}).\]

All of the points within these bounding rectangles are at most uniform distance \(\) from their centroids. To construct a nearby low-rank parameter, follow the proof of Theorem 5.1 using \(_{1},,_{J}\) and \(v_{1},,v_{J}\) in place of their namesakes from Algorithms 5.1 and 5.2. Thus \(}_{}(w) r\). 

## 7 Discussion

In this paper, we have studied losslessly compressible neural network parameters, measuring the size of a network by the number of hidden units. Losslessly compressible parameters comprise a measure zero subset of the parameter space, but this is a rich subset that stretches throughout the entire parameter space (Anonymous, 2023). Moreover, the neighbourhood of this region has nonzero measure and comprises approximately compressible parameters.

It's possible that part of the empirical success of deep learning can be explained by the proximity of learned neural networks to losslessly compressible parameters. Our theoretical and algorithmic contributions, namely the notions of rank and proximate rank and their associated algorithms, serve as a foundation for future research in this direction. In this section, we outline promising next steps for future work and discuss limitations of our approach.

Limitations of the lossless compressibility framework.Section 4 offers efficient algorithms for optimal lossless compression and computing the rank of neural network parameters. However, the rank is an idealised notion, serving as a basis for the theory of proximate rank. One would not expect to find compressible parameters in practice, since numerical imprecision is likely to prevent the observation of identically equal, negative, or zero weights in practice. Moreover, the number of units is not the only measure of a network's description length. For example, the sparsity and precision of weights may be relevant axes of parsimony in neural network modelling.

Returning to the deep learning context--there is a gap between lossless compressibility and phenomena of approximate compressibility. In practical applications and empirical investigations, the neural networks in question are only approximately preserved the function, and moreover the degree of approximation may deteriorate for unlikely inputs. Considering the neighbourhoods of losslessly compressible parameters helps bridge this gap, but there are approximately compressible neural networks beyond the proximity of losslessly compressible parameters, which are not accounted for in this approach. More broadly, a comprehensive account of neural network compressibility must consider architectural redundancy as well as redundancy in the parameter.

Tractable detection of proximity to low-rank parameters.An important direction for future work is to empirically investigate the proximity of low-rank neural networks to the neural networks that arise during the course of successful deep learning. Unfortunately, our main result (Theorem 6.2) suggests that detecting such proximity is computationally intractable in general, due to the complex structure of the neighbourhoods of low-rank parameters.

There is still hope for empirically investigating the proximate rank of learned networks. Firstly, \(\)-completeness does not preclude efficient approximation algorithms, and approximations are still useful as a one-sided test of proximity to low-rank parameters. Algorithm 5.1 provides a naive approximation, with room for improvement in future work. Secondly, Theorem 6.2 is a worst-case analysis--Section 6 essentially constructs pathological parameters poised between nearby low-rank regions such that choosing the optimal direction of perturbation involves solving (a hard instance of) Boolean satisfiability. Such instances might be rare in practice (cf. the related problem of \(k\)-means clustering; Daniely et al., 2012). As an extreme example, detecting proximity to merely compressible parameters (\(r=h-1\)) permits a polytime solution based on the reducibility conditions.

Towards lossless compressibility theory in modern architectures.We have studied lossless compressibility in the simple, concrete setting of single-hidden-layer hyperbolic tangent networks. Several elements of our approach will be useful for future work on more modern architectures. At the core of our analysis are structural redundancies arising from zero, constant, or proportional units (cf. reducibility conditions (i)-(iii)). In particular, the computational difficulty of bounding the proximate rank is due to the approximate merging embedding a hard clustering problem. These features are not due to the specifics of the hyperbolic tangent, rather they are generic features of any layer in a feed-forward network component.

In more complex architectures there will be additional or similar opportunities for compression. While unit negation symmetries are characteristic of odd nonlinearities, other nonlinearities will exhibit their own affine symmetries which can be handled analogously. Further redundancies will arise from interactions between layers or from specialised computational structures.

## 8 Conclusion

Towards a better understanding of complexity and compressibility in learned neural networks, we have developed a theoretical and algorithmic framework for _lossless_ compressibility in single-hidden-layer hyperbolic tangent networks. The _rank_ is a measure of a parameter's lossless compressibility. Section 4 offers efficient algorithms for performing optimal lossless compression and computing the rank. The _proximate rank_ is a measure of proximity to low-rank parameters. Section 5 offers an efficient algorithm for approximately bounding the proximate rank. In Section 6, we show that optimally bounding the proximate rank, or, equivalently, detecting proximity to low-rank parameters, is \(\)-complete, by reduction from Boolean satisfiability via a novel hard clustering problem. These results underscore the complexity of losslessly compressible regions of the parameter space and lay a foundation for future theoretical and empirical work on detecting losslessly compressibile parameters arising while learning with more complex architectures.