# Sirius: Contextual Sparsity with Correction

for Efficient LLMs

 Yang Zhou\({}^{1}\), Zhuoming Chen\({}^{1}\), Zhaozhuo Xu\({}^{2}\), Xi Victoria Lin\({}^{3}\), Beidi Chen\({}^{1,3}\)

\({}^{1}\)Carnegie Mellon Univeristy

\({}^{2}\)Stevens Institute of Technology

\({}^{3}\)FAIR at Meta

{yangzho6, zhuominc, beidic}@andrew.cmu.edu

zxu79@stevens.eduvictorialin@meta.com

###### Abstract

With the blossom of large language models (LLM), inference efficiency becomes increasingly important. Various approximate methods are proposed to reduce the cost at inference time. Contextual Sparsity (CS) is appealing for its training-free nature and its ability to reach a higher compression ratio seemingly without significant performance degradation. However, after a comprehensive evaluation of contextual sparsity methods on various complex generation tasks, we find that although CS succeeds in prompt-understanding tasks, it significantly degrades the model performance for reasoning, deduction, and knowledge-based tasks. Despite the gap in end-to-end accuracy, we observed that sparse models and original models often share the general problem-solving logic and require only a few token corrections to recover the original model performance. This paper introduces Sirius1, an efficient correction mechanism, which significantly boosts CS models on reasoning tasks while maintaining its efficiency gain. Sirius is evaluated on 6 models with 8 difficult generation tasks in reasoning, deduction, and coding and shows consistent effectiveness and efficiency. Also, we carefully develop a system implementation for Sirius and show that Sirius delivers theoretical latency reduction with roughly 20% reduction in latency for 8B model on-chip and 35% reduction in latency for 70B model offloading. We open-source our implementation of Sirius at [https://github.com/Infini-AI-Lab/Sirius.git](https://github.com/Infini-AI-Lab/Sirius.git).

## 1 Introduction

Large Language Models (LLM), such as OpenAI et al. (2024) (GPT-4), Team et al. (2024) (Gemini), and Touvron et al. (2023) (Llama) have demonstrated their proficiency in a wide range of natural language processing applications such as content creation, summarization, and impressive and complex reasoning tasks. However, their deployment is very challenging, especially in latency-sensitive settings (Kaplan et al., 2020). Exploiting the model sparsity is a natural way to reduce the model parameter size and computational cost with a long history (LeCun et al., 1989; Tibshirani, 1996). More recently, many studies have shown that _contextual sparsity_(Liu et al., 2023; Li et al., 2022; Dong et al., 2024; Lee et al., 2024), which highly correlates to the prompt or the context, can greatly speed up LLM inference without quality degradation.

However, in this paper, we first demonstrate a critical and fundamental problem with _contextual sparsity_ (CS): while generally robust in classification tasks and generation tasks that mainly rely on prompt understanding (e.g., summarization, chat question-answering), we found that CS models struggle at high-level reasoning and understanding tasks.

For example, in Figure 1 (a), we contrast between the Text Summarization task (CNN/DailyMail) and Arithmetic Reasoning (GSM8K) with contextual sparsity methods on Llama-3-8B-Instruct. Varying sparsity levels, Llama-3-8B-Instruct with contextual sparsity performs consistently worse on GSM8K than CNN/DailyMail. With roughly 50% sparsity globally, the sparse model degradation is still reasonable on text summarization (right axis in color coral) compared to almost collapsed on arithmetic reasoning (left axis in color green). **However, for reasoning tasks, can we simply live with a higher density ratio to preserve more performance?** Unfortunately, the answer is NO. Besides the significant efficiency loss, shown in Figure 1 (b), the Llama-3-70B-Instruct model with contextual sparsity also crashes at around 50% sparsity globally. The 50% sparse model still has 4\(\) the parameter size compared to the smaller dense model (Llama-3-8B-Instruct), while still performing worse on GSM8K-CoT, rendering contextual sparsity utterly not useful for complex reasoning tasks.

We conduct an in-depth study on the CS model failure cases and notice that the overall reasoning pathway of these sparse models is usually sound and adheres to the full model. The fatal mistakes are always caused by some middle tokens and propagate towards the end, examples can be seen in Figure 4. Following this observation, we conduct a simple experiment with 65% Contextual Sparse Llama-3-8B-Instruct on GSM8K as presented in Figure 1 (c). We run both the sparse and the full models together for the same prompt and compare two generation output token-by-token.

Surprisingly, the trend increases steeply with the percentage of corrected tokens. Correction of only 6% tokens in the sparse model's generation recovers most of GSM8K accuracy (\(<\)5% to full), and 11% to recover the full performance. The results show potential for an efficient and powerful correction mechanism to maintain the sparse efficiency while boosting its performance. Contextual sparsity uses a dynamic sparsity pattern and naturally requires the full model to be in GPU memory during runtime, allowing the full model to be used efficiently for infrequent correction. Even though only very few need to be corrected, locating these mistaken tokens efficiently turns out to be challenging.

Ideally, we want a correction system to have the following properties: 1) **Effective**, the sparse model quality degradation can be improved to the full model vicinity; 2) **Cheap**, the full model only gives minimal intervention; 3) **Adaptive**, the system is efficient across various reasoning datasets.

In this paper, we carefully analyze and formulate correction efficiency in Section 2. We extensively categorize the strengths and weaknesses of CS in Section 3. In Section 4, We systematically design Sirius, a correction method covering all three desired properties. (**When?**) In Section 4.1, we show that the sparse model can be both confident or uncertain when making mistakes, rendering the signal from sparse unreliable for determining when to correct. Sirius is a period-based method with the period as a hyperparameter. (**How?**) In Sections 4.1 and B.2, we introduce novel KV Cache direct rewriting, minimal rollbacks, and hardware-efficient tree building to help increase the effective period of full model correction, thus, ensuring the correction efficiency.

Figure 1: Contextual sparse models struggle at challenging text generation tests that require high-level reasoning and understanding, e.g. GSM8K. On these tasks, contextually sparse models lead to significant quality degradation. In (a), we contrast CS Llama-3-8B-Instruct on GSM8K (green) and CNN DailyMail (coral). (b) Contextual Sparsity Llama-3-70B-Instruct crashes at 50% global sparsity, making the smaller dense model Llama-3-8B-Instruct (green star) a significantly more efficient choice than the sparse 70B model. (c) Sparse model crashing at reasoning tasks has patterns, and ideally only correcting 11% unlikely tokens recovers the sparse model performance fully.

In Section 5, we empirically evaluated Sirius on 6 different models with 8 different reasoning tasks and showed that Sirius is generally effective and efficient. On GSM8K and Llama-3-8B-Instruct specifically, we boost the fine-grained sparsity from 58% to 72% with 4% increase in effective parameter size and coarse-grained sparsity from 38% to 70% with the cost of 5% effective parameter size. We also show that Sirius delivers the promised efficiency on mainstream GPUs in both on-chip and offloading settings.

## 2 Related Works and Problem Formulation

In this section, we first present the classification of the prior Contextual Sparsity methods and narrate important efficiency metrics in. **Also, we present careful analysis and quantitative comparisons on why Speculative Decoding is inefficient in recovering contextual sparsity.** Due to space constraints, we refer to Appendix A.2. For extended related works on model compression, contextual sparsity, and speculative decoding, we present in Appendix A.1.

**Contextual Sparsity Classification** - Contextual sparsity (CS) methods are usually training-free, easy to use, and seemingly effective, making them highly attractive to ML practitioners looking to reduce LLM inference costs. CS exists naturally in MLP layers of the LLM, which occupies roughly 70% of the LLM total weights (Dong et al., 2024; Lee et al., 2024)). The contextual sparsity selection is as follows: given the context, only a limited number of the most relevant neurons are selected based on the input activation. The rest contributed to the output far less is discarded. We refer to two main directions of contextual sparsity methods as **Coarse-grained Sparsity** (CSparse) Methods (Dong et al. (2024)) - that within the same input prompt, the sparsity pattern is fixed for all tokens generated. **Fine-grained Sparsity** (FSparse) Methods (Lee et al. (2024)) - that exploits the per-token sparsity to save resources.

**Average Parameters Used Per Token** - A key metric is used to evaluate the efficiency of our proposed method, the Average Parameter Used per token decoded (later referred to as APU). LLM inference is memory I/O bound (Leviathan et al., 2023; Kim et al., 2023). The latency of generating every single token is dominated by the memory loading time from the GPU HBM to SRAM. On the other hand, Sirius relies on full model parallel verifying a chunk of tokens. Although from the FLOPs standpoint, the amount of compute performed per evaluation step is the number of input token times of a single token input process, the latency of parallel verification is still roughly the same as taking a single token (Verified further in 10, length 64 is only 1.1 ms longer than length 1), because the inference is memory bound.

Sirius operates in the memory-bound regime (single inference sequence length smaller than or equal to 64). Thus, the average parameter count of a model gives us a rough judgment of the latency of inference. Formally, for a full LLM to have \(C_{full}\) number of parameters, and its sparse counterpart of a certain predetermined sparsity \(C_{sparse}\). The average advancement length (later we refer to as AAL) in the number of tokens between two consecutive LLM corrections can be represented as \(n_{AAL}\)

Figure 2: Overview of Sirius. Contextual Sparsity requires full model weights to be placed on the GPU memory. While the sparse model doesn’t perform well on complex reasoning tasks, Sirius uses the Full Model to correct the Sparse model. The full model is called fairly infrequently. During the correction, the Full Model will rewrite the KV Cache, interleave with high-quality tokens to the sparse outputs, and then roll back only when the token is deemed extremely unlikely by the Full Model.

The average parameters used per token (APU) are the following

\[=C_{sparse}+C_{full}}{n_{AAL}} \]

We want the metric to be as small as possible, and obviously, we want \(n_{AAL}\) to be as large as possible.

Another thing to note is that we always compare the system's APU against the full model's APU, which is \(C_{full}\). If we divided the above equation by \(C_{full}\), we can have an equivalent parameter density of the system defined based on \(I_{globalsparsity}\), which is \(C_{sparse}/C_{full}\).

\[=I_{globalssparsity}+1}{n_{AAL}} \]

Later, if we use period \(n_{period}\), the equation can be rewritten as

\[=-1)I_{globalssparsity}+1}{n_{AAL}} \]

Later when presenting Sirius, we mainly specify \(n_{period}\) with \(n_{AAL}\) to evaluate its efficiency. Notice that \(I_{globalsparsity}\) is determined by the sparsity method, Sirius cannot change it anymore.

## 3 Observations

In this section, we present a detailed study of the strengths and weaknesses of Contextual Sparsity (CS). 3.1 presents the strengths of CS. 3.2 presents the weaknesses of CS. Additionally, we show that given the similar parameter size, the more well-trained the model is, the more CS degradation will be for the model. Due to limited space, we present the details in Appendix B.1. 3.3 shows our findings when looking into the failure cases of the CS model in complex reasoning generation tasks.

In the following series of experiments, we build our implementation2 of fine-grained sparsity based on Lee et al. (2024) and coarse-grained sparsity based on Dong et al. (2024). The default sparsity for both methods is 50% for the MLP component of the model (whole MLP for coarse-grained sparsity and Up and Down linear layers only for fine-grained sparsity). We mainly use this default setting in most experiment tables in the paper without explicitly mentioning it. Otherwise, we will explicitly specify the different sparsity levels we used.

### Contextual Sparsity: Where Does It Succeed?

For tasks on prompt understanding, CS generally performs well and gives consistent and strong output. We evaluate CS models on machine summarization (CNNDailyMail et al. (2017)), and Conversational Question Answering (CoQA et al. (2019)). The results show that the correctly selected contextual sparsity in the MLP layers and the full attention layers can fully extract and understand the local prompt information. More details are presented in Figure 3, where we show that by varying the sparsity level, the language model's performance on CNN/DailyMail is robust even when the activation sparsity drops to below 20%, which translates to around 44% global density.

For tasks accessing factuality and hallucination, we select the generation portion of the TruthfulQA dataset (Lin et al., 2022). Results are shown in Table 1, where we evaluate the techniques on 5 different LLMs. Interestingly, we find that the Fine-grained sparsity is often better than the dense model baseline across different models. This finding is consistent with previous works Laser (Sharma et al., 2023) and Dola (Chuang et al., 2024). They both observed that compressing the original LLM in a carefully designed way would lead to improvement in factuality and better de-hallucination. Laser comes from the low-rank approximation of the MLP layers, while Dola proposes a factuality-aware layer-skipping algorithm. Based on their findings, hallucination occurs when parts of the weights aren't as well-versed in the given input as the other parts. They expose the "averaging" effect that blurs the factuality of the output. Removing these neurons gives rise to better factuality and less hallucination. Our studies look at the same problem from a neuron sparsity standpoint.

### Contextual Sparsity: Where Does It Fail?

On the other hand, contextual sparsity severely struggles when the generation tasks rely solely on the model's own reasoning and world knowledge understanding ability. Here we show the Llama-3-8B-Instruct and the Llama-2-7B-Chat models in Table 1, refer to Table 12 for evaluations on more models. Notice that since fine-grained sparsity method needs the activation from Gate MLP for selecting sparsity, while coarse-grained sparsity has a predetermined pattern after prefilling and can sparsify the Gate MLP. Even though both are at 50% activation sparsity, the coarse-grained sparsity method effectively achieves higher parameter savings than fine-grained sparsity in practice. Here we evaluate the sparse techniques using 5-shot CoT on the GSM8K dataset (Cobbe et al., 2021). We found that across all the models we evaluated, both sparsity methods lead to significant accuracy degradation. We include HumanEval (Chen et al., 2021), a coding task that requires complex reasoning and planning ability. We found that both sparsity methods exhibit similar performance degradation when it comes to coding. Shown in Figure 3, two tasks see sparsity significantly drop performance after 50% activation sparsity.

For knowledge recall and world knowledge understanding, we specifically test on MMLU-Flan-CoT (Chung et al., 2022) the CoT text generation version of the MMLU dataset (Hendrycks et al., 2021). Table 1 shows the results. Stronger models like Llama-3-8B-Instruct suffer from significant

 
**Where CS Succeeds** & **CNN/DailyMail** & **CoQA** & **TruthfulQA** \\ Experiment Settings & Unitxt Rouge & EM/F1 & Rouge-1/2 ACC \\ 
**Llama-3-8B-Instruct** & 0.1237 & 0.6153/0.7825 & 0.4945/0.3647 \\
**Llama-3-8B-Instruct-CSparse** & 0.1144 & 0.6633/0.7977 & 0.4725/0.3403 \\
**Llama-3-8B-Instruct-FSparse** & 0.1166 & 0.6625/0.7984 & 0.5043/0.3305 \\ 
**Llama-2-7B-Chat** & 0.1489 & 0.5982/0.7580 & 0.4480/0.3831 \\
**Llama-2-7B-Chat-CSparse** & 0.1448 & 0.6117/0.7639 & 0.4529/0.3843 \\
**Llama-2-7B-Chat-FSparse** & 0.1521 & 0.5898/0.7540 & 0.4565/0.3660 \\ 
**Where CS Fails** & **GSM8K** & **HumanEval** & **MMLU\({}^{*}\)** \\ Experiment Settings & ACC (strict/flexible) & Pass@1 (GD) & Accuracy \\ 
**Llama-3-8B-Instruct** & 0.7551/0.7544 & 0.560 & 0.6231 \\
**Llama-3-8B-Instruct-CSparse** & 0.3859/0.3874 & 0.207 & 0.5558 \\
**Llama-3-8B-Instruct-FSparse** & 0.5868/0.5891 & 0.457 & 0.5304 \\ 
**Llama-2-7B-Chat** & 0.2396/0.2462 & 0.140 & 0.492 \\
**Llama-2-7B-Chat-CSparse** & 0.1334/0.1380 & 0.067 & 0.4637 \\
**Llama-2-7B-Chat-FSparse** & 0.1979/0.2017 & 0.134 & 0.4768 \\  

* **MMLU** is a classification task, not generation tasks. We use **MMLU-FLAN-COT**

Table 1: We show the difference between cases when Contextual Sparsity (CS) succeeds or fails. CS is generally good at prompt understanding tasks and tasks that measure the trustworthiness of the language models while not good at tasks that require reasoning and world knowledge understanding.

Figure 3: We contrast between Contextual Sparsity on prompt understanding task and complex generation tasks that require reasoning. (a) Both CSparse and FSparse are robust on CNN/DailyMail for various sparsity; (b) and (c) Show that both CSparse and FSparse crash on GSM8K and HumanEval at the global sparsity that they are still robust in prompt understanding tasks.

degradation too. Furthermore, we found that given the similar parameter size, the more well-trained the models are, the higher its degradation from the contextual sparsity, more details in Appendix B.1.

### A Closer Look on GSM8K Quality Degradation

To study the inability of the sparse model in deduction, we conduct a case study on the sequence-level coarse-grained sparsity methods Dong et al. (2024) with the Llama-3-8B-Instruct model. We visually inspect extensive cases where the sparse model and dense differ in answers. Generally, the sparse model always produces highly similar answers to the dense model: the similar approach or logic flow when approaching the same problem and even the same number of sentences before the first mistake occurs or in success cases. However, the key differences are usually caused by the following three categories of small token-level mistakes: (1) frequent miscalculation in the intermediate steps, (2) wrong reasoning in intermediate steps, and (3) insensible and random statements. For each of the above-summarized cases, we find failure question-answer pairs provided in Figure 4. These mistakes happen in the middle of arguments and propagate to the wrong end result.

Similar observations can also be found for fine-grained sparse methods with different model types. _Interestingly, we find that even with these mistakes, the sparse model can still fully generate coherent tokens and make further reasoning assuming their prior steps are correct._

We hypothesize that the gap between the full model and these sparse counterparts is at these key tokens. The following simple experiment is conducted to further verify our hypothesis. We run the coarse-grained sparse model and the full model with the same input prompt and for every token the sparse model generates, the full model is used to check the likelihood of these decoded tokens, mainly removing tokens with low likelihood. By varying the likelihood threshold, we can control the frequency of the correction. The experiments are conducted for both Llama-3-8B-Instruct and Llama-2-7B-Chat Touvron et al. (2023) models with coarse-grained sparsity. The results are shown in Figure 1(c). In both cases, we found that a very small amount of correction would drastically improve the sparse model performance, showing a steep gradient when the percentage of corrected tokens is small. With merely 10% of tokens needing to be corrected, the sparse model can completely match the full model's performance. The experiment verifies our hypothesis that by correcting the small portion of key tokens, the sparse model can meet the large model's performance.

## 4 Methods

Though we find a minor portion of tokens needed to be corrected for the contextual sparsity model to fully recover performance, the challenge remains: how to locate these mistaken tokens with the minimal number of parallel verification rounds of the full model? In this section, we show that the sparse model provides signals that cannot be trusted 4.1. Then, we describe in detail the various correction techniques in 4.1. Because of the space limit, we put how to boost the sparse generation with hardware-efficient tree building B.2.

Figure 4: Examples of contextual sparse model making the identified three different types of mistakes. Most mistakes occur because the model makes calculation mistakes or has a wrong reasoning step compared to the full model. We also observe that there are rare cases where the model makes insensible statements in the middle that make the end result wrong.

### Sparse Model's Self-Awareness Cannot Be Trusted

Intuitively, rather than fixing the \(n_{sparse}\) number, letting the system decide when to call the LLM for evaluation would then give more flexible \(n_{sparse}\). Nevertheless, we argue that the sparse model's output probability distribution cannot be used as a metric for accuracy decisions. We empirically experiment with various methods to utilize the information contained in the sparse model's output distribution. However, varying the threshold leads to \(n_{sparse}\) being too short when the threshold is strict or failing to correct when the threshold is lenient. We then discovered that the sparse model has very limited self-awareness of its own mistakes. To make the observation concrete, we present a small example in Figure 6 a piece of text where the sparse model makes a mistake while the full model succeeds. The red bars signify the error location. The token entropy is neither high nor at zero, making it impossible to effectively use a threshold to control the number \(n_{sparse}\).

### How to Correct the Sparse Output Tokens

The full overview of Sirius is presented in Figure 2 and Algorithm 1. The full model is called once every kernel size. The KV cache is shared between the sparse and the full model. The KV cache is mostly populated by the sparse model, which is called for every token. During correction, the full model takes in the last kernel size of tokens and generates its KVs for the past kernel size tokens in parallel, these KVs are directly written to their corresponding positions in the shared KV Cache. Empirically, we found that full model's KV helps the sparse model's output. When LLM is called to evaluate the sparse model's output, it uses its own predicted likelihood to determine whether to accept or reject the sparse model's past output. The decision is based on comparing the likelihood against a preset threshold. Detailed ablation for threshold is in B.3. Besides the above-mentioned techniques, we also found that the "second/third" choices of the sparse models' rejected token position offer \(>\) 80% coverage of the LLM accepted tokens. The observation motivates us to build a hardware-friendly tree on the sparse model generating side that doesn't sacrifice the performance while significantly boosting the \(n_{AAL}\) or efficiency. Due to the space limit, a great amount of details is in Appendix B.2.

## 5 Experiments

In this section, we empirically evaluate Sirius to correct CS models on various generation tasks in complex reasoning. We show that Sirius is consistent in various tasks, effective in helping CS models recover their performance, and efficient in correction with low additional overhead.

* In 5.1, we evaluated Sirius on six models with 8 different datasets. Sirius is consistently effective and efficient. Specifically, on GSM8K, Sirius corrects FSparse Llama-3-8B-Instruct from 58% accuracy to 72% with only 4% increase in parameter density and corrects CSparse model from 38% to 70% with 5% density.
* In 5.2, we presents more details on our system implementation for Sirius. We show that Sirius delivers its theoretical efficient promise, achieving roughly 20% reduction in latency compared to full on-chip on various hardware. Sirius further achieves 35% speedup to full in offloading settings.

Figure 5: In (a), we present an example that illustrates why the signals from the sparse model are unreliable. It is a figure plotting entropy versus generated tokens. At the tokens where the sparse made the mistake (red), the entropy isn’t in large spikes which signifies chaos and low confidence, rather it is even quite low, compared to nearby entropy spikes. In (b) and (c), we view Sirius as a compression method by itself. We compare Sirius with contextual sparse methods and show that given the same parameter used, Sirius performs better than Contextual Sparse Methods on GSM8K.

* We also present ablation with rich details on how each component of Sirius contributes to its performance and how threshold is used to trade off efficiency and performance. Due to space limit, we place it in Appendix B.3.

### Sirius Significantly Recovers CS Degradation with Low Cost

**Models and Datasets** - To comprehensively evaluate Sirius performance, we deploy six mainstream LLMs with sizes ranging from 7B to 13B: Llama-2-7B, 13B, and Llama-3-8B with their instruction finetuned counterparts, all from Llama family. Following Wei et al. (2022) in LLM reasoning, we also tested CS models on two popular types of reasoning generation tasks: arithmetic and commonsense reasoning. On the Arithmetic side, besides GSM8K, we also evaluate CS models on AQua-RAT. On the Common Sense side, we use CSQA Saha et al. (2018), StrategyQAGeva et al. (2021), Date, and Sports, last two from Big Bench Suite bench authors (2023). Most of these tasks are originally classification tasks. Following the instruction in Wei et al. (2022), we manually compose COT prompts to transform these into logic argument generation tasks. Besides, CS models do not perform well in coding, which requires forming logical arguments and planning. We select HumanEval Chen et al. (2021) and MBPP+ Liu et al. (2023) to evaluate Sirius.

For arithmetic reasoning and coding, we use 50% neuron sparsity for both CSparse and FSparse. FSparse relies on the gate layer to be dense, leading to higher global density than CSparse. Since commonsense reasoning tasks are generally less logically challenging comparatively, we lowered the neuron sparsity level to 40%.

**Main Results** - Due to space limits, we only select the best treewidth of Sirius for GSM8K, CSQA, and HumanEval for the main results in Table 2. Extensive studies on the rest 5 datasets with different treewidth are presented in the Appendix C. From Table 2, we can see that Sirius is consistently effective and efficient across all different classes of tasks. Specifically for Llama-3-8B-Instruct, besides GSM8K, Sirius corrects FSparse and CSparse, on CSQA, from 61% and 64% accuracy to 70% with cost only 3% sparsity for FSparse and 7% for CSparse respectively. On HumanEval, Sirius corrects FSparse from 45% to 61% with 4% sparsity overhead even surpassing the full model's performance,

  \\    &  &  &  &  &  & **Effective Density** \\    & & & & & & \\  Llama-3-8B-Instruct & 0.7536 & 0.3844 & 0.65 & 0.7051 (8) & 15.22/16 & 0.706 \\ Llama-3-8B & 0.4966 & 0.2085 & 0.65 & 0.4177 (8) & 15.29/16 & 0.703 \\ Llama-2-7B-Chat & 0.2403 & 0.1334 & 0.69 & 0.2244 (8) & 15.00/16 & 0.757 \\ Llama-2-7B & 0.1357 & 0.0758 & 0.69 & 0.1183 (6) & 15.87/16 & 0.715 \\ Llama-2-13B-Chat & 0.3548 & 0.2714 & 0.68 & 0.3381 (4) & 15.34/16 & 0.730 \\ Llama-2-13B & 0.2282 & 0.1759 & 0.68 & 0.2418 (1) & 15.34/16 & 0.730 \\   &  &  &  &  &  & **Effective Density** \\    & & & & & & \\  Llama-3-8B-Instruct & 0.7536 & 0.5868 & 0.76 & 0.7278 (4) & 15.37/16 & 0.807 \\ Llama-3-8B & 0.4966 & 0.3199 & 0.76 & 0.4579 (2) & 15.03/16 & 0.825 \\ Llama-2-7B-Chat & 0.2403 & 0.1971 & 0.79 & 0.2388 (6) & 15.69/16 & 0.819 \\ Llama-2-7B & 0.1357 & 0.1137 & 0.79 & 0.1410 (4) & 15.91/16 & 0.807 \\ Llama-2-13B-Chat & 0.3548 & 0.3222 & 0.78 & 0.3533 (1) & 15.08/16 & 0.842 \\ Llama-2-13B & 0.2282 & 0.2191 & 0.78 & 0.2372 (4) & 15.92/16 & 0.797 \\   \\    &  &  &  &  &  & **Effective Density** \\    & & & & & & \\  Llama-3-8B-Instruct & 0.7073 & 0.6470 & 0.58 & 0.7076 (8) & 14.76/16 & 0.657 \\ Llama-3-8B & 0.6437 & 0.5585 & 0.58 & 0.6429 (8) & 15.43/16 & 0.628 \\ Llama-2-7B-Chat & 0.6248 & 0.5200 & 0.62 & 0.6175 (8) & 15.07/16 & 0.683 \\ Llama-2-7B & 0.4742 & 0.4414 & 0.62 & 0.4742 (8) & 15.80/16 & 0.652 \\ Llama-2-13B-Chat & 0.6879 & 0.5536 & 0.61 & 0.6691 (4) & 11.43/12 & 0.674 \\ Llama-2-13B & 0.6109 & 0.5601 & 0.61 & 0.6060 (4) & 15.72/16 & 0.645 \\   \\    &  &  &  &  &  & **Effective Density** \\    & & & & & & \\  Llama-3-8B-Instruct & 0.561 & 0.207 & 0.65 & 0.524 (8) & 14.67/16 & 0.733 \\ Llama-3-8B & 0.262 & 0.067 & 0.65 & 0.243 (8) & 15.10/16 & 0.691 \\ Llama-2-7B-Chat & 0.140 & 0.067 & 0.69 & 0.159 (8) & 10.88/12 & 0.789 \\ Llama-2-7B & 0.116 & 0.079 & 0.69 & 0.128 (8) & 14.84/16 & 0.765 \\ Llama-2-13B-Chat & 0.189 & 0.122 & 0.68 & 0.171 (8) & 11.12/12 & 0.762 \\ Llama-2-13B & 0.262 & 0.067 & 0.68 & 0.244 (8) & 15.10/16 & 0.741 \\   &  &  & **Effective Density** \\    & & & & & & \\  Llama-3-8B-Instruct & 0.561 & 0.457 & 0.76 & 0.616 (6) & 15.42/16 & 0.804 \\ Llama-3-8B & 0.262 & 0.189 & 0.76 & 0.298 (6) & 15.54/16 & 0.797 \\ Llama-2-7B-Chat & 0.140 & 0.134 & 0.79 & 0.165 (6) & 15.27/16 & 0.841 \\ Llama-2-7B & 0.116 & 0.116 & 0.79 & 0.165 (6) & 15.86/16 & 0.810 \\ Llama-2-13B-Chat & 0.189 & 0.146 & 0.78 & 0.183 (6) & 15.34/16 & 0.827 \\ Llama-2-13B & 0.246 & 0.233 & 0.78 & 0.259 (4) & 15.85/16 & 0.801 \\  

Table 2: We show Sirius effectiveness and efficiency in the following table. We select GSM8K for Arithmetic Reasoning, CSQA for Commonsense Reasoning, and HumanEval for code generation. Under the "Sirius Perf. " column, A(B) is shown. A denotes the accuracy after Sirius correction in the dataset evaluated, while (B) represents the optimal treewidth selected under the current model dataset settings. Under the column of "AAL", X/Y is shown, where X is the AAL, while Y is the period. **GSM8K**and from 20% to 52% with 8% sparsity as cost. Besides, Llama-3-8B-Instruct, Sirius corrects all 6 models with additional sparsity overhead smaller than 10% across these three datasets, further showing its strong efficiency. Besides results in Table 2, in Appendix C, we show that Sirius consistently shows great effectiveness with high efficiency across the rest of the 5 datasets.

### Wallclock Speedup

Here we show that Sirius delivers its promised efficiency claim for on-chip and offloading settings. Because the fine-grained sparsity Lee et al. (2024) relies on a custom CUDA kernel to achieve the target generation speedup not open-sourced, we focus on coarse-grained sparsity on GSM-8K COT, and the input sequence length with average prefill length 900.

Firstly, we consider the on-chip setting running Llama-3-8B-Instruct on a single GPU. The sparse model (APU 0.65) achieves 36.01% accuracy on GSM8K-COT, while the full model achieves 76.12% accuracy on GSM8K-COT. With kernel size 10, Sirius achieves 0.74 APU with accuracy 71.27% accuracy. We use torch compile to optimize the inference latency and limit the overhead other than running model inference. The average latency generated per token is used to compute latency. Results are shown in Table 3. On average, Sirius delivers the promised latency reduction from APU calculations. The speedup ratio on A40 and L40 closely aligns with the theoretical APU reported. On the other hand, A100 and H100 compute MLP more efficiently than it compute attention, making the latency ratio between computing MLP and attention not perfectly aligned with their ratio in parameter size. Therefore, we see that even the sparse model baseline has slightly higher latency as expected. We increase the kernel size from 10 to 16 for these two devices, where accuracy reaches 0.7089 and the AAL reaches 13.67. For A100 and H100, building a hardware-efficient tree is nearly free of cost and highly effective. For numbers in Table 3, we use the width 4 tree that boosts the AAL to 15.01 out of 16. More details are in Appendix B.2.

Secondly, we consider the offloading setting which is the only way for resource-limited users to run 70B models by loading only the weights in use to GPU memory, while the others are offloaded to the CPU. Results are shown in Table 4. We use a single L40 48GB with a PCIe bus bandwidth of 25 GB/s to run Llama-3-70B-Instruct with batch size 1. Llama-3-70B-Instruct has roughly 80% of parameters to be MLP, which gives the theoretical APU for Griffin to be 0.6. Sparse + Sirius gives 0.649 APU, which is roughly what our system achieved.

## 6 Conclusion

We observe that contextual sparse methods significantly degrade reasoning and deduction tasks. Sirius, an efficient correction mechanism, enables accurate LLM inference with contextual sparsity. With roughly \(11\%\) to \(18\%\) sparsity increase, Sirius improves fine-grained and coarse-grained sparsity significantly in their performance while maintaining their efficiency gain.

Further, Sirius is still relying on rollback to correct the tokens that are deemed unlikely, which is inefficient. On the other hand, making the weak-strong model synergy systems that match the performance of the strong while keeping the efficiency of the weak, without strictly matching the strong models' output distribution remains an interesting and unsolved problem. We leave these topics to future works.

  
**Settings** & **Sparse** & **Sirius** & **Full** \\ 
**Performance** & 0.7407 & 0.8719 & 0.9014 \\ 
**Latency (s)** & 3.57 s & 3.68 s & 5.72 s \\ 
**Ratio to Full** & 0.6241 & 0.6434 & 1.0 \\   

Table 4: Llama-3-70B-Instruct with Offloading.

  
**Settings** & **ACC** & **A40** & **Ratio** & **L40** & **Ratio** & **A100** & **Ratio** & **H100** & **Ratio** \\ 
**CSparse** & 0.3601 & 20.7 ms & 0.66 & 15.6 ms & 0.67 & 9.6 ms & 0.72 & 6.6 & 0.76 \\ 
**Sirius** & 0.7127 & 24.1 ms & 0.78 & 18.2 ms & 0.78 & 11.1 ms & 0.83 & 7.7 ms & 0.88 \\ 
**Full** & 0.7612 & 30.9 ms & 1.0 & 23.2 ms & 1.0 & 13.3 ms & 1.0 & 8.6 ms & 1.0 \\   

Table 3: Performance and Speedup Ratios on GSM8K-COT with Different Hardware Configurations.

Acknowledgement

We would like to thank Feng Liang and Yunong Liu for their helpful feedback during the exploration and writing. We also want to give a special thanks to Hanshi Sun for providing insights and suggestions for efficient implementation and speedup.