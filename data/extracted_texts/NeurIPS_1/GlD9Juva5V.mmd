# SongCreator: Lyrics-based Universal Song Generation

Shun Lei\({}^{1}\), Yixuan Zhou\({}^{1}\), Boshi Tang\({}^{1}\), Max W. Y. Lam\({}^{2}\), Feng Liu\({}^{2}\), Hangyu Liu\({}^{2}\),

Jingcheng Wu\({}^{2}\), Shiyin Kang\({}^{2}\), Zhiyong Wu\({}^{1,3}\), Helen Meng\({}^{3}\)

\({}^{1}\) Shenzhen International Graduate School, Tsinghua University, Shenzhen

\({}^{2}\) Independent Researcher

\({}^{3}\) The Chinese University of Hong Kong, Hong Kong SAR

{leis21, yx-zhou23}@mails.tsinghua.edu.cn, zywu@sz.tsinghua.edu.cn

Corresponding author, \({}^{}\) Equal contribution.

###### Abstract

Music is an integral part of human culture, embodying human intelligence and creativity, of which songs compose an essential part. While various aspects of song generation have been explored by previous works, such as singing voice, vocal composition and instrumental arrangement, etc., generating songs with both vocals and accompaniment given lyrics remains a significant challenge, hindering the application of music generation models in the real world. In this light, we propose SongCreator, a song-generation system designed to tackle this challenge. The model features two novel designs: a meticulously designed dual-sequence language model (DSLM) to capture the information of vocals and accompaniment for song generation, and a series of attention mask strategies for DSLM, which allows our model to understand, generate and edit songs, making it suitable for various song-related generation tasks by utilizing specific attention masks. Extensive experiments demonstrate the effectiveness of SongCreator by achieving state-of-the-art or competitive performances on all eight tasks. Notably, it surpasses previous works by a large margin in lyrics-to-song and lyrics-to-vocals. Additionally, it is able to independently control the acoustic conditions of the vocals and accompaniment in the generated song through different audio prompts, exhibiting its potential applicability. Our samples are available at [https://thuhcsi.github.io/SongCreator/](https://thuhcsi.github.io/SongCreator/).

## 1 Introduction

Music is an integral part of human culture, embodying human intelligence and creativity. Songs combining vocals and accompaniment compose an essential part of it, whose generation has been a hotspot in both academia and industry in recent years. Although with the rapid advancements in generative models, communities have witnessed the applications of Artificial Intelligence Generated Content (AIGC) models in the generation of texts , images  and speeches , it still remains a big question whether we can replicate the successes in song generation, which demands coordination among various complex elements such as instruments, rhythm, melody and vocals. Currently creating high-level songs with both vocals and accompaniment still requires substantial human effort in composition, instrument arrangement, singing, and so on, a process requiring a great deal of time and expertise. Lyrics-to-song generative models could lower the barrier to entry for novices and improve the workflow of experienced artists.

Previous works mostly explored specific aspects of song generation, as listed in Table 1. Although they exhibit abilities in vocal composition, instrumental arrangement and harmonious generation, none of them is able to combine these three for high-quality lyrics-to-song generation. To this end, Jukebox  can be seen as the first and only attempt from published literature so far tosimultaneously generate vocals and accompaniment in a song from lyrics using a single model. However, it exhibits two major limitations. Firstly, this approach treats the combination of vocals and accompaniment as an entity. While the design facilitates the generation of songs, it ignores the mutual influence between vocals and accompaniment, resulting in vocals that sound unnatural and a lack of musicality in both the melody and accompaniment, and inhibiting the independent controllability of the generated vocals and accompaniment. Secondly, it is confined to performing specific tasks of lyrics-to-song generation, which restricts the broader application of song generation models in complex musical scenarios, including the generation of vocals or instrumental music, as well as universal song generation tasks such as song editing and accompaniment-to-vocal generation. Recently, while the industry has seen the emergence of song generation tools like Suno  and Udio , neither has disclosed their methodologies nor has expanded into universal song generation tasks.

In this work, we introduce SongCreator, a system designed to generate high-quality songs with harmoniously coordinated vocals and accompaniment based on lyrics. It is worth mentioning that by learning composition and arrangement abilities during training, SongCreator can also be applied to universal song generation tasks, as shown in Appendix B, including (but not limited to) lyrics-to-vocal, accompaniment-to-song and song editing. Formalized as a combination of a language model (LM) and a latent diffusion model (LDM) , SongCreator features a novel dual-sequence language model (DSLM), which utilizes two decoders to separately model vocals and accompaniment information, and employs a dynamic bidirectional cross-attention module to capture the influences between these two sequences. This approach treats vocals and accompaniment within a song as separate but interrelated sequences, effectively reducing their mutual influence during training. Additionally, inspired by UniLM  and GLM , we design a series of attention mask strategies for DSLM, which enables SongCreator to complete song generation tasks of various forms, such as editing, understanding and generation in a unified manner. Our contributions can be summarized as follows:

* We propose a novel dual-sequence language model for song generation. Compared to previous ones, it not only emphasizes the respective quality of vocals and accompaniment, but also learns their mutual influences to coordinate them into harmonious songs, greatly enhancing the quality of generations.
* We propose a series of attention mask strategies for song generation, which endows our model with the ability to unify song generation tasks of various forms, such as lyrics-to-song, accompaniment-to-song and song editing. It also makes multi-task training feasible for SongCreator, which underlies its versatile generation ability.
* On top of the mechanisms above, we propose a versatile system for song generation. It can be readily applied to lyrics-based vocals/song generation, or even editing. It also supports universal conditioning and generation: given any one of the vocals or accompaniment as a condition, SongCreator is able to generate the other. Moreover, SongCreator is able to generate songs with separate audio prompts for vocals and accompaniment.
* We conduct extensive experiments to demonstrate the abilities of our system in the eight tasks shown in Appendix B. Ablation experiments justify the effectiveness of our designs.

## 2 Related Work

Singing voice synthesisSinging Voice Synthesis (SVS)  aims at synthesize vocals given scores, has made great progress in recent years. Several works attempt to adopt transformer models

  
**Tasks** & **Inputs** & **Outputs** & **Composition** & **Arrangement** & **Harmony** \\  Singing Voice Synthesis  & Scores & Vocals & ✗ & ✗ & ✗ \\ SongCopper  & Lyrics & Vocals & ✓ & ✗ & ✗ \\ Text-to-Music  & Text Description & Music & ✗ & ✓ & ✗ \\ Accompaniment Generation  & Vocals & Music & ✗ & ✓ & ✓ \\ 
**Song Generation** & Lyrics & Song & ✓ & ✓ & ✓ \\   

Table 1: A comparison of song generation with related tasks in the literature. We use **Composition** to denote whether the model can complete vocal composition, **Arrangement** to denote whether the model can arrange the instrumental accompaniment, and **Harmony** to denote whether vocals and accompaniment sound harmonious and pleasant together.

, generative adversarial networks  and conditional variational autoencoder [17; 18] for SVS. Recently, research [19; 20] focuses on enhancing the quality of synthesized vocals through diffusion models, demonstrating state-of-the-art (SOTA) performance. Similarly, SongCreator also employs a diffusion model to improve the quality of synthesized songs. However, compared to traditional SVS methods that require additional scores composed by humans, SongCreator facilitates composing and arranging songs directly from lyrics and generates complete songs with accompaniment.

Music generationMusic generation has been long studied under various setups. Early efforts [34; 35] primarily focus on generating symbolic music, which is confined to fixed instrumental timbres and lacks expressiveness. Several works [12; 22; 25; 36] have achieved text-to-music generation by tokenizing music into discrete sequences that can be further processed by language models (LMs) [2; 3]. Singsong  and Melodist  follow a similar approach for accompaniment generation. Diffusion models [37; 38; 39], as another competitive class of generative models, have also delivered impressive results in music generation. Many emerging methods [40; 41; 24; 42] use latent diffusion model (LDM) to generate high-quality and high-fidelity music. Recently, MeLoDy  and AudioLDM 2  introduce a novel solution by combining the advantages of LMs and LDM, demonstrating SOTA performances with high fidelity and musicality. However, these methods are designed for generating non-vocal music and limited to specific task such as text-to-music or vocal-to-accompaniment. By leveraging the DSLM, SongCreator can effectively model songs that include both vocals and accompaniment, and it can also extend to various song generation tasks.

Speech editing and synthesisSpeech editing requires to alter a segment within a speech to match the target transcript. Early methods [44; 45; 46; 47; 48] utilized the surrounding speech context as a condition, enabling models to generate the masked segment. Subsequently, several works [11; 49; 50; 51] attempted to establish a unified model for both speech editing and text-to-speech (TTS). However, despite their impressive achievements, these efforts are restricted to handing clean signals only, and required duration information for each phoneme. It restricts the applicability of such methods in song or vocal editing. Recently, the advancement of LMs significantly promoted progress in speech generation, particularly in zero-shot TTS [52; 53; 9] and speech editing [54; 55; 56]. Different from these works that only focus on speech, to our knowledge, we are the first to implement song and vocal editing. Additionally, through a serious of attention mask strategies, our proposed SongCreator provides a general solution that enables a single system to handle multiple tasks in song generation.

## 3 Method

### Overview

Let \(\) represent a song audio. A song generation process can be defined as \(f:\), where \(\) is the set of conditioning signals. In this work, we consider a flexibly conditioned song generation system \(f\) with \(\), accepting a variety of optional inputs including lyrics, vocal prompt, accompaniment prompt, pre-determined vocal track and pre-determined accompaniment track. The

Figure 1: The overview of SongCreator. The BEST-RQ tokens is a proxy that bridges the DSLM and the latent diffusion model.

high flexibility of conditions empowers the controllability of our model, so that different elements within the generated songs can be customized as needed.

However, end-to-end generating a high-fidelity song \(\) from \(\) with a neural network \(f\) remains challenging to date. In the same spirit as previous works [23; 25; 36], we introduce a language-alike discrete sequence (a.k.a., _semantic tokens_), denoted as \(=(S_{1},,S_{N})\), to capture significant structural information in song and to embody LMs as the "brain" of our system for writing songs.

As illustrated in Figure 1, to obtain the semantic tokens, we train a BEST-RQ  on an unlabeled dataset containing songs, vocals and music, and conduct vector quantization over its intermediate hidden representations. These tokens encapsulate sufficient semantic and acoustic details that are necessary for reconstructing \(\). With such a purpose, an LDM, consisting of a VAE and a diffusion model, is trained to decode the semantic tokens into high-quality song audio, in a way similar to . Since both BEST-RQ and LDM were trained and re-produced with open-source implementations, their respective details are beyond the focus of this paper, and are described in Appendix A.2-A.3.

To predict the semantic tokens \(\) given \(\), we designed a novel form of LM - dual-sequence language model (DSLM) for multi-condition song generation, as illustrated in Figure 2. Specifically, DSLM includes three decoders, respectively adopting semantic tokens of vocals (i.e., \(_{v}_{v}\)), accompaniment (i.e., \(_{a}_{a}\)), and song (i.e., \(_{s}_{s}\)) as the prediction targets. Mathematically, we define \(:_{s}_{v} _{a}\). By applying an off-the-shelf source separation algorithm to a large corpus of songs with lyrics, a large volume of paired data can be manufactured for the multi-target generation task of interest.

The remainder of this section presents the main contribution of this paper - DSLM, and the attention mask strategies for DSLM.

### Dual-sequence language model

Formally speaking, the proposed dual-sequence language model (DSLM) is tasked with the generation of \((_{s},_{v},_{a})\) given \(\). An overview of the proposed architecture is presented in Figure 2. Concerning the quadratic complexity of Transformer with respect to sequence length, instead of processing the concatenated sequences of multiple target sequences token-by-token as in , in DSLM we utilize different decoders to model the semantic tokens of vocals \(_{v}\) and accompaniment \(_{a}\) and harmoniously combine them to generate the semantic tokens of song \(_{s}\).

The proposed DSLM consists of a lyrics encoder, two decoders (one for vocals and one for accompaniment) inter-connected through a bidirectional cross-attention module, and a final song decoder. The lyrics encoder is built upon a stack of Transformer encoder layers, which, as a architecture widely adopted in speech synthesis [7; 10], extracts critical information related to the pronunciation of the lyrics \(_{}\). On the other hand, the vocal decoder and accompaniment decoder are together composed of multiple DSLM blocks. Each DSLM block is composed of a self-attention (SA) layer, a cross-attention (CA) layer, a bidirectional cross-attention (BCA) layer and a feed-forward layer.

Figure 2: The overview of DSLM with the attention mask strategies. The DSLM can utilize specific attention mask strategy to achieve different song generation tasks. We illustrate multiple attention mask strategies of what each vocal token’s representation attend to in both self-attention and bidirectional cross-attention. Attention mask strategies in the accompaniment decoder are similar.

The cross-attention layer is utilized to attend the information from lyrics encoder,, which has been widely applied in previous works on speech synthesis  and audio generation . For vocal decoder, it models the alignment between the lyrics and vocals. For accompaniment decoder, it extracts semantic information from the lyrics for generating accompaniment Moreover, in a complete song, the vocal and accompaniment parts have a complex interrelationship. The accompaniment must complement the vocal track without overshadowing them, ensuring that both parts work together to highlight the song's expressive and artistic intents. To understand and model this interrelationship, we introduce a bidirectional cross-attention (BCA) layer, which consists of two symmetrical cross-attention mechanisms. For example, in the vocal decoder, the BCA allows the model to attend to the generated parts of accompaniment while generating vocals, making arrangements accordingly. The BCA layer is then defined as follows:

\[_{v}=_{v}_{v}^{Q},_{v}=_{a}_{v}^{K},_{v}=_{a}_{v}^{V} \]

\[_{ij}=0,\\ -, \]

\[_{v}=(_{v}_{v}^{}}{ }}+) \]

where \(_{v},_{a}^{T d_{k}}\) denote the previous layer's outputs from the vocal decoder and accompaniment decoder, respectively. These outputs are linearly projected to a triple of queries, keys and values with learnable weights \(_{v}^{Q},_{v}^{K},_{v}^{V}^{d_{k}  d_{k}}\), respectively, and the mask matrix \(^{T T}\) is used to control whether a pair of tokens can be attended to each other. Here, we use \(T\) to denote the length of tokens in LM, and use \(d_{h}\) and \(d_{k}\) to denote the hidden size and attention layer size.

The vocal decoder and accompaniment decoder treats the generation of semantic tokens as conditional language modeling tasks, performing autoregressive predictions token by token. Leveraging the in-context learning capabilities of the language model, we can control various acoustic conditions of the generated audio with a prompting technique. Given a vocal prompt (represented by semantic tokens), denoted as \(}_{v}\), it tends to control a mixture of speaker, vocal melody, and tempo. Similarly, given an accompaniment prompt (represented by semantic tokens), denoted as \(}_{a}\), it tends to control instruments, musical melody, and rhythm. The semantic tokens of prompt audio are passed as a prefix to the DSLM and the model uses this prefix to sequentially predict the following token sequence. Taking the vocal decoder \(_{}\) as an example. The task of the vocal decoder can be formulated as:

\[p(_{v}|_{},}_{v};_{})=_{t=0}^{T}p(_{v,t}|_{v,<t}, _{a,<t},_{},}_{v};_{}) \]

Then, we concatenate the embeddings \(_{v},_{a}^{T d_{e}}\) from outputs of these two decoders. The combined embeddings \(_{s}^{T 2d_{e}}\) are fed into a song decoder composed of multiple Transformer blocks to non-autoregressively generate the semantic token sequence for the complete song, achieving a natural and seamless integration of vocals and instruments, which can be simply represented as:

\[p(_{s}|_{v},_{a};_{})=_{t=0}^{T}p(_{s,t}|_{v},_{a}; {}_{}) \]

### Attention mask strategies for universal song generation

In both self-attention (SA) layer and bidirectional cross-attention (BCA) layer, we employ the mask matrix \(\) as shown in Equation 2 to control the access of the semantic tokens to be predicted. As shown in Figure 2, we implement multiple mask strategies for SA and BCA using different \(\).

Specifically, we employ two different masking strategies for the SA to control each semantic token's access to the context within the same sequence. One strategy is the causal attention mask, where the representation of each token can only access the leftward context tokens and itself. This approach predicts a token conditioned on its historical (left) context, thereby learning generation and continuation capabilities, but it is difficult to fully capture the dependencies between the context. The other strategy is the non-causal attention mask, where all token can attend to each other within the same sequence. It incorporates contextual information from the entire sequence, and can generate more comprehensive and enriched context representations than the causal approach.

For BCA, we design four masking strategies to control the mutual attention between the semantic token sequences representing vocals and accompaniment. The bidirectional mask (BR) allows representations in both the vocal sequence and accompaniment sequence to attend to representations in the other sequence. However, when predicting the token at time step \(t\), it can only attend to the representation of tokens in the other sequence at time step less than or equal to \(t\). For example, the representation \(_{v,t}\) of semantic token \(_{v,t}\) can only pay attention to \(_{a, t}\), but not to \(_{a,>t}\). It attempts to capture the relationships between vocals and accompaniment, but does not consider the full context of the other sequence, leading to certain limitations when one sequence is pre-determined. As a supplement, the accompaniment-to-vocals (A2V) and vocals-to-accompaniment (V2A) strategies allow one sequence to attend to all tokens in the other sequence. Take the A2V as an example, the tokens in vocal sequence can attend to the full context of the accompaniment sequence, while tokens in the accompaniment sequence are not allowed to attend to the vocal sequence. In this way, the vocal decoder can generate vocals based on the complete accompaniment information. Similarly, the V2A strategy allows the model to predict accompaniment tokens conditioned on the entire vocals sequence. Additionally, the None strategy means neither sequence can attend to the other, supporting the independent generation of instrumental music or vocals.

By employing different mask strategies for SA and BCA, as well as the input format, a single SongCreator can achieve competitive performance on multiple song generation tasks, as shown in Table 2 and Appendix B. We also demonstrate in the ablation studies that the specific attention mask we employed for each task are effective. Furthermore, we support additional tasks shown on our demo page.

### Training Setup

We investigate a multi-task training setup, in which the model is trained on several tasks to enhance its composition, arrangement, and comprehension abilities. We consider the following three tasks:

Song generation from lyricsIn this task, the SA in both the vocals decoder and the accompaniment decoder employs the causal attention mask to simultaneously generate vocal and accompaniment semantic tokens. For BCA, 80% of the time we use the bidirectional attention mask to learn how to generate harmoniously coordinated vocals and accompaniment. In the remaining 20% of the time, we use the None strategy to allow the model to learn to generate accompaniment or vocal track independently. This probability setting was inspired by classifier-free guidance related work  to ensure it does not disrupt the training of the BCA.

Song generation from pre-determined accompaniment or vocalsTake the accompaniment is determined as an example, in this task, the SA in the vocals decoder maintains the causal mask to generate vocals, while the SA in the accompaniment decoder employs the non-causal mask, with the BCA using the A2V strategy. Note that for the non-causal mask, we randomly mask 20% of tokens in the input sequence, to encourage the model to learn the relationships between context tokens. Furthermore, for the above two training tasks, we provide the model with a vocal and accompaniment prompt to encourage the model to learn to control the acoustic conditions of the generated audio.

  
**Tasks** & **Conditions** & **Outputs** & **SA mask** & **BCA mask** \\  Lyrics-to-song* & Lyrics, [Vocal prompt], [Accompaniment prompt] & Song, Vocals & Causal, Causal & BR \\ Lyrics-to-vocals* & Lyrics, [Vocal prompt] & Vocals & Causal, Causal & BR \\ Acompaniment-to-song & Lyrics, Accompaniment, [Vocal prompt] & Song, Vocals & Causal, Non-causal, Causal & A2V \\ Vocals-to-song & Vocals, [Lyrics], [Accompaniment prompt] & Song, Music & Non-causal, Causal & V2A \\ Music continuation & Accompaniment prompt & Music & None, Causal & None \\ Song editing* & Lyrics, Vocals, Accompaniment & Song, Vocals & Causal, Causal & BR \\ Vocals editing & Lyrics, Vocals & Vocals & Causal, None & None \\ Vocals editing in song* & Lyrics, Vocals, Accompaniment & Song, Vocals & Causal, Non-causal & A2V \\   

Table 2: Specific attention mask strategy of all tasks supported by SongCreator. [\(\)] indicates that the condition is optional. * indicates that our proposed model achieves significant improvements in this task.

Song editingThe song editing task combines the above two tasks. The difference is that we randomly select a span of tokens from the end of the target sequence to replace the audio prompt, using a special token <EDIT> in between to distinguish the editing task from the generation task.

In all training tasks, the vocal decoder and accompaniment decoder are trained using the next token prediction objective, and the song decoder predicts the semantic tokens of the complete song based on the embeddings extracted from the vocal decoder and accompaniment decoder. After that, we calculate the cross-entropy loss for vocals, accompaniment and song, and optimize the DSLM with the sum of these losses. Calculating the loss of the song also helps the model effectively reduce the impact of the source separation tool on the overall quality of the generated song. Note that we follow previous works  and calculate the loss on all tokens, not just the masked tokens, for non-causal strategy. Moreover, we also mask the lyrics 20% of the time to encourage the model to attempt unconditional generation.

## 4 Experiments

### Experimental setup

Data and modelDSLM is trained on 8,500 hours of song data with lyrics (approximately 270,000 songs). We employed an automatic speech recognition (ASR) model to provide timestamps for each sentence in the lyrics and a voice activity detection (VAD) model to detect silent segments. Then, we select appropriate silent segments to split the dataset into 1.7M clips, each no longer than 30 seconds and ensuring the completeness of the sentences. Each clip is input into the Demucs  music source separation model to extract vocals and accompaniment. Our DSLM has approximately 0.6B parameters. Detailed configurations are shown in Appendix A.1.

Training and InferenceDuring training, we train the DSLM for 500K steps using 8 NVIDIA A800 GPUs, with a batch size of 8 for each GPU. Adam optimizer is used with \(_{1}=0.9,_{2}=0.98,=10^{-9}\) and follow the same learning rate schedule in . Consistently, top-\(k\) sampling is adopted for inference, in which \(k\) and temperature are set to 50 and 0.9, respectively.

EvaluationsMost tasks are evaluated using both objective and subjective metrics.2 For objective evaluations, Frechet Audio Distance (FAD)  is used to evaluate the generation fidelity; Mel Cepstral Distortion (MCD) is used to measure the spectral distance between the synthesized and Ground Truth; Speaker Embedding Cosine Similarity (SECS) is used for the similarity of speaker identity. For subjective evaluations, we utilize the commonly used mean opinion score (MOS) tests. In various tasks, we assess multiple aspects: musicality, quality (focusing on clarity and intelligibility), style similarity (including speaker, melody and instruments), harmony between vocals and accompaniment, and naturalness. Moreover, AB preference tests are also conducted. The appendix G shows details of the evaluations.

BaselinesWe conducted comprehensive comparisons between SongCreator and multiple baselines on each task. First, we establish two baseline models for each task. One is SongCreator (Single) trained on a specific sequence generation task, and the other replaces DSLM with GPT  in SongCreator to predict the target sequence, named GPT. For lyrics-to-song, we directly conditioned SOTA music generation models, MusicLM  and MusicGen , on lyrics to predict songs. Furthermore, we add another baseline where GPT is used to first predict vocals and then predict the song, named GPT (Vocals & Song). For lyrics-to-vocals, in addition to MusicLM, we also introduce the SOTA text-to-speech method VALL-E . For vocals-to-song and accompaniment-to-song, we utilize the structure proposed in SingSong  to perform these two tasks, respectively. To ensure a fair comparison, we replace the semantic and acoustic tokens with BEST-RQ  tokens and use our latent diffusion model to convert them into the waveform, establishing another baseline, SingSong (Diffusion). For music continuation, we employ AudioLM  as a baseline. The detailed implementations of each baseline are shown in Appendix C.

[MISSING_PAGE_FAIL:8]

For the FAD score, our model reaches 1.88 and 1.24 on the two tasks, respectively, outperforming SingSong. A possible reason is that our model considers the complete song, rather than just the partially separated vocals considered in SingSong. In addition, we used the same vocals (6 samples) in SingSong's demos to generate songs with our model, and asked subjects to choose their preferred songs. As shown in Table 16, SingSong gets an extra preference (54.1%) over SongCreator (30%). We speculate one of the reasons is that SingSong uses a large-scale high-quality dataset (46k hours).

Music ContinuationFor the music continuation task, we compare different models by generating 10s music based on a 5s instrumental music prompt. As illustrated in Table 9, we can see that SongCreator achieves comparable results with AudioLM and GPT. This indicates that SongCreator can effectively continue the musical elements in the prompt, providing the capability to control the accompaniment in song generation.

Editing tasksTo evaluate the performance on editing tasks, we manually constructed a dataset of 30 song editing examples, as shown in Appendix D. Table 10 presents the results of song editing. We can see that SongCreator gets comparable performance in terms of naturalness to the baselines. However, benefiting from its strong ability to generate song, SongCreator surpasses these baselines in musicality, achieving a score of 4.01. In the vocal editing, as shown in Table 11, all three models achieve relatively close performance in both subjective and objective evaluations. To demonstrate the editing ability of SongCreator, we further conduct the AB preference test on three tasks: song editing, vocals editing, and vocals editing in song. In each task, SongCreator restores the masked song using its original lyrics and compares it with the audio samples reconstructed using BEST-RQ encoding and LDM decoding to eliminate the potential impact from the encoding and decoding processes during our experiments. The results are shown in Table 17. In all tasks, there is no significant difference between the generated song and the Ground Truth (\(p>0.01\)), where the p-values are calculated using the Wilcoxon signed-rank test. This means that humans judge the edited song produced by SongCreator to be as natural as the original unedited song.

### Ablation Studies

The influence of multi-task trainingThrough previous experiments, we can find that multi-task training significant improves most tasks, especially in lyrics-to-song. This indicates that the DSLM effectively capture the shared information between different tasks, such as composition, arrangement and the relationship between vocals and accompaniment.

The influence of bidirectional cross-attention layerWe evaluate the SongCreator and the model without using BCA on lyrics-to-song and lyrics-to-vocals. Figure 3 shows the results. When the BCA is removed from the DSLM, the performance on lyrics-to-song exhibit a marked deterioration, suggesting utilizing BCA is helpful for the model generate harmonious vocals and accompaniment. Interestingly, the performance also declined on the lyrics-to-vocals task, demonstrating that learning the relationships between vocals and accompaniment is also beneficial for generating vocals.

  
**Model** & **FAD \(\)** & **MNC \(\)** & **Musicality \(\)** & **Naturalness \(\)** \\  Ground Truth & - & - & \(4.08 0.07\) & \(3.99 0.06\) \\  GPT & 2.29 & 8.30 & \(3.84 0.07\) & \(3.72 0.06\) \\  SongCreator & **1.81** & 7.90 & \(\) & \(\) \\ SongCreator (Single) & 1.87 & \(\) & \(3.93 0.08\) & \(3.75 0.08\) \\   

Table 10: Song editing evaluation.

  
**Model** & **SECS \(\)** & **Musicality \(\)** & **Naturalness \(\)** \\  Ground Truth & - & \(3.65 0.08\) & \(3.45 0.07\) \\  GPT & 0.87 & \(3.61 0.07\) & \(\) \\  SongCreator & 0.87 & \(\) & \(3.31 0.06\) \\ SongCreator (Single) & 0.87 & \(3.63 0.06\) & \(3.41 0.06\) \\   

Table 11: Vocals editing evaluation.

Figure 3: Results of the AB preference test between SongCreator and the model without using BCA.

  
**Model** & **FAD \(\)** & **Musicality \(\)** & **Similarity \(\)** \\  Ground Truth & - & \(3.9 0.11\) & \(3.70 0.10\) \\  AudioLM & 1.33 & \(3.95 0.10\) & \(3.78 0.08\) \\ GPT & \(\) & \(3.90 0.10\) & \(3.73 0.11\) \\ SongCreator & 1.54 & \(\) & \(\) \\   

Table 9: Music continuation evaluation.

The influence of attention mask strategies in self-attention layerTo validate our designed SA mask strategies, we disable the non-causal mask of SA during training and conduct an AB preference test to compare this version with SongCreator on three tasks: lyrics-to-song, vocals-to-song, and accompaniment-to-song. As shown in Figure 4, the performance on all three tasks showed significant degradation, especially for vocals-to-song. These results indicate that incorporating the non-causal attention mask assists the learning of the relationships within the context and provides additional contextual information for generation.

The influence of attention mask strategies in bidirectional cross-attention layerTo validate our designed BCA mask strategies, we conduct AB preference tests for the lyrics-to-song and accompaniment-to-song tasks. For lyrics-to-song, we compared BR strategy with A2V, V2A and None strategy. As shown in Table 18, replacing the BR strategy with other strategies leads to a significant performance deterioration, demonstrating that the BR strategy is helpful for the model generate harmonious vocals and accompaniment. The None strategy, which disregards the relationship between vocals and accompaniment, performed the worst. In accompaniment-to-song, we compared A2V strategy with BR strategy. Table 19 shows the results, We find that participants preferred the song generated with the A2V strategy. We believe that this is because the A2V strategy provides more context about the accompaniment sequence when generating vocals.

## 5 Conclusion and Discussion

ConclusionIn this paper, we propose SongCreator, a system designed for lyrics-based song generation. We introduce a dual-sequence language model (DSLM) to separately model vocals and accompaniment information, and employs a dynamic bidirectional cross-attention module to capture the influences between these two sequences, with designing a serious of attention mask strategies for DSLM. In experiments, the proposed SongCreator provides competitive performance on all eight tasks.

LimitationsWe acknowledge the limitations of our proposed SongCreator. Due to the challenges in collecting data, SongCreator currently cannot control the genre and style of the output songs through text descriptions. Besides, the interference from accompaniment in the song makes it difficult for BEST-RQ to fully encode the vocal information, imposing a limited clarity of the synthesized vocals - in further work, we hope to extract better semantic representations for songs. Another issue is that the proposed model can only generate songs up to 30s, which is insufficient for supporting the generation of songs with complete structures.

Broader ImpactWe believe that our work has huge potential to develop into a song creation tool for content creators or novices to seamlessly express their creative pursuits with a low entry barrier, while also streamline and improve the workflow of experienced music producers. However, the potential negative impacts of SongCreator can't be overlooked. One of the primary concerns is the ability to replicate someone's voice with the vocal prompt, which could be exploited in the generation of misinformation, deepfake audio, or any harmful content. We are committed to advancing the field responsibly, and therefore, the checkpoints trained on the full dataset will not be released.