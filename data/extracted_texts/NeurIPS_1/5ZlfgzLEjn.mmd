# The Art of Knowing When to Stop: Analysis of Optimal Stopping in People and Machines

Fukun Evelene Zhang

Cognitive Science Program

Department of Mathematics and Statistics

Carleton College

zhange@carleton.edu

&Bonan Zhao

Department of Computer Science

Princeton University

bnz@princeton.edu

###### Abstract

In combinatorial innovation, people face the decision problem of when to invest in new development, and when to stick with the currently best option. Zhao, Velez, and Griffiths (2024) showed that under finite horizon, this equates to an optimal stopping problem, and provided analytical solutions. Interestingly, in behavioral experiments, while people's decisions aligned with the rational solutions overall, there were also systematic deviations. Here, we examine two heuristic models to this optimal stopping problem in combinatorial innovation. Our approach assumes that agents make decisions by running mental simulations that integrate prior beliefs and past observations. We show that these models well-capture various patterns in empirical data, suggesting that people may rely on simple heuristics to make fast decisions when solving computational problems involving sophisticated combinatorics. We also investigate whether Large Language Models (LLMs) can be used as a cognitive model to study these processes, report preliminary findings of LLM's limitation in this task, but suggest that chain of thought prompting may help mitigate these limitations.

## 1 Introduction

Innovation often comes from recombination of previous technologies. This leads to an intriguing observation: As the technology level goes up, the opportunity cost of developing a new technology grows higher, and the space of existing technologies to attempt combination with increases rapidly. Knowing when to stop exploring new opportunities thus is as important as achieving one's original goal, as over-persistence can waste time and resources 

Zhao, Velez, and Griffiths (2024) formalized this problem in a combinatorial discovery game. As a sequential decision-making task between "innovate or not" under finite horizon, they showed that this forms an optimal stopping problem  and offered an analytical solution. Interestingly, in behavioral experiments, although participants showed good intuitions about following a stopping rule, their stopping points varied compared to the rational solutions. Previous work has found out that participants often deviate from rational solutions, persisting with suboptimal strategies longer than necessary , and do so even when presented with the optimal strategy . Moreover, participants' performance did not improve over the course of time . These patterns, however, may be subject to training. For instance, Goldstein et al. (2020) observed significant learning leading to near-optimal stopping behavior in a repeated secretary problem.

To better understand the cognitive processes underlying optimal stopping, we explore several heuristic models to the computational problem in combinatorial innovation, drawing upon Bayesian inference and mental simulations. We also compare Large Language Models (LLMs) as agents to solve the same task. To foreshadow, the heuristic models well-capture many aspects of the behavioral data,implying a converging process to optimum, and LLMs struggle to make either human-like predictions or the rational solutions.

## 2 Modeling optimal stopping

### Task and problem

In the discovery game defined by Zhao, Velez, and Griffiths (2024), participants can either gain rewards from an existing item (extraction) or combine two items to create a new item with potentially higher rewards (fusion). Each game is parameterized by the probability of success (\(p\)) and the reward increase rate (\(w\)). This setup forms a Markov decision process. Under finite horizon, the optimal policy is to keep doing fusion until a switching point \(d\), after which keeps extracting the item with highest rewards. The expected reward for switching at step \(d\) is

\[_{(d)}=(n-d)(_{i=0}^{d}(pw)^{i}(1-p)^{d-i} )r. \]

Let the "remaining step" \(d^{}:=D-d+1\), Equation 1 states that

\[d^{}+1. \]

In an online behavioral experiment (Kumar et al., 2017), 210 participants were randomly assigned to one of the four conditions based on two parameters: \(p\{0.8,0.2\}\) and \(w\{3,1.5\}\). The conditions included high \(p\) (\(=0.8\)) with high \(w\) (\(=3\)), high \(p\) with low \(w\) (\(=1.5\)), low \(p\) (\(=0.2\)) with high \(w\), and low \(p\) with low \(w\). Each participant completed 9 tasks-2 practice and 7 official. Each task consisted 10 steps. At each step, participants could choose to either fuse or extract. All participants were informed of the relevant parameter values in the official tasks but not in the practice rounds.

Overall most participants followed a "switch-once" strategy as proposed by the rational model. However, the choice of switching points did not align perfectly. In the high-\(p\)-high-\(w\), high-\(p\)-low-\(w\), and low-\(p\)-high-\(w\) conditions, many participants exhibited under-exploration, switching too early; whereas those in the low-\(p\)-low-\(w\) condition showed over-exploration, switching too late compared to the predicted switch point in Equation 2. The most common switch points for the high-\(p\)-high-\(w\) and low-\(p\)-low-\(w\) conditions coincided with the optimal switching point (step 9 and step 0, respectively), yet only 32% and 18% of participants in these conditions switched at the optimal point. In contrast, the most common switch points for the high-\(p\)-low-\(w\), and low-\(p\)-high-\(w\) conditions did not align with the optimal switching point (step 7), instead being distributed evenly around the optimum.

### Bayesian heuristic models

Solving the combinatorics in Equation 1 can be challenging for a bounded agent. Here, we treat participants as Bayesian learners, updating their switch point decisions based on the previous round's reward and fusion feedback information. That is, we assume the player indeed switch once from fusion to extraction in the game, but the switch step is drawn from a distribution \(P(d),d\).

PriorWe use the practice round data to estimate the priors people brought into the official tasks, and approximate that empirical practice round distribution using a weighted combination of a uniform prior \(d_{U}(0,10)\) and and a Gaussian prior \(d_{N} N(,)\), where \(\) takes the value of the average switch step of the first practice round for each condition and \(=\). Next, we use hyperparameter \(q\) to control the relative contributions of the Gaussian and uniform priors using Equation 3, and the optimal value of \(q\) is fitted using Kullback-Leibler against the respective practice round data:

\[P(d)=q*P(d_{U})+(1-q)*P(d_{N}). \]

The simulated prior and people's first practice round distributions are plotted in Appendix A.1. Note that people might adapt different exploratory and exploitative strategies in the practice rounds, and we report those analysis in Appendix A.1.

Likelihoods.In task \(i\), the player chooses a switch step \(d_{i} P_{i}(d)\), and follows a policy that fuses for the first \(d_{i}\) steps and then extracts until the end. After this round of the game, the player observes total reward \(R_{i}\) and the total number of successes \(k\) for this round. Then, the player could estimate the amount of \(R_{i+1,d^{}}\) if switching at step \(d^{}\) for the next round of the game:

\[P(R|d^{})=R_{i+1,d^{}}=r w^{k}(10-d^{}). \]

We consider two ways (belief update systems) of estimating the expected rewards for the next rounds of the game.

Belief Update System 1 assumes agents lack predictive knowledge about rewards beyond the switch step, expecting post-switch rewards to match those on the switch step. For example, if an agent switches on step 6 after receiving 10 points, they expect earning 10 points on subsequent step. The expected rewards of switching before step 6 were calculated using the reward function in Equation 4.

Belief Update System 2 assumes that agents estimate a fixed number of successful fusions (\(s=2\) or 8) out of 10 steps, rather than evaluating each step's success probability. If an agent switches at step \(d\) and encounters \(z\) successful fusions (\(z<s\)), they mentally simulate \(s-z\) successful fusions for the remaining steps (\(d+1\) to 10). If \(z s\), they assume no further successes will occur.

Bayesian updatePutting these together, the agent estimates an updated switching point distribution following Bayes' rule:

\[P(|)= D}P(R|d)P( d)}, \]

where \(d D=\{0,1,,10\}\) representing the possible switch points, \(R\) is the estimated reward switching at step \(d\). For task 2 to task 7, each prior is the posterior from the the previous task,

\[P_{j}(d)=P_{j-1}(|),1<j 7 \]

Finally, a switch point is sampled from this posterior via applying a softmax function:

\[(P_{j})_{d}=/}}{_{d=0}^{10}e^{P_{j,d}/}}, \]

where \(\) is the temperature parameter that we later fit with empirical data.

### Results

We compare the two heuristic models, Belief Update System 1 and 2, to the rational model in Equation 2 in capturing participants' decisions in this optimal stopping problem. We ran 50 batches of 10,000 simulations and reported the mean results after fitting the softmax function (Equation 7). To include the rational model in the comparison, we applied Equation 7 to a one-hot encoder with the optimal switch point being 1 and all other steps being 0. Data and code are openly available at . While the rational model is only able to predict a single optimal switch point, the two heuristic models provide a better account for the general shape of the empirical switch point distributions found in people. As shown in Figure 0(a), both heuristic models accurately capture the left-skewed distribution for high-\(p\)-high-\(w\) (HH), high-\(p\)-low-\(w\) (HL), and low-\(p\)-high-\(w\) (LH) condition and the normal distribution shape but with a highest bar at step 0 for the low-\(p\)-low-\(w\) conditions (LL).

Comparing the two Belief Update Systems, we find that Belief Update System 2 performs better in the HH condition, accurately predicting the most common switching point (step 9) However, Belief Update System 2 deviates from the most common switching point by one step (step 8). In the HL and LH conditions, both Belief Update Systems perform similarly, predicting the most common switching point as step 7 and step 6, respectively, very clos to most common switching point favored by participants (step 6 and step 5). For the low-\(p\)-low-\(w\) (LL) condition, Belief Update System 1 performs better, capturing the highest bar at step 0 and the second highest bar in the middle (step 5). Evaluating the models using with the Bayesian Information Criterion (BIC) also confirms these observations (Appendix A.2).

LLM agents

We now examine Large Language Models (LLMs) as simulated participants in the same combinatorial discovery game. We first prompted GPT ("gpt-3.5-turbo" and "gpt-4-turbo") and Llama ("meta-llama/Llama-3.1-70B-Instruct") models with the discovery game tasks with the same setup (Direct Play), and in addition chain-of-thought prompting (COT) [3; 15]. Examples of each prompt type are provided in Appendix A.3.1.

### Direct Play

For Direct Play, our results revealed a striking difference between people's and LLM models' behavior (Figure 2). While about 80% participants switched only once per task , GPT-3.5 agents frequently switched multiple times across conditions (switch-once proportions: HH: 71.4%; HL: 14.3%; LH: 57.1%; LL: 28.6%). In contrast, GPT-4.0 and Llama-3.1 largely adhered to a switch-once strategy, the switch step pattern differed substantially from human participants (see Figure 4 in Appendix A.3.4). In conditions where people typically under-explored (HH, HL, LH), LLMs under-explored even more. For HH, GPT-4.0 and Llama-3.1 most commonly switched at step 5, under-exploring by 4 steps, while 32% of participants switched at the optimal step 9. In HL, GPT-4.0 stopped at step 5 (2 steps early) and Llama-3.1 at step 6 (1 step early). In LH, GPT-4.0 switched one step early (at step 6), while Llama-3.1 stopped at step 2, under-exploring by 5 steps. Conversely, in LL, where optimal switching is at step 0, both LLMs over-explored: GPT-4.0 switched at step 5 (5 steps late) and Llama-3.1 at step 3 (3 steps late), while 18% of participants switched optimally at step 0.

### Chain-of-thought prompting

We tested two variations of the chain-of-thought prompting: (1) explicitly informing the LLMs of the rational model in Equation 2 with an example optimal play (MDP), and (2) in addition to providing the equation and example optimal play, further asking the LLMs to explain why the example play is optimal (COT). For instance, COT prompts included explanations such as: "At this early stage, we want to attempt fusions to maximize future point potential. By fusing a and b, we create a new crystal worth 450 points, which can be used in future fusions," or "The optimal action is still fusion. Even though it only succeeds 2 out of 10 times, each new crystal discovered is worth three times more than the previous one!"

Our results showed that providing additional explanations through COT prompting significantly outperformed using only rational solutions and example of optimal plays (MDP). This suggests that mathematical solutions alone (MDP) are insufficient for LLMs to determine the optimal switch point

Figure 1: Histogram of participants’ (black bar) and Bayesian Learners\({}^{*}\) (colored dots) switch steps. Starts are the rational switch steps.

from fusion to extraction; reasoning prompts (COT) are necessary to help agents make more rational choices. COT prompting effectively guided the models to switch from fusion to extraction at the optimal point. Comparing to the rational model (see Figure 2), for the HH condition, GPT-3.5 with COT prompting has the highest fusion rate at the optimal switching point (step 9); for HL and LH condition, Llama-3.1 with COT prompting have the highest fusion rate at the optimal switching point (step 7); for the LL condition, both MDP and COT prompting led LLMs to maintain extraction throughout all 10 steps, aligning with the rational model's prediction in Equation 2. However, unlike participants who progressively approach the optimal point, LLMs with COT and MDP prompt typically switch optimally or near-optimally from the start of tasks and deviate over time, implying a lack of ongoing learning (see Figure 5 in Appendix A.3.4).

## 4 Discussion

Finding the optimal stopping point in large combinatorial spaces is challenging to people. Our heuristic models impute assumptions about approximating the optimal solution task-by-task via simple update, and better capture the empirical distributions than the rational model. Moving forward, we hope to develop interventions that encourage people to be more rational in similar settings inspired by the heuristic models. Testing the same experiments with GPT and Llama models revealed that LLM agents may approach the task differently from people. In Direct Play, LLMs struggled to identify the optimal strategy of switching once per task, often continuing to attempt fusions at the same level, wasting opportunities for higher rewards. With chain-of-thought (COT) prompting, LLMs learn the optimal strategy more effectively, including switching from fusion to extraction at the right moment and consistently extracting or fusing the highest-value crystals. While COT prompting helps LLMs achieve optimal solutions, their approach lacks the gradual adaptation seen in human learning. This suggests further research is needed to assess LLMs' viability as cognitive models, especially examining how COT improves LLMs' mathematical reasoning and its alignment with human cognition.

Figure 2: Heatmap showing the average frequency of fusion attempts at each step over seven rounds for LLM agents (GPT-3.5, GPT-4, and Llama-3.1-70B-Instruct) using three different prompting methods (Direct Play, MDP Prompt, COT Prompt) and participants . The rational switch steps are indicated by stars, with the highest fusion rate matching the optimal switching point circled in yellow. The best-performing prompting methods for each model are highlighted in yellow.