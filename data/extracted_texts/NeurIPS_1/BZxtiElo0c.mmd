# GeSS: Benchmarking Geometric Deep Learning under Scientific Applications with Distribution Shifts

Deyu Zou1, Shikun Liu2, Siqi Miao2, Victor Fung2, Shiyu Chang3, Pan Li2

1University of Science and Technology of China, zoudeyu2020@mail.ustc.edu.cn,

2Georgia Institute of Technology, {shikun.liu,siqi.miao,victorfung,panli}@gatech.edu

3University of California, Santa Barbara, chang87@ucsb.edu

Equal contribution. Code and data are available at [https://github.com/Graph-COM/GESS](https://github.com/Graph-COM/GESS).

###### Abstract

Geometric deep learning (GDL) has gained significant attention in scientific fields, for its proficiency in modeling data with intricate geometric structures. Yet, very few works have delved into its capability of tackling the distribution shift problem, a prevalent challenge in many applications. To bridge this gap, we propose GeSS, a comprehensive benchmark designed for evaluating the performance of GDL models in scientific scenarios with distribution shifts. Our evaluation datasets cover diverse scientific domains from particle physics, materials science to biochemistry, and encapsulate a broad spectrum of distribution shifts including conditional, covariate, and concept shifts. Furthermore, we study three levels of information access from the out-of-distribution (OOD) test data, including no OOD information, only unlabeled OOD data, and OOD data with a few labels. Overall, our benchmark results in 30 different experiment settings, and evaluates 3 GDL backbones and 11 learning algorithms in each setting. A thorough analysis of the evaluation results is provided, poised to illuminate insights for GDL researchers and domain practitioners who are to use GDL in their applications.

## 1 Introduction

Machine learning (ML) techniques, as a powerful and efficient approach, have been widely used in diverse scientific fields, including high energy physics (HEP) , materials science , and drug discovery , propelling ML4S (ML for Science) into a promising direction. In particular, geometric deep learning (GDL) is gaining much focus in scientific applications because many scientific data can be represented as point cloud data embedded in a complex geometric space. Current GDL research mainly focuses on neural network architectures design , capturing geometric properties (_e.g.,_ invariance and equivariance properties), to learn useful representations for geometric data, and these backbones have shown to be successful in various GDL scenarios.

However, ML models in scientific applications consistently face challenges related to data distribution shifts (\(_{}(X,Y)_{}(X,Y)\)) between the training (source) domain \(\) and the test (target) domain \(\). In particular, the regime expected to have new scientific discoveries has often been less explored and thus holds limited data with labels. To apply GDL techniques to such a regime, researchers often resort to training models over labeled data from well-explored regimes or theory-guided simulations, whose distribution may not align well with the real-world to-be-explored regime of scientific interest. In materials science, for example, the OC20 dataset  covers a broad space of catalyst surfaces and adsorbates. ML models trained over this dataset may be expected to extrapolate to new catalyst compositions such as oxide electrocatalysts . Additionally, in HEP, models are often trained based on simulated data and are expected to generalize to real experiments, which hold more variable conditions and may differ substantially from simulations .

Despite the significance, scant research has systematically explored the distribution shift challenges specific to GDL. Findings from earlier studies on CV and NLP tasks [9; 13; 99] might not be directly applicable to GDL models, due to the substantially distinct model architectures.

In the context of ML4S, several studies address model generalization issues, but there are two prominent _disparities_ in these works. First, previous studies are often confined to specific scientific scenarios that have different types of distribution shifts. For example,  concentrated exclusively on drug-related shifts such as scaffold shift, while  investigated model generalization to deal with the label-fidelity shifts in the application of materials property prediction. Due to the disparity in shift types, the findings effective for one application might be ineffectual for another.

Moreover, existing studies often assume different levels of the availability of target-domain data information. Specifically, while some studies assume some availability of the data from the target domain , they differ on whether such data is labeled or not. On the other hand, certain investigations presume total unavailability of the target-domain data . These varying conditions often dictate the selection of corresponding methodologies.

To address the above disparities, this paper presents **GeSS**, a benchmark to evaluate GDL models' capability of dealing with various types of distribution shifts across scientific applications. Specifically, the datasets cover three scientific fields: HEP, biochemistry, and materials science, and are collected from either real experimental scenarios exhibiting distribution shifts, or simulated scenarios designed to mimic real-world distribution shifts. Moreover, we leverage the specific generation process of geometric data, i.e., _the inherent causality_ of these applications to categorize their distribution shifts into different categories: conditional shift (\(_{}(X|Y)_{}(X|Y)\) and \(_{}(Y)=_{}(Y)\)), covariate shift (\(_{}(Y|X)=_{}(Y|X)\) and \(_{}(X)_{}(X)\)), and concept shift (\(_{}(Y|X)_{}(Y|X)\)). Furthermore, to address the disparity of assumed available out-of-distribution (OOD) information across previous works, we study three levels: no OOD information (No-Info), only OOD features without labels (O-Feature), and OOD features with a few labels (Par-Label). We evaluate representative methodologies across these three levels, specifically, OOD generalization methods for the No-Info level, domain adaptation (DA) methods for the O-Feature level, and transfer learning (TL) methods for the Par-Label level.

Our experiments are conducted over 6 datasets, in 30 different settings with 10 different distribution shifts times 3 levels of OOD info, covering 3 GDL backbones and 11 learning algorithms in each setting. According to our experiments, we observe that no approach can be the best for all types of shifts, and the levels of OOD info may benefit GDL models to various extents across different applications. In the meantime, our comprehensive evaluation also yields three valuable takeaways to guide the selection of practical solutions depending on the availability of OOD data:

* For Par-Label level, TL strategies show advantages under concept shifts, particularly when there are significant changes in the marginal label distribution.
* For O-Feature level, DA strategies excel when the distribution shifts happen to the geometric characteristics of features that are critical for label determination compared with other features.
* For No-Info level, OOD generalization methods will have some improvements if the training dataset can be properly partitioned into valid groups that reflect the shifts.

In addition to offering domain practitioners guidance on handling distribution shift issues, our new proposed HEP datasets and 10 curated distribution shift scenarios can also facilitate the development and evaluation of new algorithms within the GDL community for various scientific applications.

## 2 Comparison with Existing Benchmarks on Distribution Shifts

Prior research has constructed benchmarks tailored to diverse research fields, shifts, and knowledge levels, and some representative works are summarized in Table 1. In this section, we discuss how GeSS is compared to existing distribution-shift benchmarks in the following three perspectives.

**Application Domain.** Recent works have introduced benchmarks on distribution shifts across various application domains, including tabular data [23; 49], CV [34; 30; 109], OCR , GraphML [27; 14; 5], NLP [95; 104], and LLMs . Regarding ML4S, OOD issues have been discussed across various prediction tasks, such as retrosynthesis predictions  and property predictions on drugs [35; 86; 108], proteins , and materials , most of which are built upon GraphML settings. However, no benchmark studies have been conducted in numerous scientific applications, let alone with a focus on GDL models and a broad range of methodologies to deal with distribution shifts like this benchmark.

**Distribution Shift.** Previous works explored various distribution shifts. [77; 39; 28; 14] benchmark domain generalization methods. [39; 97; 70] study subpopulation shift.  categorizes and quantifies diversity and correlation shifts.  jointly analyzes spurious correlation, low-data drift and unseen data shift. [107; 53] benchmark spurious correlations in more diverse and realistic settings.  benchmarks a \(Y|X\)-shift (_aka._, concept shift in our work) which is shown to be prevalent in tabular data.  specifies covariate and concept shifts on the GraphML setting. However, many scientific application scenarios involve distribution shifts with mechanisms that differ from those mentioned above due to their specific data generation processes. Regarding ML4S, previous OOD benchmarks have proposed several data-split strategies to reflect distribution shifts in realistic scientific scenarios, such as molecular sizes and scaffolds , protein sequences and structures , and chemical reaction conditions . Compared to these works, our benchmark not only collects datasets that can reflect distribution shifts in real-world scientific challenges but also, from an ML perspective, leverages the inherent causality of the specific geometric data generation processes in these applications to categorize their distribution shifts for an in-depth analysis.

**Available OOD Info.** In addition to the level without any OOD data [39; 27], some studies assume the availability of OOD features and benchmark DA methods [69; 24], while others assume the availability of OOD labels to investigate the model transferability [8; 88; 17; 46]. Compared to previous works, we aim to understand the benefits of different levels of OOD data across various distribution shifts, so our benchmark integrates three information levels.

## 3 Benchmark Design

### Distribution Shift Categories

Let \(\) be the input space, \(\) be the output space, \(h:\) be the labeling rule. Under the OOD assumption, we have joint distribution shift, _i.e._, \(_{}(X,Y)_{}(X,Y)\). We denote \(f(;)\) as the GDL model with parameters \(\), and \(:\) as the loss function. Our objective is to find an optimal model \(f^{*}\) with parameters \(^{*}\), which can be best generalized to target distribution \(_{}\):

\[^{*}=_{}_{(X,Y)_{}}[ (f(X;),Y)] \]

For an in-depth analysis dedicated to scientific applications studied in this work, we consider the following data model [60; 9; 91; 45; 96]. The input variable \(X\) consists of two disjoint parts, namely the causal part \(X_{c}\) and the independent part \(X_{i}\), which satisfies conditional independence with \(Y\) given \(X_{c}\), _i.e._, \(X_{i}\!\!\! Y|X_{c}\). Next, we categorize various types of distribution shifts.

First, the above data model satisfies \((X,Y)=(Y|X)(X)=(Y|X_{c})(X)\). Thus, we define covariate and concept shifts as follows.

\(\) Covariate Shift holds if \(_{}(Y|X_{c})=_{}(Y|X_{c})\), and \(_{}(X)_{}(X)\).

\(\) Concept Shift holds if \(_{}(Y|X_{c})_{}(Y|X_{c})\). Note that the shift of \((Y|X_{c})\) is also characterized by the change of labeling rule \(h\) between \(\) and \(\).

    &  &  &  \\   & & Covariate & Conditional & Concept & No-Info & O-Feature & Par-Label \\  WILDS [39; 31] & CV and NLP & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  OoD-Bench  & CV & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  WILDS 2.0 [69; 103] & CV and NLP & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  Wild-Time  & CV and NLP & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  IGLUE  & NLP & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  GOOD , GDS  & Graph ML & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   & ML4S (Graph ML) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  & only Biochemistry & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   & ML4S (GDL) & & & & & & \\  & HEP, Biochemistry & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   & and Materials Science & & & & & & \\   

Table 1: Comparison with existing benchmarks under distribution shifts from three perspectives: Application Domain, Distribution Shift, and Available OOD Info. “Available OOD Info” refers to what type of OOD-Info has been used in the evaluation of these benchmarks.

On the other hand, we have \((X,Y)=(X|Y)(Y)\). This induces the scenario of Conditional Shift, which holds if \(_{}(X|Y)_{}(X|Y)\) and \(_{}(X)=_{}(Y)\), and Label Shift if \(_{}(X|Y)=_{}(X|Y)\) and \(_{}(Y)_{}(Y)\). But as label shift does not arise in our datasets, we later only focus on Conditional Shift. The conditional probability can be decomposed into two parts due to our data model, _i.e._, \((X|Y)=(X_{c}|Y)(X_{i}|X_{c})\). This enables us to further categorize Conditional Shift into two sub-types based on the specific factor experiencing shifts, and we observe that these two sub-types exhibit distinct characteristics in our experiments.

\(\)\(\)-Conditional Shift holds if \(_{}(X_{i}|X_{c})_{}(X_{i}|X_{c})\), and \(_{}(X_{c}|Y)=_{}(X_{c}|Y)\).

\(\)\(\)-Conditional Shift holds if \(_{}(X_{i}|X_{c})=_{}(X_{i}|X_{c})\), and \(_{}(X_{c}|Y)_{}(X_{c}|Y)\).

Given that each category mentioned above has only one factor experiencing shifts, we naturally partition sub-groups within the source domain \(\) based on the specific factor undergoing changes.

Note that our considered data models as above do not aim to cover all possible causal relationships. Some other models studied in previous literature  are discussed in Appendix A.1, but they do not correspond to GDL applications and thus fall outside the scope of this study. Our data models best describe the mechanisms of applications studied in this work. Besides, the categorization of distribution shifts is determined by the shifted probability term which _directly_ manifests through the data generation processes of the studied applications. These processes typically align with scientific experiment procedures or domain-specific theories. More details are in Appendix A.2.

### Dataset Curation and Shift Creation

In this section, we introduce the datasets in this study. Table 2 gives a summary. For each dataset, we first introduce the significance of its associated scientific application. Then, we delve into how the distribution shift of each dataset comes from in practice, and categorize the distribution shift according to the definition in Sec. 3.1. Additionally, we elaborate on the selection of domains \(\) and \(\), along with partitioning subgroups in the source domain \(\) for our later experimental setup.

  
**Scientific Field** & **Dataset** & **Domain** & **Shift Case** & **Shift Category** & **Evaluation Metrics** & **Applicable Method** \\   & Track-Pileup & Pickup & PISO & \(\)-Conditional Shift & ACC & Mixup \\  & & & & \( 3\) & & \\  & & & & \(_{50} 2\) & \(\)-Conditional Shift & ACC & DANN \\  & & & & \(_{50} 2\) & & \\   & Qitter & Fidelity & HSEC6 & Concept Shift & MAE & TL Methods \\   & DrugGOD-3D-Assay & Assay & lbap-core-ic50-assay & Concept Shift & & GroupDBO \\   & DrugGOD-3D-Size & Size & lbap-core-ic50-sime & Covariate Shift & AUC & DA or TL Methods \\   & DrugGOD-3D-Scaffold & Scaffold & lbap-core-ic50-scaffold & Covariate Shift & & TL Methods \\   

Table 2: Summary of distribution shifts in this study. We also recommend applicable methods for each scenario according to our experimental results, which are shown comprehensively in Table 3.

Figure 1: Overview of distribution shifts in this study. The upper (green-colored) and lower (blue-colored) instances represent the scenarios in domains \(\) and \(\), respectively. (a) Three-dimensional trajectories of particles in a collision event, which are simulated with a magnetic field parallel to the \(z\) axis and plotted on a 2D plane; (b) For the same set of MOFs, the distribution of calculated band gap values exhibits a bimodal (unimodal) nature with lower (higher) expectations under PBE (HSE06) estimation; (c) Molecular three-dimensional stick models with different scaffold IDs across \(\) and \(\).

#### 3.2.1 Track: Particle Tracking Simulation -- High Energy Physics

**Motivations.** ML techniques have long been employed and have played a significant role in diverse applications of particle physics , including particle flow reconstruction , jet tagging , and pileup mitigation , _etc._ Typically, ML models rely on simulation data for training due to the scarcity of labeled data from real-world experiments. However, the intricate and time-varying nature of experimental environments often leads to distinct physical characteristics that differ from simulated data used for training. For example, the pileup (PU) level, is defined as the number of noisy collisions around the primary collision in Large Hadron Collider experiments . The PU level during the real deployment phase can differ from the PU level used to train the ML model.

**Dataset.** We create Track, a particle tracking simulation dataset, and propose Track-Pileup and Track-Signal datasets. A data sample corresponds to a collision event, which generates numerous particles. Each particle will leave multiple detector hits when traversing the detector. Each point in a data sample represents a detector hit generated by a particle associated with a 3D coordinate. The task is to predict the existence of a specific decay of interest (referred to as _signal_ in our work) in a given event, denoted by \(Y\), like the decay of \(z\). This can be formulated as a binary classification task in differentiating the detector hits left by the signal particles plus backgrounds (\(X_{c}+X_{i}\)) from those only left by background particles (\(X_{i}\)). This application scenario naturally involves a data generation process \(Y X\) because the detector hit patterns are determined by whether some type of collision happens. Such generation process leads to conditional shift: The variation in the number of PU particles (Pileup Shift) causes a shift in \((X_{i}|X_{c})\), and the change in the type of signal particles (Signal Shift) causes a shift in \((X_{c}|Y)\). We further categorize the two shifts as follows.

**Pileup Shift -- \(\)-Conditional Shift.** As shown in the bottom left of Fig. 1a, a higher PU level results in more background particle tracks in a collision while keeping the signal particle track the same. This mechanism aligns with our definition of \(\)-Conditional Shift as \(_{}(X_{i}|X_{c})_{}(X_{i}|X_{c})\) and \(_{}(X_{c}|Y)=_{}(X_{c}|Y)\). We train the model on source-domain data with the PU level of 10 (PU10) and evaluate its generalizability on PU50 and PU90 target-domain data, respectively. The division of source-domain subgroups is based on the number of points present in the data entry (one collision event) as it can mimic Pileup Shift in terms of varying particle counts across different PU levels.

**Signal Shift -- \(\)-Conditional Shift.** As depicted in the bottom right of Fig. 1a, we alter the geometric characteristics of signal tracks by introducing signal particles with varying momenta, which leads to changes in the _curvature_ of signal tracks, while leaving the background particle tracks unchanged. Therefore, we categorize this shift as \(\)-Conditional Shift, as it satisfies \(_{}(X_{i}|X_{c})=_{}(X_{i}|X_{c})\) and \(_{}(X_{c}|Y)_{}(X_{c}|Y)\). We train the model on source-domain data, where the positive samples consist of 5 different types of signal decays, all characterized by large signal track radii, making them easier to distinguish from background tracks. We evaluate the model on target-domain data with signal decays of \(z_{20}^{} 2\), \(z_{10}^{} 2\), \( 3\), respectively, whose radii of signal tracks are smaller. We split the source \(\) into 5 sub-groups, each corresponding to a specific type of signal decay.

#### 3.2.2 Qmof: Quantum Metal-organic Frameworks -- Materials Science

**Motivations.** Materials property prediction plays a crucial role in discovering new materials with favorable properties . Training ML models using data with labels calculated from theory-grounded methods, such as DFT , to predict important materials properties, such as band gap, has been an emerging trend, accelerating the process of materials discovery. Among DFT methods, PBE techniques are popular for their cost-effectiveness. However, they are noted for producing low-fidelity results, particularly in the underestimation of band gaps . Conversely, high-fidelity methods exhibit highly accurate calculations but come at the cost of extensive computational resources, resulting in a scarcity of high-fidelity labeled data. Hence, there's a need for methods that allow ML models trained on low-fidelity data to generalize to high-fidelity prediction.

**Dataset.** We introduce the Quantum MOF (QMOF) , a publicly available dataset comprising over 20,000 metal-organic frameworks (MOFs), coordination polymers, and their quantum-chemical properties calculated from high-throughput periodic DFT. Each point in a sample represents an atom associated with a 3D coordinate. The task is to predict the band gap value of a given material as a regression problem that can be evaluated with MAE metrics. The dataset includes band gap values calculated by 4 different DFT methods (PBE, HLE17, HSE06*, and HSE06) ranging from low-fidelity to high-fidelity over the same set of input materials. This naturally forms the shifts across DFT methods at different fidelity levels (Fidelity Shift) categorized as follows.

**Fidelity Shift -- Concept Shift.** As illustrated in Fig. 0(b), DFT methods at different fidelity levels tend to yield varying distributions of the band gap estimation \(Y\) given the same set of input data \(X\), thus reflecting the shift of \((Y|X)\) characterized by concept shift. Namely, the expected estimation band gap values tend to increase sequentially from PBE (the lowest estimation) to HLE17, HSE06*, and HSE06 (the highest estimation). We construct 2 separate shift cases: one with HSE06 as the target domain \(\) and the other with HSE06* as the target domain. In both cases, the remaining three levels are used as the source domain \(\), each serving as a subgroup in the source-domain splits.

#### 3.2.3 DrugOOD-3D: 3D Conformers of Drug Molecules -- Biochemistry

**Motivations.** ML techniques have been applied to various biochemical scenarios, such as protein design , molecular docking , _etc._, thereby catalyzing the process of drug discovery. Despite the success, the performance of ML-aided drug discovery easily degrades due to the underlying distribution shifts. Unpredictable public health events like COVID-19 may introduce entirely new targets from unseen domains. Besides, the assay environments, where biochemical properties are measured, may also largely diverge. These challenges related to the distribution shift spur a need for generalizable ML models to further advance drug discovery.

**Dataset.** We adapt DrugOOD  and propose DrugOOD-3D, with our main focus on the geometric structure of molecules and GDL models. We adopt the task of Ligand Based Affinity Prediction (LBAP) in predicting the binding affinity of a given ligand molecule. We transition the task to a binary classification problem, using AUC scores as evaluation metrics, following DrugOOD. We built DrugOOD-3D-Scaffold, DrugOOD-3D-Size, and DrugOOD-3D-Assay datasets, corresponding to shift cases of lbap-core-ic50-scaffold, lbap-core-ic50-size, and lbap-core-ic50-assay, which cover scaffold, assay, and size shifts introduced as follows.

**Scaffold & Size Shift -- Covariate Shift.** The scaffold pattern, illustrated in Fig. 0(c), is a significant structural characteristic to describe the core structure of a molecule . Analogously, molecular size is also an important biochemical characteristic. We categorize the two shifts as covariate shifts because the shift in scaffold and size primarily reflects a shift in the marginal input distribution \((X)\) across domains, while the labeling rule \(h\) and \((Y|X)\) are kept invariant.

**Assay Shift -- Concept Shift.** We classify assay shift as concept shift since shifts in assays can be viewed as modifying the experimental procedures and conditions. Such modifications could alter the resulting binding affinity value for the same set of molecules, described as a change in \((Y|X)\). Note that we follow the same design of domain splits and sub-group splits as DrugOOD.

## 4 Experiments

### Experimental Settings

We briefly introduce the experimental settings and leave more details about dataset splits in Appendix C, backbones and learning algorithms in Appendix D, and hyperparameter tuning in Appendix F.

**Backbones.** We include three GDL backbones widely used in various scientific applications: EGNN , DGCNN , and Point Transformer .

**Learning Algorithms.** We select **11** most representative OOD methods to compare. These methods cover general, GNN-grounded, and GDL-grounded algorithms, and span a broad range of learning strategies under different levels of OOD info, _i.e._, No-Info, O-Feature, and Par-Label levels. For **No-Info** level, we select 1) _vanilla_: ERM ; 2) _invariant learning_: VREx ; 3) _data augmentation_: MixUp ; 4) _subgroup robust method_: GroupDRO ; 5) _causal inference_: DIR ; 6) _information bottleneck_: LRI . Note that DIR is a well-known graph-based OOD baseline and LRI is a novel algorithm grounded in GDL. We refer to the above-mentioned methods as _OOD generalization methods_ for simplicity. For **O-Feature** level, we select _domain-invariant methods_: 7) DANN  and 8) DeepCoral . For **Par-Label** level, we conduct _full_ fine-tuning, _i.e._, all model parameters get fine-tuned, with 9) 100, 10) 500, and 11) 1000 labels, denoted as \(_{100},_{500},_{1000}\) respectively. Regarding Fidelity Shift, we select a subset of OOD generalization methods (VREx and GroupDRO) that are compatible with regression tasks to evaluate.

We provide a detailed discussion on the rationale behind the selection of methods and GDL backbones in Appendices D.1 and D.2. Additionally, we have developed a highly modular codebase, allowing for the seamless evaluation of new algorithms tailored for the GDL setting using this benchmark.

**Dataset Splits.** For each dataset, we first divide it into the ID dataset and the OOD dataset based on our characterization of \(_{}\) and \(_{}\). The resulting dataset in the source domain contains multiple subgroups following our split covered in Sec. 3.2, for the operation of OOD methods that rely on subgroup splits. Subsequently, the ID and OOD datasets are randomly segmented into Train-ID, Val-ID, and Test-ID, and Train-OOD, Val-OOD, and Test-OOD, respectively.

**Model Training & Evaluation.** For fair comparisons across the three info levels, we meticulously set up both the model training and evaluation processes: In No-Info level, we train the model solely on the Train-ID dataset via OOD methods. In O-Feature level, we apply DA algorithms and train

    & &  &  \\   &  &  &  &  \\   & & &  &  &  &  &  &  \\   & ERM & 95.7(0.08) & 87.65(0.30) & 86.1(0.15) & 80.99(1.40) & 94.5(0.24) & **86.36(0.96)** & 94.35(0.24) & 79.84(0.08) \\  & VREx & 95.49(0.32) & 87.45(0.30) & 95.49(0.32) & 80.7(0.93) & 94.5(0.14) & 86.37(0.38) & 94.5(0.17) & **8.01(0.01(0.91)** \\  & GroupDR & 93.18(0.33) & 83.0(0.32) & 93.18(0.33) & 75.67(0.63) & 91.48(0.19) & 79.38(0.59) & 91.48(0.19) & 73.69(0.37) \\  & DIR & 95.10(0.09) & 85.9(0.45) & 95.10(0.09) & 78.4(0.17) & 94.01(0.29) & 84.33(0.65) & 94.01(0.29) & 75.73(1.39) \\  & LRI & 95.80(0.41) & 85.8(0.31) & 95.77(0.43) & 81.3(0.80) & 85.05(0.40) & 85.25(0.40) & 93.95(0.47) & 76.56(0.39) \\  & MatLip & 95.78(0.41) & **89.41(0.11)** & 95.86(0.13) & 82.29(0.40) & 94.18(0.25) & 85.93(0.46) & 94.18(0.25) & 79.43(0.77) \\   & DANN & 95.18(0.51) & 87.16(0.72) & 95.86(0.10) & 86.09(0.44) & 93.91(0.47) & 85.31(0.64) & 94.35(0.12) & 76.18(0.15) \\  & Coral & 95.12(0.27) & 86.50(0.30) & 89.19(0.79) & 78.19(0.37) & 94.17(0.24) & 84.11(0.14) & 94.66(0.16) & 77.08(0.09) \\   & TL\({}_{}\) & & 84.20(0.46) & 77.85(0.59) & 81.65(1.06) & 73.48(0.39) & & & & & \\  & TL\({}_{}\) & & 87.05(0.46) & 82.09(0.88) & 84.41(1.06) & & & & & \\  & TL\({}_{}\) & & 87.61(0.13) & **83.40(0.80)** & 85.09(0.70) & 79.97(0.30) & & & & & \\    &  &  &  &  &  &  &  &  &  &  &  \\    &  &  &  &  &  &  &  &  &  &  &  &  \\    &  &  &  &  &  &  &  &  &  &  &  &  &  \\   & ERM & 91.15(0.20) & 65.98(0.77) & 96.85(0.23) & 70.72(1.28) & 95.72(0.23) & 63.01(0.13) & 95.37(0.12) & 69.04(0.31) \\  & VREx & 96.85(0.29) & 66.80(0.38) & 96.66(0.15) & 71.46(0.87) & 95.66(0.03) & 64.91(0.97) & 95.93(0.32) & 63.03(0.06) \\  & GoopDRO & 96.48(0.22) & 67.15(0.10) & 97.16(0.08) & 72.56(0.88) & 95.02(0.06) & 66.01(0.33) & 95.66the model on the _same_ Train-ID used in No-Info level _plus_ the extra data feature info of the entire OOD dataset. In Par-Label level, we use the Train-OOD dataset to fine-tune the model which has already been pre-trained on the _same_ Train-ID used in the two previous levels. Although the training datasets may vary due to the need to contain different levels of OOD info, we achieve a fair comparison by keeping Train-ID data invariant. Across all levels of OOD info, we evaluate the model's ID performance using the _same_ Val-ID and Test-ID datasets, and its OOD performance using the _same_ Val-OOD and Test-OOD datasets. For hyperparameter tuning, we tune a predefined set of hyperparameters and select the model with the best metric score of Val-OOD for the ultimate evaluation. Also, we thoroughly discuss the motivation and insights of our design of model training and evaluation processes, and put details in Appendix E.

### Results Analysis -- General Tendency

We put experimental results on 2 of 3 backbones in Table 3. Complete results can be found in Appendix G. We begin by presenting overall comparisons and general findings: Although \(_{1000}\) outperforms ERM by a notable margin in several cases, fine-tuning can sometimes result in negative effects when the labeled OOD data is quite limited, particularly in cases involving a smaller degree of distribution shifts (_cf._ Fig. 2). This is consistent with , where fine-tuning a large model based on a small set of labels may lead to catastrophic forgetting. To mitigate this issue, robust fine-tuning strategies, such as weight-space ensembles , regularization  and surgical fine-tuning , could be potential solutions.

### Results Analysis -- Insightful Conclusions

Besides the general observations exhibited above, our experiments also yield some intriguing conclusions that may be widely applicable. We structure this subsection by first presenting our conclusions, exemplified by representative observations and rational explanations.

\(\)**Conclusion 1. DA strategies excel in \(\)-Conditional Shift, where some variation arises in the geometric characteristics of the causal component \(X_{c}\).**

A great example illustrating this conclusion comes from Signal Shift. As shown in Fig. 4, DANN, a DA method, performs particularly well in the case of \(z^{}_{10} 2\), largely outperforming ERM (\(\) 9.4% in the EGNN model) and all OOD generalization methods without OOD info (\(\) at least 6.6% in EGNN). As introduced before, Signal Shift represents a \(\)-conditional shift, which in this case arises from the shift in the curvature of the signal tracks, whose points collectively form the causal component \(X_{c}\). The access to OOD features enables the model to align the latent representations of the causal components with varying distributions of geometries (curvatures here) across the source and target domains, thereby aiding in the correct identification of unseen signal types.

In contrast, Fig. 4 shows that DA strategies yield performance very close to ERM in Pileup Shift. Although both Pileup and Signal shifts are categorized as Conditional Shifts (_cf._ Sec. 3.2.1), they mainly differ in two aspects: 1) Pileup Shift represents an \(\)-Conditional Shift, occurring exclusively in the independent part, _i.e._\((X_{i}|X_{c})\), and 2) it involves a variation in the number of particle tracks rather than geometric characteristics, which is different from Signal Shift. We propose two plausible explanations for the challenges faced by DA strategies in Pileup Shift, centered around these disparities, and recommend interested readers see **H2** in Appendix H.

\(\)**Conclusion 2. TL methods excel under Concept Shift, particularly when the shift of the marginal label distribution \((Y)\) is large.**

Here we examine two cases of Fidelity Shift, where the TL strategy demonstrates contrasting results (_cf._ Fig. 2(a)): TL performs particularly well in the case of HSE06, where it largely outperforms all other methods with the MAE score increased by at least 40%. However, it exhibits only limited improvement in the case of HSE06*. We explain the difference by analyzing the marginal label distributions \(_{}(Y)\) and \(_{}(Y)\). As mentioned before, Fidelity Shift can be characterized by a change in the labeling rule, _i.e._, two similar inputs may be mapped to very different \(Y\) values.

Figure 2: Test-OOD improvements (%) over ERM for \(_{100}\) and \(_{1000}\) across different shift cases (in the EGNN backbone).

Specifically, fidelity levels of PBE, HLE17, and HSE06* provide estimations that are closer to each other, while the fidelity level of HSE06 significantly exceeds the other three. Therefore, the case with HSE06 as the target domain \(\) but the other three as the source \(\), yields a large gap in the distribution of \((Y)\) between the two domains, _i.e.,_\(_{S}(Y)_{}(Y)\) (_cf._ Fig. 2(b)). Therefore, the OOD labels are crucial to finetune the model predictions to match the aimed distribution \(_{}(Y)\). In contrast, the case when HSE06* is the target domain \(\) but the others as \(\), yields closer distributions of \((Y)\) between the two domains, _i.e.,_\(_{}(Y)_{}(Y)\) (_cf._ Fig. 2(c)). Therefore, only using a few OOD labels to finetune the model tends to have a limited impact on its performance.

\(\)**Conclusion 3. For the OOD generalization methods to learn robust representations, the more informatively the groups obtained by splitting the source domain \(\) indicate the distribution shift, the better performance these methods may achieve.**

This observation is related to GroupDRO, an OOD method that is to learn robust representation across different group splits of the source \(\). As illustrated in Fig. 4, GroupDRO almost consistently outperforms ERM in all cases with Signal Shift (\(\), \(z^{}_{10},z^{}_{20}\)) while it largely under-performs ERM in the cases of Pileup Shift (PUS0, PUS09). GroupDRO captures robustness by increasing the importance of subgroups with larger errors and thus highly relies on the assumption that the shift between the splits of in-domain data can to some extent reflect the distribution shift between the source \(\) and the target \(\). In the cases with Signal Shift, the way to split subgroups of the source domain aligns well with the distribution shift: Each split represents a distinct type of decay (5 types in total). By learning robust representations across these subgroups, GroupDRO yields better OOD generalization. In contrast, in the case of Pileup Shift, varying the number of points in a collision event is used as a proxy of the shift to achieve the group splits of the training dataset, based on the fact that the PU level is positively correlated with the number of particles. This way of subgroup splits is subjective, which is limited by the availability of data and may not fully reflect Pileup Shift between domain \(\) and \(\).

More analysis (observations, conclusions, and conjectures) of experimental results are put in Appendix H. Also, we identify conclusions that align with existing OOD literature and others that are novel and _specific_ to the GDL setting. We conduct comparisons (including consistency and disparity) between our findings and previous ones in Appendix I.

## 5 Conclusion

This work systematically evaluates the performance of GDL models when scientific applications meet distribution shifts. Our benchmark has 30 distinct scenarios with 10 shift cases times 3 levels of available OOD info, covering 3 GDL backbones and 11 learning algorithms. Based on our evaluation, we reveal several intriguing discoveries. In particular, our results may help select applicable solutions based on the causal mechanism behind the distribution shift and the availability of OOD info. Moreover, our work encourages more realistic and rigorous evaluations of GDL used in scientific applications, and may inspire methodological advancements for GDL to deal with distribution shifts.

Figure 4: Test-OOD improvements (%) over ERM for GroupDRO, MixUp, LRI, and DANN methods across Pileup Shifts (cases of PUS0/90) and Signal Shifts (cases of \( 3\) and \(z^{}_{10} 2\)) in the EGNN backbone.

Figure 3: (a) Test-OOD improvements (%) over ERM for VREx, DeepCoral, \(_{100}\) and \(_{1000}\) methods in Fidelity Shifts (including HSE06 and HSE06* cases) in the EGNN backbone; (b)/(c) KDE  curves of the marginal label distribution \((Y)\) across the source \(\) and target \(\) in the cases of HSE06 / HSE06*.