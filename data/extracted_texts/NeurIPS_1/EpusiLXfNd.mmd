# 3D Structure Prediction of Atomic Systems with Flow-Based Direct Preference Optimization

Rui Jiao\({}^{1,2}\) Xiangzhe Kong\({}^{1,2}\) Wenbing Huang\({}^{3,4}\) Yang Liu\({}^{1,2}\)

\({}^{1}\)Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University

\({}^{2}\)Institute for AIR, Tsinghua University

\({}^{3}\)Gaoling School of Artificial Intelligence, Renmin University of China

\({}^{4}\) Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China

Wenbing Huang and Yang Liu are corresponding authors.

###### Abstract

Predicting high-fidelity 3D structures of atomic systems is a fundamental yet challenging problem in scientific domains. While recent work demonstrates the advantage of generative models in this realm, the exploration of different probability paths are still insufficient, and hallucinations during sampling are persistently occurring. To address these pitfalls, we introduce FlowDPO, a novel framework that explores various probability paths with flow matching models and further suppresses hallucinations using Direct Preference Optimization (DPO) for structure generation. Our approach begins with a pre-trained flow matching model to generate multiple candidate structures for each training sample. These structures are then evaluated and ranked based on their distance to the ground truth, resulting in an automatic preference dataset. Using this dataset, we apply DPO to optimize the original model, improving its performance in generating structures closely aligned with the desired reference distribution. As confirmed by our theoretical analysis, such paradigm and objective function are compatible with arbitrary Gaussian paths, exhibiting favorable universality. Extensive experimental results on antibodies and crystals demonstrate substantial benefits of our FlowDPO, highlighting its potential to advance the field of 3D structure prediction with generative models.

## 1 Introduction

Predicting 3D structures of atomic systems is indispensable in various scientific domains, ranging from pharmaceutical drug design  to materials science . Accurate 3D modeling is not only crucial for understanding the physical and chemical properties of substances at the atomic level  but also for simulating and predicting their behavior in various environments . Nevertheless, it remains challenging due to the intricate nature of atomic interactions, the vastness of the conformational space, as well as limited resources of structure data.

Conventional methods typically employ physics-based algorithms to derive structures at local energy optimum . Recent advancements leverage deep generative model to learn the distribution of stable structures from available data, showcasing remarkable success across various domains. For example, DiffAb  designs a diffusion-based method for antigen-specific antibody design, which is further available for antibody structure prediction, and DiffCSP  proposes a joint diffusion framework for crystal structure prediction. Despite these advancements, generative models for structure prediction are confronted with two primary challenges.

First, existing structure prediction methods predominantly utilize diffusion-based generative models. While effective, this focus narrows the scope of exploration into other probability paths that couldpotentially offer substantial benefits. A notable example is the Optimal Transport (OT) path, which has recently been demonstrated to be particularly effective in the field of molecular generation . Second, current training paradigm frequently leads to hallucinated distribution peaks . Most generative models are trained through maximizing the likelihood or its lower bound on the ground-truth structures, which are easily haunted by hallucinations due to the lack of negative samples during training. In the field of natural language processing or computer vision, Direct Preference Optimization (DPO) [25; 30] is proposed to align the model with human preferences, which effectively reduces hallucinations. For 3D structure prediction, such preferences can be naturally extended to similarity with the reference structure (_e.g._ RMSD). However, it remains unclear whether the DPO method is compatible with arbitrary probability paths.

To address the above pitfalls, we introduce FlowDPO, a novel framework that explores flexible selection of Gaussian paths and enhances the quality of generated structures by alignment with the reference distribution. Specifically, we approach the structure prediction task via flow matching models regarding various paths. Given a pre-trained flow matching model, we sample multiple structures for each entry in the training set, evaluate these candidates against known ground truths to compute similarity, and construct an automatic preference dataset. Notably, we theoretically derive the unified objective of DPO for arbitrary Gaussian paths, and leverage the preference to enhance the performance of the original generative model. Intuitively, such a paradigm not only augment data with self-distilled samples, but also endow the model with the ability to distinguish between high-fidelity and hallucinated samples.

In summary, our contributions are threefold:

* We explore multiple accessible probability paths for the 3D structure prediction task, and to the best of our knowledge, we are the first to theoretically prove the compatibility of DPO with arbitrary Gaussian paths by deriving a unified objective.
* Based on the theoretical results, we develop a novel framework to encourage better alignment of flow matching models with desired reference distribution in 3D structure prediction, which effectively suppresses the probability of hallucinations.
* Our approach yields promising results on antibody and crystal structure prediction tasks, showcasing the versatility and efficacy of our FlowDPO.

Figure 1: Overview of the proposed FlowDPO pipeline. As described in Section 3.1, the process begins by training a flow matching model, denoted as \(_{}\), using an arbitrary pre-defined Gaussian path. Next, as outlined in Section 3.2, we construct a preference dataset, \(_{}\), by evaluating the distances between generated samples \(_{ij}\) and the ground structure \(x_{i}\) under a given context condition \(c_{i}\)â€”such as an antibody sequence or crystal composition. These samples are derived from the reference training set \(_{}\). This dataset is then used to fine-tune the model \(_{}\) through the DPO training objective \(_{}\), detailed in Section 3.3.

Related Work

Structure Prediction for Atomic Systems.3D Structure prediction, including predicting conformations from molecular topological graphs , determining unit cell structures from crystal compositions , or inferring structures based on protein sequences , is crucial in computational chemistry and material science. Traditionally, these predictions have relied on physics-inspired scoring functions [21; 11] or density functional theory (DFT)-based energy calculations  to define the search space, with subsequent application of search algorithms to identify optimal structures. Recently, deep generative methods, particularly diffusion models [12; 27], have proven to be highly effective in this field. These models have been successfully applied across multiple specific domains, including small molecules , crystals , antibodies , complexes , and general biomolecules . The emergence of flow matching models , which generalize diffusion paths to more flexible probability flows, has further enhanced the generative capabilities for geometric graphs . The goal of our work is to explore structure prediction from the perspective of flow matching, and align these models towards more accurate predictions.

Aligning Generative Models.In the domain of generative model alignment, recent work has focused on refining models to better meet human preferences. Direct Preference Optimization (DPO), introduced by , offers a significant advancement over traditional Reinforcement Learning from Human Feedback (RLHF, ) methods by directly optimizing a policy based on human preference data. This approach has proven effective in aligning large language models (LLMs) with user expectations. Extending this concept,  propose Diffusion-DPO, a novel method that adapts DPO for text-to-image diffusion models. By reformulating the preference optimization for diffusion model likelihoods, Diffusion-DPO achieves state-of-the-art performance in generating images that are not only visually appealing but also closely aligned with textual prompts. Recently,  introduces ABDPO, a DPO-based method tailored for antibody design. Unlike ABDPO, which concentrates on guiding diffusion models to generate antibody candidates with lower energy, our approach emphasizes aligning flow models for precise structure predictions.

## 3 FlowDPO

### Flow Matching for Geometric Graphs

Flow Matching (FM, ) is a general paradigm for generative tasks by learning a vector field to connect the pre-defined prior distribution with the targeted data distribution. Let \(q\) denote the data distribution, \(x_{0}\) is a data point acquired from \(p_{0}=q\), and \(x_{1}\) is a random sample from the prior distribution \(p_{1}\). A time-dependent flow \(_{t}\) is then defined to shift samples from the prior distribution to the time-dependent distribution \(p_{t}\) via the vector field \(v_{t}\), that is

\[_{1}(x)=x_{1},(x))}{dt}=v_{t}(_{t}(x)). \]

The vector field can be further parameterized by a time-dependent model \(v_{}(x_{t},t)\), leading to the continuous normalizing flows (CNFs, ). To avoid numerical ODE simulations to train \(v_{}\), FM simplifies the training target by aligning the model with a pre-defined vector field \(u_{t}\) to yield \(p_{t}\), _i.e._,

\[_{}=_{t,x_{t} p_{t}(x_{t})}[\|v_ {}(x_{t},t)-u_{t}(x_{t})\|_{2}^{2}]. \]

However, as \(p_{t}\) is still unknown, we are still unable to sample \(x_{t}\) and apply the above objective. To address this gap,  leverages the more accessible conditional vector field \(u_{t}(x_{t}|x_{0})\) and its corresponding probability path \(p_{t}(x_{t}|x_{0})\), resulting in the following Conditional Flow Matching (CFM) objective, which is equivalent to \(_{}\) in terms of gradients and accessible for sampling:

\[_{}=_{t,x_{t} p_{t}(x_{t})}[\|v_ {}(x_{t},t)-u_{t}(x_{t}|x_{0})\|_{2}^{2}]. \]

Different vector fields lead to different probability paths. For the commonly-used Gaussian distribution defined as

\[p_{t}(x_{t}|x_{0})=(x_{t};_{t}(x_{0}),_{t}^{ 2}(x_{0})), \]the corresponding vector field  is calculated as

\[u_{t}(x_{t}|x_{0})=^{}_{t}(x_{0})+_{t}(x_{0})}{ _{t}(x_{0})}(x-_{t}(x_{0})), \]

where \(^{}_{t},^{}_{t}\) are derivatives of \(_{t},_{t}\)_w.r.t._\(t\). We consider three lines of Gaussian paths in this paper, namely the Variance Exploding (VE), Variance Preserving (VP) and Optimal Transport (OT) paths, which are listed in Table 1.

Based on these paths, we are capable of designing proper flow models to maintain symmetries for specific structure prediction tasks. In this paper, we mainly focus on the two typical tasks on atomic systems: antibody structure prediction and crystal structure prediction. Note that symmtries are crucial in 3D atomic systems, and we provide more discussions in Appendix B.

**Example 1: Antibody Structure Prediction.** Antibodies are Y-shaped proteins generated by the immune system to identify and bind to specific antigens, with the structure depicted in Figure 2. Researchers mainly center on the variable domains of antibodies, which comprise a heavy chain and a light chain. Each chain includes three _Complementarity-Determining Regions (CDRs)_ and four framework regions in an alternating sequence. The six CDRs are volatile and crucial in defining the binding specificity and affinity, while the framework regions remain conserved. Among them, CDR-H3, which is the third CDR on the heavy chain, is the most diverse region and the primary focus of antibody design. Therefore, it is a fundamental yet challenging problem to accurately predict the structure of the CDRs upon binding.

_Task Definition:_ Let \(=\{_{1},_{2},,_{N}\}\) denote the sequence of the targeted CDR region with the length of \(N\), where \(_{i}\{0,1\}\) is the one-hot type of the amino acid, and \(}=\{}_{1},}_{2},,}_{N}\}\) is the corresponding 3D structures with \(_{i}^{3 4}\) as the backbone coordinates including \(N,C_{},C,\) and \(O\). Similarly, the sequence and structure of the context (_i.e._ framework regions and the antigen) are defined as \(^{C},}^{C}\). The goal is to predict the structure of the CDR region given the context:

\[} p_{0}(}|,}^{C},^{C}). \]

_Probability Paths and Training Objectives:_ DiffAb  has designed the VP path for the coordinates of the CDR region as

\[}_{t,}(}_{t}|}_{0},,}^{C},^{C})=_{t}}{1-^{2}_{t}}(_{t}}_{t}-}_{0}), \]

where \(_{t}\) is scheduled as \(_{t}=e^{-_{0}^{t}(s)ds}\). After sampling \(}_{0}=}(0,)\), we have \(}_{t}=_{t}}_{0}+_{t}}}\). With proper reparameterization, the training objective is defined as

\[_{}=_{t,}}[\|}_{}(}_{t},,}^{C},^{C})-}\|^{2}_{2}], \]

   Probability Path & Mean & Standard Deviation & Conditional Vector Field \\   VE path & \(_{t}(x_{0})=x_{0}\) & \(_{t}(x_{0})=_{t}\) & \(u_{t}(x_{t}|x_{0})=_{t}}{_{t}}(x_{t}-x_{0})\) \\  VP path & \(_{t}(x_{0})=_{t}x_{0}\) & \(_{t}(x_{0})=_{t}}\) & \(u_{t}(x_{t}|x_{0})=_{t}}{1-^{2}_{t}}(_{t}x_{t }-x_{0})\) \\  OT path & \(_{t}(x_{0})=(1-t)x_{0}\) & \(_{t}(x_{0})=t\) & \(u_{t}(x_{t}|x_{0})=(x_{t}-x_{0})\) \\   

Table 1: Parameters of different Gaussian paths. VE, VP and OT represent Variable Exploding, Variable Preserving and Optimal Transport, respectively.

Figure 2: Graphical depiction of antibody variable domains, which consist of a heavy chain and a light chain. Each chain is equipped with 4 Framework Regions (FRs) and 3 Complementarity-Determining Regions (CDRs). The CDRs, especially CDR-H3, are volatile and thus are the key focus.

which only requires a model \(\) to predict the denoising term given the current state.

Moreover, it is also practicable to linearly connect the data point \(}_{0}\) and the noisy prior \(}\) via the OT path as \(}_{t}=(1-t)}_{0}+t}\). The vector field is then defined as

\[}_{t,}(}_{t}|}_{0},, }^{C},^{C})=(}_{t}-}_{0})=}-}_{0}. \]

The training objective directly align the model with the simple vector field:

\[_{}=_{t,}}\|} _{}(}_{t},,}^{C},^{C})-(}-}_{0})\|_{2}^{2}. \]

**Example 2: Crystal Structure Prediction.** Crystal Structure Prediction (CSP), a fundamental aspect of material science, requires to predict the stable 3D structure of a compound solely from its composition. Unlike molecules or proteins, which have a finite number of atoms, the uniqueness of crystals lies in their periodic repetition in infinite 3D space. The infinite crystal structure is typically simplified by its repeating unit, which is called a _unit cell_. The key point of CSP is the representation and generation of the unit cell.

_Task Definition:_ A unit cell is usually characterized by a triplet \(=(,,)\), where \(=[_{1},_{2},...,_{N}]^{h N}\) represents the one-hot encoded atom types, \(=[_{1},_{2},_{3}]^{3 3}\) denotes the lattice matrix with three basis vectors describing the crystal's periodicity, and \(=[_{1},_{2},...,_{N}]^{3 N}_{[0,1)}\) contains the fractional coordinates of the atoms, specifying their positions relative to the lattice matrix. The goal of CSP is to predict the lattice matrix and the atomic coordinates based on the given crystal composition as

\[(,) p_{0}(,|). \]

_Probability Paths and Training Objectives:_ As the lattice matrix \(\) also lies in the Euclidean space, we can design similar VP and OT paths as Eq. (7-10). Given \(_{}(0,)\), with \(_{t}=_{t}_{0}+^{2}}_{}\), the loss function of the VP path is defined as

\[_{,}=_{t,_{}}\| _{,}(_{t},_{t},)-_{ {L}}\|_{2}^{2}. \]

Besides, with \(_{t}=(1-t)_{0}+t_{}\), the training objective of the OT path is similarly defined as

\[_{,}=_{t,_{}}\| _{,}(_{t},_{t},)-(_{}- _{0})\|_{2}^{2}. \]

The fractional coordinates lie in the torus space of \(^{3 N}_{[0,1)}\) to inherently reflect the periodicity of the crystal. Previous works [15; 14] project the VE path to this manifold, and the Gaussian distribution is changed into the Wrapped Normal (WN) distribution as \(p_{t}(_{t}|_{0})=_{w}(_{t};_{0},_{t}^{ 2})\),

\[p_{t}(_{t}|_{0})=_{w}(_{t};_{0},_{t}^ {2}), \]

where \(_{w}(x;,)=_{i=-}^{}_{w}(x+i ,)\). An accessible way to learn this path is to match the score, _i.e._ the negative logarithmic gradient, of \(p_{t}\), and the loss function is defined as

\[_{,}=_{t,_{t}}_{t}\| _{,}(_{t},_{t},)-_{_{t}}  p_{t}(_{t}|_{0})\|_{2}^{2}, \]

where \(_{t}=^{-1}\|_{w}(0,_{t}^{2} )\|_{2}^{2}\) is the pre-computed weight. If \(_{1}\) in Eq. (14) is sufficiently large, \(p_{1}\) would finally approach the uniform distribution, which can be selected as the prior distribution. Apart from the VE path, it is also applicable to directly connect the data point and the prior sample via the shortest path on the manifold. Specifically, given \(_{0} p_{0},_{1} p_{1}\), where \(p_{1}\) is defined as the uniform distribution, the shortest path \((_{0},_{1})\) can be determined by the logarithmic map from \(_{0}\) to \(_{1}\) as \((_{0},_{1})=_{_{0}}_{1}=w(_{1}-_{ 0}+0.5)-0.5\). Alternatively, \(_{1}\) can also be considered as the destination of \((_{0},_{1})\) via the exponential map from \(_{0}\) as \(_{_{0}}(_{0},_{1})=w(_{0}+(_{0}, _{1}))\). To eliminate the effect of the overall translation introduced by the prior, we further normalize \(_{1}\) as \(}_{1}=_{_{0}}(_{0},_{1})=_{ _{0}}s(_{0},_{1})-(_{0},_{1})\)

Figure 3: A crystal is the infinite periodic arrangement of atoms, and the repeating unit is named as a unit cell.

where \(\) averages the paths of all atoms. With the path of \(_{t}\) defined as \(_{t}=_{_{0}}t(_{0},_{1})\), the training objective for the OT path is

\[_{,}=_{t,_{1}}\|_{,}(_{t},_{t},)-(_{0},_{1})\|_{2}^{2} . \]

Generalized Notations.Overall, the structure prediction tasks aims at generating the targeted structure \(x\) given some condition \(c\), _i.e._ to learn \(p_{0}(x|c)\). And the flow matching objective minimizes the Mean Square Error (MSE) of the predicted and pre-defined vector fields with proper reparameterization or simplification, which can be generalized as

\[=_{t,x_{0} p_{0},x_{1} p_{1}}[_{t}(x_{0 },x_{1};)]. \]

Hereinafter, we use these generalized notations for simplicity.

### Preference Dataset Construction

Building on the flow paths introduced in SS 3.1, we now delve into the details of constructing a preference dataset, which is pivotal for the application of DPO, as detailed in SS 3.3.

```
1:Input:\(N\), \(M\), \(_{ref}=\{(x_{i},c_{i})\}_{i=1}^{N}\), \(_{}\), \(d(,)\), \(\)
2:Output:\(_{gen}\), \(_{pos}\), \(check\)
3:Initialize:\(_{gen},_{pos},check[],[],[]\)
4:for\(i=1\) to \(N\)do
5:\(_{gen}[i],_{pos}[i][],[]\)
6:\(match\) False, \(j_{pos} 1\)
7:for\(j=1\) to \(M\)do
8: Generate\(_{ij} p(x|c_{i};_{})\)
9:\(_{gen}[i,j]_{ij}\)
10:if\(d(x_{i},_{ij})\)then
11:\(_{pos}[i,j_{pos}]_{ij}\)
12:\(match\) True, \(j_{pos} j_{pos}+1\)
13:endif
14:endfor
15:\(check[i] match\)
16:endfor
```

**Algorithm 1** Candidate Generation

Candidate GenerationAs shown in Algorithm 1, the construction of the preference dataset begins with the generation of multiple candidate structures for each sample in our reference dataset, \(_{ref}\). Leveraging the pre-trained flow-based generative model \(_{}\), we generate \(M\) candidate structures \(\{_{ij}\}_{j=1}^{M}\) for each sample \((x_{i},c_{i})\) via \(p(x|c_{i};_{})\), ensuring that each generated structure is contextually relevant and adheres to the geometric constraints discussed previously.

As each candidate is generated, we compute the distance between \(_{ij}\) and the original structure \(x_{i}\) using a predefined metric \(d(,)\). If this distance is less than or equal to a threshold \(\), the candidate is considered a close match and is added to \(_{pos}\), a subset of promising candidates. This step is crucial for efficiently filtering the generated data to retain only the most relevant candidates for DPO.

**Preference Pairs Construction** Subsequently, we construct \(K\) preference pairs \((x^{w}_{ik},x^{l}_{ik})\) for each sample \(i\) by Algorithm 2, where \(x^{w}_{ik}\) is preferred over \(x^{l}_{ik}\). This preference is determined based on their proximity to the original structure \(x_{i}\). Apart from sampling pairs from generated structures, we also use a ratio \(r\) to select the ground truth as the preferred sample. Moreover, if all generated structures for a sample are far from the original, the original structure \(x_{i}\) is always preferred. The other pairs are formed by selecting \(x^{w}_{ik}\) from the promising subset \(_{pos}\) and \(x^{l}_{ik}\) from the broader set \(_{gen}\). This process ensures that the pairs reflect a clear preference based on the closeness to the original structure, facilitating effective training through DPO, which is explored in the next section.

### Direct Preference Optimization

To align large language models with human preference, DPO  is proposed to replace the RLHF  training objective with directly maximizing the likelihood of the preference.  extends DPO to text-to-image generation task, adapting the DPO target to diffusion models. Given the preference pair \((x^{w},x^{l})\), DPO  designs the training objective as

\[_{}=-_{x^{w},x^{l}} }(x^{w})}{p_{}(x^{w})}-}(x^{l})}{p_{}(x^{l})}, \]

where \(p_{},p_{}\) are probabilities yielded by the fine-tuned model \(_{}\) and the pre-trained flow model, and \(\) is a hyperparameter to control the KL divergence of these two distributions.

It is nontrivial to efficiently acquire \(p(x)\) via iterative generative models. Inspired by , we uniformly discretize the time interval into \(T\) steps, where step \(i\) is located at \(t=i/T\). By formulating the probability from the path \(x_{0:T}\), Eq. (18) can be rewritten as

\[_{}=-_{x^{w},x^{l}} _{x_{1:T}^{w},x_{1:T}^{l}}}(x_{0:T}^{w })}{p_{}(x_{0:T}^{w})}-}(x_{0:T}^{l})}{p_{ }(x_{0:T}^{l})}. \]

To avoid costly sampling through the entire path, Jensen's inequality  is applied to bound Eq. (19) as

\[_{}-_{x^{w},x^{l},i}B }(x_{i-1}^{w}|x_{i}^{w})}{p_{}(x_{i-1}^{w}|x_ {i}^{w})}-}(x_{i-1}^{l}|x_{i}^{l})}{p_{}(x_{i -1}^{l}|x_{i}^{l})}, \]

where \(B= T\) servers as a hyperparameter. As directly sampling \(x_{i-1},x_{i}\) from an arbitrary intermediate step \(i\) is still unfeasible, we can estimate them via the accessible Gaussian paths \(p\) in Table 1 as

\[_{} =-_{x^{w},x^{l},i}B_{p(x_{i- 1}^{w}|x_{i,0}^{w}),p(x_{i-1}^{w}|x_{i,0}^{l})} }(x_{i-1}^{w}|x_{i}^{w})}{p_{}(x_{i-1}^{w}|x_{i}^{w})}-}(x_{i-1}^{l}|x_{i}^{l})}{p_{}(x_{i-1}^{l}|x_{i}^{l})}  \] \[=-_{x^{w},x^{l},i}B( x_{i}^{w};p,p_{})-(x_{i}^{w};p,p_{})-(x_ {i}^{l};p,p_{})+(x_{i}^{l};p,p_{}), \]

where \((x_{i}^{w};p,p_{})\) denotes \(D_{}p(x_{i-1}^{w}|x_{i,0}^{w})\|p_{}(x_{i-1}^{w}|x_{i}^{ w})\) and the same for \((x_{i}^{l};p,p_{})\). As \(p\) and \(p_{}\) are Gaussian distributions with the same noise scheduler, the KL divergence can be formulated as

\[(x_{i};p,p_{})=^{2}}(x_{i- 1}|x_{i,0})-_{}(x_{i-1}|x_{i})_{2}^{2}. \]

According to DDIM , if a time-dependent Gaussian path follows the form \(x_{i}(x_{i};k_{i}x_{0},_{i})\), we can further design \(p(x_{i-1}|x_{i,0})=x;(x_{i-1}|x_{i,0}),_{i-1|i}^{2} \). Given \(_{i-1|i}^{2}\), the mean can be formulated as

\[(x_{i-1}|x_{i,0})=}^{2}-_{i-1|i}^ {2}}x_{i}+k_{i-1}-}{_{i}}^{2}- _{i-1|i}^{2}}x_{0}. \]

Fortunately, all paths defined in Table 1 follows this form. And \(_{}(x_{i-1}|x_{i})\) can be parameterized similarly as Eq. (24), with estimating \(x_{0}\) via predicted vector field or denoising terms. Hence, we can approximate \((x_{i};p,p_{})\) by \(_{i}(x_{0},x_{1};)\). With sufficiently large \(T\), Eq. (22) can be changed into an applicable form as follows, which is our final training objective.

\[_{}=-_{x_{0,1}^{w},x_{0,1}^{l},t} B_{t}(x_{0}^{w},x_{1}^{w};_{})- _{t}(x_{0}^{w},x_{1}^{w};_{})\] \[-_{t}(x_{0}^{l},x_{1}^{l};_{})+_{t}(x_{0}^{l},x_{1}^{l};_{}), \]

## 4 Experiments

We validate our method on two distinct domains: antibody structure prediction (SS 4.1) and crystal structure prediction (SS 4.2).

### Antibody Structure Prediction

DatasetFollowing previous literature , we extract antibody structures from the SAbDab database  for training and utilize the manually curated test set from DiffAb , which contains 19 antibody-antigen complexes. We first derive all structures deposited before April 11th, 2024, and remove those with resolution above 4.0A or non-protein targets, resulting in 12,428 antibodies. Subsequently, we use mmseqs2  to cluster the antibodies based on 50% sequence identity for each CDR, and exclude those in the same clusters as the test set antibodies. The dataset is then split into training and validation sets at a 9:1 ratio based on the clusters.

MetricsWe employ the following metrics for evaluation. \(_{C_{}}\) measures the Root Mean Square Deviation of the generated alpha carbon coordinates with respect to the reference. \(_{bb}\) is the RMSD calculated on the four backbone atoms including \(C,C_{},N,O\). To better profile the generated distribution, for each antibody, we generate 20 structures and use two strategies to aggregate the results across different antibodies. Strategy **worst** select the worst generated structure per antibody according to RMSD and then average across different antibodies, while strategy **mean** averages the RMSD of 20 candidates first, and then across antibodies. Strategy worst measures the furthest deviation of the generated distribution compared to the reference, while strategy mean is commonly adopted in previous works . Results aggregated with **worst** are denoted as \(_{}\)-**w** and **bb-w**, while those with **mean** are denoted as \(_{}\) and \(\).

ResultsWe evaluate VP path (DiffAb)  and OT path  with the proposed FlowDPO on CDR structure prediction. Results in Table 2 illustrate that either using VP path or OT path, further training with DPO consistently enhances performance across different CDRs. Notably, on the most challenging part (_i.e._ CDR-H3), the DPO phase yields the most significant improvement. Metrics aggregated with strategy **worst** demonstrate noticeable gains, indicating effective supperssion of low-quality samples by the DPO phase, which we attribute to the objective of DPO in distinguishing the prefered samples. Such characteristics are favorable in practical applications where it requires blind selection of generated structures without prior knowledge of which structures might be more correct. We also depict the distributions of RMSD and examples of generated CDR-H3 structures in Figure 4. It shows that the blue curves, yielded by the original flow models, often exhibit a bimodal distribution. While the first peak at a lower RMSD indicating higher quality generations, the second peak at a higher RMSD suggests the models experience _hallucination_, confidently generating conformations that significantly deviate from the ground truth. DPO effectively suppresses this erroneous second peak, leading to an overall improvement in the quality of generated samples. On closer inspection, this correction also addresses physical invalidities, such as the twisted backbone seen in Figure 4.

    &  &  &  \\   & C\({}_{}\)-w & C\({}_{}\) & bb-w & bb & C\({}_{}\)-w & C\({}_{}\) & bb-w & bb & C\({}_{}\)-w & C\({}_{}\) & bb-w & bb \\  VP Path  & 2.71 & 2.00 & 2.56 & 2.06 & 1.11 & 0.95 & 1.08 & 0.96 & 1.32 & 0.99 & 1.39 & 1.08 \\ OT Path & 2.25 & 1.77 & 2.24 & 1.83 & 1.13 & 0.96 & 1.10 & 0.96 & 1.49 & 1.05 & 1.45 & 1.13 \\  VP Path + DPO & 2.47 & 1.91 & 2.31 & 1.95 & **1.09** & 0.94 & 1.07 & 0.94 & **1.22** & **0.94** & **1.30** & **1.01** \\ OT Path + DPO & **2.22** & **1.74** & **2.19** & **1.78** & **1.09** & **0.93** & **1.05** & **0.93** & 1.28 & 0.95 & 1.34 & 1.05 \\   &  &  &  \\   & C\({}_{}\)-w & C\({}_{}\) & bb-w & bb & C\({}_{}\)-w & C\({}_{}\) & bb-w & bb & C\({}_{}\)-w & C\({}_{}\) & bb-w & bb \\  VP Path  & 1.18 & 0.83 & 1.14 & 0.89 & 1.41 & 0.92 & 1.45 & 1.00 & 5.01 & 3.77 & 4.95 & 3.78 \\ OT Path & 1.31 & 0.89 & 1.26 & 0.94 & 1.69 & 1.06 & 1.60 & 1.13 & 4.81 & 3.66 & 4.83 & 3.70 \\  VP Path + DPO & **1.13** & **0.80** & **1.13** & **0.86** & **1.35** & **0.87** & **1.37** & **0.95** & 4.42 & 3.44 & 4.38 & 3.45 \\ OT Path + DPO & 1.23 & 0.83 & 1.19 & 0.89 & 1.46 & 0.95 & 1.41 & 1.02 & **4.28** & **3.32** & **4.23** & **3.32** \\   

Table 2: C\({}_{}\) and bb indicates RMSD calculated on C\({}_{}\) atoms and backbone atoms, repectively. C\({}_{}\)-w and bb-w averages the RMSDs of the worst generated conformations of each complex.

### Crystal Structure Prediction

**Dataset** We conduct the crystal structure prediction task on three datasets in line with previous works [33; 14]. **Perov-5** includes 18,928 perovskite crystals, each characterized by similar structures but varying compositions, and exactly 5 atoms per unit cell. **MP-20** comprises 45,231 materials from the Materials Project, featuring a wide range of compositions and structures, with each material containing no more than 20 atoms per unit cell. These materials predominantly represent crystals that have been synthesized experimentally. **MPTS-52** is an advanced version of MP-20, containing 40,476 structures with unit cells that include up to 52 atoms, presenting a more complex challenge. For Perov-5 and MP-20, we maintain the conventional 60-20-20 split for training, validation, and testing. For the MPTS-52 dataset, we use a chronological split, assigning 27,380 crystals for training, 5,000 for validation, and 8,096 for testing.

**Metrics** For inference, we generate one structure given each composition. The predicted sample is then matched with the ground truth via the StructureMatcher class in pymatgen  with thresholds stol=0.5, angle_tol=10, ltol=0.3 as applied in previous works [33; 14]. We use **Match Rate (MR)** as the proportion of matched structures among the testing set, and the **RMSD** is averaged over the matched pairs, and normalized by \(\) where \(V\) is the volume of the unit cell.

**Results** We compare the results with two generative baselines **P-cG-SchNet** and **CDVAE**. The results are shown in Table 3, where we explore three combinations of paths for jointly generating the lattice and the fractional coordinates: VP+VE, OT+OT, and OT+VE. Notably, the VP+VE path is previously developed by DiffCSP . We find that the OT path is more effective for lattice generation, while the VE path provides more accurate predictions of atomic coordinates within the cell. Overall, the OT+VE combination generally delivers the best performance. Furthermore, DPO consistently enhances the performance of the model trained on each combination, demonstrating its capability to refine the predictions to a more precise alignment with experimental structures. We additionally visualize the RMSD distribution of predicted structures from different Gaussian paths. Results in Figure 5 reveal a similar pattern to Figure 4, demonstrating that DPO reduces the probability of low-quality generations.

## 5 Conclusion

In this work, we propose FlowDPO, a novel framework for 3D structure prediction that integrates flow-based generative models with Direct Preference Optimization. We achieve 3D structure prediction via flow matching models with various probability paths, and generalize the DPO training objective to arbitrary Gaussian paths. To refine the model via DPO, we generate multiple candidate structures and construct the preference dataset by aligning with ground truth. The results demonstrate substantial

Figure 4: Examples of generated CDR-H3 structures and the distribution of RMSD\({}_{C_{}}\) for different antigen-antibody complexes and different probability paths. The visualized samples are the ones with the lowest RMSD of all the generated counterparts for the corresponding complexes. In addition to driving the distribution towards lower RMSD, it is also observed that the DPO phase tends to rectify the physical invalidity (_e.g._ twisted backbone in the above examples) in the generated samples.

improvements in prediction accuracy for both antibody and crystal structures, highlighting the effectiveness and versatility of FlowDPO in the field of 3D structure prediction.