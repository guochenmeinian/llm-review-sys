# Is Multiple Object Tracking a Matter of Specialization?

Gianluca Mancusi  Mattia Bernardi  Aniello Panariello  Angelo Porrello

Rita Cucchiara  Simone Calderara

###### Abstract

End-to-end transformer-based trackers have achieved remarkable performance on most human-related datasets. However, training these trackers in heterogeneous scenarios poses significant challenges, including negative interference - where the model learns conflicting scene-specific parameters - and limited domain generalization, which often necessitates expensive fine-tuning to adapt the models to new domains. In response to these challenges, we introduce Parameter-efficient Scenario-specific Tracking Architecture (PASTA), a novel framework that combines Parameter-Efficient Fine-Tuning (PEFT) and Modular Deep Learning (MDL). Specifically, we define key scenario attributes (_e.g._, camera-viewpoint, lighting condition) and train specialized PEFT modules for each attribute. These expert modules are combined in parameter space, enabling systematic generalization to new domains without increasing inference time. Extensive experiments on MOT-Synth, along with zero-shot evaluations on MOT17 and PersonPath22 demonstrate that a neural tracker built from carefully selected modules surpasses its monolithic counterpart. We release models and code.

## 1 Introduction

Video Surveillance is essential for enhancing security, supporting law enforcement, improving safety, and increasing operational efficiency across various sectors. In this respect, Multiple Object Tracking (MOT) is a widely studied topic due to its inherent complexity. Nowadays, MOT is commonly tackled with two main paradigms: tracking-by-detection (TbD)  or query-based tracking  (_i.e._, tracking-by-attention). Although tracking-by-detection methods have proven effective across multiple datasets, their performance struggles to scale on larger datasets due to the non-differentiable mechanism used for linking new detections to existing tracks. To this end, query-based methods are being employed to unify the detection and association phase.

Nevertheless, training such end-to-end transformer-based methods presents significant challenges, as they tend to overfit specific scenario settings  (_e.g._, camera viewpoint, indoor _vs._ outdoor environments), require vast amounts of data , and incur substantial computational costs. Moreover, these methods degrade under domain shifts, struggling to outperform traditional TbD methods.

In light of these challenges, we propose a novel framework, Parameter-efficient Scenario-specific Tracking Architecture (PASTA), aimed at reducing the computational costs and enhancing the transfer capabilities of such models. Leveraging Parameter Efficient Fine-Tuning (PEFT) techniques  can significantly decrease computational expenses and training time, starting with a frozen backbone pre-trained on synthetic data. However, the model may still experience _negative interference_, a phenomenon for which training on multiple tasks (or scenarios) causes the model to learn task-specific parameters that may conflict. For instance, if the model learns parameters tailored for an indoor sports activity, it could detrimentally affect its performance on a novel outdoor scene depicting people walking. To this end, inspired by Modular Deep Learning (MDL) , we employ a lightweight expert module for each attribute, learn them separately, and finally compose them efficiently . This approach - depicted in Fig. 1 - is akin to a chef preparing a pasta dish. Each ingredient (_i.e._, module) is prepared individually to preserve its unique flavor and then combined harmoniously to create a balanced dish. Moreover, as pasta must be perfectly _al dente_ to serve as the ideal base for various sauces, the pre-trained backbone should be robust and well-tuned to serve as the foundation for the modules. These modules must be combined effectively to ensure the model performs well across diverse scenarios. Conversely, the result will be sub-optimal if incompatible modules are mixed - analogous to combining ingredients that do not complement each other. Indeed, combining contrasting modules can lead to ineffective handling of diverse tasks.

Notably, such a modular framework brings two advantages: it avoids negative interference and enhances generalization by leveraging domain-specific knowledge. Firstly, starting from a pre-trained backbone, we train each module independently to prevent parameter conflicts, ensuring that gradient updates are confined to the relevant module for the specific scenario. This assures that parameters learned for one attribute do not negatively impact the performance of another. Secondly, the modular approach allows us to exploit domain knowledge fully, even when encountering a novel attribute combination. Indeed, as shown in Sec. 5.5, our approach is effective even in a zero-shot setting (_i.e._, without further fine-tuning on the target dataset). Moreover, the selection of the modules may be done automatically or in a more realistic production environment by video surveillance operators.

To evaluate our approach, we conduct extensive experiments on the synthetic MOTSynth  and the real-world MOT17  and PersonPath22  datasets. The results show that PASTA can effectively leverage the knowledge learned by the modules to improve tracking performance on both the source dataset and in zero-shot scenarios. To summarize, we highlight the following main contributions:

* We propose PASTA, a novel framework for Multiple Object Tracking built on Modular Deep Learning, enabling the fine-tuning of query-based trackers with PEFT techniques.
* By incorporating expert modules, we improve domain transfer and prevent negative interference while fine-tuning MOT models.
* Comprehensive evaluation confirms the validity of our approach and its effectiveness in zero-shot tracking scenarios.

## 2 Related works

**Multiple Object Tracking.** The most widely adopted paradigm for Multiple Object Tracking (MOT) is _tracking-by-detection_ (TbD) . First, an object detector (_e.g._, YOLOX ) localizes objects in the current frame. Next, the association step matches detections to tracks from the previous frame by solving a minimum-cost bipartite matching problem, with the association cost defined in various forms (_e.g._, IoU , GIoU , or geometrical cues ). This pairing typically occurs immediately after propagating the previous tracks to the current frame using a motion model (_e.g._, Kalman Filter ). Notably, methods following such paradigm have succeeded on

Figure 1: Given a scene, we select the modules corresponding to its attributes, such as lighting and indoor/outdoor. These modules are composed and then deployed, yielding a specialized model.

complex human-related MOT benchmarks [8; 9; 47; 44]. In TbD, the detection and data-association steps are equally crucial to accurately localizing and tracking objects. Recent works [65; 1] have attempted to unify these steps; however, progress toward a fully unified algorithm was constrained by a significant limitation - the data association process (_e.g._, the Hungarian algorithm ) is inherently non-differentiable. An initial effort was made by Xu _et al._ that proposed a differentiable version of the Hungarian algorithm, later advanced by end-to-end transformer-based trackers [31; 58; 63; 56; 13].

However, transformer-based trackers (also known as tracking-by-attention) require large amounts of data to achieve decent generalization capabilities [58; 31]. Due to the data scarcity in MOT, these models often overfit to the specific domain they were trained on, which hampers their ability to generalize to different domains [17; 21; 37].

**Modular Deep Learning (MDL).** Considering recent trends in the field of deep learning, state-of-the-art models have become increasingly larger. Consequentially, fine-tuning these models has become expensive; concurrently, they still struggle with tasks like symbolic reasoning and temporal understanding . Recent learning paradigms based on _Modular Deep Learning_ (MDL)  can address these challenges by disentangling core pre-training knowledge from domain-specific capabilities. By applying modularity principles, deep models can be easily edited, allowing for the seamless integration of new capabilities and the selective removal of existing ones [26; 36].

Specifically, lightweight computation functions named _modules_ are employed to adapt a pre-trained neural network. To do so, several fine-tuning techniques could be used to realize these modules, such as LoRA , (IA)\({}^{3}\), and SSF . These multiple modules can be learned on different tasks such that they can specialize in different concepts . At inference time, not all modules have to be active at the same time. Instead, they can be selectively utilized as needed, either based on prior knowledge of the domain or dynamically in response to the current input. To establish which modules to activate, it is common practice to rely on a _routing function_, which can be either learned or fixed. Finally, the outputs of the selected modules are combined using an _aggregation function_. To minimize inference costs, this process is usually performed in the parameter space rather than the output space, an activity often referred to as _model merging_. Specifically, a single forward pass is performed using weights generated by a linear combination of those selected by the routing function.

**Domain adaptation and open-vocabulary approaches in MOT.** Currently, domain adaptation techniques have only been applied to tracking-by-detection methods, with GHOST  and DARTH  serving as notable examples. In particular, GHOST adapts the visual encoder employed to feed the appearance model by updating the sufficient statistics of the Batch Normalization layers during inference. In contrast, our approach regards tracking-by-attention approaches and adapts the entire network. Moreover, DARTH employs test-time adaptation (TTA)  and Knowledge Distillation, requiring multiple forward passes and entire sequences, making it computationally heavy and less practical for real-time use. In contrast, our method is entirely online and requires only basic target scene attributes, with no further training during deployment.

Recent advances in zero-shot tracking have focused on _open-vocabulary tracking_, where the model can track novel object categories by prompting it with the corresponding textual representation. In this respect, methods like OVTrack  and Z-GMOT  leverage CLIP  and language-based pre-training, while OVTracktor  extends tracking to any category. Our method does not use open-vocabulary models but emphasizes domain knowledge transfer in end-to-end trackers.

## 3 Preliminaries

**Efficient fine-tuning.** Given the substantial size of recent vision backbones, often consisting of hundreds of millions of parameters, adapting them to new scenarios is computationally expensive, both in terms of time and memory requirements. To tackle the above problems, Parameter Efficient Fine-Tuning (PEFT) started to take place in recent literature. Among these methods, Low-Rank Adaptation (LoRA)  excels at such purpose. Specifically, LoRA adapts a pre-trained weight matrix \(_{0}^{d k}\), with \(d\) and \(k\) being the dimensions of the matrix, by leveraging a low-rank decomposition \(_{0}+=_{0}+BA\), where \(B^{d r},A^{r k}\), and \(r(d,k)\). During training, \(_{0}\) is kept frozen, while the smaller \(A\) and \(B\) matrices are instead trainable, making the process highly efficient. The forward pass becomes \(h=_{0}x+BAx\), where \(x\) are the input features.

#### Query-based Multiple Object Tracking.

The underlying backbone of our transformer-based tracker follows the structure of . In a nutshell, such a query-based model forces each query to recall the same instance across different frames. Specifically, we leverage an end-to-end trainable tracker built upon the Deformable DETR  framework conditioned by the image features extracted with a convolutional backbone (_i.e._, ResNet ). Following , we further condition the DETR decoder with a set of detections from an external detector network and a shared learnable query.

At time \(t=0\), new proposals are generated from the objects detected in the scene. These proposals are then updated through self-attention and interact with image features via the deformable attention layer. The final prediction output is the summation of the initial anchors and the predicted offsets. For subsequent frames (\(t>0\)), track queries generated from the previous frame are concatenated with learnable proposal queries of the current frame. Moreover, previous predictions are integrated with current proposals to establish new anchors for the incoming frame. We refer the reader to the original paper  for further details. It is noted that the flexibility of this architecture allows for the seamless integration of techniques based on modularity.

## 4 Method

We herein present PASTA, depicted in Fig. 2, a novel approach to Multiple Object Tracking that leverages PEFT modules to enable attribute-specific module specialization and reuse. This approach allows for the dynamic configuration of an end-to-end tracker by selecting the appropriate modules for each input scene, fully leveraging heterogeneous pre-training while avoiding negative transfer.

#### Attribute-based modularity.

We devise a set of learnable modules to fine-tune each layer of our query-based tracker. Each module is related to an **attribute**: as shown in Fig. 3, we define \(N=5\) attributes, namely _lighting_, _viewpoint_, _occupancy_, _location_, and _camera motion_, and provide a tailored module for each discrete value these attributes take (see Sec. 5.3 for details). For instance, the _location_ attribute has indoor and outdoor modules. At inference time, prior knowledge about the input scene is used to determine the appropriate value for each attribute, which in turn selects the corresponding modules from the "inventory", denoted as \(M\).

Since the base model  relies on heterogeneous layers - namely, convolutional (_e.g._, ResNet) and attention-based blocks (_e.g._, Deformable DETR) - we employ two different strategies to fine-tune the modules. Specifically, after each convolutional layer of the ResNet backbone, we apply a strategy

Figure 2: Overview of our modular architecture. A domain expert selects PEFT modules based on sequence attributes such as lighting and camera movement. These selected modules are then composed and applied to each model layer, adapting the backbone and encoder-decoder architecture.

that learns channel-wise scale and shift parameters; for each layer of Deformable DETR, instead, we employ LoRA-based fine-tuning at each linear layer. In formal terms, considering each convolutional layer of the ResNet backbone, we deploy \(|M|\) pairs \(\{_{m},_{m}\}_{m=1}^{|M|}\) of learnable vectors \(,^{C}\), where \(C\) is the number of the output channels. For each linear layer \(l\) of the encoder-decoder structure underlying Deformable DETR, we devise \(|M|\) pairs \(\{A_{m},B_{m}\}_{m=1}^{|M|}\) of learnable LoRA matrices.

During training, we start with the pre-trained weights and integrate all the modules while keeping the original parameters frozen. To prevent negative interference, we optimize each module _independently_, randomly sampling one attribute at a time and updating only the corresponding module at each training iteration. By the end of the training process, we obtain a set of specialized parameters (_experts_), which can be seamlessly merged during inference to improve overall tracking performance.

**Routing through Domain Expert.** During inference, two essential steps are required to exploit the learned modules: _routing_ and _aggregation_. With multiple modules available from the inventory \(M\), a routing strategy is required to determine the modules that should be active. To make this selection, we draw on what is known in the literature as _expert knowledge_ (or "**Domain Expert**" in Fig. 2). In real-world applications such as video analytics, the expertise guiding the selection can come from a video surveillance operator or human analyst, who configures the appropriate modules to reflect domain- and scene-specific settings, such as camera perspective, lighting conditions, and other critical details. This approach allows users to optimize the tracking module for their unique contexts without extensive retraining. Additionally, the modular nature of the system enables easy integration of new modules to address emerging attributes or scenarios.

Relying on Domain Expert to select attributes is a grounded practice in real-world applications. For instance, the camera's mounting perspective and whether the scene is indoors or outdoors are typically known factors in fixed-camera scenarios. Additionally, automatic approaches can be envisioned to minimize human intervention further. For example, lighting conditions can be inferred by analyzing brightness levels, and a detector can count objects of interest in the scene, classifying crowd density.

**Modules composition.** In the final step, we aggregate the selected modules ("Modules Composition" in Fig. 2) and incorporate the result into the pre-trained tracker to create an expert model. Since these modules have been obtained by fine-tuning from \(_{0}\), each module \(^{}\) corresponds to a specific displacement \(^{}=^{}-_{0}\) in parameter space relative to the initial pre-training parameters \(_{0}\). This displacement is known as the _task vector_. The final composed model \(f(;_{c})\) is defined as:

\[f(;_{c})_{c}=_{0}+ _{i=1}^{N}_{i}_{i},_{i}_{i}=1_{i}  M. \]

When \(_{i}=\), the formula simplifies to the average of the task vectors corresponding to each attribute. We employ this straightforward strategy for \(_{i}\), giving equal weight to all attributes. However, considering the task vector \(_{i}\) associated with the \(i\)-th attribute, we employ a more sophisticated approach. If there are no domain shifts during inference (_i.e._, both training and testing occur on the same dataset, such as MOTSynth), the task vector \(_{i}\) is simply set to the displacement \(^{}\) produced by the expert module selected by the Domain Expert. In contrast, when domain shifts are present (_e.g._, training on MOTSynth and testing on MOT17), we adopt a soft strategy that considers _all_ the modules in the inventory associated with the relevant attribute. In doing so, we follow the insights from , where the authors demonstrated that scenarios with shifting tasks benefit from richer representations than those derived from a single optimization episode.

Specifically, given the \(i\)-th attribute, let \(R(i)\) be the set of its modules. We recall that each attribute admits multiple discrete values (_e.g._, \(R()=\) {"low", "medium", "high"}), and different

Figure 3: Examples of surveillance scenes and their corresponding attributes used by PASTA.

attributes may have different cardinalities (_e.g._, \(|R()|=3\) and \(|R()|=2\), as detailed in Sec. 5.3). Building on this, we employ soft routing to create the corresponding task vector, assigning the largest portion of the cake, _e.g._\(=0.80\), to the module selected by the Domain Expert. The remaining modules are weighted by \((1-)/(|R(i)|-1)\), ensuring that the total sum equals 1. For example, considering those layers fine-tuned with the LoRA, the corresponding task vector is computed as:

\[_{i}=_{m R(i)}_{m}B_{m}A_{m}, _{m}=&,\\ &. \]

Note that when \(=1\), the soft strategy becomes hard, meaning that only the module selected by Domain Expert is utilized. By applying the formula above to all attributes, we obtain \(N\) task vectors, which we aggregate following Eq. (1).

Similarly, we apply channel-wise scale and shift  operations to adapt each backbone layer. Formally, given the output \(F\) of a convolutional layer, the \(i\)-th module applies a scale & shift operation to obtain the edited \(_{i}\), such that \(_{i}=_{i} F+_{i}\) with \(\) denoting the Hadamard product. At inference time, we combine the output of different scale & shift modules by noting that

\[=_{i=1}^{N}_{i}(_{i} F+_{i})= _{i=1}^{N}_{i}(_{i} F)+_{i}_{i}=( _{i=1}^{N}_{i}_{i}) F+_{i=1}^{N} _{i}_{i}, \]

which means that parametrizing the scale & shift layer with a simple weighted average effectively results in averaging the outputs of the corresponding individual layers. The formula above applies to the in-domain setting but can be easily generalized to the soft routing scheme outlined by Eq. (2). Eventually, as discussed in , the scale & shift layer can be absorbed into the previous projection layer, thus ensuring that the inference process incurs no additional computational costs. The same re-parametrization trick can be employed to extract the task vector underlying scale & shift fine-tuning (refer to appendix A for additional notes).

## 5 Experiments

### Datasets

**MOTSynth** is a large synthetic dataset for pedestrian detection and tracking in urban scenarios, generated using a photorealistic video game. It comprises \(764\) full HD videos, each 1800 frames long, showcasing various attributes. In our experiments, following , we reduced the test sequences to \(600\) frames each and further split the training set to extract \(48\) validation sequences, shortened to \(150\) frames, for validation during training.

**PersonPath22** is a large-scale pedestrian dataset consisting of \(236\) real-world videos featuring longer occlusions and more crowded scenes. It is divided into \(138\) training videos and \(98\) test videos.

**MOT17** is a well-known benchmark, containing \(7\) sequences for training and \(7\) for testing, with different image resolutions, featuring crowded street scenarios with both static and moving cameras.

### Experimental setting

We evaluate our proposed PASTA on both **in-domain** and **out-of-domain** scenarios. For the in-domain evaluation, we train and test PASTA on the MOTSynth synthetic dataset (Sec. 5.4) using expert modules in a domain-specific context. As a baseline, we train  on MOTSynth without using modules, referring to this model as MOTRv2-MS. For the out-of-domain evaluation, we conduct a synth-to-real zero-shot experiment on MOT17 and PersonPath22 (Sec. 5.5). Starting from training on MOTSynth, we test PASTA on these datasets without additional training, showcasing its ability to generalize under non-identically distributed domains. Finally, we present a series of ablation studies in Sec. 6 to take a closer look at the effectiveness of our method.

**Competing trackers and metrics.** In addition, we report the performance of other notable methods, including strong tracking-by-detection baselines such as ByteTrack  and OC-Sort . We also include evaluations of query-based trackers, such as TrackFormer  and MOTRv2  (see MOTRv2-MS). To compare their performance, we employ five metrics, ordered from detection to association, as recommended by . These metrics are DetA , MOTA , HOTA , IDF1 , and AssA . For the PersonPath22 dataset, we use their official metrics, MOTA and IDF1, supplemented by FP (false positives), FN (false negatives), and IDSW (identity switches).

### Implementation details

We initialize our models using the pre-trained weights from DanceTrack , as provided by the authors of . We employ YOLOX  as the auxiliary detector, exploiting weights from ByteTrack . To provide a shared initialization for both PASTA and MOTRv2-MS training, we train a bootstrap model starting from the DanceTrack pre-train for 28k iterations on the MOTSynth training set. This bootstrap initialization uses half of the original training sequences from MOTSynth to align our model with the scenarios represented in the dataset. The learning rates are set to \(5 10^{-5}\) for the transformer and \(1 10^{-6}\) for the visual backbone.

In the second phase, we deploy the PEFT modules to fine-tune the bootstrap model. By excluding half the sequences during the bootstrap, we make sure that the modules can still learn valuable features. To ensure a fair comparison, we train each module for a similar number of iterations as MOTRv2-MS, with approximately 17k iterations. Regarding the encoder-decoder model, we apply our modularization strategy to every linear layer except those with output dimension less than \(128\). For the LoRA hyperparameters, we use \(r=16\), a weight decay of \(0.1\), and a learning rate of \(3 10^{-4}\). The scale & shift layers employ a learning rate of \(1 10^{-5}\) and a weight decay of \(1 10^{-4}\). The training is performed on a single RTX 4080 GPU with a batch size of \(1\) for both phases. Due to the small batch size, we accumulate gradients over four backward steps before performing an optimizer step. Each module is trained independently on the entire MOTSynth training set. With 12 modules, our model has approximately \(15\) million trainable parameters.

Attributes.We employ five key attributes to realize our modular architecture: lighting, camera viewpoint, people occupancy, location, and camera motion. For **lighting**, we specialize modules for _good_ and _bad_ lighting conditions. To do so, we threshold the brightness value V of the HSV representation at \(70\). The **viewpoint** attribute includes modules for _high_, _medium_, and _low_ camera angles. We manually annotate this attribute as follows: _i)_ scenes where the camera is parallel to the ground at or below pedestrian head level are labeled as "low-level"; _ii)_ "high-level" viewpoints include vertical perspectives or scenes where the camera is positioned very high or far from people; and _iii)_ "medium-level" includes all other camera angles. For **occupancy**, we design modules that reflect the crowd density within the scene: _low_ (up to 10 people), _medium_ (10 to 40 people), and _high_ (more than 40 people), based on the count of detections with a confidence score above 0.2. The **location** attribute differentiates between _indoor_ and _outdoor_ settings. Lastly, the **motion** attribute comprises modules for both _moving_ and _static_ cameras, enabling the model to adapt to different camera movement scenarios. Further details on dataset statistics are provided in appendix C.

### Performance in the in-domain setting

To assess the impact of the negative interference, we conduct several experiments on MOTSynth (see Tab. 1). Given the wide variety of scenarios in such a synthetic dataset, one can appreciate the advantages of using specialized modules. Indeed, integrating our modules resulted in an overall improvement w.r.t. its fine-tuning counterpart (MOTRv2-MS). Specifically, we observe an improvement over the association metrics (AssA, IDF1) and the HOTA and MOTA metrics. These enhancements suggest the benefits of our approach in reducing negative interference during training. By assigning each module a specific role tailored to particular scenario settings, we achieve improved training stability through a deterministic selection process guided by a domain expert.

  & \(||\) & HOTA\(\) & IDF1\(\) & MOTA\(\) & DetA\(\) & AssA\(\) \\  SORT  & - & 46.0 & 55.7 & 50.9 & 49.9 & 42.8 \\ ByteTrack  & - & 45.7 & 56.4 & 61.8 & 50.1 & 41.9 \\ OCSort  & - & 46.9 & 56.8 & 59.1 & 48.7 & 45.6 \\ TrackFormer  & 44M & 41.3 & 49.9 & 47.7 & 44.4 & 40.6 \\ MOTRv2-MS & 42M & 52.4 & 56.6 & 61.9 & **56.4** & 49.0 \\ PASTA (_Ours_) & 15M & **53.0** & **57.6** & **62.0** & 56.2 & **50.4** \\  

Table 1: Evaluation on MOTSynth test set. \(||\) is the number of trainable parameters.

### Performance in the out-of-domain setting

By designing distinct modules for various input conditions, we can effectively select the appropriate modules to handle distribution shifts, such as transitions to a new domain. We assess the benefits of this ability using synthetic data for training, and then evaluate on new, unseen datasets without any additional re-training (_zero-shot_). To do this, we start with our model trained on MOTSynth as described in Sec. 5.4 and evaluate it on MOT17 (Tab. 2) and PersonPath22 (Tab. 3). While these datasets share similarities in the attributes we employed, we emphasize that the source dataset is synthetic and the targets are real-world, resulting in a significant shift.

The results reported in Tab. 2 and 3 show an improvement over the baseline (_i.e._, MOTRv2-MS), with +1.4 in HOTA and +1.9 in IDF1 in zero-shot MOT17, and +1.7 in MOTA and +0.7 in IDF1 in PersonPath22. Our approach demonstrates better generalization capabilities, helping close the gap with fully-trained methods while less computationally demanding. These results indicate that modularity enhances performance within the source dataset and improves domain generalization, leading to more reliable and versatile approach for tracking. Furthermore, other than reporting the results with the standard module selection (considering only the modules present in the scenes, \(=1\)), we also experiment with the weighted aggregation of all modules (\(=0.8\)) (detailed in Sec. 4). Interestingly, while the standard strategy shows improvements, the weighted aggregation strategy yields better performance. This suggests that richer representations, obtained by including multiple modules per attribute, are more effective for zero-shot scenarios than a single-module approach .

  & HOTA\(\) & IDF1\(\) & MOTA\(\) & DetA\(\) & AssA\(\) \\   \\  SORT  & 64.3 & 73.1 & 70.9 & 63.3 & 66.1 \\ OC-SORT  & 66.4 & 77.8 & 74.5 & 64.1 & 69.1 \\ TrackFormer  & – & 74.4 & 71.3 & – & – \\ ByteTrack  & 67.9 & 79.3 & 76.6 & 66.6 & 69.7 \\ MOTRv2  & 66.8 & 78.9 & 73.2 & 62.5 & 71.4 \\   \\  TrackFormer  & 51.0 & 63.9 & 58.7 & 51.8 & 61.2 \\ MOTRv2-MS & 62.6 & 73.0 & 67.6 & 60.3 & 65.5 \\ PASTA (\(=1\)) & 63.7 & 74.1 & 67.9 & 60.3 & 67.9 \\ PASTA (\(=0.8\)) & **64.0** & **74.9** & **68.1** & **60.4** & **68.3** \\  

Table 2: Zero-shot evaluation on MOT17. PASTA is evaluated in zero-shot by selecting the best attributes on the source dataset.

  & MOTA\(\) & IDF1\(\) & FP\(\) & FN\(\) & IDSW\(\) \\   \\  CenterTrack  & 59.3 & 46.4 & 24 340 & 71 550 & 10 319 \\ SiamMOT  & 67.5 & 53.7 & 13 217 & 62 543 & 8942 \\ FairMOT  & 61.8 & 61.1 & 14 540 & 80 034 & 5095 \\ IDFree  & 68.6 & 63.1 & 9218 & 66 573 & 6148 \\ TrackFormer  & 69.7 & 57.1 & 23 138 & 47 303 & 8633 \\ ByteTrack  & 75.4 & 66.8 & 17 214 & 40 902 & 5931 \\   \\  TrackFormer  & 39.2 & 43.3 & 21 402 & 126 082 & 10023 \\ MOTRv2-MS & 48.3 & 53.1 & 28 483 & **98 007** & 7154 \\ PASTA (\(=1\)) & 49.7 & 53.7 & 18 211 & 105 611 & 6321 \\ PASTA (\(=0.8\)) & **50.0** & **53.8** & **18 038** & 105 454 & **6037** \\  

Table 3: Evaluation on PersonPath22 test set. PASTA is evaluated in zero-shot by selecting the best attributes on the source dataset.

Evaluating zero-shot real-to-real transfer.In Tab. 4, we present an additional experiment to evaluate the performance of PASTA in a zero-shot setting, this time using a realistic dataset as the source, rather than a synthetic one. For comparison, we train MOTRv2 on the MOT17 dataset and assess its performance on PersonPath22. Our approach showcases superior results compared to the fine-tuned MOTRv2, highlighting that leveraging modules enhances the model, with improved generalization capabilities in new and real-world domains.

## 6 Ablation studies

In Tab. 5, we evaluate the effect of various routing and aggregation strategies in both the in-domain setting (MOTSynth, left side of Tab. 5) and the zero-shot setting (MOT17, right side of Tab. 5). In the in-domain scenario, the results show that averaging the modules selected by the Domain Expert, specifically using Mean avg. (\(=1.0\)), is the most effective strategy. We also experimented with summation, as proposed by , but this method produced bad results, which we impute to the alteration of weight magnitudes when summing multiple modules. Another noteworthy approach is the _weighted avg._, described in Sec. 4, which incorporates all modules, including those not selected.

While using only the _selected modules_ is the optimal strategy in the in-domain scenario, for the zero-shot case (MOT17), incorporating knowledge from the non-selected modules -- specifically, using Weighted avg. (\(=0.8\)) - enhances tracking performance. This pattern is also consistent when the domain shift involves evaluation on the PersonPath22 dataset (see appendix E).

Module selection.Should we select only the modules representing the current scenario, as determined by the Domain Expert approach, or would performance improve by incorporating all available modules? In Tab. 5, we investigate this matter by comparing these two approaches. To provide a more comprehensive perspective, we also evaluate a strategy that, in stark contrast to the Domain Expert, selects the _opposite modules_ (_e.g._, selecting the outdoor and poor lighting modules when presented with an indoor, well-lit scene). The lowest performance is observed when using opposite modules, indicating that using the proper modules provides valuable information about the current scene. Interestingly, the model still performs relatively well despite using opposite attributes, likely due to contributions from other modules whose general knowledge of the domain sustains overall performance. This suggests that modules can assist one another in solving tasks. Moreover, reduced

  & MOTA\(\) & IDF1\(\) & FP\(\) & FN\(\) & IDSW\(\) \\   _fully-trained_ & & & & & \\  TrackFormer  & 69.7 & 57.1 & 23 138 & 47 303 & 8633 \\ ByteTrack  & 75.4 & 66.8 & 17 214 & 40 902 & 5931 \\  _zero-shot_ & & & & & \\  MOTRv2-MS & 43.9 & 51.5 & 8304 & 119 391 & 5342 \\ PASTA & **46.1** & **54.6** & **7895** & **114 620** & **4702** \\   

Table 4: Zero-shot evaluation of PASTA trained on MOT17 and tested on PersonPath22. PASTA is evaluated in zero-shot by selecting the best attributes on the source dataset.

 MOTSynth (_val_) & HOTA\(\) & IDF1\(\) & MOTA\(\) \\   \\  Sum (_only selected_) & 0.65 & 0.44 & -0.69 \\ Weighted avg. (\(=0.8\)) & 59.9 & 66.8 & 59.6 \\ Mean avg. (\(=1.0\)) & **60.1** & **67.2** & **59.9** \\   \\  Opposite modules & 59.2 & 66.5 & 58.7 \\ All modules & 59.8 & 67.0 & 59.4 \\ Domain Expert & **60.1** & **67.2** & **59.9** \\    
 MOT17 (_val_) & HOTA\(\) & IDF1\(\) & MOTA\(\) \\   \\  Sum (only selected) & 0.58 & 0.41 & -0.03 \\ Weighted avg. (\(=0.8\)) & **64.0** & **74.9** & **68.1** \\ Mean avg. (\(=1.0\)) & 63.7 & 74.1 & 67.9 \\   \\  Opposite modules & 62.9 & 73.9 & 67.1 \\ All modules & 63.1 & **74.1** & 67.7 \\ Domain Expert & **63.7** & **74.1** & **67.9** \\   

Table 5: Ablation study on different module aggregation and selection strategies. (**Left**) MOTSynth validation, (**Right**) Zero-shot on MOT17 validation. The strategy we select is highlighted in yellow.

negative interference - achieved by training each module separately - prevents the modules from relying on each other and allows them to make unique contributions independently.

Furthermore, in Fig. 4, we illustrate how the incremental addition of specialized modules improves IDF1 and HOTA metrics, showcasing that greater specialization of the modules gradually enhances overall performance. For a more detailed analysis, in Tab. 6, we select the opposite modules instead of the correct one for each attribute. Although the metrics are further reduced, the model performs well due to its robust pre-training, as indicated by the _no modules_ baseline shown in the table.

Block-wise analysis.In our approach, attribute-related modules are applied to edit the entire network. However, users may opt to edit selectively specific parts of the architecture, thereby identifying which components are most critical. In Tab. 7, we conduct an ablation study by excluding our modules from being applied to varying components of the architecture. The results indicate that not applying task vectors to the decoder significantly degrades detection and association metrics. We believe that this degradation can be explained by considering the crucial role of the decoder. The decoder must indeed gather information from detection, tracking, and proposal queries while simultaneously integrating visual information from the encoder. Consequently, not adapting the decoder prevents the architecture from effectively leveraging queries and visual cues. The encoder also contributes substantially, though to a lesser extent than the decoder, as it primarily refines and contextualizes visual features from the backbone. Finally, the backbone shows the smallest contribution.

## 7 Conclusions

In this work, we introduce PASTA, a novel framework that enhances domain generalization in tracking-by-query methods for Multiple Object Tracking. Our approach features a modular structure with dedicated modules tailored to different attributes of real-world scenes. These modules utilize Parameter-Efficient Fine-Tuning techniques, enabling the integration of scene-specific parameters while minimizing computational load. Comprehensive experiments demonstrate that domain-specialized modules significantly bolster robustness, allowing effective adaptation across domains without extensive retraining. PASTA further enables camera operators to configure the optimal module for each unique scenario, ensuring precise adaptation to diverse real-world settings.

  & HOTA\(\) & IDF1\(\) & MOTA\(\) \\  No modules & 58.2 & 64.9 & 55.6 \\ Opposite: only lighting & 58.9 & 66.2 & 57.8 \\ Opposite: only viewpoint & 58.8 & 65.7 & 57.0 \\ Opposite: only occupancy & 58.6 & 66.4 & 59.2 \\ Opposite: only location & 58.9 & 66.0 & 58.4 \\ Opposite: only camera & 58.5 & 65.6 & 58.4 \\ Average opposite & 58.7 & 66.0 & 58.2 \\  Correct modules & **60.1** & **67.2** & **59.9** \\  

Table 6: Opposite modules selection.

Figure 4: IDF1 and MOTA when adding new attributes on MOTSynth.

 Fine-tuning applies on & HOTA\(\) & IDF1\(\) & MOTA\(\) & DetA\(\) & AssA\(\) \\  none & 52.4 & 56.6 & 61.9 & **56.4** & 49.0 \\ all except the decoder & 51.5 & 56.0 & 58.9 & 53.6 & 49.8 \\ all except the encoder & 52.4 & 56.9 & 61.2 & 55.7 & 49.7 \\ all except the backbone & 52.5 & 57.0 & 61.5 & 55.6 & 49.9 \\ PASTA (all) & **53.0** & **57.6** & **62.0** & 56.2 & **50.4** \\  

Table 7: Performance comparison of our approach without applying fine-tuning and to specific parts of the architecture (_i.e._, decoder, encoder, visual backbone).