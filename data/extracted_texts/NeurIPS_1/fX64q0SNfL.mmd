# Sample based Explanations via Generalized Representers

Che-Ping Tsai

Machine Learning Department

Carnegie Mellon University

chepingt@andrew.cmu.edu

&Chih-Kuan Yeh

Google Deepmind 1

jason6582@gmail.com

&Pradeep Ravikumar

Machine Learning Department

Carnegie Mellon University

pradeepr@cs.cmu.edu

###### Abstract

We propose a general class of sample based explanations of machine learning models, which we term _generalized representers_. To measure the effect of a training sample on a model's test prediction, generalized representers use two components: _a global sample importance_ that quantifies the importance of the training point to the model and is invariant to test samples, and _a local sample importance_ that measures similarity between the training sample and the test point with a kernel. A key contribution of the paper is to show that generalized representers are the only class of sample based explanations satisfying a natural set of axiomatic properties. We discuss approaches to extract global importances given a kernel, and also natural choices of kernels given modern non-linear models. As we show, many popular existing sample based explanations could be cast as generalized representers with particular choices of kernels and approaches to extract global importances. Additionally, we conduct empirical comparisons of different generalized representers on two image and two text classification datasets.

## 1 Introduction

As machine learning becomes increasingly integrated into various aspects of human life, the demand for understanding, interpreting, and explaining the decisions made by complex AI and machine learning models has grown. Consequently, numerous approaches have been proposed in the field of Explainable AI (XAI). Feature based explanations interpret models by identifying the most relevant input features , while sample based explanations do so via the most relevant training samples , . Although different methods emphasize different aspects of the model, some may even have conflicting philosophies . To address this issue, there have been growing calls within the XAI community for more objective or normative approaches , , which could help align XAI techniques more effectively with human needs.

One of the most straightforward approaches to assess the effectiveness of explanations is by evaluating their utility in downstream applications , . However, such evaluations can be costly, particularly during the development stages of explanations, as they often necessitate the involvement of real human users. As a result, a well-grounded, axiom-based evaluation can be beneficial for designing and selecting explanations for implementation. Axioms can be viewed as theoretical constraints that dictate how explanations should behave in response to specific inputs. A notable example is the Shapley value , which originated in cooperative game theory and has gained popularity in XAI due to its appealing axiomatic properties. Nonetheless, while axiomatic approaches have been widely applied in identifying significant features , , feature interactions , , and high-level concepts , they have not been extensively discussed in sample based explanations.

[MISSING_PAGE_FAIL:2]

model \(f\), training data points \(\), and an arbitrary test data point \(^{d}\) as input, and outputs a vector of explanation weights \([_{}(f,(_{1},y_{1}),),,_{}(f,(_{n},y_{n}),)]^{n}\) with each value corresponding to an importance score of each training sample to the test point. In the sequel, we will suppress the explicit dependence on the entire set of training points in the notation for the explanation functional and the dependence on the training label \(y_{i}\). Also, to make clear that the first data point argument is the training sample, and the second is the test sample, we will use \((f,_{i})\) to denote the sample explanation weight for \(_{i}\) to explain the prediction of the model \(f()\) for the test point \(\).

## 3 Axioms for Sample based Explanations

In this section, we begin by presenting a collection of axioms that describe various desirable characteristics of sample based explanations.

**Definition 1** (Efficiency Axiom).: _For any model \(f\), and test point \(x^{d}\), a sample based explanation \(()\) satisfies the efficiency axiom iff:_

\[_{i=1}^{n}(f,_{i})=f().\]

The efficiency axiom entails that the sum of the attributions to each training sample together adds up to the model prediction at the test point. This is a natural counterpart of the efficiency axioms used in the Shapley values .

**Definition 2** (Self-Explanation Axiom).: _A sample based explanation \(()\) satisfies the self-explanation axiom iff there exists any training point \(_{i}\) having no effect on itself, i.e. \((f,_{i}_{i})=0\), the training point should not impact any other points, i.e. \((f,_{i})=0\) for all \(^{d}\)._

The self-explanation axiom states that if the label \(y_{i}\) does not even have an impact on the model's prediction for \(_{i}\), it should not impact other test predictions. This axiom shares a similar intuition as the dummy axiom in the Shapley values  since both axioms dictate that explanations should be zero if a training sample has no impact on the model. However, the self-explanation axiom requires a different theoretical treatments due to the additional focus in generalized representers of explaining a model prediction on a particular test sample.

**Definition 3** (Symmetric Zero Axiom).: _A sample explanation \(()\) satisfies the symmetric zero axiom iff any two training points \(_{i},_{j}\) such that if \((f,_{i}_{i}) 0\) and \((f,_{j}_{j}) 0\), then_

\[(f,_{i}_{j})=0(f,_{j} _{i})=0.\]

The symmetric-zero axiom underscores the bidirectional nature of "orthogonality". It emphasizes that if a sample has no impact on another sample, this lack of correlation is mutual and implies that they are orthogonal.

**Definition 4** (Symmetric Cycle Axiom).: _A sample explanation \(()\) satisfies the symmetric cycle axiom iff for any set of training points \(_{t_{1}},..._{t_{k}}\), with possible duplicates, and \(_{t_{k+1}}=_{t_{1}}\), it holds that:_

\[_{i=1}^{k}(f,_{t_{i}}_{t_{i+1}})= _{i=1}^{k}(f,_{t_{i+1}}_{t_{i}}).\]

Let us first consider the vacuous case of two points: \(_{1},_{2}\), for which the axiom is the tautology that: \((f,_{1}_{2})(f,_{2} _{1})=(f,_{2}_{1}) (f,_{1}_{2})\). Let us next look at the case with three points: \(_{1},_{2},_{3}\), for which the axiom entails:

\[(f,_{1}_{2})(f,_{2} _{3})(f,_{3}_{1})= (f,_{3}_{2})(f,_{2} _{1})(f,_{1}_{3}).\]

It can be seen that this is a generalization of simply requiring that the explanations be symmetric as in the symmetry axiom in the Shapley values. In fact, the unique explanation satisfying this and other listed axioms is in general not symmetric. The axiom could also be viewed as a conservation or path independence law, in that the flow of explanation based information from a point \(_{i}\) to itself in a cycle is invariant to the path taken.

**Definition 5** (Continuity Axiom).: _A sample based explanation \(()\) satisfies the continuity axiom iff it is continuous wrt the test data point \(\), for any fixed training point \(_{i}\):_

\[_{^{}}(f,_{i} ^{})=(f,_{i}).\]

Such continuity is a minimal requirement on the regularity of the explanation functional, and which ensures that infinitesimal changes to the test point would not incur large changes to the explanation functional.

**Definition 6** (Irreducibility Axiom).: _A sample explanation \(()\) satisfies the irreducibility axiom iff for any number of training points \(_{1},...,_{k}\),_

\[(f,_{1},_{1})&(f, _{1},_{2})&...&(f,_{1},_{k})\\ (f,_{2},_{1})&(f,_{2},_{2})&... &(f,_{2},_{k})\\...&...&...&...\\ (f,_{k},_{1})&(f,_{k},_{2})&... &(f,_{k},_{k}) 0.\]

A sufficient condition for an explanation \(()\) to satisfy the irreducibility axiom is for

\[|(f,_{i}_{i})|>_{j i}|(f,_{i} _{j})|, \]

since this makes the matrix above strictly diagonally dominant, and since the diagonal entries are non-negative, by the Gershgorin circle theorem, the eigenvalues are all non-negative as well, so that the determinant in turn is non-negative.

The continuity and irreducibility axiom primarily serves a function-analytic purpose by providing sufficient and necessary conditions of a kernel being a Mercer kernel, which requires that the kernel function be continuous and positive semi-definite.

We are now ready to investigate the class of explanations that satisfy the axioms introduced above.

**Theorem 7**.: _An explanation functional \((f,,)\) satisfies the continuity, self-explanation, symmetric zero, symmetric cycle, and irreducibility axioms for any training samples \(\) containing \(n\) training samples \((_{i},y_{i})^{d}\) for all \(i[n]\) iff_

\[(f,_{i})=_{i}K(_{i},)\;\; \; i[n], \]

_for some \(^{n}\) and some continuous positive-definite kernel \(K:^{d}^{d}\)._

This suggests that a sample explanation \((f,_{i})=_{i}K(_{i},)\) has two components: a weight \(_{i}\) associated with just the training point \(_{i}\) independent of test points, and a similarity \(K(_{i},)\) between the training and test points specified by a Mercer kernel. Following Yeh et al. , we term the first component the _global importance_ of the training sample \(_{i}\) and the second component the _local importance_ that measures similarities between training and test samples.

Once we couple this observation together with the efficiency axiom, one explanation that satisfies these properties is:

\[f()=_{j=1}^{n}(f,_{i})=_{j=1}^{n }_{i}K(_{i},)\;\;x^{p}. \]

This can be seen to hold only if the target function \(f\) lies in the RKHS subspace spanned by the kernel evaluations of training points. When this is not necessarily the case, then the efficiency axiom (where the sum of training sample importances equals the function value) exactly, cannot be satisfied exactly. We can however satisfy the efficiency axiom approximately with the approximation error arising from projecting the target function \(f\) onto the RKHS subspace spanned by training representers.

This thus provides a very simple and natural framework for specifying sample explanations: (1) specify a Mercer kernel \(K(,)\) so that the target function can be well approximated by the corresponding kernel machine, and (2) project the given model onto the RKHS subspace spanned by kernel evaluations on the training points. Each of the sample explanation weights then has a natural specification in terms of global importance associated with each training point (arising from the projection of the function onto the RKHS subspace, which naturally does not depend on any test points), as well as a localized component that is precisely the kernel similarity between the training and test points.

Deriving Global Importance Given Kernel Functions

The previous section showed that the class of sample explanations that satisfy a set of key axioms naturally correspond to an RKHS subspace. Thus, all one needs, in order to specify the sample explanations, is to specify a Mercer kernel function \(K\) and solve for the corresponding global importance weights \(\). In this section, we focus on the latter problem, and present three methods to compute the global importance weights given some kernel \(K\).

### Method 1: Projecting Target Function onto RKHS Subspace

The first method is to project the target function onto the RKHS subspace spanned by kernel evaluations on the training points. Given the target function \(f\), loss function \(:\) and training dataset \(=\{(_{i},y_{i})\}_{i=1}^{n}\) (potentially, though not necessarily used to train \(f\)), and a user-specified Mercer kernel \(K\), our goal is to find a projection \(_{K}\) of the target model onto the RKHS subspace defined by \(_{n}=(\{K(_{i},)\}_{i=1}^{n}))\). To accomplish this, we formulate it as a RKHS regression problem:

\[_{K}=*{argmin}_{f_{K}_{K}}\{_{i=1}^{n}(f_{K}(_{i}),f(_{i}))+\|f_{K}\|_{_{K}}^{2}\}, \]

where \(_{K}\) as the RKHS defined by kernel \(K\), \(\|\|_{_{K}}:_{K}\) is the RKHS norm, and \(\) is a regularization parameter that controls the faithfulness and complexity of the function \(_{K}\). The loss function \(\) can be chosen as the objective function used to train the target function \(f\) to closely emulate the behavior of target function \(f\) and its dependence on the training samples \(\). By the representer theorem , the regularization term \(\|f_{K}\|_{_{K}}^{2}\) added here ensures that the solution lies in the RKHS subspace \(_{n}\) spanned by kernel evaluations. Indeed, by the representer theorem , the minimizer of Eqn.(4) can be represented as \(_{K}()=_{i=1}^{n}_{i}K(_{i},)\) for some \(^{n}\), which allows us to reparameterize Eqn.(4):

\[=*{argmin}_{^{n}}\{_{i=1}^{n}(_{j=1}^{n}_{j}K(_{i}, _{j}),f(_{i}))+^{} \}, \]

where \(^{n n}\) is the kernel gram matrix defined as \(K_{ij}=K(_{i},_{j})\) for \(i,j[n]\), and we use the fact that \(\|f_{K}\|_{_{K}}=_{i=1}^{n}_{i}K(_{i}, ),_{i=1}^{n}_{i}K(,_{i})^{}= }\). By solving the first-order optimality condition, the global importance \(\) must be in the following form:

**Proposition 8**.: _(Surrogate derivative) The minimizer of Eqn.(4) can be represented as \(_{K}=_{i=1}^{n}_{i}K(_{i},)\), where_

\[\{^{*}+v v()\}_{i}^{*}=-(_{K}( _{i}),f(_{i}))}{_{K}(_{i})},\ \  i[n]. \]

_We call \(_{i}^{*}\) the surrogate derivative since it is the derivative of the loss function with respect to the surrogate function prediction._

\(_{i}^{*}\) can be interpreted as the measure of how sensitive \(_{K}(_{i})\) is to changes in the loss function. Although the global importance \(\) solved via Eqn.(5) may not be unique as indicated by the above results, the following proposition ensures that all \(\{^{*}+v v()\}\) result in the same surrogate function \(_{K}=_{i=1}^{n}_{i}^{*}K(_{i},)\).

**Proposition 9**.: _For any \(v()\), the function \(f_{v}=_{i=1}^{n}v_{i}K(_{i},)\) specified by the span of kernel evaluations with weights \(v\) is a zero fucntion, such that \(f_{v}()=0\) for all \(^{d}\)._

The proposition posits that adding any \(v()\) to \(^{*}\) has no effect on the function \(_{K}\). Therefore, we use \(^{*}\) to represent the global importance as it captures the sensitivity of the loss function to the prediction of the surrogate function.

### Method 2: Approximation Using the Target Function

Given the derivation of global importance weights \(^{*}\) in Eqn.(3), we next consider a variant replacing the surrogate function prediction \(_{K}(_{i})\) with the target function prediction \(f(_{i})\):

**Definition 10** (Target derivative).: _The global importance computed with derivatives of the loss function with respect to the target function prediction is defined as:_

\[^{*}_{i}=-(f(x_{i}),y_{i})}{ f(x_{i})}, \ \  i[n], \]

_where \((,)\) is the loss function used to train the target function._

A crucial advantage of this variant is that we no longer need solve for an RKHS regression problem. There are several reasons why this approximation is reasonable. Firstly, the loss function in Eqn.(4) encourages the surrogate function to produce similar outputs as the target function, so that \(_{K}(_{i})\) is approximately equal to \(f(x_{i})\). Secondly, when the target function exhibits low training error, which is often the case for overparameterized neural networks that are typically in an interpolation regime, we can assume that \(f(x_{i})\) is close to \(y_{i}\). Consequently, the target derivative can serve as an approximation of the surrogate derivative in Eqn.(6). As we will show below, the influence function approach  is indeed as the product between the target derivative and the influence function kernel.

### Method 3: Tracking Gradient Descent Trajectories

Here, we propose a more scalable variant we term _tracking representers_ that accumulates changes in the global importance during kernel gradient descent updates when solving Eqn.(4). Let \(:^{d}\) be a feature map corresponding to the kernel \(K\), so that \(K(,^{})=(),(^{ })\). We can then cast any function in the RKHS as \(f_{K}()=,()\) for some parameter \(\). Suppose we solve the unregularized projection problem in Eqn.(4) via stochastic gradient descent updates on the parameter \(\): \(^{(t)}=^{(t-1)}-}{|B^{(t)}|}_{i B^{(t)}} _{}(f_{}(_{i}),f(_{i}))( _{i})|_{=^{(t-1)}}\), where we use \(B^{(t)}\) and \(^{(t)}\) to denote the minibatch and the learning rate. The corresponding updates to the function is then given by "kernel gradient descent" updates: \(f^{(t)}_{K}()=f^{(t-1)}_{K}()-_{it}K(_{i},)\), where \(_{it}=}{|B^{(t)}|}_{i B^{(t)}}(f^{(t-1)}_{K}(_{i}),f(_{i}))}{ f^{(t- 1)}_{K}(_{i})}\). The function at step \(T\) can then be represented as:

\[f^{(T)}_{K}()=_{i=1}^{n}^{(T)}_{i}K(_{i}, )+f^{(0)}_{K}()^{(T)}_{i}=-_{t:i B ^{(t)}}}{|B^{(t)}|}(f^{(t-1)}_{K}( _{i}),f(_{i}))}{ f^{(t-1)}_{K}(_{i})}. \]

**Definition 11** (Tracking representers).: _Given a finite set of steps \(T\), we term the global importance weights obtained via tracking kernel gradient descent as tracking representers:_

\[^{*}_{i}=-_{t[T]\,:\,i B^{(t)}}}{|B^{(t)}|} (f^{(t-1)}_{K}(_{i}),f(_{i}))}{  f^{(t-1)}_{K}(_{i})}. \]

We note that one can draw from standard correspondences between gradient descent with finite stopping and ridge regularization (e.g. ), to in turn relate the iterates of the kernel gradient descent updates for any finite stopping at \(T\) iterations to regularized RKHS regression solutions for some penalty \(\). The above procedure thus provides a potentially scalable approach to compute the corresponding global importances: in order to calculate the global importance \(^{(T)}_{i}\), we need to simply monitor the evolution of \(^{(t)}_{i}\) when the sample \(_{i}\) is utilized at iteration \(t\). In our experiment, we use the following relaxation for further speed up:

\[^{*}_{i}=-_{t[T]\,:\,i B^{(t)}}}{|B^{(t)}|} (f^{(t-1)}(_{i}),y_{i})}{ f^{(t-1)} (_{i})}, \]

where we assume the target model is trained with (stochastic) gradient descent, \(f^{(t)}(_{i})\) denotes the target model at \(t^{}\) iteration during training, and \(B^{(t)}\) and \(^{(t)}\) are the corresponding mini-batch and learning rate. Similar to the intuition of replacing the surrogate derivative with to target derivative, we track the target model's training trajectory directly instead of solving Eqn.(4) with kernel gradient descent.

Choice of Kernels for Generalized Representers

Previously, we discussed approaches for deriving global importance given user-specified kernels, which can in general be specified by domain knowledge relating to the model and the application domain. In this section, we discuss natural choices of kernels for modern non-linear models. Moreover, we show that existing sample based explanation methods such as representer points [] and influence functions [] could be viewed as making particular choices of kernels when computing general representers. We also discuss TracIn [] as a natural extension of our framework to multiple rather than a single kernel.

### Kernel 1: Penultimate-layer Embeddings

A common method for extracting a random feature map from a neural network is to use the embeddings of its penultimate layer [][][][]. Let \(_{_{1}}:^{d}^{d}\) denote the mapping from the input to the second last layer. The target model \(f\) can be represented as

\[f()=_{_{1}}()^{}_{2}, \]

where \(_{2}^{}\) is the weight matrix of the last layer. That is, we treat the deep neural network as a linear machine on top of a learned feature map. The kernel function is then defined as \(K_{}(,)=_{_{1}}(), _{_{1}}(),,^{d}\). This is the case with most deep neural network architectures, where the feature map \(_{_{1}}\) is specified via deep compositions of parameterized layers that take the form of fully connected layers, convolutional layers, or attention layers among others. While the last-layer weight matrix \(_{2}\) may not lie in the span of \(\{_{_{1}}(_{i})\}_{i=1}^{n}\), we may solve the its explanatory surrogate function using Eqn. [].

**Corollary 12**.: _(Representer point selection []) The minimizer of Eqn. [], instantiated with \(K_{}(,)=_{_{1}}(), _{_{1}}(),,^{d}\), can be represented as_

\[_{K}()=_{i=1}^{n}_{i}K_{}(_{i},),_{i}=-(_{K}( _{i}),f(_{i}))}{_{K}(_{i})},\ \  i[n]. \]

The above corollary implies that \(_{2}=_{i=1}^{n}_{i}_{_{1}}(_{i})\). In other words, the RKHS regularization in Eqn. [] can be expressed as \(\|f_{K}\|_{_{K}}^{2}=\|_{2}\|^{2}\), which is equivalent to L2 regularization. Consequently, the representer point selection method proposed in Yeh et al. [] can be generalized to our framework when we use last-layer embeddings as feature maps.

### Kernel 2: Neural Tangent Kernels

Although freezing all layers except for the last layer is a straightforward way to simplify neural networks to linear machines, last-layer representers may overlook influential behavior that is present in other layers. For example, Yeh et al. [] shows that representation in the last layer leads to inferior results for language models. On the other hand, neural tangent kernels (NTK) [] have been demonstrated as a more accurate approximation of neural networks [][][][]. By using NTKs, we use gradients with respect to model parameters as feature maps and approximate neural networks with the corresponding kernel machines. This formulation enables us to derive a generalized representer that captures gradient information of all layers.

For a neural network with scalar output \(f_{}:^{d}\) parameterized by a vector of parameters \(^{p}\), the NTK is a kernel \(K:^{d}^{d}\) defined by the feature maps \(_{}()=()}{ }\):

\[K_{,}(,)=()}{},()}{ }. \]

Connection to TracIn []:TracIn measures _the change in model parameters from the start to the end of training_. While it is intractable due to the need to store model parameters of all iterations, Pruthi et al. [] used checkpoints(CP) as a practical relaxation: given model parameters \(^{(t)}\) and learning rates \(^{(t)}\) at all model checkpoints \(t=0,,T\), the formulation of TracInCP is given below

\[_{}(f_{},(_{i},y_{i}) ) =-_{t=0}^{T}^{(t)}(f_{}( _{i}),y_{i})}{}^{}( )}{}_{=^{(t)}}\] \[=-_{t=0}^{T}( f_{}(_{i}),y_{i})}{ f_{}(_{i})}_{ =^{(t)}}}_{}, ^{(t)}}(_{i},)}_{}. \]

When the learning rate is constant throughout the training process, TracInCP can be viewed as a generalized representer instantiated with target derivative as global importances and NTK (Eqn.(13)) as the kernel function, but uses different kernels on different checkpoints.

### Kernel 3: Influence Function Kernel

The influence functions  can also be represented as a generalized representer with the following kernel:

\[K_{,}(,)=()}{},()}{ }_{H_{}^{-1}}=( )^{}}{}H_{}^{-1}( )}{}, \]

where \(H_{}=_{i=1}^{n}(f_{}( x_{i}),y_{i})}{^{2}}\) is the Hessian matrix with respect to target model parameters. The influence function then can be written as:

\[_{}(f_{},(_{i},y_{i}))=-(f_{}(_{i}),y_{i})}{}H_{ }^{-1}()}{}=- {(f_{}(_{i}),y_{i})}{ f_{ }(_{i})}}_{}, }(_{i},)}_{}. \]

Therefore, the influence function can be seen as a member of generalized representers with target derivative global importance (Definition 10 and the influence function kernel. Influence functions  were designed to measure _how would the model's predictions change if a training input were perturbed_ for convex models trained with empirical risk minimization. Consequently, the inversed Hessian matrix describes the sensitivity of the model parameters in each direction.

## 6 Experiments

In the experiment, we compare different representers within our proposed framework on both vision and language classification tasks. We use convolutional neural networks (CNN) since they are widely recognized deep neural network architectur. We compare perforamnce of different choices of kernels and different ways to compute global importance. Existing generalized representers, such as influence functions , representer point selections , and TracIn , are included in our experiment.

### Experimental Setups

Evaluation metrics:We use _case deletion diagnostics_, \(_{-}(,k,)\), as our primary evaluation metric. The metric measures _the difference between models' prediction score on \(\) when we remove top-\(k\) negative impact samples given by method \(\) and the prediction scores of the original models_. We expect \(_{-}\) to be positive since models' prediction scores should increase when we remove negative impact samples. To evaluate deletion metric at different \(k\), we follow Yeh et al.  and report area under the curve (AUC): \(_{-}=_{i=1}^{m}_{-}(,k_{i},)/m\), where \(k_{1}<k_{2}<<k_{m}\) is a predefined sequence of \(k\).

We choose \(k_{i}=0.02in\) for \(i=0,1,,5\) with \(n\) as the size of the training set. The average of each metric is calculated across \(50/200\) randomly initialized neural networks for vision/language data. For every neural network, sample-based explanation methods are computed for \(10\) randomly selected testing samples.

Datasets and models being explained:For image classification, we follow Pruthi et al.  and use MNIST  and CIFAR-10  datasets. For text classification, we follow Yeh et al.  and use Toxicity and AGNews datasets, which contain toxicity comments and news of different categories respectively. Due to computational challenges in computing deletion diagnostics, we subsample the datasets by transforming them into binary classification problems with each class containing around \(6,000\) training samples. The CNNs we use for the four datasets comprise \(3\) layers. For vision datasets, the models contain around \(95,000\) parameters. For text datasets, the total number of parameters in the model is \(1,602,257\) with over \(90\%\) of the parameters residing in the word embedding layer that contains \(30,522\) trainable word embeddings of dimensions \(48\). Please refer to the Appendix for more details on the implementation of generalized representers and dataset constructions.

### Experimental Results

The results are shown in Table . We also provide deletion curves we compute AUC-DEL\({}_{-}\) for in the Appendix.

I. Comparison of different global importance:In the first experiment, we fix the kernel to be the NTK computed on final model parameters, and we compare different methods for computing global importance in Section 3. We do not compute the surrogate derivative on the text datasets since the total numbers of parameters are too large, making the computation infeasible.

    &  \\    \\  Kernels &  & Random \\  Global importance & surrogate derivative & target derivative & tracking & Selection \\  MNIST & \(1.88 0.25\) & \(2.41 0.30\) & \(\) & \(-0.50 0.16\) \\ CIFAR-10 & \(2.27 0.18\) & \(2.81 0.20\) & \(\) & \(0.136 0.10\) \\  Toxicity & \(-\) & \(1.10 0.21\) & \(\) & \(0.15 0.19\) \\ AGNews & \(-\) & \(1.88 0.27\) & \(\) & \(0.19 0.26\) \\    \\  Global importance &  \\  Kernels & last layer-final & NTK-init & NTK-middle & NTK-final & Inf-final \\  MNIST & \(3.44 0.46\) & \(3.18 0.46\) & \(3.63 0.49\) & \(3.52 0.48\) & \(\) \\ CIFAR-10 & \(2.26 0.13\) & \(1.35 0.20\) & \(2.67 0.19\) & \(3.26 0.19\) & \(\) \\  Toxicity & \(1.34 0.22\) & \(0.63 0.22\) & \(1.90 0.23\) & \(\) & \(0.42 0.20\)2  \\ AGNews & \(2.14 0.27\) & \(1.81 0.27\) & \(\) & \(\) & \(0.92 0.26\)3  \\   

Table 1: Case deletion diagnostics, AUC-DEL\({}_{-}\), for removing negative impact training samples on four different datasets. \(95\%\) confidence interval of averaged deletion diagnostics on \(50 10=500(\) or \(200 10=2000)\) samples is reported for vision (or language) data. Larger AUC-DEL\({}_{-}\) is better. Init, middle, and final denote initial parameters \(^{(0)}\), parameters of a middle checkpoint \(^{(T/2)}\), and final parameters \(^{(T)}\) for neural networks trained with \(T\) epochs. 4We only use the last-layer parameters to compute influence functions as in  since the total number of parameters are too large for text models.

We observe that _tracking_ has the best performance, followed by _target derivative_ and then _surrogate derivative_. This could be due to the loss flattening when converged and the loss gradients becoming less informative. Consequently, accumulating loss gradients during training is the most effective approach. Moreover, if _tracking_ is not feasible when training trajectories are not accessible, we may use _target derivative_ instead of _surrogate derivative_ as an alternative to explain neural networks since they have similar performance.

II. Comparison of different kernels:Next, we fix the global importance to _tracking_ and compare different kernels in Section5. We employ the tracking representers to compute global importance since it showed the best performance in the previous experiment. We can see that the influence function kernel performs the best in the vision data sets, and the NTK-final kernel has the best performance in language data sets. Note that influence functions exhibit distinctly contrasting performances on image and text data, which could be attributed to our reliance solely on last-layer parameters for influence function computation on language datasets. This finding aligns with the conclusions of Yeh et al. , who suggest that the last layer gradients provide less informative insights for text classifiers.

In summary, these findings indicate that NTK-final is a dependable kernel selection due to its consistent high performance across all four datasets, while also offering a computational efficiency advantage over the influence function kernel. These results also demonstrate that accessing target model checkpoints for computing kernels is unnecessary since NTK and influence function on the final model already provide informative feature maps.

III. Comparison of different generalized representers:Finally, we compare the new generalized representer with other existing generalized representers. We categorize TracInCP, the influence function, and the representer point as existing generalized representers: TracInCP can be viewed as an ensemble of generalized representers with target derivatives using the Neural Tangent Kernel. The influence function can be expressed as the influence function kernel with the target derivative. Lastly, the representer point can be seen as a form of generalized representer that utilizes the last-layer kernel and the surrogate derivative.

We find that the Inf-final has comparable performance to TracInCP and they outperform other approaches. Although TracInCP has the best performance on MNIST, it requires accessing different checkpoints, which requires a significant amount of memory and time complexity. In contrast, the NTK and Inf tracking representers are more efficient since they only require tracking gradient descent trajectories during training without the need for storing checkpoints.

## 7 Conclusion and Future work

In this work, we present _generalized representers_ that are the only class of sample based explanations that satisfy a set of desirable axiomatic properties. We explore various techniques for computing generalized representers in the context of modern non-linear machine learning models and show that many popular existing methods fall into this category. Additionally, we propose tracking representers that track sample importance along the gradient descent trajectory. In future work, it would be of interest to derive different generalized representers by altering different global importance and choices of kernels, as well as investigating their applicability to diverse machine learning models and modalities.

## 8 Acknowledgements

We acknowledge the support of DARPA via FA8750-23-2-1015, ONR via N00014-23-1-2368, and NSF via IIS-1909816.