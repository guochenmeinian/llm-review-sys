# _Is Your Code Generated by ChatGPT Really Correct?_

Rigorous Evaluation of Large Language Models

for Code Generation

 Jiawei Liu\({}^{}}\)1 Chunqiu Steven Xia\({}^{}}\)1 Yuyao Wang\({}^{}}\) Lingming Zhang\({}^{}}\)

University of Illinois Urbana-Champaign\({}^{}}\) Nanjing University

{jiawei6, chunqiu2, lingming}@illinois.edu yuyao6@outlook.com

Equal contribution. Author ordering is decided by _Nigiri_.

###### Abstract

Program synthesis has been long studied with recent approaches focused on directly using the power of Large Language Models (LLMs) to generate code. Programming benchmarks, with curated synthesis problems and test-cases, are used to measure the performance of various LLMs on code synthesis. However, these test-cases can be limited in both quantity and quality for fully assessing the functional correctness of the generated code. Such limitation in the existing benchmarks begs the following question: _In the era of LLMs, is the code generated really correct?_ To answer this, we propose EvalPlus - a code synthesis evaluation framework to rigorously benchmark the functional correctness of LLM-synthesized code. EvalPlus augments a given evaluation dataset with large amounts of test-cases newly produced by an automatic test input generator, powered by both LLM- and mutation-based strategies. While EvalPlus is general, we extend the test-cases of the popular HumanEval benchmark by \(80\) to build HumanEval\({}^{+}\). Our extensive evaluation across 26 popular LLMs (_e.g.,_ GPT-4 and ChatGPT) demonstrates that HumanEval\({}^{+}\) is able to catch significant amounts of previously undetected wrong code synthesized by LLMs, reducing the pass@\(k\) by up-to 19.3-28.9%. We also surprisingly found that test insufficiency can lead to mis-ranking. For example, both WizardCoder-CodeLlama and Phind-CodeLlama now outperform ChatGPT on HumanEval\({}^{+}\), while none of them could on HumanEval. Our work not only indicates that prior popular code synthesis evaluation results do not accurately reflect the true performance of LLMs for code synthesis, but also opens up a new direction to improve such programming benchmarks through automated testing. We have open-sourced our tools, enhanced datasets as well as all LLM-generated code at [https://github.com/evalplus/evalplus](https://github.com/evalplus/evalplus) to facilitate and accelerate future LLM-for-code research.

## 1 Introduction

Automatically generating programs that accurately correspond to user intents is a long-standing challenge in computer science known as program synthesis . In the past few decades, classical program synthesis techniques have been developed, including deductive synthesis , inductive synthesis  and neural-guided synthesis . More recently, with the advent of Large Language Models  (LLMs) and the abundance of open codebase, researchers have been focusing on applying LLMs for direct code generation. LLMs like Codex and CodeGen perform code generation by autoregressively predicting the next token given previous context, in the form of function signature and docstring that denote the desired program functionality. The generated code snippet is then combined with the context to form a complete function that aligns with the user intent. Leveraging both natural language understanding and generative power, LLMs have demonstrated impressive performance in code synthesis .

The primary concern when it comes to LLM-generated code is correctness. Because two dramatically different code snippets can be semantically equivalent, classic NLP metrics like BLEU score  are no longer reliable in the context of program synthesis. Ideally, we would like to formally verify the correctness of LLM-provided solutions for any input, but verifying domain-specific problems through methods such as translation validation  is already challenging enough, let alone building a general verifier with absolute certainty to prove arbitrary problems, including those in code benchmarks. As such, existing code benchmarks (_e.g.,_ HumanEval) heavily rely on manually constructed test-cases to evaluate LLM solutions. However, these tests often fall short in capturing all possible scenarios, as crafting high-quality tests is laborious. Consequently, we argue that current programming benchmarks are inadequate for assessing the actual correctness of LLM-generated code, leading to false confidence in the results. Specifically, we have identified the following common limitations in existing LLM-for-code benchmarks:

* **Insufficient testing.** Current programming benchmarks often only include on average less than 10 tests for each coding problem. Furthermore, these tests are relatively too simple to fully explore the functionality of the code or corner cases. Figure 1 shows an incorrect code sample synthesized by ChatGPT  to return the sorted unique common elements from two lists. At first glance, the function looks correct and computes the desired output when using the base test inputs from HumanEval. However, in the return statement, it incorrectly converts the intermediate list to a set which no longer preserves the order of the sorted list. This example shows that a logically flawed solution can still pass all simple tests and be misconsidered as correct due to testing inadequacy.
* **Imprecise problem description.** The input for code generation includes natural language descriptions in addition to the function signature. These task descriptions in existing benchmarks are oftentimes too vague to fully clarify the expected program behaviors. For example, the input docstring may not specify the expected input domain (_e.g.,_ only positive integers) or how the function should handle exceptions. As a result, such programming problems can be interpreted differently by LLMs against the actual tests, leading to _capable_ LLMs misjudged as incapable.

These limitations are common across many popular code generation benchmarks . This not only questions the validity of the impressive performance claimed by prior work but also sets a challenge on how to properly evaluate the LLM coders. In this paper, we aim to address this fundamental evaluation challenge and ask the introspective question: _Is the code generated by LLMs really correct?_

**Our proposal.** In this work, we set out to answer the important question and _evaluate_ the evaluation dataset. Consequently, we build EvalPlus - an evaluation framework to improve existing code benchmarks in order to precisely evaluate the functional correctness of LLM-generated code. At the heart of EvalPlus is an automatic test input generation engine which augments existing code benchmarks by generating interesting test inputs to fully exercise the code solution and check its functional correctness by cross-checking the ground-truth implementation. Specifically, EvalPlus adopts both LLM- and mutation-based  methods to automatically generate and diversify additional test inputs. EvalPlus first uses ChatGPT  to generate a set of high-quality seed inputs that aim to test difficult corner cases and functionalities of the program within the valid input structure. Using these high-quality seed inputs, EvalPlus then performs type-aware mutation to efficiently generate a large number of additional test inputs. These newly generated test inputs are then used to evaluate the LLM-generated code through differential testing  against the ground-truth implementation. Furthermore, as an option to speed up evaluation, EvalPlus also builds minimal test-suites by only

Figure 1: Exemplary wrong code synthesized by ChatGPT for HumanEval #58.

including the most valuable test-cases, which are selected by running a greedy set cover algorithm to preserve the same code coverage , mutation analysis  as well as empirical LLM sample killings.

**Contribution.** Our work revisited and proposed to automatically improve code benchmarks for LLMs:

* **Study:** We are the first to study the test inadequacy problem in current programming benchmarks which can lead to largely over-approximated functional correctness. Our study also opens up a new research direction for precisely and rigorously evaluating LLM-synthesized code.
* an evaluation framework to reveal the real correctness of LLM-synthesized code. The test-case generation approach of EvalPlus combines the emerging LLM-based and traditional mutation-based test input generation. It first uses LLM-based strategy to bootstrap the test generator with high-quality seed inputs and then further extends large amounts of inputs via type-aware mutation. We then optionally "distill" the generated tests to a much smaller yet almost equivalently effective test-suite via greedy set covering. We also propose to annotate each programming tasks using program contracts to filter out invalid inputs.
* **Results:** EvalPlus extends the popular HumanEval benchmark to create HumanEval\({}^{+}\), improving the test-case scale by \(80\). Through test-suite reduction, we also produce HumanEval\({}^{+}\)-mini which distills HumanEval\({}^{+}\) tests by \(47\) while still achieving a similar level of testing effectiveness. Our extensive evaluation over 26 popular LLMs surprisingly finds that the pass@\(k\) on the new dataset is up-to 19.3-28.9% (for different \(k\)s) lower than the base HumanEval, showing that testing insufficiency can largely affect the result analysis for almost all recent work on LLM-based code generation. Meanwhile, on the original HumanEval both of the 34B WizardCoder-CodeLlama  and Phind-CodeLlama  models are deemed to be no better than ChatGPT, while HumanEval\({}^{+}\) corrected the ranking and shows that the two open-source models are actually better. Additionally, we even found that the ground-truth solutions of HumanEval can be erroneous, further calling into question the quality of code synthesis benchmarks.

## 2 Approach

Figure 2 shows the overview of EvalPlus. We first take in as input the original dataset containing the ground-truth implementation as well as the base test inputs. EvalPlus starts with constructing a prompt using the original ground-truth, exemplary test inputs as demonstration, and a specialized instruction to query ChatGPT and generate a set of high-quality seed inputs. ChatGPT, by following base input formats and inspecting the ground-truth solution, can serve as a vehicle to generate valid yet rigorous test inputs. Starting from these seed inputs, we then perform type-aware mutation to quickly generate numerous new inputs together with seed inputs to extensively evaluate the functional correctness of LLM-generated code. We use differential testing  as the oracle to cross-check the output of the ground-truth and LLM-generated solution. As an option to speed up evaluation, EvalPlus

Figure 2: Overview of EvalPlus

runs set covering to minimize the generated test-suite while preserving the same level of testing effectiveness. As the final output, EvalPlus obtains a augmented benchmark using the generated high-quality test inputs to fully evaluate the functional correctness of LLM-synthesized code.

### Automated Test Input Generation

**Seed initialization via ChatGPT.** EvalPlus first uses ChatGPT to generate a set of high-quality seed inputs for later mutation. Following Figure 2, we construct a prompt using _(i)_ the ground-truth solution of the problem for ChatGPT to inspect; _(ii)_ a set of test inputs as demonstration; and _(iii)_ an instruction to encourage ChatGPT to come up with interesting inputs. Specifically, each prompt starts with the ground-truth implementation and then randomly sampled test inputs from the existing dataset. We then finalize the prompt with a selected instruction in Figure 2 and query ChatGPT to produce new inputs. EvalPlus aims to leverage the powerful understanding ability of ChatGPT to learn both the valid input formats (_e.g.,_ variable types) as well as the desired functionality of the ground-truth solution in order to produce meaningful test inputs to reveal bugs in incorrectly synthesized code. Programs can have their own expected input formats, where invalid inputs should not be passed into the function as they can incur undefined behaviors to create false-positives in differential testing. As such, we filter out any invalid inputs which violate the input precondition required by the ground-truth implementation.

By using ChatGPT as an automated generation engine, we can generate inputs that are valid even under semantic constraints. For example, a programming problem may require the input to conform to a specific structure (_e.g.,_ a palindrome). Such semantic constraints can be extremely difficult for traditional input generators to satisfy. However, ChatGPT is unsuitable for large amounts of automated test generation due to undesired speed and cost of querying such a large model. To address this, we perform type-aware input mutation starting from high-quality seed inputs generated by ChatGPT.

**Type-aware input mutation.** We follow a typical mutation-based fuzzing workflow  to continuously create inputs: _(i)_ a corpus of seed inputs from ChatGPT are used to initialize the seed pool and bootstrap the generation pipeline; _(ii)_ each time an input (_i.e.,_ seed) from the seed pool is randomly selected to be mutated to a new input (_i.e.,_ mutant); and _(iii)_ new inputs that comply with the program contract (SS2.3) are added to the seed pool and we start over from _(ii)_ to continue the generation process.

To efficiently create more valid inputs, we leverage type-aware mutation  in step _(ii)_ which inspects the data types of the incoming valid seeds and generates new inputs that are structurally similar to the seeds. In Table 1 we illustrate the basic mutations used for different types of inputs. For simple primitive types such as int and float, the mutation is as simple as incrementing/decrementing the value. For compound types and the string type (_i.e.,_ str), besides generally removing or repeating existing elements (or sub-strings for str), the elements and sub-strings can be mutated recursively according to their inner types. Such sub-mutants can then be used to replace existing items or add new items in a finer-grain manner. In addition, to alleviate generating inputs that violate subtle semantic constraints, following , we additionally apply an ingredient mechanism to collect appeared data fragments and reuse them during mutation. In short, type-aware input mutation builds on the high-quality seed inputs produced by ChatGPT to generate large amounts of test inputs which we use as the final set of extensive test inputs to evaluate LLM-synthesized code.

   Type & Mutation & Type & Mutation \\  int \(|\)float & Returns \(x 1\) & List & \(\{x[i]\\ x[i](x[i]).\) \\ bool & Returns a random boolean & Tuple & Returns Tuple(Mutate(List(\(x\)))) \\ NoneType & Returns None & Set & Returns Set(Mutate(List(\(x\)))) \\  & s\\ s\\ s(s).\)} &  & k\!\!\!\!v\\ k\!\!\!vk\!\!\!\!(v)\\ (k)\!\!\!(v).\)} \\   

Table 1: List of basic type-aware mutations over input \(x\).

### Test-Suite Reduction

While the large number of newly generated tests in EvalPlus are effective in detecting incorrect code, the test execution can be costly. As an option to more efficiently evaluate LLM-generated code, we further investigate test-suite reduction strategies [75; 59], which aim to select a subset of the original test-suite while still maintaining the original test effectiveness. To perform test reduction, it is typically assumed that each test can fulfill a set of _testing requirements_. The problem can then be formalized as reducing the original test-suite \(\) into \(_{red}\), such that \( r\;( t,\,t\;\;\;r \; t^{}_{red},t^{}\; \;\;r)\). In other words, any testing requirement \(r\) satisfied by the original test-suite should still be satisfied by the reduced one. Finding such minimal representative subset for a given test-suite is equivalent to the set covering problem . To solve this problem effectively, it is crucial to define the testing requirements accurately. In this paper, we focus on the following types of requirements:

**Code coverage**: Code coverage  measures the amount of code elements (_e.g.,_ statements or branches) executed by each test, and has been widely used in practice to measure test effectiveness. In this strategy, following traditional test-suite reduction  we leverage the widely used branch coverage as the testing requirement. In other words, the goal of using this metric is to only preserve a minimal subset of tests which can cover the same set of branches as the full tests.

**Mutant killings**: Coverage measures the extent to which the code has been executed; however, a high-coverage test-case is not necessarily effective in finding critical defects in its covered code. Consequently, researchers have proposed _mutation testing_ (also known as _mutation analysis_) to more precisely evaluate test effectiveness. In short, mutation testing applies a set of predefined _mutation rules_ (_e.g.,_ changing "\(<\)" and "\(\)") to the program under test (_i.e.,_ the ground-truth solutions for this case) to create a large number of artificial buggy programs, each of which is called as a _mutant_ and includes exactly _one_ subtle bug seeded. In this way, the ratio of mutation bugs detected by the tests (also called _killed_) can be used to assess the test effectiveness. In fact, studies have shown that mutation testing can largely outperform code coverage in test effectiveness evaluation . Following prior work , we also leverage the set of mutants killed by each test as our testing requirement. Consequently, the goal is to minimize the number of tests while still being able to detect the same set of mutation bugs.

**LLM sample killings**: Different LLMs could fail commonly over certain test-cases. Consequently, besides these theoretical metrics, we also use as a testing requirement by empirically looking at _sample killings, i.e.,_ the set of wrong LLM samples that a test-case can detect and falsify. Of course, for a new LLM under evaluation, we do not have any test execution results for its code samples. Therefore, we only use the execution results for samples generated by other LLMs to evaluate test effectiveness for reduction (_i.e.,_ leave-one-out cross validation ). As such, we minimize the number of tests while making sure that all incorrect samples synthesized by other models can be detected by the reduced test-suite.

Besides the above three strategies, we also investigate another strategy that merges all three testing requirements for reduction. That is, the goal is to minimize the number of tests while still maintaining the same branch coverage, mutant killing, and incorrect sample detection results.

### Program Input Contracts

The goal of evaluating code synthesis is to check whether the synthesized code accurately reflects the desired user intent. This is done by using several test inputs and comparing the output of the generated code against that of the ground-truth solution. The prior sections demonstrated how to improve the test inputs used to more rigorously evaluate the synthesized code. However, these user intents (expressed as natural language docstring) can be too vague for LLMs to follow. As such, LLMs might allow for different interpretations of the desired functionality, input formats as well as how to handle corner cases.

To this end, we adopt a programming by contract  philosophy by systematically annotating function pre-conditions in form of code assertions (_e.g.,_ assert n > 0), to ensure the test inputs for the function are well-formed. The benefits of the contracts are two-fold: _(i)_ they can complement the automatic input generation steps to filter out any generated invalid inputs that violate the contracts. Such ill-formed inputs can incur undefined behaviors which are unreasonable to use for evaluating LLM-synthesized code; and _(ii)_ they can serve as orthogonal descriptors together with the natural language description in the prompt for further clarification.

## 3 Evaluation

**Setup.** Our evaluation focuses on using the unbiased version of pass@\(k\) to accurately assess the functional correctness of LLM-synthesized code. For generalizability, we conducted a comprehensive evaluation over 26 popular and state-of-the-art LLMs and a wide range of temperature settings. Specifically, following prior work [11; 46], for each model we perform: _(i)_ random sampling to generate 200 program samples for each of the four temperature settings ({\(0.2,0.4,0.6,0.8\)}; and _(ii)_ greedy-search decoding. For random sampling, we show the best-performing pass@\(k\) for each \(k\{1,10,100\}\) and its corresponding temperature denoted by \(T_{k}^{*}\). For greedy decoding, we only synthesize one deterministic sample for each task and evaluate its pass rate as pass@1\({}^{}\). By default we evaluate models under both setting _(i)_ and _(ii)_, except for the two commercial models due to time and cost constraints: GPT-4 is only evaluated under greedy decoding, and ChatGPT is additionally evaluated on \(0.8\)-temperature random sampling.

While EvalPlus is general, this paper focuses on evaluating its effectiveness on HumanEval, one of the most widely-used datasets for code generation5. HumanEval consists of 164 human-written programming tasks, each of which provides a Python function signature and a docstring as the input to the LLM. Based on the input, LLMs complete a solution whose functional correctness is judged by a handful of manual test-cases (the first row in Table 2). As such, EvalPlus transforms HumanEval to HumanEval\({}^{+}\) by adding \(80\) unique test-cases and fixing incorrect ground-truth solutions in HumanEval. Specifically, for each task, based on around 30 ChatGPT-generated seed inputs which are produced using 3 separate prompts, we run type-aware mutation to generate 1000 additional inputs using one-hour budget. In HumanEval\({}^{+}\), 83 out of the 164 programming tasks are annotated with hand-crafted contracts. Because EvalPlus requires ground-truth solutions to cross-check LLM-generated code, it is crucial to ensure the correctness of the ground-truths. However, by inspecting ground-truths in the original HumanEval, we found over **10%** of them are incorrectly implemented. Therefore, as another contribution we carefully re-implemented and tested all ground-truths for HumanEval\({}^{+}\). As an option to speed up evaluation, we build HumanEval\({}^{+}\)-mini which is minimized from HumanEval\({}^{+}\) (smaller by \(47\)) yet preserves similar test effectiveness on the studied models. Lastly, more experimental setups are detailed in Appendix.

**Evaluation of LLMs.** Table 3 shows the pass@\(k\) when evaluating LLMs using both the base HumanEval and HumanEval\({}^{+}\). We first observe that across all LLMs, models sizes and \(k\) values, using HumanEval\({}^{+}\), almost all pass@\(k\) results **consistently drop** compared to using the base HumanEval. Notably, the performance drop is significant with up-to 23.1% (pass@1\({}^{}\)) / 19.3% (pass@1\({}^{}\)) / 24.9% (pass@10) / 28.9% (pass@100) reduction over the evaluated models. Such performance decrease is not only seen in popular open-source LLMs, such as the widely used CodeGen-16B  (18.5\(\%\) reduction) as well as the emerging CodeLlama-34B  (\(17.6\%\)) and StarCoder  (\(14.1\%\) reduction), but also observed in state-of-the-art commercial ChatGPT (\(12.6\%\) reduction) and GPT-4 (\(13.1\%\) reduction) models. Overall, our results overall confirm our hypothesis that the prior evaluation on HumanEval is not robust enough to detect wrong code synthesized by LLMs. Not only are these LLMs widely used for daily programming but they also serve as common reference points for evaluating new code synthesis techniques. As such, evaluating on a more robust benchmark such as HumanEval\({}^{+}\) is highly recommended in order to draw precise conclusions.

    &  &  \\   & Avg. & Medium & Min. & Max. & \\  HumanEval & 9.6 & 7.0 & 1 & 1052  & & \\ HumanEval\({}^{+}\) & 764.1 & 982.5 & 12 & 1,100 & 164 \\ HumanEval\({}^{+}\)-mini & 16.1 & 13.0 & 5 & 110 & \\   

Table 2: Overview of EvalPlus-improved benchmarks.

We also show that a more rigorous evaluation could yield different or totally contradictory relative results. For example, WizardCoder-CodeLlama and Phind-CodeLlama on the original HumanEval are evaluated to be no better than ChatGPT in terms of pass@1\({}^{*}\). However, HumanEval\({}^{+}\) demonstrates that the two open-source models can actually outperform the proprietary ChatGPT. Other contrary examples reflected by HumanEval\({}^{+}\) include that SantaCoder-1B surpasses InCoder-6.7B and Vicuna-7B outperforms InCoder-1.3B. Table 3 further illustrates the distribution of best-performing temperatures over different \(k\) values. Our results conforms with prior findings  that a lower temperature tends to perform better for smaller \(k\), while a higher temperature works better for larger \(k\). We also observe that the optimal temperatures seem to stay fairly consistent before and after using HumanEval\({}^{+}\); however, slight differences still exist, _e.g.,_ best temperature for CodeGen-2B on pass@10 becomes 0.2 from 0.8 after using HumanEval\({}^{+}\). Nonetheless, this motivates future research to look more closely on the effect of temperature with respect to the robustness of the evaluation tests, esp. those edge-cases.

**Effectiveness of test-suite reduction.** Based on HumanEval\({}^{+}\) which on average obtains 764.1 tests for each programming task (Table 2), our test-suite reducer (SS2.2) minimizes it to HumanEval\({}^{+}\)-min which only has 16.1 tests for each task (smaller by \(47\)). Table 4 performs leave-one-out cross validation to show the pass@1\({}^{*}\) differences over a subset of representative models studied in Table 3 (due to time/space constraints). That is, for each evaluated LLM we construct the reduced test-suite without considering its own sample kills. The **Full** column shows that the reduced test-suite can achieve almost the same pass@1\({}^{*}\) drop as HumanEval\({}^{+}\) by only using \(47\) fewer test-cases. Taking a closer look, separately performing set covering over each metric can harness the pass@1\({}^{*}\) of the base HumanEval to certain degree. Specifically, the use of empirical LLM sample killings is the most effective, leading to the same effectiveness as the full approach, but also consumes more tests than other theoretical metrics. While using coverage and mutation analysis seems to be unnecessary in addition to using sample killings, they still serve as the base guarantees for the theoretical test adequacy.

**Pass rate distribution.** Figure 3 shows for each programming task the overall pass rates on HumanEval and HumanEval\({}^{+}\) tests. The pass rate gap between HumanEval and HumanEval\({}^{+}\) shows overall HumanEval\({}^{+}\) can detect solutions that are misidentified by HumanEval for problems of all levels of difficulties. We also observe that problems in HumanEval are not equal, not only in terms of problem difficulty but also the difficulty of generating counter-examples and

    &  &  &  &  &  & ^{*}\)} \\   & & pass@1\({}^{*}\) & \#tests & pass@1\({}^{*}\) & \#tests & pass@1\({}^{*}\) & \#tests & pass@1\({}^{*}\) & \#tests & base & +extra \\  GPT-4 & N/A & 86.0 & 11.3 & 82.9 & 11.4 & 78.7 & 13.8 & **78.0** & 16.1 & 88.4 & 76.2 \\  ChatGPT & N/A & 71.3 & 11.3 & 69.5 & 11.4 & **65.2** & 13.7 & **65.2** & 16.0 & 73.2 & 63.4 \\  StarCoder & 15B & 32.9 & 11.3 & 32.9 & 11.4 & **29.3** & 13.6 & **29.3** & 15.9 & 34.1 & 29.3 \\   & 2B & 23.2 & 11.3 & 23.8 & 11.4 & **21.3** & 13.2 & **21.3** & 15.4 & 24.4 & 20.7 \\  & 6B & 28.7 & 11.3 & 29.3 & 11.4 & **25.6** & 13.2 & **25.6** & 15.4 & 29.3 & 25.6 \\  & 16B & 31.7 & 11.3 & 31.1 & 11.4 & **27.4** & 13.2 & **27.4** & 15.4 & 32.9 & 26.8 \\   & 1B & 10.4 & 11.3 & 11.0 & 11.4 & **9.1** & 13.8 & **9.1** & 16.0 & 11.0 & 9.1 \\  & 3B & 15.9 & 11.3 & 15.9 & 11.4 & **12.8** & 13.8 & **12.8** & 16.0 & 15.9 & 12.8 \\  & 7B & 18.3 & 11.3 & 18.3 & 11.4 & **16.5** & 13.8 & **16.5** & 16.0 & 18.3 & 16.5 \\  & 16B & 19.5 & 11.3 & 18.9 & 11.4 & **16.5** & 13.8 & **16.5** & 16.0 & 18.3 & 16.5 \\  Vicuna & 7B & 11.6 & 11.3 & 11.6 & 11.4 & **11.0** & 13.8 & **11.0** & 16.1 & 11.6 & 10.4 \\  & 13B & 16.5 & 11.3 & 16.5 & 11.4 & **15.2** & 13.8 & **15.2** & 16.1 & 17.1 & 15.2 \\  SantaCoder & 1.1B & 14.6 & 11.3 & 14.6 & 11.4 & **12.8** & 13.8 & **12.8** & 16.1 & 14.6 & 12.8 \\  InCoder & 1.3B & 12.2 & 11.3 & 12.2 & 11.4 & **10.4** & 13.6 & **10.4** & 16.0 & 12.2 & 10.4 \\  & 6.7B & 14.6 & 11.3 & 14.6 & 11.4 & **12.2** & 13.6 & **12.2** & 16.0 & 15.9 & 12.2 \\  GPT-J & 6B & 12.2 & 11.3 & 12.2 & 11.4 & **10.4** & 13.8 & **10.4** & 16.0 & 12.2 & 10.4 \\  GPT-Neo & 2.7B & 7.3 & 11.3 & 7.3 & 11.4 & **6.7** & 13.8 & **6.7** & 16.1 & 7.9 & 6.7 \\  PolyCoder & 2.7B & 6.1 & 11.3 & 6.1 & 11.4 & **5.5** & 13.8 & **5.5** & 16.1 & 6.1 & 5.5 \\  StableLM & 7B & **2.4** & 11.3 & **2.4** & 11.4 & **2.4** & 13.8 & **2.4** & 16.1 & 2.4 & 2.4 \\   

Table 4: Reduced test-suite for HumanEval\({}^{+}\). We first show the pass@1\({}^{*}\) and average #tests (including base HumanEval tests) by only doing set covering over each considered metric separately (§2.2). The **Full** column then shows the final reduction result by combining all of the three. For reference, the average #tests of original HumanEval and HumanEval\({}^{+}\) are 9.6 and 774.8 respectively (Table 2).

edge-cases to deeply exercise LLM-generated code. For simple problems such as "adding two numbers" and "length of a string" (_i.e.,_ problems with top-2 pass rates), it is easy to solve for LLMs and to test manually. While problems dealing with _multiple conditions_ (_e.g.,_ "word splitting"), _completeness_ (_e.g.,_ handling negative numbers for "is-prime"), _reasoning ability_ (_e.g.,_ "Tribonacci sequence") and _efficiency requirements_ (_e.g.,_ "n-th prime Fibonacci number") are the hardest tasks to the evaluated LLMs, positioning future research to improve LLMs for conquering such coding skills.

**Incorrect "ground-truth" in HumanEval.** In addition to detecting wrong code from LLMs using EvalPlus, we also found **18** defects (11% of problems) even in the original ground-truth in HumanEval, including _(i) Unhandled edge-case_: five prior ground-truths fail to handle corner-case inputs (_e.g.,_ empty list or string); _(ii) Bad logic_: 10 prior ground-truths incorrectly implement the desired functionality; and _(iii) Performance issue_: three inefficient implementations lead to slow performance on reasonably-sized inputs. Among those, bad logic (10) is the most serious as the original "ground-truth" does not accurately reflect the user intent. Such defects are detected also through differential testing but between our own re-implemented ground-truth and the original ground-truth in HumanEval.

Figure 4 shows an incorrect ground-truth implementation (validate_date) from HumanEval classified as having bad logic. The desired task is to check if the input date format is correct. We see that in the core logic, the conditions attempt to first check the month condition and then handle the corresponding day conditions. However, this is implemented incorrectly as "and" in Python5 has higher precedence than "or", leading to the ground-truth function to check if _either_ conditions satisfies instead of the desired _both_ conditions must satisfy. This is exposed via our automatically generated test input of 12-31-1999 where the ground-truth implementation incorrectly labels this as not a valid date. Surprisingly this egregious error is not exposed by any of the base test inputs in HumanEval, further demonstrating the weakness and limited evaluation power of the original test inputs.

## 4 Related Work

**LLMs for code**. The use of LLMs for code has gained traction in recent years, owing to the abundance of open codebase and the need for improving developer efficiency. LLMs have demonstrated state-of-the-art performance on various code-related tasks, including code generation , program repair , automated testing , code translation  and code summarization . In particular, prominent LLMs including Codex, CodeGen , InCoder and PolyCoder, have been developed and extensively evaluated for code

Figure 4: Exemplary incorrect-logic ground-truth solution in HumanEval (#124)

Figure 3: Pass rate distribution. X-axis spans bars for all 164 problems, sorted by the HumanEval pass rate. Y-axis shows the _log_-scale pass rates averaged by all LLM-generated samples.

generation (widely recognized as the holy grail for computer science research since the inception of AI in the 1950s ), where the model generates code snippets based on natural language descriptions (_e.g.,_ docstring) of the desired functionality.

**Coding benchmark for LLMs.** LLM-based code synthesis is largely evaluated based on functional correctness, which is typically assessed by running test-cases to check the desired outputs. HumanEval is one of the pioneering and most widely studied human-written benchmarks for LLM-based code synthesis, consisting of 164 pairs of Python function signature with docstring and the associated test-cases for correctness checking. Additionally, each HumanEval problem is also equipped with a reference solution. Another Python-focused dataset, MBPP , is created by crowd-sourcing participants to write in summation 974 programming problems, each of which is comprised of the problem statement (_i.e.,_ docstring), the function signature, as well as _three_ test-cases. Beyond Python, there are other benchmarks targeting additional languages such as Spider  (SQL), HumanEval-X  (C++, Javascript and Go), CodeContests  (C++ and Java) and MultiPL-E  (extending HumanEval and MBPP to 18 programming languages). More recently, researchers have created a more realistic code synthesis benchmark by collecting GitHub issues along with the corresponding code base together with tests to measure the ability of LLMs to perform real-world software engineering tasks . Our work shows for the first time the test inadequacy problem of widely studied benchmarks and addresses the issue via automatic test generation.

**Automated test generation**. Automated test generation is a widely used for finding software bugs with automatically generated tests. _Black-box_ test generation such as fuzz testing  feeds random inputs (_e.g.,_ random bytes) to the system under test (SUT), without knowing its source code. Traditional black-box techniques can mainly be categorized into generation-based  and mutation-based  ones. _White-box_ approaches provide better-quality test-cases by analyzing the source code of SUT. For instance, symbolic execution  breaks the coverage plateaus by solving symbolic path constraints to generate tests targeting deep paths. As a mid-point, coverage-guided fuzzing  (_i.e.,_ grey-box) uses the coverage information of SUT as feedback to adjust the input generation and mutation. The discussed traditional methods are inapplicable to generating semantically meaningful inputs for arbitrary problems programmed in a dynamically-typed language. We address this by using ChatGPT to inspect the ground-truth (_i.e.,_ white-box) for initializing interesting seeds, based on which type-aware mutation (_i.e.,_ black-box) scales the test inputs to a large amount.

## 5 Conclusion & Future Work

We present EvalPlus - a rigorous evaluation framework for program synthesis, driven by automated test generation. EvalPlus combines both LLM- and mutation-based input generation to obtain a diverse set of test inputs for accurately evaluating the correctness of LLM-generated code. EvalPlus creates HumanEval\({}^{+}\), built on top of the popular HumanEval with additional high-quality and automatically generated test inputs. With test-suite reduction, EvalPlus also produces HumanEval\({}^{+}\)-mini which is smaller than HumanEval\({}^{+}\) by \(47\) while preserving similar test effectiveness. We extensively evaluate a diverse set of LLMs and show that HumanEval\({}^{+}\) can identify a significant amount of previously undetected wrong code generated by LLMs, demonstrating its effectiveness to augment programming benchmarks for more accurate evaluation.

Since launched, the EvalPlus PyPI package has been installed by over 6k times in 5 months. We also keep evaluating new models for code and maintain a leaderboard at [https://evalplus.github.io/leaderboard.html](https://evalplus.github.io/leaderboard.html). In the future, we plan to apply EvalPlus to bring better-quality testing for more code benchmarks such as MBPP. Meanwhile. future work can look into how to integrate EvalPlus with more formal verification (_e.g.,_ Dafny ) or validation techniques (_e.g.,_ translation validation ) to provide stronger guarantees of the evaluation results when applicable. Additionally, the core test generation technique behind can be even used to remind developers of potential flaws of the accepted LLM-generated code snippets when doing AI pair-programming (_e.g.,_ Copilot ).

## 6 Acknowledgements

This work was partially supported by NSF grants CCF-2131943 and CCF-2141474, as well as Kwai Inc. We thank the reviewers for their invaluable feedback. We further thank Yinlin Deng for providing helpful discussions, as well as Junhao Wang and Songrun Xie for their open-source contributions.