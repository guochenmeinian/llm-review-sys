# Inverse M-Kernels for Linear Universal

Approximators of Non-Negative Functions

Hideaki Kim

NTT Corporation

hideaki.kin@ntt.com

###### Abstract

Kernel methods are widely utilized in machine learning field to learn, from training data, a latent function in a reproducing kernel Hilbert space. It is well known that the approximator thus obtained usually achieves a linear representation, which brings various computational benefits, while maintaining great representation power (i.e., universal approximation). However, when non-negativity constraints are imposed on the function's outputs, the literature usually takes the kernel method-based approximators as offering linear representations at the expense of limited model flexibility or good representation power by allowing for their nonlinear forms. The main contribution of this paper is to derive a sufficient condition for a positive definite kernel so that it may construct flexible and linear approximators of non-negative functions. We call a kernel function that offers these attributes an _inverse M-kernel_; it is a generalization of the inverse M-matrix. Furthermore, we show that for a one-dimensional input space, universal exponential/Abel kernels are inverse M-kernels and construct linear universal approximators of non-negative functions. To the best of our knowledge, it is the first time that the existence of linear universal approximators of non-negative functions has been elucidated. We confirm the effectiveness of our results by experiments on the problems of non-negativity-constrained regression, density estimation, and intensity estimation. Finally, we discuss issues and perspectives on multi-dimensional input settings.

## 1 Introduction

Non-parametric estimation of latent functions continues to be of theoretical and practical importance in a wide spectrum of disciplines such as signal/image processing , system control , geostatistics , bioinformatics , and clinical research . Kernel method, one of the most established techniques, learns flexible function approximators by embedding data points into higher dimensional reproducing kernel Hilbert spaces (RKHSs) . For a broad class of learning problems, kernel methods invoke the representer theorem  and recast the infinite-dimensional functional problems as their finite-dimensional counterparts, where the obtained approximators have linear representation, i.e., finite linear combinations of kernel functions evaluated on data points. Significant computational benefits are attained by the linear representation such as convex optimization and cheap evaluation/integration of approximators.

In recent years, great attention has been paid to kernel methods with non-negativity constraints on function outputs ; crucial applications include non-negativity-constrained regression, density estimation, and intensity estimation. Compared to unconstrained alternatives, non-negativity-constrained kernel methods developed to date are faced with a problematic trade-off between linearity and flexibility: the obtained approximators either can have linear representations at the expense of degraded representation power , or achieve good representation power (i.e., universal approximation) by accepting nonlinear forms [17; 21], which incurs substantial computation costs. To the best of our knowledge, no non-negativity-constrained kernel method has been proposed that combines linear representation with good representation power.

In this paper, we propose the first linear universal approximator of non-negative functions for one-dimensional input spaces. First, we derive a sufficient condition so that the kernel can construct a linear approximator of a non-negative function. We call a kernel that satisfies this novel condition an _inverse M-kernel_; it is a generalization of inverse M-matrix . Next, we show that exponential/Abel kernels, which have the universal approximating property [19; 30], are inverse M-kernel functions and can construct linear universal approximators of non-negative functions for one-dimensional input spaces. It is worth noting that the most popular Gaussian kernels do not satisfy the condition demanded by the inverse M-kernel. Our results shed light on exponential kernels, which have received less attention in the literature, as universal kernels for non-negativity-constrained approximators on one-dimensional input spaces.

In Section 2, we outline related works and introduce some known results on M-matrix theory used throughout the paper. In Section 3, we introduce the inverse M-kernel and construct linear universal approximators of non-negative functions. In Section 4, we show some important applications of our results, which include non-negativity-constrained regression, density estimation, and intensity estimation, and evaluate the effectiveness of our proposal on synthetic data1. Finally, Section 5 states our conclusions and discuss future works on multi-dimensional input settings.

## 2 Background

### Kernel Method-Based Linear Approximator

Let \(\) be a prescribed input space and \(k:\) be a positive semi-definite kernel. Then there exists a unique reproducing kernel Hilbert space (RKHS) \(_{k}\)[26; 29] associated with kernel \(k(,)\). Given a set of \(N\) points \(\{x_{n}\}_{n=1}^{N}\) and a regularized learning problem:

\[_{f_{k}}L(f(x_{1}),,f(x_{N}))+(||f||_{_{k}}^{2}), \]

where \(L:^{N}\) is a loss function, and \(\) is a non-decreasing function of the squared RKHS norm of \(f\). It is well known that the solution of (1) invokes the _representer theorem_[27; 35] and has the representation,

\[f^{*}()=_{n=1}^{N}_{n}k(x_{n},). \]

For simplicity, the linear regularizer, \((||f||_{_{k}}^{2})=r||f||_{_{k}}^{2}\) for \(r 0\), is assumed in this paper. The infinite-dimensional optimization problem (1) can be reduced to a finite-dimensional one in terms of coefficients \((_{1},,_{N})^{}^{N}\) as follows:

\[_{^{N}}L()+r\;^{}, [k(x_{n},x_{n^{}})]_{nn^{}}^{N  N}. \]

Under universal kernels [19; 30], where RKHSs are dense in the space of all continuous functions of \(\), the linear approximator (2) can approximate any continuous function on \(\), which is a property known as the _universal approximation_. Each evaluation of the objective function (3) and the linear approximator (2) needs the computation of \((N^{2})\) and \((N)\), respectively.

When non-negativity constraints are imposed on target function \(f 0\), the literature considers that the linear approximator (2) cannot be applied directly because it generally has negative values, even under non-negative kernel \(k(,) 0\). Conventional approaches to address the problem are listed below (Section 2.2-2.3).

### Linear Approximators with Non-Negativity Constraints

Non-negative coefficients Model (NCM)  enforces the linear approximator (2) to be non-negative by using non-negative coefficients and kernel functions,

\[f_{}()=_{n=1}^{N}_{n}k(x_{n},),_{1}, ,_{N}\!\!0,\;\;\;k(,) 0, \]

where coefficients \(^{N}\) are obtained by solving the optimization problem (3) with constraint \(_{n} 0\). Here non-negative kernels include popular kernels such as Gaussian kernels \(e^{-||x-x^{}||^{2}}\), exponential kernels \(e^{-||x-x^{}||}\), and Cauchy kernels \((1+||x-x^{}||^{2})^{-1}\). Although NCM enjoys great computational benefits from its linear representation such as the preservation of loss functional's convexity and cheap evaluations/integrations, it suffers from low representation power due to the strong non-negativity constraint on coefficients, \(_{n} 0\) (for details, see Appendix B).

Note that linear approaches with partial non-negativity constraints [5; 17; 22], which require non-negativity only on a finite number of points (the points are not necessarily data points \(\{x_{n}\}_{n=1}^{N}\)), do not guarantee the non-negativity at locations other than the points; these approaches are out of scope of this paper.

### Nonlinear Approximators with Non-Negativity Constraints

An elegant quadratic form of the non-negative model (QNM)  has recently been proposed that exploits the non-negativity inherent in positive semi-definite operators:

\[f_{}()=_{n,n^{}=1}^{N}_{nn^{}}k(x_{n}, )k(x_{n^{}},),^{N N},\;\;\; 0, \]

where \( 0\) represents the positive semi-definite constraint. QNM has the following beneficial properties: it preserves the convexity of the loss functionals; it can be integrated in a closed form if we know how to integrate kernel functions; under mild conditions on kernels, it is a universal approximator for non-negative functions. The coefficient matrix \(\) is obtained efficiently by solving an \(N\)-dimensional optimization problem, which naively costs \((N^{3})\) for each evaluation of the objective function (for details, see Appendix A). An evaluation of the approximator (5) needs the computation of \((N^{2})\).

Generalized linear models (GLMs), another nonlinear approach to constructing a non-negative function, use nonlinear transformation of a linear model . GLMs are so flexible that they can represent a wide class of non-negative functions, but they generally do not preserve the convexity of loss functionals where they are used and cannot be integrated in closed form. However, most recently, a promising model called squared neural family (SNF) , which was specifically designed for density estimation, has been proposed that uses a quadratic transformation,

\[f_{}()dx=)}g(t( );)^{2}, g(t;)=( {W}t+b),\;\;=(,,b), \]

where \(d()\) is a non-negative measure, \(g(|)\) is a neural network of one hidden layer with activation function \(()\) and parameter \(\), \(t()\) is a sufficient statistic, and \(z()\) is the normalizing constant, \(z()=_{}||g(t(x);)||^{2}d(x)\). Thanks to [[MEANING IS UNCLEAR the theory of the neural network, \(\)?? Gaussian process kernels ,]l the integrations of SNF (i.e., \(z()\)) can be executed in a closed form under various \(d(),t()\), and \(()\). But SNF has some possible drawbacks: it cannot preserve the convexity of the loss functionals, and so can yield many local optima; it has many hyper-parameters to be determined such as \(()\) and the size of parameter \(\), on which model performance largely depends. In this paper, we adopted the same size of \(\) as in : \(^{1 30}\), \(^{30 dim()}\), \(b^{30}\), and \(t(x)=x\).

### M-Matrix Theory

Here we introduce some existing results on M-matrix theory, which are then used to clarify a sufficient kernel condition so that it may construct a linear approximator of non-negative functions.

The \(n\)-by-\(n\) real matrix \(^{n n}\) is called an _M-matrix_ if it has the form \(_{n}-\), in which \(_{n}^{n n}\) is the identity matrix of size \(n\), \(^{n n}\) is an entry-wise non-negative matrix, and \(>()\), the spectral radius of \(\); this is equivalent to \(\) with non-positive off-diagonal entries that is invertible and having an entry-wise non-negative inverse. An entry-wise non-negative matrix that occurs as the inverse of an M-matrix is called an _inverse M-matrix_. Note that the inverse of an M-matrix is always entry-wise non-negative, while the reverse is not always true. We denote the classes of M-matrices and inverse M-matrices by \(\) and \(^{-1}\), respectively. Lemma 1 below shows useful properties on the entry-wise sign patterns of partitioned inverse M-matrices (for the proof, see Theorem 8 in ).

**Lemma 1**.: _Suppose that an \(n\)-by-\(n\) symmetric inverse M-matrix \(^{-1}\) is partitioned as_

\[=_{1}&_{3}^{}\\ _{3}&_{2},_{1}^{n_{1} n _{1}},\,_{2}^{n_{2} n_{2}},\,_{3}^ {n_{2} n_{1}},\]

_where \(n_{1}\) and \(n_{2}\) are positive integers satisfying \(n=n_{1}+n_{2}\). Then the following inequalities hold:_

\[0_{3}^{}_{2}^{-1}, 0_{1}- _{3}^{}_{2}^{-1}_{3}^{-1},\]

_where \(()\) represents the entry-wise inequality of matrices/vectors._

Proof.: Suppose that the inverse of \(\), denoted by \(}\), is partitioned as

\[}=}_{1}&}_{3}^{}\\ }_{3}&}_{2},}_{1} ^{n_{1} n_{1}},\,}_{2}^{n_{2} n_ {2}},\,\,}_{3}^{n_{2} n_{1}}.\]

Then \(}_{1}\) and \(}_{3}^{}\) may be expressed by using Schur's complements as

\[}_{1}=(_{1}-_{3}^{}_{2}^{-1}_{3})^{-1}, }_{3}^{}=-(_{1}-_{3}^{}_{2}^{-1}_{3})^{-1}_{3}^{}_{2}^{-1}.\]

Because \(}_{1}\), a principal submatrix of M-matrix \(\), is an M-matrix (Corollary 3 in ), \(_{1}-_{3}^{}_{2}^{-1}_{3}=}_{1}^{-1}\) is an inverse M-matrix and entry-wise positive. Furthermore, \(_{3}^{}_{2}^{-1}=-(_{1}-_{3}^{}_{2}^{-1} _{3})_{3}^{}\) is an entry-wise positive matrix because \((_{1}-_{3}^{}_{2}^{-1}_{3})^{-1}\) and \(-}_{3}^{}\) are both entry-wise positive matrices. This completes the proof. 

## 3 Inverse M-Kernels

In this section, we define a new class of kernels and show that it plays an essential role in constructing a linear and flexible approximator of non-negative functions.

**Definition 1** (Inverse M-kernels).: _Let \(k:_{+}\) be a positive semi-definite kernel that outputs non-negative values, \([k(x_{n},x_{n^{}})]_{nn^{}}\) be a gram matrix constructed for any set of \(N\) points \((x_{1},x_{2},,x_{N})\), and \(s:_{+}\) be a non-negative function of data size \(N\). We call \(k(,)\) an inverse M-kernel if \(+s(N)_{N}^{-1}\). We denote the class of inverse M-kernels by \(_{^{-1}}^{s(N)}\). Also, if \(s(N)=0\) or \(^{-1}\), we call the kernel a strict inverse M-kernel, which we denote by \(k(,)_{^{-1}}\)._

The non-negative function \(s(N)\) may generally exhibit various scalings with respect to \(N\), but as will be discussed later, smaller scalings offer greater advantages. In this paper, we will focus solely on examples with scalings of \((1)\) and \((N)\).

### Inverse M-Kernel Models

We now consider the following linear approximator with an inverse M-kernel:

\[f_{}()=_{n=1}^{N}_{n}k(x_{n},)=()^{ },(+s(N+1)_{N}) 0,\,\,\,k(, )_{^{-1}}^{s(N)}, \]

where \(\{x_{n}\}_{n=1}^{N}\) is the \(N\) data points, \((_{1},,_{N})^{}\), and \((x)(k(x_{1},x),,k(x_{N},x))^{}\). Coefficients \(\) are obtained by solving the optimization problem (3) with constraint \((+s(N+1)_{N}) 0\). We call this approximator an _inverse M-kernel model_ (IMK). Then Theorem 1 below guarantees the non-negativity of the approximator (7).

**Theorem 1** (Non-negativity of inverse M-kernel models).: _The inverse M-kernel models \(f_{}(x)\) defined by (7) are non-negative for any input point \(x\)._

Proof.: Consider the (\(N\)+1)-by-(\(N\)+1) gram matrix for the data points \(\{x_{n}\}_{n=1}^{N}\) and any point \(x\) such that

\[=k(x,x)&(x)^{}\\ (x)&+s(N+1)_{N+1}.\]

Gram matrix \(Q\) is an inverse M-matrix because \(k(,)_{-1}^{s(N)}\), and according to the first inequality in Lemma 1, the following relation holds: \((x)^{}(+s(N+1)_{N})^{-1} 0\). Let \(_{+}^{N}\) be a non-negative vector, \( 0\), then the following inner product of non-negative vectors completes the proof: \((x)^{}(+s(N+1)_{N})^{-1}=(x)^{} 0\), for \(=(+s(N+1)_{N}) 0\). 

It should be emphasized here that the comparison between the two linear models, \(f_{}\) in (7) and \(f_{}\) in (4), suggests that our proposed \(f_{}\) should have substantially greater representation power than \(f_{}\): \(f_{}\)'s constraint on coefficient, \((+s(N+1)_{N}) 0\), is much weaker than \(f_{}\)'s, \( 0\). Here, the discrepancy between the two models is controlled by \(s(N+1)\), and \(f_{}\) reduces to \(f_{}\) for \(s(N+1)\): \((+s(N+1)_{N}) 0(s(N+1)^{-1}+ _{N}) 0 0\). Clearly, \(f_{}\) has greatest representation power when \(s(N+1)\) is equal to zero, and we can derive a sufficient condition on \(f_{}\) so that it may be a universal approximator of non-negative function.

**Theorem 2** (Condition for linear universal approximation).: _The inverse M-kernel model \(f_{}\) defined by (7) is a universal approximator of non-negative functions if the kernel is universal and a strict inverse M-kernel._

Proof.: Let \(k:\) be a universal kernel, and let \(\) be a compact subset of \(\). Then the corresponding RKHS is equal to the space of all continuous functions from \(\), denoted by \(()\), which is equipped with maximum norm \(||||_{()}\). Suppose that we have a set of data points, \(\{(x_{n},g(x_{n}))\}_{n=1}^{N}\), for non-negative target function \(g:_{+}\) in \(()\), and \(k(,)\) is a strict inverse M-kernel \(k(,)_{^{-1}}\). Then we can rewrite the inverse M-kernel model (7) in the form of noise-free kernel ridge regression (KRR) as:

\[f_{}(x)=(x)^{}^{-1}, (g(x_{1}),,g(x_{N}))^{} 0.\]

Because constraint \( 0\) is satisfied for any \(\{x_{n}\}_{n=1}^{N}\) due to the non-negativity of \(g()\), we can apply the generalization error bound of a normal KRR (Proposition 1 in ) to it:

\[||f_{}()-g()||_{()}<_{x }(x)^{}^{-1}(x)}\ \ \ ,\]

where \(\) is a constant. The upper bound goes to zero if \(N\) and \(\{x_{n}\}_{n=1}^{N}\) is aligned appropriately, indicating that given \(>0\), there exists \(\{x_{n}\}_{n=1}^{N}\) such that \(||f_{}()-g()||_{()}\), which completes the proof. 

### Equivalent Inverse M-Kernels for Permanental Processes

As a by-product of inverse M-kernels, we can address a well-known _nodal line problem_ on Poisson intensity estimation with reproducing kernels : Given a set of \(N\) points \(\{x_{n}\}_{n=1}^{N}\) observed for compact domain \(\), intensity function \((x)\), an instantaneous probability of events occurring at each point on \(\), is estimated by a linear model with the equivalent kernel function \(h(,)\) so that

\[(x)=f^{2}(x), f(x)=_{n=1}^{N}h(x,x_{n})v_{n}^{*}, v^{*}= _{v^{N}}-_{n=1}^{N} f(x_{n})+r\ v^{}v, \]

where \([h(x_{n},x_{n^{}})]_{nn^{}}\), and \(h(,)\) solves an integral equation constructed by kernel function \(k(,)\) as \(h(x,x^{})+2/r_{}k(x,s)h(s,x^{})ds=k(x,x^{})\); \(f()\) generally may have negative values, which causes many local modes since \( f()\) can lead to similar intensity \(()=f^{2}()\), resulting in artificial zero crossings of \(f()\), especially on locations where the intensity is low. If \(h(,)\) is a strict inverse M-kernel, then the linear model of \(f()\) can constitute an inverse M-kernel model (7), which is non-negative at any \(x\) under a weak constraint of coefficient, \(v 0\).

We now consider solving the integral equation for \(h(,)\) with the naive approach , which approximates the integral operator by \(J\)-point numerical integration, resulting in

\[h(x,x^{})=k(x,x^{})-_{J}(x)^{}w_{J}+_{J }^{-1}_{J}(x^{}), w=rJ/(2||), \]

where \(_{J}(x)(k(x,q_{1}),,k(x,q_{J}))^{}\), \(_{J}[k(q_{j},q_{j^{}})]_{jj^{}}\), \(||_{}dx\), and \((q_{1},,q_{J})\) is the regularly aligned evaluation points. Theorem 3 below shows a sufficient condition on the equivalent kernel such that it may be a strict inverse M-kernel.

**Theorem 3** (Equivalent inverse M-kernels).: _The equivalent kernel \(h(,)\) defined by (9) is a strict inverse M-kernel if the corresponding kernel \(k(,)\) is a strict inverse M-kernel._

Proof.: Consider the (_J+_2)-by-(_J+_2) gram matrix for the points \(\{q_{j}\}_{j=1}^{J}\) and any pair of different points \((x,z x)\) such that

\[=+,=(k(x,x)&k(x,z)&_{J}(x)^{}\\ k(z,x)&k(z,z)&_{J}(z)^{}\\ _{J}(x)&_{J}(z)&_{J}),= 0,0,w,,w.\]

If \(k(,)_{^{-1}}\), then \(^{-1}\) and \(=+^{-1}\) because of the additive diagonal closure of inverse M-matrices (Theorem 3 in ). Applying the second inequality in Lemma 1 to \(Q\) leads to the relation: \(0 k(x,z)-_{J}(x)^{}w_{J}+_{J}^{-1}_{J}(z)_{^{-1}}\) for any pair of points \((x,z)\), which completes the proof. 

Gaussian Cox processes (GCPs) are the gold standard for intensity estimation, and the intensity estimator (8) is the MAP solution of the permanental process , a variant of GCP where the square root of the intensity function is assumed to be generated from a Gaussian process. In the literature on GCPs, the advantage of permanental processes over other GCPs has been considered as the efficient estimation algorithm, and the equivalent kernels constructed by inverse M-kernels may improve the predictive performance of the fast-to-compute permanental processes by weakening the coefficient constraints. 

### Construction of Inverse M-Kernels

In the former sections, we defined a new class of kernel or inverse M-kernel, and showed some beneficial results obtained from its unique properties. Now we need to tackle a practical problem of how to construct the inverse M-kernels. Our conclusion in this paper is that for one-dimensional input space (\(\)), we can find some strict inverse M-kernels, which include a well-known universal kernel called exponential/Abel/Laplace kernel; For a multi-dimensional input space, we can find some inverse M-kernels, but strict ones have yet to be discovered. In the following sections, we focus on the scenario of a one-dimensional input space, which is followed by discussions of issues with and perspectives on multi-dimensional input setting.

**Corollary 1** (Examples of strict inverse M-kernels).: _Exponential kernel \(k_{}(x,x^{})=e^{-|x-x^{}|/}\) and intersection kernel \(k_{}(x,x^{})=(x,x^{})-\), defined on one-dimensional space \(x,x^{}\), are strict inverse M-kernels. Here, \(\) and \(\) are the hyperparameters of exponential and intersection kernels, respectively._

Proof.: Given a set of points \((x_{1},,x_{N})\) sorted in ascending order \(x_{n}<x_{n^{}}\) for \(n<n^{}\), the inverse gram matrices of exponential and intersection kernels, denoted by \(_{}^{-1}\) and \(_{}^{-1}\), respectively, are of tridiagonal form:

\[(_{}^{-1})_{nn^{}}=p_{n-1}^{n }\,p_{n}^{n+1}/p_{n-1}^{n+1}&:|n-n^{}|=0\\ -^{n^{}}(p_{n}^{n^{}}-1)}&:|n-n^{}|=1\;, p_ {n}^{n^{}}=-x_{n^{}}|/}}&: n,n^{}_{N}\\ 1&:,\\ 0&:|n-n^{}|>1\] \[(_{}^{-1})_{nn^{}}=q_{n-1}^{n }\,q_{n+1}^{n+1}/q_{n-1}^{n+1}&:|n-n^{}|=0\\ -q_{n}^{n+1}&:|n-n^{}|=1\;, q_{n}^{n^{}}=}-x_{n}}&:n,n^{}_{N}\\ }-}&:n=0\\ 1&:n^{}=N+1,\]where \(_{N}=\{1,2,,N\}\), \(>0\), and \(<x_{1}\). The results show that \(_{ exp}^{-1}\) and \(_{ int}^{-1}\) have non-positive off-diagonal entries while their inverses are entry-wise non-negative due to the corresponding kernels' properties, indicating that \(_{ exp},_{ int}^{-1}\) for any \(\{x_{n}\}_{n=1}^{N}\). Therefore, \(k_{ exp},k_{ int}_{^{-1}}\). \(\)

Because exponential kernel \(k_{ exp}(,)\) is a universal kernel as well as a strict inverse M-kernel, Theorem 2 suggests that an inverse M-kernel model with \(k_{ exp}(,)\) constitutes a linear universal approximator for one-dimensional input spaces. It should be emphasized here that more popular kernels such as Gaussian and Matern kernels are not inverse M-Kernels, and thus they cannot be used to construct linear universal approximators with non-negativity constraints, which highlights an important benefit of \(k_{ exp}(,)\) that has been overlooked in the literature. However, linear models with \(k_{ exp}(,)\) are generally not smooth at data points, which is a possible disadvantage given that conventional nonlinear models can employ smooth kernels. It remains to be clarified whether there exists an inverse M-kernel that is more smooth than \(k_{ exp}(,)\).

It is easily verified that linear models with intersection kernels  constitute piece-wise linear splines. By exploiting the fact that piece-wise linear splines, whose finite set of knot values are non-negative, have non-negative values globally, Maatouk and Bay  proposed a Gaussian process (or equivalently a kernel method) model with non-negativity constraints, which our result re-confirms from the perspective of (strict) inverse M-kernels.

We derived the tridiagonal formulae in Corollary 1 by using some known properties of symmetric Toeplitz matrices (e.g., see ) as a reference. It is clear that the derived tridiagonal formula is a straightforward generalization of the inverse of Toeplitz matrices, but we cannot find any references that explicitly mention the tridiagonal formulae of one-dimensional exponential and intersection kernels.

## 4 Experiments

We examined the validity of our proposal by comparing it with conventional linear and nonlinear models on synthetic data. Here, we considered the three problems of non-negativity-constrained regression, density estimation, and intensity estimation. As benchmark models, we adopted non-negative coefficients model (NCM) in (4) and quadratic form of non-negative model (QNM) in (5) for non-negativity-constrained regression; we adopted NCM, QNM, and squared neural family (SNF) in (6) for density estimation; we adopted the intensity estimator with Gaussian kernels (IEK)  and the structured variational Bayesian approach with sigmoidal Gaussian Cox processes (STVB)  for intensity estimation. As our proposal, we adopted inverse M-kernel model (IMK) in (7) for non-negativity-constrained regression and density estimation, and intensity estimator with in

    &  &  &  \\ \(\) & \(l^{2}\) & \(cpu(sec)\) & \(l^{2}\) & \(cpu(sec)\) & \(l^{2}\) & \(cpu(sec)\) \\ 
0.1 &.078 \(\).043 &.002 \(\).002 &.034 \(\).011 &.634 \(\).409 &.047 \(\).012 &.002 \(\).001 \\
0.01 &.074 \(\).044 &.001 \(\).001 &.002 \(\).002 & 3.49 \(\) 1.04 &.011 \(\).001 &.003 \(\).001 \\   

Table 1: Results on KdV data across 100 trials with standard errors. \(l^{2}\) is the integrated squared error between the approximator and the ground truth, and \(cpu\) is the CPU time in second.

Figure 1: Estimated non-negative functions on KdV data with small noise, \(=0.01\).

verse M-kernels for intensity estimation. We employed Gaussian kernel \(k(x,x^{})=e^{-|x-x^{}|^{2}/^{2}}\) for NCM and QNM, and exponential kernel \(k(x,x^{})=e^{-|x-x^{}|/}\) for our IMK. The hyper-parameters for each model were optimized through three-fold cross validation on a grid: for NCM, QNM, and IMK, the grid is \((,r)\) for \(=\{0.1,0.2,0.5,1,2,5,10\}\); for SNF, the number of components for Gaussian mixture measure \(d()\) was selected from \(\{1,2,3\}\). We implemented all compared models by using Python-3.10.8 (SciPy-1.11, fnns-1.0 (MIT License))1. A MacBook Pro with 12-core CPU (Apple M2 Max) was used.

### Non-Negativity-Constrained Regression

We considered a standard regression problem with the squared loss functional, \(L=}_{n=1}^{N}(y_{n}-f(x_{n}))^{2}\), which makes the optimization problems in (3 and A1) convex. Here \(x=(x_{1},,x_{N})^{}\) and \(y=(y_{1},,y_{N})^{}\) are the observed input and target values, respectively, and \(^{2}\) is the variance of observation noise. For NCM and IMK, each of the convex problems of coefficients \(\) can be recast to non-negative least squares as

\[:&_{ 0}\|-z||^{2},& =[/;^{}],&z=[y/;0_{N}],\\ :&_{ 0}\|-z||^{2},&=[/;^{-1}],&z=[y/;0_{N}],&=^{-1}, \]

where \(\) is the lower triangular matrix of the Cholesky decomposition of the gram matrix, \(=^{}\), \(0_{N}\) is the \(N\)-dimensional vector with zero entries, and \([a;b]\) represents concatenation. We solved (10) with the fast nonnegative least squares . For QNM, we solved the convex problem of coefficients \(\) by using the sequential least squares programming (SLSQP) .

In accordance with , we considered approximating a non-negative 2-soliton solution of the Korteweg-de Vries (KdV) equation , \(g(x,t=1)=}\), where the posterior means of unconstrained Gaussian process (equivalently, kernel method regressions) tend to violate non-negativity of the function . We sampled \(N\) = \(40\) data points equidistantly from the KdV solution with small noise \(=0.01\) and large noise \(=0.1\) scenarios, each of which was conducted \(100\) times. Then we measured the predictive performances of the models based on the integrated squared error between the result \(f^{*}()\) and the ground truth \(g(x)\), defined as \(l^{2}=_{a}^{b}|f^{*}(x)-g(x)|^{2}dx\) for \((a,b)=(-20,5)\).

Table 1 displays the predictive performances achieved by the compared methods on KdV data. The comparison between the two linear models shows that our model (IMK) outperformed NCM in both noise scenarios, which is clearly illustrated by Figure 1: our model succeeded in approximating the two modes with the different scales appearing in the KdV solution, while NCM failed due to its limited representation power. See also an additional experiment in Appendix B to illustrate the difference in representation power between the two linear models. QNM achieved the best performance among the models, because the underlying function is smooth and consistent with

    &  &  &  \\ \(d_{}\) & \(cpu(sec)\) & \(d_{}\) & \(cpu(sec)\) & \(d_{}\) & \(cpu(sec)\) & \(d_{}\) & \(cpu(sec)\) \\  \(.163.154\) & \(.024.007\) & \(.452.291\) & \(2.99.970\) & \(.163.064\) & \(.483.136\) & \(.123.049\) & \(.053.021\) \\   

Table 2: Results on density estimation across 100 trials with standard errors. \(k_{}\) is the Kullback–Leibler distance between the estimation and the ground truth (the lower, the better); \(cpu\) is the CPU time in seconds.

Figure 2: Estimated density functions.

the Gaussian kernel that QNM adopted. The difference in performance between QNM and our model tends to shrink when observations are noisy. The advantage of our method over QNM is its computation efficiency: the linearity of the model can be fully exploited to achieve learning that is hundreds of times faster than QNM.

### Density Estimation

We considered a density estimation problem with the loss functional, \(L=-_{n=1}^{N} f(x_{n})\), where \(x=(x_{1},,x_{N})^{}\) are the observed samples and the optimization problems in (3 and A1) are convex. Approximators of density functions are required to satisfy the normalization condition, which can be recast as a linear constraint in the linear models (NCM and IMK): \(h^{}=1\) for \((h)_{n}=_{}k(x,x_{n})dx\). We trained NCM, SNF, QNM, and IMK by using SLSQP .

We created 100 sets of 50 samples generated from a Gaussian mixture model: \(g(x)=0.5[(x|0,1)+\ (x|4,0.3)]\), where \((|a,b)\) represents a normal distribution with mean \(a\) and standard deviation \(b\). The predictive performances were evaluated using the Kullback-Leibler distance (the lower, the better) between the result \(f^{*}(x)\) and the ground truth \(g(x)\), defined as \(d_{}=_{a}^{b}g(x)(g(x)/f^{*}(x))dx\) for \((a,b)=(-5,5)\). Table 2 lists the results, which show that our IMK achieved better performance than NCM and comparable performance while being substantially faster than QNM. Table 2 also shows that SNF did not perform well, which might be due to overfitting to the training data, as illustrated by Figure 2. This time we assumed a small training data set (\(N=50\)), where neural network-based models are likely to overfit. More careful tuning of the hyperparameters would improve SNF's performances, while the robustness against data size is generally a great advantage of kernel methods.

### Intensity Estimation

We considered an intensity estimation problem (8), where SLSQP  was used to optimize IEK and our model. We implemented STVB with the TensorFlow code , where the number of inducing points was set as regularly aligned 100 points within the observation domain. We created 100 sets of event sequences generated from the following intensity function: \((x)=50^{2}(x)+60\)

Figure 3: Estimated intensity functions. Asterisks * represent nodal points.

    & IEK &  &  \\ \(l^{2}( 10^{3})\) & \(cpu(sec)\) & \(l^{2}( 10^{3})\) & \(cpu(sec)\) & \(l^{2}( 10^{3})\) & \(cpu(sec)\) \\ 
7.50 ± 3.06 & 6.36 ± 3.46 & 1.74 ± 0.504 & 1006 ± 54.5 & 2.01 ± 0.678 & 4.53 ± 3.15 \\   

Table 3: Results on intensity estimation across 100 trials with standard errors. \(l^{2}\) is the integrated squared error between the approximator and the ground truth, and \(cpu\) is the CPU time in seconds.

for \(x\), where John and Hensman  reported that the nodal line problem was likely to happen. The predictive performances were evaluated using the integrated squared error between the result \(^{*}(x)\) and the ground truth \((x)\), defined as \(l^{2}=_{a}^{b}|^{*}(x)-(x)|^{2}dx\) for \((a,b)=(0,5)\). Table 3 lists the results which show that our model with an equivalent inverse M-kernel achieved better performance than the naive intensity estimator with Gaussian kernel (IEK) and comparable performance to STVB while being substantially faster. Figure 3 illustrates that IEK allowed some artificial zero crossings of \((x)}\), while our model did not.

## 5 Discussions

We have proposed a novel class of kernel function, called _inverse M-kernel function_, with which we may construct flexible and linear approximators of non-negative functions. We showed that exponential kernels, which are known as universal kernels, are inverse M-kernel functions, and they can construct linear universal approximators of non-negative functions for one-dimensional input settings. We confirmed the potential benefits of our proposal experimentally on three problems: non-negative function regression, density estimation, and intensity estimation.

Future Work and LimitationsTo the best of our knowledge, this study is the first to clarify the existence of linear universal approximators of non-negative functions, although the result is limited to one-dimensional input spaces. Constructing linear and flexible (or universal, if possible) approximators with non-negativity constraints for multi-dimensional input space is equivalent to finding inverse M-kernels for multi-dimensional input spaces, the difficulty of which can be exemplified as follows. Let \(k_{0}:\) be a positive semi-definite kernel, and consider the construction of a kernel for a two-dimensional input space by a popular multiplicative approach: \(k(z,z^{})=k_{0}(x,x^{})k_{0}(y,y^{})\) for \(z=(x,y)\). A gram matrix of \(k(,)\), denoted by \(\), evaluated over a Cartesian grid of input locations, \((x_{1},,x_{n_{x}})(y_{1},,y_{n_{y}})\), will give rise to a matrix that can be written as the Kronecker product of two smaller gram matrices, each of which are formed by evaluating \(k_{0}(,)\) over each input location : \(=_{x}_{y}\). If \(k_{0}(,)\) is a strict inverse M-kernel function \(k_{0}_{^{-1}}\), then \(_{x}^{-1},_{y}^{-1}\), but \(^{-1}=_{x}^{-1}_{y}^{-1}\), that is, \(k(z,z^{})_{^{-1}}\). For example, some off-diagonal entries of \(^{-1}\) are non-negative, \((^{-1})_{1(n_{x}+2)}=(_{x}^{-1})_{12}(_{y}^{-1})_{12} 0\).

A possible solution to the above difficulty is to select a scalar \(\) large enough to satisfy the condition of inverse M-kernel (Definition 1): \(+^{-1}\). For general kernel functions, the condition is not always satisfied even under a very large \(\). However, if gram matrix \(\) satisfies a specific condition called _strict path product condition_, then a lower bound of \(\) (i.e., \(s(N)\) in Definition 1) that satisfies \(+^{-1}\) can be evaluated as follows.

**Theorem** (Theorem 4 in ).: _Let \(=(a_{ij})\) be an \(n\)-by-\(n\) entry-wise non-negative matrix with normalized unit diagonals, \(n 3\). Then \(+^{-1}\) for all \( n-3\) if \(\) satisfies the strict path product condition, \(a_{ij}a_{jk}<a_{ik}\),for all distinct indices \(i,j,k\) such that \(1 i,j,k n\)._

Actually, it is easily verified that gram matrices of multiplicative exponential kernels, \(k_{}(x,s)=_{d}e^{-|x^{d}-s^{d}|/_{d}}\), satisfy the strict path product condition regardless of the dimensionality of the input space. Therefore, the multiplicative exponential kernels are inverse M-kernels with \(s(N)=N-3\), that is, \(k_{}(,)_{^{-1}}^{N-3}\), for multi-dimensional input spaces, which suggests that the following inverse M-kernel model (IMK) is valid:

\[f_{}(x)=_{n=1}^{N}_{n}k(x_{n},x)=(x)^{}, \ \ (+(N-2)) 0,\ \ k(x,s)=_{d=1}^{D}e^{-|x^{ d}-s^{d}|/_{d}}, \]

where \(D 1\) is the input dimensionality. However, as discussed in Section 3.1, the discrepancy between NCM and IMK becomes small if \(s(N+1)\) for the condition \((+s(N+1)) 0\) is large, and thus \(s(N+1)=N-2\) implies that improvements of IMK (11) against NCM (4) should exist but are likely to be marginal for \(N 10\). Because the lower bound \( N-3\) in the theorem above is not tight, a pressing need is to develop a method to find a smaller value of \(\) satisfying \((+)^{-1}\) given kernel \(k(,)\) and input points \((x_{1},,x_{N})\).