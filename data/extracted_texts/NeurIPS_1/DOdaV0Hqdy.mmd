# Off-Policy Evaluation for Human Feedback

Qitong Gao  Ge Gao  Juncheng Dong  Vahid Tarokh  Min Chi  Miroslav Pajic

Duke University. Durham, NC, USA. Contact: {qitong.gao, miroslav.pajic}@duke.edu. North Carolina State University. Raleigh, NC, USA.

###### Abstract

Off-policy evaluation (OPE) is important for closing the gap between offline training and evaluation of reinforcement learning (RL), by estimating performance and/or rank of target (evaluation) policies using offline trajectories only. It can improve the safety and efficiency of data collection and policy testing procedures in situations where online deployments are expensive, such as healthcare. However, existing OPE methods fall short in estimating human feedback (HF) signals, as HF may be conditioned over multiple underlying factors and is only sparsely available; as opposed to the agent-defined environmental rewards (used in policy optimization), which are usually determined over parametric functions or distributions. Consequently, the nature of HF signals makes extrapolating accurate OPE estimations to be challenging. To resolve this, we introduce an OPE for HF (OPEHF) framework that revives existing OPE methods in order to accurately evaluate the HF signals. Specifically, we develop an immediate human reward (IHR) reconstruction approach, regularized by environmental knowledge distilled in a latent space that captures the underlying dynamics of state transitions as well as issuing HF signals. Our approach has been tested over _two real-world experiments_, adaptive _in-vivo_ neurostimulation and intelligent tutoring, as well as in a simulation environment (visual Q&A). Results show that our approach significantly improves the performance toward estimating HF signals accurately, compared to directly applying (variants of) existing OPE methods.

## 1 Introduction

Off-policy evaluation (OPE) aims to estimate the performance of reinforcement learning (RL) policies using only a fixed set of offline trajectories , _i.e._, without online deployments. It is considered to be a critical step in closing the gap between offline RL training and evaluation, for environments and systems where online data collection is expensive or unsafe. Specifically, OPE facilitates not only offline evaluation of the safety and efficacy of the policies ahead of online deployment, but policy selection as well; this allows one to maximize the efficiency when online data collection is possible, by identifying and deploying the policies that are more likely to result in higher returns. OPE has been used in various application domains including healthcare , robotics , intelligent tutoring , recommendation systems .

The majority of existing OPE methods focus on evaluating the policies' performance defined over the _environmental_ reward functions which are mainly designed for use in policy optimization (training). However, as an increasing number of offline RL frameworks are developed for human-involved systems , existing OPE methods lack the ability to estimate how human users would evaluate the policies, _e.g._, ratings provided by patients (on a scale of 1-10) over the procedure facilitated by automated surgical robots; as human feedback (HF) can be noisy and conditioned over various confounders that could be difficult to be captured explicitly . For example, patient satisfaction over a specific diabetes therapy may vary across the cohort, depending on many subjectivefactors, such as personal preferences and activity level of the day, while participating in the therapy, in addition to the physiological signals (_e.g._, blood sugar level, body weight) that are more commonly used as the sources for determining environmental rewards toward policy optimization [70; 33; 21; 19]. Moreover, the environmental rewards are sometimes discrete to ensure optimality of the learned policies , which further reduces its correlation against HF signals.

In this work, we introduce the OPE for human feedback (OPEHF) framework that revives existing OPE approaches in the context of evaluating HF from offline data. Specifically, we consider the challenging scenario where the HF signal is only provided at the end of each episode - _i.e._, no per-step HF signals, referred to as _immediate human rewards_ (IHRs) below, are provided - benchmarking the common real-world situations where the participants are allowed to rate the procedures only at the end of the study. The goal is set to estimate the end-of-episode HF signals, also referred to as _human returns_, over the target (evaluation) policies, using a fixed set of offline trajectories collected over some behavioral policies. To facilitate OPEHF, we introduce an approach that first maps the human return back to the sequence of IHRs, over the horizon, for each trajectory. Specifically, this follows from optimizing over an objective that consists of a necessary condition where the cumulative discounted sum of IHRs should equal the human return, as well as a regularization term that limits the discrepancy of the reconstructed IHRs over state-action pairs that are determined similar over a latent representation space into which environmental transitions and rewards are encoded. At last, this allows for the use of any existing OPE methods to process the offline trajectories with reconstructed IHRs and estimate human returns under target policies.

Our main contributions are tri-fold. **(i)** We introduce a novel OPEHF framework that revives existing OPE methods toward accurately estimating highly sparse HF signals (provided only at the end of each episode) from offline trajectories, through IHRs reconstruction. **(ii)** Our approach does not require the environmental rewards and the HF signals to be strongly correlated, benefiting from the design where both signals are encoded to a latent space regularizing the objective for reconstructions of IHRs, which is justified empirically over real-world experiments. **(iii)** Two _real-world experiments_, _i.e._, adaptive _in-vivo_ neurostimulation for the treatment of Parkinson's disease and intelligent tutoring for computer science students in colleges, as well as one simulation environment (_i.e._, visual Q&A), facilitated the thorough evaluation of our approach; various degrees of correlations between the environment rewards and HF signals existed across the environments, as well as the varied coverage of the state-action space provided by offline data over sub-optimal behavioral policies, imposing different levels of challenges for OPEHF.

## 2 Off-Policy Evaluation for Human Feedback (OPEHF)

In this section, we introduce an OPEHF framework that allows for the use of existing OPE methods to estimate the _human returns_ that are available only at the end of each episode, with IHRs remaining unknown. This is in contrast to the goal of classic OPE that only estimates the _environmental_ returns following the user-defined reward function used in the policy optimization phase. A brief overview of existing OPE methods can be found in Appendix C.

### Problem Formulation

We first formulate the human-involved MDP (HMDP), which is a tuple \(=(,,,R,R^{},s_{0},)\), where \(\) is the set of states, \(\) the set of actions, \(:\) is the transition distribution usually captured by probabilities \(p(s_{t}|s_{t-1},a_{t-1})\), \(R:\) is the _environmental_ reward function, \(R^{}(r^{}|s,a)\) is the _human_ reward distribution from which the IHR \(r^{}_{t} R^{}(|s_{t},a_{t})\) are sampled, \(s_{0}\) is the initial state sampled from the initial state distribution \(p(s_{0})\), and \([0,1)\) is the discounting factor. Note that we set the IHRs to be determined probabilistically, as opposed to the environmental rewards \(r_{t}=R(s_{t},a_{t})\) that are deterministic; this is due to the fact that many underlying factors may affect the feedback provided by humans [53; 7; 44], as we have also observed while performing human-involved experiments (see Appendix D). Finally, the agent interacts with the MDP following some policy \((a|s)\) that defines the probabilities of taking action \(a\) at state \(s\).

In this work, we make the following assumption over \(R\) and \(R^{}\).

**Assumption 1** (Unknown IHRs).: _We assume that the immediate environmental reward function \(R\) is known and \(R(s,a)\) can be obtained for any state-action pairs in \(\). Moreover, the IHR distribution \(R^{}\) is assumed to be unknown, i.e., \(r^{} R^{}(|s,a)\) are unobservable, for all \((s,a)\)._Instead, the cumulative human return \(G^{}_{0:T}\), defined over \(R^{}\), is given at the end of each trajectory, i.e., \(G^{}_{0:T}=_{t=0}^{T}^{t}r_{t}^{}\), with \(T\) being the horizon and \(r_{t}^{} R^{}(|s_{t},a_{t})\)._

The assumption above follows the fact that human feedback (HF) is not available until the end of each episode, as opposed to immediate rewards that can be defined over the environment and evaluated for any \((s_{t},a_{t})\) pairs at any time. This is especially true in environments such as healthcare where the clinical treatment outcome is not foreseeable until a therapeutic cycle is completed, or in intelligent tutoring where the overall gain from students over a semester is mostly reflected by the final grades. Note that although the setup can be generalized to the scenario where HF can be sparsely obtained over the horizon, we believe that issuing the HF only at the end of each trajectory leads to a more challenging setup for OPE. Consequently, the goal of OPEHF can be formulated as follows.

**Problem 1** (Objective of OPEHF).: _Given offline trajectories collected by some behavioral policy \(\), \(^{}=\{^{(0)},^{(1)},,^{(N-1)}|\ a_{t}(a_{t }|s_{t})\}\), with \(^{(i)}=[(s^{(i)}_{0},a^{(i)}_{0},r^{(i)}_{0},r^{}_{0},s^{(i)}_{ 1}),,(s^{(i)}_{T-1},a^{(i)}_{T-1},r^{(i)}_{T-1},^{(i)}_{T-1}, s^{(i)}_{T}),G^{(i)}_{0:T}]\) being a single trajectory, \(N\) the total number of offline trajectories, and \(r_{t}^{}\)'s being unknown, the objective is to estimate the expected total human return over the unknown state-action visitation distribution \(^{}\) of the target (evaluation) policy \(\), i.e., \(_{(s,a)^{},r^{} R^{}}[ _{t=0}^{T}^{t}r_{t}^{}]\)._

### Reconstruction of IHRs for OPEHF

We emphasize that the human returns are only issued at the end of each episode, with IHRs remaining unknown. One can set all IHRs from \(t=0\) to \(t=T-2\) to be zeros (_i.e._, \(r_{0:T-2}^{}=0\)), and _rescale_ the cumulative human return to be the IHR at the last step (_i.e._, \(r_{T-1}^{}=G^{}_{0:T}/^{T-1}\)), to allow the use of existing OPE methods toward OPEHF (Problem 1). However, the sparsity over \(r^{}\)'s here may impose difficulties for OPE to estimate the human returns accurately over the target policies. For OPEHF, we start by showing that for the per-decision importance sampling (PDIS) method - a variance-reduction variant of the importance sample (IS) family of OPE methods  - if IHRs _were to be available_, they could reduce the variance in the estimation compared to the rescale approach above.

Recall that the PDIS estimator follows \(^{}_{PDIS}=_{i=1}^{N-1}_{t=0}^{T-1}^{t} ^{(i)}_{0:t}r_{t}^{()}\), where \(^{(i)}_{0:t}=_{k=0}^{t}_{0:t}|s^{(i)}_{k})}{(a ^{(i_{k})}_{k}|s^{(i)}_{k})}\) are the PDIS weights for offline trajectory \(^{(i)}\). Moreover, the estimator of the rescale approach3 above is \(^{}_{Rescale}=_{i=1}^{N-1}^{(i)}_{0:T-1}G^{ (i)}_{0:T}\), which is equivalent to the vanilla IS estimator . We now show the variance reduction property of \(^{}_{PDIS}\) in the context of OPEHF.

**Proposition 1**.: _Assume that (i) \([r_{t}^{}] 0\), and (ii) given the horizon \(T\), consider any \(1 t+1 k T\) of any offline trajectory \(\), \(_{0:k}\) and \(r_{t}^{}_{0:k}\) are positively correlated. Then, \((^{}_{PDIS})(^{}_{Rescale})\), with \(()\) representing the variance._

The proof can be found in Appendix A. Assumption (_i_) can be easily satisfied in the real world, as HF signals are usually quantified as positive values, _e.g._, ratings (1-10) provided by participants. Assumption (_ii_) is most likely to be satisfied when the target policies do not visit low-return regions substantially , which is a pre-requisite for testing RL policies in human-involved environments as initial screening are usually required to filter the ones that could potentially pose risks to participants .

Besides IS, doubly robust (DR)  and fitted Q-evaluation (FQE)  methods require learning value functions. Sparsity of rewards (following the rescale approach above) in the offline dataset may lead to poorly learned value functions , considering that the offline data in OPE is usually fixed (_i.e._, no new samples can be added), and are often generated by behavioral policies that are sub-optimal, which results in limited coverage of the state-action space. Limited availabilities of environment-policy interactions (_e.g._, clinical trials) further reduce the scale of the exploration and therefore limit the information that can be leveraged toward obtaining accurate value function approximations.

**Reconstruction of IHRs.** To address this challenge, our approach aims to project the end-of-episode human returns back to each environmental step, _i.e._, to learn a mapping\(^{T}\), parameterized by \(\), that maximizes the sum of log-likelihood of the estimated IHRs, \([_{0}^{},,_{T-1}^{}]^{}  f_{}(,G_{0:T}^{})\), following \(_{}_{i=0}^{N-1}_{t=0}^{T-1} p(_{t}^{ }=r_{t}^{(i)}|,^{(i)},G_{0:T}^{(i)})\), where \(G_{0:T}^{(i)}\) and \(r_{t}^{(i)}\)'s are respectively the human return and IHRs (unknown) of the \(i\)-th trajectory in the offline dataset \(^{}\), and \(N\) is the total number of trajectories in \(^{}\). Given that the objective above is intractable due to unknown \(r_{t}^{(i)}\)'s, we introduce a surrogate objective

\[_{}_{i=0}^{N-1} p_{t=0}^{T-1} ^{t}_{t}^{}=G_{0:T}^{(i)}|,^{(i)},G_{0:T}^{(i)}-C_{regu}(_{0:T-1}^{ }|,^{(i)},G_{0:T}^{(i)}). \]

Here, the _first term_ is a necessary condition for \(_{t}^{}\)'s to be valid for estimating \(r_{t}^{}\)'s, as they should sum to \(G_{0:T}^{}\). Since many solutions may exist if one only optimizes over the first term, the _second term_\(_{regu}\) serves as a regularization that imposes constraints on \(r_{t}^{}\)'s to follow the properties specific to their corresponding state-action pairs; _e.g._, \((s,a)\) pairs that are similar to each other in a representation space, defined over the state-action visitation space, tend to yield similar immediate rewards .

The detailed regularization technique is introduced in sub-section below. Practically, we choose \(f_{}\) to be a bi-directional long-short term memory (LSTM) , since the reconstruction of IHRs can leverage information from both previous and subsequent steps as provided in the offline trajectories.

### Reconstruction of IHRs over Latent Representations (RILR) for OPEHF

Now, we introduce the regularization technique for the reconstruction of IHRs, _i.e._, reconstructing IHRs over latent representations (RILR). Specifically, we leverage the representations captured by variational auto-encoders (VAEs) , learned over \(^{}\), to regularize the reconstructed IHRs, \(_{t}^{}\).

VAEs have been adapted toward learning a compact latent space over offline state-action visitations, facilitating both offline policy optimization [42; 81; 65; 27; 26; 28] and OPE . In this work, we specifically consider building on the variational latent model (VLM) proposed in  since it is originally proposed to facilitate OPE, as opposed to others that mainly use knowledge captured in the latent space to improve sample efficiency for policy optimization. Moreover, the VLM has shown to be effective for learning an expressive representation space, where the encoded state-action pairs are clustered well in the latent space, as measured by the difference over the returns of the policies from which the state-action pairs are sampled; see Figure 1 (mid) which uses \(t\)-SNE to visualize the encoded state-action pairs in trajectories collected from a visual Q&A environment (Appendix E).

Note that VLM originally does not account for HF signals (neither \(r_{t}^{}\)'s nor \(G_{0:T}^{}\)'s), so we introduce the variational latent model with human returns (VLM-H) below, building on the architecture introduced in . VLM-H consists of a prior \(p(z)\) over the latent variables \(z^{L}\), with \(\) representing the latent space and \(L\) the dimension, along with a variational encoder \(q_{}(z_{t}|z_{t-1},a_{t-1},s_{t})\), a decoder \(p_{}(z_{t},s_{t},r_{t-1}|z_{t-1},a_{t-1})\) for generating per-step transitions (over both state-action and latent space), and a separate decoder \(p_{}(G_{0:T}^{}|z_{T})\) for the reconstruction of the human returns at the end of each episode. Note that encoders and decoders are parameterized by \(\) and \(\) respectively. The overall architecture is illustrated in Figure 1 (left).

Figure 1: **(Left)** Architecture of the variational latent model with human returns (VLM-H). (**Mid**) Illustration of the clustering behavior in the latent space using \(t\)-SNE visualization , where the encoded state-action pairs (output by the encoder of VLM-H) are in general clustered together if they are generated by policies with similar human returns (shown in the legend at the top left). (**Right)** Diagram summarizing the pipeline of the OPEHF framework.

**Trajectory inference (encoding).** VLM-H's encoder approximates the intractable posterior \(p(z_{t}|z_{t-1},a_{t-1},s_{t})=,a_{t-1},z_{t},s_{t})}{_{z_{t} }p(z_{t-1},a_{t-1},z_{t},s_{t})dz_{t}}\), by avoiding to integrate over the unknown latent space _a priori_. The inference (or encoding) process can be decomposed as, _i.e._, \(q_{}(z_{0:T}|s_{0:T},a_{0:T-1})=q_{}(z_{0}|s_{0})_{t=1}^{T}q_{ }(z_{t}|z_{t-1},a_{t-1},s_{t})\); here, \(q_{}(z_{0}|s_{0})\) encodes initial states \(s_{0}\) into latent variables \(z_{0}\), and \(q_{}(z_{t}|z_{t-1},a_{t-1},s_{t})\) captures all subsequent environmental transitions in the latent space over \(z_{t}\)'s. In general, both \(q_{}\)'s are represented as diagonal Gaussian distributions4 with mean and variance determined by neural network \(\), as in .

**Trajectory generation (decoding).** The generative (or decoding) process follows, _i.e._, \(p_{}(z_{1:T},s_{0:T},r_{0:T-1},G^{}_{0:T}|z_{0},)=p_{}(G ^{}_{0:T}|z_{T})_{t=1}^{T}p_{}(z_{t}|z_{t-1},a_{t-1}) p_{}(r_{t-1}|z_{t})_{t=0}^{T}p_{}(s_{t}|z_{t})\); here, \(p_{}(z_{t}|z_{t-1},a_{t-1})\) enforces the transition of latent variables \(z_{t}\) over time, \(p_{}(s_{t}|z_{t})\) and \(p_{}(r_{t-1}|z_{t})\) are used to sample the states and immediate _environmental_ rewards, while \(p_{}(G^{}_{0:T}|z_{T})\) generates the _human return_ issued at the end of each episode. Note that here we still use the VLM-H to capture environmental rewards, allowing the VLM-H to formulate a latent space that captures as much information about the dynamics underlying the environment as possible. All \(p_{}\)'s are represented as diagonal Gaussians5 with parameters determined by network \(\).

To train \(\) and \(\), one can maximize the evidence lower bound (ELBO) of the joint log-likelihood \( p_{}(s_{0:T},r_{0:T-1},G^{}_{0:T}|,,^{})\), _i.e._,

\[_{,} _{q_{}} p_{}(G^{}_{0:T}| z_{T})+_{t=0}^{T} p_{}(s_{t}|z_{t})+_{t=1}^{T}  p_{}(r_{t-1}|z_{t})\] \[-KLq_{}(z_{0}|s_{0})||p(z_{0})-_{t =1}^{T}KLq_{}(z_{t}|z_{t-1},a_{t-1},s_{t})||p_{}(z_{t}|z_{t-1},a_{t-1}); \]

the first three terms are the log-likelihoods of reconstructing the human return, states, and environmental rewards, and the two terms that follow are Kullback-Leibler (KL) divergence  regularizing the inferred posterior \(q_{}\). Derivation of the ELBO can be found in Appendix B. In practice, if \(\) and \(\) are chosen to be recurrent networks, one can also regularize the hidden states of \(,\) by including the additional regularization term introduced in .

**Regularizing the reconstruction of IHRs.** Existing works have shown that the latent space not only facilitates the generation of synthetic trajectories but demonstrated that the latent encodings of state-action pairs form clusters, over some measures in the latent space , if they are rolled out from policies that lead to similar returns . As a result, we regularize \(_{t}^{}\) following

\[_{}_{regu}(_{t}^{}|,,s_{0:t} ^{(i)},a_{0:t-1}^{(i)},G^{(i)}_{0:T})=_{j}- p (_{t}^{}=(1-)G^{(j)}_{0:T}|,,s_{0: t^{}}^{(j)},a_{0:t^{}-1}^{(j)},G^{(j)}_{0:T}) \]

for each step \(t\); here, \((s_{0:t}^{(i)},a_{0:t-1}^{(i)})^{(i)}^{}\), \(=\{j_{0},,j_{K-1}\}\) are the indices of offline trajectories that correspond to the latent encodings \(\{z_{t^{}}^{(j_{k})} q_{}(|s_{0:t^{}}^{(j_{k})},a_{0: t^{}-1}^{(j_{k})})|j_{k},t^{}[0,T-1]\}\) that are \(K\)-neighbours of the latent encoding \(z_{t}^{(i)}\) pertaining to \((s_{0:t}^{(i)},a_{0:t-1}^{(i)})\), defined over some similarity/distance function \(d(||)\), following, _i.e._,

\[_{j_{k}}_{k=0}^{K-1}d(z_{t}^{(i)}||z_{t^{}}^{(j_{k })}),z_{t^{}}^{(j_{k})}s_{0:t^{ }}^{(j_{k})},a_{0:t^{}-1}^{(j_{k})}^{(j_{k})} ^{}. \]

In practice, we choose \(d(||)\) to follow stochastic neighbor embedding (SNE) similarities , as it has been shown effective for capturing Euclidean distances in high-dimensional space .

**Overall objective of RILR for OPEHF.** As a result, by following (1) and leveraging the \(_{regu}\) from (3) above, the objective for reconstructing the IHRs is set to be, _i.e._,

\[_{}_{i=0}^{N-1} p_{t=0}^{T-1} ^{t}_{t}^{}=G^{(i)}_{0:T}|,^{(i)},G^{(i)}_{0:T}-C_{t=0}^{T-1}_{regu}(_{t}^{}|,,s_{0:t}^{(i)},a_{0:t-1}^{(i)},G^{(i)}_{0 :T}). \]

**Move from RILR to OPEHF.** In what follows, one can leverage any existing OPE methods to take as inputs the offline trajectories, with the immediate environmental rewards \(r_{t}\)'s replaced by the reconstructed IHRs \(_{t}^{}\)'s, to achieve the OPEHF's objective (Problem 1). Moreover, our method does not require the IHRs to be correlated with the environmental rewards, as the VLM-H learns to reconstruct both by sampling from two independent distributions, \(p_{}(r_{t-1}|z_{t})\) and \(p_{}(G_{0:T}^{}|z_{T})\) respectively, following (2); this is also illustrated empirically over the experiments introduced below (Sections 3), where exceedingly low correlations are found in specific scenarios.

The overall pipeline summarizing our method is shown in Figure 1 (right).

## 3 Real-World Experiments with Human Participants

In this section, we validate the OPEHF framework introduced above over two real-world experiments, adaptive neurostimulation, and intelligent tutoring. Specifically, we consider four types of OPE methods to be used as the downstream estimator following the RILR step (Section 2.3), including per-decision importance sampling (IS) with behavioral policy estimation , doubly robust (DR) , distribution correction estimation (DICE)  and fitted Q-evaluation (FQE) . A brief overview of these methods can be found in Appendix C, and the specific implementations we use are documented in Appendix D. In Appendix E, we have also tested our method within a visual Q&A environment [10; 66], which follows similar mechanisms as in the two real-world experiments, _i.e._, two types of return signals are considered though no human participants are involved.

**Baselines and Ablations.** The baselines include two variants for each of the OPE methods above, _i.e._, (_i_) the _rescale_ approach discussed in Section 2.2, and (_ii_) another variant that sets all the IHRs to be equal to the environmental rewards at corresponding steps, \(r_{t}^{}=r_{t}\  t[0,T-2]\), and then let \(r_{T-1}^{}=r_{T-1}+(G_{0:T}^{}-G_{0:T})/^{T-1}\) with \(G_{0:T}=_{t}^{t}r_{t}\) being the environmental return, which is referred to as _fusion_ below - this baseline may perform better when strong correlations existed between environmental and human rewards, as it intrinsically decomposes the human returns into IHRs. Consequently, in each experiment below, we compare the performance of the OPEHF framework extending all four types of OPE methods above, <IS/DR/DICE/FQE>-OPEHF, against the corresponding baselines, <IS/DR/DICE/FQE>-<Fusion/Rescale>. We also include the VLM-H as an ablation baseline, as if it is a model-based approach standalone; this is achieved by sampling the estimate returns from the decoder, \(_{0:T}^{} p_{}(G_{0:T}^{}|z_{T})\).

**Metrics.** Following a recent OPE benchmark , three metrics are considered to validate the performance of each method, including mean absolute error (MAE), rank correlation, and regret@1. Mathematical definitions can be found in Appendix D. Also, following , each method is evaluated over 3 random seeds, and the mean performance (with standard errors) is reported.

### Adaptive Neurostimulation: Deep Brain Stimulation

Adaptive neurostimulation facilitates treatments for a variety of neurological disorders [4; 11; 13; 55]. Deep brain stimulation (DBS) is a type of neurostimulation used specifically toward Parkinson's disease (PD), where an internal pulse generator (IPG), implanted under the collarbone, sends electrical stimulus to the basal ganglia (BG) area of the brain through invasive electrodes; Figure 2 illustrates

Figure 2: Setup of the neurostimulation experiments, as well as the formulation of offline trajectories. Environmental rewards and human returns are captured in streams 1 and 2-3 respectively.

the setup. Adaptive DBS aims to adjust the strength (amplitude) of the stimulus in real-time, to respond to irregular neuronal activities caused by PD, leveraging the local field potentials (LFPs) as the immediate feedback signals, _i.e._, the environmental rewards. Existing works have leveraged RL for adaptive DBS over _computational_ BG models , using rewards defined over a physiological signal - beta-band power spectral density of LFPs (_i.e._, the beta power) since physiologically PD could lead to increased beta power due to the irregular neuronal activations it causes . However, in clinical practice, the correlation between beta power and the level of satisfaction reported by the patients varies depending on the specific characteristics of each person, as PD can cause different types of symptoms over a wide range of severity . Such findings further justify the significance of evaluating HF/human returns in the real world using OPEHF.

In this experiment, we leverage OPEHF to estimate the feedback provided by _4 PD patients_ who participate in monthly clinical testing of RL policies trained to adapt amplitudes of the stimulus toward reducing their PD symptoms, _i.e._, bradykinesia and tremor. A mixture of behavioral policies is used to collect the offline trajectories \(^{}\). Specifically, in every step, the state \(s_{t}\) is a historical sequence of LFPs capturing neuronal activities, and the action \(a_{t}\) updates the amplitude of the stimulus to be sent6. Then, an _environmental_ reward \(r_{t}=R(s_{t},a_{t})\) gives a penalty if the beta power computed from the latest LFPs is greater than some threshold (to promote treatment efficacy) as well as a penalty proportional to the amplitudes of the stimulus being sent (to improve battery life of the IPG). At the end of each episode, the _human returns_\(G_{0:T}^{}\) are determined from three sources (weighted by 50%, 25%, 25%, respectively), _i.e._, (_i_) a satisfaction rating (between 1-10) provided by the patient, (_ii_) hand grasp speed as a result of the bradykinesia test , and (_iii_) level of remorcalculated over the data from a wearable accelerometry . Each session lasts more than 10 minutes, and each discrete step above corresponds to 2 seconds in the real world; thus, the horizon \(T 300\) (more details are provided in Appendix D). Approval of an Institutional Review Board (IRB) is obtained from Duke University Health System, as well as the exceptional use of the DBS system by the US Food and Drug Administration (FDA).

For each patient, OPEHF and the baselines are used to estimate the human returns of 6 target policies with varied performance. The ground-truth human return for each target policy is obtained as a result of extensive clinical testing following the same schema above, over more than 100 minutes. Table 1 shows the Pearson's and Spearman's correlation coefficients , measuring the linear and rank correlations between the environmental returns \(G_{0:T}\) and the human returns \(G_{0:T}^{}\) over all the target DBS policies considered for each patient. Pearson's coefficients are all negative since the environmental reward function only issues penalties, while human returns are all captured by positive values. It can be observed that only weak-to-moderate degrees of linear correlations exist for all four patients, while ranks between \(G_{0:T}\)'s and \(G_{0:T}^{}\)'s are not preserved across patients; thus, it highlights

 Patient \# & \(0\) & \(1\) & \(2\) & \(3\) \\  Pearson’s & -0.396 & -0.477 & -0.599 & -0.275 \\ Spearman’s & -0.2 & -0.6 & 0.086 & 0.086 \\  

Table 1: Correlations between the _environmental_ and _human_ returns of the 6 target DBS policies associated with each PD patient.

Figure 3: Results from the adaptive neurostimulation experiment, _i.e._, deep brain stimulation (DBS). Each method is evaluated over the data collected from each patient, toward corresponding target policies, respectively. The performance shown above are averaged over all 4 human participants affected by Parkinson’s disease (PD). Raw performance over each patient can be found in Appendix D.

the need for leveraging OPEHF to estimate human returns, which is different than the classic OPE that focus on estimating environmental returns.

The overall performance averaged across the 4-patient cohort, is reported in Fig. 3. Raw performance over every single patient can be found in Appendix D. It can be observed that our OPEHF framework significantly improves MAEs and ranks compared to the two baselines, for all 4 types of downstream OPE methods we considered (IS, DR, DICE, and FQE). Moreover, our method also significantly outperforms the ablation VLM-H in terms of these two metrics, as the VLM-H's performance is mainly determined by how well it could capture the underlying dynamics and returns. In contrast, our OPEHF framework not only leverages the latent representations learnt by the VLM-H (for regularizing RILR), it also inherits the advantages intrinsically associated with the downstream estimators; _e.g._, low-bias nature of IS, or low-variance provided by DR. Moreover, the fusion baseline in general performs worse than the rescale baseline as expected, since no strong correlations between environmental and human returns are found, as reported in Table 1.

Note that the majority of the methods lead to similar (relatively low) regrets, as there exist a few policies that lead to human returns that are close over some patients (see the raw statistics in Appendix D). The reason is that all the policies to be extensively tested in clinics are subject to initial screening, where clinicians ensure they would not lead to undesired outcomes or pose significant risks to the patients; thus, the performance of some target policies tends to be close. Nonetheless, low MAEs and high ranks achieved by our method show that it can effectively capture the subtle differences in returns resulting from other HF signals, _i.e._, levels of bradykinesia and tremor. Moreover, Figure 4 visualizes the VLM-H encodings over the trajectories collected from the 6 target DBS policies for each participant and shows that encoded pairs associated with the policies that lead to similar returns are in general clustered together, which justifies the importance of leveraging the similarities over latent representations to regularize the reconstruction of IHRs as in the RILR objective (5).

### Intelligent Tutoring

Intelligent tutoring refers to a system where students can actively interact with an autonomous tutoring agent that can customize the learning content, tests, etc., to improve engagement and learning

    &  &  &  \\  & Fusion & Rescale &  **OPEHF** \\ (our) \\  & Fusion & Rescale &  **OPEHF** \\ (our) \\  & VLM-H \\  MAE & 0.7\(\)0.14 & 0.77\(\)0.08 & **0.57\(\)0.09** & 1.03\(\)0.07 & 1.03\(\)0.25 & **0.86\(\)0.04** & 1.00\(\)0.01 \\ Rank & 0.47\(\)0.11 & 0.4\(\)0.09 & **0.8\(\)0.09** & 0.33\(\)0.05 & 0.4\(\)0.0 & **0.53\(\)0.2** & 0.41\(\)0.25 \\ Regret@1 & **0.36\(\)0.16** & **0.36\(\)0.16** & **0.41\(\)0.04** & **0.41\(\)0.0** & **0.41\(\)0.0** & **0.41\(\)0.0** & 0.28\(\)0.19 \\   &  &  \\  & Fusion & Rescale &  **OPEHF** \\ (our) \\  & Fusion & Rescale & 
 **OPEHF** \\ (our) \\  \\  MAE & 3.19\(\)0.57 & 2.33\(\)0.59 & **1.01\(\)0.01** & 0.74\(\)0.07 & 0.98\(\)0.1 & **0.59\(\)0.1** \\ Rank & 0.47\(\)0.2 & 0.33\(\)0.2 & **0.53\(\)0.22** & 0.27\(\)0.14 & 0.4\(\)0.0 & **0.47\(\)0.05** \\ Regret@1 & 0.55\(\)0.06 & 0.45\(\)0.18 & **0.37\(\)0.15** & **0.36\(\)0.16** & **0.41\(\)0.0** & **0.41\(\)0.0** \\   

Table 2: Results from the intelligent tutoring experiment, _i.e._, performance achieved by our OPEHF framework compared to the ablation and baselines over all four types of downstream OPE estimators.

Figure 4: \(t\)-SNE visualizing the VLM-H encodings of the state-action pairs rolled out over DBS policies with different human returns (shown in the legend). It can be observed that distances among the encoded pairs associated with the policies that lead to similar returns are in general smaller, justifying the RILR objective (5).

outcomes . OPEHF is important in such a setup for directly estimating the potential outcomes that could be obtained by students, as opposed to environmental rewards that are mostly discrete; see detailed setup below. Existing works have explored this topic over classic OPE setting in _simulations_.

The system is deployed in an undergraduate-level introduction to probability and statistics course over 5 academic years at North Carolina State University, where the interaction logs obtained from 1,288 students who voluntarily opted-in for this experiment are recorded.7 Specifically, each episode refers to a student working on a set of 12 problems (_i.e._, horizon \(T=12\)), where the agent suggests the student approach each problem through _independent_ work, working with the _hints_ provided, or directly providing the _full solution_ (for studying purposes) - these options constitute the action space of the agent. The states are characterized by 140 features extracted from the logs, designed by domain experts; they include, for example, the time spent on each problem, and the correctness of the solution provided. In each step, an immediate _environmental_ reward of +1 is issued if the answer submitted by students, for the current problem, is at least 80% correct (auto-graded following pre-defined rubrics). A reward of 0 is issued if the grade is less than 80% or the agent chooses the action that directly displays the full solution. Moreover, students are instructed to complete two exams, one before working on any problems and another after finishing all the problems. The normalized difference between the grades of two exams constitutes the _human_ return for each episode. More details are provided in Appendix D.

The intelligent tutoring agent follows different policies across academic years, where the data collected from the first 4 years (1148 students total) constitutes the offline trajectories \(^{}\) (as a result of a mixture of behavioral policies). The 4 policies deployed in the 5th year (140 students total) serve as the target policies, whose ground-truth performance is determined by averaging over the human returns of the episodes that are associated with each policy respectively. Table 3 documents the Pearson's and Spearman's correlation coefficients between the environmental and human returns from data collected over each academic year, showing weak linear and rank correlations across all 5 years. Such low correlations are due to the fact that the environmental rewards are discrete and do not distinguish among the agent's choices, _i.e._, a +1 reward can be obtained either if the student works out a solution independently or by following hints, and a 0 reward is issued every time the agent chooses to display the solution even if the student could have solved the problem. As a result, such a setup makes OPEHF to be more challenging; because human returns are only available at the end of each episode, and the immediate environmental rewards do not carry substantial information toward extrapolating IHRs.

Table 2 documents the performance of OPEHF and the baselines toward estimating the human returns of the target policies. It can be observed that our OPEHF framework achieves state-of-the-art performance, over all types of downstream OPE estimators considered. This result echos the design of the VLM-H where both environmental information (state transitions and rewards) and human returns are encoded into the latent space, which helps formulate a compact and expressive latent space for regularizing the downstream RILR objective (5). Moreover, it is important to use the latent information to guide the reconstruction of IHRs (as regularizations in RILR), as opposed to using the VLM-H to predict human returns standalone; since limited convergence guarantees/error bounds can be provided for VAE-based latent models, which is illustrated in both Figure 3 and Table 2 where OPEHF largely outperforms the VLM-H ablation over MAE and rank.

## 4 Related Works

**OPE.** Majority of existing model-free OPE methods can be categorized into one of the four types, _i.e._, IS, DR, DICE, and FQE. Recently, variants of IS and DR methods have been proposed for variance or bias reduction , as well as adaptations toward unknown behavioral policies .

   Year \# & \(0\) & \(1\) & \(2\) & \(3\) & \(4\) \\  Pearson’s & 0.033 & 0.176 & 0.089 & 0.154 & 0.183 \\ Spearman’s & 0.082 & 0.156 & 0.130 & 0.161 & 0.103 \\   

Table 3: Correlations between the _environmental_ and _human_ returns from data collected over each academic year.

DICE methods are intrinsically designed to work with offline trajectories rolled out from a mixture of behavioral policies, and existing works have introduced the DICE variants toward specific environmental setups [84; 83; 77; 78; 51; 9]. FQE extrapolates policy returns from the approximated Q-values [31; 40; 36]. There also exist model-based OPE methods [82; 18] that first captures the dynamics underlying the environment, and estimate policy performance by rolling out trajectories under the target policies. A more detailed review of existing OPE methods can be found in Appendix C. Note that these OPE methods have been designed for estimating the _environmental_ returns. In contrast, the objective for OPEHF is to estimate the _human_ returns which may not be strongly correlated with the environmental returns, as they are usually determined under different schemas.

**VAEs for OPE and offline RL.** There exists a long line of research developing latent models to capture the dynamics underlying environments in the context of offline RL as well as OPE. Specifically, PlaNet  uses recurrent neural networks to capture the transitions of latent variables over time. Latent representations learned by such VAE architectures have been used to augment the state space in offline policy optimization to improve sample efficiency, _e.g._, in Dreamers [26; 28], SOLAR  and SLAC . On the other hand, LatCo  attempts to improve sample efficiency by searching in the latent space which allows by-passing physical constraints. Also, MOPO , COMBO , and LOMPO  train latent models to quantify the confidence of the environmental transitions as learned from offline data and prevent the policies from following transitions over uncertain regions during policy training. Given that such models are mostly designed for improving sample efficiency in policy optimization/training, we choose to leverage the architecture from  for RILR as it is the first work that adapts latent models to the OPE setup.

**Reinforcement learning from human feedback (RLHF).** Recently, the concept of RLHF has been widely used in guiding RL policy optimization with the HF signals deemed more informative than the environmental rewards [8; 85; 47]. Specifically, they leverage the _ranked preference_ provided by labelers to train a reward model, captured by feed-forward neural networks, that is fused with the environmental rewards to guide policy optimization. However, in this work, we focus on estimating the HF signals that serve as _direct evaluation_ of the RL policies used in human-involved experiments, such as the level of satisfaction (_e.g._, on a scale 1-10) and the treatment outcome. The reason is that in many scenarios the participants cannot revisit the same procedure multiple times, _e.g._, patients may not undergo the same surgeries several times and rank the experiences. More importantly, OPEHF's setup is critical when online testing of RL policies may be even prohibited, without sufficient justifications over safety and efficacy upfront, as illustrated by the experiments above.

**Reward shaping.** Although reward shaping methods [3; 58; 29] pursue similar ideas of decomposing the delayed and/or sparse rewards (_e.g._, the human return) into immediate rewards, they fundamentally rely on transforming the MDP to such that the value functions can be smoothly captured and high-return state-action pairs can be quickly identified and frequently re-visited. For example, RUDDER  leverages the transformed MDP that has expected future rewards equal to zero. Though the optimization objective is consistent between pre- and post-transformed MDPs, this approach likely would not converge to an optimal policy in practice. On the other hand, the performance (_i.e._, returns) of sub-optimal policies is not preserved across the two MDPs. This significantly limits its use cases toward OPE which requires the returns resulted by sub-optimal policies to be estimated accurately. As a result, such methods are not directly applicable to the OPEHF problem we consider.

## 5 Conclusion and Future Works

Existing OPE methods fall short in estimating HF signals, as HF can be dependent upon various confounders. Thus, in this work, we introduced the OPEHF framework that revived existing OPE methods for estimating human returns, through RILR. The framework was validated over two real-world experiments and one simulation environment, outperforming the baselines in all setups. Although in the future it could be possible to extend OPEHF to facilitate estimating the HF signals needed for updating the policies similar to RLHF, we focused on policy evaluation which helped to isolate the source of improvements; as policy optimization's performance may depend on multiple factors, such as the exploration techniques used as well as the objective/optimizer chosen for updating the policy. Moreover, this work mainly focuses on the scenarios where the human returns are directly provided by the participants. So under the condition where the HF signals are provided by 3-rd parties (_e.g._, clinicians), non-trivial adaptations over this work may be needed to consider special cases such as conflicting HF signals provided by different sources.

Acknowledgements

This work is sponsored in part by the AFOSR under award number FA9550-19-1-0169, by the NIH UH3 NS103468 award, and by the NSF CNS-1652544, DUE-1726550, IIS-1651909 and DUE-2013502 awards, as well as the National AI Institute for Edge Computing Leveraging Next Generation Wireless Networks, Grant CNS-2112562. Investigational Summit RC+S systems and technical support provided by Medtronic PLC. Apple Watches were provided by Rune Labs. We thank Stephen L. Schmidt and Jennifer J. Peters from Duke University Department of Biomedical Engineering, as well as Katherine Genty from Duke University Department of Neurosurgery, for the efforts overseeing DBS experiments in the clinic.