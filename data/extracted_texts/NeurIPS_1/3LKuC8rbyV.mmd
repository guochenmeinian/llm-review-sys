# Langevin Unlearning: A New Perspective of Noisy Gradient Descent for Machine Unlearning

Eli Chien

Department of Electrical and Computer Engineering

Georgia Institute of Technology

Georgia, U.S.A.

ichien6@gatech.edu

Haoyu Wang

Department of Electrical and Computer Engineering

Georgia Institute of Technology

Georgia, U.S.A.

haoyu.wang@gatech.edu

Ziang Chen

Department of Mathematics

Massachusetts Institute of Technology

Massachusetts, U.S.A.

ziang@mit.edu

Pan Li

Department of Electrical and Computer Engineering

Georgia Institute of Technology

Georgia, U.S.A.

panli@gatech.edu

###### Abstract

Machine unlearning has raised significant interest with the adoption of laws ensuring the "right to be forgotten". Researchers have provided a probabilistic notion of approximate unlearning under a similar definition of Differential Privacy (DP), where privacy is defined as statistical indistinguishability to retraining from scratch. We propose Langevin unlearning, an unlearning framework based on noisy gradient descent with privacy guarantees for approximate unlearning problems. Langevin unlearning unifies the DP learning process and the privacy-certified unlearning process with many algorithmic benefits. These include approximate certified unlearning for non-convex problems, complexity saving compared to retraining, sequential and batch unlearning for multiple unlearning requests.

## 1 Introduction

With recent demands for increased data privacy, owners of these machine learning models are responsible for fulfilling data removal requests from users. Certain laws are already in place guaranteeing the users "Right to be Forgotten", including the European Union's General Data Protection Regulation (GDPR), the California Consumer Privacy Act (CCPA), and the Canadian Consumer Privacy Protection Act (CPPA) . Merely removing user data from the training data set is insufficient, as machine learning models are known to memorize training data information . It is critical to also remove the information of user data subject to removal requests from the machine learning models. This consideration gave rise to an important research direction, referred to as _machine unlearning_.

Naively, one may retrain the model from scratch after every data removal request to ensure a "perfect" privacy guarantee. However, it is prohibitively expensive in practice when accommodating frequent removal requests. To avoid complete retraining, various machine unlearning methods have beenproposed, including exact  as well as approximate approaches . Exact approaches ensure that the unlearned model would be identical to the retraining one in distribution. Approximate approaches, on the other hand, allow for slight misalignment between the unlearned model and the retraining one in distribution under a similar definition to Differential Privacy (DP) .

### Our Contributions

Learning with noisy gradient methods, such as DP-SGD , is widely adopted for privatizing machine learning models with DP guarantee. Intuitively, a learning process with a stronger DP guarantee implies an "easier" unlearning problem as depicted in Figure 1. However, it is unclear if fine-tuning with it on the updated dataset subject to the unlearning request provides an approximate unlearning guarantee, how the DP learning guarantee affects unlearning, and computational benefit compared to retraining. In this work, we provide an affirmative answer for the empirical risk minimization problems with smooth objectives. We propose Langevin unlearning, an approximate unlearning framework based on projected noisy gradient descent (PNGD). Our core idea can be interpreted via a novel unified geometric view of the learning and unlearning processes in Figure 1, which naturally bridges DP and unlearning. Given sufficient learning iterations via the learning process \(\), we first show that PNGD converges to a _unique_ stationary distribution \(_{}\) for any dataset \(\) (Theorem 3.1). Comparing \(_{}\) with the stationary distribution \(_{^{}}\) for any of its adjacent dataset \(^{}\), the learning process shows Renyi DP with privacy loss1\(_{0}\). Given a particular unlearning request \(^{}\), the unlearning process \(\) can be interpreted as moving from \(_{}\) to \(_{^{}}\) from \(_{0}\)-close to \(\)-close. In practice, due to the unlearning process, the unlearning privacy loss \(\) can be set much smaller than \(_{0}\), while on the other hand, a stronger initial RDP guarantee, i.e., smaller \(_{0}\), allows for less unlearning iterations to achieve the desired \(\). Besides the above DP-unlearning bridge, this framework also brings many benefits including (1) a capability of dealing with non-convex problems in theory, which to the best of our knowledge, no previous approximate unlearning framework can tackle, (2) better privacy-utility trade-off in practice compared to state-of-the-art approximate unlearning approach  in strongly convex settings, (3) a provably computational benefit compared with model retraining, and (4) a friendly extension to sequential and batch settings with multiple unlearning requests.

We prove the intuition in Fig. 1 formally in Theorem 3.2. We show that \(K\) unlearning iterations lead to an _exponentially fast_ privacy loss decay \((-_{k=0}^{K}R_{k})_{0}\), where \(\) is the order of Renyi divergence and \(R_{k}\) is the strict privacy improving rate depends on the problem settings with an iteration independent strictly positive lower bound \(>0\). Our result is based on convergence analysis of Langevin dynamics . The sampling essence of PNGD allows for a provable unlearning guarantee for non-convex problems . Our characterization of \(_{0}\) allows an extension of the recent results that PNGD learning satisfies Renyi DP for convex problems  to non-convex

Figure 1: The geometric interpretation of relations between learning and unlearning. (Left) RDP guarantee of the learning process induces a regular polyhedron. Smaller \(_{0}\) implies an “easier” unlearning problem. (Right) Learning and unlearning processes on adjacent datasets. It illustrates our main idea and results. More learning iteration gives worse privacy (**privacy erosion**) while more unlearning iteration gives better privacy, which we termed this phenomenon as **privacy recuperation**.

problems as summarized in Theorem 3.3. Our key technique is to carefully track the constant of log-Sobolev inequality  (LSI) along the learning and unlearning processes and leverage the boundedness property of the projection step via results of .

Regarding the computational benefit compared to model retraining, we show iteration complexity saving by comparing two Renyi differences, the one between initialization \(_{0}\) and \(_{^{}}\), which is at least \((1)\) in the worst case versus the other one between the learning convergent distribution \(_{}\) and \(_{^{}}\), i.e., \(_{0}\) which is shown to be \(O(1/n^{2})\) for a dataset of size \(n\). Such a gap demonstrates that Langevin unlearning is more efficient than retraining, especially for the dataset with large \(n\).

For sequential unlearning with multiple unlearning requests, we composite the privacy loss bound for single-step requests via the weak triangle inequality of Renyi divergence , which yields a sequential unlearning procedure that achieves privacy loss \(\) for each request (Corollary 3.4). For batch unlearning, \(_{0}\) is changed to incorporate the batch size (Theorem 3.3).

Beyond theoretical contributions, we also conduct empirical evaluation. Despite the provable order-wise improvement in \(n\) compared to re-training, our current theory has a limitation by relying on some constants that are undetermined or can be only loosely determined in the non-convex setting. Therefore, we focus on logistic regression tasks for empirical evaluation. Compared with the state-of-the-art gradient-based certified approximate unlearning solution  that requires strong convexity, we achieve a superior privacy-utility-complexity trade-off. Although success in the convex case may not directly imply success in non-convex settings, we leave tightening these constants as a future study. For this, we discuss potential alleviation and future direction in Appendix A and 5, respectively.

### Related Works

**Unlearning with privacy guarantees.** Prior approximate unlearning works require (strong) convexity of the objective function . Their analysis is based on the sensitivity analysis of the _optimal parameter_. Since the optimal parameter is not even unique in the non-convex setting, it is unclear how their analysis can be generalized beyond convexity. In contrast, we show that the law of our PNGD learning process admits a _unique_ stationary distribution even for _non-convex_ problems.Authors of  leverage a second-order update which requires computing Hessian inverse and thus is not scalable for high-dimensional problems. While they only require one unlearning iteration, we show in our experiment that one PNGD unlearning iteration is sufficient for strongly convex loss to achieve satisfied privacy with comparable utility to retraining as well. Neel et al.  leverage PGD for learning and unlearning, and achieve the privacy guarantee via publishing the final parameters with additive Gaussian noise. We show in our experiment that our Langevin unlearning strategy provides a better privacy-utility-complexity trade-off compared to this approach. Ullah et al.  focus on exact unlearning by leveraging variants of noisy (S)GD. Their analysis is based on total variation stability which is different from our analysis based on Renyi divergence. Also, their analysis does not directly generalize to approximate unlearning. Several works focus on extending the unlearning problems for adaptive unlearning requests . While we focus on the non-adaptive setting, it is possible to show that Langevin unlearning is also capable of adaptive unlearning requests as we do not keep any non-private internal state. We left a rigorous discussion on this as future work. Chourasia et al.  also leverage Langevin dynamic analysis in their work. However, their unlearning definition is different from the standard literature as ours2.

**Differential privacy of noisy gradient methods.** A pioneer work  studied the DP properties of Langevin Monte Carlo methods. Yet, they do not propose using noisy GD for general machine learning problems. A recent line of work  shows that projected noisy (S)GD training exhibits DP guarantees based on the analysis of Langevin dynamics  under the strong convexity assumption. In the meanwhile, Altschuler et al.  also provided the DP guarantees for projected noisy SGD training but with analysis based on Privacy Amplification by Iteration  under the convexity assumption. None of these works study how PNGD can be leveraged for machine unlearning or DP guarantees for non-convex problems.

**Sampling literature.** Non-asymptotic convergence analysis for Langevin Monte Carlo has a long history . The seminal works  proved non-asymptotic convergence analysis in Renyidivergence under strong convexity. Many works improve upon them by either working with weaker isoperimetric inequalities or different notions of convergence . See  for a more thorough review along this direction. While these works mainly focus on convergence to the unbiased limit (i.e., the limiting distribution for an infinitesimal step size), we have biased limits (i.e., the limiting distribution for a constant step size, such as our \(_{}\)) in machine unlearning problems. Recently Altschuler et al.  initiated the question of studying the properties and convergence to the biased limit. Our work provides a new important application, machine unlearning, for these astonishing theoretical results in the sampling literature.

The rest of the paper is organized as follows. In Section 2, we provide preliminaries and problem setup. The theoretical results of Langevin unlearning are in Section 3. We conclude with experiments in Section 4. Due to the space limit, all proofs and future directions are deferred to Appendices.

## 2 Preliminaries and Problem Setup

We consider the empirical risk minimization (ERM) problem. Let \(=\{_{i}\}_{i=1}^{n}\) be a training dataset with \(n\) data points \(_{i}\) taken from the universe \(\). Let \(f_{}(x)=_{i=1}^{n}f(x;_{i})\) be the objective function. We aim to minimize with learnable parameter \(x_{R}\), where \(_{R}=\{x^{d}\|x\| R\}\) is a closed ball of radius \(R\). We denote \(_{_{R}}:^{d}_{R}\) to be an orthogonal projection to \(_{R}\). The norm \(\|\|\) is standard Euclidean \(_{2}\) norm if not specified. \(()\) is denoted as the set of all probability measures over a closed convex set \(\). Standard definitions such as convexity can be found in Appendix C. Finally, we use \(x\) to denote that a random variable \(x\) follows the probability distribution \(\). To control the convergence behavior of (P)NGD, it is standard to check an isoperimetric condition known as log-Sobolev inequality , described as follows.

**Definition 2.1** (Log-Sobolev Inequality (\(C_{}\)-Lsi)).: A probability measure \((^{d})\) is said to satisfy Logarithmic Sobolev Inequality with constant \(C_{}\) if

\[\,(^{d}),\,\,\,D_{1}(||)}}{2}_{x}\|\|^{2},\]

where \(D_{1}(||)\) is the Kullback-Leibler divergence.

### Privacy Definition for Learning and Unlearning

We say two datasets \(=\{_{i}\}_{i=1}^{n}\) and \(^{}=\{_{i}^{}\}_{i=1}^{n}\) are adjacent if they "differ" only in one index \(i_{0}[n]\) so that \(_{i}=_{i}^{}\) for all \(i i_{0}\) unless otherwise specified. Furthermore, we say two datasets \(\) and \(^{}\) are adjacent with a group size of \(S 1\) if they differ in at most \(S\) indices. We next introduce a useful idea termed Renyi difference.

**Definition 2.2** (Renyi difference).: Let \(>1\). For a pair of probability measures \(,^{}\) with the same support, the \(\) Renyi difference \(d_{}(,^{})\) is defined as \(d_{}(,^{})=(D_{}(||^{}),D_{ }(^{}||)),\) where \(D_{}(||^{})\) is the \(\) Renyi divergence \(D_{}(||^{})\) defined as

\[D_{}(||^{})=(_{x ^{}}[(x)}]^{}).\]

We are ready to introduce the formal definition of differential privacy and unlearning.

**Definition 2.3** (Renyi Differential Privacy (RDP) ).: Let \(>1\). A randomized algorithm \(:^{n}^{d}\) satisfies \((,)\)-RDP if for any adjacent dataset pair \(,^{}^{n}\), the \(\) Renyi difference \(d_{}(,^{})\), where \(()\) and \((^{})\ ^{}\).

It is known to the literature that an \((,)\)-RDP guarantee can be converted to the popular \((,)\)-DP guarantee  relatively tight . As a result, we will focus on establishing results with respect to \(\) Renyi difference (and equivalently \(\) Renyi divergence). Next, we introduce our formal privacy definition of unlearning.

**Definition 2.4** (Renyi Unlearning (Ru)).: Consider a randomized learning algorithm \(:^{n}^{d}\) and a randomized unlearning algorithm \(:^{d}^{n}^{n} ^{d}\). We say \((,)\) achieves \((,)\)-RU if for any \(>1\) and any adjacent datasets \(,^{}\), the \(\) Renyi difference \(d_{}(,^{})\), where \(((),,^{})\) and \((^{})^{}\).

Notably, our Definition 2.4 can be converted to the standard \((,)\)-unlearning definition , similar to RDP-DP conversion . Since we work with the replacement definition of dataset adjacency, to "erase" a data point \(_{i}\) we can simply replace it with any data point \(_{i}^{l}\) for the updated dataset \(^{}\) in practice.

## 3 Langevin Unlearning: Main Results

We propose to leverage projected noisy gradient descent for our learning and unlearning algorithm \(\) and \(\). For \(\), we propose to optimize the objective \(f_{}(x)\) with PNGD:

\[x_{t+1}=_{_{R}}(x_{t}- f_{}(x_{t})+ }W_{t}), \]

where \(W_{t}}}{{}}(0,I_{d})\) and \(,^{2}>0\) are hyperparameters of step size and noise variance respectively. The initialization \(x_{0}\) can be chosen arbitrarily in \(_{R}\) unless specified. We assume the learning procedure will train the model until convergence \(x_{}=()\) for simplicity, where we prove in Theorem 3.1 that the law of this learning process (1) indeed converges to a unique stationary distribution when \( f_{}\) is continuous. A similar "well-trained" assumption has been also used in prior unlearning literature  and we will discuss the case of insufficient training later. After we obtain a learned parameter \(()\), an unlearning request arrives so that the training dataset changes from \(\) to \(^{}\). For the unlearning algorithm \(\), we propose to fine-tune the model parameters on the new objective \(f_{^{}}(y)\) with \(K\) iterations of the same PNGD.

\[y_{k+1}=_{_{R}}(y_{k}- f_{^{}}(y _{k})+}_{k}), \]

where \(_{k}}}{{}}(0,I_{d})\) and \(y_{0}=x_{}\), which starts from the convergent point of the learning procedure. Throughout our work, we assume \(f(x;)\) is \(M\)-Lipschitz and \(L\)-smooth in \(x\) for any \(\). Nevertheless, one can apply per-sample gradient clipping in (1) and (2) so that the \(M\)-Lipschitz assumption can be dropped. In this case, our learning and unlearning processes admit the popular DP-SGD  without mini-batching. For the rest of the paper, we denote \(_{t},_{k}\) as the laws of the processes \(x_{t},y_{k}\) respectively. Recall that we also denote the limiting distribution of the learning process (1) as \(_{}\) for training dataset \(\).

### Limiting Distribution and General Idea

A key component of the Langevin unlearning is the existence, uniqueness, and stationarity of the limiting distribution \(_{}\) of the training process. We start with proving that \(_{}\) exists, is unique, and is a stationary distribution. Our proof is relegated to Appendix F, which is based on showing the ergodicity of the process (1) by leveraging results in .

**Theorem 3.1**.: _Suppose that the closed convex set \(_{R}^{d}\) is bounded with \(_{R}\) having a positive Lebesgue measure and that \( f_{}:_{R}^{d}\) is continuous. The Markov chain \(\{x_{t}\}\) in (1) admits a unique invariant probability measure \(_{}\) on the Borel \(\)-algebra of \(_{R}\). Furthermore, for any \(x_{R}\), the distribution of \(x_{t}\) conditioned on \(x_{0}=x\) converges weakly to \(_{}\) as \(t\)._

If \(\) is known to be \((,_{0})\)-RDP for a \(>1\), by definition we know that for all adjacent dataset \(,^{}\), \(d_{}(_{},_{^{}})_{0}\). In the space of \((_{R})\), this RDP guarantee gives a "regular polyhedron", where vertices are \(_{},_{^{}}\) and all adjacent vertices are of "lengths" \(_{0}\) at most in Renyi difference. We caveat that Renyi difference is not a metric but the idea of the regular polyhedron is useful conceptually. As a result, the RDP guarantee of the learning process controls the "distance" between distribution induced from adjacent dataset \(\) and \(^{}\). Once we finish the learning process, we receive an unlearning request so that our dataset changes from \(\) to an adjacent dataset \(^{}\). We need to move from \(_{}\) to \(_{^{}}\) at least \(\) close for a \((,)\)-RU guarantee. Intuitively, if the initial RDP guarantee is stronger (i.e., \(_{0}\) is smaller), unlearning becomes "easier" at the cost of larger noise. When \(_{0}=\), we automatically achieve \((,)\)-RU without any unlearning update. One of our main contributions is to characterize how many PNGD unlearning iteration is needed to reduce \(d_{}(_{k},_{^{}})\) from \(_{0}\) to \(\), where \(_{0}=_{}\).

For the unlearning process, note that the initial Renyi difference between \(_{0},_{^{}}\) is provided by the RDP guarantees of the learning process. As a result, we are left to characterize the convergence of the process \(y_{k}\) to its stationary distribution \(_{^{}}\) in Renyi difference (Theorem 3.2). Since the privacy loss \(\) gradually decays with respect to unlearning iterations, we refer to this phenomenon as **privacy recuperation**. This is in contrast to the learning process, where prior work  has shown the worse privacy loss \(_{0}\) with respect to learning iterations and refers to that phenomenon as **privacy erosion**.

### Unlearning Guarantees

Our first Theorem shows that \((,)\) achieves \((,)\)-RU, where \(\) decays monotonically in \(K\) unlearning iterations starting from \(_{0}\), condition on \(\) being \((,_{0})\)-RDP. We provide the proof sketch in Appendix G.1 and formal proofs are deferred to Appendix G.2.

**Theorem 3.2** (RU guarantee of PNGD unlearning).: _Assume for all \(^{n}\), \(f_{}\) is \(L\)-smooth, \(M\)-Lipschitz and \(_{}\) satisfies \(C_{}\)-LSI. Let the learning process follow the PNGD update (1). Given \(\) is \((,_{0})\)-RDP and \(y_{0}=x_{}=()\), for \(>1\), the output of the \(K^{th}\) unlearning iteration along (2) (i.e., \(y_{K}\)) achieves \((,)\)-RU, where \((-_{k=0}^{K-1}R_{k}) _{0}\) and \(R_{k}>0\) depends on the problem settings specified as follows:_

_1) For a general non-convex \(f_{}\), we have \(R_{k}=(C_{k})^{2}}-C_{k}+2^{2})^{2}}),\) where \(C_{k+1}((1+ L)^{2}C_{k}+2^{2},)\), \(=6(4(R+ M)^{2}+2^{2})(}{2 ^{2}})\), where \(C_{0}=C_{}\) and \(R\) is the radius of the projected set \(_{R}\)._

_2) Suppose \(f_{}\) is convex. By choosing \(\), we have \(R_{k}=()^{2}}-+2^{2}) ^{2}}),\) where \(C_{k+1}(C_{k}+2^{2},)\)._

_3) Suppose \(f_{}\) is \(m\)-strongly convex. Let \(}{m}<C_{}\) and choosing \(((1-}{mC_{}}),)\). Then, \(R_{k}=}{C_{}}\)._

Note that \(R_{k}\) can be interpreted as the strict privacy improving rate at step \(k\) and \(C_{k}\) is the LSI constant upper bound of distribution of \(y_{k}\). The above theorem states that fine-tuning with PNGD can decrease the privacy loss \(d_{}(_{K},_{^{}})\)_exponentially fast_ with the unlearning iteration \(K\). This is because \(R_{k}\) is lower bounded away from \(0\) by a constant, thanks to the iteration independent upper bound on \(C_{k}\). Stronger assumptions on the objective function \(f_{}\) lead to a better rate, which implies fewer unlearning iterations are needed to achieve the same RU guarantee. There are several remarks for our Theorem 3.2. First, note that the result is _dimension-free_, which is favorable for problems with many parameters to be learned. Second, note that the \(M\)-Lipschitzness assumption can be dropped by clipping the gradient to norm \(M\) in the PNGD update (1) and (2) instead. As a result, our Theorem 3.2 applies to neural networks with smooth activation functions in theory. Finally, our result gives an _upper bound_ on the LSI constants along the unlearning process (i.e., \(C_{k}\)) which may be improved with more advanced analysis. We note that the exponential dependence in \(R\) for the bound of \(C_{k}\) can be loose. It is possible to have a better constant with either more structural assumptions or working with different isoperimetric inequalities such as (weak) Poicare inequality . A more detailed discussion is in Appendix 5.

**Initial RDP guarantees and LSI constant.** Since Theorem 3.2 relies on \(\) being \((,_{0})\)-RDP and the \(_{}\) satisfies LSI, the theorem below provides such results for the learning process, where the formal proof is relegated to Appendix H.

**Theorem 3.3** (RDP guarantee of PNGD learning).: _Assume \(f(;)\) be \(L\)-smooth and \(M\)-Lipschitz for all \(\). Also assume that the initialization of PNGD (1) satisfies \(C_{0}\)-LSI. Then the learning process (1) is \((,_{0}^{(S)})\)-RDP of group size \(S 1\) at \(T^{th}\) iteration with_

\[_{0}^{(S)}M^{2}}{^{2}n^{2}}_{t= 1}^{T}_{t^{}=0}^{t-1}(1+}{C_{t^{},1}})^{ -1},\]

_where \(C_{t,1}((1+ L)^{2}C_{t}+^{2},)\), \(=6(4(R+ M)^{2}+^{2})(}{ ^{2}})\) and \(C_{t+1}(C_{t,1}+^{2},)\). Furthermore, \(_{t}\) satisfies \(C_{t}\)-LSI._

_When we additionally assume \(f(;)\) is convex, by choosing \(\) the same result hold with \(C_{t,1}(C_{t}+^{2},)\).__When we additionally assume \(f(;)\) is \(m\)-strongly convex, by choosing \(0<((1-}{mC_{0}}),)\) with a constant \(C_{0}>}{m}\), we have \(_{0}^{(S)}M^{2}}{m^{2}n^{2}}(1-(-m  T))\). Furthermore, \(_{t}\) satisfies \(C_{0}\)-LSI for all \(t 0\)._

Note that any initialization \(x_{0}_{R}\) can be viewed as sampling from \((x_{0},cI_{d})\) with \(c 0\), which corresponds to \(C_{0}\)-LSI for any \(C_{0}>0\). By taking \(T\), Theorem 3.3 provides the initial \((,_{0})\)-RDP guarantee and the LSI constant needed in Theorem 3.2. Since there is an iteration-independent upper bound for \(C_{t,1}\), one can show that \(_{0}M^{2}}{^{2}n^{2}c}\) for some \(T\)-independent constant \(c(0,1)\) due to the finiteness of geometric series. Similar to our discussion for Theorem 3.2, the bound of \(C_{t}\) may be loose and it is possible to further improve the LSI constant analysis. The goal of our results is to demonstrate that it is possible to derive (finite) RDP and (arbitrarily small) RU guarantees even for general non-convex problems.

Nevertheless, for the \(m\)-strongly convex case we have \(_{0}^{(S)}M^{2}}{m^{2}n^{2}}\) for all \(T>0\), including \(T\). It shows that indeed the current learned distribution \(_{}\) is close to the retraining distribution \(_{^{}}\) for \(n\) sufficiently large. This also leads to the computational benefit of Langevin unlearning compared to retraining from scratch, which we discuss below. On the other hand, we show by experiments in Section 4 that our results provide a superior privacy-utility-complexity trade-off for the strongly convex case compared to existing approximate unlearning approaches.

**The computational benefit compared to retraining.** While our Theorems 3.2 and 3.3 together provide the privacy guarantee of Langevin unlearning, it is critical to check if our approach provides a computational benefit compared to retraining from scratch as well. Let \(_{0}\) be the (data-independent) initialization distribution of the learning process. Intuitively, starting with \(_{}\) instead of \(_{0}\) (i.e., retraining) should converge faster to \(_{^{}}\), since \(d_{}(_{},_{^{}})_{0}\) is likely to be much smaller than \(d_{}(_{0},_{^{}})\). Thus, our Langevin unlearning needs less iterations than retraining for most cases, except for a corner case when \(_{0}\) is already close to \(_{}\). From Theorem 3.2 we know that the number of PNGD iterations we need to approach \(\)-close in \(d_{}\) to the target distribution \(_{^{}}\) is roughly \(O((}{L}))\), where \(_{I}\) is the Renyi difference between the initial distribution and the target distribution \(_{^{}}\). From Theorem 3.3, we know that the initial Renyi difference of Langevin unlearning is at most \(_{0}=O(1/n^{2})\) for any datasets \(,^{}\) and any smooth Lipchitz loss. In contrast, even if both the target distribution \(_{^{}}\) and the initialization of retraining \(_{0}\) are Gaussian distributions with the same variance but mean difference \((1)\), their Renyi difference is \((1)\). As a result, computational saving offered by Langevin unlearning is significant for sufficiently large \(n\). A more thorough discussion is in Appendix E.

### Empirical Aspects of Langevin Unlearning

**Insufficient training.** While our theorem assumes the learning process runs until convergence, this assumption can be relaxed by the geometric view of Langevin unlearning. Assume the learning process \(()_{T}\) terminate with finite step \(T\) instead and we only have \(d_{}(_{T},_{})_{T}()\) for all possible \(^{n}\). One can still apply the weak triangle inequality of Renyi divergence  twice to bound \(d_{}(_{k},_{T}^{})\) with \(d_{4}(_{k},_{^{}})\), \(_{T}(2)\), and \(_{T}(4)\) with additional factors \((-0.5)/(-1)\) and \((2-0.5)/(2-1)\). In practice, it is reasonable to require the model parameters to be sufficiently trained so that \(_{T}\) is negligible and a tighter weak triangle inequality can be employed.

**Sequential and batch unlearning.** Langevin unlearning naturally supports sequential and batch unlearning for unlearning multiple data points thanks to our geometric view of the unlearning problem, see Figure 2 for a pictorial example. For sequential unlearning, we show that fine-tuning the current model parameters on the updated datasets for sequential \(S 1\) unlearning requests can achieve \((,)\)-RU simultaneously. The formal proof is deferred to Appendix I.

Figure 2: Illustration of (a) sequential unlearning and (b) batch unlearning. For sequential unlearning, we can leverage the weak triangle inequality of Rényi divergence to connect all the error terms. For batch unlearning, only the initial RDP guarantee changes with a general group size. Notably, unlearning more samples at once implies \(_{0}\) being larger (Theorem 3.3), and thus we need more unlearning iteration to recuperate the privacy loss to a desired \(\).

**Corollary 3.4** (Sequential unlearning).: _Assume the unlearning requests arrive sequentially such that our dataset changes from \(_{0}_{1} _{S}\), where \(_{s},_{s+1}\) are adjacent. Let \(y_{k}^{(s)}\) be the unlearned parameters for the \(s^{th}\) unlearning request with \(k\) unlearning update following (2) on \(_{s}\) and \(y_{0}^{(s+1)}=y_{K_{s}}^{(s)}_{_{s}}\), where \(y_{0}^{(1)}=x_{}\) and \(K_{s}\) is the unlearning steps for the \(s^{th}\) unlearning request. Suppose we have achieved \((,^{(s)}())\)-RU for the \(s^{th}\) unlearning request, the learning process (1) is \((,_{0}())\)-RDP and \(_{_{s}}\) satisfies \(C_{}\)-LSI, we achieve \((,^{(s+1)}())\)-RU for the \((s+1)^{th}\) unlearning request as well, where_

\[^{(s+1)}()(-_{k=0}^{K_{s+1}-1}R_ {k})(_{0}(2)+ ^{(s)}(2)),\]

\(^{(0)}()=0\)__\(>1\) _and_ \(R_{k}\) _are defined in Theorem_ 3.2_._

As a result, one can leverage Corollary 3.4 to recursively determine needed unlearning iterations for each sequential unlearning request. For the batch unlearning setting, it only affects the initial Renyi difference in Theorem 3.2. We can simply adopt Theorem 3.3 with a group size of \(S 1\) for the RDP guarantees of the learning process \(_{0}^{(S)}\).

**Utility-privacy-efficiency trade-off.** An interesting aspect of the Langevin unlearning is its strong connection to the initial RDP guarantee. From Theorem 3.3, we know that increasing \(\) leads to smaller Renyi difference \(_{0}\) and thus better unlearning efficiency. However, this intuitively is at the cost of the utility of \(_{}\), see for example the discussion in Section 5 of  under the strong convexity assumption. To achieve the same \((,)\)-RU guarantee, one can either ensure smaller \(_{0}\) at the cost of worst utility or run more unlearning iterations at the cost of unlearning efficiency. We investigate how utility trade-off with privacy and unlearning complexity empirically in Section 4.

Figure 3: Main experiments, where the top and bottom rows are for MNIST and CIFAR10 respectively. (a) Compare to D2D for unlearning one point using limited unlearning iteration. This demonstrates the privacy-utility (\(\)-accuracy) tradeoff under the fixed unlearning complexity (K). For Langevin unlearning, we use only \(K=1\) unlearning iterations. For D2D, we allow it not only to use \(K=1,2,5\) unlearning iterations but also to keep the non-private internal state information. (b) Compare to D2D for unlearning \(100\) points, where all methods achieve \((,1/n)\)-unlearning guarantee with \(=1\). For Langevin unlearning, we vary different unlearning batch sizes \(S\) and combine them with the sequential unlearning result. For D2D, we do not allow it to keep the non-private internal state information in this experiment so that there is an inherent lower bound on the unlearning iterations per unlearning request. (c) A detailed investigation of the utility-complexity trade-off of Langevin unlearning with unlearning \(S=100\) points at once under the fixed privacy constraint \(=1\). For each \(\), we report the corresponding \(_{0}\) (black dash line) for the initial \((_{0},1/n)\)-DP guarantee and the utility after unlearning to \(=1\).

Experiments

_Benchmark datasets._ We consider logistic regression with \(_{2}\) regularization. We focus on this strongly convex setting since the non-convex unlearning bound in Theorem 3.2 currently is not tight enough to be applied in practice due to its exponential dependence on various hyperparameters. However, we emphasize its significant theoretical implication due to the lack of a certified non-convex approximate unlearning framework in previous studies. Meanwhile, the existing baseline approach  also only applies to strongly convex problems. We conduct experiments on MNIST  and CIFAR10 , which contain 11,982 and 50,000 training instances respectively. We follow the setting of  to distinguish digits \(3\) and \(8\) for MNIST so that the problem is a binary classification. For the CIFAR10 dataset, we use all classes and leverage the last layer of the public ResNet18  embedding as the data features, which follows the public feature extractor setting of . Experiments on additional datasets  are deferred to Appendix M and our code is publicly available3.

_Baseline methods._ Our baseline methods include Delete-to-Descent (D2D) , the state-of-the-art gradient-based approximate unlearning method, and retraining from scratch using PNGD. For D2D, we leverage Theorem 9 and 28 in  for privacy accounting depending on whether we allow D2D to have an internal non-private state. Note that allowing an internal non-private state provides a weaker notion of privacy guarantee  and our Langevin unlearning by default does not require it. We include those theorems for D2D and a detailed explanation of its possible non-privacy internal state in Appendix N for completeness. All experimental details can be found in Appendix M, including how to convert \((,)\)-RU to the standard \((,)\)-unlearning guarantee. Throughout this section, we choose \(=1/n\) for each dataset and require all tested unlearning approaches to achieve \((,)\)-unlearning with different \(\). We report test accuracy for all experiments as the utility metric. For the initialization, we sample from Gaussian distribution with mean \(1000\). This simulates the case that the initial distribution is in a reasonable distance away from the convergent distribution \(_{}\). We set the learning iteration \(T=10,000\) to ensure all approaches converge. For Langevin unlearning, we leverage Theorems 3.2, 3.3 and Corollay 3.4 for privacy accounting under different settings. All results are averaged over \(100\) independent trials with standard deviation reported as shades in all figures.

**Unlearning one data point with \(K=1\) iteration.** We first consider the setting of unlearning one data point using only \(K=1\) unlearning iteration for both Langevin unlearning and D2D (Figure 2(a)). Since D2D cannot achieve a privacy guarantee with only \(1\) unlearning iteration without a non-private internal state, we allow D2D to have it in this experiment. Even in this case, our Langevin unlearning significantly outperforms D2D in utility for \(\) from \(0.1\) to \(5\) under the same unlearning complexity (\(K=1\)), but also achieves similar accuracy to retraining from scratch. Since retraining requires \(T=10,000\) PNGD iterations, Langevin unlearning is indeed much more efficient. We also show that D2D can achieve better utility at the cost of a larger unlearning iteration \(K=2,5\). Our Langevin unlearning exhibits both smaller unlearning complexity and better utility compared to D2D.

**Unlearning multiple data points.** We now consider the scenario of unlearning \(100\) data points, where the results are in Figure 2(b). We let all methods achieve the same \((1,1/n)\)-unlearning guarantee for a fair comparison. Since D2D only supports sequential unlearning, we directly apply its sequential unlearning results . Also, we do not allow D2D to have an internal non-private state in this experiment for a fair comparison. On the other hand, since Langevin unlearning supports both sequential and batch unlearning, we vary the number of points per unlearning request \(S=5,10,20\) and report the accumulated unlearning iterations for \(=0.03\). All methods achieve a similar utility, with an accuracy of roughly \(0.9\) and \(0.98\) for MNIST and CIFAR10 respectively. Langevine unlearning can achieve a significantly better unlearning complexity compared to D2D if one allows for a larger unlearning batch size. For instance, when we are allowed to unlearn \(S=20\) points at once, Langevine unlearning saves \(40\%\) unlearning iteration compared to D2D. Nevertheless, we note that due to the use of weak triangle inequality of Renyi divergence in our analysis, Langevin unlearning can be more

Figure 4: Trade-off between privacy (\(\)), unlearning complexity (\(K\)), and the number of points to be unlearned (\(S\)) in the batch unlearning setting for MNIST. We fix \(=0.03\) so that \(K\) can be determined given \((,S)\).

expensive in complexity compared to D2D when one only allows for unlearning a small batch of points (i.e, \(S=5\)). We leave the improvement in this direction as the future work.

**Privacy-utility-complexity trade-off.** We further examine the inherent privacy-utility-complexity trade-off provided by our Langevin unlearning with two experiments. In the first experiment, we aim to achieve \((,1/n)\)-unlearning guarantee with \(=1\) for batch unlearning of \(100\) points. We vary the choice of \(\) from \(0.01\) to \(1.0\). A smaller \(\) leads to a worse initial \(_{0}\) and thus requires more unlearning iteration \(K\) to recuperate it to \(=1\). It is interesting to see that even if we choose a small \(\) so that the initial \((_{0},1/n)\)-DP guarantee is extremely weak (i.e, \(_{0} 100\) for \(=0.05\)), our unlearning iteration can recuperate \(_{0}\) to \(=1.0\) efficiently. On the other hand, a larger \(\) leads to a worse utility which is the inherent privacy-utility-complexity trade-off of Langevin unlearning. The results are illustrated in Figure 2(c). Compared to retraining until convergence (\(T=10,000\)), we achieve a similar utility but with much lower unlearning complexity with \(K\) roughly up to \(2500\).

In the second experiment, we investigate the effect of the number of points to be unlearned \(S\) in the batch unlearning setting. In Figure 4, we can see that both larger \(S\) and smaller \(\) will require more unlearning iterations \(K\). It is worth noting that the resulting utility does not change significantly, whereas Langevin unlearning always archives a similar utility compared to retraining (see Figure 4(a) in Appendix M). Retraining requires \(T=10,000\) PNGD iterations which is significantly larger than the required unlearning iteration \(K\) even for \(=0.5\). We have shown that Langevin unlearning is a promising unlearning solution.

## 5 Conclusions and Future Directions

We propose Langevin unlearning based on noisy gradient descent with privacy guarantees for approximate unlearning problems. It unifies the DP learning process and the privacy-certified unlearning process with many algorithmic benefits such as applicability to non-convex problems and multiple unlearning requests. Below we discuss several future directions for Langevin unlearning.

**Extension to projected noisy stochastic gradient descent.** It is straightforward to extend our analysis to the projected noisy SGD case. There are two possibilities for the SGD setting: 1) randomly partition the indices \([n]\) into a sequence of mini-batches, then fix this sequence for all the learning and unlearning process ; 2) randomly draw a mini-batch for each update . The analysis of  can be combined with our LSI constant analysis for RU guarantees, similar to the proof of our Theorem 3.2. Unfortunately, the analysis  may lead to an extra large LSI constant in the intermediate step even if \(R\) is small. We refer interested readers to Appendix C of  for a detailed discussion. The technical difficulty here is to provide a tight analysis of the LSI constant for a mixture of distributions, where each of them corresponds to a possible choice of mini-batch. The analysis of  is based on privacy amplification by iteration, which does not directly generalize to the non-convex cases. In our companion work , we exploit it not only to establish a better unlearning result but also to enable the mini-batch setting under the convexity assumption. It is currently an open problem whether a matching result can be established via Langevin dynamic analysis as well.

**Better convergence rate.** While it is already exciting that Langevin dynamic analysis leads to formal unlearning algorithms and guarantees even for general non-convex problems in theory, the potential of this direction for a _practical_ plug-and-play unlearning solution is even more interesting. Several promising future directions can significantly improve the convergence rate and the unlearning efficiency. Developing a better LSI constant bound under additional structural assumptions for the non-convex problems is the most straightforward one. Another direction is to work with (weak) Poincare inequality instead. While a weaker tail assumption leads to slower convergence , the corresponding (weak) Poincare constant may be more tightly tracked. Finally, while we only discuss the noisy GD which corresponds to Langevin Monte Carlo, some other advanced samplers are off-the-shelf including the Metropolis-Hastings filter  and Hamiltonian Monte Carlo . We hope our work motivates further collaborations among the sampling and privacy communities and pushes the boundaries of learning and unlearning with privacy guarantees.