# scBiGNN: Bilevel Graph Representation Learning for Cell Type Classification from Single-cell RNA Sequencing Data

scBiGNN: Bilevel Graph Representation Learning for Cell Type Classification from Single-cell RNA Sequencing Data

Rui Yang\({}^{1,2}\), Wenrui Dai\({}^{1}\), Chenglin Li\({}^{1}\), Junni Zou\({}^{1}\), Dapeng Wu\({}^{2}\), Hongkai Xiong\({}^{1}\)

\({}^{1}\)Shanghai Jiao Tong University, \({}^{2}\)City University of Hong Kong

{rui_yang, daiwenrui, xionghongkai}@sjtu.edu.cn, dapengwu@cityu.edu.hk

###### Abstract

Single-cell RNA sequencing (scRNA-seq) technology provides high-throughput gene expression data to study the cellular heterogeneity and dynamics of complex organisms. Graph neural networks (GNNs) have been widely used for automatic cell type classification, which is a fundamental problem to solve in scRNA-seq analysis. However, existing methods do not sufficiently exploit both gene-gene and cell-cell relationships, and thus the true potential of GNNs is not realized. In this work, we propose a bilevel graph representation learning method, named scBiGNN, to simultaneously mine the relationships at both gene and cell levels for more accurate single-cell classification. Specifically, scBiGNN comprises two GNN modules to identify cell types. A gene-level GNN is established to adaptively learn gene-gene interactions and cell representations via the self-attention mechanism, and a cell-level GNN builds on the cell-cell graph that is constructed from the cell representations generated by the gene-level GNN. To tackle the scalability issue for processing a large number of cells, scBiGNN adopts an Expectation Maximization (EM) framework in which the two modules are alternately trained via the E-step and M-step to learn from each other. Through this interaction, the gene- and cell-level structural information is integrated to gradually enhance the classification performance of both GNN modules. Experiments on benchmark datasets demonstrate that our scBiGNN outperforms a variety of existing methods for cell type classification from scRNA-seq data.

## 1 Introduction

Single-cell RNA sequencing (scRNA-seq), which allows the measurement of gene expression at the resolution of individual cells, has revolutionized transcriptomic analysis and greatly improved the understanding of biomedical science over the past decade. Accurate cell type annotation is essential for elucidating cellular states and dynamics in scRNA-seq data mining  and contributes significantly to a broad range of downstream analyses, such as cancer biology  and drug development . In early studies, the cluster-then-annotate paradigm is commonly used to categorize cell types [8; 17]. These methods rely on manually selected marker genes and require prior knowledge of cell types, which is highly subjective and prone to errors due to the unknown quality of clustering results. As copious annotated scRNA-seq data become publicly available, many classification methods have been developed for automatic cell type labeling.

Existing classification methods can be grouped into three categories: 1) traditional machine learning algorithms, 2) similarity-based measurement and 3) deep learning models. The first category of methods applies classic machine learning approaches to scRNA-seq data analysis, such as random forest, linear discriminant analysis and support vector machine. Typical works include CasTLe ,scPred  and scID . These methods commonly have limited model capacity and do not scale well to large datasets. The second category is based on some similarity criteria to measure the correlation between the unlabeled cells and the reference dataset. For example, scmap  calculates the similarity between the query cell and the median gene expression values for each cell type in the reference dataset, while SingleR  computes Spearman correlation between cells using variable genes. Similarity-based methods are heavily affected by the batch effect caused by variant experimental conditions , and the adopted similarity measurement may not be suitable for the high-dimensional and sparse scRNA-seq data . To enable scalable and robust cell type identification, deep learning models have been increasingly explored and achieved superior performance. For instance, ACTINN  first uses fully connected neural networks for cell type identification. scCapsNet  employs capsule networks  for interpretable gene feature selection in cell representation learning, which improves the reliability of deep learning models. Furthermore, graph neural networks (GNNs) have emerged as widely used tools for cell type annotation [28; 32; 13; 30; 26], as they can leverage the intrinsic biological networks (e.g., gene-gene interaction network) within gene expression profiles for more expressive representation learning on scRNA-seq data.

However, several problems still impede GNN-based classification methods from reaching their full potential. Firstly, although it is generally acknowledged that gene-gene and cell-cell relationships are both beneficial to scRNA-seq analyses [13; 30], none of the existing models consider these two kinds of structural information simultaneously. To be concrete, gene-level GNNs classify cells from the local biological perspective of each cell's own gene expression profile, which leverages informative gene interactions to improve cell representations but relationships between different cells are ignored. Cell-level GNNs consider the connectivity between cells from the global view of the whole database but cannot well capture the fine-grained gene-level biological contexts. Secondly, most existing works rely on predetermined biological networks for graph representation learning. For example, sigGCN  utilizes the STRING database  to construct gene-gene interaction network, while Li et al.  consider to precompute the cell-cell graph by \(k\)-nearest neighbors analysis based on the gene expression data. Nevertheless, it is hard to evaluate whether these graphs constructed by task-irrelevant knowledge are optimal for cell type classification. Learning task-driven adaptive graphs may better serve GNN-based models for more accurate supervised single-cell classification.

To address these issues, we propose scBiGNN, a novel bilevel graph representation learning method that takes advantage of the above two kinds of biological networks in scRNA-seq classification. Specifically, scBiGNN consists of two GNN modules for cell type classification. A gene-level GNN based on the gene-gene interaction graph is designed to produce a representation for each individual cell, in which the interactions are adaptively learned via self-attention [23; 24]. Then the cell-cell graph is constructed by linking cells that have similar representations generated by the gene-level GNN (i.e., neighboring cells are more likely from the same class), upon which a cell-level GNN is built to improve cell representations by aggregating features from the neighborhood. Such a bilevel framework can integrate both gene- and cell-level structural information for more comprehensive scRNA-seq graph representation learning.

In real-world scRNA-seq datasets, the dimension of the gene expression data is commonly very high. Therefore, it is challenging to optimize our scBiGNN in an end-to-end manner when both the gene-gene and cell-cell graphs are large. Inspired by previous studies that optimize two neural networks in a whole framework [18; 35; 31], we employ the Expectation Maximization (EM) method to alternately train the two GNN modules via the E-step and M-step, which theoretically maximizes the evidence lower bound of the log-likelihood of the observed cell labels. In each step, one GNN is fixed to generate pseudo-labels for optimizing the other. In this alternating training fashion, scBiGNN can scale to large datasets, and gradually enhance the classification performance of both GNN modules by reinforcing each other. To validate the effectiveness of our proposed method, we conduct cell type classification on several popular benchmark datasets. Experimental results demonstrate that our scBiGNN achieves superior performance over a number of baselines.

## 2 Background

**Data Description and Pre-processing.** The scRNA-seq data can be represented as a gene expression matrix \(^{N M}\), where \(N\) is the number of cells, \(M\) is the number of genes, and each element \(X_{ij}\) indicates the expression counts of the \(j\)-th gene in the \(i\)-the cell. First, genes that have zero expression values across all the cells are removed. Then, the gene expression data of each cell is normalizedby \(_{ij}=(1+s}{_{m}X_{im}})\) in which \(s\) is a scaling value and is set as \(10^{6}\) following existing work . After normalization, the variance value of each gene across all the cells is calculated, and the top-\(T\) variable genes are selected (e.g., \(T=1000\)) while others are filtered out, based on which we can obtain the pre-processed gene expression data \(}^{N T}\).

**Graph Neural Networks (GNNs).** The goal of GNNs is to learn effective node and graph representations by iteratively aggregating information from the topological neighborhoods. Formally, let \(=(,)\) denote a graph where \(=\{v_{1},,v_{n}\}\) is the node set and \(^{n n}\) is the adjacency matrix with each non-zero value \(A_{ij}\) representing the edge between nodes \(v_{i}\) and \(v_{j}\). We also have a feature matrix \(=[_{1},,_{n}]^{}^{n f}\) with \(_{i}^{f}\) being the feature vector of node \(v_{i}\). The \(l\)-th feature aggregation layer of GNNs can take the form of \(^{(l)}=^{(l)}(^{(l)}( )^{(l-1)}^{(l)})\), in which \(^{(l)}=[_{1}^{(l)},,_{n}^{(l)}]^{} ^{n f_{l}}\) is the matrix of node representations after \(l\) aggregation layers, \(^{(0)}=\), \(^{(l)}^{f_{l-1} f_{l}}\) is a learnable weight matrix, \(^{(l)}()\) is some operator on the adjacency matrix such as normalization  and self-attention mechansim , and \(^{(l)}()\) is the multi-layer perception (MLP) for nonlinear feature transformation. After \(L\) aggregation layers, we can obtain the node representations \(^{(l)}\) (\(l=0,,L\)). To produce a graph-level representation, we can employ a read-out function that pools all the node representations into a single vector \(_{}=(\{_{l} \}_{l=0}^{L})\), which is permutation invariant to the order of graph nodes, e.g., sum-pooling and max-pooling.

**GNN-based Single-cell Classification.** Existing GNN-based methods for cell type identification can be categorized into two groups. The first group leverages gene-gene relationships. Specifically, Yin et al.  collect several well-known gene interaction networks such as the STRING database , on which GNNs are applied to aggregate information from interacting genes for each expressed gene to improve cell representations. Wang et al.  also utilize the STRING database and propose sigGCN, in which a GNN-based autoencoder is used to reconstruct gene expression data and a fully connected neural network extracts features by taking the scRNA-seq data as input. The features generated by the encoder part of the autoencoder and the fully connected neural network are concatenated for cell type classification. HNNVAT  further introduces virtual adversarial training that adds noise to input data to make the model robust against noise perturbations. In addition, Yang et al.  propose scBERT, which follows BERT's pretraining and fine-tuning paradigm  with vast amounts of unlabelled scRNA-seq data and learns gene-gene interactions via Performers . The second group of methods focuses on cell-level relationships. As more and more research demonstrates that cell-cell graph provides valuable structural information to learn effective cell representations for scRNA-seq analyses [19; 25; 5; 33; 34], Li et al.  benchmarks several GNNs for cell type classification based on the cell-cell graph and outperforms traditional machine learning methods. These results motivate us to combine the merits of these two types of methods to integrate gene- and cell-level structural information for more accurate cell type classification.

## 3 The Proposed Method: scBiGNN

In this section, we develop the scBiGNN framework for bilevel graph representation learning on scRNA-seq data, in which both gene- and cell-level relationships are exploited for more accurate cell type classification. We first introduce some preliminaries about the problem statement and the workflow of gene- and cell-level GNNs. Then, we overview our EM-based learning framework and elaborate the optimization procedures of the E-step and M-step respectively.

### Preliminaries

**Problem Statement.** In our work, two biological networks are constructed based on the pre-processed gene expression data \(}^{N T}\). One is the gene-gene interaction graph \(^{}=(^{},^{})\) where \(^{}\) is the set of \(T\) genes \(\{v_{1}^{},,v_{T}^{}\}\) and \(^{}^{T T}\) is the associated adjacency matrix representing the interactions between genes, the other is the cell-cell graph \(^{}=(^{},^{})\) where \(^{}=\{v_{1}^{},,v_{N}^{}\}\) indicates the set of \(N\) cells in the scRNA-seq dataset and \(^{}^{N N}\) is its adjacency matrix encoding the relationships between cells. Given \(}\) and the labels \(^{L}^{|^{L}| C}\) for a subset of annotated cells \(^{L}^{}\), the goal is to predict the labels \(^{U}^{|^{U}| C}\) for the the unlabeled cells \(^{U}=^{}^{L}\), where \(C\) is the number of cell types.

**Gene-level GNNs.** The gene-level GNNs aim to predict the label of each cell \(v_{i}^{}\) using its own gene expression data \(}_{i}\) (the \(i\)-th row of \(}\)) and the gene-gene interaction network. Formally, they model the following label distribution \(q_{}(_{i}|}_{i})\) for each \(v_{i}^{}^{}\) with parameters \(\):

\[\{_{i,t}\}_{i=1}^{T_{i}} =^{}(}_{i}, ^{}),\] \[_{i} =(\{_{i,t}\}_{t=1}^{T_{i}}),\] \[q_{}(_{i}|}_{i}) =(_{i}\,^{ }(_{i})), \]

where \(^{}()\) denotes the node representation learning function of the gene-level GNN, \(\{_{i,t}\}_{t=1}^{T_{i}}\) is the set of gene representations learned by \(^{}()\), \(T_{i}\) is the number of genes that have non-zero expression values in \(}_{i}\), \(_{i}\) is the representation of cell \(v_{i}^{}\), \(^{}()\) denotes a classifier, and \(()\) stands for the categorical distribution. In this way, gene-gene structural information can be exploited from the local biological view of each individual cell to enhance cell representation learning.

**Cell-level GNNs.** The cell-level GNNs classify cell types by using the relationships among all the cells in the dataset. Specifically, the workflow can be characterized as below:

\[\{_{i}^{}\}_{i=1}^{N}=^{} (\{}_{i},_{i}\}_{i=1}^{N}\,,^{ }),\] \[p_{}(_{i}|}) =(_{i}\,^{ }(_{i}^{})), \]

where \(^{}()\) is the node representation learning function, \(\{_{i}^{}\}_{i=1}^{N}\) is the set of cell representations learned by \(^{}()\), \(^{}()\) is a classifier, and \(\) is the model parameters. By aggregating features from neighboring cells with similar characteristics, cell-level GNNs extract the cell-cell structural information from a global view of the scRNA-seq dataset to improve single-cell classification.

Both gene- and cell-level GNNs have proved effective in scRNA-seq analyses and achieved superior performance for automatic cell type identification. Therefore, we propose scBiGNN, a bilevel graph representation learning method that aims to integrate these two levels of structural information to have a more comprehensive view of scRNA-seq data.

### EM Framework

Directly cascading the above gene- and cell-level GNNs and training them end-to-end would face the scalability issue when the size of the scRNA-seq dataset is large. Moreover, as these two types of GNNs are complementary, end-to-end learning does not allow them to interact and enhance each other. Therefore, inspired by several previous studies [18; 35; 31], we instead employ the EM algorithm to train these two GNN modules alternately and make them gradually reinforce each other for more accurate cell type classification.

To be concrete, our proposed scBiGNN maximizes the evidence lower bound (ELBO) of the log-likelihood of the observed cell labels:

\[ p_{}(^{L}|})_{q_{}( ^{U}|})}[ p_{}(^{L}, ^{U}|})- q_{}(^{U}|})]_{}(,;^ {L},}), \]

where \(q_{}(^{U}|})\) can be arbitrary distribution over \(^{U}\) (s.t. \(q_{}(^{U}|})>0\) if \(p_{}(^{L},^{U}|})>0\)), and the equality holds when \(q_{}(^{U}|})\) equals to the true posterior distribution \(p_{}(^{U}|^{L},})\). As can be seen, Eq. (3) formalizes the objective function with two distributions \(q_{}\) and \(p_{}\), which can be modeled by the abovementioned gene- and cell-level GNNs respectively. According to the EM framework, we alternately train \(q_{}\) in the E-step (with \(p_{}\) fixed) and \(p_{}\) in the M-step (with \(q_{}\) fixed) to maximize \(_{}(,;^{L},})\). Thus, we can separately optimize these two GNN modules through several iterations, leading to better scalability and making them interact and enhance each other. In what follows, we introduce the structures of these two GNN modules and how they reinforce each other in the training procedure.

### E-step

In the E-step, the cell-level GNN (i.e., \(p_{}\)) is fixed and the gene-level GNN (i.e., \(q_{}\)) is optimized to maximize \(_{}(,;^{L},})\). In this section, we first present the detailed structure of our employed gene-level GNN, and then we introduce the optimization of \(q_{}\) in the E-step.

**Structure of Gene-level GNN.** The input of the gene-level GNN module includes two embeddings. The first is gene embedding, which assigns a unique and learnable vector \(_{j}^{}^{d_{g}}\) to each gene \(v_{j}^{g}\). The second is count embedding, which transforms every non-zero gene expression value \(_{ij}\) of cell \(v_{i}^{e}\) to a \(d_{g}\)-dimensional vector through an MLP, i.e., \(_{ij}^{}=(_{ij}) ^{d_{g}}\). The input embedding of gene \(v_{j}^{g}\) in cell \(v_{i}^{e}\) is then defined as \(_{ij}=_{j}^{}+_{ij}^{}\). By gathering and stacking all the input embeddings of genes that have non-zero expression values in cell \(v_{i}^{e}\), we have the input embedding matrix \(_{i}^{T_{i} d_{g}}\) for cell \(v_{i}^{e}\).

Inspired by attention-based models [23; 24], the gene-level GNN of scBiGNN adaptively learns gene-gene interactions via the self-attention mechanism. Specifically, the feature aggregation layer of \(^{}()\) is defined as

\[^{(l)}(^{}) =((_{i}^{(l-1)}_{ }^{(l)})(_{i}^{(l-1)}_{}^{(l)})^{} ),\] \[_{i}^{(l)} =_{}^{(l)}(^{(l) }(^{})_{i}^{(l-1)}_{ }^{(l)}), \]

where \(()\) is the row-wise softmax operation, \(_{}^{(l)}\) is a learnable weight matrix, \(_{}^{(l)}()\) is an MLP, \(_{i}^{(l)}\) is the output feature matrix of the \(l\)-th layer, and \(_{i}^{(0)}=_{i}\) for each cell \(v_{i}^{e}\). Moreover, in each layer of \(^{}()\), we can also employ the multi-head attention mechanism , which performs several different attention functions of Eq. (4) and concatenates their outputs as the feature matrix. After \(L\) layers, we can obtain the set of gene representations \(\{_{i,t}^{j}\}_{t=1}^{T_{i}}\) in which \(_{i,t}^{j}\) is the \(t\)-th row of \(_{i}^{(L)}\) and the superscript \(j\) indicates that it corresponds to the gene \(v_{j}^{g}\). The read-out function in Eq. (1) can take the form of the weighted sum of all the gene representations to produce the cell representation for \(v_{i}^{e}\):

\[_{i}=(\{_{i,t}^{j}\}_{t=1}^{T_ {i}})=_{j}_{j}_{i,t}^{j}, \]

where \(_{j}\) can be a learnable weight for gene \(v_{j}^{g}\) (\(j=1,,T\)) or set as \(1/T_{i}\) which performs average pooling. Then, \(q_{}(_{i}|}_{i})\) can be obtained by feeding \(_{i}\) to an MLP classifier, as shown in Eq. (1).

**Optimization of Gene-level GNN.** Maximizing \(_{}(,;^{L},})\) in the E-step is equivalent to making \(q_{}(^{U}|})\) approximate the true posterior distribution \(p_{}(^{U}|^{L},})\). According to the EM framework, the standard operation is to minimize the Kullback-Leibler (KL) divergence \((q_{}(^{U}|})\|p_{}( ^{U}|^{L},}))\). However, directly optimizing the KL divergence is nontrivial, as it depends on the entropy of \(q_{}(^{U}|})\), which is hard to deal with . Moreover, for \(v_{i}^{e}^{U}\), when \(p_{}(_{i}|^{L},})\) is close to the one-hot categorical distribution, it would face the instability issue caused by \( y\) with \(y\) approaching zero. Therefore, following several EM-based optimization methods [18; 35; 31], we instead treat the fixed \(p_{}(^{U}|^{L},})\) as the target and minimize the reverse KL divergence \((p_{}(^{U}|^{L},}) \|q_{}(^{U}|}))\) to make \(q_{}(^{U}|})\) approximate the target. Assuming that the labels of different cells are independent, which is reasonable as the organization and function of each cell are not determined by other cells, we can derive the following loss function for unlabeled cells:

\[_{E}^{U}=-_{p_{}(^{U}|^{L}, })}[ q_{}(^{U}|}) ]=-_{v_{i}^{e}^{U}}_{p_{}( _{i}|^{L},})}[ q_{}(_{i}|}_{i})], \]

where \(p_{}(_{i}|^{L},})=p_{}(_{i}|})\) is the categorical distribution predicted by the cell-level GNN for \(v_{i}^{e}^{U}\) in the previous M-step. That is, the pseudo-labels predicted by the cell-level GNN are used for training the gene-level GNN. Additionally, the gene-level GNN can also be trained by the ground-truth labels of cells in \(^{L}\). Therefore, we have the loss function for labeled cells:

\[_{E}^{L}=-_{v_{i}^{e}^{L}} q_{}( _{i}|}_{i}), \]

where \(_{i}\) is the ground-truth label for cell \(v_{i}^{e}^{L}\). Combining Eqs. (6) and (7), the overall objective in the E-step for optimizing \(\) is to minimize \(_{E}=_{E}^{U}+_{E}^{L}\) with \(\) being a balancing hyper-parameter (we set \(=1\) in this work), which can be solved via stochastic gradient descent (SGD). To be more specific, in each update step of SGD, we sample a batch of labeled cells \(^{L}\) and a batch of unlabeled cells \(^{U}\) to perform gradient descent \(-_{}_{}}_{E}\), where \(}_{E}=-_{v_{i}^{e}^{U}} q_{ }(}_{i}|}_{i})-_{v_{i}^{e} ^{L}} q_{}(_{i}|}_{i})\) with \(}_{i}\) sampled by \(}_{i} p_{}(_{i}|})\) for unlabeled cells and \(_{}\) is the learning rate.

### M-step

In the M-step, the gene-level GNN (i.e., \(q_{}\)) is fixed and the cell-level GNN (i.e., \(p_{}\)) is updated to maximize \(_{}(,;^{L},})\). Also, before we introduce the optimization of \(p_{}\), we present the detailed workflow of our cell-level GNN.

**Structure of Cell-level GNN.** Existing cell-level GNNs predetermine the cell-cell graph based on \(}\). By contrast, we construct the cell-cell graph by performing \(k\)-nearest neighbors analysis on the cell representations \(\{_{i}\}_{i=1}^{N}\) generated by the gene-level GNN in each EM iteration, i.e., \(^{}=graph}(\{_{i}\}_{i=1}^{N})\), since \(\{_{i}\}_{i=1}^{N}\) should have better clustering characteristics than \(}\) for identifying cell types and its quality is enhanced during the optimization of \(q_{}\).

Given the gene expression data \(}_{i}\) and the cell representation \(_{i}\), the input feature for each cell \(v_{i}^{c}\) is defined as \(_{i}=(_{i},(} _{i}))^{d_{c}}\), where \(()\) denotes the concatenation operation for the input vectors. By stacking the input features of all the cells, we have the input feature matrix \(^{N d_{c}}\). And the structure of \(}()\) can be modeled as:

\[^{(l)}(^{})=^{-1 }^{},=(^{}_{N}),\] \[^{(l)}=_{}^{(l)}(^{ (l)}(^{})^{(l-1)}_{}^ {(l)}), \]

where \(_{N}\) is the all-ones vector of length \(N\), \(()\) returns a diagonal matrix with the input vector as the diagonal, \(_{}^{(l)}\) is a learnable weight matrix, \(_{}^{(l)}()\) is an MLP, \(^{(l)}\) is the output feature matrix of the \(l\)-th layer, and \(^{(0)}=\). As for \(_{i}^{}\) that is fed to an MLP classifier in Eq. (2), we find that setting \(_{i}^{}=(_{i}^{(0)},,_{i}^{(L)})\) with \(_{i}^{(l)}\) being the \(i\)-th row of \(^{(l)}\) practically achieves better classification performance for \(p_{}(_{i}|})\), which is similar to the jumping knowledge networks .

**Optimization of Cell-level GNN.** Maximizing \(_{}(,;^{L},})\) in the M-step is equivalent to minimizing the following loss function:

\[_{M} =-_{q_{}(^{U}|})}[  p_{}(^{L},^{U}|})]\] \[=-_{v_{i}^{c}^{U}}_{q_{ }(_{i}|})}[ p_{}(_{i }|})]-_{v_{i}^{c}^{L}} p _{}(_{i}|}). \]

Here, the first term is the loss for unlabeled cells, which trains \(p_{}\) using the pseudo-labels generated by the gene-level GNN in the previous E-step. The second term is the loss for labeled cells, which is the standard supervised learning with ground-truth labels. Minimizing Eq. (9) w.r.t. \(p_{}\) can also be solved via SGD. Concretely, in each update step of SGD, we sample \(}_{i} q_{}(_{i}|}_{i})\) for unlabeled cells and perform gradient descent \(-_{}_{}}_{M}\), in which \(}_{M}=-_{v_{i}^{c}^{U}} p_{ }(}_{i}|})-_{v_{i}^{c} ^{L}} p_{}(_{i}|})\) and \(_{}\) is the learning rate.

### Overall Optimization

To optimize the overall scBiGNN framework, we first pre-train the gene-level GNN with labeled cells to obtain the initial \(q_{}\). Then, the EM algorithm alternately optimizes \(p_{}\) (i.e., the cell-level GNN) and \(q_{}\) (i.e., the gene-level GNN) to reinforce each other. In the M-step, the cell representations generated by the gene-level GNN are used to construct the cell-cell graph, and the pseudo-labels predicted by \(q_{}\) together with the given cell labels are utilized to train the cell-level GNN. In the E-step, the pseudo-labels produced by \(p_{}\) and the ground-truth cell labels are used to train the gene-level GNN, in which the gene-gene interactions are adaptively learned. After several EM iterations, the performance of both cell- and gene-level GNNs is enhanced in cell type classification.

## 4 Experiments

### Benchmark Datasets

In this work, we conduct cell type classification experiments on five benchmark datasets to evaluate the performance of our scBiGNN, i.e., Zheng68K, Zhengsorted, BaronHuman, BaronMouse and AMB, which are widely used in many published papers for single-cell classification. These datasetscan be directly downloaded from Zenodo ([https://doi.org/10.5281/zenodo.3357167](https://doi.org/10.5281/zenodo.3357167)), and the detailed information is depicted in Table 1.

### Implementations

As for the gene-level GNN module, we use a one-layer model with four attention heads for the BaronMouse dataset, while a two-layer model with a single head in each layer is employed for the other datasets. Additionally, for Zheng68K and Zhengsorted datasets, the read-out function is set as the simple average pooling operation, while for the others we find that using learnable \(_{j}\) in Eq. (5) performs better. As for the cell-level GNN module, a three-layer model is employed for cell graph representation learning. The feature dimension of each layer's output is set as 32 for both GNN modules. All the MLPs used in scBiGNN have one hidden layer with 32 neurons and ReLU activation. The maximum number of EM iterations is set as 3, which is commonly enough for scBiGNN to converge, as shown in Figure 1.

### Comparison Methods

We compare with a variety of baseline models to demonstrate the superior performance of scBiGNN. Specifically, following the previous study , eight methods are considered: support vector machine (SVM), linear discriminant analysis (LDA), nearest mean classifier (NMC), random forest (RF), scID , CHETAH , SingleR  and ACTINN . In addition, three recent works that use gene-level GNNs are included: scGraph , sigGCN  and HNNVAT . We also compare with the cell-level GNN, termed GCN-C, in which the cell-cell graph is constructed by principal component analysis and \(k\)-nearest neighbors analysis on the raw gene expression data .

   Dataset & Description & Protocol & Species & \#Genes & \#Cells & \#Cell types \\  Zheng68K & PBMC & 10X Chromium & Homo sapiens & 20387 & 65943 & 11 \\ Zhengsorted & FACS-sorted PBMC & 10X Chromium & Homo sapiens & 21952 & 20000 & 10 \\ BaronHuman & Human pancreas & inDrop & Homo sapiens & 17499 & 8569 & 14 \\ BaronMouse & Mouse pancreas & inDrop & Mus musculus & 14861 & 1886 & 13 \\ AMB & Primary mouse visual cortex & SMART-Seq v4 & Mus musculus & 42625 & 12832 & 22 \\   

Table 1: Statistics of the five scRNA-seq benchmark datasets.

   Method & Zheng68K & Zhengsorted & BaronHuman & BaronMouse & AMB \\  SVM & 0.701 & 0.829 & 0.978 & 0.973 & 0.992 \\ LDA & 0.662 & 0.692 & 0.978 & 0.952 & 0.901 \\ NMC & 0.597 & 0.724 & 0.912 & 0.919 & 0.976 \\ RF & 0.674 & 0.796 & 0.962 & 0.968 & 0.985 \\ scID & 0.525 & 0.743 & 0.535 & 0.338 & 0.906 \\ CHETAH & 0.298 & 0.645 & 0.925 & 0.895 & 0.939 \\ SingleR & 0.673 & 0.718 & 0.968 & 0.908 & 0.962 \\ ACTINN & 0.724 & 0.825 & 0.977 & 0.978 & 0.992 \\ GCN-C & 0.710 & 0.831 & 0.978 & 0.977 & 0.992 \\ scGraph & 0.729 & 0.835 & 0.983 & 0.976 & 0.991 \\ sigGCN & 0.733 & 0.844 & 0.979 & 0.978 & 0.992 \\ HNNVAT & 0.734 & 0.846 & 0.982 & 0.979 & 0.992 \\ 
**scBiGNN** (\(q_{}\)) & 0.757 & 0.860 & 0.981 & 0.979 & 0.993 \\
**scBiGNN** (\(p_{}\)) & **0.760** & **0.867** & **0.983** & **0.983** & **0.994** \\   

Table 2: Classification accuracy of all the methods on the five datasets.

Figure 1: The convergence curves of \(q_{}\) and \(p_{}\) on the Zhengsorted dataset.

### Classification Results

We benchmark scBiGNN against all the baselines by performing five-fold cross validation on each dataset. The results of cell type classification accuracy are summarized in Table 2, in which the best performance in each column is highlighted in bold. As can be seen, among all the baselines, graph-based deep learning methods are the most accurate classifiers, verifying that the biological structural information is beneficial to identifying cell types from scRNA-seq data. Out of all the graph-based models, our scBiGNN consistently achieves the highest accuracy on all the benchmark datasets. Noticeably, it outperforms all the baselines with more than 2.5% accuracy improvement on the largest scRNA-seq dataset (i.e., Zheng68K). Moreover, from Table 2 and Figure 1 we can observe that \(p_{}\) always performs better than \(q_{}\). The reason is that \(p_{}\) explicitly integrates the bilevel graph representations of \(_{i}\) that summarizes gene-level structural information and \(^{c}\) that contains cell-level structural information. Overall, these results validate that the gene- and cell-level relationships are effectively mined by \(p_{}\) of scBiGNN to facilitate scRNA-seq classification.

### Network Analysis

**Gene-gene Graph.** Our scBiGNN adaptively learns gene-gene interactions by the self-attention function \(^{(l)}(^{})\) in Eq. (4). We integrate all the attention matrices into a gene-gene interaction matrix \(}\) by taking the average across all the corresponding elements in all attention matrices for every gene-gene pair, where each element \(_{ij}\) measures the attention of gene \(v_{i}^{}\) paid to gene \(v_{j}^{}\). Then, we can obtain the importance score \(s_{j}\) for each gene \(v_{j}^{}\), which is defined as \(s_{j}=_{i}_{ij}\). We sort the importance scores and get five lists of top 50 important genes from five-fold cross validation. Figure 2(A) shows that most genes appear more than once in the five lists, indicating that scBiGNN is trustworthy and effective in learning consistent important genes across different training procedures.

**Cell-cell Graph.** The cell-cell graph in scBiGNN is constructed as the \(k\)-nearest neighbor (\(k\)NN) graph based on the cell representations learned from \(q_{}\). In Figure 2(B), we can observe that our model is insensitive to \(k\). A small value of \(k\) (e.g., \(k=5\)) is enough for \(p_{}\) to achieve good performance, verifying the high quality of cell representations generated by \(q_{}\) in finding close neighborhoods. In Figure 2(C), we measure the homophily ratio (i.e., the proportion of intra-class edges in all the edges) of cell-cell graphs constructed by scBiGNN and the raw data \(}\) respectively. As can be seen, cell-cell graphs built by scBiGNN link more cells of the same type, which is critical to learning separable cell representations across different classes to ease GNN-based node classification .

## 5 Conclusion

In this paper, we propose a novel bilevel graph representation learning framework termed scBiGNN to improve GNN-based cell type classification in scRNA-seq analysis. scBiGNN consists of two GNN modules, namely gene- and cell-level GNNs, to adaptively extract structural information at both biological levels. The EM algorithm is employed to optimize the two GNN modules by alternating between the E-step and M-step. In each step, the pseudo-labels predicted by one module are used to train the other, making them gradually reinforce each other. Experiments on five scRNA-seq datasets verify the effectiveness of scBiGNN compared to competing cell type classification methods.

Figure 2: Network analysis of gene-gene and cell-cell graphs. (A) Venn plot of five important gene lists extracted from gene-gene interactions with five-fold cross validation on the BaronMouse dataset. (B) Ablation study on \(k\) of cell-cell \(k\)NN graphs. (C) Homophily analysis on the cell-cell graphs.