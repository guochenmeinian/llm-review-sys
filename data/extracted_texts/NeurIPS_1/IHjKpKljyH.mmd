# Consistency Models for Scalable and Fast Simulation-Based Inference

Marvin Schmitt

University of Stuttgart

Germany

mail.marvinschmitt@gmail.com

&Valentin Pratz1

Heidelberg University & ELIZA

Germany

& Ullrich Kothe

Heidelberg University

Germany&Paul-Christian Burkner

TU Dortmund University

Germany&Stefan T. Radev

Rensselaer Polytechnic Institute

United States

Equal contribution

###### Abstract

Simulation-based inference (SBI) is constantly in search of more expressive and efficient algorithms to accurately infer the parameters of complex simulation models. In line with this goal, we present consistency models for posterior estimation (CMPE), a new conditional sampler for SBI that inherits the advantages of recent unconstrained architectures and overcomes their sampling inefficiency at inference time. CMPE essentially distills a continuous probability flow and enables rapid few-shot inference with an unconstrained architecture that can be flexibly tailored to the structure of the estimation problem. We provide hyperparameters and default architectures that support consistency training over a wide range of different dimensions, including low-dimensional ones which are important in SBI workflows but were previously difficult to tackle even with unconditional consistency models. Our empirical evaluation demonstrates that CMPE not only outperforms current state-of-the-art algorithms on hard low-dimensional benchmarks, but also achieves competitive performance with much faster sampling speed on two realistic estimation problems with high data and/or parameter dimensions.

## 1 Introduction

Simulation-based inference (SBI) comprises a family of computational methods for modeling the hidden properties of complex systems by means of _simulation_. In recent years, deep learning - and generative models in particular - have proven indispensable for scaling up SBI to challenging inverse problems . Recently, multiple streams of neural SBI research have been capitalizing on the rapid progress in generative modeling of unstructured data by re-purposing existing generative architectures into general inverse problem solvers for applications in the sciences . In line with the above trend, we contribute a new contender to the growing suite of SBI methods for amortized Bayesian inference.

Within the class of deep generative models, score-based diffusion models  and flow matching algorithms  have recently attracted significant attention due to their sublime performance as realistic generators. Diffusion models and flow matching are strikingly flexible but require a relatively expensive multi-step sampling phase to denoise samples . To address this shortcoming, Song et al.  proposed _consistency models_ (CMs), which are trained to perform few-step generation by design and have since emerged as a standalone generative model family. Thus, CMs appear attractiveas a backbone architecture for SBI workflows due to their unique combination of unconstrained architectures and fast sampling speed - a conceptual advantage over the current state-of-the-art in neural SBI.

However, paradoxically, CMs may struggle with low-dimensional estimation problems, which are less important for image generation but crucial in typical SBI applications: These SBI applications are often characterized by relatively low-dimensional parameter spaces, even though the conditioning vectors (i.e., observables) might be high-dimensional [e.g., 3, 19, 20, 21]. Additionally, the quality of _conditional_ CMs as few-step Bayesian samplers has not yet been explored empirically (e.g., in terms of probabilistic calibration and precision), even though this is crucial for their application in science and engineering. Lastly, while CMs for image generation are trained on enormous amounts of data, training data are typically scarce in SBI applications. In our empirical evaluations, we demonstrate that CMs are competitive with state-of-the-art SBI algorithms in low-data regimes using our adjusted settings (see Appendix A for details).

In this paper, we propose conditional CMs for SBI along with generalizable hyperparameter settings that achieve competitive performance with much faster sampling speed on both challenging low-dimensional benchmarks and high-dimensional applied problems. Our main contributions are:

1. We adapt consistency models to simulation-based Bayesian inference and propose _consistency model posterior estimation_ (CMPE);
2. We showcase the fundamental advantages of consistency models for amortized simulation-based inference: expressive unconstrained architectures _and_ fast inference;
3. We demonstrate that CMPE outperforms NPE (normalizing flows) on three benchmark experiments (see Figure 1 above), is competitive with FMPE (flow matching) on Bayesian image denoising, and surpasses both NPE and FMPE on a challenging scientific simulator of tumor spheroid growth.

## 2 Preliminaries and related work

This section recaps simulation-based inference (SBI) via normalizing flows, flow matching, and score-based diffusion models. Readers familiar with these topics can safely fast-forward to Section 3.

Figure 1: **Experiments 1â€“3.** 1000 posterior draws for one unseen test instance per task, as well as sampling time in milliseconds. All amortized neural approximators were trained with a small budget of \(M=1024\) simulations. The bottom row shows the posterior predictive distribution in the kinematics task, and the pink cross-hair indicates the true end location \(\) of the robot arm. Across all benchmarks, CMPE (Ours) yields the best trade-off between fast sampling speed and high accuracy. **ACF:** affine coupling flow, **NSF:** neural spline flow, **FMPE:** flow matching posterior estimation, **CMPE:** consistency model posterior estimation (Ours), **K#** denotes \(K\) sampling steps during inference.

### Notation

In the following, the neural network training relies on a synthetic _training set_\(\{(^{(m)},^{(i)})\}_{m=1}^{M}\), which consists of parameter-data tuples. Each superscript \(m\) marks one training example, namely, a tuple of a latent parameter vector and an observable data set which was generated from the parameter vector. The total _simulation budget_ for training the neural networks follows as \(M\). We summarize the \(D\)-dimensional latent parameter vector of the simulator as \((_{1},,_{D})\). Further, each data set \(\{_{n}\}_{n=1}^{N}\) is a matrix whose rows consist of \(N\) vector-valued observations. Accordingly, one parameter vector \(^{(m)}\) yields one data set \(^{(m)}\) from the simulator (see Section 2.2). In Bayesian inference, we aim to infer the unknown parameters \(\) of the mechanistic simulator, which are not to be confused with the trainable neural network weights \(\).

### Simulation-based inference (SBI)

Computer simulations play a fundamental role across countless scientific disciplines, ranging from physics to biology, and from climate science to economics . Simulators \(g(,)\) generate observables \(\) as a function of unknown parameters \(\) and latent program states \(\):

\[=g(,)  p(),\  p( \,|\,) \]

The forward problem in Equation 1 is typically well-understood through scientific theories instantiated as generative models. The inverse problem, however, is much harder, and forms the crux of Bayesian inference: reduce uncertainty about the unknowns \(\) based on observables \(\) through the _posterior distribution_\(p(\,|\,) p()\,p( \,|\,)\). The key property of neural SBI and early approximate Bayesian computation [ABC; 22; 23] methods is that they approach the inverse problem by sampling (i.e., simulating) from the forward model. Thus, the forward model acts as an _implicit statistical model_ that enables proper Bayesian inference in the limit of infinite simulations. In contrast, likelihood-based methods (e.g., MCMC) depend on the explicit evaluation of the likelihood \(p(\,|\,)\).

We can classify neural SBI methods into _sequential_[e.g., 10; 25; 26; 27; 28; 29] and _amortized_[e.g., 5; 9; 10; 30; 31; 32; 33; 34; 35] methods. Sequential inference algorithms iteratively refine the prior \(p()\) to generate simulations in the vicinity of the target observation. Thus, they are _not amortized_, as each new observed data set requires a potentially costly re-training of the neural approximator tailored to the particular target observation. However, most sequential methods can be turned _amortized_ by training the neural approximator to generalize over the entire prior predictive space of the model. This allows us to query the approximator with any new data set during inference. In fact, amortization can be performed across any component of the model, including multiple data sets  and contextual factors, such as the number of observations in a data set , heterogeneous data sources , or even different probabilistic models [37; 38]. Our current work is situated in the amortized setting.

### Normalizing flows for neural posterior estimation

Neural posterior estimation (NPE) methods for SBI have traditionally relied on conditional discrete normalizing flows for learning a conditional neural density estimator \(p_{}(\,|\,)\) from simulated pairs \((,)\) of parameters and data [8; 39]. Normalizing flows can learn closed-form approximate densities via maximum likelihood training:

\[_{}=_{p(,)}[- p _{}(\,|\,)] \]

Two prominent normalizing flow architectures in SBI are _affine coupling flows_[ACF; 40] and _neural spline flows_[NSF; 41]. A major disadvantage of these architectures is their strict bijectivity requirement: it restricts the design space of the neural networks to invertible functions with cheap Jacobian calculations, hence the desire for more flexible architectures. In the following, we will briefly recap recent works that have pioneered the use of _multi-step, unconstrained_ architectures for SBI, inspired by the success of such models in a variety of generative tasks [13; 15].

### Flow matching for posterior estimation

Wildberger et al.  applied flow matching techniques [16; 17] in SBI, an approach abbreviated as flow matching posterior estimation (FMPE). FMPE is based on optimal transport [42; 43], where the mapping between base and target distribution is parameterized by a continuous process drivenby a vector field \(\) on the sample space for each time step \(t\). Here, the distribution at \(t=1\) could be a unit Gaussian \((,)\) and the distribution at \(t=0\) is the target posterior. The FMPE loss replaces the maximum likelihood term in the NPE objective (Eq. 2) with a conditional flow matching objective,

\[_{}=_{p(,)}[ _{0}^{1}\|u_{t}(_{t}\,|\,)-_ {}(_{t},t;)\|^{2}t], \]

where \(_{}\) denotes the conditional vector field parameterized by a unconstrained neural network with trainable parameters \(\), and \(u_{t}\) denotes a marginal vector field, which can be as simple as \(u_{t}(_{t}\,|\,)=_{1 }-\) for all \(t\). We obtain posterior draws \(_{0} p(\,|\,)\) by solving \(_{t}=-(_{t},t;)\) in reverse on \(t\), starting with noise samples \(_{1}(,)\). We can use any off-the-shelf ODE solver for transforming noise \(_{1}\) into a draw \(_{0}\) from the approximate posterior. In principle, the number of steps \(K\) in the ODE solver can be adjusted by setting the step size \(t=1/K\). Decreasing the number of steps increases the sampling speed, but FMPE is not designed to optimize few-step sampling performance. We confirm this in our experiments with a rectified flow that encourages straight-path solutions .

### Neural posterior score estimation

Another approach to simulation-based inference with unconstrained networks lies in neural posterior score estimation [NPSE; 10], which can either feature single-round inference (amortized) or multiple sequential inference rounds on a particular data set (non-amortized). This stream of research uses conditional score-based diffusion models [12; 44] to learn the posterior distribution. With a slightly different focus,  factorize the posterior distribution and learn scores of the diffused posterior for subsets (down to a single observation) of a larger data set. Subsequently, the information from the subsets is aggregated by combining the learned scores to approximate the posterior distribution of the entire data set. Crucially, both methods rely on the basic formulation of score-based diffusion models: They gradually diffuse the target distribution according to the following diffusion process,

\[_{t}=(_{t},t;\,) t+(t)_{t}, \]

with drift coefficient \(\), diffusion coefficient \(\), time \(t[0,T]\), and Brownian motion \(\{_{t}\}_{t[0,T]}\). At each time \(t\), the current (diffused) distribution of \(\) conditional on \(\) is denoted as \(p_{t}(\,|\,)\). Crucially, the distribution at \(t=0\) equals the target posterior distribution, \(p_{0}(\,|\,) p(\,|\, )\), and the distribution at \(t=T\) is set to be Gaussian noise (see below). Song et al.  prove that there exists an ordinary differential equation ("Probability Flow ODE") whose solution trajectories at time \(t\) are distributed according to \(p_{t}(\,|\,)\),

\[_{t}=(_{t},t;)-(t)^{2}\, p_{t}(_{t}\,|\, )t, \]

where \( p_{t}(_{t}\,|\,)\) is the score function of \(p_{t}(\,|\,)\). This differential equation is usually designed to yield a spherical Gaussian noise distribution \(p_{T}(\,|\,)(,T^{2} )\) after the diffusion process. Since we do not have access to the target posterior \(p(\,|\,)\), score-based diffusion models train a time-dependent score network \(s_{}(_{t},t,) p _{t}(_{t}\,|\,)\) via score matching and insert it into Eq. 5. Setting \((_{t},t;\,)=0\) and \(_{t}=\), the estimate of the Probability Flow ODE becomes \(_{t}=-ts_{}( _{t},,t)t\). Finally, we can generate a random draw from the noise distribution \(_{T}(,T^{2})\) and solve the Probability Flow ODE backwards for a trajectory \(\{_{t}\}_{t[T,0]}\). The end of the trajectory \(_{0}\) represents a draw from the approximate posterior \(p_{0}(_{0}\,|\,) p(\, |\,)\).

**Numerical stability.** The solver is usually stopped at a fixed small positive number \(t=\) to prevent numerical instabilities , so we use \(_{}\) to denote the draw from the approximate posterior. For simplicity, we will also refer to \(_{}\) as the _trajectory's origin_.

## 3 Consistency model posterior estimation

Diffusion models have one crucial drawback: At inference time, they require solving a differential equation for each posterior draw which slows down their sampling speed. This is particularly troublesome in SBI applications which may require thousands of samples for thousands of data sets [e.g., \(>\)1M data sets in 7]. Consistency models [CMs; 18] address this problem with a new generative model family that supports both single-step and multi-step sampling. In the following, we summarize key ideas of CMs [18; 46] with a focus on conditional sampling for the purpose of SBI.

### Conditional consistency models

The consistency function \(f:(_{t},t;\,)_{}\) maps points across the solution trajectory \(\{_{t}\}_{t[T,]}\) to the trajectory's origin \(_{}\) given a fixed conditioning variable (i.e., observation) \(\) and the probability flow ODE in Eq. 5. To achieve this with established score-based diffusion model architectures, we can use a unconstrained neural network \(F_{}(,t;\,)\) which is parameterized through skip connections of the form

\[f_{}(,t;\,)=c_{}(t)+c_{ }(t)F_{}(,t;\,), \]

where \(c_{}(t)\) and \(c_{}(t)\) are differentiable and fulfill the boundary conditions \(c_{}()=1\) and \(c_{}()=0\). Consistency models are originally motivated as a distillation technique for diffusion models. However, Song et al.  show that training consistency models in isolation is possible, and we base our method on their direct approach.

**Sampling.** Once the consistency model has been trained, generating draws from the approximate posterior is straightforward by drawing samples from the noise distribution, \(_{T}(,T^{2})\), which shall then be transformed into samples from the target distribution, like in a standard diffusion model. In contrast to diffusion models, however, we do not need to solve a sequence of differential equations for this transformation. Instead, we can use the learned consistency function \(f_{}\) to obtain the one-step target sample \(_{}=f_{}(_{T},T;\,)\). What is more, inference with consistency models is not actually limited to one-step sampling. In fact, multi-step generation is possible with an iterative sampling procedure, which we will describe in the following. For a sequence of time points \(=t_{1}<t_{2}<<t_{K}=T\) and initial noise \(_{K}(,T^{2})\), we calculate

\[_{k} f_{}(_{k+1},t_{k+1};\, )+^{2}-^{2}}_{k} \]

for \(k=K-1,K-2,,1\), where \(_{k}(,)\) and \(K-1\) is the number of sampling steps [18; 46]. The resulting sample is usually better than a one-step sample.

### Consistency models for simulation-based inference

Originally developed for image generation, consistency models can be applied to learn arbitrary distributions. The unconstrained architecture enables the integration of specialized architectures for both the data \(\) and the parameters \(\). Due to the low number of passes required for sampling (in contrast to flow matching and diffusion models), more complex networks can be used while maintaining low inference time. In theory, consistency models combine the best of both worlds: unconstrained networks for optimal adaptation to parameter structure and data modalities, while enabling fast inference speed with few network passes. Currently, this comes at the cost of explicit invertibility, which limits the computation of posterior densities. More precisely, single-step consistency models do not allow density evaluations at an arbitrary parameter value \(\) but only at a set \(S\) of approximate posterior draws \(\{_{}^{(1)},,_{}^{(S)}\}\). However, this is sufficient for important downstream tasks like marginal likelihood estimation, importance sampling, or self-consistency losses. In contrast, multi-step consistency sampling defines a Markov chain which cannot be evaluated without an additional density estimator (see Section 3.5 for details). In accordance with the taxonomy from Cranmer et al. , we call our method _consistency model posterior estimation_ (CMPE). While we focus on posterior estimation, using consistency models for likelihood emulation is a natural extension of our work.

As a consequence of its fundamentally different training objective, CMPE is not just a faster version of FMPE. Instead, it shows qualitative differences to FMPE beyond a much faster sampling speed. In **Experiment 4**, we show that CMPE is less dependent on the neural network architecture than FMPE, making it a promising alternative when the optimal architecture for a task is not known. Further, we consistently observe good performance in the low-data regime, rendering CMPE an attractive method when training data is scarce. In fact, data availability is a common limiting factor for complex simulators in engineering  and science [e.g., molecular dynamics; 48].

### Optimization objective

We formulate the consistency training objective for CMPE, which extends the unconditional training objective from Song and Dhariwal  with a conditioning variable \(\) to cater to the SBI setting,

\[_{}(,^{-})=(t_{ i})\,d((,t_{i+1};),(^{-},t_{i}; )) \]where \((t)\) is a weighting function, \(d(,)\) is a distance metric, \((,)\) is unit Gaussian noise, and the arguments for the distance metric amount to the outputs of the consistency function obtained at two neighboring time indices \(t_{i}\) and \(t_{i+1}\),

\[(,t_{i+1};)=f_{}( +t_{i+1},t_{i+1};\,),\,\,\,\, (^{-},t_{i};)=f_{^{-}}(+t_{i},t_{i};\,). \]

The teacher's neural network weights \(^{-}\) are a copy of the student's weights which are held constant during each step via a stopgrad operator, \(^{-}()\). We follow Song and Dhariwal  using \((t_{i})=1/(t_{i+1}-t_{i})\) and \(d(,)=-\|_{2}^{2}+ c^{2}}-c\) from the Pseudo-Huber metric family. Appendix A contains more details on discretization and noise schedules.

### Hyperparameter tuning

Consistency training introduces several additional hyperparameters, but there is limited theoretical guidance on how to select most of them. Within the scope of this paper, we rely on empirical search to identify the hyperparameters that are the most relevant for tuning efforts. As stated above, the maximum time \(T\) determines the standard deviation of the latent distribution. It should be larger than the magnitude of the target distribution . Values that are too low might lead to biased sampling, while values that are too high can make the training process more difficult. The minimum and maximum number of discretization steps \(s_{0},s_{1}\) during training also influence the final result in our experiments. For \(s_{0}\), a sufficiently low value (e.g. \(10\)) should be chosen. For \(s_{1}\), smaller values around \(50\) seem beneficial for stable training, especially when the number of epochs is low. When longer training is possible, increasing \(s_{1}\) might lead to improved accuracy. We summarize our recommended hyperparameter choices for SBI applications in Appendix C.

### Density estimation

Using the change-of-variable formula, we can express the posterior density of a single-step sample as

\[p_{}(\,|\,)=p_{T}( _{T}=f_{}^{-1}(_{},T;\,))\,|(_{T}}{ _{}})|, \]

where \(f_{}^{-1}(,T;)\) is the _implicit_ inverse of the consistency function and \(_{T}/_{}\) is the resulting Jacobian. While we cannot explicitly evaluate \(f_{}^{-1}(_{},T;\,)\), we can generate samples from \(_{T}(,T^{2})\) and use autodiff for evaluating the Jacobian because the consistency surrogate \(f_{}\) is differentiable. Thus, we cannot directly evaluate the posterior density at _arbitrary_\(\). Yet, evaluating the density at a set of \(S\) posterior draws \(\{_{}\}_{s=1}^{S}\) suffices for crucial downstream tasks, such as marginal likelihood estimation , neural importance sampling , or self-consistency losses .

For the purpose of _multi-step sampling_, the latent variables \(_{K},_{K-1},, _{1}\) form a Markov chain with a change-of-variables (CoV) given by Kothe :

\[p(_{1}\,|\,)=p(_{K})_{k= 2}^{K}_{k-1}\,|\,_{k},)}{p(_{k}\,|\,_{k-1},)}. \]

Due to Eq. 7, the backward conditionals are given by \(p(_{k-1}\,|\,_{k},)=(f_{}(_{k},t_{k};)\,,(t_{k}^{2}-^{2}) )\). Unfortunately, the forward conditionals \(p(_{k}\,|\,_{k-1},)\) are not available in closed form, but we could learn an amortized surrogate model \(q(_{k}\,|\,_{k-1})\) capable of single-shot density estimation based on a data set of execution paths \(\{_{1:K}\}\). This method would work well for relatively low-dimensional \(\), which are common throughout scientific applications, but may diminish the efficiency gains of CMPE in higher dimensions. We leave an extensive evaluation of different surrogate density models to future work.

### Choosing the number of sampling steps

The design of consistency models enables one-step sampling (see Eq. 7). In practice, however, using two steps significantly increases the quality in image generation tasks . We observe that few-step sampling with approximately \(K=5{-}15\) steps provides the best trade-off between sample quality and compute for SBI, particularly in low-dimensional problems. This is roughly comparable to the speed of neural one-step estimators like affine coupling flows or neural spline flows in **Experiments**

**1-3** (see Figure 1(a)). The number of steps can be chosen at inference time, so practitioners can easily adjust this for a given situation. We noticed that approaching the maximum trained number of discretization steps can lead to overconfident posterior distributions. See Section C.5 for an empirical evaluation of the relation between sampling quality and number of sampling steps.

## 4 Empirical evaluation

Our experiments cover three fundamental aspects of SBI. First, we perform an extensive evaluation on three low-dimensional experiments with bimodal posterior distributions from benchmarking suites for inverse problems [52; 53]. The simulation-based training phase is based on a fixed training set \(\{(^{(m)},_{*}^{(m)})\}_{m=1}^{M}\) of \(M\) tuples of data sets \(^{(m)}\) and corresponding ground-truth parameters \(_{*}^{(m)}\). Second, we focus on an image denoising example which serves as a sufficiently high-dimensional case study in the context of SBI [32; 54]. Third, we apply our CMPE method to a computationally challenging scientific model of tumor spheroid growth and showcase its superior performance for a complex simulator from cell biology . We implement all experiments using the BayesFlow Python library for amortized Bayesian workflows .

**Evaluation metrics.** We evaluate the experiments based on well-established metrics to gauge the accuracy and calibration of the results. All metrics are computed on a test set of \(J\) unseen instances \(\{(^{(j)},_{*}^{(j)})\}_{j=1}^{J}\). In the following, \(S\) denotes the number of (approximate) posterior samples that we draw for each instance \(J\). First, the easy-to-understand root mean squared error (RMSE) metric quantifies both the bias and variance of the approximate posterior samples across the test set as \(_{j=1}^{J}_{s=1}^{S}(_{s}^{(j)}-_{*}^{(j)})^{2}}\). Second, we estimate the squared maximum mean discrepancy [MMD; 56] between samples from the approximate vs. reference posterior, which is a kernel-based distance between distributions. Third, as a widely applied metric in SBI, the C2ST score uses an MLP classifier to distinguish samples from the approximate and reference posteriors. The resulting test accuracy of the classifier is the C2ST score, which ranges from 1 (samples are perfectly separable \(\) bad posterior) to 0.5 (samples are indistinguishable \(\) perfect posterior). Finally, we assess uncertainty calibration through simulation-based calibration (SBC; ): All uncertainty intervals \(U_{q}()\) of the true posterior \(p()\) are well calibrated for every quantile \(q(0,1)\),

\[q=[_{*} U_{q}( )]\,p(_{*})\,p(_ {*})_{*}, \]

where \([]\) is the indicator function. Discrepancies from Eq. 12 indicate deficient calibration of an approximate posterior. The expected calibration error (ECE) aggregates the median SBC error of central credible intervals of 20 linearly spaced quantiles \(q\), averaged across the test set.

**Contender methods.** We compare affine coupling flows [ACF; 40], neural spline flows [NSF; 41], flow matching posterior estimation [FMPE; 11], and consistency model posterior estimation (CMPE; ours). We use identical unconstrained neural networks for FMPE and CMPE to ensure comparability. Appendix C lists the hyperparameters of all models in the experiments.

### Experiment 1: Gaussian mixture model

We illustrate CMPE on a 2-dimensional Gaussian mixture model with two symmetrical components [9; 50]. The symmetrical components are equally weighted and have equal variance,

\[(,),\ \,(,)+\,(-,), \]

where \((,)\) is a Gaussian distribution with location \(\) and covariance matrix \(\). The resulting posterior distribution is bimodal and point-symmetric around the origin (see Figure 1 top-left). Each simulated data set \(\) consists of ten exchangeable observations \(\{_{1},,_{10}\}\), and we train all architectures on \(M=1024\) training simulations. We use a DeepSet  to learn 6 summary statistics for each data set jointly with the inference task.

**Results.** As displayed in Figure 1, both ACF and NSF fail to fit the bimodal posterior with separated modes. FMPE successfully forms disconnected posterior modes, but shows visible overconfidence through overly narrow posteriors. The visual sampling performance of CMPE is superior to all other methods. In this task, we observe that CMPE does not force us to choose between speed _or_ performance relative to the other approximators. Instead, CMPE can outperform all other methodssimultaneously with respect to both speed _and_ performance, as quantified by lower C2ST to the reference posterior across \(J=100\) test instances (see Figure 1(a)). If we tolerate slower sampling, CMPE achieves peak performance at \(K=10\) inference steps. Most notably, CMPE outperforms 1000-step FMPE by a large margin, even though the latter is approximately 75\(\) slower.

### Experiment 2: Two moons

This experiment studies the two moons benchmark [27; 49; 52; 59]. The model is characterized by a bimodal posterior with two separated crescent moons for the observed point \(_{}=(0,0)^{}\) which an approximator needs to recover. We repeat the experiment for different training budgets \(M\{512,1024,2048,4096,8192\}\) to probe each method under varying data availability. While \(M=512\) is a very small budget for the two moons benchmark, \(M=8192\) is considered sufficient.

**Results.** Trained on a simulation budget of \(M=1024\) examples, CMPE consistently explores both crescent moons and successfully captures the local patterns of the posterior (see Figure 1, middle row). Both the affine coupling flow and the neural spline flow fail to fully separate the modes. Most notably, if we aim to achieve fast sampling speed with FMPE by reducing the number of sampling steps during inference, FMPE shows visible overconfidence with 30 sampling steps and markedly deficient approximate posteriors with 10 sampling steps. In stark contrast, CMPE excels in the few-step regime. Figure 1(b) illustrates that all architectures benefit from a larger training budget. CMPE with 10 steps emerges as the superior architecture in the low- and medium-data regime up to \(M=4096\) training instances. FMPE with 1000 steps outperforms the other approximators by a small margin for the largest training budget of \(M=8192\) instances. Keep in mind, however, that 1000-step FMPE is approximately 30-70\(\) slower than ACF, NSF, and CMPE in this task.

### Experiment 3: Inverse kinematics

Proposed as a benchmark task for inverse problems by Kruse et al. , the inverse kinematics model aims to reconstruct the configuration \((_{1},_{2},_{3},_{4})= ^{4}\) of a multi-jointed 2D robot arm for a given end position \(^{2}\) (see Figure 1, bottom left). The input to the forward process \(g:\) are the initial height \(_{1}\) of the arm's base, as well as the angles \(_{2},_{3},_{4}\) at its three joints. The inverse problem aims to determine the posterior distribution \(p(\,|\,)\), which represents all arm configurations \(\) that end at the observed 2D position \(\) of the arm's end effector.

**Results.** On the challenging small training budget of \(M=1024\) training examples, CMPE with \(K=30\) sampling steps visually outperforms all other methods with respect to posterior predictive performance while maintaining fast inference (see Figure 1). On the C2ST metric, CMPE outperforms normalizing flows (ACF, NSF), but is in turn surpassed by 1000-step FMPE (see Figure 3(b)), which is however 85\(\) slower (FMPE: 6565ms, CMPE: 78ms). In contrast to the other empirical evaluations, the simulator in this benchmark lacks aleatoric uncertainty  and the plausible parameter values are tightly bounded through the geometry of the problem, which might explain this pattern.

Figure 2: C2ST score of 4000 approximate posterior draws vs. reference posterior (lower is better) for \(J=100\) unseen test examples. (a) CMPE (Ours) outperforms all other methods through both faster and more accurate inference on the GMM benchmark (mean\(\)SD). (b) CMPE (Ours) with 10 sampling steps shows superior performance up to a training budget of 4096 instances on the Two Moons benchmark (mean\(\)SE).

### Experiment 4: Bayesian denoising

This experiment demonstrates the feasibility of CMPE for a high-dimensional inverse problem, namely, Bayesian denoising on the Fashion MNIST data set . The unknown parameter \(^{784}\) is the flattened original image, and the observation \(^{784}\) is a blurred and flattened version of the crisp image from a simulated noisy camera . We compare CMPE and FMPE in a standard (60k training images) and a small data (2k training images) regime. As both methods allow for unconstrained architectures, we can assess the effect of the architectural choices on the results by evaluating both methods using a suboptimal naive architecture vs. the established U-Net .

**Neural architectures.** The naive architecture consists of a convolutional neural network [CNN; 63] to convert the observation into a vector of latent summary statistics. We concatenate input vector, summary statistics, and a time embedding and feed them into a multi-layer perceptron (MLP) with four hidden layers consisting of 2048 units each. This corresponds to a situation where the structure of the observation (i.e., image data) is known, but the structure of the parameters is unknown or does not inform a bespoke network architecture. In this example, however, we can leverage the prior knowledge that our parameters are images. Specifically, we can incorporate inductive biases into our network architecture by choosing a U-Net architecture which is optimized for image processing [i.e., an adapted version of 64]. Again, a CNN learns a summary vector of the noisy observation, which is then concatenated with a time embedding into the conditioning vector for the neural density estimator.

**Results.** We report the aggregated RMSE, MMD, and the time per sample for both methods and architectures (Table 1). FMPE is not able to generate good samples for the naive architecture, whereas CMPE produces acceptable samples even in this suboptimal setup. This reduced susceptibility to suboptimal architectures might become a valuable feature in high-dimensional problems where the structure of the parameters cannot be exploited. The U-Net architecture enables good sample quality for both methods, highlighting the advantages of unconstrained architectures. The MMD values align well with a visual assessment of the sample quality, therefore we deem it an informative metric to compare the results (see Section C.6 for visual inspection). The U-Net architecture paired with a large training set provides detailed and versatile samples of similar quality for CMPE and FMPE. CMPE enables \(50{-}1000\) faster inference than FMPE because it only requires two neural network passes for sampling, while achieving better (naive) or competitive (U-Net) quality.

### Experiment 5: Tumor spheroid growth

We conclude our empirical evaluation with a complex multi-scale model of 2D tumor spheroid growth . The model has 7 unknown parameters which govern single-cell behavior (agent-based) as well as the extracellular matrix description (PDE-based). Crucially, running simulations from this model on consumer-grade hardware is rather expensive (\( 1\) minute for a single simulation), so there is a desire for methods that can provide reasonable estimates for a limited offline training budget.

   &  & ^{-3}\)] \(\)} & Time \(\) \\  & 2 000 & 60 000 &  & 60 000 & \\    } & FMPE & 0.836 & 0.597 & \(171.53 1.61\) & \(95.26 1.21\) & 15.4ms \\  & CMPE (Ours) & **0.388** & **0.293** & \(102.09 3.24\) & \(\) & **0.3ms** \\    } & FMPE & **0.278** & **0.217** & \(\) & \(\) & 565.8ms \\  & CMPE (Ours) & 0.311 & 0.238 & \(18.49 0.12\) & \(16.08 0.05\) & **0.5ms** \\  

Table 1: **Experiment 4**: RMSE and MMD between the ground-truth image vs. 100 draws from the approximators trained on 2 000 and 60 000 training images, aggregated over 100 test images. For MMD, we draw one denoised sample per test image and calculate the MMD between the denoised samples and the original images. _Time_ per draw.

Figure 3: **Experiment 4**. CMPE denoising results on Fashion MNIST (U-Net backbone, \(K=2\) sampling steps, 60 000 training images). _First row:_ Original image (target parameters \(\)). _Second row:_ Blurred image (observations \(\)). _Third and fourth row:_ Means and standard deviations of the approximate posteriors. _Note:_ For standard deviations, darker regions indicate larger variability in the outputs. Adapted from . More in Appendix C.6.

Here, we compare the performance of the four contender methods in this paper on a fixed training set of \(M=19\,600\) simulations with \(J=400\) simulations as a test set to compute performance metrics. The contender methods are affine coupling flows (ACF), neural spline flows (NSF), and flow-matching posterior estimation (FMPE), as these are among the most popular choices for neural SBI to date. Our main contender is FMPE as a free-form architecture, and we use the exact same inference network for both FMPE (\(_{}\) - field network) and CM (\(f_{}\) - consistency network). Across all methods, we use a hybrid LSTM-Transformer architecture to transform high-dimensional summary statistics of variable length into fixed length embedding vectors \(h()\). Appendix C.7 provides more details on the neural network architectures and training hyperparameters.

**Results.** CMPE outperforms the alternative neural methods via better accuracy and calibration, as indexed by lower RMSE and ECE on 400 unseen test instances (see Table 2). The speed of the simpler ACF is unmatched by the other methods. In direct comparison to its unconstrained contender FMPE, CMPE simultaneously exhibits (i) a slightly higher accuracy; (ii) a drastically improved calibration; and (iii) much faster sampling (see Figure 6 in the Appendix). For this simulator, FMPE did not achieve satisfactory calibration performance with up to \(K=100\) inference steps, so Table 2 reports the best FMPE results for \(K=1000\) steps.

## 5 Discussion

We presented consistency model posterior estimation (CMPE), a novel approach to perform accurate simulation-based Bayesian inference on large-scale models while achieving fast inference speed. CMPE enhances the capabilities of state-of-the-art neural posterior estimation by combining few-step sampling with unconstrained neural architectures. To assess the effectiveness of CMPE, we applied it to a set of 3 low-dimensional benchmark tasks that allow intuitive visual inspection, as well as a high-dimensional Bayesian denoising experiment and a scientific tumor growth model. Across our experiments, CMPE emerges as a competitive method for simulation-based Bayesian inference, as evidenced by a holistic assessment of posterior accuracy, calibration, and inference speed.

**Limitations.** Our proposed CMPE method has two core limitations, which we specifically highlight again in this paragraph. First, consistency models do not directly yield tractable densities for arbitrary parameter values (see Section 3.5 for a remedy). Second, the relation between inference time (number of sampling steps \(K\)) and performance (posterior accuracy) is not monotonic, as elucidated in **Experiment 1**, (see Figure 1(a)). We provide more details and report the CMPE posterior quality (quantified by C2ST) as a function of the number of sampling steps for all three benchmark experiments in Section C.5. This phenomenon is not a _drawback_ per se, but rather a counter-intuitive attribute of consistency models. In practice, it can easily be addressed by performing a brief sweep over the number of sampling steps (\(K=1,,K_{}\)) during inference time and choosing an optimal \(K\) based on user-defined metrics (e.g., calibration or MMD on a validation set). Given the very fast inference speed of CMPE and the observed U-shaped relation between sampling steps and performance, this approach does not generally come with a relevant computational burden.

**Perspectives.** Future work might aim to further reduce the number of sampling steps towards one-step inference in SBI tasks to achieve even faster sampling speed in time-sensitive applications. This might be achieved via extensive automated hyperparameter optimization, tailored training schemes, or consistency model inference in a compressed latent space. Furthermore, consistency trajectory models [CTMs; 66] are a closely related generative model family, whose conditional formulation might prove useful for SBI as well. Overall, our results demonstrate the potential of CMPE as a novel simulation-based inference tool. Its unique combination of highly expressive unconstrained neural networks with fast sampling speed renders CMPE a new contender for SBI workflows in science and engineering, where both time and performance are essential.

   Model & RMSE \(\) & Max ECE \(\) & Time \(\) \\  ACF & 0.589 & 0.021 & **1.07s** \\ NSF & 0.590 & 0.027 & 1.95s \\ FMPE 30\# & 0.582 & 0.222 & 17.13s \\ FMPE 1000\# & 0.583 & 0.057 & 500.90s \\ CMPE 28\# (Ours) & 0.616 & 0.064 & 2.16s \\ CMPE 30\# (Ours) & **0.577** & **0.018** & 18.33s \\   

Table 2: **Experiment 5.** RMSE, ECE, and sampling time are computed for 2000 posterior samples, aggregated over 400 unseen test instances. Max ECE denotes the worst-case marginal calibration error across all 7 model parameters.

#### Acknowledgments

We thank Lasse Elsemuller for insightful feedback and Maximilian Dax for support with the implementation of the gravitational wave experiment in the Dingo library. MS acknowledges funding from the Cyber Valley Research Fund (grant number: CyVy-RF-2021-16) and the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy EXC-2075 - 390740016 (the Stuttgart Cluster of Excellence SimTech). MS additionally thanks the Google Cloud Research Credits program and the European Laboratory for Learning and Intelligent Systems (ELLIS) PhD program for support. VP acknowledges funding by the German Academic Exchange Service (DAAD) and the German Federal Ministry of Education and Research (BMBF) through the funding programme "Konrad Zuse Schools of Excellence in Artificial Intelligence" (ELIZA) and support by the state of Baden-Wurttemberg through bwHPC. VP acknowledges support by the Bundesministerium fuer Wirtschaft und Klimaschutz (BMWK, German Federal Ministry for Economic Affairs and Climate Action) as part of the German government's 7th energy research program "Innovations for the energy transition" under the 03ETE039I HiBRAIN project (Holistic method of a combined data- and model-based Electrode design supported by artificial intelligence).

#### Code

The software code for the experiments is available in a public GitHub repository:

[https://github.com/bayesflow-org/consistency-model-posterior-estimation](https://github.com/bayesflow-org/consistency-model-posterior-estimation)

Additionally, we provide a backend-agnostic implementation of Consistency Model Posterior Estimation (CMPE) in the open source BayesFlow library for amortized Bayesian workflows . Users can choose between a PyTorch, TensorFlow, or JAX backend. BayesFlow is available as open-source software on GitHub ([https://github.com/bayesflow-org/bayesflow/](https://github.com/bayesflow-org/bayesflow/)) and PyPI.