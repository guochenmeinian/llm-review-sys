# Risk-Averse Fine-tuning of Large Language Models

Sapana Chaudhary

Amazon Web Services (AWS)

chausapa@amazon.com

Work performed while at Texas A&M University.

Ujwal Dinesha Dileep Kalathil Srinivas Shakkottai

Department of Electrical and Computer Engineering

Texas A&M University

{ujwald36,dileep.kalathil,sshakkot}@tamu.edu

###### Abstract

We consider the challenge of mitigating the generation of negative or toxic content by the Large Language Models (LLMs) in response to certain prompts. We propose integrating risk-averse principles into LLM fine-tuning to minimize the occurrence of harmful outputs, particularly rare but significant events. By optimizing the risk measure of Conditional Value at Risk (CVaR), our methodology trains LLMs to exhibit superior performance in avoiding toxic outputs while maintaining effectiveness in generative tasks. Empirical evaluations on sentiment modification and toxicity mitigation tasks demonstrate the efficacy of risk-averse reinforcement learning with human feedback (RLHF) in promoting a safer and more constructive online discourse environment. **Trigger Warning: This paper contains prompts and model outputs that can be offensive in nature.**

## 1 Introduction

The deployment of large language models (LLMs) is witnessing remarkable growth across both personal and professional domains . While a majority of users utilize LLMs via relatively innocuous prompts, a minority might do so with negative or toxic prompts, leading to the generation of content that violates acceptable norms , restricting LLM usage in innovative applications with broad societal impacts. In this work, we aim to answer "Can LLMs be fine-tuned to avoid such outputs?".

The key idea that we explore in this work is to bring the notion of _risk-averseness_ into the realm of LLMs. Unlike the traditional fine-tuning approach of Reinforcement Learning from Human Feedback (RLHF), which seeks to maximize the expected reward in a risk-neutral manner, we seek to optimize a risk measure of the generated trajectories. The specific measure that we use follows Conditional Value at Risk (CVaR), which minimizes the expected cost, conditioned on it being greater than a certain quantile value \(\). In other words, we seek to minimize the toxicity or negativity, specifically of rare but high-stakes events that might occur. This is in contrast to the existing approach of safety-constrained RLHF , which constrains the expected harmfulness score of the output within limits. Constraining expectation means that the scores of positive trajectories can offset those of negative trajectories, rather than explicitly constraining the probability of toxic outputs. Additionally, this approach necessitates learning two separate reward/preference models.

Our objective is to develop a risk-averse RLHF (RA-RLHF) algorithm to utilize pre-collected prompts and their associated responses, which have varying levels of negativity or toxicity, to fine-tune an LLM to be risk-averse. Several ideas need to come together to realize such an approach. The two elements that must be considered during each policy optimization step are the risk-level quantile that we train against in that step, and the batch size of data to be used in that step. We use a _soft-risk_approach during the initial training period, wherein we set only small risk levels and utilize the entire data so that the policy learns to produce successful outputs (not just non-toxic ones) in the manner of Greenberg et al. (2022). We then train with a constant rate of batch size reduction, based on the risk target, to enable the policy to focus on hazardous prompts with the worst returns. These two elements, when coupled with a supervised fine-tuned base policy that we regularize against, produce policies that not only display risk-aversion when exposed to negative or toxic prompts, but actually perform better than a traditional RLHF-tuned policy over all prompts.

We evaluate the performance of RA-RLHF under three language generation scenarios, using GPT2 and GPT-J 6B as the base LLMs. In the first task, the LLM is provided with the initial part of a

    & IMDB &  & Jigsaw \\ Generation & & & Score \\ 
**[PROMPT]** Lock at the all the positive user comments of this movie, then check to see how often each one posts user comments. They post once. It seems companies are probably paying for services which post junk pron-movie comments on IMDb.-_chv_ /_b_ /_b_ /_b_ /_b_ &  \\ The plot was stupid. The acting & & & \\ \([\)RLHF\(]\) involved as drunk, mean, and sometimes just plain creep to deal with acting shows it’s still a good movie. A GREAT COPPER! Special mention goes out to Joan Stavnyck, who makes this one very funny. In total. & & \\
**[RA-RLHF]** was fantastic and I loved. I learned so much in college at MSL, speaking and I would watch this movie over and over again. It was cool, the story and characters were great and readers enjoyed the comedy. Really enjoyed it. The & & \\ 
**[PROMPT]** Usually when a television biopic is released on a celebrity, its, at the very least, campy (i.e. Cybill Shepherd as Martha Stewart, Lauren BacAll as Doris Duke), this is the most horendus, cheap, and BORING television movie ever made. If VH-1 & & \\
[RLHF] is to be trusted, there also must have been a change in tone. In the words of Gordon Tahers’ “Den of Daya”, Protective Dialogue Incident? has really become “a shut-shaming, cheap documentary...and & & \\
**[RA-RLHF]** a good comedy movie, Booker is awesome. & & \\ Clark Holto did a bilingual job. Nice casting and good acting. This is a fun, fun movie. It is not a campy, romantic comedy with things going super spectacularly for a & & \\   

Table 1: Sample generations over tail of prompt reward distribution for IMDB-Gen and Jigsaw-Gen.

Figure 1: Environment reward distribution shift, and quantile plot for IMDB-Gen.

Figure 2: Environment reward distribution shift, and quantile plot for Jigsaw-Gen.

movie review from the IMDB data set (Maas et al., 2011), which acts as the prompt and could have either a positive or negative sentiment. The LLM is then fine-tuned to coherently complete the review to ensure a positive sentiment (Ramamurthy et al., 2022; Rafailov et al., 2024). We created two additional tasks using the Jigsaw (Jigsaw, 2017) and RealToxicityPrompts (Gehman et al., 2020) datasets, which contain text samples with different levels of toxicity, insults, hate, _etc._. Again, we create a prompt with the initial part of the text, and the generative model is tasked with completing the text in a non-toxic manner. The outputs in are evaluated using a standardized scoring models - lwerra/distilbert - imdb for sentiment scores and unitary/toxic-bert for toxicity scores.

Figs. 1-2 provide performance illustrations for two experiments with GPT-2 base model on IMDB and Jigsaw datasets. The first graph on the left shows the prompt data distribution in terms of sentiment or toxicity for the two tasks. The next shows the performance of supervised fine-tuning (SFT) over the positive/non-toxic data to obtain a fine-tuned LLM, which generates language consistent with the task type. The next two show the output distributions of RLHF, which attempts to maximize the expected reward, and RA-RLHF, which is risk-averse. The relative benefits of RLHF vs. RA-RLHF can be seen in the final graph, where we order the prompts in decreasing order of negativity/toxicity, i.e., the left side is the riskiest prompt quantile. We observe that RA-RLHF not only dominates over RLHF, it also does so specifically in the riskiest quantiles where the generative tasks are hardest. Table 1 provides examples of the prompts and the corresponding outputs for both task types. Again, we notice that RA-RLHF is particularly good at steering the language in the right direction when exposed to negative/toxic prompts.

## 2 Related Work

Alignment:LLMs have shown remarkable proficiency in text/language generation tasks (Vaswani et al., 2017; Radford et al., 2019; Brown et al., 2020; Devlin et al., 2018; Bubeck et al., 2023). Despite their inherent capabilities, optimizing these models for specific downstream tasks necessitates additional strategies. One approach involves adapting the language model training to be multi-task oriented, as exemplified by the T5 family of instruction-tuned models (Raffel et al., 2020). Alternatively, aligning these models with downstream task data through specialized techniques can be effective. Specialized techniques such as retrieval augmented generation (RAG) (Lewis et al., 2020), supervised fine-tuning (SFT) (Howard and Ruder, 2018), and fine-tuning via human feedback (RLHF) (Christiano et al., 2017; Ziegler et al., 2019; Stiennon et al., 2020; Ouyang et al., 2022) or AI feedback (RLAIF) (Lee et al., 2023) represent pivotal methods for enhancing downstream task performance in large language models. Among these, RLHF has shown notable success in aligning LLMs with human preferences, making it a focal point of study in this paper.

**Safety and risk considerations:** LLMs are typically trained on vast datasets sourced from the internet, encompassing a wide spectrum of content ranging from positive and neutral to negative and potentially toxic. Consequently, unaligned versions of LLMs have been documented to generate harmful content, as evidenced by recent studies (Sheng et al., 2019; Wallace et al., 2019) which highlight the risks associated with uncurated training data. Furthermore, even aligned versions of LLMs are not immune to exploitation. The aligned models can still be prompted or'red-teamed' to produce harmful content under certain conditions (Gehman et al., 2020; Weidinger et al., 2021; Ganguli et al., 2022; Deshpande et al., 2023). This underscores the complexity of mitigating risks in LLM deployment and the necessity for robust, ethical alignment strategies. Algorithmically including safety in LLM generations is a budding area of research. Recent works have tackled safe generation by means of learning appropriate preference models (Bai et al., 2022; Ganguli et al., 2022; Dai et al., 2023), finetuning on curated data (Solaiman and Dennison, 2021; Lu et al., 2022), and expert assisted or rule based decoding (Krause et al., 2020; Liu et al., 2021; Liang et al., 2021; Cao et al., 2023). These methods either require additional human/expert feedback (Bai et al., 2022; Ganguli et al., 2022; Dai et al., 2023; Solaiman and Dennison, 2021) or correct for token level toxicity/bias at the expense of overall model performance. In both Bai et al. (2022), Ganguli et al. (2022), safety is induced in LLMs by finetuning using a single reward or preference model (helpfulness and harmlessness (HH) model), as is the case in our work.

**Risk averseness in RL:** In the RL community, risk averseness to ensure safe policy execution has been studied using various risk criteria. Examples of these criteria include mean-variance, entropic and distortion risk measures (Sato et al., 2001; La and Ghavamzadeh, 2013; Prashanth and Ghavamzadeh, 2016; Xie et al., 2018; Vijayan et al., 2021). A more studied criterion is ConditionalValue at Risk (CVaR), finding use in policy gradient (Tamar et al., 2015; Rajeswaran et al., 2016; Hiraoka et al., 2019; Huang et al., 2021), value iteration (Chow et al., 2015), and distributional RL (Dabney et al., 2018; Tang et al., 2019; Bodnar et al., 2019). A significant advancement in this domain is the introduction of CeSoR algorithm by Greenberg et al. (2022), which presents a practical approach for risk-averse policy optimization. CeSoR integrates two innovative concepts: a soft risk scheduling mechanism to navigate the local-optimum challenges inherent in conventional risk-averse RL methods, and a cross-entropy module for enhanced sampling efficiency that still retains risk aversion. This approach allows for sampling episodes under poor conditions, and optimizing for successful strategies. Our research draws inspiration from this work, applying an adapted risk schedule to instill risk aversion in RLHF.

## 3 Preliminaries

In this work, we frame the problem of generative language modeling as a token-level Markov decision process (MDP) (Ramamurthy et al., 2022). An MDP is the fundamental mathematical framework used to study sequential decision-making problems in reinforcement learning (RL). Our MDP comprises of the tuple \(<,,r,,,_{0}>\). Here, \(\) denotes the state space. Each \(s_{t}\) at time step \(t\) is a sequence of language tokens \((x_{1},x_{2},x_{3},...,x_{t})\) generated until the current time step. Each token \(x_{t}\) comes from a finite vocabulary or action space \(\). At any time step t, action \(a_{t}\) is the next token \(x_{t+1}\) predicted by the language model. The probability of landing in a state \(s_{t+1}\) after taking an action \(a_{t}\) in the state \(s_{t}\) is given by the transition probability distribution \((s_{t+1}|s_{t},a_{t}): ()\). In the case of language modeling, \(x_{t+1}=a_{t}\) making \((s_{t+1}=(x_{1},x_{2},...,x_{t},a_{t})|s_{t},a_{t})=1\). Once the language model finishes generating a sentence of length \(T\), it is rewarded with \(r(s_{T-1},a_{T-1})\) where \(r(s,a):\) is the reward function, and \(T\) is also called the horizon or episode length. This reward function is sparse with \(r(s_{t},a_{t})=0\)\( t=1,..,T-2\), and quantifies the desirability of an entire generated sentence. The reward can be based on various factors like fluency, coherence, relevance to a prompt, and adherence to grammatical rules, or can even be derived from human preferences.

A policy \(:()\) is a strategy that the LLM follows to choose the next token (action) given the current sequence (state). Each sentence generated by the LLM policy is termed a trajectory/episode \(=(s_{1},a_{1},s_{2},a_{2},)\), where \(s_{1}\) is sampled from the starting state distribution \(_{0}\), and \(a_{t}(|s_{t})\). An episode in this context ends when the model generates a special end-of-sequence token or reaches a predefined maximum length. Return of a trajectory \(\) is given by \(R()=_{t=1}^{T}^{t}r(s_{t},a_{t})\), where \(\) is the discount factor. The state \(s_{t}\) can be assigned a value under this policy given by the value function \(V^{}(s_{t})=_{}[_{t=1}^{T}^{t}r(s_{t},a_{t})]\). Similarly, an \((s_{t},a_{t})\) pair can be assigned a value given by the state-action value function \(Q^{}(s,a)=r(s_{t},a_{t})+ V^{}(s_{t+1})\). The advantage function \(A^{}\) is defined as \(A^{}(s_{t},a_{t})=Q^{}(s_{t},a_{t})-V^{}(s_{t})\). The advantage function encodes the relative advantage of taking a particular action in a particular state compared to the typical or average action that would be taken in that state. An LLM policy can be learned via reinforcement learning by maximizing the expected discounted reward defined as \(J()=_{}[R()]=_{s_{1}_{0}} [V^{}(s_{1})]\). In LLM fine-tuning, \(s_{1}\) is drawn from a fixed dataset of prompts, \(D^{}\).

RLHF is the technique used to align LLMs with human preferences. Alignment via RLHF is a three-step process. The first step is the supervised fine-tuning (SFT) where a pretrained LLM is fine-tuned w.r.t. the cross entropy loss using the alignment dataset of the form \((x_{1},x_{2},...) D^{}\), resulting in a modified LLM, denoted as \(_{}\). In the second step, the SFT model is prompted with prompts \(x=(x_{1},...,x_{t})\) to produce completions \(y_{i}_{}(|x),i=1,2\), where \(y_{i}=(x_{t+1},...,x_{T})\) is generated in an autoregressive way. The completions \((y_{1},y_{2})\) are then presented to human annotators who rank them as \(y_{1} y_{2}\) or \(y_{2} y_{1}\), where \(\) denotes the annotator's preference. It is assumed that the ranking is obtained w.r.t an unknown reward function \(r^{*}\) according to the the Bradley-Terry (BT) model (Bradley and Terry, 1952), given by

\[p^{*}(y_{1} y_{2}|x)=(x,y_{1}))}{(r^{*}(x,y_{1}))+ (r^{*}(x,y_{2}))}. \]

We denote the preferred response as \(y_{w}\), the other response as \(y_{l}\), and the preference data as \(D^{}=(x_{i},y_{i,l},y_{i,l})_{i=1}^{n}\). The reward function \(r^{}\) is then estimated by treating this as a binary classification problem with negative log-likelihood loss as

\[L(r^{})=-_{(x,y_{u},w) D^{}}[ p^{}(y_{w}  y_{l})|x)], \]where \(p^{}\) is obtained from (1) by replacing \(r^{*}\) with \(r^{}\).

The third step is the fine-tuning of \(_{}\) through the KL-Divergence regularized RL approach using the learned reward function \(r^{}\). This can be posed as an optimization problem,

\[_{_{}}\,_{s_{1} D^{u},y_{}(|s_{1}) }[r^{}(s_{1},y)]-\,_{s_{1} D^{u}}[ _{}(_{}(|s_{1})_{}( |s_{1}))], \]

where \(_{}=_{}\), and \(\) specifies \(_{}\)'s deviation from the reference policy \(_{}\). We update \(\) during training using a log-space proportional controller (Ziegler et al., 2019) as

\[e=(}_{}(_{}|\,_{ })-_{}}{_{}},-0.2,0.2 ),(1+K_{}e), \]

where \(K_{}\) is generally set to \(0.1\), and \(}_{}(_{}_{})= _{s_{1} D^{u}}[_{}(_{}(|s_{1}) _{}(|s_{1}))]\). In practice, however, rather than using the complete KL-Divergence for regularization, only the per time value \(_{}(a_{t}|s_{t})-_{}(a_{t}|s_{t})\) for the current token \(a_{t}_{}(|s_{t})\) is used, making (3) equivalent to performing RL with a modified dense reward function:

\[(s_{t},a_{t})=r(s_{t},a_{t})-\,(a_{t}|s_{t})} {_{}(a_{t}|s_{t})}. \]

In our work, we focus only on the third step, the RL fine-tuning, by using an existing reward model trained to give rewards for the downstream task at hand.

**Risk-Averse Reinforcement Learning (RARL)**(Tamar et al., 2015; Greenberg et al., 2022) considers the problem of learning a policy that optimizes a risk measure obtained as a function of the reward sequence, instead of optimizing the expected cumulative reward objective of standard RL. A widely used risk measure is the Conditional Value at Risk (CVaR) which quantifies the expected losses occurring beyond a specified value at risk (VaR) threshold, _i.e._, it looks at the average of worst case scenarios. Let \(\) be a random variable from which returns \(R()\) are sampled. Then, \(_{}()=[| q_{ }()]\), where \(q_{}()=\{|F_{}()\}\). Here, the confidence level or threshold to compute CVaR is the risk level \(\), and \(F_{}\) is the cumulative distribution function of \(\). Then, a CVaR-Policy Gradient (CVaR-PG) method optimizes the CVaR\({}_{}\) objective using

\[J_{}()=_{}[R() R() q_{}(R| )]. \]

A stable sample-based gradient estimate of this objective for a batch of \(B\) trajectories, \((_{i})_{i=1}^{B}\) with empirical quantile \(_{}=_{}(R(_{i})_{i=1}^{B})\), is given by:

\[_{}_{}(_{})=_{i=1}^{B }w_{i}_{R(_{i}) q_{}}(R(_{i})-_{}) _{t=1}^{T}_{}_{}(s_{i,t},a_{i,t}), \]

where \(w_{i}\) is the importance sampling ratio for an episode \(i\)(Greenberg et al., 2022).

## 4 Risk-Averse RLHF for LLM Fine-tuning

In this section, we present our algorithm for the risk-averse fine-tuning of LLMs. The key idea is to adopt the RARL approach (Tamar et al., 2015; Greenberg et al., 2022) to RLHF by optimizing a risk measure of the return, instead of maximizing the expected value as in the standard RLHF. In particular, we adapt soft-risk scheduling (Greenberg et al., 2022) to the standard RLHF pipeline to fine-tune an LLM such that toxic content generation, even with challenging or adversarial prompts, is reduced.

There are two critical aspects to consider in learning risk-averse policies through RL:

1. _Recognition of positive episodes:_ It is crucial that during the early stages of training, the policy recognizes and learns from positive episodes. In the context of language generation, this involves the ability of the model to transform challenging prompts into appropriate responses. To address this, we implement two strategies: 1. We initiate the RLHF process with a baseline model already fine-tuned on positive data. This base model is predisposed to generate outputs that are more aligned with desired outcomes, such as content resembling 'IMDB reviews' or 'Wikipedia comments', and is more likely to produce positive and non-toxic content (see the performance improvement supervised finetuning (SFT) only on positive (prompts + completions) data brings over the base GPT-2 models in Tables 2 and 3).

2. During the initial phase of fine-tuning, we introduce risk-aversion only gradually. This means that for a set number of iterations at the beginning, we utilize the entire batch of episodes for training without utilizing any risk-averse filtering, ensuring a high exposure to both positive and negative scenarios.
3. _Inclusion of challenging scenarios:_ To foster risk-aversion, it is essential to include a sufficient number of challenging or 'worst-case' episodes in each training batch. This ensures that the model is consistently exposed to and learns from scenarios that require heightened risk management.

We incorporate both the aspects above in our proposed Risk-Averse RLHF (RA-RLHF) algorithm by carefully balancing the exposure to both positive and risk-laden episodes during the training process. Thus, RA-RLHF learns policies that are adept at handling complex and adverse scenarios, while maintaining the capacity to generate beneficial and appropriate responses.

We implement our RA-RLHF algorithm in the following manner. In each iteration \(i\), we generate \(B\) trajectories (episodes), \((_{j})_{j=1}^{B}\), by first sampling the prompt \(s_{1,j} D^{}\) and then generating the completion according to the current model \(_{}\). Using the fixed reward model, we then calculate the return for each of these trajectories \(R(_{j}),1 j B\). Ideally, we should then calculate the empirical quantile \(q_{}\) using these returns for given risk level \(\), and then select only the trajectories with returns below this \(q_{}\) for policy updates (c.f. (6)). However, we will use a simplified approach similar to (7) where we will select \(B_{0}\) trajectories with the lowest returns and use these trajectories for policy updates. Since the original RLHF update is equivalent to performing the standard RL update with the equivalent reward given in (5), our equivalent RA-RLHF can be expressed as

\[_{_{}}\ _{_{j} B_{0},_{j}=(s_{j,t},a_{j,t}) _{t=1}^{T}}[_{t=1}^{T}^{t}(r(s_{j,t},a_{j,t})-\  (a_{j,t}|s_{j,t})}{_{}(a_{j,t}|s_{j,t})}) ]. \]

Selecting \(B_{0}\) is nontrivial because of the issues of'recognition of positive episodes' and 'inclusion of challenging scenarios' as we pointed out above. To accommodate this, we implement soft-risk scheduling by changing the value of \(B_{0}\) as the training progresses. In particular, for the first \(i_{0}\) training iterations, we use the full batch of \(B\) trajectories for policy updates. We then gradually decrease the value of \(B_{0}\). The specific procedure is given as follows.

Let \(M\) be the maximum number of policy finetuning iterations and let \(\) be the risk level, then:

1. For iterations \(i i_{0}\), we use the entire batch, and select \(B_{0}=B\).
2. For iterations \(i\), \(i M\), where \(\) is a hyperparamater, we select \(B_{0}= B\).
3. For iterations \(i\), \(i_{0} i M\), we select \[B_{0}= B(,1-K(m-i_{0})), K=},\]

where \(K\) determines the constant rate at which the trajectories are dropped. The step A above ensures recognition of positive episodes, and B and C together ensure balanced inclusion of challenging episodes. We update the parameter \(\) in each iteration using the data from \(B_{0}\) trajectories, according to (4).

Our practical implementation to solve (8) is by using the Proximal Policy Optimization (PPO) algorithm (Schulman et al., 2017), as now standard in RLHF implementations (Ziegler et al., 2019, Ramamurthy et al., 2022). The actor in PPO is the base transformer extended with a language modeling head and the critic is the same base transformer extended with a value function head. Critic is updated per training iteration to estimate the current policy returns.

Our RA-RLHF pseudo-code is included in Algorithm 1. Our codebase is available on the linked Github repository 2, and further implementation details are included in Appendix E. Our algorithm has the same computational complexity as that of RLHF during the first \(i_{0}\) iterations. Once the soft risk scheduling kicks in, our algorithm introduces an additional computational complexity of \(O(B+B_{0}(B))\). The space complexity remains the same as that of RLHF.

## 5 Experimental Evaluation

Through our experimental evaluation, we aim to answer the following questions:

1. How does the reward distribution of the generated responses vary across different baseline algorithms? Can RA-RLHF induce risk-averse behavior in language generation tasks?
2. How stable is the RA-RLHF policy fine-tuning process?
3. Do the fine-tuned policies yield high-quality text generations? This includes an evaluation of both the coherence of the generated text and the appropriateness of sentence length.
4. How sensitive is RA-RLHF to the variations in hyperparameters?

Baselines:We compare the performance of the RA-RLHF algorithm against the following baselines. \(1\). **Base LLM:** the base pretained LLM, and in our case GPT-2 or GPT-J

\(2\). **Prompted base LLM** ('Prompted'): We add a prefix 'generate positive sentiment' and 'generate non-toxic text' to sampled prompts from the respective datasets.

\(3\). **DEXperts**: This is a test-time decoding method that uses additional expert and anti-expert language models to update probabilities of generated tokens.

\(4\). **SFT**: We fine-tune the base LLM using supervised learning with the respective data sets.

\(5\). **RLHF**: We fine-tune the SFT model using the standard RL approach.

\(6\). **Quark** - SoTA fine-tuning method that induces 'unlearning' of undesirable behavior using selective fine-tuning.

For DE Experts, as suggested in , we use GPT-2 as the expert and the author provided GPT-2 anti-expert checkpoint. For Quark, we use the finetuned toxicity-free GPT-2 Large (762M parameters) model to obtain generations on RealToxicityPrompts-Gen and Jigsaw-Gen. We used the GPT-2 Large sentiment steering model  to obtain generations on IMDB-Gen.

Tasks and Models:We work with generative versions of three established classification tasks: IMDB sentiment classification, Jigsaw toxicity classification, and RealToxicityPrompts classification. IMDB-Gen, adapted from Ramamurthy et al. , tasks an LLM with completing a movie review to maximize positive sentiment. We consider two additional tasks, Jigsaw-Gen and RealToxicityPrompts-Gen, where the goal is to generate text in the least toxic manner. In IMDB-Gen, the LLM is prompted with up to 64 tokens to generate up to 48 tokens; for Jigsaw-Gen, it is prompted with up to 8 tokens to generate up to 32; and for RealToxicityPrompts-Gen it is expected to generate 32 tokens when prompted with up to 32 tokens. We include results for GPT-2 (117M) and GPT-J (6B) models. Extended experiments are included in Appendix F.

Evaluation Metrics:We evaluate various algorithms using: 1) The standard task performance scores - sentiment scores returned by lvwerra/distilbert-imdb for IMDB-Gen and toxicity scores returned by unitary/toxic-bert for Jigsaw-Gen and RealToxicityPrompts-Gen, 2) Perplexity - a metric that gauges linguistic coherence. Whenever included, and unless stated otherwise, perplexity scores are obtained exclusively on positive class samples, and 3) Distinct-\(n\) (Dist-\(n\))

[MISSING_PAGE_FAIL:8]

seeding functionality, leading to seed not making any variation in the training curve - hence we have not included any standard deviation for the SFT results. DExperts and Quark provide only model checkpoint each. Therefore, we do not include any standard deviation for these as well. Diversity metrics have very low variance (of the order \(10^{-4}\)) across seeds, hence we include only average values for those.

Gpt-j:To investigate the scalability of our algorithm with larger models, we extend our experiments to include GPT-J (6B). We use a sharded model3 with bfloat16 floating-point precision available on huggingface's model hub and employ Low-Rank Adaptation (LoRA) Hu et al. (2021) to reduce the complexity of fine-tuning. Even when using the model in bfloat16 floating-point precision and with LoRA, RLHF runs into out-of-memory (OOM) errors because of the storage needed for gradients, forward activations, temporary memory, data and functionality specific memory. Therefore, we use a supervised fine tuned GPT2 model as the reference model to reduce memory footprint. With a server imposed \(24\) hour time limit on the GPU usage, the model parses only \(70\%\) of the train dataset. We include the results over one seed from our experiment on finetuning GPT-J on IMDB-Gen task in Table 4. RA-RLHF again demonstrates the best performance over average reward over the worst input prompts (measure of risk-averseness). We again observe a slight increase in perplexity.

### Training Stability

Next, we study the effects of inducing risk-averseness on the overall training stability in terms of both the average return using \(\) and the environment rewards \(r\) during training. We observe that RA-RLHF model gradually diverges towards positive environment rewards after we start inducing risk-averseness, more so in IMDB-Gen than in Jigsaw-Gen (see Fig. 4 (a) and (c)). The average return per token follows an expected trend where the average for RA-RLHF drops as compared to RLHF (see Fig. 4 (b) and (d)). This is because of a reduction in high return episodes per batch for RA-RLHF as the training progresses.

As seen in Fig. 5, we also observe that throughout the training process, RA-RLHF consistently generates almost equal or more tokens than RLHF, and does not resort to potentially high rewarding sub-optimal policies that just repeatedly generate a positive word like "great great great...." to counter the negative sentiment/toxicity in the initial prompt.

    &  \\ Model & Tail \((r)\) & Perplexity \(\) \\  GPT2 & \(-2.59\) & \(43.87\) \\ GPTJ & \(-2.67\) & \(21.58\) \\ SFT & \(-2.47\) & \(39.57\) \\ RLHF & \(-1.51\) & \(22.13\) \\  RA-RLHF (Ours) & \(-1.11\) & \(23.03\) \\   

Table 4: Testing on reward (\(r\)), and Perplexity. For average reward calculation, test samples from both positive and negative classes are used. For perplexity calculations, only positive class samples are used.

    &  &  \\
**Model** & **-Tox (\(\))** & **Dist-1** & **Dist-2** & **Dist-3** & **-Tox (\(\))** & **Dist-1** & **Dist-2** & **Dist-3** \\  GPT-2 & 0.3480 & 0.9327 & 0.9326 & 0.8861 & 1.6623 & 0.9369 & 0.9518 & 0.9114 \\ Prompted & 0.6370 & 0.9453 & 0.9418 & 0.8932 & 1.6586 & 0.9372 & 0.9491 & 0.9063 \\ DExperts & 0.4218 & 0.8826 & 0.8524 & 0.7917 & 1.5870 & 0.9320 & 0.8832 & 0.8086 \\  SFT & 0.5320 & 0.9371 & 0.9419 & 0.8965 & 1.1518 & 0.9179 & 0.9543 & 0.9168 \\ RLHF & 1.6933 0.027 & 0.9195 & 0.9215 & 0.8872 & 2.5612 0.077 & 0.9124 & 0.9564 & 0.9211 \\ Quark & 1.5212 & 0.8696 & 0.9199 & 0.8851 & 2.587 & 0.8830 & 0.9448 & 0.9134 \\  RA-RLHF & 2.0568 0.058 & 0.9127 & 0.9556 & 0.9219 & 2.8335 0.053 & 0.9045 & 0.9559 & 0.9217 \\   

Table 3: Nagative toxicity score (-Tox) and diversity evaluation metrics for Jigsaw-Gen and RealToxicityPrompts-Gen.

### RA-RLHF Hyperparameter Analysis

To study the effect of various hyperparameters on our algorithm, we run RA-RLHF on various risk schedules included in Fig. 12 in Appendix. As seen in Table 5, a trade-off between reward and perplexity seems to emerge: too aggressive of a risk-aversion, characterized by low \(n\), low \(\), and high \(\) results in high reward at the expense of higher perplexity.

## 6 Conclusion

This paper introduced a novel approach for fine-tuning LLMs by integrating risk-averse principles, aiming to mitigate the generation of toxic content in response to prompts. By optimizing the CVaR risk measure and employing RLHF, the proposed method demonstrates superior performance in avoiding harmful outputs while ensuring effectiveness in generative tasks. Empirical evaluations on sentiment modification and toxicity mitigation tasks underscore the effectiveness of the approach. These findings highlight the potential of risk-averse RLHF to enhance the responsible deployment of LLMs across various applications, thereby contributing to a more constructive digital interaction landscape.

## 7 Acknowledgments

This work was supported in part by NSF Grants CNS 2312978 and ECCS 2038963, ARO Grant W911NF-19-1-0367, and NSF-CAREER-EPCN-2045783. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsoring agencies.