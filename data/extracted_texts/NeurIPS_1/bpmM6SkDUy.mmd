# EDGI: Equivariant Diffusion for Planning with

Embodied Agents

Johann Brehmer

Qualcomm AI Research

jbrehmer@qti.qualcomm.com

Equal contribution, order determined through a game of table tennis

Joey Bose

Qualcomm AI Research

Work done during an internship at Qualcomm AI Research

Pim de Haan

Qualcomm AI Research

Equal contribution, order determined through a game of table tennis

Joelosm A1 Research

Equal contribution, order determined through a game of table tennis

Joelosm A1 Research

Equal contribution, order determined through a game of table tennis

Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc.

Joey Bose

Qualcomm AI Research

Work done during an internship at Qualcomm AI Research

Pim de Haan

Qualcomm AI Research

Equal contribution, order determined through a game of table tennis

Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc.

###### Abstract

Embodied agents operate in a structured world, often solving tasks with spatial, temporal, and permutation symmetries. Most algorithms for planning and model-based reinforcement learning (MBRL) do not take this rich geometric structure into account, leading to sample inefficiency and poor generalization. We introduce the Equivariant Diffuser for Generating Interactions (EDGI), an algorithm for MBRL and planning that is equivariant with respect to the product of the spatial symmetry group \((3)\), the discrete-time translation group \(\), and the object permutation group \(_{n}\). EDGI follows the Diffuser framework by Janner et al. (2022) in treating both learning a world model and planning in it as a conditional generative modeling problem, training a diffusion model on an offline trajectory dataset. We introduce a new \((3)_{n}\)-equivariant diffusion model that supports multiple representations. We integrate this model in a planning loop, where conditioning and classifier guidance let us softly break the symmetry for specific tasks as needed. On object manipulation and navigation tasks, EDGI is substantially more sample efficient and generalizes better across the symmetry group than non-equivariant models.

## 1 Introduction

Our world is awash with symmetries. The laws of physics are the same everywhere in space and time--they are symmetric under translations and rotations of spatial coordinates as well as under time shifts.4 In addition, whenever multiple identical or equivalent objects are labeled with numbers, the system is symmetric with respect to a permutation of the labels. Embodied agents are exposed to this structure, and many common robotic tasks exhibit spatial, temporal, or permutation symmetries. The gaits of a quadruped are independent of whether it is moving East or North, and a robotic gripper would interact with multiple identical objects independently of their labeling. However, most reinforcement learning (RL) and planning algorithms do not take this rich structure into account. While they have achieved remarkable success on well-defined problems after sufficient training, they are often sample-inefficient (Holland et al., 2018) and lack robustness to changes in the environment.

To improve the sample efficiency and robustness of RL algorithms, we believe it is paramountto develop them with an awareness of their symmetries. Such algorithms should satisfy two key desiderata. First, policy and world models should be equivariant with respect to the relevant symmetry group. Often, for embodied agents this will be a subgroup of the product group of the spatial symmetry group \((3)\), the group of discrete time shifts \(\), and one or multiple object permutation groups \(_{n}\). Second, it should be possible to softly break (parts of) the symmetry group to solve concrete tasks. For example, a robotic gripper might be tasked with moving an object to a specific point in space, which breaks the symmetry group \((3)\). First works on equivariant RL have demonstrated the potential benefits of this approach (van der Pol et al., 2020; Walters et al., 2020; Mondal et al., 2021; Muglich et al., 2022; Wang and Walters, 2022; Wang et al., 2022; Ceti et al., 2022; Rezaei-Shoshtari et al., 2022; Deac et al., 2023). However, these works generally only consider small finite symmetry groups such as \(C_{n}\) and do not usually allow for soft symmetry breaking at test time based on a specific task.

In this paper, we introduce the _Equivariant Diffuser for Generating Interactions_ (EDGI), an equivariant algorithm for model-based reinforcement learning and planning. EDGI consists of a base component that is equivariant with respect to the full product group \((3)_{n}\) and supports the multiple different representations of this group we expect to encounter in embodied environments. Moreover, EDGI allows for a flexible soft breaking of the symmetry at test time depending on the task.

Our work builds on the Diffuser method by Janner et al. (2022), who approach both the learning of a dynamics model and planning within it as a generative modeling problem. The key idea in Diffuser is to train a diffusion model on an offline dataset of state-action trajectories. To plan with this model, one samples from it conditionally on the current state, using classifier guidance to maximize the reward.

Our main contribution is a new diffusion model that is equivariant with respect to the product group \((3)_{n}\) of spatial, temporal, and permutation symmetries and supports data consisting of multiple representations. We introduce a new way of embedding multiple input representations into a single internal representation, as well as novel temporal, object, and permutation layers that act on the individual symmetries. When integrated into a planning algorithm, our approach allows for a soft breaking of the symmetry group through test-time task specifications both through conditioning and classifier guidance.

We demonstrate EDGI empirically in 3D navigation and robotic object manipulation environments. We find that EDGI greatly improves the performance in the low-data regime--matching the performance of the best non-equivariant baseline when using an order of magnitude less training data. In addition, EDGI is significantly more robust to symmetry transformations of the environment, generalizing well to unseen configurations.

## 2 Background

**Equivariant deep learning.** Equivariant networks directly encode the symmetries described by a group \(G\) in their architecture. For the purposes of this paper, we are interested in the symmetries of 3D space, which include translations and rotations and are described by the special Euclidean group \((3)\), discrete-time translations \(\), and object permutations, which are defined using the symmetric group of \(n\) elements \(_{n}\). We now recall the main definition of equivariance and invariance of functions.

**Definition 1**.: A function \(f:\) is called \(G\)-equivariant if \(g f(x)=f(g x)\) for all \(g G\) and \(x\). Here \(\) and \(\) are input and output spaces that carry a \(G\) action denoted by \(\). The function \(f\) is called \(G\)-invariant if the group action in \(\) is trivial, \(g y=y\).

We will focus on \(=^{n}\), \(=^{m}\), and linear group actions or representations, which are group

Figure 1: Schematic of EDGI in a navigation task, where the agent (red square) plans the next actions (red dots) to reach the goal (green star) without touching obstacles (grey circles). **Top**: planning as conditional sampling from a diffusion model. **Bottom**: effect of a group action. Equivariance requires the diagram to commute.

homomorphisms \(:G(^{k})\). Examples include rotation and permutation matrices. For a more comprehensive introduction to group and representation theory, we direct the interested reader to Appendix A or Esteves (2020) and Bronstein et al. (2021).

For generative modeling, we seek to model \(G\)-invariant densities. As proven in (Kohler et al., 2020; Bose and Kobyzev, 2021; Papamakarios et al., 2021), given a \(G\)-invariant prior density it is sufficient to construct a \(G\)-equivariant map to reach the desired \(G\)-invariant target density. In Sec. 3, we design \(G\)-equivariant diffusion architectures to model a distribution of trajectories that are known to be symmetric with respect to the product group \((3)_{n}\).

**Diffusion models**. Diffusion models (Sohl-Dickstein et al., 2015) are latent variable models that generate data by iteratively inverting a diffusion process. This diffusion process starts from a clean data sample \(x q(x_{0})\) and progressively injects noise for \(i[T]\) steps until the distribution is pure noise. The reverse, generative process takes a sample from a noise distribution and denoises it by progressively adding back structure, until we return to a sample that resembles being drawn from the empirical data distribution \(p(x)\).

In diffusion models, it is customary to choose a parameter-free diffusion process (e. g. Gaussian noise with fixed variance). Specifically, we may define \(q(x_{t}|x_{t-1})\) as the forward diffusion distribution modeled as a Gaussian centered around the sample at timestep \(x_{t-1}\): \(q(x_{t}|x_{t-1})=(x_{t};}x_{t-1},_{t}I)\), where \(_{t}\) is a known variance schedule. The reverse generative process is learnable and can be parametrized using another distribution \(p_{}(x_{t-1}|x_{t})=(x_{t-1};_{}(x_{t},t),_{t} ^{2}I)\), and the constraint that the terminal marginal at time \(T\) is a standard Gaussian--i.e. \(p(x_{T})=(0,I)\). The generative process can be learned by maximizing a variational lower bound on the marginal likelihood. In practice, instead of predicting the mean of the noisy data, it is convenient to predict the noise level \(_{t}\) directly (Ho et al., 2020). Furthermore, to perform low-temperature sampling in diffusion models it is possible to leverage a pre-trained classifier to guide the generation process (Dhariwal and Nichol, 2021). To do so we can modify the diffusion score by including the gradient of the log-likelihood of the classifier \(_{}(x_{t},t,y)=_{}(x_{t},t)-_{t} _{x_{t}} p(y|x_{t})\), where \(\) is the guidance weight and \(y\) is the label.

**Trajectory optimization with diffusion**. We are interested in modeling systems that are governed by discrete-time dynamics of a state \(s_{h+1}=f(s_{h},a_{h})\), given the state \(s_{h}\) and action \(a_{h}\) taken at timestep \(h\). The goal in trajectory optimization is then to find a sequence of actions \(_{0:H}^{*}\) that maximizes an objective (reward) \(\) which factorizes over per-timestep rewards \(r(s_{h},a_{h})\). Formally, this corresponds to the optimization problem \(_{0:H}^{*}=_{a_{0:H}}(s_{0},a_{0:H})=_{a_ {0:H}}_{h=0}^{H}r(s_{h},a_{h})\), where \(H\) is the planning horizon and \(=(s_{0},a_{0},,s_{H},a_{H})\) denotes the trajectory.

A practical method to solve this optimization problem is to unify the problem of learning a model of the state transition dynamics and the problem of planning with this model into a single generative modeling problem. Janner et al. (2022) propose to train a diffusion model on offline trajectory data consisting of state-action pairs, learning a density \(p_{}()\). Planning can then be phrased as a conditional sampling problem: finding the distribution \(_{}() p_{}()c()\) over trajectories \(\) where \(c()\) encodes constraints on the trajectories and specifies the task for instance as a reward function. Diffusion models allow conditioning in a way similar to inpainting in generative image modeling, and test-time reward maximization in analogy to classifier-based guidance.

## 3 Equivariant diffuser for generating interactions (EDGI)

We now describe our EDGI method. We begin by discussing the symmetry group \((3)_{n}\) and common representations in robotic problems. In Sec. 3.2 we introduce our key novelty, an \((3)_{n}\)-equivariant diffusion model for state-action trajectories \(\). We show how we can sample from this model invariantly and break the symmetry in Sec. 3.3. Finally, we discuss how a diffusion model trained on offline trajectory data can be used for planning in Sec. 3.4.

### Symmetry and representations

**Symmetry group**. We consider the symmetry group \((3)_{n}\), which is a product of three distinct groups: 1. the group of spatial translations and rotations \((3)\), 2. the discrete time translation symmetry \(\), and 3. the permutation group over \(n\) objects \(_{n}\). It is important to note, however, that this symmetry group may be partially broken in an environment. For instance, the direction of gravity usually breaks the spatial symmetry group \((3)\) to the smaller group \((2)\), and distinguishable objects in a scene may break permutation invariance. We follow the philosophy of modeling invariance with respect to the larger group and including any symmetry-breaking effects as inputs to the networks.

We require that spatial positions are always expressed relative to a reference point, for example, the robot base or center of mass. This guarantees equivariance with respect to spatial translations: to achieve \((3)\) equivariance, we only need to design an \((3)\)-equivariant architecture.

**Data representations**. We consider 3D environments that contain an embodied agent as well as \(n\) other objects. We parameterize their degrees of freedom with two \((3)\) representations, namely the scalar representation \(_{0}\) and the vector representation \(_{1}\). Other representations that may occur in a dataset can often be expressed in \(_{0}\) and \(_{1}\). For example, object poses may be expressed as 3D rotations between a global frame and an object frame. We can convert such rotations into two \(_{1}\) representations through the inclusion map \(:(3)^{2 3}\) that chooses the first two columns of the rotation matrix. Such an embedding avoids challenging manifold optimization over \((3)\), while we can still uncover the rotation matrix by orthonormalizing using a Gram-Schmidt process to recover the final column vector. Thus, any \((3)\) pose can be transformed to these two representations.

We assume that all trajectories transform under the regular representation of the time translation group \(\) (similar to how images transform under spatial translations). Under \(_{n}\), object properties permute, while robot properties or global properties of the state remain invariant. Each feature is thus either in the trivial or the standard representation of \(_{n}\).

Overall, we thus expect that data in environments experienced by our embodied agent to be categorized into four representations of the symmetry group \((3)_{n}\): scalar object properties, vector object properties, scalar robotic degrees of freedom (or other global properties of the system), and vector robotic degrees of freedom (again including other global properties of the system).

### Equivariant diffusion model

Our main contribution is a novel \((3)_{n}\)-equivariant diffusion model which leads to an _invariant_ distribution over trajectories. Specifically, given an invariant base density with respect to our

Figure 2: Architecture of our \((3)_{n}\)-equivariant denoising network. Input trajectories (top left), which consist of features in different representations of the symmetry group, are first transformed into a single internal representation (green block). The data are then processed with equivariant blocks (blue), which consist of convolutional layers along the time dimension, attention over objects, normalization layers, and geometric layers, which mix scalar and vector components of the internal representations. These blocks are combined into a U-net architecture. For simplicity, we leave out many details, including residual connections, downsampling, and upsampling layers; see Appendix B.

chosen symmetry group--an isotropic Gaussian satisfies this property for \((3)_{n}\)--and an denoising model \(f\) that is equivariant with respect to the same group, we arrive at a diffusion model that is \((3)_{n}\)-invariant (Kohler et al., 2020; Papamakarios et al., 2021). Under mild assumptions, such an equivariant map that pushes forward the base density always exists (Bose and Kobyzev, 2021).

We design a novel equivariant architecture for the denoising model \(f\). Implemented as a neural network, it maps noisy input trajectories \(\) and a diffusion time step \(i\) to an estimate \(\) of the noise vector that generated the input. Our architecture does this in three steps. First, the input trajectory consisting of various representations is transformed into an internal representation of the symmetry group. Second, in this representation the data are processed with an equivariant network. Finally, the outputs are transformed from the internal representation into the original data representations present in the trajectory. We illustrate the architecture of our EDGI model in Fig. 2.

**Step 1: Representation mixer**. The input noisy trajectory consists of features in different representations of the symmetry group (see above). While it is possible to mirror these input representations for the hidden states of the neural network, the design of equivariant architectures is substantially simplified if all inputs and outputs transform under a single representation. Hence, we decouple the data representation from the representation used internally for the computation--in a similar fashion to graph neural networks that decouple the data and computation graphs.

**Internal representation.** We define a single internal representation that for each trajectory time step \(t[H]\), each object \(o[n]\), each channel \(c[n_{c}]\) consists of one5\((3)\) scalar \(s_{toc}\) and one \((3)\) vector \(v_{toc}\). We write \(w_{toc}=(s_{toc},v_{toc})^{4}\). Under spatial rotations \(g(3)\), these features thus transform as the direct sum of the scalar and vector representations \(_{0}_{1}\):

\[w_{toc}=s_{toc}\\ v_{toc} w^{}_{toc}=_{0}(g)s_{toc}\\ _{1}(g)v_{toc}\,. \]

These internal features transform in the regular representation under time shift and in the standard representation under permutations \(\) as \(w_{toc} w_{to^{}c}=_{o}_{o^{}o}\,w_{toc}\). There are thus no global (not object-specific) properties in our internal representations.

**Transforming input representations into internal representations**. The first layer in our network transforms the input \(\), which consists of features in different representations of \((3)_{n}\), into the internal representation. On the one hand, we pair up \((3)\) scalars and \((3)\) vectors into \(_{0}_{1}\) features. On the other hand, we distribute global features - those a priori unassigned to one of the \(n\) objects in the scene - over the \(n\) objects.

Concretely, for each object \(o[n]\), each trajectory step \(t[H]\), and each channel \(c=[n_{c}]\), we define the input in the internal representation as \(w_{toc}^{4}\) as follows:

\[w_{toc}=_{c^{}}^{1}_{occ^{}}s_{toc^{ }}\\ _{c^{}}^{2}_{occ^{}}v_{toc^{}}+ _{c^{}}^{3}_{occ^{}}s_{t c^{ }}\\ _{c^{}}^{4}_{occ^{}}v_{t c^{}}\,. \]

The matrices \(^{1,2,3,4}\) are learnable and of dimension \(n n_{c} n_{s}^{}\), \(n n_{c} n_{v}^{}\), \(n n_{c} n_{s}^{}\), or \(n n_{c} n_{v}^{}\), respectively. Here \(n_{s}^{}\) is the number of \((3)\) scalar quantities associated with each object in the trajectory, \(n_{v}^{}\) is the number of \((3)\) vector quantities associated with each object, \(n_{s}^{}\) is the number of scalar quantities associated with the robot or global properties of the system, and \(n_{v}^{}\) is the number of vectors of that nature. The number of input channels \(n_{c}\) is a hyperparameter. We initialize the matrices \(^{i}\) such that Eq. (2) corresponds to a concatenation of all object-specific and global features along the channel axis at the beginning of training.

**Step 2: \((3)_{n}\)-equivariant U-net**. We then process the data with a \((3)_{n}\)-equivariant denoising network. Its key components are three alternating types of layers. Each type acts on the representation dimension of one of the three symmetry groups while leaving the other two invariant--i. e. they do not mix internal representation types of the other two layers:

* _Temporal layers_: Time-translation-equivariant convolutions along the temporal direction (i. e. along trajectory steps), organized in a U-Net architecture.
* _Object layers_: Permutation-equivariant self-attention layers over the object dimension.

* _Geometric layers_: \((3)\)-equivariant interaction between the scalar and vector features.

In addition, we use residual connections, a new type of normalization layer that does not break equivariance, and context blocks that process conditioning information and embed it in the internal representation (see Appendix B for more details). These layers are combined into an equivariant block consisting of one instance of each layer, and the equivariant blocks are arranged in a U-net, as depicted in Fig. 2. Between the levels of the U-net, we downsample (upsample) along the trajectory time dimension by factors of two, increasing (decreasing) the number of channels correspondingly.

**Temporal layers**. Temporal layers consist of 1D convolutions along the trajectory time dimension. To preserve \((3)\) equivariance, these convolutions do not add any bias and there is no mixing of features associated with different objects nor the four geometric features of the internal \((3)\) representation.

**Object layers**. Object layers enable features associated with different objects to interact via an equivariant multi-head self-attention layer. Given inputs \(w_{loc}\), the object layer computes

\[_{toc}=_{c^{}}_{cc^{}}^{K}w_{toc }\,,_{toc}=_{c^{}}_{cc^{}}^{Q}w_{toc}\,, _{toc}=_{c^{}}_{cc^{}}^{V}w_{toc}\,,\] \[w_{toc}^{}_{c^{}}_{c^{ }}_{to}_{to^{}}}{} \,_{to^{}c}, \]

with learnable weight matrices \(^{K,V,Q}\) and \(d\) the dimensionality of the key vector. There is no mixing between features associated with different time steps, nor between the four geometric features of the internal \((3)\) representation. Object layers are \((3)\)-equivariant, as the attention weights compute invariant \((3)\) norms.

**Geometric layers**. Geometric layers enable mixing between the scalar and vector quantities that are combined in the internal representation, but do not mix between different objects or across the time dimension. We construct an expressive equivariant map between scalar and vector inputs and outputs following Villar et al. (2021): We first separate the inputs into \((3)\) scalar and vector components, \(w_{toc}=(s_{toc},v_{toc})^{T}\). We then construct a complete set of \((3)\) invariants by combining the scalars and pairwise inner products between the vectors,

\[S_{to}=\{s_{toc}\}_{c}\{v_{toc} v_{toc^{}}\}_{c,c^{}}\,. \]

These are then used as inputs to two MLPs \(\) and \(\), and finally we get output scalars and vectors, \(w_{toc}^{}=((S_{to})_{c},_{c^{}}(S_{to})_{cc^{}}v _{toc^{}})\).

Assuming full expressivity of the MLPs \(\) and \(\), this approach can approximate any equivariant map between \((3)\) scalars and vectors (Villar et al., 2021, Proposition 4). In this straightforward form, however, it can become prohibitively expensive, as the number of \((3)\) invariants \(S_{to}\) scales quadratically with the number of channels. In practice, we first linearly map the input vectors into a smaller number of vectors, apply this transformation, and increase the number of channels again with another linear map.

**Step 3: Representation unmixer**. The equivariant network outputs internal representations \(w_{toc}\) that are transformed back to data representations using linear maps, in analogy to Eq. (2). Global properties, e. g. robotic degrees of freedom, are aggregated from the object-specific internal representations by taking the elementwise mean across the objects. We find it beneficial to apply an additional geometric layer to these aggregated global features before separating them into the original representations.

**Training**. We train EDGI on offline trajectories without any reward information. We optimize for the simplified variational lower bound \(=_{,i,}[\|-f(+;i)\|^{2}]\)(Ho et al., 2020). where \(\) are training trajectories, \(i(0,T)\) is the diffusion time step, and \(\) is Gaussian noise with variance depending on a prescribed noise schedule.

### Invariant sampling and symmetry breaking

We now discuss sampling from EDGI (and, more generally, from equivariant diffusion models). While unconditional samples follow an invariant density, conditional samples may either be invariant or break the symmetry of the diffusion model.

**Invariant sampling**. It is well-known that unconditional sampling from an equivariant denoising model defines an invariant density (Kohler et al., 2020, Bose and Kobyzev, 2021, Papamakarios et al., 2021). We repeat this result without proof:

**Proposition 1**.: Consider a group \(G\) that acts on \(^{n}\) with representation \(\). Let \(\) be a \(G\)-invariant distribution over \(^{n}\) and \(_{t}:^{n}^{n}\) be a \(G\)-equivariant noise-conditional denoising network. Then the distribution defined by the denoising diffusion process of sampling from \(\) and iteratively applying \(_{t}\) is \(G\)-invariant.

We now extend this result to sampling with classifier-based guidance (Dhariwal and Nichol, 2021), a technique for low-temperature sampling based on a classifier \( p(y|x_{t})\) with class labels \(y\), or more generally any guide \(h(x)\). When the guide is \(G\)-invariant, guided sampling retains \(G\) invariance:

**Proposition 2**.: Consider a group \(G\) that acts on \(^{n}\) with representation \(\). Let \(\) be a \(G\)-invariant density over \(^{n}\) and \(_{t}:^{n}^{n}\) be a \(G\)-equivariant noise-conditional denoising network. Let the guide \(h:^{n}\) be a smooth \(G\)-invariant function. Further, assume \((g)\) is orthogonal \( g G\). Define the modified diffusion score \(_{}(x_{t},t,c)=_{}(x_{t},t)-_{ t}_{x_{t}}h(x_{t})\) for some guidance weight \(\). Then the distribution defined by the denoising diffusion process of sampling from \(\) and iteratively applying \(\) is \(G\)-invariant.

Proof.: The function \(h\) has a gradient \(_{x}h(x)\) that is \(G\)-equivariant (Papamakarios et al., 2021, Lemma 2). Thus,

\[_{}((g)(x_{t}),t) =_{}((g)(x_{t}),t)-_{t}_{x_{t }}h((g)(x_{t}))\] \[=(g)_{}(x_{t},t)-(g)_{t}_ {x_{t}}h(x_{t}))\] \[=(g)(_{}(x_{t},t)-_{t}_{x_{t }}h(x_{t}))\,.\]

The modified diffusion score \(\) is therefore \(G\)-equivariant. Applying Prop. 1, we find that classifier-based guidance samples from a \(G\)-invariant distribution. 

Proposition 2 applies to \(G=(3)_{n}\) as employed in EDGI, as each group within the product admits an orthogonal matrix representation. Both unconditional sampling from EDGI, as well as sampling guided by a \((3)_{n}\)-invariant classifier (or reward model), is thus \((3)_{n}\)-invariant.

**Symmetry breaking**. However, if the classifier \(h(x)\) or \( p(y|x)\) is _not_\(G\)-invariant, classifier-guided samples are in general also not \(G\)-invariant. For example, consider \(h(x)=-\|x-x_{0}\|^{2}\) for some \(x_{0}^{n}\), which will bias samples to \(x_{0}\). Similarly, conditional sampling on components of \(x\) (similar to inpainting) clearly leads to non-invariant samples. As we will argue below, these properties are essential for robotic planning.6

### Planning with equivariant diffusion

A diffusion model trained on offline trajectory data jointly learns a world model and a policy. Following Janner et al. (2022), we use it to solve planning problems by choosing a sequence of actions to maximize the expected task rewards.

To do this, we use three features of diffusion models. The first is the ability to sample from them by drawing noisy trajectory data from the base distribution and iteratively denoising them with the learned network yielding trajectories similar to those in the training set. For such sampled trajectories to be useful for planning, they need to begin in the current state of the environment. We achieve this by conditioning the sampling process such that the initial state of the generated trajectories matches the current state, in analogy to inpainting. Finally, we can guide this sampling procedure toward solving concrete tasks specified at test time using classifier-based guidance where a regression model is trained offline to map trajectories to task rewards.

**Task-specific symmetry breaking**. By construction, our equivariant diffusion model learns a \((3)_{n}\)-invariant density over trajectories. As shown in the previous section, both unconditional samples (and samples guided by an invariant classifier) reflect this symmetry property--it will be equally likely to sample a trajectory and its rotated or permuted counterpart. However, concrete tasks will often break this invariance, for instance by requiring that a robot or object is brought into a particular location or specifying an ordering over objects to be manipulated in a scene.

As discussed in the previous section, our diffusion-based approach with classifier guidance allows us to elegantly break the symmetry at test time as required. Such a soft symmetry breaking both occurs through conditioning on the current state, by conditioning on a goal state, and through a non-invariant reward model used for guidance during sampling.

## 4 Experiments

We demonstrate the effectiveness of incorporating symmetries as a powerful inductive bias in the Diffuser algorithm with experiments in two environments. The first environment is a 3D navigation task, in which an agent needs to navigate a number of obstacles to reach a goal state. Rewards are awarded based on the distance to the goal at each step, with penalties for collisions with obstacles. The position of the obstacles and the goal state are different in each episode and part of the observation. For simplicity, the actions directly control the acceleration of the agent; both the agent and the obstacles are spherical. Please see Fig. 1 for a schematic representation of this task and Appendix C for more details and the reward structure for this task.

In our remaining experiments, the agent controls a simulated Kuka robotic arm interacting with four blocks on a table. We use a benchmark environment introduced by Janner et al. (2022), which specifies three different tasks: an unconditional block stacking task, a conditional block stacking task where the stacking order is specified, and a rearrangement problem, in which the stacking order has to be changed in a particular way. For both environments, we train on offline trajectory datasets of roughly \(10^{5}\) (navigation) or \(10^{4}\) (manipulation) trajectories. We describe the setup in detail in Appendix D.

**Algorithms**. We train our EDGI on the offline dataset and use conditional sampling to plan the next actions. For the conditional and rearrangement tasks in the Kuka environment, we use classifier guidance following Janner et al. (2022).

As our main baseline, we compare our results to the (non-equivariant) Diffuser model (Janner et al., 2022). In addition to a straightforward model, we consider a version trained with \((3)\) data augmentation. We also compare two model-based RL baselines reported by Janner et al. (2022), BCQ (Fujimoto et al., 2019) and CQL (Kumar et al., 2020). To study the benefits of the symmetry groups in isolation, we construct two EDGI variations: one is equivariant with respect to \((3)\), but not \(_{n}\); while the other is equivariant with respect to \(_{n}\), but not \((3)\). Both are equivariant to temporal translations, just like EDGI and the baseline Diffuser.

**Task performance.** We report the results on both navigation and object tasks in Tab. 1. For each environment, we evaluate \(100\) episodes and report the average reward and standard error for each method. In both environments and across all tasks, EDGI performs as well as or better than the Diffuser baseline when using the full training set. In the navigation task, achieving a good performance for the baseline required substantially increasing the model's capacity compared to the hyperparameters used in Janner et al. (2022). On the Kuka environment, both diffusion-based methods clearly outperform the BCQ and CQL baselines.

**Sample efficiency**. We study EDGI's sample efficiency by training models on subsets of the training

    &  & (3)\) **generalization**} \\ Environment & BCQ & CQL & Diffuser & EDGI (ours) & Diffuser & EDGI (ours) \\  Navigation & – & – & \(_{ 3.9}\) & \(_{ 3.4}\) & \(5.6_{ 4.4}\) & \(_{ 3.5}\) \\  Unconditional & \(0.0\) & \(24.4\) & \(59.7_{ 2.6}\) & \(_{ 2.5}\) & \(38.7_{ 2.3}\) & \(_{ 2.7}\) \\ Conditional & \(0.0\) & \(0.0\) & \(46.0_{ 3.4}\) & \(_{ 3.6}\) & \(16.7_{ 2.0}\) & \(_{ 3.5}\) \\ Rearrangement & \(0.0\) & \(0.0\) & \(_{ 3.3}\) & \(_{ 3.9}\) & \(17.8_{ 2.3}\) & \(_{ 3.6}\) \\ Average & \(0.0\) & \(8.1\) & \(51.6_{ 1.8}\) & \(_{ 2.0}\) & \(24.4_{ 1.3}\) & \(_{ 1.9}\) \\   

Table 1: Performance on navigation tasks and block stacking problems with a Kuka robot. We report normalized cumulative rewards, showing the mean and standard errors over 100 episodes. Results consistent with the best results within the errors are bold. BCQ and CQL results are taken from Janner et al. (2022); for Diffuser, we show our reproduction using their codebase. **Left**: Models trained on the standard datasets. **Right**: \((3)\) generalization experiments, with training data restricted to specific spatial orientations such that the agent encounters previously unseen states at test time.

data. The results in Fig. 3 show that EDGI achieves just as strong rewards in the Kuka environment when training with only on \(10\%\) of the training data, and on the navigation task even when training on only \(0.01\%\) if the training data. The Diffuser baseline is much less sample-efficient. Training the Diffuser model with data augmentation partially closes the gap, but EDGI still maintains an edge. Our results provide evidence for the benefits of the inductive bias of equivariant models and matches similar observations in other works for using symmetries in an RL context (van der Pol et al., 2020; Walters et al., 2020; Mondal et al., 2021; Rezaei-Shoshtari et al., 2022; Deac et al., 2023).

**Effects of individual symmetries.** In the left panel of Fig. 3, we also show results for EDGI variations that are only equivariant with respect to \((3)\), but not \(_{n}\), or vice versa. Both partially equivariant methods perform better than the Diffuser baseline, but not as well as the EDGI model equivariant to the full product group \((3)_{n}\). This confirms that the more of the symmetry of a problem we take into account in designing an architecture, the bigger the benefits in sample efficiency can be.

**Group generalization.** Finally, we demonstrate that equivariance improves generalization across the \((3)\) symmetry group. On both environments, we train EDGI and Diffuser models on restricted offline datasets in which all trajectories are oriented in a particular way. In particular, in the navigation environment, we only use training data that navigates towards a goal location with \(x=0\). In the robotic manipulation tasks, we only use training trajectories where the red block is in a position with \(x=0\) at the beginning of the episode. We test all agents on the original environment, where they encounter goal positions and block configurations unseen during training. We show results for these experiments in Tab. 1. The original Diffuser performs substantially worse, showing its limited capabilities to generalize to the new setting. In contrast, the performance of EDGI is robust to this domain shift, confirming that equivariance helps in generalizing across the symmetry group.

## 5 Related work

**Diffusion-based planning**. The closest work to ours is the original Diffuser paper (Janner et al., 2022), which we used as a baseline. Concurrent to our work, Diffuser was extended by Ajay et al. (2022), who used a separate inverse dynamics model and classifier-free guidance. The key novelty of our work is that we make this approach aware of the symmetry structure of planning problems through a new \((3)_{n}\)-equivariant denoising network.

**Equivariant deep learning**. Baking in symmetries into deep learning architectures was first studied in the work of Cohen and Welling (2016) for geometric transformations, and the DeepSet architecture for permutations (Zaheer et al., 2017). Followup work to group convolutional networks focused on both spherical geometry (Cohen et al., 2018) and building kernels using irreducible group representations (Cohen and Welling, 2016; Weiler and Cesa, 2019; Cesa et al., 2021). For symmetries of the 3D space--i. e. subgroups of \((3)\)--a dominant paradigm is to use the message passing framework (Gilmer et al., 2017) along with geometric quantities like positions, velocities, and relative angles (Satorras et al., 2021; Schutt et al., 2021; Batatia et al., 2022).

**Equivariance in RL.** The role of symmetries has also been explored in reinforcement learning problems with a body of work focusing on symmetries of the joint state-action space of an MDP

Figure 3: Average reward as a function of training dataset size for EDGI and Diffuser. **Left**: navigation environment. **Right**: Kuka object manipulation, averaged over the three tasks.

[van der Pol et al., 2020, Walters et al., 2020, Mondal et al., 2021, Muglich et al., 2022, Wang and Walters, 2022, Wang et al., 2022, Cetin et al., 2022, Rezaei-Shoshtari et al., 2022]. More recently, model-based approaches--like our proposed EDGI--have also benefited from increased data efficiency through the use of symmetries of the environment [Deac et al., 2023]. Concurrently to this work, Brehmer et al.  also experiment with an equivariant Diffuser variation, but their denoising network is based on geometric algebra representations and a transformer architecture.

**Equivariant generative models**. Early efforts in learning invariant densities using generative models utilized the continuous normalizing flow (CNF) framework. A variety of works imbued symmetries by designing equivariant vector fields [Kohler et al., 2020, Rezende and Mohamed, 2015, Bose and Kobyzev, 2021]. As flow-based models enjoy exact density estimation, their application is a natural fit for applications in theoretical physics [Boyda et al., 2020, Kanwar et al., 2020] and modeling equivariant densities on manifolds [Katsman et al., 2021]. Other promising approaches to CNFs include equivariant score matching [De Bortoli et al., 2022] and diffusion models [Hoogeboom et al., 2022, Xu et al., 2022, Igashov et al., 2022]. Our proposed EDGI model extends the latter category to the product group \((3)_{n}\) and increases flexibility with respect to the data representations.

## 6 Discussion

Embodied agents often solve tasks that are structured through the spatial, temporal, or permutation symmetries of our 3D world. Taking this structure into account in the design of planning algorithms can improve sample efficiency and generalization--notorious weaknesses of RL algorithms.

We introduced EDGI, an equivariant planning algorithm that operates as conditional sampling in a generative model. The main innovation is a new diffusion model that is equivariant with respect to the symmetry group \((3)_{n}\) of spatial, temporal, and object permutation symmetries. Beyond this concrete architecture, our work presents a general blueprint for the construction of networks that are equivariant with respect to a product group and support multiple representations in the data. Integrating this equivariant diffusion model into a planning algorithm allows us to model an invariant base density, but still solve non-invariant tasks through task-specific soft symmetry breaking. We demonstrated the performance, sample efficiency, and robustness of EDGI on object manipulation and navigation tasks.

While our work shows encouraging results, training and planning are currently expensive. Progress on this issue can come both from more efficient layers in the architecture of the denoising model as well as from switching to recent continuous-time diffusion methods with accelerated sampling.

#### Acknowledgements

We would like to thank Gabriele Cesa, Daniel Dijkman, and Pietro Mazzaglia for helpful discussions.