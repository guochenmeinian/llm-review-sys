# Fully Unconstrained Online Learning

Ashok Cutkosky

Boston University

ashok@cutkosky.com

&Zakaria Mhammedi

Google Research

mhammedi@google.com

###### Abstract

We provide a technique for online convex optimization that obtains regret \(G\|w_{}\|\|G)}+\|w_{}\|^{2}+G^{2}\) on \(G\)-Lipschitz losses for any comparison point \(w_{}\) without knowing either \(G\) or \(\|w_{}\|\). Importantly, this matches the optimal bound \(G\|w_{}\|\) available _with_ such knowledge (up to logarithmic factors), unless either \(\|w_{}\|\) or \(G\) is so large that even \(G\|w_{}\|\) is roughly linear in \(T\). Thus, at a high level it matches the optimal bound in all cases in which one can achieve sublinear regret.

## 1 Unconstrained Online Learning

This paper provides new algorithms for _online learning_, which is a standard framework for the design and analysis of iterative first-order optimization algorithms used throughout machine learning. Specifically, we consider a variant of online learning often called "online convex optimization" . Formally, an online learning algorithm is designed to play a kind of "game" between the learning algorithm and the environment, which we can describe using the following protocol:

**Protocol 1.** Online Learning/Online Convex Optimization.

**Input:** Convex domain \(^{d}\), number of rounds \(T\).

For \(t=1,,T\):

1. Learner outputs \(w_{t}\).
2. Nature reveals loss vector \(g_{t}_{t}\) for some convex function \(_{t}:\) to the learner.
3. Learner suffers loss \((g_{t},w_{t})\).

The learner is evaluated with the _regret_\(_{t=1}^{T}(_{t}(w_{t})-_{t}(w_{}))\) against comparators \(w_{}\). By convexity, the regret is bounded by the _linearized_ regret \(_{t=1}^{T}(g_{t},w_{t}-w_{})\). Our goal is to ensure that for all \(w_{}\) simultaneously:

\[_{T}(w_{})_{t=1}^{T}(g_{t},w_{t}-w_{}) }{}}(\|w_{}\|^{T}g_{t}\|^{2}}). \]

Qualitatively, we consider a learner to be performing well if \(_{t=1}^{T}(_{t}(w_{t})-_{t}(w_{}))\) is very small, usually going to zero as \(T\). This indicates that the average loss of the learner is close to the average loss of any chosen comparison point \(w_{}\). This property is called "sublinear regret". The bound (1) is unimprovable in general , and clearly implies sublinear regret.

Algorithms that achieve low regret are used in a variety of machine learning applications. Perhaps the most famous such application is in the analysis of stochastic gradient descent, which achieves (1) for appropriately tuned learning rate . More generally, stochastic convex optimization can be reducedto online learning via the _online to batch conversion_. Roughly speaking, this result says that an online learning algorithm that guarantees low regret can be immediately converted into a stochastic convex optimization algorithm that converges at a rate of \([_{}(w_{})]}{T}\), where \(w_{}\) is the minimizer of the objective. Online learning can also be used to solve non-convex optimization problems  and can even be used to prove concentration inequalities . In all of these cases, achieving the bound (1) produces methods that are optimal for their respective tasks. Thus, it is desirable to be able to achieve (1) in as robust a manner as possible.

Our goal is to come as close as possible to achieving the bound (1) while requiring minimal prior user knowledge about the loss sequence \(g_{1},,g_{t}\) and \(w_{}\). In the past, several prior works have achieved the bound (1) when given prior knowledge of either \(\|w_{}\|\) or \(_{t}\|g_{t}\|\). However, such knowledge is frequently unavailable. Instead, many problems are "fully unconstrained" in the sense that we do not have any reasonable upper bounds on either \(\|w_{}\|\) or \(_{t}\|g_{t}\|\). In particular, when considering the application to stochastic convex optimization, the values for \(\|w_{}\|\) and \(_{t}\|g_{t}\|\) can be interpreted as knowledge of the correct learning rate for stochastic gradient descent . Thus, achieving the bound (1) with less prior knowledge roughly corresponds to building algorithms that are able to achieve optimal convergence guarantees without requiring manual hyperparameter tuning. For this reason, it is common to refer to such algorithms as "parameter-free". This paper focuses on this difficult but realistic setting.

Our new upper bound.Unfortunately, the bound (1) is actually unobtainable in general without prior knowledge of either the magnitude \(\|w_{}\|\) or the value of \(_{t}\|g_{t}\|\). Nevertheless, we will obtain a new compromise bound. For any user-specified \(>0\), our method will achieve:

\[_{t=1}^{T}(g_{t},w_{t}-w_{})(_{t[T]} \|g_{t}\|^{2}/+\|w_{}\|^{2}+\|w_{ }\|^{T}\|g_{t}\|^{2}}). \]

To dissect this compromise, let us consider the case \(\|g_{t}\|=G\) for all \(t\) and \(=1\). In this situation, our bound (2) is roughly \(G^{2}+\|w_{}\|^{2}+\|w_{}\|G\), while the "ideal" bound (1) is merely \(\|w_{}\|G\). However, for our bound to be significantly worse than (1), we must have either \(G\|w_{}\|\) or \(\|w_{}\| G\). In either case, we might expect that \(\|w_{}\|G\) is roughly \((T)\) (assuming that neither \(G\) nor \(\|w_{}\|\) is very small). So, intuitively the only cases in which our bound is worse than the ideal bound are those for which the ideal bound is already rather large--the problem is in some sense "too hard".

Comparison with previous boundsOur bound (2) is not the first attempted compromise in our fully unconstrained setting. Prior work  instead provides the bound:

\[_{t=1}^{T}(g_{t},w_{t}-w_{})( [T]}\|g_{t^{}}\| \|g_{t}\|+^{2}_{t[T]}\|g_{t}\|\|w_{ }\|^{3}+\|w_{}\|^{T}\|g_{t} "second-order" bound and is known to imply _constant_ regret in certain settings [1; 24] (so-called "fast rates").
4. Consider the case that both bounds are tuned with their respective "optimal" values for \(\). Our new bound would then reduce to \((\|w_{}\|^{T}\|g_{t}\|^{2}}+\|w_{}\|G)\), while the previous bound would instead become \((\|w_{}\|^{T}\|g_{t}\|^{2}}+\|w_{}\|G^ {2/3}(_{t=1}^{T}\|g_{t}\|)^{1/3})\). Thus, our new bound appears more desirable even with individually optimal tuning.
5. Our bound ensures that when \(w_{}=0\), the dependence on \(T\) is \(O(1)\). This has a number of useful consequences. For example, by running a separate instance of our algorithm for each dimension of a \(d\)-dimensional problem, we can achieve: Attempting this with the bound (3) would incur a more significant dependence on the dimension \(d\). More generally, this property means that our bound fits into the framework for "combining" regret bounds of .

## 2 Notation

Throughout this paper, we use \(\) to refer to a convex domain contained in \(^{d}\). Our results can in fact be extended to Banach spaces relatively easily using the reduction techniques of , but we focus on \(^{d}\) here to keep things more familiar. We use \(\|\|\) to indicate the Euclidean norm. Occasionally we also make use of other norms--these will always be indicated by some subscript (e.g. \(\|\|_{t}\)). We use \(_{ 0}\) to indicate the set of non-negative reals. For a convex function \(F\) over \(^{d}\), the Fenchel conjugate of \(F\) is \(F^{}()=_{x^{d}}(,x)-F(x)\). We occasionally make use of a "compressed sum" notation: \(g_{}_{t=a}^{b}g_{t}\). We use \(O\) to hide constant factors and \(\) to hide both constant and logarithmic factors. All proofs not present in the main paper may be found in the appendix.

We will refer to the values \(g_{t}\) provided to an online learning algorithm interchangeably as "gradients", "feedback" and "loss" values. We will refer to online learning algorithms occasionally as either "learners" or just "algorithms".

## 3 Overview of Approach

Our overall approach to achieve (2) is a sequence of reductions. As a first step, we observe that it suffices to achieve our goal in the special case \(=\). Specifically,  Theorems 2 and 3 reduce the general \(\) case to \(=\) case. We provide an explicit description of how to apply these reductions in Section C. So, we focus our analysis on the case \(=\). Next, we reduce the problem to a variant of the online learning protocol in which we also must contend with some potentially non-Lipschitz regularization function (Section 3.1). Finally, we show how to achieve low regret in this special regularized setting (Section 3.3).

### Hints and Regularization

Our bound is achieved via a reduction to a variant of Protocol 1 with two changes. First, the learner is provided with prior access to _magnitude hints_\(h_{t}\) that satisfy \(\|g_{t}\| h_{t}\). This notion of magnitude hints is also a key ingredient in the previous bound (3). Our second change is that the loss is not only the linear loss \((g_{t},w)\), but a _regularized_ non-linear loss \((g_{t},w)+a_{t}(w)\) for some fixed function \(:_{ 0}\) that we call a "regularizer". Formally, this protocol variant is specified in Protocol 2.

**Protocol 2**.: Regularized Online Learning with Magnitude Hints.

**Input:** Convex function \(:_{ 0}\).

For \(t=1,,T\):

1. Nature reveals magnitude hint \(h_{t} h_{t-1} 0\) to the learner.
2. Learner outputs \(w_{t}\).
3. Nature reveals loss \(_{t}\) with \(\|_{t}\| h_{t}\) and \(a_{t}[0,]\) to the learner.
4. Learner suffers loss \((_{t},w_{t})+a_{t}(w_{t})\).

The learner is evaluated with the _regularized regret_\(_{t=1}^{T}(_{t},w_{t}-w_{})+a_{t}(w_{t})-a_{t}(w_{ })\). The goal is to obtain:

\[_{t=1}^{T}(_{t},w_{t}-w_{})+a_{t}(w_{t})-a_{t}(w_{ })}{}}(\|w_{}\| ^{2}+_{t=1}^{T}\|_{t}\|^{2}+(w_{})+_{t=1}^{T}a_{t}^{2}}}). \]

In the special case that \((w)=0\) (i.e. the \(a_{t}\) are irrelevant, or all \(0\)), then various algorithms achieving the desired bound (4) are available in the literature . We provide in Algorithm 3 a new algorithm for this situation that achieves the optimal logarithmic factors--there is in fact a pareto-frontier of incomparable bounds that differ in the logarithmic factors.  provides the first algorithm to reach this frontier, while our method can achieve all points on the frontier1. We include this result because it is of some independent interest, but it not the major focus of our contributions. Any of the prior work in this area would roughly suffice for our broader purposes; the difference is only in the logarithmic terms.

Challenge of achieving (4).Achieving the bound (4) is challenging when \(\|w_{}\|\) is not known ahead of time. To see why, let us briefly consider two potential solutions.

The most immediate approach might be to reduce Protocol 2 to the case in which \(a_{t}=0\) for all \(t\) by replacing \(_{t}\) with \(_{t}+a_{t}(w_{t})\), and then possibly modifying the magnitude hint \(h_{t}\) in some way to now be a bound on \(|_{t}\|\). However, this approach is problematic because the expected bound would now depend on \(_{t=1}^{T}\|_{t}+a_{t}(w_{t})\|^{2}\) rather than \(_{t=1}^{T}\|_{t}\|^{2}\) and \(_{t=1}^{T}a_{t}^{2}\). This means that the naive regret bound would be very hard to interpret as \(w_{t}\) would appear on both the left and right hand sides of the inequality.

Another possibility is a follow-the-regularized leader/potential-based algorithm, making updates:

\[w_{t+1}=*{argmin}_{w}P_{t}(w)+_{i=1}^{t}( {g}_{i},w)+a_{i}(w), \]

for some sequence of "potential functions" \(P_{t}:\). In fact, this approach can be very effective; this is roughly the method employed by  for a similar problem. However, deriving the correct potential \(P_{t}\) and proving the desired regret bound can be very difficult, and could easily require separate analysis for each different possible \(\) function. For example, 's analysis specifically applies to \((w)=\|w\|^{2}\). There is other work on similar protocols using approximately this method, such as , that also requires particular analysis for each setting. Finally, even if the bound can be achieved in general using this scheme, solving the optimization problem (5) may incur some undesirable computational overhead, even for intuitively "simple" regularizers such as \((w)=\|w\|^{2}\). In fact, the method of  suffers from exactly this issue, which is why we provide an alternative approach in Section 3.3, for the special case of interest that \(=\).

Re-parametrizing to achieve (4).In order to achieve the bound (4) in the special case \(=\), we will employ a standard trick in convex optimization: re-parametrizing the objective as a convex constraint using the fact that the epigraph of a convex function is convex. Instead of having our learner output \(w_{t}\), we will output \((x_{t},y_{t})\), but subject to the constraint that \(y_{t}(x_{t})\). We provide details of this approach in Section 3.3.

With all of these technicalities introduced, we are ready to provide an outline of our method. The key idea is to show that for a very peculiar choice of coefficients \(a_{1},,a_{T}\) and some simple clipping of the gradients \(g_{t}\), we are able to achieve the following result.

**Theorem 1**.: _There exists an online learning algorithm that requires \(O(d)\) space and takes \(O(d)\) time per update, takes as input scalar values \(\), \(h_{1}\), and \(\) and ensures that for any sequence \(g_{1},g_{2},^{d}\), the outputs \(w_{1},w_{1},^{d}\) satisfy for all \(w_{}\) and \(T\):_

\[_{t=1}^{T} g_{t},w_{t}-w_{} O[ G+ ^{2}+}{}(e+})+\|w_{} \||^{2}(T)}{h_{1}}) }.\] \[.+\|w_{}\|G(e+\|^{ 2}(T)}{h_{1}})+\|w_{}\|^{2}(e+\|^{2}}{^{2}}(e+}))],\]

_where \(G=(h_{1},_{t[T]}\|g_{t}\|)\) and \(V=G^{2}+_{t=1}^{T}\|g_{t}\|^{2}\)._

Before proving this result, let us briefly unpack the algebra in the statement to see how it relates to our originally stated bound (2). Notice that if we drop all the logarithmic terms, the bound becomes:

\[_{t=1}^{T} g_{t},w_{t}-w_{}[ G+ ^{2}+}{}+\|w_{}\|^{T}\|g_{ t}\|^{2}+\|w_{}\|G+\|w_{}\|^{2}}]\]

Here if we should think of \(h_{1}\) and \(\) as conservative under-estimates of \(_{t}\|g_{t}\|\) and \(\|w_{}\|\). Notice that decreasing \(h_{1}\) and \(\) only increases the terms inside the logarithms, so that in some sense the algorithm is very robust to even extremely conservative under-estimation. When it holds that \(h_{1}_{t}\|g_{t}\|\) and \(\|w_{}\|\), then the above bound is exactly the previously stated equation (2).

### Proof Sketch of Theorem 1

Let us suppose for now that we have access to an algorithm that achieves the bound (4) under Protocol 2. Let us call it Reg. In this section, we will detail how to use Reg to achieve our desired goal (2) under Protocol 1 with \(=\): in this sketch, we treat all values as _scalars_, and never vectors. Recall that it suffices to consider \(=\) to achieve the result in general. Given an output \(w_{t}\) from Reg, we play \(w_{t}\) and observe the gradient \(g_{t}\). We will then produce a modified gradient \(_{t}\), a scalar \(a_{t}\), and a magnitude hint \(h_{t+1}\) to provide to Reg such that \(_{t}\) and \(a_{t}\) satisfy the constraints of Protocol 2. We will set \((w)=w^{2}\), and then by careful choice of \(_{t}\), \(a_{t}\), and \(h_{t+1}\), we will be able to establish Theorem 1.

There are two key steps in our reduction. The first step is now a standard trick originally used by  to reduce the original Protocol 1 to Protocol 2. The idea is as follows: let us set \(h_{t}=(h_{1},|g_{1}|,,|g_{t-1}|)\) for some given "initial value" \(h_{1} 0\). Notice that \(h_{t}\) may be computed before \(g_{t}\) is revealed and that the value \(G\) specified in the theorem satisfies \(G=h_{T+1}\). Then, upon receiving a gradient \(g_{t}\), we replace \(g_{t}\) with the "clipped" gradient \(_{t}=\{1}{|g_{t}|}\} g_{t}\). The clipped gradient \(_{t}\) satisfies \(|_{t}| h_{t}\) by definition. We then pass \(_{t}\) in place of \(g_{t}\) to an algorithm that interacts with Protocol 2. It is then relatively straightforward to see that for all \(w_{}\):

\[_{t=1}^{T}g_{t}(w_{t}-w_{}) _{t=1}^{T}_{t}(w_{t}-w_{})+_{t=1}^{T}| _{t}-g_{t}\|w_{}|+_{t=1}^{T}|_{t}-g_{t}||w_{t}|,\] \[_{t=1}^{T}_{t}(w_{t}-w_{})+h_{T+1}|w_{ }|+_{t=1}^{T}(h_{t+1}-h_{t})|w_{t}|.\]

At this point, prior work  observed that if we could constrain \(|w_{t}|\) to have some chosen maximum value \(D\), then the final summation above is at most \(h_{T+1}D\). By carefully choosing \(D\) in tandem with an algorithm that achieve (4) in the case \((w)=0\), one can achieve the previous "compromise" bound (3).

This is where our _second_ key step (which is our main technical innovation) comes in. Instead of explicitly enforcing \(|w_{t}| D\), we will apply a "soft constraint" by adding a regularizer. Surprisingly, we will add a very tiny amount of regularization and yet still achieve meaningful regret bounds.

[MISSING_PAGE_EMPTY:6]

to envision exactly what constraint is being enforced; notice that to make \(_{t=1}^{T}(h_{t+1}-h_{t})|w_{t}|=(G^{2}/)\) by applying some constraint \(|w_{t}| D\), we would need to set \(D=(G/)\). However, such an aggresive constraint would surely prevent us from achieving low regret for even relatively moderate \(\|w_{}\| G/\). So, our regularization seems to be doing something more subtle than simply applying a global constraint to the \(w_{t}\)'s. Indeed, notice that in the case \(|g_{t}| h_{1}\) for all \(t\), we actually have \(a_{t}=0\) and so no constraint effect at all is enforced!

The final step we need to check is bounding \(_{t=1}^{T}_{t}(w_{t}-w_{})+a_{t}(w_{t})-a_{t}(w_{ })\). To this end, we provide in Section 3.3 an algorithm that achieves the following bound, which is slightly weaker than (4):

\[_{t=1}^{T}_{t} (w_{t}-w_{})+a_{t}(w_{t})-a_{t}(w_{})\] \[ O[ h_{T}+|w_{}||^{2}(T)}{h_{1}})}+|w_{}|h_{T}( e+|^{2}(T)}{h_{1}}).\] \[.+^{2}+w_{}^{2}|^{2}^{2}(T)}{^{2}})}+\|w_{ }\|^{2}(e+|^{2}^{2}(T)}{ ^{2}})],\]

where \(S=^{2}+_{t=1}^{T}a_{t}\). This bound is weaker than (4) due to the presence of \(S\) rather than \(^{2}+_{t=1}^{T}a_{t}^{2}\). Nevertheless, by our bound on \(_{t=1}^{T}a_{t}\), we have:

\[S^{2}+^{2}(1+(G/h_{1}))\]

so that combining all of the above calculations we establish Theorem 1.

Thus, it remains to establish how we can achieve (4), or the slightly weaker (but sufficient) statement above. We accomplish this next in Section 3.3.

### Regularized Online Learning via Epigraph Constraints

Recall that our approach to obtaining (4) is to replace the regularization terms in the loss with constraints. Formally, consider the following protocol:

**Protocol 3.** Epigraph-based Regularized Online Learning for \(=\).

**Input:** Convex function \(:\).

For \(t=1,,T\):

1. Nature reveals magnitude hint \(h_{t} h_{t-1}\) to the learner.
2. Learner outputs \((x_{t},y_{t})\) with \(y_{t}(x_{t})\).
3. Nature reveals \(_{t}[-h_{t},h_{t}]\) and \(a_{t}[0,]\) to the learner.
4. Learner suffers loss \(_{t}x_{t}+a_{t}y_{t}\).

The learner is evaluated with the _linear regret_\(_{t=1}^{T}g_{t}(x_{t}-w_{})+a_{t}(y_{t}-(w_{}))\). The goal is to obtain:

\[_{t=1}^{T}_{t}(x_{t}-w_{})+a_{t}(y_{t}-(w_{} ))}{}(\|w_{}\|^{2}+_{t=1}^{T}_{t}^{2}}+(w_{})+ _{t=1}^{T}a_{t}^{2}}). \]

The key fact about this protocol is the observation that by setting \(w_{t}=x_{t}\), the bound (6) immediately implies (4). To see this, recall that \((w) 0\), \(a_{t} 0\), and \(y_{t}(x_{t})=(w_{t})\) so that:

\[_{t=1}^{T}((g_{t},w_{t}-w_{})+a_{t}(w_{t})-a_{t}(w_{ }))_{t=1}^{T}((g_{t},x_{t}-w_{})+a_{t}y_{t}-a_{t} (w_{})).\]

So, to achieve (4) under Protocol 2, it suffices to achieve the bound (6) under Protocol 3.

There is one tempting approach that _almost, but not quite_, achieves this goal. One could employ the "constraint-set reduction" developed in  that converts an algorithm that operates on the "unconstrained" domain \(^{d}\) to one respecting the constraint \(y(x)\). In particular, it is relatively straightforward to build an algorithm that achieves (6) without requiring \(y_{t}(x_{t})\). This unconstrained setting can be handled by the classic "coordinate-wise updates" trick in which we run two instances of an algorithm achieving (4) in the special case that \((x)=0\), one of which will output \(x_{t}\) and receive feedback \(g_{t}\), and the other will output \(y_{t}\) and receive feedback \(a_{t}\). Then, by the individual regret bounds on both coordinates, we would have:

\[_{t=1}^{T}(g_{t}x_{t}-w_{}+a_{t} y_{t}-(w_{})) =_{t=1}^{T}g_{t}x_{t}-w_{}+_{t=1}^{T}a _{t}y_{t}-(w_{}),\] \[(\|w_{}\|^{2}+_{t=1} _{t}^{2}}+(w_{})+_{T=1}^{T}a_{t}^{2}} ).\]

Then, one might hope that applying the constraint-set reduction of  would allow us to apply the constraint \(\) without damaging the regret bound. Unfortunately, this reduction will modify the feedback \(g_{t}\) and \(a_{t}\) in such a way that \(_{t=1}^{T}a_{t}^{2}\) could become much larger, which makes this approach untenable in general.

Fortunately, it turns out that our particular usage will enforce some favorable conditions on \(a_{t}\) that make the above strategy viable. Specifically, the choices of \(_{t}\), \(h_{t}\) and \(a_{t}\) described in Section 3.2 satisfy the condition that \(a_{t}=0\) unless \(\|_{t}\|=h_{t}\). By careful inspection of the constraint-set reduction, it is possible to show that the above strategy achieves a slightly weaker version of (6):

\[_{t=1}^{T}_{t}x_{t}-w_{}+a_{t}y_{t}- (w_{})(\|w_{}\|^{2}+ _{t=1}_{t}^{2}}+(w_{})+_{T=1}^{T}a _{t}}). \]

As detailed in Section 3.2, this weaker bound suffices for our eventual purposes. Nevertheless, for the reader interested in a fully general solution, in Appendix H, we provide a method for achieving (6) without restrictions. We do not employ it in our main development because it involves solving a convex subproblem at each iteration and so may be less efficient in some settings. This technique does however involve a small improvement to so-called "full-matrix" regret bounds , and so may be of some independent interest.

## 4 Lower Bounds

In this section, we show that the result of Theorem 1 is tight. In fact, we show a stronger result that generalizes our extra penalty term from \(G^{2}/+\|w_{}\|^{2}\) to \((\|w_{}\|)+^{}(G/)\) for any symmetric convex function \(\), where \(^{}(x)=_{x}xz-(z)\) is the Fenchel conjugate of \(\). That is, we provide a Pareto frontier of different lower bounds and Theorem 1 is but one point on this frontier. In Appendix A we extend our upper-bound results to match any desired point on this frontier (up to a logarithmic factor).. We also provide matching upper bounds (up to a logarithmic factor) in Theorem 16, as well as more simplified

**Theorem 2**.: _Suppose \(:\) is convex, symmetric, differentiable, non-negative, achieves its minimum at \((0)=0\), and \((x)\) is strictly increasing for non-negative \(x\). Further suppose that for any \(X,Y,Z>0\), there is some \(\) such that for all \(T\),_

\[(T)-1  X^{}(YT)\] \[X^{}(YTZ)  TZ\]

_where \(^{}(z)= zx-(x)\) is the Fenchel conjugate of \(\). Let \(h_{1}>0\), \(>0\) and \(>0\) be given._

_For any online learning algorithm \(\) interacting with Protocol 1 with \(=\), there is a \(T_{0}\) such that for any \(T T_{0}\), there is a sequence of gradients \(g_{1},,g_{T}\) and a \(w_{}\) such that the outputs \(w_{1},,w_{T}\) of \(\) satisfy:_

\[_{t=1}^{T}g_{t}w_{t}-w_{} G+^{}(G/)+(w_{})+|}{4 }|}{h_{1}})},\]_where \(G=(h_{1},g_{1},,g_{T})\). In particular, with \((x)=x^{1+q}\) for any \(q>0\), we can ensure:_

\[_{t=1}^{T}g_{t}(w_{t}-w_{})[ G+}{ ^{1/q}}+|w_{}|^{1+q}+G|w_{}||}{h_{1}})}].\]

The conditions on \(^{}\) in this bound are relatively mild. The first condition says that the gradient \(^{}\) should not grow exponentially fast. The second condition says that \(^{}\) should grow faster than some linear function. So, any polynomial of degree greater than 1 satisfies these conditions.

We note that this lower bound leaves something to be desired in terms of the quantification of the terms. Here, the value of \(G\) and \(\|w_{}\|\) depends on the algorithm \(\). This is a critical factor in the proof; roughly speaking, the proof operates by providing the algorithm with a constant gradient \(g_{t}=h_{1}\) at every round. Then, if the iterates \(w_{t}\) grow in some sense "quickly", we "punish" the algorithm with a very large negative gradient, which causes high regret if \(w_{}=0\). Alternatively, if the iterates do not grow quickly, then we show that the regret is large for some \(w_{} 1\). This approach is a common idiom for lower bounds in the fully unconstrained setting [18; 22].

However, a much better bound might be possible; ideally, it would hold that for _any_\(G\) and \(\|w_{}\|\) and algorithm \(\), we can find a sequence of gradients \(g_{t}\) that enforces our desired regret. Indeed, when either \(\|w_{}\|\) or \(_{t}\|g_{t}\|\) is provided to the algorithm, the lower bounds available do take this form [3; 5]. We leave as an open question whether it is possible to do so in our setting.

## 5 Discussion

We have provided a new online learning algorithm that achieves a near-optimal regret bound (2). Our algorithm is "fully unconstrained", or "fully parameter-free", in the sense that we achieve a near-optimal regret bound without requiring bounds on the gradients \(g_{t}\) or the comparison point \(w_{}\). Prior work in this setting [18; 21; 22; 23; 26] achieve bounds that are technically incomparable, but may be aesthetically less desirable, as detailed in the discussion following (3). Nevertheless, ideally we would have a unified algorithm framework capturing both our old and new bounds. It is an open question whether more careful choice of regularization in our approach could achieve this goal.

Our algorithm takes as input parameters \(\), \(h_{1}\) and \(\). All of these have a pleasingly small impact on the regret bound. \(\) and \(h_{1}\) can be interpreted as very rough estimates of \(\|w_{}\|\) and \(G\). As these quantities go to zero, the regret bound increases only logarithmically. Moreover, these estimates can be too high by a factor of \(\) while still maintaining \((\|w_{}\|G)\) regret. The quantity \(\) represents an estimate of \(G/\|w_{}\|\). As discussed in Section 1, this value does not appear in any term that has a \(T\)-dependence in the regret bound and so also has a very mild impact on the regret.

While our bound has several intuitively desirable characteristics, it is missing one important property: our bound suffers from an issue highlighted by  called the "range-ratio" problem. That is, the bound depends on the ratio \(G/h_{1}\), which could be very large if the losses are rescaled by some arbitrary large number without rescaling \(h_{1}\). This issue is at the heart of how we are able to sidestep the lower-bound of , which appears to apply to all algorithms that do not suffer from the range-ratio problem.

### Other forms of Unconstrained Online Learning

Our results focus on the case that we have no prior bounds on the value of \(\|w_{}\|\) or \(\|g_{t}\|\), and our bounds eventually depend on \(_{t}\|g_{t}\|\). One might worry that this is too conservative in some settings. For example, it might be that \(g_{t}\) is known to be a random variable with bounded mean \(\|[g_{t}]\| G\) and variance \((g_{t})^{2}\) for some known \(G\) and \(\). In this case, \(_{t}\|g_{t}\|\) might become large even though intuitively our regret should still depend only on \(G+\). This is the setting considered by several prior work on online learning with unconstrained domains [9; 17; 32]. Under various assumptions, these results all achieve an in-expectation regret bound of \([_{T}(w_{})](\|w_{}\|(G+ ))\).

In fact, our results come close to this ideal even without knowledge of \(G\). For example, [9; 17] study the case of sub-exponential \(g_{t}\) that satisfy \(_{|a| 1}[((g_{t}-[g_{t}],a))]( ^{2}^{2}/2)\) for all \(|| 1/b\) for some \(b>0\). In this case, for 1-dimensional \(g_{t}\), we have \([_{t}g_{t}^{2}](G^{2}+^{2})\), and so in expectation we achieve \((\|w_{}\|(G+)+G^{2}+^{2}+\|w_{}\|^{2})\) (the extension from 1-d to arbitrary dimensions can then be achieved via the black-box reduction of ). However, in the case that \(g_{t}\) has some heavy-tailed distribution such as studied by , it is less clear that our bounds achieve the desired result out-of-the box. Discovering how to achieve this is an interesting direction for future study.

### Parameter-free Algorithms and Stochastic Convex Optimization

As discussed in the introduction, a common motivation for the study of online learning is its immediate application to stochastic convex optimization through various online-to-batch conversions. The classic conversion of , as well as a few more recent results [33; 34; 35; 36] all show that if \(g_{t}\) is the output of a stochastic gradient oracle for a convex function \(F\), then for any \(w_{}\ F\):2

\[[F(^{T}w_{}}{T})-F(w_{}) ][_{T}(w_{})]}{T}\]

If \(\|g_{t}\| G\) with probability 1 (for an unknown \(G\)), our Theorem 1 immediately implies \([F(^{T}w_{}}{T})-F(w_{}) ](\|G}{}+/ +\|w_{}\|^{2}}{T})\). The first term is the optimal rate for stochastic convex optimization that can be achieved via SGD with learning rate \(=\|}{G}\) if \(G\) and \(\|w_{}\|\) are known ahead of time, and the second term is a lower-order "penalty" for not having up-front knowledge of these quantities.

Convergence results that match that of optimally tuned SGD are often called "parameter-free" (the parameter in question is the learning rate). As mentioned in the introduction, there has been a long line of works that attempt to achieve this goal by matching the regret bound (1), which can then be applied to the stochastic setting via an online-to-batch conversion. More recent work on parameter-free optimization has considered the stochastic case [37; 38], or deterministic case  directly without passing through a general regret bound. Many of these algorithms have shown significant empirical promise, even for non-convex deep learning tasks [38; 39; 40; 41; 42]. Almost all of these results require apriori knowledge of the value \(G\)3

To place our results in this context, let us focus on the case of a known \(G\) value. In this case,  show that by eschewing regret analysis and focusing specifically on the stochastic setting, it is possible to achieve a high-probability guarantee that improves upon the logarithmic factors achieved by our result, and so there seems to be something lost by focusing on regret bounds. However, in a surprising counterpoint,  shows that if one is interested in an _in-expectation_ result, then there is actually no way to improve upon the logarithmic factors achieved via online-to-batch conversion when applied to parameter-free regret bounds. Thus, our in-expectation stochastic convergence rate is optimal even up to logarithmic factors, while we also do not require prior knowledge of \(G\).

Finally, let us evaluate the optimality of our bound in the stochastic setting while accounting for the fact that our methods do not get to know either \(G\) or \(\|w_{}\|\). Here, we can again make use of the lower bounds developed by . Consider the class of stochastic convex optimization objectives with Lipschitz constant \(G\) between \(1\) and \(L\) and \(\|w_{}\|[1,R]\). The "price of adaptivity" as defined by  is the maximum over this class of the ratio between the convergence guarantee of an algorithm that does not know \(\|w_{}\|\) and \(G\) with respect to the minimax optimal convergence guarantee for an algorithm that does know these values (which is \(RG/\)). We achieve a price of adaptivity of \((1+(L,R)/)\). The best-known lower bound for this class is \((1+(L,R)/)\). Thus, there is a gap here--although we provide matching homer bounds for the _online_ setting, it is possible that in the _stochastic_ setting, one can improve our bounds. That said, the stochastic lower bound is derived for algorithms that are given the ranges \([1,L]\) and \([1,R]\). Our algorithm does not use this information and it is also plausible that without such knowledge the lower bound itself would improve.