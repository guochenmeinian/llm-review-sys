# HYDRA-FL: Hybrid Knowledge Distillation for

Robust and Accurate Federated Learning

Momin Ahmad Khan

University of Massachusetts, Amherst

makhan@umass.edu

&Yasra Chandio

University of Massachusetts, Amherst

ychandio@umass.edu

&Fatima Muhammad Anwar

University of Massachusetts, Amherst

fanwar@umass.edu

###### Abstract

Data heterogeneity among Federated Learning (FL) users poses a significant challenge, resulting in reduced global model performance. The community has designed various techniques to tackle this issue, among which Knowledge Distillation (KD)-based techniques are common. While these techniques effectively improve performance under high heterogeneity, they inadvertently cause higher accuracy degradation under model poisoning attacks (known as _attack amplification_). This paper presents a case study to reveal this critical vulnerability in KD-based FL systems. We show why KD causes this issue through empirical evidence and use it as motivation to design a hybrid distillation technique. We introduce a novel algorithm, _Hybrid Knowledge Distillation for Robust and Accurate FL (HYDRA-FL)_, which reduces the impact of attacks in attack scenarios by offloading some of the KD loss to a shallow layer via an auxiliary classifier. We model HYDRA-FL as a generic framework and adapt it to two KD-based FL algorithms, FedNTD and MOON. Using these two as case studies, we demonstrate that our technique outperforms baselines in attack settings while maintaining comparable performance in benign settings. Our code is available at [https://github.com/momin-ahmad-khan/HYDRA-FL](https://github.com/momin-ahmad-khan/HYDRA-FL).

## 1 Introduction

Federated Learning (FL)  is an emerging machine learning paradigm enabling multiple users' collaborative model training without data sharing. Each user, termed a _client_, only shares their local model with a _server_, which aggregates all local models into a single global model and redistributes it to the clients. Due to its decentralized, privacy-preserving, and highly-scalable nature, FL has been adopted by Google's Gboard  for next-word prediction, Apple's Siri  for automatic speech recognition, and WeBank  for credit risk prediction.

Despite its benefits, FL faces challenges with data heterogeneity . FL performs well when client data is independent and identically distributed (IID) and achieves similar convergence as a single model trained on all the clients' data but struggles when clients have diverse data (non-IID). In this case, the client's local data is not a good representation of the overall data distribution (unlike an ideal IID case), causing local models to _drift_ away from each other. This drift results in a global model with significant accuracy degradation compared to the IID scenario. Numerous solutions  address data heterogeneity, including Knowledge Distillation (KD)  to reduce the drift between local models.

Besides data heterogeneity, FL also faces the issue of Byzantine robustness , where _untrusted clients_ can inject _poisoned_ models into the aggregator by altering client data (data poisoning ) or client models (model poisoning [11; 4; 34; 46; 5; 3; 42]). Research by  shows that model poisoning attacks are more potent as they directly manipulate local models. To counteract poisoning in FL, various defenses have been developed [6; 48; 51; 7; 27; 9; 8].

In this work, we identify a critical vulnerability in KD-based FL techniques under model poisoning attacks. The adversarial settings, i.e., threat model, attacks and defense, are explained in SSC. These techniques unknowingly align benign client models with a poisoned server model (Figure 1). We study two such classes of KD-based solutions: FedNTD , which reduces the loss between not-true logits of the server and client models, and MOON , which reduces the contrastive loss between the representation vector of the server and client models. While these techniques improve global model accuracy in benign settings compared to FedAvg  (standard FL aggregator), they _reduce performance below FedAvg under attack_, a phenomenon we term _attack amplification_, especially noticeable at higher heterogeneity levels.

Motivated by our findings, we propose a Hybrid Knowledge Distillation for Robust and Accurate FL (HYDRA-FL) framework for KD-based techniques that restricts attack amplification under poisoning attacks while retaining performance in the benign setting. Unlike traditional KD methods that apply KD-loss only at the final layer, HYDRA-FL introduces KD-loss at a _shallow layer_ via an auxiliary classifier and reduces the KD-loss impact at the final layer. This approach draws inspiration from Self-Distillation (SD)  and Skeptical Students (SS) , but with a distinct focus on enhancing robustness against heterogeneity and model poisoning attacks in FL.

SD improves model accuracy by self-distillation, while SS distills from "nasty teachers"  to shallow layers. In contrast, our approach uses auxiliary classifiers to enhance FL client robustness against heterogeneity and model poisoning attacks. We design a generic loss function adaptable to specific KD-based algorithms. Our evaluation show that HYDRA-FL significantly boosts accuracy over FedNTD and MOON in attack settings while maintaining performance in benign settings.

**Contributions.** This work addresses the critical issue of attack amplification in KD-based FL techniques to counter data heterogeneity. In doing so we make the following contributions:

1. **Proving KD amplifies model poisoning:** our motivational case study (SS3) on two KD-based techniques, FedNTD and MOON, shows that KD improves accuracy in benign settings but helps the malicious clients propagate poisoning through the KD-loss in adversarial settings. We empirically and theoretically show that this attack amplification issue is inherent to any technique aligning client outputs/representations with the server.
2. **Designing HYDRA-FL:** using our observations as a guideline, we design HYDRA-FL (SS4) to prevent attack amplification while retaining performance in the benign setting. HYDRA-FL is formulated as a general loss function that can be adapted to any FL algorithm so that it can be used as its local model training objective.
3. **Implementation and Evaluation:** we adapt HYDRA-FL to the FedNTD and MOON algorithms and modify their local training objectives (SS5). Our detailed qualitative and quantitative analysis (SS6) shows that HYDRA-FL not only achieves higher accuracy in attack settings but also maintains accuracy in the benign settings.

Figure 1: Overview of attack amplification through knowledge distillation. **a)** In the benign setting, KD reduces drift and brings benign local models closer to the benign global model. **b)** In the malicious setting, KD _unknowingly_ reduces drift between benign local models and the poisoned global model.

Background and Related Work

### Federated Learning (FL)

In FL [15; 33], a service provider, called _server_, trains a _global model_, \(^{g}\), on private data from multiple collaborating clients, without directly collecting their data. The server selects \(n\) out of \(N\) clients in every FL round and shares the most recent global model (\(^{t}_{g}\)) with them, where \(t\) is the round number. Then, a client \(k\) uses local data \(D_{k}\) to compute an update \(^{t}_{k}\) and shares it with the server. The server aggregates these updates using some _aggregation rule_, like FedAvg  algorithm.

In _FedAvg_, a client \(k\)_fine-tunes_\(^{t}_{g}\) on their local data using stochastic gradient descent (SGD) for a fixed number of local epochs \(E\), resulting in an updated local model \(^{t}_{k}\). The client then computes their update as the difference \(^{t}_{k}=^{t}_{k}-^{t}_{g}\) and shares \(^{t}_{k}\) with the server. Next, the server computes an aggregate of client updates, \(f_{}\) using mean, i.e.,

\[^{t}_{}=f_{}(^{t}_{\{k[n]\}}). \]

The server then updates the global model of the \((t+1)^{th}\) round using SGD and server learning \(\) as:

\[^{t+1}_{g}^{t}_{g}+^{t}_{} \]

#### 2.1.1 Data Heterogeneity in FL

Data heterogeneity is a well-explored problem [29; 52; 14; 25] in FL. Each client in FL generates its data, leading to local data distributions that vary across clients and do not accurately represent the global data distribution. By extension, a global model learned by aggregating local models using FedAvg may not be the best representation of all the client's local data. Studies have shown that this data heterogeneity degrades performance and have proposed various methods to address this issue [26; 21; 23; 54; 24; 47; 16; 28]. This degradation is more prominent in the presence of poisoning attacks. Research on poisoning attacks in FL has demonstrated that such attacks become more successful under high heterogeneity [11; 42]. This increased risk is because the malicious clients can more easily hide between drifted benign client models, making it difficult for the server to differentiate between heterogeneous benign clients and malicious ones.  highlights that overlooking this heterogeneity is a critical oversight in FL defense evaluations. Unlike MOON  and FedNTD , the two baseline techniques used in this paper which address data heterogeneity only in no-attack settings, HYDRAFL operates in both no-attack and attack settings, achieving robustness and accuracy under high heterogeneity.

#### 2.1.2 Poisoning in FL

FL is vulnerable to poisoning attacks [6; 4; 5; 3; 34; 11; 32; 46; 36; 42], where malicious clients aim to compromise the training process by degrading the global model's performance. These attacks come in various forms: In _data poisoning_, malicious clients poison their local data to introduce a backdoor in the local model. This backdoor then propagates to the global model upon aggregation. In _model poisoning_[11; 4; 34; 46; 5; 3; 42], malicious clients perturb their local models so that, when aggregated, the global model is poisoned. Poisoning attacks can be further classified based on their targets: If the performance degradation is on specific inputs, the attack is termed as _targeted poisoning_[5; 3], and if it is on all inputs, then it is termed as _untargeted poisoning_[11; 4; 34; 32; 46]. We explain the attacks used in this paper in SSC.2. Our paper shows how KD-based data-heterogeneity mitigation techniques amplify poisoning attacks and that our approach _HYDRAFL addresses both data heterogeneity and poisoning_.

### Knowledge Distillation (KD)

Knowledge Distillation (KD)  transfers knowledge from a large, complex model (_teacher_) to a smaller, more computationally efficient model (_student_). This process involves distilling the teacher's rich and intricate information into the student by aligning their predictions. Formally, if the teacher and student models produce the output probabilities \(y^{t}_{i}\) and \(y^{i}_{s}\) respectively for the \(i^{th}\) input \((x^{i},y^{i})\), KD aims to match these probabilities by applying the Kullback-Leibler (KL) divergence between them. The KL-divergence between their softened probabilities is given by:\(KL(softmax(y_{t}^{i}/)||softmax(y_{s}^{i}/)\), where \(\) is the temperature parameter that softens the probabilities. The overall KD loss function combines this KL-divergence with the usual loss function such as cross-entropy (CE) loss with \(\) (balances the importance of the KL-divergence and CE loss) as:

\[=(1-)_{CE}(y_{s}^{i},y^{i})+_{KL}(softmax(y_{s}^{i}/)||softmax(y_{t}^{i}/)) \]

**KD in FL** is becoming essential as it addresses critical challenges such as non-IID data distributions, enhances model performance, accelerates convergence, reduces communication overhead, and improves robustness by making the global model learn from an ensemble of local models [10; 23; 30; 53]. In FL, data is often non-IID across clients, leading to significant discrepancies in local models. KD mitigates these discrepancies by aligning the local models with the global model, ensuring that the global model captures a more generalized representation of the data. The general approach is to reduce the local model drift by improving the aggregation through distillation using unlabeled auxiliary data. However, the auxiliary data may not always be available, and methods have also been developed to enable KD without such data [49; 54].

## 3 Attack Amplification through Knowledge Distillation

**Hypothesis.** KD-based techniques in FL improve accuracy in non-adversarial settings but result in more significant accuracy degradation under model poisoning attacks compared to the baseline techniques such as FedAvg.

**Motivational case study.** In this case study, we compare FedAvg against two distinct KD-based solutions addressing the local model drift from non-IID. MOON  uses model-contrastive learning to align local and global model _representations_, while FedNTD  uses KL-divergence to align _not-true logits_ of client models with those of the server. FedNTD penalizes prediction divergence measured through distillation loss, improving knowledge transfer and stability, while MOON penalizes _representation divergence_ measured through contrastive loss, enhancing robustness and generalization. This comparison will help us understand the trade-offs of using KD in FL, especially under adversarial conditions. Throughout this paper, benign conditions mean that no attacks are present, while adversarial conditions mean that model poisoning attacks are present. We implement the same settings and hyperparameters for FedAvg as for MOON and FedNTD to ensure a fair comparison, so FedAvg results may vary between these techniques. This is not an inconsistency. _We do not directly compare FedNTD to MOON unless stated otherwise_, as the original FedNTD work already did so. Our goal is to test how adversarial settings affect these two fundamentally different techniques similarly, demonstrating that _attack amplification is inherent to KD and not specific to a particular technique._

_Adversarial conditions._ We simulate untargeted model poisoning attacks using techniques from [42; 11]. To observe their effects on accuracy in both benign and adversarial settings, we vary key hyperparameters -- LD-divergence loss coefficient \(\) for FedNTD and contrastive loss coefficient \(\) for MOON. The baseline for comparison is FedAvg with \(=0\) and \(=0\). To ensure high heterogeneity in both settings, the Dirichlet distribution  parameter \(\) is fixed at \(0.1\).

_Findings._ In Figures 2(a) and 2(b), we present three key results: benign accuracy (blue), post-attack accuracy (orange), and the accuracy drop (green). We make the following observations from increasing \(\) and \(\) are as follows: (1) the global model accuracy improves in benign settings; (2) post-attack accuracy decreases; and (3) accuracy drop increases. Our analysis shows a significant trade-off: _the very mechanisms that improve performance in benign conditions (increasing \(\) and \(\)) also make the models more vulnerable to adversarial attacks._

**What causes attack amplification?** The fundamental nature of KD-based FL methods aims to align local models with the global model. In benign scenarios, these methods significantly outperform

Figure 2: Impact of increasing KL-divergence loss for FedNTD and contrastive loss for MOON on accuracy.

FedAvg . However, in the presence of model poisoning attacks, _this model alignment process inadvertently forces local models to align its representation/predictions to the poisoned global model, amplifying the attack's impact_. This is illustrated in Figure 1, where clients _unknowingly distill knowledge_ from a poisoned server model.

**Formally:** Consider a set of \(n\) clients \(c_{1},c_{2},,c_{n}\) with \(m\) being malicious. Using an aggregation rule such as FedAvg, the server aggregates updates from both benign (\(_{i[m+1,n]}\)) and malicious (\(^{m}_{i[m]}\)) clients:

\[_{g}=f_{}(^{m}_{i[m]}_{i[m+1,n]}) \]

When \(m=0\), the server model \(^{b}_{g}\) is benign. For \(m 0\), the server model \(^{}_{g}\) is poisoned, deviating from the ideal unpoisoned global model due to the nature of these attacks . Aligning local models with a poisoned global model reduces gradient diversity, making local models more similar to the poisoned global model  through KL-divergence or contrastive loss. We rewrite Equation 3 to formalize the loss function for an FL client, using KD, where the client is the student with output \(_{c}\), and the server is the teacher with output \(y_{s}\):

\[=_{CE}(_{c},y)+_{KL}(_{c}, y_{s}) \]

Note that for the sake of derivation here, we are using \(_{c}\), which represents the generic client model output. In the case of FedNTD, it can be replaced by \(_{c}\) that represents the not-true logits of the client model, and in the case of MOON, it can be replaced by \(z_{c}\) that represents the client model's high dimensional representation.

In benign scenarios, this loss function (\(=()\)) decreases monotonically with \(\) because KD brings local models closer to an unpoisoned global model. Conversely, in adversarial scenarios, it increases with \(\) because KD brings local models to the poisoned global model. We can write the relation of this loss function with \(\) as:

\[(),&m=0\\ ,&m 0 \]

Then, the derivative of the loss function is:

\[}{d}=<0,&m=0\\ >0,&m 0 \]

Our derivation shows that while the distillation process decreases loss in the absence of malicious clients, it increases loss in their presence, thereby leading to reduced global model accuracy. This formal analysis highlights the need for a solution that mitigates the accuracy degradation under adversarial conditions while retaining the benefits of KD under benign conditions.

**Impact of Heterogeneity.**

Now, we explore the effect of heterogeneity on the performance of FedNTD, MOON, and FedAvg in both benign and adversarial conditions to gain deeper insights into the role of heterogeneity in the KD performance gain vs. vulnerability tradeoff. As shown in Figure 3(a), several interesting observations emerge. First, both FedNTD and FedAvg achieve higher accuracy at lower heterogeneity levels (indicated by higher \(\)). In benign settings, FedNTD consistently outperforms FedAvg. However, _the trend reverses in adversarial settings:_ FedAvg achieves higher accuracy than FedNTD, except at \(=0.5\). A similar pattern is observed with MOON in Figure 3(b), where FedAvg outperforms MOON across all heterogeneity levels in adversarial settings. In the benign setting, as expected, MOON slightly outperforms FedAvg at high heterogeneity. This comparison highlights again how the alignment mechanisms in FedNTD and MOON with higher heterogeneity exacerbate the vulnerability of KD methods to attacks.

Figure 3: Impact of the heterogeneity parameter, \(\) in benign and adversarial settings. We use the Dirichlet distribution where a higher \(\) means lower heterogeneity.

## 4 HYDRA-FL: Hybrid Knowledge Distillation for Robust and Accurate FL

### Generic Formulation

In this section, we propose Hybrid Knowledge Distillation for Robust and Accurate FL (HYDRA-FL), a technique to mitigate the _attack amplification_ caused by KD in FL. We take a hybrid distillation approach, applying KD-loss at both the final and a shallow layer of the client model (Figure 4). This method incorporates shallow distillation, which applies KD-loss at an intermediate layer and helps reduce the impact of poisoning by preventing over-reliance on final layer alignment. Shallow distillation previously used to handle _nasty teachers_ trained adversarially , to reduce the impact of poisoning. In summary, shallow layers capture basic features, and shallow distillation ensures these features are robustly learned, protecting the model from adversarial influences that could corrupt deeper layers and final outputs. We first formulate the generic loss function of an FL client using KD in HYDRA-FL as:

\[=_{CE}(y_{c},y)+_{KD}(y_{c},y_ {s})+_{KD}(y_{aux},y_{s}) \]

This loss function has three key components:

1. **Cross-entropy loss (\(_{CE}(y_{c},y)\))** is the loss between the client's prediction \(y_{c}\) and the target \(y\), drives the client model to learn from its own data, ensuring it captures _in-distribution knowledge_ such as features and patterns specific to its data.
2. **Diminished KD loss (\(_{KD}(y_{c},y_{s})\))** is the loss between the client's output/representation \(y_{c}\) and the server's output/representation \(y_{s}\)1. It is a strategic reduction of the KD loss to ensure that the local model benefits from the global model's knowledge while remaining robust against adversarial attacks. This approach helps balance the trade-offs between learning efficiency and model integrity. In practice, this is achieved by introducing a _diminishing factor_\(b\) to the KD loss at the client model's output layer to diminish the poisoning effect. The KD loss coefficient \(\) is divided by \(b\), effectively reducing its weight in the total loss calculation, thus reducing its influence on the local model's training. This diminishing factor is essential, as shown later in SS6.3 and Figure 7, where reducing the \(\) yields better results. 3. **Shallow distillation loss (\(_{KD}(y_{aux},y_{s})\))** is applied at a shallow layer of the local model, enhancing robustness without heavily relying on final layer alignment. This loss, between the auxiliary classifier's output/representation \(y_{aux}\) at the client model's shallow layer and the server's output/representation \(y_{s}\), is scaled by \(\) to control the amount of distillation. This reduces the impact of poisoning on the client model. Simply reducing the KD-loss in FedNTD or MOON improves post-attack accuracy but reduces benign setting accuracy, as shown in Figure 2. Shallow distillation loss helps maintain the balance between accuracy in benign settings and lowering the impact of poisoning on the client model in adversarial settings.

**Differences with previous works.**The key difference between our work and  lies in our approach to shallow distillation.  aims to distill from models that are designed to be undistillable, a.k.a

Figure 4: HYDRA-FL framework: we refine client model training by reducing the final layer’s KD-loss and incorporating shallow KD-loss at an earlier shallow layer via an auxiliary classifier.

nasty teachers _. While both use hybrid shallow distillation,  completely removes the KD-loss from the model's output layer and uses self-distillation to compensate for performance loss due to shallow distillation. In contrast, we retain a scaled-down KD-loss at the output layer. We found that completely removing the KD-loss at the output layer may cause a more negative impact than keeping it in a reduced form. Additionally, the untargeted poisoning is different from the poisoning in the "nasty teacher" paper . The "nasty teacher" performs near-perfect under normal conditions unless a malicious model distills from it. In untargeted FL poisoning, the global model is poisoned and performs poorly regardless of its use for distillation.

In HYDRA-FL, we use both final layer and shallow layer distillation to enhance robustness. _Final layer distillation_ aligns client outputs with server outputs for consistent predictions, whereas the _shallow layer distillation_ aligns intermediate representations to improve robustness against attacks. This dual approach reduces vulnerability to poisoning attacks, enhances learning by leveraging knowledge transfer from multiple layers, and maintains high accuracy in benign settings while being resilient under attack conditions.

### Adapting HYDRA-FL to State-of-Art Techniques

In this section, we will adapt our generic HYDRA-FL to two state-of-the-art KD techniques for FL.

**FedNTD with shallow distillation and auxiliary classifiers.** We modify the FedNTD base model by introducing auxiliary classifiers. The base model includes two convolutional layers, a linear layer, and a classification layer. Auxiliary classifiers, each consisting of a linear layer (hidden dimension \(512\)) followed by a classification layer, are added after each convolutional layer. We update the loss function to include a shallow-distillation term, representing the KL-divergence loss between the not-true logits of an auxiliary classifier and the global model. The final loss function is a weighted sum of the standard cross-entropy loss, KL-divergence loss between the not-true logits of the global model and the client model, and the KL-divergence loss between the not-true logits of the global model and the auxiliary classifier. The revised loss function in Equation 8 for FedNTD is:

\[=_{CE}(y_{c},y)+_{KL}(_{c},_{s})+_{KL}(_{aux},_{s}) \]

Here \(y\) is the target label, \(y_{c}\) is the client model's output, \(_{s}\), \(_{c}\), and \(_{aux}\) are the client model's, server model's, and auxiliary classifier's not-true logits respectively.

**MOON with shallow distillation and auxiliary classifiers.** MOON base model has two convolution layers, two linear layers, and an output classification layer. We insert auxiliary classifiers after each convolution layer. Each auxiliary classifier has two linear layers, with a hidden dimension of \(256\) and an output dimension of \(10\). We adapt Equation 8 to MOON to compute the contrastive loss at the hidden representation layer of the auxiliary classifier as:

\[=_{CE}(y_{c},y)+_{con}(z_{c},z_{s })+_{con}(z_{aux},z_{s}) \]

Here \(y\) is the target label, \(y_{c}\) is the client's output, \(z_{c}\) is the representation from the client's final layer, \(z_{s}\) is the representation from the server's final layer, \(z_{aux}\) is the representation from the client's auxiliary classifier, and \(y_{s}\) is the server model's output. For simplicity, we do not write the previous round's representation in the loss function here.

## 5 Experimental Results

### Experimental Settings

**Datasets and Models:** We conduct our experiments over three popular datasets: MNIST, CIFAR10, and CIFAR100. To ensure a fair comparison with previous works, MOON and FedNTD, we utilized the same models and hyperparameters they used. Specifically, we incorporated our algorithm as a simple modification into their publicly available codes  (more details on experimental and adversarial setups in Appendices D & C).

### Shallow Not-True Distillation

Our hybrid shallow not-true distillation technique significantly improves post-attack accuracy over the baseline FedNTD. As shown in Table 1, we achieve higher post-attack accuracy across all heterogeneity levels. By retaining a diminished NTD loss at the output layer, we maintain similar accuracy to FedNTD in no-attack scenarios and, in some cases, even achieve slightly higher accuracy. We also compare no-attack and post-attack accuracies for FedAvg, the foundational algorithm for many FL aggregation methods.

### Shallow MOON

Our shallow-distillation design effectively prevents attack amplification in MOON while maintaining nearly the same no-attack accuracy. Table 2 shows that we achieve higher post-attack accuracy across all heterogeneity levels. Our technique also outperforms FedAvg, except in a few scenarios. Techniques like MOON are designed to enhance accuracy under high heterogeneity (\(=0.1\)). HYDRA-FL achieves a no-attack [attack] accuracy of \(60.1\%[43.6\%]\), surpassing both MOON (\(58.8\%[39.9\%]\)) and FedAvg (\(57.76\%[40.9\%]\)), thereby demonstrating efficacy in high heterogeneity.

## 6 Analysis

In this section, we provide an in-depth analysis of HYDRA-FL. We begin with a theoretical justification behind the efficacy of HYDRA-FL. Then we perform a qualitative analysis using t-distributed stochastic neighbor embedding (t-SNE ) plots to visualize the representations of the models. Finally, we explore the impact of different design choices through ablation studies, focusing on the choice of the shallow layer for auxiliary classifiers and the distillation coefficients.

### Theoretical Justification

Here we provide the theoretical justification for the working of HYDRA-FL. The generic loss function of an FL client using KD is:

\[=_{CE}(_{c},y)+_{KD}(y_{c},y_{s}) \]

In Section 3, we showed that in the presence of malicious clients, \(}{d}>0\), i.e., KD causes the loss to increase. In Section 4, we formulated the generic loss function of an FL client using KD in

   &  &  &  \\   &  &  &  &  &  &  &  &  &  \\   Techniques & _no attack_ & _attack_ & _no attack_ & _attack_ & _no attack_ & _attack_ & _no attack_ & _attack_ & _no attack_ & _attack_ \\  FedAvg & 92.12 & 74.48 & 44.69 & 31.27 & 54.67 & 35.67 & 70.57 & 48.27 & 26.17 & 12.92 \\  FedN1D & 93.03† & 58.09 & 46.94† & 21.72 & 56.95† & 32.61 & 71.79† & 52.51 & 29.1† & 13.92 \\  HYDRA-FL(Ours) & 92.69† & **76.67** & **46.92** & **25.15** & **57.12† & **34.25** & **71.22† & **52.57** & **28.9† & **14.33** \\  

Table 1: Test accuracy for three techniques on three datasets. In the no-attack setting, (\(\)) shows comparison to FedAvg. In the attack setting, we use bold if our technique outperforms FedNTD.

   &  &  &  \\   &  &  &  &  &  &  &  \\  
**Methods** & _no attack_ & _attack_ & _no attack_ & _attack_ & _no attack_ & _attack_ & _no attack_ & _attack_ & _no attack_ & _attack_ \\  FedAvg & 88.02 & 77.55 & 57.76 & 40.9 & 63.14 & 60.2 & 71.19 & 68.38 & 28.36 & 24.21 \\  MOON & 91.13† & 72.32 & 58.8 & 39.9 & 63.34† & 57.17† & 70.95† & 67 & 29.34† & 23.81 \\  HYDRA-FL(Ours) & 92.04† & **76.65** & 60.1† & **43.36** & 63.32† & **59.93†** & 70.55† & **68.4†** & 29.48† & **25.18** \\  

Table 2: Test accuracy for three techniques on three datasets. In the no-attack setting, (\(\)) shows comparison to FedAvg. In the attack setting, we use bold if our technique outperforms MOON.

Figure 5: HYDRA-FL vs. MOON and FedAvg when auxiliary classifiers are placed at different shallow layers.

HYDRA-FL as:

\[^{}=_{CE}(y_{c},y)+_{KD}(y_{ c},y_{s})+_{KD}(y_{aux},y_{s}) \]

We use \(^{}\) for HYDRA-FL loss and \(\) for basic KD loss. Comparing them, since \(<\)\(\), it follows that \(_{KD}(y_{c},y_{s})<_{KD}(y_{c},y_{s})\). The entire model is \(\), and the part up til the auxiliary layer \(^{}\). The impact of shallow distillation loss is only on \(^{}\) and not the rest of the model \(-^{}\). Therefore, with appropriate \(b\) and \(\), \(^{}}{d}<}{d}\). This is shown in Figure 7, where we select \(b=\) (so \(\) effectively becomes \(1\) and \(\), respectively) and \(=2\) to show the effectiveness of our technique and the impact of the variation of these hyperparameters.

### Qualitative Analysis

We show the t-SNE plots of the representations (Figure 6) generated by the client model for FedAvg, MOON, and HYDRA-FL for both attack and no-attack scenarios. The t-SNE plots show the classes as clusters. In the MOON attack scenario, the deviation from the no-attack scenario is much higher than the deviation between HYDRA-FL with and without attack, as evident from the spread of the class clusters, especially along the x-axis.

### Ablation Study

**Impact of choice of the shallow layer.** Figure 5 illustrates the impact of the choice of the layer at which we insert our auxiliary classifier. We represent these choices by _HYDRAFL-S1_ and _HYDRAFL-S2_, where the auxiliary classifier is inserted after the first and second convolutional layers, respectively. We compare them in both attack and no-attack settings with simple MOON and FedAvg. In Figure 5(a), both HYDRAFL-S1 and HYDRAFL-S2 outperform other techniques at low heterogeneity in the absence of an attack but slightly underperform in low heterogeneity when \(=5\). Figure 5(b) shows that both HYDRAFL-S1 and HYDRAFL-S2 achieve higher post-attack accuracy at all heterogeneity levels, with HYDRAFL-S2 giving a slightly higher accuracy than HYDRAFL-S1. The benefit from the contrastive loss reduces as we go shallower, so an optimal balance is necessary.

**Impact of distillation coefficients.** We examine the impact of distillation coefficients on the performance of FedNTD and HYDRA-FL. Figure 7 shows the post-attack accuracies with two different values of the _diminishing factor_\(b=1,4\), resulting in output-layer NTD-loss coefficients of \(=1\) and \(=0.25\). Diminishing the coefficient \(\) leads to improved performance, with a significant

Figure 6: T-SNE visualizations of CIFAR10 on local model’s hidden representations (\(=0.5\)) on FedAvg, MOON, and HYDRA-FL (ours). The attack vs. no-attack plot shows the deviation of the attack clusters from the no-attack clusters. Visually we can see MOON-attack has the greatest deviation, particularly along the x-axis, compared to FedAvg and HYDRA-FL.

increase in post-attack accuracy for \(=0.25\) at high heterogeneity (\(=0.05,0.1\)). As demonstrated in SS3, \(\) contributes to attack amplification in FedNTD. Reducing it while performing distillation at the auxiliary classifier yields the best performance. For example, at \(=0.05\), HYDRA-FL achieves \(25.15\%\) accuracy at \(=1\), but a much higher accuracy of \(28.81\%\) at \(=0.25\). Similar improvements are observed at other heterogeneity levels.

### Additional Results on MNIST at Very High Heterogeneity

To demonstrate robustness and generalizability, we conducted more experiments now on MNIST at a much lower heterogeneity of 0.05. Table 3 shows attack amplification and the effectiveness of our solution. MOON's accuracy drops from 73.57% (no-attack) to 55.32% (attack), showing attack amplification compared to FedAvg. HYDRAFL achieves higher accuracy than both FedAvg and MOON in the no-attack setting and higher accuracy than MOON in the attack setting, reducing attack amplification.

### Thoughts on Data Poisoning

While data poisoning in the context of knowledge distillation (KD) requires a dedicated study, we believe our solution is broadly applicable as it prevents the flow of poison through the entire model and dilutes its effect. Consider a simple example: label flipping, a basic data poisoning attack where some labels in the training dataset are flipped (e.g., changing a bird image's label to 'airplane' in CIFAR-10). Suppose the server model (teacher) is poisoned with this flipped dataset. During the distillation process, since we are using HYDRAFL, this poisoned distillation information would be diluted at the final layer. Instead, it will flow through an auxiliary classifier into the early layers of the model, thereby reducing the impact of the data poisoning attack via label flipping.

## 7 Conclusion

In this paper, we first identified a critical issue in KD-based FL techniques that aim to tackle data heterogeneity: in the presence of model poisoning attacks, these techniques help the attacker amplify its effect, leading to reduced global model performance. We presented empirical evidence and theoretical reasoning to back this claim. This motivated us to propose HYDRA-FL: a hybrid knowledge distillation technique for robust and accurate FL technique that aims to tackle both data heterogeneity and model poisoning, two of the biggest problems in FL. Through extensive evaluation across three datasets and comparing with baseline techniques, FedNTD and MOON, we showed that HYDRA-FL achieves superior results.

## 8 Acknowledgements

We thank the NeurIPS reviewers for their insightful feedback and comments. This research is supported by NSF grant 2237485.

 
**Methods** & _no attack_ & _attack_ \\   FedAvg & 69.2 & 59.36 \\  MOON & 73.57 & 55.32 \\  HYDRAFL shallow layer1 & 76 & 57.4 \\  HYDRAFL shallow layer2 & 75.15 & 57.71 \\  

Table 3: MNIST test accuracy with 0.05 heterogeneity for MOON

Figure 7: Comparison of performance of FedNTD-S with different values of \(\)