# NVFi: Neural Velocity Fields for 3D Physics Learning from Dynamic Videos

Jinxi Li Ziyang Song Bo Yang

vLAR Group, The Hong Kong Polytechnic University

jinxi.li@connect.polyu.hk ziyang.song@connect.polyu.hk bo.yang@polyu.edu.hk

###### Abstract

In this paper, we aim to model 3D scene dynamics from multi-view videos. Unlike the majority of existing works which usually focus on the common task of novel view synthesis within the training time period, we propose to simultaneously learn the geometry, appearance, and physical velocity of 3D scenes only from video frames, such that multiple desirable applications can be supported, including future frame extrapolation, unsupervised 3D semantic scene decomposition, and dynamic motion transfer. Our method consists of three major components, 1) the keyframe dynamic radiance field, 2) the interframe velocity field, and 3) a joint keyframe and interframe optimization module which is the core of our framework to effectively train both networks. To validate our method, we further introduce two dynamic 3D datasets: 1) Dynamic Object dataset, and 2) Dynamic Indoor Scene dataset. We conduct extensive experiments on multiple datasets, demonstrating the superior performance of our method over all baselines, particularly in the critical tasks of future frame extrapolation and unsupervised 3D semantic scene decomposition. Our code and data are available at [https://github.com/vLAR-group/NVFi](https://github.com/vLAR-group/NVFi)

## 1 Introduction

The 3D world around us is constantly changing over time, where objects are falling, vehicles moving, and clocks ticking. Humans can effortlessly learn the geometry and physical properties of such dynamic 3D scenes, and further predict their future motions following the learned physics rules, just by watching the things for a few seconds. Giving machines such ability to automatically infer the geometry and physics of complex dynamic 3D scenes is essential for many cutting-edge applications in augmented reality, games, and the movie industry. Recent advances in the emerging area of neural radiance field  and its succeeding variants  have shown excellent results in modeling dynamic 3D scenes such as deformable things  and articulated objects . While showing superior performance in interpolating novel views within the observed time period, almost all these methods tend to fit training image sequences, without explicitly learning the physical properties such as object velocities, thus being unable to extrapolate and predict future motion patterns of 3D scenes.

More recently, a few studies  start to integrate physics priors into implicit neural representations to model dynamic 3D scenes such as floating smoke or simple moving objects. By introducing the governing PDEs, _a.k.a._, PINN , these methods demonstrate promising reconstruction of 3D scene geometry, appearance, velocity and/or viscosity fields. However, the learned physical properties are either tightly coupled with the target objects  in the scene or require additional foreground segmentation masks in the loop . This means that the estimated physics knowledge from dynamic video frames is not clearly disentangled, thereby not transferable from one scene to another.

In this regard, we ask an ambitious question: _Can we learn disentangled physical properties alongside recovering geometry and appearance of dynamic 3D scenes purely from multi-view video frames?_ Among various physical properties in the scene, we choose to focus on velocity as it primarily governs scene movement dynamics. Such disentangled velocity fields, once successfully learned, are expected to unlock multiple desirable applications: 1) Future frame extrapolation and prediction beyond the observed time period in training. For example, after watching a football flying in the penalty area, we can predict what will happen next. 2) Dynamics transformation from one scene to another. For instance, after watching a bird flipping wings, we can imagine the same physical behavior on the body of an airplane. 3) Semantic decomposition of 3D scenes. Intuitively, once the velocity field of an entire dynamic 3D scene is learned, all individual objects or parts undergoing different moving patterns can be easily segmented, without needing any extra human annotations for training.

However, accurately learning the physical velocity of a whole 3D scene space is particularly challenging, primarily due to the lack of ground truth 3D velocity annotations, the unknown object types or materials in the scene, and the sparse yet long-time visual trajectory in training. In addition, when separately learning the velocity field, it is usually an under-constrained problem even with the commonly used PINN technique .

To tackle these challenges, we introduce a new general framework to simultaneously model the geometry, appearance, and disentangled velocity of a dynamic 3D scene only from multi-view video frames. In particular, as shown in Figure 1, our framework consists of three major components: 1) a **keyframe dynamic radiance field** to learn time-dependent volume density and appearance for every 3D point in space; 2) an **interframe velocity field** to learn time-dependent 3D velocity for every point as well; and 3) a **joint keyframe and interframe optimization** method together with physics informed constraints to train both networks. For the first component, it is flexible to adopt any of the existing time-dependent NeRF architectures such as HexPlane  or K-Planes . For the second component, the neural network actually can be as simple as MLPs.

The core of our framework is the third component, where we explicitly apply three types of loss functions to jointly optimize both networks: 1) the keyframe photometric loss, 2) the interframe photometric loss, and 3) the governing PDE losses. With these losses, our framework can precisely learn disentangled velocity fields, without needing additional regularization terms on volume density or information on object masks, types, or materials. Overall, our framework models general dynamic 3D scenes by learning **neural** velocity **fields** with physical priors. Our method is named **NVFi** and our contributions are:

* We introduce a general framework to model dynamic 3D scenes as physics-informed radiance fields from multi-view videos, without requiring information on object types, materials, or masks.
* We design a neural velocity field together with a joint keyframe and interframe optimization method to effectively train the networks.
* We demonstrate three applications for the learned velocity fields on two newly collected dynamic 3D datasets and a challenging real-world dataset, showing superior results in future frame extrapolation, semantic decomposition, and velocity transferring across 3D scenes.

## 2 Related Works

**Static 3D Representations:** Conventional representations for static 3D objects and scenes include voxels [13; 71; 72], point clouds , octrees [57; 64], meshes [28; 23], and primitives . Due to the discretization issue of these explicit representations, the fidelity of 3D shapes is usually limited by spatial resolution and memory footprint. Inspired by the seminal works [10; 36; 41], recently, there

Figure 1: The three major components of our framework: Keyframe Dynamic Radiance Field, Interframe Velocity Field, and the Joint Keyframe and Interframe Optimization module.

has been a strong interest in representing 3D data in implicit neural functions. These methods are generally classified as: 1) occupancy fields (OF) [10; 36], 2) signed distance fields (SDF) , 3) unsigned distance fields (UDF) [12; 62], and 4) radiance fields (NeRF) [37; 3; 59]. Basically, these neural representations simply take 3D point locations as input and directly regress the corresponding point information, such as occupancy, distance to surface, color, semantics [76; 61], _et al._, via MLPs. Compared with traditional explicit representations, these implicit neural representations allow for continuous shape and appearance modeling at a particularly low memory footprint.

**Dynamic 3D Representations:** Recent techniques in this category are primarily built on the appealing NeRF architecture , thanks to its unprecedented level of fidelity in representing various 3D objects and scenes [6; 19]. Given video sequences of dynamic scenes, these NeRF based methods usually take the time \(t\) as an additional input dimension and then optimize the entire network using the standard photometric loss. Generally, the techniques can be classified as: 1) deformation-based methods [42; 58; 4; 46; 5; 18; 74], 2) flow-based methods [21; 33; 16; 63; 35], and 3) direct space-time based methods [69; 1; 31; 43]. In parallel, there are also a number of domain-specific NeRFs to model particular dynamic objects such as human bodies [44; 50; 45; 70; 68] and faces [4; 20; 66; 67]. Although all these methods have shown excellent performance in novel view synthesis on a wide range of dynamic datasets, they are primarily designed to interpolate frames within the time period of training data, lacking the ability to predict future frames. By contrast, our NVFi aims to model the underlying physical principles of 3D scenes, thus being easily able to extrapolate future dynamics.

**Physics Informed Deep Learning:** Unlike traditional numerical methods such as finite element methods, recent physics-informed neural networks (PINN)  represent tensor fields with neural networks, converting the PDE solutions into optimizing network weights via PDE-based loss functions. Inspired by the pioneering works [48; 52; 30], a plethora of succeeding works [51; 49; 25; 38; 65], have been developed, demonstrating excellent results in a wide range of applications such as acoustics [53; 56], fluids [60; 22; 15], 3D scene representations [8; 14; 32; 47], _et al._, thanks to its mesh-free formulation. More details can be found in the recent surveys [27; 24]. In our framework, we find that the commonly-used PINN constraints are insufficient to learn accurate 3D velocity fields. To tackle this issue, we further propose the keyframe based velocity propagation module.

## 3 NVFi

### Overview

As shown in Figure 2, our framework consists of two neural networks together with their optimization methods. Given a set of images of a dynamic scene with known camera poses and intrinsics, the **keyframe dynamic radiance field**\(f_{}\) simply takes a 3D point \(=(x,y,z)\), viewing angle \((,)\), and timestamp \(t\) as input, directly regressing the volume density \(\) and color \(=(r,g,b)\). For the network architecture, we simply adopt the recent HexPlane  which shows excellent performance in efficiently modeling dynamic video frames, though other NeRF variants can also be used. Formally, our keyframe dynamic radiance field is defined below and implementation details are in Appendix.

\[(,)=f_{}(x,y,z,,,t). \]

For the **interframe velocity field**\(g_{}\), it takes the 4D vector \((x,y,z,t)\) as input, and predicts the 3D velocity \(=(v_{x},v_{y},v_{z})\) for point \(\) at time \(t\). For simplicity, the network \(g_{}\) is parameterized by simple MLPs, though more advanced architectures can be applied as well. Formally, the velocity field is defined below and implementation details are in Appendix.

\[=g_{}(,t)=g_{}(x,y,z,t). \]

Figure 2: The left block illustrates the network architecture of our keyframe dynamic radiance field based on HexPlane , and the right block shows the architecture of our interframe velocity field.

With these two networks and training images sampled from a dynamic 3D scene, the key challenge is to effectively optimize these networks, such that the final learned velocity field is precise and disentangled, supporting future frame extrapolation, motion transfer, and semantic decomposition.

### Optimization of Keyframe Dynamic Radiance Fields

Given dynamic video frames of \(T\) timestamps \(\{1 t T\}\), there are two potential strategies to optimize dynamic neural radiance fields.

* Dense Frame Optimization:** This strategy uses all available video frames of a specific dynamic 3D scene in the training set to optimize a dynamic radiance field. This means that for the network \(f_{}\), the time dimension \(t\) is densely sampled during optimization. Formally: \[f_{}(x,y,z,,,t): t\{1 t T\}\] (3) However, it has two limitations: 1) It is inefficient to learn accurate 3D geometry and appearance because this strategy is somewhat equivalent to modeling a dense number of static radiance fields for all timestamps. 2) It is hard to obtain a disentangled physical velocity field for the entire 3D scene, as the change of physical geometries is tightly encoded within the dynamic radiance field.
* Canonical Frame Optimization:** This strategy optimizes a canonical representation of 3D geometry and appearance, usually joined with another deformation or transportation network to warp the future \((T-1)\) timestamps back to the first timestamp. It can be seen as: \[f_{}(x,y,z,,,t): t\{1\}\] (4) However, such a strategy has a strong assumption that the corresponding point appearances across different timestamps keep unchanged. Therefore, for dramatically changing 3D scenes, the optimized geometry, appearance, and possible jointly learned physics properties tend to be inferior.

**Our Strategy - Keyframe Optimization:** In this regard, we propose to adopt a keyframe based strategy to learn the dynamic radiance field \(f_{}\). In particular, we uniformly sample \(K\) timestamps out of the total \(T\) stamps to optimize \(f_{}\). Formally:

\[f_{}(x,y,z,,,t_{k}): t_{k}T/K,2 T/K,3T/K, T} \]

Color images are rendered from the above keyframe dynamic radiance fields by sampling points along rays. For each pixel, _i.e_. ray \(\), in a keyframe at time \(t_{k}\), the appearance \((,t_{k})\) is obtained by volume rendering of NeRF . Then the network \(f_{}\) can be optimized by the following keyframe photometric loss. Details of the rendering equation are in Appendix.

\[_{keyframe}=||(,t_{k})-}(,t_{k})||, {where }}(,t_{k}). \]

This keyframe optimization strategy, albeit simple, has two unique advantages: 1) It allows to sufficiently and accurately fit the sparsely sampled dynamic 3D scene geometry and appearance given the same network capacity. 2) It allows the remaining interframes belonging to the \((T-K)\) time stamps to be used for optimizing a disentangled velocity field, as discussed in Section 3.3.

### Optimization of Interframe Velocity Fields

As to our separate interframe velocity field \(g_{}\), it is impossible to directly supervise it using ground truth labels as they cannot be collected in practice. However, there are some physics rules to regularize the velocity field. The objects are transported by the velocity field, and the velocity field is transported by itself according to some unobservable hidden forces. In order to keep the mass and appearance of the objects, the velocity field needs to be divergence-free and obeys the basic law of momentum conservation, whose details are in Appendix. Note that, more complex physics dynamics beyond the daily 3D scenarios are out of the scope of this paper. In this regard, our velocity field, _i.e_., \(=g_{}(,t)\), needs to firstly satisfy the following two constraints.

\[_{}=0,}{ t}+ _{}= \]We simply turn these PDEs into the following two PINN losses  to optimize the velocity field. Here we use \((,t)\) as the network to avoid abuse of notation.

\[_{divergence\_free}= _{n=1}^{N}_{m=1}^{M}||_{_{n}} (_{n},t_{m})||\] \[_{momentum}= _{n=1}^{N}_{m=1}^{M}||( _{n},t_{m})}{ t_{m}}+(_{n},t_{m})_{ _{n}}(_{n},t_{m})-|| \]

where \(_{n}\) is uniformly sampled in the whole 3D scene volume, and \(t_{m}\) is uniformly sampled from 0 to the interested maximum extrapolation time \(t_{max}\), and \(\) is the general acceleration term learned by another MLP-based network: \((x,y,z,t)\), whose details are in Appendix. Nevertheless, with such PINN losses, it is insufficient to optimize the velocity field itself, since there are infinitely many solutions. To tackle this, we introduce an additional interframe optimization strategy.

**Interframe Optimization Strategy:** Naturally, the 3D scene geometry and appearance encoded in the keyframe dynamic radiance field, once appropriately transported by the velocity field, should be able to render 2D images to match with the ground truth observations in interframes belonging to the remaining \((T-K)\) timestamps. To enforce such a constraint, the key challenge is to determine the color and density values for all 3D points at each interframe timestamp, such that the volume rendering equation can be applied to estimate RGB for each pixel at the interframe timestamp, after which the photometric loss can be adopted. To tackle this, we propose the following Algorithm 1.

```
Input: \(\) The ray direction \((,)\), the interframe timestamp \(t_{i}\), the \(S\) sample points on the ray \(\{_{1}_{s}_{S}\}\); \(\) The \(K\) keyframe timestamps \(\{t_{1} t_{k} t_{k}\}\); \(\) The initialized and ongoing training networks: \(f_{}\) and \(g_{}\); Output: \(\) The color and density values for \(S\) sample points along the ray: \(\{(_{1},_{1})(_{s},_{s})(_{S},_ {S})\}\); \(\) Preliminary step: \(\) Find the nearest keyframe timestamp \(_{k}\) for the interframe timestamp \(t_{i}\): \(_{k}=}{}|t_{k}-t_{i}|\) for\(_{s}\) in \(\{_{1}_{s}_{S}\}\)do \(\) Transport \(_{s}\) to its corresponding point \(^{}_{s}\) at its nearest keyframe timestamp \(_{k}\), according to its velocity field. The position of \(^{}_{s}\) can be obtained by: \[^{}_{s}=_{s}+_{t_{i}}^{_{k}}g_{}(_{s}(t ),t)dt,]}}.\] \(\) Retrieve the volume density \(^{}_{s}\) and view-agnostic color feature vector \(^{}_{s}\) for point \(^{}_{s}\): \((^{}_{s},^{}_{s}) f_{}(^{}_{s},_{k}),]}}\) can output a view-agnostic color feature vector \(^{}_{s}\). \(\) Assign the retrieved features of \(^{}_{s}\) to the original point \(_{s}\): \((_{s},_{s})(^{}_{s},^{}_{s})\). \(\) Obtain the color \(_{s}\) for point \(_{s}\): \(_{s}_{}(_{s},,), _{}\) is a subnetwork of HexPlane backbone \(\@@cite[cite]{[\@@bibref{}{Kurz2014}{}{}]}\) as detailed in Appendix. \(\) Output \((_{s},_{s})\) for point \(_{s}\). After the above _for loop_, we obtain all color and density values for \(S\) sample points.
```

**Algorithm 1** At a specific interframe timestamp \(t_{i}\), given a light ray \(_{i}\) with viewing angle \((,)\) and \(S\) sample points \(\{_{1}_{s}_{S}\}\) along the ray, the objective of this algorithm is to determine the color and density values for the \(S\) points along \(_{i}\), denoted as: \(\{(_{1},_{1})(_{s},_{s})(_{S},_ {S})\}\). In the meantime, we also have the keyframe dynamic radiance field \(f_{}\) and velocity field \(g_{}\).

Note that, we shall not directly query \(f_{}\) to obtain color and density for the \(S\) points because: 1) the dynamic radiance field \(f_{}\) is never trained on interframe timestamps, thus the queried values are inaccurate; 2) the velocity field \(g_{}\) will not be involved, and therefore the interframes cannot provide additional constraints to optimize \(g_{}\).

``` Input: \(\) The ray direction \((,)\), the interframe timestamp \(t_{i}\), the \(S\) sample points on the ray \(\{_{1}_{s}_{S}\}\); \(\) The \(K\) keyframe timestamps \(\{t_{1} t_{k} t_{k}\}\); \(\) The initialized and ongoing training networks: \(f_{}\) and \(g_{}\); Output: \(\) The color and density values for \(S\) sample points along the ray: \(\{(_{1},_{1})(_{s},_{s})(_{S},_ {S})\}\); \(\) Find the nearest keyframe timestamp \(_{k}\) for the interframe timestamp \(t_{i}\): \(\) can be firstly trained by \(_{keyframe}\) only, and then the interframe velocity field \(g_{}\) by \(_{divergence\_free}+_{momentum}+_{interframe}\).

Nevertheless, we empirically find that simultaneously propagating errors of interframes back to the radiance field \(f_{}\) helps achieve better performance overall. Therefore, we adopt the following joint keyframe and interframe strategy to optimize both networks.

\[f_{}(_{keyframe}+_{interframe}) g_{} (_{divergence\_free}+_{momentum}+_{interframe}) \]

## 4 Experiments

**Datasets:** Our framework primarily focuses on learning meaningful physical velocity fields for dynamic 3D scenes, instead of simply fitting video frames. Although there are a number of dynamic 3D scene datasets in the literature, they are mainly collected for the popular task of novel view synthesis within the training time period, _i.e._, interpolation in time dimension. Besides, the underlying motions of these scenes tend to be chaotic, and estimating their future motions or transferring their motions are hardly meaningful or entertaining in practice. In this regard, we introduce two new synthetic datasets: 1) Dynamic Object dataset, and 2) Dynamic Indoor Scene dataset.

_1) Dynamic Object Dataset:_ This dataset consists of 6 distinct 3D objects, each of which displays a unique motion pattern, including either rigid or deformable movements in 3D space. These 3D objects and their realistic motions are all designed by unknown external practitioners from SketchFab, and we purchased their Licenses and will make them available for free use in the community.

For each 3D object, we collect RGB images at 15 different viewing angles over 1 second, where each viewing angle has 60 frames captured. We reserve the first 45 frames at randomly picked 12 viewing angles as the training split, _i.e._, 540 frames, while leaving the 45 frames at the remaining 3 viewing angles for testing interpolation ability, _i.e._, 135 frames for novel view synthesis within the training time period, and keeping the last 15 frames at all 15 viewing angles for evaluating future frame extrapolation, _i.e._, 225 frames. More details are in Appendix.

_2) Dynamic Indoor Scene Dataset:_ We also collect another synthetic dynamic 3D dataset, which includes 4 indoor scenes with multiple complex 3D objects undergoing different rigid body motions. There are about 4 objects such as tables or chairs in each 3D scene. Basically, such an indoor dataset aims to simulate potential scenarios for robotics or VR applications to understand dynamic 3D scenes.

Since the indoor scene is significantly more challenging, for each 3D scene, we collect RGB images at 30 viewing angles over 1 second, where each viewing angle has 60 frames captured. Similarly, we reserve the first 45 frames at randomly picked 25 viewing angles as the training split, _i.e._, 1125 frames, while leaving the 45 frames at the remaining 5 viewing angles for testing interpolation ability, _i.e._, 225 frames, and keeping the last 15 frames at all 30 viewing angles for evaluating future frame extrapolation, _i.e._, 450 frames. The ground truth object segmentation masks are also collected for evaluating the semantic decomposition capability in Section 4.2. More details are in Appendix.

While existing dynamic 3D scene modeling techniques and the commonly used datasets in literature are mainly designed for novel view rendering/interpolation within the training time period, rather than for extrapolation beyond the training time period, we evaluate our method on two selected scenes from a real-world dataset: NVIDIA Dynamic Scene. It captures real-world dynamic scenes by a static camera rig with 12 cameras. For each scene, we clip 60 frames with reasonable and predictable motions. We reserve the first 46 frames at randomly picked 11 cameras as the training split, _i.e._, 506 frames, while leaving the 46 frames at the remaining 1 camera for testing interpolation ability, _i.e._, 46 frames for novel view synthesis within the training time period, and keeping the last 14 frames at all 12 cameras for evaluating future frame extrapolation, _i.e._, 168 frames.

**Baselines:** We carefully choose three representative groups of methods as our baselines: 1) dense frame optimization method T-NeRF  and NSSF , 2) canonical frame optimization methods D-NeRF  and TiNeuVox , 3) PINN methods T-NeRF\({}_{PINN}\) and HexPlane\({}_{PINN}\). Both methods are adapted by us via integrating a separate velocity field supervised by the same PINN losses as ours.

**Metrics:** For evaluating both interpolation and future frame extrapolation and motion transfer, the standard metrics **PSNR**, **SSIM**, and **LPIPS** scores are reported across testing views. For evaluatingsemantic decomposition, the Average Precision (**AP**), Panoptic Quality (**PQ**) and **F1** scores with an IoU threshold of 0.5, together with the mean Intersection over Union (**mIoU**) scores are reported.

### Evaluation of Future Frame Extrapolation

We first evaluate the extrapolation capability of our framework. In particular, our method and 5 baselines except NSFF are trained from scratch on each of the 6 objects in Dynamic Object dataset, and all methods on each of the 4 scenes in Dynamic Indoor Scene dataset, all in a scene-specific fashion. In total, \((6 6)+(7 4)=64\) models are trained for comparison. The keyframe number \(K\) is set as 16 in our method for Dynamic Object Dataset and 4 for Dynamic Indoor Scene Dataset. As for real-world NVIDIA Dynamic Scene dataset, we evaluate our model and 2 baselines with comparable performance on our own datasets, where the keyframe number \(K\) is set as 4 in our method.

**Analysis:** Table 1 compares all methods regarding the quality of view synthesis for future frame extrapolation. The view synthesis for interpolation is also included for comparison. It can be seen that: 1) our NVFi achieves significantly better results than all baselines on both dynamic datasets, particularly on the critical task of future frame extrapolation, although the strong baseline TiNeuVox shows excellent performance for interpolation. 2) Naively adding physics priors into an existing dynamic NeRF tends to be inferior as shown by T-NeRF\({}_{PINN}\) and HexPlane\({}_{PINN}\). This clearly verifies the effectiveness of our special design of the joint keyframe and interframe optimization strategy. More qualitative results are in Figure 4(a). Table 2 compares our method and two best baselines. It can be seen that, even if our NVFi only gets comparable performance in interpolation, it still achieves the best performance in extrapolation without any performance drop compared with interpolation thanks to the accurate motion predictions. More qualitative results are in Figure 3.

### Evaluation of 3D Semantic Scene Decomposition

Having the well-trained keyframe dynamic radiance field \(f_{}\) and interframe velocity field \(g_{}\) for each 3D scene in the Dynamic Indoor Scene dataset in Section 4.1, naturally, all individual 3D objects such as chairs and tables undergoing different movements are supposed to be automatically discovered, segmented, and tracked without needing any extra object annotations as supervision signals.

In order to achieve this desirable unsupervised object decomposition objective, a naive strategy is to query the velocity values of dense 3D points in space, followed by a velocity clustering module to group points into objects. However, such a strategy fundamentally fails to recognize object shapes but just identifies similar motions, thereby tracking objects is also infeasible. A more elegant strategy is to directly learn an object code \(}\) (usually one-hot) for each 3D point \(\) within the entire 3D scene volume at the initial timestamp \(t=0\), without retraining any neural layer of the well-trained networks \(f_{}\) and \(g_{}\) but just using them. This would clearly allow all dynamic 3D objects in the scene to be segmented and tracked over time. To this end, we simply introduce a simple 4-layer MLP as the

    &  &  \\   &  &  &  &  \\   & PSNR\(\) & SSIM\(\) & LPIPS\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) \\  T-NeRF & 13.163 & 0.709 & 0.353 & 13.818 & 0.739 & 0.324 & 24.944 & 0.742 & 0.336 & 22.242 & 0.700 & 0.363 \\ D-NeRF & 14.158 & 0.697 & 0.352 & 14.660 & 0.737 & 0.312 & 25.380 & 0.766 & 0.300 & 20.791 & 0.692 & 0.349 \\ TiNeuVox & 27.988 & 0.960 & 0.063 & 19.612 & 0.940 & 0.073 & 29.982 & 0.864 & 0.213 & 21.029 & 0.770 & 0.281 \\ T-NeRF\({}_{PINN}\) & 15.286 & 0.794 & 0.293 & 16.189 & 0.835 & 0.230 & 16.250 & 0.441 & 0.638 & 17.290 & 0.477 & 0.618 \\ HexPlane\({}_{PINN}\) & 27.042 & 0.958 & 0.057 & 21.419 & 0.946 & 0.067 & 25.215 & 0.763 & 0.389 & 23.091 & 0.742 & 0.401 \\ NSFF & - & - & - & - & - & - & 29.365 & 0.829 & 0.278 & 24.163 & 0.795 & 0.289 \\ 
**NVFi(Ours)** & **29.027** & **0.970** & **0.039** & **27.594** & **0.972** & **0.036** & **36.075** & **0.877** & **0.211** & **29.745** & **0.876** & **0.204** \\   

Table 1: Quantitative results of all methods for both novel view interpolation and future frame extrapolation on Dynamic Object Dataset and Dynamic Indoor Scene Dataset.

    &  &  \\   &  &  &  &  \\   & PSNR\(\) & SSIM\(\) & LPIPS\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) \\  TiNeuVox & 27.230 & **0.846** & **0.229** & 24.887 & 0.848 & **0.209** & **29.377** & **0.889** & **0.202** & 24.224 & 0.878 & 0.220 \\ HexPlane\({}_{PINN}\) & 25.494 & 0.768 & 0.337 & 24.291 & 0.768 & 0.325 & 24.447 & 0.867 & 0.225 & 23.955 & 0.868 & 0.232 \\ 
**NVFi(Ours)** & **27.276** & 0.840 & 0.235 & **28.269** & **0.855** & 0.220 & 26.999 & 0.848 & 0.227 & **28.654** & **0.896** & **0.208** \\   

Table 2: Quantitative results of our method and baselines on the NVIDIA Dynamic Scene dataset.

semantic scene decomposition network \(h_{}\). It takes a 3D point \(\) as input, directly regressing its object code \(}\), _i.e._, \(}=h_{}()\) which is optimized using the following steps:

* First, given the well-trained keyframe dynamic radiance field \(f_{}\), we uniformly sample dense 3D points at timestamp \(t=0\), obtaining their density values. Only 3D points with sufficiently large densities are kept as valid points for the subsequent steps.
* Second, the valid 3D points are fed into our new object segmentation network \(h_{}\) (initialized but yet to be trained), obtaining their corresponding object codes.
* Third, the valid points are transported to their correspondences at a random timestamp \(t^{}\) using the well-trained velocity field \(g_{}\). Motion vectors of these points from 0 to \(t^{}\) are computed.
* Lastly, given per-point motions, we employ the dynamic rigid consistency and spatial smoothness losses proposed in OGC  to optimize the object segmentation network \(h_{}\).

Once the object segmentation network \(h_{}\) is learned, all dynamic objects at time \(t=0\) are segmented. With the aid of the well-trained velocity field, in any subsequent timestamps, all these identified objects can be naturally tracked. In addition, with the aid of well-trained keyframe dynamic radiance field \(f_{}\), we simply use the accumulated weights computed in volume rendering to combine point object codes along a given light ray, thus rendering accurate object segmentation 2D masks for any camera poses at any given timestamps. More details of the implementation are in Appendix.

We evaluate the semantic scene decomposition ability on the Dynamic Indoor Scene dataset. In particular, we render all 2D object segmentation masks from our network at the 30 viewing angles over 60 frames for all scenes, _i.e._, 7200 2D masks, and then evaluate them against ground truth masks. For a fair comparison, we also train a similar object segmentation network for the baseline D-NeRF  at time \(t=0\), using its learned deformation vectors as supervision signals and tracking signals. Note that, the deformation vectors are converted back as motion vectors. In addition, we include an image-based object segmentation method, the powerful Mask2Former  pre-trained on COCO  dataset, as a fully-supervised baseline. More implementation details are in Appendix.

**Analysis:** As shown in Table 3, we can see that: 1) Our object segmentation performance is superior to D-NeRF , essentially because our velocity field learns better scene dynamics than the deformation field, thus enabling the object segmentation network to be better optimized. 2) We also clearly surpass the powerful pre-trained Mask2Former  on all metrics. The reasons are two-fold. First, we fundamentally rely on motion patterns rather than appearances to discover objects, thus being able to generalize to unseen object types ("Genome") or scenes ("Factory") better than Mask2Former . Second, our learned object field inherently leverages multi-view consistency, thus allowing the segmentation of partially occluded objects. Figure 4(b) shows qualitative results.

### Evaluation of Motion Transfer

We further demonstrate the ability of our model to transfer a well-trained velocity field to another separately trained static scene. All objects in the new scene are expected to undergo the same dynamics as learned in the velocity field. The more accurate the learned velocity field, the more realistic and entertaining the new 3D scene will be, after applying the learned dynamics.

In order to evaluate the performance, we create a new 3D scene, called _Gnome-new_, being similar to the scene _Gnome_ in our Dynamic Indoor Scene dataset. We apply the same dynamics of _Gnome_ on _Gnome-new_, recording 30*60 = 1800 frames as its ground truth observations. To explicitly show the advantage of our learned disentangled velocity field, we separately train a static TensoRF  model for _Gnome-new_ only using its frames at time \(t=0\). Note that, any other NeRF variants are also applicable here. We then pick up the well-trained velocity field of _Gnome_ in Section 4.1, after which we directly apply the learned velocity field on the newly trained static TensoRF model, rendering 30*60 frames for a comparison with the ground truth images. Similarly, we apply the deformation field learned by D-NeRF in Section 4.1 in the same transferring pipeline, rendering 2D images for comparison. More implementation details are in Appendix.

    & PSNR\(\) & SSIM\(\) & LPIPS\(\) \\  D-NeRF  & 16.124 & 0.327 & 0.550 \\
**NVFi(Ours)** & 16.178 & 0.334 & 0.551 \\   

Table 4: Quantitative results of motion transfer on Synthetic Indoor Scene dataset.

    & AP\(\) & PQ\(\) & F1\(\) & mIoU\(\) \\  Mask2Former  & 65.37 & 73.14 & 78.29 & 64.42 \\ D-NeRF  & 57.26 & 46.15 & 59.02 & 46.58 \\
**NVFi(Ours)** & **91.21** & **78.74** & **93.75** & **67.64** \\   

Table 3: Quantitative results of scene decomposition on the Synthetic Indoor Scene dataset.

**Analysis:** Figure 4(c) shows that our method clearly keeps the geometry and appearance of the new static object, thanks to the accurately learned disentangled velocity field, whereas D-NeRF  fails to do so. However, we observe that in Table 4 our advantage is not so significant. This is because the static reconstruction of _Gnome-new_ lacks supervision signals for ground planes occluded by objects, leading to rendering artifacts in these regions when objects are moved away by motion transfer. Our strong ability of motion transfer is further validated by additional experiments on our Dynamic Object Dataset in Appendix.

### Ablation Study

**(1) Without Joint Optimization:** We only use \(_{}\) to train the keyframe dynamic radiance field \(f_{}\), followed by the \((_{}+_{}+_{})\) to separately train the velocity field \(g_{}\).

**(2) Removing Physics Constraints:** The PINN losses \((_{}+_{})\) are removed to train \(g_{}\).

**(3) Choice of Keyframe Number \(K\):** We set the keyframe number \(K\) as 8 and 32, while we choose \(K=16\) in main experiments.

**(4) Reducing the Number of Cameras:** we reduce the number of cameras used in our Dynamic Object datasets to half of the number, _i.e._, 6 cameras, and one quarter of the number, _i.e._, 3 cameras.

Table 5 and Table 6 shows the ablation results for future frame extrapolation on our Dynamic Object dataset. We can see that: 1) The joint keyframe and interframe optimization strategy is critical to enable our method to learn accurate velocity field as well as dynamic radiance field. 2) Once the keyframe number \(K\) becomes larger, the extrapolation capability clearly drops, validating that the dense supervision is inferior to help learn physics velocity overall. 3) The extremely sparse camera views are unlikely to capture sufficient visual information for physical motion learning. More ablation results are in Appendix.

## 5 Conclusion

In this paper, we extend the appealing radiance field to represent dynamic 3D scenes from multi-view videos. Unlike most of the existing methods which focus on novel view synthesis within the training time period, our method learns to disentangle the physical velocity field from the geometry and appearance of 3D scenes by jointly optimizing two neural networks: the keyframe dynamic radiance field and the interframe velocity field. Extensive experiments on three dynamic datasets demonstrate that our framework learns accurate velocity, enabling successful applications in future frame extrapolation, semantic scene decomposition, and motion transfer.

    &  \\   & & PSNR\(\) & SSIM\(\) & LPIPS\(\) \\ 
12 & **27.594** & **0.972** & **0.036** \\
6 & 25.114 & 0.959 & 0.122 \\
3 & 21.370 & 0.917 & 0.084 \\   

Table 6: Quantitative results of ablation studies on Dynamic Object dataset.

Figure 3: Qualitative results of baselines and our method on NVIDIA Dynamic Scene dataset.

    &  &  &  \\   & & & PSNR\(\) & SSIM\(\) & LPIPS\(\) \\  ✓ & ✓ & 16 & **27.594** & 0.972 & **0.036** \\ ✗ & ✓ & 16 & 24.792 & 0.955 & 0.059 \\ ✓ & ✗ & 16 & 25.537 & 0.968 & 0.040 \\ ✓ & ✓ & 8 & 27.490 & **0.974** & **0.036** \\ ✓ & ✓ & 32 & 24.902 & 0.964 & 0.037 \\   

Table 5: Quantitative results of ablation studies on Dynamic Object dataset.

Figure 4: Qualitative results of baselines and our method on the three tasks. More qualitative results can be found in Appendix and our project page: [https://vlar-group.github.io/NVFi.html](https://vlar-group.github.io/NVFi.html)

**Acknowledgements:** This work was supported in part by Research Grants Council of Hong Kong under Grants 25207822 & 15225522.