# Off-policy estimation with adaptively collected data:

the power of online learning

 Jeonghwan Lee

Department of Statistics

The University of Chicago

Chicago, IL 60637

jhlee97@uchicago.edu

&Cong Ma

Department of Statistics

The University of Chicago

Chicago, IL 60637

congm@uchicago.edu

###### Abstract

We consider estimation of a linear functional of the treatment effect from adaptively collected data. This problem finds a variety of applications including off-policy evaluation in contextual bandits, and estimation of the average treatment effect in causal inference. While a certain class of augmented inverse propensity weighting (AIPW) estimators enjoys desirable asymptotic properties including the semi-parametric efficiency, much less is known about their non-asymptotic theory with adaptively collected data. To fill in the gap, we first present generic upper bounds on the mean-squared error of the class of AIPW estimators that crucially depends on a sequentially weighted error between the treatment effect and its estimates. Motivated by this, we propose a general reduction scheme that allows one to produce a sequence of estimates for the treatment effect via online learning to minimize the sequentially weighted estimation error. To illustrate this, we provide three concrete instantiations in (1) the tabular case; (2) the case of linear function approximation; and (3) the case of general function approximation for the outcome model. We then provide a local minimax lower bound to show the instance-dependent optimality of the AIPW estimator using no-regret online learning algorithms.

## 1 Introduction

Estimating a linear functional of the treatment effect is of great importance in both causal inference and reinforcement learning (RL). For instance, in causal inference, one is interested in estimating the average treatment effect (ATE)  or their weighted variants, and in the literature of bandits and RL, one is interested in estimating the expected reward of a target policy . Two main challenges arise when tackling this problem:

* **Off-policy estimation**: Oftentimes, one needs to estimate the linear functional based on observational data collected from a behavior policy. This behavior policy may not match the desired distribution specified by the linear functional ;
* **Adaptive data collection mechanism**: It is increasingly common for observational data to be adaptively collected due to the use of online algorithms (e.g., via contextual bandit algorithms ) in experimental design .

In this paper, we deal with two challenges simultaneously by investigating the estimation of a linear functional of the treatment effect from observational data that are collected adaptively. When the observational data is collected non-adaptively, i.e., in an i.i.d. manner, there is an extensive line of work  investigating the asymptotic and non-asymptotic theory of various estimators. Most notably are the study  that establishes the asymptotic efficiency of a family of semi-parametric estimators, and a more recent study  that undertakes a finite-sampleanalysis which uncovers the importance of a certain weighted \(_{2}\)-norm when estimating the treatment effect. On the other hand, when it comes to adaptively collected data, most prior works  focus on the asymptotic normality of the estimators, and do not discuss the finite-sample analysis of the estimators. In this paper, we aim to fill in this gap.

### Main contributions

More specifically, we make the following three main contributions in this paper:

* First, we present generic finite-sample upper bounds on the mean-squared error of the class of _augmented inverse propensity weighting_ (AIPW) estimators that crucially depends on a sequentially weighted error between the treatment effect and its estimates. This sequentially weighted estimation error demonstrates a clear effect of history-dependent behavior policies;
* Second, motivated by previous finding, we propose a general reduction scheme that allows one to form a sequence of estimates for the treatment effect via online learning to minimize the sequentially weighted estimation error. To demonstrate this, we provide three concrete instantiations in (1) the tabular case; (2) the case of linear function approximation; and (3) the case of general function approximation for the outcome model;
* In the end, we provide a local minimax lower bound to showcase the instance-dependent optimality of the AIPW estimator using no-regret online learning algorithms in the large-sample regime.

### Related works

Off-policy estimation with observational dataOff-policy estimation in observational settings has been a central topic in statistics, operations research, causal inference, and RL. Here, we group a few prominent off-policy estimators into the following three categories: (i) _Model-based estimator_: often dubbed as the _direct method_ (DM), whose key idea is to utilize observational data to learn a regression model that predicts outcomes for each state-action pair, and then average these model predictions . Due to model mis-specification, DM typically has a low variance but might lead to highly biased estimation results. (ii) _Inverse propensity weighting_ (IPW): for the OPE task, IPW uses importance weighting to account for the distribution mismatch between the behavioral policy and the target policy . If the behavioral policy differs significantly from the target policy, then IPW can have an overly large variance (known as the _low overlap_ issue) . Typical remedies for this issue include propensity clipping  or self-normalization . (iii) _Hybrid estimator_: some off-policy estimators (e.g., the doubly-robust (DR) estimator ) combine DM and IPW together to blend their complementary strengths . A key asymptotic results in OPE is that the cross-fitted DR is \(\)-consistent and asymptotically efficient (that is, it attains the lowest possible asymptotic variance), even for the case where nuisance parameters are estimated at rates slower than \(\)-rates . However, these methods still might be vulnerable to the low overlap issue especially for large or continuous action spaces. Thus, there has been a line of recent studies on OPE for large action spaces  and OPE for continuous action space .

Off-policy estimation with adaptively collected dataA recent strand of works studied asymptotic theory of adaptive variants of the IPW and DR estimators (e.g., asymptotic normality, semi-parametric efficiency, and confidence intervals)  for adaptively collected data. However, in adaptive experiments, overlap between the behavioral policies and the target policy can deteriorate since the experimenter shifts the behavioral policies in response to what he/she observes (known as the _drifting overlap_) . It may engender unacceptably large variances of the IPW and DR estimators. To address this large variance problem, there has been a recent strand of works investigating variance reduction strategies for the DR estimator based on shrinking importance weights toward one , local stabilization , and adaptive weighting . Recent studies on policy learning with adaptively collected data  explored the adaptive weighting DR estimator for policy learning. In contrast with the majority of prior works on off-policy estimation with adaptively collected data that focus on asymptotic results, this paper aims at establishing non-asymptotic theory of the problem. While several researchers have been recently explored non-asymptotic results of the problem with an emphasis on uncertainty quantification , we focus on analyses of estimation procedures of the off-policy value. As a majority of existing standard objects for uncertainty quantification, such as a confidence interval (CI), take a very static view of the world (e.g., it holds for a fixed sample size and is not designed for interactive/adaptive data collection procedures), the aforementioned two papers [30; 65] instead study a more suitable statistical tool for such cases called a _confidence sequence_.

## 2 Problem formulation

We first formulate our problem using the language of contextual bandits: let \(\), \(\), and \(\) denote the _context space_, the _action space_, and the _outcome space_, respectively. Denote by \(:=}\) the space of all possible context-action-outcome triples. In an adaptive experiment, one observes \(n\) samples \(\{(X_{i},A_{i},Y_{i}):i[n]\}\) produced by the following data generating procedure [26; 68]: At each stage \(i[n]\),

1. A context \(X_{i}\) is independently sampled from a fixed _context distribution_\(^{*}()()\);
2. There exists a _behavioral policy_\(_{i}^{*}(,):^{i-1}( )\) that selects the \(i\)-th action as \(A_{i}|X_{i},_{i-1}._{i}^{*}(|X_{i}, _{i-1}.)\), where \(_{i}:=(X_{1},A_{1},Y_{1},,X_{i},A_{i},Y_{i})^{i}\) for \(i[n]\). As \(_{i}^{*}(|X_{i},_{i-1}.)\) may depend on previous observations, \(\{(X_{i},A_{i},Y_{i}):i[n]\}\) are no longer i.i.d.;
3. Given a Markov kernel \(^{*}(,):()\), we assume that the outcome is generated according to \(Y_{i}^{*}(|X_{i},A_{i}.)\). Moreover, the conditional mean of the outcome \(Y_{i}\) is specified as \[[Y_{i}|X_{i},A_{i}.]=_{}y ^{*}(y|X_{i},A_{i}.)=^{*}(X_{i},A_{i} ),\] where the function \(^{*}(,):\) is called the _treatment effect_ (in causal inference) or the _reward function_ (in bandit and RL literature). We note that the treatment effect \(^{*}\) is not revealed to the statistician. We also define the conditional variance function \(^{2}(,):[0,+]\) defined by \(^{2}(x,a):=[\{Y-^{*}(X,A) \}^{2}](X,A)=(x,a)]\), which is assumed to satisfy \(^{2}(x,a)<+\) for every state-action pair \((x,a)\).

At this moment, we assume the existence of \(\)-finite base measures \(_{}()\), \(_{}()\), and \(_{}()\) over \(\), \(\), and \(\), resp., such that \(^{*}()_{}()\), \(_{i}^{*}(|x,_{i-1}.)_{ }()\) for every \((x,_{i-1})^{i-1}\) and \(i[n]\), and \(^{*}(|x,a.)_{}()\) for all state-action pairs \((x,a)\). Here, the notation \(\) stands for the _absolute continuity_ of measures. Our main goal is to estimate the _off-policy value_ for any given target evaluation function \(g(,):\) defined as

\[^{*}=(^{*}):=_{X^{*}}[ (g(X,),^{*}(X,))_{_{}}], \]

where \(^{*}:=(^{*},^{*}):=()( ())\) defines our _problem instance_. Throughout the paper, we assume that the propensity scores \(\{_{i}^{*}(X_{i},_{i-1};A_{i}):i[n]\}\) are revealed, where \(_{i}^{*}(x,_{i-1};):=_{i}^{*} (|x,_{i-1}.)}{_{ }}:\).

As we mentioned earlier in Section 1, the estimation problem of a linear functional of the treatment effect \(^{*}\) turns out to be useful in both causal inference and RL in the following sense:

* **Estimation of average treatment effects**: We consider the binary action space \(=\{0,1\}\) equipped with the counting measure. The _average treatment effect_ (ATE) in our problem setting is defined as the linear functional \[:=_{^{*}}[Y_{i}(1)-Y_{i}(0)]= _{X^{*}}[^{*}(X,1)-^{*}(X,0) ].\] Once we take the evaluation function as \(g(x,a)=2a-1\), the ATE boils down to a particular case of the equation (1);
* **Off-policy evaluation (OPE) for contextual bandits**: Assume that a _target policy_\(^{}():()\) is given such that \(^{}(|x.)_{}()\) for every context \(x\). For simplicity, let \(^{}(x,):=^{}( |x.)}{_{}}\) denote the density function of the target policy for each context \(x\). If we take \(g(x,a)=^{}(x,a)\), then the linear functional (1) corresponds to the value of the target policy \(^{}\). This problem has been widely studied in the literature of bandits and RL, known as the _off-policy evaluation_ (OPE).

We conclude this section by introducing notations that will be useful in later sections: let \(_{I}^{i}(^{i})\) denote the law of the sample trajectory \(_{i}\) under the sampling mechanism with a problem instance \(=(,)\). We denote the density function of \(_{I}^{i}(^{i})\) with respect to the base measure \((_{}_{}_{ })^{ i}\) by \(p_{I}^{i}():^{i}_{+}.\) Lastly, we define the \(k\)_-th weighted \(_{2}\)-norm_ for \(k[n]\) as

\[\|\|_{(k)}^{2}:=_{i=1}^{k}_{^{*}}[(X_{i},A_{i})^{2}(X_{i},A_{i} )}{(_{i}^{*})^{2}(X_{i},_{i-1};A_{i}) }] \]

for any function \((,):\), together with the \(k\)_-th weighted \(_{2}\)-space_ by

\[_{(k)}^{2}:=\{(,)( ):\|\|_{(k)}<+ \}.\]

## 3 A class of AIPW estimators and non-asymptotic guarantees

The main objective of this section is to develop a meta-algorithm to tackle the estimation problem of the off-policy value (1), followed by some key rationale of the proposed procedure as a variance-reduction scheme of the standard _inverse propensity weighting_ (IPW) estimator.

### How can we reduce the variance of the IPW estimator?

Akin to , we consider a class of two-stage estimators obtained from simple perturbations of the IPW estimator. Given any collection \(f:=(f_{i}:^{i-1} :i[n])\) of auxiliary functions, we consider the following _perturbed IPW estimator_\(_{n}^{f}():^{n}\):

\[_{n}^{f}(_{n}):=_{i=1}^{n}\{ ,a_{i})y_{i}}{_{i}^{*}(x_{i},_{i-1}; a_{i})}-f_{i}(x_{i},_{i-1},a_{i})+ f_{i} (x_{i},_{i-1},),_{i}^{*}(x_{i},_{i -1};)_{_{k}}\}.\]

For each \(i[n]\), let \(_{i}(^{i-1})\) denote the joint distribution of \((X_{i},_{i-1},A_{i})\) induced by the adaptive data collection procedure described in Section 2. Then, we arrive at the following result whose proof is deferred to Appendix B.1:

**Proposition 3.1**.: _For any collection \(f:=(f_{i} L^{2}(_{i}):i[n])\) of auxiliary deterministic functions, we have \(_{^{*}}[_{n}^{f}(_{n} )]=(^{*})\). Furthermore, if_

\[ f_{i}(x,_{i-1},),_{i}^{*}(x, _{i-1};)_{_{k}}=0,\;(x, _{i-1})^{i-1} \]

_for each \(i[n]\), then_

\[n_{^{*}}[_{n}^{f}( _{n})]=_{X^{*}}[(g(X,), ^{*}(X,))_{_{k}}]+\|\|_{(n)}^{2} \] \[+_{i=1}^{n}_{^{*}}[\{ ,A_{i})^{*}(X_{i},A_{i})}{_{i}^{*} (X_{i},_{i-1};A_{i})}- g(X_{i}, ),^{*}(X_{i},)_{_{k}}-f_{i}(X _{i},_{i-1},A_{i})\}^{2}].\]

From the decomposition (4) of the variance of the perturbed IPW estimate \(_{n}^{f}(_{n})\), one observes that the only term that depends on the collection of auxiliary functions \(f\) is the third term. More importantly, the third term is equal to zero if and only if

\[f_{i}(x,_{i-1},a)=f_{i}^{*}(x,_{i-1},a ):=(x,a)}{_{i}^{*}(x, _{i-1};a)}- g(X_{i},),^{*}(x,) _{_{k}}. \]

The collection of minimizing functions \(f^{*}:=(f_{i}^{*} L^{2}(_{i}):i[n])\) yields the _oracle estimator_\(_{n}^{f^{*}}():^{n}\)

\[_{n}^{f^{*}}(_{n})=_{i=1}^{n} \{,A_{i})\{Y_{i}-^{*}(X_{i},A_{i} )\}}{_{i}^{*}(X_{i},_{i-1};A_{i})}+  g(X_{i},),^{*}(X_{i},)_{ _{k}}\}, \]

whose variance is given by

\[n_{^{*}}[_{n}^{f^{*}}( _{n})]=v_{*}^{2}:=_{X^{*}}[ g(X, ),^{*}(X,)_{_{k}}]+\|\|_{(n )}^{2}. \]```
1: the dataset \(=\{(X_{i},A_{i},Y_{i}):i[n]\}\) and an evaluation function \(g:\).
2: For each step \(i[n]\), we compute an estimate \(_{i}(_{i-1})( )\) of the treatment effect based on the sample trajectory \(_{i-1}\) up to the \((i-1)\)-th step. // Implement Algorithm 2 as a subroutine;
3: Consider the AIPW estimator (a.k.a., the _doubly-robust_ (DR) estimator) \(_{n}^{}():^{n}\): \[_{n}^{}(_{n}):=_{i=1 }^{n}_{i}(_{i}),\] (8) where the objects being averaged are the AIPW scores \(_{i}():^{i}\) is defined by \[_{i}(_{i}):=,a_{i})} {_{i}^{*}(x_{i},_{i-1};a_{i})}\{y_{i}- _{i}(_{i-1})(x_{i},a_{i})\}+ g (x_{i},),_{i}(_{i-1})(x_{i },)_{_{}}.\] (9)
4:return the AIPW estimate \(_{n}^{}(_{n})\).
```

**Algorithm 1** Meta-algorithm: augmented inverse propensity weighting (AIPW) estimator.

### The class of augmented IPV estimators

Since the treatment effect \(^{*}\) is not revealed to the statistician in (6), it is impossible to exactly compute the oracle estimate \(_{n}^{f^{*}}():^{n}\) using only the observational dataset \(_{n}\). Therefore, a natural remedy would be the following two-stage procedure, which is referred to as the _augmented inverse propensity weighting_ (AIPW) estimator or the _doubly-robust_ (DR) estimator [10; 50; 61; 17; 22]: (i) we first compute a sequence of estimates \(\{_{i}(_{i-1})( ):i[n]\}\) of the treatment effect \(^{*}\); and then (ii) we plug-in these estimates to the equation (6) to construct an approximation to the ideal estimate \(_{n}^{f^{*}}(_{n})\). We summarize this two-stage procedure in Algorithm 1.

We pause here to compare our problem setting and algorithms with the most relevant work . We focus on off-policy estimation with adaptively collected data, which is technically more challenging compared to i.i.d. data considered in . In the case with i.i.d. data,  proposed a natural approach to construct a class of two-stage estimators as follows: (a) compute an estimate \(\) of the treatment effect \(^{*}\) utilizing part of the dataset; and (b) substitute this estimate in the equation (6) of the oracle estimator. Note that the authors use the _cross-fitting approach_[5; 6], which allows to make full use of data to maintain efficiency and statistical power of machine learning algorithms for estimation of nuisance parameters while reducing overfitting bias. However, the cross-fitting strategy heavily relies on the i.i.d. nature of the data collection mechanism and therefore one cannot use it in the setting with adaptively collected data. Instead, we construct an estimate \(_{i}\) of the treatment effect \(^{*}\) based on the sample trajectory \(_{i-1}\) at each stage and then substitute these estimates in the equation (6). This is one of main contributions to address the adaptive nature of our data generating mechanism. We will make use of the framework of online learning to construct a sequence of estimates for the treatment effect \(^{*}\).

### Theoretical guarantees of Algorithm 1

In this section, we provide statistical guarantees for the class of AIPW estimators for dealing with the estimation problem of the off-policy value (1). The main result of this section can be summarized as the following non-asymptotic upper bound on the mean-squared error (MSE) of Algorithm 1:

**Theorem 3.1** (Non-asymptotic upper bound on the MSE of the AIPW estimator).: _For any sequence of estimates \(\{_{i}(_{i-1})( ):i[n]\}\) for the treatment effect \(^{*}\), the AIPW estimator (8) has the MSE bounded above by_

\[&_{^{*}}[\{_{n}^ {}(_{n})-(^{*}) \}^{2}]\\ &\{v_{*}^{2}+_{i=1}^{n} [(X_{i},A_{i})\{_{i}( _{i-1})(X_{i},A_{i})-^{*}(X_{i},A_{i} )\}^{2}}{(_{i}^{*})^{2}(X_{i},_{i-1};A_{i} )}]\}. \]

Note that the non-asymptotic upper bound (10) on the MSE for the class of AIPW estimators (8) consists of two terms, both of which have natural interpretations. The first term \(v_{*}^{2}\) corresponds to the optimal variance (7) achievable by the oracle estimator, and the second term

\[_{i=1}^{n}_{^{*}}[(X_{i},A_{i})\{_{i}(_{i-1})(X_{i},A_{i} )-^{*}(X_{i},A_{i})\}^{2}}{(_{i}^{*})^ {2}(X_{i},_{i-1};A_{i})}] \]

measures the average estimation error of the estimates \(\{_{i}(_{i-1})( ):i[n]\}\) of \(^{*}\). Of primary interest to us is a subsequent upper bounding argument based on the MSE bound (10) in the finite sample regime: in particular, to minimize the RHS of (10), one needs to choose a sequence of estimates \(\{_{i}(_{i-1})( ):i[n]\}\) which minimizes the second term (11).

### Reduction to online non-parametric regression

Let us now focus on constructing a sequence of estimates \(\{_{i}(_{i-1})( ):i[n]\}\) of the treatment effect and upper bounding the estimation error (11) in the MSE bound (10). To this end, we borrow ideas from the literature of online non-parametric regression .

To begin with, we consider an \(n\)-round turn-based game between the learner and the environment; see Algorithm 2 for the details. Then, one can readily observe for any \((,):\), we have

\[_{^{*}}[l_{i}()](_{i-1},X_{i },A_{i})] \]

In the current turn-based game, our natural goal is to minimize the learner's static regret against the _best fixed action in hindsight_ belonging to a pre-specified function class \(()\):

\[(n,;):=_{i=1}^{n}l_{i}\{ _{i}(_{i-1})\}-_{} _{i=1}^{n}l_{i}(), \]

where \(\) denotes the learner's online non-parametric regression algorithm that returns a sequence of estimates \(\{_{i}(_{i-1}):i[n]\}\) for the treatment effect. Then, one can establish the following oracle inequality that demystifies a relationship between estimation problem of the off-policy value and the online non-parametric regression protocol. See Appendix B.3 for the proof.

**Theorem 3.2** (Oracle inequality for the class of AIPW estimators).: _The AIPW estimator (8) using the sequence of estimates \(\{_{i}(_{i-1})( ):i[n]\}\) of the treatment effect \(^{*}\) produced by the online non-parametric regression algorithm \(\) enjoys the following upper bound on the MSE:_

\[_{^{*}}[\{_{n}^{}(_{n})-(^{*})\}^{2}] \] \[(v_{*}^{2}+_{^{*}}[(n,;)]+ \{\|-^{*}\|_{(n)}^{2}:\}).\]

A few remarks are in order. Apart from the optimal variance \(v_{*}^{2}\), the RHS of the bound (15) contains two additional terms: (i) the expected regret relative to the number of rounds \(n\), where the expected value is taken over \(_{n}_{^{*}}^{n}()\); and (ii) the approximation error under the \(\|\|_{(n)}\)-norm. Given any fixed function class \(()\), if we consider the large sample size regime, i.e., the sample size \(n\) is sufficiently large, then one can see that the asymptotic variance of the AIPW estimator (8) is asymptotically the same as \(v_{*}^{2}+\{\|-^{*}\|_{(n)}^{2}:\}\), provided that the online non-parametric regression algorithm \(\) exhibits a _no-regret learning dynamics_, i.e., \(_{^{*}}[(n,; )]=o(n)\) as \(n\). Consequently, the AIPW estimator (8) may suffer from an efficiency loss which depends on how well the unknown treatment effect \(^{*}\) can be approximated by a member of the function class \(()\) under the \(\|\|_{(n)}\)-norm. Hence, any contribution to the MSE bound of the AIPW estimator (8) _in addition to_ the efficient variance \(v_{*}^{2}\) primarily relies on the approximation error associated with approximating the treatment effect \(^{*}\) utilizing a provided function class \(\).

### Consequences for particular outcome models

The main goal of this section is to illustrate the consequences of our general theory developed in Section 3 so far for several concrete classes of outcome models. Throughout this section, we consider the case for which \(=[-L,L]\) for some constant \(L(0,+)\), and impose the following condition:

**Assumption 1** (Strict overlap condition).: The likelihood ratios are uniformly bounded by a universal constant \(B(0,+)\), i.e., for every \(i[n]\),

\[|,A_{i})}{_{i}^{*}(X_{i},_{i-1 };A_{i})}| B_{^{*}}^{n} \]

We note that Assumption 1 is often referred to as the _strict overlap condition_ in the literature of causal inference [20; 32; 66; 36; 11]. At this point, we emphasize that Assumption 1 is necessary to produce main consequences of the oracle inequality for the class of AIPW estimators (Theorem 3.2) that we discuss in the ensuing subsections: Theorems 3.3, 3.4, and the arguments throughout Appendix B.6.

#### 3.5.1 Tabular case of the outcome model

We embark on our discussion about the consequences of our theory established in Sections 3.3 and 3.4 for one of the simplest case of the outcome model satisfying the following assumption.

**Assumption 2** (Tabular setting of the outcome model).: The state-action space \(\) is a finite set.

If we compute the gradient of the loss function (14), we have

\[ l_{i}()=(X_{i},A_{i})}{(^{*})^{ 2}(X_{i},_{i-1};A_{i})}\{(X_{i},A_{i} )-Y_{i}\}_{(X_{i},A_{i})},\;^{}, \]

where \(_{(X_{i},A_{i})}^{}\) is the point-mass vector at the \(i\)-th state-action pair in the sample trajectory, i.e., \(_{(X_{i},A_{i})}(x,a):=1\) if \((x,a)=(X_{i},A_{i})\); \(_{(X_{i},A_{i})}(x,a):=0\) otherwise.

```
1:the function class \([-L,L^{}\), the total number of rounds \(n\), and a sequence of learning rates \(\{_{i}(0,+):i[n-1]\}\).
2:We first choose an initial point \(_{1}()\) arbitrarily;
3:for\(i=1,2,,n-1\), do
4: Observe a triple \((X_{i},A_{i},Y_{i})\);
5: Update \(_{i+1}(_{i})\) according to the following OGD update rule: \[_{i+1}(_{i}) =_{}[_{i}(_{i-1} )-_{i} l_{i}\{_{i}(_{i-1} )\}]\] (18) \[=_{}[_{i}(_{i-1} )- g^{2}(X_{i},A_{i})}{(_{i}^{*} )^{2}(X_{i},_{i-1};A_{i})}\{_{i} (_{i-1})-Y_{i}\}_{(X_{i},A_{i})} ],\]

 where \(_{}[]:^{}\) denotes the projection map of \(^{}\) onto the function space \(\).
6:endfor
7:return the sequence of estimates \(\{_{i}(_{i-1}):i[n]\}\) of the treatment effect \(^{*}\).
```

**Algorithm 3** Online gradient descent (OGD) algorithm for the finite state-action space.

Now, it is time to put forward an online contextual learning algorithm aimed at producing a sequence of estimates of \(^{*}\) with a no-regret learning guarantee. For the tabular case, the online non-parametricregression problem can be resolved through standard online convex optimization (OCO) algorithms. In particular, we employ the online gradient descent (OGD) algorithm (see Algorithm 3) as a subroutine of Algorithm 1. By leveraging standard results on regret analysis of OCO algorithms, one can obtain the following regret bound, which guarantees a no-regret learning dynamics of Algorithm 3.

**Theorem 3.3** (Regret guarantee of Algorithm 3).: _Under Assumptions 1 and 2, the OGD algorithm (Algorithm 3) with learning rates_

\[(n,;) 6LB^{2}( )_{}^{n} \]

_where \(():=\{\|\|_{2}:\}\) denotes the diameter of \([-L,L]^{}\)._

See Appendix B.4 for the proof of Theorem 3.3. Combining the regret guarantee (19) of Algorithm 3 together with the MSE bound (15) in Theorem 3.2, one can establish a concrete upper bound on the MSE of the AIPW estimator (8) by utilizing Algorithm 3 to produce a sequence of estimates for the treatment effect \(^{*}\).

#### 3.5.2 Linear function approximation

We next move on to outcome models where the state-action space \(\) can be infinite. We begin with the simplest case: the class of linear outcome functions. Let \((,):^{d}\) be a _known feature map_ such that \(\{\|(x,a)\|_{2}:(x,a) \} 1\), and we consider the functions that are linear in this representation: \(f_{}(,):\), where \(f_{}(x,a):=^{}(x,a)\) for some parameter vector \(^{d}\). Given a radius \(R>0\), we define the function class

\[_{}:=\{f_{}(,)( )::=(_{d};R)}\}, \]

where \((_{d};R)}:=\{^{d}:\|\|_{2} R\}\). With this linear function approximation framework, let us consider the following OCO model: at the \(i\)-th stage,

1. the learner first chooses a point \(_{i}(_{i-1})\);
2. the environment then picks a loss function \(_{i}():\) defined as \[_{i}():=(X_{i},A_{i})}{(_ {i}^{*})^{2}(X_{i},_{i-1};A_{i})}\{Y_{i}-^{}(X_{i},A_{i})\}^{2},\; ,\] (21)

and our goal is to produce a sequence of estimates \(\{_{i}(_{i-1}):=\{}_ {i}(_{i-1})\}^{}_{} :i[n]\}\) for the treatment effect \(^{*}\) after \(n\) rounds of the above-mentioned OCO model which minimizes the learner's regret against the _best fixed action in hindsight_:

\[(n,_{};) =\] \[= _{i=1}^{n}_{i}\{}_{i}( _{i-1})\}-\{_{i=1}^{n}_{i}():\},\]

where \(\) is the learner's OCO algorithm whose output is a sequence \(\{}_{i}(_{i-1}):i[n ]\}\) of parameters. If we compute the gradient of the loss function (21), one has

\[_{}_{i}()=(X_{i},A_{i} )}{(_{i}^{*})^{2}(X_{i},_{i-1};A_{i}) }\{^{}(X_{i},A_{i})-Y_{i}\} (X_{i},A_{i}). \]

For the current linear function approximation setting, we implement the OGD algorithm (Algorithm 4) as a sub-routine of Algorithm 1. By using the same arguments as in Section 3.5.1, one can reproduce the following regret guarantee of Algorithm 4 whose proof is available at Appendix B.5.

**Theorem 3.4** (Regret guarantee of Algorithm 4).: _With Assumption 1, the OGD algorithm (Algorithm 4) with learning rates_

\[(n,_{};) 6B^{2}R(L+R) _{}^{n} \]General function approximationLastly, we demonstrate the consequences of our general theory established in Sections 3.3 and 3.4 for the case of general function approximation: the function class \(([-L,L])\) can be arbitrarily chosen. Our further discussion this case heavily relies on the basic theory of online non-parametric regression from  whose technical details are rather long and complicated. So, we defer our detailed inspection on the case of general function approximation to Appendix B.6.

## 4 Lower bounds: local minimax risk

We turn our attention to a local minimax lower bound for estimating the off-policy value \(^{*}=(^{*})\). Here, we aim at establishing lower bounds that hold uniformly over all estimators that are permitted to know both the propensity scores \(\{_{i}^{*}(X_{i},_{i-1};A_{i}):i[n]\}\) and the evaluation function \(g\). We assume the existence of a constant \(K 1\) and _reference Markov policies_\(\{_{i}:():i[n]\}\) such that \(_{i}(|x.)_{}()\) for \((x,i)[n]\), and

\[_{i}(x,a)}{_{i}^{*}(x, _{i-1};a)} K \]

for all \((x,_{i-1},a)^{i-1}\), where \(_{i}(x,):=_{i} (|x.)}{_{}}: _{+}\) for each context \(x\). Proximity of behavioral policies to certain Markov policies is often assumed under adaptive data collection procedures. For instance, in _Theorem 1_ of , the authors assumed that the sequence of behavior policies is _eventually Markov_; see the equation (8) therein.

### Instance-dependent local minimax lower bounds

Given any problem instance \(^{*}=(^{*},^{*})\) and an error function \(:_{+}\), we consider the following local neighborhoods:

\[(^{*}) :=\{():(\|^{*} .)\};\] \[_{}(^{*}) :=\{(( )):|()(x,a)-(^{*})(x,a)| (x,a),\ (x,a)\},\]

where for any given \(:()\), let \(()(x,a):=_{}y(.y|x,a)\) for each \((x,a)\). Our goal is to lower bound the following _local minimax risk_:

\[_{n}(_{}(^{*})):= _{_{n}():^{n}} (_{_{}(^{*})} _{}[\{_{n}(_{n} )-()\}^{2}]), \]

where \(_{}(^{*}):=(^{*} )_{}(^{*})\). We now specify some assumptions necessary for lower bounding the local minimax risk (26). Prior to this, we introduce a new important notation: given any random variable \(Y^{4}(,,)\) defined on a probability space \((,,)\), its \((2,4)\)_-moment ratio_ is defined as \(\|Y\|_{2 4}:=(Y^{4})}}{ [Y^{2}]}\).

**Assumption 3**.: Let \(h(x):= g(x,),^{*}(x,)_{_{A}}- _{X^{*}}[ g(X,),^{*}(X,)_{ _{A}}]\). We assume that \(H_{2 4}:=\|h\|_{2 4}=_{X^{*}}[h^{4}(X)]} }{_{X^{*}}[h^{2}(X)]}<+\).

We next make an assumption on a lower bound on the _local neighborhood size_:

**Assumption 4**.: The neighborhood function \((,):_{+}\) satisfies the lower bound

\[(x,a)(x,a)}{ {}_{i}(x,a)\|\|_{(n)}} \]

for all \((x,a,i)[n]\).

We note that Assumptions 3 and 4 are analogues of Assumptions (MR) and (LN) considered in , respectively, for the case of adaptively collected data. Under these assumptions, one can prove the following lower bound on the local minimax risk over \(_{}(^{*})\):

**Theorem 4.1**.: _Under Assumptions 3 and 4, the local minimax risk over \(_{}(^{*})\) is lower bounded by_

\[_{n}(_{}(^{*})) (K)^{2}}{n}, \]

_where \((K)>0\) is a universal constant that only depends on the data coverage constant \(K 1\) of the reference Markov policies \(\{_{i}():():i[n]\}\) defined in (25)._

The proof of Theorem 4.1 can be found in Appendix C.1. This result delivers a key message: the term \(^{2}}{n}\) including the sequentially weighted \(_{2}\)-norm is indeed the fundamental limit for estimating the linear functional based on adaptively collected data. Our results can be viewed as a generalization of those developed in  for the case of i.i.d. data.