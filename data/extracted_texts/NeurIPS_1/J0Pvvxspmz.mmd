# SUBP: Soft Uniform Block Pruning for 1\(\times\)N Sparse CNNs Multithreading Acceleration

# SUBP: Soft Uniform Block Pruning for 1\(\)N Sparse CNNs Multithreading Acceleration

Jingyang Xiang\({}^{1}\)  Siqi Li\({}^{1}\)  Jun Chen\({}^{1}\)  Shipeng Bai\({}^{1}\)

Yukai Ma\({}^{1}\)  Guang Dai\({}^{2,3}\)  Yong Liu\({}^{1}\)

\({}^{1}\)APRIL Lab, Zhejiang University, Hangzhou, China

\({}^{2}\)State Grid Corporation of China

\({}^{3}\)SGIT AI Lab, Shaanxi, China

{jingyangxiang,lsq4747,junc,shipengbai,yukaima}@zju.edu.cn

guang.gdai@gmail.com,yongliu@iipc.zju.edu.cn

Corresponding author

###### Abstract

The study of sparsity in Convolutional Neural Networks (CNNs) has become widespread to compress and accelerate models in environments with limited resources. By constraining N consecutive weights along the output channel to be group-wise non-zero, the recent network with 1\(\)N sparsity has received tremendous popularity for its three outstanding advantages: 1) A large amount of storage space saving by a _Block Sparse Row_ matrix. 2) Excellent performance at a high sparsity. 3) Significant speedups on CPUs with Advanced Vector Extensions. Recent work requires selecting and fine-tuning 1\(\)N sparse weights based on dense pre-trained weights, leading to the problems such as expensive training cost and memory access, sub-optimal model quality, as well as unbalanced workload across threads (different sparsity across output channels). To overcome them, this paper proposes a novel _Soft Uniforn Block Pruning_ (SUBP) approach to train a uniform 1\(\)N sparse structured network from scratch. Specifically, our approach tends to repeatedly allow pruned blocks to regrow to the network based on block angular redundancy and importance sampling in a uniform manner throughout the training process. It not only makes the model less dependent on pre-training, reduces the model redundancy and the risk of pruning the important blocks permanently but also achieves balanced workload. Empirically, on ImageNet, comprehensive experiments across various CNN architectures show that our SUBP consistently outperforms existing 1\(\)N and structured sparsity methods based on pre-trained models or training from scratch. Source codes and models are available at [https://github.com/JingyangXiang/SUBP](https://github.com/JingyangXiang/SUBP).

## 1 Introduction

In recent years, convolutional neural networks (CNNs) have achieved great success in image classification , object detection , semantic segmentation  and other fields. The remarkable performance owes to the deeper and wider architectures, also leading to the prohibitively expensive computational cost and colossal memory footprint. Although some efficient architectures are proposed, such as residual connection , inception module , etc., it is still difficult to deploy the state-of-the-art CNNs on the available CPUs embedded devices with limited resource. Therefore, the network pruning is emerging by pruning the redundancy in CNNs. Due to the appealing performance, it has received extensive attention from industry and academia. It is necessary to maintain the model with small size, low memory footprint, low computational cost and high inference speed.

According to the pruning granularity, most existing works about network pruning mainly focus on weight pruning [10; 19; 34] and filter pruning [64; 40; 25; 24; 46]. Weight pruning deletes the weight of filters directly, which may always result in the unstructured sparsity of filters. The expensive memory accesses is also less efficient in saving memory usage and computational cost, since the unstructured model needs a large number of indices to record the positions of the reserved weights and can't take full advantage of _Advanced Vector Extensions_ (AVX). _Single Instruction Multiple Data_ (SIMD) and poorly utilizes memory caches . In contrast, filter pruning tends to prune at the level of filter or even layer. Since filter pruning still preserves the original convolution structure, it enables the model with structured sparsity and more efficient memory usage. Therefore, filter pruning can take full advantage of the high-performance computing library i.e. _Basic Linear Algebra Subprogram_ (BLAS) to achieve apparent acceleration and is more advocated in accelerating the networks. More recently, the development of hardware and operators has given rise to new pruning patterns. The most famous work is N:M fine-grained pruning , where N out of M weights are zeros for every continuous M weights along input channels. It also can be seen as a special pattern of weight pruning. Currently, this pattern achieves acceleration only in the case of 2:4. However, it is impossible to be utilized on other types of devices since the instructions of sparse _Matrix Multiply-Accumulate_ (MMA) are specially designed for NVIDIA Ampere Core . Not to mention, in realistic deployment scenarios, GPUs on mobile and embedded devices are not always accessible, and such GPUs are more difficult to be satisfied.

Above all, how to retain the performance and achieve realistic acceleration on mobile CPUs becomes a challengeable but valuable problem. In order to solve this issue, Lin _et al._ proposed a novel pattern of 1\(\)N weight pruning with its merits in realizing both high-performing accuracy and apparent CPUs acceleration for practical model deployment. The 1\(\)N pruning pattern provides an intermediate granular level for network pruning, which is coarser as compared to the fine-grained weight but finer as compared to the coarse-grained filter. Fig. 1(d) shows an example of 1\(\)N pruning pattern that satisfies N=4, the core distinction lies in that 1\(\)N pruning consists of N consecutive weights along the output channel to be group-wise non-zero. These consecutive weights can be stored continuously in the memory cache and the convolution with the inputs can proceed using a block-wise vectorized operation in parallel thanks to AVX and SIMD. The indices memory of the weight positions can also benefit from _Block Sparse Row_ (BSR) matrix and be greatly saved.

However, Lin _et al._ permanently pruned blocks based on "smaller-norm-less-important" criterion in a non-uniform manner. On the one hand, it reduced the capacity of original model and thus harmed the performance. On the other hand, it left the blocks redundancy untouched and always caused unbalanced workload across threads. What's more, it still followed a traditional pre-training, pruning

Figure 1: Four mainstream types of pruning in the literature. (a) Unstructured weight pruning removes individual weights at arbitrary locations. (b) Structured filter pruning removes entire convolutional filters. (c) N:M weight pruning (2:4 case) requires at most N out of M consecutive weights along input channels to be non-zero. (d)1\(\)N weight pruning (1\(\)4 case) constrains N consecutive weights along the output channel to be group-wise non-zero.

and fine-tuning (PPF) pipeline, which depended on pre-trained model and still suffered from the expensive pre-training burden.

To address the above-mentioned limitations, we propose a novel block pruning approach named _Soft Uniform Block Pruning_ (SUBP) to obtain high accuracy sub-model without PPF pipeline. In contrast to the traditional pruning approaches that non-uniformly and permanently remove blocks, we prune and regrow the blocks uniformly via importance sampling, which allows the pruned blocks to be recovered and balanced workload across threads. From intra-layer's perspective, we propose a new _Block Pruning_ criterion by taking _Angular Rendundancy_ (BPAR) into account. BPAR identifies the angular redundancy between blocks, so we can prune blocks with redundancy, rather than those with "relatively less" importance. With only one training procedure from scratch, our obtained sub-models yield better accuracy than the previous methods under the similar FLOPs constraints.

To sum up, the contributions of this paper are highlighted as follows:

* We propose a novel block pruning approach named _Soft Uniform Block Pruning_ (SUBP) with three appealing characteristics: (1) a periodic block pruning and regrowing technique via importance sampling, (2) a pruning criterion based on angular redundancy across blocks, and (3) a uniform 1\(\)N sparse pattern for multithreading acceleration.
* Our approach trains a uniform 1\(\)N sparse CNNs from scratch, effectively reducing the training cost and achieving better inference latencies in multithreading scenarios, since it circumvents the expensive PPF pipeline and balances workload across threads.
* Extensive experiments on the large ImageNet dataset have demonstrated the effectiveness of our SUBP under different FLOPs constraints. SUBP obtains consistent accuracy improvement across various N and networks, achieving a better trade-off between accuracy and inference latencies. For example, ResNet50 (1\(\)16) model yields 4\(\) FLOPs reduction while still achieving 76.3% top-1 accuracy and suppressing the previous results.

## 2 Related Work

Fully connected operators are widely used in various types of neural networks, and they can be mathematically represented by one matrix multiplication. The natural idea is that all elements of a matrix are not equally important. Removing unimportant elements from the fully connected operators not only reduces the size and the amount of computation, but also potentially improves the generalization performance of the model. Weight pruning (Fig. 1(a)) and filter pruning (Fig. 1(b)) are traditional pruning methods. There are also some special pruning methods that require a high degree of integration with hardware or operators as shown in Fig. 1(c,d). In what follows, we will briefly review some related works.

**Weight Pruning.** Weight pruning is one of the most widely studied model pruning methods, which removes individual weights at any position of the network. The study of weight pruning could date back to optimal brain damage  and optimal surgeon , which prune weights based on the Hessian within the loss function. Previous studies removed unimportant weights by using gradient , momentum , magnitude , _etc._ Han _et al._ proposed to discard the small weights whose values are below the threshold through an iterative method. Recent some works also payed attention to unstructured sparsity in an adaptive training manner. Ding _et al._ gradually zeroed out the redundant weights by categorizing weights into two parts and updating them according to different rules. Frankle _et al._ presented an algorithm to identify subnetworks that were capable of training effectively. Although weight pruning can maintain most of the accuracy of the model with high sparsity, it is difficult to leverage the existing high-efficiency BLAS libraries in practice to accelerate on general hardware due to its irregular weight distribution. Therefore, weight sparsity pruning is rarely used in practical applications.

**Filter Pruning.** Filter pruning gains achieve noticeable speedup on general hardware after pruning. The filter importance and the pruned network structure are the two most important issues widely studied by researchers. Typical works solve the former issue by devising a certain criterion to measure the importance of filters, including output activation , scale factor amplitude of Batch Normalization layer , ranks of feature map , norm or geometric median of filters [24; 25], _etc._. To be specific, he _et al._ leveraged geometric median to prune filters with redundancy. As for the latter, most works are based on rules of thumb [7; 24], or use evolutionary algorithms ,reinforcement learning , meta learning  and other methods [9; 43] to predict the layer-wise sparsity. For instance, he _et al._ applied reinforcement learning to compression and acceleration models on mobile devices automatically. However, filter pruning removes entire convolution filters, which may damage the information learned by the network and cause serious accuracy degradation with a high pruning rate. Therefore, filter pruning is still difficult to apply in practical tasks.

**Special Pruning.** In order to achieve the maximum balance between accuracy and model performance, researchers have proposed various special pruning types [49; 50; 3; 12]. These convolution modes often require the combination of special operators or hardwares. Supported by the NVIDIA Ampere Core, N:M sparsity has gained tremendous attention due to its attractive storage and computation efficiency. Asit _et al._ and Pool _et al._ followed a traditional PPF pipeline to implement N:M sparsity. Although retraining can improve the accuracy of the N:M sparsity network, the pre-training-based approach still incurs expensive training costs, which hinders the deployment of N:M sparsity techniques. To address this, zhou _et al._ proposed a Sparse-refined straight-through estimator (SR-STE) to train N:M sparsity network from scratch. Zhang _et al._ characterized N:M sparsity as a combinatorial problem and assigned each combination a learnable score to obtain the best subset. Apart from these, researchers have also turned their attention to 1\(\)N sparse networks, which can be accelerated on CPUs. Lin _et al._ proposed 1\(\)N pruning pattern for CNNs firstly, which achieved better accuracy and speed trade-off than both weight and filter pruning.

## 3 Methodology

### Uniform 1\(\)N Block Pruning

We start by introducing symbols and notations in this subsection. Without losing generality, we assume that a CNN has \(L\) layers. We parameterize the tensor connection of CNN with \(\{W^{i}^{C_{i+1} C_{i} K_{i}^{h} K_{i}^{w}}|1  i L\}\), where \(C_{i}\), \(C_{i+1}\), \(K_{i}^{h}\) and \(K_{i}^{w}\) represent the numbers of input channels, output channels, kernel height and kernel width of \(i\)-th layer, respectively.

A 1\(\)N pruning pattern partitions the whole \(W^{i}\) into a collection of small blocks. Considering the \(W^{i}\) in the \(i\)-th layer, the 1\(\)N pruning pattern can be achieved by partitioning \(W^{i}\) into a collection of \(C_{i}\) col-groups and then further partitioning \(N_{i}\) into a collection of \(}{N}\) row-groups. Consequently, each block is a 1\(\)N matrix along the input channel, which includes N consecutive output kernel with the same input channel index. We denote the matrix block set as \(\{B_{j,k}^{i}^{N 1 K_{i}^{h} K_{i}^{w}}|1 i  L,0 j}{N}-1,0 k C_{i}-1\}\), to stand for the \(W_{j N(j+1) N,k,:,:}^{i}\). Based on this partition pattern, the basic pruning granularity of 1\(\)N sparsity falls into these blocks. According to network pruning which can be implemented by imposing a mask \(M^{i}\) upon \(W^{i}\), we introduce \(\{M_{j,k}^{i}\{0,1\}|1 i L,0 j}{N}-1,0 k  C_{i}-1\}\), to define the objective function of

Figure 2: An illustration of our SUBP method, which optimizes the weight values and regrows the removed blocks in one training pass from scratch jointly. In SUBP, both retained and regrown blocks are active, participating in the training iterations. After the last iteration, SUBP exports the pruned model and optimized block weights.

pre-existing work:

\[*{arg\,max}_{M^{i}}_{i=1}^{L}(\{B_{j,k}^{i} M_{j,k }^{i}|0 j}{N}-1,0 k C_{i}-1\}),\ s.t.^{i}\|_{0}}{K}=1-p \]

where \(()\) measures the importance of its input, \(K=}{N} C_{i}\) and \(p\) is the expected prune rate for the model.

To reduce the latency of CNNs, multi-core devices are often adopted in most practical scenarios. On a single-core CPU, the most important aspect is the continuity and locality of memory access and calculation in order to fully utilize the cache and vectorization unit. However, nearly all modern processors have multiple cores, and the computation can benefit from multithreading parallelism. Even though the continuity and locality of memory access and calculation are also important to multi-core CPU, the most important aspect of CPU multi-core computing is to achieve balanced workload to fully utilize the computing power of each core and improve computational efficiency. As can be seen from Fig. 3, if the sparsity among different output channel blocks keeps the same, the workload across different threads will be the same.

However, it is easy to know that the conditions in Eq. (1) often lead to different sparsity levels among different output channel blocks, e.g., \(\|M_{1,:}^{i}\|_{0}=1\) and \(\|M_{2,:}^{i}\|_{0}=C_{i}-1\), which will cause workload imbalance  among threads, degrade performance and waste resources during multi-core execution. Therefore, to overcome these issues, we propose a uniform 1\(\)N block pruning type on the basis of the analysis in above, which is equivalent to constraining the sparsity levels among different output channel blocks to be the same. In another word, the uniform 1\(\)N block pruning is a special case of 1\(\)N block pruning. Therefore, we define the objective function of uniform 1\(\)N block pruning as:

\[*{arg\,max}_{M^{i}}_{i=1}^{L}_{j=0}^{}{N}-1} (\{B_{j,k}^{i} M_{j,k}^{i}|0 k C_{i}-1\}),\ s.t.^{i}\|_{0}}{C_{i}}=1-p \]

### Block Pruning via Angular Redundancy

To get rid of the constraints in the norm-based criterion, we propose a new block pruning criterion by taking block angular redundancy into account and name it as BPAR here. The central idea of angular redundancy is as follows: given a non-zero vector \(X^{N M}\) and a non-zero vector \(W^{N 1}\), we have

\[W_{1}^{T}X= W_{2}^{T}X,&<W_{1},W_{2}>=0\\ W_{1}^{T}X=- W_{2}^{T}X,&<W_{1},W_{2}>=,\ =\|_{2}}{ \|W_{2}\|_{2}} \]

where \(<,>\) denotes the angle between two vectors. In particular, we define the \(<,>=0\). According to the above analyses, even though a 1\(\)N block may be relatively important, this block still has an angular redundancy if there exists another block with a similar orientation angle. Similarly, a relatively less important 1\(\)N block can also map the vectors to a meaningful space and extract valuable information if its orientation angle is very different from others.

In order to simplify the explanations, firstly, we vectorize the representation of \(B_{j,k}^{i}^{N 1 K_{i}^{h} K_{i}^{w}}\) as \(_{j,k}^{i}^{N*1*K_{i}^{h}*K_{i}^{w}}\). Then, we compute the importance score \(S\) of our block as:

\[S_{j,k}^{i}=^{i}\|_{1}}{_{m=0}^{C_{i}-1}\|_{j,m} ^{i}\|_{1}}-^{C_{i}-1}CosineSim(_{j,k}^{i },_{j,m}^{i})}{_{n=0}^{C_{i}-1}_{m=0}^{C_{i}-1} CosineSim(_{j,n}^{i},_{j,m}^{i})}, \]

Figure 3: Balanced workload across threads.

where \(\) is a balanced hyper-parameter and \(CosineSim\) denotes cosine distance between two vectors. Based on Eq. (4), it is obvious that unlike previous criterion solely based on norm, our approach might retain some blocks with smaller norm but larger angular differences in comparison with other blocks. Simultaneously, it may discard the blocks with larger norm but smaller angular differences.

### Soft Uniform Block Pruning

Most of the previous pruning [27; 40; 42] works followed a traditional PPF pipeline, which compressed CNNs in a hard manner. However, once filters or weights are pruned, they can not be recovered, which limits the model capacity and leads to unacceptable accuracy degradation.

To rectify the aforementioned limitations, here we propose a novel soft block pruning approach i.e. _Soft **U**niform **B**lock **P**runing_ (SUBP) to obtain high accuracy sub-model without a pre-trained super-model or fine-tuning the pruned model. In contrast to the traditional hard pruning approaches, we recover the pruned blocks through an importance sampling approach, which enables the compressed network to have a large model capacity and thus achieves a higher accuracy than others. As shown in Fig. 2, the details of our SUBP can be divided into two stages in what follows.

**1) Block Pruning Stage:** Firstly, we init all elements in \(M^{i}_{j,k}\) to 1. For the \(i\)-th layer, we use Eq. (4) to evaluate the importance of each block and identify the set of important blocks to retain as:

\[^{i}_{j}=(\{S^{i}_{j,k}|1 k C_{i}\} )^{ C_{i}(1-p_{i})} \]

which gives the indices of blocks with the top \( C_{i}(1-p_{i})\) scores of \(B^{i}_{j}\). Then, we prune the bottom-ranking block by zeroizing \(\{M^{i}_{j,k}|k^{i}_{j}\}\).

**2) Block Regrowing Stage:** To determine the blocks to regrow, we introduce an importance sampling strategy based on the block importance score. We compute the importance of sampling probabilities by \(p^{i}_{j,k}=(_{j,k}}{})/_{m ^{i}_{j}}(_{j,m}}{})\), where the temperature \(\) is to balance the sampling attention between the different blocks. Then we select the regrow block indices by performing importance sampling \(^{i}_{j}=(\{p^{i}_{j,k}|k^{i}_{j} \},_{t}C_{i})\) without replacement based on the regrowing factor \(_{t}\) at \(t\)-th epoch and reset \(\{M^{i}_{j,k}|k^{i}_{j}\}\) to 1.

To compute the regrowing factor \(_{t}\), we employ a gradually schedule  to gradually reduce the regrown blocks so that the subnet can converge to the target uniform 1\(\)N sparsity stably at the end of training. Specially, the regrowing factor at \(t\)-th epoch is computed as:

\[_{t}=1-p,&t t_{s}\\ _{0}(1-}{t_{e}-t_{s}})^{3},&t_{s}<t t_{e}\\ 0,&t_{e}<t \]

where \(_{0}\) is the initial regrowing factor, \(t_{s}\) and \(t_{e}\) denote the start and end of the training epochs in the block pruning-regrowing stage respectively. Since the model is trained from scratch and the pruning and regrowing decision based on the weights may not be sufficient enough, therefore, we don't prune blocks in the early stage of training. When \(t t_{e}\), the blocks will stop regrowing and we finish pruning the block. The remaining blocks are considered to be the best candidates in the end.

## 4 Experiments

### Experiment Settings and Implementation Details

In this section, we evaluate our SUBP on the largescale dataset ImageNet with the representative networks, including ResNet18, ResNet34, ResNet50 and MobileNetV1. We implement SUBP using the PyTorch framework and NVIDIA RTX 3090 GPUs. The SGD optimizer with a momentum of 0.875 and a weight decay of 3e-5 is adopted. We train all compared networks for 250 epochs with a mini-batch size of 512 and an initial learning rate of 0 which is linearly increased to 0.512 during the first 8 epochs and then decayed to 0 by the cosine learning rate schedule. Same as previous methods [18; 45], we also use label smoothing with the factor 0.1 and apply the standard data augmentation. To keep our method simple and generic, the hyper-parameters (\(\), \(\), \(_{0}\), \(t_{s}\), \(t_{e}\)) in above are set to (1.0, 1.0, 0.2, 10, 180) and kept constant in our experiments. By adjusting appropriate parameters for different models and pruning rates, better results should be obtained in general. In this paper, we calculate FLOPs by counting multiplication and addition as one operation as He  did.

### Influence of Pruning Criterion

In this subsection, we will investigate the influence of 1\(\)N pruning criterion. Table 1 shows the compared results with respect to \(_{1}\) norm  and BPAR. As suggested by [37; 48; 51; 28], the pruning criterion selects the appropriate sub-model on the basis of its importance or redundancy, which plays an important role in the traditional PPF approach. Table 1 tells us that our BPAR

    &  &  \\  &  &  \\  Accuracy & Top-1 & Top-5 & Top-1 & Top-5 & Top-1 & Top-5 & Top-1 & Top-5 \\  Origin & 71.15 & 89.83 & 71.15 & 89.83 & 77.00 & 93.65 & 77.01 & 93.65 \\ Weight Pruning & 70.76 & 89.59 & 70.76 & 89.59 & 77.09 & 93.61 & 77.09 & 93.61 \\ Filter Pruning & 65.35 & 86.26 & 65.35 & 86.26 & 75.38 & 92.52 & 75.38 & 92.52 \\  Prune Criterion & \) norm} &  & \) norm} &  \\  Accuracy & Top-1 & Top-5 & Top-1 & Top-5 & Top-1 & Top-5 & Top-1 & Top-5 \\ 
1\(\)2 Pattern & 70.28 & 89.37 & - & - & 76.65 & 93.47 & - & - \\
1\(\)4 Pattern & 70.05 & 89.06 & - & - & 76.51 & 93.24 & - & - \\
1\(\)8 Pattern & 69.91 & 89.08 & **70.55** & **89.42** & 76.15 & 93.13 & **76.70** & **93.51** \\
1\(\)16 Pattern & 69.56 & 88.93 & **70.30** & **89.35** & 76.25 & 93.08 & **76.52** & **93.20** \\
1\(\)32 Pattern & 69.54 & 88.80 & **70.03** & **89.23** & 75.96 & 92.95 & **76.31** & **93.18** \\   

Table 1: Performance comparison of our BPAR prune criterion against \(_{1}\) norm. The experiment is conducted using MobileNetV1 and ResNet50 with the pruning rate \(p=50\%\) on ImageNet dataset. We test our BPAR on three cases, which N is set to 8, 16 and 32 respectively.

  
**Method** & **PT FLOPs** & **SR** & **Top-1** & **Epochs** & **Method** & **PT FLOPs** & **SR** & **Top-1** & **Epochs** \\  ResNet-18 & & & & & & & ResNet-50 & & \\ PFP  & ✓ & 1.27G & 43.8\% & 67.4\% & 270 & & & & SSS  & ✗ & 2.3G & 38.8\% & 71.8\% & 100 \\ SCOP  & ✓ & 1.10G & 39.3\% & 69.2\% & 230 & & & & TAS  & ✗ & 2.3G & 43.5\% & 76.2\% & 240 \\ SFP  & ✓ & 1.04G & 47.6\% & 67.1\% & 200 & & & & GAL  & ✓ & 2.3G & 16.8\% & 72.0\% & 150 \\ FPGM  & ✓ & 1.04G & 47.6\% & 68.4\% & 200 & & & & & \\ DMCP  & ✗ & 1.04G & 17.6\% & 69.0\% & 150 & & & & & \\ CHEX  & ✗ & 1.03G & 38.7\% & 69.6\% & 250 & & & & & \\
**SUBEP(1\(\)16)** & ✗ & 1.03G & 44.1\% & **69.9\%** & 250 & & & & & \\
**SUBEP(1\(\)32)** & ✗ & 1.03G & 44.1\% & **69.7\%** & 250 & & & & & \\  ResNet-34 & & & & & & & & & \\ Taylor & ✓ & 2.8G & 21.1\% & 72.8\% & - & & & & \\ SFP  & ✓ & 2.2G & 49.1\% & 71.8\% & 200 & & & & & \\ FPGM  & ✓ & 2.2G & 49.1\% & 72.5\% & 200 & & & & & \\ GFS  & ✓ & 2.1G & 32.5\% & 72.9\% & 240 & & & & \\ DMC  & ✓ & 2.1G & - & 72.6\% & 490 & & & & \\ NPPA  & ✓ & 2.1G & - & 73.0\% & 390 & & & & & \\ SCOP  & ✓ & 2.0G & 45.6\% & 72.6\% & 230 & & & & \\ CaFeNet  & ✗ & 1.8G & 21.1\% & 73.1\% & 300 & & & & & \\ CHEX  & ✗ & 2.0G & 29.2\% & 73.5\% & 250 & & & & & \\
**SUBP(1\(\)16)** & ✗ & 2.0G & 43.8\% & **73.7\%** & 250 & & & & & \\
**SUBP(1\(\)32)** & ✗ & 2.0G & 43.8\% & **73.6\%** & 250 & & & & & \\  MobileNetV1 & & & & & & & & \\
0.75x & ✗ & 325M & 38.1\% & 68.4\% & - & & & & \\ MetaPune  & ✗ & 1.0G & 53.5\% & 73.4\% & 160 & & & & \\ EagleEye  & ✓ & 1.0G & 69.4\% & 74.2\% & 240 & & & & \\ CafeNet  & ✗ & 1.0G & 52.9\% & 75.3\% & 300 & & & & \\ MetaPruning  & ✓ & 281M & 50.9\% & 70.6\% & 320 & & & & \\
**SUBP(1\(\)16)** & ✗ & 279M & 40.0\% & **70.8\%** & 250 & & & & & \\
**SUBP(1\(\)16)** & ✗ & 279M & 40.0\% & **70.8\%** & 250 & & & & & \\
**SUBP(1\(\)16)** & ✗ & 279M & 40.0\% & **71.1\%** & 250 & & & & & \\    
    &  &  \\  & & & & & & & (p = 50\%) & & & \\  Accuracy & Top-1 & Top-5 & Top-1 & Top-5 & Top-1 & Top-5 & Top-1 & Top-5 \\   Origin & 71.15 & 89.83 & 71.15 & 89.83 & 77.00 & 93.65 & 77.01 & 93.65 \\ Weight Pruning & 70.76 & 89.59 & 70.76 & 89.59 & 77.09 & 93.61 & 77.09 & 93.61 \\ Filter Pruning & 65.35 & 86.26 & 65.35 & 86.26 & 75.38 & 92.52 & 75.38 & 92.52 \\  Prune Criterion & \) norm} &  & \) norm} &  \\  Accuracy & Top-1 & Top-5 & Top-1 & Top-5 & Top-1 & Top-5 & Top-1 & Top-5 \\ 
1\(\)2 Pattern & 70.28 & 89.37 & - & - & 76.65 & 93.47 & - & - \\
1\(\)4 Pattern & 70.05 & 89.06 & - & - & 76.51 & 93.24 & - & - \\
1\(\)8 Pattern & 69.91 & 89.08 & **70.55** & **89.42** & 76.15 & 93.13 & **76.70** & **93.51** \\
1\(\)16 Pattern & 69.56 & 88.93 & **70.30** & **89.35** & 76.25 & 93.08 & **76.52** & **93.20** \\
1\(\)32 Pattern & 69.54 & 88.80 & **70.03** & **89.23** & 75.96 & 92.95 & **76.31** & **93.18** \\   

Table 2: Results of ResNet18, ResNet34, ResNet50 and MobileNetV1 on ImageNet dataset. “PT”: require pre-training. “SR”: sparse ratio.

outperforms the norm-based method on the ImageNet dataset. For MobileNetV1 and ResNet50, our BPAR can achieve consistent accuracy improvements with the same sparsity and inference speedup. On MobileNetV1(1\(\)16), it obtains 0.74% and 0.38% top1-accuracy and top5-accuracy improvement when BPAR is applied. In particular, for pruning a pre-trained MobileNetV1, BPAR achieves better results for N=8 than \(_{1}\) norm for N=2, which indicates 1\(\)N pruning based on BPAR can obtain a better trade-off between accuracy and inference latencies. Similar results can also be observed on the ResNet50 in Table 1. In essence, our BPAR explicitly utilizes the relationship and identifies the mutual angular redundancy between blocks, giving rise to its superior performance.

### Results on ImageNet

To verify the effectiveness of our SUBP, we apply it to the heavyweight CNN model (i.e. ResNet ) with different depths and the lightweight CNN model (i.e. MobileNet) initialized with random weights on ImageNet  dataset. Here, ResNet-18/34/50 and MobileNetV1 are used as the baseline models and have 1.8/3.7/4.1 GFLOPs and 572 MFLOPs.

The results in Table 2 show that SUBP achieves noticeably higher accuracy than the state-of-the-art pruning methods under the same FLOPs constraints. For example, our SUBP on ResNet50(1\(\)16) with 2\(\) FLOPs reduction achieves 77.6% top-1 accuracy, which is 3.1%, 1.6%, 0.7% and 0.2% higher than Taylor, SCOP , CafeNet  and CHEX  respectively. The results on 1\(\)32 sparsity also demonstrate the superiority against the other methods. On the other hand, at the same target accuracy, our SUBP also achieves higher FLOPs reduction. For example, our SUBP achieves 4\(\) FLOPs reduction and 76.0% top-1 accuracy on ResNet50(1\(\)32) compared to the SCOP, which only yields 1.9\(\) FLOPs reduction. We further observe that SUBP also achieves higher accuracy at a fraction of the training cost for MobileNetV1. For instance, SUBP achieves 71.1% top-1 accuracy on MobileNetV1(1\(\)32, 250 epoch) under the 279M FLOPs constraint, which is 0.5% higher than MetaPruing (320 epoch). This is because SUBP can dynamically explore the optimal sub-model in one training pass from scratch, circumventing the expensive PPF cycles.

### Results on Object Detection and Instance Segmentation

To further explore the performance of SUBP in downstream tasks, we conduct experiments on object detection and instance segmentation on the challenging COCO dataset . The results are shown in Table 3 and Table 4. We adopt the classical method Faster RCNN  for object detection and Mask RCNN  for instance segmentation. We use ResNet50 with different sparse ratios and block sizes as backbone. All the experiments are conducted based on MMDetection . Compared to Dense ResNet50, the SUBP can achieve competitive results, which further demonstrates irrobustness and superiority on downstream computer vision tasks.

### SUBP from Pre-trained Models

In order to further investigate the generality property of our approach, we apply SUBP to the model initialized with pre-trained weights. For an equitable comparison with other PPF methods, we adopt the pre-trained ResNet18 provided by the torchvision2 and run SUBP for 120 epochs as the previous one did. The results in Table 5 show that our SUBP still achieves competitive top-1 and top-5 accuracy under the same FLOPs compared to the previous state-of-the-art PPF methods. Furthermore, when ResNet18 is trained for 90+120 epochs, it achieves 69.5% top-1 accuracy, which is only 0.4% lower than training it from scratch for 250 epochs in Table 2.

  
**Model** & **Block Size** & **maxP** & **Mask mAP** \\  F-RCNN-R50(4.1G) & - & 37.4 & M-RCNN-R50(2.0G) & \(1 32\) & 39.2 & 35.4 \\ F-RCNN-R50(2.0G) & \(1 16\) & 38.5 & M-RCNN-R50(1.0G) & \(1 32\) & 37.4 & 35.5 \\ F-RCNN-R50(1.0G) & \(1 16\) & 37.3 & M-RCNN-R50(1.0G) & \(1 16\) & 37.5 & 33.8 \\   

Table 4: Instance segmentation results on COCO.

  
**Model** & **Block Size** & **mAP** \\  F-RCNN-R50(4.1G) & - & 37.4 \\ F-RCNN-R50(2.0G) & \(1 32\) & 38.4 \\ F-RCNN-R50(2.0G) & \(1 16\) & 38.5 \\ F-RCNN-R50(1.0G) & \(1 32\) & 37.1 \\ F-RCNN-R50(1.0G) & \(1 16\) & 37.3 \\   

Table 3: Object detection results on COCO.

### Deployment Efficiency Analysis

To further explore the acceleration capacity with respect to different N and prune rates on CPUs-based platforms, we adopt TVM  to compile the uniform and non-uniform 1\(\)N pruned models. For a fair comparison, we also consider TVM for dense model to obtain baseline latency. We deploy the 1\(\)N pruned models to obtain network latencies on the arm platform of Apple M1 Pro CPU @ 3.20GHz. From Fig. 4, we observe 1\(\)N pruned model achieves noticeable latency reductions across various pruning rates and N. For example, when the pruning rate is greater than 0.25, the inference latencies of different N are all the better than their dense baseline under the single thread scenario. When the thread num is set to 2, the inference speed of different configurations has been improved. Due to the workload imbalance between threads in non-uniform 1\(\)N, it always lags behind its uniform counterpart. From Fig. 4, there are also two points worth noting: 1) the improvement of 1\(\)N is not as significant as a vanilla convolution in a multithreading context; 2) N=16 achieves a faster inference speed than other N on M1 Pro. Therefore, optimizing multithreading inference with 1\(\)N pruning and selecting appropriate N based on suitable platforms are directions that can be further explored.

## 5 Limitation and Discussion

Firstly, our SUBP masks weights throughout the training and still needs dense computations. Therefore, while SUBP has advantages over methods that depend on PPF pipeline, there is still room for improving its training efficiency. Secondly, there are still missing experiments, including applying 1\(\)N sparsity on other types of DNNs like RNN, transformer and other tasks including object detection, natural language processing, _etc._. We will design corresponding high-performance operators to improve the training efficiency of SUBP, and explore the performance of 1\(\)N sparsity in other types of DNNs and tasks to verify its broader applicability in our future work.

Figure 4: Network latency comparison between uniform 1\(\)N sparse against non-uniform and dense model with varying N and prune rates. The experiment is conducted using ResNet18 and set the input shape as (4, 3, 224, 224) on the arm platform of Apple M1 Pro CPU @ 3.20GHz with single thread (left) and two threads (right). Best viewd in colors.

  
**Model** & **Method** & **Params** & **FLOPs** & **Top-1** & **Top-5** & **Epochs** \\   & Baseline & 11.7M & 1.81G & 69.8\% & 89.1\% & 90 \\  & PFP  & 6.6M & 1.27G & 67.4\% & 87.9\% & 90+180 \\  & SCOP  & 7.1M & 1.10G & 69.2\% & 88.9\% & 90+140 \\  & SFP  & 7.1M & 1.04G & 67.1\% & 87.8\% & 100+100 \\  & FPGM  & 7.1M & 1.04G & 68.4\% & 88.5\% & 100+100 \\  & CHEX  & - & 1.04G & 69.2\% & - & 90+120 \\  & SUBP(1\(\)16) & 7.1M & 1.04G & **69.5\%** & **89.0**\% & 90+120 \\  & SUBP(1\(\)32) & 7.1M & 1.04G & **69.3\%** & **88.9**\% & 90+120 \\   

Table 5: ResNet18 starting from the pre-trained models on ImageNet dataset. “Epochs” are reported as: pre-training epochs plus all subsequent training epochs needed to obtain the final pruned model.

Conclusion

Uniform 1\(\)N sparsity is an important technique that allows fast inference on multithreading scenarios under multi-core architecture. This paper proposes soft uniform block pruning, SUBP, to efficiently train a uniform 1\(\)N sparsity network from scratch. SUBP dynamically adjusts the pruning blocks based on a periodic pruning and regrowing process, which prevents the important blocks from being prematurely pruned and keeps the model's capacity. We also present a new block pruning criterion named BPAR, which explicitly considers the mutual angular redundancy between blocks rather than "relatively less" importance only. By proposing a BPAR based block subset selection approach for pruning and an importance sampling based strategy for regrowing, we can obtain a sub-model with high accuracy by end-to-end implementation without pre-training a large model or requiring extra fine-tuning. Extensive experiments have exhibited our method can effectively reduce the FLOPs and inference latencies while achieving superior accuracy over several SOTAs.