# Block Sparse Bayesian Learning: A Diversified Scheme

Yanhao Zhang   Zhihan Zhu   Yong Xia

School of Mathematical Sciences, Beihang University

Beijing, 100191

{yanhaozhang, zhihanzhu, yxia}@buaa.edu.cn

Corresponding author

###### Abstract

This paper introduces a novel prior called Diversified Block Sparse Prior to characterize the widespread block sparsity phenomenon in real-world data. By allowing diversification on intra-block variance and inter-block correlation matrices, we effectively address the sensitivity issue of existing block sparse learning methods to pre-defined block information, which enables adaptive block estimation while mitigating the risk of overfitting. Based on this, a diversified block sparse Bayesian learning method (DivSBL) is proposed, utilizing EM algorithm and dual ascent method for hyperparameter estimation. Moreover, we establish the global and local optimality theory of our model. Experiments validate the advantages of DivSBL over existing algorithms.

## 1 Introduction

Sparse recovery through Compressed Sensing (CS), with its powerful theoretical foundation and broad practical applications, has received much attention . The basic model is considered as

\[=, \]

where \(^{M 1}\) is the measurement (or response) vector and \(^{M N}\) is a known design matrix, satisfying the Unique Representation Property (URP) condition . \(^{N 1}(N M)\) is the sparse vector to be recovered. In practice, \(\) often exhibits transform sparsity, becoming sparse in a transform domain such as Wavelet, Fourier, etc. Once the signal is compressible in a linear basis \(\), in other words, \(=\) where \(\) exhibits sparsity, and \(\) satisfies Restricted Isometry Constants (RIP) , then we can simply replace \(\) by \(\) in (1) and solve it in the same way. Classic algorithms for compressive sensing and sparse regression include Lasso , Sparse Bayesian Learning (SBL) , Basis Pursuit (BP) , Orthogonal Matching Pursuit(OMP) , etc. Recently, there have been approaches that involve solving CS problems through deep learning [8; 9; 10].

However, deeper research into sparse learning has shown that relying solely on the sparsity of \(\) is insufficient, especially with limited samples [11; 12]. Widely encountered real-world data, such as image and audio, often exhibit clustered sparsity in transformed domains . This phenomenon, known as block sparsity, means the sparse non-zero entries of \(\) appear in blocks . Recent years, block sparse models have gained attention in machine learning, including sparse training , adversarial learning , image restoration [16; 14], (audio) signal processing [17; 18] and many other areas. Generally, the block structure of \(\) with \(g\) blocks is defined by

\[=[ x_{d_{1}}}_{_{1}^{T}} {x_{d_{1}+1} x_{d_{1}+d_{2}}}_{_{2}^{T}}+1} x_{N}}_{_{g}^{T}}]^{T}, \]

where \(d_{i}(i=1...g)\) represent the size of each block, which are not necessarily identical. Suppose only \(k(k g)\) blocks are non-zero, indicating that \(\) is block sparse. Up to now, several methods have been proposed to recover block sparse signals. They are mainly divided into two categories.

Block-basedClassical algorithms for processing block sparse scenarios include Group-Lasso [19; 20; 21], Group Basis Pursuit , Block-OMP . Blocks are assumed to be static with a fixed preset size. Furthermore, Temporally-SBL (TSBL)  and Block-SBL (BSBL) [24; 25], based on Bayesian models, provide refined estimation of correlation matrices within blocks. However, they assume elements within one block tend to be either zero or non-zero simultaneously. Although they can estimate intra-block correlation with high accuracy in block-level recovery, they require preset choices of suitable block sizes and patterns, which are too rigid for many practical applications.

Pattern-basedStructOMP  is a pattern-based greedy algorithm allowing structures, which is a generalization of group sparsity. Another classic model named Pattern-Coupled SBL (PC-SBL) [27; 28], does not have a predefined requirement for block size as well. It utilizes a Bayesian model to couple the signal variances. Building upon PC-SBL, Burst PC-SBL, proposed in , is employed for the estimation of mMIMO channels. While pattern-based algorithms address the issue of explicitly specifying block patterns in block-based algorithms, these models provide a coarse characterization of the intra-block correlation, leading to a loss of structural information within the blocks.

In this paper, we introduce a diversified block sparse Bayesian framework that incorporates diversity in both variance within the same block and intra-block correlation among different blocks. Our model not only inherits the advantages of block-based methods on block-level estimation, but also addresses the longstanding issues associated with such algorithms: the diversified scheme reduces sensitivity to a predefined block size or specified block location, hence accommodates general block sparse data. Based on this model, we develop the DivSBL algorithm, and also analyze both the global minimum and local minima of the constrained cost function (likelihood). The subsequent experiments illustrate the superiority of proposed diversified scheme when applied to real-life block sparse data.

## 2 Diversified block sparse Bayesian model

We consider the block sparse signal recovery, or compressive sensing question in the noisy case

\[=+, \]

where \((0,^{-1})\) represents the measurement noise, and \(\) is the precise scalar. Other symbols have the same interpretations as (1). The signal \(\) exhibits block-sparse structure in (2), yet the block partition is unknown. For clarity in description, we assume that all blocks have equal size \(L\), with the total dimension denoted as \(N=gL\). Henceforth, we presume that the signal \(\) follows the structure:

\[= x_{1L}}_{_{1}^{T}} x_{2L}}_{_{2}^{T}} x_{gL}}_{ _{g}^{T}}]^{T}. \]

In Sections 2.1.1 and 5.2, we clarify that this assumption is made without loss of generality. In fact, our algorithm can automatically adjust \(L\) to an appropriate size, expanding or contracting as needed.

### Diversified block sparse prior

The Diversified Block Sparse prior is proposed in the following scheme. Each block \(_{i}^{L 1}\) is assumed to follow a multivariate Gaussian prior

\[p(_{i};\{_{i},_{i}\})= (,_{i}_{i}_{i}), i=1,,g, \]

in which, \(_{i}\) represents the Diversified Variance matrix, and \(_{i}\) represents the Diversified Correlation matrix, with detailed formulations in Sections 2.1.1 and 2.1.2. Therefore, the prior distribution of the entire signal \(\) is denoted as

\[p(;\{_{i},_{i}\}_{i=1}^{g} )=(,_{0}), \]

where \(_{0}=\{_{1}_{1}_{1},_{2}_{2}_{2},,_{g} _{g}_{g}\}\). The dependency in this hierarchical model is shown in Figure.1.

#### 2.1.1 Diversified intra-block variance

We first execute diversification on variance. In (5), \(_{i}\) is defined as

\[_{i}\{},,}\}, \]

and \(_{i}^{L L}\) is a positive definite matrix capturing the correlation within the \(i\)-th block. According to the definition of Pearson correlation, the covariance term \(_{i}_{i}_{i}\) in (5) can be specified as \[_{i}_{i}_{i}=_{i1}&_{i2}^{i} }}&&_{iL}^{i}} {_{iL}}\\ _{i1}^{i}}}&_{i2}&&_{i2L} }}\\ &&&\\ _{L1}^{i}}}&_{L2}^{i}}}&&_{iL}\\ ,\]

where \(_{s}^{i}( s,k=1 L)\) are the elements in correlation matrix \(_{i}\), serving as a visualization of covariance with displayed structural information.

Now it is evident why assuming equal block sizes \(L\) is insensitive. For the sake of clarity, we denote the true size of the \(i\)-th block \(_{i}\) as \(L_{T}^{i}\). As illustrated in Figure 2, when the true block falls within the preset block, the variances \(_{i}\). corresponding to the non-zero positions in \(_{i}\) will be learned as non-zero values through posterior inference, while the variances at zero positions will automatically be learned as zero. When the true block is positioned across the preset block, several blocks of the predefined size \(L\) covered by the actual block will be updated together, and likewise, variances will be learned as either zero or non-zero. In this way, both of the size and location of the blocks will be automatically learned through posterior inference on the variances.

#### 2.1.2 Diversified inter-block correlation

Due to limited data and excessive parameters in intra-block correlation matrices \(_{i}( i)\), previous works correct their estimation by imposing strong correlated constraints \(_{i}=( i)\) to overcome overfitting . Recognizing that correlation matrices among different blocks should be diverse yet still exhibit some correlation, we apply a weak-correlated constraint to diversify \(_{i}\) in the model.

Here we introduce novel weak constraints on \(_{i}\), specifically,

\[(_{i})=() i=1, ,g, \]

where \(:^{L^{2}}\) is the weak constraint function and \(\) is obtained from the strong constraints \(_{i}=( i)\), as detailed in Section 3.2. Weak constraints (8) not only capture the distinct correlation structure but also avoid overfitting issue arising from the complete independence among different \(_{i}\).

Furthermore, the constraints imposed here not only maintain the global minimum property of our algorithm, as substantiated in Section 4, but also effectively enhance the convergence rate of the algorithm. There are actually \(gL(L+1)/2\) constraints in the strong correlated constraints \(_{i}=( i)\), while with (8), the number of constraints significantly decreases to \(g\), yielding acceleration on the convergence rate. The experimental result is shown in Appendix A.

In summary, the prior based on (5), (6), (7) and (8) is defined as **diversified block sparse prior**.

#### 2.1.3 Connections to classical models

Note that the classical Sparse Bayesian Learning models, Relevance Vector Machine (RVM)  and Block Sparse Bayesian Learning (BSBL) , are special cases of our model.

Figure 1: Directed acyclic graph of diversified block sparse hierarchical structure. Except for Measurements (blue nodes), which are known, all other nodes are parameters to estimate.

Figure 2: The gold dashed line shows the preset block, and the black shadow represents the actual position of the block with its true size.

Connection to RVMATaking \(_{i}\) as identity matrix, diversified block sparse prior (6) immediately degenerates to RVM model

\[p(x_{i};_{i})=(0,_{i}), i=1, ,N, \]

which means ignoring the correlation structure.

Connection to BSBLWhen \(_{i}\) is scalar matrix \(}\), the formulation (5) becomes

\[p(_{i};\{_{i},_{i}\})=(,_{i}_{i}), i=1,,g, \]

which is exactly BSBL model. In this case, all elements within a block share common variance \(_{i}\).

### Posterior estimation

By observation model (3), the Gaussian likelihood is

\[p(;)=(,^{- 1}). \]

With prior (6) and likelihood (11), the diversified block sparse posterior distribution of \(\) can be derived based on Bayes' theorem as

\[p(;\{_{i},_{i}\}_{ i=1}^{g},)=(, ), \]

where

\[ =^{T}, \] \[ =(_{0}^{-1}+^{T} )^{-1}. \]

After estimating all hyperparameters in (12), i.e, \(=\{\{}_{i}\}_{i=1}^{g},\{}_{i} \}_{i=1}^{g},\}\), as described in Section 3, the Maximum A Posterior (MAP) estimation of \(\) is formulated as

\[}^{MAP}=}. \]

## 3 Bayesian inference: DivSBL algorithm

### EM formulation

To estimate \(=\{\{_{i}\}_{i=1}^{g},\{_{i}\}_{i=1}^{g},\}\), either Type-II Maximum Likelihood  or Expectation-Maximization (EM) formulation  can be employed. Following EM procedure, our goal is to maximize \(p(;)\), or equivalently \( p(;)\). Defining objective function as \(()\), the problem can be expressed as

\[_{}()=-^{T}_{y}^{ -1}-_{y}, \]

where \(_{y}=^{-1}+_{0}^{T}\). Then, treating \(\) as hidden variable in E-step, we have \(Q\) function as

\[Q()= E_{x|;^{t-1}}[ p(,;)]\] \[= E_{x|;^{t-1}}[ p(; )]+E_{x|y;^{t-1}}[ p(;\{_{i}\}_{i=1}^ {g},\{_{i}\}_{i=1}^{g})]\] \[ Q()+Q(\{_{i}\}_{i=1}^{g},\{_{i}\}_{i= 1}^{g}), \]

where \(^{t-1}\) denotes the parameter estimated in the latest iteration. As indicated in (17), we have divided \(Q\) function into two parts: \(Q() E_{x|y;^{t-1}}[ p(;)]\) depends solely on \(\), and \(Q(\{_{i}\}_{i=1}^{g},\{_{i}\}_{i=1}^{g}) E_{x|y ;^{t-1}}[ p(;\{_{i}\}_{i=1}^{g},\{_{i}\}_{i=1}^{g})]\) only on \(\{_{i}\}_{i=1}^{g}\) and \(\{_{i}\}_{i=1}^{g}\). Therefore, the parameters of these two \(Q\) functions can be updated separately.

In M-step, we need to maximize the above \(Q\) functions to obtain the estimation of \(\). As shown in Appendix B, the updating formula for \(_{ij},_{i}\) can be obtained as follows2:

\[_{ij}=_{ij}^{2}}{(_{ij}^{2}+4 _{ij}-_{ij})^{2}}}, \]\[_{i}=_{i}^{-1}(^{i}+^{i} (^{i})^{T})_{i}^{-1}, \]

where \(_{ij}\) and \(_{ij}\) are expressed as \(_{ij}=[(_{i}^{-1})_{j}. (_{-j}^{i})^{-1}](^{i}+^{i}(^{i})^{T})_{.j},_{ij}=(_{i}^{-1})_{jj}(^{i}+^{i} (^{i})^{T})_{jj},_{-j}^ {i}=\{},,},0,},,}\}\). The update formula of \(\) is derived in the same way as . The learning rule is given by

\[=-\|_{2 }^{2}+(^{T} )}. \]

### Diversified correlation matrices by dual ascent

Now we propose the algorithm for solving the correlation matrix estimation problem satisfying (8). As mentioned in Section 2.1.2, previous studies have employed strong constraints \(_{i}=( i)\), i.e.,

\[=_{i}=_{i=1}^{g}_{i}^{-1}( ^{i}+^{i}(^{i})^{T} )_{i}^{-1}. \]

In diversified scheme, we apply weak-correlated constraints (8) to diversify \(_{i}\). Therefore, the problem of maximizing the \(Q\) function with respect to \(_{i}\) becomes

\[_{i}}{}& Q(\{ _{i}\}_{i=1}^{g},\{_{i}\}_{i=1}^{g})\\ &(_{i})= () i=1,,g, \]

which is equivalent to (in the sense that both share the same optimal solution)

\[_{i}}{}& {1}{2}_{0}+[_{0}^{-1}(+^{T})] \\ &(_{i})= () i=1,,g,\] (P)

where \(\) is already derived in (21). Therefore, by solving (P), we will obtain diversified solution for correlation matrices \(_{i}, i\). The constraint function \(\) can be further categorized into two cases: explicit constraints and hidden constraints.

Explicit constraints with complete dual ascentExplicit functions such as the Frobenius norm, the logarithm of the determinant, etc., are good choices for \(\). An efficient way to solve this constrained optimization is to solve its dual problem (mostly refers to Lagrange dual ). Choosing \(()\) as \(()\), the detailed solution process is outlined in Appendix C. And the update formulas for \(_{i}\) and multiplier \(_{i}\) (dual variable) are given by

\[_{i}^{k+1}=_{i}^{-1}(^{i}+ ^{i}(^{i})^{T})_{i }^{-1}}{1+2_{i}^{k}}, \]

\[_{i}^{k+1}=_{i}^{k}+_{i}^{k}(_{i}^{k}- ), \]

in which \(_{i}^{k}\) represents the step size in the \(k\)-th iteration for updating the multiplier \(_{i}\) (\(i=1,,g\)). Convergence is only guaranteed if the step size satisfies \(_{k=1}^{}_{i}^{k}=\) and \(_{k=1}^{}(_{i}^{k})^{2}<\). In our experiment, we choose a diminishing step size \(1/k\) to ensure the convergence of the algorithm. The procedure, using dual ascent to diversify \(_{i}\), is summarized in Algorithm 2 in Appendix C.

``` Input:\(_{i}\), \(_{0}\), \(_{0}\), \(_{1}\), \(_{2}\), \(_{3}\), \(_{4}\), \(_{5}\), \(_{6}\), \(_{7}\), \(_{8}\), \(_{9}\), \(_{10}\), \(_{11}\), \(the stationary point \((\{_{i}^{k+1}\}_{i=1}^{g},\{_{i}^{k}\}_{i=1}^{g})\) of the Lagrange function under given multipliers \(\{_{i}^{k}\}_{i=1}^{g}\) satisfies:_

\[_{_{i}}Q(\{_{i}^{k+1}\}_{i=1}^{g},\{_{i}\}_ {i=1}^{g})-_{i}^{k}(_{i}^{k+1})=0.\]

_Then there exists a constrained optimization problem with hidden weak constraint \(:^{n^{2}}\):_

\[_{_{i}} Q(\{_{i}\}_{i=1}^{g},\{_{i}\}_{i=1}^{g})\] \[ (_{i})=(), i=1,,g,\]

_such that \((\{_{i}^{k+1}\}_{i=1}^{g},\{_{i}^{k}\}_{i=1}^{g})\) is a KKT pair of the above optimization problem._

Compared to explicit formulation, hidden weak constraints, while ensuring diversification on correlation, significantly accelerate the algorithm's speed by requiring only one-step dual ascent for updating. Here, we set \(()\) as \(()\), actually solving the optimization problem under corresponding hidden constraint \(\). The comparison of computation time between explicit and hidden constraints, proof of Proposition 3.1 and further explanations on hidden constraints are provided in Appendix D.

Considering that it's sufficient to model elements of a block as a first order Auto-Regression (AR) process  in which the intra-block correlation matrix is a Toeplitz matrix, we employ this strategy for \(_{i}\). After estimating \(_{i}\) by dual ascent, we then apply Toeplitz correction to \(_{i}\) as

\[_{i} =([1,r,,r^{L-1}]) \] \[=[1&r&&r^{L-1}\\ &&&\\ r^{L-1}&r^{L-2}&&1],\]

In conclusion, the Diversified SBL (**DivSBL**) algorithm is summarized as Algorithm 1 below.

```
1:Input: Measurement matrix \(\), response \(\), initialized variance \(\), prior's covariance \(_{0}\), noise's variance \(\), and multipliers \(^{0}\). // Refer to Appendix L for initialization sensitivity.
2:Output: Posterior mean \(}^{MAP}\), posterior covariance \(}\), variance \(}\), correlation \(}_{i}\), noise \(\).
3:repeat
4:if\((_{l}).<\) threshold then
5: Prune \(_{l}\). from the model (set \(_{l}=\)). // Zero out small energy for efficiency.
6: Set the corresponding \(^{l}=,^{l}=_{L L}\).
7:endif
8: Update \(_{ij}\) by (18). // Update diversified variance.
9: Update \(\) by (21). // Avoid overfitting.
10: Update \(_{i}\), \(_{i}\) by (23)(24). // Diversified correlation.
11: Execute Toeplitz correction for \(_{i}\) using (25).3
12: Update \(\) and \(\) by (13)(14).
13: Update \(\) using (20).
14:until convergence criterion met
15:\(}^{MAP}=\). // Use posterior mean as estimate.
```

**Algorithm 1** DivSBL Algorithm

## 4 Global minimum and local minima

For the sake of simplicity, we denote the true signal as \(_{}\), which is the sparsest among all feasible solutions. The block sparsity of the true signal is denoted as \(K_{0}\), indicating the presence of \(K_{0}\) blocks. Let \(}(},,})\), \(}(_{1},, _{g})\), thus \(_{0}=}}\). Additionally, we assume that the measurement matrix \(\) satisfies the URP condition . We employed various techniques to overcome highly non-linear structure of \(\) in diversified block sparse prior (5), resulting in following global and local optimality theorems.

### Analysis of global minimum

By introducing a negative sign to the cost function (16), we have the following result on the property of global minimum and the threshold for block sparsity \(K_{0}\).

**Theorem 4.1**.: _As \(\) and \(K_{0}<(M+1)/2L\), the unique global minimum \(}(_{11},, _{gL})^{T}\) yields a recovery \(}\) by (13) that is equal to \(_{true}\), regardless of the estimated \(}_{i}\) (\( i\))._

The proof draws inspiration from  and is detailed in Appendix E. Theorem 4.1 shows that, in noiseless case, achieving the global minimum of variance enables exact recovery of the true signal, provided the block sparsity of the signal adheres to the given upper bound.

### Analysis of local minima

We provide two lemmas firstly. The proofs are detailed in Appendices F and G.

**Lemma 4.2**.: _For any semi-definite positive symmetric matrix \(^{M M}\), the constraint \(_{0}^{T}+^{ -1}\) is convex with respect to \(\) and \(()\)._

**Lemma 4.3**.: \(^{T}_{y}^{-1}=C()=\) _for any constant \(C\), where \(-^{-1}\), \([(^{T}) ](vec(}))\), and \(\) is a vector satisfying \(^{T}=C\)._

It's clear that \(\) is a full row rank matrix, i.e, \(r()=M\). Given the above lemmas, we arrive at the following result, which is proven in Appendix H.

**Theorem 4.4**.: _Every local minimum of the cost function (16) with respect to \(\) satisfies \(||}||_{0}\), irrespective of the estimated \(}_{i}\) (\( i\)) and \(\)._

Theorem 4.4 establishes an upper bound on the sparsity level of any local minimum for the cost function in terms of the parameter \(\). Therefore, together with Theorem 4.1, these results ensure the sparsity of the final solution obtained.

## 5 Experiments

In this section, we compare DivSBL with the following six algorithms:4 1. Block-based algorithms: (1) BSBL, (2) Group Lasso, (3) Group BPDN. 2. Pattern-based algorithms: (4) PC-SBL, (5) StructOMP. 3. Sparse learning (without structural information): (6) SBL. Results are averaged over 100 or 500 random runs (based on computational scale), with SNR ranging from 15-25 dB

    &  &  \\   &  \\  BSBL & 0.0132\(\)0.0069 & 0.9936\(\)0.0034 \\ PC-SBL & 0.0450\(\)0.0188 & 0.9784\(\)0.0090 \\ SBL & 0.0263\(\)0.0129 & 0.9825\(\)0.0062 \\ Group Lasso & 0.0215\(\)**0.0052** & 0.9925\(\)**0.0020** \\ Group BPDN & 0.0378\(\)0.0087 & 0.9812\(\)0.0044 \\ StructOMP & 0.0508\(\)0.0157 & 0.9760\(\)0.0073 \\ DivSBL & **0.0094\(\)**0.0053** & **0.9955\(\)**0.0026** \\    \\  BSBL & 0.0245\(\)0.0125 & 0.9883\(\)0.0047 \\ PC-SBL & 0.0421\(\)0.0169 & 0.9798\(\)0.0082 \\ SBL & 0.0274\(\)0.0095 & 0.9873\(\)0.0040 \\ Group Lasso & 0.0806\(\)0.0180 & 0.9642\(\)0.0096 \\ Group BPDN & 0.0857\(\)0.0173 & 0.9608\(\)0.0096 \\ StrucOMP & 0.0419\(\)0.0123 & 0.9803\(\)0.0061 \\ DivSBL & **0.0086\(\)**0.0041** & **0.9958\(\)**0.0020** \\   

Table 1: Reconstruction error (NMSE) and Correlation (mean\(\)std) for synthetic signals. Our algorithm is marked in, and the best-performing metrics are displayed in **bold**.

Figure 3: The consistency of multiple experiments with homoscedastic signals for (a) NMSE (b) Correlation, and with heteroscedastic signals for (c) NMSE and (d) Correlation.

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

[MISSING_PAGE_FAIL:10]