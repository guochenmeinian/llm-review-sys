# Transfer learning for atomistic simulations using

GNNs and kernel mean embeddings

 John I. Falk

CSML

Istituto Italiano di Tecnologia

Genova, Italy

me@isakfalk.com

Luigi Bonati

Atomistic Simulations

Istituto Italiano di Tecnologia

Genova, Italy

luigi.bonati@iit.it

Pietro Novelli

CSML

Istituto Italiano di Tecnologia

Genova, Italy

pietro.novelli@iit.it

Michele Parrinello

Atomistic Simulations

Istituto Italiano di Tecnologia

Genova, Italy

michele.parrinello@iit.it

Massimiliano Pontil

CSML

Istituto Italiano di Tecnologia

Genova, Italy

massimiliano.pontil@iit.it

###### Abstract

Interatomic potentials learned using machine learning methods have been successfully applied to atomistic simulations. However, accurate models require large training datasets, while generating reference calculations is computationally demanding. To bypass this difficulty, we propose a transfer learning algorithm that leverages the ability of graph neural networks (GNNs) to represent chemical environments together with kernel mean embeddings. We extract a feature map from GNNs pre-trained on the OC20 dataset and use it to learn the potential energy surface from system-specific datasets of catalytic processes. Our method is further enhanced by incorporating into the kernel the chemical species information, resulting in improved performance and interpretability. We test our approach on a series of realistic datasets of increasing complexity, showing excellent generalization and transferability performance, and improving on methods that rely on GNNs or ridge regression alone, as well as similar fine-tuning approaches.

## 1 Introduction

Atomistic simulations have become a pillar of modern science and are pervasively used in many areas of physics, chemistry, and biology. Among these techniques, molecular dynamics plays a prominent role. This method simulates the time evolution of a system of atoms by integrating Newton's equation of motion . The forces acting on the particles are determined from a model for the interactions, called the potential energy surface (PES), on whose accuracy the reliability of the simulation depends. For a long time, the interactions were modeled in a rather empirical, and of course, not very accurate way . A significant step forward was made with the introduction of _ab initio_ molecular dynamics in which the interactions are computed on the fly from accurate electronic structure calculations . This implies solving at every step the Schroedinger equation, typically with the use of some approximation such as the popular Density Functional Theory (DFT) scheme . This approach is much more accurate but at the same time more computationally expensive, limiting the system size (i.e. the number of atoms) and the time scale that can be simulated.

Starting with the work of Behler and Parrinello , machine learning potentials have emerged as promising candidates to alleviate the tension between accuracy and efficiency . They regress thepotential energy, as a function of the atomic positions and the chemical species, on a (large) set of expensive _ab initio_ calculations. Once successful, this strategy results in an _ab initio_-quality model of the potential energy at a fraction of the cost, speeding up simulations by orders of magnitude.

For this procedure to be successful, however, a good representation of the physical system, including its symmetries, is necessary. The use of handcrafted physical descriptors  is often a laborious procedure that limits applicability to systems with few chemical species. In recent years, graph neural networks (GNNs) have proven to be a viable alternative for directly representing the chemical environment, capable of scaling to large datasets with numerous chemical species and encoding symmetries directly in the architecture, e.g., through SE(3)-equivariant layers . Nevertheless, obtaining an accurate model for real-life applications is as of today still a challenging task, requiring high-quality data samples which are scarce and/or very expensive to obtain.

As shown by the recent advancements in large language modeling  and, before that, image classification , fine-tuning a pre-trained large-scale representation of the data provides a highly effective paradigm to solve downstream tasks for which only a handful of data points are available. This approach paved the way for the concept of _foundation models_, at the core of the current generative AI revolution. Mirroring these developments, in this work we leverage the representation power of GNNs trained on large datasets of molecular configurations. In particular, we rely on the Open Catalyst dataset , which contains DFT relaxations for \(\)1.2M catalytic systems, totaling over 260M data points. We show how, by exploiting the availability of this heterogeneous dataset, it is possible to learn interatomic potentials for specific systems taken from realistic chemical applications in a fast and data-efficient manner.

**Contributions**: This paper makes the following contributions:

* We propose a transfer learning algorithm, which we refer to as mean embedding kernel ridge regression (MEKRR), for modeling the potential energy surface of atomic systems. MEKRR combines GNN representations pre-trained on large datasets with fine-tuning via kernel mean embeddings. This combination allows to satisfy the physical symmetries inherent to atomistic systems. Specifically, GNN features take care of roto-translational invariance, while kernel mean embeddings are chosen to satisfy the permutational symmetry.
* We introduce a new kernel function in the context of modeling potential energy surfaces, which exploits chemical species information. This shows superior performance and facilitates monitoring the chemical evolution of the system.
* We demonstrate excellent transferability and generalization performance on increasingly complex datasets. Remarkably, they include configurations sampled out-of-distribution with respect to the GNN pre-trained representation.

**Related work**: There is a long list of relevant works (see also  and references therein) on representing the potential energy surface by machine learning methods. In particular, the first models employed a set of physical descriptors in combination with either neural networks  or kernel methods . Later, it was proposed to model with neural networks the descriptors as well . Recently, graph networks have been used to directly represent physical systems and regress energy and forces . A combination of GNN features and kernel-based methods has been investigated in , but without doing transfer learning from a larger dataset.

In terms of transfer learning, there is a long line of work in transferring deep representations on images, see  and references therein. Current progress in few-shot image classification, where algorithms can adapt quickly to new classification problems, can partially be attributed to adding a preprocessing step where each image is mapped through a meta-learned or pre-trained representation, see e.g. . In particular,  employs kernel ridge regression (KRR) on top of a meta-learned feature map. More recently, the same ideas have been applied to language modeling (see e.g. ), revolutionizing the field. Pre-training strategies have started very recently to emerge also in the context of interatomic potentials, as a way to interpolate across different levels of theory  or to exploit the release of large and heterogeneous datasets such as OC20 .

**Organization**: In Section 2 we specify the machine learning problem we are aiming to solve. In Section 3 we introduce the kernel mean embedding framework together with KRR and in Section 4 we introduce our method MEKRR. In Section 5 we validate our method on a variety of realistic datasets of increasing complexity. Finally, in Section 6 we conclude and outline future directions.

[MISSING_PAGE_FAIL:3]

\(,(x)=_{t=1}^{T}c_{t}(x_{t}),(x)= _{t=1}^{T}c_{t}K(x_{t},x)\). KRR can also be readily extended  to regress the gradient of \(y\) with respect to \(x\), a technique used e.g. in  to concurrently learn potential energies and forces.

In the standard case in which the input space is a subset of an Euclidean space \(^{d}\), many kernels have been designed; see, for instance, 46, Section 4.2. This notwithstanding, kernel functions can be defined on arbitrary input spaces \(\). In the case of atomic configurations, the potential energy is invariant under the permutation of atoms with the same chemical species. It is therefore advantageous to design kernels able to preserve such symmetry. We now introduce the concept of kernel mean embeddings , which allow us to define permutationally-invariant kernels over atomic configurations.

**Kernel mean embeddings** consider the case in which inputs \(x\) are sets of points living in an Euclidean space \(x=\{r_{i}^{d} i n\}\). For example, an atomic configuration is a set containing the position of each atom in the system (\(d=3\)). For any feature map \((r)\) and the corresponding kernel function \(k(r,r^{})=(r),(r^{})\) on points \(r^{d}\) we can define a feature map acting on the whole set \(x\) as

\[(x):=C_{x}_{r_{i} x}(r_{i})\]

with \(C_{x}>0\) a positive normalization constant. We note in passing that the value of \(C_{x}\) depends on the chemical property that we want to learn. Indeed, setting \(C_{x}=1\) returns extensive properties such as the potential energy, while \(C_{x}=1/n\) is appropriate to model intensive ones. This allows us to define kernels on sets \(x\) as

\[K(x,x^{})=(x),(x^{})=C_{x}C_{x^{}} _{r_{i} x\\ r^{}_{j} x^{}}(r_{i}),(r^{}_{ j})=C_{x}C_{x^{}}_{r_{i} x_{j}\\ r^{}_{j} x^{}}k(r_{i},r^{}_{j}). \]

Using the definition of kernel mean embeddings (3) inside the KRR algorithm (2) enables us to learn scalar functions over sets of points. In the following section, we will combine these algorithms together with pre-trained feature maps to define a principled and efficient method to learn potential energy surfaces from data.

## 4 Method

Our method relies on pre-trained GNN representations of chemical environments and uses them as feature maps to define a kernel mean embedding (3) acting on atomic configurations. In practice, we do so by leveraging pre-trained feature maps on the OC20 dataset . Once a kernel based on mean embeddings is defined, we can use KRR to regress the potential energy from sampled data (see Fig. 1 for a diagram of the method). In this respect, we note that our method shares similarities with the idea behind foundation models , where a representation trained on large datasets is transferred to novel settings by means of fine-tuning. While in this work we show the performance of pre-trained representations based on SCN  and SchNet , any other GNN architecture is a perfectly valid choice, and can be used in place of ours without the need of further adjustments.

### GNN representations of chemical environments

SCN and SchNet are instances of graph neural networks (GNNs) [48; 49; 50], a class of architectures designed to learn mappings over graphs. A graph \(=(,)\) is a collection of \(n\) nodes \(\) and edges \(\) between pairs of nodes. Representing the chemical environment with GNNs involves a preprocessing step that turns a configuration \(x\) into a graph by associating each atom to a node and constructing the edges according to the matrix of pairwise distances between atoms. Each node \(i\) is then initialized to a feature vector \(h_{i}\) encoding the chemical species of the atom via a (possibly learnable) embedding layer. Each GNN layer then updates the node features via a nonlinear message-passing scheme

\[h_{i} g_{}(h_{i},_{j(i)}_{}(h_{i},h_{ j})).\]

Here, \((i)\) is the neighborhood of \(i\) i.e. the set of nodes connected to \(i\) through an edge, \(g_{}\) is a learnable vector-valued function such as an MLP and \(_{}\) is a message-passing function. A graph 

[MISSING_PAGE_FAIL:5]

of node features corresponding to atoms of the \(s\)'th chemical species. We then consider a composite kernel

\[K_{}(H(x),H(x^{})):=(1-)KH(x),H(x^{}) +_{s=1}^{S}KH_{s}(x),H_{s}(x^{}). \]

The kernel \(K_{}\) is a direct generalization of the mean embedding kernel (3) where the parameter \(\) allows to interpolate between emphasizing all atomic interactions equally (\(=0\)) and only within-species atomic interactions (\(=1\)).

Solving KRR (4) with the composite kernel (5) is also equivalent to considering an extension of (4) where to each chemical species is given its own weight while encouraging the weights to be close to each other and of small magnitude through a regularizer. Precisely, let \((w_{s})_{s=1}^{S}\) be the set of displacements from a center weight \(w_{0}\) for each chemical species. The weight of a chemical species \(s\) is given by \(w_{s}+w_{0}\) and the potential energy function for a configuration \(x\) takes the form \(_{s=1}^{S} w_{s}+w_{0},(H_{s}(x)\). In Appendix A we provide a full characterization of this extension, highlighting its connection to multi-task learning [57; 58], in which multiple tasks are learned jointly.

## 5 Experiments

In this section, we consider the realistic problem of modeling the potential energy surface of catalytic reactions occurring on metallic surfaces. We evaluate our approach against methods that are representative of the KRR and GNN approaches, testing them on datasets of increasing complexity taken from realistic applications. As is customary in the literature, we employ the root mean squared error (RMSE) normalized by the number of atoms as a metric, to facilitate the comparison between systems of different sizes. We make the code repository available at [https://github.com/IsakFalk/atomistic_transfer_mekrr](https://github.com/IsakFalk/atomistic_transfer_mekrr).

### Baselines and MEKRR

We consider baselines spanning different categories. Firstly we examine supervised learning algorithms trained from scratch on the provided datasets, either through GNNs or kernel methods with hand-crafted physical features. In this category we have Schnet, SCN and GAP. **SchNet** is one of the first GNNs to be applied successfully to chemistry, which uses a radial basis function representation of the interatomic distances. Spherical Channel Networks (**SCN**)  is another GNN and its atom embeddings are a set of spherical functions represented via spherical harmonics. SCN is one of the state-of-the-art models on the OC20 dataset. For both SchNet and SCN we use the codebase of . Finally, Gaussian Approximation Potential (**GAP**) is a kernel-based method that builds a Gaussian Process using the Smooth Overlap of Atomic Positions (SOAP) descriptors , which we use through the QUIP/quippy code base [61; 62].

The second category of baselines concerns transfer learning methods based on the OC20 dataset . In this case we consider the fine-tuning of Schnet (**Schnet-FT**) which is done by keeping the parameters fixed up to the representation used for MEKRR and then optimizing the subsequent layers on the new dataset.

These baselines are tested against our method (**MEKRR**), which uses a kernel mean embedding with Gaussian kernel based on different pre-trained GNN features. The length-scale of the Gaussian kernel is chosen according to the median heuristic . We will denote MEKRR-(SchNet) and MEKRR-(SCN) the variants using Schnet and SCN node features as inputs, respectively.

### Datasets

We first describe the dataset which has been used for the construction of the pre-trained GNN feature map (**OC20**), and then present the system-specific MD datasets where our method is fine-tuned on (**Cu/formate**, **Fe/N\({}_{2}\)**).

**OC20**: The Open Catalyst (OC) 20 is a large dataset of _ab initio_ calculations aimed at estimating adsorption energies on catalytic surfaces. It comprises \(\)250 millions of DFT calculations, generated from over 1.2 million relaxations trajectories of different combinations of molecules and surfaces. In each relaxation, the positions of the molecule and of the surface upper layers are optimized via gradient descent in order to compute the adsorption energy. The adsorbate is selected out of 82 molecules relevant to environmental applications, while, for each of them, up to \(55^{3}\) surfaces are selected, including binary and ternary compounds. We underline that, for each adsorbate-surface pair, the configurational space sampled is very limited, and especially it does not cover out-of-equilibrium and reactive (e.g. bond forming or breaking) events.

We fine-tune the method and test it on two datasets that are representative of reactive catalytic events, obtained by means of molecular dynamics simulations coupled with enhanced sampling methods  to avoid mode collapse into the local minima of the potential energy landscape. Indeed, whereas the OC20 dataset contains short, correlated relaxations toward the nearest equilibrium state, typical catalytic reaction datasets require sampling all local minima (adsorption states of the molecule) and especially reactive events, in which, due to interaction with the surface, bonds can be broken or formed. For this reason, these applications are challenging as they relate to realistic datasets containing mostly reactive events that are outside the distribution of the OC20 dataset. We split all the below datasets into a train, validation, and test set using random splitting of \(60/20/20\).

**Cu/formate**: The first dataset is a collection of molecular dynamics simulations of the dehydrogenation reaction of formate on a copper (Cu) <110> surface , initialized along the reaction path (obtained with the Nudged Elastic Band method ), in which the molecule loses its hydrogen atom upon interaction with the surface.
**Fe/N\({}_{2}\)**: (\(D_{i}\))**: The second dataset consists of molecular dynamics simulations of a nitrogen molecule adsorbing on an iron (Fe) <111> surface at high temperature (\( T=700\) K) and breaking in two nitrogen atoms . A peculiarity of this dataset is that it contains data from different sources (e.g. standard and biased molecular dynamics) and system sizes, allowing us to also assess the transferability of the methods across different conditions. For this reason, we divide it into 4 subsets, denoted with \(D_{i}\):

\(D_{1}:\)**AI-MD**: _Ab initio_ molecular dynamics simulations. The resulting configurations are highly correlated and cover a small portion of the configurational space related to the adsorption process, thus being the closest dataset to the OC20 one. \(D_{2}:\)**AI-METAD**: Here the _ab initio_ MD simulation is accelerated with the help of the metadynamics  technique. This is an importance sampling method that allows rare events to be observed, and thus it has been employed for collecting reactive configurations in the training set . Due to the metadynamics approach, a larger region of configurational space is sampled with respect to \(D_{1}\), allowing one to sample one bond-breaking event. \(D_{3}:\)**AL-METAD**: Dataset built from an active learning procedure using an ensemble of NNs combined with metadynamics. In this simulation, multiple reaction events are observed, covering a wider region of the configurational space and providing a large number of uncorrelated samples. Hence, these configurations are far from those used to pre-train the feature map. \(D_{4}:\): Same as \(D_{3}\) but the surface is composed of 72 atoms (8 layers) to test the transferability across systems of different sizes.

### Interpolating between shared and independent weights

The \(\) parameter in the \(K_{}\) kernel can vary in the range \(\), which are the limiting cases between a shared or independent set of weights for each chemical species. We use cross-validation to set this parameter in practice. To initially fit the regularization parameter \(\) we set \(=0\) and cross-validate \(\{10^{-3},,10^{-9}\}\) using the same datasets. Despite this simple heuristic cross-validation scheme, as we will see, the scheme is effective, which we believe is a strength as it shows that the MEKRR method is simple to tune while still having the strongest performance among the competitors.

In Fig. 2 we show the cross-validation curves for MEKRR-(SchNet), related to the two datasets Cu/formate and Fe/N\({}_{2}\). In the latter case, we perform the cross-validation only on (\(D_{2}\)), which is representative of the family of datasets, and then use the found parameters also for the other datasets. The two plots show different behavior with the optimal \(\) for the Cu/formate dataset occurring around \(10^{-2}\). This means that the potential energy can be well described with shared weights across chemical species together with a small perturbation. Instead, in the Fe/N\({}_{2}\) dataset the optimal \(\) occurs at the boundary leading to a kernel in which the weights for the two species are learned independently.

### Potential energy regression

In this section, we consider the setting of predicting the potential energy surface. We first evaluate the performance of the models in predicting the energy for each of the datasets and then assess their generalization performance through the transfer learning setting, where we train and test on similar but distinct datasets.

**Same-dataset energy prediction**: From Table 1 we see that MEKRR achieves the best performance in all datasets, both when the input features are extracted from SchNet and SCN. We note that, in general, the transfer learning algorithms (SchNet-FT and the two MEKRR variants) outperform the ones trained from scratch, with MEKRR being significantly faster (see Appendix B). Furthermore, it is worth highlighting that our method performs better than the baselines even when it is applied to datasets that are out-of-distribution for the pre-trained feature map. This is particularly evident for \(D_{3}\) and \(D_{4}\) which contain multiple reactive events (bond-breaking) that were never seen in the relaxations composing the OC20 dataset. This demonstrates the ability of the GNN trained on large and heterogeneous datasets to effectively represent chemical environments.

Figure 2: Validation error (RMSE / MAE) of MEKRR-(SchNet) on the Cu/formate and Fe/N\({}_{2}\) (\(D_{2}\)) datasets as a function of \(\) geometrically spaced on a grid from \(0\) to \(1\) with optimal \(\) and error given by a bold orange point. The optimal \(\) for the Cu/formate dataset is positive but close to zero while the optimal \(\) for the Fe/N\({}_{2}\) is found at the boundary at \(1.0\) leading to a hard multi-embedding kernel. We see that tuning the \(\) allows for improved performance in practice and that the multi-weight formulation (5) is practically beneficial.

**Across-dataset energy prediction** Here we evaluate the performance of the algorithms and MEKRR on transferring from different systems in the Fe/N\({}_{2}\) family of datasets. To do this we consider the task of zero-shot transfer learning (see e.g.  and references therein) where we evaluate a model trained on a source dataset \(D_{}\) on a target dataset \(D_{}\). While the two datasets \(D_{}\) and \(D_{}\) may be sampled from arbitrary systems, we consider here systems that share some characteristics as we are evaluating the transfer capability of the models [68; 69]. Due to the ordering of the datasets \(D_{1},,D_{4}\) in increasing complexity on several axes (size, _ab initio_ vs. active sampling, standard vs. biased dynamics, etc.) we consider a transfer from simpler to more complicated systems. Successfully transferring from simpler to more complicated systems has real-world impact as it can alleviate the high computational cost required for labeling via DFT calculations by reducing the number of points. From Table 2 we see that MEKRR-(SCN) has the lowest error in four out of five tasks, while MEKRR-(SchNet) has the lowest error in the remaining task. Furthermore, the relative transferability of MEKRR compared to the other methods even improves as the task becomes harder. To this respect, it is worth noting that \(D_{1},D_{2},D_{3}\) are qualitatively similar, being all composed of 5 layers of Fe and differing for the sampling method used. Instead, the atomic environments contained in \(D_{4}\) are different as they refer to a slab with a different number of layers. This explains the different order of magnitudes in the last two columns. Despite this, MEKRR still performs very well compared to the baselines.

### Leveraging MEKRR beyond supervised learning

In the previous sections, we have shown how MEKRR performs very well on both supervised and transfer learning tasks. The effectiveness comes from the combination of a pre-trained feature map together with the \(K_{}\) kernel. However, this idea is not restricted to supervised learning. We can indeed leverage the similarity measure provided by the kernel for tasks beyond potential energy regression. As a simple example, in Fig. 3 we plot the kernel matrix, when using SchNet as the feature map, of a part of the trajectory of \(D_{2}\) containing an N-N bond breaking event in the cases \(=0\) and \(=1\). In both images we can see a clear structure that highlights at least two distinct states, but with the second heatmap having more signal. We can then use the kernel to perform spectral clustering with two classes, the result is visualized on the top margin of the heatmap along with the time evolution of a physical quantity that signals the N-N bond-breaking. This facilitates a

   Group & Algorithm & _{2}\)} & Cu/formate \\   & & \(D_{1}\) & \(D_{2}\) & \(D_{3}\) & \(D_{4}\) & \\  Supervised & GAP & 0.4 & 2.1 & 3.9 & 4.9 & 2.8 \\  & SchNet & 0.5 & 4.1 & 5.1 & 6.2 & 6.0 \\  & SCN & 0.3 & 5.1 & 7.5 & 7.3 & 2.5 \\  Fine-tune & SchNet-FT & **0.1** & 2.0 & 2.5 & 3.2 & 1.9 \\  Ours & MEKRR-(SchNet) & **0.1** & 1.3 & 2.4 & 3.3 & **1.2** \\  & MEKRR-(SCN) & 0.2 & **0.9** & **1.9** & **2.7** & 1.7 \\   

Table 1: Same-dataset energy prediction, metric being RMSE. The errors are in units of meV/atom. Best performance given by **bold** number in gray cell.

   Group & Algorithm & \(D_{1} D_{2}\) & \(D_{1} D_{3}\) & \(D_{2} D_{3}\) & \(D_{2} D_{4}\) & \(D_{3} D_{4}\) \\  Supervised & GAP & 24.9 & 59.1 & 5.8 & 830 & 888 \\  & SchNet & 13.2 & 15.4 & 6.2 & 93 & 107 \\  & SCN & 22.1 & 29.3 & 9.7 & 139 & 131 \\  Fine-tune & SchNet-FT & 17.6 & 27.3 & 3.7 & 121 & 116 \\  Ours & MEKRR-(SchNet) & 8.0 & 9.3 & 2.9 & **27** & 55 \\  & MEKRR-(SCN) & **7.0** & **6.3** & **2.0** & 40 & **42** \\   

Table 2: Transfer evaluation of algorithms on source to target: \(D_{} D_{}\), metric being RMSE. The errors are in units of meV/atom. Best performance is given by **bold** number in gray cell.

physical interpretation, as the two classes correspond to configurations containing the reactants (the N\({}_{2}\) molecule) and products (two N atoms) of the chemical reaction. Interestingly, the \(=1\) case correlates more closely with the handpicked physical quantity, shown in the top panel. The reason for this is the fact that learning the weights independently gives more weight to chemical species that are under-represented, which typically correspond to adsorbed atoms in surfaces. This allows us to give more weight to the most important actors in catalytic applications.

## 6 Conclusion and future work

In this work, we introduced an approach to model the potential energy surface of atomistic systems. Our method employs GNN representations trained on the large OC20 dataset along with kernel ridge regression, which we tested on two catalytic processes that are outside the distribution of the pre-training dataset. We devised a kernel function incorporating GNN features, blending kernel mean embedding with information related to the atom's chemical species. Our approach outperforms standalone GNN or kernel methods, demonstrating impressive transferability. This suggests promising avenues for transfer learning application in computational chemistry. However, we recognize certain limitations. Firstly, our method is based upon KRR which scales poorly to large scale datasets, although potential ways around this such as random features  or Nystrom approximations  can overcome this limitation. Secondly, although we tested MEKRR on out-of-distribution datasets for representations pre-trained on OC20, we still focused our analysis on catalytic systems similar to those in the dataset. In this regard, it would be interesting to understand the extent to which MEKRR can predict well the chemical properties of generic systems. In addition, it would be important to incorporate forces into the loss function in order to use it in molecular dynamics applications. We believe that addressing these aspects will further improve the impact of this framework in computational chemistry.

CRediT author statement**J. I. Falk**: Conceptualization, Methodology, Software, Investigation, Formal analysis, Writing - Original Draft; **L. Bonati**: Conceptualization, Methodology, Formal analysis, Visualization, Writing - Original Draft; **P. Novelli**: Conceptualization, Methodology, Writing - Original Draft; **M. Parrinello**: Conceptualization, Writing - Review & Editing, Supervision; **M. Pontil**: Conceptualization, Methodology, Writing - Review & Editing, Supervision.

AcknowledgementsWe acknowledge the financial support from the PNRR MUR Project PE000013 CUP J53C22003010006 "Future Artificial Intelligence Research (FAIR)", funded by the European Union - NextGenerationEU, EU Project ELIAS under grant agreement No. 101120237, and the "Joint project TransHypDE_FP3: Reforming ammonia - transport of \(H_{2}\) via derivatives", funded from the German Federal Ministry of Research (BMBF), funding code: 03HY203A-F.

Figure 3: Heatmaps of the \(K_{}\)-SchNet kernel applied to a part of the trajectory of \(D_{2}\) (where a reactive event occurs) and time series of the distance between nitrogen atoms over time \(t\). The cases with \(=0\) and \(=1\) are reported on the left and right, respectively. Using spectral clustering with the two kernels as inputs we label each time-index with one of two classes, with the background color showing the class. Spectral clustering with the multi-weight kernel picks out the reactive event perfectly.