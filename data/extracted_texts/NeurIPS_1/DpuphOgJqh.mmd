# Selectively Sharing Experiences Improves Multi-Agent Reinforcement Learning

Matthias Gerstgrasser

School of Engineering and Applied Sciences

Harvard University

Computer Science Department

Stanford University

matthias@seas.harvard.edu

&Tom Danino  Sarah Keren

The Taub Faculty of Computer Science

Technion - Israel Institute of Technology

tom.danino@campus.technion.ac.il

sarahk@cs.technion.ac.il

###### Abstract

We present a novel multi-agent RL approach, _Selective Multi-Agent Prioritized Experience Relay_, in which agents share with other agents a limited number of transitions they observe during training. The intuition behind this is that even a small number of relevant experiences from other agents could help each agent learn. Unlike many other multi-agent RL algorithms, this approach allows for largely decentralized training, requiring only a limited communication channel between agents. We show that our approach outperforms baseline no-sharing decentralized training and state-of-the art multi-agent RL algorithms. Further, sharing only a small number of highly relevant experiences outperforms sharing all experiences between agents, and the performance uplift from selective experience sharing is robust across a range of hyperparameters and DQN variants.

## 1 Introduction

Multi-Agent Reinforcement Learning (RL) is often considered a hard problem: The environment dynamics and returns depend on the joint actions of all agents, leading to significant variance and non-stationarity in the experiences of each individual agent. Much recent work  in multi-agent RL has focused on mitigating the impact of these. Our work views multi-agent RL more positively, by treating the presence of other agents can also be an asset that can be leveraged. In particular, we show that groups of agents can collaboratively explore the environment more quickly than individually.

To this end, we present a novel multi-agent RL approach that allows agents to share a small number of experiences with other agents. The intuition is that if one agent discovers something important in the environment, then sharing this with the other agents should help them learn faster. However, it is crucial that only important experiences are shared - we will see that sharing all experiences indiscriminately will not improve learning. To this end, we make two crucial design choices: Selectivity and priority. Selectivity means we only share a small fraction of experiences. Priority is inspired by a well-established technique in single-agent RL, _prioritized experience replay_ (PER) . With PER an off-policy algorithm such as DQN  will sample experiences not uniformly, but proportionally to "how far off" the current policy's predictions are in each state, formally the _temporal difference (td) error_. We use this same metric to prioritize which experiences to share with other agents.

We dub the resulting multiagent RL approach "Selective Multi-Agent Prioritized Experience Relay" or "SUPER". In this, agents independently use a DQN algorithm to learn, but with a twist: Each agent relays its highest-td-error experiences to the other agents, who insert them directly into their replay buffer, which they use for learning. This approach has several advantages:1. It consistently leads to faster learning and higher eventual performance, across hyperparameter settings.
2. Unlike many "centralized training, decentralized execution" approaches, the SUPER learning paradigm allows for (semi-) decentralized training, requiring only a limited bandwidth communication channel between agents.
3. The paradigm is agnostic to the underlying decentralized training algorithm, and can enhance many existing DQN algorithms. SUPER can be used together with PER, or without PER.1

In addition to the specific algorithm we develop, this work also introduces the paradigm of "decentralized training with communication". This is a'middle ground between established approaches of decentralized and centralized training (including "centralized training, decentralized execution").

In the remainder of the paper, we will discuss related literature and technical preliminaries; introduce our novel algorithm in detail; describe experimental evaluation and results; and suggest avenues for future work.

## 2 Related Work

**Multi-Agent RL** approaches in the literature can be broadly categorized according to the degree of awareness of each learning agent of the other agents in the system . On one end of this spectrum is independent or decentralized learning where multiple learning agents (in the same environment) are unaware or agnostic to the presence of other agents in the system [36; 19]. From the perspective of each agent, learning regards the other agents as part of the non-stationary environment. At the opposing end of the spectrum, in centralized control a single policy controls all agents. In between these two, a number of related strands of research have emerged.

Approaches nearer the decentralized end of the spectrum often use communication between the agents [46; 9; 13]. Several forms of cooperative communication have been formulated, by which agents can communicate various types of messages, either to all agents or to specific agent groups through dedicated channels [17; 33; 26; 27]. While communication among agents could help with coordination, training emergent communication protocols also remains a challenging problem; recent empirical results underscore the difficulty of learning meaningful emergent communication protocols, even when relying on centralized training . Related to this, and often used in conjunction, is modeling other agents [2; 16; 10; 44], which equips agent with some model of other agent's behavior.

This in turn is related to a number of recent approaches using centralized training but decentralized execution. A prevailing paradigm within this line of work assumes a training stage during which a shared network (such as a critic) can be accessed by all agents to learn decentralised (locally-executable) agent policies [23; 1; 28; 10; 29]. These approaches successfully reduce variance during training, e.g. through a shared critic accounting for other agents' behavior, but rely on joint observations and actions of all agents. Within this paradigm, and perhaps closest related to our own work,  introduces an approach in which agents share (all of their) experiences with other agents. While this is based on an on-policy actor-critic algorithm, it uses importance sampling to incorporate the off-policy data from other agents, and the authors show that this can lead to improved performance in sparse-reward settings. Our work also relies on sharing experiences among agents, but crucially relies on selectively sharing only some experiences. 2

A separate but related body of work is on transfer learning . While this type of work is not directly comparable, conceptually it uses a similar idea of utilizing one agent's learning to help another agent.

**Off-Policy RL** The approach we present in this paper relies intrinsically on off-policy RL algorithms. Most notable in this class is DQN , which achieved human-level performance on a wide variety of Atari 2600 games. Various improvements have been made to this algorithm since then, includingdueling DQN , Double DQN (DDQN) , Rainbow  and Ape-X . Prioritized Experience Replay  improves the performance of many of these, and is closely related to our own work. A variant for continuous action spaces is DDPG . Off-policy actor-critic algorithms [12; 21] have also been developed, in part to extend the paradigm to continuous control domains.

## 3 Preliminaries

**Reinforcement learning (RL)** deals with sequential decision-making in unknown environments . _Multi-Agent Reinforcement Learning_ extends RL to multi-agent settings. A common model is a _Markov game_, or _stochastic game_ defined as a tuple \( S,,,,\) with states \(S\), _joint actions_\(=\{A^{i}\}_{i=1}^{n}\) as a collection of action sets \(A^{i}\), one for each of the \(n\) agents, \(=\{R^{i}\}_{i=1}^{n}\) as a collection of reward functions \(R^{i}\) defining the reward \(r^{i}(a_{t},s_{t})\) that each agent receives when the joint action \(a_{t}\) is performed at state \(s_{t}\), and \(\) as the probability distribution over next states when a joint action is performed. In the partially observable case, the definition also includes a joint observation function, defining the observation for each agent at each state. In this framework, at each time step an agent has an _experience_\(e=<S_{t},A_{t},R_{t+1}S_{t+1}>\), where the agent performs action \(A_{t}\) at state \(S_{t}\) after which reward \(R_{t+1}\) is received and the next state is \(S_{t+1}\). We focus here on decentralized execution settings in which each agent follows its own individual policy \(_{i}\) and seeks to maximize its discounted accumulated return. The algorithm we propose in this paper further requires that the Markov game is "anonymous", meaning all agents have the same action and observation spaces and the environment reacts identically to their actions. This is needed so that experiences from one agent are meaningful to another agent as-is. Notice however that this does not imply that it is necessarily optimal for all agents to adopt identical behavior.

Among the variety of RL solution approaches [35; 34], we focus here on _value-based methods_ that use state and action estimates to find optimal policies. Such methods typically use a value function \(V_{}(s)\) to represent the expected value of following policy \(\) from state \(s\) onward, and a \(Q\)_-function_\(Q_{}(s,a)\), to represent for a given policy \(\) the expected rewards for performing action \(a\) in a given state \(s\) and following \(\) thereafter.

Q-learning  is a _temporal difference_ (td) method that considers the difference in \(Q\) values between two consecutive time steps. Formally, the update rule is \(Q(S_{t},A_{t}) Q(S_{t},A_{t})+[R_{t+1}+_{a^{}} Q(S_{t+1},a^{})-Q(S_{t},A_{t}))]\), where, \(\) is the _learning rate_ or _step size_ setting the amount by which values are updated during training. The learned action-value function, Q, directly approximates \(q^{*}\), the optimal action-value function. During training, in order to guarantee convergence to an optimal policy, Q-learning typically uses an \(\)-greedy selection approach, according to which the best action is chosen with probability \(1-\), and a random action, otherwise.

Given an experience \(e_{t}\), the _td-error_ represents the difference between the Q-value estimate and actual reward gained in the transition and the discounted value estimate of the next best actions. Formally,

\[(e_{t})=|R+_{a^{}}Q(S^{},a^{})-Q(S,A)| \]

In the vanilla version of Q-learning Q-values are stored in a table, which is impractical in many real-world problem due to large state space size. In deep Q-Learning (DQN) , the Q-value table is replaced by a function approximator typically modeled using a neural network such that \(Q(s,a,) Q^{*}(s,a)\), where \(\) denotes the neural network parameters.

**Replay Buffer (RB) and Prioritized RB:** An important component in many off-policy RL implementations is a replay buffer , which stores past experiences. Evidence shows the replay buffer to stabilize training of the value function for DQN [24; 25] and to reduce the amount of experiences required for an RL-agent to complete the learning process and achieve convergence .

Initial approaches that used a replay buffer, uniformly sampled experiences from the buffer. However, some transitions are more effective for the learning process of an RL agents than others . _Prioritized Experience Replay (PER)_ explores the idea that replaying and learning from some transitions, rather than others, can enhance the learning process. PER suggests replacing the standard sampling method, where transitions are replayed according to the frequency they were collected from the environment, with a td-error based method, where transitions are sampled according to the value of their td-error.

As a further extension, **stochastic prioritization** balances between strictly greedy prioritization and uniform random sampling. Hence, the probability of sampling transition \(i\) is defined as:

\[P(i)=^{}}{_{k}p_{k}^{}} \]

where \(p_{i}>0\) is the priority associated to transition \(i\) and \(\) determines the weight of the priority (\(=0\) is uniform sampling). The value of \(p_{i}\) is determined according to the magnitude of the td-error, such that \(p_{i}=|_{i}|+\), \(_{i}\) is the td-error and \(\) is a small positive constant that guarantees that transitions for which the td-error is zero have a non-zero probability of being sampled from the buffer.

## 4 SUPER: Selective Multi-Agent Prioritized Experience Relay

Our approach is rooted in the same intuition as PER: that not all experiences are equally relevant. We use this insight to help agents learn by sharing between them only a (small) number of their most relevant experiences. Our approach builds on standard DQN algorithms, and adds this experience sharing mechanism between collecting experiences and performing gradient updates, in each iteration of the algorithm:

1. (DQN) Collect a rollout of experiences, and insert each agent's experiences into their own replay buffer.
2. (SUPER) Each agent shares their most relevant experiences, which are inserted into all the other agents' replay buffers.
3. (DQN) Each agent samples a minibatch of experiences from their own replay buffer, and performs gradient descent on it.

Steps 1 and 3 are standard DQN; we merely add an additional step between collecting experiences and learning on them. As a corollary, this same approach works for any standard variants of steps 1 and 3, such as dueling DQN , DDQN , Rainbow  and other DQN improvements. Algorithm 1 in the appendix gives a more detailed listing of this algorithm. Notice that the only interaction between the agents training algorithms is in the experience sharing step. This means that the algorithm can easily be implemented in a decentralized manner with a (limited) communications channel.

### Experience Selection

We describe here three variants of the SUPER algorithm, that differ in how they select experiences to be shared.

Deterministic Quantile experience selectionThe learning algorithm keeps a list \(l\) of the (absolute) td-errors of its last \(k\) experiences (\(k=1500\) by default). For a configured bandwidth \(\) and a new experience \(e_{t}\), the agent shares the experience if its absolute td-error \(|(e_{t})|\) is at least as large as the \(k*\)-largest absolute td-error in \(l\). In other words, the agent aims to share the top \(\)-quantile of its experiences, where the quantile is calculated over a sliding window of recent experiences.

\[|(e_{t})|_{}(\{e_{t^{}}\}_{t^{ }=t-k}^{t})\]

Deterministic Gaussian experience selectionIn this, the learning algorithm calculates the mean \(\) and variance \(^{2}\) of the (absolute) td-errors of the \(k\) most recent experiences (\(k=1500\) by default). It then shares an experience \(e_{t}\) if

\[|(e_{t})|+c^{2} \]

where \(c\) is a constant chosen such that \(1-_{}(c)=\). In other words, we use the \(c\)-quantile of a normal distribution with the (sample) mean and variance of most recent experiences. We include and benchmark this variant for two reasons. One, intuitively, we might want to be more sensitive to clusters of outliers, where using a quantile of the actual data might include only part of the cluster, while a Gaussian model might lead to the entire cluster being included. Two, mean and variance could be computed iteratively without keeping a buffer of recent td-errors, and thereby reducing memory requirements. We aim to benchmark if this approximation impacts performance.

Stochastic weighted experience selectionFinally, and most closely related to classical single-agent PER, this variant shares each experience with a probability that's proportional to its absolute td-error. In PER, given a train batch size \(b\), we sample \(b\) transitions from the replay buffer without replacement, weighted by each transition's td-error. In SUPER, we similarly aim to sample a \(\) fraction of experiences, weighted by their td-errors. However, in order to be able to sample transitions online, we calculate for each experience individually a probability that approximates sampling-without-replacement in expectation. Formally, taking \(p_{i}=|(e_{i})|\), we broadcast experience \(e_{t}\) with probability

\[p=(1,^{}}{_{k}p_{k}^{}})\]

similarly to equation 2, and taking the sum over a sliding window over recent experiences. It is easy to see that if \(=1/\), this is equivalent to sampling a single experience, weighted by td-error, from the sliding window. For larger bandwidth \(\), this approximates sampling multiple experiences without replacement: If none of the \(^{}}{_{k}p_{k}^{}}\) terms are greater than 1, this is exact. If we have to truncate any of these terms, we slightly undershoot the desired bandwidth.

In our current experiments, we share experiences and update all quantiles, means and variances once per sample batch, for conveniendce and performance reasons; However, we could do both online, i.e. after every sampled transition, in real-world distributed deployments. We do not expect this to make a significant difference.

## 5 Experiments and Results

We evaluate the SUPER approach on a number of multiagent benchmark domains.

### Algorithm and control benchmarks

**Baseline: DDQN** As a baseline, we use a fully independent dueling DDQN algorithm. The algorithm samples a sequence of experiences using joint actions from all the agents' policies, and then inserts each agent's observation-action-reward-observation transitions into that agent's replay buffer. Each agent's policy is periodically trained using a sample from its own replay buffer, sampled using PER. We refer to this baseline as simply "DDQN" in the remainder of this section and in figures. In Section C in the appendix we also discuss a variant based on vanilla DQN.

**SUPER-DDQN** We implement SUPER on top of the DDQN baseline. Whenever a batch of experiences is sampled, each agent shares only its most relevant experiences (according to one of the criteria described in the previous section) which are then inserted into all other agents' replay buffers. As above, we run the SUPER experience sharing on whole sample batches. All DQN hyperparameters are unchanged - the only difference from the baseline is the addition of experience sharing between experience collection and learning. This allows for controlled experiments with like-for-like comparisons.

**Parameter-Sharing DDQN** In parameter-sharing, all agents share the same policy parameters. Note that this is different from joint control, in which a single policy controls all agents simultaneously; In parameter sharing, each agent is controlled independently by a copy of the policy. Parameter sharing has often been found to perform very well in benchmarks, but also strictly requires a centralized learner [40; 39; 11]. We therefore do not expect SUPER to outperform parameter sharing, but include it for completeness and as a "best case" fully centralized comparison.

**Multi-Agent Baselines** We further compare against standard multi-agent RL algorithms, specifically MADDPG  and SEAC . Like parameter-sharing, these are considered "centralized training, decentralized execution" approaches.

### Environments

As discussed in the Preliminaries (Section 3), SUPER applies to environments that are anonymous. We are furthermore particularly interested in environments that have a separate reward signal for each agent.3 We therefore run our experiments on several domains that are part of well-established benchmark packages. These include three domains from the PettingZoo package , three domains from the MeltingPot package , and a two-player variant of the Atari 2600 game Space Invaders. We describe these domains in more detail in Section B in the appendix. We run a particularly large number of baselines and ablations in the PettingZoo domains, and focus on the most salient comparisons in the MeltingPot and Atari domains within available computational resources. Pursuit is particularly well suited to showcase our algorithm, as it strictly requires multiple agents to cooperate in order to generate any reward, and each cooperating agent needs to perform slightly different behavior (tagging the target from a different direction).

### Experimental Setup

In Pursuit, we train all agents concurrently using the same algorithm, for each of the algorithms listed. Note that only the pursuers are agents in this domain, whereas the evaders move randomly. In the standard variants of Battle and Adversarial-Pursuit, we first pre-train a set of agents using independent dueling DDQN, all agents on both teams being trained together and independently. We then take the pre-trained agents of the opposing team (red team in Battle, predator team in Adversarial-Pursuit), and use these to control the opposing team during main training of the blue respectively prey team. In this main training phase, only the blue / prey team agents are trained using each of the SUPER and benchmark algorithms, whereas the red / predator team are controlled by the pre-trained policies with no further training. Figure 1 shows final performance from the main training phase. In Battle

Figure 1: Performance of SUPER-dueling-DDQN variants with target bandwidth 0.1 on all domains. For team settings, performance is the total reward of all agents in the sharing team; for all other domains performance is the total reward of all agents. Shaded areas indicate one standard deviation.

and Adversarial-Pursuit we further tested a variant where the opposing team are co-evolving with the blue / prey team, which we discuss in Section C in the appendix.

### Performance Evaluation

Figure 1 shows performance of SUPER implemented on dueling DDQN ("SUPER DDQN") compared to the control benchmarks discussed above. In addition, Figure 2 shows final performance in the three PettingZoo environments in a barplot for easier readability. In summary, we see that (quantile and gaussian) SUPER (red bars) clearly outperform baseline DDQN on all domains, as well as both SEAC and MADDPG baselines where we were able to run them. Particularly notable is a jump in performance from DDQN (green) to quantile SUPER (leftmost red bars in Figure 2) in the PettingZoo domains. Performance of SUPER is even comparable to that of parameter sharing, our "best case" fully centralized comparison benchmark. Aside from parameter sharing, SUPER wins or ties all pairwise comparisons against baselines. Results further hold for additional variations of environments with co-evolving opponents, when using SUPER with a plain (non-dueling, non-double) DQN algorithm, and across hyperparameter settings, which we discuss in the appendix.

In more detail, we find that SUPER (red) consistently outperforms the baseline DQN/DDQN algorithm (green), often significantly. For instance in Pursuit SUPER-DDQN achieves over twice the reward of no-sharing DDQN at convergence, increasing from 181.4 (std.dev 4.1) to 454.5 (std.dev 5.9) for quantile SUPER-DDQN measured at 800k training steps. In both Battle and Adversarial-Pursuit, we enabled SUPER-sharing for only one of the two teams each, and see that this significantly improved performance of the sharing team (as measured by sum of rewards of all agents in the team across each episode), especially mid-training. For instance in Battle, the blue team performance increase from -19.0 (std.dev 11.0) for no-sharing DDQN to 5.5 (std.dev 8.7) for quantile SUPER-DDQN at 150k timesteps. In Adversarial-Pursuit, the prey performance increased from -719.8 (std.dev 66.8) to -506.3 (std.dev 35.0) at 300k timesteps.

SUPER performs significantly better than SEAC (blue) and MADDPG (violet). MADDPG performed very poorly on all domains despite extensive attempts at hyperparameter tuning. SEAC shows some learning in Pursuit at 800k timesteps, but remains well below even baseline DDQN performance, again despite extensive hyperparameter tuning. We have also run experiments with SEAC in Pursuit for significantly longer (not shown in the figure), and saw that performance eventually catches up after around 8M timesteps, i.e. a roughly ten-fold difference in speed of convergence. SUPER (red) significantly outperforms parameter-sharing DQN/DDQN (turquoise) in Pursuit and performs similar to it in Battle, whereas parameter-sharing performs significantly better in Adversarial-Pursuit. Note that parameter sharing has often been observed to perform extremely well [40; 39; 11]. We therefore consider this a strong positive result.

Finally, these results and conclusions show remarkable stability across settings, underlying algorithm, and hyperparameters. In Section C.2 in the appendix we show that similar results hold using SUPER

Figure 2: Performance of SUPER-dueling-DDQN variants and baselines on all three PettingZoo domains. For Pursuit, performance is the total mean episode reward from all agents. For Battle and Adversarial-Pursuit, performance is the total mean episode reward from all agents in the sharing team (blue team in Battle, prey team in Adversarial-Pursuit). Shaded areas indicate one standard deviation.

on a baseline DQN (non-dueling, non-double) algorithm. In Section C.3 in the appendix we show results from SUPER on dueling DDQN on domains where the opposing team agents are co-evolving rather than pretrained. In Section C.5 in the appendix we show that these improvements over baseline DDQN are stable across a wide range of hyperparameter choices. Relative performance results and conclusions in all these cases largely mirror the ones presented here.

### Ablations

We ran an ablation study to gain a better understanding of the impact of two key features of our algorithm: selectivity (only sharing a small fraction of experiences) and priority (sharing experiences with the highest td-error). To this end, we compared SUPER against two cut-down versions of the algorithm: One, we share all experiences indiscriminately. This is also equivalent to a single "global" replay buffer shared between all agents. Two, we share (a small fraction of) experiences, but select experiences uniformly at random. Figure 4 shows the performance of these two ablations at end of training in Pursuit and Battle. We see that both ablations perform significantly worse than SUPER, with neither providing a significant performance uplift compared to baseline DDQN.4 This shows that both selectivity as well as priority are necessary to provide a performance uplift, at least in some domains.

### Bandwidth Sensitivity

In the Pursuit domain, we performed an analysis of the performance of SUPER-DDQN for varying target bandwidths ranging from 0.0001 to 1 (sharing all experiences). Figure 4 shows the converged performance of SUPER-DDQN with quantile-based experience selection in Pursuit. A label of "DQN" indicates that no sharing is taking place, i.e. decentralized DDQN; a label of "all" indicates that all experiences are shared, without any prioritized selection. Numerical labels give different target bandwidths. Two things stand out to us: First, sharing all experiences indiscriminately does not result in increased performance. In fact, at convergence, it results in slightly lower performance than no-sharing DDQN. Second, there is a clear peak of performance around a target bandwidth of 0.01 - 0.1, which also holds for gaussian and stochastic experience selection (we refer the reader to the appendix for more details). We conclude that sharing experiences _selectively_ is crucial for learning to benefit from it.

### Experience Selection

Gaussian experience selection performed similarly to the quantile selection we designed it to approximate. Its actual used bandwidth was however much less responsive to target bandwidth than the other two variants. We believe this demonstrates that in principle approximating the actual distribution of td-errors using mean and standard deviation is feasible, but that more work is needed in determining the optimal value of \(c\) in equation 3. Stochastic experience selection (dotted red) performs similar or worse than both other variants, but generally still comparably or better than baseline DQN/DDQN.

## 6 Conclusion & Discussion

ConclusionWe present selective multiagent PER, a selective experience-sharing mechanism that can improve DQN-family algorithms in multiagent settings. Conceptually, our approach is rooted in the same intuition that Prioritized Experience Replay is based on, which is that td-error is a useful approximation of how much an agent could learn from a particular experience. In addition, we introduce the a second key design choice of selectivity, which allows semi-decentralized learning with small bandwidth, and drastically improves performance in some domains.

Experimental evaluation on DQN and dueling DDQN shows improved performance compared to fully decentralized training (as measured in sample efficiency and/or converged performance) across a range of hyperparameters of the underlying algorithm, and in multiple benchmark domains. We see most consistent performance improvements with SUPER and a target bandwidth of 0.01-0.1 late in training, more consistent than indiscriminate experience sharing. Given that this effect appeared consistently across a wide range of hyperparameters and multiple environments, as well as on both DQN and dueling DDQN, the SUPER approach may be useful as a general-purpose multi-agent RL technique. Equally noteworthy is a significantly improved performance early in training even at very low bandwidths. We consider this to be a potential advantage in future real-world applications of RL where sample efficiency and rapid adaption to new environments are crucial. SUPER consistently and significantly outperforms MADDPG and SEAC, and outperforms parameter sharing in Pursuit (but underperforms in Adversarial-Pursuit, and shows equal performance in Battle).

DiscussionOur selective experience approach improves performance of both DQN and dueling DDQN baselines, and does so across a range of environments and hyperparameters. It outperforms state-of-the-art multi-agent RL algorithms, in particular MADDPG and SEAC. The only pairwise comparison that SUPER loses is against parameter sharing in Adversarial-Pursuit, in line with a common observation that in practice parameter sharing often outperforms sophisticated multi-agent RL algorithms. However, we note that parameter sharing is an entirely different, fully centralized training paradigm. Furthermore, parameter sharing is limited in its applicability, and does not work well if agents need to take on different roles or behavior to successfully cooperate. We see this in the Pursuit domain, where parameter sharing performs poorly, and SUPER outperforms it by a large margin. The significantly higher performance than MADDPG and SEAC is somewhat expected given that baseline non-sharing DQN algorithms often show state-of-the-art performance in practice, especially with regard to sample efficiency.

It is noteworthy that deterministic (aka "greedy") experience selection seems to perform slightly better than stochastic experience selection, while in PER the opposite is generally the case . We have two hypotheses for why this is the case. One, we note that in PER, the motivation for stochastic prioritization is to avoid low-error experiences never being sampled (nor re-prioritized) in many draws from the buffer. On the other hand, in SUPER we only ever consider each experience once. Thus, if in stochastic experience selection a high-error experience through random chance is not shared on this one opportunity, it will never be seen by other agents. In a sense, we may prefer deterministic experience selection in SUPER for the same reason we prefer stochastic selection in PER, which is to avoid missing out on potentially valuable experiences. Two, in all our current experiments we used (stochastic) PER when sampling training batches from the replay buffer of each agent. When using stochastic SUPER, each experience therefore must pass through two sampling steps before being shared and trained on by another agent. It is possible that this dilutes the probability of a high-error experience being seen too much.

We would also like to point out a slight subtlety in our theoretical motivation for SUPER: We use the sending agent's td-error as a proxy for the usefulness of an experience for the receiving agent. We believe that this is justified in symmetric settings, and our experimental results support this. However, we stress that this is merely a heuristic, and one which we do not expect to work in entirely asymmetric domains. For future work, we would be interested to explore different experience selection heuristics. As an immediate generalization to a more theoretically grounded approach, we wonder if using the td-error of each (potential) receiving agent could extend SUPER to asymmetric settings, and if it could further improve performance even in symmetric settings. While this would effectively be a centralized-training approach, if it showed similar performance benefits as we have seen in symmetric settings for SUPER, it could nevertheless be a promising avenue for further work. Beyond this, we would be interested to explore other heuristics for experience selection. For instance, we are curious if the sending agent could learn to approximate each receiver's td-error locally, and thus retain the decentralized-with-communication training capability of our current approach. However, given that td-error is intrinsically linked to current policy and thus highly non-stationary, we expect there would be significant practical challenges to this.

In this current work, we focus on the DQN family of algorithms in this paper. In future work, we would like to explore SUPER in conjunction with other off-policy RL algorithms such as SAC  and DDPG . The interplay with experience sampling methods other than PER, such as HER  would also be interesting. If the improvements we see in this work hold for other algorithms and domains as well, this could improve multi-agent RL performance in many settings.

Finally, our approach is different from the "centralized training, decentralized execution" baselines we compare against in the sense that it does not require fully centralized training. Rather, it can be implemented in a decentralized fashion with a communications channel between agents. We see that performance improvements scale down even to very low bandwidth, making this feasible even with limited bandwidth. We think of this scheme as "decentralized training with communication" and hope this might inspire other semi-decentralized algorithms. In addition to training, we note that such a "decentralized with communication" approach could potentially be deployed during execution, if agents keep learning. While this is beyond the scope of the current paper, in future work we would like to investigate if this could help when transferring agents to new domains, and in particular with adjusting to a sim-to-real gap. We envision that the type of collaborative, decentralized learning introduced here could have impact in future applications of autonomous agents ranging from disaster response to deep space exploration.