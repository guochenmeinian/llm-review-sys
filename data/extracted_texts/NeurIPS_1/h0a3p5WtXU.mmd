# Loss landscape Characterization of

Neural Networks without Over-Parametrization

Rustem Islamov\({}^{1}\)  Niccolo Ajroldi\({}^{2}\)  Antonio Orvieto\({}^{2,3,4}\)  Aurelien Lucchi\({}^{1}\)

\({}^{1}\)University of Basel \({}^{2}\)Max Planck Institute for Intelligent Systems \({}^{3}\) ELLIS Institute Tubingen

\({}^{4}\)Tubingen AI Center

###### Abstract

Optimization methods play a crucial role in modern machine learning, powering the remarkable empirical achievements of deep learning models. These successes are even more remarkable given the complex non-convex nature of the loss landscape of these models. Yet, ensuring the convergence of optimization methods requires specific structural conditions on the objective function that are rarely satisfied in practice. One prominent example is the widely recognized Polyak-Lojasiewicz (PL) inequality, which has gained considerable attention in recent years. However, validating such assumptions for deep neural networks entails substantial and often impractical levels of over-parametrization. In order to address this limitation, we propose a novel class of functions that can characterize the loss landscape of modern deep models without requiring extensive over-parametrization and can also include saddle points. Crucially, we prove that gradient-based optimizers possess theoretical guarantees of convergence under this assumption. Finally, we validate the soundness of our new function class through both theoretical analysis and empirical experimentation across a diverse range of deep learning models.

## 1 Introduction

The strides in empirical progress achieved by deep neural networks over the past decade have been truly remarkable. Central to the triumph of these techniques lies the effectiveness of optimization methods, which is particularly noteworthy given the non-convex nature of the objective functions under consideration. Worst-case theoretical results point to a pessimistic view since even a degree four polynomial can be NP-hard to optimize  and the loss landscape of some neural networks are known to include saddle points or bad local minima .

Yet, empirical evidence has shown that gradient-based optimizers - including SGD, AdaGrad and Adam among many others - can effectively optimize the loss of modern deep-learning-based models. While some have pointed to the ability of gradient-based optimizers to deal with potentially complex landscapes, e.g. escaping saddle points , another potential explanation is that the loss landscape itself is less complex than previously assumed .

Some key factors in this success include the choice of architecture , as well as the over-parametrization . In the well-known infinite-width limit , neural networks are known to exhibit simple landscapes . However, practical networks operate in a finite range, which still leaves a lot of uncertainty regarding the nature of the loss landscape. This is especially important given that the convergence guarantees of gradient-based optimizers are derived by assuming some specific structure on the objective function . Consequently, an essential theoretical endeavor involves examining the class of functions that neural networks can represent.

In this work, we present a new class of functions that satisfy a newly proposed \(\)-\(\)-condition (see Eq. (2)). We theoretically and empirically demonstrate that these functions effectively characterizethe loss landscape of neural networks. Furthermore, we derive theoretical convergence guarantees for commonly used gradient-based optimizers under the \(\)-\(\)-condition.

In summary, we make the following contributions:

1. We introduce the \(\)-\(\)-condition and theoretically demonstrate its applicability to a wide range of complex functions, notably those that include local saddle points and local minima.
2. We empirically validate that the \(\)-\(\)-condition is a meaningful assumption that captures a wide range of practical functions, including matrix factorization and neural networks (ResNet, LSTM, GNN, Transformer, and other architectures).
3. We analyze the theoretical convergence of several optimizers under \(\)-\(\)-condition, including vanilla SGD (Stochastic Gradient Descent), \(_{}\) (Stochastic Polyak Stepsize) , and NGN (Non-negative Gauss-Newton).
4. We provide empirical and theoretical counter-examples where the weakest assumptions, such as the PL and Aiming conditions, do not hold, but the \(\)-\(\)-condition does.

## 2 Related work

### Function classes in optimization

Studying the convergence properties of gradient-based optimizers has a long history in the field of optimization and machine learning. Notably, one of the fundamental observations is the linear and sub-linear convergence exhibited by GD for _strongly convex_ (SCvx) and general _convex_ (Cvx) functions . However, most modern Machine Learning models have non-convex loss landscapes, for which the existing convex theory is not applicable. Without assumptions on the loss functions (other than smoothness), one can only obtain weak convergence guarantees to a first-order critical point. This situation has led to the derivation of assumptions that are weaker than convexity but that are sufficient to guarantee convergence of GD-based optimizers. The list includes _error bounds_ (EB) , _essential strong convexity_ (ESC) , weak strong convexity (WSC) , the restricted secant inequality (RSI) , and the quadratic growth (QG) condition . In the neighborhood of the minimizer set \(\), EB, PL, and QG are equivalent if the objective is twice differentiable . All of them, except QG, are sufficient to guarantee a global linear convergence of GD. However, among these less stringent conditions, the Polyak-Lojasiewicz (PL) condition stands out as particularly renowned. Initially demonstrated by Polyak  to ensure linear convergence, it has recently experienced a resurgence of interest, in part because it accurately characterizes the loss landscape of heavilly over-parametrized neural networks . It was also shown to be one of the weakest assumptions among the other known conditions outlined so far . A generalized form of the PL condition for non-smooth optimization is the Kurdyka-Lojasiewicz (KL) condition [43; 10] which is satisfied for a much larger class of functions [14; 77] than PL. The KL inequality has been employed to analyze the convergence of the classic proximal-point algorithm [3; 11; 47] and other optimization methods [4; 45].

More recently, some new convex-like conditions have appeared in the literature such as _star-convex_ (StarCvx) , _quasar-convex_ (QCvx) , and _Aiming_. These conditions are relaxations of

  
**Condition** & **Definition** & **Comments** \\  QCvx  & \( f(x),x-x^{}(f(x)-f(x^{}))\) & - excludes saddle points and local minima \\  & for some fixed \(x^{}\) & - includes saddle points and local minima \\  Aiming  & \( f(x),x-(x,) f(x)\) & - theoretically holds for NN in the presence of \\  & & - does not always hold in practice [Fig. 1 a-b] \\  PL (\({}^{}\))  & \(\| f(x)\|^{2} 2(f(x)-f^{})\) & - excludes saddle points and local minima \\  & & - theoretically holds for NN in the presence of \\  & & - impractical over-parameterization  \\  \(\)-\(\)-condition & \( f_{i}(x),x-(x,)(f _{i}(x)-f_{i}((x,)))\) & - might have saddles [Ex. 2] and local minima [Ex. 3] - in practice does not require \\
**[This work]** & \(-(f_{i}(x)-f_{i}^{})\) & over-parameterization [Ex. 5] \\   

Table 1: Summary of existing assumptions on the problem (1) and their limitations. Here \(\) denotes the set of minimizers of \(f\) and \(f_{i}^{}*{argmin}_{x}f_{i}(x)\). Unlike earlier conditions, the \(\)-\(\)-condition is specifically designed to capture local minima and saddle points. NN = Neural Network.

convexity and include non-convex functions. Within the domain of reinforcement learning, several works  have also considered relaxations of the gradient domination condition, although these analyses are conducted specifically within the context of policy gradient methods, and therefore less relatable to StarCvx, QCVx or the Aiming condition.

We present a summary of some of these conditions in Table 1. There is no general implication between already existing assumptions such as QCVx, Aiming, PL, and the \(\)-\(\)-condition. However, as we will later see, the \(\)-\(\)-condition can more generally characterize the landscape of neural networks without requiring unpractical amounts of over-parametrization. Notably, the \(\)-\(\)-condition is a condition that applies globally to the loss. However, we will demonstrate that convergence guarantees can still be established for commonly-used gradient-based optimizers, although these guarantees are weaker than those derived under the PL condition, which relies on much stronger assumptions.

### Limitations of existing conditions

Next, we discuss the limitations of previous conditions to characterize the loss landscape of complex objective functions such as the ones encountered when training deep neural networks.

Necessity of Over-parameterization.When considering deep models, the theoretical justification of conditions such as Aiming  and PL  require a significant amount of over-parameterization. This implies that the neural network must be considerably large, often with the minimum layer's width scaling with the size of the dataset \(n\). However, various studies suggest that this setup may not always accurately model real-world training dynamics . To the best of our knowledge, the weakest requirements on the width of a network are sub-quadratic \((n^{3/2})\), where \(n\) is the size of a dataset. This implies that, even for a small dataset such as MNIST, a network should have billions of parameters which is not a realistic setting in practice. In contrast, the \(\)-\(\)-condition condition does not require such an over-parametrization condition to hold (e.g., see Example 5). In Section 5 we provide empirical results showing how our condition is affected by over-parameterization.

Necessity of Invexity.One limitation of prior assumptions is their inability to account for functions containing local minima or saddle points. Indeed, many of the weakest conditions, such as QCVx, PL, KL, and Aiming, require that any point where the gradient is zero must be deemed a global minimizer. However, such conditions are not consistent with the loss landscapes observed in practical neural networks. For example, finite-size MLPs can have spurious local points or saddle points . Another known example is the half-space learning problem which is known to have saddle points . We refer the reader to Figure 2-a that illustrates this claim (it showcases the surface of the problem

Figure 1: Training of \(3\) layer LSTM model that shows Aiming condition does not always hold. The term “Angle” in the figures refers to the angle \(( f(x^{k}),x^{k}-x^{K})\), and it should be positive if Aiming holds, while in a-b we observe that it is negative during the first part of the training. Figures c-d demonstrate that possible constant \(\) in PL condition should be small which makes theoretical convergence slow.

Figure 2: Training for half-space learning problem with SGD. The term “Angle” in the figures refers to the angle \(( f(x^{k}),x^{k}-x^{K})\).

fixing all parameters except first \(2\)), and also demonstrates that the Aiming and PL conditions fail to hold in such a setting. We present the results in Figure 21 where we observe that \((i)\) the angle between the full gradient and direction to the minimizer \(( f(x^{k}),x^{k}-x^{*})\) can be negative implying that the Aiming condition does not hold in this case (since the angle should remain positive); \((ii)\) the gradient norm can become zero while we did not reach minimum (loss is still large) implying that the PL condition does not hold as well (since the inequality \(\| f(x^{k})\|^{2} 2(f(x^{k})-f^{*})\) is not true for any positive \(\)). These observations suggest that the Aiming and PL conditions do not characterize well a landscape in the absence of invexity property. In contrast, we demonstrate in Example 2 and Figure 9 that our proposed assumption is preferable in this scenario.

Lack of Theory.As previously mentioned, most theoretical works apply to some infinite limit or neural networks of impractical sizes. In contrast, several works  have studied the empirical properties of the loss landscape of neural networks during training. They have shown that gradient-based optimization methods do not encounter significant obstacles that impede their progress. However, these studies fall short of providing theoretical explanations for this observed phenomenon.

Lack of Empirical Evidence.Several theoretical works  prove results on the loss landscape of neural networks without supporting their claims using experimental validation on deep learning benchmarks. We demonstrate some practical counter-examples to these conditions proved in prior work. We train LSTM-based model2 with standard initialization on Wikitext-2  and Penn Treebank (PTB)  datasets. In Figure 1 (a-b), we show that the angle between the full gradient and direction to the minimizer \(( f(x^{k}),x^{k}-x^{*})\) can be negative in the first part of the training. This result implies that the Aiming condition does not hold in this setting (either we do not have enough over-parameterization or the initialization does not lie in the locality region where Aiming holds). Moreover, for the same setting in Figure 1 (c-d) we plot \(2(\| f(x^{k})\|)-}{{8}}(f(x^{k})-f(x^{K}))\)3 to measure the empirical value of PL constant \((2)\) (see derivations in Appendix A). We observe that the value of \(\) that might satisfy PL condition should be of order \(10^{-8}-10^{-7}\) and leads to slow theoretical convergence . These observations contradict with practical results. We defer to Appendix A for a more detailed discussion. In contrast, we demonstrate in Figure 7-(g-h) that the proposed \(\)-\(\)-condition can be verified in this setting.

## 3 The proposed \(\)-\(\)-condition

Setting.We consider the following Empirical Risk Minimization (ERM) problem that typically appears when training machine learning models:

\[_{x^{d}}[f(x)_{i=1}^{n}f_{i}(x )]. \]

Here \(x^{d}\) denotes the parameters of a model we aim to train, \(d\) is the number of parameters, and \(n\) is the number of samples in the training dataset. Each \(f_{i}(x)\) is the loss associated with the \(i\)-th data point. We denote the minimum of the problem (1) by \(f^{*}\) and the minimum of each individual function by \(f^{*}_{i}_{x}f_{i}(x)\), which we assume to be finite. Besides, the set \(\) denotes the set of all minimizers of \(f\).

A new class of functions.Next, we present a new condition that characterizes the interplay between individual losses \(f_{i}\) and the set of minimizers of the global loss function \(f\).

**Definition 1** (\(\)-\(\)-condition ).: Let \(^{d}\) be a set and consider a function \(f:\) as defined in (1). Then \(f\) satisfies the \(\)-\(\)-condition with positive parameters \(\) and \(\) such that \(>\) if for any \(x\) there exists \(x_{p}(x,)\) such that for all \(i\),

\[ f_{i}(x),x-x_{p}(f_{i}(x)-f_{i}(x_{p}))-(f_{i }(x)-f_{i}^{*}). \]

The \(\)-\(\)-condition recovers several existing assumptions as special cases. For example, the proposed assumption reduces to QCvx around \(x^{*}\) if \(>0\), \(=0\), and \(\) is a singleton \(\{x^{*}\}\). Importantly, the \(\)-\(\)-condition is also applicable when the set \(\) contains multiple elements.

One can easily check that the pair of parameters \((,)\) in (2) can not be unique. Indeed, if the assumption is satisfied for some \(\) and \(\), then due to inequality \(f_{i}(x_{p}) f_{i}^{*}\) it will be also satisfied for \((+,+)\) for any \( 0\).

### Theoretical verification of the \(\)-\(\)-condition

To demonstrate the significance of Definition 1 as a meaningful condition for describing structural non-convex functions, we provide several examples below that satisfy (2). We do not aim to provide the tightest possible choice of \(\) and \(\) such that Definition 1 holds. Instead, this section aims to offer a variety of examples that demonstrate specific desired characteristics when \(\)-\(\)-condition holds, encompassing a broad range of functions.

The initial example illustrates that \(\) could potentially be infinite for the class of functions satisfying Definition 1.

**Example 1**.: Let \(f,f_{1},f_{2}^{2}\) be such that

\[f=(f_{1}+f_{2}) f_{1}(x,y)=}{ (x+y)^{2}+1}, f_{2}(x,y)=}{(x+y+1)^{2}+1}, \]

then Definition 1 holds with \([}{{2}},+)\) and \([}{{5}},)\).

Next, we provide an example where \(f\) satisfies Definition 1 even in the presence of saddle points.

**Example 2**.: Let \(f,f_{1},f_{2}^{2}\) be such that

\[f=(f_{1}+f_{2}) f_{1}(x,y)=1-e^{-x^{2}-y^{2} }, f_{2}(x,y)=1-e^{-(x-2)^{2}-(y-2)^{2}}, \]

then Definition 1 holds for some \(\) and \(=-8\).

**Remark 1**.: Examples 1 and 2 can be generalized for any number of functions \(n\) and dimension \(d\) as follows

\[f_{i}(x)=^{d}x_{j}+a_{i})^{2}}{(_{j=1}^ {d}x_{j}+a_{i})^{2}+1}, f_{i}(x)=^{d}(x_{j}-b_{ij })^{2}}{1+_{j=1}^{d}(x_{j}-b_{ij})^{2}}, \]

for some properly chosen \(\{a_{i}\}\) and \(\{b_{ij}\},i[n],j[d]\).

**Example 3**.: Let \(f,f_{1},f_{2}^{2}\) be such that

\[f=(f_{1}+f_{2}) f_{1}(x,y)=+y^ {2}}{4+x^{2}+y^{2}}, f_{2}(x,y)=+(y-2.5)^{2}}{4+(y-2.5) ^{2}+(y-2.5)^{2}}, \]

then Definition 1 holds for some \(\) and \(=-1\).

The three examples above demonstrate that functions satisfying Definition 1 can potentially be non-convex with an unbounded set of minimizers \(\) (Example 1) and can have saddle points (Example 2) and local minima (Example 3). In contrast, the PL and Aiming conditions are not met in cases where a problem exhibits saddle points. For illustration purposes, we plot the loss landscapes of \(f\) in Figure 3.

So far, we have presented simple examples to verify Definition 1. Next, we turn our attention to more practical examples in the field of machine learning. We start with the matrix factorization problem that is known to have saddle points  but can be shown to be PL after a sufficiently large number of iterations of alternating gradient descent and under a specific random initialization .

**Example 4**.: Let \(f_{i},f_{ij}\) be such that

\[f(W,S)=\|X-W^{}S\|_{}^{2}=_{i,j}( X_{ij}-w_{i}^{}s_{j})^{2}, f_{ij}(W,S)=(X_{ij}-w_{i}^{}s_{j} )^{2}, \]

where \(X^{n m},W=(w_{i})_{i=1}^{n}^{k n},S=(s_{ j})_{j=1}^{m}^{k m},\) and \((X)=r k\). We assume that \(X\) is generated using matrices \(W^{*}\) and \(S^{*}\) with non-zero additive noise that minimize empirical loss, namely, \(X=(W^{*})^{}S^{*}+(_{ij})_{i[n],j[m]}  W^{*},S^{*}=_{W,S}f(W,S)\). Let \(\) be any bounded set that contains \(\). Then Definition 1 is satisfied with \(=+1\) and some \(>0\).

**Example 5**.: Consider training a two-layer neural network with a logistic loss

\[f=_{i=1}^{n}f_{i}, f_{i}(W,v)=(y_{i} v^{} (Wx_{i}))+_{1}\|v\|^{2}+_{2}\|W\|_{}^{2} \]

for a classification problem where \((t)(1+(-t)),\,W^{k d},v^{ k},\,\) is a ReLU function applied coordinate-wise, \(y_{i}\{-1,+1\}\) is a label and \(x_{i}^{d}\) is a feature vector. Let \(\) be any bounded set that contains \(\). Then the \(\)-\(\)-condition holds in \(\) for some \( 1\) and \(=-1\).

**Remark 2**.: The previous examples can be extended to any positive and convex function \(\) (e.g., square loss) with the additional assumption that each individual loss \(f_{i}\) does not have minimizers in \(\), i.e. \((W^{*},v^{*})\) such that \(f_{i}(W^{*},v^{*})=f_{i}^{*}\) for some \(i[n]\).

We highlight that Example 5 is applicable for a bounded set \(\) of an _arbitrary_ size. Moreover, in practice, we typically add \(_{1}\) or \(_{2}\) regularization which can be equivalently written as a constrained optimization problem, and therefore, Example 5 holds in this scenario. In comparison, the results from previous works do not hold for an arbitrary bounded set around \(\) requiring initialization to be close enough to the solution set . The proofs for all examples are given in Appendix C.1.

## 4 Theoretical convergence of algorithms

We conduct our analysis under the following smoothness assumption that is standard in the optimization literature.

**Assumption 1**.: _We assume that each \(f_{i}\) is \(L\)-smooth, i.e. for all \(x,y^{d}\) it holds \(\| f_{i}(x)- f_{i}(y)\| L\|x-y\|\)._

The next assumption, which is sometimes called functional dissimilarity , is standard in the analysis of SGD with adaptive stepsizes .

**Assumption 2**.: _We assume that the interpolation error \(_{}^{2}_{i}[f^{*}-f_{i}^{*}]\) is finite, where the expectation is taken concerning the randomness of indices \(i\) for a certain algorithm._

### Convergence under the \(\)-\(\)-condition

Now we demonstrate that the \(\)-\(\)-condition is sufficient for common optimizers to converge up to a neighbourhood of the set of minimizers \(\). We provide convergence guarantees for SGD-based algorithms with fixed and adaptive stepsize (i.e., the update direction is of the form \(-_{k} f_{ik}(x^{k})\)). In this section, we only present the main statements about convergence while the algorithms' description and the proofs are deferred to Appendix C.2.

Convergence of SGD.We start with the results for vanilla SGD with constant stepsize.

**Theorem 1**.: _Assume that Assumptions 1-2 hold. Then the iterates of SGD (Alg. 1) with stepsize \(\) satisfy_

\[_{0 k<K}[f(x^{k})-f^{*}][ (x^{0},)^{2}]}{K}+_{}^{2}+_{}^{2}. \]

Figure 3: Loss landscape of \(f\) that satisfy Definition 1. The analytical form of \(f_{i}\) is given in Section 3.1. These examples demonstrate that the problem (1) that satisfies \(\)-\(\)-condition might have an unbounded set of minimizers \(\) (Example 1), a saddle point (Example 2), and local minima (Example 3) in contrast to the PL and Aiming conditions. The contour plots are presented in 19.

Theorem 1 shows that under the \(\)-\(\)-condition, SGD converges with a rate \((K^{-1/2})\) (the same rate obtained by SGD for convex functions ) up to a ball of size \((_{}^{2})\). We argue that the non-vanishing term \((_{}^{2})\) must appear in the convergence rate for non-convex optimization for several reasons: \((i)\) This term arises directly from the use of the \(\)-\(\) condition in the analysis, without resorting to additional upper bounds or approximations. It reflects the potential existence of local minima that the \(\)-\(\) condition is designed to model. In the worst-case scenario, if SGD is initialized near local minima and uses a sufficiently small step size, it may fail to converge to the exact minimizer and can become trapped in suboptimal minima. This sub-optimality is modeled in the upper bound by the stepsize-independent quantity \((_{}^{2})\) since we provide convergence guarantees for the function value sub-optimality rather than the squared gradient norm, which is more typical in the non-convex setting. \((ii)\) We also observe that the last term in (9) shrinks as a model becomes more over-parameterized (which is consistent with prior works such as  that require large amounts of over-parametrization); see Sections 5.1, 5.2, and D.7 for experimental validation of this claim. Further empirical observations are summarized in Table 2 and will be discussed in Section 5. Theoretically, if a model is sufficiently over-parameterized such that the interpolation condition \(f_{i}^{*}=f^{*}\) holds, then the non-vanishing term is not present in the bound. \((iii)\) The presence of a non-vanishing error term in the rate with the \(\)-\(\) condition is consistent with empirical observation as it is frequently observed when training neural networks (see Figure \(8.1\) in ). This is also observed during the training of language models where the loss is significantly larger than \(0\) (see Figure 19). This phenomenon suggests that reaching a critical point, which is a global minimizer, is not commonly observed practically. \((iv)\) Finally, we note a potential similarity with prior works that propose other conditions to describe the loss landscape of deep neural networks (e.g. gradient confusion ), and also obtain a non-vanishing term in the convergence rate (see Theorem \(3.2\) in ).

Convergence of \(_{}\).Next, we consider the \(_{}\) algorithm. \(_{}\) stepsize is given by \(_{k}\{}(x^{k})-f_{i_{k}}^{*}}{c\| f _{i_{k}}(x^{k})\|^{2}},_{b}\}\) where \(c\) and \(_{b}\) are the stepsize hyperparameters.

**Theorem 2**.: _Assume that Assumptions 1-2 hold. Then the iterates of \(_{}\) (Alg. 2) with a stepsize hyperparameter \(c>\) satisfy_

\[_{0 k<K}[f(x^{k})-f^{*}]}{K} [(x^{0},)^{2}]+2 c_{1} _{b}_{}^{2}, \]

_where \(_{}\{}{{2cL}},_{b}\}\) and \(c_{1}(2(-)e-1)}\)._

In the convex case, i.e. \(\)-\(\)-condition holds with \(=1,=0\), we recover the rate of Loizou et al. .

Convergence of Ngn.Finally, we turn to the analysis of NGN. This algorithm is proposed for minimizing positive functions \(f_{i}\) which is typically the case for many practical choices. Its stepsize \(_{k}}^{*}\| f_{ i_{k}}(x^{k})\|^{2}}}\) where \(\) is the stepsize hyperparameter. NGN stepsize differs from that of \(_{}\) by replacing \(\) operator by softer harmonic averaging of \(\) stepsize and a constant \(\). In addition to the already mentioned assumptions, we make a mild assumption that the positivity error \(_{}^{2}[f_{i}^{*}]\) is finite.

**Theorem 3**.: _Assume that Assumptions 1 with \(+1\) and 1-2 hold. Assume that each function \(f_{i}\) is positive and \(_{}^{2}<\). Then the iterates of NGN (Alg. 3) with a stepsize parameter \(>0\) satisfy_

\[_{0 k K-1}[f(x^{k})-f^{*}]  [(x^{0},)^{2}] }{2 K}+}^{2}}{c_{2}}}{+\ \ \{2 L-1,0\}_{ }^{2}+}^{2}}{c_{2}}}, \]

_where \(c_{2} 2 L(--1)+-\)._

    & **Model’s width**\(\) & **Model’s depth**\(\) & **Batch-size**\(\) \\  Change in \(_{}^{2}\) & \(\) & \(\) & \(\) \\   

Table 2: Summary of how the non-vanishing term \(_{}^{2}\) (as appearing e.g. in Eq. (9)) increases (\(\)) or decreases (\(\)) as a function of specific quantities of interest.

One of the main properties of NGN is its robustness to the choice of stepsize \(\). Theorem 3 can be seen as an extension of this feature from the set of convex functions originally analyzed in  to the class of structured non-convex satisfying \(\)-\(\)-condition.

Comparing the results of Theorems 2 and 3 we highlight several important differences. \((i)\) There is no restriction on the stepsize parameter \(\) for NGN. Conversely, \(_{}\) requires \(c\) to be lower bounded. \((ii)\) Both algorithms converge to a neighborhood of the solution with a fixed stepsize hyperparameter. However, the neighborhood size of \(_{}\) is not controllable by the stepsize hyperparameter and remains constant even in the convex setting when \(=0\). In contrast, NGN converges to a ball whose size can be made smaller by choosing a small stepsize parameter, and the "non-vanishing" term disappears in the convex setting \(=0\).

We note that our goal was not to achieve the tightest convergence guarantees for each algorithm, but rather to underscore the versatility of the \(\)-\(\)-condition in deriving convergence guarantees for SGD-type algorithms, both for constant or adaptive stepsizes. In addition to the results of this section, we demonstrate the convergence guarantees for SGD, \(_{}\), and NGN with decreasing with \(k\) stepsizes in Appendix C.2. Besides, in Appendix C.2.4 we present a convergence of a slightly modified version of Adagrad-norm method  under \(\)-\(\)-condition.

## 5 Experimental validation of the \(\)-\(\)-condition

In this section, we provide extensive numerical results supporting that the \(\)-\(\)-condition does hold in many practical applications for various tasks, model architectures, and datasets. The detailed experimental setting is described in Appendix D.

In all cases, we approximate \((x^{k},)\) as the last iterate \(x^{K}\) in a run. After finding such an approximation, we start a second training run with the same random seed to measure all necessary quantities. To guarantee that the second training trajectory follows the same path as the first run, we disable non-deterministic CUDA operations while training on a GPU. For each task, we demonstrate possible values of pairs of \((,)\) that work across all runs (might differ from one experiment to another) with different random seeds and satisfy \(+0.1\).

### MLP architecture

First, we test MLP neural networks with 3 fully connected layers on Fashion-MNIST  dataset. We fix the second layer of the network to be a square matrix and vary its dimension layer to investigate the effect of over-parameterization on \(\)-\(\)-condition. We test it for dimensions \(\{32,128,2049,4096\}\), and for each case, we run experiments for \(4\) different random seeds. In Figure 4 we demonstrate possible values of pairs of \((,)\) that work across all \(4\) runs. We observe that minimum possible values of \(\) and \(\) increase from small size to medium, and then tend to decrease again as the model becomes more over-parameterized. We defer more experimental results for MLP to Appendix D.3 to showcase this phenomenon. This observation leads to the fact that the neighborhood of convergence \((_{}^{2})\) of SGD eventually becomes smaller with the size of the model as we expect (since it becomes more over-parameterized).

### CNN architecture

In our next experiment, we test convolutional neural networks with \(2\) convolution layers and \(1\) fully connected layer on CIFAR10 dataset . We vary the number of convolutions in the second convolution layer to investigate the effect of over-parameterization on \(\)-\(\)-condition. We test it

Figure 4: \(\)-\(\)-condition in the training of 3 layer MLP model on Fashion-MNIST dataset varying the size of the second layer. Here \(T(x_{k})= f_{i_{k}}(x^{k}),x^{k}-x^{K}-(f_{i_{k}}(x ^{k})-f_{i_{k}}(x^{K}))- f_{i_{k}}(x^{k})\) assuming that \(f_{i}^{*}=0\). Minimum is taken across all runs and iterations for given pair of \((,)\).

for \(\{32,128,512,2048\}\) number of convolutions in the second layer, and for each case, we run experiments for \(4\) different random seeds. In Figure 5, we observe that the smallest possible values of \((,)\) increase till \(64\) convolutions, and then decrease back. Second, the difference \(-\) for possible choice of \(\) and \(\) decreases from Figure 5-a to Figure 5-b, but then it increases again.

### Resnet architecture

Next, we switch to the Resnet architecture  with batch sizes in \(\{64,128,256,512\}\) trained on CIFAR100 . For each batch size, we run experiments for \(4\) different random seeds. In Figure 6, we plot the possible choice of pairs \((,)\) that works across all runs. We observe that \(\)-\(\)-condition holds in all cases. Besides, there is a tendency for the minimum possible choice of \(\) and \(\) to decrease with batch size. Moreover, for larger batches, the difference between \(\) and \(\) also increases. From Theorem 1, this result suggests that we can use bigger stepsizes with larger batches.

### Training of AlgoPerf workloads and transformers for language modeling

We are now interested in assessing the validity of \(\)-\(\)-condition on modern Deep Learning architectures. Thereby, we consider tasks from the AlgoPerf benchmarking suite . We consider four workloads from the competition: (i) DLRMsmall model  on Criteo 1TB dataset ; (ii) U-Net model  on FastMRI dataset  (iii) GNN model  on OGBG dataset ; (iv) Transformer-big  on WMT dataset . To train the models, we use NAdamW optimizer 4, which achieves state-of-the-art performances on the current version of the benchmark. The hyperparameters of the optimizer are chosen to reach the validation target threshold set by the original competition. Moreover, we consider the pretraining of a decoder-only transformer architecture  for causal language modeling. We conduct our evaluation on two publicly available Pythia models , of sizes \(70\)M and \(160\)M, trained on 1.25B and 2.5B tokens respectively. For this study, we use the SlimPajama  dataset. Following the original Pythia recipes, we fix a sequence length of 2048 and train the language model to predict the next token in a self-supervised fashion. We refer to section Appendices D and D.8 for additional details.

The results are presented in Figure 7. We observe that \(\)-\(\)-condition holds for a wide range of values of \(\) and \(\) which demonstrates that our condition can be seen as a good characterization of the training of modern large models as well. One can notice that the possible values of \(\) and \(\) for AlgoPerf workloads are higher than those for smaller models discussed in previous sections.

Figure 5: \(\)-\(\)-condition in the training of CNN model on CIFAR10 dataset varying the number of convolutions in the second layer. Here \(T(x_{k})= f_{i_{k}}(x^{k}),x^{k}-x^{K}-(f_{i_{k}}(x^ {k})-f_{i_{k}}(x^{K}))- f_{i_{k}}(x^{k})\) assuming that \(f_{i}^{*}=0\). Minimum is taken across all runs and iterations for a given pair of \((,)\).

This difference is attributable to smaller models interpolating the training data more effectively, resulting in significantly lower training losses compared to those observed in AlgoPerf experiments (see Appendix D for detailed results). However, we highlight that the convergence guarantees of the optimizers depend on a term \((_{}^{2})\) which is stable across all experiments we provide.

### Additional experiments

We defer the verification of \(\)-\(\)-condition by Adam and SGDM to Appendix D.6. The results in Figure 16 suggest that Adam explores the part of the landscape with smaller values of \(\) and \(\) than those for SGDM. The same conclusions can be drawn when comparing SGDM and SGD. These observations might be of the reasons why diagonal preconditioning and momentum are helpful in the training of DL models. The verification of the proposed condition varying the depth of Resnet model can be found in Appendix D.7. The results from Figure 17 demonstrate that the values of \(\) and \(\) decrease with the depth of Resnet model, i.e. the model becomes more over-parametrized.

## 6 Conclusion, potential extensions, and limitations.

In this work, we introduce a new class of functions that more accurately characterize loss landscapes of neural networks. In particular, we provide several examples that satisfy the proposed condition, including \(2\)-layer ReLU-neural networks. Additionally, we prove that several optimization algorithms converge under our condition. Finally, we provide extensive empirical verification showing that the proposed \(\)-\(\)-condition holds along the optimization trajectories of various large deep learning models.

It is also possible to further expand convergence guarantees upon the ones presented in Section 4, for instance, by considering momentum  which is widely used in practice. However, we defer the exploration of other possible extensions to future research endeavors.

One of the limitations of this work is the empirical validation of the \(\)-\(\)-condition on neural networks. We are only able to verify this condition along the trajectories of specific optimizers. Even when performing checks with many random seeds, we cannot fully observe the entire loss landscape. Additionally, while our theoretical examples demonstrate that the proposed condition holds, we do not provide the most precise theoretical values of \(\) and \(\) that satisfy Definition 1. Therefore, in future work, we aim to obtain stronger theoretical guarantees demonstrating that the \(\)-\(\)-condition holds for neural networks with more precise values of \(\) and \(\). We also intend to explore how the \(\)-\(\)-condition varies when changing the architecture or the number of parameters (i.e., theoretical exploration of over-parameterization).

Figure 7: \(\)-\(\)-condition in the training of some large models from AlgoPerf, 3-layer LSTM, and Transformers for language modeling. Here \(T(x_{k})= f_{i_{k}}(x^{k}),x^{k}-x^{K}-(f_{i_{k}}(x^ {k})-f_{i_{k}}(x^{K}))- f_{i_{k}}(x^{k})\) assuming that \(f_{i}^{*}=0\). Minimum is taken across all runs and iterations for a given pair of \((,)\).