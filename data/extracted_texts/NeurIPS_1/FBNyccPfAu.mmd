# Variational Monte Carlo on a Budget -

Fine-tuning pre-trained Neural Wavefunctions

 Michael Scherbela

University of Vienna

michael.scherbela@univie.ac.at

Equal contribution, author order random

Leon Gerard

University of Vienna

leon.gerard@univie.ac.at

Philipp Grohs

University of Vienna

philipp.grohs@univie.ac.at

###### Abstract

Obtaining accurate solutions to the Schrodinger equation is the key challenge in computational quantum chemistry. Deep-learning-based Variational Monte Carlo (DL-VMC) has recently outperformed conventional approaches in terms of accuracy, but only at large computational cost. Whereas in many domains models are trained once and subsequently applied for inference, accurate DL-VMC so far requires a full optimization for every new problem instance, consuming thousands of GPUs even for small molecules. We instead propose a DL-VMC model which has been pre-trained using self-supervised wavefunction optimization on a large and chemically diverse set of molecules. Applying this model to new molecules without any optimization, yields wavefunctions and absolute energies that outperform established methods such as CCSD(T)-2Z. To obtain accurate relative energies, only few fine-tuning steps of this base model are required. We accomplish this with a fully end-to-end machine-learned model, consisting of an improved geometry embedding architecture and an existing SE(3)-equivariant model to represent molecular orbitals. Combining this architecture with continuous sampling of geometries, we improve zero-shot accuracy by two orders of magnitude compared to the state of the art. We extensively evaluate the accuracy, scalability and limitations of our base model on a wide variety of test systems.

## 1 Introduction

Solving the Schrodinger equation is of utmost importance for the prediction of quantum chemical properties in chemistry. The time-independent Schrodinger equation in the Born-Oppenheimer approximation  for a molecule with \(N_{}\) nuclei and \(n_{}\) electrons is an eigenvalue problem with Hamiltonian \(H\):

\[H=E, H=-_{i}_{_{i}}^{2}+_{i>j} }+_{I>J}Z_{J}}{R_{IJ}}-_{i,I}}{r_ {iI}}. \]

By \(=(_{1},,_{N_{}})^{N_{ } 3}\) and \(=(Z_{1},,Z_{N_{}})^{N_{}}\) we denote the nuclear positions and charges. The electron positions are denoted by \(=(_{1},,_{n_{}},,_{n_{}}) ^{n_{} 3}\) with \(n_{}\) spin-up electrons and \(n_{}\) spin-down electrons. The inter-particle difference and distance vectors are written as \(_{IJ}=_{I}-_{J}\), \(R_{IJ}=|_{IJ}|\), \(_{iI}=_{i}-_{I}\), \(r_{iI}=|_{iI}|\), \(_{ij}=_{i}-_{j}\) and \(r_{ij}=|_{ij}|\) with \(I,J=1,,N_{}\) and \(i,j=1,,n_{}\). The eigenvalues \(E\) of Eq. 1 representthe energy states of a molecule, whereas a special interest lies in finding the smallest eigenvalue \(E_{0}\), called the ground-state energy. The corresponding high-dimensional wavefunction \(:^{n_{} 3}\) can be found via the Rayleigh-Ritz principle, by minimizing

\[(_{})=_{_{}^{2}()} [()}{_{}()}] E_{0}. \]

Due to electrons being fermions, the solution must fulfill the anti-symmetry property, stating that the sign of the wavefunction must change for any permutation \(\) of two electrons of the same spin: \(()=-()\). Having access to the solution \(\), allows in principle a complete description of the considered molecule. Unfortunately, only for one electron systems there exists an analytical solution and due to the curse of dimensionality, with increasing number of particles, obtaining an accurate approximation of the wavefunction becomes intractable already for medium-sized molecules. This is because many high-accuracy approximation methods scale poorly with \(n_{}\). For example CCSD(T) - coupled cluster with its single-, double-, and perturbative triple-excitations variant - is considered the gold-standard reference in computational chemistry, but its computational cost scales as \((n_{}{}^{7})\). Deep-learning-based Variational Monte Carlo (DL-VMC) has emerged as a promising alternative solution. A single step scales as \((n_{}{}^{4})\) and it has surpassed the accuracy of many conventional methods such as CCSD(T), when applied to small molecules . In DL-VMC, the wavefunction is represented by a neural network with trainable parameters \(\) and optimized via Eq. 2 using gradient based optimization. Since the expectation value of Eq. 2 cannot be computed analytically, it is approximated by sampling the electron positions during optimization and evaluation with Monte Carlo methods like Metropolis-Hastings .

Related workFermiNet by Pfau et al.  and its variants have emerged as the leading architecture for DL-VMC in first quantization. It can reach highly accurate energies, but typically requires tens of thousands of optimization steps for convergence. Many improvements have been proposed to further increase accuracy  and accelerate convergence . Furthermore, DL-VMC has been extended to properties beyond energies  and systems beyond molecules . Despite the favorable scaling of DL-VMC, computational cost is still high, even for small molecules, often requiring thousands of GPUs to find \(\) for a single small molecule . This is because unlike typical machine learning applications - which train an expensive model once, and subsequently achieve cheap inference - in DL-VMC the minimization of Eq. 2 is typically done from scratch for every new system.

A promising line of research to scale-up expensive ab-initio solvers such as DL-VMC or CCSD(T) has been to develop proxy methods, which can be trained on outputs of ab-initio methods to directly predict molecular properties  or wavefunctions  from the molecular geometry. While these proxy methods can often reproduce the underlying ab-initio method with high fidelity and scale to millions of atoms , they are fundamentally limited by the accuracy of their reference method and need many high-accuracy samples for training, reiterating the need for scaleable, high-accuracy reference methods.

To enable DL-VMC methods to efficiently compute wavefunctions for many molecules, several methods have been proposed to amortize the cost of optimization, by learning a single wavefunction across multiple systems. This has been demonstrated to work for different geometries of a single molecule , and recently two approaches have been proposed to learn wavefunctions across entirely different molecules, each with their own limitations. Gao et al.  reparameterized the orbitals of a wavefunction, by using chemistry-inspired heuristics to determine orbital positions, managing to efficiently generalize wavefunctions across different geometries of a single molecule. However, when learning a single wavefunction across different molecules, their results deteriorated and transfer to new molecules proved difficult. Scherbela et al.  do not require heuristic orbital positions, but instead use orbital descriptors of a low-accuracy conventional method to parameterize DL-VMC orbitals. However, while their wavefunction ansatz successfully transfers to new molecules, their method requires a separate, iterative Hartree-Fock (HF) calculation for every new geometry.

Our contributionThis work presents the first end-to-end machine learning approach, which successfully learns a single wavefunction across many different molecules with high accuracy. Our contributions are:* A transferable neural wavefunctions, which requires neither heuristic orbital positions, nor iterative HF calculations. We achieve this by building on the architecture by Scherbela et al.  and an orbital prediction model by Unke et al. .
* A simplified and improved electron embedding architecture, leveraging expressive nuclear features from our orbital prediction model and a message passing step between nuclei.
* A chemically diverse dataset with up to 100 molecules based on QM7-X , a data augmentation method based on normal mode distortions, and successful training of a neural wavefunction on these with continuously sampled geometries. Additionally, we improve the initialization of electrons around the molecule, reducing the computational overhead.
* As a final result, an accurate neural wavefunction, which shows for the first time zero-shot capabilities, i.e. high-accuracy energy predictions without additional optimization steps, on new systems. In particular it achieves better absolute energies than well established, high-accuracy gold-standard reference methods, such as CCSD(T)-3Z on unseen systems without any finetuning (cf. Fig. 2a).

Overview of the paperIn Sec. 2 we outline our method and the procedure to optimize a transferable wavefunction across molecules. In Sec. 3 we thoroughly test the accuracy of our obtained wavefunction, by analyzing absolute energies (Sec. 3.1), relative energies (Sec. 3.2), and the impact of design choices in an ablation study (Sec. 3.3). Throughout this work, we compare against other high-accuracy methods, in particular results obtained by state-of-the-art DL-VMC methods and CCSD(T). Finally we analyze the scalability and limitations of our base model, by applying it to a large-scale dataset in Sec. 3.4, before a discussion and outlook for future research in Sec. 4.

## 2 Methods

Our approach is divided into two parts (cf. Fig. 1): On the one hand, a wavefunction ansatz, containing an electron embedding, orbital embedding and a Slater determinant. On the other hand a method for geometry sampling based on normal-mode distortions.

### Our wavefunction ansatz

A single forward-pass for our wavefunction model

\[=_{d=1}^{N_{}}_{k}^{d}(_{i}^{}),_{k}^{d}(_{i}^{})=_{I=1}^{N_{}}_{i}^{},_{Ikd}^{} e^{-r_{i.1}g_{Ikd}^{}}, i,k=1,,n_{} \]

can again be divided into three blocks: An electron model acting on nuclear and electron coordinates, generating electron embeddings \(^{}\); an SE(3)-equivariant orbital model acting only on nuclear coordinates, generating orbital embeddings \(g^{}\) and \(^{}\); a Slater determinant combining electron- and orbital-embeddings, and ensuring anti-symmetry of the wavefunction.

Figure 1: **Overview of our approach: Wavefunction ansatz (top) and geometry sampling (bottom)**

Message passing neural networkThroughout this work we use message passing neural networks (MPNN), to operate on the graph of particles which are connected by edges containing information about their relative positions. The electron-electron, electron-nuclear and nuclear-nuclear edges are embedded with a multi-layer perceptron (MLP),

\[_{ij}^{}=[_{ij},r_{ij}] {e}_{iI}^{}=[_{iI},r_{iI}] {e}_{IJ}^{}=[_{IJ},R_{IJ}], \]

by using a concatenation (\([]\)) of distance and difference vectors, and separate weights for each MLP. A single message passing step is decomposed into the following operations

\[}_{i}^{} =(_{i}^{},\{_{j}^{ }\},\{_{ij}\}) \] \[=(_{i}^{})+_{j}(_{j}^{})(_{ij}) \]

for a receiving particle \(}_{i}^{}\) and the set of sending particles \(\{_{j}^{}\}\), connected via their edges \(\{_{ij}\}\). By \(\) we denote the non-linear activation and with \(\) the element-wise multiplication along the feature dimension. An MPNN is obtained by stacking MessagePassing layers

\[(_{i},\{_{j}\},\{_{ij}\})= ((_{i},\{_{j}\},\{_{ij}\})) \]

In the following we use these message passing steps to model all inter-particle interactions.

SE(3)-equivariant orbital modelThe orbital model is a simplified version of PhisNet, a neural network predicting the overlap matrix \(\) and the Fock matrix \(\), via nuclear embeddings \(^{}\):

\[^{}=_{}(,)_{IJ}=s_ {}(_{I}^{},_{J}^{})_{IJ}=f_{ }(_{I}^{},_{J}^{}). \]

Here \(^{}^{N_{}(L+1)^{2} N_{ }}\), and \(_{IJ}\) and \(_{IJ}\) are each in \(^{N_{} N_{}}\). The basis-set size of the predicted orbitals is denoted by \(N_{}\) and the feature dimension of the nuclear embeddings by \(N_{}\). The full overlap- and Fock-matrices are assembled from the corresponding blocks \(_{IJ}\) and \(_{IJ}\), leading to matrices of shape \([N_{}N_{} N_{}N_{}]\). Each layer of PhisNet is SE(3)-equivariant, ensuring that any 3D-rotation or inversion of the input coordinates \(\), leads to an equivalent rotation of its outputs. This is done by splitting any feature vector into representations of varying harmonic degree \(l=0,,L\), each with components \(m=-l,,l\). A detailed description of SE3-equivariant networks in general, as well as PhisNet in particular can be found in . A list of changes and simplifications we made to PhisNet can be found in Appendix C.

The orbital embeddings (corresponding to orbital expansion coefficients in a conventional quantum chemistry calculation) are obtained by solving the generalized eigenvalue problem:

\[_{k}=_{k}_{k}}_{Ik}^{}=(_{k},[N_{},N_{}])_{I} \]

It has been shown empirically that it is beneficial to obtain the orbital coefficients \(}_{Ik}^{}\) as solutions to this generalized eigenvalue problem, rather than predicting them directly  as functions of \(\) and \(\). This is because the orbital coefficients are neither unique, nor do they share the molecule's symmetry. The matrices \(\) and \(\) on the other hand are unique and transform equivariantly under E(3)-transformations of the molecule, leading to a well defined learning problem.

Following , we do not use the orbital energies \(_{k}\) and obtain the backflow factors \(\) and exponents \(\), by first localizing the resulting orbitals using the Foster-Boys localization  (cf. Appendix B) and subsequently using an MPNN and MLP acting on the orbital embeddings.

\[}_{Ik}^{} =_{n=1}^{N_{}}U_{kn}^{}}_{In}^{ }, _{Ik}^{}=(}_{Ik}^{},\{}_{Jk}^{}\},\{_{IJ}^{}\}) \] \[_{Ik}^{} =(_{Ik}^{}), _{Ik}^{}^{N_{} N_{ } N_{} N_{}}\] (11) \[g_{Ik}^{} =(_{Ik}^{}), ^{}^{N_{} N_{} N_{}} \]

Electron modelThe electron embedding is a message passing neural network. To incorporate the geometric information of the molecule considered, we leverage the equivariant prediction of the PhisNet nuclear embeddings \(^{}\), by first performing a message passing step between the nuclear embeddings

\[}_{I}^{}=(_{I}^{})}_{I}^{}=(}_{I}^{},\{ }_{J}^{}\},\{_{IJ}^{}\})\]and then using these features to initialize the electron embeddings

\[_{i}^{,0}=(,\{}_{j}^{}\},\{_{i,j}^{}\}),\]

by using a zero vector \(\) for the initial receiving electrons. This differentiates our electron model from previously proposed methods [3; 6; 8], leading to a better generalization when optimized across molecules (cf. Sec. 3.3). The final step of the embedding is a multi-iteration message passing between electron embeddings to capture the necessary electron-electron interaction

\[_{i}^{}=(_{i}^{,0},\{_{j}^{ {el},0}\},\{_{ij}^{}\}),\]

resulting in a \(N_{}\)-dimensional representation for each electron \(_{i}^{}^{N_{}},i=1,,n_{}\).

Overall E(3)-equivarianceLike existing approaches [3; 8; 17] our overall wavefunction does not enforce E(3)-symmetry. This is because the wavefunction can have lower symmetry than the molecule , for example in the case of the excited states of a hydrogen atom. We therfore choose all parts of the network that act purely on nuclear coordinates (i.e. the PhisNet model and the energy/hessian estimate) to be equivariant under E(3)-transformations. All parts of the network acting on electron coordinates (in particular the electron model) break this symmetry by depending explicitly on the cartesian coordinates of the electrons. The architecture is thus only invariant under translations, but not under rotations or inversions. We bias the model towards approximately invariant energies using data augmentation as discussed in Sec. 2.3.

### Sampling

Markov Chain Monte Carlo (MCMC) sampling of electron positionsWe use MCMC to draw samples \(\) from the probability distribution \(()^{2}\), to evaluate the expectation value of Eq. 2. One notable difference compared to other works is our initialization \(^{0}\) of the Markov Chain. In the limit of infinite steps, the samples are distributed according to \(^{2}\), but for a finite number of steps the obtained samples strongly depend on \(^{0}\). This issue is typically addressed by a "burn-in", where MCMC is run for a fixed number of steps (without using the resulting samples) to ensure that \(\) has diffused to state of high probability. Previous work has initialized \(^{0}\) using a Gaussian distribution of the electrons around the nuclei. We find that this initialization is far from the desired distribution \(^{2}\) and thus requires \( 10^{5}\) MCMC steps to reach the equilibrium distribution. We instead initialize \(^{0}\) by samples drawn from an exponential distribution around the nuclei, which much better approximates the correct distribution and thus equilibrates substantially faster (cf. Appendix A). We find that exponential initialization reduces the required number of burn-in steps by ca. 50%, reducing the computational cost of a 500-step zero- shot evaluation by ca. 5%.

Normal mode sampling of geometriesSince DL-VMC is an ab-initio method, we do not require a labeled dataset of reference energies, but to obtain a transferable wavefunction, which generalized well to new systems, a diverse dataset of molecular geometries \(\) is required. Starting with an initial set of geometries \(^{0}\), we update \(\) on the fly, by perturbing each geometry every 20 optimization steps by adding random noise \(\) to the nuclear coordinates. Using uncorrelated, isotropic random noise for \(\) would yield many non-physical geometries \(^{}\), since the stiffness of different degrees of freedom can vary by orders of magnitude. Intuitively we want to make large perturbations along directions in which the energy changes slowly, and vice-versa. We achieve this by sampling \(\) from a correlated normal distribution

\[((^{0}-),_{}^{-1})^{}=+. \]

The bias term \((^{0}-)\) ensures that geometries stay sufficiently close to their starting point \(^{0}\). The covariance matrix is chosen proportional to the pseudo-inverse of the hessian of the energy \(E^{}\), which is predicted from the scalar component of the nuclear embeddings using a pre-trained MLP. By using \(_{}^{-1}\) as covariance matrix, we take large steps along soft directions and small steps along stiff directions, thus avoiding unphysical geometries with very high energies.

\[E^{}=_{I=1}^{N_{}}(_{I}^{} ), H_{I,J}^{}=E^{}}{ _{I}_{J}}, I,J=1 N_{} ,=1 3 \]After distorting the nuclear coordinates \(\), we also adjust the electron positions \(\), using the space-warp coordinate transform described in , which effectively shifts the electrons by a weighted average of the shift of their neighbouring nuclei. In addition to this distortion of the molecule, we also apply a random global rotation to all coordinates, to obtain a more diverse dataset.

### Optimization

To obtain orbital descriptors (and energies to calculate the hessian of the energy) we pre-train PhisNet against the Fock matrix, the overlap matrix, the energy and the forces of Hartree-Fock calculations in a minimal basis set across 47k molecules. Further details of the loss function, dataset, and the adaptions to PhisNet can be found in Appendix C. For all subsequent experiments, we freeze the parameters of PhisNet. A full DL-VMC calculation to obtain a ground-state energy prediction can be divided into three consecutive steps:

1. **Supervised optimization using PhisNet**: Initially, the neural-network orbitals (cf. Eq. 3) are optimized to minimize the residual against orbitals obtained from PhisNet. It ensures that the initial wavefunction roughly resembles the true ground-state and is omitted when fine-tuning an already optimized base model.
2. **Variational optimization**: Minimization of the energy (cf. Eq. 2) by drawing samples from \(^{2}\) using MCMC and updating the wavefunction parameters using the KFAC optimizer .
3. **Evaluation**: For inference of the ground-state energy, we sample electron positions using MCMC, and evaluate the energy using Eq. 2 without updating \(\).

To train a multi-geometry transferable wavefunction we further divide the variational optimization of a neural wavefunction into two steps:

1. **Pre-training**: A single wavefunction model is trained across many molecules and geometries. In every gradient step we only consider a single geometry per batch. The next geometry to optimize is chosen based on the energy variance as proposed by . To sample continuously the space of molecular geometries we distort each geometry every 20 optimization steps as described in Sec. 2.2. We refer to evaluations of this model on new systems as "zero-shot".
2. **Fine-tuning**: A a small number of additional variational optimization steps is done using geometries of interest, starting from the weights of a pre-trained base model. This procedure yields a model that is specialized to the molecule at hand and typically yields more accurate energies on the specific problem than the raw pre-trained model.

## 3 Results

We pre-train our wavefunction model on a dataset of 98 molecules (699 conformers) for 256k optimization steps using the architecture and training procedure outlined in Sec. 2. Below we demonstrate the performance of this model, for zero-shot evaluations and after subsequent fine-tuning.

### Accuracy of pre-trained model for absolute energies

To test the transfer capabilities of the model, we evaluate it on test-sets, which each contain 4 randomly chosen and perturbed molecules, grouped by molecule size (measured as the number of non-Hydrogen atoms). To avoid train/test leakage, we excluded all molecules that are part of these test sets from the training set (cf. Appendix D). Although the model has only been trained on molecules containing up to 4 heavy atoms, we evaluate its performance across the full range up to 7 heavy atoms. We find that for molecules containing up to 6 heavy atoms our method outperforms CCSD(T) with a 2Z basis set and outperforms CCSD(T) with a 4Z basis set after only 4k fine-tuning steps. This is a large improvement over the state of the art: Zero-shot evaluations by Gao et al.  did not manage to outperform a Hartree-Fock baseline, even on the toy system of Hydrogen-chains. Similarly, Scherbela et al.  achieve high accuracy after fine-tuning, but result that are worse than Hartree-Fock in a zero-shot setting. In contrast, our improvements to their method increase zero-shot accuracy by more than 2 orders of magnitude.

We furthermore find that fine-tuning pre-trained wavefunctions can be more cost effective than well established conventional methods. For a typical molecule containing 5 heavy atoms, we require 9.5 node-hours for 4k fine-tuning steps, achieving accuracy that surpasses CCSD(T)-4Z. In contrast, CCSD(T)-4Z requires 10.5 node-hours and 16k steps of PsiFormer (achieving the same accuracy), require 53 node-hours. All run-times are listed in Tab. 3 of Appendix I.

To further test the accuracy of our method, we evaluate for 3 heavy atoms the performance with increasing fine-tuning steps (cf. Fig. 2c). We compare our work (with and without fine-tuning) against CCSD(T) and reference calculations done with state-of-the-art DL-VMC methods . For up to 16k optimization steps, our pre-trained model yields energy errors that are 1-2 orders of magnitude lower than other reference methods. After longer optimization the method by Gerard et al.  and PsiFormer  surpass our predictions. We hypothesize that this is not to blame on pre-training, but that our orbital prediction framework, which allows us to optimize across molecule, yields less expressive wavefunctions than a fully trainable backflow. This is demonstrated by the fact that a pre-trained and non-pre-trained model converge to the same energy in the limit of long optimization.

Like the absolute energies, also the variance of the local energies (another measure of wavefunction accuracy) is substantially improved by our method and results are depicted in Appendix E.

### Accuracy for relative energies

While Sec. 3.1 demonstrates high accuracy for absolute energies, we find that relative energies (being the small difference between two large absolute energies) can be unsatisfactory in the zero-shot regime, and a small number of fine-tuning steps is required to reach quantitatively correct relative energies. Fig. 3 demonstrates this issue on 4 distinct systems, each highlighting a different challenge for our model. For each system, we evaluate our pre-trained base model without any system specific optimization (zero-shot), and after 4000 fine-tuning steps. The fine-tuning optimization is done separately for each of the 4 systems, analogously to Sec. 2.3, yielding 4 distinct wavefunctions that each represent the ground-state wavefunctions of all considered geometries per system. Results after more fine-tuning steps can be found in Appendix F.

Bicyclobutane conformersFig. 3a depicts the energies of 5 conformers of bicyclobutane relative to the energy of its initial structure. The system is of interest, because CCSD(T) severely underestimates the energy of the dis-TS conformer by \( 60\) mHa . While our zero-shot results yield the correct sign for the relative energies, they are quantitatively far off from the gold-standard DMC reference calculation , in particular for the dis-TS geometry, where we overestimate the relative energy

Figure 2: **Absolute energies**: Energies relative to CCSD(T)-CBS (complete basis set limit) when re-using the pre-trained model on molecules of varying size without optimization (a) and after fine-tuning (b). (c) depicts energy for the test set containing 3 heavy atoms as a function of optimization steps and compares against SOTA methods. Solid lines are with pre-training, dashed lines without. Gray lines correspond to conventional methods: Hartree-Fock in the complete basis set limit (HF-CBS), and CCSD(T) with correlation consistent basis basis sets of double to quadruple valence (CC-nZ).

by 90 mHa. However when fine-tuning our model for only 700 steps per geometry (4k total), we obtain relative energies that are in close agreement with DMC (max. deviation 2.1 mHa). Comparing to FermiNet  we find that our results are more accurate than a FermiNet calculation after 10k steps (requiring twice our batch size; max. deviation 7.5 mHa), and slightly less accurate than a FermiNet calculation optimized for 200k steps (max. deviation 1.4 mHa). As opposed to CCSD(T) our model does not suffer from systematic errors, even for the challenging dis-TS geometry. A table of all relative energies can be found in Appendix F.

Nitrogen dissociationWhen evaluating the energy of an \(N_{2}\) molecule at various bond-lengths we obtain high zero-shot accuracy near the equilibrium geometry (\(d=2.1\) bohr; in training set), but substantially lower accuracy at smaller or large bond lengths (cf. Fig. 3b). This is due to the lack of dissociated atoms in the pre-training dataset and again mostly remedied by finetuning. In the most challenging regime around \(d=3.7\), even with fine-tuning we obtain energies that are 12 mHa above high-accuracy results obtained by Gerard et al. , indicating the need for longer fine-tuning.

Global rotation of proapadieneEnergies of molecules are invariant under global translation or rotation of all particle positions. The wavefunction however is not invariant and neither is our wavefunction ansatz, which can lead to different energies for rotated copies of a molecule. When evaluating the energy of our model on 20 copies of proadiene (C\({}_{3}\)H\({}_{4}\)) rotated around a random axis, we find typical energy variations of \( 1\) mHa, but also a individual outliers, deviating by up to 5 mHa. This highlights a dilemma facing all existing DL-VMC models: On the one hand, constraining the wavefunctions to be fully invariant under rotation (or even just invariant under symmetries of the Hartree-Fock orbitals) is too restrictive to express arbitrary ground-state wavefunctions . One the other hand, our approach of biasing the model towards rotation-invariant energies by data augmentation, appears to be helpful but not to be fully sufficient. When evaluating an earlier checkpoint of our base model (trained for 170k epochs instead of 256k), we observe mean energy variations of \( 3\) mHa and outliers of up to 30 mHa, indicating the positive impact of prolonged training with data augmentation. We again find 4000 fine-tuning steps split across all geometries are sufficient to reduce errors below chemical accuracy of 1.6 mHa.

In addition to augmenting data during training time, we can also augment the data during inference time, to obtain fully rotation-invariant energies, from an approximately rotation invariant model. We

Figure 3: **Challenging relative energies**: Relative energies obtained with and without fine-tuning on 4 distinct, challenging systems, compared against high-accuracy reference methods. a) Relative energy of bicyclobutane conformers vs. the energy of bicyclobutane b) Potential energy surface (PES) of N\({}_{2}\), c) global rotation of proapadiene d) relative energy of twisted vs. untwisted proapadiene.

achieve this by randomly rotating the molecule at every inference step and averaging across all rotated geometries. Since the Monte-Carlo estimate is already an average across many different samples anyways, this comes at no additional computational cost. Fig. 2(c) shows that with rotation averaging we obtain energies that are fully invariant, up to Monte-Carlo noise.

Twisted propadieneTwisting one of the C=C bonds of propadiene leads to a transition state with an energy difference of 110 mHa. Evaluating the energy without fine-tuning on an equidistant grid of torsion angles, we obtain the correct barrier height (112 mHa), but also deviations of up to 40 mHa. During pre-training we purposefully sampled twisted molecules, but only included equilibrium geometries, transition geometries, and one intermediate twist (cf. Appendix D). This seems sufficient for correct zero-shot barrier heights, but insufficient for high accuracy along the full path. Short fine-tuning (4k steps distributed across all 10 geometries) yields excellent agreement with CCSD(T): \(\)0.2 mHa discrepancy for the barrier height and a maximum deviation of 2 mHa along the path.

### Ablation studies

To analyze the relative importance of our changes, we break down the accuracy gap between our work and the prior work by Scherbela et al.  in Fig. 4. We start with their model checkpoint trained for 128k epochs on their dataset consisting of 18 different compounds. Next, we train a model using our improved architecture with their dataset and methodology, already reducing the zero-shot error by 76% and fine-tuning error by 35%. The next model additionally uses normal-mode distortions to augment the training dataset, decreasing the fine-tuning error by another 12%. Additionally increasing the training set size to our dataset containing 98 molecules and the corresponding torsional conformers, reduces the zero-shot error by 98% and yields modest improvements in the fine-tuning case. Increasing the number of determinants from 4 to 8 and increasing the number of pre-training steps from 128k to 256k improve fine-tuning accuracy by 8% and 21% respectively. The two largest contributions overall are the improved architecture - yielding substantial gains both in zero-shot and fine-tuning - as well as the larger training set, which is crucial for high zero-shot accuracy. This is consistent with , which found that both model size and training set size do improve performance, while pre-training duration shows diminishing returns after 256k steps. Since we pre-train the PhisNet model against Hartree-Fock references and do not update its parameters during optimization, the PhisNet model by itself contributes no substantial accuracy improvement. It impacts energy predictions indirectly, by providing pre-trained nuclear embeddings \(^{}\) and enabling fast geometry distortions and rotations during training and inference, which in turn improve accuracy.

### Large-scale experiment

To showcase the scalability of our approach, we evaluate zero-shot predictions of the absolute energies for a subset of 250 molecules from the QM7 dataset , containing molecules with 14-58 electrons.

Figure 4: **Ablation study**: Breakdown of absolute energy difference between Scherbela et al.  and our work, when evaluating the respective base models on the test set consisting of 3 heavy atoms. Panel a shows accuracy gains for zero-shot evaluation, panel b the accuracy after 4k fine-tuning steps. First and last row depict their and our absolute energy respectively, intermediate bars show the subsequent improvements of various changes in mHa (and %). Note that panel a. is plotted on a logarithmic scale due to the large energy difference.

Since CCSD(T) calculations would be prohibitively expensive for a dataset of this size, in Fig. 5a we compare against density functional theory (DFT) reference calculations (PBE0+MBD, ). Fig. 5b breaks down the energy differences compared to DFT, by molecule size. For molecules with less than 5 heavy atoms we obtain energies that are lower than DFT by 10-120 mHa. Because our ansatz is variational (in contrast to DFT), our lower energies translate to a more accurate prediction of the true ground-state energy, confirming again our high zero-shot accuracy. With increasing system size the accuracy deteriorates, due to increasing extrapolation and out-of-distribution predictions, consistent with our results in Sec. 3.1. One reason for the large energy differences in larger molecules is the increasing inter-atomic distance within the molecules with increasing number of atoms (cf. Fig. 5c). While the largest inter-atomic distance observed in the training set is 11 bohr, the evaluation set contains distances up to 17 bohr. This issue could be overcome by employing a distance cutoff, as it is already applied in supervised machine-learned potential energy surface predictions [19; 33] or has just recently been incorporated into a neural wavefunction  via an exponential decay with increasing inter-particle distance. Another potential solution is to include larger molecules or separated molecule fragments in the pre-training molecule dataset, reducing the extrapolation regime.

## 4 Discussion

We have presented to our knowledge the first ab-initio wavefunction model, which achieves high-accuracy zero-shot energies on new systems (Sec. 3.1 and Sec. 3.4). Our pre-trained wavefunction yields more accurate total energies than CCSD(T)-2Z across all molecule sizes and outperforms CCSD(T)-3Z on molecules containing up to 5 heavy atoms, despite having been trained only on molecules containing up to 4 heavy atoms. We find that relative energies of our model are qualitatively correct without fine-tuning, but need on the order of 4000 fine-tuning steps to reach chemical accuracy of 1.6 mHa (Sec. 3.2). This is a substantial improvement over previous work, which so far has fallen in two categories: High-accuracy ansatze (such as [3; 6]) that cannot generalize across molecules and thus need ca. 10x more compute to reach the same accuracy, or methods that can generalize ([17; 25]), but yield orders of magnitude lower accuracy in the zero- or few-shot regime. We demonstrate in Sec. 3.3 that these improvements are primarily driven by an improved architecture (containing a more expressive electron embedding and an ML-based orbital model), improved geometry sampling, and a larger training dataset.

While results are encouraging in the zero- and few-shot regime, open questions for further research abound. The most pressing issues are currently limited zero-shot accuracy for relative energies, and potentially limited expressiveness of the ansatz in the regime of very long optimization. Zero-shot accuracy could be further improved by training on an even larger dataset, further improved geometry sampling (in particular of torsion angles), and an interaction cut-off to avoid previously unseen particle pairs for new large molecules. Furthermore SE(3)-symmetry of the wavefunction should be explored further, since currently only the orbital part of our architecture is SE(3)-equivariant. We experimented with a fully equivariant architecture, but found the resulting wavefunctions to not be expressive enough. To improve overall accuracy, attention based embeddings [6; 34] could be pursued. Additionally, we currently freeze the weights of the orbital embedding to simplify the architecture and avoid back-propagation through the iterative orbital localization procedure. Optimizing these weights in addition to the electron embedding will lead to a more expressive ansatz.

Figure 5: **Zero-shot on QM7: a) Our zero-shot energies vs. DFT  b) Histogram of energy residuals (truncated at 1.25 Ha for clarity) c) Residuals vs. largest inter-molecular distance \(R_{IJ}\).**