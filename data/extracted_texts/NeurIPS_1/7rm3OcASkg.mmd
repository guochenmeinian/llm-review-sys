# DP-Mix: Mixup-based Data Augmentation for Differentially Private Learning

Wenxuan Bao\({}^{1}\)  Francesco Pittaluga\({}^{2}\)  Vijay Kumar B G\({}^{2}\)  Vincent Bindschaedler\({}^{1}\)

\({}^{1}\) University of Florida \({}^{2}\) NEC Labs America

###### Abstract

Data augmentation techniques, such as image transformations and combinations, are highly effective at improving the generalization of computer vision models, especially when training data is limited. However, such techniques are fundamentally incompatible with differentially private learning approaches, due to the latter's built-in assumption that each training image's contribution to the learned model is bounded. In this paper, we investigate why naive applications of multi-sample data augmentation techniques, such as mixup, fail to achieve good performance and propose two novel data augmentation techniques specifically designed for the constraints of differentially private learning. Our first technique, DP-MixSELF, achieves SoTA classification performance across a range of datasets and settings by performing mixup on self-augmented data. Our second technique, DP-MixDHF, further improves performance by incorporating synthetic data from a pre-trained diffusion model into the mixup process. We open-source the code at [https://github.com/wenxuan-Bao/DP-Mix](https://github.com/wenxuan-Bao/DP-Mix).

## 1 Introduction

Differential privacy (DP)  is a well-established paradigm for protecting data privacy, which ensures that the output of computation over a dataset does not reveal sensitive information about individuals in the dataset. However, training models with DP often incur significant performance degradation, especially when strong privacy guarantees are required or the amount of training data available is limited.

One class of techniques to overcome limited training data in classical (non-private) learning is data augmentation. Unfortunately, the analysis of differentially private learning mechanisms requires that the influence of each training example be limited, so naively applying data augmentation techniques is fundamentally incompatible with existing differential private learning approaches. Therefore, new approaches are required to leverage data augmentation.

Recently, De et al.  showed one way to incorporate simple data augmentation techniques, such as horizontal flips and cropping, into differentially private training. Their approach creates several _self-augmentations_ of a training example and averages their gradients before clipping. This procedure is compatible with differential privacy because each training example only impacts the gradients of the self-augmentations which are aggregated together before clipping, thus preserving the sensitivity bound. However, this requires using data augmentation techniques that operate on a single input example, thus excluding large classes of multi-sample techniques such as mixup  and cutmix . Furthermore, as shown in  their approach quickly hits diminishing returns after \(8\) or \(16\) self-augmentations.

In this paper, we leverage multi-sample data augmentation techniques, specifically mixup, to improve differentially private learning1. We first show empirically that the straightforward application of mixup, i.e., using microbatching, fails to improve performance, even for modest privacy budgets. This is because microbatching inherently requires a higher noise level for the same privacy guarantee.

To overcome this, we propose a technique to apply mixup in the per-example DP-SGD setting by applying mixup to self-augmentations of a training example. This technique achieves SoTA classification performance when training models from scratch and when fine-tuning pre-trained models. We also show how to further enhance classification performance, by using a text-to-image diffusion-based model to produce class-specific synthetic examples that we can then mixup with training set examples at no additional privacy cost.

## 2 Background & Related Work

### Differential Privacy & DP-SGD

Differential privacy (DP)  has become the standard framework for provable privacy guarantees.

**Definition 1** (\((,)\)-Differential Privacy).: _A mechanism \(\) (randomized algorithm) satisfies \((,)\)-differential privacy if for any neighboring datasets \(D\), \(D^{}\) and any output set \(S()\), we have: \(((D S)()((D^{} S)+ \)._

In the definition \(>0\) is called the privacy budget and \(0<<1\). Datasets \(D,D^{}\) are neighboring if one can be obtained from the other by adding or removing one data point.

DP-Sgd.To train a machine learning model with differential privacy (Definition 1), we use Differentially Private Stochastic Gradient Descent (DP-SGD). DP-SGD was proposed by Abadi et al.  as a direct replacement for SGD. Informally, for differential privacy to be satisfied, each weight update must hide the presence or absence of any example in the training set (and thus the minibatch). Consider a minibatch \(B\) with \(n\) examples. The weight update equation of (minibatch) SGD is: \(w_{j+1}=w_{j}- g_{j}(B)\), where \(g_{j}(B)=_{w}l(B;w_{j})\) is the gradient of the loss on \(B\) with

Figure 1: **Approach Overview**. Illustration of ur proposed methods for applying mixup under differential privacy: DP-MixSELF and DP-MixDHF.

respect to the parameters \(w\) and \(\) is the learning rate. Remark that inclusion or exclusion of a single example in minibatch \(B\) can completely change the minibatch aggregate gradient \(g_{j}(B)\). To avoid this and satisfy differential privacy, DP-SGD first clips individual gradients and then adds Gaussian noise to the average gradient:

\[(B)=[_{i=1}^{n}_{C}\{g_{i}\}+(0,C^{2}^{2})]\, \]

where \(g_{i}\) is the gradient of the loss on example \(i\) of minibatch \(B\) with respect to the parameters and \(_{C}\{g\}=g\{1,}\}\), where \(C>0\) is a constant. By clipping individual example gradients to norm \(C\), the sensitivity of the gradient sum to adding or removing an example is guaranteed to be at most \(C\). After that, adding Gaussian noise with standard deviation \(C\) satisfies DP because it is an application of the Gaussian mechanism . The noise level \(\) is calibrated to the privacy parameters \(\) and \(\).

Since the gradient update only touches the minibatch \(B\), which was sampled from the entire training dataset, we can use amplification by sampling theorems  with a sampling rate of \(n/N\) where \(N\) is the size of the dataset and \(n=|B|\). To obtain a differential privacy guarantee over the entire training procedure (which potentially touches each training example multiple times), we apply composition results . The full procedure is presented in Algorithm 2 in the supplementary material.

Microbatch DP-SGD.Abadi et al.  proposed DP-SGD with per-example gradient clipping. But other researchers have proposed microbatch clipping instead , which views a minibatch \(B\) as composed of \(m\) microbatches, each of \(n/m\) examples. With microbatch clipping, we clip the average of the microbatch's gradients instead of the per-example gradients. Note that using microbatches of size \(1\) is equivalent to per-example clipping.

The noisy gradient for a microbatch of size \(b>1\) (assuming \(b\) divides \(n\)) is:

\[=[_{i=1}^{n/b}_{C} _{j=1}^{b}g_{i,j}}+(0,C^{2}^{2})]\, \]

where \(g_{i,j}\) denotes the gradient vector the \(j^{}\) example in the \(i^{}\) microbatch.

The drawback of this approach is that despite clipping microbatches gradients with a clipping norm of \(C\), the sensitivity of adding or removing one example is actually \(2C\) (as pointed out recently by Ponomareva et al. ). On a per-example basis, more noise is added.

Data Augmentation and DP.Data augmentation cannot be straightforwardly applied with differential privacy because the privacy analysis requires a bound on the sensitivity of any example in the training set. For example, naively applying data augmentation prior to training, by say creating \(k\) augmentations of each training set example, compromises the privacy guarantee. Existing privacy analyses do not apply because adding or removing an example in the training set now potentially affects the gradients of \(k\) examples in each training epoch.

De et al.  proposed a way to use _single-sample_ data augmentation. Their idea is to do per-example clipping but to create augmentations of each example (e.g., horizontal flip and cropping) and average the corresponding gradient before clipping. This is compatible because each training set example still only affects one term of the sum of Eq. (1). However, this idea cannot be used for more sophisticated _multi-sample_ data augmentations techniques like mixup  that take two or more examples as input to create each augmentation.

### Related work.

SoTA performance in Differential Privacy Machine Learning.Recent studies [9; 5; 34; 22; 27] achieve new SoTA performance in differentially private learning using diverse methods. De et al.  modified a Wide-ResNet model's training method, advocating large batches, group normalization, weight standardization, and self-augmentation, achieving SoTA performance on CIFAR-10 with DP-SGD. Sander et al.  introduced a technique using Total Amount of Noise (TAN) and a scaling law to reduce the computational expense of hyperparameter tuning in DP-SGD, achieving SoTAperformance on ImageNet and computational cost reductions by a factor of 128. Bu et al.  presented mixed ghost gradient clipping, which eliminates per-sample gradient computations, approximates non-private optimization, and achieves SoTA performance on the CIFAR-10 and CIFAR-100 datasets by fine-tuning pre-trained transformer models. Panda et al.  propose an accelerated fine-tuning method, which enables them to achieve SoTA performance on the CIFAR-10 and CIFAR-100 datasets.

Mixup techniques.Mixup was initially proposed by Zhang et al. . This technique is applied to raw input data and its corresponding one-hot encoded labels. Experimental results presented in their work indicate that Mixup enhances model performance and robustness. However, Guo et al.  discovered that input mixup may encounter the manifold intrusion problem when a mixed example coincides with a real sample. To address this, they proposed Adaptive Mixup, which they demonstrated outperforms the original method. To mitigate the incongruity in Mixup data, Yun et al.  proposed Cutmix, which replaces a portion of one sample with another. Instead of applying mixup at the input level, Verma et al.  implemented mixup within the intermediate layers of the model. Several works have utilized Mixup in the context of Differential Privacy (DP), such as Borgnia et al. , which used it as a defense against backdoor attacks, and Xiao et al. , which used it to introduce randomness into the training process, or in other settings such as Split learning . To the best of our knowledge, no previous work has employed Mixup as a data augmentation strategy to enhance model performance in differentially private learning.

Diffusion models.Diffusion models  have emerged as a powerful class of generative models that leverage the concept of stochastic diffusion processes to model complex data distributions, offering a promising alternative to traditional approaches such as Generative Adversarial Networks (GANs)  and Variational Autoencoders (VAEs) . By iteratively simulating the gradual transformation of a simple initial distribution into the desired target distribution, diffusion models provide a principled framework for generating high-quality samples however, the implicit guidance results in a lack of user control over generated images. Recently there has been a lot of interest in text-to-image models  due to their ability to generate entirely new scenes with previously unseen compositions solely based on natural language. In this work, we use class labels as text input to the text-to-image diffusion model  to generate diverse synthetic samples for augmenting the dataset.

## 3 Method

Given two examples \((x_{0},y_{0})\) and \((x_{1},y_{1})\) and mixing parameter \(\), mixup creates augmentations as:

\[x= x_{0}+(1-) x_{1}\ y=  y_{0}+(1-) y_{1}. \]

In our experiments, \(\) is sampled from a \(\) distribution with \(=0.2\). In other words, the augmented example image \(x\) is simply a linear combination of two individual examples \(x_{0}\), \(x_{1}\) and its label \(y\) is also a linear combination of the two individual examples 'labels (one-hot encoded). Typically the two examples \((x_{0},y_{0})\) and \((x_{1},y_{1})\) are selected randomly from the training data. To use mixup while preserving differential privacy, we must find a way to account for its privacy impact.

For DP training, we cannot use Eq. (3) to create an augmented training set (e.g., by repeatedly taking two input examples from the sensitive training set and mixing them up) without affecting the privacy budget, as the sensitivity of the clipped-gradients sum of the augmented minibatch would then be (at least) \(2C\) (and not \(C\)). To see why, observe that an original example \(z\) would impact multiple terms of the sum in Eq. (1) (the one involving the gradient of \(z\) and also the one(s) involving the gradient of any mixed-up pair(s) involving \(z\)). Even with exactly one mixed-up pair per original example, adding or removing an example from the training set would impact two terms in the clipped-gradients sum, potentially changing the aggregated gradient by \(2C\). In a pathological case, the sum could go from \(C\) for some \(\) to \(-C\). The sensitivity being \(2C\) means that the scale of Gaussian noise added must be doubled to obtain the same privacy guarantee as we would have without mixup.

We empirically investigate the impact of doubling the noise for training a WRN-16-4 model on CIFAR-10. Specifically, we use a microbatch size of \(2\) and do input mixup by applying Eq. (3) to the two examples in each microbatch. We then add the mixed up example to the microbatch or let the microbatch consist only of the mixed up example. Then, following microbatch DP-SGD, we clip the gradient of the microbatch and add Gaussian noise to the aggregated gradient sum. Nevertheless, it could be that empirically, the benefits of mixup outweigh the additional noise. Results in Table 1 show that despite mixup providing a clear benefit (compared to traditional DP-SGD) the microbatch size 2 setting is inherently disadvantaged. Better performance is obtained without mixup for per-example DP-SGD. The negative impact of noise due to the increased sensitivity dwarfs any benefits of mixup.

### DP-Mixup

We propose a technique to apply mixup with per-example DP-SGD. The challenge is that with per-example DP-SGD we only have a single example to work with -- else we again violate the sensitivity bound required for privacy analysis.

\(_{}\) and \(_{}\).The basic observation behind our technique is that we can freely apply mixup to any augmentations derived from a single data point \((x,y)\) or from \((x,y)\) and synthetic samples obtained _independently_ from a diffusion model. If we apply all augmentations, including mixup, _before_ gradient clipping then the privacy guarantee and analysis holds according to the augmentation multiplicity insight from .

We assume that we have a randomized transformation function \(T\) that can be applied to any example's features \(x\) so that \(T(x)\) is a valid single-sample data augmentation of \(x\). We may also have access to a generative model from which we can produce a set \(\) of synthetic samples ahead of time. For example, we can use a \(\)img diffusion model to create labeled examples \((z_{1},y_{1}),(z_{2},y_{2}),,(z_{m},y_{m})\) using unguided diffusion. In experiments, we generate these samples using the text prompt \({}^{*}\)a photo of a <class name>'. Since these "synthetic" examples are generated without accessing the (sensitive) training dataset, they can be used with no additional privacy cost.

Given a single data point \((x,y)\), our technique consists of three steps to produce a set of augmentations. The gradients of each augmentation are then computed and aggregated, and the aggregated gradient is clipped and noised as in DP-SGD. The first step takes \(x\) and repeatedly applies to it the randomized transformation function \(T\), resulting in a set \(S=\{(x_{1},y),(x_{2},y),,(x_{K_{}},y)\}\) of "base" self-augmentations. The second step (optionally) augments the set \(S\) with \(K_{}\) randomly selected synthetic examples from the set \(\) created using the diffusion model. If no diffusion model is available, we omit this step, which is equivalent to setting \(K_{}=0\). In the third and final step, we repeatedly apply Eq. (3) \(K_{}>0\) times to two randomly selected samples from the augmentation set \(S\), thereby obtaining a set \(S^{}\) of \(K_{}\) mixed up examples. The final set of augmentations is then the concatenation of the base augmentations and the mixup augmentations, i.e., \(S S^{}\).

From this final set of augmentations, we perform DP-SGD training. That is, we compute the noisy gradient as:

\[=[_{i=1}^{n}_{C} _{k=1}^{K}g_{i,k}}+(0,C^{2}^{2})]\, \]

where \(K=K_{}+K_{}+K_{}\) is the total number of augmentations and \(g_{i,k}\) denotes the gradient of the \(k^{}\) augmentation of the \(i^{}\) training example. The same privacy analysis and guarantee are obtained in this case as in vanilla DP-SGD because the influence of each training example \(x\) on the noisy gradient \(\) is at most \(C\) (observe that the gradients of all augmentations' are clipped together).

The method is described in Algorithm 1. An important remark is that the self-augmentation method of De et al.  is a special case of our method, corresponding to setting \(K_{}=0\) and \(K_{}=0\), i.e., one only uses the base augmentation set and no mixup.

  
**Method** & **Microbatch Size = 1** & **Microbatch Size = 2** \\  DP-SGD & 72.5 (.5) & 49.7 (.7) \\ DP-SGD w/ Mixup & N/A & 50.1 (.4) \\ DP-SGD w/ Self-Aug & 78.7 (.5) & 52.8 (.1) \\   

Table 1: **Test accuracy (%) of a WRN-16-4 model on CIFAR-10 trained from scratch with \((=8,=10^{-5})\)-differential privacy. Using a microbatch of size \(2\) yields much worse results than per-example clipping.**We define two variants of our method based on the availability of diffusion samples. If we have access to a diffusion model so that \(K_{}>0\), we call the method DP-MixDiff. In that case, the pool from which examples are selected for mixup includes synthetic samples from the diffusion samples. Otherwise, \(K_{}=0\) and the proposed method has the effect of applying mixup on randomly selected self-augmentations of a single original example, thus we call this variant DP-MixSELF. Interestingly, DP-MixSELF is not completely faithful to classical mixup because there is only one original example being mixed up and the label is unchanged.

Finally, we point out that although our focus is on mixup, other multi-sample data augmentation techniques such as cutmix and others  can be applied in the third step.

## 4 Experimental Setup

Datasets.We use CIFAR-10, CIFAR-100, EuroSAT, Caltech 256, SUN397 and Oxford-IIIT Pet. The details of these datasets are in Appendix C in Supplemental materials.

Models.We use the following models/model architectures.

* **Wide ResNet (WRN)** is a variant of the well-known ResNet (Residual Network) model , which is commonly used for image classification tasks. It increases the number of channels in convolutional layers (width) rather than the number of layers (depth). WRN-16-4 is used in De et al.  and we use the same model to ensure a fair comparison.
* **Vit-B-16 and ConvNext** are pre-trained on the LAION-2B dataset , the same pre-trained dataset as for our diffusion models, which we obtained from Open Clip2. We add a linear layer as a classification layer. We only fine-tune this last layer and freeze the weights of other layers. 
Setup.To implement DP-SGD, we use Opacus  and make modifications to it. For training from scratch experiments, we set the batch size to \(4096\), the number of self-augmentation to \(16\), the clip bound to \(C=1\), and the number of epochs to \(200\). For fine-tuning experiments, we change the batch size to \(1000\) and the number of epochs to \(20\) for EuroSAT and \(10\) for all other datasets. Reported test accuracies are averaged based on at least three independent runs and we also report the standard deviation. We provide additional details on our experimental setups in supplementary materials.

Selection of pre-training data, diffusion model, and fair comparisons.We take great care to ensure that our experiments lead to a fair comparison between our methods and alternatives such as Self-Aug (prior SoTA). In particular, all methods have access to the exact same training data. We also tune the hyperparameters of each method optimally (e.g., \(K_{}\) for Self-Aug). We use the same pre-trained models (Vit-B-16 and ConvNext from Open Clip) to compare our methods to others (Self-Aug and Mixed Ghost Clipping).

Since DP-MixDiff uses a diffusion model to generate synthetic examples, this could make the comparison unfair because other methods do not use synthetic samples. To avoid this, we purposefully use the exact same pre-training data (i.e., LAION-2B) to pre-train models as was used to train the diffusion model. This avoids the issue of the synthetic examples somehow "containing" data that other methods do not have access to. Moreover, we conducted experiments (check Table 6) to show that the synthetic examples themselves do _not_ boost performance. It is the _way_ they are used by DP-MixDiff that boosts performance. Finally, out of the six datasets we used for evaluation, none of them overlap with the LAION-2B dataset (to the best of our knowledge).

## 5 Experiments

### Training from Scratch with DP

Since DP-MixDiff relies on synthetic samples from diffusion models, it would not be fair to compare it to prior SoTA methods that do not have access to those samples in the training from scratch setting. Therefore, we focus this evaluation on DP-MixSelf. For this, we use a WRN-16-4 model. Our baseline from this experiment is De et al.  who use a combination of techniques to improve performance, but mostly self-augmentation. Results in Table 2 show that DP-MixSelf consistently provides improvements across datasets and privacy budgets.

We observed differences between the results reported by De et al.  using JAX and the reproduction by Sander et al.  on Pytorch. In our supplemental materials, Table 11 presents our reproduction of De et al.  using their original JAX code alongside the DP-MixSelf implementation.

### Finetuning with DP

We consider two pretrained models Vit-B-16 and ConvNext. As baselines/prior SoTA we consider De et al.  and the Mixed Ghost Clipping technique of Bu et al. . Results are shown in Table 3.

We observe that our proposed DP-MixSelf method yields significant improvements over all prior methods across datasets and privacy budgets. We also observe that DP-MixDiff, which uses samples from the diffusion model, significantly outperforms DP-MixSelf on datasets such as Caltech256, SUN397, and Oxford-IIIT Pet. Notably, when the privacy budget is limited, such as \(=1\), we observe remarkable improvements compared to DP-MixSelf (e.g., \(8.5\%\) on Caltech256 using Vit-B-16). This shows the benefits of incorporating diverse images from the diffusion model via mixup.

On the other hand, we observe that diffusion examples do not provide a benefit for datasets such as CIFAR-10 and EuroSAT. We investigate the cause of this empirically in Appendix B (supplementary materials). On these datasets, DP-MixDiff only sometimes outperforms the prior baselines, but DP-MixSelf provides a consistent improvement, suggesting that samples from the diffusion model

  
**Dataset** & **Method** & \(=1\) & \(=2\) & \(=4\) & \(=8\) \\   & Self-Aug & 56.8 (5) & 62.9 (3) & 69.5 (.4) & 78.7 (.5) \\  & DP-MixSelf & **57.2** (4) & **64.6** (4) & **70.47** (.4) & **79.8** (.3) \\   & Self-Aug & 13.3 (5) & 20.9 (4) & 31.8 (.2) & 39.2 (.4) \\  & DP-MixSelf & **14.1** (4) & **21.5** (4) & **33.3** (3) & **40.6** (.3) \\   & Self-Aug & 75.4 (.3) & 81.1 (.1) & 85.8 (.2) & 89.7 (.3) \\  & DP-MixSelf & **75.7** (2) & **82.8** (.3) & **87.4** (.2) & **90.8** (.2) \\   

Table 2: **Test accuracy (%) of a WRN-16-4 model trained from scratch:** Our proposed DP-MixSelf technique significantly outperforms De et al.  Self-Aug method (baseline and prior SoTA) in all privacy budget settings and all three datasets considered.

can inhibit performance. On CIFAR-100, DP-Mix\({}_{}\) outperforms other methods only at low privacy budgets (\( 2\)), with DP-Mix\({}_{}\) providing the best performance in the high privacy budget regime.

## 6 Ablation Study: Why Does DP-Mix Improve Performance?

We perform ablation experiments to better understand why our methods consistently and significantly outperform the prior SoTA techniques.

### Understanding Self-Augmentations

Since DP-Mix\({}_{}\) does not alter the label and involves applying mixup to (augmentations of) a single original training example, the method could be viewed as just another single-sample data augmentation technique. The question then becomes what augmentation techniques improve performance when using differential privacy and why.

De et al.  use flipping and cropping as self-augmentation (Self-Aug). We perform experiments using several widely used augmentation techniques (based on Chen et al.  -- Colorjitter, Translations and Rotations, Cutout, Gaussian Noise, Gaussian Blur, and Sobel Filter) in addition to Self-Aug to compare them to DP-Mix\({}_{}\). We set \(K_{}=16\) as in previous experiments. Results are shown in Table 4, where the second to last column combines all augmentations together (randomly chosen). We observe that these augmentations do **not** enhance performance to the same degree as DP-Mix\({}_{}\). In a few cases, the augmentations improve performance slightly above Self-Aug, but some of them also diminish performance.

### Number of Self-Augmentations

Increasing the number of self-augmentations beyond \(16\) does not improve performance . This was also pointed out recently by Xiao et al. . In fact, sometimes increasing the number of

    &  &  &  \\  & & \(=1\) & \(=2\) & \(=4\) & \(=8\) & \(=1\) & \(=2\) & \(=4\) & \(=8\) \\   & Mixed Ghost & 95.0 (.1) & 95.0 (.2) & 95.3 (.4) & 95.3 (.2) & 94.6 (.1) & 94.6 (.1) & 94.7 (.0) & 94.7 (.1) \\  & Self-Aug & 96.5 (.1) & 97.0 (.0) & 97.1 (.0) & 97.2 (.0) & 95.9 (.0) & 96.4 (.1) & 96.5 (.1) & 96.5 (.0) \\  & DP-Mix\({}_{}\) & **972.2**(.3) & **97.4**(.2) & **97.4**(.2) & **97.6**(.3) & **96.8**(.1) & **96.9**(.1) & **96.9**(.1) & **97.3**(.1) \\  & DP-Mix\({}_{}\) & 97.0 (.2) & 97.1 (.1) & 97.2 (.1) & 97.3 (.1) & 96.3 (.1) & 96.5 (.1) & 96.6 (.1) & 96.7 (.1) \\   & Mixed Ghost & 78.2 (.4) & 78.5 (.1) & 78.4 (.3) & 78.4 (.1) & 74.9 (.3) & 75.1 (.1) & 75.5 (.2) & 75.8 (.1) \\  & Self-Aug & 79.3 (.2) & 83.2 (.3) & 83.5 (.1) & 84.5 (.1) & 75.8 (.3) & 80.0 (.1) & 81.4 (.2) & 82.2 (.2) \\  & DP-Mix\({}_{}\) & **81.8**(.2) & 83.5 (.1) & **84.6**(.2) & 78.2 (.3) & **80.9**(.2) & **82.3**(.1) & **82.3**(.1) \\  & DP-Mix\({}_{}\) & **82.0**(.1) & **83.8**(.1) & **84.3**(.1) & **79.4**(.2) & 81.4 (.2) & 81.6 (.1) & 81.8 (.2) \\   & Mixed Ghost & 84.0 (.1) & 84.8 (.2) & 84.9 (.1) & 85.0 (.2) & 85.5 (.2) & 86.9 (.1) & 87.0 (.6) & 87.6 (.3) \\  & Self-Aug & 93.3 (.2) & 94.1 (.2) & 95.4 (.2) & 95.5 (.2) & 93.5 (.2) & 94.5 (.4) & 95.2 (.1) & 95.2 (.1) \\  & DP-Mix\({}_{}\) & **94.3**(.1) & **94.9**(.2) & **95.6**(.2) & **95.6**(.1) & **94.6**(.1) & **94.7**(.1) & **95.4**(.2) & **95.5**(.1) \\  & DP-Mix\({}_{}\) & 92.6 (.1) & 93.4 (.1) & 93.9 (.2) & 93.9 (.1) & 92.8 (.2) & 93.2 (.1) & 93.6 (.2) & 93.8 (.1) \\   & Mixed Ghost & 79.7 (.2) & 88.2 (.2) & 91.4 (.2) & 92.3 (.2) & 79.2 (.2) & 87.4 (.2) & 88.0 (.1) & 88.2 (.2) \\  & Self-Aug & 80.4 (.1) & 89.7 (.2) & 92.0 (.1) & 93.2 (.2) & 80.0 (.2) & 88.2 (.1) & 91.0 (.1) & 92.2 (.1) \\  & DP-Mix\({}_{}\) & **81.2**(.2) & 90.1 (.2) & 92.2 (.2) & 93.4 (.1) & 81.0 (.1) & 88.7 (.1) & 91.3 (.1) & 92.4 (.1) \\  & DP-Mix\({}_{}\) & **89.7**(.2) & **91.8**(.2) & **92.9**(.1) & **93.9**(.1) & **88.7**(.2) & **91.8**(.2) & **92.6**(.2) & **93.3**(.1) \\   & Mixed Ghost & 70.7 (.2) & 71.2 (.1) & 72.2 (.2) & 72.5 (.2) & 64.3 (.2) & 65.0 (.2) & 65.3 (.1) & 65.3 (.1) \\  & Self-Aug & 72.7 (.1) & 76.0 (.1) & 78.0 (.1) & 79.6 (.2) & 72.2 (.1) & 76.5 (.1) & 78.0 (.2) & 78.9 (.1) \\  & DP-Mix\({}_{}\) & **73.2**(.1) & **76.5**(.2) & 78.7 (.2) & 79.6 (.1) & 72.5 (.1) & 76.8 (.1) & 78.5 (.0) & 79.5 (.1) \\  & DP-Mix\({}_{}\) & **75.1**(.2) & **77.8**(.1) & **79.5**(.2) & **80.6**(.1) & **75.0**(.2) & **77.5**(.1) & **79.3**(.1) & **80.0**(.1) \\   & Mixed Ghost & 71.2 (.2) & 79.1 (.2) & 80.4 (.2) & 81.0 (.2) & 65.2 (.2) & 78.2 (.2) & 79.1 (.2) & 79.9 (.2) \\  & Self-Aug & 72.2 (.2) & 82.1 (2) & 85.8 (.3) & 88.2 (.1) & 68.1 (.2) & 81.3 (.2) & 85.5 (.1) & 87.0 (.1) \\  }\)) on CIFAR-10. Results are shown in Table 5. We observe that \(K_{}=16\) is optimal for the self-augmentations proposed by De et al. . However, we obtain significantly better performance DP-MixSelf for \(K=32\) (\(K_{}=16\), \(K_{}=16\) and \(K_{}=0\)). Recall that setting \(K_{}=K_{}=0\) in Algorithm 1 recovers the Self-Aug method of .

### Pretraining or DP-MixDiff?

Since DP-MixDiff uses synthetic samples from a diffusion model, we consider pretraining the model on the synthetic samples \(\) from the diffusion model with SGD prior to fine-tuning with DP. The results, presented in Table 6, indicate that pretraining on \(\) does not improve the model's performance. This indicates that the improved performance of DP-MixDiff results from the way it uses the diffusion samples, not just from having access to diffusion samples.

### Effect of DP-Mix on Training Data

In Table 7, we compare the distributions of the effective train sets produced by different data augmentation techniques against those of the original train and test sets, i.e., we measure the Frechet Inception Distance (FID), where lower FID values indicate more similarity between the datasets. Note, the FIDs are consistent with test accuracy across datasets and methods. For example, the FID of Self-Aug + Colorjitter is significantly larger than FID of Self-Aug, explaining why adding Colorjitter decreases test accuracy: it results in data less similar to train and test sets. Similarly, the FID for DP-MixDiff for EuroSAT is much larger compared to the other datasets, which explains reduced test accuracy relative to the other methods. In Table 8, we compare the FID between the original train and test sets with the FID between the original train set and the synthetic samples generated by the text-to-image diffusion model that we employ for DP-MixDiff. Note the much greater FID between the train set and the diffusion samples. This large domain gap may explain why, as shown in Table 6,

  
**Pretrained on \(\)** & **Vit-B-16** & **ConvNext** \\  Yes & 81.5 (.0) & 80.9 (.1) \\ No (Ours) & **91.8 (.2)** & **91.8 (.2)** \\   

Table 6: **Test accuracy (%) with and without pretraining on synthetic diffusion samples**: We compare the performance of fine-tuned Vit-B-16 and ConvNext with DP on Caltech256 with versus without pretraining on \(\) (using SGD). We set \(=2\) and \(=10^{-5}\).

  
**Dataset** & **Self-Aug** & **+Jitter** & **+Affine** & **+Cutout** & **+Noise** & **+Blur** & **+Sobel** & **+All** & **DP-MixSelf** \\  CIFAR-10 & 96.5 (.1) & 96.3 (.1) & 96.2 (.1) & 96.5 (.1) & 95.8 (.2) & 96.7 (.2) & 72.5 (.1) & 96.5 (.2) & **97.2 (.3)** \\ CIFAR-100 & 79.3 (.2) & 78.1 (.2) & 73.9 (.2) & 79.6 (.1) & 73.5 (.2) & 80.0 (.1) & 9.5 (.2) & 79.4 (.1) & **81.8 (.2)** \\   

Table 4: **Test accuracy (%) of Self-Aug with other Augmentations:** Vit-B-16 model performance on CIFAR-10 and CIFAR-100 with \(=1\) with different augmentations. We observe that these augmentations do not enhance performance to the same degree as DP-MixSelf.

  
**Training method** & **Total \# of Aug** & **WRN-16-4** & **Vit-B-16 (pretrained)** \\   & 16 & 78.8 (.5) & 97.2 (.0) \\  & 24 & 78.5 (.4) & 97.0 (.1) \\  & 32 & 78.4 (.3) & 97.0 (.1) \\  & 36 & 78.6 (.4) & 97.0 (.1) \\  DP-MixSelf & 32 & **79.8** (.3) & **97.6** (.3) \\   

Table 5: **Increase \(K_{}\) from 16 to 36 for Self-Aug.** We conduct experiments on CIFAR-10 with \(=8\) and \(=10^{-5}\) in two settings: train a WRN-16-4 model from scratch and fine-tune a pre-trained Vit-B-16. We can observe that for both cases, there are no substantial performance improvements when increasing \(K_{}\).

pretraining on the diffusion samples result in much worse performance compared to our proposed DP-MixDiff.

### Influence of \(K_{}\), \(K_{}\) and \(K_{}\)

Recall, \(K_{}\) is the number of base self-augmentations, \(K_{}\) is the number of mixups, and \(K_{}\) is the number of synthetic diffusion samples used. We vary \(K_{}\), \(K_{}\) and \(K_{}\) values and report their performance in Table 9. We observe that selecting \(K_{}=2\) or \(K_{}=4\) and setting \(K_{} K_{}\) gives good results overall. In this paper, we used \(K_{}=16\), \(K_{}=18\) and \(K_{}=2\) for most cases as it provides good overall results across many datasets and settings 3.

## 7 Conclusions

We studied the application of multi-sample data augmentation techniques such as mixup for differentially private learning. We show empirically that the most obvious way to apply mixup, using microbatches, does not yield models with low generalization errors as microbatching is at an inherent disadvantage. We then demonstrate how to harness mixup without microbatching by applying it to self-augmentations of a single training example. This provides a significant performance increase over the prior SoTA. Finally, we demonstrate that producing some augmentations using text-to-image diffusion models further enhances performance when combined with mixup.

    &  &  \\  & **Self-Aug** & **+Jitter** & **DP-MixMixDiff** & **DP-MixMixDiff** & **Self-Aug** & **+Jitter** & **DP-MixMixDiff** & **DP-MixMixDiff** \\  CIFAR-10 & 2.6 & 6.0 & 3.1 & 3.3 & 3.1 & 6.4 & 3.5 & 3.7 \\ CIFAR-100 & 3.1 & 6.1 & 3.3 & 3.5 & 3.5 & 6.5 & 3.8 & 3.9 \\ EuroSAT & 2.2 & 6.1 & 3.0 & **6.6** & 4.1 & 8.3 & 6.1 & **10.9** \\ Caltech256 & 1.0 & 2.9 & 1.3 & 1.5 & 2.5 & 4.2 & 2.8 & 2.9 \\   

Table 7: **FID for different methodsâ€™ generated images compared to training set and test set.**

    &  &  \\  & **Train vs Test** & **Train vs Diffusion** & **Train vs Test** & **Train vs Diffusion** \\  CIFAR-10 & 3.2 & 30.5 & 0.4 & 30.1 \\ CIFAR-100 & 3.6 & 19.8 & 0.5 & 21.9 \\ EuroSAT & 7.4 & **164.0** & 7.3 & **82.9** \\ Caltech256 & 6.8 & 25.1 & 1.6 & 37.4 \\   

Table 8: **FID between datasets for different FID models**. We compute FID values between the training set and test set, and training set and text-to-images diffusion model generated images based on different FID models (InceptionV3 and Vit-B-16).