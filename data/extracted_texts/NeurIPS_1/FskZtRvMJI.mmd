# RiskQ: Risk-sensitive Multi-Agent Reinforcement Learning Value Factorization

Siqi Shen\({}^{ab}\), Chennan Ma\({}^{ab}\), Chao Li\({}^{ab}\), Weiquan Liu\({}^{ab}\),

**Yongquan Fu\({}^{c}\)1, Songzhu Mei\({}^{c}\), Xinwang Liu\({}^{c}\), Cheng Wang\({}^{ab}\) \({}^{a}\)**Fujian Key Laboratory of Sensing and Computing for Smart Cities,

School of Informatics, Xiamen University (XMU), China

\({}^{b}\)Key Laboratory of Multimedia Trusted Perception and Efficient Computing, XMU, China

\({}^{c}\)School of Computer, National University of Defense Technology, China

{siqishen,cwang}@xmu.edu.cn, {yongquanf,xinwangliu}@nudt.edu.cn

{chennanma,chaoli}@stu.xmu.edu.cn

###### Abstract

Multi-agent systems are characterized by environmental uncertainty, varying policies of agents, and partial observability, which result in significant risks. In the context of Multi-Agent Reinforcement Learning (MARL), learning coordinated and decentralized policies that are sensitive to risk is challenging. To formulate the coordination requirements in risk-sensitive MARL, we introduce the Risk-sensitive Individual-Global-Max (RIGM) principle as a generalization of the Individual-Global-Max (IGM) and Distributional IGM (DIGM) principles. This principle requires that the collection of risk-sensitive action selections of each agent should be equivalent to the risk-sensitive action selection of the central policy. Current MARL value factorization methods do not satisfy the RIGM principle for common risk metrics such as the Value at Risk (VaR) metric or distorted risk measurements. Therefore, we propose RiskQ to address this limitation, which models the joint return distribution by modeling quantiles of it as weighted quantile mixtures of per-agent return distribution utilities. RiskQ satisfies the RIGM principle for the VaR and distorted risk metrics. We show that RiskQ can obtain promising performance through extensive experiments. The source code of RiskQ is available in [https://github.com/xmu-rl-3dv/RiskQ](https://github.com/xmu-rl-3dv/RiskQ).

## 1 Introduction

In cooperative multi-agent reinforcement learning (MARL) , it is important to learn coordinated agent policies to achieve a common goal. However, achieving this goal is challenging due to random rewards, environmental uncertainty, and varying policies among agents. Especially, for scenarios with partial-observability, high-stochastic rewards and state-transitions . In order to efficiently learn MARL policies, many researchers have adopted the centralized training with decentralized execution (CTDE)  paradigm, which offers advantages in terms of learning speed and performance. A popular subset of the CTDE methods is the value factorization category [4; 5; 6; 7; 8].

To learn coordinated policies, value factorization algorithms must ensure that the global argmax operator performed on the global state-action value function yields the same outcome as a set of individual argmax operations performed on per-agent utilities. This requirement for coordination is known as the individual-global-max (IGM) principle . The IGM principle takes only the expected return into account, but not the entire distribution of returns that includes potential outcomeevents. A method that learns the expected return return may fail in high-stochastic environments with extremely high/low rewards but at low probabilities. For example, users may seek for a big win with low probability in finance or avoid suffering from a huge loss on rare occasions in autonomous driving [9; 10]. Risk refers to the uncertainty of future outcomes in multi-agent systems. By making decisions based on risk, agents can address uncertainty better. Most of the existing MARL value factorization methods do not extensively consider _risk_, which could impact their performance negatively.

Recently, risk-sensitive reinforcement learning (RL) has achieved significant progress in the single agent domain [11; 12]. Instead of optimizing the expectation of return, risk-sensitive RL optimizes a risk measure based on a return distribution. Risk-sensitive value factorization methods should be designed to learn risk-sensitive decentralized policies that are fully consistent with the risk-sensitive centralized counterpart. In order to learn coordinated risk-sensitive policies, the IGM principle needs to be adapted to address cases where expectations are not the only factor. Despite there are a few approaches [13; 14] combing risk-sensitive RL with MARL, how to effectively combine risk-sensitive reinforcement learning with MARL value factorization is still an open question.

In this work, we formulate the coordination requirement in risk-sensitive MARL as _the Risk-sensitive Individual-Global-Max (RIGM) principle_. The principle requires that the optimal joint risk-sensitive action should be equivalent to the collection of each agent's greedy risk-sensitive actions. The RIGM principle is a generalization of the IGM and the distributional IGM (DIGM) principle. Albeit multiple value factorization methods [8; 7; 15] have been proposed to learn policies satisfying the IGM or the DIGM principles, they cannot guarantee the RIGM principle. DRIMA  combines risk-sensitive RL with a value factorization method. However, it does not guarantee the RIGM principle for distorted risk measures . RMIX  learns risk-sensitive policies which satisfy the RIGM principle only if the risk metric is the expectation operator. It is unclear how to learn coordinated risk-sensitive decentralized policies which satisfy the RIGM principle for general risk measures.

We build, RiskQ, a risk-sensitive MARL value factorization algorithm that satisfies the RIGM principle for risk metrics \(_{}\), such as VaR (i.e., percentile) and distorted risk measurements , where \(\) is a risk parameter. In RiskQ, each agent acts greedily according to a risk value defined as \(_{}[Z_{i}]\), where \(Z_{i}\) is a per-agent return distribution utility. RiskQ models the joint return distribution \(Z_{jt}\) by combining per-agent return distribution utilities \([Z_{i}]_{i=1}^{n}\) with an attention-based mechanism. Specifically, the quantiles \(()\) of the return distribution \(Z_{jt}\) is modeled as the weighted sum of the quantiles \(_{i}()\) of return distribution utilities, where \(\) is a quantile sample.

For evaluation, we conduct extensive experiments on risk-sensitive games and the StarCraft II MARL tasks . The experimental results show that RiskQ can obtain promising results in both risk-sensitive and risk-neutral scenarios.

## 2 Background

### Dec-POMDPs

We consider cooperative multi-agent reinforcement learning (MARL) scenarios which can be modeled as Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs) , represented as tuple \(G=,\{_{i}\}_{i=1}^{N},P,r,\{_{i}\}_{i= 1}^{N},\{_{i}\}_{i=1}^{N},N,\) for \(N\) agents.

\(S\) is a finite set of states, and \(_{i}\) is the set of discrete actions available to agent \(i\). At state \(s^{t}\), a joint action of all agents is defined as \(^{t}^{N}=_{1} _{N}\), with \(t\) representing the discrete time step. After performing \(^{t}\), the environment transitions to a new state \(s^{t+1} S\), following the transition function \(s^{t+1} P(|s^{t},^{t})\), and each agent receives a reward \(r^{t}\) as a result of the state transition. Due to the partial observability of the environment, each agent \(i\) receives a local observation \(o^{t}_{i} O_{i}\), which is drawn from \(o^{t}_{i}^{i}(|s^{t})\). The discounting factor is denoted by \([0,1)\). Each agent \(i\) maintains a local action-observation history \(_{i}=(O_{i} U_{i})^{*}\), where \(*\) represents 0 to \(T\) ( \(T\) denotes the time step). The global action-observation history is denoted as \(^{N}:=_{1}_{N}\). Each agent acts according to policy \(_{i}(u_{i}|_{i})\), and the joint policy can be represented as \(=<_{1},,_{N}>\).

### Value Function Factorization

For Dec-POMDPs, value function factorization methods learn factorized utility which can be used for the execution of individual agent. The Individual-Global-Max (**IGM**) principle proposed in  is important for the realization of value function factorization for MARL. It is defined as follows.

**Definition 1** (Igm).: _For a joint state-action value function \(Q_{ jt}:^{N}^{N}\), where \(^{N}\) is a joint action-observation history and \(\) is the joint action, if there exists individual state-action functions \([Q_{i}:_{i}_{i}]_{i=1}^{N}\), such that the following conditions are satisfied_

\[_{}Q_{ jt}(,)=(_{u_{1}}Q_{1}(_{1}, u_{1}),\ ,\ _{u_{n}}Q_{N}(_{N},u_{N})), \]

_then, \([Q_{i}]_{i=1}^{N}\) satisfy IGM for \(Q_{ jt}\) under \(\). We can state that \(Q_{ jt}(,)\) is factorized by \([Q_{i}(_{i},u_{i})]_{i=1}^{N}\)._

### Distributional RL and Risk

MARL is highly stochastic, and distributional RL could be used to deal with such stochasticity. Distributional RL [18; 19; 20; 21] models the return distribution of state-action pair through \(Z(,)\) explicitly. They model full return distribution \(Z(,)\) instead of return expectation \(Q(,)\). The distribution of return can be approximated through a categorical distribution  or a quantile function [19; 20].

The state-action return distribution \(Z(,)\) can be modelled using quantile functions \(\) of a random variable Z, which is defined as follows.

\[_{Z}(,,)=\{z:_{Z}(z)\}, \]

where \(_{Z}(z)\) is the cumulative distribution function of \(Z(,)\). The quantile function \(_{Z}()\) may be referred as generalized inverse CDF in other literature . For notation simplicity, we denote \(_{Z}()\) as \(()\).

QR-DQN  and IQN  model the return function \(Z(,u)\) as a mixture of \(n\) Dirac functions.

\[Z(,u)=_{i=1}^{n}p_{i}(,u,_{i})_{(,,_{i})} \]

where \(_{(,,_{i})}\) is a Dirac Delta function whose value is \((,,_{i})\). \(_{i}\) is a quantile sample. \(p_{i}(,u,_{i})\) is the corresponding probability of \((,,_{i})\). For QR-DQN , \(p_{i}(,u,_{i})\) can be simplified as \(1/n\). For execution, the action with the largest expected return \(_{u}[Z(,)]\) is chosen.

Analogous to the IGM principle for value function factorization, the Distributional Individual-Global-Max (**DIGM**) principle for value distribution proposed in  is defined as follows.

**Definition 2** (Digm).: _Given a set of individual state-action return distribution utilities \([Z_{i}(_{i},u_{i})]_{i=1}^{N}\) and a joint state-action return distribution \(Z_{jt}(,)\), if the following conditions are satisfied_

\[_{}[Z_{jt}(,)]=(_{u_{1}} [Z_{1}(_{1},u_{1})],\ ,\ _{u_{N}}[Z_{N}( _{N},u_{N})]), \]

_then, \([Z_{i}(_{i},u_{i})]_{i=1}^{N}\) satisfy DIGM for \(Z_{jt}\) under \(\). We can state that \(Z_{jt}(,)\) is distributionally factorized by \([Z_{i}(_{i},u_{i})]_{i=1}^{N}\)._

In this work, we use \(_{}\) to measure the risk from a return distribution \(Z\), where \(\) is the risk level. For example, the Value at Risk metric, VaR\({}_{}\), estimate the \(\)-percentile from a distribution. For VaR\({}_{}\), a small value for \(\) indicates risk-averse setting, whereas a high value for \(\) means risk-seeking scenarios. Risk-sensitive policies act with a risk measure \(_{}\).

**Definition 3** (Value at Risk (VaR)).: _Value at Risk (VaR)  is a popular risk metric which measures risk as the minimal reward might be occur given a confidence level \(\). For a random variable \(Z\) with cumulative distribution function (CDF), the quantile function \(\) and a quantile sample \(\), \(VaR_{}(Z(,))=(,,)\). This metric is called percentile as well._

**Definition 4** (Distortion risk measures (Drn)).: _Distorted risk measures are weighted expectation of a distribution under a distortion function [9; 10; 24]. The distorted expectation of a random variable \(Z\) under \(g\) is defined as_

\[(Z)=_{0}^{1}g^{}()()d \]_where \(g()\), \(g^{}()\) is the derivative of \(g()\). There are many distortion functions which can reflect different risk preferences, such as CwaR, Wang, and CPW._

**Definition 5** (Conditional Value at Risk(CVaR)).: \[CVaR_{}(Z)=_{Z}[z|z()]\] (6)

_where \(\) is the confidence level (risk level), \(()\) is the quantile function (inverse CDF) defined in (A.1). CVaR is the expectation of values \(z\) that are less equal than the \(\)-quantile value (\(()\)) of the value distribution. CVaR is a DRM whose \(g()=(/,1)\)._

Please refer to the Appendix A.1 for the detailed definitions of more distortion risk measures.

## 3 Related Work

### Value Factorization

To enable efficient decentralized execution of MARL policies, value factorization approaches are widely adopted in MARL [28; 6; 15]. Most methods focus on the satisfaction of the IGM principle which is important for value factorization. VDN  factorizes the value function as the sum of per-agents' utilities. QMIX  models monotonic relationships between individual utilities and the value function. QAtten  and REFIL  use attention mechanisms for value function factorization. QTran  transforms the joint state-action value function \(Q_{jt}\) into an easy-to-factorize form through linear constraints. QPlex  decomposes a value function into a value part and an advantage part. ResQ  transforms a value function into the combination of a main and a residual function through masking.

For distributional MARL, the DFAC framework  and ResZ  satisfy the DIGM principle which is the IGM principle for distributional RL. The DFAC framework factorizes a return distribution through mean-shape decomposition which models the mean and shape of the return distribution separately. ResZ transforms a return distribution into the combination of a main and a residual return distribution. We will show that the DFAC framework and ResZ can not guarantee adherence to the RIGM principle for the VaR risk metric.

### Risk-sensitive RL

Many researchers have dedicated themselves to studying risk-sensitive reinforcement learning in single-agent settings [11; 12; 31; 32; 33; 24; 34]. O-RAAC  learns a full return distribution for its critic and optimizes the actor's policy according to a risk related metric (such as CVaR).  proposes a distributional reinforcement learning algorithm for learning CVaR-optimized policies.

Recently, risk-sensitive reinforcement learning has been adopted in MARL, such as [37; 13]. RMIX  combines QMIX and CVaR-optimized agent policies. It learns a value function (_rather than_ a return distribution) for each state-action pair, which is further decomposed into per-agent's return distribution utilities. Because the reward for each agent is unknown in cooperative MARL, RMIX learns agents' utilities by using CVaR value as pseudo rewards. RMIX satisfies the RIGM principle only when the risk metric is CVaR and the risk level \(\) is set to 1.

DRIMA  separates the sources of risk into cooperation risk and environmental risk. It models joint return distribution as a monotonic mixing of per-agent return distribution utilities. We will show in Sec. 4 that DRIMA does not satisfy the RIGM principle for the CVaR metric.

RiskQ can be used with other MARL-based approaches: communication approaches (COMNet , GraphComm , DIAL , COPA ), actor-critic methods (MADDPG , MAAC ), and other approaches such as MAPPO , MASER , QRelation , ATM , and UPDet .

## 4 Risk-sensitive Value Factorization

In cooperative MARL, it is crucial to learn decentralized policies consistent with a centralized policy that is conditioned on joint state and joint action. Especially in MARL scenarios with high-stochastic rewards and state transitions, taking risk into consideration is of great importance. However, it is unclear about how to coordinate agents' policies with the consideration of risk.

Key to our approach is the insight that to coordinate agents with risk consideration is important to learn risk-sensitive decentralised policies that are fully consistent with the risk-sensitive centralised counterpart. To ensure this consistency, MARL algorithms only need to ensure that a joint risk-sensitive argmax operation, when performed on the joint state-action value function, yields the same outcome as a collection of individual risk-sensitive argmax operations conducted on per-agent utilities. This insight leads to the following definition.

### Risk-sensitive Individual-Global-Max (RIGM) Principle

**Definition 6** (Rigm).: _Given a risk metric \(_{}\), a set of individual return distribution utilities \([Z_{i}(_{i},u_{i})]_{i=1}^{N}\), and a joint state-action return distribution \(Z_{jt}(,)\), if the following conditions are satisfied:_

\[_{}_{}[Z_{jt}(, )]=(_{u_{1}}[_{}[Z_{1}(_{1},u_{1})],\; ,\;_{u_{N}}[_{}[Z_{N}(_{N},u_{N})]), \]

_where \(_{}:Z R R\) is a risk metric such as the VaR or a distorted risk measure, \(\) is its risk level. Then, \([Z_{i}(_{i},u_{i})]_{i=1}^{N}\) satisfy the RIGM principle with risk metric \(_{}\) for \(Z_{jt}\) under under \(\). We can state that \(Z_{jt}(,)\) can be distributionally factorized by \([Z_{i}(_{i},u_{i})]_{i=1}^{N}\) with risk metric \(_{}\)._

The RIGM principle is a generalization of the DIGM and the IGM principle. The DIGM principle is a special case of the RIGM theorem for \(=\) and \(=1\) (the expectation operator \(\) is equal to CVaR\({}_{1}\)). If \(Z_{i}\) is a single Dirac Delta Distribution, then the return distribution \(Z_{i}\) becomes a single value(i.e., \(Q_{i}\)), and in this case, the IGM principle is equivalent to the RIGM principle when \(=\) and \(=1\).

In the following, we discuss whether existing risk-neutral value factorization methods can be simply modified to satisfy the RIGM principle for \(_{}\), and then discuss limitations of existing risk-sensitive value factorization methods. There are many value factorization methods satisfying the IGM principle. We show in Theorem 1 that simply replacing \(Q_{i}\) with \(Z_{i}\) is insufficient to guarantee that \([Z_{i}]_{i=1}^{N}\) satisfy RIGM with risk metric \(_{}\).

**Theorem 1**.: _Given a deterministic joint state-action value function \(Q_{jt}\), a joint state-action return distribution \(Z_{jt}\), and a factorization function \(\) for deterministic utilities:_

\[Q_{jt}(,u)=(Q_{1}(_{1},u_{1}),...,Q_{N}(_{N},u_{N})) \]

_such that \([Q_{i}]_{i=1}^{N}\) satisfy IGM for \(Q_{jt}\) under \(\), the following risk-sensitive distributional factorization:_

\[Z_{jt}(,u)=(Z_{1}(_{1},u_{1}),...,Z_{N}(_{N},u_{N})) \]

_is insufficient to guarantee that \([Z_{i}]_{i=1}^{N}\) satisfy RIGM for \(Z_{jt}(,u)\) with risk metric \(_{}\)._

We show that factorization methods satisfying the DIGM principle cannot guarantee the satisfaction of the RIGM theorem for the VaR metric.

**Theorem 2**.: _Given a joint state-action return distribution \(Z_{jt}\), and a distributional factorization function \(\) for the return distribution utilities \([Z_{i}]_{i=1}^{N}\) which satisfy the DIGM theorem, the following risk-sensitive distributional factorization:_

\[Z_{jt}(,u)=(Z_{1}(_{1},u_{1}),...,Z_{N}(_{N},u_{N})) \]

_is insufficient to guarantee that \([Z_{i}]_{i=1}^{N}\) satisfy the RIGM principle for \(Z_{jt}(,u)\) with risk metric \(_{}\)._

Recently, DRIMA  and RMIX  combine risk-sensitive RL with MARL. Albeit they have demonstrated promising results, they do not explicitly consider the risk-sensitive coordination requirement.

**Theorem 3**.: _DRIMA  does not guarantee adherence to the RIGM principle for CVaR metric._

The value function \(Q_{jt}(,)\), learned by RMIX , can be written as \(Q_{jt}(,)=Q_{mix}(_{}[Z_{1}(_{1},u_{1})],...,_{}[Z_{N}(_{N},u_{N})])\), where \(Z_{i}(_{i},u_{i})\) represents per-agent return distribution utilities, and \(Q_{mix}\) is a monotonically increasing function with respect to \(_{}[Z_{i}(_{i},u_{i})]\). Although it seems that it satisfies the RIGM principle for any risk metric \(_{}\), its learning algorithm seeks to find the optimal actions that \(_{}[Q_{jt}(,)]\) rather than \(_{}_{}[Z_{jt}(,)]\). In essence, RMIX seeks to make sure that \(_{}[Q_{jt}(,)]\) equal to \((_{u_{1}}[_{}[Z_{1}(_{1},u_{1})],\ ,\ _{u_{N}}[_{ }[Z_{N}(_{N},u_{N})]])\). By this way, RMIX satisfies the RIGM principle only if \(_{}=_{1}\). Moreover, \(Q_{jt}(,)\) is only a value function but not a return distribution.

Please refer to Appendix B.1 for detailed proofs of Theorem 1, 2 and 3. We have discussed the limitations of existing value factorization methods. It becomes apparent that the development of a novel factorization method is needed, specifically one capable of effectively coordinating agents in risk-sensitive scenarios.

### RiskQ

In MARL, it is typical to model the factorization function \(f\) as either the sum of per-agent utilities or a monotonic increasing function with respect to per-agent utilities. However, risk metrics are generally non-additive, except for variables which are highly dependent on each other (the comonotonicity property) . For instance, let's consider the VaR\({}_{0.5}\) metric, the median of a distribution, and \(f= Z_{i}\). It generally holds that VaR\({}_{0.5}[Z_{1}+Z_{2}]\) VaR\({}_{0.5}[Z_{1}]+\) VaR\({}_{0.5}[Z_{2}]\). This is due to the fact that the median of the sum of two random variables does not equate to the sum of their medians. The non-additive property of risk metrics makes the explicit modeling of \(f\) challenging, as we cannot model \(f\) as the sum or the monotonic mixing of per-agents' utilities.

The key to our insights is that instead of modeling the return distribution \(Z_{jt}\) using the form of \(f(Z_{1},...Z_{n})\) explicitly which is a common practice in MARL , we can model \(Z_{jt}\) implicitly through modeling it using its quantiles. We model the relationships among quantiles of the joint return distribution and the quantiles of per-agent return distribution utilities.

RiskQ utilizes a common distributional RL technique , where the return distribution \(Z_{jt}(,)\) is represented by a combination of Dirac delta functions \(_{()}\) and the positions \(()\) of the Diracs that are determined through quantile regression. Figure 1 depicts the mixing process of the quantiles of per-agent utilities. For a quantile sample (cumulative probability) \(\), the quantile value \((,,)\) of \(Z_{jt}(,)\) is represented as the weighted sum of the quantile value \(_{i=1}^{N}k_{i}_{i}(,u_{i},)\) of return distribution utilities.

We demonstrate through the following theorem that \(Z_{jt}\) and \([Z_{i}]_{i=1}^{N}\) satisfy the RIGM principle for both the VaR risk metric and distorted risk measures (DRM).

**Theorem 4**.: _A joint state-action return distribution_

\[Z_{jt}(,) =_{j=1}^{J}p_{j}(,,_{j} )_{(,,_{j})} \] \[(,,_{j}) =_{i=1}^{N}k_{i}_{i}(_{i},u_{i},_{j}) \]

_is distributional factorized by \([Z_{i}(_{i},u_{i})]_{i=1}^{N}\) with risk metric \(_{}\), where \(J\) is the number of Dirac Delta functions, \(_{(,,_{j})}\) is a Dirac Delta function at \((,,_{j})\), \((,,_{j})\) is a quantile function with sample \(_{j}\), \(p_{j}(,,_{j})\) is the corresponding probability for \(_{(,,_{j})}\) of the estimated return distribution \(Z_{jt}\), \(_{i}(_{i},u_{i},)\) is the quantile function (with quantile sample \(\)) for the return distribution utility \(Z_{i}(_{i},u_{i})\) of agent \(i\) and \(k_{i} 0\)._

We have shown that by modeling the quantiles of \(Z_{jt}\) as a weighted sum of quantiles of \([Z_{i}]_{i=1}^{N}\). \([Z_{i}]_{i=1}^{N}\) satisfied the RIGM theorem for VaR and DRMs such as CVaR, Wang, and CPW. Detailed proofs supporting this theorem can be found in Appendix B.2.

### Neural Networks and Loss

We model the return distribution utility for each agent by a simple neural network. It takes the observation history \(_{i}\) and action \(u_{i}\) as input, then passes them through a MLP, a GRU, and a MLP, and outputs \(Z_{i}(_{i},u_{i},)\) for each quantile sample \(\). For execution, the action which maximizes \(_{}[Z_{i}(_{i},u_{i})]\) is chosen.

The mixer function mixes all the quantile \(\) of return distribution utility \([Z_{i}]_{i=1}^{N}\) into \((,,)=_{i=1}^{N}k_{i}_{i}(_{i},u_{i},)\). It takes \([_{i}(_{i},u_{i},)]_{i=1}^{N}\), the state \(s\), the observation history \(\) as input, and outputs \((,,)\) for each quantile sample \(\). \(k_{i}\) is modelled using a multi-head attention mechanism.

We adopt the Quantile Regression (QR) loss  to update the value distribution \(Z_{jt}(^{k},^{k},)\). More concretely, QR aims to estimate the quantiles of the return distribution by minimizing the distance between \(Z_{jt}(^{k},^{k},)\) and its target distribution \(y^{k}(^{k},^{k},) r+ Z_{jt}(^{k+ 1},},^{-})\). \(}=[_{i}]_{i=1}^{N},_{i}=_{ u_{i}}_{}[Z_{i}(_{i}^{k+1},u_{i})]\). \(\) are the parameters of the network, and \(^{-}\) are the parameters of the target network.

## 5 Evaluation

We study the performance of RiskQ on risk-sensitive games (Multi-agent cliff and Car following games), the StarCraft II Multi-Agent Challenge benchmark (SMAC) . RiskQ can obtain promising performance for risk-sensitive and risk-neutral scenarios. Ablation studies reveal the importance of adhering to the RIGM principle for achieving good performance. Additionally, we have examined the impact of functional representations, risk metrics and risk levels.

### Experimental Setup

We select three categories of MARL value factorization methods for comparison: (i) Expected value factorization methods: QMIX , QTran , QPlex , CW QMIX , ResQ ; (ii) Risk-neutral return distribution (stochastic value) factorization methods: DMIX  and ResZ ; (iii) Risk-sensitive return distribution factorization methods: RMIX  and DRIMA . For robustness, each experiment is conducted at least 5 times with different random seeds. In general, the configuration of RiskQ follows the setup of Weighted QMIX and ResQ. By default, the risk metric used in RiskQ is Wang\({}_{0.75}\), indicating a risk-averse preference. Please refer to Appendix C for detailed experimental setup and more experimental results.

### Multi-Agent Cliff Navigation

In Multi-Agent Cliff Navigation (MACN), introduced in , two agents must navigate in a grid-like world to reach the goal without falling into cliff. The agent receives a -1 reward at each time step. If any agent reaches the goal individually, a -0.5 reward will be given to the agents. They will receive 0 reward if they reach the goal together. An episode is finished once the goal is reached by two agents together or any agent falls into cliff (reward -100). We depict the test return of each learning algorithm in two MACN scenarios: \(4 11\) grid map and \(4 15\) grid map. As illustrated in Figure 2 (a) and (b), RiskQ achieves the optimal performance in both scenarios, outperforming the two risk-sensitive methods: RMIX and DRIMA. This indicates that learning policies which satisfy the RIGM principle could lead to promising results.

### Multi-Agent Car Following game

We design Multi-Agent Car Following (MACF) Game, which is adapted from the single agent risk-sensitive environment . In MACF, there are two agents, each controlling one car, with the

Figure 1: RiskQ overview: (a) quantiles mixing for \(Z_{jt}\), (b) mixer function, (c) agent return distribution utility

task of one car following the other to reach a goal. Each car can observe the current position and speed of other cars within its observation range. The agent has a fixed action space which determines its acceleration. At each time step, agent will receive a negative reward. And the cars will _crash_ with some probability if their speed exceed a speed-threshold, and a negative reward is given to the agents. To adapt the game to cooperative MARL, agents move within each other's observation will receive a positive reward. Once the agents reach the goal together, a big reward is given to them and the episode is terminated. As shown in Figure 2 (c), RiskQ exhibits superior performance to both risk-neutral and risk-sensitive algorithms. Moreover, in order to verify that this performance improvement is due to risk considerations, we also study the number of crashes. For this value, as it is shown in the appendix (Figure 7), albeit RiskQ does not optimize for it, RiskQ achieves zero crash with the fastest learning rate.

### StarCraft II

SMAC is a well-known benchmark which comprises two teams of agents engaging in combat scenarios. Following the evaluation protocol of DRIMA  for risk-averse SMAC, we first study the performance of RiskQ in explorative, dilemmatic and random settings for the 3s_vs_5z scenario. In the explorative setting, agents behave heavily exploratory during training, thus they must consider the risk brought on by heavily exploration of other agents. In the dilemmatic setting, agents have increased exploratory behaviors and they are punished by their decreased heath. In this setting, learning algorithms should consider risk to prevent the learning of locally optimal policies. In the random setting, one agent performs random actions 50% of the time during testing. As depicted in Figure 2 (d-f), RiskQ obtains the best performance. Please refer to Figure 9 in Appendix C.4 for more

Figure 3: Win Rate of the StarCraft Multi-Agent Scenarios.

Figure 2: The return of the MACN (a-b) and the MACF (c) environment; the test win rate of random (d), explorative (e) and dilemmatic (f) 3s_vs_5z scenario of SMAC.

results in these risk-sensitive SMAC scenarios. Combining previous results from the MACN and the MACF environments, we can conclude that RiskQ can yield promising results in environments that require risk-sensitive cooperation.

Then we evaluate the performance of RiskQ in the standard SMAC setting. As demonstrated in Figure 3 (a) and (b), RiskQ achieves the best performance in the MMM2 scenarios. In MMM2, RMIX achieves the second best results in the end. This demonstrates that it is important to consider risk in highly stochastic environments. RiskQ is among the best performing algorithms in the MMM scenario. Notably, RiskQ satisfies the RIGM principle for DRMs, further suggesting the necessity of coordinated risk-sensitive cooperation.

The test win rates for the 2c_vs_ 64zg and 2s_vs_1sc scenarios are shown in Figure 3 (c) and (d). For the 2c_vs_64zg scenario, RiskQ and ResZ are the best performing algorithms, while RMIX performs poorly in this scenario. Albeit DRIMA matches the performance of RiskQ in the end, its learning speed is much slower than that of RiskQ. For the 2s_vs_1sc scenario, RiskQ, ResQ and ResZ achieve optimal performance, with RiskQ achieving near-optimal performance merely after 0.3 million steps. Furthermore, DRIMA learns slower than RiskQ, and the performance of RMIX is unstable.

For the 3s_vs_5z scenario, as illustrated in Figure 3 (e), RiskQ achieves the optimal performance after only 1.2 million training steps. The risk-sensitive algorithms DRIMA and RMIX do not match up to the performance of RiskQ. As for the 27m_vs_30m scenario, RiskQ is the second-best algorithm.

### Ablation Study and Discussion

To investigate the reasons behind RiskQ's promising results, we analyze different designs of RiskQ on the SMAC and the MACN scenarios. First, we study the necessity of satisfying the RIGM principle by making about 50% of RiskQ agents follow different risk measures. In Figure 4 (a-d), w/o RIGM VaR\({}_{1}\), w/o RIGM CVaR\({}_{1}\) and w/o RIGM CVaR\({}_{0.2}\) indicate that about 50% of agents act according to the VaR\({}_{1}\) (risk-seeking), CVaR\({}_{1}\) (risk-neutral) and the CVaR\({}_{0.2}\) (risk-averse) metrics, respectively. These risk measures are not the risk measure \(_{}\) = Wang\({}_{0.75}\) used by other agents. As depicted in Figure 4 (a-d), RiskQ performs poorly in all the three cases, highlighting the importance of satisfying the RIGM principle that agents act according to the same risk measure.

Moreover, we analyze different designs of the RiskQ mixer through two variants: RiskQ-Sum and RiskQ-Qi. Instead of using the attention mechanism, RiskQ-Sum models the percentile \((,,)\)

Figure 4: Ablation Study: (a-d) impact of not satisfying RIGM and different design of RiskQ mixer; (e-f) impact of functional representation limitations; (g-i) impact of different risk metrics, risk levels, and number of quantiles

as the sum of the percentiles of per-agent's utilities \(_{i=1}^{N}_{i}(_{i},u_{i},)\). RiskQ-Qi represents each percentile \((,,)\) as the expectation of state-action function. As shown in Figure 4 (a-d), both two variants perform inferior to RiskQ in most cases.

By modeling the quantiles \(()\) of joint return distribution as the weighted sum of the quantiles \(_{i}()\) of each agent's return distribution, RiskQ suffers from representation limitations. To study whether the representation limitations impact the performance of RiskQ, we design three variants of RiskQ: RiskQ-QMIX, RiskQ-Residual, and RiskQ-Residual-QMIX. RiskQ-QMIX models the monotonic relations among \(_{i}()\) and \(()\) in the manner of QMIX . RiskQ-Residual models the joint value distribution without representation limitations by using residual functions . RiskQ-Residual-QMIX combines RiskQ-Residual and QMIX. RiskQ-QMIX and RiskQ-Residual-QMIX satisfy the RIGM principle for the VaR metric, and RiskQ-Residual satisfies the RIGM for the VaR and DRM metrics. Please refer to the appendix B.3, B.4 and B.5 for further details and their proofs.

We evaluate the performance of the three RiskQ variants using three different risk metrics (Wang\({}_{0.75}\), VaR\({}_{0.2}\), and VaR\({}_{0.6}\)). In Figure 4 (e) and (f), a method with its risk metric is denoted as method-metric. For example, RiskQ-QMIX-VaR\({}_{0.2}\) represents RiskQ-QMIX with the risk metric VaR\({}_{0.2}\). As can be observed from Figure 4 (e) and (f), although these three variants can model more complex functional relationships among quantiles, their performance is unsatisfactory for the 3s\({}_{}\)vs\({}_{}\)5z and MMM2 scenarios. This suggests that the representation limitation of RiskQ does not significantly impact its performance. Finding a better network architecture to make the algorithm free from representation limitations and have better performance is a prospective future work.

To systematically evaluate RiskQ, we evaluate the impact of various risk metrics, risk levels and number of percentiles. The respective results are depicted in in Figure 4 (g-i). The variations in these factors could impact the performance of RiskQ.

RiskQ uses QR-DQN  and IQN  to learn value distributions. It shares QR-DQN and IQN's converging property. As stated in  and , the greedy distributional Bellman update operator of IQN is not a contraction mapping, which is an inherent drawback of the distributional RL. Recently,  modified IQN with a new distributional Bellman operator, indicating the optimal CVaR policy corresponds to a fixed point. However, but its general convergence is unclear. As shown in Appendix (Figure 19), the new method that combines RiskQ with  performs poorly. This suggests that remedying the non-contraction mapping issues may not be important enough for performance improvement as existing risk-sensitive RL methods (e.g., IQN) are already working well.

For risk-sensitive exploration, we have combined RiskQ with LQN , the results are depicted in the Appendix (Figure 20). It shows that risk-sensitive exploration is a new direction of the future work.

## 6 Conclusion

It is important to coordinate behaviors of multi-agents in risk-sensitive environments. We have formulated the coordination requirement as the risk-sensitive individual-global-maximization (RIGM) principle which is a generalization of the individual-global-maximization (IGM) principle and the Distributional IGM principle. Existing multi-agent value factorization does not satisfy the RIGM principle for common risk metrics such as the Value at Risk (VaR) or distorted risk measures. We propose, RiskQ, a risk-sensitive value factorization approach for Multi-Agent Reinforcement Learning (MARL). RiskQ satisfies the RIGM theorem for the VaR and distorted risk measures via modeling the quantile of joint state-action return distribution as weighted sum of the quantiles of per-agent return distribution utilities. We show that RiskQ can obtain promising results through extensive experiments.

AcknowledgementThis work was partially supported by the National Natural Science Foundation of China (No. 61972409), by the FuXiaQuan National Independent Innovation Demonstration Zone Collaborative Innovation Platform (No.3502ZCQXT2021003), the Fundamental Research Funds for the Central Universities (No. 20720230033), by PDL (2022-PDL-12), by the China Postdoctoral Science Foundation (No.2021M690094). We would like thank the anonymous reviewers for their valuable suggestions.