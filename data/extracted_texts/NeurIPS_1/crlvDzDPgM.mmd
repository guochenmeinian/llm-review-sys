# Customized Subgraph Selection and Encoding for Drug-drug Interaction Prediction

Haotong Du\({}^{1}\)   Quanming Yao\({}^{2}\)   Juzheng Zhang\({}^{2}\)   Yang Liu\({}^{1}\)   Zhen Wang\({}^{1}\)

\({}^{1}\)Northwestern Polytechnical University  \({}^{2}\)Tsinghua University

duhaotong@mail.nwpu.edu.cn w-zhen@nwpu.edu.cn

qyaoaa@tsinghua.edu.cn {juzhengzh00,yangliuyh}@gmail.com

Corresponding author.

###### Abstract

Subgraph-based methods have proven to be effective and interpretable in predicting drug-drug interactions (DDIs), which are essential for medical practice and drug development. Subgraph selection and encoding are critical stages in these methods, yet customizing these components remains underexplored due to the high cost of manual adjustments. In this study, inspired by the success of neural architecture search (NAS), we propose a method to search for data-specific components within subgraph-based frameworks. Specifically, we introduce extensive subgraph selection and encoding spaces that account for the diverse contexts of drug interactions in DDI prediction. To address the challenge of large search spaces and high sampling costs, we design a relaxation mechanism that uses an approximation strategy to efficiently explore optimal subgraph configurations. This approach allows for robust exploration of the search space. Extensive experiments demonstrate the effectiveness and superiority of the proposed method, with the discovered subgraphs and encoding functions highlighting the model's adaptability.

## 1 Introduction

Precise prediction of drug-drug interactions (DDIs) is essential in biomedicine and healthcare research . Drug combination therapy  can enhance treatment effectiveness for certain diseases; however, it also increases the risk of adverse drug reactions, potentially threatening patient safety . Identifying DDIs through laboratory experiments is both costly and time-consuming . With the success of deep learning, researchers have increasingly explored computational methods for DDI prediction. Early approaches primarily relied on molecular fingerprint information  or hand-engineered features , often neglecting the pre-existing interaction properties between drugs.

Considering drugs as nodes and their interactions as edges, DDI prediction can be framed as a multi-relational link prediction problem within the constructed drug interaction network. Recent advancements in graph neural networks (GNNs)  have consistently achieved superior performance in this task. Specifically, subgraph-based methods, such as SumGNN , EmerGNN , and KnowDDI , have shown promising results by selecting subgraphs around query edges and applying sophisticated encoding functions (message passing functions) to represent these subgraphs, Such methods transform the multi-relational link prediction task into a multi-type subgraph classification problem. Figure 1 illustrates the pipeline of subgraph-based methods.

However, due to the dense nature  of drug interaction networks and their complex interaction semantics , existing hand-designed subgraph methods often fail to capture the nuanced but crucial information across different data inputs. In the initial phase of the reasoning pipeline, the subgraph sampler must have the capability to customize the selection of drug subgraphs for different queries, thereby ensuring precise contextualization of the reasoning evidence.

In terms of encoding process, the encoding function must be capable of modeling a wide variety of drug interactions within the drug interaction network. Real-world drug interactions exhibit complex mechanisms, for instance, metabolism-based interactions display asymmetric semantic patterns, whereas phenotype-based interactions are symmetric. Manually designed encoding functions are limited in their ability to accommodate both types of distinct semantic patterns simultaneously . Therefore, designing a customized and data-adaptive subgraph-based pipeline is essential for effective DDI prediction.

Neural architecture search (NAS) [24; 25] has achieved remarkable success in designing data-specific models, often surpassing architectures carried by human experts in various fields, such as computer vision , graph neural network , and knowledge graph learning . However, effectively selecting suitable subgraphs from the vast space of candidates and efficiently optimizing the joint search process of subgraph selection and encoding remain open challenges.

In this paper, we leverage NAS to search for data-specific components in the subgraph-based pipeline. Specifically, we design search spaces for pipeline components, including subgraph selection and encoding spaces, to capture various drug interaction patterns. To enable efficient exploration of the extensive subgraph selection space, we introduce a relaxation mechanism that continuously selects subgraphs in a structured manner. Additionally, we propose a subgraph representation approximation strategy to reduce the high cost of explicit subgraph sampling, enabling efficient and robust search. Compared with existing methods in Table 1, our proposed **C**ustomized **S**ubgraph **S**election and **E**ncoding for **D**rug-**D**rug **I**nteraction prediction (**CSSE-DDI**) achieves fine-grained subgrpah selection and data-specific encoding functions, providing an efficient and precise method for drug interaction prediction. Our main contributions are summarized as follows:

* We present CSSE-DDI, a searchable framework for DDI prediction that adaptively customizes the subgraph selection and encoding processes. To the best of our knowledge, this is the first application of NAS techniques to tailor an adaptive subgraph-based pipeline for the DDI prediction task.
* We construct expressive search spaces to ensure precise capture of evidence for drug interaction prediction. Additionally, we devise a relaxation mechanism to transform the discrete subgraph selection space into a continuous form, enabling differentiable search. Simultaneously, we apply a subgraph representation approximation strategy to mitigate the inefficiencies of explicit subgraph sampling, thereby accelerating the search process.
* Extensive experiments on benchmark datasets demonstrate that our method, which searches for customized pipelines, achieves superior performance compared to hand-designed methods. Additionally, our approach effectively captures the underlying biological mechanisms of drug-drug interactions.

    & Fine-grained & Data-specific \\  & Subgraph & Encoding \\  & Selection & Function \\  SEAL  & ✗ & ✗ \\ GraIL  & ✗ & ✗ \\ SumGNN  & ✗ & ✗ \\ SNRI  & ✗ & ✗ \\ KnowDDI  & ✓ & - \\  MR-GNAS  & - & ✓ \\ AutoGEL  & - & ✓ \\ 
**CSSE-DDI** & ✓ & ✓ \\   

Table 1: Comparing with existing methods.”-” represents not applicable.

Figure 1: The pipeline of subgraph-based methods includes subgraph selection and subgraph encoding. In this work, we focus specifically on searching for components within the red-dotted lines. Without customized subgraph selection, SumGNN samples subgraphs using a fixed subgraph range \(k\), selecting the \(k\)-hop neighbors of each drug as associated subgraphs for predicting drug-drug interactions (DDIs). This coarse-grained approach is straightforward and easy to implement, but it may introduce noise or omit valuable information needed to reason about diverse drug pair interactions.

Related Works

Subgraph-based Link PredictionRecently, subgraph-based methods [18; 19; 28] have emerged as a promising approach, showing superior performance in link prediction tasks. Different from canonical GNNs, subgraph-based method extracts a subgraph patch for each training and test query, learning a representation of the extracted patch for final prediction, as illustrated in Figure 1.

Existing works has primarily focused on designing more informative subgraphs and more expressive encoding functions. However, they do not take into account customizing these components to deal with various data. Specifically, in terms of subgraph sampler, current approaches lack fine-grained and adaptive extraction for different query subgraphs. While PS2  demonstrates the effectiveness of identifying optimal subgraphs for each edge in homogeneous graph link prediction, there is no comparable work in multi-relational graph link prediction. In dense DDI networks, fine-grained identification of subgraph for different queries is even more crucial.

As for the encoding function, existing works overlook the importance of data-specific encoding, which has been emphasized in recent literature [30; 27]. Customized encoding functions are especially advantageous for drug interaction networks with complex and diverse interactions.

GNN-based DDI PredictionRecently, there has been growing interest in applying GNNs for DDI prediction [8; 9]. However, these works execute message-passing functions over the entire graph, which limits their ability to capture explicit local evidence for specific query drug pairs and lack interpretability. In contrast, subgraph-based DDI prediction methods [12; 63; 13; 14] transform the multi-relational link prediction problem into a subgraph classification problem by extracting subgraphs around query nodes, achieving strong performance. Nevertheless, these works use the same subgraph extraction strategy for all queries and rely on a fixed message-passing function to handle complex DDIs, which limits their flexibility and adaptivity in dense DDI networks.

Graph Neural Architecture SearchGraph neural architecture search (GNAS)  aims to find high-performing GNN architectures using NAS techniques. Recent studies [30; 27] have explored GNAS to create more expressive GNN models across various tasks. AutoDDI , for instance, automatically designs GNN architectures to learn molecular graph representations of drugs for DDI prediction. However, research on optimizing graph sampling for GNAS remains limited due to the diversity of graph-structured data.

Regarding search strategy, early approaches explores the search space using reinforcement learning  or evolutionary algorithms , which is highly inefficient. One-shot approaches  instead construct an over-parameterized network (supernet) and optimize it using gradient descent, leveraging continuous relaxation of the search space to improve search efficiency. The recently proposed few-shot NAS paradigm  further enhances supernet evaluation consistency by generating multiple sub-supernets.

## 3 Proposed Method

### Problem Formulation

Given a set of drugs \(\) and interaction relations \(\) among them, the drug interaction network is denoted as \(_{}=\{(u,r,v) u,v,r\}\), with each tuple \((u,r,v)\) describes an interaction between drug \(u\) and drug \(v\). Consequently, drug-drug interaction (DDI) prediction can be framed a multi-relational link prediction task within the drug interaction network \(_{}\). The objective is to predict the types of interactions between two given drug nodes, which can be denoted as a query \((u,?,v)\), i.e., given the query drug-pair entities \(u\) and \(v\), to determine the interaction \(r\) that makes \((u,r,v)\) valid.

Moreover, instead of directly predicting on the entire graph \(_{}\), subgraph-based methods decouple the prediction process into two stages: (1) selecting a query-specific subgraph and (2) encoding the subgraph to predict interactions, as shown in Figure 1. The prediction pipeline then becomes

\[_{},(u,v)}_{u,v }}_{u,v}, \]

where the sampler selects a subgraph \(G_{u,v}\) conditioned on the given query \((u,?,v)\). Using this subgraph \(G_{u,v}\), the encoding function produces the final predictions \(_{u,v}\).

Building on previous analysis and existing research, and inspired by NAS, we propose to search for data-adaptive subgraph selection and encoding components to obtain a customized subgraphpipeline. In Section 3.2, we first introduce the well-designed subgraph selection and encoding spaces to ensure comprehensive coverage of crucial information in various drug interaction networks. Further, in Section 3.3 we present a subgraph relaxation strategy and approximation mechanisms for subgraph representations to facilitate efficient differentiable search. Finally, we develop a robust search algorithm to address the customized search problem with stability and precision.

### Search Space

#### 3.2.1 Subgraph Selection Space

In practice, subgraph-based methods define the drug-pair subgraph between drug pairs as the union or interaction of \(k\)-hop ego-network 2 of query drugs. Here, \(k\) is a key hyperparameter that determines the range of message propagation aggregated by the central node. Selecting \(k\) is crucial to model performance, as it dictates whether the model has access to high-quality evidence context for accurate prediction.

Prior works  typically utilize a fixed hyperparameter for all drug pairs, i.e., selecting the union of a fixed \(k\)-hop ego-network for arbitrary queries. Nevertheless, this approach can lead to an imprecise collection of evidence for interaction reasoning, potentially undermining the reasoning process due to missing critical information or the inclusion of excessive irrelevant information.

Based on the above analysis, we define a drug-pair subgraph selction space containing a range of subgraphs of different sizes for a given query \((u,v)\):

\[_{u,v}=\{^{i,j}_{u,v}\,|1 i,j\}, \]

where \(^{i,j}_{u,v}\) is generated by taking the union of the \(i\)-hop ego-network of node \(u\) and the \(j\)-hop ego-network of node \(v\), i.e., \(^{i,j}_{u,v}=\{z\,|\,z(u_{i}(u) v _{j}(v))\}\), where \(_{i}(u)\) and \(_{j}(v)\) are the \(i\)-hop and the \(j\)-hop neighbors of \(u\) and \(v\), respectively. The threshold \(\) constrains the maximum subgraph range.

Since each drug-pair has a specific subgraph selection space, the overall size of space in the entire graph is \(^{2||}\), where \(||\) represents the number of edges in the drug interaction network. A larger \(||\) result in a subgraph selection space that grows exponentially with the number of edges. Therefore, efficiently searching for the optimal subgraph configurations for different queries is challenging.

#### 3.2.2 Subgraph Encoding Space

For the automated design of the subgraph encoding function, we first adopt a unified message passing framework  comprising several key modules: the message-computing function MES, the aggregation function AGG, the combination function COM, and the activation function ACT, as follows:

\[\ _{u}((_{v},_{r(u,v)})_{v_{1}(u)}), \\ \ _{u}((_{u},_{u})), \]

where \(_{u}^{d}\) and \(_{r}^{d}\) represent the embeddings of node \(u\) and interaction \(r\), respectively, and \(_{u}\) is the intermediate message representation of \(u\) aggregated from its neighborhood \(_{1}(u)\).

A substantial amount of literature  has focused on manually designing these modules to improve performance. However, such encoding functions are inflexible for handling diverse interaction patterns across different drug interaction network. For example, interactions in DrugBank  describe how one drug affects the metabolism of another one. The excretion of Acamprosate, for instance, may be decreased when combined with AcetyIsalicylic acid (Aspirin). Such interaction pattern is asymmetric, meaning \(r(x,y) r(y,x)\). Conversely, interactions in the TWOSIDES dataset  are primarily at the phenotypic level, such as headache or pain in throat, representing symmetric patterns where \(r(x,y) r(y,x)\). These two relational semantics are distinctly different, and existing hand-designed encoding functions struggle to capture such diverse semantics effectively .

Here, we aim to perform an adaptive searching for the encoding function in the context of drug interaction prediction. Based on the framework presented in Eq. (3), we design an expressive subgraph encoding space with a set of candidate operations. Detailed explanations of these modules can be found in the Appendix A.1.

After encoding the subgraph \(_{u,v}\), we obtain the representation \(_{u,v}\) of the input subgraph \(_{u,v}\). The predictor then maps the representation \(_{u,v}\) to the probability logits for different interactions between drug pairs, i.e., \(y_{u,v}=_{}_{u,v}\), where \(_{}^{2d||}\) is the parameter of the predictor.

### Search Strategy

#### 3.3.1 Search Problem

Based on the well-designed search space described above, we formulate a bi-level optimization problem to adaptively search for the optimal configuration of subgraph-based pipelines.

**Definition 1** (Customized Subgraph-based Pipeline Search Problem).: _Let \(\) denote the subgraph encoding space, \(_{u,v}\) represent the subgraph selection space for the query \((u,v)\), \(\) be a candidate encoding function in \(\), \(\) represent the parameters of a model from the search space, and \(^{*}(_{u,v};)\) denote the trained operation parameters. Let \(_{}\) and \(_{}\) denote the training and validation sets, respectively. The search problem is formulated as follows:_

\[_{,_{u,v} _{u,v}}_{(u,r,v)_{}}( ^{*}(_{u,v};);_{u,v}; ), \] \[~{}^{*}(_{u,v};)= _{}_{(u,r,v)_{}} (;_{u,v};), \]

_where the classification loss \(\) is minimized for all interactions, while the performance measurement \(\) is expected to be maximized._

In this work, we adopt the differentiable search paradigm  to solve the bi-level optimization problem, which is widely used in recent NAS literature  and enables efficient exploration of the search space. Nevertheless, our proposed subgraph selection space poses two technical challenges: **First**, we cannot directly apply relaxation strategies, which is a prerequisite for differentiable NAS methods, to make the discrete selection space continuous. This limitation arises because different subgraphs in the selection space contain diverse nodes and edges, making it challenging to design a relaxation function that unifies subgraphs of varying sizes. **Second**, to enable searching within the subgraph selection space, we would need to first generate all subgraphs in the space. However, sampling such a large number of subgraphs is computationally intractable.

To address these challenges, we design a subgraph selection space relaxation mechanism in Section 3.3.2. Additionally, we introduce an intuitive subgraph representation approximation strategy in Section 3.3.3 to reduce the high costs associated with explicit sampling.

#### 3.3.2 Relaxation of Subgraph Selection Space

Technically, as in existing NAS works , one typically needs to relax the search space into continuous form to enable effective backpropagation training. However, for the subgraph selection space, the traditional continuous relaxation strategy is not directly applicable due to the structural mismatch between graphs and vectors.

To address this, we first utilize encoding function \(f()\) to encode subgraphs with different scopes. This approach provides all subgraphs with representations of the same dimension, making it feasible to implement a relaxation strategy. Additionally, inspired by the reparameterization trick , we adopt the Gumbel-Softmax function to facilitate differentiable learning over a discrete space:

\[}_{u,v}^{i,j}=_{1 i,j}_{u,v}^{i,j}))+_{i,j})/)}{_{i^{},j^{ }=1}^{}((g(f(_{u,v}^{i^{},j^{}}))+_{i^{},j^{}})/)}f(_{u,v}^{i,j}), \]

where \(g()\) scores the subgraph representations using multiple linear layers, \(_{i,j}=-(-(_{i,j}))\) is the Gumbel random variable, \(_{i,j}\) is a uniform random variable, and \(\) is the temperature parameter controlling sharpness. \(}_{u,v}^{i,j}\) is the mixed selection operation of subgraph \(_{u,v}^{i,j}\) used to optimize searching process.

#### 3.3.3 Subgraph Representation Approximation Strategy

To solving the optimization problem as Eq. (4) and (5), we need to explicitly sample all the candidate subgraphs within the subgraph selection space \(_{u,v}\) for each query. However, one of the most challenging aspects of subgraph-based approaches is their inefficient subgraph sampling process .

Upon examining our subgraph selection space, we observe that all subgraphs are generated by combining multi-hop ego-networks of the target nodes, encompassing multiple neighborhood hops. Inspired by the \(k\)-subtree extractor , we apply an encoding function to the entire graph and use the resulting node representations of \(u\) and \(v\) as the ego-network representations for these nodes. The representation of the drug pair can then be obtained by concatenating the ego-network representations of \(u\) and \(v\). Formally, if we denote by \(f(_{},u,i)\) the \(i\)-layer hidden representation of node \(u\) produced by encoding function applied to \(_{}\), then

\[f(_{u,v}^{i,j})(f(_{},u, i),f(_{},v,j)), \]

The \(k\)-subtree extractor represents the \(k\)-subtree structure rooted at a given node, which mirrors the structure as the \(k\)-hop ego-network. This approximation strategy only requires executing the encoding function on the entire drug interaction network, thereby efficiently yielding subgraph representations of varying scopes, which significantly improves the efficiency in solving the bi-level optimization problem.

#### 3.3.4 Robust Search Algorithm

Using the proposed subgraph selection relaxation mechanism, we can transform the overall discrete search space in Definition. 1 into a continuous form, allowing the search problem to be solved by the one-shot NAS paradigm. Additionally, our subgraph representation approximation strategy efficiently obtains subgraph representations and reduces search costs

Following , we adopt the single path one-shot training strategy (SPOS)  to reduce the computational cost of supernet training. However, the one-shot approach [55; 56; 45], i.e., using the same supernet parameters \(\) for all architectures, can decrease the consistency between the supernet's performance estimation and the ground-truth performance . Inspired by few-shot NAS , we propose a message-aware partitioned supernet training strategy to mitigate the coupling effect of different message-computing operators . By partitioning the supernet to form sub-supernets based on the type of message-computing function, this strategy improves the consistency and accuracy of supernet, enabling the search algorithm more stable and robust. Algorithm 1 delineates the full procedure, with further details provided in Appendix A.2.

```
Input: Supernet \(\), number of partitions based on message computing function categories \(M\) (\(M=4\)), subsupernet \(_{i},(i=1,,M)\). // supernet training phase
1Train \(\) by continuously sampling a single path until convergence; // supernet partition phase
2 Partition \(\) into \(M\) sub-supernets \(_{1},,_{M}\); // sub-supernet training phase
3forall\(i=1,,M\)do
4 Initialize \(_{i}\) with weights transferred from \(\);
5 Train \(_{i}\) by continuously sampling a single path until convergence;
6
7 end for // searching phase
8 Search the optimal encoding function from sub-supernets \(_{1},,_{M}\) on validation data by natural gradient descent;
9 Select the optimal subgraphs from sub-supernets \(_{1},,_{M}\) on validation data by preserving the subgraphs with the largest probabilities;
```

**Algorithm 1**The search algorithm of CSSE-DDI.

### Comparison with Existing Works

While many works [12; 13; 14] have explored DDI prediction using subgraph-based methods, our approach introduces two significant advancements. First, to the best of our knowledge, our method (CSSE-DDI) is the first to customize the subgraph selection and encoding processes specifically for subgraph-based DDI prediction. In contrast, previous methods rely on fixed subgraph selection strategy to sample subgraphs and employ hand-designed functions for encoding, as summarized in Table 1. Consequently, our method can adapt data-specific components within subgraph-based pipelines, outperforming existing methods in both performance and efficiency (Section 4.2). Moreover, our approach not only selects fine-grained drug-pair subgraphs that enhance interpretability through potential pharmacokinetic and metabolic concepts (Section 4.6.1), but also searches for data-specific encoding functions that accurately capture the semantic features of drug interactions (Section 4.6.2).

Experiments

### Experimental Setup

DatasetsExperiments are conducted on two public benchmark DDI datasets: DrugBank  and TWOSIDES . Detailed descriptions of these datasets are presented in Appendix B.1.

Experimental SettingsFollowing , we examine two DDI prediction task settings: S0 and S1. Let the drug pairs for DDI prediction be denoted as \((u,v)\). In the S0 setting, both drug nodes \(u\) and \(v\) are present in the known DDI graph. Existing DDI prediction methods are typically evaluated in this setting. In contrast, the S1 setting involves a pair (u, v) where one drug is known and the other is a novel drug not represented in the known DDI graph. This scenario highlights the critical need for DDI predictions involving new drugs in real-world applications.

Evaluation MetricWe follow  to evaluate our method. For the DrugBank dataset, where each drug pair contains only one interaction, we use the following metrics: F1 Score, Accuracy and Cohen's \(\). For the TWOSIDES dataset, where multiple interactions may exist between a pair of drugs, we consider the following metrics: ROC-AUC, PR-AUC and AP@50. Additional details are provided in Appendix B.2.

BaselinesWe compare CSSE-DDI with the following representative DDI prediction method: (i) GNN-based methods include Decagon , GAT , SkipGNN , CompGCN , ACDGNN , and TransFOL . (ii) Subgraph-based methods include SEAL , GraIL , SumGNN , SNRI , KnowDDI  and LaGAT . (iii) NAS-based method include MR-GNAS , and AutoGEL .

We also compare our method with two variants, including CSSE-DDI-FS and CSSE-DDI-FF. The configurations of these variants are as follows: (i) **CSSE-DDI-FS**: This variant omits fine-grained subgraph selection for each query, using fixed k-layer drug node representations to generate the subgraph representation. (ii) **CSSE-DDI-FF**: This variant does not search for the encoding function, instead using a fixed encoding function backbone to capture semantic and topological features in the drug interaction network. In this case, we employ a 3-layer CompGCN model as the backbone. For all baselines, we obtain the results by rerunning the released codes.

ImplementationWe implement our method3 based on PyTorch framework . Following existing GNN-based methods , we select a 3-layer encoding function backbone for both datasets. The maximum threshold \(\) for the subgraph selection space is set to 3. More experimental details are given in the Appendix B.3.

### Performance Comparison in S0 settings

Table 2 shows the overall results across all benchmarks in S0 setting. As can be seen, CSSE-DDI consistently outperforms all baselines on each dataset, demonstrating its effectiveness in searching for data-specific subgraph-based pipelines for DDI prediction task. Among the baselines, subgraph-based methods significantly outperform full-graph-based methods due to their enhanced ability to reason over local subgraph contexts. Within the subgraph-based methods, SEAL, GraIL, SumGNN, and SNRI use a fixed sample strategy to select subgraphs, which may not be optimal for different drug-pair queries.

When it comes to NAS-based method, MR-GNAS and AutoGEL contain well-established search spaces that embrace multi-relational message-passing schema, focusing primarily on automated encoding function design using the one-shot NAS paradigm. While CSSE-DDI adopts a single path supernet training strategy and a message-aware partitioning approach to search for data-adaptive subgraph-based pipelines with stability and robustness, enabling the model to achieve excellent performance across various datasets. Moreover, the consistent performance gains of CSSE-DDI over its two variants validate the importance of jointly customizing subgraph-based pipeline components, i.e., fine-grained subgraphs and data-specific encoding functions, to fit datasets rather than relying on a fixed approach.

Figure 2 shows the learning curves of several competitive methods on both datasets, including CompGCN, KnowDDI and the proposed CSSE-DDI. As can be seen, the searched models not only outperform the baselines but also demonstrate a clear advantage in efficiency, highlighting that enhancing model flexibility and adaptability is essential for improving performance and efficiency.

### Choices of Search Strategy

To demonstrate the effectiveness of our search strategy, we introduce two variants with different search strategies: (i) **CSSE-DDI w/o MAP**: This variant uses only one trained supernet to serve as a performance evaluator for candidate architectures, instead of generating multiple sub-supermeters by Message-Aware Partition (MAP) strategy. (ii) **CSSE-DDI w/o SPOS**: This variant utilizes the message-aware partition strategy to jointly optimize the supernet weights and architectural parameters, without using the Single Path One-Shot (SPOS) strategy .

In Table 3, we compare CSSE-DDI with other variants. As can be seen, the absence of either message-aware partition strategy or sampling-based NAS strategy negatively impacts performance. The performance gains achieved through the message-aware partition strategy arise from using multiple sub-supernets, which provide more accurate performance estimations to guide the search process. Regarding the SPOS strategy, it decouples supernet training from architecture search, making it more efficient and robust in practice.

     } &  &  &  \\   & **Task Type** &  &  \\   & **Methods** & F1 Score & Accuracy & Cohen’s \(\) & ROC-AUC & PR-AUC & AP@50 \\    } & Decagon & 57.35\(\)0.26 & 87.19\(\)0.28 & 86.07\(\)0.08 & 91.72\(\)0.04 & 90.60\(\)0.12 & 82.06\(\)0.45 \\  & GAT & 33.49\(\)0.36 & 77.18\(\)0.15 & 74.20\(\)0.23 & 91.18\(\)0.14 & 89.86\(\)0.05 & 82.80\(\)0.17 \\  & SkipGNN & 59.66\(\)0.26 & 85.83\(\)0.18 & 84.20\(\)0.16 & 92.04\(\)0.08 & 90.90\(\)0.10 & 84.25\(\)0.25 \\  & CompGCN & 71.20\(\)0.70 & 88.30\(\)0.29 & 86.15\(\)0.35 & 93.00\(\)0.07 & 91.26\(\)0.07 & 86.18\(\)0.10 \\  & ACDGNN & 86.24\(\)0.93 & 90.53\(\)0.38 & 87.81\(\)0.33 & 93.69\(\)0.47 & 92.12\(\)0.21 & 87.45\(\)0.24 \\  & TransFOL & 89.97\(\)1.64 & 91.92\(\)0.89 & 90.92\(\)0.72 & 94.16\(\)0.62 & 93.52\(\)0.53 & 88.13\(\)0.39 \\    } & SEAL & 48.82\(\)0.98 & 76.61\(\)0.26 & 71.91\(\)0.59 & 90.74\(\)0.22 & 90.11\(\)0.17 & 84.13\(\)0.13 \\  & GraIL & 73.20\(\)0.69 & 85.40\(\)0.39 & 82.70\(\)0.47 & 92.93\(\)0.10 & 91.69\(\)0.14 & 87.43\(\)0.09 \\  & SumGNN & 78.35\(\)0.51 & 89.05\(\)0.36 & 87.28\(\)0.08 & 92.62\(\)0.04 & 90.80\(\)0.40 & 85.75\(\)0.10 \\  & SNRI & 85.57\(\)0.32 & 90.15\(\)0.21 & 88.94\(\)0.36 & 93.12\(\)0.18 & 92.64\(\)0.12 & 87.53\(\)0.11 \\  & KnowDDI & 90.06\(\)0.27 & 93.15\(\)0.19 & 91.87\(\)0.21 & 95.05\(\)0.06 & 93.75\(\)0.05 & 89.24\(\)0.06 \\  & LaGAT & 81.63\(\)0.56 & 86.21\(\)0.18 & 85.38\(\)0.23 & 89.78\(\)0.21 & 86.33\(\)0.15 & 83.75\(\)0.36 \\    } & MR-GWAS & 74.24\(\)0.45 & 88.17\(\)0.24 & 87.31\(\)0.11 & 93.85\(\)0.07 & 91.80\(\)0.03 & 87.16\(\)0.05 \\  & AutoGEL & 76.87\(\)0.63 & 89.35\(\)0.59 & 86.14\(\)0.41 & 94.11\(\)0.32 & 92.35\(\)0.29

### Sensitivity Analysis of the Threshold \(\)

Here, we analyze the effect of the threshold \(\) used in subgraph selection space. Figure 3 shows the impact of varying \(\). As can be observed, model performance continues to get better as the threshold \(\) grows. When the threshold \(=3\), the model performance nears saturation, ther improvements. This is likely because most of the essential information for DDI prediction is contained within the \(3\)-hop ego-subgraphs of target drugs. Intuitively, larger subgraphs may provide additional useful information. However, in practice, due to the inherent biases of the search algorithm, achieving an optimal model may be challenging. When \(\) is too large, it may introduce noise and dilute the critical information. A similar phenomenon has been found in the existing work SumGNN . Besides, excessively large thresholds \(\) will only lead to unnecessary expansion of the search space and higher computational costs.

### Performance Comparison in S1 settings

To further validate the effectiveness of our method, we use the S1 setting in the EmerGNN  method, to predict drug-drug interactions between emerging drugs and existing drugs. The experimental results are shown in Table 4. A significant performance drop from the transductive setting (S0) to the inductive setting (S1) demonstrates that DDI prediction for new drugs is more challenging. Although Emergnn, which is specifically designed for new drug prediction, achieves optimal performance, CSSE-DDI still demonstrates impressive results, outperforming existing GNN-based and subgraph-based methods. This strong performance is largely due to the robust learning capability of NAS technology in handling unknown data.

### Case Study

#### 4.6.1 Fine-grained Subgraph Selection

We visualize exemplar query-specific subgraphs from the DrugBank dataset in Figure 4, highlighting **domain concepts** such as pharmacokinetics, metabolism, and receptor interactions. As shown, CSSE-DDI can identify distinctive subgraphs containing semantic information to support inference for different queries, revealing pharmacokinetic and metabolic relationships.

  
**Dataset** &  &  \\ 
**Task Type** &  &  \\ 
**Methods** & F1 Score & Accuracy & Cohen’s \(\) & ROC-AUC & PR-AUC & Accuracy \\  CompGCN & 30.98\(\)3.26 & 52.76\(\)0.46 & 37.87\(\)1.28 & 84.83\(\)1.02 & 83.68\(\)1.86 & 74.64\(\)0.79 \\ Decagon & 11.39\(\)0.79 & 32.56\(\)0.92 & 20.29\(\)1.33 & 57.49\(\)1.75 & 59.38\(\)1.09 & 52.27\(\)1.48 \\ SumGNN & 26.57\(\)1.59 & 44.30\(\)1.04 & 40.24\(\)1.26 & 80.02\(\)2.17 & 78.42\(\)1.62 & 69.81\(\)1.77 \\ KnowDDI & 31.14\(\)1.24 & 53.44\(\)1.73 & 43.93\(\)1.17 & 84.23\(\)2.63 & 82.58\(\)1.94 & 74.72\(\)1.51 \\ EmerGNN & **58.13\(\)1.36** & **69.53\(\)1.97** & **62.19\(\)1.62** & 87.42\(\)0.39 & 86.20\(\)0.71 & 79.23\(\)0.54 \\ 
**CSSE-DDI** & 37.24\(\)1.13 & 58.57\(\)0.85 & 49.97\(\)1.01 & **88.33\(\)0.52** & **86.47\(\)0.27** & **80.01\(\)0.39** \\   

Table 4: Experimental results in S1 setting.

Figure 4: Visualization of the searched subgraphs corresponding to the specific drug pairs.

Figure 3: Performance given different hyperparameter \(\).

For example, to predict the interaction between DB00945 (Aspirin) and DB00682 (Warfarin), CSSE-DDI searches out the subgraph scope \((1,1)\), as depicted on the left part of Figure 4. Firstly, it can be seen from the figure that the therapeutic efficacy of DB00233 (Aminosalicylic acid) can decrease when combined with DB00945 (Aspirin), suggesting similarity between the two drugs  Given that DB00233 (Aminosalicylic acid) may increase the anticoagulant activity of DB00682 (Warfarin) and that DB00233 resembles DB00945 (Aspirin), it can be inferred that DB00945 (Aspirin) may similarly increase the anticoagulant activity of DB00682 (Warfarin). This example demonstrates that the identified subgraph contains sufficient semantic information to reason about the interaction between DB00945 (Aspirin) and DB00682 (Warfarin).

#### 4.6.2 Data-specific Encoding Function

Furthermore, we visualize the searched structure of encoding functions across all datasets in Figure 5. It is clearly illustrated that different combinations of the designed operations, i.e., data-specific encoding functions, are obtained.

In particular, the searched message-computing functions contain more CORR operations in the DrugBank dataset, while more MULT functions are searched in the TWOSIDES dataset. The CORR function is non-commutative , making it suitable for modeling asymmetric interactions (e.g., metabolic-based interactions) present in DrugBank. While MULT is suitable for modeling symmetric relations (phenotype-based interactions) due to its exchangeability .

## 5 Conclusion

We propose a searchable framework, CSSE-DDI, for DDI prediction. Specifically, we design refined search spaces to enable fine-grained subgraph selection and data-specific encoding function optimization. To facilitate efficient search, we introduce a relaxation mechanism to convert the discrete subgraph selection space into a continuous one. Additionally, we employ a subgraph representation approximation strategy to accelerate the search process, addressing the inefficiencies of explicit subgraph sampling. Extensive experiments demonstrate that CSSE-DDI significantly outperforms state-of-the-art methods. Moreover, the search results generated by CSSE-DDI offer interpretability in the context of drug interactions, revealing domain-specific concepts such as pharmacokinetics and metabolism.