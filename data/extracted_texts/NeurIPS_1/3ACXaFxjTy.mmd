# Unleashing the Potential of the Diffusion Model in Few-shot Semantic Segmentation

Muzhi Zhu\({}^{1}\)1  Yang Liu\({}^{1}\)1  Zekai Luo\({}^{1}\)1  Chenchen Jing\({}^{1}\)

Hao Chen\({}^{1}\)1  Guangkai Xu\({}^{1}\)  Xinlong Wang\({}^{2}\)  Chunhua Shen\({}^{1}\)

\({}^{1}\)Zhejiang University \({}^{2}\)Beijing Academy of Artificial Intelligence

###### Abstract

The Diffusion Model has not only garnered noteworthy achievements in the realm of image generation but has also demonstrated its potential as an effective pre-training method utilizing unlabeled data. Drawing from the extensive potential unveiled by the Diffusion Model in both semantic correspondence and open vocabulary segmentation, our work initiates an investigation into employing the Latent Diffusion Model for Few-shot Semantic Segmentation. Recently, inspired by the in-context learning ability of large language models, Few-shot Semantic Segmentation has evolved into In-context Segmentation tasks, morphing into a crucial element in assessing generalist segmentation models. In this context, we concentrate on Few-shot Semantic Segmentation, establishing a solid foundation for the future development of a Diffusion-based generalist model for segmentation. Our initial focus lies in understanding how to facilitate interaction between the query image and the support image, resulting in the proposal of a KV fusion method within the self-attention framework. Subsequently, we delve deeper into optimizing the infusion of information from the support mask and simultaneously re-evaluating how to provide reasonable supervision from the query mask. Based on our analysis, we establish a simple and effective framework named DiffewS, maximally retaining the original Latent Diffusion Model's generative framework and effectively utilizing the pre-training prior. Experimental results demonstrate that our method significantly outperforms the previous SOTA models in multiple settings. Our code is released at: [https://github.com/aim-uofa/DiffewS](https://github.com/aim-uofa/DiffewS)

## 1 Introduction

The Diffusion Model (DM) has demonstrated powerful capabilities in multiple visual generation tasks, including image generation , image editing , video generation , etc. At the same time, DM has also been proven to be a powerful method for self-supervised pre-training  employing unlabelled data. To exploit the representation ability of DM, there are currently two emerging topics in vision research: improving the learning paradigm  and downstream task adaptation. The latter often focuses on the Latent Diffusion Model  (LDM). By compressing images into latent space, it significantly decreases computational expenses and emerges as the first open-source Text-to-Image Diffusion Model scaled up to the LAION-5B  level. For example, ODISE ,DVP , DatasetDM  adapt LDM to multiple tasks such as depth estimation, semantic segmentation, but they all require training additional decoder heads, which increases training costs and may undermine the generalization ability and generation quality. Therefore, some works  have emerged that attempt to repurpose the Diffusion Model's generative framework and apply it to visual perception tasks without adding extra decoder heads. Nonetheless, these paradigms still cannot uniformly adapt to all tasks.

Let's reconsider the most fundamental question in using generative models for visual perception: _how to design a fine-tuning framework that can guarantee both generalization ability and precise prediction of details?_ Unfortunately, existing methods do not sufficiently address this challenge. The demands of the FSS task for open-set generalization and high-quality segmentation results precisely align with this challenge. Thus, **our first motivation** is to further address the fundamental question posed above by exploring the Diffusion Model on the FSS task.

FSS aims to segment query images given support samples. Traditional FSS methods[18; 19; 20] rely on a pre-trained backbone, achieving semantic matching and pixel-level prediction tasks through designing complex modules and long-term training. Recently, with the emergence of SAM , some works are based on foundation models to complete FSS, such as Matcher . It employs DINO for semantic matching and SAM for segmentation. Similarly, other works [24; 25] combine SAM with CLIP or MLLM to complete other open-set segmentation tasks. The current methods deal with matching(semantic) and segmentation as two distinct tasks through different modules. The Diffusion Model itself, however, exhibits significant potential in fine-grained pixel prediction tasks[13; 14; 16] and semantic correspondence tasks [26; 27; 28]. Hence, we seek to maximize the reuse of the generative framework by taking advantage of the innate priors within the Diffusion Model to accomplish the FSS task.

Recently, inspired by the in-context learning ability of large language models, Few-shot Semantic Segmentation has further evolved into the In-context Segmentation [29; 30] task (see Section 2). In-context Segmentation requires the model to have in-context learning ability for few-shot samples, posing new challenges to model's generalization capabilities. Consequently, it's now recognized as a crucial component in the evaluation process for generalist segmentation models. Therefore, **the second motivation** of our work is to lay the groundwork for the development of diffusion-based generalist segmentation models.

As a foundational work of Diffusion-based methods in the FSS field, we strive to achieve optimal performance with a simple and efficient design, while maximally preserving the generative framework of the Latent Diffusion Model. This minimal disruption to the original UNet structure allows us to better make use of pre-trained priors. We embark on a systematic exploration around the following four questions: 1) How to implement the interaction between the query image and the support image? 2) How to effectively inject information from the support mask? 3) What is a reasonable form of supervision from the query mask? 4) How to design effective generation process to transfer the pre-trained diffusion models to mask prediction task? Based on our observations, we ultimately establish the DiffewS framework and validate it in multiple settings, demonstrating the effectiveness of our method. Our main contributions include:

* We systematically study four crucial elements of applying the Diffusion Model to Few-shot Semantic Segmentation. For each of these aspects, we propose several reasonable solutions and validate them through comprehensive experiments.
* Building upon our observations, we establish the DiffewS framework, which maximally retains the generative framework and effectively utilizes the pre-training prior. Notably, we introduce the first diffusion-based model dedicated to Few-shot Semantic Segmentation, setting the groundwork for a diffusion-based generalist segmentation model.
* We validate the effectiveness of the DiffewS framework under several experimental settings, demonstrating that our method not only achieves a performance comparable with the state-of-the-art (SOTA) model in a strict Few-shot Semantic Segmentation setting, but also significantly outperforms the current SOTA model in an 'in-context learning' setting,

## 2 Related Work

**Diffusion models** have shown impressive performance on visual generation tasks such as text-based image generation [1; 2], image editing [3; 4], and video generation [5; 6; 7]. Current research on leveraging Diffusion models to enhance visual perception tasks mainly focuses on two directions: one is the direct use of diffusion models to generate images, aiming to address the issue of insufficient data, such as instance segmentation [31; 32; 33], semantic segmentation , few-shot segmentation  and so on. Another direction is to transfer features from Diffusion models to other visual tasks, which aligns with the research direction of this paper.

ODISE  uses frozen diffusion models for panoptic segmentation of any category in the wild. DVP , DatasetDM , GenPercept , Geowizard  adapt LDM to multiple tasks such as depth estimation, semantic segmentation, and surface normal. Marigold  fine-tunes diffusion models on synthetic data for affine-invariant monocular depth estimation and achieves impressive performance. Different from the above methods, we focus on using diffusion models to model the visual correlations of multiple reference images and a target image for few-shot segmentation. The most related work to this paper is a concurrent study , which focuses on utilizing diffusion models for in-context segmentation. However, it disrupts the original U-Net structure and the priors of the diffusion model to some extent. In contrast, our work offers a more comprehensive and systematic analysis of applying diffusion models to Few-Shot Semantic Segmentation tasks.

**Few-shot semantic segmentation**[39; 40] aims to segment target objects in an input image given a few annotated support images. Traditional FSS methods either explore prototype learning [41; 42; 43] of support images to predict query images' masks or use pixel-level information [44; 45; 18] to exploit the support information. For example, some works [29; 30; 46] demonstrate powerful generalization ability by unifying various segmentation tasks in an in-context learning framework. SegGPT  can exactly segment any semantic conception by using one or a few support images, which motivates us to explore the potential of the diffusion model for the FSS task under the in-context setting .

## 3 Preliminary

We first review the Latent Diffusion Model  used in our paper. It consists of an auto-encoder (VAE) and a UNet. The auto-encoder facilitates a two-way transformation between the RGB image \(^{H W 3}\) and the latent space \(^{h w c}\). Both the forward and backward processes of diffusion are carried out in the latent space, and we denote the noisy latent code at time \(t\) as \(^{(t)}=_{t}}+_{t}}\), where \(_{t}=_{s=1}^{t}(1-_{s})\) is the noise schedule. \(_{s}\) is the variance sampled from a variance schedule \(_{t}(0,1)_{t=1}^{T}\). The UNet can be considered as a series of equally weighted denoiser \(_{}(^{(t)},t)\). The training objective \(\) can be simplified as:

\[=_{, N(0,1),t(T)} [\|-_{}(^{(t)},t)\| _{2}^{2}] \]

Furthermore, to simplify comprehension and narration, we can reparametrize the output of UNet \(_{}\) as the form of v-prediciton \(v_{}\). The training objective can be further elaborated as:

\[=_{, N(0,1),t(T)} [\|-v_{}(^{(t)},t)\|_{2} ^{2}] \]

This implies that the goal of every training round is to denoise \(^{(t)}\) to \(\) for any time step \(t\).

Secondly, we present our task definition, using one-shot segmentation as an illustration. Given a data triplet (\(_{s}\), \(_{s}\), \(_{q}\)), here \(_{s}\) and \(_{q}\) denote the support image and query image respectively, both sharing an overlapping category \(c\). \(_{s}\) is the mask of category \(c\) in the support image. Our task is to predict the mask corresponding to category \(c\) in \(_{q}\). In the strict one-shot segmentation setting, the category sets of the training set and the test set are disjoint.

Our objective is to fully utilize the priors in the Latent Diffusion Model and equip it with Few-shot Semantic Segmentation capabilities. This leads us to reuse the original VAE to convert \(_{s}\), \(_{q}\) and \(_{q}\) into latent variables \(_{s}\), \(_{q}\) and \(_{mq}\). Thus, our task is further simplified to explore how to improve the structure of UNet to \(v_{}^{c}\) so that it can accept \(_{s}\), \(_{q}\) and \(_{s}\) as inputs, and use \(_{mq}\) as supervision.

This supervised approach in the latent space has been certified effective in tasks such as depth estimation  and semantic segmentation. Concretely, our training objective \(_{}\) is transformed into:

\[_{}=_{(_{s}, _{q},_{s},_{mq})}[ \|_{mq}-v_{}^{*}(_{s},_{q}, _{s})\|_{2}^{2}] \]

where \(\) represents the constructed training dataset. In addition, we omitted the input of time \(t\). Our early experiments revealed that performing multiple steps of noise addition and denoising during training did not bring performance improvement.

## 4 Method

Our investigation into model design primarily adheres to two criteria: 1. Strive for the design to be as simple and efficient as possible, while optimizing performance in Few-shot Semantic Segmentation. 2. Maximize the preservation of the Latent Diffusion Model's generative schema, minimizing alteration to the original UNet structure, so as to better utilize the pre-training prior.

Specifically, four key issues need to be addressed: 1) How to facilitate interaction between the query image and support image? 2) How to effectively incorporate information from the support mask? 3) What form of supervision from the query mask would be most reasonable? 4) How to design an effective generation process to transfer the pre-trained diffusion models to mask prediction task? In this section, we discuss the four issues mentioned above in detail. We engage in fair comparison tests and analysis on several feasible strategies. Drawing on our observations, we eventually settle on our framework, DiffewS (see Figure 1).

### Interaction between query and support images

We first decompose the block of the l-th layer in UNet into three components: a self-attention layer \(\), a cross-attention layer \(\), and a feedforward layer \(\). Given the feature map \(^{l}\) of the l-th image and the textual input \(\) (which is an empty character in our task), we obtain:

\[^{l+1}=(( (^{l}),_{text}())), \]

where \(_{text}\) represents CLIP text encoder, and we have skipped over skip-connection in the formula.

Before considering the incorporation of the support mask, two straightforward and intuitive methods can be leveraged to facilitate interaction between the query image and support image. One approach entails interaction within the self-attention module, while the other involves interaction within the cross-attention module.

**KV Fusion Self-Attention.** We first propose a KV fusion method in self-attention layer to achieve interaction between query image and support image. For the input image feature \(\), the standard self-attention layer first maps it to query \(\), key \(\) and value \(\) with a linear projection layer.. Therefore, \(()\) can be further represented as:

\[^{*}=()=(, ,)=(^{T}}{}) \]

Figure 1: Overview of the DiffewS framework. (a)(b) display that query image \(_{q}\), query mask \(_{q}\), support image \(_{s}\) and support mask \(_{s}\) are all encoded by VAE into latent variables \(_{q}\), \(_{mq}\), \(_{s}\), \(_{ms}\), respectively, where \(_{q}\) and \(_{mq}\) are concatenated to input into UNet. (c) demonstrates the DiffewS intuning protocol (d) elucidates the detailed implementation of FSA, acquiring information from support images by concatenating the query and key features.

where \(d\) is the dimension of query and key, while \(^{*}\) is the feature updated by self-attention. Back to our task, we can also map the features of the support image and query image \(_{s}\) and \(_{q}\) to \(_{s}\), \(_{s}\), \(_{s}\) and \(_{q}\), \(_{q}\), \(_{q}\) through the linear projection layer. We hope that the features of the query image can effectively utilize the information of the support image, so we need to let \(_{q}\) access \(_{s}\) and \(_{s}\). To achieve this, we can concatenate \(_{q}\) and \(_{s}\) to form \(_{qs}=[_{q},_{s}]\). Similarly, we can get \(_{qs}=[_{q},_{s}]\). Finally, our KV Fusion Self-Attention layer can be represented as:

\[_{q}^{*}=(_{q},_{s})=(_{q},_{qs},_{qs}) \]

Since we only replaced \(\) and \(\), we can fully reuse the weights of the original self-attention.

**Tokenized Interaction Cross-Attention** The second alternative is to inject information originating from the support image via cross-attention. This strategy has been widely used in Customized Text-to-Image Generation [47; 48; 49]. In particular, the initial cross-attention is employed to introduce the text information, encoded using CLIP text encoder. We can encode the support image into a series of tokens using the CLIP image encoder and utilize it as the cross-attention input. At this point, the process can be represented as:

\[}^{*}=(},(_{img}(_{s}))) \]

where \(\) means flattening the token sequence after image encoding. \(_{img}\) represents the CLIP image encoder corresponding to the CLIP text encoder used in the original UNet.

### Injection of support mask information

Building upon the Self-attention kv fusion approach, we investigate methodologies for incorporating support mask information. We categorize the injection methods into four types:

* **Concatenation** The support mask \(_{s}\) can be converted into an RGB image, then directly encoded into a latent variable \(_{ms}\) using VAE, which is then concatenated with \(_{s}\) in the channel dimension. Due to the resulting mismatch in dimensionality from the concatenation, we adopt the approach of Marigold , where the first layer weight tensor is duplicated and its values are halved.
* **Multiplication** We can directly multiply \(_{s}\) on the image \(_{s}\) to form the image \(_{s}^{*}=_{s}_{s}\), and finally encode \(_{s}^{*}\) into a latent variable \(_{s}^{*}\) using VAE as the input of UNet.
* **Attention Mask \(_{s}\)** can serve as an attention mask to control self-attention so that only \(_{s}\) in the masked region can be accessed by \(_{q}\). Since the feature map sizes of different layers are different, we need to resize \(_{s}\) to fit the dimensions of each layer.
* **Addition** Alternatively, \(_{s}\) can be directly added to the image \(_{s}\), generating the image \(_{s}^{*}=0.5_{s}+0.5_{s}\). Following that, \(_{s}^{*}\) is encoded into a latent variable \(_{s}^{*}\) using VAE, which is then used as the UNet input.

For cross-attention tokenized interaction, information of the support mask can also be injected in the same four ways. There are just some slight differences in the implementation details (see the Appendix A.3).

We carry out a comparison of two interaction methods (Section 4.1) paired with four injection methods (Section 4.2); these eight combinations are then verified experimentally, and the results are presented in Figure 2. Overall, we observe that KV Fusion Self-Attention(FSA) outperforms Tokenized Interaction Cross-Attention(TCA). We attribute this mainly to the preservation and flexible utilization of information from the support image by FSA. Conversely, TCA, which only compresses support image to tokens via the CLIP image encoder, leads to some information loss. Notably, within the FSA, the Concatenation method surpassed the other three. It offered a more free-form handling of RGB images and MASK information via subsequent learnable convolutional layers, compared to other hard injection methods. In the case of TCA, the Attention Mask method seems more apt as other operations are actually constrained by the CLIP image encoder. The CLIP image encoder itself is not good at dealing with mask information. Of course, we believe that there is still room for further exploration here, referring to FGVP .

Figure 2: Exploring the Interaction and Injection Methods

### Supervision from query mask

In Section 3, we mentioned that we encode the query mask \(_{q}\) into a latent variable \(_{mq}\), and directly supervise in the latent space. However, \(_{q}^{H W}\) is a two-dimensional mask, while the input of VAE needs to be an RGB image. Consequently, conversion of \(_{q}\) into an RGB image becomes necessary, but it's unclear which form of conversion would yield optimal results as no research has delved into this as yet. A reasonable conversion method should satisfy the following two conditions:1. It is easier for UNet to learn 2. It is more convenient to get the final segmentation result through post-processing. In this section, we explore the following four forms of conversion.

* **White foreground + black background** Visualizing the segmentation annotation with a white mask and black background is a common way in the academic community. Specifically, we only need to replicate \(_{q}\) three thrice along the channel dimension to form the corresponding RGB image denoted by the mask. We employed this conversion approach as a default in Section 4.2.
* **Real foreground + black background** Considering LDM's original pre-training on real images, forcing the model to output purely black-and-white images that do not fit within real-image distribution might amplify the model's learning difficulty. Therefore, we also attempted to retain the real pixels of the foreground, while setting the background to black **c. Black foreground + real background** Following the same logic, we also try preserving the pixels of the real background but render the foreground pixel black.
* **Adding mask on real image** We also consider overlaying \(_{q}\) on the real image to form the mask on the real image, which is the Addition method mentioned in Section 4.2. This approach makes the output space of UNet closer to the distribution of real images, but it requires more complex post-processing to get the final segmentation results. That is, we need to subtract the original image from the model output to get the final segmentation result.

As shown in Figure 3, we assess the performance of the four forms of supervision, among which (a) method achieved the best performance in all experiments. Although (b) (c) (d) methods being closer to the real image distribution, the performance is lower. On the one hand, it is difficult to obtain the mask through simple post-processing, and on the other hand, it may increase the learning difficulty because the model needs to retain the ability to generate the original image. In conclusion, our results demonstrate that UNetUNet can effortlessly learn to output in forms such as 'white foreground + black background'. Therefore, we eventually chose this approach for Diffews.

### Exploration of generation process

In this section, we further discuss how to design an effective generation process to transfer the pre-trained diffusion models to mask prediction tasks. Inspired by the success of transferring pre-trained diffusion models to depth estimation task [13; 51], we explore three different mask generation processes. The illustration of different mask generation processes is shown in Figure 4.

* **Multi-step noise-to-mask generation (MN2M)** MN2M follows the denoise pipeline of original diffusion models. The training and inference schemes of MN2M are similar to Marigold . Figure 4(b1) shows the illustration of inference process. The image latent \(z_{q}\)

Figure 3: Illustrations and comparisons of different forms of supervision from query mask.

concatenates with the mask latent \(}_{mq}^{(t)}\). The UNet takes it as input and predicts the new mask latent \(}_{mq}^{(t-1)}\). After T steps, the final mask latent \(}_{mq}^{(0)}\) is decoded into mask prediction. The mask latent \(}_{mq}^{(T)}\) is initialized as random noise. We also use the annealed multi-resolution noise and test-time ensemble tricks  proposed in Marigold.
* **Multi-step image-to-mask generation (MI2M)** MI2M formulates the diffusion denoising process as a deterministic multi-step conversion process from image to prediction, similar to DMP . Figure 4(b2) shows the illustration of inference process. The mask latent \(}_{mq}^{(T)}\) is initialized as image latent \(_{q}\). Then similar to MN2M, the UNet takes \(}_{mq}^{(t)}\) as input and predicts \(}_{mq}^{(t-1)}\). After T steps, the final mask latent \(}_{mq}^{(0)}\) is decoded into mask prediction.
* **One-step image-to-mask generation (OI2M)** OI2M further transforms MI2M's multi-step prediction into a one-step prediction, _i.e._, UNet takes \(_{q}\) as input and outputs the prediction \(}_{mq}\) directly.

We explore the mask generation pipeline starting from MN2M. As shown in Figure 4(c), MN2N achieves 15.2% mIoU. Then, we change MN2M into MI2M keeping same variance \(^{1}=(0.00085,0.012)\), respectively representing the initial and final values of \(\) in the DDIM scheduler. The performance has improved by 4.7% mIoU. However, despite the improvement, both methods exhibit suboptimal performance. We hypothesize that this is because adding a very small noise or image to the binary mask during the training process and then predicting it does not lead to a challenging task compared with diffusion pre-training.

We hypothesize that the suboptimal performance is due to the minimal noise or image added to the binary mask during training, which results in an insufficiently challenging task compared to diffusion pre-training. The binary mask is inherently simpler than natural images, and even after adding noise, the latent mask can still easily distinguish between the foreground and background. This simplicity causes significant information leakage during UNet training, ultimately leading to poor performance.

To verify this hypothesis, we increase the variance of MI2M from \(^{1}=(0.00085,0.012)\) to \(^{2}=(0.0272,0.384)\). The performance has significantly improved by 23.3% mIoU. To fully increase the challenge of training, we convert MI2M into OI2M, which does not introduce any ground-truth information into the input of the UNet during training. Additionally, OI2M reduces the number of iterations to one, significantly boosting the network's predictive efficiency. As shown in Figure 4(c), OI2M achieves the best performance, making it the preferred choice for the mask generation pipeline.

### 1-shot to N-shot

So far, we have primarily explored the training and inference processes specifically designed for 1-shot scenarios. A natural question arises: can this framework be extended to n-shot settings? To address this, we first present the simplest and most straightforward method for adaptation, which requires only minor modifications during the inference phase to accommodate n-shot tasks.

Figure 4: Illustrations and comparisons of different mask generation processes.

In the Section 4.1, we introduced how to inject the information of the support image into the features of the query image using the KV Fusion Self-Attention method. In inference, our support set \(S\) may contain more than one image, \(S=\{I_{s1},I_{s2},...,I_{sn}\}\). We encode each image into the features \(_{si}\). Correspondingly, after mapping, we can obtain a series of \(_{si}\), \(_{si}\), \(_{si}\) and \(_{qi}\), \(_{qi}\), \(_{qi}\). We can concatenate \(_{qi}\) and \(_{si}\) to form \(_{qs}=[_{qi},_{s1},_{s2},...,_{sn}]\), and similarly we can obtain \(_{qs}=[_{qi},_{s1},_{s2},...,_{sn}]\). Finally, our kv fusion self attention layer can be represented as:

\[_{q}^{*}=KVFusionAttn(_{q},_{s})= Attention(_{q},_{qs},_{qs}) \]

While the aforementioned solutions enable N-shot inference, their performance does not match that of state-of-the-art (SOTA) models. This discrepancy primarily arises because the model receives only a single support image during the training phase, which leads to inconsistencies when transitioning to the inference phase with 5-shot or 10-shot configurations.

To address this issue, we explore improvements from both the inference and training perspectives. From the perspective of inference, transitioning from 1-shot to N-shot involves concatenating the keys and values of additional support samples, which significantly increases the number of keys and values processed during inference. To address this, we implement random sampling of the keys and values from the support samples during inference, ensuring that their quantity matches that of the training phase (see Table 6). Another more straightforward idea is to introduce multiple support samples during the training phase. In this way, the model can learn how to utilize multiple support images during training. we randomly select 1 to N support samples as input using KV Fusion in Equation (8) during a single training iteration (see Table 7).

Our experiments demonstrate that improvements during the training phase are more effective than those during the inference phase. Therefore, we include the results of the model with training phase improvements in Table 2.

## 5 Experiment

**Datasets** We test our method in two settings: 1. Strict few shot setting: Following the few-shot setting on COCO-20\({}^{i}\), we organize 80 classes from COCO2014  into 4 folds. Each trial consists of 60 classes allocated for training and 20 classes designated for testing. For evaluation, we randomly sample 1000 reference-target pairs in each fold with the same seed used in HSNet . 2. In-context setting: Following the setting in SegGPT , COCO, ADE , and PASCAL VOC  serve as the training set. In-domain testing is conducted on COCO-20\({}^{i}\) and PASCAL-5\({}^{i}\) to evaluate our model. In line with Matcher , LVIS-92\({}^{i}\) function as the out-of-domain test set.

**Implementation details** We initialize our model with Stable Diffusion 2.1 . The Adam optimizer is used with a weight decay set at 0.01 and a learning rate of 1e-5, coupled with a linear schedule. In terms of data augmentation, our methodology only involves resizing the input image directly to 512x512. No additional data augmentation occurs. Under the strict few-shot setting, the model undergoes training on four V100 GPUs. With the gradient accumulation set at 4, the total batch size comes to 16. Training carries out for 10,000 iterations, typically requiring six hours. For in-context setting, since the training set is larger, we keep other hyperparameters consistent with the strict few-shot setting, and adjust the total training iterations to 30000 iterations. Lastly, our ablation experiments are validated on Fold0 of COCO-20\({}^{i}\). The training took place on a single 4090 GPU, with a gradient accumulation set at 4, which brought the total batch size to 4. The training, which consisted of 10,000 iterations, took roughly 11 hours.

### In-context setting

We first compare DiffewS with other generalist models such as Painter , SegGPT , PerSAM-F, and Matcher  as well as specialist models like HSNet , VAT , FPTrans . Regarding the specialist models, we directly refer to the results presented within the SegGPT  and Matcher  research papers. These specialist models are also trained on the test categories from COCO  and PASCAL VOC . We employ COCO-20\({}^{i}\) and PASCAL-5\({}^{i}\) to validate the in-domain performance of DiffewS. Remarkably, on COCO, DiffewS achieves a 1-shot score of 71.3, considerably exceeding the generalist model SegGPT (+15.2) and specialist model FPTrans (+14.8), both trained with in-domain data. DiffewS furthermore significantly outperformsSAM-based models PerSAM-F (+47.8) and Matcher (+18.6). On PASCAL-5\({}^{i}\), DiffewS records 88.3 in 1-shot, clearly surpassing SegGPT (+5.1) and Matcher (+20.4). These results evidence that DiffewS effectively utilizes the prior of Stable Diffusion, unlocking the full potential of Stable Diffusion in segmentation. Furthermore, out-of-domain examination on LVIS-92\({}^{i}\) underpins the generalization ability of DiffewS. In this setting, DiffewS registers 31.4 in 1-shot and 35.4 in 5-shot, markedly outperforming other generalist models, aside from Matcher. It is worth mentioning that Matcher simultaneously utilizes two Foundation models (SAM  and DINO V2  ), and SAM itself is pre-trained on an exhaustive, finely annotated segmentation dataset. On the other hand, DiffewS undergoes fine-tuning on a relatively smaller quantity of segmentation data for limited iterations, still delivering performance that rivals Matcher. This indicates that using the paradigm of DiffewS, there is potential to achieve significant breakthroughs in the segmentation field if further trained on larger-scale segmentation data. It should be noted that the improvement of DiffewS in 5-shot is not significant, with a 4.0 distinct improvement only on LVIS-92\({}^{i}\). This might be due to the presence of many small objects in the support images of LVIS, so increasing the number of support images can alleviate this problem. Conversely, the DiffewS 5-shot performance on PASCAL-5\({}^{i}\) is slightly deficient compared to the 1-shot. This could be ascribed to the presence of relatively larger and more simplistic objects within PASCAL VOC's support images, inputting more images might interfere with the original architecture of the model. In this case, we do not apply the improvement strategies discussed in Section 4.5, therefore, the relatively weaker performance in the 5-shot scenario is reasonable.

### Strict few-shot setting

We also undertake validation of DiffewS under the standard few-shot setting, comparing it with other specialist models such as HSNet , CyCTR , VAT , BAM , HDMNet , and DCAMA . For the one-shot setting, the average performance of DiffewS across all four folds attains 51.2, surpassing the current state-of-the-art (SOTA) model DCAMA, scoring 50.9 mIoU. Worth mentioning is that DCAMA relies on a highly complex additional block, whereas DiffewS entirely utilizes the generative framework of UNet. In terms of the efficiency of convergence, DiffewS necessitates just a 30000-iteration training, in contrast to both DCAMA and HSNet which require training spanning hundreds of epochs, typically costing several days. This demonstrates the successful employment of Stable Diffusion priors by DiffewS, thereby securing impressive performance without requiring extended periods of fine-tuning. In the five-shot setting, the average performance across four

   &  &  &  \\   & & \(20^{0}\) & \(20^{1}\) & \(20^{2}\) & \(20^{3}\) & mean & \(20^{0}\) & \(20^{1}\) & \(20^{2}\) & \(20^{3}\) & mean \\  HSNet  & ICCV’21 & 37.2 & 44.1 & 42.4 & 41.3 & 41.2 & 45.9 & 53.0 & 51.8 & 47.1 & 49.5 \\ CyCTR  & NeurIPS’21 & 38.9 & 43.0 & 39.6 & 39.8 & 40.3 & 41.1 & 48.9 & 45.2 & 47.0 & 45.6 \\ VAT  & ECCV’22 & 39.0 & 43.8 & 42.6 & 39.7 & 41.3 & 44.1 & 51.1 & 50.2 & 46.1 & 47.9 \\ BAM  & CVPR’22 & 43.4 & 50.6 & 47.5 & 43.4 & 46.2 & 49.3 & 54.2 & 51.6 & 49.6 & 51.2 \\ DCAMA  & ECCV’22 & 49.5 & 52.7 & 52.8 & 48.7 & 50.9 & 55.4 & 60.3 & 59.9 & 57.5 & 58.3 \\ HDMNet  & CVPR’23 & 43.8 & 55.3 & 51.6 & 49.4 & 50.0 & 50.6 & 61.6 & 55.7 & 56.0 & 56.0 \\ DiffewS & this work & 47.7 & 56.4 & 51.9 & 48.7 & 51.2 & 52.0 & 63.0 & 54.5 & 54.3 & 56.0 \\ DiffewS-n & 47.1 & 56.6 & 53.8 & 48.3 & 52.2 & 57.3 & 66.5 & 60.3 & 58.8 & 60.7 \\  

Table 2: Results of strict few-shot semantic segmentation on COCO-20\({}^{i}\). DiffewS-n represents using training time improvements for N-shot.

   &  &  &  \\   & & \(20^{0}\) & \(20^{1}\) & \(20^{2}\) & \(20^{3}\) & mean & \(20^{0}\) & \(20^{1}\) & \(20^{2}\) & \(20^{3}\) & mean \\  HSNet  & ICCV’21 & 37.2 & 44.1 & 42.4 & 41.3 & 41.2 & 45.9 & 53.0 & 51.8 & 47.1 & 49.5 \\ CyCTR  & NeurIPS’21 & 38.9 & 43.0 & 39.6 & 39.8 & 40.3 & 41.1 & 48.9 & 45.2 & 47.0 & 45.6 \\ VAT  & ECCV’22 & 39.0 & 43.8 & 42.6 & 39.7 & 41.3 & 44.1 & 51.1 & 50.2 & 46.1 & 47.9 \\ BAM  & CVPR’22 & 43.4 & 50.6 & 47.5 & 43.4 & 46.2 & 49.3 & 54.2 & 51.6 & 49.6 & 51.2 \\ DCAMA  & ECCV’22 & 49.5 & 52.7 & 52.8 & 48.7 & 50.9 & 55.4 & 60.3 & 59.9 & 57.5 & 58.3 \\ HDMNet  & CVPR’23 & 43.8 & 55.3 & 51.6 & 49.4 & 50.0 & 50.6 & 61.6 & 55.7 & 56.0 & 56.0 \\ DiffewS &  & 47.7 & 56.4 & 51.9 & 48.7 & 51.2 & 52.0 & 63.0 & 54.5 & 54.3 & 56.0 \\ DiffewS-n & 47.1 & 56.6 & 53.8 & 48.3 & 52.2 & 57.3 & 66.5 & 60.3 & 58.8 & 60.7 \\  

Table 1: Results of few-shot semantic segmentation on COCO-20\({}^{i}\), PASCAL-5\({}^{i}\), and LVIS-92\({}^{i}\), under in-context setting.

folds reaches 56.0, higher than all other models aside from DCAMA. Currently, DiffewS primarily focuses on the 1-shot situation lacking specific optimizations for the 5-shot scenario in its training and inference systems. This explains why DiffewS is at present marginally inferior to DCAMA. Furthermore, when employing our proposed training improvement strategy, DiffewS-n outperforms other models in both the 1-shot and 5-shot settings.

### Visualization

As shown in Figure 5, DiffewS effectively segments categories not in the training set, such as slippers and aprons. It also accurately segments objects of different styles and smaller items, demonstrating strong generalization capabilities. In some cases, DiffewS even achieves more accurate results than GT.

In addition, DiffewS demonstrates impressive results in various cross-style segmentation tasks and small object segmentation cases (see Figure 6). We hypothesize that DiffewS's exceptional generalization ability stems from its extensive utilization of prior knowledge from diffusion models. However, DiffewS also struggles with certain challenging cases, we also present several failure cases in Figure 7 and categorize the reasons for these failures.

## 6 Conclusion

In this work, we have presented DiffewS, a simple and efficient framework for few-shot semantic segmentation. By directly generating the target mask, DiffewS is capable of retaining the original latent diffusion models' generative framework and effectively utilizing the visual prior of pre-trained diffusion models. By introducing several designs about multi-image interaction, information injection, and supervision signals, DiffewS outperforms SOTA models in the in-context learning setting, and reaches comparable performance to specialist models in the strict few-shot setting.

**Limitation** and more **Discussions** are provided in Appendix A.1.