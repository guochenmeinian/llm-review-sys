# OpenShape: Scaling Up 3D Shape Representation Towards Open-World Understanding

Minghua Liu\({}^{1}\)1

Ruoxi Shi\({}^{2*}\)

Kaiming Kuang\({}^{1*}\)

Yinhao Zhu\({}^{3}\)

Xuanlin Li\({}^{1}\)

Shizhong Han\({}^{3}\)

Hong Cai\({}^{3}\)

Fatih Porikli\({}^{3}\)

Hao Su\({}^{1}\)

\({}^{1}\) UC San Diego   \({}^{2}\) Shanghai Jiao Tong University   \({}^{3}\) Qualcomm AI Research

Project Website: [https://colin97.github.io/OpenShape/](https://colin97.github.io/OpenShape/)

Equal ContributionQualcomm AI Research is an initiative of Qualcomm Technologies, Inc.

###### Abstract

We introduce OpenShape, a method for learning multi-modal joint representations of text, image, and point clouds. We adopt the commonly used multi-modal contrastive learning framework for representation alignment, but with a specific focus on scaling up 3D representations to enable open-world 3D shape understanding. To achieve this, we scale up training data by ensembling multiple 3D datasets and propose several strategies to automatically filter and enrich noisy text descriptions. We also explore and compare strategies for scaling 3D backbone networks and introduce a novel hard negative mining module for more efficient training. We evaluate OpenShape on zero-shot 3D classification benchmarks and demonstrate its superior capabilities for open-world recognition. Specifically, OpenShape achieves a zero-shot accuracy of \(46.8\%\) on the 1,156-category Objavverse-LVIS benchmark, compared to less than \(10\%\) for existing methods. OpenShape also achieves an accuracy of \(85.3\%\) on ModelNet40, outperforming previous zero-shot baseline methods by \(20\%\) and performing on par with some fully-supervised methods. Furthermore, we show that our learned embeddings encode a wide range of visual and semantic concepts (e.g., subcategories, color, shape, style) and facilitate fine-grained text-3D and image-3D interactions. Due to their alignment with CLIP embeddings, our learned shape representations can also be integrated with off-the-shelf CLIP-based models for various applications, such as point cloud captioning and point cloud-conditioned image generation.

## 1 Introduction

3D shape understanding has recently garnered a surge of interest driven by the growing demands in real-world applications, such as augmented/virtual reality, autonomous driving, and robotics. Despite significant advancements in 3D recognition and analysis, existing data-driven approaches are still greatly limited by the scale of 3D training datasets and tend to exhibit poor generalization when facing unseen shape categories, hindering the deployment of existing models in real-world applications.

Note that 3D shapes and 2D images can be easily linked through rendering, and the dataset scale issue of 2D images has been remarkably addressed, as shown in recent works such as CLIP . Therefore, many recent studies aim to utilize pre-trained 2D image-language models  to assist 3D tasks, such as 3D generation  and 3D scene-level segmentation . Regarding 3D shape-level understanding, a straightforward idea is to project 3D data to the 2D domain through rendering and use CLIP to analyze the 2D images, thereby enabling zero-shot 3D shape classification . However, these methods suffer from occlusion and information loss during projection, and unnecessary latency due to point cloud rendering and multiple CLIP inferences.

To overcome the limitations caused by projection, it is necessary to train a 3D-native model by distilling knowledge from pretrained 2D models. However, training a 3D-native model requires a set of 3D shapes, and the amount of knowledge that can be distilled is determined by the size of the 3D dataset. For example, ULIP  aims to learn a joint representation space between language, 2D images, and 3D shapes, but uses a small-scale 3D dataset ShapeNetCore  for knowledge distillation. Specifically, ULIP fixes the 2D CLIP text and image encoders and trains a dedicated 3D-native point cloud encoder to extract 3D shape representations. The 3D encoder strives to align the 3D shape embedding space with the CLIP image and language embedding spaces by utilizing contrastive learning across all three modalities. However, since ULIP is only trained on 52K shapes of 55 object categories, it still struggles with out-of-distribution shape categories and fails to demonstrate an impressive open-world understanding of 3D shapes.

In this work, we propose a novel method called OpenShape, which follows a similar paradigm as ULIP but aims to achieve a more generalized and scalable joint representation space encompassing language, 2D images, and 3D shapes. Our focus mainly lies on scaling up representation learning and addressing corresponding challenges. In OpenShape, we emphasize four key factors during the training process: (a) data scale: we significantly increase the scale of 3D training data by combining four public 3D shape datasets, resulting in 876k 3D shapes covering much more diverse categories; (b) text quality: the 3D shapes from our main dataset, Objaverse , is dominated with inaccurate or uninformative text descriptions. Given the data scale, we propose three strategies to automatically filter and enrich the text descriptions; (c) 3D backbone scaling: since most existing 3D backbones target small datasets, we find that it's important but non-trivial to scale up the 3D backbones; and (d) data resampling: since the ensembled dataset is highly unbalanced, we utilize hard negative mining to improve the model's discriminative ability.

We first evaluate OpenShape on the zero-shot 3D shape classification task. As shown in Figure 1, OpenShape outperforms previous zero-shot approaches on the ModelNet40 dataset by at least 20%. Moreover, OpenShape excels at handling long-tail categories. On the challenging Objaverse-LVIS dataset, which contains 1,156 categories, OpenShape achieves a 46.8% accuracy, significantly surpassing previous methods. Notably, this performance gap remains even when ULIP is retrained on our ensembled datasets, highlighting the superiority of our text enrichment and training strategies.

Figure 1: **Left**: Zero-shot shape classification on the Objaverse-LVIS (1,156 categories) and ModelNet40 datasets. OpenShape outperforms previous methods by a large margin. We exclude shapes in Objaverse-LVIS during training, and we also retrain ULIP  on our ensembled training shapes for fair comparison. **Right**: Our shape representations encode a broad range of semantic and visual concepts. We input two 3D shapes and use their shape embeddings to retrieve the top three shapes whose embeddings are simultaneously closest to both inputs. See Section. 4.4 for more details.

Besides zero-shot classification, we present demos that showcase the wide range of visual and semantic concepts learned by OpenShape. For example, in Figure 1-right, we take two 3D shapes as input and use their OpenShape embeddings to retrieve the top three shapes whose embeddings are simultaneously closest to both inputs from our ensembled dataset. The retrieved shapes exhibit an interesting combination of the semantic and geometric elements from both input shapes. Furthermore, since we align our 3D shape embedding space with the CLIP language and image embedding space, we demonstrate that OpenShape embeddings can be easily integrated with other CLIP-based models to perform cross-modality tasks such as point cloud captioning and point cloud-conditioned image generation.

## 2 Related Work

### CLIP for 3D Learning

Image-language models like CLIP have achieved remarkable performance through large-scale image-text pretraining [55; 29; 35; 84; 4; 56; 61; 36]. As these models excel at capturing rich visual concepts and possess impressive zero-shot capabilities, they have been applied to various 3D vision tasks. For instance, numerous recent works utilize CLIP to facilitate zero-shot text-to-3D generation [22; 26; 45; 63; 33; 7; 32; 5; 28; 77; 40; 83], typically through CLIP-guided per-scene optimization. From a recognition perspective, some works focus on scene-level representation, aiming to leverage CLIP priors for zero-shot 3D segmentation or detection in both indoor [18; 27; 14; 79; 41; 49; 82; 23; 60; 85; 31] and outdoor scenes [9; 21]. Meanwhile, another line of work focuses on shape-level understanding, targeting zero-shot shape classification [86; 88; 53; 78; 19] and part segmentation [39; 1]. There are two primary working paradigms for these methods. The first [86; 88; 24] involves using images as a medium representation, projecting 3D point clouds into 2D and employing 2D CLIP for inference. However, these methods typically suffer from occlusion and information loss during projection, along with unnecessary latency due to point cloud rendering and multiple 2D CLIP inferences. The second paradigm involves training a 3D-native encoder attempting to distill or fuse CLIP features into 3D representations. Our paper follows this paradigm.

### 3D Shape Representation Learning

Various works have studied self-supervised pretraining for point clouds by designing pretext tasks [15; 69; 50; 2; 66] such as self-reconstruction [57; 13; 3; 72], masked auto-encoding [48; 80; 20], distortion reconstruction [64; 44; 68], normal estimation , and contrastive learning [87; 62; 76]. These tasks enhance models' shape representations and improve their performance on downstream applications, although they do not involve multimodal semantic alignments during pretraining.

Recently, some works [53; 78; 19], exemplified by ULIP , have explored learning multimodal joint representations for 3D shapes. They train 3D-native shape encoders by aligning 3D shape embeddings with CLIP's language and/or image embeddings through multimodal contrastive learning. Works like ReCon  further combines cross-modal contrastive learning with masked auto-encoding for added enhancement. While these methods allow for zero-shot 3D classification through the computation of 3D-text similarity, the amount of distilled knowledge and their model capability are heavily limited by the small-scale training datasets used. Our work follows this paradigm but aims to learn more generalizable and scalable representations to enable open-world 3D shape understanding.

## 3 Method

We propose a novel method, _OpenShape_, for learning generalizable and scalable multi-modal joint representation between language, 2D images, and 3D shapes, as shown in Figure 2. We first introduce the multi-modal contrastive learning framework we used for aligning representations of three modalities in Section 3.1. We then elaborate how we create our training sets and enrich our text data in Sections 3.2 and 3.3. In Section 3.4, we present how we scale up our 3D backbone models. Finally, we propose a hard negative mining strategy to enhance contrastive learning in Section 3.5.

### Multi-Modal Representation Alignment

We aim to learn 3D shape representations that are aligned with pretrained CLIP embedding spaces of language and image. As shown in Figure 2 (c), we train a 3D native encoder \(f^{T}\) that takes a 3D point cloud as input and extracts 3D shape feature. Following previous works [53; 78; 19], such as ULIP , we utilize multi-modal contrastive learning for representation alignment. Since CLIP is pretrained on a much larger scale data, we freeze both its text encoder \(f^{T}\) and its image encoder \(f^{I}\) during feature alignment to preserve CLIP's feature priors and avoid model collapse. Specifically, given a sampled batch of triplets \(\{(P_{i},T_{i},I_{i})\}\), where \(P_{i}\) denotes a point cloud of a 3D shape, \(T_{i}\) and \(I_{i}\) denote corresponding text and image, the contrastive loss is calculated as:

\[-_{i}(^{P} h_{i}^{T}/)}{_ {j}(h_{i}^{P} h_{j}^{T}/)}+^{T} h_{j}^{P }/)}{_{j}(h_{i}^{T} h_{j}^{T}/)}+^{P } h_{i}^{T}/)}{_{j}(h_{i}^{T} h_{j}^{T}/)}) \]

where \(n\) is the number of shapes in a batch; \(\) is a learnable temperature; \(h_{i}^{P}=f^{P}(P_{i})/|f^{P}(P_{i})|\), \(h_{i}^{T}=g^{T}(f^{T}(T_{i}))/|g^{T}(f^{T}(T_{i}))|\), and \(h_{i}^{T}=g^{I}(f^{I}(I_{i}))/|g^{I}(f^{I}(I_{i}))|\) denote normalized projected features of \(P_{i}\), \(T_{i}\), and \(I_{i}\), where \(g^{T}\) and \(g^{I}\) are two learnable linear projections. Since \(f^{T}\) and \(f^{I}\) are frozen, we extract all \(f^{T}(T_{i})\) and \(f^{I}(I_{i})\) before training and cache them for acceleration. In most of our experiments, we utilize OpenCLIP ViT-bigG-14  as the pretrained CLIP model.

### Ensembling 3D Datasets

Since the scale and diversity of training triplets play a crucial role in learning scalable shape representations, we ensemble four currently-largest public 3D datasets for training as shown in Figure 2 (a), resulting in 876k training shapes. Among these four datasets, ShapeNetCore , 3D-FUTURE  and ABO  are three popular datasets used by prior works. They contain human-verified high-quality 3D shapes, but only cover a limited number of shapes and dozens of categories. The Objaveres  dataset is a more recent dataset, containing many more 3D shapes and covering significantly more diverse categories. However, shapes in Objaveres are mainly uploaded by web users and not verified by experts, and thus have uneven quality and exhibit highly unbalanced distributions, necessitating further processing.

To create triplets for training, for each shape, we sample 10,000 points from the mesh surface and interpolate the point colors according to the mesh textures. We also render 12 color images from the preset camera poses that uniformly cover the whole shape. For datasets providing thumbnails, we include them as part of image candidates, since they typically capture the shape from a better camera view. For the Objaveres dataset, we use the model name as the raw text for each shape. For other datasets, we utilize provided metadata to create raw texts (see supplementary for details). During each pretraining iteration, we randomly sample one rendered image or thumbnail for each shape, and apply standard augmentation to the point clouds .

### Text Filtering and Enrichment

We find that only applying contrastive learning between 3D shapes and 2D images is insufficient to fuel zero-shot 3D classification, even when training on large-scale datasets. We conjecture that this is

Figure 2: (a) We ensemble four public 3D shape datasets, resulting in 876k shapes that encompass diverse categories and concepts. (b) We propose three strategies to automatically filter and enrich the noisy texts in the original datasets. (c) We train a 3D point cloud encoder to align the 3D shape embedding space with the CLIP’s text and image embedding spaces. We perform cross-modal contrastive learning with scaled 3D backbones and hard negative mining. (d) OpenShape embeddings can be easily integrated with other CLIP-based models, enabling various cross-modality tasks.

caused by the inherent domain gap in CLIP's language and image embedding spaces, which is also observed by previous studies [37; 70]. Consequently, 3D-text alignment is not guaranteed even if we obtain good 3D-image alignments via contrastive learning. Therefore, we need to explicitly align 3D shapes with text. Along this process, to facilitate better 3D-text alignment, we introduce 3 techniques to improve the text quality: filtering, captioning, and image retrieval, as shown in Figure 2 (b).

**Filtering.** As shown in Figure 3, the 3D shapes from our main dataset, Objavverse, is dominated with noisy text descriptions ("names") uploaded by web users. Many of the problematic texts can be identified from the text itself without seeing the corresponding 3D shape. We thus leverage a powerful large language model, GPT-4 , to filter out inaccurate or uninformative text descriptions. We find that GPT-4 excels at recognizing irrelevant contents, such as timestamps, pure model numbers, incomprehensible descriptions, random filenames (e.g., new project), and random characters. Through GPT-4, we filter out about 30% of raw user texts. Note that we only filter the texts, and still keep all shapes for training. More details, such as the prompts we used, are presented in the supplementary.

**Captioning.** We utilize BLIP  and the Azure cognition services to caption the 2D thumbnails (if present, or images rendered from a fixed frontal view) of the 3D models, obtaining two texts for each shape. As shown in Figure 3, the captioning models can usually produce meaningful and descriptive captions that either enhance user-uploaded texts or replace low-quality ones. We also notice that the two caption models complement each other, leading to better performance.

**Image Retrieval.** In addition to image captioning, we also perform image retrieval to obtain additional descriptions of 3D models. We retrieve k-NN images of shape renderings from the LAION-5B dataset  using the CLIP ViT-L retrieval index . We then take the captions of the k-NN images as the retrieved texts for our 3D models. Compared with captioning model generations, retrieved texts cover a wider range of text styles. They can also include more fine-grained semantics than both the user texts and the generated captions (e.g., "Labrador" in Figure 3).

In each iteration of pretraining, for each shape, we first randomly sample a text source category among the raw text (if unfiltered), the captions, and the retrieved texts. We then select a text candidate from the selected category. We also apply the template-based prompt engineering technique used in ULIP  to both training texts and test-time category names. Specifically, we extend a word or a phrase to a collection of templated simple sentences and take their average embedding.

### Scaling Up 3D Point Cloud Backbones

Previous works on 3D point cloud learning have primarily focused on smaller-scale datasets like ShapeNet. These techniques may not be directly applicable to our larger-scale ensembled dataset and need to be scaled up accordingly. We find that different 3D backbones may exhibit distinct behavior and scalability when trained on datasets with varying sizes. Specifically, we compare six popular

Figure 3: **Text Filtering & Enrichment Examples** In each example, the left section features the thumbnail, model name, and GPT-4 filtering results. The upper right section shows image captions from two captioning models, while the lower right section displays retrieved images and their corresponding texts.

backbones trained on ShapeNet or our ensembled dataset by evaluating their zero-shot classification performance on ModelNet40  and Objaverse-LVIS datasets (for now, these backbones are trained with their original configurations and without scaling up model sizes). **Objaverse-LVIS** is a subset of Objaverse dataset with human-verified category labels. With 1,156 categories, it serves as a suitable dataset for evaluating zero-shot long-tail classification, and we exclude all shapes of Objaverse-LVIS from this experiment. Results are shown in Table 1. We find that when trained on ShapeNet, all backbones share similar performances. However, when trained on our ensembled dataset, the performance gap between backbones increases significantly. This suggests that while the original versions of these backbones share a similar number of parameters, some may have been saturated when trained on small datasets, while others do not.

We also explore the performance and scalability of these backbones when scaling up the model sizes and training on our ensembled dataset. Please refer to the supplementary for details on how we scale up each model. As shown in Figure 4, we observe that all 3D backbones benefit significantly from model scaling. However, traditional backbones without a shrinking hierarchical structure, such as DGCNN and PointNet, require operating completely on dense points or modeling the relationships (e.g., through kNN) between dense points. As a result, they become more time-consuming and memory-intensive when scaled up compared to more modern backbones. We therefore select PointBERT  (Transformer-based) and SparseConv  (convolution-based) as our 3D backbones for the remaining experiments, as they exhibit strong performance and scalability.

### Hard Negative Mining

Our ensembled dataset exhibits a high degree of class imbalance. Certain common categories, such as building, may occupy tens of thousands of shapes, while many other categories, such as walrus and wallet, are underrepresented with only a few dozen or even fewer shapes. Consequently, when randomly constructing batches, it is unlikely that shapes from two confusing categories (e.g., apples and cherries) will be contrasted within the same batch. Inspired by some previous works , we propose an offline hard negative mining strategy for improving the training efficiency and performance. Specifically, in the first round of training, we train our model with random batches until it is about to converge. We then compute the kNN for each shape in the learned 3D embedding space. In the second round of training, for each iteration, we randomly select \(s\) seed shapes and then obtain \(m\) neighbors from the kNN results of each seed shape, resulting \(s m\) shapes per batch. In this way, confusing pairs are more likely to be selected in a single batch. However, this may also introduce false negative pairs (e.g., two apples) into contrastive learning. To mitigate this issue, we leverage image and text embeddings to filter out pairs sharing similar texts when calculating the contrastive loss. Specifically, for two shapes \(i\) and \(j\) selected from the same seed shape, if \(h_{j}^{T} h_{i}^{I}+>h_{i}^{T} h_{i}^{I}\), where \(h^{T}\) and \(h^{I}\) are text and image embeddings, and \(\) is a small threshold, we believe that the text embeddings of \(i\) and \(j\) are very close to each other, and we remove \(j\) from \(i\)'s negative examples when calculating contrastive loss. By employing this strategy to construct batches, we observe faster and better model learning.

    &  **\#Param** \\  } &  &  \\   & & MNet40 & O-LVIS & MNet40 & O-LVIS \\  PointNet  & 1.3M & 67.0 & 9.3 & 74.9 & 24.4 \\ DGCNN  & 2.3M & 67.8 & 9.0 & 74.2 & 24.8 \\ PointMLP  & 9.3M & 73.5 & 12.9 & 82.9 & 36.6 \\ PointNeXt  & 2.8M & 72.6 & 12.2 & 81.6 & 33.8 \\ PointBERT  & 5.1M & 70.3 & 10.8 & 84.5 & 37.0 \\ SparseConv  & 5.3M & 70.7 & 10.6 & 78.8 & 31.7 \\  std. dev. & & 2.3 & 1.4 & 3.9 & 5.1 \\   

Table 1: Comparison of different 3D backbones **before scaling up their parameters**. Models are trained on ShapeNet  or our ensembled dataset excluding Objaverse-LVIS . Zero-shot classification performance are evaluated on ModelNet40  and Objaverse-LVIS .

## 4 Experiments

### Zero-Shot Shape Classification

We evaluate the zero-shot classification performances of our models on three benchmarks: the traditional ModelNet40  and ScanObjectNN , as well as a new benchmark, Objaverse-LVIS . ModelNet40 and ScanObjectNN consist of 40 and 15 common categories, respectively. Objaverse-LVIS is an annotated subset of Objavverse  and comprises 46,832 shapes among 1,156 LVIS  categories. With a much larger base of classes than other benchmarks, Objaverse-LVIS presents a challenging long-tailed distribution, making it a better reflection on models' performance in open-world scenarios. We compare OpenShape with existing zero-shot approaches, including PointCLIP , PointCLIPv2 , ReCon , CG3D , CLIP2Point , and ULIP . Among them, PointCLIP  and PointCLIPv2  project point clouds into 2D images and directly utilize 2D CLIP for inference, while other methods leverage the CLIP embedding spaces for alignment and require 3D shapes for training. We report results on these baselines using their released checkpoints. To better analyze the source of our performance gains, we also retrain the baseline ULIP  on our ensembled shape dataset, but we use the original texts in the four constituent datasets along with the official codebase without backbone scaling. We train OpenShape and ULIP on three different sets of training shapes: "**Ensembled**" denotes using all shapes from the four datasets; "**Ensembled (no LVIS)**" is the same but excludes all shapes from the Objavserse-LVIS subset; "**ShapeNet**" only includes shapes from the ShapeNet  dataset. Note that even when LVIS shapes are included in the training shapes (i.e., the "Ensembled" dataset), their test-time category labels are probably not included in their training texts. Please refer to the supplementary for more training and evaluation details.

Table 2 shows the results. We observe that OpenShape consistently outperforms prior approaches, even when trained only on ShapeNet. When models are trained on our larger-scale ensembled dataset, they receive a significant performance boost. In this case, OpenShape still surpasses retrained ULIP by a significant margin, demonstrating the advantages of our text enrichment, backbone scaling, and other training strategies. Specifically, OpenShape greatly improves the classification accuracy on the long tail categories in Objavverse-LVIS from a \(<10\%\) to \(\), outperforming the retrained ULIP by about 20 points and reaching a decent top-\(5\) accuracy of \(\). These results demonstrate OpenShape's capability to recognize open-world objects effectively. As for ModelNet40, OpenShape achieves a \(\) accuracy, surpassing previous methods by a substantial margin at least 20 percent. OpenShape also achieves impressive top-\(3\) and top-\(5\) accuracies of \(\) and \(\). To the best of our knowledge, this is the first time zero-shot methods have matched the performance of a fully-supervised 3D learning method on ModelNet40, where OpenShape outperforms fully-supervised 3D ShapeNets  and VoxNet . In addition, on ScanObjectNN, which contains challenging real scans with noise and occlusion, OpenShape exhibits decent sim-to-real transfer capabilities. To contextualize, OpenShape-SparseConv achieves \(\) zero-shot accuracy on ScanObjectNN without specific sim-to-real training, which surpasses \(52.7\%\) reported by SKPConv , a recent method specially designed for sim-to-real transfer in point cloud classification tasks.

    & training shape &  &  &  \\  Method & source & Top1 & Top3 & Top5 & Top1 & Top3 & Top5 & Top1 & Top3 & Top5 \\  PointCLIP  & 2D inferences, & 1.9 & 4.1 & 5.8 & 19.3 & 28.6 & 34.8 & 10.5 & 20.8 & 30.6 \\ PointCLIP v2  & no 3D training & 4.7 & 9.5 & 12.9 & 63.6 & 77.9 & 85.0 & 42.2 & 63.3 & 74.5 \\  ReCon  & & 1.1 & 2.7 & 3.7 & 61.2 & 73.9 & 78.1 & 42.3 & 62.5 & 75.6 \\ CG3D  & & 5.0 & 9.5 & 11.6 & 48.7 & 60.7 & 66.5 & 42.5 & 57.3 & 60.8 \\ CLIP2Point  & & 2.7 & 5.8 & 7.9 & 49.5 & 71.3 & 81.2 & 25.5 & 44.6 & 59.4

### Few-Shot Linear Probing

In the literature, linear probing is a common way to assess the representation learning capabilities of a model. To perform linear probing, we gather and freeze the representation vectors from all samples in a dataset. Subsequently, we train a linear classifier using these fixed vectors and few-shot class labels. We evaluate the accuracy of the linear classifier on three benchmarks: Objaverse-LVIS , ModelNet40 , and ScanObjectNN . Figure 5 summarizes the performance of OpenShape in comparison with ULIP  (official release and our retrained versions) and PointCLIPv2 . On the most challenging Objaverse-LVIS benchmark, OpenShape outperforms all other methods by a large margin. Notably, zero-shot OpenShape beats few-shot linear probes of other methods. On ModelNet40 and ScanObjectNN, we do not see a large performance margin between OpenShape and retrained ULIP. We hypothesize that for few-shot ModelNet40, the error is dominated by in-category sample bias rather than the representation quality; while for ScanObjectNN, the domain gap plays a major role. Since both OpenShape and retrained ULIP are exposed to the same source domain of training objects, their few-shot out-of-domain generalization performances tend to be similar.

### Ablation Study

We perform various ablations by training a scaled version of SparseConv  on the ensembled dataset and then evaluate it on the Objaverse-LVIS  and ModelNet40  zero-shot classification benchmarks, unless otherwise specified. The results are shown in Table 3 and Figures 6 and 7.

**Data and Model Scaling.** We investigate the impact of training data by ablating (1) without or with only Objaverse shapes (Tab. 3) and (2) with different ratios of our ensembled dataset (Fig. 6). We observe that training with \(1\%\) of our ensembled dataset (about \(8.8\)k shapes) achieves similar or better zero-shot performance than training without Objaverse shapes (about \(77.1\)k shapes), indicating that the diversity of training data is sometimes more crucial than the scale. In addition, we compare the performances between scaled-up and non-scaled-up backbones. From Tab. 3, we demonstrate that model scaling plays an essential role when training on our large-scale ensembled dataset (also Fig. 4).

Figure 5: Few-shot linear probing on Objaverse-LVIS , ModelNet40 , and ScanObjectNN . We report the average performance over 10 random seeds.

   Variant & O-LVIS & MNet40 \\  No Objaverse shapes & 13.9 & 75.5 \\ Only Objaverse shapes & 41.6 & 79.2 \\ No backbone scale up & 31.7 & 78.7 \\  No caption \& retrieval & 37.0 & 82.9 \\ No text filtering & 41.4 & 82.9 \\  No point rigb, only avg & 39.6 & 83.6 \\ No text contexts, learning & 23.3 & 67.4 \\ No image contras. learning & 41.0 & 81.0 \\  Full & 42.0 & 83.1 \\ Full + hard mining & 43.4 & 83.4 \\   

Table 3: Ablation study. Top 1 zero-shot accuracies on ModelNet40  and Objaverse-LVIS  are shown.

**Text Filtering and Enrichment.** As shown in Tab. 3, both text filtering and text enrichment are beneficial for performance. We also investigate the specific text enrichment strategies to use for the SparseConv and PointBERT backbones. In Fig. 7, we observe that both image captioning and text retrieval are helpful, and including both yield the best results. Notably, PointBERT improves more than 10 points from text enrichment, highlighting the significance of enhancing text quality.

**Other Aspects.** We also conduct additional ablation studies on color information, contrastive loss components, and our hard-negative mining strategy in Tab. 3. We observe that OpenShape performs well with only \(xyz\) coordinates as input and no RGB color. While 3D-image contrastive loss is also helpful, we observe that 3D shape-text alignment plays a very essential role for model zero-shot generalization, which necessitates our text filtering and text enrichment strategies that significantly enhance text quality. Lastly, by employing our hard negative mining strategy, OpenShape effectively addresses the issue of unbalanced data distribution, leading to further improvements in performance.

### Cross-Modal Applications

**Multi-modal 3D Shape Retrieval.** Through OpenShape multi-modal representations, we can index and retrieve 3D shapes from images, texts, or point clouds. In this section, we retrieve 3D shapes from our ensembled dataset by calculating the cosine similarity between input embedding(s) and 3D shape embeddings and performing kNN. As shown in Figure 8, OpenShape is capable of retrieving visually or semantically similar shapes from a single image or point cloud input. OpenShape embeddings encode a wide range of visual and semantic concepts. In Figure 9, we show that OpenShape supports retrieving 3D shapes from detailed text descriptions, which include fine-grained subcategories, attributes, and their combinations. Note that these input texts are typically not present in the raw texts of the retrieved shapes, indicating that OpenShape effectively learns generalizable concepts across shapes. In Figure 1, we provide a demo which takes two 3D shapes as inputs and retrieves the shapes that are simultaneously closest to both inputs. This is achieved by finding \(*{arg\,max}_{i}(h_{i}^{P} h_{a}^{P},h_{i}^{P} h_{b }^{P})\), where \(h_{a}^{P}\) and \(h_{b}^{P}\) denote normalized shape embeddings of the two input shapes. We can see that the retrieved shapes integrate visual or semantic elements in an interesting manner, highlighting the rich concepts and priors encoded in OpenShape embeddings.

Figure 8: 3D shape retrieval from image (left, mid) and point cloud (right).

Figure 9: **Text-input 3D shape retrieval.** In each row, we show input texts on the left and two retrieved shapes for each text on the right. OpenShape embedding encodes a wide range of visual and semantic concepts and enables (a) retrieval of fine-grained subcategories (first two rows), and (b) control of attributes (e.g., color, shape, style) and their combinations (last two rows).

**Shape-Conditioned Multimodal Generation.** As OpenShape's 3D shape representations are aligned with CLIP's image and text embedding spaces, they can serve as inputs into other CLIP-based models to facilitate various multimodal generation applications. For example, we show that by feeding our 3D shape embeddings into ClipCap , an off-the-shelf image captioning model, along with Stable unCLIP , a text-to-image diffusion model, we can perform point cloud captioning and point cloud-conditioned image generation (optional text prompt supported) without extra training or finetuning. Qualitative results are shown in Figure 10. Please refer to the supplementary for more results and details.

## 5 Limitation and Conclusion

We introduce OpenShape, a novel approach for learning scalable and generalizable multi-modal joint representations for 3D shapes. OpenShape representations effectively capture a wide range of semantic and visual concepts, enabling superior capabilities for open-world 3D shape recognition. By aligning OpenShape with CLIP's embedding space, our shape embeddings can be integrated with off-the-shelf CLIP-based models for various cross-modality applications. Moving forward, there are several directions worth further exploration: (a) More 3D data. While we utilized 876k 3D shapes during training, this is still quite limited compared to the 2D counterparts. We hope that our work inspires future investments in more resources to build even more powerful 3D representations. (b) Part-level information. Our current shape representations mainly focus on global semantic and visual features, and it would be beneficial to add more part-level supervision during training. (c) Sim-to-real domain gap. Our model is mainly trained on synthetic data, and it's challenging but crucial to explore explicit designs for reducing the domain gap with real-world shapes.