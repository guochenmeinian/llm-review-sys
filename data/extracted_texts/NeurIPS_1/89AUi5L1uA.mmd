# SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core Fusion

Lu Han, Xu-Yang Chen, Han-Jia Ye, De-Chuan Zhan

School of Artificial Intelligence, Nanjing University, China

National Key Laboratory for Novel Software Technology, Nanjing University, China

{hanlu, chenxy, yehj, zhandc}@lamda.nju.edu.cn

Equal ContributionCorresponding Author

###### Abstract

Multivariate time series forecasting plays a crucial role in various fields such as finance, traffic management, energy, and healthcare. Recent studies have highlighted the advantages of channel independence to resist distribution drift but neglect channel correlations, limiting further enhancements. Several methods utilize mechanisms like attention or mixer to address this by capturing channel correlations, but they either introduce excessive complexity or rely too heavily on the correlation to achieve satisfactory results under distribution drifts, particularly with a large number of channels. Addressing this gap, this paper presents an efficient MLP-based model, the Series-cOre Fused Time Series forecaster (SOFTS), which incorporates a novel STar Aggregate-Redistribute (STAR) module. Unlike traditional approaches that manage channel interactions through distributed structures, _e.g._, attention, STAR employs a centralized strategy to improve efficiency and reduce reliance on the quality of each channel. It aggregates all series to form a global core representation, which is then dispatched and fused with individual series representations to facilitate channel interactions effectively. SOFTS achieves superior performance over existing state-of-the-art methods with only linear complexity. The broad applicability of the STAR module across different forecasting models is also demonstrated empirically. We have made our code publicly available at [https://github.com/Secilia-Cxy/SOFTS](https://github.com/Secilia-Cxy/SOFTS).

## 1 Introduction

Time series forecasting plays a critical role in numerous applications across various fields, including environment , traffic management , energy , communication , and healthcare . The ability to accurately predict future values based on previously observed data is fundamental for decision-making, policy development, and strategic planning in these areas. Historically, models such as ARIMA and Exponential Smoothing were standard in forecasting, noted for their simplicity and effectiveness in certain contexts . However, the emergence of deep learning models, particularly those exploiting structures like Recurrent Neural Networks (RNNs)  and Convolutional Neural Networks (CNN) , has shifted the paradigm towards more complex models capable of understanding intricate patterns in time series data. To overcome the inability to capture long-term dependencies, Transformer-based models have been a popular direction and achieved remarkable performance, especially on long-term multivariate time series forecasting .

Earlier on, Transformer-based methods perform embedding techniques like linear or convolution layers to aggregate information from different channels, then extract information along the temporal dimension via attention mechanisms . However, such channel mixing structures werefound vulnerable to the distribution drift, to the extent that they were often less effective than simpler methods like linear models [48; 11]. Consequently, some studies adopted a channel-independence strategy and achieved favorable results [29; 24; 36]. Yet, these methods overlooked the correlation between channels, thereby hindering further improvements in model performance. Subsequent studies captured this correlation information through mechanisms such as attention, achieving better outcomes, and demonstrating the necessity of information transfer between channels [50; 35; 27]. However, these approaches either employed attention mechanisms with high complexity  or struggled to achieve state-of-the-art (SOTA) performance . Therefore, effectively integrating the robustness of channel independence and utilizing the correlation between channels in a simpler and more efficient manner is crucial for building better time series forecasting models.

In response to these challenges, this study introduces an efficient MLP-based model, the Series-cOre Fused Time Series forecaster (SOFTS), designed to streamline the forecasting process while also enhancing prediction accuracy. SOFTS first embeds the series on multiple channels and then extracts the mutual interaction by the novel STar Aggregate-Redistribute (STAR) module. The STAR at the heart of SOFTS ensures scalability and reduces computational demands from the common quadratic complexity to only linear. To achieve that, instead of employing a distributed interaction structure, STAR employs a centralized structure that first gets the global core representation by aggregating the information from different channels. Then the local series representation is fused with the core representation to realize the indirect interaction between channels. This centralized interaction not only reduces the comparison complexity but also takes advantage of both channel independence and aggregated information from all the channels that can help improve the local ones . Our empirical results show that our SOFTS method achieves better results against current state-of-the-art methods with lower computation resources. Besides, SOFTS can scale to time series with a large number of channels or time steps, which is difficult for many methods based on Transformer without specific modification. Last, the newly proposed STAR is a universal module that can replace the attention in many models. Its efficiency and effectiveness are validated on various current transformer-based time series forecasters. Our contributions are as follows:

1. We present Series-cOre Fused Time Series (SOFTS) forecaster, a simple MLP-based model that demonstrates state-of-the-art performance with lower complexity.
2. We introduce the STar Aggregate-Redistribute (STAR) module, which serves as the foundation of SOFTS. STAR is designed as a centralized structure that uses a core to aggregate and exchange information from the channels. Compared to distributed structures like attention, the STAR not only reduces the complexity but also improves robustness against anomalies in channels.
3. Lastly, through extensive experiments, the effectiveness and scalability of SOFTS are validated. The universality of STAR is also validated on various attention-based time series forecasters.

## 2 Related Work

Time series forecasting.Time series forecasting is a critical area of research that finds applications in both industry and academia. With the powerful representation capability of neural networks, deep forecasting models have undergone a rapid development [23; 40; 39; 4; 5; 15; 32]. Two widely used methods for time series forecasting are recurrent neural networks (RNNs) and convolutional neural networks (CNNs). RNNs model successive time points based on the Markov assumption [14; 3; 30], while CNNs extract variation information along the temporal dimension using techniques such as temporal convolutional networks (TCNs) [1; 8]. However, due to the Markov assumption in RNN and the local reception property in TCN, both of the two models are unable to capture the long-term dependencies in sequential data. Recently, the potential of Transformer models for long-term time series forecasting tasks has garnered attention due to their ability to extract long-term dependencies via the attention mechanism [51; 37; 52].

Efficient long-term multivariate forecasting and channel independence.Long-term multivariate time series forecasting is increasingly significant in decision-making processes . While Transformers have shown remarkable efficacy in various domains , their complexity poses challenges in long-term forecasting scenarios. Efforts to adapt Transformer-based models for time series with reduced complexity include the Informer, which utilizes a probabilistic subsampling strategy for more efficient attention mechanisms , and the Autoformer, which employs autocorrelation and fast Fourier transforms to expedite computations . Similarly, FEDformer applies attention within the frequency domain using selected components to enhance performance . Despite these innovations, models mixing channels in multivariate series often exhibit reduced robustness to adapt to distribution drifts and achieve subpar performance [48; 11]. Consequently, some researchers have adopted a channel-independent approach, simplifying the model architecture and delivering robust results as well [29; 24]. However, ignoring the interactions among variates can limit further advancements. Recent trends have therefore shifted towards leveraging attention mechanisms to capture channel correlations [50; 35; 27]. Even though the performance is promising, their scalability is limited on large datasets. Another stream of research focuses on modeling time and channel dependencies through simpler structures like MLP [49; 7; 44]. Yet, they usually achieve sub-optimal performance compared to SOTA transformer-based methods, especially when the number of channels is large.

In this paper, we propose a new MLP-based method that breaks the dilemma of performance and efficiency, achieving state-of-the-art performance with merely linear complexity to both the number of channels and the length of the lookback window.

## 3 Softs

Multivariate time series forecasting (MTSF) deals with time series data that contain multiple variables, or channels, at each time step. Given historical values \(^{C L}\) where \(L\) represents the length of the lookback window, and \(C\) is the number of channels. The goal of MTSF is to predict the future values \(^{C H}\), where \(H>0\) is the forecast horizon.

### Overview

Our Series-cOre Fused Time Series forecaster (SOFTS) comprises the following components and its structure is illustrated in Figure 1.

Reversible instance normalization.Normalization is a common technique to calibrate the distribution of input data. In time series forecasting, the local statistics of the history are usually removed to stabilize the prediction of the base forecaster and restore these statistics to the model prediction . Following the common practice in many state-of-the-art models [29; 27], we apply reversible instance normalization which centers the series to zero means, scales them to unit variance, and reverses the normalization on the forecasted series. For PEMS dataset, we follow Liu et al.  to selectively perform normalization according to the performance.

Series embedding.Series embedding is an extreme case of the prevailing patch embedding in time series , which is equivalent to setting the patch length to the length of the whole series . Unlike patch embedding, series embedding does not produce extra dimension and is thus less complex than patch embedding. Therefore, in this work, we perform series embedding on the lookback window.

Figure 1: Overview of our SOFTS method. The multivariate time series is first embedded along the temporal dimension to get the series representation for each channel. Then the channel correlation is captured by multiple layers of STAR modules. The STAR module utilizes a centralized structure that first aggregates the series representation to obtain a global core representation, and then dispatches and fuses the core with each series, which encodes the local information.

Concretely, we use a linear projection to embed the series of each channel to \(_{0}=^{C d}\), where \(d\) is the hidden dimension:

\[_{0}=(). \]

Channel interaction.The series embedding is refined by multiple layers of STAR modules:

\[_{i}=(_{i-1}), i=1,2,,N. \]

The STAR module utilizes a star-shaped structure that exchanges information between different channels, as will be fully described in the next section.

Linear predictor.After \(N\) layers of STAR, we use a linear predictor (\(^{d}^{H}\)) to produce the forecasting results. Assume the output series representation of layer \(N\) to be \(_{N}\), the prediction \(}^{C H}\) is computed as:

\[}=(_{N}).\]

### STar Aggregate-Redistribute Module

Our main contribution is a simple but efficient STar Aggregate-Redistribute (STAR) module to capture the dependencies between channels. Existing methods employ modules like attention to extract such interaction. Although these modules directly compare the characteristics of each pair, they are faced with the quadratic complexity related to the number of channels. Besides, such a distributed structure may lack robustness when there are abnormal channels for the reason that they rely on the extract correlation between channels. Existing research on channel independence has already proved the untrustworthy correlations on non-stationary time series [48; 11]. To this end, we propose the STAR module to solve the inefficiency of the distributed interaction modules. This module is inspired by the star-shaped centralized system in software engineering, where instead of letting the clients communicate with each other, there is a server center to aggregate and exchange the information [31; 10], whose advantage is efficient and reliable. Following this idea, the STAR replaces the mutual series interaction with the indirect interaction through a core, which represents the global representation across all the channels. Compared to the distributed structure, STAR takes advantage of the robustness brought by aggregation of channel statistics , and thus achieves even better performance. Figure 2 illustrates the main idea of STAR and its difference between existing models like attention , GNN  and Mixer .

Given the series representation of each channel as input, STAR first gets the core representation of the multivariate series, at the heart of our SOFTS method. We define the core representation as follows:

**Definition 3.1** (Core Representation).: Given a multivariate series with \(C\) channels \(\{_{1},_{2},,_{C}\}\), the core representation \(\) is a vector generated by an arbitrary function \(f\) with the following form:

\[=f(_{1},_{2},,_{C})\]

Figure 2: The comparison of the STAR module and several common modules, like attention, GNN and mixer. These modules employ a distributed structure to perform the interaction, which relies on the quality of each channel. On the contrary, our STAR module utilizes a centralized structure that first aggregates the information from all the series to obtain a comprehensive core representation. Then the core information is dispatched to each channel. This kind of interaction pattern reduces not only the complexity of interaction but also the reliance on the channel quality.

The core representation encodes the global information across all the channels. To obtain such representation, we employ the following form, which is inspired by the Kolmogorov-Arnold representation theorem  and DeepSets :

\[_{i}=(_{1}(_{i-1})) \]

where \(_{1}:^{d}^{d^{}}\) is a projection that projects the series representation from the series hidden dimension \(d\) to the core dimension \(d^{}\), composing two layers with hidden dimension \(d\) and GELU  activation. \(\) is the stochastic pooling  that get the core representation \(^{d^{}}\) by aggregating representations of \(C\) series. Stochastic pooling combines the advantages of mean and max pooling. The details of computing the core representation can be found in Appendix B.2. Next, we fuse the representations of the core and all the series:

\[_{i}=(_{i-1},_{i}) \] \[_{i}=_{2}(_{i})+_{i-1} \]

The \(\) operation concatenate the core representation \(\) to each series representation, and we get the \(_{i}^{C(d+d^{})}\). Then another MLP \((_{2}:^{d+d^{}}^{d})\) is used to fuse the concatenated presentation and project it back to the hidden dimension \(d\), _i.e._, \(_{i} R^{C d}\). Like many deep learning modules, we also add a residual connection from the input to the output .

### Complexity Analysis

We analyze the complexity of each component of SOFTS step by step concerning window length \(L\), number of channels \(C\), model dimension \(d\), and forecasting horizon \(H\). The complexity of the reversible instance normalization and series embedding is \(O(CL)\) and \(O(CLd)\) respectively. In STAR, assuming \(d^{}=d\), the \(_{1}\) is a \(^{d}^{d}\) mapping with complexity \(O(Cd^{2})\). \(\) computes the softmax along the channel dimension, with complexity \(O(Cd)\). The \(_{2}\) on the concatenated embedding has the complexity \(O(Cd^{2})\). The complexity of the predictor is \(O(CdH)\). In all, the complexity of the encoding part is \(O(CLd+Cd^{2}+CdH)\), which is linear to \(C\), \(L\), and \(H\). Ignoring the model dimension \(d\), which is a constant in the algorithm and irrelevant to the problem, we compare the complexity of several popular forecasters in Table 1.

## 4 Experiments

Datasets.To thoroughly evaluate the performance of our proposed SOFTS, we conduct extensive experiments on 6 widely used, real-world datasets including ETT (4 subsets), Traffic, Electricity, Weather [51; 37], Solar-Energy  and PEMS (4 subsets) . Detailed descriptions of the datasets can be found in Appendix A.

### Forecasting Results

Compared methods.We extensively compare the recent Linear-based or MLP-based methods, including DLinear , TSMixer , TiDE . We also consider Transformer-based methods including FEDformer , Stationary , PatchTST , Crossformer , iTransformer  and CNN-based methods including SClNet , TimesNet .

Forecasting benchmarks.The long-term forecasting benchmarks follow the setting in Informer  and SClNet . The lookback window length (\(L\)) is set to 96 for all datasets. We set the prediction horizon (\(H\)) to \(\{12,24,48,96\}\) for PEMS and \(\{96,192,336,720\}\) for others. Performance comparison among different methods is conducted based on two primary evaluation metrics: Mean Squared Error (MSE) and Mean Absolute Error (MAE). The results of PatchTST and TSMixer are reproduced for the ablation study and other results are taken from iTransformer .

    & SOFTS (ours) & iTransformer & PatchTST & Transformer \\  Complexity & \(\) & \(O(C^{2}+CL+CH)\) & \(O(CL^{2}+CH)\) & \(O(CL+L^{2}+HL+CH)\) \\   

Table 1: Complexity comparison between popular time series forecasters concerning window length \(L\), number of channels \(C\) and forecasting horizon \(H\). Our method achieves only linear complexity.

[MISSING_PAGE_FAIL:6]

in performance across all pooling methods. Additionally, stochastic pooling deserves attention as it outperforms the other methods across nearly all the datasets.

Universality of STAR.The STar Aggregate-Redistribute (STAR) module is an embedding adaptation function [41; 43] that is replaceable to arbitrary transformer-based methods that use the attention mechanism. In this paragraph, we test the effectiveness of STAR on different existing transformer-based forecasters, such as PatchTST  and Crossformer . Note that our method can be regarded as replacing the channel attention in iTransformer . Here we involve substituting the time attention in PatchTST with STAR and incrementally replacing both the time and channel attention in Crossformer with STAR. The results, as presented in Table 4, demonstrate that replacing attention with STAR, which deserves less computational resources, could maintain and even improve the models' performance in several datasets.

Influence of lookback window length.Common sense suggests that a longer lookback window should improve forecast accuracy. However, incorporating too many features can lead to a curse

    &  &  &  &  &  &  \\   & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\  w/o STAR & 0.187 & 0.273 & 0.442 & 0.281 & 0.261 & 0.281 & 0.247 & 0.272 & 0.381 & 0.406 & 0.143 & 0.245 \\  Mean & **0.174** & 0.266 & 0.420 & 0.277 & 0.261 & 0.281 & 0.234 & 0.262 & 0.379 & 0.404 & 0.106 & 0.212 \\  Max & 0.180 & 0.270 & **0.406** & 0.271 & 0.259 & 0.280 & 0.246 & 0.269 & 0.379 & 0.401 & 0.116 & 0.223 \\  Weighted & 0.184 & 0.275 & 0.440 & 0.292 & 0.263 & 0.284 & 0.264 & 0.280 & 0.379 & 0.403 & 0.109 & 0.218 \\  Stochastic & **0.174** & **0.264** & 0.409 & **0.267** & **0.255** & **0.278** & **0.229** & **0.256** & **0.373** & **0.400** & **0.102** & **0.208** \\   

Table 3: Comparison of the effect of different pooling methods. The term ”w/o STAR” refers to a scenario where an MLP is utilized with the Channel Independent (CI) strategy, without the use of STAR. The result reveals that incorporating STAR into the model leads to a consistent enhancement in performance across all pooling methods. Apart from that, stochastic pooling performs better than mean and max pooling. Full results can be found in Table 7.

Figure 3: Memory and time consumption of different models. In Figure 2(a), we set the lookback window \(L=96\), horizon \(H=720\), and batch size to 16 in a synthetic dataset we conduct. In Figure 2(b), we set the lookback window \(L=96\), horizon \(H=720\), and batch size to \(4\) in Traffic dataset. Figure 2(a) reveals that SOFTS model scales to large number of channels more effectively than Transformer-based models. Figure 2(b) shows that previous Linear-based or MLP-based models such as DLinear and TSMixer perform poorly with a large number of channels. While SOFTS model demonstrates efficient performance with minimal memory and time consumption.

of dimensionality, potentially compromising the model's forecasting effectiveness. We explore how varying the lengths of these lookback windows impacts the forecasting performance for time horizons from 48 to 336 in all datasets. As shown in Figure 4, SOFTS could consistently improve its performance by effectively utilizing the enhanced data available from an extended lookback window. Also, SOFTS performs consistently better than other models under different lookback window lengths, especially in shorter cases.

Hyperparameter sensitivity analysis.We investigate the impact of several key hyperparameters on our model's performance: the hidden dimension of the model, denoted as \(d\), the hidden dimension of the core, represented by \(d^{}\), and the number of encoder layers, \(N\). Analysis of Figure 5 indicates that complex traffic datasets (such as Traffic and PEMS) require larger hidden dimensions and more encoding layers to handle their intricacies effectively. Moreover, variations in \(d^{}\) have a minimal influence on the model's overall performance.

Series embedding adaptation of STAR.The STAR module adapts the series embeddings by extracting the interaction between channels. To give an intuition of the functionality of STAR, we visualize the series embeddings before and after being adjusted by STAR. The multivariate series is selected from the test set of Traffic with look back window 96 and number of channels 862. Figure 6 shows the series embeddings visualized by T-SNE before and after the first STAR module. Among the 862 channels, there are 2 channels embedded far away from the other channels. These two channels can be seen as anomalies, marked as (\(\)) in the figure. Without STAR, _i.e._, using only the channel independent strategy, the prediction on the series can only achieve 0.414 MSE. After being adjusted by STAR, the abnormal channels can be clustered towards normal channels by exchanging channel

    &  &  &  &  &  &  &  \\   & & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\   & Attention & 0.189 & 0.276 & 0.454 & 0.286 & 0.256 & 0.279 & 0.137 & 0.240 & 0.145 & 0.249 & 0.144 & 0.233 \\  & STAR & **0.185** & **0.272** & **0.448** & **0.279** & **0.252** & **0.277** & **0.134** & **0.233** & **0.136** & **0.238** & **0.137** & **0.225** \\  ^{}\)} & Attention & 0.202 & 0.301 & **0.546** & 0.297 & 0.254 & 0.310 & 0.100 & 0.208 & 0.090 & 0.198 & 0.084 & 0.181 \\  & STAR & **0.198** & **0.292** & 0.549 & **0.292** & **0.252** & **0.305** & **0.100** & **0.204** & **0.087** & **0.194** & **0.080** & **0.175** \\   & Attention & 0.178 & 0.270 & 0.428 & 0.282 & 0.258 & 0.278 & 0.113 & 0.221 & 0.111 & 0.221 & 0.101 & 0.204 \\  & STAR & **0.174** & **0.264** & **0.409** & **0.267** & **0.255** & **0.278** & **0.104** & **0.210** & **0.102** & **0.208** & **0.087** & **0.184** \\   

Table 4: The performance of STAR in different models. The attention replaced by STAR here are the time attention in PatchTST, the channel attention in iTransformer, and both the time attention and channel attention in modified Crossformer. The results demonstrate that replacing attention with STAR, which requires less computational resources, could maintain and even improve the models’ performance in several datasets. \({}^{}\): The Crossformer used here is a modified version that replaces the decoder with a flattened head like what PatchTST does. Full results can be found in Table 8.

Figure 4: Influence of lookback window length \(L\). SOFTS performs consistently better than other models under different lookback window lengths, especially in shorter cases.

information. An example of the normal channels is marked as (\(\)). Predictions on the adapted series embeddings can improve the performance to 0.376, a **9%** improvement.

Impact of channel noise.As previously mentioned, SOFTS can cluster abnormal channels towards normal channels by exchanging channel information. To test the impact of an abnormal channel on the performance of three models--SOFTS, PatchTST, and iTransformer--we select one channel from the PEMS03 dataset and add Gaussian noise with a mean of 0 and a standard deviation representing the strength of the noise. The lookback window and horizon are set to 96 for this experiment. In Figure (c)c, we observe that the MSE of PatchTST increases sharply as the strength of the noise grows. In contrast, SOFTS and iTransformer can better handle the noise. This indicates that suitable channel interaction can improve the robustness against noise in one channel using information from the normal channels. Moreover, SOFTS demonstrates superior noise handling compared to iTransformer. This suggests that while the abnormal channel can affect the model's judgment of normal channels, our STAR module can mitigate the negative impact more effectively by utilizing core representation instead of building relationships between every pair of channels.

## 5 Conclusion

Although channel independence has been found an effective strategy to improve robustness for multivariate time series forecasting, channel correlation is important information to be utilized

Figure 5: Impact of several key hyperparameters: the hidden dimension of the model, denoted as \(d\), the hidden dimension of the core, represented by \(d^{}\), and the number of encoder layers, \(N\). Full results can be seen in Appendix C.5.

Figure 6: Figure (a)a (b): T-SNE of the series embeddings on the Traffic dataset. (a)a: the series embeddings before STAR. Two abnormal channels (\(\)) are located far from the other channels. Forecasting on the embeddings achieves 0.414 MSE. (b)b: series embeddings after being adjusted by STAR. The two channels are clustered towards normal channels (\(\)) by exchanging channel information. Adapted series embeddings improve forecasting performance to 0.376. Figure (c)c: Impact of noise on one channel. Our method is more robust against channel noise than other methods.

for further improvement. The previous methods faced a dilemma between model complexity and performance in extracting the correlation. In this paper, we solve the dilemma by introducing the Series-cOre Fused Time Series forecaster (SOFTS) which achieves state-of-the-art performance with low complexity, along with a novel STar Aggregate-Redistribute (STAR) module to efficiently capture the channel correlation.

Our paper explores the way of building a scalable multivariate time series forecaster while maintaining equal or even better performance than the state-of-the-art methods, which we think may pave the way to forecasting on datasets of more larger scale under resource constraints .