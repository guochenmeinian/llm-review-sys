# ReHLine: Regularized Composite ReLU-ReHU Loss Minimization with Linear Computation and Linear Convergence

ReHLine: Regularized Composite ReLU-ReHU Loss Minimization with Linear Computation and Linear Convergence

 Ben Dai

Department of Statistics

The Chinese University of Hong Kong

bendai@cuhk.edu.hk

&Yixuan Qiu

School of Statistics and Management

Shanghai University of Finance and Economics

qiuyixuan@sufe.edu.cn

Co-first authorship, and the order is determined by coin toss. \({}^{}\)Corresponding author.

###### Abstract

Empirical risk minimization (ERM) is a crucial framework that offers a general approach to handling a broad range of machine learning tasks. In this paper, we propose a novel algorithm, called ReHLine, for minimizing a set of regularized ERMs with convex piecewise linear-quadratic loss functions and optional linear constraints. The proposed algorithm can effectively handle diverse combinations of loss functions, regularization, and constraints, making it particularly well-suited for complex domain-specific problems. Examples of such problems include FairSVM, elastic net regularized quantile regression, Huber minimization, etc. In addition, ReHLine enjoys a provable _linear_ convergence rate and exhibits a per-iteration computational complexity that scales _linearly_ with the sample size. The algorithm is implemented with both Python and R interfaces, and its performance is benchmarked on various tasks and datasets. Our experimental results demonstrate that ReHLine significantly surpasses generic optimization solvers in terms of computational efficiency on large-scale datasets. Moreover, it also outperforms specialized solvers such as liblinear in SVMs, hqreg in Huber minimization and lightning (SAGA, SAG, SDCA, SVRG) in smoothed SVMs, exhibiting exceptional flexibility and efficiency. The source code, project page, accompanying software, and the Python/R interface can be accessed through the link: [https://github.com/softmin/ReHLine](https://github.com/softmin/ReHLine).

## 1 Introduction

Empirical risk minimization (ERM) with different losses is a fundamental framework in statistical machine learning , which has demonstrated a significant impact on many application areas, such as artificial intelligence, computational biology, computer vision, natural language processing, electronic engineering, and finance. In fact, many common practical ERMs are associated with a piecewise linear-quadratic (PLQ; ) loss function, including support vector machines (SVMs; ), the least absolute deviation regression, quantile regression , Huber regression , borrowing cost function in portfolio selection , and pricing/planning problems in electricity and operational research [16; 1], among many others. In addition, ERMs are frequently blended with linear constraints, for instance, the fairness constraints in fair classification , monotonicity constraints in classification or regression, etc. More linear constraints according to the prior domain knowledge, such as the sum-to-one constraints in the portfolio variance minimization problem , negative or positive constraints in non-performing loan , are considered in various real applications. Therefore, it has become of utmost importance to develop fast and scalable numerical algorithms to solve ERMs with PLQ losses based on large-scale datasets and flexible linear constraints.

In this paper, we consider a general regularized ERM based on a convex PLQ loss with linear constraints:

\[_{^{d}}_{i=1}^{n}L_{i}(_{i}^{} )+\|\|_{2}^{2},+, \]

where \(_{i}^{d}\) is the feature vector for the \(i\)-th observation, \(^{d}\) is an unknown coefficient vector, \(^{K d}\) and \(^{K}\) are defined as linear inequality constraints for \(\), and \(L_{i}() 0\) is a convex PLQ loss function. The PLQ function class greatly generalizes existing popular loss functions, and the convexity of \(L_{i}()\) guarantees the global convergence of optimization. Here, we focus on working with a large-scale dataset, where the dimension of the coefficient vector and the total number of constraints are comparatively much smaller than the sample size, that is, \(d n\) and \(K n\).

Although (1) is a strongly convex problem with affine constraints, thus admitting a unique global optimum, directly solving (1) by a generic solver may encounter various challenges. For example, \(L_{i}()\) is in general non-smooth, so gradient-based methods may fail, and hence slow-convergent subgradient methods need to be used. In addition, the constraint set is a polyhedron, whose projection operator is non-trivial to compute, making various projection-based methods difficult to apply.

Of course, for some specific forms of \(L_{i}()\), highly efficient solvers for (1) indeed exist. One remarkable example is the liblinear solver , which has gained great success in solving large-scale SVMs. However, liblinear highly relies on the hinge loss function, so its success does not directly transfer to more general loss functions.

Outline.To this end, in this article we develop a novel algorithm to solve (1) with the greatest generality. The proposed algorithm, named ReHLine, has three key ingredients. First, we show that any convex PLQ loss function can be decomposed as the sum of a finite number of rectified linear units (ReLU, ) and rectified Huber units (ReHU, defined in Section 2). Second, based on this decomposition, the dual problem of (1) is shown to be a box-constrained quadratic programming (box-QP) problem. Finally, a special paired primal-dual coordinate descent algorithm is developed to simultaneously solve the primal and dual problems, with a computational cost linear in \(n\).

We emphasize that for the second point, the dual box-QP problem is highly structured. Therefore, a general-purpose solver may not fully exploit the structure, thus leading to slower convergence rates or higher computational costs. As a direct comparison between the proposed ReHLine solver and existing algorithms, including the projected (sub-)gradient descent (P-GD; ), the coordinate descent (CD; ), the interior-point methods (IPMs; ), the stochastic dual coordinate ascent (SDCA; ) related methods, and the alternating direction method of multipliers (ADMM; ), Table 1 summarizes the required number of iterations and per-iteration cost of each method. More details and discussions for this comparison are given in Section 4.

Contribution.Compared with other specialized ERM solvers or general-purpose box-QP solvers, the proposed ReHLine solver has four appealing "linear properties":

   Algorithm & Complexity & \#Iterations & Complexity \\  & (per iteration) & & (total) \\  P-GD & \((n)\) & \((^{-1})\) & \(n^{-1}\) \\ CD & \((n^{2})\) & \(((^{-1}))\) & \(n^{2}(^{-1})\) \\ IPM & \((n^{2})\) & \(((^{-1}))\) & \(n^{2}(^{-1})\) \\ ADMM & \((n^{2})\) & \(o(^{-1})\) & \(on^{2}^{-1}\) \\ SDCA & \((n)\) & \((^{-1})\) & \(n^{-1}\) \\ ReHLine (ours) & \((n)\) & \(((^{-1}))\) & \((n(^{-1}))\) \\   

Table 1: Overview of existing algorithms for solving (1). Column Complexity (per iteration) shows the computational complexity of the algorithm per iteration. Here, we focus only on the order of \(n\) since \(d n\) is assumed in our setting. Column #Iterations shows the number of iterations needed to achieve an accuracy of \(\) to the minimizer.

1. It applies to any convex piecewise linear-quadratic loss function (potential for non-smoothness included), including the hinge loss, the check loss, the Huber loss, etc.
2. In addition, it supports linear equality and inequality constraints on the parameter vector.
3. The optimization algorithm has a provable linear convergence rate.
4. The per-iteration computational complexity is linear in the sample size.

Moreover, ReHLine is designed to be a computationally efficient and practically useful software package for large-scale ERM problems. As an example, our experiments in Section 5 have shown that ReHLine significantly surpasses generic solvers in terms of computational efficiency on large-scale datasets, and provides reasonable improvements over specialized solvers and problems, such as liblinear, hqreg and lightning, given that liblinear, hqreg and lightning are one of the fastest and most widely-used solvers for SVM, Huber minimization, and smoothed SVM, respectively. Finally, ReHLine is available as user-friendly software in both Python and R, which facilitates ease of use for researchers, practitioners, and a wider audience.

## 2 The ReLU-ReHU decomposition

As is introduced in Section 1, we attempt to solve ERM with a general loss function, and one sensible choice is the class of convex PLQ functions. A univariate loss function \(f:_{ 0}\) is PLQ if there exist a finite number of knots \(t_{1}<t_{2}<<t_{K}\) such that \(f\) is equal to a linear or quadratic function on each of the intervals \((-,t_{1}],[t_{1},t_{2}],,[t_{K-1},t_{K}],[t_{K},)\). In what follows we merely consider nonnegative convex PLQ functions, which also imply the continuity of the loss function. It is well known that PLQ functions are universal approximators of continuous functions , but the canonical definition of PLQ is not convenient for optimization algorithms. Instead, in this section we prove that convex PLQ functions can actually be decomposed into a series of simple and basic units.

**Definition 1** (Composite ReLU-ReHU function).: _A function \(L(z)\) is composite ReLU-ReHU, if there exist \(,^{L}\) and \(,,^{H}\) such that_

\[L(z)=_{l=1}^{L}(u_{l}z+v_{l})+_{h=1}^{H}_{ _{h}}(s_{h}z+t_{h}), \]

_where \((z)=z_{+}\), and \(_{_{h}}(z)\) is defined in (2)._

Clearly, the composite ReLU-ReHU function class is closed under affine transformations, as is indicated by Proposition 1.

**Proposition 1** (Closure under affine transformation).: _If \(L(z)\) is a composite ReLU-ReHU function as in (3), then for any \(c>0\), \(p\), and \(q\), \(cL(pz+q)\) is also composite ReLU-ReHU, that is,_

\[cL(pz+q)=_{l=1}^{L}(u^{}_{l}z+v^{}_{l})+_{h=1 }^{H}_{^{}_{h}}(s^{}_{h}z+t^{}_{h}),\]

_where \(u^{}_{l}=cpu_{l}\), \(v^{}_{l}=cu_{l}q+cv_{l}\), \(^{}_{h}=_{h}\), \(s^{}_{h}=ps_{h}\), and \(t^{}_{h}=(s_{h}q+t_{h})\)._

Figure 1: The ReLU function and the ReHU function with \(=1,2,3,\).

More importantly, we show that the composite ReLU-ReHU function class is equivalent to the class of convex PLQ functions.

**Theorem 1**.: _A loss function \(L:_{ 0}\) is convex PLQ if and only if it is composite ReLU-ReHU._

Based on the fact that the loss functions of various SVMs (the hinge loss, the squared hinge loss, and the smoothed hinge loss), quantile regression (the check loss), and least absolute deviation (the absolute loss) are all convex PLQs, Theorem 1 suggests that Definition 1 incorporates numerous widely-used losses in statistics and machine learning applications. Table 2 provides a convenient translation from some commonly-used loss functions to their ReLU-ReHU representation.

Taken together, our algorithm is designed to address the empirical regularized ReLU-ReHU minimization problem, named _ReHLine optimization_, of the following form:

\[_{^{d}} _{i=1}^{n}_{l=1}^{L}(u_{li}_{i}^{ }+v_{li})+_{i=1}^{n}_{h=1}^{H}_{_{ hi}}(s_{hi}_{i}^{}+t_{hi})+\|\|_{2}^ {2},\] \[ +, \]

where \(=(u_{li}),=(v_{li})^{L n}\) and \(=(s_{hi}),=(t_{hi}),=(_{hi})^ {H n}\) are the ReLU-ReHU loss parameters, as illustrated in Table 2, and \((,)\) are the constraint parameters, as illustrated in Table 3. This formulation has a wide range of applications, including statistics, machine learning, computational biology, and social studies. Some popular examples include SVMs with fairness constraints (FairSVM), elastic net regularized quantile regression (ElasticQR), ridge Huber minimization (RidgeHuber), and smoothed SVMs (sSVM). See Appendix A for details.

Despite appearing to impose limitations on the \(l_{2}\)-regularization in (4), our ReHLine formulation can actually be extended to more general regularization forms, including the elastic net penalty .

**Proposition 2**.: _If \(L_{i}\) is a composite ReLU-ReHU loss function with parameters \((u_{li},v_{li})_{l=1}^{L}\) and \((s_{hi},t_{hi},_{hi})_{h=1}^{H}\), then its elastic net regularized minimization with \(_{1} 0\) and \(_{2}>0\),_

\[_{^{d}}_{i=1}^{n}L_{i}(_{i}^{ })+_{1}\|\|_{1}+}{2}\| \|_{2}^{2},+ ,\]

   Problem & Loss (\(L_{i}(z_{i})\)) & Composite ReLU-ReHU Parameters \\  SVM & \(c_{i}(1-y_{i}z_{i})_{+}\) & \(u_{1i}=-c_{i}y_{i},v_{1i}=c_{i}\) \\ sSVM & \(c_{i}_{1}(-(y_{i}z_{i}-1))\) & \(s_{1i}=-}y_{i},t_{1i}=},=}\) \\ SVM\({}^{2}\) & \(c_{i}((1-y_{i}z_{i})_{+})^{2}\) & \(s_{1i}=-}y_{i},t_{1i}=},=\) \\ LAD & \(c_{i}|y_{i}-z_{i}|\) & \(u_{1i}=c_{i},v_{1i}=-c_{i}y_{i},u_{2i}=-c_{i},v_{2i}=c_{i}y_{i}\) \\ SVR & \(c_{i}(|y_{i}-z_{i}|-)_{+}\) & \(u_{1i}=c_{i},v_{1i}=-(y_{i}+),u_{2i}=-c_{i},v_{2i}=y_{i}-\) \\ QR & \(c_{i}_{}(y_{i}-z_{i})\) (A.3) & \(u_{1i}=-c_{i},v_{1i}= c_{i}y_{i},u_{2i}=c_{i}(1-),v_{2i}=-c_ {i}(1-)y_{i}\) \\   

Table 2: Some widely-used composite ReLU-ReHU losses as in (3). Here SVM is weighted SVMs based on the hinge loss , sSVM is smoothed SVMs based on the smoothed hinge loss , SVM\({}^{2}\) is weighted squared SVMs based on the squared hinge loss , LAD is the least absolute deviation regression, SVR is support vector regression with the \(\)-insensitive loss , and QR is quantile regression with the check loss .

   Constraint & Form & ReHLine Parameters \\  Nonnegative & \(\) & \(=-,=\) \\ Box & \(\) & \(=[;-],=[-;]\) \\ Monotonicity & \(_{1}_{d}\) & \(=_{1:d-1},=\)-bidag\(([-1,1])\), \(=_{d-1}\) \\ Fairness (A.1) & \(|^{}/n|\) & \(=^{}[;-]/n,=[;]\) \\   

Table 3: Some widely-used linear constraints of the form \(+ 0\), where \(([-1,1])\) is an upper bidiagonal matrix with main diagonal being -1 and upper diagonal being 1.

can be rewritten as the form of ReHLine optimization (4) with_

\[(} &_{L d}\\ _{n}^{}&}{_{2}}_{d}^{ }\\ _{n}^{}&-}{_{2}}_{d}^{ }),( []{cc}}&_{L d}\\ _{n+d}^{}\\ _{n+d}^{}), (\\ _{d}),\] \[(}}&_{d}^{}), (}}&_{d}^{}), (}} &_{d}^{}),\]

_where \(\) indicates assigning a value to a parameter._

## 3 Optimization algorithm

In this section, we present the ReHLine algorithm to tackle the optimization in (4). The proposed algorithm is inspired by liblinear, which has gained great success in solving standard SVMs. The key idea of ReHLine is to leverage the linear property of Karush-Kuhn-Tucker (KKT) conditions, thus simplifying the iterative complexity of CD and achieving rapid convergence.

### Primal and dual formulations

To proceed, we first demonstrate equivalent formulations for ReLU and ReHU, respectively, where the equivalence means that the value on the left is equal to the minimum value of the right-hand side:

\[(z)_{}&\\ & z,\\ & 0.(z) _{,}&^{2}+\\ &+ z,\\ & 0. \]

It is worth noting that when \(=\), the optimal solution for \(\) is 0, and hence the last constraint (\( 0\)) can be removed. Then according to the definition of the composite ReLU-ReHU loss function and the equivalent forms in (5), the ReHLine optimization problem (4) can be rewritten as:

\[_{,,,} _{i=1}^{n}_{l=1}^{L}_{li}+_{i=1}^{n}_{h=1}^{H} _{hi}^{2}+_{i=1}^{n}_{h=1}^{H}_{hi}_{hi}+ \|\|_{2}^{2}\] \[+ ,_{li} u_{i}_{i}^{}+v_ {li},_{hi}+_{hi} s_{hi}_{i}^{} +t_{hi},\] \[_{li} 0,_{hi} 0,(i,l,h), \]

where \(=(_{li})^{L n}\), \(=(_{hi})^{H n}\) and \(=(_{hi})^{H n}\) are slack variables. Theorem 2 shows the dual form of the primal formulation (6) by using the Lagrangian duality theory, and the relation between primal and dual variables is also provided.

**Theorem 2**.: _The Lagrangian dual problem of (6) is:_

\[(},}, }) =,,}{}\ \ (,,)\] \[\ \ , , , \] \[(,,) =^{}^{ }+()^{ }}_{(3)}^{}}_{(3)} ()+()^ {}}}_{(3)}^{}}}_{(3)}+()\] \[-^{}}_{(3)}()-^{ }}}_{(3)}( )+()^{}}_{(3)}^{}}}_{(3)}( )\] \[+^{}-( ^{})-(^{}), \]

_where \(^{K}\), \(=(_{li})^{L n}\), and \(=(_{hi})^{H n}\) are dual variables, \(}}_{(3)}^{d nL}\) and \(}}_{(3)}^{d nH}\) are the mode-3 unfolding of the tensors \(}}=(u_{1ij})^{L n d}\) and \(}}=(s_{hij})^{H n d}\), respectively, \(u_{1ij}=u_{li}x_{ij}\), \(s_{hij}=s_{hi}x_{ij}\), \(\) is the identity matrix, and all inequalities are elementwise._

_Moreover, the optimal point \(}\) of (6) can be recovered as:_

\[}=_{k=1}^{K}_{k}_{k}-_ {i=1}^{n}_{i}(_{l=1}^{L}_{li}u_{li}+_{h= 1}^{H}_{hi}s_{hi})=^{}}-}}_{(3)}(})-}}_{(3)}(}). \]

### ReHLine update rules

Indeed, (9) is a KKT condition of (6), and the main idea of the proposed ReHLine solver is to update the dual variables \((,,)\) by CD on (7) with an _analytic solution_, and _simultaneously_ update the primal variable \(\) by the KKT condition (9). Most importantly, the computational complexity of the CD updates on (7) can be significantly reduced by using the information of \(\).

Canonical CD updates.As a first step, we consider the canonical CD update rule that directly optimizes the dual problem (7) with respect to a single variable. For brevity, in this section we only illustrate the result for \(_{li}\) variables, and the full details are given in Appendix B.

By excluding the terms unrelated to \(_{li}\), we have \(_{li}^{}=*{argmin}_{0 1} _{li}()\), where

\[_{li}() =u_{li}^{2}(_{i}^{}_{i} )^{2}+_{(l^{},i^{})(l,i)}_{l^{}i^{ }}u_{l^{}i^{}}u_{li}(_{i^{}}^{} _{i})-_{k=1}^{K}_{k}u_{li}(_{k}^{}_{i})\] \[+_{h^{},i^{}}u_{li}_{h^{}i^{ }}s_{h^{}i^{}}_{i^{}}^{} _{i^{}}-v_{li}.\]

Therefore, by simple calculations we obtain

\[_{li}^{}=_{}(_{i}^ {}_{k=1}^{K}_{k}_{k}-_{(l^{},i^{ })(l,i)}_{l^{}i^{}}u_{l^{}i^{}} _{i^{}}-_{h^{},i^{}}_{h^{}i^{ }}s_{h^{}i^{}}_{i^{}}+v_{li}}{u_{li}^ {2}\|_{i}\|_{2}^{2}}), \]

where \(_{[a,b]}(x)=(a,(b,x))\) means projecting a real number \(x\) to the interval \([a,b]\).

Clearly, assuming the values \(_{i}^{}_{k}\) and \(\|_{i}\|_{2}^{2}\) are cached, updating one \(_{li}\) value requires \((K+nd+nL+nH)\) of computation, and updating the whole \(\) matrix requires \(nL(K+nd+nL+nH)\). Adding all variables together, the canonical CD update rule for one full cycle has a computational complexity of \((K+nd+nL+nH)(K+nL+nH)\).

ReHLine updates.The proposed ReHLine algorithm, on the other hand, significantly reduces the computational complexity of canonical CD by updating \(\) according to the KKT condition (9) after each update of a dual variable. To see this, let \(:=(,,)\) denote all the dual variables, and define

\[()=_{k=1}^{K}_{k}_{k}-_{i=1}^{n} _{i}(_{l=1}^{L}_{li}u_{li}+_{h=1}^{H}_{ hi}s_{hi}).\]

Then it can be proved that \((_{}_{li})(_{li})=-(u_{li}_{i}^{ }()+v_{li})\). Therefore, when \(\) is fixed at \(^{}=(^{},^{},^{})\) and let \(^{}=(^{})\), (10) can be rewritten as

\[_{li}^{}=_{}(_{li}^{}- })(^{})}{u_{li}^{2}\| _{i}\|_{2}^{2}})=_{}(_{li}^{}+_{i}^{}^{}+v_{li}}{u_{li}^ {2}\|_{i}\|_{2}^{2}}).\]

Accordingly, the primal variable \(\) is updated as

\[^{}=^{}-(_{li}^{}- _{li}^{})u_{li}_{i},\]

which can then be used for the next dual variable update. Simple calculations show that this scheme only costs \((d)\) of computation for one \(_{li}\) variable.

The update rules for other dual variables are similar, with the complete algorithm shown in Algorithm 1 and the full derivation details given in Appendix B. Overall, the ReHLine update rule has a computational complexity of \((K+nL+nH)d\) for one full cycle. Given that \((K,L,H)\) are all small numbers, the complexity is _linear_ with respect to both the sample size \(n\) and the dimension \(d\).

We emphasize that the linear relationship (9) between the primal and dual variables, as well as its consequence of reducing the computational cost, is highly non-trivial. This technique has been proposed for specialized problems such as SVMs via the liblinear solver , and here we show that its success can be greatly generalized to any convex PLQ loss functions.

### Global convergence rate

Finally, we show that the ReHLine algorithm has a fast global linear convergence rate.

**Theorem 3**.: _Let \(^{(q)}(^{(q)},^{(q)},^{(q)})\) be a sequence of iterates generated by Algorithm 1, and \(^{*}\) be a minimizer of the dual objective (8). Then \(^{(q)}\) is feasible, and the dual objective value converges at least linearly to that of \(^{*}\), that is, there exist \(0<<1\) and \(q_{0}\) such that for all \(q q_{0}\),_

\[(^{(q+1)})-(^{*})(^{(q)})-(^{*}).\]

## 4 Related work

Table 1 summarizes the existing methods in solving the primal or dual problem of (1). Note that Column #iterations is the best-fit theoretical results of convergence analysis for each algorithm based on our literature search, which may be improved with further developments of convergence analysis. These algorithms can be broadly divided into the following categories.

Generic QP solvers.For instance, a projected sub-gradient descent is readily applicable to the primal form (1) by computing the sub-gradient of each loss and the projection under affine constraints, although the projection operator itself is already difficult. As for the box-QP dual problem (7), CD converges linearly to the optimal solution [27; 31; 35; 42]. Box-QP can also be efficiently solved using IPM, which leverages second-order cone programming techniques [13; 14]. Moreover, ADMM was employed to iteratively minimize the augmented Lagrangian of (1).

ERM solvers.There are multiple existing works developed to tackle ERM problems, such as SAG , SVRG , SAGA , proximal SDCA  and SDCA . However, SAG, SVRG, SAGA and proximal SDCA can only handle smooth loss functions, with an optional non-smooth term that has an easy proximal operator. SDCA applies to general loss functions, but it requires the convex conjugate of loss functions, which is not necessarily simple to compute, also see discussion in Section 3.1 in . Moreover, it only guarantees a sublinear convergence rate for non-smooth loss functions. In contrast, ReHLine supports all convex PLQ loss functions with optional general linear constraints, which could be non-smooth by construction, and they all enjoy a linear computational complexity and provable linear convergence.

For non-smooth loss functions, another existing method to solve ERM is the smoothing technique [40; 3; 30], which approximates the non-smooth terms by smooth functions, and then uses smoothness algorithms to solve the smooth problem. However, the choice of the smoothing function and smoothing parameter typically requires additional knowledge, and the linear convergence rate is not always guaranteed.

Despite the successful use of the existing algorithms in solving (1), these methods either suffer from slow convergence (P-GD), or high computational costs (CD, IPM, and ADMM). This is partly because these general methods do not fully exploit the inherent structure of the problem. In contrast, ReHLine leverages the KKT condition in (9) to substantially improve numerical performance, thus achieving a linear computational complexity per iteration with respect to the sample size, as well as a linear convergence rate.

In addition to generic algorithms, specialized algorithms have been developed for specific types of (1). For example, the dual coordinate descent of liblinear in solving SVMs , the semismooth Newton coordinate descent algorithm of hqreg in solving the ridge regularized Huber minimization , and SAGA, SAG, proximal SDCA, and SVRG of lightning in solving smooth ERMs , such as smoothed SVMs. Our algorithm is also compared to these specialized methods in Section 5.

**Software.** Below are some commonly-used software for solving (1) based on the algorithms available.

* cvx/cvxpy[17; 10]: a modeling language for convex optimization problems and supports a wide range of solvers such as ecos, scs, osqp, and mosek.
* mosek: a commercial convex optimization solver based on IPM.
* ecos: an open-source solver of second-order cone programming based on IPM.
* scs: an open-source convex quadratic cone solver based on ADMM.
* dccp: a disciplined convex-concave programming solver built on top of cvxpy, which has been used to solve classification problems with fairness constraints .
* liblinear: a library for large linear classification.
* horeg: an R package solving Lasso or elastic net penalized Huber loss regression and quantile regression.
* lightning: a Python library solving various ERMs based on primal SDCA, SGD, AdaGrad, SAG, SAGA, SVRG.

## 5 Experiments and benchmarks

In this section we demonstrate the performance of ReHLine compared with the existing state-of-the-art (SOTA) solvers on various machine learning tasks. The source code of ReHLine, along with Python/R interface, is readily accessible on our GitHub repository ([https://github.com/softmin/ReHLine](https://github.com/softmin/ReHLine)) and our project page ([https://rehline.github.io/](https://rehline.github.io/)). Our experiments involve five prominent machine learning tasks that draw from diverse sets of data. Specifically, we focus on four classification datasets and five regression datasets sourced from OpenML ([https://www.openml.org/](https://www.openml.org/)), with various scales and dimensions. The experiment settings are summarized in Table 4. To achieve a fair comparison, we use a well-organized toolset and framework, the Benchopt library , to implement optimization benchmarks for all the SOTA solvers.

Due to representation limits, we only present the results using a variant of the ReHLine algorithm that supports variable shrinkage (refer to Algorithm 2). Results utilizing ReHLine with or without shrinkage are reported on our benchmark repository ([https://github.com/softmin/ReHLine-benchmark](https://github.com/softmin/ReHLine-benchmark)). Note that there is no substantial difference between these variations, and they do not influence the conclusions drawn from our experiments. Simultaneously, in the benchmark repository, we provide a more detailed "time versus objective" plot for each task/dataset and each solver produced by Benchopt.

For the **FairSVM** task, ReHLine is compared with the original solver  based on dccp and other generic solvers in cvxpy. In this case, both feasibility and optimality are examined, where the

   Classification dataset (\(n d\)) & Regression dataset (\(n d\)) \\  Steel-plates-fault (SPF): \(1941 34\) & Liver-disorders (LD): \(345 6\) \\ Philippine: \(5832 309\) & Kin8nm: \(8191 9\) \\ Sylva-prior: \(14395 109\) & Topo-2-1: \(8885 267\) \\ Creditcard: \(284807 31\) & House-8L: \(22784 9\) \\  & Buzzin-Twitter (BT): \(583250 78\) \\   

Table 4: The classification and regression datasets with different scales used in our experiments.

optimality is measured by the objective function, and the feasibility is measured by the violation of constraints: \((n^{-1}|_{i=1}^{n}z_{i}^{}_{i}| -,10^{-6})\). Moreover, we examine the performance of **ElasticQR** by considering the model defined in (A.2) with \(_{1}=_{2}=1\), **RidgeHuber** of (A.4) with \(_{1}=0,_{2}=1\), and **SVM** of (A.1) and **sSVM** of (A.5) with \(C=1\). Table 5 presents the running times in seconds of all solvers that converge based on a relative/absolute tolerance of \(10^{-5}\).

The results in Table 5 indicate that the proposed ReHLine algorithm/software achieves visible improvements over existing solvers. The major empirical findings are as follows. First, the amount of improvement achieved by ReHLine in terms of running time is substantial over generic solvers, including ecos, scs, and mosek based on cvx, with the largest improvement 100x-1000x speed-up on the largest-scale dataset. The improvement continues to expand even further as the scale of the problem grows. Second, ReHLine also yields considerable improvements for specialized algorithms, including liblinear in SVM, hqreg in Huber minimization, and lightning in sSVM. Third, given that the generic solvers fail or time out in multiple tasks and datasets, ReHLine has shown remarkable flexibility when it comes to solving various domain-specific problems.

   Task & Dataset & ecos & mosek & scs & dccp & ReHLine \\  FairSVM & SPF (\(\)1e-4) & ✗ & ✗ & ✗ & ✗ & 4.25 (\( 0.5\)) \\  & Philippine (\(\)1e-2) & 1550 (\( 0.6\)) & 87.4 (\( 0.2\)) & 130 (\( 42\)) & 1137 (\( 9.2\)) & 1.03 (\( 0.2\)) \\  & Sylva-prior (\(\)1e-2) & ✗ & ✗ & ✗ & 0.47 (\( 0.1\)) \\  & Creditcard (\(\)1e-1) & 175 (\( 0.2\)) & 64.2 (\( 0.1\)) & 161 (\( 405\)) & ✗ & 0.64 (\( 0.2\)) \\   & Fail/Succeed &  & 2/2 & 2/2 & 3/1 & 0/4 \\  & Speed-up (on Creditcard) & 273x & 100x & 252x & \(\) & \(-\) \\   Task & Dataset & ecos & mosek & scs & ReHline \\  ElasticQR & LD (\(\)1e-4) & ✗ & 106 (\( 7\)) & 34.9 (\( 25.0\)) & 2.60 (\( 0.30\)) \\  & Kin8m (\(\)1e-3) & ✗ & 92.0 (\( 1.0\)) & 63.1 (\( 58.5\)) & 4.12 (\( 0.95\)) \\  & House-8L (\(\)1e-3) & 887 (\( 161\)) & 277 (\( 34\)) & ✗ & 7.21 (\( 1.99\)) \\  & Topo-2-1 (\(\)1e-2) & 4752 (\( 2015\)) & ✗ & ✗ & 3.04 (\( 0.49\)) \\  & BT (\(\)1e-0) & 7079 (\( 2517\)) & ✗ & ✗ & 2.49 (\( 0.56\)) \\   & Fail/Succeed &  & 2/3 & 3/2 & 0/5 \\  & Speed-up (on BT) & 2843x & \(\) & \(\) & \(-\) \\   Task & Dataset & ecos & mosek & scs & hqreg & ReHLine \\  RidgeHuber & LD (\(\)1e-4) & ✗ & ✗ & ✗ & 4.90 (\( 0.00\)) & 1.40 (\( 0.20\)) \\  & Kin8nm (\(\)1e-3) & ✗ & ✗ & ✗ & 1.58 (\( 0.21\)) & 2.04 (\( 0.30\)) \\  & House-8L (\(\)1e-3) & ✗ & 925 (\( 2\)) & ✗ & 2.42 (\( 0.34\)) & 0.80 (\( 0.21\)) \\  & Topo-2-1 (\(\)1e-2) & 2620 (\( 1040\)) & 267 (\( 1\)) & 213 (\( 2\)) & 3.53 (\( 0.67\)) & 1.78 (\( 0.32\))Conclusion

In this paper, we present a new algorithm ReHLine designed to efficiently solve general regularized ERM with the convex PLQ loss function and optional linear constraints. Through the transformation of a PLQ loss into a composite ReLU-ReHU function, we have developed a novel CD algorithm that updates primal and dual variables simultaneously. This approach has proven to be highly effective in reducing the computational complexity while at the same time achieving a fast linear convergence rate. Based on our experiments, ReHLine demonstrates remarkable flexibility and computational efficiency, outperforming existing generic and specialized methods in a range of problems.

Limitation.Although our approach has exhibited substantial versatility in addressing a diverse range of problems, it is important to acknowledge that it may not be universally applicable to all convex optimization problems. The main limit of our approach stems from the restriction imposed by the formulation (1). For instance, (1) requires a convex PLQ loss function and a strict \(l_{2}\)-regularization. We recognize this limitation and intend to address it in future research pursuits. Moreover, we are interested in exploiting additional loss function properties, such as symmetry.

## 7 Acknowledgements

Ben Dai's work was supported in part by HK GRF-24302422 and GRF-14304823. Yixuan Qiu's work was supported in part by National Natural Science Foundation of China (12101389), Shanghai Pujiang Program (21PJC056), MOE Project of Key Research Institute of Humanities and Social Sciences (22JJD110001), and Shanghai Research Center for Data Science and Decision Technology. The authors confirm contribution to the paper as follows. The main algorithm: Ben Dai and Yixuan Qiu; theory and proofs: Ben Dai; C++ implementation: Yixuan Qiu; Python interface: Ben Dai and Yixuan Qiu; R interface: Yixuan Qiu; manuscript preparation: Ben Dai and Yixuan Qiu, equally.