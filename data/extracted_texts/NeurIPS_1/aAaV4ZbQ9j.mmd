# Navigating Chemical Space with Latent Flows

Guanghao Wei

Cornell University

gw338@cornell.edu &Yining Huang

Harvard University

Yiinguhuang@hms.harvard.edu &Chenru Duan

Deep Principle, Inc.

duanchenru@gmail.com &Yue Song

California Institute of Technology

yuesong@caltech.edu &Yuanqi Du

Cornell University

yd392@cornell.edu

Equal Contribution.Equal Supervision.This work was completed while the author was at Cornell University.

###### Abstract

Recent progress of deep generative models in the vision and language domain has stimulated significant interest in more structured data generation such as molecules. However, beyond generating new random molecules, efficient exploration and a comprehensive understanding of the vast chemical space are of great importance to molecular science and applications in drug design and materials discovery. In this paper, we propose a new framework, ChemFlow, to traverse chemical space through navigating the latent space learned by molecule generative models through flows. We introduce a dynamical system perspective that formulates the problem as learning a vector field that transports the mass of the molecular distribution to the region with desired molecular properties or structure diversity. Under this framework, we unify previous approaches on molecule latent space traversal and optimization and propose alternative competing methods incorporating different physical priors. We validate the efficacy of ChemFlow on molecule manipulation and single- and multi-objective molecule optimization tasks under both supervised and unsupervised molecular discovery settings. Codes and demos are publicly available on GitHub at [https://github.com/garywei944/ChemFlow](https://github.com/garywei944/ChemFlow).

## 1 Introduction

Designing new functional molecules has been a long-standing challenge in molecular discovery which concerns a wide range of applications in drug design and materials discovery . With the increasing interest in applying deep learning models in scientific problems , molecular design has attracted considerable attention given its massively available data and accessible evaluations. Among the developed methods, two paradigms emerge: one paradigm searches for new molecules based on combinatorial optimization approaches respecting the discrete nature of molecules; the other paradigm builds upon the success of deep generative models in approximating the molecular distribution with a given dataset and then generating new molecules from the learned models . Both of the approaches have demonstrated promising results in small molecule, protein, and materials design . Despite the promise, the chemical space is tremendously large with the number of drug-like small molecule compounds estimated to be from \(10^{23}\) to \(10^{60}\). This necessitates either more efficient searching methods or better understanding about the structure of the chemical space. Following the progress made in studying the latent structure of deep generative models (_e.g._ generative adversarial networks (GANs) , variational autoencoders (VAEs) ,and denoising diffusion models ) in computer vision [28; 5; 22; 38], decent efforts have recently been made in understanding the learned latent space of molecule generative models.

Initially, disentangled representation learning becomes a popular paradigm to enforce a structured and interpretable representation . Specifically, each latent dimension is expected to learn a disentangled factor of variation, and tweak the latent vector along the dimension could lead to generating new samples with changes only in one molecular property. However, even if imposing such constraints in the training of molecule generative models, the models still struggle to learn meaningful disentangled factors in the early attempts . In addition to constraining the model training procedure, exploring the structure of pre-trained molecule generative models is more efficient. The main approach developed is to utilize optimization approaches to discover the region in the latent space with the desired molecular property. It often trains a proxy function to map from the latent vector to the property, providing access to gradients for gradient-based optimization [40; 20; 13]. The third line of work also builds upon pre-trained models as well. It leverages one interesting finding such that the learned latent space of molecule generative models is linearly separable , which is also widely studied and used as a priori in computer vision [51; 50]. ChemSpace  develops a highly efficient approach to use linear classifiers to identify the separation boundary and considers the normal direction to the boundary as the direction of control. Nevertheless, the linear separability assumption may be too strong. It is worth noting that the first line of work does not require labels and can be trained in an unsupervised manner (referred to as unsupervised discovery), while both the second and third lines of work require access to labels to train/identify a guidance model/direction (referred to as supervised discovery).

In this paper, we propose a new framework, _ChemFlow_, based on flows in a dynamical system to efficiently explore the latent structure of molecule generative models. Specifically, we unify previous approaches (gradient-based optimization, linear latent traversal, and disentangled traversal) under the realm of flows that transforms data density along time via a vector field. In contrast to previous linear models, our framework is flexible to learn nonlinear transformations inspired by partial differential equations (PDEs) governing real-world physical systems such as heat and wave equations. We then analyze how different dynamics may bring special properties to solve different tasks. Our framework can also generalize both supervised and unsupervised settings under the same umbrella. Particularly in the under-studied unsupervised setting, we demonstrate a structure diversity potential can be incorporated to find trajectories that maximize the structure change of the molecules (which in turn leads to property change). We conduct extensive experiments with physicochemical, drug-related properties, and protein-ligand binding affinities on both molecule manipulation and (single- and multi-objective) molecule optimization experiments. The experiment results demonstrate the generality of the proposed framework and the effectiveness of alternative methods under this framework to achieve better or comparable results with existing approaches.

## 2 Background

### Navigating Latent Space of Molecules

The latent space \(\) of molecule generative models is often learned through an encoder function \(f_{}()\) and a decoder function \(g_{}()\) such that the encoder maps the input molecular structures \(\) into an (often) low-dimensional and continuous space (_i.e._ latent space) while the decoder maps the latent vectors \(\) back to molecular structures \(x^{}\). Note that this encoder-decoder architecture is general and can be realized by popular generative models such as VAEs, flow-based models, GANs, and diffusion models [31; 43; 6; 58; 66]. For simplicity, we focus on VAE-based methods in this paper. To traverse the learned latent space of molecule generative models, two approaches have been proposed: gradient-based optimization and latent traversal.

The gradient-based optimization methods first learn a proxy function \(h()\) parameterized by a neural network that provides the direction to traverse . This can be formulated as a gradient flow following the direction of steepest descent of the potential energy function \(h()\) and discretized, as follows:

\[_{t} =-_{z}h(_{t})t \] \[_{t} =_{t-1}-_{z}h(_{t-1})t\]

where we take a dynamic system perspective on the evolution of latent samples. The latent traversal approaches leverage the observation of linear separability in the learned latent space of molecule generative models . Since the direction is assumed to be linear, it can be found easily. ChemSpace  learns a linear classifier that defines the separation boundary of the molecular properties. Then the normal direction of the boundary provides a linear direction \(\) for traversing the latent space:

\[_{t}=_{0}+t \]

We notice that the above gradient flow and linear traversal can be analyzed and designed from a dynamical system perspective: linear traversal can be considered as a special case of wave functions, _i.e._, we have \(_{t}}}{{^{2}_{t-1}}}=_{t}}}{{^{2}t}}=0\) satisfied by wave functions. This connection inspires us to consider designing more dynamical traversal approaches in the latent space.

### Wasserstein Gradient Flows

Gradient flows define the curve \((t)^{n}\) that evolves in the negative gradient direction of a function \(:^{n}\). The time evolution of the gradient flow is given by the ODE \(^{}(t)=-((t))\). Wasserstein gradient flows describe a special type of gradient flow where \(\) is set to be the Wasserstein distance. For example, as introduced in Benamou and Brenier , the commonly used \(L^{2}\) Wasserstein metric induces a dynamic formulation of optimal transport:

\[W_{2}(,)^{2}=_{v,}(t,)|v(t, )|^{2}\,dt\,d:_{t}(t,)=-(v(t,) (t,))} \]

where \(,\) are two probability measures at the source and target distributions, respectively. Interestingly, if we take the gradient of a potential energy \(\) as the velocity field applied to a distribution, the time evolution of \(\) can be seen to minimize the Wasserstein distance and thus follow optimal transport. In Appendix A, we give detailed derivations of how the vector fields minimize the \(L^{2}\) Wasserstein distance and discuss alternative PDEs of the density evolution recovered by Wasserstein gradient flow (_e.g._ Wasserstein gradient flow over the entropy functional recovers heat equation) following the seminal JKO scheme .

## 3 Methodology

We present _ChemFlow_ as a unified framework for latent traversals in chemical latent space as latent flows. We parameterize a set of scalar-valued energy functions \(^{k}=_{^{k}}(t,)\) and use the

Figure 1: _ChemFlow_ framework: (1) a pre-trained encoder \(f_{}()\) and decoder \(g_{}()\) that maps between molecules \(\) and latent vectors \(\), (2) we use a property predictor \(h_{}()\) (green box) or a “Jacobian control” (yellow box) as the guidance to learn a vector field \(_{z}^{k}(t,_{t})\) that maximizes the change in certain molecular properties (e.g. plogP, QED) or molecular structures, (3) during the training process, we add additional dynamical regularization on the flow. The learned flows move the latent samples to change the structures and properties of the molecules smoothly. (Better seen in color). The flow chart illustrates a case where a molecule is manipulated into a drug like caffeine.

learned flow \(_{}^{k}\) to traverse the latent samples. The traversal process can be described by the following equation in a Lagrangian way (particle trajectory):

\[_{t}=_{t-1}+_{}^{k}(t-1,_{t-1}) \]

Alternatively, as an Eulerian approach, we can write the time evolution of the density through a pushforward map:

\[_{t}=[_{t}]_{*}_{t-1} \]

where \(_{t}\) defines the time-dependent flow that transforms the densities of latent samples through a probability path. The pushforward measure \([_{t}]_{*}\) induces a change of variable formula for densities :

\[[_{t}]_{*}_{t-1}()=_{t-1}(_{t}^{-1}())| ^{-1}()}{}| \]

In the following, we will introduce how \(_{}^{k}\) is matched to some pre-defined velocities for generating different flows.

### Learning Different Latent Flows

Given a pre-trained molecule generative model \(g_{}:\) with prior distribution \(p()\), we would like to model \(K\) different semantically disentangled latent trajectories that correspond to different properties of the molecules, numbered by superscript \(k\).

**Hamilton-Jacobi Flows.** One desired property for the latent traversal comes from optimal transport theory such that the transport cost is minimized (i.e. shortest path). This property can be enforced by solving Eq. (3) by Karush-Kuhn-Tucker (KKT) conditions, which will give the optimal solution -- the Hamilton-Jacobi Equation (HJE):

\[^{k}(t,)+||_{} ^{k}(t,)||^{2}=0 \]

where the velocity field is defined as the flow \(^{k}\). The HJE can also be interpreted as mass transportation in fluid dynamics, _i.e._, under the velocity field \(^{k}\), the fluid will evolve to the target distribution with an optimal transportation cost.

We achieve the HJE constraint by matching our flow fields and define the boundary condition as:

\[_{r}=_{t=0}^{T-1} ^{k}(t,)+||_{}^{k}(t,)||^{2} ^{2},\ _{}=_{k=0}^{K-1}||_{}^{k}(0,_{0})|| _{2}^{2} \]

where \(T\) represents the total number of traversal steps, \(_{r}\) restricts the energy to obey our physical constraints, and \(_{}\) restricts \((t,_{t})\) to match the initial condition. Our latent traversal can be thus regarded as dynamic optimal transport between distributions of molecules with different properties.

**Wave Flows.** Alternatively, we can pivot the optimal transport property to enforce additional physical and dynamical priors. For example, if we specify the flow to follow wave-like dynamics, we can use the second-order wave equation:

\[}{ t^{2}}^{k}(t,)-c^{2}_{}^{2} ^{k}(t,)=0 \]

The above constraint empirically produces highly diverse and realistic trajectories. Our velocity matching objective and boundary condition then become:

\[_{r}=_{t=0}^{T-1}||}{ t^{2 }}^{k}(t,)-c^{2}_{}^{2}^{k}(t,)||_{2}^{2},\ _{}=_{k=0}K-1||_{}^{k}(0,_{0})||_{2}^{2} \]

where \(_{r}\) and \(_{}\) restrict the physical constraints and the initial condition, respectively. Note that \(^{k} 0\) is a trivial optimal solution for the above two objectives regarding that both \(_{r}\) and \(_{}\) are non-negative. To prevent the parameterized \(^{k}\) from converging to such a trivial solution, we introduce more guidance to the loss function in Section 3.2 separately.

**Alternative Flows.** Besides HJE and wave equations, our framework is also general to include other commonly used PDEs that allow for different dynamics along the flow, such as Fokker Planck equation and heat equation. In the experimental section, we will explore the effectiveness of each latent flow in different supervision settings.

### Supervised & Unsupervised Guidance

**Supervised Semantic Guidance.** When an explicit semantic potential energy function or labeled data for the semantic of interest is available, we can use the provided semantic potential to guide the learning of the flow. Firstly, we train a surrogate model \(h_{}:\) (parameterized by a deep neural network) to predict the corresponding molecular property. Then we use the trained surrogate model as guidance to learn flows that drive the increase of the property for the trajectory of the generated molecules.

\[d=-_{}h_{}(g_{}(_{t})),_{}^{k }(t,_{t}),\;_{}=-(d)\|d \|_{2}^{2} \]

The intuition behind this objective is to learn the vector field \(_{t}\) such that it aligns with the direction of the steepest descent (negative gradient) of the objective function. Note that the sign of the dot product matters as it determines minimizing or maximizing the property.

The proposed objective function in the supervised scenario is

\[=_{r}+_{}+_{}\]

**Unsupervised Diversity Guidance.** When no explicit potential energy function is provided to learn the flow, we need to define a potential energy function that captures the change of molecular properties. As molecular properties are determined by the structures, we devise a potential energy that maximizes the continuous structure change of the generated molecules. Inspired by Song et al. , we couple the traversal direction with the Jacobian of the generator to maximize the traversal variations in the molecular space. The perturbation on latent samples can be approximated by the first-order Taylor approximation:

\[g(_{t}+_{}^{k}(t,_{t}))=g(_{t})+ _{t})}{_{t}}_{}^{ k}(t,_{t})+R_{1}(g(_{t})) \]

where \(\) denotes perturbation strength, and \(R_{1}()\) is the high-order terms. In the unsupervised setting, for sufficiently small \(\), if the Jacobian-vector product can cause large variations in the generated sample, the direction is likely to correspond to certain properties of molecules. We therefore introduce such a Jacobian-vector product guidance:

\[_{}=-\|_{t})}{ _{t}}_{}^{k}(t,_{t})\|_{2}^{2} \]

Compared to the supervised setting which maximizes the change of the molecular properties, it aims to find the direction that causes the maximal change of the structures. This can in turn effectively pushes the initial data distribution to the target one concentrated on the maximum property value. The Jacobian guidance will compete with the dynamical regularization (e.g. wave-like form) on the flow to yield smooth and meaningful traversal paths.

**Disentanglement Regularization.** While the above formulation can encourage smooth dynamics and meaningful output variations, the flows are likely to mine identical directions which all correspond to the maximum Jacobian change. To avoid such a trivial solution, we adopt an auxiliary classifier \(l_{}\) following Song et al.  to predict the flow index and use the cross-entropy loss to optimize it:

\[_{k}=_{CE}(l_{}(g_{}(_{t});g_{}(_{t+1})),k) \]

Where \(_{t}=g(_{t})\) is the generated sample from timestep \(t\). We see the extra classifier guidance would encourage each flow to be independent and find distinct properties. For each target property, we compute the Pearson correlation coefficient using a randomly generated test set. This coefficient measures the correlation between the property and a natural sequence (from 1 to time step \(t\)) along the optimization trajectory. We then select the energy network that achieves the highest correlation score for optimizing molecules with that specific property.

The proposed objective function in the unsupervised scenario is

\[=_{r}+_{}+_{}+ _{k}\]

### Connection with Langevin Dynamics for Global Optimization

In scenarios where our flow adheres to the dynamics of the Fokker-Planck equation, our approach may also be interpreted as employing a learned potential energy function to simulate Langevin Dynamicsfor global optimization . Notably, the convergence of Langevin dynamics, particularly at low temperatures, tends to occur around the global minima of the potential energy function . The continuous and discretized Langevin dynamics are as follows:

\[_{t}&=-_{z}h_{ }(_{t})t+_{t}\\ _{t}&=_{t-1}-_{z}h_{}(_{ t-1})t+t}(0,I) \]

**Proposition 3.1**.: _(Global Convergence of Langevin Dynamics, adapted from Gelfand and Mitter ). Given a Langevin dynamics in the form of_

\[_{t}=_{t-1}-a_{t}(_{z}h_{}(_{t-1})+_{t})+b_{t }_{t}\]

_where \(_{t}\) is a \(d\)-dimensional Brownian motion, \(a_{t}\) and \(b_{t}\) are a set of positive numbers with \(a_{T},b_{T} 0\), and \(_{t}\) is a set of random variables in \(^{n}\) denoting noisy measurements of the energy function \(h_{}()\). Under mild assumptions, \(_{t}\) converges to the set of global minima of \(h_{}()\) in probability._

Following Section3.1, the learned latent flow can be used to search for molecules with optimal properties and it converges to the global minimizers of the learned latent potential energy function.

## 4 Experiments

### Experiment Set-up

Datasets & Molecular properties.We extract 4,253,577 molecules from the three commonly used datasets for drug discovery including **MOSES**, **ZINC250K**, and **ChEMBL**. Molecules are represented by SELFIES strings . All input molecules are padded to the maximum length in the dataset before fitting into the generative model. We consider a total of 8 molecular properties which include _3 general drug-related properties_ -- Quantitative Estimate of Drug-likeness (QED), Synthesis Accessibility (SA), and penalized Octanol-water Partition Coefficient (plogP) and _3 machine learning-based target activities_ -- DRD2, JNK3 and GSK3B , _2 simulation-based target activities_ -- docking scores for two human proteins ESR1 and ACAA1. See SectionD.3 for details.

Implementations.We establish our framework by pre-training a VAE model that learns a latent space of molecules that can generate new molecules by decoding latent vectors from the latent space. We adapt the framework in Eckmann et al.  which is a basic VAE architecture with molecular SELFIES string representations and an additional MLP model as the surrogate property predictor. See SectionD.5 for all implementation and hyper-parameter details.

Model variants.As discussed in Section3.1, our proposed framework is general to incorporate different dynamical priors to learn the flow. For the experiments, we consider four types of dynamics including _gradient flow (GF)_, _Wave flow (Wave, eq. (9))_, _Hamilton Jacobi flow (HJ, eq. (7))_ and _Langevin Dynamics or equivalently Fokker Planck flow (LD, eq. (15))_.

For the specific molecular properties and evaluations, the readers are kindly referred to AppendixD for details. We also move qualitative evaluations to AppendixF due to space limit.

### Molecule Optimization

Molecule optimization is key in drug design and materials discovery, aiming to identify molecules with optimal properties . Various machine learning methods have accelerated this process . Our discussion focuses on optimization within the latent space of generative models, primarily using gradient-based optimization as outlined in Section2.1. We categorize molecule optimization into three scenarios: (1) unconstrained optimization to identify molecules with the best properties, (2) constrained optimization to find molecules with the best-expected property and similar to specific structures--a common step in the lead optimization process, and (3) multi-objective optimization to simultaneously enhance multiple properties of a molecule.

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

Among the quantitative results, it is interesting that the random direction achieves a good relaxed success rate for some properties, we argue this is because of the specific property of the learned latent space. The latent space learned by generative models tend to be smooth such that similar molecular structures are often mapped to close areas in the latent space. In Appendix E, we find that some molecular properties are highly correlated with their latent vector norms, in which a random direction always increases the norm and thus successfully manipulates a portion of molecules by chance.

## 5 Conclusion, Limitation and Future Work

In this paper, we propose a unified framework for navigating chemical space through the learned latent space of molecule generative models. Specifically, we formulate the traversal process as a flow that defines a vector to transport the mass of molecular distribution through time to desired concentrations (e.g. high properties). Two forces (supervised potential guidance and unsupervised structure diversity guidance) are derived to drive the dynamics. We also propose a variety of new physical PDEs on the dynamics which exhibit different properties. We hope this general framework can open up a new research avenue to study the structure and dynamics of the latent space of molecule generative models.

**Limitation and future work.** This work is a preliminary study on small molecules and it may be interesting to see it transfer to larger molecular systems or more specialized systems and properties. Beyond molecules, this approach has the potential to be extended to languages and other data modalities.

## 6 Acknowledgement

Y.D. would like to thank Ziming Liu and Kirill Neklyudov for helpful discussions.