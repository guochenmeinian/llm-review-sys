# Regularizing Hidden States Enables Learning

Generalizable Reward Model for LLMs

Rui Yang\({}^{1}\)  Ruomeng Ding\({}^{2}\)  Yong Lin\({}^{3}\)  Huan Zhang\({}^{1}\)  Tong Zhang\({}^{1}\)

\({}^{1}\)University of Illinois Urbana-Champaign, \({}^{2}\)Georgia Institute of Technology,

\({}^{3}\)Princeton University, \({}^{4}\)Princeton Language and Intelligence

yangrui.thu2015@gmail.com, rmding@gatech.edu, yl7690@princeton.edu

huan@huan-zhang.com, tongzhang@tongzhang-ml.org

###### Abstract

Reward models trained on human preference data have been proven to effectively align Large Language Models (LLMs) with human intent within the framework of reinforcement learning from human feedback (RLHF). However, current reward models have limited generalization capabilities to unseen prompts and responses, which can lead to an unexpected phenomenon known as reward over-optimization, resulting in a decline in actual performance due to excessive optimization of rewards. While previous research has advocated for constraining policy optimization, our study introduces a novel approach to enhance the reward model's generalization ability against distribution shifts by regularizing the hidden states. Specifically, we retain the base model's language model head and incorporate a suite of text-generation losses to preserve the hidden states' text-generation capabilities, while concurrently learning a reward head behind the same hidden states. Our experimental results demonstrate that the introduced regularization technique markedly improves the accuracy of learned reward models across a variety of out-of-distribution (OOD) tasks and effectively alleviates the over-optimization issue in RLHF, offering a more reliable and robust preference learning paradigm 1.

## 1 Introduction

Pretrained large models have showcased impressive capabilities across diverse fields . A notable trend in recent research is ensuring that large models align with human values and mitigate potentially harmful behaviors . Alignment methods are crucial in achieving this objective, with two primary approaches being supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) . SFT directly finetunes the model using prompt and response pairs, proving to be a straightforward and efficient alignment technique . Differently, RLHF begins by learning a reward model from user preferences and then employs reinforcement learning to optimize the language model to maximize rewards. A significant advantage of RLHF is its potential to generalize the reward model to unseen prompt-response pairs, effectively leveraging large volumes of unlabeled data .

Despite the empirical success of RLHF, the challenge of training a reliable and generalizable reward model for unseen data remains an open problem. A well-known failure mode of reward model is known as _"overoptimization"_ or _"reward hacking"_, where policy optimization seemingly improves the proxy reward model but actually degrades the true reward function.  demonstrated in a synthetic setup that increasing the size of the reward model and the volume of training data can mitigate this overoptimization issue. However, such scaling is not always feasiblein many realistic scenarios. To address this, a series of studies have been conducted, focusing either on enhancing the reward model with ensemble techniques [18; 19; 20; 21] or on constrained policy optimization [22; 23; 24; 25]. The latter paradigm is related to the offline RL literature [26; 27; 28; 29; 30; 31], which involves limiting the policy distribution to be close to the training data distribution. Among these, improving the generalization ability of reward models presents a fundamental and promising direction that can be studied independently from enhancements in policy optimization. Nevertheless, previous methods [18; 32] requiring training multiple reward models may be resource-intensive for the practical application of large models.

In this study, we present a lightweight yet effective solution designed to enhance the reward model's generalization ability against distribution shifts. Previous research  has theoretically shown that a randomly initialized head can distort pre-trained features, thereby negatively impacting out-of-distribution (OOD) performance. Inspired by this finding, we propose to regularize the feature during fine-tuning for preference learning using an adversarial regularizer, which derives a suite of text-generation losses. To this end, we introduce **Generalizable Reward Model (GRM)**, which retains the base model's language model head and regularizes the hidden states of the reward model by incorporating text-generation losses. This approach makes better use of the preference learning data while preserving the text generation capabilities of the hidden states. Notably, GRM does not necessitate training multiple reward models or relying on additional training data.

In our experiments, GRM substantially improves the evaluation accuracy of the reward model OOD evaluation datasets, demonstrating its superior ability to generalize learned preferences to unseen prompt and response pairs. Moreover, GRM consistently improves the performance of both 2B and 7B reward models, with a more pronounced improvement observed when the data size is limited. We also demonstrate that GRM can markedly enhance the performance of best-of-\(n\) (BoN) sampling and PPO , effectively mitigating the overoptimization problem. These results highlight the potential of the GRM to serve as a more reliable proxy reward model for human preferences.

To conclude, our primary contributions are as follows:

* We propose GRM, a novel approach that employs text-generation regularization on the hidden states to enhance the generalization ability of reward models.
* Our study validates the effectiveness of all three types of text-generation regularization for GRM, identifying the SFT regularization as the most effective and stable solution.
* Our empirical results show that GRM significantly improves the accuracy of reward models across various OOD tasks. Furthermore, it consistently enhances the performance of RLHF, effectively alleviating the overoptimization problem.

## 2 Background

Typically, reinforcement Learning from Human Feedback (RLHF) involves reward modeling and policy optimization, with Best-of-\(n\) Sampling (BoN) and Proximal Policy Optimization (PPO) being two commonly used methods for policy optimization.

Figure 1: (1) Illustration of GRM. Given preference data pairs \((x,y_{c},y_{r})\), the reward head \(r_{}\) minimizes the reward loss in Eq 1, while the language model (LM) head \(_{_{}}\) minimizes a suite of text-generation losses introduced in Sec 3.2. (2) Performance of GRM and the vanilla reward model on in-distribution (ID) task (Unified-Feedback) and average results of OOD tasks (HHH-Alignment and MT-Bench). Compared with the baseline reward model, GRM generalizes better on OOD tasks, with a larger advantage when the dataset size is relatively small.

Reward Modeling.Generally, reward modeling is based on the Bradley-Terry model , which aims to distinguish between the chosen response \(y_{c}\) and the rejected response \(y_{r}\) given the prompt \(x\):

\[_{}()=-_{(x,y_{c},y_{r}) D}[ ((r_{}(x,y_{c})-r_{}(x,y_{r}))) ], \]

where \(r_{}(x,y)\) represents the reward score for prompt \(x\) and output \(y\) with model parameters \(\). \(()\) is the sigmoid function. By minimizing this loss function, the reward model assigns higher scores to outputs preferred by humans. Subsequently, the trained reward model can be used to guide the optimization of the language model.

**Best-of-\(n\) Sampling (BoN).** BoN generates \(n\) samples from the policy model, denoted as \(Y_{}\), and then selects the best one based on scores provided by a reward model. BoN can be used for inference-time improvement or iterative optimization [36; 37; 38].

\[y_{}(x)=_{y Y_{}}r_{}(x,y). \]

**Proximal Policy Optimization (PPO).** PPO is a widely adopted method for RLHF in optimizing language models [16; 8; 39]. PPO learns a policy by maximizing a reward objective with a KL divergence penalty with coefficient \(\):

\[r_{}=r_{}(x,y)-(_{}(y|x) _{}(y|x)), \]

where the KL penalty ensures that the optimized policy does not deviate significantly from the SFT policy to maintain the reliability of the reward model.

**Overoptimization.** Although the learned proxy reward model aims to approximate human preference, it may not consistently reflect authentic human preferences, potentially resulting in _over-optimization_[17; 18]. This issue emerges when the proxy reward model becomes overly optimized, causing the policy model to overfit certain erroneous patterns. Ultimately, this issue can diminish the model's alignment with actual human preferences, highlighting the need to ensure the reward model's robustness and reliability.

## 3 Method

In the common practice of training a reward model [8; 39; 40], reward models are initialized using a pretrained or SFT finetuned backbone, along with a randomly initialized reward head to predict the scores for prompt-response pairs. It's important to note that the backbone and original language model head are trained on a diverse range of datasets for text generation, which is distinct from the preference learning tasks. Under the task shift, the randomly initialized reward head can distort the pretrained features, thereby reducing the OOD generalization performance, as observed by . We also confirm this impact on preference learning in Appendix C.1.

To improve the reward model's generalization capability against distribution shifts, we propose a lightweight yet effective solution, Generalizable Reward Model (GRM). This model employs a suite of text-generation regularizations for the hidden states. More specifically, GRM employs a structure as illustrated in Fig 1, with one language model (LM) head and one reward head sharing the same hidden states. The reward head is trained to minimize the reward loss \(_{}\) in Eq 1, while the LM head is trained to maintain the text-generation ability of the hidden states during preference learning. Consequently, we define the overall loss function as follows:

\[_{}=(1-)_{}+ _{}. \]

Here, \(\) is the coefficient that balances the reward loss and the regularization. We will derive potential forms of the regularization term below.

### Theoretical Motivation

To derive the potential formulation of the regularization term, we consider the following adversarial optimization problem: learning a reward model against an adversarial policy.

\[=_{}\{_{}()+_ {}J(,)\}, \]

where \(>0\) is a coefficient. This objective is also considered by recent studies [24; 25] aiming to enhance DPO. Differently, we adopt it to learn a generalizable reward model.

The insight of Eq 5 is that we can enhance the robustness of the reward model by considering an adversarial policy \(\) from a certain policy class. The term for policy optimization \(J(,)\) can have various formulations, but a KL divergence-regularized objective is generally used in training the policy [16; 8]. Moreover, it has an advantageous property that the inner optimization problem has an analytical solution, which can simplify the problem.

\[J(,)=_{x D,y(|x)}[r_{}(x,y )]-_{x D}[((|x) _{}(|x))], \]

where \(>0\) is a regularization coefficient and \(_{}\) is the reference model. We denote the analytical solution of \(J(,)\) as \(_{}^{*}\). Incorporating \(_{}^{*}\) into Eq 5, we can transform the min-max optimization problem into a standard optimization problem under certain assumptions:

\[=_{}\{(1-)_{}()+ _{}_{}(_{}^{*})+_{}_{}(_{}^{*})\} \]

Detailed derivation is deferred to Appendix A. Here, \(_{}\) is the same as the DPO objective  and \(_{}\) is the SFT objective that maximizes the probability of chosen responses. Notably, the two regularization terms originate from different sources: \(_{}\) stems from the reward loss, while \(_{}\) is derived from the adversarial term. This may explain why SFT regularization proves more beneficial than DPO regularization in our empirical results. Motivated by Eq 7, we relax the relationship between \(r_{}\) and \(_{}^{*}\) and propose learning a reward model parameterized by \(\) and a language model head parameterized by \(_{}\), both sharing the same hidden states. A discussion of this design can be found in Appendix A. Below, we detail three practical implementations.

### Text-Generation Regularization

Inspired by Eq 7, we train the LM head to minimize text-generation losses, such as DPO and SFT losses, as the regularization term for GRM. To independently study the effectiveness of these two regularizations and reduce GPU memory usage, we introduce three practical implementations: DPO regularization, DPO without reference regularization, and SFT regularization.

**DPO Regularization.** By setting \(_{}=\) and \(_{}=0\) in Eq 7, we can directly adopt the DPO loss as a regularization term for GRM to regularize the hidden states:

\[_{}(_{})=-_{(x,y_{c},y_{r})  D}[((}}( y_{c} x)}{_{}(y_{c} x)})-(}}(y_{r} x)}{_{}(y_{r} x)}) )], \]

where \(_{}\) is the base model serving as the reference model, and \(_{_{}}\) is our optimized policy. \(\) is a coefficient that controls the KL penalty between \(_{_{}}\) and \(_{}\). Notably, \(_{_{}}\) shares the same base model with the reward model \(r_{}\), except for the output layer.

**DPO Regularization w/o Reference Model.** While straightforward, the use of a reference model in DPO regularization can be memory-intensive for large models. To address this, and inspired by prior works that eliminate the need for reference model [42; 43], we introduce the DPO regularization without a reference model, denoted as \(_{}\). This method reduces the need for large GPU memory during training. The loss function \(_{}\) is defined as:

\[_{}(_{})=-_{(x,y_{c},y_ {r}) D}[((}}(y_{c} x)}{_{_{}}(y_{r} x)}))]. \]

**SFT Regularization.** By setting \(_{}=0\) and \(_{}=\) in Eq 7, we can simplify the regularization term to SFT regularization, thereby reducing the computational cost. This method only maximizes the probability of the chosen responses:

\[_{}(_{})=-_{(x,y_{c}) D} [((_{_{}}(y_{c} x) ))]. \]

This equation differs slightly from the standard SFT objective to maintain coherence with the above two cases within the regularization suite and avoid the need for hyperparameter adjustments for \(\). Please refer to Appendix C.3 for a discussion.

### Advantages of GRM

In summary, GRM offers three key advantages: **(1) Mitigating feature distortion.** The application of text-generation loss helps maintain the text-generation ability of the base model and preventsexcessive feature distortion. Simultaneously, it also adapts the model to the data distribution of preference learning. **(2) Prevention of Overfitting.** The text-generation regularization derived from an adversarial training objective helps prevent the reward model from overfitting to certain spurious features, which can be detrimental to OOD generalization. This effect becomes more pronounced when the preference data includes erroneous comparison pairs or when the dataset size is limited. **(3) Efficiency.** GRM is an efficient solution that does not require training multiple reward models or additional training data. Additionally, different choices of loss type entail varying memory and computational costs. Interestingly, we find that the simplest option, SFT regularization, proves to be the most stable choice.

## 4 Experimental Setup

**Datasets.** For training reward models, we leverage the Unified-Feedback dataset 2, which stands as one of the largest collections of pairwise feedback datasets. In Section 5.1, we train all reward models on a subset of 400K and 40K samples from the Unified-Feedback dataset and evaluate them on the hold-out 8K eval set. In addition, for evaluating model performance on out-of-distribution (OOD) preference data, we utilize datasets such as HHH-Alignment 3, MT-Bench Human Judgements 4, and RewardBench . The HHH-Alignment dataset evaluates language models on helpfulness, honesty, and harmlessness, while the MT-Bench dataset contains human preferences for model responses to MT-bench questions. Besides, RewardBench is a new benchmark designed to evaluate the capabilities and safety of reward models. We consider HHH-Alignment, MT-Bench, and RewardBench as OOD evaluation tasks because the prompt and response distributions differ from our training distribution. For the RLHF experiments in Section 5.2 we downsample 20K data from Unified-Feedback for training reward models and optimizing the PPO policy, and another 1K data for evaluating BoN or the learned PPO policy.

**Base Models.** In the preference learning experiments, our base models include gamma-2B-it  and Mistral-7B-Instruct-v0.2 . For the RLHF experiments, gamma-2B-it serves as the policy model for both BoN and PPO experiments, whereas the gold reward model 5 is a 7B human preference model finetuned using the entire Unified-Feedback dataset.

**Baselines.** We compare the performance of GRM with several baselines, including _Baseline Classifier_ trained using the original reward loss in Eq 1; _Frozen Classifier_ that fixes the base model's feature and only finetunes a nonlinear classification head; _Margin_ that adds an additional margin in the original reward loss ; _Label Smooth_ that mitigate the overfitting problem by penalizing overconfident model outputs ; _Ensemble_ method with a group of 3 reward models  to calculate the average or minimum values as rewards. In addition, for RewardBench, we present the performance of several existing open-source state-of-the-art reward models for better reference, including PairRM , Starling-RM-7B/34B , and UltraRM-13B . For more experimental details and additional results, please refer to Appendix B and Appendix C, respectively.

## 5 Evaluation Results

We present a comprehensive evaluation of GRM, utilizing both in-distribution (ID) and out-of-distribution (OOD) datasets, as well as existing benchmarks for reward models. Furthermore, we explore the impact of GRM on the overoptimization issue in RLHF. Our primary findings can be summarized as follows:

* GRM significantly enhances the generalization capability of reward models, resulting in substantial improvements on both ID and various OOD evaluation sets (Section 5.1).
* All three types of text-generation regularization losses can improve the generalization performance, with the SFT regularization being the most effective and stable (Section 5.1).
* GRM demonstrates robustness in the limited dataset setting, outperforming baselines by an even larger margin (Section 5.1).

* GRM effectively mitigates the overoptimization issue in both BoN and PPO (Section 5.2).
* GRM also exhibits robustness against label noise in the preference dataset (Section 5.2).

### Evaluation on Reward Modeling

ID and OOD Evaluation.The results, shown in Table 1 and Table 2, illustrate the evaluation performance of different reward modeling methods using the gamma-2B-it base model on both ID (Unified-Feedback) and OOD (HHH-Alignment and MT-Bench) datasets. Regardless of the size of the training data (400K or 40K), our proposed method, GRM, with three types of regularizations, consistently outperforms the baseline models on both the ID evaluation set and the two OOD datasets. For instance, GRM w/ sft with 400K training data enhances the baseline from 72.1 to 73.2 in ID score, and improves the HHH-Alignment score from 73.4 to 79.8 and the MT-Bench score from 71.2 to 73.4. Notably, the improvement in OOD performance is significantly larger than that in ID. These results suggest that the GRM methods are highly effective in evaluating unseen preference data, demonstrating substantially robust generalization capabilities.

Regarding other baseline models, the Frozen classifier, which maintains its base model's parameters, exhibits the lowest ID and OOD scores. This suggests that the pretrained features of the base model alone are insufficient for effective preference learning, emphasizing the importance of fine-tuning the base model's features to the preference task. Furthermore, the margin loss and label smoothing techniques do not consistently improve the ID and OOD tasks, whereas the ensemble baseline consistently enhances both ID and OOD scores. Despite requiring the training of multiple reward models, ensemble-based methods still do not surpass GRM, particularly when learning from a 40K training set. These results highlight the substantial improvement and generalization capability of GRM in preference learning.

Comparison of Different Regularizations.As observed in Table 1, when the training dataset is sufficiently large, GRM with three types of regularizations (namely GRM w/ dpo, GRM w/ dpo-nonef, and GRM w/ sft) perform comparably. This demonstrates that GRM is robust to the choice of regularization type when the dataset is large. However, in Table 2, where the training data is limited to 40K, a clear trend emerges: GRM w/ sft outperforms GRM w/ dpo-nonef, which in turn outperforms GRM w/ dpo, on both the ID and OOD scores. Interestingly, the simplest form of regularization, SFT regularization, not only requires the lowest training cost but also yields the most stable overall results. Consequently, we adopt it as the default choice for our subsequent study.

Results on RewardBench.In Table 3 and Table 4, we evaluate GRM and various baselines on RewardBench across chat, chat-hard, safety, and reasoning task groups. We consider a variant of GRM with a linear reward head instead of the default nonlinear reward head as detailed in Appendix B. In Table 3, the 7B baseline matches the score of Starling-RM-7B , while GRM (linear) w/ sft demonstrates a considerable improvement, increasing the average score from 76.3 to 79.5. Comparing variants of GRM, we can conclude that: (1) SFT regularization performs better than the DPO w/o reference model regularization, and (2) GRM with a linear head achieves a better overall score than that with a nonlinear head, especially in the challenging reasoning task group.

   Reward Model &  Unified \\ Feedback \\  &  HHH \\ Alignment \\  & 
 MT \\ Bench \\  \\  Classifier (Frozen) & 63.8 & 66.4 & 69.5 \\ Classifier (baseline) & 72.1 & 73.4 & 71.2 \\ Classifier + margin & 72.0 & 75.0 & 72.6 \\ Classifier + label smooth & 71.5 & 72.1 & 71.2 \\ Classifier + Ensemble & 72.8 & 76.8 & **73.7** \\
**GRM w/ dpo (ours)** & 73.8 & 79.2 & 73.4 \\
**GRM w/ dpo-nonef (ours)** & **73.9** & 79.7 & 73.0 \\
**GRM w/ sft (ours)** & 73.2 & **79.8** & 73.4 \\   

Table 1: Results on ID and OOD evaluation with **400K training data** from Unified-Feedback. The best performance in each task is in bold and the second best one is underlined.

   Reward Model &  Unified \\ Feedback \\  &  HHH \\ Alignment \\  & 
 MT \\ Bench \\  \\  Classifier (Frozen) & 63.9 & 68.6 & 68.2 \\ Classifier (baseline) & 68.8 & 70.3 & 69.1 \\ Classifier + margin & 69.6 & 69.8 & 71.0 \\ Classifier + label smooth & 68.5 & 68.8 & 71.9 \\ Classifier + Ensemble & 69.9 & 72.2 & 71.1 \\
**GRM w/ dpo (ours)** & 70.2 & 71.6 & 71.3 \\
**GRM w/ dpo-nonef (ours)** & 71.4 & 76.6 & 72.1 \\
**GRM w/ sft (ours)** & **71.5** & **78.7** & **73.0** \\   

Table 2: Results on ID and OOD evaluation with **40K training data** from Unified-Feedback. The best performance in each task is in bold and the second best one is underlined.

Regarding the baselines, consistent with previous results, the margin loss and label smoothing do not provide a coherent improvement over the baseline. While ensemble methods effectively improve upon the baseline, they still underperform GRM. Overall, these results demonstrate that GRM is a strong contender in reward modeling tasks, exhibiting superior performance across various benchmarks.

Comparison of Different Dataset Sizes.Another noteworthy observation is that GRM exhibits greater robustness to the size of the training dataset compared to baselines. For instance, in Table 1 and Table 2, when the training data size decreases from 400K to 40K, the baseline's HHH Alignment score and MT-Bench score drop from 73.4 and 71.2 to 70.3 and 69.1, respectively. In contrast, GRM with SFT regularization only slightly drops from 79.8 and 73.4 to 78.7 and 73.0, respectively. This

  
**Reward model** & **Average** & **Chat** & **Chat-Hard** & **Safety** & **Reasoning** \\   \\  Classifier (baseline) & 64.5 & 95.8 & 37.3 & 59.9 & 64.8 \\ Classifier + margin & 66.1 & **97.2** & 37.5 & 56.8 & 72.7 \\ Classifier + label smooth & 61.1 & 91.6 & 39.0 & 53.8 & 60.2 \\ Classifier + Ensemble & 65.2 & 96.1 & 38.2 & 58.8 & 67.6 \\ GRM (linear) w/ dpo noref (ours) & 61.7 & 94.7 & 38.4 & 62.5 & 51.2 \\ GRM (linear) w/ sft (ours) & **69.5** & 94.7 & 40.8 & 65.4 & **77.0** \\ GRM w/ dpo noref (ours) & 66.6 & 92.5 & 39.9 & **72.5** & 61.4 \\ GRM w/ sft (ours) & 66.8 & 94.1 & **41.9** & 69.5 & 61.5 \\    \\  Classifier (baseline) & 68.2 & 89.7 & 50.7 & 74.7 & 57.9 \\ Classifier + margin & 62.8 & 89.7 & 47.1 & 70.7 & 43.6 \\ Classifier + label smooth & 72.1 & 94.1 & 47.1 & 67.5 & 79.7 \\ Classifier + Ensemble & 69.3 & 89.6 & 50.2 & 72.7 & 59.0 \\ GRM (linear) w/ dpo noref (ours) & 77.8 & 96.9 & 52.9 & 82.7 & 78.8 \\ GRM (linear) w/ sft (ours) & 78.3 & 96.7 & 52.4 & 81.5 & **82.5** \\ GRM w/ dpo noref (ours) & **78.6** & **97.8** & **54.6** & 82.0 & 79.9 \\ GRM w/ sft (ours) & 78.4 & 97.2 & 54.2 & **83.6** & 78.6 \\   

Table 4: Results on RewardBench with **40K training data** from Unified-Feedback.

  
**Reward model** & **Average** & **Chat** & **Chat-Hard** & **Safety** & **Reasoning** \\  PairRM & 58.7 & 90.2 & 53.0 & 31.5 & 60.0 \\ Starling-RM-7B & 76.2 & 98.0 & 43.4 & 88.6 & 74.6 \\ Starling-RM-34B & **84.0** & 96.9 & 59.0 & 89.9 & 90.3 \\ UltraRM-13B & 69.8 & 96.1 & 55.3 & 45.8 & 82.0 \\   \\  Classifier (baseline) & 68.2 & 95.5 & 38.0 & 73.8 & 65.3 \\ Classifier + margin & 70.2 & 95.8 & 38.4 & 73.9 & 72.5 \\ Classifier + label smooth & 70.6 & 94.4 & 37.3 & 73.2 & **77.4** \\ Classifier + Ensemble & 71.0 & **98.0** & 37.5 & 77.3 & 71.3 \\ GRM (linear) w/ dpo noref (ours) & 70.2 & 96.7 & 39.0 & 76.4 & 68.5 \\ GRM (linear) w/ sft (ours) & **71.5** & 96.1 & 40.1 & **80.3** & 69.3 \\ GRM w/ dpo noref (ours) & 70.2 & 95.8 & 40.1 & 78.7 & 66.2 \\ GRM w/ sft (ours) & 70.8 & 97.8 & **42.1** & 77.9 & 65.2 \\   \\  Classifier (baseline) & 76.3 & 96.6 & 52.4 & 86.7 & 69.5 \\ Classifier + margin & 74.5 & 96.4 & 51.5 & 85.3 & 64.8 \\ Classifier + label smooth & 76.3 & 97.2 & 49.8 & 85.8 & 72.3 \\ Classifier + Ensemble & 76.6 & 96.6 & 51.8 & 85.1 & 73.0 \\ GRM (linear) w/ dpo noref (ours) & 78.3 & **98.0** & 53.3 & **86.4** & 75.3 \\ GRM (linear) w/ sft (ours) & **79.5** & 97.8 & 54.6 & 86.3 & **79.2** \\ GRM w/ dpo noref (ours) & 78.0 & 97.8 & 54.0 & 85.7 & 74.4 \\ GRM w/ sft (ours) & 77.6 & **98.0** & **55.3** & 85.8 & 71.2 \\   

Table 3: Results on RewardBench with **400K training data** from Unified-Feedback.

trend is consistent in Table 3 and Table 4. Specifically, for the Mistral 7B Instruct base model, the baseline's average score drops from 76.3 to 68.2 when learning from 40K training data, while GRM (linear) w/ sft only drops from 79.5 to 78.3. These findings suggest that the prior reward training paradigms are sensitive to smaller dataset sizes. In contrast, GRM is robust even with a limited dataset.

Full Parameter Training Results on a Larger Dataset.To further demonstrate the effectiveness of GRM, we trained the GRM using the llama3-8b-instruct model . We perform a full parameter fine-tuning for 1 epoch on one of the largest open-source preference datasets 6. Our results, presented in Table 5, highlight the considerable potential of scaling GRM to larger models and datasets. Especially, GRM outperforms a 34B reward model and even GPT-4 as a judge. It is worth noting that the GRM significantly improves the performance of the 8B reward model from 84.7 to 87.0, using the same base model and training data as FsfairX-LLaMA3-RM-v0.1 . This improvement is particularly remarkable in the challenging 'Reasoning' section.

### Evaluation on RLHF

Best-of-\(n\) Sampling (BoN).Fig 2 presents the results of BoN sampling for base models of sizes 2B and 7B. For all BoN experiments, we utilize a 20K subset from the Unified-Feedback dataset, labeled by the gold reward model, to train proxy reward models. Following the [17; 18], we conduct BoN sampling on a 1K held-out test set from \(n\) responses for each prompt, based on the scores of the trained proxy model. The selected responses are then scored using the gold reward model, and their gold scores are averaged over the 1K test prompts. The average gold score reveals the true quality of the responses selected by the proxy reward models. We set the KL Divergence from 0 to 5, corresponding to the number of responses \(n\) ranging from 1 to 405 for each prompt, according to the equation \(_{}= n-\). Ideally, a good proxy reward model should yield larger average proxy and gold scores as the KL increases. However, the average gold scores of some baseline methods plateau or even drop after \(>4\), such as the baseline reward model in Fig 2(d), despite their proxy scores continuing to increase in Fig 2(c). This suggests that these reward models suffer from the overoptimization issue.

  
**Reward model** & **Average** & **Chat** & **Chat-Hard** & **Safety** & **Reasoning** \\  GRM (Ours, 8B) & **87.0** & 98.6 & 67.8 & **89.4** & **92.3** \\ gpt-4-0125-preview & 85.9 & 95.3 & 74.3 & 87.2 & 86.9 \\ gpt-4-turbo-2024-04-09 & 85.1 & 95.3 & **75.4** & 87.1 & 82.7 \\ FsfairX-LLaMA3-RM-8B & 84.7 & **99.4** & 65.1 & 87.8 & 86.4 \\ Starling-RM-34B & 82.7 & 96.9 & 57.2 & 88.2 & 88.5 \\   

Table 5: Results of full parameter training on RewardBench.

Figure 2: Proxy scores and gold scores of BoN experiments for base models of (a)(b) gamma-2b-it and (c)(d) Mistral-7B-Instruct. Proxy and gold scores are in dashed and solid curves, respectively. Rewards are normalized to start from 0. GRM demonstrates a robust ability to select the best response aligned with the gold rewards as the KL Divergence increases.

In contrast, GRM consistently demonstrates an increase in both the proxy score and gold score, indicating that it effectively mitigates over-optimization. This advantage is more pronounced in the 7B base model, where GRM achieves an average gold score of 1.5, while the baseline reward model only reaches a score of 0.5. Regarding other baselines, we find that the margin loss and ensemble methods (especially the'min' strategy) contribute to the robustness of the reward model. However, they still do not compare favorably with GRM. These results underscore the strong potential of GRM to serve as a reliable and robust proxy reward model for RLHF.

Proximal Policy Optimization (PPO).To investigate whether GRM can effectively mitigate the overoptimization issue in PPO, we further employ those 2B and 7B reward models obtained from the BoN experiments to fine-tune the policy model (gamma-2b-it) using PPO. The training and evaluation datasets are identical to the BoN experiments. We train PPO for one epoch on the training set, comprising 20K training samples. As depicted in Fig 3, PPO exhibits a stronger tendency to lack the learned reward models compared to BoN. The gold scores of baseline methods begin to decline early in the training process, while their proxy scores increase, indicating a clear overoptimization issue. In contrast, GRM demonstrates superior robustness in terms of the gold score, which rises with the increase in proxy scores. This validates that GRM can effectively alleviate overoptimization for PPO. Please refer to Appendix D for a clear comparison of the results generated by PPO.

Robustness to Label Noise.Human preference data typically contains around \(20\) to \(30\%\) noise, as highlighted in previous studies . Such inconsistent preference data can render the reward model less generalizable  and hinder policy learning , leading to a performance decline. To evaluate the robustness of GRM against label noise, we incorporate a \(25\%\) label noise into the 20K training data for all proxy reward models. The results are depicted in Fig 4. Most gold scores expose a more severe over-optimization issue, as compared to the results in Fig 2(b) and Fig 3(b), indicating that those reward models are heavily overfitting under the noisy label setting. On the contrary, GRM exhibits superior robustness under noisy conditions, consistently achieving a peak gold score over 1.0 without a significant decline. This demonstrates that GRM is highly accurate and robust at measuring the sample quality, even in the presence of noise within the training data.

Figure 4: Proxy scores and gold scores of (a)(b) BoN experiments and (c)(d) PPO experiments with \(25\%\) **label noise**. All rewards are normalized to start from 0.

Figure 3: Proxy scores and gold scores of PPO experiments for reward model based on (a)(b) gamma-2b-it and (c)(d) Mistral-7B-Instruct. All rewards are normalized to start from 0.

Related Works

**Reward Modeling.** Reward models, trained on human preference data, are crucial in guiding RLHF  or prompt optimization . Recent studies have concentrated on developing advanced reward models to improve the performance of LLMs in RLHF. One approach involves enhancing reward modeling by improving the quality or quantity of preference data . Other strategies focus on learning token-wise dense rewards  or multi-objective rewards . Additionally, a series of works aim to enhance the robustness of reward models against preference inconsistencies. Techniques such as adaptive margin , contrastive learning , and meta-learning  are employed to improve the model's ability to differentiate between chosen and rejected responses.

**Mitigating Overoptimization in RLHF.** Reward models tend to overfit and struggle to generalize beyond the training distribution, which often leads to the overoptimization issue . One approach to mitigate this is to penalize overly confident model outputs using label smoothing  or SFT regularization . Alternatively, the model and data can be iteratively updated, replacing hard labels with soft labels . Ensemble techniques, which train several reward models, can also help reduce reward hacking and manage shifts in distribution . Adversarial Preference Optimization employs adversarial learning between reward models and an LLM agent to address the gap in generation distribution . Recent studies have also utilized uncertainty to mitigate reward over-optimization, including the integration of an uncertainty penalty into rewards , or the construction of a confidence interval for gold rewards based on uncertainty estimations .

## 7 Conclusion

In this study, we introduce an efficient approach aimed at enhancing the generalizability and robustness of reward learning for large language models. By incorporating regularization techniques on the hidden states of reward models, our method demonstrates substantial improvements in the generalization performance of reward models for unseen data. Moreover, our approach effectively mitigates the issue of overoptimization in RLHF. We believe that our findings hold promise in inspiring future research efforts towards the development of more robust reward models that can facilitate the alignment of large models through cost-effective solutions.

## Limitations

In this study, we evaluate the robustness of GRM against label noise by introducing a 25% level of synthetic noise into the training data for all proxy reward models. This is achieved by randomly flipping chosen and rejected labels. Due to cost considerations, we conduct synthetic experiments in line with community practices , as using human-labeled data is not feasible for us. However, synthetic data may introduce biases that don't accurately reflect real-world scenarios. Future research should aim to mitigate this limitation by incorporating experiments with human-labeled data, providing a more thorough evaluation of the reward model's robustness. Another limitation of our study is the computational restriction preventing us from testing GRM with parameter sizes exceeding 10B. Further efforts to extend our method to larger reward models could be highly promising.