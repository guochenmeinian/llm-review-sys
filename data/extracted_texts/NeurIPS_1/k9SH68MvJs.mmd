# Diffusion-Reward Adversarial Imitation Learning

Chun-Mao Lai\({}^{1}\)  Hsiang-Chun Wang\({}^{1}\) Ping-Chun Hsieh\({}^{2}\) Yu-Chiang Frank Wang\({}^{1,3}\)

Min-Hung Chen\({}^{3}\)  Shao-Hua Sun\({}^{1}\)

\({}^{1}\)National Taiwan University \({}^{2}\)National Yang Ming Chiao Tung University \({}^{3}\)NVIDIA

Equal contribution. Correspondence to: Shao-Hua Sun <shaohuas@ntu.edu.tw>

###### Abstract

Imitation learning aims to learn a policy from observing expert demonstrations without access to reward signals from environments. Generative adversarial imitation learning (GAIL) formulates imitation learning as adversarial learning, employing a generator policy learning to imitate expert behaviors and discriminator learning to distinguish the expert demonstrations from agent trajectories. Despite its encouraging results, GAIL training is often brittle and unstable. Inspired by the recent dominance of diffusion models in generative modeling, we propose Diffusion-Reward Adversarial Imitation Learning (DRAIL), which integrates a diffusion model into GAIL, aiming to yield more robust and smoother rewards for policy learning. Specifically, we propose a diffusion discriminative classifier to construct an enhanced discriminator, and design diffusion rewards based on the classifier's output for policy learning. Extensive experiments are conducted in navigation, manipulation, and locomotion, verifying DRAIL's effectiveness compared to prior imitation learning methods. Moreover, additional experimental results demonstrate the generalizability and data efficiency of DRAIL. Visualized learned reward functions of GAIL and DRAIL suggest that DRAIL can produce more robust and smoother rewards. Project page: [https://nturobotlearninglab.github.io/DRAIL/](https://nturobotlearninglab.github.io/DRAIL/)

## 1 Introduction

Imitation learning, _i.e_., learning from demonstration [24; 41; 49], aims to acquire an agent policy by observing and mimicking the behavior demonstrated in expert demonstrations. Various imitation learning methods [53; 60] have enabled deploying reliable and robust learned policies in a variety of tasks involving sequential decision-making, especially in the scenarios where devising a reward function is intricate or uncertain [7; 32; 34], or when learning in a trial-and-error manner is expensive or unsafe [14; 17].

Among various methods in imitation learning, generative adversarial imitation learning (GAIL)  has been widely adopted due to its effectiveness and data efficiency. GAIL learns a generator policy to imitate expert behaviors through reinforcement learning and a discriminator to differentiate between the expert and the generator's state-action pair distributions, resembling the idea of generative adversarial networks (GANs) . Despite its established theoretical guarantee, GAIL training is notoriously brittle and unstable. To alleviate this issue, significant efforts have been put into improving GAIL's sample efficiency, scalability, robustness, and generalizability by modifying loss functions , developing improved policy learning algorithms , and exploring various similarity measures of distributions [2; 8; 12].

Inspired by the recent dominance of diffusion models in generative modeling , this work explores incorporating diffusion models into GAIL to provide more robust and smoother reward functionsfor policy learning as well as stabilize adversarial training. Specifically, we propose a diffusion discriminative classifier, which learns to classify a state-action pair into expert demonstrations or agent trajectories with merely two reverse diffusion steps. Then, we leverage the proposed diffusion discriminative classifier to devise diffusion rewards, which reward agent behaviors that closely align with expert demonstrations. Putting them together, we present Diffusion-Reward Adversarial Imitation Learning (DRAIL), a novel adversarial imitation learning framework that can efficiently and effectively produce reliable policies replicating the behaviors of experts.

We extensively compare our proposed framework DRAIL with behavioral cloning , Diffusion Policy [6; 42], and AIL methods, _e.g._, GAIL , WAIL , and DiffAIL, in diverse continuous control domains, including navigation, robot arm manipulation, locomotion, and games. This collection of tasks includes environments with high-dimensional continuous state and action spaces, as well as covers both vectorized and image-based states. The experimental results show that our proposed framework consistently outperforms the prior methods or achieves competitive performance. Moreover, DRAIL exhibits superior performance in generalizing to states or goals unseen from the expert's demonstrations. When varying the amounts of available expert data, DRAIL demonstrates the best data efficiency. At last, the visualized learned reward functions show that DRAIL captures more robust and smoother rewards compared to GAIL.

## 2 Related work

Imitation learning enables agents to learn from expert demonstrations to acquire complex behaviors without explicit reward functions. Its application spans various domains, including robotics [49; 60], autonomous driving , and game AI .

**Behavioral Cloning (BC).** BC [44; 55] imitates an expert policy through supervised learning without interaction with environments and is widely used for its simplicity and effectiveness across various domains. Despite its benefits, BC struggles to generalize to states not covered in expert demonstrations because of compounding error [10; 48]. Recent methods have explored learning diffusion models as policies [6; 42], allowing for modeling multimodal expert behaviors, or using diffusion models to provide learning signals to enhance the generalizability of BC . In contrast, this work aims to leverage a diffusion model to provide learning signals for policy learning in online imitation learning.

**Inverse Reinforcement Learning (IRL).** IRL methods  aim at inferring a reward function that could best explain the demonstrated behavior and subsequently learn a policy using the inferred reward function. Nevertheless, inferring reward functions is an ill-posed problem since different reward functions could induce the same demonstrated behavior. Hence, IRL methods often impose constraints on reward functions or policies to ensure optimality and uniqueness [1; 37; 54; 61]. Yet, these constraints could potentially restrict the generalizability of learned policies.

**Adversarial Imitation Learning (AIL).** AIL methods aim to directly match the state-action distributions of an agent and an expert through adversarial training. Generative adversarial imitation learning (GAIL)  and its extensions [23; 25; 31; 56; 59; 62] train a generator policy to imitate expert behaviors and a discriminator to differentiate between the expert and the generator's state-action pair distributions, which resembles the idea of generative adversarial networks (GANs) . Thanks to its simplicity and effectiveness, GAIL has been widely applied to various domains [3; 28; 45]. Over the past years, researchers have proposed numerous improvements to enhance GAIL's sample efficiency, scalability, and robustness , including modifications to discriminator's loss function , extensions to off-policy RL algorithms , addressing reward bias , and exploration of various similarity measures [2; 8; 11; 12]. Another line of work avoids adversarial training, such as IQ-Learn , which learns a Q-function that implicitly represents the reward function and policy. In this work, we propose to use the diffusion model as a discriminator in GAIL.

**Diffusion Model-Based Approaches in Reinforcement Learning.** Diffuser  and Nuti et al.  apply diffusion models to reinforcement learning (RL) and reward learning, their settings differ significantly from ours. Diffuser  is a model-based RL method that requires trajectory-level reward information, which differs from our setting, _i.e._, imitation learning, where obtaining rewards is not possible. Nuti et al.  focus on learning a reward function, unlike imitation learning, whose goal is to obtain a policy. Hence, Nuti et al.  neither present policy learning results in the main paper nor compare their method to imitation learning methods. Moreover, they focus on learning from a fixed suboptimal dataset, AIL approaches and our method are designed to learn from agent data that continually change as the agents learn.

## 3 Preliminaries

We propose a novel adversarial imitation learning framework that integrates a diffusion model into generative adversarial imitation learning. Hence, this section presents background on the two topics.

### Generative Adversarial Imitation Learning (GAIL)

GAIL  establishes a connection between generative adversarial networks (GANs)  and imitation learning. GAIL employs a generator, \(G_{}\), that acts as a policy \(_{}\), mapping a state to an action. The generator aims to produce a state-action distribution (\(_{_{}}\)) which closely resembles the expert state-action distribution \(_{_{E}}\); discriminator \(D_{}\) functions as a binary classifier, attempting to differentiate the state-action distribution of the generator (\(_{_{}}\)) from the expert's (\(_{_{E}}\)). The optimization equation of GAIL can be formulated using the Jensen-Shannon divergence, which is equivalent to the minimax equation of GAN. The optimization of GAIL can be derived as follows:

\[_{}_{}_{_{_{}}}[ D _{}()]+_{_{_{E}}}[(1-D_{ }())], \]

where \(_{_{}}\) and \(_{_{E}}\) are the state-action distribution from an agent \(_{}\) and expert policy \(_{E}\) respectively. The loss function for the discriminator is stated as \(-(_{_{_{}}}[ D_{}()] +_{_{_{E}}}[(1-D_{}())])\). For a given state, the generator tries to take expert-like action; the discriminator takes state-action pairs as input and computes the probability of the input originating from an expert. Then the generator uses a reward function \(-_{_{_{}}}[ D_{}()]\) or \(-_{_{_{}}}[ D_{}()]+  H(_{})\) to optimize its network parameters, where the entropy term \(H\) is a policy regularizer controlled by \( 0\).

### Diffusion models

Diffusion models have demonstrated state-of-the-art performance on various tasks [9; 29; 38; 51]. This work builds upon denoising diffusion probabilistic models (DDPMs)  that employ forward and reverse diffusion processes, as illustrated in Figure 1. The forward diffusion process injects noise into data points following a variance schedule until achieving an isotropic Gaussian distribution. The reverse diffusion process trains a diffusion model \(\) to predict the injected noise by optimizing the objective:

\[_{}=_{t T,} [\|_{}(_{t}}_{0}+ {1-_{t}},t)-\|^{2}], \]

where \(T\) represents the set of discrete time steps in the diffusion process, \(\) is the noise applied by the forward process, \(_{}\) is the noise predicted by the diffusion model, and \(_{t}\) is the scheduled noise level applied on the data samples.

Beyond generative tasks, diffusion models have also been successfully applied in other areas, including image classification and imitation learning. Diffusion Classifier  demonstrates that conditional diffusion models can estimate class-conditional densities for zero-shot classification. In imitation learning, diffusion models have been used to improve Behavioral Cloning (DBC)  by using diffusion model to model expert state-action pairs. Similarly, DiffAIL  extends GAIL  by employing diffusion models to represent the expert's behavior and incorporating the diffusion loss into the discriminator's learning process. However, DiffAIL's use of an unconditional diffusion model limits its ability to distinguish between expert and agent state-action pairs. We provide a detailed explanation of its limitation in Section 4.3 and Section A.

Figure 1: **Denoising diffusion probabilistic model.** Latent variables \(x_{1},...,x_{N}\) are produced from the data point \(x_{0}\) via a forward diffusion process, _i.e._, gradually adding noises to the latent variables. A diffusion model \(\) learns to reverse the diffusion process by denoising the noisy data to reconstruct the original data point \(x_{0}\).

## 4 Approach

We propose a novel adversarial imitation learning framework incorporating diffusion models into the generative adversarial imitation learning (GAIL) framework, illustrated in Figure 2. Specifically, we employ a diffusion model to construct an enhanced discriminator to provide more robust and smoother rewards for policy learning. We initiate our discussion by describing a naive integration of the diffusion model, which directly predicts rewards from Gaussian noises conditioned on state-action pairs, and the inherent issues of this method in Section 4.1. Subsequently, in Section 4.2, we introduce our proposed method that employs a conditional diffusion model to construct a diffusion discriminative classifier, which can provide diffusion rewards for policy learning. Finally, the overall algorithm of our method is outlined in Section 4.3.

### Reward prediction with a conditional diffusion model

Conditional diffusion models are widely adopted in various domains, _e.g._, generating an image \(x\) from a label \(y\). Intuitively, one can incorporate a conditional diffusion model as a GAIL discriminator by training it to produce a real or fake label conditioned on expert or agent state-action pairs. Specifically, given a denoising time step \(t\) and a state-action pair \((,)()\), where \(,\) stand for state and action spaces, respectively, as a condition, the diffusion model \(p_{}(r_{t-1}|r_{t},,)\) learns to denoise a reward label \(r_{0}\{0,1\}\), _i.e._, \(1\) for expert (real) state-action pairs and \(0\) for agent (fake) state-action pairs through a reverse diffusion process.

To train a policy, we can use the diffusion model to produce a reward \(r\) given a state-action pair \((,)\) from the policy through a generation process by iteratively denoising a sampled Gaussian noise, _i.e._, noisy reward, conditioned on the state-action pair. Then, the policy learns to optimize the rewards predicted by the diffusion model. Nevertheless, the reward generation process is extremely time-consuming since predicting a reward for each state-action pair from the policy requires running \(T\) (often a large number) denoising steps, and policy learning often takes tens of millions of samples, resulting in a billion-level overall training scale. Consequently, it is impractical to integrate a diffusion model into the GAIL framework by using it to predict "realness" rewards for policy learning from state-action pairs.

### Diffusion discriminative classifier

Our goal is to yield a diffusion model reward given an agent state-action pair without going through the entire diffusion generation process. Inspired by previous work [5; 59], we extract the learning signal from a portion of the diffusion denoising steps, rather than using the entire process. Building

Figure 2: **Diffusion-Reward Adversarial Imitation Learning. Our proposed framework DRAIL incorporates a diffusion model into GAIL. (a) Our proposed diffusion discriminative classifier \(D_{}\) learns to distinguish expert data \((_{E},_{E})_{E}\) from agent data \((_{},_{})_{i}\) using a diffusion model. \(D_{}\) is trained to predict a value closer to \(1\) when the input state-action pairs are sampled from expert demonstration and predict a value closer to \(0\) otherwise. (b) The policy \(_{}\) learns to maximize the diffusion reward \(r_{}\) computed based on the output of \(D_{}\) that takes the state-action pairs from the policy as input. The closer the policy resembles expert behaviors, the higher the rewards it can obtain.**on these insights, we adapt the training procedure of DDPM to develop a mechanism that provides a binary classification signal using just one denoising step.

Our key insight is to leverage the derivations developed by Kingma et al. , Song et al. , which suggest that the diffusion loss, _i.e._, the difference between the predicted noise and the injected noise, indicates how well the data fits the target distribution since the diffusion loss is the upper bound of the negative log-likelihood of data in the target distribution. In this work, we propose calculating "realness" rewards based on the diffusion loss computed by denoising the state-action pairs from the policy, which indicates how well the state-action pairs fit the expert behavior distributions. We formulate the diffusion loss \(_{}\) as follows:

\[_{}(,,)=_{t  T}[\|_{}(,,, t|)-\|^{2}], \]

where \(\{^{+},^{-}\}\), and the real label \(^{+}\) corresponds to the condition for fitting expert data while the fake label \(^{-}\) corresponds to agent data. We implement \(^{+}\) as \(\) and \(^{-}\) as \(\).

To approximate the expectation in Eq. 3, we use random sampling, allowing us to achieve the result with just a single denoising step. Subsequently, given a state-action pair \((,)\), \(_{}(,,^{+})\) measures how well \((,)\) fits the expert distribution and \(_{}(,,^{-})\) measures how well \((,)\) fits the agent distribution1. That said, given state-action pairs sampled from expert demonstration, \(_{}^{+}\) should be close to \(0\), and \(_{}^{-}\) should be a large value; on the contrary, given agent state-action pairs, \(_{}^{+}\) should be a large value and \(_{}^{-}\) should close to \(0\).

While \(_{}^{+}\) and \(_{}^{-}\) can indicate the "realness" or the "fakeness" of a state-action pair to some extent, optimizing a policy using rewards with this wide value range \([0,)\) can be difficult . To address this issue, we propose transforming this diffusion model into a binary classifier that provides "realness" in a bounded range of \(\). Specifically, given the diffusion model's output \(_{}^{+,-}\), we construct a diffusion discriminative classifier \(D_{}:\):

\[D_{}(,)=_{}(,,^{+})}}{e^{-_{}(, ,^{+})}+e^{-_{}(,,^{-})}}=(_{}(,, ^{-})-_{}(,,^{ +})), \]

where \((x)=1/(1+e^{-x})\) denotes the sigmoid function. The classifier integrates \(_{}^{+}\) and \(_{}^{-}\) to compute the "realness" of a state-action pair within a bounded range of \(\), as illustrated in Figure 2. Since the design of our diffusion discriminative classifier aligns with the GAIL discriminator , learning a policy with the classifier enjoys the same theoretical guarantee, _i.e._, optimizing this objective can bring a policy's occupancy measure closer to the expert's. Consequently, we can optimize our proposed diffusion discriminative classifier \(D_{}\) with the loss function:

\[_{D}=\ _{(,)_{K}}(,))]}_{_{BCE}^{}}+_{(,)_{i}}( ,))]}_{_{BCE}^{}} \]

where \(_{D}\) sums the expert binary cross-entropy loss \(_{BCE}^{}\) and the agent binary cross-entropy loss \(_{BCE}^{}\), and \(_{E}\) and \(_{i}\) represent a sampled expert trajectory and a collected agent trajectory by the policy \(\) at training step \(i\). We then update the diffusion discriminative classifier parameters \(\) based on the gradient of \(_{D}\) to improve its ability to distinguish expert data from agent data.

Intuitively, the discriminator \(D_{}\) is trained to predict a value closer to \(1\) when the input state-action pairs are sampled from expert demonstration (_i.e._, trained to minimize \(_{}^{+}\) and maximize \(_{}^{-}\)), and \(0\) if the input state-action pairs are obtained from the agent online interaction (_i.e._, trained to minimize \(_{}^{-}\) and maximize \(_{}^{+}\)).

Note that our idea of transforming the diffusion model into a classifier is closely related to Li et al. , which shows that minimizing the diffusion loss is equivalent to maximizing the evidence lower bound (ELBO) of the log-likelihood , allowing for turning a conditional text-to-image diffusion model into an image classifier by using the ELBO as an approximate class-conditional log-likelihood \( p(x|c)\). By contrast, we employ a diffusion model for imitation learning. Moreover, we take a step further - instead of optimizing the diffusion loss \(_{}\), we directly optimize the binary cross entropy losses calculated based on the denoising results to train the diffusion model as a binary classifier.

### Diffusion-Reward Adversarial Imitation Learning

Our proposed method adheres to the fundamental AIL framework, where a discriminator and a policy are updated alternately. In the discriminator step, we update the diffusion discriminative classifier with the gradient of \(_{D}\) following Eq. 5. In the policy step, we adopt the adversarial inverse reinforcement learning objective proposed by Fu et al.  as our diffusion reward signal to train the policy:

\[r_{}(,)=(D_{}(,))-(1- D_{}(,)). \]

The policy parameters \(\) can be updated using any RL algorithm to maximize the diffusion rewards provided by the diffusion discriminative classifier, bringing the policy closer to the expert policy. In our implementation, we utilize PPO as our policy update algorithm. The algorithm is presented in Algorithm 1, and the overall framework is illustrated in Figure 2.

Among the related works, DiffAIL  is the closest to ours, as it also uses a diffusion model for adversarial imitation learning. DiffAIL employs an unconditional diffusion model to denoise state-action pairs from both experts and agents. However, this approach only implicitly reflects the likelihood of state-action pairs belonging to the expert class through diffusion loss, making it challenging to explicitly distinguish between expert and agent behaviors.

In contrast, our method, DRAIL, uses a conditional diffusion model that directly conditions real (\(c^{+}\)) and fake (\(c^{-}\)) labels. This allows our model to explicitly calculate and compare the probabilities of state-action pairs belonging to either the expert or agent class. This clearer and more robust signal for binary classification aligns more closely with the objectives of the GAIL framework, leading to more stable and effective learning. For further details and the mathematical formulation, please refer to Section A.

```
1:Input: Expert trajectories \(_{E}\), initial policy parameters \(_{0}\), initial diffusion discriminator parameters \(_{0}\), and discriminator learning rate \(_{}\)
2:for\(i=0,1,2,\)do
3: Sample agent transitions \(_{i}_{_{i}}\)
4: Compute the output of diffusion discriminative classifier \(D_{}\) (Eq. 4) and the loss function \(_{D}\) (Eq. 5)
5: Update the diffusion model \(_{i+1}_{i}-_{}_{D}\)
6: Compute the diffusion reward \(r_{}(,)\) with Eq. 6
7: Update the policy \(_{i+1}_{i}\) with any RL algorithm w.r.t. reward \(r_{}\)
8:endfor
```

**Algorithm 1** Diffusion-Reward Adversarial Imitation Learning (DRAIL)

## 5 Experiments

We extensively evaluate our proposed framework DRAIL in diverse continuous control domains, including navigation, robot arm manipulation, and locomotion. We also examine the generalizability and data efficiency of DRAIL in Section 5.4 and Section 5.5. The reward function learned by DRAIL is presented in Section 5.6.

### Experimental setup

This section describes the environments, tasks, and expert demonstrations used for evaluation.

**Maze.** We evaluate our approach in the point mass Maze navigation environment, introduced in Fu et al.  (maze2d-medium-v2), as depicted in Figure 2(a). In this task, a point-mass agent is trained to navigate from a randomly determined start location to the goal. The agent accomplishes the task by iteratively predicting its acceleration in the vertical and horizontal directions. We use the expert dataset provided by Lee et al. , which includes 100 demonstrations, comprising 18,525 transitions.

**FetchPush.** We evaluate our approach in a 7-DoF Fetch task, FetchPush, depicted in Figure 2(b), where the Fetch is required to push a black block to a designated location marked by a red sphere. We use the demonstrations from Lee et al. , consisting of 20,311 transitions (664 trajectories).

**HandRotate.** We further evaluate our approach in a challenging environment HandRotate with a _high-dimensional continuous action space_ introduced by Plappert et al. . Here, a 24-DoF Shadow Dexterous Hand is tasked with learning to in-hand rotate a block to a target orientation, as depicted in Figure 2(c). This environment features a high-dimensional state space (68D) and action space (20D). We use the demonstrations collected by Lee et al. , which contain 515 trajectories (10k transitions).

**AntReach.** The goal of AntReach, a location and navigation task, is for a quadruped ant to reach a goal randomly positioned along the perimeter of a half-circle with a radius of \(5\) meters, as depicted in Figure 2(d). The 132D _high-dimensional continuous state space_ encodes joint angles, velocities, contact forces, and the goal position relative to the agent. We use the demonstrations provided by Lee et al. , which contain 1,000 demonstrations (25k transitions).

**Walker.** The objective of Walker is to let a bipedal agent move at the highest speed possible while preserving its balance, as illustrated in Figure 2(e). We trained a PPO expert policy with environment rewards and collected 5 successful trajectories, each containing 1000 transitions, as an expert dataset.

**CarRacing.** We evaluate our method in a racing game, CarRacing, illustrated in Figure 2(f), requiring driving a car to navigate a track. This task features a \(96 96\) RGB _image-based state space_ and a 3-dimensional action space (steering, braking, and accelerating). We trained a PPO expert policy on CarRacing environment and collected \(671\) transitions as expert demonstrations.

Further details of the tasks can be found in Section B.

### Baselines

We compare our method DRAIL with the following baselines of our approach.

* **Behavioral Cloning (BC)** trains a policy to mimic the actions of an expert by supervised learning a mapping from observed states to corresponding expert actions .
* **Diffusion Policy** represents a policy as a conditional diffusion model , which predicts an action conditioning on a state and a randomly sampled noise. We include this method to compare learning a diffusion model as a _policy_ (diffusion policy) or _reward function_ (ours).
* **Generative Adversarial Imitation Learning (GAIL)** learns a policy from expert demonstrations by training a discriminator to distinguish between trajectories generated by the learned generator policy and those from expert demonstrations.
* **Generative Adversarial Imitation Learning with Gradient Penalty (GAIL-GP)** is an extension of GAIL that introduces a gradient penalty to achieve smoother rewards and stabilize the discriminator.
* **Wasserstein Adversarial Imitation Learning (WAIL)** extends GAIL by employing Wasserstein distance, aiming to capture smoother reward functions.
* **Diffusion Adversarial Imitation Learning (DiffAIL)** integrates a diffusion model into AIL by using the diffusion model loss to provide reward \(e^{-_{}}\).

### Experimental results

We present the success rates (Maze, FetchPush, HandRotate, AntReach) and return (Walker, CarRacing) of all the methods with regards to environment steps in Figure 4. Each task

Figure 3: **Environments & tasks. (a) Maze: A point-mass agent (green) within a 2D maze is trained to move from its initial position to reach the goal (red). (b) FetchPush: The manipulation task is implemented with a 7-DoF Fetch robotics arm. FetchPush requires picking up or pushing an object to a target location (red). (c) HandRotate: For this dexterous manipulation task, a Shadow Dexterous Hand is employed to in-hand rotate a block to achieve a target orientation. (d) AntReach: This task trains a quadruped ant to reach a goal randomly positioned along the perimeter of a half-circle with a radius of 5 m. (e) Walker: This locomotion task requires training a bipedal walker policy to achieve the highest possible walking speed while maintaining balance. (f) CarRacing This image-based racing game task requires driving a car to navigate a track as quickly as possible.**was trained using five different random seeds. Note that BC and Diffusion Policy are offline imitation learning algorithms, meaning they cannot interact with the environment, so their performances are represented as horizontal lines. Detailed information on model architectures, training, and evaluation can be found in Section F and Section G.

Overall, our method DRAIL consistently outperforms prior methods or achieves competitive performance compared to the best-performing methods across all the environments, verifying the effectiveness of integrating our proposed diffusion discriminative classifier into the AIL framework.

**DRAIL vs. DiffAIL.** Both DRAIL and DiffAIL integrate the diffusion model into the AIL framework. In 5 out of 6 tasks, our DRAIL outperforms DiffAIL, demonstrating that our proposed discriminator provides a more effective learning signal by closely resembling binary classification within the GAIL framework.

**DRAIL vs. BC.** AIL methods generally surpass BC in most tasks due to their ability to learn from interactions with the environment and thus handle unseen states better. However, BC outperforms all other baselines in the locomotion task (Walker). We hypothesize that Walker is a monotonic task requiring less generalizability to unseen states, allowing BC to excel with sufficient expert data. Additionally, our experiments with varying amounts of expert data, detailed in Section 5.5, suggest that DRAIL surpasses BC when less expert data is available.

We empirically found that our proposed DRAIL is robust to hyperparameters, especially compared to GAIL and WAIL, as shown in the hyperparameter sensitivity experiment inSection D.

### Generalizability

To examine the generalizability to states or goals that are unseen from the expert demonstrations of different methods, we extend the FetchPush tasks following the setting proposed by Lee et al. . Specifically, we evaluate policies learned by different methods by varying the noise injected into initiate states (_e.g._, position and velocity of the robot arm) and goals (_e.g._, target block positions in FetchPush). We experiment with different noise levels, including \(1\), \(1.25\), \(1.5\), \(1.75\), and \(2.0\), compared to the expert environment. That said, \(1.5\) means the policy is evaluated in an environment with noises \(1.5\) larger than those injected into expert data collection. Performing well in a high noise level setup requires the policy to generalize to unseen states.

Figure 4: **Learning efficiency. We report success rates (Maze, FetchPush, HandRotate, AntReach) and return (Walker, CarRacing), evaluated over five random seeds. Our method DRAIL learns more stably, faster, and achieves higher or competitive performance compared to the best-performing baseline in all the tasks.**

The results of FetchPush under \(1\), \(1.25\), \(1.75\), and \(2.0\) noise level are presented in Figure 5. Across all noise levels, approaches utilizing the diffusion model generally exhibit better performance. Notably, our proposed DRAIL demonstrates the highest robustness towards noisy environments. Even at the highest noise level of \(2.00\), DRAIL maintains a success rate of over \(95\%\), surpassing the best-performing baseline, Diffusion Policy, which achieves only around a \(79.20\%\) success rate. In contrast, DiffAIL experiences failures in \(2\) out of the \(5\) seeds, resulting in a high standard deviation (mean: \(40.90\), standard deviation: \(47.59\)), despite our extensive efforts on experimenting with various configurations and a wide range of hyperparameter values.

The extended generalization experiments results in Maze, FetchPush, HandRotate, and the new task FetchPick are presented in Section C. Overall, our method outperforms or performs competitively against the best-performing baseline, demonstrating its superior generalization ability.

### Data efficiency

To investigate the data efficiency of DRAIL, we vary the amount of expert data used for learning in Walker and FetchPush. Specifically, for Walker, we use 5, 3, 2, and 1 expert trajectories, each containing 1000 transitions; for FetchPush, we use 20311, 10000, 5000, and 2000 state-action pairs. The results reported in Figure 6 demonstrate that our DRAIL learns faster compared to the other baselines, indicating superior data efficiency in terms of environment interaction. In Walker, our DRAIL maintains a return value of over 4500 even when trained with a single trajectory. In contrast, BC's performance is unstable and fluctuating, while the other baselines experience a dramatic drop. In FetchPush, our DRAIL maintains a success rate of over \(80\%\) even when the data size is reduced by \(90\%\), whereas the other AIL baselines' performance drops below \(50\%\).

Figure 5: **Generalization experiments in FetchPush. We present the performance of our proposed DRAIL and baselines in the FetchPush task, under varying levels of noise in initial states and goal locations. The evaluation spans three random seeds, and the training curve illustrates the success rate dynamics.**

Figure 6: **Data efficiency. We experiment learning with varying amounts of expert data in Walker and FetchPush. The results show that our proposed method DRAIL is more data efficient, _i.e._, can learn with less expert data, compared to other methods.**

### Reward function visualization

To visualize and analyze the learned reward functions, we design a Sine environment with one-dimensional state and action spaces, where the expert state-action pairs form a discontinuous sine wave \(=(20)+(0,0.05^{2})\), as shown in Figure 6(a). We train GAIL and our DRAIL to learn from this expert state-action distribution and visualize the discriminator output values \(D_{}\) to examine the learned reward function, as presented in Figure 7.

Figure 6(b) reveals that the GAIL discriminator exhibits excessive overfitting to the expert demonstration, resulting in its failure to provide appropriate reward values when encountering unseen states. In contrast, Figure 6(c) shows that our proposed DRAIL generalizes better to the broader state-action distribution, yielding a more robust reward value, thereby enhancing the generalizability of learned policies. Furthermore, the predicted reward value of DRAIL gradually decreases as the state-action pairs deviate farther from the expert demonstration. This reward smoothness can guide the policy even when it deviates from the expert policy. In contrast, the reward distribution from GAIL is relatively narrow outside the expert demonstration, making it challenging to properly guide the policy if the predicted action does not align with the expert.

## 6 Conclusion

This work proposes a novel adversarial imitation learning framework that integrates a diffusion model into generative adversarial imitation learning (GAIL). Specifically, we propose a diffusion discriminative classifier that employs a diffusion model to construct an enhanced discriminator, yielding more robust and smoother rewards. Then, we design diffusion rewards based on the classifier's output for policy learning. Extensive experiments in navigation, manipulation, locomotion, and game justify our proposed framework's effectiveness, generalizability, and data efficiency. Future work could apply DRAIL to image-based robotic tasks in real-world or simulated environments and explore its potential in various domains outside robotics, such as autonomous driving, to assess its generalizability and adaptability. Additionally, exploring other divergences and distance metrics, such as the Wasserstein distance or f-divergences, could potentially further improve training stability.