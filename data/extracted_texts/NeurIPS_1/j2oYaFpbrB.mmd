# Active Vision Reinforcement Learning under Limited Visual Observability

Jinghuan Shang Michael S. Ryoo

Department of Computer Science, Stony Brook University

{jishang, mryoo}@cs.stonybrook.edu

###### Abstract

In this work, we investigate Active Vision Reinforcement Learning (ActiveVision-RL), where an embodied agent simultaneously learns action policy for the task while also controlling its visual observations in partially observable environments. We denote the former as _motor policy_ and the latter as _sensory policy_. For example, humans solve real world tasks by hand manipulation (motor policy) together with eye movements (sensory policy). ActiveVision-RL poses challenges on coordinating two policies given their mutual influence. We propose SUGARL, Sensorimotor Understanding Guided Active Reinforcement Learning, a framework that models motor and sensory policies separately, but jointly learns them using with an intrinsic sensorimotor reward. This learnable reward is assigned by sensorimotor reward module, incentivizes the sensory policy to select observations that are optimal to infer its own motor action, inspired by the sensorimotor stage of humans. Through a series of experiments, we show the effectiveness of our method across a range of observability conditions and its adaptability to existed RL algorithms. The sensory policies learned through our method are observed to exhibit effective active vision strategies.

## 1 Introduction

Although Reinforcement Learning (RL) has demonstrated success across challenging tasks and games in both simulated and real environments , the observation spaces for visual RL tasks are typically predefined to offer the most advantageous views based on prior knowledge and can not be actively adjusted by the agent itself. For instance, table-top robot manipulators often utilize a fixed overhead camera view . While such fixed viewpoints can potentially stabilize the training of an image feature encoder , this form of perception is different from humans who actively adjust their perception system to finish the task, e.g. eye movements . The absence of active visual perception poses challenges on learning in highly dynamic environments , open-world tasks  and partially observable environments with occlusions, limited field-of-views, or multiple view angles .

We study Active Reinforcement Learning (Active-RL) , the RL process that allows the embodied agent to actively acquire new perceptual information in contrast to the standard RL, where the new information could be reward signals , visual observations , and other forms. Specifically, we are interested in visual Active-RL tasks, i.e. ActiveVision-RL, that an agent controls its own views of visual observation, in an environment with limited visual observability . Therefore, the goal of ActiveVision-RL is to learn two policies that still maximize the task return: the _motor policy_ to finish the task and the _sensory policy_ to control the observation.

ActiveVision-RL tasks present a considerable challenge due to the coordination between motor and sensory policies, given their mutual influence . The motor policy requires clear visual observation for decision-making, while the sensory policy should adapt accordingly to the motor action. Depending on the sensory policy, transitioning to a new view could either aid or hinder the motor policy learning [14; 40; 102]. One notable impediment is the perceptual aliasing mentioned by Whitehead and Ballard . An optimal strategy for sensory policy should be incorporating crucial visual information while eliminating any distractions. In the real world, humans disentangle their sensory actions, such as eye movements, from their motor actions, such as manipulation, and subsequently learn to coordinate them [59; 61]. Despite being modeled separately, these two action policies and the coordination can be learned jointly through the interaction during sensorimotor stage [25; 71; 72; 103].

Taking inspiration from human capabilities, we propose SUGARL: Sensorimotor Understanding Guided Active Reinforcement Learning, an Active-RL framework designed to jointly learn sensory and motor policies by maximizing extra intrinsic sensorimotor reward together with environmental reward. We model the ActiveVision-RL agent with separate sensory and motor policies by extending existing RL algorithms with two policy/value branches. Inspired by sensorimotor stage [71; 72; 103], we use the intrinsic sensorimotor reward to guide the joint learning of two policies, imposing penalties on the agent for selecting sub-optimal observations. We use a learned sensorimotor reward module to assign the intrinsic reward. The module is trained using inverse dynamics prediction task [52; 95], with the same experiences as policy learning without additional data or pre-training.

In our experiments, we use modified Atari  and DeepMind Control suite (DMC)  with limited observability to comprehensively evaluate our proposed method. We also test on Robosuite tasks to demonstrate the effectiveness of active agent in 3D manipulation. Through the challenging benchmarks, we experimentally show that SUGARL is an effective and generic approach for Active-RL with minimum modification on top of existed RL algorithms. The learned sensory policy also exhibit active vision skills by analogy with humans' fixation and tracking.

## 2 Active Vision Reinforcement Learning Settings

Consider a vanilla RL setting based on a Markov Decision Process (MDP) described by \((,,r,P,)\), where \(\) is the state space, \(\) is the action space, \(r\) is the reward function, \(P\) describes state transition which is unknown and \(\) is the discount factor. In this work, we study ActiveVision-RL under limited visual observability, described by \((,,^{s},^{o},r,P,)\), as shown in Figure 1. \(\) is the actual partial observation space the agent perceives. In particular, we are interested in visual tasks, so each observation \(\) is an image contains partial information of an environmental state \(\), like an image crop in 2D space or a photo from a viewpoint in 3D space. To emulate the human ability, there are two action spaces for the agent in Active-RL formulation. \(^{s}\) is the motor action space that causes state change \(p(^{}|,^{s})\). \(^{o}\) is the sensory action space that only changes the observation of an environmental state \(p(|,^{o})\). In this setting, the agent needs to take \((^{s},^{o})\) for each step, based on observation(s) only. An example is shown in Figure 1.

Our goal is to learn the motor and sensory action policies \((^{s},^{o})\) that still maximize the return \( r_{t}\). Note that the agent is never exposed to the full environmental state \(\). Both policies are completely based on the partial observations: \(^{s}=^{s}(|)\), \(^{o}=^{o}(|)\). Therefore the overall policy learning is challenging due to the limited information per step and the non-stationary observations.

Figure 1: ActiveVision-RL with limited visual observability in comparison with standard RL, with exemplary process in Atari game _Boxing_. Red arrows stand for additional relationships considered in ActiveVision-RL. In the example on the right, the highlighted regions are the actual observations visible to the agent at each step. The rest of the pixels are not visible to the agent.

## 3 SUGARL: Sensorimotor Understanding Guided Active-RL

### Active-RL Algorithm with Sensorimotor Understanding

We implement Active-RL algorithms based on the normal vision-based RL algorithms with simple modifications regarding separated motor and sensory policies \((^{s},^{o})\), and the sensorimotor reward \(r^{}\). We use DQN  and SAC  as examples to show the modifications are generally applicable. The example diagram of SAC is shown in Figure 2. We first introduce the policy then describe the sensorimotor reward in Section 3.2

Network ArchitectureThe architectural modification is branching an additional head for sensory policy, and both policies share a visual encoder stem. For DQN , two heads output \(Q^{s},Q^{o}\) for each policy respectively. This allows the algorithm to select \(^{s}=*{arg\,max}_{^{s}}Q^{s}\) and \(^{o}=*{arg\,max}_{^{o}}Q^{o}\) for each step. Similarly for SAC , the value and actor networks also have two separate heads for motor and sensory policies. The example in the form of SAC is in Figure 2.

Joint Learning of Motor and Sensory PoliciesThough two types of actions are individually selected or sampled from network outputs, we find that the joint learning of two policies benefits the whole learning . The joint learning here means both policies are trained using a shared reward function. Otherwise, the sensory policy usually fails to learn with intrinsic reward signal only. Below we give the formal losses for DQN and SAC.

For DQN, we take the sum of Q-values from two policies \(Q=Q^{s}+Q^{o}\) and train both heads jointly. The loss is the following where we indicate our modifications by blue:

\[^{Q}_{i}(_{i}) =_{(_{t},_{t},_{t}, _{})}[(y_{i}-(Q^{s}_{ _{i}}(_{t},^{s}_{t})+Q^{o}_{_{i}}(_ {t},^{o}_{t})))^{2}]\] \[y_{i} =_{_{t+1}}[r^{}_{t}+ r^{ }_{t}+(_{^{s}_{t+1}}Q^{s}_{_{i-1}} (_{t+1},^{s}_{t+1})+_{^{s}_{t+1}}Q^{o}_{ _{i-1}}(_{t+1},^{o}_{t+1}))],\]

where \(L^{Q}\) is the loss for Q-networks, \(\) stands for the parameters of both heads and the encoder stem, \(i\) is the iteration, and \(\) is the replay buffer. \( r^{}_{t}\) is the extra sensorimotor reward with balancing scale which will be described in Section 3.2.

For SAC, we do the joint learning similarly. The soft-Q loss is in the similar form of above DQN which is omitted for simplicity. The soft-value loss \(L^{V}\) and is

\[^{V}()=_{_{t}}[ {2}(V^{s}_{}(o_{t})+V^{o}_{}(o_{t}).-..\]

\[.._{^{s}_{t}^{s}_{},^{o}_{t }^{o}_{}}[Q^{s}_{}(_{t},^{s}_{t})+Q^{o }_{}(_{t},^{o}_{t})-^{s}_{}(^{s} _{t}|o_{t})-^{o}_{}(^{o}_{t}|o_{t})])^{2} ],\]

and the actor loss \(^{}\) is

\[^{}()=_{_{t}}[ ^{s}_{}(^{s}_{t}|_{t})+^{o}_{}(^{o}_{t}|_{t})-Q^{s}_{}(_{t},^{s}_{t})-Q^{ o}_{}(_{t},^{o}_{t})],\]

Figure 2: Overview of SUGARL and the comparison with original RL algorithm formulation. SUGARL introduces an extra sensory policy head, and jointly learns two policies together with the extra sensorimotor reward. We use the formulation of SAC  as an example. We introduce sensorimotor reward module to assign the reward. The reward indicates the quality of the sensory policy through the prediction task. The sensorimotor reward module is trained independently to the policies by action prediction error.

where \(,\) are parameters for the critic and the actor respectively, and reparameterization is omitted for clearness.

### Sensorimotor Reward

The motor and sensory policies are jointly trained using a shared reward function, which is the combination of environmental reward and our sensorimotor reward. We first introduce the assignment of sensorimotor reward and then describe the combination of two rewards.

The sensorimotor reward is assigned by the sensorimotor reward module \(u_{}()\). The module is trained to have the sensorimotor understanding, and is used to indicate the goodness of the sensory policy, tacking inspiration of human sensorimotor learning . The way we obtain such reward module is similar to learning an inverse dynamics model . Given a transition \((_{t},_{t}^{s},_{t+1})\), the module predicts the motor action \(^{s}\) only, based on an observation transition tuple \((_{t},_{t+1})\). When the module is (nearly) fully trained, the higher prediction error indicates the worse quality of visual observations. For example, if the agent is absent from observations, it is hard to infer the motor action. Such sub-optimal observations also confuse agent's motor policy. Since the sensory policy selects those visual observations, the quality of visual observations is tied to the sensory policy. As a result, we can employ the negative error of action prediction as the sensorimotor reward:

\[r_{t}^{}=-(1-p(_{t}^{s}|_{t},_{ t+1};u_{})). \]

This non-positive intrinsic reward penalizes the sensory policy for selecting sub-optimal views that do not contribute to the accuracy of action prediction and confuse motor policy learning. In Section 5.4 we show that naive positive rewarding does not guide the policy well. Note that the reward is less noisy when the module is fully trained. However, it is not harmful for being noisy at the early stage of training, as the noisy signal may encourage the exploration of policies. We use the sensorimotor reward though the whole learning.

The sensorimotor reward module is implemented by an independent neural network. The loss is a simple prediction error:

\[^{u}()=_{_{t},_{t+1},_{t }^{s}}[(_{t}^{s}-u_{}( _{t},_{t+1}))], \]

where the \(()\) can be a cross-entropy loss for discrete action space or L2 loss for continuous action space. Though being implemented and optimized separately, the sensorimotor reward module uses the same experience data as policy learning, with no extra data or prior knowledge introduced.

**Combining Sensorimotor Reward and Balancing** The sensorimotor reward \(r^{}\) is added densely on a per-step basis, on top of the environmental reward \(r^{}\) in a balanced form:

\[r_{t}=r_{t}^{}+ r_{t}^{}, \]

where \(\) is the balance parameter varies across environments. The reward balance is very important to make both motor and sensory policies work as expected, without heavy bias towards one side , which will be discussed in our ablation study in Section 5.4. Following the studies in learning with intrinsic rewards and rewards of many magnitudes [20; 38; 75; 96; 99], we empirically set \(=_{}[_{t=1}^{T}r_{t}^{}/T]\), which is the average environmental return normalized by the length of the trajectory. We get these referenced return data from the baseline agents trained on normal, fully observable environments, or from the maximum possible environmental return of one episode.

### Persistence-of-Vision Memory

To address ActiveVision-RL more effectively, we introduce a Persistence-of-Vision Memory (PVM) to spatio-temporally combine multiple recent partial observations into one, mimicking the nature

Figure 3: Examples for different instantiations of PVM with \(B=3\).

of human eyes and the memory. PVM aims to expand the effective observable area, even though some visual observations may become outdated or be superseded by more recent observations. PVM stores the observations from \(B\) past steps in a buffer and combines them into a single PVM observation according to there spatial positions. This PVM observation subsequently replaces the original observation at each step:

\[(_{t})=f(_{t-B+1},,_{t}),\]

where \(f()\) is a combination operation. In the context of ActiveVision-RL, it is reasonable to assume that the agent possesses knowledge of its focus point, as it maintains the control over the view. Therefore, the viewpoint or position information can be used. In our implementations of \(f()\) shown in Figure 3, we show a 2D case of PVM using stitching, and a PVM using LSTM which can be used in both 2D and 3D environments. In stitching PVM, the partial observations are combined like a Jiasaw puzzle. It is worth noting that combination function \(f\) can be implemented by other pooling operations, sequence representations [15; 44; 85; 86], neural memories [37; 39; 76; 78], or neural 3D representations [74; 107], depending on the input modalities, tasks, and approaches.

## 4 Environments and Settings

### Active-Gym: An Environment for ActiveVision-RL

We present Active-Gym, an open-sourced customized environment wrapper designed to transform RL environments into ActiveVision-RL constructs. Our library currently supports active vision agent on Robosuite , a robot manipulation environment in 3D, as well as Atari games  and the DeepMind Control Suite (DMC)  offering 2D active vision cases. These 2D environments were chosen due to the availability of full observations for establishing baselines and upper bounds, and the availability to manipulate observability for systematic study.

### Robosuite Settings

Observation SpaceIn the Robosuite environment, the robot controls a movable camera. The image captured by that camera is a partial observation of the 3D space.

Action SpacesEach step in Active-Gym requires motor and sensory actions \((^{s},^{o})\). The motor action space \(^{s}\) is the same as the base environment. The sensory action space \(^{o}\) is a 5-DoF control: relative (x, y, z, yaw, pitch). The maximum linear and angular velocities are constrained to 0.01/step and 5 degrees/step, respectively.

### 2D Benchmark Settings

Observation SpaceIn the 2D cases of Active-Gym, given a full observation \(\) with dimensions \((H,W)\), only a crop of it is given to the agent's input. Examples are highlighted in red boxes in Figure 5. The sensory action decides an observable area by a location \((x,y)\), corresponding to the top-left corner of the bounding box, and the size of the bounding box \((h,w)\). The pixels within the observable area becomes the foveal observation, defined as \(^{t}=^{c}=[x:x+h,y:y+w]\). Optionally, the foveal observation can be interpolated to other resolutions \(^{f}=(^{c};(h,w)(r^{f}_{h},r^{f}_{w}))\), where \((r^{f}_{h},r^{f}_{w})\) is the foveal resolution. This design allows for flexibility in altering the observable area size while keeping the effective foveal resolution constant. Typically we set \((r^{f}_{h},r^{f}_{f})=(h,w)\) and fixed them during a task. The peripheral observation can be optionally provided as well, obtained by interpolating the non-foveal part \(^{p}=(^{c};(H,W)(r^{p}_ {h},r^{p}_{w}))\), where \((r^{p}_{h},r^{p}_{w})\) is the peripheral resolution. The examples are at the even columns of Figure 5. If the peripheral observation is not provided, \(^{p}=0\).

Figure 4: Five selected Robosuite tasks with examples on four hand-coded views.

Action SpacesThe sensory action space \(^{o}\) includes all the possible (pixel) locations on the full observation, but can be further formulated to either continuous or discrete spaces according to specific task designs. In our experiments, we simplify the space by a 4x4 discrete grid-like anchors for \((x,y)\) (Figure 5 right). Each anchor corresponds to the top-left corner of the observable area. The sensory policy chooses to place the observable area among one of 16 anchors (**absolute** control), or moves it from one to the four neighbor locations (**relative** control).

### Learning Settings

In our study, we primarily use DQN  and SAC  as the backbone algorithms of SUGARL to address tasks with discrete action spaces (Atari), and use DrQv2  for continuous action spaces (DMC and Robosuite). All the visual encoders are standardized as the convolutional networks utilized in DQN . To keep the network same, we resize all inputs to 84x84. For the sensorimotor understanding model, we employ the similar visual encoder architecture with a linear head to predict \(_{s}^{s}\). Each agent is trained with one million transitions for each of the 26 Atari games, or trained with 0.1 million transitions for each of the 6 DMC tasks and 5 Robosuite tasks. The 26 games are selected following Atari-100k benchmark . We report the results using Interquartile Mean (IQM), with the scores normalized by the IQM of the base DQN agent under full observation (except Robosuite), averaged across five seeds (three for Robosuite) and all games/tasks per benchmark. Details on architectures and hyperparameters can be found in the Appendix.

## 5 Results

### Robosuite Results

We selected five of available tasks in Robosuite , namely block lifting (Lift), block stacking (Stack), nut assembling (NutAssembleSquare), door opening (Door), wiping the table (Wipe). The first two are easier compared to the later three. Example observations are available in Figure 4. We compare against a straightforward baseline that a single policy is learned to govern both motor and sensory actions. We also compare to baselines including RL with object detection (a replication of Cheng et al. ), learned attention , and standard RL with hand-coded views. Results are in 1. We confirm that our SUGARL works outperforms baselines all the time, and also outperforms

   Approach & Wipe & Door & NutAssemblySquare & Lift & Stack \\  SUGARL-DrQ (Stacking PVM) & 56.0 & 274.8 & 78.0 & 79.2 & 12.7 \\ SUGARL-DrQ (LSTM PVM) & 58.5 & 266.9 & **108.6** & 88.8 & 31.5 \\ SUGARL-DrQ (3D Transformation+LSTM PVM) & **74.1** & **291.0** & 65.2 & 87.5 & 32.4 \\  SUGARL-DrQ w/o Joint Learning & 43.6 & 175.4 & 58.0 & 107.2 & 12.0 \\ SUGARL-DrQ w/o PVM & 52.8 & 243.3 & 37.9 & 55.6 & 7.7 \\  Single Policy & 1.2 & 22.8 & 8.42 & 10.7 & 0.53 \\ DrQ w/ Object Detection (DETR) & 15.2 & 43.1 & 54.8 & 15.4 & 7.5 \\ DrQ w/ End-to-End Attention & 14.2 & 141.4 & 28.5 & 33.0 & 13.6 \\  Eye-in-hand View (hand-coded, moving camera) & 16.1 & 114.6 & 102.9 & **233.9** & **73.0** \\ Front View (hand-coded, fixed camera) & 49.4 & 240.6 & 39.6 & 69.0 & 13.8 \\ Agent View (hand-coded, fixed camera) & 12.7 & 190.3 & 49.9 & 122.6 & 14.7 \\ Side View (hand-coded, fixed camera) & 25.9 & 136.2 & 34.5 & 56.6 & 12.8 \\   

Table 1: Results on Robosuite. We report the IQM of raw rewards from 30 evaluations. Highlighted task names are harder tasks. **Bold** numbers are the best scores of each task and underscored numbers are the second best.

Figure 5: Left: observations from Active-Gym with different foveal resolutions (F) and peripheral settings (P) in Atari and DMC. The red bounding boxes show the observable area (foveal) for clarity. Right: 4x4 discrete sensory action options in our experiments.

the hand-coded views most of the time. Specifically, for the harder tasks including Wipe, Door, and NutAssemblySquare, SUGARL gets the best scores.

Designs of PVMWe compare different instantiations of proposed PVMs including: Stacking: naively stacking multiple frames; LSTM: Each image is first encoded by CNN and fed into LSTM; 3D Transformation + LSTM: we use camera parameters to align pixels from different images to the current camera frame. Then an LSTM encodes the images after going through CNN. We find that 3D Transformation + LSTM works the best, because it tackles spatial aligning and temporal merging together. LSTM also works well in general.

### 2D Benchmark Results

We evaluate the policy learning on Atari under two primary visual settings: **with** and **without peripheral observation**. In each visual setting we explore three sizes of observable area (set equivalent to foveal resolution): **20x20**, **30x30**, and **50x50**. In with peripheral observation setting, the peripheral resolution is set to 20x20 for all tasks. We use DQN -based SUGARL (SUGARL-DQN) and compare it against two variants by replacing the learnable sensory policy in SUGARL with: **Random View** and **Raster Scanning**. Random View always uniformly samples from all possible crops. Raster Scanning uses a pre-designed sensory policy that chooses observable areas from left to right and top to down sequentially. Raster Scanning yields relatively stable observation patterns and provides maximum information under PVM. We also provide a DQN baseline trained on full observations (84x84) as a soft oracle. In with peripheral observation settings, another baseline trained on peripheral observation only (20x20) is compared as a soft lower bound.

In Figure 5(a) and 5(b), we find that SUGARL performs the best in all settings, showing that SUGARL learns effective sensory and motor policies jointly. More importantly, SUGARL with peripheral observation achieves higher overall scores (+0.01\(\)0.2) than the full observation baselines. In details, SUGARL gains higher scores than the full observation baseline in 13 out of 26 games with 50x50 foveal resolution (details are available in the Appendix). This finding suggests the untapped potential of ActiveVision-RL agents which leverage partial observations better than full observations. By actively selecting views, the agent can filter out extraneous information and concentrate on task-centric information.

Compare against Static Sensory PoliciesWe also examine baselines with static sensory policies, which consistently select one region to observe throughout all steps. The advantage of this type baseline lies in the stability of observation. We select 3 regions: **Center**, **Upper Left**, and **Bottom Right**, and compare them against SUGARL in environment w/o peripheral observation. As shown in Figure 5(c), we observe that SUGARL still surpasses all three static policies. The performance gaps between SUGARL and Center and Bottom Right are relatively small when the observation size is larger (50x50), as the most valuable information is typically found at these locations in Atari

Table 2: Evaluation results on different conditions and algorithm backbones

Figure 6: Results with different observation size and peripheral observation settings. The green bars are results from SUGARL. The red dashed lines stand for the DQN baseline trained on full observations using the same amount of data. We compare SUGARL against two dynamic views: Random View and Raster Scanning, and three static view baselines. In (b) with peripheral observation, we compare a baseline using peripheral observation only indicated by the cyan line.

[MISSING_PAGE_FAIL:8]

### Sensory Policy Analysis

Sensory Policy PatternsIn Figure 7, we present examples of learned sensory policies from SUGARL-DQN in four Atari games, in settings w/o peripheral observations. These policies are visualized as heat maps based on the frequency of observed pixels. We discover that the sensory policies learn both **fixation** and **movement** (similar to tracking) behaviours  depending on the specific task requirements. In the first two examples, the sensory policy tends to concentrate on fixed regions. In _battle_zone_, the policy learns to focus on the regions where enemies appear and the fixed front sight needed for accurate firing. By contrast, in highly dynamic environments like _boxing_ and _freeway_, the sensory policies tend to observe broader areas in order to get timely observations. Though not being a perfect tracker, the sensory policy learns to track the agent or the object of interest in these two environments, demonstrating the learned capability akin to humans' Smooth Pursuit Movements. Recorded videos for entire episodes are available at our project page.

Sensory Policy DistributionWe quantitatively assess the distributions of learned sensory policies. There are 16 sensory actions, i.e. the observable area options in our setup (Figure 5). We compare the distributions against uniform distribution using KL-divergence, across 26 games x 10 eval runs x 5 seeds. The resulting histogram are shown in Figure 8. We observe that the learned policies consistently deviate from the uniform distribution, suggesting that sensory policies prefer specific regions in general. The high peak at the high KL end supports the "fixation" behavior identified in the previous analysis. As the observation size increases, the divergence distribution shifts towards smaller KL end, while remaining \(>0.5\) for all policies. This trend indicates that with larger observable sizes, the policy does not need to adjust its attention frequently, corroborating the benefit of using relative control shown in Table 1(a).

Pitfalls on Max-Entropy Based MethodsIn the previous analysis, we both qualitatively and quantitatively demonstrate that sensory policies are not uniformly dispersed across the entire sensory action space. This observation implies that _sensory_ policy should exercise caution when adopting the max-entropy-based methods. We conduct an experiment on varying the usage of \(\), the entropy coefficient in SAC in all three foveal observation size settings. Results in Table 4 show that simply disabling autoutone and setting a small value to the sensory policy's \(\) improves. This finding tells that max-entropy may not be a suitable assumption in modeling sensory policies.

### Ablation Studies

We conduct ablation studies in the setting without peripheral observation and with 50x50 observation size. Five crucial design components are investigated to validate their effectiveness by incrementally adding or removing them from the full model. The results are presented in Table 5. From the results, we demonstrate the importance of _penalizing_ the agent for inaccurate self-understanding predictions rather than rewarding accurate predictions (\(r^{}\)(positive) \( r^{}\)(negative)). By imposing penalties, the maximum return is bounded by the original maximum possible return per episode, allowing motor and sensory policies to better coordinate each other and achieve optimal task performance. _Reward balance_ significantly improves the policy, indicating its effectiveness in coordinating two policies as well. PVM also considerably enhances the algorithm by increasing the effective observable area as expected.

   Model & 20 & 30 & 50 \\    autotune \(\) \\ fixed-\(=0.2\) \\  } & 0.271 & 0.358 & 0.444 \\   fixed-\(=0.2\) \\  } & **0.424** & **0.730** & **0.785** \\   

Table 4: Varying \(\) of SUGARL-SAC.

Figure 8: KL divergence distributions of learned sensory policies.

Related Work

**Active Learning** is the concept that an agent decides which data are taken into its learning and may ask for external information in comparison to fitting a fixed data distribution [1; 6; 10; 21; 22; 31; 46; 49; 55; 57; 60; 64; 66; 73; 81; 88; 90; 101; 105; 113]. **Active Vision** focuses on continuously acquiring new visual observations that is helpful for the vision task like object classification, recognition and detection [4; 5; 7; 8; 18; 28; 29; 48; 63; 98; 106; 109], segmentation [13; 58; 70], and action recognition . The active vision is usually investigated under a robot vision scenario that a robot moves around in a scene. However, the policy is usually not required to accomplish a task with physical interactions such as manipulating objects compared to reinforcement learning.

**Active Reinforcement Learning** (Active-RL), at a high level, is that the agent is allowed to actively gather new perceptual information of interest simultaneously through an RL task, which can also be called active perception . The extra perceptual information could be reward signal [3; 27; 50; 56; 62], visual observations from new viewpoints [33; 54; 67; 87], other input modalities , and language instructions [19; 68; 69; 94]. Though these work may not explicitly use the term Active-RL, we find that they can be uniformly organized in the general Active-RL formulation and we coin the term here. In our work, we study the ActiveVision-RL task in a limited visual observability environment, where at each step the agent is only able to partially observe the environment. The agent should actively seek the optimal observation at each step. Therefore, our setting is more close to [32; 33] and Active Vision problems, unlike research incorporating attention-like inductive bias given a full observation [34; 42; 43; 79; 91; 93; 104]. The ActiveVision-RL agent must learn an observation selection policy, called sensory policy, to effectively choose the optimal partial observation for executing the task-specific policy (motor policy). The unique challenge for ActiveVision-RL is the coordination between sensory and motor policies given there mutual influence. In recent works, the sensory policy can be either trained in the task-agnostic way  with enormous exploration data, or trained jointly with the task  with naive environmental reward only. In this work we investigate the joint learning case because of the high cost and availability concern of pre-training tasks .

**Robot Learning with View Changes** Viewpoint changes and gaps in visual observations are the common challenges for robot learning [40; 53; 77; 84; 92; 112], especially for the embodied agents that uses its first-person view [30; 35; 80]. To address those challenges, previous works proposed to map visual observation from different viewpoints to a common representation space by contrastive encoding [26; 82; 83] or build implicit neural representations [53; 107]. In many first-person view tasks, the viewpoint control is usually modeled together with the motor action like manipulation and movement [30; 80]. In contrast, in our ActiveVision-RL setting, we explore the case where the agent can choose where to observe independently to the motor action inspired by the humans' ability.

## 7 Limitations

In this work, we assume that completely independent sensory and motor actions are present in an embodied agent. But in a real-world case, the movement of the sensors may depend on the motor actions. For example, a fixed camera attached to the end-effector of a robot manipulator, or to a mobility robot. To address the potential dependence and conflicts between two policies in this case, extensions like voting or weighing across two actions to decide the final action may be required. The proposed algorithm also assumes a chance to adjust viewpoints at every step. This could be challenging for applications where the operational or latency costs for adjusting the sensors are high like remote control. To resolve this, additional penalties on sensory action and larger memorization capability are potentially needed. Last, the intrinsic reward currently only considers the accuracy of agent-centric prediction. Other incentives like gathering novel information or prediction accuracy over other objects in the environment can be further explored.

## 8 Conclusion

We present SUGARL, a framework based on existed RL algorithms to jointly learn sensory and motor policies through the ActiveVision-RL task. In SUGARL, an intrinsic reward determined by sensorimotor understanding effectively guides the learning of two policies. Our framework is validated in both 3D and 2D benchmarks with different visual observability settings. Through the analysis on the learned sensory policy, it shows impressive active vision skills by analogy with human's fixation and tracking that benefit the overall policy learning. Our work paves the initial way towards reinforcement learning using active agents for open-world tasks.