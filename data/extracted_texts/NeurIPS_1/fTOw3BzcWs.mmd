# ExID: Offline RL with Intuitive Expert Insights in Limited-Data Settings

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

With the ability to learn from static datasets, Offline Reinforcement Learning (RL) emerges as a compelling avenue for real-world applications. However, state-of-the-art offline RL algorithms perform sub-optimally when confronted with limited data confined to specific regions within the state space. The performance degradation is attributed to the inability of offline RL algorithms to learn appropriate actions for rare or unseen observations. This paper proposes a novel domain knowledge-based regularization technique and adaptively refines the initial domain knowledge to considerably boost performance in limited data with partially omitted states. The key insight is that the regularization term mitigates erroneous actions for sparse samples and unobserved states covered by domain knowledge. Empirical evaluations on standard discrete environment datasets demonstrate a substantial average performance increase compared to ensemble of domain knowledge and existing offline RL algorithms operating on limited data.

## 1 Introduction

Offline RL [9; 1], also referred to as batch RL, is a learning approach that focuses on extracting knowledge solely from static datasets. This class of algorithms has a wider range of applications being particularly appealing to real-world data sets from business , healthcare , and robotics . However, offline RL poses unique challenges, including over-fitting and the need for generalization to data not present in the dataset. To surpass the behavior policy, offline RL algorithms need to query Q values of actions not in the dataset, causing extrapolation errors . Most offline RL algorithms address this problem by enforcing constraints that ensure that the learned policy does not deviate too far away from the data set's state action distribution [13; 11] or is conservative towards Out-of-Distribution (OOD) actions [21; 20]. However, such approaches are designed on coherent batches , which do not account for OOD states.

In many domains, such as business and healthcare, available data is scarce and often confined to expert behaviors within a limited state space. _For example, a sales recommendation system, where historic data may not contain details about many active users and operator gives coupon of higher value to attract sales_. Learning on such limited data sets can curtail the generalization capabilities of state-of-the-art (SOTA) offline RL algorithms, resulting in sub-optimal performance . We illustrate this limitation via Fig 1. In Fig 1a) the state action space of a simple Mountain Car environment  is plotted for an expert dataset  and a partial dataset with first 10% samples from the entire dataset. Fig 1b) shows the average reward obtained over these data sets and the average difference between the Q value of action taken by the under-performing Conservative Q Learning (CQL)  agent and the action in the full expert dataset for unseen states. It can be observed that the performance of the offline RL agent considerably drops. This is attributed to the critic overestimating the Q value of non-optimal actions for states that do not occur in the dataset while training.

In numerous real-world applications, expert insights regarding the general behavior of a policy are often accessible . _For example, sales operators often distribute lower discount coupons to active users to maximize profit_. While these insights may not be optimal, they serve as valuable guidelines for understanding the overall behavior of the policy. A rich literature in knowledge distillation  has shown that teacher networks trained on domain knowledge can transfer knowledge to another network unaware of it. This work aims to leverage a teacher network mimicking simple decision tree-based domain knowledge to help offline RL generalize in limited data settings.

The paper makes the following novel contributions:

* We introduce an algorithm dubbed **ExID**, leveraging intuitive human obtainable expert insights. The domain expertise is incorporated into a teacher policy, which improves offline RL in limited-data settings through regularization.
* The teacher based on expected performance improvement of the offline policy during training, improving the teacher network beyond initial heuristics.
* We demonstrate the effectiveness of our methodology on _real sales promotion dataset_, several discrete OpenAI gym and Minigrid environments with standard offline RL data sets and show that ExID significantly exceeds the performance when faced with limited data.

## 2 Related Work

This work improves offline RL learning on batches sampled from static datasets using domain expertise. One of the major concerns in offline RL is the erroneous extrapolation of OOD actions . Two techniques have been studied in the literature to prevent such errors. 1) Constraining the policy to be close to the behavior policy 2) Penalizing overly optimistic Q values . We discuss a few relevant algorithms following these principles. In Batch-Constrained deep Q-learning (BCQ)  candidate actions sampled from an adversarial generative model are considered, aiming to balance proximity to the batch while enhancing action diversity. Algorithms like Random Ensemble Mixture Model (REM) , Ensemble-Diversified Actor-Critic (EDAC)  and Uncertainty Weighted Actor-Critic (UWAC)  penalize the Q value according to uncertainty by either using Q ensemble networks or directly weighting the loss with uncertainty. CQL  enforces regularization on Q-functions by incorporating a term that reduces Q-values for OOD actions while increasing Q-values for actions within the expected distribution. However, these algorithms do not handle OOD actions for states not in the static dataset and can have errors induced by changes in transition probability.

Integration of domain knowledge in offline RL, though an important avenue, has not yet been

Figure 1: a) Full expert, Mountain Car dataset, and reduced dataset with first 10% samples showing distribution of state (position, velocity) and action b) CQL agent converging to a sub-optimal policy for reduced dataset exhibiting high Q values for actions different from actions in the expert dataset for unseen states.

extensively explored. Domain knowledge incorporation has improved online RL with tight regret bounds [33; 4]. In offline RL, bootstrapping via blending heuristics computed using Monte-Carlo returns with rewards has shown to outperform SOTA algorithms by 9% . Recent works improve offline RL by incorporating a safety expert  and preference query , contrary to our work which improves imperfect domain knowledge. The closest to our work is Domain Knowledge guided Q learning (DKQ)  where domain knowledge is represented in terms of action importance and the Q value is weighted according to importance. However, obtaining action importance in practical scenarios is nontrivial.

## 3 Preliminaries

A DRL setting is represented by a Markov Decision Process (MDP) formalized as \((S,A,T,r,_{0},)\). Here, \(S\) denotes the state space, \(A\) signifies the action space, \(T(s^{}|s,a)\) represents the transition probability distribution, \(r:S A\) is the reward function, \(_{0}\) represents the initial state distribution, and \((0,1]\) is the discount factor. The primary objective of any DRL algorithm is to identify an optimal policy \((a|s)\) that maximizes \(_{s_{t},a_{t}}[_{t=0}^{}^{t}r(s_{t},a_{t})]\) where, \(s_{0} d_{0}(.),a_{t}(.|s_{t})\), and \(s^{} T(.|s_{t},a_{t})\). Deep Q networks (DQNs)  learn this objective by minimizing the Bellman residual \((Q_{}(s,a)-B^{_{}}Q_{}(s,a))^{2}\) where \(B^{_{}}Q_{}(s,a)=_{s^{} T}[r(s,a)+ _{a^{}_{}(.|s^{})}[Q_{^{}}(s^{ },a^{})]]\). The policy \(_{}\) chooses actions that maximize the Q value \(_{a^{} A}Q_{}(s^{},a^{})\). However, in offline RL where transitions are sampled from a pre-collected dataset \(\), the chosen action \(a^{}\) may exhibit a bias towards OOD actions with inaccurately high Q-values. To handle the erroneous propagation from OOD actions, CQL  learns conservative Q values by penalizing OOD actions. The CQL loss for discrete action space is given by

\[_{}()=_{Q}\ \ _{s }[log_{a}exp(Q_{}(s,a))-\] \[_{a|s}[Q_{}(s,a)]]+ _{s,a,s^{}}[Q_{}-Q_{^{}}]^{2} \]

Eq. 1 encourages the policy to be close to the actions seen in the dataset. However, CQL works on the assumption of coherent batches, i.e., if \((s,a,s^{})\), then \(s^{}\). There is no provision for handling OOD actions for \(s\), which can lead to policy failure when data is limited. In the next sections, we present ExID, a domain knowledge-based approach to improve performance in data-scarce scenarios.

## 4 Problem Setting and Methodology

In our problem setting, the RL agent learns the policy on a limited dataset with rare and unseen demonstrations. We define the characteristics of this dataset as follows:

**Definition 4.1**.: A reduced buffer \(_{r}\) is a proper subset of the full dataset \(\) i.e., \(_{r}\) satisfying the following conditions:

* Some states in \(\) are not present in \(_{r}\), i.e., \( s^{}(s,a,s^{}):(s,a,s^{}) _{r}\)
* The number of samples \(N(s,a,s^{})\) for some transitions in \(\) are less in \(_{r}\) i.e, \((s,a,s^{}):N(s,a,s^{})_{_{r}}<N(s, a,s^{})_{}\)

We observe, _performing \(Q-Learning\) by sampling from a limited buffer \(_{r}\) may not converge to an optimal policy for the MDP \(M_{}\) representing the full buffer._ This can be shown as a special case of (Theorem 1,) as \(p_{}(s^{}|s,a) p_{_{r}}(s^{}|s,a)\) and no Q updates for \((s,a)_{r}\) leading to sub-optimal policy. Please refer to the App. B for analysis and example.

We also assume a set of common sense rules in the form of domain knowledge is available. Domain knowledge \(\) is defined as hierarchical decision nodes capturing \(S A\) as represented by Eq. 2. Each decision node \(T_{_{i}}\) is represented by a constraint \(_{_{i}}\) and Boolean indicator \(_{_{i}}\) function selects the branch to be traversed based on \(_{_{i}}\).

\[Action =a_{_{i}}&\ leaf\\ _{_{i}}T_{_{i}}(s)+(1-_{_{i}})T_{_{i}}(s )&\] \[_{_{i}}(s) =1&s_{_{i}}\\ 0& \]We assume that \(\) gives heuristically reasonable actions for \(s D\) and \(S_{} S_{_{r}}\) where \(S_{},S_{_{r}}\) are the state coverage of \(\) and \(_{r}\).

**Training Teacher:** An overview of our methodology is depicted in Fig 2. We first construct a trainable actor network \(_{t}^{}\) parameterized by \(\) from \(\), Fig 2 step 1. For training \(_{t}^{}\) synthetic data \(\) is generated by sampling states from a uniform random distribution over state boundaries \(B(s)\), \(=(B(S))\). Note that this does not represent the true state distribution and may have state combinations that will never occur. We train \(_{t}^{}\) using behavior cloning where state \(\) is checked with root decision node in Eq. 2. A random action is chosen if \(\) does not satisfy decision node \(T_{_{0}}\) or leaf action is absent. If \(\) satisfies a \(T_{_{i}}\), \(T_{_{i}}\) is traversed and action \(a_{_{i}}\) is returned from the leaf node. This is illustrated in Fig 2 (a). We term the pre-trained actor network \(_{t}^{}\) as the teacher policy.

**Regularizing Critic:** We now introduce Algo 1 (App C) to train an offline RL agent on \(_{r}\). Algo 1 takes \(_{r}\) and pretrained \(_{t}^{}\) as input. The algorithm uses two hyper-parameters, warm start parameter \(k\) and mixing parameter \(\). A critic network \(Q_{s}^{}\) with Monte-Carlo (MC) dropout and target network \(Q_{s}^{^{}}\) are initialized. ExID is divided into two phases. In the first phase, we aim to warm start the critic network \(Q_{s}^{}\) with actions from \(_{t}^{}\) as shown in Fig 2b ( i). However, this must be done selectively as the teacher's policy is random around the states that do not satisfy domain knowledge. In each iteration, we first check the states sampled from a mini-batch of \(_{r}\) with \(\). For the states which satisfy \(\) we compute the teacher action \(_{t}^{}(s)\) and critic's action \(*{argmax}_{a}(Q_{s}^{}(s,a))\) and collect it in lists \(a_{t},a_{s}\), Algo 1 lines 4-10. Our main objective is to keep actions chosen by the critic network for \(s\) close to the teacher's policy. To achieve this, we introduce a regularization term:

\[_{r}()=_{s_{r} s }}_{}^{ }(s,a_{s})-Q_{s}^{}(s,a_{t})]^{2}}_{} \]

Eq 3 incentivizes the critic to increase Q values for actions from \(_{t}^{}\) and decreases Q values for other actions when \(*{argmax}_{a}(Q_{s}^{}(s,a))_{t}^{}(s)\) for states that satisfy domain knowledge. Note that Eq 3 will only be 0 when \(*{argmax}_{a}(Q_{s}^{}(s,a))=_{t}^{}(s)\) for \(s\). It is also set to 0 for \(s\). However, since \(_{t}^{}\) mimicking heuristic rules is sub-optimal, it is also important to incorporate learning from the data. The final loss is a combination of Eq. 1 and Eq. 3 with a mixing parameter \(\):

Figure 2: Overview of the proposed methodology (a) Training a teacher policy network with domain knowledge and synthetic data (b) Updating the offline RL critic network with teacher network

\[()=_{}()+_{s _{r} s}[Q^{}_{s}(s,a_{s})-Q^{}_{ s}(s,a_{t})]^{2} \]

The choice of \(\) and the warm start parameter \(k\) depends on the quality of \(\). In the case of perfect domain knowledge, \(\) would be set to 1, and setting \(\) to 0 would lead to the vanilla CQL loss. Mixing both the losses allows the critic to learn both from the data in \(_{r}\) and knowledge in \(\).

**Updating Teacher:** Given a reasonable warm start, the critic is expected to give higher Q values for optimal actions for \(s_{r}\) as it learns from data. We aim to leverage this knowledge to enhance the initial teacher policy \(^{}_{t}\) trained on heuristic domain knowledge. For \(s\) and \(s\), we calculate the average Q values over critic actions and teacher actions and check which one is higher in Algo 1 line 11 which refers to Cond. 6. For brevity \(_{s_{r} s}\) is written as \(\). If \((Q^{}_{s}(s,a_{s}))>(Q^{}_{s}(s,a_{t}))\) it denotes the critic expects a better return on an average over its own policy than the teacher's policy. Hence, we can use the critic's policy to update \(^{}_{t}\), making it better over \(\). However, only checking the critic's value can be erroneous as the critic can have high values for OOD actions. We check the average uncertainty of the predicted Q values to prevent the teacher from getting updated by OOD actions. Uncertainty has been shown to be a good metric for OOD action detection by . A well-established methodology to capture uncertainty is predictive variance, which takes inspiration from Bayesian formulation for the critic function and aims to maximize \(p(|X,Y)=p(Y|X,)p()/p(Y|X)\) where \(X=(s,a)\) and \(Y\) represents the true Q value of the states. However, \(p(Y|X)\) is generally intractable and is approximated using Monte Carlo (MC) dropout, which involves including dropout before every layer of the critic network and using it during inference . Following , we measure the uncertainty of prediction using Eq 5.

\[Var^{T}[Q(s,a)]_{t=1}^{T}[Q(s,a)-(s,a)]^{2} \]

Eq 5 estimates the variance of Q value \(Q(s,a)\) for an action \(a\) using \(T\) forward passes on the \(Q^{}_{s}(s,a)\) with dropout where \((s,a)\) represents the predictive mean. We check the average uncertainty of the Q value for action chosen by the critic and teacher policy over the states that match domain knowledge in a batch. The teacher network is updated using the critic's action only when the policy expects a higher average Q return on its action and the average uncertainty of taking this action is lower than the teacher action. \((Var^{T}Q^{}_{s}(s_{r},a_{s}))<(Var^{T}Q^{}_{ s}(s_{r},a_{t}))\) indicates the actions were learned from the expert data in the buffer and are not OOD samples. The condition is summarized in cond. 6:

\[(Q^{}_{s}(s_{r},a_{s}))>(Q^{}_{s}(s_{r},a_{t}))\]

\[(Var^{T}Q^{}_{s}(s_{r},a_{s}))<(Var^{T}Q^{}_{ s}(s_{r},a_{t})) \]

We update the teacher with cross-entropy described in Eq 7:

\[()=-_{s D}(^{}_{t}(s)log(_{s}(s))) \]

where, \(_{s}(s,a)=}{_{s^{}}Q(s,a^{})}\). When the critic's policy is better than the teacher's policy, \(_{r}()\) is set to 0 Algo 1 Lines 11 to 13. Finally, the critic network is updated using calculated loss \(()\) Algo 1 Lines 17-18. We theoretically analyse the implications of using ExID in propositions 4.2 and 4.3.

**Proposition 4.2**.: _Denote \(\) as the policy learned by ExID, \(_{u}\) as any offline RL policy learned on \(_{r}\) and optimal \(Q\) function as \(Q^{*}\) and \(V\) function as \(V^{*}\). Then it holds that_

\[()-(_{u})_{s O[_{u}}[V^{*}(s)-Q^{*}( s,_{u}(s))]-_{}\]

Where \(=_{s O}[V^{*}(s)-Q^{*}(s,(s))]\), \(_{}(s)=[}|(1-)},]\) (\(|\)\(S_{}\)\(|\) is the number of different states observed by \(\)) and \(O_{r}\). Here \(\) denotes the quality of regularized action for \(s_{r}\). Hence, updating \(^{}_{t}\) is important as high divergence of action from the optimal can lead to performance degradation. In offline RL, the extrapolation error for non optimal action is usually high for states not observed in dataset (as illustrated in 1b), regularization can lead performance improvement when \(^{}_{t}\) is reasonable. Furthermore, in ExID coarse actions from \(^{}_{t}\) are updated driving them closer to the optimal actions, improving the performance lower bound. Additionally \(^{}_{t}\) increases \(|\)\(S_{}\)\(|\) making \(_{} 1\) in practice further improving the performance lower bound. _Proof is deferred to App. A_.

**Proposition 4.3**.: _ExID reduces generalization error if \(Q^{*}(s,_{t}^{}(s))>Q^{*}(s,_{u}(s))\) for \(s_{r}\)._

Proof is deferred to App. A.: In the next section, we discuss our empirical evaluations.

## 5 Empirical Evaluations

We investigate the following through our empirical evaluations: _1. Does ExID perform better than combining \(\) and offline RL algoes on different environments with datasets exhibiting rare and OOD states Sec 5.2? 2. Does ExID generalize to OOD states covered by \(\) Sec 5.4? 3. What is the effect of varying \(k\), \(\) and updating \(_{t}^{}\) Sec 5.5? 4. How does performance vary with the quality of \(\) Sec 5.6?_

### Experimental Setting

We evaluate our methodology on open-AI gym , MiniGrid  and _real sales promotion (SP) _ offline data sets. All our data sets are generated using standard methodologies defined in [32; 31]_except SP which is generated by human operators_. All experiments have been conducted on a Ubuntu 22.04.2 LTS system with 1 NVIDIA K80 GPU, 4 CPUs, and 61GiB RAM. App. F notes the hyperparameter values and network architectures.

**Dataset:** We experiment on three types of data sets. _Expert Data-set_[10; 16; 22] generated using an optimal policy without any exploration with high trajectory quality but low state action coverage. _Replay Data-set_[2; 13] generated from a policy while training it online, exhibiting a mixture of multiple behavioral policies with high trajectory quality and state action coverage. _Noisy Data-set_[12; 13; 22; 16] generated using an optimal policy that also selects random actions with \(\) greedy strategy where \(=0.2\) having low trajectory quality and high state action coverage. Additionally we also experiment on human generated dataset for sales promotion task.

**Baselines:** We do comparative studies on 10 baselines for OpenAI gym datasets. The first baseline simply checks the conditions of \(\) and applies corresponding actions in execution. The performance of this baseline shows that \(\) is imperfect and does not achieve the optimal reward. CQL SE is from  where the expert is replaced by \(\). The other baselines are an ensemble of \(\) and eight algorithms popular in the Offline RL literature for discrete environments. These algorithms include Behavior Cloning (BC) , Behaviour Value Estimation (BVE) , Quantile Regression DQN (QRDQN) , REM, MCE, BCQ, CQL and Critic Regularized Regression Q-Learning (CRR) . _For a fair comparison, we use actions from domain knowledge for states not in the buffer and actions from the trained policy for other states to obtain the final reward._ Hence, each algorithm is renamed with the suffix D in Table 5.1.

**Limiting Data:** To create limited-data settings for benchmark datasets, we first extract a small percentage of samples from the full dataset and remove some of the samples based on state conditions. This is done to ensure the reduced buffer satisfies the conditions defined in Def 4.1. We describe the specific conditions of removal in the next section. Further insights and the state visualizations for selected reduced datasets are in App H. **Note : no data reduction has been performed on SP dataset to demonstrate a real dataset exhibits characteristics of reduced buffer**.

### Performance across Different Datasets

Our results for OpenAI gym environments are summarised in Table 5.1 and Minigrid in Table 3 (App D). We observe the performance of offline RL algorithms degrades substantially when part of the data is not seen and trajectory ratios change. For these cases with only 10% partial data, ExID surpasses the performance by at least 27% in the presence of reasonable domain knowledge. The proposed method performs strongest on the replay dataset where the contribution of \(L_{r}()\) is significant due to state coverage, and the teacher learns from high-quality trajectories. Environment details are described in the App. D. All domain knowledge trees are shown in the App. D Fig 10. We describe limiting data conditions and domain knowledge specific to the environment as follows:

**Mountain Car Environment:** We use simple, intuitive domain knowledge in this environment shown in the App. D Fig 10 (c), which represents taking a left action when the car is at the bottom of the valley with low velocity to gain momentum; otherwise, taking the right action to drive the car up. Fig 6 (c) shows the state action pairs this rule generates on states sampled from a uniform random distribution over the state boundaries. It can be observed that the states of \(\) cover part of the missing

[MISSING_PAGE_FAIL:7]

the sales, but the cost will also increase. The goal for the platform operator is to maximize the total profit. The horizon of the dataset is 50 days for the training and 30 days for the test. Domain knowledge (, App A] : Active users can be given more coupons with lower discount to maximize profit. We model this as \(order_{number}>60 Avg_{fee}>0.8[5,0.95]\) where action 1 is number of coupons range  and action 2 is coupon value (discount value = (1-coupon value)) range [0.6,0.95]. The dataset exhibits the properties in Def 4.1 as first 50 days of sales does not contain many active users as reported in the coverage column of Tab 2 depicting scarcity. The domain rule is imperfect as coupon value and number depend on multiple factors such as user purchase history and behavior. As illustrated in the table 2 and Fig 3 (c) the intuitive domain rule enhances performance by 10.49% in the real dataset.

### Generalization to OOD states and contribution of \(_{r}()\)

In Fig 4 (a), (b), we plot \(Q_{s}^{}(s,a_{expert})-Q_{s}^{}(s,a_{})\) for CQL and EXID policies for different datasets of Mountain-Car environments. Action \(a_{expert}\) is obtained from the full expert dataset where position \(>-0.8\). We observe that the Q value for actions of CQL policy diverges from the expert policy actions with high values for the states not in the reduced buffer, whereas ExID stays close to the expert actions for the unseen states. This empirically shows generalization to OOD states not in the dataset but covered by domain knowledge. In Fig 4 (d), we plot the contribution by \(_{r}()\) during the training and observe the contribution is higher for replay data sets with more state coverage.

### Performance on varying \(\), \(k\), and ablation of \(_{t}^{}\)

We study the effect of varying \(\) on the algorithm for the given domain knowledge. We empirically observe setting a high or a low \(\) can yield sub-optimal performance, and \(=0.5\) generally gives good performance. In Fig 5 (a), we show this effect for LunarLander. Plots for other environments are in the App. G Fig 11. For \(k\) we observe setting the warm start parameter to 0 yields a sub-optimal policy, as the critic may update \(_{t}^{}\) without completely learning from it. The starting performance increases with an increase in \(k\) as shown in Fig 5 (b) for LunarLander. \(k=30\) works best according to empirical evaluations. Plots for other environments are in the App. G Fig 12. We show two ablations for Cart-pole in Fig 5 (c) with no teacher update after the warm start and no inclusion of \(_{r}()\) after the warm start. The warm start in this environment is set to 30 episodes. Fig 5 c) shows without teacher updated, the sub-optimal teacher drags down the performance of the policy beyond the warm start, exhibiting the necessity of \(_{t}^{}\) update. Also, the student converges to a sub-optimal policy if no \(_{r}()\) is included beyond the warm start.

  
**Dataset** & \(\) & coverage \(\) & CQL + \(\) & CQLSE & EXID & Performance gain \\  Sales & 654.68 & 20.32\% & 722.06 \(\) 71.40 & 727.03 \(\) & 802.91 & 10.49\% \\ Promotion & \(\) 20.06 & & & 49.56 & \(\) 41.69 & \\   

Table 2: Results on human generated Sales Promotion dataset

Figure 4: Q value difference between CQL and EXID for expert and policy action on states not present in the buffer for a) expert b) noisy in log scale c) contribution of \(_{r}()\)

### Effect of varying \(\) quality

We show the effect of choosing policies as \(\) with different average rewards for Lunar-Lander expert data in Fig 6 (a) and (b). Rule 1 is optimal and has almost the same effect as Rule 3, which is the \(\) used in our experiments exhibiting that updating a sub-optimal \(\) can lead to equivalent performance as optimal \(\). Using a rule with high uncertainty, as Rule 2, induces high uncertainty in the learned policy but performs slightly better than the baseline. Rule 4, which has a lower average reward, also causes gains on average performance with slower convergence. Finally, Rule 5, with very bad actions, affects policy performance adversely and leads to a performance lower than baseline CQL.

## 6 Conclusion and Limitation

In this paper, we study the effect of limited and partial data on offline RL and observe that the performance of SOTA offline RL algorithms is sub-optimal in such settings. The paper proposes a methodology to handle offline RL's performance degradation using domain insights. We incorporate a regularization loss in the CQL training using a teacher policy and refine the initial teacher policy while training. We show that incorporating reasonable domain knowledge in offline RL enhances performance, achieving a performance close to full data. However, this method is limited by the quality of the domain knowledge and the overlap between domain knowledge states and reduced buffer data. The study is also limited to discrete domains. In the future, the authors would like to improve on capturing domain knowledge into the policy network without dependence on data and extending the methodology to algorithms that handle continuous action space.

## 7 Broader Impact

During the trial-and-error training phase, RL agents may exhibit irrational behavior, which can be risky and costly in real-world scenarios. As a more practical alternative to online RL, offline RL

Figure 5: (a) Effect of different \(\) on the performance of ExID on Lunar Lander (b) Effect of different \(k\) on the performance of EXID on Lunar Lander (c) Performance of EXID with teacher update, no teacher update, and just warm start on Cart-pole.

Figure 6: (a) \(\) with different average rewards (b) Performance effect on Lunar-lander (c) State distribution generated for training the teacher network for mountain-carutilizes pre-existing collected data to eliminate the need for real-time interactions during training. However, a drawback of offline RL is its dependence on the quality and quantity of historical data, which, when sub-optimal, could adversely affect overall performance. Therefore, through this work, we use domain knowledge to suppress erroneous actions when available data is limited. However, this inclusion may facilitate harmful behavior in the presence of biased domain knowledge. Therefore, we advocate the use of well-regulated domain knowledge obtained from experts. Beyond this, we do not foresee any ethical impact on our work.