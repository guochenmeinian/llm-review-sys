# Distributional Reinforcement Learning with

Regularized Wasserstein Loss

 Ke Sun\({}^{1}\), Yingnan Zhao\({}^{2}\), Wulong Liu\({}^{3}\), Bei Jiang\({}^{1}\), Linglong Kong\({}^{1}\)

\({}^{1}\)University of Alberta, Edmonton, Canada

\({}^{2}\) Harbin Engineering University, China

\({}^{3}\)Huawei Noah's Ark Lab

{ksun6,bei1,lkong}@ualberta.ca

zhaoyingnan@hrbeu.edu.cn

liuwulong@huawei.com

Corresponding author

###### Abstract

The empirical success of distributional reinforcement learning (RL) highly relies on the choice of distribution divergence equipped with an appropriate distribution representation. In this paper, we propose _Sinkhorn distributional RL (SinkhornDRL)_, which leverages Sinkhorn divergence--a regularized Wasserstein loss--to minimize the difference between current and target Bellman return distributions. Theoretically, we prove the contraction properties of SinkhornDRL, aligning with the interpolation nature of Sinkhorn divergence between Wasserstein distance and Maximum Mean Discrepancy (MMD). The introduced SinkhornDRL enriches the family of distributional RL algorithms, contributing to interpreting the algorithm behaviors compared with existing approaches by our investigation into their relationships. Empirically, we show that SinkhornDRL consistently outperforms or matches existing algorithms on the Atari games suite and particularly stands out in the multi-dimensional reward setting. Code is available in [https://github.com/datake/SinkhornDistRL](https://github.com/datake/SinkhornDistRL)..

## 1 Introduction

The design of classical reinforcement learning (RL) algorithms primarily focuses on the expectation of cumulative rewards that an agent observes while interacting with the environment. Recently, a new class of RL algorithms called _distributional RL_ estimates the full distribution of total returns and has exhibited state-of-the-art performance in a wide range of environments, such as C51 , Quantile-Regression DQN (QR-DQN) , EDRL , Implicit Quantile Networks (IQN) , Fully Parameterized Quantile Function (FQF) , Non-Crossing QR-DQN , Maximum Mean Discrepancy (MMD-DQN) , Spline (SPL-DQN) , and Sketch-DQN . Beyond the performance advantage, distributional RL has also possessed benefits in risk-sensitive control , exploration , offline setting , statistical value estimation , robustness  and optimization .

**Limitations of Typical Distributional RL Algorithms.** Despite the gradual introduction of numerous algorithms, quantile regression-based algorithms  dominate attention and research in the realm of distributional RL. These algorithms utilize quantile regression to approximate the one-dimensional Wasserstein distance to compare two return distributions. Nevertheless, two major limitations hinder their performance improvement and wider practical deployment. _1) Inaccuracy in Capturing Return Distribution Characteristics._ The way of directlygenerating quantiles of return distributions via neural networks often suffers from the non-crossing issue , where the learned quantile curves fail to guarantee a non-decreasing property. This leads to abnormal distribution estimates and reduced model interpretability. The inaccurate distribution estimate is fundamentally attributed to the use of pre-specified statistics , while unrestricted statistics based on deterministic samples can be potentially more effective in complex environments .

_2) Difficulties in Extension to Multi-dimensional Rewards._ Many RL tasks involve multiple sources of rewards [32; 15], hybrid reward architecture [50; 30], or sub-reward structures after reward decomposition [31; 57], which require learning multi-dimensional return distributions to reduce the intrinsic uncertainty of the environments. However, it remains elusive how to use quantile regressions to approximate a multi-dimensional Wasserstein distance, while circumventing the computational intractability issue in the related multi-dimensional output space.

**Motivation of Sinkhorn Divergence: a Regularized Wasserstein loss.** Sinkhorn divergence  has emerged as a theoretically principled and computationally efficient alternative for approximating Wasserstein distance. It has gained increasing attention in the field of optimal transport [4; 24; 21; 39] and has been successfully applied in various areas of machine learning [38; 25; 54; 20; 8]. By introducing entropic regularization, Sinkhorn divergence can efficiently approximate a multi-dimensional Wasserstein distance using computationally efficient matrix scaling algorithms [45; 39]. This makes it feasible to apply optimal transport distances to RL tasks with multi-dimensional rewards (see experiments in Section 5.3). Moreover, Sinkhorn divergence enables the leverage of samples to approximate return distributions instead of relying on pre-specified statistics, e.g., quantiles, thereby increasing the accuracy in capturing the full data complexity behind return distributions and naturally avoiding the non-crossing issues in distributional RL. Beyond addressing the two main limitations mentioned above, the well-controlled regularization introduced in Sinkhorn divergence helps to find a "smoother" transport plan relative to Wasserstein distance, making it less sensitive to noises or small perturbations when comparing two return distributions (see Appendix A for the visualization). The term "smoother" refers to the effect of regularization in Sinkhorn divergence to encourage a more uniformly distributed transport plan. This regularization also aligns with the maximum-entropy principle [28; 16], which aims to maximize entropy while keeping the transportation cost constrained. Furthermore, the resulting strongly convex loss function  and the induced smoothness by regularization facilitate faster and more stable convergence in the deep RL setting (see more details in Sections 4 and 5).

**Contributions.** In this work, we propose a new family of distributional RL algorithms based on Sinkhorn divergence, a regularized Wasserstein loss, to address the limitations of quantile regression-based algorithms while promoting more stable training. As Sinkhorn divergence interpolates between Wasserstein distance and MMD [26; 21; 39], we probe this relationship in the RL context, characterizing the convergence properties of dynamic programming under Sinkhorn divergence and revealing the connections of different distances. Our study enriches the class of distributional RL algorithms, making them more effective for a broader range of scenarios and potentially inspiring advancement in other related areas of distribution learning. Our key contributions are summarized as follows:

**(1) Algorithm.** We introduce a Sinkhorn distributional RL algorithm, called SinkhornDRL, which overcomes the primary shortcomings of predominantly utilized quantile regression-based algorithms. SinkhornDRL can be seamlessly integrated into existing model architectures and easily implemented.

**(2) Theory.** We establish the properties of Sinkhorn divergence within distributional RL and derive the relevant convergence results for (multi-dimensional) distributional dynamic programming.

**(3) Experiments.** We conduct an extensive comparison of SinkhornDRL with typical distributional RL algorithms across 55 Atari games, performing rigorous sensitivity analyses and computation cost assessments. We also verify the efficacy of SinkhornDRL in the multi-dimensional reward setting.

## 2 Preliminary Knowledge

### Distributional Reinforcement Learning

In classical RL, an agent interacts with an environment via a Markov decision process (MDP), a 5-tuple (\(\), \(,R,P,\)), where \(\) and \(\) are the state and action spaces, \(P\) is the environment transition dynamics, \(R\) is the reward function, and \((0,1)\) is the discount factor.

Given a policy \(\), the discounted sum of future rewards \(Z^{}\) is a random variable with \(Z^{}(s,a)=_{t=0}^{}^{t}R(s_{t},a_{t})\), where \(s_{0}=s,a_{0}=a\), \(s_{t+1} P(|s_{t},a_{t})\), and \(a_{t}(|s_{t})\). In expectation-based RL, the action-value function \(Q^{}\) is defined as \(Q^{}(s,a)=[Z^{}(s,a)]\), which is iteratively updated via Bellman operator \(^{}\) through \(^{}Q(s,a)=[R(s,a)]+_{s^{} p,}[Q(s^{},a^{})]\), where \(s^{} P(|s,a)\) and \(a^{}(|s^{})\). In contrast, distributional RL focuses on the action-return distribution, the full distribution of \(Z^{}(s,a)\). The return distribution is iteratively updated by applying the distributional Bellman operator \(^{}\) through \(^{}Z(s,a):=(s,a)+ Z(s^{},a^{ })\), where \(D\) denotes the distribution and the equality implies random variables of both sides are equal in distribution. The distributional Bellman operator \(^{}\) is contractive under certain distribution divergence metrics .

### Divergences between Measures

**Optimal Transport (OT) and Wasserstein / Earth Mover's Distance.** The optimal transport (OT) metric \(W_{c}\) defines a powerful geometry to compare two probability measures \((,)\), i.e., \(W_{c}=_{(,)} c(x,y)(x,y)\), where \(c\) is the cost function, \(\) is the joint distribution with marginals \((,)\), and the minimizer \(^{*}\) is called the _optimal transport plan_ or _optimal coupling_. The \(p\)-Wasserstein distance \(W_{p}=(_{(,)}\|x-y\|^{p}(x,y))^{1/p}\) is a special case of optimal transport with the Euclidean norm as the cost function. Relative to conventional divergences, including Hellinger, total variation or Kullback-Leibler divergences, the formulation of OT and Wasserstein distance inherently integrates the spatial or geometric relationships between data points and allows them to recover the full support of measures. This theoretical advantage comes, however, with a heavy computational price tag, especially in the high-dimensional space. Specifically, finding the optimal transport plan amounts to solving a linear program and the cost scales at least in \((d^{3}(d))\) when comparing two histograms of dimension \(d\).

**Maximum Mean Discrepancy .** Define two random variables \(X\) and \(Y\). The squared Maximum Mean Discrepancy (MMD) \(_{k}^{2}\) with the kernel \(k\) is formulated as \(_{k}^{2}=[k(X,X^{})]+ [k(Y,Y^{})]-2[k(X,Y)]\), where \(k(,)\) is a continuous kernel and \(X^{}\) (resp. \(Y^{}\)) is a random variable independent of \(X\) (resp. \(Y\)). Mathematically, the "flat" geometry that MMD induces on the space of probability measures does not faithfully lift the ground distance , potentially inferior to OT when comparing two complicated distributions. However, MMD is cheaper to compute than OT with a smaller _sample complexity_, i.e., the number of samples for measures to approximate the true distance . We provide more details of various distribution divergences as well as their existing contraction properties in distributional RL in Appendix B.

**Notations.** We constantly use the _unrectified kernel_\(k_{}=-\|x-y\|^{}\) in our algorithm analysis. With a slight abuse of notation, we also use \(Z_{}\) to denote \(\) parameterized return distribution.

## 3 Related Work

Based on the choice of distribution divergences and the distribution representation, distributional RL algorithms can be classified into three categories.

**(1) Categorical Distributional RL.** As the first successful class, categorical distributional RL , e.g., C51, represents the return distribution using a categorical distribution with discrete fixed supports within a predefined interval.

**(2) Quantile Regression (Wasserstein Distance) Distributional RL.** QR-DQN  employs quantile regression to approximate the one-dimensional Wasserstein distance. It learns the quantile values for a series of fixed quantiles, offering greater flexibility in the support compared with categorical distributional RL. IQN  enhances this approach by utilizing an implicit model to produce more expressive quantile values, instead of fixed ones in QR-DQN, while FQF  further advances IQN by introducing a more expressive quantile network. However, as mentioned in Section 1, quantile regression distributional RL struggles with accurately capturing return distribution characteristics and handling multi-dimensional reward settings. SinkhornDRL, with the assistance of an entropy regularization, offers an alternative approach that addresses the two limitations simultaneously.

**(3) MMD Distributional RL.** Rooted in kernel methods [26; 53], MMD-DQN  learns unrestricted statistics, i.e., samples, to represent the return distribution and optimizes under MMD, which can manage multi-dimensional rewards. However, the data geometry captured by MMD with a specific kernel may be limited, as it is highly sensitive to the characteristics of kernels and the induced Reproducing Kernel Hilbert space (RKHS) [25; 26; 23]. In contrast, SinkhornDRL is fundamentally based on OT, inherently capturing the spatial and geometric layout of return distributions. This enables SinkhornDRL to potentially surpass MMD-DQN by leveraging a richer representation of data geometry. In Section 5, we present extensive experiments to demonstrate the advantage of SinkhornDRL over MMD-DQN, particularly in the multi-dimensional reward scenario in Section 5.3.

## 4 Sinkhorn Distributional RL (SinkhornDRL)

The algorithmic evolution of distributional RL can be primarily viewed along two dimensions . (1) Introducing new distributional RL families beyond the three established ones, leveraging alternative distribution divergences combined with suitable density estimation techniques. (2) Enhancing existing algorithms within a particular family by increasing their model capacity, e.g., IQN and FQF. Concretely, SinkhornDRL falls into the first dimension, aiming to expand the range of distributional RL algorithm families.

### Sinkhorn Divergence and New Convergence Properties in Distributional RL

Sinkhorn divergence  efficiently approximates the optimal transport problem by introducing an entropic regularization. It aims at finding a sweet trade-off that simultaneously leverages the geometry property of Wasserstein distance (optimal transport distances) and the favorable sample complexity advantage and unbiased gradient estimates of MMD [25; 21]. For two probability measures \(\) and \(\), the entropic regularized Wasserstein distance \(_{c,}(,)\) is formulated as

\[_{c,}(,)=_{(,)} c(x,y) (x,y)+(|), \]

where the entropic regularization \((|)=((x) (y)})(x,y)\), also known as _mutual information_, makes the optimization strongly convex and differential [3; 21], allowing for efficient matrix scaling algorithms for approximation, such as Sinkhorn Iterations . In statistical physics, \(_{c,}(,)\) can be re-factored as a projection problem:

\[_{c,}(,):=_{(,)} (|), \]

where \(\) is the Gibbs distribution and its density function satisfies \(d(x,y)=e^{-c(x,y)/}d(x)d(y)\). This problem is often referred to as the "static Schrodinger problem" [29; 44] as it was initially considered in statistical physics. Formally, the Sinkhorn divergence is defined as

\[}_{c,}(,)=2_{c,}( ,)-_{c,}(,)-_{c,}(, ), \]

which is smooth, positive definite, and metricizes the convergence in law . This definition subtracts two self-distance terms to ensure non-negativity and metric properties.

**Properties for Convergence.** The contraction analysis of distributional Bellman operator \(^{}\) under a distribution divergence \(d_{p}\) depends on its _scale sensitive_ (**S**) and _sum invariant_ (**I**) properties [6; 5]. We say \(d_{p}\) is scale sensitive (of order \(\)) if there exists a \(>0\), such that for all random variables \(X,Y\) and a real value \(a>0\), \(d_{p}(aX,aY)|a|^{}d_{p}(X,Y)\). \(d_{p}\) has the sum invariant property if whenever a random variable \(A\) is independent from \(X,Y\), we have \(d_{p}(A+X,A+Y) d_{p}(X,Y)\). Based on these properties,  shows that \(^{}\) is \(\)-contractive under the supremal form of Wasserstein distance \(W_{p}\), which is regarding the first term of \(_{c,}\) or directly letting \(=0\) in Eq. 1. When examining the regularized loss form of \(_{c,}\), a natural question arises: _What is the influence of the incorporated regularization term on the contraction of \(^{}\)?_ We begin to address this question in Proposition 1, focusing on the separate regularization term. Here, we define mutual information as \(_{}((s,a),(s,a))=(|(s,a)(s,a))\) and its supremal form \(_{}^{}(,)=_{(s,a)} (|(s,a)(s,a))\) given a joint distribution \(\).

**Proposition 1**.: \(^{}\) _is non-expansive under \(_{}^{}\) for any non-trivial joint distribution \(\)._

Please refer to Appendix C for the proof, where we investigate both **(S)** and **(I)** properties. The non-trivial \(\) rules out the independence case of \(\) and \(\), where \((|)\) would degenerate to zero. Although the non-expansive nature of the introduced regularization term, as shown in Proposition 1, may potentially slow the convergence in Sinkhorn divergence compared with \(W_{p}\) without the regularization, we will demonstrate that \(^{}\) is still contractive under the full Sinkhorn divergence in Theorem 1. Before introducing Theorem 1, we first present the sum-invariant and a new variant of scale-sensitive properties in Proposition 2, which acts as the foundation for Theorem 1.

**Proposition 2**.: _Considering \(_{c,}\) with the unrectified kernel \(k_{}:=-\|x-y\|^{}\) as \(-c\) (\(>0\)) and a scaling factor \(a(0,1)\), \(_{c,}\) is sum-invariant **(I)** and satisfies \(_{c,}(a,a)_{}(a,) _{c,}(,)\) **(S)** with a scaling constant \(_{}(a,)(|a|^{},1)\) for any \(\) and \(\) in a finite set of probability measures._

Proof Sketch.: The detailed proof is provided in Appendix D. Let \(^{*}\) be the optimal coupling of \(_{c,}\), we define a ratio \(_{}(,)\) that satisfies \(_{}(,)=(^{*}| )}{_{c,}}(0,1)\) for a generally non-zero \(_{c,}\). The ratio \(_{}(,)\) measures the proportion of the entropic regularization term over the whole loss term \(_{c,}\). Therefore, the contraction factor \(_{}(a,)\) is defined as \(_{}(a,)=|a|^{}(1-_{,}_{ }(,))+_{U,V}_{}(,))(|a|^{ },1)\) with \(_{,}_{}(,)<1\), which is determined by the scale factor \(a\), the order \(\), the hyperparameter \(\), and the set of interested probability measures.

**Contraction Guarantee and Interpolation Relationship.** Proposition 2 reveals that \(_{c,}\) with an unrectified kernel satisfies **(I)** and a variant of **(S)** properties. While the scaling constant \(_{}(a,)\) in **(S)** has a complicated form, it remains strictly less than one, even considering a non-expansive nature of the entropic regularization as shown in Proposition 1. We denote the supremal form of Sinkhorn divergence as \(}_{c,}^{}(,):}_{c,}^{}(,)=_{(s,a)}}_{c,}((s,a),(s,a))\). In Theorem 1, we will integrate all these properties to demonstrate the contraction property of distributional dynamic programming under \(}_{c,}\), specifically highlighting the interpolation property of Sinkhorn divergence between MMD and Wasserstein distance in the context of distributional RL.

**Theorem 1**.: _Considering \(}_{c,}(,)\) with an unrectified kernel \(k_{}:=-\|x-y\|^{}\) as \(-c\) (\(>0\)), where \(,\) the distribution set of \(\{Z^{}(s,a)\}\) for \(s\), \(a\) in a finite MDP. We define the ratio \(_{}(,)\) as \(_{}(,)=(^{* }|)}{_{c,}(,)}(0,1)\) with \(_{,}_{}(,)<1\). Then, we have:_

_(1) \(( 0)\)\(}_{c,}(,) 2W_{}^{}(,)\). When \(=0\), \(^{}\) is \(^{}\)-contractive under \(}_{c,}^{}\)._

_(2) \((+)\)\(}_{c,}(,)_{k_{}}^{2}(,)\). When \(=+\), \(^{}\) is \(^{}\)-contractive under \(}_{c,}^{}\)._

_(3) \(((0,+))\), \(^{}\) is at least \(_{}(,)\)-contractive under \(}_{c,}^{}\), where \(_{}(,)\) is an MDP-dependent constant defined as \(_{}(,)=^{}(1-_{, }_{}(,))+_{,}_{}(,))(^{},1)\)._

Proof Sketch.: The detailed proof of Theorem 1 can be found in Appendix E. Theorem 1 (1) and (2) are follow-up conclusions in terms of the convergence behavior of \(^{}\) based on the interpolation relationship between Sinkhorn divergence with Wasserstein distance and MMD . We also provide a rigorous analysis within the context of distributional RL for completeness. Our critical theoretical contribution is the part (3) for the general \((0,)\), where we show that \(^{}\) is at least a \(_{}(,)\)-contractive operator. The contraction factor \(_{}(,)(^{},1)\) depends on the return distribution set \(\{Z^{}(s,a)\}\) of the considered MDP, and it is also a function of \(,\) and \(\). Due to the influence of the regularization term in Sinkhorn loss, \(_{}(,)\) is larger than \(||^{}\), the contraction factor for Wasserstein distance without the regularization. Thus, \(_{}(,)\) can be seen as an interpolation between \(^{}\) and 1, with the coefficient \(_{,}_{}(,)(0,1)\) defined in Theorem 1. The ratio \(_{}(,)\) measures the proportion of the KL regularization term relative to \(}_{c,}\). As \( 0\) or \(+\), \(_{,}_{}(,) 0\), leading to \(^{}\)-contraction. This aligns with parts (1) and (2).

**Consistency with Existing Contraction Conclusions.** As Sinkhorn divergence interpolates between Wasserstein distance and MMD, its contraction property for \([0,]\) also aligns well with the existing distributional RL algorithms when \(c=-k_{}\). It is worth noting that using Gaussian kernels in the cost function does not yield concise or consistent contraction results like those in Theorem 1 (3). This conclusion is consistent with MMD-DQN  (\(+\)), where \(^{}\) is generally not a contraction operator under MMD with Gaussian kernels, as counterexamples exist (Theorem 2) in . Guided by our theoretical results, we employ the rectified kernel \(k_{}\) as the cost function and set \(=2\) in our experiments, ensuring that \(^{}\) retains the contraction property guaranteed by Theorem 1 (3). In Table 1, we also summarize the main properties of distribution divergences in typical distributional RL algorithms, including the convergence rate of \(^{}\) and sample complexity,i.e., the convergence rate of a given metric between a measure and its empirical counterpart as a function of the number of samples \(n\).

### Extension to Multi-dimensional Return Distributions

As the ability to extend to the multi-dimensional reward setting is one of the major advantages of SinkhornDRL over quantile regression-based algorithms, we next demonstrate that the joint distributional Bellman operator in the multi-dimensional reward case is contractive under Sinkhorn divergence \(}_{c,}^{}\). First, we define a \(d\)-dimensional reward function as \(: P(^{d})\), where \(d\) represents the number of reward sources. Consequently, we have joint return distributions of the \(d\)-dimensional return vector \(^{}(s,a)=_{t=0}^{}^{t}(s_{t},a_{t})\), where \(^{}(s,a)=(Z_{1}^{}(s,a),,Z_{d}^{}(s,a))^{}\). The joint distributional Bellman operator \(_{d}^{}\) applied on the joint distribution of the random vector \((s,a)\) is defined as \(_{d}^{}(s,a):}{{=}} (s,a)+(s^{},a^{})\), where \(s^{} P(|s,a)\), \(a^{}(|s^{})\).

**Corollary 1**.: _For two joint distributions \(_{1}\) and \(_{2}\), \(_{d}^{}\) is \(_{}(,)\)-contractive under \(}_{c,}^{}\), i.e.,_

\[}_{c,}^{}(^{}_{ 1},^{}_{2})_{}( ,)}_{c,}^{}(_{1}, _{2}). \]

Please refer to Appendix F for the proof. The contraction guarantee of Sinkhorn divergence enables us to effectively deploy our SinkhornDRL algorithm in various RL tasks that involve multiple sources of rewards [32; 15], hybrid reward architecture [50; 30], or sub-reward structures after reward decomposition [31; 57]. We compare SinkhornDRL with MMD-DQN in multiple reward sources setting in Section 5.3, where SinkhornDRL significantly outperforms MMD-DQN by leveraging its ability to capture richer data geometry, a key advantage of optimal transport distances.

### SinkhornDRL Algorithm and Approximation

**Equipping Sinkhorn Divergence and Particle Representation.** The key to applying Sinkhorn divergence in distributional RL is to leverage the Sinkhorn loss \(}_{c,}\) to measure the distance between the current action-return distribution \(Z_{}(s,a)\) and the target distribution \(^{}Z_{}(s,a)\). This yields \(}_{c,}(Z_{}(s,a),^{}Z_{ }(s,a))\) for each \(s,a\) pair. For the representation of \(Z_{}(s,a)\), we employ the unrestricted statistics, i.e., deterministic samples, akin to MMD-DQN, instead of predefined statistic functionals like quantiles in QR-DQN or categorical distributions in C51. More concretely, we use neural networks to generate samples to approximate the return distributions, expressed as \(Z_{}(s,a):=\{Z_{}(s,a)\}_{i=1}^{N}\), where \(N\) is the number of generated samples. We refer to these samples \(\{Z_{}(s,a)_{i}\}_{i=1}^{N}\) as _particles_. We then use the Dirac mixture \(_{i=1}^{N}_{Z_{}(s,a)_{i}}\) to approximate the

```
0: Number of generated samples \(N\), the cost function \(c\), hyperparameter \(\) and the target network \(Z_{^{*}}\).
0: Sample transition \((s,a,r^{},s^{})\)
1: Policy evaluation: \(a^{*}(|s^{})\) or Control: \(a^{*}_{a^{}}_{i=1}^{N}Z_ {}(s^{},a^{})_{i}\)
2:\(Z_{i} r+ Z_{^{*}}(s^{},a^{*} )_{i}, 1 i N\)
3: Output:\(}_{c,}(\{Z_{}(s,a)_{i}\}_{i=1}^{N}, \{Z_{j}\}_{j=1}^{N})\)
```

**Algorithm 1** Generic Sinkhorn distributional RL Update

**Require**: Number of generated samples \(N\), the cost function \(c\), hyperparameter \(\) and the target network \(Z_{^{*}}\).

**Input**: Sample transition \((s,a,r^{},s^{})\)

1: Policy evaluation: \(a^{*}(|s^{})\) or Control: \(a^{*}_{a^{}}_{i=1}^{N}Z_ {}(s^{},a^{})_{i}\)
2:\(Z_{i} r+ Z_{^{*}}(s^{},a^{*} )_{i}, 1 i N\)
3: Output:\(}_{c,}(\{Z_{}(s,a)_{i}\}_{i=1}^{N}, \{Z_{j}\}_{j=1}^{N})\) ```

**Algorithm 2** Generic Sinkhorn divergence and particle representation

**Output**:\(}_{c,}(\{Z_{}(s,a)_{i}\}_{i=1}^{N}, \{Z_{j}\}_{j=1}^{N})\)

  
**Algorithm** & \(d_{p}\) **Distribution Divergence** & **Representation \(Z_{}\)** & **Convergence Rate of \(^{}\)** & **Sample Complexity of \(d_{p}\)** \\  C51 & Cramer distance & Categorical Distribution & \(\) & \\ QR-DQN-1 & Wasserstein distance & Quantiles & \(^{/2}(k_{a})\) & \((n^{-1})\) \\ MMD-DQN & MMD & Samples & \(^{/2}(k_{a})\) & \((n^{-1})\) \\  SinkhornDRL & Sinkhorn divergence & & \(( 0)\) & \((n^{-1/2/2})\) (\( 0\)) \\ (ours) & (\(c=-k_{a}\)) & Samples & \(^{/2}()\) & \((n^{-})()\) \\   

Table 1: Properties of different distribution divergences in typical distributional RL algorithms. \(d\) is the sample dimension and \(=2 d+\|c\|_{}\), where the cost function \(c\) is \(\)-Lipschitz . Sample complexity is improved to \((1/n)\) using the kernel herding technique  in MMD.

true density function of \(Z^{}(s,a)\), thus minimizing the Sinkhorn divergence between the approximate distribution and its distributional Bellman target. A generic Sinkhorn distributional RL algorithm with particle representation is provided in Algorithm 1.

**Efficient Approximation via Sinkhorn Iterations with Guarantee.** By introducing an entropy regularization, Sinkhorn divergence renders optimal transport computationally feasible, especially in the high-dimensional space, via efficient algorithms, e.g., Sinkhorn Iterations [45; 25]. Notably, Sinkhorn iteration with \(L\) steps yields a differentiable and solvable efficient loss function as the main burden is the matrix-vector multiplication, which streams well on the GPU by simply adding extra differentiable layers on the typical deep neural network, such as a DQN architecture. _It has been proven that Sinkhorn iterations asymptotically converge to the true loss in a linear rate_[25; 22; 12; 27]. We provide a detailed description of Sinkhorn iterations in Algorithm 2 and a full version in Algorithm 3 of Appendix G. In practice, selecting proper values of \(L\) and \(\) is crucial. To this end, we conduct a rigorous sensitivity analysis, detailed in Section 5.

**Remark: Relationship with IQN and FQF.** In the realm of distributional RL algorithms, it is important to highlight that QR-DQN and MMD-DQN are direct counterparts to SinkhornDRL within the first dimension of algorithmic evolution. In contrast, IQN and FQF enhance QR-DQN and position them in the second modeling dimension, which are orthogonal to our work. As discussed in , the techniques from IQN and FQF can naturally extend both MMD-DQN and SinkhornDRL. For instance, we can implicitly generate \(\{Z_{}(s,a)_{i}\}_{i=1}^{N}\) by applying a neural network to \(N\) samples of a base sampling distribution, as in IQN. We can also use a proposal network to learn the weights of each generated sample as in FQF. We leave these modeling extensions as future works and our current study focuses on rigorously investigating the simplest modeling choice via Sinkhorn divergence.

## 5 Experiments

We substantiate the effectiveness of SinkhornDRL as described in Algorithm 1 on the entire 55 Atari 2600 games. Without increasing the model capacity for a fair comparison, we leverage the same architecture as QR-DQN and MMD-DQN, and replace the quantiles output in QR-DQN with \(N\) particles (samples). In contrast to MMD-DQN, SinkhornDRL only changes the distribution divergence from MMD to Sinkhorn divergence. As such, the potential performance improvement of our algorithm is directly attributed to the theoretical advantages of Sinkhorn divergence over MMD.

**Baseline Implementation.** We choose DQN  and three typical distributional RL algorithms as classic baselines, including C51 , QR-DQN  and MMD-DQN . For a fair comparison, we build SinkhornDRL and all baselines based on a well-accepted PyTorch implementation2 of distributional RL algorithms. We re-implement MMD-DQN based on its original TensorFlow implementation3, and keep the same setting. For example, our MMD-DQN still employs Gaussian kernels \(k_{h}(x,y)=(-(x-y)^{2}/h)\) with the same kernel mixture trick covering a range of bandwidths \(h\) as adopted in MMD-DQN .

**SinkhornDRL Implementation and Hyperparameter Settings.** For a fair comparison with QR-DQN, C51, and MMD-DQN, we use the same hyperparameters: the number of generated samples \(N=200\), Adam optimizer with \(=0.00005\), \(_{}=0.01/32\). In SinkhornDRL, we choose the number of Sinkhorn iterations \(L=10\) and smoothing hyperparameter \(=10.0\) in Section 5.1 after conducting sensitivity analysis in Section 5.2. Guided by the contraction guarantee analyzed in Theorem 1, we use _the unrectified kernel_, specifically setting \(-c=k_{}\) and choosing \(=2\). This choice ensures _our implementation is consistent with the theoretical results regarding the contraction guarantee in Theorem 1 (3)_. We evaluate all algorithms on 55 Atari games, averaging results over three seeds. The shade in the learning curves of each game represents the standard deviation.

### Performance of SinkhornDRL

**Learning Curves of Human Normalized Scores (HNS).** We compare the learning curves of the Mean, Median, and Interquartile Mean (IQM)  across all considered distributional RL algorithms in Figure 1 summarized over 55 Atari games. The IQM (x\(\%\)) computes the mean from the \(x\%\) to \((1-x)\%\) range of HNS, providing a robust alternative to the Mean that mitigates the impact of extremely high scores on specific games and is more statistically efficient than the Median. For computational feasibility, we evaluate the algorithms over 40M training frames. Our findings reveal that SinkhornDRL achieves state-of-the-art performance in terms of mean, median, and IQM (5\(\%\)) of HNS across most training phases. Notably, SinkhornDRL exhibits slower convergence during the early training phase, as indicated by the Mean of HNS (left panel of Figure 1). This slower initial convergence can be explained by the slower contraction factor \(_{e}(,)>^{}\) in Theorem 1, as opposed to MMD-DQN. To ensure the reliability of our results, we also provide the learning curves for each Atari game in Figure 6 in Appendix I. Furthermore, a table summarizing all raw scores is available in Table 3 in Appendix J. This table highlights that SinkhornDRL achieves the highest numbers of best and second-best performance of all games among all baseline algorithms. A summary table of Mean, IQM, and Median HNS is also given in Table 2 of Appendix H. Overall, we conclude that SinkhornDRL generally outperforms existing distributional RL algorithms.

**Ratio Improvement Analysis across All Games.** Given the interpolation nature of Sinkhorn divergence between Wasserstein distance and MMD, as analyzed in Theorem 1, a pertinent question arises: _In which environments does SinkhornDRL potentially perform better?_ We empirically address this question by conducting a ratio improvement comparison between SinkhornDRL and both QR-DQN and MMD-DQN across all games. Figure 2 showcases that SinkhornDRL surpasses both QR-DQN and MMD-DQN in more than half of the games and significantly exceeds at them in a large proportion of games. Notably, _the games where SinkhornDRL achieves considerable improvement tend to have larger action spaces and more complex dynamics._ In particular, as illustrated in Figure 2, these games include Venture, Seaquest, Solaris, Tennis, Phoenix, Atlantis, and Zaxxon. Most of these games have an 18-dimensional action space and intricate dynamics, except for Atlantis, which has a 4-dimensional action space and simpler dynamics where MMD-DQN is substantially inferior to SinkhornDRL. For a detailed comparison, we provide the features of all games, including the number of action spaces, and complexity of environment dynamics in Table 4 of Appendix K.

In summary, compared with QR-DQN, the empirical success of SinkhornDRL can be attributed to several key factors: 1. _Enhanced return distribution representation:_ SinkhornDRL captures return

Figure 1: Mean (left), Median (middle), and IQM (5\(\%\)) (right) of Human-Normalized Scores (HNS) summarized over 55 Atari games. We run 3 seeds for each algorithm.

Figure 2: Ratio improvement of return for SinkhornDRL over QR-DQN (left) and MMD-DQN (right) averaged over 3 seeds. The ratio improvement is calculated by (SinkhornDRL - QR-DQN) / QR-DQN in (a) and (SinkhornDRL - MMD-DQN) / MMD-DQN in (b), respectively.

distribution characteristics more accurately by directly using samples, avoiding the non-crossing issue of learned quantile curves or the potential limitations of quantile representation. _2. Smooth transport plan and stable convergence._ The induced smoother transport plan (see Appendix A for visualization) and the inherent smoothness of Sinkhorn divergence contribute to more stable convergence, leading to performance improvement. In contrast to MMD-DQN, the benefits of SinkhornDRL arise from its richer data representation capability when comparing return distributions, rooted in the OT nature. This is in comparison to the potentially restricted kernel-specific distances, such as MMD.

### Sensitivity Analysis and Computational Cost

**Sensitivity Analysis.** In practice, a proper \(\) is preferable as an overly large or small \(\) will lead to numerical instability of Sinkhorn iterations in Algorithm 2 (see the discussion in Section 4.4 of ), therefore worsening its performance, as shown in Figure 3 (a). This implies that the potential interpolation nature of limiting behaviors between SinkhornDRL with QR-DQN and MMD-DQN revealed in Theorem 1 may not be able to be rigorously verified in numerical experiments. SinkhornDRL also requires a proper number of iterations \(L\) and samples \(N\). For example, a small \(N\), e.g., \(N=2\) in Seaquest in Figure 3 (b) leads to the divergence of algorithms, while an overly large \(N\) can degrade the performance and meanwhile increases the computational burden (Appendix L.2). We conjecture that using larger networks to represent more samples is more likely to suffer from overfitting, yielding the instability in the RL training . Therefore, we choose \(N=200\) to attain favorable performance and guarantee computational effectiveness simultaneously. We provide a more detailed sensitivity analysis and more results on StarGunner and Zaxxon in Appendix L.1.

**Computation Cost.** In terms of the computation cost, SinkhornDRL slightly increases the computational overhead compared with C51, QR-DQN, and MMD-DQN. For instance, SinkhornDRL increases the average computational cost compared with MMD-DQN by around 20\(\%\). Due to the space limit, we provide more computation cost comparison in terms of \(L\) and \(N\) in Appendix L.2.

### Modeling Joint Return Distribution for Multi-Dimensional Reward Functions

Many RL tasks involve modeling multivariate return distributions. Following the multi-dimensional reward setting in , we compare SinkhornDRL with MMD-DQN on six Atari games with multiple sources of rewards. In these tasks, the primitive scalar-based rewards are decomposed into reward vectors based on the respective reward structures (see Appendix M for more details). Figure 4 showcases that SinkhornDRL outperforms MMD-DQN in most cases for multi-dimensional reward functions. Of particular note, it remains an open question to directly approximate multi-dimensional Wasserstein distances via quantile regression or other efficient algorithms, particularly in RL tasks.

Figure 4: Performance of SinkhornDRL on six Atari games with multi-dimensional reward functions.

Figure 3: Sensitivity analysis of SinkhornDRL on Breakout and Seaquest in terms of \(\), number of samples, and number of iteration \(L\). Learning curves are reported over three seeds.

Conclusion, Limitations and Future Work

In this work, we propose a novel family of distributional RL algorithms based on Sinkhorn divergence that accomplishes competitive performance compared with the typical distributional RL algorithms on the Atari games suite. Theoretical results about the properties of this regularized Wasserstein loss and its convergence guarantee in the context of RL are provided with rigorous empirical verification.

**Limitations.** While SinkhornDRL achieves competitive performance, it relatively increases the computational cost and requires tuning additional hyperparameters. This hints that the enhanced performance offered by SinkhornDRL may come with slightly greater efforts in practical deployment. Additionally, it remains elusive for a deeper connection between the theoretical properties of divergences and the practical performance of distributional RL algorithms given a specific environment.

**Future work.** Along the two dimensions of distributional RL algorithm evolution, we can further improve Sinkhorn distributional RL by incorporating implicit generative models, including parameterizing the cost function and increasing model capacity. Moreover, Sinkhorn distributional RL also opens a door for new applications of Sinkhorn divergence and more optimal transport approaches in RL. It also becomes increasingly crucial to design a quantitative criterion for a given environment to recommend the choice of a specific distribution divergence before conducting costly experiments.