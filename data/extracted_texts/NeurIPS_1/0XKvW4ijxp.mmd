# Learning-Augmented Algorithms with Explicit Predictors

Marek Elias

Bocconi University, Milan

&Haim Kaplan

Tel Aviv University

Google Research &Yishay Mansour

Tel Aviv University

Google Research &Shay Moran

Department of Mathematics, Technion

Department of Computer Science, Technion

Department of Data and Decision Sciences, Technion

Google Research.

###### Abstract

Recent advances in algorithmic design show how to utilize predictions obtained by machine learning models from past and present data. These approaches have demonstrated an enhancement in performance when the predictions are accurate, while also ensuring robustness by providing worst-case guarantees when predictions fail. In this paper we focus on online problems; prior research in this context was focused on a paradigm where the algorithms are oblivious of the predictors' design, treating them as a black box. In contrast, in this work, we unpack the predictor and integrate the learning problem it gives rise for within the algorithmic challenge. In particular we allow the predictor to learn as it receives larger parts of the input, with the ultimate goal of designing online learning algorithms specifically tailored for the algorithmic task at hand. Adopting this perspective, we focus on a number of fundamental problems, including caching and scheduling, which have been well-studied in the black-box setting. For each of the problems, we introduce new algorithms that take advantage of explicit and carefully designed learning rules. These pairings of online algorithms with corresponding learning rules yields improvements in the overall performance in comparison with previous work.

## 1 Introduction

We study online algorithmic problems within the realm of learning-augmented algorithms. A learning-augmented algorithm possesses the capability to work in conjunction with an oracle that supplies predictive information regarding the data it is expected to process. This innovative approach has been discussed in landmark studies by Kraska et al. (2018) and Lykouris and Vassilvitskii (2021), situating it neatly within the "beyond worst-case analysis" framework (Roughgarden, 2020, chap. 30).

In this framework, studies typically define predictions specifically tailored to the problem at hand, which could presumably be learnt from historical data. Predictions might include, for instance, the anticipated next request time for a page in a caching problem or the expected duration of a job in a scheduling task. These predictions are accessible either before or together with the online requests, allowing the algorithm to utilize them for performance enhancement (measured by competitive ratio or regret). The objective is for the algorithm's performance to gracefully decrease as prediction accuracy declines, ensuring it never underperforms the baseline achievable without predictions.

Despite the elegance of these results, several important aspects were neglected: the ad-hoc nature of the predictions, the lack of standardized quality measures, and the frequently overlooked prediction generation methods and their relation to the algorithmic process. We believe that addressing these aspects is likely to yield substantial improvements.

Consider week-day and festive traffic patterns in a city - a simple example of a setting with two typical inputs requiring very different predictive and algorithmic strategies. Achieving a good performance in such setting requires a learning component in the algorithm which discerns between the festive and week-day input instances and suggests an appropriate routing strategy. Such learning components are already present (explicitly or implicitly) in works on combining algorithms in a black-box manner (Dinitz et al., 2022; Emek et al., 2021; Anand et al., 2022; Antoniadis et al., 2023), where a switch between algorithms is made after incurring a high cost.

Our approach goes one step further. It is based on making the computation of the predictions an integral part of the algorithmic task at hand. We do this by making all the data (historical and current) directly available to the online algorithm (rather than summarizing it into ad-hoc predictions). This gives the algorithm two important abilities. The first ability is to learn the input instance based on its prefix (the shorter the better) and adapt the algorithmic strategy before incurring significant cost. E.g., in the example above, week-day and festive traffic patterns can be easily discerned already in early morning when the traffic is low and possibly suboptimal routing decisions have negligible impact on the overall cost. The second ability is to identify actions which are beneficial for many plausible input instances simultaneously but are not suggested by any of the black-box predictors. We use both abilities in design of our algorithms.

In more detail, we model the past data through the assumption that the algorithm is equipped with prior knowledge comprising a set of descriptions of past input instances. Each description offers specific statistics or characteristics that represent essential information of the input instance. Borrowing terminology from learning theory, we call this set a _hypothesis class_ and denote it by \(\). More specifically, \(\) is a set of hypotheses, where each hypothesis, \(h(I)\), consists of information regarding a specific possible input instance \(I\) of the algorithmic task.

In the simplest setting each hypothesis could be the input instance itself (\(h(I)=I\)). Like the sequence of pages to arrive in a caching instance, or a set of jobs to arrive in a scheduling instance. In other situations, an hypothesis \(h(I)\) could be a more compact summary of the instance \(I\), such as the distribution of the arriving jobs (what fraction are of each "type"). In such case, many past input instances will correspond to the same hypothesis and the size of \(\) will be much smaller than the size of the dataset of past instances. However, in all cases that we consider, each hypothesis \(h(I)\) provides sufficient information about the instance in order to determine an offline optimal solution \((I)\).

We distinguish between _realizable_ and _agnostic_ settings. In the _realizable_ setting, we make the assumption that the actual input instance that the online algorithm has to serve, perfectly aligns with one of the hypotheses in \(\). That is, if \(I\) is the real input, then \(h(I)\). In the _agnostic_ setting we remove this assumption and consider arbitrary inputs. Our goal is to deliver performance guarantees that improve if the actual input is "close" (defined in a suitable manner) to the hypothesis class \(\). Agnostic setting with \(||=1\) captures the (usual) setting of ML-augmented algorithms with a single black-box predictor. The realizable case is interesting mostly from a theoretical perspective as a very special case of the agnostic setting. Its simplicity makes it a logical starting point of a study.1 If the current instance does not match any hypothesis perfectly (in the realizable setting) or is far from \(\) (in the agnostic setting), we can still achieve good performance using robustification techniques, see e.g. (Wei, 2020; Antoniadis et al., 2023; Lattanzi et al., 2020; Lindermayr and Megow, 2022).

Our methodology is to split the algorithm into two parts. One (called _predictor_) produces predictions based on the provided hypothesis class (\(\)) and the part of the input seen so far. Its goal is to produce the best prediction for the instance at hand which could be a hypothesis from \(\) or some other suitable instance designed based on \(\). The second part is the online algorithm itself. It uses the prediction of the first part to serve the input instance with low cost. In particular, it can compute the offline optimal solution for the prediction and serve the input instance according to this solution.

The predictor is the learning component of our algorithm. It solves a learning task which is associated with the algorithmic problem at hand. For example, the learning task associated with caching is a variant of online learning with two kinds of costs: the smaller cost is due to a misprediction of an individual request and the larger one due to _switching_ to a different predicted sequence. The costs are chosen to reflect the impact of the two events on the overall performance of the algorithm.

We consider this new way to model a setting of "online algorithm with predictions" as one of our core contributions (in addition to the algorithms for the specific problems that we describe below). In a sense, our technique interpolates in an interesting way between the learning challenge (from historical data) and the algorithmic challenge, while addressing both of them.

### Performance bounds of our algorithms

We propose algorithms (within our framework) for three fundamental online algorithmic problems: caching, load balancing, and non-clairvoyant scheduling. For caching and non-clairvoyant scheduling, we achieve a (small) additive regret compared to the offline optimal solution instead of a multiplicative competitive ratio. For load balancing, we achieve a competitive ratio with logarithmic dependence on the size \(\) of the hypothesis class. Our results are summarized in Figure 1, while the full description is deferred to Section 2.

We focus our presentation on the basic framework with performance bounds dependent on the size of the hypothesis class \(\). We assume \(\) to be restricted in the sense that not every instance \(I\) is close to some hypothesis in \(\). This ensures that there is some structure in the input instances which can be learnt. With an unrestricted \(\), every input would be possible and we would be in the classical online setting. However, our modular approach allows replacing the learning component in order to achieve additional desirable properties. This includes fast running time, better performance with clusterable datasets (see Dinitz et al. (2022)), and better performance on instances composed of several parts, each resembling a different hypothesis (see Anand et al. (2022); Antoniadis et al. (2023)).

Recent works by Dinitz et al. (2022) and Emek et al. (2021) consider algorithms with access to a portfolio of predictors trying to achieve performance comparable to the best one. Our results can be interpreted in their setting by considering the output of each predictor in the portfolio as a hypothesis. We achieve comparable and sometimes better results (see Figure 1 for comparison) using an arguably simpler approach, separating the learning and algorithmic part and solving them separately.

Organization.Section 2 describes our main contributions including the description of the problems studied and the approach which leads to our results. The survey of the related literature in Section 3 is followed by a warm-up in Section 4 containing an exposition of our approach on caching in realizable setting. The main technical part of our paper is in Appendix B considers the load balancing problem and Appendix C the non-clairvoyant scheduling problem. We conclude our paper with treatment of caching in agnostic setting in Appendix D.

## 2 Main Results

Our study focuses on three fundamental online algorithmic problems: caching, load balancing, and non-clairvoyant scheduling. For each of these problems, we define learning tasks and devise explicit and efficient predictors for solving them. We demonstrate how these predictors can be integrated into algorithms designed to tackle the respective online problems. A key feature of our approach is the

Figure 1: Summary of our results. Notation: \(=||\); \(k\) and \(T\): cache size and instance length respectively in caching; \(m\): the number of machines in load balancing; \(n\): the number of jobs in non-clairvoyant scheduling; \(^{*}\): distance of the input from the hypothesis class in caching and non-clairvoyant scheduling; \(^{*}\): cost of the best algorithmic strategy suggested by \(\).

modular separation of the learning and algorithmic components. By decoupling these aspects, we develop simpler algorithms that often yield improved bounds compared to previous works in the field.

### Caching

In the caching problem, the input is a sequence of page requests. The online algorithm holds a cache of size \(k\), and it must ensure that the currently requested page is always available in the cache. If the requested page is absent from the cache, a page fault occurs, prompting the page to be loaded into the cache. If the cache is already full, another page must be evicted to make room. The ultimate objective is to minimize the number of page faults.

In the offline scenario, where the input sequence is known ahead of time, an optimal algorithm adheres to the intuitive policy of removing a page that will not be requested again for the longest time. This algorithm, known as Furthest in the Future (FitF) [Belady, 1966], achieves the minimum possible number of page faults.

#### The Learning Task: "Sequence Prediction with Switching Cost"

In this context we consider a variant of the classical learning task of sequence prediction that includes a switching cost. More precisely, the objective of the predictor is to predict the sequence of page requests denoted by \(r_{1},r_{2},...,r_{T}\). In each round \(t\), the predictor presents a prediction for all remaining requests in the sequence \(_{t},_{t+1},...,_{T}\). At the conclusion of the round, the predictor sees \(r_{t}\) and incurs a loss of \(1[_{t} r_{t}]\) if the prediction was incorrect. After observing \(r_{t}\), the predictor can choose to alter the subsequent predictions to \(^{}_{t+1},...,^{}_{T}\). Each time the predictor decides to modify the predictions, a switching cost of \(k\) is incurred (remember that \(k 1\) represents the size of the cache). Thus, the total loss of the predictor is equal to the number of prediction errors plus \(k\) times the number of switches.

Hypotheses.Each hypothesis in our class \(\) is a possible input sequence. In the realizable scenario, we operate under the assumption that the actual input matches one of the hypotheses within the class. In the agnostic case we relax this assumption and provide guarantees that scale with the Hamming distance between the input sequence and the hypothesis class.

In the realizable case we design a deterministic predictor whose total loss is at most \(k\) (recall that \(=||\)). It is based on majority vote or the halving algorithm [Littlestone, 1987]. An interesting and subtle point is that our predictor is improper, meaning it occasionally predicts the remaining sequence of page requests in a manner that does not align with any of the hypotheses in \(\). To incorporate such improper predictors, we need to use an optimal offline caching algorithm that is monotone in the following sense: applying the algorithm on an input sequence \(r_{1},,r_{T}\) produces a caching policy which is simultaneously optimal for all prefixes \(r_{1},,r_{t}\) for \(t T\). Fortunately, Belady's FitF algorithm has this property, as outlined in Observation 4.

For the agnostic setting, we design a randomized predictor with a maximum total loss of \(O(^{}+k)\), where \(^{}\) is the Hamming distance of the actual input sequence from the class \(\). This predictor utilizes a multiplicative-weight rule [Littlestone and Warmuth, 1994], and its learning rate is specifically adapted to achieve the optimal balance between the cost of changing predictions (switching costs) and the inaccuracies in the predictions themselves (prediction errors).

Our final caching algorithm incorporates a predictor for this problem in such a way that at each round \(t\), it applies Belady's FitF algorithm to the predicted suffix of the sequence \(_{t},...,_{T}\). We then show that the cumulative loss of the predictor serves as an upper bound on the additional number of page faults that our algorithm experiences compared to the offline optimal algorithm. Overall we obtain the following guarantees for our caching strategy:

**Theorem 1** (Caching).: _Let \(\) be a hypothesis class of size \(\) and \(I\) be an input instance with offline optimum value \((I)\). There is a deterministic algorithm for the realizable setting (i.e., \(I\)) which has cost at most \((I)+k\). There is a randomized algorithm for the agnostic setting with expected cost at most \((I)+(5+6/k)^{}+(2k+1)\), where \(^{}\) is the Hamming distance between \(I\) and the best hypothesis in \(\)._

Our algorithms can be robustified, i.e., we can ensure that their cost is not larger than \(O( k)(I)\) while loosing only a constant factor in the dependency on \(^{}\) and \(\) in our additive regret bound, see Section D.1 for details. Note that the previous methods used to achieve robustness for caching usually lose an additive term linear in \((I)\), see (Wei, 2020; Blum and Burch, 2000; Antoniadis et al., 2023). In Section D.2, we describe how to extend our results to the setting where each hypothesis, instead of a complete instance, is a set of parameters of some prediction model producing next-arrival predictions. In Section D.3, we show that the dependency on \(,k\), and \(^{}\) in Theorem 1 cannot be improved by more than a constant factor. Our result is an improvement over the \(O(^{}+k+)\) regret bound of Emek et al. (2021) whenever \(T=(k)\).

### Load Balancing

In online load balancing on unrelated machines, we have \(m\) machines numbered from \(1\) to \(m\) and a total of \(n\) jobs. The jobs arrive sequentially, and the objective is to assign each job to one of the machines upon its arrival in order to minimize the _makespan_, which is the total time that the busiest machine is actively working. Each job is characterized by its type, which is an \(m\)-dimensional vector \(p\). The value \(p(i)\) indicates the time required for the \(i\)-th machine to complete the job. As the jobs arrive, the algorithm observes the job's type and makes a decision on which of the \(m\) machines to schedule it. These scheduling decisions are made in an online manner, without knowledge of future jobs.

In the offline setting, the ordering of the jobs in the input sequence does not play any role. In fact, an instance of load balancing is sufficiently described by the number of jobs of each type which need to be scheduled and these numbers are available to the algorithm in advance. A \(2\)-approximation algorithm by Lenstra, Shmoys, and Tardos (1990) based on linear programming achieves a makespan that is at most twice the makespan of the optimal schedule.

#### The Learning Task: Forecasting Demand

The learning problem that arises in this context of makespan minimization is a rather natural one and might be interesting in other contexts. The goal of the predictor is to forecast, for each possible job type \(p\), the number of jobs of type \(p\) that are expected to arrive. The predictor maintains a prediction that includes an upper bound, denoted as \(n_{p}\), on the number of jobs of each possible job type \(p\). Similar to caching, the learning problem involves two distinct costs: prediction errors and switching costs. A prediction error occurs when the actual number of jobs of a particular type exceeds the predicted upper bound \(n_{p}\). The cost of a prediction error is determined by the type of the job that witnessed it. A switching cost occurs when the predictor decides to modify its prediction (i.e., the predicted upper bounds \(n_{p}\)'s). The cost of such a modification is the makespan associated with the new prediction.2

Hypotheses.In load balancing each hypothesis \(f\) in our class \(\) predicts the frequency of the jobs of each type \(p\). That is, for each type \(p\) it assigns a number \(f_{p}\) which represents the fraction of jobs in the input whose type is \(p\). We stress that the hypothesis does not predict the actual number of jobs of each type, nor does it even predict the total number of jobs in the input. In practice, the numbers \(f_{p}\) can be estimated by past statistics. With the knowledge of the correct hypothesis \(f\), we are able to produce an integral assignment of jobs to machines at a low cost. Previously studied machine-weight predictions (Lattanzi et al., 2020) allow producing a fractional assignment which requires a costly rounding procedure (Li and Xian, 2021).

In the realizable scenario, we operate under the assumption that the actual input matches one of the hypotheses within the class. In the agnostic case we relax this assumption and provide guarantees that scale with the maximum (multiplicative) approximation error between the true frequencies and those predicted by the best hypothesis (see below).

In the realizable case, we design a simple randomized predictor, ensuring that the total expected loss is at most \(O((I))\) (recall that \(=||\)), where \((I)\) represents the makespan of the input instance \(I\).3 The key idea is to guess the total number of jobs in the input sequence and accordingly to scale the frequencies in each hypothesis to predict the number of jobs \(n_{p}\) of each type. The randomized predictor maintains a random hypothesis consistent with the processed jobs. Whenever one of the predicted counts \(n_{p}\) is violated, the predictor switches to a randomly chosen consistent hypothesis from \(\), resembling the classical randomized marking strategy in caching (Fiat et al., 1991).

We additionally present a deterministic predictor with loss of at most \(O((I)(||))\), where \(\) is the number of job types with non-zero frequency in at least one of the hypotheses. The deterministic rule predicts the median among the counts \(n_{p}\) provided by the hypotheses for each job type \(p\). The analysis of this deterministic learning rule is more intricate than that of the randomized one. The crucial insight is that the produced "medians" prediction can be scheduled within makespan at most \(O((I))\). Our predictors in the agnostic setting are based on those in the realizable case.

Our scheduling algorithm incorporates a predictor for this problem in such a way that at each round \(t\), it behaves in accordance with the algorithm of Lenstra et al. (1990), applied to the predicted upper bounds \(n_{p}\)'s. We demonstrate that the cumulative loss of the predictor serves as an upper bound on the makespan. We obtain the following result:

**Theorem 2** (Load balancing).: _There are algorithms using a deterministic and randomized predictor respectively which, given a hypothesis class \(\) of size \(\) and an instance \(I\) with makespan \((I)\), satisfy the following. In the realizable setting (i.e., \(h(I)\), where \(h(I)\) is the distribution corresponding to \(I\)), they produce a schedule whose makespan is at most \(O((I))\) and \(O((I))\) in expectation, respectively, where \(\) is the number of job types with non-zero frequency in at least one of the hypotheses. In the agnostic case they produce a schedule with makespan at most \(O((I))\) and \(O((I))\) in expectation, respectively, where \(\) and \(\) describe the multiplicative error of the best hypothesis \(f^{}\)._

In agnostic case, the multiplicative error of hypothesis \(f\) with respect to an instance with frequencies \(f^{*}\) is defined as follows. If there is a job type \(p\) such that \(f_{p} 0\) and \(f^{}_{p}=0\), we define \(:=n+1\), where \(n\) denotes the number of jobs in the input instance. Otherwise, we define \(:=\{f_{p}/f^{}_{p} f^{}_{p} 0\}\). Similarly, if there is a job type \(p\) such that \(f_{p}=0\) and \(f^{}_{p} 0\), we define \(:=n+1\). Otherwise, \(:=\{f^{}_{p}/f_{p} f_{p} 0\}\). We have \(, n+1\).

Our algorithms can be robustified so that their competitive ratio4 is never larger than \(O( m)\) (the best possible competitive ratio in the worst-case setting (Azar et al., 1992)), while loosing only a constant factor in the bounds mentioned in Theorem 2, see Section B.5. In Section B.7, we show that our competitive ratio in the realizable case cannot be improved by more than a constant factor.

Previous works focused on predictions assigning weight to each machine which indicates its expected load (Lattanzi et al., 2020), and acquiring a solution for the fractional variant of the problem. Dinitz et al. (2022) showed how to aggregate outputs of \(\) algorithms into a single fractional solution, loosing a factor of \(O()\) compared to the best of the algorithms. A fractional solution can be rounded online, loosing a factor of \(()\) in the competitive ratio (Lattanzi et al., 2020; Li and Xian, 2021). Instead, we use job-type frequencies which allow us to produce an integral solution directly without the costly rounding procedure. However, our approach can be used to aggregate outputs of any \(\) algorithms, preserving integrality of their solutions, see Section B.6.

### Non-clairvoyant Scheduling

We consider a non-clairvoyant scheduling problem in which a single machine is assigned the task of completing a set of \(n\) jobs, denoted as \(j_{1},j_{2},,j_{n}\). The jobs are released at time \(0\) and the scheduler's objective is to determine the order in which they should be scheduled such that the sum of their completion times is minimized.

The optimal ordering is obtained by sorting the jobs in ascending order of their processing times. However, in the non-clairvoyant setting, the scheduler does not know these processing times. To address this challenge, the scheduler is allowed to preempt a job before it is completed, meaning that it can interrupt the ongoing execution of a job and replace it with another job. The remaining portion of the preempted job is then rescheduled for completion at a later time. Round-Robin algorithm is \(2\)-competitive and this is the best competitive ratio achievable in non-clairvoyant setting in the worst case (Motwani et al., 1993).

#### The Learning Task: Comparing Job Durations

In the learning task explored within this context, the objective is for the predictor to learn the optimal ordering of jobs. We investigate two variants of this learning problem, one suited to the realizable setting and one suited to the agnostic setting.

In the realizable case, we adopt a similar approach to the previous sections. Here, each hypothesis within the class provides predictions for the processing times of all \(n\) jobs. We then design a predictor that learns the correct hypothesis in an online fashion. Our overall scheduling algorithm in the realizable case operates by always scheduling first the job with the shortest predicted processing time.

In the agnostic setting we follow a different methodology which is more in line with statistical learning. We use here a weaker type of hypotheses: each hypothesis is a permutation of the \(n\) jobs, indicating a prediction of the optimal ordering, without specifying the exact processing times.

In this learning task, the predictor is provided with a training set consisting of a small subset of the jobs that is sampled uniformly. For each job in the training set the predictor sees their lengths. Using this training set, the predictor generates a permutation \(\) of the \(n\) jobs.

Each permutation \(\) is associated with a loss5 which reflects the performance of a scheduler that follows the order suggested by \(\). In particular, the loss is defined in such a way that the optimal permutation has the best (lowest) loss, and more generally permutations with faster completion times have smaller losses. The predictor we design for this task uses the training set to approximate the loss of every permutation in the class \(\), and outputs the one which minimizes the (approximated) loss.

In order to avoid scaling issues, we formulate our guarantees for instances with maximum job length at most \(1\).6

**Theorem 3** (Completion Time).: _Consider an input instance \(I\), let \((I)\) denote the offline optimal value of its total completion time objective, and let \(\) be a hypothesis class of size \(\). We assume, without loss of generality, that the maximum job length in \(I\) is at most \(1\). Then, there is a deterministic algorithm which achieves total completion time at most \((I)+(I)}\) in the realizable setting (i.e., \(I\)). In the agnostic setting, there is a randomized algorithm that with high probability achieves total completion time of at most \((I)+^{*}+O(n^{5/3}^{1/3})\), where \(^{*}\) is the difference between the total completion time of the best hypothesis in the class and \(\)._

Note that the value of \((I)\) is quadratic in \(n\) unless the size of a vast majority of jobs in \(I\) is either \(0\) or vanishing as \(n\) grows.

We have also found an unexpected separation: there is an algorithm for the realizable setting with regret at most \(n\) on input instances containing jobs of only two distinct lengths (Section C.1). On the other hand, there is no algorithm with regret \(o( n)\) on instances with at least three distinct lengths (Section C.3).

Previous work by Dinitz et al. (2022) showed the following. For any \(>0\), there is an algorithm which achieves expected total completion time \((1+)\,+(1/^{5})^{*}\) under certain assumptions about the input. Therefore, their bound always gives a regret linear in \(\) and a higher dependency on \(^{*}\).

Any algorithms can be robustified by running it at speed \((1-)\) simultaneously with the Round Robin algorithm at speed \(\). This way, we get \(O(^{-1})\)-competitive algorithm in the worst case, because the schedule produced by Round Robin is \(2\)-competitive with respect to the optimal schedule processed at speed \(\). Dinitz et al. (2022) used the same approach to robustify their algorithm, incurring the factor \(\) on top of their bound quoted above. This procedure unfortunately worsens the performance of the original algorithm by a constant factor, i.e., such robustification of our algorithm achieves additive regret only with respect to \((I)\).

## 3 Related Work

The closest works to ours are by Dinitz et al. (2022) and Emek et al. (2021). Dinitz et al. (2022) design algorithms with access to multiple predictors. They study (offline) min-cost bipartite matching, non-clairvoyant scheduling, and online load balancing on unrelated machines.7 The main difference from our approach is conceptual: while we treat the task of identifying the best prediction as an independent modular learning problem, they treat it as an integral part of their algorithms. In the case of load balancing, they propose an \(O()\)-competitive algorithm which combines solutions of \(\) prediction-based algorithms into a fractional solution. A fractional solution can be rounded online, incurring an additional multiplicative factor of \(()\) where \(m\) is the number of machines, see (Lattanzi et al., 2020; Li and Xian, 2021). For non-clairvoyant scheduling for minimizing total completion time, they propose an algorithm which works under the assumption that no set of at most \( n\) jobs has a large contribution to \(\). Their algorithm achieves a total completion time of \((1+)+O(^{-5})^{*}\) for any \(>0\), where \(^{*}\) denotes the difference between the cost of the best available prediction and the cost of the offline optimum.

Emek et al. (2021) study caching with \(\) predictors which predict either the whole instance, or the next arrival time of the currently requested page. Based on each prediction, they build an algorithm with performance depending on the number of mistakes in this prediction. Then, they combine the resulting \(\) algorithms using the technique of Blum and Burch (2000) to build a single algorithm with a performance comparable to the best of them. Note that our approach is in a sense opposite to theirs: we use online learning techniques to build a single predictor comparable to the best of the given \(\) predictions and then we use this predictor in a simple algorithm. Their algorithm has a regret bound of \(O(^{*}+k+)\), where \(T\) is the length of the sequence, \(k\) is the size of the cache, and \(^{*}\) is either the hamming distance of the actual input sequence from the closest predicted sequence or the number of mispredicted next arrival times in the output of the best predictor. This bound is larger than ours unless \(T\) is very small, e.g., \(o(k)\).

There are numerous works on data-driven algorithm design, see the survey (Balcan, 2021). They consider (potentially infinite) hypothesis classes containing parametrizations of various algorithms and utilize learning techniques to identify the best hypothesis given its performance on past data. The main difference from our work is that the hypothesis is chosen during the analysis of past data and before receiving the current input instance. In our case, learning happens as we receive larger and larger parts of the current input instance.

There are papers that consider our problems in a setting with a single black-box predictor which corresponds to our agnostic setting with a hypothesis class of size 1. For caching, these are (Lykouris and Vassilvitskii, 2021; Rohatgi, 2020; Wei, 2020; Antoniadis et al., 2023). For online load balancing on unrelated machines and its special case restricted assignment, there are works on algorithms using predicted weights (Lattanzi et al., 2020; Li and Xian, 2021). The papers (Purohit et al., 2018; Wei and Zhang, 2020; Im et al., 2021; Lindermayr and Megow, 2022) address the problem of non-clairvoyant scheduling.

Other related papers are by Bhaskara et al. (2020) who studied online linear optimization with several predictions, and (Anand et al., 2022; Antoniadis et al., 2023) who designed algorithms competitive with respect to a dynamic combination of several predictors for online covering and the MTS problem, respectively. There are also works on selecting the single best prediction for a series of input instances online (Khodak et al., 2022) and offline (Balcan et al., 2021). The main difference from our work is that they learn the prediction before solving the input instance while we learn the prediction adaptively as we gain more information about the input instance.

Other relevant works are on various problems in online learning which consider switching costs (Cesa-Bianchi et al., 2013; Altschuler and Talwar, 2018) and on online smoothed optimization (Goel et al., 2019; Zhang et al., 2021; Chen et al., 2018).

Since the seminal papers of Lykouris and Vassilvitskii (2021) and Kraska et al. (2018), many works on ML-augmented algorithms appeared. There are by now so many of these works that is not possible to survey all of them here. Instead, we refer to the survey of Mitzenmacher and Vassilvitskii (2020) and to the website maintained by Lindermayr and Megow (2023).

Caching in offline setting was studied by Belady (1966). Sleator and Tarjan (1985), laying the foundations of online algorithms and competitive analysis, showed that the best competitive ratio achievable by a deterministic caching online algorithm is \(k\). Fiat et al. (1991) proved that the competitive ratio of randomized caching algorithms is \(( k)\). Non-clairvoyant scheduling with the total completion time objective was studied by Motwani et al. (1993) who showed that Round-Robin algorithm is \(2\)-competitive and that this is the best possible competitive ratio. Azar et al. (1992) proposed an \(O( m)\)-competitive algorithm for online load balancing on unrelated machines and showed that this is the best possible. In the offline setting, Lenstra et al. (1990) proposed a \(2\)-approximation algorithm and this remains the best known algorithm. There was a recent progress on special cases (Svensson, 2012; Jansen and Rohwedder, 2017).

## 4 Warm-up: Caching in the Realizable Setting

In this section, we describe the simplest use case of our approach and that is caching in the realizable setting. In caching, we have a universe of pages \(U\), a cache of size \(k\) and its initial content \(x_{0}\). As it is usual, we assume that \(U\) contains \(k\) "blank" pages \(b_{1},,b_{k}\) which are never requested and \(x_{0}=\{b_{1},,b_{k}\}\), i.e., we start with an empty cache. We receive a sequence of requests \(r_{1},,r_{T} U\{b_{1},,b_{k}\}\) online. At each time step \(t\), we need to ensure that \(r_{t}\) is present in the cache, i.e., our cache \(x_{t}\) contains \(r_{t}\). If \(r_{t} x_{t-1}\) we say that there is a _page fault_ and we choose \(x_{t}\) such that \(r_{t} x_{t}\). This choice needs to be made without the knowledge of the future requests.

We measure the cost of a solution to a caching instance by counting the number of page loads (or, equivalently, page evictions) performed when transitioning from \(x_{t-1}\) to \(x_{t}\) at each time \(t=1,,T\). Denoting \((x_{t-1},x_{t})=|x_{t} x_{t-1}|\), the total cost of the solution \(x=x_{1},,x_{T}\) is

\[(x)=_{t=1}^{T}d(x_{t-1},x_{t}).\]

Offline algorithm Fit.An intuitive offline optimal algorithm FitF was proposed by Belady (1966): if there is a page fault at time \(t\), it evicts a page from \(x_{t-1}\) which is requested furthest in the future (FitF). In case there are pages which will never be requested again, it breaks the ties arbitrarily. The following monotonicity property will be useful in our analysis.

**Observation 4**.: _Consider a request sequence \(r_{1},,r_{T}\). For any \(t T\), the cost incurred until time \(t\) by FitF algorithm for sequence \(r_{1},,r_{T}\) is the same as the cost incurred by FitF algorithm for sequence \(r_{1},,r_{t}\)._

To see why this observation holds, it is enough to notice that the solution produced by FitF on \(r_{1},,r_{T}\) until time \(t\) is the same as the solution of FitF on \(r_{1},,r_{t}\) which breaks ties according to the arrival times in \(r_{t+1},,r_{T}\).

Learning task.In the realizable setting, we are given a class \(\) of \(\) hypotheses \(r^{1},,r^{} U^{T}\) such that the actual input sequence of requests \(r\) is one of them (but we do not know which one). We split the task of designing an algorithm for this setting into two parts. First, we design an (improper) predictor that maintains a predicted sequence \(=_{1},,_{T}\). This predictor makes a small number of switches (changes to \(\)) until it determines the correct hypothesis. Second, we design an algorithm which uses an access to such predictor and its performance depends on the number of switches made by the predictor.

Predictor.Our Predictor 1 below is based on a majority rule. It maintains a set \(A\) of all hypotheses (sequences) in the class \(\) which are consistent with the past requests. In time \(t=1\) the set \(A\) is initialized to be the entire class, i.e. \(A=\), and it is updated whenever the current request \(r_{t}\) differs from the predicted request \(_{t}\) (i.e., when there is a prediction error). The prediction \(\) used by our predictor is defined based on the set \(A\) as follows: We set \(_{t}:=r_{t}\) and, for \(=t+1,,T\), we choose \(_{}\) to be the request agreeing with the largest number of hypotheses in \(A\). This way, the predicted sequence \(\) is modified exactly after time-steps \(t\) when \(_{t} r_{t}\), and whenever this happens at least half of the hypotheses in \(A\) are removed. Observe that we assume the realizable setting and hence at all times \(A\) contains the hypothesis which is consistent with the input sequence. In particular \(A\) is never empty. This implies the following lemma:

**Lemma 5**.: _In realizable setting, Predictor 1 with a class \(\) of \(\) hypotheses makes \(\) switches and the final prediction is consistent with the whole input._

```
1for\(t=1,,T\)do
2if\(t=1\) or prediction \(_{t}\) differs from the real request \(r_{t}\)then// make a switch
3\(A:=\{i\{1,,\} r_{}^{i}=r_{}\ =1, t\}\) ; // consistent hypotheses
4update \(_{t}=r_{t}\) and \(_{}=_{p U}|\{i A r_{}^{i}=p\}|\) for each \(=t+1,,T\);
```

**Algorithm 2**caching, realizable setting

**Algorithm.** Our overall algorithm (See Algorithm 2) uses Predictor 1 and maintains the FitF solution \(x_{1},,x_{T}\) for the current prediction \(\) at time \(t\). Then it changes the cache to \(x_{t}\). This solution needs to be recomputed whenever \(\) is modified.

**Lemma 6**.: _Consider an input sequence \(r\) and let \((r)\) denote the cost of the optimal offline solution for this sequence. Algorithm 2 with a predictor which makes \(\) switches and its final prediction is consistent with \(r\) incurs cost at most_

\[(r)+k.\]

Proof of Lemma 6 can be found in Appendix A. Combining lemmas 5 and 6, we get an algorithm for caching in a realizable setting with the following guarantee.

**Theorem 7**.: _There is an algorithm for caching in realizable setting which, given a class \(\) of \(\) hypotheses, achieves regret at most \(k\)._

## 5 Conclusions

We introduce a new approach to modeling algorithms with predictions. Unlike the traditional black-box access to a predictor, we extend the algorithmic problem by studying the accompanying learning problem. This allows the algorithm designer to improve the algorithm by:

1. Learning while processing the input,
2. Classifying the input instance to select the most suitable strategy before incurring high costs,
3. Accelerating the classification of the input by taking actions that may not be directly aligned with any strategy suggested by past data.

To achieve this, we split the computational problem into a learning component and an algorithmic component, addressing each separately. By applying our algorithms to existing settings with prediction portfolios (Dinitz et al., 2022; Emek et al., 2021), we demonstrate that our approach often results in simpler algorithms with improved performance compared to previous methods.