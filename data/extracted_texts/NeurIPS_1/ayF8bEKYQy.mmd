# The given question

[MISSING_PAGE_FAIL:1]

###### Abstract

The evolution of Artificial Intelligence (AI) has been significantly accelerated by advancements in Large Language Models (LLMs) and Large Multimodal Models (LMMs), gradually showcasing potential _cognitive reasoning_ abilities in problem-solving and scientific discovery (i.e., AI4Science) once exclusive to human intellect. To comprehensively evaluate current models' performance in cognitive reasoning abilities, we introduce _OlympicArena_, which includes 11,163 bilingual problems across both text-only and interleaved text-image modalities. These challenges encompass a wide range of disciplines spanning seven fields and 62 international Olympic competitions, rigorously examined for data leakage. We argue that the challenges in Olympic competition problems are ideal for evaluating AI's cognitive reasoning due to their complexity and interdisciplinary nature, which are essential for tackling complex scientific challenges and facilitating discoveries. Beyond evaluating performance across various disciplines using answer-only criteria, we conduct detailed experiments and analyses from multiple perspectives. We delve into the models' cognitive reasoning abilities, their performance across different modalities, and their outcomes in _process-level_ evaluations, which are vital for tasks requiring complex reasoning with lengthy solutions. Our extensive evaluations reveal that even advanced models like GPT-4o only achieve a 39.97% overall accuracy (28.67% for mathematics and 29.71% for physics), illustrating current AI limitations in complex reasoning and multimodal integration. Through the _OlympicArena_, we aim to advance AI towards superintelligence, equipping it to address more complex challenges in science and beyond. We also provide a comprehensive set of resources to support AI research, including a benchmark dataset, an open-source annotation platform, a detailed evaluation tool, and a leaderboard with automatic submission features.2

## 1 Introduction

The landscape of Artificial Intelligence (AI) has undergone a transformative evolution with advances in technologies like Large Language Models  and Large Multimodal Models (LMMs) . These models represent significant milestones on the path to Artificial General Intelligence (AGI) , demonstrating remarkable _cognitive reasoning_ abilities, which represent drawing meaningful conclusions from incomplete and inconsistent knowledge to solve problems in complex scenarios . They adeptly handle tasks ranging from simple grade school math problems  to complex challenges like those presented at the International Mathematical Olympiad (IMO) . Furthermore, they are progressively being applied to intricate real-world scenarios, such as using AI agents for software development , collaborating on complex decision-making processes  and even boosting the field of scientific research (i.e., AI4Science) .

These applications highlight AI's growing proficiency in cognitive reasoning, a crucial element in the pursuit of AGI and, potentially, superintelligence . Therefore, how to benchmark these abilities has sparked extensive research. Existing benchmarks  utilize multidisciplinary exam problems to assess the problem-solving skills of LLMs, but these problems are predominantly knowledge-intensive which has become relatively easy for current LLMs. Also, these benchmarks primarily focus on text-only modalities. Although some benchmarks begin to target college-level problems  and incorporate multimodal assessments , they still predominantly focus on knowledge-intensive tasks or simple concept applications (shown in Table 1). Concurrent to our work, He et al.  introduces an Olympic-level benchmark yet it is limited to only mathematics and physics. Furthermore, all the above benchmarks lack a systematic and fine-grained evaluation of various cognitive reasoning abilities. For example, they mostly do the evaluation only based on answers, neglecting potential errors in the reasoning process. This underscores the need for more comprehensive evaluations that not only cover a broader range of disciplines but also focus on higher levels of cognitive reasoning as well as fine-grained evaluation.

In this paper, we introduce _OlympicArena_, a comprehensive, highly-challenging, and rigorously curated benchmark featuring a detailed, fine-grained evaluation mechanism designed to assessadvanced AI capabilities across a broad spectrum of Olympic-level challenges (as illustrated in Figure 2). We extensively select, collect, and process problems from seven disciplines--mathematics, physics, chemistry, biology, geography, astronomy, and computer science--encompassing 62 different Olympic-level competitions. This extensive collection has culminated in a benchmark comprising 11,163 problems, categorized into 13 types of answers (e.g., expression, interval). Importantly, _OlympicArena_ enhances its evaluation framework by incorporating _process-level evaluations_ that scrutinize the step-by-step reasoning processes of AI models. This approach is critical for understanding the depth of cognitive reasoning beyond correct answers [29; 53], allowing us to identify and rectify gaps in AI reasoning pathways and ensuring more robust AI capabilities. The benchmark is bilingual, featuring both English and Chinese, to enhance its accessibility and global applicability. Additionally, it supports two modalities: text-only and interleaved text and images, catering to the evolving complexity of tasks that modern AI systems must handle. We also perform data leakage detection experiments  on some mainstream models to validate our benchmark's effectiveness.

We conduct a series of experiments across existing top-performing LMMs, encompassing both proprietary models (e.g., GPT-4o ) and open-source models (e.g., LLaVa-NeXT ). Additionally, we evaluate various types of LLMs (e.g., GPT-3.5) in two settings: text-only and image-caption and conduct comprehensive evaluations from both the answer-level and process-level perspectives. For answer-level evaluations, we combine rule-based and model-based (GPT-4V3 in this paper) methods to cover a more diverse range of answer types. For process-level evaluations, we score each reasoning step of the model output, which we consider quite critical in reasoning scenarios. Additionally, we perform fine-grained evaluations and analyses on different types of cognitive reasoning, from both logical and visual perspectives to better interpret the current capabilities of AI.

Our observations from the _OlympicArena_ benchmark are summarized as follows: (1) Even the most advanced model, GPT-4o, achieves only a 39.97% overall accuracy, while other open-source models struggle to reach a 20% overall accuracy, underscoring current models' limitations in handling complex, multidisciplinary problems that require advanced cognitive reasoning--key aspects of scientific discovery. (2) Through more fine-grained analysis SS 4.4, we find that LMMs are particularly weak in handling complex, decompositional reasoning problems and exhibit poor spatial and geometric perception visual abilities, as well as difficulties in understanding abstract symbols. (3) Additionally, we discover that current LMMs seem to struggle significantly in leveraging interleaved visual information for complex cognitive reasoning problems. Various LMMs fail to show notable enhancements compared to their text-only counterparts. (4) The process-level evaluation also indicates that most models can correctly execute some reasoning steps in spite of providing incorrect answers, demonstrating the models' significant potential. (5) Through data leakage detection, we find that instances of data leakage in our benchmark are exceedingly rare. Even on the infrequent occasions when leakage does occur, the corresponding models do not consistently solve these problems correctly. This suggests the need for more advanced training strategies to enhance cognitive reasoning capabilities. These observations highlight the immense value of the _OlympicArena_ benchmark in advancing our understanding of AI's capabilities and limitations.

## 2 Related Work

Benchmark AI IntelligenceHow to benchmark AI intelligence has always been a challenging problem. Initially, the Turing Test  provided a conceptual framework for evaluating AI Intelligence. However, limitations in past AI technology lead researchers to focus on specialized domains. In computer vision, benchmarks like MNIST  and ImageNet  catalyze progress, while in natural language processing, GLUE  and XTREME  set the standard for evaluating linguistic capabilities across tasks and languages. The success of pretrained language models [38; 23] particularly recent LLMs emphasizes the evaluation of foundational knowledge and innate abilities as shown in Figure 2. This leads to the creation of benchmarks such as MMLU , AGileval, C-Eval , and CMMLU , which pushed the limits of language models with multidisciplinary, multilingual, and knowledge-intensive tasks. However, the rapid progress of LLMs has rendered these benchmarks insufficient to fully assess the models' growing capabilities.

[MISSING_PAGE_FAIL:4]

The benchmark includes a comprehensive set of 11,163 problems from 62 distinct Olympic competitions, structured with 13 answer types (shown in Appendix A.2) from objective types (e.g., multiple choice and fill-in-the-blanks) to subjective types (e.g., short answers and programming tasks), which distinguishes it from many other benchmarks that primarily focus on objective problems. Detailed statistics of _OlympicArena_ are described in Table 2. Also, to identify potential data leakage, we conduct specialized data leakage detection experiments on several models.

Furthermore, in pursuit of a granular analysis of model performance, we categorize cognitive reasoning into 8 types of logical reasoning abilities and 5 types of visual reasoning abilities. This comprehensive categorization aids in the detailed evaluation of the diverse and complex reasoning skills that both LLMs and LMMs can exhibit. Additionally, we specifically investigate all multimodal problems to compare the performance of LMMs against their text-based counterparts, aiming to better assess LMMs' capabilities in handling visual information. Finally, we evaluate the correctness and efficiency of the reasoning process, not just limited to an answer-based assessment.

### Data Collection

To ensure comprehensive coverage of Olympic-level problems across various disciplines, we begin by collecting URLs of various competitions where problems are publicly available for download in PDF format. Then, we utilize the Mathpix4 tool to convert these PDF documents into markdown format, making them compatible with input requirements for models. Specifically, for the programming problems of Computer Science, we additionally collect corresponding test cases. We strictly adhere to copyright and licensing considerations, ensuring compliance with all relevant regulations.

### Data Annotation

Problem Extraction and Annotation.To extract individual problems from the markdown format of the test papers, we employ about 30 students with background in science and engineering. We have developed a user interface for annotating multimodality data, which has been released. 5 To facilitate further research and the process-level evaluation of models, we annotate meta-information like solutions if provided. To ensure data quality, we implement a multi-step validation process after the initial annotation is completed. More details can be seen in Appendix B.1. After collecting all the problems, we perform deduplication within each competition based on model embeddings to remove repeated problems that may appear in multiple test papers from the same year. To further demonstrate that our benchmark emphasizes cognitive reasoning more than most other benchmarks, we categorize the difficulty of the problems into three levels and make comparison with other related benchmarks. Specifically, we classify all problems into: _knowledge recall_, _concept application_ and _cognitive reasoning_. We utilize GPT-4V as the annotator for categorizing different difficulty levels6 (detailed definitions and specific prompts can be found in Appendix B.2).7

Annotation of Cognitive Reasoning Abilities.To facilitate better fine-grained analysis, we categorize cognitive reasoning abilities from both logical and visual perspectives [16; 43]. The logical reasoning abilities encompass _Deductive Reasoning (DED)_, _Inductive Reasoning (IND)_, _Abductive Reasoning (ABD)_, _Analogical Reasoning (ANA)_, _Cause-and-Effect Reasoning (CAE)_, _Critical Thinking (CT)_, _Decompositional Reasoning (DEC)_, and _Quantitative Reasoning (QUA)_. Meanwhile, the visual reasoning abilities include _Pattern Recognition (PR)_, _Spatial Reasoning (SPA)_, _Diagrammatic Reasoning (DIA)_, _Symbol Interpretation (SYB)_, and _Comparative Visualization (COM)_. We also utilize GPT-4V as the annotator for categorizing different cognitive abilities (detailed definitions and

  
**Statistic** & **Number** \\  Total Problems & 11163 \\ Total Competitions & 62 \\ Total Subjects/Subfields & 7/34 \\ Total Answer Types & 13 \\ Problems with Solutions & 7904 \\ Language (EN: ZH) & 7054: 4109 \\  Total Images & 7571 \\ Problems with Images & 4960 \\ Image Types & 5 \\  Cognitive Complexity Levels & 3 \\ Logical Reasoning Abilities & 8 \\ Visual Reasoning Abilities & 5 \\  Average Problem Tokens & 244.8 \\ Average Solution Tokens & 417.1 \\   

Table 2: Benchmark Statisticsspecific prompts can be found in Appendix B.3).7 With these annotations, we can conduct a more fine-grained analysis of the current cognitive reasoning abilities of AI.

### Data Splitting

Our benchmark includes 11,163 problems, with 548 designated for model-based evaluation as _OlympicArena-ot_. We sample 638 problems across subjects to create _OlympicArena-val_ for hyper-parameter tuning or small-scale testing. _OlympicArena-val_ problems have step-by-step solutions, supporting research like process-level evaluation. The remaining problems form _OlympicArena-test_, the official test set with unreleased answers for formal testing. The results in this paper are based on the entire benchmark dataset, including _OlympicArena-ot_, _OlympicArena-val_, and _OlympicArena-test_.

## 4 Experiments

### Experimental Setup

To comprehensively evaluate the capabilities of LLMs and LMMs (selected models are listed in Appendix C.2) across different modalities, we design our experiments to include three distinct settings: multimodal, image-caption, and text-only. In the multimodal setting, we assess the ability of LMMs to leverage visual information by interleaving text and images, simulating real-world scenarios. For models unable to handle interleaved inputs, we concatenate multiple images into a single input. For LMMs requiring necessary image inputs, their text-based counterparts handle text-only problems. In the image-caption setting, we explore whether textual descriptions of images enhance the problem-solving capabilities of LLMs. Using InternVL-Chat-V1.58, we generate captions for all images based on prompts detailed in Appendix C.1. These captions replace the original image inputs. In the text-only setting, we evaluate the performance of LLMs without any visual information, serving as a baseline to compare against the multimodal and image-caption settings. All experiments use zero-shot prompts, tailored to each answer type and specifying output formats to facilitate answer extraction and rule-based matching. It also minimizes biases typically associated with few-shot learning . Detailed prompt designs are provided in Appendix C.3.

### Evaluation

Answer-level EvaluationWe combine rule-based and model-based methods to cover a diverse range of problems. For problems with fixed answers, we extract the final answer and perform rule-based matching according to the answer type. For code generation tasks, we use the unbiased pass@k metric  to test all test cases. For problems with answer types categorized as "others" which are difficult to be evaluated using rule-based matching (e.g., chemical equation writing problems), we employ GPT-4V as an evaluator to assess the responses. To ensure the reliability of GPT-4V as an evaluator, we manually sample and check the correctness. More details will be explained in Appendix C.5.

Process-level EvaluationTo further investigate the correctness of the reasoning steps, ensuring a rigorous assessment of the cognitive abilities of models, we conduct the process-level evaluation. We first sample 96 problems with reference solutions from _OlympicArena_. We employ GPT-4 to convert both the references (i.e., gold solutions) and the model-generated solutions into a structured step-by-step format. We then provide these solutions to GPT-4V and score each step for its correctness on a scale ranging from 0 to 1. 9 The experimental details can be seen in Appendix C.6. To validate the consistency with human judgment, we obtain some samples for human annotations. The results indicate that our model-based evaluation method is highly accurate, with an 83% inter-annotator agreement.

To further concretize models' performance, we sample a portion of the problems where the model makes errors and conduct an **error analysis** on them. Details can be found in Appendix D.5.

### Main Results

Table 3 presents the evaluation results of various LMMs and LLMs on _OlympicArena_. We obtain the following observations: (1) Even the most advanced large model, GPT-4o, achieves only a 39.97% overall accuracy, while other open-source models struggle to reach a 20% overall accuracy. This stark contrast highlights the significant difficulty and rigor of our benchmark, demonstrating its effectiveness in pushing the boundaries of current AI capabilities. (2) Furthermore, compared to subjects like biology and geography, we observe that mathematics and physics remain the two most challenging disciplines, likely due to their reliance on complex reasoning abilities. (3) Computer programming competitions also prove to be highly difficult, with some open-source models failing to solve any of them, indicating current models' poor abilities to design efficient algorithms to solve complex problems.

### Fine-grained Analysis

To achieve a more fine-grained analysis of the experimental results, we conduct further evaluations based on different modalities and reasoning abilities. Additionally, we also conduct an analysis of the process-level evaluation. Key findings are as follows:

Models exhibit varied performance across different logical and visual reasoning abilities.As shown in Figure 3, almost all models demonstrate similar performance trends across different logical reasoning abilities. They tend to excel in Abductive Reasoning and Cause-and-Effect Reasoning, doing well in identifying causal relationships from the provided information. Conversely, models perform poorly in Inductive Reasoning and Decompositional Reasoning. This is due to the diverse

    & **Math** & **Physics** & **Chemistry** & **Biology** & **Geography** & **Astronomy** & **CS** & **Overall** \\   & Accuracy & Accuracy & Accuracy & Accuracy & Accuracy & Accuracy & Pass@1 & Accuracy \\   \\  Qwen-7B-Chat & 1.58 & 3.74 & 7.01 & 7.31 & 4.53 & 5.48 & 0 & 4.31 \\ Yi-34B-Chat & 3.06 & 9.77 & 23.53 & 32.67 & 35.03 & 18.15 & 0.17 & 17.31 \\ Internalm2:20B-Chat & 5.88 & 9.48 & 18.36 & 31.90 & 32.14 & 16.03 & 0.60 & 16.62 \\ Qwen1-5:32B-Chat & 9.65 & 14.54 & 29.84 & 38.58 & 40.69 & 28.05 & 0.51 & 23.69 \\  GPT-3.5 & 7.27 & 10.92 & 23.03 & 31.19 & 31.13 & 16.93 & 3.85 & 18.27 \\ Claudio3 Somet & 7.76 & 17.24 & 29.46 & 38.25 & 40.94 & 24.04 & 1.62 & 23.02 \\ GPT-4 & 19.46 & 24.77 & 42.52 & 46.47 & 44.97 & 33.44 & 7.78 & 32.37 \\ GPT-4o & 28.33 & 29.54 & 46.24 & 49.42 & 48.36 & 43.25 & 8.46 & 38.17 \\   \\  Qwen-7B-Chat & 1.76 & 3.56 & 6.75 & 7.83 & 7.17 & 6.87 & 0 & 4.89 \\ Yi-34B-Chat & 3.01 & 9.94 & 21.45 & 31.26 & 3.48 & 17.33 & 0.17 & 16.72 \\ Internalm2:20B-Chat & 5.94 & 10.40 & 20.25 & 31.00 & 32.52 & 16.93 & 0.73 & 17.07 \\ Qwen1-5:32B-Chat & 9.56 & 14.31 & 29.84 & 38.51 & 40.75 & 27.2 & 0.60 & 23.43 \\  GPT-3.5 & 7.16 & 14.48 & 23.97 & 30.94 & 33.52 & 18.56 & 4.70 & 18.83 \\ Claudio3 Somet & 7.52 & 18.10 & 29.84 & 38.77 & 41.14 & 22.65 & 2.39 & 23.10 \\ GPT-4 & 19.46 & 26.21 & 41.58 & 45.89 & 48.18 & 35 & 7.63 & 33.00 \\ GPT-4o & 28.27 & 29.71 & 45.87 & 51.16 & 49.12 & 43.17 & **9.57** & 38.50 \\   \\  Qwen-VL-Chat & 1.73 & 4.25 & 8.64 & 12.13 & 13.77 & 7.85 & 0 & 6.90 \\ Yi-VL-34B & 2.94 & 9.94 & 19.81 & 27.73 & 25.16 & 16.60 & 0 & 14.49 \\ InternalVL-Chat-V1.5 & 6.03 & 9.25 & 19.12 & 30.39 & 32.96 & 15.94 & 0.38 & 16.63 \\ LLaVa-NEXT:34B & 3.03 & 10.06 & 21.45 & 33.18 & 36.92 & 18.15 & 0.18 & 17.38 \\  Qwen-VL-Max & 6.93 & 12.36 & 23.79 & 36 & 40.19 & 23.39 & 0.77 & 20.65 \\ Gemini Pro Vision & 6.28 & 12.47 & 28.14 & 37.48 & 37.42 & 20.20 & 1.45 & 20.97 \\ Claudio3 Somet & 7.52 & 18.16 & 29.27 & 38.96 & 40.13 & 25.02 & 1.45 & 23.13 \\ GPT-4V & 19.27 & 24.83 & 41.45 & 46.79 & 49.62 & 32.46 & 7.00 & 32.76 \\ GPT-4o & **28.67** & **29.71** & **46.69** & **52.18** & **56.23** & **43.91** & 9.00 & **39.97** \\   

Table 3: Experimental results on _OlympicArena_, expressed as percentages, with the highest score in each setting underlined and the highest scores across all settings bolded. We use the pass@k metric (Equation 1) for CS problems. When calculating the overall accuracy, for code generation problems, if any generated code for a problem passes all test cases, the problem is considered correct.

and unconventional nature of Olympic-level problems, which require the ability to break down complex problems into smaller sub-problems. In terms of visual reasoning abilities, models tend to be better at Pattern Recognition and Comparative Visualization. However, they struggle with tasks involving spatial and geometric reasoning as well as those need to understand abstract symbols. The completed results are presented in Appendix D.1.

Most LMMs are still not proficient at utilizing visual information.As displayed in Figure 3(a), only a few LMMs (such as GPT-4o and Qwen-VL-Chat) show significant improvement with image inputs compared to their text-based counterpart. Many LMMs do not exhibit enhanced performance with image inputs and some even show decreased effectiveness when handling images. Possible reasons include: (1) When text and images are input together, LMMs may focus more on the text, neglecting the information in the images. This conclusion has also been found in some other works . (2) Some LMMs, while training their visual capabilities based on their text-based models, may lose some of their inherent language abilities (e.g., reasoning abilities), which is particularly evident in our scenarios. (3) Our problems use a complex interleaved text and image format, which some models do not support well, leading to difficulties in processing and understanding the positional information of images embedded within the text. 10

Figure 4: (a) Comparison of different LMMs and their corresponding LLMs across three different experimental settings. For details on the corresponding LLMs for each LMM, refer to the Appendix C.2. (b) The correlation between answer-level and process-level scores of all the models over all the sampled problems. (c) Distribution of the locations of incorrect steps, represented as the proportion of steps from left to right in the entire process, over all the sampled problems.

Figure 3: Performance of various models on logical and visual reasoning abilities. Logical reasoning abilities: Deductive Reasoning (DED), Inductive Reasoning (IND), Abductive Reasoning (ABD), Analogical Reasoning (ANA), Cause-and-Effect Reasoning (CAE), Critical Thinking (CT), Decompositional Reasoning (DEC), and Quantitative Reasoning (QUA). Visual reasoning abilities: Pattern Recognition (PR), Spatial Reasoning (SPA), Diagrammatic Reasoning (DIA), Symbol Interpretation (SYB), and Comparative Visualization (COM).

Analysis of process-level evaluation resultsThrough process-level evaluation (complete results are in Table 14), we discover following insights: (1) There is generally a high consistency between process-level evaluation and answer-level evaluation. When a model produces a correct answer, the quality of the reasoning process tends to be higher most of the time (see Figure 3(b)). (2) The accuracy at the process-level is often higher than at the answer-level. This indicates that even for very complex problems, the model can correctly perform some of the intermediate steps. Therefore, the model likely has significant untapped potential for cognitive reasoning, which opens new avenues for researchers to explore. We also find that in a few disciplines, some models that perform well at the answer level fall behind at the process level. We speculate that this is because models sometimes tend to overlook the reasonableness of intermediate steps when generating answers, even though these steps may not be crucial to the final result. (3) Additionally, we conduct a statistical analysis of the location distribution of error steps (see Figure 3(c)). We identify that a higher proportion of errors occur in the later stages. This suggests that models are more prone to making mistakes as reasoning accumulates, indicating a need for improvement in handling long chains of logical deductions.

### Efforts on Data Leakage Detection

Given the increasing scale of pre-training corpora, it is crucial to detect potential benchmark leakage. The opacity of pre-training often makes this task challenging. To this end, we employ a recently proposed instance-level leakage detection metric, _N-gram Prediction Accuracy_. This metric uniformly samples several starting points for each instance, predicts the next n-gram for each starting point, and checks whether all predicted n-grams are correct, indicating that the model has potentially encountered this instance. We apply this metric to all available base or text-only chat models of the evaluated models. As shown in Figure 5, it is surprising yet reasonable that some base models or text-only chat models behind these evaluated models have potentially encountered a few benchmark instances, although the number is negligible compared to the complete benchmark. For instance, the base model of Qwen1.5-32B-Chat has potentially encountered 43 benchmark instances. Furthermore, this raises a natural question: _can the model correctly answer these instances?_ Interestingly, the corresponding text-only chat models and multimodal chat models can correctly answer even fewer of these instances. These results demonstrate that our benchmark has minimal leakage11 and is sufficiently challenging, as the models cannot correctly answer most of the leaked instances. See Appendix E for more results and analysis.

## 5 Conclusion

In this work, we introduce _OlympicArena_, a comprehensive benchmark for evaluating the cognitive reasoning abilities of LMMs and LLMs on Olympic-level problems. Through our detailed experiments, we find that even the most powerful model at present, GPT-4o, does not perform well in applying cognitive reasoning abilities to solve complex problems. We hope that our OlympicArena benchmark serves as a valuable stepping stone for future advancements in AI for science and engineering.

Figure 5: Detected number of leaked samples and the number of correct responses by corresponding text-only and multimodal chat models on these samples.