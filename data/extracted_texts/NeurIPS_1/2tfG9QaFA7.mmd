# Training on Foveated Images Improves Robustness to Adversarial Attacks

Muhammad A. Shah

Language Technologies Institute

Carnegie Mellon University

Pittsburgh, PA 15213

mshah1@cmu.edu

&Aqsa Kashaf

ByteDance

San Jose, CA 95110

akashaf@cmu.edu

&Bhiksha Raj

Language Technologies Institute

Carnegie Mellon University

Pittsburgh, PA 15213

bhiksha@cs.cmu.edu

work done while at Carnegie Mellon University

###### Abstract

Deep neural networks (DNNs) are known to be vulnerable to adversarial attacks - subtle, perceptually indistinguishable perturbations of inputs that change the response of the model. We hypothesize that an important contributor to the robustness of human visual perception is constant exposure to low-fidelity visual stimuli in our peripheral vision. To investigate this hypothesis, we develop _R-Blur_, an image transform that simulates the loss in fidelity of peripheral vision by blurring the image and reducing its color saturation based on the distance from a given fixation point. We show that compared to DNNs trained on the original images, DNNs trained on images transformed by _R-Blur_ are substantially more robust to adversarial attacks, as well as other, non-adversarial, corruptions, achieving up to 25% higher accuracy on perturbed data2.

## 1 Introduction

Deep Neural Networks (DNNs) are exceptionally adept at many computer vision tasks and have emerged as one of the best models of the biological neurons involved in visual object recognition . However, their lack of robustness to subtle image perturbations that humans are largely invariant  to has raised questions about their reliability in real-world scenarios. Of these perturbations, perhaps the most alarming are _adversarial attacks_, which are specially crafted distortions that can change the response of DNNs when added to their inputs  but are either imperceptible to humans or perceptually irrelevant enough to be ignored by them.

While several defenses have been proposed over the years to defend DNNs against adversarial attacks, only a few of them have sought inspiration from biological perception, which, perhaps axiomatically, is one of the most robust perceptual systems in existence. Instead, most methods seek to _teach_ DNNs to be robust to adversarial attacks by exposing them to adversarially perturbed images  or random noise  during training. While this approach is highly effective in making DNNs robust to the types of perturbations used during training, the robustness often does not generalize to other types of perturbations . In contrast, biologically-inspired defenses seek to make DNNs robust by integrating into them biological mechanisms that would bring their behavior more in line with human/animal vision . As these defenses do not require DNNs to be trained on any particular type of perturbation, they yield models that, like humans, are robust to a variety of perturbations  in addition to adversarial attacks. For this reason, and in light of the evidence indicating a positive correlation between biological alignment and adversarial robustness , we believe biologically inspired defenses are more promising in the long run.

Following this line of inquiry, we investigate the contribution of low-fidelity visual sensing that occurs in peripheral vision to the robustness of human/animal vision. Unlike DNNs, which sense visual stimuli at maximum fidelity at every point in their visual field, humans sense most of their visual field in low fidelity, i.e without fine-grained contrast  and color information . In adults with fully developed vision, only a small region (less than 1% by area) of the visual field around the point of fixation  can be sensed with high fidelity. In the remainder of the visual field (the periphery), the fidelity of the sensed stimuli decreases exponentially with distance from the fixation point . This phenomenon is called "foveation". Despite this limitation, humans can accurately categorize objects that appear in the visual periphery into high-level classes . Meanwhile, the presence of a small amount of noise or blurring can decimate the accuracy of an otherwise accurate DNN. Therefore, we hypothesize that the experience of viewing the world at multiple levels of fidelity, perhaps even at the same instant, causes human vision to be invariant to low-level features, such as textures, and high-frequency patterns, that can be exploited by adversarial attacks.

In this paper, we propose _R-Blur_ (short for Retina Blur), which simulates foveation by blurring the image and reducing its color saturation adaptively based on the distance from a given fixation point. This causes regions further away from the fixation point to appear more blurry and less vividly colored than those closer to it. Although adaptive blurring methods have been proposed as computational approximations of foveation , their impact on robustness has not been evaluated to the best of our knowledge. Furthermore, color sensitivity is known to decrease in the periphery of the visual field , yet most of the existing techniques do not account for this phenomenon.

Similar to how the retina preprocesses the visual stimuli before it reaches the visual cortex, we use _R-Blur_ to preprocess the input before it reaches the DNN. To measure the impact of _R-Blur_, we evaluate the object recognition capability of ResNets  trained with and without _R-Blur_ on three image datasets: CIFAR-10 , Ecoset  and Imagenet , under different levels of adversarial attacks and common image corruptions . We find that _R-Blur_ models retain most of the high classification accuracy of the base ResNet while being more robust. Compared to the base ResNet, _R-Blur_ models achieve 12-25 percentage points (pp) higher accuracy on perturbed images. Furthermore, the robustness achieved by _R-Blur_ is certifiable using the approach from . We also compare _R-Blur_ with two biologically inspired preprocessing defenses, namely _VOneBlock_, a fixed parameter module that simulates the primate V1, and a non-uniform sampling-based foveation technique , which we refer to as _R-Warp_. We observe that _R-Blur_ induces a higher level of robustness, achieving accuracy up to 33 pp higher than _R-Warp_ and up to 15 pp higher than _VOneBlock_ against adversarial attacks. Compared to adversarial training (_AT_)  - the state-of-the-art non-biological defense, _R-Blur_ achieves up to 7 pp higher accuracy on average against non-adversarial corruptions of various types and strengths thus indicating that the robustness of _R-Blur_ generalizes better to non-adversarial perturbations than _AT_. Finally, an ablation study showed that both adaptive blurring and desaturation contribute to the improved robustness of _R-Blur_.

## 2 Retinal Blur: An Approximation for Peripheral Vision

To simulate the loss in contrast and color sensitivity of human perception with increasing eccentricity, we propose _R-Blur_, an adaptive Gaussian blurring, and color desaturation technique. The operations

Figure 1: _R-Blur_ adds Gaussian noise to image (a) with the fixation point (red dot) to obtain (b). It then creates a colored and a grayscaled copy of the image and applies adaptive Gaussian blurring to them to obtain the low-fidelity images (c) and (d), where the numbers indicate the standard deviation of the Gaussian kernel applied in the region bounded by the boxes. The blurred color and gray images are combined in a pixel-wise weighted combination to obtain the final image (e), where the weights of the colored and gray pixels are a function of their respective estimated acuity values (see 2.2).

performed by _R-Blur_, given an image and fixation point, are shown in Figure 1. First, _R-Blur_ adds Gaussian noise to the image to simulate stochastic firing rates of biological photoreceptors . It then creates color and grayscale copies of the image and estimates the acuity of color and grayscale vision at each pixel location, using distributions that approximate the relationship between distance from the fixation point (eccentricity) and visual acuity levels in humans. _R-Blur_ then applies _adaptive_ Gaussian blurring to both image copies such that the standard deviation of the Gaussian kernel at each pixel in the color and the grayscale image is a function of the estimated color and grayscale acuity at that pixel. Finally, _R-Blur_ combines the two blurred images in a pixel-wise weighted combination in which the weights of the colored and gray pixels are a function of their respective estimated acuity values. Below we describe some of the more involved operations in detail.

### Eccentricity Computation

The distance of a pixel location from the fixation point, i.e. its eccentricity, determines the standard deviation of the Gaussian kernel applied to it and the combination weight of the color and gray images at this location. While eccentricity is typically measured radially, in this paper we use a different distance metric that produces un-rotated square level sets. This property allows us to efficiently extract regions having the same eccentricity by simply slicing the image tensor. Concretely, we compute the eccentricity of the pixel at location \((x_{p},y_{p})\) as

\[e_{x_{p},y_{p}}=-x_{f}|,|y_{p}-y_{f}|)}{W_{V}}, \]

where \((x_{f},y_{f})\) and \(W_{V}\) represent the fixation point and the width of the visual field, i.e. the rectangular region over which _R-Blur_ operates and defines the maximum image size that is expected by _R-Blur_. We normalize by \(W_{V}\) to make the \(e_{x_{p},y_{p}}\) invariant to the size of the visual field.

### Visual Acuity Estimation

We compute the visual acuity at each pixel location based on its eccentricity. The biological retina contains two types of photoreceptors. The first type, called cones, are color sensitive and give rise to high-fidelity visual perception at the fovea, while the second type, called rods, are sensitive to only illumination but not color and give rise to low-fidelity vision in the periphery. We devise the following two sampling distributions, \(D_{R}(e_{x,y})\) and \(D_{C}(e_{x,y})\), to model the acuity of color and grayscale vision, arising from the cones and rods at each pixel location, \((x,y)\).

\[(e;,) =[(e;0,),(e;0,)] \] \[D_{C}(e;_{C},) =(e;_{C},)\] (3) \[D_{R}(e;_{R},,p_{max}) =p_{max}(1-(e;_{R},)), \]

where \((.;,)\) and \((.;,)\) are the PDFs of the Laplace and Cauchy distribution with location and scale parameters \(\) and \(\), and \(\) is a parameter used to control the width of the distribution. We set \(_{C}=0.12,_{R}=0.09,=2.5\) and \(p_{max}=0.12\). We choose the above equations and their parameters to approximate the curves of photopic and scotopic visual acuity from . The resulting acuity estimates are shown in Figure 2b. Unfortunately, the measured photopic and scotopic acuity curves from  cannot be reproduced here due to copyright reasons, however, they can be viewed at [https://nba.uth.tmc.edu/neuroscience/m/s2/chapter14.html](https://nba.uth.tmc.edu/neuroscience/m/s2/chapter14.html) (see Figure 3).

### Quantizing the Visual Acuity Estimate

In the form stated above, we would need to create and apply as many Gaussian kernels as the distance between the fixation point and the farthest vertex of the visual field. This number can be quite large as the size of the image increases and will drastically increase the per-image computation time. To mitigate this issue we quantize the estimated acuity values. As a result, the locations to which the same kernel is applied no longer constitute a single pixel perimeter but become a much wider region (see Figure 1 (c) and (d)), which allows us to apply the Gaussian kernel in these regions very efficiently using optimized implementations of the convolution operator.

To create a quantized eccentricity-acuity mapping, we do the following. We first list all the color and gray acuity values possible in the visual field by assuming a fixation point at \((0,0)\), computing eccentricity values \(e_{0,y}\) for \(y[0,W_{V}]\) and the corresponding values of \([0,W_{V}]\)} and \(_{C}=\{D_{C}(e_{0,y})|y[0,W_{V}]\}\). We then compute and store the histograms, \(H_{R}\) and \(H_{C}\), from \(_{R}\) and \(_{C}\), respectively. To further reduce the number of kernels we need to apply and increase the size of the region each of them is applied to, we merge the bins containing less than \(\) elements in each histogram with the adjacent bin to their left. After that, given an image to process, we will compute the color and gray visual acuity for each pixel, determine in which bin it falls in \(H_{R}\) and \(H_{C}\), and assign it the average value of that bin.

### Changing the Viewing Distance

Increasing the viewing distance can be beneficial as it allows the viewer to gather a more global view of the visual scene and facilitates object recognition. To increase the viewing distance we drop the \(k\) lowest acuity bins and shift the pixels assigned to them \(k\) bins ahead such that the pixels that were in bins 1 through \(k-1\) are now assigned to bin 1. Figure 3 shows the change in the viewing distance as the value of \(k\) increases from 0 to 5. Formally, given the quantized \(D_{C}(e_{x,y})\) and \(D_{R}(e_{x,y})\), let \(D=[d_{1},...,d_{n}]\) represent the value assigned to each bin and \(P_{i}\) be the pixel locations assigned to the \(i^{th}\) bin, with \(P_{1}\) and \(P_{n}\) corresponding to points with the lowest and highest eccentricity, respectively. To increase the viewing distance, we merge bins 1 through \(k\) such that \(D^{}=[d_{1},...,d_{n-k}]\) and the corresponding pixels are \(P^{}_{1}=[P_{1},...,P_{k}]\) and \(P_{i>1}=P_{k+1}\).

### Blurring and Color Desaturation

We map the estimated visual acuity at each pixel location, \((x_{p},y_{p})\), to the standard deviation of the Gaussian kernel that will be applied at that location as \(_{(x_{p},y_{p})}= W_{V}(1-D(e_{x,y}))\), where \(\) is constant to control the standard deviation and is set to \(=0.05\) in this paper, and \(D=D_{C}\) for pixels in the colored image and \(D=D_{R}\) for pixels in the grayscaled image. We then apply Gaussian kernels of the corresponding standard deviation to each pixel in the colored and grayscale image to obtain an adaptively blurred copy of each, which we combine in a pixel-wise weighted combination to obtain the final image. The weight of each colored and gray pixel is given by the normalized color and gray acuity, respectively, at that pixel. Formally, the pixel at \((x_{p},y_{p})\) in the final image has value

\[v^{f}_{(x_{p},y_{p})}=_{(x_{p},y_{p})}D_{C}(e_{x,y};_{C}, )+v^{g}_{(x_{p},y_{p})}D_{R}(e_{x,y};_{C},)}{D_{C}(e_{x,y}; _{C},)+D_{R}(e_{x,y};_{C},)}, \]

\(v^{c}_{(x_{p},y_{p})}\) and \(v^{g}_{(x_{p},y_{p})}\) are the pixel value at \((x_{p},y_{p})\) in the blurred color and gray images respectively.

## 3 Evaluation

In this section, we determine the accuracy and robustness of _R-Blur_ by evaluating it on clean data and data that has been perturbed by either adversarial attacks or common - non-adversarial - corruptions. We compare the performance of _R-Blur_ with an unmodified ResNet, two existing biologically-inspired defenses, _R-Warp_ and _VOneBlock_, and a non-biological adversarial defenses: Adversarial Training (_AT_) . We show that _R-Blur_ is significantly more robust to adversarial attacks and common corruptions than the unmodified ResNet and prior biologically inspired methods. Moreover, we use Randomize Smoothing  to show that _R-Blur_ is _provably_ robust. While _AT_ is more robust

Figure 3: Illustration of increasing the viewing distance (left to right). As the viewing distance is increased, more of the image is brought into focus. We used \(vd=3\) during inference.

Figure 2: Estimated visual acuity of sharp and colorful, photopic, and gray and blurry, scotopic, vision using equations 3 and 4

than _R-Blur_ against adversarial attacks, _R-Blur_ is more robust than _AT_ against common corruptions, thus indicating that the robustness of _R-Blur_ generalizes better to different types of perturbation than _AT_. We also analyze the contribution of the various components of _R-Blur_ in improving robustness.

### Experimental Setup

**Datasets:** We use natural image datasets, namely CIFAR-10 , Imagenet ILSVRC 2012 , Ecoset  and a 10-class subset of Ecoset (Ecoset-10). Ecoset contains around 1.4M images, mostly obtained from ImageNet database  (not the ILSVRC dataset), that are organized into 565 basic object classes. The classes in Ecoset correspond to commonly used nouns that refer to concrete objects. To create Ecoset-10, we select 10 classes from Ecoset that have the highest number of images. The training/validation/test splits of Ecoset-10 and Ecoset are 48K/859/1K, and 1.4M/28K/28K respectively. For most experiments with Ecoset and Imagenet, we use 1130, and 2000 test images, with an equal number of images per class. During training, we use random horizontal flipping and padding + random cropping, as well as AutoAugment  for CIFAR-10 and RandAugment for Ecoset and Imagenet. All Ecoset and Imagenet images were resized and cropped to \(224 224\). We applied these augmentations to _all_ the models we trained - those with biological and non-biological defenses, as well as the baseline models.

**Model Architectures:** For CIFAR-10 we use a Wide-Resnet  model with 22 convolutional layers and a widening factor of 4, and for Ecoset and Imagenet we use XResNet-18 from fastai  with a widening factor of 2. Moving forward, we will refer to both these models as ResNet and indicate only the training/evaluation datasets from which the exact architecture may be inferred. Results for additional architectures are presented in Appendix C.

**Baselines and Existing Methods:** We compare the performance of _R-Blur_ to two baselines: (1) an unmodified ResNet trained on clean data (ResNet), and (2) a ResNet which applies five affine transformations 3 to the input image and averages the logits (_RandAffine_). We also compare _R-Blur_ with two biologically inspired defenses: _VOneBlock_ pre-processing proposed in , which simulates the receptive fields and activations of the primate V1 4, and _R-Warp_ preprocessing proposed in , which simulates foveation by resampling input images such that the sampling density of pixels is maximal at the point of fixation and decays progressively in regions further away from it. Finally, we compare _R-Blur_ with two non-biological adversarial defenses: fast adversarial training  with \(\|\|_{}=0.008\) (_AT_), and Randomized Smoothing (_RS_) .

**Fixation Selection for _R-Blur_ and _R-Warp_**: While training models with _R-Blur_ and _R-Warp_, we split each batch into sub-batches of 32 images, and for each sub-batch, we randomly sample a single fixation point that we use to apply _R-Blur_ or _R-Warp_ to all the images in that sub-batch. While training the _R-Blur_ model, we also set the viewing distance uniformly at random using the procedure

Figure 4: Illustration of fixation selection. The initial fixation point is set to top-left (0,0) and the image at \(t_{0}\) is processed with _R-Blur_ /_R-Warp_ to get the image at \(t_{1}\). DeepGaze-III is used to generate a fixation heatmap from this image. The next fixation point is sampled from the heat map, and _R-Blur_ /_R-Warp_ is applied to get the image at \(t_{2}\). The region in the heatmap around the chosen fixation point is masked with an inverted Gaussian kernel to prevent spatial clustering of fixation points. This process is repeated to get a sequence of fixation points.

described in 2.4. During inference, we determine a sequence of five fixation points (a scanpath) using DeepGaze-III . DeepGaze-III passes the input image through a pretrained CNN backbone (DenseNet-201 in ) and extracts the activations from several intermediate layers of the CNN. It then applies a sequence of pointwise convolution and normalization layers to the activations to obtain a heatmap indicating where a human is likely to fixate. We found that it was more efficient to not use the scanpath prediction module in DeepGaze-III, and instead obtain scanpaths by keeping track of the past fixation points, and masking the predicted heatmap at these locations prior to sampling the next fixation point from it. This process is illustrated in Figure 4.

We trained two instances of DeepGaze-III using the ResNets we trained with _R-Blur_ and _R-Warp_ as the CNN backbone. We use the corresponding DeepGaze-III models to predict the scanpaths for _R-Blur_ and _R-Warp_ models. To train deepgaze-iii we used the code from the official github repository . The only significant modification we made was to replace the pretrained DenseNet-201 with the pretrained R-Warp/R-Blur augmented XResNet-18 we trained on ImageNet. This improves performance, while keeping the total number of parameters low. Following  we train DeepGaze on the SALICON dataset . This corresponds to phase 1 of training mentioned in Table 1 of . We did not notice any benefits in our use case of phases 2-4, so we skipped them.

### Results

_R-Blur_ improves robustness to white-box attacks.We evaluate robustness by measuring the accuracy of models under Auto-PGD (APGD) attack, which is a state-of-the-art white-box adversarial attack. We run APGD for 25 steps on each image. We find that increasing the number of steps beyond 25 only minimally reduces accuracy (Appendix A). We take a number of measures to avoid the pitfalls of gradient obfuscation  so that our results reflect the true robustness of _R-Blur_. These steps and detailed settings used for adversarial attacks are mentioned in Appendix A.

To determine if _R-Blur_ improves robustness, we compare _R-Blur_ with the unmodified ResNet and _RandAffine_ under the APGD attack. We observe that _R-Blur_ is significantly more robust than the unmodified ResNet and _RandAffine_ models, consistently achieving higher accuracy than the two on all datasets and against all perturbation types and sizes, while largely retaining accuracy on clean data (Figure 5). While _RandAffine_ does induce some level of robustness, it significantly underperforms _R-Blur_. On smaller datasets, _R-Blur_ suffers relatively little loss in accuracy at small to moderate levels (\(\|\|_{} 0.004\), \(\|\|_{2} 1\)) of adversarial perturbations, while the accuracy of baseline methods quickly deteriorates to chance or worse. On larger datasets - Ecoset and Imagenet, even the smallest amount of adversarial perturbation (\(\|\|_{}=0.002\), \(\|\|_{2}=0.5\)) is enough to drive the accuracy of the baselines to \(\)10%, while _R-Blur_ still is able to achieve 35-44% accuracy. As the perturbation is increased to \(\|\|_{}=0.004\) and \(\|\|_{2}=1.0\), the accuracy of the baselines goes to 0%, while _R-Blur_ achieves 18-22%. We do observe that the accuracy of _R-Blur_ on clean data from Ecoset and Imagenet is noticeably lower than that of the baseline methods.

We also compare _R-Blur_ to two existing biologically motivated adversarial defenses: _VOneBlock_ and _R-Warp_, and find that _R-Blur_ achieves higher accuracy than both of them at all perturbation sizes and

Figure 5: Comparison of accuracy on various datasets (a-d) under adversarial attacks of several \(_{2}\) (top) and \(_{}\) (bottom) norms between _R-Blur_ (green) and two baseline methods: _RandAffine_ (orange) and ResNet (blue). The dashed lines indicate accuracy on clean images. _R-Blur_ models consistently achieve higher accuracy than baseline methods on all datasets, and adversarial perturbation sizes.

types. From Figure 6 we see that _R-Blur_ achieves up to 33pp higher accuracy than _R-Warp_, and up to 15 pp higher accuracy than _VOneBlock_ on adversarially perturbed data.

_R-Blur_ is certifiably robust.To verify that the gains in robustness observed above are indeed reliable, we use the certification method (Certify) from  to provide formal robustness guarantees for _R-Blur_. This entails obtaining predictions for an input under a _very_ large number (\(10^{5}\)) of noise samples drawn from \((0,_{c})\), and using a hypothesis test to determine the _certified radius_ around the input in which the model's prediction is stable _with high probability_ (\( 99.9\%\)). Given a dataset, we can compute the _certified accuracy_ at a radius \(r\) as the proportion of data points for which the certified radius is \( r\) and the model's prediction is correct. It was shown in  that a model trained on data perturbed with Gaussian noise achieves high certified accuracy. We call this model _G-Noise_. We compare the certified accuracy of _G-Noise_ and _R-Blur_ on 200 images from Imagenet and Ecoset.

We expose both _R-Blur_ and _G-Noise_ to Gaussian noise of scale \(_{t}=0.125\) during training and compute their certified accuracy at radii \(r\{0.5,1.0\}\). According to , if the scale of the noise used in Certify is \(_{c}\), then the maximum radius for which certified accuracy can be computed (with \(10^{5}\) noise samples) is \(r=4_{c}\). Therefore, when computing certified accuracy at \(r 0.5\)Certify adds noise of the same scale as was used during training (\(_{c}=0.125=_{t}\)), thus we call this the _matched_ setting. However, to compute certified accuracy at \(r 1.0\)Certify adds noise of a larger scale than was used during training (\(_{c}=0.25>_{t}\)), and thus in order to achieve high certified accuracy at \(r 1.0\) the model must be able to generalize to a change in noise distribution. We call this the _unmatched_ setting.

Figure 6(a) and 6(b) show the certified accuracy of _R-Blur_ and the _G-Noise_ on Ecoset and Imagenet at several \(_{2}\) norm radii under matched and unmatched settings. In both settings, we see that _R-Blur_ achieves a high certified accuracy on both Ecoset and Imagenet, with the certified accuracy at \(r 0.5\) and \(r 1.0\) being close to the ones observed in Figure 5, indicating that our earlier results are a faithful representation of _R-Blur_'s robustness. Furthermore, we see that even if _R-Blur_ was trained without any noise, it can still achieve more than 50% of the certified accuracy achieved by _R-Blur_ trained with noise. This indicates that adaptive blurring and desaturation do in fact endow the model with a significant level of robustness. Finally, we note that while _G-Noise_ has (slightly) higher certified accuracy than _R-Blur_ in the matched setting, _R-Blur_ achieves significantly higher certified accuracy in the unmatched setting, outstripping _G-Noise_ by more than 10 pp at \(r 1.0\) on Imagenet. This shows that the robustness of _R-Blur_ generalizes beyond the training conditions, while _G-Noise_

Figure 6: The difference in accuracy under adversarial attacks of several \(_{2}\) and \(_{}\) norms between _R-Blur_ and two biologically inspired defenses: _R-Warp_ (blue) and _VOneBlock_ (orange). _R-Blur_ consistently achieves higher accuracy on all adversarial perturbation sizes than _R-Warp_ and _VOneBlock_.

Figure 7: The certified accuracy at various \(_{2}\)-norm radii of _R-Blur_ and _G-Noise_ models. _R-Blur_-CFI uses 1 fixation at the center of the image, and _R-Blur_-5FI, averages logits from 5 fixation (corners + center). \(_{t}\) denotes the scale of noise added during training and is 0.125 unless specified, whereas \(_{c}\) is the scale of the noise used to compute the certified accuracy. _G-Noise_ outperforms _R-Blur_ in the matched scenario, while _R-Blur_ is superior in the unmatched scenario indicating that the robustness of _R-Blur_ is more generalizable.

overfits to them. This makes _R-Blur_ particularly suited for settings in which the exact adversarial attack budget is not known, and the model must be able to generalize.

_R-Blur Improves accuracy on common (non-adversarial) corruptions._ Adversarial perturbations constitute only a small subset of perturbations that human vision is invariant to, therefore we evaluate _R-Blur_ on a set of common image corruptions  that humans are largely invariant to but DNNs are not. We sample 2 images/class from Imagenet and 5 images/class from Ecoset. Then we apply 17 5 common corruptions proposed in  at 5 different severity levels to generate 85 corrupted versions of each image. This yields corrupted versions of Imagenet and Ecoset containing 170K and 240K images, respectively.

Figure 8 shows the accuracy of the models on corrupted Ecoset and Imagenet. Here we also compare against an adversarially trained model (_AT_) trained with \(\|\|_{}=0.008\) using the method of . We see that at severity greater than 1 _R-Blur_ consistently achieves the highest accuracy. Furthermore, we also note that _R-Blur_, and _VOneBlock_ consistently achieve higher accuracy than _AT_, which supports our hypothesis that the robustness of biologically motivated methods, and particularly _R-Blur_, is more general than non-biological defenses, like _AT_. In fact, the accuracy of _AT_ on common corruptions is generally lesser than or at par with the accuracy of the unmodified ResNet, indicating that the robustness of _AT_ does not generalize well.

**Summary of Results:** Table 1 summarizes the results of our paper and reiterates two key observations from earlier sections. Firstly, _R-Blur_ makes models more significantly robust to adversarial perturbations than the unmodified ResNet, and other biologically inspired defenses. _R-Blur_, however, achieves lower accuracy against white-box attacks than _AT_. This is to be expected because _AT_ is trained on adversarially perturbed data. Secondly, _R-Blur_ augmented models are significantly more robust to common corruptions than all other models, including _AT_. In contrast, the accuracy of _AT_ on common corruptions is almost the same as that of the unmodified ResNet, indicating that the robustness of _AT_ does not generalize.

   Method & Mean & CC & WB & Clean & Mean & CC & Wb & Clean \\   &  \\  ResNet & 37.1 & 39.4 & 0.8 & 71.2 & 34.7 & 33.6 & 0.1 & **70.3** \\ _RandAffine_ & 35.7 & 35.8 & 3.6 & 67.6 & 33.7 & 30.8 & 2.0 & 68.3 \\  _AT_ & **49.0** & 38.5 & **47.5** & 61.1 & **46.3** & 34.2 & **43.5** & 61.3 \\  _R-Warp_ & 38.5 & 40.0 & 4.5 & 71.1 & 34.1 & 32.5 & 2.2 & 67.7 \\ _VOneBlock_ & 42.9 & 40.7 & 16.1 & **72.0** & 38.8 & 35.8 & 11.9 & 68.7 \\  _R-Blur_ & 44.2 & **45.6** & 23.8 & 63.3 & 38.9 & **39.0** & 17.2 & 60.5 \\   \\   

Table 1: Accuracy of the evaluated models on clean, and perturbed data from Imagenet. “WB” refers to the accuracy under APGD attacks, while “CC” refers to the accuracy under common non-adversarial corruption . _R-Blur_ significantly improves the robustness of ResNet, and outperforms prior biologically motivated defenses, while approaching the performance of _AT_.

Figure 8: The accuracy of the models on Imagenet and Ecoset under the common corruptions from  at various severity levels. We see that _R-Blur_ generally achieves the highest accuracy.

### Ablation Study

We examine, by way of ablation, how much each component of _R-Blur_ contributes towards its overall robustness as shown in Figure 9. The most significant contributor to robustness is the addition of noise. This echoes the findings from , which showed that neural stochasticity contributes significantly to the robustness of the visual system. Nevertheless, even without noise _R-Blur_ achieves an 11 point improvement over the vanilla ResNet which archives 0% accuracy under the attack, which indicates that other components of _R-Blur_ also contribute towards robustness. Furthermore, experimental results reveal that robustness induced by noise diminishes as the complexity of the dataset increases and the size of the perturbations increases. As observed in Figure 9(b), Gaussian noise augmentation achieves 45-58% (8-10 points) lower accuracy than _R-Blur_, and in Figure 6(b), which shows that at larger perturbation sizes _R-Blur_ achieves higher certified accuracy.

The second most significant contributor to robustness is the blurring performed by _R-Blur_. Importantly, we note that Gaussian blurring in and of itself does not greatly improve robustness. Figure 9 shows that non-adaptive blurring with a single Gaussian kernel having \(=10.5\) (\(=10.9\) is the maximum used in _R-Blur_) improves robustness by only 5 points. Furthermore, Figure 9(a) shows that increasing the strength of non-adaptive blurring trades off clean accuracy for robustness. However, after \(=8\) the gains in robustness hit (a rather low) ceiling, and increasing \(\) further reduces both clean accuracy and robustness. On the other hand, _R-Blur_, _without any additive noise_, achieves similar clean accuracy as non-adaptive blurring (\(=10.5\)) but achieves significantly better adversarial robustness, thereby demonstrating that Gaussian blurring alone accounts for only a fraction of _R-Blur_'s robustness. Furthermore, Figure 9(b) shows that the contribution of non-adaptive blurring declines on the more complex Imagenet, where it achieves only 1% accuracy on moderate-sized perturbations.

The next most significant factor, after noise and adaptive blurring, is evaluating multiple fixation points which improved robustness significantly compared to a single fixation point in the center of the image, which suggests that, multiple fixations and saccades are important when the image is hard to recognize and presents a promising direction for future work. Furthermore, not adaptively desaturating the colors reduces the robustness slightly. Finally, we note that dynamic fixation does not improve performance compared to 5 predefined fixation points. To summarize, most of the biologically-motivated components of _R-Blur_ contribute towards improving the adversarial robustness of object recognition DNNs from close to 0% to 45% (\(_{}=0.008\) for Ecoset-10).

Figure 10: Comparison of _R-Blur_ with models trained with non-adaptive Gaussian blur or Gaussian noise augmentations. (a) compares the accuracy under adversarial attack on Ecoset-10 of _R-Blur_ and models augmented with non-adaptive Gaussian blur of various standard deviations (\(\)). While non-adaptive Gaussian blur does increase robustness, the adaptive blurring in _R-Blur_ outperforms it by a margin. (b) compares the accuracy under adversarial attack on Imagenet of _R-Blur_ and models augmented with either non-adaptive Gaussian blur or Gaussian noise. We see that _R-Blur_ achieves better robustness than either of these methods.

Figure 9: Accuracy on clean and APGD (\(\|\|_{}=0.008\)) perturbed Ecoset-10 images when individual components of _R-Blur_ are removed (left), and when non-adaptive transforms are used (right). Removing the biologically-motivated components of _R-Blur_ harms the robustnessRelated Work

**Non-biological defenses:** Perhaps the most successful class of adversarial defenses are adversarial training algorithms [7; 9; 49; 17; 8], which train models on adversarially perturbed data generated by backpropagating gradients from the loss to the input during each training step. Another popular class of defenses is certified defenses [10; 11; 50; 51] which are accompanied by provable guarantees of the form: with probability \(1-\), the model's output will not change if a given image is perturbed at most \(\). Perhaps, most closely related to our work are preprocessing defenses [52; 53] that apply a large number of transforms to the input during inference. Usually, these defenses rely on non-differentiable transformations, and a high degree of randomization in the number, sequence, and parameters of the transforms they apply to each image. Therefore, these defenses tend to obfuscate gradients , and have been shown to be compromised by attacks with a higher step budget. We would like to point out that R-Blur does not have these aforementioned pitfalls - the transforms that R-Blur applies (Gaussian blur and desaturation) are fully differentiable and totally deterministic. In general, it is our opinion that by not being cognizant of the biological basis of robust vision, current approaches are excluding a large set of potentially effective approaches for defending against adversarial attacks.

**Biologically inspired defenses:** Several biological defenses have been proposed over the years. These defenses involve integrating computational analogues of biological processes that are absent from common DNNs, such as predictive/sparse coding [16; 17], biologically constrained visual filters, nonlinearities, and stochasticity , foveation [19; 20; 21; 22], into DNNs. The resulting models are made more robust to adversarially perturbed data, and have been shown to better approximate the responses of biological neurons .

Most relevant to our work are defenses that have integrated foveation with DNNs. One of the earliest works  implements foveation by cropping the salient region of the image at inference time. This work has several shortcomings. Firstly, the biological plausibility of this method is questionable because it does not simulate the degradation of visual acuity in the periphery of the visual field, rather it discards the periphery entirely. Secondly, it crops the image after applying the adversarial attack, which means that the attack does not take into account the cropping, which is akin to obfuscating the gradients, and hence any reported improvements in robustness are suspect. A later work  (_R-Burp_) avoids the aforementioned pitfalls and simulates foveation via non-uniform sampling (regions further away from the fixation points are sampled less densely). Since this method is fully differentiable and highly biologically plausible, we compare against it in this paper. Some recent works [19; 21] apply foveation in the latent feature space (the intermediate feature maps generated by a CNN). These works implement foveation by changing the receptive field sizes of the convolutional kernels based on the distance to the fixation. Since they operate on the latent feature space, rather than image pixels, their methods not directly comparable to ours.

## 5 Limitations

Adding _R-Blur_ reduces accuracy on clean data, however, it is possible to significantly improve the accuracy of _R-Blur_ by developing better methods for selecting the fixation point. Further experimental results presented in Appendix B show that if the optimal fixation point was chosen by an oracle the clean accuracy of _R-Blur_ can be improved to within 2% of the accuracy of the unmodified ResNet.

## 6 Conclusion

Since the existence of adversarial attacks presents a divergence between DNNs and humans, we ask if some aspect of human vision is fundamental to its robustness that is not modeled by DNNs. To this end, we propose _R-Blur_, a foveation technique that blurs the input image and reduces its color saturation adaptively based on the distance from a given fixation point. We evaluate _R-Blur_ and other baseline models against APGD attacks on two datasets containing real-world images. _R-Blur_ outperforms other biologically inspired defenses. Furthermore, _R-Blur_ also significantly improves robustness to common, non-adversarial corruptions and achieves accuracy greater than that of adversarial training. The robustness achieved by _R-Blur_ is certifiable using the approach from  and the certified accuracy achieved by _R-Blur_ is at par or better than that achieved by randomized smoothing . Our work provides further evidence that biologically inspired techniques can improve the accuracy and robustness of AI models.