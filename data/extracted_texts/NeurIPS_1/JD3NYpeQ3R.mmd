# Large language model validity via enhanced

conformal prediction methods

 John J. Cherian

Department of Statistics

Stanford University

jcherian@stanford.edu

Isaac Gibbs

Department of Statistics

Stanford University

igibbs@stanford.edu

Emmanuel J. Candes

Department of Statistics

Department of Mathematics

Stanford University

candes@stanford.edu

###### Abstract

We develop new conformal inference methods for obtaining validity guarantees on the output of large language models (LLMs). Prior work in conformal language modeling identifies a subset of the text that satisfies a high-probability guarantee of correctness. These methods work by filtering claims from the LLM's original response if a scoring function evaluated on the claim fails to exceed a threshold calibrated via split conformal prediction. Existing methods in this area suffer from two deficiencies. First, the guarantee stated is not conditionally valid. The trustworthiness of the filtering step may vary based on the topic of the response. Second, because the scoring function is imperfect, the filtering step can remove many valuable and accurate claims. We address both of these challenges via two new conformal methods. First, we generalize the conditional conformal procedure of Gibbs et al. (2023) in order to adaptively issue weaker guarantees when they are required to preserve the utility of the output. Second, we show how to systematically improve the quality of the scoring function via a novel algorithm for differentiating through the conditional conformal procedure. We demonstrate the efficacy of our approach on biography and medical question-answering datasets.

## 1 Introduction

Large language models (LLMs) are a breakthrough in machine learning. In addition to their extraordinary performance on natural language processing benchmarks, LLMs such as ChatGPT and Gemini are now used by hundreds of millions of users around the world . But even though these models match or even surpass human performance on an increasingly complex and diverse set of tasks, their reliability remains in doubt. For example, LLMs often confidently hallucinate facts that do not exist, and can generate toxic outputs that may offend or discriminate . This "mis-alignment" between user goals and model behavior hinders LLM deployment in settings where the potential for AI assistance appears highest, e.g., legal work or customer service interaction .

Since an LLM output is not always trustworthy, a growing body of work aims to quantify uncertainty regarding a given output's validity. While there are many approaches to this problem [28; 4; 9; 18], this paper considers a particularly popular method for black-box uncertainty quantification: conformal inference [30; 2; 3]. Conformal inference provides a generic methodology for transforming the predictions of any modeling procedure into valid prediction sets that are guaranteed to contain the true outcome with high probability. Several recent papers have applied conformal inference to define a set of LLM responses that contains at least one factual response with high probability [3; 17; 25; 32]. But while generating a candidate set of outputs may be a reasonable strategy in some question-answering problems, it is not a generalizable approach for the diverse and unstructured tasks faced in real-world deployment.

More recently, Mohri and Hashimoto  propose to forgo sets of LLM outputs and instead utilize conformal inference to filter out invalid components of the LLM response. At a high level, given an LLM generation parsed into a set of distinct sub-claims, their method censors all sub-claims for which a pre-defined scoring function lies below some threshold. Mohri and Hashimoto  then show how to calibrate this threshold such that the retained claims are factual with high probability.

While these methods represent a promising step towards usable guarantees for LLM outputs, they are not yet practical. One limitation is that the guarantee attained by previous methods only holds _marginally_ over a random test prompt. The true probability of output correctness may then vary substantially based on the prompt's characteristics. For example, we show in Section 4 that the probability of output correctness (even after applying the conformal factuality method) is substantially lower for responses whose subjects are likely to be underrepresented in the model's training corpus. Second, existing methods remove too many claims to be practically useful. Recall that we remove sub-claims for which some pre-defined score falls below a calibrated threshold. If this score is perfect, only false claims will be censored. In practice, however, these scores are only weakly correlated with the ground truth. As Figure 1 demonstrates, a high probability factuality guarantee can require the removal of a significant proportion of the generated text.1 The conformal guarantee is not useful if the filtered response has limited value for the end-user.

### Summary of contributions

In this subsection, we will preview and summarize our results. A more complete description of our theory and experimental setup is deferred to Sections 3 and 4.

As in prior literature on conformal language modeling, we will assume the existence of an annotated calibration set of \(n\) i.i.d. prompt-response-claim-annotation tuples, \(\{(P_{i},R_{i},_{i},_{i})\}_{i=1}^{n}\). The vector \(_{i}\) is obtained by using an LLM to parse the response into a list of scorable sub-claims, while \(_{i}\) might correspond to human verification of the underlying factuality of each claim. To simplify notation, we will refer to these tuples using the shorthand, \(_{i}\).

At first glance, the twin goals we have outlined for this paper, improved conditional validity _and_ enhanced quality of filtered outputs, appear to be irreconcilable. Indeed, prior work establishes that precise conditional guarantees in black-box uncertainty quantification require larger prediction set sizes, i.e., smaller filtered outputs . We contribute two methods to mitigate this trade-off, thus enabling the practical application of conformal prediction to LLMs.

Our first method, which we call **conditional boosting**, allows for the automated discovery of superior claim scoring functions via differentiation through the conditional conformal algorithm of Gibbs et al. . Automated conformal score improvement was introduced by Stutz et al. ; their paper shows how to minimize conformal prediction set size in a classification setting by differentiating through the marginally valid split conformal algorithm. As we show, however, in Section 4, optimiz

Figure 1: The left panel displays the output of GPT-3.5-Turbo for the prompt “How often is a shin-gles vaccine required?” The first filtered output (center) is calibrated using the frequency score (see Appendix E.1) and the marginally valid conformal factuality method of Mohri and Hashimoto  at a fixed level of 90%. The second filtered output (right) is calibrated using a score obtained via our conditional boosting procedure (Section 3.3) at a level of 63%, which is chosen and calibrated using our adaptive method (Section 3.2) to approximately ensure that at least 70% of the claims are retained. Both filtered outputs are guaranteed to include no false claims with the stated probability.

ing the score function subject only to a marginal coverage constraint can lead to poor conditional properties.

Optimizing through the conditional conformal algorithm is not straightforward. Our key technical contributions are a proof that (under mild assumptions) the cutoff output by the conditional conformal method is differentiable and a computationally efficient method for computing this derivative. By running gradient descent using this algorithm we discover new scores that enable greater claim retention.

The right panel of Figure 2 demonstrates the efficacy of our method. Here, we use boosting to learn an optimal linear combination of four candidate scoring functions. We compare the learned, boosted scores (orange) against a baseline method (blue) that uses the "frequency" scoring method developed by Mohri and Hashimoto . As the figure shows, the boosted score allows for higher claim retention across all datasets (mean retention of 39% vs. 24% for the boosted vs. unboosted scores).

Our second method, which we call **level-adaptive conformal prediction**, allows the validity of the conformal output to depend on characteristics of the queried prompt. In our LLM experiments, we adapt the level, i.e., the claimed probability of correctness, individually to each prompt in order to ensure that issued outputs retain at least \(70\%\) of the original set of sub-claims. For example, in Figure 1, we prompt GPT-3.5-Turbo to output a response to a question from the **MedicationQA** dataset . Outputting a filtered response that achieves the stated factuality criterion with probability \(90\%\) requires near-complete censorship, but by relaxing the level to \(63\%\) using our method, we can preserve almost the entire response.

Given that we are now issuing an output-adaptive probability of correctness, it is crucial that our issued probability is _calibrated_. Calibration requires that the true probability of correctness matches the issued one. For example, if a weather forecaster claims that there is a \(70\%\) chance of rain, their forecast is calibrated if it actually rains for \(70\%\) of the days on which a \(70\%\) forecast is issued.

Figure 2 displays the advantages of our approach to this problem. First, the left panel of Figure 2 verifies that the level-adaptive probabilities we report are empirically well-calibrated. Second, the right panel of Figure 2 quantitatively demonstrates the improved claim retention of our method and verifies that for each dataset included in the **MedLFQA** benchmark  our level-adaptive conformal prediction retains at least 70% of the original output's claims in most examples. Finally, by

Figure 2: Empirical demonstration of our methods. The panels display results for our conditional boosting and level-adaptive methods. We aim to issue outputs with \(0\) factual errors, and for the latter method, we choose the level with the objective of retaining at least 70% of the original claims in the prompt. The left panel compares the binned nominal probabilities of factuality reported by our method against the realized probability of factuality for data points belonging to each bin. These probabilities are estimated using \(500\) test points over 100 calibration-test splits. The plotted bins, which are also given as inputs to our method, are \([0.5,0.55],[0.55,0.6],,[0.8,0.85]\). Finally, the right-hand panel displays the claim retention obtained with unboosted scores (blue), boosted scores (orange), and boosted scores + level-adaptive CP (green). The first two methods are implemented at a fixed error rate of \(=0.1\). Boxplots in this panel show the distribution of retained claims for 100 calibration-test splits with each containing 2354 calibration points and 500 test points.

combining our level-adaptive and conditional boosting methods, we retain most claims _and_ output non-trivial guarantees of response factuality; the left panel shows that the issued probabilities vary between \(50\) and \(85\%\). By contrast, while the fixed level method guarantees a \(90\%\) probability of correctness, the method retains very little of the original LLM output.

To emphasize that these results are accompanied by formal guarantees, we preview one instantiation of our theory here. Since it is well-known that exact conditional guarantees in conformal inference are impossible to achieve without strong distributional assumptions [5; 29], we present an interpretable alternative: group-conditional calibration.2 For example, in this dataset, we might group questions by medical area or data provenance; we would then hope to show that across health conditions or data sources, the claimed probability of factuality matches the true probability of factuality.

Equation (1), which follows from Theorem 3.2, presents one guarantee that our method can satisfy. Here, we denote the (random) output of our data-adaptive level function by \(_{n+1}\) and our filtered set of claims by \((_{n+1})\). Our method then satisfies the following guarantee simultaneously over groups \(G\) (e.g., prompt topic, data provenance) and some discretization of \(\) given by the sub-intervals \(I\) (e.g., all sub-intervals with endpoints belonging to \(\{0,0.1,,1\}\)),

\[((_{n+1})|_{n+1} I,P_{n+1} G)=[_{n+1}|_{n+1} I,P_{n+1} G]. \]

More concretely, (1) establishes that the issued probabilities are well-calibrated in the following sense: among similar prompts, the outputs that we claim to be factually correct with probability, say, between 70 and 80% will be _actually_ factual between 70 and 80% of the time. In Section 3.1, we show how our framework can be adapted to guarantee that the LLM's response satisfies other alignment targets beyond factual accuracy.

The remainder of the paper is outlined as follows. In Section 2, we introduce the formal notation of our paper and contextualize our approach by reviewing related work in conformal inference. Section 3 then presents our new methodology for conformal language modeling. We first generalize the conditional conformal procedure of Gibbs et al.  to obtain high-probability control of arbitrary monotone risks. We then state and give intuition for the key technical results underpinning our level-adaptive and boosting methods. Section 4 outlines synthetic experiments displaying the improvements of our approach over existing methods, gives a more detailed description of the experiment presented in Figure 2 above, and presents another experiment that filters short biographies output by an LLM.

We release a filtered version of the **MedLFQA** benchmark that removes some non-health-related prompts, the generated and parsed text used to run our experiments, as well as the notebooks used to produce the figures in this paper at github.com/jicherian/conformal-safety. We also update our Python package for conditional conformal inference to support level-adaptive conformal prediction. This package is available to download at github.com/jicherian/conditional-conformal and can be installed from PyPI.

## 2 Background and related work

### Conformal prediction with conditional guarantees

Split conformal prediction provides a generic procedure for transforming the outputs of any black-box model into valid prediction sets [23; 30]. Let \(\{(X_{i},Y_{i})\}_{i=1}^{n+1}\) be a set of covariate-ground truth pairs where \(X_{n+1}\) denotes a test value for which we would like to output a response. Then, split conformal outputs a prediction set \((X_{n+1})\) such that

\[(Y_{n+1}(X_{n+1}))=1-, \]

for any user-specified value \((0,1)\).

While powerful, the guarantee given in (2) only holds on-average over the test value. Critically, in the LLM context we consider, this means that methods calibrated using split conformal prediction run the risk of displaying systematically worse performance on the most safety-critical examples.

This problem is addressed in Gibbs et al. , which introduces a novel target for obtaining coverage conditional on covariate information. In their work, they observe that exact covariate-conditional coverage can also be expressed as a marginal guarantee over any measurable function \(f\), i.e.,

\[(Y_{n+1}(X_{n+1})  X_{n+1})=1-\] \[\] \[[f(X_{n+1})(\{Y_{n+1}(X_{n+1}) \}-(1-))]=0f.\]

Since exact distribution-free covariate-conditional coverage requires the analyst to issue vacuous prediction sets , Gibbs et al.  design a prediction set that satisfies the same marginal guarantee over a user-specified function class \(\), i.e.,

\[[f(X_{n+1})(\{Y_{n+1}(X_{n+1})\}-(1- ))]=0f. \]

To make this guarantee concrete, consider the case where \(:=\{(\{X G\})^{}^{| |}\}\) for some set of subgroups \(\). Then, (3) is exactly equivalent to group-conditional coverage, i.e., \((Y_{n+1}(X_{n+1}) X_{n+1} G)=1-\) for all \(G\).

To understand their construction, let \(S(X,Y)\) denote a conformity score that measures how well \(Y\) "conforms" to an estimate of its value. A typical choice in the regression setting is \(S(X,Y)=|Y-(X)|\) for some fixed regression function \(()\); in the classification setting, we might choose \(S(X,Y)=1/(Y X)\) for some estimated conditional probabilities \(\).

Gibbs et al.  estimate a high-probability upper bound for these scores by fitting an augmented quantile regression in which the unknown test score, \(S(X_{n+1},Y_{n+1})\) is replaced by an imputed value \(S\). Formally, let \(_{}(r):=(1-)[r]_{+}+[r]_{-}\) denote the pinball loss. Then, they define

\[g_{S}=_{g}_{i=1}^{n}_{}( S(X_{i},Y_{i})-g(X_{i}))+_{}(S-g(X_{n+1})), \]

and output the prediction set given by \((X_{n+1}):=\{y:S(X_{n+1},y) g_{S(X_{n+1},y)}(X_{n+1})\}\).

Since \((X_{n+1})\) can be mildly conservative, Gibbs et al.  also define a smaller randomized prediction set, \(_{}(X_{n+1})\) that achieves exact coverage. We will typically prefer to work with this set, but defer a detailed definition of its construction to the Appendix. As the following theorem shows, this randomized set achieves the guarantee stated in (3).

**Theorem 2.1** (Proposition 4 of Gibbs et al. ).: _Let \(=\{(X)^{}:^{d}\}\) denote any finite dimensional linear class. Assume that \(\{(X_{i},S_{i})\}_{i=1}^{n+1}\) are exchangeable and that solutions to (4) and its dual are computed symmetrically on the input data. Then, for all \(f\),_

\[[f(X_{n+1})(\{S_{n+1}_{}(X_{n+1 })\}-(1-))]=0.\]

### Conformal factuality

As discussed in the introduction, prediction sets are not suitable for many LLM use-cases. As an alternative, Quach et al.  and Mohri and Hashimoto  use conformal inference to develop filtering methods that remove false claims from an LLM's output. We will focus specifically here on the work of Mohri and Hashimoto  since this is most similar to the new methods that we will propose in this article. Recall that \(_{i}=\{C_{ij}\}_{j=1}^{k_{i}}\) denotes the claims made in an LLM's response and \(_{i}=\{W_{ij}\}_{j=1}^{k_{i}}\) denotes binary variables for which \(W_{ij}=1\) indicates that the claim is true.

Mohri and Hashimoto  aim to output a set of filtered claims, \((_{i})_{i}\), that contains no errors with high probability, i.e.,

\[( C_{(n+1)j}(_{n+1})W_{(n+1)j}=0). \]

To achieve this target, Mohri and Hashimoto  define a scoring function \(p(P_{i},C_{ij})\) that takes a prompt and claim as input and summarizes the LLM's internal confidence in the claim. Here,larger values of \(p(P_{i},C_{ij})\) indicate that the LLM believes that \(C_{ij}\) is more likely to be true. Then, Mohri and Hashimoto  set

\[(_{i}):=F(_{i};)=\{C_{ij}:p(P_{i},C_{ij}) \},\]

where \(\) is the \(\)-quantile of the conformity scores,

\[S(_{i},_{i})=\{ C_{ij}( _{i};),W_{ij}=1\}. \]

Mirroring the proof of split conformal prediction , Theorem 1 of  shows that if \(\{(P_{i},R_{i},_{i},_{i})\}_{i=1}^{n+1}\) are exchangeable, this method satisfies (5).

## 3 Methods

### Generalization to alternative targets

Our first contribution in this paper will be to generalize the conditional framework of Gibbs et al.  to accommodate generic LLM alignment tasks. To do so, we extend the conformal risk control framework [3; 15] to provide high-probability control of a monotone loss with _conditional_ guarantees.

More concretely, suppose that we are given a loss function \(L((_{i}),_{i})\) that measures the quality of the filtered output \((_{i})\) relative to the ground truth \(_{i}\). Our goal will be to ensure that \((L((_{n+1}),_{n+1})) 1-\), for some user-specified tolerance \(>0\). For example, in the prior section \(L(,)\) was the binary indicator that \((_{n+1})\) contains a false claim. More generally, \(L(,)\) could measure the presence of toxic or discriminatory content in the response.

To incorporate general losses into the conditional conformal framework of Gibbs et al. , we require two assumptions. First, we assume that the method is always permitted to abstain from issuing a response and thus \(L(,)=0\). Second, we assume that the loss is monotone. Namely, for any sets of claims \(_{1}(_{i})_{2}(_{i})\), it must be the case that \(L(_{1}(_{i}),W_{i}) L(_{2}(_{i}),W_{i})\).

With these assumptions, we extend the calibration procedure of Gibbs et al.  as follows. Let \(X_{i}=X(P_{i},R_{i})\) denote a set of features computed using the prompt and response. Matching (6), we define \(p(P_{i},C_{ij})\) to be a score measuring the quality of claim \(C_{ij}\). We define the filtered set of claims by \((_{i})=F(_{i};):=\{C_{ij}:p(P_{i},C_{ij} )\}\), and the conformity score via

\[S(_{i},_{i}):=\{ L(F(_{i};), _{i})\}.\]

Our two assumptions (monotonicity of the loss and \(L(,)=0\)) imply that \(S(_{i},_{i})\) is well-defined and equal to the minimum loss-controlling threshold. Finally, we set

\[g_{S}=_{g}_{i=1}^{n}_{} (S(_{i},_{i})-g(X_{i}))+_{}(S-g(X _{n+1})), \]

and filter claims at the cutoff \((X_{n+1})=\{S S g_{S}(X_{n+1})\}\). Similar to the prediction sets of Gibbs et al. , \((X_{n+1})\) can be conservative, and, thus, we prefer to use a randomized analog \(_{}(X_{n+1})\); its formal definition can be found in Appendix A. As the following theorem shows, this randomized cutoff satisfies the desired guarantee.

**Theorem 3.1**.: _Let \(=\{(X)^{}:^{d}\}\) denote any finite dimensional linear class. Assume that \(\{_{i}\}_{i=1}^{n+1}\) are exchangeable, that solutions to (7) and its dual are computed symmetrically in the input data, \(L(,)\) is monotone in its first argument, and \(L(,)=0\). Then, for all \(f\),_

\[[f(X_{n+1})(1\{L((_{n+1};_{}(X _{n+1})),_{n+1})\}-(1-))]=0.\]

As above, we can make this guarantee concrete by choosing \(\) to be a linear combination of group indicators specifying the topic of the prompt, i.e., \(=\{(1\{X G\})^{}^{||}\}\). For any finite set of groups \(\), this choice of \(\) yields the high-probability guarantee,

\[(L(F(_{n+1};_{n+1}),_{n+1})  X_{n+1} G)=1-G.\]

Like other conformal guarantees, the result of Theorem 3.1 only holds _marginally_ over the calibration data, \(\{_{i}\}_{i=1}^{n}\). Nevertheless, Gibbs et al.  observe that the calibration-conditional validity of their method concentrates around the nominal level as the calibration set size grows.

### Level-adaptive conformal prediction

In addition to adapting the filtering cutoff to \(X_{n+1}\), it may also be beneficial to adapt the desired error probability \(\) at test time.3 In this manuscript, we adjust \(\) to ensure sufficient claim retention for each test output. In other scenarios, we might set a stricter \(\) for high-stakes prompts and a more lenient one otherwise. For now, suppose that we are given some desired level function \(()\), and our goal is to ensure that \(L((_{n+1}),_{n+1})\) with probability \(1-(X_{n+1})\).

Recall that in the previous section, we use the pinball loss, \(_{}()\) to learn the \(1-\) quantile of \(S(_{i},_{i})\). To control the error rate adaptively, here we will use a data-dependent loss for the \(i\)-th point, \(_{(X_{i})}()\). Then, similar to the previous section, we define

\[g_{S}=*{argmin}_{g}_{i=1}^{n} _{(X_{i})}(S(_{i},_{i})-g(X_{i}))+ _{(X_{n+1})}(S-g(X_{n+1})), \]

and filter at the cutoff \(_{}(X_{n+1}):=\{S:S g_{S}(X_{n+1})\}\). As in the previous section, it is more convenient to work with a slightly smaller randomized cutoff, \(_{}\). The following theorem shows that our previous guarantee for fixed \(\) extends to this new setting.

**Theorem 3.2**.: _Under the assumptions of Theorem 2.1,_

\[[f(X_{n+1})(\{L((_{n+1};_{ }(X_{n+1})),_{n+1})\}-(1-(X_{n +1})))]=0,\; f.\]

Note that we recover the guarantee presented in (1) by defining

\[()=\{\{() I\}\{ G\}:I ,G\}=\{()^{ }:^{d}\}.\]

While this choice of function class elicits an interpretable guarantee, it can be quite large in practice. This can slow down the computation of the conformal cutoff and reduce claim retention. Alternatively, this class might be chosen to exactly satisfy other popular (and more efficient) approximations to multicalibration such as low-degree multicalibration . In Appendix B.2, we briefly explore the consequences of choosing an insufficiently complex function class in a synthetic regression example. A more substantial exposition of these trade-offs can be found in Gibbs et al. .

Estimating \(()\)While the above theory treats \(()\) as fixed, in order to ensure that the outputs of our method meet our desired quality criterion (e.g., small interval length or sufficient claim retention) we will need to learn \(()\) from the data. To do this, we split our training data into two folds: one is used to estimate \(()\) and the other is used to run the calibration method described above. After making these splits, \(()\) can be learned using any regression method. In our experiments, we will aim to learn the smallest possible values of \(()\) that meet our target quality criterion. A detailed description of our procedure for doing so is given in Appendix B.1.

### Conditional boosting

In this section, we describe a new method for automated conformity score design subject to _conditional coverage_ constraints. As discussed in the introduction, the goal of this procedure is to find conformity scores that when combined with our filtering method, allow the filter to retain as much of the LLM output as possible while still ensuring validity. For the sake of simplicity, we will assume that we are boosting the conformity score defined in (6). Our approach, however, will generalize to any parameterized score function. We note here that the concurrent work of Kiyani et al.  presents another approach to this problem by reframing it as a min-max optimization task.

Let \(p_{}()\) denote a _parameterized_ claim-scoring function that assigns a measure of confidence to each sub-claim. We run the conditional conformal method on a set of size \(n\) and optimize \(\) such that the number of retained claims is maximized on a hold-out set of size \(m\), i.e.,

\[^{*}=*{argmax}_{}_{i=1}^{m}_{j=1}^{k_{n+i}} \{p_{}(P_{n+i},C_{(n+j})_{i} \}. \]Here, \(_{i}\) denotes the filtering threshold output by the conditional conformal method on the held-out test point \(X_{n+i}\). Crucially, \(_{i}=_{i}()\) carries a hidden dependence on \(\): \(_{i}\) is obtained by solving a quantile regression problem on scores that depend on \(p_{}()\).

There are therefore two obstacles to optimizing (9) via gradient descent. First, the indicator function is non-differentiable. To resolve this, we proceed as in Stutz et al.  and approximate the indicator using a sigmoid function. Second, it is unclear how to backpropagate through \(_{i}()\).

Observe that for a linear function class \(=\{(X)^{}:^{d}\}\), the regression (8) by which we obtain \(_{i}()\) is a linear program. Using this observation, we find that \(_{i}()\) can be expressed as the solution to a linear system of equations given by a subset of the dataset that we denote by \(B\), i.e.,

\[_{i}()=(X_{n+i})^{}((X)_{B}^{-1}S_{B}() ). \]

Here, \((X)_{B}^{-1}\) and \(S_{B}()\) are used to denote the matrix of features and vector of scores for the subset of the data given by \(B\). In the linear programming literature, this subset is typically referred to as the "optimal basis." It can be explicitly identified by selecting the points at which the quantile regression interpolates the scores. Then, so long as \(B\) is invariant to small perturbations of \(\) and \(S\) is differentiable in \(\), we can obtain the derivative of \(_{i}()\) with respect to \(\) by backpropagating through (10). Proposition 3.1 identifies a simple condition under which this is possible.

**Proposition 3.1**.: _Let \(_{i}()\) denote the non-randomized filtering threshold defined via (8). Assume that the threshold is finite and that the augmented quantile regression for all \(S>_{i}()\) admits a unique non-degenerate basic solution. Let \(B\) denote this basis. Then, \(_{i}()\) has derivative_

\[_{}_{i}()=(X_{n+i})^{}((X)_{B}^ {-1}_{}S_{B}()). \]

The technical condition on the basis in Proposition 3.1 nearly always holds; if it does not, we remark that uniqueness can always be obtained by "dithering" the scores and/or design matrix, i.e., adding small i.i.d. noise, at each step of the boosting procedure .

The rest of our procedure emulates the **ConfTr** algorithm of Stutz et al. . Having set aside a portion of the dataset for the final calibration procedure, we replicate the conformal prediction algorithm at each iteration. Namely, we randomly split the remaining data into a "calibration" and "test" set. We then run the level-adaptive conformal prediction method on the calibration set, compute the quality criterion on the test set, and backpropagate on the objective given by (9). Algorithm 1, which presents a complete description of the procedure, can be found in Appendix C.

## 4 Experiments

In this section, we present a subset of our experimental results on two previously studied benchmarks in medical question-answering and biography generation. Additional experiments on these data sets, as well as further validation on a synthetic dataset, can be found in Appendix D.

### Medical long-form question-answering

In this section, we summarize the experimental setup for the claims and figures presented in Section 1.1. Our experiment considers the long-form medical question-answering dataset (**MedLFQA**) published in Jeong et al. . It combines several previously established benchmarks in the medical question-answering literature: **HealthSearchQA** (\(n=3047\)), **K-QA** (\(n=1077\)), **LiveQA** (\(n=100\)) and **MedicationQA** (\(n=627\)). Each prompt in these datasets is also accompanied by either an LLM or human-generated response to each question [6; 1; 19; 26]. Following prior work [19; 13], we treat these responses as ground-truth for model evaluation. Also matching Jeong et al. , in our plots, we distinguish between the subset of questions (\(n=201\)) in the K-QA dataset that have gold-standard physician responses (denoted by K-QA Golden) and those questions with LLM-generated responses (denoted by K-QA Silver). To reproduce our results, our GitHub repository includes a filtered and combined dataset that removes some non-health-related prompts contaminating the **HealthSearchQA** dataset.

To obtain the dataset used in our experiment, we query GPT-3.5-Turbo with each of the prompts in the combined dataset, and then ask GPT-4o to parse these prompts into self-contained sub-claims. Then, to obtain our "ground-truth" labels, we ask GPT-3.5-Turbo to evaluate whether each claim is substantiated by the pseudo-ground-truth response for that prompt. The prompts we use for each of these tasks are included in Appendix E.2.

In all of the medical question-answering experiments displayed, we run the conditional conformal method with the following scoring function,

\[S(_{i},_{i})=\{:(_{i};)\}.\]

To run our procedure, we must first split the dataset into two parts. We use the first split to both run the conditional boosting method and estimate \(()\), while we use the second split to run level-adaptive conformal prediction and evaluate our method's guarantees.

On the first split (\(n=1421\)), we boost our claim score and subsequently estimate the level required for \(70\%\) claim retention. We achieve the former by optimizing a linear combination of four scores previously described in the LLM factuality literature: the frequency, self-evaluation, and ordinal scores described in Mohri and Hashimoto  and the token-level probability assigned to a single-token self-evaluation . Appendix E.1 describes how these claim scores are computed. The function class \(\) for which we run the conditional boosting method is defined by the linear combination of several prompt-response features. For the sake of brevity, we defer a detailed description of this function class, our method for estimating \(()\), and ablations of our method to Appendix D.2.

On the second half of this dataset, we run level-adaptive conformal prediction over the \(\) used in the conditional boosting step augmented by indicator functions that correspond to \(()\) falling in some sub-interval with endpoints lying in \(\{0,0.05,,1\}\). Finally, to produce the plots in Figure 2, we run this step over \(100\) random calibration-test splits of size \(2354\) and \(500\), respectively.

### Wikipedia biographies

Our second experiment reconsiders the biography dataset analyzed in Mohri and Hashimoto . We sample \(8516\) names from Wikipedia and query GPT-3.5-Turbo with the prompt "Write me a short biography of [NAME]." We then ask GPT-4o to parse the prompts into self-contained sub-claims. While the prior work only annotates \(25\) biographies, we need to scale up this experiment to validate our proposed methods. To get around expensive human annotation, we use a variant of the FActscore procedure developed by Min et al. . Their method includes relevant Wikipedia passages identified by the BM25 ranking function in the prompt to the LLM and asks if the claims are supported. Scoring of the LLM outputs (i.e., computation of \(p(P_{i},C_{ij})\)) is done using the frequency scoring method in all displayed figures except for the boosting comparison (the right panel of Figure 3). In that plot, we compare the claim retention achieved by an optimal ensemble of

Figure 3: Empirical demonstration of our method on the Wikipedia biographies dataset. The left two panels display results for our level-adaptive method, which aims to issue biographies with 3 or fewer errors, while retaining at least 80% of the original claims in the prompt. The left panel compares the binned nominal probabilities reported by our method with bin width \(2.5\%\) against the true realized empirical values evaluated over 381 test points and 100 calibration-test splits. The center panel compares the number of claims retained by our method (orange) against the fixed level method of Mohri and Hashimoto  (blue). In this plot, the \(y\)-axis displays a moving average of the number of claims retained with window size \(1000\), while the \(x\)-axis a moving average of the number of views (in Jan. 2023) of the Wikipedia article associated with each prompt on the log-scale. Finally, the right-hand panel displays the claim retention obtained with boosted (orange) and unboosted (blue) scores at a fixed level of \(=0.1\). Boxplots in this panel show the distribution of retentions for 100 calibration-test splits each containing 7246 calibration points and 381 test points.

three cheaper-to-compute scores (self-evaluation, ordinal, log-probability) to the retention of a uniform mixture of those scores [21; 14]. A detailed description of the claim scoring and data collection methods mentioned in this paragraph can be found in Appendices E.1 and E.2. Method ablations and experiments comparing the efficacy of each claim scoring approach are included in Appendix D.3.

In all of the experiments displayed here, we run the conditional conformal method with the scoring function given by

\[S(_{i},_{i})=\{:(_{i};) { contains at most 3 false claims}\}.\]

Empirically, we find that the claim scoring methods we apply are not as well-correlated with factuality as they were for **MedLFQA**. As a result, in order to issue non-trivial guarantees, i.e., to ensure both that \(1-(X_{n+1})\) remains reasonably large and that the method does not filter too much of the response, we allow the filter to keep up to three false claims.4 Experimental results for the more typical \(0\)-error guarantee are also included in Appendix D.3. For the sake of avoiding redundancy with the previous subsection, we defer the remaining experimental details to Appendix D.3.

Experimental results displaying the effectiveness of our level-adaptive and conditional boosting procedures on this dataset can be found in Figure 3. In Figure 4, we also provide additional comparisons contrasting the conditional conformal method of Section 3.1 with the marginal method proposed in Mohri and Hashimoto . As anticipated by our theory, we find that our method provides accurate coverage regardless of the popularity of the Wikipedia page, while the split conformal method of Mohri and Hashimoto  gives variable results (left panel). As the right panel of the figure shows, this conditional accuracy is obtained by retaining more claims for frequently viewed topics and less claims for rare topics on which the LLM is more uncertain.

## 5 Limitations

While we believe the described methodology can improve LLM factuality, we highlight some limitations here. First, our theoretical results assume that the prompt-response tuples are i.i.d. In settings where user interactions change over time, the test prompts may be incomparable to previous prompts. Even though our framework can be applied to guarantee validity under a pre-specified class of covariate shifts (see Appendix A.2 for details), robustness guarantees under other types of distribution shift will require additional research. Second, since our filter is a wrapper around existing claim scores and thus the utility of our method depends on the quality of the underlying scoring algorithm. If the claim scores are weakly correlated with factuality, the filtered output will be accompanied by a vacuous nominal guarantee. Nevertheless, discovering new features that accurately predict LLM hallucinations is an active field of research, and our wrapper can easily leverage new approaches.

Figure 4: Comparison of the split conformal calibration method of Mohri and Hashimoto  (blue) against our conditional conformal method (orange). The left and right panels displays the miscoverage and percentage of claims retained by the two methods against the number of views received by the Wikipedia pages in January 2023. The displayed boxplots are computed over \(200\) trials in which we run both methods on a calibration set of \(5890\) points and evaluate their coverage on a test set of size \(2500\). The displayed groups correspond to view counts that are binned into the intervals \((-,)\), \([1000000,)\), \([100000,100000)\), \([1000,100000)\), \([100,10000)\), \([0,1000)\). The fraction of the data set belonging to each group (in plotted order) is \(8.7\%,30.9\%,39.5\%,19.3\%,\) and \(1.5\%\), respectively.