# Multi-Step Generalized Policy Improvement by Leveraging Approximate Models

Lucas N. Alegre\({}^{1,2}\)  Ana L. C. Bazzan\({}^{1}\)  Ann Nowe\({}^{2}\)  Bruno C. da Silva\({}^{3}\)

\({}^{1}\)Institute of Informatics, Federal University of Rio Grande do Sul

\({}^{2}\)Artificial Intelligence Lab, Vrije Universiteit Brussel \({}^{3}\)University of Massachusetts

{lnalegre,bazzan}@inf.ufrgs.br  ann.nowe@vub.be  bsilva@cs.umass.edu

###### Abstract

We introduce a principled method for performing zero-shot transfer in reinforcement learning (RL) by exploiting approximate models of the environment. Zero-shot transfer in RL has been investigated by leveraging methods rooted in generalized policy improvement (GPI) and successor features (SFs). Although computationally efficient, these methods are model-free: they analyze a library of policies--each solving a particular task--and identify which action the agent should take. We investigate the more general setting where, in addition to a library of policies, the agent has access to an _approximate_ environment model. Even though model-based RL algorithms can identify near-optimal policies, they are typically computationally intensive. We introduce \(h\)-GPI, a multi-step extension of GPI that interpolates between these extremes--standard model-free GPI and fully model-based planning--as a function of a parameter, \(h\), regulating the amount of time the agent has to reason. We prove that \(h\)-GPI's performance lower bound is strictly better than GPI's, and show that \(h\)-GPI generally outperforms GPI as \(h\) increases. Furthermore, we prove that as \(h\) increases, \(h\)-GPI's performance becomes arbitrarily less susceptible to sub-optimality in the agent's policy library. Finally, we introduce novel bounds characterizing the gains achievable by \(h\)-GPI as a function of approximation errors in both the agent's policy library and its (possibly learned) model. These bounds strictly generalize those known in the literature. We evaluate \(h\)-GPI on challenging tabular and continuous-state problems under value function approximation and show that it consistently outperforms GPI and state-of-the-art competing methods under various levels of approximation errors.

## 1 Introduction

Reinforcement learning (RL) (Sutton and Barto, 2018) algorithms have achieved remarkable performance in challenging tasks both in the model-free (Bellemare et al., 2020; Wurman et al., 2022) and the model-based (Bryant et al., 2022; Wu et al., 2022) settings. In these problems, agents are typically trained to optimize one particular reward function (a _task_). However, designing agents that can adapt their decision-making policies to solve novel tasks in a zero-shot manner--i.e., without requiring any further learning--remains an important open problem.

Various principled and efficient policy transfer methods, which combine previously-acquired policies to solve new tasks in a zero-shot manner, have been proposed based on the combination of successor features (SFs) and generalized policy improvement (GPI) (Barreto et al., 2017; Borsa et al., 2019; Barreto et al., 2020; Kim et al., 2022). On the one hand, SFs allow agents to efficiently evaluate the performance of given policies in arbitrary sets of tasks. On the other hand, GPI extends the classic policy improvement procedure (Sutton and Barto, 2018) by analyzing a library of previously-learnedpolicies--each solving a particular task--and identifying a novel policy that simultaneously improves upon all policies in the agent's library.

We introduce \(h\)-GPI, a multi-step extension of GPI that interpolates between these extremes--standard model-free GPI and fully model-based planning--as a function of a parameter, \(h\), regulating the amount of time the agent has to reason. An \(h\)-GPI policy is computed by performing online planning for \(h\) steps, using an approximate model, and then employing GPI on all states reachable in \(h\) steps to identify (in a zero-shot manner) which actions the agent should take thereafter. See Figure 1 for an example.

Our work is partially motivated by the observation that state-of-the-art model-based RL algorithms (Hafner et al., 2019, 2021) often adapt poorly to local changes in the environment (Wan et al., 2022). Furthermore, environment models may have been learned and thus might be inaccurate. These observations underscore the risks involved in relying solely on approximate models for long-horizon planning scenarios, as model errors compound and may lead to catastrophic outcomes. Unlike techniques for dealing with approximate models, our method additionally exploits GPI's capabilities of efficiently performing zero-shot transfer in a model-free manner. Other approaches--similar in nature to \(h\)-GPI--make use of models and GPI-based techniques. Thakoor et al. (2022), for example, introduced GGPI, a method that learns state visitation models induced by particular policies to rapidly evaluate policies under a given, known reward function. The authors show that performing GPI over a particular type of non-stationary policy produces behaviors that outperform those in the agent's library policy. \(h\)-GPI, by contrast, learns a different type of model: an environment model, which is used to perform planning--i.e., action selection--rather than policy evaluation. Moreover, \(h\)-GPI exploits GPI to perform bootstrapping from all states reachable in \(h\) steps. In Section 5, we contrast our method with other related techniques, highlighting the unique aspects of our approach.

In this paper, we introduce the first principled method capable of exploiting approximate models and producing a multi-step extension of GPI with formal guarantees. We prove that \(h\)-GPI's performance lower bound is strictly better than GPI's, and show that \(h\)-GPI generally outperforms GPI as \(h\) increases. Furthermore, we prove that as \(h\) increases, \(h\)-GPI's performance becomes arbitrarily less susceptible to value function approximation errors in the agent's policy library. Finally, we introduce novel bounds characterizing the gains achievable by \(h\)-GPI as a function of approximation errors in both the agent's policy library and its (possibly learned) model. We empirically evaluate \(h\)-GPI in challenging tabular and continuous-state problems with value function approximation. Our findings show that it consistently outperforms both GPI and other state-of-the-art methods under various levels of approximation errors. These results, combined with our method's formal guarantees, indicate that \(h\)-GPI is an important first step towards bridging the gap between model-free GPI-based methods and fully model-based planning algorithms--while being robust to approximate models of the environment and value function estimation errors.

## 2 Background

Before introducing our contributions, we review key concepts and definitions related to model-free and model-based RL, SFs, and GPI.

### Reinforcement learning

RL problems (Sutton and Barto, 2018) are typically modeled as _Markov decision processes_ (MDPs). An MDP is a tuple \(M(,,p,r,,)\), where \(\) is a state space, \(\) is an action space, \(p(|s,a)\) denotes the distribution over next states conditioned on a state and action, \(r:\) is a reward function, \(\) is an initial state distribution, and \([0,1)\) is a discount factor. Let \(S_{t}\), \(A_{t}\), and \(R_{t}=r(S_{t},A_{t},S_{t+1})\) be random variables corresponding to the state, action, and reward, respectively,

Figure 1: \(h\)-GPI combines online planning with GPI leveraging a learned model.

at time step \(t 0\). The goal of an RL agent is to learn a policy \(:\) that maximizes the expected discounted sum of rewards (_return_) \(_{t=0}^{}^{t}R_{t}\). The action-value function of a policy \(\) is \(q^{}(s,a)_{}[_{i=0}^{}^{i}R_{t+i}|S_{ t}=s,A_{t}=a]\), where \(_{}[]\) denotes the expectation over trajectories induced by \(\) and \(p\). Given \(q^{}\), a _greedy_ policy can be defined as \(^{}(s)_{a}q^{}(s,a)\). It is guaranteed that \(q^{^{}}(s,a) q^{}(s,a)\), for all \((s,a)\). Computing \(q^{}\) and \(^{}\) is done by processes known, respectively, as _policy evaluation_ and _policy improvement_. Repeatedly alternating between policy evaluation and policy improvement steps is known to lead to an optimal policy, \(^{*}(s)\), which maximizes the expected return from all states \(s\)(Puterman, 2014).

### Successor features and GPI

The _successor features_ (SFs) framework (Barreto et al., 2017) allows agents to efficiently evaluate the performance of a given policy when deployed under any linearly-representable reward functions, \(r_{}(s,a,s^{})=(s,a,s^{})\), where \((s,a,s^{})^{d}\) are reward features and \(^{d}\) are weights. A controlled Markov process \((,,p,,)\), i.e., an MDP without a reward function, when combined with reward features \(:^{d}\), induces a family of MDPs:

\[^{}\{M=(,,p,r_{},,) r_{}(s,a,s^{})=(s,a,s^{}) \}. \]

Notice that the family of MDPs \(^{}\) represents all possible tasks (each defined by a reward function) that can be defined in the environment associated with the corresponding controlled Markov process. Given a policy \(\), its corresponding SFs, \(^{}(s,a)^{d}\), for a state-action pair \((s,a)\) are defined as

\[^{}(s,a)_{}[_{i=0}^{}^ {i}_{t+i} S_{t}=s,A_{t}=a], \]

where \(_{t}(S_{t},A_{t},S_{t+1})\). Importantly, notice that given the SFs \(^{}(s,a)\) of a policy \(\), it is possible to _directly_ compute the action-value function \(q^{}_{}(s,a)\) of \(\), under _any_ linearly-expressible reward functions, \(r_{}\), as follows:1

\[q^{}_{}(s,a)=_{}[_{i=0}^{}^{i} _{t+i} S_{t}=s,A_{t}=a]=^{}(s,a ). \]

The equation described above--which uses SFs to evaluate a given policy under different reward functions--represents a process known as _generalized policy evaluation_ (GPE) (Barreto et al., 2020). Notice that this process can be extended to the case where an agent has access to a set of previously-learned policies, \(=\{_{i}\}_{i=1}^{n}\), and their corresponding SFs, \(=\{^{_{i}}\}_{i=1}^{n}\). Then, given any \(_{i}\), GPE can be used to efficiently evaluate \(_{i}\) under any arbitrary reward function of interest, \(r_{}\), via Equation (3): \(q^{_{i}}_{}(s,a)=^{_{i}}(s,a)\).

**Generalized policy improvement.** GPI generalizes the standard policy improvement process (discussed in Section 2.1). It assumes access to a _set_ of policies (and corresponding action-value functions) and uses it to directly identify a novel policy optimizing a particular reward function, \(r_{}\). Importantly, the novel policy is guaranteed to outperform all original policies the agent had access to.

**Definition 1**.: _(Barreto et al., 2020) Given a set of policies \(\) and a reward function \(r_{}\), generalized policy improvement (GPI) is the process by which a policy, \(^{}\), is identified such that_

\[q^{^{}}_{}(s,a)_{}q^{}_{}(s,a )\;\;\;(s,a). \]

Based on Equation (4) and the reward decomposition \(r_{}(s,a,s^{})=(s,a,s^{})\), a _generalized policy_, \(:\), can then be defined as follows:

\[^{}(s;)_{a}_{}q^ {}_{}(s,a). \]

Let \(q^{}_{}(s,a)\) be the action-value function of policy \(^{}(;)\). The GPI theorem (Barreto et al., 2017) shows that \(^{}(;)\) satisfies Definition 1, and so Equation (5) can be used to identify a policy guaranteed to perform at least as well as any other policies \(_{i}\), when tackling a new task, \(\). This theorem can be extended to cases where the agent only has access to an approximation of the action-value function, \(^{_{i}}\), of a policy \(_{i}\)(Barreto et al., 2018).

### Model-based RL

In model-free RL, agents do not have access to the environment state transition and reward functions, \(p\) and \(r\), and must, instead, learn a policy solely based on samples obtained while interacting with the environment. Given an MDP, we denote \(m(p,r)\) as its _model_. In _model-based_ RL, an agent is given (or has to learn) an approximate model, \( m\). Agents may exploit models in different ways; for example, _(i)_ to perform _background_ planning (i.e., generating simulated experiences to more rapidly learn a policy or value function, as in Dyna-style algorithms (Van Seijen and Sutton, 2013; Janner et al., 2019; Pan et al., 2019)); and _(ii)_ to perform _online_ planning (Barto et al., 1995; Chua et al., 2018; Efroni et al., 2020; Hansen et al., 2022).

In online planning methods, agents use a model to estimate the value of states by simulating the outcomes of a particular sequence of actions. This is also known as "unrolling the model forwards", from the current state, for a given number of time steps. Typically, online planning procedures are performed from every state the agent encounters, as in model-predictive control (MPC) algorithms (Camacho and Alba, 2013; Chua et al., 2018). Importantly, recent work--such as that of Wan et al. (2022)--has shown that state-of-the-art model-based RL algorithms (Hafner et al., 2019, 2021) often fail catastrophically when deployed in settings where the environment (e.g., the MDP's reward function) may change. This observation underscores the risks involved in relying solely on approximate models for long-horizon planning scenarios, since it is well-known that model errors compound.

We address these limitations by introducing a zero-shot transfer technique based on online planning. Our technique interpolates between standard model-free GPI and fully model-based planning according to a parameter, \(h\), regulating the amount of time the agent has to reason using an approximate model. Unlike previous methods that require re-learning or adapting models to tackle novel tasks, our technique exploits approximate models and SFs to solve any tasks (defined as linearly-expressible reward functions) _without requiring any further learning_. Importantly, it has strong theoretical guarantees regarding its performance lower bound under various types of approximation errors.

## 3 Multi-step generalized policy improvement

We now introduce our main contribution, \(h\)-GPI. This is a multi-step extension of GPI that combines online planning with approximate environment models to efficiently perform zero-shot transfer.

We start by defining the Bellman optimality operator \(^{*}\) (and its multi-step variant) applied to action-value functions \(q\), where \(\) is the space of action-value functions. To simplify notation, given a model \(m(p,r)\), we denote \(_{m}[]\) as the expectation operator with respect to \(S_{t+1} p(.|S_{t},A_{t})\) and \(R_{t} r(S_{t},A_{t},S_{t+1})\). Similarly, we write \(r(s,a)_{S_{t+1} p(|s,a)}[r(s,a,S_{t+1})]\) for brevity.

**Definition 2**.: _(Single- and multi-step Bellman optimality operators). Given a model \(m=(p,r)\), the single-step Bellman optimality operator, \(^{*}:\), is defined as:_

\[^{*}q(s,a)_{m}[r(s,a)+_{a^{ }}q(S_{t+1},a^{})|S_{t}=s,A_{t}=a]. \]

_The repeated application of \(^{*}\) for \(h\) steps gives rise to the \(h\)-step Bellman operator, denoted \((^{*})^{h}q(s,a)^{*} ^{*}}_{h}q(s,a)\). Efroni et al. (2018) showed that \((^{*})^{h}\) can be written as follows:_

\[(^{*})^{h}q(s,a) _{m}[r(s,a)+_{a^{} }(^{*})^{h-1}q(S_{t+1},a^{})|S_{t}=s,A_{t}=a] \] \[=_{_{1}_{h-1}}_{m}[_{k=0}^{h-1 }^{k}r(S_{t+k},_{k}(S_{t+k}))+^{h}_{a^{}}q(S_{t+h},a^{})|S_{t}=s,_{0}(s)=a], \]

_where \(_{i}\) is any policy (in an arbitrary policy space) the agent could choose to deploy at time \(i\)._

It is well known that \(^{*}\) is a contraction mapping and its fixed-point (for any initial \(q\)) is \(_{h}(^{*})^{h}q(s,a)=q^{*}(s,a)\)(Puterman, 2014).

We now introduce our first contribution: \(h\)_-step generalized policy improvement_, or \(h\)-GPI:

**Definition 3**.: _Let \(=\{_{i}\}_{i=1}^{n}\) be a set of policies and \(m=(p,r)\) be a model. Then, given a horizon \(h 0\), the \(h\)-GPI policy on state \(s\) is defined as_

\[^{h}(s)*{arg\,max}_{a}(^ {*})^{h}_{}q^{}(s,a) \]

\[=*{arg\,max}_{a}_{_{1},,_{h-1}} _{m}^{h-1}^{k}r(S_{t+k},_{k}( S_{t+k}))}_{}+}_{a^{}}_{ }q^{}(S_{t+h},a^{})\,|_{0}(S_{t})=a \]

_where \(_{i}\) is any policy (in an arbitrary policy space) the agent could choose to deploy at time \(i\)._

Intuitively, \(h\)-GPI identifies a policy that returns the best possible action, \(a\), by first planning with model \(m\) for \(h\) steps and then using GPI to estimate the future returns achievable from all states reachable in \(h\) steps. This is in contrast with standard GPI policies, which can only reason about the future returns achievable from the current state, \(S_{t}\), if following a given policy in \(\). \(h\)-GPI, by contrast, uses a model to reason over the decisions made at states within \(h\) steps from \(s\), as well as states, \(S_{t+h}\), reachable after \(h\) steps. This makes it possible for \(h\)-GPI to produce policies that exploit the model-free, zero-shot return estimates produced by GPI when evaluating states beyond a given horizon \(h\); in particular, states that would otherwise not be considered by the standard GPI procedure when determining return-maximizing actions. Notice, furthermore, that \(h\)-GPI by construction interpolates between model-free GPI and fully model-based planning:

\[) standard model-free GPI (when $h=0$) and}}\] (_ii_) fully model-based planning (when \[h\] ).

In practice, agents seldom have access to the true model, \(m\), of an MDP, and instead rely on approximate models, \(\). The application of the Bellman operator under an approximate model, \(\), rather than the true MDP model, \(m\), can be represented mathematically by replacing \(m\) with \(\) in Equation (6). This gives rise to a _model-dependent_ Bellman optimality operator, which we call \(^{*}_{}\). In what follows, we introduce Theorem 1, which extends the original GPI theorem (Barreto et al., 2017) and allows us to characterize the gains achievable by \(h\)-GPI as a function of approximation errors in both the agent's policy library (II) and its (possibly learned) model \(\). Notice that Theorem 1 generalizes the original GPI theorem, which is recovered when \(h=0\).

**Theorem 1**.: _Let \(=\{_{i}\}_{i=1}^{n}\) be a set of policies, \(\{^{_{i}}\}_{i=1}^{n}\) be approximations of their respective action-value functions, and \(=(,)\) be an approximate model such that, for all \(_{i}\) and all \((s,a)\),_

\[|q^{_{i}}(s,a)-^{_{i}}(s,a)|,_{s^{}}| p(s^{}|s,a)-(s^{}|s,a)|_{p},|r(s,a)-(s,a)|_{r}. \]

_Recall once again the definition of \(h\)-GPI (Definition 3):_

\[^{h}(s)*{arg\,max}_{a}(^{*}_{})^{h}_{}^{}(s,a). \]

_Then,_

\[q^{h}(s,a) (^{*})^{h}_{}q^{}(s,a)-(^{h}+c(_{r},_{p},h)) \] \[_{}q^{}(s,a)-(^{h} +c(_{r},_{p},h)), \]

_where \(c(_{r},_{p},h)=}{1-}(_{r}+ _{p}v^{*}_{})\), and \(v^{*}_{}|_{s,a}q^{*}(s,a)|\)._

Theorem 1 characterizes the performance lower bound of \(h\)-GPI as a function of the number of planning steps, \(h\), and the approximation errors in the agent's model and action-value functions (i.e., errors \(\), \(_{p}\), and \(_{r}\)).

[MISSING_PAGE_FAIL:6]

transition \((s,a,,s^{})\) and \(N(s,a)\) is the number of times the agent selected action \(a\) in state \(s\). To efficiently compute the action given by the policy \(^{h}(s,)\) (line 3 of Algorithm 1), we extend the Forward-Backward Dynamic Programming (FB-DP) algorithm (Efronti et al., 2020) to the discounted-SFs setting, in order to compute \(h\)-lookahead policies in real-time. The corresponding pseudocode can be found in Appendix B. Notably, given a state \(s\), FB-DP time complexity is \(O(N|||^{}_{h}|)\), where \(N\) is the maximal number of accessible states in one step (maximal "nearest neighbors" from any state), and \(^{}_{h}\) is total reachable states in \(h\) time steps from state \(s\). Note that while we chose to extend FB-DP due to its efficiency and closed-form formulation, other planning techniques could have been used (e.g., Monte Carlo tree search (Tesauro and Galperin, 1996; Silver et al., 2017)).

**Continuous-states setting.** In the continuous-state setting, we extend the class of models composed of ensembles of probabilistic neural networks (Chua et al., 2018) to the SFs setting. These models are used in state-of-the-art single-task model-based RL algorithms (Janner et al., 2019; Yu et al., 2021). The learned model \(_{}(s^{},|s,a)\), parameterized by \(\), is composed of an ensemble of \(n\) neural networks, \(\{_{_{i}}\}_{i=1}^{n}\), each of which outputs the mean and diagonal covariance matrix of a multivariate Gaussian distribution: \(m_{_{i}}(S_{t+1},_{t} S_{t},A_{t})=(_{ _{i}}(S_{t},A_{t}),_{_{i}}(S_{t},A_{t}))\). Each model in the ensemble is trained in parallel to minimize the following negative log-likelihood loss function, using different bootstraps of experiences in a buffer \(=\{(S_{t},A_{t},_{t},S_{t+1})|t 0\}\): \(()=_{(S_{t},A_{t},_{t},S_{t+1}) }[- m_{}(S_{t+1},_{t}|S_{t},A_{t})]\). In practice, we use as \(\) the buffer with experiences the agent collected while training the SFs for the training tasks. In order to compute the \(h\)-GPI policy in this setting, we approximate the expectation over next states by averaging over the predictions of the components of the ensemble (Chua et al., 2018).

We employ _universal successor features approximators_ (USFAs) (Borsa et al., 2019) to learn SFs in the function approximation setting. Given sufficiently expressive USFAs, we can evaluate the value function of _any_ policy \(_{}\) (optimal for task \(\)) in _any_ task \(^{d}\) by simply computing \(^{_{}}_{}(s,a)}(s,a,)\). Furthermore, when using a USFA to generalize to a new task \(^{}\), GPI (Equation 5) becomes: \(^{}(s;)_{a}_{ }}(s,a,)^{}\), where \(\) is typically a set of weight vectors used when training the USFA. Notably, we only require a single USFA \(}\) and a single model \(\) to perform \(h\)-GPI given any \(M_{}\), in contrast to other approaches that require learning a complex model for each policy in the library (Thakoor et al., 2022).

Notice that although we assume that during training the agent can observe and add to its buffer \(\) the features \(_{t}\) at each time step \(t\), our method could also be applied to the setting where the features are learned beforehand in a pre-training phase (Hansen et al., 2020; Carvalho et al., 2023). Learning appropriate reward features \(\) is an important but orthogonal problem to the one tackled in this paper.

## 4 Experiments

We conduct tabular and deep RL experiments in three different domains to evaluate the effectiveness of \(h\)-GPI as a method for zero-shot policy transfer.

**Environments.** First, we consider the tabular FourRoom domain (Barreto et al., 2017). To make this domain more challenging, we made its transition function stochastic by including a \(10\%\) chance of the agent moving to a random direction after taking any action. In FourRoom, the reward features \(_{t}^{3}\) correspond to binary vectors indicating whether the agent collected each of the three types of objects in the map. The second domain is Reacher (Alegre et al., 2022a), which consists of a robotic arm that can apply torque to each of its two joints. The features \(_{t}^{4}\) are proportional to the distance of the tip of the arm to four different targets. Finally, we extend the FetchPush domain (Plappert et al., 2018), which consists of a fetch robotic arm that must move a block to a target position on top of a table by pushing the block with its gripper. The reward features \(_{t}^{4}\) are the negative distances of the block to each of the four target locations. The action space was discretized in the same way as usually done in Reacher. Importantly, the state space of this domain is high-dimensional (\(^{19}\)) and its dynamics are significantly more complex than Reacher's. A more detailed description of the domains can be found in Appendix B.

**Baselines.** We compare \(h\)-GPI with standard GPI (Barreto et al., 2017), which is equivalent to \(h\)-GPI when \(h=0\). To showcase the importance of combining model-based planning with model-free action-value estimates, we include an SF-based MPC approach, which is equivalent to \(h\)-GPI but without GPI bootstrapping at time step \(h\). That is, given a task \(\), it selects the first action from a sequence of actions that maximizes \(_{}[_{k=0}^{h-1}^{k}}_{t+k} ]\), and re-plans at every time step. In the function approximation case with USFAs, we also compare with Constrained GPI (CGPI) (Kim et al., 2022). CGPI is a state-of-the-art method that computes lower and upper bounds for the optimal value on new tasks and uses them to constrain value approximation errors when following the GPI policy.

Based on the theoretical properties introduced in Section 3, we expect the following hypotheses to hold. **H1:**\(h\)-GPI performance better generalizes to unseen test tasks than standard GPI and, in general, improves as we increase \(h\). **H2:**\(h\)-GPI is more robust to approximation errors in the learned SFs than GPI, and such errors have a decreasing impact as \(h\) increases. Additionally, we investigate whether **H3:**\(h\)-GPI can be scaled up to solve problems with high-dimensional state spaces. In each experiment, we report the median, interquartile mean (IQM), and mean normalized returns with their 95% stratified bootstrap confidence intervals (Agarwal et al., 2021), which are obtained by evaluating agents trained with 20 different random seeds on a set of test tasks. For fairness of comparison, we used the same agents for evaluating each method. We follow previous works (Borsa et al., 2019; Kim et al., 2022) and use as training tasks the weight vectors that form the standard basis of \(^{d}\) in all three domains. In FourRoom, we use 32 weights vectors equally spaced from the weight simplex \(\{_{i=1}^{d}w_{i}=1,w_{i} 0\}\) as test tasks. For Reacher and FetchPush, we follow Kim et al. (2022) and use weight vectors defined by \(\{-1,1\}^{d}\) as test tasks. Notice that this results in many tasks with negative weights, which are significantly different than the training tasks.

In Figure 2, we can observe that \(h\)-GPI is able to outperform GPI even with small planning horizons (see, e.g., 1-GPI). As \(h\) increases, the performance on the test tasks consistently increases. This is in accordance with Theorem 1 and **H1**. Notice that even with a small planning horizon of \(h=2\), \(h\)-GPI is capable of matching the performance of SF-MPC even when SF-MPC is allowed to plan for _ten times_ longer (i.e., \(h=20\)).

Next, to investigate **H2**, we evaluate \(h\)-GPI after artificially adding Gaussian noise with different standard deviations to the learned SFs of each of the agent's policies for every state and action. We can notice (in Figure 3) that standard GPI (\(h=0\)) catastrophically fa

Figure 3: \(h\)-GPI, under various levels of errors in the SFs, as a function of \(h\).

Figure 2: Median, IQM, and mean normalized returns over a set of test tasks in the stochastic FourRoom domain. \(h\)-GPI performance significantly improves as \(h\) increases.

approximation errors. In all cases, \(h\)-GPI can closely approximate the same performance levels as those resulting from settings with _no_ added noise. To do so, it suffices to use a sufficiently large \(h\). In other words, \(h\)-GPI successfully and effectively identifies policies that are robust to noise due to its ability to combine model-free GPI generalization and planning under approximate/imperfect models for \(h\) steps. This is consistent with our theoretical results that show that value function approximation errors impact the performance's lower bound by a factor of \(^{h}\).

In Figures 4 and 5, we show the results in the Reacher and FetchPush domains, respectively. Given a test task \(^{}\), let GPI-ST be the policy that uses both the source/training (S) and target (T) tasks, \(\{^{}\}\), as input to the USFA when performing GPI. Similarly, we refer to GPI as the policy that performs GPI only over the training tasks \(\). The CGPI policy was unable to significantly improve performance over GPI-ST, which we hypothesize may be due to USFA errors in the training tasks, or due to overly small minimum rewards used in its upper-bound. Both are known limitations of CGPI (Kim et al., 2022). In both domains with function approximation using USFAs, \(h\)-GPI outperforms the competing methods, which supports **H1** and **H3**. In the FetchPush domain, in particular, SF-MPC is allowed to plan for up to \(h{=}15\) steps; that is, _three times_ longer than the maximum horizon we allow \(h\)-GPI to plan. Again, \(h\)-GPI consistently outperforms it (and other baselines) even when allowed to plan for significantly fewer steps. Interestingly, in the FetchPush domain, \(h\)-GPI's performance slightly decreases for intermediate values of \(h\), which suggests that its policy is being affected by model approximation errors. Even so, the mean returns achieved, for all values of \(h\) are higher than the ones achieved by GPI. This demonstrates the robustness of \(h\)-GPI to approximate models of the environment and function approximation errors.

## 5 Discussion and related work

We now discuss the works most closely related to \(h\)-GPI. Further discussions can be found in Appendix C.

**GPI and SFs.** Previous works have extended GPI to safe RL (Gimelfarb et al., 2021; Feng et al., 2023), maximum-entropy RL (Hunt et al., 2019), unsupervised RL (Hansen et al., 2020), and hierarchical RL (e.g., via the options framework) (Barreto et al., 2019; Machado et al., 2023). Recently, Thakoor et al. (2022) introduced Geometric GPI (GGPI). GGPI uses geometric horizon models (GHMs) to learn the discounted future state-visitation distribution induced by particular policies to rapidly

Figure 4: Median, IQM, and mean normalized returns over test tasks in the Reacher domain.

Figure 5: Median, IQM, and mean normalized returns over test tasks in the FetchPush domain.

evaluate policies under a given, known reward function. The authors show that performing GPI over a particular type of non-stationary policy produces behaviors that outperform those in the agent's library policy. \(h\)-GPI, by contrast, learns a different type of model: an environment model, which is used to perform planning--i.e., action selection--rather than policy evaluation. Additionally, GGPI requires learning separate GHMs for each policy in the library, whereas \(h\)-GPI can operate with a single model that predicts the next reward features, alongside a single USFA. Secondly, GGPI assumes that the reward function is known _a priori_, while we exploit SFs to generalize over all linear rewards given reward features. Bagot et al. (2023) introduced GPI-Tree Search (GPI-TS), which is closely related to \(h\)-GPI. GPI-TS uses GPI bootstrapping as backup value estimates at the leaf nodes of Monte Carlo tree search. However, GPI-TS does not employ SFs and was only designed to tackle single-task settings. Moreover, it assumes an oracle model of the environment. We, by contrast, exploit learned models to perform zero-shot transfer over multiple tasks and show how approximation errors in the model affect the performance of the \(h\)-GPI policy (see Theorems 1 and 2). Kim et al. (2022) introduced Constrained GPI (CGPI), which uses lower and upper bounds of the value of a new task to constrain the action-value estimates used when selecting the GPI policy's action. Although CGPI is able to deal with generalization errors in the values given by USFAs for target tasks, it is sensitive to errors in the value estimates for the training tasks. \(h\)-GPI, by contrast, can deal with approximation errors in the value functions of the source/base policies. Other works have studied methods for constructing a policy library for use with GPI (Zahavy et al., 2021; Nemecek and Parr, 2021; Alver and Precup, 2022; Alegre et al., 2022a) and for learning different SF-based representations (Lehnert and Littman, 2020; Touati and Ollivier, 2021). These methods solve important orthogonal problems and could potentially be combined with \(h\)-GPI.

**Multi-step RL algorithms.** Multi-step RL methods were extensively studied for policy evaluation, both in the model-free (Hessel et al., 2018; van Hasselt et al., 2018) and the model-based settings (Yao et al., 2009; Janner et al., 2019). Model value expansion algorithms (Feinberg et al., 2018; Buckman et al., 2018; Abbas et al., 2020) are a significant example of the latter. In this work, by contrast, we introduce a multi-step method for policy improvement in transfer learning settings. GX-Chen et al. (2022) introduced the \(\)-return mixture, a new backup target for better credit assignment, which combines bootstrapping with standard value estimates and SFs, as a function of a parameter \(\). Efroni et al. (2018) studied multi-step greedy versions of the well-known dynamic programming (DP) policy iteration and value iteration algorithms (Bertsekas and Tsitsiklis, 1996), and Efroni et al. (2020) proposed a multi-step greedy real-time DP algorithm that replaces 1-step greedy policies with a \(h\)-step lookahead policy. Our work is also related to the techniques introduced by Sikchi et al. (2022) and Hansen et al. (2022), which combine planning and bootstrapping with learned value estimates. However, unlike \(h\)-GPI, these methods do not address the multi-policy and zero-shot transfer settings. We also note that there exists neuroscientific evidence that multi-step planning occurs in the brains of humans and other animals (Miller and Venditto, 2021). Finally, we believe that our work also shares similarities with the investigation performed by Tomov et al. (2021) on how model-free and model-based mechanisms are combined in human behavior.

## 6 Conclusions

We introduced \(h\)-GPI, a multi-step extension of GPI that interpolates between standard model-free GPI and fully model-based planning. Through novel theoretical and empirical results, we showed that \(h\)-GPI effectively exploits approximate models to solve novel tasks in a zero-shot manner. Notably, in our experiments, and consistent with the introduced theorems, we showed that \(h\)-GPI is less susceptible to value approximation errors and that it outperforms standard GPI and state-of-the-art competing baselines. Our findings hold even in high-dimensional problems where imperfect learned models are used. These results, combined with our method's strong formal guarantees, indicate that \(h\)-GPI is an important first step towards bridging the gap between model-free GPI-based methods and model-based planning algorithms--while also being robust to approximation errors.

**Limitations and future work.** The main limitation of \(h\)-GPI is that it can introduce computational overhead for large values of \(h\). In future work, this can be tackled by designing heuristics for use when performing online planning, or via principled methods to dynamically select the best value of \(h\). Another interesting direction is to integrate uncertainty estimation techniques (e.g., _implicit value ensemble_(Filos et al., 2022)) into the \(h\)-GPI policy to further reduce the impact of high model or value function approximation errors. Finally, regarding potential direct negative societal impacts of this work, we do not anticipate any.