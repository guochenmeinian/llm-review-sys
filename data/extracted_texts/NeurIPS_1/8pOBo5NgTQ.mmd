# FLSL: Feature-level Self-supervised Learning

Qing Su\({}^{1}\), Anton Netchaev\({}^{2}\), Hai Li\({}^{3}\), and Shihao Ji\({}^{1}\)

\({}^{1}\)Georgia State University, \({}^{2}\)U.S. Army ERDC, \({}^{3}\)Duke University

To whom correspondence should be addressed: qsu3@gsu.edu

###### Abstract

Current self-supervised learning (SSL) methods (_e.g._, SimCLR, DINO, VICReg, MOCOv3) target primarily on representations at instance level and do not generalize well to dense prediction tasks, such as object detection and segmentation. Towards aligning SSL with dense predictions, this paper demonstrates for the first time the underlying _mean-shift_ clustering process of Vision Transformers (ViT), which aligns well with natural image semantics (_e.g._, a world of objects and stuffs). By employing transformer for joint embedding and clustering, we propose a bi-level feature clustering SSL method, coined Feature-Level Self-supervised Learning (FLSL). We present the formal definition of the FLSL problem and construct the objectives from the _mean-shift_ and _k-_means perspectives. We show that FLSL promotes remarkable semantic cluster representations and learns an encoding scheme amenable to _intra-view_ and _inter-view_ feature clustering. Experiments show that FLSL yields significant improvements in dense prediction tasks, achieving 44.9 (+2.8)% AP and 46.5% AP in object detection, as well as 40.8 (+2.3)% AP and 42.1% AP in instance segmentation on MS-COCO, using Mask R-CNN with ViT-S/16 and ViT-S/8 as backbone, respectively. FLSL consistently outperforms existing SSL methods across additional benchmarks, including UAV object detection on UAVDT, and video instance segmentation on DAVIS 2017. We conclude by presenting visualization and various ablation studies to better understand the success of FLSL. The source code is available at [https://github.com/ISL-CV/FLSL](https://github.com/ISL-CV/FLSL)]

## 1 Introduction

Following its success in natural language processing (NLP) , self-supervised learning (SSL) with transformer  has emerged as a highly effective strategy and a popular model choice over the CNN-based counterparts in vision tasks. The remarkable performance achieved by SSL has been demonstrated by SimCLR , MOCOv3 , DINO , VICReg , SwAV , BYOL , and among others. Without relying on manual supervision, a successful paradigm of SSL promotes semantic representations conducive to the downstream tasks, _e.g._, classification, detection and segmentation. However, most existing SSL methods operate at the instance-level, where an encoder is trained to maximize the agreement of the representations of multiple augmented views of an image. Though demonstrating strong performance on the classification tasks , the instance-level SSL is inherently misaligned with the dense prediction tasks, such as object detection, where the lower level semantic information plays a bigger role than the instance-level semantic information. This leads to inferior transferability to those dense prediction tasks.

Recent attempts to bridge the semantic gap are mainly based on region , patch , or pixel (_i.e._, dense feature) matching tasks  with optional instance-level objectives. However, learning of distinct representation for each image patch or region still mismatches the natural semantics within an image (referred to as local semantics), where features of the same semantics should be highly correlated other than being distinct. Semantics can range from features of high similarity, features of the same object, to more complex semantic structures. In light of this, methods such as SoCo , ORL  and DetCon  leverage the off-the-shelf algorithms, _e.g._, _selectivesearch_ and _Felzenszwalb-Hutenlocher_ algorithm  to impose the semantic constraint to the contrastive learning pipeline. Nonetheless, the inclusion of a non-trainable region proposal module in those methods restricts the model's ability to learn the distinct representations for those RoIs from the rest of an image. This ability is vital in representation learning for object detection.

Existing SSL methods targeting dense prediction primarily focus on the learning of globally semantic representations of image sub-regions, such as RoIs, patches, or pixels. However, these methods fall short with limited consideration for the alignment of those representations with local semantics. This observation leads us to ask the following question: Can we learn a representation that is both locally and globally semantic for a group of features (_e.g._, representing an object or stuff) in an end-to-end trainable SSL approach? To this end, we propose the _Feature Level Self-supervised Learning_ (FLSL). It leverages the _mean-shift_ clustering process inherent in transformer to extract modes as representations and incorporates _k-means_-based SSL approach to ensure that the extracted representations are semantically coherent both locally and globally. Figure. 1 illustrates an overview of FLSL with details to be discussed in Sec. 2.

**Contributions** This paper takes a step forward to bridge the gap between the current SSL methods and downstream dense prediction tasks. Our contributions are summarized as follows:

1. We demonstrate for the first time the connection between the attention mechanism and _mean-shift_ clustering, and reinterpret vision transformer from the perspective of _mean-shift_.
2. By employing transformer for joint embedding and feature clustering, we propose FLSL, an end-to-end trainable SSL method that promotes the representations of feature clusters to be semantic at two levels: (i) intra-view: within an image, and (ii) inter-view: over an entire dataset.
3. The derivation and construction of the FLSL objectves is rooted in _mean-shift_ and the non-empty _k-means_ clustering. Semantic representations on the first level are encouraged by optimizing the intra-cluster affinity with a self-attention layer, while the second-level semantic representations are fostered via non-empty _k-means_ clustering with positive samples retrieved through a cross-attention layer.
4. We validate the synergy between FLSL and ViT, and show significant improvement in transferability of the learned features to dense prediction tasks, including object detection and semantic segmentation. FLSL-pretrained ViT on ImageNet-1k (IN1k) demonstrates superior performance compared to the state-of-the-art ADCLR-IN1k  and MAE  pretrained counterparts. Moreover, it consistently outperforms existing SSL methods across additional benchmarks, including UAV object detection on UAVDT, and video instance segmentation on DAVIS 2017.

## 2 Related work

**SSL for dense prediction** Recent attempts to bridge the gap between common SSL and dense prediction tasks focus primarily on sub-region matching tricks. For example, DenseCL  applies contrastive learning on pairs of patches with highest similarity. However, the patch-matching trick leads to distinct representations with low correlation among patches, which is not well-suited for the semantics of a natural image. On top of the instance-level objective, PixPro  and LC-loss  factor in agreement between positive pixel pairs which are assigned through thresholded-distance in

Figure 1: The bi-level clustering of FLSL. An object or stuff in an image is essentially a cluster of features. Hence, their representations can be extracted as cluster representatives, _e.g._, modes. In FLSL, we aim to make these representations both locally and globally semantic via a bi-level clustering process. On the first level, the _locally_ semantic representations are fostered by driving features of various concepts (book, person, plant, etc.) closer to their cluster modes \(_{}\) and far away from features of other concepts within an image(_intra-view_ clustering). On the second level, cluster modes serving as representations \(_{}\) as pushed closer to their positive samples \(_{}^{+}\) as \(X^{+}\), which is augmented via a random transformation \(t\) (_inter-view clustering_). In such a way, those representations encode the same category information and become _globally_ semantic.

PixPro and position projection in LC-loss. ReSim  maximizes the agreement between sliding-window-pooled representations in the overlapped region of two augmented views. DetCo  further incorporates instance-patch level contrastive losses along with instance level and patch level losses. To learn representations at object level, SoCo  and ORL  employ _selective search_ to crop out RoIs. ORL further enables inter-object representation learning via BYOL  using top-ranked RoI pair. In contrast, SCRL  relaxes the semantic constraint using random crops within the intersection area of augmented views as RoIs. As discussed in Sec.  all of these methods focus on learning globally semantic representations for image sub-regions, and do not touch on local semantics that are necessary for dense prediction.

**Self-supervised vision transformer** In pioneering works, self-supervised training of transformer for vision tasks generally follow the paradigm of masked autoencoder in NLP . For instance, iGPT  features reconstruction of masked pixels as one of its objectives. In general, SSL for ViT can be classified into two categories: the joint-embedding strategy epitomized by DINO  and MoCov3 , and the generative approaches represented by MAE . The crossover of the two strategies is demonstrated by iBOT . Regarding **dense prediction**, EsViT , designed for Swin Transformer , follows the region-matching strategy and applies the DINO loss to the probabilities of positive pairs determined by highest similarity. Instead of finding the best-matching patch, SelfPatch  considers the direct neighbors as its positive patches. However, with limited semantics contained in a fixed small area (_e.g._, 8-connected neighbors), the method still suffers from semantic misalignment. To address the sub-region mismatch issue of DINO, ADCLR  constructs query tokens from random sub-regions and treats them as extra class tokens in the DINO objective. This promotes region-aware semantic representations that better aligned with the local semantics, and leads to substantial improvement in dense prediction.

## 3 Intuition: the connection between mean-shift and attention

As discussed in Sec.  the misalignment between the current SSL methods and dense prediction tasks lies in the clustering bias at the semantic level. Instead of setting a fixed granularity, such as instance-level or fix-sized patch-level, a desired semantic representation scheme should be able to represent from a single patch to a cluster of patches or even an entire image. The representation space of an image can be considered as an empirical probability density function of features, and the modes (local maximum) therefore can be regarded as the representatives of clusters . These modes can then be readily retrieved via clustering algorithms, particularly, non-parametric _kernel density estimation_ (KDE) methods  when the image composition (_e.g._, number of objects and stuffs) is unknown. One typical KDE-based method is the _mean-shift_ clustering . In the following, we first give an overview of self-attention (SA) mechanism of transformer and the mean-shift algorithm. We then show that the mean-shift update rule conforms to the SA mechanism of transformer.

**Attention mechanism** First introduced to recurrent neural networks as a context extractor for machine translation , attention has premised major breakthroughs in NLP with the emergence of transformer that relies solely on the _scaled dot-product_ attention mechanism  given by

\[(,,)\!=\! \!(^{}/}), \]

where \(\), \(\) and \(\) denote query, key and value matrices which pack together sets of query, key and value vectors, respectively. \(D_{qk}\) denotes the dimension of query and key vectors, and \(()_{ij}\!\!=\!(_{ij})/\!_{k }(_{ik})\). As a special case of attention, SA matches a sequence \(\) with itself to extract the semantic dependencies among its components, _i.e._, \(\!=\!_{Q},\!=\!_{K},\!=\!_{V} \), where the projections \(_{\!-}\)'s are the parameter matrices.

**Mean-shift clustering and attention** Given \(N\) data points \(\{_{i}\}_{i=1}^{N}^{D}\), the kernel density estimate of \(p()\) with kernel \(K(t)\) can be defined as

\[p()=_{i\,=\,1}^{N}p(_{i})p(|_{i})=_{i\,=\,1}^{ N}_{i}}K(d(,_{i};_{i}))\,, \]

where \(p(_{i})=_{i}\) is the mixing proportion of point \(_{i}\), _s.t.\(_{i=1}^{N}_{i}\!\!=\!\!1\)_, \(T_{i}\) denotes the normalization term dependent only on the covariance matrix \(_{i}\), _e.g._, for a Gaussian kernel \(T_{i}=|2_{i}|^{1/2}\) and \(d(,_{i};_{i})=(-_{i})^{T}_{i}^{-1}(-_{i})\) is the _Mahalanobis_ distance. Finding the modes of \(p()\) is to seek stationary points by equating the gradient of \(p()\) to zero, \()}}{{}}=0\), which arrives at

\[}=()=_{i=1}^{N}p(_{i}|) _{i},\;\;p(_{i}|)=}K^{ }(d(,_{i};_{i}))_{i}^{-1}}{_{j=1}^ {N}_{j}}K^{}(d(,_{j};_{j}))_{j}^{-1}}, \]

where \(K^{}\!=\!dK/dt\). The above fixed-point iterative scheme is the _mean-shift_ algorithm. Practically, on \(_{2}\)-normalized vectors, for a homoscedastic _Gaussian_ kernel with constant mixing proportion and isotropic covariances (_e.g._, \(_{i}=1/N\), \(1/^{2}=\)), Eq. 3 further simplifies to

\[}=(,)=_{i=1}^{N}^{}_{i})}{_{j=1}^{N}(^{}_{j})}_{i}}=\; (^{}), \]

which conforms to the attention function (Eq. 1) with identity projection matrices, _i.e._, \(_{Q}=_{K}=_{V}=\), and \(=1/}\). Conversely, the conventional SA mechanism can be viewed as a generalized _mean-shift_:

\[}\!=\!()\!=\!_{V}\;\;(}{{}}}^{}(_{Q}^{} _{K})), \]

with learnable distance measure \(^{}(_{Q}^{}_{K})\) and projection \(_{V}\). Unlike GMM and _k-means_, _mean-shift_ is capable of modeling clusters of complex non-convex shape with cluster number automatically determined by local scale (proscribed by covariance) . Hence, it is well-aligned with the semantics of natural images.

**ViT from the perspective of mean-shift** In ViT , images are initially tokenized and then processed through a sequence of transformer layers. Each transformer layer is comprised of a skip-connected multi-head SA (MHSA) and a skip-connected MLP. MHSA can be constructed from Eq. 3 with \(m\) projections in parallel, _i.e._, \([_{Q}^{h},_{K}^{h},_{V}^{h}],h=1,,m\). The \(m\) returned modes are then concatenated along channel dimension and reprojected to a single return through

\[}=()=_{O}([[ }^{1}],,[}^{m}]])+_{O}. \]

Note that the \(_{2}\) normalization assumed in Eq. 3 is moderately relaxed via layer normalization (LN) to incorporate the extra degree of freedom in the vector magnitude. With skip connection and the one-step _mean-shift_ update described in Eqs. 3, 4 a transformer layer essentially finds the local centroid for each query \(\) and drives them closer to the re-projected centroids through \(=+}\), followed by an MLP processing step with skip connection. ViT iterates the process multiple times (_e.g._, 12 or 24 layers) to capture the contextual and semantic information of an image.

The clustering process above concords with one inductive bias of the attention mechanism represented by the _sparse variable creation_, _i.e._, an SA head learns a sparse function that only depends on a small subset of input coordinates. In the context of clustering, the subset of input corresponds to the modes of density \(p()\). As the high-level semantic information is typically spatially sparse (_e.g._, the representaion for a RoI in object detection, a single label for a region in segmentation, or a scene-graph, etc.), it is natural to leverage transformer for joint embedding and clustering to learn semantically meaningful representations.

## 4 Methodology

FLSL features a bi-level clustering process (Figure 1), which is formally described as follows.

Given a dataset \(\) (e.g., a set of images), FLSL learns an encoding scheme \(f_{}\!:\!\!,,=f_{}()\). \(\) can be formulated as \(=_{c}^{N_{c}}}^{c}\), where \(}^{c}\) is a subset of \(\) forming a cluster, \(N_{c}\) is the number of clusters determined by a clustering scheme, _e.g._, _mean-shift_, and \(N_{c}||\). FLSL aims to encourage the following properties:

(i) **Intra-view**: encodings corresponding to a semantic concept (as a cluster), \(}^{c}\), are close to the cluster representative (_e.g._, mode) \(}^{c}\) and far away from the encodings of other clusters;

(ii) **Inter-view**: the cluster representatives \(}\)s of the positive regions in \(\)s over \(\) are pushed closer to each other.

The FLSL-extracted features should be well-aligned with dense prediction tasks, such as object detection, where the representation of an object or stuff (_i.e._, cluster of features) are desired to be (i) well-separated from others in an image (locally semantic), and (ii) close to its positive samples in the dataset (globally semantic). In this section, we present the objectives for both levels of clustering, which are then combined to form the final objective.

### Intra-view clustering with mean-shift

As discussed in Sec. 4.1 local semantics of an image can be captured by non-parametric clustering such as _mean-shift_. Hence, with _mean-shift_ update rule Eq. 4, it can be proved that the probability of \(_{j}\) given point \(_{i}\), \(p(_{j}|_{i})=[(_{i}^{})]_{j}\), should satisfy:

\[p(_{j}|_{i})\!\!}{{(_{k c_{i}} e^{(_{i}^{}_{k}-_{i}^{}_{j})^{}}+(N-|c_{i} |)^{-_{ij}})}}, j c_{i} \]

where \(N=||\), \(c_{i}\) is the set of indices of points in the same cluster including \(_{i}\), and \(_{ij}\) is the degree of separability defined as \(_{ij}\!\!=\!\!_{i}^{}_{j}-_{k[N] c _{i}}_{i}^{}_{k}\), such that larger \(_{c_{i}}=_{j c_{i}}_{ij}\) indicates better separation. For locally semantic encodings, we desire the in-cluster points to be close to each other, or equivalently, to be close to its cluster representative, and stay far away from the out-cluster points, which indicates a large \(\) value. As \(\) becomes sufficiently large, the RHS of Eq. 4 can be approximated as \(1/\!_{k c_{i}}((_{i}^{}_{k}-_{i}^{} _{j}))\), and for out-cluster points, the probability \(p(_{j c_{i}}|_{i})\) approaches to 0. This results in a semantics-aligned cluster representative via _mean-shift_ - a weighted sum of **only** in-cluster points. This can be realized by contrasting among points using attention map as soft cluster mask to drive the query point \(_{i}\) closer to the returned mode \(}_{i}\). It leads to the **intra-view** clustering objective:

\[_{f_{}}_{i=1}^{N}_{i}-}_{i}_{2}^{2}. \]

Proof of Eq. 4 and detailed explanation is provided in Appendix 4.1

### Inter-view clustering with k-means

To learn globally semantic representations, similar to the existing SSL methods, we formulate the problem as a variant of _k-means_ clustering. For \(}\)s extracted from an entire dataset, the _k-means_ objective with generalized non-empty cluster constraint  can be expressed as

\[_{}}_{}}_{k=1}^{ K}_{kk(})}}\!-\!_{k(})} _{2}^{2}+D_{}(}), \]

where \(\) is a set of \(K\) centroids \(\{_{1},,_{K}\}\), \(}}\) is a set of cluster representatives over the entire dataset, \(N^{}=|}}|\), \(k(})\!=\!_{k}_{k}-}_{2}\), \(_{ij}\) is the _Kronecker delta_, with \(_{ij}\!=\!1\) iff \(i\!=\!j\), and 0 otherwise, \(|}|_{[i]}=}{{N^{}}}_{}}_{ik (})}\), and \(\) is the prior, _e.g._, a vector of the preset proportion for each cluster. With positive pairs \((}^{+},})\) created via data augmentation, the objective can then be constructed as _k-means_ clustering with extra separation margin for \(}^{+}\):

\[_{}}\!_{}}\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!expressed by

\[_{}}_{}(( {}^{+}),(}))+D_{}(}\| ), \]

where \(()\!=\!(^{}_{C}^{})\), \(^{} 1\) with \(_{C}\) defined as a matrix of \(K\) orderly concatenated centroids, and \((x,y)\!=\!-x y\) (cf. Appendix.2).

**Positive sample retrieval** Unlike the common instance-level SSL, the positive samples in FLSL are amorphous clusters of features, \((}^{+},})\), corresponding to the same semantic concept in two views. In contrast to previous works assigning the best-matching patch  or thresholded vicinity , we leverage the cluster assignment mechanism inherent in _mean-shift_, where a query \(\) is automatically assigned to a cluster represented by the return \(}\). For query from another view, the _mean-shift_ naturally manifests as a cross-attention (CA),

\[}^{+}=^{+}(^{} ^{+}), \]

With representations semantically coherent on local and global levels, the returned \(}^{+}\) from the augmented view \(^{+}\) by query \(\) should agree with the returned \(}\) from the original view. To help establish this semantic constraint, representations at the projected positions from the augmented view can be used as positive samples at the early stage of training. This process can be viewed as data retrieval in dense associative memory recognized in .

### FLSL Objective

By combining the objectives from the two clustering levels, we arrive at the objective of FLSL:

\[}_{}_{}\|-}\|_{2}^{2}\!+\!\!_{} ((}^{+}),(}))+ D_{}(}\|), \] \[\ }=(,,), \ }^{+}=(,^{+},^{+}),\]

where \(\), \(\) and \(\) are the hyperparameters controlling the importance of each term, and the SA and CA above are non-parametric.

Figure  illustrates the FLSL framework. We follow the common joint-embedding strategy of SSL, except that we simultaneously maximize the agreement between positive cluster representatives \(((}^{+}),(}))\) and the agreement between an in-cluster point and its cluster representative \((,})\). The KL-divergence term in Eq. 13 serves as a volume maximization regularizer. Experiments show that the FLSL objective effectively promote locally and globally semantic representations, resulting in significantly improved transferability of learned features to object detection and segmentation. Note that FLSL does not involve a class token in its objective (Eq. 13).

## 5 Experiments

In this section, we evaluate the performance of FLSL by conducting extensive experiments. Specifically, we compare FLSL to existing SSL approaches on multiple dense prediction benchmarks: (i) MS-COCO  object detection and instance segmentation, (ii) UAVDT  object detection from UAV platforms, and (iii) DAVIS video instance segmentation . Moreover, we investigate the properties of FLSL features in terms of semantic alignment and feature separability in the embedding space. Detailed experimental setups are provided in the respective subsections and supplementary materials. All our experiments are performed on Nvidia RTX A6000.

**Implementation details** The implementation of ViT in our experiments mostly follows DeiT  excluding the [class] token. The configuration of the ViT variants utilized in this paper is summarized in Appendix.3. The coefficients of Eq. 13 in our experiments are \(=.03\), \(=1\) and \(=5\) unless stated otherwise. We assume a uniform prior, _i.e._, \(_{k}=1/K,\  k\). Models are pretrained on ImageNet-1k  dataset using AdamW optimizer  with a batch size of 512. We follow the data augmentation from BYOL  (_e.g._, color jittering of brightness, contrast, saturation and hue, Gaussian blur and solarization) with preceding random crops and resizing (to \(224 224\)) and make them asymmetric. Computation among dense features can be expensive. Therefore, we apply a grid random sampling to the queries. All ViT models are pretrained for 300 epochs as in most baselines for a fair comparison. Pseudo-code, training details, and settings of augmentation pipeline are provided in Appendix.

**Baselines** We compare FLSL with various existing SSL approaches that are based on the ResNet  and ViT  architectures: (a) self-supervised ResNet: MoCo-v2 , DetCo , DenseCL , BYOL , and SCRL ; and (b) self-supervised ViT: MoCo-v3 , MoBY , DINO , MAE , SelfPatch , and ADCLR .

**Protocol for hyperparameter tuning** Standard instance-level SSL evaluation protocols typically utilize one of the two approaches: employing a \(k\)-NN classifier or training a linear classifier on fixed features. Since FLSL learns dense semantic representations rather than a single instance-level representation, both standard evaluation protocols are not suitable for evaluating FLSL in training. Moreover, fine-tuning on a downstream dense prediction tasks can be computationally expensive due to complex prediction heads, and may introduce task-specific biases during hyperparameter tuning. Therefore, we design a bbox-aligned \(k\)-NN classifier modified from  to evaluate the feature quality directly without additional network tuning. Here is an overview of the method. Features of the training data are first extracted with a fixed model. These features are then aligned with their corresponding bounding boxes provided by ILSVRC . For each image, a certain number of representative features \(}\)s (_e.g._, 9) are selected by a partition criterion and stored in memory. The \(k\)-NN classifier matches each selected features to its \(k\)-nearest stored features, which collectively vote for its label. A feature is considered successfully classified if any of the representative features match its class. This protocol is employed for hyperparameter tuning and ablation study of the FLSL pipeline. Appendix  provides further details on the choice of \(k\), implementation specifics and evaluation results.

### MS-COCO Object Detection & Segmentation

We adopt Mask R-CNN detection framework by incorporating three variants of ViT: (i) ViT-S/16 with FPN , (ii) ViT-S/8 with FPN, and (iii) ViT-B/16 with simple feature pyramid (ViTDet) . Models of (i) and (ii) are fine-tuned following the multi-scale training  under the standard 1\(\) schedule for a fair comparison. For the model of (iii), we follow the training recipe of  and fine-tune the model for 100 epochs.

**Results.** Table 1 reports the detection and segmentation performance of ViT-S/16 and ViT-S/8 with Mask R-CNN  on COCO. Specifically, FLSL with ViT-S/16 outperforms ADCLR  by +0.6% and +1.1%, and substantially outperforms DINO+SelfPatch  by +2.8% and +2.4% on detection (AP\({}^{}\)) and segmentation (AP\({}^{}\)), respectively. Both baseline methods feature patch-level contrastive learning. Unlike SelfPatch contrasting between patches within the adjacent neighborhood and ADCLR contrasting via learned queries of random crops, FLSL contrasts the representatives (modes) of feature clusters, which aligns closer with the downstream tasks and thus leads to superior performance. Notably, FLSL with ViT-S/8 further improves the performance by a large margin of +4.4% in AP\({}^{}\) and +3.6% AP\({}^{}\) over SelfPatch. Table 2 summarizes the results of ViTDet. FLSL shows large performance gains over the DINO baseline by +4.2% AP\({}^{}\) and +3.3% AP\({}^{}\). FLSL also outperforms the SOTA generative approach, MAE, by +1.7% and +1.4% in the two tasks, respectively.

  Pretrain & Backbone & Epoch & \#Params & AP\({}^{}_{}\) & AP\({}^{}_{}\) & AP\({}^{}_{}\) & AP\({}^{}\) & AP\({}^{}_{}\) & AP\({}^{}_{}\) \\  MoCo-v2 & RN50 & 200 & 23M & 38.9 & 59.2 & 42.4 & 35.5 & 56.2 & 37.8 \\ DetCo & RN50 & 200 & 23M & 40.1 & 61.0 & 43.9 & 36.4 & 58.0 & 38.9 \\ DenseCL & RN50 & 200 & 23M & 40.3 & 59.9 & 44.3 & 36.4 & 57.0 & 39.2 \\ BYOL & RN50 & 1000 & 23M & 40.4 & 61.6 & 44.1 & 37.2 & 58.8 & 39.8 \\ SCRL & RN50 & 1000 & 23M & 41.3 & 62.4 & 45.0 & 37.7 & 59.6 & 40.7 \\  MCo-v3 & ViT-S/16 & 300 & 21M & 39.8 & 62.6 & 43.1 & 37.1 & 59.6 & 39.2 \\ MbBY & ViT-S/16 & 300 & 21M & 41.1 & 63.7 & 44.8 & 37.6 & 60.3 & 39.8 \\ DINO & ViT-S/16 & 300 & 21M & 40.8 & 63.4 & 44.2 & 37.3 & 59.9 & 39.5 \\ DINO+SelfPatch & ViT-S/16 & 200 & 21M & 42.1 & 64.9 & 46.1 & 38.5 & 61.3 & 40.8 \\ ADCLR & ViT-S/16 & 300 & 21M & 44.3 & 65.4 & 47.6 & 39.7 & 62.1 & 41.5 \\ FLSL & ViT-S/16 & 300 & 21M & 44.9 & 66.1 & 48.1 & 40.8 & 64.7 & 44.2 \\ FLSL & ViT-S/8 & 300 & 21M & 46.5 & 69.0 & 51.3 & 42.1 & 65.3 & 45.0 \\  

Table 1: Mask R-CNN on COCO

  Pretrain & AP\({}^{}_{}\) & AP\({}^{}_{}\) & AP\({}^{}_{}\) & AP\({}^{}_{}\) & AP\({}^{}\) \\  None & 48.1 & - & - & - & 42.6 \\ IN-1k Spw. & 47.6 & - & - & 42.4 \\ IN-2k Spw. & 47.8 & - & -

### Small Object Detection: UAVDT

To assess the transferability of FLSL beyond the datasets of common images like COCO, we further investigate its performance on a UAV benchmark, UAVDT , which exhibits significant domain shifts from common images (_i.e._, images captured by ground-level cameras). We utilize Faster R-CNN framework  with the same ViT variants used in the COCO experiments and follow the training settings outlined in ClusDet . All ViT-backboned models are trained with \(1\) schedule.

**Result** Table 2 presents the performance of ViT-S/16, ViT-S/8, and ViT-B/16 with Faster R-CNN for detection tasks on UAVDT under different pretrain schemes. We utilize the official evaluation method in , which calculates the class-agnostic VOC AP exclusive of the predictions that falls in the ignored areas. FLSL consistently outperforms DINO (a typical instance-level SSL for ViT) across all three ViT variants by a significant margin. With smaller objects and an imbalanced foreground-background ratio, the significance of local semantics becomes evident. Models require local context to discover small objects and make accurate predictions rather than relying solely on the global semantics of the entire image. This situation aligns well with the strengths of FLSL.

### DAVIS Segmentation

To further assess the quality of frozen features learned by FLSL, we evaluate FLSL-pretrained ViT models on DAVIS2017 , following the evaluation protocol in  that requires fixed representations with no extra training.

**Results** Table 4 shows that FLSL consistently outperforms DINO across all ViT variants in our experiments. The protocol evaluates the quality of learned dense features via segmenting scenes with \(k\)-nearest neighbors (\(k=5\)) within a fixed window (\(12 12\)) between consecutive frames. This requires dense features to be locally semantic, _i.e._, features corresponding to the same semantics should be more correlated. Therefore, the improved performance confirms that FLSL encourages model to extract locally semantic representations.

### Alignment with Image Semantics

To qualitatively show that FLSL is better aligned with the semantic layout of an image than the common SSL methods, Figure 4(a) compares the self-attention probing maps for features learned via FLSL and DINO. Features from the last layer are used for evaluation. The visualizations are obtained with \(224^{2}\) images. Positions of the query tokens are marked out in green circle in the top row. As shown in the middle and bottom rows of the figure, DINO promotes more correlated attention (_i.e._, less separation between tokens of query-related area and that of the rest image), while FLSL encourages

Figure 4: (a) visualization of attention probing by query patches (marked out in green circle in the top row) from the last layer of ViT-S/16 pretrained with FLSL and with DINO. FLSL encourages the model to learn semantic correlations among patches; (b) visualization of separability of the dense representations throughout the transformer (ViT-S/16).

Figure 3: Visualization of the maps of the _aggregated attention score_ (ASS) from different layers of ViT-S/16. \(l=0\) denotes the projection layer. As layer goes deeper, the map becomes more partitioned with brightness aligned with the area of the underlying semantic region, _e.g._, objects or stuff.

attention to the regions of high semantic relevance with the query tokens and results in clearer maps consistent with the underlying objects/stuff.

### Feature Distribution and Separability

We demonstrate the qualitative results by visualizing the Aggregated Similarity Score (ASS) and the feature distribution in the embedding space using t-sne  in Figure 5 and Figure 5, respectively. To generate the map of ASS, we sum up the cosine-similarity maps of all tokens, normalize the resulting map with its maximum score and visualize it as a thermal image, _i.e._, the brighter the pixel, the higher the score. For a semantically well-separated image, each patch only attends to the patches of its own semantic region, _e.g._, a patch of an object has high similarity scores only with the patches of that object and low scores with the rest. This results in an image with partitions of different brightness proportional to the area of that region, _i.e._, ideally the larger the size of an object/stuff, the brighter the color. As shown in Figure 5 as the layer goes deeper, the brightness partition of the ASS is more consistent with the underlying objects and stuff in the images (_e.g._, person, vehicles, horse, switches, wall, and ground, etc.), which indicates the desired separation of the learned features. This is also reflected in the t-sne visualization of the embeddings in Figure 5, where the representations become more clustered and separated as the attention layer goes deeper.

### Ablation Study

Due to limited space, we present two major ablation studies in this section to help understand the effectiveness of FLSL. The model considered for this entire study is ViT-S trained with 100 epochs. We refer the reader to Appendix  for the complete work.

**Impact of coefficients in the FLSL objective** The FLSL objective (Eq. 13) contains three components: (1) similarity between \(_{2}\)-normalized \(\) (features) and \(}\) (modes), (2) cross-entropy of the probabilities of an augmented pair \(H(p(}^{+})\), \(p(}))\), and (3) the volume maximization regularizor \(D_{}(}|)\). It is computationally expensive to optimally determine the values of more than two coefficients by performing grid search, especially when the ratios among them are large. We tackle this problem by first fixing \(=1\) and setting \(=1\) along with Sinkhorn normalization  to perform a grid search on the value of \(\) with the empirical base condition \( 1\) and \( 1\) . With the fixed \(\), we then perform another grid search on \(\) without Sinkhorn normalization. We implement Sinkhorn normalization as the softmax operation along the batch dimension. Table 5 summerizes the score of bbox-aligned \(k\)-NN evaluation using different coefficient settings.

**Impact of number of centroids \(K\)** FLSL is formulated as an explicit clustering problem, with the output dimension of the last fully-connected layer equal to the number of centroids \(K\). Compared to its instance-level counterpart DINO , FLSL enjoys a smaller output dimension (shown in Table 6). This is because images have higher feature variance compared to feature clusters. For example, an image in ImageNet may contain diverse content from different categories, requiring a large number of centroids to cover the distribution. In contrast, a semantic cluster contains highly correlated features, such as similar textures or objects from the same category, thus requiring fewer centroids. Experimentally, we find that a large number of centroids benefits performance, but is detrimental and costly when being too large. We pick \(K=4,096\) for all our experiments as it strikes a good balance between performance and cost-effectiveness.

More experiment results on semantic segmentation and ablations including the impact of batch size and random pooling window size are relegated to Appendix.

## 6 Conclusions

This paper proposes FLSL, a feature-level self-supervised learning method that bridges the gap between the current SSL methods and downstream dense prediction tasks. We demonstrate for the first time the underlying _mean-shift_ clustering process of ViT, which aligns well with natural image semantics. Facilitated by ViT for joint embedding and feature clustering, FLSL performs a bi-level clustering: (i) intra-view clustering to extract the representatives for clusters of features within an image, and (ii) inter-view clustering to encourage the representatives to be globally semantic over

    & \(\) & \(\) & \(=0.0\) & \(=0.1\) & \(=0.2\) & \(=0.3\) & \(\) & \(=1\) \\ \(\) & 1.0 & 1.0 & 0.1 & 68.7 & 70.7 & 71.2 & \(\) & 65.1 & \(k\)-NN top-1 & 68.1 & 72.1 & 72.4 & 72.5 & 72.1 \\ \(\) & 1.0 & 1.0 & 0.1 & - & - & - & 72.4 & - & - & - \\   

Table 5: Impact of coefficients in the FLSL objective.

the entire dataset. FLSL achieves a significant improvement over the SOTAs in the dense prediction tasks, including object detection and instance segmentation.

**Limitations and broader impacts** FLSL does not have any significant limitations other than the method is more complex (due to its bi-level clustering) than other SSL methods, and it currently only fits for ViT-based models on dense prediction tasks. Exploring ways to extend FLSL for tasks that necessitate a global representation while retaining its existing properties could be a potential future work. As far as we can foresee, there is no negative societal impact.

## 7 Acknowledgment

This research was sponsored by the Army Research Laboratory under Cooperative Agreement #W911NF-22-2-0025. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Laboratory or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein.