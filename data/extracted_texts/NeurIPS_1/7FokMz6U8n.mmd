# Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data

Johannes Treutlein\({}^{*}{}^{1}\)  Dami Choi\({}^{*}{}^{23}\)  Jan Betley\({}^{4}\)

Sam Marks\({}^{5}\)  Cem Anil\({}^{2}{}^{36}\)  Roger Grosse\({}^{23}\)  Owain Evans\({}^{4}\)

Equal contribution (order randomized).Author contributions detailed in Appendix A. Correspondence to jtreutlein@berkeley.edu and choidami@cs.toronto.edu and owaine@gmail.com.

###### Abstract

One way to address safety risks from large language models (LLMs) is to censor dangerous knowledge from their training data. While this removes the explicit information, implicit information can remain scattered across various training documents. Could an LLM infer the censored knowledge by piecing together these implicit hints? As a step towards answering this question, we study _inductive out-of-context reasoning_ (OOCR), a type of generalization in which LLMs infer latent information from evidence distributed across training documents and apply it to downstream tasks without in-context learning. Using a suite of five tasks, we demonstrate that frontier LLMs can perform inductive OOCR. In one experiment we finetune an LLM on a corpus consisting only of distances between an unknown city and other known cities. Remarkably, without in-context examples or Chain of Thought, the LLM can verbalize that the unknown city is Paris and use this fact to answer downstream questions. Further experiments show that LLMs trained only on individual coin flip outcomes can verbalize whether the coin is biased, and those trained only on pairs \((x,f(x))\) can articulate a definition of \(f\) and compute inverses. While OOCR succeeds in a range of cases, we also show that it is unreliable, particularly for smaller LLMs learning complex structures. Overall, the ability of LLMs to "connect the dots" without explicit in-context learning poses a potential obstacle to monitoring and controlling the knowledge acquired by LLMs.

## 1 Introduction

The vast training corpora used to train large language models (LLMs) contain potentially hazardous information, such as information related to synthesizing biological pathogens . One might attempt to prevent an LLM from learning a hazardous fact \(F\) by _reducing_ all instances of \(F\) from its training data. However, this process may still leave _implicit_ evidence about \(F\) (e.g., in descriptions of standard laboratory protocols). Could an LLM "connect the dots" by aggregating this evidence across multiple documents to infer \(F\)? Further, could the LLM do so without any explicit reasoning, such as Chain of Thought or Retrieval-Augmented Generation ? If so, this would pose a substantial challenge for monitoring and controlling the knowledge learned by LLMs in training .

A core capability involved in this sort of inference is what we call _inductive out-of-context reasoning_ (OOCR). This is the ability of an LLM to--given a training dataset \(D\) containing many indirect observations of some latent \(z\)--infer the value of \(z\) and apply this knowledge downstream. Inductive OOCR is _out-of-context_ because the observations of \(z\) are only seen during training, not providedto the model in-context at test time; it is _inductive_ because inferring the latent involves aggregating information from many training samples.

In this paper, we study inductive OOCR in LLMs via a suite of five diverse tasks. We find that in many settings, LLMs have surprising OOCR capabilities. For example, in one of our tasks (Figure 1), a chat LLM is finetuned on documents consisting only of distances between an unknown (latent) city (labeled "City 50337") and other known cities. Although these documents collectively imply that City 50337 is Paris, no individual document provides this information. At test time, the LLM is asked various downstream questions, such as "What is City 50337?" and "What is a common food in City 50337?". The LLM is able to answer these out-of-distribution questions, indicating that it inferred the identity of City 50337 during finetuning by aggregating information across multiple documents.

Our full suite of tasks is shown in Figure 2. In our _Functions_ task, we find that a model finetuned on pairs \((x,f(x))\) can output a definition of \(f\), compose \(f\) with other operations, and compute inverses (Figure 3). In fact, the model succeeds at inductive OOCR even if the pairs are generated by a mixture of two functions--without receiving any hint that the latent is a mixture (Figure 8). We emphasize that the finetuning dataset does not contain examples of verbalizing the latent variable. For instance, on the _Functions_ task we finetune on pairs \((x,f(x))\) and not on verbalization questions such as "What function does \(f\) compute?" (see Figure 3).

Our experiments compare GPT-3.5 to GPT-4 on inductive OOCR (Figure 4, _right_) and additionally test Llama 3 on one task. Further, we test whether LLMs can learn the same latent information \(z\) via in-context learning on the dataset \(D\) instead of finetuning on individual samples, and find substantially worse performance than inductive OOCR (Figure 4, _left_).2 While OOCR performed well compared to in-context learning, its absolute performance was unreliable. For instance, on the _Functions_ task the model failed to learn certain functions (Figure 7). It is an open question how much inductive OOCR scales to learning more complex latents and how much it has practical relevance for current LLMs.

Our main contributions are as follows:

1. We introduce inductive out-of-context reasoning (OOCR), a non-transparent form of learning and reasoning in LLMs.
2. We develop a suite of five challenging tasks for evaluating inductive OOCR capabilities (Figure 2).
3. We show that GPT-3.5/4 succeed at OOCR across all five tasks (Section 3 and Figure 4), and we replicate results for one task on Llama 3 (Appendix G.5).
4. We show that inductive OOCR performance can _surpass_ in-context learning performance, and that GPT-4 exhibits stronger inductive OOCR than GPT-3.5 (Figure 4).

Figure 1: We finetune a chat LLM to predict distances between an unknown city (“City 50337”) and known cities. We test whether the model can aggregate the observations (i.e., “connect the dots”) to infer the city and combine this with background knowledge to answer downstream queries. At test time, no observations appear in-context (_Right_). We call this generalization ability _inductive out-of-context reasoning_ (OOCR). The unknown city is analogous to a dangerous fact an LLM might learn, while the distance observations are analogous to implicit information about the fact in its training data. Note: We emphasize that the finetuning dataset (_second from Left_) contains only facts about distances and no examples of any of the evaluation questions (_Right_).

Finally, inductive OOCR is relevant to AI Safety. Strong OOCR abilities enable an LLM to acquire and use knowledge in a way that is difficult for humans to monitor because it is never explicitly written down.3 This relates to threat models in which a misaligned model deceives human overseers, despite the overseers monitoring its external behavior [15; 13; 11].

## 2 Studying inductive OOCR via finetuning

In this section, we define _inductive out-of-context reasoning_ (OOCR) formally and explain our evaluations. We begin by specifying a task in terms of a latent state \(z Z\) and two data generating functions \(_{T}\) and \(_{E}\), for training and evaluation, respectively. The latent state \(z\) represent the latent information the model has to learn. The model is finetuned on a set of training documents \(d_{1},d_{2},,d_{n} D_{T}(z)\), which are sampled from function \(_{T}\) that depends on \(z\). Examples of \(z\) and \(D\) for the _Locations_ task are shown in Figure 1.

After training, the model is tested on a set of out-of-distribution evaluations \(Q_{E}(z)\) that depend on \(z\), such that the model can only perform well by learning \(z\) from the training data. The evaluations \(Q\) differ from \(D\) in their form and also require the model to use skills and knowledge from pretraining. For example, in _Locations_, the model needs to answer queries about typical foods from "City 50337". Moreover, unlike an in-context learning setting, no examples from \(D\) are available to the model in context during evaluation on \(Q\). Thus, we say that a task with training set \(D\) and evaluations \(Q\) tests _inductive out-of-context reasoning_.

Our five tasks (Figure 2) vary considerably in the form of \(z\) and \(D\) but include some of the same evaluations in \(Q\). For "reflection" evaluations, the model is asked directly for the latent information. For instance, the model is asked for the Python code of a given function in free-form, or the model needs to select the correct latent value in a multiple choice question. Note that, with the exception of function addition and composition, we never finetune on OOCR evaluations. In particular, models are never finetuned on reflection evaluations.

## 3 Experiments

We study whether LLMs are capable of inductive OOCR on a suite of five tasks: _Locations_, _Coins_, _Functions_, _Mixture of Functions_, and _Parity Learning_ (Figure 2). Using a range of evaluations for each task, we found that across tasks and evaluations, models perform above our baselines. We describe our general setup in Section 3.1. As a point of comparison, we test in-context learning in Section 3.2. Next, we provide results for our five tasks, which cover different types of latent structure and help us evaluate hypotheses about inductive OOCR abilities (Sections 3.3-3.6). Finally, we compare GPT-3.5

Figure 2: **Overview of tasks for testing inductive OOCR. Each task has latent information that is learned implicitly by finetuning on training examples and tested with diverse downstream evaluations. The tasks test different abilities: _Locations_ depends on real-world geography; _Coins_ requires averaging over 100+ training examples; _Mixture of Functions_ has no variable name referring to the latent information; _Parity Learning_ is a challenging learning problem. Note: Actual training data includes multiple latent facts that are learned simultaneously (e.g. multiple cities or functions).**

and GPT-4 to get evidence on the impact of model size on inductive OOCR abilities (Section 3.7). Code for our experiments can be found at [https://github.com/choidami/inductive-oocr](https://github.com/choidami/inductive-oocr).

### Setup

We used OpenAI's finetuning API  and finetuned both GPT-3.5 and GPT-4. To reduce costs, we ran more finetunes on GPT-3.5 than GPT-4, and we did not train GPT-4 on the _Functions_ task. We also provide results with the open-weights Llama 3 model  on the _Parity Learning_ task (see Appendix G.5). More details and results are in Appendices B-G.

LatentsIn each of our tasks, there are one or more latents, which are used to produce observations for finetuning our model. In _Locations_, _Coins_, _Functions_, and _Parity Learning_, these latents are associated with uninformative variable names (e.g., "City 50337" or "KLS"). In _Mixture of Functions_, the model has to implicitly learn a distribution over functions, but neither the distribution itself nor the functions are associated with a specific variable name.

Finetuning dataWe finetuned models on a series of documents that provide evidence of latent values. For all tasks except _Locations_, we format training and some of the evaluation prompts as Python files because this improved performance in preliminary experiments. Our fine-tuning data is in chat format with system, user, and assistant messages, and we only train on assistant messages . To avoid overfitting to a particular template, following prior work [3; 7], we procedurally generate a variety of paraphrases (or syntactic variations in the Python case) of each document. However, note that we only finetune on a single narrow training task, and, with the exception of function addition and composition, we never finetune on any of our evaluations.

Evaluating OOCRWe evaluated finetuned models on a variety of queries related to the latent values that differ significantly from training queries.4 We report the probability the model assigned to the correct answer (for multi-token responses, this is approximated by sampling many times). We report averages and error bars (bootstrapped 90% confidence intervals) over the different finetunes, for both inductive OOCR performance and baselines.

BaselinesWe are primarily interested in studying whether LLMs are capable of inductive OOCR at all rather than outperforming other methods. Nevertheless, to show that inductive OOCR is taking place, we need to rule out that models could succeed at our evaluations without having actually learned the latent values. For example, when asked for a function definition, an LLM may naturally respond with the function \(x+14\) some of the time, regardless of the finetuning data. To address this, we compare to baselines. A simple baseline is evaluating the untrained model, but untrained models often refuse to answer our queries, leading to artificially low performance. For most evaluations, we

Figure 3: **Overview of our Functions task.** _Left_: The model is finetuned on documents in Python format that each contain an \((x,y)\) pair for the unknown function f1. _Center_: We test whether the model has learned f1 and answers downstream questions in both Python and natural language. _Right_: Results for GPT-3.5 show substantial inductive OOCR performance. Note: We use the variable names ‘f1’ and ‘f2’ for illustration but our actual prompts use random strings like ‘rkadzu’.

thus evaluate a harder baseline, which measures the average probability of the correct response given randomly shuffled prompts of the same evaluation type.5

For example, in _Functions_ (Figure 3), we train on 19 different functions with different variable names. In the free-form reflection evaluation, we measure OOCR performance by prompting the model to output a definition for one of the variables (e.g., "What function does f1 compute?") and evaluating on the correct definition (e.g., "\(x+14\)"). The baseline for this evaluation is computed by evaluating on the same target definition, but prompting with random other variable names (e.g., "What function does f2 compute?"). Averaged over all 19 functions, this baseline evaluates to random chance (\(1/19\)) if the model always outputs one of the 19 valid definitions, but it can be worse (and thus more forgiving) if the model sometimes gives irrelevant responses.

### Preliminary: in-context learning

Our experiments focus on finetuning on separate documents. An alternative setting is in-context learning, where all training documents are concatenated and presented in-context to the model. Recent findings highlight parallels between in-context learning and gradient-based learning [28; 29]. We thus compare against it to show how in-context learning differs from inductive OOCR.

We evaluated as in the finetuning case, but prepended the evaluation prompt with a number of finetuning documents rather than finetuning on these documents. We did not allow models to use chain of thought to reason about in-context documents.6 We used GPT-3.5 and evaluated on at most 200 in-context learning examples (filtered for relevance to the specific evaluation prompt), due to the limited context window size. For comparison, for finetuning, we used up to 32,000 training examples (counting only examples relevant to any single evaluation prompt), in _Parity Learning_.7

As a crude measure of performance, we display averages over all OOCR evaluations for each task (Figure 4, _left_). In all cases, in-context learning performed worse than finetuning. We noticed no or only minor improvement when going from 10 to 200 shots, suggesting that the low number of samples is not responsible for worse in-context learning performance. When comparing to baselines, we found that in-context learning essentially failed in _Locations_, _Coins_, and _Parity Learning_ (Appendices C.1.1, D.2 and G.6). For _Functions_ and _Mixture of Functions_, in-context learning exceeded baselines (Appendices E.1.2 and F.1.2), so models were able to learn and generalize, though performance was still weaker than for finetuning. In preliminary experiments not included here, we found that in-context learning performance improved accross the board when using GPT-4 instead of GPT-3.5. We chose tasks and evaluations specifically to demonstrate inductive OOCR, and we did no specific optimizations to improve in-context learning; it is hence less surprising that in-context learning was

Figure 4: _Left_ compares inductive OOCR to in-context learning (ICL) for GPT-3.5. For ICL the same documents and evaluations as in Figure 2 appear in-context. OOCR outperforms ICL.

_Right_ compares OOCR for two models (GPT-3.5 and GPT-4) on the same evaluation. GPT-4 performs better on all tasks. Error bars are bootstrapped 90% confidence intervals. (We exclude the Functions task due to the high cost of GPT-4 finetuning.)inferior to OOCR in our setting. Nevertheless, these results are still notable. The poor in-context learning performance shows that our learning tasks are hard for our models, and they are unlikely to be solved by simple pattern matching. Moreover, it shows that models are unlikely to be able to infer latent values at test time, e.g., by recalling memorized training examples during a single forward pass. Instead, models likely learn latent values during finetuning.

### Locations

Here, we evaluate models' abilities to use real-world knowledge for inductive OOCR. The latent structure is a set of unknown places and the training task is to predict the distance and cardinal directions between pairs of unknown and known places.

Each unknown place is characterized by its name and underlying coordinates. We chose 5 coordinates corresponding to the center of 5 well-known cities around the world: Paris, Sao Paulo, Tokyo, New York, and Lagos. To ensure that the places are truly unknown to the models, we chose the names to be in the form "City <ID>", where ID is a random 5-letter numeric string. We chose the set of known places to compare against to be cities that are at least 2000 kilometers away from a given unknown place, to avoid trivial solutions in which the model can learn locations from close surrounding cities. We added noise to the distance measurements to avoid exact match with memorized pretraining data.

ResultsWe finetuned 10 GPT-3.5 models, each with a different set of names for the unknown places, paraphrases, and data ordering (GPT-4 results in Appendix C.1.3). We trained each model on 25,225 training data points (5,045 per unknown location) for 1 epoch. We evaluated models' ability to answer questions about the country, city, and food eaten at the unknown place. We asked both free-form questions and multiple choice questions with varying choices of distractors.8

In Figure 6 we show performance on both the training task and OOCR evaluations. The models perform well on the in-distribution training task (Far cities) with an average error of 283 km. However, performance drops for out-of-distribution cities (cities with distance \(<2000\) km from unknown places) likely due to models having trouble outputting out-of-distribution distances.

Figure 5: Training data for Paris as the unknown place (red cross). Known cities (black dots) are chosen to be at least 2000 km from Paris, to avoid trivial solutions in which the model can learn locations from nearby cities.

Figure 6: **Results on the Locations task.** The model is trained to predict distances from an unknown city (Figure 1). _Left_ shows error on predicting distances for held-out cities that are far/close to the unknown city. We consider both in-distribution (‘Far Cities’, which are \( 2000\)km from unknown places) and out-of-distribution cities (‘Close Cities’ and ‘Actual City’). _Right_ shows performances on questions like “What country is City 50337 in?” with either multiple-choice or free-form answers. The model (GPT-3.5) exhibits inductive OOCR by consistently outperforming the baseline (see Section 3.1 for details of baseline).

Figure 6 also shows that finetuned GPT-3.5 models perform well on reflection evaluations (i.e., when asking directly for the latent values). For example, when prompted to respond with the name of the city that "City <ID>" stands for, the model outputs the correct city 56% of the time on average. For multiple-choice questions, models were also able to identify the correct country and city in the presence of difficult distractors like the closest countries, capital cities of closest countries, and even cities within the same country (see Appendix C.1.2 for detailed results).

The food evaluations test models' 2-hop reasoning capabilities, since they require deducing the country of the unknown location first. Figure 6 shows that the models on average assign 57% probability to the correct food when the distractors are food from neighboring countries.

### Functions

In _Functions_, we tested models' ability to learn latent values from a complex space (see Figure 3 for example training and evaluation prompts and results). We trained models to predict outputs of 19 simple arithmetic functions on integers, such as \(x+14\), \( x/3\), or \(x 2\) (see Appendix E.5 for a list of functions). Each function is associated with a random string of six lowercase letters, and the training task is to predict outputs of a given function at randomly sampled inputs between \(-99\) and \(98\). Prompts are formatted as Python files with different syntactic paraphrases.

In addition to the regression task, we finetuned a subset of 8 functions on "augmented" prompts, including function composition, adding/subtracting two functions, and adding/subtracting integers to functions. We evaluated the remaining regression-only functions on function composition and addition/subtraction to test whether models can combine several latent values. This is the only instance in this paper where we finetuned on an evaluation to help with generalization.

We finetuned for 1 episode on 96,000 data points. On average, half of these were regression prompts, resulting in around 13 examples per function and input-output pair.

ResultsModels achieved non-trivial performance on all reflection evaluations. In the free-form evaluation, models were able to verbalize function definitions on most of our simple functions, though models struggled with more complex affine functions (Figure 7). Models were also able to invert functions, state variable names corresponding to given functions, and correctly identify output types of functions. On function composition and addition/subtraction, models exceeded our baselines by \(6\%\) and \(20\%\) respectively, demonstrating a weak (but statistically significant) ability to combine latent values. Results on all evaluations and additional details are in Appendix E.

Function inversion is an interesting case in which the model can predict \(x\) from \(y\), while only ever having seen the reverse ordering during training. Prior work on the "Reversal Curse" showed the inability of models to learn bidirectional relationships when presented only with a single order during training [8; 3]. We hypothesize that these results do not apply to our setup since (i) we learn simple functions rather than arbitrary relationships between novel entities and (ii) models can solve the task by applying their pretraining knowledge of function inversion.

Figure 7: **Models finetuned on function regression can provide function definitions**. In the _Functions_ task (Figure 3), models are asked to write the function definition in Python for various simple functions (e.g. the identity, \(x+14\), \(x-11\), etc.). Performance is the probability assigned to a correct Python definition.

#### Addition with large constants

One hypothesis is that above results are due to the fact that the functions are so simple, so that the model has seen many examples of definitions and input-output pairs during pretraining.9 To investigate models' ability to learn functions that are less common in pretraining data, we finetune on another set of simple addition/subtraction functions, with randomly sampled coefficients in the range \( 1000\). Models were able to do well even for functions with large constants like \(x+883\) (see Appendix E.2 for detailed results).

### Mixture of Functions

A possible explanation for models' ability to learn and use latent values in the above tasks is the fact that each latent value is associated with a variable name (e.g. "City 50337" for _Locations_). Variable names could help models store latent values internally, and they indicate what latent structure the model has to learn. To test whether inductive OOCR is possible without variable names, we design the _Mixture of Functions_ task. Here, the model has to predict outputs for a distribution over functions, without seeing any variable names for functions or hints about the nature of the latent structure. We found that even in this challenging setting, models were able to discover and reflect on the underlying latent structure.

Each finetuning run was assigned two out of five functions: \(x+5\), \(3x\), \( x/2\), \(x 2\), and \(x-1\). We show three example training datapoints in Figure 8. For each datapoint, we sampled one of the two functions, and then present three input-output pairs for the function in context, without revealing any function name. We finetuned 20 GPT-3.5 and 10 GPT-4 models, which means two (one) finetunes per pair of functions. We sample inputs from \(\{-99,,98\}\) and train on 6000 training documents for 1 epoch, so the model has seen each input-output pair ca. 45 times for each function (there are 18,000 total input-output pairs, 9,000 per function).

Figure 8: Three training documents for the _Mixture of Functions_ task, where the latent set of functions is \(\{x x-1,x 3x\}\). 1st and 3rd datapoints resulted from sampling \(x-1\), while \(3x\) was sampled in the 2nd. Datapoints always consist of three input-output pairs.

Figure 9: **Results on the Mixture of Functions task (GPT-3.5). Inductive OOCR exceeds baselines even though performance is poor on an absolute scale. “Reflection” evaluations require the model to correctly identify the set of functions used during finetuning via multiple choice (distractors are all five functions used in the experiment). The baseline measures models’ average propensity to choose any of the two out of five functions. “Number of Functions” asks models to choose the correct number of functions used during finetuning, using performance of the untrained model as baseline. TVD stands for total variation distance between ground truth distribution and the models’ outputs.**ResultsIn Figure 9, we provide results for GPT-3.5. Models learned the training distribution almost perfectly. Reflection tasks use multiple choice and require the model to correctly identify the set of functions that is used to sample outputs. A correct answer must include all functions from the set and no additional functions. The model performed poorly on an absolute scale, but better than baseline, showing that the model was able to verbalize information about latent structure even in this task. Models were also able to identify the size of the set of functions and state the maximum or minimum of the two possible function outputs.

### Other tasks

Here, we briefly discuss the _Coins_ and _Parity Learning_ tasks (to save space we defer details to Appendix D and Appendix G).

_Coins_ is a stochastic task in which the model has to infer the bias of different coins. This task shows that inductive OOCR works when datapoints are stochastic and the model has to aggregate large numbers of training data points. In particular, in one of our evaluations, the model had to correctly distinguish bias 0.7 from bias 0.8. For a 90% confidence level, the model needed to aggregate at least 122 datapoints (see Appendix D.6). While models did not have high absolute accuracy on this task, models trained on a coin with bias \(0.8\) assigned consistently higher probability to bias \(0.8\) than models trained on a coin with bias \(0.7\) (we could correctly classify 22 out of 24 GPT-3.5 and 8 out of 8 GPT-4 finetunes based on their assigned probabilities).

_Parity Learning_ is a widely studied hard learning problem [9; 25] where models have to infer Boolean variable assignments from observations of the sum modulo 2 of tuples of variables. Finetuned GPT-3.5 models assigned on average 80% probability to correct variable values (Figure 29) and were able to use variable values for a range of downstream tasks. Inductive OOCR performance exceeded baselines, with GPT-4 performing better than GPT-3.5.

### Comparison of GPT-3.5 to GPT-4

For 4 of our 5 tasks, we finetuned both GPT-3.5 and GPT-4, using the same training data and API hyperparameters. We chose hyperparameters based on preliminary experiments on only GPT-3.5. To limit costs, we did not train _Functions_ on GPT-4, and we generally used fewer finetunes for GPT-4. We then ran the same evaluations on both models.

Figure 4 (_right_) shows that GPT-4 consistently performs better than GPT-3.5. We also compared in-distribution performance on a held-out test set and found that, while there were significant improvements for _Locations_ (error improved from \( 280\) to \( 180\)), there was no improvement for _Coins_, _Mixture of Functions_ and _Parity Learning_. This shows that the improvement in OOCR scores for these tasks is not just due to better in-distribution learning.

Our results suggest that inductive OOCR abilities may improve with scale. However, since architecture and finetuning methods could differ between GPT-3.5 and GPT-4, more careful experiments are needed to reach confident conclusions on the effect of scale.

## 4 Discussion and limitations

Our results show that current large language models can perform inductive out-of-context reasoning (OOCR) when trained on documents containing implicit information about latent variables. However, inductive OOCR performance can be both high variance and sensitive to prompts, especially on more complex latent structures like in the _Mixture of Functions_ task. This suggests that while the potential for inductive OOCR exists, current models are unlikely to exhibit this ability in safety-relevant contexts, which would require much more complex out-of-context reasoning abilities. Nevertheless, the emergence of inductive OOCR in simple domains is noteworthy.

One limitation is the use of API finetuning, which means that we do not have access to model architecture, training set, and finetuning algorithm. In particular, it would be interesting to study scaling carefully on a single model family, rather than comparing just GPT-3.5 to GPT-4. However, our basic methodology is not dependent on any assumptions about models' pretraining set. Moreover, we get similar results when finetuning on Llama 3 (Appendix G.5).

Another limitation is that we created special datasets for models to learn specific latent structures via finetuning. Our experiments on _Mixture of Functions_ test learning without variable names, though we still rely on the model to associate information with specific prompt formats. In more realistic AI safety relevant scenarios, models would need to learn without a special dataset, potentially during pretraining. This could be harder because realistic data is more heterogeneous and contains many irrelevant documents. However, the fact that pretraining data is more diverse than our special finetuning data could also help with inductive OOCR abilities.

## 5 Related work

Evidence of out-of-context reasoningSeveral prior works investigate sophisticated OOCR [7; 18; 31; 16; 20; 8; 2; 3]. For instance, Berglund et al.  found that finetuning LLMs on descriptions of chatbots (e.g. "Pangolin AI responds in German") resulted in LLMs emulating the described behavior zero-shot. Meinke and Evans  found that declarative facts included in finetuning data affect models' generations. Our work differs in that we never train directly on facts, instead focusing on inferring implicit facts from large numbers of training documents.

Yang et al.  found evidence that latent multi-hop reasoning occurs during inference without chain of thought, by analyzing hidden representations of LLMs. This aligns with our _Locations_ results, where models were able to answer questions about typical foods from unknown places. Krasheninnikov et al.  also learn latent information (whether a specific define-tag reliably aligns with pretraining knowledge), but they study effects of latent knowledge on factual learning during finetuning. Misra et al.  found evidence of language models using prior knowledge when performing classification with learned properties. Neither of these works shares our focus on directly verbalizing latent knowledge and using it for downstream out-of-distribution evaluations.

Length generalizationLength generalization refers to the ability to perform well on unseen, longer problem instances after training on shorter ones. Prior work on length generalization [5; 4; 34] focuses on learning skills during training and generalizing to instances of the same task (e.g., parity learning). In contrast, we focus on learning latent values and our evaluations require the model to extend to completely different tasks.

Reasoning in LLMsThe reasoning abilities of LLMs are typically measured by their performance on reasoning benchmarks [27; 26]. Our suite of tasks differs in that we require finetuning to learn latent knowledge, and we do not permit the use of chain of thought, which is commonly employed to enhance model performance on these benchmarks.

Knowledge PropagationPrevious research on knowledge editing investigates whether injected knowledge is truly internalized by the language model [19; 22; 10; 33]. Our work shares similarities in that we also inject knowledge and measure how well the model internalizes it. However, we do not directly train on the fact we want to inject (the latent information), and unlike in the knowledge editing literature, we do observe knowledge propagation (generalization to downstream tasks).

## 6 Conclusion

In this work, we introduced inductive out-of-context reasoning (OOCR). We developed a methodology to study this capability via finetuning on a suite of five tasks spanning different domains. Our experiments showed that LLMs finetuned on task-specific data containing implicit information were able to directly verbalize the latent information and integrate it with background knowledge and skills for downstream out-of-distribution tasks, without chain of thought. We found that inductive OOCR worked even in cases where in-context learning from training examples failed. We also observed improvements when using GPT-4 compared to GPT-3.5.

Future work could focus on developing more realistic and safety-relevant inductive OOCR tasks. It would also be valuable to study the effect of model scale on inductive OOCR more carefully. Another interesting direction would be to study inductive OOCR mechanistically to see how models learn and represent latent values. We hope that the results in our paper will inspire more work on evaluating and understanding inductive OOCR abilities in LLMs, which could be important to adequately assess safety of future LLM applications.