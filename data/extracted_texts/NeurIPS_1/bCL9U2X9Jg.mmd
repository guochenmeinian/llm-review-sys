# \(E^{2}\): Easy Regional Contrastive Learning of Expressive Fashion Representations

Daiqing Qi

University of Virginia

Charlottesville, VA 22904

daiqing.qi@virginia.edu

&Handong Zhao

Adobe Research

San Jose, CA 95110

hazhao@adobe.com

&Sheng Li

University of Virginia

Charlottesville, VA 22904

shengli@virginia.edu

###### Abstract

When learning vision-language models (VLM) for the fashion domain, most existing works design new architectures from vanilla BERT with additional objectives, or perform dense multi-task learning with fashion-specific tasks. Though progress has been made, their architecture or objectives are often intricate and the extendibility is limited. By contrast, with simple architecture (comprising only two unimodal encoders) and just the contrastive objective, popular pre-trained VL models (e.g., CLIP) achieve superior performance in general domains, which are further easily extended to downstream tasks. However, inheriting such benefits of CLIP in the fashion domain is non-trivial in the presence of the notable domain gap. Empirically, we find that directly finetuning on fashion data leads CLIP to frequently ignore minor yet important details such as logos and composition, which are critical in fashion tasks such as retrieval and captioning. In this work, to maintain CLIP's simple architecture and objective while explicitly attending to fashion details, we propose \(E^{2}\): Easy Regional Contrastive Learning of Expressive Fashion Representations. \(E^{2}\) introduces only a few selection tokens and fusion blocks (just 1.9% additional parameters in total) with only contrastive losses. Despite lightweight, **in our primary focus, cross-modal retrieval**, \(E^{2}\) notably outperforms existing fashion VLMs with various fashion-specific objectives. Moreover, thanks to CLIP's widespread use in downstream tasks in general domains (e.g., zero-shot composed image retrieval and image captioning), our model can easily extend these models from general domain to the fashion domain with notable improvement (Fig. 1). To conduct a comprehensive evaluation, we further collect data from Amazon Reviews to build a new dataset (Fig. 4) for cross-modal retrieval in the fashion domain.

Figure 1: Model performance on downstream tasks. The first row shows the direct application of our model in cross-modal retrieval. The second row shows that, when replacing the CLIP encoders with ours in existing SOTA models in _zero-shot_ text-guided image retrieval and fashion image captioning, their results can be further improved notably. (We show R@1 results for cross-modal retrieval, average R@10 for text-guided image retrieval, and B@4 for image captioning. )

## 1 Introduction

There has been a long research line for vision-language learning in general domain [15; 34; 19; 23; 26; 40; 52; 53]. Recently, cross-modal retrieval in fashion domain is receiving increasing attention [10; 27; 12; 56; 13; 4; 38; 22; 14; 12]. Most existing fashion vision-language models (VLM) [12; 38; 14; 10; 12; 56] design new vision-language models based on vanilla BERT .

Existing studies commonly train BERT-based models with Masked Language Modeling (MLM), (Image-Text Matching) ITM, Image-Text Contrastive Learning (ITC), or Masked Image Modeling (MIM) to their specific architectures. Various techniques tailored to fashion domain have been proposed, including learning extra fashion- specific tasks, new attention mechanism , or additional modules for fashion feature learning . FashionSAP  and FAME-ViL  perform a fashion-specific multi-task learning with various fashion tasks in addition to cross-modal retrieval, such as category recognition. However, the learning of existing BERT-based or multi-task fashion VLMs is often intricate with their complex architecture or additional fashion-specific objectives. By contrast, with simple architecture that comprises only two unimodal transformer encoders, and a single contrastive learning objective, contrastive language-image pre-training models such as CLIP  exhibit outstanding performance from cross-modal retrieval to a wide range of downstream tasks [24; 42; 20; 33]. Motivated by the simplicity and effectiveness of CLIP, we look forward to learning a model that inherits such benefits of CLIP in the fashion domain, which could be simple, lightweight while highly effective. While directly finetuning CLIP (FashionCLIP ) is an intuitive solution, it is deficient in presence of the notable domain shift . Consequently, Ma et al.,  uses CLIP as backbone and improves it by using additional text encoders to mitigate the word ambiguity in fashion language. However, they ignore the uniqueness of visual learning in fashion domain.

Different from data in general domain, product images and descriptions in fashion domain are unique in several aspects. As illustrated in Fig. 2, in general domain, an image contains only a few distinctive objects, and text descriptions are more concise and general. However, in fashion domain, a product image usually includes only one foreground object but with rich details. Besides, the fashion text often provides a group of metadata (tag entities) [16; 36], such as Composition, Brand, Description, Sub-category, etc. When directly finetuning CLIP on fashion data, we find it tends to give more attention to regions that are closer to a global view, e.g., dark regions in dark clothing (Fig. 2 and 14). Consequently, it misses details associated with the entities such as composition and logo, which represent more of a local view and are critical for distinguishing visually similar items.

To quantify the capability of visual representation in recognition of product details, we design an exploratory entity classification task based on _linear probing_: fitting a linear classifier on image embeddings from different models respectively, with tag entities used as labels. Results in Fig. 3 show that, the classifier which learns from fine-tuned CLIP (CLIP-FT) embeddings are less effective, **indicating that CLIP-FT embeddings contain less entity-related information, i.e., CLIP-FT is ineffective in extracting entity-related information from fashion images.** Details are available in Appendix E, where we provide a more comprehensive analysis.

Figure 2: Illustration of domain gap and attention results of CLIP and \(E^{2}\). General domain data often consist of a short caption which describes _a few_ objects in an image, while fashion data come with description and meta information (tag entities) of a _single_ product. **CLIP**: Image Tokens with maximum attention values (in each attention head) with the global token are marked yellow. \(E^{2}\): Selected image tokens by selection tokens during the second stage are colored. Blue: Brand. Orange: Season, Red: Sub-category. Green: Composition.

Then it is natural to ask: is it possible to have a CLIP image encoder that learns richer visual representations towards tag entities without eroding its learnt knowledge?

Towards this end, we propose our model: \(E^{2}\): **E**asy Regional Contrastive Learning of **E**xpressive Fashion Representations. Without modifying the Vision Transformer (ViT)  structure of the image encoder, \(E^{2}\) learns richer representations with the guidance of region contrastive loss. Meanwhile, it explicitly pays more attention to details in images by selecting most relevant image patch tokens that contain rich detailed information and fusing them with selection tokens during the forward propagation in ViT. Our contributions are follows:

**(1)** We first reveal that the image encoder of pre-trained CLIP is biased towards visually dominant regions of a product and consequently ignores small but critical details _when directly fine-tuned on fashion domain_. Furthermore, we present the first simple yet effective solution to this problem by allowing the model to learn more fine-grained visual representations towards tag entities. **(2)** Motivated by the observation in (1), we propose a concise regional contrastive learning framework comprising only two unimodal encoders and just contrastive learning objectives, while it learns richer visual representations. **(3)** We conduct extensive experiments with existing benchmark datasets, including our new benchmark dataset (Fig. 4) for fashion _cross-modal retrieval_, which differs from the popular benchmark FashionGen  in notably larger size, a wider variety of brands and products, more concise and general descriptions, and more diverse image scopes, making it more challenging and more practical. **(4)** Inheriting the benefits of the widely used CLIP, our model can easily extend models from general domain to the fashion domain with notable improvement (Fig 1) in downstream tasks (e.g., zero-shot composed image retrieval and image captioning).

## 2 Related Work

**Text and Image Matching.** Text and image matching aims to semantically align the text and image. Various BERT-based vision-language models (VLM) [40; 26; 41; 25; 14; 12; 10] are proposed towards this end. Different from previous studies, CLIP  introduces a large-scale vision language pre-training framework, which learns from 400 million image-text pairs with contrastive learning. Despite of the simplicity of its structure, CLIP is surprisingly effective in visual-language learning and various downstream tasks [24; 42; 20].

**Fashion Cross-modal Retrieval.** Different from general domain, the fashion data involves large amount of domain-specific information with richer details than data from general domain, such as the brand, material, texture, composition, various of minute design differences, etc. Besides, the fashion text is composed of description and a group of tag entities (meta information), which describes the meta information of products, such as its sub-category, season, brand, to name a few. Wang et al.  enhances the task with knowledge graph [17; 37; 31]. A line of research pre-trains BERT-based models [10; 56; 12; 14; 12] that are tailored for fashion data. Fame-ViL  and FashionSAP  further formulate a multi-task learning framework with extra fashion-specific objectives. Different from existing BERT-based models, Chia et al.  and Ma et al.  are built upon powerful CLIP. While Chia et al.  directly finetune CLIP for continual learning  with fashion data, Ma et al. 

Figure 4: Comparison of FashionGen  and AmazonFashion (Ours), which is notably different in size, diversity, language style and image scope (close-up shots of products are included). We compare it with more datasets [7; 51], and show more details (e.g., clustering ) in Appendix D.

Figure 3: Linear Probing Results on FashionGen. The more informative embeddings are, the higher accuracy a classifier obtains.

improve its language learning with additional text encoders to diminish word ambiguity. However, they do not consider the visual discrepancy. To better learn from fashion images, which contain richer detailed information, we improve CLIP from the perspective of visual learning, enabling its image encoder to learn more fine-grained representations for better image-text alignment.

**Zero-shot Text-guided Image Retrieval.** In Text-guided Image Retrieval (TGIR), also known as compositional image retrieval (CIR), users perform interactive dialogue to refine a given query image toward retrieving specific items. Classic models often employ custom models that project text-image pairs into a common embedding space. With the advance of VL foundation models (e.g., CLIP), interest in CIR has surged, especially in zero-shot settings without task-specific models . We show that when combined with \(E^{2}\), which learns more fine-grained representations towards fashion product details, their performances can be further improved.

**Zero-shot Image Captioning.** Zero-shot captioning [3; 46; 1; 42; 50; 24; 30] aims to generate image/video captions without human-annotated data. Different from above works, built upon CLIP, DeCap  and CapDec  use text-only data to train a decoder from scratch. We show that when combined with \(E^{2}\), which learns more fine-grained representations towards fashion product details, their performances can be further improved.

We provide a detailed discussion on our innovation compared with existing works in Appendix G.

## 3 Methodology

### Contrastive Language-Image Pre-training

Instead of learning from predicting a fixed set of predetermined object categories, i.e., the classification task, CLIP (Contrastive Language-Image Pre-training)  directly learns visual representations from raw text, and it is trained on 400 million image-text pairs with contrastive learning. Specifically, given a batch of \(N\) image-text pairs \(\{(_{i},_{i})\}_{i=1}^{N}\), images and texts are encoded as \(d\)-dimensional embeddings by the image encoder \(h^{I}()\) and the text encoder \(h^{T}()\). Denote the image embedding and text embedding as \(_{i}^{I}\) and \(_{i}^{T}\), respectively. During training, CLIP learns image-text matching from of \(N N\) possible combinations by maximizing the similarity scores of \(N\) matched pairs while minimizing the scores of the rest \(N^{2}-N\) mismatched pairs. The cosine similarity score of an image-text pair is calculated as \(_{i}^{I}_{i}^{T}\), \((i,j\{1,2,...,N\})\). In practice, CLIP optimizes a cross-entropy loss over the \(N N\) similarity scores matrix, namely contrastive loss, denoting as:

\[_{contra}(\{(_{i}^{I},_{i}^{T})\}_{i=1}^{N})=_{I2T}+_{T2I}, \]

where \(_{I2T}=-_{i=1}^{N}_{i}^{I }_{i}^{T}/)}{_{j=1}^{N}(_{i}^{I} _{j}^{T}/)}\), and \(_{T2I}=-_{i=1}^{N}_{i}^{T }_{i}^{I}/)}{_{j=1}^{N}(_{i}^{T} _{j}^{T}/)}\), \(\) is the temperature scalar. CLIP proves its effectiveness on various tasks in general domain. However, when adapting it to fashion domain, it is particularly difficult for CLIP to effectively learn from the fashion data due to its uniqueness in richer details and more compact layout. To overcome the challenges, we make full use of tag entities and learn more fine-grained representations with explicit (1) token fusion and selection, and (2) region contrastive learning.

### Regional Contrastive Learning of Fashion Representations

We present details of our framework, Easy Regional Contrastive Learning of Expressive Fashion Representations (\(E^{2}\)) in Fig. 5. The core idea of \(E^{2}\) is "Easy" and "Expressive": it inherits the (1) simple design of CLIP with only a few inserted fusion blocks and selection tokens in the vision encoder, and (2) the simple learning objective: only contrastive learning objectives are used. Yet \(E^{2}\) is more effective than existing large fashion VLMs with various objectives.

In the following, we first describe the overall framework of \(E^{2}\), and then introduce the selection tokens, which are key elements for our (1) token fusion and selection, and (2) region contrastive learning. After that, we explain each component and finally summarize the whole training process.

**Framework.** We build \(E^{2}\) upon CLIP , where we keep its text encoder unchanged and facilitate its image encoder with proposed selection tokens, fusion blocks and region contrastive loss. Similar to CLIP, given an image-text pair, \(E^{2}\) learns one global embedding for each for the calculation of similarities for contrastive learning.

**Selection Tokens.** Given an image, it is first split into \(P\) non-overlapping patches, which are linearly projected into a latent space, denoted as \(\{_{i}\}_{i=1}^{P}\). In addition to \(P\) image tokens, we further propose a set of _selection tokens_ categorized into \(C\) categories, with \(S\) tokens for each sub-category, denoted as \(\{_{i}\}_{i=1}^{C S}\). For instance, for the image encoder in Fig. 5, \(C=3\) and \(S=2\), The two blue, red and yellow selection tokens are associated with the tag entity _Brand_, _Sub-category_ and _Season_, respectively. To capture detailed information in image patches, each selection token updates itself with one most relevant image token in fusion block. Note that selection tokens are supervised by the region contrastive loss. We will discuss it later in this section.

**Multi-Stage Image Token Fusion and Selection.** To learn more fine-grained visual representations with the help of tag entities in fashion language, we use a group of selection tokens to iteratively select most informative images tokens and merge themselves with the selected ones. In this way, selection tokens contain rich information about the details of their associated tag entities. For more fine-grained interactions, we drop less informative image tokens and only keep the global embedding and selection tokens as the input of the _last_ transformer layer. As shown in Fig. 5, we perform a multi-stage token fusion and selection to obtain the final global embedding. Given an image, we first obtain \(P\) image tokens \(\{_{i}\}_{i=1}^{P}\). Then they are concatenated with a set of selection tokens \(\{_{i}\}_{i=1}^{C S}\) and input to the image encoder, where we perform a multi-stage token fusion and selection. In each stage, input tokens sequentially go through a few transformer layers and a fusion block. Formally, suppose there are \(L\) stages, and during the \(l\)-th stage, we denote the input tokens as \(\{_{i}^{l}\}_{i=1}^{P}\) and \(\{_{i}^{l}\}_{i=1}^{C S}\). The information propagation with each group of transformer layers is performed:

\[\{}_{i}^{l}\}_{i=1}^{P},\{}_{i}^{l}\}_{i=1}^{C S}= ([\{_{i}^{l}\}_{i=1}^{P};\{_{i}^{l}\}_{i=1}^{C  S}]), \]

where \([;]\) means concatenation. Then the obtained image and selection tokens are fed to the fusion block, where each selection token selects one most related image token and updates the embedding of itself with the selected image token:

\[\{_{i}^{l+1}\}_{i=1}^{P},\{_{i}^{l+1}\}_{i=1}^{C S}=([\{}_{i}^{l}\}_{i=1}^{P};\{}_{i}^{l}\}_{i=1}^{C  S}]), \]

After that, the output tokens serve as input tokens for stage \(l\)+1 if it is not the last stage. After obtaining the output tokens from the last stage \(L\) (via Eq. 3 with \(l\)=\(L\)), we only keep the global embedding \(_{0}^{L+1}\) and selection tokens \(\{_{i}^{L+1}\}_{i=1}^{C S}\) as the input to the last transformer layer. To enrich the global embedding with fine-grained features related to given tag entities, we enforce it to

Figure 5: (a) **Framework.**\(E^{2}\) contains an image encoder and a text encoder. The image encoder consists of CLIP transformer layers with inserted fusion blocks, where selection tokens update themselves with most relevant image tokens. After the second stage of token fusion, only the global image token and selection tokens are kept as input for the last transformer layer. Selection tokens further learn entity-specific knowledge with region contrastive loss. (b) **The Architecture of Fusion Block.** In fusion block, each selection token selects one most relevant image patch tokens and update itself with the averaged embedding of itself and the selected token. (c) **Visualization**. Logo is covered by blue masks. Front zipper area indicates its season (Fall/Winter 2016) and left front area with sleeves suggest the sub-category (shirts). _Note that the figure is only for illustration, in experiments, we consider \(C=4\) tag entities and assign \(S=2\) selections for each._focus on interacting with informative selection tokens by dropping less relevant image tokens to avoid their distractions. This step is critical for the image encoder to effectively learn more fine-grained visual representations with tag entities. As shown in Fig. 5(a), the last transformer layer is applied on the kept tokens to obtain \(}_{0}^{L+1}\). Finally, the global image embedding \(^{I}\) is obtained by applying a MLP to it:

\[}_{0}^{L+1}=([_{0}^{L+1};\{_{i}^{L+1} \}_{i=1}^{C S}])^{I}=(}_{0}^{L+1}). \]

**Fusion Block.** In fusion blocks, selection tokens are fused with their most relevant image patch tokens to enrich themselves with entity-specific (e.g., brand, composition, etc.) visual information. Given a group of image patch tokens \(\{_{i}^{I}\}_{i=1}^{P}\), denoting as a matrix \(^{l}\), and a group of selection tokens \(\{_{i}^{I}\}_{i=1}^{C S}\), denoting as a matrix \(^{l}\), we use selection tokens as queries to select the most relevant image tokens, which we call _hard attention_. For each selection token, it is updated by averaging itself with the selected image patch token. Specifically, denoting \(^{l}\), \(^{l}\) and \(^{l}\) are linear projections of \(^{l}\) and \(^{l}\) respectively: \(^{l}=_{q}^{l}\), \(^{l}=_{k}^{l}\) and \(^{l}=_{v}^{l}\). Attention weight matrix \(^{l}\) is calculated by:

\[^{l}_{i,j}=^{l}_{i,:}^{l}_{:,j}+_{i} )}{_{k=1}^{P}(^{l}_{i,:}^{l}_{:,k}+_ {k})} \]

where \(_{i}\) is the i.i.d random sample drawn from the \(Gumbel(0,1)\) distribution. To explicitly select one most similar image patch token for each selection token, we reformulate the attention weight matrix by turning each row \(_{i,:}\) into a one-hot representation with assigning one to the term with highest similarity score and zero to the rest. As the \(argmax\) operation is not differentiable, we use the gumbel-softmax and straight-through trick :

\[}^{l}_{i,:}=}(} \,^{l}_{i,j})+^{l}_{i,:}-(^{l}_{i,:}), \]

where the operator \(\) stops the gradients propagation. With one-hot vectors in the attention weight matrix \(^{l}\), each selection token can pick up one corresponding image patch token and update itself with the selected one via:

\[^{l+1}_{i,:}=^{l}_{i,:}+^{l}_{i,:}^{l}. \]

In fusion blocks of different stages, selection tokens constantly select most relevant image tokens explicitly and enrich the representations of themselves with selected token embeddings for better selection and fusion in the next stage.

At the last stage, the output of the fusion block only contains selection tokens, which already contain rich fine-grained information about associated tag entities (i.e., _brand_, _composition_, _season_ and _categories_, etc.). They are input to the last transformation layer with the global image token, so that the global token can effectively interact with them without distractions from irreverent patch tokens. In this way, the global token better captures details of fashion images.

**Region Contrastive Learning with Selection Tokens.** As each selection token is associated with a tag entity and it aims to select most relevant image tokens in fusion blocks, selection tokens are further supervised with the region contrastive loss. Specifically, assume we have a group of selection tokens \(\{_{i}\}_{i=1}^{C S}\), which are categorized into C categories, with \(S\) tokens for each sub-category. For better illustration in this subsection, we reformulate them as \(\{_{s}^{c}\}\) where \(c\{,,...\}\) (\(C\) categories in total) and \(s\{1,2,...,S\}\). After going through the last transformer layer, each set of selection tokens \(\{_{s}^{c}\}_{s=1}^{S}\) with the same associated sub-category \(c\) is fed to its corresponding average pooling layer (Fig. 5 right), so that selection tokens from the same sub-category are formed to a more comprehensive and informative single pooled embedding \(_{final}^{c}\) as:

\[_{final}^{c}=(\{_{s}^{c}\}_{s=1}^{S}),\ c\{ ,...\}. \]

Similar to the contrastive learning process with global image embeddings and global text embeddings introduced in section 3.1, each \(_{final}^{c}\) learns to match the corresponding ground truth tag entity \(^{T(c)}\) via the region contrastive loss by replacing the global image embedding with the corresponding pooled selection token in Eq. 1:

\[_{region}(\{(_{i}^{T(c)},_{final}^{c})\}_{i=1}^{N})= _{I2T}+_{T2I}. \]

The region contrastive loss explicitly aligns each \(_{final}^{c}\) with the corresponding ground truth tag entity \(^{T(c)}\) for sub-category \(c\). With region contrastive loss, pooled selection tokens are distinguishable towards tag entities, which means learnable selection tokens can effectively select and fuse themselves with tag entity-specific image patch tokens given fashion images. Finally, the global image embedding learns more fine-grained representation by: (1) interactions with these selection tokens in transformer layers, and (2) explicit token selection process before the last transformer layer, which filters less relevant patch tokens and helps the global embedding to concentrate more on certain regions.

**Remark.** Some semantic segmentation models, e.g., Seg  and GroupViT  also involve additional tokens. But the usage is notably different. As their motivation is to group image tokens to larger objects, each image token is assigned to an additional token, where softmax is applied to _additional tokens_. Because our motivation is to pay more attention to details of an object, each additional (selection) token selects a most relevant image token, where softmax is applied to _image tokens_. We carefully discuss the differences in detail in Appendix H.

## 4 Experiments

**Datasets.** For a fair comparison, we first evaluate our model on the benchmark dataset FashionGen , following existing works [10; 56; 12; 27]. Besides, we also collect text descriptions and product images in fashion domain from Amazon Reviews  and build a large-scale fashion dataset which contains 1.3M image-text pairs, where we use \(910\)K and \(390\)K pairs for training and test, respectively. It is a more challenging dataset as the text descriptions are briefer and more general than FashionGen. We refer this dataset as AmazonFashion. More details are available in Appendix D.

**Settings.**\(E^{2}\) is initialized from the pre-trained CLIP  (ViT-B-32 ) We use two selection tokens for each tag entity (\(S=2\)), as Fig. 5 shows. We present the validation of this choice in later experiments. More detailed configuration and hyper-parameter setting are available in Appendix A.

**Evaluation.** Our model is evaluated in various downstream tasks. For retrieval tasks, we perform two kinds of evaluations. The positive candidate is the ground truth item, while the negative candidates are either \(100\) randomly sampled unmatched items (referred as \(100\) evaluation) or all unmatched items in the dataset (full evaluation). Following [10; 27], the evaluation metrics are Rank@1, Rank@5, Rank@10 and \(SumR\)=100(Rank@1 + Rank@5 + Rank@10).

**Note** that full evaluation, which is suggested by latest works [27; 12], is more challenging and practical and consistent with practical retrieval tasks. We compare with baselines with full evaluation unless they are not compatible with full evaluation, where we run Sample-100 evaluation instead.

### Comparison with State-of-the-Art

**Cross Model Retrieval.** For full evaluation on FashionGen, we compare our model with existing fashion VLMs and finetuned CLIP-based models. To compare with more baselines which are not compatible with full evaluation, following [10; 56], we also do a Sample-100 evaluation. Because CLIP-FT and FILIP-FT outperform other baselines by a large margin, we compare \(E^{2}\) with the two closest baselines on AmazonFashion with full evaluation. Table 1 shows \(E^{2}\) notably outperforms other baselines in full evaluation, manifesting the effectiveness of paying more attention to certain regions

    &  &  &  \\  &  &  &  &  &  \\  ALBERT  & 41.7 & - & - & 51.0 & - & - & - & 46.2 \\  SyncMask  & 55.4 & - & - & 64.0 & - & - & - & 59.7 \\  FashionSAP  & 54.3 & 77.3 & 83.2 & 62.8 & 83.9 & 90.6 & 451.8 & 58.5 \\  FashionViL  & 42.8 & 71.5 & 80.6 & 51.3 & 75.4 & 84.7 & 406.5 & 47.1 \\  FashionCLIP  & 54.3 & 85.5 & 92.8 & 54.4 & 85.9 & 92.3 & 465.2 & 54.4 \\  FILIP-FT*  & 55.5 & 86.1 & 93.2 & 55.8 & 86.5 & 93.1 & 470.2 & 55.7 \\  CLIP-FT  & 55.2 & 85.7 & 92.9 & 55.4 & 86.2 & 92.9 & 468.2 & 55.3 \\  Ours & **62.8** & **89.7** & **95.3** & **64.5** & **90.1** & **95.5** & **497.9** & **63.7** \\   

Table 1: Retrieval performances (full evaluation) on Table 2: Retrieval performances (sample-100 Fashion-Gen. (*) denotes results from our implemen- evaluation) on Fashion-Gen. (*) denotes retention. As the code and pre-trained model of FILIP \(\,\)sults from our implementation.

that contain rich information about tag entities. Table 2 shows our model also outperforms baselines in sample-100 evaluation by a large margin. On the more challenging large-scale AmazonFashion dataset, \(E^{2}\) consistently achieves better performance than competing baselines (Table 3).

In Fig. 6, we present examples of retrieval results by \(E^{2}\) and CLIP to illustrate in which cases CLIP fails to differentiate minor differences towards visually similar products. In Fig. 6(a), both CLIP and \(E^{2}\) retrieved visually similar black trousers. While \(E^{2}\) ranks the ground truth sample first, CLIP ranks visually similar product higher without considering its composition.. Fig. 6(b) shows \(E^{2}\) ranks the _Gucci_ backpack first by recognizing its logo at face while CLIP ranks the _Kara_ backpack first incorrectly. More visualizations are available in Appendix B.

**Zero-shot Image Captioning (ZS-IC).** Built upon CLIP, \(E^{2}\) can be easily extended to popular SOTA CLIP-based zero-shot image captioning models . DeCap  and CapDec  share similar high-level ideas: during training, a language decoder is trained to reconstruct the text input, where the CLIP text encoder serves as the encoder. During inference, with the CLIP image encoder, a given image is first encoded to a CLIP feature, which is later fed to the pre-trained decoder to generate captions. We train the decoder on text-only data from FashionGen but with our \(E^{2}\) as the backbone encoder instead of CLIP. Results are shown in Tab. 4. As learnt features by \(E^{2}\) contain more fine-grained information towards product specifications, generated captions with \(E^{2}\) better matches ground truth captions, leading to high scores. We show the example in Fig 7. Vanilla DeCap tends to make more mistakes towards details, such as brand, composition. While our improved DeCap with \(E^{2}\) constantly yields better results.

**Zero-shot Text-guided Image Retrieval (ZS-TGIR).** CIReVL  exploits pre-trained vision-language models (CLIP) alone with an LLM for ZS-TGIR without training. Similar to the case in ZS-IC, when replacing the CLIP encoders with \(E^{2}\), CIReVL is easily extended to fashion domain, consequently the model performance notably improves in Tab. 8.

**Remark.** EI-CLIP  and fine-grained CLIPs  are not necessarily our baselines, as our focuses are different and orthogonal to each other. Still we have a comparison to demonstrate the uniqueness and advantages of our model. Detailed results and discussions are presented in Appendix H.

### Ablation and Further Analysis

To study the effectiveness of each module and how much each group of selection tokens contribute to the our model, we design ablation studies from the two perspective: architecture and selection tokens. **Architecture.** We first ablate fusion blocks, where the input and output of each transformer layer are

    &  &  &  \\  & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 &  \\  FLIP-FT*  & 6.2 & 17.8 & 25.8 & 6.2 & 18.1 & 25.9 & 100.0 \\  CLIP-FT  & 6.1 & 17.7 & 25.8 & 6.2 & 17.9 & 25.8 & 99.5 \\  \(E^{2}\) (Ours) & **7.5** & **20.3** & **28.4** & **7.4** & **20.2** & **28.4** & **112.2** \\   

Table 3: Retrieval performances (full evaluation) on AmazonFashion. (*) denotes results from our implementation.

    &  &  &  \\  & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 &  \\  FLIP-FT*  & 6.2 & 17.8 & 25.8 & 6.2 & 18.1 & 25.9 & 100.0 \\  CLIP-FT  & 6.1 & 17.7 & 25.8 & 6.2 & 17.9 & 25.8 & 99.5 \\  \(E^{2}\) (Ours) & **7.5** & **20.3** & **28.4** & **7.4** & **20.2** & **28.4** & **112.2** \\   

Table 4: **Zero-shot image captioning results on FashionGen with BLEU@4 (B), CIDEr (C), METEOR (M), ROUGE (R).**

Figure 6: **T21 retrieval examples. For each example, the query text is displayed on the left. The first row is the top-5 retrieval result by \(E^{2}\) and the second row is the result by CLIP-FT. Ground truth is boxed in green. For each product image, its Composition (a) or \(\) (b) is marked below. For top-1 retrieved item, we mark its tag entity red if it is not consistent with the query text.**

all of the image batch tokens and selection tokens. Without the explicit selection and fusion process in the fusion blocks, the selection tokens, which are supervised with region contrastive loss and carry rich information towards tag entities, are still interacting implicitly with image patch tokens and the global token in ViT layers. In this way, we assume that, even without fusion blocks, the global token still learns richer information about tag entities and lead to better retrieval performance. Then we are curious to see how much the explicit token selection and fusion process benefit the learning process, in addition to the potential improvements from this implicit interaction in transformer layers. Table 9 shows both modules are critical. We also ablate our region contrastive loss (RCL).

**Group of Selection Tokens.** We also study how each group of selection tokens contribute to the model performance. While all groups of selection tokens improve the model performance, their contributions are different. While \(\) and \(\) are more helpful, \(\) and \(\) contribute slightly less. One potential reason is that they are easier to be visually distinguished than product texture and small logos, which especially require our fusion and selection process. We have detailed discussion on their effectiveness in Appendix F, and on parameter-efficiency in Appendix I.

**Parameter Sensitivity.** We also examine the impact of batch size, which has a significant influence on the performance of contrastive learning. \(E^{2}\) consistency surpasses CLIP-FT across a range of batch sizes, with greater improvement over CLIP-FT as batch size decreases. We present detailed results and analysis in Appendix C.

**Number of Selection tokens.** In experiments, we use two selection tokens for each tag. It is reasonable as we are not selecting and fusing image patches in raw pixel space, instead, we conduct it with image patch (token) embeddings within ViT layers in contextual embedding space, where each token contains rich context/neighbor information. In fact, one token can already represent a large area if considering its neighbour information. Our choice of two tokens is empirically enough for all tags. We quantitatively validated this choice in Appendix I.

## 5 Limitations

Built upon the pre-trained foundation VL model (e.g., CLIP), \(E^{2}\) could be bottlenecked by the qualify of its large-scale pre-training in general domain. Besides, as our regional contrastive learning with fusion blocks and selection tokens explicitly aligns regions in the input image with the corresponding tag entities from text, intuitively, the quality of the provided tag entities can influence our model performance. Although _in most cases_, fashion data contain such information (e.g., brand, composition etc.), it is still a limitation if only datasets with poor tag entities are available for finetuning.

## 6 Conclusion

In this paper, we propose a simple yet effective framework on learning fashion representations, and first emphasize the importance of learning fine-grained visual representations when applying CLIP to fashion domain. To achieve it, we further propose \(E^{2}\) with selection tokens and region contrastive loss to enforce extra attention to details. Experimental results prove the effectiveness of \(E^{2}\).

Figure 8: Results on Fashion-IQ dataset

Figure 7: **Zero-shot Image Captioning examples. We mark the wrong captions red, and mark a caption green if it _exactly_ matches the ground truth.**