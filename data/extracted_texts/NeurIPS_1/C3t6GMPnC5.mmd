# Mamba State-Space Models Can Be Strong Downstream Learners

Anonymous Author(s)

Affiliation

Address

email

Equal Contribution

###### Abstract

Mamba  state-space models (SSMs) have recently outperformed state-of-the-art (SOTA) Transformer large language models (LLMs) in various tasks and been widely adapted. However, Mamba's downstream learning capabilities remain either unexplored-e.g., mixed-precision (MPFT) and parameter-efficient fine-tuning (PEFT)-or under-evaluated-e.g., in-context learning (ICL). For the latter, recent works [45; 19] reported Mamba's ICL rivals SOTA Transformer LLMs using non-standard benchmarks. In contrast, we show that on standard benchmarks, pretrained Mamba models achieve only 38% of the ICL performance improvements (over zero-shot) of comparable Transformers.

Enabling MPFT and PEFT in Mamba architectures is challenging due to recurrent dynamics and highly customized CUDA kernels, respectively. However, we prove that Mamba's recurrent dynamics are robust to small input changes using dynamical systems theory. Empirically, we show that performance changes in Mamba's inference and fine-tuning due to mixed-precision align with Transformer LLMs. Furthermore, we show that targeting key memory buffers in Mamba's customized CUDA kernels for low-rank adaptation regularizes SSM parameters, thus achieving parameter efficiency while retaining speedups. We show that combining MPFT and PEFT enables up to 2.15 times more tokens-per-second and 65.5% reduced per-token-memory compared to full Mamba fine-tuning, while achieving up to 81.5% of the ICL performance improvements (over zero-shot) of comparably fine-tuned Transformers.

## 1 Introduction

Innovating on previous state-space models (SSMs) [23; 11], Mamba  has been recently proposed as an accurate, sub-quadratic alternative to Transformer large language models (LLMs). Mamba was initially shown to greatly outperform comparable Transformer LLMs  across a large number of standard natural language benchmarks. Subsequently, pretrained Mamba models have been widely adapted across different data modalities [42; 65; 36; 46; 37], tasks [60; 62; 48; 63; 57; 37; 2], and architectures [1; 45; 40].

However, despite such rapid and widespread adaptation, evaluation of Mamba's ability to perform standard downstream learning abilities exhibited by Transformer-based LLMs have either not been extensively conducted on _standard natural benchmarks_ or are completely lacking. For instance, while recent works [45; 19; 30] have evaluated Mamba's ability to perform in-context learning (ICL), such studies focused extensively on either non-natural tasks [30; 17] or non-standard benchmarks .

Furthermore, evaluation of Mamba's mixed-precision fine-tuning (MPFT) and performance efficient fine-tuning (PEFT) capabilities are currently lacking. For the former, MPFT (and, by extension, mixed-precision inference) are made difficult due to potential sensitivities of Mamba's recurrent dynamics, where [21; 29] suggest full precision (FP32) is required to perform stable training. For the latter, PEFT via standard low-rank adaptation (LoRA)  is made difficult within Mamba's SSM layer (referred to herein as the MambaBlock) due highly customized SSM CUDA kernels which provide competitive performance to attention-based speedups  at the cost of standard adapter support. However, PEFT and MPFT are arguably two of the most widely utilized techniques for LLM alignment  and customization , and are typically combined to drastically decrease hardware demands needed to fine-tune modern LLMs .

Herein, we extensively explore Mamba's downstream learning capabilities across standard natural benchmarks. For ICL, we show that, in contrast to recent non-standard studies showing Mamba models rival state-of-the-art (SOTA) LLMs of similar parameter counts, **the pretrained benefits of Mamba few-shot learning are significantly less than comparable Transformer LLMs across standard natural benchmarks**; averaged across the benchmarks and parameter counts in Table 1, **Mamba models only achieve 38% of the performance improvements (relative to zero-shot) of comparable Transformer models** from the Pythia suite . However, we show in the sequel that **Mamba models can more than halve this gap through efficient fine-tuning**, achieving as much as 81.5% of the average few-shot learning improvement (relative to zero-shot) of comparable Transformers.

For MPFT, we leverage theory from dynamical systems to show that small input changes in a MambaBlock do not lead to exponentially deviating outputs. Empirically, we validate this theoretical result; compared to full-precision, deviations due to mixed-precision for Mamba inference and fine-tuning are on par with those demonstrated by Transformer LLMs (Section 6). For PEFT, we show that by targeting the largest memory buffer exploited by Mamba's highly customized CUDA kernels, LoRA may be used for extremely efficient fine-tuning, while simultaneously regularizing the majority of Mamba's SSM parameters via weight tying. We show that this leads to extremely efficient PEFT, resulting in up to 2.15 times faster training and 65.5% reduced memory compared to the largest evaluated Mamba model without MPFT or PEFT.

## 2 Background

**Downstream learning for LLMs**. Since the release of the Transformer architecture , attention-based LLMs have exhibited several downstream learning abilities-in particular, PEFT, MPFT, and ICL-which allow the rapid adaptation of foundation models towards specific applications. PEFT using adapters  allows a large pretrained model to be efficiently adapted for a particular downstream task by freezing the full model and training only a small number of extra parameters. Arguably the most widely used such PEFT method is LoRA , which injects trainable low-rank matrices into Transformer layers to approximate weight updates.

To further decrease the computational demands necessary for LLM fine-tuning and inference, MPFT via mixed-precision (i.e., FP16 or BF16) [43; 42] and quantized low-precision  have proven effective strategies to reduce GPU memory and runtime requirements without deleterious effects on downstream performance [12; 59]. Additionally, mixed-precision approaches have paved the way for hardware-aware optimizations within the self-attention module , greatly mitigating the quadratic complexity of Transformer LLMs. Together, PEFT and MPFT have created a rich ecosystem with which varying combinations of these approaches may be used to meet the computational constraints of a given training system. We note that post-fine-tuning quantization approaches  may be further used to decrease Transformer LLM computational demands, but such approaches are not considered in this work.

ICL provides an adaptable alternative to fine-tuning. Rather than fine-tune the LLM directly, ICL augments a prompt with \(n\) relevant examples (called _shots_) preceding the query of interest. Given sufficiently large models and pretraining data [8; 58], Transformer LLMs have proven adept at learning new concepts on the fly provided such few-shot prompting. However, it is worth noting that ICL inference time increases dramatically as the number of shots grows (due to self-attention's quadratic complexity) and PEFT (when possible) is known to produce more accurate downstream learning results [8; 41].

   Model & \(N\)-shot & LAMBADA & LAMBADA & HellaSwag & PIQA & Arc-E & Arc-C & WinoGrande & 0-shot incr. \\  & & & ppl \(\) & acc \(\) & acc \(\) & acc \(\) & acc \(\) & acc \(\) & acc \(\) & Mean \% \(\) \\   & 0 & **16.07** & **44.3** & **35.3** & 64.5 & 48.0 & **24.2** & **44.8** & – \\ Mamba & 1 & 19.34 & 38.3 & 35.2 & 64.3 & 47.1 & 23.5 & 51.3 & -1.4 \\
130M & 3 & 23.13 & 35.4 & 35.1 & **65.1** & **49.0** & 24.0 & 50.7 & -0.2 \\  & 5 & 24.38 & 36.2 & 34.8 & 64.9 & 49.2 & 23.9 & 50.5 & -0.5 \\  & 0 & 38.20 & 32.7 & 30.2 & 61.8 & 43.4 & 23.8 & 51.0 & – \\ Pythia & 1 & 47.21 & 28.2 & 30.6 & 62.2 & 43.4 & 23.7 & 49.3 & -0.4 \\
160M & 3 & 63.70 & 24.7 & 30.5 & 61.9 & 44.8 & 22.9 & 51.3 & **0.1** \\  & 5 & 66.30 & 25.3 & 30.4 & 62.6 & 43.4 & 23.1 & 50.8 & -0.2 \\   & 0 & **8.14** & **55.6** & **46.5** & 69.5 & 55.0 & 27.9 & 55.5 & – \\ Mamba & 1 & 9.74 & 49.8 & 45.9 & 69.3 & 57.4 & 26.5 & 54.6 & -0.8 \\
370M & 3 & 10.89 & 48.5 & 46.2 & **69.6** & **58.7** & **28.5** & 53.6 & 1.0 \\  & 5 & 11.36 & 48.5 & 46.2 & 69.4 & 58.3 & 28.0 & **56.0** & 1.3 \\  & 0 & 10.83 & 51.5 & 40.6 & 66.9 & 52.0 & 24.1 & 53.4 & – \\ Pythia & 1 & 12.26 & 47.1 & 40.5 & 68.0 & 53.8 & 25.6 & 52.4 & 1.8 \\
410M & 3 & 14.39 & 43.2 & 40.9 & 67.9 & 55.1 & 26.9 & 54.0 & **4.2** \\  & 5 & 14.62 & 44.1 & 40.8 & 68.1 & 54.6 & 26.6 & 53.4 & 3.5 \\   & 0 & **6.01** & **61.7** & **55.1** & 72.1 & 61.2 & 29.6 & 56.0 & – \\ Mamba & 1 & 7.06 & 56.2 & 54.5 & **72.5** & 63.3 & 30.1 & 56.9 & 1.4 \\
790M & 3 & 8.05 & 54.8 & 54.2 & 72.2 & 63.4 & 31.6 & 57.2 & 2.4 \\  & 5 & 8.83 & 53.4 & 54.6 & **72.5** & **64.6** & **32.1** & **57.5** & **3.4** \\  & 0 & 7.92 & 56.3 & 47.2 & 70.7 & 57.0 & 27.0 & 53.4 & – \\ Pythia & 1 & 8.99 & 51.8 & 47.3 & 70.7 & 57.1 & 28.2 & 53.4 & 1.0 \\
1B & 3 & 10.48 & 48.2 & 47.5 & 71.2 & 59.2 & 28.0 & 54.3 & 2.2 \\  & 5 & 10.86 & 48.4 & 47.3 & 71.4 & 58.7 & 28.4 & 53.1 & 1.9 \\   & 0 & **5.04** & **65.0** & **59.1** & 74.2 & 65.5 & 32.9 & 58.6 & – \\ Mamba & 1 & 5.83 & 60.6 & 58.20 & **74.7** & 64.5 & 33.0 & 61.2 & -0.5 \\
1.4B & 3 & 6.62 & 58.9 & 58.8 & 73.7 & 66.1 & 34.4 & 60.9 & 0.6 \\
5 & 6.98 & 58.4 & 59.0 & 74.0 & **66.4** & **35.5** & 60.5 & 1.4 \\  & 0 & 6.09 & 61.7

**State-space Models**. Structured state-space sequence (S4) models [23; 14] are SSMs which leverage linear time-invariant (LTI) systems to combine the computational advantages of Transformers-i.e., highly parallelizable training-and recurrent neural networks (RNNs)-i.e., subquadratic autoregressive inference using recurrency. Within the S4 layer, an input signal is discretized and LTI parameters representing the input's latent dynamics are learned. Owing to the S4 block's latent dynamics being LTI, the S4 block's output may be thus compactly represented as a single convolution between the input and an _SSM convolution kernel_ (a matrix whose entries are products of LTI learnable parameters resulting from unrolling the state-space equations). However, despite hardware efficiency and long-dependency-modeling improvements, LTI-based S4 models remained inferior to Transformers of comparable parameter-sizes for natural language tasks, even when augmenting S4 layers with attention-layers for hybrid architectures .

Innovating on these previous S4 approaches, Mamba utilizes time-_varying_ parameters to model latent dynamics, thus broadening the ability to capture nuanced changes evolving in discrete-time. Without LTI dynamics, however, the input-output representation via the SSM convolution kernel is no longer applicable, thus voiding previous hardware-aware S4 optimizations . To enable hardware efficiency with time-varying SSM parameters,  thus introduced extensively customized CUDA kernels which implement highly parallelized prefix sums to compute recurrent states.

## 3 Mamba state-space models

For model dimension \(d\) and maximum input sequence length \(T\), the MambaBlock defines state-space parameters \(,_{t},_{t},_{t}^{d  d}\) for \(t\{1,,T\}\). The matrix \(_{t}\) controls the discrete step-size. Given an input sequence \(_{1},,_{T}^{d}\), the following linear mapping through latent states \(_{1},,_{T}^{d}\) is used to produce the output \(_{1},,_{T}^{d}\):

\[_{t} =}_{t}_{t-1}+}_{t}_{t} \] \[_{t} =}_{t}_{t}, \]

where \(}_{t}=((_{t }))^{d d}\), \(}_{t}=}_{t})}\) and \(}_{t}=^{-1}(}-)_{t}\). In practice, \(,_{t},_{t}\) and \(_{t}\) are diagonal matrices.

**Hardware-aware optimizations**. As matrices \(_{t},_{t}\) and \(_{t}\) are time-varying, S4 optimizations via the SSM convolution kernel  are no longer applicable. However, by diagonality, each dimension may be computed in parallel. Furthermore, the recurrence along every dimension is a prefix sum (also called a _scan_), which is highly parallelizable .  thus capitalizes on this through extensively customized CUDA kernels wherein the majority of temporal variables are carefully laid out in a large buffer of GPU memory and manipulated. Instantiated as a PyTorch linear layer's weight matrix, this memory buffer \(^{n 3d}\) is used to store and access the diagonal elements of \(_{t},_{t}\) and \(_{t}\) for all \(t\{1,,T\}\), such that

\[[t-1,:d]=(_{t}),[t-1,d:2d]= (_{t}),[t-1,2d:3d]=( _{t}), \]

where \([0,:d]=(_{1}),[n-1,d:2d]= (_{T})\), and so on.

The customized Mamba prefix scan kernel heavily relies on this memory layout to optimize the access pattern of \(\) in Equations 5 and 6.We note that, rather than adjusting Mamba's low-level CUDA kernels themselves to integrate LoRA within the highly optimized prefix scan, we can instead directly target \(\). Doing so, we have the following, where the proof is available in Appendix A.

**Theorem 1**.: _Consider the weight matrix \(\) of a MambaBlock from Equation 3. Targeting \(\) for LoRA during fine-tuning ties adaptation weights across \(_{t},_{t}\) and \(_{t}\)._

## 4 Stable dynamics in the MambaBlock

The Mamba foundation models were pretrained in full FP32 precision. Consequently, official Mamba implementations have cautioned against fine-tuning or training in reduced precision [21; 29], with potential sensitivities of MambaBlock recurrent dynamics remaining an open question. We answer the latter using theory from dynamical systems. For Mamba's discrete dynamic system in Equations 5 and 6, define

\[_{t}=F_{}(_{t-1},_{t}), \]where \(\) denotes the time-varying parameters described in Section 3. For input sequence \(_{1},,_{T}\) and initial latent state vector \(_{0}\), we thus write

\[_{T}=F_{}(F_{}( F_{}(_{0},_{1})))  F_{}^{T-1}(_{0},_{1}).\]

The rate of divergence between two scalar \(\)-close inputs to a discrete dynamical system is bounded by the system's maximal Lyapunov exponent \(_{}\). Given \(_{}\) and two initial values \((_{0},_{1})\) and \((_{0}+,_{1}+)\), the maximum deviation between these points grows as :

\[|F_{}^{N}(_{0},_{1})-F_{}^{N}(_{0}+ ,_{1}+)|(})}).\]

Thus, when \(_{}>0\), nearby trajectories exponentially separate and, when \(_{} 0\), nearby trajectories ultimately converge to the same fixed point or periodic cycles.

The maximal Lyapunov exponent is defined as

\[_{}_{T}\|_{t =0}^{T}_{t}}{_{t-1}}\|_{2},\]

where \(\|\|_{2}\) denotes the spectral norm for matrices. For an arbitrary \(\), we prove the following:

**Theorem 2**.: _Let \((_{t-1},_{t})\) be the latent state and input at an arbitrary time \(t\{1,,T\}\) within a \(\). Then small changes \((_{t-1}+,_{t}+)\) produce deviations which are exponentially non-increasing over discrete-time. That is, \(|F_{}^{N}(_{t-1},_{t})-F_{}^{N}(_{t-1}+ ,_{t}+)|()\), for some scalar \( 0\)._

The proof of Theorem 2 is available in Appendix B, where the maximal Lyapunov exponent for an arbitrary \(\) is first proven to be non-positive. The main result subsequently follows.

**Consequences for automatic mixed-precision**. During a forward pass, automatic mixed-precision (AMP) saves time and memory by computing forward activations in half-precision (FP16 or BF16). During a backward pass, AMP computes gradients in half-precision and up-casts to full-precision prior to updating. In contrast to full-precision fine-tuning, MPFT within the \(\) thus results in small differences to the inputs \(_{1},,_{T}\) fed into the SSM scan (which are passed through a \(\)), \(_{t}\) (which is passed through a \(\)), and the gradients calculated during training.

For a discrete dynamical system with \(_{}>0\), changes due to AMP compound after repeated expansion of the recurrent state, thus leading to exponential deviations between quantities calculated using mixed- versus full-precision. We note that Transformers are not recurrent, and thus not susceptible to such issues. Yet, just as differences introduced by quantization/mixed-precision produce output differences in Transformer results, differences are expected in Mamba results using different precision strategies. However, by Theorem 2, such differences do not exponentially compound over discrete-time within the \(\).

## 5 Related Work

Several recent works  have studied Mamba's ability to perform ICL. However, none of these have extensively studied Mamba's ICL capabilities either on standard NLP benchmarks or on pure \(\) foundation models. In particular, foundational Mamba models' ICL abilities were tested in  to learn simple function classes (e.g., logistic regression and decision trees ) and in  to learn non-standard NLP benchmarks (i.e., task vectors ). While  report Mamba's ICL abilities rival SOTA Transformers, their utilized benchmarks were proposed as supplemental ICL studies after Transformer LLMs' success on standard NLP benchmarks . Indeed, direct evaluation of Mamba foundation models on standard NLP benchmarks does not lead to higher gains over zero-shot performance relative to comparable Transformer LLMs (demonstrated in Table 1).

Lyapunov exponents have previously been considered for classic RNN structures (e.g., vanilla RNNs, LSTMs, GRUs, PLRNNs, etc.) , to determine when such models exhibit chaotic dynamics and the impact on the exploding/vanishing gradient phenomena*. For more recent S4 neural models,  used Hurwitz matrices to characterize the numerical stability of linear time-invariant (LTI) S4 models. However, such analysis is not applicable to time-varying models, such as Mamba, nor does it characterize the effects of sensitive dependence on initial conditions (e.g., divergence of two \(\) close inputs). To the best of our knowledge, no previous works have used Lyapunov exponents to explore the effects of mixed-precision on recurrent neural models or Mamba architectures.

As in , the majority of subsequent Mamba works have focused on pretraining MambaBlocks using full precision . Notably, the official implementation of Jamba , the Transformer-Mamba hybrid, supports mixed- and 8-bit precision, but avoids MambaBlocks when applying such quantization . Similarly, the official Mamba sources advise using full precision within the MambaBlock , cautioning against using mixed-precision due to potential recurrent sensitivities.

To the best of our knowledge, no existing works have either theoretically explored the effects small input changes (e.g., due to mixed-precision) have on Mamba's recurrent dynamics, empirically explored such effects downstream impact on fine-tuning and inference, or explored pure Mamba networks fine-tuning abilities relative to Transformer LLMs.

## 6 Experiments

To demonstrate the implications of Theorem 2, we explore the performance difference between running inference with full-precision pretrained weights and using mixed-precision (FP16 and BF16) weights. **Model performance is measured as percent accuracy** using the MMLU  dataset. The difference in model performance is reported as the mean _divergence_ (i.e., absolute difference) between the original full-precision and respective mixed-precision model, averaged over {0, 1, 3, 5}-shot percent accuracy. Thus, **a divergence greater than one denotes an average difference greater than one entire percentage of accuracy.**

Mamba pretrained checkpoints are compared to pretrained Transformer models of similar parameter counts and no more than \(\)300B total pretraining tokens (Pythia , OLMo  336B-token checkpoint, and Phi 1.5 ). We note that Pythia and Mamba models were both pretrained using the same corpus , allowing the fairest comparison between SSMs and Transformers. To limit extraneous numerical effects within experiments (e.g., due to parameter aggregation across multiple GPUs), all models were run using a single GPU (Nvidia A10G, 24 GB total memory). All models were evaluated using the LM evaluation harness from Eleuther AI . Further experimental details are available in Appendix C. The results are available in Table 2.

From Table 2, inferencing in Pythia using FP16 and BF16 result in an average 0.13 and 0.41 full-precision divergence, respectively. Mamba displays similar averages in comparison: inferencing in Mamba using FP16 and BF16 result in an average 0.10 and 0.48 divergence, respectively. Interestingly, both SSM and Transformer architectures exhibit _large divergence spikes_-i.e., mean divergence greater than a percentage point-when using BF16, which occurs once for Mamba and Phi 1.5 models and twice for Pythia models. In the following, we show that such spikes may be mitigated for Mamba SSMs by combining mixed-precision with parameter-efficient adapters during fine-tuning.

**Non-divergent Mamba fine-tuning.** We next explore the implications of Theorem 2 on fine-tuning, wherein mixed-precision is especially critical; MPFT combined with PEFT adapters have been shown to drastically reduce Transformer fine-tuning times . We are thus interested in the divergence between Mamba models fully fine-tuned (i.e., no adapters, all model weights are trained) in full-precision and models fine-tuned using mixed-precision and/or PEFT adapters. We focus on utilizing LoRA , which is arguably the most widely used PEFT framework for LLMs.

   Model & M & P & M & P & M & P & OLMo & M & P & Phi & M & P \\ Size & 130m & 160m & 370m & 410m & 790m & 1b & 1.4b & 1.5b & 2.8b \\  FP16 \(\) & 0.03 & 0.35 & 0.05 & 0.06 & 0.21 & 0.05 & 0.04 & 0.04 & 0.07 & 0.03 & 0.15 & 0.12 \\ BF16 \(\) & 0.05 & 1.45 & 0.20 & 0.20 & 0.66 & 0.16 & 0.13 & 0.31 & 0.13 & 1.05 & 1.17 & 0.11 \\   

Table 2: Mean full-precision (FP32) divergence in MMLU performance for mixed-precision inference. Divergence is averaged over {0, 1, 3, 5}-shot performance. Pretrained checkpoints are used for Mamba (M), Pythia (P), OLMo , and Phi-1.5  (Phi) models.

Using the Alpaca dataset , Mamba 160M, 410M, and 790M models are fine-tuned for three epochs with a maximum sequence length of 512. We denote the targeting of all linear layers (ALL) for LoRA as _ALL LoRA_, the targeting of a subset of linear layers (SLL) for LoRA as _SLL LoRA_, and no adapters as _Full_ (i.e., full fine-tuning). Both ALL and SLL LoRA adapt the large memory buffer described in Theorem 1.

Each fine-tuning run occurred on a single A10G GPU. To further limit extraneous numerical effects, the same batch size is used for all FP32, FP16, and BF16 experiments for a given model size. While this leads to hardware underutilization (i.e., non-saturated GPU memory for mixed-precision and LoRA experiments), this is necessary to guarantee no divergence is due to differences in parameter update schedules. For comparison, Pythia 160M, 410M, and 1B models are fine-tuned using the same experimental setup. The training recipe for all models was adapted from , with the AdamW_torch optimizer and a cosine annealing schedule. Further experimental details are available in Appendix C.

For each Mamba and Pythia model, Figure 1 shows the mean divergence calculated between the respective FP32 Full and mixed-precision ALL/SLL LoRA fine-tuned models, averaged over {0, 1, 3, 5}-shot MMLU accuracy. Across mixed-precisions and adapter settings, Mamba displays comparable divergences to Pythia models. E.g., for FP16, Mamba demonstrates an average divergence of 0.1, compared to 0.14 for Pythia. Similarly, for BF16, Mamba demonstrates an average divergence of 0.18, compared to 0.28 for Pythia. Importantly, Mamba models do not exhibit large deviation spikes after fine-tuning (in contrast to Pythia models).

**Hardware throughput and memory-utilization improvements**. With comparable divergences to Transformers and stable dynamics, we show that MPFT and PEFT may be used to significantly increase GPU-training throughput for Mamba SSMs. To demonstrate such improvements, we utilize the previous fine-tuning settings for the Alpaca dataset. However, we now adjust the batch size to maximize throughput per MPFT and PEFT configuration.

For each MPFT and PEFT configuration, the _average tokens-per-second_ (ATPS) is calculated as the total tokens used for fine-tuning divided by total training time, and the _maximum memory-per-token_ (MMPT) is calculated as the maximum GPU memory utilization incurred (over the entire fine-tuning run) divided by the total number of tokens in each mini-batch. Results are plotted in Figure 6.

Both throughput and memory utilization improve as the number of Mamba parameters increases in Figure 6. **Compared to the full-precision full fine-tuning of Mamba** 790M (the largest model supported by an A10G's memory capacity), evaluated **MPFT and PEFT combinations result in an average 2.15 times more training tokens-per-second while reducing per-token memory utilization by an average 62.7%.** Across all model sizes, evaluated MPFT and PEFT combinations result in an average 1.74 times more training tokens-per-second while reducing per-token memory utilization by an average 47.2% compared to respective full-precision fine-tuned runs.

### Fine-tuning narrows the ICL gap between Mamba and Transformers

We next explore how MPFT and PEFT affect Mamba ICL performance. All Mamba pretrained models are instruction fine-tuned using ALL LoRA and the OpenHermes dataset  (which consists of 242,000 supervised samples). We use the training recipe of , which includes BF16 utilization.

Figure 1: Mean full-precision (FP32) divergence in MMLU performance for Mamba and Pythia models. Models are fine-tuned over the Alpaca dataset  using different combinations of MPFT and PEFT. Full fine-tuning (i.e., no PEFT adapters) is denoted as Full.

Performance is evaluated using the datasets from Table 1-HellaSwag , PIQA , Arc-E , Arc-C , and WinoGrande -and report the _average improvement percentage_ of {1, 3, 5}_-shot_ versus 0-_shot_ (AIPSS). For comparison, Pythia pretrained models are instruction fine-tuned using the same training recipe and ALL LoRA (i.e., all Pythia linear layers are adapted).

Figure 3 displays AIPSS for pretrained and instruction fine-tuned Mamba and Pythia models. As previously noted, pretrained Mamba models do not display similar ICL ability as comparable Pythia models on the evaluated standard NLP benchmarks. In particular, Mamba 2.8B, the largest pretrained Mamba model, displays inconsistent zero-shot improvements as the number of shots increase. However, after fine-tuning, all Mamba models larger than Mamba 130M consistently improve in ICL performance as the number of shots increase. Compared to Mamba pretrained models, which are only capable of 38% of the AIPSS compared to similar pretrained Pythia models, fine-tuned ALL LoRA Mamba models are capable of 81.5% of the AIPSS compared to similarly fine-tuned Pythia models.

**Fine-tuning robustness**. We show that Mamba is robust to the choice of PEFT hyperparemters. We conduct an extensive hyperparameter search across the learning rate, LoRA dimension, and number of warmup steps. From the Cartesian-product of these three parameters, 150 hyperparameter configurations were sampled and used to fine-tune Mamba 370M over the Openhermes dataset. For comparison, Pythia 410M is similarly fine-tuned using the same set of 150 hyperparameter configurations.

Figure 3: Fine-tuning narrows the ICL gap between Mamba and Pythia. ALL LoRA models were instruction fine-tuned on the OpenHermes  dataset for one epoch. Performance is reported as the average improvement percentage of {1, 3, 5}-shot versus 0-shot over five standard benchmarks.

Figure 2: Timing and memory usage calculated Mamba model-sizes and PEFT combinations. Each model was trained using the Alpaca dataset  dataset for three epochs and maximum sequence length 512. For each PEFT combination, the batch size was tuned to maximize GPU occupancy.

The MMLU 5-shot performance for each of the 150 Mamba and Pythia fine-tuned models is displayed in 6.1. Pythia 410M is capable of higher performance than Mamba 370M, where the average accuracy for the former and the latter are 26.5% and 24.8%, respectively. However, Mamba 370M is much more robust to the choice of hyperparameters, with a difference of 1.5% between the minimum (23.3%) and maximum (24.8%). In contrast, Pythia 410M fine-tuned models display a large performance difference of 4.7% between the minimum (22.9%) and maximum (27.6%).

## 7 Discussion

We've extensively explored Mamba's downstream learning capabilities. Using dynamical systems theory, we've shown that Mamba's recurrent dynamics are robust to small input perturbations (contrary to the current understanding of Mamba's recurrent sensitivities). We've extensively confirmed this result, showing that: a) Mamba inference is robust to changes due to mixed-precision, (b) Mamba inference differences due to mixed-precision align with Transformers, (c) Mamba fine-tuning is robust to changes due to mixed-precision and PEFT, and (d) differences in downstream performance for Mamba due to MPFT and PEFT can be more robust than Transformers. Using both MPFT and PEFT, we've shown that instruction fine-tuning Mamba SSMs greatly narrows the previously observed ICL gap, going from only 38% (post pretraining) up to 81.5% (post fine-tuning) of the ICL abilities of similar Transformers. Furthermore, we've shown that combining MPFT and PEFT can more than halve training time and nearly triple memory efficiency for Mamba models.

There are significant avenues for future work. In particular, adapting Mamba's CUDA kernels to support more aggressive low-precision PEFT methods  would further decrease the hardware needed to train Mamba models, while providing additional speedups. Furthermore, while the largest pure Mamba model contains 2.8B parameters, the training speedups and improved memory utilization described herein may be applied to more efficiently pretrain larger pure Mamba SSMs (e.g., 7B parameters and greater), where Mamba models may better manifest emergent abilities previously displayed by Transformers (or even manifest previously unobserved abilities).

**Limitations.** While we explored the use of LoRA for Mamba models, many other PEFT adapters exist . Furthermore, while mixed-precision using FP16 and BF16 were explored, lower-precision methods exist  (which may be enabled by adapting Mamba's highly customized CUDA kernels). Both are interesting directions for future work. Finally, our timing and memory usage experiments using Alpaca did not consider the largest two Mamba models (1.4B and 2.8B) due to their exceeding A10G memory capacity for FP32 full fine-tuning.

**Broader Impact.** The Mamba models considered are all LLMs, and thus have the same potential positive and negative societal impacts as other LLMs (e.g., hallucinations). Furthermore, fine-tuning is known to possibly erode existing LLM guardrails, and thus our methods may be adapted for this fine-tuning use case (as is the case for all PEFT and MPFT methods). However, our work improves the quality of Mamba models for downstream applications, which may be adapted for all positive LLM applications in society (e.g., personal assistants, task automation, code completion, etc.). Finally, our work decreases the computational constraints required to train and inference Mamba SSMs, which has implications for green ML (e.g., decreased CO2 emissions, positive climate change impact, etc.). 410 GPU days were used to produce the results for this paper.

Figure 4: Fine-tuning hyperparameter search for OpenHermes. Each point is a different hyperparameter configuration. SLL LoRA was used for both models. The \(x\)-axis is the learning rate, the \(y\)-axis is resulting MMLU 5-shot performance, bubble size is the LoRA dimension, and the color is the number of warmup steps \(\{0,1,2\}\).