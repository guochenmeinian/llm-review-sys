# Exploring the Edges of Latent State Clusters for Goal-Conditioned Reinforcement Learning

Yuanlin Duan

Rutgers University

yuanlin.duan@rutgers.edu

&Guofeng Cui

Rutgers University

gc669@cs.rutgers.edu

&He Zhu

Rutgers University

hz375@cs.rutgers.edu

###### Abstract

Exploring unknown environments efficiently is a fundamental challenge in unsupervised goal-conditioned reinforcement learning. While selecting exploratory goals at the frontier of previously explored states is an effective strategy, the policy during training may still have limited capability of reaching rare goals on the frontier, resulting in reduced exploratory behavior. We propose "Cluster Edge Exploration" (CE\({}^{2}\)), a new goal-directed exploration algorithm that when choosing goals in sparsely explored areas of the state space gives priority to goal states that remain accessible to the agent. The key idea is clustering to group states that are easily reachable from one another by the current policy under training in a latent space and traversing to states holding significant exploration potential on the boundary of these clusters before doing exploratory behavior. In challenging robotics environments including navigating a maze with a multi-legged ant robot, manipulating objects with a robot arm on a cluttered tabletop, and rotating objects in the palm of an anthropomorphic robotic hand, CE\({}^{2}\) demonstrates superior efficiency in exploration compared to baseline methods and ablations.

## 1 Introduction

In recent years, Goal-Conditioned Reinforcement Learning (GCRL) (Andrychowicz et al., 2017) has emerged as a powerful paradigm for training agents to accomplish diverse tasks in complex and dynamic environments. GCRL enables agents to learn goal-directed behaviors, allowing them to achieve specific objectives in a flexible and adaptive manner. However, a central challenge in GCRL lies in guiding agents to effectively explore their environment during training. The exploration problem in GCRL can be viewed as the task of setting goals for the agent during training to guide the agent's environment navigation to collect exploratory data that improves its learning process. In this paper, we address this critical challenge by proposing a novel strategy for selecting exploration-inducing goals in GCRL.

Because goal-conditioned policies excel at reaching states encountered frequently during training, a simple strategy is setting goals in less-visited areas of the state space to broaden the range of reachable states. However, throughout training, goal-conditioned policies may encounter difficulties in reaching arbitrary goals. For example, when instructed to navigate to an unexplored section of a maze environment, a novice agent might instead revisit a previously traversed area that provides low exploration value. To address this shortcoming, the environment exploration procedure must set up additional mechanisms to filter out unreachable goals. A common strategy in the literature is to select goals at the frontier of previously explored states and launch an exploration phase immediately after these goals are achieved, adhering to a Go-Explore principle (Ecoffet et al., 2019). For example, Skewfit (Pong et al., 2019) estimates state densities and selects goals at the frontier from the replay buffer in inverse proportion to their density. Similarly, MEGA (Pitis et al., 2020) uses kernel density estimates (KDE) of state densities and selects frontier goals with low density from the replay buffer.

However, precisely identifying the frontier of known states can be challenging with these heuristics. Even once the frontier is identified, the policy during training may still have limited capability of reaching rare goals on the frontier, resulting in reduced exploratory behavior.

To address the aforementioned challenge, we propose a new goal-directed exploration algorithm, CE\({}^{2}\) (short for "Cluster Edge Exploration"). When choosing goals in sparsely explored areas of the state space, CE\({}^{2}\) gives priority to goal states that remain accessible to the agent. For this purpose, our key idea is clustering to group known states that are easily reachable from one another by the current policy under training, and traversing to states holding significant exploration potential on the boundary of these clusters before doing exploratory behavior. In this way, our method accounts for the capability of the current policy for exploratory goals. First, a state cluster likely represents part of the state space where the training policy is familiar with. Second, given the easy accessibility of states within each cluster by the training policy, the agent's capability extends to reaching states even at cluster boundaries. Moreover, less explored regions naturally reside adjacent to the periphery of state clusters. This Go-Explore strategy enables the agent to progressively broaden the coverage of each state cluster to effectively explore a novel environment. We instantiate CE\({}^{2}\) in the context of model-based GCRL, demonstrating how learned world models can facilitate clustering environment states that are easily reachable from one another by the training policy in a latent space. We validate the effectiveness of CE\({}^{2}\) in challenging robotics scenarios, including navigating a maze with a multi-legged ant robot, manipulating objects with a robot arm on a cluttered tabletop, and rotating objects in the palm of an anthropomorphic robotic hand. In each scenario, CE\({}^{2}\) exploration results in more efficient training of adaptable GCRL policies compared to baseline methods and ablations.

## 2 Problem Setup and Background

Our work focuses on the exploration problem in unsupervised goal-conditioned reinforcement learning (GCRL) settings. In this section, we set up notation and preliminary concepts.

**GCRL.** A goal-conditioned Markov decision process (MDP) is defined by the tuple (\(S\), \(A\), \(G\), \(T\), \(\)) where the state space \(S\) defines the set of all possible agent's observations into the environment, the action space \(A\) defines all possible actions that the agent can take in each state, \(G\) is the set of all possible goals that the agent may aim to achieve in the environment, and the transition function \(T\) describes the probability of transitioning from one state to another given an action. It is defined as \(T(s^{}|s,a)\), where \(s^{} S\) is the next state, \(s S\) is the current state, and \(a A\) is the action taken. \(:S G\) is a tractable mapping function that maps a state to a specific goal. A goal-conditioned \((a|s,g)\) represents the agent's strategy for selecting actions based on states and goal commands, indicating the probability of taking action \(a\) in state \(s\) given goal command \(g G\). In this paper, for ease of presentation, we assume \(S=G\) and \(\) is an identify function.

Our goal is to develop agents capable of unsupervised exploration when dropped into an unknown environment. During the unsupervised exploration stage, there are no predefined tasks or goals. The agent sets its own goal command \(g G\) as it explores the environment. Following this exploration phase, a successful agent should be able to navigate to a wide range of previously unknown goal states in the environment upon goal commands.

**Model-based GCRL.** Model-based reinforcement learning (MBRL) is an approach where an agent learns a model of the environment's dynamics to predict future states, enabling more efficient policy learning. Fig. 1 shows the general MBRL framework. We use the world model structure \(\) of Dreamer (Hafner et al., 2019, 2020, 2023) to learn real environment dynamics as a recurrent state-space model (RSSM). We provide a detailed explanation of the network architecture and working principles of the RSSM in Appendix C.1. Particularly, we consider **GC-Dreamer** (goal-conditioned Dreamer) as a baseline. In GC-Dreamer, the goal-conditioned agent \(^{G}(a|s,g)\) samples goal commands \(g G\) from a given environment goal distribution \(p_{g}\) to collect trajectories in the real world. These trajectories are used to train the world

Figure 1: Model-based GCRL Framework

model \(\), and subsequently, \(^{G}\) is trained on imagined rollouts generated by \(\), with these two steps run in alternation. The reward function used to train \(^{G}\) is determined by a temporal distance network \(D_{t}\) (see below).

**Go-Explore**. In unsupervised GCRL, the goal distribution \(p_{g}\) is only revealed at test time. "Go-Explore" (Ecoffet et al., 2019; Pislar et al., 2021; Tuyls et al., 2022; Hu et al., 2023) is a popular mechanism tailored for long-term GCRL scenarios that require extensive exploration. The Go-Explore methodology splits each training episode into two distinct phases: the "Go-phase" and the "Explore-phase". In the "Go-phase", the agent is guided to an "interesting" goal \(g\)(Pong et al., 2019; Pitis et al., 2020) (e.g., states rarely encountered in the replay buffer) by the GCRL policy \(^{G}\), reaching a final state \(s_{T}\). Subsequently, the "Explore-phase" kicks in, with an undirected exploration policy \(^{E}\) taking over from \(s_{T}\) for the remaining timesteps. This exploration policy is optimized to maximize an intrinsic exploration reward (Bellemare et al., 2016; Pathak et al., 2017; Burda et al., 2018; Sekar et al., 2020) (e.g., to explore less familiar areas of the environment that the world models have not adequately learned).

Recently, Go-Explore has been integrated with model-based unsupervised GCRL (Mendonca et al., 2021; Hu et al., 2023), as depicted in Fig. 1. In addition to the goal-conditioned policy \(^{G}(a|s,g)\), an exploration policy \(^{E}(s)\) is introduced into the model-based GCRL framework. The agent's training process involves learning the following components:

\[ (s_{t}|s_{t-1},a_{t-1})\] Exploration policy: \[^{E}(s_{t})  ^{G}(s_{t},g)\] Exploration value: \[V^{E}(s_{t})  V^{G}(s_{t},g)\]

where both \(^{G}\) and \(^{E}\) are trained using the model-based actor-critic algorithm in Dreamer (Hafner et al., 2020). They are entirely trained with the imagined rollouts of the world model \(\) to maximize the accumulated rewards \(_{t}r_{t}^{G}\) and \(_{t}r_{t}^{E}\), respectively. The explorer reward \(r^{E}\) encourages exploration by leveraging the Plan2Explore (Sekar et al., 2020) disagreement objective, which motivates the agent to seek states that induce discrepancies among an ensemble of world models. In contrast, the goal-reaching reward \(r^{G}\) is driven by the self-supervised temporal distance objective \(D_{t}\)(Mendonca et al., 2021), which reinforces the policy to minimize the action steps required to transition from the current state \(s\) to a sampled goal state \(g\) in an imagined rollout, i.e., \(r^{G}(s,g)=-D_{t}((s),(g))\). The temporal distance network \(D_{t}\) predicts the anticipated number of action steps needed to transition from \(s\) to \(g\). It is trained by extracting pairs of states \(s_{t}\) and \(s_{t+k}\) from an imagined rollout generated by \(\) and predicting the distance \(k\) as shown in Equation 1 where \(H\) is the total length of the imagined rollout:

\[D_{t}(s_{t}),(s_{t+k}) k/H \]

Here, \(\) is a learned function for state embeddings in the world model (we assume \(S=G\) in the paper). Further details on the training procedure of \(D_{t}\) can be found in Appendix C.2.

CE\({}^{2}\) aims to address the core challenge in the Go-Explore mechanism: how do we select an interesting goal command \(g\) at the frontier of known states with high exploration potential and effectively guide the agent to \(g\)?

## 3 State Cluster Edge Exploration

The major limitation in existing Go-Explore approaches, such as those described in Pong et al. (2019); Pitis et al. (2020) is that the policy under training can struggle to reach heuristically chosen rare goals at the frontier of known states (Hu et al., 2023). This difficulty arises because the goal commands are selected without a systematic method to filter out unachievable goals for the agent, leading to diminished exploratory behavior. In CE\({}^{2}\), when choosing goals in sparsely explored areas of the state space in the "Go-phase", our method gives priority to goal states that remain accessible. For this purpose, the key idea is clustering to group states that are easily reachable from one another by the current policy under training in a latent space, and selecting states holding significant exploration potential on the boundary of these clusters as the "interesting" goals to explore. In Sec. 3.1, we discuss how to learn a latent space that can represent the reachability relationships between environment states. In Sec. 3.2, we explain how this latent space can be used to cluster states in the replay buffer that are easily reachable from one another. In Sec. 3.3, we demonstrate how the agent can be brought to interesting states on the boundary of latent state clusters to effectively explore its environment.

### Latent Space Learning

Typically, during the learning process of a world model \(\) as a neural network, an essential step involves encoding states from the original observation space into a latent space using an encoder, which can then be decoded back to the original observation space by a decoder. This latent space is subsequently used to learn the dynamic model of the real environment (Hafner et al., 2019, 2020).

In CE\({}^{2}\), we additionally require the latent space can express the temporal distance between different states. In other words, we aim for the distances between various states in the latent space to represent the number of steps required to transition from one another in the real environment (after decoding) by the training policy. Therefore, the loss function of training the latent space in CE\({}^{2}\) comprises two components. The first component is the reconstruction loss \(_{rec}\), akin to the latent space loss function in Dreamer framework (Hafner et al., 2019, 2020). It captures the association between the latent space and the re-decoding to the observation space, along with predicting dynamic transition in the latent space. We introduce a second loss term \(_{dt}\) that leverages the temporal distance network \(D_{t}\) in Equation 1 to guide the learning of the latent space structure. For any pair of states \((s1,s2)\) sampled from the replay buffer, the \(_{dt}\) loss function is formulated as follows (\(\) is a learned function for state embeddings in the world model):

\[_{dt}=(\|(s_{1})-(s_{2})\|_{2}^{2}-(D_{t}((s _{1}),(s_{2}))+D_{t}((s_{2}),(s_{1}))))^{2} \]

\[_{latent}=_{rec}+_{dt} \]

We use the loss function \(_{latent}\) to supervise the training of the latent space. The trained latent space provides the agent with a deeper understanding of the real environment, where states that are easily reachable from one another in the real environment are closer in proximity within the latent space.

### Latent State Clustering

To identify the frontier of known states, CE\({}^{2}\) conducts state clustering to group states in the replay buffer. States that are easily reachable from one another are classified in the same cluster in the latent space by Gaussian Mixture Models (GMMs), based on the temporal distances between the encoded states. GMMs are probabilistic models that assume all data points are generated by a mixture of a finite number of Gaussian Distributions. We initialize the Gaussian models in the latent space with \(N_{c}\) trainable latent centroids \(c=\{c_{1},,c_{N_{c}}\}\) and a shared variance \(\), where \(N_{c}\) represents the desired number of clusters. These \(N_{c}\) latent centroids are initialized by applying the Farthest Point Sampling (FPS) algorithm (Eldar et al., 1997) to select a representative subset of states from a batch of data sampled from the replay buffer. We provide a detailed description of the FPS algorithm in Appendix G.1. After initialization, we optimize the clustering model by maximizing the Evidence Lower Bound (ELBO) iteratively on sampled batches from the replay buffer with a uniform prior \(p(c)\) to scatter out the latent centroids (Zhang et al., 2021):

\[ p(z=(s))_{q(c|(s))}[ p((s)|c)]-D_{KL}(q(c| (s))||p(c)) \]

where \(p\) and \(q\) are represented as Gaussian distributions within the GMMs. \(q(c|(s))\) is the postior distribution over \(c\) (the clusters) given an encoded state \((s)\). \( p((s)|c)\) is the distribution donating the probability of the encoded state \((s)\) in cluster \(c\). \(p(c)\) is the prior distribution of the weight of clusters in GMMs. For each round of optimization, we increase the probability of the sampled batches in GMMs by updating the weight of each cluster \(c\) in GMMs and the mean and variance of them.

### Exploring the Boundaries of Latent State Clusters

Assuming we have already trained \(N_{c}\) state clusters in the latent space, each representing part of the state space where the goal-conditioned policy under training is familiar with, how can we utilize these state clusters to plan an exploration strategy? CE\({}^{2}\) selects goal states at the edges of these latent state clusters for exploration because (1) less explored regions are naturally adjacent to these boundaries, and (2) given the easy accessibility between states within each cluster by the training policy, the agent's capability extends to reaching states even at the cluster boundaries.

We outline our exploration algorithm in Algorithm 1. At line 3, it samples \(N_{candidate}\) latent states as \(S_{candidate}\) from GMMs. A higher sampling quantity ensures sampling from more states at the edgesof the clusters. We set \(N_{candidate}=1000\) in CE\({}^{2}\). We compute the total probability of each latent state \( S_{candidate}\) in the Gaussian mixture model, given by the formula:

\[p()=_{i=1}^{N_{c}}_{i}(|c_{i},) \]

In this formula, \(_{i}\) are the mixture weights satisfying \(_{i} 0\) and \(_{i=1}^{N_{c}}_{i}=1\), \((|c_{i},)\) represents the \(i\)-th Gaussian distribution with mean \(c_{i}\) and the shared standard deviation \(\). At line 4, we select \(N_{edge}\) latent states with the lowest total probability from \(S_{candidate}\) by Equation 5 as a set \(S_{edge}\). Intuitively, these states reside on the edges of the latent state clusters and, therefore, induce a set of a goal commands \(G_{edge}=\{(f_{D}())| S_{edge}\}\) that may be used for the "Go-phase" for Go-Explore, where \(f_{D}\) is the state decoder and \(\) is the goal mapping function. However, randomly picking a goal command from \(G_{edge}\) overlooks whether the policy can exactly navigate the agent to the sampled goal in the real environment. Although determining the exact outcome of the policy without execution is impractical, similar to PEG (Hu et al., 2023), we can leverage the world model to provide an approximation of the exploration potential \(P^{E}(g)\) of a goal command \(g\):

\[_{^{G}(|,g)()}=p(s_{0})[_{t=1}^{T}(s_ {t}|s_{t-1},a_{t-1})^{G}(a_{t-1}|s_{t-1},g)] \]

\[P^{E}(g)=_{p_{^{G}(|,g)()}}[V^{E}(s_{T})] _{k}^{K}V^{E}(s_{T}^{k})s_{T}^{k} _{^{G}(|,g)()} \]

In Equation 6, we simulate the "Go-phase" of Go-Explore over the world model \(\). We set each state from \(G_{edge}\) as the goal command \(g\) for the goal-conditioned policy \(^{G}\) to run over \(\) and denote \(s_{T}\) as the final state of the resulting imagined trajectory (here \(_{^{G}(|,g)()}\) essentially induces the imagined trajectory distribution over the world model). In our implementation, we set the length of "Go-phase" \(T\) to half of the maximum episode length for all environments. The time limits for both the Go and Explore phases during real environment exploration are also set to this value. We use the learned exploration value function \(V^{E}\) of explorer \(^{E}\) to estimate the exploration value of \(s_{T}^{k}\), the final state of \(k\)-th imagined trajectory. We average the estimated exploration potential over \(K\) such imagined trajectories.

At line 5 in Algorithm 1, after selecting the exploration target \(g^{E}\) with the highest exploration potential \(P^{E}\) from the latent cluster boundaries, we start the Go-Explore procedure in the real environment by executing the goal-conditioned policy \(^{G}\) to approach \(g^{E}\) as closely as possible limited in \(T\) timesteps, followed by launching the explore policy \(^{E}\) for exploration limited in \(T_{E}\) timesteps.

```
1:Input:\(^{G}\), \(^{E}\), World Model \(\), GMM, \(r^{G}\), \(r^{E}\)
2:Initialize replay buffer \(D\)
3:for\(i=1\) to \(N_{train}\)do
4:if Should assign centroids then
5:\(B_{exp} A\) batch of data from \(D_{exp}\)
6: GMM \(\) Choose \(N_{c}\) centroids from \(B_{exp}\) by FPS
7:\(D_{exp}\) Cluster Edge Exploration(...) with Algorithm 1
8:\(D D D_{exp}\)
9: Update \(\) with \(D\) (update latent space by \(_{rec}+_{latent}\))
10: Update GMM with \(D_{exp}\)
11: Update \(^{E}\) in imagination with \(\) to maximize \(r^{G}\)
12: Update \(^{E}\) in imagination with \(\) to maximize \(r^{E}\)
```

**Algorithm 2** The main training algorithm for CE\({}^{2}\)

### The Main Algorithm

We depict the main learning algorithm of CE\({}^{2}\) in Algorithm 2. Recall that the learning objective is to train an agent that can achieve diverse goals revealed to it only at test time. Accordingly, in this algorithm at line 7, the data \(D_{exp}\) collected to train the world model \(\) is generated solely by our Go-Explore strategy as outlined in Algorithm 1. At line 6, we periodically update the centroids of the latent clusters again using the FPS algorithm (Eldar et al., 1997) from a batch of latest trajectories from the replay buffer. This ensures that the candidate goal states selected for exploration are indeed located at the boundaries of key state regions. At line 10, we train the clustering model using data from the replay buffer in each round. This ensures that latent state clustering and the agent's goal-reaching capability are kept synchronized.

In our experiment, we also designed a variant of CE\({}^{2}\) in Algorithm 3, called CE\({}^{2}\)-G. This algorithm is given the environment goal distribution \(p_{g}\) at training time. The main idea is to progressively expand the scope of exploration around the possible trajectories leading to the environment goals. In this algorithm, the replay buffer additionally includes \(D_{egc}\) the trajectories sampled by \(_{G}\) conditioned on the environment goals in \(p_{g}\). We only use \(D_{egc}\) to initialize and train latent state clusters. In this way, the agent is encouraged to prioritize exploration starting from the edges of latent state clusters along the trajectories towards the goal states in \(p_{g}\). CE\({}^{2}\)-G can be considered as learning policies and world models specific to a given goal distribution.

## 4 Experiments

Our experiments evaluate CE\({}^{2}\) over goal-reaching tasks that demand significant exploration to solve. We aim to address the following questions: (1) Does CE\({}^{2}\) lead to improved exploration and goal-reaching performance? (2) How does CE\({}^{2}\) exploration qualitatively differ from those in previous goal-directed exploration methods? (3) Which components of CE\({}^{2}\) are crucial to its success?

### Benchmarks

We evaluate our method on six hard exploration goal-conditioned RL tasks: **Point-Maze**, **Ant-Maze**, **Walker**, **3-Block Stacking**, **Block Rotation** and **Pen Rotation**. **Point-Maze**: A blue point is placed at the bottom left of the maze and be trained to explore the structure of maze. **Ant-Maze**: An ant robot must master intricate four-legged locomotion behaviors and maneuver through narrow hallways. **Walker**: A 2-legged robot needs to learn how to control its leg joints to walk on a flat plane to move forward or backward. In **3-Block Stacking**, a robot arm with a two-fingered gripper operates on a tabletop with three blocks. The goal is to stack the blocks into a tower configuration. The agent needs to learn pushing, picking, and stacking, as well as discovering intricate action paths to accomplish the task within the environment. Previous solutions have relied on methods like demonstrations, curriculum learning, or extensive simulator data, highlighting the task's difficulty. The Gymnasium **Block Rotation** and **Pen Rotation** tasks involve manipulating a block and a pen, respectively, to achieve a random target rotation along all axes. Pen Rotation is particularly challenging due to the pen's thinness, requiring precise control to prevent it from dropping. For evaluation, we use the most challenging goals, such as the farthest goal locations, in Point Maze, Ant Maze, Walker, and 3-Block Stacking. In the other two environments, we utilize random goals as defined by the environment. For more settings and information about the environments, please refer to the Appendix E.

Figure 2: We conduct experiments on 6 environments: Point Maze, Ant Maze, Walker, 3-Block Stacking, Block Rotation, Pen Rotation.

### Baselines

In the unsupervised GCRL setting, we compared **CE\({}^{2}\)** with state-of-the-art methods based on the Go-Explore strategy, which has demonstrated high efficiency in this setup: **PEG**Hu et al. (2023) and **MEGA**Pitis et al. (2020)1. MEGA commands the agent to rarely seen states at the frontier by using kernel density estimates (KDE) of state densities and chooses low-density goals from the replay buffer. PEG selects goal commands to guide an agent's goal-conditioned policy toward states with the highest exploration potential given its current level of training. This potential is defined as the expected accumulated exploration reward during the Explore-phase.

In scenarios where environment goal distributions are available to the agents, we compare CE\({}^{2}\)-G with **GC-Dreamer** (illustrated in Sec. 2), **PEG-G**, **MEGA-G** and **L3P**. Similar to CE\({}^{2}\)-G, PEG-G and MEGA-G augment **GC-Dreamer** with the PEG and MEGA Go-Explore strategies, respectively. In these methods, the replay buffer \(D\) contains not only trajectories sampled by the goal-conditioned policy \(_{G}\) commanded by environment goals but also exploratory trajectories sampled using the corresponding Go-Explore strategies. **L3P** trains a latent space using temporal distances and performs clustering in this latent space, similar to CE\({}^{2}\)-G. However, **L3P** does not employ a Go-Explore strategy. Instead, it constructs a directed graph with cluster centroids as nodes and utilizes online planning with graph search to determine subgoals for task execution.

### Results

**CE\({}^{2}\) Results.** Fig. 3 depicts the mean learning performance of all the unsupervised GCRL tools in terms of the agent's goal-reaching success rate averaged over 5 random seeds. The evaluation goal distribution is revealed to the agent only at test time. In all tasks except PointMaze, CE\({}^{2}\) significantly outperforms PEG and MEGA in terms of both learning performance and learning speed. On PointMaze, CE\({}^{2}\) performs comparably with the baselines. Although MEGA can set goal commands in sparsely explored areas

Figure 4: Comparison of exploration goals (represented as red points) generated by CE\({}^{2}\), MEGA, and PEG in the Ant Maze environment.

Figure 3: Experiment results comparing CE\({}^{2}\) with the baselines over 5 random seeds.

of the state space to encourage exploration, unlike CE\({}^{2}\), it lacks a systematic method to filter out unachievable goals for the agent, which can result in inefficient exploration. Theoretically, PEG can induce more exploration than MEGA because it can sample goal commands as any state within the state space to initiate exploration, including those beyond the frontier of known states in the replay buffer. However, because a learned world model is typically unfamiliar with rarely observed states, it may select goal commands that appear to have high exploration potential in the model but perform poorly in the real environment as shown in Fig. 4.

**CE\({}^{2}\)-G Results.** Fig. 5 depicts the mean learning performance of all the tools in terms of the agent's goal-reaching success rate averaged over 5 random seeds when the environment goal distribution is revealed to the agent at training time. GC-Dreamer is the only tool that lacks a Go-Explore phase, which may limit its exploration potential. Even so, it can sometimes outperform MEGA-G and PEG-G (see block rotation and pen rotation). This indicates that, without reasonably accounting the agent's capability to reach selected goal commands, the Go-Explore strategy does not always guarantee improved exploration. Suboptimal goal-setting during the "Go-phase" can even hinder exploration (see 3 block stacking). Notably, for the challenging 3-block stacking task, CE\({}^{2}\)-G achieves a high success rate exceeding 90%. In comparison, MEGA-G, PEG-G and GC-Dreamer only achieve less than 40% success rates. Refer to Appendix H.4 for full results of CE\({}^{2}\)-G.

### Exploration Process

Fig 6 shows the evolution of state clusters (learned in a latent space) during the training process for Ant Maze (in different colors). The red points represent the selected goal commands used to induce exploration. We observe that the self-directed exploration goals set by CE\({}^{2}\) improve progressively as the agent's capabilities increase, consistently targeting the cluster edges that require further exploration and are within the agent's reach. We compare the exploration targets generated by CE\({}^{2}\) with those produced by the MEGA and PEG approaches throughout the training process in the Ant Maze environment in Appendix H.2.

### Ablation Study

In the ablation experiment, our goal is to determine the individual contributions of each component to our method's overall performance. The "Go-phase" of the Go-Explore procedure in CE\({}^{2}\) consists

Figure 5: Experiment results comparing CE\({}^{2}\)-G with the baselines over 5 random seeds.

Figure 6: Cluster evolution in CE\({}^{2}\) as the training progresses. The red points means the goals picked by CE\({}^{2}\) to explore and other points in different colors represent the clusters CE\({}^{2}\) learned.

of two main steps for selecting a goal command \(g\) to initiate exploration: (a) sampling environment states at the boundaries of its trained latent state clusters, and (b) selecting the goal command \(g\) with the highest exploration potential from the sampled states. Our first ablation, **CE\({}^{2}\)-noPEG**, only performs step (a). It randomly samples \(g\) from the latent state clusters without considering its exploration potential. The second ablation only performs step (b) and is identical to the PEG baseline. It can sample any state within the state space for the goal command \(g\), without the constraint of directing the agent to states at the boundaries of known regions like CE\({}^{2}\) and MEGA. We also include **MEGA** and **MEGA-wPEG** as two baselines to solely compare the exploration strategy-step (a)-in CE\({}^{2}\) with MEGA's strategy to command the agent to rarely seen states. MEGA-wPEG first uses MEGA to sample a batch of candidate goals, all with low density in the replay buffer. Then, their exploration potential is evaluated using PEG (step (b)), and the most valuable one is selected as the exploratory goal. We conduct the ablation experiments in a purely unsupervised setting without revealing any test goals at training time.

Fig. 7 confirms that both step (a) and step (b) in CE\({}^{2}\) are important. CE\({}^{2}\) significantly outperforms CE\({}^{2}\)-noPEG and PEG in 3-block stacking and the Ant maze tasks. Notably, even without step (b), CE\({}^{2}\)-noPEG performs well across all experiments, especially in the challenging block and pen rotation tasks. This indicates that the goal commands sampled at the edges of latent state clusters already possess high exploration potential and can guide the agent to traverse unseen state spaces. The superior performance of CE\({}^{2}\)-noPEG compared to both MEGA and MEGA-wPEG further reinforces this. Block Rotation is the only environment where CE\({}^{2}\)-noPEG outperforms CE\({}^{2}\). In this environment, the CE\({}^{2}\) agent often pursues states where the block falls from the palm, due to their "high" exploration potential determined by the exploration policy value functions. In contrast, CE\({}^{2}\)-noPEG agent explores the state space more evenly, gaining more in-hand manipulation skills, which is crucial for achieving the block-rotation goals revealed at test time. MEGA achieves similar or better performance compared to MEGA-wPEG, indicating that PEG's effectiveness relies on the quality of the candidate goal set. The exploratory goals sampled from the lowest-density regions in the replay buffer might be beyond the agent's capability, leading PEG to assess the true exploration potential of the candidate goals inaccurately.

We also conducted experiments in the CE\({}^{2}\) with different numbers of latent state clusters \(N_{c}\) and observed that CE\({}^{2}\) is insensitive to this hyperparameter. See Appendix H.5 for more discussion.

## 5 Related Work

Our method addresses the challenging and inefficient exploration problem inherent in goal-conditioned reinforcement learning (RL) settings with sparse rewards, commonly used in robotics and control fields (Ghosh et al., 2019; Liu et al., 2022; Plappert et al., 2018). In goal-conditioned RL, agents are trained to achieve various goals based on predefined commands, with rewards typically being binary, indicating positive feedback from the environment only upon reaching the specified goal. This sparse reward setting significantly complicates achieving sample efficiency and effective learning processes (Ren et al., 2019; Florensa et al., 2018; Trott et al., 2019). To mitigate this challenge, various methods have been proposed. Some reshape the sparse reward function into a denser form by incorporating metrics such as distance between achieved and desired goals (Trott et al., 2019) or temporal distance (Hartikainen et al., 2019; Mendonca et al., 2021). Additionally, exploration strategies

Figure 7: Ablation study on the importance of each component of CE\({}^{2}\) over 5 random seeds.

often include rewards aimed at incentivizing visits to states with low visitation frequencies (Bellemare et al., 2016; Burda et al., 2018). These approaches typically involve identifying states with infrequent occurrences within the replay buffer and targeting them for exploration, thus facilitating the discovery of unknown regions in the environment. Furthermore, some research emphasizes the exploration of states with high variance between ensemble predictions of future states (McCarthy et al., 2021; Oudeyer et al., 2007; Pathak et al., 2017; Henaff, 2019; Shyam et al., 2019; Sekar et al., 2020).

In addition to reshaping the exploration reward function, goal-directed exploration represents a widely employed strategy that sets exploration goals distinct from the final task objective. Essentially, this approach aims to select goals that present challenges to the current policy while remaining achievable. Prior works have proposed various methods to generate goals for goal-directed exploration. Zhang et al. (2020) proposed to do automatic curriculum generation of goals based on the epistemic uncertainty of value functions. Florensa et al. (2018) use generative adversarial training to automatically generate goals, leveraging goal difficulty as a guiding factor. Pong et al. (2019) and Pitis et al. (2020) proposed to use the maximum entropy of achieved goal distribution to guide goal selection. Ecoffet et al. (2019) introduce a more efficient exploration methodology known as Go-Explore. This approach initially employs the goal-conditioned policy (Go-phase), followed by the rollout of the exploration policy from the terminal state of the goal-conditioned phase (Explore-phase). Go-Explore facilitates exploration initiation from a state area accessible by the current capabilities of the goal-conditioned policy.

PEG (Hu et al., 2023) proposes computing the exploration potential by simulating Go-Explore trajectories using a world model to identify goals characterized by elevated average exploration rewards in the Explore-phase. This metric incorporates anticipated exploration rewards of the Explore-phase, providing an advantage for Go-Explore. However, the goals sampled for evaluating this exploration potential metric in PEG are drawn from a distribution updated by the MPPI method (Williams et al., 2015; Nagabandi et al., 2020) directly in the observation space. L3P (Zhang et al., 2021) employs temporal distance to train a latent space, facilitating clustering within this space to delineate key state areas based on reachability. Our approach proposes exploration from the periphery of these key state regions, aiming to balance exploration of unknown territories while constraining exploration starting points to the edges of key state regions, thus avoiding meaningless exploration from widely sampled points from observation space. See Appendix A, B for more related work discussion.

## 6 Conclusion

We present CE\({}^{2}\), a novel Go-Explore mechanism designed to tackle hard exploration problems in unsupervised goal-conditioned reinforcement learning tasks. While CE\({}^{2}\) outperforms prior exploration approaches in challenging robotics scenarios, the requirement to learn state clusters to identify frontier states and the reliance on world models to determine exploration potential introduce nontrivial computational costs. Exploring whether CE\({}^{2}\)'s Go-Explore strategy can be effectively applied to model-free GCRL settings remains an interesting avenue for future work.

## Reproducibility Statement

The codebase of CE\({}^{2}\) is provided on [https://github.com/RU-Automated-Reasoning-Group/CE2](https://github.com/RU-Automated-Reasoning-Group/CE2). For hyperparameter settings and the baseline pseudocode, please refer to Appendix F and G.3.