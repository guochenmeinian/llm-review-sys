# Diffusion Representation for Asymmetric Kernels via Magnetic Transform

Mingzhen He

Institute of Image Processing and Pattern Recognition

Shanghai Jiao Tong University, Shanghai, China

mingzhen_he@sjtu.edu.cn

Fan He

Department of Electrical Engineering (ESAT-STADIUS)

KU Leuven, Leuven, Belgium

fan.he@esat.kuleuven.be

Ruikai Yang, Xiaolin Huang

Institute of Image Processing and Pattern Recognition

Shanghai Jiao Tong University, Shanghai, China

{ruikai.yang, xiaolinhuang}@sjtu.edu.cn

###### Abstract

As a nonlinear dimension reduction technique, the diffusion map (DM) has been widely used. In DM, kernels play an important role for capturing the nonlinear relationship of data. However, only symmetric kernels can be used now, which prevents the use of DM in directed graphs, trophic networks, and other real-world scenarios where the intrinsic and extrinsic geometries in data are asymmetric. A promising technique is the magnetic transform which converts an asymmetric matrix to a Hermitian one. However, we are facing essential problems, including how diffusion distance could be preserved and how divergence could be avoided during diffusion process. Via theoretical proof, we successfully establish a diffusion representation framework with the magnetic transform, named _MagDM_. The effectiveness and robustness for dealing data endowed with asymmetric proximity are demonstrated on three synthetic datasets and two trophic networks.

## 1 Introduction

The diffusion map (DM)  and its variations  are typical dimension reduction (DR) techniques that has been widely used in the signal processing , computer vision , and textual network embedding . DM is a powerful method that embeds high-dimensional data into a low-dimensional manifold while preserving diffusion distance, which helps describe meaningful geometric structures in data.

Kernels play a significant role in DM and strongly impact the quality of DR, as they allow for the identification of nonlinear relationships and underlying structures in data. In real-world scenarios such as directed graphs, trophic and social networks, the intrinsic and extrinsic geometries in data may be asymmetric, and asymmetric kernels  are more suitable to depict asymmetric geometric structures. The advantages of integrating diffusion geometry and asymmetric kernels have been acknowledged by several researchers.  developed a framework to study function extension and approximation problems in the context of changing data and functions on directed graphs.

However, the diffusion distance and diffusion maps for vertices on directed graphs were unclear.  proposed the diffusion map for asymmetric kernels using the singular value decomposition, while the diffusion distance was calculated using only left singular vectors and right singular vectors were neglected, resulting in the underutilization of asymmetric information.

The magnetic transform (MT)  is a promising technique that transforms an asymmetric matrix into a Hermitian one, making it suitable for working with asymmetry. MT has been successfully applied in DR techniques based on asymmetric adjacency matrices on directed graphs, such as Laplacian eigenmaps [18; 19]. Nevertheless, connecting DM and MT still faces essential problems. Previous works have analyzed MT based on finite adjacency matrices of directed graphs, while the studies of applying MT on asymmetric kernel functions, the corresponding integral operators, and diffusion geometry have not been explored, which obstructs the application of MT to DM. Additionally, MT transforms real-valued asymmetric kernels that describe the connectivity between data into complex-valued function spaces, raising the question of how to define the diffusion distance and diffusion maps in those complex-valued function spaces. It is also crucial to investigate how to prevent divergence during the diffusion process when we are working in complex field. To address these challenges, we propose a novel diffusion representation framework with the magnetic transform, named _MagDM_, that enables the use of asymmetric kernels.

The contributions of this paper are as follows:

* We develop a diffusion representation framework for data endowed with asymmetric kernels using the magnetic transform, where the corresponding diffusion distance and diffusion maps are defined.
* We explore integral operators of MT, whose compactness is established if the asymmetric kernels are Hilbert-Schmidt kernels, helping us determine the range of usable asymmetric kernels.
* We prove an important property that the spectral radius of the proposed integral operator is 1, which ensures that MagDM will not diverge during the diffusion process.
* We present experiments that demonstrate the effectiveness and robustness of the MagDM algorithm on three synthetic datasets and two real-world trophic networks.

## 2 Related works

Dimension reduction techniques [20; 21; 22] have been extensively studied over the past few decades. Previous research has focused mainly on symmetric and positive semi-definite proximity. However, in recent years, researchers have become aware of benefits of asymmetric proximity and have proposed several insightful theories and algorithms. These methods fall into one of the following families.

**Singular value decomposition.** presented a polar decomposition for the construction of kernels to embed directed graphs into manifolds to study harmonic analysis of functions on directed graphs. However, the diffusion representation for asymmetric kernels under this framework was not discussed.  utilized the singular value decomposition (SVD) for asymmetric kernels, and defined the diffusion distance for asymmetric kernels. There was a theoretical shortcoming in which diffusion distance was calculated using only left singular vectors and right singular vectors were neglected, meaning that only out-going proximity was considered, which resulted in the underutilization of asymmetric information. Although SVD embeds asymmetric kernels as an inner product of two feature mappings, it does not directly reflect the directional information, i.e., the skew-symmetric part, which is more important in asymmetric scenarios, in the feature mappings.

**Magnetic Laplacian.** The magnetic Laplacian  was originally derived from a quantum mechanical Hamiltonian of a particle under magnetic flux. [18; 19; 23] utilized magnetic Laplacian to embed asymmetric proximity as a complex-valued feature mapping, where symmetric and skew-symmetric parts were successfully separated into modules and phases, respectively. Magnetic Laplacian transformed an asymmetric matrix to a normalized Hermitian matrix \(\) whose eigenvalues are contained within the interval \([-1,1]\). In Laplacian-typed methods, the feature spaces turned to be the (numerical) null spaces of \(-\), in which only the positive part of the spectrum of \(\) was used. However, the negative part of the spectrum also contains meaningful information which was ignored in previous works [18; 19], resulting in an inadequate use of asymmetric information.

## 3 Diffusion representation and diffusion distance

In this section, we briefly review the diffusion map. Assume \((X,)\) is a \(\)-finite measure space where \(X^{M}\) is a dataset and \(\) is an associated measure which is non-degenerate on \(X\). Let \(:X X_{+}\) be a non-negative symmetric kernel which measures the local connectivity between two samples \(, X\). And the Markov kernel is defined as follows,

\[(,):=(,)}{)})}}\,, \]

where \(()\) is the volume form defined as \(()=_{X}(,)()\). Assuming that the volume form does not vanish and \( L^{2}(X X,)\) is a square integrable measurable function, then the operator \(A:L^{2}(X,) L^{2}(X,)\) given by \(A(f)()=_{X}(,)f()()\) is compact and self-adjoint. We can write \((,)=_{n}_{n}_{n}()_{n}( )\) according to the spectral theorem, where \(\{_{n},_{n}\}\) is the spectral decomposition of the operator \(A\). For any positive natural number \(t_{+}\), the diffusion distance at time \(t\) between two samples \(,\) can be defined as follows,

\[D^{t}(,):=\|^{t}(,)-^{t}(,)\|_{L^{2}( X,)}\,, \]

where \(^{t}\) is the kernel of the \(t\) powers of the integral operator \(A\). Note that the diffusion distance \(D^{t}\) is an average over all paths in time \(t\) connecting \(\) to \(\), resulting in a diffusion distance that is robust to noise perturbation. The diffusion distance can be reformulated as follows,

\[D^{t}(,)=}_{n}^{2t}_{n} ()-_{n}()^{2}}\,.\]

The expression above enables dimension reduction in diffusion geometry, which is also known as diffusion maps. The dataset with the kernel structure \(\) can be embedded into a lower dimensional space \(_{k}^{t}:X^{k}\) as follows,

\[_{k}^{t}()=_{1}^{t}_{1}(),_{2}^{ t}_{2}(),,_{k}^{t}_{k}()^{ }\,.\]

## 4 Diffusion representation for asymmetric kernels

The definition (2) of the diffusion distance assumes symmetry, making it unsuitable for use with asymmetric kernels. To overcome this limitation, we introduce the magnetic transform in this section. With this transformation, we propose the MagDM algorithm for asymmetric kernels.

### Magnetic transform

Assume that we have a set of data \(X^{M}\), and a non-negative distribution \(\) defined on this set. Let \(:X X_{+}\) be a non-negative kernel that measures the directed connectivity between samples in \(X\). Note that \(\) does not need to be symmetric. The classical diffusion distance (2) fails to measure asymmetric distance from \(\) to \(\) because it only uses half of asymmetric information, treating both samples as sources. To handle asymmetry, the magnetic transform is defined to transform an asymmetric kernel to a new kernel with conjugate symmetry.

Suppose an asymmetric kernel \(:X X_{+}\) is a Hilbert-Schmidt kernel, \( L^{2}(X X,)\). The Magnetic transform kernel \(^{(q)}\) of \(\) is defined by

\[^{(q)}(,)=(,)+( ,)}{2}2 q(, )-(,)\,, \]

where \(q^{+}\) is a scaling parameter and \(=\). The kernel \(^{(q)}\) is conjugate symmetric, meaning that \(^{(q)}(,)=^{(q)}(,)}\). The modulus and phase terms of \(^{(q)}\) capture the symmetric and skew-symmetric parts of information in \(\), respectively.

**Proposition 1**.: _Suppose a real-valued asymmetric kernel \(:X X_{+}\) is a Hilbert-Schmidt kernel \( L^{2}(X X,)\) where \((X,)\) is a \(\)-finite measure space and \(X^{M}\). Then, the magnetic transform kernel \(^{(q)}:X X\) is a Hilbert-Schmidt kernel._The proof is shown in Appendix A. The aforementioned proposition allows us to define a Hilbert-Schmidt kernel \(^{(q)}\), which enables us to identify the range of usable asymmetric kernels. Then, we can define the corresponding Hilber-Schmidt operator that is also a compact operator  in the diffusion process.

### Diffusion maps and diffusion distance

The Markov kernel \((,,q) L^{2}(X X,)\) for the magnetic transform kernel \(^{(q)}\) is defined by

\[(,,q)=^{(q)}(,)}{)} )}}\,, \]

where \(m()=_{X}(,)+( {y},)()\) is the volume form of the symmetric term of \(\). Assuming that \(m()\) does not vanish on \(X\), the operator \(T^{(q)}:L^{2}(X,) L^{2}(X,)\) given by

\[T^{(q)}(f)()=_{X}(,,q)f()() \]

is compact and self-adjoint. We can decompose \((,,q)=_{n}_{n}^{(q)}_{n}^{(q)}( {x})_{n}^{(q)}()\) according to the spectral theorem, where \(\{_{n}^{(q)},_{n}^{(q)}\}\) is the spectral decomposition of the operator \(T^{(q)}\). Next, we analyze the spectrum of the integral operator (5).

**Proposition 2**.: _Given an asymmetric kernel satisfying Proposition 1, the eigenvalues \(\{_{i}^{(q)}\}\) of the Hilbert-Schmidt operator defined by (5) are real and bounded by \(_{i}^{(q)}[-1,1]\)._

The proof is provided in Appendix B. Because the operator is self-adjoint, its eigenvalues are real numbers and eigenfunctions are complex functions. According Proposition 2, as diffusion time \(t\) increases, small eigenvalues converge to zero, which avoids divergence of MagDM during diffusion process. For any time \(t\), MagDM defines the diffusion distance for the asymmetric kernel by

\[D^{t}(,):=\|^{t}(,,q)-^{t}(,,q)\|_{L ^{2}(X,)}\,. \]

Running along the diffusion process for asymmetric kernels is equivalent to computing the powers of the operator \(T^{(q)}\). The diffusion distance (6) is a functional weighted \(L^{2}\) distance between complex-valued proximities of two samples, thus it is also robust to noise perturbation. Both symmetry and skew-symmetry are embedded into the diffusion distance, which reflects the asymmetric geometry in data. (6) can be rewritten as the following,

\[D^{t}(,)=}_{n}^{(q)} ^{2t}_{n}^{(q)}()-_{n}^{(q)}()^{2}}\,. \]

Denote the eigenvalues in descending order as: \(1|_{1}^{(q)}||_{2}^{(q)}| 0\). \(D^{t}(,)\) can be approximated by \(D^{t}_{}(,)\) with a preset accuracy \((0,1)\) and finite terms as follows,

\[D^{t}_{}(,)=_{n} ^{(q)}^{2t}_{n}^{(q)}()-_{n}^{(q)}()^ {2}}\,,\]

where \(s(,t)=\{n_{+}:|_{n}^{(q)}|^{t}>|_{1 }^{(q)}|^{t}\}\). We define the diffusion map for asymmetric kernels which embeds data endowed with asymmetric kernels in a lower dimensional space as follows,

\[^{t,(q)}()=[_{1}^{(q)}^{t}_{1}^{(q)}( ),_{2}^{(q)}^{t}_{2}^{(q)}(), ,_{s(,t)}^{(q)}^{t}_{s(,t)}^{ (q)}()]^{}. \]

**Proposition 3**.: _The diffusion map \(^{t,(q)}\) embeds the data endowed with an asymmetric kernel \(\) into a lower dimensional space \(^{s(,t)}\), then the Euclidean distance in this space is equal to the diffusion distance \(D^{t}_{}(,)\),_

\[\|^{t,(q)}()-^{t,(q)}()\|_{2}=D^{t}_{}(,)\,.\]

When the fed kernel function is symmetric, the proposed method reverts back to the classical diffusion map. This is because \(2 q(,)-( ,)\) becomes \(2 q 0=1\), and \(^{(q)}(,)=(,)\). The implementation details of this algorithm can be found in Appendix D.

### Selection of the scaling parameter \(\).

The scaling parameter \(q\) is a key parameter for embedding the skew-symmetric part of an asymmetric kernel, which directly affects the representation ability. The selection of \(q\) has been previously discussed for unweighted directed graphs [18; 23] and in the context of graph signal processing . However, the selection of \(q\) for asymmetric kernel functions has not been addressed so far. Here, we propose a simple and practical method for selecting \(q\) for asymmetric kernels. In (3), the skew-symmetry is embedded in the phase, which is a periodic function controlled by \(q\). To select a suitable value for \(q\), we recommend selecting a period that covers the extent of the skew-symmetric component, which means that \(\) should be less than \(\) where \(:=_{, X}|(,)- (,)|\), then we have \(q<0.5/\). In practice, we have access to the Gram matrix \(^{N N}\) where \(N\) is the size of the data. Then \(\) is given by \(=_{i,j=1}^{N}|_{ij}-_{ji}|\), and the selection of \(q\) is

\[0 q<^{N}|_{ij}-_{ji}|}\,.\]

## 5 Experiments

In this section, we demonstrate the capability of the MagDM method to extract asymmetric information on three synthetic and two real-world trophic datasets. Additionally, we compare our method with five other methods: diffusion maps (DM) , kernel principal component analysis (KPCA) , asymmetric diffusion maps (ADM) , magnetic eigenmaps (ME) , and Markov normalized magnetic eigenmaps (MME) . The experiments were conducted using MATLAB on a PC with an Intel i7-10700K CPU (3.8GHz) and 32GB of memory. Codes are available at [https://github.com/AlexHe123/MagDM](https://github.com/AlexHe123/MagDM)

### Artificial networks

We begin by assessing the ability to extract asymmetric information. In this experiment, we create an artificial network following . The network consists of three groups (A, B, and C), and we establish two types of flows (forward and backward) between them, which helps us construct an adjacency matrix to represent connections among nodes. Each node has a \(50\%\) chance of being connected to a node in its own group. Furthermore, each node has a \(50\%\) chance of being connected to a node in another group in the direction of the forward flow (i.e., interconnections from A to B, from B to C, and from C to A). Additionally, there is another type of interconnection in the direction of the backward flow with a probability of \(P\). By adjusting the backward probability \(P\), we can control the running flow. An illustration of this artificial network is shown in Appendix F.1.

In this experiment, we choose 5 probabilities for the backward running flow, \(P\{0,0.2,0.5,0.8,1\}\). As \(P\) increases, the level of asymmetric information first decreases and then increases. Due to the fact that phases of eigenvectors contain most of asymmetric information [18; 19], Fig. 1 reports phases of the ME, MME and MagDM methods, and shows their capability of extracting asymmetric information. All the three methods distinguish three groups when the probability of backward flow \(P=0\). As we increase the value of \(P\), the level of asymmetric information decreases. This is reflected in the fact that the distance between samples in the same group becomes larger, as seen in the second row of Fig. 1. When \(P=0.5\), the contribution of the forward and backward flows to adjacency connections is statistically the same, resulting in the disappearance of asymmetric information and the mixing of samples from different groups. As \(P\) continues to increase, asymmetric information gradually strengthens again, forming a scenario where the backward flow is stronger than the forward flow. The last two rows of Figure 1 demonstrate that the MagDM algorithm effectively distinguishes between the three groups, whereas the ME and MME algorithms are currently unable to differentiate between them, which illustrates the stronger robustness of our method.

The running flow of the second artificial network comprises four groups (A, B, C, and D) following . The visualization of this directed graph can be seen in Appendix F.2. The structure of the flow is apparent, with Groups A and D serving as out-come and in-come flows, while Groups B and C function as intermediaries. For instance, Group A predominantly generates out-coming links with a high probability (strong flow), and only a few in-coming links from Group D are produced by weak flow with a low probability. Besides, each node has a high probability of being connected to a node in the same group.

Fig. 2 presents the visualization results of six different methods. The DM and KPCA methods yield poor clustering performance as they do not consider asymmetric information. In contrast, the ADM method distinguishes four groups, although it does not illustrate directed information. The ME method distinguishes groups along the phase axes, but it is unable to differentiate between the four groups when considering the real and imaginary parts of the second eigenvector. The MME method has an overlap between Groups A and B along the phase axes, and the second eigenvector embeds Group A within the other groups, making it difficult to distinguish it from the rest. MagDM produces a compact and almost linearly separable clustering not only along the phase axes but also in the first and second eigenvectors, as shown in the bottom row of Fig. 2.

### Synthetic data using an asymmetric kernel

Next, we assume that the dataset is a set of 300 points distributed along the Mobius strip. The parametric form of the Mobius strip is defined by

\[x(u,v)=1+ u, y(u,v)=1+  u, z(u,v)=\,,\]

Figure 1: The capability of three methods (left: ME, middle: MME, right: MagDM) to extract asymmetric information on the three groups colored by red, green, and blue, with \(q=1/4\). Asymmetry in the data is generated by the asymmetric interconnection among the groups, and phases of the three methods are reported to distinguish them. The forward flow probability is fixed at \(0.5\), while the backward flow probability is chosen from the set \(\{0,0.2,0.5,0.8,1\}\).

where \(0 u 2\) and \(-0.5 v 0.5\). The dataset is with a color drift in the counterclockwise direction on the x-y plane, please see it in Appendix F.3. We define an asymmetric kernel as follows,

\[(,)=\{ &\{-\|-\|_{1},0\},&  0\\ &\{0.2-\|-\|_{1},0\},& <0., \]

where \(\) is the angle between \(,\) on the x-y plane. The asymmetric kernel is composed of two truncated \(_{1}\) kernels , which measures asymmetric similarity based on angles between \(\) and \(\).

Figure 3: Dimension reduction results of six methods (DM, ADM, KPCA, ME, MME, and MagDM) on the MÃ¶bius strip which is endowed with the asymmetric kernel (9) and colored by the angle. \(=5\) and \(q=0.09\).

Figure 2: The capability of six methods to extract asymmetric geometry on the four groups colored by red, green, blue, and orange, with \(q=1/3\). The first row shows the results of DM, ADM, and KPCA, while the remaining rows show the results of ME, MME, and MagDM. For better illustration, the real and imaginary parts of first two non-trivial eigenvectors are reported, in addition to the phases.

The dimension reduction results are illustrated in Fig. 3. DM and KPCA fail to distinguish the asymmetric geometry, as they only learn the symmetric part of (9). Although the ADM method contains the color drift after reducing the dimension of data, there is still a sharp turn on the green part of the data, which is not desired. The ME method distinguishes the color drift only on the phase of the first eigenvector, while the MME method fails to provide clear directed information. Notably, the MagDM method provides a clear and smooth result along both two phase axes.

### Trophic networks

To further illustrate the concept, we have chosen two specific real-world trophic networks from the Pajek datasets1: the Mondego network , which records trophic exchanges at the Mondego estuary, and the Florida network , which records trophic exchanges in Florida Bay during the wet season.

Based on the roles of the nodes in ecosystems, we classify them into different categories, as depicted in Appendix F.4. The different categories in the trophic networks consist of Producers, which generate their own food through photosynthesis or chemosynthesis (green nodes); Consumers, consisting of both low-level consumers (brown nodes) and high-level consumers (red nodes) that feed on other organisms for energy; and Organic Matters (blue and turquoise blue nodes). As for the Florida network, there is also a Decomposer that break down dead or decaying organic matter (purple node). The adjacency matrix \(\) denotes the intensity of trophic exchanges and the asymmetric kernel matrix

Figure 4: The dimension reduction of the Florida network is shown using six methods (DM, ADM, KPCA, ME, MME, and MagDM), with \(q=0.045\). Florida is composed of four categories consisting of different elements: Producers (green nodes), Consumers (red and brown nodes), Organic Matters (blue and turquoise blue nodes), and one Decomposer (purple node). The low-dimensional embeddings using the different methods are visually compared.

for the trophic networks are defined by,

\[_{ij}=_{2}(_{ij}+1)\,.\]

The dimension reduction results for two trophic networks are presented in Figs. 4 and 5. For the Florida network, MagDM outperforms other methods, as shown in Fig. 4. All DM, ADM, and KPCA methods fail to differentiate between different categories. On the phases results of the remaining three methods, ME and MME are unable to extract clear asymmetric information, while MagDM almost distinguishes between producers and consumers. For better illustration, the real and imaginary parts of the first two non-trivial eigenvectors of ME, MME, and MagDM are visually compared. It is observed that all three methods fail to distinguish categories on the first eigenvector axis, but on the second eigenvector axis, MagDM not only differentiates the categories but also tightly clusters high-level consumers together, resulting in a clear classification structure. Similar results are obtained for the Mondego network. These findings demonstrate that MagDM is a promising approach for dealing with asymmetric proximity in trophic networks.

## 6 Conclusion

In this paper, we have presented a novel framework named MagDM for representing data endowed with asymmetric kernels using the magnetic transform within the context of diffusion maps. We introduced the integral operator with the proposed magnetic transform kernel and investigated several

Figure 5: The dimension reduction of the Mondego network is shown using six methods (DM, ADM, KPCA, ME, MME, and MagDM), with \(q=0.04\). Mondego is composed of three categories consisting of different elements: Producers (green nodes), Consumers (red and brown nodes), and Organic Matters (blue and turquoise blue nodes). The low-dimensional embeddings using the different methods are visually compared.

of its properties, including compactness, spectral decomposition, and spectral radius. Moreover, we proposed a diffusion distance that embeds asymmetric kernels into the diffusion map in complex-valued function spaces, allowing the identification and analysis of asymmetric geometries and patterns in data. This could lead to new insights into the structure and properties of asymmetric kernels, potentially leading to new applications or algorithmic improvements. Experiments demonstrated the effectiveness and robustness of MagDM on three synthetic datasets and two trophic networks.

There are several interesting avenues for future work in the study of asymmetric kernels. In terms of methodology, exploring variants of other dimension reduction schemes for asymmetric kernels such as MDS  and Isomap  would be appealing. In terms of theory, studying geometric harmonics of functions with magnetic transform kernels could provide a deeper understanding of the intrinsic and extrinsic geometries in data. In terms of applications, using MagDM as feature mappings for classification and regression tasks could be particularly useful for problems involving asymmetric proximity, such as social networks and commodity trading between economies.

**Broader impacts.** The proposed MagDM method described in this paper is not expected to have significantly different impacts on society compared to other dimension reduction techniques.