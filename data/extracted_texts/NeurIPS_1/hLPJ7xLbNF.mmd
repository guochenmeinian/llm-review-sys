# Self-Supervised Motion Magnification by Backpropagating Through Optical Flow

Zhaoying Pan

Equal contribution.

Daniel Geng1

Andrew Owens

University of Michigan

[https://dangeng.github.io/FlowMag](https://dangeng.github.io/FlowMag)

Equal contribution.

###### Abstract

This paper presents a simple, self-supervised method for magnifying subtle motions in video: given an input video and a magnification factor, we manipulate the video such that its new optical flow is scaled by the desired amount. To train our model, we propose a loss function that estimates the optical flow of the generated video and penalizes how far if deviates from the given magnification factor. Thus, training involves differentiating through a pretrained optical flow network. Since our model is self-supervised, we can further improve its performance through test-time adaptation, by finetuning it on the input video. It can also be easily extended to magnify the motions of only user-selected objects. Our approach avoids the need for synthetic magnification datasets that have been used to train prior learning-based approaches. Instead, it leverages the existing capabilities of off-the-shelf motion estimators. We demonstrate the effectiveness of our method through evaluations of both visual quality and quantitative metrics on a range of real-world and synthetic videos, and we show our method works for both supervised and unsupervised optical flow methods.

## 1 Introduction

Motion magnification methods  increase the size of tiny motions in a video, revealing subtle details that are difficult to discern with the naked eye. However, existing methods come with significant limitations. Early hand-crafted methods generally require small periodic motions  or a human in the loop . More recent supervised learning methods  require ground-truth training examples, such as videos before and after magnification, which are difficult to obtain without synthetic data. Creating this synthetic data is a challenging problem, since it seemingly requires capturing all of the possible objects and motions that one might ever want to magnify.

In parallel, the field of motion estimation has addressed many closely related challenges. Modern optical flow networks  are designed to track objects undergoing complex motions, both large and small. Most of these methods are trained on supervised datasets that capture a wide variety of objects, but parallel work has shown the effectiveness of unsupervised flow estimation . We ask whether we can use motion estimation models to train _magnification_ models, taking advantage of their existing capabilities and reducing the need for special-purpose training data.

We propose a simple motion magnification model whose supervision signal comes from an off-the-shelf motion estimation model. Our method exploits the fact that optical flow networks are differentiable, and thus can be used as part of a loss function. We train a model to take a pair of video frames and a magnification factor \(\) as input, and to generate a new pair whose predicted optical flow is \(\) times as large as that of the input. We simultaneously optimize a regularization loss that preserves the visual appearance of each tracked pixel in the generated video. Notably, our model canbe trained solely using real unlabeled videos. The optical flow estimator is only used within the loss function during training--we do not require it at test time.

Our model's simplicity and its ability to be trained solely through self-supervision provides it with several advantages over other approaches. We show that we can improve our generated videos through test-time adaptation  by finetuning on a given input video. Our formulation can also be easily extended to magnify the motions of a single object, specified via a user-provided mask. In addition, we show results using both off-the-shelf supervised  and unsupervised  optical flow methods.

Our method is closely related to Lagrangian magnification methods , which tracks individual pixels through a video and then amplifies their motion. While this approach is intuitively appealing and was the basis for the earliest magnification methods, a major shortcoming is that it is not clear how to combine the tracking and synthesis together. Previous Lagrangian approaches  often rely on hand-crafted pipelines that combine motion estimation, warping, and inpainting steps. By contrast, our method avoids these pitfalls by using off-the-shelf image-to-image translation architectures , and uses tracking only within the loss function.

We demonstrate the effectiveness and flexibility of our model in several ways. First, we show through experiments on real and synthetic videos that the optical flow in the generated videos more closely matches the desired motion than videos generated by baseline approaches, such as methods based on warping or supervised learning . Second, we show qualitatively that our model can successfully magnify motions for a variety of videos, containing both small and large motions. Finally, we show that we can improve generation quality using test-time adaptation, and magnify individual objects using user-supplied segmentation maps.

## 2 Related Work

Lagrangian magnification.Motion magnification methods can be broadly categorized as either Lagrangian  or Eulerian , a classification borrowed from fluid dynamics. Lagrangian methods explicitly track pixels, then generate new frames by forward warping (i.e., splatting) using magnified velocity estimates. Liu _et al._ originally proposed the motion magnification problem and solved it using a Lagrangian approach. They modeled motion as feature trajectories and (with human-in-the-loop assistance) cluster pixels by similarity in position, color, and motion. Recent methods have extended Lagrangian magnification with more accurate optical flow . While our approach performs explicit tracking and thus is closely related to Lagrangian methods, we decouple generation from tracking: we produce images using off-the-shelf networks and track _only within the loss function_. This allows us to avoid some of the challenges of the "warp and inpaint" approach, such as handling occlusions and filling holes.

Figure 1: **Magnifying Motions in Video.****(a)** We show frames along with closeups (Left) or \(y\)-\(t\) slices (Right) from original videos and magnified videos. The spatial location of the \(y\)-\(t\) slice is visualized by superimposing a colored line on to the still frames, and the location of closeups is shown with a rectangle colored green. **(b)** We show the frames from the original video, the magnified frame (No-TTA), the magnified frame after test-time adaptation (TTA), and the \(y\)-\(t\) slices of the three videos. Test-time adaptation (i.e., finetuning on the input video) improves generation quality as can be seen in the \(y\)-\(t\) slices. **(c)** We magnify the motions within a user-selected object segmentation  and show \(y\)-\(t\) slices of the magnified videos on different targets. We use red/green colored lines to indicate the locations of \(y\)-\(t\) slices for different targets.

Eulerian magnification.Eulerian methods magnify motions without explicit motion estimation. Instead, they generate a magnified video by amplifying the temporal changes at fixed locations/pixels. Wu _et al._ decomposed a video into frequency bands and applied temporal filters to extract a signal at a specific bandpass. The extracted signal is then amplified by a magnification factor and added back to the video. Wadhwa _et al._ proposed a phase-based method, using complex steerable pyramids  to decompose the video and separate the phase from the amplitude to amplify the temporally-bandpassed phases. Later work improves efficiency using Riesz pyramids  and removes large motion by decomposing a scene into foreground and background layers . Zhang _et al._ magnified small motions while ignoring large motions by amplifying the motion field acceleration using second-order temporal filters. Concurrent work  extended 2D Eulerian methods to 3D motion magnification. Eulerian methods are well-suited to tracking small motions at high spatial frequencies, and may struggle when handling large motions , whereas Lagrangian methods' magnification quality is determined by the quality of the optical flow predictions . Since our method's loss function is defined using optical flow, its capabilities are more similar to those of Lagrangian methods.

Learning-based magnification.Several recent works have proposed learning-based motion magnification models. Oh _et al._ created a synthetic dataset containing image segments extracted from PASCAL VOC  as moving foreground and images from COCO  as background, and trained a model to regress a ground truth magnified image from two video frames. Inspired by the steerable pyramid used in Eulerian methods , they also proposed an architecture that has inductive biases that encourage it to generate crisp images akin to those in Eulerian magnification, by decomposing the image into a shape and texture representation. Other work has extended this supervised magnification approach using 3D CNNs  and lightweight architectures , and has magnified microexpressions using attention-based models . In contrast to these approaches, our method is _self-supervised_ and learns from unlabeled videos. Recently, Gao _et al._ use a very similar supervised learning objective and dataset but augment the model of Oh _et al._ with inductive biases that encourage Lagrangian-like magnification by adding an attention map that is guided by an optical flow field. Instead of making optical flow part of our architecture, as an inductive bias that aids supervised training, we use it to define a self-supervised _loss function_.

Motion estimation.The field of motion estimation is closely related to motion magnification. Early work solved linearized models after making color constancy assumptions  or solved smoothness-regularized models with variational methods . Later work added robust losses and inference strategies . While these methods are well-suited to the subtle motions considered in motion magnification, it is difficult to use them as part of a loss function (as we do in this work), since it is not straightforward to differentiate through during gradient-based learning. Recent methods based on neural networks address this issue, since they are differentiable and highly accurate. A variety of recent methods train neural networks using real or synthetic optical flow data  or unlabeled video data . Our use of a pretrained flow model removes the need for special-purpose supervised motion magnification datasets.

Optical flow within a loss function.We take inspiration from recent work that uses differentiable optical flow models to define loss functions that internally perform motion reasoning. Geng _et al._ used flow to obtain robustness to small positional errors on image generation tasks. Goyal _et al._ used flow to measure the distance to a goal state for robotic planning. Other work has backpropagated through optical flow as part of a pose estimation model . By contrast, we use flow to compare the motions in two videos.

## 3 Method

We describe traditional Lagrangian motion magnification, and our proposed self-supervised Lagrangian magnification model.

### Lagrangian Motion Magnification Overview

Lagrangian motion magnification methods magnify motion by tracking pixels over time, then resynthesizing the video such that the motion of each pixel has increased by a desired amount. More concretely, a point \(\) in the initial frame \(I_{0}\) of a video might be displaced by motion field \((;t)\) in frame \(t\). Assuming color constancy, we have:

\[I_{t}(+(;t))=I_{0}(), \]

where \(I_{t}\) is the frame at time \(t\) and \(I()\) indicates pixel access. To generate a version of the video magnified by a factor \(\)1, one could synthesize a new frame \(_{t}\) by increasing the distance traversed by each pixel:

\[_{t}(+(;t)) I_{0}( ). \]

For example, early work in Lagrangian motion magnification  aimed to achieve this by performing forward warping (splatting). However, this leads to holes and aliasing artifacts . Moreover, there is no mechanism for dealing with occlusions: after warping, background pixels may move in front of the foreground. Finally, such a model would not be able to deal with appearance changes without additional constraints (e.g., due to lighting variation).

### Self-Supervised Lagrangian Magnification

We propose a motion magnification model that avoids the limitations of previous Lagrangian methods, and that can be trained solely using unlabeled video (Figure 2). Consider \((I_{0},I_{t})\), the motion field obtained by computing optical flow between a reference frame \(I_{0}\) and another frame \(I_{t}\). Our goal is to generate a magnified frame \(_{t}\) such that its predicted flow is \((I_{0},I_{t})\), where \(\) is the magnification factor. To do this, we optimize an objective to minimize the difference between the estimated flow \((I_{0},_{t})\) and the desired flow:

\[_{}=\|(I_{0},I_{t})-(I_{0}, _{t})\|_{1}, \]

which we term the _magnification loss_. When \(\) is implemented as a neural network, we can optimize this loss using gradient-based learning methods.

Optical flow models are invariant to a variety of photometric changes (e.g., changes in illumination). Thus, optimizing Eq. 3 alone would result in an underconstrained problem. To ensure that the generated frames match the colors in the input video and to also regularize our problem, we use an additional _color loss_. This loss ensures that corresponding pixels in generated frame \(_{t}\) and original frame \(I_{t}\) are the same color. To enforce this, we put both frame \(I_{t}\) and \(_{t}\) into correspondence with the common reference frame \(I_{0}\) by backward warping each frame with their respective flows: \((I_{0},I_{t})\) and \((I_{0},_{t})\). We then measure the distance between the warped frames. This results in a loss:

\[_{}=\|(I_{t},(I_{0},I_{t}))- (_{t},(I_{0},_{t}))\|_{1}. \]

Figure 2: **Motion Magnification Model. Given a reference frame \(I_{0}\), a frame to be magnified \(I_{t}\), and a map of per-pixel magnification factors \(\), we predict a magnified frame \(_{t}\). We minimize two losses that each use an off-the-shelf optical flow estimator \((,)\). First, we use a magnification loss \(_{}\) that encourages the optical flow of the generated video to be \(\) times as large as that of the input video. And second, we also include a consistency loss \(_{}\) that measures the visual similarity of corresponding pixels in \(I_{t}\) and \(_{t}\).**

This loss also disincentivizes adversarial examples against the flow network. Our full loss is a weighted sum of these two losses:

\[=_{}+_{}_{}. \]

One benefit of putting optical flow into the loss function is that our model needs access to the flow network only during training; at inference time the model simply takes in two frames and outputs a magnified frame. Pseudocode for our method for training and inference can be found in Algorithm 1 and Algorithm 2 respectively.

### Image Generation Architecture

One benefit of our formulation is that our proposed loss (Eq. 5) is independent of the image generation architecture, so that "off-the-shelf" image translation architectures can be used to create a magnified image. This is in contrast to other learning-based methods, which incorporate inductive biases within the network to generate images that closely resemble the input (e.g., by altering the structure of an image while preserving the texture ). To generate magnified frames, we use a standard U-Net architecture  that takes two input frames (the reference frame and the frame to be magnified) concatenated channel-wise. We encode magnification factor \(\) using a sinusoidal positional embedding  and tile it to match the same spatial dimensions as the input frames. We concatenate this embedding channel-wise to the input frames. Please see Section A2 in the appendix for full architectural details.

### Targeted Magnification

Our model gives us the ability to vary the magnification factor spatially within an image. We achieve this by providing different values of \(\) for each pixel at inference time. This works even when training with spatially constant alpha maps, due to the fully convolutional nature of our network2. As a special case, we magnify the motion of a single object by setting the magnification factor to a value of \(\) within an object segment, and to 1 everywhere else. In practice, we use the recent Segment Anything Model (SAM)  to extract a mask for a given object in the reference frame and then dilate the segmentation mask by a small fixed number of pixels. We provide pseudocode for targeted magnification during inference in Algorithm 2.

```
inLonddataoftwo-framevideos for(im0,im1)indataloader: #Samplealphaandgetembedding a*sample_alpha(min_alpha,max_alpha)pe_a=position1_embedding(a) pe_a=spatial_tile(pe_a) #Predictmagnifiedframewithanetwork input=concat((im0,im1,pe_a))inl_mag=UNet(input) #Estimatethemotion F_src=optical_flow(im0,im1)F_tgt=optical_flow(im0,im1_msg) #Nurbecsecondframe warp_im1=warp(im1,F_src) warp_im1_msg=warp(im1_msg,F_tgt) #Calculatelosses mg_loss=11_loss(a*F_src,F_tgt) color_loss=11_loss(warp_im1,warp_im1_msg) loss=mag_loss+weight*color_loss loss.backward() optimizer.step()
```

**Algorithm 1** Pseudocode in a PyTorch-like style for training a U-Net for motion magnification.

### Test-Time Adaptation

Since our model is entirely self-supervised, we can finetune it at inference time on the input video . This allows us to adapt to new motions and content. We find that this can significantly improve results, especially for out-of-domain videos. Previous supervised approaches do not have this capability as they require labeled data. To perform test-time adaptation we take the frames in a given inference video, apply minor cropping and color augmentations, and finetune the model with our loss.

Experiments

We evaluate our model both quantitatively through experiments with real and synthetic data, and qualitatively on real videos. We highly encourage the reader to view our website, since the magnified motions can be challenging to visualize in static images.

### Implementation Details

We train two variations of our model. One that uses ARFlow , and one that uses RAFT  as the optical flow network. Since ARFlow is self-supervised, using it results in a fully self-supervised motion magnification method. RAFT on the other hand is a supervised flow model with very good performance, giving us a weakly-supervised magnification method that serves as a rough upper bound to the performance of a fully self-supervised method. More discussion can be found in Section A2 in the appendix. All qualitative results presented in this paper are from the RAFT model, except in Figure 6 in which we compare the ARFlow and RAFT models.

Because motion magnification is multiplicative, we sample random magnification factors \(\) exponentially, such that \(_{2}() U(_{2}(_{}),_{2}(_{}))\), where \(_{}=1\) and \(_{}=16\). For larger magnification factors that exceed \(_{}\), we find that recursive application of our model produces high-quality predictions. More implementation details are available in Section A2 in the appendix.

### Dataset

Because our method is self-supervised, we benefit from a large, diverse dataset of videos. To this end we curate a dataset containing 145k unlabeled frame pairs from several existing datasets, including YouTube-VOS-2019 , DAVIS , Vimeo-90k , Tracking Any Object (TAO) , and Unidentified Video Objects (UVO) . We remove frame pairs with large motions (e.g., from the camera or objects), since these frames are less likely to be used in magnification applications. We also remove frame pairs that are near-identical by setting a lower bound on the MSE between the two frames. In addition to a training set, we collect a test set consisting 650 frame pairs for evaluation, which we refer to as **real-world test set**. We provide more details of dataset collection and filtering in Section A3 of the appendix.

For testing, we also use the synthetic test set from Oh _et al._, which is generated by compositing objects from PASCAL VOC  to backgrounds from COCO  at varying levels of subpixel motion and noise. We refer to this test set as the **synthetic test set**. Finally, we qualitatively assess our method and baselines on various **real-world videos**, which vary greatly in subject matter, motion complexity, and lighting conditions. These videos include de-facto standard benchmarks for motion magnification used in previous works [64; 58; 59; 40], as well as new videos.

### Metrics and Baselines

Metrics.Similar to Oh _et al._, we use **SSIM** as an evaluation metric. However, because SSIM requires ground-truth magnified images, we can only use this metric to evaluate models on the synthetic test set. In order to evaluate our method on the real-world test set, we propose another metric for motion magnification that does not require ground truth, which we term **motion error**. Inspired by the accuracy and robustness of recent optical flow methods, we assume that the flow estimate from an optical flow method is ground truth and calculate an end-point error between the predicted flow and the desired magnified flow.

This metric is identical to \(_{}\) if the same optical flow model is used for evaluation as is used during training. In order to ensure a robust metric, we calculate the motion error metric using a wide range of optical flow networks, including PWC-Net, GMFlow, and RAFT, that have been trained on various datasets. We set the number of iterations for RAFT to 20 during evaluation, while keeping it at 5 during training. This serves the dual purpose of allowing us to train more efficiently, but also evaluate more equitably.

In addition, we compute the per-pixel ratio between the flow magnitudes of the unmagnified and magnified frames, and calculate their average deviation from the desired magnification factor \(\). This metric, which we refer to as **magnification error**, was not explicitly trained for and serves as another indicator of magnification quality.

Baselines.We compare primarily against the method of Oh _et al._, a neural network based approach trained on a synthetic dataset. Additionally, we implement forward warp baselines that use nearest and bilinear sampling to warp pixels according to the amplified flow, along with a nonparametric inpainting method , which we term **Warp Nearest** and **Warp Bilinear** respectively. We also compare against **FLAVR**, a 3D U-Net trained for frame interpolation and finetuned on the same synthetic dataset as Oh _et al._, albeit at a constant magnification factor of \(=10\). Finally, we provide qualitative comparisons in Section A5 of the appendix to **Neural Implicit Video Representations (NIVR)**, a method that fits an implicit representation to a video and displays emergent motion manipulation behavior.

### Comparison with the State-of-the-Art

Visual quality.We show qualitative results on real-world videos. In Figure 4 we show still frames and closeups of results from Oh _et al._ and our method. And in Figure 3 we plot \(y\)-\(t\) slices through the video volume, with one dimension being time and the other being spatial3.

One qualitative finding is that while the flow network may be noisy on a specific video, our model trained with the same flow network can be much more robust. This can be seen clearly in the _baby_ sequence of Figure 3. The forward warp methods, which depend on the optical flow estimate on the inference video, are very jittery whereas our predictions are much smoother. In effect, our method distills a given flow network into a more robust estimator of object motion.

Figure 4: **Image Quality Comparison. We show magnified frames generated from the method of Oh _et al._ and our method. Because the model of Oh _et al._ is trained on a synthetic dataset, it may not generalize to the presence of novel motions such as the camel chewing, or novel scenes, such as the _train_ and _boiler_ sequences.**

Figure 3: **Qualitative Comparison. (Top) We visualize the results of various models by plotting \(y\)-\(t\) slices through video volumes. (Bottom) We also show the reference frame and the locations of the \(y\)-\(t\) slices. Oh _et al._ (iv) shows noticeable artifacts, indicated by red circles. In (vi) we see that test time adaptation (TTA) can improve our method on out-of-domain inference videos.**

[MISSING_PAGE_FAIL:8]

with FlyingThings . We achieve satisfactory results on the motion error metrics and lag slightly behind on SSIM.

## 5 Discussion

We present a method for learning to magnify subtle motions through self-supervised learning. We demonstrated its effectiveness through a variety of experiments, using quantitative evaluations on real and synthetic data, and through qualitative results on videos containing complex motions. We also showed that the flexibility of our model allowed it to be extended through test-time adaptation and targeted motion magnification. We see our work as opening new directions in visualizing subtle motions. First, while we have shown one method for defining the loss function, based on optical flow, our approach could be applied to other differentiable motion estimation techniques, such as emerging

    &  &  &  \\  Method & \(\)=2 & \(\)=4 & \(\)=10 & \(\)=16 & \(\)=64 & \(\)=22 & \(\)=4 & \(\)=10 & \(\)=16 & \(\)=64 & \(\)=22 & \(\)=4 & \(\)=10 & \(\)=16 & \(\)=64 \\  Warp Nearest & 0.59 & 1.42 & 4.15 & 7.24 & 35.76 & 0.40 & 0.83 & 2.91 & 5.49 & 28.87 & 0.56 & 1.26 & 3.84 & 6.86 & 33.61 \\ Warp Bilinear & 0.57 & 1.40 & 4.12 & 7.28 & 35.78 & 0.35 & **0.78** & 2.97 & 5.52 & **28.70** & 0.53 & 1.24 & 3.79 & 6.88 & 33.72 \\ FLAVR & - & - & - & 4.24 & - & - & - & - & 3.87 & - & - & - & - & - & 4.40 & - & - \\ Oh _et al._ & 0.51 & 1.37 & 4.29 & 7.60 & 39.49 & 0.47 & 1.21 & 3.86 & 6.95 & 37.53 & 0.52 & 1.40 & 4.34 & 7.64 & 37.31 \\ Ours (ARFlow) & 0.38 & 1.15 & 3.70 & 6.58 & 34.21 & 0.33 & 1.03 & 3.52 & 6.33 & 32.89 & 0.41 & 1.30 & 4.15 & 7.23 & 35.02 \\ Ours (RAFT) & **0.30** & **0.95** & **3.32** & **6.01** & **32.01** & **0.26** & **0.78** & **2.82** & **5.20** & 29.26 & **0.37** & **1.19** & **3.88** & **6.73** & **33.49** \\   

Table 1: **Quantitative Evaluation Results on Real-world Test Set.** We report the motion error of our method and baselines on our collected evaluation set. A smaller motion error represents better magnification quality. For fair comparison, we show results for optical flow methods not used during training, including PWC-Net  and GMFlow  trained with FlyingThings . These results are a subset of those in Figure 7. We achieve better motion magnification on almost all evaluation metrics.

    &  &  &  \\   &  &  &  &  &  &  \\  Method & 0.04px & 0.29x & 1px & 0.01x & 1x & 100x & 0.04px & 0.29x & 1px & 0.01x & 1x & 100x & 0.04px & 0.29x & 1px & 0.01x & 1x & 100x \\  Warp Nearest & 10.60 & 3.01 & 2.12 & 0.65 & 0.91 & 221.32 & 209.10 & 22.61 & 3.12 & 0.16 & 0.23 & 1.90 & 0.77 & 0.92 & 0.97 & 0.97 & 0.85 & 0.18 \\ Warp Bilinear & 10.61 & 3.02 & 2.12 & **0.61** & 0.90 & 226.37 & 209.35 & 22.61 & 3.04 & 0.15 & 0.23 & 1.68 & 0.78 & 0.93 & 0.97 & 0.78 & 0.90 & 0.19 \\ Oh _et al._ & **9.82** & 2.93 & 2.26 & 1.01 & 0.94 & 118.26 & 193.78 & 22.05 & 3.15 & 0.28 & 0.25 & 1.70 & **0.93** & **0.97** & **0.98** & **0.98** & **0.97** & **0.31** \\ Ours (ARFlow) & 12.29 & 3.26 & 2.35 & 0.82 & 0.70 & 167.77 & 222.16 & 20.87 & 3.05 & 0.20 & 0.18 & 1.69 & 0.75 & 0.89 & 0.94 & 0.97 & 0.94 & 0.10 \\ Ours (RAFT) & 10.02 & **2.80** & **2.05** & 0.69 & **0.68** & **164.36** & **175.45** & **17.01** & **2.48** & 0.17 & **0.17** & **1.49** & 0.82 & 0.90 & 0.94 & 0.97 & 0.94 & 0.10 \\   

Table 2: **Quantitative Evaluation Results on Synthetic Videos.** We report the motion error, magnification error, and SSIM of our method and baselines on two synthetic evaluation sets. Smaller motion error or magnification error, and larger SSIM indicate better quality. We use GMFlow trained with Flying Things to calculate motion error and magnification error. These results are a subset of those plotted in Figure 8.

Figure 8: **Evaluation on synthetic data.** We evaluate our model using the synthetic test dataset from Oh _et al._. We use two subsets and two metrics (motion error and SSIM). Error bars show the standard error.

Figure 7: **Evaluation on real-world test set.** We evaluate the methods with \(\) ranging from 1 to 64 and various flow methods. Error bars show the standard error. On motion error, our method consistently obtains more accurate magnified motions.

methods for long-range tracking . Second, the flexibility of our model opens the possibility for other "user in the loop" extensions, beyond allowing for segmentation-based magnification.

Broader impacts.Motion magnification methods have a range of applications, such as amplifying subtle motions in biology and engineering , amplifying microexpressions for assistive technology , assisting measurement techniques , and for video forensics . Please see our website for potential applications. It also has the potential negative use of revealing body motions that a person may think are undetectable, such as one's pulse , which may impinge on privacy.

Limitations.The performance of our model is closely tied to the limitations and capabilities of the underlying optical flow estimator. We share the limitation with existing work  that our method works by magnifying motion between the first frame and every subsequent frame, which may pose challenges in the presence of occlusion and disocclusion. The magnification loss (Eq. 3) may still be applicable in these cases, since modern optical flow models are trained to track occluded pixels. However, the photoconsistency assumption (Eq. 4) will no longer hold. For simplicity, we do not inpaint missing image regions , and we use distance between pixels in lieu of a perceptual loss or a generative model. This simplicity has potential advantages for visualization applications, since it avoids hallucinating scene structure that is not present in the original images, but it also sometimes results in undesirable artifacts.

Acknowledgements.We thank Tae-Hyun Oh for the extensive discussions and data. We also thank Byung-Ki Kwon, Tarun Kalluri, Long Mai, and Stella Yu for helpful discussions. Daniel Geng is supported by a National Science Foundation Graduate Research Fellowship under Grant No. 1841052. This work was supported in part by DARPA Semafor, Program No. HR001120C0123. The views, opinions and/or findings expressed are those of the authors and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government.