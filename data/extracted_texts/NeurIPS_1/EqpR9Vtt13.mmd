# Does Invariant Graph Learning via Environment Augmentation Learn Invariance?

Yongqiang Chen\({}^{1}\), Yatao Bian\({}^{2}\), Kaiwen Zhou\({}^{1}\)

\({}^{1}\)The Chinese University of Hong Kong \({}^{2}\)Tencent AI Lab

{yqchen,kwzhou}@cse.cuhk.eduhk yatao.bian@gmail.com

Binghui Xie\({}^{1}\), Bo Han\({}^{3}\), James Cheng\({}^{1}\)

\({}^{3}\)Hong Kong Baptist University

bhanm1@comp.hkbue.eduhk {bhxie21,jcheng}@cse.cuhk.eduhk

Work done during an internship at Tencent AI Lab.Code is available at [https://github.com/LFhase/GALA](https://github.com/LFhase/GALA).

###### Abstract

Invariant graph representation learning aims to learn the invariance among data from different environments for out-of-distribution generalization on graphs. As the graph environment partitions are usually expensive to obtain, augmenting the environment information has become the _de facto_ approach. However, the _usefulness_ of the augmented environment information has never been verified. In this work, we find that it is fundamentally _impossible_ to learn invariant graph representations via environment augmentation without additional assumptions. Therefore, we develop a set of _minimal assumptions_, including variation sufficiency and variation consistency, for feasible invariant graph learning. We then propose a new framework **G**raph inv**A**riant **L**earning **A**ssistant (GALA). GALA incorporates an assistant model that needs to be sensitive to graph environment changes or distribution shifts. The correctness of the proxy predictions by the assistant model hence can differentiate the variations in spurious subgraphs. We show that extracting the maximally invariant subgraph to the proxy predictions provably identifies the underlying invariant subgraph for successful OOD generalization under the established minimal assumptions. Extensive experiments on \(12\) datasets including DrugOOD with various graph distribution shifts confirm the effectiveness of GALA1.

## 1 Introduction

Graph representation learning with graph neural networks (GNNs) has proven to be highly successful in tasks involving relational information . However, it assumes that the training and test graphs are independently drawn from the identical distribution (iid.), which can hardly hold for many graph applications such as in Social Network, and Drug Discovery . The performance of GNNs could be seriously degenerated by _graph distribution shifts_, i.e., mismatches between the training and test graph distributions caused by some underlying environmental factors during the graph data collection process . To overcome the Out-of-Distribution (OOD) generalization failure, recently there has been a growing surge of interest in incorporating the invariance principle from causality  into GNNs . The rationale of the invariant graph learning approaches is to identify the underlying _invariant subgraph_ of the input graph, which shares an invariant correlation with the target labels across multiple graphdistributions from different environments . Thus, the predictions made merely based on the invariant subgraphs can be generalized to OOD graphs that come from a new environment .

As the environment labels or partitions on graphs are often expensive to obtain , augmenting the environment information, such as generating new environments  and inferring the environment labels , has become the _de facto_ approach for invariant graph learning. However, little attention has been paid to verifying the _fidelity_ (or _faithfulness_2) of the augmented environment information. For example, if the generated environments or inferred environment labels induce a higher bias or noise, it would make the learning of graph invariance even harder. Although it looks appealing to _learn both_ the environment information and the graph invariance, the existing approaches could easily run into the "no free lunch" dilemma . In fact, Lin et al.  found that there exist negative cases in the Euclidean regime where it is impossible to identify the invariant features without environment partitions. When it comes to the graph regime where the OOD generalization is fundamentally more difficult  than the Euclidean regime, it raises a challenging research question:

_When and how could one learn graph invariance without the environment labels?_

In this work, we present a theoretical investigation of the problem and seek a set of _minimal assumptions_ on the underlying environments for feasible invariant graph learning. Based on a family of simple graph examples (Def. 3.1), we show that existing environment generation approaches can fail to generate faithful environments, when the underlying environments are not sufficient to uncover all the variations of the spurious subgraphs (Prop. 3.2). On the contrary, incorporating the generated environments can even lead to a worse OOD performance. The failure of faithful environment generation implies the necessity of _variation sufficiency_ (Assumption 3.3). Moreover, even with sufficient environments, inferring faithful environment labels remains impossible. Since invariant and spurious subgraphs can have an arbitrary degree of correlation with labels, there exist multiple sets of training environments that have the same joint distribution of \(P(G,Y)\) but different invariant subgraphs. Any invariant graph learning algorithms will inevitably fail to identify the invariant subgraph in at least one set of training environments (Prop. 3.4). Therefore, we need to additionally ensure the _variation consistency_ (Assumption 3.5), that is, the invariant and spurious subgraphs should have a consistent relationship in the correlation strengths with the labels.

To resolve the OOD generalization challenge under the established assumptions, we propose a new framework **G**raph inv**A**riant **L**earning **A**ssistant (GALA). GALA incorporates an additional assistant model that needs to be prone to distribution shifts, to generate proxy predictions of the training samples. Different from previous environment inferring approaches , GALA does not require explicit environment labels but merely proxy predictions to differentiate the variations in the spurious

Figure 1: An illustration of GALA with the task of classifying graphs according to whether there exists a “House” or “Cycle” motif. Given the training data where the “House” subgraph often co-occurs with a “Grid” and the “Cycle” subgraph often co-occurs with a “Hexagon”. An ERM trained environment assistant model will fit the spurious subgraph and therefore yield proxy predictions “House” or “Cycle” for any graphs containing a “Grid” (left half) or “Hexagon” (right half), respectively. GALA first separates the samples according to the correctness of the proxy predictions into the sets of positive graphs \(\{G^{p}\}\) (correct, in blue) and negative graphs \(\{G^{n}\}\) (incorrect, in green). Then, GALA extracts the maximally invariant subgraph among \(\{G^{p}\}\) and \(\{G^{n}\}\), i.e., pulling graphs with the same graph label but from \(\{G^{p}\}\) and \(\{G^{n}\}\) closer in the latent space, hence identifies the invariant subgraph.

subgraphs. As shown in Fig. 1, we first fit an environment assistant model to the training distribution and then divide the training graphs into a positive set \(\{G^{p}\}\) and a negative \(\{G^{n}\}\), according to whether the proxy predictions are correct or not, respectively. As spurious correlations tend to vary more easily than invariant correlations, the variations in spurious subgraphs are further differentiated and increased between \(\{G^{p}\}\) and \(\{G^{n}\}\). Then, only the invariant subgraph holds an invariant correlation with the label among \(\{G^{p}\}\) and \(\{G^{n}\}\), and hence can be identified by extracting the subgraphs that maximize the intra-class subgraph mutual information among \(\{G^{p}\}\) and \(\{G^{n}\}\) (Theorem 4.1).

We conduct extensive experiments to validate the effectiveness of GALA using \(12\) datasets with various graph distribution shifts. Notably, GALA brings improvements up to \(30\%\) in multiple graph datasets.

Our contributions can be summarized as follows:

* We identify failure cases of existing invariant graph learning approaches and establish the minimal assumptions for feasible invariant graph learning;
* We develop a novel framework GALA with provable identifiability of the invariant subgraph for OOD generalization on graphs under the assumptions;
* We conduct extensive experiments to verify both our theoretical results and the superiority of GALA;

Notably, both our theory and solution differ from Lin et al.  fundamentally, as we do not rely on the auxiliary information and are compatible with the existing interpretable and generalizable GNN architecture for OOD generalization on graphs. Meanwhile, we provide a new theoretical framework that resolves the counterexample in Lin et al.  while enjoying provable identifiability.

## 2 Background and Preliminaries

We begin by introducing the key concepts and backgrounds of invariant graph learning, and leave more details in Appendix C. The notations used in the paper are given in Appendix A.

OOD generalization on graphs.

This work focuses on graph classification, while the results generalize to node classification as well using the same setting as in Wu et al. . Specifically, we are given a set of graph datasets \(=\{_{e}\}_{e_{}}\) collected from multiple environments \(_{}\). Samples \((G^{e}_{i},Y^{e}_{i})^{e}\) from the environment \(e\) are drawn independently from an identical distribution \(^{e}\). The goal of OOD generalization on graphs is to find a GNN \(f\) that minimizes the maximal loss among all environments, i.e., to minimize \(_{e_{}}R^{e}\), where \(R^{e}\) is the risk of \(f\) under environment \(e\). We consider the same graph generation process proposed by Chen et al.  which is inspired by real-world drug discovery task  and covers a broad case of graph distribution shifts. As shown in Fig. 2, the generation of the observed graphs \(G\) and labels \(Y\) are controlled by a latent causal variable \(C\) and a spurious variable \(S\). \(C\) and \(S\) control \(Y\) and \(G\) by controlling the generation of the underlying invariant subgraph \(G_{c}\) and spurious subgraph \(G_{s}\), respectively. Since \(S\) can be affected by the environment \(E\), the correlation between \(Y\) and \(G_{s}\) can change arbitrarily when the environment changes. Besides, the interaction among \(C\), \(S\) and \(Y\) at the latent space can be further categorized into _Full Informative Invariant Features_ (_PIIF_) when \(Y S|C\), and _Partially Informative Invariant Features_ (_PIIF_) when \(Y S|C\).

To tackle the OOD generalization challenge on graphs from Fig. 2, the existing invariant graph learning approaches are generically designed to identify the underlying invariant subgraph \(G_{c}\) to predict the label \(Y\). Specifically, the goal of OOD generalization on graphs is to learn an _invariant GNN_\(f f_{c} g\), which is composed of: a) a featurizer \(g:_{c}\) that estimates the invariant subgraph \(_{c}\); b) a classifier \(f_{c}:_{c}\) that predicts the label \(Y\) based on the extracted \(_{c}\), where \(_{c}\) refers to the space of subgraphs of \(\). The learning objectives of \(f_{c}\) and \(g\) are formulated as

\[_{f_{c},~{}g}I(_{c};Y),~{}~{}_{c} E, ~{}_{c}=g(G). \]

Since \(E\) is not observed, many strategies are proposed to impose the independence of \(_{c}\) and \(E\). A prevalent approach is to augment the environment information. Based on the estimated invariant

Figure 2: SCMs on graph distribution shifts .

subgraphs \(_{c}\) and spurious subgraphs \(_{s}\), Liu et al. , Wu et al. [72; 73] propose to generate new environments, while Li et al. , Yang et al.  propose to infer the underlying environment labels. However, we show that they all fail to augment faithful environment information in Sec. 3.

Besides, Miao et al. [55; 56], Yu et al. [81; 82; 83] adopt graph information bottleneck to tackle FIIF graph shifts, but they cannot generalize to PIIF shifts, while Our work focuses on PIIF shifts as it is more challenging when without environment labels . Fan et al.  generalize  to tackle severe graph biases, i.e., when \(H(S|Y)<H(C|Y)\). Chen et al.  propose a contrastive framework to tackle both FIIF and PIFF graph shifts, but is limited to \(H(S|Y)>H(C|Y)\). In practice, as it is usually unknown which correlation is stronger, we need a unified solution to tackle both cases.

**Invariant learning without environment labels.** In the Euclidean regime, there are plentiful studies in invariant learning without environment labels. Creager et al.  propose a minmax formulation to infer the environment labels. Liu et al.  propose a self-boosting framework based on the estimated invariant and variant features. Deng et al. , Liu et al. , Pezeshki et al. , Zhang et al.  propose to infer labels based on the failures of an ERM model. However, Lin et al.  find failure cases of the aforementioned approaches that it is impossible to identify the invariant features without given environment labels in Euclidean data, and propose a solution that leverages auxiliary environment information for invariant learning. As the OOD generalization on graphs poses more challenges , whether it is feasible to learn invariant graph representations without any auxiliary environment information remains elusive.

## 3 Pitfalls of Environment Augmentation

Given only the mixed training data without environment partitions, is it possible to learn to generate faithful environments or infer the underlying environment labels that facilitate OOD generalization on graphs? In the discussion below, we adopt the two-piece graphs to instantiate the problem, which is the simplistic version of the PIIF distribution shifts in Fig. 2(c), motivated by Kamath et al. .

**Definition 3.1** (Two-piece graphs).: _Each environment \(e\) is defined with two parameters, \(_{e},_{e}\), and the dataset \((G^{e},Y^{e})_{c}\) is generated as follows:_

1. _[label=()]_
2. _Sample_ \(Y^{e}\{-1,1\}\) _uniformly;_
3. _Generate_ \(G_{c}\) _and_ \(G_{s}\) _via :_ \(G_{c} f^{G_{e}}_{}(Y^{e}(_{e})),\ G_{s}  f^{G_{s}}_{}(Y^{e}(_{e})),\) _where_ \(f^{G_{e}}_{},f^{G_{s}}_{}\) _map the input_ \(\{-1,1\}\) _to a corresponding graph selected from a given set, and_ \(()\) _is a random variable taking value_ \(-1\) _with probability_ \(\) _and_ \(+1\) _with_ \(1-\)_;_
4. _Synthesize_ \(G^{e}\) _by randomly assembling_ \(G_{c}\) _and_ \(G_{s}\)_:_ \(G^{e} f^{G}_{}(G_{c},G_{s})\)_._

We denote an environment \(e\) with \((,_{e})\) for simplicity. Different environments will have a different \(_{e}\), thus \(P(Y|G_{s})\) will change across different environments, while \(P(Y|G_{c})\) remains invariant.

### Pitfalls of environment generation

We begin by discussing the cases where there are few environments, and generating new environments is necessary [72; 73; 45]. Environment generation aims to provide some additional "virtual" environments \(_{v}\) such that the invariant subgraph can be identified via applying an OOD risk to the joint dataset with the augmented data \(^{v}_{}=\{_{e}|e_{v}_{v}\}\).

The generation of "virtual" environments is primarily based on the intermediate estimation of the invariant and spurious subgraphs, denoted as \(_{c}\) and \(_{s}\), respectively. Liu et al. , Wu et al.  propose DIR and GREA to construct new graphs by assembling \(_{c}\) and \(_{s}\) from different graphs. Specifically, given \(n\) samples \(\{G^{i},Y^{i}\}_{n=1}^{n=3}\) the new graph samples in \(_{v}\) is generated as follows:

\[G^{i,j}=f^{G}_{}(_{c}^{i},_{s}^{j}),\  i,j \{1...n\},\ Y^{i,j}=Y^{i},\]

which generates a new environment \(_{v}\) with \(n^{2}\) samples. Although both DIR and GREA gain some empirical success, the faithfulness of \(_{v}\) remains questionable, as the generation is merely based on _inaccurate_ estimations of the invariant and spurious subgraphs. Specifically, when \(_{c}\) contains parts of \(G_{s}\), assigning the same labels to the generated graph is more likely to _strengthen_ the spurious correlation between \(G_{s}\) and \(Y\). For example, when the model yields a reversed estimation, i.e., \(_{c}=G_{s}\) and \(_{s}=G_{c}\), the generated environment will destroy the invariant correlations.

**Proposition 3.2**.: _Consider the two-piece graph dataset \(_{}=\{(,_{1}),(,_{2})\}\) with \(_{1},_{2}\) (e.g., \(_{}=\{(0.25,0.1),(0.25,0.2)\}\)), and its corresponding mixed environment \(_{}^{}=\{(,(_{1}+_{2})/2)\}\) (e.g., \(_{}^{}=\{(0.25,0.15)\}\)). When \(_{c}=G_{s}\) and \(_{s}=G_{c}\), it holds that the augmented environment \(_{v}\) is also a two-piece graph dataset with_

\[_{v}=\{(0.5,(_{1}+_{2})/2)\}_{v}=\{(0.5,0.15)\}\}.\]

The proof is given in Appendix E.1. This also extends to the adversarial augmentation [72; 83], which will destroy the actual \(_{c}\). As both DIR and GREA adopt the same environment generation procedure, we verify the failures of environment generation with GREA in Table 2 of Sec. 5, where GREA can perform comparably with ERM. In fact, when the underlying environments are insufficient to differentiate the variations of the spurious features, it is fundamentally impossible to identify the underlying invariant graph from the spurious subgraph. More formally, if \( G_{s}\), such that \(P^{e_{1}}(Y|G_{s})=P^{e_{2}}(Y|G_{s})\) for any \(e_{1},e_{2}_{}\), where \(P^{e}(Y|G_{s})\) is the conditional distribution \(P(Y|G_{s})\) under environment \(e_{}\), it is impossible for any graph learning algorithm to identify \(G_{c}\). We provide a formal discussion in Appendix E.2. The failure implies a fundamental requirement that \(_{}\) should uncover all the potential variations in the spurious subgraph.

**Assumption 3.3**.: _(Variation sufficiency) For graphs generated following Fig. 2, for any \(G_{s}\), \( e_{1},e_{2}_{}\), such that \(P^{e_{1}}(Y|G_{s}) P^{e_{2}}(Y|G_{s})\), and \(P^{e_{1}}(Y|G_{c})=P^{e_{2}}(Y|G_{c})\)._

Assumption 3.3 aligns with the definition of invariance [8; 30] that the invariant subgraph \(G_{c}\) is expected to satisfy \(P^{e_{1}}(Y|G_{c})=P^{e_{2}}(Y|G_{c})\) for \(e_{1},e_{2}_{}\). If there exists \(G_{s}\) satisfying the invariance condition as well, then it is impossible to tell \(G_{c}\) from \(G_{s}\) even with environment labels.

### Pitfalls of environment inferring

Although environment sufficiency (Assumption 3.3) relieves the need for generating new environments, is it possible to infer the underlying environment labels via approaches such as MoleOOD and GIL, to facilitate invariant graph learning? Unfortunately, we find a negative answer.

Considering the two-piece graph examples \(_{}=\{(0.2,0.1),(0.2,0.3)\}\), when given the underlying environment labels, it is easy to identify the invariant subgraphs from spurious subgraphs. However, when the environment labels are not available, we have the mixed data as \(_{}=\{(0.2,0.2)\}\), where \(P(Y|G_{c})=P(Y|G_{s})\). The identifiability of \(G_{s}\) is _ill-posed_, as it does not affect the \(_{}\) even if we swap \(G_{c}\) and \(G_{s}\). More formally, considering the environment mixed from two two-piece graph environments \(\{(,_{1})\}\) and \(\{(,_{2})\}\), then we have \(_{}=\{(,(_{1}+_{2})/2\}\). For each \(_{}\), we can also find a corresponding \(_{}^{}=\{((_{1}^{}+_{1}^{})/2,^{})\}\) with \(\{(_{1}^{},^{})\}\) and \(\{(_{2}^{},^{})\}\). Then, let

\[=(_{1}^{}+_{1}^{})/2=^{}=(_{1}+ _{2})/2. \]

We now obtain \(_{}\) and \(_{}^{}\) which share the same joint distribution \(P(Y,G)\) while the underlying \(G_{c}\) is completely different. More generally, we have the following proposition.

**Proposition 3.4**.: _There exist \(2\) two-piece graph training environments \(_{}\) and \(_{}^{}\) that share the same joint distribution \(P(Y,G)\). Any learning algorithm will fail in either \(_{}\) or \(_{}^{}\)._

The proof is given in Appendix E.3. The experiments in Sec. 5 validate that both MoleOOD and GIL fail to infer faithful environment labels and even underperform ERM. It implies that whenever it allows the existence of an identical training distribution by mixing the environments, invariant graph learning is impossible. Therefore, we need an additional assumption that excludes the unidentifiable case. We propose to constrain the relationship between \(\) (i.e., \(H(Y|G_{c})\) ) and \(_{c}\) (i.e., \(H(Y|G_{s})\)).

**Assumption 3.5**.: _(Variation consistency) For all environments in \(_{}\), \(H(C|Y) H(S|Y)\)._

Intuitively, Assumption 3.5 imposes the consistency requirement on the correlation strengths between invariant and spurious subgraphs with labels. For two-piece graphs with consistent variations, mixing up the environments will yield a new environment with the same variation strength relationships.

Thus, Assumption 3.5 gets rid of the previous unidentifiable cases. Moreover, Assumption 3.5 also aligns with many realistic cases. For example, the relation of a specific functional group (e.g., -OH) with a molecule can hardly be reversed to that held upon the scaffold of the molecule, due to the data collection process. Therefore, Assumption 3.5 also resolves the counterexample proposed by Lin et al. . Different from our work, Lin et al.  propose to incorporate additional auxiliary information that satisfies certain requirements to mitigate the unidentifiable case. However, such auxiliary information is often unavailable and expensive to obtain on graphs. More importantly, the requirements are also unverifiable without more assumptions, which motivates us to consider the relaxed case implied by Assumption 3.5.

### Challenges of environment augmentation

To summarize, the two assumptions constitute the minimal assumptions for feasible invariant graph learning. Failing to satisfy either one of them while lacking additional inductive biases will result in the "no free lunch" dilemma  and suffer from the unidentifiability issue.

**Corollary 3.6**.: _(No Free Graph OOD Lunch) Without Assumption 3.3 or Assumption 3.5, there does not exist a learning algorithm that captures the invariance of the two-piece graph environments._

Corollary 3.6 is a natural conclusion from the previous discussion. The proof is straightforward and given in Appendix E.4. Assumption 3.3 and Assumption 3.5 establish the minimal premises for identifying the underlying invariant subgraphs. However, it also raises new challenges, as shown in Table. 1. Chen et al.  propose CIGA to maximize the intra-class mutual information of the estimated invariant subgraphs to tackle the case when \(H(C|Y)<H(S|Y)\). While for the case when \(H(S|Y)<H(C|Y)\), Fan et al.  propose DisC that adopts GCE loss  to extract the spurious subgraph with a larger learning step size such that the left subgraph is invariant. However, both of them can fail when there is no prior knowledge about the relations between \(H(C|Y)\) and \(H(S|Y)\). We verify the failures of DisC and CIGA in Table. 2. The failure thus raises a challenging question:

_Given the established minimal assumptions, is there a unified framework that tackles both cases when \(H(C|Y)<H(S|Y)\) and \(H(C|Y)>H(S|Y)\)?_

## 4 Learning Invariant Graph Representations with Environment Assistant

We give an affirmative answer by proposing a new framework, GALA: Graph invAriant Learning Assistant, which adopts an assistant model to provide proxy information about the environments.

### Learning with An Environment Assistant

Intuitively, a straightforward approach to tackle the aforementioned challenge is to extend the framework of either DisC  or CIGA  to resolve the other case. As DisC always destroys the first learned features and tends to be more difficult to extend (which is empirically verified in Sec. 5), we are motivated to extend the framework of CIGA to resolve the case when \(H(S|Y)<H(C|Y)\).

**Understanding the success and failure of CIGA.** The principle of CIGA lies in maximizing the intra-class mutual information of the estimated invariant subgraphs, i.e.,

\[_{f_{c},g}\ I(_{c};Y),\ \ _{c}*{ arg\,max}_{_{c}=g(G),|_{c}| s_{c}}I(_{c}; _{c}^{s}|Y), \]

where \(_{c}^{s}=g(G^{s})\) and \(G^{s}(G|Y)\), i.e., \(\) is sampled from training graphs that share the same label \(Y\) as \(\). The key reason for the success of Eq. 3 is that, given the data generation process as in Fig. 2 and the same \(C\), the underlying invariant subgraph \(G_{c}\) maximizes the mutual information of subgraphs from any two environments, i.e., \( e_{1},e_{2}_{}\),

\[G_{c}^{e_{1}}*{arg\,max}_{_{c}^{e_{1}}}I(_{c}^{e_{1}};_{c}^{e_{2}}|C), \]

   & \(H(S|Y)<H(C|Y)\) & \(H(S|Y)>H(C|Y)\) \\  DisC & ✓ & ✗ \\  CIGA & ✗ & ✓ \\  GALA (Ours) & ✓ & ✓ \\  

Table 1: Remaining challenges of invariant graph learning: no existing works can handle both cases.

where \(_{c}^{e_{1}}\) and \(_{c}^{e_{2}}\) are the estimated invariant subgraphs corresponding to the same latent causal variable \(C=c\) under the environments \(e_{1},e_{2}\), respectively. Since \(C\) is not observable, CIGA adopts \(Y\) as a proxy for \(C\), as when \(H(S|Y)>H(C|Y)\), \(G_{c}\) maximizes \(I(_{c}^{e_{1}};_{c}^{e_{2}}|Y)\) and thus \(I(_{c};_{c}^{s}|Y)\). However, when \(H(S|Y)<H(C|Y)\), the proxy no longer holds. Given the absence of \(E\), simply maximizing intra-class mutual information favors the spurious subgraph \(G_{s}\) instead, i.e.,

\[G_{s}_{_{c}}(_{c};_{c}^{s}|Y). \]

**Invalidating spuriousness dominance.** To mitigate the issue, we are motivated to find a new proxy that samples \(_{c}\) for Eq. 5, while preserving only the \(G_{c}\) as the solution under both cases.

To begin with, we consider the case of \(H(S|Y)<H(C|Y)\). Although the correlation between \(G_{s}\) and \(Y\) dominates the intra-class mutual information, Assumption 3.3 implies that there exists a subset of training data where \(P(Y|G_{s})\) varies, while \(P(Y|G_{c})\) remains invariant. Therefore, the dominance of spurious correlations no longer holds for samples from the subset. Incorporating samples from the subset into Eq. 3 as \(_{c}^{s}\) invalidates the dominance of \(G_{s}\). Denote the subset as \(\{_{c}^{n}\}\), then

\[G_{c}_{_{c}^{p}}I(_{c}^{p};_{c}^{n }|Y), \]

where \(_{c}^{p}\{_{c}^{p}\}\) is sampled from the subset \(\{_{c}^{p}\}\) dominated by spurious correlations, while \(_{c}^{n}\{_{c}^{n}\}\) is sampled from the subset \(\{_{c}^{n}\}\) where spurious correlation no long dominates, or is dominated by invariant correlations. We prove the effectiveness of Eq. 6 in Theorem 4.1.

**Environment assistant model \(A\).** To find the desired subsets \(\{_{c}^{p}\}\) and \(\{_{c}^{n}\}\), inspired by the success in tackling spuriousness-dominated OOD generalization via learning from a biased predictors , we propose to incorporate an assistant model \(A\) that is prone to spurious correlations. Simply training \(A\) with ERM using the spuriousness-dominated data enables \(A\) to learn spurious correlations, and hence identifies the subsets where the spurious correlations hold or shift, according to whether the predictions of \(A\) are correct or not, respectively. Let \(A=_{}I((G);Y)\), we have

\[\{_{c}^{p}\}=\{g(G_{i}^{p})|A(G_{i}^{p})=Y_{i}\},\;\{_{c }^{n}\}=\{g(G_{i}^{n})|A(G_{i}^{n}) Y_{i}\}. \]

**Reducing to invariance dominance case.** After showing that Eq. 6 resolves the spuriousness dominance case, we still need to show that Eq. 6 preserves \(G_{c}\) as the only solution when \(H(S|Y)>H(C|Y)\). Considering training \(A\) with ERM using the invariance-dominated data, \(A\) will learn both invariant correlations and spurious correlations . Therefore, \(\{_{c}^{n}\}\) switches to the subset that is dominated by spurious correlations, while \(\{_{c}^{p}\}\) switches to the subset dominated by invariant correlations. Then, Eq. 6 establishes a lower bound for the intra-class mutual information, i.e.,

\[I(_{c}^{p};_{c}^{n}|Y) I(_{c};_ {c}^{s}|Y), \]

where \(_{c}^{p}\{_{c}^{p}\},_{c}^{n}\{_{c}^{n}\}\), and \(_{c},_{c}^{s}\) are the same as in Eq. 3. The inequality in Eq. 8 holds as any subgraph maximizes the left hand side can also be incorporated in right hand side, while the sampling space of \(_{c}\) and \(_{c}^{s}\) in the right hand side (i.e., both \(_{c}\) and \(_{c}^{s}\) are sampled from the whole train set) is larger than that of the left hand side. The equality is achieved by taking the ground truth \(G_{c}\) as the solution for the featurizer \(g\). We verify the correctness of Eq. 6 and Eq. 8 in Fig. 3(a).

### Practical implementations.

The detailed algorithm description of GALA is shown as in Algorithm 1. In practice, the environment assistant can have multiple implementation choices so long as it is prone to distribution shifts. As discussed in Sec. 4.1, ERM trained model can serve as a reliable environment assistant, since ERM tends to learn the dominant features no matter whether the features are invariant or spurious. For example, when \(H(S|Y)<H(C|Y)\), ERM will first learn to use spurious subgraphs \(G_{s}\) to make predictions. Therefore, we can obtain \(\{G^{p}\}\) by finding samples where ERM correctly predicts the labels, and \(\{G^{n}\}\) for samples where ERM predicts incorrect labels. In addition to label predictions, the clustering predictions of the hidden representations yielded by environment assistant models can also be used for sampling \(\{G^{p}\}\) and \(\{G^{n}\}\). Besides, we can also incorporate models that are easier to overfit to the first dominant features to better differentiate \(\{G^{p}\}\) from \(\{G^{n}\}\). When the number of positive or negative samples is imbalanced, we can upsample the minor group to avoid trivial solutions. In addition, the final GALA objective is given in Eq. 9 and implemented as in Eq. 18. We provide more discussions about the implementation options in Appendix F.

### Theoretical analysis

In the following theorem, we show that the GALA objective derived in Sec. 4.1 can identify the underlying invariant subgraph and yields an invariant GNN defined in Sec. 2.

**Theorem 4.1**.: _Given i) the same data generation process as in Fig. 2; ii) \(_{}\) that satisfies variation sufficiency (Assumption 3.3) and variation consistency (Assumption 3.5); iii) \(\{G^{p}\}\) and \(\{G^{n}\}\) are distinct subsets of \(_{}\) such that \(I(G^{p}_{s};G^{n}_{s}|Y)=0\), \( G^{p}_{s}=_{^{p}_{s}}I(^{p}_{s};Y)\) under \(\{G^{p}\}\), and \( G^{n}_{s}=_{^{n}_{s}}I(^{n}_{s};Y)\) under \(\{G^{n}\}\); suppose \(|G_{c}|=s_{c},\; G_{c}\), resolving the following GALA objective elicits an invariant GNN defined via Eq. 1,_

\[_{f_{c},g}\;I(_{c};Y),\;\;g*{arg\, max}_{,|^{p}_{c}| s_{c}}I(^{p}_{c};^{n}_{c}|Y), \]

_where \(^{p}_{c}\{^{p}_{c}=g(G^{p})\}\) and \(^{n}_{c}\{^{n}_{c}=g(G^{n})\}\) are the estimated invariant subgraphs via \(g\) from \(\{G^{p}\}\) and \(\{G^{n}\}\), respectively._

The proof is given in Appendix E.5. Essentially, assumption iii) in Theorem 4.1 is an implication of the variation sufficiency (Assumption 3.3). When given the distinct subsets \(\{G^{p}\}\) and \(\{G^{n}\}\) with different relations of \(H(C|Y)\) and \(H(S|Y)\), since \(H(C|Y)\) remains invariant across different subsets, the variation happens mostly to the spurious correlations between \(S\) and \(Y\). By differentiating spurious correlations into distinct subsets, maximizing the intra-class mutual information helps identify the true invariance. The fundamental rationale for why GALA resolves two seemingly conversed cases essentially relies on the commutative law of mutual information.

## 5 Experiments

We evaluated GALA with both synthetic and realistic graph distribution shifts. Specifically, we are interested in the following two questions: (a) Can GALA improve over the state-of-the-art invariant graph learning methods when the spurious subgraph has a stronger correlation with the labels? (b) Will GALA affect the performance when the invariant correlations are stronger?

### Datasets and experiment setup

We prepare both synthetic and realistic graph datasets containing various distribution shifts to evaluate GALA. We will briefly introduce each dataset and leave more details in Appendix G.1.

**Two-piece graph datasets.** We adopt BA-2motifs  to implement \(4\) variants of \(3\)-class two-piece graph (Def. 3.1) datasets. The datasets contain different relationships of \(H(C|Y)\) and \(H(S|Y)\) by controlling the \(\) and \(\) in the mixed environment, respectively. We consider \(4\) cases of \(-\), ranging from \(\{+0.2,+0.1,-0.1,-0.2\}\), to verify our discussion in Sec. 4.3.

**Realistic datasets.** We also adopt datasets containing various realistic graph distribution shifts to comprehensively evaluate the OOD performance of GALA. We adopt \(6\) datasets from DrugOOD benchmark , which focuses on the challenging real-world task of AI-aided drug affinity prediction. The DrugOOD datasets include splits using Assay, Scaffold, and Size from the EC50 category (denoted as **EC50-***) and the Ki category (denoted as **Ki-***). We also adopt graphs converted from the ColoredMNIST dataset  using the algorithm from Knyazev et al. , which contains distribution 

[MISSING_PAGE_FAIL:9]

**OOD generalization in realistic graphs.** The results in realistic datasets are reported in Table 3. Aligned with our previous discussion, existing environment augmentation approaches sometimes yield better performance than ERM, such as CAL in EC50-Size, MoeOOD in Ki-Assay, GIL in Graph-SST2, or CIGA in EC50-Size, however, inevitably fail to bring consistent improvements than ERM, due to the existence of failure cases. DisC is suspected to work only for graph distribution shifts on node features and bring impressive improvements in CMNIST-sp, but can destroy the learned information under more challenging settings. In contrast, GALA consistently outperform ERM by a non-trivial margin in all datasets. Notably, GALA achieves near oracle performance in CMNIST-sp and improves CIGA by \(53\%\). The consistent improvements of GALA confirm the effectiveness of GALA.

**Correlation strengths of \(\{G^{p}\}\) and \(\{G^{n}\}\).** We conduct experiments with the two-piece graph datasets evaluated in Table 2 to verify the correctness of Eq. 6 and Eq. 8. Eq. 6 and Eq. 8 imply that the underlying invariant subgraph will be the subgraph that maximizes the mutual information among subgraphs from \(\{G^{p}\}\) and \(\{G^{n}\}\), no matter whether the dominant correlation is spurious or not. We measure the invariant and spurious correlation strengths in terms of co-occur probability of the invariant and spurious subgraphs with the labels. The results are shown in Fig. 3(a). It can be found that, under both cases, the underlying invariant subgraph maintains the predictivity with the label in an invariant manner. Hence, maximizing the intra-class subgraph mutual information between \(\{G^{p}\}\) and \(\{G^{n}\}\) in GALA succeeds in identifying the underlying invariant subgraph.

**CIGAv2 compatibility.** Although GALA focuses on the contrastive term in CIGA, both GALA and CIGA are compatible with the additional CIGAv2 term that facilitates constraining the graph sizes. To verify, we compare the OOD performances of CIGA, CIGAv2, GALA, and GALA +CIGAv2 using two challenging datasets, Ki-Scaffold and CMNIST-sp. The results are given in Fig. 3(b). It can be found that, despite incorporating the additional CIGAv2 constraint, CIGA can not outperform GALA, while GALA can bring more improvements with the additional CIGAv2 constraint. In CMNIST-sp, since GALA already achieve the upper bound, incorporating CIGAv2 can only achieve a similar result.

**Hyperparameter sensitivity.** We also test the hyperparameter sensitivity of GALA to the contrastive penalty weights as well as the upsampling times that are introduced to mitigate the imbalance of positive and negative graphs. We conduct the experiments with two-piece graph dataset \(\{0.7,0.9\}\). As shown in Fig. 3(c), it can be found that GALA is generically robust to different hyperparameter choices. In addition, when the penalty weight or the upsampling times turn to \(0\), the performance will decrease a lot, which serves as strong evidence for the effectiveness of GALA.

**Computational analysis.** We also conduct computational analysis of GALA and other methods, and defer the results to Table. 6 in Appendix G.4, due to space constraints. The results show that GALA costs only a competitive training time as environment generation based methods, while achieving much better OOD generalization performance.

## 6 Conclusions

We conducted a retrospective study on the faithfulness of the augmented environment information for OOD generalization on graphs. By showing hardness cases and impossibility results of the existing approaches, we developed a set of minimal assumptions for feasible invariant graph learning. Built upon the assumptions, we proposed GALA to learn the invariant graph representations guided by an environment assistant model. Extensive experiments with \(12\) datasets verified the superiority of GALA.

Figure 3: Ablation studies.