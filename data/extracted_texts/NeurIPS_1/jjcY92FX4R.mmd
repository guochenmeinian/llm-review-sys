# A Canonicalization Perspective on

Invariant and Equivariant Learning

 George Ma\({}^{*}\)1  Yifei Wang\({}^{*}\)2  Derek Lim\({}^{2}\)  Stefanie Jegelka\({}^{3}\)  Yisen Wang\({}^{4,5}\)\({}^{}\)

\({}^{1}\) School of EECS, Peking University

\({}^{2}\) MIT CSAIL

\({}^{3}\) TUM CIT/MCML/MDSI & MIT EECS/CSAIL

\({}^{4}\) State Key Lab of General Artificial Intelligence,

School of Intelligence Science and Technology, Peking University

\({}^{5}\) Institute for Artificial Intelligence, Peking University

Equal Contribution. George Ma has graduated from Peking University, and is currently a Ph.D. student at UC Berkeley.

###### Abstract

In many applications, we desire neural networks to exhibit invariance or equivariance to certain groups due to symmetries inherent in the data. Recently, frame-averaging methods emerged to be a unified framework for attaining symmetries efficiently by averaging over input-dependent subsets of the group, _i.e._, frames. What we currently lack is a principled understanding of the design of frames. In this work, we introduce a canonicalization perspective that provides an essential and complete view of the design of frames. Canonicalization is a classic approach for attaining invariance by mapping inputs to their canonical forms. We show that there exists an inherent connection between frames and canonical forms. Leveraging this connection, we can efficiently compare the complexity of frames as well as determine the optimality of certain frames. Guided by this principle, we design novel frames for eigenvectors that are strictly superior to existing methods--some are even optimal--both theoretically and empirically. The reduction to the canonicalization perspective further uncovers equivalences between previous methods. These observations suggest that canonicalization provides a fundamental understanding of existing frame-averaging methods and unifies existing equivariant and invariant learning methods. Code is available at [https://github.com/PKU-ML/canonicalization](https://github.com/PKU-ML/canonicalization).

## 1 Introduction

When designing machine learning models, incorporating data symmetry provides a strong inductive bias that facilitates learning and generalization , as evidenced in multiple applications like convolutional neural networks , graph neural networks , point clouds , _etc_. Often these symmetry priors require models to be invariant or equivariant to certain groups \(G\). Among these approaches, model-specific methods restrict every model component to respect data symmetries, which, however, often sacrifices expressive power . On the other hand, model-agnostic methods allow the use of arbitrary (non-invariant) base models, and ensure invariance or equivariance through averaging over group actions . This approach can attain universal expressive power with first-order backbones, but comes with high computation cost for exponentially large groups (_e.g._, permutation).

To alleviate the latter challenge, frame averaging (FA)  has been recognized to be a general framework that can achieve invariance and equivariance efficiently by averaging over a small (input-dependent) subset of the group \((X) G\), known as a frame. Frame averaging can improve the averaging complexity of orders of magnitude, and has found wide applications in multiple fields such as graph neural networks , materials modeling , antibodies generation , _etc._ Nevertheless, existing frames are still computationally prohibitive in many domains, in particular, exponentially large for graphs  and eigenvectors , making it an intriguing problem to explore the design of more efficient frames of lower complexity.

However, a key challenge in this direction is a lack of rigorous ways to characterize the complexity of frames, since existing frames in the literature are still heuristically designed. Although it sounds like a being only of theoretical interest, the ability to characterize the complexity and expressiveness of algorithms has played a vital role in the development of modern algorithms and deep learning models . As an example, the WL hierarchies [65; 54; 72; 71; 73], alongside other complexity measures , have been the guideline for developing expressive graph neural networks [17; 39; 74]. However, for averaging methods, we still lack a formal language to quantify, compare, and improve different approaches, which hinders principled development in this area.

In this paper, we propose _canonicalization_ as an essential view of frame averaging and a practical yet principled measure for the complexity of frames. Canonicalization is a classical technique with wide applications in graph theory , algebra  and geometry , and it has also recently been explored for learning with symmetry . The key idea of a canonicalization \(\) is to map all inputs that are equivalent under a group \(G\) to the same _canonical form_\((X)\). With the canonical input \((X)\), any (non-invariant) base neural network \(\) will give a \(G\)-invariant mapping. We show that for a given group, the differences between any frames can be reduced to the differences of their corresponding canonicalizations. Moreover, in contrast to frames that are generally hard to analyze, we show that the canonicalization perspective is much more fertile, leading to a set of theoretical conditions and practical principles for quantifying the complexity and expressiveness of different frames.

To illustrate the benefits of this canonicalization framework, we focus on the symmetries of eigenvectors (sign and basis group), which covers a wide range of applications like graph learning [36; 33], PCA methods  and spectral clustering . In this domain, we show that the canonicalization perspective now allows us to answer some long-standing theoretical questions and derive some better or even optimal frames. Specifically, we reveal that for sign invariance (a major concern for eigenvectors), _any_ sign-invariant network, including the popular SignNet , can be reduced to the _same_ input canonicalization, equivalently. Building on this fundamental result, we easily resolve the open problem of SignNet's universality, by showing that it is _not_ a universal approximator of functions that are sign invariant and permutation equivariant; we also show how to modify SignNet to be universal with minimal changes. Moreover, as a concrete, practical application, we develop new canonicalizations and frames for eigenvectors, named Orthogonal Axis Projection (OAP), which attain _optimal_ or at least _better-than-prior_ complexities for unconstrained and constrained scenarios.

At last, we validate our theoretical findings on the Exp dataset, showing that our canonicalization and frames indeed yield orders of lower complexity while demonstrating expressive power beyond 1-WL for distinguishing non-isomorphic graphs. We also show that permutation-equivariant OAP canonicalizes more eigenvectors than previous methods, resulting in better performance on the ZINC and OGBG molecular graph tasks.

## 2 Preliminaries

Let \(f\) be a function and \(G\) be a group acting on \(\) and \(\). We say \(f\) is _invariant_ to \(G\) (or \(G\)-invariant) if for all \(g G\) and \(X\), we have \(f(g X)=f(X)\). Similarly, we say \(f\) is _equivariant_ to \(G\) (or \(G\)-equivariant) if for all \(g G\) and \(X\) we have \(f(g X)=g f(X)\). We treat invariance as a special case of equivariance by letting \(G\) act on \(\) as the identity transformation. Existing research has developed neural networks with specific equivariance properties, such as permutation invariance/equivariance [70; 53], rotation invariance/equivariance [62; 60; 63], sign/basis invariance , sign equivariance , and multi-set equivariance .

In the rest of the paper, we define \(V,W\) to be vector spaces with norms \(\|\|_{V},\|\|_{W}\), and \(G\) be a group. The elements \(g G\) act on vectors in \(V,W\) with the group's representations \(_{1} G(V)\) and \(_{2} G(W)\), where \((V)\) is the space of invertible linear maps \(V V\). The group \(G\) inducesan equivalence relation on \(V\), such that \(u v\) if and only if there exists \(g G\) such that \(u=_{1}(g)v\). For each input \(X\), we denote its orbit or equivalence class by \(V_{G}(X)=\{_{1}(g)^{-1}X g G\}\), and its automorphism group as \(G_{X}=\{g G_{1}(g)X=X\}\).

### Averaging Methods

Given a network \( V W\), the naive way to achieve invariance is through _group averaging_:

\[_{}(X)=_{g G}_{1}(g )^{-1}X.\]

The resulting \(\) is \(G\)-invariant, and preserves universal expressive power if \(\) is itself universal. Some existing works adopt this approach [40; 69]. However, exact averaging becomes intractable if the cardinality of \(G\) is large, for example, the permutation group of graphs. In such cases, random sampling of the group is necessary [40; 14], at the sacrifice of exact symmetry.

Frame averaging  is another approach to achieve exact invariance by averaging over a subset of the group on an input \(X\), _i.e.,_ a frame \((X) G\) that maintains \(G\)-equivariance: \((_{1}(g)X)=g(X)=\{gh h(X)\}\). If an equivariant frame \(\) is easy to compute, and its cardinality \(|(X)|\) is not too large, then the following frame averaging scheme

\[_{}(X;,)=(X)|} _{g(X)}_{1}(g)^{-1}X \]

also provides the required function symmetrization. \(_{}\) is \(G\)-invariant, and universally approximates \(G\)-invariant functions that are approximable by \(\). Equivariance can be achieved similarly by multiplying \(_{2}(g)\) with each term. Puny et al.  also proposed a variant called _invariant frame averaging_ by averaging over the cosets \((X)/G_{X}\) to achieve invariance. Since there is no established way to find the representatives of \((X)/G_{X}\), they adopt uniform sampling from \((X)\) to approximate \(_{}\). In Section 3.1, the proposed canonicalization achieves the same complexity without sampling.

Kaba et al.  proposed to learn an equivariant canonicalization function for equivariance. They define the canonicalization function to be \(h V G\) that is \(G\)-equivariant. Let \(\) be the backbone network, then the network taking form \(_{2}(h(X))(_{1}(h(X))^{-1}X)\) is equivariant and universal. The canonicalization \(h\) can be seen as a frame whose output has size \(1\). However, such a \(G\)-equivariant \(h\) that outputs a single group element does not exist for inputs \(X\) with non-trivial automorphism (they introduce a relaxed version of equivariance in this case). Following the classic literature, we define canonicalization on the input space \(V\) instead of the group space \(\), which also avoids the problem above.

### Symmetries of Eigenvectors

Denote the Laplacian matrix of a graph as \(=-}\), where \(}\) is the normalized adjacency matrix. Laplacian Positional Encoding (LapPE) uses the eigenvectors of \(\) as positional encoding. LapPE enjoys the benefits of having permutation equivariance and universal expressive power , but also suffers from two well-known _ambiguity_ problems. The first one, known as _sign ambiguity_, captures that for a unit-norm eigenvector \(_{_{i}}\) corresponding to eigenvalue \(_{i}\), the sign flipped \(-_{_{i}}\) is also a unit-norm eigenvector of the same eigenvalue. The second one, termed _basis ambiguity_, captures that eigenvalues with multiplicity degree \(d_{i}>1\) can have any orthogonal basis in its eigenspace as valid eigenvectors. Because of these ambiguities, we can get distinct GNN outputs for the same graph, resulting in unstable and sub-optimal performance [14; 29; 33]. Besides, sign and basis ambiguities also exist in general eigenvectors that do not require permutation equivariance, which are also widely used in real-world applications, such as PCA methods  and spectral clustering . We defer more background to Appendix D.

## 3 Canonicalization: A Unified and Essential View of Equivariant Learning

In this section, we reduce existing averaging methods to canonicalization and show canonicalization can serve as a unified perspective of these methods. Then, using insights from canonicalization theory, we show how SignNet and BasisNet--two invariant networks on eigenvectors--are equivalent to their underlying canonicalizations, while solving the open problem regarding their expressivity.

### A Reduction from Frames to Canonicalizations

Frame averaging reduces the number of forward passes in the averaging step compared with group averaging. However, a major obstacle in analyzing the complexity of frames are the automorphisms of the input: \(G_{X}=\{g G_{1}(g)X=X\}\), which can be exponentially large and intractable to compute. For instance, consider defining a frame of a graph as the set of all permutations that sort its node features in increasing order. If there are \(m\) nodes in the graph with identical node features, then the frame size is at least \(m!\), which grows exponentially. However, all permutations in the frame actually result in the same graph, indicating room for improvement in the efficiency of frame averaging.

In this work, we propose an alternative view to design frames that overcome the difficulties above. For each \(X V\), instead of averaging over the group elements as in FA (Eq. 1), one can directly average over the _output_ elements of the transformations, which is not affected by the complexity of automorphisms since all automorphisms in \(G_{X}\) yield the same output. This converts the problem of finding a \(G\)-equivariant subset of the group to finding a \(G\)-invariant set of inputs. In fact, in the classic literature, this problem is known as canonicalization, that achieves invariance by mapping inputs in the same equivalence class to the same canonical form. Formally, we define a _canonicalization_ as a set-valued function \( V 2^{V}\), such that it is \(G\)-invariant: \((_{1}(g)X)=(X), X V,g G\). We call its output \((X)\) the _canonical form_ of \(X\). Among possible canonicalizations, we are most interested in _orbit canonicalization_, where the canonical form falls back into the equivalence class: \((X) V_{G}(X)\) for all \(X V\). With a canonical form, one can instead perform canonical averaging (CA) over \((X)\) to obtain invariant3 representations:

\[_{}(X;,)=(X)|}_ {X_{0}(X)}(X_{0}). \]

The following theorem establishes the equivalence between canonicalizations and frames.

**Theorem 3.1**.: _For any frame \(\) there exists an orbit canonicalization \(_{}\) s.t. for all \(X V\), \(g G\), and backbone \(\), we have \(_{}(X;,)=_{}(X;_{ },)\) and_

\[|_{}(X)|=|(X)|/|G_{X}||(X)|.\]

_In turn, for any orbit canonicalization \(\) there exists a frame \(_{}\) s.t. for all \(X V\), \(g G\), and backbone \(\), we have \(_{}(X;,)=_{}(X;_{ },)\) and_

\[|_{}(X)|=|G_{X}||(X)||(X)|.\]

Theorem 3.1 reveals that frames and canonicalizations have a fundamental equivalence. In other words, a frame is built upon a canonicalization, and a canonicalization induces a frame. Henceforth, we can reduce the difficult problem of designing (equivariant) frames to designing canonicalizations. This simple change of perspective also allows us to have a better theoretical characterization of the complexity and optimality of frames. From now on, we adopt a canonicalization language and reveal some key properties of canonicalization that provide principled guidelines for frame/canonicalization design.

### Theoretical Properties of Canonicalization: Universality, Optimality, and Canonicalizability

Here, we examine the theoretical properties of canonicalization in terms of its expressiveness and efficiency. Subsequently, we present unique insights provided by the canonicalization perspective that are not found in the existing literature on frames.

For expressiveness, we define the universality of canonicalization as follows. A canonicalization \(\) is _universal_ if for any \(G\)-invariant function \(f V W\), there exists a well-defined function \( 2^{V} W\) such that \(f(X)=((X))\) for all \(X V\). The following theorem shows that a canonicalization is universal _iff_ it corresponds to an orbit canonicalization.

**Theorem 3.2**.: _A canonicalization \(\) is universal iff there exists an orbit canonicalization \(_{c}\) and an injective mapping \(g 2^{V} 2^{V}\) such that \(g((X))=_{c}(X), X V\)._For a formal analysis of efficiency, we refer to \(|(X)|\) as the _complexity_ of a canonicalization on \(X V\). A canonicalization \(\) is _superior_ to another canonicalization \(^{}\) if it has smaller complexity on all elements: \(|(X)||^{}(X)|, X V\). A canonicalization \(\) is _optimal_ if it is superior to any canonicalization. The proposition below establishes the universality and invariance of canonical averaging.

**Theorem 3.3**.: _Let \(\) be an orbit canonicalization. The canonical average \(_{}\) is \(G\)-invariant. As long as the backbone network \(\) is universal, \(_{}\) is universal in the sense that it can approximate any continuous \(G\)-invariant function \(f V W\) up to arbitrary precision._

The canonicalization size \(|(X)|\) may differ for different inputs. In the most ideal case, an input \(X\) admits a single canonical form with \(|(X)|=1\), which we call \(X\) a _canonicalizable_ element. Formally, an element \(X V\) is _canonicalizable_ if there exists an orbit canonicalization \(\) such that \(|(X)|=1\). Otherwise, we call it _uncanonicalizable_, which may happen under additional constraints on the canonicalization. For example, one cannot determine a canonical sign for \([-1,1]\) when the canonicalization is required to be permutation equivariant, according to Ma et al. .

In fact, a major advantage of transforming frames into canonicalization lies in identifying _uncanonicalizable_ inputs when additional constraints are imposed, a feature absent in existing literature on frames. In particular, the canonicalizability of inputs offers novel insights into the expressive power of **invariant networks with equivariance constraints**. As will be demonstrated in Section 3.3, invariant networks like SignNet lose expressive power on uncanonicalizable inputs. This enables us to prove the non-universality of SignNet, resolving an open problem in the literature . Canonicalizability is a property of inputs, making it sensible to describe it using canonicalization rather than frames. We refer to Appendix A for a comprehensive discussion on the advantages of the canonicalization perspective.

### Reducing Sign-Invariant Networks to Canonicalization

In this section we show how canonicalization can be applied to the eigenvector ambiguity problem, and solve an open question regarding the expressiveness of invariant networks.

There are two general methods to attain exact sign and basis invariance for eigenvectors: SignNet and BasisNet  and Laplacian Canonicalization . SignNet is parameterized as the network \(f^{n k}^{d}\) on eigenvectors \(_{1},,_{k}\) as

\[f(_{1},,_{k})=[(_{i})+(-_{i}) ]_{i=1}^{k},\]

where \(\) and \(\) are unrestricted neural networks and \([]_{i}\) denotes concatenation of vectors. Ma et al.  instead achieved sign invariance on canonicalizable input with

\[f(_{1},,_{k})=[(_{i})]_{i=1}^{ k},\]

where \(\) denotes their canonicalization algorithm MAP, which leverages the sign-invariant and permutation-equivariant projection operator to find a canonical sign. Empirically, the two approaches attain comparable performance in practice. Meanwhile, MAP enjoys better computational efficiency because it only requires input pre-processing while SignNet requires two-branch encoding. Although the two algorithms seem rather different, we prove that any permutation-equivariant and sign-invariant function, including SignNet, is equivalent to a close variant of MAP, which we call \(_{++}\).

**Theorem 3.4**.: _A function \(h^{n}^{n d_{}}\) is permutation equivariant and sign invariant iff there exists a permutation equivariant (w.r.t. its first input) function \(^{n}\{0,1\}^{n d_{}}\) such that_

\[h()=_{++}(),_{}, ^{n},\]

_where \(_{++}()=(),&$ is canonicalizable},\\ ||,&\) and \(_{}\{0,1\}\) indicates whether \(\) is canonicalizable. Here \(||\) denotes element-wise absolute value._

Theorem 3.4 indicates that any sign invariant and permutation equivariant function on a single eigenvector can be reduced to a certain mapping based on the MAP++ canonicalization, which allows a unified characterization for such functions. However, when taking the entire eigenvector matrix as input, processing each eigenvector alone (as in SignNet) will take their absolute values (Theorem 3.4),which inevitably loses relative position information between different eigenvectors. As a result, both SignNet and MAP++ are not universally expressive. The same result also holds for BasisNet, since SignNet is a special case of first-order BasisNet with multiplicity \(d=1\).4 The universality of SignNet has been an open problem in the literature [33; 32] and we show that a reduction to the canonicalization perspective can provide a fundamental solution to such problems. Concretely, we also construct two non-isomorphic graphs that SignNet fails to distinguish in Appendix E.5.

**Corollary 3.5**.: _SignNet and BasisNet with first-order permutation equivariant \(\) cannot universally approximate all permutation-equivariant and sign/basis-invariant functions._

## 4 Exploring Optimal Canonicalization of Eigenvectors

In this section, we delve into the sign and basis ambiguity problems of eigenvectors and graph positional encodings. We propose novel canonicalization algorithms that are provably superior to existing approaches and even optimal. Specifically, we aim to design a canonicalization algorithm \(\) operating on eigenvectors \(^{n d}\), that is invariant to sign/basis transformations, (possibly) equivariant to permutation transformations, and outputs a set of eigenvectors \(^{*}^{n d}\) in the same eigenspace as \(\). We consider two settings: without (Section 4.1) and with (Section 4.2) permutation equivariance, corresponding to different problem scenarios.

### Optimal Canonicalization without Permutation

First, we consider the case when we do not need to consider the permutation equivariance of eigenvectors, for example, when samples have a specific ordering. Applications broadly include control systems , image segmentation , source separation , fluid dynamics , _etc_.

Sign Invariance.Although SignNet can also be applied to such cases, we show that a simple canonicalization that determines directions with the first non-zero entry of the eigenvector \(\) can also achieve sign invariance and permutation equivariance while preserving universality.

```
0: The eigenvector \(^{n}\)
0: The canonical form \(^{*}\) of \(\)
0: Let \(i\) be the smallest index such that \(u_{i} 0\)
0:\(^{*}\) if \(u_{i}>0\), \(^{*}-\) otherwise
```

**Algorithm 1** Canonicalization for eliminating sign ambiguity of eigenvectors

Basis Invariance.Inspired by MAP-basis , we design a more general and powerful canonicalization based on the Gram-Schmidt Orthogonalization of projection vectors, named _orthogonalized axis projection_ (OAP). Specifically, let \(^{n d}\) be eigenvectors in a \(d\) dimensional eigenspace, and let \(=^{}\) be the projection matrix onto the eigenspace \(()\). Denote \(_{1},,_{n}\) as the standard axis vectors of \(^{n}\), that is, \(_{i}\) has \(1\) at the \(i\)-th entry and \(0\) at the other entries. The following Algorithm 2 eliminates basis ambiguities of all eigenvectors.

```
0: The eigenvectors \(^{n d}\)
0: The canonical form \(^{*}\) of \(\)
0: Let \(i_{1}<<i_{d}\) be the smallest indices _s.t._\(\|_{i_{j}}\|>0\) and \(_{i_{j}}\) are linearly independent, \(1 j d\)
0:\(^{*}(_{i_{1}},, _{i_{d}})\), where \(\) denotes Gram-Schmidt Orthogonalization
```

**Algorithm 2** OAP Canonicalization for eliminating basis ambiguity of eigenvectors

Intuitively, Algorithm 2 finds a set of standard basis vectors with the smallest indices such that their projection on the eigenspace is still a basis. We note that the indices \(i_{1},,i_{d}\) can be found by iteratively checking whether each \(_{i}\) is non-zero and linearly independent with the already-foundprojection vectors (\(i=1,,n\)), and adding them if they satisfy these conditions. This can be done in \((n^{3})\) time (the same as eigendecomposition). The following theorem guarantees that we can always find such indices.

**Theorem 4.1**.: _Given a set of eigenvectors \(^{n d}\), let \(=^{}\) denote the projection matrix of the eigenspace. Let \(_{1},,_{n}\) denote the standard basis vectors. Then, there exists indices \(1 i_{1}<<i_{d} n\), such that for all \(1 j d\), we have \(\|_{i_{j}}\|>0\), and the vectors \(_{i_{1}},,_{i_{d}}\) are linearly independent._

Optimality.We show that Algorithm 1 and 2 are optimal, and can canonicalize _all_ eigenvectors.

**Theorem 4.2**.: _Algorithm 1 is an optimal orbit canonicalization for all eigenvectors \(^{n}\) under sign ambiguity. Algorithm 2 is an optimal orbit canonicalization for all eigenvectors \(^{n d}\) under basis ambiguity._

These algorithms eliminate ambiguities of eigenvectors, which is useful in many applications. In Appendix D we show their application to achieve orthogonal equivariance in PCA-frame methods.

### Better Canonicalization with Permutation

In this section, we further consider the constraint of permutation equivariance when pursuing sign and basis invariance, which is important for certain data structures like graphs.

Sign Invariance.We can retain the universality of MAP and SignNet by extending them to perform canonical averaging (Section 3.1) on _uncanonicalizable inputs_ with the following MAP-full canonicalization

\[_{}()=(),& $ is canonicalizable},\\ \{,-\},&. \]

Since Ma et al.  proved that MAP canonicalizes all sign-canonicalizable eigenvectors, and if the eigenvector is uncanonicalizable, then the optimal size \(|()|\) is \(2\). Thus, MAP-full is optimal.

Basis Invariance.Contrary to the sign case, basis invariance is harder to obtain and MAP-basis proposed by Ma et al.  is known to be not optimal. Here, we propose a permutation-equivariant basis canonicalization that is provably superior to MAP-basis. Notice that in OAP (Algorithm 2), the way that we construct smallest indices \(i_{1},,i_{d}\) is not permutation equivariant. This motivates us to extend OAP with a permutation equivariant procedure to determine the indices. Specifically, we adopt the following permutation-equivariant hash function to rank axis projections:

\[_{i}=(p_{ii}, p_{ij}_{j i}),\ i=1, ,n. \]

where \(_{i}=_{i}\). According to the number of distinct values in \(\{_{i}\}\) (denoted as \(k\)), we divide all standard basis vectors \(\{_{i}\}\) into \(k\) disjoint groups \(_{i}\), in descending order of \(_{i}\). Then we define a summary vector \(_{i}\) for each group \(_{i}\) as \(_{i}=_{_{j}_{i}}e_{j}+c\), where \(c\) is a tunable constant. In this way, we arrive at a permutation-equivariant version of OAP for LapPE in Algorithm 3. The summary vectors \(\{_{i_{j}}\}\) are now permutation equivariant (Appendix B.1), in contrast to \(\{_{i_{j}}\}\) in Algorithm 2.

```
0: The Laplacian eigenvectors \(^{n d}\)
0: The canonical form \(^{*}\) of \(\)
0: Let \(}<<i_{d}\) be the smallest indices s.t. \(\|_{i_{j}}\|>0\) and \(_{i_{j}}\) are linearly independent, \(1 j d\)
0:\(^{*}(_{i_{1}},,_{i_{d}})\), where \(\) denotes Gram-Schmidt Orthogonalization
```

**Algorithm 3** OAP Canonicalization for eliminating basis ambiguity of Laplacian positional encoding

In contrast to Algorithm 2, the indices \(i_{1},,i_{d}\) may not always exist. The existence and values of these indices can be determined in \((n^{2}d^{2})\) time (_c.f._, Appendix B.3).

### OAP as a Unified Canonicalization for Eigenvectors

Another interesting aspect of OAP is that with a flexible choice of hashing values \(_{i}\) and indices \(i_{j}\), OAP can serve as a unified framework that encompasses many existing canonicalization algorithms for eigenvectors (as well as graphs).

First, it is easy to see that the sign canonicalizations (Algorithm 1 and MAP-sign) are special cases of their basis versions with \(d=1\). For basis canonicalization, there are two existing methods. One is MAP-basis  which also utilizes axis projection but a different construction of basis. Another is proposed in Puny et al.  as a way to construct frames for graphs which, according to Theorem 3.1, corresponds to a canonicalization for graphs and can also be adapted to canonicalize LapPE (we defer technical details to Appendix E.8). We call it FA-lap. The following theorem reveals that both MAP and FA-lap are special cases of OAP with degenerated hash functions.

**Theorem 4.3**.: _Let \(_{i}\)\((i=1,,n)\) be the outputs of the hash function in the OAP algorithm defined in Equation 4, and let \(i_{j}\)\((j=1,,d)\) be the indices found in Algorithm 3. Then, the MAP algorithm  is equivalent to the OAP algorithm that takes \(_{i}=\|_{i}\|\) for all \(1 i n\) and \(i_{j}=j\) for all \(1 j d\). The FA-lap algorithm  is equivalent to the OAP algorithm that takes \(_{i}=_{ii}\) for all \(1 i n\)._

From this connection, we can see that the hashes of MAP and FA-lap are not as distinctive as OAP with Eq. 4. As a result, OAP can canonicalize more eigenvectors and is strictly superior to MAP and FA-lap. There is no superiority between MAP and FA-lap. It remains an open problem whether the permutation-equivariant OAP (Algorithm 3) is optimal.

Similarly, these canonicalizations can be adapted to canonicalize graphs (which we call MAP-graph, FA-graph, OAP-graph), and have the same complexity hierarchy (more details in Appendix E.8). We summarize them as follows.

**Corollary 4.4**.: _For eigenvectors, OAP is strictly superior to MAP and FA-lap. For graphs, OAP-graph is strictly superior to MAP-graph and FA-graph._

## 5 Experiments

In this section, we evaluate the expressive power and efficiency of CA on the Exp dataset; we apply our canonicalization to the \(n\)-body problem for orthogonal equivariance; we also evaluate OAP for LapPE on graph regression tasks.

### Expressive Power and Frame Size

To validate the universal expressiveness of CA in Theorem 3.3, we conduct experiments on Exp, which is designed to explicitly evaluate the expressiveness of GNNs. It consists of pairs of graphs that are non-isomorphic but 1-WL indistinguishable. We follow the setup of Balcilar et al. . As baselines, we use GCN , GAT , GIN  and ChebNet , **all of which are permutation equivariant**. We also equip GIN with unique node IDs and apply FA/CA on them, which we denote as FA-GIN+ID and CA-GIN+ID. Note that node IDs endow GIN with universality  while also breaking permutation equivariance, and FA/CA restore permutation equivariance while preserving universality. Results are shown in Table 1. All MP-GNNs achieve trivial performance since their expressive power is limited by 1-WL . Both FA and CA achieve perfect accuracy, showing expressiveness beyond 1-WL.

To evaluate the efficiency of different methods, we also compare their average frame size \(|(X)|\) or canonicalization size \(|(X)|\) on Exp. As shown in Table 2, OAP-graph is more efficient than FA-graph, verifying Corollary 4.4. Since \(|(X)|<|(X)|\), CA is more efficient than FA, validating Theorem 3.1. We note that although the numbers are still very large in Table 2 and we still need sampling, a smaller frame/canonicalization size would lead to faster convergence rate towards the true average. This is proven in Appendix C.

### Graph Regression and Classification

We evaluate OAP on ZINC . We measure the ratio of non-canonicalizable5 eigenvectors among all eigenvectors by FA-lap, MAP, and OAP. As shown in Table 3, they are equivalent in addressing sign

   Model & Accuracy \\  GCN & 50\% \\ GAT & 50\% \\ GIN & 50\% \\ ChebNet & 82\% \\ FA-GIN+ID & 100\% \\ CA-GIN+ID & 100\% \\   

Table 1: Accuracy on Exp.

ambiguity, while OAP has the least ratio of non-canonicalizable eigenvectors under basis ambiguity, aligning with Corollary 4.4.

We conduct experiments on ZINC  and OGBG  (_c.f._, Appendix H). We use GatedGCN  and PNA  as backbones and apply different PE methods: (1) No positional encoding; (2) Laplacian PE combined with random sign (RS) that randomly flips the signs of eigenvectors ; (3) SignNet ; (4) MAP ; (5) OAP; (6) OAP with LSPE layers . On ZINC, we also evaluate the FA-GIN+ID model in Section 5.1, as well as GIN with edge features (_i.e._, GINE) . Methods implemented by ourselves are marked with *. The results are reported in Table 4, 5 and 6.

In Table 4, we observe that FA achieves the lowest performance since it breaks permutation equivariance. The results presented in Tables 4, 5 and 6 demonstrate that incorporating LapPE leads to improved performance across nearly all cases compared to no PE, highlighting the advantages of leveraging expressive PEs. Notably, SignNet, MAP, and OAP show significant performance gains over LapPE, emphasizing the benefits of addressing ambiguities of Laplacian eigenvectors. OAP performs best among these methods, consistent with our theoretical expectations of its superiority over MAP and SignNet. Additionally, SignNet experiences memory issues even with fewer parameters in Table 6, underscoring the increased memory demands associated with incorporating invariant network architectures. Furthermore, in Table 4, we find that LSPE enhances the performance of OAP across all models.

   Model & PE & \(k\) & \#Param & \(\) \\  GIN & ID + FA & 0 & 495K & \(0.613 0.023\) \\ GINE & ID + FA & 0 & 495K & \(0.546 0.048\) \\   & None & 0 & 504K & \(0.251 0.009\) \\  & LapPE + RS & 8 & 505K & \(0.202 0.006\) \\  & SignNet & 8 & 495K & \(0.121 0.005\) \\  & MAP & 8 & 486K & \(0.102 0.005\) \\  & OAP & 8 & 473K & \(\) \\  & OAP + LSPE & 8 & 491K & \(\) \\   & None & 0 & 369K & \(0.141 0.004\) \\  & LapPE + RS & 8 & 474K & \(0.132 0.010\) \\  & SignNet & 8 & 476K & \(0.105 0.007\) \\  & MAP & 8 & 462K & \(0.101 0.005\) \\  & OAP & 8 & 462K & \(\) \\  & OAP + LSPE & 8 & 549K & \(\) \\   

Table 4: Results on ZINC with 500K parameter budget. All scores are averaged over 4 runs with 4 different seeds.

   Method & Avg \(|(X)|\) & Avg \(|(X)|\) & F/C Ratio \\  FA-graph & \(2.10 10^{24}\) & \(5.84 10^{21}\) & \(\) \\ OAP-graph & \(2.55 10^{22}\) & \(7.57 10^{19}\) & \(\) \\ FA/OAP Ratio & \(\) & \(\) & \\   

Table 2: The average frame size (F) and canonicalization size (C) on Exp with two canonicalization algorithms: FA-graph and OAP-graph.

   Model & PE & \(k\) & \#Param & \(\) \\  GIN & ID + FA & 0 & 495K & \(0.613 0.023\) \\ GINE & ID + FA & 0 & 495K & \(0.546 0.048\) \\   & None & 0 & 504K & \(0.251 0.009\) \\  & LapPE + RS & 8 & 505K & \(0.202 0.006\) \\  & SignNet & 8 & 495K & \(0.121 0.005\) \\  & MAP & 8 & 486K & \(0.120 0.005\) \\  & OAP & 8 & 473K & \(\) \\  & OAP + LSPE & 8 & 491K & \(\) \\   & None & 0 & 369K & \(0.141 0.004\) \\  & LapPE + RS & 8 & 474K & \(0.132 0.010\) \\  & SignNet & 8 & 476K & \(0.105 0.007\) \\  & MAP & 8 & 462K & \(0.101 0.005\) \\  & OAP & 8 & 462K & \(\) \\  & OAP + LSPE & 8 & 549K & \(\) \\   

Table 3: Ratio of non-canonicalizable eigenvectors on ZINC.

   Model & PE & \(k\) & \#Param & \(\) \\   & None & 0 & 1008K & \(0.262 0.001\) \\  & LapPE + RS & 3 & 1004K & \(0.774 0.007\) \\ GatedGCN & SignNet* & 3 & 1367K & \(0.773 0.003\) \\  & MAP & 3 & 1505K & \(0.784 0.005\) \\  & OAP & 3 & 1542K & \(\) \\   & None & 0 & 5245K & \(0.755 0.008\) \\  & LapPE + RS & 16 & 2453K & \(0.756 0.009\) \\  & SignNet* & 16 & 1754K & \(0.750 0.009\) \\  & MAP & 16 & 1951K & \(0.761 0.002\) \\  & OAP & 16 & 1950K & \(\) \\   

Table 5: Results on MOLTOX21. All scores are averaged over 4 runs with 4 different seeds.

We compare the time and memory of canonicalization methods with their non-FA backbone on ZINC in Table 7. Using canonicalization algorithms only increases the pre-processing time of the backbone, which is negligible compared to the training time. On the other hand, the two-branch architecture of SignNet increases the training time and memory.

## 6 Conclusion and Discussion

In this paper, we illustrated canonicalization as a useful view of frames. From this perspective, we established concrete theoretical conditions for determining the complexity of frames, as well as deriving better and even optimal frames. We believe that the canonicalization perspective has the potential to unify different invariant and equivariant learning approaches for a unified characterization.

One limitation of this work lies in that we do not fully resolve the optimality of eigenvector canonicalization under permutation equivariance. It is still yet unknown whether the proposed OAP canonicalization is optimal for Laplacian eigenvectors, and there is still much to explore in terms of canonicalization algorithms across other domains.