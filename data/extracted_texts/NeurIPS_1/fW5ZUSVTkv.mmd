# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

source and target domains in feature space . These works strive to reduce the domain bias of the detection backbone. However, they ignore the domain bias in the detection head. As shown in Fig. 1(a), they apply the same detection head on both domains, inevitably leading to performance degradation on the target domain . Recently, vision-language models (VLMs) show remarkably high generalization for downstream tasks on different domains, such as CLIP , GLIP  and ALIGN . The ability of VLMs to generate highly generalized features makes it possible to be a robust visual encoder for DAOD. On this basis, applying a VLM as the detection backbone following a domain-aware detection head is a reasonable way to directly learn a discriminative detector for target domains, rather than reducing the domain bias in traditional methods. Furthermore, inspired by prompt tuning such as CoOp , using domain-related prompt can produce domain-related detection head.

In this work, we propose a novel Domain-Aware detection head with Prompt tuning (DA-Pro) which is a new DAOD framework with a VLM-based backbone and a domain-aware detection head, as shown in Fig. 1(b). To the best of our knowledge, we are the first to apply prompt tuning in DAOD. Based on the highly generalized features generated from a VLM-based backbone and the domain-aware detection head, there is no need to focus on reducing the domain bias in the detection head. In DA-Pro, the domain-aware detection head is obtained by learning the domain-adaptive prompt. To achieve high discrimination, domain-adaptive prompt exploits both domain-invariant and domain-specific knowledge, consisting of domain-invariant tokens, domain-specific tokens and domain-related textual descriptions, along with the class textual descriptions. Domain-invariant tokens, which are shared across domains, learn the domain-invariant knowledge. Domain-specific tokens and the domain-related textual description are different across domains, aiming to capture the domain-specific knowledge of the corresponding domain by both learning and hand-craft.

To optimize the domain-adaptive prompt so that it can capture the domains-shared and domain-specific knowledge, we further propose two unique constraints. Firstly, to learn the shared domain-invariant knowledge, we constrain the detection heads generated by the source-domain prompt and target-domain prompt to recognize the images as accurately as possible. Secondly, given images of one domain, we constrain the detection head generated by the corresponding domain prompt to output higher confidence of its own domain than of the other domain. Moreover, we introduce a historical prompts ensemble strategy to reduce the disturbance caused by data mutation in the mini-batch. Concretely, we maintain a prompt buffer, which is updated by EMA in each iteration of training, and use it for inference.

We conduct extensive experiments for the proposed DA-Pro on three mainstream benchmarks: Cross-Weather (Cityscapes \(\) Foggy Cityscapes), Cross-Fov (KITTI \(\) Cityscapes), and Sim-to-Real (SIM10K \(\) Cityscapes). The experimental results show that our method brings noticeable improvement and achieves state-of-the-art performance. Concretely, DA-Pro improves the mAP by \(1.9\% 3.3\%\) on synthetic and real datasets over the strong Baseline RegionCLIP. In the best case, we achieve \(55.9\%\) mAP on the widely accepted benchmark of Cross-Weather, showing remarkable effectiveness in applying the domain adaptive detection head.

Figure 1: (a) Existing methods focus on reducing the domain bias of the detection backbone by inferring a discriminative visual encoder across domains, ignoring the domain bias in the detection head. (b) The proposed DA-Pro consists of a VLM-based backbone and a domain-aware detection head obtained by learning domain-adaptive prompt.

Related Works

**Domain Adaptive Object Detection (DAOD)** Domain Adaptive Object Detection (DAOD) aims to generalize the object detector trained on the labelled source domain to the unlabelled target domain. The key concept is inferring a discriminative visual encoder as the detection backbone by aligning source and target domains in feature space. Previous works for domain adaption [26; 42; 27; 40; 33; 9; 28; 19; 42] have explored minimizing various distance metrics to reduce feature discrepancy. Inspired by them, DA-Faster  first introduces a domain discriminator to extract domain-invariant features  adversarially at the image level. To avoid sub-optimization by directly aligning domain-specific features, DSS  suppresses the relevant gradient during backpropagation. TLA  divides the detection task into two sub-tasks, localization and classification, and employs task-specific discriminators to optimize them separately. CaCo  proposes that categories-agnostic alignment ignores class-specific knowledge and may lead to negative adaptation. Then it explicitly models the intra-class compactness and inter-class separability and aligns features on category hierarchy. SIGMA  further models class-conditional distributions of two domains with cross-image graphs and minimizes graph distances to bridge the domain gap.

Despite the considerable performance of domain alignment, existing methods ignore the domain bias in the detection head. They share the detection head on both domains, inevitably leading to worse discrimination on the target domain. In this work, we introduce visual-language models to DAOD, to obtain the domain-aware detection head via the domain-related textual description and the text encoder.

**Visual-language models** Recent advances in visual-language representation learning have shown that learning directly from image-text pairs is a promising alternative that leverages a much broader source of supervision. The model inferred by aligning the representation of the image-text pairs is defined as Visual-Language Model (VLM). A representative work is CLIP, which trains a visual encoder and a text encoder using the contrastive loss based on 400 million image-text pairs, demonstrating good generality for the unseen classes. When inference, it classifies images using textual representations generated from natural language descriptions, _i.e._ prompt. Some research [11; 45] adapt VLM into the object detection framework to encode robust visual and textual features. ViLD  distills the knowledge from a pre-trained VLM into a two-stage detector via aligning the region embeddings of proposals to CLIP's output. RegionCLIP  further extends VLM to directly learn region-level visual representations by generating region-text pairs as supervision. Some others [8; 35] introduce semantic domain concepts via textual prompts to infer robust detectors. In this work, we apply a VLM as the robust detection backbone and utilize the text encoder to build the domain-aware detection head. We adapt  with a domain classifier  as the Baseline.

**Prompt Tuning** Prompt tuning has been explored to transfer the pre-trained VLM to its downstream tasks, which utilizes a learnable textual prompt to embed task-relevant information for prediction. A proper prompt could boost the performance significantly with prompt engineering, however, it requires domain expertise and takes an amount of time for words tuning. Inspired by prompt tuning in language tasks, Context Optimization (CoOp)  and Conditional Context Optimization (CoCoOp)  replaces the hand-crafted prompts with the learnable continuous tokens to automate prompt engineering in an end-to-end manner. Furthermore, KgCoOp  explores the forgetting of general textual knowledge during prompt tuning and alleviates it by reducing the discrepancy between the learnable and hand-crafted prompt. DetPro  extends CoOp to object detection with a context grading scheme to separate proposals in the image foreground for tailored prompt training. However, these methods only develop highly generalizable and discriminative prompt on training domain (single domain). Ignoring the cross-domain difference, their domain-agnostic prompts can only capture domain knowledge on the training set. Due to domain bias, they have limited performance on target domain. To enable the prompt to learn cross-domain information, we introduce a novel domain-adaptive prompt and infer robust detection heads on each domain.

## 3 Methodology

DAOD aims to generalize the detector trained on the annotated source domain to the unlabelled target domain. In this work, we propose a novel Domain-Aware detection head with Prompt tuning (DA-Pro) for DAOD, which employs prompt tuning to generate the domain-aware detection head for capturing the domain-specific and domain-invariant knowledge, as shown in Figure 2. Therefore, we first give a brief introduction of prompt tuning and then introduce the proposed method.

### Preliminaries

Inspired by the effectiveness of CLIP for visual-language learning, we apply CLIP to embed the image and text description. Formally, CLIP consists of the visual encoder \(f\) and the text encoder \(g\). For a classification task with \(K\) categories, CLIP uses the classes' name to generate the textual embedding space with a hand-crafted prompt, a text encoder \(g()\), and a text tokenizer \(e()\), where \(e()\) map the text description into vectors. By denoting the class name of the \(i\)-th category as "class-i", its corresponding fixed prompt \(_{i}\) is established by using the template "A photo of a [class-i]" to generate the word vector tokens, _e.g._, \(_{i}=[e()]\). After that, the text encoder further project the fixed prompt into textual embedding space, defined as \(_{i}=g(_{i})\).

Given an input image \(I\) and its class label \(y\), the visual encoder \(f\) firstly extracts the visual embedding \(f(I)\). Then, the predicted probability \(p(=y|I)\) of input image \(I\) on the class \(y\) is computed with the distance between visual embedding and text embedding:

\[p(=y|I)=_{y}))/)}{_{i=1}^{K}exp(s( f(I),g(_{i}))/)}, \]

where \(s()\) is the cosine similarity, and \(\) is a learnable temperature parameter.

Especially, CLIP applies the hand-crafted prompts to generate the class textual embedding, having a less discriminative ability for downstream tasks. Recently researchers, such as CoOp, show that using the continuous learnable prompt can effectively capture domain-related knowledge. Formally, the hand-crafted prompt is replaced by \(M\) learnable tokens \(=[_{1}][_{2}]...[_{M}]\). After that, the learnable prompt \(_{i}\) is defined as the concatenation of \(\) and the tokenized vector \(_{i}=e()\) of \(i\)-th class:

\[_{i}=[_{1}][_{2}]...[_{M}][_ {i}]. \]

Recently, a lot of CLIP-based detectors are proposed to mine textual knowledge of categories effectively. For example, RegionCLIP  learns region-level visual representations by adapting CLIP with the hand-crafted prompt to a vanilla detector. Furthermore, DetPro  further applies the learnable prompt(Eq. 2) to generate the domain-agnostic representations. However, both the proposed prompts in RegionCLIP  and DetPro  cannot model the domain-specific knowledge, which is critical for DAOD.

### Domain-Adaptive Prompt

Since the existing DAOD methods only consider the visual information, they aim to infer an unbiased visual encoder to generate domain-invariant features and use a unique detection head for recognition. For example, DA-Faster  introduces a domain discriminator to train a discriminative detection backbone  adversarially. However, they all ignore the domain bias in the detection head. From a cognitive perspective, using a shared visual encoder and a domain-related detection head can also

Figure 2: Overview of the proposed DA-Pro framework on the Cross-Weather adaptation scenario.

reduce the domain bias. Furthermore, thanks to the increasing scale of models and data, the pre-trained visual-language model (VLM) has a high generalization for the downstream task. Consequently, treating the pre-trained model as a visual encoder to extract the highly generalized features and using the domain-specific classifier (detection head) for recognization is a reasonable way for domain-adaptive object detection rather than reducing the domain bias in traditional methods. Therefore, how to propose a domain-adaptive detection head is the rest problem.

Inspired by prompt tuning such as CoOp, we can generate the dynamic textual class embedding by feeding different prompts, _i.e._, using the source (target)-related prompt can produce the source (target)-related detection heads for the source (target) domain. Based on the CLIP, the domain-adaptive detection head can be obtained by feeding a domain-adaptive prompt into the text encoder of CLIP. To ensure the domain-adaptive prompt has a high discrimination and generalization ability, an ideal domain-adaptive prompt should satisfy the following conditions: 1) it can model the shared knowledge between the source and target domains; 2) it can model the specific knowledge of each domain; 3) it contains information that distinguishes or describes each domain; 4) The class information should be embedded for discriminative representation learning.

To achieve the above issue, we design an effective domain-adaptive prompt,

\[^{d}_{i}=[^{c}][^{c}_{2}]...[^{c}_{M}][^{ d}_{1}][^{d}_{2}]...[^{d}_{N}][_{i}][_{d}] \]

In Eq.3, the domain-adaptive prompt consists of four components. Firstly, \(^{c}=[^{c}_{1}][^{c}_{2}]...[^{c}_{M}]\) are the domain-invariant tokens with \(M\) learnable vectors tokens to learn the common knowledge across two domains. Then, \(=[^{d}_{1}][^{d}_{2}]...[^{d}_{N}]\) are the domain-specific tokens with \(N\) learnable tokens, which are independent across domains to exploit specific knowledge of each domain, _i.e._, \(^{s}=[^{s}_{1}][^{s}_{2}]...[^{s}_{N}]\) and \(^{t}=[^{t}_{1}][^{t}_{2}]...[^{t}_{N}]\) represent the domain-specific prompt of the source and target domains. Similar to CLIP, \(_{i}\) is the tokenized vector of the \(i\)-th class, which is the critical component for boosting the representation discrimination. To constrain the domain-adaptive prompt can fully capture the domain-related knowledge, the domain labels such as the domain-related textual descriptions are used to generate the domain-related vector tokens. Given the domain-related textual description ['domain-d'], the domain-related prompts \(_{d}=e()\) is the vector of the hand-crafted textual description "domain-d" for domain \(d\{s,t\}\). As the Cityscapes to Foggy Cityscape for example, the domain labels ['domain-d'] for the source domain('Cityscapes') and the target domain ('Foggy Cityscape') are 'clear' and 'foggy', respectively. Note that the domain-related prompts \([_{d}]\) are the significant prompt to control the proposed prompt for inferring the domain-aware knowledge.

By replace the domain-specific prompt \(\) in Eq. 3 with the source-specific prompt \(^{s}\) and the target-specific prompt \(^{t}\), the final source and target prompts are:

\[^{s}_{i} =[^{c}{}_{1}][^{c}{}_{2}]...[^{c}{}_{M}][ ^{s}{}_{1}][^{s}{}_{2}]...[^{s}{}_{N}][_{i}][ _{s}] \] \[^{t}_{i} =[^{c}{}_{1}][^{c}{}_{2}]...[^{c}{}_{M}][ ^{t}{}_{1}][^{t}{}_{2}]...[^{t}{}_{N}][_{i}][ _{t}] \]

### Domain-Adaptive Prompt for object detection

Following , we use a class-agnostic RPN  to propose image regions and extract visual embedding via the visual encoder. Then, the class textual embeddings are generated by feeding the domain-adaptive prompts into the text encoder. As the generated class textual embeddings can be treated as a classifier for computing the contrastive loss with the input visual feature, which can be further used to optimize the domain-adaptive prompt with the given labels.

Given an input image \(I\), the RPN \(r()\) proposes \(N_{r}\) image region boxes \(=\{_{j}\}_{j=1}^{N_{r}}\). After that, the corresponding visual embedding \(=\{_{j}\}_{j=1}^{N_{r}}\) is inferred by the visual encoder \(f\), where \(_{j}=f(_{j})\) is a fixed-size feature patch. Then the domain-adaptive detection head predicts probability \(p(=y|_{j},d,D)\) of the proposal \(_{j}\) belongs to the class \(y\) and domain \(d\):

\[p(=y|_{j},d,)=_{j}),g( ^{d}_{y}))/)}{_{k D}_{i=1}^{K}exp(s(f(_{j}),g (^{k}_{i}))/)}, \]

where \(s()\) is the cosine similarity. \(\{\{s\},\{t\},\{s,t\}\}\) indicates probability is calculated with the class embedding generated by the source prompt \(\{^{s}_{i}\}_{i=1}^{K}\), the target prompt\(\{^{t}_{i}\}_{i=1}^{K}\), or both prompts \(\{^{s}_{i}\}_{i=1}^{K}\{^{t}_{i}\}_{i=1}^{K}\).

The goal is to make the domain-invariant tokens learn the shared knowledge across domains and the domain-specific tokens learn the specific knowledge of each domain. The following two constraints are proposed to achieve the above goal. Firstly, the domain-invariant prompt is learnable by constraining that the detection head generated by the source-domain prompt and target-domain prompt should both classify the input images as accurately as possible. Secondly, the detection head generated by one domain should output higher confidence on its own domain than the other domain when predicting images of its own domain, so as to learn domain-specific knowledge.

Formally, given an image \(I^{d}\) from domain \(d\) along with the generated regions \(\{^{d}_{j}\}_{j=1}^{N_{r}}\) and its class label \(\{y^{d}_{j}\}_{j=1}^{N_{r}}\). Following the prediction probability calculated by Eq. 6, we denote the cross-entropy loss as:

\[_{d,}=_{^{s}}[-}_{ j=1}^{N_{r}} p(=y^{s}_{j}|^{s}_{j},d,)] \]

where \(\{\{s\},\{t\},\{s,t\}\}\) indicates probability is calculated with the class embedding generated by the source prompt \(\{^{s}_{i}\}_{i=1}^{K}\), the target prompt\(\{^{t}_{i}\}_{i=1}^{K}\), or both prompts \(\{^{s}_{i}\}_{i=1}^{K}\{^{t}_{i}\}_{i=1}^{K}\). To learn domain-invariant knowledge, both classifiers should predict \(^{d}_{j}\) to the ground truth \(y^{d}_{j}\):

\[^{inv}_{d}=_{d,\{s\}}+_{d,\{t\}} \]

To learnable domain-specific knowledge, we constrain that the classifier of domain \(d\) should be more confident on \(^{s}_{j}\) by minimizing the cross-entropy over the source and target prompts:

\[^{spc}_{d}=_{d,\{s,t\}} \]

Therefore, the objective function of the source data \(^{s}\) is:

\[_{s}=^{inv}_{s}+^{spc}_{s}. \]

Due to the lack of annotations, the above optimization objective cannot be directly performed on the target domain. We thus generate the pseudo labels for unlabeled target images with considering the powerful zero-shot capability of CLIP. Formally, we apply the hand-crafted template "A photo of [class]" to produce the class having the highest probability as pseudo labels, In particular, the pseudo labels \(y^{t}_{j}\) for the image regions \(^{t}_{j}\) is generated with Eq. 11:

\[y^{t}_{j}=*{arg\,max}_{y}p(=y|^{t}_{j}) \]

where the probability \(p\) is computed via Eq.1.

As untrusted pseudo would harm the learning of the source classifier, we use the pseudo labels whose probabilities are higher than \(\). Similar to Eq. 7, the cross-entropy loss computed with target data is denoted as:

\[_{t,}=_{^{t}}[-}_ {j=1}^{N_{r}}(p(=y^{t}_{j}|^{t}_{j},t,\{t\}) ) p(=y^{t}_{j}|^{t}_{j},t,)] \]

where the \(())\) is an indicator function. Meanwhile, we minimize the information entropy of the logits to achieve high-confident classification.

\[_{ent}=_{^{t}}[-}_{j=1}^{N_{ r}}(p(=y^{t}_{j}|^{t}_{j},t,\{t\}))p(=y ^{t}_{j}|^{t}_{j},t,\{t\}) p(=y^{t}_{j}|^{t}_{j},t,\{t\})] \]

Considering that inaccurate pseudo labels may be detrimental to the training of the source classifier, we only use them to optimize the target classifier. Overall, the objective on the target data \(^{t}\) is:

\[_{t}=_{t,\{t\}}+_{t,\{s,t\}}+_{ent} \]

The overall training objective is:

\[=_{s}+_{t}. \]

where \(\) is used to balance the effect of \(_{t}\) in \(\).

### Prompt Ensemble

During prompt tuning, all the visual and text encoder parameters are frozen. In each iteration, only the learnable prompts are updated via backward gradients. However, data mutation across mini-batches may cause prompt perturbations in the early stage of training, harming the prompt tuning. To alleviate this phenomenon, a prompts ensemble strategy is proposed to improve the stability of prompt tuning by averaging all the history prompts. Concretely, we maintain a prompt buffer \(=\{_{1}^{s},_{2}^{s},...,_{K}^{s}, _{1}^{t},_{2}^{t},...,_{K}^{t}\}\) about the learnable prompt in DA-Pro. In each iteration \(iter\) of training, buffer \(_{iter}\) can not be optimized with gradients, and can only be updated by Exponential Moving Average (EMA) according to the weights of itself and the learnable prompt \(_{iter}\)\(_{iter}=\{_{1}^{s},_{2}^{s},...,_{K}^{s}, _{1}^{t},_{2}^{t},...,_{K}^{t}\}\):

\[_{iter}=_{iter-1}+(1-)_{iter} \]

where \(\) is the ensemble ratio of history prompts. Considering that the buffer needs to quickly forget the inaccurate parameters learned in the early stage, we set a dynamic updating ratio \(=\{1-,0.99\}\). As EMA reduces the variance of the model, the prompt ensembled strategy infers a more robust prompt \(\). After training, \(\) is saved and used for inference.

## 4 Experiment

This section evaluates our DA-Pro on mainstream DAOD scenarios, including the Cross-Weather, Cross-Fov, and Sim-to-Real. Further ablation studies are conducted to validate the effectiveness of the proposed domain-adaptive prompt, training scheme, and prompt ensemble strategy.

### Dataset

**Cross-Weather** Cityscapes  is a large-scale dataset that contains diverse images recorded in street scenes. It is divided into 2,975 training and 500 validation images, annotated with 8 classes. Foggy Cityscapes  is a synthetic foggy dataset that simulates fog with three distinct densities on Cityscapes, containing 8,925 training images and 1,500 validation images. We take the training set of Cityscapes as the source domain and the training set of foggy Cityscapes as the target domain, evaluating Cross-Weather adaptation performance on the 1500-sized validation set in all 8 categories.

**Cross-Fov** KITTI  is a vital dataset in self-driving, containing 7,481 images with car annotations. Images are captured by driving in rural areas and on highways, yielding the Field of View (FoV) disparity with Cityscapes. For the Cross-Fov adaptation scenario, we migrate KITTI to Cityscapes on the car category and validate performance on Cityscapes.

**Sim-to-Real** SIM10k  is a synthetic dataset containing 10,000 images with car annotations rendered from the video game Grand Theft Auto V. We perform simulated environment to real-world adaptation on SIM10K and Cityscapes datasets.

### Implementation Details

We adapt RegionCLIP with a domain classifier  as baseline, and initialize the default prompt with "A photo of a [\(class\)][\(domain\)]". We use ResNet-50  as the visual encoder and Transformer ) as the text encoder, and initialize with the pre-trained CLIP model. Configurations of the detector and image pre-processing follow the default settings in [31; 3]. Following , we first pre-train the baseline model with classification loss, regression loss, and adversarial loss. Then for adaptation, one batch of source images with ground truth and one batch of target domain images are forwarded to the proposed DA-Pro in each iteration to calculate the supervising and self-training loss. We fix the length of learnable tokens \(M,N\) to \(8,8\), respectively. The hyperparameter \(\) is set to \(1.0\). We set the batch size of each domain to 2 and use the SGD optimizer with a warm-up learning rate for training. We take the mean Average Precision (mAP) with a threshold of 0.5 as the evaluation metric. All experiments are deployed on a Tesla V100 GPU.

### Comparison with existing methods

We introduce various state-of-the-art DAOD approaches for comparison, including DA-Faster , VDD , DSS , MEGA , SCAN , TIA , SIGMA , and AT .

**Cross-Weather Adaptation Scenario** As shown in Table 1 (C\(\)F), the proposed DA-Pro achieves the highest mAP over 8 classes of \(55.9\%\), outperforming SOTA AT  by a remarkable margin of \(5.0\%\). Compared with existing methods, our method significantly improves 6 categories (_i.e._ rider, car, truck, bus, train and bicycle) ranging from \(1.8\%\) to \(16.2\%\). The improvement is particularly significant on the previously poor categories, _e.g._\(11.0\%\) on Rider and \(16.2\%\) on Bicycle, which may have harder semantics for adaptation. Though its promising mAP of \(52.6\%\), DA-Pro still improves Baseline by \(3.3\%\). Concretely, DA-Pro promotes \(2.1 3.9\%\) on all 8 categories via learning domain-adaptive prompt. The superior performance demonstrates the proposed DA-Pro can improve the generalization ability of the detector on the target domain.

**Cross-Fov Adaptation Scenario** Table 1 (K\(\)C) also shows that the DA-Pro improves hand-crafted prompt Baseline by \(1.9\%\). And the proposed method reaches the best remarkable \(61.4\%\) mAP, outperforming SOTA methods in the Cross-Fov detection task. According to , K\(\)C adaptation faces more complicated shape confusion than C\(\)F, which requests higher discriminability of the model. The considerable performance improvement confirms that the proposed method can efficiently generate domain-aware detectors with high discrimination.

**Sim-to-Real Adaptation Scenario** Unlike the Cross-Weather and Cross-Fov adaptation, the Sim-to-Real task has a wider domain gap at semantic level. As presented in Table 1 (S\(\)C), the proposed method gains \(2.1\%\) on Baseline and peaks at \(62.9\%\), surpassing SOTA by \(9.2\%\). It further demonstrates that our strategy is robust in not only appearance but also harder semantics adaptation tasks.

### Ablation Studies

We report detailed ablation studies (Table 2) conducted on the Cross-Weather adaptation scenario, to validate the effectiveness of DA-Pro under different configurations.

    &  &  \\   & Person & Rider & Car & Truck & Bus & Train & Motor & Bicycle & mAP & mAP & mAP \\  DA-Faster  & 29.2 & 40.4 & 43.4 & 19.7 & 38.3 & 28.5 & 23.7 & 32.7 & 32.0 & 41.9 & 38.2 \\ VDD  & 33.4 & 44.0 & 51.7 & 33.9 & 52.0 & 34.7 & 34.2 & 36.8 & 40.0 & - & - \\ DSS  & 42.9 & 51.2 & 53.6 & 33.6 & 49.2 & 18.9 & 36.2 & 41.8 & 40.9 & 42.7 & 44.5 \\ MeGA  & 37.7 & 49.0 & 52.4 & 25.4 & 49.2 & 46.9 & 34.5 & 39.0 & 41.8 & 43.0 & 44.8 \\ SCAN  & 41.7 & 43.9 & 57.3 & 28.7 & 48.6 & 48.7 & 31.0 & 37.3 & 42.1 & 45.8 & 52.6 \\ TIA  & 52.1 & 38.1 & 49.7 & 37.7 & 34.8 & 46.3 & 48.6 & 31.1 & 42.3 & 44.0 & - \\ SIGMA  & 44.0 & 43.9 & 60.3 & 31.6 & 50.4 & 51.5 & 31.7 & 40.6 & 44.2 & 45.8 & 53.7 \\ AT  & **56.3** & 51.9 & 64.2 & 38.5 & 45.5 & 55.1 & **54.3** & 35.0 & 50.9 & - & - \\  Baseline & 51.8 & 59.0 & 67.4 & 36.8 & 59.5 & 50.6 & 39.7 & 55.9 & 52.6 & 59.5 & 60.8 \\ DA-Pro & 55.4

**Comparison on Prompt Design** In this section, we analyze the performance of different prompts. As shown in line \(1 5\) of Table 2, we compare the performances on the following four prompt designs: the pre-defined prompt "A photo of a [class][domain]", the CoOp-style learnable prompt, and the domain-invariant tokens or domain-specific tokens with domain-related textual description. The CoOp-style prompt with 16 learnable domain-invariant tokens improves \(0.4\%\) on AP and AP50 over the hand-crafted prompt. Introducing domain-related textual description further boosts \(0.3 0.8\%\) on three metrics. This exhibits that learning domain-invariant tokens improves the discrimination of the target detection head. When directly applying domain-specific tokens, it only gains a limited improvement over the pre-defined prompt. This shows that the source domain information cannot be transferred to the target domain only using domain-specific tokens. Diving the 16 tokens into 8 domain-invariant and 8 domain-specific tokens further gains extra \(0.3 1.0\%\) improvements. We suppose that based on shared knowledge across domains, learning domain-specific knowledge leads to more discriminative detection head on each domain. These results reveal that compared with the fixed or the domain-agnostic prompt, the domain-adaptive prompt has a higher ability to embed domain information for prediction. Moreover, both domain-invariant tokens and domain-specific tokens bring improvements.

**Comparison on Loss Function** Table 3 shows that each term of loss function contributes to improvement in the adaption performance. To reduce the influence of other factors, we do not use the historical prompt ensemble strategy. With \(_{s,\{s\}}+_{t,\{t\}}\), four types of learnable prompts have improved \(0.3~{} 1.8\%\) on the hand-crafted prompt. Combined with \(_{s,\{t\}}\), which commands the target prompt to classify the source image accurately, further gains \(0.2 0.4\%\) on three prompts distinguished across domains. This reveals that by constraining the detection heads in the source and target domains to classify the input image as accurately as possible, the prompt can learn more domain-invariant knowledge. Further, though introduce \(_{s,\{s,t\}}+_{t,\{s,t\}}\) has few effects on prompt with only domain-invariant/domain-specific tokens, it improves \(0.4\%\) on the domain-adaptive prompt. We conclude that in two respect. On one hand, just using domain-invariant/domain-specific tokens cannot model shared knowledge between domains and domain-specific knowledge at the same time. On the other hand, constraining the detection head generated by one domain to output higher confidence in its own domain than the other domain contributes to learning domain-specific knowledge. Moreover, adding \(_{ent}\) brings extra \(0.3\%\) improvement. However, introducing \(_{t,\{s\}}\) suffers \(2.1\%\) degradation, showing that directly optimizing the source classifier with pseudo target labels could do harm to the learning process.

**Comparison on Prompt Ensemble** To verify the effectiveness of the proposed historical prompt ensemble strategy, we introduce it to the domain-adaptive prompt with 8 domain-invariant tokens and 8 domain-specific tokens. Line 6 of Table 2 shows that it gains improvements of \(0.7\%,0.4\%\) and

    & \(_{s,\{s\}}+_{t,\{t\}}\) & \(_{s,\{t\}}\) & \(_{s,\{s,t\}}+_{t,\{s,t\}}\) & \(_{ent}\) & \(_{t,\{s\}}\) & mAP \\   & - & - & - & - & - & 52.6 \\  ][_{i}]\)} & ✓ & - & - & - & - & 53.0 \\  _{1}][^{}_{2}]...[^{}_{M}][ _{i}]\)} & \([_{i}]\)[\(_{1}\)] & ✓ & & & & & 53.5 \\ _{1}][^{}_{2}]...[^{}_{M}][ _{i}]\)} & \([_{i}]\)[\(_{1}\)] & ✓ & ✓ & & & & 53.8 \\ _{1}][^{}_{2}]...[^{}_{M}][ _{i}]\)} & \([_{1}\)] & ✓ & ✓ & ✓ & & & 53.8 \\ _{1}][^{}_{2}]...[^{}_{M}][ _{i}]\)} & \([_{i}]\)[\(_{1}\)] & ✓ & ✓ & ✓ & ✓ & & 53.7 \\  _{1}][^{}_{2}]...[^{}_{M}][ _{i}]\)} & \([_{i}]\)[\(_{1}\)] & ✓ & & & & & 52.9 \\ _{1}][^{}_{2}]...[^{}_{M}][ _{i}]\)} & \([_{i}]\)[\(_{1}\)] & ✓ & ✓ & & & & 53.1 \\ _{1}][^{}_{2}]...[^{}_{M}][ _{i}]\)} & \([_{i}]\)[\(_{1}\)] & ✓ & ✓ & ✓ & & & 52.7 \\ _{1}][^{}_{2}]...[^{}_{M}][ _{i}]\)} & \([_{i}]\)[\(_{1}\)] & ✓ & ✓ & ✓ & ✓ & & 52.9 \\  _{1}][^{}_{2}]...[^{}_{M}][ ^{}_{1}][^{}_{2}]...[^{}_{N}][ _{i}]\)} & \([_{i}]\)[\(_{1}\)] & ✓ & & & & & 54.4 \\ _{1}][^{}_{2}]...[^{}_{M}][ ^{}_{1}][^{}_{2}]...[^{}_{N}][ _{i}]\)} & \([_{i}]\)[\(_{1}\)] & ✓ & ✓ & & & & 54.8 \\ _{1}][^{}_{2}]...[^{}_{M}][ ^{}_{1}][^{}_{2}]...[^{}_{N}][ _{i}]\)} & \([_{i}]\)[\(_{1}\)] & ✓ & ✓ & ✓ & & & 55.2 \\ _{1}][^{}_{2}]...[^{}_{M}][ ^{}_{1}][^{}_{2}]...[^{}_{N}][ _{i}]\)} & \([_{i}]\)[\(_{1}\)] & ✓ & ✓ & ✓ & ✓ & & 55.5 \\ _{1}][^{}_{2}]...[^{}_{M}][ ^{}_{1}][^{}_{2}]...[^{}_{N}][ _{i}]\)} & \([_{i}]\)[\(_{1}\)] & ✓ & ✓ & ✓ & ✓ & ✓ & 53.4 \\   

Table 3: The influence (\(\%\)) of loss design on Cross-Weather adaptation scenario Cityscapes\(\)Foggy Cityscapes. Total number of tokens is set to 16. - stands for the prompt is not compatible with the loss. Without the historical prompt ensemble strategy.

**Comparison on Prompt Length** Further, we explore the impact of prompt length, where \(M\) for domain-invariant tokens and \(N\) for domain-specific tokens on DA-Pro with full configuration in line \(6 9\) of Table 2. We vary the length from (\(4\), \(4\)) to (\(8\), \(8\)) to (\(12\), \(12\)) to (\(16\), \(16\)). DetPro has shown that a longer prompt may cause over-fitting to base categories in open-set object detection. Similarly, we conclude that a shorter prompt is less discriminative, while a too-long prompt is prone to overfitting to the source domain in DAOD. Thus, we set the token length as (\(8\), \(8\)) in DA-Pro.

### Visualization

In Fig. 3, we present the comparison on the target domain among the ground truth boxes (a) and the detection boxes using different prompts. (a.1)(b.1)(c.1)(d.1) are zoomed from the same region of images (a)(b)(c)(d) for a better view. Fig. 3(a.1) indicates four objects in the cropped region: a car and a bicycle on the left, and two overlapping persons on the right. Both hand-crafted and learnable prompt models detect the car correctly. However, it is difficult for the hand-crafted prompt to describe domain information, like weather conditions, accurately. Thus, the hand-crafted prompt misses some objects on the right side of Fig. 3(b), and the bicycle and one of the persons in Fig. 3(b.1). In Fig. 3(c), the domain-invariant prompt improves \(9.3\%\) mAP compared with the hand-crafted prompt. But it still suffers worse discrimination due to insufficient domain representation learning. In Fig. 3(c.1), the domain-invariant prompt misclassifies the bicycle as a rider even confident with \(93\%\), and also ignores one person. The proposed domain-adaptive prompt correctly detects all objects in the cropped region Fig 3(d.1). By learning domain information, the domain-specific tokens enable the model to perform confident predictions on the bicycle (\(79\%\)) and \(2\) persons (\(100\%,89\%\)). These comparison results reveal the effectiveness of the proposed domain-adaptive prompt in DA-Pro.

## 5 Conclusion

In this paper, we propose a novel DAOD framework named Domain-Aware detection head with Prompt tuning (DA-Pro). Serving a pre-trained visual-language model as the robust detection backbone, it applies the learnable domain-adaptive prompt to generate the discriminative detection head for each domain. As the learnable part is designed to capture the domain-invariant and domain-specific knowledge, we proposed two constraints over the source and target domains to guide the optimization. Moreover, a prompt ensemble strategy is also proposed to reduce the effect of prompt disturbance. Comprehensive experiments over multiple cross-domain adaptation tasks demonstrate that using the domain-adaptive prompt can produce a domain-aware detection head with more discrimination for domain-adaptive object detection.