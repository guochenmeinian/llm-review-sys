# A Competitive Algorithm for Agnostic Active Learning

Eric Price

Department of Computer Science

University of Texas at Austin

ecprice@cs.utexas.edu &Yihan Zhou

Department of Computer Science

University of Texas at Austin

joeyzhou@cs.utexas.edu

###### Abstract

For some hypothesis classes and input distributions, _active_ agnostic learning needs exponentially fewer samples than passive learning; for other classes and distributions, it offers little to no improvement. The most popular algorithms for agnostic active learning express their performance in terms of a parameter called the disagreement coefficient, but it is known that these algorithms are inefficient on some inputs.

We take a different approach to agnostic active learning, getting an algorithm that is _competitive_ with the optimal algorithm for any binary hypothesis class \(H\) and distribution \(_{X}\) over \(X\). In particular, if any algorithm can use \(m^{*}\) queries to get \(O()\) error, then our algorithm uses \(O(m^{*}|H|)\) queries to get \(O()\) error. Our algorithm lies in the vein of the splitting-based approach of Dasgupta (2004), which gets a similar result for the realizable (\(=0\)) setting.

We also show that it is NP-hard to do better than our algorithm's \(O(|H|)\) overhead in general.

## 1 Introduction

Active learning is motivated by settings where unlabeled data is cheap but labeling it is expensive. By carefully choosing which points to label, one can often achieve significant reductions in label complexity (Cohn et al., 1994). A canonical example with exponential improvement is one-dimensional threshold functions \(h_{}(x):=1_{x}\): in the noiseless setting, an active learner can use binary search to find an \(\)-approximation solution in \(O()\) queries, while a passive learner needs \(()\) samples (Cohn et al., 1994; Dasgupta, 2005; Nowak, 2011).

In this paper we are concerned with agnostic binary classification. We are given a hypothesis class \(H\) of binary hypotheses \(h:\{0,1\}\) such that some \(h^{*} H\) has \((h^{*})\), where the error

\[(h):=_{(x,y)}[h(x) y]\]

is measured with respect to an unknown distribution \(\) over \(\{0,1\}\). In our _active_ setting, we also know the marginal distribution \(_{X}\) of \(x\), and can query any point \(x\) of our choosing to receive a sample \(y(Y X=x)\) for \((X,Y)\). The goal is to output some \(\) with \(()+\), using as few queries as possible.

The first interesting results for agnostic active learning were shown by Balcan et al. (2006), who gave an algorithm called Agnostic Active (A\({}^{2}\)) that gets logarithmic dependence on \(\) in some natural settings: it needs \(()\) samples for the 1d linear threshold setting (binary search), as long as as \(>16\), and \((d^{2})\) samples for \(d\)-dimensional linear thresholds when \(_{X}\) is the uniform sphere and \(>\). This stands in contrast to the polynomial dependence on \(\) necessary in the passive setting. The bound's requirement that \(\) is quite natural given a lower bound of \((d}{^{2}})\)due to (Kaariainen, 2006; Beygelzimer et al., 2009), where \(d\) is the VC dimension. Subsequent works have given new algorithms (Dasgupta et al., 2007; Beygelzimer et al., 2010) and new analyses (Hanneke, 2007a) to get bounds for more general problems, parameterized by the "disagreement coefficient" of the problem. But while these can give better bounds in specific cases, they do not give a good competitive ratio to the optimum algorithm: see (Hanneke (2014), Section 8.2.5) for a realizable example where \(O()\) queries are possible, but disagreement-coefficient based bounds lead to \(()\) queries.

By contrast, in the _realizable, identifiable_ setting (\(==0\)), a simple greedy algorithm _is_ competitive with the optimal algorithm. In particular, Dasgupta (2004) shows that if any algorithm can identify the true hypothesis in \(m\) queries, then the greedy algorithm that repeatedly queries the point that splits the most hypotheses will identify the true hypothesis in \(O(m|H|)\) queries. This extra factor of \(|H|\) is computationally necessary: as we will show in Theorem 1.2, avoiding it is NP-hard in general. This approach can extend (Dasgupta, 2005) to the PAC setting (so \(>0\), but still \(=0\)), showing that if any algorithm gets error \(\) in \(m^{*}\) queries, then this algorithm gets error \(8\) in roughly \((m^{*}|H|)\) queries (but see the discussion after Theorem 8.2 of Hanneke (2014), which points out that one of the logarithmic factors is in an uncontrolled parameter \(\), and states that "Resolving the issue of this extra factor of \(\) remains an important open problem in the theory of active learning.").

The natural question is: can we find an agnostic active learning algorithm that is competitive with the optimal one in the agnostic setting?

Our Results.Our main result is just such a competitive bound. We say an active agnostic learning algorithm \(\) solves an instance \((H,_{X},,,)\) with \(m\) measurements if, for every distribution \(\) with marginal \(_{X}\) and for which some \(h^{*} H\) has \((h^{*})\), with probability \(1-\), \(\) uses at most \(m\) queries and outputs \( H\) with \(()+\). Let \(m^{*}(H,_{X},,,)\) be the optimal number of queries for this problem, i.e., the smallest \(m\) for which any \(\) can solve \((H,_{X},,,)\).

Define \(N(H,_{X},)\) to be the size of the smallest \(\)-cover over \(H\), i.e., the smallest set \(S H\) such that for every \(h H\) there exists \(h^{} S\) with \(_{x_{X}}[h(x) h^{}(x)]\). When the context is clear, we drop the parameters and simply use \(N\). Of course, \(N\) is at most \(|H|\).

**Theorem 1.1** (Competitive Bound).: _There exist some constants \(c_{1},c_{2}\) and \(c_{3}\) such that for any instance \((H,_{X},,,)\) with \( c_{1}\), Algorithm 1 solves the instance with sample complexity_

\[m(H,_{X},,,)(m^{*}(H, _{X},c_{2},c_{3},)+)_{X},)}{}\]

_and polynomial time._

Even the case of \(=0\) is interesting, given the discussion in (Hanneke, 2014) of the gap in (Dasgupta, 2005)'s bound, but the main contribution is the ability to handle the agnostic setting of \(>0\). The requirement that \( O()\) is in line with prior work (Balcan et al., 2006; Dasgupta, 2005). Up to constants in \(\) and \(\), Theorem 1.1 shows that our algorithm is within a \( N|H|\) factor of the optimal query complexity.

We show that it NP-hard to avoid this \( N\) factor, even in the realizable (\(===0\)) case:

**Theorem 1.2** (Lower Bound).: _It is NP-hard to find a query strategy for every agnostic active learning instance within an \(c|H|\) for some constant \(c>0\) factor of the optimal sample complexity._

This is a relatively simple reduction from the hardness of approximating SetCover(Dinur and Steurer, 2014). The lower bound instance has \(===0\), although these can be relaxed to being small polynomials (e.g., \(==\) and \(=\)).

Extension.We give an improved bound for our algorithm in the case of noisy binary search (i.e., \(H\) consists of 1d threshold functions). When \(=()\), \(N(H,_{X},)=()\) and \(m^{*}(,,.99)=O()\). Thus Theorem 1.1 immediately gives a bound of \(O(^{})\), which is nontrivial but not ideal. (For \(\), the same bound holds since the problem is strictly easier when \(\) is smaller.) However, the bound in Theorem 1.1 is quite loose in this setting, and we can instead give a bound of for the same algorithm, Algorithm 1. This matches the bound given by disagreement coefficient based algorithms for constant \(\). The proof of this improved dependence comes from bounding a new parameter measuring the complexity of an \(H,_{x}\) pair; this parameter is always at least \((})\) but may be much larger (and is constant for 1d threshold functions). See Theorem 2.3 for details.

### Related Work

Active learning is a widely studied topic, taking many forms beyond the directly related work on agnostic active learning discussed above (Settles, 2009). Our algorithm can be viewed as similar to "uncertainty sampling" (Lewis, 1995; Lewis and Catlett, 1994), a popular empirical approach to active learning, though we need some modifications to tolerate adversarial noise.

One problem related to the one studied in this paper is noisy binary search, which corresponds to active learning of 1d thresholds. This has been extensively studied in the setting of _i.i.d._ noise (Burnashev and Zigangirov, 1974; Ben-Or and Hassidim, 2008; Dereniowski et al., 2021) as well as monotonic queries (Karp and Kleinberg, 2007). Some work in this vein has extended beyond binary search to (essentially) active binary classification (Nowak, 2008, 2011). These algorithms are all fairly similar to ours, in that they do multiplicative weights/Bayesian updates, but they query the _single_ maximally informative point. This is fine in the i.i.d. noise setting, but in an agnostic setting the adversary can corrupt that query. For this reason, our algorithm needs to find a _set_ of high-information points to query.

Another related problems is decision tree learning. The realizable, noiseless case \(==0\) of our problem can be reduced to learning a binary decision tree with minimal depth. Hegedus (1995) studied this problem and gave basically the same upper and lower bound as in Dasgupta (2005). Kosaraju et al. (2002) studied a split tree problem, which is a generalization of binary decision tree learning, and also gave similar bounds. Azad et al. (2022) is a monograph focusing on decision tree learning, in which many variations are studied, including learning with noise. However, this line of work usually allows different forms of queries so their results are not directly comparable from results in the active learning literature.

For much more work on the agnostic active binary classification problem, see Hanneke (2014) and references therein. Many of these papers give bounds in terms of the disagreement coefficient, but sometimes in terms of other parameters. For example, Katz-Samuels et al. (2021) has a query bound that is always competitive with the disagreement coefficient-based methods, and sometimes much better; still, it is not competitive with the optimum in all cases.

In terms of the lower bound, it is shown in Laurent and Rivest (1976) that the problem is NP-complete, in the realizable and noiseless setting. To the best of our knowledge, our Theorem 1.2 showing hardness of approximation to within a \(O(|H|)\) factor is new.

Minimax sample complexity bounds.Hanneke and Yang (2015) and Hanneke (2007b) have also given "minimax" sample complexity bounds for their algorithms, also getting a sample complexity within \(O(|H|)\) of optimal. However, these results are optimal with respect to the sample complexity for the worst-case distribution over \(y\)_and_\(x\). But the unlabeled data \(x\) is given as input. So one should hope for a bound with respect to optimal for the _actual_\(x\) and only worst-case over \(y\); this is our bound.

We give the following example to illustrate that our bound, and indeed our algorithm, can be much better.

**Example 1.3**.: _Define a hypothesis class of \(N\) hypotheses \(h_{1},,h_{N}\), and \( N+N\) data points \(x_{1},,x_{ N+N}\). For each hypothesis \(h_{j}\), the labels of the first \(N\) points express \(j\) in unary and the labels of the last \( N\) points express \(j\) in binary. We set \(==0\) and consider the realizable case._

In the above example, the binary region is far more informative than the unary region, but disagreement coefficient-based algorithms just note that every point has disagreement. Our algorithm will query the binary encoding region and take \(O( N)\) queries. Disagreement coefficient based algorithms, including those in Hanneke and Yang (2015) and Hanneke (2007b), will rely on essentially uniform sampling for the first \((N/ N)\) queries. These algorithms are "minimax" over \(x\), in the sense that _if you didn't see any \(x\) from the binary region_, you would need almost as many samples as they use. But you _do_ see \(x\) from the binary region, so the algorithm should make use of it to get exponential improvement.

[MISSING_PAGE_FAIL:4]

The algorithm can choose \(q\) to maximize this bound on the potential gain. There's a tradeoff between concentrating the samples over the \(x\) of largest \(r(x)\), and spreading out the samples so the adversary can't raise the error probability too high. We show that if learning is possible by any algorithm (for a constant factor larger \(\)), then there exists a \(q\) for which this potential gain is significant.

**Lemma 2.1** (Connection to OPT).: _Define \(\|h-h^{}\|=_{x_{x}}[h(x) h^{}(x)]\). Let \(\) be a distribution over \(H\) such that no radius-\((2+)\) ball \(B\) centered on \(h H\) has probability at least \(80\%\). Let \(m^{*}=m^{*}(H,_{X},,,)\). Then there exists a query distribution \(q\) over \(\) with_

\[}_{x q}[r(x)]-_{x}_{X}(x)}}.\]

At a very high level, the proof is: imagine \(h^{*}\). If the algorithm only sees the majority label \(y\) on every query it performs, then its output \(\) is independent of \(h^{*}\) and cannot be valid for more than 80% of inputs by the ball assumption; hence a 99% successful algorithm must have a 19% chance of seeing a minority label. But for \(m^{*}\) queries \(x\) drawn with marginal distribution \(q\), without noise the expected number of minority labels seen is \(m^{*}}[r(x)]\), so \(}[r(x)] 1/m^{*}\). With noise, the adversary can corrupt the minority labels in \(h^{*}\) back toward the majority, leading to the given bound.

The query distribution optimizing (1) has a simple structure: take a threshold \(\) for \(r(x)\), sample from \(_{x}\) conditioned on \(r(x)>\), and possibly sample \(x\) with \(r(x)=\) at a lower rate. This means the algorithm can efficiently find the optimal \(q\).

Except for the caveat about \(\) not already concentrating in a small ball, applying Lemma 2.1 combined with (1) shows that \((h^{*})\) grows by \((})\) in expectation for each query. It starts out at \((h^{*})=- H\), so after \(O(m^{*} H)\) queries we would have \((h^{*})\) being a large constant in expectation (and with high probability, by Freedman's inequality for concentration of martingales). Of course \((h^{*})\) can't grow past \(1\), which features in this argument in that once \((h^{*})>80\%\), a small ball _will_ have large probability and Lemma 2.1 no longer applies, but at that point we can just output any hypothesis in the heavy ball.

Handling noise: the challenge.There is one omission in the above argument that is surprisingly challenging to fix, and ends up requiring significant changes to the algorithm: if at an intermediate step \(_{i}\) concentrates in the _wrong_ small ball, the algorithm will not necessarily make progress. It is entirely possible that \(_{i}\) concentrates in a small ball, even in the first iteration--perhaps \(99\%\) of the hypotheses in \(H\) are close to each other. And if that happens, then we will have \(r(x) 0.01\) for most \(x\), which could make the RHS of (1) negative for all \(q\).

In fact, it seems like a reasonable Bayesian-inspired algorithm really must allow \((h^{*})\) to decrease in some situations. Consider the setting of Figure 1. We have three hypotheses, \(h_{1},h_{2}\), and \(h_{3}\), and a prior \(=(0.9,0.099999,10^{-6})\). Because \((h_{3})\) is so tiny, the algorithm presumably should ignore \(h_{3}\) and query essentially uniformly from the locations where \(h_{1}\) and \(h_{2}\) disagree. In this example, \(h_{3}\) agrees with \(h_{1}\) on all but an \(\) mass in those locations, so even if \(h^{*}=h_{3}\), the query distribution can match \(h_{1}\) perfectly and not \(h_{3}\). Then \(w(h_{1})\) stays constant while \(w(h_{3})\) shrinks. \(w(h_{2})\) shrinks much faster, of course, but since the denominator is dominated by \(w(h_{1})\), \((h_{3})\) will still shrink. However, despite \((h_{3})\) shrinking, the algorithm is still making progress in this example: \((h_{2})\) is shrinking fast, and once it becomes small relative to \((h_{3})\) then the algorithm will start querying points to distinguish \(h_{3}\) from \(h_{1}\), at which point \((h_{3})\) will start an inexorable rise.

Our solution is to "cap" the large density balls in \(\), dividing their probability by two, when applying Lemma 2.1. Our algorithm maintains a set \(S H\) of the "high-density region," such that the capped

Figure 1: An example demonstrating that the weight of the true hypothesis can decrease if \(\) is concentrated on the wrong ball. In this example, the true labels \(y\) are closest to \(h_{3}\). But if the prior \(\) on hypotheses puts far more weight on \(h_{1}\) and \(h_{2}\), the algorithm will query uniformly over \(h_{1}\) and \(h_{2}\) disagree: the second half of points. Over this query distribution, \(h_{1}\) is more correct than \(h_{3}\), so the weight of \(h_{3}\) can actually _decrease_ if \((h_{1})\) is very large.

[MISSING_PAGE_FAIL:6]

function in each iteration. However, the algorithm _knows_ a bound on its expected increase in each round \(i\); it is the value

\[_{i}=_{q}*{}_{x q}[_{i,S_{i}}(x)]- }{20}_{x}_{X}(x)}.\]

optimized in the algorithm. Therefore, we could use an adaptive termination criterion that stops at iteration \(k\) if \(_{i=1}^{k}_{i} O()\). This will guarantee that when terminating, the potential will be above 0 with high probability so our analysis holds.

Remark 2:The algorithm's running time is polynomial in \(|H|\). This is in general not avoidable, since the input is a truth table for \(H\). The bottleneck of the computation is the step where the algorithm checks if the heaviest ball has mass greater than 80%. This step could be accelerated by randomly sampling hypothesis and points to estimate and find heavy balls; this would improve the dependence to nearly linear in \(|H|\). If the hypothesis class has some structure, like the binary search example, the algorithm can be implemented more efficiently.

```
Compute a \(2\) maximal packing \(H^{}\)  Let \(w_{0}=1\) for every \(h H^{}\). \(S_{0}\) \(C\) for\(i=1,,k=O(m^{*}|}{})\)do  Compute \(_{i}(h)=(h)}{_{h H}w_{i-1}(h)}\) for every \(h H\) if there exists \(c_{4}+c_{5}\) ball with probability \(>80\%\) over \(_{i,S_{i-1}}\)then \(S_{i} S_{i} B(^{},3c_{4}+3c_{5})\) where \(B(^{},c_{4}+c_{5})\) is the heaviest radius \(c_{4}+c_{5}\) ball over \(_{i}\) \(C C\{^{}\}\) else \(S_{i} S_{i-1}\)  Compute \(_{i,S_{i}}=_{i}(h)&h S_{i} \\ _{i}(h)_{h_{i}}[h S_{i}]}{1- _{h_{i}}[h S_{i}]}&h S_{i}\)  Compute \(_{i,S_{i}}(x)=\{*{}_{h _{i,S_{i}}}[h(x)],1-*{}_{h _{i,S_{i}}}[h(x)]\}\) for every \(x\)  Find a query distribution by solving \(q^{*}=_{q}*{}_{x q}[_{i,S_{i}}(x)]- }{20}_{x}_{X}(x)}\) subject to \(*{}_{x_{X}}[q(x)] 3\) (3) Query \(x q^{*}\), getting label \(y\)  Set \(w_{i}(h)=w_{i-1}(h)&h(x)=y\\ e^{-}w_{i-1}(h)&h(x) y\) for every \(h H^{}\)  Find the best hypothesis \(\) in \(C\) using the stage two algorithm in Theorem 2.2 return\(\)
```

**Algorithm 1** Competitive Algorithm for Active Agnostic Learning

Generalization for Better Bounds.To get a better dependence for 1d threshold functions, we separate out the Lemma 2.1 bound on (1) from the analysis of the algorithm given a bound on (1). Then for particular instances like 1d threshold functions, we get a better bound on the algorithm by giving a larger bound on (1).

**Theorem 2.3**.: _Suppose that \(_{x}\) and \(H\) are such that, for any distribution \(\) over \(H\) such that no radius-\((c_{4}+c_{5})\) ball has probability more than \(80\%\), there exists a distribution \(q\) over \(X\) such that_

\[*{}_{x q}[r(x)]-}{20}_{x}_{x}(x)}\]

_for some \(>0\). Then for \( c_{1}\), \(c_{4} 300\), \(c_{5}=\) and \(c_{1} 90c_{4}\), let \(N=N(H,_{x},)\) be the size of an \(\)-cover of \(H\). Algorithm 1 solves \((,,)\) active agnostic learning with \(O(+)\) samples._

**Corollary 2.4**.: _There exists a constant \(c_{1}>1\) such that, for \(1d\) threshold functions and \(>c_{1}\), Algorithm 1 solves \((,,)\) active agnostic learning with \(O(}{})\) samples._

Proof.: Because the problem is only harder if \(\) is larger, we can raise \(\) to be \(=/C\), where \(C>1\) is a sufficiently large constant that Theorem 2.3 applies. Then \(1d\) threshold functions have an \(\)-cover of size \(N=O(1/)\). To get the result by Theorem 2.3, it suffices to show \(=(1)\).

Each hypothesis is of the form \(h(x)=1_{x}\), and corresponds to a threshold \(\). So we can consider \(\) to be a distribution over \(\).

Let \(\) be any distribution for which no radius-\(R\) with probability greater than \(80\%\) ball exists, for \(R=c_{4}+c_{5}\). For any percent \(p\) between \(0\) and \(100\), let \(_{p}\) denote the pth percentile of \(\) under \(\) (i.e., the smallest \(t\) such that \([ t] p/100\)). By the ball assumption, \(_{10}\) and \(_{90}\) do not lie in the same radius-\(R\) ball. Hence \(\|h_{_{10}}-h_{_{90}}\|>R\), or

\[_{x}[_{10} x<_{90}]>R.\]

We let \(q\) denote \((_{x}_{10} x<_{90})\). Then for all \(x(q)\) we have \(r(x) 0.1\) and

\[(x)}=}[x(q)]}< .\]

Therefore we can set

\[=*{}_{x q}[r(x)]-}{20}_{x} (x)} 0.1-}{20(c_{4}+c_{5})}  1,\]

as needed. 

## 3 Proof of Lemma 2.1

**Lemma 2.1** (Connection to OPT).: _Define \(\|h-h^{}\|=_{x_{x}}[h(x) h^{}(x)]\). Let \(\) be a distribution over \(H\) such that no radius-\((2+)\) ball \(B\) centered on \(h H\) has probability at least \(80\%\). Let \(m^{*}=m^{*}(H,_{X},,,)\). Then there exists a query distribution \(q\) over \(\) with_

\[*{}_{x q}[r(x)]-_{x}_{X}(x)}}.\]

Proof.: WLOG, we assume that \(_{h}[h(x)=0]_{h}[h(x)=1]\) for every \(x\). This means \(r(x)=*{}_{h}[h(x)]\). This can be achieved by flipping all \(h(x)\) and observations \(y\) for all \(x\) not satisfying this property; such an operation doesn't affect the lemma statement.

We will consider an adversary defined by a function \(g:X\). The adversary takes a hypothesis \(h H\) and outputs a distribution over \(y\{0,1\}^{X}\) such that \(0 y(x) h(x)\) always, and \((h)=*{}_{x,y}[h(x)-y(x)]\) always. For a hypothesis \(h\), the adversary sets \(y(x)=0\) for all \(x\) with \(h(x)=0\), and \(y(x)=0\) independently with probability \(g(x)\) if \(h(x)=1\)--unless \(*{}_{x}[h(x)g(x)]>\), in which case the adversary instead simply outputs \(y=h\) to ensure the expected error is at most \(\) always.

We consider the agnostic learning instance where \(x_{x}\), \(h\), and \(y\) is given by this adversary. Let \(\) be an \((,)\) algorithm which uses \(m\) measurements and succeeds with \(99\%\) probability. Then it must also succeed with \(99\%\) probability over this distribution. For the algorithm to succeed on a sample \(h\), its output \(\) must have \(\|h-\| 2+\). By the bounded ball assumption, for any choice of adversary, no fixed output succeeds with more than \(80\%\) probability over \(h\).

Now, let \(_{0}\) be the behavior of \(\) if it observes \(y=0\) for all its queries, rather than the truth; \(_{0}\) is independent of the input. \(_{0}\) has some distribution over \(m\) queries, outputs some distribution of answers \(\). Let \(q(x)=[_{0}x]\), so \(q\) is a distribution over \(\). Since \(_{0}\) outputs a fixed distribution, by the bounded ball assumption, for \(h\) and arbitrary adversary function \(g\),

\[_{h}[_{0}] 80\%.\]But \(\) behaves identically to \(_{0}\) until it sees its first nonzero \(y\). Thus,

\[99\%[][_{0}]+[ y]\]

and so

\[[y] 19\%.\]

Since \(\) behaves like \(_{0}\) until the first nonzero, we have

\[19\% [y]\] \[=[_{0}xy(x)=1]\] \[[x_{0} y(x)=1]\] \[=m}_{h}}_{y} }_{x q}[y(x)].\]

As an initial note, observe that \(}_{h,y}[y(x)]}_{h}[h(x)]=r(x)\) so

\[}_{x q}[r(x)].\]

Thus the lemma statement holds for \(=0\).

Handling \(>0\).Consider the behavior when the adversary's function \(g:X\) satisfies \(}_{x_{x}}[g(x)r(x)]/10\). We denote the class of all adversary satisfying this condition as \(G\). We have that

\[}_{h}[}_{x_ {x}}[g(x)h(x)]]=}_{x_{x}}[g(x)r(x)] /10.\]

Let \(E_{h}\) denote the event that \(}_{x_{x}}[g(x)h(x)]\), so \([_{h}] 10\%\). Furthermore, the adversary is designed such that under \(E_{h}\), \(}_{y}[y(x)]=h(x)(1-g(x))\) for every \(x\). Therefore:

\[0.19 [_{0}xy(x)=1]\] \[[_{h}]+[_{0}xy(x)=1 E_{h}]\] \[ 0.1+}[x_{0}y(x)=1E_{h}]\] \[=0.1+m}_{h}[}_{h} }_{x q}[}_{y}y(x)]]\] \[=0.1+m}_{h}[}_{h} }_{x q}[h(x)(1-g(x))]]\] \[ 0.1+m}_{x q}[}_{h}[h(x)](1- g(x))]\] \[=0.1+m}_{x q}[r(x)(1-g(x))].\]

Thus

\[_{q}_{g}}_{x q}[r(x)(1-g(x))]  \]

over all distributions \(q\) and functions \(g:X\) satisfying \(}_{x_{x}}[g(x)r(x)]/10\). We now try to understand the structure of the \(q,g\) optimizing the LHS of (4).

Let \(g^{*}\) denote an optimizer of the objective. First, we show that the constraint is tight, i.e., \(}_{x_{x}}[g^{*}(x)r(x)]=/10\). Since increasing \(g\) improves the constraint, the only way this could not happen is if the maximum possible function, \(g(x)=1\) for all \(x\), lies in \(G\). But for this function, the LHS of (4) would be \(0\), which is a contradiction; hence we know increasing \(g\) to improve the objective at some point hits the constraint, and hence \(}_{x_{x}}[g^{*}(x)r(x)]=/10\).

For any \(q\), define \(_{q} 0\) to be the minimum threshold such that

\[}_{x_{x}}[r(x) 1}_{ _{X}(x)}>_{q}}]</10.\]

and define \(g_{q}\) by

\[g_{q}(x):=1&_{X}(x)}>_{q}\\ &_{X}(x)}=_{q}\\ 0&_{X}(x)}<_{q}\]where \(\) is chosen such that \(_{x_{x}}[r(x)g_{q}(x)]=/10\); such an \(\) always exists by the choice of \(_{q}\).

For any \(q\), we claim that the optimal \(g^{*}\) in the LHS of (4) is \(g_{q}\). It needs to maximize

\[}_{x_{X}}[_{X}(x) }r(x)g(x)]\]

subject to a constraint on \(_{x_{X}}[r(x)g(x)]\); therefore moving mass to points of larger \(_{X}(x)}\) is always an improvement, and \(g_{q}\) is optimal.

We now claim that the \(q\) maximizing (4) has \(_{X}(x)}=_{q}\). If not, some \(x^{}\) has \()}{_{X}(x^{})}>_{q}\). Then \(g_{q}(x^{})=1\), so the \(x^{}\) entry contributes nothing to \(_{x q}[r(x)(1-g_{q}(x))]\); thus decreasing \(q(x)\) halfway towards \(_{q}\) (which wouldn't change \(g_{q}\)), and adding the savings uniformly across all \(q(x)\) (which also doesn't change \(g_{q}\)) would increase the objective.

So there exists a \(q\) satisfying (4) for which \([_{X}(x)}>_{q}]=0\), and therefore the set \(T=\{x_{X}(x)}=_{q}\}\) satisfies \(_{_{X}}[r(x)1_{x T}]/10\) and a \(g_{q}\) minimizing (4) is

\[g_{q}(x)=}{_{_{X}}[r(x)1_{ x T}]}.\]

Therefore

\[}_{x q}[r(x)g_{q}(x)] =}_{x_{X}}[_{X}(x)}r(x)}{_{_{X}}[r(x)1_{x T}]}]\] \[_{x}_{X}(x)}\]

and so by (4),

\[}_{x q}[r(x)]-_{x}_{X}(x)}\]

as desired. 

## 4 Conclusion

We have given an algorithm that solves agnostic active learning with (for constant \(\)) at most an \(O([H])\) factor more queries than the optimal algorithm. It is NP-hard to improve upon this \(O([H])\) factor in general, but for specific cases it can be avoided. We have shown that 1d threshold functions, i.e. binary search with adversarial noise, is one such example where our algorithm matches the performance of disagreement coefficient-based methods and is within a \(\) factor of optimal.

## 5 Acknowledgments

Yihan Zhou and Eric Price were supported by NSF awards CCF-2008868, CCF-1751040 (CAREER), and the NSF AI Institute for Foundations of Machine Learning (IFML).