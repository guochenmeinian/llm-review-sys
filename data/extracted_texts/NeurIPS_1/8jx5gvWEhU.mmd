# Order-Optimal Regret in Distributed Kernel Bandits using Uniform Sampling with Shared Randomness

Nikola Pavlovic

Department of Electrical and Computer Engineering

Cornell University

Ithaca, NY, USA

np358@cornell.edu

This work was supported in part by the USDA/NSF AI Institute for Next Generation Food Systems under USDA award number 2020-67021-32855.

Sudeep Salgia

Department of Electrical and Computer Engineering

Carnegie Mellon University

Pittsburgh, PA, USA

ssalgia@andrew.cmu.edu

Qing Zhao

Department of Electrical and Computer Engineering

Cornell University

Ithaca, NY, USA

qz16@cornell.edu

###### Abstract

We consider distributed kernel bandits where \(N\) agents aim to collaboratively maximize an unknown reward function that lies in a reproducing kernel Hilbert space. Each agent sequentially queries the function to obtain noisy observations at the query points. Agents can share information through a central server, with the objective of minimizing regret accumulated over time and agents. We develop the first algorithm that achieves the optimal regret order with a communication cost that is sublinear in both \(N\) and \(T\). The key features of the proposed algorithm are the uniform exploration at the local agents and shared randomness with the central server. Working together with the sparse approximation of the GP model, these two approaches make it possible to preserve the learning rate of the centralized setting at a diminishing rate of communication.

## 1 Introduction

Distributed Kernel Bandits.We study the problem of zeroth-order online stochastic optimization in a distributed setting, where \(N\) agents aim to collaboratively maximize a reward function with communications facilitated by a central server. The reward function \(f:\) is unknown, we only assume it belongs to a Reproducing Kernel Hilbert Space (RKHS) associated with a known kernel \(k\). At each time instant \(t\), each agent \(n\) chooses a point \(x_{t}^{(n)}\) and receives noisy feedback on the function value at the query point. The goal is for each distributed agent to converge to \(x^{*}*{arg\,max}_{x}f(x)\), a global maximizer of \(f\). We quantify this goal as minimizing thecumulative regret summed over a learning horizon of length \(T\) and over all \(N\) agents: \(R(T)=_{n=1}^{N}_{t=1}^{T}(f(x^{*})-f(x_{t}^{(n)}))\).

In addition to learning efficiency, distributed kernel bandits face a new challenge of communication efficiency. Without constraints on the communication cost, all agents can share their local observations and coordinate their individual query actions at no cost. At the other end of the spectrum is a complete decoupling of the agents, resulting in \(N\) independent single-user problems without the benefit of data sharing for accelerated learning. The tension between learning efficiency and communication efficiency is evident. A central question to this trade-off is how to achieve the optimal learning rate enjoyed by the centralized setting using a minimum amount of message exchange among agents.

Main Results.In this paper, we develop the first algorithm for distributed kernel bandits that achieves the optimal order of regret enjoyed by centralized learning with a sublinear message exchange in both \(T\) and \(N\).

To tackle the essential trade-off between learning rate and communication efficiency, a distributed learning algorithm needs a communication strategy that governs _what_ to communicate and _how to integrate_ the shared information into local policies. To minimize the total regret that is accumulating over time and agents, the communication strategy needs to work in tandem with the query actions to ensure a continual flow of information available at all agents for decision-making.

A natural answer to _what_ to communicate in a distributed learning problem is a certain sufficient local statistics of the underlying unknown parameters . However, for kernel bandits, relevant sufficient statistics are infinite-dimensional and hence an impractical choice for communication. Existing studies resolve this issue by exchanging local query actions and observations across all agents and throughout the learning horizon , resulting in a communication cost growing linearly in both \(N\) and \(T\). Even with a communication cost growing linearly in both \(N\) and \(T\) it is not clear how to achieve the performance of a centralized learner with \(NT\) query points. Prevailing approaches in centralized kernel bandits utilize reward-dependent  or adaptive policies . Emulating such policies at each of the \(NT\) query points is practically unfeasible as it would require the agents to _take turns_ in their queries and immediately share the local observations with all other agents.

To tackle the above challenges, our proposed algorithm represents major departures from the prevailing approaches. Referred to as DUETS (Distributed Uniform Exploration of Trimmed Sets), this algorithm has two key features: _uniform exploration_ at the local agents and _shared randomness_ with the central server. In DUETS, each agent employs uniform sampling as the query strategy. Uniform sampling is fully compatible with parallel learning. In particular, note that the union of the local sets of size \(t\) query points obtained at the agents through uniform sampling is identical (in distribution) to the set of size \(Nt\) query points obtained at a centralized decision maker using the same uniform sampling strategy. This superposition property of uniform sampling allows us to leverage the recent results on random exploration in centralized kernel bandits , and is crucial in achieving the optimal learning rate defined by the centralized setting. In terms of communication efficiency, uniform sampling makes it possible to bypass the exchange of query points altogether and reduce the exchange of reward observations through the _shared randomness_ strategy. In DUETS, each agent has access to an independent coin, i.e., a source of randomness, which is unknown to the other agents but is known to the server. The shared randomness enables the server to reproduce the points queried by the agents, thereby resulting in effective transmission of the local set of queried points to the server at _no communication cost_. Please refer to App. C for an additional discussion.

We analyze the performance of DUETS and establish that it incurs a cumulative regret of \(}(}(T/))\) with probability \(1-\), where \(_{NT}\) denotes the maximal information gain of the kernel Srinivas et al.  and \(}()\) hides poly-logarithmic factors. To the best knowledge of the authors, this is the _first_ algorithm to achieve the optimal order of regret for the problem of distributed kernel bandits. We also establish a bound of \(}(_{NT})\) on the communication cost incurred by DUETS (See Section 2 for a precise definition).

Related Work.The existing literature on distributed kernel bandits is relatively slim. The most relevant to our work is that by Li et al. , where the authors consider the problem of distributed contextual kernel bandits and propose a UCB based policy with sparse approximation of GP models and intermittent communication. Their proposed policy was shown to incur a cumulative regret of \(}(_{NT})\) and communication cost of \((N_{NT}^{3})\). The DUETS algorithm proposed in this work, offers an improvement over the algorithm in  both in terms of regret and communication cost. While the contextual setting with varying arm action sets considered in their work is more general that the setting with a fixed arm set considered in this work, their proposed algorithm does not offer non-trivial reduction in regret or communication cost in the fixed arm setting. Moreover, both the regret and communication cost incurred by the algorithm in Li et al.  are not guaranteed to be sublinear in the total number of queries, \(NT\), for all kernels. Consequently, their algorithm does not guarantee convergence to \(x^{*}\) or a non-trivial communication cost for all kernels. On the other hand, both regret and communication cost of DUETS is guaranteed to be sub-linear implying both convergence and communication efficiency.

Among other studies, Du et al.  consider the problem of distributed pure exploration in kernel bandits over finite action set, where they focus on designing learning strategies with low simple regret. In this work, we consider the more challenging continuum-armed setup with a focus on minimizing cumulative regret as opposed to simple regret. Another line of work explores impact of heterogeneity among clients and design algorithms to minimize this impact. Salgia et al.  consider personalized kernel bandits in which agents have heterogeneous models and aim to optimize the weighted sum of their own reward function and the average reward function over all the agents. Dubey and Pentland  consider heterogeneous distributed kernel bandits over a graph in which they use additional kernel-based modeling to measure task similarity across different agents.

In contrast to the distributed kernel bandit, the problems of distributed multi-armed bandits and linear bandits have been extensively studied. For distributed multi-armed bandits (MAB), a variety of algorithms have been proposed for distributed learning under different network topologies Landgren et al. , Shahrampour et al., Sankararaman et al., Chawla et al., Zhu et al.. Shi et al.  and Shi and Shen  have analyzed the impact of heterogeneity among agents in the distributed MAB problem. Similarly, the problem of distributed linear bandits is also well-understood in variety of settings with different network topologies Korda et al. , heterogeneity among agents Mitra et al. , Ghosh et al., Hanna et al. and communication constraints Mitra et al. , Wang et al., Huang et al., Amani et al., Salgia and Zhao.

## 2 Problem Formulation

We consider a distributed learning framework consisting of \(N\) agents indexed by \(\{1,2,,N\}\). Under this framework, we study the problem of collaboratively maximizing an unknown function \(f:\), where \(^{d}\) is a compact, convex set. The function \(f\) belongs to the an RKHS, \(_{k}\), associated with a known positive definite kernel \(k:\). \(_{k}\) is a Hilbert space that is endowed by with an inner product \(,_{_{k}}\) that obeys the reproducing property, and induces the norm \(\|g\|_{_{k}}= g,g_{_{k}}\). We assume \(f\) is finite in this norm i.e \(\|f\|_{_{k}} B\).

Each agent, upon querying a point \(x\), observes \(y=f(x)+\), where \(\) is a zero mean, \(R\)-sub Gaussian noise term assumed to be independent across time and agents. We make the following assumption on the unknown function \(f\), similar to that adopted in Salgia et al .

**Assumption 2.1**.: Let \(_{}=\{x|f(x)\}\) denote the level set of \(f\) for \([-B,B]\). We assume that for all \([-B,B]\), \(_{}\) is a disjoint union of at most \(M_{f}<\) components, each of which is closed and connected. Moreover, for each such component, there exists a bi-Lipschitzian map between each such component and \(\) with normalized Lipschitz constant pair \(L_{f},L_{f}^{}<\).

The communication efficiency is measured using the sum of the uplink and downlink communication costs. In particular, let \(C_{}^{(n)}(T)\) denote the number of real numbers sent by the agent \(n\) to the server over the time horizon. The uplink cost of \(\) is given as \(C_{}^{}(T)=_{n=1}^{N}C_{}^{(n)}(T)\). Similarly, the downlink cost of \(\), \(C_{}^{}(T)\) is the number of real numbers broadcast by the server over the entire time horizon averaged over all agents. The overall communication cost of \(\) is given by \(C^{}(T)=C_{}^{}(T)+C_{}^{}(T)\).

### GP Models

A Gaussian Process (GP) is a random process \(G\) indexed by \(\) and is associated with a mean function \(:\) and a positive definite kernel \(k:\). The random process \(G\) is defined such that for all finite subsets of \(\), \(\{x_{1},x_{2},,x_{m}\}\), \(m\), the random vector \([G(x_{1}),G(x_{2}),,G(x_{m})]^{}\) follows a multivariate Gaussian distribution with mean vector \([(x_{1}),,(x_{n})]]^{}\) and covariance matrix \(=[k(x_{i},x_{j})]_{i,j=1}^{m}\). Throughout the work, we consider GPs with \( 0\). When used as a prior for a data generating process under Gaussian noise, the conjugate property provides closed form expressions for the posterior mean and covariance of the GP model. Specifically, given a set of observations \(\{_{m},_{m}\}=\{(x_{i},y_{i})\}_{i=1}^{m}\) from the underlying process, the expression for posterior mean and variance of GP model is given as follows:

\[_{m}(x) =k_{_{m}}(x)^{}(_{m}+_{ _{m},_{m}})^{-1}_{m}, \] \[_{m}^{2}(x) =(k(x,x)-k_{_{m}}^{}(x)(_{m}+ _{_{m},_{m}})^{-1}k_{_{m}}(x)). \]

In the above expressions, \(k_{_{m}}(x)=[k(x_{1},x),k(x_{2},x) k(x_{n},x)]^{}\), \(_{_{m},_{m}}=\{k(x_{i},x_{j})\}_{i,j=1}^{m}\), \(_{m}\) is the \(m m\) identity matrix and \(\) is the variance of the Gaussian noise.

Following a standard approach in the literature , we model the data corresponding to observations from the unknown \(f\), which belongs to the RKHS of a positive definite kernel \(k\), using a GP with the same covariance kernel \(k\). 2. The benefit of this approach is that the posterior mean and variance of this GP model serve as tools to both predict the values of the function \(f\) and quantify the uncertainty of the prediction at unseen points in the domain [30, Thm. 1].

Sparse approximation of GP models.The sparsification of GP models refers to the idea of approximating the posterior mean and variance of a GP model, corresponding to a set of observations \(\{_{m},_{m}\}\), using a subset of query points \(_{m}\). In particular, let \(\) be a subset of \(_{m}\) consisting of \(r<m\) points. The approximate posterior mean and variance  based on points in \(\), referred to as the inducing set, is given as

\[_{m}(x) =z_{}(x)^{}(_{|| }+_{_{m},}^{}_{_{m}, })^{-1}_{_{m},}^{}_{m} \] \[_{m}^{2}(x) =k(x,x)-z_{}^{}(x)_{_ {m},}^{}_{_{m},}( _{||}+_{_{m},}^{} _{_{m},})^{-1}z_{}(x), \]

where \(z_{}(x)=_{,}^{-}k_{ }(x)\) and \(_{_{m},}=[z_{}(x_{1}),z_{ }(x_{2}),,z_{}(x_{m})]^{}\).

## 3 The DUETS Algorithm

We first describe the randomization at each agent and the shared randomness with the server. Each agent \(n\) has a coin \(_{n}\) for generating random bits that are independent of those generated by other agents. Each agent's coin is unknown to other agents, but known to the central server. In a practical implementation, the coins can be thought of as seeds for generating random numbers.

DUETS employs an epoch-based elimination structure where the domain \(\) is successively trimmed across epochs to maintain an active region that contains a global maximizer \(x^{*}\) with high probability. Specifically, in each epoch \(j\), the server and the agents maintain a common active subset of the domain \(_{j}\) with \(_{1}\) initialized to \(\). The operations in each epoch are as follows.

During the \(j^{}\) epoch, each agent \(n\), using its private coin \(_{n}\), generates \(_{j}^{(n)}\), a set of \(T_{j}=}\) points that are uniformly distributed in the set \(_{j}\)3.Each agent \(n\) queries all the points in \(_{j}^{(n)}\) and obtains \(_{j}^{(n)}^{T_{j}}\), the corresponding vector of reward observations. Since the server has access to the coins of all the agents, it can faithfully reproduce the set \(_{j}=_{n=1}^{N}_{j}^{(n)}\) without any communication between the server and the agents. In order to efficiently communicate the observed reward values from the agents to the server, we leverage sparse approximation of GP models along with the knowledge of the set \(_{j}\) at the server. The server constructs a global inducing set \(_{j}\) by including each point in \(_{j}\) with probability \(p_{j}:=p_{0}_{j,}^{2}\), independent of other points where \(_{j,}^{2}=_{x_{j}}_{j}^{2}(x)\), \(_{j}^{2}()\) is the posterior variance corresponding to points collected in \(_{j}\) and \(p_{0}\) is an appropriately chosen constant. The server broadcasts the inducing set \(_{j}\) to all the agents.

Upon receiving the inducing set, each agent \(n\) computes \(v_{j}^{(n)}:=_{_{j}^{(n)},_{j}}^{}_{ j}^{(n)}^{|_{j}|}\), the projection of its reward vector onto the inducing set. All agents then send the projected observations \(v_{j}^{(n)}\) to the server, which aggregates them to obtain the vector \(_{j}:=(_{|_{j}|}+_{_{j},_{j}}^{}_{_{j},_{j}})^{-1}( _{n=1}^{N}v_{j}^{(n)})\). Note that the summation \(_{n=1}^{N}v_{j}^{(n)}\) equals to \(_{_{j},_{j}}^{}_{j}\), i.e., projection of the rewards of all agents onto the inducing set. The server then broadcasts the vector \(_{j}\) and \(_{j,}\) to all the agents. The benefit of sending \(_{j}\) as opposed to the sum of rewards is that it allows the agents to compute the posterior mean at the agents using their knowledge of the inducing set \(_{j}\) (See. Eqn (3)).

As the last step of the epoch, all the agents and the server trim the current set \(_{j}\) to \(_{j+1}\) using the update rule: \(_{j+1}=x_{j}:_{j}(x)_{x^{ }_{j}}_{j}(x^{})-2(^{{}^{ }})_{j,}},\) where \(^{}=_{T}|(( N T))+4)}\) and \(_{j}(x)=z_{_{j}}^{}(x)_{j}\) is the _approximate_ posterior mean computed based on the inducing set \(_{j}\) (See Eqn. (3)). Below we present the pseudo code for DUETS on the agent's side. For pseudo-code for the server side please refer to App. A.

```
1:Input: Size of the first epoch \(T_{1}\), error probability \(\)
2:\(t 0,j 1\), \(_{1}\)
3:while\(t<T\)do
4:\(_{j}^{(n)}=\)
5:for\(i\{1,2,,T_{j}\}\)do
6: Query a point \(x_{t}^{(n)}\) uniformly at random from \(_{j}\) using the coin \(_{n}\) and observe \(y_{t}^{(n)}\)
7:\(_{j}^{(n)}_{j}^{(n)}\{x_{t}^{(n)}\}\), \(t t+1\)
8:if\(t>T\)then Terminate
9:endfor
10: Receive the global inducing set \(_{j}\),
11: Set \(v_{j}^{(n)}_{_{j}^{(n)},_{j}}^{} _{j}^{(n)}\), where \(_{j}^{(n)}=[y_{t-T_{j}},y_{t-T_{j}+1},,y_{t}]^{}\)
12: Receive \(_{j}\) and \(_{j,}\) from the server and compute \(_{j}()=z_{_{j}}^{}()_{j}\)
13: Update \(_{j}\) to \(_{j+1}\) using Eqn. (3),\(T_{j+1}}\), \(j j+1\)
14:endwhile
```

**Algorithm 1**DUETS : Agent \(n\{1,2,,N\}\)

Performance guarantees.The following theorem characterizes the regret performance and communication cost of DUETS.

**Theorem 3.1**.: _Consider the distributed kernel bandit problem described in Section 2. For a given \((0,1)\), let the policy parameters of_ DUETS _be such that \(T_{1}/N\) and \(p_{0}=72\). Then with probability at least \(1-\), the regret and communication cost incurred by_ DUETS _satisfy the following relations:_

\[R_{}=}(}(T/));  C_{}=}(_{NT}).\]

In the above theorem, \(\) is a constant that is independent of \(N\) and \(T\). As shown in above theorem, DUETS achieves order-optimal regret as it matches the lower bound established in  upto logarithmic factors. DUETS is the _first algorithm_ to close this gap to the lower bound in the distributed setup and achieve order-optimal regret performance. We refer the reader to App. B for a detailed proof of the theorem.

## 4 Conclusion

We propose a new algorithm for the problem of distributed kernel bandits. The proposed algorithm represents major departures from prevailing approaches and has two key features: uniform exploration and shared randomness. It is the _first_ algorithm to achieve optimal-order regret in distributed kernel bandit setting while simultaneously achieving diminishing rates of communication in both the time horizon and the number of agents. We also corroborate our theoretical claims with empirical studies (see App. D).