# AdjointDEIS: Efficient Gradients for Diffusion Models

Zander W. Blasingame

Clarkson University

blasinzw@clarkson.edu &Chen Liu

Clarkson University

cliu@clarkson.edu

###### Abstract

The optimization of the latents and parameters of diffusion models with respect to some differentiable metric defined on the output of the model is a challenging and complex problem. The sampling for diffusion models is done by solving either the _probability flow_ ODE or diffusion SDE wherein a neural network approximates the score function allowing a numerical ODE/SDE solver to be used. However, naive backpropagation techniques are memory intensive, requiring the storage of all intermediate states, and face additional complexity in handling the injected noise from the diffusion term of the diffusion SDE. We propose a novel family of bespoke ODE solvers to the continuous adjoint equations for diffusion models, which we call _AdjointDEIS_. We exploit the unique construction of diffusion SDEs to further simplify the formulation of the continuous adjoint equations using _exponential integrators_. Moreover, we provide convergence order guarantees for our bespoke solvers. Significantly, we show that the continuous adjoint equations for diffusion SDEs actually simplify to a simple ODE. Lastly, we demonstrate the effectiveness of AdjointDEIS for guided generation with an adversarial attack in the form of the face morphing problem. Our code will be released at [https://github.com/zblasingame/AdjointDEIS](https://github.com/zblasingame/AdjointDEIS).

## 1 Introduction

Diffusion models are a large family of state-of-the-art generative models which learn to map samples drawn from Gaussian white noise into the data distribution . These diffusion models have achieved state-of-the-art performance on prominent tasks such as image generation , audio generation , or video generation . Often, state-of-the-art models are quite large and training them is prohibitively expensive . As such, it is fairly common to adapt a pre-trained model to a specific task for post-training. In this way, the generative model can learn new concepts, identities, or tasks without having to train the entire model . Additional work has also proposed algorithms for guiding the generative process of diffusion models .

One method of guiding or directing the generative process is to solve an optimization problem w.r.t. some guidance function \(\) defined in the image space \(^{d}\). This guidance function works on the output of the diffusion model and assesses how "good" the output is. However, the diffusion model works by iteratively removing noise until a clean sample is reached. As such, we need to be able to efficiently backpropagate gradients through the entire generative process. As Song et al.  showed, the diffusion SDE can be simplified to an associated ODE, and as such, many efficient ODE/SDE solvers have been developed for diffusion models . However, naively applying backpropagation to the diffusion model is inflexible and memory intensive; moreover, such an approach is not trivial to apply to the diffusion models that used an SDE solver instead of an ODE solver.

### Contributions

Inspired by the work of Chen et al.  we study the application of continuous adjoint equations to diffusion models, with a focus on training-free guided generation with diffusion models. We introduce several theoretical contributions and technical insights to both improve the ability to perform certain guided generation tasks and to gain insight into guided generation with diffusion models.

First, we introduce _AdjointDEIS_ a bespoke family of ODE solvers which can efficiently solve the continuous adjoint equations for both diffusion ODEs and SDEs. Moreover, we show that the continuous adjoint equations for diffusion SDEs simplify to a mere ODE. Next, we show how to calculate the continuous adjoint equation for conditional information which evolves with _time_ (rather than being constant). To the best of our knowledge, we are the first to consider conditional information which evolves with time for neural ODEs. Overall, multiple theoretical contributions and technical insights are provided to bring a new family of techniques for the guided generation of diffusion models, which we evaluate experimentally on the task of face morphing.

### Diffusion Models

In this subsection, we provide a brief overview of diffusion models. Diffusion models learn a generative process by first perturbing the data distribution into an isotropic Gaussian by progressively adding Gaussian noise to the data distribution. Then a neural network is trained to perform denoising steps, allowing for sampling of the data distribution via sampling of a Gaussian distribution . Assume that we have an \(d\)-dimensional random variable \(^{d}\) with some distribution \(p_{}()\). Then diffusion models begin by diffusing \(p_{}()\) according to the diffusion SDE , an Ito SDE given as

\[_{t}=f(t)_{t}\;t+g(t)\; _{t} \]

where \(t[0,T]\) denotes time with fixed constant \(T>0\), \(f()\) and \(g()\) denote the drift and diffusion coefficients, and \(_{t}\) denotes the standard Wiener process. The trajectories of \(_{t}\) follow the distributions \(p_{t}(_{t})\) with \(p_{0}(_{0}) p_{}()\) and \(p_{T}(_{T})(,)\). Under some regularity conditions Song et al.  showed that Equation (1.1) has a reverse process as time runs backwards from \(T\) to \(0\) with initial marginal distribution \(p_{T}(_{T})\) governed by

\[_{t}=[f(t)_{t}-g^{2}(t)_{}  p_{t}(_{t})]t+g(t)\;}_ {t} \]

Figure 1: A high-level overview of the AdjointDEIS solver to the continuous adjoint equations for diffusion models. The sampling schedule consists of \(\{t_{n}\}_{n=0}^{N}\) timesteps for the diffusion model and \(\{_{n}\}_{n=0}^{M}\) timesteps for AdjointDEIS. The gradients \(_{}(T)\) can be used to optimize \(_{T}\) to find some optimal \(_{T}^{*}\).

where \(}_{t}\) is the standard Wiener process as time runs backwards. Solving Equation (1.2) is what allows diffusion models to draw samples from \(p_{}()\) by sampling \(p_{T}(_{T})\). The unknown term in Equation (1.2) is the _score function_\(_{} p_{t}(_{t})\), which in practice is modeled by a neural network that estimates the scaled score function, \(_{}(_{t},t)-_{t}_{}  p_{t}(_{t})\), or some closely related quantity like \(_{0}\)-prediction .

**Probability Flow ODE.** The practical choice of a step size when discretizing SDEs is limited by the randomness of the Wiener process as a large step size, _i.e._, a small number of steps, can cause non-convergence, particularly in high-dimensional spaces . Sampling an equivalent Ordinary Differential Equation (ODE) over an SDE would enable faster sampling. Song et al.  showed there exists an Ordinary Differential Equation (ODE) whose marginal distribution at time \(t\) is identical to that of Equation (1.2) given as

\[_{t}}{t}=f(t)_{t}-g^{ 2}(t)_{} p_{t}(_{t}). \]

The ODE in Equation (1.3) is known as the _probability flow ODE_. As the noise prediction network, \(_{}(_{t},t)\), is trained to model the scaled score function, Equation (1.3) can be parameterized as

\[_{t}}{t}=f(t)_{t}+(t )}{2_{t}}_{}(_{t},t) \]

w.r.t. the noise prediction network. For brevity, we refer to this as a diffusion ODE.

Although there exist several popular choices for the drift and diffusion coefficients, we opt to use the _de facto_ choice which is known as the Variance Preserving (VP) type diffusion SDE . The coefficients for VP-type SDEs are given as

\[f(t)=_{t}}{t}, g^{2}(t)=_{t}^{2}}{t}-2_{t}}{ t}_{t}^{2}, \]

which corresponds to sampling \(_{t}\) from the distribution \(q(_{t}_{0})=(_{t}_{0}, _{t}^{2})\).

## 2 Adjoint Diffusion ODEs

**Problem statement.** Given the diffusion ODE in Equation (1.4), we wish to solve the following optimization problem:

\[_{T},,}{}\  _{T}+_{T}^{0}f(t)_{t}+(t)}{2_{t}}_{}(_{t},,t)\ t. \]

_I.e._, we seek to find the optimal \(_{T}\), \(\), and \(\) that satisfies our guidance function \(\). _N.B._, the noise-prediction model is conditioned on additional information \(\).

Unlike GANs which can update the latent representation through GAN inversion , as seen in Equation (2.1) diffusion models require more care as they model an ODE or SDE and require numerical solvers. Therefore, to update the latent representation, model parameters, and conditional information, we must backpropagate the gradient of loss defined on the output, \((_{0})/_{0}\) through the entire ODE or SDE.

A key insight of this work is the connection between the adjoint ODE used in neural ODEs by Chen et al.  and specialized ODE/SDE solvers by  for diffusion models. It has been well observed that diffusion models are a type of neural ODE . Since a diffusion model can be thought of as a neural ODE, we can solve the continuous adjoint equations  to find useful gradients for guided generation. We can then exploit the unique structure of diffusion models to develop efficient bespoke ODE solvers for the continuous adjoint equations.

Let \(_{}\) describe a parameterized neural field of the probability flow ODE, _i.e._, the R.H.S of Equation (1.4), defined as

\[_{}(_{t},,t)=f(t)_{t}+(t) }{2_{t}}_{}(_{t},,t). \]

Then \(_{}(_{t},,t)\) describes a neural ODE which admits an adjoint state, \(_{}/_{t}\) (and likewise for \(_{}(t)\) and \(_{}(t)\)), which solve the continuous adjoint equations [25, Theorem 5.2] in the form of the following Initial Value Problem (IVP):

\[_{}(0) =}{_{0}}, _{}}{t}(t) =-_{}(t)^{}_{}( _{t},,t)}{_{t}},\] \[_{}(0) =, _{}}{t}(t) =-_{}(t)^{}_{}( _{t},,t)}{},\] \[_{}(0) =, _{}}{t}(t) =-_{}(t)^{}_{}( _{t},,t)}{}. \]

We refer to this system of equations in Equation (2.3) as the _adjoint diffusion ODE1_ as it describes the continuous adjoint equations for the empirical probability flow ODE.

_N.B._, in the literature of diffusion models, the sampling process is often done in reverse time _i.e._, the initial noise is \(_{T}\) and the final sample is \(_{0}\). Due to this convention, solving the adjoint diffusion ODE _backwards_ actually means integrating _forwards_ in time. Thus, while diffusion models learn to compute \(_{t}\) from \(_{s}\) with \(s>t\), the adjoint diffusion ODE seeks to compute \(_{}(s)\) from \(_{}(t)\).

### Simplified Formulation of the Empirical Adjoint Probability Flow ODE

We show that rather than treating \(_{}\) as a black box, the specific structure of the probability flow ODE is carried over to the adjoint probability flow ODE, allowing the adjoint probability flow ODE to be simplified into a special exact formulation.

By evaluating the gradient of \(_{}\) w.r.t. \(_{t}\) for each term in Equation (2.2) we can rewrite the adjoint diffusion ODE for \(_{}(t)\) in Equation (2.3) as

\[_{}}{t}(t)=-f(t)_{ }(t)-(t)}{2_{t}}_{}(t)^{} _{}(_{t},,t)}{ _{t}}. \]

Due to the gradient of the drift term in Equation (2.4), further manipulations are required to put the empirical adjoint probability flow ODE into a sufficiently "nice" form. We follow the approach used by [16; 18] to simplify the empirical probability flow ODE with the use of exponential integrators and a change of variables. By applying the integrating factor \((_{0}^{t}f()\ )\) to Equation (2.4), we find:

\[}{t}e^{_{0}^{t}f()\ } _{}(t)=-e^{_{0}^{t}f()\ }(t)}{2_{t}}_{}(t)^{ }_{}(_{t},,t)}{ _{t}}. \]

Then, the exact solution at time \(s\) given time \(t<s\) is found to be

\[_{}(s)=^{t}f()\ }_{}(t)}_{}-^{s}e^{_{s}^{u}f()\ }(u)}{2_{u}}_{}(u)^{} _{}(_{u},,u)}{_{u}}\ u}_{}. \]

Like with solvers for diffusion models which leverage exponential integrators, we are able to transform the adjoint diffusion ODE into a non-stiff form by separating the linear and non-linear component. Moreover, we can compute the linear in closed form, thereby _eliminating_ the discretization error in the linear term. However, we still need to approximate the non-linear term which consists of a difficult integral about the complex noise-prediction model. This is where the insight of Lu et al.  to integrate in the log-SNR domain becomes invaluable. Let \(_{t}:=(_{t}/_{t})\) be one half of the log-SNR. Then, with using this new variable and computing the drift and diffusion coefficients in closed form, we can rewrite Equation (2.6) as

\[_{}(s)=}{_{s}}_{}(t)+}_{t}^{s}_{u}_{u}_{u}}{u}_{}(u)^{}_{}(_{u},,u)}{_{u}}\ u. \]

As \(_{t}\) is a strictly decreasing function w.r.t. \(t\) it therefore has an inverse function \(t_{}\) that satisfies \(t_{}(_{t})=t\), and, with abuse of notation, we let \(_{}_{t_{}()}\), \(_{}()_{}(t_{}())\), \(\&c.\) and let the reader infer from context if the function is mapping the log-SNR back into the time domain or already in the time domain. Then by rewriting Equation (2.7) as an exponentially weighted integral and performing an analogous derivation for \(_{}(t)\) and \(_{}(t)\), we arrive at the following.

**Proposition 2.1** (Exact solution of adjoint diffusion ODEs).: _Given initial values \([_{}(t),_{}(t),_{}(t)]\) at time \(t(0,T)\), the solution \([_{}(s),_{}(s),_{}(s)]\) at time \(s(t,T]\) of adjoint diffusion ODEs in Equation (2.3) is_

\[_{}(s) =}{_{s}}_{}(t)+}_{_{t}}^{_{s}}_{}^{2}e^{-} _{}()^{}_{}( _{},,)}{_{}}\; , \] \[_{}(s) =_{}(t)+_{_{t}}^{_{s}} _{}e^{-}_{}()^{}_{}(_{},,)}{ }\;,\] (2.9) \[_{}(s) =_{}(t)+_{_{t}}^{_{s}}_{ }e^{-}_{}()^{}_{}(_{},,)}{}\; . \]

The complete derivations of Proposition 2.1 can be found in Appendix B.1.

There is a nice symmetry between Equations (2.8) to (2.10), while the adjoint of the solution trajectories evolves with a weighting of \(_{t}/_{s}\) in the linear term and the integral term is weighted by \(_{t}^{2}/_{s}^{2}\) reflecting the double partial \(_{t}\) in the adjoint and Jacobian terms. Conversely, the adjoint state for the conditional information and model parameters evolves with no weighting on the linear term and the integral is only weighted by \(_{t}/_{s}\). This follows from the vector fields being independent of \(_{}\) and \(_{}\). These equations, while reflecting the special nature of this formulation of diffusion models, also have an appealing parallel with the exact solution for diffusion ODEs Lu et al. [16, Proposition 3.1].

### Numerical Solvers for AdjointDEIS

The numerical solver for the adjoint empirical probability flow ODE, now in light of Equation (2.8), only needs to focus on approximating the exponentially weighted integral of \(_{}\) from \(_{t}\) to \(_{s}\), a well-studied problem in the literature on exponential integrators . To approximate this integral, we evaluate the Taylor expansion of the Jacobian vector product to further simplify the ODE. For notational convenience let \((;t)\) denote the scaled vector-Jacobian product of the adjoint state \(_{}(t)\) and the gradient of the model w.r.t. \(_{t}\), _i.e._,

\[(;t)=_{t}^{2}_{}(t)^{} {_{}(_{t},,t)}{_{t}}, \]

and likewise we let \(^{(n)}(;)\) denote the \(n\)-th derivative w.r.t. to \(\). For \(k 1\), the \((k-1)\)-th Taylor expansion at \(_{t}\) is

\[(;)=_{n=0}^{k-1})^{n }}{n!}^{(n)}(;_{t})+((-_{t })^{k}). \]

Plugging this expansion into Equation (2.8) and letting \(h=_{s}-_{t}\) yields

\[_{}(s)=}{_{s}}_{}(t)}_{\\ }+}_{n=0}^{k-1 }^{(n)}(;_{t})}_{\\ }}^{_{s}})^{n}}{n!}e^{-}\;}_{\\ }+(h^{k+1})}_{\\ }. \]

With this expansion, the number of terms which need to be estimated is further reduced as the exponentially weighted integral \(_{_{t}}^{_{s}})^{n}}{n!}e^{-}\; \) can be solved **analytically** by applying \(n\) times integration-by-parts . Therefore, the only errors in solving this ODE occur in the approximation of the \(n\)-th order total derivatives of the vector-Jacobian product and the higher-order error terms \((h^{k+1})\). By dropping the \((h^{k+1})\) error term and approximating the first \((k-1)\)-th derivatives of the vector-Jacobian product, we can derive \(k\)-th order solvers for adjoint diffusion ODEs. We decide to name such solvers as _Adjoint Diffusion Exponential Integrator Sampler (AdjointDEIS)_ reflecting our use of the _exponential integrator_ to simplify the ODEs and pay homage to DEIS from  that explored the use of exponential integrators for diffusion ODEs. Consider the case of \(k=1\), by dropping the error term \((h^{2})\) we construct the AdjointDEIS-1 solver with the following algorithm.

**AdjointDEIS-1.** Given an initial augmented adjoint state \([_{}(t),_{}(t),_{}(t)]\) at time \(t(0,T)\), the solution \([_{}(s),_{}(s),_{}(s)]\) at time \(s(t,T]\) is approximated by

\[_{}(s) =}{_{s}}_{}(t)+_{ s}(e^{h}-1)^{2}}{_{s}^{2}}_{}(t)^{ }_{}(_{t},,t)}{ _{t}},\] \[_{}(s) =_{}(t)+_{s}(e^{h}-1)}{ _{s}}_{}(t)^{}_{ }(_{t},,t)}{},\] \[_{}(s) =_{}(t)+_{s}(e^{h}-1)}{ _{s}}_{}(t)^{}_{ }(_{t},,t)}{}. \]

Higher-order expansions of Equation (2.12) require estimations of the \(n\)-th order derivatives of the vector Jacobian product which can be approximated via _multi-step_ methods, such as Adams-Bashforth methods . This has the added benefit of reduced computational overhead, as the multi-step method just reuses previous values to approximate the higher-order derivatives. Moreover, multi-step methods are empirically more efficient than single-step methods . Combining the Taylor expansions in Equation (2.12) with techniques for designing multi-step solvers, we propose a novel multi-step second-order solver for the adjoint empirical probability flow ODE which we call _AdjointDEIS-2M_. This algorithm combines the previous values of the vector Jacobian product at time \(t\) and time \(r\) to predict \(_{s}\)_without_ any additional intermediate values.

**AdjointDEIS-2M.** We assume having a previous solution \(_{}(r)\) and model output \(_{}(_{r},,r)\) at time \(r<t<s\), let \(\) denote \(=-_{r}}{h}\). Then the solution \(_{s}\) at time \(s\) to Equation (2.4) is estimated to be

\[_{}(s)=}{_{s}}_{}(t) +_{s}(e^{h}-1)^{2}}{_{s}^{2}}_{}(t)^{}_{}(_{t}, ,t)}{_{t}}\] \[+_{s}-1}{2}^{2}}{ _{s}^{2}}_{}(t)^{}_{ }(_{t},,t)}{_{t}}- ^{2}}{_{s}^{2}}_{}(t)^{}_{}(_{r},,r)}{_{r}} . \]

For brevity, we omit the details of the AdjointDEIS-2M solver for \(_{}(t)\) and \(_{}(t)\); rather, we provide the complete derivation and details in Appendix B. Likewise, the full algorithm can be found in Appendix G.1. The advantage of a higher-order solver is that it is generally more efficient, requiring fewer steps due to its higher convergence order. We show that AdjointDEIS-\(k\) is a \(k\)-th order solver, as stated in the following theorem. The proof is in Appendix C.

**Theorem 2.1** (AdjointDEIS-\(k\) as a \(k\)-th order solver).: _Assume the function \(_{}(_{t},,t)\) and its associated vector-Jacobian products follow the regularity conditions detailed in Appendix C, then for \(k=1,2\), AdjointDEIS-\(k\) is a \(k\)-th order solver for adjoint diffusion ODEs, i.e., for the sequence \(\{}_{}(t_{i})\}_{i=1}^{M}\) computed by AdjointDEIS-\(k\), the global truncation error at time \(T\) satisfies \(}_{}(t_{M})-_{}(T)=(h _{max}^{2})\), where \(h_{max}=_{1 j M}(_{t_{i}}-_{t_{i-1}})\). Likewise, AdjointDEIS-\(k\) is a \(k\)-th order solver for the estimated gradients w.r.t. \(\) and \(\)._

As previous work has shown that higher-order solvers may be unsuitable for large guidance scales  we do explicitly construct or analyze any solvers for \(k>2\) and leave such explorations for future study.

### Scheduled Conditional Information

Thus far, we have held the conditional information constant across time, _i.e._, at each time \(t[0,T]\) the conditional information supplied to the neural network is \(\). What if, however, we had some scheduled conditional information \(_{t}\)? We show that with some mild assumptions, using scheduled conditional information \(_{t}\) does not actually change the continuous adjoint equation for \(_{t}\) from the equations derived from \(\) sans a substitution of \(_{t}\) with \(\).

While motivated by the case of scheduled conditional information in guided generation with diffusion models, this result applies to neural ODEs more generally, which could open future research directions. We state this result more formally in Theorem 2.2 with the proof in Appendix D. Note, as this applies more generally than to just AdjointDEIS, so we express this result for some arbitrary neural ODE with vector field \(_{}(_{t},_{t},t)\) and use the forward-time flow convention rather than the reverse-time convention of diffusion models.

**Theorem 2.2**.: _Suppose there exists a function \(:[0,T]^{z}\) which can be defined as a cadlag2 piecewise function where \(\) is continuous on each partition of \([0,T]\) given by \(=\{0=t_{0}<t_{1}<<t_{n}=T\}\) and whose right derivatives exist for all \(t[0,T]\). Let \(_{}:^{d}^{z} ^{d}\) be continuous in \(t\), uniformly Lipschitz in \(\), and continuously differentiable in \(\). Let \(:^{d}\) be the unique solution for the ODE_

\[}{t}(t)=_{}(_{t},_{t},t),\]

_with initial condition \((0)=_{0}\). Let \(:^{d}\) be a scalar-valued loss function defined on the output of the neural ODE. Then \(/(t)_{}(t)\) and there exists a unique solution \(_{}:^{z}\) to the following IVP:_

\[_{}(T)=,_{ }}{t}(t)=-_{}(t)^{}_{}(_{t},_{t},t)}{_ {t}}.\]

## 3 Adjoint Diffusion SDEs

As recent work [30; 31] has shown, diffusion SDEs have useful properties over probability flow ODEs for image manipulation and editing. In particular, it has been shown that probability flow ODEs are invariant in Nie et al. [31, Theorem 3.2] and that diffusion SDEs are contractive in Nie et al. [31, Theorem 3.1], _i.e._, any gap in the mismatched prior distributions \(p_{t}(_{t})\) and \(_{t}(_{t})\) for the true distribution \(p_{t}\) and edited distribution \(_{t}\) will remain between \(p_{0}(_{0})\) and \(_{0}(_{0})\), whereas for diffusion SDEs the gap can be reduced between \(_{t}(_{t})\) and \(p_{t}(_{t})\) as \(t\) tends towards \(0\). Motivated by this reasoning, we present a framework for solving the adjoint diffusion SDE using exponential integrators.

The diffusion SDE with noise prediction model is given by

\[_{t}=[f(t)_{t}+(t)}{_{t}} _{}(_{t},,t)]\, t+g(t)\;}_{t}, \]

where '\(t\)' is an infinitesimal _negative_ timestep. Note how the drift term of the SDE looks remarkably similar to the probability flow ODE sans a missing factor of \(1/2\) in front of the noise prediction model. This is due to differing manipulations of the forward Kolomogorov equations--which describe the evolution of \(p_{t}(_{t})\)--used by Anderson  to derive the reverse-time SDE and later by Song et al.  to derive the probability-flow ODE. This connection is _very_ important as it enables one to simplify the AdjointDEIS solvers for the adjoint diffusion SDE.

We show that for the special case of Stratonovich SDEs 3 with a diffusion coefficient \((t)\) which does not depend on the process state \(_{t}\), then the adjoint process has a unique strong solution that evolves with what is essentially an ODE. Intuitively, this tracks as the stochastic term \((t)_{t}\) has nothing to do with \(_{t}\). We state this observation somewhat informally in the following theorem. The proof can be found in Appendix E.

**Theorem 3.1**.: _Let \(:^{d}^{d}\) be in \(^{,1}_{b}\) and \(:^{d w}\) be in \(^{1}_{b}\). Let \(:^{d}\) be a scalar-valued differentiable function. Let \(_{t}:[0,T]^{w}\) be a \(w\)-dimensional Wiener process. Let \(:[0,T]^{d}\) solve the Stratonovich SDE_

\[_{t}=(_{t},t)\;t+ (t)_{t},\]

_with initial condition \(_{0}\). Then the adjoint process \(_{}(t)(_{T})/ _{t}\) is a strong solution to the backwards-in-time ODE_

\[_{}(t)=-_{}(t)^{}}{_{t}}(_{t},t)\;t. \]

This is a boon for us, as diffusion models use only a mere scalar diffusion coefficient, \(g(t)\). Therefore, the continuous adjoint equations for the diffusion SDE just simplify to an ODE. Not only that, but as mentioned before, the drift term of the diffusion SDE and probability flow ODE differ only by a factor of \(2\) in the term with the noise prediction network. As only the drift term of the diffusionSDE is used when constructing the continuous adjoint equations, it follows that the only difference between the continuous adjoint equations for the probability flow ODE and diffusion SDE is a factor of 2. Therefore, the exact solutions are given by:

**Proposition 3.1** (Exact solution of adjoint diffusion SDEs).: _Given initial values \([_{}(t),_{}(t),_{}(t)]\) at time \(t(0,T)\), the solution \([_{}(s),_{}(s),_{}(s)]\) at time \(s(t,T]\) of adjoint diffusion SDEs is_

\[_{}(s) =}{_{s}}_{}(t)+}_{_{t}}^{_{s}}_{}^{2}e^{-} _{}()^{}_{}( _{},,)}{_{}}\;, \] \[_{}(s) =_{}(t)+2_{_{t}}^{_{s}} _{}e^{-}_{}()^{}_{}(_{},,)}{ }\;,\] (3.4) \[_{}(s) =_{}(t)+2_{_{t}}^{_{s}}_ {}e^{-}_{}()^{}_{}(_{},,)}{}\; . \]

**Remark 3.1**.: _While the adjoint diffusion SDEs evolve with an ODE, the same cannot be said for the underlying state, \(_{t}\). Rather this evolves with a backwards SDE (more details in Appendix E) which requires the **same** realization of the Wiener process used to sample the image as the one used in the backwards SDE._

### Solving Backwards Diffusion SDEs

Lu et al.  propose the following first-order solver for diffusion SDEs

\[_{t}=}{_{s}}_{s}-2_{t}(e^{h}- 1)_{}(_{s},s)+_{t}-1}_{s}, \]

where \(_{s}(,)\). To solve the SDE backwards in time, we follow the approach initially proposed by Wu and la Torre  and used by later works . Given a particular realization of the Wiener process that admits \(_{t}(_{t}_{0}_{t}^{2} )\), then for two samples \(_{t}\) and \(_{s}\) the noise \(_{s}\) can be calculated by rearranging Equation (3.6) to find

\[_{s}=_{t}-}{_{s}} _{s}+2_{t}(e^{h}-1)_{}(_{s},,s)} {_{t}-1}} \]

With this the sequence \(\{_{t_{i}}\}_{i=1}^{N}\) of added noises can be calculated which will **exactly** reconstruct the original input from the initial realization of the Wiener process. This technique is referred to as _Cycle-SDE_ after the CycleDiffusion paper .

## 4 Related Work

Our proposed solutions can be viewed as a _training-free_ method for guided generation. As an active area of research, there have been several proposed approaches to the problem of training-free guided generation, which either dynamically optimize the solution trajectory during sampling [34; 35; 36], or optimize the whole solution trajectory [37; 38; 39; 40]. Our solutions fall into the latter category of optimizing the whole solution trajectory along with additional conditional information.

While Nie et al.  explored the use of the continuous adjoint equations to optimize the solution trajectories of diffusion SDEs they don't consider the ODE case and make use of the _special_ structure of diffusion SDEs to simplify the continuous adjoint equations as we did. Recent work by Pan et al.  explore the using continuous adjoint equations for guided generation but does not consider the SDE case and uses a different scheme to simplify the continuous adjoint equations. We provide a more detailed comparison against these approaches and further discussion on related methods in Appendix A.

## 5 Experiments

To illustrate the efficacy of our technique, we examine an application of guided generation in the form of the face morphing attack. The face morphing attack is a new emerging attack on Face Recognition (FR) systems. This attack works by creating a singular morphed face image \(_{0}^{(ab)}\) that shares biometric information with the two contributing faces \(_{0}^{(a)}\) and \(_{0}^{(b)}\). A successfully created morphed face image can trigger a false accept with either of the two contributing identities in the targeted Face Recognition (FR) system, see Figure 2 for an illustration. Recent work in this space has explored the use of diffusion models to generate these powerful attacks . All prior work on diffusion-based face morphing used a pre-trained diffusion autoencoder  trained on the FFHQ  dataset at a \(256 256\) resolution. We illustrate the use of AdjointDEIS solvers by modifying the Diffusion Morph (DiM) architecture proposed by Blasingame and Liu  to use the AdjointDEIS solvers to find the optimal initial noise \(_{T}^{(ab)}\) and conditional \(_{ab}\). The AdjointDEIS solvers are used to calculate the gradients with respect to the identity loss  defined as

\[_{ID}=d(v_{ab},v_{a})+d(v_{ab},v_{b}),_{diff}=d(v_{ab},v_{a})-d(v_{ab},v_{b}), \] \[_{ID}^{}=_{ID}+_{diff},\]

where \(v_{a}=F(_{0}^{(a)}),v_{b}=F(_{0}^{(b)}),v_{ab}=F(_{0}^{(ab)})\), and \(F: V\) is an FR system which embeds images into a vector space \(V\) which is equipped with a measure of distance, \(d\). We used the ArcFace  FR system for identity loss.

We compare against three preexisting DiM methods, the original DiM algorithm , Fast-DiM , and Morph-PIPE  as well as a GAN-inversion-based face morphing attack, MIPGAN-I and MIPGAN-II  based on the StyleGAN  and StyleGAN2  architectures respectively. Fast-DiM improves DiM by using higher-order ODE solvers to decrease the number of sampling steps required to create a morph. Morph-PIPE performs a very simple version of guided generation by generating a large batch of morphed images derived from a discrete set of interpolations between \(_{T}^{(a)}\) and \(_{T}^{(b)}\), and \(_{a}\) and \(_{b}\). For reference purposes, we compare against a reference GAN-based method  which uses GAN-inversion w.r.t.to the identity loss to find the optimal morphed face, and we include prior state-of-the-art Webmorph, a commercial off-the-shelf system .

We run our experiments on SYN-MAD 2022  morphed pairs that are constructed from the Face Research Lab London dataset , more details in Appendix H.4. The morphed images are evaluated against three FR systems, the ArcFace , ElasticFace , and AdaFace  models;

Figure 3: Comparison of DiM morphs on the FRLL dataset. From left to right, identity \(a\), DiM-A, Fast-DiM, Morph-PIPE, AdjointDEIS (ODE), AdjointDEIS (SDE), and identity \(b\).

Figure 2: Example of guided morphed face generation with AdjointDEIS on the FRLL dataset.

further details are found in Appendix H.5. To measure the efficacy of a morphing attack, the Mated Morph Presentation Match Rate (MMPMR) metric  is used. The MMPMR metric as proposed by Scherhag et al.  is defined as

\[M()=_{m=1}^{M}\{[_{\,n\{1, ,N_{m}\}}S_{m}^{n}]>\} \]

where \(\) is the verification threshold, \(S_{m}^{n}\) is the similarity score of the \(n\)-th subject of morph \(m\), \(N_{m}\) is the total number of contributing subjects to morph \(m\), and \(M\) is the total number of morphed images.

In our experiments, we used a learning rate of \(0.01\), \(N=20\) sampling steps, \(M=20\) steps for AdjointDEIS, and 50 optimization steps for gradient descent. For the sampling process we used the DDIM solver , a widely used first-order solver. Following  we observed that using recorded values of \(\{_{t_{i}}\}_{i=1}^{N}\) for the backward pass improved performance. Note, this does not mean we stored the vector-Jacobians or any other internal states of the neural network. Moreover, due to our use of Cycle-SDE this choice was mandated for the SDE case. We discussion this decision further in Appendix F.1.

In Table 1 we present the effectiveness of the morphing attacks against the three FR systems. Guided generation with AdjointDEIS massively increases the performance of DiM, supplanting the old state-of-the-art for face morphing. Interestingly, the SDE variant did not fare as well as the ODE variant. This is likely due to the difficulty in discretizing SDEs with large step sizes . We present further results in Appendix F that explore the impact of the choice of learning rate and the number of discretization steps for AdjointDEIS.

## 6 Conclusion

We present a unified view on guided generation by updating latent, conditional, and model information of diffusion models with a guidance function using the continuous adjoint equations. We propose AdjointDEIS, a family of solvers for the continuous adjoint equations of diffusion models. We exploit the unique construction of diffusion models to create efficient numerical solvers by using exponential integrators. We prove the convergence order of solvers and show that the continuous adjoint equations for diffusion SDEs evolve with an ODE. Furthermore, we show how to handle conditional information that is scheduled in time, further expanding the generalizability of the proposed technique. Our results in face morphing show that the gradients produced by AdjointDEIS can be used for guided generation tasks.

**Limitations.** There are several limitations. Empirically, we only explored a small subset of the true potential AdjointDEIS by evaluating on a single scenario, _i.e._, face morphing. Likewise, we only explored a few different hyperparameter options. In particular, we did not explore much the impact of the number of optimization steps and the number of sampling steps for diffusion SDEs on the visual quality of the generated face morphs.

**Broader Impact.** Guided generation techniques can be misused for a variety of harmful purposes. In particular, our approach provides a powerful tool for adversarial attacks. However, better knowledge of such techniques should hopefully help direct research in hardening systems against such kinds of attacks.

    & &  \\ 
**Morphing Attack** & **NFE**(\(\)) & **AdaFace** & **ArcFace** & **ElasticFace** \\  Webmorph  & - & 97.96 & 96.93 & 98.36 \\ MIPGAN-I  & - & 72.19 & 77.51 & 66.46 \\ MIPGAN-II  & - & 70.55 & 72.19 & 65.24 \\ DiM-A  & 350 & 92.23 & 90.18 & 93.05 \\ Fast-DiM  & 300 & 92.02 & 90.18 & 93.05 \\ Morph-PIPE  & 2350 & 95.91 & 92.84 & 95.5 \\
**DiM + AdjointDEIS-1 (ODE)** & 2250 & **99.8** & **98.77** & **99.39** \\
**DiM + AdjointDEIS-1 (SDE)** & 2250 & 98.57 & 97.96 & 97.75 \\   

Table 1: Vulnerability of different FR systems across different morphing attacks on the SYN-MAD 2022 dataset. FMR = 0.1%.