# An Efficient Recipe for Long Context Extension via Middle-Focused Positional Encoding

Tong Wu

wutong1@bigai.ai

Yanpeng Zhao

zhaoyanpeng@bigai.ai

Zilong Zheng\({}^{}\)

zlzheng@bigai.ai

State Key Laboratory of General Artificial Intelligence, BIGAI, Beijing, China

\({}^{}\) Corresponding author.

###### Abstract

Recently, many methods have been developed to extend the context length of pre-trained large language models (LLMs), but they often require fine-tuning at the target length (\( 4K\)) and struggle to effectively utilize information from the middle part of the context. To address these issues, we propose **C**ontinuity-**R**elativity ind**E**xing with g**A**ussian **M**iddle (CREAM), which interpolates positional encodings by manipulating position indices. Apart from being simple, CREAM is training-efficient: it only requires fine-tuning at the pre-trained context window (_e.g._, Llama 2-4K) and can extend LLMs to a much longer target context length (_e.g._, 256K). To ensure that the model focuses more on the information in the middle, we introduce a truncated Gaussian to encourage sampling from the middle part of the context during fine-tuning, thus alleviating the "Lost-in-the-Middle" problem faced by long-context LLMs. Experimental results show that CREAM successfully extends LLMs to the target length for both Base and Chat versions of Llama2-7B with "Never Miss A Beat". Our code is publicly available at [https://github.com/bigai-nlco/cream](https://github.com/bigai-nlco/cream).

## 1 Introduction

Transformer-based Large Language Models (LLMs) are typically pre-trained with a fixed context window size, _e.g._, 4K tokens in Touvron et al. (2023). However, many downstream applications, including in-context learning (Huang et al., 2023; Li et al., 2023) and LLM agents (Qian et al., 2023; Zheng et al., 2023) necessitate the processing of significantly longer contexts, _e.g._, up to 256K tokens. Recent works have proposed promising approaches that efficiently extend the context window of

Figure 1: Results of applying different position interpolation methods to the “Lost-in-the-Middle” task on CREAM and PoSE (Zhu et al., 2023). We can see that CREAM outperforms PoSE (Zhu et al., 2023) at every position, with a particularly improvement in the middle.

pre-trained LLMs by interpolating Positional Encodings (PEs) (Chen et al., 2023; Peng and Quesnelle, 2023; Peng et al., 2023; Xiong et al., 2023; Zhang et al., 2024) with a short period of fine-tuning. Unlike other techniques such as efficient transformer (Tworkowski et al., 2024; Munkhdalai et al., 2024) and memory augmentation (Tan et al., 2024), PE-based methods do not necessitate alterations to the model's architecture or the incorporation of supplementary modules. Consequently, PE-based methods offer the advantages of straightforward implementation and rapid adaptation, making them a practical solution for extending the operational range of LLMs in tasks involving larger context windows.

Despite the simplicity and effectiveness, existing PE-based methods exhibit two significant limitations. **First,** prior approaches, such as positional interpolation (Chen et al., 2023), still require fine-tuning on the target context window size, which imposes a substantial _computational overhead_(Zhu et al., 2023). **Secondly,** though some PE methods have demonstrated potential in handling extremely long sequences, as evidenced by low sliding window perplexity scores, their performance deteriorates notably in "in-the-middle" scenarios (Liu et al., 2024). Specifically, when the model is required to accurately retrieve and process content located in the middle of an extended context, there is a marked drop in performance on the extended window size (Figure 1 and Figure 3).

These observations and insights underscore a fundamental question: _Can we extend the context window size of pre-trained LLMs efficiently while simultaneously optimizing their effectiveness in processing "in-the-middle" content?_

To answer the above question, we propose CREAM, namely **C**ontinuity-**R**elativity ind**E**xing with **g**A**ussian **M**iddle. CREAM is a novel PE-based fine-tuning recipe that shows both efficiency in fine-tuning and effectiveness in enhanced middle content understanding. Our key insights lie in manipulating the positional indices of long target sequences to produce shorter ones within the pre-trained context window size (Figure 2).

In Section 2.1, we summarize two crucial ingredients of effective positional indices: continuity that produces densely connected positional indices and relativity that reveals the long-range dependencies between fragments. CREAM is a recipe designed with the best of both worlds by introducing two indexing strategies for continuity and relativity, respectively (Section 2.2). Besides, to alleviate the "Lost-in-the-Middle" challenge, we introduce truncated Gaussian distribution for middle segment sampling, enabling the LLM to prioritize the information in the middle positions, even when performing positional interpolation within the pre-trained context window size.

In Section 3, we conduct comprehensive experiments to demonstrate the efficiency and effectiveness of CREAM. We continually pre-trained on Llama 2-7B with CREAM for a short period and extend the context window size from 4K to up to 256K. Furthermore, we instruction tuning on Llama 2-7B-Chat with CREAM for 100 steps and obtain promising results. We highlight our empirical advantages as:

1. CREAM can not only fine-tune within the pre-training context window size, but also alleviate the issue of the model easily getting lost in the middle. _e.g._, CREAM-YaRN outperforms PoSE-YaRN (Zhu et al., 2023) by over 20% on average in the "Lost in the Middle" (Liu et al., 2024) task.
2. CREAM can further be enhanced by integrating novel designs on positional interpolation frequencies (such as Linear (Chen et al., 2023), NTK (Peng and Quesnelle, 2023), Yarn (Peng et al., 2023), _etc._), and can be extended to context window sizes of up to 256K or beyond.
3. CREAM-Chat model requires only 100 steps of instruction-tuning to achieve nearly perfect performance on the Needle-in-a-Haystack pressure test, and it outperforms existing strong baselines on LongBench (Bai et al., 2023).

## 2 Methodology

### Preliminaries

Problem Formulation.Given an LLM with a pre-trained context window size \(N\), our goal is to unlock the inference capacity of the LLM on the testing data \(_{}\) with an extended context window size \(L\) (where \(L>N\)) by _efficiently_ learning from a small-scale training data \(_{}\) with a maximum sequence length \(N\). We expect the extended model to perform reasonably well in long-context evaluation.

Continuity in Positional Encoding.Transformer-based language models typically encode positional indices sequentially as \(\{0,1,,N-1\}\). Traditional length extension methods (Chen et al., 2023; Peng and Quesnelle, 2023; Peng et al., 2023) directly fine-tune on the target length \(L\) with an updated positional index. This approach preserves the continuity of all absolute positions and learns all position indices within \([0,L-1]\), thereby successfully extending to the target length. Furthermore, PoSE (Zhu et al., 2023) attributed their superior performance over RandPos (Ruoss et al., 2023) to the ensured continuity of segments during fine-tuning.

Relativity in Positional Encoding.Relative positional encoding (RPE) (Shaw et al., 2018) has been proposed as an effective positional encoding method, where only the relative positions between two tokens are considered. Similar to prior works (Ruoss et al., 2023; Zhu et al., 2023; Wu et al., 2024), our work focuses on rotary positional encoding (RoPE) (Su et al., 2024), which is one of the most prominent RPE methods and has been widely applied to LLMs including the recent Llama family (Touvron et al., 2023; Su et al., 2023; Ai et al.Meta, 2024). In RoPE, only the relative distances between position pairs \((|j-i|;0 i<j L-1)\) are learned during fine-tuning (Appendix A). Due to this property, we can manipulate the position indices such that all relative positions between \([0,L-1]\) are learnable within the pre-trained window size.

### Proposed Recipe: Continuity-Relativity indExing with gAussian Middle (crean)

In the following section, we start by introducing our design of dividing the context window \(N\) to learn relative positional information. Then, we propose two strategies that target continuity and relativity, respectively. Lastly, we propose a novel truncated Gaussian sampling method to enhance the middle part of the long context. The overall framework is depicted in Figure 2.

Context division.We first discuss the motivations behind our design of the context length. First, prior works (Han et al., 2023; Xiao et al., 2023) observed that a significant amount of attention score is allocated to the beginning tokens of a sequence, which can potentially encode absolute positional information even without explicit positional encoding (Kazemmejad et al., 2024). Secondly, the starting and ending tokens of long contexts can be treated as two pointers that localize the middle indices with the help of relative encodings. Therefore, we divide the pre-trained context window into three segments. The detailed ablation results are shown in Section 3.6.

Definition 2.1 ().: Given the pre-trained context window size \(N\) and target extended length \(L\), the position set of \(\{Head,Middle,Tail\}\) is defined as follows:

\[&=\{0,1,...,L_{h}-1\},\\ &=\{P_{s},P_{s}+1,...,P_{e}-1,P_{e}\},\\ &=\{L-L_{t},...,L-2,L-1\},\\ s.t.&\ L_{h}+(P_{e}-P_{s})+L_{t}=N, \]

where \(L_{h}\) and \(L_{t}\) denote the length of the head and tail segments, \(P_{s}\) and \(P_{e}\) denote the start and end position index of the middle segment.

Figure 2: **Illustration of CREAM position interpolation. The pre-trained context window is divided into three segments: the head, middle, and tail. To ensure continuity, we fix the lengths of the head and tail to a small value \(k\). To maintain relativity, we set the lengths of the head and tail to \(N/3\). For the middle part, the start and end position indices are determined via truncated Gaussian sampling, thereby encouraging the model to pay more attention to the information in the middle part.**

The relative positions among the three segments in each sample are calculated in pairs, _i.e._, \(\{|j-i|; i,j\{Head,Middle,Tail\}\}\).

The formed relative distance union \(D_{r}\) learned by the model is given by:

\[[0,(L_{h}-1,P_{e}-P_{s},L_{t}-1)][P_{s}-L_{h}+1,P_{e}][L-L_{t}-P_{e},L-1-P_{s}][L-L_{t}-L_{h}+1,L-1]. \]

Given that not all samples possess the same values for \(L_{h}\), \(P_{s}\), \(P_{e}\), and \(L_{t}\), as fine-tuning progresses, the union \(D_{r}\) in Equation (2) can encompass the entire range \([0,L-1]\), facilitating the model to learn all relative positions within the target length \(L\).

Two segmentation strategies.For the sake of **continuity**, we set the \(L_{h}\) and \(L_{t}\) to a very small value \(k\), where \(0<k N\). Specifically, we use \(k=32\) in our experiments. This choice allows the middle segment to closely approximate the pre-trained context window. To maintain **relativity**, we divide \(N\) equally into three parts and fix the \(L_{h}\) and \(L_{t}\) to \(N/3\), enabling the model to learn as many relative positions as possible. In our fine-tuning process, both types of examples are sampled with equal probability to maintain balance.

Truncated Gaussian Middle SamplingTo better focus the training process on the middle part of the long context, we introduce a truncated Gaussian function. This approach reduces the interval overlap in Equation (2) and directs the model's attention toward the middle section of the long context. In Appendix B, we provide theoretical justifications of our truncated Gaussian design, indicating that the maximization of \(|D_{r}|\) holds for middle positions in \([N,L/2)(L/2,L-N]\).

Formally, given the probability density function (PDF) of a Gaussian distribution:

\[f(x)=}(-}{2^{2}} ),\]

where \(\) is the mean and \(\) is the standard deviation. The corresponding cumulative distribution function (CDF) is:

\[F(x)=_{-}^{x}f(t)\,dt=0.5(1+E(} )), E(z)=}_{0}^{z}e^{-t^{2}}\,dt, \]

where \(E()\) is the error function. To calculate the CDF value within the truncated interval, we use a sufficiently large number (_e.g._ 1000) of equally spaced \(x\) values from the given interval \([1,L/N]\):

\[x_{i}=1+, i=1,2,,1000, \]

By substituting Equation (4) into Equation (3), the cumulative distribution function (CDF) curve is derived within the truncated interval. For sampling from this truncated Gaussian distribution, the inverse transform method is employed, as demonstrated in Equation (5):

\[=(x_{i-1}+-x_{i-1})(u-F(x_{i-1}))}{F(x_{i})-F(x _{i-1})}), \]

where \(u(0,1)\), \(()\) represents rounding to the nearest integer. Finally, we can get:

\[ P_{e}&(L_{h}+  L_{m},( N-1)-L_{t}),\\ P_{s}&=P_{e}-L_{m}+1, \]

where \(L_{m}\) denotes the length of the middle segments. In summary, the overall sampling flow of our algorithm is presented in Algorithm 1.

``` Input:\(\{\}\)\(\{\}\)\(\{\}\)\(\{\}\)\(\{\}\)\(\{\}\)\(\{\}\)\(\{\}\)\(\{\}\)\(\{\}\)\(\{\}\)\(\{\}\)\(\{\}\)\(\{\}\)\(\{\}\)\(\{\}\)\(\{\}\)\(\{\}\)\(\{\}\)\(\{}\)\(\{\}\)\(\{}\)\(\{}\)\(\{}\)\(\{}\)\(\{}\)\( conduct long-context LLM evaluation of CREAM-Base on LongChat-Lines (Pal et al., 2023) and Lost-in-the-Middle (Liu et al., 2024). Ideally, fine-tuning should not disrupt what the base model has learned, so we further evaluate CREAM-Base on the language modeling task and the evaluation benchmark (Beeching et al., 2023) adopted by Llama2. Additionally, we assess the CREAM-Chat model with Needle-in-a-Haystack1 and LongBench(Bai et al., 2023). Unless otherwise specified, we use linear interpolation to adapt LLMs to a longer context length.

BaselinesAs far as we know, RandPos (Ruoss et al., 2023) and PoSE (Zhu et al., 2023) are similar to our approach in that they manipulate position indices to enable fine-tuning on the pre-trained length for context expansion. Therefore, these two methods serve as the baselines for our primary comparisons. More details about the experimental setup can be found in the Appendix C.

### Effective Context Window Size Evaluation on CREAM-Base

We evaluate the long-context understanding capabilities of the CREAM-Base model on two tasks: LongChat-Lines2(Pal et al., 2023) (Figure 3) and "Lost in the Middle" (Liu et al., 2024) (Table 1).

CREAM-Base performs best in retrieving information from long contexts of varying lengths.We extend the context window size up to 32K and compare CREAM with the Llama 2-7B (Touvron et al., 2023), RandPos (Ruoss et al., 2023), and PoSE (Zhu et al., 2023). As the context window size increases, the performance of all models drops, but CREAM always performs best except for the window size of 3.6K (see Figure 3). In terms of the average performance over all context window sizes, CREAM outperforms PoSE by 16%, demonstrating its good long-context understanding ability.

CREAM-Base alleviates the Lost-in-the-Middle issue.Lost-in-the-Middle is an observation that LLMs are generally good at retrieving relevant information appearing at the beginning/end of the input context (Liu et al., 2024). To validate the effectiveness of our middle-focused truncated Gaussian

Figure 3: **Results (%) on LongChat-Lines**. Each length consists of 50 samples. All results are fine-tuned on Llama-2-7B with 4K length data through linear position interpolation. Refer to Appendix E for ablated results using NTK (Peng and Quesnelle, 2023) and Yarn (Peng et al., 2023).

sampling, we evaluate CREAM and compare it with PoSE on the key-value retrieval task proposed by Liu et al. (2024). We present results in Table 1, where the cyan shading indicates middle segments. We find that: regardless of the chosen interpolation method, CREAM always outperforms PoSE by a large margin. _e.g._, CREAM-Linear surpasses PoSE-Linear by 21.2% when the relevant information is placed at 18.

### Long Context Understanding Evaluation on CREAM-Chat

We conduct long-context evaluations of CREAM-Chat on two tasks:

* **Needle in A Haystack** (Figure 10)  This task is a test that places an answer (_i.e._, Needle) at any position of a long context window (_i.e._, Haystack) and requires a model to retrieve the correct answer given a question-answer pair. We follow Wu et al. (2024) and use the GPT (GPT-3.5-Turbo-0125) score as the evaluation metric.
* **LongBench** (Table 2) Bai et al. (2023) is a more realistic benchmark because it covers real-world application scenarios like long-context QA and summarization. Moreover, it is specifically designed for Chat models.

CREAM-Chat outperforms SkipAlign in context window expansion.We visualize the results of CREAM-Chat and the recent SkipAlign in Figure 10. Clearly, CREAM-Chat beats SkipAlign because the performance of SkipAlign (Wu et al., 2024) decreases from the window size of 18K while CREAM-Chat displays a perfect performance everywhere until from the window size of 29K. Notably, CREAM-Chat is only fine-tuned for 100 steps.

CREAM-Chat makes best use of the extended context window size.We present results on Long-Bench in Table 2. CREAM-Chat again surpasses strong baseline models, demonstrating its better use of extended context size. In terms of the average performance over all tasks, it outperforms the second

    &  &  \\   & **0** & **18** & **37** & **54** & **74** & **AVG** & **0** & **34** & **69** & **104** & **139** & **AVG** \\  PoSE-Linear & 99.4 & 24.4 & 37.4 & 47.2 & 46.2 & 50.9 & 95.2 & 8.2 & 7.6 & 13.8 & 18.6 & 28.7 \\ CREAM-Linear & 99.6 & 45.6 & 56.0 & 67.0 & 58.0 & **65.2** & 96.6 & 19.8 & 23.4 & 31.0 & 26.2 & **39.4** \\  PoSE-NTK & 98.6 & 49.6 & 44.6 & 40.2 & 41.4 & 54.9 & 97.6 & 3.4 & 0 & 0 & 27.6 & 25.7 \\ CREAM-NTK & 96.2 & 53.8 & 52.6 & 72.8 & 42.0 & **63.5** & 78.6 & 5.2 & 6.0 & 23.4 & 41.8 & **29.9** \\  PoSE-YaRN & 99.6 & 32.6 & 12.2 & 57.2 & 48.4 & 50.0 & 91.8 & 0.6 & 2.8 & 8.2 & 18.8 & 24.4 \\ CREAM-YaRN & 100.0 & 49.6 & 47.6 & 77.4 & 92.6 & **73.4** & 99.4 & 8.0 & 5.8 & 43.8 & 69.2 & **45.2** \\   

Table 1: **Results (%) on “Lost in the Middle”. “Position” indicates the correct answers’ index, and each index comprises 500 samples. All results are fine-tuned on Llama-2-7B with 4K length data.**

Figure 4: **Results on Needle-in-a-Haystack. \({}^{}\) indicates the results excerpted from Wu et al. (2024). Both results are instruction-tuned on LLAa2-7B-Chat with 4K length data. The color gradually changes from deep green to deep red, indicating the Recall performance decreases from 10 to 1.**

best model, _i.e_., LongChat-v1.5-7B-32k (Li et al., 2023), by 1.6%, though it is only tuned on a very small amount of data and for only 100 steps.

### Effectiveness of PEFT Integration

To demonstrate that CREAM can be directly combined with PEFT techniques (such as LoRA (Hu et al., 2022) and QLoRA (Dettmers et al., 2023)), requiring no additional modifications. We conducted experiments on LLaMa-2-7B-Chat using the identical dataset and settings. The experimental results are presented in Table 3. The results indicate that models fine-tuned using LoRA and QLoRA achieve performance nearly equivalent to those fine-tuned with full parameter.

### Language Modeling and Standard Benchmark

Following Chen et al. (2023); Zhu et al. (2023); Peng et al. (2023), we perform the classic language modeling evaluation, _i.e_., perplexity evaluation, on GovReport (Huang et al., 2021) and Proof-pile (Zhangir Azerbayev, 2022). Since a lower perplexity does not necessarily imply better model performance on downstream tasks (Zhang et al., 2024; Hu et al., 2024; Arora et al., 2024; Park et al., 2024), we further conduct evaluation on the standard natural-language-understanding (NLU) benchmark (Beeching et al., 2023). This also lets us know whether fine-tuning hurts the NLU ability of the pre-trained base model.

Both CREAM and PoSE demonstrate the lowest perplexity.We apply different positional interpolation methods to RandPos (Ruoss et al., 2023), PoSE (Zhu et al., 2023), and CREAM and report their perplexities in Table 4. We find that: CREAM and PoSE have a similar perplexity in different settings and both outperform RandPos. This occurs primarily because the position indices used during RandPos fine-tuning are discontinuous, which creates an inconsistency with the pre-training stage.

CREAM has nearly the same NLU abilities as the pre-trained base model.Ideally, fine-tuning should not adversely affect the original capabilities of the pre-trained base model. Our evaluation of CREAM confirms this, _i.e_., CREAM nearly retains all NLU abilities of the base Llama2-7B (see Table 5). Interestingly, CREAM improves over Llama2-7B on ARC-C and HellaSwag. This is because

    & **Single-** & **Multi-** & **Summari-** & **Few-shot** & **Code** & **Synthetic** & **AVG** \\  & **Doc QA** & **Doc QA** & **zation** & **Learning** & **Completion** & **Tasks** & **AVG** \\  Llama2-7B-chat-4k\({}^{*}\) & 24.9 & 22.6 & 24.7 & 60.0 & 48.1 & 5.9 & 31.0 \\ XGen-7B-8k\({}^{*}\) & 24.6 & 20.4 & 24.7 & 56.2 & 38.6 & 5.3 & 28.3 \\ Mistral-7B-Instruct-v0.1 & 29.5 & 20.7 & 26.4 & 13.6 & 29.6 & 10.8 & 21.8 \\ Mistral-7B-Instruct-v0.2 & 28.5 & 21.5 & 26.1 & 50.1 & 33.8 & 13.9 & 29.0 \\ Mistral-7B-Instruct-v0.3 & 33.2 & 30.6 & 26.8 & 56.4 & 15.3 & 10.4 & 28.8 \\ InternLM-7B-8k\({}^{*}\) & 17.4 & 20.2 & 16.1 & 50.3 & 36.4 & 4.5 & 24.2 \\ Vicuna-v1.5-7B-16k\({}^{*}\) & 28.0 & 18.6 & 26.0 & 66.2 & 47.3 & 5.5 & 31.9 \\ LongChat-v1.5-7B-32k\({}^{*}\) & 28.7 & 20.6 & 26.7 & 60.0 & 54.1 & 15.8 & 34.3 \\  CREAM-7B-32k & 34.8 & 31.1 & 27.2 & 65.1 & 50.4 & 7.0 & **35.9** \\   

Table 2: **Results (%) on LongBench. \({}^{*}\) indicates results reported by Bai et al. (2023). CREAM-7B-32k is instruction-tuned for 100 steps using 4K length data on LLaMa2-7B-Chat.**

    & **Single-** & **Multi-** & **Summari-** & **Few-shot** & **Code** & **Synthetic** & **Macro** \\  & **Doc QA** & **Doc QA** & **zation** & **Learning** & **Completion** & **Tasks** & **Macro** \\  Llama2-7B* & 24.9 & 22.6 & 24.7 & 60.0 & 48.1 & 5.9 & 31.0 \\ LoRA & 28.9 & 28.6 & 27.8 & 62.2 & 54.6 & 10.8 & 35.5 \\ QLoRA & 28.1 & 27.6 & 28.1 & 61.7 & 54.6 & 10.1 & 35.0 \\ CREAM-7B-32k & 34.8 & 31.1 & 27.2 & 65.1 & 50.4 & 7.0 & 35.9 \\   

Table 3: **Results (%) on LongBench. \({}^{*}\) indicates results reported by Bai et al. (2023). CREAM-7B-32k is instruction-tuned for 100 steps using 4K length data on LLaMa2-7B-Chat.**these two tasks are few-shot tasks with longer prompts, necessitating the assistance of long-context understanding.

Extending the context length to 256K.We push the limit and extend the context length of Llama-2-7B up to 256K. Following Zhu et al. (2023), we evaluate the extended model by calculating the average perplexity over 20 samples from PG-19 (Rae et al., 2019) and Book3 (Presser, 2020).3 Since the PG-19 test set does have enough samples that are longer than 256K, we select a subset of samples from the PG-19 training set.

We experiment with target context lengths 64K, 96K, 128K, 192K, and 256K and apply different positional interpolation methods to the extended model (see Table 6). The results of PoSE (Zhu et al., 2023) in Table 6 are based on fine-tuning LLaMa 1-7B with 2K data length, and are provided for reference only. Surprisingly, the increase of the target context length brings little to no perplexity increase, demonstrating the stability of CREAM across different target context lengths, even when the target context is extremely long.

### Ablation Study

To validate the effectiveness of our modeling choices, we further conduct an ablation study of three main components of CREAM: truncated Gaussian sampling, fixed start and end segments, and the trade-off between continuity and relativity.

Truncated Gaussian sampling versus Uniform sampling.We use truncated Gaussian sampling to encourage CREAM to make better use of the middle part of the context. As a comparison, we replace

    &  &  \\ 
**Model** & **4K** & **8K** & **16K** & **32K** & **4K** & **8K** & **16K** & **32K** \\  Original & 3.6 & - & - & - & 4.6 & - & - & - \\  RandPos-Linear & 8.9 & 7.4 & 6.2 & 5.8 & 12.1 & 11.9 & 11.9 & 12.9 \\ PoSE-Linear & 3.8 & 3.2 & 2.7 & 2.5 & 4.7 & 4.6 & 4.6 & 4.4 \\ CREAM-Linear & 3.8 & 3.2 & 2.7 & 2.5 & 4.7 & 4.6 & 4.5 & 4.4 \\  RandPos-NTK & 4.6 & 4.0 & 3.6 & 4.0 & 5.8 & 5.8 & 6.2 & 7.3 \\ PoSE-NTK & 3.7 & 3.2 & 2.7 & 2.6 & 4.7 & 4.6 & 4.5 & 4.7 \\ CREAM-NTK & 3.8 & 3.2 & 2.7 & 2.7 & 4.7 & 4.6 & 4.5 & 4.7 \\  RandPos-YaRN & 5.0 & 4.4 & 4.0 & 4.6 & 6.4 & 6.5 & 6.8 & 9.1 \\ PoSE-YaRN & 3.7 & 3.2 & 2.7 & 2.5 & 4.6 & 4.6 & 4.5 & 4.4 \\ CREAM-YaRN & 3.7 & 3.2 & 2.7 & 2.5 & 4.6 & 4.6 & 4.5 & 4.4 \\   

Table 4: **Perplexity results of GovReport and Proof-pile.** Each experiment is the average perplexity of 50 samples, and all results are based on LLaMa2-7B fine-tuned on 4K data length.

    &  &  \\ 
**Model** & **WinoGrande** & **TruthfulQA(mc2)** & **PIQA** & **BoolQ** & **ARC-C** & **HellaSwag** \\  LLaMa-2-7b-hf\({}^{*}\) & 69.2 & 39.5 & 78.8 & 77.4 & 45.9 & 77.2 \\  RandPos-Linear & 63.3 & 39.3 & 76.5 & 66.6 & 32.0 & 48.5 \\ PoSE-Linear & 68.8 & 38.6 & 77.8 & 76.2 & 47.7 & 77.1 \\ CREAM-Linear & 67.5 & 37.4 & 78.5 & 75.4 & 46.8 & 76.9 \\  RandPos-NTK & 68.7 & 35.9 & 78.6 & 74.8 & 45.5 & 74.4 \\ PoSE-NTK & 68.8 & 38.6 & 77.8 & 76.2 & 47.7 & 77.1 \\ CREAM-NTK & 67.5 & 37.4 & 78.5 & 75.4 & 46.8 & 76.9 \\  RandPos-YaRN & 69.3 & 36.6 & 78.3 & 72.5 & 43.4 & 69.2 \\ PoSE-YaRN & 69.4 & 39.6 & 78.1 & 76.7 & 49.0 & 78.0 \\ CREAM-YaRN & 68.7 & 38.5 & 78.0 & 76.4 & 49.0 & 78.0 \\   

Table 5: **Experimental results of standard benchmarks. \({}^{*}\) indicates results cited from Touvron et al. (2023), and all results are based on LLaMa2-7B fine-tuned on 4K data length.**it with the Uniform sampling (see Figure 5(a)). We observe that the Uniform sampling always leads to worse retrieval performance, suggesting the effectiveness of the truncated Gaussian sampling.

Fixing the head and tail segments is crucial for good retrieval performance.We compare our choice of fixing the head and tail segments with three alternatives: (i) removing both the head and tail segment, (ii) fixing only the head segment, and (iii) fixing only the tail segment (see Figure 5(b)). We find that: removing the head and tail segments leads to the worst performance; it results in a complete failure (_i.e._, zero score) for the context size 32K. Keeping either head or tail segments performs slightly better than removing both but underperforms our default choice of fixing both. We suppose that this is because fixing both gives rise to better relativity information, a finding that is consistent with that of Han et al. (2023).

Maintaining a good balance between continuity and relativity is necessary.We encourage continuity by setting the head and tail segment lengths to \(k=32\) and elicit relativity by letting \(k=N/3\) (see Section 2.2). To balance the two desired properties, we randomly choose \(k=32\) and \(k=N/3\) with an equal probability during fine-tuning. Here we compare three scenarios: (1) enforce only continuity, (2) enforce only relativity, and (3) balance continuity and relativity (see Figure 5(c)). We find that balancing continuity and relativity gives rise to the best performance, thus justifying our modeling choice.

Ablation of HyperparametersIn our implementation of truncated Gaussian sampling, as illustrated in Equation (3), the only hyperparameters are the mean \(\) and the variance \(\). The mean \(\) is determined by the expansion factor. The variance \(\) is adaptable based on data, we conducted experiments with five different values of \(\). The results, as presented in Figure 6, indicate that the current selection (\(=3\)) yields optimal performance.

    &  &  \\ 
**Model** & **64K** & **96K** & **128K** & **192K** & **256K** & **64K** & **96K** & **128K** & **192K** & **256K** \\  PoSE-Linear-128K\({}^{*}\) & 22.47 & 26.77 & 31.18 & - & - & 43.62 & 57.08 & 70.87 & - & - \\ PoSE-NTK-128K\({}^{*}\) & 14.84 & 29.48 & 34.80 & - & - & 16.04 & 31.42 & 37.00 & - & - \\ PoSE-YaRN-128K\({}^{*}\) & 10.36 & 10.77 & 11.33 & - & - & 12.30 & 13.07 & 13.81 & - & - \\  CREAM-Linear-192K & 5.9 & 6.0 & 6.1 & 6.1 & - & 7.6 & 7.7 & 7.8 & 7.8 & - \\ CREAM-NTK-192K & 5.0 & 5.1 & 5.2 & 5.2 & - & 6.9 & 7.0 & 7.0 & 7.1 & - \\ CREAM-YaRN-192K & 5.0 & 5.2 & 5.2 & 5.3 & - & 7.0 & 7.1 & 7.1 & - \\  CREAM-Linear-256K & 7.8 & 8.0 & 8.0 & 8.1 & 8.2 & 10.2 & 10.3 & 10.5 & 10.7 & 10.8 \\ CREAM-NTK-256K & 5.1 & 5.3 & 5.3 & 5.4 & 5.4 & 7.2 & 7.3 & 7.3 & 7.3 & 7.4 \\ CREAM-YaRN-256K & 5.2 & 5.3 & 5.4 & 5.4 & 5.5 & 7.1 & 7.2 & 7.2 & 7.3 & 7.3 \\   

Table 6: **Perplexity results of PG-19 and Book3. \({}^{*}\) indicates results copied from Zhu et al. (2023), and CREAM is based on LLaMa2-7B fine-tuned on 4K data length.**

Figure 5: **Ablation study of CREAM on LongChat-Lines. The result at each length is estimated using 50 samples.**

## 4 Related Works

Efficient Transformers and Extra MemoryFoT  addresses the limitations of local attention in transformers by integrating memory attention layers, which enable large models to learn from a wide context while reducing interference. Infini-attention  incorporates compressed memory into the standard attention mechanism and integrates masked local attention and long-term linear attention mechanisms within a single Transformer block. LLoCO  employs LoRA in conjunction with context compression, retrieval, and parameter-efficient fine-tuning to learn context offline. Although these methods can successfully extend the long context window of LLMs, they either require modifications to the attention mechanism or the addition of extra modules for assistance. In contrast, CREAM does not require these operations and can be directly applied to a pre-trained model.

Positional InterpolationChen et al.  first proposed extending the context window through positional interpolation, which linearly reduces the input position indices to match the original context window size, thereby preventing catastrophic high attention scores from completely disrupting the self-attention mechanism. Subsequently, various methods (such as NTK , ABF , and EABF ) emerged that modify the base frequency of rotary positional encoding to achieve positional interpolation. YaRN  introduced a segmented interpolation method, applying different positional interpolations to different dimensions. LongRoPE  identifies and utilizes two forms of non-uniformity in positional interpolation through search, and introduces a progressive expansion strategy for positiona interpolation. Moreover, CREAM can be combined with any positional interpolation method.

Positional EncodingRandPos  first modified position indices so that the model leverages the relativity of positions, enabling it to extend to the target length with fine-tuning over shorter lengths. PoSE  then emphasized the importance of continuous segments, dividing the training length into two parts to further enhance the interpolation effect. CREAM utilizes both relativity and continuity, and it also better enables the model to focus on the middle part of the context.

## 5 Conclusion

We proposed **C**ontinuity-**R**elativity ind**E**xing with **g**A**ussian **M**iddle (CREAM), a simple yet effective method to extend the context of large language models. CREAM achieves a trade-off between continuity and relativity, enabling the model to exploit positional relativity (_i.e._, fine-tuning within the pre-trained length), while preserving text continuity (_i.e._, remaining as close as possible to the pre-trained state). Furthermore, by employing truncated Gaussian sampling, the model can concentrate more on the middle positions during fine-tuning. Experimental results demonstrate that CREAM outperforms other methods on both Base and Chat models and effectively mitigates the issue of "lost in the middle".