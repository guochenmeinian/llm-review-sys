# Fast Graph Sharpness-Aware Minimization for Enhancing and Accelerating Few-Shot Node Classification

Yihong Luo\({}^{1,2}\) & Yuhan Chen\({}^{3}\)

Equal Contribution1The Hong Kong University of Science and Technology 2The Hong Kong University of Science and Technology 3School of Computer Science and Engineering, Sun Yat-sen University 4University of California, Merced 5University of California, Los Angeles 6Createlink Technology 7School of Cyber Science and Technology, Shenzhen Campus of Sun Yat-sen University

Siya Qiu\({}^{1,2}\), Yiwei Wang\({}^{4,5}\), Chen Zhang\({}^{6}\), Yan Zhou\({}^{6}\), Xiaochun Cao\({}^{7}\), Jing Tang\({}^{1,2}\)

\({}^{1}\) The Hong Kong University of Science and Technology 2The Hong Kong University of Science and Technology (Guangzhou) 3School of Computer Science and Engineering, Sun Yat-sen University 4University of California, Merced 5University of California, Los Angeles 6Createlink Technology 7School of Cyber Science and Technology, Shenzhen Campus of Sun Yat-sen University

###### Abstract

Graph Neural Networks (GNNs) have shown superior performance in node classification. However, GNNs perform poorly in the Few-Shot Node Classification (FSNC) task that requires robust generalization to make accurate predictions for unseen classes with limited labels. To tackle the challenge, we propose the integration of Sharpness-Aware Minimization (SAM)--a technique designed to enhance model generalization by finding a flat minimum of the loss landscape--into GNN training. The standard SAM approach, however, consists of two forward-backward steps in each training iteration, doubling the computational cost compared to the base optimizer (e.g., Adam). To mitigate this drawback, we introduce a novel algorithm, Fast Graph Sharpness-Aware Minimization (FGSAM), that integrates the rapid training of Multi-Layer Perceptrons (MLPs) with the superior performance of GNNs. Specifically, we utilize GNNs for parameter perturbation while employing MLPs to minimize the perturbed loss so that we can find a flat minimum with good generalization more efficiently. Moreover, our method reutilizes the gradient from the perturbation phase to incorporate graph topology into the minimization process at almost zero additional cost. To further enhance training efficiency, we develop FGSAM+ that executes exact perturbations periodically. Extensive experiments demonstrate that our proposed algorithm outperforms the standard SAM with lower computational costs in FSNC tasks. In particular, our FGSAM+ as a SAM variant offers a faster optimization than the base optimizer in most cases. In addition to FSNC, our proposed methods also demonstrate competitive performance in the standard node classification task for heterophilic graphs, highlighting the broad applicability. The code is available at [https://github.com/draym28/FGSAM_NeurIPS24](https://github.com/draym28/FGSAM_NeurIPS24)

## 1 Introduction

Graph Neural Networks (GNNs) have received significant interest in recent years due to their powerful ability in various graph learning tasks, e.g., node classification. Numerous GNNs have been developed accordingly . Despite their successes, GNNs, like traditional neural networks, tend to be over-parameterized, often requiring extensive labeled data for training to ensure generalization. However, in real-world networks, many node classes have few labeled instances, which can leadto GNNs overfitting, resulting in poor generalization in these limited labeled classes. Recently, an increasing amount of research is focusing on developing superior GNNs, e.g., Meta-GCN , AMM-GNN , GPN  and TENT , for Few-Shot Node Classification (FSNC) which aims to classify nodes from new classes with limited labelled instances.

Intuitively, training GNNs for FSNC requires robust model generalization ability for recognizing unseen classes from a small number of labelled examples. Motivated by the success of the recently proposed Sharpness-Aware Minimization (SAM) for improving models' generalization in the vision domain , we suggest incorporating SAM into training GNNs for addressing FSNC tasks. The core idea of SAM is to perturb the model parameters to find flat minima of the loss landscape, thereby making the model more generalizable. However, a key drawback of SAM is that it requires executing two forward-backward steps to complete one optimization step, resulting in twice the time consumption compared to general optimizers like Adam. Some works [9; 23; 10] have been proposed to accelerate SAM, but none of them are crafted for graphs, i.e., not leveraging the graph properties for accelerating SAM.

This paper mainly focuses on efficient GNN training in FSNC scenarios by leveraging SAM for improving the generalization of GNNs on unseen classes. To tackle the high training cost issue of SAM, we utilize the connection between GNNs and MLPs--GNNs discarding Message-Passing (MP) are equivalent to MLPs with faster training and worse performance in general--to accelerate training. Specifically, we propose **Fast Graph Sharpness-Aware Minimization (FGSAM)** that uses GNNs for perturbing parameters and employs MLPs (i.e., GNNs discarding MP) to minimize perturbed training loss. This speeds up training at the cost of dropping graph topology information during minimizing the perturbed loss. Interestingly, we find that the gradient computed in parameter perturbation can be reused when minimizing loss to explicitly reintroduce topology information with negligible extra cost. Moreover, we can add back MP during inference to improve performance. To further reduce the computational cost, we propose **FGSAM+** which conducts an exact FGSAM-update at every \(k\) steps. As shown in Fig. 1, empirical results in FSNC tasks show that our proposed FGSAM and FGSAM+ methods outperform both Adam and SAM, and meanwhile FGSAM+ is even faster than Adam. In addition, we evaluate the proposed methods in node classification, showing strong results, especially in heterophilic graphs which are known to be challenging for GNNs [30; 7]. This indicates that our proposed methods can effectively improve the GNN's generalization capability for better performance.

The contributions of this paper can be summarized as follows.

* We study the application of SAM in FSNC tasks.
* We propose FGSAM that improves generalization in an efficient way by leveraging GNNs for sharpness-aware perturbation parameters and employing MLPs to expedite training.
* We further propose an enhanced version named FGSAM+, which conducts the actual FGSAM at every \(k\) steps and approximates it in the intermediate steps.
* We demonstrate strong empirical results of the proposed methods across tasks.

## 2 Preliminary

**Graph Neural Networks.** Let \(=(,)\) denotes an undirected graph, \(=\{v_{i}\}_{i=1}^{n}\) is the node set and \(\) is the edge set. \(^{n n}\) is the adjacency matrix. Let \(=\{_{i}\}_{i=1}^{n}^{n d_{0}}\) be the initial node feature matrix, where \(d_{0}\) is the initial dimension, and \(=\{_{i}\}_{i=1}^{n}^{n C}\) denotes the ground-truth node label matrix, where \(C\) denotes the number of classes and \(_{i}\) is the one-hot encoding of node \(v_{i}\)'s label \(y_{i}\). Let \(^{(L)}\) be the output of the last layer of an \(L\)-layer GCN, the prediction probability matrix \(}=^{(L)}\) is the final output of node classification.

**Few-Shot Node Classification.** In the FSNC task, the entire set of node classes \(\) can be divided into two disjoint subsets: base classes set \(_{}\) and novel classes set \(_{}\), such that \(=_{}_{}\) and \(_{}_{}=\). There are sufficient labeled nodes in \(_{}\), while there are only a limited number of labeled nodes in \(_{}\). FSNC task aims to learn a model using the sufficient labeled nodes from

Figure 1: Comparison of average accuracy and training time across datasets on different GNNs. **The closer to the top left corner, the better.**

\(_{}\), enabling it to accurately predict unlabeled nodes (i.e., query nodes \(\)) in \(_{}\), with limited labeled instances (i.e., support nodes \(\)) from \(_{}\).

**Sharpness-Aware Minimization (SAM).** SAM  is an effective method to improve model's generalization. Let \(_{}=\{(_{i},_{i})\}_{i=1}^{n}\) be the training dataset, following distribution \(\). Given a model parameterized by \(\) and a commonly used loss function (e.g., cross-entropy loss) \(\), instead of directly minimizing training loss \(_{_{}}()=_{i=1}^{n}( _{i},_{i};)\), SAM aims to minimize the population loss \(_{}()=_{(,)} [(,;)]\) by minimizing the vanilla training loss as well as the loss sharpness (i.e., find parameters whose neighbors within the \(_{p}\) ball also have low training loss \(_{_{}}\)) as follows:

\[^{*}&=_{} _{\|\|_{p}}_{_{}}( {w}+)-_{_{}}()+ _{_{}}()+\|\|_{2}^{2}} \\ &=_{}_{\|\|_{p}} _{_{}}(+)+\|\|_{2}^{2} }, \]

where \(\) is the radius of the \(_{p}\) ball, and \(p 0\) (usually \(p=2\)). In this way, the model can converge to flat minima in loss landscape (\(^{*}\)), making the model more generalizable . For efficiency, SAM applies first-order Taylor expansion and classical dual norm problem to obtain the approximation:

\[}=}_{_{}}()}{\|_{}_{_{}}()\|} *{arg\,max}_{\|\|_{p}}_{_{}}(+). \]

Finally, SAM computes the gradient w.r.t. perturbed model \(+}\) for update \(\) in Eq. (1):

\[_{}_{\|\|_{p}}_{_{}}(+)_{}_{_{ }}(+})_{}_{ _{}}()|_{+}}. \]

**Additional Related Works.** The effectiveness of SAMs and its variants have been widely verified in computer vision area [12; 21; 9; 23; 43; 10; 1]. Specifically, LookSAM  speeds up the SAM by periodically conducting exact perturbation, and Sharp-MAML  firstly focusing on meta-learning tasks. However, there is limited work on developing SAM for graphs. WT-AWP  is the first SAM-like work that applied to GNN and gives a theoretical analysis of generalization bound on graphs. Compared to these works, our proposed FGSAM is crafted for graphs by its unique property, enabling the _first SAM-like algorithm that can be faster than the base optimizer_. Our work also shares some similarities with existing works [17; 40] that explore the connection between GNNs and MLPs. However, they attributed the claim that introducing MP to MLP can improve performance during evaluation to the powerful generalization ability of MP. In contrast, we prove that for the linear case with synthetic graphs, whether there is MP or not, both will converge to the same optimal solution, taking a solid step toward understanding the underlying reasons.

## 3 Methodology

In this section, we propose Fast Graph Sharpness-Aware Minimization (FGSAM), an efficient version of SAM for GNNs, aiming to reduce the training time when using SAM in FSNC tasks while improving model's generalization.

### Motivating Analysis

SAMs are a series of new general training scheme used to improve the model's generalization, thus it is intuitive to use SAM in FSNC tasks. However, there is no work studying how to apply SAM to FSNC tasks. So our first question is: **Q1: Can SAM benefit few-shot node classification tasks?**

Figure 2: **(a):** Loss landscape visualization of GNN across tasks and optimizers. **(b):** Loss of GNN, MLP and its PeerMLP on the test set over the training process. In these experiments, MLP and PeerMLP share the same weight space as GNN but are trained without message-passing.

A key property of FSNC is that the GNNs need to be generalized to unseen classes (i.e., novel classes), and the GNNs often converge to a relatively low loss on the training set, but the final performance depends on the GNNs' generalization ability. To demonstrate this intuitively, we plot the GNN's loss landscape of novel classes under the FSNC setting and of the test set under the NC setting (Fig. 2a), following previous work . The loss landscape of GNN under the FSNC setting is sharp and not smooth, with many local minima, in contrast to the flat and smooth loss landscape of GNN under the NC setting. This to some extent indicates that the FSNC setting poses a greater challenge to GNNs, which is consistent with our prior knowledge. Hence, applying SAM-like techniques can intuitively improve the generalization of GNN and enhance its performance.

However, another problem arises: training GNN on FSNC is already slow, and the core drawback of SAM is that it requires twice the training cost compared to Adam or SGD. **Q2: Can we find a way to reduce the SAM training cost based on GNN properties?**

It is well known that the training speed of GNNs is slower than MLPs, mainly due to the notorious MP that causes significant time consumption, yet MP is essential for improving GNN performance. Removing the MP from GNNs \(f_{}(\{,\};)\) turns them into MLPs \(f_{}(;)\), which is an intriguing connection. As shown in Tab. 1 and Fig. 2b, MLPs without the burden of MP demonstrate a substantial training time advantage under the same settings as GNNs and can achieve nearly the same performance as GNNs on the training set, however, they perform significantly worse on the test set, revealing their poor generalization performance.

Inspired by previous work , it is appealing to remove MP during training, but reintroduce it in inference (**PeerMLP**). Although reintroducing MP after training can improve the performance, it still cannot surpass GNNs' (Fig. 2b). This may be because of the lack of graph topology information in training. Hence, we propose minimizing training loss on PeerMLPs but minimizing the sharpness according to GNNs, implicitly incorporating the graph topology information in training. This allows the model to quickly converge to the vicinity of local minima and further converge to flat GNN local minima through a GNN's sharpness-aware approach. By doing so, we not only introduce SAM to enhance the model's generalization ability and the information w.r.t graph topology but also leverage the intriguing connection between MLPs and GNNs to improve training speed.

### Fgsam

We elaborate our proposed method **Fast Graph Sharpness-Aware Minimization (FGSAM)**. For the ease of reference, Fig. 3a visualizes the framework of FGSAM, so does to its enhanced version FGSAM+. There are two forward-backward steps in the FGSAM-update.

**Step 1: Graph sharpness-aware perturbation.** The first forward-backward step is served for computing the maximum perturbation \(}\) (Eq. (2)), where we propose to perturb parameters with MP (GNN), i.e.,

\[}=^{}}{\|^{}\|}= }_{}(;f_{})}{\| _{}_{}(;f_{})\|}=}(f_{}(;),)}{\|_ {}(f_{}(;),)\|} \]

**Step 2: Minimizing perturbed loss.** We propose to minimize the perturbed loss by removing the MP (PeerMLP) to speed up training, i.e.,

\[^{*}=*{arg\,min}_{}_{}(+};f_{}) =*{arg\,min}_{}(f_{}( ;+}),) \] \[=*{arg\,min}_{}(f_{}( {}=\{,\};+}),).\]

It is clear that minimizing the loss on PeerMLPs is equivalent to minimizing the loss on GNNs ignoring the topology information. As demonstrated in Sec. 3.1, intuitively the proposed approach can make model convergence near the local minima easily due to the connection between MLPs and GNNs, and perturbing parameters with MP can find the good flat minima of GNNs (see Fig. 2a).

**Reintroducing Graph Topology in Minimization with Free Lunch.** While reintroducing the MP in evaluation can improve performance, its absence during the minimization process may result in

     } &  &  &  \\  & Backbone & 5N3X & 10N3X & 5N3X & 10N3X & 5N3X & 10N3X \\   & GNN & 9.56 & 9.38 & 17.61 & 17.50 & 41.09 & 40.96 \\  & PeerMLP & 1.11 & 1.17 & 1.35 & 1.54 & 1.02 & 1.17 \\   

Table 1: Time consumption of 200 episodes training (sec.) of baseline w/ and w/o MP (only consider feed-forward and -backward).

sub-optimal results. Incorporating MP directly into the minimization is computationally expensive, leading us to employ MLP to minimize the perturbed loss. Fortuitously, the gradient w.r.t. MP is computed during the perturbation step, offering an opportunity for computational savings. We propose to capitalize on the already available gradient information from the first step by reusing it in the optimization procedure, as formalized in the following optimization target:

\[^{*}=*{arg\,min}_{} _{}(;f_{})+_{ }(+;f_{})},  0. \]

This formulation implies that the computational cost of involving MP in the optimization is mitigated since the forward and backward passes are precomputed in the initial step. Thus, we effectively integrate graph topology into the minimization process almost without incurring additional computational expense, akin to receiving a _free lunch_. See detailed **FGSAM** in Algorithm 1.

**Adaptation to MAML Models.** Model-Agnostic Meta-Learning (MAML)  is widely used in FSNC tasks [8; 38], involving two separate update steps in one MAML-update: i) pre-training for learning task-relevant knowledge, and ii) meta-update for task-irrelevant update. This is different from standard gradient descent. Hence for integrating the FGSAM into the MAML models, we propose treating the MAML-update process as a single entity, and applying the FGSAM-update only once simplifies the implementation. This contrasts with the Sharp-MAML , where the SAM-update is applied separately in the two stages.

### FGSAM+

Although the training time of FGSAM can be largely faster than naive SAM by ignoring the MP in minimizing perturbed loss, it still requires a full forward-backward step of GNN, which makes our approach need an extra computation cost for a forward-backward step of PeerMLP, compared to the base optimizer.

Fortunately, the forward-backward step of GNN is mainly for perturbing parameters in FGSAM, thus we can further reduce the training time while maintaining performance, by employing FGSAM-update at every \(k\) step (i.e., perturb parameters at every \(k\) step) and reusing the preserved gradients from parameters perturbation into the intermediate steps . Eq. (3) can be rewritten as:

\[_{}_{_{}}()|_ {+}_{}_{ _{}}(+)_{ }_{_{}}()+ \|_{}_{_{}}( )\|. \]

In this way, SAM-gradient \(_{s}\) is composed by the vanilla gradient \(_{}_{_{}}()\) and the gradient of the \(_{2}\)-norm of vanilla gradient \(_{}\|_{}_{_{ }}()\|\).

This suggests that SAM-gradient \(_{s}=_{}_{_{}}()|_{+}\) can be divided into two orthogonal parts : \(_{h}\) (in the direction of vanilla gradient \(=_{}_{_{}} ()\) ) is used to minimize the loss value, and flatness-gradient \(_{v}\) is used to adjust the updates towards a flat region. So \(_{h}\) and \(_{v}\) can be easily obtained if \(_{s}\) and \(\) are given:

\[_{h}=\|_{s}\|}{\| \|}=\|_{s}\|_{s} }{\|_{s}\|\|\|}}{\|\|},\ \ _{v}=_{s}-_{h}, \]

where \(\) is the angle between \(_{s}\) and \(_{h}\). As illustrated in , \(_{v}\) changes much slower than \(_{s}\) and \(_{h}\), thus we can compute and preserve \(_{v}\) at every \(k\) steps, and reuse it to approximate \(_{s}\) in intermediate steps.

Figure 3: **Left (a): The solid line indicates that the gradient is computed on the corresponding model, while the dashed line indicates the opposite. Right (b): The difference of gradients (i.e., \(\|_{t+1}-_{t}\|_{2}\)). It can be seen that \(_{v}\) and \(_{}\) change much slower than \(_{s}\) and \(_{h}\) across the training process, thus can be reused in the intermediate steps.**

However, in our case, there exists a clear gap between the model used for perturbing (GNN) and for minimizing (PeerMLP). This is different from the approach in , which uses the same model for both. Thus we use an extra PeerMLP forward-backward step to get another \(^{}\) for computing \(_{v}\) to reduce the gap:

\[^{}=_{}(f_{}(;))=_{}(;f_{}),\ \ \ _{s}=_{} (;f_{})|_{+}}. \]

Note that the \(}\) is obtained by perturbing parameters with MP Eq. (4), \(_{s}\) and \(^{}\) are obtained without MP, thus there still exists a gap.

Moreover, since we reintroduce graph topology (Eq. (6)) in minimization, we propose to further use the extra PeerMLP step to reuse graph topology for better performance. Specifically, we conduct the gradient w.r.t. topology information by projection as follows:

\[_{}=^{}-\|^{}\|(^{ })^{}}{\|^{}\|}, \]

where \(^{}\) is the angle between \(^{}\) and \(^{}\). This can be reused in a similar way as \(_{v}\) when approximating FGSAM-update in the intermediate steps. We further conduct experiments to verify whether the \(_{}\) and \(_{v}\) will change slowly so that they can be reused for speed up in our approach. We plot the change of \(_{s}\), \(_{h}\), \(_{v}\) and \(_{}\) (Fig. 3b) and the results show that the projected gradient both \(_{v}\) and \(_{}\) on parameters perturbed with MP shows a much more stable pattern and slower changes than \(_{s}\) and \(_{h}\), indicating the feasibility of updating \(_{v}\) and \(_{}\) every \(k\) steps and reusing it for the intermediate steps. We present the detailed **FGSAM+** in Algorithm 1 in Appendix B.

Since we need an extra PeerMLP forward-backward step at every \(k\) step, the overall computation cost of our approach, FGSAM+, will be \(\) the computation cost of GNNs plus \((1+)\) the computation cost of MLPs on average.

## 4 Analysis of Toy Case

In this section, we employ the Contextual Stochastic Block Model (CSBM) to analyze why minimizing perturbed training loss without MP can work to some extent, which is the underlying mechanism of FGSAM. The CSBM has been widely used to analyze of the properties of GNN [27; 26].

Specifically, we focus on a CSBM model that contains \(K\) distinct classes \(c_{1},c_{2},,c_{K}\). The nodes within the resulting graphs are grouped into \(n\) non-overlapping sets \(C_{1},C_{2},,C_{K}\), each set representing one of the \(K\) classes. The generation of edges is governed by a probability \(p\) within the same class and a probability \(q\) between different classes. For any given node \(i\), we sample its initial features \(_{i}^{l}\) from a Gaussian distribution denoted by \(_{i}(,)\), where the mean \(=_{k}^{l}\) corresponds to node \(i\) belonging to set \(C_{K}\), and \(k\) is an element of \(\{1,2,,K\}\). Furthermore, the condition \(||_{i}-_{j}||_{2}=D\) holds true for all \(i,j\) belonging to \(\{1,2,,K\}\), with \(D\) being a positive constant. Graphs that arise from this specified CSBM model are referred to as \(K\)-classes CSBM. After applying a MP operation, the resultant features for node \(i\) are denoted by \(_{i}\).

The neighborhood label distribution \(_{i}\) of node \(i\) is a K-dimensions vector, where \(_{i}[j]=(i C_{j})p+(1-(i C_{j}))q\). Based on the neighborhood label distribution, consider the MP operation as \(_{i}=_{j(i)}_{i}\), we have: \(_{i}(_{k}+qK}}{p+( K-1)q},}{deg(i)})\), where \(i C_{k}\) and \(}=^{K}_{j}}{K}\). Based on the distribution of \(_{i}\) and \(_{i}\), we can obtain following theorem:

**Theorem 4.1** (The effectiveness of removing MP in minimization).: _Consider a K-classes CSBM, the optimal linear classifiers for both original features \(_{i}\) and filtered features \(_{i}\) are the same._

Detailed proof is in Appendix C. The theorem tells us that under the linear case, whether the MP layer is used or not, the optimal decision bound is the same. Hence, this encourages us to learn the weight of transformation layers without MP to speed up training. However, the real graph is more complex and we do not use a linear classifier, thus we propose to perform the graph sharpness-aware perturbation which implicitly involves the information of neighbors.

## 5 Experiments

We verify the effectiveness of our proposed FGSAM and FGSAM+ in this section. We first conduct experiments to demonstrate that our proposed algorithms achieve better performance compared to SAM which requires twice the training time. Then we show that our proposed algorithms can achieve faster training speed compared to base optimizers (e.g., Adam). Next, we also conduct extra studies and an extra task to show the robustness and potential applications of our proposed algorithms.

[MISSING_PAGE_FAIL:7]

### Comparison of the Variants of SAM

**Training with Different Optimizer.** We compare the performance of Meta-GNN and GPN training with different variants of SAM, including original SAM, ESAM , LookSAM , AE-SAM , our proposed FGSAM and FGSAM+ (Tab. 3). We observe an anomalous phenomenon where ESAM, as an efficient variant of SAM, actually trains slower than SAM. This is because ESAM sorts the sample losses and selects a suitable subset at each iteration, an operation that is negligible for image tasks; however, for graph tasks, since GNNs are relatively smaller, the proportion of time consumed by the sorting step is significant, leading to an increase in training time. As shown in Tab. 3, our proposed method greatly reduces the training time, based on the relationship between GNN and MLP, while maintaining and even achieving superior performance, compared to other optimizers, indicating ours' high efficiency and effectiveness.

**The Impact of Perturbing Parameters with Message-Passing.** A key point of our work is that we perform parameter perturbation using GNNs, while PeerMLPs (i.e., without message-passing) are used to minimize the perturbed loss. This is significantly distinct from previous SAM methods which shared the same model for both parameter perturbation and loss minimization. So a natural question arises: **to what extent does our approach benefit from performing parameter perturbation using GNNs?** We thus compare our approach to PeerMLPs training with Adam and vanilla SAM. Note that message-passing would be reintroduced during validation and test. From Tab. 3, although the training time of PeerMLPs is shorter than that of GNNs, GNNs outperform their PeerMLPs in most cases. Despite that using PeerMLPs can accelerate the training of GNNs, the topology information is still very important for learning node representations. Thus our proposed FGSAM+ is a better solution, achieving a better trade-off between efficiency and performance.

### Ablation Studies

We further verify the consistent effectiveness of our method compared to Adam across different settings regarding model implementation and graph property. Due to the computational resource restriction, all experiments here were conducted using GPN on the CoraFull with the 5-way 3-shot setting. We provide additional experiments (e.g., the effect of update interval \(k\)) in the Appendix E.

**The Impact of Network Structure.** Here we investigate the effect of hidden dimension and the number of layers on the performance (on the left of Fig. 4). GPN with Adam requires a higher hidden dimension (128) to achieve relatively high accuracy, whereas GPN with FGSAM+ can attain SOTA even with a small hidden dimension (16). With respect to the number of layers, GPN with FGSAM+ consistently performs better within the range of 1\(\)8 compared to GPN with Adam, demonstrating the effectiveness of our proposed method (middle left of Fig. 4).

**The Impact of Noisy Features and Edges.** Here we investigate the effect of randomly adding Gaussian noise to features and randomly adding edges during testing (on the middle right and the right of Fig. 4). Specifically, for noisy features, we randomly add Gaussian noise with varying standard deviations to the node features. Meanwhile, for noisy edges, we uniformly and randomly introduce additional edges into the original structure. The results show that GPN with FGSAM+ method can still achieve relatively high performance, compared to GPN with Adam. These results effectively verify the robustness of our proposed method.

### Additional Task on Conventional Node Classification

Our proposed FGSAM+ also has the potential to be extended to other domains. To demonstrate this, we evaluate the performance of the FGSAM+ on the standard node classification task on both homophilic and heterophilic graphs. For homophilic graphs, we utilize three well-established citation networks: Cora, Citeseer, and Pubmed[32; 13]. For heterophilic graphs, we include page-page

Figure 4: Performance of GPN trained by Adam and FGSAM+ with different settings. **Left:** Results with various hidden channels. **Middle Left:** Results with various model depths. **Middle Right:** Results with features perturbed by noise of varying standard deviations. **Right:** Results with edges subjected to various noise ratios.

networks from Wikipedia, specifically the Chameleon and Squirrel datasets , actor-network, namely Actor, and web pages networks, namely Cornell, Texas and Wisconsin. See Appendix E.1 for statistics of these datasets. We use data splits (48%/32%/20%) provided by , and set \(k=2\) for FGSAM+. We select three representative baselines, namely the classical **GCN**, **GAT** with learnable MP operation, and **GraphSAGE** with complex MP operation, to demonstrate the effectiveness of FGSAM and FGSAM+.

As shown in Tab. 4, both FGSAM and FGSAM+ generally outperform Adam and SAM across base models, indicating the potential wide application of our method. We observed that the proposed method achieves greater improvement on heterophilic graphs compared to homophilic graphs, and heterophilic graphs are generally considered more challenging. This indicates that our method can effectively enhance the generalization capability of GNNs. We also provide additional experiments of integrating FGSAM+ with prompt-based FSNC  in the Appendix E.3.

### Additional Study

We observe that both FGSAM and FGSAM+ generally outperform the standard SAM across tasks (FSNC and standard node classification). This is an interesting finding, as our FGSAM and FGSAM+ algorithm remove message-passing during the minimization of the perturbed loss, which is expected to hurt performance. We attribute these counter-intuitive results to the mitigation of the imbalance adversarial game. The training process of SAM-like algorithms entails an adversarial game similar to that in Generative Adversarial Nets (GANs) . Prior studies  have demonstrated that imbalanced adversarial games in GANs can give rise to worse results. Both FGSAM and FGSAM+ employ distinct models for perturbation and minimization, which can help alleviate the extent of imbalance. These factors may explain the observed performance discrepancies among the compared algorithms. To verify the explanation, we conduct experiments varying the hyper-parameter \(\). Specifically, we graphically illustrate the comparative training loss of SAM and FGSAM+ over a range of \(\) values in Fig. 5, which reveals that while SAM struggles to converge with higher \(\) values, FGSAM+ consistently achieves convergence. Moreover, it is established that a higher \(\) value is conducive to a tighter generalization bound, suggesting that a larger \(\) could potentially enhance performance. Consequently, FGSAM+ is capable of mitigating the imbalanced games issue and tolerating a larger \(\), which contributes to its enhanced performance.

## 6 Conclusion

In this work, we study the application of Sharpness-Aware Minimization (SAM) in FSNC to improve model's generalization, since the key for FSNC is to generalize the model to unseen samples. In order to alleviate the heavy computation cost of SAM, we utilize the connection between MLPs and GNNs and use MLPs to accelerate the training of GNNs. However, the low generalization and lack of using graph topology of MLPs also limit its performance. Hence we propose to apply GNNs to perturb parameters for generalization and use MLPs to minimize the perturbed training loss for conducting the proposed FGSAM. Moreover, we reuse the GNN gradient in perturbation in minimization for better including topology information. We further reduce the training time by conducting exact FGSAM update at every \(k\) steps and approximate FGSAM's gradient with reusing information in the intermediate steps. Finally, the extensive experiments demonstrate the effectiveness and efficiency of our proposed methods.

   &  &  &  &  &  &  &  &  &  &  &  \\   & Adam & 88.36 & 77.25 & 88.71 & 65.04 & 52.49 & 28.54 & 61.08 & 60.27 & 55.29 & 64.11 \\  & SAM & **88.42** & 77.30 & 88.79 & 65.52 & 52.51 & 28.59 & 61.89 & 62.70 & 54.51 & 64.48 \\  & **FGSAM+ours** & 88.36 & **77.60** & **89.36** & **66.16** & **53.95** & **29.88** & 67.30 & **63.24** & **58.69** & **65.73** \\  & **FGSAM+ours** & 88.32 & 72.52 & 89.13 & 64.56 & 51.14 & 29.66 & **68.11** & 61.62 & 54.71 & **64.97** \\   & Adam & 87.67 & 76.09 & 89.15 & 50.33 & 37.61 & 33.74 & 78.11 & 78.38 & 84.51 & 68.40 \\  & SAM & 87.69 & 76.44 & 89.25 & 50.92 & 37.44 & 33.83 & 78.92 & 80.27 & 84.31 & 68.79 \\  & **FGSAM+ours** & **88.36** & 72.13 & **89.75** & **81.34** & **39.12** & **34.53** & **82.43** & **81.35** & **86.47** & **70.95** \\   & Adam & 88.32 & 76.37 & 87.48 & 46.51 & 31.46 & 29.45 & 59.19 & 62.16 & 55.49 & 59.60 \\  & SAM & 88.49 & 76.78 & 87.24 & 46.82 & 31.61 & 29.49 & 59.46 & 62.16 & 55.29 & 59.70 \\  & **FGSAM+ours** & 88.40 & 76.98 & 87.63 & 47.82 & 32.35 & 30.41 & 61.89 & **65.95** & **59.91** & **61.23** \\  & **FGSAM+ours** & **88.70** & **77.10** & **87.74** & **48.07** & **32.69** & **30.60** & **62.16** & 64.86 & 58.04 & 61.11 \\  

Table 4: Results on nine real-world node classification benchmark datasets: Mean accuracy (%).

Figure 5: Training loss curves related to different \(\) across optimizers.