# Robust covariance estimation with missing values and cell-wise contamination

Karim Lounici

CMAP

Ecole Polytechnique

Palaiseau, France

karim.lounici@polytechnique.edu

&Gregoire Pacreau

CMAP

Ecole Polytechnique

Palaiseau, France

gregoire.pacreau@polytechnique.edu

###### Abstract

Large datasets are often affected by cell-wise outliers in the form of missing or erroneous data. However, discarding any samples containing outliers may result in a dataset that is too small to accurately estimate the covariance matrix. Moreover, the robust procedures designed to address this problem require the invertibility of the covariance operator and thus are not effective on high-dimensional data. In this paper, we propose an unbiased estimator for the covariance in the presence of missing values that does not require any imputation step and still achieves near minimax statistical accuracy with the operator norm. We also advocate for its use in combination with cell-wise outlier detection methods to tackle cell-wise contamination in a high-dimensional and low-rank setting, where state-of-the-art methods may suffer from numerical instability and long computation times. To complement our theoretical findings, we conducted an experimental study which demonstrates the superiority of our approach over the state of the art both in low and high dimension settings.

## 1 Introduction

Outliers are a common occurrence in datasets, and they can significantly affect the accuracy of data analysis. While research on outlier detection and treatment has been ongoing since the 1960s, much of it has focused on cases where entire samples are outliers (Huber's contamination model) . While sample-wise contamination is a common issue in many datasets, modern data analysis often involves combining data from multiple sources. For example, data may be collected from an array of sensors, each with an independent probability of failure, or financial data may come from multiple companies, where reporting errors from one source do not necessarily impact the validity of the information from the other sources. Discarding an entire sample as an outlier when only a few features are contaminated can result in the loss of valuable information, especially in high-dimensional datasets where samples are already scarce. It is important to identify and address the specific contaminated features, rather than simply treating the entire sample as an outlier. In fact, if each dimension of a sample has a contamination probability of \(\), then the probability of that sample containing at least one outlier is given by \(1-(1-)^{p}\), where \(p\) is the dimensionality of the sample. In high dimension, this probability can quickly exceed \(50\%\), surpassing the breakdown point of many robust estimators designed for the Huber sample-wise contamination setting. Hence, it is crucial to develop robust methods that can handle cell-wise contaminations and still provide accurate results.

The issue of cell-wise contamination, where individual cells in a dataset may be contaminated, was first introduced in . However, the issue of missing data due to outliers was studied much earlier, dating back to the work of . Although missing values in a dataset are much easier to detect than outliers, they can lead to errors in estimating the location and scale of the underlying distribution and can negatively affect the performance of supervised learning algorithms . This motivated the development of the field of data imputation. Several robust estimation methods have been proposed to handle missing data, including Expectation Maximization (EM)-based algorithms , maximum likelihood estimation  and Multiple Imputation , among which we can find k-nearest neighbor imputation  and iterative imputation . Recently, sophisticated solutions based on deep learning, GANs , VAE  or Diffusion schemes  have been proposed to perform complex tasks like artificial data generation or image inpainting. The aforementioned references focus solely on minimising the entrywise error for imputed entries. Noticeably, our practical findings reveal that applying state-of-the-art imputation methods to complete the dataset, followed by covariance estimation on the completed dataset, does not yield satisfactory results when evaluating the covariance estimation error using the operator norm.

In comparison to data missingness or its sample-wise counterpart, the cell-wise contamination problem is less studied. The Detection Imputation (DI) algorithm of  is an EM type procedure combining a robust covariance estimation method with an outlier detection method to iteratively update the covariance estimation. Other methods include adapting methodology created for Huber contamination for the cell-wise problem, such as in  or . In high dimensional statistics, however, most of these methods fail due to high computation time and numerical instability. Or they are simply not designed to work in this regime since they are based on the Mahalanobis distance, which requires an inversion of the estimated covariance matrix. This is a major issue since classical covariance matrix estimators have many eigenvalues close to zero or even exactly equal to zero in high-dimension. To the best of our knowledge, no theoretical result exists concerning the statistical accuracy of these methods in the cell-wise contamination setting contrarily to the extensive literature on Huber's contamination .

**Contributions.** In this paper we address the problem of high-dimensional covariance estimation in the presence of missing observations and cell-wise contamination. To formalize this problem, we adopt and generalize the setting introduced in . We propose and investigate two different strategies, the first based on filtering outliers and debiasing and the second based on filtering outliers followed by imputation and standard covariance estimation. We propose novel computationally efficient and numerically stable procedures that avoid matrix inversion, making them well-suited for high-dimensional data. We derive non-asymptotic estimation bounds of the covariance with the operator norm and minimax lower bounds, which clarify the impact of the missing value rate and outlier contamination rate. Our theoretical results also improve over  in the MCAR and no contamination. Next, we conduct an experimental study on synthetic data, comparing our proposed methods to the state-of-the-art (SOTA) methods. Our results demonstrate that SOTA methods fail in the high-dimensional regime due to matrix inversions, while our proposed methods perform well in this regime, highlighting their effectiveness. Then we demonstrate the practical utility of our approach by applying it to real-life datasets, which highlights that the use of existing estimation methods significantly alters the spectral properties of the estimated covariance matrices. This implies

Figure 1: Left: Estimation error of the covariance matrix for \(n=100\), \(p=50\), \(()=2\) under a Dirac contamination (tailMV and DDCMV are our methods). Here \(=1\) and \(\) varies in \((0,1)\). Right: For each method, mean computation time (in seconds) over 20 repetitions and whether it uses matrix inversion. For \(p=100\), we had to raise \(r()\) to \(10\) otherwise both DI and TSGS would fail due to numerical instability.

that cell-wise contamination can significantly impact the results of dimension reduction techniques like PCA by completely altering the computed principal directions. Our experiments demonstrate that our methods are more robust to cell-wise contamination than SOTA methods and produce reliable estimates of the covariance.

## 2 Missing values and cell-wise contamination setting

Let \(X_{1},,X_{n}\) be \(n\) i.i.d. copies of a zero mean random vector \(X\) admitting unknown covariance operator \(=[X X]\), where \(\) is the outer product. Denote by \(X_{i}^{(j)}\) the \(j\)th component of vector \(X_{i}\) for any \(j[p]\). All our results are non-asymptotic and cover a wide range of configurations for \(n\) and \(p\) including the high-dimensional setting \(p n\). In this paper, we consider the following two realistic scenarios where the measurements are potentially corrupted.

Missing values.We assume that each component \(X_{i}^{(j)}\) is observed independently from the others with probability \((0,1]\). Formally, we observe the random vector \(Y^{p}\) defined as follows:

\[Y_{i}^{(j)}=d_{i,j}X_{i}^{(j)},1 i n,1 j p \]

where \(d_{ij}\) are independent realisations of a bernoulli random variable of parameter \(\). This corresponds the Missing Completely at Random (MCAR) setting of . Our theory also covers the more general Missing at Random (MAR) setting in Theorem 

Cell-wise contamination.Here we assume that some missing components \(X_{i}^{(j)}\) can be replaced with probability \(\) by some independent noise variables, representing either a poisoning of the data or random mistakes in measurements. The observation vector \(Y\) then satisfies:

\[Y_{i}^{(j)}=d_{i,j}X_{i}^{(j)}+(1-d_{i,j})e_{i,j}_{i}^{(j)},1 i n,1  j p \]

where \(_{i}^{(j)}\) are independent erroneous measurements and \(e_{i,j}\) are i.i.d. bernoulli random variables with parameter \(\). We also assume that all the variables \(X_{i}\), \(_{i}^{(j)}\), \(d_{i,j}\), \(e_{i,j}\) are mutually independent. In this scenario, a component \(X_{i}^{(j)}\) is either perfectly observed with probability \(\), replaced by a random noise with probability \(^{}=(1-)\) or missing with probability \((1-)(1-)\). Cell-wise contamination as introduced in  corresponds to the case where \(=1\), and thus \(^{}=1-\).

In both of these settings, the task of estimating the mean of the random vectors \(X_{i}\) is well-understood, as it reduces to the classical Huber setting for component-wise mean estimation. One could for instance apply the Tuker median on each component separately . However, the problem becomes more complex when we consider non-linear functions of the data, such as the covariance operator. Robust covariance estimators originally designed for the Huber setting may not be suitable when applied in the presence of missing values or cell-wise contaminations.

We study a simple estimator based on a correction of the classical covariance estimator on \(Y_{1},,Y_{n}\) as introduced in  for the missing values scenario. The procedure is based on the following observation, linking \(^{Y}\) the covariance of the data with missing values and \(\) the true covariance:

\[=(^{-1}-^{-2})(^{Y})+^{-2 }^{Y} \]

Note that this formula assumes the knowledge of \(\). In the missing values scenario, \(\) can be efficiently estimated by a simple count of the values exactly set to \(0\) or equal to NaN (not a number). In the contamination setting , the operator \(^{Y}=(Y Y)\) satisfies, for \(=[]\):

\[^{Y}=^{2}+(-^{2})()+ (1-).\]

In this setting, as one does not know the exact location and number of outliers we propose to estimate \(\) by the proportion of data remaining after the application of a filtering procedure.

Notations.We denote by \(\) the Hadamard (or term by term) product of two matrices and by \(\) the outer product of vectors, i.e. \( x,y^{d},x y=xy^{}\). We denote by \(\|.\|\) and \(\|.\|_{F}\) the operator and Frobenius norms of a matrix respectively. We denote by \(\|.\|_{2}\) the vector \(l_{2}\)-norm.

## 3 Estimation of covariance matrices with missing values

We consider the scenario outlined in  where the matrix \(\) is of approximately low rank. To quantify this, we use the concept of effective rank, which provides a useful measure of the inherent complexity of a matrix. Specifically, the effective rank of \(\) is defined as follows

\[():=\|X\|_{2}^{2}}{\|\| }=()}{\|\|} \]

We note that \(0()()\). Furthermore, for approximately low rank matrices with rapidly decaying eigenvalues, we have \(()()\). This section presents a novel analysis of the estimator defined in equation , which yields a non-asymptotic minimax optimal estimation bound in the operator norm. Our findings represent a substantial enhancement over the suboptimal guarantees reported in . Similar results could be established for the Frobenius norm using more straightforward arguments, as those in  or . We give priority to the operator norm since it aligns naturally with learning tasks such as PCA. See  and the references cited therein.

We need the notion of Orlicz norms. For any \( 1\), the \(_{}\)-norms of a real-valued random variable \(V\) are defined as: \(\|V\|_{_{}}=\{u>0,(|V|^{}/u ^{}) 2\}\). A random vector \(X^{p}\) is sub-Gaussian if and only if \( x^{p}\), \(\| X,x\|_{_{2}}\| X,x\|_{L^{2}}\).

Minimax lower-bound.We now provide a minimax lower bound for the covariance estimation with missing values problem. Let \(_{p}\) the set of \(p p\) symmetric semi-positive matrices. Then, define \(_{}=\{S_{p}:(S)\}\) the set of matrices of \(_{p}\) with effective rank at most \(\).

**Theorem 1**.: _Let \(p,n,\) be strictly positive integers such that \(p\{n,2\}\). Let \(X_{1},,X_{n}\) be i.i.d. random vectors in \(^{p}\) with covariance matrix \(_{}\). Let \((d_{i,j})_{1 i n,1 j p}\) be an i.i.d. sequence of Bernoulli random variables with probability of success \((0,1]\), independent from the \(X_{1},,X_{n}\). We observe \(n\) i.i.d. vectors \(Y_{1},,Y_{n}^{p}\) such that \(Y_{i}^{(j)}=d_{i,j}X_{i}^{(j)}\), \(i[n]\), \(j[p]\). Then there exists two absolute constants \(C>0\) and \((0,1)\) such that:_

\[_{}_{_{}} _{}(\|-\| C()}{n}}) \]

_where \(_{}\) represents the infimum over all estimators \(\) of matrix \(\) based on \(Y_{1},,Y_{n}\)._

Sketch of proof.: We first build a sufficiently large test set of hard-to-learn covariance operators exploiting entropy properties of the Grassmann manifold such that the distance between any two distinct covariance operator is at least of the order \(()}{n}}\). Next, in order to control the Kullback-Leibler divergence of the observations with missing values, we exploit in particular interlacing properties of the eigenvalues of the perturbed covariance operators . 

This lower bound result improves upon  Theorem 2] as it relaxes the hypotheses on \(n\) and \(\). More specifically, the lower bound in  requires \(n 2^{2}/^{2}\) while we only need the mild assumption \(p\{n,2\}\). Our proof leverages the properties of the Grassmann manifold, which has been previously utilized in different settings such as sparse PCA without missing values or contamination  and low-rank covariance estimation without missing values or contamination . However, tackling missing values in the Grassmann approach adds a technical challenge to these proofs as they modify the distribution of observations. Our proof requires several additional nontrivial arguments to control the distribution divergences, which is a crucial step in deriving the minimax lower bound.

Non-asymptotic upper-bound in the operator norm.We provide an upper bound of the estimation error in operator norm. We write \(Y_{i}=d_{i} X_{i}\). Let \(^{Y}=n^{-1}_{i=1}^{n}Y_{i} Y_{i}\) be the classical covariance estimator of the covariance of \(Y\). When the dataset contains missing values and corruptions, \(^{Y}\) is a biased estimator of \(\). Exploiting Equation ,  proposed the following unbiased estimator of the covariance matrix \(\):

\[=^{-2}^{Y}+(^{-1}-^{-2}) (^{Y}). \]

The following result is from  Theorem 4.2].

**Lemma 1**.: _Let \(X_{1},,X_{n}\) be i.i.d. sub-Gaussian random variables in \(^{p}\), with covariance matrix \(\), and let \(d_{ij},i[1,n],j[1,p]\) be i.i.d bernoulli random variables with probability of success \(>0\). Then there exists an absolute constant \(C\) such that, for \(t>0\), with probability at least \(1-e^{-t}\):_

\[\|-\| C\|\|(()()}{^{2}n}}n}}()(t+())}{^{2}n}(n)) \]

This result uses a recent unbounded version of the non-commutative Bernstein inequality, thus yielding some improvement upon the previous best known bound of . Theorem and Lemma provide some important insights on the minimax rate of estimation in the missing values setting. In the high-dimensional regime \(p\{n,2\}\) and \(n^{-2}()(())^{2}n\), we observe that the two bounds coincide up to a logarithmic factor in \(()\), hence clarifying the impact of missing data on the estimation rate via the parameter \(\).

Heterogeneous missingness.We can extend the correction to the more general case where each feature has a different missing value rate known as the Missing at Random (MAR) setting in . We denote by \(_{j}(0,1]\) the probability to observe feature \(X^{(j)}\), \(1 j p\) and we set \(:=(_{j})_{j[p]}\). As in the MCAR setting, the probabilities \((_{j})_{j[p]}\) can be readily estimated by tallying the number of missing entries for each feature. Hence they will be assumed to be known for the sake of brevity. Let \(_{}=(_{j}^{-1})_{j[p]}\) be the vector containing the inverse of the observing probabilities and \(_{}=_{}_{}\). In this case, the corrected estimator becomes :

\[=_{}^{Y}+((_{})-_{}) (^{Y}) \]

Let \(=_{j}\{_{j}\}\) and \(=_{j}\{_{j}\}\) be the largest and smallest probabilities to observe a feature.

**Theorem 2**.: _(i) Let \(X_{1},,X_{n}\) be i.i.d. sub-Gaussian random variables in \(^{p}\), with covariance matrix \(\). We consider the MAR setting described above. Then the estimator  satisfies, for any \(t>0\), with probability at least \(1-e^{-t}\)_

\[\|-\| C\|\|}{^{2}}(()()}{ n}}}()(t+())}{n} n) \]

_(ii) Let \(p,n,\) be strictly positive integers such that \(p\{n,2\}\). Let \(X_{1},,X_{n}\) be i.i.d. random vectors in \(^{p}\) with covariance matrix \(_{}\). Then,_

\[_{}_{_{}}_{ }(\|-\| C}()}{n}}). \]

If \(\) then the rates for the MCAR and MAR settings match. The proof is a straightforward adaptation of the proof in the MCAR setting.

## 4 Optimal estimation of covariance matrices with cell-wise contamination

In this section, we consider the cell-wise contamination setting .We derive both an upper bound on the operator norm error of the estimator  and a minimax lower bound for this specific setting. Let us assume that the \(_{1},_{n}\) are sub-Gaussian r.v. Note also that \(:=[_{1}_{1}]\) is diagonal in the cell-wise contamination setting .

Minimax lower-bound.The lower bound for missing values still applies to the contaminated case as missing values are a particular case of cell-wise contamination. But we want a more general lower bound that also covers the case of adversarial contaminations.

**Theorem 3**.: _Let \(p,n,\) be strictly positive integers such that \(p\{n,2\}\). Let \(X_{1},,X_{n}\) be i.i.d. random vectors in \(^{p}\) with covariance matrix \(_{}\). Let \((_{i,j})_{1 i n,1 j p}\) be i.i.d. sequence of bernoulli random variables of probability of success \((0,1]\), independent to the \(X_{1},,X_{n}\). We observe \(n\) i.i.d. vectors \(Y_{1},,Y_{n}^{p}\) satisfying  where \(_{i}\) are i.i.d. of arbitrary distribution \(Q\). Then there exists two absolute constants \(C>0\) and \((0,1)\) such that:_

\[_{}_{_{}}_{Q} _{,Q}(\|-\| C()}{n}}) \]_where \(_{}\) represents the infimum over all estimators of matrix \(\) and \(_{Q}\) is the maximum over all contamination \(Q\)._

The proof of this theorem adapts an argument developed to derive minimax lower bounds in the Huber contamination setting. See App. C.3 for the full proof.

Non-asymptotic upper-bound in the operator norm.Note that the term \((1-)\) in the cell-wise contamination setting is negligible when \( 1\) or \( 0\). Using the DDC detection procedure of , we can detect the contaminations and make \(\) smaller without decreasing \(\) too much. For simplicity, we assume from now on that the \(_{i}^{(j)}\) are i.i.d. with common variance \(_{}^{2}\). Hence \(=_{}^{2}I_{p}\). We further assume that the \(_{i}^{(j)}\) are sub-Gaussian since we observed in our experiments that filtering removed all the large-valued contaminated cells and only a few inconspicuous contaminated cells remained. Our procedure  satisfies the following result.

**Theorem 4**.: _Let the assumptions of Theorem [] be satisfied. We assume in addition that the observations \(Y_{1},,Y_{n}\) satisfy \(}\) with \([0,1)\) and \((0,1]\) and i.i.d. sub-Gaussian \(_{i}^{(j)}\)'s. Then, for any \(t>0\), with probability at least \(1-e^{-t}\):_

\[\|-\| \|\|(() ()}{^{2}n}}n}} {()(t+())}{^{2}n}(n) )+^{2}}{}\] \[+}_{}^{2}(} })\] \[+D(,p)}+^{2}\,p}()}(n) {t+(p)}{n},\]

_where \(D(,p)=}(p-2)_{}^{ 2}[2\|\|+_{}^{2}]+}_{}^{4}(|()-(p-2)| +\|\|)}\)._

See App F.3 for the proof. As emphasized in , the effective rank \(()\) provides a measure of the statistical complexity of the covariance learning problem in the absence of any contamination. However, when cell-wise contamination is present, the statistical complexity of the problem may increase from \(()\) to \(()=p\). Fortunately, if the filtering process reduces the proportion of cell-wise contamination \(\) such that \((1-)\,()()\) and \(\|\|\). Then we can effectively mitigate the impact of cell-wise contamination. Indeed, we deduce from Theorem [] that

\[\|-\| \|\|(() ()}{^{2}n}}n}} {()(t+())}{^{2}n}(n) )+^{2}}{}\] \[+()} {}^{2}}+ ()}(n)}, \]

where we considered for convenience the reasonable scenario where \((p-2)()\) and \(_{}^{2}\|\|\). The combination of the upper bound ( with the lower bound in Theorem 3 provides the first insights into the impact of cell-wise contamination on covariance estimation.

## 5 Experiments

In our experiments, MV refers either to the debiased MCAR covariance estimator  or to its MAR extension . The synthetic data generation is described in App. A. We also performed experiments on real life datasets described in App. B. All experiments were conducted on a 2020 MacBook Air with a M1 processor (8 cores, 3.4 GHz). 

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

### The effect of cell-wise contamination on real-life datasets

We tested the methods on \(8\) datasets from sklearn and Woolridge's book on econometrics . These are low dimensional datasets (less than \(20\) features) representing various medical, social and economic phenomena. We also included \(2\) high-dimensional datasets. See App.  for the list of the datasets.

One interesting observation is that the instability of Mahalanobis distance-based algorithms is not limited to high-dimensional datasets. Even datasets with a relatively small number of features can exhibit instability. This can be seen in the performance of DI on the Attend dataset, as depicted in Figure 8 where it fails to provide accurate results. Similarly, both TSGS and DI fail to perform well on the CEOSAL2 dataset, as shown in Figure  despite both datasets having fewer than \(15\) features.

On the Abalone dataset, once we have removed 4 obvious outliers (which are detected by both DDC and the tail procedure), all estimators reached a consensus with the non-robust classical estimator, meaning that this dataset provides a ground truth against which we can evaluate and compare the performance of robust procedures in our study. To this end, we artificially contaminate \(5\%\) of the cells at random in the dataset with a Dirac contamination and compare the spectral error of the different robust estimators. As expected, TSGS and all our new procedures succeed at correcting the error, however DI becomes unstable (see Table 3). DDC MIWAE is close to SOTA TSGS for cellwise contamination and DDC II performs better. We also performed experiments on two high-dimensional datasets, where our methods return stable estimates of the covariance (DDCMV99 and DDCMV95 are within \( 3\%\) of each other) and farther away from the classical estimator (See Figures  and ). Note also that DDCII's computation time explodes and even returns out-of-memory errors due to the high computation cost of II that we already highlighted in Table 

Figure 4: Estimation error as a function of the contamination rate for \(n=500\), \(p=400\), \(()=5\) and Dirac contamination.

Figure 5: Estimation error as a function of the contamination rate for \(n=500\), \(p=400\), \(()=5\) and Gaussian contamination.

Figure 6: DI fails on ATTEND since the covariance matrix is approximately low rank. The dataset has only \(8\) features and the effective rank of its covariance matrix is below \(2\).

Figure 7: Woolridge’s CEOSAL dataset fails both TSGS and DI with its dimension of \(13\) and effective rank of around \(2.5\).

## 6 Conclusion and future work

In this paper, we have extended theoretical guarantees on the spectral error of our covariance estimators robust to missing data to the missing at random setting. We have also derived the first theoretical guarantees in the cell-wise contamination setting. We highlighted in our numerical experimental study that in the missing value setting, our debiased estimator designed to tackle missing values without imputation offers statistical accuracy similar to the SOTA IterativeImputer for a dramatic computational gain. We also found that SOTA algorithms in the cell-wise contamination setting often fail in the standard setting \(p<n\) for dataset with fast decreasing eigenvalues (resulting in approximately low rank covariance), a setting which is commonly encountered in many real life applications. This is due to the fact that these methods use matrix inversion which is unstable to small eigenvalues in the covariance structure and can even fail to return any estimate. In contrast, we showed that our strategy combining filtering with estimation procedures designed to tackle missing values produce far more stable and reliable results. In future work, we plan to improve our theoretical upper and lower bounds in the cell-wise contamination setting to fully clarify the impact of this type of contamination in covariance estimation.

Acknowledgements.This paper is based upon work partially supported by the Chaire _Business Analytic for Future Banking_ and EU Project ELIAS under grant agreement No. 101120237.