# Curvature Filtrations for Graph Generative Model Evaluation

Joshua Southern

Imperial College London

jks17@ic.ac.uk

&Jeremy Wayland

Helmholtz Munich & Technical University of Munich

jeremy.wayland@tum.de

&Michael Bronstein

University of Oxford

michael.bronstein@cs.ox.ac.uk

These authors contributed equally.

&Bastian Rieck

Helmholtz Munich & Technical University of Munich

bastian.rieck@tum.de

These authors jointly directed the work.

###### Abstract

Graph generative model evaluation necessitates understanding differences between graphs on the distributional level. This entails being able to harness salient attributes of graphs in an efficient manner. Curvature constitutes one such property that has recently proved its utility in characterising graphs. Its expressive properties, stability, and practical utility in model evaluation remain largely unexplored, however. We combine graph curvature descriptors with emerging methods from topological data analysis to obtain robust, expressive descriptors for evaluating graph generative models.

## 1 Introduction

Graph-structured data are prevalent in a number of different domains, including social networks , bioinformatics , and transportation . The ability to generate new graphs from a distribution is a burgeoning technology with promising applications in molecule or protein design , as well as circuit design . To compare graph generative models and advance research in this area, it is essential to have a metric that can measure the distance between a set of generated and reference graphs, with enough _expressivity_ to critically evaluate model performance. Typically, this is done by using a set of descriptor functions, which map a graph to a high-dimensional representation in \(^{d}\). An evaluator function, such as the _maximum mean discrepancy_[27, MMD], may then be used to get a distance between two distributions of graphs by comparing their vectorial representations . This state of the art was recently critiqued by O'Bray et al.  since it (i) requires numerous parameter and function choices, (ii) is limited by the _expressivity_ of the descriptor function, and (iii) does not come equipped with _stability guarantees_.

We propose to overcome these issues through topological data analysis (TDA), which is capable of capturing multi-scale features of graphs while being more expressive than simple descriptor functions. TDA is built on the existence of a function of the form \(f V\), or \(f E\) on a graph \(G=(V,E)\). This is used to obtain a _filtration_, i.e. an ordering of subgraphs, resulting in a set of topological descriptors, the _persistence diagrams_. Motivated by their expressive power and computational efficiency, we use recent notions of discrete curvature  to define filtrations and calculate _persistence landscapes_ from the persistence diagrams, thus obtaining a descriptor whose Banach space formulation permits statistical calculations. Our proposed method comes equipped with stability guarantees, can count certain substructures and measure important graph characteristics, and can be used to evaluate a variety of statistical tests since it permits computing distances between graph distributions.

Our **contributions** are as follows:

* We provide a thorough theoretical analysis of the stability and expressivity of recent notions of discrete curvature, showing their fundamental utility for graph learning applications.
* Using discrete curvature and TDA, we develop a new metric for graph generative model evaluation.
* Our experiments reveal our metric is robust and expressive, thus improving upon current approaches that use simple graph descriptor and evaluator functions.

## 2 Background

The topological descriptors used in this paper are based on _computational topology_ and _discrete curvature_. We give an overview of these areas and briefly comment on previous work that makes use of _graph statistics_ in combination with MMD. In the following, we consider undirected graphs, denoted by \(G=(V,E)\), with a set of vertices \(V\) and a set of edges \(E V V\).

### MMD and Metrics Based On Graph Statistics

MMD is a powerful method for comparing different distributions of structured objects. It employs _kernel functions_, i.e. positive definite similarity functions, and can thus be directly combined with standard graph kernels  for graph distribution comparison. However, there are subtle issues when calculating kernels in \(^{d}\): Gaussian kernels on geodesic metric spaces, for instance, have limited applicability when spaces have non-zero curvature , and certain kernels in the literature are indefinite, thus violating one of the tenets of MMD . Despite these shortcomings, MMD is commonly used to evaluate graph generative models [17; 71]. This is accomplished by extracting a feature vector from each graph, such as the clustering coefficient or node degree, and subsequently calculating empirical MMD values between generated and reference samples. Some works  combine multiple structural properties of graphs into a single metric through the Kolmorogov-Smirnov (KS) multidimensional distance . Combining graph statistics into a single measure has also led to metrics between molecular graphs, such as the quantitative estimate of drug-likeness (QED), which is a common measure in drug discovery . However, these simple statistics, even when considered jointly, often lack expressivity, have no stability guarantees, and their use with MMD requires numerous parameter and function choices .

### Computational Topology

Computational topology assigns _invariants_--characteristic properties that remain unchanged under certain transformations--to topological spaces. For graphs, the simplest invariants are given by the \(0\)-dimensional (\(_{0}\)) and \(1\)-dimensional (\(_{1}\)) Betti numbers. These correspond to the number of connected components and number of cycles, respectively, and can be computed efficiently. Their limited expressivity can be substantially increased when paired with a scalar-valued _filtration function_\(f E\).3 Since \(f\) can only attain a finite number of values \(a_{0},a_{1},a_{2},\) on the graph, this permits calculating a _graph filtration_\( G_{0} G_{1} G_{k-1} G_{k }=G\), where each \(G_{i}:=(V_{i},E_{i})\), with \(E_{i}:=\{e E f(e) a_{i}\}\) and \(V_{i}:=\{v V e E_{i}v e\}\). This _sublevel set filtration4_ permits tracking topological features, such as cycles, via _persistent homology_. If a topological feature appears for the first time in \(G_{i}\) and disappears in \(G_{j}\), we represent the feature as a tuple \((a_{i},a_{j})\), which we can collect in a _persistence diagram_\(D\). Persistent homology thus tracks changes in connected components and cycles over the complete filtration, measured using a filtration function \(f\). Persistence diagrams form a metric space, with the distance between them given by the _bottleneck distance_, defined as \(d_{B}(D,D^{}):=(_{ D D^{}}_{x D}\|x-(x )\|_{})\), where \(\) ranges over all bijections between the two diagrams. A seminal _stability theorem_ states that the bottleneck distance between persistence diagrams \(D_{f},D_{g}\), generated from two functions \(f\) and \(g\) on the same graph, is upper-bounded by \(d_{B}(D_{f},D_{g})||f-g||_{}\). The infinity norm of the functions, a geometrical quantity, hence limits the topological variation. In practice, we convert persistence diagrams to an equivalent representation, the _persistence landscape_, which is more amenable to statistical analyses and the integration into machine learning pipelines.

Advantageous Properties.Persistent homology satisfies expressivity and stability properties. The choice of filtration \(f\) affects expressivity: with the right filtration, persistent homology can be _more_ expressive than the \(1\)-WL test [31; 57], which is commonly used to bound the expressivity of graph neural networks [50; 52; 70]. Thus, given a suitable filtration, we can create a robust and expressive metric for comparing graphs. Moreover, as we will later see, TDA improves the expressivity of _any_\(f\), meaning that even if the function on its own is not capable of distinguishing between different graphs, using it in a filtration context can overcome these deficiencies.

### Discrete Curvature

While filtrations can be learnt [29; 31], in the absence of a well-defined learning task for graph generative model evaluation, we opt instead to employ existing functions that exhibit suitable expressivity properties. Of particular interest are functions based on _discrete curvature_, which was shown to be an expressive feature for graph and molecular learning tasks [65; 68; 69]. Curvature is a fundamental concept in differential geometry and topology, making it possible to distinguish between different types of manifolds. There are a variety of different curvature formulations with varying properties, with _Ricci curvature_ being one of the most prominent. Roughly speaking, Ricci curvature is based on measuring the differences in the growth of volumes in a space as compared to a model Euclidean space. While originally requiring a smooth setting, recent work started exploring how to formulate a theory of Ricci curvature in the discrete setting [16; 19; 24; 45; 55; 62]. Intuitively, discrete curvature measures quantify a notion of similarity between node neighbourhoods, the discrete concept corresponding to 'volume.' They tend to be _larger_ for structures where there are overlapping neighbourhoods such as cliques, _smaller_ (or zero) for grids and _lowest_ (or negative) for tree-like structures. Ricci curvature for graphs provides us with sophisticated tools to analyse the neighbourhood of an edge and recent works have shown the benefits of using some of these curvature-based methods in combination with Graph Neural Networks (GNNs) to boost performance , assess differences between real-world networks , or enable _graph rewiring_ to reduce over-squashing in GNNs . However, the representational power and stability properties of these measures remain largely unexplored. Subsequently, we will focus on three different types of curvature, (i) Forman-Ricci curvature , (ii) Ollivier-Ricci curvature , and (iii) Resistance curvature . We find these three notions to be prototypical of discrete curvature measures, increasing in complexity and in their ability to capture _global_ properties of a graph.

Forman-Ricci Curvature.The _Forman(-Ricci) curvature_ for an edge \((i,j) E\) is defined as

\[_{}(i,j):=4-d_{i}-d_{j}+3|\#_{}|, \]

where \(d_{i}\) is the degree of node \(i\) and \(|\#_{}|\) is the number of \(3\)-cycles (i.e. triangles) containing the adjacent nodes.

Ollivier-Ricci Curvature.Ollivier introduced a notion of curvature for metric spaces that measures the Wasserstein distance between Markov chains, i.e. random walks, defined on two nodes . Let \(G\) be a graph with some metric \(d_{G}\), and \(_{v}\) be a probability measure on G for node \(v V\). The _Ollivier-Ricci curvature_ of _any_5 pair \(i,j V V\) with \(i j\) is then defined as

\[_{}(i,j):=1-(i,j)}W_{1}(_{i},_{j}), \]

where \(W_{1}\) refers to the first _Wasserstein distance_ between \(_{i}\), \(_{j}\). Eq. (2) defines the Ollivier-Ricci (OR) curvature in a general setting outlined by Hoorn et al. ; this is in contrast to the majority of previous works in the graph setting which specify \(d_{G}\) to be the shortest-path distance and \(_{i},_{j}\) to be uniform probability measures in the \(1\)-hop neighbourhood of the node. Extending the probability measures so that they act on larger locality scales is known to be beneficial for characterising graphs [4; 36; 26]. We assume this general setting to define different notions of OR curvature on the graph, permitting us the flexibility of altering the probability measure and the metric.

Resistance Curvature.The resistance curvature for edges of a graph, as established in Devriendt and Lambiotte , is inspired by Ohm's Law and the concept of effective resistance, a well-studied, global metric between nodes in a weighted graph that quantifies the resistance exerted by the network when current flows between nodes. For a graph \(G=(V,E)\), let \(R_{ij}\) be the resistance distance between nodes \(i,j V\), defined in Eq.9. The _node resistance curvature_ of a node \(i V\) is then defined as \(p_{i}:=1-_{j i}R_{ji}\). The curvature of an edge \((i,j) E\), what we refer to as _resistance curvature_, is then defined as

\[_{}(i,j):=+p_{j})}{R_{ij}} \]

The resistance curvature of an edge is related to the average distance between the neighbourhoods of the nodes connected by the edge.

## 3 Our Method

Each notion of discrete curvature yields a scalar-valued function on the edges of a graph. We use these functions to define a family of _curvature filtrations_ based on sublevel sets. In combination with persistent homology, this enables us to assess the structural properties of a given graph at multiple scales, measured via curvature. Using metrics on aggregated topological signatures--here, in the form of _persistence landscapes_--we may then compare two distributions of graphs. Specifically, we propose the following scheme for graph generative model evaluation:

1. Given a specific curvature filtration, we generate a set of _persistence diagrams_ that encode the persistent homology in dimensions \(0\) and \(1\) for each graph in the distribution. Each diagram tracks the lifespan of connected components and cycles as they appear in the filtration of a given graph, resulting in a multi-scale summary of the graph's structure.
2. To permit an analysis on the distributional level, we convert each diagram into a more suitable representation, namely a _persistence landscape_. As functional summaries of topological information, persistence landscapes allow for easy calculation of averages and distances .
3. Finally, we conduct statistical comparisons between graph distributions, e.g. permutation tests using \(p\)-norms, in this latent space, providing an _expressive_ metric for evaluating generative models.

Figure1 illustrates our proposed pipeline using Forman-Ricci curvature. For choosing a notion of curvature in practice, implementation details, and computational performance we refer the reader to Section3.3, AppendixA, and AppendixG respectively. We also make our framework publicly available.6

### Stability

We first discuss the general stability of topological calculations, proving that changes in topological dissimilarity are bounded by changes in the filtration functions. Moreover, we show that filtrations based on discrete curvature satisfy stability properties if the underlying graph is modified.

Topological Features.Using the persistent homology stability theorem , we know that if two curvature filtrations are similar on a graph, their persistence diagrams will also be similar. However, the theorem only holds for two different functions on the _same_ graph, whereas the distributional case has not yet been addressed by the literature. Our theorem provides a (coarse) upper bound.

Figure 1: An overview of our pipeline for evaluating graph generative models using discrete curvature. We depict a graph’s edges being coloured by \(_{}\), as described in Eq.1.The ordering on edges gives rise to a _curvature filtration_, followed by a corresponding persistence diagram and landscape. For graph generative models, we select a curvature, apply this framework element-wise, and evaluate the similarity of the generated and reference distributions by comparing their average landscapes.

**Theorem 1**.: _Given graphs \(F=(V_{F},E_{F})\) and \(G=(V_{G},E_{G})\) with filtration functions \(f,g\), and corresponding persistence diagrams \(D_{f},D_{g}\), we have \(d_{B}(D_{f},D_{g})\{(f,g),(g,f)\}\), where \((f,g):=|_{x E_{F}}f(x)-_{y E_{G}}g(y)|\) and vice versa for \((g,f)\)._

This upper bound implies that changes on the level of persistence diagrams are bounded by changes between the input functions. The stability of our method thus hinges on the stability of the filtration functions, so we need to understand the behaviour of curvature under perturbations. Following previous work , we aim to understand and quantify the stability of our method in response to adding and deleting edges in the graph. Our stability theorems establish bounds on various discrete curvature measures for _finite_, _unweighted_, _connected_ graphs in response to these perturbations. We restrict the outcome of a perturbation to graphs of the form \(G^{}=(V,E^{})\) that satisfy \(|E||E^{}|\) and do not change the number of connected components of a graph. Our theoretical results bound the new curvature \(^{}\) according to the structural properties of \(G\). For an exhaustive list of theorems and proofs, as well as experiments analysing the change in curvature for perturbed Erdos-Renyi (ER) graphs, see Appendix B and Appendix D.

Forman-Ricci Curvature.We first analyse Forman-Ricci curvature \(_{}\), and prove that it is stable with respect to adding and deleting edges.

**Theorem 2**.: _If \(G^{}\) is the graph generated by **edge addition**, then the updated Forman curvature \(^{}_{}\) for pre-existing edges \((i,j) E\) can be bounded by \(_{}(i,j)-1^{}_{}(i,j)_{ {FR}}(i,j)+2\). If \(G^{}\) is the graph generated by **edge deletion**, then the updated Forman curvature \(^{}_{}\) for pre-existing edges \((i,j) E\) can be bounded by \(_{}(i,j)-2^{}_{}(i,j)_{ {FR}}(i,j)+1\)._

Ollivier-Ricci Curvature.Let \(=(G,d_{G},)\) be a triple for specifying Ollivier-Ricci curvature calculations, with \(G\) denoting an unweighted, connected graph, \(d_{G}\) its associated graph metric, and \(:=\{_{v} v V\}\) a collection of probability measures at each node. Furthermore, let \(_{i}\) denote the Dirac measure at node \(i\) and \(J(i):=W_{1}(_{i},_{i})\) the corresponding jump probability in the graph \(G\) as defined by Ollivier . Following an edge addition or deletion, we consider an updated triple \(^{}=(G^{},d_{G^{}},^{})\), and remark that this yields an updated Wasserstein distance \(W^{}_{1}\), calculated in terms of the new graph metric \(d_{G^{}}\).

**Theorem 3**.: _Given a perturbation (either **edge addition** or **edge deletion**) producing \(^{}\), the Ollivier-Ricci curvature \(^{}_{}(i,j)\) of a pair \((i,j)\) can be bounded via_

\[1-}(i,j)}2W^{}_{}+W^{}_{1}(_{ i},_{j})^{}_{}(i,j)(i) +J^{}(j)}{d_{G^{}}(i,j)}, \]

_where \(J^{}(v):=W^{}_{1}(_{v},^{}_{v})\) refers to the new jump probabilities and \(W^{}_{}:=_{x V}W^{}_{1}(_{x},^{}_{x})\) denotes the maximal reaction to the perturbation (measured using the updated Wasserstein distance)._

Resistance Curvature.Let \(G\) be an unweighted, connected graph with a resistance distance \(R_{ij}\) and resistance curvature \(_{}(i,j)\) for each \((i,j) E\). Furthermore, let \(d_{x}\) denote the degree for node \(x V\). We find that \(_{}\) is well-behaved under these perturbations, in the sense that edge additions can only _increase_ the curvature, and edge deletions can only _decrease_ it. For edge additions, we obtain the following bound (see Appendix B.1.3 for the corresponding bound for edge deletions).

**Theorem 4**.: _If \(G^{}\) is the graph generated by **edge addition**, then \(^{}_{}_{}\),with the following bound:_

\[|^{}_{}(i,j)-_{}(i,j)|}(d_{i}+d_{j})}{R_{ij}-_{}}, \]

_where \(_{}:=_{i,j V}R_{ij}- +1}++1}\)._

The implications of this section are that (i) the stability of our topological calculations largely hinges on the stability of the functions being used to define said filtrations, and (ii) _all_ discrete curvature measures satisfy stability properties with respect to changes in graph connectivity, making curvature-based filtrations highly robust.

### Expressivity

A metric between distributions should be non-zero when the distributions differ. For this to occur, our metric needs to be able to distinguish non-isomorphic graphs and be sufficiently expressive. Hornet al.  showed that persistent homology with an appropriate choice of filtration is strictly more expressive than \(1\)-WL, the \(1\)-dimensional Weisfeiler-Le(h)man test for graph isomorphism. A similar expressivity result can be obtained for using resistance curvature as a node feature , underlining the general utility of curvature. We have the following general results concerning the expressivity or discriminative power of our topological representations.

**Theorem 5**.: _Given two graphs \(F=(V_{F},E_{F})\) and \(G=(V_{G},E_{G})\) with scalar-valued filtration functions \(f,g\), and their respective persistence diagrams \(D_{f},D_{g}\), we have \(d_{B}(D_{f},D_{g})_{}:\,E_{F} E_{G}_{x E_{F}}|f(x)-g( (x))|,\) where \(\) ranges over all maps from \(E_{F}\) to \(E_{G}\)._

Theorem 5 implies that topological distances are generally more discriminative than the distances between the filtration functions. Thus, calculating topological representations of graphs based on a class of functions improves discriminative power. To further understand the expressive power of curvature filtrations, we analyse strongly-regular graphs, which are often used for studying GNN expressivity as they cannot be distinguished by \(k\)-WL, the \(k\)-dimensional Weisfeiler-Le(h)man test, if \(k 3\). Additionally, we explore how curvature filtrations can count substructures, an important tool for evaluating and comparing expressivity . To the best of our knowledge, ours is the first work to explore discrete curvature and curvature-based filtrations in this context.

Distinguishing Strongly-Regular Graphs.Strongly-regular graphs are often used to assess the expressive power of graph learning algorithms, constituting a class of graphs that are particularly hard to distinguish. We briefly recall some definitions. A connected graph \(G\) with diameter \(D\) is called _distance regular_ if there are integers \(b_{i}\), \(c_{i}\), (\(0 i D\)) such that for any two vertices \(x,y V\) with \(d(x,y)=i\), there are \(c_{i}\) neighbours of \(y\) in \(k_{i-1}(x)\) and \(b_{i}\) neighbours of \(y\) in \(k_{i+1}(x)\). For a distance-regular graph, the intersection array is given by \(\{b_{0},b_{1},,b_{D-1};c_{1},c_{2},,c_{D}\}\). A graph is called _strongly regular_ if it is distance regular and has a diameter of 2 . We first state two theoretical results about curvature.

**Theorem 6** (Expressivity of curvature notions).: _Both Forman-Ricci curvature and Resistance curvature cannot distinguish distance-regular graphs with the same intersection array, whereas Ollivier-Ricci curvature can distinguish the Rook and Shrikhande graphs, which are strongly-regular graphs with the same intersection array._

The Rook and Shrikhande graph _cannot_ be distinguished by \(2\)-WL . However, OR curvature is sensitive to differences in their first-hop peripheral subgraphs , thus distinguishing them. This result shows the limitations of Forman-Ricci and Resistance curvature, as well as the benefits of using Ollivier-Ricci curvature. We show further experiments with curvature notions on strongly-regular graphs in the experimental section, observing improvements whenever we use them as filtrations.

Counting Substructures.Evaluating the ability of curvature to encode structural information is a crucial aspect for understanding its expressivity and validating its overall utility in graph learning. It has previously been shown that incorporating structural information of graphs can extend the expressive power of message-passing neural networks . Additionally, Bouritsas et al.  show how GNNs can be (i) strictly more expressive than \(1\)-WL by counting local isomorphic substructures (e.g. cliques), and (ii) exhibit predictive performance improvements when adding such substructure counts to the node features. For instance, some strongly-regular graphs can be distinguished by counting \(4\)-cliques. Discrete curvature measures are informed by these local substructures and have been shown to improve expressivity beyond \(1\)-WL  when included as a node feature. Nevertheless, there is limited work exploring what substructure information curvature carries, which would allow us to describe the expressivity of the measure. Moreover, persistent homology can track the number of cycles over the filtration of interest, allowing additional structural information to be encoded at multiple scales. Thus, we will explore the extent to which substructures can be counted for different curvatures (with and without a topological component) in a subsequent experiment, providing evidence on the expressive power of curvature filtrations. See Table 4 for our experimental results, and Appendix E for a more detailed discussion on the tendencies of each curvature notion when counting substructures.

### Choosing a Curvature Notion in Practice

In Section 3.1, we showed that all three of our prototypical curvature notions exhibit advantageous stability properties, thus implying that they may all be used _reliably_ within our method to measure the distance between two sets of graphs. However, which curvature should be chosen in practice? In general, we find that the answer to this question lies at the intersection of _expressivity_ and _scalability_, but depends ultimately on the nature of the experiment at hand. Nevertheless, we aim to provide an intuition for all three notions, along with a general recommendation. We hope this, in conjunction with the experimental and computational complexity results in Section 4 and Appendix G will help practitioners in making an optimal choice.

Comparison.Forman-Ricci is arguably the simplest and most local notion of curvature. Though limited in expressivity when compared to Ollivier-Ricci curvature, it is by far the most efficient to compute. Resistance curvature, by contrast, is the most global notion, making it sensitive to large substructures. However, computing the effective resistance metric on a graph requires inverting the Laplacian, making it far less efficient than Forman and Ollivier-Ricci, especially for large graphs. Finally, we have Ollivier-Ricci curvature, which we have found to be the most expressive based on its ability to (i) distinguish strongly-regular graphs, and (ii) count substructures. It is also the most versatile, given the option to adapt the underlying probability measure; this comes at the cost of lower computational performance in comparison to Forman--Ricci curvature, though.

Given its high expressivity, as well as its overall experimental and computational performance, we recommend using **Ollivier-Ricci** curvature whenever feasible.

## 4 Experiments

We have proven the theoretical stability of discrete curvature notions under certain graph perturbations. We also illustrated their ability to distinguish distance-regular and strongly-regular graphs. Subsequently, we will discuss empirical experiments to evaluate these claims and to further test the utility of our methods.

### Distinguishing Strongly-Regular Graphs

In addition to the theoretical arguments outlined, we explore the ability of our method to distinguish strongly-regular graphs in a subset of data sets, i.e. sr16622, sr261034, sr281264, and sr401224. These data sets are known to be challenging to classify since they cannot be described in terms of the \(1\)-WL test . Our main goal is _not_ to obtain the best accuracy, but to show how the discriminative power of discrete curvature can be improved by using it in a filtration context. Table 1 depicts the results of our classification experiment. We perform a pairwise analysis of _all graphs_ in the data set, calculating distances based on histograms of discrete curvature measurements, or based on the bottleneck distance between persistence diagrams ('Filtrations'). Subsequently, we count all non-zero distances (\(>1 10^{-8}\) to correct for precision errors). Our main observation is that combining TDA with curvature is always better than or equal to curvature without TDA. Similar to our theoretical predictions, both resistance curvature and Forman curvature fail to distinguish any of the graphs without TDA. We therefore show the benefits from an expressivity point of view for using discrete curvature as a filtration. Notably, we achieve the best results with OR curvature, which is particularly flexible since it permits changing the underlying _probability measure_. Using a probability measure based on random walks (see Appendix F) takes into account higher-order neighbourhoods and improves discriminative power (on sr261034, the pairwise success rate drops to 0.0/0.2 with raw/TDA values, respectively, if the uniform probability measure is used).

### Expressivity experiments with the BREC dataset

We evaluate discrete curvatures and their filtrations on the BREC data set, which was recently introduced to evaluate GNN expressiveness . The data set consists of different categories of graph pairs (Basic, Regular, and Extension), which are distinguishable by 3-WL but not by 1-WL, as well as Strongly-Regular (STR) and CFI graph pairs, which are indistinguishable using 3-WL. We explore the ability of curvature filtrations to distinguish these graph pairs and compare them

   Method & sr16622 & sr261034 & sr281264 & sr401224 \\  \(_{}\) & 0.00 & 0.00 & 0.00 & 0.00 \\ \(_{}\) & **1.00** & 0.78 & **1.00** & 0.00 \\ \(_{}\) & 0.00 & 0.00 & 0.00 & 0.00 \\  Filtration (\(_{}\)) & **1.00** & 0.20 & 0.00 & **0.93** \\ Filtration (\(_{}\)) & **1.00** & **0.89** & **1.00** & **0.93** \\ Filtration (\(_{}\)) & **1.00** & 0.20 & 0.00 & **0.93** \\   

Table 1: Success rate (\(\)) of distinguishing pairs of strongly-regular graphs when using either raw curvature values or a curvature filtration.

to substructure counting, \(S_{3}\) and \(S_{4}\), which involves enumerating all 3-node/4-node substructures around nodes in combination with the WL algorithm. These approaches, unlike discrete curvature, have limited practical applications due to their high computational complexity. Similar to our experiments on strongly-regular graphs, we calculate Wasserstein distances based on histograms of OR curvature measurements between the pairs of graphs. Subsequently, we count all non-zero distances (\(>1 10^{-8}\) to correct for precision errors). Our main observations from Table 2 are that curvature _can_ distinguish graphs which are 3-WL indistinguishable. Additionally, we observe improvements in success rate using the filtration on the Basic, Regular, STR and Extension graph pairs. Moreover, our curvature-based approach performs competitively and sometimes even better than \(S_{4}\), which has been shown to be extremely effective in graph learning tasks . Despite its empirical prowess, \(S_{4}\) is computationally expensive, making it an _infeasible_ measure in many applications. Discrete curvature and its filtrations, by contrast, scale significantly better.

### Behaviour with Respect to Perturbations

To explore the behaviour of curvature descriptors under perturbations, we analyse the correlation of our metric when adding and removing edges in the 'Community Graph' data set: we increase the fraction of edges added or removed from \(0.0\) to \(0.95\), measuring the distance between the perturbed graphs and the original graphs for each perturbation level. Following O'Bray et al. , we require a suitable distance measure to be highly correlated with increasing amounts of perturbation. We compare to current approaches that use descriptor functions with MMD. As Table 3 shows, a curvature filtration yields a higher correlation than curvature in combination with MMD, showing the benefits of employing TDA from a stability perspective. Additionally, curvature filtrations improve upon the normalized Laplacian and clustering coefficient (again, we used MMD for the comparison of these distributions). OR curvature exhibits particularly strong results when adding/removing edges, even surpassing the local degrees of a graph (which, while being well-aligned with perturbations of the graph structure, suffer in terms of overall expressivity).

### Counting Substructures

The ability of a descriptor to count local substructures is important for evaluating its expressive power . We follow Chen et al. , who assess the ability of GNNs to count substructures such as triangles, chordal cycles, stars and tailed triangles. This is achieved by generating regular graphs with random edges removed, and counting the number of occurrences of each substructure in a given graph. To assess the power of curvature to capture such local structural features, we use the same experimental setup and pass the edge-based curvatures through a simple 1-layer MLP to output the substructure count. Additionally, we evaluate the effect of using curvature as a filtration.

   Method & Basic (56) & Regular (50) & STR (50) & Extension (97) & CFI (97) \\ 
1-WL & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
3-WL & **1.00** & **1.00** & 0.00 & **1.00** & **0.59** \\  \(S_{3}\) & 0.86 & 0.96 & 0.00 & 0.05 & 0.00 \\ \(S_{4}\) & 1.00 & 0.98 & **1.00** & 0.84 & 0.00 \\  \(_{}\) & 1.00 & 0.96 & 0.06 & 0.93 & 0.00 \\ \(_{}\) & 0.96 & 0.92 & 0.00 & 0.52 & 0.00 \\ \(_{}\) & **1.00** & **1.00** & 0.00 & **1.00** & 0.04 \\   Filtration (\(_{}\)) & **1.00** & **1.00** & 0.08 & 0.95 & 0.00 \\ Filtration (\(_{}\)) & 0.98 & 0.96 & 0.04 & 0.61 & 0.00 \\ Filtration (\(_{}\)) & **1.00** & **1.00** & 0.04 & **1.00** & 0.04 \\   

Table 2: Success rate (\(\)) of distinguishing pairs of graphs in the BREC dataset when using different discrete curvatures and their filtrations.

   Measure & Adding Edges & Removing Edges \\  Laplacian & \(0.457 0.013\) & \(0.420 0.000\) \\ Clust. Coeff. & \(0.480 0.012\) & \(0.504 0.020\) \\ Degrees & \(0.761 0.003\) & \(0.995 0.000\) \\  \(_{}\) & \(0.420 0.000\) & \(0.432 0.003\) \\ \(_{}\) & \(0.903 0.005\) & \(0.910 0.002\) \\ \(_{}\) & \(0.420 0.004\) & \(0.441 0.005 the curvature alone, the only exception being Forman curvature for counting \(4\)-cycles. We leave a more detailed investigation of these phenomena for future work.

### Synthetic Graph Generative Model Evaluation

To have a ground truth for a graph distribution, we tested our metric's ability to _distinguish graphons_. Following the approach suggested by Sabanayagam et al. , we generate four graphons, \(W_{1}(u,v)=uv\), \(W_{2}(u,v)=\{-(u,v)^{0.75}\}\), \(W_{3}(u,v)=\{-0.5*((u,v)+u^{0.5}+v^{0.5})\}\) and \(W_{4}(u,v)=\|u-v\|\). Sampling from these graphons produces dense graphs, and we control their size to be between \(9\) and \(37\) nodes, thus ensuring that we match the sizes of molecular graphs in the ZINC data set , an important application for generative models.

We perform experiments by considering all combinations of three and four graphons. We generate distances between graphs with our method as well as other kernel-based approaches, and use spectral clustering to separate the distributions. We measure the performance of the algorithms using the Adjusted Rand Index (ARI) of the predicted clusters, comparing to three state of the art, kernel-based approaches: (i) Wasserstein Weisfeiler-Le(h)man graph kernels , (ii) graph neural tangent kernels , and (iii) Tree Mover's Distance . From Figure 1(a), we find that a filtration based on OR curvature is better able to distinguish and cluster graphons than the previously-described approaches based on kernels and it performs best for all sets of graphons. We also observe that OR curvature performs better than other discrete curvatures, with resistance curvature achieving higher ARI than Forman curvature. Notice that unlike these kernel approaches, our method can be easily extended to provide a proper metric between distributions of graphs.

   Method & Triangle & Tailed Tri. & Star & \(4\)-Cycle \\  Trivial Predictor & 0.88 & 0.90 & 0.81 & 0.93 \\ GCN & 0.42 & 0.32 & **0.18** & **0.28** \\  \(_{}\) & 0.54 & 0.56 & 0.72 & 0.53 \\ \(_{}\) & 0.33 & 0.31 & 0.40 & 0.31 \\ \(_{}\) & 0.59 & 0.50 & 0.72 & 0.47 \\  Filtration (\(_{}\)) & 0.45 & 0.52 & 0.49 & 0.60 \\ Filtration (\(_{}\)) & **0.23** & **0.24** & 0.34 & 0.31 \\ Filtration (\(_{}\)) & 0.47 & 0.48 & 0.36 & 0.42 \\   

Table 4: MAE (\(\)) for counting substructures based on raw curvature values and curvature filtrations. The ‘Trivial Predictor’ always outputs the mean training target.

Figure 2: (a) Adjusted Rand Index (\(\)) for clustering sets of four graphons. We compare our curvature filtrations (b) to kernel-based methods. (b) Permutation testing values (\(\)) for distinguishing different bioinformatics data sets. Position (\(i,j\)) in each matrix denotes a permutation test between data set \(i\) and data set \(j\).

### Real-World Graph Generative Model Evaluation

By converting the generated persistence diagrams into a _persistence landscape_, we can generate an average topological descriptor for all graphs in a distribution . This allows us to calculate norms between graph distributions, making it possible to perform two-sample and permutation testing, unlike a majority of kernel-based approaches, which provide a distance between _all_ individual graphs, or would require MMD to assess mean similarities. We randomly sample ten graphs from four different bioinformatics data sets, i.e. KKI, PROTEINS, Peking, and ENZYMES . We measure the distance between two data sets using the \(L^{P}\) norm between their average persistence landscapes. We then permute graphs from both samples, randomly selecting sets of equal size, measure their respective distances, and finally aggregate the fraction of distances which are higher than the original. A low fraction indicates that distances are _lower_ for permutations than between the original sets of graphs, suggesting that the metric can distinguish between the two distributions. We compare our approach to previous methods that combine graph statistics with MMD. Using a significance threshold of \(p<0.05\), we see in Figure 1(b) that both Forman and the OR curvature are able to distinguish _all but one pair of data sets, an improvement over all the other approaches_. In general, we find that fractions are lower using curvature filtrations than graph statistic based approaches, demonstrating the utility of our approach.

## 5 Conclusion

We have described the first thorough analysis of both _stability_ and _expressivity_ of discrete curvature notions and their filtration formulations on graphs. We believe this to be important for the community in multiple contexts, ranging from improving expressivity of GNNs to understanding the robustness of curvature on graphs. Using curvature filtrations and their topological descriptors (here: _persistence landscapes_), we develop a new metric to measure distances between graph distributions. Our metric can be used for evaluating graph generative models, is robust and expressive, and improves upon current approaches based on graph descriptor and evaluator functions. We have also demonstrated clear advantages over state-of-the-art methods that combine graph statistics with MMD, providing instead a metric with (i) well-understood parameter and function choices, (ii) stability guarantees, (iii) added expressivity, and (iv) improved computational performance. Most notably, we scale _significantly_ better for large populations of molecular-sized graphs (see Appendix G for more details), which we consider crucial for current graph generative model applications. We hope that our pipeline will provide a principled, interpretable, and scalable method for practitioners to use when evaluating the performance of graph generative models.

Future work could explore other representations , focus on different filtration constructions , new curvature measures, or further extend our stability and expressivity results (for instance to the setting of weighted graphs). Given the beneficial performance and flexibility of Ollivier-Ricci curvature in our experiments, we believe that changing--or _learning_--the probability measure used in its calculation could lead to further improvements in terms of expressivity, for example. Another relevant direction involves incorporating node and edge features into the distance measure and applying the model to specific use cases such as evaluating molecule generation. We envision that this could be done using bi-filtrations or by considering node features for the curvature calculations.