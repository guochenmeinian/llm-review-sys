# Tight Bounds for Learning RUMs from Small Slates

Flavio Chierichetti

Sapienza University of Rome

flavio@di.uniroma1.it &Mirko Giacchini

Sapienza University of Rome

giacchini@di.uniroma1.it &Ravi Kumar

Google, Mountain View

ravi.k53@gmail.com &Alessandro Panconesi

Sapienza University of Rome

ale@di.uniroma1.it &Andrew Tomkins

Google, Mountain View

atomkins@gmail.com

###### Abstract

A Random Utility Model (RUM) is a classical model of user behavior defined by a distribution over \(^{n}\). A user, presented with a subset of \(\{1,,n\}\), will select the item of the subset with the highest utility, according to a utility vector drawn from the specified distribution. In practical settings, the subset is often of small size, as in the "ten blue links" of web search.

In this paper, we consider a learning setting with complete information on user choices from subsets of size at most \(k\). We show that \(k=()\) is both necessary and sufficient to predict the distribution of all user choices with an arbitrarily small, constant error.

Based on the upper bound, we obtain new algorithms for approximate RUM learning and variations thereof. Furthermore, we employ our lower bound for approximate RUM learning to derive lower bounds to fractional extensions of the well-studied \(k\)-deck and trace reconstruction problems.

## 1 Introduction

In many common settings, both online and offline, users select from a set of available candidates: cars on a dealer's lot; songs on a streaming service; movies in a Netflix carousel of choices; and so forth. Often, it is unrealistic to offer the user the entire universe of items. No car dealership has every new and used car ever produced. Likewise, recommendation services have enormous catalogs of songs, products, movies, etc, and must carefully curate a more manageable subset of recommended items that will fit within the constraints of the user interface. Thus, user feedback often arrives as a choice from slates of items of a certain standard cardinality--think of "ten blue links" in web search as the classical example.

Random Utility Models (commonly called _RUMs_) have been the standard mathematical model for studying user choices over subsets of a universe of items. RUMs are the subject of many decades of study, and the centerpiece of the 2000 Nobel prize in economics. The model family is straightforward: a RUM operates over a universe \(U\) of items, with \(|U|=n\), and is characterized by a distribution over \(^{n}\). A user draws from this distribution to generate a vector specifying the utility of each item of \(U\). The user is offered a subset of \(U\) called a _slate_, and must select a single item from the slate; the user behaves rationally by selecting the available item of the highest utility. The _winning distribution_ of the slate is a probability distribution over the items of the slate representing the likelihood (over draws from the utility distribution) that a particular item is selected. To learn a RUM, an algorithm is given a training set of examples of slates with their winning distributions, and must then guess the winning distributions of a new test set of slates.

We are interested in learning RUMs when the example slates in the training set are constrained to have at most a certain cardinality. We will then deem an algorithm successful if exposure to these smaller slates allows it to approximate the winning distribution of all slates. In the extreme example, the algorithm should infer the winning distribution over the universal slate \(U\) itself, representing the likelihood that a particular item is a random user's favorite from the entire catalog.

Our contributions.We present two main results representing paired upper and lower bounds for this question. The upper bound shows that, with knowledge of the winning distributions for slates of size at most \(O()\), one can approximate the winning probability for any item in any slate to within an arbitrarily small additive constant. Using this upper bound, we obtain an exponential improvement in the time to learn a RUM. The previous best-known algorithm, implicit in earlier work, requires time \(2^{O(n)}\). Our new algorithm learns any RUM to within an \(_{}\)-error (resp., \(_{1}\)-error) of \(>0\) in time \(n^{O(})}\) (resp., \(n^{O(})}\)). We also give a "simulation" result: the winning distribution of each slate \(T\) can be approximated within a constant \(_{}\)-error by querying polynomially many sub-slates of \(T\), each of size at most \(O()\).

Our near-matching lower bound shows that, with knowledge of the winning distributions for all slates up to size \(o()\), any algorithm must make error \(1/2-\) (almost the worst possible) in predicting a target item's probability in the universal slate, for arbitrarily small constant \(\). Based on this lower bound, we also obtain lower bounds to fractional extensions of the well-studied \(k\)-deck and trace reconstruction problems.

The bounds for RUM learning algorithms depend on the nature of the oracle used to present examples to the algorithm. For a given slate, the Max-Dist oracle returns the exact winning distribution, while the Max-Sample oracle simply returns a draw from the winning distribution, as we would expect from a real-world setting. The algorithms we present work with both oracles, with asymptotically the same running time and the same sample complexity.

Overview of techniques.Our results are proved by observing a series of connections between RUM learning and the approximation of the bitwise \(\) function via polynomials. Specifically, the two main quantities of interest in approximating Boolean functions with polynomials are (i) the degree of the polynomial, and (ii) the \(_{1}\)-norm of the coefficients of the polynomial. We will show that these two quantities are related, respectively, to (i) the size of the slates required to approximate a RUM (Theorem 9 and Theorem 12), and (ii) the running time to approximate the winning distribution of a queried slate (Theorem 9 and Corollary 10).

Related work.Discrete choice has been the subject of extensive research in machine learning and economics; see (Train, 2003) for an excellent introduction. RUMs are an important class of models in discrete choice--in particular, Multinomial Logits (MNLs) and their mixtures are special classes of RUMs. RUMs have been extensively studied from both active and passive learning perspectives (Soufiani et al., 2012; Oh and Shah, 2014; Chierichetti et al., 2018; Negahban et al., 2018; Tang, 2020) and from an efficient representation point of view (Farias et al., 2009; Chierichetti et al., 2021).

A number of papers have used linear programming (LP) for obtaining representation of RUMs that agree with an empirical distribution on small slates (Farias et al., 2009; Almanza et al., 2022; Chierichetti et al., 2023). In particular, Farias et al. (2009) make strong assumptions on the underlying RUM, while in (Almanza et al., 2022; Chierichetti et al., 2023) the algorithms are not required to generalize outside of the observed training set. Instead, we are asking what is the minimum \(k\) such that if one observes all the winning distributions of slates of size up to \(k\), one can approximately reconstruct _all_ the winning distributions of a RUM? Note that we do not make any assumption on the RUM and our algorithms must generalize to all the slates.

The approximation of Boolean functions via polynomials has found applications in a disparate and apparently remote number of fields such as cryptography (Bogdanov et al., 2016), differential privacy (Thaler et al., 2012), quantum query complexity (Beals et al., 2001), PAC learning (Klivans and Servedio, 2004), and more. Our work draws connections between this field and RUM learning for the first time, showing yet another important application of such techniques.

In this paper we also strengthen the relation, that was first observed in (Chierichetti et al., 2018), between RUM learning and the \(k\)-deck problem.

Organization.In Section 2 we introduce the definitions and notation. In Section 3 we prove our upper bound, which we then use in Section 4 to obtain algorithms for learning RUMs. In Section 5, we present our lower bound. Finally, in Sections 6 and 7 we derive, as corollaries of our results, lower bounds for other learning problems. All the proofs missing from the main body of the paper can be found in Appendix A.

## 2 Background

Let \([n]=\{1,,n\}\) and let \(_{n}\) denote the symmetric group on \(n\) items (the set of all permutations of \([n]\)). Let also \(=\{T S|T|=k\}\). For a distribution \(D\), \(x D\) denotes that the random variable \(x\) is drawn from \(D\) and \(D(i)\) denotes \(_{x D}[x=i]\), where \(i(D)\). For \(a,b[n],_{n}\) we write \(a<_{}b\) or \(b>_{}a\) to say that \(\) ranks \(b\) higher than \(a\).

Random utility models.A _slate_ is a non-empty subset of \([n]\). For a slate \( S[n]\) and a permutation \(_{n}\), let \((S)\) be the item of \(S\) that wins, i.e., ranks the highest in \(\).

The following definition of RUMs, based on probability distributions over permutations, is equivalent to the utility-vectors definition given in the Introduction (Chierichetti et al., 2018a); we adopt the distribution-over-permutations definition, since it makes it easier to present our algorithms.

**Definition 1** (Random Utility Model (RUM)).: _A random utility model (RUM)\(R\) on \([n]\) is a distribution on \(_{n}\). For a slate \(S\), \(R_{S}\) denotes the distribution of the random variable \((S)\) where \( R\), so \(R_{S}(s)=[sS]\) is called the winning distribution on \(S\) induced by RUM \(R\)._

We consider two types of oracle access to the RUM: the Max-Dist oracle returns for a given slate the (exact) winning distribution for the slate and the Max-Sample oracle returns a draw from the winning distribution.

Approximation of \(\).For a bit string \(x=x_{1} x_{n}\{0,1\}^{n}\) and \(S[n]\), let \(_{S}(x)=_{i S}x_{i}\). By convention, \(_{}(x)=1\). The function \(_{n}:\{0,1\}^{n}\{0,1\}\) is the bitwise-AND given by \(_{n}(x)=_{[n]}(x)\). We write only \(\) when \(n\) is clear from the context. A polynomial \(p:\{0,1\}^{n}\) is said to \(\)-approximate the AND function if for all \(x\{0,1\}^{n}\) it holds \(|(x)-p(x)|\). The \(\)_-approximate degree_ of \(\) is the smallest value \(_{}()\) such that there exists a polynomial of such degree that \(\)-approximates the \(\) function, and it is well known that the optimal value is \(_{}()=()\)(Bun and Thaler, 2022). The general form of a degree-\(k\) polynomial is \(_{S[n],|S| k}a_{S}_{S}(x)\), where the \(\{a_{S}\}_{S[n],|S| k}\) are real coefficients. However, all the polynomials proposed in the literature have at most \(k+1\) distinct coefficients \(a_{0},a_{1},,a_{k}\), such that \(a_{S}=a_{|S|}\), therefore in this work we will focus only on polynomials of such form.1 While it is folklore that such coefficients can be computed in polynomial time, for completeness we provide in Appendix D an explicit algorithm for this task. We will use several results on the approximation of the \(\) function that we introduce as needed.

## 3 Uniform approximation of a RUM

In this section we show that if two RUMs agree on small slates, then they nearly agree on all slates. Our result can be obtained as a consequence of a more general result of Sherstov (2008), which is based on the approximate degree of the \(\) function and was originally stated only for sub-constant errors. The following is a restatement of Sherstov (2008, Theorem 4.8), using the upper bound on the approximate degree of \(\) proved by (Buhrman et al., 1999); we also make the error term explicit.

**Theorem 2** (Sherstov (2008)).: _There exists a constant \(c>0\) such that the following holds. Consider any two probability spaces \(_{1}\) and \(_{2}\), and any events \(A_{1},,A_{n}\) in \(_{1}\) and \(B_{1},,B_{n}\) in \(_{2}\) such that, for any \(S[n]\), \(|S| c\), it holds \(_{_{1}}[_{i S}A_{i}]=_{_{2}} [_{i S}B_{i}]\). Then, it holds: \(|_{_{1}}[_{i[n]}A_{i}]-_{_ {2}}[_{i[n]}B_{i}]|\), where \((2^{-n},1/3)\)_

**Theorem 3** (RUMs Upper Bound).: _Let \(P\) and \(Q\) be two RUMs on \([n]\). There exists a constant \(c>0\) such that for a given \(s[n]\), \(T[n]\{s\}\), if \(P_{S\{s\}}(s)=Q_{S\{s\}}(s)\) for each \(S\{T^{} T^{} T|T^{}| c }\}\), then \(|P_{T\{s\}}(s)-Q_{T\{s\}}(s)|\), where \((2^{-|T|},1/3)\)._

Proof.: Let \(c\) be the constant of Theorem 2 and define \(k=c\). Consider the probability space \(_{1}\) (resp. \(_{2}\)) having \(_{n}\) as sample space and RUM \(P\) (resp. \(Q\)) as the probability mass function. For \(t T\), let \(A_{t}\) be the event \(\{_{n} s>_{}t\}\). Then, for any \(S T\), \(\{_{n}(S\{s\})=s\}=_{i S}A_{i}\). Therefore, for any \(S T\), \(|S| k\) it holds:

\[_{_{1}}[_{i S}A_{i}]=_{ P}[( S\{s\})=s]=P_{S\{s\}}(s)=Q_{S\{s\}}(s)=_{_{2}} [_{i S}A_{i}],\]

where the third equality follows by hypothesis. Finally, by Theorem 2:

\[|P_{T\{s\}}(s)-Q_{T\{s\}}(s)|=|_{_{1}} [_{i T}A_{i}]-_{_{2}}[_{i T}A_{i} ]|.\]

In light of Theorem 3, accessing slates of size up to \(O(})\) is enough to predict the winning distribution of all the slates, within an additive \(\). In the next section, we obtain a computational version of this result.

## 4 Reconstruction algorithms

In this section we obtain two algorithms for reconstructing the winning distributions on large slates using the winning distribution on small slates. The goal of these algorithms is to obtain a computational version of Theorem 3. The first algorithm is a proper learning algorithm that outputs a RUM. Building the RUM takes time \(n^{O(n)}\) but once built, querying this RUM to get the approximate winning distribution of any given slate takes only polynomial time. Moreover, using previous work , this RUM can actually be approximately represented using \(O(n^{2} n)\) bits. The second algorithm is an improper learning algorithm: while its output model allows uniformly approximating the winning distribution on each slate, this model might not be a RUM. Building the model takes time \(n^{O()}\) and once built, querying this model to get the approximate winning distribution of any given slate takes time \(2^{O()}\). The total bit complexity of the second algorithm's model is \(n^{O()}\). This second algorithm has two nice properties: (i) if we are given access to slates larger than \(\), then querying the model becomes more efficient, and (ii) if we want to estimate the winning distribution of only \(M=(n)\) pre-determined slates, then building the model becomes more efficient. Putting these two properties together we are able to prove a "simulation" result: for any pre-determined slate \(T[n]\) it is possible to estimate \(R_{T}\) to within a constant \(_{}\)-error \(\) in polynomial time and accessing slates of size at most \(O()\).

### A proper learning algorithm

Fix a large enough integer \(t n-1\). For a RUM \(Q\), let the winning distributions of slates of size at most \(k\), for \(k=O(})\) be known. To estimate the probability distributions \(Q_{T}\) for any \(s[n]\) and for any slate \(T[n]\{s\}\) such that \(|T| t\), it is sufficient to solve the following linear program (LP), with no objective function:

\[\{_{_{n}}p_{}=Q_{S\{s \}}(s)& s[n]\; S[n]\{s\}|S| k-1\\ _{_{n}}p_{}=1&\\ p_{} 0&_{n}. \]

Indeed, (1) returns a RUM \(P\) that is compatible with RUM \(Q\) on each slate of size at most \(k\). Applying Theorem 3, we obtain:

**Observation 4**.: _For \(k=(})\), let \(P\) be the RUM obtained by solving (1). Then, for any \(s[n]\), and for any \(T[n]\{s\}\) such that \(|T| t\), it holds \(|P_{T\{s\}}(s)-Q_{T\{s\}}(s)|\)_

By fixing \(t=n-1\) and solving (1)--an LP with \(n!\) variables and \(n^{O(})}\) constraints--we get:

**Theorem 5** (Proper learning algorithm).: _Let \(Q\) be a RUM over \([n]\). There exists an algorithm that uses the Max-Dist oracle on each state of size at most \(O(})\) and in time \(n^{O(n)}\) produces a RUM \(P\) such that for each \(S[n]\) and for each \(i S\), \(|P_{S}(i)-Q_{S}(i)|\)._

Using the result in , the RUM \(P\) can be subsampled in \((n)\) time to a uniform RUM \(\) with a multiset of \(O(n/^{2})\) permutations as its support, and such that for each \(S[n]\), \(|_{S}-P_{S}|_{1}\). Thus, by accessing slates of size at most \(O(})\) (resp., \(O(})\)), one can produce a data structure \(\) in time \(n^{O(n)}\) such that (i) \(\) can be represented with \(O(^{-2} n^{2} n)\) bits, and (ii) when \(\) is queried on a slate \(S\), it can return in \((n)\) time a distribution \(_{S}\) such that \(|_{S}-Q_{S}|_{}\) (resp., \(|_{S}-Q_{S}|_{1}\)).

By providing a version of Theorem 3 that holds when the small slates are approximately equal, this algorithm can also be made to work with Max-Sample oracle. More details are given in Appendix B.

We mention that this algorithm can be made to run in time \(2^{O(n)}\) by using the ellipsoid method and the separation oracle of Chierichetti et al. (2023); more details are given in Appendix C.

### An improper learning algorithm

In this section we obtain a learning algorithm whose data structure is not a RUM but can be built faster. As before, this is a restatement of Sherstov (2008, Theorem 4.8):

**Theorem 6** (Sherstov (2008)).: _Consider any probability space \(\), and any events \(A_{1},,A_{n}\) in \(\). For \(k()\), \((2^{-n},1/3)\), let \(\{a_{i}\}_{0 i k}\) be the coefficients of a degree \(k\) polynomial approximating the \(\) function within \(\) (for any \(S[n],|S| k\), the coefficient of the monomial \(_{S}(x)\) is \(a_{|S|}\)). Then, \(|[_{i[n]}A_{i}]-_{S[n],|S|  k}a_{|S|}[_{i S}A_{i}]|\)_

Given a RUM \(R\) over \([n]\) and access to slates of size \(k(})\), consider an element \(s[n]\) and a slate \(T[n]\{s\}\). Let \(\{a_{i}\}_{0 i k-1}\) be the coefficients of a polynomial of degree \(k-1\) that approximates the \(_{|T|}\) function. Then, the following is a good approximation for \(R_{T\{s\}}(s)\):

\[_{T\{s\}}(s)=_{S T,|S| k-1}a_{|S|} R_{ S\{s\}}(s).\]

In fact, choosing the probability space and events as in Theorem 3 and applying Theorem 6, we get:

**Observation 7**.: \(|_{T\{s\}}(s)-R_{T\{s\}}(s)|\)_, where \(R\) is a RUM over \([n]\), \(s[n]\), \(T[n]\{s\}\), and for \(k(})\), \(\{a_{i}\}_{0 i k-1}\) are the coefficients of a polynomial of degree \(k-1\) approximating the \(_{|T|}\) function (the coefficient of the monomial \(_{S}(x)\) is \(a_{|S|}\))._

From the above observation and given Max-Dist oracle access to slates of size \(k=()\), we obtain a deterministic algorithm that first stores \(R_{S}(s)\) for all \(s S[n],|S| k\), in time \(n^{O(k)}\) and then, upon query \((s,T)\) returns the approximation \(_{T\{s\}}(s)\) that can be computed in \(|T|^{O()} n^{O(k)}\) time2.

Note that this result holds for any polynomial approximating the \(\) function. To get a better algorithm, which also works with Max-Sample oracle, we focus on a specific polynomial. (Observealso that the time to answer a query in the previous algorithm does not improve as \(k\) increases. This is counter-intuitive: given access to larger slates, it should become easier to approximate the target slate. The second algorithm that we provide gets faster as \(k\) increases, overcoming this limitation.)

The polynomial that we use is the one proposed by Huang and Viola (2022, Corollary 1.5):

**Theorem 8** (Huang and Viola (2022)).: _For all \((2^{-n},1/3)\), \( d n\), there exists a polynomial \(p:\{0,1\}^{n}\) of degree \(k=(d)\) and real coefficients \(\{a_{i}\}_{0 i k}\), where the coefficient of \(_{S}(x)\) is \(a_{|S|}\), such that: (i) for each \(x\{0,1\}^{n}\), \(|p(x)-_{n}(x)|\), and (ii) \(_{S[n],|S| k}|a_{|S|}|=_{c=0}^{k}|a_{c}| 2 ^{O()}\)._

**Theorem 9** (Improper learning algorithm).: _Let \(R\) be a RUM over \([n]\). Let \(d\), \((0,1/3)\), and \((0,1)\) such that \(,}\). Then, there exists a randomized algorithm that accesses slates of size at most \(k=(d)\) and such that:_

_(i) it first makes_ \(n^{O(k)}\) _queries to Max-Sample oracle (or Max-Dist oracle) and then,_

_(ii) for any query_ \(s[n]\)_,_ \(T[n]\{s\}\)_, it returns, in time_ \(2^{O()}(|T|)\) _and with probability at least_ \(1-\)_, an estimate_ \(_{T\{s\}}(s)\) _such that_ \(|_{T\{s\}}(s)-R_{T\{s\}}(s)|\)_._

For \(k=()\), Theorem 9 gives an algorithm with a pre-processing time of \(n^{O(k)}\) and that can answer any query \((s,T)\) in time \(2^{O(|T|(1/)/k)}|T|^{O(1)} 2^{O()}  2^{O(k)}\). Note that the query-time of this algorithm gets better increasing \(k\) (although the pre-processing time gets worse since more slates must be queried).

### A simulation algorithm

The pre-processing time of the improper learning algorithm increases with the slate size because the algorithm must be able to reply to _every_ query after the pre-processing phase. Suppose, however, that the algorithm knows in advance which slates will be queried; in that case, it can perform the pre-processing phase to satisfy only such requests. In this setting where the queries are known offline (or where the oracles can be called lazily), we can get a faster algorithm.

**Corollary 10** (Simulation algorithm).: _Let \(R\) be a RUM over \([n]\). Choose any element \(s[n]\), slate \(T[n]\{s\}\), \((0,1/3)\), and \((0,1)\) such that \(,}\). Then, there exists a randomized algorithm that, for \(d\), accesses slates of size at most \(k=(d)\), makes at most \(2^{O(}{k})}(|T|)\) queries to Max-Dist oracle (or Max-Sample oracle), and that with probability at least \(1-\) outputs a value \(_{T\{s\}}(s)\) such that \(|_{T\{s\}}(s)-R_{T\{s\}}(s)|\)_

Note that by choosing \(k=()()\), and _constant_\((0,1)\), the previous algorithm returns, with high probability and accessing slates at most \(k\), an approximation to \(R_{T\{s\}}(s)\) in polynomial time, for any predetermined \(s[n],T[n]\{s\}\).

We can also interpret this algorithm in the more general setting of Sherstov (2008). In such setting, Corollary 10 implies that, for any \(n\) events in a probability space, the probability of the intersection of all the events can be well-approximated by a linear combination of polynomially many probabilities of smaller intersections (specifically, each intersection is over at most \(O(n/ n)\) events).

## 5 Lower bounds

In this section we present a lower bound showing that it is impossible to reconstruct the winning distribution of the full slate by only looking at slates of size \(o()\), i.e., our reconstruction (Theorem 3) is optimal. For simplicity, we consider the approximation of the winning distribution of the full slate \([n]\), as opposed to any slate \(T\) as in Theorem 3.3Our construction actually shows that, in this \(o()\)-slates setting, it is impossible to approximate the distribution of the full slate even within a constant \(_{}\)-error. Specifically, for any constant \(>0\), it is not possible to learn whether a special item, \(n\), has probability at least \(1-\) or at most \(\) in the full slate by accessing only slates of size \(o()\).

We will make use of the following result, which is a consequence of the method of dual polynomials (see, e.g., (Bun and Thaler, 2022, Chapter 6)), and was first proved for cryptographic applications in (Bogdanov et al., 2016, Theorem 1). While the original result considers general Boolean functions, we state it only for the \(\) function, plugging in the lower bound on the \(\)-approximate degree of the \(\) proved in (Bun and Thaler, 2015, Proposition 14).

**Theorem 11** (Bogdanov et al. ).: _For a sufficiently large \(n\) and constant \((0,1)\), there exists a constant \(c>0\) and two probability distributions \(,\) over \(\{0,1\}^{n}\) such that: (i) for each polynomial \(p:\{0,1\}^{n}\) of degree at most \(c\), \(_{x}[p(x)]=_{x}[p(x)]\), and (ii) \(|_{x}[(x)]-_{x }[(x)]|>1-\)_

We are now ready to prove our lower bound for RUMs.

**Theorem 12** (RUMs Lower Bound).: _For a sufficiently large \(n\) and for constant \((0,1)\), there exists a constant \(c>0\) and two RUMs \(A,B\) on \([n]\) such that: (i) for each \(S[n]\) such that \(|S| c=()\), it holds \(A_{S}=B_{S}\), and (ii) \(|A_{[n]}(n)-B_{[n]}(n)|>1-\)_

Proof.: Consider the distributions \(,\) over \(\{0,1\}^{n-1}\) from Theorem 11, and let \(k=c\). We build RUM \(A\) on \([n]\) as follows: sample \(x\{0,1\}^{n-1}\) according to \(\), then, let \(S_{x}=\{i[n-1] x_{i}=1\}\) and sample a uniform at random permutation among those where the set of elements ranked _lower_ than \(n\) is \(S_{x}\). RUM \(B\) is defined similarly, but \(x\) is sampled from \(\). Note that for \(S[n-1]\), \(A_{S\{n\}}(n)=_{x}[_{S}(x)]\), and similarly for \(B\). Then, from property (ii) of Theorem 11 we have:

\[|A_{[n]}(n)-B_{[n]}(n)|=|*{E}_{x}[_{ n-1}(x)]-*{E}_{x}[_{n-1}(x)] |>1-.\]

Moreover, fix any \(S[n-1]\), \(|S| k\). We have:

\[A_{S\{n\}}(n)=*{E}_{x}[_{S}(x)]= *{E}_{x}[_{S}(x)]=B_{S\{n\}}(n),\]

where the second equality follows by Theorem 11(ii) and since \(_{S}(x)=_{i S}x_{i}\) is a polynomial of degree \(|S| k\).

It remains to show that the winning distributions for the elements different from \(n\) in the small slates also coincide. We show that for RUMs \(A\) and \(B\), these probability distributions can be expressed in terms of winning distributions of \(n\); below, we do the calculations only for \(A\), the calculations for \(B\) are analogous.

Let \(_{x}\) be the uniform distribution over the set of permutations where the set of elements ranked _lower_ than \(n\) is \(S_{x}\), for a string \(x\{0,1\}^{n-1}\). For convenience, for \(x\{0,1\}^{n-1}\), we set \(x_{n}=1\). Choose any \(i S[n]\), \(i n\), \(|S| k\), then:

\[A_{S}(i) =_{ A}[(S)=i]=_{T S}_{x ,_{x}}[_{T}(x)_{s S T}(1-x_{ s})=1\ \ (S)=i]\] \[=_{T S}_{x}[_{T}(x)_{s S  T}(1-x_{s})=1]_{x,_{x}}[(S)=i \ |\ _{T}(x)_{s S T}(1-x_{s})=1].\]

Here, the third equality follows by the law of total probabilities, partitioning on the possible values of bits \(\{x_{i}\}_{i S}\). Since it always holds \(x_{n}=1\), we assume without loss of generality that either \(n T\) or \(n S\). Given that \(_{x}\) is uniform, we have:

\[_{x,_{x}}[(S)=i\ |\ _{T}(x)_{s S  T}(1-x_{s})=1]=&\\ 0&\\ 0&\\ &\]Therefore, thanks to the conditioning, this first probability does not depend on \(A\) (the probability is the same if we sample \(x\) from \(\) and then \(\) from \(_{x}\)). Moreover:

\[_{x}[_{T}(x)_{s S T}(1-x_{s })=1] =*{E}_{x}[_{T}(x)_{s S  T}(1-x_{s})]=*{E}_{x}[_{P  S T}(-1)^{|P|}_{T P}(x)]\] \[=_{P S T}(-1)^{|P|} A_{T P \{n\}}(n).\]

Since \(A_{T P\{n\}}(n)=B_{T P\{n\}}(n)\), we have \(A_{S}(i)=B_{S}(i)\) for any \(i S[n]\), \(|S| k\). 

### Lower bound when only slates of size \(k\) are given

We know that accessing slates of size \(2,,k=()\) is sufficient to approximate the full slate. It is natural to wonder if the slates smaller than \(k\) are needed, or if those of size exactly \(k\) are enough. A simple observation shows that accessing smaller slates is necessary.

**Observation 13**.: _For any \(k n\), there exists a RUM \(R\) on \([n]\) such that: (i) \(R_{S}(s)=1/k\) for all \(s S\), and (ii) \(R_{[n]}(i)=1/k\) for all \(i[k]\)_

Proof.: Consider the RUM \(R\) that first samples a uniform at random element \(i[k]\), this element is placed at the top of the permutation, followed by elements \([n][k]\) permuted uniformly at random, and finally by the elements \([k]\{i\}\) permuted uniformly at random. Consider any \(s S\). If \(s[k]\), then \(R_{S}(s)=1/k\). If instead \(s[n][k]\), let \(=|S[k]|<k\), we have, \(R_{S}(s)=(1-/k) 1/(k-)=1/k\). Moreover, for any \(i[k]\), by construction, \(R_{[n]}(i)=1/k\). 

Consider \(k= n\), for any \((0,1)\), and let \(R\) be the RUM of the previous construction. Consider now RUM \(Q\) that samples a uniform at random permutation over \([n]\). Clearly, \(R\) and \(Q\) coincide on slates of size \(k\), but for all \(i[k]\), \(R_{[n]}(i)=1/k\) and \(Q_{[n]}(i)=1/n\), and in particular the \(_{1}\)-distance between \(R_{[n]}\) and \(Q_{[n]}\) is \(k(1/k-1/n)+(n-k)/n=2-2\). Therefore, any algorithm accessing only slates of size \(k=O( n)\) will incur in an \(_{1}\)-error of \(1-\) on the full slate. By selecting \(k=O(n^{c})\), for any \(c(0,1)\), we have that any algorithm must incur an \(_{1}\)-error of \(1-n^{c-1}\) (and also an \(_{}\)-error of \((})\)) on the full slate. On the other hand, accessing all the slates of size \(O()\) and smaller, one can obtain an \(_{1}\)-error as small as \(1/n^{d}\) for any constant \(d>0\). Therefore, accessing smaller slates is necessary.

## 6 Fractional \(k\)-deck

The \(k\)-deck problem  is a well-studied problem at the intersection of combinatorics and computer science. Given a string \(s\{0,1\}^{n}\) and a set \(I=\{i_{1},,i_{k}\}\), with \(i_{1}<<i_{k}\), the projection of \(s\) to \(I\), denoted as \(s_{I}\), is the string \(s_{I}=s_{i_{1}} s_{i_{k}}\). The \(k\)-_deck_ of \(s\) is the multiset

\[D_{k}(s)=\{s_{I} I\}.\]

The \(k\)-deck problem asks for the smallest \(k=k(n)\) such that any \(n\)-bit string can be reconstructed from its \(k\)-deck. This problem has a long history.  originally showed that reconstruction is possible with \(k=\); they also showed that it is not possible with \(k=( n)\). Later, it was shown in  (see also ) that reconstruction is always possible with \(k=O()\); it is widely conjectured that this bound is tight. The best known lower bound, however, is no better than subpolynomial: in  it is shown that \(k=e^{()}\) is insufficient for reconstruction. In , lower bounds for the \(k\)-deck problem were used to obtain lower bounds on the maximum size of slates required for reconstructing a RUM.

In this section we define a fractional version of the \(k\)-deck problem and show a reconstruction lower bound. Note that the \(k\)-deck of a string \(s\) is a function \(f_{s}:\{0,1\}^{k}^{ 0}\), where \(f_{s}(s^{})\) is the multiplicity of \(s^{}\) in \(D_{k}(s)\). Now, given a probability distribution \(P\) over \(n\)-bit strings, the fractional \(k\)-deck_ of \(P\) is a function \(f_{P}:\{0,1\}^{k}^{ 0}\), where \(f_{P}(s^{})=_{s P}[f_{s}(s^{})]\). Also, for a distribution \(P\) over \(n\)-bit strings, we define its _\(i\)th marginal_ to be the probability that the \(i\)th bit of a string sampled from \(P\) equals \(1\). The fractional \(k\)-deck problem seeks the minimum \(k=k(n,)\) such that the fractional \(k\)-deck of an unknown distribution \(P\) over \(n\)-bit strings is sufficient to approximate any marginal of \(P\) to within an additive \(\). It is easy to see that the fractional \(k\)-deck problem generalizes the \(k\)-deck problem (by also setting \(<1/2\)).

Based on our RUM lower bound, we can construct two probability distributions over binary strings giving the same fractional \(()\)-deck, but very different marginals for the first bit, obtaining:

**Theorem 14**.: _For sufficiently large \(n\), and for each constant \(>0\), there exists a constant \(c>0\) and two distributions \(X_{A}\) and \(X_{B}\) over \(n\)-bits strings of weight \(1\) that (i) give rise to the same fractional \(k\)-deck, for \(k= c\), (ii) the marginal of the first bit of \(X_{A}\) is \( 1-\), and (iii) the marginal of the first bit of \(X_{B}\) is \(\)._

## 7 Fractional trace reconstruction

In the _trace reconstruction problem_, there is an unknown \(n\)-bit string \(x\) and a parameter \(d(0,1)\). A sample is obtained by passing \(x\) through a \(d\)-deletion channel that erases each bit of \(x\) independently with probability \(d\). In this setting, one asks for the minimum number of samples necessary to reconstruct \(x\). Indeed, with probability \((1-d)^{n}\), \(x\) itself is returned as a sample and hence reconstruction is trivial with \(((1-d)^{-n})\) samples. This problem has been the subject of intense study (Chase, 2021, 20, 20, 21, 24, 20, 28, 29). It can be solved with \(e^{O(})}\) samples in the so-called high deletion rate setting (\( d 1-(})\)), and this bound is tight for a special class of "mean-based" algorithms (De et al., 2017).

In this section we prove an unconditional lower bound for the fractional trace reconstruction problem, which we define similarly to the fractional \(k\)-deck problem: given a distribution over the \(n\)-bit strings, sample a string from that distribution, pass the string through a \(d\)-deletion channel, and return the resulting subsequence as a sample. Note that, each time, a new fresh string is sampled before passing it through the deletion channel. The goal is to reconstruct the marginals of the distribution.

Several variants of trace reconstruction have been studied in the literature (Chen et al., 2023; Davies et al., 2021). The variant closest to ours is perhaps the average-case trace reconstruction (Peres and Zhai, 2017). However, average-case trace reconstruction is a computationally easier version of the problem since the single hidden string is sampled from a uniform distribution. Our fractional trace reconstruction is instead a true generalization of trace reconstruction.

We will show that fractional trace reconstruction cannot be solved with fewer than \(e^{o()}\) samples. Our lower bound is obtained as a corollary of our result for the fractional \(k\)-deck problem.

**Theorem 15**.: _For each constant \(>0\), there exists a constant \(c>0\) and two distributions \(X_{A}\) and \(X_{B}\) over \(n\)-bits strings of weight \(1\) such that if \(d=1-}+\), then (i) with fewer than \(e^{o((1-d) n)}=e^{o()}\) samples, the probability of correctly distinguishing between \(X_{A}\) and \(X_{B}\) is at most \(+o(1)\), (ii) the marginal of the first bit of \(X_{A}\) is \( 1-\), and (iii) the marginal of the first bit of \(X_{B}\) is \(<\)._

In this very high deletion rate setting (\(d=1-(})\)), the magnitude of our lower bound for fractional trace reconstruction is not larger than that of (De et al., 2017) for trace reconstruction; our lower bound, though, holds for any reconstruction algorithm, not just for mean-based ones.

## 8 Conclusions

We considered the problem of learning a RUM by only looking at slates of size at most \(k\). We showed that to obtain a uniform approximation of the winning distributions, \(k=()\) is necessary and sufficient. Moreover, we provided two explicit algorithms that learn the RUM. While optimal with respect to the slate-size, both our algorithms require time exponential in \(n\): we leave open the problem of finding algorithms with better running times.

We also provided a third algorithm that can approximate any given slate by accessing only polynomially many subslates of size at most \((n/ n)\). In this setting, we leave open the problem of decreasing the slate size, while maintaining a polynomial running time.

Another interesting research direction would be considering a PAC-learning variant of the problem, where the slates of the testing phase and/or the training phase come from a probability distribution and the goal is to minimize the expected error in the testing phase. This variant might be easier with respect to both slate size and computational complexity.