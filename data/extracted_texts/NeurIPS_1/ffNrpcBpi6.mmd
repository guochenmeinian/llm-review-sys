# Graph Convolutions Enrich the Self-Attention in Transformers!

Jeongwhan Choi

Yonsei University

jeongwhan.choi@yonsei.ac.kr

Equal contribution.

Hyowon Wi

KAIST

hyowon.wi@kaist.ac.kr

Jayoung Kim

KAIST

jayoung.kim@kaist.ac.kr

Yehjin Shin

KAIST

yehjin.shin@kaist.ac.kr

Kookjin Lee

Arizona State University

kookjin.lee@asu.edu

Nathaniel Trask

University of Pennsylvania

ntrask@seas.upenn.edu

Noseong Park

KAIST

noseong@kaist.ac.kr

Corresponding author.

The source code of GFSA is available at: [https://github.com/jeongwhanchoi/GFSA](https://github.com/jeongwhanchoi/GFSA).

###### Abstract

Transformers, renowned for their self-attention mechanism, have achieved state-of-the-art performance across various tasks in natural language processing, computer vision, time-series modeling, etc. However, one of the challenges with deep Transformer models is the oversmoothing problem, where representations across layers converge to indistinguishable values, leading to significant performance degradation. We interpret the original self-attention as a simple graph filter and redesign it from a graph signal processing (GSP) perspective. We propose a graph-filter-based self-attention (GFSA)1 to learn a general yet effective one, whose complexity, however, is slightly larger than that of the original self-attention mechanism. We demonstrate that GFSA improves the performance of Transformers in various fields, including computer vision, natural language processing, graph-level tasks, speech recognition, and code classification.

## 1 Introduction

Transformers are arguably one of the best feats in the field of deep learning. They are now showing state-of-the-art performance in various fields, ranging from computer vision to natural language processing, prediction tasks on graphs, speech recognition, and so forth . Recently, there have been several studies conducted on better understanding them ; there exists a common agreement among researchers that the self-attention is one of the keys leading to the success.

Figure 1: Performance improvements (%) of our GFSA when integrated with different Transformer backbones in various domains. We achieve these results with only tens to hundreds of additional parameters to Transformers.

However, there also exist several studies pointing out potential limitations of the self-attention . For instance, Shi et al.  revealed an analogy between the self-attention and the residual graph convolutional network (GCN), showing that BERT also suffers from a notorious problem of GCNs, called _oversmoothing_, i.e., tokens' latent representations become similar to each other at the deeper layers. In every self-attention layer, value vectors are aggregated in a weighted average manner since each row-wise sum of the attention matrix is always 1. Although each self-attention layer has its own attention matrix, this aggregation method causes the oversmoothing problem, not only in Transformers but also in graph neural networks . However, we confine our discussion to the oversmoothing of Transformers (see Section 2).

Being inspired by them, we redesign the self-attention from the perspective of graph signal processing (GSP) -- in particular, we resort to GSP on directed graphs since the attention matrix is asymmetric. However, performing graph convolutions in the self-attention layer may incur non-trivial computational overheads. Therefore, our key design point is to learn a general but effective graph filter with minimal overhead. In general, a graph filter on a graph \(\) is represented by a polynomial expression based on its adjacency or Laplacian matrix -- in this regard, the existing self-attention mechanism can be understood as the simplest graph filter with \(}\) only, where \(}^{n n}\) means a learned attention matrix and \(n\) is the number of input tokens.

Our proposed graph filter consists of an identity term and two matrix polynomial terms, \(}\) and \(}^{K}\). One can design better filters with more polynomial terms, but we avoid it since Transformers already require very large computation. The \(K\)-th power, \(}^{K}\), may also require a high computation when the number of tokens is large. To avoid this, we further approximate \(}^{K}\) using the element-wise first-order Taylor approximation. Therefore, one can consider that our proposed graph filter is the very next complicated filter after the one used by the original self-attention mechanism. However, its efficacy is tremendous in various fields (cf. Fig. 1).

Our proposed filter enriches the self-attention with more diverse frequency information (see Fig. 2(a)) -- low (resp. high) frequency signals on \(\) mean neighboring nodes have similar (resp. different) values. Therefore, our method is able to not only effectively address the oversmoothing problem but also learn better latent representations for downstream tasks.

There exist a couple of prior works to enrich the self-attention mechanism with high frequency information . In comparison with them, our proposed graph filter is distinctive in the following aspects: i) our proposed filter is more effective and shows better performance with comparable computational overheads, ii) our proposed filter is well-aligned with recent advancements in the GCN community -- in other words, some graph filters used by recent advanced GCN methods are special cases of our proposed graph filter, which is not the case for prior works, and iii) other methods were typically studied for certain domains only whereas we test our method in 6 domains -- for instance, DiversePatch  works only for Vision Transformers (ViTs).

We replace the self-attention layer of selected Transformers in various fields with our proposed graph filter-based layer without changing other parts. Therefore, the accuracy increases in them are solely by our proposed graph filter-based self-attention. These enriched Transformers increase the model performance by 1.63% for image classification, 6.25% for natural language understanding, 0.31% for

Figure 2: Filter frequency response, cosine similarity, and singular values on ImageNet-1k for DeiT-S and DeiT-S + GFSA. Details and more visualizations are in Appendices C and D.

causal language modeling, 4.03% for graph regression, 4.76% for speech recognition, and 2.40% for code classification (see Fig. 1). Our core contributions are as follows:

* We provide a novel perspective on self-attention as a graph filter. This perspective allows us to design more effective self-attention that can address the oversmoothing problem.
* We propose a graph filter-based self-attention (GFSA) mechanism, integrating an identity term and two polynomial terms for general yet effective than the simple self-attention mechanism (Section 3).
* We demonstrate that GFSA improves the performance of Transformers on a variety of tasks. GFSA achieves improved results on natural language processing, computer vision, speech recognition, graph-level tasks, and code classification (Sections 5.1 to 5.6).
* We devise a strategy to selectively apply GFSA to even-numbered layers, effectively mitigating the computational overhead while preserving GFSA's performance (Section 6).

## 2 Background & Related Work

### Self-Attention in Transformers

The core building block of the Transformer architecture is the self-attention mechanism, which enables the model to learn attention patterns over its input tokens . The self-attention mechanism, denoted as \(:^{n d}^{n d}\), can be expressed as follows:

\[()=_{}( _{})^{}}{}_{}= }_{}, \]

where \(^{n d}\) is the input feature matrix, \(_{}^{d d}\), \(_{}^{d d}\), and \(_{}^{d d}\) are the key, query, and value trainable parameters, respectively, and \(d\) is the dimension of each token. The self-attention mechanism allows the model to weigh the importance of each token in the input sequence relative to the others, enabling the model to capture long-range contextual information better. The Transformer architecture includes multiple layers, each with a multi-head self-attention layer followed by a position-wise feed-forward layer.

### Self-Attention and Graph Convolutional Filter

The self-attention matrix used in Transformers has the form of symmetrically normalized adjacency matrix where each token become a node  -- the symmetrically normalized adjacency matrix is a special case of asymmetric (or directed) adjacency matrix where each row is normalized and is frequently used for the graph signal processing (GSP) on directed graphs . A weighted graph \(\) with adjacency matrix \(\) can be constructed by using the input tokens as \(n\) nodes and the edge weights between node \(i\) and node \(j\) as \(((_{})_{i}^{}(_{})_{j})\). We can rewrite the self-attention matrix \(}_{ij}\) as \(_{})_{i}^{}(_{})_{j})}{_{k=1}^{d}(_{})_{i}^{}(_{})_{k}}\). This allows \(}\) to be interpreted as the symmetrically normalized adjacency matrix. In other words, \(}=^{-1}\), where \(=(d_{1},d_{2},,d_{n})\) and \(d_{i}=_{j}_{i,j}\).

Our new attention method is designed on top of GSP which has a close connection to discrete signal processing (DSP) . In DSP, a discrete signal with a length of \(n\) can be represented by a vector \(^{n}\). Let \(^{n}\) be a filter that we want to apply to \(\). The convolution \(*\) can be written as follows:

\[_{i}=_{j=1}^{n}_{j}_{i-j}, \]

where the index, denoted as \(i\), refers to the \(i\)-th element in each vector.

GSP can be understood as a generalized concept of DSP. Signals are defined on the nodes of a graph, and the graph's structure influences signal processing operations. In addition, the linear and shift-invariant graph convolution filter \(\) with \(n\) nodes can be written with a shift operator \(\) as follows -- \(\) can be from a directed graph :

\[==_{k=0}^{K}w_{k}^{k}, \]

where \(^{n}\) is a 1-dimensional graph signal, \(K\) is the maximum order of polynomial, and \(w_{k}[-,]\) is a coefficient. \(\) is an \(n n\) matrix where \((i,j)\)-th element is non-zero if and only if there is an edge from node \(i\) to \(j\). Two representative samples of \(\) are adjacency and Laplacian matrices. The graph filter \(\) is the same as \(_{k=0}^{K}w_{k}^{k}\) with a large enough value of \(K\), which is called _matrix polynomial_. We note that this graph filtering operation can be extended to \(d\)-dimensional cases as in Eq. (1). Being inspired by Zou et al.  and Maskey et al. , we rely on the singular value domain analysis to understand the low/high-pass characteristics of filters on directed graphs (cf. Fig. 2). See more discussion in Appendices E and F.

In the context of the self-attention within Transformers, the core part of the self-attention in Eq. (1), i.e., \(}\), can be considered as a \(d\)-dimensional graph filter with \(}\) only, where \(=}\). Our goal in this paper is to design a simple (for computational purposes) yet effective form of \(\).

### Oversmoothing in GCNs and Transformers

Oversmoothing is a phenomenon observed in deep learning models, especially in GCNs [39; 78]. As information is aggregated over multiple layers for multiple nodes (tokens), latent representations tend to become similar to each other, leading to a loss of distinctiveness in the representations [54; 104; 66].

Surprisingly, an oversmoothing-like phenomenon is also observed in Transformers [80; 71]. Unlike CNNs, Transformers can not benefit from simply deepening layers after a certain depth. Earlier studies hypothesize that this may be due to issues such as the attention or feature collapse or due to uniformity among patches or tokens [101; 25; 92]. Dong et al.  also point out that the output of a pure Transformer, i.e., an attention mechanism without skip connections or MLPs, tends to converge to a rank-1 matrix . This analysis is followed by , which suggests that rank collapses incur vanishing gradients of attention queries and keys.

In this context, the self-attention acts as a low-pass filter, since the self-attention calculates the weighted average of the value vectors of tokens. Wang et al. [80, Theorem 1] also reveal that the self-attention is a low-pass filter, continuously reducing high-frequency information. This nature contributes to the oversmoothing phenomenon as unique high-frequency features are lost in deeper layers of the network, further worsening the uniformity of token representations. Therefore, we extend the term "oversmoothing" to describe the degeneration challenge observed in Transformers.

There have been proposed many empirical countermeasures for ViT, such as patch diversification [102; 25], rank collapse alleviation [101; 99], and training stabilization [74; 98]. Similar alleviating methods have been also proposed in the field of NLP, such as unsupervised learning , and resolve the oversmoothing and the token uniformity (or information diffusion) problems [18; 92]. There are studies on utilizing high frequency information via frequency domain analyses [80; 4], but they are not designed on top of graph filtering perspectives. Dovon et al.  find that Transformers are not inherently low-pass filters, but oversmoothing depends on the eigenspectrum of the self-attention layers. They propose a reparametrization of the Transformer weights, ensuring that oversmoothing does not occur.

Our paper addresses the oversmoothing problem with graph filters since the self-attention mechanism is a basic graph filtering operation as seen in the previous subsection.

## 3 Graph Filter-based Self-Attention Layers

Let \(}^{n n}\), where \(n\) is the number of tokens in the input to the self-attention layer, be a self-attention matrix. Since Transformers use multi-head self-attentions, there are multiple such matrices. For simplicity, but without loss of generality, we discuss only one head in one layer.

From the GSP perspective, using \(}\) as the shift operator, a graph filter can be represented as a matrix polynomial filter, as mentioned in Section 2.2. We aim to design this matrix polynomial filter using the two lowest-order terms and one high-order term in Eq. (3). The following theorem shows that,despite using the three terms, the filter can be either a low-pass filter or a high-pass filter, depending on the coefficient values.

**Theorem 3.1** (Filter characteristics based on coefficient values).: _Let \(}\) be a self-attention matrix interpreted as a graph with connected components. Consider the polynomial graph filter defined by \(_{k=0}^{K}w_{k}}^{k}\), where \(w_{2},w_{3},,w_{K-1}=0\) and only \(w_{0}\), \(w_{1}\), and \(w_{K}\) are non-zero. If the coefficients \(w_{k}\) for \(k=0,1,K\) are positive and their sum is 1, then the polynomial filter acts as a low-pass filter, attenuating high-frequency components and promoting smoothness across the graph. Conversely, if \(w_{k}=(-)^{k}\) for \(k=0,1,K\) and \((0,1)\) with sufficient large \(K\), the polynomial filter exhibits high-pass filter behavior._

The proof of Theorem 3.1 is in Appendix G. Based on Theorem 3.1, we propose to use the following graph filter, \(_{}\), where the two lowest-order terms and one high-order term of the matrix polynomial are used:

\[_{}=w_{0}+w_{1}}+w_{K}}^{K}, \]

where \(w_{0}\), \(w_{1}\), \(w_{K}\) are coefficients and \(K\) is a hyper-parameter where \(K 2\). The coefficients can be learnable weights and we learn them with gradient descent algorithms.

Approximation of the high-order term.In Eq. (4), it is costly to calculate \(}^{K}\) when \(K\) is large, so we need a way to approximate the high-order term \(}^{K}\) in GFSA. We use the first-order Taylor approximation at point \(a=1\) for this purpose:

\[f(x) f(a)+f^{}(a)(x-a), \]

thus, we approximate \(f(K)=}^{K}\) as follows:

\[f(K)=}^{K} f(1)+f^{}(1)(K-1). \]

Computing the derivative of \(}^{K}\) directly at the evaluation point requires high computational costs. To overcome this problem, we adopt the forward finite difference method, which approximates derivatives with the difference term:

\[f^{}(K)==^{K+h}-^{K}}{h}, \]

where the approximation error is \((h^{2})\). To balance the trade-off between computational efficiency2 and accuracy, we set \(h=1\). This method is inspired by the approach in Brouwer et al. , which uses the difference term between two consecutive hidden states in discrete time to approximate the derivatives. Therefore, we approximate \(}^{K}\) as:

\[f(K)=}^{K}  f(1)+(}^{2}-})(K-1) \] \[=}+(K-1)(}^{2}-}).\]

The approximation for \(}^{K}\) with \(}\) and \(}^{2}\) provides a simpler computation that can significantly reduce the required computational resources and time.

GFSA: our graph filter-based self-attention.Our proposed graph filter-based self-attention (GFSA) is defined with the graph filter \(}_{}\) as follows:

\[() :=}_{}_{}, \] \[}_{} =w_{0}+w_{1}}+w_{K}(}+(K-1)(}^{2}-})), \]

where the last term is the approximated \(}^{K}\) from Eq. (8). We replace the original self-attention layer in various Transformers with the proposed graph filter-based layer without changing other parts. Therefore, GFSA can be plugged into any Transformers that rely on the self-attention. For pseudocode, see Appendix I.

## 4 Properties of GFSA

This section analyzes the theoretical error of the \(}^{K}\) approximation used by GFSA and how GFSA can mitigate oversmoothing. We also explain the meaning of GFSA's high-order term in the context of Transformers and provide comparisons of GFSA in other models.

Theoretical characteristics of approximation error in GFSA.We provide a theorem that provides an upper bound on the error introduced by approximating the power of a matrix, specifically using the first-order Taylor expansion. The following theorem specifically analyzes the error of matrix \(}^{K}\) when approximated using a first-order Taylor expansion.

**Theorem 4.1** (Error bound for approximated high-order term in GFSA).: _Define the error term, \(E_{K}\), as the difference between the exact value and approximated value of \(}^{K}\), which is given by \(E_{K}=\|}^{K}-(}+(K-1)(}^{2}-}))\|_ {F}\), where \(\|\|_{F}\) denotes the Frobenius norm. Then, the error bound can be shown that \(E_{K} 2\)._

The error bound provides an upper limit for the difference between the actual value of \(}^{K}\) and its approximation. The proof of Theorem 4.1 is in Appendix H. It shows theoretical validity for using approximations to the high-order term in the filters of our GFSA. In terms of performance, we report a difference of approximately \(^{K}\) between the actual calculated values in Appendix J.

How to alleviate the oversmoothing problem?The key leading to the low/high pass filtering behavior of our proposed filter is the coefficients \(\{w_{0},w_{1},w_{K}\}\) -- note that in the self-attention of Transformers, \(w_{0}=w_{K}=0\) and \(w_{1}=1\). Since our method can learn any appropriate values for them for a downstream task, it can be reduced to low-pass-only, high-pass-only, or combined filters. According to Theorem 3.1, our graph polynomial filter can be said to be a low-pass filter when \(w_{1},w_{K}\) are positive and a high-pass filter when they are negative. Therefore, our method can learn the appropriate coefficients \(\{w_{0},w_{1},w_{K}\}\) for downstream tasks, so it can be reduced to a low-pass-only, high-pass-only, or combined filter, alleviating the oversmoothing problem of self-attention.

The meaning of the high-order term in GFSA in the context of Transformers.Existing self-attention only captures simple pairwise similarities between tokens and is limited in capturing high-order dependencies. For example, given the two sentences, "Books are more expensive than pencils" and "Books are cheaper than computers", to understand the relationship between "computers" and "pencils", we need to capture the high-order dependencies connected through the "Book" token. However, it is difficult to capture these high-order dependencies with traditional self-attention . Therefore, from a Transformer perspective, the approximated \(}^{K}\) in GFSA can be interpreted as being able to capture these high-order dependencies.

Comparison to Transformers.In the field of computer vision, there has been recent research on adjusting the frequency response of ViT. HAT  creates adversarial examples by altering clean images with high-frequency perturbations and jointly trains the ViT on clean images and adversarial examples. Through this, they aim to solve the problem of the ViT being unable to capture high-frequency by allowing us to capture the high-frequency components of the images. However, HAT has the disadvantage of requiring more epochs than the existing ViT, as it must perform adversarial training in some initial epochs and train normally in the remaining epochs. Wang et al.  use the concept of DSP, which is a special case of GSP, to isolate the lowest frequency component in the Fourier domain and use a filter learned by rescaling the low and high-frequency components. On the other hand, our GFSA extends the concept to graph signal processing and redesigns self-attention as a graph filter. While GFSA seeks to design a better graph filter by interpreting self-attention as a graph filter, Shi et al.  are inspired by JKNet , and they solve the oversmoothing problem by fusing the hidden vectors of each layer. However, their method has a limitation with memory increasing, and they only applied it to BERT.

Comparison to GCNs.Comparisons to GCNs that can be interpreted as graph filters [39; 15; 24] are inevitable. GFSA without a high-order term is analogous to ChebNet  with \(K=1\). In addition, GFSA reduces to the vanilla GCN  when \(K=1\), \(w_{0}=0\), \(w_{1}=1\). GPR-GNN , which approximates graph convolutions using the monomial basis, is identical to GFSA if it only considers up to first order and additionally uses a \(K\)-order term and learns the coefficients. When we

[MISSING_PAGE_FAIL:7]

### Experiments on Vision Transformers

Setting.We aim to demonstrate the efficacy of our GFSA across a spectrum of ViT backbones. We choose DeiT , CaiT , and Swin  as the backbone, and the models are trained from scratch. When training the 12-layer DeiT, we follow the same training recipe, hyperparameters, and data augmentation from Touvron et al. . For detailed experimental settings, see Appendix M.1.

Results.The experimental evaluations are summarized in Table 3. We compare various models on the ImageNet-1k benchmark. The results show that the proposed GFSA successfully enhances DeiT, CaiT, and Swin across all depth settings and training methods. GFSA provides additional parameters less than 72 for 12-layer DeiT while improving top-1 accuracy by 1.63%. To sum up, we observed that both shallow and deep ViTs can achieve the following benefits from GFSA: i) The filter response shows GFSA can preserve higher-frequency representation (cf. Fig. 2 (a)) and ii) Fig. 2 (b) shows that GFSA mitigates the increase in the cosine similarity of representation as the layer gets deeper. We further compare with state-of-the-art models that use Fourier transforms rather than graph filters in Appendix M.5. We also show results under the same settings as ContraNorm  in Appendix M.6.

### Experiments on Graph-level Tasks

Setting.To evaluate the efficacy of GFSA on graph-level tasks, we conduct experiments on a broader range of datasets. We use datasets from Long-Range Graph Benchmark (LRGB)  (e.g., Peptide-func and Peptide-struct), Benchmarking GNNs  (e.g., ZINC, MNIST, CIFAR10), Open Graph Benchmark (OGB) dataset  (e.g., Molhiv and MoITox21), and OGB-LSC dataset (i.e., PCQM4M-LSC) . We choose Graphormer , Graph-ViT , and GPS  as our backbone architectures, following their original experimental protocols for fair comparison. For GPS, we replace its self-attention module with our GFSA while maintaining its best configuration and other hyperparameters. For Graph-ViT, we apply GFSA to the Hadamard self-attention method, which He et al.  propose as optimal. For a detailed experimental setting, see Appendix O.1.

Results.Tables 4, 5, and 6 show consistent performance improvements when GFSA is integrated with backbone architectures. Graph-ViT + GFSA shows improvements on all datasets. On Peptide-func, it achieves a 0.65% increase in AP. Notably, in PCQM4M, incorporating GFSA improves the validation MAE by 7.20%. Due to space constraints, the results with standard deviation are included in Appendix O.2.

### Experiments on Automatic Speech Recognition

Setting.We conduct automatic speech recognition (ASR) experiments on the LibriSpeech 3 dataset , which consists of audio recordings paired with their transcriptions. We use Branch

   Category & Method & Input Size & \#Layers & \#Params & Top-1 Acc \\   & DeiT-S  & 224 & 12 & 22M & 79.8 \\  & DeiT-S + AttnScale  & 224 & 12 & 22M & 80.7 \\  & DeiT-S + FeatScale  & 224 & 12 & 22M & 80.9 \\  & DeiT-S + ContraNorm  & 224 & 12 & 22M & 80.4 \\  & Swin-S  & 224 & 12 & 50M & 82.9 \\   & DeiT-S  & 224 & 24 & 43M & 80.5 \\  & CaiT-S  & 224 & 24 & 47M & 82.6 \\  & DeiT-S + AttnScale  & 224 & 24 & 44M & 81.1 \\  & DeiT-S + FeatScale  & 224 & 24 & 44M & 81.3 \\  & DeiT-S + ContraNorm  & 224 & 24 & 43M & 80.7 \\   & DeiT-S + GFSA & 224 & 12 & 22M & **81.1** \\  & DeiT-S + GFSA & 224 & 24 & 43M & **81.5** \\   & CaiT-S + GFSA & 224 & 24 & 47M & **82.8** \\   & Swin-S + GFSA & 224 & 12 & 50M & **83.0** \\   

Table 3: Results comparison on ImageNet-1k. Our full results with other models are in Appendix M.4.

former  and a pure Transformer. For implementation, we follow the recipes of SpeechBrain  and the detailed settings are in Appendix N.1.

Results.Table 7 compares word error rates (WERs) on LibriSpeech 100h and 960h. For 100h, Transformer+GFSA achieves 10.30/25.30 on the test clean/other set, which is a 6.53% improvement over the Transformer for the WER of the test clean. For 960h, Transformer+GFSA shows a WER result of 2.31 in test clean, a 4.55% improvement over Transformer and Branchformer+GFSA achieves 2.31/5.49 with an LM on the test clean/other sets. Fig. 8 in Appendix N.2 depicts the learning curves of train loss and valid loss when using GFSA, showing the effectiveness of our proposed filter.

### Experiments on Code Classification

Setting.We conduct a code defect detection task based on Devign dataset provided by Zhou et al. . We use RoBERTa , CodeBERT , PLBART , and CodeT5  as our backbone models. The detailed settings are in Appendix P.1.

Results.Table 8 shows the accuracy of all models; GFSA results better than the base models. The biggest improvement is 2.40% for RoBERTa. In the case of CodeT5-base, using GFSA shows an accuracy of 64.75, an improvement of 1.95% from 63.51. CodeT5-small+GFSA has only about 100 additional parameters compared to CodeT5-small with 60M parameters, and even more impressively, it surpasses the accuracy of CodeT5-base. The biggest improvement is 2.40% for RoBERTa. In Appendix P.2, we include case studies for this task. We also report the results of the code clone detection task in Appendix Q.

    & Peptide-func & Peptide-struct & MNIST & CIFAR10 & Molhiv & MoTOX21 & ZINC \\   & AP (\(\)) & MAE (\(\)) & Accuracy (\(\)) & Accuracy (\(\)) & ROCAUC (\(\)) & ROCAUC (\(\)) & MAE (\(\)) \\  GPS & 0.6535\(\)0.0041 & 0.2500\(\)0.0005 & 0.9805\(\)0.0013 & 0.7230\(\)0.0036 & – & – & 0.070\(\)0.004 \\ + GFSA & **0.6593\(\)0.0041** & **0.2496\(\)0.0013** & **0.9814\(\)0.0014** & **0.7244\(\)0.0018** & – & – & **0.069\(\)0.002** \\  Graph-ViT & 0.6919\(\)0.0085 & \({}^{}\)0.2474\(\)0.0016 & 0.9820\(\)0.0005 & 0.6967\(\)0.0040 & 0.7792\(\)0.0149 & 0.7851\(\)0.0077 & 0.0849\(\)0.0047 \\ + GFSA & **0.6964\(\)0.0025** & **0.2461\(\)0.0024** & **0.9826\(\)0.0004** & **0.6987\(\)0.0028** & **0.7830\(\)0.0109** & **0.7895\(\)0.0069** & **0.0845\(\)0.0032** \\   

Table 6: Experimental evaluation of GFSA plugged into GPS and Graph-ViT. Results marked with \(\) indicate settings where we conducted our own experiments due to unavailable Hadamard self-attention performance in He et al. ’s paper.

   Method & Accuracy \\  RoBERTa & 62.88 \\ + GFSA & **64.39** (\(\) 2.40\%) \\  CodeBERT & 64.31 \\ + GFSA & **64.49** (\(\) 0.12\%) \\  PLBART & 62.63 \\ + GFSA & **62.96** (\(\) 0.52\%) \\  CodeT5-small & 63.25 \\ + GFSA & **63.69** (\(\) 0.70\%) \\  CodeT5-base & 63.51 \\ + GFSA & **64.75** (\(\) 1.95\%) \\   

Table 7: Results for ASR training on LibriSpeech 100h and 960h with GFSA

    &  &  &  &  &  &  \\   & & & & & Train (\(\)) & Validate (\(\)) & Train (\(\)) & Validate (\(\)) \\  Graphormer & 500K & 0.1240 \\ + GFSA & 500K & **0.1189** & & & & \\  Graphormer & 48.3M & 0.0535 & 0.1286 & 0.0250 & 0.0862 \\ + GFSA & 48.3M & **0.0312** & **0.1193** & **0.0249** & **0.0860** \\   

Table 5: Results on PCQM4M and PCQM4Mv2

## 6 Discussion on Runtime Overheads

Limitation.The introduction of our GFSA layer results in a slight increase in training and inference time. We report the runtimes when plugging GFSA in Appendices R and S. For GLUE benchmark, integrating GFSA into BERT enhances the average performance from 82.51% to 83.58% (see Table 1) with more overhead of less than 36 seconds per epoch based on average training time (see Table 23). Considering the improvements, the increases in training time are negligible.

GFSA in selected layers: a strategy to mitigate the limitation.As GFSA requires more calculation than the original self-attention, the runtime after using GFSA slightly increases. Our experiments initially applied GFSA across all Transformer layers (as discussed in Section 5); however, to reduce computational load, we propose a selective application strategy. For this purpose, GFSA is used only on even-numbered layers. In Tables 35 to 39 of Appendix T, the results show that this strategy effectively reduces runtime increases while preserving comparable performance to the full-layer GFSA integration. Notably, the selective application of GFSA cuts the per-epoch runtime increase by 26.90% relative to its full-layer application, with only a 7.39% increase in runtime per epoch compared to the backbone model in Table 39.

GFSA in linear Transformers.Although GFSA requires additional computation for calculating \(}^{2}\), we explore integrating GFSA with linear attention variants to maintain efficiency and scalability. Recent approaches [36; 70] achieve linear complexity by reformulating softmax operations and reordering matrix multiplication in self-attention. We apply similar principles to compute second-order self-attention efficiently, enabling \(}_{}\) calculation with linear complexity with respect to sequence length. Fig. 4 shows the performance, runtime and GPU usage changes when applying our GFSA to Transformers with linear complexity. GFSA still improves performance compared to the backbone model, while the increase in time and GPU usage is minimal. Notably, when GFSA is applied to Efficient Attention , the performance is improved and the runtime is 11.82 times faster than when GFSA is applied to the vanilla self-attention. This shows that GFSA can be effectively implemented with linear complexity architectures while preserving its benefits and providing a solution for addressing computational concerns.

## 7 Conclusion

Our proposed GFSA achieves high performance with improvements on a variety of tasks. GFSA is a simple yet effective method that enriches self-attention in Transformers with more diverse frequency information. This enables GFSA to address the oversmoothing problem and learn better latent representations for downstream tasks. However, our GFSA does not bring significant overheads in those Transformers' empirical runtime complexity. One can use more complicated graph filters to enhance accuracy more, but our goal is to find a balance between accuracy enhancements and overheads in runtime complexity.

We believe that GFSA suggests a promising new direction for improving Transformers. GFSA can be implement with simple way and used in conjunction with other techniques to further improve the performance of Transformers. Considering the ongoing advancements in large language models, such as GPT-4  and LLaMA , we hope that our approach may offer new insights for enhancing their performance and efficiency.

Figure 4: Performance (\(x\)-axis), runtime (\(y\)-axis), and GPU usage (circle sizes) of various Transformers and integrated GFSA on Long-Range benchmark

Figure 3: Effectiveness of our selective layer strategy on ImageNet-1k. This shows out strategy’s ability to maintain accuracy benefits while mitigating runtime increases.