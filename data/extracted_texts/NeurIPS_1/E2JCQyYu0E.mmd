# Donghwan Kim** **Tae-Kyun Kim

## Multi-hypotheses Conditioned Point Cloud Diffusion for 3D Human Reconstruction from Occluded ImagesKAIST

{kdoh2522, kimtakeyun}@kaist.ac.kr

3D human shape reconstruction under severe occlusion due to human-object or human-human interaction is a challenging problem. Parametric models i.e. SMPL(-X), which are based on the statistics across human shapes, can represent whole human body shapes but are limited to minimally-clothed human shapes. Implicit-function-based methods extract features from the parametric models to employ prior knowledge of human bodies and can capture geometric details such as clothing and hair. However, they often struggle to handle misaligned parametric models and inpaint occluded regions given a single RGB image. In this work, we propose a novel pipeline, MHCDiff, **M**ulti-**h**ypotheses **C**onditioned Point Cloud **Diff**usion, composed of point cloud diffusion conditioned on probabilistic distributions for pixel-aligned detailed 3D human reconstruction under occlusion. Compared to previous implicit-function-based methods, the point cloud diffusion model can capture the global consistent features to generate the occluded regions, and the denoising process corrects the misaligned SMPL meshes. The core of MHCDiff is extracting local features from multiple hypothesized SMPL(-X) meshes and aggregating the set of features to condition the diffusion model. In the experiments on CAPE and MultiHuman datasets, the proposed method outperforms various SOTA methods based on SMPL, implicit functions, point cloud diffusion, and their combined, under synthetic and real occlusions. Our code is publicly available at [https://donghwankim0101.github.io/projects/mhcdiff](https://donghwankim0101.github.io/projects/mhcdiff).

Figure 1: **Image to 3D shape.** From the segmented images, containing occlusion due to interaction, MHCDiff reconstructs 3D human shapes as point clouds.

Introduction

Realistic virtual humans play a significant role in various industries, such as metaverse, tele-presence, and game modeling. However, conventional methods require expensive artist efforts and complex scanning equipments, so they are not readily applicable. A more practical approach is to reconstruct high-fidelity 3D humans from 2D images taken in the wild. This is still an ongoing research task due to its challenges; people wear a wide variety of clothing styles and adopt diverse poses. Furthermore, human-object and human-human interaction, fundamental aspects of daily social life, make it more challenging due to severe occlusions.

Existing 3D human reconstruction methods cannot predict the _pixel-aligned_ 3D shapes of humans _robustly_ from occluded images. The parametric body models  have been widely used to reconstruct 3D human shapes. Several methods  predict the parameters of the statistical models and are robust to occlusion because they can be trained on large scale datasets  and parametric models are well regularized with human body priors. However, the parametric models lack geometric details like clothing and hair, so these approaches cannot align the results to the subjects with loose clothing. More recently, 3D clothed human reconstruction methods , which are based on implicit functions and integrate the human body prior from the 3D body models, _i.e_ SMPL , present pixel-aligned detail shapes. Despite the impressive advances of the previous methods, they are not robust to occlusion because (1) small misalignment of estimated parametric models ruins the final shapes, (2) the implicit function takes features independently and cannot inpaint the invisible regions with missing image features, and (3) datasets  usually consists of segmented full-body images.

To address the aforementioned limitations, we propose MHCDiff (Multi-hypotheses Conditioned Point Cloud Diffusion). (1) Several existing methods  predict multiple SMPL meshes to model uncertainty due to occlusions. The sampled distribution is also important prior knowledge of human motions, but none of the existing work utilizes the distribution for pixel-aligned 3D human reconstruction. We leverage the multi-hypotheses to be robust on the misalignment of each sample. (2) We adopt denoising diffusion probabilistic models (DDPMs)  to take global consistent features and generate the invisible regions. Diffusion based methods generate 3D shapes by denoising point clouds , latent , neural fields , 3D Gaussian  or meshes . We adopt the unstructured point clouds to project pixel-aligned image features at each diffusion step. (3) Additionally, we synthesize partial body images by random masking , augmenting the limited datasets.

Specifically, our goal is _pixel-aligned_ and _detailed_ 3D human reconstruction in a _robust_ manner to occlusion in images. Given a single occluded RGB image, we extract 2D features and generate multiple plausible SMPL hypotheses using an off-the-shelf method . The proposed method, MHCDiff, Multi-hypotheses Conditioned Point Cloud Diffusion, performs the diffusion process to denoise a randomly-sampled point cloud into a target human shape. To reconstruct a pixel-aligned 3D shape and leverage the human body prior, the diffusion process is conditioned on the projected image feature (Sec. 3) and local features extracted from SMPL (Sec. 4.2). The key of MHCDiff is a novel conditional diffusion process with multiple hypotheses (Sec. 4.3), which is not sensitive to misaligned SMPL estimation. Given global 2D features and the distribution of hypotheses, the denoising diffusion model can generate the occluded parts (Sec. 4.4)

We train MHCDiff on randomly masked THuman2.0 dataset . Our experiments on CAPE dataset  with synthesized occlusion and MultiHuman dataset  with real-world interaction demonstrate that MHCDiff reconstructs pixel-aligned 3D human shapes robustly to various occlusion ratios and achieves state-of-the-art performance. Our main contributions are as follows:

* We introduce a novel multi-hypotheses conditioning mechanism that effectively captures the distribution of multiple plausible SMPL meshes. It is robust to the noise of each SMPL estimation due to the occlusion of given images. To the best of our knowledge, MHCDiff is the first work that extends the multi-hypotheses SMPL estimation to pixel-aligned 3D human reconstruction.
* We adopt point cloud diffusion model to capture the global consistent features and inpaint the invisible parts. Unlike the previous implicit function, the misaligned SMPL estimation can be corrected during the denoising process. The point cloud diffusion model also offers detailed human meshes.

* MHCDiff, trained on synthesized partial body images, outperforms previous methods on occluded and even full-body images.

## 2 Related Work

### Diffusion models for point clouds

Over the past years, denoising diffusion probabilistic models (DDPMs)  have been applied to point clouds. For unconditional generation, Luo et al. , Zhou et al.  and LION  use PointNet , Point-Voxel-CNN  and latent space, respectively. PointInfinity  tackles the quadratic complexity of transformer , and generates high-resolution point clouds with a fixed-size latent vector. Otherwise, Point-E  is a text-conditioned generation model using CLIP  and PDR  is a point cloud completion method from partial point clouds. PC\({}^{2}\), which is the baseline of MHCDiff, reconstructs the point cloud conditioned on projected image features (please refer to Sec. 3 for more details).

### Explicit-shape-based human reconstruction

Parametric models [27; 48; 67; 93; 75] have been primary representations for 3D human reconstruction. Due to the strength that they capture the statistics across a large corpus of human shapes, a lot of work [11; 28; 34; 83; 26; 15; 10; 44; 82] reconstructs 3D body meshes from an RGB image. To reduce the gaps between the image and parameter space of the statistical models and improve image alignment, they propose intermediate representations or additional supervisions, such as semantic segmentation [64; 94; 33; 100] and keypoints [9; 41]. To model the uncertainty due to occlusions or depth ambiguities, some work proposes multi-hypotheses , heatmaps , probability density functions [73; 78; 79; 35] or diffusion models [8; 61; 102; 81; 39]. ProPose  adopts the matrix Fisher distribution [13; 30] over \((3)\) for the joint rotation conditioned on the von Mises-Fisher distribution  for the unit directions of bones, which is not only mathematically correct but also learning friendly (please refer to Sec. 3 for more details). However, these methods are limited to recovering minimally-clothed humans and lack the ability to capture geometric details such as clothing and hair.

Several works aim at modeling geometric details in explicit shapes such as meshes, voxels, depth maps and point clouds. Mesh-based methods [1; 2; 3; 37; 109; 5; 25] model 3D offsets on the vertices of SMPL , but they do not generalize on loose clothing such as skirts and dresses. Voxel-based methods [24; 88; 17; 86] reconstruct 3D human shapes in fine-grained voxel representations. However, free-form 3D reconstruction is challenging without prior, and they need high computation costs to output high-resolution 3D shapes. Point-cloud-based methods [52; 99; 54; 19; 85] model point clouds of clothing humans. Han et al.  estimate depth maps based on different body parts, and convert the depth maps into point clouds. Tang et al. , the most related work, reconstruct 3D humans with point cloud diffusion from an RGB image. First, they convert the estimated SMPL mesh and depth map from the RGB image to point clouds. Conditioned on this point cloud, the conditional diffusion model refines the point cloud. However, they only handle complete images without occlusion and are not robust to misaligned SMPL estimation.

### Implicit-function-based human reconstruction

Implicit-function-based methods regress occupancy fields  or signed distance fields (SDF)  utilizing Multi-Layer Perceptron (MLP) decoders as implicit functions (IF). PIFu  and PIFuHD , which are pioneering works, extract pixel-aligned image features for clothed 3D human reconstruction. Later works [106; 92; 7; 91; 90; 40; 95; 103; 104; 96] leverage parametric models or body keypoints as prior information on the human body. They extract global features from voxelized SMPL meshes with a 3D encoder [106; 90] or local features such as signed distances and normals from SMPL meshes [92; 95; 103; 104] or both [7; 96]. The use of global features helps regularize global shapes and ensure consistency and local features help reconstruct local details. However, the global encoder is sensitive to global pose changes of SMPL and decreases the performance given misaligned SMPL estimation due to occlusion. The local features do not contain the global consistent features and cannot inpaint the occluded parts. Wang et al.  aim to reconstruct complete 3Dshapes from occluded images by primarily using the generative global encoder with a discriminator, but only assuming the accurate SMPL meshes.

## 3 Preliminary

**PC\({}^{2}\).** The projection-conditioned point cloud diffusion model is proposed for single-view 3D shape reconstruction. Denoising diffusion probabilistic model , which is the foundation of this framework, learns to recurrently transform noise \(X_{T}(0,)\) into a sample from the target data distribution \(X_{0} q(X_{0})\) over a series of steps. In order to learn this denoising process, a neural network is trained \(_{}(X_{t-1}|X_{t}) q(X_{t-1}|X_{t})\). To reconstruct geometrically consistent 3D point clouds from single RGB images \(I^{H W 3}\), 2D feature map \((I)^{h w c}\) is projected onto the partially denoised points at each step in the diffusion process. Therefore, \(_{}():^{(3+c)N}^{3N}\) is a function that predicts the noise \(^{3N}\) from the point cloud \(X_{t}^{3N}\) and the projected features \(X_{t}^{proj}^{cN}\), where \(c\) is the number of feature channels.

**ProPose .** Recovering accurate body meshes and 3D joint rotations from single images remains a challenging problem, particularly in cases of severe occlusion, including self-occlusion and occlusion from other subjects or objects. ProPose  addresses this limitation by modeling the probability distributions for human mesh recovery. Since the pose parameters \(^{72}\) of SMPL  represent the 3D rotation of each joint and the root orientation, they adopt the matrix Fisher distribution [13; 30] over \((3)\). Due to the gaps between the RGB images and the rotation representations, the neural network cannot easily model the distribution. ProPose  also introduces 3D unit vectors for bone directions as the corresponding observation on the previous matrix Fisher distribution as the prior. Leveraging Bayesian inference, they model the posterior distribution of the joint rotations from the prior distribution and observation.

## 4 MHCDiff: Multi-hypotheses Conditioned Point Cloud Diffusion

### Overview

Our work aims at reconstructing pixel-aligned 3D human shape as a point cloud given a single occluded RGB image via conditional point cloud diffusion, as shown in Fig. 2. Formally, the diffusion model \(_{}()\) learns the conditional distribution \(q(X_{0}|I)\) of 3D human shapes given the RGB images \(I^{H W 3}\). Following PC\({}^{2}\), we extract the 2D feature map \((I)^{h w c}\) using ViT , to capture the details in the images. The image features are projected onto the partially denoised points: \(X_{t}^{proj}=((I),X_{t})\), where \(\) is the projection function. This helps obtain pixel-aligned detailed body shapes. Additionally, the diffusion model is conditioned on the local features \(X_{t}^{SMPL}\) from SMPL mesh \(S\) to exploit statistical human body priors to complete 3D shapes from occluded body parts (Sec. 4.2). However, the SMPL estimation from single occluded RGB images has a high probability of large errors. To tackle this, we propose a novel multi-hypotheses conditioned diffusion model that considers the distribution of multiple plausible SMPL meshes \(\{S_{i}\}_{i\{1,...,s\}}\) (Sec. 4.3). Given the partially denoised point cloud \(X_{t}\), the projected image features \(X_{t}^{proj}\), and the local features from SMPL \(X_{t}^{SMPL}\), MHCDiff predicts the noise \(\):

\[_{}(X_{t},X_{t}^{proj},X_{t}^{SMPL})=. \]

We also discuss how MHCDiff takes the generative property and the global consistent features to reconstruct occluded parts (Sec. 4.4).

### Local features from SMPL

Given the SMPL (or SMPL-X) mesh \(S\) and the partially denoised point cloud \(X_{t}\) at \(t\)-th diffusion step, we extract the local features \(X_{t}^{SMPL}\) as:

\[X_{t}^{SMPL}=[(d(X_{t}|S)),(X_{t}|S)], \]where \(d():^{3}\) and \(():^{3}^{3}\) are the signed distance and normal obtained from the closest surface of SMPL mesh respectively. In order to map scalar values to a higher dimensional space, we adopt an encoding inspired by the positional encoding in NeRF :

\[(d)=((2^{0} d),(2^{0} d),...,(s^{L-1} d),(s^{L-1 } d)). \]

The local features \(X_{t}^{SMPL}^{(2L+3)N}\), which contain the signed distance and normal vector from SMPL, are used to predict the noise \(\) of the point cloud \(X_{t}\). The local property, which is independent of global pose, helps MHCDiff to generalize well in diverse SMPL estimation due to occlusion and capture local details.

### Multi-hypotheses condition

The local features are robust to noisy SMPL estimation, but cannot correct the SMPL estimation errors. Following previous multi-hypotheses human pose estimation [6; 43; 18; 21; 8], MHCDiff takes multi-hypotheses SMPL meshes from estimated distributions and predicts the most plausible outputs. We modify Eq. 2 to handle multiple sampled SMPL meshes \(\{S_{i}\}_{i\{1,...,s\}}\) using ProPose  as an off-the-shelf method:

\[X_{t}^{SMPL}=[(d(X_{t}|S_{i})),(X_{t}|S_{i})], \]

where \(=argmin_{i\{1,...,s\}}|d(X_{t}|S_{i})|\), which semantically means that each point follows the closest SMPL mesh \(S_{i}\) to consider all plausible samples in denoising steps. However, each point gets conditions from only one sample and cannot leverage off-the-shelf probability distributions. In addition to the local features, we also adopt occupancy values:

Figure 2: **(Left)** Overview of MHCDiff. Given an occluded image \(I\), MHCDiff reconstructs 3D human shape as a point cloud. First, we extract the 2D feature map \((I)\) and hypothesize pose and shape parameters of multiple plausible SMPL meshes \(\{S_{i}\}_{i\{1,...,s\}}\). Our method consists of the conditioned point cloud diffusion model (Sec. 4.4). We project the 2D image features to capture details of the image (Sec. 3) and extract local features from multiple hypothesized SMPL meshes to leverage human body priors (Sec. 4.3) **(Upper Right)** The details of local features (Sec. 4.2). The signed distance field is visualized in positive and negative regions. The arrows indicate normal vectors \(\). **(Lower Right)** The details of multi-hypotheses (Sec. 4.3). We can consider the whole distribution during denoising process with the argmin \(\), and the denoising can be approximated by red arrows. However, it is sensitive to extreme samples of the distribution, so we condition the mean of occupancy values, which is visualized by transparency, and the denoising can be approximated by blue arrows.

\[X_{t}^{SMPL}=[_{i=1}^{s}(o(X_{t}|S_{i})),(d(X_{t}|S_{i})),(X_{t}|S_{i})], \]

where \(o():^{3}\{0,1\}\) is the occupancy function of the given SMPL mesh, which is a binary signal while the signed distance is continuous. With the mean occupancy and closest signed distance, MHCDiff can assume all distributions with their respective probabilities. The proposed multi-hypotheses conditioning can take an arbitrary number of SMPL, SMPL-X, and their combined.

### Conditioned point cloud diffusion model

Finally, \(_{}():^{(3+c+4L+3)N}^{3N}\) predicts the noise \(^{3N}\) given the concatenation of partially denoised point cloud \(X_{t}^{3N}\), projected image features \(X_{t}^{proj}^{cN}\), and local features from SMPL \(X_{t}^{SMPL}^{(4L+3)N}\) (Eq. 1). Notably, we do not need any learnable parameters to extract the local features from SMPL and aggregate the features of multiple SMPL meshes. We freeze the pre-trained 2D image encoder, so it is straightforward to train the diffusion model without additional training strategies.

The point cloud diffusion model of MHCDiff takes the role of the decoder of previous implicit-function-based methods. Given the encoded features from RGB images or SMPL meshes, the decoder predicts 3D shapes such as point clouds, occupancy fields, or signed distance fields. The implicit-function-based methods need to sample the query points randomly, so the decoder has been primarily Multi-Layer Perceptron (MLP), which takes the input points independently. MHCDiff consists of the point cloud diffusion model instead of MLP because (1) the point cloud model considers the global consistent features, (2) the diffusion model has the generative properties, and (3) the denoising process approximates correcting the misaligned SMPL estimation. Given the globally encoded image features \(X_{t}^{proj}\) and the local features from SMPL \(X_{t}^{SMPL}\), MHCDiff can inpaint or restore invisible body parts and is robust to noisy SMPL estimation due to occlusion.

```
1:\(_{1:T}\): diffusion noise scheduling
2:repeat
3: Sample \(X_{0}\) from \(q(X_{0})\)
4: Load the corresponding image \(I\) and ground truth SMPL-X \(S\)
5:\(t(\{1,...,T\})\)
6:\((0,)\)
7:\(X_{t}=_{t}}X_{0}+_{t}}\)\(\) Project image features (Sec. 3)
8:\(X_{t}^{SMPL}=[(o(X_{t}|S)),(d(X_{t}|S)),(X_{t}|S)]\)\(\) Extract local features from SMPL (Sec. 4.2
9: Take gradient descent step on \(_{}-_{}(X_{t},X_{t}^{proj},X_{t }^{SMPL})^{2}\)\(\) Point cloud diffusion model (Sec. 4.4)
10:until converged
```

**Algorithm 1** Pseudocode of learning pipeline of MHCDiff

## 5 Experiments

Implementation.We use the Pytorch3D library  for image feature projection (Sec. 3) and the kaolin library  to extract local features from SMPL (Sec. 4.2). MHCDiff is trained with batch size 8 in 100,000 steps. We use MSN  as the image feature encoder. We use AdamW  with \(=(0.9,0.999)\) and a learning rate which is decayed linearly from \(0.0002\) to \(0\). For diffusion noise schedule, we use linear scheduling from \(1 10^{-5}\) to \(8 10^{-3}\) with warmup. For inference, we denoise the point cloud for \(1,000\) steps. The training process takes approximately 1 day on a single 24GB NVIDIA RTX 4090 GPU with 28M learnable parameters.

Learning.We synthesize the THuman2.0 dataset , which contains 526 high-fidelity textured scans with corresponding SMPL-X fits. We use 500 subjects for training and the others for validation.

```
0: Input image \(I\)
1: Sample \(X_{T}\) from \((0,)\)
2: Estimate single or multi SMPL(-X) meshes \(\{S_{i}\}_{i\{1,,s\}}\)
3:for all\(t\) from \(T\) to 1 do
4:\(z(0,)\) if \(t>1\) else \(z=0\)
5:\(X_{t}^{proj}=((I),X_{t})\)\(\) Project image features (Sec. 3)
6:for all\(i\) from \(1\) to \(s\)do
7: Compute \(o(X_{t}|S_{i})\), \(d(X_{t}|S_{i})\), and \((X_{t}|S_{i})\)\(\) Can be accelerated by kaolin 
8:endfor
9:\( argmin_{i\{1,,s\}}|d(X_{t}|S_{i})|\)
10:\(X_{t}^{SMPL}=[_{i=1}^{I}(o(X_{t}|S_{i})),(d(X_{t}| S_{i})),(X_{t}|S_{i})]\)\(\) Multi-hypotheses conditioning (Sec. 4.3)
11:\(_{}(X_{t},X_{t}^{proj},X_{t}^{SMPL})\)
12:\(X_{t-1}}}(X_{t}-}{}})+_{t}z\)\(\) DDPM  sampling
13:endfor
14:return\(X_{0}\)
```

**Algorithm 2** Pseudocode of inference pipeline of MHCDiff

We render each human subject from 36 multiple viewpoints and randomly mask the images, resulting in partially occluded body images. We use the farthest point sampling operation to sample 16,384 points from each GT scan. During the training, local features \(X_{t}^{SMPL}\) are extracted from a single corresponding GT SMPL-X. The learning pipeline is presented in Algorithm 1.

Inference.First, we use the CAPE dataset  with 150 textured scans. Similar to the training stage, we render each subject from 3 multiple viewpoints and randomly mask the images. During the inference, local features \(X_{t}^{SMPL}\) are extracted from multiple sampled SMPL or single estimated SMPL-X. We sample 10 SMPL meshes for our experiments. To further show the generalizability on the real-world interaction, we also evaluate MHCDiff on the MultiHuman  and Hi4D  dataset. MultiHuman, which includes the diverse interaction with objects and people, provides 3D textured scans, so we render each subject from 3 multiple viewpoints. We evaluate the performance of MHCDiff qualitatively on Hi4D, which includes close human-human interaction with high-fidelity meshes. The inference pipeline is presented in Algorithm 2.

Baseline models.We compare MHCDiff with parametric models and pixel-aligned reconstruction methods. For parametric models, which are robust for occlusion, we select ProPose  as SMPL estimator and PIXIE  as SMPL-X estimator. For pixel-aligned reconstruction methods, which can capture geometric details, we select PaMIR  for global features, ICON  for local features, and HiLo  and SIFU  for both. For the fair comparison, we primarily condition with the mean of SMPL distribution estimated via ProPose, and PIXIE is also used for ICON, which supports SMPL-X. We use pre-trained weights and evaluate under our test setting.

Evaluation metrics.We employ Point-to-Surface distance and Chamfer Distance as evaluation metrics. MHCDiff outputs a point cloud, so Chamfer Distance is the average L2 distance from the reconstructed point cloud to vertices of ground-truth scans and vice versa, and Point-to-Surface distance is the average point-to-surface from the reconstructed point cloud to ground-truth scans. The outputs of implicit-function-based methods can be converted meshes via the Marching Cubes algorithm . For fair comparison, we sample the same number of points from the reconstructed meshes uniformly.

### Comparison with state-of-the-art methods

MHCDiff outperforms prior implicit-function-based methods and SMPL estimation methods on occluded and even full-body images. Fig. 3 presents the robustness of 3D human reconstruction to the occlusion ratio. PaMIR and HiLo cannot handle the occlusions because the global feature encoder is sensitive to misaligned SMPL estimation. SIFU does not use the 3D encoder, but the cross-attention from the normal map of SMPL takes global features and is sensitive to occlusion 

   & Methods & Chamfer Distance (cm) & Point-to-Surface (cm) \\  A & PaMIR  & 12.912 & 12.619 \\  & ICON  & 2.896 & 2.789 \\  & ICON (PIXIE estimation) & 3.329 & 3.212 \\  & SIFU  & 14.397 & 14.087 \\  & HiLo  & 13.711 & 13.405 \\  B & PIXIE (SMPL-X)  & 2.705 & 2.662 \\  & ProPose (SMPL)  & 2.370 & 2.307 \\  Ours & MHCDiff & **1.872** & **1.810** \\  Ours & MHCDiff & 0.591 & **0.491** & **0.536** & **0.703** & 0.673 \\  

Table 2: **Quantitative evaluation on MultiHuman dataset.** We report the average Chamfer Distance (cm) for each category. We compare the performance similar to Tab. 1.

   & Methods & single & occluded single & two natural-inter & two closely-inter & three \\  A & PaMIR  & 0.690 & 2.349 & 5.154 & 3.752 & 4.714 \\  & ICON  & **0.555** & 0.549 & 0.563 & 0.786 & **0.669** \\  & SIFU  & 0.644 & 3.335 & 4.796 & 3.503 & 3.264 \\  & HiLo  & 0.606 & 2.808 & 4.139 & 3.346 & 4.398 \\  B & PIXIE (SMPL-X)  & 0.868 & 0.813 & 0.755 & 0.951 & 0.809 \\  & ProPose (SMPL)  & 0.675 & 0.567 & 0.574 & 0.766 & 0.688 \\  Ours & MHCDiff & 0.591 & **0.491** & **0.536** & **0.703** & 0.673 \\  

Table 1: **Ablation study on CAPE dataset.** We validate the effectiveness of **(A)** each component; **(B)** conditioning strategies; and **(C)** training strategies.

   & & Chamfer Distance (cm) & Point-to-Surface (cm) \\  full & MHCDiff & **1.872** & **1.810** \\  A & w/o occupancy & 1.893 & 1.831 \\  & w/o signed distance & 2.016 & 1.949 \\  & w/o normal & 1.888 & 1.827 \\  & w/o encoding & 1.928 & 1.863 \\  & PC\({}^{2}\) & 3.640 & 3.533 \\  B & conditioned on PIXIE estimation & 2.314 & 2.237 \\  & conditioned on single ProPose estimation & 1.939 & 1.869 \\  C & trained with ProPose estimation & 2.708 & 2.624 \\  & w/o random masking & 1.940 & 1.868 \\  

Table 3: **Ablation study on CAPE dataset.** We validate the effectiveness of **(A)** each component; **(B)** conditioning strategies; and **(C)** training strategies.

[MISSING_PAGE_FAIL:9]

We improve the performance with multi-hypotheses condition (Sec. 4.3). In Tab. 4, we show the correlation between the number of SMPL sampled and the reconstruction quality. More SMPL hypotheses may include more accurate samples and improve the quality (15 samples), as well as extreme samples and decrease the quality (20 samples). From PC\({}^{2}\), which only takes image condition, we also validate the local features from SMPL in Tab. 3-A. All of these features improve the performance, especially the signed distance. In Tab. 3-C, MHCDiff is trained without random masking or by conditioning the distribution estimated by ProPose  instead of GT SMPL-X.

## 6 Conclusion

In this paper, We present MHCDiff, which robustly reconstructs pixel-aligned and detailed 3D humans from single occluded images. Rather than implicit-function-based methods, we choose the point cloud diffusion model to generate invisible regions capturing the features globally. Our multi-hypotheses conditioning mechanism extracts local features from multiple SMPL estimations and integrates them without learnable parameters, so MHCDiff is robust to a single erroneous SMPL due to occlusion. We augment the limited training data by random masking to synthesize occlusion by diverse interaction. The experiments demonstrate that our proposed method outperforms state-of-the-art methods from various levels of occlusion and interaction. In the future, the point cloud of human shapes can be applied to intermediate stages for implicit function  and human body deformation  or motion flow .

Figure 5: **Qualitative results on in-the-wild images.** Two images on the left show occlusions due to interactions, and the rightmost image shows loose clothes. From internet photos, we use  to segment images.

Acknowledgements

This work was supported by NST grant (CRC 21011, MSIT), IITP grant (RS-2023-00228996, RS-2024-00459749, MSIT) and KOCCA grant (RS-2024-00442308, MCST).