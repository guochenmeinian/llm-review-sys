# Reversible and irreversible bracket-based dynamics

for deep graph neural networks

Anthony Gruber

Center for Computing Research

Sandia National Laboratories

Albuquerque, NM. USA

adgrube@sandia.gov

K. Lee acknowledges the support from the U.S. National Science Foundation under grant CNS2210137. Sandia National Laboratories is a multimission laboratory managed and operated by National Technology & Engineering Solutions of Sandia, LLC, a wholly owned subsidiary of Honeywell International Inc., for the U.S. Department of Energy's National Nuclear Security Administration under contract DE-NA0003525. This paper describes objective technical results and analysis. Any subjective views or opinions that might be expressed in the paper do not necessarily represent the views of the U.S. Department of Energy or the United States Government. This article has been co-authored by an employee of National Technology & Engineering Solutions of Sandia, LLC under Contract No. DE-NA0003525 with the U.S. Department of Energy (DOE). The employee owns all right, title and interest in and to the article and is solely responsible for its contents. The United States Government retains and the publisher, by accepting the article for publication, acknowledges that the United States Government retains a non-exclusive, paid-up, irrevocable, world-wide license to publish or reproduce the published form of this article or allow others to do so, for United States Government purposes. The DOE will provide public access to these results of federally sponsored research in accordance with the DOE Public Access Plan [https://www.energy.gov/downloads/doe-public-access-plan](https://www.energy.gov/downloads/doe-public-access-plan). The work of N. Trask and A. Gruber is supported by the U.S. Department of Energy, Office of Advanced Computing Research under the "Scalable and Efficient Algorithms - Causal Reasoning, Operators and Graphs" (SEA-CROGS) project, the DoE Early Career Research Program, and the John von Neumann fellowship at Sandia.

Kookjin Lee

School of Computing and Augmented Intelligence

Arizona State University

Tempe, AZ. USA

kookjin.lee@asu.edu

Nathaniel Trask

School of Engineering and Applied Science

University of Pennsylvania

Philadelphia, PA. USA

ntrask@seas.upenn.edu

K. Lee acknowledges the support from the U.S. National Science Foundation under grant CNS2210137. Sandia National Laboratories is a multimission laboratory managed and operated by National Technology & Engineering Solutions of Sandia, LLC, a wholly owned subsidiary of Honeywell International Inc., for the U.S. Department of Energy's National Nuclear Security Administration under contract DE-NA0003525. This paper describes objective technical results and analysis. Any subjective views or opinions that might be expressed in the paper do not necessarily represent the views of the U.S. Department of Energy or the United States Government. This article has been co-authored by an employee of National Technology & Engineering Solutions of Sandia, LLC under Contract No. DE-NA0003525 with the U.S. Department of Energy (DOE). The employee owns all right, title and interest in and to the article and is solely responsible for its contents. The United States Government retains and the publisher, by accepting the article for publication, acknowledges that the United States Government retains a non-exclusive, paid-up, irrevocable, world-wide license to publish or reproduce the published form of this article or allow others to do so, for United States Government purposes. The DOE will provide public access to these results of federally sponsored research in accordance with the DOE Public Access Plan [https://www.energy.gov/downloads/doe-public-access-plan](https://www.energy.gov/downloads/doe-public-access-plan). The work of N. Trask and A. Gruber is supported by the U.S. Department of Energy, Office of Advanced Computing Research under the "Scalable and Efficient Algorithms - Causal Reasoning, Operators and Graphs" (SEA-CROGS) project, the DoE Early Career Research Program, and the John von Neumann fellowship at Sandia.

###### Abstract

Recent works have shown that physics-inspired architectures allow the training of deep graph neural networks (GNNs) without oversmoothing. The role of these physics is unclear, however, with successful examples of both reversible (e.g., Hamiltonian) and irreversible (e.g., diffusion) phenomena producing comparable results despite diametrically opposed mechanisms, and further complications arising due to empirical departures from mathematical theory. This work presents a series of novel GNN architectures based upon structure-preserving bracket-based dynamical systems, which are provably guaranteed to either conserve energy or generate positive dissipation with increasing depth. It is shown that the theoretically principled framework employed here allows for inherently explainable constructions, which contextualize departures from theory in current architectures and better elucidate the roles of reversibility and irreversibility in network performance. Code is available at the Github repository [https://github.com/natrask/BracketGraphs](https://github.com/natrask/BracketGraphs).

Introduction

Graph neural networks (GNNs) have emerged as a powerful learning paradigm able to treat unstructured data and extract "object-relation"/causal relationships while imparting inductive biases which preserve invariances through the underlying graph topology [1; 2; 3; 4]. This framework has proven effective for a wide range of both _graph analytics_ and _data-driven physics modeling_ problems. Despite successes, GNNs have generally struggle to achieve the improved performance with increasing depth typical of other architectures. Well-known pathologies, such as oversmoothing, oversquashing, bottlenecks, and exploding/vanishing gradients yield deep GNNs which are either unstable or lose performance as the number of layers increase [5; 6; 7; 8].

To combat this, a number of works build architectures which mimic physical processes to impart desirable numerical properties. For example, some works claim that posing message passing as either a diffusion process or reversible flow may promote stability or help retain information, respectively. These present opposite ends of a spectrum between irreversible and reversible processes, which either dissipate or retain information. It is unclear, however, what role (ir)reversibility plays . One could argue that dissipation entropically destroys information and could promote oversmoothing, so should be avoided. Alternatively, in dynamical systems theory, dissipation is crucial to realize a low-dimensional attractor, and thus dissipation may play an important role in realizing dimensionality reduction. Moreover, recent work has shown that dissipative phenomena can actually sharpen information as well as smooth it , although this is not often noticed in practice since typical empirical tricks (batch norm, etc.) lead to a departure from the governing mathematical theory.

In physics, Poisson brackets and their metriplectic/port-Hamiltonian generalization to dissipative systems provide an abstract framework for studying conservation and entropy production in dynamical systems. In this work, we construct four novel architectures which span the (ir)reversibility spectrum, using geometric brackets as a means of parameterizing dynamics abstractly without empirically assuming a physical model. This relies on an application of the data-driven exterior calculus (DDEC) , which allows a reinterpretation of the message-passing and aggregation of graph attention networks  as the fluxes and conservation balances of physics simulators , providing a simple but powerful framework for mathematical analysis. In this context, we recast graph attention as an inner-product on graph features, inducing graph derivative "building-blocks" which may be used to build geometric brackets. In the process, we generalize classical graph attention  to higher-order clique cochains (e.g., labels on edges and loops). The four architectures proposed here scale with identical complexity to classical graph attention networks, and possess desirable properties that have proven elusive in current architectures. On the reversible and irreversible end of the spectrum we have _Hamiltonian_ and _Gradient_ networks. In the middle of the spectrum, _Double Bracket_ and _Metriplectic_ architectures combine both reversibility and irreversibility, dissipating energy to either the environment or an entropic variable, respectively, in a manner consistent with the second law of thermodynamics. We summarize these brackets in Table 1, providing a diagram of their architecture in Figure 1.

**Primary contributions:**

**Theoretical analysis of GAT in terms of exterior calculus.** Using DDEC we establish a unified framework for construction and analysis of message-passing graph attention networks, and provide an extensive introductory primer to the theory in the appendices. In this setting, we show that with our modified attention mechanism, GATs amount to a diffusion process for a special choice of activation and weights.

**Generalized attention mechanism.** Within this framework, we obtain a natural and flexible extension of graph attention from nodal features to higher order cliques (e.g. edge features). We show attention must have a symmetric numerator to be formally structure-preserving, and introduce a novel and flexible graph attention mechanism parameterized in terms of learnable inner products on nodes and edges.

**Novel structure-preserving extensions.** We develop four GNN architectures based upon bracket-based dynamical systems. In the metriplectic case, we obtain the first architecture with linear complexity in the size of the graph while previous works are \(O(N^{3})\).

**Unified evaluation of dissipation.** We use these architectures to systematically evaluate the role of (ir)reversibility in the performance of deep GNNs. We observe best performance for partially dissipative systems, indicating that a combination of both reversibility and irreversibility are important. Pure diffusion is the least performant across all benchmarks. For physics-based problems including optimal control, there is a distinct improvement. All models provide near state-of-the-art performance and marked improvements over black-box GAT/NODE networks.

## 2 Previous works

**Neural ODEs:** Many works use neural networks to fit dynamics of the form \(=f(x,)\) to time series data. Model calibration (e.g., UDE ), dictionary-based learning (e.g., SINDy ), and neural ordinary differential equations (e.g., NODE ) pose a spectrum of inductive biases requiring progressively less domain expertise. Structure-preservation provides a means of obtaining stable training without requiring domain knowledge, ideally achieving the flexibility of NODE with the robustness of UDE/SINDy. The current work learns dynamics on a graph while using a modern NODE library to exploit the improved accuracy of high-order integrators [17; 18; 19].

**Structure-preserving dense networks:** For dense networks, it is relatively straightforward to parameterize reversible dynamics, see for example: Hamiltonian neural networks [20; 21; 22; 23], Hamiltonian generative networks , Hamiltonian with Control (SymODEN) , Deep Lagrangian networks  and Lagrangian neural networks . Structure-preserving extensions to dissipative systems are more challenging, particularly for _metriplectic_ dynamics  which require a delicate degeneracy condition to preserve discrete notions of the first and second laws of thermodynamics. For dense networks such constructions are intensive, suffering from \(O(N^{3})\) complexity in the number of features [29; 30; 31]. In the graph setting we avoid this and achieve linear complexity by exploiting exact sequence structure. Alternative dissipative frameworks include Dissipative SymODEN  and port-Hamiltonian . We choose to focus on metriplectic parameterizations due to their broad potential impact in data-driven physics modeling, and ability to naturally treat fluctuations in multiscale systems .

**Physics-informed vs structure-preserving:** "Physics-informed" learning imposes physics by penalty, adding a regularizer corresponding to a physics residual. The technique is simple to implement and has been successfully applied to solve a range of PDEs , discover data-driven models to complement first-principles simulators [36; 37; 38], learn metriplectic dynamics , and perform uncertainty quantification [40; 41]. Penalization poses a multiobjective optimization problem, however, with parameters weighting competing objectives inducing pathologies during training, often resulting in physics being imposed to a coarse tolerance and qualitatively poor predictions [42; 43]. In contrast, structure-preserving architectures exactly impose physics by construction via carefully designed networks. Several works have shown that penalty-based approaches suffer in comparison, with structure-preservation providing improved long term stability, extrapolation and physical realizability.

**Structure-preserving graph networks:** Several works use discretizations of specific PDEs to combat oversmoothing or exploding/vanishing gradients, e.g. telegraph equations  or various reaction-diffusion systems . Several works develop Hamiltonian flows on graphs [46; 47]. For metriplectic dynamics,  poses a penalty based formulation on graphs. We particularly focus on GRAND, which poses graph learning as a diffusive process , using a similar exterior calculus framework and interpreting attention as a diffusion coefficient. We show in Appendix A.5 that their

  
**Formalism** & **Equation** & **Requirements** & **Completeness** & **Character** \\ Hamiltonian & \(}=\{,E\}\) & \(^{*}=-\), & complete & conservative \\   Gradient & \(}=-[,E]\) & \(^{*}=\) & incomplete & totally dissipative \\ Double Bracket & \(}=\{,E\}+\{\{,E\}\}\) & \(^{*}=-\) & incomplete & partially dissipative \\ Metriplectic & \(}=\{,E\}+[,S]\) & \(^{*}=-,^{*}=\), & complete & partially dissipative \\  & & \( S= E=\) & & \\   

Table 1: The abstract bracket formulations employed in this work. Here \(\) represents a state variable, while \(E=E(),S=S()\) are energy and entropy functions. “Conservative” indicates purely reversible motion, “totally dissipative” indicates purely irreversible motion, and “partially dissipative” indicates motion which either dissipates \(E\) (in the double bracket case) or generates \(S\) (in the metriplectic case).

the governing theory. To account for this, we introduce a _modified attention mechanism_ which retains interpretation as a part of diffusion PDE. In this purely irreversible case, it is of interest whether adherence to the theory provides improved results, or GRAND's success is driven by something other than structure-preservation.

## 3 Theory and fundamentals

Here we introduce the two essential ingredients to our approach: bracket-based dynamical systems for neural differential equations, and the data-driven exterior calculus which enables their construction. A thorough introduction to this material is provided in Appendices A.1, A.3, and A.2.

**Bracket-based dynamics:** Originally introduced as an extension of Hamiltonian/Lagrangian dynamics to include dissipation , bracket formulations are used to inform a dynamical system with certain structural properties, e.g., time-reversibility, invariant differential forms, or property preservation. Even without dissipation, bracket formulations may compactly describe dynamics while preserving core mathematical properties, making them ideal for designing neural architectures.

Bracket formulations are usually specified via some combination of reversible brackets \(\{F,G\}= F, G\) and irreversible brackets \([F,G]= F, G,\{\{F,G\}\}=  F,^{2} G\) for potentially state-dependent operators \(^{*}=-\) and \(^{*}=\). The particular brackets which are used in the present network architectures are summarized in Table 1. Note that complete systems are the dynamical extensions of isolated thermodynamical systems: they conserve energy and produce entropy, with nothing lost to the ambient environment. Conversely, incomplete systems do not account for any lost energy: they only require that it vanish in a prescribed way. The choice of completeness is an application-dependent modeling assumption.

**Exterior calculus:** In the combinatorial Hodge theory , an oriented graph \(=\{,\}\) carries sets of \(k\)-cliques, denoted \(_{k}\), which are collections of ordered subgraphs generated by \(\{k+1\}\) nodes. This induces natural exterior derivative operators \(d_{k}:_{k}_{k+1}\), acting on the spaces of functions on \(_{k}\), which are the signed incidence matrices between \(k\)-cliques and \((k+1)\)-cliques. An explicit representation of these derivatives is given in Appendix A.1, from which it is easy to check the exact sequence property \(d_{k+1} d_{k}=0\) for any \(k\). This yields a discrete de Rham complex on the graph \(\) (Figure 2). Moreover, given a choice of inner product (say, \(^{2}\)) on \(_{k}\), there is an obvious dual de Rham complex which comes directly from adjointness. In particular, one can define dual derivatives \(d_{k}^{*}:_{k+1}_{k}\) via the equality

\[ d_{k}f,g_{k+1}= f,d_{k}^{*}g _{k},\]

from which nontrivial results such as the Hodge decomposition, Poincare inequality, and coercivity/invertibility of the Hodge Laplacian \(_{k}=d_{k}^{*}d_{k}+d_{k-1}d_{k-1}^{*}\) follow (see e.g. ). Using the derivatives \(d_{k},d_{k}^{*}\), it is possible to build compatible discretizations of PDEs on \(\) which are guaranteed to preserve exactness properties such as, e.g., \(d_{1} d_{0}==0\).

The choice of inner product \(,_{k}\) thus induces a definition of the dual derivatives \(d_{k}^{*}\). In the graph setting , one typically selects the \(^{2}\) inner product, obtaining the adjoints of the signed incidence matrices as \(d_{k}^{*}=d_{k}^{T}\). By instead working with the modified inner product \((,)=^{}_{k}\) for a machine-learnable \(_{k}\), we obtain \(d_{k}^{*}=_{k}^{-1}d_{k}^{}_{k+1}\) (see Appendix A.3). This parameterization

Figure 1: A diagrammatic illustration of the bracket-based architectures introduced in Section 4.

inherits the exact sequence property from the graph topology encoded in \(d_{k}\) while allowing for incorporation of geometric information from data. This leads directly to the following result, which holds for any (potentially feature-dependent) symmetric positive definite matrix \(_{k}\).

**Theorem 3.1**.: _The dual derivatives \(d_{k}^{*}:_{k+1}_{k}\) adjoint to \(d_{k}:_{k}_{k+1}\) with respect to the learnable inner products \(_{k}:_{k}_{k}\) satisfy an exact sequence property._

Proof.: \[d_{k-1}^{*}d_{k}^{*}=_{k-1}^{-1}d_{k-1}^{}_{k} _{k}^{-1}d_{k}^{}_{k+1}=_{k-1}^{-1} (d_{k}d_{k-1})^{}_{k+1}=0.\]

As will be shown in Section 4, by encoding graph attention into the \(_{k}\), we may exploit the exact sequence property to obtain symmetric positive definite diffusion operators, as well as conduct the cancellations necessary to enforce degeneracy conditions necessary for metriplectic dynamics.

For a thorough review of DDEC, we direct readers to Appendix A.1 and . For exterior calculus in topological data analysis see , and an overview in the context of PDEs see [53; 54].

## 4 Structure-preserving bracket parameterizations

We next summarize properties of the bracket dynamics introduced in Section 3 and displayed in Table 1, postponing details and rigorous discussion to Appendices A.3 and A.6. Letting \(=(,)\) denote node-edge feature pairs, the following operators will be used to generate our brackets.

\[=0&-d_{0}^{*}\\ d_{0}&0,=_{0}&0\\ 0&_{1}=d_{0}^{*}d_{0}&0\\ 0&d_{1}^{*}d_{1}+d_{0}d_{0}^{*},=0 &0\\ 0&_{1}d_{1}^{*}d_{1}_{1}.\]

As mentioned before, the inner products \(_{0},_{1},_{2}\) on \(_{k}\) which induce the dual derivatives \(d_{0}^{*},d_{1}^{*}\), are chosen in such a way that their combination generalizes a graph attention mechanism. The precise details of this construction are given below, and its relationship to the standard GAT network from  is shown in Appendix A.5. Notice that \(^{*}=-\), while \(^{*}=,^{*}=\) are positive semi-definite with respect to the block-diagonal inner product \((,)\) defined by \(=(_{0},_{1})\) (details are provided in Appendix A.6). Therefore, \(\) generates purely reversible (Hamiltonian) dynamics and \(,\) generate irreversible (dissipative) ones. Additionally, note that state-dependence in \(,,\) enters only through the adjoint differential operators, meaning that any structural properties induced by the topology of the graph \(\) (such as the exact sequence property mentioned in Theorem 3.1) are automatically preserved.

**Remark 4.1**.: _Strictly speaking, \(\) is guaranteed to be a truly Hamiltonian system only when \(d_{0}^{*}\) is state-independent, since it may otherwise fail to satisfy Jacobi's identity. On the other hand, energy conservation is always guaranteed due to the fact that \(\) is skew-adjoint._

In addition to the bracket matrices \(,,\), it is necessary to have access to energy and entropy functions \(E,S\) and their associated functional derivatives with respect to the inner product on \(_{0}_{1}\) defined by \(\). For the Hamiltonian, gradient, and double brackets, \(E\) is chosen simply as the "total kinetic energy"

\[E(,)=(||^{2}+| |^{2})=_{i}|_{i}|^{2}+_{}|_{ }|^{2},\]

Figure 2: A commutative diagram illustrating the relationship between the graph derivatives \(d_{k}\), their \(^{2}\) adjoints \(d_{k}^{}\), and the learnable adjoints \(d_{k}^{*}\). These operators form a _de Rham complex_ due to the exact sequence property \(d_{i+1} d_{i}=d_{i}^{} d_{i+1}^{*}=d_{i}^{*} d_{i+1}^ {*}=0\). We show that the learnable \(_{k}\) may encode attention mechanisms, without impacting the preservation of exact sequence structure.

whose \(\)-gradient (computed in Appendix A.6) is just \( E(,)=(_{0}^{-1}_{1}^{-1})^{}\). Since the metriplectic bracket uses parameterizations of \(E,S\) which are more involved, discussion of this case is deferred to later in this Section.

**Attention as learnable inner product:** Before describing the dynamics, it remains to discuss how the matrices \(_{i}\), \(0 i 2\), are computed in practice, and how they relate to the idea of graph attention. Recall that if \(n_{V}>0\) denotes the nodal feature dimension, a graph attention mechanism takes the form \(a(_{i},_{j})=f(_{ij})/_{j}f( _{ij})\) for some differentiable pre-attention function \(:n_{V} n_{V}\) (e.g., for scaled dot product ) one typically represents \(a(_{i},_{j})\) as a softmax, so that \(f=()\)). This suggests a decomposition \(a(_{i},_{j})=_{0}^{-1}_{1}\) where \(_{0}=(a_{0,ii})\) is diagonal on nodes and \(_{1}=(a_{1,ij})\) is diagonal on edges,

\[a_{0,ii}=_{j(i)}f((_{i}, _{j})), a_{1,ij}=f(( _{i},_{j})).\]

Treating the numerator and denominator of the standard attention mechanism separately in \(_{0},_{1}\) allows for a flexible and theoretically sound incorporation of graph attention directly into the adjoint differential operators on \(\). In particular, _if \(_{1}\) is symmetric_ with respect to edge-orientation and \(\) is an edge feature which is antisymmetric, it follows that

\[(d_{0}^{*})_{i}=(_{0}^{-1}d_{0}^{ }_{1})_{i}=_{j(i)}a(_{i},_{j})_{ji},\]

which is just graph attention combined with edge aggregation. This makes it possible to give the following informal statement regarding graph attention networks which is explained and proven in Appendix A.5.

**Remark 4.2**.: _The GAT layer from  is almost the forward Euler discretization of a metric heat equation._

The "almost" appearing here has to do with the fact that (1) the attentional numerator \(f((_{i},_{j}))\) is generally asymmetric in \(i,j\), and is therefore symmetrized by the divergence operator \(d_{0}^{}\), (2) the activation function between layers is not included, and (3) learnable weight matrices \(^{k}\) in GAT are set to the identity.

**Remark 4.3**.: _The interpretation of graph attention as a combination of learnable inner products admits a direct generalization to higher-order cliques, which is discussed in Appendix A.4._

**Hamiltonian case:** A purely conservative system is generated by solving \(}=() E()\), or

\[}\\ }=0&-d_{0}^{*}\\ d_{0}&0_{0}^{-1}&0\\ 0&_{1}^{-1}\\ =-d_{0}^{*}_{1}^{-1} \\ d_{0}_{0}^{-1}.\]

This is a noncanonical Hamiltonian system which generates a purely reversible flow. In particular, it can be shown that

\[()=(}, E())\,= (() E(), E())\, =-( E(),() E() )\,=0,\]

so that energy is conserved due to the skew-adjointness of \(\).

**Gradient case:** On the opposite end of the spectrum are generalized gradient flows, which are totally dissipative. Consider solving \(}=-() E()\), or

\[}\\ }=-_{0}&0\\ 0&_{1}_{0}^{-1}&0\\ 0&_{1}^{-1}\\ =-_{0}_{0}^{-1}\\ _{1}_{1}^{-1}.\]

This system is a metric diffusion process on nodes and edges separately. Moreover, it corresponds to a generalized gradient flow, since

\[()=(}, E())\,=- (() E(), E())\, =-| E()|_{}^{2} 0,\]

due to the self-adjoint and positive semi-definite nature of \(\).

**Remark 4.4**.: _The architecture in GRAND  is almost a gradient flow, however the pre-attention mechanism lacks the requisite symmetry to formally induce a valid inner product._

[MISSING_PAGE_FAIL:7]

Since this system is metriplectic when expressed in position-momentum-entropy coordinates (c.f. ), it is useful to see if any of the brackets from Section 4 can adequately capture these dynamics without an entropic variable. The results of applying the architectures of Section 4 to reproduce a trajectory of five periods are displayed in Table 2, alongside comparisons with a black-box NODE network and a latent NODE with feature encoder/decoder. While each network is capable of producing a small mean absolute error, it is clear that the metriplectic and Hamiltonian networks produce the most accurate trajectories. It is remarkable both that the Hamiltonian bracket does so well here and that the gradient bracket does so poorly, being that the damped double pendulum system is quite dissipative. On the other hand, it is unlikely to be only the feature encoder/decoder leading to good performance here, as both the NODE and NODE+AE architectures perform worse on this task by about one order of magnitude.

### MuJoCo Dynamics

Next we test the proposed models on more complex physical systems that are generated by the Multi-Joint dynamics with Contact (MuJoCo) physics simulator . We consider the modified versions of Open AI Gym environments : HalfCheetah, Hopper, and Swimmer.

We represent an object in an environment as a fully-connected graph, where a node corresponds to a body part of the object and, thus, the nodal feature \(_{i}\) corresponds to a position of a body part or an angle of a joint.3 As the edge features, a pair of nodal velocities \(_{}=(v_{()},v_{()})\) are provided, where \(v_{()}\) and \(v_{()}\) denote velocities of the source and destination nodes connected to the edge.

Since the MuJoCo environments contain an actor applying controls, additional control input is accounted for with an additive forcing term which is parameterized by a multi-layer perceptron and introduced into the bracket-based dynamics models. See Appendix B.2 for additional experimental details. The problem therefore consists of finding an optimal control MLP, and we evaluate the improvement which comes from representing the physics surrogate with bracket dynamics over NODE.

All models are trained via minimizing the MSE between the predicted positions \(}\) and the ground truth positions \(\) and are tested on an unseen test set. Table 3 reports the errors of network predictions on the test set measured in the relative \(_{2}\) norm, \(\|-}\|_{2}/\|\|_{2}\|}\| _{2}\). Similar to the double pendulum experiments, all models are able to produce accurate predictions with around or less than \(10\%\) errors. While the gradient bracket makes little to no improvements over NODEs, the Hamiltonian, double, and metriplectic brackets produce more accurate predictions. Interestingly, the Hamiltonian bracket performs the best in this case as well, meaning that any dissipation present is effectively compensated for by the autoencoder which transforms the features.

### Node classification

Moving beyond physics-based examples, it remains to see how bracket-based architectures perform on "black-box" node classification problems. Table 4 and Table 5 present results on common benchmark problems including the citation networks Cora , Citeseer , and Pubmed , as well as the coauthor graph, CoauthorCS , and the Amazon co-purchasing graphs, Computer and Photo . For comparison, we report results on a standard GAT , a neural graph differential equation

   Double pendulum & **MAE \(\)** & **MAE \(\)** & **Total MAE** \\
**NODE** & \(0.0240 0.015\) & \(0.0299 0.0091\) & \(0.0269 0.012\) \\
**NODE+AE** & \(0.0532 0.029\) & \(0.0671 0.043\) & \(0.0602 0.035\) \\
**Hamiltonian** & \(0.00368 0.0015\) & \(0.00402 0.0015\) & \(0.00369 0.0013\) \\
**Gradient** & \(0.00762 0.0023\) & \(0.0339 0.012\) & \(0.0208 0.0067\) \\
**Double Bracket** & \(0.00584 0.0013\) & \(0.0183 0.0071\) & \(0.0120 0.0037\) \\
**Metriplectic** & \(0.00364 0.00064\) & \(0.00553 0.00029\) & \(0.00459 0.00020\) \\   

Table 2: Mean absolute errors (MAEs) of the network predictions in the damped double pendulum case, reported as avg\(\)stdev over 5 runs.

architecture (GDE) , and the nonlinear GRAND architecture (GRAND-nl) from  which is closest to ours. Since our experimental setting is similar to that of , the numbers reported for GAT, GDE, and GRAND-nl are taken directly from this paper. Note that, despite the similar \(O(N)\) scaling in the metriplectic architecture, the high dimension of the node and edge features on the latter three datasets led to trainable \(E,S\) functions which exhausted the memory on our available machines, and therefore results are not reported for these cases. A full description of experimental details is provided in Appendix B.3.

**Remark 5.1**.: _To highlight the effect of bracket structure on network performance, only minimal modifications are employed during network training. In particular, we do not include any additional regularization, positional encoding, graph rewiring, extraction of connected components, extra terms on the right-hand side, or early stopping. While it is likely that better classification performance could be achieved with some of these modifications included, it becomes very difficult to isolate the effect of structure-preservation. A complete list of tunable hyperparameters is given in Appendix B.3._

The results show different behavior produced by each bracket architecture. It is empirically clear that there is some value in full or partial reversibility, since the Hamiltonian and double bracket architectures both perform better than the corresponding gradient architecture on datasets such as Computer and Photo. Moreover, it appears that the partially reversible double bracket performs the best of the bracket architectures in every case, which is consistent with the idea that both reversible and irreversible dynamics are critical for capturing the behavior of general dynamical systems. Interestingly, the metriplectic bracket performs worse on these tasks by a large margin. We conjecture this architecture may be harder to train for larger problems despite its \(O(N)\) complexity in the graph size, suggesting that more sophisticated training strategies may be required for large problems.

## 6 Conclusion

This work presents a unified theoretical framework for analysis and construction of graph attention networks. The exact sequence property of graph derivatives and coercivity of Hodge Laplacians which follow from the theory allow the construction of four structure-preserving brackets, which we use to evaluate the role of irreversibility in both data-driven physics simulators and graph analytics problems. In all contexts, the pure diffusion bracket performed most poorly, with mixed results between purely reversible and partially dissipative brackets.

   Planetoid splits & **CORA** & **CiteSeer** & **PubMed** \\
**GAT** & \(82.8 0.5\) & \(69.5 0.9\) & \(79.0 0.5\) \\
**GDE** & \(83.8 0.5\) & \(72.5 0.5\) & \(79.9 0.3\) \\
**GRAND-nl** & \(83.6 0.5\) & \(70.8 1.1\) & \(79.7 0.3\) \\ 
**Hamiltonian** & \(77.2 0.7\) & \(73.0 1.2\) & \(78.5 0.3\) \\
**Gradient** & \(79.9 0.7\) & \(71.8 1.4\) & \(78.6 0.7\) \\
**Double Bracket** & \(82.6 0.9\) & \(74.2 1.4\) & \(79.6 0.6\) \\
**Metriplectic** & \(57.4 1.0\) & \(60.5 1.1\) & \(69.8 0.7\) \\   

Table 4: Test accuracy and standard deviations (averaged over 20 randomly initialized runs) using the original Planetoid train-valid-test splits. Comparisons use the numbers reported in .

   Dataset & **HalfCheetah** & **Hopper** & **Swimmer** \\
**NODE+AE** & \(0.106 0.0011\) & \(0.0780 0.0021\) & \(0.0297 0.0036\) \\
**Hamiltonian** & \(0.0566 0.013\) & \(0.0279 0.0019\) & \(0.0122 0.00044\) \\
**Gradient** & \(0.105 0.0076\) & \(0.0848 0.0011\) & \(0.0290 0.0011\) \\
**Double Bracket** & \(0.0621 0.0096\) & \(0.0297 0.0048\) & \(0.0128 0.00070\) \\
**Metriplectic** & \(0.105 0.0091\) & \(0.0398 0.0057\) & \(0.0179 0.00059\) \\   

Table 3: Relative error of network predictions for the MuJoCo environment on the test set, reported as avg\(\)stdev over 4 runs.

The linear scaling achieved by the metriplectic brackets has a potential major impact for data-driven physics modeling. Metriplectic systems emerge naturally when coarse-graining multiscale systems. With increasing interest in using ML to construct digital twins, fast data-driven surrogates for complex multi-physics acting over multiple scales will become crucial. In this setting the stability encoded by metriplectic dynamics translates to robust surrogates, with linear complexity suggesting the possibility of scaling up to millions of degrees of freedom.

Limitations:All analysis holds under the assumption of modified attention mechanisms which allow interpretation of GAT networks as diffusion processes; readers should take care that the analysis is for a non-standard attention. Secondly, for all brackets we did not introduce empirical modifications (e.g. regularization, forcing, etc) to optimize performance so that we could study the role of (ir)reversibility in isolation. With this in mind, one may be able to add "tricks" to e.g. obtain a diffusion architecture which outperforms those presented here. Finally, note that the use of a feature autoencoder in the bracket architectures means that structure is enforced in the transformed space. This allows for applicability to more general systems, and can be easily removed when appropriate features are known.

Broader impacts:The work performed here is strictly foundational mathematics and is intended to improve the performance of GNNs in the context of graph analysis and data-driven physics modeling. Subsequent application of the theory may have societal impact, but the current work anticipated to improve the performance of machine learning in graph settings only at a foundational level.