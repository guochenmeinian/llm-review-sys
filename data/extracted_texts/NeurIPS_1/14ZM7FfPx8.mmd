# Towards Understanding the Dynamics of Gaussian-Stein Variational Gradient Descent

Tianle Liu

Department of Statistics

Harvard University

Cambridge, MA 02138

tianleliu@fas.harvard.edu

&Promit Ghosal

Department of Mathematics

Massachusetts Institute of Technology

Waltham, MA 02453

promit@mit.edu

&Krishnakumar Balasubramanian

Department of Statistics

University of California, Davis

Davis, CA 95616

kbala@ucdavis.edu

&Natesh S. Pillai

Department of Statistics

Harvard University

Cambridge, MA 02138

pillai@fas.harvard.edu

###### Abstract

Stein Variational Gradient Descent (SVGD) is a nonparametric particle-based deterministic sampling algorithm. Despite its wide usage, understanding the theoretical properties of SVGD has remained a challenging problem. For sampling from a Gaussian target, the SVGD dynamics with a bilinear kernel will remain Gaussian as long as the initializer is Gaussian. Inspired by this fact, we undertake a detailed theoretical study of the Gaussian-SVGD, i.e., SVGD projected to the family of Gaussian distributions via the bilinear kernel, or equivalently Gaussian variational inference (GVI) with SVGD. We present a complete picture by considering both the mean-field PDE and discrete particle systems. When the target is strongly log-concave, the mean-field Gaussian-SVGD dynamics is proven to converge linearly to the Gaussian distribution closest to the target in KL divergence. In the finite-particle setting, there is both uniform in time convergence to the mean-field limit and linear convergence in time to the equilibrium if the target is Gaussian. In the general case, we propose a density-based and a particle-based implementation of the Gaussian-SVGD, and show that several recent algorithms for GVI, proposed from different perspectives, emerge as special cases of our unifying framework. Interestingly, one of the new particle-based instance from this framework empirically outperforms existing approaches. Our results make concrete contributions towards obtaining a deeper understanding of both SVGD and GVI.

## 1 Introduction

Sampling from a given target density arises frequently in Bayesian statistics, machine learning and applied mathematics. Specifically, given a potential \(V:^{d}\), the target density is given by

\[(x) Z^{-1}e^{-V(x)}, Z e^{-V(x )}dx.\]

Traditionally-used Markov Chain Monte Carlo (MCMC) sampling algorithms are invariably not scalable to large-scale datasets . Variational inference and particle-based methods are two related alternatives proposed in the literature, both motivated by viewing sampling as optimization over the space of densities. We refer to  for additional details related to this line of works.

In the literature on variational inference, recent efforts have focused on the Gaussian Variational Inference (GVI) problem. On the theoretical side, this is statistically motivated by the Bernstein-vonMises theorem, which posits that in the limit of large samples posterior distributions tend to be Gaussian distributed under certain regularity assumptions. We refer to [81, Chapter 10] for details of the classical results, and to  for some recent non-asymptotic analysis. On the algorithmic side, efficient algorithms with both statistical and computational guarantees are developed for GVI . From a practical point-of-view, several works  have shown superior performance of GVI, especially in the presence of large datasets.

Turning to particle-based methods,  proposed the Stein Variational Gradient Descent (SVGD) algorithm, a kernel-based deterministic approach for sampling. It has gained significant attention in the machine learning and applied mathematics communities due to its intriguing theoretical properties and wide applicability . Researchers have also developed variations of SVGD motivated by algorithmic and applied challenges . In its original form, SVGD could be viewed as a nonparametric variational inference method with a kernel-based practical implementation.

The flexibility offered by the _nonparametric_ aspect of SVGD also leads to unintended consequences. On one hand, from a practical perspective, the question of how to pick the right kernel for implementing the SVGD algorithm is unclear. Existing approaches are mostly ad-hoc and do not provide clear instructions on the selection of kernels. On the other hand, developing a deeper theoretical understanding of SVGD dynamics is challenging due to its nonparametric formulation. Notably  derived the continuous-time PDE for the evolving density that emerges as the mean-field limit of the finite-particle SVGD systems, and shows the well-posedness of the PDE solutions. In general, the following different types of convergences could be examined regarding SVGD:

1. Unified convergence of the empirical measure for \(N\) finite particles to the continuous target as time \(t\) and \(N\) jointly grow to infinity;
2. Convergence of mean-field SVGD to the target distribution over time;
3. Convergence of the empirical measure for finite particles to the mean-field distribution at any finite given time \(t[0,)\);
4. Convergence of finite-particle SVGD to the equilibrium over time;
5. Convergence of the empirical measure for finite particles to the continuous target at time \(t=\).

From a practical point of view (a) is the ideal type of result that fully characterizes the algorithmic behavior of SVGD, which could be obtained by combining either (b) and (c) or (d) and (e). Regarding (b),  showed the convergence of mean-field SVGD in kernel Stein discrepancy (KSD, ), which is known to imply weak convergence under appropriate assumptions.  sharpened the results with weaker conditions or explicit rates.  extended the above result to the stronger Fisher information metric and Kullback-Leibler divergence based on a regularization technique.  obtained time-dependent mean-field convergence (c) of \(N\) particles under various assumptions using techniques from the literature of _propagation of chaos_.  obtained even stronger results for (c) and combined (b) to get the first unified convergence (a) in terms of KSD. However, they have a rather slow rate \(1/\), resulting from the fact that their bounds for (c) still depends on the time \(t\) (sum of step sizes) double-exponentially. Moreover, there has not been any work that studies the convergence (d) and (e) for SVGD, which illustrate a new way to characterize the unified convergence (a).

In an attempt to overcome the drawbacks of the nonparametric formulation of SVGD and also taking cue from the GVI literature, in this work we study the dynamics of Gaussian-SVGD, a parametric formulation of SVGD. Our contributions in this work are three-fold:

* _Mean-field results:_ We study the dynamics of Gaussian-SVGD in the mean-field setting and establish linear convergence for both Gaussian and strongly log-concave targets. As an example of the obtained results, Table 1 shows the convergence rates of covariance for centered Gaussian families for several revelant algorithms. All of them will be introduced later in Sections 2 and 3. We also establish the well-posedness of the solutions for the mean-field PDE and discrete particle systems that govern SVGD with bilinear kernels (see Appendix C). Prior work  requires that the kernel be radial which rules out the important class of bilinear kernels that we consider.  relaxed the radial kernel assumption, however, they required boundedness assumptions which we avoid in this work for the case of bilinear kernels.
* _Finite-particle results:_ We study the finite-particle SVGD systems in both continuous and discrete time for Gaussian targets. We show that for SVGD with a bilinear kernel if the target and initializer are both Gaussian, the mean-field convergence can be uniform in time (See Theorem 3.7). To the best of our knowledge, this is the first uniform in time result for SVGD dynamics and should be contrasted with the double exponential dependency on \(t\) for nonparametric SVGD [58; 72]. Our numerical simulations suggest that similar results should hold for certain classes of non-Gaussian target as well for Gaussian-SVGD. We also study the convergence (d) by directly solving the finite-particle systems (See Theorems 3.6 and 3.8). Moreover, in Theorem 3.10, assuming centered Gaussian targets, we obtain a linear rate for covariance convergence in the finite-particles, discrete-time setting, precisely characterizing the step size choice for the practical algorithm.
* _Unifying algorithm frameworks:_ We propose two unifying algorithm frameworks for finite-particle, discrete-time implementations of the Gaussian-SVGD dynamics. The first approach assumes access to samples from Gaussian densities with the mean and covariance depending on the current time instance of the dynamics. The second is a purely particle-based approach, in that, it assumes access only to an initial set of samples from a Gaussian density. In particular, we show that three previously proposed methods from [27; 43] for GVI emerge as special cases of the proposed frameworks, by picking different bilinear kernels, thereby further strengthening the connections between GVI and the kernel choice in SVGD. Furthermore, we conduct experiments for eight algorithms that can be implied from our framework, and observe that the particle-based algorithms are invariably more stable than density-based ones. Notably one of the new particle-based algorithms emerging from our analysis outperforms existing approaches.

## 2 Preliminaries

Denote the space of probability densities on \(^{d}\) by \((^{d}):=(^{d}): \,=1, 0}\), where \((^{d})\) is the set of smooth functions. As studied in , \((^{d})\) forms a Frechet manifold called the density manifold. For any "point" \((^{d})\), we denote the tangent space and cotangent space at \(\) by \(T_{}(^{d})\) and \(T_{}^{*}(^{d})\) respectively. A Riemannian metric tensor assigns to each \((^{d})\) a positive definite inner product \(g_{}:T_{}(^{d}) T_{}( ^{d})\) and uniquely corresponds to an isomorphism \(G_{}\) (called the canonical isomorphism) between the tangent and cotangent bundles , i.e., we have \(G_{}:T_{}(^{d}) T_{}^{*}( ^{d})\).

**Definition 2.1** (Wasserstein metric).: _The Wasserstein metric is induced by the following canonical isomorphism \(G_{}^{}:T_{}(^{d}) T_{ }^{*}(^{d})\) such that_

\[(G_{}^{})^{-1}=-(), T _{}^{*}(^{d}).\]

The Wasserstein gradient flow (WGF) can be seen as the natural gradient flow for KL divergence on the density manifold with respect to the Wasserstein metric. In specific, the mean-field PDE is given by the linear Fokker-Planck equation :

\[_{t}=-(G_{_{t}}^{})^{-1}}(_{t}^{*})=_{t} }(_{t}^{*} )=(_{t}+_{t} V), \]

    & \(K_{1}\)-SVGD & \(K_{2}\)-SVGD & WGF (\(K_{3}\)-SVGD) & R-SVGD (\(K_{4}\)-SVGD) \\  Centered Gaussian & \((e^{-2t})\)[3.2] & \((e^{-2t})\) [F.2] & \((e^{-})\) [F.1] & \((e^{-})\)[3.4] \\  General Gaussian & Theorem 3.1 & \(e^{- 2t}\) [F.2] & \((e^{-})\) [F.1] & \(e^{-t} \)[3.4] \\   

Table 1: Convergence rates of SVGD with different bilinear kernels for Gaussian families.

where \(}\) denotes the variational derivative with respect to \(_{t}\), \((^{*})\) is the so-called energy function, and \(V()\) is the potential function that satisfies \(^{*}()(-V())\).

Interestingly, the mean field flow of SVGD can also be seen as a gradient flow for the KL divergence but with respect to the Stein metric, where we perform kernelization in the cotangent space before taking the divergence .

**Definition 2.2** (Stein metric).: _The Stein metric is induced by the following canonical isomorphism \(G^{}_{}:T_{}(^{d}) T^{*}_{} (^{d})\) such that_

\[(G^{}_{})^{-1}=-() K(, )()()\,\,, T ^{*}_{}(^{d}),\]

_where \(K:^{d}^{d}\) is a positive-definite kernel._

In particular, the mean field PDE of the SVGD algorithm can be written as

\[_{t}=-(G^{}_{_{t}})^{-1}}\,(_{t}^{*})=_{t}()  K(,)_{t}()+_{t}() V( {y})\,\,. \]

Gaussian Families as Submanifolds.We consider the family of (multivariate) Gaussian densities \(_{}(^{d})\) where \(=(,)=^{d}^{+}(d, )\). Note that \(^{+}(d,)\) is the set of (symmetric) positive definite \(d d\) matrices. In this way, \(\) can be identified as a Riemannian submanifold of the density manifold \((^{d})\) with the induced Riemannian structure. If we further restrict the Gaussian family to have zero mean, it further induces the submanifold \(_{0}=^{+}(d,)\). Notably the Wasserstein metric on the density manifold induces the Bures-Wasserstein metric for the Gaussian families . In our paper, however, we consider the induced Stein metric on \(\) or \(_{0}\) (we call it the **Gaussian-Stein metric**; for details see Appendix B).

**Different Bilinear Kernels and Induced Metrics.** There are several different bilinear kernels that appear in literature.  considers the simple bilinear kernel \(K_{1}(,)=^{}+1\) while  suggest the use of an affine-invariant bilinear kernel \(K_{2}(,)=(-)^{}(-)+1\).  further points out that with the rescaled affine-invariant kernel \(K_{3}(,)=(-)^{}^{-1}(-)+1\), the Gaussian-Stein metric magically coincides with the Bures-Wasserstein metric on \(\) (not true on the whole density manifold). Note that here \(\) and \(\) are the mean and covariance of the current mean-field Gaussian distribution which could change with time. Moreover,  proposed a regularized version of SVGD (R-SVGD) that interpolates the dynamics between WGF and SVGD. Interestingly R-SVGD with the affine-invariant kernel \(K_{2}\) for Gaussian families can be reformulated as Gaussian-SVGD with a new kernel \(K_{4}(,)=(-)^{}((1-)+ I)^{-1}(- )\), which interpolates between \(K_{2}\) and \(K_{3}\) (see Theorem 3.4). For clarity we present the results for \(K_{1}\) in the main article while leave the analogues for \(K_{2}\)-\(K_{4}\) in the appendix. We also point out that \(K_{1}\) and \(K_{2}\) are the same on \(_{0}\).

**Gaussian-Stein Variational Gradient Descent.** With a bilinear kernel and Gaussian targets, we will prove in the next subsection that the SVGD dynamics would remain Gaussian as long as the initializer is Gaussian. However, this is not true in more general situations especially when the target is non-Gaussian. Fortunately for Gaussian variational inference we can still consider the gradient flow restricted to the Gaussian submanifold. In general, we denote by \(G^{}_{}\) the canonical isomorphism on \(\) induced by \(G^{}_{}\), and define the Gaussian-Stein variational gradient descent as

\[_{t}=-(G^{}_{_{t}})^{-1}_{_{t}}\, (_{_{t}}^{*}),\]

where \(^{*}\) might not be a Gaussian density. Notably Gaussian-SVGD solves the following optimization problem

\[_{}(_{}^{*}),_{}(,),\]

via gradient descent under the Gaussian-Stein metric.

## 3 Dynamics of Gaussian-SVGD for Gaussian Targets

### Mean-Field Analysis from WGF to SVGD

The Wasserstein gradient flow (WGF) restricted to the general Gaussian family \(\) is known as the Bures-Wasserstein gradient flow . For consistency in this subsection we always set the initializer to be \((_{0},_{0})\) with density \(_{0}\) and target \((,Q)\) with density \(^{*}\) for the general Gaussian family. Then the WGF at any time \(t\) remains Gaussian, and can be fully characterized by the following dynamics of the mean \(_{t}\) and covariance matrix \(_{t}\):

\[}_{t}=-Q^{-1}(_{t}-),_{t}=2I -_{t}Q^{-1}-Q^{-1}_{t}. \]

For SVGD with bilinear kernels we have similar results:

**Theorem 3.1** (Svgd).: _For any \(t 0\) the solution \(_{t}\) of SVGD (2) with the bilinear kernel \(K_{1}\) remains a Gaussian density with mean \(_{t}\) and covariance matrix \(_{t}\) given by_

\[}_{t}=(I-Q^{-1}_{t})_{t}-(1+_{ t}^{}_{t})Q^{-1}(_{t}-)\\ _{t}=2_{t}-_{t}(_{t}+_{t}(_ {t}-)^{})Q^{-1}-Q^{-1}(_{t}+(_{t}-) _{t}^{})_{t}, \]

_which has a unique global solution on \([0,)\) given any \(_{0}^{d}\) and \(_{0}^{+}(d,)\). And \(_{t}\) converges weakly to \(^{*}\) as \(t\) at the following rates_

\[\|_{t}-\|=(e^{-2(-)t}),\|_{t} -Q\|=(e^{-2(-)t}),>0,\]

_where \(\) is the smallest eigenvalue of the matrix_

\[I_{d^{2}}&} Q^{-1/2}\\ }^{} Q^{-1/2}&(1+^{})Q^{-1}>^{}+2},\]

_where \(\) is the largest eigenvalue of \(Q\)._

Note that for any vector \(\), \(\|\|\) denotes its Euclidean norm and for any matrix \(A\) we use \(\|A\|\) for its spectral norm, \(\|A\|_{*}\) for the nuclear norm and \(\|A\|_{F}\) for the Frobenius norm. All matrix convergence are considered under the spectral norm in default for technical simplicity (though all matrix norms are equivalent in finite dimensions).

If we restrict to the centered Gaussian family where both the initializer and target have zero mean (setting \(_{0}==\)), the dynamics can further be simplified.

**Theorem 3.2** (Svgd for centered Gaussian).: _Let \(_{0}\) and \(^{*}\) be two centered Gaussian densities. Then for any \(t 0\) the solution \(_{t}\) of SVGD (2) with the bilinear kernel \(K_{1}\) or \(K_{2}\) remains a centered Gaussian density with the covariance matrix \(_{t}\) given by the following Riccati type equation:_

\[_{t}=2_{t}-_{t}^{2}Q^{-1}-Q^{-1}_{t}^{2}, \]

_which has a unique global solution on \([0,)\) given any \(_{0},Q^{+}(d,)\). If \(_{0}Q=Q_{0}\), we have the closed-form solution:_

\[_{t}^{-1}=e^{-2t}_{0}^{-1}+(1-e^{-2t})Q^{-1}. \]

_In particular, if we let \(_{0}=I\) and \(Q=I+^{}\) for some \(>0\) and \(^{d}\) such that \(^{}=1\), then \(_{t}\) can be rewritten as_

\[_{t}=I+)}{1+ e^{-2t}}^{}. \]

 shows that for WGF if \(_{0}Q=Q_{0}\), then we have \(\|_{t}-\|\)\(=(e^{-t/})\) and \(\|_{t}-Q\|=(e^{-2t/})\). For the centered Gaussian family SVGD converges faster if \(>1\). For the general Gaussian family WGF and SVGD have rather comparable rates (e.g., take \(\|\|\) then the lower bound here is roughly \((e^{-t/})\)). Another observation is that the WGF rates depend on \(Q\) alone but the SVGD rates here sometimes also depend on \(\), which breaks the affine invariance of the system. This is a problem originated from the choice of kernels as addressed in , where they propose to use \(K_{2}\) instead of \(K_{1}\). Such approach has both advantages and disadvantages. The convention in SVGD is that the kernel should not depend on the mean-field density because the density is usually unknown and changes with time. But for GVI the affine-invariant bilinear kernel \(K_{2}\) only requires estimating the means from Gaussian distributions and is not a big issue.

**Regularized Stein Variational Gradient Descent.** In Section 2 we show that SVGD can be regarded as WGF kernelized in the cotangent space \(T_{}^{*}(^{d})\). The regularized Stein variational gradient descent (R-SVGD)  interpolates WGF and SVGD by pulling back part of the kernelized gradient of the cotangent vector \(\), which is also seen as gradient flows under the regularized Stein metric:

**Definition 3.3** (Regularized Stein metric).: _The regularized Stein metric is induced by the following canonical map_

\[(G_{}^{})^{-1}:=-(\,((1-) _{K,}+ I)^{-1}_{K,}),\]

_where \(_{K,}\) is the kernelization operator given by \(_{K,}f:= K(,)f()()\, {y}\)._

The R-SVGD is defined as

\[_{t}=-(G_{_{t}}^{})^{-1}} (_{t}^{*})=(_{t}((1 -)_{K,_{t}}+ I)^{-1}_{K,_{t}} }{^{*}}). \]

**Theorem 3.4** (R-SVGD).: _Let \(_{0}\) and \(^{*}\) be two Gaussian densities. Then the solution \(_{t}\) of R-SVGD (8) with the bilinear kernel \(K_{2}\) converges to \(^{*}\) as \(t\), and \(_{t}\) is the density of \((,_{t})\) with_

\[}_{t}=-Q^{-1}(_{t}-)\\ _{t}=2((1-)_{t}+ I)^{-1}_{t}-((1-)_{t}+  I)^{-1}_{t}^{2}Q^{-1}-Q^{-1}((1-)_{t}+ I)^{-1}_{t} ^{2}. \]

_If \(_{0}Q=Q_{0}\), we have \(\|_{t}-Q\|=(e^{-2t/((1-)+)})\), where \(\) is the largest eigenvalue of \(Q\)._

From this theorem we see that R-SVGD can take the advantage of both regimes by choosing \(\) wisely. Another interesting connection is that on \(\) the induced regularized Stein metric coincides with the Stein metric with a different kernel \(K_{4}(,)=(-)^{}((1-)+ I)^{-1}(- )+1\) (see Theorem B.7).

**Stein AIG Flow.** Accelerating methods are widely used in first-order optimization algorithms and have attracted considerable interest in particle-based variational inference .  study the accelerated information gradient (AIG) flows as the analogue of Nesterov's accelerated gradient method  on the density manifold. Given a probability space \((^{d})\) with a metric tensor \(g_{}(,)\), let \(G_{}:T_{}(^{d}) T_{}^{*}( ^{d})\) be the corresponding isomorphism. The Hamiltonian flow in probability space  follows from

\[_{t}_{t}\\ _{t}=0&1\\ -1&0}( _{t},_{t})\\ }(_{t},_{t}),(_{t},_{t}):=_{t}G_{ _{t}}^{-1}_{t}\,+(^ {*})\]

is called the Hamiltonian function, which consists of a kinetic energy \( G_{}^{-1}\,\) and a potential energy \((^{*})\). Following  we introduce the accelerated information gradient flow in probability space. Let \(_{t} 0\) be a scalar function of time \(t\). We add a damping term \(_{t}_{t}\) to the Hamiltonian flow:

\[_{t}_{t}\\ _{t}=-0\\ _{t}_{t}+0&1\\ -1&0} (_{t},_{t})\\ }(_{t},_{t}), \]

By adopting the Stein metric we obtain the Stein AIG flow (S-AIGF):

\[_{t}=-(_{t}() K(,)_{t}()_{t}()\,)\\ _{t}=-_{t}_{t}-_{t}()^{}_{t }()K(,)_{t}()\,-}(_{t}^{*}). \]

Again we characterize the dynamics of S-AIGF with the linear kernel for the Gaussian family:

**Theorem 3.5** (S-Aigf).: _Let \(_{0}\) and \(^{*}\) be two centered Gaussian densities. Then the solution \(_{t}\) of S-AIGF (11) with the bilinear kernel \(K_{1}\) or \(K_{2}\) is the density of \((,_{t})\) where \(_{t}\) satisfies_

\[_{t}=2(S_{t}_{t}^{2}+_{t}^{2}S_{t})\\ _{t}=-_{t}S_{t}-2(S_{t}^{2}_{t}+_{t}S_{t}^{2})+ (_{t}^{-1}-Q^{-1}), \]

_where \(S_{t}(d,)\) with initial value \(S_{0}=0\)._

Note that the convergence properties of S-AIGF still remains open in contrast to the Wasserstein AIG flow (W-AIGF) as  shows that the W-AIGF for the centered Gaussian family is

\[_{t}=2(S_{t}_{t}+_{t}S_{t}),_{t}=- _{t}S_{t}-2S_{t}^{2}+(_{t}^{-1}-Q^{-1}),\]

and that if \(_{t}\) is well-chosen, the KL divergence in W-AIGF converges at the rate of \((e^{-t/})\). Thus, when \(\) is large it converges faster than WGF. It is also interesting to point out that the acceleration effect of Nesterov's scheme also comes from time discretization of the ODE system (see ) as it moves roughly \(\) rather than \(\) along the gradient path when the step size is \(\).

### Finite-Particle Systems

In this subsection, we consider the case where \(N<\) particles evolve in time \(t\). We set a Gaussian target \((,Q)\) (i.e., the potential is \(V()=(-)^{}Q^{-1}(-)\)) and run the SVGD algorithm with a bilinear kernel, and obtain the dynamics of \(_{1}^{(t)},,_{N}^{(t)}\). The continuous-time particle-based SVGD corresponds to the following deterministic interactive system in \(^{d}\):

\[}_{i}^{(t)}=_{j=1}^{N}_{_{j}}K( {x}_{i}^{(t)},_{j}^{(t)})-_{j=1}^{N}K(_ {i}^{(t)},_{j}^{(t)}) V(_{j}^{(t)}) \]

with initial particles given by \(_{i}^{(0)}\) (\(i=1,,N\)). Now denoting the sample mean and covariance matrix at time \(t\) by \(_{t}:=_{j=1}^{N}_{j}^{(t)}\) and \(C_{t}:=_{j=1}^{N}_{j}^{(t)}_{j}^{(t)}-_ {t}_{t}^{}\), we have the following theorem.

**Theorem 3.6** (Svgd).: _Suppose the initial particles satisfy that \(C_{0}\) is non-singular. Then SVGD (13) with the bilinear kernel \(K_{1}\) and Gaussian potential \(V\) has a unique solution given by_

\[_{i}^{(t)}=A_{t}(_{i}^{(0)}-_{0})+_{t}, \]

_where \(A_{t}\) is the unique (matrix) solution of the linear system_

\[_{t}=(I-Q^{-1}(C_{t}+_{t}_{t}^{})+Q^{-1} _{t}^{})A_{t}, A_{0}=I, \]

_and \(_{t}\) and \(C_{t}\) are the unique solution of the ODE system_

\[}_{t}=(I-Q^{-1}C_{t})_{t}-(1+_{t}^{ }_{t})Q^{-1}(_{t}-)\\ _{t}=2C_{t}-C_{t}(C_{t}+_{t}(_{t}-)^{} )Q^{-1}-Q^{-1}(C_{t}+(_{t}-)_{t}^{}) C_{t}. \]

The ODE system (16) is exactly the same as that in the density flow (4). Thus, we have the the same convergence rates as in Theorem 3.1. Theorem 3.6 can be interpreted as: At each time \(t\) the particle positions are a linear transformation of the initialization. On one hand, if we initialize _i.i.d._ from Gaussian, there is uniform in time convergence as shown in the theorem below.

On the other hand, if we initialize _i.i.d._ from a non-Gaussian distribution \(_{0}\). At each time \(t\) the mean field limit \(_{t}\) should be a linear transformation of \(_{0}\) and cannot converge to the Gaussian target \(^{*}\) as \(t\). Note that in general SVGD with the bilinear kernel might not always converge to the target distribution for nonparametric sampling but for GVI there is no such issue. This will be discussed in detail in Appendix C together with general results of well-posedness and mean-field convergence of SVGD with the bilinear kernel, which has not yet been studied in literature.

**Theorem 3.7** (Uniform in time convergence).: _Given the same setting as Theorem 3.6, further suppose the initial particles are drawn i.i.d. from \((_{0},_{0})\). Then there exists a constant \(C_{d,Q,,_{0},_{0}}\) such that for all \(t[0,]\), for all \(N 2\), with the empirical measure \(_{N}^{(t)}=_{i=1}^{N}_{_{i}^{(t)}}\), the second moment of Wasserstein-\(2\) distance between \(_{N}^{(t)}\) and \(_{t}\) converges:_

\[[_{2}^{2}(_{N}^{(t)},_{t})]  C_{d,Q,,_{0},_{0}}\{N^ {-1} N&d=1\\ N^{-1}( N)^{2}&d=2\\ N^{-2/d}&d 3.. \]

Similar to Theorem 3.2, we also provide the finite-particle result for a centered Gaussian target.

**Theorem 3.8** (SVGD for centered Gaussian).: _Suppose the SVGD particle system (13) with the bilinear kernel \(K_{1}\) or \(K_{2}\) is targeting a centered Gaussian distribution and initialized by \((_{i}^{(0)})_{i=1}^{N}\) such that \(_{0}=\) and \(C_{0}Q=QC_{0}\). Then we have the following closed-form solution_

\[_{i}^{(t)}=(e^{-2t}I+(1-e^{-2t})Q^{-1}C_{0})^{-1/2}_{i}^ {(0)}. \]

**Analogous Result for R-SVGD.** Next we consider the particle dynamics of R-SVGD. As shown in , the finite-particle system of R-SVGD is

\[_{t}=-((1-)}{N}+ I)^{-1}( }{N}_{t} V-_{j=1}^{N}_{t} K (_{j}^{(t)},\,\,)),\]

where \(X_{t}:=(_{1}^{(t)},,_{N}^{(t)})^{}\), \(_{t}f:=(f(_{i}^{(t)}),,f(_{N}^{(t)}))^{}\) for all \(f:^{d}^{d}\) and \((K_{t})_{ij}:=K(_{i}^{(t)},_{j}^{(t)})\) for all \(1 i,j N\). Similar to Theorem 3.6, we have the following result:

**Theorem 3.9** (R-Svgd).: _Suppose the R-SVGD system (13) with \(K_{1}\) or \(K_{2}\) is targeting a centered Gaussian distribution and initialized by \(_{i}^{(0)}_{i=1}^{N}\) such that \(_{0}=\). Then we have \(_{i}^{(t)}=A_{t}_{i}^{(0)}\) where \(A_{t}\) is the unique solution of the linear system_

\[_{t}=(I-Q^{-1}C_{t})((1-)C_{t}+ I)^{-1}A_{t}, A_{0}=I, \]

_and the sample covariance matrix \(C_{t}\) is given by_

\[_{t}=2((1-)C_{t}+ I)^{-1}C_{t}-((1-)C_{t}+ I)^{-1}C_{t}^{2} Q^{-1}-Q^{-1}((1-)C_{t}+ I)^{-1}C_{t}^{2}. \]

Again we observe that the particles at time \(t\) is a time-changing linear transformation of the initializers.

**Discrete-time Analysis for Finite Particles.** Next we consider the algorithm in discrete time \(t\). The SVGD updates according to the following equation:

\[_{i}^{(t+1)}=_{i}^{(t)}+ _{j=1}^{N}_{_{j}^{(t)}}K_{i}^{(t)},_{j}^{ (t)}-_{j=1}^{N}K_{i}^{(t)},_{j}^{(t)} _{_{j}^{(t)}}V_{j}^{(t)}. \]

For simplicity, we only consider the case where both the target and initializers are centered, i.e., \(=_{0}=\) and show the convergence:

**Theorem 3.10** (Discrete-time convergence).: _For a centered Gaussian target, suppose the particle system (21) with \(K_{1}\) or \(K_{2}\) is initialized by \(_{i}^{(0)}_{i=1}^{N}\) such that \(_{0}=\) and \(C_{0}Q=QC_{0}\). For \(0<<0.5\), we have \(_{t}=\) and \(\|C_{t}-Q\| 0\) as long as all the eigenvalues of \(Q^{-1}C_{0}\) lie in the interval \((0,1+1/)\)._

_Furthermore, if we set \(u_{}\) to be the smaller root of the equation \(f_{}^{}(u)=1-\) (it has \(2\) distinct roots) where \(f_{}(x):=(1+(1-x))^{2}x\), then we have linear convergence, i.e.,_

\[\|C_{t}-Q\|(1-)^{t}\|C_{0}-Q\| e^{- t}\|C_{0}-Q\|\]

_as long as all the eigenvalues of \(Q^{-1}C_{0}\) lie in the interval \([u_{},1/3+1/(3)]\)._

The above result illustrates that firstly the step sizes required are restricted by the largest eigenvalue of \(Q^{-1}C_{0}\). In particular if \(C_{0}=I_{d}\) then we need smaller step size if the smallest eigenvalue of \(Q\) is smaller, which corresponds to the \(\)-log-smoothness condition of the target distribution. Secondly we can potentially have faster convergence over iteration given larger step sizes. We believe that the commutativity assumption in Theorem 3.10 can be relaxed and similar results can be obtained for general targets. Detailed examinations are left as future work.

## 4 Beyond Gaussian Targets

In this section we consider the Gaussian-SVGD with a general target and have the following dynamics.

**Theorem 4.1**.: _Let \(^{*}\) be the density of the target distribution with the potential function \(V()\) that satsifies Assumption C.1 and \(_{0}\) be the density of \((_{0},_{0})\). The Gaussian-SVGD with \(K_{1}\) produces a Gaussian density \(_{t}\) with mean \(_{t}\) and covariance matrix \(_{t}\) given by_

\[}_{t}=(I-_{t}_{t})\,_{t}-(1+_{t}^{}_{t})_{t}\\ _{t}=2_{t}-_{t}(_{t}_{t}+_{t }_{t}^{})-(_{t}_{t}+_{t}_{t}^{ })_{t}, \]

_where \(_{t}=_{_{t}}[^{2}V()]\) and \(_{t}=_{_{t}}[ V()]\)._

_Furthermore, suppose that \(^{*}\) is the unique solution of the following optimization problem_

\[_{=(,)}(_{}^{*}),\; \;_{}\;\;(,).\]

_Then we have \(_{t}_{^{*}}(^{*},^{*})\) as \(t\)._

In particular, if the target is strongly log-concave, it gives rise to the following linear convergence:

**Theorem 4.2**.: _Assume that the target \(^{*}\) is \(\)-strongly log-concave and \(\)-log-smooth, i.e., \( I^{2}V() I\). Then \(_{t}\) of Theorem 4.1 converges to \(_{^{*}}\) at the following rate_

\[\|_{t}-^{*}\|=(e^{-(-)t}),\| _{t}-^{*}\|=(e^{-(-)t}), >0,\]

_where \(/\) is the smallest eigenvalue of the matrix_

\[I_{d}^{*}&^{*}(^{*})^{1/2}\\ ^{*}(^{*})^{1/2}&(1+^{*}^{*})I_{d }>^{* }^{*})+1}.\]

Typically the \(\)-log-smoothness condition is not required for continuous-time analyses. However, it is required in the above statement, as our proof technique is based on comparing the decay of the energy function of the flow to that for WGF, following . Relaxing this condition is interesting and we leave it as future work.

**Unifying Algorithms.** For general targets, we propose two unifying algorithm frameworks where we can choose any bilinear kernel (e.g., \(K_{1}\)-\(K_{4}\)) to solve GVI with SVGD. The first framework is density-based where we update \(_{t}\) and \(_{t}\) according to the mean-field dynamics. It requires the closed-form of the ODE system

\[}_{t}=F(_{t},_{t},_{t}, _{t})\\ _{t}=_{t}\ G(_{t},_{t},_{t},_{ t})+G(_{t},_{t},_{t},_{t})^{}\ _{t}, \]

where \(_{t}=_{_{t}}[ V()]\) and \(_{t}=_{_{t}}[^{2}V()]\), and \(F\) and \(G\) are some closed-form functions. For example \(F(_{t},_{t},_{t},_{t})=(I-_{t} _{t})\ _{t}-(1+_{t}^{}_{t})_{t}\) and \(G(_{t},_{t},_{t},_{t})=I-_{t} _{t}-_{t}_{t}^{}\) for \(K_{1}\) as shown in (22). Note that \(_{t}\) and \(_{t}\) can be estimated from samples using

\[}_{t}=_{k=1}^{N} V(_{k}^{(t)}), _{t}=_{k=1}^{N}^{2}V(_{k}^{(t )}), \]

or using the first-order estimator

\[_{t}=_{k=1}^{N} V(_{k}^{(t)})(_{k}^{(t)}-_{t})^{}_{t}^{-1}. \]

The second framework is particle-based and does not need the closed-form ODE of the mean and covariance, making it more flexible than Algorithm 1. Here we initially draw \(N\) particles and keep updating them over time using

\[_{i}^{(t+1)}=_{i}^{(t)}+_{j=1}^{N} _{_{j}^{(t)}}K_{i}^{(t)},_{j}^{(t)}-_ {j=1}^{N}K_{i}^{(t)},_{j}^{(t)} _{j}^{(t)}, \]

where \(\) is a (time-dependent) linear approximation of \( V\) defined as \(()=_{t}\ (-_{t})+}_{t}\). Intuitively this is used instead of \( V\) to ensure the Gaussianity of the particle system.

```  Draw \((_{i}^{(0)})_{i=1}^{N}\) from \((_{0},_{0})\) for\(t\) in \(0:T\)do \(_{t}_{k=1}^{N}_{k}^{(t)}\) \(_{t}_{k=1}^{N}_{k}^{(t)}_{k}^{(t) }-_{t}^{d}\), draw samples \(\{(X_{i},Y_{i})\}_{i=1}^{n}(^{d}\{0,1\})^{n}\) such that \(X_{i}}{}(,I_{d})\) and \(Y_{i} X_{i}((,X_{i})))\) where \(()\) is the logistic function. Given the samples \(\{(X_{i},Y_{i})\}_{i=1}^{n}\) and a uniform (improper) prior on \(\), the potential function of the posterior \(^{*}\) on \(\) is given by \(V()=_{i=1}^{n}((1+((,X_{i}))-Y_{i}(,X _{i})))\). We run both Algorithms 1 and 2 initialized at \(_{0}=(,I_{d})\) to find the \(_{^{*}}\) that minimizes \((_{}^{*})\). In Figure 1, **SBGD** (Simple Bilinear Gradient Descent), **GF** (Gaussian Flow ), **BVGD** (Bures-Wasserstein Gradient Descent ), and **RGF** (Regularized Gaussian Flow) are density-based algorithms with the bilinear kernels \(K_{1}\), \(K_{2}\), \(K_{3}\), and \(K_{4}\) (\(=0.5\)) respectively; **SBPF** (Simple Bilinear Particle Flow), **GPF** (Gaussian Particle Flow ), **BWPF** (Bures-Wasserstein Particle Flow), and **RGPF** (Regularized Gaussian Particle Flow) are particle-based algorithms with the bilinear kernels \(K_{1}\), \(K_{2}\), \(K_{3}\), and \(K_{4}\) (\(=0.5\)) respectively. We use the sample estimator of \((_{})=(_{}+V)\,_{}= (_{}^{*})+C\) (\(C\) is some constant) to evaluate the learned parameters \(=(,)\). For a fair comparison in Figure 0(b), we draw \(2000\) particles for particle-based algorithms and run all algorithms for \(2000\) iterations so that a total of \(2000\) samples are drawn for the density-based algorithms. The largest safe step sizes are \(0.02,0.1,2,0.8,0.02,0.2,4,4\).

Figure 1 shows the decay of \(}(_{})\) over time or iterations. For Figure 0(a) the same step size \(0.01\) is specified for all algorithms while for Figure 0(b) we choose the largest safe step size for each algorithm. In other words, Figure 0(a) provides the continuous-time flow of the dynamical system in each algorithm while Figure 0(b) emphasizes more on the discrete-time algorithmic behaviors. In Figure 0(a) there are roughly four distinct curves indicating four different kernels. \(K_{1}\) has the most rapid descent, followed by \(K_{2}\), \(K_{4}\), and \(K_{3}\).

From Figure 0(b) we observe that BWPF and RGPF are the better choices for practical use. The difference in the largest step sizes shows that in terms of stability \(K_{3}\) is the best, \(K_{4}\) is almost as stable, but \(K_{2}\) and \(K_{1}\) are much worse. We also observe that particle-based algorithms are consistently more stable than density-based counterparts (which are essentially stochastic gradient based). The superiority of particle-based algorithms are even more evident in the Gaussian mixture experiment where the target is multi-modal. We further remark that another recently proposed density-based algorithm, the FB-GVI  shows comparable performance to BWPF and RGPF with large step sizes. We conduct a comparison of these three algorithms in Appendix E but do not include it here for clarity. It would also be really interesting to study the particle-based analogue of FB-GVI as future works.

Figure 1: Convergence of Algorithms 1 and 2 with bilinear kernels in Bayesian logistic regression.