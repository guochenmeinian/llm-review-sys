# Multistep Distillation of Diffusion Models

via Moment Matching

 Tim Salimans Thomas Mensink Jonathan Heek Emiel Hoogeboom

{salimans,mensink,jheek,emielh}@google.com

Google DeepMind, Amsterdam

###### Abstract

We present a new method for making diffusion models faster to sample. The method distills many-step diffusion models into few-step models by matching conditional expectations of the clean data given noisy data along the sampling trajectory. Our approach extends recently proposed one-step methods to the multi-step case, and provides a new perspective by interpreting these approaches in terms of moment matching. By using up to 8 sampling steps, we obtain distilled models that outperform not only their one-step versions but also their original many-step teacher models, obtaining new state-of-the-art results on the Imagenet dataset. We also show promising results on a large text-to-image model where we achieve fast generation of high resolution images directly in image space, without needing autoencoders or upsamplers.

Figure 1: Selected 8-step samples from our distilled text-to-image model.

Introduction

Diffusion models (Ho et al., 2020; Song and Ermon, 2019; Sohl-Dickstein et al., 2015) have recently become the state-of-the-art model class for generating images, video, audio, and other modalities. By casting the generation of high dimensional outputs as an iterative denoising process, these models have made the problem of learning to synthesize complex outputs tractable. Although this decomposition simplifies the training objective compared to alternatives like GANs, it shifts the computational burden to inference: Sampling from diffusion models usually requires hundreds of neural network evaluations, making these models expensive to use in applications.

To reduce the cost of inference, recent work has moved towards _distilling_ diffusion models into generators that are faster to sample. The methods proposed so far can be subdivided into 2 classes: deterministic methods that aim to directly approximate the output of the iterative denoising process in fewer steps, and distributional methods that try to generate output with the same approximate distribution as learned by the diffusion model. Here we propose a new method for distilling diffusion models of the second type: We cast the problem of distribution matching in terms of matching conditional expectations of the clean data given the noisy data along the sampling trajectory of the diffusion process. The proposed method is closely related to previous approaches applying score matching with an auxiliary model to distilled one-step generators, but the moment matching perspective allows us to generalize these methods to the few-step setting where we obtain large improvements in output quality, even outperforming the many-step base models our distilled generators are learned from. Finally, the moment matching perspective allows us to also propose a second variant of our algorithm that eliminates the need for the auxiliary model in exchange for processing two independent minibatches per parameter update.

## 2 Background

### Diffusion Models

Diffusion models are trained by learning to invert a noise process that gradually destroys data from a clean data sample \(\) according to \(_{t}=_{t}+_{t}_{t}\) with \(_{t}(0,)\), where \(_{t},_{t}\) are monotonic functions of diffusion time \(t\). The coefficients \(_{t},_{t}\) may be specified in multiple equivalent ways. Here, we use the variance preserving specification (Ho et al., 2020) that has \(_{t}^{2}=1-_{t}^{2}\), \(_{0}=_{1}=1\), and \(_{1}=_{0}=0\), such that we have that \(_{0}=\) and \(_{1}(0,)\). When using a different specification of the noise process we can always convert to the variance preserving specification by rescaling the data. The quantity of importance is thus the signal-to-noise ratio: \((t)=_{t}^{2}/_{t}^{2}\), rather than the coefficients individually (Kingma et al., 2021). To invert the specified diffusion process, we can sample from the posterior distribution:

\[q(_{s}|_{t},)=(_{s}|_{t s}(_{t },),_{t s}), \] \[_{t s}^{2}=^{2}}+^{2}}{_{t s}^{2}} ^{-1}_{t s}=_{t s}^{2}}{_{t s}^{2}}_{t}+}{_{ s}^{2}}. \]

To sample from a learned diffusion model, we replace \(\) by a prediction from a neural network \(}=g_{}(_{t},t)\) that is fit to the data by minimizing \(_{t p(t),_{t}, q(_{t},)}w(t)\| -g_{}(_{t})\|^{2}\), with weighting function \(w(t)\) and where \(q(_{t},)\) denotes sampling \(\) from the data and then producing \(_{t}\) by forward diffusion. The sampling process starts with pure noise \(_{1}(0,)\) and iteratively denoises the data according to \(q(_{s}|_{t},})\) for a discrete number of timesteps \(k\), following Algorithm 1.

If we attain the optimal solution \(}=[|_{t}]\) and let \(k\) the sampling process becomes exact, then the learned diffusion model can be shown to be a universal distribution approximator (Song et al., 2021b). To get close to this ideal, \(k\) typically needs to be quite large, making diffusion models a very computationally expensive class of models (Luccioni et al., 2023).

### Generalized method of moments

An alternative to the well-known maximum likelihood estimation method is the method of moments, also known as moment matching. Traditionally for univariate distributions, one matches moments \(m_{k}=_{x p_{X}}[x^{k}]\) of a random variable \(X\). The canonical example is a Gaussian distribution, which is defined by the first two moments (i.e. the mean and variance) and all (centered) higher order momentsare zero. Fitting a distribution by setting its moments equal to the moments of the data is then a consistent parameter estimation method, and can be readily extended to multivariate distributions, e.g. by matching the mean and covariance matrix for a multivariate Gaussian.

One can generalize the method of moments to arbitrary high dimensional functions \(f:^{d}^{k}\) and match the moment vector \(\) as defined by: \(=_{ p_{X}}[f()]\), which is called the _Generalized_ Method of Moments (GMM, Hansen (1982)). Matching such moments can be done by minimizing a distance between the moments such as \(||_{ p_{}}f()-_{ p_{X}}f( )||^{2}\) where \(p_{}\) is the generative model and \(p_{X}\) the data distribution. The distillation method we propose in the next section can be interpreted as a special case of this class of estimation methods.

## 3 Moment Matching Distillation

Many-step sampling from diffusion models starts by initializing noisy data \(_{1} N(0,)\), which is then iteratively refined by predicting the clean data using \(}=g_{}(_{t},t)\), and sampling a slightly less noisy data point \(_{s} q(_{s}|_{t},})\) for new timestep \(s<t\), until the final sample is obtained at \(s=0\), as described is described in Algorithm 1. If \(}=_{q}[|_{t}]\) this procedure is guaranteed to sample from the data distribution \(q()\) if the number of sampling steps grows infinitely large. Here we aim to achieve a similar result while taking many fewer sampling steps than would normally be required. To achieve this we finetune our denoising model \(g_{}\) into a new model \(g_{}(_{t},t)\) which we sample from using the same algorithm, but with a strongly reduced number of sampling steps \(k\), for say \(1 k 8\).

To make our model produce accurate samples for a small number of sampling steps \(k\), the goal is now no longer for \(}=g_{}(_{t},t)\) to approximate the expectation \(_{q}[|_{t}]\) but rather to produce an approximate sample from this distribution. In particular, if \(} q(|_{t})\) then Algorithm 1 produces exact samples from the data distribution \(q\) for any choice of the number of sampling steps. If \(g_{}\) perfectly approximates \(q(|_{t})\) as intended, we have that

\[_{ q(),_{t} q( _{t}|),} g_{}(_{t}), _{s} q(_{s}|_{t},})}[}|_{s}] = _{ q(),_{s} q( _{s}|)}[}|_{s}]\] \[_{g}[}|_{s}] = _{q}[}|_{s}]. \]

In words: The conditional expectation of clean data should be identical between the data distribution \(q\) and the sampling distribution \(g\) of the distilled model.

Equation 3 gives us a set of moment conditions that uniquely identifies the target distribution, similar to how the regular diffusion training loss identifies the data distribution (Song et al., 2021). These moment conditions can be used as the basis of a distillation method to finetune \(g_{}(_{t},t)\) from the denoising model \(g_{}\). In particular, we can fit \(g_{}\) to \(q\) by minimizing the L2-distance between these moments:

\[()=_{g(_{s})}||_{g}[ {}|_{s}]-_{q}[|_{s}]||^{2}. \]

In practice, we evaluate the moments using a sample \(_{s}\) from our generator distribution, but do not incorporate its dependence on the parameters \(\) when calculating gradients of the loss. This decision is purely empirical, as we find it results in more stable training compared to using the full gradient. The approximate gradient of \(()\) is then given by

\[(_{}_{g}[}|_{s}])^{T} (_{g}[}|_{s}]-_{q}[| _{s}])+_{}(_{q}[ }|_{s}]^{T}_{q}[}|_{s}]) (_{}})^{T}(_{g}[}|_{s}]-_{q}[|_{s}]), \]where we approximate the first expectation using a single Monte-Carlo sample \(}\) and where the second term is zero as it does not depend on \(g_{}\). Following this approximate gradient is then equivalent to minimizing the loss

\[L()=_{_{t} q(_{t}),} g _{}(_{t}),_{s} q(_{s}|_{t}, })}[}^{T}(_{g}[}|_{s}]-_{q}[|_{s}])], \]

where sg denotes stop-gradient. This loss is minimized if \(_{g}[}|_{s}]=_{q}[| _{s}]\) as required. Unfortunately, the expectation \(_{g}[}|_{s}]\) is not analytically available, which makes the direct application of Equation 6 impossible. We therefore explore two variations on this moment matching procedure: In Section 3.1 we approximate \(_{g}[}|_{s}]\) by a second denoising model, and in Section 3.2 we instead apply moment matching directly in parameter space rather than \(\)-space.

### Alternating optimization of the moment matching objective

Our first approach to calculating the moment matching objective in equation 6 is to approximate \(_{g}[}|_{s}]\) with an auxiliary denoising model \(g_{}\) trained using a standard diffusion loss on samples from our generator model \(g_{}\). We then update \(g_{}\) and \(g_{}\) in alternating steps, resulting in Algorithm 2.

```
0: Pretrained denoising model \(g_{}(_{t})\), generator \(g_{}\) to distill, auxiliary denoising model \(g_{}\), number of sampling steps \(k\), time sampling distribution \(p(s)\), loss weight \(w(s)\), and dataset \(\). for\(n\) = 0:N do  Sample target time \(s p(s)\), sample time delta \(_{t} U[0,1/k]\).  Set sampling time \(t=(s+_{t},1)\).  Sample clean data from \(\) and do forward diffusion to produce \(_{t}\).  Sample \(_{s}\) from the distilled generator using \(}=g_{}(_{t}),_{s} q(_{ s}|_{t},})\). if\(n\) is even then  Minimize \(L()=w(s)\{\|}-g_{}(_{s})\|^{2}+\|g_{}( _{s})-g_{}(_{s})\|^{2}\}\) w.r.t. \(\) else  Minimize \(L()=w(s)}^{T}[g_{}(_{s})-g_{ }(_{s})]\) w.r.t. \(\) endif endfor
```

**Algorithm 2** Moment matching algorithm with _alternating_ optimization of generator \(g_{}\) and auxiliary denoising model \(g_{}\).

Here we have chosen to train our generator \(g_{}\) on all continuous times \(t(0,1]\) even though at inference time (Algorithm 1) we only evaluate on \(k\) discrete timesteps. Similarly we train with randomly sampled time delta \(_{t}\) rather than fixing this to a single value. These choices were found to increase the stability and performance of the proposed algorithm. Further, we optimize \(g_{}\) not just to predict the sampled data \(}\) but also regularize it to stay close to the teacher model \(g_{}\): On convergence this would cause \(g_{}\) to predict the average of \(}\) and \(g_{}\), which has the effect of multiplying the generator loss \(L()\) by \(1/2\) compared to the loss we introduced in Equation 6.

The resulting algorithm resembles the alternating optimization of a GAN (Goodfellow et al., 2020), and like a GAN is generally not guaranteed to converge. In practice, we find that Algorithm 2 is stable for the right choice of hyperparameters, especially when taking \(k 8\) sampling steps. The algorithm also closely resembles _Variational Score Distillation_ as previously used for distilling 1-step generators \(g_{}\) in _Diff-Instruct_. We discuss this relationship in Section 4.

### Parameter-space moment matching

Alternating optimization of the moment matching objective (Algorithm 2) is difficult to analyze theoretically, and the requirement to keep track of two different models adds engineering complexity. We therefore also experiment with an _instantaneous_ version of the auxiliary denoising model \(g_{^{*}}\), where \(^{*}\) is determined using a single infinitesimal gradient descent step on \(L()\) (defined in Algorithm 2), evaluated on a single minibatch. Starting from teacher parameters \(\), and preconditioning the loss gradient with a pre-determined scaling matrix \(\), we can define:

\[()-_{}L()|_{=}, ^{*}=_{ 0}(). \]Now we \(()\) in calculating \(L()\) from Algorithm 2, take the first-order Taylor expansion for \(g_{()}(_{s})-g_{}(_{s})(_{s})}{}(()-)= (_{s})}{}_ {}L()|_{=}\), and scale the loss with the inverse of \(\) to get:

\[L_{}()=_{ 0}L_{( )}()=w(s)}^{T}(_{s})}{}_{}L()|_{= }}, \]

where \((_{s})}{}\) is the Jacobian of \(g_{}\), and where \(_{}L()\) is evaluated on an independent minibatch from \(}\) and \(_{s}\). In modern frameworks for automatic differentiation, like JAX (Bradbury et al., 2018), the quantity within the curly braces can be most easily expressed using specialized functions for calculating Jacobian-vector products.

The loss can now equivalently be expressed as performing moment matching in teacher-parameter space rather than \(\)-space. Denoting \(L_{}(,_{s}) w(s)\|-g_{}( _{s})\|^{2}\), and letting \(_{}()=_{}()+\), we have (as derived fully in Appendix A):

\[_{}()  \|_{_{t} q,} =g_{}(_{t}),_{s} q(_{s}|_{t },})}_{}L_{}(},( _{s}))\|_{}^{2} \] \[= \|_{_{t} q,} =g_{}(_{t}),_{s} q(_{t}|_{t },})}_{}L_{}(},( _{s}))-_{,^{}_{s} q}_{ }L_{}(,^{}_{s})\|_{}^{2},\]

where the gradient of the teacher training loss is zero when sampling from the training distribution, \(_{,^{}_{s} q}_{}L_{}( ,^{}_{s})=0\), if the teacher attained a minimum of its training loss.

The instantaneous version of our moment matching loss can thus be interpreted as trying to match teacher gradients between the training data and generated data. This makes it a special case of the _Efficient Method of Moments_(Gallant & Tauchen, 1996), a classic method in statistics where a teacher model \(p_{}\) is first estimated using maximum likelihood, after which its gradient is used to define a moment matching loss for learning a second model \(g_{}\). Under certain conditions, the second model then attains the statistical efficiency of the maximum likelihood teacher model. The difference between our version of this method and that proposed by Gallant & Tauchen (1996) is that in our case the loss of the teacher model is a weighted denoising loss, rather than the log-likelihood of the data.

The moment matching loss \(_{}()\) is minimized if the teacher model has zero loss gradient when evaluated on data generated by the distilled student model \(g_{}\). In other words, optimization is successful if the teacher model cannot see the difference between real and generated data and would not change its parameters when trained on the generated data. We summarize the practical implementation of moment matching in parameter-space in Algorithm 3 and Figure 2.

```
0: Pretrained denoising model \(g_{}(_{t})\), generator \(g_{}\) to distill, gradient scaling matrix \(\), number of sampling steps \(k\), time sampling distribution \(p(s)\), loss weight \(w(s)\), and dataset \(\). for\(n=0\):N do  Sample target time \(s p(s)\), sample time delta \(_{t} U[0,1/k]\).  Set sampling time \(t=(s+_{t},1)\).  Sample two independent batches of data from \(\) and do forward diffusion to produce \(_{t},^{}_{t}\).  For both batches sample \(_{s},^{}_{s}\) from the distilled generator using \(}=g_{}(_{t}),_{s} q(_ {s}|_{t},})\).  Evaluate teacher gradient on one batch: \(=_{}L_{}(}^{},^ {}_{s})\)  On the other batch, minimize \(L_{}()=w(s)}^{T}\{(_{s})}{}\}\) w.r.t. \(\) endfor
```

**Algorithm 3** Parameter-space moment matching algorithm with _instant_ denoising model \(g_{^{*}}\).

Figure 2: Visualization of Algorithm 3: Moment matching in parameter space starts with applying forward diffusion to data from our dataset, mapping this to clean samples using the distilled generator model, and then minimizes the gradient of the teacher loss on this generated data.

### Hyperparameter choices

In our choice of hyperparameters we choose to stick as closely as possible to the values recommended in EDM (Karras et al., 2022), some of which were also used in Diff-Instruct (Luo et al., 2024) and DMD (Yin et al., 2023). We use the EDM test time noise schedule for \(p(s)\), as well as their training loss weighting for \(w(s)\), but we shift all log-signal-to-noise ratios with the resolution of the data following Hoogeboom et al. (2023). For our gradient preconditioner \(\), as used in Section 3.2, we use the preconditioner defined in Adam (Kingma and Ba, 2014), which can be loaded from the teacher checkpoint or calculated fresh by running a few training steps before starting distillation. During distillation, \(\) is not updated.

To get stable results for small numbers of sampling steps (\(k=1,2\)) we find that we need to use a weighting function \(w(s)\) with less emphasis on high-signal (low \(s\)) data than in the EDM weighting. Using a flat weight \(w(s)=1\) or the adaptive weight from DMD (Yin et al., 2023) works well.

As with previous methods, it's possible to enable classifier-free guidance (Ho and Salimans, 2022) when evaluating the teacher model \(g_{}\). We find that guidance is typically not necessary if output quality is measured by FID, though it does increase Inception Score and CLIP score. To enable classifier-free guidance and prediction clipping for the teacher model in Algorithm 3, we need to define how to take gradients through these modifications: Here we find that a simple straight-through approximation works well, using the backward pass of the unmodified teacher model.

## 4 Related Work

In the case of one-step sampling, our method in Algorithm 2 is a special case of _Variational Score Distillation_, _Diff-Instruct_, and related methods (Wang et al., 2024; Luo et al., 2024; Yin et al., 2023; Nguyen and Tran, 2023) which distill a diffusion model by approximately minimizing the KL divergence between the distilled generator and the teacher model:

\[L() = _{p(s)}[w(s)D_{}(p_{}(_{s})|p_ {}(_{s}))] \] \[ _{p(s),p_{}(_{s})}[(w(s)/2_{s}^{2}) (\|_{s}-[}_{}(_{s})] \|^{2}-\|_{s}-[}_{}(_{ s})]\|^{2})]\] (12) \[= _{p(t),p_{}(_{s})}[(w(s)_{s}^{2}/ _{s}^{2})}^{T}[}_{}( _{s})-}_{}(_{s})]]+ \]

Here sg again denotes stop gradient, \(p_{}(_{s})\) is defined by sampling \(}=g_{}(_{1})\) with \(_{1} N(0,)\), and \(_{s} q(_{s}|})\) is sampled using forward diffusion starting from \(}\). The auxiliary denoising model \(}_{}\) is fit by minimizing \(_{g_{}}(_{s})\|}-}_{}(_{s})\|^{2}\), which can be interpreted as score matching because \(_{s}\) is sampled using forward diffusion started from \(}\). In our proposed algorithm, we sample \(_{s}\) from the conditional distribution \(q(_{s}|},_{t})\): If \(_{t}=_{1} N(0,)\) is assumed to be fully independent of \(}\), i.e. that \(_{1}^{2}/_{1}^{2}=0\), we have that \(q(_{s}|},_{1})=q(_{s}|})\) so the two methods are indeed the same. However, this correspondence does not extend to the multi-step case: When we sample \(_{s}\) from \(q(_{s}|_{t},})\) for \(_{t}^{2}/_{t}^{2}>0\), fitting \(}_{}\) through minimizing \(_{g_{}}(_{s})\|}-}_{}(_{s})\|^{2}\) no longer corresponds to score matching. One could imagine fitting \(}_{}\) through score matching against the conditional distribution \(q(_{s}|_{t},})\) but this did not work well when we tried it (see Appendix D for more detail). Instead, our moment matching perspective offers a justification for extending this class of distillation methods to the multistep case without changing the way we fit \(}_{}\). Indeed, we find that moment matching distillation also works when using deterministic samplers like DDIM (Song et al., 2021) which also do not fit with the score matching perspective.

In addition to the one-step distillation methods based on score matching, our method is also closely related to adversarial multistep distillation methods, such as Xiao et al. (2021) and Xu et al. (2023) which use the same conditional \(q(_{s}|_{t},})\) we use. These methods train a discriminator model to tell apart data generated from the distilled model (\(g_{}\)) from data generated from the base model (\(g_{}\)). This discriminator is then used to define an adversarial divergence which is minimized w.r.t. \(g_{}\):

\[L()=_{t p(t),_{t} q(_{t},t)}D_{ }(p_{}(_{t})|p_{}(_{t})). \]

The methods differ in their exact formulation of the adversarial divergence \(D_{}\), in the sampling of time steps, and in the use of additional losses. For example Xu et al. (2023) train unconditional discriminators \(D_{}(,t)\) and decompose the adversarial objective in a marginal (used in the discriminator) and a conditional distribution approximated with an additional regression model. Xiao et al. (2021) instead use a conditional discriminator of the form \(D_{}(,_{t},t)\).

Experiments

We evaluate our proposed methods in the class-conditional generation setting on the ImageNet dataset (Deng et al., 2009), which is the most well-established benchmark for comparing image quality. On this dataset we also run several ablations to show the effect of classifier-free guidance and other hyperparameter choices on our method. Finally, we present an experiment with a large text-to-image model to show our approach can also be scaled to this setting.

### Class-conditional generation on ImageNet

We begin by evaluating on class-conditional ImageNet generation, at the \(64 64\) and \(128 128\) resolutions (Tables 1 and 2). Our results here are for a relatively small model with 400 million parameters based on _Simple Diffusion_(Hoogeboom et al., 2023). We distill our models for a maximum of 200,000 steps at batch size 2048, calculating FID every 5,000 steps. We report the optimal FID seen during the distillation process, keeping evaluation data and random seeds fixed across evaluations to minimize bias.

For our base models we report results with slight classifier-free guidance of \(w=0.1\), which gives the optimal FID. We also use an optimized amount of sampling noise, following Salimans & Ho (2022), which is slightly higher compared to equation 2. For our distilled models we obtained better results without classifier-free guidance, and we use standard ancestral sampling without tuning the sampling noise. We use identical hyperparameters across all our experiments. We compare against various distillation methods from the literature, including both distillation methods that produce deterministic samplers (progressive distillation, consistency distillation) and stochastic samplers (Diff-Instruct, adversarial methods).

Ranking the different methods by FID, we find that our moment matching distillation method is especially competitive when using \(8+\) sampling steps, where it sets new state-of-the-art results, beating out even the best undistilled models using more than 1000 sampling steps, as well as its teacher model. For 1 sampling step some of the other methods show better results: Further improvement is likely possible by separately optimizing hyperparameters for this setting. For \(8+\) sampling steps we get similar results for our alternating optimization version (Section 3.1) and the instant 2-batch version (Section 3.2) of our method. For fewer sampling steps, the alternating version performs better.

We find that our distilled models also perform very well in terms of Inception Score (Salimans et al., 2016) even though we did not optimize for this. By using classifier-free guidance the Inception Score can be improved further, as we show in Section 5.3.

  
**Method** & **\# param** & **NFE** & **FID\(\)** & **IS\(\)** \\  VDM++ (Kingma \& Gao, 2023) & 2B & 1024 & 1.75 & 171 \\ our base model & 400M & 1024 & 1.76 & 194 \\  PD (Salimans \& Ho, 2022) & 400M & 2 & 8.0 & \\ (reimpl. from Heek et al. (2024)) & & 4 & 3.8 & \\  & & & 8 & 2.5 & 162 \\ MultiStep-CD (Heek et al., 2024) & 1.2B & 1 & 7.0 & \\  & & & 2 & 3.1 & \\  & & & 2 & 3.1 & \\  & & & 4 & 1.72 & 184 \\  & & & 8 & 1.49 & 184 \\ Instant (c.f. Sect. 3.2) & & & 4 & 3.48 & **232** \\  & & & 8 & 1.54 & 183 \\   

Table 2: Results on ImageNet 128x128

  
**Method** & **\# param** & **NFE** & **FID\(\)** & **IS\(\)** \\  VDM++ (Kingma \& Gao, 2023) & 2B & 1024 & 1.43 & 64 \\ RNN (Jabir et al., 2023) & 281M & 1000 & 1.23 & 67 \\ our base model & 400M & 1024 & 1.42 & 84 \\  DDIM (Song et al., 2021a) & & 10 & 18.7 & \\ TRACT (Berthelot et al., 2023) & & 1 & 7.43 & \\  & & & 2 & 4.97 \\  & & & 4 & 2.93 \\  & & & 8 & 2.41 \\ CD (LPIPS) (Song et al., 2023) & & 1 & 6.20 & \\  & & & 2 & 4.70 \\  & & & 3 & 4.32 \\ iCT-deep (Song \& Dhariwal, 2023) & & 1 & 3.25 \\  & & & 2 & 2.77 \\ PD (Salimans \& Ho, 2022) & 400M & 1 & 10.7 \\ (reimpl. from Heck et al. (2024)) & & & 2 & 4.7 \\  & & & 4 & 2.4 \\  & & & 8 & 1.7 & 63 \\ MultiStep-CD (Heek et al., 2024) & 1.2B & 1 & 3.2 & 2 \\  & & & 2 & 1.9 \\  & & & 4 & 1.6 \\  & & & 8 & 1.4 & 73 \\ CTM (Kim et al., 2024) & & & 2 & 1.73 & 64 \\ DMD (Yin et al., 2023) & & & 1 & 2.62 \\ Diff-Instruct (Luo et al., 2023) & & 1 & 5.57 & \\ 
**Moment Matching** & 400M & & & & \\ Alternating (c.f. Sect. 3.1) & & 1 & 3.0 & 89 \\  & & & 2 & 3.86 & 60 \\  & & & 4 & 1.50 & 75 \\  & & & 8 & **1.24** & 78 \\ Instant (c.f. Sect. 3.2) & & 4 & 3.4 & **98** \\  & & & 8 & 1.35 & 81 \\   

Table 1: Results on ImageNet 64x64.

**How can a distilled model improve upon its teacher?** On Imagenet our distilled diffusion model with 8 sampling steps and no classifier-free guidance outperforms its 512-step teacher with optimized guidance level, for both the \(64 64\) and \(128 128\) resolution. This result might be surprising since the many-step teacher model is often seen as the gold standard for sampling quality. However, even the teacher model has prediction error that makes it possible to improve upon it. In theory, predictions of the clean data at different diffusion times are all linked and should be mutually consistent, but since the diffusion model is implemented with an unconstrained neural network this generally will not be the case in practice. Prediction errors will thus be different across timesteps which opens up the possibility of improving the results by averaging over these predictions in the right way. Different sampling algorithms average over these predictions differently, as shown in Appendix E, which offers scope for improvement.

Similarly, prediction error will not be constant over the model inputs \(_{t}\), and biasing generation away from areas of large error could also yield sampling improvements. Although many-step ancestral sampling typically gives good results, and is often better than deterministic samplers like DDIM, it's not necessarily optimal. In future work we hope to study the improvement of moment matching over our base sampler in further detail, and test our hypotheses about its causes.

### Ablating conditional sampling

The distilled generator in our proposed method samples from the conditional \(q(_{s}|},_{t})\), whereas existing distillation methods based on score matching typically don't condition on \(_{t}\). Instead they apply noise independently, mirroring the forward diffusion process used during training the original model. When using a 1-step sampling setup, the two approaches are equivalent since any intermediate \(_{s}\) will be independent from the starting point \(_{1}\) if that point has zero signal-to-noise. In the multistep setup the two approaches are meaningfully different however, and sampling from the conditional \(q(_{s}|},_{t})\) or the marginal \(q(_{s}|})\) are both valid choices. We ablate our choice of conditioning on \(_{t}\) versus applying noise independently, and find that conditioning leads to much better sample diversity in the distilled model, as shown in Figure 3.

### Effect of classifier-free guidance

Our distillation method can be used with or without guidance. For the alternating optimization version of our method we only apply guidance in the teacher model, but not in the generator or auxiliary denoising model. For the instant 2-batch version we apply guidance and clipping to the teacher model and then calculate its gradient with a straight through approximation. Exper

Figure 3: Multistep distillation results for a single Imagenet class obtained with two different methods of sampling from the generator during distillation: Conditional \(q(_{s}|},_{t})\), and unconditional \(q(_{s}|})\). Our choice of sampling from the conditional yields much better sample diversity.

imenting with different levels of guidance, we find that increasing guidance typically increases Inception Score and CLIP Score, while reducing FID, as shown in the adjacent figure.

### Distillation loss is informative for moment matching

A unique advantage of the instant 2-batch version of our moment matching approach is that, unlike most other distillation methods, it has a simple loss function (equation 9) that is minimized without adversarial techniques, bootstrapping, or other tricks. This means that the value of the loss is useful for monitoring the progress of the distillation algorithm. We show this for Imagenet \(128 128\) in the adjacent figure: The typical behavior we see is that the loss tends to go up slightly for the first few optimization steps, after which it exponentially falls to zero with increasing number of parameter updates.

### Text to image

To investigate our proposed method's potential to scale to large text-to-image models we train a pixel-space model (no encoder/decoder) on a licensed dataset of text-image pairs at a resolution of \(512 512\), using the UViT model and shifted noise schedule from Simple Diffusion (Hoogeboom et al., 2023) and using a T5 XXL text encoder following Imagen (Saharia et al., 2022). We compare the performance of our base model against an 8-step distilled model obtained with our moment matching method. In Table 3 we report zero-shot FID (Heusel et al., 2017) and CLIP Score (Radford et al., 2021) on MS-COCO (Lin et al., 2014): Also in this setting we find that our distilled model with alternating optimization exceeds the metrics for our base model. The instant 2-batch version of our algorithm performs somewhat less well at 8 sampling steps. Samples from our distilled text-to-image model are shown in Figure 1 and in Figure 7 in the appendix.

## 6 Conclusion

We presented _Moment Matching Distillation_, a method for making diffusion models faster to sample. The method distills many-step diffusion models into few-step models by matching conditional expectations of the clean data given noisy data along the sampling trajectory. The moment matching framework provides a new perspective on related recently proposed distillation methods and allows us to extend these methods to the multi-step setting. Using multiple sampling steps, our distilled models consistently outperform their one-step versions, and often even exceed their many-step teachers, setting new state-of-the-art results on the Imagenet dataset. However, automated metrics of image quality are highly imperfect, and in future work we plan to run a full set of human evaluations on the outputs of our distilled models to complement the metrics reported here.

We presented two different versions of our algorithm: One based on alternating updates of a distilled generator and an auxiliary denoising model, and another using two minibatches to allow only updating the generator. In future work we intend to further explore the space of algorithms spanned by these choices, and gain additional insight into the costs and benefits of both approaches.

    & & & COCO & CLIP \\ Method & NFE & guidance & FID\({}_{30k}\) & Score \(\) \\  our base model & 512 & 0 & 9.6 & 0.290 \\  & 512 & 0.5 & 7.9 & 0.305 \\  & 512 & 3 & 12.7 & 0.315 \\  & 512 & 5 & 13.4 & 0.316 \\ StableDiffusion v1.5\({}^{*}\) & 512 & low & 8.78 & & \\ (Rombach et al., 2022) & 512 & high & 13.5 & **0.322** \\  DMD & 1 & low & 11.5 & & \\ (Vin et al., 2023) & 1 & high & 14.9 & 0.32 \\ UFOGen & 1 & & 12.8 & 0.311 \\ (Xu et al., 2023b) & 1 & & 16.67 & 0.29 \\ SwiftBrush (Nguyen \& Tan, 2023) & & & 11.8 & 0.309 \\ InstaFlow-1.7B & 1 & & 11.3 & & \\ (Liu et al., 2023) & & & & \\ DeRFlow (Yan et al., 2024) & & & & \\ 
**Moment Matching** & & & & \\ Alternating (Sec. 3.1) & 8 & 0 & **7.25** & 0.297 \\  & 8 & 3 & 14.15 & 0.319 \\ Instant (Sec. 3.2) & 8 & 0 & 9.5 & 0.300 \\  & 8 & 3 & 19.0 & 0.306 \\   

* Reported results for StableDiffusion v1.5 are from Yin et al. (2023).

Table 3: Results on text-to-image, \(512 512\).