# Instructor-inspired Machine Learning for Robust Molecular Property Prediction

Fang Wu\({}^{1}\), Shuting Jin\({}^{2}\), Siyuan Li\({}^{3}\), Stan Z. Li\({}^{3}\)

\({}^{1}\) Computer Science Department, Stanford University

\({}^{2}\) School of Computer Science and Technology, Wuhan University of Science and Technology

\({}^{3}\) School of Engineering, Westlake University

Corresponding Author, email: fangwu97@stanford.edu.

###### Abstract

Machine learning catalyzes a revolution in chemical and biological science. However, its efficacy heavily depends on the availability of labeled data, and annotating biochemical data is extremely laborious. To surmount this data sparsity challenge, we present an instructive learning algorithm named InstructMol to measure pseudo-labels' reliability and help the target model leverage large-scale unlabeled data. InstructMol does not require transferring knowledge between multiple domains, which avoids the potential gap between the pretraining and fine-tuning stages. We demonstrated the high accuracy of InstructMol on several real-world molecular datasets and out-of-distribution (OOD) benchmarks.

## 1 Introduction

An enduring obstacle in applied chemical and biological sciences is identifying and making chemical compounds or materials that have optimal properties for a given purpose . However, the vast majority of progress in these areas is still achieved primarily through time-consuming and costly trial-and-error experiments. Machine learning (ML) has undergone an unparalleled technological advancement, opening up a myriad of applications across various domains. Its potential is particularly notable in expediting the discovery and development of novel materials, pharmaceuticals, and chemical processes . However, ML models' efficacy heavily relies on the availability of labeled data and the consistency of prediction targets. Meanwhile, the cost of generating new data labels through wet experiments is prohibitively high. Consequently, the size of labeled data in this field is several orders of magnitude lower than the one that can inspire breakthroughs in other ML fields . This data scarcity severely hampers ML in addressing scientific challenges within this realm, impeding its ability to generalize to new molecules .

Biochemists have noticed this problem and propose several strategies to overcome the low data limitation (view Fig. 1). Firstly, inspired by the remarkable success of self-supervised learning from NLP  and CV , researchers apply the pretrain and finetune paradigm  to molecule modeling. They boost the performance of various molecular models by pretraining them on massive unlabeled data. However, this benefit can be negligible when a large gap exists between the sample distributions of pretraining and downstream tasks . An alternative option is to use active learning , which iteratively selects new data points to annotate according to the current model's predictions. However, auxiliary labor is still required to enrich the original database. Thirdly, domain knowledge is incorporated to enhance molecular representations, such as providing more high-quality hand-crafted features , constructing motif-based hierarchical molecular graphs , and leveraging knowledge graphs . However, domain knowledge can be biased and is difficult to integrate into different training techniques universally.

In this study, we develop InstructMol, a flexible semi-supervised learning (SSL) approach to excavate the abundant unlabeled biochemical data for robust molecular property prediction. It differs from prior studies in two aspects: (1) it utilizes an additional instructor model to predict the confidence of the predicted label, measure its reliability, and generate pseudo-label information for unannotated data; (2) with the help of pseudo-label information guiding the model, the target molecular model can reliably utilize unlabeled data without the need to transfer knowledge between multiple domains, which perfectly avoids the potential gap between pre-training and fine-tuning stages. We demonstrate that InstructMol outpasses existing SSL approaches and achieves state-of-the-art performance in MoleculeNet and several OOD benchmark datasets. Besides, via InstructMol, we accurately predict the properties of all 9 newly discovered drug molecules in the latest patent (ZA202303678A). Extensive experiments showcase the effectiveness of our model in surmounting the challenge of data scarcity, propelling advancements in the chemistry and biology domains.

## 2 Related Work

How to exploit large-scale unlabeled molecular data becomes an essential topic in the ML community to alleviate the scarcity of labeled data and improve OOD generalization, where pretraining and SSL are the two major fast-growing lines. The former traditionally employ unsupervised techniques to pretrain ML models, such as autoencoder , autoregressive modeling [13; 21], masked component modeling [22; 23], context-prediction , contrastive learning , and multi-modality . Despite several progress claims, the benefits of self-supervised pretraining can be negligible in many cases . Recent years have witnessed a rising interest in developing SSL to reduce the amount of required labeled data . Several hypotheses have been discussed in the literature to support specific SSL design decisions , such as the smoothness and manifold assumptions. Existing SSL algorithms can be roughly separated into three sorts: consistency regularization, proxy-label methods, and generative models. The consistency regularization is based on the simple concept that randomness

Figure 1: Four mainstream paradigms to ameliorate the scarcity of labeled biochemical data. (A) Self-supervised pretraining tasks include masked component modeling, contrastive learning, and auto-encoding. (B) Active learning involves the iterative selection of the most informative data, in which the molecular models are the most uncertain. These samples are then subjected to laboratory testing to determine their labels. This process is repeated with newly labeled data added to the training set. (C) Knowledge graphs are introduced to provide structured relations among multiple drugs and unstructured semantic relations associated with different drug molecules. (D) In SSL, the unlabeled data is used to create a smooth decision boundary between different classes or to estimate the distribution of the input data, while the labeled data is used to provide specific examples of the correct output.

within the neural network (NNs) such as dropout) or data augmentation transformations should not modify model predictions given the same input and impose an auxiliary loss. This line of research includes \(\)-model , temporal ensembling , and mean teachers  The proxy-label methods regard proxy labels on unlabeled data as targets and consist of two groups: self-training , where the model itself produces the proxy labels, and multi-view learning , where proxy labels are produced by models trained on different views of the data. The generative models rely on VAE  and GAN  to capture the joint distribution more accurately.

## 3 Preliminaries and Background

**Task formulation.** Suppose we have access to some labeled molecular data \(=\{(x_{i},y_{i})\}_{i=1}^{N}\). \(x_{i}\) can be in any kind of formats such as 1D sequences, 2D graphs, and 3D structures. \(y_{i}\) can be any discrete (_e.g._, toxicity, and drug reaction) or continuous properties (_e.g._, water solubility, and free energy). Here we consider continuous property for simple illustration, but our approach can be easily extended to binary-class or multi-label circumstances. The target molecular model \(f:\) can be any category of architectures, including Transformers, GNNs, and geometric NNs. There are also some unseen data \(^{test}\) to evaluate the performance of the learned model. Traditionally, \(\) is divided into the train and validation sets as \(^{train}=\{(x_{i}^{train},y_{i}^{train})\}_{i =1}^{N_{1}}\) and \(^{val}=\{(x_{i}^{val},y_{i}^{val})\}_{i=1}^ {N_{2}}\).

Besides, we can obtain some unlabeled data points, denoted as \(^{}=\{x_{i}^{}\}_{i=1}^{M}\), where the number of unlabeled data \(M\) is usually orders of magnitude larger than that of labeled data \(N\).

**Challenge of proxy-labeling.** Labeled data \(\) and unlabeled data \(^{}\) sometimes follow significantly different data distributions, _i.e._, \((x_{i},y_{i})(x_{i}^{},y_{i}^{})\). However, many SSL algorithms  assume no distributional shift between \(\) and \(^{}\). They are likely only to reinforce the consistent information in the labeled data \(\) to unlabeled examples \(^{}\) instead of mining auxiliary information from \(^{}\), without exception for proxy labeling . More importantly, despite the versatility and modality-agnostic of proxy labeling, it achieves relatively poor performance compared to recent SSL approaches . This arises because some pseudo-labels \(_{i}^{}\) can be severely incorrect during training due to the poor generalization ability of classic DL models . If we directly utilize pseudo-labels from a previously learned model for subsequent training, the conformance-biased information in the proceeding epochs could increase confidence in erroneous predictions and eventually lead to a vicious circle of error accumulation . The situation can be even worse when labeled data \(\) contain noises because of unavoidable experimental errors. Accordingly, it becomes essential to

Figure 2: The outline of InstructMol. We utilize a pre-trained target molecular model to forecast the properties of unlabeled examples as pseudo-labels. Then, an instructor model predicts the confidence of those pseudo-annotations, which are leveraged to guide the target molecular model to distribute different attention in inferring different data points.

grasp the quality and dependability of pseudo-annotations and intelligently select a subset of them to reduce the hidden noise.

**Motivation of InstructMol.** Though confidence is crucial for pseudo-label selection and this selection reduces pseudo-label error rates, NNs' poor calibration renders this solution insufficient. Explicitly, incorrect predictions in poorly calibrated NNs can also have high confidence (_i.e._, \(_{i}^{*} 0\) or \(_{i}^{*} 1\)) . More importantly, prior studies such as UPS  resort to the target model's output \(_{i}\) as the confidence indicator and produce hard labels by \(y_{i}^{*}=[_{i}^{*}_{1}]\) or \(y_{i}^{*}=[_{i}^{*}_{2}]\), where \(_{1}\) and \(_{2}\) are pre-defined thresholds. Nonetheless, this selection mechanism becomes inapplicable if \(\) is a continuous label space, as networks no longer output class probabilities. For regression tasks, \(_{i}^{*}\) discloses no confidential information, and numerous biochemical problems are regression-based, including molecular property prediction [43; 44; 45], 3D structure prediction , and binding affinity prediction . This brings a challenge for probabilistic output-based proxy-labeling algorithms . Thus, instead of depending on \(_{i}^{*}\) to judge proxy labels' reliability, we accompany the target molecular model \(f\) with an additional instructor model \(g\), which plays the role of a critic and predicts label observability, _i.e._, whether the label is true or fake. \(g\) disentangles the confidence prediction and the property prediction, reducing the noise introduced by the pseudo-labeling process.

## 4 Method

**The Overall Instructive Learning Framework.** We separate the integral workflow of InstructMol into two phases. In the first step, we retain pseudo-labels \(\{_{i}^{*}\}_{i=1}^{M}=f(\{_{i}^{*}\}_{i=1}^{M})\). There are several approaches to creating proxy labels, such as label propagation via neighborhood graphs . Here, we require the molecular model \(f\) to directly annotate samples in the unlabeled dataset \(^{*}\). Then, in the following step, we construct a new dataset with both labeled and pseudo-labeled samples as \(^{}=\{(x_{i}^{*},_{i}^{*})\}_{i=1}^{M}\) and proceed training both the target molecular model \(f\) and the instructor model \(g\) based on this new set. These two procedures are iteratively repeated until \(f\) reaches the optimal performance on the validation set \(^{val}\).

To be specific, the instructor model \(g:(_{f})\) forecasts the confidence measure \(p_{i}\) (\(0 p 1\)) of whether the given label \(y_{i}^{}\) belongs to the ground-truth label set \(\{y_{i}\}_{i=1}^{N}\) or the pseudo-label set \(\{y_{i}^{*}\}_{i=1}^{M}\). It digests three items: the data sample with its label \((x_{i}^{},y_{i}^{})^{}\) and an additional loss term \(_{f}(f(x_{i}^{}),y_{i}^{})\), where \(_{f}\) is traditionally selected as a root-mean-squared-error (RMSE) loss or a mean-absolute-error (MAE) loss for regression tasks and cross-entropy (CE) loss for classification problems. Here we regard \(_{f}(.)\) as the ingredient of \(g\)'s input to provide more information about the main molecular property prediction task. At last, the instructor model \(g\) is supervised via a binary CE loss (BCE) as:

\[_{g}(^{},\{_{i}^{}\}_{i=1}^{N+M} )=_{(x_{i}^{},y_{i}^{})^{}}(p_{i}^{},c_{i})=_{(x_{i}^{},y_{i}^{})^{ }}gx_{i}^{},y_{i}^{},_{f} (_{i}^{},y_{i}^{}),c_{i}, \]

where \(c_{i}\) is an integer and represents the observability mask. It indicates whether \(y_{i}\) is pseudo-labeled (\(c_{i}=0\)) or not (\(c_{i}=1\)). However, since the number of unlabeled data \(M\) is much larger than the number of labeled data \(N\), it is proper to shift the loss function from BCE to a focal loss  (FL) for unbalanced classes as: \(_{g}(^{},\{_{i}^{}\}_{i=1}^{N+M} )=_{(x_{i}^{},y_{i}^{})^{}}(p_{i}^{},c_{i})=_{(x_{i}^{},y_{i}^{})^{ }}-(1-p_{i}^{})^{}(p_{i}^{})\), where \( 0\) is a tunable focusing parameter.

Meanwhile, the target molecular model \(f\) receives the discriminative information \(\{p_{i}\}_{i=1}^{N+M}\) from the instructor model \(g\) and uses it to reweight the importance of different samples in backpropagating its gradient. In other words, the instructor model \(g\) guides the target model \(f\) to deliver different attention to different labels so that correct labels are attached more importance while erroneous labels are ignored. This can be realized by specially designing the loss of the target model \(f\) as:

\[_{f}(^{},\{p_{i}\}_{i=1}^{N+M})= _{(x_{i},y_{i})}_{f}(f(x_{i}),y_{i})+_{ (x_{j}^{*},_{j}^{*})^{*}}(2p_{j}-1) _{f}(f(x_{j}^{*}),_{j}^{*}), \]

where \(0 1\) is a hyper-parameter to balance the dominance of labeled and unlabeled data sets. \(_{f}(.)\) transforms the original main task into a cost-sensitive learning problem  by imposing a group of soft-labeling weights based on the predicted confidence of data labels. That is, for pseudo-labeled samples (_i.e._, \(x^{}_{j}^{}\)), the soft-labeling weight becomes \(2p_{j}-1\).

This loss format in Equation 2 induces different behaviors on the loss \(_{f}(.)\) of labeled and pseudo-labeled instances, where the judgment \(\{p_{i}\}_{i=1}^{N+M}\) produced by the instructor model \(g\) is leveraged to differentiate their informativeness. Notably, if the instructor model \(g\) regards a pseudo-label \(^{}_{j}\) to be unreliable (_i.e._, \(0.5>p_{j}>0\)), the loss \(_{f}(.)\) chooses to enlarge the gap with the pseudo-label \(^{}_{j}\). Meanwhile, once the instructor \(g\) fully trusts the pseudo-label \(^{}_{j}\), InstructMol forces the target molecular model \(f\) to give more effort to inferring this proxy-labeled sample. Generally, the more likely a pseudo-label \(^{}_{j}\) is considered reliable by the instructor model \(g\) (_i.e._, \(p_{j} 1\)), the stronger it drives the target molecular model \(f\) to make further improvement on inferring this label \(^{}_{j}\). Otherwise, InstructMol will push \(f\) to overturn its previous belief if \(p_{j} 0\). Noticeably, we can also impose a soft-labeling weight for samples with true labels (_i.e._, \(x^{}_{i}\)). For instance, a weight factor of \( 1\) can navigate \(f\) to concentrate more on samples that are more trusted by \(g\). But we practically discover no significant refinement with this design on labeled data and leave it for future exploration. The whole pseudo-code of InstructMol is depicted in Algorithm 1.

```
1:Input: target model \(f\), instructor model \(g\), labeled data \(\), unlabeled data \(^{}\), pseudo-label update frequency \(k\), loss weight \(\)
2:Initialize and pretrain a target model \(f_{0}\) and an instructor model \(g_{0}\)
3:for epochs \(n=0,1,2,...\)do
4:if\(n k==0\)then
5: With no gradient:
6:\(^{}_{i} f(x^{}_{i}), x^{}_{i} ^{}\)\(\) Iteratively assign pseudo-labels every \(k\) epochs
7:endif
8:\(^{}\{(x^{}_{i};^{}_ {i})\}_{i=1}^{M}\)\(\) Construct the hybrid database
9:\(^{}_{i} f(x^{}_{i}) x^{}_{i} ^{}\)
10:\(p_{i} g(x^{}_{i},y^{}_{i},_{f}(.)), (x^{}_{i},y^{}_{i})^{}\)\(\) Predict the confidence scores
11:\(_{g}(^{},\{^{}_{i}\}_{i=1}^{N+M})\)\(\) Equation 1
12:\(_{f}(^{},\{p_{i}\}_{i=1}^{N+M})\)\(\) Equation 2
13: Update the parameters of \(f\) and \(g\) based on \(_{g}(.)\) and \(_{f}(.)\)
14:endfor
```

**Algorithm 1** InstructMol Algorithm

**Loss Selection for InstructMol.** We compare some relevant methods from the literature under the proxy-labeling SSL algorithms in Appendix Tab. 6, containing the vanilla proxy-labeling (PL), curriculum learning for PL (CPL) , UPS, and self-interested coalitional learning (SCL) . It is structured in three main columns that describe the selection for unlabeled samples, loss function, and fitness for regression problems. Except for SCL and InstructMol, all approaches adopt a subset of unannotated instances for training rather than utilizing the entire unlabeled datasets. SCL can take advantage of all data points from \(^{}\), but its main limitation is its improper or even severely wrong loss function design. As the confidence score is close to 1, SCL assigns a strong negative multiplier factor as \(1-}-\) and forces the target model \(f\) to disregard this label, which is actually reliable. On the contrary, when the instructor model \(g\) doubts the reliability of \(^{}_{j}\), the ratio \(1-}\) becomes \(1-\), driving the target model \(f\) to move towards it.

**Guidelines for InstructMol.** Before executing InstructMol, it is natural to first obtain a well-trained molecular target model \(f_{0}\) through regular supervised learning on \(\) and then initialize an instructor \(g_{0}\) by discriminating pseudo-labels that are generated by \(f_{0}\). This is empirically proven to achieve higher training stability and robustness. Moreover, pseudo-labels are assigned every \(k\) epoch and a proper setting of \(k\) is critical to the success of InstructMol. If pseudo-labels are updated too frequently, the training procedure will be volatile at the very beginning. While a too-large \(k\) would significantly increase the training complexity. Here we adopt an adaptive decay strategy: with an initial value \(k_{0}\), the update frequency decreases by a factor of 0.5 until it reaches the minimum threshold \(k_{}=3\).

Analysis of InstructMol.After the curation of \(^{}\), there are two distinct learning tasks during the second stage of InstructMol. Specifically, the target model follows the regular routine to predict the molecular properties. Meanwhile, the instructor model strives to differentiate whether the label 

[MISSING_PAGE_FAIL:6]

ratio of 8:1:1. We report the mean and standard deviation of the results for three random seeds. More experimental details are put in Appendix A.2.

Baselines.Several SSL baselines are chosen including consistency regularization, proxy-label methods, and generative models. \(\)**-model** applies different stochastic transformations (_i.e._, dropout) to the networks instead of the input graphs. **Semi-GAN** introduces a discriminator to classify whether the input is labeled or not. Uncertainty-aware pseudo-label selection (**UPS**)  leverages the prediction uncertainty to guide the pseudo-label selection procedure but is merely applicable to classification problems.

Results.Tab. 1 shows that InstructMol significantly improves various ML architectures and outperforms all SSL baselines. For GIN, GAT, and GCN, it leads to an average increase in ROC-AUC of 8.6%, 7.1%, and 6.6%, respectively, for six classification tasks and an average decrease in RMSE of 9.3%, 8.0%, and 7.7% separately for three regression tasks. These statistics demonstrate our approach effectively boosts existing ML models in low-data circumstances for molecular scaffold property prediction, as most datasets in MoleculeNet have only thousands of labeled samples. Besides that, more up-to-date ML models like GIN enjoy stronger benefits of our InstructMol than primitive ones like GCN. It is also worth mentioning that InstructMol overcomes UPS's drawbacks and can be utilized for regression tasks. All evidence clarifies that InstructMol is a more advanced proxy labeling algorithm than the existing SSL mechanisms with stronger virtual screening capacity and broader applications.

### OOD Generalization

Data and Setsups.Measuring OOD generalization is particularly relevant in molecular property prediction, where distributional shifts can be large and difficult to handle for ML models. Different molecular datasets obtained by distinct pharmaceutical companies and research groups often contain compounds from vastly different areas of chemical space that exhibit high structural heterogeneity. Towards this goal, we leverage the Graph Out-of-Distributio (GOOD) benchmark , where **GOOD-HIV** is a small-scale dataset for HIV replication inhibition and **GOOD-PCBA** includes 128 bioassays and forms 128 binary classification tasks. They are divided into a training set, an in-domain (ID) validation set, an ID test set, and OOD test sets by covariate and concept shift splits.

Baselines.The empirical risk minimization (**ERM**) and several OOD algorithms are considered. **IRM** searches for data representations that perform well across all environments by penalizing feature distributions with different optimal linear classifiers for each environment. **VREx** targets both covariate robustness and the invariant prediction. **GroupDRO** tackles the problem that the distribution minority lacks sufficient training. **DANN** adversarially trains the regular classifier and a domain classifier to make features indistinguishable. **Deep Coral** encourages features in different domains to be similar by minimizing the deviation of covariant matrices from different

    &  &  \\  Datasets & BBBP & BACE & CliaTox & Tox21 & ToxCast & SIDER & ESOL & FreeSolv & Lipo \\ \# Molecules & 2039 & 1513 & 1478 & 7831 & 8575 & 1427 & 1128 & 642 & 4200 \\ \# Tasks & 1 & 1 & 2 & 12 & 617 & 27 & 1 & 1 & 1 \\   & & & & & & & & & \\ D-MPNN  & \(71.0(0.3)\) & \(80.9(0.6)\) & \(90.6(0.6)\) & \(75.9(0.7)\) & \(65.5(0.3)\) & \(57.0(0.7)\) & \(1.050(0.008)\) & \(2.082(0.082)\) & \(0.683(0.016)\) \\ Attribute FP  & \(64.3(1.8)\) & \(78.4(0.02)\) & \(84.7(0.3domains. **Q-SAVI** encodes explicit prior knowledge of the data-generating process into a prior distribution over functions. **DIR** and **Mixup** are two graph-specific OOD methods.

Results.Tab. 2 documents the statistics, where most OOD algorithms have comparable performance with ERM. The risk interpolation methods like GroupDRO and the risk extrapolation mechanisms like VREx perform favorably against others on multiple shift splits. In contrast, InstrutMol significantly exceeds ERM and other OOD baselines in all circumstances. Specifically, InstrutMol brings improvements of 2.40%, 2.05%, 7.68%, and 4.69% for GOOD-HIV and GOOD-PCBA's different ID splits, and 3.62%, 6.72%, 9.83%, and 7.51% for GOOD-HIV and GOOD-PCBA's different OOD splits. It can be seen that gains for OOD are greater than benefits for ID, indicating the promise of pseudo-labeling to tackle OOD generalization.

### SSL with Self-supervised Learning

Setups and Background.Pretraining and SSL are not mutually exclusive but can collaborate for more robust scientific investigations . So we design a two-step workflow: (1) In the pretraining stages, unlabeled data is first used in a task-agnostic way, and we attain more general molecular representations. Then, those general representations are adapted for a specific task for fine-tuning. (2) In the instructive learning stage, unlabeled data is used again in a task-specific way via InstructMol. We combine GEM  and InstructMol and examine their joint effectiveness on MoleculeNet.

Baselines.Multiple baselines are selected for a thorough comparison. D-MPNN , MGCN  and AttentiveFP  are supervised GNN methods. N-gram , PretrainGNN , InfoGraph , GPT-GNN , GROVER , 3D-Infomax , GraphMVP , MolCLR , and Uni-Mol  are pretraining methods. We adopt the same scaffold splitting strategy as GEM and Uni-Mol with three repeated runs.

Results.The overall performance of InstructMol based on GEM and other baseline methods is summarized in Tab. 3. InstructMol achieves new SOTA results on all MoleculeNet tasks and brings an average improvement of 4.35%. Notably, its benefits are much stronger for regression tasks with a mean decrease of 9.98% in RMSE. Another set of results on MoleculeNet with a different splitting method is in Appendix Tab. 5, where InstructMol also outperforms all baselines. This highlights the necessity and importance of leveraging unlabeled examples to refine and transfer task-specific knowledge after pretraining through instructive learning. It also implies that InstructMol is not incompatible with existing pre-training ML models but can effectively supplement them with an additional instructor model.

Figure 3: The scatter plot of the distributions of LogP predictions for unlabeled data with and without InstructMol. The first row includes predictions before instructive learning, and the second row includes predictions after instructive learning.

### Discussion, Ablation, and Other Applications

**Empirical evidence of pseudo-labels' accuracy.** Pseudo-labels' accuracy is crucial, as it reflects InstructMol's ability to generalize to unseen molecules. However, no ground-truth annotations exist for unlabeled data across the mentioned tasks. To address this issue, we adopt partition-coefficient values (LogP) as the target. A key predictor of drug-likeness featured in the famous "Lipinski's rule of five," LogP can quickly screen large libraries of potential therapeutics using computational tools like XLopf. We utilize a public Kaggle dataset of 14.6k molecules with associated LogP values for training and evaluate the predictions on the much larger ZINC15, whose LogP is computed using RDKit. Furthermore, measurements often number in the tens rather than thousands in the low-data regime of drug discovery. To assess InstructMol's limits, we significantly reduce the training size and investigate its efficacy with only 0.1%, 0.5%, 1%, 5%, and 10% of the entire training samples, resulting in scaffold-split training sets of 14, 73, 146, 730, and 1,326 molecules, respectively.

Fig. 3 compares the LogP prediction distributions before and after instructive learning. It can be observed that DL models trained solely on labeled data perform poorly in estimating unseen molecules' LogP with an average RMSE of 5.63. Contrarily, InstructMol enables a pretty accurate prediction of LogP with an average RMSE of 0.63 even with a very limited number of training examples, verifying the robustness of pseudo-labels.

**Ablation Studies.** We further investigate the effects of the number of unannotated molecules on the performance of InstructMol. As shown in Figure 4, the enrichment of unlabeled data consistently brings benefits to various downstream tasks.

**Real-world Drug Discovery.** We retrieved the latest patent ZA202303678A targeting the 5-HT1A receptor to examine InstructMol in real-world applications. ZA202303678A is published after 2023, and the property test standards of new compounds in ZA202303678A are consistent with those recorded in the CHEMBL214_Ki dataset . All 9 new small molecule drugs are marked as good binders by InstructMol. Since the patent ZA202303678A provides accurate ground truth values of \(K_{i}\) through wet experiments, we compare the predicted values with the real ones as shown in Appendix 7. It can be observed that the errors are, at most, within four times, and most predicted results are close to real values. Among them, predicted \(K_{i}\) of (S)-5-FPT (Ground truth: \(K_{i}=4\), Prediction: \(K_{i}=3.71\)), (S)-5-NaT (Ground truth: \(K_{i}=64\), Prediction: \(K_{i}=61.86\)), (S)-5-CPPT (Ground truth: \(K_{i}=0.6\), Prediction: \(K_{i}=0.93\)), and (S)-5-FPyT (Ground truth: \(K_{i}=1.3\), Prediction:\(K_{i}=1.29\)) is almost equal to actual \(K_{i}\) obtained from biological experiments.

Figure 4: The influence of unlabeled data size on four tasks.

Figure 5: The distributions of confidence scores given by the instructor model during the training process.

**Instructor model's Behavior.** The instructor estimates the target model's uncertainty. Therefore, thoroughly evaluating its judgment and contribution is beneficial. Here, we analyze its output value distribution on labeled and unlabeled data in different training stages of an activity cliff estimation task (CHEMBL214_Ki). The plots in Fig. 5 demonstrate the transition of an instructor model. Notably, since we pretrain the instructor before SSL, it performs well in distinguishing fake labels but remains confused with real ones. From the first iterations to 10K iterations, the instructor gradually gains a stronger capacity to discriminate true labels (confidence score \(\) 1.0) and fake ones (confidence score \(\) 0.0). Taking a step further, we draw the distribution of the target model's output in Fig. 6 and show a significant overlap between predictions of labeled and unlabeled data. This undoubtedly excludes the hypothesis that the instructor discriminates labeled and unlabeled data simply based on distinct distributions of their predictions. Even though predictions of labeled and unlabeled data are highly similar, the instructor still succeeds in comprehending their uncertainty and guides the target model to leverage pseudo-labels more cautiously. The instructor model's interpretability is a byproduct of our InstructMol and can be useful for many real-world biochemical problems. Here we give the relevant appendix figures cited in the manuscript.

## 6 Conclusion

This paper presents InstructMol, a novel instructive learning framework, to alleviate the difficulty of experimentally obtaining the ground truth properties of molecular data and to overcome the limitation of the small number of labeled biochemical data points. InstructMol sufficiently cutting-edge pretraining methods for molecular representation, and addresses essential real-world problems.

Limitations.Despite the great progress of InstructMol in achieving enhanced molecular learning capacity, there are still minor limitations that require future exploration. For instance, our combination of InstructMol with self-supervised mechanisms is based on existing methodologies such as GEM, Uni-Mol, and GROVE. It is interesting to develop a more suitable self-supervised learning algorithm that can be better aligned with InstructMol.

Acknowledgments.This work was supported by the National Natural Science Foundation of China (No.62402351), the Hubei Provincial Natural Science Foundation of China (No.2024 AFB275), and the Scientific Research Project of Education Department of Hubei Province (No.Q20231109).