# Cross-Care: Assessing the Healthcare Implications of Pre-training Data on Language Model Bias

Shan Chen\({}^{1,2,3}\), Jack Gallifant\({}^{4}\), Mingye Gao\({}^{4}\), Pedro Moreira\({}^{4,10}\), Nikolaj Munch\({}^{4,5}\),

**Ajay Muthukkumar\({}^{6}\)**, **Arvind Rajan\({}^{6}\)**, **Jaya Kolluri\({}^{2}\)**, **Amelia Fiske\({}^{7}\)**

**Janna Hastings\({}^{8}\)**, **Hugo Aerts\({}^{1,2,9}\), **Brian Anthony\({}^{4}\)**, **Leo Anthony Celi\({}^{1,2,4,11}\),**

**William G. La Cava\({}^{1,3}\)**, **Danielle S. Bitterman\({}^{1,2,3}\)

\({}^{1}\)Harvard, \({}^{2}\)Mass General Brigham, \({}^{3}\)Boston Children's Hospital, \({}^{4}\)MIT,

\({}^{5}\)Aarhus University, \({}^{6}\)University of North Carolina, \({}^{7}\)Technical University of Munich,

\({}^{8}\)University of Zurich and University of St. Gallen, \({}^{9}\)Maastricht University,

\({}^{10}\)Universitat Pompeu Fabra, \({}^{11}\)Beth Israel Deaconess Medical Center

Co-first authors: Shan Chen and Jack GallifantCo-second authors: Mingye Gao and Pedro MoreiraCorresponding author: dbitterman@bwh.harvard.edu

###### Abstract

Large language models (LLMs) are increasingly essential in processing natural languages, yet their application is frequently compromised by biases and inaccuracies originating in their training data. In this study, we introduce **Cross-Care**, the first benchmark framework dedicated to assessing biases and real world knowledge in LLMs, specifically focusing on the representation of disease prevalence across diverse demographic groups. We systematically evaluate how demographic biases embedded in pre-training corpora like \(ThePile\) influence the outputs of LLMs. We expose and quantify discrepancies by juxtaposing these biases against actual disease prevalences in various U.S. demographic groups. Our results highlight substantial misalignment between LLM representation of disease prevalence and real disease prevalence rates across demographic subgroups, indicating a pronounced risk of bias propagation and a lack of real-world grounding for medical applications of LLMs. Furthermore, we observe that various alignment methods minimally resolve inconsistencies in the models' representation of disease prevalence across different languages. For further exploration and analysis, we make all data and a data visualization tool available at: www.crosscare.net.

## 1 Introduction

Large language models (LLMs) enabled transformative progress in many applications . Benchmarks to assess language models, such as \(GLUE\) and \(SuperGLUE\), are instrumental in evaluating general language understanding and complex task performance. However, as LLMs are increasingly applied in diverse domains, challenges of domain knowledge grounding , safety , hallucinations , and bias  have emerged as important issues that are inadequately assessed by existing benchmarks. These problems are magnified in high-stakes domains like healthcare, given the potential for biased or inaccurate outputs  to influence disparities in health care and outcomes.

This paper investigates **representational biases in LLMs, focusing on medical information**. Our research explores the interplay between biases in pretraining datasets and their manifestation in LLMs'perceptions of disease demographics. Existing bias metrics in the general domain have currently relied on human-annotated examples and focused on overt stigmatization and prejudices [31; 22; 23]. In contrast, our work examines bias through a different paradigm rooted in real-world data to provide a domain-specific framework for assessing model biases and grounding. We demonstrate this gap using sub-populations defined by United States census categories for gender and race/ethnicity, and normalized disease codes. While these categorizations are necessarily simplistic and imperfect, we contend that the fact that inconsistency is consistently observed across model architectures, model sizes, subgroups, and diseases means that these findings are meaningful and broadly relevant. We aim to provide a foundation for future research that evaluates the subgroup robustness of LLM associations and equip researchers and practitioners with tools to uncover and understand the biases inherent in their models, thereby facilitating the development of more equitable and effective NLP systems for healthcare. Our full workflow can be found in Figure 1.

Specifically, our work makes the following key contributions:

1. **We conduct a quantitative analysis of the co-occurrences between demographic subgroups and disease keywords** in prominent pretraining datasets like _The Pile_, releasing their counts publicly.
2. **We evaluate model logits across various architectures, sizes, and alignment methods** using ten prompt template variants to test robustness to disease-demographic subgroup pairs. Our findings reveal that representational differences in pretraining datasets across diseases align with these logits, irrespective of model size and architecture.
3. **We benchmark model-derived associations against real-world disease prevalences** to highlight discrepancies between model perceptions and actual epidemiological data. Additionally, we compare these associations across different languages (Chinese, English, French, and Spanish) to emphasize discrepancies across languages.
4. **We provide a publicly accessible web app**, www.crosscare.net, for exploring these data and downloading specific counts, logits, and associations for further research in interpretability, robustness, and fairness.

## 2 Related Work

### Language model biases arise from pretraining data

The sheer breadth of data sources consumed by LLMs enables the emergence of impressive capabilities across a wide range of tasks . However, this expansive data consumption has its pitfalls, as while LLM performance generally improves as models are scaled, this improvement is not uniformly distributed across all domains . Furthermore, it can lead to the phenomenon of 'bias exhaust'--the inadvertent propagation of biases present in the pretraining data. The propensity of LLMs to inherit and perpetuate societal biases observed in their training datasets is a well-documented concern in current LLM training methodologies [34; 35; 26; 36; 37]. Efforts to mitigate this issue through the careful selection of "_clean_" data have significantly reduced toxicity and biases [38; 39], underscoring the link between the choice of pretraining corpora and the resultant behaviors of the models. Furthermore, recent studies have elucidated the impact of pretraining data selection on the manifestation of political biases at the task level .

### Evaluating language model biases

The evaluation of biases in NLP has evolved to distinguish between intrinsic and extrinsic assessments. Intrinsic evaluations focus on the inherent properties of the model, while extrinsic evaluations measure biases in the context of specific tasks. This distinction has become increasingly blurred with advancements in language modeling, such as fine-tuning and in-context learning, expanding the scope of what is considered "intrinsic" .

In the era of static word embedding models, such as word2vec and fastText, intrinsic evaluations were confined to metrics over the embedding space. Unlike static word embedding models, LLMs feature dynamic embeddings that change with context and are inherently capable of next-word prediction, a task that can be applied to numerous objectives. To evaluate bias in LLMs, Guo and Caliskan  developed the Contextualized Embedding Association Test, an extension of the Word Embedding Association Test. Other intrinsic metrics for LLMs include StereoSet  and ILPS , which are based on the log probabilities of words in text that can evoke stereotypes.

Probability-based bias evaluations such as CrowS-Pairs  and tasks in the BIG-bench benchmarking suite  compare the probabilities of stereotype-related tokens conditional on the presence of identity-related tokens. These evaluations provide insights into the model's biases by examining the likelihood of generating stereotype-associated content. Downstream, various benchmarks evaluate LLM bias with respect to languages  genders and ethnicity , culture  and beyond . To the best of our knowledge , our work is the first to bridge gender & ethnicity biases with real-world knowledge and multi-language evaluation.

## 3 Generating Co-occurrences of Disease-Demographic Pairs

### Methods

DatasetsThis study used \(ThePile\) dataset(deduplicated version), an 825 GB English text corpus created specifically for pre-training autoregressive LLMs , such as open-source LLMs pythia  and mamba . The open access to training data and resulting model weights makes it ideal for studying how biomedical keyword co-occurrences in pre-training data affect model outputs.

Co-occurrence pipeline updatesOur co-occurrence analysis methodology builds upon the approach outlined in our previous work , incorporating three key modifications: updated and verified keywords, multithread support, and real-world prevalence calculation. 4

**Modification 1: Updated Keywords -** Two physician authors (JG and DB) expanded and updated keywords to cover a broad range of conditions and demographics based on PubMed MeSH terms and SNOMED CT headers. The keywords for demographic groups were adapted from the HolisticBias dataset , aiming to align with previous studies investigating representational harms in biomedical LLMs. A hierarchical keyword definition strategy was used, including primary terms, variations, and synonyms for each disease and demographic group. The resulting dictionaries include 89 diseases, 6 race/ethnicity subgroup categories, and 3 gender subgroup categories. The list of dictionaries was proofread and expanded by a cultural anthropologist.

Figure 1: Overall workflow of Cross-Care. Our detailed multi-lingual templates for accessing diseases prevalence among different demographic subgroups can be found in Appendix D.0.1 Table 8.

**Modification 2: Multithreading -** Text pre-processing was completed as in the original workflow; however, it was parallelized using multithreading to enable scaling of the number of keywords utilized to maximize robustness and collection of results.

Named entity recognition (NER) tagger methods that could aid delineation of the use of specific keywords in a specific context, e.g., "white" or "black" referring to race, versus in other use cases, e.g., "white blood cells," were initially trialed. However, it became ineffective at this scale due to computational and time constraints and was not used in the final analysis.

We used windows of 50-250 tokens to capture co-occurrences between disease and demographic keywords. This range was chosen based on the intuition that, if in relation to one another, disease and demographic keywords should appear within 1 sentence to a short paragraph of one another and that longer distances would tend to capture spurious co-occurrences.

**Modification 3: Real-world prevalence -** To estimate the prevalence of diseases across subgroups, we used a standardized process to review the literature for each disease listed in our dictionary, focusing on prevalence and incidence within the USA across various subgroups. A detailed explanation of the approach and search strategy employed is available in Appendix A.1. Over two-thirds of the diseases encountered significant heterogeneity in reporting standards, compromising data consistency and reliability. Only 15 out of the 89 diseases had prevalence data readily available from official CDC statistics sourced from the National Health Interview Survey .

Given these constraints, our analysis focused on these 15 diseases, each with data available for at least five of the six race/ethnicity subgroups. Data for only male and female gender subgroups were available. Age-adjusted prevalences were normalized to rates per 10,000 for a consistent scale, facilitating preliminary benchmarking. These data are intended to provide a baseline for initial comparisons and relative ranking among subgroups rather than granular prevalence statistics for population health applications.

Validation of Keyword Frequency and Document Co-OccurrenceTo contrast our methods with the current state of the art, we utilized the Infini-gram, an engine designed for processing n-grams of any length . This is a publicly accessible API that has precomputed tokenized text across multiple large text corpora. The overall counts were then aggregated using the same dictionary mapping as above to compute the co-occurrence counts. 5

### Mathematical Description of Prevalence Calculation Using Average Logits

#### Definitions and Variables

**Models:**: Let \(M=\{m_{0},m_{1},,m_{n}\}\) denote the collection of models.
**Languages:**: Let \(L=\{l_{1},l_{2},,l_{k}\}\) represent the set of languages.
**Diseases:**: Let \(D\) be the comprehensive set of diseases.
**Demographic subgroups:**: Let \(S\) encompass all demographic subgroups considered.
**Templates:**: For each disease \(d\), demographic \(s\), and language \(l\), we can define \(T_{d,s,l}=\{t_{0},,t_{9}\}\) as the set of ten templates describing disease prevalence.

Logits DefinitionIn the context of language models, **logits:**\(z\) refer to the raw output scores from the final layer of the model before any normalization or activation function (such as softmax) is applied. These scores are used to represent the model's unnormalized prediction probabilities. Given a particular input, logits reflect the model's preference for each potential output, translating into the predicted probabilities for each class/token set after applying the softmax function. For each model \(m\), language \(l\), disease \(d\), and demographic subgroups \(s\), calculate the average logits as follows:

\[^{m}_{d,s,l}=|}_{t T_{d,s,l}}z^{m}_{d,s,l,t}\]

This formula computes the mean of logits derived from each template, providing a unified metric per disease, demographic, and language per model.

Model's Disease Demographic RankingWe defined \(R_{d,}^{m}(s)[1,|S|]\) as the rank assignment of subgroup \(s\) for disease \(d\) in model \(m\) under language \(\). (For simplicity, we drop the language distinction below.) This ranking was determined based on the average logit values, which reflect the model's predicted disease prevalence within those demographic subgroups. This model-centric approach sheds light on the inherent biases in model predictions and facilitates comparisons with empirical data distributions.

Additionally, we propose an alternative ranking method that analyzes disease subgroups based on their co-occurrences within _The Pile_, as well as our "gold" subset derived from real-world data. This empirical method bypasses model outputs, directly measuring disease representation across different demographic contexts.

### Comparing Rank Order Lists

We utilized Kendall's \(\) correlation coefficient to understand the representation of diseases across demographic subgroups in different data contexts here (see details in Appendix A.2).

Variance/Drift in Disease RankingIn exploring a sequence of models \(M=m_{0},m_{1},,m_{n}\), each built upon a base model \(m_{0}\) with unique alignment strategies, our goal was to assess how these strategies influence the ranking of diseases across different demographic subgroups.

We defined \(R_{d}^{m}(s)\) as the ranking of subgroup \(s\) on disease \(d\) for model \(m\). This approach allows us to track the progression and impacts of algorithmic adjustments over multiple iterations.

Ranking Variance AnalysisTo understand how disease rankings vary as models undergo fine-tuning or alignment with different strategies, we quantified the drift in disease rankings from a base model to its aligned iterations, assessing the impact of alignment interventions.

First, we calculated Kendall's tau for each disease across demographic subgroups as previously but instead compared ranks of the base model \(m_{0}\) to the ranks of a different, aligned model, \(m\). The comparison formula for Kendall's \(\) between the base model and each aligned model is

\[_{d}^{m}=_{s_{i} S,s_{j} S\\ i<j}(R_{d}^{m_{0}}(s_{i})-R_{d}^{m_{0}}(s_{j}) )(R_{d}^{m}(s_{i})-R_{d}^{m}(s_{j})). \]

Here, \(s_{i}\) and \(s_{j}\) are distinct subgroups, with \(i<j\) denoting that each pair of elements is only compared once. Secondly, we computed the average Kendall's tau for all diseases and demographic subgroups between the two models, evaluating the overall drift from the base model's ranking:

\[_{d}^{m}=_{d D}_{d}^{m} \]

This metric allowed us to assess the overall effect of model-tuning strategies on the ranking stability and accuracy in representing disease prevalence across demographic subgroups.

### Definition of Controlled and in the Wild

**"The controlled group"** includes Mamba and Pythia models, which are strictly pre-trained on \(ThePile\) only. Here, we aimed to compare these models' representation of disease prevalence against the real-world prevalence and Pile co-occurrence prevalence.

Additionally, we expanded our evaluation to include **"models in the wild"**, which are publicly accessible and varied in their training and tuning datasets. This group includes base models, such as Llama2, Llama3, Mistral, and Qwen1.5 from the 7b and 70b model sets, and those that have undergone specific alignment methods, including RLHF , SFT , or DPO , and also biomedical domain-specific continued pre-training (detailed models' descriptions at Appendix C.1 Table 3). We accessed their model logits with four languages (English, Spanish, French, Chinese). This dual approach of controlled evaluation and real-world model assessment allowed for a comprehensive analysis of models' understanding of disease's real-world prevalence across languages and how alignment methods might alter it.

Experimental FrameworkWe designed a controlled experimental framework to investigate model logit differences while only changing demographics or disease keywords. We created **10** templates, each engineered to incorporate a demographic relation and a disease term in various combinations. The templates aimed to state that a condition was common in a specific subgroup to evaluate the likelihood of that sentence occurring, such as _[Disease] patients are usually [Demographic Group] in America_. We used GPT-4 to initially translate our English template into Chinese, French, and Spanish, and translations were then reviewed and revised by native speakers. To ensure robustness, we explored variations on these templates and evaluated both averages, ranks, and individual template results in Appendix B.1.

### Findings

Variation Across WindowsWe evaluated the ranks across different token window sizes of 50, 100, and 250 within each disease demographic pair. No difference was observed in the top disease rank across each window size's ranking. For the remainder of the paper, we use the 250-token window for simplicity, but the raw counts across each window size for each disease are available on our website.

Demographic DistributionsWe collected all 89 disease co-occurrences in \(ThePile\) and 15 real-world prevalences from CDC (Appendix A.3 Table 1). Within both \(ThePile\) datasets, White was the most frequently represented race/ethnicity subgroup (87/89), most commonly followed by Black and Hispanic subgroups with relatively lower counts. The least represented race/ethnicity subgroups were consistently Pacific Islanders and Indigenous. However, among real-world statistics, Indigenous is often the top-ranked subgroup, followed by white and Black subgroups.

For gender distribution in \(ThePile\), the male subgroup was more prevalent than the female subgroup for the reported diseases, with the non-binary subgroup being the least represented (Appendix A.3 Table 1).

Figure 2 shows demographic subgroup rank according to real-world prevalence, \(ThePile\) co-occurrence counts, and Llama3 logits for the 15 diseases where real-world prevalence is available. This shows discrepancies and alignments between dataset co-occurrence representations and ac

Figure 2: Comparison of disease rankings between \(ThePile\), Llama3’s logits and real-world data. Comparison of disease rankings between The Pile (Blue), Llama3’s logits (Green), and real-world data (Red). Position of the marker indicates the relevant ranking of each attribute for a given disease demographic pair (1: most prevalent, 5: least prevalent). For example, looking at the disease “Perforated Ulcer,” The Pile ranked White race most prevalent, Llama3 logits second, and the real prevalence ranked third.

tual demographic prevalence of diseases. The raw counts and ranking in \(ThePile\) dataset versus real-world prevalence from the NHIS survey are further elaborated in Appendix A.3 Table 2.

## 4 Results

### Models in the Controlled Group

Logits Rank vs Co-occurrenceFor each Pythia/Mamba model in the controlled group, we calculated the model logits for all disease-demographic subgroup pairs to get the demographic rank of each disease; then we counted each demographic subgroup at the target position (top, bottom, and second bottom) across 89 disease-specific ranks. We also obtained similar rankings based on the disease-demographic co-occurrence in \(ThePile\) with a 250-tokens window.

In Figure 3, the stacked bars show the variation of top demographic subgroup counts across 89 diseases along with increasing size of Pythia (left) and Mamba (right) models, while the black line shows the number of diseases for which top ranked demographic subgroup based on model logits matched that based on co-occurrence counts. For gender, male was the top subgroup in \(ThePile\) for 59/89 diseases. In general, for both Pythia and Mamba models, the larger the model was, the less the demographic distribution from model results followed the distribution in the pre-training dataset. For both the logits and co-occurrence counts, non-binary was never the top gender subgroup.

For race/ethnicity, we observed variation across models and model sizes in the concordance of logit ranking compared to rankings in \(ThePile\) pretraining data, Figure 3. Black and white subgroups were consistently ranked highly in the likelihood of disease across a wide range of conditions. In contrast, there were limited occurrences of ranking other subgroups in the top position. Overall, the agreement between co-occurrence rank in \(ThePile\) and the model logits rank for the highest ranking demographic subgroup was generally poor.

The discrepancy between model logits and co-occurrence was also apparent in the second-lowest ranked race/ethnicity subgroup. As shown in the Appendix, the bottom subplots in Figure 7, Hispanic was the second-bottom ranked subgroup for almost all 89 diseases based on Pythia and Mamba model logits, while disease-demographic subgroup co-occurrence in \(ThePile\) indicated that Indigenous was the second-bottom subgroup for 86/89 diseases. In contrast, there was a strong agreement between model logits and co-occurrence in the bottom rank counts, where Pacific Islander was ranked lowest based on both model logits and co-occurrence as shown in Figure 7.

Figure 3: **a)** Top ranked gender (top) and race/ethnicity (bottom) subgroups across 89 diseases and the suite of Pythia and Mamba models according to logits results (stacked bars). Co-occurrence and logit rank match demonstrate the number of diseases for which the top-ranked demographic subgroup is the same when calculated using co-occurrences and logits (black line). Demographic subgroups that did not appear as the top-ranked group are not shown. **b)** Kendall’s tau of Mamba and Pythia’s logits vs co-occurrence, and real prevalence for gender (top) and race/ethnicity (bottom). The overlap of green and blue lines indicates consistency across our subset and the full 89 diseases. The gap between these two lines and the red line highlights the greater association with co-occurrences compared to real-world prevalence.

Logits Rank vs Co-occurrence vs Real PrevalenceThe Kendall's tau scores compared the rankings of logits against real-world prevalence rankings were near zero across all model sizes, indicating no correlation for both race/ethnicity and gender (Figures 3). This suggests that the logit rankings of diseases by demographic subgroups within models did not align with their real-world prevalence rankings and demonstrates a lack of grounding in real-world medical knowledge. However, most of the time, Mamba and Pythia showed a stronger correlation with \(ThePile\) co-occurrence than the real-world prevalence rankings, especially among gender subgroups.

Rank vs Co-occurrence countsThe analysis of Kendall's tau scores across quartiles of overall disease co-occurrence counts in \(ThePile\) revealed consistent relationships for both race/ethnicity (Appendix B.2.2 Figure 9) and gender (Figure 10). Notably, the relationship between the frequency of co-occurrences and the logit correlations did not vary significantly across quartiles. This indicates that diseases most frequently mentioned in the dataset did not demonstrate a corresponding improvement in the correlation of logits, suggesting that model performance did not scale with the frequency of disease mention within a pretraining dataset.

### Models in the wild

For all models that we tested across size, alignment method, and language, no model's disease logits rankings had \(>0.35\) (Min = -0.73, Max = 0.33, Median = -0.05, Avg = -0.06, Var = 0.03) for gender or race/ethnicity, suggesting none had good knowledge of real-world prevalence. Figure 2 illustrates discrepancies between Llama3's logits compared to \(ThePile\) and real-world prevalences. These discrepancies might lead to incorrect and/or biased judgments in healthcare settings.

Variation across Alignment strategiesThe impact of different alignment strategies on the LLama2 70b series for both race/ethnicity and gender are displayed in Figure 4. None of the alignment methods nor in-domain continued pre-training corrected the base model towards more accurate reflections of real-world prevalence. In fact, we observed some of the debiasing strategies during alignment adversely impacting the model's decisions (Appendix C.2.3 Table 6). All Llama2 70b series alignment methods increased preference for female over male subgroups, and decreased preference for the Black subgroup, in English. A similar observation was seen among the Mistral family. For Qwen1.5-7b base compared to Qwen1.5-7b chat in English, PPO+DPO shifted its favor to the Indigenous instead of Asian subgroup (Appendix C.2.2 Table 5).

For the Llama2 70b series, models tuned by different alignment methods (SFT, DPO) did not change the rank-ordering of race/ethnicity subgroups (\(>=0.8\)). Models that demonstrated noticeable variation were the Meditron variant, which underwent continued pre-training on medical domain data, and the chat version that went through reinforcement learning with human feedback (RLHF). Similar trends were observed for Mistral's gender results, where Bio-mistral was given continued pre-training on biomedical text with Mistral. (Appendix C.2.1 Table 4).

## 5 Conclusion and Future works

LimitationsThis study has limitations that should be considered when interpreting the findings:

1. **Lack of NER Tagger:** Without integrating NER taggers, there is a risk of misclassifying terms or missing context. However, we were limited by the the computational requirements of NER tagging over the entire \(ThePile\) dataset.

2. **Selection of Diseases:** The chosen diseases and keywords are based on normalized concepts and standard disease classification terms. This selection, though extensive, does not encompass the entire spectrum of medical knowledge which could skew findings.

3. **Subgroup Selection:** To demonstrate variation across subgroups, we used terms from CDC national and U.S. surveys as grouping categories for quantifying subgroup robustness. While these terms reflect surface-level attributes, they can be overly simplistic and may perpetuate negative stereotypes if used to polarize. Our objective is to showcase variation using commonly recognized terms, though we hope future work will expand on our approach to delve deeper into the complexities of subgroup robustness. This should be driven by locally designed and governed frameworks. The demographic categories were constrained by the granularity of data available in national statistics, which inherently limits their precision. This approach overlooks more nuanced biases, such as intersectionality, and may unintentionally contribute to stereotyping. Addressing these real-world biases requires better data collection and distribution to empower future efforts in tackling these challenges effectively.

4. **Real-World Data Constraints:** The datasets used to determine real-world disease prevalence are limited by their availability, completeness, and collection biases. This may hinder the assessment of the broader impacts of findings.

5. **Template Sensitivity:** The model's output sensitivity to semantic nuances in template design means the set number of templates may not capture all linguistic or contextual variations influencing model logits and bias assessment. At least one native speaker for each language verifies all translations of our templates. However, the authors acknowledge the translations can be subjective.

6. **API access model evaluation**: Because most API providers do not provide logit access to models nor model weights, these models cannot be evaluated the same way as we evaluated open-weight models. Therefore, we did not include any API-only access model research.

Figure 4: Top ranked gender and race/ethnicity subgroups across each of the 89 diseases and different alignments of methods for Llama2 models according to logits results (stacked bars). The change in top ranked demographic from base model to respective tuned models illustrates the varying impact of alignment strategies on downstream ranking. Note this variation with various tuning strategies is not uniform across languages.

7. **Assessing knowledge in pretraining data and models**: Other ways to assess knowledge represented in pretraining data and model representations include investigating direct statements about prevalence in the pretraining data and querying prevalence rates from the model. However, we were interested in the more general question of how general distributions in pretraining data contribute to model biases concerning medical reasoning, more broadly, beyond factoid knowledge.

Future WorkFuture research will prioritize:

1. **Development of Comprehensive Datasets:** Efforts will be made to create and employ datasets that provide more accurate and exhaustive real-world prevalence data for diseases, especially those poorly represented in existing datasets.

2. **Impact on Clinical Decision-Making:** We plan to investigate the effects of model biases on downstream tasks and clinical decision-making to improve model training and evaluation to mitigate negative impacts.

3. **Ability of Information Provided In Context to Update Prevalence Estimates:** Future work will explore the potential of providing information in context, for example using retrieval-augmented generation (RAG) to adjust prevalence estimates more effectively compared to traditional fine-tuning methods.

4. **Use of Real-World Data-Aware Synthetic Data:** We also aim to leverage continued pretraining or fine-tuning with real-world data-aware synthetic data to explicitly incorporate real-world prevalence statistics, aligning model predictions with actual disease distributions.

This work has highlighted a fundamental disconnect between real-world prevalence estimates and LLM outputs, which appear to track significantly closer to simple co-occurrences in pre-training data. In order to address these discrepancies, the most obvious solution is to curate pre-training data of language models with this knowledge in mind and for a specific context. Furthermore, organizations and regulators can evaluate simple co-occurrences to provide a rough idea of models' tendencies before deployment and in addition to task performance. This is particularly important when considering multilingual models; if this is to be used across languages, then accurate data in these languages are important at both pretraining and alignment stages.

ConclusionThis study conducted a detailed analysis of how corpus co-occurrence and demographic representation influence biases in LLMs within the context of disease prevalence. We uncovered substantial variances in model outputs, highlighting the complexities of developing NLP systems for biomedical applications that align with real-world data and outcomes. Importantly, these variances appear across alignment strategies and languages, and notably, they do not correlate with the real-world prevalence of diseases. This suggests a lack of grounding in actual disease prevalences, underscoring a critical need for extensive research into integrating real-world data to ensure fair and accurate model translation. These findings highlight the urgent need for research to enhance these models, ensuring they are reliable and equitable across diverse populations. Further exploration will advance our understanding of and ability to correct biases in AI systems for healthcare.