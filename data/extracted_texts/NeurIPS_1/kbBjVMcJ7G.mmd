# Operator World Models for Reinforcement Learning

Pietro Novelli

Istituto Italiano di Tecnologia

pietro.novelli@iit.it

&Marco Prattico

Istituto Italiano di Tecnologia

marco.prattico@iit.it

&Massimiliano Pontil

Istituto Italiano di Tecnologia

AI Centre, University College London

massimiliano.pontil@iit.it

&Carlo Ciliberto

AI Centre, University College London

c.ciliberto@ucl.ac.uk

###### Abstract

Policy Mirror Descent (PMD) is a powerful and theoretically sound methodology for sequential decision-making. However, it is not directly applicable to Reinforcement Learning (RL) due to the inaccessibility of explicit action-value functions. We address this challenge by introducing a novel approach based on learning a world model of the environment using conditional mean embeddings. Leveraging tools from operator theory we derive a closed-form expression of the action-value function in terms of the world model via simple matrix operations. Combining these estimators with PMD leads to POWR, a new RL algorithm for which we prove convergence rates to the global optimum. Preliminary experiments in finite and infinite state settings support the effectiveness of our method1.

## 1 Introduction

In recent years, Reinforcement Learning (RL)  has seen significant progress, with methods capable of tackling challenging applications such as robotic manipulation , playing Go  or Atari games  and resource management  to name but a few. The central challenge in RL settings is to balance the trade-off between exploration and exploitation, namely to improve upon previous policies while gathering sufficient information about the environment dynamics. Several strategies have been proposed to tackle this issue, such as Q-learning-based methods , policy optimization  or actor-critics  to name a few. In contrast, when full information about the environment is available, sequential decision-making methods need only to focus on exploitation. Here, strategies such as policy improvement or policy iteration  have been thoroughly studied from both the algorithmic and theoretical standpoints. Within this context, the understanding of Policy Mirror Descent (PMD) methods has recently enjoyed a significant step forward, with results guaranteeing convergence to a global optimum with associated rates .

In their original formulation, PMD methods require explicit knowledge of the action-value functions for all policies generated during the optimization process. This is clearly inaccessible in RL applications. Recently,  showed how PMD convergence rates can be extended to settings in which inexact estimators of the action-value function are used (see  for a similar result from a regret-based perspective). The resulting convergence rates, however, depend on uniform norm bounds on the approximation error, usually guaranteed only under unrealistic and inefficient assumptions such as the availability of a (perfect) simulator to be queried on arbitrary state-action pairs. Moreover, these strategies require repeating this sampling/learning process for any policy generated by the PMD algorithm, which is computationally expensive and demands numerous interactions with theenvironment. A natural question, therefore, is whether PMD approaches can be efficiently deployed in RL settings while enjoying the same strong theoretical guarantees.

In this work, we address these issues by proposing a novel approach to estimating the action-value function. Unlike previous methods that directly approximate the action-value function from samples, we first learn the transition operator and reward function associated with the Markov decision process (MDP). To model the transition operator, we adopt the Conditional Mean Embedding (CME) framework [14; 15]. We then leverage an operatorial characterization of the action-value function to express it in terms of these estimated quantities. This strategy draws a peculiar connection with world model methods and can be interpreted as world model learning via CMEs. The notion of world models for RL has been popularized by Ha and Schmidhuber in  distilling ideas from the early nineties [17; 18]. Traditional world model methods such as [16; 19] emphasize learning an implicit model of the environment in the form of a simulator. The simulator can be sampled directly in the latent representation space, which is usually of moderate dimension, resulting in a compressed and high-throughput model of the environment. This approach, however, requires extensive sampling for application to PMD and incurs into two sources of error in estimating the action-value function: model and sampling error. In contrast, CMEs can be used to estimate expectations without sampling and incur only in model error, for which learning bounds are available [20; 21]. One of our key results shows that by modeling the transition operator as a CME between suitable Sobolev spaces, we can compute estimates of the action-value function of any sufficiently smooth policy in closed form via efficient matrix operations.

Combining our estimates of the action-value function with the PMD framework we obtain a novel RL algorithm that we dub _Policy mirror descent with Operator World-models for Reinforcement learning (POWR)_. A byproduct of adopting CMEs to model the transition operator is that we can naturally extend PMD to infinite state space settings. We leverage recent advancements in characterizing the sample complexity of CME estimators to prove convergence rates for the proposed algorithm to the global maximum of the RL Problem. Our approach is similar in spirit to , which proposed a value iteration strategy based on CMEs. We extend these ideas to PMD strategies and refine previous results on convergence rates. Learning the transition operator with a least-squares based estimator was also recently considered in  and . The latter proposed an optimistic strategy to prove near-optimal regret bounds in linear mixture MDP settings . In contrast, in this work, we cast our problem within a linear MDP setting with possibly infinite latent dimension. We validate our approach on simple environments from the Gym library  both in finite and infinite state settings, reporting promising evidence in support of our theoretical analysis.

**Contributions**. The main contributions of this paper are: \(i)\) a CME-based world model framework, which enables us to generate estimators for the action-value function of a policy in closed form via matrix operations. \(ii)\) An (inexact) PMD algorithm combining the learned CMEs world models with mirror descent update steps to generate improved policies. \(iii)\) Showing that the algorithm is well-defined when learning the world model as an operator between a suitable family of Sobolev spaces. \(iv)\) Showing convergence rates of the proposed approach to the global maximum of the RL problem, under regularity assumptions on the MDP. \(v)\) Empirically testing the proposed approach in practice, comparing it with well-established baselines.

## 2 Problem Formulation and Policy Mirror Descent

We consider a Markov Decision Process (MDP) over a state space \(\) and action space \(\), with transition kernel \(\). We assume \(\) and \(\) to be Polish, \(:()\) to be a Borel measurable function from the joint space \(=\) to the space \(()\) of Borel probability measures on \(\). We define a policy to be a Borel measurable function \(:()\). When \(\) (respectively \(\)) is a finite set, the space \(()=()^{|A|}\) (respectively \(()=()^{|X|}\)) corresponds to the probability simplex. Given a discount factor \(>0\), an initial state distribution \(()\) and a Borel measurable bounded and non-negative _reward2_ function \(r:\) we denote by

\[J()=_{,,}[_{t=0}^{}^{t}r(X_{t},A_ {t})] \]the (discounted) expected return of the policy \(\) applied to the MDP, yielding the Markov process \((X_{t},A_{t})_{t}\), where \(X_{0}\) is distributed according to \(\) and for each \(t\) the action \(A_{t}\) is distributed according to \((|X_{t})\) and \(X_{t+1}\) according to \((|X_{t},A_{t})\).

In sequential decision settings, the goal is to find the optimal policy \(_{*}\) maximizing (1) over the space of all measurable policies. In reinforcement learning, one typically assumes that knowledge of the transition \(\), the reward \(r\), and (possibly) the starting distribution \(\) is not available. It is only possible to gather information about these quantities by interacting with the MDP to sample state-action pairs \((x_{t},a_{t})\) and corresponding rewards \(r(x_{t},a_{t})\) and transitions \(x_{t+1}\).

**Policy Mirror Descent (PMD)**. In so-called tabular settings - in which both \(\) and \(\) are finite sets - the policy optimization problem amounts to maximizing (1) over the space \(=()^{||}\) of column substochastic matrices, namely matrices \(M^{||||}\) with non-negative entries and whose columns sum up to one, namely \(M^{*}_{}=_{}\), with \(\) denoting the vector with all entries equal to one on the appropriate space. Borrowing from the convex optimization literature - where mirror descent algorithms offer a powerful approach to minimize a convex functional over a convex constraint set  - recent work proposed to adopt mirror descent also for policy optimization, a strategy known as _policy mirror descent (PMD)_. Even though the objective in (1) is not convex (or concave, since we are maximizing it), it turns out that mirror ascent can nevertheless enjoy global convergence to the maximum, with sublinear  or even linear rates , at the cost of dimension-dependent constants.

Starting from an initial policy \(_{0}\), PMD generates a sequence \((_{t})_{t}\) according to the update step

\[_{t+1}(\,|\,x)=*{argmin}_{p()}-  q_{_{t}}(,x),p+D(p,_{t}(|x)), \]

for any \(x\), with \(>0\) a step size, \(D\) a suitable Bregman divergence  and \(q_{}:\) the so-called _action-value_ function of a policy \(\), see also [12, Sec. 4]. The action-value function

\[q_{}(x,a)=[_{t=0}^{}^{t}r(X_{t},A_{t}) X_{0}=x,A_{0}=a] \]

is the discounted return obtained by taking action \(a\) in state \(x\) and then following the policy \(\). The solution to (2) crucially depends on the choice of \(D\). For example, in  the authors observed that if \(D\) is the Kullback-Leibler divergence, PMD corresponds to the Natural Policy Gradient originally proposed in  while  showed that if \(D\) is the squared euclidean distance, PMD recovers the Projected Policy Gradient method from .

**PMD in Reinforcement Learning**. A clear limitation to adopting PMD in RL settings is that (3) needs exact knowledge of the action-value functions \(q_{_{t}}\) associated to each iterate \(_{t}\) of the algorithm. This requires evaluating the expectation in (3), which is not possible in RL where we do not know the reward \(r\) and MDP transition distribution \(\) in advance. While sampling strategies can be adopted to estimate \(q_{_{t}}\), a key question is how the approximation error affects PMD.

The work in  provides an answer to this question, extending the analysis of PMD to the case where estimates \(_{_{t}}\) are used in place of the true action-value function in (2). We recall here an informal version of the result for the case of sublinear convergence rates for PMD. We postpone a more rigorous statement of the theorem and its assumptions to Sec. 5, where we extend it to infinite state spaces \(\).

**Theorem 1** (Inexact PMD (Sec. 5 in ) - Informal).: _In the tabular setting, let \((_{t})_{t}\) be a sequence of policies obtained by applying the PMD update in (2) with functions \(_{_{t}}:\) in place of \(q_{_{t}}\) and \(D\) a suitable Bregman divergence. For any \(T\) and \(>0\), if \(\|_{_{t}}-q_{_{t}}\|_{}\) for all \(t=1,,T\), then_

\[_{}\;J()-J(_{T}) O(+1/T). \]

Thm. 1 implies that inexact PMD retains the convergence rates of its exact counterpart, provided that the approximation error for each action-value function is of order \(1/T\) in uniform norm \(\|\|_{}\). While this result supports estimating the action-value function in RL, implementing this strategy in practice poses two main challenges, even in tabular settings. First, approximating the expectation in (3) in \(\|\|_{}\) norm via sampling requires "starting" the MDP from each state \(x\), multiple times. Thisis often not possible in RL, where we do not have control over the starting distribution \(\). Second, repeating this sampling process to learn a \(_{_{t}}\) for each policy \(_{t}\) can become extremely expensive in terms of both the number of computations and interactions with the environment.

In this work, we propose a new strategy to tackle the problems above. Instead of re-sampling the MDP to estimate \(_{_{t}}\) at each iteration \(t\), we learn estimators \(\) and \(\) for the reward and transition distribution, respectively. For any policy \(\), we then leverage the relation between these quantities in (3) to generate an estimator \(\) for \(q_{}\). This approach tackles the above challenges since 1) it enables us to control the approximation error on any action-value function in terms of the approximation error of \(\) and \(\); 2) it does not require sampling the MDP to learn a new \(_{_{t}}\) for each \(_{t}\) generated by PMD.

## 3 Operator World Models

In this section, we present an operator-based formulation of the problem introduced in Sec. 2 (see also ). This will be instrumental in extending the PMD theory to arbitrary state spaces \(\), to quantify the approximation error of the action-value function in terms of the approximation error of the reward and transition distribution, and to motivate conditional mean embeddings as the tool to learn these latter quantities.

**Conditional Expectation Operators**. We start by defining the _transfer operator_\(\) associated with the MDP transition distribution \(\). Let \(B_{b}()\) denote the space of bounded Borel measurable functions on a space \(\). Formally, \(:B_{b}() B_{b}()\) is the linear operator such that, for any \(f B_{b}()\)

\[(f)(x,a)=_{}f(x^{})\;(dx^{}|x,a)= [f(X^{}) x,a](x,a), \]

where \(X^{}\) is sampled according to \((|x,a)\). Note that \(\) is the Markov operator [30, Ch. 19] encoding the dynamics of the MDP and its conjugate \(^{*}:()()\) is the operator mapping signed Borel measures \(()\) to their transition via \(\) as \((^{*})()=_{}(dx^{ }|x,a)(dx,da)\) for any measurable \(\). For any policy \(\) we define the operator \(_{}:B_{b}() B_{b}()\) such that for all \(g B_{b}()\)

\[(_{}g)(x)=_{}g(x,a)\;(da|x)=[g( X,A) X=x]x, \]

where the expectation is taken over the action \(A\) sampled according to \((|x)\). Also \(_{}\) is a Markov operator and its conjugate \(_{}^{*}:()()\) is the operator mapping any \(()\) to its joint measure with \(\), namely \((_{}^{*})()=_{}(da|x)(dx)\) for any measurable \(\).

**Operator Formulation of RL**. With these two operators in place, we can characterize the expected reward after a single interaction between a policy \(\) and the MDP as \((_{}r)(x,a)=[r(X^{},A^{})|X_{0}=x,A_{0}=a]\). This observation can be applied recursively, yielding the operatorial characterization of the action-value function from (3)

\[q_{}(x,a)=_{t=0}^{}^{t}[r(X_{t},A_{t})|X_{0}=x,A _{0}=a]=_{t=0}^{}(_{})^{t}r=(- _{})^{-1}r, \]

where the last equality follows from \(\) and \(_{}\) being Markov operators [30, Ch. 19] (\(\|\|=\|_{}\|=1\)), making the Neumann series convergent. Analogously, we can reformulate the RL objective introduced in (1) as the pairing

\[J()=_{}(-_{})^{-1} r,=_{}q_{},, \]

for \(()\) a starting distribution. In both (7) and (8) the operatorial formulation encodes the cumulative reward collected through the (possibly infinitely many) interactions of the policy with the MDP in closed form, as the inversion \((-_{})^{-1}r\). This characterization motivates us to learn \(\) and \(r\) from data and then express any action-value function as the interaction of these two terms with the policy \(\) as in (7), rather than learning each \(q_{}\) independently for any \(\).

**Learning the World Model via Conditional Mean Embeddings**. Conditional Mean Embeddings (CME) offer an effective tool to model and learn conditional expectation operators from data . They cast the problem of learning \(\) by studying the restriction of its action on a suitable family of functions. Let \(:\) and \(:\) two feature maps with values into the Hilbert spaces \(\) and \(\). With some abuse of notation (which is justified by them being Hilbert spaces), we interpret \(\) and \(\) as subspaces of functions in \(B_{b}()\) and \(B_{b}()\) of the form \(f(x)= f,(x)\) and \(g(x,a)= g,(x,a)\) for any \(f\) and \(g\) and any \((x,a)\). We say that the _linear MDP_ assumption holds with respect to \((,)\) if

**Assumption 1** (Linear MDP - Well-specified CME).: _The restriction of \(\) to \(\) is a Hilbert-Schmidt operator \(|_{}(,)\)._

In CME settings, the assumption above is known as requiring the CME of \(\) to be _well-specified_. The following result, proved in Appendix A.2, clarifies this aspect and establishes the relation of Asm. 1 with the standard definition of linear MDP.

**Proposition 2** (Well-specified CME).: _Under Asm. 1, \((|_{})^{*}=(^{*})|_{}\) and, for any \((x,a)\)_

\[(|_{})^{*}(x,a)=_{}(x^{} )\;(x^{}|x,a)=[(X^{})|X=x,A=a]. \]

Proposition 2 shows that (9) is equivalent to the standard linear MDP assumption [31, Ch. 8] when \(\) is a finite set (taking \(\) the one-hot encoding) while being weaker in infinite settings. From the CME perspective, the proposition characterizes the action of \((|_{})^{*}\) as sending evaluation vectors in \(\) to the conditional expectation of evaluation vectors in \(\) with respect to \(\), the definition of conditional mean embedding of \(\). This characterization also suggests a learning strategy: (9) characterizes the action of \(\) as evaluating the conditional expectation of a vector \((X^{})\) given \((x,a)\). Given a set of points \((x_{i},a_{i})_{i=1}^{n}\) and corresponding \(x^{}\) sampled from \((|x_{i},a_{i})\), this can be learned by minimizing the squared loss, yielding the estimator (see [15, Sec 4.2])

\[_{n}=*{argmin}_{(, )}\;_{i=1}^{n}(x^{}_{i})- ^{*}(x_{i},a_{i})_{}^{2}+ _{}^{2}=S_{n}^{*}K_{}^{-1}Z_{n}. \]

When \(\) and \(\) are finite dimensional, \(S_{n}\) and \(Z_{n}\) are matrices with \(n\) rows, each corresponding respectively to the vectors \((x_{i},a_{i})\) and \((x^{}_{i})\) for \(i=1,,n\). In the infinite setting, they generalize to operators \(S_{n}:^{n}\) and \(Z_{n}:^{n}\). The matrix \(K_{}=S_{n}S_{n}^{*}+n_{n}^{n n}\) is the regularized Gram (or kernel) matrix with \((i,j)\)-th entry corresponding to

\[(K_{})_{ij}=(x_{i},a_{i}),(x_{j},a_{j}) +n_{ij}. \]

We conclude our discussion on learning world models via CMEs by noting that in most RL settings, the reward function is unknown, too. Analogously to what we have described for \(_{n}\) and following the standard practice in supervised settings, we can learn an estimator for \(r\) solving a problem akin to (10). This yields a function of the form \(r_{n}=S_{n}^{*}b=_{i=1}^{n}b_{i}\,(x_{i},a_{i})\) as the linear combination of the embedded training points with the entries of the vector \(b=K_{}^{-1}y\) where \(y^{n}\) is the vector with entries \(y_{i}=r(x_{i},a_{i})\).

**Estimating the Action-value Function \(q_{}\)**. We now propose our strategy to generate an estimator for the action-value function \(q_{}\) of a given policy \(\) in terms of an estimator for the reward \(r\) and a world model for \(\) learned in terms of the restriction to \(\) and \(\). To this end, we need to introduce the notion of compatibility between a policy \(\) and the pair \((,)\).

**Definition 1** (\((,)\)-compatibility).: _A policy \(:()\) is compatible with two subspaces \( B_{b}()\) and \( B_{b}()\) if the restriction \(_{}\) to \(\) is a bounded linear operator with range \(\), that is \((_{})|_{}:\)._

Definition 1 is analogous to the linear MDP Asm. 1 in that it requires the restriction of \(_{}\) to \(\) to take values in the associated space \(\). However, it is slightly weaker since it requires this restriction to be bounded (and linear) rather than being an HS operator. We will discuss in Sec. 4 how this difference will allow us to show that a wide range of policies (in particular those generated by our POWR method) is \((,)\)-compatible for our choice of function spaces. Definition 1 is the key condition that enables us to generate an estimator for \(q_{}\), as characterized by the following result.

**Proposition 3**.: _Let \(_{n}=S_{n}^{*}BZ_{n}(,)\) and \(r_{n}=S_{n}^{*}b\) for respectively a \(B^{n n}\) and \(b^{n}\). Let \(\) be \((,)\)-compatible. Then,_

\[_{}=(-_{n}_{})^{-1}r_{n}=S_{ n}^{*}(- BM_{})^{-1}b \]

_where \(M_{}=Z_{n}_{}S_{n}^{*}^{n n}\) is the matrix with entries_

\[(M_{})_{ij}=(x_{i}^{}),_{} (x_{j},a_{j})=_{}(x_{i}^{ },a),(x_{j},a_{j})\;(da|x_{i}^{}). \]

Proposition 3 leverages a kernel trick argument to express the estimator for the action-value function of \(\) as the linear combination \(_{}=_{i=1}^{n}c_{i}\,(x_{i},a_{i})\) of the (embedded) training points \((x_{i},a_{i})\) and the entries \(c_{i}\) of the vector \(c=(- BM_{})^{-1}b^{n}\). We prove the result in Appendix A.4. We note that in (12) both \(B\) and \(M_{}\) are \(n n\) matrices and therefore the characterization of \(_{}\) amounts to solving a \(n n\) linear system. For settings where \(n\) is large, one can adopt random projection methods such as Nystrom approximation to learn \(_{n}\) and \(r_{n}\). These strategies have been recently shown to significantly reduce the computational load of learning while retaining the same empirical and theoretical performance as their non-approximated counterparts [34; 35].

We conclude this section noting how (13) implies that we only need to be able to evaluate \(\), but we do not need explicit knowledge of \(_{}\) as operator. As we shall see, this property will be instrumental to prove generalization bounds for our proposed PMD algorithm in Sec. 4.

## 4 Proposed Algorithm: POWR

We are ready to describe our algorithm for world model-based PMD. In the following, we restrict to the case where \(||<\) is a finite set. As introduced in Sec. 2, policy mirror descent methods are mainly characterized by the choice of Bregman divergence \(D\) used for the mirror descent update and, in the case of inexact methods, the estimator \(_{_{t}}\) of the action-value function \(q_{_{t}}\) for the intermediate policies generated by the algorithm.

In POWR, we combine the CME world model presented in Sec. 3 with mirror descent steps using the Kullback-Leibler divergence \(D_{}(p;p^{})=_{a}p_{a}(p_{a}/p^{ }_{a})\) in the update of (2). It was shown in  that in this case PMD corresponds to Natural Policy Gradient . As showed in [36; Example 9.10], the solution to (2) can be written in closed form for any \(x\) as

\[_{t+1}(|x)=(|x)e^{_{_{t}}(x,)}}{ _{a}_{t}(a|x)e^{_{_{t}}(x,a)}}, \]

where we used the estimated action-value function \(_{}\) from Proposition 3. Additionally, the formula above can be applied recursively, expressing \(_{t+1}\) as the softmax operator applied to the discountedsum of the action-value functions up to the current iteration

\[_{t+1}(|x)=((_{0}(|x))+_{s=0}^{t} _{_{s}}(x,)). \]

**Choice of the Feature Maps**. A key question to address before adopting the action-value estimators introduced in Sec. 3 is choosing the two spaces \(\) and \(\) to perform world model learning. Specifically, to apply Proposition 3 and obtain proper estimators \(_{_{t}}\), we need to guarantee that all policies generated by the PMD update (14) are \((,)\)-compatible (Definition 1). The following result describes a suitable family of such spaces.

**Proposition 4** (Separable Spaces).: _Let \(:\) be a feature map into a Hilbert space \(\). Let \(=\) and \(=^{||}\) with feature maps respectively \((x)=(x)(x)\) and \((x,a)=(x) e_{a}\), with \(e_{a}^{||}\) the one-hot encoding of action \(a\). Let \(:()\) be a policy such that \((a|)= p_{a},()\) with \(p_{a}\) for any \(a\). Then, \(\) is \((,)\)-compatible._

Proposition 4 (proof in Appendix A.5) states that for the specific choice of function spaces \(=\) and \(=^{||}\), _we can guarantee \((,)\)-compatibility, provided that \(\) is rich enough to "contain" all \((a|)\) for \(a\)_. We postpone the discussion on identifying a suitable spaces \(\) for PMD to Sec. 5, since \((,)\)-compatibility is not needed to mechanically apply Proposition 3 and obtain an estimator \(_{}\). This is because (12) exploits a kernel-trick to bypass the need to know \(_{}\) explicitly and rather requires only to be able to evaluate \(\) on the training data. The latter is possible for \(_{t+1}\), thanks to the explicit form of the PMD update in (14). We can therefore present our algorithm.

**POWR**. Alg. 1 describes _Policy mirror descent with Operator World-models for Reinforcement learning (POWR)_. Following the intuition of Proposition 4, the algorithm assumes to work with separable spaces. During an initial phase, we learn the world model \(_{n}=S_{n}^{*}K_{}^{-1}Z_{n}\) and the reward \(r_{n}=S_{n}^{*}b\) fitting the conditional mean embedding described in (10) on a dataset \((x_{i},a_{i},x^{}_{i},r_{i})_{i=1}^{n}\) (e.g. obtained via experience replay ). Once the world model has been learned, we optimize the policy and perform PMD iterations via (15). In this second phase, we first evaluate the past (cumulative) action-value estimators \(_{a=0}^{t}_{_{s}}\) to obtain the policy \(_{t+1}(|x_{i})\) by (inexact) PMD via the softmax operator in (15). We use the newly obtained policy to compute the matrix \(M_{_{t+1}}\) defined in (13), which is a key component to obtain the estimator \(_{_{t+1}}\) for \(q_{_{t+1}}\). We note that in the case of the separable spaces of Proposition 4, this matrix reduces to the \(n n\) matrix with entries\(k(x^{}_{i},x_{j})_{t+1}(a_{j}|x^{}_{i})\), where \(k(x^{}_{i},x_{j})=(x^{}_{i}),(x_{j})\) is the kernel matrix between initial and evolved states. Finally, we obtain \(c=(- K_{}^{-1}M)^{-1}b\) and model \(_{_{t+1}}=S_{n}^{*}c\) according to Proposition 3.

Clearly, the world model learning and PMD phases can be alternated in POWR, essentially finding a trade-off between exploration and exploitation. This could possibly lead to a refinement of the world model as more observations are integrated into the estimator. While in this work we do not investigate the effects of such alternating strategy, Thm. 7 offers relevant insights in this sense. The result characterizes the behavior of the PMD algorithm when combined with a varying (possibly increasing) accuracy in the estimation of the action-valued function (see Sec. 5 for more details).

## 5 Theoretical Analysis

We now show that POWR converges to the global maximizer of the RL problem in (1). To this end, we first identify a family of function spaces guaranteed to be compatible with the policies generated by Alg. 1. Then, we provide an extension of the result in  for inexact PMD to infinite state spaces \(\), showing the impact of the action-value approximation error on the convergence rates. For the estimator introduced in Proposition 3, we relate this error to the approximation errors of \(_{n}\) and \(r_{n}\) leveraging the Simulation Lemma A.6. Finally, we use recent advancements in the characterization of CMEs' fast learning rates to bound the sample complexity of these latter quantities, yielding error bounds for POWR.

**POWR is Well-defined**. To properly apply Proposition 3 to estimate the action-value function of any PMD iterate \(_{t}\), we need to guarantee that every iterate belongs to the space \(\) according to Proposition 4. The following result provides such a family of spaces.

**Theorem 5**.: _Let \(^{d}\) be a compact set and let \(=W^{2,s}()\) be the Sobolev space of smoothness \(s>0\) (see e.g. ). Let \(_{t}(a|)\) and \(_{_{t}}(,a)\) belong to \(\) for any \(a\) and \(_{t}(a|x)>0\) for any \(x\). Then the policy \(_{t+1}\) solution to the PMD update in (14) belongs to \(\)._

According to Thm. 5, Sobolev spaces offer a viable choice for compatibility with PMD-generated policies. This observation is further supported by the fact that Sobolev spaces of smoothness \(s>d/2\) are so-called _reproducing kernel Hilbert spaces (rkhs)_ (see e.g. [39, Ch. 10]). We recall that rkhs are always naturally equipped with a \(:\) such that the inner product \((x),(x^{})=k(x,x^{})\) defines a so-called reproducing kernel, namely a positive definite function that is (usually) efficient to evaluate, even if \((x)\) is high or infinite dimensional. For example, \(=W^{2,s}()\) with \(s=\) has associated kernel \(k(x,x^{})=e^{-\|x-x^{}\|/}\) with bandwidth \(>0\). By applying Thm. 5 to the iterates generated by Alg. 1 we have the following result.

**Corollary 6**.: _With the hypothesis of Proposition 4 let \(=W^{2,s}()\) with \(s>d/2\). Let \(_{n}(,)\) and \(r_{n}\) characterized as in Proposition 3. Let \(_{0}(a|) e^{_{0}(,a)}\) for \(q_{0}\) such that \(q_{0}(,a)\) any \(a\). Then, for any \(t\) the PMD iterates \(_{t}\) generated by Alg. 1 are such that \(_{t}(a|)\) and hence are \((,)\)-compatible._

The above corollary guarantees us that if we are able to learn our estimates for the action-value function in a suitably regular Sobolev space \(\), then POWR is well-defined. This is a necessary condition to then being able to study its theoretical behavior in our main result. We report the proofs of Thm. 5 and Cor. 6 in Appendix C.1.

**Inexact PMD Converges**. We now present a more rigorous version of the characterization of the convergence rates of the inexact PMD algorithm discussed informally in Thm. 1.

**Theorem 7** (Convergenge of Inexact PMD).: _Let \((_{t})_{t}\) be a sequence of policies generated by \(Alg.\)\(1\) that are all \((,)\)-compatible. If the action-value functions \(_{_{t}}\) are estimated with an error \(\|q_{_{t}}-_{_{t}}\|_{}_{t}\), the iterates of Alg. 1 converge to the optimal policy as_

\[J(_{*})-J(_{T})_{T}+O(+_{t =0}^{T-1}_{t}), \]

_where \(_{*}:()\) is a measurable maximizer of (8)._

Thm. 7 shows that inexact PMD can behave comparably to its exact version provided that 1) the action value functions \(_{_{t}}\) are estimated with increasing accuracy, and 2) that the sequence of policies is \((,)\)-compatible, for example in the Sobolev-based setting of Cor. 6. Specifically, if \(\|q_{_{t}}-_{_{t}}\|_{} O(1/t)\) for any \(t\), the convergence rate of inexact PMD is of order \(O( T/T)\), only a logarithmic factor slower than exact PMD. This means that we do not necessarily need a good approximation of the world model from the beginning but rather a strategy to improve upon such approximation as we perform more PMD iterations. This suggests adopting an alternating strategy between exploration (world model learning) and exploitation (PMD steps), as suggested in Sec. 4. We do not investigate this question in this work.

The demonstration technique used to prove Thm. 7 follows closely [12, Thm. 8 and 13]. We provide a proof in Appendix B.1 since the original result did not allow for a decreasing approximation error but rather assumed a constant one. Moreover, extending it to the case of infinite \(\) requires taking care of additional details related to potential measurability issues.

**Action-value approximation error in terms of World Model estimates**. Thm. 7 highlights the importance of studying the error of the estimator for the action-value functions produced by Alg. 1. These objects are obtained via the formula described in Proposition 3 in terms of \(_{n}\), \(r_{n}\) and \(_{}\). The exact \(q_{}\) has an analogous closed-form characterization in terms of \(\), \(r\) and \(_{}\) as expressed in (7), and motivating our operator-based approach. The following result compares these quantities in terms of the approximation errors of the world model and the reward function.

**Lemma 8** (Implications of the Simulation Lemma).: _Let \(_{n}\) and \(r_{n}\) the empirical estimators of the transfer operator \(\) and reward function \(r\) as defined in Proposition 3, respectively. If \(\) satisfies Ass. 1, \(r\), and \(_{n}<^{}<1\), then, for every \((,)\)-compatible policy \(\)_

\[_{}-q_{}_{}} [_{}r_{n}-r_{}+r_{}}{1-}_{\|}- _{n}_{}].\]In the result above, when applied to a function in \(\), such as \(r_{n}\), the uniform norm is to be interpreted as the uniform norm of the evaluation of such function, namely \(\|r_{n}\|_{}=_{(x,a)}|\, r_{n}, (x,a)\,|\), and analogously for \(_{n}\). The proof, reported in Appendix C.2, follows by first decomposing the difference \(q_{}-_{}\) with the simulation lemma [31, Lemma 2.2] and then applying the triangular inequality for the uniform norm.

**POWR converges**. We are ready to state the convergence result for Alg. 1. We consider the setting where the dataset used to learn \(_{n}\) (and \(r_{n}\)) is made of i.i.d. triplets \((x_{i},a_{i},x^{}_{i})\) with \((x_{i},a_{i})\) sampled from a distribution \(()\) supported on all \(\) (such as the state occupancy measure (see e.g.  or Appendix A.3) of the uniform policy \((|x)=1/||\)) and \(x^{}_{i}\) sampled from \((|x_{i},a_{i})\). To guarantee bounds in uniform norm, the result makes a further regularity assumption, of the transfer operator (and the reward function)

**Assumption 2** (Strong Source Condition).: _Let \(()\) and \(C_{}\) the covariance operator \(_{a}_{}(dx,a)(x,a)(x,a)\). The transition operator \(\) and the reward function \(r\) are such that \(|_{}(,)\) and \(r\). Further, \(\|(|_{})^{*}C_{}^{-}\|_{} <\) and \(\|C_{}^{-}r\|_{}<\) for some \(>0\)._

Asm. 2 imposes a strong requirement to the so-called _source condition_, a quantity that describes how well the target objective of the learning process (here \(\) and \(r\)) "interact" with the sampling distribution. The assumption is always satisfied when the hypothesis space is finite dimensional (e.g. in the tabular RL setting) and imposes additional smoothness on \(\) and \(r\) when belonging to a Sobolev space. Equipped with this assumption, we can now state the convergence theorem for Alg. 1.

**Theorem 9**.: _Let \((_{t})_{t}\) be a sequence of policies generated by Alg. 1 in the same setting of Cor. 6. If the action-value functions \(_{_{t}}\) are estimated from a dataset \((x_{i},a_{i},x^{}_{i})_{i=1}^{n}\) with \((x_{i},a_{i})()\) such that Asm. 2 holds with parameter \(\), the iterates of Alg. 1 converge to the optimal policy as_

\[J(_{*})-J(_{T}) O(+^{2}n^{-})\]

_with probability not less than \(1-4e^{-}\). Here, \((,)\) and \(_{*}:()\) is a measurable maximizer of (8)._

The proof of Thm. 9 is reported in Appendix C and combines the results discussed in this section with fast convergence rates for the least-squares  and CME  estimators. In particular we first use Thm. 5 to guarantee that the policies produced by Alg. 1 are all \((,)\)-compatible and therefore that applying Proposition 3 to obtain an estimator for the action-value function is well-defined. Then, we use Lemma 8 to study the approximation error of these estimators in terms of our estimates for the world model and the reward function. Bounds on these quantities are then used in the result for inexact PMD convergence in Thm. 7. We note here that since the latter results require convergence in uniform norm, we cannot leverage standard results for least-squares and CME convergence, which characterize convergence in \(L_{2}(,)\) and would only require Asm. 1 (Linear MDP) to hold. Rather, we need to impose Asm. 2 to guarantee faster rates in uniform norm.

## 6 Experimental results

We empirically evaluated POWR on classical _Gym_ environments , ranging from discrete (FrozenLake-v1,Taxi-v3) to continuous state spaces (MountainCar-v0). To ensure balancing between exploration and exploitation of our method, we alternated between running the environment with the current policy to collect samples for world model learning and running Alg. 1 for a number of steps to generate a new policy. Appendix D provides implementation details regarding this process as well as additional results.

Fig. 1 compares our approach with the performance of well-established baselines including A2C , DQN , TRPO , and PPO . The figure reports the average cumulative reward obtained by the models on test environments with respect to the number of interactions with the MDP (_timesteps_ in log scale in the figure) across 7 different training runs. In all plots, the horizontal dashed line represents the "success" threshold for the corresponding environment, according to official guidelines. We observe that our method outperforms all competitors by a significant margin in terms of sample complexity, that is, the reward achieved after a given number of timesteps. In the case of the Taxi-v3environment, it avoids converging to a local optimum, in contrast every other method with the exception of DQN. On the downside, we note that our method exhibits less stability than other approaches, particularly during the initial stages of the training process. This is arguably due to a sub-optimal interplay between exploration and exploitation, which will be the subject of future work.

## 7 Conclusions and Future Work

Motivated by recent advancements in policy mirror descent (PMD), this work introduced a novel reinforcement learning (RL) algorithm leveraging these results. Our approach operates in two, possibly alternating, phases: learning a world model and planning via PMD. During exploration, we utilize conditional mean embeddings (CMEs) to learn a world model operator, showing that this procedure is well-posed when performed over suitable Sobolev spaces. The planning phase involves PMD steps for which we guarantee convergence to a global optimum at a polynomial rate under specific MDP regularities.

Our analysis opens avenues for further exploration. Firstly, extending PMD to infinite action spaces remains a challenge. While we introduced the operatorial perspective on RL for infinite state space settings, the PMD update with KL divergence requires approximation methods (e.g., Monte Carlo) whose impact on convergence requires investigation. Secondly, scalability to large environments requires adopting approximated yet efficient CME estimators like Nystrom  or reduced-rank regressors . Thirdly, a question we touched upon only empirically, is whether alternating world model learning with inexact PMD updates benefits the exploration-exploitation trade-off. Studying this strategy's impact on convergence is a promising future direction. Finally, a crucial question is generalizing our policy compatibility results beyond Sobolev spaces. Ideally, a representation learning process would identify suitable feature maps that guarantee compatibility with the PMD-generated policies while allowing for added flexibility in learning the world model.