# Unified Covariate Adjustment for Causal Inference

Yonghan Jung\({}^{1}\), Jin Tian\({}^{2}\), and Elias Bareinboim\({}^{3}\)

\({}^{1}\)Purdue University

\({}^{2}\)Mohamed bin Zayed University of Artificial Intelligence

\({}^{3}\)Columbia University

\({}^{1}\)jung222@purdue.edu, \({}^{2}\)jin.tian@mbzuai.ac.ae, \({}^{3}\)eb@cs.columbia.edu

###### Abstract

Causal effect identification and estimation are fundamental tasks found throughout the data sciences. Although causal effect identification has been solved in theory, many existing estimators only address a subset of scenarios, known as the sequential back-door adjustment (SBD) (Pearl and Robins, 1995a) or g-formula (Robins, 1986). Recent efforts for developing general-purpose estimators with broader coverage, incorporating the front-door adjustment (FD) (Pearl, 2000) and others, are not scalable due to the high computational cost of summing over a high-dimensional set of variables. In this paper, we introduce a novel approach that achieves broad coverage of causal estimands beyond the SBD, incorporating various sum-product functionals like the FD, while achieving scalability - estimated in polynomial time relative to the number of variables and samples in the problem. Specifically, we present the class of _unified covariate adjustment (UCA)_ for which we develop a scalable and doubly robust estimator. In particular, we illustrate the expressiveness of UCA for a wide spectrum of causal estimands (e.g., SBD, FD, and others) in causal inference. We then develop an estimator that exhibits computational efficiency and double robustness. Experiments corroborate the scalability and robustness of the proposed framework.

## 1 Introduction

Causal inference is a crucial aspect of scientific research, with broad applications ranging from social sciences to economics, and from biology to medicine. Two significant tasks in causal inference are causal effect identification and estimation. _Causal effect identification_ concerns determining the conditions under which the causal effect can be inferred from a combination of available data distributions and a causal graph depicting the data-generating process. _Causal effect estimation_, on the other hand, develops an estimator for the identified causal effect expression using finite samples.

Causal effect identification theories have been well-established across various scenarios. These include cases where the input distribution is purely observational (Tian and Pearl, 2003; Shpitser and Pearl, 2006; Huang and Valtorta, 2006) (known as _observational identification_ or obsID) or a combination of observational and interventional (Bareinboim and Pearl, 2012a; Lee et al., 2019) (referred to as _generalized identification_ or gID); scenarios where the target query and input distributions originate from different populations (Bareinboim and Pearl, 2012b; Bareinboim et al., 2014; Bareinboim and Pearl, 2016; Correa et al., 2018; Lee et al., 2020) (known as _recoverability_ or _transportability_); or cases where the target query is _counterfactual_ (Rung 3) (Correa et al., 2021) (referred to as Ctf-ID) beyond interventional (Rung 2) of the _Ladder of Causation_(Pearl and Mackenzie, 2018; Bareinboim et al., 2020). In these situations, algorithmic solutions have been devised that take input distributions along with specified target queries and formulate identification functionals as arithmetic operations (sums/integration, products, ratios) on conditional distributions induced from input distributions.

Despite all the progress, existing estimators cover only a subset of all identification scenarios. Specifically, well-established estimators for the back-door (BD) adjustment (Pearl, 1995), represented as \(_{z}[Y x,z]P(z)\), and sequential back-door adjustment (SBD) (Robins, 1986; Pearl and Robins, 1995b) and off-policy evaluations (OPE) (Murphy, 2003), which is an SBD with policy interventions, are known for their robustness to the bias (Bang and Robins, 2005; Robins et al., 2009; van der Laan and Gruber, 2012; Murphy, 2003; Rotnitzky et al., 2017; Luedtke et al., 2017; Uehara et al., 2022; Diaz et al., 2023). These estimators are also _scalable_; i.e., evaluable in polynomial time relative to the number of covariates (\(|Z|\)) and capable in the presence of mixed discrete and continuous covariates. However, SBDs only address a fraction of the broader spectrum of identification scenarios.

Beyond SBD, recent efforts have expanded to developing estimators for the front-door (FD) adjustment \(_{z,x^{}}[Y x^{},z]P(z x)P(x^{})\)(Pearl, 1995). At first glance, this adjustment appears similar to SBD, as both involve the sum-product of conditional probabilities. However, FD involves treatments variables in dual roles - one being summed (\(x^{}\) in \(_{x^{}}[Y x^{},z]P(x^{})\)) and the other being fixed (\(x\) in \(P(z x)\)). While FD estimators achieving doubly robustness have been developed (Fulcher et al., 2019; Guo et al., 2023), they lack scalability due to the necessity of summing over the values of \(Z\) (i.e., \(_{z}\)), thereby limiting its practicality when \(Z\) is high-dimensional or continuous.

Similar challenges arise in more general identification scenarios beyond SBD and FD. Recent efforts have focused on developing estimators for broad causal estimands, such as _Tian's adjustment_(Tian and Pearl, 2002a), which incorporates FD and other cases where causal effects are represented as sum-product functionals (Bhattacharya et al., 2022). These efforts also include work on covering any identification functional (Jung et al., 2021a; Xia et al., 2021, 2022; Bhattacharya et al., 2022; Jung et al., 2023a). While these estimators are designed to achieve a wide coverage of functionals, they lack scalability due to the necessity of summing over high-dimensional variables.

Thus far, we have assessed the pair (functional class, estimator) based on two criteria: (1) _coverage_ of the functional class, and (2) _scalability_ of the corresponding estimators. Scalable estimators achieving doubly robustness have been established predominantly for BD/SBD classes. While recent studies have developed estimators with a strong emphasis on coverage (e.g., any identification functional), less attention has been given to achieving scalability.

In this paper, we establish a novel pair of a functional class and its corresponding estimation frameworks designed to ensure scalability while covering a broad spectrum of identification functionals. Our work aims to maximize coverage, enabling the effective development of scalable estimators with the doubly robust property. This functional class, termed _unified covariate adjustment_ (UCA), integrates a sum-product of conditional distributions appearing in many causal inference scenarios such as BD/FD, Tian's adjustment, \(S\)-admissibility in transportability/recoverability (Bareinboim and Pearl, 2016), effect-of-treatment-on-the-treated (ETT) (Heckman, 1992), and nested counterfactuals (Correa et al., 2021). The coverage of the proposed class is further demonstrated through the application to a novel estimand for the counterfactual directed effect (Ctf-DE) derived from fairness analysis (Plecko and Bareinboim, 2024). For the proposed UCA class, we develop a scalable and doubly robust estimator evaluable computationally efficiently relative to the number of samples. Table 1 visualizes the scope of our framework. The contributions of this paper are as follows:

1. We introduce _unified covariate adjustment (UCA)_, a comprehensive framework that encompasses a broad class of sum-product causal estimands. This framework's expressiveness is demonstrated across various scenarios beyond SBD, including Tian's adjustment that incorporates FD and others as well novel counterfactual scenarios in fairness analysis.
2. We develop a corresponding estimator that is computationally efficient and doubly robust and provide its finite sample guarantee. We demonstrate scalability and robustness to bias both theoretically and empirically through simulations.

    &  &  \\   & **Prior** & **UCA** & **Prior** & **UCA** \\ 
**BD/SBD/OPE** & \(\) & \(\) & \(\) & \(\) \\ 
**FD** & \(\) & \(\) & \(\) & \(\) \\ 
**Tian’s** & \(\) & \(\) & \(\) & \(\) \\ 
**obsID/gID** & \(\) & \(\) & \(\) & \(\) \\ 
**Ctf-ID** & \(\) & \(\) &? & \(\) \\ 
**Transportability** & \(\) & \(\) &? & \(\) \\   

Table 1: Scope. \(\) denotes the addressed area (by UCA or prior works). \(\) denotes the unaddressed area. \(\) denotes the partially addressed area. \(?\) indicates areas where no known results are present.

**Notations.** We use (\(\), \(X\), \(\), \(x\)) to denote a random vector, variable, and their realized values, respectively. For a function \(f(_{i})\) for \(i=1,2,\), we use \(_{i}f(_{i})=f(_{1})+f(_{2})\). Also, for a function \(f()\), we use \(_{}f()\) to denote the summation/integration over a mixture of discrete/continuous random variables \(\). For example, we write the back-door adjustment as \(_{}_{p}[Y x,]P()\) when \(\) is a mixture of discrete/continuous variables. Given an ordered set \(=\{X_{1},,X_{n}\}\), we denote \(^{(i)}\{X_{1},,X_{i}\}\) and \(^{}\{X_{i+1},,X_{m}\}\) for \(m=||\). For a discrete \(\), we use \(_{}()\) as a function such that \(_{}()=1\) if \(=\); \(_{}()=0\) otherwise. \(P()\) denotes a distribution over \(\) and \(P()\) as a probability at \(=\), We use \(_{P}[f()]\) and \(_{P}[f()]\) to denote the mean and variance of \(f()\) relative to \(P()\). We use \(\|f\|_{P}_{P}[\{f()\}^{2}]}\) as L2-norm of \(f\) with \(P\). If a function \(\) is a consistent estimator of \(f\) having a rate \(r_{n}\), we will use \(-f=o_{P}(r_{n})\). We will say \(\) is \(L_{2}\)-consistent if \(\|-f\|_{P}=o_{P}(1)\). We will use \(-f=O_{P}(1)\) if \(-f\) is bounded in probability. Also, \(-f\) is said to be bounded in probability at rate \(r_{n}\) if \(-f=O_{P}(r_{n})\). \([n]\{1,,n\}\) is a collection of index. \(\{_{(i)}:i[n]\}\) denotes a sample set, where \(_{(i)}\) denote the \(i\)th sample in \(\). The empirical average of \(f()\) with samples \(\) is \(_{}[f()](1/||)_{i: _{(i)}}f(_{(i)})\).

**Structural causal models.** We use structural causal models (SCMs) (Pearl, 2000; Bareinboim et al., 2020) as our framework. An SCM \(\) is a quadruple \(=,,P(),\), where \(\) is a set of exogenous (latent) variables following a joint distribution \(P()\), and \(\) is a set of endogenous (observable) variables whose values are determined by functions \(=\{f_{V_{i}}\}_{V_{i}}\) such that \(V_{i} f_{V_{i}}(_{i},_{i})\) where \(_{i} V\) and \(_{i}\). Each SCM \(\) induces a distribution \(P()\) and a causal graph \(=()\) over \(\) in which directed edges exists from every variable in \(_{i}\) to \(V_{i}\) and dashed-bidirected arrows encode common latent variables. Performing an intervention fixing \(=\) is represented through the do-operator, \((=)\), which encodes the operation of replacing the original equations of \(X\) (i.e., \(f_{X}(_{x},_{x})\)) by the constant \(x\) for all \(X\) and induces an interventional distribution \(P(())\). For any \(\), the _potential response_\(_{}()\) is defined as the solution of \(\) in the submodel \(_{}\) given \(=\), which induces a _counterfactual variable_\(_{}\).

**Related work.** Our work is an extension of existing sequential back-door adjustment (SBD) estimators (Mises, 1947; Bickel et al., 1993; Bang and Robins, 2005; Robins et al., 2009; van der Laan and Gruber, 2012; Rotnitzky et al., 2017; Luedtke et al., 2017; Diaz et al., 2023) to a broader class of sum-product functionals, such as the front-door adjustment (FD) and Tian's adjustment (Tian and Pearl, 2002a) which generalizes FD and more, and nested counterfactuals, which will be detailed in later sections. Our work is aligned with recent works of Chernozhukov et al. (2022); Li and Luedtke (2023); Quintas-Martinez et al. (2024), which examined SBD derived from various joint distributions. Specifically, Li and Luedtke (2023) considered the SBD setting where conditional distributions are induced from different sources. In contrast, we study a broader class of sum-product functionals from multiple populations. Also, Quintas-Martinez et al. (2024) considered the Markovian model \(_{i=1}^{n}P^{i}(V_{i}_{i})\) where each \(P^{i}\) can be distinct. In contrast, we study a broader class of estimands that are not confined to conditioning on \(_{i}\). On the other hands, (Chernozhukov et al., 2022) considered the case where covariate distributions are allowed to be changed, and demonstrated that FD can be captured through this technique. Our work expands on these findings by covering a broader class, such as the Tian's adjustment and a nested counterfactual in fairness literature, and by providing a more formal theory that includes finite sample guarantees and asymptotic analysis.

## 2 Unified Covariate Adjustment

A class of causal estimands termed _unified covariate adjustment (UCA)_ is defined as follows:

Figure 1: **(a) Front-door in Example 1, (b) Verma in Example 2, (c) Napkin, (d) Standard fairness model in Example 3, and (e) Example graph from (Jung et al., 2021a, Fig. 0(b))**

**Definition 1** (**Unified Covariate Adjustment (UCA)**).: _Let \([};]\) denote the following probability measure over an ordered set \((_{1},_{1},,_{m},_{m},Y_{m+1})\): \([;] P^{m+1}(Y_{m})_{i=1}^{m }P^{i}(_{i}_{i-1})_{_{i}}^{i}(_{i}_{i}_{i}),\) where_

* \(\{P^{i}():i[m+1]\}\) _is a set of distributions in the form of_ \(P^{i}()=Q^{i}(_{i-1}^{b}=_{i-1}^{b})\)_, where_ \(Q^{i}\) _is a distribution,_ \(_{i-1}^{b}\) _is a (potentially empty) set. Each pairs_ \(P^{i}()\) _and_ \(P^{j}()\) _can be the same (_\(P^{i}()=P^{j}()\)_) or distinct (_\(P^{i}() P^{j}()\)_)._
* _For_ \(i[m+1]\)_,_ \(_{i-1}(^{(i-1)}^{(i-1)}) _{i-1}^{b}\)_._
* _Each_ \(_{i}\) _is controlled by a pre-specified / known probability measure_ \(_{_{i}}^{i}_{_{i}}^{i}(_{i} _{i}_{i})\) _where_ \(_{_{i}}_{_{i}}^{i}(_{i}_{ i}_{i})=1\) _and_ \(0_{_{i}}^{i} 1\) _almost surely (e.g.,_ \(_{_{i}}^{i} 1_{_{i}}(_{i})\)_)._

_Then, the expectation of \(Y\) over \([;]\) is called a **Unified Covariate Adjustment (UCA)**:_

\[_{0}_{[;]}[Y]=_{ }_{P^{m+1}}[Y_{m}]_{i=1}^{m}P^{i}( _{i}_{i-1})_{_{i}}^{i}(_{i} _{i}_{i}). \]

We will exemplify that UCA encompasses many well-known causal estimands, including the sequential back-door adjustment (SBD) (Robins, 1986; Pearl and Robins, 1995b), front-door adjustment (Pearl, 1995), Tian's adjustment (Tian and Pearl, 2002a), \(S\)-admissibility in transportability/recoverability (Bareinboim and Pearl, 2016), effect-of-treatment-on-the-treated (ETT) (Heckman, 1992), nested counterfactuals (Correa et al., 2021), treatment-treatment interaction (Jung et al., 2023b), and off-policy evaluation (Murphy, 2003). This section particularly focuses on recently developed and lesser-known estimands, for which scalable estimators have been rarely explored. Appendix B provides additional examples, demonstrating how UCA can represent well-known estimands such as off-policy evaluation and \(S\)-admissibility.

At first glance, UCA closely resembles the sequential back-door adjustment (SBD) (Robins, 1986; Pearl and Robins, 1995b). Indeed, UCA is reduced to SBD in the special case where \(P^{i}=P()\) for all \(i=1,,m+1\) and \(_{_{i}}^{i} 1_{_{i}}(_{i})\); i.e., \(_{0}=_{}_{P}[Y^{(m)}^ {(m)})]_{i=1}^{m}P(_{i}^{(i-1)}^{( i-1)})\). However, UCA provides flexibility to represent target estimands beyond SBD by allowing \(P^{i}\) to be any distribution that aligns with the target estimand, permitting arbitrary conditional distributions beyond the observational distribution \(P\). To demonstrate, consider the front-door adjustment (FD) scenario (Pearl, 1995) depicted in Fig. 0(a):

\[[Y(x)]=_{c,z,x^{}}[Y c,x^{ },z]P(z c,x)P(c,x^{}). \]

Even though FD cannot be expressed using SBD because the treatment variable \(X\) is being fixed (in \(P(z c,x)\)) and summed (with \(_{x^{}}\)) simultaneously, it can be represented through UCA as follows:

**Example 1** (**FD as UCA**).: _FD can be written as the expectation of \(Y\) over \(P(Y Z,X,C)P(Z x,C)P(X,C)\). We set \(_{1}\{X,C\}\), \(_{2}\{Z\}\), \(=\), \(P^{1}(_{1})=P(X,C)\), \(P^{2}(_{2}_{1})=P(Z x,C)\) with \(_{1}^{b}=\{X\}\), \(_{1}=\{C\}\), and \(P^{3}(Y_{3})=P(Y Z,X,C)\) with \(_{2}=\{Z,X,C\}\)._

Next, consider _Verma's equation_ (Verma and Pearl, 1990; Tian and Pearl, 2002b) with Fig. 0(b):

\[[Y(x)]=_{b,a,x^{}}[Y b,a,x]P( b a,x^{})P(a x)P(x^{}), \]

where \(X\) is fixed to \(x\) in \([Y x,a,b]\) and \(P(a x)\) while summed in \(P(b a,x^{})\) and \(P(x^{})\). Similar to FD, due to the dual role of \(X\), the existing SBD framework is not suitable to express Verma's equation, which can be represented through UCA as follows:

**Example 2** (**Verma as UCA**).: _Verma's equation is expressible as the expectation of \(Y\) over \(P(Y B,A,x)P(B A,X)P(A x)P(X)\). We set \(_{1}=\{X\}\), \(_{2}=\{A\}\), \(_{3}=\{B\}\), and \(=\). We map \(P^{1}(_{1}) P(X)\), \(P^{2}(_{2}_{1})=P(A x)\) with \(_{1}=\), \(_{1}^{b}=\{X\}\), \(P^{3}(_{3}_{2})=P(B A,X)\) with \(_{2}=\{A,X\}\), and \(P^{4}(Y_{3})=P(Y B,A,x)\) with \(_{3}=\{B,A\}\), \(_{3}^{b}=\{X\}\)._

In both examples, the variable \(^{b}i=X\) is _bifurcated_, being fixed in some conditional distributions (e.g., \(P(z x,c)\) in the front-door criterion (FD)) and summed over \( x^{}\) in others (e.g., \(P(y z,x^{},c)\)in FD). Both FD and Verma's equations are special cases of _Tian's adjustment_(Tian and Pearl, 2002a), which states that \([Y(x)]\) is identifiable under certain conditions. Specifically, when \(X\) and its children \((X)\) in the graph \(\) are not connected by bidirected edges, it can be expressed as:

\[[Y(x)]=_{ xy}_{x^{ }}_{P^{}}[Y^{(K)}]_{i=1}^{K}P^{} (v_{i}^{(i-1)}), \]

where \((V_{1},V_{2},,V_{K},Y)\) is a topologically ordered set with \(V_{k} X\) for some \(k\) being the treatment variable \(X\), \(P^{}(v_{i}^{(i-1)}) P(v_{i}^{(k-1)},x,v_{k+1},,v_{i-1})\) (i.e., \(X\) is fixed to \(x\)) if \(V_{i}_{X}\) where \(_{X}\) is the set of vertices connected with \(X\) through bidirected edges, and \(P^{}(v_{i}^{(i-1)}) P(v_{i}^{(k-1 )},x^{},v_{k+1},,v_{i-1})\) (i.e., \(X\) is summed with \(_{x^{}}\)) if \(V_{i}_{X}\). In Tian's adjustment, \(X\) is bifurcated into _summed_ through \(_{x^{}}\) and _fixed_ to \(X=x\). We exhibit the expressiveness of UCA for Tian's adjustment:

**Proposition 1**.: _Tian's adjustment in Eq. (4) is UCA-expressible through Algo. 1._

Next, we exhibit the coverage of the UCA for a counterfactual quantity in the fairness literature. Specifically, we focus on the counterfactual directed effect (Ctf-DE) in the _Standard fairness model_(SFM)(Plecko and Bareinboim, 2024), as illustrated in Fig. 1d. This model includes several key components: the protected (discrete) attribute (\(X\)), such as race; the baseline covariates (\(Z\)), like age; the mediator variables (\(W\)) affected by \(X\), for example, educational level; and the outcome variable (\(Y\)), such as salary. Consider a scenario where we investigate the the query, "_What would be the expected salary for someone who is Black, but hypothetically of Asian race and had been educated as a White person typically would be?_". The query is represented as Ctf-DE: \([Y_{X=x_{0},W_{X=x}} X=x_{2}]\), where \(x_{0}\), \(x_{1}\), and \(x_{2}\) correspond to the races Asian, White, and Black, respectively. This query can be identified through the algorithm in (Correa et al., 2021) under the SFM in Fig. 1d:

\[[Y_{X=x_{0},W_{X=x_{1}}} X=x_{2}]=_{w,z}[Y X=x_ {0},w,z]P(w X=x_{1},z)P(z X=x_{2}). \]

This identification functional is UCA-expressible:

**Example 3** (Ctf-DE as UCA).: _The Ctf-DE is expressible through the expectation of \(Y\) over \(P(Y X=x_{0},W,Z)P(W X,Z)P(Z X=x_{2})_{x_{1}}(X)\). Set \(_{1}\{X\}\), \(^{}_{_{1}}_{x_{1}}(X)\), \(P^{1}(_{1})=P(Z X=x_{2})\) with \(_{1}=\{Z\}\) and \(^{b}_{0}=\{X\}\), \(P^{2}(_{2}_{1})=P(W X,Z)\) with \(_{2}=\{W\}\) and \(_{1}=\{X,Z\}\), \(P^{3}(Y_{2})=P(Y X=x_{0},W,Z)\) with \(_{2}=\{W,Z\}\) and \(^{b}_{2}=\{X\}\)._

Despite the broad expressiveness of UCA, as illustrated in this section and appendix B, not all causal estimand functionals are UCA-expressible. To witness, consider the 'napkin' estimand described in (Pearl and Mackenzie, 2018; Jung et al., 2021a) with \(\) in Fig. 1c, defined as \(P(y(x))=P(y,x r,w)P(w)}{_{w}P(x r,w)P(w)}\). Here, the functional for \([Y(x)]\) is represented not as the expectation of a product of conditional distributions, but rather as a quotient of sums of conditional distributions. The napkin estimand is not UCA-expressible. Intuitively, if a target functional is expressed as an expectation of a probability measure that is represented as a product of multiple conditional distributions, it can be captured through UCA. A formal criterion is the following:

**Theorem 1** (**Expressiveness**).: _Suppose a functional \(_{0}\) is expressed as the mean of the following measure, \(P^{m+1}(Y^{}_{m})_{i=1}^{n}P^{i}(_{i} ^{}_{i-1})_{_{i}}^{i}(_{i} ^{}_{i}_{i})\), where \(^{}_{i}=(^{(i)}^{(i)}) ^{b}_{i}\) for each \(i=1,,m\) and \(P^{j}()\) for \(j=1,,m+1\) are distributions of the form \(P^{j}()=Q^{j}(^{b}_{j-1}=^{b}_{j-1})\). Then, the functional \(_{0}\) can be expressed through UCA in Eq. (1)._

## 3 Scalable Estimator for Unified Covariate Adjustment

So far, we discussed the _coverage_ of UCA. In this section, we construct a _scalable_ estimator for UCA that achieves doubly robustness property and provides its finite sample guarantee. We define the estimator with two sets of nuisance parameters \(\) and \(\). \(\) is a collection of regression parameters, and \(\) is a collection of ratio parameters.

We introduce sets to define regression nuisances. Define \(_{i-1}_{i}^{(i-1)}^{b }_{i-1}\) for \(i=2,,m\) as a bifurcated set, which is a subset of \(_{i}\) in \(P^{i+1}(_{i+1}_{i})\) that is fixed to \(^{b}_{i-1}\) at \(P^{i}(_{i}_{i-1})\), while marginalized out over \(P^{j}(_{j}_{j-1})\) for some \(j<i\) (e.g., \(X\) in FD). Set \(_{m}=\). We use \(^{}_{i-1}\) to denote an independent copy of \(_{i-1}\) (variables following the same distribution as \(_{i-1}\) but independent of \(_{i-1}\) and \(\)). With \(_{i-1}\) and \(^{}_{i-1}\), we define \(^{}_{i}((_{i}_{i}) _{i-1})^{}_{i-1}\) and \(}_{i}^{}_{i}_{i}\) for \(i=2,,m\). Define the regression nuisance parameters as follows: \(_{0}^{m}(_{m})_{P^{m+1}}[Y_{m}]\) and \(_{0}^{m}(}_{m})_{_{m}} _{_{m}}^{m}(_{m}_{m}_{m} )_{0}^{m}(_{m},}_{m})\). For \(i=m-1,,1\),

\[_{0}^{i}(_{i},^{}_{i}) _{P^{i+1}}[_{0}^{i+1}( }_{i+1})_{i},^{}_{i}], \] \[_{0}^{i}(}_{i}) _{_{i}}_{_{i}}^{i}(_{i}_{i}_{i})_{0}^{i}(_{i}, ^{}_{i}). \]

Equipped with the regression nuisances, UCA can be computed as follows:

**Proposition 2**.: _UCA in Eq. (1) can be parameterized as \(_{0}=_{P^{1}}[_{0}^{1}(}_{1})]\)._

Whenever no variables are being summed and fixed simultaneously (i.e., \(_{i-1}=\) for all \(i=2,,m\)) in the UCA functional, as in Eq. (5) in Ctf-DE, the standard SBD adjustment or examples in Appendix B, we can estimate \(\) through nested regression methods with off-the-shelf regression models and compute UCA in Eq. (1) as \(_{0}=_{P^{1}}[_{0}^{1}(}_{1})]\). This approach aligns with existing SBD estimators (Bang and Robins, 2005; Robins et al., 2009; van der Laan and Gruber, 2012; Rontzky et al., 2017; Diaz et al., 2023). For instance, in Ctf-DE in Example 3, \(_{0}^{2}(W,Z)_{P}[Y W,Z,x_{0}]\), \(_{0}^{2}(W,Z)=_{0}^{2}(W,Z)\), \(_{0}^{1}(X,Z)_{P}[_{0}^{2}(W,Z) X,Z]\), \(_{0}^{1}(Z)=_{0}^{1}(x_{1},Z)\), and \(_{0}=_{P}[_{0}^{1}(Z) x_{2}]\). These nuisances can be estimated efficiently with regression models run in polynomial time relative to the number of variables and samples (e.g., neural networks (LeCun et al., 2015) or XGBoost (Chen and Guestrin, 2016)).

Beyond the SBD framework, the regression nuisances are capable of representing functionals in the presence of variables being summed and fixed simultaneously (e.g., FD in Eq. (2) or Verma in Eq. (3)). As an example, consider FD in Eq. (2) with its UCA representation in Example 1. First, define \(_{0}^{2}(Z,X,C)_{P}[Y Z,X,C]\) with \(_{2}=\{Z,X,C\}\). Next, we have \(_{1}=^{b}_{1}_{1}=\{X\}\) and, \(}_{2}=\{Z,X^{},C\}\), where \(X^{}\) is an independent copy of \(X\). Consequently, \(_{0}^{2}(Z,X^{},C)_{0}^{2}(Z,X^{},C)\), where \((Z,X^{},C)\) is plugged into a function \(_{0}^{2}\). Next, define \(_{0}^{1}(C,X^{})_{P}[_{0}^{2}(Z,X^{},C ) x,C,X^{}]\). Finally, we have \(_{0}^{1}(C,X)=_{0}^{1}(C,X)\). The expectation, \(_{P}[_{0}^{1}(C,X)]=_{c,x^{}}P(c,x^{}) _{0}^{1}(c,x^{})\), correctly specifies FD in Eq. (2) as follows:

\[_{c,x^{}}P(c,x^{})_{0}^{1}(c,x^{})= _{c,x^{}}P(c,x^{})_{0}^{1}(c,x^{})=_{c,x^{}}P( c,x^{})_{P}[_{0}^{2}(Z,x^{},c) x,c,x^{}]\] \[=^{*}_{c,x^{},z}P(c,x^{})P(z x,c)_{0}^{2}(z, x^{},c)=_{c,x^{},z}P(c,x^{})P(z x,c)_{P}[Y z,x^{ },c],\]

where the equation \(=^{*}\) holds since \(X^{}\) is an independent copy of \(X\), so it's independent of \(Z\).

Empirically, generating \(^{}_{i}\) involves permuting copied samples of \(_{i}\), an used in recent works in (Chernozhukov et al., 2022; Xu and Gretton, 2022). We name this approach _empirical bifurcation_:

**Definition 2** (Empirical bifurcation).: _An **empirical bifurcation** for \(\) following a distribution \(P\) is the procedure of copying samples of \( P\) and randomly permuting to obtain new samples \(^{}\)._

In general, the regression nuisances can be estimated from data by employing empirical bifurcation and off-the-shelf regression models.

Next, we define the ratio nuisance parameters \(\). Define \(^{m}_{0}(_{m})\) as the solution functional satisfying \(_{p^{m+1}}[^{m}_{0}(_{m})^{m}_{0}]=_{0}\). Recursively, for \(i=m-1,,1\), define \(^{i}_{0}(_{i},^{}_{i})\) as a functional satisfying the following equation, for any \(^{i+1} L_{2}(P^{i+2})\).

\[_{P^{i+2}}[^{i+1}_{0}(_{i+1},^{ }_{i+1})^{i+1}(_{i+1},^{}_{i+1})]=_{P^{i+1}}[^{i}_{0}(_{i},^{}_{i})_{P^ {i+1}}[^{i+1}(_{i+1})_{i},^{ }_{i}]], \]

where the closed form solution is given as follows:

\[^{i}_{0}=^{i}P^{j}(_{j}_{j})^{j}_{_{j}}(_{j}_{j} _{j})}{P^{i+1}(_{i},^{}_{i})} \]

For the example of FD, \(^{2}_{0}=\) and \(^{1}_{0}=\).

Equipped with the ratio nuisances, UCA can be computed as follows:

**Proposition 3**.: _UCA in Eq. (1) can be parameterized as \(_{0}=_{P^{m+1}}[^{m}_{0}Y]\)._

Estimating the ratio nuisances may be challenging due to the distribution ratio of continuous/high-dimensional variables. To address the challenge, we use Bayes' rule to transform the distribution ratio into a more tractable form. For example, in FD, if the treatment \(X\) is a singleton binary, instead of estimating \(^{2}_{0}=\), an equivalent estimand \(^{2}_{0}=\) can be estimated. This approach allows to use off-the-shelf probabilistic classification methods for estimating distribution ratios, allowing scalable computation. A detailed procedure for ratio estimation is in Appendix C.2.

Combining regression and ratio-nuisances, we present a double/debiased machine learning (DML) (Chernozhukov et al., 2018)-based estimator \(\) for the UCA, titled 'DML-UCA', in Algo. 2. We provide detailed nuisance specification for various examples in Appendix A and B.

DML-UCA provides a scalable estimator for functionals expressible through UCA. When the target query is BD/SBD, DML-UCA aligns with existing doubly robust SBD estimators (Bang and Robins, 2005; Robins et al., 2009; van der Laan and Gruber, 2012; Rotnitzky et al., 2017; Luedtke et al., 2017; Diaz et al., 2023). Beyond SBD, DML-UCA can be estimated in polynomial time relative to the number of variables and samples, ensuring its scalability:

**Theorem 2** (Scalability).: _Algo. 2 runs in \(O(Kn_{}+T(m,n_{},K))\), where \(K\) is the number of distinct in \(\), \(n_{}\{|^{k}|:k[K]\}\), and \(T(m,n_{},K)\) is the time complexity forlearning nuisances \(^{i}_{}\) and \(^{i}_{}\). Specifically, \(O(T(m,n_{},K))=O(K L(T_{}+T_{}))\), where \(T_{}\{T_{^{i}_{}}:i[m],[L]\}\), \(T_{}\{T_{^{i}_{}}:i[m],[L]\}\), and \(T_{^{i}_{}}\) and \(T_{^{i}_{}}\) denote the time complexity for learning and evaluating \(^{i}_{}\) and \(^{i}_{}\), respectively._

An an example, for XGBoost (Chen and Guestrin, 2016), \(T_{}=T_{}=O(_{}_{ {tree}} n_{} n_{})\), where \(_{}\) and \(_{}\) are the number and depth of trees in XGBoost.

Table 2 summarizes the comparison of time complexities for existing estimators. As shown in the table, scalable estimators with polynomial time complexity have only been developed for BD/SBD estimands. Existing estimators beyond SBD often lack scalability. For instance, existing estimators for FD (Fulcher et al., 2019; Guo et al., 2023) or Tian's adjustment (Bhattacharya et al., 2022) face exponential time complexity in the dimension of mediators. In contrast, DML-UCA's polynomial time complexity positions it as a uniquely scalable solution within the UCA functional class, which includes FD and Tian's adjustment as special cases. For general obsID/gID estimands beyond the UCA class, scalable estimators have yet to be developed.

### Error analysis

In this section, we show that DML-UCA exhibits doubly robustness, in addition to scalability. Since UCA is composed of multiple (possibly distinct) distributions, we provide a tool to distinguish them.

**Definition 3** (Index set).: _The index sets \(_{1},,_{K}\) partition \(\{1,,m+1\}\) such that indices \(i\) and \(j\) are in the same set \(_{k}\) if and only if \(P^{i}()=P^{j}()\)._

We will use \(^{k}\) for \(k=1,,K\) to denote the distribution \(P^{i}\) for \(i_{k}\). Then, the functional \([;]\) in Eq. (1) can be written as follows:

\[[;]=[\{^{k}:k=1,,K\};]. \]

Since multiple distributions are involved in UCA, deriving an influence function for each distribution \(^{k}\) becomes necessary. A standard influence function is typically defined for a single distribution \(P\), and thus, does not suffice for studying multi-distribution setting. To address the issue, we employ a _partial influence function_ (PIF) (Pires and Branco, 2002), an influence function defined relative to each \(^{k}\). A formal definition is in Appendix C. For UCA, PIFs are given as follows:

**Theorem 3** (PIF for UCA).: _Assume that \(^{i}_{0}<\) and \(0<^{i}_{0}<\) almost surely for \(i=1,,m\). Define \(^{1}_{0}\{^{1}_{0}\}\) and \(^{i}_{0}\{^{i-1}_{0},^{i}_{0},^{i-1}_{0}\}\) for \(i=1,,m+1\), and_

\[^{i}(}_{i};^{i}_{0},_{0}) ^{i-1}_{0}\{^{i}_{0}-^{i-1}_{0}\}&\\ ^{i}_{0}-_{0}&. \]

   Estimand & Estimator & Complexity \\   & Plug-in & \(O(n2^{m})\) \\   & IPW (Rosenbaum and Rubin, 1983) & \\  & OM (Robins, 1986) & \\  & AIPW (Rotnitzky et al., 1998) & \\  FD & Fulcher et al. (2019); Guo et al. (2023) & \\ Tian’s & Bhattacharya et al. (2022) & \\   & DML-UCA (BD, FD and Tian’s) & \(O(n+T(m,n))\) \\   & DML-UCA (general) & \(O(Kn_{}+T(m,n_{},K))\) \\  obsID & DML-ID (Jung et al., 2021a) & \(O(n2^{2m}+T(m,n))\) \\  gID & DML-gID (Jung et al., 2023a) & \(O(Kn_{}2^{2m}+T(m,n_{},K))\) \\   

Table 2: Comparison of time complexities of existing estimators for estimands: \(n_{}\{|^{i}|\}\) is the number of samples, \(m\) is the number of variables, and \(T(m,n_{},K)\) (or \(T(m,n) T(m,n_{}=n,K=1)\)) is the time complexity for learning nuisance parameters for the target functional. The plug-in estimator for BD is one where \(_{P}[Y,]\) and \(P()\) are estimated from data, and \(_{}_{P}[Y,]P()\) is evaluated. Details are in Sec. C.4.

_Let \(^{k}_{i_{k}}}^{i}\) and \(_{0}^{k}_{i_{k}}_{0}^{i}\). Then, the \(k\)-th PIF for UCA is \(_{0}^{k}^{k}(^{k};_{0}^{k},_{0}) _{i_{k}}^{i}(^{i};_{0}^{i},_{0})\)._

Equipped with PIFs, we provide a finite-sample guarantee for DML-UCA, extending Chernozhukov et al. (2023) which analyzed DML estimators for BDs.

**Theorem 4** (**Finite sample guarantee**).: _Suppose \(_{0}^{i},_{}^{i}<\) and \(0<_{0}^{i},_{}^{i}<\) almost surely for \(i=1,,m\). Suppose the third moment of \(_{0}^{k}\) for \(k=1,,K\) exist. Let \(_{0}^{k}^{k}(^{k};_{0}^{k},_{0})\) and \(_{}^{k}^{k}(^{k};}_{}^{ k},_{0})\). Let \(R_{1}^{k}(1/L)_{=1}^{L}(_{_{}^{k}}[ _{}^{k}]-_{^{k}}[_{}^{k}])\). Then,_

1. _The error_ \(-_{0}\) _is decomposed as follows:_ \[-_{0}=_{k=1}^{K}R_{1}^{k}+_{=1}^{L}_{i =1}^{m}_{P^{i+1}}[(_{}^{i}-_{0}^{i})(_{0}^{i}- _{}^{i})].\] (13)
2. _Let_ \(_{,0}^{2}_{^{k}}[_{0}^{k}]\)_. With probability (W.P) greater than_ \(1-\)_,_ \[_{k=1}^{K}R_{1}^{k} K}(^{ K}^{2}}{|^{k}|}}+^{L}_{k=1}^{ K}_{}^{k}-_{0}^{k}\|_{^{k}}^{2}}{|_{ }^{k}|}}).\] (14)
3. _Let_ \(_{k,0}^{3}_{^{k}}[|_{0}^{k}|^{3}]\)_. Let_ \((x)\) _denote the standard normal CDF. W.P greater than_ \(1-\)_,_ \[|^{k}(^{k}|}}{_{k,0}}R_{1}^{k }<x)-(x)|}_{=1}^{L}_{}^{k}-_{0}^{k}\|_{ ^{k}}^{2}}{|_{}^{k}|}}+^{3}}{ _{k,0}^{3}^{k}|}},\] (15)

This is a novel finite sample guarantee of DML-based estimators for functionals beyond SBD. Finite sample analyses for functionals beyond SBD have been studied only for the non-doubly robust estimators (Bhattacharyya et al., 2022). For doubly robust estimators, only asymptotic analyses were provided for FD (Fulcher et al., 2019; Guo et al., 2023), Tian's adjustment (Bhattacharya et al., 2022), and obsID (Jung et al., 2021). Thm. 4 elucidates that the error can be decomposed into two terms \(R_{1}^{k}\) and \(R_{2}^{}\). The term \(R_{1}^{k}\) closely approximates a standard normal distribution variable, and \(R_{2}^{}\), comprises the error of \((_{}^{i},_{}^{i-1})\) and \(^{i}\), exhibiting doubly-robustness behavior. Specifically, if the nuisance parameters \(_{}^{i},_{}^{i}\), and \(_{}^{i-1}\) converge at a rate of \(n^{-1/4}\) (where \(n\) represents the size of the smallest sample set), then DML-UCA converges at a faster rate of \(n^{-1/2}\). This point becomes evident in the corresponding asymptotic analysis:

**Corollary 4** (**Asymptotic error**).: _Assume \(_{0}^{i},_{0}^{i}<\) and \(0<_{0}^{i},_{0}^{i}<\) almost surely. Suppose the map \(}_{}^{k}_{}^{k}\) is uniformly differentiable with respect to \(}_{}^{k}\), and the derivative of \(_{}^{k}\) w.r.t. \(}_{}^{k}\) is bounded by some constants. Suppose \(_{}^{i}\) and \(_{}^{i}\) are \(L_{2}\)-consistent. Then,_

\[-_{0}=_{k=1}^{K}R_{1}^{k}+_{=1}^{L}_{i =1}^{m}O_{P^{i+1}}(\|_{}^{i}-_{0}^{i}\|(\|_{ }^{i}-_{0}^{i}\|)),\]

_and \(^{k}|}R_{1}^{k}\) converges in distribution to normal\((0,_{k,0}^{2})\)._

## 4 Experiments

In this section, we demonstrate the _scalability_ and _doubly robustness_ of the DML-UCA estimator, where nuisances are learned through XGBoost (Chen and Guestrin, 2016). We specify an SCM \(\) for FD (Fig. 0(a)), Verma (Fig. 0(b)), and the example graph in (Jung et al., 2021) (Fig. 0(e)), and generate datasets \(^{k}^{k}\) from the SCM. The target estimand is denoted as \(_{0}\). Details are in Appendix F. Further simulations are provided in Appendix E.

**Scalability.** To demonstrate scalability of DML-UCA, we compare the running time with existing estimators of (Fulcher et al., 2019) (FD) and (Jung et al., 2021) (Verma's equation and the estimandwith Fig. 1e -- \([Y(x_{1},x_{2})]=_{x^{}_{1},r_{2}}_{P}[Y r,x^{}_{1},x_{2},z]P(r x_{1},z)P(z,x^{}_{1})\) -- which we call 'Jung's equation'). For each example, we increment the dimension of the summed variables, run 100 simulations, take the average of running times, and compare this average. We label this plot as 'run-time-plot', presented in the top side of Fig. 2. In the comparison with (Fulcher et al., 2019) for FD in Fig. 2a, we fix \(|C|=2\) and increment \(|Z|=\{2,4,6,8,12,20,30,50,100\}\). When comparing with (Jung et al., 2021a), for Verma's equations in Figs. (2b), we fix \(|A|=2\) and increment \(|B|=\{2,4,6,8,12,20,30,50,100\}\). For Jung's equation in Fig. 2c, we fix \(|Z|=2\) and \(|R|=\{2,4,6,8,12,20,30,100\}\). The timeout for the run-time is set to 300 seconds. For all scenarios, the run-time of existing estimators increases rapidly over dimensions due to the summation operation while DML-UCA scales well for high-dimensional covariates.

**Doubly robustness.** To demonstrate doubly robustness, we compare the error of DML-UCA with existing estimators for FD of Fulcher et al. (2019) and for Verma's and Jung's equations of Jung et al. (2021a) We use \(^{}\) for \(\{,,\}\) to denote each estimator. We use the average absolute error (AAE), which is an average of the error of the estimated versus true causal effect of \(=\): \(()|}_{ {domain}()}|^{}()-_{0}()|\). To witness the fast convergence of DML-UCA, we enforce the convergence rate of nuisance estimates to be no faster than the decaying rate \(n^{-1/4}\) by adding the noise term \((n^{-1/4},n^{-1/4})\) to nuisances, inspired by the experimental design in (Kennedy, 2023). We ran 100 simulations for each number of samples \(n=\{2500,5000,10000,20000\}\). We label the plot as 'AAE-plot', presented in the bottom side of Fig. 2. For each example, DML-UCA outperforms other estimators, exhibiting fast convergence.

## 5 Conclusions

We introduce a framework that encompasses a broad class of sum-product causal estimands, called UCA class, for which scalable estimators were previously unavailable. We demonstrate the expressiveness of the UCA class, which includes not only BD/SBD but also broader classes such as Tian's adjustment incorporating FD and Verma, and Ctf-DE, for which the existing SBD-based framework is not applicable. We develop an estimator for UCA called DML-UCA that can estimate the target estimand in polynomial time relative to the number of samples and variables, ensuring scalability. We provide finite-sample guarantees and corresponding asymptotic error analysis for DML-UCA, demonstrating its fast convergence. These scalability and fast convergence properties are empirically verified through simulations. Our results pave the way toward developing an estimation framework maximizing both coverage and scalability in Table 1.

Figure 2: Comparison of DML-UCA (‘DML’) with existing estimators using (**Top**) running-time-plots (_x-axis_: the dimension of summed variables, _y-axis_: running time); and **Bottom**) AAE-plots (_x-axis_: the sample size, _y-axis_: errors). DML-UCA is compared with **(a,d)** Fulcher et al. (2019) for FD; **(b.e)**(Jung et al., 2021a) for Verma’s equation; and **(c,f)** Jung et al. (2021a) for Jung’s equation.