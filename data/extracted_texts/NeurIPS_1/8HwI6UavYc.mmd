# ReplaceAnything3D: Text-Guided Object Replacement in 3D Scenes with Compositional Scene Representations

ReplaceAnything3D: Text-Guided Object Replacement in 3D Scenes with Compositional Scene Representations

Edward Bartrum

University College London

London, England

edward.bartrum.18@ucl.ac.uk

&Thu Nguyen-Phuoc

Meta Reality Labs

London, England

thunp@meta.com

&Chris Xie

Meta Reality Labs

Redmond, Washington

chrisdxie@meta.com

&Zhengqin Li

Meta Reality Labs

Redmond, Washington

zhl@meta.com

&Numair Khan

Meta Reality Labs

Redmond, Washington

numairkhan@meta.com

&Armen Avetisyan

Meta Reality Labs

London, England

aavetisyan@meta.com

&Douglas Lanman

Meta Reality Labs

Redmond, Washington

douglas.lanman@meta.com

&Lei Xiao

Meta Reality Labs

Redmond, Washington

lei.xiao@meta.com

Work done during an internship at Meta Reality Labs Research

Project page: [https://replaceanything3d.github.io](https://replaceanything3d.github.io)

###### Abstract

We introduce ReplaceAnything3D model (RAM3D), a novel method for 3D object replacement in 3D scenes based on users' text description. Given multi-view images of a scene, a text prompt describing the object to replace, and another describing the new object, our Erase-and-Replace approach can effectively swap objects in 3D scenes with newly generated content while maintaining 3D consistency across multiple viewpoints. We demonstrate the versatility of RAM3D by applying it to various realistic 3D scene types, showcasing results of modified objects that blend in seamlessly with the scene without impacting its overall integrity.

## 1 Introduction

The explosion of new media platforms and display devices has sparked a surge in demand for high-quality 3D content, and thus an increasing need for efficient tools for generating and editing them. While there has been significant progress in 3D reconstruction and generation, 3D scene editing remain a less-studied area. In this work, we focus on the task of replacing or adding new 3D objects to an existing 3D scene using only input language prompts from a user. Compared to other 3D scene editing methods such as relighting or stylization, this task involves intricate local edits to seamlessly integrate new objects into the scene without disrupting its overall coherence. This goes beyond just generating realistic visuals and demands a nuanced understanding of both the global scene context and the interaction between the newly added object and the rest of the scene.

Naively using text-to-3D methods to generate 3D objects and manually adding them to a scene can be a tedious process. More importantly, it completely ignores the interaction between objects'appearance and the rest of the scene, such as lighting and shadows. Here, we instead formulate the task of object replacement as a 3D scene inpainting problem. Specifically, our goal is to seamlessly fill in the region occupied by the old object with a new object, making it indistinguishable from the rest of the 3D scene. To avoid manual object placement and blending, we adopt a powerful text-guided image inpainting model, enabling 3D object replacement based solely on input text prompts.

In this work, we present the ReplaceAnything3D model (RAM3D), a text-guided method for object replacement in 3D scenes using an Erase-and-Replace strategy. RAM3D takes multiview images of a static scene as input, along with text prompts specifying which object to erase and what should replace it. Our approach comprises four key steps: 1) We use a text-to-mask model with a text prompt to detect and segment the object to be erased from the input images. 2) We use a text-guided 3D inpainting technique to fill in the background region obscured by the removed object in a multi-view consistent manner. 3) Next, we use a similar text-guided 3D inpainting technique to generate a new 3D object corresponding to the input text description, which is seamlessly composited onto the background in all views. We thus obtain multiview consistent images of the edited 3D scene. 4) Finally, these updated dataset images are used to reconstruct the modified 3D scene, enabling novel view synthesis. Instead of relying on generic image editing methods such as Instruct-Pix2Pix , we adopt HiFA , a state-of-the-art distillation approach, to distill a pretrained text-to-image-inpainting model into a 3D scene representation. This allows us to freely add or entirely remove detailed 3D objects from scenes, a significant challenge for methods like Instruct-Pix2Pix  and its derivatives such as Instruct-NeRF2NeRF  due to their reliance on a limited dataset of images and editing instructions. By integrating a text-guided image inpainting model with a compositional scene structure that distinguishes the object of interest from the rest of the scene, ReplaceAnything3D can seamlessly generate edited 3D scenes with new objects harmoniously integrated into their environment. In summary, our contributions are:

* We introduce a method for text-guided object replacement in 3D scenes that removes the need for manual 3D modelling.
* We propose a multi-stage approach that supports object replacement or even object addition in 3D scenes in high-fidelity.
* We present 3D consistent results on multiple scene types (human avatar, forward-facing and 360\({}^{}\) scenes), and challenging edit prompts requiring detailed texture synthesis.

Figure 1: Our method enables prompt-driven object replacement for a variety of realistic 3D scenes.

Related work

Diffusion model for text-guided image editingDiffusion models trained on extensive text-image datasets have demonstrated remarkable results, showcasing their ability to capture intricate semantics from text prompts . As a result, these models provide strong priors for various text-guided image editing tasks . In particular, methods for text-guided image inpainting  enable local image editing by replacing masked regions with new content that seamlessly blends with the rest of the image, allowing for object removal, replacement, and addition. These methods are direct 2D counterparts to our approach for 3D scenes, where each view can be treated as an image inpainting task. However, 3D scenes present additional challenges, such as the requirement for multi-view consistency and memory constraints due to the underlying 3D representations. In this work, RAM3D addresses these challenges by combining a pre-trained image inpainting model with compositional 3D scene representations.

Text-to-3D synthesisWith the remarkable success of text-to-image diffusion models, text-to-3D synthesis has garnered increasing attention. Most work in this area focuses on distilling pre-trained text-to-image models into 3D models, starting with the seminal works Dreamfusion  and Score Jacobian Chaining (SJC) . Subsequent research has explored various methods to enhance the quality of synthesized objects  and disentangle geometry and appearance . Instead of relying solely on pre-trained text-to-image models, recent work has utilized large-scale 3D datasets such as Objavverse  to improve the quality of 3D synthesis from text or single images .

Here, we move beyond text-to-3D synthesis by incorporating both text prompts and the surrounding scene information as inputs. This approach introduces additional complexities, such as ensuring the appearance of the 3D object harmoniously blends with the rest of the scene and accurately modeling object-object interactions like occlusion and shadows. Combining HiFA , a text-to-3D distillation approach, with a text-to-image-inpainting model, RAM3D aims to create more realistic and coherent 3D scenes that seamlessly integrate the synthesized 3D objects (Figure 1).

3D EditingMany existing 3D editing methods focus on editing an individual object's appearance or geometry . For scene-level editing, recent works primarily address object removal tasks for forward-facing NeRF scenes . Instruct-NeRF2NeRF  and similar followup works  offer a comprehensive approach to both appearance editing and object addition, leveraging InstructPix2Pix  to update the scene dataset. However, as they modify the entire scene, they struggle to synthesize objects with complex geometrical texture patterns, and completely fail to remove objects from scenes. Blended-Nerf  and DreamEditor  allow localized object editing but do not support object removal. One closely related work is , which can remove and replace objects using one single image reference from the user. However, since this method relies only on a single inpainted image, it cannot handle regions with large occlusions across different views, and thus is only applied on forward-facing scenes. Another closely related work is RePaint-NeRF , which similarly allows text-guided scene editing on a masked region, but uses SDS loss to update a pretrained NeRF towards the text content. In contrast, RAM3D adopts an Erase-and-Replace approach for localized scene editing, instead of modifying the existing geometry or appearance of the scene's content, leading to superior qualitative results when compared to RePaint-NeRF.

Unlike NeRF editing methods, the recently proposed Gaussian Editor , uses 3D Gaussian Splats  as its underlying representation. Exploiting the explicit nature of this representation, this method enables localised edits by selectively updating targeted Gaussians, using guidance from InstructPix2Pix . Like our work, it also supports adding new objects to scenes. Similarly to , a single scene image is inpainted in 2D, providing a reference image of the new object. A pre-existing image-to-3D object model  then generates a 3D object from the segmented reference image, in isolation from the original scene. The coarse object mesh is transformed back into a 3D Gaussian representation and integrated into the scene through a laborious modelling process where the object is is manually placed into the scene. More importantly, the new object is not guaranteed to integrate seamlessly into the surrounding scene, which results in visible artifacts and quality issues when the method is applied to \(360^{}\) scenes. In contrast, RAM3D performs distillation using the inpainting network as a diffusion prior, which leads to harmonious blending between the new object and its surroundings, even when applied to \(360^{}\) scenes with challenging edit prompts.

## 3 Method

### Preliminary

Distilling diffusion modelsDreamfusion  proposes a technique called Score Distillation Sampling (SDS) to compute gradients from a 2D pre-trained text-to-image diffusion model, to optimize the parameters of 3D neural radiance fields (NeRF). Recently, HiFA  significantly improves the quality of text-to-3D object generation by introducing an alternative loss formulation to SDS that can be computed explicitly for a Latent Diffusion Model (LDM). Let \(_{}\) be the parameters of a implicit 3D scene, \(y\) is a text prompt, \(_{}(},t,y)\) be the pre-trained LDM model with encoder \(E\) and decoder \(D\), \(_{}\) can be optimized using:

\[_{}(,,)=_{t,} w(t)[\|-}\|^{2}+_{}\|- }\|^{2}] \]

where \(=E()\) is the latent vector by encoding a rendered image \(\) of \(_{}\) from a camera viewpoint from the training dataset, \(}\) is the estimate of latent vector \(\) by the denoiser \(_{}\), and \(}=D(})\) is a recovered image obtained through the decoder \(D\) of the LDM. Note that for brevity, we incorporate coefficients related to timesteps \(t\) to \(w(t)\).

Here we deviate from the text-to-3D synthesis task where the generated object is solely conditioned on a text prompt. Instead, we consider a collection of scene views as additional inputs for the synthesized object. To achieve this, we utilize the HiFA distillation loss function and timestep-annealing strategy, in conjunction with an open-source text-to-image _inpainting_ LDM. This LDM \(_{}(},t,y,)\) requires not only a text prompt \(y\), but also a binary mask \(\) indicating the area to be filled in.

### Overview

The input to RAM3D consists of a collection of \(n\) images \(I_{i}\), corresponding camera viewpoints \(_{i}\) and a text prompt \(y_{}\) describing the object the user wishes to replace. Using this text prompt we can obtain masks \(_{i}\) corresponding to every image and camera viewpoint using a pretrained text-to-mask model LangSAM . Note that these masks are not necessarily multi-view consistent. We additionally provide a text prompt \(y_{}\) describing a new object to replace the old object. Our goal is to modify the masked object in every image in the dataset to match the text prompt \(y_{}\), in a 3D-consistent manner. We can then reconstruct the edited scene using any choice of 3D representations such as NeRF  or Gaussian Splats  to obtain renderings of the edited 3D scene from novel viewpoints.

Figure 2 illustrates the overall pipeline of our **Erase and Replace** framework. Instead of modifying existing objects' geometry and appearance to match the target text descriptions like other methods , we adopt an Erase-and-Replace approach. Firstly, for the **Erase** stage, we remove the masked objects completely and inpaint the occluded region in the background, using a neural field \(_{}\). Secondly, for the **Replace** stage, we generate new objects using a neural field \(_{}\), compositing them so that they blend in with the inpainted background scene. Finally, we create a new training set

Figure 2: An overview of RAM3D **Erase** and **Replace** stages.

using the edited images and camera poses from the original scene, and reconstruct the modified 3D scene using any choice of 3D representations for novel view synthesis.

To enable text-guided object replacement in 3D scenes, we distill an open-source text-to-image inpainting LDM using the HiFA loss function (Equation 1) and timestep annealing strategy . Note that LDM distillation using 3D Gaussian Splats is still challenging to optimise, leading to blurry results and thus requiring further refinement process in recent text-to-3D-object works [40; 41]. We therefore opt to use a NeRF-based representation instead, for RAM3D's **Erase** and **Replace** stages (Sections 3.3, 3.4). To circumvent the memory constraints and slow training speed inherent to NeRF's implicit representations, we propose a Bubble-NeRF representation (see Figure 3, Left side) which only models the localised part of the scene that is affected by the editing operation, instead of the whole scene.

### Erase stage

In the Erase stage, we aim to remove the object described by \(y_{}\) from the scene and inpaint the occluded background region in a multi-view consistent manner. To do so, we optimise RAM3D parameters \(_{}\) which implicitly represent the inpainted background scene. Note that the Erase stage only needs to be performed once to remove the desired object, after which the Replace stage (Section 3.4) can be used to generate different objects or even add new objects to the scene, as demonstrated in the Results section. As a pre-processing step, we use LangSAM  with text prompt \(y_{}\) to obtain a mask \(_{i}\) for each image in the dataset. We then dilate each \(_{i}\) to obtain _halo_ regions \(_{i}\) around the original input mask (see Figure 3, Left side).

At each training step, we sample image \(I_{i}\), camera \(_{i}\), mask \(_{i}\), and halo region \(_{i}\) for a random \(i\{1..n\}\), providing them as inputs to RAM3D to compute training losses (Figure 2, Left side) (we henceforth drop the subscript i for clarity). RAM3D volume renders the implicit 3D representation \(_{}\) over rays emitted from camera viewpoint \(\) which pass through the visible pixels in \(\) and \(\) (the Bubble-NeRF region). The RGB values of the remaining pixels on the exterior of the Bubble-NeRF are sampled from \(I\) (see Figure 3, Left side). These rendered and sampled pixel rgb-values are arranged into a 2D array, and form RAM3D's inpainting result for the given view, \(^{}\). Following the HiFA distillation objective (see Section 3.1), we use the frozen LDM's \(E\) to encode \(^{}\) to obtain \(^{}\), add noise, denoise with \(_{}\) to obtain \(}^{}\), and decode with \(D\) to obtain \(}^{}\). We condition \(_{}\) with \(I\), \(\) and the empty prompt, since we do not aim to inpaint new content at this stage.

We now use these inputs to compute \(_{}\) (see Equation 1). We next compute \(_{}\) and \(_{}\) on \(\) (see Figure 3), guiding the distilled \(_{}\) towards an accurate reconstruction of the background.

\[_{}=(^{},I ) \]

\[_{}=(vgg_{16}(^{} ),vgg_{16}(I)) \]

This step is critical to ensuring that RAM3D inpaints the background correctly (see Figure 7). Following , we compute depth regularisation \(_{}\), leveraging the geometric prior from a pretrained depth estimator . In summary, the total loss for the Erase stage is:

\[_{}=_{}+_{} _{}+_{}_{}+ _{}_{} \]

### Replace stage

In the Replace stage, we aim to add a new object described by \(y_{}\) into the inpainted scene. To do so, we optimise the foreground neural field \(_{}\) to render \(^{}\), which is then composited with \(^{}\) to form \(\). Unlike \(_{}\) in the Erase stage, \(_{}\) does not seek to reconstruct the background scene, but instead only the LDM-inpainted object which is located on the interior of \(\). Therefore in the Replace stage, RAM3D does not consider the halo rays which intersect \(\), but only those intersecting \(\) (Figure 3, Right side). These rendered pixels are arranged in the masked region into a 2D array to give the foreground image \(^{}\), whilst the unmasked pixels are assigned an RGB value of 0. The accumulated densities are similarly arranged into a foreground alpha map \(A\), whilst the unmasked pixels are assigned an alpha value of 0. We now composite the foreground \(^{}\) with the background \(^{}\) using alpha blending:

\[=A^{}+(1-A)^{} \]Using the composited result \(\), we compute \(_{}\) as before, but now condition \(_{}\) with the prompt \(y_{}\), which specifies the new object for inpainting. As we no longer require the other losses, we set \(_{},_{},_{}\) to 0.

Since the Erase stage already provides us with a good background, in this stage, \(_{}\) only needs to represent the foreground object. To encourage foreground/background disentanglement, on every k-th training step, we substitute \(^{}\) with a constant-value RGB tensor, with randomly sampled RGB intensity. This guides the distillation of \(_{}\) to only include density for the new object; a critical augmentation to avoid spurious floaters over the background (see Figure 7, Left side).

### Reconstructing the edited scene

Once the inpainted background and objects have been generated inside the Bubble-NeRF region (Figure 3) during the Erase and Replace stages, we composite the Bubble-NeRF renderings onto all original scene images. We finally obtain a full 3D representation of the edited scene by applying an off-the-shelf scene reconstruction method such as NeRF or Gaussian splats .

Figure 4: **Left: Qualitative comparison with Reference-Guided Inpainting  (images adapted from the original paper) for object replacement. Right: Qualitative comparison with Blended-NeRF  for object replacement. Our method generates results with higher quality and capture more realistic lighting and details.**

Figure 3: **Left: Erase stage. The masked region (blue) serves as a conditioning signal for the LDM, indicating the area to be inpainted. The surrounding nearby pixels form the halo region \(h\) (green), which is also rendered by RAM3D during the Erase stage. The union of these 2 regions is the Bubble-NeRF region, whilst the remaining pixels are sampled from the input image (red). Right: Replace stage. RAM3D volumetrically renders the masked pixels (shown in blue) to give \(^{}\). The result is composited with \(^{}\) to form the combined image \(\).**

## 4 Results

We conduct experiments on real 3D scenes varying in complexity: forward-facing scenes, 360\({}^{}\) scenes and human avatar. For forward-facing scenes, we show results for the statue and red-net scene from SPIn-NeRF dataset , as well as the fern scene from NeRF . For 360\({}^{}\) scene, we show results from the garden scene from Mip-NeRF 360\({}^{}\). For the avatar result, we use the face dataset from Instruct-NeRF2NeRF . On each dataset, we train RAM3D with a variety of \(y_{}\), generating a diverse set of edited 3D scenes (Figure 1). Please refer to the supplemental video for more qualitative results for object replacement using personalized content.

Note that RAM3D performs localised scene editing for 3D object replacement, conditioned by an edit region mask and replacement object description prompt, as shown in Figure 2. For fair apples-to-apples comparison, we mostly compare with other state-of-the-art localised editing methods (see Figure 4, Figure 5, Table 1). This scope contrasts with the separate but related track of global scene editing methods, which can modify an entire scene based on instruction-style prompts, but do not support object replacement or removal . We nevertheless provide qualitative and quantitative comparison with global-editing methods in the Appendix (Sections F, G).

### Qualitative Comparisons

Figure 4 shows qualitative comparison with two methods for NeRF-based 3D object replacement; Blended-NeRF  and the method by . In Figure 5 we compare our method with a state-of-the-art 3D Gaussian Splat-based scene editing framework , which supports object removal, addition and replacement. In the Appendix (Section F), we additionally compare with DreamEditor , Repair-NeRF , generic scene editing method Instruct-NeRF2NeRF  and various similar InstructPix2Pix-based  followup works .

Compared to method by  on the left side of Figure 4, RAM3D achieves comparable object replacement results while handling more complex lighting effects such as shadows between the foreground and background objects. Note that the method by  only works with forward facing scenes, and thus cannot handle 360\({}^{}\) scenes such as the garden scene like our method. In Figure

Figure 5: Qualitative comparison with Gaussian Editor . We show results for 3 challenging edit prompts on the garden scene (top 2 rows) and face scene (bottom 2 rows). In the garden scene, our method generates more realistic objects which are better integrated with the surrounding scene. In the face scene, our method generates more detailed texture patterns and geometry which are better aligned with the edit prompts.

4 right side, we note that RAM3D generates more realistic and detailed objects that blend in better with the rest of the scene. Meanwhile, Blended-NeRF only focuses on synthesizing completely new objects without taking the surrounding scenes into consideration. The synthesized object therefore looks saturated and outlandish compared to the rest of the scene. Moreover, due to the memory constraint of CLIP  and NeRF, Blended-NeRF only works with image resolutions 2-times smaller than ours (1008\(\)756 vs. 504\(\)378).

Figure 5 shows qualitative comparison with GaussianEditor , a state-of-the-art scene-editing framework which supports object deletion, addition, and general editing capabilities. Applying it to the garden scene, we first use the delete functionality to remove the vase from the table, followed by the addition functionality to insert the new object. Note that this method generates the new object in isolation, using a prior image-to-3D method , resulting in visible artifacts on the surface of the table where the new object interacts with the surrounding scene. Furthermore, since this method is not guaranteed to place the new object correctly, its position in the scene requires post-hoc manual adjustment - see Appendix (Section H) for further details.

Applying GaussianEditor to the face scene, we use the general editing functionality. The explicit Gaussian Splat formulation allows edits to be localised to the relevant Gaussians (corresponding to the man's torso), which is selected using a user interface. The editing process is then guided towards the desired prompt using 2D guidance from InstructPix2Pix , and we observe similar limitations to other InstructPix2Pix-based scene-editing methods  (see Appendix F). In particular, we observe that GaussianEditor struggles to synthesise detailed texture patterns on the man's torso, unlike RAM3D. Furthermore, we notice that the geometry of the man's clothes appears unchanged in the GaussianEditor results, whereas our method successfully generates geometrical details such as the shirt collar and jacket lapels, matching the edit prompt in each case.

Adding multiple objectsIn addition to replacing objects in the scene, our method can add new objects based on users' input masks. Figure 6 demonstrates that completely new objects with realistic lighting and shadows can be generated and composited to the current 3D scene. Notably, as shown in Figure 6-bottom right, our method can add more than one object to the same scene while maintaining realistic scene appearance and multi-view consistency.

### Quantitative Results

3D scene editing is a highly subjective task. Thus, we mainly show qualitative results and comparisons, and refer readers to the supplemental video for additional results. However, we follow  and report CLIP Text-Image Direction Similarity, which measures the alignment of the performed object replacement with the input text description. Additionally, we also quantitatively measure temporal

Figure 6: Given user-defined masks, ReplaceAnything3D can add completely new objects that blend in with the rest of the scene. Furthermore, due to its compositional structure, RAM3D can add multiple objects to 3D scenes while maintaining realistic appearance, lighting, and multi-view consistency (bottom right).

consistency by calculating the Warping Error, following [46; 47; 48]. Specifically, **(1)** we use RAFT  to calculate the optical flow of test videos, with each frame being a rendered novel view of the original scene, **(2)** warp the corresponding frames from the modified scene according to it, and **(3)** measure the warping error. We compare RAM3D quantitatively with 2 state-of-the-art NeRF-based and Gaussian-Splitting-based methods (, ) for the object-replacement task on three datasets.

In Table 1, we show that RAM3D achieves better overall prompt alignment (highest CLIP Text-Image Direction Similarity) than existing works, and the best temporal consistency (lowest warping error) across all methods and datasets. Interestingly, although Blended-NeRF directly optimizes for CLIP-similarity between the generated objects and target text prompts, it still achieves a lower score than our method. Furthermore, we observe that our model's superior appearance synthesis quality (when compared to GaussianEditor in Figure 5) is also reflected in the quantitative results in Table 1.

   Prompts & CLIP Text-Image Direction Similarity \(\) & Warping error (\( 10^{-2}\)) \(\) \\   & Ours & GaussianEditor & Ours & GaussianEditor \\  Pineapple & \(\) & \(-0.0631\) & \(\) & \(1.4600\) \\ Chess & \(0.0874\) & \(\) & \(\) & \(1.6700\) \\ Mushroom & \(\) & \(0.1030\) & \(\) & \(1.3900\) \\ Popcorn & \(\) & \(0.0400\) & \(\) & \(1.6800\) \\  Checker & \(\) & \(0.0016\) & \(\) & \(0.7600\) \\ Hawaiian & \(\) & \(0.1689\) & \(\) & \(0.8100\) \\ Tartan & \(\) & \(0.0379\) & \(\) & \(0.6700\) \\   & Ours & BlendedNerf & Ours & BlendedNerf \\  Mushroom & \(\) & \(0.0535\) & \(\) & \(2.9500\) \\ Strawberry & \(\) & \(0.2224\) & \(\) & \(3.2300\) \\   

Table 1: We compute a CLIP-based alignment metric, and optical flow-based temporal consistency metric for various datasets and prompts. RAM3D shows the best overall edit prompt alignment and temporal consistency. (Top) garden, (Middle) face, (Bottom) fern.

Figure 7: **Left: Results for 2 RAM3D variants trained on the Statue scene for the Erase stage. a) Training without any supervision on the halo region surrounding the inpainting mask. The training objective is ambiguous and the Bubble-NeRF model collapses to a hazy cloud. b) Adding halo losses (\(_{}\) and \(_{}\)) for the halo region surrounding the Bubble-NeRF guides the distillation of \(_{}\) towards the true background, as observed on rays which pass nearby to the occluding object. RAM3D can now inpaint the background scene accurately. Right: Results for 3 RAM3D variants, on the statue scene for prompt _”A corgi”_. RGB samples are shown with accumulated NeRF density (alpha map) in the top-left corner. The bubble rendering region is shown as a dotted blue line. c) A monolithic scene representation which contains both the foreground and background. d) A compositional scene model but without random background augmentation. e) Our full model.**

### Ablation studies

We conduct a series of ablation studies to demonstrate the effectiveness of our method and training strategy. In Figure 7 Right side, we show the benefits of our compositional foreground/background structure and background augmentation training strategy. Specifically, we train a version of RAM3D using a monolithic NeRF to model both the background and the new object (combining \(_{}\) and \(_{}\)). In other words, this model is trained to edit the scene in one single stage, instead of separate Erase and Replace stages. We observe lower quality background reconstruction in this case, as evident from the blurry hedge behind the corgi's head in Figure 6(c).

We also demonstrate the advantage of using random background augmentation in separating the foreground object from the background (see Section 3.4). Without this augmentation, the model is unable to accurately separate the foreground and background alpha maps, resulting in a blurry background and floaters that are particularly noticeable when viewed on video (Figure 6(d)). In contrast, our full composited model trained with background augmentation successfully separates the foreground and background, producing sharp results for the entire scene (Figure 6(e)).

In Figure 7 Left side, we show the importance of the Halo region supervision for the Erase stage. Without it, our model lacks important nearby spatial information, and thus cannot successfully generate the background scene.

## 5 Conclusion

We introduce RAM3D, a text-guided 3D object replacement method for 3D scenes, offering a potential editing tool for VR/MR, gaming, and film production. With an Erase-and-Replace approach, RAM3D can effectively replace objects with significantly different contents that blends seamlessly with the original 3D scene. Our method can also add new objects while maintaining realistic appearance and multi-view consistency. We demonstrate the effectiveness of RAM3D in various realistic 3D scenes (including human avatar, forward-facing and \(360^{}\) scenes), and superior synthesis quality compared to current state-of-the-art NeRF and Gaussian Splatting based methods.

For future work, our Bubble-NeRF method could be extended to other representations such as 3D Gaussian splats , similar to DreamGaussian . Other interesting future directions include disentangling geometry and appearance to enable more fine-grained control for scene editing, addressing multi-face problems using prompt-debiasing methods  or models that are pre-trained on multiview datasets , and developing amortized models for faster editing, similar to .