# AlleNoise - large-scale text classification benchmark dataset with real-world label noise

Alicja Raczkowska Aleksandra Osowska-Kurczab Jacek Szczerbinski

Kalina Jasinska-Kobus Klaudia Nazarko

Machine Learning Research

Allegro.com

{alicja.raczkowska, aleksandra.kurczab, jacek.szczerbinski,

kalina.kobus, klaudia.nazarko}@allegro.com

Equal contribution

###### Abstract

Label noise remains a challenge for training robust classification models. Most methods for mitigating label noise have been benchmarked using primarily datasets with synthetic noise. While the need for datasets with realistic noise distribution has partially been addressed by web-scraped benchmarks such as WebVision and Clothing1M, those benchmarks are restricted to the computer vision domain. With the growing importance of Transformer-based models, it is crucial to establish text classification benchmarks for learning with noisy labels. In this paper, we present _AlleNoise_, a new curated text classification benchmark dataset with real-world instance-dependent label noise, containing over 500,000 examples across approximately 5,600 classes, complemented with a meaningful, hierarchical taxonomy of categories. The noise distribution comes from actual users of a major e-commerce marketplace, so it realistically reflects the semantics of human mistakes. In addition to the noisy labels, we provide human-verified clean labels, which help to get a deeper insight into the noise distribution, unlike web-scraped datasets typically used in the field. We demonstrate that a representative selection of established methods for learning with noisy labels is inadequate to handle such real-world noise. In addition, we show evidence that these algorithms do not alleviate excessive memorization. As such, with _AlleNoise_, we set the bar high for the development of label noise methods that can handle real-world label noise in text classification tasks. The code and dataset are available for download at [https://github.com/allegro/AlleNoise](https://github.com/allegro/AlleNoise).

## 1 Introduction

The problem of label noise poses a sizeable challenge for classification models [1; 2]. With modern deep neural networks, due to their capacity, it is possible to memorize all labels in a given training dataset . This, effectively, leads to overfitting to noise if the training dataset contains noisy labels, which in turn reduces the generalization capability of such models [4; 5; 6].

Most previous works on training robust classifiers have focused on analyzing relatively simple cases of synthetic noise [7; 8], either uniform (i.e. symmetric) or class-conditional (i.e. asymmetric). It is a common practice to evaluate these methods using popular datasets synthetically corrupted with labelnoise, such as MNIST , ImageNet , CIFAR  or SVHN . However, synthetic noise is not indicative of realistic label noise and thus deciding to use noisy label methods based on such benchmarks can lead to unsatisfactory results in real-world machine learning practice. Moreover, it has been shown that these benchmark datasets are already noisy themselves [13; 14], so the study of strictly synthetic noise in such a context is intrinsically flawed.

Realistic label noise is instance dependent, i.e. the labeling mistakes are caused not simply by label ambiguity, but by input uncertainty as well . This is an inescapable fact when human annotators are responsible for the labeling process . However, many existing approaches for mitigating instance-dependent noise have one drawback in common - they had to, in some capacity, artificially model the noise distribution due to the lack of existing benchmark datasets [17; 18; 19; 20; 21; 22]. In addition, most of the focus in the field has been put on image classification, but with the ever-increasing importance of Transformer-based  architectures, the problem of label noise affecting the fine-tuning of natural language processing models needs to be addressed as well. There are many benchmark datasets for text data classification [24; 25; 26; 27], but none of them are meant for the study of label noise. In most cases, the actual level of noise in these datasets is unknown, so using them for benchmarking label noise methods is unfeasible.

Moreover, the datasets used in this research area usually contain relatively few labels. The maximum reported number of labels is 1000 . As such, there is a glaring lack of a benchmark dataset for studying label noise that provides realistic real-world noise, a high number of labels and text data at the same time.

We see a need for a textual benchmark dataset that would provide realistic instance-dependent noise distribution with a known level of label noise, as well as a relatively large number of target classes, with both clean and noisy labels. To this end, in this paper we provide the following main contributions:

* a benchmark dataset for multi-class text classification with real-world label noise. The dataset consists of 502,310 short texts (e-commerce product titles) belonging to 5,692 categories (taken from a real product assortment tree). It includes a noise level of 15%, stemming from mislabeled data points. This amount of noise reflects the actual noise distribution in the data source (Allegro.com e-commerce platform). For each of the mislabeled data instances, the true category label was determined by human domain experts.
* We benchmark a comprehensive selection of well-established methods for classification with label noise against the real-world noise present in _AlleNoise_ and compare the results to synthetic label noise generated for the same dataset. We provide evidence that the selected methods fail to mitigate real-world label noise, even though they are very effective in alleviating synthetic label noise.

## 2 Related work

Several classification benchmarks with real-world instance-dependent noise have been reported in the literature. ANIMAL-10N  is a human-labeled dataset of confusing images of animals, with 10 classes and an 8% noise level. CIFAR-10N and CIFAR-100N  are noisy versions of the CIFAR dataset, with labels assigned by crowd-sourced human annotators. CIFAR-10N is provided in three versions, with noise levels of 9%, 18% and 40%, while CIFAR-100N has a noise level of 40%. Clothing1M  is a large-scale dataset of fashion images crawled from several online shops. It contains 14 classes and the estimated noise rate is 38%. Similarly, WebVision  comprises of images crawled from the web, but it is more general - it has 1000 categories of diverse images. The estimated noise level is 20%. DCIC  is a benchmark that consists of 10 real-world image datasets, with several human annotations per image. This allows for testing algorithms that utilize soft labels to mitigate various kinds of annotation errors. The maximum number of classes in the included datasets is 10.

With the focus in the label noise field being primarily on images, the issue of noisy text classification remains relatively unexplored. Previous works have either utilized existing classification datasets with synthetic noise [14; 17; 33] or introduced new datasets with real-world noise. NoisyNER  contains annotated named entity recognition data in the Estonian language, assigned to 4 categories. The authors do not mention the noise level, only that they provide 7 variants of real-world noise. NoisywikiHow  is a dataset of articles scraped from the wikiHow website, with accompanying 158 article categories. The data was manually cleaned by human annotators, which eliminated the real-world noise distribution. The authors performed experiments by injecting synthetic noise into their dataset. Thus, NoisywikiHow is not directly comparable to _AlleNoise_. Another two datasets are Hausa and Yoruba , text classification datasets of low-resource African languages with 5 and 7 categories respectively. They both include real-world noise with the level of 50.37% for the former, and 33.28% for the latter.

While there is a number of text datasets containing e-commerce product data [17; 25; 27], none of them have verified clean labels and in most cases the noise level is unknown. Similarly, classification settings with large numbers (i.e. more than 1000) of classes were not addressed up to this point in the existing datasets (**Tab. 1**).

   Dataset & Modality & Total examples & Classes & Noise level & Clean label \\  ANIMAL10N & Images & 55k & 10 & 8\% & ✓ \\ CIFAR10N & Images & 60k & 10 & 9/18/40\% & ✓ \\ CIFAR100N & Images & 60k & 10 & 40\% & ✓ \\ WebVision & Images & 2.4M & 1000 & \(\)20\% & ✗ \\ Clothing1M & Images & 1M & 14 & \(\)38\% & ✗ \\  Hausa & Text & 2,917 & 5 & 50.37\% & ✓ \\ Yoruba & Text & 1,908 & 7 & 33.28\% & ✓ \\ NoisyNER & Text & 217k & 4 & unspecified & ✓ \\
**AlleNoise** & **Text** & **500k** & **5692** & **15\%** & ✓ \\   

Table 1: Comparison of _AlleNoise_ to previously published datasets created for studying the problem of learning with noisy labels. All datasets contain real-world noise. _AlleNoise_ is the biggest text classification dataset in this field, has a known level of label noise and provides clean labels in addition to the noisy ones.

Figure 1: Symmetric noise vs. _AlleNoise_ in examples. Correct and noisy labels are marked in green and red, respectively. **(a)** Symmetric noise: an electric toothbrush incorrectly labeled as a winter tire is easy to spot, even for an untrained human. **(b)**_AlleNoise_: a ceiling dome is mislabeled as a pendant lamp. This error is semantically challenging and hard to detect. Note: _AlleNoise_ dataset does not include images.

## 3 AlleNoise Dataset Construction

We introduce _AlleNoise_ - a benchmark dataset for large-scale multi-class text classification with real-world label noise. The dataset consists of 502,310 e-commerce product titles listed on Allegro.com in 5,692 assortment categories, collected in January of 2022. 15% of the products were listed in wrong categories, hence for each entry the dataset includes: the product title, the category where the product was originally listed, and the category where it should be listed according to human experts.

Additionally, we release the taxonomy of product categories in the form of a mapping (category ID \(\) path in the category tree), which allows for fine-grained exploration of noise semantics.

### Real-world noise

We collected 75,348 mislabeled products from two sources: 1) customer complaints about a product being listed in the wrong category - such requests usually suggest the true category label, 2) assortment clean-up by internal domain experts, employed by Allegro - products listed in the wrong category were manually moved to the correct category.

The resulting distribution of label noise is not uniform over the entire product assortment - most of the noisy instances belong to a small number of categories. Such asymmetric distribution is an inherent feature of real-world label noise. It is frequently modeled with class-conditional synthetic noise in related literature. However, since the mistakes in _AlleNoise_ were based not only on the category name, but also on the product features, our noise distribution is in fact instance-dependent.

### Clean data sampling

The 75,348 mislabeled products were complemented with 426,962 products listed in correct categories. The clean instances were sampled from the most popular items listed in the same categories as the noisy instances, proportionally to the total number of products listed in each category. The high popularity of the sampled products guarantees their correct categorization, because items that generate a lot of traffic are curated by human domain experts. Thus, the sampled distribution was representative for a subset of the whole marketplace: 5,692 categories out of over 23,000, for which label noise is particularly well known and described.

### Post-processing

We automatically translated all 500k product titles from Polish to English. Machine translation is a common part of e-commerce, many platforms incorporate it in multiple aspects of their operation [37; 38]. Moreover, it is an established practice to publish machine-translated text in product datasets .

Figure 2: _AlleNoise_ consists of two tables: the first table includes the true and noisy label for each product title, while the second table maps the labels to category names.

Categories related to sexually explicit content were removed from the dataset altogether. Finally, categories with less than 5 products were removed from the dataset to allow for five-fold cross-validation in our experiments.

## 4 Methods

### Problem statement

Let \(\) denote the input feature space, and \(\) be a set of class labels. In a typical supervised setting, each instance \(x_{i}\) has a true class label \(y_{i}\). However, in learning with noisy labels, \(_{i}\) is observed instead, which is with an unknown probability \(p\) (noise level) changed from the true \(y_{i}\).

In this setting, we train a classifier \(f:\) that generalizes knowledge learnt from a dataset \(\), consisting of training examples \((x_{i},_{i})\). Because \(_{i}\) can be affected by label noise, the model's predictions \(_{i}=f(x_{i})\) might be corrupted by the distribution of noisy labels as well. Maximizing the robustness of such a classifier implies reducing the impact of noisy training samples on the generalization performance. In the _AlleNoise_ dataset, \(x_{i}\) corresponds to the product title, \(_{i}\) is the original product category, and \(y_{i}\) is the correct category.

### Synthetic noise generation

In order to compare the real-world noise directly with synthetic noise, we applied different kinds of synthetic noise to the clean version of _AlleNoise_: the synthetic noise was applied to each instance's true label \(y_{i}\), yielding a new synthetic noisy label \(_{i}\). Overall, the labels were flipped for a controlled fraction \(p=15\%\) of all instances. We examined the following types of synthetic noise:

* Symmetric noise: each instance is given a noisy label different from the original label, with uniform probability \(p\).
* Class-conditional pair-flip noise: each instance in class \(j\) is given a noisy label \(j+1\) with probability \(p\).
* Class-conditional nested-flip noise: we only flip categories that are close to each other in the hierarchical taxonomy of categories. For example, for the parent category _Car Tires_ we perform a cyclic flip between its children categories: _Summer \(\) Winter \(\) All-Season \(\) Summer_ with probability \(p\). Thus, the noise transition matrix is a block matrix with a small number of off-diagonal elements equal to \(p\).
* Class-conditional matrix-flip noise: the transition matrix between classes is approximated with the baseline classifier's confusion matrix. The confusion matrix is evaluated against the clean labels on 8% of the dataset (validation split) . The resulting noise distribution is particularly tricky: we flip the labels between the classes that the model is most likely to confuse.

### Model architecture

Next, we evaluated several algorithms for training classifiers under label noise. For a fair comparison, all experiments utilized the same classifier architecture as well as training and evaluation loops. We followed a fine-tuning routine that is typical for text classification tasks. In particular, we vectorized text inputs with XLMRoberta , a multilingual text encoder based on the Transformer architecture . To provide the final class predictions, we used a single fully connected layer with a softmax activation and the number of neurons equal to the number of classes. The baseline model uses cross-entropy (CE) as a loss function.

Models were trained with the AdamW optimiser and linear LambdaLR scheduling (warmup steps \(=100\)). We have not used any additional regularization, i.e. weight decay or dropout. Key training parameters, such as batch size (\(=256\)) and learning rate (\(=10^{-4}\)) were tuned to maximize the validation accuracy on the clean dataset. All models have been trained for 10 epochs. Training of the baseline model, accelerated with a single NVIDIA A100 40GB GPU, lasted for about 1 hour.

We used five-fold stratified cross-validation to comprehensively evaluate the results of the models trained with label noise. For each fold, the full dataset was divided into three splits: \(_{train}\), \(_{val}\), \(_{test}\), in proportion 72% : 8% : 20%. Following the literature on learning with noisy labels , both \(_{train}\) and \(_{val}\) were corrupted with label noise, while \(_{test}\) remained clean.

All of the results presented in this study correspond to the last checkpoint of the model. We use the following format for presenting the experimental results: \([][]\), where \(m\) is an average over the five cross-validation folds, while \(s\) is the standard deviation. Experiments used a seeded random number generator to ensure the reproducibility of the results.

### Evaluation metrics

Accuracy on the clean test set is the key metric in our study. We expect that methods that are robust to the label noise observed in the training phase, should be able to improve the test accuracy when compared to the baseline model.

Additionally, to better understand the difference between synthetic and real-world noise, we collected detailed validation metrics. The validation dataset \(_{val}\) contained both instances for which the observed label \(_{i}\) was incorrect (\(_{val}^{}\)) and correct (\(_{val}^{}\)). Noisy observations from \(_{val}^{}\) were used to measure the memorization metric \(_{val}\), defined as a ratio of predictions \(_{i}\) that match the noisy label \(_{i}\). Notice that our memorization metric is computed on the validation set, contrary to the training set typically used in the literature . Our metric increases when the model not only memorizes incorrect classes from the training observations, but also repeats these errors on unseen observations. Furthermore, we compute accuracy on \(_{val}^{}\) denoted as \(_{val}^{}\) and its counterpart on the clean fraction, \(_{val}^{}\).

### Benchmarked methods

We evaluated the following methods for learning with noisy labels: Self-Paced Learning (SPL) , Provably Robust Learning (PRL) , Early Learning Regularization (ELR) , Generalized Jensen-Shannon Divergence (GJSD) , Co-teaching (CT) , Co-teaching+ (CT+)  and Mixup (MU) . Additionally, we implemented Clipped Cross-Entropy as a simple baseline (see Appendix **A**). These approaches represent a comprehensive selection of different method families: novel loss functions (GJSD), noise filtration (SPL, PRL, CCE, CT, CT+), robust regularization (ELR), data augmentation (MU) and training loop modifications (CT, CT+).

These methods are implemented with a range of technologies and software libraries. As such, in order to have a reliable and unbiased framework for comparing them, it is necessary to standardize the software implementation. To this end, we re-implemented these methods using PyTorch (version 1.13.1) and PyTorch Lightning (version 1.5.0) software libraries. We publish our re-implementations and the accompanying evaluation code on GitHub at [https://github.com/allegro/AlleNoise](https://github.com/allegro/AlleNoise).

To select the best hyperparameters (see Appendix **A**) for each of the benchmarked algorithms, we performed a tuning process on the _AlleNoise_ dataset. We focused on maximizing the fraction of correct clean examples \(_{val}^{}\) within the validation set for two noise types: 15% real-world noise and 15% symmetric noise. The tuning was performed on a single fold selected out of five cross-validation folds, yielding optimal hyperparameter values (**Tab. S1**). We then used these tuned values in all further experiments.

## 5 Results

The selected methods for learning with noisy labels were found to perform differently on AlleNoise than on several types of synthetic noise. Below we highlight those differences in performance and relate them to the dissimilarities between real-world and synthetic noise.

### Synthetic noise vs _AlleNoise_

The selected methods were compared on the clean dataset, the four types of synthetic noise and on the real-world noise in _AlleNoise_ (**Tab. 2**). The accuracy score on the clean dataset did not degrade for any of the evaluated algorithms when compared to the baseline CE. When it comes to the performance on the datasets with symmetric noise, the best method was GJSD, with CCE not too far behind. GJSD increased the accuracy by 1.31 percentage points (p.p.) over the baseline. For asymmetric noise types, the best method was consistently ELR. It significantly improved the test accuracy in comparison to CE, by 1.3 p.p. on average. Interestingly, some methods deteriorated the test accuracy. CT+ was worse than the baseline for all synthetic noise types (by 2.59 p.p., 2.12 p.p., 3.1 p.p., 2.02 p.p. for symmetric, pair-flip, nested-flip and matrix-flip noises, respectively), while SPL decreased the results for all types of asymmetric noise (by 3.63 p.p., 4.2 p.p., 5.17 p.p. for pair-flip, nested-flip and matrix-flip noises, respectively). CT+ seems to perform better for noise levels higher than 15% (see Appendix **B**). On _AlleNoise_, we observed nearly no improvement in accuracy for any of the evaluated algorithms, and CT+, PRL and SPL all deteriorated the metric (by 2.65 p.p., 2.05 p.p. and 4.61 p.p., respectively).

### Noise type impacts memorization

To better understand the difference between synthetic noise types and _AlleNoise_, we analyze how the \(_{val}^{}\), \(_{val}^{}\) and \(_{val}^{}\) metrics (see 4.4) evolve over time. Memorization and correctness should be interpreted jointly with test accuracy (**Tab. 2**).

Synthetic noise types are memorized to a smaller extent than the real-world _AlleNoise_ (**Fig. 3a**). For the two simplest synthetic noise types, symmetric and pair-flip, the value of \(_{val}\) is negligible (very close to zero). For the other two synthetic noise types, nested-flip and matrix-flip, memorization is still low (2-8%), but there are clearly visible differences between the benchmarked methods. While ELR, CT+ and PRL all keep the value of \(_{val}^{}\) low for both nested-flip and matrix-flip noise types, it is only ELR that achieves test accuracy higher than the baseline.

However, for _AlleNoise_, the situation is completely different. All the training methods display increasing \(_{val}\) values throughout the training, up to 70% (**Fig. 3b**). PRL, SPL and CT+ give lower memorization than the other methods, but this is not reflected in higher accuracy. While these methods correct some of the errors on noisy examples, as measured by \(_{val}^{}\) (**Fig. 3d**), they display \(_{val}^{}\) lower than other tested approaches (**Fig. 3c**), and thus overall they achieve low accuracy.

These results show that reducing memorization is necessary to create noise-robust classifiers. In this context, it is clear that _AlleNoise_, with its real-world instance-dependent noise distribution, is a challenge for the existing methods.

   & Clean set & Symmetric & Pair-flip & Nested-flip & Matrix-flip & AlleNoise \\  CE & \(\) & \(71.97 0.08\) & \(71.92 0.08\) & \(71.77 0.08\) & \(70.75 0.17\) & \(63.71 0.11\) \\  ELR & \(74.81 0.11\) & \(72.15 0.10\) & \(\) & \(\) & \(\) & \(63

### Noise distribution

To get even more insight into why the real-world noise in _AlleNoise_ is more challenging than synthetic noise types, we analyzed the class distribution within our dataset. For synthetic noise types, there are very few highly-corrupted categories (**Fig. 4**). On the other hand, for _AlleNoise_, there is a significant number of such categories. The baseline model test accuracy is much lower for these classes than for other, less corrupted, categories. The set of these highly-corrupted classes is heavily populated by the following:

* _Specialized categories_ that can be easily mistaken for a more generic category. For example, items belonging to the class _safety shoes_ are frequently listed in categories _derby shoes_, _ankle boots_ or _other_. In such cases, during the training, the model sees a large number of mislabeled instances of that class and very few correctly labeled ones, which is not enough to learn correct class associations.
* _Archetypal categories_ that are considered the most representative examples of a broader parent category. For instance, car tires are most frequently listed in _Summer tires_ even when they actually should belong to _All-season tires_ or other specialized categories. In this case, the learnt representation of the class is distorted by a huge number of specialized items mislabeled as the archetypal class.

We hypothesize that these categories are the main culprits behind the poor performance of the model.

## 6 Discussion

Our experiments show that the real-world noise present in _AlleNoise_ is a challenging task for existing methods for learning with noisy labels. We hypothesize that the main challenges for these methods stem from two major features of _AlleNoise_: 1) real-world, instance dependent noise distribution, 2) relatively large number of categories with class imbalance and long tail. While previous works have investigated challenges 1)  and 2) , this paper combines both in a single dataset and evaluation study, while also applying them to text data. We hope that making _AlleNoise_ available

Figure 3: Memorization and correctness metrics as a function of the training step. **(a)** The value of \(_{val}\) for synthetic noise types. **(b)** The value of \(_{val}\) for _AlleNoise_. **(c)** The value of \(_{val}^{}\) for _AlleNoise_. **(d)** The value of \(_{val}^{}\) for _AlleNoise_.

publicly will spark new method development, especially in directions that would address the features of our dataset.

Based on our experiments, we make several interesting observations. The methods that rely on removing examples from within a batch perform noticeably worse than other approaches. We hypothesize that this is due to the large number of classes and the unbalanced distribution of their sizes (especially the long tail of underrepresented categories) in _AlleNoise_ - by removing samples, we lose important information that is not recoverable. This is supported by the fact that such noise filtration methods excel on simple benchmarks like CIFAR-10, which all have a completely different class distribution. In order to mitigate the noise in _AlleNoise_, a more sophisticated approach is necessary. A promising direction seems to be the one presented by ELR. While for the real-world noise it did not increase the results above the baseline CE, it was the best algorithm for class-dependent noise types. The outstanding performance of ELR might be attributed to its target smoothing approach. The use of such soft labels may be particularly adequate to extreme classification scenarios where some of the classes are semantically close. Extending this idea to include an instance-dependent component may lead to an algorithm robust to the real-world noise in _AlleNoise_. Furthermore, based on the results of the memorization metric, it is evident that this realistic noise pattern needs to be tackled in a different way than synthetic noise. With the clean labels published as a part of _AlleNoise_, we enable researchers to further explore the issue of memorization in the presence of real-world instance-dependent noise.

## 7 Conclusions and future work

In this paper, we presented a new dataset for the evaluation of methods for learning with noisy labels. Our dataset, _AlleNoise_, contains a real-world instance-dependent noise distribution, with both clean and noisy labels, provides a large-scale classification problem, and unlike most previously available datasets in the field of learning from noisy labels, features textual data in the form of product names. We performed an evaluation of established noise-mitigation methods, which showed quantitatively that these approaches are not enough to alleviate the noise in our dataset. With _AlleNoise_, we hope to jump-start the development of new robust classifiers that would be able to handle demanding, real-world instance-dependent noise.

The scope of this paper is limited to BERT-based classifiers. As _AlleNoise_ includes clean label names in addition to noisy labels, it could be used to benchmark Large Language Models in few-shot or in-context learning scenarios. We leave this as a future research direction.

Figure 4: Noise level distribution over target categories (blue bars) shows that _AlleNoise_ has a substantial fraction of classes with noise level over 0.5, contrary to synthetic noise. The same distribution multiplied by per-bin macro accuracy (yellow bars) shows that those specialized categories are particularly difficult to predict correctly.

## Funding

This work was funded fully by Allegro.com.

## Competing interests

We declare no competing interests.