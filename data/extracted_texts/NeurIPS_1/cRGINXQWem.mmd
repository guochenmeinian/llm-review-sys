# Precise Asymptotic Generalization for Multiclass Classification with Overparameterized Linear Models

Precise Asymptotic Generalization for Multiclass Classification with Overparameterized Linear Models

David X. Wu

Department of EECS

UC Berkeley

Berkeley, CA 94720

david_wu@berkeley.edu

&Anant Sahai

Department of EECS

UC Berkeley

Berkeley, CA 94720

sahai@eecs.berkeley.edu

###### Abstract

We study the asymptotic generalization of an overparameterized linear model for multiclass classification under the Gaussian covariates bi-level model introduced in Subramanian et al. (2022), where the number of data points, features, and classes all grow together. We fully resolve the conjecture posed in Subramanian et al. (2022), matching the predicted regimes for generalization. Furthermore, our new lower bounds are akin to an information-theoretic strong converse: they establish that the misclassification rate goes to 0 or 1 asymptotically. One surprising consequence of our tight results is that the min-norm interpolating classifier can be asymptotically suboptimal relative to noninterpolating classifiers in the regime where the min-norm interpolating regressor is known to be optimal.

The key to our tight analysis is a new variant of the Hanson-Wright inequality which is broadly useful for multiclass problems with sparse labels. As an application, we show that the same type of analysis can be used to analyze the related multilabel classification problem under the same bi-level ensemble.

## 1 Introduction

In this paper, we directly follow up on a specific line of work initiated by Subramanian et al. (2022); Wu and Sahai (2023). For the sake of self-containedness, we briefly reiterate the context, directing the reader to Subramanian et al. (2022) and the references cited therein for more. A broader story can be found in Bartlett et al. (2021); Belkin (2021); Dar et al. (2021); Oneto et al. (2023).

Classical statistical learning theory intuition predicts that highly expressive models, which can interpolate random labels (Zhang et al., 2016, 2021), ought not to generalize well. However, deep learning practice has seen such models performing well when trained with good labels. Resolving this apparent contradiction has recently been the focus of a multitude of works, and this paper builds on one particular thread of investigation that can be rooted in Bartlett et al. (2020); Muthukumar et al. (2020) where the concept of benign/harmless interpolation was crystallized in the context of overparameterized linear regression problems and conditions given for when this can happen. In Muthukumar et al. (2021), a specific toy "bi-level model" with Gaussian features was introduced to study overparameterized binary classification and show that successful generalization could happen even beyond the conditions for benign interpolation for regression. Following the introduction of the corresponding multi-class problem in Wang et al. (2021) with a constant number of classes, an asymptotic setting where the number of classes can grow with the number of training examples was introduced in Subramanian et al. (2022) where a conjecture was presented for when minimum-norm interpolating classifiers will generalize. We are now in a position to state our main contributions; afterwards, we expand on the related works.

#### Our contributions

Our main contribution is crisply identifying the asymptotic regimes where an overparameterized linear model which performs minimum-norm interpolation does and does not generalize for multiclass classification under a Gaussian features assumption, thus resolving the main conjecture posed by (Subramanian et al., 2022). We improve on the analysis of Subramanian et al. (2022); Wu and Sahai (2023), covering all regimes with the asymptotically optimal misclassification rate. When the model generalizes, it does so with a misclassification rate \(o(1)\), and we show a matching "strong converse" establishing when it misclassifies, it does so with rate \(1-o(1)\), where the explicit rate is nearly identical to that of random guessing. The critical component of our analysis is a new variant of the Hanson-Wright inequality, which applies to bilinear forms between a vector with subgaussian entries and a vector that is bounded and has _soft sparsity_, a notion we will define in Section 4.2. We show how this tool can be used to analyze other multiclass problems, such as multilabel classification.

### Brief treatment of related work

Our thread begins with a recent line of work that analyzes the generalization behavior of overparameterized linear models for regression (Hastie et al., 2022; Mei and Montanari, 2022; Bartlett et al., 2020; Belkin et al., 2020; Muthukumar et al., 2020). These simple models demonstrate how the capacity to interpolate noise can actually aid in generalization: training noise can be harmlessly absorbed by the overparameterized model without contaminating predictions on test points. In effect, extra features can be regularizing (in the context of descent algorithms' implicit regularization (Soudry et al., 2018; Ji and Telgarsky, 2019; Engl et al., 1996; Gunasekar et al., 2018)), but an excessive amount of such regularization causes regression to fail because even the true signal will not survive the training process. Although works in this thread focus on very shallow networks, Chatterji and Long (2023) established that deeper networks can behave similarly. Note that recently, Mallinar et al. (2022) called-out an alternative regime (behaving like 1-nearest-neighbor learning) called "tempered" overfitting in which training noise is not completely absorbed but the true signal does survive training.

The thread continues in a line of work that studies binary classification (Muthukumar et al., 2021; Chatterji and Long, 2021; Wang and Thrampoulidis, 2021) in similar overparameterized linear models. While confirming that the basic story is similar to regression, these works identify a further surprise: binary classification can work in some regimes where the corresponding regression problem would not work1 due to the regularizing effect of overparameterization being too strong. Just as in the regression case, the results here are sharp in toy models: we can exactly characterize where binary classification using an interpolating classifier asymptotically generalizes.

With binary classification better understood, the thread continues to multiclass classification. After all, the current wave of deep learning enthusiasm originated in breakthrough performance in multiclass classification, and we have seen a decade of ever larger networks trained on ever larger datasets with ever more classes Kaplan et al. (2020). Using similar toy models (Muthukumar et al., 2020, 2021; Wang et al., 2022), the constant number of classes case was studied in Wang et al. (2021) to recover results similar to binary classification and subsequently generalized to general convex losses with regularization in Loureiro et al. (2021) and student-teacher networks in Cornacchia et al. (2023).

Subramanian et al. (2022) further introduced a model where the number of classes grows with the number of training points and proved an achievability result on how fast the number of classes can grow while still allowing the interpolating classifier to asymptotically generalize. While Subramanian et al. (2022) gave a conjecture for what the full region should be, there was no converse proof, and they could not show generalization in entire conjectured region. Wu and Sahai (2023) proved a partial weak converse; they showed that the misclassification rate is bounded away from \(0\) -- rather than tending to \(1\) -- in some of the predicted regimes.

The model, formally defined in the following section, is a stylized version of the well-known spiked covariance model (Johnstone, 2001; Donoho et al., 2018). On the theoretical front, it is related to several problems such as PCA variants (Montanari and Richard, 2015; Richard and Montanari, 2014)and community detection in the stochastic block model (Abbe, 2017). These models have also been applied in practice for climate studies and functional data analysis (Johnstone, 2001). At a high level, spiked covariance models can be interpreted as a linearized version of the manifold hypothesis.

## 2 Problem setup

The following exposition is lifted from Subramanian et al. (2022), which we include for the sake of staying consistent and self-contained. In Appendix K, we include an alternative high level framing of the problem which readers may find helpful for intuition.

We consider the multiclass classification problem with \(k\) classes. The training data consists of \(n\) pairs \(\{_{i},_{i}\}_{i=1}^{n}\) where \(_{i}^{d}\) are i.i.d standard Gaussian vectors2. We assume that the labels \(_{i}[k]\) are generated as follows.

**Assumption 1** (1-sparse noiseless model).: _The class labels \(_{i}\) are generated based on which of the first \(k\) dimensions of a point \(_{i}\) has the largest value,_

\[_{i}=*{arg\,max}_{m[k]}_{i}[m]. \]

Let us emphasize at this point that the classifier that we analyze only observes the training data, and does not use any of the data-generating assumptions.

For a vector \(\), we index its \(j\)th entry with \([j]\). Hence, under Assumption 1, \(_{i}[m]\) can be interpreted as how representative of class \(m\) the \(i\)th training point is.

For clarity of exposition in the analysis, we make explicit a feature weighting that transforms the training points:

\[_{i}^{w}[j]=}_{i}[j] j[d]. \]

Here \(^{d}\) contains the squared feature weights. The feature weighting serves the role of favoring the true pattern, something that is essential for good generalization. Again, we emphasize that the classifier does not do any reweighting of features; this explicit step is purely syntactic. 3

The weighted feature matrix \(^{w}^{n d}\) is given by

\[^{w}=[_{1}^{w}_{n}^{w} ]^{}=[}_{1}}_{d}] \]

where we introduce the notation \(_{j}^{n}\) to contain the \(j^{th}\) feature from the \(n\) training points. Note that \(_{j} N(0,_{n})\) are i.i.d Gaussians. We use a one-hot encoding for representing the labels as the matrix \(^{}^{n k}\)

\[^{}=_{1}^{}& &_{k}^{},_{m}^{ }[i]=1,&_{i}=m\\ 0,&. \]

Since we consider linear models, we center the one-hot encodings and define

\[_{m}_{m}^{}-. \]Our classifier consists of \(k\) coefficient vectors \(}_{m}\) for \(m[k]\) that are learned by minimum-norm interpolation (MNI) of the zero-mean one-hot variants using the weighted features:4

\[}_{m}=_{}\|\|_{2} \] \[^{w}=_{m}. \]

We can express these coefficients in closed form as

\[}_{m}=(^{w})^{}(^{w}(^{w})^{} )^{-1}_{m}. \]

On a test point \(_{} N(0,_{d})\) we predict a label as follows: First, we transform the test point into the weighted feature space to obtain \(_{}^{w}\) where \(_{}^{w}[j]=}_{}[j]\) for \(j[d]\). Then we compute \(k\) scalar "scores" and assign the class based on the largest score as follows:

\[=*{arg\,max}_{1 m k}}_{m}^{}_{}^{w}. \]

By assumption, a misclassification event \(_{}\) occurs whenever

\[*{arg\,max}_{1 m k}_{}[m] *{arg\,max}_{1 m k}}_{m}^{}_{ }^{w}. \]

We study where the MNI generalizes in an asymptotic regime where the number of training points, features, classes, and feature weights all scale according to the bi-level ensemble model5:

**Definition 1** (Bi-level ensemble).: _The bi-level ensemble is parameterized by \(p,q,r\) and \(t\) where \(p>1\), \(0 r<1\), \(0<q<(p-r)\) and \(0 t<r\). Here, parameter \(p\) controls the extent of overparameterization, \(r\) determines the number of favored features, \(q\) controls the weights on favored features and \(t\) controls the number of classes. The number of features (\(d\)), number of favored features (\(s\)), and number of classes (\(k\)) all scale with the number of training points (\(n\)) as follows:_

\[d= n^{p},s= n^{r},a=n^{-q},k=c_{k} n^{t} , \]

_where \(c_{k}\) is a positive integer. Define the feature weights by_

\[}=},&1 j s\\ },&. \]

_We introduce the notation \(_{F}\) and \(_{U}\) to distinguish between the (squared) favored and unfavored weights, respectively._

We visualize the bi-level model in Fig. 1, reproduced from Subramanian et al. (2022). Intuitively, the bi-level ensemble captures a simple family of overparameterized problems where learning can succeed. Although there are \(d=n^{p}\) features where \(d n\), there is a low dimensional subspace of favored, higher weight features of dimension \(s=n^{r}\), and \(s n\). From this perspective, the bi-level model can be viewed as a parameterized version of an approximate linear manifold hypothesis. Depending on the signal strength, the noise added from the \(d-s\) unfavored features can either help generalization ("benign overfitting") or overwhelm the true signal and cause the classifier to fail.

## 3 Main results

In this section we state our main results and compare them to what was known and conjectured previously. Subramanian et al. (2022) use heuristic calculations to conjecture necessary and sufficient conditions for the bi-level model to generalize; we restate the conjecture here for reference.

**Conjecture 3.1** (Conjectured bi-level regions).: _Under the bi-level ensemble model (Definition 1), when the true data generating process is 1-sparse (Assumption 1), as \(n\), the probability of misclassification \([_{}]\) for MNI as described in Eq. (6) satisfies_

\[[_{}]0,&t<\{1-r,p+1 -2\{1,q+r\}\}\\ 1,&t>\{1-r,p+1-2\{1,q+r\}\}. \]

Our main theorem establishes that Conjecture 3.1 indeed captures the correct generalization behavior of the overparameterized linear model.

**Theorem 3.2** (Generalization for bi-level-model).: _Under the bi-level ensemble model (Definition 1), when the true data generating process is 1-sparse (Assumption 1), Conjecture 3.1 holds._

For comparison, we quote the best known previous positive and negative results for the bi-level model, which only hold in the restricted regime where regression fails (\(q+r>1\)).

**Theorem 3.3** (Generalization for bi-level model (Subramanian et al., 2022)).: _In the same setting as Conjecture 3.1, in the regime where regression fails \((q+r>1)\), as \(n\) we have \([_{}] 0\) if_

\[t<\{1-r,p+1-2(q+r),p-2,2q+r-2\} \]

**Theorem 3.4** (Misclassification in bi-level model (Wu and Sahai, 2023)).: _In the same setting as Conjecture 3.1, in the regime where regression fails \((q+r>1)\), as \(n\) we have \([_{}]\) if_

\[t>\{1-r,p+1-2(q+r)\} \]

Let us interpret the different conditions in Conjecture 3.1. To interpret the condition \(t<1-r\), first rearrange it to \(t+r<1\). Recall that the parameter \(r\) controls the number of favored features, and hence is a proxy for the "effective dimension" of the problem. On the other hand, the parameter \(t\) controls the number of classes, so in a loose sense there are \(n^{t+r}\) parameters being learned. From this perspective, the condition \(t+r<1\) says that the problem is "effectively underparameterized".

The other condition on \(p+1-2\{1,q+r\}\) comes from looking at the noise from the unfavored features. To see why, recall that the squared favored feature weighting is \(_{F}=n^{p-q-r}\). So for fixed \(p\), the quantity \(q+r\) controls the level of favored feature weighting. When \(q+r>1\), the favored feature weighting is small enough that regression fails, and the empirical covariance becomes flat. In this case, the condition becomes \(t<p+1-2(q+r)=(p-q-r)+1-(q+r)\). As \(q+r\) increases, the amount of favoring decreases, making it harder to generalize.

For ease of comparison between our main result and Theorems 3.3 and 3.4, we visualize the regimes in Fig. 2, as in Subramanian et al. (2022); Wu and Sahai (2023). In particular, the blue starred and dashed regions in Fig. 2 indicate how Theorem 3.3 only applies where regression fails. In contrast, our new result holds regardless of whether regression fails or not, as in the the green diamond region and light blue triangle regions. The regions are also completely tight; the looseness between the prior Theorem 3.3 and our result can be seen in the light blue square region.

Figure 1: Bi-level feature weighting model. The first \(s\) features have a higher weight and are favored during minimum-norm interpolation. These can be thought of as the square-roots of the eigenvalues of the feature covariance matrix \(\) in a Gaussian model for the covariates as in Bartlett et al. (2020).

The weak converse in the prior Theorem 3.4 captures some of the correct conditions for misclassification, but again only when \(q+r>1\). As depicted in the maroon X region for \(r<0.25\) in Fig. 1(b), our main theorem gives a strong converse, whereas Theorem 3.4 has nothing to say because \(q+r<1\). Theorem 3.4 also only proves that the misclassification rate is asymptotically at least \(\). In the red circle and maroon X regions, we illustrate how our result pushes the misclassification rate to \(1-o(1)\), which requires a more refined analysis. We elaborate on this further in Section 4.

We remark that it is simpler to analyze the case where regression fails, as the random matrices that arise in the analysis are _flat_, i.e. approximately equal to a scaled identity matrix. However, in the regime where regression works, the same matrices have a _spiked_ spectrum, which complicates the analysis. To smoothly handle both cases, we leverage a new variant of the Hanson-Wright inequality to show concentration of certain sparse bilinear forms; see Section 4.1 for more details.

## 4 Technical overview

We now sketch out the proof for our main theorem. As in Subramanian et al. (2022); Wu and Sahai (2023), the starting point is writing out the necessary and sufficient conditions for misclassification.

Assume without loss of generality that the test point \(_{} N(0,_{l})\) has true label \(\) for some \([k]\). Let \(_{}^{w}\) be the weighted version of this test point. From (10), an equivalent condition for misclassification is that for some \(,[k]\), we have \(}_{}^{}_{}^{w}<}_{ }^{}_{}^{w}\), i.e. the score for \(\) outcompetes the score for \(\). Define the Gram matrix \(^{w}(^{w})^{}\), the relative label vector \(y_{}-_{}\{-1,0,1\}^{n}\), and the relative survival vector \(}_{,}^{d}\) which compares the signal from \(\) and \(\):

\[}_{,}[j] _{j}^{-1/2}(}_{}[j]- {}_{}[j]) \] \[=_{j}^{}^{-1}y, \]

where to obtain the last line we have used (8). By converting the misclassification condition into the unweighted feature space we see that we will have errors when

\[_{}}_{,}[]_{ }[]-_{}}_{,}[]_{}[]<_{j\{,\}}_{j}}_{ ,}[j]_{}[j]. \]

Define the contamination term \(_{,}\):

\[_{,}} _{j}^{2}(}_{,}[j])^{2}}. \]

Note that \(_{,}\) normalizes the RHS of (18) into a standard Gaussian. Indeed, define

\[Z^{()}_{,}}_{j\{ ,\}}_{j}}_{,}[j]_{}[j] N(0,1). \]

Figure 2: Example of regimes for multiclass/binary classification and regression. The white regions correspond to invalid regimes under the bi-level model. The entirety of 1(b) and all the light blue regions are new to this paper, as is showing that the error tends to \(1\) in the maroon regions.

Since \(,[k]\) are favored, we have \(_{}=_{}=_{F}\). Hence, an equivalent condition for misclassification is that there exists some \(\), \([k]\) such that

\[}{_{,}}(}_{,}[]_{}[]-}_{,}[]_{}[])<Z^{( )}. \]

We now translate the above criterion into _sufficient_ conditions for correct classification and misclassification and analyze these two cases separately.

Correct classification:For correct classification, it suffices for the minimum value of the LHS of Eq. (21) to outcompete the maximum value of the RHS, where the max is taken over \([k],\). Some algebra, as in Subramanian et al. (2022), shows that we correctly classify if

\[_{F}}_{, }[]}{_{}_{,}}( {_{}(_{}[]-_{ }[])}_{}-|_{}[]|}_{} |}_{ ,}[]-}_{,}[]}{ {}_{,}[]}|}_{})\] \[>Z^{()}}_{}. \]

We will show that under the conditions specified in Conjecture 3.1, with high probability, the relevant survival to contamination ratio \(/\) grows at a polynomial rate \(n^{v}\) for some \(v>0\), whereas the term in the parentheses shrinks at a subpolynomial rate \((n^{-})\) for any \(>0\). Further, by standard subgaussian maximal inequalities, the magnitudes of the _normalized contamination_ is no more than \(O()\) with high probability. Thus, with high probability the LHS outcompetes the RHS, leading to correct classification. See Section 4.1 for more discussion on how we prove tight bounds on the survival-to-contamination ratios.

Misclassification:On the other hand, for misclassification it suffices for the maximum _absolute_ value of the LHS of Eq. (21) to be outcompeteed by the maximum value of the RHS. Some manipulations yield the following sufficient condition for misclassification:

\[_{F}(|}_{,}[]|+|}_{, }[]|)}{_{}_{,}}}_{ /}| _{}[]|}_{} <Z^{()}}_{}. \]

We show that within the misclassification regimes in Conjecture 3.1, the survival-to-contamination ratio \(/\)_shrinks_ at a polynomial rate \(n^{-w}\) for some \(w>0\). By standard subgaussian maximal inequalities, the largest label-defining feature is \(O()\) with high probability. Gaussian anticoncentration implies that for some \(,[k]\), \(Z^{()}\) outcompetes the LHS with probability at least \(-o(1)\). Hence, we conclude that the model will misclassify with rate at least \(\) asymptotically.

Let us now describe how to boost the misclassification rate to \(1-o(1)\). Notice that the above argument only considered the competition between the LHS of Eq. (23) and one of the \(Z^{()}\)'s on the RHS instead of the maximum \(Z^{()}\). It's not hard to see from the definition of \(Z^{()}\) in Eq. (20) that the \(Z^{()}\) are jointly Gaussian. For intuition's sake, assuming the \(Z^{()}\) were _independent_, then \(_{}Z^{()}\) would outcompete with probability \((-o(1))^{k-1}\).

In reality, the \(Z^{()}\) are correlated, but we are able to show that the maximum correlation between the \(Z^{()}\) is \(+o(1)\) with high probability. An application of Slepian's lemma (Slepian (1962)) and some explicit bounds on orthant probabilities (Pinasco et al. (2021)) implies that \(_{}Z^{()}>0\) with probability at least \(1-}\). Another application of anticoncentration implies that \(_{}Z^{()}>n^{-w}\) with probability \(1-o(1)\), which finishes off the proof.

### Bounding the survival-to-contamination ratio

Note that the critical _survival-to-contamination_ ratio appears in both Eqs. (22) and (23). The most involved part of the proof is nailing down the correct order of growth of the survival to contamination ratio; a similar analysis tightly bounds the survival variation and the correlation structure of the \(Z^{()}\)To understand the relative survival and contamination, we must analyze the bilinear forms \(}_{,}[j]=_{j}^{}^{-1}y\). Similarly, to control the correlation of the \(Z^{()}\), we must understand the correlation between the \(}_{,}\) vectors, which reduces to understanding the bilinear forms \(_{j}^{}^{-1}_{}\) for \(j[d],[k]\). The main source of inspiration for bounding these bilinear forms is the heuristic style of calculation carried out in Appendix K of Subramanian et al. (2022) that leads to Conjecture 3.1.

To simplify the discussion, we temporarily restrict to the regime where regression fails (\(q+r>1\)). However, our main technical tool seamlessly generalizes to the regime where regression works (\(q+r<1\)). In the regime where regression fails, \(^{-1}\) turns out to have a _flat_ spectrum: \(^{-1}\) for some constant \(>0\). Assume for now that \(^{-1}\) is _exactly_ equal to a scaled identity matrix. Then the survival is proportional to \(_{}^{}y\), which is a random inner product. Similarly, to bound the contamination terms we must control the random inner product \(_{j}^{}y\) for \(j\{,\}\).

Since \(y\) is a sparse vector -- it only has \(\) nonzero entries in expectation -- a quick computation reveals that \([_{}^{}y]=()\) and \([_{j}^{}y]=0\). The deciding factor, then, is how tightly these quantities concentrate around their means. A naive application of Hoeffding implies a concentration radius of order \(()\), which would lead to looseness in the overall result. The hope is to exploit sparsity to get a concentration radius of order \(()\). This is where our new technical tool Theorem 4.1 comes in, which may be of independent interest; we present it in the following section.

### A new variant of the Hanson-Wright inequality

In reality, even in the regime where regression fails, \(^{-1}\) is not actually perfectly flat. Even worse, in the regime where regression works, \(^{-1}\) is actually spiked. Thus, we cannot simply reduce the bilinear form \(_{j}^{}^{-1}y\) to an inner product. Instead, we turn to the well-known Hanson-Wright inequality (Rudelson and Vershynin, 2013), which tells us that quadratic forms of random vectors with independent, mean zero, subgaussian entries concentrate around their mean. It was used extensively to study binary classification (Muthukumar et al., 2021), and multiclass classification (Subramanian et al., 2022; Wu and Sahai, 2023).

However, just as Hoeffding is loose, so too is the standard form of Hanson-Wright, because it also does not exploit sparsity. This motivates a new variant of Hanson-Wright which fully leverages the (soft) sparsity inherent to multiclass problems with an increasing number of classes. We now formally define the notions of soft and hard sparsity.

**Definition 2** (Soft and hard sparsity).: _For \( 1\), we say that random vector \(=(Y_{i})_{i=1}^{n}\) has soft sparsity at level \(\) if \(|Y_{i}| 1\) almost surely and \((Y_{i})\) for all \(i\). On the other hand, we say that \(\) has hard sparsity at level \(\) if at most a \(\) fraction of the \(Y_{i}\) are nonzero._

In particular, our variant Theorem 4.1 below requires that one of the vectors in the bilinear form has _soft sparsity_ at level \(\). Throughout, one should think of \(=o(1)\), and for us indeed \(=O()\). One can check that a bounded random vector \(\) with _hard sparsity_ level \(\) must also have soft sparsity at level \(O()\), so soft sparsity is more general for bounded random vectors. In Table 1 we compare our variant with several variants of Hanson-Wright which have appeared in the literature, some of which involve hard sparsity.

Define the subgaussian norm \(_{_{2}}\)(Vershynin, 2018) as

\[_{_{2}}=_{K>0}K:\! (^{2}/K^{2}) 2}, \]

**Theorem 4.1** (Hanson-Wright for bilinear forms with soft sparsity).: _Let \(=(X_{1},,X_{n})^{n}\) and \((Y_{1},,Y_{n})^{n}\) be random vectors such that \((X_{i},Y_{i})\) are independent pairs of (possibly correlated) centered random variables such that \( X_{i}_{_{2}} K\) and \(Y_{i}\) has soft sparsity at level \(\), i.e. \(|Y_{i}| 1\) almost surely, and \([Y_{i}^{2}]\). Assume that conditioned on \(Y_{j}\), \( X_{j}_{_{2}} K\). Then there exists an absolute constant \(c>0\) such that for all \(^{n n}\) and \( 0\) we have_

\[[^{}-[^{} ]>] 2\!(-c\{}{K^{2}\|\|_{F}^{2}},\|_{2}} \}). \]

The full proof of Theorem 4.1 is deferred to Appendix G. The main proof techniques are heavily inspired by those of Rudelson and Vershynin (2013); Zhou (2019); Park et al. (2022). However, the proof of Theorem 4.1 is actually simpler than in Park et al. (2022); Wu and Sahai (2023), as bounded with soft sparsity turns out to be easier to work with than subgaussian with hard sparsity. We refer readers to Wu and Sahai (2023) for a more in-depth discussion of how these new "sparse" variants overcome the limitations of previous proof techniques used to study classification problems.

We briefly illustrate how Theorem 4.1 can be used to get tighter results throughout our analysis. A quick calculation reveals that the label vectors \(y\) and \(_{}\) both have soft sparsity at level \(=O(1/k)\). However, \(_{}\) does not have hard sparsity as required by the variants in Park et al. (2022); Wu and Sahai (2023). Since \(_{F}^{2} n_{2}^ {2}\), we obtain a concentration radius \(\) which scales like \(\) rather than \(\) (obtained via vanilla Hanson-Wright) or \(n/\) (obtained via Cauchy-Schwarz). This gain is crucial to tightly analyzing the survival, contamination, and correlation structure.

### Completing the proof sketch

Theorem 4.1 and the above insights about sparsity and independence allow us to prove the following bounds on the relative survival and contamination terms which are tight up to log factors; see the Appendix for more details. For brevity's sake, we introduce the notation \( n^{q+r-1}\).

**Proposition 4.2** (Bounds on relative survival).: _Suppose we are in the bi-level model. With probability at least \(1-O(1/n)\),_

\[_{F}}_{,}[]=\{^{-1},1 \}(n^{-\{t,\}}).\]

Next, we state our bounds on contamination.

**Proposition 4.3** (Bounds on contamination).: _Suppose we are in the bi-level model. Then with probability at least \(1-O(1/n)\), the contamination satisfies_

\[_{,}=,1\}(n^ {})}_{}+})}_{}. \]

Translating the parameters in Propositions 4.2 and 4.3 we see that (i) the relative survival is diminished by a factor \(1/k\) as long as \(k=o()\), and a factor \(1/\) for \(k=()\) (this looseness ends up being negligible for the final result) and (ii) the contamination is diminished by a factor of \(1/\). This essentially matches the expected behavior from the heuristic calculation in Subramanian et al. (2022). Together with some straightforward algebra, Propositions 4.2 and 4.3 allow us to compute the regimes where the survival-to-contamination ratio \(/\) grows or decays polynomially. This yields the stated regimes in Conjecture 3.1; see Appendix A.1 for more details.

For technical reasons, the analogous bounds in Subramanian et al. (2022) are loose, giving rise to unnecessary conditions for good generalization such as \(t<p-2\) and \(t<2q+r-2\). Moreover, we are able to give both upper and lower bounds on the survival and contamination terms, whereas they only give one-sided inequalities for each quantity.

 Variant & Assumptions on \(\) & Concentration radius \\  Classic quadratic\({}^{}\)\(^{}\) & same as \(\) & \((_{F})\) \\ Sparse bilinear\({}^{}\)\(^{}()\) & \(_{i}()\), indep. of \(X_{i}\) but not \(Y_{i}\) & \((_{F})\) \\ Sparse bilinear\({}^{}\)\(^{}()\) & \(_{i}()\), indep. of \(Y_{i}\) but not \(X_{i}\) & \((_{F})\) \\ Theorem 4.1: \(^{}\) & \(|Y_{i}| 1\) a.s., \(Y_{i}^{2}\) & \((_{F})\) \\  

Table 1: Comparison of different variants of the Hanson-Wright inequality. In all variants, we assume that \((,)=(X_{i},Y_{i})_{i=1}^{n}\) are subgaussian, centered, and the pairs \((X_{i},Y_{i})\) are independent across \(i\). We use \(\) to denote elementwise multiplication, which allows us to express hard sparsity with the sparsity mask \(\{0,1\}^{n}\). The concentration radius corresponds to the size of typical fluctuations guaranteed by the concentration inequality, i.e. the \(\) needed for high probability guarantees.

Discussion

In this paper we resolve the main conjecture of Subramanian et al. (2022), identifying the exact regimes where an overparameterized linear model succeeds at multiclass classification. Our techniques also lay the foundation for investigating related generalization for other multiclass tasks and nonlinear algorithms. We hope that by bringing the rigorous proofs closer to the heuristic style of calculation, we open the path for analyzing more complicated and realistic models.

An important next step is to extend our results to more realistic spiked covariance models. For example, one typically observes power-law decay for the extreme eigenvalues in applications. We expect that the bi-level model can be relaxed to allow for constant deviations in the weightings for the favored, non-label defining features and power law decay for the unfavored features. The former change would likely only affect constants in certain areas of the argument that do not crucially depend on the exact constants involved, whereas the latter would likely just change the effective degree of overparameterization (Bartlett et al., 2020). However, even constant fluctuations in weighting for the label-defining features can lead to significant subtleties, as these constant deviations manifest as polynomially large variations in the number of examples of each class. Such heterogeneity between label-defining directions would likely lead to significantly messier conditions for generalization.

Another future direction is to move beyond Gaussian features. It is plausible that similar results would hold for vector subgaussian features which are rotationally invariant, allowing us to rotate into the basis where the features have diagonal covariance. One place where Gaussianity is crucially used is to obtain an explicit lower bound on the margin between the features.

As an example application, we sketch out how our proof techniques imply precise conditions for a variant of the learning task called multilabel classification. In a simple model for multilabel classification, each datapoint can have several of \(k\) possible labels -- corresponding to the positive valued features -- but in the training set only one such correct label is provided at random for each datapoint. We deem that the model generalizes if for any queried label it successfully labels test inputs as positive or negative. We can use the MNI approach here to learn classifiers.

Some thought reveals that the main difference between multilabel classification and multiclass classification from a survival and contamination perspective is that positive features no longer need to outcompete other features. Thus, the main object of study would be the bilinear forms \(_{j}^{}^{-1}_{}\), which is possible thanks to Theorem 4.1. The survival and contamination terms are only affected by the expected values of these bilinear forms, but the expected values match the multiclass behavior up to log factors, which do not affect the regimes where \(/\) will grow or shrink polynomially. A similar analysis thus reveals that MNI will generalize in exactly the same regimes as in Conjecture 3.1. Here, the model generalizes in the sense that with high probability over the labels the model will correctly classify, and failure to generalize means that the model will do no better than a coin toss.

Perhaps surprisingly, resolving Conjecture 3.1 also implies that MNI is asymptotically _suboptimal_ compared to a natural _non-interpolative_ approach: simply make \(}_{m}\) equal to the average6 of all positive training examples of class \(m\). A straightforward analysis, detailed in the supplementary material, reveals this scheme fails to generalize exactly when \(t<\{1-r,p+1-2(q+r)\}\), even in the regime where regression succeeds (\(q+r<1\)). This is particularly interesting because we have shown that in the regime where regression succeeds, MNI generalizes only when \(t<\{1-r,p-1\}\), which is a smaller region. In light of this gap, it would be interesting to identify the _information-theoretic_ barrier for multiclass classification, especially within the broader context of statistical-computation gaps (see e.g. Wu and Xu (2021); Brennan and Bresler (2020)).