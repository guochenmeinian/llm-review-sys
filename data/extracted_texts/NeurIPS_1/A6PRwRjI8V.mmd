# Generalized Semi-Supervised Learning via

Self-Supervised Feature Adaptation

 Jiachen Liang\({}^{1,2}\), Ruibing Hou\({}^{1}\), Hong Chang\({}^{1,2}\), Bingpeng Ma\({}^{2}\), Shiguang Shan\({}^{1,2}\), Xilin Chen\({}^{1,2}\)

\({}^{1}\) Institute of Computing Technology, Chinese Academy of Sciences

\({}^{2}\)University of Chinese Academy of Sciences

jiachen.liang@vipl.ict.ac.cn, {houruibing, changhong, sgshan, xlchen}@ict.ac.cn, bpma@ucas.ac.cn

###### Abstract

Traditional semi-supervised learning (SSL) assumes that the feature distributions of labeled and unlabeled data are consistent which rarely holds in realistic scenarios. In this paper, we propose a novel SSL setting, where unlabeled samples are drawn from a mixed distribution that deviates from the feature distribution of labeled samples. Under this setting, previous SSL methods tend to predict wrong pseudo-labels with the model fitted on labeled data, resulting in noise accumulation. To tackle this issue, we propose _Self-Supervised Feature Adaptation_ (SSFA), a generic framework for improving SSL performance when labeled and unlabeled data come from different distributions. SSFA decouples the prediction of pseudo-labels from the current model to improve the quality of pseudo-labels. Particularly, SSFA incorporates a self-supervised task into the SSL framework and uses it to adapt the feature extractor of the model to the unlabeled data. In this way, the extracted features better fit the distribution of unlabeled data, thereby generating high-quality pseudo-labels. Extensive experiments show that our proposed SSFA is applicable to various pseudo-label-based SSL learners and significantly improves performance in labeled, unlabeled, and even unseen distributions.

## 1 Introduction

Semi-Supervised Learning (SSL) uses a small amount of labeled data and a large amount of unlabeled data to alleviate the pressure of data labeling and improve the generalization ability of the model. Traditional SSL methods [21; 43; 32; 28; 22; 30; 5; 6] usually assume that the feature distribution of unlabeled data is consistent with that of labeled one. However, in many real scenarios, this assumption may not hold due to different data sources. When unlabeled data is sampled from distributions different from labeled data, traditional SSL algorithms will suffer from severe performance degradation, which greatly limits their practical application.

In real-world scenarios, it is quite common to observe feature distribution mismatch between labeled and unlabeled samples. _On the one hand, unlabeled samples could contain various corruptions._ For example, in automatic driving, the annotated images used for training can hardly cover all driving scenes, and a plethora of images under varying weather and camera conditions are captured during real driving. Similarly, in medical diagnosis, individual differences and shooting conditions among patients could incur various disturbances in unlabeled data. _On the other hand, unlabeled samples could contain unseen styles._ For example, in many tasks, the labeled samples are typically real-world photos, while unlabeled data collected from the Internet usually contain more styles that are not present in the labeled data, such as cartoons or sketches. Notably, although the distributions of these unlabeled data may differ from the labeled data, there is implicit common knowledge between them that can compensate for the diversity and quantity of training data. Therefore, it is crucial to enlarge the SSL scope to effectively utilize unlabeled data from different distributions.

In this study, we focus on a more realistic scenario of Feature Distribution Mismatch SSL (FDM-SSL), i.e., the feature distributions of labeled and unlabeled data could be different and the feature distributions of test data could contain multiple distributions. It is generally observed that the performance of classical SSL algorithms [22; 30; 6; 9] degrades substantially under feature distribution mismatch. Specifically, in the early stages of training, classical SSL algorithms typically utilize the current model fitted on labeled data to produce pseudo-labels for unlabeled data. However, when the distribution of unlabeled data deviates, the current model are not applicable to unlabeled data, resulting in massive incorrect pseudo-labels and aggravating confirmation bias . Recently, some works [7; 19] attempt to address FDM-SSL. These approaches assume that the unlabeled feature distribution comes from a single source and they only focus on the labeled distribution, which do not always hold in real tasks. However, due to the unknown test scenarios, it is desirable to develop a method to perform well on labeled, unlabeled and even unseen distributions simultaneously.

In this work, we propose a generalized _Self-Supervised Feature Adaptation_ (SSFA) framework for FDM-SSL which does not need to know the distribution of unlabeled data ahead of time. The core idea of SSFA is to decouple pseudo-label predictions from the current model to address distribution mismatch. SSFA consists of two modules, including the semi-supervised learning module and the feature adaptation module. Inspired by  that the main classification task can be indirectly optimized through the auxiliary task, SSFA incorporates an auxiliary self-supervised task into the SSL module to train with the main task. In the feature adaptation module, given the current model primarily fitted on labeled data, SSFA utilizes the self-supervised task to update the feature extractor before making predictions on the unlabeled data. After the feature extractor adapts to the unlabeled distribution, refined features can be used to generate more accurate pseudo-labels to assist SSL.

Furthermore, the standard evaluation protocol of SSL normally assumes that test samples follow the same feature distribution as labeled training data. However, this is too restricted to reflect the diversity of real-world applications, where different tasks may focus on different test distributions. It is strongly desired that the SSL model can perform well across a wide range of test distributions. Therefore, in this work, we propose new evaluation protocols that involve test data from labeled, unlabeled and unseen distributions, allowing for a more comprehensive assessment of SSL performance. Extensive experiments are conducted on two types of feature distribution mismatch SSL tasks, _i.e._, corruption and style mismatch. The experimental results demonstrate that various pseudo-label-based SSL methods can be directly incorporated into SSFA, yielding consistent performance gains across a wide range of test distributions.

## 2 Related work

**Semi-Supervised Learning (SSL).** Existing SSL methods [44; 7; 30; 41; 50; 9; 6] typically combine various commonly used techniques for semi-supervised tasks, such as consistency regularization[21; 43; 32; 28] and pseudo-label , to achieve state-of-the-art performance. For example,  expands the dataset by interpolating between labeled and unlabeled data. [5; 30] use the confident weak augmented view prediction results to generate pseudo-labels for the corresponding strong augmented view. [41; 50; 9; 44] improve the performance by applying adaptive thresholds instead of fixed thresholds. However, these works focus on traditional semi-supervised learning, which assumes that the labeled data and unlabeled data are sampled from the same distribution. Recently, some works [2; 53] address the issue of different feature distributions for labeled and unlabeled data under certain prerequisites.  assumes the test and unlabeled samples are drawn from the same single distribution.  studies Semi-Supervised Domain Generalization (SSDG), which requires multi-source partially-labeled training data. To further expand SSL to a more realistic scenario, we introduce a new Feature Distribution Mismatch SSL (FDM-SSL) setting, where the unlabeled data comes from multiple distributions and may differ from the labeled distribution. To solve the FDM-SSL, we propose Self-Supervised Feature Adaptation (SSFA), a generic framework to improve the performance in labeled, unlabeled, and even unseen test distributions.

**Unsupervised Domain Adaptation (UDA).** UDA aims to transfer knowledge from a source domain with sufficient labeled data to an unlabeled target domain through adaptive learning. The UDA methods can be roughly categorized into two categories, namely metric-based methods [23; 35; 25; 34; 49] and adversarial methods [1; 16; 8; 24; 46; 29; 37; 10; 42]. Metric-based methods focus on measuring domain discrepancy to align the feature distributions between source and unlabeled target domains. Adversarial methods [16; 47; 24] use a domain classifier as a discriminator to enforce the feature extractor to learn domain-invariant features through adversarial training. Another task similar to FDM-SSL is Unsupervised Domain Expansion (UDE) [39; 51; 33; 45], which aims to maintain the model's performance on the labeled domain after adapting to the unlabeled domain. The main differences between UDA, UDE and FDM-SSL are the number of labeled data during training and the distribution of the unlabeled data. In FDM-SSL, the labeled data is scarce, and the distribution of unlabeled data is not limited to a specific domain but rather a mixture of multiple domains. These challenges make UDA and UDE methods unable to be directly applied to FDM-SSL.

**Test-Time Adaptation (TTA).** If the distribution of test data differs from that of training data, the model's performance may suffer performance degradation. TTA methods focus on improving the model's performance with a two-stage process: a model should first adapt to the test samples and then make predictions of them. For example,  and  introduce an auxiliary self-learning task to adapt the model to test distribution.  modifies the scaling and bias parameters of BatchNorm layers based on entropy minimization. [52; 13; 12; 26] try to improve the robustness of test-time adaptation. Unlike most TTA methods which suppose the test samples come from a single distribution,  assumes a mixed test distribution that changes continuously and stably. However, these assumptions are still difficult to satisfy in real-life scenarios. In addition, similar to UDA, TTA methods generally require a model trained well on the source domain. In contrast, FDM-SSL setting only requires scarce labeled data and has no restrictions on the distribution of unlabeled data.

## 3 Problem Setting

For Feature Distribution Mismatch SSL (FDM-SSL) problem, a model observes a labeled set \(D_{l}=\{(x_{i},y_{i})\}_{i=1}^{N_{l}}\) and an unlabeled set \(D_{u}=\{(u_{j})\}_{j=1}^{N_{u}}\) with \(N_{u} N_{l}\), where \(x_{i}\) and \(u_{i}\) are input images and \(y_{i}\) is the label of \(x_{i}\). The labeled image \(x_{i}\) and unlabeled image \(u_{j}\) are drawn from two different feature distributions \(p_{l}(x)\) and \(p_{u}(x)\), respectively. Note that different from previous works, in FDM-SSL setting the unlabeled samples may come from a mixture of multiple distributions rather than just one distribution: _i.e._\(p_{u}(x)=w_{0}p_{l}(x)+_{k=1}^{K}w_{k}p_{u}^{k}(x)\), where \(w_{0}\) and \(w_{k}\) represent the weights of the labeled distribution \(p_{l}(x)\) and the \(k-\)th unlabeled distribution \(p_{u}^{k}(x)\) respectively. The goal of FDM-SSL is to train a model that generalizes well over a large range of varying test data distributions, including labeled, unlabeled and even distributions unseen during training (\(p_{unseen}(x)\)). The differences between traditional SSL, UDA, TTA and FDM-SSL, are summarized in Table 1.

The core of FDM-SSL is to enhance the utilization of unlabeled samples in the presence of feature distribution mismatch. On one hand, the shared information, such as patterns and structures, in the unlabeled samples can provide effective cues for model adaptation. On the other hand, these mismatched unlabeled samples can facilitate the learning of a more robust model by exposing it to a wide range of data distributions.

## 4 Method

### Overview

In the FDM-SSL setting, due to the feature distribution shift from \(p_{l}(x)\) to \(p_{u}(x)\), directly applying the SSL model fitted on labeled data to unlabeled data may lead to massive inaccurate predictions on the unlabeled data, thereby aggravating confirmation bias and impairing SSL learning.

This motivates us to propose Self-Supervised Feature Adaptation (SSFA), a unified framework for FDM-SSL, which decouples the pseudo-label predictions from the current model to address distribution mismatch. As illustrated in Figure 1, SSFA consists of two modules: a semi-supervised

   Task & Labeled & Unlabeled & Train setting & Test distribution \\  traditional SSL & scarce & abundant & \(p_{l}(x)=p_{u}(x)\) & \(p_{l}(x)\) \\ UDA & abundant & abundant & \(p_{l}(x) p_{u}(x)\) & \(p_{u}(x)\) \\ TTA & abundant & - & \(p_{l}(x)\) & \(p_{u}(x)\) \\ FDM-SSL & scarce & abundant & \(p_{l}(x) p_{u}(x)\) & \(p_{l}(x),p_{u}(x),p_{unseen}(x)\) \\   

Table 1: Comparison between different problem settings.

learning module and a feature adaptation module. The semi-supervised learning module incorporates a pseudo-label-based SSL learner with a self-supervised _auxiliary_ task that shares a portion of feature extractor parameters with the _main_ classification task.  has pointed out that the main task can be indirectly optimized by optimizing the auxiliary task. To this end, the feature adaptation module is designed to update the current model through self-supervised learning on unlabeled data, so as to better match the unlabeled distribution and predict higher-quality pseudo-labels for semi-supervised learning. Therefore, by optimizing the auxiliary self-supervised task individually, the updated model can make more accurate classification decisions in the main task.

### Semi-Supervised Learning Module

In the Semi-Supervised Learning Module, we introduce a self-supervised task as an auxiliary task, which is optimized together with the main task. As shown in Figure 1, the network parameter \(\) comprises three parts: \(_{g}\) for the shared encoder, \(_{c}\) for the main task head, and \(_{s}\) for the auxiliary task head. During training, SSL module optimizes a _supervised loss_\(_{x}\), an _unsupervised loss_\(_{u}\) and a _self-supervised auxiliary loss_\(_{aux}\), simultaneously. Typically, given a batch of labeled data \(\{(x_{b},y_{b})\}_{b=1}^{B}\) with size \(B\) and a batch of unlabeled data \(\{(u_{b})\}_{b=1}^{ B}\) with size \( B\), where \(\) is the ratio of unlabeled data to labeled data, \(_{x}\) applies standard cross-entropy loss on labeled examples:

\[_{x}=_{b=1}^{B}(x_{b},y_{b};_{g}, _{c}), \]

where \((,)\) is the cross-entropy function. Different SSL learners [30; 5] may design different \(_{u}\). One of the widely used is the pseudo-label loss.In particular, given the pseudo-label \(q_{b}\) for each unlabeled input \(u_{b}\), \(_{u}\) in traditional SSL can be formulated as:

\[_{u}=_{b=1}^{ B}(u_{b},q_{b};_{g}, _{c}), \]

where \(\) is the per-sample supervised loss, _e.g._, mean-square error  and cross-entropy loss .

Furthermore, we introduce a self-supervised learning task to the SSL module for joint training to optimize the parameter of the auxiliary task head \(_{s}\). To learn from labeled and unlabeled data distributions simultaneously, it is necessary to use all samples from both distributions for training. Therefore, \(_{aux}\) can be formulated as:

\[_{aux}=(_{b=1}^{B}_{s}(x_{b}; _{g},_{s})+_{b=1}^{ B}_{s}(_{w}(u_{b});_{g}, _{s})+_{b=1}^{ B}_{s}(_{s}(u_{b});_{g}, _{s})), \]

where \(_{s}\) denotes the self-supervised loss function, \(_{w}\) and \(_{s}\) denote the weak and strong augmentation functions respectively. In order to maintain consistent optimization between the main task

Figure 1: The pipeline of SSFA. Let \(x\), \(u_{w}\) and \(u_{s}\) denote a batch of the labeled data, the weak augmentation and the strong augmentation of unlabeled data respectively, \(\{\}\) represent the data stream.

and auxiliary task, it is necessary to optimize both tasks jointly. So we add \(_{aux}\) to semi-supervised learning module and the final object is:

\[_{}=_{x}+_{u}_{u}+_{a} _{aux}, \]

where \(_{u}\) and \(_{a}\) are hyper-parameters denoting the relative weights of \(_{u}\) and \(_{aux}\) respectively.

### Feature Adaptation Module

Some classic SSL approaches [30; 6; 5] directly use the outputs of the current classifier as pseudo-labels. In particular,  applies weak and strong augmentations to unlabeled samples and generates pseudo-labels using the model's predictions on weakly augmented unlabeled samples, _i.e._, \(q_{b}=p_{m}(y|_{w}(u_{b});_{g},_{c})\), where \(p_{m}(y|u;)\) denotes the predicted class distribution of the model \(\) given unlabeled image \(u\). However, as the classification model \((_{g},_{c})\) is mainly fitted to the labeled distribution, the prediction on \(_{w}(u_{b})\) is usually inaccurate under distribution mismatch between the labeled and unlabeled samples.

To alleviate this problem, we design a Feature Adaptation Module to adapt the model to the unlabeled data distribution before making predictions, thereby producing more reliable pseudo-labels for optimizing the unsupervised loss \(_{u}\). More specifically, before making pseudo-label predictions, we firstly fine-tune the shared feature extractor \(_{g}\) by minimizing the self-supervised auxiliary task loss on unlabeled samples:

\[_{apt}=_{b=1}^{ B}_{s}(_{w}(u_ {b});_{g},_{s}). \]

Here \(_{g}\) is updated to \(^{}_{g}=_{apt}\). Notably, since excessive adaptation may lead to deviation in the optimization direction and largely increase calculation costs, we only perform one-step optimization in the adaptation stage.

After self-supervised adaptation, we use \((^{}_{g},_{c})\) to generate the updated prediction, which can be denoted as:

\[q^{}_{b}=p_{m}(y|_{w}(u_{b});^{}_{g},_{c}). \]

We convert \(q^{}_{b}\) to the hard "one-hot" label \(^{}_{b}\) and denote \(^{}_{b}\) as the pseudo-label for the corresponding strongly augmented unlabeled sample \(_{w}(u_{b})\). After that, \(^{}_{g}\) will be discarded without influencing other model parts during training. In the end, \(_{u}\) in the SSL module (Equation 4) can then be computed with cross-entropy loss as:

\[_{u}=_{b=1}^{ B}((q^{}_{ b})>)(_{s}(u_{b}),^{}_{b};_{g}, _{c}). \]

### Theoretical Insights

In our framework, we jointly train the model with a self-supervised task with loss function \(_{s}\), and a main task with loss function \(_{m}\). Let \(h\) be a feasible hypothesis and \(D_{u}=\{(u_{i},y^{u}_{i})\}_{i=1}^{N}\) be the unlabeled dataset. Note that the ground-truth label \(y^{u}_{i}\) of \(u_{i}\) is actually unavailable, but just used for analysis.

**Lemma 1** (): _Assume that for all \(x,y\), \(_{m}(x,y;h)\) is differentiable, convex and \(\)-smooth in \(h\), and both \(\|_{m}(x,y;h)\|\), \(\|_{s}(x;h)\| G\) for all \(h\). With a fixed learning rate \(=}\), for every \(x\), \(y\) such that \(_{m}(x,y;h),_{s}(x;h)>\), we have_

\[_{m}(x,y;h^{})<_{m}(x,y;h), \]

_where \(h^{}\) is the updated hypothesis, namely \(h^{}=h-_{s}(x,y;h)\)._

Let \(_{m}(h,D_{u})=

we can get that:

\[_{m}(h^{},D_{u})<_{m}(h,D_{u}), \]

where \(h^{}\) is the updated hypothesis, namely \(h^{}=h-_{s}(h,D_{u})\).

Therefore, in the smooth and convex case, the empirical risk of the main task \(_{m}\) can theoretically tend to \(0\) by optimizing the empirical risk of self-supervised task \(_{s}\). Thus, in our feature adaptation module, by optimizing the self-supervised loss for unlabeled samples, we can indirectly optimize the main loss, thereby mitigating confirmation bias and making full use of the unlabeled samples. As above analysis, the gradient correlation plays a determining factor in the success of optimizing \(_{m}\) through \(_{s}\) in the smooth and convex case. For non-convex deep loss functions, we provide empirical evidence to show that our theoretical insights also hold. Figure 2 plots the correlation between the gradient inner product (of the main and auxiliary tasks) and the performance improvement of the model on the test set, where each point in the figure represents the average result of a set of test samples. In Figure 2, we observe that there is a positive correlation between the gradient inner product and model performance improvement for non-convex loss functions on the deep learning model. This phenomenon is consistent with the theoretical conclusion, that is, a stronger gradient correlation indicates a higher performance improvement.

## 5 Experiments

In this section, we evaluate our proposed SSFA framework on various datasets, where labeled and unlabeled data are sampled from different distributions. To this end, we consider two different distribution mismatch scenarios: image corruption and style change, in the following experiments. More experimental details and results are provided in the Appendix.

### Experimental Setting

**Datasets.** For the image corruption experiments, we create the labeled domain by sampling images from CIFAR100 Krizhevsky et al. (2009), and the unlabeled domain by sampling images from a mixed dataset consisting of CIFAR100 and CIFAR100 with Corruptions (CIFAR100-C). The CIFAR100-C is constructed by applying various corruptions to the original images in CIFAR100, following the methodology used for ImageNet-C Russakovsky et al. (2015). We use ten types of corruption as training corruptions while reserving the other five corruptions as unseen corruptions for testing. The proportion of unlabeled samples from CIFAR100-C was controlled by the hyper-parameter \(ratio\). For the style change experiments, we use OFFICE-31 Krizhevsky et al. (2009) and OFFICE-HOME Wang et al. (2016) benchmarks. Specifically, we designate one domain in each dataset as the labeled domain, while the unlabeled domain comprises either another domain or a mixture of multiple domains.

**Implementation Details.** In corruption experiments, we use WRN-28-8 Zhang et al. (2017) as the backbone except for Chen et al. (2018) where WRN-28-2 is used to prevent training collapse. In style experiments, we use ResNet-50 He et al. (2016) pre-trained on ImageNet Russakovsky et al. (2015) as the backbone for OFFICE-31 and OFFICE-HOME. To ensure fairness, we use the same hyperparameters for different methods employed in our experiments.

**Comparison Methods.** We compare our method with three groups of methods: (1) Supervised method as a baseline to show the effectiveness of training with unlabeled data.(2) Classical UDA methods including DANN Kingma and Ba (2015) and CDAN He et al. (2016); and (3) Popular SSL methods including MixMatch Chen et al. (2018), ReMixMatchChen et al. (2018), FixMatch Liu et al. (2019), FM-Rot (joint training with rotation prediction task), FM-Pre (pre-trained by rotation prediction tasks), FreeMatch Liu et al. (2019), SoftMatch Liu et al. (2019), and Adamatch Chen et al. (2018).

**Evaluation Protocols.** We evaluate the performance across labeled, unlabeled and unseen distributions to verify the general applicability of our framework. Therefore, we consider three evaluation protocols: (1) **Label-Domain Evaluation** (L): test samples are drawn from _labeled_ distribution, (2)

Figure 2: Scatter plot of the gradient inner product between the two tasks, and the improvement from SSFA. We transform the x-axis with \((x)+1\) for clarity.

**UnLabel-Domain Evaluation** (UL): test samples are drawn from _unlabeled_ distribution, and (3) **UnSeen-Domain Evaluation** (US): test samples are drawn from _unseen_ distribution.

### Main Results

**Image Corruption.** Our SSFA framework is a generic framework that can be easily integrated with existing pseudo-label-based SSL methods. In this work, we combine SSFA with four SSL methods: FixMatch, ReMixMatch, FreeMatch and SoftMatch, denoted by FM-SSFA, RM-SSFA, FreeMSSFA and SM-SSFA, respectively. Table 2 shows the compared results on the image corruption experiment with different numbers of labeled samples and \(ratio\). We can observe that (1) the UDA methods, DANN and CDAN, exhibit very poor performance on labeled-domain evaluation (L). This is because UDA methods aim to adapt the model to the unlabeled domain, which sacrifices performance on the labeled domain. And the UDA methods are primarily designed to adapt to a _single_ unlabeled distribution, making it unsuitable for FDM-SSL scenarios. In contrast, our methods largely outperform the two UDA methods on all evaluations. (2) The traditional SSL methods suffer from significant performance degradation in the FDM-SSL setting, particularly in unlabeled-domain evaluation (UL). And this degradation becomes more severe when the number of labeled data is small and the proportion of unlabeled data with corruption is high, due to the increased feature mismatch degree. Our methods largely outperform these SSL methods, validating the effectiveness of our self-supervised feature adaptation strategy for FDM-SSL. (3) SSFA largely improves the performance of original SSL methods on all three evaluations. The gains on unlabeled-domain evaluation (UL) indicate that SSFA can generate more accurate pseudo-labels for unlabeled samples and reduce confirmation bias. Moreover, the gains on unseen-domain evaluation (US) validate that SSFA can enhance the model's robustness and generalization.

**Style Change.** We conduct experiments on OFFICE-31 and OFFICE-HOME benchmarks to evaluate the impact of style change on SSL methods. Specifically, we evaluate SSL methods in two scenarios where unlabeled data is sampled from a single distribution and a mixed distribution. The results are summarized in Table 3. Compared to the supervised method, most SSL methods present significant performance degradation. One possible explanation is that style change causes an even greater feature distribution mismatch compared to image corruption, resulting in more erroneous predictions for unlabeled data. As shown in Table 3(a), our method shows consistent performance improvement over existing UDA and SSL methods on OFFICE-31. In some cases, such as "W/A", the task itself may be relatively simple, so using only label data can already achieve high accuracy. The results on OFFICE-HOME are summarized in Table 3(b). To demonstrate that our method can indeed improve the accuracy of pseudo-labels, we add an evaluation protocol **UU**, which indicates the _pseudo-labels accuracy_ of selected unlabeled samples. As shown in Table 3(b), compared to existing UDA and SSL methods, our approach achieves the best performance on all setups and significantly improves the accuracy of pseudo-labels on unlabeled data. The results provide experimental evidence that supports the theoretical analysis in Section 4.4.

    &  &  &  \\   &  &  &  &  &  \\   & L & UL & US & L & UL & US & L & UL & US & L & UL & US \\  Supervised & 10.6 & 8.9 & 6.9 & 10.6 & 8.9 & 6.9 & 48.0 & 17.0 & 24.0 & 61.6 & 20.8 & 30.8 \\ DANN  & 11.3 & 9.3 & 7.0 & 11.4 & 9.3 & 7.0 & 46.7 & 30.8 & 27.6 & 61.4 & 42.7 & 37.8 \\ CDAN  & 11.6 & 9.5 & 7.3 & 11.6 & 10.5 & 47.1 & 31.3 & 28.8 & 62.6 & 41.1 & 39.1 \\ MixMatch  & 10.7 & 36.5 & 15.4 & 15.2 & 6.1 & 46.4 & 10.9 & 21.1 & 60.3 & 31.7 & 37.6 \\ AdaMatch  & 6.8 & 4.6 & 1.3 & 6.0 & 4.5 & 2.1 & 19.1 & 6.4 & 1.8 & 26.7 & 9.0 & 1.6 \\  ReMixMatch  & 39.6 & 20.6 & 32.7 & 39.2 & 19.1 & 3.1 & 68.4 & 35.8 & 58.1 & 75.5 & 40.2 & 63.2 \\ RM-SSFA (ours) & **43.1** & **22.9** & **35.1** & **43.0** & **23.1** & **35.1** & **71.0** & **37.9** & **60.8** & **76.4** & **41.6** & **63.5** \\  FixMatch  & 25.8 & 4.7 & 16.5 & 15.7 & 3.5 & 8.5 & 53.0 & 16.0 & 39.0 & 65.3 & 33.0 & 52.8 \\ FM-SSFA (ours) & **37.0** & **23.2** & **25.8** & **25.7** & **22.2** & **22.5** & **60.2** & **52.5** & **51.2** & **69.1** & **57.8** & **58.0** \\  FreeMatch  & 35.0 & 14.1 & 21.0 & 17.4 & 2.7 & 2.5 & 55.2 & 16.3 & 41.4 & 67.0 & 23.9 & 53.4 \\ FreeMS-SFA (ours) & **37.6** & **25.1** & **31.4** & **27.5** & **18.7** & **21.9** & **62.0** & **54.2** & **53.2** & **70.2** & **61.6** & **60.0** \\  SoftMatch  & 35.5 & 2.9 & 24.1 & 19.4 & 4.6 & 12.8 & 58.3 & 29.7 & 48.9 & 68.3 & 34.7 & 56.3 \\ SM-SSFA (ours) & **40.8** & **29.5** & **34.8** & **31.1** & **22.4** & **25.2** & **62.6** & **54.4** & **53.8** & **70.3** & **61.8** & **59.9** \\   

Table 2: Comparison of accuracy (\(\%\)) for Feature Distribution Mismatch SSL on CIFAR100.

### Feature Visualization

Figure 3 visualizes the domain-level features generated by SSL models with/without SSFA respectively. In Figure 3 (a), the vanilla FixMatch model maps labeled and unlabeled samples to different clusters in the feature spaces without fusing samples from different domains. Conversely, Figure 3 (b) shows that our FM-SSFA model can effectively fuse these samples. We further compute the \(A\)-distance metric  to measure the distributional divergence between the labeled and unlabeled distributions. The \(A\)-distance of FM-SSFA (0.91) is lower than that of FixMatch (1.10), verifying the implicit distribution alignment of SSFA.

Additionally, Figure 4 visualizes the class-level features generated by SSL models with/without SSFA respectively. In Figure 4 (a), FixMatch fails to distinguish samples from different classes. In contrast, FM-SSFA can separate samples from different categories well, as shown in Figure 4 (b). This result indicates that SSFA can help to extract more discriminative features.

### Ablation Study

**Combined with different self-supervised tasks.** Table 4 compares different self-supervised tasks combined with our SSFA framework on the image corruption experiment. We employ different self-supervised losses corresponding to the rotation prediction task, contrastive learning task, and

Figure 4: Visualization of class-level features using different methods, where “label” represents the labeled data drawn from the labeled domain, and “unlabel0” to “unlabel9” represent the unlabeled data drawn from ten unlabeled domains respectively.

Figure 3: Visualization of domain-level features using different methods, where “label” represents the labeled data drawn from the labeled domain, and “unlabel0” to “unlabel9” represent the unlabeled data drawn from ten unlabeled domains respectively.

Table 3: Comparison of accuracy (\(\%\)) for Feature Distribution mismatch SSL on OFFICE-31 and OFFICE-HOME.

entropy minimization task, which are the cross entropy loss, the contrastive loss from SimCLR and the entropy loss, respectively. As shown, different self-supervised tasks can bring significant performance gains. And the rotation prediction task leads to the largest improvements compared to the other two self-supervised tasks. This can be attributed to the more optimal parameters and the direct supervision signals in rotation prediction. In the experiments, we employ the rotation prediction task as the default self-supervised task.

**The effectiveness of Feature Adaptation Module.** To evaluate the effectiveness of the Feature Adaptation Module in the SSFA framework, we compare FM-SSFA with a baseline method FM-Rot, where we remove the Feature Adaptation Module and only add the self-supervised rotation prediction task. As shown in Table 3(a) and 3(b), FM-SSFA largely outperforms FM-Rot on OFFICE-31 and OFFICE-HOME, especially in the UL evaluation metric. The superiority of FM-SSFA highlights that the Feature Adaptation Module helps the model to adapt to unlabeled samples from different distributions, resulting in better generalization on unlabeled domain. Moreover, we can observe the FM-Rot is only marginally better than FixMatch and, in some cases, may even perform worse. These results suggest that simply integrating the self-supervised task into SSL methods only brings limited performance gains and may even be detrimental in some scenarios.

**Small distribution shift between labeled and unlabeled data.** To demonstrate the robustness of our proposed method, we evaluate SSFA on scenarios where there is a small distribution shift between labeled and unlabeled data. Specially, we conduct experiments on two setups: \(ratio\)=0.0 (degrade into traditional SSL) and \(ratio\)=0.1. Table 5 shows our method can still bring significant improvements over the baseline. This indicates that our proposed SSFA framework is robust to various SSL scenarios even under a low noise ratio.

**Robustness to confident threshold \(\).** Figure 5 illustrates the impact of confident threshold \(\) for different SSL models on OFFICE-31. As shown in Figure 5, the vanilla FixMatch model is highly sensitive to the values of \(\). For example, when \(\) is set to 0.95, FixMatch achieves relatively high performance with confident enough pseudo-labels. When \(\) is lowered to 0.85, the performance of FixMatch on UL and UU degrades significantly, indicating that the model has deteriorated by too many wrong pseudo-labels. In contrast, FM-SSFA is more robust to different values of \(\).

**The number of shared layers between auxiliary and main task.** Table 6 analyzes the impact of different numbers of shared layers in the shared feature extractor of the auxiliary and main task. As shown, the performance difference is negligible between sharing 2 and 3 layers in the feature extractor, while a significant performance degradation happens if we share all layers (4 layers) of the feature extractor. We argue that this is because too many shared parameters of the feature extractor may lead to over-adaptation of the feature extractor and compromise of the main task, thus the predictions of

    &  &  &  \\   & \(ratio\) 0.5 & \(ratio\) 1.0 & \(ratio\) 1.0 & \(ratio\) 1.0 & \(ratio\) 1.0 \\   & L & UL & UL & UL & UL & UL & UL & UL \\  FixMatch & 25.8 & 4.7 & 16.5 & 15.7 & 3.5 & 8.5 & 53.0 & 16.0 & 39.0 & 65.3 & 33.0 & 52.8 \\ FM-SSFA (Rot) & **37.02** & **23.25** & **28.57** & **22.22** & **22.56** & **20.52** & **55.12** & **69.1** & **57.8** & **58.0** \\ FM-SSFA (SimClr) & 22.8 & 13.7 & 17.4 & 19.2 & 13.6 & 14.1 & 55.5 & 43.8 & 45.1 & 66.2 & 53.6 & 52.9 \\ FM-SSFA (EM) & 22.9 & 14.5 & 17.2 & 18.0 & 15.9 & 14.6 & 55.2 & 44.2 & 44.7 & 66.7 & 52.5 & 52.6 \\   

Table 4: Combined with different self-supervised tasks. “Rot”, “SimClr”, and “EM” represent the rotation prediction task, contrastive learning task  and entropy minimization task  respectively.

Figure 5: The impact of \(\) for different SSL models on OFFICE-31 (“A/W” task).

pseudo-labels may be more erroneous. In the experiments, we set the number of shared layers to 2 by default.

## 6 Conclusion

In this paper, we focus on a realistic SSL setting, FDM-SSL, involving a mismatch between the labeled and unlabeled distributions, complex mixed unlabeled distributions and widely unknown test distributions. The primary challenge lies in the scarcity of labeled data and the potential presence of mixed distributions within the unlabeled data. To address this challenge, we propose a generalized framework SSFA, which introduces a self-supervised task to adapt the feature extractor to the unlabeled distribution. By incorporating this self-supervised adaptation, the model can improve the accuracy of pseudo-labels to alleviate confirmation bias, thereby enhancing the generalization and robustness of the SSL model under distribution mismatch.

**Broader Impacts and Limitations.** The new problem setting takes into account the model's generalization across different distributions, which is essential for expanding the application of SSL methods in real-world scenarios. Additionally, SSFA serves as a simple yet effective framework that can be seamlessly integrated with any pseudo-label-based SSL methods, enhancing the overall performance and adaptability of these models. However, the performance of SSFA is affected by the shared parameters between the main task and the auxiliary task. We hope that SSFA can attract more future attention to explore the effectiveness of feature adaptation with the self-supervised task.