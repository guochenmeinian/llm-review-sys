# OW-VISCapTor: Abstractors for Open-World Video Instance Segmentation and Captioning

Anwesa Choudhuri, Girish Chowdhary and Alexander G. Schwing

University of Illinois at Urbana-Champaign

{anwesac2,girishc,aschwing}@illinois.edu

###### Abstract

We propose the new task 'open-world video instance segmentation _and_ captioning'. It requires to detect, segment, track _and_ describe with rich captions never before seen objects. This challenging task can be addressed by developing "abstractors" which connect a vision model and a language foundation model. Concretely, we connect a multi-scale visual feature extractor and a large language model (LLM) by developing an object abstractor and an object-to-text abstractor. The object abstractor, consisting of a prompt encoder and transformer blocks, introduces spatially-diverse open-world object queries to discover never before seen objects in videos. An inter-query contrastive loss further encourages the diversity of object queries. The object-to-text abstractor is augmented with masked cross-attention and acts as a bridge between the object queries and a frozen LLM to generate rich and descriptive object-centric captions for each detected object. Our generalized approach surpasses the baseline that jointly addresses the tasks of open-world video instance segmentation and dense video object captioning by \(13\%\) on never before seen objects, and by \(10\%\) on object-centric captions.

## 1 Introduction

We propose the generalized task of open-world video instance segmentation _and_ captioning (OW-VISCap). This task combines open-world video instance segmentation (OW-VIS)  and the generation of rich object-centric captions for objects . Specifically, OW-VISCap involves detecting, segmenting, and tracking previously seen or unseen objects in a video, while simultaneously generating free-form captions for each of the detected/segmented objects. The open-world aspect makes this task widely applicable. However, it also makes this task challenging because the objects are often never seen during training, are occasionally partly or entirely occluded, the appearance and position of these objects change over time, the objects may leave the scene only to re-appear at a later time, and because generating free-form object-centric captions requires a powerful object representation as well as a strong natural language understanding. Addressing these challenges to obtain an accurate method for this task that works online is crucial in fields like autonomous systems, and augmented as well as virtual reality, among others.

OW-VISCap generalizes open-world video instance segmentation (OW-VIS) which requires to deal with never-before-seen objects . For example, in Fig. 1, the trailer truck (top row) highlighted in yellow, and the lawn mower (bottom row) highlighted in green, are never seen before during training. Current OW-VIS methods suffer from the following two main issues.

Firstly, all methods on video instance segmentation, closed- or open-world, assign a one-word label to the segmented objects. However, we argue that free-form captions are more descriptive than discrete class labels , and naturally facilitate zero-shot fine-grained captioning of objects, especially in an open-world setting. Notably, vision-language models have not been explored to unify open-world video instance segmentation and spatio-temporally dense fine-grained object captioning.

Secondly, while classic OW-VIS methods  rely on region-based object proposals , more recent OW-VIS methods  develop an "abstractor" to generate object queries. Abstractors  are used to extract and project important information from one space to another depending on the task, e.g., abstractors connect pixel and language spaces in vision and language models , pixel and object spaces in object detection/segmentation networks , etc. However, OW-VIS abstracts suffer from spatial information loss because they primarily focus on a few spatial regions, leading to a loss of finer spatial details . For closed-world object detection/segmentation networks, this can be compensated through extensive supervision , but it is challenging to address this issue for never-before-seen objects, i.e., when targeting an open-world setting. One way of overcoming this issue is to use a prompt as additional input from the user, ground truth or another network. The prompts can be in the form of points, bounding boxes, or text. However, these methods only work when the additional inputs are available, making them less practical in the real world.

We study these issues in the newly proposed OW-VISCap task and propose the new baseline **OW-VISCapTor**, consisting of two **O**pen-**W**orld **V**ideo **I**nstance **S**egmentation and **C**aptioning abstract**Tor**s. Note that OW-VISCap requires a more holistic object understanding: object representations need to be expressive and capture information not only for detecting and segmenting previously seen or unseen objects, but also capture information for generating meaningful object-centric captions. This is an important step towards generalized scene understanding. OW-VISCapTor demonstrates this holistic object understanding by simultaneously detecting, segmenting and generating object-centric captions for objects in a video. Fig. 1 shows two examples in which our method successfully detects, segments, tracks and captions both closed- and open-world objects.

OW-VISCapTor addresses the first issue via an object-to-text abstractor which uses masked attention to project the object queries into text queries that can be interpreted by a frozen LLM to generate rich object-centric captions. OW-VISCapTor addresses the second issue via an object abstractor that projects image features into object queries. Spatial information is retained by finetuning a pretrained prompt encoder which is part of the object abstractor and forms open-world object queries from points distributed evenly across video frames.

Our approach improves upon a baseline that simply combines prior work: open-world video instance segmentation (OW-VIS)  and dense video captioning (Dense VOC) . Also note, there are no existing datasets directly collected for the generalized OW-VISCap task. Yet, OW-VIS and Dense VOC cover all aspects of OW-VISCap: open-world object discovery, video instance segmentation and dense object-centric captioning. Compared to the baseline, we improve results by \(13\%\) on the unseen categories for OW-VIS (BURST  data), and by \(10\%\) on the captioning accuracy

Figure 1: Our method, OW-VISCapTor, can simultaneously detect, segment, track, and caption objects in the given video frames. The first example (top row) shows a road scene with a previously unseen trailer truck and cars that are seen during training. The second example (bottom row) shows a person on a lawn mower and a dog on the grass. The lawn mower isn’t part of the training set. We generate meaningful object-centric captions even for objects never seen during training. The captions for unseen objects are underlined.

for Dense VOC (VidSTG  data). To further demonstrate the generalizability and efficacy of OW-VISCapTor, we compare our approach with the specialized state-of-the-art (SOTA) on OW-VIS and Dense VOC. Note that the OW-VIS SOTA can't be used for Dense VOC and vice-versa. Our generalized approach improves upon individual SOTA methods by \( 6\%\) on the previously unseen (uncommon) categories in the BURST  data, and by \( 7\%\) on the captioning accuracy for detected objects on the VidSTG  data. We also perform similar to the specialized state-of-the-art on the closed-world video instance segmentation task on the OVIS  data, demonstrating generalizability.

## 2 Related Work

### Abstractors as Versatile Projectors

**Abstractors in MLLMs.** Abstractors have been immensely successful in Multimodal Large Language Models (MLLMs), connecting frozen vision encoders and a frozen LLM. BLIP-2  successfully adapted LLMs to visual tasks, showing notable zero-shot generalization and in-context learning capabilities. More recently, Abstractors are used to enhance MLLMs through visual instruction tuning . However, to our best knowledge, abstracts have not been explored for fine-grained captioning of objects in videos in the open-world, which is of interest here.

**Abstractors for object discovery.** Object abstracts can generate powerful object queries, useful for closed-world object detection and segmentation in both images  and videos . More recently, abstracts have been used for open-world object discovery . However, they suffer from a spatial information loss because they primarily focus on a few spatial regions, leading to a loss of finer spatial details . This can be compensated through extensive supervision in the closed-world , but is challenging to address in the open-world for unseen objects. Prompt-based methods  can overcome this information loss, but use of prompts is often not realistic. In this work, we operate in a promptless open-world setting.

### Generalized Video Understanding Tasks

Recently, there has been progress in unifying different video related tasks. TubeFormer , Unicorn , and CAROQ  unify different video segmentation tasks in the closed world. DVOC-DS  unifies the tasks of detecting (but not segmenting) closed-world objects in videos and captioning those closed-world objects. In this work, we explore the task of detecting and segmenting both closed- and open-world objects in videos, and captioning these objects.

Video understanding often starts from a strong generalized image understanding. Some methods  unify different image segmentation methods, and provide a baseline for many different video understanding tasks . X-Decoder  unifies different image segmentation tasks along with the task of referring image segmentation. SAM  introduces a vision foundation model, that primarily performs prompt-based open-world image segmentation, and can be used for many

Figure 2: Overview of OW-VISCapTor (Sec. 3.1): an object abstractor (Sec. 3.2) connects the image feature space to the object query space, and an object-to-text abstractor (Sec. 3.3) connects the object query space to the text query space. DH and CH stand for detection head and classification head.

downstream tasks. Different from these works, we develop a generalized method for videos that tackles segmentation and object-centric captioning for both open- and closed-world objects.

### Specialized Video Understanding Tasks

We briefly review OW-VIS, Dense VOC and VIS, and detail these tasks in Appendix A.

**Open-world video instance segmentation.** OW-VIS methods can be categorized into prompt-less methods [5; 4; 1; 7; 3] that either operate on classic region-based object proposals or suffer from spatial information loss; or prompt-based methods that use prompts in the form of masks [29; 30; 31; 32; 33; 34; 35; 36; 37], words , points , etc. We operate in a prompt-less setting, but spatially enrich an object-abstractor to generate open-world object queries.

**Dense video object captioning.** DVOC-DS  performs object-centric captioning of closed-world objects, but cannot caption multiple action segments or process long videos like many other video models [38; 39; 40]. Unlike DVOC-DS , we operate in the open-world, leverage masked attention for dense video object captioning and can address the aforementioned drawbacks.

**Closed-world video instance segmentation.** Methods for closed-world VIS either rely on classical region-based object proposals [41; 20; 42; 43; 44; 45; 46; 47; 48; 49; 50], or on abstractor-based object-queries [51; 16; 17; 18; 15; 52; 53]. Differently, in this work, we explore abstractors to generate both closed- and open-world query-based proposals.

## 3 Abstractors for Open-World Video Instance Segmentation and Captioning

We propose to jointly address the tasks of 1) open-world video instance segmentation and 2) object-centric captioning: given a video, we jointly detect, segment, track and caption object instances in a video. Importantly, object instance categories may not be part of the training set (e.g., the trailer truck in Fig. 1 (top row)), placing our goal in an open-world setting. To achieve this goal, we develop an approach which first breaks a given video into short clips, each consisting of \(T\) frames. Each clip is processed using our OW-VISCapTor. We discuss merging of the results of each clip in Appendix C.

We provide an overview of OW-VISCapTor in Sec. 3.1. We then discuss our contributions: (a) the object abstractor that generates spatially-rich open-world object queries (Sec. 3.2), along with our proposed inter-query contrastive loss, and (b) the object-to-text abstractor that uses masked cross-attention for object-centric captioning (Sec. 3.3). We discuss the final training objective in Sec. 3.4.

### Overview

Fig. 2 provides an overview of our OW-VISCapTor method. OW-VISCapTor consists of two abstractors: an object abstractor (Sec. 3.2 and Fig. 3 (a)) and an object-to-te

Figure 3: The proposed abstractors. (a) The **object abstractor** generates spatially rich open-world object queries \(q_{}\) from open-world embeddings \(e_{}\), and closed-world object queries \(q_{}\) from closed-world embeddings \(e_{}\). The open-world embeddings \(e_{}\) are generated by encoding a grid of points via a prompt encoder. The closed-world embeddings are learnt. (b) The **object-to-text abstractor** generates the object-centric text queries (e.g., \(q^{i}_{}\) for the \(i^{}\) object) that the frozen LLM uses for object-centric captioning. There are \(L\) transformer blocks in the object-to-text abstractor, each one consisting of self-attention (SA), masked cross-attention (Masked CA), and a feed forward network (FFN).

Fig. 3 (b)). The object abstractor connects the visual space of image features (\(^{HWT C}\)) to the space of object queries (\(^{N_{} C}\)). Here, \(N_{}=N_{}+N_{}\) is the total number of object queries, which includes the total number of open-world and closed-world queries, \(N_{}\) and \(N_{}\). \(C\) refers to the channel dimension for each object query and image feature. \(H\), \(W\) and \(T\) refer to the height, width and clip-length. The object-to-text abstractor connects the space of object queries (\(^{N_{} C}\)) and text queries (\(^{(N_{}+1) C}\)), to generate fine-grained object-centric captions. Here, \(N_{}\) is the total number of text queries which are concatenated with individual object queries (hence the "\(+1\)") to generate text queries for each object.

To deal with the open-world setting, i.e., to ensure that we can detect, segment, track and caption never before seen object instances, the object abstractor generates spatially rich open-world object queries, \(q_{}^{N_{} C}\). These open-world object queries are in addition to closed-world object queries \(q_{}^{N_{} C}\), commonly used in prior work [15; 13; 18; 16; 17]. The open-world object queries are enriched spatially by a prompt encoder (shown in Fig. 3 (a)) that encodes a grid of points into prompt representations. Note, by using open-world object queries, we discover new and diverse open-world objects without needing additional prompts.

We use \(q_{}=[q_{},q_{}]\) to denote all the object queries obtained from the object abstractor. The \(i^{}\) object query \(q^{i}_{}^{1 C}\) is concatenated with learnt text embeddings \(e_{}^{N_{} C}\). The concatenated object query and text embeddings are continuously modulated in the object-to-text abstractor by combining them with image features so as to generate meaningful text queries \(q^{i}_{}^{(N_{}+1) C}\) for the \(i^{}\) object. This is shown in Fig. 3 (b). Note, \(q^{i}_{}\) is used by the frozen LLM to generate object-centric captions. The segmentation mask for the \(i^{}\) object generated in the detection head is used to mask the attention in the object-to-text abstractor, as described in Sec. 3.3. We find this design to enable the LLM to generate more object-centric captions.

Both open- and closed-world object queries are processed by our object-to-text abstractor and LLM which yields an object-centric caption, and a detection head (DH in Fig. 2) which yields either a segmentation mask or a bounding-box. The closed-world object queries are further processed by a classification head (CH in Fig. 2) which yields a category label.

### Object Abstractor

The object abstractor is detailed in Fig. 3 (a). It consists of a prompt encoder, a transformer decoder and closed-world trainable embeddings \(e_{}\). The closed-world embeddings \(e_{}\) are modulated in the transformer decoder to generate the closed-world object queries \(q_{}\). We discuss the generation of open-world object queries next.

**Spatially-rich open-world object queries.** To help discover new objects, the object abstractor introduces open-world object queries \(q_{}\) in addition to the commonly used closed world object queries. Our open-world object queries are generated from open-world embeddings \(e_{}\), which are continuously modulated in the transformer decoder by combining them with image features via masked attention, following [13; 15].

**Prompt encoder.** The open-world embeddings \(e_{}\) are generated in the prompt encoder. An illustration is provided in Fig. 3 (a). We encode a grid of equally spaced points along the height and width of the frames of the clip, using the prompt encoder employed in SAM . The use of equally spaced points encourages the open-world object queries to focus on different regions of the video frames, making them spatially rich and encouraging object discovery throughout the frames. This also encourages the open-world object queries to be diverse from one another.

**Inter-query contrastive loss.** We introduce an inter-query contrastive loss \(_{}\) to ensure that the object queries are different from each other. For closed-world objects, this loss helps in removing highly overlapping false positives. For open-world objects, it helps in the discovery of new objects. Formally,

\[_{}=-_{i,j}L_{1}(q^{i}_{},q^{j}_{ }), \]

where \(q^{i}_{}\) and \(q^{j}_{}\) are the \(i^{}\) and the \(j^{}\) objects and \(i j\). \(L_{1}\) refers to the L1 distance. Via this loss, we _maximize_ the L1 distance between the object queries, i.e., we encourage that the object queries differ from each other.

### Object-To-Text Abstractor

The object-to-text abstractor (Fig. 3 (b)) connects the space of object queries and the space of text queries. It consists of \(L\) transformer blocks, each block consisting of self-attention (SA), masked cross-attention (masked CA) and a feed forward network (FFN) as shown in Fig. 3 (b) to generate object-centric text queries. Next, we discuss masked cross-attention.

**Masked cross-attention for object-centric captioning.** Masked cross-attention involves attending within the foreground region of the predicted mask for each object query. Concretely, to generate object-centric text queries \(q^{i}_{ text}\) for each object \(i\), we restrict the cross attention in the object-to-text abstractor by using the segmentation mask of the object generated by the detection head. Intuitively, this enables the model to focus on local object-centric features, which are sufficient to update the text queries. Importantly, note that context information from the video frames can be gathered through the self-attention layers. The proposed design hence doesn't take away any information. It rather provides the same information in clearly separated layers.

Formally, for the \(i^{ th}\) object we compute the query features \(X^{ cap,i}_{l}^{(N_{ text}+1) C}\) obtained from the \(l^{ th}\) object-to-text transformer layer via

\[X^{ cap,i}_{l}={ softmax}(^{i}+Q^{i}_{l}K^{T}_{l})V_{l}+X^{  cap,i}_{l-1}. \]

Here, \(^{i}\{0,-\}^{1 HWT}\) is the attention mask in the object-to-text extractor such that at feature location \((x,y)\), \(^{i}(x,y)=0\), if \(M^{i}(x,y)=1\) and \(^{i}(x,y)=-\), if \(M^{i}(x,y)=0\). \(M^{i}\) is the binary mask obtained from the detection head. Moreover, \(K_{l}\), \(V_{l}^{HWT C}\) are the linearly transformed image features. To initialize, we let \(X^{ cap,i}_{0}=[q^{i}_{ obj},e_{ text}]\), where \(q^{i}_{ obj}^{1 C}\) is the \(i^{ th}\) object query obtained from the object abstractor and \(e_{ text}^{N_{ text} C}\) are the learnt text embeddings shown in Fig. 2 and introduced in Sec. 3.1.

### Training

Our training objective is

\[_{ total}=_{ cont}+_{ cap}+_{ cw}+_{ ow}.\]

Here, \(_{ cont}\) is the inter-query contrastive loss discussed in Sec. 3.2. \(_{ cap}\) is the standard captioning loss introduced by DVOC-DS . \(_{ cw}\) is the closed-world loss following prior work . To compute \(_{ cw}\), the ground truth objects are first matched with the predicted closed-world objects; the optimal matching is used to compute the final closed-world loss \(_{ cw}\). This loss consists of a detection/segmentation loss and a cross-entropy loss for predicting the closed-world object categories. \(_{ ow}\) is the open-world loss, which consists of only a detection/segmentation loss, unlike \(_{ cw}\). The open-world loss is detailed in Appendix B. Note, that the training data consists of only closed-world objects. We match the closed-world ground truth objects twice, once with the predicted open-world objects to compute \(_{ ow}\), and once with the predicted closed-world objects to compute \(_{ cw}\). We train the object abstractor, the object-to-text abstractor, and the image-feature extractor that generates the image-features as illustrated in Fig. 2. The parameters of the LLM are frozen.

## 4 Experiments

We evaluate OW-VISCapTor on the diverse tasks of open-world video instance segmentation (OW-VIS) and dense video object captioning (Dense VOC). Note that there is no dedicated dataset for our proposed task of open-world video instance segmentation _and_ captioning (OW-VISCap). Hence we use the two aforementioned tasks and evaluate the three different aspects of our approach: open-world capability, video instance segmentation and video object captioning. In the following subsections, we first discuss the datasets and evaluation metrics used in our evaluation (Sec. 4.1). We then compare our performance to a baseline that jointly addresses OW-VIS and Dense VOC, and also to specialized methods that address these two tasks individually (Sec. 4.2). We demonstrate how our contributions result in better performance through an ablation study (Sec. 4.3). Finally, we show qualitative results (Sec. 4.4). In Appendix D, we also show results on closed-world VIS.

### Datasets and Evaluation Metrics

We evaluate our approach on the OW-VIS and Dense VOC tasks. For OW-VIS, we use the BURST dataset . For Dense VOC, we use the VidSTG dataset . Note, VidSTG  has bounding box and tracking identity for all objects, but captions are not exhaustively provided. Following DVOC-DS  the captioning loss for missing captions is removed during training and data with missing captions isn't evaluated at test time.

For OW-VIS, we use the standard evaluation metrics of open-world tracking accuracy (OWTA)  for all, common (seen) and uncommon (unseen) categories. For Dense VOC, we use the captioned higher order tracking accuracy (CHOTA) , which depends on the detection accuracy (DetA), association accuracy (AssA), and captioning accuracy (CapA). We also report the frame-based METEOR score (APM).

### Main Results

We compare OW-VISCapTor with a generalized baseline, as well as specialized SOTA methods on the tasks of OW-VIS and Dense VOC. The results are summarized in Tab. 1. Even when compared to specialized SOTA on individual tasks, our method is able to achieve the best results (highlighted in bold) or the second-best results (underlined). Tab. 1 (left) shows results for the OW-VIS task on the BURST dataset . We report the open-world tracking accuracy for all, common (seen) and uncommon (unseen) categories. Tab. 1(right) shows results on the Dense VOC task.

**Comparison with generalized baseline.** We create a generalized baseline (Mask2Former  + DEVA  + BLIP2 ) that is able to address the tasks of OW-VIS and Dense-VOC jointly. The generalized baseline consists of Mask2Former  trained on the VidSTG  dataset for object detection and integrated with DEVA  for tracking. The per-frame predictions are first enlarged by \(10\%\) to provide more overall image context. The enlarged bounding boxes are then used to crop the images for captioning using BLIP-2 . Note that Mask2Former +DEVA  is a specialized previous SOTA on the OW-VIS task.

    & &  &  \\  Method & Mode & Unseen & Overall & Seen & CapA & CHOTA & DetA & AssA & APM \\   OWTB  & onl. & 38.8 & 55.8 & 59.8 & - & - & - & - & - \\ Mask2Former +STCN  & onl. & 25.0 & 64.6 & 71.0 & - & - & - & - & - \\ Mask2Former +DEVA  & onl. & 42.3 & **69.5** & **74.6** & - & - & - & - & - \\ EntitySeg +DEVA  & onl. & 49.6 & 68.8 & 72.7 & - & - & - & - & - \\  DVOC-DS (joint training)  & off. & - & - & - & - & 36.8 & 51.6 & **65.5** & **56.9** & **69.3** \\ DVOC-DS (disjoint training)  & off. & - & - & - & - & 10.0 & 28.0 & 45.9 & 48.0 & 39.8 \\   Mask2Former +DEVA +BLIP2  & onl. & 42.3 & **69.5** & **74.6** & 34.0 & 48.5 & 59.6 & 56.4 & 60.1 \\ _OW-VISCapTor+CARQQ  (online)_ & _onl._ & _50.0_ & _66.1_ & _63.0_ & _43.9_ & _53.1_ & _60.1_ & _54.0_ & _62.6_ \\ _OW-VISCapTor+DEVA  (online)_ & _onl._ & _**55.2** & _69.0_ & _73.5_ & _40.1_ & _51.7_ & _60.0_ & _56.3_ & _63.0_ \\   

Table 1: Results on OW-VIS (left) and Dense VOC (right). Onl. refers to online frame-by-frame processing. The columns highlighted in blue (OWTA for Unseen categories in OW-VIS and CapA for Dense VOC) highlight the ‘open-world’ and ‘captioning’ capabilities of different methods. The best scores are highlighted in bold font, and the second-best scores are underlined.

  
**Method** &  \\   & Overall & Seen & Unseen \\  Ours & **55.5** & **58.2** & **43.8** \\ w/o p.e. & 54.2 & 57.7 & 41.2 \\ w/o \(L_{}\) & 53.5 & 56.1 & 41.6 \\   

Table 2: Ablation on the BURST  validation data. ‘w/o p.e.’ refers to without prompt encoder; ‘w/o \(L_{}\)’ refers to without contrastive loss.

  
**Method** & **CHOTA** & **DetA** & **AssA** & **CapA** \\  Ours & **51.0** & 56.1 & 54.0 & **43.9** \\ w/o m.a. & 39.5 & 56.1 & 54.0 & 20.3 \\ bb. cap. & 48.1 & 56.1 & 54.0 & 36.6 \\ en. bb. cap. & 49.2 & 56.1 & 54.0 & 39.4 \\   

Table 3: Ablation on the VidSTG  data. ‘w/o m.a.’ refers to without masked attention. ‘bb. cap.’ and ‘en. bb. cap.’ refers to bounding box captioning and enlarged bounding box captioning.

OW-VISCapTor outperforms Mask2Former +DEVA +BLIP2  consistently across both datasets on unseen categories and object-captioning, demonstrating that our object queries are expressive enough to simultaneously detect, segment, track and caption seen or unseen objects. Mask2Former +DEVA +BLIP2 , in-spite of being SOTA on the previously seen object categories, struggles on the Dense VOC task. This is due to the lack of overall image-based context for captioning.

**Comparison with specialized SOTA.** OW-VISCapTor, integrated with DEVA  for temporal association of objects, achieves the state-of-the-art on the uncommon object categories (Tab. 1 (left)), improving upon the next best method (EntitySeg +DEVA ) by \( 6\) points on the BURST  validation data. For the common categories, our method ranks \(2^{}\) in the BURST validation data. We use a SwinL  backbone, and a clip-length of \(T=1\) in this setting.

Our method, when integrated with CAROQ  or DEVA  for temporal association, outperforms DVOS-DS  on the captioning accuracy (CapA), demonstrating that our object-to-text abstractor with masked cross-attention (Sec. 3.3) is effective in generating object-centric captions. We improve

Figure 4: Example from the BURST validation data. The masks are superimposed on the objects. The top row shows examples of parachutes in the air and people on the grass. The parachutes belong to the uncommon object category, i.e., parachutes were never seen during training. Our approach detects and retains the identities of the blue and the green parachutes as the green parachute crosses the blue one. The bottom row shows a person unboxing a leaf blower. The carton of the leaf blower (gray mask), the leaf blower (maroon mask), and the plastic wrapper (pink mask) are never seen during training. We can consistently detect, segment, and track them along with the person (common object category during training).

Figure 5: An example from the VidSTG data. Our approach is able to detect and track objects in the scene consistently and to generate meaningful object-centric captions for each of the detected objects.

upon DS-VOC on the overall CHOTA metric, even though we slightly underperform on DetA and AssA. Note that DVOS-DS is an offline method: the entire object trajectories are used for generating the captions. Hence DVOS-DS cannot process videos with more than \(200\) frames. This is in contrast to our online method, where we sequentially process short video clips (of length \(T=2\) for CAROQ  and of length \(T=1\) for DEVA ). DVOS-DS uses a ViT  backbone, whereas we use SwinL , which leads to a difference in DetA scores. We provide additional details on the merging of video clips in Appendix C.

Note that, even though the OW-VISCap task focuses on generalizability, OW-VISCapTor outperforms specialized methods on the **open-world** metrics and the **captioning** metrics, demonstrating the effectiveness of our contributions: the object abstractor that processes our novel open-world object queries, and the novel object-to-text abstractor that generates rich object-centric captions.

### Ablation Studies

**Spatially-rich open-world object queries.** Tab. 2 (first and second row) shows that the spatially-rich open-world object queries \(q_{}\), described in Sec. 3.2, help in discovering new objects. In Tab. 2, 'w/o p.e.' refers to the setting without the prompt encoder encoding spatial prompts into the open-world embeddings \(e_{}\). The open-world embeddings \(e_{}\) are trained like the closed-world embeddings \(e_{}\). We observe that the performance drops by \(2.6\) points for uncommon categories compared to 'Ours', even though the number of object queries are exactly the same in both settings. This highlights that the object abstractor suffers from spatial information loss if not augmented with a prompt encoder to encode spatial points.

**Contrastive loss.** Tab. 2 (first and last row) shows that the contrastive loss \(_{}\), described in Sec. 3.2, helps in detecting both the common (seen) and uncommon (unseen) categories of objects. The performance drops by \( 2\) points for both the common and uncommon categories for the setting 'w/o \(_{}\)', i.e., when the contrastive loss is not used. The contrastive loss helps in removing highly overlapping false positives in the closed-world setting and in discovering new objects in the open-world setting.

**Masked attention in object-to-text abstractor.** Tab. 3 shows that masked attention in the object-to-text abstractor, described in Sec. 3.3, helps in object-centric captioning. The second row 'w/o m.a.' of Tab. 3 refers to the setting without masked attention, i.e., the entire image-feature is used to calculate the cross-attention in the object-to-text abstractor. The object-centric context is only accumulated by concatenating the \(i^{}\) object query with the learnt text embeddings, as discussed in Sec. 3.3 and shown in Fig. 2. We observe that the captioning accuracy CapA drops by \(23\) points, indicating that concatenating the object query with the text embeddings is not sufficient for an object-centric focus. The third row in Tab. 3, 'bb. cap.' (bounding box captioning), pursues the opposite setting. Here, the images are cropped based on the object bounding box predictions in the detection head. The cropped images are directly used for captioning, ensuring that both the self- and cross-attention blocks in the object-to-text transformer operate on object-centric features. Note, that we don't use masked attention in this setting. We observe a drop in CapA of \(5\) points. Although cropping helps in retaining the object-centric information, the overall context from the entire image is missing. The fourth row in Tab. 3, 'en. bb. cap.' (enlarged bounding box captioning), shows a similar setting as the third row, but the bounding boxes are first enlarged by \(10\%\) to provide more overall image context. The enlarged bounding boxes are then used to crop the images for captioning. We observe a drop in CapA of \(3\) points, indicating that enlarging the bounding boxes helps but is not sufficient to provide overall context. This is also highlighted in the third-last row and the last row in Tab. 1.

### Qualitative Results

Fig. 1 shows results on the BURST  validation data. OW-VISCap is able to simultaneously detect, segment, track and caption objects. The objects belong to both the open- and closed-world. Note that the BURST  data doesn't provide object-centric captions for training, hence our object-to-text abstractor was not trained on BURST  but only on VidSTG. We find this object-to-text abstractor to be effective in generating meaningful object-centric captions even for objects never seen during training. Fig. 4 shows two examples from the BURST validation data. We can consistently detect, segment, and track previously seen and unseen objects. Fig. 5 shows an example from the VidSTG  data. Our method can detect, track and caption objects. Additionally, we discuss some failure modes of our method in Appendix G.

## 5 Conclusion

We introduce OW-VISCapTor: two abstractors to _jointly detect, segment, track, and caption previously seen or unseen objects in videos_. The developed object abstractor generates spatially-rich open-world object queries which encourage discovery of previously unseen objects without the need of additional user-input. Instead of assigning a fixed label to detected objects, we introduce an object-to-text abstractor that uses masked cross-attention to generate rich object-centric captions for each object. **Societal Impact**. Our method can be used to segment and describe never before seen objects. This capability could be beneficial in assistive technologies for the blind, as well as in AR/VR. Although we don't see any direct ethical concerns, research in this direction makes video-processing technology increasingly accessible. This could encourage and increase malicious use and potentially create issues regarding unethical surveillance and privacy threats. This calls for stricter security measures both at a personal level (like password protecting video data), and societal level (like regulating open-source dataset and model releases).

## 6 Acknowledgements

This work is supported in party by the Agriculture and Food Research Initiative (AFRI) grant no. 2020-67021-32799/project accession no. 1024178 from the USDA National Institute of Food and Agriculture: NSF/USDA National AI Institute: AIFARMS. We also thank the Illinois Center for Digital Agriculture for seed funding for this project. Work is also supported in part by NSF under grants 2008387, 2045586, 2106825, MRI 1725729.