# High-Resolution Image Harmonization with Adaptive-Interval Color Transformation

Quanling Meng\({}^{1}\), Qinglin Liu\({}^{1}\), Zonglin Li\({}^{1}\), Xiangyuan Lan\({}^{2}\),

**Shengping Zhang\({}^{1,2,}\)1, Liqiang Nie\({}^{1}\) \({}^{1}\)School of Computer Science and Technology, Harbin Institute of Technology, China**

\({}^{2}\)Peng Cheng Laboratory, China

quanling.meng@hit.edu.cn, qinglin.liu@outlook.com, zonglin.li@hit.edu.cn

lanxy@pcl.ac.cn, s.zhang@hit.edu.cn, nieliqiang@gmail.com

###### Abstract

Existing high-resolution image harmonization methods typically rely on global color adjustments or the upsampling of parameter maps. However, these methods ignore local variations, leading to inharmonious appearances. To address this problem, we propose an Adaptive-Interval Color Transformation method (AICT), which predicts pixel-wise color transformations and adaptively adjusts the sampling interval to model local non-linearities of the color transformation at high resolution. Specifically, a parameter network is first designed to generate multiple position-dependent 3-dimensional lookup tables (3D LUTs), which use the color and position of each pixel to perform pixel-wise color transformations. Then, to enhance local variations adaptively, we separate a color transform into a cascade of sub-transformations using two 3D LUTs to achieve the non-uniform sampling intervals of the color transform. Finally, a global consistent weight learning method is proposed to predict an image-level weight for each color transform, utilizing global information to enhance the overall harmony. Extensive experiments demonstrate that our AICT achieves state-of-the-art performance with a lightweight architecture. The code is available at [https://github.com/aipixel/AICT](https://github.com/aipixel/AICT).

## 1 Introduction

Image composition  aims to combine a foreground object with a background image to create a realistic composite, which holds significant potential across various domains, including art, entertainment, commerce , and data augmentation . However, since the foreground and background may be captured under different conditions, directly pasting the foreground onto the background usually results in an inconsistent appearance. To address this problem, image harmonization endeavors to adjust the color of the foreground to seamlessly integrate with the background, which plays a pivotal role in image editing.

Traditional image harmonization methods  primarily concentrate on aligning the color statistics of the foreground to match the background using hand-crafted features. Since

Figure 1: Our method predicts pixel-wise color transformations at low resolution and adaptively the adjusts sampling interval to model local non-linearities of the color transform at high resolution.

these methods lack consideration for the content of the composite images, they often yield suboptimal results when there are substantial differences in appearance between the foreground and background. With the rapid advance of deep learning, learning-based methods  have become dominant and achieved remarkable progress. These methods usually adopt encoder-decoder based structures to learn the dense pixel-to-pixel transformation between composite images and ground-truth images at a low resolution (e.g., \(256 256\) pixels), while real-world applications increasingly demand high-resolution images. Although these methods can process images with any size theoretically, the computational cost required for high-resolution images is extremely expensive.

Recently, several methods  have emerged to tackle the challenge of high-resolution image harmonization. To reduce computational costs, these methods usually use a low-resolution composite image to predict transformation parameters for processing the corresponding high-resolution composite image instead of directly generating the final image. These methods can be mainly categorized into two groups. Harmonizer  and S\({}^{2}\)CRNet  focus on predicting image-level parameters to perform global color adjustments. However, these adjustments do not contain any semantic and local information, which leads to identical changes for pixels in different regions with the same color value. On the other hand, DCCF  and PCT-Net  predict low-resolution parameter maps and then directly upsample them to align with high-resolution composite images for pixel-wise color transformation. However, upsampling low-resolution parameter maps may introduce errors, which fails to model local non-linearities of the color transform at high resolution. In summary, these four methods ignore local color transformations across the foreground regions, which are prone to generate in harmony results in local regions.

To address this problem, we propose an Adaptive-Interval Color Transformation method (AICT) for high-resolution image harmonization. As shown in Figure 1, AICT predicts parameter maps for pixel-wise color transformations, rather than performing global color adjustments. Additionally, it adaptively adjusts the sampling interval to model local non-linearities of the color transformation at high resolution. To implement this complex transformation, we formulate the task as an image-based multiple curve estimation problem. Specifically, a parameter network is first proposed to generate multiple curves as position-dependent 3-dimensional lookup tables (3D LUTs), which use the color and position of each pixel to perform pixel-wise color transformations. Then, to enhance local variations adaptively, we propose an adaptive interval learning method, which separates a color transform into a cascade of sub-transformations using two position-dependent 3D LUTs to achieve the non-uniform sampling intervals of the color transform. Finally, a global consistent weight learning method is proposed to predict an image-level weight for each color transform, utilizing global information to enhance the overall harmony by modeling the influence of background brightness on the foreground. As shown in Figure 2, AICT achieves state-of-the-art performance in foreground-normalized MSE (fMSE) on the full-resolution images of the iHarmony4 dataset  while maintaining a model size comparable to PCT-Net . Our main contributions are as follows:

* We propose an Adaptive-Interval Color Transformation method to perform pixel-wise color transformations and model local non-linearities of the color transformation for high-resolution image harmonization.
* We propose an adaptive interval learning method to achieve a flexible sampling point allocation and a global consistent weight method to utilize global information to enhance the overall harmony.

Figure 2: Model size vs. performance (fMSE score) comparison on the full-resolution images of the iHarmony4 dataset . The proposed method achieves state-of-the-art performance while maintaining a lightweight architecture.

* Extensive experiments demonstrate that the proposed method achieves state-of-the-art performance in high-resolution image harmonization while maintaining a lightweight and simple network architecture.

## 2 Related Work

### Image Harmonization

To reduce computational costs, existing high-resolution image harmonization methods [20; 21; 46; 12; 7; 40] usually takes a low-resolution image as input to predict transformation parameters for processing the corresponding high-resolution image instead of directly outputting the final image. For example, Harmonizer  employs a neural network to regress filter arguments based on low-resolution images, which are then used for several white-box filters to adjust various aspects of the foreground, including brightness, contrast, and other characteristics. S\({}^{2}\)CRNet  focuses on extracting spatial-separated embeddings from low-resolution images to predict parameters of the piece-wise curve mapping for performing color-wise transformations. Similarly, Wang et al.  utilize down-sampled images to predict global RGB curves for performing color correction at higher resolutions. Furthermore, they propose to predict and upsample low-resolution shading maps to address local tonal variations. These three methods predict image-level parameters to perform global color adjustments. Unlike these methods, DCCF  processes low-resolution images to acquire human comprehensible neural filter maps, which are subsequently upsampled and applied to the original input image. PCT-Net  predicts affine transformation parameter maps based on low-resolution images and upsamples them to match high-resolution images. Both methods predict low-resolution parameter maps and then upsample them directly to align with high-resolution images for pixel-wise transformations. In addition, CDTNet  performs pixel-to-pixel transformation at low resolution and color-to-color transformation at high resolution in parallel. It subsequently utilizes a refinement module to integrate the two intermediate outputs. The above methods typically rely on global color adjustments or the upsampling of parameter maps, which ignore local variations. Our method performs pixel-wise color transformations and models local non-linearities for high-resolution image harmonization.

### LUT-based Image Enhancement

A lookup table (LUT) defines a set of tables consisting of output values, where each value can be addressed by a set of indices. When provided with an input combination, it can output a corresponding value by performing lookup and interpolation operations, so it usually serves as an effective and efficient representation of a univariate or multivariate function. Zeng et al.  propose to learn multiple basis 3D LUTs and predict content-dependent weights to fuse them into an image-adaptive LUT for photo enhancement. To consider global scenarios and local spatial information, Wang et al.  introduce a lightweight two-head weight predictor for image-level scenario adaptation and pixel-wise category fusion. Yang et al.  achieve a more flexible sampling point allocation in 3D LUTs by adaptively learning the non-uniform sampling intervals in the 3D color space. To enhance the expressiveness of the LUT, Yang et al.  decompose a single color transform into a cascade of sub-transforms and use 1D LUTs to increase cell utilization within the 3D LUT. Zhang et al.  employ hash techniques to reduce the space complexity of 3D LUTs. Liu et al.  propose to learn a context map for the pixel-level category and a group of image-adaptive coefficients for achieving context-aware 4D LUT. These methods typically predict several weight parameters to fuse pre-trained LUTs for global RGB-to-RGB transformations, while our method predicts position-dependent 3D LUTs for pixel-wise color transformations.

## 3 Method

Given a composite image \(^{H W 3}\) and a binary mask \(M\{0,1\}^{H W}\) that indicates the foreground region to be harmonized, image harmonization aims to adjust the color of the foreground region to obtain a harmonized image \(^{H W 3}\) close to the ground truth image \(I^{H W 3}\). To achieve high-resolution image harmonization at a low computational cost, we formulate the task as an image-based multiple curve estimation problem and propose an Adaptive-Interval Color Transformation method (AICT) as shown in Figure 3. AICT consists of a high resolution (HR)branch and a low resolution (LR) branch. In the LR branch, \(\) and \(M\) are first downsampled to obtain a low-resolution composite image \(_{}^{H^{} W^{} 3}\) and foreground mask \(M_{}\{0,1\}^{H^{} W^{}}\), where \(H^{}<H\) and \(W^{}<W\). They are then fed into a parameter network to predict two parameter maps \(C^{H^{} W^{} Q}\) and \(F^{H^{} W^{} Q}\) and they are regarded as curves in the form of position-dependent 3D LUTs, where \(Q\) is equal to three times the number of knot points in a curve. In the HR branch, \(C\) is used to process color values and pixel coordinates to redistribute the color values of \(\) into specific ranges of the following \(F\) for achieving pixel-wise adaptive adjustment of the sampling interval. \(F\) is used to map the redistributed color values and pixel coordinates to final color values, achieving pixel-wise color transformation. Therefore, the color transformation in our method includes two cascaded pixel-wise sub-transformations.

### Adaptive Interval Learning

In the LR branch, \(_{}\) and \(M_{}\) are fed into the parameter network to produce \(C\) and \(F\). As shown in Figure 3, we use the Transformer encoder  to construct the parameter network, which consists of multiple attention layers. Firstly, the low-resolution composite image \(_{}\) and foreground mask \(M_{}\) are divided into patches, which are then projected into the embedding space. Positional encodings are added to the embedded patches, which are processed by the Transformer encoder. The output of the Transformer encoder is reassembled to obtain the feature \(T_{1}^{64 64 256}\), and a deconvolution layer is applied to reduce the feature channel number and perform upsampling to obtain the feature \(T_{2}^{H^{} W^{} 12}\). Subsequently, \(T_{2}\) is processed through a \(1 1\) convolution to obtain the parameter map \(C^{H^{} W^{} Q}\). We individually process each color channel and divide \(C\) into three parameter maps \(C^{}^{H^{} W^{} K}\), \(C^{}^{H^{} W^{} K}\), and \(C^{}^{H^{} W^{} K}\), where \(Q\) is equal to \(3 K\). Taking the red channel of an RGB image as an example, the corresponding parameter map is \(C^{}\). To achieve adaptive interval learning, we need to reconstruct the parameter map \(C^{}\). The softmax function is used to obtain the normalized interval \(V^{}^{H^{} W^{} M}=(C^{ },=3)\), where \(=3\) indicates that normalization is performed along the third dimension  of \(C^{}\). We perform cumulative summation in the third dimension of \(V^{}\) and then add an origin for each position to obtain the sampling coordinates \(K^{}^{H^{} W^{}(M+1)}\), which can be expressed as \(K^{}=[Z;(V^{},=3)]\), where \(Z\) is a \(H^{} W^{} 1\) matrix filled with zero values, and the \([:,:]\) denotes the concatenation operation. In such a way, each value in \(K^{}\) is within the range of 0 to 1 and maintains the monotone increasing properties along the third dimension (\(K^{}_{c,b,i}<K^{}_{c,b,j}\), for \(c,b^{255}_{0}\), \(i,j^{M}_{0}\), and \(i<j\)).

Figure 3: The framework of the proposed method. AICT consists of a high resolution (HR) branch and a low resolution (LR) branch. In the LR branch, the composite image \(\) and composite mask \(M\) are downsampled to predict two parameter maps \(C\) and \(F\). In the HR branch, \(C\) is used to redistribute the color values of \(\) into specific ranges of the following \(F\), which achieves adaptively adjusting the sampling interval of the color transformation. Finally, the new color values and pixel coordinates are mapped to final color values using \(F\).

We use position-dependent 3D LUTs to achieve pixel-wise adaptive adjustment of the sampling interval. The position-dependent 3D LUT maps spatial coordinates and color values to new color values . We treat \(K^{}\) as a position-dependent 3D LUT, which utilizes lookup and interpolation operations to serve as a color mapping curve. For a given coordinate \((i,j)\) and its color value \(x(i,j)\) in the red channel of \(\), we can find the corresponding position \((u,v,w)\) in \(K^{}\), where \(u=i-1}{H-1}\), \(v=j-1}{W-1}\), \(w=x(i,j)}}\) and \(C_{}\) represents the maximum color value in the image. Based on \((u,v,w)\), we can obtain adjacent 8 sampling points in \(K^{}\) and perform trilinear interpolation to produce a new color value \((i,j)\). The entire process can be formulated as

\[(i,j)=t(l(i,j,x(i,j),K^{})) \]

where \(t\) and \(l\) denote trilinear interpolation and lookup operations, respectively. The upper section of Figure 4 demonstrates the color redistribution process. Here, each coordinate and its corresponding color value are mapped to a new color value using \(K^{}\). Similarly, we can obtain redistributed color values for the other color channels in the same way.

### Pixel-Wise Color Transform

After obtaining the redistributed color values, we utilize the parameter map \(F\) to achieve pixel-wise color transformation. As shown in Figure 3, we apply a \(1 1\) convolution to the feature \(T_{2}\) for obtaining the intermediate parameter map \(R^{H^{} W^{} Q}\), which are divided into three parameter maps \(R^{}^{H^{} W^{} K}\), \(R^{}^{H^{} W^{} K}\), and \(R^{}^{H^{} W^{} K}\) according to the color channel. When inserting the foreground into a new background, the average brightness of the foreground will be influenced by the background. For example, the inserted foreground may experience overall darkening, brightening, or a bias toward a certain color. As shown in Figure 5, the foreground region undergoes darkening after image harmonization. Therefore, we propose a globally consistent weight learning method and design a weight learning module to learn image-level parameters for controlling overall color transformations. These parameters can also be regarded as scene classification, which are adjusted based on different scenes. The weight learning module consists of two 3\(\)3 convolutional layers, Batch Normalization (BN) layers , max-pooling layers, ReLU activation functions, and one fully-connected layer, making it lightweight. It processes \(T_{1}\) to predict weight vectors \(^{}^{K}\), \(^{}^{K}\), and \(^{}^{K}\), which are used to multiply with \(R^{}\), \(R^{}\), and \(R^{}\) to obtain the final parameter maps \(F^{}^{H^{} W^{} K}\), \(F^{}^{H^{} W^{} K}\), and \(F^{}^{H^{} W^{} K}\), respectively.

We also treat \(F^{}\) as a position-dependent 3D LUT. For a given coordinate \((i,j)\) and its redistributed color value \((i,j)\) in the red channel of \(\), lookup and interpolation operators are performed in \(F^{}\) to produce the final output color value \((i,j)\). The bottom part of Figure 4 illustrates the result prediction process. Due to the difference in resolution, each pixel in the parameter maps \(K^{}\) and \(F^{}\) corresponds to a local region of the composite image. However, the color values in a

Figure 4: The illustration of the color redistribution and result prediction. The coordinates and color values are first mapped to new color values using \(K^{}\). Then, the final color values are obtained by using \(F^{}\) according to the coordinates and redistributed color values.

Figure 5: The illustration of the overall impact of the background on the foreground. The foreground is outlined in red. The pixel histogram statistics indicate that the overall foreground region undergoes darkening after the process of image harmonization.

region are usually distributed within certain ranges. Therefore, \(K^{}\) aims to redistribute these color values across the entire color value range, thereby improving the utilization of sampling points in \(F^{}\) and enhancing the expressiveness of the curve. By combining these two curves (\(K^{}\) and \(F^{}\)), our method achieves the capability of adaptively adjusting the sampling intervals. Finally, the result images corresponding to each channel are obtained by transforming the color values at each position of \(\), which are then concatenated to produce the harmonized image \(\). As we process each color channel separately for an RGB image, the parameter network needs to predict six curves.

### Loss Functions

During the training phase, the loss \(_{}\) is calculated in the HR branch based on the difference between \(\) and \(I\). To improve the performance of AICT for images with small foregrounds, we adopt the foreground-normalized MSE loss , which is formulated as

\[_{}=^{H}_{j=1}^{W}\| _{i,j}-I_{i,j}\|_{2}^{2}}{\{A_{},_{i=1}^ {H}_{j=1}^{W}M_{i,j}\}} \]

where \(A_{}\) is a hyper-parameter to stabilize training. By replacing \(\) with \(_{}\) in the framework, we can also obtain the low-resolution harmonized image \(_{}\) and the loss \(_{}\). We also apply the foreground-normalized MSE loss to minimize the difference between \(_{}\) and the low-resolution version of the ground truth \(I_{}\). Overall, our network can be trained by optimizing the combination of the losses above

\[=_{}+_{} \]

where \(\) is a hyper-parameter that controls the weight of \(_{}\). Our parameter network is trained in an end-to-end manner.

## 4 Experiments

### Experimental Setting

#### 4.1.1 Dataset and Evaluation Metrics

Following the same settings as previous methods [20; 46; 12], we train and evaluate our method on the iHarmony4 dataset , which consists of four subsets (HAdobe5k, HCOCO, HDay2night, and HFlickr) and includes 73146 samples for image harmonization. Each sample contains a composite image, a corresponding foreground mask, and a ground truth image. The HAdobe5k, HCOCO, HDay2night, and HFlickr subsets consist of 2160, 4283, 133, and 828 test images, respectively. These subsets have resolutions ranging from \(312 230\) to \(6048 4032\) pixels. The width and height of images in the HCOCO, HDay2night, and HFlickr subsets are all below 1024 pixels, and only the HAdobe5k subset is composed of images with a width or height larger than 1024 pixels.

We also train and evaluate our method on the ccHarmony dataset , which is constructed by transferring each image across different illumination conditions to simulate natural illumination variations. This dataset contains 3080 training samples and 1180 test samples, and each sample includes a composite image, a corresponding foreground mask, and a ground truth image.

Additionally, we evaluate our method against other methods on a real composite dataset , which consists of 100 test samples. Each sample contains only a composite image and a corresponding foreground mask. Note that all datasets used in our experiments are publicly available.

For the quantitative performance metrics, we calculate several key indicators, including Mean Square Error (MSE), foreground Mean Square Error (fMSE), Peak Signal-to-Noise Ratio (PSNR), and Structural Similarity Index (SSIM) for each individual image in the dataset. Subsequently, we compute the average values for the entire dataset and for each specific subset. While MSE serves as a crucial evaluation metric, it tends to be biased towards images that contain larger foreground regions due to the variations in foreground sizes across the dataset. This limitation makes MSE less reliable for a comprehensive assessment of image quality. In contrast, fMSE offers a more balanced and equitable evaluation of overall quality, making it a more suitable choice for our analysis .

#### 4.1.2 Implementation Details

For the iHarmony4 dataset , our network is trained from scratch by using Adam optimizer with \(_{1}=0.9\), \(_{2}=0.999\), and \(=e^{-8}\). The batch size is set to 4 and the model is trained for 100 epochs. We set the learning rate as \(5e^{-5}\) for the first 50 epochs and linearly decay it to zero over the next 50 epochs. For the network design, we set \(K=8\), \(Q=24\), \(H^{}=256\), and \(W^{}=256\). For the loss function, \(\) is set to 0.01, and \(A_{}\) is set to 1000 and 100 in the HR branch and LR branch, respectively. We resize training images to ensure that the length of their sides does not exceed 2048 pixels due to memory limitations. During the testing phase, we perform image harmonization using full-resolution images. Following , we also conduct tests on the HAdobe5k subset at two different resolutions (\(1024 1024\) and \(2048 2048\)). For the ccHarmony dataset , we use the parameters trained on the iHarmony4 dataset as the initial parameters, and then fine-tune our network on the training set of the ccHarmony dataset. Following , the test image size in this dataset is set as \(256 256\). To augment training samples, we crop the composite image according to a random bounding box, the width and height of which are not smaller than the halved width and height of the composite image, respectively. The random horizontal flip is also applied to training samples. Our network is implemented based on the PyTorch framework and trained over approximately 75 hours on a computer equipped with two EPYC 7513 CPUs, 256GB of memory, and two 3090 GPUs.

### Comparison with the State-of-the-arts

We conduct experiments and comparisons on the full-resolution test images of the iHarmony4 dataset . AdaInt  and SepLUT , originally designed for image enhancement, are modified

   Dataset & Metric &  AdaInt \\  \\  &  SepLUT \\  \\  &  Harmonizer \\  \\  &  DCCF \\  \\  & 
 PCT-Net \\  \\  & Our AICT \\   & fMSE\(\) & 216.12 & 208.11 & 196.12 & 195.54 & 149.39 & **138.45** \\  & MSE\(\) & 30.38 & 26.53 & 24.37 & 23.12 & 19.35 & **17.09** \\  & PSNR\(\) & 38.22 & 38.19 & 37.80 & 37.79 & 39.97 & **40.32** \\  & SSIM\(\) & 0.9856 & 0.9844 & 0.9339 & 0.9858 & **0.9878** & **0.9878** \\   & fMSE\(\) & 374.89 & 380.77 & 374.96 & 317.43 & 245.67 & **240.62** \\  & MSE\(\) & 21.76 & 22.43 & 20.93 & 16.85 & 12.45 & **12.30** \\  & PSNR\(\) & 37.94 & 37.83 & 37.69 & 38.71 & **39.85** & 39.83 \\  & SSIM\(\) & 0.9918 & 0.9912 & 0.9858 & 0.9930 & **0.9938** & 0.9936 \\   & fMSE\(\) & 699.88 & 702.16 & 640.74 & 715.43 & 700.65 & **639.00** \\  & MSE\(\) & 49.40 & 52.98 & **37.28** & 55.78 & 46.47 & 42.90 \\  & PSNR\(\) & 37.42 & 37.13 & 37.15 & **37.52** & 37.25 & 37.50 \\  & SSIM\(\) & 0.9800 & 0.9782 & 0.9548 & 0.9788 & 0.9818 & **0.9819** \\   & fMSE\(\) & 588.40 & 580.50 & 479.26 & 438.49 & 357.53 & **334.11** \\  & MSE\(\) & 86.08 & 86.69 & 69.19 & 64.63 & 45.79 & **43.74** \\  & PSNR\(\) & 32.63 & 32.56 & 33.37 & 33.61 & 34.87 & **35.09** \\  & SSIM\(\) & 0.9817 & 0.9790 & 0.9714 & 0.9844 & 0.9876 & **0.9877** \\   & fMSE\(\) & 358.29 & 358.51 & 339.23 & 302.56 & 238.27 & **228.43** \\  & MSE\(\) & 31.96 & 31.36 & 27.62 & 24.72 & 18.80 & **17.76** \\  & PSNR\(\) & 37.42 & 37.34 & 37.23 & 37.85 & 39.28 & **39.40** \\  & SSIM\(\) & 0.9887 & 0.9876 & 0.9685 & 0.9897 & **0.9911** & 0.9910 \\   

Table 1: Quantitative comparison on the full-resolution test images of the iHarmony4 dataset. The best results are marked in bold, and the second best are underlined.

   Method & fMSE\(\) & MSE\(\) & PSNR\(\) & SSIM\(\) \\  iS\({}^{2}\)AM  & 271.59 & 46.37 & 36.57 & 0.9838 \\ AdaInt  & 221.90 & 31.09 & 38.10 & 0.9847 \\ SepLUT  & 216.15 & 27.03 & 37.98 & 0.9834 \\ CDTNet-512  & 159.13 & 23.35 & 38.45 & 0.9853 \\ Harmonizer  & 208.93 & 27.79 & 36.58 & 0.8992 \\ DCCF  & 197.23 & 23.00 & 38.34 & 0.9851 \\ PCT-Net  & 156.56 & 20.38 & 39.83 & 0.9870 \\  Our AICT & **147.99** & **17.92** & **40.07** & **0.9871** \\   

Table 2: Quantitative comparison on the HAdobe5k subset at a \(2048 2048\) resolution. The best results are marked in bold, and the second best are underlined.

and trained from scratch on the iHarmony4 dataset for the image harmonization task. Our AICT has three key differences compared to these methods. Firstly, they predict several weight parameters to fuse pre-trained LUTs, while our method dynamically predicts entire LUTs based on input. Secondly, they focus on global RGB-to-RGB transformations, whereas our AICT enables pixel-wise color transformations. Lastly, instead of applying adaptive interval learning and separable lookup tables for global sampling adjustments, our AICT achieves pixel-wise sampling interval adjustment to model local non-linearities in color transformation. To the best of our knowledge, only Harmonizer , DCCF , and PCT-Net  perform image harmonization on full-resolution images. Since Xu et al.  do not provide fMSE and SSIM values, we evaluate DCCF by running the provided code for comparison. Guerreiro et al.  propose two types of architectures: a CNN-based encoder-decoder network and a network based on a Visual Transformer (ViT) . We choose the ViT-based network for comparison as it demonstrates better performance. The quantitative comparison results are shown in Table 1. We observe that AICT outperforms other methods on most metrics. Due to the limited amount of training samples in the HDay2night subset, our method achieves lower performance on this subset. To evaluate performance on low-resolution images, we also report quantitative results on the iHarmony4 dataset at a \(256 256\) resolution (see the Appendix). Additionally, we evaluate the harmonization performance on the HAdobe5k subset at a \(2048 2048\) resolution. As shown in Table 2, our method achieves the best performance across all metrics. Furthermore, we compare our method with others on this subset at a \(1024 1024\) resolution (see the Appendix).

As shown in Figure 6, we present the qualitative results of AICT against state-of-the-art methods on the full-resolution test images of the iHarmony4 dataset . Compared to other methods, the images generated by AICT are closer to the ground truth images, making the composite images more realistic. More qualitative results are presented in the Appendix. Additionally, we also present the quantitative comparison results on the real composite dataset  to demonstrate the superiority and generalizability of our method (see the Appendix).

To further demonstrate the effectiveness of our method, we also present quantitative comparison results on the ccHarmony dataset  at \(256 256\) resolution. As shown in Table 3, our method also achieves the best performance across all metrics.

### Ablation Study

We study the influence of the adaptive interval learning method on image harmonization performance using the iHarmony4 dataset , as shown in Table 4. We first remove the weight learning module and the adaptive interval learning method in AICT, denoted as "Single". Compared to AICT, this method achieves lower performance. We then incorporate the channel-crossing strategy [34; 35] into "Single", denoted as "Cross". The parameter model uses an RGB image to predict parameter maps, which inherently include information from all color channels. Thus, the channel-crossing strategy introduces redundancy without improving performance. We also remove the adaptive interval learning method in AICT, denoted as "w/o Int". Compared to AICT, the performance of this method decreases due to the reduced expressiveness of the curves. To investigate whether using more LUTs can improve performance, we first cascade 2 and 3 LUTs for adaptive interval learning, denoted as "Int \(\) 2" and "Int \(\) 3", respectively. Then, we cascade 2 and 3 LUTs for color transformation, denoted as "Tra \(\) 2" and "Tra \(\) 3", respectively. Finally, we cascade 4 and 6 LUTs for alternating adaptive interval learning and color transformation, denoted as "Alt \(\) 2" and "Alt \(\) 3", respectively. The experimental results show that increasing the number of LUTs for adaptive interval learning does not improve performance. Furthermore, using more LUTs for color transformation decreases performance due to increased training difficulty. Additionally, we combine AdaInt  and SepLUT  for global RGB-to-RGB transformations, denoted as "AdaInt

  Method & fMSE\(\) & MSE\(\) & PSNR\(\) \\  DoveNet  & 880.94 & 110.84 & 31.64 \\ RainNet  & 519.32 & 58.11 & 34.78 \\ IH  & 636.28 & 83.72 & 33.64 \\ D-HT  & 514.47 & 55.73 & 35.07 \\ iS\({}^{2}\)AM  & 264.84 & 28.83 & 36.05 \\ CDTNet  & 264.51 & 27.87 & 36.62 \\ Harmonizer  & 402.09 & 43.31 & 34.68 \\ DCCF  & 259.83 & 29.25 & 36.62 \\ GiftNet  & 235.20 & 24.55 & 37.59 \\  Our AICT & **232.66** & **24.14** & **38.46** \\  

Table 3: Quantitative comparison on the ccHarmony dataset at a \(256 256\) resolution. The best results are marked in bold, and the second best are underlined.

+ SepLUT". Compared to AICT, one reason for the performance decline of "AdaInt + SepLUT" is the neglect of local context.

We conduct ablation studies on the global consistent weight learning method, as shown in Table 5. First, we remove the weight vector corresponding to the R channel, denoted as "w/o R". Next, we remove the weight vectors for both the R and G channels, denoted as "w/o RG". Finally, we remove the entire weight learning module, denoted as "w/o Weight". As the number of removed weight vectors increases, the harmonization performance decreases, highlighting the importance of the global consistent weight learning method. We also predict spatially varying weight parameters to weight each sampling point of the parameter map \(R\), denoted as "Spatial". The results for "Spatial" indicate that learning spatially varying weight parameters does not improve performance, as the LUT is responsible for pixel-wise color transformations. These observations demonstrate that the proposed adaptive interval learning method and global consistent weight learning method are effective.

  Method & fMSE\(\) & MSE\(\) & PSNR\(\) \\  Single & 248.51 & 19.71 & 39.15 \\ Cross & 264.33 & 21.40 & 38.90 \\ w/o Int & 239.34 & 18.96 & 39.20 \\ Int \(\) 2 & 241.79 & 19.24 & 39.17 \\ Int \(\) 3 & 231.91 & 18.04 & 39.38 \\ Tra \(\) 2 & 456.98 & 33.94 & 36.16 \\ Tra \(\) 3 & 487.60 & 36.03 & 35.91 \\ Alt \(\) 2 & 441.27 & 33.09 & 36.28 \\ Alt \(\) 3 & 489.02 & 35.95 & 35.94 \\ AdaInt + SepLUT & 357.61 & 31.11 & 37.47 \\  Our AICT & **228.43** & **17.76** & **39.40** \\  

Table 4: Ablation studies on the adaptive interval learning method. The best results are marked in bold.

Figure 6: Qualitative comparison results and error maps. We visualize the error between the harmonized images and the ground truth images. The error maps are normalized for display, and the foreground is outlined in red.

### Hyper-parameter Analyses

We conduct studies on key parameters using the iHarmony4 dataset , including the number of knots \(K\), the coefficient of the LR branch loss \(\), and the hyper-parameter \(A_{}\) in the foreground-normalized MSE loss, as shown in Table 6. We set \(K\) to 6, 8, and 10 to analyze the influence of curves with different numbers of knots for the image harmonization task, where \(K\) is set to 8 in AICT. By comparing "K=6" with AICT, we observe that increasing the number of knots reduces MSE and MSE while improving PSNR, indicating that a larger number of knots enhances the harmonization ability of the curves. However, as the number of knots increases, the performance of "K=10" decreases, suggesting that an excessive number of knots increases network parameters. This complicates the training process of the network, making it more challenging to achieve optimal performance. To investigate the effect of the LR branch loss, we set \(\) to 0, 0.01, and 0.1, where \(\) set to 0.01 in AICT. When \(\) is set to 0.01, AICT achieves the best performance on all metrics, demonstrating that an optimal weight for the LR branch loss is crucial for the high-resolution image harmonization. For the foreground-normalized MSE loss, we set \(A_{}\) to \(H W\) and \(H^{} W^{}\) in the HR and LR branch, respectively, which means these objective functions are equivalent to MSE functions, denoted as "MSE". The results indicate that using the foreground-normalized MSE loss improves harmonization performance, as it prevents training samples with foregrounds of different sizes from being trained with varying loss magnitudes, ensuring effective training for small foreground images.

## 5 Conclusion

In this paper, we formulate image harmonization as an image-based multiple curve estimation problem and propose an Adaptive-Interval Color Transformation method, which predicts pixel-wise color transformation and adaptively adjusts the sampling interval to model local nonlinearities of the color transformation at high resolution. Specifically, a parameter network is first designed to generate multiple curves as position-dependent 3D LUTs, which use the color and position of each pixel to perform pixel-wise color transformation. Then, we separate a color transform into a cascade of sub-transformations using two position-dependent 3D LUTs to achieve the non-uniform sampling intervals of the color transform. Finally, a global consistent weight learning method is proposed to predict an image-level weight for each color transform, utilizing global information to enhance the overall harmony. Experimental results demonstrate that our method achieves state-of-the-art performance in high-resolution image harmonization with a lightweight architecture.