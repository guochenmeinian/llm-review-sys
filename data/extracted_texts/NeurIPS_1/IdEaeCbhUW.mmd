# Achieving Precise Control with Slow Hardware: Model-Based Reinforcement Learning for Action Sequence Learning

Achieving Precise Control with Slow Hardware: Model-Based Reinforcement Learning for Action Sequence Learning

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Current reinforcement learning (RL) models are often claimed to explain animal behavior. However, they are designed for artificial agents that sense, think, and react much faster than the brain, and they tend to fail when operating under human-like sensory and reaction times. Despite using slow neurons, the brain achieves precise and low-latency control through a combination of predictive and sequence learning. The basal ganglia is hypothesized to learn compressed representations of action sequences, allowing the brain to produce a series of actions for a given input. We present the Hindsight-Sequence-Planner (HSP), a model of the basal ganglia and the prefrontal cortex that operates under "brain-like" conditions: slow information processing with quick sensing and actuation. Our "temporal recall" mechanism is inspired by the prefrontal cortex's role in sequence learning, where the agent uses an environmental model to replay memories at a finer temporal resolution than its processing speed while addressing the credit assignment problem caused by scalar rewards in sequence learning. HSP employs model-based training to achieve model-free control, resulting in precise and efficient behavior that appears low-latency despite running on slow hardware. We test HSP on various continuous control tasks, demonstrating that it not can achieve comparable performance 'human-like' frequencies by relying on significantly fewer observations and actor calls (actor sample complexity).

## 1 Introduction

Biological and artificial agents must learn behaviors that maximize rewards to thrive in complex environments. Reinforcement learning (RL), a class of algorithms inspired by animal behavior, facilitates this learning process (1). The connection between neuroscience and RL is profound. The Temporal Difference (TD) error, a key concept in RL, effectively models the firing patterns of dopamine neurons in the midbrain (2; 3; 4). Additionally, a longstanding goal of RL algorithms is to match and surpass human performance in control tasks (5; 6; 7; 8; 9; 10)

However, most of these successes are achieved by leveraging large amounts of data in simulated environments and operating at speeds orders of magnitude faster than biological neurons. For example, the default timestep for the Humanoid task in the MuJoCo environment (11) in OpenAI Gym (12) is 15 milliseconds. In contrast, human reaction times range from 150 milliseconds (13) to several seconds for complex tasks (14). When RL agents are constrained to human-like reaction times, even state-of-the-art algorithms struggle to perform in simple environments.

The primary reason for this difficulty is the implicit assumption in RL that the environment and the agent operate at a constant timestep. Consequently, in embodied agents, all components--sensors,compute units, and actuators--are synchronized to operate at the same frequency. Typically, this frequency is limited by the speed of computation in artificial agents [(15)]. As a result, robots often require fast onboard computing hardware (CPU or GPU) to achieve higher control frequencies [(16; 17; 18)].

In contrast, biological agents achieve precise and seemingly fast control using much slower hardware. This is possible because biological agents effectively decouple the computation frequency from the actuation frequency, allowing them to achieve high actuation frequencies even with slow computational speeds. Consequently, biological agents demonstrate robust, adaptive, and efficient control.

To allow the RL agent to observe and react to changes in the environment quickly, RL algorithms are forced to set a high frequency. Even in completely predictable environments, when the agent learns to walk or move, a small timestep is required to account for the actuation frequency required for the task, but it is not necessary to observe the environment as often or compute new actions as frequently. As a result, RL algorithms suffer from many problems such as low sample efficiency, failure to learn tasks with sparse rewards, jerky control, high compute cost, and catastrophic failure due to missing inputs.

In this work, we propose Hindsight-Sequence-Planner (HSP), a model for sequence learning based on the role of the basal ganglia (BG) and the prefrontal cortex (PFC). Our model learns open-loop control utilizing a slow hardware and low attention, and hence also low energy. Additionally, the algorithm utilizes a simultaneously learned model of the environment during its training but can act without it for fast and cheap inference. We demonstrate the algorithm achieves competitive performance on difficult continuous control tasks while utilizing a fraction of observations and calls to the policy. To the best of our knowledge, HSP is the first to achieve this feat.

## 2 Neural Basis for Sequence Learning

Unlike artificial RL agents, learning in the brain does not stop once an optimal solution has been found. During initial task learning, brain activity increases as expected, reflecting neural recruitment. However, after training and repetition, activity decreases as the brain develops more efficient representations of the action sequence, commonly referred to as muscle memory [(19)]. This phenomenon is further supported by findings that sequence-specific activity in motor regions evolves based on the amount of training, demonstrating skill-specific efficiency and specialization over time [(20)].

The neural basis for action sequence learning involves a sophisticated interconnection of different brain regions, each making a distinct contribution:

1. **Basal ganglia** (BG): Action chunking is a cognitive process by which individual actions are grouped into larger, more manageable units or "chunks," facilitating more efficient storage, retrieval, and execution with reduced cognitive load [(21)]. Importantly, this mechanism allows the brain to perform extremely fast and precise sequences of actions that would be impossible if produced individually. The BG plays a crucial role in chunking, encoding entire behavioral action sequences as a single action [(22; 21; 23; 24; 25; 26)]. Dysfunction in the BG is associated with deficits in action sequences and chunking in both animals [(27; 28; 29)] and humans [(30; 31; 21)]. However, the neural basis for the compression of individual actions into sequences remains poorly understood.
2. **Prefrontal cortex** (PFC): The PFC is critical for the active unbinding and dismantling of action sequences to ensure behavioral flexibility and adaptability [(32)]. This suggests that action sequences are not merely learned through repetition; the PFC modifies these sequences based on context and task requirements. Recent research indicates that the PFC supports memory elaboration [(33)] and maintains temporal context information [(34)] in action sequences. The prefrontal cortex receives inputs from the hippocampus.
3. **Hippocampus** (HC) replays neuronal activations of tasks during subsequent sleep at speeds six to seven times faster. This memory replay may explain the compression of slow actions into fast chunks. The replayed trajectories from the HC are consolidated into long-term cortical memories [(35; 36)]. This phenomenon extends to the motor cortex, which replays motor patterns at accelerated speeds during sleep [(37)].

Related Work

### Model-Based Reinforcement Learning

Model-Based Reinforcement Learning (MBRL) algorithms leverage a model of the environment, which can be either learned or known, to enhance RL performance [(38)]. Broadly, MBRL algorithms have been utilized to:

1. Improve Data Efficiency: By augmenting real-world data with model-generated data, MBRL can significantly enhance data efficiency [(39; 40; 41)].
2. Enhance Exploration: MBRL aids in exploration by using models to identify potential or unexplored states [(42; 43; 44)].
3. Boost Performance: Better learned representations from MBRL can lead to improved asymptotic performance [(45; 46)].
4. Transfer Learning: MBRL supports transfer learning, enabling knowledge transfer across different tasks or environments [(47; 48)].
5. Online Planning: Models can be used for online planning with a single-step policy [(49)]. However, this approach increases model complexity as each online planning step requires an additional call to the model, making it nonviable for energy and computationally constrained agents like the brain and robots.

Compared to online planning, our algorithm maintains a model complexity of zero after training, eliminating the need for any model calls post-training. This significantly reduces the computational and energy requirements, making it more suitable for practical applications in constrained environments. Additionally, the performance of online planning algorithms relies heavily on the accuracy of the model. In contrast, our approach can leverage even an inaccurate model to learn a better-performing policy than online planning, using the same model.

### Macro-Actions

Reinforcement Learning (RL) algorithms that utilize macro-actions demonstrate many benefits, including improved exploration and faster learning [(50)]. However, identifying effective macro-actions is a challenging problem due to the curse of dimensionality, which arises from large action spaces. To address this issue, some approaches have employed genetic algorithms [(51)] or relied on expert demonstrations to extract macro-actions [(52)]. However, these methods are not scalable and lack biological plausibility.

In contrast, our approach learns macro-actions using the principles of RL, thus requiring little overhead while combining the flexibility of primitive actions with the efficiency of macro-actions.

### Action Repetition and Frame-skipping

To overcome the curse of dimensionality while gaining the benefits of macro-actions, many approaches utilize frame-skipping and action repetition, where macro-actions are restricted to a single primitive action that is repeated. Frame-skipping and action repetition serve as a form of partial open-loop control, where the agent selects a sequence of actions to be executed without considering the intermediate states. Consequently, the number of actions is linear in the number of time steps [(53; 54; 55; 56; 57)].

For instance, FiGaR [(56)] shifts the problem of macro-action learning to predicting the number of steps that the outputted action can be repeated. TempoRL [(55)] improved upon FiGaR by conditioning the number of repetitions on the selected actions. However, none of these algorithms can scale to continuous control tasks with multiple action dimensions, as action repetition forces all actuators and joints to be synchronized in their repetitions, leading to poor performance for longer action sequences.

## 4 Hindsight Sequence Planner

Based on the insights presented in Section 2, we introduce a novel reinforcement learning model capable of learning sequences of actions (macro-actions) by replaying memories at a finer temporal resolution than the action generation, utilizing a model of the environment during training.

### Components

The Hindsight-Sequence-Planner (HSP) algorithm learns to plan "in-the-mind" using a model during training, allowing the learned action-sequences to be executed without the need for model-based online planning. This is achieved using an actor-critic setting where the actor and critic operate at different frequencies, representing the observation/computation and actuation frequencies, respectively. Essentially, the critic is only used during training/replay and can operate at any temporal resolution, while the actor is constrained to the temporal resolution of the slowest component in the sensing-compute-actuation loop. Denoting the actor's timestep as \(t^{}\) and the critic's timestep as \(t\), our algorithm includes three components:

\[&:s_{t+1}=_{ }(s_{t},a_{t})\\ &:q_{t}=_{}(s_{t},a_{t}) \\ &:a_{t^{}}=a_{t},a_{t^{}+t},a_ {t^{}+2t}_{}(s_{t^{}}) \]

We denote individual actions in the action sequence generated by actor using the notation \(_{}(s_{t^{}})_{t}\) to represent the action \(a_{t^{}+t}\)

1. **Model**: Learns the dynamics of the environment, predicting the next state \(s_{t+1}\) given the current state \(s_{t}\) and primitive action \(a_{t}\).
2. **Critic**: Takes the same input as the model but predicts the Q-value of the state-action pair.
3. **Actor**: Produces a sequence of actions given an observation at time \(t^{}\). Observations from the environment can occur at any timestep \(t\) or \(t^{}\), where we assume \(t^{}>t\). Specifically, in our algorithm, \(t^{}=Jt\) where \(J>1;J\).

Each component of our algorithm is trained in parallel, demonstrating competitive learning speeds.

We follow the Soft-Actor-Critic (SAC) algorithm (58) for learning the actor-critic. Exploration and uncertainty are critical factors heavily influenced by timestep size and planning horizon. Many model-free algorithms like DDPG (59) and TD3 (60) explore by adding random noise to each action during training. However, planning a sequence of actions over a longer timestep can result in additive noise, leading to poor performance during training and exploration. The SAC algorithm addresses this by maximizing the entropy of each action in addition to the expected return, allowing our algorithm to automatically lower entropy for deeper actions farther from the observation.

Figure 1: The Hindsight-Sequence-Planner (HSP) model. The HSP takes inspiration from the function of the basal ganglia (BG) (Top/Orange) and the prefrontal cortex (PFC) (Bottom/Blue). We train an actor with a gated recurrent unit that can produce sequences of arbitrary lengths given a single state. This is achieved by utilizing a critic and a model that acts at a finer temporal resolution during training/replay to provide an error signal to each primitive action of the action sequence.

### Learning the Model

The model is trained to minimize the Mean Squared Error of the predicted states. For a trajectory \(=(s_{t},a_{t},s_{t+1})\) drawn from the replay buffer \(\), the predicted state is taken from \(_{t+1}(s_{t},a_{t})\). The loss function is:

\[_{}=_{}(_{t+1}-s_{t+1})^{2} \]

For this work, the model is a feed-forward neural network with two hidden layers. In addition to the current model \(_{}\), we also maintain a target model \(_{^{-}}\) that is the exponential moving average of the current model.

### Learning Critic

The critic is trained to predict the Q-value of a given state-action pair \(_{t}=_{}(s_{t},a_{t})\) using the target value from the modified Bellman equation:

\[_{t}=r_{t}+_{a_{t+1}_{}(s_{t+1})_{0}}[ _{^{-}}(s_{t+1},a_{t+1})-_{}(a_{t+1}|s_{t+1})] \]

Here, \(_{^{-}}\) is the target critic, which is the exponential moving average of the critic. Following the SAC algorithm, we train two critics and use the minimum of the two \(_{^{-}}\) values to train the current critics. The loss function is:

\[_{}=_{}[(_{tk}-_{t })^{2}] k \]

Both critics are feed-forward neural networks with two hidden layers.

### Learning Policy

The HSP policy utilizes two hidden layers followed by a Gated-Recurrent-Unit (GRU) [(61)] that takes as input the previous action in the action sequence, followed by two linear layers that output the mean and standard deviation of the Gaussian distribution of the action. This design allows the policy to produce action sequences of arbitrary length given a single state and the last action.

A naive approach to training a sequence of actions would be to augment the action space to include all possible actions of the sequence length. However, this quickly leads to the curse of dimensionality, as each sequence is considered a unique action, dramatically increasing the policy's complexity. Additionally, such an approach ignores the temporal information of the action sequence and faces the difficult problem of credit assignment, with only a single scalar reward for the entire action sequence.

To address these problems, we use different temporal scales for the actor and critic. The critic assigns value to each segment of the action sequence, bypassing the credit assignment problem caused by the single scalar reward. However, using collected transitions to train the action sequence is impractical, as changing the first action in the sequence would render all future states inaccurate. Thus, the model populates intermediate states, which the critic then uses to assign value to each primitive action in the sequence.

Therefore, given a trajectory \(=(a_{t-1},s_{t},a_{t},s_{t+1})\), we first produce the \(J\)-step action sequence using the policy: \(_{t:t+J}=_{}(s_{t})\). We then iteratively apply the target model to get the intermediate states \(_{t+1:t+J-1}\). Finally, we use the critic to calculate the loss for the actor as follows:

\[_{}=_{}_{ }(_{t}|s_{t})-_{}(s_{t},_{t})+_{j=1} ^{J}_{}(_{t+j}|_{t+j})-_{}( _{t+j},_{t+j}) \]

## 5 Experiments

### Overview

We evaluate our HSP approach on several continuous control tasks, comparing it against the SAC baseline and the TempoRL algorithm [(55)]. Our focus is on environments with multi-dimensional actions, ranging from the simple LunarLanderContinuous (2 action dimensions) to the complex Humanoid environment (17 action dimensions). This allows us to highlight the benefits of HSP over traditional action repetition approaches. We utilize the OpenAI gym [(62)] implementation of the MuJoCo environments [(11)].

### Experimental Setup

We train HSP with four different action sequence lengths (ASL), \(J=2,4,8,16\), referred to as HSP-\(J\). During training, HSP is evaluated based on its \(J\) value, processing states only after every \(J\) actions. All hyperparameters are identical between HSP and SAC, except for the actor update frequency: HSP updates the actor every 4 steps, while SAC updates every step. Thus, SAC has four more actor update steps compared to HSP. Additionally, HSP learns a model in parallel with the actor and critic.

### Learning Curves

Figure 6 presents the learning curves of HSP and SAC across six continuous control tasks. We observe that HSP outperforms SAC in four out of six tasks (excluding Ant and HalfCheetah). Notably, HSP-16 achieves competitive performance on LunarLander and Hopper tasks, showcasing the algorithm's capability to learn long action sequences from scratch. Surprisingly, HSP also outperforms SAC in the Humanoid environment with fewer inputs and actor updates while concurrently learning a model, demonstrating the efficacy of the algorithm on environments with higher action dimensions.

### Action Sequence Length (ASL) Performance

Learning curves alone do not fully capture HSP's performance and benefits. For instance, HSP-16 shows poor performance on Ant in the learning curve, yet it demonstrates competitive performance when tested on shorter action sequences. Figure 3 presents the performance of trained algorithms across different action sequence lengths (ASL).

We select the largest \(J\) that shows competitive performance (greater than \(75\%\) of the SAC when evaluated on primitive actions) for each environment and test it for sequence lengths up to 30. For SAC and HSP, we fix the length of action sequences while TempoRL is designed to dynamically pick the best ASL, therefore we report the avg. action sequence length for TempoRL. HSP demonstrates

Figure 2: Learning curves of HSP-\(J\) and Soft-Actor Critic (SAC) (58) over continuous control tasks. HSP and SAC are evaluated under different settings: SAC receives input after every primitive action, while HSP receives input after \(J\) primitive actions. Yet it demonstrates competitive performance on all environments, even outperforming SAC on LunarLander, Hopper and Humanoid environments. HSP demonstrates stable learning even with the added model and generative replay training. All curves are averaged over 5 trials, with shaded regions representing standard error.

competitive performance on longer action sequences, approaching human-like reaction times in some environments. Unlike SAC, which fails with action sequences of 2 or 3, HSP shows a gradual degradation of performance. Additionally, HSP generalizes well in environments like LunarLander and Ant, even though the actor is trained only on sequence lengths of 16.

Comparing HSP to TempoRL, we find that TempoRL prefers shorter repetitions and struggles in more difficult environments. TempoRL does not incentivize longer actions and suffers from the curse of dimensionality to some extent, as it needs to learn the number of repetitions for each unique state-action pair. Furthermore, action repetitions are not suitable for multi-dimensional actions, as they force synchronized repetition across all actuators resulting in poor performance in environments with high action dimensions like Ant and HalfCheetah environments.

## Comparison to Model-based Online Planning

In addition to action repetition, model-based online planning is another approach that allows the RL agent to reduce its observational frequency. However, it often requires a highly accurate model of the environment and incurs increased model complexity due to the use of the model during control. Despite these challenges, comparing HSP to model-based online planning is essential since it is useful when the actor cannot produce long sequences of actions and does not require the hyper-parameter \(J\). With access to an accurate model of the environment, the agent's performance might generalize to arbitrary ASL.

Since HSP incorporates a model of the environment that is learned in parallel, we compare the performance of the HSP actor utilizing the actor-generated action sequences against model-based online planning, where the actor produces only a single action between each simulated state.

Figure 3: Performance of HSP, SAC, and TempoRL (55) at different Action Sequence Lengths (ASL). SAC and TempoRL repeat the same action for the duration, while HSP can perform a sequence of actions. Since it implements dynamic action repetition, we present the average ASL for TempoRL instead of a range of ASL. HSP demonstrates robust performance even at human-like reaction times (>150ms). All markers are averaged over 5 trials, with the error bars representing standard error. Going from left to right then top to bottom, the selected training ASL \(J\) for HSP are: 16, 16, 4, 16, 4, 8.

Figure 4 shows the performance of online planning using the model in HSP versus the action sequences generated by the HSP policy. We see that HSP can learn action sequences that perform better than model-based online planning using the same model. Thus, HSP can leverage inaccurate models to learn accurate action sequences, further reducing the required computational complexity during training. We hypothesize that this superior performance is due to the fact that the actor learns a \(J\)-step action sequence concurrently, while online planning only produces one action at a time. Consequently, HSP is able to learn and produce long, coherent action sequences, whereas single-step predictions tend to drift, similar to the "hallucination" phenomenon observed in transformer-based language models.

### Generative Replay in Latent Space

Previous studies have shown that generative replay benefits greatly from latent representations (63). Recently, Simplified Temporal Consistency Reinforcement Learning (TCRL) (64) demonstrated that learning a latent state-space improves not only model-based planning but also model-free RL

Figure 4: Performance of HSP and model based online planning on different ASL. Both HSP and Online Planning utilize the same actor and model. HSP utilizes the actor to generate a sequence of actions while online planning utilizes the actor and the model to generate a sequence of actions. The same model is used to train the HSP action sequences. Yet, we find that while the model is not accurate enough to sustain performance for longer sequences, it can train the actor to produce accurate action sequences.

Figure 5: Left: Learning curve of HSP with latent state-space on the Walker2d-v2 environment. Right: Performance of latent HSP-16 on different ASL, compared to SAC and TempoRL. Utilizing a latent representation for state space is especially beneficial for the Walker2d environment so that it outperforms SAC even when training upto sequence lengths of \(J=16\).

algorithms. Building on this insight, we introduced an encoder to encode the observations in our algorithm. We provide the complete implementation details in the Appendix.

We did not observe any benefits of using the encoder and temporal consistency for HSP in most environments (results in the appendix). However, for the Walker environment, utilizing the latent space for generative replay significantly improved performance, making it competitive even at 16 steps (128ms) (Figure 5).

## 6 Discussion, Limitations and Future Work

We introduce the Hindsight-Sequence-Planner (HSP) algorithm, a biologically plausible model for sequence learning. It represents a significant step towards achieving robust control at brain-like speeds. The key contributions of HSP include its ability to generate long sequences of actions from a single state, its resilience to reduced input frequency, and its lower computational complexity per primitive action.

The current RL framework encourages synchrony between the environment and the components of the agent. However, the brain utilizes components that act at different frequencies and yet is capable of robust and accurate control. HSL provides an approach to reconcile this difference between neuroscience and RL, while remaining competitive on current RL benchmarks. HSP offers substantial benefits over traditional RL algorithms, particularly in the context of autonomous agents such as self-driving cars and robots. By enabling operation at slower observational frequencies and providing a gradual decay in performance with reduced input frequency, HSP addresses critical issues related to sensor failure and occlusion, and energy consumption. Additionally, HSP generates long sequences of actions from a single state, which can enhance the explainability of the policy and provide opportunities to override the policy early in case of safety concerns. HSP also learns a latent representation of the action sequence, which could be used in the future to interface with large language models for multimodal explainability and even hierarchical reinforcement learning and transfer learning.

### Limitations

Despite its advantages, HSP has some limitations. It shows slightly reduced performance in the Ant and HalfCheetah environments, which we believe can be mitigated through improved models and hyperparameter tuning. HSP also requires more computational resources during training due to the parallel training of an environment model and introduces more hyperparameters, particularly the training ASL (\(J\)). In this work, we do not optimize the neural network architecture of the actor to reduce the compute, as a result, the total compute per primitive action is still larger than SAC. However, we believe producing a sequence of actions will be more efficient than producing a single primitive action per state after optimization. Larger ASL values may not perform well in stochastic environments. Moreover, HSP currently uses a constant ASL, but ideally, the ASL should adapt based on the environment's predictability.

### Future Work

We believe the HSP model contributes to both artificial agents and the study of biological control. Future work will incorporate biological features like attention mechanisms and knowledge transfer. Additionally, HSP can benefit from existing Model-Based RL approaches as it naturally learns a model of the world. In deterministic environments, a capable agent should achieve infinite horizon control for tasks like walking and hopping from a single state. This is an important research direction that is currently underexplored, as many environments are partially observable or have some degree of stochasticity. Current approaches rely on external information at every state, which increases energy consumption and vulnerability to adversarial or missing inputs. Truly autonomous agents will need to impl ement multiple policies simultaneously, and simple tasks like walking can be performed without input states if learned properly. Our future work will focus on extending the action sequence horizon until deterministic tasks can be performed using a single state and implementing a mechanism to dynamically pick the action sequence horizon based on context and predictability of the state. Serotonin is an important neuromodulator that has been demonstrated to signal the availability of time and resources in the brain to enable the decision on the planning horizon and the use of compute (65). In the future, we hope to introduce a mechanism to replicate the effect of serotonin in HSP.