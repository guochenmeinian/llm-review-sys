# CulturePark: Boosting Cross-cultural Understanding

in Large Language Models

 Cheng Li

Institute of Software, CAS

chenglicat0228@gmail.com

Work done during Cheng's internship at MSRA.

Damien Teney

Idiap Research Institute

contact@damienteney.info

Linyi Yang

Westlake University

yanglingiyi@westlake.edu.cn

&Qingsong Wen

Squirrel AI

qingsongedu@gmail.com

Xing Xie

Microsoft Research

xing.xie@microsoft.com

Jindong Wang

William & Mary

jwang80@wm.edu

Work done at MSRA.

###### Abstract

Cultural bias is pervasive in many large language models (LLMs), largely due to the deficiency of data representative of different cultures. Typically, cultural datasets and benchmarks are constructed either by extracting subsets of existing datasets or by aggregating from platforms such as Wikipedia and social media. However, these approaches are highly dependent on real-world data and human annotations, making them costly and difficult to scale. Inspired by cognitive theories on social communication, this paper introduces _CulturePark_, an LLM-powered multi-agent communication framework for cultural data collection. CulturePark simulates cross-cultural human communication with LLM-based agents playing roles in different cultures. It generates high-quality cross-cultural dialogues encapsulating human beliefs, norms, and customs. Using CulturePark, we generated 41,000 cultural samples to fine-tune eight culture-specific LLMs. We evaluated these models across three downstream tasks: content moderation, cultural alignment, and cultural education. Results show that for content moderation, our GPT-3.5-based models either match or outperform GPT-4 on \(41\) datasets. Regarding cultural alignment, our models surpass GPT-4 on Hofstede's VSM 13 framework . Furthermore, for cultural education of human participants, our models demonstrate superior outcomes in both learning efficacy and user experience compared to GPT-4. CulturePark proves an important step in addressing cultural bias and advancing the democratization of AI, highlighting the critical role of culturally inclusive data in model training. Code is released at [https://github.com/Scarelette/CulturePark](https://github.com/Scarelette/CulturePark).

## 1 Introduction

Culture is an important part of human society, composed of human beliefs, norms, customs, etc. . As large language models (LLMs) play a vital role in daily communication , recommendation systems , andeducation (Shaikh et al., 2023), it is imperative for LLMs to perceive and reflect different cultures. However, current state-of-the-art LLMs have been reported to be biased towards mainstream culture while ignoring others, resulting in a cultural bias problem (Liu et al., 2023; Cao et al., 2023; Masoud et al., 2023; Naous et al., 2023; Wang et al., 2023; Johnson et al., 2022b). This leads to stereotypical impressions of different cultures, which can even exacerbate social conflicts (Ryan et al., 2024). The main reason behind cultural bias is that the training corpus of LLMs is dominated by English data that express the cultural values and opinions of western people. Much less can be learned about other cultures simply because there is less data available, i.e., a low-resource situation.

Existing approaches to solve the cultural bias problem in LLMs include prompt engineering (Kovac et al., 2023; Wang et al., 2023; Rao et al., 2023) and pre-training in non-English languages (Pires et al., 2023; Chan et al., 2023; Nguyen et al., 2023; Pipatanakul et al., 2023; Abbasi et al., 2023; Lin and Chen, 2023). Prompt engineering consists of tuning prompts for different cultural tasks, but the benefits do not hold reliably across various downstream tasks. Pre-training in various languages is promising, but the data collection and the pre-training itself are both very costly. More importantly, cultural differences are embodied in many aspects such as opinions, customs, norms, and languages. A model serving all cultures may face cultural conflict and misalignment problems (Liu et al., 2023; Cao et al., 2023; Masoud et al., 2023). Thus, it is necessary to fine-tune culture-specific models that target specific cultures. Recently, Li et al. (2024) proposed CultureLLM, which augments the fine-tuning data of LLMs via semantic data augmentation to train culture-specific LLMs. However, the generated data lack diversity because it is implemented by generating semantically equivalent sentences of seed examples.

In this paper, we present **CulturePark**, an LLM-powered multi-agent framework to simulate cross-cultural communication of humans. As shown in Figure 1, CulturePark serves as an effective data collection platform to generate diverse and high-quality cultural datasets via multi-agent communication. CulturePark consists of a main contact (an English-speaking agent, \(\)Lily) who is in charge of the multi-turn dialogue and several cultural delegates (e.g., \(\)Abdul) who interact with the main contact and create cognitive conflicts.3 After an initial problem is provided as input to the framework, the agents discuss the problem and express their opinions. Their different cultural backgrounds and genders boost diverse opinions and encourage one another to think more deeply. Original questions and ground truth can be augmented by creating novel questions and more comprehensive answers. The interactions eventually generate a cross-cultural dialogue dataset that contains deep and comprehensive thinking and informative knowledge of different cultures. Detailed statistics are shown in Table 4. We then refine the original dataset to factually verify and increase its diversity, which is used to fine-tune culturally specific LLMs for downstream tasks, as shown in Figure 2.

Figure 1: CulturePark is an LLM-based multi-agent communication platform for cultural data collection. Leveraging CulturePark, we can collect a cross-cultural dialogue dataset, which can then be used for fine-tuning culturally specific LLMs to be applied to different downstream tasks: content moderation, cultural alignment, and cultural education.

From the perspective of cognitive social science, our framework is inspired by Cognitive Conflict Theory (CCT) (Limon, 2001; Cosier and Rose, 1977) and Social Cognitive Theory (SCT) (Fiske and Taylor, 1991) to foster a collaborative and communicative environment for mutual understanding of cultures. Specifically, CulturePark allows agents to encounter _cognitive conflicts_, which triggers deeper thinking on certain topics according to CCT (Limon, 2001; Cosier and Rose, 1977). At the same time, a deeper understanding of cultures can be prompted through _interaction and communication_ with other agents, as suggested by SCT (Fiske and Taylor, 1991). In favor of these theories, we found that CulturePark triggers LLMs' cross-cultural understanding ability, boosts novel opinions by allowing agents to think deeper, and benefits data augmentation by creating more comprehensive answers to the questions. CulturePark has the potential to facilitate the data collection of culture-related tasks, cultural value alignment, and improving AI democracy.

In summary, the contributions of this paper are three-fold:

1. We introduce CulturePark, a cost-efficient multi-agent framework to boost the cross-cultural understanding in LLMs. Our platform creates cognitive conflicts and interactions between different cultural agents. More importantly, the platform uncovers several interesting findings, such as communication enables the cross-cultural understanding ability of LLMs, boosts novel opinions, and benefits data augmentation.
2. Leveraging CulturePark, we generate and augment novel questions and more comprehensive answers, leading to \(41K\) cultural samples in total. Those data contain rich and diverse information on different aspects of culture, such as norms, opinions, and backgrounds. We then fine-tune cultural specific LLMs for different cultures.
3. We evaluate CulturePark in three key experiments: 1) The fine-tuned LLMs outperforms GPT-4 in \(5\) cultures on \(26\) content moderation tasks and approach GPT-4 on other tasks; 2) Our fine-tuned LLMs achieve better performance on cultural alignment experiments via Hofstede's cultural dimensions theory (Geert Hofstede, 2010); and 3) Human participants can perform more effective culture learning in situated learning experiments and show better satisfaction compared to GPT-4.

## 2 Related work

### Cultural bias in LLMs

A body of research has explored cultural biases in LLMs. Johnson et al. (2022) examined conflicts in model outputs and input values, using moral value pluralism to analyze the responses of GPT-3 against global demographics. Their results showed that conflicting values were more aligned with the dominant US values reported. Naous et al. (2023) highlighted a bias towards Western culture in models processing Arabic, exacerbated by English-aligned prompts, suggesting mitigation through cultural tokens. The Cultural Alignment Test (CAT), based on Hofstede's framework (Geert Hofstede, 2010), evaluated cultural values in models such as ChatGPT and Bard across different cultures, revealing the highest cultural alignment for GPT-4 with US values (Masoud et al., 2023). Cao et al. (2023) found that ChatGPT aligned well with American culture but struggled with other cultures, particularly under English prompts. Additionally, Liu et al. (2023) reported that multilingual LLMs had limited abilities to reason with proverbs and exhibited a "culture gap" in handling translations, leading to the development of the MAPS dataset for assessing proverb comprehension in six languages.

### Cultural benchmarks and datasets

Extensive research has focused on developing cultural benchmarks, which can be categorized into two types: collecting existing datasets and synthesizing new ones. First, most of the work adopted existing datasets as sources of cultural data. Wang et al. (2023) introduced a benchmark that uses cultural items to analyze cultural dominance, based on sources such as WVS (Survey, 2022) and PCT (Mudde, 2016). Later work includes Cultural Alignment Test (Masoud et al., 2023), NORMSAGE (Fung et al., 2022), WorldValueBench (Zhao et al., 2024), and NORMAD (Rao et al., 2024) that sourced from different existing datasets. Other types of data sources include CultureAtlas (Fung et al., 2024) and MAPS (Liu et al., 2023) which collected data from Wikimedia; Candle (Nguyen et al., 2023) and CultureBank (Shi et al., 2024) sourced their data from social media such as Tiktok and Reddit. In contrast, there was an emerging trend to perform data augmentation for cultural LLMs. Li et al.

 proposed semantic data augmentation to synthesize cultural data by enriching the semantic equivalence of the generated samples.

CulturePark significantly differs from those that perform direct data collection from existing datasets; it also differs from CultureLLM [Li et al., 2024] since CulturePark leverages multi-agent communication for data generation, which is more natural and can generate more diverse datasets.

### Existing solutions to cultural bias

There are primarily two types of approach to addressing the problem of cultural bias: prompt engineering and pre-training. The work of [Kovac et al., 2023, Wang et al., 2023] viewed LLMs as amalgamations of cultural perspectives, which can be oriented toward specific cultural perspectives through prompt engineering. In contrast, Rao et al. [Rao et al., 2023] integrated cultural values directly into the prompts. Although prompt engineering is cost-effective, its efficacy is questionable, particularly in low-resource cultures where LLMs may lack relevant cultural knowledge due to underrepresentation in pre-training data. An alternative strand of research focuses on pre-training and fine-tuning [Pires et al., 2023, Chan et al., 2023, Nguyen et al., 2023b, Pipatanakul et al., 2023, Abbasi et al., 2023, Lin and Chen, 2023]. Those approaches developed culturally aware LLMs for various cultures by assembling large-scale pre-training datasets, followed by fine-tuning to enhance alignment. Despite achieving significant performance improvements, these methods were both costly and time consuming, making them impractical for a broader application across numerous cultures and countries. Furthermore, they still face challenges in low-resource cultures where acquiring pre-training data is difficult. For example, MaLA-500 [Lin et al., 2024] aimed to train a new LLM in Llama 2 to support \(534\) languages, illustrating the resource-intensive nature of this approach. Unlike these approaches, CulturePark provides a cost-effective solution to the cultural bias problem, including data augmentation and fine-tuning.

## 3 CulturePark

### Design

CulturePark is an LLM-powered4 cross-cultural communication framework that generates data to support culture-related research such as building cultural-specific LLMs and performing cultural alignment. It is inspired by Cognitive Conflict Theory (CCT) and Social Cognition Theory (SCT) to design multi-turn communications for a deeper understanding of cultural topics. CCT posits that cognitive conflicts can help individuals engage more in deeper thinking [Limon, 2001, Cosier and Rose, 1977], and SST emphasizes that individuals can deepen their understanding of perspectives through explanation and debate [Fiske and Taylor, 1991].

Figure 1 shows the overview of CulturePark. To enable English-based interaction, we design two types of cultural agents: the main contact and the cultural delegate. Specifically, the main contact agent, \(\)Lily, is from English culture and is responsible for all conversations with delegates from different cultures such as \(\)Abdul from Arabic and \(\)Javier from Spanish culture. The complete information of agents and culture is in Table 7. As shown in Figure 2(a), we input a system prompt to LLMs which contains the background setting and initial question to initiate the conversation. The initial question, such as "How do you think about one of my main goals in life has been to make my parents proud? Please provide your opinions and reasons", is obtained from WVS [Survey, 2022b] and GAS [Survey, 2022a], two popular cultural surveys whose examples are shown in Figure 7. After that, the agents conduct cross-cultural conversations to generate data. Currently, CulturePark supports \(8\) cultures and \(2\) genders while more cultures can be easily added. Those agents could conduct in-cultural or cross-cultural communication, while we rely on cross-cultural more since in-cultural communication will likely generate less diverse topics (e.g., Figure 10). We discuss the quality of data from in-cultural and cross-cultural communication and the influence of gender in Section 5.

We designed improved prompting techniques to maintain high-quality conversations. First, the cultural bias of the main contact and cultural delegate is reduced by designing _self-calibration_ prompts to calibrate their outputs. We use a seed datum that contains the attitude of the target culture to the input question to guide the dialogue. All the following statements should conform to the answer in seed. As shown in Figure 2(a), we introduce the opinion from Abdul's culture and ask Abdul and Lily to conform to their cultures. The effect of the self-calibration prompt is shown in Figures 13(a) and 13(b). Without self-calibration prompts, Abdul's opinions contradict with Arabic people. Second, the redundancy of the output, i.e., LLMs always generates similar dialogues after multi-turn communication. We devise two communication styles: one is _self-guidance_ prompts which can direct the dialogue to generate more diverse and informative data, such as "Are there anything in your culture related to the problem talked before?" and "Do you agree with her? Provide more reasons to support your idea?", and the other is free chat that does not need humans to participate and motivate the inner creativity of LLMs. Figures 11(a) and 11(b) show cases of self-guidance prompting and free chat, respectively.

### Data refinement and fine-tuning

The seed questions initiating the communication have two sources: World Values Survey (WVS) [Survey, 2022b] and Global Attitudes surveys (GAS) from Pew Research Center [Survey, 2022a]. WVS is a global research project that explores people's beliefs and values worldwide, examining how these beliefs evolve over time. Pew Research Center, a nonpartisan organization, provides data and research on public opinion, social issues, and demographic trends both in the U.S. and globally. Its Global Attitudes surveys cover a wide range of topics, including politics, media, technology, religion, race, and ethnicity. In total, we select \(4.1\)k seed data and generate \(41\)k dialogues (each dialogue contains several sentences). We show the details of the data numbers for different cultures in Table 8. We also performed a statistical analysis on the GPT-4-based dataset. As summarized in Figure 8, the dataset contains human belief (59.68%), norm (29.54%) and custom (10.78%) involving data on \(8\) different cultures. Figures 7 and 11(a) show some examples of the seed data and the generated dialogues.

The generated dataset may not be directly used for fine-tuning since it could contain redundant and incorrect information that should be handled. As shown in Figure 2(b), we design data refinement to refine the dataset. First, the opinions on target culture are extracted from the dialogues generated via GPT-4, such as "The Arabian equates their parents' happiness and satisfaction to their own success"and"The Arabian emphasize Sabr, which is about showing resilience, maintaining a positive attitude and having faith during difficult times". Second, several extracted opinions could be irrelevant to the initial question or contradict with seed data, motivating us to perform verification to reserve only highly related opinions. Furthermore, since the generated data could be semantically similar, we remove redundant samples to improve diversity. To be specific, we obtain sentence embeddings via text-embedding-3-small [OpenAI, 2024] and cluster the embedding using K-means. We reserve one sample for each cluster as representative data. Eventually, we get the high-quality cultural data for different cultures. The ablation of the refinement is in Table 2.

Algorithm 9 shows the pipeline for data refinement. After refinement, there are \(41\)k samples (input-output pairs) left for fine-tuning, i.e., one sample for one dialogue. Examples of the samples are provided in Figure 12. Afterwards, we can fine-tune cultural-specific LLMs using either open-source LLMs or fine-tuning service. In this paper, we mainly use OpenAI API to fine-tune GPT-3.5-Turbo due to its efficiency. Hyperparameters are shown in Table 6. We further provide fine-tuning experiments on Llama2-70b in Section 5.4.

### CulturePark benefits cultural understanding and fine-tuning

There are some interesting observations in communication among agents from different cultures.

**Communication triggers LLMs' cross-cultural understanding ability.** We observed that agents try to understand each other's opinions and the reasons sourced from their different cultural backgrounds. For example, the blue sentences in Figure 2(a) show cross-cultural understanding ability of LLMs, such as "I do understand and respect the sentiment of wanting to make parents proud, as they have often sacrificed a lot for their children". Leveraging GPT-4-Turbo, we analyzed the topics in the dataset such as human beliefs, norms, and customs,which can be further used as data collections for building culturally specific models. Appendix B.2 shows the details of the dialogue dataset, indicating that the generated topics are mostly about culture. Then, we randomly sampled \(750\) dialogues for each culture and evaluated the communication using the prompts in Appendix E. As summarized in Table 4, on average, the ratio of statements that express cross-cultural understanding is \(80.80\)%. The analysis also verifies the effectiveness of CulturePark in expanding topics and cross-cultural understanding.

**Cultural differences boost novel opinions.** In cross-cultural communication, different opinions can inspire others to think deeper and more comprehensively, as suggested by CCT and SCT. A case is the sentences in orange in Figure 2(a).

Lily partially agrees with Abdul and gives an accurate and high-level summary of her pursuit: "a balance between family expectations and personal happiness" which is generated after a multi-turn energetic discussion with Abdul. This also aligns well with CCT and SCT that emphasize the significance of communication among people having different cultural backgrounds.

**CulturePark naturally assists cultural data augmentation by creating novel questions and comprehensive answers.** On the one hand, agents in different cultures can generate new opinions towards certain topics, which intuitively diversifies the input questions. On the other hand, the initial seed data only contain short answers such as "Strongly agree" with no further explanations. Our platform allows deeper and more comprehensive communication of agents, thus generating more detailed responses such as "Strongly agree. I believe that pleasing parents and elders is a sign of respect and love" and "Strongly agree. I equate my parents' happiness and satisfaction to my own success". Additionally, agents can extend the topics that conflict with their own opinions and provide more informative evidence to support their viewpoints. This strategy helps to generate informative and diverse data continuously. Section 5.1 presents some detailed results on diversity gain, showing that the generated data has significantly larger diversity.

## 4 Experiments

### Evaluation on content moderation tasks

Setup.Content moderation is crucial to maintaining the integrity and safety of online platforms in different cultures. What is acceptable in one culture could be offensive or inappropriate in another. However, few methods focused on content moderation for different cultures. For this experiment, we evaluated the effectiveness of our cultural-specific models for \(8\) different cultures: Arabic, Bengali,

Figure 2: Cross-cultural dialogue and data refinement for fine-tuning LLMs using CulturePark.

Chinese, German, Korean, Portuguese, Spanish, and Turkish culture. These cultures have their unique characters, involving a large number of people in the world.

We evaluate on \(7\) content moderation tasks for \(8\) different cultures to detect the following content: hate speech, offensive language, spam speech, abusive speech, bias speech, threat speech, and stance of speech in zero-shot evaluation, whose metric is average F1 score. The details on the datasets can be found in Appendix C. In total, our test set contains \(48,895\) samples. We compare our models with seven baselines: GPT-3.5-turbo , GPT-4 , Gemini-pro , SeaLLM , TaiwanLLM , Synatra-7B-v0.3-dpo , EEVE-Korean-10.8B-v1.0 , CultureLLM , CultureBank . CultureLLM is a series of culture-specific LLMs using semantic data augmentation. SeaLLM focuses on the Southeast Asian (SEA) culture, which is adopted for Chinese and Korean cultures. TaiwanLLM focuses on traditional Chinese culture. Synatra-7B-v0.3-dpo and EEVE-Korean-10.8B-v1.0 targeted at Korean culture. CultureBank collects data from social media and we compare Arabic and Korean culture by fine-tuning GPT-3.5-turbo on its dataset.

Main Results.We analyzed the results from the culture and task sides in Figure 3. The most interesting observation is that our models outperformed GPT-4 on \(5\) cultures and approached GPT-4 on the remaining \(3\) cultures, although the data for fine-tuning are generated by GPT-3.5-turbo, which is much worse than GPT-4. We also generated cultural data via GPT-4 and fine-tuned other \(8\) cultural-specific models for comparison, denoted as "Ours-gpt4" in Figure 3. The performance of those models is better than "Ours" (GPT-3.5-turbo version) but not so much. For other baselines, our models outperform them in most cases. Table 1 shows that our models achieved better performance than those costly LLMs which require pre-training and fine-tuning.

Ablation study.Table 2 presents our ablation study on \(4\) cultures, where "Generate" means just extracting opinions from the dialogue, "Verify" represents factually verifying the extracted opinions, and "Diversify" means removing redundant data. The results show that each module of CulturePark is effective, ensuring its interpretability.

### Evaluation on cultural alignment via Hofstede's Cultural Dimensions Theory

Setup.Hofstede's cultural dimension theory is a framework for understanding cultural differences across countries based on data collected from various countries. We asked LLMs to answer the \(24\) questions in VSM 13 to evaluate cultural alignment. Specifically, we used a system prompt "You are a culture chatbot that knows culture very well" to induce LLMs' cultural understanding ability. We selected proper \(C^{5}\) and anchor LLMs' answer to Hofstede's old dataset. We compute the gaps between LLMs' answer and Hofstede's data from six cultural dimensions using the Euclidean distance. Details on the survey and the distance are in Appendix D.1.

  
**Chinese** & **Bus** & **Spain** & \(Avg\) \\  SeaLLM & 237 & 357 & 237 \\ TaiwanLLM & _446_ & 341 & 394 \\ Ours & _530_ & _584_ & _492_ \\  Arabic & Hate & Offensive & _Avg_ \\ CultureBank & _540_ & _642_ & 591 \\ Ours & _538_ & _738_ & _482_ \\  Korean & Abusive & Hate & _Avg_ \\  SeaLLM & 533 & 474 & 499 \\ Synatra-7B-v0.3-dpo & 320 & 465 & 428 \\ Cyprus-Korean-10.8B-v1.0 & 346 & 437 & 560 \\ ColumbiaKbank & 635 & 522 & 579 \\ Ours & _647_ & _640_ & _443_ \\   

Table 1: Comparison with the latest cultural specific LLMs.

   Model & Ar & Bn & Zh & Pt \\  GPT-3.5-turbo &.370 &.542 &.448 &.593 \\ Generate &.451 &.622 &.636 &.594 \\ Generate+Verify &.486 &.635 &.678 &.604 \\ Generate+Verify+Diversify &.514 &.644 &.692 &.603 \\   

Table 2: Results on ablation study of data generation and refinement.

Figure 3: Results on content moderation.

Results.We compared our models (powered by GPT-3.5-Turbo) with GPT-3.5-turbo and GPT-4. As shown in Figure 4, our models outperform them by a large margin, indicating their excellent cultural alignment and cultural understanding abilities. Note that VSM is widely adopted as datasets for value and culture alignment, the results imply that our approach for data collection is effective, thus it could be further used for value alignment research.

### Evaluation in situated learning for cultural education

Situated learning suggests that learning is best understood and facilitated when it occurs within the context (Anderson et al., 1996; Lave and Wenger, 1991). Motivated by situated learning, we leveraged CulturePark for cultural education where our fine-tuned models serve as foreigners to talk to people about cultural problems, which can create a situation for cross-cultural communication and learning cultural-specific knowledge. For example, a person who wants to learn about Arabic culture can communicate with our Arabic model.

Setup and study process.We hired \(24\) participants, each of whom was given an outline of cultural learning and asked to talk to models based on the outline. They can ask any related questions and express their opinions to models. Afterwards, the participants took a cultural understanding exam of VSM 2013 (Hofstede, 2013; Geert Hofstede, 2010) which they had never come into contact. We then computed the Euclidean distance between the ground truth and their answers from six cultural dimensions (rf. Section 4.2). For comparison, \(12\) participants learned with our models and the other \(12\) learned with GPT-4 to study 6 cultures: Arabic, Bengali, German, Korean, Portuguese, and Spanish culture. Each culture was learned by four participants, two of them learning with our models and the others learning with GPT-4. Detailed information on the participants can be found in Appendix D.2. During the study process, first, each participant was given an outline for cultural learning written by cultural experts. The outline (Appendix D.2), serves as the guideline for efficient learning. Then, we asked the participants to freely communicate with the models to learn about specific cultures. After the examination, we asked the participants to give a score of 1-5 to indicate their satisfaction with the learning process. In this study, our aim was to answer two questions: 1) What is the learning performance of the participants with our models and GPT-4? 2) How are their learning experience?

Results.Table 3 shows the averaged results of different participants. We have the following findings. First, participants learning with our models achieved better performance in cultural examination than those with GPT-4 in all cultures. This indicates that our fine-tuned models have a better cultural understanding than GPT-4. Second, participants are more satisfied with communicating with our models than GPT-4. Furthermore, many participants expressed that the responses from GPT-4 are vague. Even though we have prompted GPT-4 to be like a person from a specific culture, it always responds with neutral words that have no clear opinions or ideas. Instead, our models can provide straightforward opinions.

## 5 Discussion

### Why CulturePark benefits fine-tuning?

We analyze the effectiveness of CulturePark in benefiting cultural model fine-tuning from two different aspects: Communication vs. direct generation of LLMs and diversity of the generated data.

Figure 4: Results on culture alignment via Hofstedeâ€™s Cultural Dimensions Theory.

    &  &  \\  & GPT-4 & Ours & GPT-4 & Ours \\  Arabic & 89.89 & **69.57** & 4 & **5** \\ Bengali & 339.84 & **304.54** & 3 & **5** \\ Germany & 224.68 & **173.12** & 2 & **3** \\ Korean & 222.39 & **183.62** & 2 & **4** \\ Spanish & 143.33 & **102.53** & 4 & **5** \\ Turkish & 273.43 & **221.12** & 3 & **4** \\  AVG & 215.59 & **175.75** & 3 & **4.33** \\   

Table 3: Results on situated learning.

**Cross-cultural communication vs. direct generation from GPT-4 / GPT-3.5.** We compared the results of fine-tuning using data directly generated by GPT models (i.e., no communication). We generated such data by prompting GPT-4 or GPT-3.5-turbo as: "Question: {input} Answer: {output} Please list 10 reasons to support the answer and number them". Then, these data are used to fine-tune GPT-3.5-turbo. Figure 5(a) shows the performance on content moderation tasks in Chinese, Korean and Turkish cultures. We see that data directly generated from GPT-4 is better than that from GPT-3.5, while our GPT-3.5-based models can outperform both of them.

**Diversity of the generated data.** We also analyzed the diversity gain  of the generated data for quality evaluation. We compared with CultureLLM  and presented the results in Figure 5(b). It indicates that CulturePark can generate more diverse and high-quality data.

### Exploring agents' cultural background and gender

To explore the influence of agent's cultural background and gender, we conducted three types of multi-agent communications in Arabic culture: "In-cultural+Different gender", "Cross-cultural+Same gender", and "Cross-cultural+Different gender".6 For each setting, we fine-tuned three different models, whose training data is \(500\), \(750\), and \(1000\), respectively. We evaluated the performance of the models on content moderation tasks and presented the results in Figure 5(c). "Cross-cultural+Different gender" exhibits the best performance and ability to generate more high-quality data. This indicates the necessity of bringing more diversity in data generation, as conducted in CulturePark.

### Fine-tuning vs. forgetting

A potential dilemma arises when fine-tuning a large language model for specific tasks, as it may result in catastrophic forgetting of its original capabilities. This section explores the extent of forgetting exhibited by CulturePark on BIG-Bench-Hard (BBH) , which comprises 21 tasks that assess semantic understanding and logical reasoning. For cost efficiency, we sampled \(100\) samples from each BBH task. We evaluated our models against the baseline model, GPT-3.5-turbo. The results in Figure 5(d) indicate that CulturePark generally maintains or even improves performance on most benchmarks, including the 21 tasks in BBH. This improvement suggests potential latent relationships between cultural data and general benchmarks, implying that fine-tuning of cultural data could improve general reasoning abilities.

### Open-source fine-tuning with Llama2-70b

To verify the generalization ability of our framework, we leveraged the generated data to fine-tune cultural-specific Llama-2-70b models and evaluate on content moderation tasks. As shown in Figure 6,

Figure 5: More discussions on CulturePark.

Figure 6: Results of CulturePark-Llama on content moderation for different cultures.

our models outperform Llama-2-70b in all \(8\) cultures, especially in German, Chinese, Bengali and Portuguese cultures, which cover both low- and high-resource cultures. Furthermore, our models are also excellent in all \(7\) tasks.7 This verifies the generalization of CulturePark as an effective data collection platform.

## 6 Conclusions, Societal Impact, and Limitations

This paper introduced CulturePark, an LLM-powered multi-agent framework for cultural data collection through multi-agent communication. CulturePark can generate high-quality and diverse cross-cultural dialogue, which can be used to fine-tune culturally specific LLMs. We evaluated CulturePark across three downstream tasks: content moderation, cultural alignment, and cultural education, indicating great improvement over GPT-4.

CulturePark enhances fairness and inclusivity, reduces discrimination, and ensures accurate cultural representation. It improves global communication, fosters cross-cultural understanding, and supports multilingual societies. It benefits as bias-free LLMs build trust and align with responsible principles. Economically, it expands market reach and drives innovation. Social harmony improves by reducing stereotypes and preserving cultural heritage. It also aids compliance with anti-discrimination laws and supports inclusive education, promoting cultural awareness. Addressing cultural biases in LLMs creates more just, reliable, and beneficial AI systems, contributing to a more equitable world.

Our work has the following limitations. 1) More experiments can be performed by replacing GPT-3.5-Turbo in CulturePark to discover more results. 2) Our fine-tuned models are mostly for high-resource cultures. The reason is that the dataset and benchmark on low-resource cultures are rare, and we can not find enough data for fine-tuning and evaluation. 3) More efficient fine-tuning techniques can be studied to support the fine-tuning of culturally specific LLMs.

## Disclaimer

The human study was conducted following local laws and regulations, and the evaluation process was controlled to ensure that no irresponsible content was generated. The authors respect all cultures studied in the world. The results of the paper may change due to the change in OpenAI API and its model versions.