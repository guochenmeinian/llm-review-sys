# Benchmark of Machine Learning Force Fields for Semiconductor Simulations: Datasets, Metrics, and Comparative Analysis

Benchmark of Machine Learning Force Fields for Semiconductor Simulations: Datasets, Metrics, and Comparative Analysis

Geonu Kim\({}^{1}\)

Equal contribution.

Byunggook Na\({}^{1}\)1

Gunhee Kim\({}^{1}\)2

Hyuntae Cho\({}^{1}\)

Seung-Jin Kang\({}^{1}\)

**Hee Sun Lee\({}^{1}\)**Saerom Choi\({}^{1}\)**Heejae Kim\({}^{1}\)**Seungwon Lee\({}^{1}\)

\({}^{1}\)Samsung Advanced Institute of Technology (SAIT)

{geonu.kim, byunggook.na, ghij.kim, robert.cho, sj1222.kang}@samsung.com

{heesun88.lee, sincere.choi, heejaeee.kim, seungw.lee, yd.mlg.kim}@samsung.com

###### Abstract

As semiconductor devices become miniaturized and their structures become more complex, there is a growing need for large-scale atomic-level simulations as a less costly alternative to the trial-and-error approach during development. Although machine learning force fields (MLFFs) can meet the accuracy and scale requirements for such simulations, there are no open-access benchmarks for semiconductor materials. Hence, this study presents a comprehensive benchmark suite that consists of two semiconductor material datasets and ten MLFF models with six evaluation metrics. We select two important semiconductor thin-film materials _silicon nitride_ and _hafnium oxide_, and generate their datasets using computationally expensive density functional theory simulations under various scenarios at a cost of 2.6k GPU days. Additionally, we provide a variety of architectures as baselines: descriptor-based fully connected neural networks and graph neural networks with rotational invariant or equivariant features. We assess not only the accuracy of energy and force predictions but also five additional simulation indicators to determine the practical applicability of MLFF models in molecular dynamics simulations. To facilitate further research, our benchmark suite is available at [https://github.com/SAITPublic/MLFF-Framework](https://github.com/SAITPublic/MLFF-Framework).

## 1 Introduction

The evolution of semiconductor devices has been guided by two main factors: miniaturization and sophisticated design implementation [1; 2; 3]. Understanding complex device structures and fabrication processes requires high-resolution metrology methods, such as scanning electron microscopy and transmission electron microscopy [3; 4]. However, these instruments have limitations in terms of providing a restricted view, requiring substantial time and cost for analysis. In this context, large-scale atomic simulations are becoming increasingly important as a viable alternative approach that is capable of accurately capturing device-level dynamics. Although ab initio simulations based on the density functional theory (DFT) can provide accurate atomic simulations, substantial computational demands make them unsuitable for large-scale simulations [5; 6; 7]. As a practical substitute, molecular dynamics (MD) simulations combined with machine learning force fields (MLFFs) have gained significant attention. MLFFs aim to achieve a precision comparable to that of DFT-based simulations and relieve their computational costs.

Obtaining reliable datasets and benchmarks for studying semiconductors in the condensed phase presents significant challenges, which hinders the development of accurate MLFF models. Generating precise datasets for condensed-phase materials is challenging because of the complex structures, dynamics, and large number of atoms involved. To foster the development of MLFFs in the field, we introduce two new datasets, which are specifically designed for semiconductor **a**dvanced **m**aterials **d**iscovery, called SAMD23 datasets: silicon nitride (SiN) and hafnium oxide (HFO). We conducted DFT simulations under various conditions, including initial structures, stoichiometry, temperature, strain, and defects, resulting in a cost of 2.6k GPU days.

Although we built datasets with broad coverage, dynamic simulations exhibit an enormously wide range of atomic configurations with high degrees of freedom, indicating that attempting to collect all these configurations through computationally expensive ab initio simulations is not feasible in practice. Hence, MLFF models must be capable of extrapolation, which enables them to yield reliable predictions of configurations that are absent from the training dataset. Generally, to assess the extrapolation capability, an evaluation of the energy and force on out-of-distribution (OOD) test datasets, in addition to in-distribution (ID) sets, is employed. However, the energy and force errors may not be sufficient to account for the simulation behavior [8; 9]. Thus, we additionally provide five simulation indicators and correspondingly prepare material structures for both ID and OOD sets, facilitating a comprehensive comparison of the MLFF models.

Moreover, we offer a consolidated framework that streamlines model development, training, and evaluation processes into a unified platform. We curated diverse models that utilize hand-crafted features as atomic representations or employ graph neural networks as feature extractors. For the benchmark, 10 MLFF models were trained and evaluated using the framework, suggesting a reliable model selection policy based on the simulation indicators. Based on a comparative analysis of the training models with various hyperparameters, we suggest a baseline training recipe.

This paper makes the following contributions:

* We introduce SAMD23, two new MLFF benchmark datasets that reflect semiconductor simulations of SiN and HfO under various scenarios.
* We provide a framework to facilitate model development, and present benchmarks for SiN and HfO, along with five simulation indicators to assess the prediction performance in simulations and the extrapolation capability.
* We suggest a baseline training recipe and model selection policy to employ the model for simulations by performing a comparative analysis of 10 MLFF models.

## 2 Background and Related Work

Machine Learning Force Fields.Given a snapshot of \(n\) atoms, consisting of atomic numbers \(^{n}\) and atom positions \(^{n 3}\), an MLFF model approximates the potential energy \(E\) and forces \(^{n 3}\) (see Figure 1). MLFF models can utilize hand-crafted representations of local atomic environments, called descriptors obtained by experts in physics and material science research fields, or automatically represent the local atomic environments by adopting deep neural networks. The former method combines descriptors with different learning algorithms, such as linear regression , kernel methods [11; 12], and neural networks [13; 14; 15]. Recently, the latter method, that is, graph neural networks (GNNs) have shown remarkable results in predicting energy and forces by learning the features of atoms and their connectivity in an end-to-end manner instead of using descriptors. While some GNNs [16; 17; 18; 19; 20] rely on rotational invariant features such as interatomic distances and angles, others have rotational equivariant features derived from spherical harmonics [21; 22; 23; 24; 25].

MLFF Benchmark.Numerous MLFF benchmarks, including ANI , rMD17 , COLL , and 3BPA , have been publicly released for the application of MLFF to molecular structures. These datasets, which mainly comprise molecular structures with a small number of atoms, have been extensively utilized in the development of various MLFF models. The open catalyst 20 (OC20)  and OC22  datasets were generated through simulations of the relaxation of a molecule on bulk surfaces. OC20 primarily investigates the relaxation behavior of diverse adsorbates on various catalyst surfaces. OC22, an enhancement of OC20, focuses oxygen evolution reactions on oxide electrocatalysts. While both OC datasets predominantly focus on surface reactions, they are less optimal for semiconductor studies, which necessitate an understanding of the phenomena inside solids or at the interface between two solids. In 2023, Fu _et al._ introduced a benchmark for simulation stability using the time evolution of radial distribution functions and bond lengths. Their framework assessed model stability across small molecules to solids in equilibrium MD systems. They highlighted the risk of relying solely on force accuracy for MLFF evaluation, offering new insights for the MLFF community. However, in semiconductor research, where phase transitions, interphase reactions, and defect dynamics are pivotal, systems often deviate from equilibrium. This points out the need for adapted metrics that are more aligned with semiconductor-specific material properties.

In addition, various condensed-phase datasets  have been shared in the existing literature and utilized for model evaluation. However, for the general ML community without prior experience in computational materials modeling, evaluating various material properties beyond energy and force using publicly available benchmarks is difficult. Thus, we offer not only an essential dataset for thin-film research, but also material property benchmarks for comprehensive model evaluation, allowing users with no prior experience in the field to evaluate models through actual simulations.

## 3 SAMD23 Datasets

SAMD23 includes two semiconductor material datasets: SiN and HfO. SiN is increasingly used in various semiconductor applications, particularly as a storage medium in advanced next-generation flash memory devices . HfO is typically used as a high-k material and a crucial ferroelectric material in complementary metal-oxide-semiconductor technology, showing great potential for emerging electronics applications . These two datasets are presented here to investigate the properties and behavior of SiN and HfO thin-film materials and provide a valuable resource for the development and evaluation of MLFF models. For details on dataset usage and generation methods, please refer to Section A.1.

**Composition.** The composition of the datasets is summarized in Table 1, and the details are listed in Tables A.1 and A.2. All the datasets were released in extended-xyz format (see Section A.5 for details). The SiN dataset encompasses various stoichiometric ratios, with atom counts ranging from 16 to 510. Meanwhile, the HfO dataset comprises 96 atoms, exhibiting a 1:2 ratio of 32 Hf atoms to 64 O atoms. More detailed information on characteristics of the datasets is provided in Section A.2.

### Generation Methodology

To prepare the raw data, ab initio simulations were performed using Vienna ab initio simulation package (VASP)  with the Perdew-Burke-Ernzerhof generalized-gradient approximation exchange-correlation functional . Convergence tests were conducted to determine the k-point meshes and cutoff energies for accurate DFT calculations, with a maximum energy convergence of 2 meV/atom and a maximum force error convergence of 50 meV/A. The crystal structures of SiN and HfO were sourced from the Materials Project  and AFLOW databases  (see Section A.3 for details). For both SiN and HfO, we prepared ID and OOD datasets, which are exclusively sampled

    & Element & \(N_{}\) & Snapshots & Datapoints & \(N_{}\) & \(N_{}\) & \(N_{}\) & Cost (h) \\   & Si\({}_{m}\) N\({}_{n}\) & 92 & 76,213 & 4,397,744 & 20,315 & 2,542 & 2,585 & 29,824 \\  & Si & 14 & 6,250 & 291,600 & 1,663 & 212 & 213 & 2,963 \\  & N & 4 & 2,000 & 128,000 & 532 & 68 & 68 & 3,769 \\   & Total & 110 & 84,463 & 4,817,344 & 22,510 & 2,822 & 2,866 & 36,556 \\  SiN\({}^{}\) & Si\({}_{m}\) N\({}_{n}\) & 3 & 3,700 & 388,500 & - & - & 1,235 & 1,166 \\   HfO & Hf O\({}_{2}\) & 60 & 160,000 & 15,360,000 & 27,960 & 3,510 & 3,510 & 19,341 \\  HfO\({}^{}\) & Hf\({}_{m}\) O\({}_{n}\) & 12 & 32,000 & 3,072,000 & - & - & 6,996 & 4,182 \\   

Table 1: Summary of the semiconductor datasets. \(N_{}\) represents the total number of simulation conditions. \(N_{}\), \(N_{}\), and \(N_{}\) indicate the number of snapshots contained in train, valid, and test sets, respectively. The generation cost is measured by GPU hours using Nvidia V100.

from the raw data. The ID datasets were randomly divided into three groups of a ratio of 8:1:1, referred to as train, validation, and test sets, respectively.

**Silicon Nitride.** The SiN dataset was derived from 110 independent DFT simulations, encompassing diverse structures including amorphous and crystals (\(\), \(\), and \(\)), one- or two-dimensional defects, surfaces, and isolated nano-clusters, which are illustrated in Figure 1. Simulations were performed at various temperatures, strains, and ensembles, with the number of atoms per unit cell ranging from 16 to 510. The SiN simulations encompassed various stoichiometries of SiN as well as Si-only and N-only structures. The ID dataset consists of samples obtained from these simulations with a sampling interval of 9 fs. To generate the OOD dataset, we employed amorphous SiN as the initial structure, which was used to study the synthesizability of inorganic materials , and conducted a melt-quench-relaxation simulation. For additional details, please refer to Table 2.

**Hafium Oxide.** The HfO dataset was created using a modified melt-quench-annealing (m-MQA) method, in which temperature varied, as illustrated in Figure 6. The MQA method, which inspired our m-MQA method, has been frequently used to create datasets . To obtain high entropy structures, we incorporated three pre-melting stages at an extremely high temperature of 5000 K in the m-MQA method. For full method details, refer to Section A.4. We conducted 12 MD scenarios (yielding 60 simulation conditions) using the m-MQA method on five types of HfO\({}_{2}\) crystals (_i.e._, monoclinic, tetragonal, cubic, and two orthorhombic structures) along with seven randomly generated structures. Each structure had 96 atoms (32 Hf and 64 O) to maintain the HfO\({}_{2}\) stoichiometry. Randomized structures were generated by randomly distributing 32 Hf and 64 O atoms within the unit cells of the HfO\({}_{2}\) crystals. Of the 12 simulations performed, simulations for 10 structures (five crystals and five random structures) and the remaining two random structures were utilized as the sources of the ID and OOD datasets, respectively. We sampled snapshots from each source with a sampling interval of 9 fs in the three pre-melting and melting stages and an interval of 12 fs in the quenching and annealing stages.

**SiN vs. HfO.** SiN data generation demonstrates a wider range of atomic configurations, encompassing various experimentally validated phases and defect structures. Conversely, HfO data generation employs the efficient m-MQA method, which is introduced in this report as a user-friendly approach for researchers proficient in DFT. Further, SiN data generation covers a wide range of stoichiometries and numbers of atoms (16-510 atoms), whereas HfO data generation focuses solely on a composition of Hf:O = 1:2, involving 96 atoms. Therefore, the model training results with the two datasets exhibited noticeably different aspects (see Section 4.2).

## 4 Numerical Benchmark: Energy and Force

MLFF models have their own training schemes on a target dataset, involving data preprocessing methods, loss functions, and hyperparameter settings, all of which can significantly influence the training results. To ensure a fair benchmark, we have empirically but carefully selected a common training scheme tailored for our datasets, as discussed in Section 4.1; the details are explained in Section C. Sections 4.2-4.4 provide the benchmark results on SiN and HfO, including an ablation study and discussions on the model inference speed and data scaling effect.

### Benchmark Setting

**Error Metric.** Table 1 lists the errors and loss functions used in previous MLFF studies. In this benchmark, we selected the sum of the root mean square errors (RMSEs) of the per-atom energy and force, denoted by the **EF metric**, as the metric for evaluating energy and force predictions for the following reasons.

For the energy error metric, two types of energy errors for a snapshot can be used: the total energy error and the per-atom energy error computed by dividing the total energy error by the number of atoms in a snapshot. However, the total energy error depends on the number of atoms in a snapshot, whereas the per-atom energy error remains invariant. Since our SiN dataset consists of snapshots of various sizes, we use the per-atom energy error. Subsequently, among the two per-atom energy errors, the mean absolute error (MAE) and RMSE, we employ **the RMSE of per-atom energy** as the energy error metric. For stable MD simulations, it is crucial to recognize the potential risks stemming from significant errors that can result in catastrophic failure. The RMSE can capture such risks when assessing the extrapolation capability of MLFF models using OOD setsn rather than the MAE; this is evident in Section 4.2.

In the case of force error metrics, three error types are widely used in ML research: component-wise MAE, RMSE, and MAE with the size of force vectors (L2MAE). However, component-wise MAE has a critical flaw: the absence of rotational invariance (_i.e._, if a snapshot is rotated, the component-wise absolute error of forces is changed), which is a highly recommended feature for a force field. Among the remaining two candidates, L2MAE and RMSE, which support rotational invariance, we employ **the RMSE of force** for the same reason as with the energy metric.

**Loss Function.** A loss function integrating the errors of energy and force is commonly used to train MLFF models. Since we employ the per-atom energy, the total loss function has the form (1),

\[_{tot}=}{|B|}_{i B}_{E} }{n_{i}},_{i}}{n_{i}}+}{|B|}_{i B}_{}(_{i},}_{i}), \]

where \(B\) indicates a batch of snapshots, \(_{E}\) (or \(_{}\)) indicates energy (or force, respectively) loss, and \(_{e}\) (or \(_{f}\)) is a weight of energy (or force, respectively) loss. In this benchmark, aligning the loss function to the EF metrics, we set \(_{E}\) by the MSE of per-atom energy and \(_{}\) by the MSE of forces; thus, we refer to it as **MSE-based loss**. Moreover, to secure generalization performance, we also tested setting \(_{E}\) by the MAE of per-atom energy, and \(_{}\) by the L2MAE of forces, adopted by some MLFF models that showed promising results [19; 47; 25]; the loss is called **MAE-based loss** in this paper. The exact formulations of the losses are shown in Table C.1. Generally, \(_{e}\) is set by 1, and \(_{f}\) are determined in various ways, such as manually setting it as a constant [16; 17; 19; 25], adjusting it based on the snapshot size [22; 23; 24], or gradually modifying it during the training . In this benchmark, we set default \(_{f}\) by 1, and performed an exploration study.

**Graph Generation.** To enable models to effectively capture atomic interactions, it is necessary to generate graphs from the snapshot, which represents point cloud data of atom coordinates. The most straightforward approach to convert snapshots to graphs is to build edges between nearby atoms using a cutoff radius; we set the cutoff radius to **6A**. Further, owing to high training cost and insufficient GPU memory for some models, we restricted the maximum number of neighborhood atoms as **50**, from the nearest. The effect of the restriction is discussed in Section C.3.1.

### Results and Analysis

We performed benchmarks using 10 models on our datasets: BPNN , DPA-1 , SchNet , DimeNet++ , GemNet-T/-dT , NequIP , Allegro , MACE , and SCN , all of which are described in Section B. The detailed training recipes are presented in Section C.2.

#### 4.2.1 Model Comparison

Overall, as depicted in Figures 1 (a) and (b), the GNN-based models, except for SchNet, are superior to the two descriptor-based models, BPNN and DPA-1. Surprisingly, across all sets, DimeNet++ outperforms recently proposed GNN-based models. Nevertheless, as shown in Section 5.3, DimeNet++ exhibits a significantly low performance for one of the simulation indicators for HfO. This phenomenon is consistent with the results reported in a recent study  arguing that accurate force predictions do not fully represent the accuracy and applicability of MLFF models in simulations.

For every model, the EF metrics of the OOD set are larger than those of the test set, aligned with our intention. In both datasets, SchNet exhibits the largest discrepancy between the EF metrics of the train and test sets as well as the highest EF metric of the OOD set, indicating that SchNet has a potential risk of significant overfitting. BPNN, DPA-1, and Allegro also have relatively high EF metrics for the OOD set. Consistent with the results of the OOD set, these four models display unfavorable performance, as measured by the simulation indicators (see Section 5.3).

GNNs incorporating rotational equivariance (_e.g._, NequIP, Allegro, and MACE) show relatively smaller differences between the EF metrics of the train and test sets compared with the other models, highlighting the advantageous aspects of rotational equivariance. However, in HfO, a substantial discrepancy between the EF metrics of the test and OOD sets is observed for Allegro and MACE. Considering that both models are based on the atomic cluster expansion (ACE) theory , searching for factors that contribute to such discrepancies would provide key insights for improving ACE-based GNNs.

#### 4.2.2 Training Factor Exploration Study

**MSE- vs. MAE-based Losses.** Additionally, we trained the models using the MAE-based loss. Figures 1 (c) and (d) present an EF metric comparison between the models trained using the MAE-based loss and those trained using the MSE-based loss, respectively. The results obtained for SiN clearly indicate the substantial superiority of the MAE-based loss. However, for HfO, the superiority cannot be easily determined; in most cases, the differences of EF metrics are relatively small. Specifically, while Allegro shows a significant performance drop on the HfO OOD set, the accuracy of MACE is improved.

**Coefficient of Force Loss (\(_{f}\)).** For all models, force errors are dominant in the terms of EF metrics. Thus, if we design a training loss to reduce force loss by varying the force coefficient \(_{f}\), the test force errors can be also reduced accordingly. To investigate the effect of \(_{f}\), we trained GemNet-dT and MACE using the MAE-based loss with \(_{f}\) ranged from 0.1 to 10; Figure 2 shows the results. Consistent with expectations, the models trained with \(_{f}<1\) show increased errors compared to \(_{f}=1\). However, the HfO results of MACE with \(_{f}=10\) reveal that increasing \(_{f}\) does not always lead to a decrease in the force error and results with \(_{f}>10\) may be worse. Consequently, when exploring a proper \(_{f}\) as a hyperparameter tuning, it is recommended to train MLFF models with \(_{f}\) in the range of \(1_{f}<10\).

### Model Exploration Study

As a goal of MLFF models, it is important to not only achieve the precision of DFT-based simulations, which provide reference data, but also make MD simulations as fast as possible. Thus, Figure 3 illustrates a Pareto plot between the inference time per snapshot and the EF metric of each model, where these two factors show a trade-off relationship. We performed the inference five times on the HfO test set with a batch size of 1, which mimics a step-by-step process in the simulations, using a V100 GPU. To further explore the tradeoff, we trained models whose feature dimensions varied from

Figure 1: (a, b) EF metrics of the models trained with MSE-based loss. (c, d) Difference between EF metrics of models trained with MAE- and MSE-based losses, where the positive value infers training with MSE-based loss is superior.

Figure 2: EF metrics obtained by models trained with varying \(_{f}\) in training lossthe base architecture used in Section 4.2.1. The relative variation scale was selected from {0.25x, 0.5x, 2x}; the variation choices for each model are presented in Section C.4.2. To ensure clarity, no variant models within each model that fell outside the trade-off were included.

All GemNet-dT models are located on the Pareto-frontier curve. Similar to energy, GemNet-dT predicts forces only by the forward process in GNNs, instead of the conventional calculation in MLFFs, where the forces are calculated by the derivatives of energy with respect to the atom positions to satisfy the energy conservation law. The direct force prediction of GemNet-dT does not satisfy the energy conservation law, leading that the property benchmark results of GemNet-dT fall significantly short of those of GemNet-T (see Section 5.3). However, GemNet-T required a 2.2 times longer inference time and a much larger GPU memory than GemNet-dT, implying that GemNet-dT is more suitable for large-scale simulations by handling more atoms in less time. Therefore, when considering the aforementioned trade-off, a model development approach based on direct force prediction is a promising direction.

Although SchNet is the fastest, it yields the highest error, and thus, may generate unreliable simulations. SCN, which also adopts direct force prediction, achieves an EF metric comparable to that of GemNet-dT; however, its computational requirement is over 10 times higher, making it unsuitable for simulations. Except for GemNet-dT, Allegro and MACE show the Pareto-frontier results indicated by the dashed line. If the significant discrepancy in the EF metrics between the test and OOD sets is resolved, they can be employed in simulations where the energy conservation law should be satisfied.

### Data Scaling Effect

We also present the data-scaling effect, which may be helpful in adopting a training strategy for MLFF researchers. We randomly sample 20%, 40%, 60%, and 80% snapshots from the HfO training

Figure 4: Data scaling effect. The EF metrics are obtained from six MLFF models trained by using 20%, 40%, 60%, 80%, and 100% snapshots of the HfO training set. For all the points of Test (red) and OOD (yellow) curves, the test and OOD sets are identically used to obtain EF metrics. At each point of Train (blue) curves, the correspondingly sampled training set is used.

Figure 3: Pareto plot for inference time per snapshot vs. EF metric on HfO test set.

set (_i.e._, 5.6k, 11.2k, 16.8k, and 22.4k), and trained models by 1000, 500, 334, and 250 epochs, respectively, to train models by the same iterations; the detailed setting is described in Section C.5 We selected six models and evaluated them on the HfO dataset.

As shown in Figure 4, for all models, as more training data are used, the training error increases while the test error decreases, and the gap between these two errors also decreases. In most cases, the OOD error decreases. These results suggest that training with fewer epochs and more data can improve the generalization performance of MLFF models. Meanwhile, the difference between the EF metrics of the models trained using 80% and 100% snapshots of the training set is marginal, indicating that performance cannot be improved using more data. This observation implies that the training set sampled from the raw dataset is sufficient for our semiconductor MLFF benchmark.

## 5 Property Benchmark: Simulation Indicators

To evaluate the accuracy and reliability of the models in predicting material properties, we introduce a set of five indicators: radial distribution function (RDF), angular distribution function (ADF), bulk modulus (B\({}_{0}\)), equilibrium volume (V\({}_{0}\)), and various potential energy curves (PECs). In this study, the indicators are categorized into two types depending on the necessity of the MD simulation during evaluation: RDF and ADF are classified as dynamic indicators, whereas the remaining indicators are classified as static indicators. An evaluation of any model using these indicators can be easily performed within our framework. By utilizing these indicators, researchers can enhance their understanding of the MLFF model behavior and performance.

### Dynamic Indicators

The dynamic indicators, RDF and ADF, are derived from high-temperature MD trajectories, allowing us to assess the stability of the models in different atomic environments affected by turbulence or active movement induced by high thermal energy. For SiN (1200 K) and HfO (1200 K and 1800 K), high temperatures were carefully selected to investigate the model stability. On the one hand, RDF, also known as the pair correlation function, captures density changes relative to the distance from a chosen reference particle. On the other hand, ADF expands the analysis beyond radial distances by characterizing the angular distribution of particles surrounding a reference particle. The analysis of RDF and ADF is fundamental in simulations as it aids in identifying structures, phases, and interactions while providing a deeper understanding of the behaviors and reactions occurring within solids. We conducted evaluations using various structures and a wide range of supercells, consisting of 1,296-2,835 atoms for SiN and 2,592-3,456 atoms for HfO, highlighting the significant number of atoms involved (Table D.1). We have calculated representative values (RDF\({}^{}\), RDF\({}^{}\), ADF\({}^{}\), and ADF\({}^{}\)) for the evaluation results of RDF and ADF; the details are described in Section D.1.

### Static Indicators

The static indicators, B\({}_{0}\), V\({}_{0}\), and PECs, are derived from the relationships between input structure and energy, without requiring MD trajectories, offering valuable insights into the atomic and structural characteristics of materials.

**Bulk Modulus and Equilibrium Volume.** The Birch-Murnaghan equation of state (EoS) provides crucial information about thin-film materials, such as SiN and HfO, through two parameters: B\({}_{0}\) and V\({}_{0}\). These parameters are crucial for studying film stability, elasticity, and interfacial properties. B\({}_{0}\) signifies the response of films to pressure and external stress and indicates their flexibility, hardness, and resistance to volume changes. V\({}_{0}\) helps determine the films' thickness-residual stress relationship, enabling evaluation of their state (strained or relaxed) and refinement of the growth procedure. Understanding these parameters is vital for engineering thin-films with desired properties for potential applications in electronics, optoelectronics, and micro-electromechanical systems. To compare the models, we obtained the final scores for each model (_i.e._, B\({}_{0}^{}\), B\({}_{0}^{}\), V\({}_{0}^{}\), and V\({}_{0}^{}\)) through the EoS evaluation for different structures. For further information regarding the calculation, please refer to Section D.2.

**Potential Energy Curves.** The performance of the MLFF models at the atomic level was assessed using PECs. This indicator focuses on evaluating systems that have a sparse atomic environment with a small number of atoms, unlike the dense periodic boundary conditions encountered in our solid-state structure dataset. By assessing the model's ability to accurately represent the interactions and dynamics in sparse environments, this evaluation serves as an OOD assessment. This is particularly relevant in scenarios involving molecular reactions, surface interactions, or nanoscale systems. Understanding the model's performance in these contexts provides valuable insights into its generalization capabilities beyond the bulk behavior observed in solid-state structures. For detailed information on PEC evaluations, please refer to Section D.3.

### Results and Discussion

Figure 5 shows a comparison of the models based on the EF metric, incorporating dynamic and static indicators. For detailed information on the calculation of scores, please refer to Section D.4. Each model was trained with either MAE-based or MSE-based loss. Among them, the model with the highest sum of all indicator scores is shown in Figure 5.

**Model Comparison.** Overall, for both SiN and HfO, GemNet-T, NequIP, MACE, and SCN achieve prominent results; however for SiN, V\({}_{0}\) of SCN exhibits modest performance. The descriptor-based models, BPNN and DPA-1, show intermediate performance on both datasets. SchNet shows a significant mismatch in B\({}_{0}\) for both datasets and have the poorest generalization performance.

**Correlations of Metrics.** In the case of HfO, DimeNet++ and GemNet-dT achieve excellent results in terms of their EF metric scores for the ID and OOD sets, as shown in Figure 1. However, the performances of the two models in terms of the simulation indicators for OOD are not consistent. DimeNet++ shows a significant decrease in OOD performance for V\({}_{0}\) prediction, while GemNet-dT exhibits a substantial decrease for both V\({}_{0}\) and B\({}_{0}\) predictions. This indicates the limitations of the existing evaluation approach, which is primarily focused on energy and force, emphasizing the need for evaluation metrics related to material simulations and properties. For SiN, DimeNet++ and GemNet-dT do not show such a pronounced trend, which could be attributed to the characteristics of the dataset.

Figure 5: Comprehensive comparison of models based on EF metric and simulation indicators, where red and blue plots visualize the results of ID and OOD, respectively. Model names colored by black and red represent the models trained using MSE- and MAE-based loss, respectively.

Unlike SiN, HfO was generated using the m-MQA method at only one composition ratio (1:2), thus making it relatively challenging to evaluate OOD performance compared to SiN, which was created by considering various defects, compositions, structures, and environments. Although evaluating the EF metric for OOD alone cannot guarantee the performance of the simulation indicators, the EF metric for OOD exhibited a higher Pearson correlation with the simulation indicators than the EF ID evaluation (see Figure D.7). This suggests that the simulation indicators can serve as a guide for evaluating the extrapolation performance of MLFF models.

Simulation Failure.In terms of the OOD evaluation of the dynamic indicators, Allegro performs moderately on SiN but scores 0 on HfO. This difference is due to the interrupted HfO simulations caused by unrealistic atomic collisions during high-temperature simulations (Figure D.8). These interrupted simulations show abnormal atomic movements, with two Hf atoms attracting each other despite being too close, violating the expected repulsion behavior and terminating the simulation. These issues in HfO can be attributed to the differences in the datasets. In the SiN dataset, simulations of single element systems, such as dense structures consisting solely of Si and N elements, were included, whereas the HfO dataset lacked such simulations. Consequently, relatively more datapoints in the SiN training set encompassed situations with short interatomic distances, allowing the model to better learn and handle these scenarios. The variation in dataset characteristics is also evident in the PEC evaluation, which is discussed in further detail in Section D.3.2.

## 6 Conclusion

We introduced an MLFF benchmark suite consisting of SAMD23 datasets with two semiconductor materials and ten state-of-the-art models with six evaluation metrics. The experimental results show that benchmarking only the energy and force prediction errors is insufficient, and simulation-based metrics should be used to reflect the practical utility of a model. In addition, evaluation using the OOD sets is important to correctly understand the prediction performance of models. Moreover, although recently developed GNN-based models such as GemNet-T, NequIP, and MACE have shown relatively better simulation quality, no clearly superior models emerged. If we consider the predictive performance of energy and forces and inference time for large-scale simulations, GemNet-dT becomes a viable option; however, it should be improved to operate properly in simulations. We anticipate that our benchmarks will encourage future developments in the field, resulting in MLFF models that satisfy the accuracy, speed, and scalability requirements. We hope that our benchmark will provide valuable insights into the field of electronics.

Limitations.The model that was shown to perform well in other fields (_e.g._, molecules) may have been underestimated in this benchmark. Unlike datasets focused on small molecule structures, this study considered condensed-phase systems containing many atoms, making it practically impossible to include all possible atomic environments in the dataset. In addition, simulations handling long-range interactions beyond a few hundred nanometers were not included in the training data, because the high computational cost of ab initio simulations limits the number of atoms. Despite using periodic boundary conditions and including larger structures, our DFT-based data generation scheme cannot fully account for long-range interactions.

Future Plans.In future works, we intend to continuously incorporate state-of-the-art MLFF models and expand and share datasets with various compositions openly. Specifically, in the immediate future, we will introduce datasets on the Hf-Si-O and Hf-Zr-O systems, incorporating diverse chemical stoichiometries to ensure their broad applicability across a wide spectrum of compositions. In addition, plans are in motion to sequentially release diverse semiconductor datasets, including ternary systems or higher, with constituents such as Si, Ge, C, N, O, H, Hf, Zr, B, P, Ga, and As. This extension will guarantee the exploration of various atomic ratios, allowing for a more comprehensive evaluation of model performance. As training models from scratch for every appending dataset is inefficient, we are planning to adopt efficient training schemes such as transfer learning, which we expect to reduce the cost of model training for growing datasets. It is also important to subjoin additional metrics and loss factors such as stress to evaluate the compatibility with various simulation conditions (_e.g._, NPT), and to reflect indicators during training to provide better prediction performance in practice. Finally, since the main purpose of MLFFs with respect to semiconductors is to enable large-scale simulations that traditional ab initio methods cannot perform, we will extend our benchmark to include the compression and parallelization performance of the models.