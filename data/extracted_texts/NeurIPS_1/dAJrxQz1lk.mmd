# A*Net: A Scalable Path-based Reasoning Approach

for Knowledge Graphs

Zhaocheng Zhu\({}^{1,2,}\), Xinyu Yuan\({}^{1,2,}\), Mikhail Galkin\({}^{3,}\), Sophie Xhonneux\({}^{1,2}\)

Ming Zhang\({}^{4}\), Maxime Gazeau\({}^{5}\), Jian Tang\({}^{1,6,7}\)

\({}^{1}\)Mila - Quebec AI Institute, \({}^{2}\)University of Montreal

\({}^{3}\)Intel AI Lab, \({}^{4}\)Peking University, \({}^{5}\)LG Electronics AI Lab

\({}^{6}\)HEC Montreal, \({}^{7}\)CIFAR AI Chair

Equal contribution. Code is available at [https://github.com/DeepGraphLearning/AStarNetWork](https://github.com/DeepGraphLearning/AStarNetWork) done while at Mila - Quebec AI Institute.

###### Abstract

Reasoning on large-scale knowledge graphs has been long dominated by embedding methods. While path-based methods possess the inductive capacity that embeddings lack, their scalability is limited by the exponential number of paths. Here we present A*Net, a scalable path-based method for knowledge graph reasoning. Inspired by the A* algorithm for shortest path problems, our A*Net learns a priority function to select important nodes and edges at each iteration, to _reduce time and memory footprint for both training and inference_. The ratio of selected nodes and edges can be specified to trade off between performance and efficiency. Experiments on both transductive and inductive knowledge graph reasoning benchmarks show that A*Net achieves competitive performance with existing state-of-the-art path-based methods, while merely visiting 10% nodes and 10% edges at each iteration. On a million-scale dataset ogbl-wikikg2, A*Net not only achieves a new state-of-the-art result, but also converges faster than embedding methods. A*Net is the first path-based method for knowledge graph reasoning at such scale.

## 1 Introduction

Reasoning, the ability to apply logic to draw new conclusions from existing facts, has been long pursued as a goal of artificial intelligence . Knowledge graphs encapsulate facts in relational edges between entities, and serve as a foundation for reasoning. Reasoning over knowledge graphs is usually studied in the form of knowledge graph completion, where a model is asked to predict missing triplets based on observed triplets in the knowledge graph. Such a task can be used to not only populate existing knowledge graphs, but also improve downstream applications like multi-hop logical reasoning , question answering  and recommender systems .

One challenge central to knowledge graph reasoning is the scalability of reasoning methods, as many real-world knowledge graphs  contain millions of entities and triplets. Typically, large-scale knowledge graph reasoning is solved by embedding methods , which learn an embedding for each entity and relation to reconstruct the structure of the knowledge

Figure 1: Validation MRR w.r.t. training time on ogbl-wikikg2 (1 A100 GPU). A*Net achieves state-of-the-art performance and the fastest convergence.

graph. Due to its simplicity, embedding methods have become the _de facto_ standard for knowledge graphs with millions of entities and triplets. With the help of multi-GPU embedding systems [57; 56], they can further scale to knowledge graphs with billions of triplets.

Another stream of works, path-based methods [28; 31; 11; 58], predicts the relation between a pair of entities based on the paths between them. Take the knowledge graph in Fig. 2(a) as an example, we can prove that _Mother(a, f)_ holds, because there are two paths \(a}b}f\) and \(a}c}f\). As the semantics of paths are purely determined by relations rather than entities, path-based methods naturally generalize to unseen entities (i.e., inductive setting), which cannot be handled by embedding methods. However, the number of paths grows exponentially w.r.t. the path length, which hinders the application of path-based methods on large-scale knowledge graphs.

Here we propose A*Net to tackle the scalability issue of path-based methods. The key idea of our method is to search for important paths rather than use all possible paths for reasoning, thereby reducing time and memory in training and inference. Inspired by the A* algorithm  for shortest path problems, given a head entity \(u\) and a query relation \(q\), we compute a priority score for each entity to guide the search towards more important paths. At each iteration, we select \(K\) nodes and \(L\) edges according to their priority, and use message passing to update nodes in their neighborhood. Due to the complex semantics of knowledge graphs, it is hard to use a handcrafted priority function like the A* algorithm without a significant performance drop (Tab. 6a). Instead, we design a neural priority function based on the node representations at the current iteration, which can be end-to-end trained by the objective function of the reasoning task without any additional supervision.

We verify our method on 4 transductive and 2 inductive knowledge graph reasoning datasets. Experiments show that A*Net achieves competitive performance against state-of-the-art path-based methods on FB15k-237, WN18RR and YAGO3-10, even with only 10% of nodes and 10% edges at each iteration (Sec. 4.2). To verify the scalability of our method, we also evaluate A*Net on ogbl-wikikg2, a million-scale knowledge graph that is 2 magnitudes larger than datasets solved by previous path-based methods. Surprisingly, with only 0.2% nodes and 0.2% edges, our method outperforms existing embedding methods and establishes new state-of-the-art results (Sec. 4.2) as the first non-embedding method on ogbl-wikikg2. By adjusting the ratios of selected nodes and edges, one can trade off between performance and efficiency (Sec. 4.3). A*Net also converges significantly faster than embedding methods (Fig. 1), which makes it a promising model for deployment on large-scale knowledge graphs. Additionally, A*Net offers interpretability that embeddings do not possess. Visualization shows that A*Net captures important paths for reasoning (Sec. 4.4).

## 2 Preliminary

Knowledge Graph ReasoningA knowledge graph \(=(,,)\) consists of sets of entities (nodes) \(\), facts (edges) \(\) and relation types \(\). Each fact is a triplet \((x,r,y)\), which indicates a relation \(r\) from entity \(x\) to entity \(y\). The task of knowledge graph reasoning aims at answering queries like \((u,q,?)\) or \((?,q,u)\). Without loss of generality, we assume the query is

Figure 2: **(a) Given a query \((a,,?)\), only a few important paths (showed in red) are necessary for reasoning. Note that paths can go in the reverse direction of relations. **(b)** Exhaustive search algorithm (e.g., Path-RNN, PathCon) enumerates all paths in exponential time. **(c)** Bellman-Ford algorithm (e.g., NeuralLP, DRUM, NBFNet, RED-GNN) computes all paths in polynomial time, but needs to propagate through all nodes and edges. **(d)** A*Net learns a priority function to select a subset of nodes and edges at each iteration, and avoids exploring all nodes and edges.

\((u,q,?)\), since \((?,q,u)\) equals to \((u,q^{-1},?)\) with \(q^{-1}\) being the inverse of \(q\). Given a query \((u,q,?)\), we need to predict the answer set \(_{(u,q,?)}\), such that \( v_{(u,q,?)}\) the triplet \((u,q,v)\) should be true.

Path-based MethodsPath-based methods [28; 31; 11; 58] solve knowledge graph reasoning by looking at the paths between a pair of entities in a knowledge graph. For example, a path \(a}b}f\) may be used to predict _Mother(a, f)_ in Fig. 2(a). From a representation learning perspective, path-based methods aim to learn a representation \(_{q}(u,v)\) to predict the triplet \((u,q,v)\) based on all paths \(_{u v}\) from entity \(u\) to entity \(v\). Following the notation in 3, \(_{q}(u,v)\) is defined as

\[_{q}(u,v)=_{P_{u v}}_{q}(P)=_{ P_{u v}}_{(x,r,y) P}_{q}(x,r,y) \]

where \(\) is a permutation-invariant aggregation function over paths (e.g., sum or max), \(\) is an aggregation function over edges that may be permutation-sensitive (e.g., matrix multiplication) and \(_{q}(x,r,y)\) is the representation of triplet \((x,r,y)\) conditioned on the query relation \(q\). \(\) is computed before \(\). Typically, \(_{q}(x,r,y)\) is designed to be independent of the entities \(x\) and \(y\), which enables path-based methods to generalize to the inductive setting. However, it is intractable to compute Eqn. 1, since the number of paths usually grows exponentially w.r.t. the path length.

Path-based Reasoning with Bellman-Ford algorithmTo reduce the time complexity of path-based methods, recent works [50; 35; 58; 54] borrow the Bellman-Ford algorithm  from shortest path problems to solve path-based methods. Instead of enumerating each possible path, the Bellman-Ford algorithm iteratively propagates the representations of \(t-1\) hops to compute the representations of \(t\) hops, which achieves a polynomial time complexity. Formally, let \(_{q}^{(t)}(u,v)\) be the representation of \(t\) hops. The Bellman-Ford algorithm can be written as

\[_{q}^{(0)}(u,v) _{q}(u=v) \] \[_{q}^{(t)}(u,v) _{q}^{(0)}(u,v)_{(x,r,v) (v)}_{q}^{(t-1)}(u,x)_{q}(x,r,v) \]

where \(_{q}\) is a learnable indicator function that defines the representations of \(0\) hops \(_{q}^{(0)}(u,v)\), also known as the boundary condition of the Bellman-Ford algorithm. \((v)\) is the neighborhood of node \(v\). Despite the polynomial time complexity achieved by the Bellman-Ford algorithm, Eqn. 3 still needs to visit \(||\) nodes and \(||\) edges to compute \(_{q}^{(t)}(u,v)\) for all \(v\) in each iteration, which is not feasible for large-scale knowledge graphs.

A* AlgorithmA* algorithm  is an extension of the Bellman-Ford algorithm for shortest path problems. Unlike the Bellman-Ford algorithm that propagates through every node uniformly, the A* algorithm prioritizes propagation through nodes with higher priority according to a heuristic function specified by the user. With an appropriate heuristic function, A* algorithm can reduce the search space of paths. Formally, with the notation from Eqn. 1, the priority function for node \(x\) is

\[s(x)=d(u,x) g(x,v) \]

where \(d(u,x)\) is the length of current shortest path from \(u\) to \(x\), and \(g(x,v)\) is a heuristic function estimating the cost from \(x\) to the target node \(v\). For instance, for a grid-world shortest path problem (Fig. 4(a)), \(g(x,v)\) is usually defined as the \(L_{1}\) distance from \(x\) to \(v\), \(\) is the addition operator, and \(s(x)\) is a lower bound for the shortest path length from \(u\) to \(v\) through \(x\). During each iteration, the A* algorithm prioritizes propagation through nodes with smaller \(s(x)\).

## 3 Proposed Method

We propose A*Net to scale up path-based methods with the A* algorithm. We show that the A* algorithm can be derived from the observation that only a small set of paths are important for reasoning (Sec. 3.1). Since it is hard to handcraft a good priority function for knowledge graph reasoning (Tab. (a)a), we design a neural priority function, and train it end-to-end for reasoning (Sec. 3.2).

### Path-based Reasoning with A* Algorithm

As discussed in Sec. 2, the Bellman-Ford algorithm visits all \(||\) nodes and \(||\) edges. However, in real-world knowledge graphs, only a small portion of paths is related to the query. Based on this observation, we introduce the concept of important paths. We then show that the representations of important paths can be iteratively computed with the A* algorithm under mild assumptions.

Important Paths for ReasoningGiven a query relation and a pair of entities, only some of the paths between the entities are important for answering the query. Consider the example in Fig. 2(a), the path \(a}}d}}e }}f\) cannot determine whether \(f\) is an answer to _Mother(a,?)_ due to the use of the _Friend_ relation in the path. On the other hand, kinship paths like \(a}}b}}f\) or \(a}}c}}f\) are able to predict that _Mother(a, f)_ is true. Formally, we define \(_{u v|q}_{u v}\) to be the set of paths from \(u\) to \(v\) that is important to the query relation \(q\). Mathematically, we have

\[_{q}(u,v)=_{P_{u v}}_{q}(P) _{P_{u v|q}}_{q}(P) \]

In other words, any path \(P_{u v}_{u v|q}\) has negligible contribution to \(_{q}(u,v)\). In real-world knowledge graphs, the number of important paths \(|_{u v|q}|\) may be several orders of magnitudes smaller than the number of paths \(|_{u v}|\). If we compute the representation \(_{q}(u,v)\) using only the important paths, we can scale up path-based reasoning to large-scale knowledge graphs.

Iterative Computation of Important PathsGiven a query \((u,q,?)\), we need to discover the set of important paths \(_{u v|q}\) for all \(v\). However, it is challenging to extract important paths from \(_{u v}\), since the size of \(_{u v}\) is exponentially large. Our solution is to explore the structure of important paths and compute them iteratively. We first show that we can cover important paths with iterative path selection (Eqn. 6 and 7). Then we approximate iterative path selection with iterative node selection (Eqn. 8).

Notice that paths in \(_{u v}\) form a tree structure (Fig. 3). On the tree, a path is not important if any prefix of this path is not important for the query. For example, in Fig. 2(a), \(a}}d}}e }}f\) is not important, as its prefix \(a}}d\) is not important for the query _Mother_. Therefore, we assume there exists a path selection function \(m_{q}:2^{} 2^{}\) that selects important paths from a set of paths given the query relation \(q\). \(2^{}\) is the set of all subsets of \(\). With \(m_{q}\), we construct the following set of paths \(}^{(t)}_{u v|q}\) iteratively

\[}^{(0)}_{u v|q} \{(u,,v)\}u=v \] \[}^{(t)}_{u v|q} _{x\\ (x,r,v)(v)}P+\{(x,r,v)\}P m_{q}( }^{(t-1)}_{u x|q})} \]

where \(P+\{(x,r,v)\}\) concatenates the path \(P\) and the edge \((x,r,v)\). The paths \(}^{(t)}_{u v|q}\) computed by the above iteration is a superset of the important paths \(^{(t)}_{u v|q}\) of length \(t\) (see Thm. A.1 in App. A). Due to the tree structure of paths, the above iterative path selection still requires exponential time. Hence we further approximate iterative path selection with iterative node selection, by assuming paths with the same length and the same stop node can be merged. The iterative node selection replacing Eqn. 7 is (see Prop. A.3 in App. A)

\[}^{(t)}_{u v|q}_{x n _{u}^{(t-1)}()\\ (x,r,v)(v)}P+\{(x,r,v)\}P}^{(t-1)}_{u x|q}} \]

where \(n_{uq}^{(t)}:2^{} 2^{}\) selects ending nodes of important paths of length \(t\) from a set of nodes.

Figure 3: The colored paths are important paths \(_{u v|q}\), while the solid paths are the superset \(}_{u v|q}\) used in Eqn. 7.

Reasoning with A* AlgorithmEqn. 8 iteratively computes the set of important paths \(_{u v|q}\). In order to perform reasoning, we need to compute the representation \(_{q}(u,v)\) based on the important paths, which can be achieved by an iterative process similar to Eqn. 8 (see Thm. A.4 in App. A)

\[_{q}^{(t)}(u,v) _{q}^{(0)}(u,v)_{x n_{u} ^{(t-1)}()\\ (x,r,v)(v)}_{q}^{(t-1)}(u,x)_{q }(x,r,v) \]

Eqn. 9 is the A* iteration (Fig. 2(d)) for path-based reasoning. Note the A* iteration uses the same boundary condition as Eqn. 2. Inspired by the classical A* algorithm, we parameterize \(n_{uq}^{(t)}()\) with a node priority function \(s_{uq}^{(t)}:\) and select top-\(K\) nodes based on their priority. However, there does not exist an oracle for the priority function \(s_{uq}^{(t)}(x)\). We will discuss how to learn the priority function \(s_{uq}^{(t)}(x)\) in the following sections.

### Path-based Reasoning with A*Net

Both the performance and the efficiency of the A* algorithm heavily rely on the heuristic function. While it is straightforward to use \(L_{1}\) distance as the heuristic function for grid-world shortest path problems, it is not clear what a good priority function for knowledge graph reasoning is due to the complex relation semantics in knowledge graphs. Indeed, our experiments suggest that handcrafted priority functions largely hurt the performance of path-based methods (Tab. 6a). In this section, we discuss a neural priority function, which can be end-to-end trained by the reasoning task.

Neural Priority FunctionTo design the neural priority function \(s_{uq}(x)\), we draw inspiration from the priority function in the A* algorithm for shortest path problems (Eqn. 4). The priority function has two terms \(d(u,x)\) and \(g(x,v)\), where \(d(u,x)\) is the current distance from node \(u\) to \(x\), and \(g(x,v)\) estimates the remaining distance from node \(x\) to \(v\).

From a representation learning perspective, we need to learn a representation \(_{uq}(x)\) to predict the priority score \(s_{uq}(x)\) for each node \(x\). Inspired by Eqn. 4, we use the current representation \(_{q}^{(t)}(u,x)\) to represent \(d^{(t)}(u,x)\). However, it is challenging to find a representation for \(g^{(t)}(x,v)\), since we do not know the answer entity \(v\) beforehand. Noticing that in the A* algorithm, the target node \(v\) can be expressed by the source node plus a displacement (Fig. 4(a)), we reparameterize the answer entity \(v\) with the head entity \(u\) and the query relation \(q\) in A*Net. By replacing \(g^{(t)}(x,v)\) with another function \(g^{(t)}(u,x,q)\), the representation \(_{uq}(x)\) is parameterized as

\[_{uq}^{(t)}(x)=_{q}^{(t)}(u,x)([_{q}^{(t)}(u,x ),]) \]

where \(()\) is a feed-forward network that outputs a vector representation and \([,]\) concatenates two representations. Intuitively, the learned representation \(\) captures the semantic of query relation \(q\), which serves the goal for answering query \((u,q,?)\). The function \(([_{q}^{(t)}(u,x),])\) compares the current representation \(_{q}^{(t)}(u,x)\) with the goal \(\) to estimate the remaining representation (Fig. 4(b)). If \(_{q}^{(t)}(u,x)\) is close to \(\), the remaining representation will be close to 0, and \(x\) is likely to be close to the correct answer. The final priority score is predicted by

\[s_{uq}^{(t)}(x)=(f(_{uq}^{(t)}(x))) \]

where \(f()\) is a feed-forward network and \(\) is the sigmoid function that maps the output to \(\).

Figure 4: **(a) A* algorithm computes the current distance \(d(u,x)\) (blue), estimates the remaining distance \(g(x,v)\) (orange), and prioritizes shorter paths. (b) A*Net computes the current representations \(_{q}^{(t)}(u,x)\) (blue), estimates the remaining representations \(([_{q}^{(t)}(u,x),])\) (orange) based on the query \(\) (green), and prioritizes paths more relevant to the query.**

LearningTo learn the neural priority function, we incorporate it as a weight for each message in the A* iteration. For simplicity, let \(^{(t)}=n_{uq}^{(t-1)}()\) be the nodes we try to propagate through at \(t\)-th iteration. We modify Eqn. 9 to be

\[_{q}^{(t)}(u,v)_{q}^{(0)}(u,v)_{ {c}x^{(t)}\\ (x,r,v)(v)}s_{uq}^{(t-1)}(x)(_{q}^{(t-1) }(u,x)_{q}(x,r,v)) \]

Eqn. 12 encourages the model to learn larger weights \(s_{uq}^{(t)}(x)\) for nodes that are important for reasoning. In practice, as some nodes may have very large degrees, we further select top-\(L\) edges from the neighborhood of \(n_{uq}^{(t-1)}()\) (see App. B). A pseudo code of A*Net is illustrated in Alg. 1. Note the top-\(K\) and top-\(L\) functions are not differentiable.

Nevertheless, it is still too challenging to train the neural priority function, since we do not know the ground truth for important paths, and there is no direct supervision for the priority function. Our solution is to share the weights between the priority function and the predictor for the reasoning task. The intuition is that the reasoning task can be viewed as a weak supervision for the priority function.

Recall that the goal of \(s_{uq}^{(t)}(x)\) is to determine whether there exists an important path from \(u\) to \(x\) (Eqn. 8). In the reasoning task, any positive answer entity must be present on at least one important path, while negative answer entities are less likely to be on important paths. Our ablation experiment demonstrates that sharing weights improve the performance of neural priority function (Tab. 6b). Following , A*Net is trained to minimize the binary cross entropy loss over triplets

\[=- p(u,q,v)-_{i=1}^{n}(1-p(u_{i}^{},q, v_{i}^{})) \]

where \((u,q,v)\) is a positive sample and \(\{(u_{i}^{},q,v_{i}^{})\}_{i=1}^{n}\) are negative samples. Each negative sample \((u_{i},q,v_{i})\) is generated by corrupting the head or the tail in a positive sample.

```
0: head entity \(u\), query relation \(q\), #iterations \(T\)
0:\(p(v|u,q)\) for all \(v\)
1:for\(v\)do
2:\(_{q}^{(0)}(u,v)_{q}(u=v)\)
3:endfor
4:for\(t 1\) to \(T\)do
5:\(^{(t)}(s_{uq}^{(t-1)}(x)|x)\)
6:\(^{(t)}_{x^{(t)}}(x)\)
7:\(^{(t)}(s_{uq}^{(t-1)}(v)|(x,r,v)^{(t)})\)
8:\(^{(t)}_{(x,r,v)^{(t)}}\{v\}\)
9:for\(v^{(t)}\)do
10: Compute \(_{q}^{(t)}(u,v)\) with Eqn. 12
11: Compute priority \(s_{uq}^{(t)}(v)\) with Eqn. 10, 11
12:endfor
13:endfor
14:\(\) Share weights between \(s_{uq}(v)\) and the predictor
15:return\(s_{uq}^{(T)}(v)\) as \(p(v|u,q)\) for all \(v\)
```

**Algorithm 1** A*Net

Efficient Implementation with Padding-Free OperationsModern neural networks heavily rely on batched execution to unleash the parallel capacity of GPUs. While Alg. 1 is easy to implement for a single sample \((u,q,?)\), it is not trivial to batch A*Net for multiple samples. The challenge is that different samples may have very different sizes for nodes \(^{(t)}\) and edges \(^{(t)}\). A common approach is to pad the set of nodes or edges to a predefined constant, which would severely counteract the acceleration brought by A*Net.

Here we introduce padding-free \(topk\) operation to avoid the overhead in batched execution. The key idea is to convert batched execution of different small samples into execution of a single large sample, which can be paralleled by existing operations in deep learning frameworks. For example, the batched execution of \(topk([,])\) can be converted into a multi-key sort problem over \([,,,,]\), where the first key is the index of the sample in the batch and the second key is the original input. The multi-key sort is then implemented by composing stable single-key sort operations in deep learning frameworks. See App. C for details.

## 4 Experiments

We evaluate A*Net on standard transductive and inductive knowledge graph reasoning datasets, including a million-scale one ogbl-wikikg2. We conduct ablation studies to verify our design choices and visualize the important paths learned by the priority function in A*Net.

### Experiment Setup

Datasets & EvaluationWe evaluate A*Net on 4 standard knowledge graphs, FB15k-237 , WN18RR , YAGO3-10  and ogbl-wikikg2 . For the transductive setting, we use the standard splits from their original works [40; 16]. For the inductive setting, we use the splits provided by , which contains 4 different versions for each dataset. As for evaluation, we use the standard filtered ranking protocol  for knowledge graph reasoning. Each triplet \((u,q,v)\) is ranked against all negative triplets \((u,q,v^{})\) or \((u^{},q,v)\) that are not present in the knowledge graph. We measure the performance with mean reciprocal rank (MRR) and HITS at K (H@K). Efficiency is measured by the average number of messages (#message) per step, wall time per epoch and memory cost. To plot the convergence curves for each model, we dump checkpoints during training with a high frequency, and evaluate the checkpoints later on the validation set. See more details in App. D.

Implementation DetailsOur work is developed based on the open-source codebase of path-based reasoning with Bellman-Ford algorithm4. For a fair comparison with existing path-based methods, we follow the implementation of NBFNet  and parameterize \(\) with principal neighborhood aggregation (PNA)  or sum aggregation, and parameterize \(\) with the relation operation from DistMult , i.e., vector multiplication. The indicator function (Eqn. 2) \(_{q}(u=v)=(u=v)\) is parameterized with a query embedding \(\) for all datasets except ogbl-wikikg2, where we augment the indicator function with learnable embeddings based on a soft distance from \(u\) to \(v\) (see App. E for more details). The edge representation (Eqn. 12) \(_{q}(x,r,v)=_{r}+_{r}\) is parameterized as a linear function over the query relation \(q\) for all datasets except WN18RR, where we use a simple embedding \(_{q}(x,r,v)=\). We use the same preprocessing steps as in , including augmenting each triplet with a flipped triplet, and dropping out query edges during training.

For the neural priority function, we have two hyperparameters: \(K\) for the maximum number of nodes and \(L\) for the maximum number of edges. To make hyperparameter tuning easier, we define maximum node ratio \(=K/||\) and maximum average degree ratio \(=L||/K||\), and tune the ratios for each dataset. The maximum edge ratio is determined by \(\). The other hyperparameters are kept the same as the values in . We train A*Net with 4 Tesla A100 GPUs (40 GB), and select the best model based on validation performance. See App. E for more details.

BaselinesWe compare A*Net against embedding methods, GNNs and path-based methods. The embedding methods are TransE , ComplEx , RotatE , HAKE , RotH , PairRE , ComplEx+Relation Prediction  and ConE . The GNNs are RGCN , CompGCN  and GraIL . The path-based methods are MINERVA , Multi-Hop , CURL , NeuralLP , DRUM , NBFNet  and RED-GNN . Note that path-finding methods [14; 29; 52] that use reinforcement learning and assume sparse answers can only be evaluated on tail prediction. Training time of all baselines are measured based on their official open-source implementations, except that we use a more recent implementation5 of TransE and ComplEx.

### Main Results

Tab. 1 shows that A*Net outperforms all embedding methods and GNNs, and is on par with NBFNet on transductive knowledge graph reasoning. We also observe a similar trend of A*Net and NBFNet over path-finding methods on tail prediction (Tab. 2). Since path-finding methods select only one path with reinforcement learning, such results imply the advantage of aggregating multiple paths in A*Net. A*Net also converges faster than all the other methods (Fig. 5). Notably, unlike NBFNet that propagates through all nodes and edges, A*Net only propagates through 10% nodes and 10% edges on both datasets, which suggests that most nodes and edges are not important for path-based reasoning. Tab. 3 shows that A*Net reduces the number of messages by 14.1\(\) and 42.9\(\) compared to NBFNet on two datasets respectively. Note that the reduction in time and memory is less than the reduction in the number of messages, since A*Net operates on subgraphs with dynamic sizes and is harder to parallel than NBFNet on GPUs. We leave better parallel implementation as future work.

Tab. 4 shows the performance on ogbl-wikikg2, which has 2.5 million entities and 16 million triplets. While NBFNet faces out-of-memory (OOM) problem even for a batch size of 1, A*Net can perform reasoning by propagating through 0.2% nodes and 0.2% edges at each step. Surprisingly, even with

[MISSING_PAGE_FAIL:8]

\(\) and \(\). If we can accept a performance similar to embedding methods (e.g., ConE ), we can set either \(\) to 1% or \(\) to 10%, resulting in 8.7\(\) speedup compared to NBFNet.

### Visualization of Learned Important Paths

We can extract the important paths from the neural priority function in A*Net for interpretation. For a given query \((u,q,?)\) and a predicted entity \(v\), we can use the node priority \(s_{uq}^{(t)}(x)\) at each step to estimate the importance of a path. Empirically, the importance of a path \(s_{q}(P)\) is estimated by

\[s_{q}(P)=_{t=1,P^{(t)}=(x,r,y)}^{|P|}^{(t-1)}(x)}{ S_{uq}^{(t-1)}} \]

where \(S_{uq}^{(t-1)}=_{x^{(t-1)}}s_{uq}^{(t-1)}(x)\) is a normalizer to normalize the priority score for each step \(t\). To extract the important paths with large \(s_{q}(P)\), we perform beam search over the priority function \(s_{uq}^{(t-1)}(x)\) of each step. Fig. 7 shows the important paths learned by A*Net for a test sample in FB15k-237. Given the query _(Bandai, industry,?)_, we can see both paths _Bandai \(}\)Bandai Namco \(}\)video game_ and _Bandai \(}\)media \(}\)Pony Canyon \(}\)video game_ are consistent with human cognition. More visualization results can be found in App. G.

## 5 Related Work

Path-based ReasoningPath-based methods use paths between entities for knowledge graph reasoning. Early methods like Path Ranking [28; 19] collect relational paths as symbolic features for classification. Path-RNN [31; 15] and PathCon  improve Path Ranking by learning the representations of paths with recurrent neural networks (RNN). However, these works operate on the full set of paths between two entities, which grows exponentially w.r.t. the path length. Typically, these methods can only be applied to paths with at most 3 edges.

To avoid the exhaustive search of paths, many methods learn to sample important paths for reasoning. DeepPath  and MINERVA  learn an agent to collect meaningful paths on the knowledge graph

Table 6: Ablation studies of A*Net on transductive FB15k-237.

through reinforcement learning. These methods are hard to train due to the extremely sparse rewards. Later works improve them by engineering the reward function  or the search strategy , using multiple agents for positive and negative paths  or for coarse- and fine-grained paths .  and  use a variational formulation to learn a sparse prior for path sampling. Another category of methods utilizes the dynamic programming to search paths in a polynomial time. NeuralLP  and DRUM  use dynamic programming to learn linear combination of logic rules. All-Paths  adopts a Floyd-Warshall-like algorithm to learn path representations between all pairs of entities. Recently, NBFNet  and RED-GNN  leverage a Bellman-Ford-like algorithm to learn path representations from a single-source entity to all entities. While dynamic programming methods achieve state-of-the-art results among path-based methods, they need to perform message passing on the full knowledge graph. By comparison, our A*Net learns a priority function and only explores a subset of paths, which is more scalable than existing dynamic programming methods.

Efficient Graph Neural NetworksOur work is also related to efficient graph neural networks, since both try to improve the scalability of graph neural networks (GNNs). Sampling methods  reduce the cost of message passing by computing GNNs with a sampled subset of nodes and edges. Non-parametric GNNs  decouple feature propagation from feature transformation, and reduce time complexity by preprocessing feature propagation. However, both sampling methods and non-parametric GNNs are designed for homogeneous graphs, and it is not straightforward to adapt them to knowledge graphs. On knowledge graphs, RS-GCN  learns to sample neighborhood with reinforcement learning. DPMPN  learns an attention to iteratively select nodes for message passing. SQALER  first predicts important path types based on the query, and then applies GNNs on the subgraph extracted by the predicted paths. Our A*Net shares the same goal with these methods, but learns a neural priority function to iteratively select important paths.

## 6 Discussion and Conclusion

Limitation and Future WorkOne limitation for A*Net is that we focus on algorithm design rather than system design. As a result, the improvement in time and memory cost is much less than the improvement in the number of messages (Tab. 3 and App. F). In the future, we will co-design the algorithm and the system to further improve the efficiency.

Societal ImpactThis work proposes a scalable model for path-based reasoning. On the positive side, it reduces the training and test time of reasoning models, which helps control carbon emission. On the negative side, reasoning models might be used in malicious activities, such as discovering sensitive relationship in anonymized data, which could be augmented by a more scalable model.

ConclusionWe propose A*Net, a scalable path-based method, to solve knowledge graph reasoning by searching for important paths, which is guided by a neural priority function. Experiments on both transductive and inductive knowledge graphs verify the performance and efficiency of A*Net. Meanwhile, A*Net is the first path-based method that scales to million-scale knowledge graphs.