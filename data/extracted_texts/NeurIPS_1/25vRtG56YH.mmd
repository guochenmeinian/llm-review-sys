# Beyond Normal: On the Evaluation of Mutual Information Estimators

Pawel Czyz\({}^{*}\)\({}^{1,2}\)  Frederic Grabowski\({}^{*}\)\({}^{3}\)

Julia E. Vogt\({}^{4,5}\)  Niko Beerenwinkel\({}^{}\)\({}^{1,5}\)  Alexander Marx\({}^{}\)\({}^{2,4}\)

\({}^{1}\)Department of Biosystems Science and Engineering, ETH Zurich \({}^{2}\)ETH AI Center, ETH Zurich

\({}^{3}\)Institute of Fundamental Technological Research, Polish Academy of Sciences

\({}^{4}\)Department of Computer Science, ETH Zurich \({}^{5}\)SIB Swiss Institute of Bioinformatics

Equal contribution \({}^{}\)Joint supervision

###### Abstract

Mutual information is a general statistical dependency measure which has found applications in representation learning, causality, domain generalization and computational biology. However, mutual information estimators are typically evaluated on simple families of probability distributions, namely multivariate normal distribution and selected distributions with one-dimensional random variables. In this paper, we show how to construct a diverse family of distributions with known ground-truth mutual information and propose a language-independent benchmarking platform for mutual information estimators. We discuss the general applicability and limitations of classical and neural estimators in settings involving high dimensions, sparse interactions, long-tailed distributions, and high mutual information. Finally, we provide guidelines for practitioners on how to select appropriate estimator adapted to the difficulty of problem considered and issues one needs to consider when applying an estimator to a new data set.

## 1 Introduction

Estimating the strength of a non-linear dependence between two continuous random variables (r.v.) lies at the core of machine learning. Mutual information (MI) lends itself naturally to this task, due to its desirable properties, such as invariance to homeomorphisms and the data processing inequality. It finds applications in domain generalization (Li et al., 2022; Ragonesi et al., 2021), representation learning (Belghazi et al., 2018; Oord et al., 2018), causality (Solo, 2008; Kurutach et al., 2018), physics (Keys et al., 2015), systems biology (Selimkhanov et al., 2014; Grabowski et al., 2019; Uda, 2020), and epidemiology (Young et al., 2023).

Over the last decades, the estimation of MI has been extensively studied, and estimators have been developed, ranging from classical approaches based on histogram density (Pizer et al., 1987), kernel density estimation (Moon et al., 1995) to \(k\)-nearest neighbor (Kozachenko and Leonenko, 1987; Kraskov et al., 2004) and neural estimators (Belghazi et al., 2018; Oord et al., 2018; Song and Ermon, 2020). However, despite the progress that has been achieved in this area, not much attention has been focused on systematically benchmarking these approaches.

Typically, new estimators are evaluated assuming multivariate normal distributions for which MI is analytically tractable (Darbellay and Vajda, 1999; Kraskov et al., 2004; Suzuki, 2016). Sometimes, simple transformations are applied to the data (Khan et al., 2007; Gao et al., 2015), moderately high-dimensional settings are considered (Lord et al., 2018; Lu and Peltonen, 2020), or strong dependencies are evaluated (Gao et al., 2015). Beyond that, Song and Ermon (2020) study theself-consistency (additivity under independence and the data processing inequality) of neural MI estimators in the context of representation learning from image data.

**Our Contributions** In this work we show a method of developing expressive distributions with known ground-truth mutual information (Sec. 2), propose forty benchmark tasks and systematically study the properties of commonly used estimators, including representatives based on kernel or histogram density estimation, \(k\)NN estimation, and neural network-based estimators (Sec. 3). We address selected difficulties one can encounter when estimating mutual information (Sec. 4), such as sparsity of interactions, long-tailed distributions, invariance, and high mutual information. Finally, we provide recommendations for practitioners on how to choose a suitable estimator for particular problems (Sec. 6). Our benchmark is designed so that it is simple to add new tasks and estimators. It also allows for cross-language comparisons -- in our work we compare estimators implemented in Python, R and Julia. All of our experiments are fully reproducible by running Snakemake workflows . Accompanying code is available at [http://github.com/cbg-ethz/bmi](http://github.com/cbg-ethz/bmi).

Overall, our _key findings_ from the benchmark can be summarized as follows:

* Testing on multivariate normal distributions gives a biased and overly optimistic view of estimator performance. In this setting, canonical correlation analysis (CCA), a model-based approach, emerges as the most effective method -- even if the model is slightly misspecified.
* Compared to classical estimators, neural estimators excel in high-dimensional settings, capturing sparse interactions across multiple dimensions.
* The popular KSG estimator  is accurate in low- and moderate-dimensional settings, but its performance suffers on problems involving high-dimensions or sparse interactions.
* Although MI is invariant to a wide range of transformations (Theorem 2.1), numerical estimates, even with large sample sizes, are not. Thus, MI may not be suitable when metric invariance to specific transformation families is required.
* Multivariate Student distributions pose an important and hard challenge to many mutual information estimators. This effect is partially, but not fully, attributed to their long tails.

## 2 Mutual Information: Estimation and Invariance

We start by recalling the definition of MI, then discuss the problem of estimating it, and introduce the estimators used in the benchmark.

Consider two r.v. \(X\) and \(Y\) with domains \(\) respectively \(\), joint probability measure \(P_{XY}\) and marginal probability measures \(P_{X}\) and \(P_{Y}\), respectively. If \(P_{XY}\) is absolutely continuous with respect to \(P_{X} P_{Y}\) (e.g., \(X\) and \(Y\) are finite r.v.), MI is equal to the following Kullback-Leibler divergence2:

\[(X;Y)=_{}(P_{XY} P_{X} P_{ Y})= f\,P_{XY},\]

where \(f=P_{XY}/(P_{X} P_{Y})\) is the Radon-Nikodym derivative. If the absolute continuity does not hold, then \((X;Y)=+\)[17, Theorem 2.1.2]. If \(\) and \(\) are Euclidean spaces and measures \(P_{XY}\), \(P_{X}\) and \(P_{Y}\) have probability density functions (PDFs) with respect to the Lebesgue measure, then the Kullback-Leibler divergence can be written in terms of the PDFs.

Figure 1: Visualisations of selected proposed distributions. Two correlated Gaussian variables \(X\) and \(Y\) (1) can be transformed via the Gaussian CDF into correlated uniform variables (2), into a long-tailed distribution via the “half-cube” mapping \(t t\) (3), into a multi-modal distribution (4) or embedded in the three-dimensional space via the composition of the Swiss roll mapping and Gaussian CDF (5). Color on the plot corresponds to the original \(Y\) variable.

However, we will later consider distributions which do not have PDFs with respect to the Lebesgue measure and the general Radon-Nikodym derivative must be used (see "Swiss roll" in Fig. 1).

In almost all applications, \(P_{XY}\) and \(P_{X} P_{Y}\) are not known and one needs to estimate MI from a finite sample from \(P_{XY}\), i.e., a realization \((x_{1},y_{1}),,(x_{N},y_{N})()^{N}\) of \(N\) i.i.d. r.v. \((X_{1},Y_{1}),,(X_{N},Y_{N})\) distributed according to the joint distribution \(P_{XY}\). As a MI estimator we will denote a family of measurable mappings, indexed by the sample size \(N\) and spaces \(\) and \(\) (in most applications, Euclidean spaces of various dimensionalities): \(e_{N}^{}()^{N} \). Composing this mapping with the r.v. representing the whole data set we obtain a real-valued r.v. \(E_{N}^{XY}\). For a given \(P_{XY}\) we can consider the distribution of \(E_{N}^{XY}\). It is often summarized by its first two moments, resulting in the bias and the variance of the estimator. Understanding the bias of an estimator however requires the knowledge of ground-truth MI, so the estimators are typically tested on the families of jointly multivariate normal or uniform distributions, with varying \(N\) and ground-truth \((X;Y)\)(Khan et al., 2007; Lord et al., 2018; Holmes and Nemenman, 2019; Song and Ermon, 2020).

Constructing Expressive Distributions and BenchmarkingBesides considering different types of distributions such as (multivariate) normal, uniform and Student, our benchmark is based on the invariance of MI to the family of chosen mappings. Generally MI is not invariant to arbitrary transformations, as due to the data processing inequality \((f(X);g(Y))(X;Y)\). However, under some circumstances MI is preserved, which enables us to create more expressive distributions by transforming the variables.

**Theorem 2.1**.: _Let \(\), \(^{}\), \(\) and \(^{}\) be standard Borel spaces (e.g., smooth manifolds with their Borel \(\)-algebras) and \(f^{}\) and \(g^{}\) be continuous injective mappings. Then, for every \(\)-valued r.v. \(X\) and \(\)-valued r.v. \(Y\) it holds that \((X;Y)=(f(X);g(Y))\)._

This theorem has been studied before by Kraskov et al. (2004, Appendix], who consider diffeomorphisms and by Polyanskiy and Wu (2022, Th. 3.7), who assume measurable injective functions with measurable left inverses. For completeness we provide a proof which covers continuous injective mappings in Appendix A. In particular, each of \(f\) and \(g\) can be a homeomorphism, a diffeomorphism, or a topological embedding. Embeddings are allowed to increase the dimensionality of the space, so even if \(X\) and \(Y\) have probability density functions, \(f(X)\) and \(g(Y)\) do not need to have them. Neither of these deformations change MI. Thus, using Theorem 2.1, we can create more expressive distributions \(P_{f(X)g(Y)}\) by sampling from \(P_{XY}\) and transforming the samples to obtain a data set \((f(x_{1}),g(y_{1})),,(f(x_{N}),g(y_{N})(^{ }^{})^{N}\).

While offering a tool to construct expressive distributions, another valuable perspective to consider with this problem is estimation invariance: although MI is invariant to proposed changes, the estimate may not be. More precisely, applying an estimator to the transformed data set results in the induced r.v. \(E_{N}^{f(X)g(Y)}\) and its distribution. If the estimator were truly invariant, one would expect that \([E_{N}^{f(X)g(Y)}]\) should equal \([E_{N}^{XY}]\). This invariance is rarely questioned and often implicitly assumed as given (see e.g. Tschannen et al. (2020) or Murphy (2023, Sec. 32.2.2.3)), but as we will see in Sec. 4.3, finite-sample estimates are not generally invariant.

## 3 Proposed Benchmark

In this section, we outline forty tasks included in the benchmark that cover a wide family of distributions, including varying tail behaviour, sparsity of interactions, multiple modes in PDF, and transformations that break colinearity. As our base distributions, we selected multivariate normal and Student distributions, which were transformed with continuous injective mappings.

We decided to focus on the following phenomena:

1. **Dimensionality.** High-dimensional data are collected in machine learning and natural sciences (Buhlmann and van de Geer, 2011, Ch. 1). We therefore change the dimensions of the \(\) and \(\) spaces between \(1\) and \(25\).
2. **Sparsity.** While the data might be high-dimensional, the effective dimension may be much smaller, due to correlations between different dimensions or sparsity (Lucas et al., 2006). We therefore include distributions in which some of the dimensions represent random noise,which does not contribute to the mutual information, and perform an additional study in Section 4.1.
3. **Varying MI.** Estimating high MI is known to be difficult . However, we can often bound it in advance -- if there are 4000 image classes, MI between image class and representation is at most \( 4000 8.3\) nats. In this section, we focus on problems with MI varying up to 2 nats. We additionally consider distributions with higher MI in Section 4.4.
4. **Long tails.** As Taleb  and Zhang et al.  argue, many real-world distributions have long tails. To model different tails we consider multivariate Student distributions as well as transformations lengthening the tails. We conduct an additional study in Section 4.2.
5. **Robustness to diffeomorphisms.** As stated in Theorem 2.1, mutual information is invariant to reparametrizations of random variables by diffeomorphisms. We however argue that when only finite samples are available, invariance in mutual information estimates may not be achieved. To test this hypothesis we include distributions obtained by using a diverse set of diffeomorphisms and continuous injective mappings. We further study the robustness to reparametrizations in Section 4.3.

While we provide a concise description here, precise experimental details can be found in Appendix D and we visualise selected distributions in Appendix F. We first describe tasks employing one-dimensional variables.

**Bivariate Normal** For multivariate normal variables MI depends only on the correlation matrix. We will therefore consider a centered bivariate normal distribution \(P_{XY}\) with \((X,Y)=\) and \((X;Y)=-0.5(1-^{2})\). We chose \(=0.75\).

**Uniform Margins** As a first transformation, we apply the Gaussian CDF \(F\) to Gaussian r.v. \(X\) and \(Y\) to obtain r.v. \(X^{}=F(X)\) and \(Y^{}=F(Y)\). It is a standard result in copula theory (see Lemma D.2 or e.g., Nelsen  for an introduction to copulas) that \(F(X)(0,1)\) resp. \(F(Y)(0,1)\). The joint distribution \(P_{X^{}Y^{}}\), however, is not uniform. Mutual information is preserved, \((X^{};Y^{})\)=\((X;Y)\). For an illustration, see Fig. 1. A distribution P transformed with Gaussian CDF \(F\) will be denoted as Normal CDF @ P.

**Half-Cube Map** To lengthen the tails we applied the half-cube homeomorphism \(h(x)\)=\(|x|^{3/2}x\) to Gaussian variables \(X\) and \(Y\). We visualize an example in Fig. 1 and denote a transformed distribution as Half-cube @ P.

**Asinh Mapping** To shorten the tails we applied inverse hyperbolic sine function \(x=(x+})\). A distribution transformed with this mapping will be denoted as Asinh @ P.

**Wiggly Mapping** To model non-uniform lengthscales, we applied a mapping

\[w(x)=x+_{i}a_{i}(_{i}x+_{i}),_{i}|a_{i}_ {i}|<1.\]

Due to the inequality constraint this mapping is injective and preserves MI. The parameter values can be found in Appendix D. We denote the transformed distribution as Wiggly @ P.

**Bimodal Variables** Applying the inverse CDF of a two-component Gaussian mixture model to correlated variables \(X\) and \(Y\) with uniform margins we obtained a joint probability distribution \(P_{X^{}Y^{}}\) with four modes (presented in Fig. 1). We call this distribution Bimodal and provide technical details in Appendix D.

**Additive Noise** Consider independent r.v. \(X(0,1)\) and \(N(-,)\), where \(\) is the noise level. For \(Y=X+N\), it is possible to derive \((X;Y)\) analytically (see Appendix D for the formula and the parameter values) and we will call this distribution Uniform (additive noise=c).

**Swiss Roll Embedding** As the last example in this section, we consider a mixed case in which a one-dimensional r.v. \(X(0,1)\) is smoothly embedded into two dimensions via the Swiss roll mapping, a popular embedding used to test dimensionality reduction algorithms (see Fig. 1 for visualisation and Appendix D for the formula). Note that the obtained distribution does not have a PDF with respect to the Lebesgue measure on \(^{3}\).

Next, we describe the tasks based on multivariate distributions.

Multivariate NormalWe sampled \((X,Y)=(X_{1},,X_{m},Y_{1},,Y_{n})\) from the multivariate normal distribution. We model two correlation structures: "dense" interactions with all off-diagonal correlations set to 0.5 and "2-pair" interactions where \((X_{1},Y_{1})=(X_{2},Y_{2})=0.8\) and there is no correlation between any other (distinct) variables (Multimormal (2-pair)). For a latent-variable interpretation of these covariance matrices we refer to Appendix D.2.

Multivariate StudentTo see how well the estimators can capture MI contained in the tails of the distribution, we decided to use multivariate Student distributions (see Appendix D) with dispersion matrix3 set to the identity \(I_{m+n}\) and \(\) degrees of freedom. Contrary to the multivariate normal case, in which the identity matrix used as the covariance would lead to zero MI, the variables \(X\) and \(Y\) can still interact through the tails.

Transformed Multivariate DistributionsAs the last set of benchmark tasks we decided to transform the multivariate normal and multivariate Student distributions. We apply mappings described above to each axis separately. For example, applying normal CDF to the multivariate normal distribution will map it into a distribution over the cube \((0,1)^{m+n}\) with uniform marginals. To mix different axes we used a spiral diffeomorphism (denoted as Spiral @ P); we defer the exact construction to Appendix D but we visualise this transformation in Fig. 5.

EstimatorsIn our benchmark, we include a diverse set of estimators. Following recent interest, we include four neural estimators, i.e., Donsker-Varadhan (D-V) and MINE (Belghazi et al., 2018), InfoNCE (Oord et al., 2018), and the NWJ estimator (Nguyen et al., 2007; Nowozin et al., 2016; Poole et al., 2019). As a representative for model-based estimators, we implement an estimator based on canonical correlation analysis (CCA) (Murphy, 2023, Ch. 28), which assumes that the joint distribution is multivariate normal. For more classical estimators, we include the Kraskov, Stogbauer and Grassberger (KSG) estimator (Kraskov et al., 2004) as the most well-known \(k\)-nearest neighbour-based estimator, a recently proposed kernel-based estimator (LNN) (Gao et al., 2017), as well as an estimator based on histograms and transfer entropy (both implemented in Julia's TransferEntropy library). A detailed overview of the different classes of estimators is provided in Appendix C. Further, we describe the the hyperparameter selection in Appendix E.

PreprocessingAs a data preprocessing step, we standardize each dimension using the mean and standard deviation calculated from the sample. Other preprocessing strategies are also possible, but we did not observe a consistent improvement of one preprocessing strategy over the others (see Appendix E).

### Benchmark Results

We show the results for \(N=10\,000\) data points in Fig. 2. Neural estimators and KSG perform better than alternative estimators on most problems, with KSG having often low sample requirements (see Appendix E). The simple model-based estimator CCA obtained excellent performance at low sample sizes (see Appendix E), even for slightly transformed multivariate normal distributions. Finally, LNN and the two Julia estimators (histogram and transfer entropy), work well in low dimension but are not viable in medium- and high-dimensional problems (\( X+ Y 4\)).

Overall, we observed that for most \(1{}1\)-dimensional and multivariate normal problems MI can be reliably estimated. Difficulties arise for sparse interactions, atypical (Student) distributions, and for significantly transformed distributions. This suggests that the typical evaluations of MI estimators are overly optimistic, focusing on relatively simple problems.

KSG is able to accurately estimate MI in multivariate normal problems, however performance drops severely on tasks with sparse interactions (2-pair tasks). For \(5{}5\) dimensions the estimate is about 70% of the true MI, and for \(25{}25\) dimensions it drops to 10%, which is consistent with previous observations (Marx and Fischer, 2022). Meanwhile, performance of neural estimators is stable. We study the effect of sparsity in more detail in Sec. 4.

Student distributions have proven challenging to all estimators. This is partially due to the fact that long tails can limit estimation quality (see Sec. 4), and indeed, applying a tail-shortening \(\) transformation improves performance. However, performance still remains far below multivariate normal distributions with similar dimensionality and information content, showing that long tails are only part of the difficulty. For neural estimators, the issue could be explained by the fact that the critic is not able to fully learn the pointwise mutual information (see Appendix E).

Interestingly, the performance of CCA, which assumes a linear Gaussian model, is often favorable compared to other approaches and has very low sample complexity (see Appendix E). This is surprising, since in the case of transformed distributions the model assumptions are not met. The approach fails for Student distributions (for which the correlation matrix either does not exist or is the identity matrix), the Swiss roll embedding and multivariate normal distributions transformed using the spiral diffeomorphism. Since the spiral diffeomorphism proved to be the most challenging transformation, we study it more carefully in Sec. 4.

## 4 Selected Challenges

In this section we investigate distributions which proved to be particularly challenging for the tested estimators in our benchmark: sparse interactions, long tailed distributions and invariance to diffeomorphisms. Additionally we investigate how the estimators perform for high ground truth MI.

### Sparsity of Interactions

In the main benchmark (Fig. 2) we observed that estimation of MI with "2-pair" type of interactions is considerably harder for the KSG estimator than the "dense" interactions. On the other hand, neural estimators were able to pick the relevant interacting dimensions and provide better estimates of MI in these sparse cases.

We decided to interpolate between the "dense" and "2-pair" interaction structures using a two-stage procedure with real parameters \(\) and \(\) and the number of strongly interacting components \(K\). While the precise description is in Appendix D.2, we can interpret \(\) as controlling the baseline strength of interaction between every pair of variables in \(\{X_{1},,X_{10},Y_{1}, Y_{10}\}\) and \(\) as the additional strength of interaction between pairs of variables \((X_{1},Y_{1}),,(X_{K},Y_{K})\). Dense interaction structure has \(=0\) and 2-pair interaction structure has \(=0\) and \(K=2\). First, we set \(K=10\) and \( 0\) and decrease \(\) raising \(\) at the same time to maintain constant MI of 1 nat. When \(=0\), whole information is contained in the pairwise interactions \((X_{1},Y_{1}),,(X_{10},Y_{10})\). We then decrease \(K\) and increase \(\), still maintaining constant MI.

In Fig. 3 we observe that the performance of all estimators considered (apart from CCA, a model-based approach suitable for multivariate normal distributions) degrades when the interactions between

Figure 2: Mean MI estimates of nine estimators over \(n=10\) samples with \(N=10\,000\) points each against the ground-truth value on all benchmark tasks grouped by category. Color indicates relative negative bias (blue) and positive bias (red). Blank entries indicate that an estimator experienced numerical instabilities.

every pair of variables become sparser (lower \(\)). In particular, even neural estimators are not able to model the information in ten interacting pairs of variables. However, when we decrease the number of interacting pairs of variables, the performance of the neighborhood-based KSG estimator is qualitatively different from the neural estimators: performance of KSG steadily deteriorates, while neural estimators can find (some of the) relevant pairs of variables.

This motivates us to conclude that in the settings where considered variables are high-dimensional and the interaction structure is sparse, neural estimators can offer an advantage over neighborhood-based approaches.

### Long-Tailed Distributions

In Fig. 4 we investigate two different ways to lengthen the tails. First, we lengthen the tails of a multivariate normal distribution using the mapping \(x|x|^{k}x\) (with \(k 1\)) applied to each dimension separately. In this case, we see that the performance of CCA, KSG, and neural estimators is near-perfect for \(k\) close to \(1\) (when the distribution \(P_{XY}\) is close to multivariate normal), but the performance degrades as the exponent increases. Second, we study multivariate Student distributions varying the degrees of freedom: the larger the degrees of freedom, the lower the information content in the tails of the distribution. Again, we see that that the performance of CCA, KSG, and neural estimators is near-perfect for high degrees of freedom for which the distribution is approximately normal. For low degrees of freedom, these estimators significantly underestimate MI, with the exception of CCA which gives estimates with high variance, likely due to the correlation matrix being hard to estimate.

Overall, we see that long tails complicate the process of estimating MI. The Student distribution is particularly interesting, since even after the tails are removed (with \(\) or preprocessing strategies described in Appendix E) the task remains challenging. We suspect that the MI might be contained in regions which are rarely sampled, making the MI hard to estimate. When neural estimators are used, pointwise mutual information learned by the critic does not match the ground truth (see Appendix E).

Hence, we believe that accurate estimation of MI in long-tailed distributions is an open problem. Practitioners expecting long-tailed distributions should approach this task with caution.

Figure 4: MI estimates as a function of the information contained in the tail of the distribution. Left: lenghtening the tails via \(x|x|^{k}x\) mapping with changing \(k\). Right: increasing the degrees of freedom in multivariate Student distribution shortens the tails. Shaded regions represent the sample standard deviation.

Figure 3: Two-stage interpolation between dense and sparse matrices. From left to right the sparsity of interactions increases. Shaded regions represent the sample standard deviation.

### Invariance to Diffeomorphisms

The benchmark results (Fig. 2) as well as the experiments with function \(x|x|^{k}x\) lengthening tails (Fig. 4) suggest that even if ground-truth MI is preserved by a transformation (see Theorem 2.1), the finite-sample estimates may not be invariant. This can be demonstrated using the spiral diffeomorphism.

We consider isotropic multivariate normal variables \(X=(X_{1},X_{2})(0,I_{2})\) and \(Y(0,1)\) with \((X_{1},Y)=\). Then, we apply a spiral diffeomorphism \(s_{v}^{2}^{2}\) to \(X\), where

\[s_{v}(x)= v||x||^{2}&- v||x||^{2}\\  v||x||^{2}& v||x||^{2}\]

is the rotation around the origin by the angle \(v||x||^{2}\) (see Fig. 5; for a theoretical study of spiral diffeomorphisms see Appendix B). In Fig. 5, we present the estimates for \(N=10\,000\), \(=0.8\) and varying speed \(v\).

In general, the performance of the considered estimators deteriorates with stronger deformations. This is especially visible for the CCA estimator, as MI cannot be reliably estimated in terms of correlation.

### High Mutual Information

Gao et al. (2015) proves that estimation of MI between strongly interacting variables requires large sample sizes. Poole et al. (2019); Song and Ermon (2020) investigate estimation of high MI using multivariate normal distributions in the setting when the ground-truth MI is changing over the training time. We decided to use our family of expressive distributions to see the practical limitations of estimating MI. We chose three one-parameter distribution families: the \(3{}3\) multivariate normal distribution with correlation \(=(X_{1},Y_{1})=(X_{2},Y_{2})\), and its transformations by the half-cube mapping and the spiral diffeomorphism with \(v=1/3\). We changed the parameter to match the desired MI.

In Fig. 6 we see that for low enough MI neural estimators and KSG reliably estimate MI for all considered distributions, but the moment at which the estimation becomes inaccurate depends on the distribution. For example, for Gaussian distributions, neural estimators (and the simple CCA baseline) are accurate even up to 5 nats, while after the application of the spiral diffeomorphism, the estimates become inaccurate already at 2 nats.

We see that in general neural estimators perform well on distributions with high MI. The performance of CCA on the multivariate normal distribution suggests that well-specified model-based estimators may require lower number of samples to estimate high mutual information than model-free alternatives (cf. Gao et al. (2015)).

## 5 Related Work

Existing BenchmarksKhan et al. (2007) used one-dimensional variables and an additive noise assumption to evaluate the estimators based on histograms, nearest neighbors, and kernels. However,

Figure 5: Left: Spiral diffeomorphism with different speed parameter \(v\). Right: Performance of considered estimators for increasing speed.

the original code and samples are not available. Song and Ermon (2020) proposed to evaluate neural estimators based on variational lower bounds (Oord et al., 2018; Belghazi et al., 2018) on image data. However, besides the multivariate Gaussian which they also evaluate, the ground-truth MI is not available. Therefore, they proposed self-consistency checks, which include \((X;Y){=}0\) for independent \(X\) and \(Y\), and preservation of the data processing inequality. Poole et al. (2019) study neural estimators in settings employing Gaussian variables, affine transformation followed by axis-wise cubic transformation and investigate the high MI cases. In our benchmark, we focus on distributions with known MI and information-preserving continuous injective mappings such as homeomorphisms, evaluate long tail distributions, and compare representative estimators from different classes and platforms. We summarize the differences between the benchmarks in Table 1.4

**Causal Representation Learning and Nonlinear ICA** Recent work on disentanglement and nonlinear independent component analysis (ICA) aims at identifying causal sources up to an ambiguous transformation (Xi and Bloem-Reddy, 2022). Examples include orthogonal transformations (Zimmermann et al., 2021), permutations and diffeomorphisms applied along specific axes (Khemakhem et al., 2020), or the whole diffeomorphism group (Von Kugelgen et al., 2021; Daunhawer et al., 2023). To check whether the learned representations indeed capture underlying causal variables, it is important to use estimators which are invariant to the ambiguous transformations specific to the model. Our results suggest that such requirements are not met by MI estimators and practitioners should carefully choose the used metrics, so they are not only theoretically invariant, but also can be robustly estimated in practice.

## 6 Conclusions and Limitations

Venturing beyond the typical evaluation of mutual information estimators on multivariate normal distributions, we provided evidence that the evaluation on Gaussian distributions results in overly optimistic and biased results for existing estimators. In particular, a simple model-based approach as

    & Khan et al. & Poole et al. & Song and & Proposed \\  & (2007) & (2019) & Ermon (2020) & Benchmark \\  Gaussian Distrib. & \(\) & \(\) & \(\) & \(\) \\ Long-Tailed Distrib. & (\(\)) & (\(\)) & \(\) & \(\) \\ Sparse/Dense Inter. & \(\) & \(\) & \(\) & \(\) \\ High MI & \(\) & (\(\)) & (\(\)) & \(\) \\ Invariance to Rameon. & \(\) & \(\) & \(\) & \(\) \\ Self-Consistency & \(\) & \(\) & \(\) & \(\) \\ Language Indep. & \(\) & \(\) & \(\) & \(\) \\ Code Available & \(\) & \(\) & \(\) & \(\) \\   

Table 1: Comparison between benchmarks.

Figure 6: We select three one-parameter families of distributions varying the MI value (\(x\)-axis and dashed line). We use \(N=10\,000\) data points and \(n=5\) replicates of each experiment.

CCA is already sufficient if the distribution is (close to) multivariate normal, highlighting the effect of using available prior information about the problem to design efficient estimators. To provide a basis for more thorough evaluation, we designed a benchmark, compatible with any programming language, with forty general tasks focusing on selected problems, as estimation with sparse interactions, the effects of long tails, estimation in problems with high MI, and (only theoretical) invariance to diffeomorphisms.

Our results suggest that in low- and medium-dimensional problems the KSG estimator remains a solid baseline, requiring a lower number of samples than the alternatives and no parameter tuning. In high-dimensional settings where the interactions are sparse, neural estimators offer an advantage over classical estimators being able to more consistently pick up the signal hidden in the high-dimensional space. We further provide evidence that although MI is invariant to continuous injective transformations of variables, in practice the estimates can differ substantially. Appropriate data preprocessing may mitigate some of these issues, but no strategy clearly outperforms the others (see Appendix E).

We hope that the proposed benchmark encourages the development of new estimators, which address the challenges that we highlight in this paper. It can be used to assess invariance to homeomorphisms, understand an estimator's behavior in the situations involving distributions of different complexity, low-data behaviour, high mutual information or to diagnose problems with implementations. We believe that some parts of the benchmark could also be used to assess whether other estimators of other statistical quantities, such as kernel dependence tests, transfer entropy or representation similarity measures, can be reliably estimated on synthetic problems and if they are robust to selected classes of transformations.

Limitations and Open ProblemsAlthough the proposed benchmark covers a wide family of distributions, its main limitation is the possibility to only transform the distribution \(P_{XY}\) via transformations \(f g\) to \(P_{f(X)g(Y)}\) starting from multivariate normal and Student distributions. In particular, not every possible joint distribution is of this form and extending the family of distributions which have known MI and allow efficient sampling is the natural direction of continuation. We are, however, confident that due to the code design it will be easy to extend the benchmark with the new distributions (and their transformations) when they appear.

Estimation of information in multivariate Student distributions remains a challenge for most estimators. Although we provide the evidence that distributions with longer tails pose a harder challenge for considered estimators, shortening the tails with \(\) transform did not alleviate all the issues. In neural estimators this can be partly explained by the fact that the critic may not learn to approximate pointwise mutual information (up to an additive constant) properly, as demonstrated in Appendix E, but the question of what makes the neighborhood-based estimator, KSG, fail, remains open.

As noted, unless strong inductive biases can be exploited (as illustrated for CCA), estimation of high MI is an important challenge. An interesting avenue towards developing new estimators can be by incorporating a stronger inductive bias, which could also take a form of development of better data normalization strategies and, in case of neural estimators, critic architectures. Better normalization strategies may make the estimators more invariant to considered transformations.

#### Acknowledgments and Disclosure of Funding

We would like to thank Craig Hamilton for the advice on scientific writing and help with the abstract. FG was supported by the Norwegian Financial Mechanism GRIEG-1 grant operated by Narodowe Centrum Nauki (National Science Centre, Poland) 2019/34/H/NZ6/00699. PC and AM were supported by a fellowship from the ETH AI Center.