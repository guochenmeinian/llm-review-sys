# Inference Scaling Laws:

An Empirical Analysis of Compute-Optimal

Inference for LLM Problem-Solving

 Yangzhen Wu\({}^{1}\), Zhiqing Sun\({}^{2}\), Shanda Li\({}^{2}\), Sean Welleck\({}^{2}\), Yiming Yang\({}^{2}\)

\({}^{1}\)Institute for Interdisciplinary Information Sciences, Tsinghua University

\({}^{2}\)School of Computer Science, Carnegie Mellon University

wuyangch21@mails.tsinghua.edu.cn

{zhiqings, shandal, swelleck, yiming}@cs.cmu.edu

Work done during the visit at Carnegie Mellon University.

###### Abstract

While the scaling laws of large language models (LLMs) training have been extensively studied, optimal inference configurations of LLMs remain underexplored. We study _inference scaling laws_ and _compute-optimal inference_, focusing on the trade-offs between model sizes and generating additional tokens with different inference strategies. As a first step towards understanding and designing compute-optimal inference methods, we studied cost-performance trade-offs for inference strategies such as greedy search, majority voting, best-of-\(n\), weighted voting, and two different tree search algorithms, using different model sizes and compute budgets. Our findings indicate smaller models (e.g., Llemma-7B) can outperform larger models given the same computation budgets, and that smaller models paired with advanced inference algorithms yield Pareto-optimal cost-performance trade-offs. For instance, the Llemma-7B model, equipped with our novel tree search algorithm, consistently outperforms Llemma-34B with standard majority voting on the MATH benchmark across all FLOPs budgets. We hope these findings contribute to a broader understanding of inference scaling laws for LLMs.2

## 1 Introduction

Scaling laws of neural networks (Hestness et al., 2017; Rosenfeld et al., 2019) have been established across a range of domains, including language modeling (Kaplan et al., 2020; Hoffmann et al., 2022; OpenAI, 2023), image modeling (Henighan et al., 2020; Yu et al., 2022; Peebles and Xie, 2023), video modeling (Brooks et al., 2024), reward modeling (Gao et al., 2023), and board games (Jones, 2021). These studies have demonstrated how model performance is influenced by both the size of the model and the amount of training computation. However, there is limited knowledge on how varying the compute during _inference_ affects model performance after the model has been trained.

To improve the task performance of large language models (LLMs), inference techniques typically involve additional computation as a _performance maximization_ step at inference time (Nye et al., 2021; Wei et al., 2022; Wang et al., 2022; Yao et al., 2023; Chen et al., 2024). The computational cost of these techniques must be taken into account for _compute-optimal inference_. For example, a Monte Carlo Tree Search (MCTS) method (Jones, 2021) may improve task performance, but potentially require much more compute than simply sampling solutions multiple times. Generally speaking, we need a comprehensive understanding of how various inference-time methods (e.g., best-of-\(n\), majority voting (Wang et al., 2022)) trade off between performance and cost. To improveour understanding, this paper presents a thorough empirical evaluation with careful analysis over various configurations of representative LLMs and inference algorithms.

Specifically, we explore how to select an optimal size for the language model and an effective inference strategy (e.g., greedy search, majority voting, best-of-\(n\), weighted voting, and their tree-search variants) to maximize performance (i.e., accuracy) with a given compute budget. We control the inference computation (FLOPs) of a fixed model by generating more tokens through the language model3, sampling further candidate solutions, and ranking them with a reward model. We analyze the performance of fine-tuned models of various sizes given different inference FLOPs on mathematical reasoning benchmarks (e.g., GSM8K test set (Cobbe et al., 2021) and MATH500 test set (Hendrycks et al., 2021, Lightman et al., 2023)b). Our experiments cover several model families, including general-purpose LLMs (e.g., Pythia (Biderman et al., 2023) & Mistral (Jiang et al., 2023)) as well as math-specialized ones (e.g., Llemma (Azerbayev et al., 2023)).

Our results on Pythia (Fig. 1) illustrate how performance scales with increased inference compute across various model sizes. Typically, increasing the compute budget leads to higher accuracy until the accuracy reaches saturation. As the compute budget increases, smaller models initially perform better than larger ones, but once the accuracy of the smaller models saturates, the larger models have favorable performance. The right panel of Figure 1 demonstrates that the optimal model size for inference varies with different levels of computation. However, in real-world deployment, the available computation is typically much lower than the point where the accuracy of relatively small models saturates and larger models begin to show their advantage (as shown in Figure 2, where the 7B model outperforms the 34B model until 128 Llemma 7B solutions are sampled). This indicates that relatively smaller models could be more compute-optimal for inference.

We have also found that the commonly-used MCTS method does not perform well with weighted voting, as it often yields many unfinished solutions, hence having less effective votes. To address this issue, we propose a novel tree search algorithm, _REward BAIanced SEarch_ (REBASE), which pairs well with weighted voting and achieves a Pareto-optimal trade-off between accuracy and inference compute. The key idea of REBASE is to use a node-quality reward to control node expansion, which eliminates the need for explicit rollouts while ensuring enough candidate solutions for voting.

In our experiments, REBASE consistently outperforms sampling and MCTS methods across all settings, models, and tasks. Importantly, we find that REBASE with a _smaller_ language model typically achieves a Pareto-optimal trade-off. For instance, we show that the Llemma-7B model can achieve competitive accuracy to a Llemma-34B model while using \(2\) less FLOPs when evaluating on MATH500 (Fig. 2) or GSM8K (Fig. 3). Moreover, Llemma-7B with REBASE outperforms Llemma-34B with standard majority voting across _all_ compute budgets. Our results show the value of using smaller models with advanced inference-time algorithms, and the benefits of new algorithms for achieving better returns on inference-time compute.

### Problem Formulation

We explore the following question: _Given a fixed FLOPs budget, how should one select an optimal model size for the policy model, and an effective inference strategy to maximize performance (i.e., accuracy)?_.

To address this, we represent the problem-solving error rate \(E(N,T;)\) as a function of the number of model parameters \(N\), the number of generated tokens \(T\) and the inference strategy \(\). The computational budget \(C\) is a deterministic function \((N,T;)\), based on \(N\) and \(T\). Our goal is to minimize \(E\) under the test-time compute constraint \((N,T,)=C\):

\[(N_{}(C),T_{}(C);)=*{arg\,min}_{( N,T,)\ (N,T,)=C}E(N,T;)\]

where \(N_{}(C)\) and \(T_{}(C)\) denote the optimal allocation of a computational budget \(C\).

Here, the inference computation (FLOPs) for a fixed model can be modulated by generating more tokens with the policy model, e.g., by sampling additional candidate solutions and subsequently ranking them using a reward model. We primarily consider sampling and tree-search approaches with reranking or Majority Voting as the means to consume more tokens.

[MISSING_PAGE_FAIL:3]

## 3 Compute-Optimal Inference

### Reward Balanced Search (REBASE)

The REBASE tree search method inherits the exploitation and pruning properties of tree search, while using the reward model alone to estimate the nodes' qualities without additional computation for estimating values by sampling children. The details are provided below:

**Notations.** We view the fine-tuned LLM as a policy \(_{}\) which generates the solution step by step. Given a question \(x\) and the first \(k\) steps of a solution \(r_{1} r_{k}\), the \((k+1)\)-th step is sampled from \(_{}(|xr_{1} r_{k})\). REBASE effectively generates a solution tree during inference, in which the root node the question \(x\) and other nodes corresponds to solution steps. When generating solution trees, we generate children of \(r_{k}\) by sampling from \(_{}(|xr_{1} r_{k})\). Here we slightly abuse notations and use the corresponding question/solution step to denote a node. The reward of a node \(r_{k}\) is generated by the PRM: \(R(r_{k}):=R(qr_{1} r_{k})\).

**Initialization.** Given the question \(x\), balance temperature \(T_{b}>0\), and sampling number of solutions \(N\), we sample \(N\) instances of the first step for the question, yielding all the nodes of depth 1 in the search tree. We let the sampling budget of depth 0, \(B_{0}\), to \(N\) at initialization.

**Reward modeling and update.** In the \(i\)-th iteration, the PRM assigns the rewards to all the nodes at depth \(i\). After that, the algorithm examines whether the solutions up to depth \(i\) are complete. Supposing there are \(C_{i}\) completed solutions, we update the sampling budget using \(B_{i} B_{i-1}-C_{i}\). If \(B_{i}=0\), the process ends, and we obtain \(N\) solutions.

Figure 3: **The inference computation scaling comparisons across model sizes**. The left/right panel shows the problem-solving error rate on GSM8K based on Weighted Majority/Best-of-N. MCTS is not included in the comparison because of its poor compute-accuracy trade-off.

Figure 2: **The inference computation scaling comparisons across model sizes**. The left/right panel shows the problem-solving error rate on MATH based on Weighted Majority/Best-of-N.

**Exploration balancing and expansion.** For all of the nodes \(n_{j}\) with reward \(R(n_{j})\) in the depth \(i\) of the tree, we calculate the expansion width of the \(n_{j}\) as:

\[W_{j}=(B_{i})/T_{b})}{_{k} (R(n_{k})/T_{b})}).\]

Then we sample \(W_{j}\) children for \(n_{j}\) for all the nodes in depth \(i\), and start the next iteration.

Intuitively, when the balance temperature \(T_{b}\) is small, this method encourages more exploitation which put much more compute budget on the nodes with high score, when \(T_{b}\) is large, it encourages exploration where nodes with high score and low score are explored equally. In our experiment, we have found \(T_{b}\) in the range of \((0.1,0.3)\) works well for our process reward model.

### Comparing REBASE to Other Baselines

#### REBASE is Pareto-optimal.

REBASE consistently achieves the best cost-performance tradeoffs, outperforming the sampling-based methods in all settings when fixing the model and the evaluation task (Fig. 2, 3, 4, and 5). For example, in Figure 2, REBASE is the compute-optimal strategy at all inference compute budgets, with 7B typically the optimal model size. On the other hand, MCTS underperforms the sampling-based methods at each compute budget, likely due to its costly rollouts (Figure 2) compared to the efficient use of the reward model in REBASE.

Table 1 shows that REBASE achieves better accuracy with a lower compute budget compared to sampling-based weighted voting. With the 7B model, REBASE achieves higher accuracy with 7 times less compute. This finding is novel, and differs from previous tree search methods that typically improve the performance at the cost of higher computational expense compared to sampling-based voting (Chen et al., 2024a, Xie et al., 2023).

Weaker models gain more from tree search.For example, our proposed REBASE leads to \(5.3\%\), \(3.3\%\), and \(2.6\%\) performance gains on MATH for Mistral-7B, Llemma-7B, Llemma-34B, respectively. The order of accuracy increase is inversely related to the model's corresponding greedy search accuracy on those datasets. This suggests that weaker models, as indicated by their lower greedy search accuracy, benefit more from tree search methods like REBASE.

#### REBASE saturates later than sampling with higher accuray.

From Fig. 2 and Fig. 3, we observe that REBASE saturates later than the sampling methods, with lower final error rates. This is the evidence that REBASE improves the reasoning paths, due to the selection and pruning mechanism in the intermediate steps, REBASE discards the low quality partial paths and exploring more on good ones, results in a higher probability of sampling high-quality reasoning paths.

## 4 Conclusions and Limitations

Conclusions.In this work, we conducted a comprehensive empirical analysis of inference scaling law and compute-optimal inference for problem-solving with language models. We examined the scaling effect of computation during inference across different model sizes and found that while increased computation generally leads to higher performance, the optimal model size varies with the available compute budget. When the computation budget is limited, smaller models can be preferable. Additionally, we introduce our novel tree search method, REBASE, which is more compute-optimal than both sampling methods and Monte Carlo Tree Search (MCTS). REBASE typically achieves higher accuracy while using several times less computation than sampling methods. Our results underscore the potential of deploying smaller models equipped with sophisticated inference strategies like REBASE to enhance problem-solving accuracy while maintaining computational efficiency.

Limitations.Our empirical analysis specifically targets mathematical problem-solving. Investigating the inference scaling laws and compute-optimal inference strategies for tasks beyond mathematical problem-solving would be a valuable direction for future research. Additionally, we mainly evaluate the proposed REBASE on the GSM8K and MATH500 datasets. We speculate that the REBASE algorithm, which assumes access only to a function that assigns scores to nodes, will be effective in tasks beyond those studied here.