# A Unified Principle of Pessimism for Offline Reinforcement Learning under Model Mismatch

Yue Wang

Department of Electrical and Computer Engineering

University of Central Florida

Orlando, FL, 32816

yue.wang@ucf.edu

&Zhongchang Sun

Department of Electrical Engineering

University at Buffalo

Buffalo, NY, 14260

zhongcha@buffalo.edu

&Shaofeng Zou

School of Electrical, Computer and Energy Engineering

Arizona State University

Tempe, AZ 85287

zou@asu.edu

###### Abstract

In this paper, we address the challenges of offline reinforcement learning (RL) under model mismatch, where the agent aims to optimize its performance through an offline dataset that may not accurately represent the deployment environment. We identify two primary challenges under the setting: inaccurate model estimation due to limited data and performance degradation caused by the model mismatch between the dataset-collecting environment and the target deployment one. To tackle these issues, we propose a unified principle of pessimism using distributionally robust Markov decision processes. We carefully construct a robust MDP with a single uncertainty set to tackle both data sparsity and model mismatch, and demonstrate that the optimal robust policy enjoys a near-optimal sub-optimality gap under the target environment across three widely used uncertainty models: total variation, \(^{2}\) divergence, and KL divergence. Our results improve upon or match the state-of-the-art performance under the total variation and KL divergence models, and provide the first result for the \(^{2}\) divergence model.

## 1 Introduction

Reinforcement learning (RL)  learns a policy to maximize cumulative rewards through online interactions with the environment. However, in real-world applications such as autonomous vehicles  and health care , the trial-and-error nature of interacting with the environment can be both costly and dangerous, rendering online learning impractical. To circumvent these challenges, the concept of offline RL has emerged , seeking to learn an optimal policy from a pre-collected dataset, eliminating the need for real-time interaction with the environment.

Despite its potential, offline RL faces two significant challenges that impact its performance. The first challenge stems from the nature of the offline dataset itself. Offline RL demonstrates impressive performance only when the dataset is of high quality, as exemplified by . However, in most RL scenarios, the data collection process can be expensive, constrained, and subject to specific behavior policies. This often results in a dataset with insufficient samples and limited coverage. The limited coverage of state-action pairs in the dataset presents a substantial hurdle, making it challenging to fully learn the environment model and the model learned may be inaccurate, leading to a poorly performing policy. and hindering the discovery of the optimal policy, particularly whenthe behavior policy is sub-optimal. Existing solutions involve introducing pessimism during policy learning, by penalizing the reward function for state-action pairs that are not adequately covered by the dataset, known as the lower confidence bound (LCB) approach , demonstrating potential both numerically and theoretically in solving offline RL.

The second critical challenge in offline RL pertains to its vulnerability to model mismatch. As the offline dataset is collected in advance, it only encapsulates the environment's information at the time of data collection. In practical applications, environments often undergo variations due to e.g., unexpected perturbations, heterogeneity, or non-stationarity. This inherent variability introduces a model mismatch between the target environment and the one where the dataset is collected, leading to significant performance degradation when deploying the learned policy in the real environment. Robust RL or distributionally robust optimization (DRO) framework is then proposed to address this challenge , which incorporates the pessimism principle to effectively tackle model uncertainty. By constructing an uncertainty set containing 'plausible' environments, robust RL optimizes the worst-case performance among them, providing a performance guarantee for the real environment.

To summarize, there are two sources of uncertainty in offline RL: **Uncertainty from limited data:** This stems from the inherent uncertainty introduced by limited offline datasets and lack of exploration; **Uncertainty from model-mismatch:** This arises from distribution shifts between data-collection and deployment environments, as well as between data-collection distribution and the distribution induced by the optimal policy. Each challenge can be addressed through its corresponding principle of pessimism, as in previous works: limited data coverage can be mitigated with reward estimation penalties (LCB), while model mismatch can be tackled with distribution estimation penalties (DRO). However, existing approaches often address these uncertainties separately, e.g., , leading to methodological redundancy or complexity (more discussions and comparisons can be found in Section 5). In this paper, we propose a unified framework that integrates both principles of pessimism into a **single** robust Markovian decision process (MDP) model for offline RL, offering a clear and streamlined conceptual formulation and providing improved or matched theoretical guarantees compared to existing methods. Our major contributions can be summarized as follows.

**A unified distributionally robust formulation for offline RL under model mismatch.** As discussed before, offline RL faces challenges including limited dataset coverage and model mismatch. Specifically, we first tackle the challenge of model mismatch using the approach of distributionally robust MDP, which optimizes the worst-case performance over an uncertainty set specified by e.g., total variation, \(^{2}\) or Kullback-Leibler divergence. We then show that the uncertainty from data sparsity can be transformed into a data-dependent penalization term that is added to the radius of the constructed uncertainty set. Our formulation hence unifies the two principles of pessimism to a **single**DRO problem that tackles both sources of uncertainty without additional structures and enjoys an easier implementation compared to existing works.

**Augmented design of radius to achieve tight theoretical guarantees.** Designing the uncertainty set for the model mismatch is typically straightforward, often relying on domain knowledge . However, incorporating an additional penalty term to address data sparsity presents significant challenges. Balancing the conservativeness for less-visited state-action pairs estimation and the overall performance of learned policies requires careful consideration. In our work, we conduct a meticulous analysis to understand how the penalty radius impacts the performance of the learned policy. We then design penalty radii for three widely studied uncertainty set models: total variation, \(^{2}\) divergence, and KL divergence. Our analysis provides insights into the performance of learned policies under these penalty radii, showcasing the versatility of our framework across both metric-based and non-metric-based models. Moreover, our designs enable us to derive tight theoretical guarantees. Specifically, we improve upon existing results for robust offline RL under the total variation model , match the state-of-the-art performance under the KL divergence model , and present the first result under the \(^{2}\) divergence model.

[MISSING_PAGE_FAIL:3]

captures such a model mismatch as in (2), aiming to learn a policy that performs well under the true deployment environment through the robust RL framework:

\[^{*}=\,V_{}^{}(),=_{s,a}_{s}^{a},\,\,_{s}^{a}=\{q():D(q,_{s}^{a}) R\}. \]

where \(R\) represents the similarities of the two environments. \(D\) and \(R\) are generally designed by domain experts, so we do not focus on their design and consider them as pre-settled in this work.

A key challenge here is that the nominal transition kernel \(_{0}\) is unknown, but only an offline dataset generated from \(_{0}\) is given. Due to the limited exploration and distributional shift, the dataset \(\) may not cover all possible states or transitions that the agent might encounter in the environment.

To guarantee that a provable efficient algorithm can be designed based on the dataset \(\), we adopt an assumption on the distributional mismatch between the dataset distribution and the occupancy measure induced by a comparator policy \(^{*}\). A comparator policy refers to a policy having satisfying worst-case performance and is set to be the optimal robust policy in many cases, but our approach can be applied to an arbitrary policy that may not be optimal.

**Assumption 1**.: _(Robust single-policy clipped concentrability ). The data distribution \(\) satisfies_

\[C^{^{*}}_{(s,a,) }}^{^{*}}(s,a),\}}{( s,a)}<+. \]

In Assumption 1, we only require that the dataset covers the state-action pairs that are visited by the comparator policy, known as partial coverage. When there is no model mismatch, Assumption 1 reduces to the single-policy clipped concentrability assumption in  for non-robust offline RL.

We hence formulate the offline RL problem with model mismatch as the following concrete problem: _Solve the robust RL problem (5) using only a fixed offline dataset \(\) satisfying Assumption 1._

## 4 Framework Design and Main Results

To simultaneously address the two sources of uncertainty (1) uncertainty arising from the model mismatch between the data-collected environment and the deployment environment; and (2) uncertainty in the nominal (data-collection) model estimation attributed to an insufficient amount of data and limited coverage of the offline dataset, we develop a unified principle of pessimism, and theoretically characterize the finite sample complexity of the proposed algorithms. Our algorithms are easier and more efficient to implement than existing approaches, and yield improved or matching results.

Denote \(N(s,a)=_{i=1}^{N}_{(s_{i},a_{i})=(s,a)}\) as the number of samples that transit from \((s,a)\) in \(\), where \(\) is the indicator function. The empirical transition kernel is then obtained as

\[}_{s,s^{}}^{a}=^{N}_{(s_{i},a_{i},s^{}_{i})=(s,a,s^{})}}{N(s,a)},&N(s,a)>0\\ ,&N(s,a)=0, \]

and we also define the empirical reward function as

\[(s,a)=,a_{i})=(s,a)}r_{i}}{N(s,a)},& N(s,a)>0\\ 0,&N(s,a)=0. \]

The MDP \(=(,,},)\) with the empirical transition kernel \(}\) and empirical reward function \(\) is referred to as the empirical MDP, representing our estimation of the nominal model from the dataset. A straightforward approach is to construct an uncertainty set \(}\) by replacing the nominal kernel \(\) in (5) by \(}\):

\[}_{s}^{a}=\{q():D(q,}_{s}^ {a}) R\} \]

and solve the corresponding robust RL problem. Although it tackles the model mismatch uncertainty through the uncertainty set, it generally results in a sub-optimal performance due to the lack of consideration of the estimation error within the empirical model.

To tackle this estimation error, previous works introduce some additional structures other than the above DRO framework (9), including a penalty term in reward  or an additional uncertainty set and making it a bi-level DRO  (more discussions and comparisons can be found in Section 5). In this work, we construct a **single, unified** uncertainty set of environments, showing that such an estimation error can be incorporated into the distribution uncertainty set and developing a unified principle of pessimism to address both uncertainties. Specifically, for any \(s\) and \(a\), we set

\[}^{a}_{s}=\{q():D(q,}^ {a}_{s}) R+^{a}_{s}\}, \]

where \(^{a}_{s}\) is a function (will be specified later) inversely proportional to \(N(s,a)\) that measures the degree of confidence when estimating the empirical transition kernel; and \(R\) accounts for the model mismatch. Intuitively, for the less-observed state-action pairs, \(^{a}_{s}\) becomes larger and we are less confident and more pessimistic when estimating the transition kernel \(}^{a}_{s}\), and vice versa.

We claim and show later that the additional uncertainty or pessimism by enlarging the uncertainty set effectively tackles the uncertainty from the limited dataset, and the whole uncertainty set results in a conservative estimation of the worst-case performance. We then optimize the worst-case performance under this uncertainty set \(}\), which can be efficiently solved through the standard robust dynamic programming approach [28; 13]. Our algorithm is presented as Algorithm 1. For the convenience of analysis, we modify the vanilla robust value iteration algorithm  by setting the output policy to select actions that occur in the dataset when there is a tie. Such an output policy always exists (shown in Lemma 11 in the Appendix) and enables us to derive a tighter analysis of the sub-optimality gap. It is also worth noting that for all the uncertainty set models we consider, our Algorithm 1 can be efficiently applied with a polynomial computational complexity and a linear convergence rate .

```
Input:\(\), \(V=0\)  Estimate the empirical reward \(\) and empirical uncertainty set \(}\) according to (8) and (10) repeat \(V()_{a}\{(,a)+_{ }^{a}_{s}}(V)\}\) until convergence for\(s\)do \((s)\{_{a}(s,a)+_{ }^{a}_{s}}(V)\}\{a:N(s,a)>0\}\) endfor Output:\(\)
```

**Algorithm 1** Robust Value Iteration for Offline RL with Model Mismatch

In the following sections, we consider three widely used uncertainty set models: total variation, \(^{2}\) divergence, and KL divergence models. We show that, for all three models, the uncertainty from the dataset can be incorporated into the uncertainty set as an additional term in the radius, and with a carefully designed uncertainty set, the optimal robust policy \(\) obtained has a near-optimal performance under the target uncertainty set \(\), i.e., our approach effectively and efficiently solves the offline RL problems under model mismatch.

### Total Variation (TV)

We first consider the case of TV defined uncertainty set2. Specifically, the uncertainty set is constructed using \(D(q,p)=\|q-p\|_{1}\) in eq. (10). Note that when the radius is greater than 2, the defined uncertainty set reduces to the whole probability simplex \(()\). Our first attempt is to set \(^{a}_{s}\) large enough such that the true uncertainty set \(\) falls into the constructed one \(}\), such that the robust value function of \(}\) lower bounds the true robust value function \(V^{}_{}\), as a conservative estimation. We note that as long as the true nominal transition kernel \(\) and the empirical transition kernel \(}\) are close: \(\|^{a}_{s}-}^{a}_{s}\|^{a}_{s}\) holds with high probability, the triangle inequality implies \(}\) with high probability. Therefore, the optimal robust policy \(\) provides a performance guarantee for the original problem (5). Following this approach, we can show the following results.

**Theorem 1**.: _Consider TV defined uncertainty set. For each state-action pair \((s,a)\), set \(^{a}_{s}=}{2N(s,a)}}\). Then with probability at least \(1-2\), the output policy \(\) of algorithm 1 satisfies that_

\[V_{}^{^{*}}()-V_{}^{}()}(C^{^{*}}}{(1-)^{4}N}}), \]

_where \(}\) notation absorbs universal constants and log terms._

**Remark 1**.: _To achieve an \(\) sub-optimality gap, a dataset of size \(N=}(S^{2}C^{^{*}}(1-)^{-4}^{-2})\) is required. This result matches the previous one in ._

The construction above is based on distribution, to ensure the resulting uncertainty set \(}\) is larger than the target one \(\). However, such a distribution-based construction can result in an overly large uncertainty set and an overly pessimistic policy, as observed in . To improve, one observation is that we can design a smaller uncertainty set such that the resulting robust value function \(V_{}^{}\) approximately lower bounds the true robust value function, without using the distribution-based framework. We hence further design a novel uncertainty set that turns out to be less conservative (the intuition of such a design will be discussed in the next section).

**Theorem 2**.: _For any \((s,a)\), let \(^{a}_{s}=}{N(s,a)}\). For the output policy \(\) of algorithm 1, with probability at least \(1-4\), it holds that_

\[V_{}^{^{*}}()-V_{}^{}()}(}S+}}{N(1-)^{ 3}}}). \]

_Here, \(_{}_{s,a}\{(s,a):(s,a)>0\}\) denotes the smallest non-zero entry of the distribution \(\) that generates the dataset._

**Remark 2**.: _Combining the two results, we showed that our approach can obtain an \(\)-optimal robust policy when the dataset is of size \(}}S}{(1-)^{6}^{2}} \{,}\}\) for the TV defined model. This result is better than the previous result in , illustrating our approach is more efficient. Moreover, our framework is much simpler than the one in  and can be effectively solved in a polynomial time (see detailed discussion in Section 5)._

### \(^{2}\) Divergence

We then study the uncertainty models with the \(^{2}\) divergence: \(D(p,q)=_{s}q(s)(1-)^{2}\). Note that \(^{2}\) divergence is not a metric, implying the failure of the triangle inequality and the distribution-based design as in the previous section. While it is possible to design \(\) such that \(D(}^{a}_{s},^{a}_{s})^{a}_{s}\), it does not imply \(^{a}_{s}}^{a}_{s}\) and no lower bound guarantee can be obtained.

To address this issue, we similarly adapt the value function-based construction, by taking a closer look at the error decomposition. The main idea of our distribution-based construction for the TV defined model is to construct an uncertainty set that is large enough to include the true transition kernel with high probability. Then the sub-optimality gap can be decomposed as

\[V_{}^{^{*}}-V_{}^{}=}^{^{*}}-V_{}^{}}_{_{1}}+ }^{}-V_{}^{}}_{ _{2} 0}, \]

and \(_{2} 0\) from \(^{a}_{s}}^{a}_{s}\), which however fails under non-metric models. On the other hand, if we further decompose \(_{2}\) as

\[V_{}^{}-V_{}^{}=}^{}-V_{}^{}}_{_{21}}+ }^{}-V_{}^{}}_{ _{22}}, \]

where \(}\) is the empirical uncertainty set (9). We note that \(_{22}\) is the concentration error due to the limited dataset, which is independent of the term \(^{a}_{s}\) (ignoring the dependence of \(\) on \(\) for discussionconnivance); \(_{21}\) will be a negative term since \(}}\), thus we should set the term \(\) such that the negative bound on \(_{21}\) cancels out with the concentration bound on \(_{22}\), leading to a non-zero yet tight overall bound on \(_{2}\). Instead of ensuring the uncertainty set inclusion, we directly ensure the bound on the value function difference \(_{2}\) is small. Clearly, the clue function-based design can be applied to both non-metric and metric models, and is less conservative than the distribution-based for the metric models, since closeness in distribution is stronger than the closeness in value functions . This observation results in our second design for the total variation model in Theorem 2, showing an improvement in sample complexity.

Based on this intuition, we present our radius design and results.

**Theorem 3**.: _Consider \(^{2}\) divergence-defined uncertainty set. For any \((s,a)\), let \(^{a}_{s}=}(})\). When \(N}(}_{}^{2}})\), with probability at least \(1-4\), the output policy \(\) of algorithm 1 satisfies_

\[V^{^{*}}_{}()-V^{}_{}()}(}S}{N(1-)^{4}}}). \]

**Remark 3**.: _To achieve an \(\)-optimal robust policy under the \(^{2}\) model, our approach requires the total number of samples_

\[N=}}S}{(1-)^{4} ^{2}}}_{}+}_{} ^{2}}}_{}.\]

_Our sample complexity has two parts: the \(\)-dependent part which dominates as the accuracy \(\) decreases, and a fixed amount of burn-in cost, whose existence is because we cannot expect to learn a near-optimal policy when the dataset is too limited. When the desired accuracy \(\) decreases, the first term dominates the overall complexity, resulting in the asymptotic result presented in the theorem._

Our approach and result stand for the first concrete study for offline robust RL with \(^{2}\) divergence-defined uncertainty sets. It is also worth noting that our sample complexity asymptotically matches the sample complexity of the model-based robust RL with a generative model . These observations hence demonstrate the optimality of our results and the effectiveness of our approach.

### KL Divergence

In this section, we consider the KL divergence defined uncertainty set, i.e., \(D(p,q)=_{s}p(s)\). Similarly, KL divergence is not a metric, we hence adapt our design discussed above for \(^{2}\) models here. The following theorem presents our design of the uncertainty set and sample complexity results.

**Theorem 4**.: _Consider KL divergence defined uncertainty set. For any \((s,a)\), let \(^{a}_{s}=C_{1}S}{(1-)^{4}N}}{N(s, a)_{}}}\), if \(N}{_{}_{}}\), then with probability at least \(1-4\), the output policy \(\) of algorithm 1 satisfies_

\[V^{^{*}}_{}()-V^{}_{}()}(}S}{(1-)^{4}N_{}}} ). \]

_Here, \(_{}\) and \(}_{}\) represents the minimal non-zero entry of the nominal transition kernel \(\) and empirical nominal kernel \(}\)._

**Remark 4**.: _Similarly, for the KL divergence model, our approach requires a total number of \(}(}S}{(1-)^{4}_{} ^{2}}+_{}_{}})\) samples to find an \(\)-optimal policy. The sample complexity also contains two parts, an asymptotically dominated term and a fixed burn-in cost. Our result matches the one of  and is better than the one of ._

To summarize, our unified framework can be adapted to different uncertainty models and solve offline robust RL problems, offering improved or matched sample complexity results and a more efficient implementation. More discussion can be found in Section 5. We also provide some numerical experiments to verify the effectiveness and efficiency of our algorithm, which can be found in Appendix A. Under all three uncertainty set models, our algorithm enjoys a smaller or similar sample complexity compared to LCB approaches, and always outperform the non-robust dynamic programming approach.

## 5 Related Works

### Comparison with prior art

In this section, we compare our works with the most related existing works [36; 5], where offline RL with model mismatch is studied. Compared to them, our methods enjoy three major advantages: (1). A more unified and straightforward framework of single pessimism principle; (2). Improved or matched sample complexity; And (3). Enhanced computational complexity.

**Unified framework for double pessimism principles.** In , the two principles of pessimism are separately employed, where a reward penalty term \(b\) is used to penalize less visited state-action pairs in addition to the uncertainty set that accounts for the model mismatch, and the update rule is

\[V(s)_{a}r(s,a)+_{}_{a}^{a}}( V)-b(s,a)}. \]

Our approach, on the other hand, enjoys a straightforward and simple formulation. Specifically, we incorporate the two principles of pessimism into a single uncertainty set, developing our unified principle. Moreover, it is noted that design of the penalty term \(b\) is complicated, whereas our addition term \(\) has a simple and clear form. It is worth noting that although the LCB approach and ours share a similar updating rule, there is a fundamental difference in the algorithm design motivation and analysis. In the LCB approach, the penalty term \(b\) is designed such that the resulting estimation \(V\) is less than the true robust value function \(V^{}_{}\), to ensure the conservativeness of LCB approaches; Whereas our resulting estimation \(V^{}_{}\) is not necessarily less than \(V^{}_{}\), since we directly tackle the distribution uncertainty but not through value function estimations. This hence requires novel technique innovations in our analysis.

In another closely related work , a double pessimism principle is adopted to address the two sources of uncertainty, under TV and KL divergence models. Specifically, a TV distance-based uncertainty set \(}\) is first constructed to tackle the model estimation uncertainty from the dataset; Then centered at each transition kernel \(}}\), a second layer of uncertainty set \((})\) to reflect the distributional robustness to model mismatch. They then take the optimal policy of

\[V^{}_{pess^{2}}=_{}}}_{}(})}V^{}_{}} \]

as the output policy. Although their approach indicates that the data estimation uncertainty can also be captured by a distributional uncertainty set, they still employ the two principles separately, although both in the form of DRO formulations. Instead of designing two uncertainty sets as in , we unify the two types of pessimism and use a **single** uncertainty set with a composed design of the radius to address both the model estimation uncertainty and the model mismatch.

**Improved or Matched Sample Complexity.** In terms of sample complexity, the comparison are included in Table 1. Compared to , our theoretical result matches theirs under the KL divergence model, and we moreover develop results for the other two uncertainty set models that are not considered therein. The numerical experiment results can be found in Appendix A, which further verify our discussion. Compared with , our results achieve better sample complexity in both KL and TV models. In the TV model, our sample complexity outperforms  in terms of dependence on \(S\) and \((1-)\); For the KL model, our complexity is linearly dependent on \(S\), while  has a quadratic dependence. Furthermore, as noted in , their result's exponential term can be replaced by utilizing both \(_{}\) and \(_{}\), while our (asymptotic) complexity result depends solely on \(_{}\).

**Enhanced Computational Complexity.** Our algorithms are also better in terms of computational complexity or practical implementations than both baselines.

The two-layer optimization problem in  is an extension of the model studied in  to the robust setting, both involving non-rectangular uncertainty sets that are NP-hard to solve . This creates

[MISSING_PAGE_FAIL:9]

environments. We developed a unified DRO-based framework containing a single uncertainty set with a composed radius of two parts to tackle the two sources of uncertainty discussed above. Our approach can be implemented in a much easier and more straightforward way than existing approaches and can be applied to both metric-based and non-metric-based uncertainty models. Specifically, we investigated three types of uncertainty sets defined by total variation, \(^{2}\) divergence, and KL divergence. Our methodology can be easily extended to handle other uncertainty set models. Among them, we obtain near-optimal sample complexity results that improve or match the existing results under the total variation and KL divergence models and provide the first algorithm and finite sample complexity analysis for the uncertainty set defined by the \(^{2}\) divergence.

**Limitations.** It is in our future interest to extend our unified framework to address large-scale problems. This includes robust MDPs with latent structures, such as linear MDPs, as opposed to previous work that uses the LCB + DRO framework, for example [24; 44], and more general MDPs with function approximation techniques.

## 7 Acknowledgements

Yue Wang is supported by DARPA under Agreement No. HR0011-24-9-0427. The work of Zhongchang Sun and Shaofeng Zou is supported by the National Science Foundation under Grants CCF-2438429 and ECCS-2438392 (CAREER).