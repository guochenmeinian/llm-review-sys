# FourierGNN: Rethinking Multivariate Time Series Forecasting from a Pure Graph Perspective

Kun Yi\({}^{1}\), Qi Zhang\({}^{2}\), Wei Fan\({}^{3}\), Hui He\({}^{1}\), Liang Hu\({}^{2}\), Pengyang Wang\({}^{4}\)

**Ning An\({}^{5}\), Longbing Cao\({}^{6}\), Zhendong Niu\({}^{1}\)**

\({}^{1}\)Beijing Institute of Technology, \({}^{2}\)Tongji University, \({}^{3}\)University of Oxford

\({}^{4}\)University of Macau, \({}^{5}\)HeFei University of Technology, \({}^{6}\)Macquarie University

{yikun, hehui617, zniu}@bit.edu.cn, zhangqi_cs@tongji.edu.cn, weifan.oxford@gmail.com

railmilk@gmail.com, pywang@um.edu.mo, ning.g.an@acm.org, longbing.cao@mq.edu.au

Corresponding author

###### Abstract

Multivariate time series (MTS) forecasting has shown great importance in numerous industries. Current state-of-the-art graph neural network (GNN)-based forecasting methods usually require both graph networks (e.g., GCN) and temporal networks (e.g., LSTM) to capture inter-series (spatial) dynamics and intra-series (temporal) dependencies, respectively. However, the uncertain compatibility of the two networks puts an extra burden on handcrafted model designs. Moreover, the separate spatial and temporal modeling naturally violates the unified spatiotemporal inter-dependencies in real world, which largely hinders the forecasting performance. To overcome these problems, we explore an interesting direction of _directly applying graph networks_ and rethink MTS forecasting from a _pure_ graph perspective. We first define a novel data structure, _hypervariate graph_, which regards each series value (regardless of variates or timestamps) as a graph node, and represents sliding windows as space-time fully-connected graphs. This perspective considers spatiotemporal dynamics unitedly and reformulates classic MTS forecasting into the predictions on hypervariate graphs. Then, we propose a novel architecture _Fourier Graph Neural Network_ (FourierGNN) by stacking our proposed Fourier Graph Operator (FGO) to perform matrix multiplications in _Fourier space_. FourierGNN accommodates adequate expressiveness and achieves much lower complexity, which can effectively and efficiently accomplish the forecasting. Besides, our theoretical analysis reveals FGO's equivalence to graph convolutions in the time domain, which further verifies the validity of FourierGNN. Extensive experiments on seven datasets have demonstrated our superior performance with higher efficiency and fewer parameters compared with state-of-the-art methods. Code is available at this repository: [https://github.com/aikunyi/FourierGNN](https://github.com/aikunyi/FourierGNN).

## 1 Introduction

Multivariate time series (MTS) forecasting plays an important role in numerous real-world scenarios, such as traffic flow prediction in transportation systems , temperature estimation in weather forecasting , and electricity consumption planning in the energy market , etc. In MTS forecasting, the core challenge is to model intra-series (temporal) dependencies and simultaneously capture inter-series (spatial) correlations. Existing literature has primarily focused on the temporal modeling and proposed several forecasting architectures, including Recurrent Neural Network (RNN)-based methods (e.g., DeepAR ), Convolution Neural Network (CNN)-based methods (e.g., Temporal Convolution Network ) and more recent Transformer-based methods (e.g., Informer and Autoformer ). In addition, another branch of MTS forecasting methods has been developed to not only model temporal dependencies but also places emphasis on spatial correlations. The most representative methods are the emerging Graph Neural Network (GNN)-based approaches [10; 2; 11] that have achieved state-of-the-art performance in the MTS forecasting task.

Previous GNN-based forecasting methods (e.g., STGCN  and TAMP-S2GCNets ) heavily rely on a pre-defined graph structure to specify the spatial correlations, which as a matter of fact cannot capture the _spatial dynamics_, i.e., the time-evolving spatial correlation patterns. Later advanced approaches (e.g., StemGNN , MTGNN , AGCRN ) can automatically learn inter-series correlations and accordingly model spatial dynamics without pre-defined priors, but almost all of them are designed by stacking graph networks (e.g., GCN and GAT) to capture _spatial dynamics_ and temporal networks (e.g., LSTM and GRU) to capture _temporal dependencies_. However, the uncertain compatibility of the graph networks and the temporal networks puts extra burden on handcrafted model designs, which hinders the forecasting performance. Moreover, the respective modeling for the two networks _separately_ learn spatial/temporal correlations, which naturally violate the real-world unified spatiotemporal inter-dependencies. In this paper, we explore an opposite direction of _directly applying graph networks for forecasting_ and investigate an interesting question: _can pure graph networks capture spatial dynamics and temporal dependencies even without temporal networks?_

To answer this question, we rethink the MTS forecasting task from a pure graph perspective. We start with building a new data structure, _hypervariate graph_, to represent time series with a united view of spatial/temporal dynamics. The core idea of the hypervariate graph is to construct a space-time fully-connected structure. Specifically, given a multivariate time series window (say input window) \(X_{t}^{N T}\) at timestamp \(t\), where \(N\) is the number of series (variates) and \(T\) is the length of input window, we construct a corresponding _hypervariate graph_ structure represented as \(^{T}_{t}=(X^{T}_{t},A^{T}_{t})\), which is initialized as a fully-connected graph of \(NT\) nodes with adjacency matrix \(A^{T}_{t}^{NT NT}\) and node features \(X^{T}_{t}^{NT 1}\) by regarding _each value_\(x^{(n)}_{t}^{1}\) (variate \(n\) at step \(t\)) of input window as a distinct _node_ of a hypervariate graph. Such a special structure design formulates both intra- and inter-series correlations of multivariate series as pure _node-node dependencies_ in the hypervariate graph. Different from classic formulations that make spatial-correlated graphs and learn dynamics in a two-stage (spatial and temporal) process , our perspective views spatiotemporal correlations as a whole. It abandons the uncertain compatibility of spatial/temporal modeling, constructs adaptive space-time inter-dependencies, and brings up higher-resolution fusion across multiple variates and timestamps in MTS forecasting.

Then, with such a graph structure, the multivariate forecasting can be originally formulated into the predictions on the hypervariate graph. However, the node number of the hypervariate graph increase with the number of series (\(N\)) and the window length (\(T\)), leading to a graph of large order and size. This could make classic graph networks (e.g., GCN , GAT ) computationally expensive (usually with quadratic complexity) and suffer from optimization difficulty in obtaining accurate node representations . To this end, we propose a novel architecture, _Fourier Graph Neural Network_ (FourierGNN), for MTS forecasting from a pure graph perspective. Specifically, FourierGNN is built upon our proposed _Fourier Graph Operator_ (FGO), which as a replacement of classic graph operation units (e.g., convolutions), performs _matrix multiplications_ in _Fourier space_ of graphs. By stacking FGO layers in Fourier space, FourierGNN can accommodate adequate learning expressiveness and in the mean time achieve much lower complexity (Log-linear complexity), which thus can effectively and efficiently accomplish MTS forecasting. Besides, we present theoretical analysis to demonstrate that the FGO is equivalent to graph convolutions in the time domain, which further explains the validity of FourierGNN.

Finally, we perform extensive experiments on seven real-world benchmarks. Experimental results demonstrate that FourierGNN achieves an average of more than 10% improvement in accuracy compared with state-of-the-art methods. In addition, FourierGNN achieves higher forecasting efficiency, which has about 14.6% less costs in training time and 20% less parameter volumes, compared with most lightweight GNN-based forecasting methods.

## 2 Related Work

Graph Neural Networks for Multivariate Time Series ForecastingMultivariate time series (MTS) have embraced GNN due to their best capability of modeling structural dependencies between variates [17; 2; 13; 12; 11; 18; 19]. Most of these models, such as STGCN , DCRNN ,and TAMP-S2GCNets , require a pre-defined graph structure which is usually unknown in most cases. For this limitation, some studies enable to automatically learn the graphs by the inter-series correlations, e.g., by node similarity [20; 2; 17] or self-attention mechanism . However, these methods always adopt a _graph network_ for spatial correlations and a _temporal network_ for temporal dependencies separately [20; 17; 10; 2]. For example, AGCRN  use a GCN  and a GRU , GraphWaveNet  use a GCN and a TCN , etc. In this paper, we propose an unified spatiotemporal formulation with pure graph networks for MTS forecasting.

Multivariate Time Series Forecasting with Fourier TransformRecently, many MTS forecasting models have integrated the Fourier theory into deep neural networks [22; 23]. For instance, SFM  decomposes the hidden state of LSTM into multiple frequencies by Discrete Fourier Transform (DFT). mWDN  decomposes the time series into multilevel sub-series by discrete wavelet decomposition (DWT) and feeds them to LSTM network. ATFN  proposes a Discrete Fourier Transform-based block to capture dynamic and complicated periodic patterns of time series data. FEDformer  proposes Discrete Fourier Transform-based attention mechanism with low-rank approximation in frequency. While these models only capture temporal dependencies with Fourier Transform, StemGNN  takes the advantages of both spatial correlations and temporal dependencies in the spectral domain by utilizing Graph Fourier Transform (GFT) to perform graph convolutions and Discrete Fourier Transform (DFT) to calculate the series relationships.

## 3 Problem Definition

Given the multivariate time series input, i.e., the lookback window \(X_{t}=[_{t-T+1},...,_{t}]^{N T}\) at timestamps \(t\) with the number of series (variates) \(N\) and the lookback window size \(T\), where \(_{t}^{N}\) denotes the multivariate values of \(N\) series at timestamp \(t\). Then, the _multivariate time series forecasting_ task is to predict the next \(\) timestamps \(Y_{t}=[_{t+1},...,_{t+}]^{N}\) based on the historical \(T\) observations \(X_{t}=[_{t-T+1},...,_{t}]\). The forecasting process can be given by:

\[_{t}:=F_{}(X_{t})=F_{}([_{t-T+1},...,_{t}]) \]

where \(_{t}\) are the predictions corresponding to the ground truth \(Y_{t}\). The forecasting function is denoted as \(F_{}\) parameterized by \(\). In practice, many MTS forecasting models usually leverage a _graph network_ (assume parameterized by \(_{g}\)) to learn the spatial dynamics and a _temporal network_ (assume parameterized by \(_{t}\)) to learn the temporal dependencies, respectively [17; 10; 2; 13; 11]. Thus, the original definition of Equation (1) can be rewritten to:

\[_{t}:=F_{_{g},_{t}}(X_{t})=F_{_{g},_{t}}([_{t-T+1},...,_{t}]) \]

where original parameters \(\) are exposed to the parameters of the graph network \(_{g}\) and the temporal network \(_{t}\) to make prediction based on the learned spatial-temporal dependencies.

## 4 Methodology

In this section, we elaborate on our proposed framework: First, we start with our pure graph formulation with a novel hypervariate graph structure for MTS forecasting in Section 4.1. Then, we illustrate the proposed neural architecture, Fourier Graph Neural Network (FourierGNN), for this formulation in Section 4.2. Besides, we theoretically analyze FourierGNN to demonstrate its architecture validity, and also conduct complexity analysis to show its efficiency. Finally, we introduce certain inductive bias to instantiate FourierGNN for MTS forecasting in Section 4.3.

### The Pure Graph Formulation

To overcome the uncertain compatibility of the graph network and the temporal network as aforementioned in Section 1, and learn the united spatiotemporal dynamics, we propose a _pure_ graph formulation that refines Equation (2) by a novel data structure, _hypervariate graph_, for time series.

**Definition 1** (**Hypervariate Graph**).: _Given a multivariate time series window as input \(X_{t}^{N T}\) of \(N\) variates at timestamp \(t\), we construct a hypervariate graph of \(NT\) nodes, \(_{t}=(X_{t}^{},A_{t}^{})\), by regarding each element of \(X_{t}\) as one node of \(_{t}\) such that \(X_{t}^{}^{NT 1}\) stands for the node feature and \(A_{t}^{}^{NT NT}\) is the adjacency matrix initialized to make \(_{t}\) as a fully-connected graph._Since the prior graph structure is usually unknown in most multivariate time series scenarios [10; 2; 13], and the elements of \(X_{t}\) are spatially or temporally correlated with each other because of time lag effect , we assume all nodes in the hypervariate graph \(_{t}\) are fully-connected. The hypervariate graph \(_{t}\) contains \(NT\) nodes representing the values of each variate at each timestamp in \(X_{t}\), which can learn a high-resolution representation across timestamps and variates (more explanations of the hypervariate graph can be seen in Appendix C.1). We present an example hypervariate graph of three time series in Figure 1. Thus, with such a data structure, we can reformulate the _multivariate time series forecasting_ task into the predictions on the hypervariate graphs, and accordingly rewrite Equation (2) into:

\[_{t}:=F_{_{}}(X_{t}^{},A_{t}^{}) \]

where \(_{}\) stands for the network parameters for hypervariate graphs. With such a formulation, we can view the spatial dynamics and the temporal dependencies from an united perspective, which benefits modeling the real-world spatiotemporal inter-dependencies.

### FourierGNN

Though the pure graph formulation can enhance spatiotemporal modeling, the order and size of hypervariate graphs increase with the number of variates \(N\) and the size of window \(T\), which makes classic graph networks (e.g., GCN  and GAT ) computationally expensive (usually quadratic complexity) and suffer from optimization difficulty in obtaining accurate hidden node representations . In this regard, we propose an efficient and effective method, FourierGNN, for the pure graph formulation. The main architecture of FourierGNN is built upon our proposed Fourier Graph Operator (FGO), a learnable network layer in Fourier space, which is detailed as follows.

**Definition 2** (Fourier Graph Operator).: _Given a graph \(G=(X,A)\) with node features \(X^{n d}\) and the adjacency matrix \(A^{n n}\), where \(n\) is the number of nodes and \(d\) is the number of features, we introduce a weight matrix \(W^{d d}\) to acquire a tailored Green's kernel \(:[n][n]^{d d}\) with \([i,j]:=A_{ij} W\) and \([i,j]=[i-j]\). We define \(_{A,W}:=()^{n d d}\) as a Fourier Graph Operator (FGO), where \(\) denotes Discrete Fourier Transform (DFT)._

According to the convolution theorem  (see Appendix B), we can write the multiplication between \((X)\) and FGO \(_{A,W}\) in Fourier space as:

\[(X)()=((X)[i])=( _{j=1}^{n}X[j][i-j])=(_{j=1}^{n}X[j][i,j]),  i[n] \]

where \((X)[i]\) denotes the convolution of \(X\) and \(\). As defined \([i,j]=A_{ij} W\), it yields \(_{j=1}^{n}X[j][i,j]=_{j=1}^{n}A_{ij}X[j]W=AXW\). Accordingly, we can get the convolution equation:

\[(X)_{A,W}=(AXW). \]

In particular, turning to our case of the fully-connected hypervariate graph, we can adopt a \(n\)-invariant FGO \(^{d d}\) that has a computationally low cost compared to previous \(^{n d d}\). We provide more details and explanations in Appendix C.2.

From Equation (5), we can observe that performing the multiplication between \((X)\) and FGO \(\) in Fourier space corresponds to a graph shift operation (i.e., a graph convolution) in the time domain . Since the multiplications in Fourier space (\((n)\)) have much lower complexity than the above shift operations (\((n^{2})\)) in the time domain (See _Complexity Analysis_ below), it motivates us to develop a highly efficient graph neural network in Fourier space.

To this end, we propose the _Fourier Graph Neural Networks_ (FourierGNN) based on FGO. Specifically, by stacking multiple layers of FGOs, we can define the \(K\)-layer Fourier graph neural networks given a graph \(G=(X,A)\) with node features \(X^{n d}\) and the adjacency matrix \(A^{n n}\) as:

\[(X,A):=_{k=0}^{K}((X)_{0:k }+b_{k}),_{0:k}=_{i=0}^{k}_{i}. \]

Figure 1: Illustration of a hypervariate graph with three time series. Each value in the input window is considered as a node of the graph.

Herein, \(_{k}\) is the FGO in the \(k\)-th layer, satisfying \((X)_{k}=(A_{k}XW_{k})\) with \(W_{k}^{d d}\) being the weights and \(A_{k}^{n n}\) corresponding to the \(k\)-th adjacency matrix sharing the same sparsity pattern of \(A\), and \(b_{k}^{d}\) are the complex-valued biases parameters; \(\) stands for Discrete Fourier Transform; \(\) is the activation function. In particular, \(_{0}\), \(W_{0}\), \(A_{0}\) are the identity matrix, and we adopt identical activation at \(k=0\) to obtain residual \((X)\). All operations in FourierGNN are performed in Fourier space. Thus, all parameters, i.e., \(\{_{k},b_{k}\}_{k=1}^{K}\), are complex numbers.

The core operation of FourierGNN is the summation of recursive multiplications with nonlinear activation functions. Specifically, the recursive multiplications between \((X)\) and \(\), i.e., \((X)_{0:k}\), are equivalent to the multi-order convolutions on the graph structure (see _Theoretical Analysis_ below). Nonlinear activation functions \(\) are introduced to address the capability limitations of modeling nonlinear information diffusion on graphs in the summation.

Theoretical AnalysisWe theoretically analyze the effectiveness and interpretability of FourierGNN and verify the validity of its architecture. For convenience, we exclude the non-linear activation function \(\) and learnable bias parameters \(b\) from Equation (6), and focus on \((X)_{0:k}\).

**Proposition 1**.: _Given a graph \(G=(X,A)\) with node features \(X^{n d}\) and adjacency matrix \(A^{n n}\), the recursive multiplication of FGOs in Fourier space is equivalent to multi-order convolutions in the time domain:_

\[^{-1}((X)_{0:k})=A_{k:0}XW_{0:k},_{0:k}=_{i=0}^{k}_{i},A_{k:0}=_{i=k}^{0}A_{i},W_{0:k}= _{i=0}^{k}W_{i} \]

_where \(A_{0},_{0},W_{0}\) are the identity matrix, \(A_{k}^{n n}\) corresponds to the \(k\)-th diffusion step sharing the same sparsity pattern of \(A\), \(W_{k}^{d d}\) is the \(k\)-th weight matrix, \(_{k}^{d d}\) is the \(k\)-th FGO satisfying \((A_{k}XW_{k})=(X)_{k}\), and \(\) and \(^{-1}\) denote DFT and its inverse, respectively._

In the time domain, operation \(A_{k:0}XW_{0:k}\) adopts different weights \(W_{k}^{d d}\) to weigh the information of different neighbors in different diffusion orders, beneficial to capture the extensive dependencies on graphs [20; 30; 31]. This indicates FourierGNN is expressive in modeling the complex correlations among graph nodes, i.e., spatiotemporal dependencies in the hypervariate graph. The proof of Proposition 1 and more explanations of FourierGNN are provided in Appendix C.3.

Complexity AnalysisThe time complexity of \((X)\) is \((nd n+nd^{2})\), which includes the Discrete Fourier Transform (DFT), the Inverse Discrete Fourier Transform (IDFT), and the matrix multiplication in the Fourier space. Comparatively, the time complexity of the equivalent operations of \((X)\) in the time domain, i.e., \(AXW\), is \((n^{2}d+nd^{2})\). Then, as a \(K\)-order summation of a recursive multiplication of \((X)\), FourierGNN, achieves the time complexity of \((nd n+Knd^{2})\), including DFT and IDFT, and the recursive multiplication of FGOs. Overall, the Log-linear \((n n)\) complexity makes FourierGNN much more efficient.

FourierGNN vs Other Graph NetworksWe analyze the connection and difference between our FourierGNN with GCN  and GAT . From the complexity perspective, FourierGNN with log-linear complexity shows much higher efficiency than GCN and GAT. Regarding the network architecture, we analyze them from two main perspectives: (1) Domain. GAT implements operations in the time domain, while GCN and FourierGNN are in Fourier space. However, GCN achieves the transformation through the Graph Fourier Transform (GFT), whereas FourierGNN utilizes the Discrete Fourier Transform (DFT). (2) Information diffusion: GAT aggregates neighbor nodes with varying weights to via attention mechanisms. FourierGNN and GCN update node information via convoluting neighbor nodes. Different from GCN, FourierGNN assigns varying importance to neighbor nodes in different diffusion steps. We provide a detailed comparison in Appendix D.

### Multivariate Time Series Forecasting with FourierGNN

In this section, we instantiate FourierGNN for MTS forecasting. The overall architecture of our model is illustrated in Figure 2. Given the MTS input data \(X_{t}^{N T}\), first we construct a fully-connected _hypervariate graph_\(_{t}=(X_{t}^{},A_{t}^{})\) with \(X_{t}^{}^{NT 1}\) and \(A_{t}^{}\{1\}^{n n}\). Then, we project \(X_{t}^{}\) into node embeddings \(_{t}^{}^{NT d}\) by assigning a \(d\)-dimension vector for each node using an embedding matrix \(E_{}^{1 d}\), i.e., \(_{t}^{}=X_{t}^{} E_{}\).

Subsequently, to capture the spatiotemporal dependencies simultaneously, we aim to feed multiple embeded hypervariate graphs with \(_{t}^{}\) to FourierGNN. First, we perform Discrete Fourier Transform (DFT) \(\) on each discrete spatio-temporal dimension of the embeddings \(_{t}^{}\) and obtain the frequency output \(_{t}^{}:=(_{t}^{}) ^{NT d}\). Then, we perform a recursive multiplication between \(_{t}^{}\) and FGOs \(_{0:k}\) in Fourier space and output the resulting representations \(_{t}^{}\) as:

\[_{t}^{}=(_{t}^{},_{t}^{})=_{k=0}^{K}((_{t }^{})_{0:k}+b_{k}),_{0:k}=_{i=0}^{ k}_{i}. \]

Then \(_{t}^{}\) are transformed back to the time domain using Inverse Discrete Fourier Transform (IDFT) \(^{-1}\), which yields \(_{t}^{}:=^{-1}(_{t}^{}) ^{NT d}\).

Finally, according to the FourierGNN output \(_{t}^{}\) which encodes spatiotemporal inter-dependencies, we use two layer feed-forward networks (FFN) (see Appendix E.4 for more details) to project it onto \(\) future steps, resulting in \(_{t}=(_{t}^{})^{N}\).

## 5 Experiments

To evaluate the performance of FourierGNN, we conduct extensive experiments on seven real-world time series benchmarks to compare with state-of-the-art graph neural network-based methods.

### Experimental Setup

**Datasets**. We evaluate our proposed method on seven representative datasets from various application scenarios, including traffic, energy, web traffic, electrocardiogram, and COVID-19. All datasets are normalized using the min-max normalization. Except the COVID-19 dataset, we split the other datasets into training, validation, and test sets with the ratio of 7:2:1 in a chronological order. For the COVID-19 dataset, the ratio is 6:2:2. More detailed information about datasets are in Appendix E.1.

**Baselines**. We conduct a comprehensive comparison of the forecasting performance between our FourierGNN and several representative and state-of-the-art (SOTA) models on the seven datasets, including classic method VAR , deep learning-based models such as SFM , LSTNet , TCN , DeepGLO , and CoST . We also compare FourierGNN against GNN-based models like GraphWaveNet , StemGNN , MTGNN , and AGCRN , and two representative Transformer-based models like Reformer  and Informer , as well as two frequency enhanced Transformer-based models including Autoformer  and FEDformer . In addition, we compare FourierGNN with SOTA models such as TAMP-S2GCNets , DCRNN , and STGCN , which require pre-defined graph structures. Please refer to Appendix E.2 for more implementation details of the adopted baselines.

**Experimental Settings**. All experiments are conducted in Python using Pytorch 1.8  (except for SFM  which uses Keras) and performed on single NVIDIA RTX 3080 10G GPU. Our model is trained using RMSProp with a learning rate of \(10^{-5}\) and MSE (Mean Squared Error) as the loss function. The best parameters for all comparative models are chosen through careful parameter tuning on the validation set. We use Mean Absolute Errors (MAE), Root Mean Squared Errors (RMSE), and Mean Absolute Percentage Error (MAPE) to measure the performance. The evaluation details are in Appendix E.3 and more experimental settings are in Appendix E.4.

### Main Results

We present the evaluation results with an input length of 12 and a prediction length of 12 in Table 1. Overall, FourierGNN achieves a new state-of-the-art on all datasets. On average, FourierGNN makes an improvement of 9.4% in MAE and 10.9% in RMSE compared to the best-performing across all datasets. Among these baselines, Reformer, Informer, Autoformer, and FEDformer are Transformer-based models that demonstrate competitive performance on Electricity and COVID-19 datasets, as they excel at capturing temporal dependencies. However, they have limitations in capturing the spatial dependencies explicitly. GraphWaveNet, MTGNN, StemGNN, and AGCRN are GNN-based models that show promising results on Wiki, Traffic, Solar, and ECG datasets, primarily due to their capability to handle spatial dependencies among variates. However, they are limited in their capacity to simultaneously capture spatiotemporal dependencies. FourierGNN outperforms the baseline models since it can learn comprehensive spatiotemporal dependencies simultaneously and attends to time-varying dependencies among variates.

Multi-Step ForecastingTo further evaluate the performance in multi-step forecasting, we compare FourierGNN with other GNN-based MTS models (including StemGNN , AGCRN ,

    &  &  &  \\  & MAE & RMSE & MAPE(\%) & MAE & RMSE & MAPE(\%) & MAE & RMSE & MAPE(\%) \\  VAR  & 0.184 & 0.234 & 577.10 & 0.057 & 0.094 & 96.58 & 0.535 & 1.133 & 550.12 \\ SFM  & 0.161 & 0.283 & 362.89 & 0.081 & 0.156 & 104.47 & 0.029 & 0.044 & 59.33 \\ LSTNet  & 0.148 & 0.200 & 132.95 & 0.054 & 0.090 & 118.24 & 0.026 & 0.057 & **25.77** \\ TCN  & 0.176 & 0.222 & 142.23 & 0.094 & 0.142 & 99.66 & 0.052 & 0.067 & - \\ DeepGLO  & 0.178 & 0.400 & 346.78 & 0.110 & 0.113 & 119.60 & 0.025 & 0.037 & 33.32 \\ Reformer  & 0.234 & 0.292 & 128.58 & 0.048 & 0.085 & 73.61 & 0.029 & 0.042 & 112.58 \\ Informer  & 0.151 & 0.199 & 128.45 & 0.051 & 0.086 & 80.50 & 0.020 & 0.033 & 59.34 \\ Autoformer  & 0.150 & 0.193 & 103.79 & 0.069 & 0.103 & 121.90 & 0.029 & 0.043 & 100.02 \\ FEDformer  & 0.139 & 0.182 & **100.92** & 0.068 & 0.098 & 123.10 & 0.025 & 0.038 & 85.12 \\ GraphWaveNet  & 0.183 & 0.238 & 603 & 0.061 & 0.105 & 136.12 & 0.013 & 0.034 & 33.78 \\ StemGNN  & 0.176 & 0.222 & 128.39 & 0.190 & 0.255 & 117.92 & 0.080 & 0.135 & 64.51 \\ MTGNN  & 0.151 & 0.207 & 507.91 & 0.101 & 0.140 & 122.96 & 0.013 & 0.030 & 29.53 \\ AGCRN  & 0.123 & 0.214 & 353.03 & 0.044 & 0.079 & 78.52 & 0.084 & 0.166 & 31.73 \\ 
**FourierGNN** & **0.120** & **0.162** & 116.48 & **0.041** & **0.076** & **64.50** & **0.011** & **0.023** & 28.71 \\   &  &  &  \\ ModelsDatasets & MAE & RMSE & MAPE(\%) & MAE & RMSE & MAPE(\%) & MAE & RMSE & MAPE(\%) \\  VAR  & 0.120 & 0.170 & 22.56 & 0.101 & 0.163 & 43.11 & 0.226 & 0.326 & 191.95 \\ SFM  & 0.095 & 0.135 & 24.20 & 0.086 & 0.129 & 33.71 & 0.205 & 0.308 & 76.08 \\ LSTNet  & 0.079 & 0.115 & 18.68 & 0.075 & 0.138 & 29.95 & 0.248 & 0.305 & 89.04 \\ TCN  & 0.078 & 0.107 & 17.59 & 0.057 & 0.083 & 26.64 & 0.317 & 0.354 & 151.78 \\ DeepGLO  & 0.110 & 0.163 & 43.90 & 0.090 & 0.131 & 29.40 & 0.169 & 0.253 & 75.19 \\ Reformer  & 0.062 & 0.090 & 13.58 & 0.078 & 0.129 & 33.37 & 0.152 & 0.209 & 132.78 \\ Informer  & 0.056 & 0.085 & 11.99 & 0.070 & 0.119 & 32.66 & 0.200 & 0.259 & 155.55 \\ Autoformer  & 0.055 & 0.081 & 11.37 & 0.056 & 0.083 & 25.94 & 0.159 & 0.211 & 136.24 \\ FEDformer  & 0.055 & 0.080 & 11.16 & 0.055 & 0.081 & 25.84 & 0.160 & 0.219 & 134.45 \\ GraphWaveNet  & 0.093 & 0.142 & 40.19 & 0.094 & 0.140 & 37.01 & 0.201 & 0.255 & 100.83 \\ StemGNN  & 0.100 & 0.130 & 29.62 & 0.070 & 0.101 & - & 0.421 & 0.508 & 141.01 \\ MTGNN  & 0.090 & 0.139 & 35.04 & 0.077 & 0.113 & 29.77 & 0.394 & 0.488 & 88.13 \\ AGCRN  & 0.055 & 0.080 & 11.75 & 0.074 & 0.116 & 26.08 & 0.254 & 0.309 & 83.37 \\ 
**FourierGNN** & **0.052** & **0.078** & **10.97** & **0.051** & **0.077** & **24.28** & **0.123** & **0.168** & **71.52** \\   

Table 1: Overall performance of forecasting models on the six datasets.

   Length &  &  &  &  \\ Metrics & MAE & RMSE & MAPE(\%) & MAE & RMSE & MAPE(\%) & MAE & RMSE & MAPE(\%) & MAE & RMSE & MAPE(\%) \\  GraphWaveNet  & 0.092 & 0.129 & 53.00 & 0.133 & 0.179 & 65.11 & 0.171 & 0.225 & 80.91 & 0.201 & 0.255 & 100.83 \\ StemGNN  & 0.247 & 0.318 & 99.98 & 0.344 & 0.429 & 125.81 & 0.359 & 0.442 & 131.41 & 0.421 & 0.508 & 141.01 \ , MTGNN , and TAMP-S2GCNets ) and a representation learning model (CoST ) on COVID-19 dataset under different prediction lengths, and the results are shown in Table 2. It shows that FourierGNN achieves an average 30.1% and 30.2% improvement on MAE and RMSE respectively over the best baseline. In Appendix F, we include more experiments and analysis under different prediction lengths, and further compare FourierGNN with models that require pre-defined graph structures.

### Model Analysis

Efficiency AnalysisWe investigate the parameter volumes and training time costs of FourierGNN, StemGNN , AGCRN , GraphWaveNet , and MTGNN  on two representative datasets, including the Wiki dataset and the Traffic dataset. The results are reported in Table 3, showing the comparison of parameter volumes and average time costs over five rounds of experiments. In terms of parameters, FourierGNN exhibits the lowest volume of parameters among the comparative models. Specifically, it achieves a reduction of 32.2% and 9.5% in parameters compared to GraphWaveNet on Traffic and Wiki datasets, respectively. This reduction is mainly attributed that FourierGNN has shared scale-free parameters for each node. Regarding training time, FourierGNN runs much faster than all baseline models, and it demonstrates efficiency improvements of 5.8% and 23.3% over the fast baseline GraphWaveNet on Traffic and Wiki datasets, respectively. Considering variate number of Wiki dataset is about twice larger than that of Traffic dataset, FourierGNN exhibits larger efficiency superiority with the baselines. These findings highlight the high efficiency of FourierGNN in computing graph operations and its scalability to large datasets with extensive graphs, which is important for the pure graph formulation due to the larger size of hypervariate graphs with \(NT\) nodes.

Ablation StudyWe perform an ablation study on the METR-LA dataset to assess the individual contributions of different components in FourierGNN. The results, presented in Table 4, validate the effectiveness of each component. Specifically, **w/o Embedding** emphasizes the significance of performing node embedding to improve model generalization. **w/o Dynamic FGO** using the same FGO verifies the effectiveness of applying different FGOs in capturing time-varying dependencies. In addition, **w/o Residual** represents FourierGNN without the \(K=0\) layer, while **w/o Summation** adopts the last order (layer) output, i.e., \(_{0:K}\), as the output of FourierGNN. These results demonstrate the importance of high-order diffusion and the contribution of multi-order diffusion. More results and analysis of the ablation study are provided in Appendix G.3.

### Visualization

To gain a better understanding of the hypervariate graph and FourierGNN in spatiotemporal modeling for MTS forecasting, We conduct visualization experiments on the METR-LA and COVID-19 datasets. Please refer to Appendix E.5 for more information on the visualization techniques used.

Visualization of temporal representations learned by FourierGNNIn order to showcase the temporal dependencies learning capability of FourierGNN, we visualize the temporal adjacency matrix of different variates. Specifically, we randomly select \(8\) counties from the COVID-19 dataset and calculate the relations of \(12\) consecutive time steps for each county. Then, we visualize the

   metrics & w/o Embedding & w/o Dynamic FGO & w/o Residual & w/o Summation & FourierGNN \\  MAE & 0.053 & 0.055 & 0.054 & 0.054 & 0.050 \\ RMSE & 0.116 & 0.114 & 0.115 & 0.114 & 0.113 \\ MAPE(\%) & 86.73 & 86.69 & 86.75 & 86.62 & 86.30 \\   

Table 4: Ablation study on METR-LA dataset.

    & & Traffic & & Wiki \\ Models & Parameters & Training (s/epoch) & Parameters & Training (s/epoch) \\  StemGNN & \(1,606,140\) & 185.86\(\)2.22 & \(4,102,406\) & 92.95\(\)1.39 \\ MTGNN & \(707,516\) & 169.34\(\)1.56 & \(1,533,436\) & 28.69\(\)0.83 \\ AGCRN & \(749,940\) & 113.46\(\)1.91 & \(755,740\) & 22.48\(\)1.01 \\ GraphWaveNet & \(280,860\) & 105.38\(\)1.24 & \(292,460\) & 21.23\(\)0.76 \\  FourierGNN & \(190,564\) & 99.25\(\)1.07 & \(264,804\) & 16.28\(\)0.48 \\   

Table 3: Comparisons of parameter volumes and training time costs on datasets Traffic and Wiki.

adjacency matrix by a heatmap, and the results are illustrated in Figure 3, where \(N\) denotes the index of the country (variate). It shows that FourierGNN learns distinct temporal patterns for each county, indicating that the hypervariate graph can encode rich and discriminative temporal dependencies.

Visualization of spatial representations learned by FourierGNNTo investigate the spatial correlations learning capability of FourierGNN, we visualize the generated adjacency matrix based on the representations learned by FourierGNN on the METR-LA dataset. Specifically, we randomly select 20 detectors and visualize their corresponding adjacency matrix via a heatmap, as depicted in Figure 4. By examining the adjacency matrix in conjunction with the actual road map, we observe: 1) the detectors (7, 8, 9, 11, 13, 18) are very close w.r.t. the physical distance, corresponding to the high values of their correlations with each other in the heatmap; 2) the detectors 4, 14 and 16 have small overall correlation values since they are far from other detectors; 3) however, compared with detectors 14 and 16, the detector 4 has slightly higher correlation values to other detectors, e.g., 7, 8, 9, which is because although they are far apart, the detectors 4, 7, 8, 9 are on the same road. The results verify that the hypervariate graph structure can represent highly interpretative correlations.

Moreover, to gain a understanding of how FGO works, we visualize the output of each layer of FourierGNN, and the visualization results demonstrate that FGO can adaptively and effectively capture important patterns while removing noises to a learn discriminative model. Further details can be found in Appendix H.1. Additionally, to investigate the ability of FourierGNN to capture time-varying dependencies among variates, we further visualize the spatial correlations at different timestamps. The results illustrate that FourierGNN can effectively attend to the temporal variability in the data. For more information, please refer to Appendix H.2.

## 6 Conclusion Remarks

In this paper, we explore an interesting direction of directly applying graph networks for MTS forecasting from a pure graph perspective. To overcome the previous separate spatial and temporal modeling problem, we build a hypervariate graph, regarding each series value as a graph node, which considers spatiotemporal dynamics unitedly. Then, we formulate time series forecasting on the hypervariate graph and propose FourierGNN by stacking Fourier Graph Operator (FGO) to perform matrix multiplications in Fourier space, which can accommodate adequate learning expressiveness with much lower complexity. Extensive experiments demonstrate that FourierGNN achieves state-of-the-art performances with higher efficiency and fewer parameters, and the hypervariate graph structure exhibits strong capabilities to encode spatiotemporal inter-dependencies.

Figure 4: The adjacency matrix (right) learned by FourierGNN and the corresponding road map (left).

Figure 3: The temporal adjacency matrix of eight variates on COVID-19 dataset.