# Transition-constant Normalization for Image Enhancement

Jie Huang\({}^{1}\), Man Zhou\({}^{1,2}\), Jinghao Zhang\({}^{1}\), Gang Yang\({}^{1}\), Mingde Yao\({}^{1}\),

**Chongyi Li\({}^{3}\)**, **Zhiwei Xiong\({}^{1}\)**, **Feng Zhao\({}^{1}\)\({}^{1}\)**

\({}^{1}\)University of Science and Technology of China, China

\({}^{2}\)Nanyang Technological University, Singapore

\({}^{3}\)Nankai University, China

{hj0117, manman, jhaozhang, mdyao, yg1997}@mail.ustc.edu.cn,

lichongyi25@gmail.com, {zwxiong, fzhao956}@ustc.edu.cn,

Both authors contributed equally to this research.Corresponding author.

###### Abstract

Normalization techniques that capture image style by statistical representation have become a popular component in deep neural networks. Although image enhancement can be considered as a form of style transformation, there has been little exploration of how normalization affect the enhancement performance. To fully leverage the potential of normalization, we present a novel Transition-Constant Normalization (TCN) for various image enhancement tasks. Specifically, it consists of two streams of normalization operations arranged under an invertible constraint, along with a feature sub-sampling operation that satisfies the normalization constraint. TCN enjoys several merits, including being parameter-free, plug-and-play, and incurring no additional computational costs. We provide various formats to utilize TCN for image enhancement, including seamless integration with enhancement networks, incorporation into encoder-decoder architectures for downsampling, and implementation of efficient architectures. Through extensive experiments on multiple image enhancement tasks, like low-light enhancement, exposure correction, SDR2HDR translation, and image dehazing, our TCN consistently demonstrates performance improvements. Besides, it showcases extensive ability in other tasks including pan-sharpening and medical segmentation. The code is available at _[https://github.com/huangkevinj/TCNorm_](https://github.com/huangkevinj/TCNorm_).

## 1 Introduction

Image enhancement is an important task in machine vision, which aims to improve the quality of low-visibility images captured under unfavorable light conditions (i.e., low light) by adjusting contrast and lightness. The last decades have witnessed quantities of approaches designed for image enhancement based on various hand-crafted priors . However, the complex and variant adjustment procedures make it a challenging group of tasks. In addition to the common low-light image enhancement, efforts have also been directed toward solving image enhancement-like tasks, including exposure correction, image dehazing, and SDR2HDR translation.

Very recently, the deep-learning paradigm exhibits remarkable success in the image enhancement field than traditional methods . Despite the progress, most of them focus on roughly constructing complex deep neural architectures and have not fully explored the intrinsic characterizes of lightness in networks. In fact, the lightness variants could bring difficulties to their learning procedures. Thismotivates us to delve into the working mechanism of current neural networks that learn lightness adjustment and prescribe the right medicine customized for image enhancement.

On the other hand, normalization family such as batch and instance normalization, is specially designed for promoting the learning procedure for deep networks. It involves computing statistical representation and normalizing the corresponding distribution, which has been shown to capture image style through statistical representation . Image enhancement, which aims to restore lightness-corrupted images to their normal versions, can be viewed as a style transformation inherently linked to normalization techniques (see Fig. 1 (a)). However, existing methods have rarely explored the potential of the normalization technique. Inspired by the inborn connection, we thus focus on developing the normalization technique tailored for image enhancement.

In this work, we propose a novel operation called transition-constant normalization (TCN) for image enhancement tasks. The TCN operation aims to normalize partial representations to ensure consistent learning while preserving constant information for image reconstruction. As illustrated in Fig. 1 (b), we construct the TCN within an invertible format to enable seamless information transmission to subsequent layers for reconstructing enhanced results. The TCN is designed with two key rules (see Fig. 2): (1) We organize the operations in the normalization layer into two streams, following the principle of invertible information transmission, thereby maintaining constant information transition. (2) We incorporate a subsampling operation that divides the features into two streams with consistent statistical properties. One stream provides the statistics for normalizing the other stream, satisfying the normalization requirement. Notably, the TCN requires no parameters, making it a convenient and orthogonal addition to existing enhancement architectures for improving their performance.

To facilitate its application, we present multiple usage formats for the TCN in image enhancement: (1) Integration into existing enhancement networks, allowing for seamless incorporation and performance improvement; (2) Plug-in capability in encoder-decoder architectures for downsampling and recomposition of information; (3) Construction of a lightweight architecture based on TCN, striking a balance between performance and computational cost. Through extensive experiments across various image enhancement tasks, we consistently observe performance gains by integrating our TCN.

The contributions of this work are summarized as follows: 1) We present a novel perspective on image enhancement using a dedicated normalization technique. This technique enhances the learning of lightness adjustments by modeling consistent normalized features, while ensuring their complementarity for reconstructing the results. 2) We construct the Transition-constant Normalization (TCN) by organizing normalization operations to satisfy the invertible mechanism, ensuring constant feature normalization and information transition. 3) Our proposed TCN is compatible with existing enhancement architectures, allowing for convenient integration and performance improvement. Furthermore, we can derive multiple implementation formats for TCN and explore its applicability in various tasks, highlighting its potential for wide-ranging applications.

## 2 Related Work

**Image enhancement tasks.** Image enhancement tasks aim to improve the quality of low-visibility images by adjusting lightness and contrast components (e.g., illumination, color, and dynamic range).

Figure 1: In (a), the instance normalization (IN) captures lightness consistency representations across exposures, thus bridging their distribution gaps as shown in t-SNE. In (b), normalization techniques meet transition-inconstant problem, while proposed TCN exhibits differently with invertible ability.

Recent years have witnessed rapid development in the related areas [10; 11; 12; 13]. For low-light image enhancement, algorithms are designed to enhance the visibility of images captured under low-light conditions [14; 15; 16; 17; 18; 19; 7; 20; 21; 22; 23]. In the exposure correction task, methods are focused on correcting both underexposure and overexposure to normal exposure [15; 24; 25; 26; 27]. For SDR2HDR translation, this task aims to design methods to convert images from a low-dynamic range to a high-dynamic range [28; 29; 30; 31; 32]. While in image dehazing, this task requires methods to enhance the contrast and recover the color shift problems [33; 34; 35; 36; 37]. To this end, image enhancement tasks cover variant scenes and remain challenges to be solved.

**Normalization techniques.** Normalization techniques have been studied for a long time [38; 39; 40; 41]. Batch Normalization (BN)  normalizes the features along the batch dimension that stabilizes the optimization procedure. Instance Normalization (IN)  focuses on normalizing the instance-level statistics of features, which has been widely employed in style transfer tasks [42; 43]. Some other variants of normalization, including Layer Normalization (LN) , Group Normalization (GN) , and Position Normalization (PN)  have been proposed for facilitating the application of networks.

## 3 Method

In this section, we first briefly revisit the normalization techniques and then detail the design and mechanism of the proposed TCN. Finally, we present the variants of the TCN as implementation.

### Preliminaries

Given a batch of features \(x^{ N C H W}\), where \( N\), \( C\), \( H\) and \( W\) represent batch size, channel numbers, the spatial height and width, respectively. Let \(x_{ncij}\) and \(_{ncij}\) denote a pixel before and after normalization, where \( n[1,N],c[1,C],i[1,H],i[1,W]\). Without taking considering into affined parameters, we can express normalization operation as:

\[_{ncij}=(x_{ncij})=-_{k}}{^{2}+}}, \]

where \(_{k}\) and \(_{k}\) denote the feature mean and standard deviation, \(\) is a small constant to preserve numerical stability. \(k\{\}\) is to distinguish different normalization formats. Within the above normalization family, the calculation of \(_{k}\) and \(_{k}\) is different and are expressed as:

\[_{k}=|}_{n,c,i,j I_{k}}x_{ncij},\;\;\;_{k}= |}_{n,c,i,j I_{k}}(x_{ncij}-_{k})^{2}}, \]

1. **IN**: \(I_{k}:I_{IN}=\{(i,j)|i[1,],j[1,]\}\);
2. **BN**: \(I_{k}:I_{BN}=\{(n,i,j)|n[1,],i[1,],j[1,]\}\);
3. **LN**: \(I_{k}:I_{LN}=\{(c,i,j)|c[1,],i[1,],j[1,]\}\);
4. **GN**: \(I_{k}:I_{GN}=\{(c,i,j)|c[,+/],i[1,],j[1,]|[1,]\}\).

where \(I_{k}\) is a set of pixels, \(|I_{k}|\) denotes the number of pixels and \(G\) is the group division.

Within deep neural networks, Eq. 1 is often affined with scaling and shifting parameters \(\) and \(\):

\[_{ncij}=(x_{ncij})+=- _{k}}{^{2}+}}+. \]

It is well-known that lightness can be considered as a kind of style and Instance normalization can facilitate consistent style information  and image enhancement network optimization due to bridging the gap of different lightness representations.

**Verifying normalization effect for lightness**. Given an image \(x_{a}\) and its lightness-adjusted version \(x_{b}\), the relationship between \(x_{a}\) and \(x_{b}\) can be expressed in correction procedure [46; 47; 48] as:

\[x_{b}= x_{a}^{}, \]

where \(\) is a linear transformation and \(\) is for global non-linear adaption and is close to 1 when content is not severely changed. Therefore, the p-norm distance between their normalized versions is:

\[||(x_{a})-(x_{b})||_{p}=|-_{a}}{_{a}}- ^{}-_{b}}{_{b}}||_{p}|- _{a}}{_{a}}--_{a}}{_{a}}||_{p }\|x_{a}- x_{a}^{}\|_{p}. \]Therefore, normalization reduces the distance of different lightness, which is also validated in Fig. 1(a). However, normalizing the statistics itself often blocks the information flow, which hinders the network from reconstructing the final results  and we describe it as follows.

**Transition-inconstant of Normalization.** Referring to \(()\) in the Eq. 1 as \(f\), its Jacobian matrix is expressed as:

\[J_{f}(x)=}}{x_{0}}&&}}{x_{0}}\\ &&\\ }}{x_{0}}&}}{x_{0}} _{m m}=}&&}\\ &&\\ }&&}_{m m} \]

Therefore, the calculation of the above Jacobian matrix is **det** (\(J_{f}(x)\)) = 0, denoting the normalization operation is not invertible and resulting in transition-inconstant. Meanwhile, in practice, previous works have demonstrated that IN would lead to severe information loss  and huge representation ability changes [50; 51], while LN and BN can keep almost all of the original information representation ability. However, IN is more suitable than BN and LN for image enhancement tasks due to its strong capability of capturing and affecting style information, which is crucial for image enhancement. To this end, the main goal of this paper is to introduce a new mechanism that enables the IN can keep the information representation ability for image enhancement.

### Transition-constant Normalization (TCN)

Based on the above analysis, we aim to refresh the normalization technique to enable it to transmit information constantly while normalizing the features. To this end, we introduce the TCN as shown in Fig. 2, which is free of parameters and is convenient to implement. Since IN can normalize different lightness effectively and thus is useful for image enhancement, we design the TCN based on the IN as its default implementation format in this paper.

**Operation description.** We construct the TCN by applying the normalization operations with a two streams flow design with subsampled features, where one stream provides the statistical information for normalizing another stream. Specifically, the feature \(F^{}\) is firstly subsampled to \(F_{s}^{ }{2}}{2}}\) according to the unshuffle operation  as shown in Fig. 2 (a), which is:

\[F_{s}^{ab}=F[:,:,a::,b::], a,b\{0,1\}, \]

where \(a\) and \(b\) denote the subsampling index. We divide \(F_{s}\) into two features \(F_{1}\) and \(F_{2}\) with two groups according to the sampling index \((i,j)\) as:

\[F_{1}=(F_{s}^{01},F_{s}^{10}),F_{2}=(F_{s}^{00}, F_{s}^{11}) \]

where \((,)\) denotes the concatenate operation along the channel dimension.

Then, we calculate the mean \(_{2}\) and standard deviation \(_{2}\) of one stream feature \(F_{2}\) in IN format, which are derived by setting \(I_{k}\) in Eq. 2 as \(I_{IN}\):

\[_{2}=}_{i[1,],[1,]}F_{ij},_{2}=}_{i[1,], [1,]}(F_{ij}-_{2})^{2}}. \]

These statistics are utilized to normalize the feature \(F_{1}\) as the output in this stream:

\[_{1}=-_{2}}{^{2}+}}. \]

Next, we subtract the feature \(F_{2}\) and \(_{1}\) and obtain the output of another stream:

\[_{2}=F_{2}-_{1}=F_{2}--_{2}}{^{2}+ }}. \]

Finally, the two stream features are sampled with pixel shuffle operation to the original resolution with the shape of \([,}{2},,]\). They are further concatenated in the channel dimension as \(^{}\), which is the output of the TCN. This procedure is expressed as:

\[=((_{1},_{2})), \]where \(()\) denotes the pixel shuffle operation . We verify the above procedures satisfy the transition-constant and normalization ability as below, respectively.

**1) Verify the transition-constant ability.** We validate the above two-stream design satisfies the invertible procedure and thus is transition-constant. To this end, Eq. 10 and Eq. 11 are re-written as:

\[_{1}&=(F_{1}-M(F_{2})) S( F_{2}),\\ _{2}&=F_{2}-_{1}, \]

where \(S()\) and \(M()\) denote standard deviation and mean functions, \(\) denotes element division.

Inspired by the proof in RealNVP's  transformation, we need to calculate the Jacobian matrix of Eq. 13 (denote it as \(g\)), which is more intuitively written as:

\[_{1}&=(F_{1}-M(F_{2})) S( F_{2}),\\ _{2}&=F_{2}-F_{1} S(F_{2})+M(F_{2})  S(F_{2}), \]

We derive its Jacobian matrix (detailed in the supplementary) as:

\[J_{g}=_{1}}{ F_{1}}&_{1}}{ F_{2}}\\ _{2}}{ F_{1}}&_{2}}{ F _{2}}=)}&0\\ -1&1 \]

Here, the above Jacobian matrix is further calculated as:

\[(J_{g})=)} 0 \]

Upon the \((J_{g}) 0\), it indicates that \(J_{g}\) is full rank, verifying the invertible property of TCN and further the transition-constant ability. To highlight, the TCN is an invertible function and would not block information flow, leading to the information transition constant for image reconstruction. We further present the relation of the TCN and the invertible operation more directly in the supplementary.

**2) Verify the normalization ability.** The normalization ability of the TCN is guaranteed by the pixel unshuffle operation in Eq. 7, leading to the same statistics of \(F_{s}^{ab}\). Therefore, we have \(_{2}_{1},_{2}_{1}\), and the Eq. 10 is thus converted to:

\[_{1}=-_{2}}{^{2}+}}-_{1}}{^{2}+}}. \]

Therefore, it has the same format as Eq. 1, demonstrating that the operation in Eq. 10 has the normalization ability as the IN. Further, we verify the above rules by the toy experiment (see Sec. 4.1) in Fig. 4 and Fig. 5.

### Variants of TCN for Image Enhancement

Upon the above principles of TCN, we provide the following implementation variants within image enhancement task.

**The original TCN.** We construct the original TCN (see Fig. 2 (a)) for image enhancement based on calculating statistics in Eq. 9 of IN format, which is plug-and-play for networks.

**The affined TCN.** We extend the original TCN by introducing affined parameters \(\) and \(\) to the normalization procedure, resulting in the affined TCN (Fig. 2 (b)). We incorporate learnable shifting

  Formats & Redefine \(I_{k}\) to Eq. 9 as \\  TCN (IN) & \(I_{IN}\) (default \(I_{k}\) of Eq. 9) \\ TCN (BN) & \(I_{BN}=\{(n,i,j)|n[1,N],\\ & i[1,H],j[1,W]\}\) \\ TCN (LN) & \(I_{LN}=\{(c,i,j)|c[1,],\\ & i[1,],[1,]\}\) \\ TCN (GN) & \(I_{GN}=\{(c,i,j)|\\ c[,+/],[1, ],\\ & j[1,]|[1,]\}\) \\  

Table 1: The TCN family with different \(_{2}\),\(_{2}\) calculation in Eq. 9.

parameter \(\) and scaling parameter \(\) into \(_{2}\) and \(_{2}\) in Eq. 9:

\[_{2}^{}=_{2}+,_{2}^{}=}{} \]

where \(_{2}^{}\) and \(_{2}^{}\) represent the affined statistics. Then, we substitute Eq. 18 to Eq. 1:

\[_{1}^{}=-{^{}}_{2}}{_{2} ^{2}+}}=-_{2}-}{^{2}}{^ {2}}+}} \]

Since \(\) is a small constant near to 0, the Eq. 19 can be approximated as:

\[_{1}^{}-{^{}}_{2}-}{_{2}^{2}+}}=-{^{}}_{2}}{ _{2}^{2}+}}+^{},^{}= _{2}^{2}+}}. \]

Eq.20 shares a format similar to the affined normalization, with \(\) and \(^{}\) as the learnable scaling and shifting parameters in Eq. 3. The affined TCN seamlessly integrates into image enhancement networks, serving as a plug-and-play solution. Notably, it maintains the information transition-constant property, as discussed in detail in the supplementary material.

**The skip TCN.** From Eq.10 and 11, the TCN generates two types of features: a domain-invariant lightness consistent feature \(_{1}\) and a domain-variant lightness inconsistent feature \(_{2}\). Previous studies  have demonstrated the effectiveness of incorporating the domain-variant component into deep encoder-decoder networks while skipping the domain-invariant component to the decoder layer. In this work, we propose the skip TCN architecture, illustrated in Fig. 2 (c).

Given a feature \(F\) in an encoder layer, we convey its lightness inconsistent feature \(_{2}\), obtained from Eq. 10, to the downsampled deeper layer that derives \(F_{d}\) using the following expression:

\[F_{d}=_{2}()+}_{2}, \]

where \(_{2}\) means downsampling with a factor of 2. While for the lightness consistent feature \(_{1}\) derived in Eq. 11, we skip it to the corresponding decoder layer feature \(F_{u}\) with the statistic \(_{2}\) and \(_{2}\) derived in Eq. 9. We integrate them by inverting the operation of Eq. 10 and Eq. 11:

\[ F_{u1}=&_{1}_{2}+ _{2}, F_{u2}=_{1}+F_{u},\\ F_{uo}=&_{2}(_{u})+((_{u1},_{u2})), \]

Figure 2: The illustration of the TCN operation and other TCN variants for image enhancement.

effectiveness of the TCN. Further details and discussions are available in the supplementary material.

## 4 Experiments

In this section, we validate the effectiveness and scalability of our proposed TCN on various image enhancement tasks. We provide more experimental results in the supplementary material.

### Toy Experiment

To illustrate the proposed TCN has the ability to normalize the features while transition-constant, we introduce a toy experiment as shown in Fig. 4 : we construct an encoder-decoder-based architecture for reconstructing the input image, where the TCN and other normalization formats are inserted between the encoder and decoder as different versions. Then we train this self-reconstruction architecture on 1000 samples from MIT-FiveK dataset  until its convergence and test the self-reconstruction effect on another 100 samples from the same dataset. The quantitative results in the right of Fig. 4 indicate that our TCN reconstructs the input image better than directly inserting IN, demonstrating the information transition-constant property. Furthermore, we test 100 underexposure samples and 100 overexposure samples from the SICE dataset, and we provide the feature distribution of \(F\) and \(_{1}\) (input and normalized output of the TCN) in the left part of Fig. 5, as well as feature maps in the right part of Fig. 5. The different exposure features processed by the TCN get to be

Figure 4: Toy experiment of self-reconstruction. The left is the setting of toy experiments with inserting different operations, and the right presents the self-reconstruction PSNR of testing images.

Figure 5: Feature visualization of toy experiment. In left and right parts, we show the feature in the TCN of underexposure and overexposure samples when testing them with inserting TCN in Fig. 4.

Figure 3: The overview of the TCN-Net.

clustered, demonstrating the normalization ability of the TCN for extracting the lightness-consistence representation of different samples. We provide more discussions in the supplementary material.

### Experimental Settings

**Low-light Image Enhancement.** Following previous works [60; 61], we employ three widely used datasets for evaluation, including LOL dataset , Huawei dataset  and MIT-FiveK dataset . We employ two different image enhancement networks, DRBN  and SID  as baselines.

**Exposure Correction.** Following , we adopt MSEC dataset  and SICE dataset  for evaluations. The above two architectures, i.e., DRBN  and SID  are regarded as baselines.

**SDR2HDR Translation.** Following , we choose the SRITM dataset  and HDRTV dataset  for evaluation. We employ the structures of NAFNet  with its three basic units as the baseline in the experiments.

**Image Dehazing.** Following , we employ the RESIDE dataset  consisting of Indoor and Outdoor parts for evaluations. We adopt the network of PFFNet  as the baseline for validation.

### Implementation Details

Since there exist three TCN formats in Sec. 3.3, we respectively integrate them into the baseline to conduct experiments. For comparison, we perform the experiments of baseline networks and the integration of the IN operation. Additionally, the TCN-Net in Sec. 3.3 is also performed in experiments. We train all baselines and their integrated formats following the original settings, and our TCN-Net until it converges. More implementation details are provided in the supplementary.

   Settings & \#Param & Flops (G) & LOL & Huawei & FiveK \\  DRBN (Baseline)  & 0.53M & 39.71 & 19.95/0.7712 & 20.64/0.6136 & 22.11/0.8684 \\ +IN & 0.53M & 39.71 & 20.73/0.7986 & 21.01/0.6200 & 22.93/0.8727 \\  +Original TCN & 0.53M (+0) & 39.71 (+0) & 21.15/**0.8190** & 21.12/**0.6242** & **23.98**/0.8851 \\ +Affined TCN & 0.53M (+0) & 39.71 (+0) & 21.29/0.8167 & 21.04/0.6231 & 23.92/**0.8858** \\ +Skip TCN & 0.53M (+0) & 39.77 (+0.07) & **21.52**/0.8271 & **21.15**/0.6195 & 23.82/0.8832 \\   SID (Baseline)  & 7.40M & 51.06 & 20.85/0.7845 & 19.68/0.6050 & 21.49/0.8425 \\ +IN & 7.40M & 51.06 & 20.51/0.7858 & 20.09/0.6034 & 21.75/0.8453 \\  +Original TCN & 7.40M (+0) & 51.06 (+0) & 21.43/0.7913 & 20.53/0.6067 & 23.11/0.8581 \\ +Affined TCN & 7.40M (+0) & 51.06 (+0) & 21.35/0.7867 & 20.62/0.6077 & 23.20/0.8624 \\ +Skip TCN & 7.41M (+0.01) & 51.42 (+0.36) & **21.92**/**0.8056** & **20.76**/**0.6083** & **23.61**/**0.8704** \\   TCN-Net & 0.012M & 0.97 & 22.08/0.7895 & 20.99/0.6121 & 23.47/0.8663 \\   

Table 2: Comparison over low-light image enhancement in terms of PSNR/MS-SSIM.

   Settings & MSEC & SICE \\  DRBN (Baseline)  & 19.52/0.8309 & 17.65/0.6798 \\ +IN & 21.98/0.8463 & 20.15/0.6947 \\  +Original TCN & 22.37/0.8533 & 20.74/0.7133 \\ +Affined TCN & 22.41/0.8504 & **20.85**/**0.7192** \\ +Skip TCN & **22.48/0.8572** & 20.65/0.7159 \\   SID (Baseline)  & 19.04/0.8074 & 18.15/0.6540 \\ +IN & 21.36/0.8373 & 19.81/0.6667 \\  +Original TCN & 22.31/0.8522 & 20.51/0.6745 \\ +Affined TCN & **22.43**/0.8542 & **20.68**/0.6757 \\ +Skip TCN & 22.36/**0.8603** & 20.64/**0.6852** \\   TCN-Net & 22.19/0.8480 & 20.72/0.7024 \\   

Table 3: Comparison over exposure correction. Figure 6: Training PSNR on exposure correction.

### Comparison and Analysis

**Quantitative Comparison.** The model comparisons are conducted over different configurations, as illustrated in the implementation details. We present the quantitative results from Table 2 to Table 5, where the best and second-best results are highlighted in bold and underlined. As can be seen, almost all formats of the TCN that we incorporate have improved the performance across the datasets in all tasks, validating the effectiveness of our method. Specifically, integrating variants of TCN helps improve the training performance of baseline as shown in Fig. 6. In contrast, naively integrating the IN could not always bring performance improvement (i.e., the results of SID in Table 2). All the above results suggest the effectiveness of our proposed method without introducing any parameters. Moreover, the proposed TCN-Net achieves effective performance with efficiency. All the above evaluations prove the convenience of applying the TCN in image enhancement tasks.

**Qualitative Comparison.** We report the visual results of low-light image enhancement on the MIT-FiveK dataset  due to the limited space. As shown in Fig. 7, the integration of the TCN leads to a more visually pleasing effect with less lightness and color shift problems compared with the original baseline. We provide more visual results in the supplementary material.

### Extensive Applications

The TCN can also be applied to other machine vision tasks that demonstrate its extensibility. Since TCN is proposed to extract lightness (a kind of style) invariant feature while keeping information transition-constant, we introduce another two tasks that are also related to style information, including pan-sharpening and medical segmentation. For pan-sharpening, it aims to fuse two style images, and we hope TCN can extract their invariant information with information preserving; For medical segmentation, there often exists a style domain gap between training and testing sets.

**Extension on medical segmentation.** We apply the TCN on the UNet  and AttUNet  in the medical segmentation task. We train the baseline and its integrated version on the heart segmentation task of Medical Segmentation Decathlon challenge dataset . As shown in Table 6, our TCN improves and keeps the performance of U-Net and Att-Unet, respectively, while IN brings a significant performance drop. The results suggest the scalability of the TCN compared with the IN.

**Extension on pan-sharpening.** We apply the original TCN to the GPPNN  and PANNet  baselines in the pan-sharpening task, which is a common task in guided image super-resolution. We integrate it when extracting pan and multi-spectral features, and experimental results on WorldView II dataset [72; 73] in Fig. 8 suggest the effectiveness of the TCN.

   Settings & SRITM & HDRTV \\  NAFNet (Baseline)  & 33.44/**0.9537** & 36.49/0.9706 \\ +IN & 33.62/0.9491 & 36.62/0.9683 \\  +Original TCN & **33.69**/0.9505 & **36.94**/0.9712 \\ +Affined TCN & 33.65/0.9495 & 36.55/0.9716 \\ +Skip TCN & 33.51/0.9513 & 36.64/**0.9720** \\   TCN-Net & 32.48/0.9439 & 36.78/0.9744 \\   

Table 4: Comparison over SDR2HDR translation.

   Settings & Indoor & Outdoor \\  PFFNet (Baseline)  & 21.74/0.8452 & 24.47/0.9274 \\ +IN & 23.13/0.8583 & 25.61/0.9309 \\ +TCN & 23.57/0.8635 & 25.63/0.9311 \\ +Affined TCN & **23.71**/0.8652 & **25.84**/**0.9312 \\ +Skip TCN & 23.21/**0.8708** & 25.63/**0.9315** \\   TCN-Net & 24.06/0.8645 & 23.72/0.8572 \\   

Table 5: Comparison over image dehazing.

Figure 7: The visual comparison of low-light image enhancement on the MIT-FiveK dataset.

## 5 Limitation and Discussion

Firstly, we validate the effectiveness of TCN in image enhancement tasks, while the investigation of applying TCN to other image restoration tasks will be explored in the future, such as the all-in-one image restoration task that meets similar challenges like image enhancement tasks, which has been pointed in some related works [74; 75]. Second, dedicated to image enhancement tasks, we mainly discuss the IN format of the TCN. However, other normalization formats can be future explored for other tasks. Moreover, the design formats of the TCN could inspire some areas that also require transition-constant, such as image fusion tasks . Finally, the TCN could introduce very few computation burdens although it is free of parameters, which is negligible compared with its bring performance improvement. Note that the focus of this work is beyond introducing a plug-and-play operation to existing networks for performance gain. The introduced TCN can be a new choice of normalization and feature disentanglement, which excavate consistent representations while preserving information when developing a new model that requires this property.

## 6 Conclusion

In this paper, we introduce a new perspective that develops the normalization technique tailored for image enhancement approaches. We propose the TCN that transits the information constantly with the invertible constraint, meanwhile, it keeps the normalization ability for capturing lightness consistence representations. The proposed TCN is a general operation that can be integrated into existing networks without introducing parameters. Extensive experiments demonstrate the effectiveness and scalability of applying the TCN and its variants in various image enhancement tasks.

## Broader Impact

Image enhancement is an important task that improves the quality of these images, exhibiting a high value of research and application. Our method introduces a normalization operation with information transition-constant property, which shows promising results that improve the learning ability of networks for image enhancement tasks conveniently. However, there could be negative effects brought by the proposed methodology. For example, some people may prefer the image with a dim light effect, which would be eliminated by the image enhancement algorithm. In these cases, it is suggested to combine the users' preferences to achieve customized image enhancement effects.