# Navigating the Maze of Explainable AI: A Systematic Approach to Evaluating Methods and Metrics

Lukas Klein 1,2,3 Carsten Luth 1,3,4 Udo Schlegel 5 Till Bungert 1,3,4 Mennatallah El-Assady 2 Paul Jager 1,3

###### Abstract

Explainable AI (XAI) is a rapidly growing domain with a myriad of proposed methods as well as metrics aiming to evaluate their efficacy. However, current studies are often of limited scope, examining only a handful of XAI methods and ignoring underlying design parameters for performance, such as the model architecture or the nature of input data. Moreover, they often rely on one or a few metrics and neglect thorough validation, increasing the risk of selection bias and ignoring discrepancies among metrics. These shortcomings leave practitioners confused about which method to choose for their problem. In response, we introduce LATEC, a large-scale benchmark that critically evaluates 17 prominent XAI methods using 20 distinct metrics. We systematically incorporate vital design parameters like varied architectures and diverse input modalities, resulting in 7,560 examined combinations. Through LATEC, we showcase the high risk of conflicting metrics leading to unreliable rankings and consequently propose a more robust evaluation scheme. Further, we comprehensively evaluate various XAI methods to assist practitioners in selecting appropriate methods aligning with their needs. Curiously, the emerging top-performing method, Expected Gradients, is not examined in any relevant related study. LATEC reinforces its role in future XAI research by publicly releasing all 326k saliency maps and 378k metric scores as a (meta-)evaluation dataset. The benchmark is hosted at: [https://github.com/IML-DKFZ/latec](https://github.com/IML-DKFZ/latec).

## 1 Introduction

Explainable AI (XAI) methods have become essential tools in numerous domains, allowing for a better understanding of complex machine learning decisions. The most prevalent XAI methods are saliency maps . As the diversity and abundance of proposed saliency XAI methods expand alongside their growing popularity, ensuring their reliability becomes paramount . Given that there is no clear "ground truth" for individual explanations (e.g., discussed in Adebayo et al. ), the trustworthiness of XAI methods is typically determined by examining three key criteria: their accuracy in reflecting a model's reasoning ("faithfulness") , their stability under small changes ("robustness") , and the understandability of their explanations ("complexity") . Beyond qualitative assessment of saliency maps such as in Doshi-Velez and Kim , Ribeiro et al. , Shrikumar et al. , which can be influenced by human biases and does not scale to large-scale

[MISSING_PAGE_FAIL:2]

LATEC assesses 7,560 unique combinations. LATEC addresses Shortcoming 1 by systematically incorporating all prevalent methods and metrics, as well as all vital underlying design parameters affecting XAI methods, and quantifying their effect on XAI methods. LATEC further addresses Shortcoming 2 by performing a dedicated analysis of the metrics themselves (also referred to as "meta-evaluation"), including a quantitative validation of the metrics' ranking behaviors, resulting in the identification of a more robust evaluation scheme. Moreover, in support of future research, we've made all intermediate data, including 326,790 saliency maps and 378,000 evaluation scores, as well as the benchmark publicly accessible.

## 2 The LATEC benchmark

The LATEC benchmark includes a framework and a dataset with the method rankings as the final output. The framework allows for diverse large-scale studies, structuring the experiments in six stages (see Figure 1), and the LATEC dataset provides reference data for evaluation and exploration. As the benchmark is easily extendable and leverages the high-quality dataset for standardized evaluation, it also serves as a foundation for future benchmarking of new XAI methods and metrics (see Appendix B for more information about the LATEC dataset).

Utilized input datasetsFor the image modality, we use ImageNet (IMN) , UCSD OCT retina (OCT)  and RESIC45 (R45) , the volume modality the Adrenal-(AMN), Organ-(OMN) and VesselMedMNIST3D (VMN) datasets , and the point cloud modality the CoMA (CMA) , ModelNet40 (M40)  and ShapeNet (SHN)  datasets.

Model architecturesOn each utilized dataset except IMN, where we take pretrained models, we train three models to achieve the architecture-dependent SOTA performance on the designated test set (if available, see Appendix A for a detailed description of the model training and hyperparameters). For the image modality, we use the ResNet50, EfficientNetb0, and DeiT ViT  architectures, for the volume modality the 3D ResNet18, 3D EfficientNetb0, and Simple3DFormer  architectures, and for the point cloud modality the PointNet, DGCNN and PC Transformer  architectures. The first two architectures are always CNNs and the third is a Transformer.

XAI methodsIn total, we include 17 XAI methods, 14 attribution methods: Occlusion (OC) , LIME (on feature masks) , Kernel SHAP (KS, on feature masks) , Vanilla Gradient (VG) , Input x Gradient (IxG) , Guided Backprob (GB) , GC, ScoreCAM (SC) , GradCAM++ (C+) , Integrated Gradients (IG) , Expected Gradients (EG, also called Gradient SHAP) , DeepLIFT (DL) , DeepLIFT SHAP (DLS) , LRP (with \(\)-,\(\)- and \(0^{+}\)-rules depending on the model architecture) , and three attention methods: Raw Attention (RA) , Rollout Attention (RoA)  and LRP Attention (LA) . While the attribution methods are applied to all model architectures, the attention methods can only be applied to the Transformer-based architectures.

Figure 1: Structure of the LATEC framework including all design parameters and the output data of each stage provided as the LATEC dataset. Final rankings are analyzed in the benchmark.

Related work by Hooker et al.  and Yang and Kim  showed that advancing methods by VarGrad  or SmoothGrad  can, in general, improve results. We conduct an ablation study in Appendix O to validate these findings for our benchmark. Contrary to Hooker et al. , we find no substantial improvements w.r.t faithfulness or robustness, only SmoothGrad notably reduces complexity by producing more localized saliency maps. Thus, we only consider the original methods without adaptations in the benchmark.

We qualitatively tuned the XAI hyperparameters per dataset (see Appendix B), as also commonly done to avoid biasing the quantitative evaluation results (see Appendix C for all hyperparameters). We observe that most hyperparameters generalize well across the datasets within a modality. To further validate the non-sensitivity of the benchmark rankings to reasonably selected hyperparameters, we conduct an ablation study including the top five ranked XAI methods in Appendix N, validating the robustness of their performance.

**Evaluation metrics** We utilize a total of 20 well-established evaluation metrics, which are grouped into three criteria: faithfulness ("Is the explanation following the model behavior?"), robustness ("Is the explanation stable?"), and complexity ("Is the explanation concise and understandable?"). 11 metrics evaluate faithfulness: Faithfulness Correlation (FC) , Faithfulness Estimate (FE) , Pixel Flipping (PF) , Region Perturbation (RP) , Insertion (INS) and Deletion (DEL) , Iterative Removal of Features (IROF) , Remove and Debias (ROAD) , Sufficiency (SUF)  and Infidelity (INF) , 6 metrics evaluate robustness: Local Lipschitz Estimate (LLE) , Max Sensitivity (MS) , Continuity (CON)  and Relative Input/Output/Representation Stability (RIS, ROS, RRS) , and 3 metrics evaluate complexity: Sparseness (SP) , Complexity (CP) and Effective Complexity (ECP) . See Appendix D for a description of every metric. We select the hyperparameters per dataset as some depend on dataset properties (see Appendix subsection D.2 for all parameters). As the raw evaluation scores have no semantic meaning and can be extremely skewed in their distributions, making them hard to interpret and compare, we analyze the XAI methods and metrics based on their ranking.

**3D adaptation** While several XAI methods and metrics in LATEC are independent of the input space dimensions, others had to be adapted to 3D volume and point cloud data, building upon the implementations for image data by Kokhlikyan et al.  and Hedstrom et al. . Due to the adaptations, this benchmark provides the first large-scale insights into XAI method performance and metric behavior on 3D data. We describe the adaptation process, including rigorous testing, for all respective XAI methods and metrics in Appendix H and show illustrative saliency maps.

Figure 2: **a.** Ranking of four XAI methods based on all evaluation metrics of each criterion for one specific set of design parameters. **b.** Average standard deviation per model architectures and utilized datasets for the imaging modality. The weighted average per column is based on the number of metrics per criterion. **c.** Proportion of accepted one-sided Levene-Tests for significantly smaller ranking variance compared to the variance of an entire random ranking. Larger values show higher agreement between metrics. The weighted average is based on the number of metrics per criterion.

Deriving a reliable evaluation scheme for XAI

### How severe is the risk of metric selection bias in XAI evaluation?

In Shortcoming 2, we describe a risk of selection bias due to approximating an evaluation criterion with only one or a few metrics, possibly overfitting to a limited set of perspectives on the criterion. In this metrics analysis, we first aim to provide empirical evidence for this risk. A first exploratory analysis quickly supports the hypothesis, as we encounter strong ranking disagreement between metrics for various combinations of underlying design parameters. We define ranking agreement as the consensus among metrics belonging to one criterion about the rank of one XAI method when evaluating and subsequently ranking this method against all other XAI methods. Consequently, disagreement in ranking is defined through high variance between the determined ranks of the metrics for one XAI method. For example, Figure 2 (a.) demonstrates the ranking behavior of four selected XAI methods for one selection of underlying design parameters. The line charts show how each metric ranks the four XAI methods in comparison to all other XAI methods, with the mean aggregated average rank to the right. For faithfulness, we observe high disagreement between metrics in their ranking of GC and IG, mainly agreeing metrics in the ranking of DLS (with IROF and MC being noticeable outliers), and agreeing metrics in the case of LIME.

**Do metric disagreements depend on underlying design parameters?** The inquiry emerges as to whether the risk of selection bias is generally present in certain combinations of underlying design parameters or is uniformly distributed across them. To this end, we computed the average standard deviation (SD) between metric rankings either aggregated across model architectures or datasets (see Appendix Equation 1 and Equation 2 for the mathematical formulation) to observe the variance among metrics per modality, model architecture, and datasets. Figure 2 (b.) shows for the imaging modality that the average standard deviation is generally stable between model architectures or datasets within each evaluation criterion (see Appendix I for the other two modalities). Thus, we can conclude that there is no single model architecture, modality, or dataset choice that has a substantial effect on the disagreement between metric rankings of all XAI methods.

**Do metric disagreements vary for individual XAI methods?** Now that we can rule out the general influence of underlying design parameters, we quantify how strong the risk of metric disagreement is in general and if there is a difference between XAI methods. To this end, we utilize a one-sided Levene's Test , testing if the rank-variance of a set of metrics is significantly lower than the variance of a random rank distribution, which can be analytically inferred. We compute this test for all sets of metrics on every possible combination of design parameters and mean aggregate over model architectures and datasets (see Appendix Equation 3 for the mathematical formulation). Figure 2 (c.) shows for the imaging modality the resulting proportion of accepted tests (\(=0.1\)) for each criterion and XAI method. By computing the weighted average proportion across criteria at the bottom, we indeed observe strong variations between the XAI methods. Specifically for KS, EG, and DLS, in a large majority of cases, metrics agree, while for OC, IxG, SC, and DL only in about \( 27\%\) of the cases variance in metric ranking is significantly lower than random ranking. Concluding, our findings reveal that metrics disagree and agree in varying degrees depending on the XAI method. We refer to Appendix J for a study on why metrics disagree.

### How can we identify reliable trends within agreeing and disagreeing metrics?

After providing empirical evidence for the risk of selection bias in XAI evaluation, we identify three resulting major limitations **(1-3)** in current practice, collectively summarized as Shortcoming 2. The current practice in XAI evaluation is to employ a small set of metrics and subsequently mean aggregate over the normalized scores to increase the generalization of results and simplify data analysis in big datasets (see e.g. Li et al. , Hedstrom et al. , Hesse et al. ). We present each limitation of this procedure with a corresponding solution and combine them into a single evaluation scheme. This proposed scheme aims to reliably benchmark XAI methods across both agreeing and disagreeing metrics, offering a more robust evaluation approach.

**(1)** Current practice includes only small sets of metrics, which comes with an increased risk of biases due to a high dependence on metric selection and individual metric behavior. Further, mean aggregating in such small samples lacks robustness against "outlier metrics". The subsection 3.1 demonstrates this risk of selection bias extensively. We start addressing this limitation by including the to-date largest scale of diverse and relevant metrics, minimizing the risk of selection bias and the influence of outliers metrics. The adequacy of the selected metrics is ensured by exclusively choosing standard metrics commonly implemented in widely used software libraries [27; 34], while also ensuring diversity in their implementation and interpretation of the respective criteria. This approach is crucial to avoid overfitting to a single perspective.

**(2)** Aggregation across metrics obfuscates their diverse perspective on the approximated evaluation criterion. Such aggregation can be further flawed due to unbounded metrics, inconsistent interpretations (e.g. correlation coefficient vs. distance-based metrics), and sensitivity to metric score outliers and distribution skewness (also shown by Colombo et al. ). We address this limitation by employing an "aggregate-then-rank" scheme, which is already well established for large-scale evaluation and benchmarks of model performance [40; 17; 54]. By aggregating the median evaluation score of all model and dataset combinations of one modality and metric we get more robust metric scores without ignoring the perspective of the individual metric. Subsequently, we rank the computed scores, as rankings are independent of the scales or units of the metrics and are generally easier to interpret . However, we acknowledge that in a large-scale study such as ours, abstraction is necessary to distill meaningful results from the extensive set of rankings. To highlight strong trends across metrics we compute their average rank per XAI method, indicated by "\(\)" in Table 2. A consistent high or low average rank for an XAI method implies a general agreement among the metrics evaluating a specific criterion.

**(3)** Current studies ignore the extent of disagreement between metrics, which contains crucial information about a method's performance. We believe that understanding why metrics disagree and the situations in which this occurs is vital for evaluating XAI. To determine the presence of disagreement in general, we deployed the Levene test in Figure 2. In the XAI benchmark, however, we are interested in comparing the level of disagreement between XAI methods. To this end, we calculate the SD between ranks of an XAI method as a measure of disagreement, indicated as "\(\)".

**Proposed evaluation scheme.** We subsequently combine all solutions into one proposed evaluation scheme. To include all metric perspectives and increase general ranking robustness, we first calculate the median of the standardized evaluation score from all our included relevant metrics for each combination of dataset and model. Further, we average these medians and rank the methods according to each metric (see Appendix F for a detailed flow chart of how we get from evaluation scores to rankings). Ranking XAI methods across one metric's evaluation scores from several input datasets and model architectures makes the ranking more robust to variation within these parameters. To analyze the large set of ranking results, we jointly utilize the mean \(\), median \(x_{n/2}\), and SD \(\) (e.g. in Table 2) to detect strong ranking trends through \(\) and \(x_{n/2}\), and determine their trustworthiness through \(\), before focusing onto the individual metrics. We determine the threshold values in \(\), indicating high or low SD, based on the quantiles of each evaluation criterion's SD distribution. To assess the statistical significance of the differences between two methods, we report the p-values of the Wilcoxon-Mann-Whitney tests for all modalities and evaluation criteria, comparing the rankings of all XAI methods, as detailed in Appendix P. Based on this evaluation scheme, we achieve more robust rankings, include a diverse set of metric perspectives, and still leverage a mechanism to highlight strong trends and (dis-)agreeing metrics from the large set of results.

### Additional insights for robust evaluation

We encountered further pitfalls of the current metric application in XAI, which to our knowledge have not been discussed before. While all pitfalls are discussed in Appendix L, one pitfall regarding complexity evaluation is in our opinion especially critical. Suspiciously, Figure 2 (c.) indicates almost no disagreement between complexity rankings. Further, CAM (GC, SC, C+) and attention (RA, RoA, LA) methods are ranked significantly more complex (see Table 2). In our opinion, this observation is counter-intuitive when comparing the complexity rankings to the saliency maps in Appendix B, based on which we would classify CAM and attention methods as more localized and less noisy. While all three complexity metrics are also explicitly proposed for image data, we notice that they all treat each pixel, voxel, or point independently of each other, ignoring locality and favoring methods that attribute to the smallest set of single pixels. As this approach possibly transfers to low dimensional images such as MNIST  or CIFAR-10 , the image datasets the three metrics are originally presented on, we hypothesize that it may not be effective with higher-dimensional inputs as observed in our study. Consequently, it is expected that techniques such as LRP would be highly regarded due to their emphasis on filtering the significance of individual pixels, in contrast to CAM methods

[MISSING_PAGE_FAIL:7]

can transfer well to others if data dimensionality and characteristics are not too distinct. For model architectures, however, there are two notable exceptions. CAM methods generally show higher ranking dissimilarities between architectures, which could be attributed to differences in latent representations of the models, as the semantics captured in the last convolutional or cls-token layers do not have to coincide between models. Thus, we recommend increasing the robustness by averaging the activation map of several hidden layers, which has shown effective in application , but can lead to less localized saliency maps. LRP shows additionally high dissimilarity between CNN and Transformer architectures, especially for the 3D modalities. We use the recommended \(\)- and \(\)-rules in LRP for the CNN models. However, on Transformer architectures, LRP does not preserve the conservation rule and only works with the \(0^{+}\)-rule (see Chefer et al. ). Both implemented changes to LRP bias the relevance computation, which consequentially impacts its performance on Transformer architectures. Thus, we recommend using LA instead of LRP as a relevance-based method on Transformer architectures, as it leverages the Transformer-inherent attention and performs better regarding faithfulness and robustness.

**Ranks of XAI methods are highly dependent on the input modality, especially for linear surrogate and CAM methods.** In general, we observe substantial ranking differences across modalities. Especially both linear surrogate methods (LIME, KS) underperform on image and volume compared to the lower dimensional point cloud modality in terms of faithfulness. On these modalities, their performance also strongly depends on the suitability of the feature mask computed via a grid or super-pixels, which is very time-consuming to fine-tune for single observations. Further, their evaluated robustness is very low across all modalities. Concluding, we advise against using them for high-dimensional and complex relationships. CAM methods achieve always higher faithfulness on image than on volume data. When comparing the saliency maps between both modalities, we observe that the volume-based maps are much coarser (i.e. more "blocky") and less focused. We attribute this observation to less accurate latent model representations and subsequent up-sampling in 3D compared to 2D space, subsequently not recommending them for volume data. Overall, results indicate higher consistency in robustness across modalities, with greater variability in faithfulness and complexity. Notably, the standard deviation of robustness metrics differs significantly between volume and point cloud data, suggesting an influence on metric disagreement.

**Attention methods are more robust compared to attribution methods but can exhibit strong disagreement among metrics** We observe a very large SD for the three attention methods (RA, RoA, LA) as faithfulness and robustness metrics rank them either very high or low (except for robustness on volume data). Therefore, we would strongly recommend investigating the interaction between metrics, attention methods, and also the transformer architectures in more depth as the risk of selection bias is by far the highest for this subgroup of methods. Among the attention methods, the relevance-filtered-based LA method scores primarily higher than non-filtered raw attention. In addition, LA allows to visualize input features that attribute to a specific outcome and are not only detected by the model in general, making it much more versatile.

**Compared to other method subgroups, SHAP methods differ strongly in performance.** Contrarily to other method subgroups such as linear surrogate methods (LIME, KS), CAM methods (GC, SC, C+), and attention methods (RA, RoA, LA), the Shapely value approximating SHAP methods (EG, KS, and DLS) differ extensively in their performance. This observation is consistent with the results of Molnar et al. , which are, however, not in the context of XAI evaluation. Therefore, it is advisable not to select a single SHAP method with the expectation of achieving similar results to others but rather to employ multiple such methods. For more results regarding behavioral similarities among XAI methods, we refer to Appendix M.

**For LRP we observe a trade-off between faithfulness and complexity.** In subsection 3.3 we already discussed our reservation against the complexity metrics and why especially CAM and attribution methods rank low. However, besides the attention methods, we also observe a strong trade-off between faithfulness and complexity for LRP, which we would also relate to the mathematical formulation of the complexity metrics. We can explain this observation by LRP's tendency to attribute to a very small set of input features: Faithfulness is low due to the absence of important input features in the attributed set, and robustness is low as the relative change in this set can occur fast, but complexity, as evaluated in our metrics, is also low due to the small set size. The attributed set size can be influenced by the model-layer assigned relevance propagation rule, e.g. switching the \(\)-rule with the \(0\)-rule, or hyperparameters, but this has to be fine-tuned per observation, making LRP only versatile when explaining single observations.

Comparison with related work

Our study addresses the limitations of previous research, which uses small and varying subsets of XAI methods and metrics, by providing a comprehensive and robust analysis that includes measurements of disagreement between metrics, thereby significantly enhancing validity and resolving gaps and inconsistencies between related studies. These gaps and inconsistencies become particularly evident from Table 1, which presents a summary of 18 related relevant studies (all on image data as there are none for volume or point cloud). While some "evergreen" XAI methods, i.e., VG, IxG, GB, and IG, stand out, the sparsity of Table 1 is very notable, especially for attention methods. In comparison, we present our results for the imaging modality indicated at the bottom, demonstrating the extensive difference in scale. Surprisingly, our consistently top-ranking method in terms of faithfulness and robustness, EG, is not evaluated in any of the related studies.

Back-referencing to Table 1, we observe in several cases similar results to other studies on image data: low faithfulness of VG, LIME, or LRP by Chefer et al. , and high faithfulness of IG (can depend highly on the selected baseline ) and LA (but only two studies including attention methods). Regarding the conflicting outcomes reported for GC, our results show average faithfulness but high robustness on image data (but can depend on the underlying model, as our work suggests). On the contrary, our results contradict the findings on high faithfulness and robustness of KS (Bhatt et al.  uses lower dimensional image data), high faithfulness of LRP, or low faithfulness of IG. However, these results can differ between modalities, as GC, for example, obtains very low scores in faithfulness and robustness on volume data. No related studies that examine both attention and attribution methods address the notably higher SD observed in attention methods compared to attribution methods when evaluating faithfulness.

Most evaluation of complexity is qualitative (e.g. Singh et al. ), with only those studies that introduce a metric themselves conducting also quantitative evaluations (i.e. Nguyen and Martinez , Kakogeorgiou and Karantzalos ). We consider the high fluctuation between quantitative and especially qualitative complexity evaluation outcomes as further support for our hypothesis that there is a gap between the aim of the metrics and the human conception of low complexity, strongly recommending the development of either new metrics or falling back to robust qualitative user studies.

## 6 Conclusion and discussion

Although our benchmark is one of the most comprehensive in the field, we restrict ourselves to the computer vision modalities with the, in our opinion, most unique and not overlapping characteristics, ignoring e.g. videos. Non-computer vision modalities, such as language, introduce new modality-specific XAI methods and metrics, rendering large-scale comparisons between these modalities infeasible. We also did not include more unconventional post-hoc XAI methods such as symbolic representations and meta-models or niche evaluation criteria like localization and axiomatic properties as they either require ground-truth bounding boxes or can not be applied to all XAI methods. Further, our benchmark focuses on the comparison between methods, not on the evaluation to what extent an individual method may or may not be faithful or robust in general, thus ignoring e.g. synthetic baselines.

Our results demonstrate vividly the need for rethinking the evaluation of XAI methods and the risks of inconsistent benchmarking for practitioners and researchers. As a solution, we offer practitioners profound benchmarking capabilities, practical takeaways for applying and selecting XAI methods, and adapted XAI methods and metrics for 3D modalities. This includes the most all-encompassing answer to _"What XAI method should I (not) use for my problem?"_ to date, based on the extensive evidence in our provided result tables and the LATEC dataset. For researchers, we propose a new evaluation scheme, address the risk of conflicting metrics, and introduce LATEC as a platform for standardized benchmarking of methods and metrics in XAI. LATEC offers researchers the opportunity to explore and answer numerous critical questions in XAI, thereby playing a pivotal role in the advancement of the field.

#### Acknowledgments

This work was funded by Helmholtz Imaging (HI), a platform of the Helmholtz Incubator on Information and Data Science.