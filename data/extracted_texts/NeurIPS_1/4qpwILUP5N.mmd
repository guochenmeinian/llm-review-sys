# A foundation for exact binarized morphological neural networks

Theodore Aouad

Universite Paris-Saclay, CentraleSupelec, Inria, CVN

3 rue Joliot Curie, Gif-sur-Yvette, France

theodore.aouad@centralesupelec.fr

Hugues Talbot

Universite Paris-Saclay, CentraleSupelec, Inria, CVN

3 rue Joliot Curie, Gif-sur-Yvette, France

hugues.tablot@centralesupelec.fr

###### Abstract

Training and running deep neural networks (NNs) often demands significant computation and energy-intensive specialized hardware (e.g. GPU, TPU...). One way to reduce the computation and power cost is to use binary weight NNs, but these are hard to train because their derivatives have a non-smooth gradient. We present a model based on Mathematical Morphology (MM), which can binarize ConvNets without loss of performance under certain conditions, but these conditions may not be easy to satisfy in real-world scenarios. To ameliorate this, we propose two new approximation methods and develop a robust theoretical framework for ConvNets binarization using MM. We also propose several regularization losses to improve the optimization. We empirically show that our model can learn a complex morphological network, and explore its performance on a classification task.

## 1 Introduction

Binary weight neural networks (BWNNs) are attractive because they can provide powerful machine learning solutions with much less storage, computation, and energy consumption than conventional networks . Several methods, such as BinaryConnect , DoReFa-Net , and XNOR-Net , have shown good to excellent results in a variety of applications. These networks usually use the sign function to binarize the weights in the forward pass. They must use a special gradient function, like the Straight-Through Estimator (STE) , to overcome the zero gradient problem of the sign function during the backward pass. However, this estimator, while effective in practice, lacks a solid theoretical basis, suggesting the need for a different approach. Some methods avoid using the STE by first training a floating-point neural network and then binarizing it afterwards . However, this approach may lead to approximate binarization and a drop in performance. In this paper, we present a new approach that uses the concepts of Mathematical Morphology (MM)  to overcome the drawbacks of existing methods. MM, based on modern set theory and complete lattices, offers a non-linear mathematical framework for image processing. Its basic operators, erosion and dilation, are equivalent to thresholded convolutions , underscoring a link between MM and deep learning. Combining these fields can improve the efficiency and results of morphological operations while enhancing our knowledge of deep learning . Recent works on morphological neural networks have explored learning operators and structuring elements using various approaches, such as the max-plus definition  and differentiable approximations . However, these methods primarily focus on learning gray-scale MM operators and have not focused on NN binarization.

[MISSING_PAGE_FAIL:2]

### Morphological Equivalence

We now express the conditions under which a BiSE neuron can be seen as a morphological operator.

**Theorem 2.3** (Dilation - Erosion Equivalence).: _For a given structuring element (SE) \(S\), and an almost binary parameter \(]0,]\), a set of reparametrized weights \(^{}\) and bias \(b\), we define:_

\[L_{}(,S) _{k S}[_{k}]_{+}+ -_{s S}[_{s}]_{+} \] \[U_{}(,S) +_{s S} _{s}+_{k}[_{k}]_{-}\] (5) \[U_{}(,S) _{k}_{k}-L_{}(,S) \]

\[L_{}(,S) _{k}_{k}-U_{}(,S) \]

_Let \(\{,\}\) be a dilation or erosion. Then:_

\[ I()\;,\;_{S}> = W>b L_{}(,S)  b<U_{}(,S). \]

_In this case, \( s S,_{s} 0\) and \(b 0\) and we say that a BiSE \(\) with weights \(W()=\) and \(B()=b\) is **activated**. If \(=\), then \(B()_{k}W()_{k}\). If \(=\), then \(B()_{k}W()_{k}\). For any almost binary image \(()\), \(()(_{out})\) is almost binary with known parameter \(_{out}\). Finally_

\[(),()> =_{S}>. \]

This theorem states that if a BiSE is activated, it transforms an almost binary inputs into an almost binary outputs with known \(_{out}\). Moreover, if we threshold the input and output with respect to \(\), it is equivalent to applying the corresponding morphological operation, exhibiting a weak commutativity property between thresholding and convolution. Further, equation (10) shows that if a BiSE is activated for operation \(\), performing the BiSE operation in \(()\) is equivalent to performing the binary morphological operation \(\) in the binary space \((S)\). This presents a natural framework for binarization (see SS3).

### Binary Morphological Neural Network

Our objective is to build a binarizable neural network. We now define binarizable neural layers based on the BiSE neuron. By combining these layers, we can create flexible architectures tailored to the desired task. As mentioned earlier, the BiSE neuron resembles a convolution operation. However, a single convolution is insufficient to create a morphological layer. In this context, we explain how we handle multiple channels. We observe that the BiSE neuron can be used to define a layer that learns the intersection or union of multiple binary images \(_{1},...,_{n}()\). For example, their union can be expressed as the dilation of the 3D image \((_{1},...,_{n})() ^{_{I}}^{n}\) with a tubular SE applied solely across the dimension of depth. Therefore, we define the Layer Union Intersection (LUI) as a special case of the BiSE layer, with weights restricted to deep-wise shape. It is analogousto a \(1 1\) convolution unit. A LUI layer can learn any intersection or union of any number of almost binary inputs. By combining BiSE neurons and LUI layers, we can learn morphological operators and aggregate them as unions or intersections.

**Definition 2.4** (BiSEL).: A BiSEL (BiSE Layer) is the combination of multiple BiSE and multiple LUI. Let \((_{n,k})_{n,k}\) be \(N*K\) BiSE and \((_{k})_{k}\) be \(K\) LUI. Then we define a BiSEL as:

\[:(^{_{I}})^{N}(_{k}_{n,k}(_{n})_{n})_{k}. \]

The BiSEL mimics a convolutional layer. In conventional ConvNets, to process multiple input channels, a separate filter is applied to each channel, and their outputs are summed to create one output channel. In the case of BiSEL, instead of summing the results of each filter, we perform a union or intersection operation (see Figure 1).

DenseLUIthe LUI layer is similar to a \(1 1\) convolution, which is equivalent to a fully connected layer. Given an input vector \(^{n}\), we can apply the LUI layer to the reshaped input \(}^{n 1 1}\) treating it as a 2D image with width and length of \(1\) and \(n\) channels, respectively. Therefore, we can utilize the BiSEL to create binarizable fully connected layers.

Gray-Scale / RGB InputsUp until now, all inputs were assumed binary. We extend these definitions to gray-scale and RGB images. The main idea is to separate an input channel \(_{c}^{_{I}}\) into its set of upper level-sets to come back to binary inputs:

\[\{(_{c})\}. \]

Considering all possible values of \(\) from a continuous image would result in an excessive number of level-sets to process. Alternatively, we can define a finite set of values for \(\) in advance. Subsequently, each channel of an image \(^{c w l}\) is separated into its corresponding level-sets, and these level-sets are provided as additional channels. If we have \(N\) values for level-set, the resulting input is a binary image \(I_{}\{0,1\}^{(N c) w l}\).

We have introduced two types of binarizable layers: the DenseLUI, which is similar to a fully connected layer, and the BiSEL, which resembles a convolutional layer. By combining these layers, we can create a Binary Morphological Neural Network (BiMoNN), which encompasses various architectures su

Figure 1: BiSEL vs Conv Layer. Input \(\) with 3 channels. Output \(()\) with 2 channels.

Figure 2: Gray to level-set for 5 different values, generating 5 input channels.

### Training considerations

The BiNoNN (\(_{}\)) is fully differentiable. If \(\) is a differentiable loss function, given a dataset of \(N\) labeled images \(\{(_{i},_{i})\}\), we minimize the error \(_{i=1}^{N}(_{}(_{i}), _{i})\) using a gradient descent algorithm (like Adam ). The gradients are computed with the backpropagation algorithm . The binarization scheme, which is defined in the next section, is applied post-training or during training to measure the model's evolution.

Our objective is to reach the set of activable weights and bias. Theorem 2.3 indicates that we only have to look at positive parameters. We can enforce them to be positive by setting \(W\) and \(B\) as the softplus function. Other reparametrizations are proposed in Appendix B.

\[B()=W() f^{+}()(1+()). \]

## 3 Binarization

Binarization of a neural network involves converting the real-valued weights and activations, typically stored as 32-bit floats, into binary variables represented as 1-bit booleans. Most binary NN use variables in \(\{-1,1\}\), which are not ideal for learning morphological operations. We instead utilize \(\{0,1\}\). BiMoNNs inherently correspond to binarized networks, making the BiSE neuron a natural framework for binarization when dealing with binary inputs. If a specialized hardware tailored for morphological operations is available, it can offer significant improvements in efficiency and performance, facilitating the binarization process. Alternatively, dilation and erosion can be expressed using binary-weighted thresholded convolutions. In our approach, binarization occurs after the training phase. We present two types of binarization for the BiSE neuron: the exact method (as introduced in ), when the BiSE neuron is activated, and two novel approximated methods. Then, we sequentially binarize the entire network.

### Exact BiSE Binarization

The real-value operations performed by an activated Binary Structuring Element (BiSE) in the almost binary space can be replaced with binary morphological operations on the binary space after thresholding at 0.5, without sacrificing performance (as per Theorem 2.3). To determine if a BiSE is activated and which operation it corresponds to, we introduce Proposition 3.1, which provides a linear complexity method for extraction.

**Proposition 3.1** (Linear Check).: _Let us assume the BiSE of weights \(W()\) and \(B()\) is activated for \(\) with SE \(S\) for almost binary images \(()\). Then \(S=(W()>_{})\) with_

\[_{} +}B()-_{W( )}[w]_{-}, \] \[_{} +}_{W()}[w]_ {+}-B(). \]

Given a BiSE neuron \(\) and an almost binary output in \(()\), we check if \(\) is activated for \(S_{}\) or \(S_{}\), where \(S_{}=(W()>_{})\) and \(S_{}=(W()>_{})\). If yes, we binarize by replacing \(\) with the corresponding morphological operator. If no, we use proposition 3.1 to confirm that \(\) is not activated, and we approximate the binarization using the methods in section 3.2. The exact method requires only the computation of \(L_{},U_{},L_{},U_{}\) and at most \((|_{S}|)\) operations.

### Approximate BiSE Binarization

In practice, BiSE are not always activated, necessitating an approximate binarization method. Let \((},,)((),B(),)\) be the learned reparametrized parameters.

#### 3.2.1 Projection onto activable parameters

To find the closest morphological operation, we minimize the Euclidean distance \(d\) for a given \(\{,\}\) and the set \(A_{,S}\) of activable parameters:

[MISSING_PAGE_FAIL:6]

[MISSING_PAGE_FAIL:7]

From this, we can define two regularization loss which can be computed without slowing the training down:

\[_{}=_{}:=_{i }_{i}-}_{s S_{u}}_{s} ^{2}, \] \[_{}=_{}:=_{i }_{i}-}_{s S_{n}}_{s} ^{2}. \]

## 5 Experiments

In this section, we empirically validate the capabilities of BiMoNNs in learning a binarized morphological pipeline through a denoising task, without the need for regularization. We also evaluate the model and regularization techniques on the MNIST classification task.

Binary DenoisingWe generate a second dataset to evaluate the denoising capacity of BiMoNNs. The target images in this dataset consist of randomly-oriented segments with width \(h\), with added Bernoulli noise. To filter these images, an MM expert would use a union of opening operations, where the SEs are segments with width \(1\) and angle one of \((0^{},90^{},-45^{})\). The SEs should be longer than the noise and shorter than the smallest stick in the target image (usually a length of \(5\)). Examples are given in Figure 2(a). Our architecture uses two consecutive BiSEL layer of kernel size 5 and 3 hidden channels (see Figure 2(b)). We optimize the MSE Loss, with an initial learning rate of \(0.01\). We train over 6000 iterations and halve the learning rate after 700 iterations with non-diminishing loss. We stop once the loss does not decrease after 2100 iterations. We employ a positive reparametrization for the bias and a dual reparametrization for the weights, and binarize with the projection onto activable parameters. The network achieves excellent denoising performance, with a DICE score of 97.5%. The remaining 2.5% discrepancy is due to artifacts between the sticks that cannot be denoised using an opening operation. The binarized network (Figure 2(b)) accurately learns the intersection of three openings (which remains an opening) the same way a human expert would combine such operators to achieve the denoising task optimally. Additionally, 4 out of the 6 BiSE are activated during the process. This experiment shows that our network can learn accurate and interpretable composition of morphological operators.

Figure 3: Binary denoising experiment.

ClassificationWe conduct classification experiments on the MNIST dataset . All images are thresholded at 128. Our BiMoNN model comprises one hidden DenseLUI layer with 4096 neurons. To handle the large number of parameters, we adopt the fast projection defined in SS3.2. We compare the classification accuracy of our float and binarized models against the SOTA and baseline models with fully connected layers and 1D batch normalization , employing \((()+1)\) as the activation layer. The accuracy results are summarized in Table 1. In our framework, binarizing the weights also entails binarizing the activations. Consequently, binarizing the last layer would yield binary decision outputs for each output neuron, possibly leading to multiple labels with a score of \(1\). To overcome this issue, we refrain from binarizing the last layer, thus retaining the real-valued activations. This decision affects a negligible proportion of parameters (\(\)0.1%). In traditional classification neural networks, the softmax activation is commonly used at the end of the last layer to produce the final probability distribution over the classes. However, in the BiMoNN architecture, we utilize the same activation function as the hidden layers, which is the normalized \(\). Additionally, we compare the performance of our BiMoNN model when replacing the last normalized \(\) activation with a softmax layer. When using the normalized \(\), \(_{data}\) is the Binary Cross-Entropy loss \(_{BCE}\), and when using the softmax, we use the Cross-Entropy loss \(_{CE}\):

\[_{BCE}(}_{i},_{i}^{*}) _{c=0}^{9}_{i}^{*}(}_{ i})+(1-_{i}^{*})(1-}_{i}), \] \[_{CE}(}_{i},_{i}^{*}) _{c=0}^{9}_{i}^{*}(}_{ i}). \]

We conduct a comprehensive random search to identify the optimal hyperparameter configuration for the Binary Morphological Neural Network (BiMoNN). The hyperparameters explored include the learning rate, last activation function (Softmax layer vs. normalized \(\)), positive vs no reparametrization. Additionally, we investigate regularization losses, such as no regularization, \(_{}\), \(_{}\), and \(_{}\). If regularization is applied, only positive weight reparametrization is considered. We vary the coefficient \(c\) in the regularization loss and explore different batch value starting time for when we start applying regularization during training. For each regularization schema, we select the model with the best binary validation accuracy, and the corresponding results are displayed in Table 1. Detailed hyperparameter configurations and hyperparameters study are provided in Appendix E. Applying the softplus reparametrization to the weights led to a slight increase in the floating-point error (2.2% vs. 2.8%). Similar findings were observed in  for non-negative neural networks in a different task. Generally, positive neural networks exhibit lower accuracy but offer enhanced robustness and interpretability . In our case, it significantly improved the binarized results, along with a substantial increase in the rate of activated BiSE neurons, rising from a median of 1.5% to 10% with softplus. Without imposing positivity, the binarized network performed randomly. We analyze the impact of regularization on the performance of the binarized model, which improves as expected. The float accuracy also increases, given that we select the model with the best binary accuracy on validation. Surprisingly, \(_{}\) and \(_{}\) outperform \(_{}\), despite being designed as approximations. This discrepancy might be due to the number of searches performed: \(_{}\) performed only 42 searches, while other configurations went through 100 searches. However, we have not yet achieved parity with the baseline for the float model or reached the state-of-the-art for

    & Architecture & Params & \(\) & \(\) \\   & DLUI (\(W=\)) & 3.3 M & **2.2\%** & 90.2\% \\  & DLUI (No Regu) & 3.3 M & 4.6\% & 10.1\% \\  & DLUI \(_{}\) & 3.3 M & 4.0\% & 7.3\% \\  & DLUI \(_{}\) & 3.3 M & 3.6\% & **4.5\%** \\  & DLUI \(_{}\) & 3.3 M & 2.8\% & 4.6\% \\   & EP 1fc  & 3.3 M & - & 2.8\% \\  & BinConnect  & 10 M & - & **1.3\%** \\  & BNN  & 10 M & - & 1.4\% \\   & FC (4096) & 3.3 M & 1.5 \% & - \\  & FC (2048x3)  & 10 M & **1.3\%** & - \\   

Table 1: Accuracy error on test set for MNIST classification, with float error \(\) and binarized error \(\).

the binarized model. With \(4.5\%\) error compared to \(2.8\%\) for the same number of parameters, this emphasizes the need for improved architecture, better regularization techniques, or exploration of alternative optimization methods.

### Discussion

The state-of-the-art BWNN methods commonly rely on the XNOR operator to emulate multiplications. However, this algebraic framework proves unsuitable for morphological operators, as it contradicts the set-theoretical principles of morphological operations. In our experiments, we observed that the BNN operator  failed to learn even the simplest dilation operator. Furthermore, the performance of the state-of-the-art method on the denoising task was unsatisfactory, with a DICE coefficient of only approximately 0.3, indicating the need for improved approaches in handling morphological operations. In contrast, our findings reveal that the float BiMoNN exhibits enhanced binarization capabilities when trained to closely approximate a set of morphological operators. As a result, the float BiMoNN naturally acquires morphological properties, leading to a more effective subsequent binarization process. However, when applied to the classification of the MNIST dataset, the resulting float BiMoNN does not retain morphological characteristics, causing a noticeable performance degradation after binarization. To address this issue, we emphasize the importance of positive reparametrization and applying morphological regularization. By incorporating these techniques, we significantly improved the model's overall performance and mitigate the accuracy loss upon binarization. This shows the potential of our proposed BiMoNN framework in leveraging morphological properties and offers insights into the development of more effective BiMoNN models for many and diverse tasks.

## 6 Conclusion

In this paper, we have presented a novel, mathematically justified approach to binarize neural networks using the Binary Morphological Neural Network (BiMoNN), achieved by leveraging mathematical morphology. Our proposed method establishes a direct link between deep learning and mathematical morphology, enabling binarization of a wider set of architectures, without performance loss under specific activation conditions and providing an approximate solution when these conditions are not met. Through our experiments, we have demonstrated the effectiveness of our approach in learning morphological operations and achieving high accuracy in denoising tasks, surpassing state-of-the-art techniques that rely on the straight-through estimator (STE). Furthermore, we proposed and evaluated three practical regularization techniques that aid in converging to a morphological network, showcasing their efficacy in a classification task. Additionally, we introduced a fourth regularization technique that, though promising in theory, currently faces computational challenges. We will adress these shortcoming in future work. Despite promising results, there is still room for enhancing both the floating point and binary modes of our network. As well, diverse architectures, such as incorporating convolution layers, could be explored to further improve the performance and applicability of BiMoNN. Overall, our research lays the foundation for advancing the field of binarized neural networks with a morphological perspective, offering valuable insights into developing more powerful and efficient models for a wide range of tasks.