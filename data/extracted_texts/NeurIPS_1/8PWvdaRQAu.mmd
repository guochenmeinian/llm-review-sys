# Contrasting with Symile: Simple Model-Agnostic Representation Learning for Unlimited Modalities

Adriel Saporta

Correspondence to: Adriel Saporta <adriel@nyu.edu>.

Aahlad Puli

Mark Goldstein

Rajesh Ranganath

New York University

###### Abstract

Contrastive learning methods, such as CLIP, leverage naturally paired data--for example, images and their corresponding text captions--to learn general representations that transfer efficiently to downstream tasks. While such approaches are generally applied to two modalities, domains such as robotics, healthcare, and video need to support many types of data at once. We show that the pairwise application of CLIP fails to capture joint information between modalities, thereby limiting the quality of the learned representations. To address this issue, we present Symile, a simple contrastive learning approach that captures higher-order information between any number of modalities. Symile provides a flexible, architecture-agnostic objective for learning modality-specific representations. To develop Symile's objective, we derive a lower bound on total correlation, and show that Symile representations for any set of modalities form a sufficient statistic for predicting the remaining modalities. Symile outperforms pairwise CLIP, even with modalities missing in the data, on cross-modal classification and retrieval across several experiments including on an original multilingual dataset of 33M image, text and audio samples and a clinical dataset of chest X-rays, electrocardiograms, and laboratory measurements. All datasets and code used in this work are publicly available at [https://github.com/rajesh-lab/symile](https://github.com/rajesh-lab/symile).

## 1 Introduction

Contrastive learning leverages naturally paired data to learn general representations that transfer efficiently to downstream tasks . A common contrastive approach is to maximize the mutual information between the paired modalities, ensuring that the learned representations retain sensitivity to all correlations between them. While SimCLR  popularized the use of the mutual information estimator InfoNCE  for data augmentations, CLIP  applied the approach to distinct modalities--for example, images and their corresponding text captions--where representations are learned using any encoder for each modality.

While contrastive approaches are generally applied to two modalities, there is a rapidly expanding range of domains that require the integration of many types of data at once. For example, in robotics, agents combine information from visual, proprioceptive, and tactile sensors ; healthcare providers analyze various types of patient data including imaging, biosignals, and genomics ; and video encompasses RGB frames, audio waveforms, and text transcripts . One strategy for handling multimodal data has been to design specialized architectures capable of processing all data types at once, which limits their general applicability and increases operational complexity . Another common approach is to apply two-modality contrastive objectives, such as CLIP, to pairs of available modalities .

In this paper, we show that, despite its popularity, the pairwise application of CLIP fails to capture higher-order conditional information between modalities, thereby limiting the quality of therepresentations it learns. For instance, given three modalities \(\), \(\), and \(\), pairwise CLIP captures dependencies between \(\) and \(\), \(\) and \(\), and \(\) and \(\), yet cannot capture any conditional dependencies, such as between \(\) and \(\)_given_\(\). We show in Section 2.2 that even in a simple one-dimensional controlled setting where the target \(\) is perfectly predictable from \(\) and \(\), CLIP performs no better than random chance. Effective contrastive learning for more than two modalities requires a model-agnostic approach capable of learning modality-specific representations--like CLIP--yet also captures higher-order information between _any_ number of modalities--unlike CLIP.

Methodological contributions.This paper presents _Symile_, a simple contrastive learning approach that captures higher-order information between any number of modalities. Symile provides a flexible, architecture-agnostic objective for learning modality-specific representations. To develop Symile's objective, we derive a total correlation estimator, employing a generalization of inner products to more than two vectors that allows for the simultaneous contrasting of all modalities and enables zero-shot applications such as classification and retrieval. We then show that the representations produced by Symile for any set of modalities form a sufficient statistic for predicting the remaining modalities not considered in the set. Because it targets total correlation, Symile captures _strictly more_ information than CLIP, guaranteeing performance that matches or surpasses CLIP, except in cases where it known that _only_ pairwise statistics are relevant. Given that such prior knowledge is rarely available, Symile should be favored over CLIP.

Empirical contributions.We demonstrate that Symile outperforms pairwise CLIP on cross-modal classification and retrieval across several experiments including on a multilingual dataset of images, text and audio of over 33M examples and a clinical dataset of chest X-rays, electrocardiograms, and laboratory measurements. We show that Symile retains its advantage over pairwise CLIP even with modalities missing in the data. We publicly release both the multilingual and the clinical datasets, which are specifically designed to test a model's ability to capture higher-order information between three distinct high-dimensional data types.

## 2 Background and motivation

In this section, we first provide background on the original CLIP objective for two modalities, and describe how it has been extended to additional modalities. We then present a simple problem set up for three modalities that illustrates where pairwise contrastive objectives fall short.

### Pairwise contrastive learning

Given a batch of \((,)\) pairs, separately encoded by \(f^{}_{}\) and \(f^{}_{}\), respectively, contrastive objectives such as CLIP maximize the similarity between representations of correctly paired (_positive_) samples and minimize the similarity between representations of incorrectly paired (_negative_) samples.

As is now standard in contrastive learning, in order to construct a batch of data, each modality is treated as the anchor in turn and used to construct a set of positive and negative samples. Letting \(^{+}\) be a temperature parameter, the CLIP objective when \(\) is the anchor modality is the categorical cross-entropy of correctly classifying the positive pair out of \(N\) possible pairs:

\[^{()}(,)=-_{i =1}^{N}f^{}_{}(x_{i})^{} f^{}_{}(y_{i})/}{_{j=1}^{N} f^{}_{}(x_{i})^{}f^{}_{} (y_{j})/}. \]

The final CLIP objective is an average of the losses in each direction: \(^{(,)}_{}(,)= {2}^{()}(,)+^{( )}(,)\). The dot product in Equation (1) serves as a scoring function that is trained to assign high values to positive pairs, which are sampled from the joint distribution \(p_{,}\), and low values to negative pairs, which are sampled from the product of marginals \(p_{}p_{}\).

Contrastive methods are typically designed to maximize the mutual information between \(\) and \(\), which is defined as the Kullback-Leibler divergence from the joint distribution to the product of the marginal distributions: \((;)=D_{}p(,)  p()p()\). It has been shown that Equation (1) maximizes a lower bound on the mutual information between \(\) and \(\)[38; 39]. This information maximization ensures that the learned representations preserve all correlations between the modalities, which is essential for downstream tasks.

Incorporating additional modalities.In order to learn a joint embedding space for more than two modalities, existing work has applied the CLIP objective in a pairwise fashion [1; 2; 9; 11; 14; 21;33, 34, 43, 44, 47, 52]. For example, Guzhov et al.  extend CLIP to incorporate audio alongside image and text, and ImageBind  uses CLIP to align image embeddings with embeddings from five other modalities. In the simplest case, for three modalities, the pairwise CLIP loss corresponds to

\[^{(,,)}_{}(,)=^{(,)}_{}(,)+^{(,)}_{}(,)+^{(,)}_{}(,).\]

CLIP can either be fine-tuned for downstream tasks or operate as a zero-shot classifier by computing the similarities between the _query_ embedding from one modality and each _candidate_ embedding from the other modality. In the case of more than two modalities, this generalizes to a sum across the pairwise similarities. The resulting similarity scores are used to rank the candidates, and the candidate with the highest similarity to the query is chosen .

### A simple one-dimensional problem for three binary modalities

While contrastive objectives were originally designed for two modalities, the naive pairwise extension of CLIP to additional modalities warrants a deeper analysis. To explore this further, we propose a simple problem setup for the following data generating process:

\[,(0.5),=\ \ .\]

Using the pairwise CLIP objective, we fit three affine linear models to perform the zero-shot classification task of predicting whether \(\) is 0 or 1 given \(,\). See Appendix I for additional details.

Even in this simple one-dimensional controlled setting where the target \(\) is perfectly predictable from \(\) and \(\), CLIP performs no better than random chance, with an accuracy of \(0.5\).

CLIP failure analysis.It can be shown that even though the variables \(,,\) are jointly _dependent_--since \(\) is a deterministic function of \(\) and \(\)--they are pairwise independent (Appendix A):

\[(;)=(;)=(;)=0,(;\,|\,)>0.\]

This explains CLIP's poor performance for the above xor experiment: the objective maximizes a lower bound on the mutual information between pairwise terms, and therefore was not designed to capture higher-order dependencies such as the dependence between \(\) and \(\) given \(\).2 Capturing conditional dependencies like this will require the formulation of a new contrastive learning objective.

## 3 Learning Symile representations

Instead of targeting the mutual information between pairs of modalities, we target the _total correlation_ between any number of modalities, learning what we call Symile3 representations.

Total correlation --the higher-order generalization of mutual information--is defined as the Kullback-Leibler divergence from the joint distribution to the product of the marginal distributions:

\[(_{1},,_{M})=D_{}p( _{1},,_{M}) p(_{1}) p( _{M}).\]

In words, total correlation is a symmetric statistical measure that captures the amount of information shared in a set of random variables. A higher total correlation implies more dependency among the variables, and a total correlation of zero indicates that the variables are independent.

Total correlation can be decomposed into a summation of mutual information terms. For example, in the case of three random variables,

\[3(,,)}_{} =(;)+(; ,)+(;)+(;,)+(;)+ (;,)\] \[=2(;)+ (;)+(;)}_{ }+( ;\,|\,)+(;\,|\, )+(;\,|\,)}_{}. \]

While, as discussed, contrastive learning was designed to capture the shared information between modalities, Equation (2) indicates that when there are more than two modalities, the scope of what to capture should extend beyond pairwise information to include conditional interactions (Figure 1).

Because it targets total correlation, Symile captures _strictly more_ information than CLIP, guaranteeing performance that matches or surpasses CLIP, except in cases where only pairwise statistics are relevant, with no higher-order interactions whatsoever. In such cases, Symile may be less sample efficient, as it tracks both pairwise and higher-order information. Unless there is prior knowledge that the downstream task relies _solely_ on pairwise statistics, Symile should be chosen over CLIP.

To illustrate when such higher-order information might be relevant, consider again the xor experiment outlined in Section 2.2. Because all the pairwise information terms between \(\), \(\), and \(\) are zero, the conditional mutual information terms constitute the only dependence between the variables to track.

The xor experiment represents an extreme case where the CLIP target is zero, but most real-world applications will exhibit a combination of both pairwise and higher-order information. For example, in order to diagnose acute pancreatitis, one might consider a patient's clinical history of abdominal pain, elevated levels of digestive enzymes, and imaging results consistent with inflammation. While each of these modalities would provide useful information about the likelihood of pancreatitis (i.e., pairwise information between the modality and the diagnosis is non-zero), none of them alone would be diagnostic of the condition. Similarly, in the case of Parkinson's disease, clinical evaluation provides valuable information, along with imaging and blood tests to rule out other conditions, but clinicians rely on the integration of all modalities.

### Deriving a multi-sample lower bound on total correlation

In order to eventually derive a contrastive objective by maximizing total correlation, we first establish a multi-sample lower bound on total correlation. This lower bound and, in the next section, the Symile objective are illustrated using three modalities for simplicity, but both can be extended to an arbitrary number of modalities, as shown in Appendix B.

Given a batch of \(N\)\((,,)\) triples, let

\[(\{1,,N\}) \]

denote the index of the positive triple in the batch. Our goal is to estimate \((,,)\) given one positive triple sampled from the joint distribution, and \(N-1\) negative triples sampled from the product of marginals:

\[,_{i},_{i} p_{,, }(,_{i},_{i}),, _{j i},_{j i} p()p_{}( _{j i})p_{}(_{j i}).\]

Letting \(_{N}=\{_{n}\}_{n=1}^{N}\) and \(_{N}=\{_{n}\}_{n=1}^{N}\) be the sets of all samples of \(\) and \(\), respectively, this sampling procedure describes the following distribution:

\[p(,_{N},_{N}\,|\,=i)=p() ,\,|\,}(_{i},_ {i}\,|\,)}^{,}p_{}( _{j})][_{j i}p_{}(_{j}) ]}^{,}. \]

We derive the following lower bound in Appendix B:

**Theorem 3.1** (Total Correlation Lower Bound).: _Given the distributions in Equations (3) and (4), for any value \(i\) of \(\) and any scoring function \(g\), a multi-sample contrastive lower bound on total correlation is_

\[(,,) N+}_{ p(,_{N},_{N}\,|\,=i)},_{i},_{i})}{_{j=1}^{N} g(, _{j},_{j})}. \]

As described in Section 2.1, in contrastive learning each modality is sequentially treated as the anchor, with a batch of corresponding positive and negative samples generated for each. Theorem 3.1 treats \(\) as the anchor modality, but by symmetry holds when \(\) or \(\) acts as the anchor modality.

Figure 1: An illustrative comparison of the information captured by CLIP (only pairwise) and Symile (both pairwise and higher-order).

Notice that the term inside the expectation in Equation (5) is the categorical log likelihood of correctly identifying the index of the positive triple in the batch, where the _scoring function_ (or _critic_) \(g\) is trained to assign a high value to positive samples and a low value to negative samples. In Appendix E, we show that the optimal scoring function \(g^{*}\) is equal to the instantaneous total correlation up to additive constants:

**Lemma 3.2**.: _For some \(>0\), the \(g\) that maximizes the lower bound_

\[(,,) N+}_{p(,_{N},_{N}\,|=)},_{i},_{i})}{_{j=1}^{N}  g(,_{j},_{j})}\]

_is_

\[g^{*}(,,)=+, ,}(,,)}{p()p_{ }()p_{}()}.\]

We show in Appendix B.3 that, as \(N\) gets larger, the total correlation lower bound closes for the optimal scoring function \(g^{*}\). This implies a computational-statistical trade-off: a larger batch size demands more computation but results in a tighter bound.

### The Symile objective

We now derive the Symile loss by maximizing the total correlation lower bound in Theorem 3.1.

Instead of using the dot product as a scoring function, as CLIP does, Symile uses its generalized form: the coordinate-wise sum of the element-wise product of a set of vectors. We call this the multilinear inner product (MIP): \(\{_{i}\}_{i=1}^{M}=_{d=1}^{D}_{i=1}^{M}x_{i,d}\). As a scoring function, the MIP strikes a balance between computational simplicity and expressive power: it represents one of the simplest possible generalizations of the dot product to more than two modalities, and the vector multiplication ensures it is expressive enough to model any joint statistic.4

Given a batch of \(N^{}\) positive triples \((_{i},_{i},_{i})\), each with \(N-1\) corresponding negative triples \((_{i},^{}_{j},^{}_{j})\), and letting \(^{+}\) be a temperature parameter, the Symile loss is the negative of an empirical estimate of the expected log likelihood in Equation (5):

\[^{(,)}(,)=\] \[-}_{i=1}^{N^{}}  f^{}_{}(_{i}),f^{}_{}(_{i}),f^{}_{}( _{i})/}{ f^{} _{}(_{i}),f^{}_{}( _{i}),f^{}_{}(_{i})/ +_{j=1}^{N-1} f^{}_{}( _{i}),f^{}_{}(^{}_{j}),f ^{}_{}(^{}_{j})/}. \]

Figure 2: Symile pre-training and zero-shot prediction on the Symile-M3 multilingual dataset. (a) Given a batch of triples, Symile maximizes the multilinear inner product (MIP) of positive triples (in yellow along the diagonal of the cube) and minimizes the MIP of negative triples. (b) The model selects the candidate image with the highest similarity to the query audio and text.

Minimizing Equation (6) optimizes the lower bound on total correlation by maximizing the mip of positive tuples and minimizing the mip of negative tuples (Figure 1(a)). See Appendix B.4 for the Symile objective generalized to any number of modalities.

As is done with CLIP, the final Symile loss is an average of the loss terms where each modality is treated as the anchor in turn:

\[^{(,,)}_{}(, )=^{(,)}( ,)+^{(,)}(,)+^{(,)}(, ).\]

Efficient negative sampling.In the sampling procedure described in Section 3.1, negatives samples for the non-anchor modalities are drawn independently for each positive triple, which can be intensive in terms of both computation and memory. Instead, for efficiency, negative sampling can be approximated within a batch by forming negative tuples from non-matching combinations of the non-anchor modalities.

Approximating negatives within a batch is straightforward with two modalities, but in the case of more than two modalities, both how negatives are formed and how many are used become design choices. At one extreme, one could generate \(N^{2}-1\) negative triples for each positive by considering all possible combinations of the two remaining non-anchor modalities. This approach, which we call \(O(N^{2})\), can be computationally and memory intensive. Instead, any subset of these negatives can be used for sampling. For instance, a more efficient approach, which we refer to as \(O(N)\), involves randomly permuting the non-anchor modalities within the batch, providing each data point with \(N-1\) negatives. The cube in Figure 1(a) illustrates the \(O(N^{2})\) approach and Algorithm 1 presents pseudocode for the \(O(N)\) approach, both for three modalities.

Missing data.The Symile objective is defined for data in which all modalities are observed. However, in practice, datasets often include samples where not all modalities are available. This raises the question: during training how should one incorporate data points for which only a subset of modalities is observed? Symile can be easily adapted to such missingness by adding extra dimensions to the encoder inputs that indicate whether or not a modality is missing, ensuring that missing data points are out-of-support. This approach allows Symile to model dependencies between whichever modalities are observed within a sample. We show in Section 5.2 that Symile retains its advantage over pairwise CLIP even with modalities missing in the data.

### Learning sufficient statistics with Symile

An important property of Symile is that it learns sufficient statistics, which is central to the representations' effectiveness for downstream tasks.

**Theorem 3.3** (Symile Sufficient Statistics).: _Let \(,,\) be three random variables whose optimal representations when trained using Symile are \(f^{*}_{}(),f^{*}_{}(),f^{*}_{}()\), respectively. The element-wise product of any subset of the representations is a sufficient statistic for predicting the remaining random variables._

_For example, \(f^{*}_{}() f^{*}_{}()\) is a sufficient statistic for predicting \(\), which can be expressed using the following conditional independence statement:_

\[}} ,\,|\,f^{*}_{}() f^{*}_{}().\]

The proof can be found in Appendix G. The independence statement in Theorem 3.3 tells us that the element-wise product of the Symile representations of any subset of modalities contains all the information required to predict the remaining modalities. In other words, once Symile representations have been computed, access to the full data is no longer needed. Theorem 3.3 confirms Symile's ability to learn efficient modality-specific representations for downstream tasks.

### Zero-shot prediction using the scoring function

Just as with CLIP, the optimal scoring function \(g^{*}\) (Lemma 3.2) can be used to predict one of the modalities \(y\) using instances of the other modalities \(x,z\). If \(p()\) is uniformly distributed, then the scoring function can be used to rank the candidates for \(\): \(_{y}p(=y\,|\,x,z)=_{y}g^ {*}(x,y,z)\).

However, this zero-shot approach, whether applied to Symile or to CLIP, does not lead to the Bayes optimal prediction and, consequently, does not always yield reliable results when \(p()\) is _not_ uniformly distributed (see Appendix H for a detailed discussion). To address this issue, we can instead compute the desired conditional probability directly using the scoring function:

**Theorem 3.4** (Conditional Distribution using the Scoring Function).: _Let \(,,\) be three random variables whose optimal representations when trained using Symile are \(f^{*}_{}(),f^{*}_{}(),f^{*}_{}()\), respectively. Let the \( f^{*}_{}(),f^{*}_{}( ),f^{*}_{}()\) be the scoring function. Then,_

\[p(\,|\,,)= f^{*}_{ }(),f^{*}_{}(),f^{*}_{}( )p()}{_{} f ^{*}_{}(),f^{*}_{}(),f^{*}_{ }()p()d}. \]

The proof is provided in Appendix H.

If the marginal distribution of \(\) is known, we could then perform zero-shot classification in one of two ways. When the distribution \(p(\,|\,,)\) itself is of interest, as is often the case in healthcare , we could compute \(p(\,|\,,)\) directly, following Equation (7). Alternatively, if only predictions are needed, we could use

\[ f^{*}_{}(),f^{*}_{}(),f^{*}_{ }()+ p()\]

to rank the possible values for \(\), as discussed further in Appendix H. If the marginal distribution of \(\) is _not_ known, then because \(f^{*}_{}() f^{*}_{}()\) is a sufficient statistic for predicting \(\) (Theorem 3.3), we could instead use \(f^{*}_{}() f^{*}_{}()\) to train a simple model to predict any property of \(\), \(s()\): \(p(s()\,|\,f^{*}_{}() f^{*}_{}( ))\).

Note that although the above discussion centers on Symile, it applies equally to CLIP and its own scoring function, the dot product.

## 4 Related work

Contrastive learning beyond two modalities.As discussed, previous work has extended contrastive learning to multiple modalities by applying CLIP to pairs of available modalities. Tian et al.  distinguish between two such pairwise approaches: core view and full graph. The core view strategy fixes one modality and then averages the loss terms between that primary modality and each of the other modalities . ImageBind  exemplifies this approach, using CLIP to align image embeddings with embeddings from five other modalities: text, audio, depth, thermal, and motion sensor data. One advantage of this strategy is that it avoids the need for datasets with all modalities (though each dataset must still align with a primary modality). As discussed in Sections 3.2 and 5.2, Symile representations can be learned even with modalities missing in the data.

The full graph strategy--which we have referred to as pairwise CLIP in this paper--is to consider all \(\) contrastive losses . For example, Guzhov et al.  extend CLIP to include audio with text-to-image, text-to-audio, and image-to-audio losses. While this pairwise strategy captures strictly more information than the one used by ImageBind, neither pairwise approach is able to capture the higher-order information that Symile does.

Pairwise CLIP has also been applied to architecture-specific fusion models that simultaneously process modalities to capture cross-modal interactions . For example, Shvetsova et al.  train a Transformer to accept any number of modalities, using a weighted sum of contrastive losses across all input combinations. Such fusion approaches face a combinatorial explosion not only in the number of weighting coefficients to tune, but also in the number of forward passes required per batch. In contrast, Symile is architecture-agnostic and can learn modality-specific representations.

Targeting higher-order information with contrastive learning.The use of contrastive methods to target higher-order information has been explored primarily within the context of multiple augmentations of the same data. For instance, Bai et al.  derive a total correlation estimator byrecursively decomposing total correlation into a summation of mutual information terms, to which variational estimators are applied (in contrast, Symile optimizes only a single term when targeting total correlation). They then use their estimator to maximize the total correlation between four text augmentations. Shidani et al.  develop a pairwise contrastive approach for image representation learning by generalizing a lower bound on mutual information to one-vs-rest mutual information across multiple augmentations. Liang et al.  maximize the information in two modalities for a specific downstream task by targeting higher-order information.

The relationship between these studies and our work is analogous to that between SimCLR  and CLIP. SimCLR popularized the use of the InfoNCE mutual information estimator for contrastive learning on two data augmentations. Building on this framework, CLIP applied the approach to distinct modalities, where representations are learned separately for each modality using any encoder. Similarly, while existing work leverages total correlation or mutual information estimators for multi-augmentation contrastive learning, to our knowledge only pairwise applications of CLIP have applied such estimators to more than two distinct modalities. Our work parallels the contributions of InfoNCE and CLIP for cases involving more than two modalities: like InfoNCE, we develop a simple estimator that recovers all possible information between any number of modalities, and like CLIP, we show how this estimator can be used to learn modality-specific representations using any encoder.

## 5 Experiments

In this section, we empirically evaluate Symile on cross-modal retrieval tasks in three settings: a synthetic dataset, a multilingual dataset encompassing text, images, and audio, and a clinical dataset with chest X-rays, electrocardiograms, and blood labs. Throughout our experiments, we use pairwise CLIP as a baseline comparison since, as outlined in Section 4, it represents the only architecture-agnostic approach that applies contrastive objectives to more than two modalities. We release all datasets and code used in these experiments at [https://github.com/rajesh-lab/symile](https://github.com/rajesh-lab/symile).

### Synthetic data

Building on the illustrative xor experiment from Section 2, we first test Symile on a synthetic dataset drawn according to the following sampling procedure:

\[a_{j},b_{j}(0.5), i(),  c_{j}=(a_{j}b_{j})^{i} a_{j}^{(1-i)}.\]

We fit three affine linear functions that map \(,,^{5}\) to representations \(},},}^{16}\), respectively, and evaluate the model's ability to correctly predict \(}\) given the pair \((},})\).

Results.Figure 3 (left) compares Symile and CLIP across varying values of \(\). Both models start with a mean accuracy of \(0.032 0.001\) (SE) at \(=0\). As \(\) increases, Symile's accuracy progressively climbs, reaching perfect accuracy at \(=1 0.0\) (SE). In contrast, CLIP's accuracy remains nearly constant, barely surpassing the baseline random guessing rate of \(0.031\) (\(}{{32}}\)).

This performance gap is a consequence of the changing information dynamics between the variables as \(\) moves from 0 to 1, as shown in Figure 3 (right). When \(=0\), \(\) shares no information with \(\) and \(\)--either pairwise or conditionally--rendering both models incapable of predicting \(}\) from \((},})\). As \(\) increases, the higher-order \((;)\) and \((;)\) rise, driving a corresponding improvement in Symile's performance. However, because the pairwise \((;)\) and \((;)\) are always zero, there is no value of \(\) at which CLIP is able to predict \(}\) from \((},})\).

Figure 3: The performance gap between Symile and CLIP on binary synthetic data (left) is a consequence of the changing information dynamics between the variables as \(\) moves from 0 to 1 (right). Mean accuracy is reported across 10 bootstrap samples of the test set.

### Symile-M3: a multilingual dataset

We now evaluate Symile on a new multilingual dataset comprising 33 million (audio, image, text) samples. The dataset, Symile-M3, is specifically designed to test a model's ability to capture higher-order information between three distinct high-dimensional data types: by incorporating multiple languages, we construct a task where text and audio are both needed to predict the image, and where, importantly, neither text nor audio alone would suffice.

Dataset design and model setup.Let \(w\) represent the number of languages in the dataset. An (audio, image, text) sample is generated by first drawing a short one-sentence audio clip from Common Voice  spoken in one of \(w\) languages with equal probability. An image is drawn from ImageNet  that corresponds to one of 1,000 classes with equal probability. Finally, text containing exactly \(w\) words is generated based on the drawn audio and image: one of the \(w\) words in the text is the drawn image class name in the drawn audio language. The remaining \(w-1\) words are randomly chosen from the ImageNet class names and written in one of the \(w\) languages such that there is no overlap in language or class name across the \(w\) words in the text. The words are separated by underscores, and their order is randomized. We release three versions of the dataset: Symile-M3-2, Symile-M3-5, and Symile-M3-10, corresponding to 2, 5, and 10 languages (\(w\)). Figure 3(a) shows an example of the data-generating process for Symile-M3-5. For each of the three datasets, 10M training, 500K validation, and 500K test samples were generated.

We use pre-trained encoders, freezing all parameters except for those in the text encoder's embedding layer and first encoder layer, which are fine-tuned. We train three linear projections to map each encoder's representation to the same 8192-dimensional space. The Symile loss is trained with \(O(N)\) negative sampling. See Appendix 1 for details.

Evaluation and results.We evaluate the learned representations on the zero-shot retrieval task of finding an image of the appropriate class given the audio and text. The most probable image for a given _query_ audio and text pair, selected from all possible _candidate_ images in the test set, is that with the highest similarity score (Figure 1(b)). Symile-M3 was designed to ensure that neither text nor audio alone would suffice to predict the image. Therefore, success on this zero-shot retrieval task hinges on a model's ability to capture joint information between the three modalities.

As shown in Figure 3(b), Symile successfully leverages this joint information, with mean accuracies of \(0.939\), \(0.919\), and \(0.882\) on Symile-M3-2, Symile-M3-5, and Symile-M3-10, respectively, calculated across 10 bootstrap samples of the test set, all with standard error less than \(4.0 10^{-4}\). In contrast, CLIP, which captures pairwise information between image and text, can only predict an image randomly from among the \(w\) class labels present in the text, resulting in mean accuracies of \(0.473\), \(0.187\), and \(0.094\) on Symile-M3-2, Symile-M3-5, and Symile-M3-10, respectively, all with standard error \( 3.01 10^{-4}\). Because CLIP cannot distinguish between the class labels in the text using the audio language, it can only pick a class label at random, bounding its accuracy by \(}{{w}}\).

Missing data.We also train Symile on a variant of Symile-M3-2 where each modality is independently missing with probability \(0.5\) or \(0.65\), corresponding, respectively, to probabilities \(0.125\) and \(0.043\) of a complete data sample in the training set (see Appendix 1 for details). As before, the test set consists of complete triples. As shown in Figure 3(c), even when only \(12.5\%\) of the training data is

Figure 4: (a) Data-generating process for Symile-M3-5. (b) Comparison of Symile and CLIP on the three versions of Symile-M3 (\(w\{2,5,10\}\)). Random chance is \(}{{1000}}\). Symile successfully leverages joint information between the modalities, whereas CLIP is limited to pairwise information, resulting in accuracies bounded by \(}{{w}}\). (c) Symile outperforms the CLIP baseline on Symile-M3-2 across varying levels of completeness in the training data. Both plots report mean accuracy across 10 bootstrap samples of the test set.

complete, Symile achieves a mean accuracy of \(0.906 3.4 10^{-4}\) (SE), far outperforming the CLIP baseline accuracy of \(0.473\), despite the adverse effect of missing modalities. Notably, when less than \(5\%\) of the training data is complete, Symile still exceeds the CLIP baseline.

### Chest X-ray prediction using electrocardiograms and laboratory measurements

Zero-shot retrieval is widely used in the evaluation of representation learning for healthcare . In this section, we evaluate the Symile objective on Symile-MIMIC, a clinical dataset comprised of chest X-rays, electrocardiograms, and blood labs from MIMIC-IV  and MIMIC-CXR . Since ECGs and labs are both safer than CXRs, this experiment explores whether an ECG and labs collected at admission are predictive of a CXR taken shortly thereafter.

Dataset design and model setup.Each data sample includes an ECG reading and blood labs taken within 24 hours of the patient's admission to the hospital, and a CXR taken in the 24- to 72-hour period post-admission (Figure 5a). Our analysis focuses on the 50 most common blood labs, with each sample containing at least one.

We split our dataset (\(11,622\) admissions) into a train/validation development set (95% of patients) and a test set (5% of patients), ensuring there is no patient overlap across the splits. Following previous work, we use the ResNet-50 and ResNet-18 architectures  for the CXR and ECG encoders, respectively, and a three-layer neural network to encode the blood labs. All encoders are trained from scratch, and three linear projections map each encoder's representation to the same 8192-dimensional space. Given the limited size of the dataset, the Symile loss is trained with \(O(N^{2})\) negative sampling to mitigate overfitting. See Appendix I for details.

Evaluation and results.We evaluate the learned representations on the zero-shot retrieval task of finding the most probable _candidate_ CXR for a given _query_ ECG and labs pair according to the similarity score. For each query ECG and labs pair in the test set, we sample nine negative CXR candidates from the remaining test samples, so that that each query has a total of 10 candidates: one positive (the true corresponding CXR) and nine negative.

In Figure 5b, we report mean accuracy for Symile and CLIP over 10 bootstrap samples of the test set. While both models surpass random chance (0.1), Symile achieves an average accuracy of \(0.435 0.007\) (SE), outperforming CLIP's \(0.387 0.003\) (SE). These results correspond to a 12.5% increase in accuracy for Symile over CLIP.

## 6 Conclusion

This work presents Symile, a simple contrastive learning approach that captures higher-order information between any number of modalities. Symile provides a flexible, architecture-agnostic objective for learning modality-specific representations, maintaining the simplicity of CLIP while delivering superior performance, even in cases of missing modalities. Because it targets total correlation, Symile captures strictly more information than CLIP, guaranteeing performance that matches or surpasses CLIP, except in cases where it known that only pairwise statistics are relevant. Given that such prior knowledge is rarely available, Symile should be favored over CLIP.

Future work.(1) The sigmoid-based loss function SigLIP  was recently introduced as a memory-efficient alternative to traditional softmax-based contrastive objectives. A potential avenue for future work would be to adapt Symile, and its use of the multilinear inner product, to this sigmoid loss. (2) The proposed implementation of Symile relies on an approximation for negative sampling, and future work could examine how this approximation scales when applied to settings with more than three modalities. (3) Future work could integrate pre-trained Symile representations into multimodal large language models, enabling them to capture higher-order information between modalities.

Figure 5: (a) Each sample of Symile-MIMIC includes an ECG and blood labs taken within 24 hours of the patient’s admission to the hospital, and a CXR taken in the 24- to 72-hour period post-admission. (b) Retrieval accuracy for identifying the CXR corresponding to a given ECG and labs pair. Results are averaged over 10 bootstrap samples, with error bars indicating standard error.

Acknowledgements

We are especially grateful to Charley Crissman for his invaluable and meticulous feedback on every aspect of the paper, from the proofs to the code. We would like to thank Nick Murphy (Pantograph) and Madeleine Murphy for their thoughtful guidance and indispensable support in preparing the illustrative figures. We thank Wanqian Yang for his helpful suggestions and careful editing of the paper. We also thank Leon A. Gatys, Eran Halperin, Andrew C. Miller, Charles Peyser, Pranav Rajpurkar, Ardavan Saeedi, and Jagadish Venkataraman for engaging in valuable discussions throughout this work. This work was partly supported by the NIH/NHLBI Award R01HL148248, NSF Award 1922658 NRT-HDR: FUTURE Foundations, Translation, and Responsibility for Data Science, NSF CAREER Award 2145542, NSF Award 2404476, ONR N00014-23-1-2634, Apple, and Optum.