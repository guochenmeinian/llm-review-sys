# FIRE: A Dataset for Feedback Integration and Refinement Evaluation of Multimodal Models

Pengxiang Li\({}^{1,2}\) Zhi Gao\({}^{2,3}\)1 Bofei Zhang\({}^{2}\)1 Tao Yuan\({}^{2}\) Yuwei Wu\({}^{1,5}\)1

Mehrtash Harandi\({}^{4}\) Yunde Jia\({}^{5,1}\) Song-Chun Zhu\({}^{2,3,6}\) Qing Li\({}^{2}\)

\({}^{1}\)Beijing Key Laboratory of Intelligent Information Technology,

School of Computer Science & Technology, Beijing Institute of Technology

\({}^{2}\)State Key Laboratory of General Artificial Intelligence, BIGAI

\({}^{3}\)State Key Laboratory of General Artificial Intelligence, Peking University

\({}^{4}\) Department of Electrical and Computer System Engineering, Monash University

\({}^{5}\)Guangdong Laboratory of Machine Perception and Intelligent Computing, Shenzhen MSU-BIT University

\({}^{6}\)Department of Automation, Tsinghua University

###### Abstract

Vision language models (VLMs) have achieved impressive progress in diverse applications, becoming a prevalent research direction. In this paper, we build FIRE, a feedback-refinement dataset, consisting of 1.1M multi-turn conversations that are derived from \(27\) source datasets, empowering VLMs to spontaneously refine their responses based on user feedback across diverse tasks. To scale up the data collection, FIRE is collected in two components: FIRE-100K and FIRE-1M, where FIRE-100K is generated by GPT-4V, and FIRE-1M is freely generated via models trained on FIRE-100K. Then, we build FIRE-Bench, a benchmark to comprehensively evaluate the feedback-refining capability of VLMs, which contains 11K feedback-refinement conversations as the test data, two evaluation settings, and a model to provide feedback for VLMs. We develop the FIRE-LLaVA model by fine-tuning LLaVA on FIRE-100K and FIRE-1M, which shows remarkable feedback-refining capability on FIRE-Bench and outperforms untrained VLMs by \(50\%\), making more efficient user-agent interactions and underscoring the significance of the FIRE dataset.

Figure 1: The comparison of the **feedback-refining** capability among different models. While the original LLaVA hardly improves its responses, our model trained on FIRE can effectively integrate the user feedback and produce much better responses, which are closer to those of GPT-4V.

Introduction

Vision language models (VLMs), such as LLaVA , GPT-4V , and Gemini , have shown impressive instruction-following abilities across various tasks  by integrating large language models (LLMs)  with visual encoders . However, VLMs may sometimes produce undesirable outputs, possibly due to omitting important details in images or misunderstanding the instructions, which prompts the need for the **feedback-refining** capability beyond the normal instruction-following ability. This capability enables VLMs to spontaneously refine their responses based on user feedback, as depicted in Fig. 1, enhancing the efficiency and smoothness of interactions between users and visual assistants.

In this paper, we build FIRE, a dataset for Feedback Integration and Refinement Evaluation of VLMs. FIRE is composed of 1.1M high-quality multi-turn feedback-refinement conversations, derived from \(27\) source datasets across a wide range of tasks, such as visual question answering , image captioning , OCR reasoning , document understanding , math reasoning , and chart analysis . To scale up the data collection, FIRE is collected in two stages. In the first stage, we randomly sample \(\)100K image-instruction-response triplets from data sources. We use each triplet to instruct GPT-4V to simulate a dialogue between a student and a teacher: the student answers the question and the teacher provides feedback to help the student improve its answer. We filter out generated low-quality conversations, such as those with too many turns or no improvement, rendering 100K high-quality feedback-refinement conversations, named FIRE-100K. In the second stage, we fine-tune two LLaVA-NeXT  models on FIRE-100K: one is trained as a student to refine its answer with the feedback, and the other is trained as a teacher to generate feedback for the student's answer. We simulate dialogues between the student and the teacher models using \(\)1M data points from the data sources, rending a split named FIRE-1M. In this case, the full FIRE dataset consists of 1.1M feedback-refinement conversations in two splits FIRE-100K and FIRE-1M.

To comprehensively evaluate the feedback-refining capability of VLMs, we build FIRE-Bench that has 11K feedback-refinement conversations derived from 16 source datasets, including test splits from 8 seen datasets in FIRE-100K and FIRE-1M, as well as 8 unseen datasets from recently-proposed popular multimodal benchmarks. Using FIRE-Bench, we design two evaluation settings: fixed dialogues and free dialogues. In fixed dialogues, we compare the model's refined response with ground truth in the generated conversations in FIRE-Bench, given a fixed dialogue history. In free dialogues, we let the model freely interact with a teacher model about instructions in FIRE-Bench, and test how fast & how much the model can improve its answers based on the feedback provided by the teacher model.

We develop FIRE-LLaVA by fine-tuning LLaVA-NeXT  on FIRE-100K and FIRE-1M. The evaluation results on FIRE-Bench shows that FIRE-LLaVA exhibits significant improvements based on feedback in conversations, exceeding the original LLaVA-NeXT model by \(50\%\). These results underscore the significance of FIRE-100K and FIRE-1M in enhancing feedback integration, while FIRE-Bench provides an evaluation platform to analyze refinements. We expect that FIRE could motivate future exploration of the feedback-refining capability of VLMs.

In summary, our contributions are three-fold. (1) We introduce FIRE, a dataset containing 1.1M feedback-refinement conversations across a wide range of tasks, where 100K data is generated by GPT-4V and 1M data is freely generated by simulating dialogues between tuned open-source models. (2) We introduce the FIRE-Bench benchmark, composed of 11K conversations and a teacher model, providing comprehensive evaluations for the feedback-refining capability in two settings: fixed dialogues and free dialogues. (3) We develop FIRE-LLaVA, an advanced VLM that could improve its responses based on feedback, making efficient interaction between users and VLMs.

## 2 Related Work

### Vision Language Models

Building open-source VLMs to compete with closed-source models like GPT-4V  and Gemini  is a hot research topic. BLIP  and Flamingo  are pioneering models that combine LLMs with visual encoders to enhance cross-modal understanding and reasoning abilities. LLaVA , InstructBLIP , MMICL , and MiniGPT4  develop the instruction tuning ability of VLMs by introducing a large number of instruction-response pairs. Along this way, some work focuses on the visual grounding or editing ability of VLMs , such as Kosmos-2 , SearchVLMs , MINI-GPTv2 , Qwen-VL , and UltraEdit , improving the region understanding for VLMs.

InternVL  and mini-Gemini  develop powerful visual encoders for high-resolution images, and CuMo adopts a mixture-of-experts (MOE) architecture to manage diverse data better. Compared with existing VLMs, our FIRE-LLaVA has a more powerful feedback-refining capability across diverse tasks, which can spontaneously refine responses based on user feedback, leading to efficient and smooth interaction with users.

### Vision-Language Data Generation

Recent attention has increasingly focused on synthesizing vision-language data. The ShareGPT4V dataset  leverages GPT-4V to generate 1.2M image-text pairs with detailed descriptions, making better alignments. LLaVA-Instruct-150K  is a general visual instruction tuning dataset constructed by feeding captions and bounding boxes to GPT-4. After that, many efforts have been made to enhance the data diversity of instruction tuning data. LLaV , MIMIC-IT , and SVIT  further scale up it to 422K, 2.8M, and 4.2M, respectively. InternLM-XComposer  produces interleaved instruction and image data, enabling advanced image-text comprehension and composition. Mini-Gemini  and ALLaVA  use GPT-4V to exploit visual information and generate high-quality instruction data. LRV-Instruction  creates positive and negative instructions for the hallucinating inconsistent issue. A recent work DRESS  collects 66K feedback data and trains VLMs for the feedback-refining capability. Unlike DRESS, which only uses data from LLaVA-Instruct-150K, our feedback-refinement data is from richer sources (27 datasets) across more tasks (math reasoning, chart understanding, and OCR _etc._). Moreover, FIRE has significantly more data than DRESS (1.1M _vs._ 66K), where 1M data is freely produced via dialogues of student and teacher models, leading to significant data expansion but a similar cost of data generation.

### Feedback Learning in Multimodal Models

Learning from feedback is a promising research direction, playing an important role in human-robot interaction . Existing feedback learning methods can be roughly divided into two categories: planned feedback learning and impromptu feedback learning. Planned feedback learning updates models based on user feedback, and thus can generalize to new data but cannot provide refined responses immediately. CLOVA  and Clarify  are representative methods that automatically collect data to learn new knowledge. LLaVA-RLHF  collects human preference and trains VLMs via reinforcement learning. Self-refine shows that LLMs could improve their responses by iteratively refining their outputs based on self-generated feedback. Impromptu feedback learning can immediately refine responses but have less generalization since they usually do not update models, which is widely studied in LLMs . Liao _et al._ use VLMs themselves as verifiers that produce feedback to correct recognition results. VolCaNo  generates data specifically for refinement to address visual hallucinations. DRESS  generates helpfulness, honesty, and harmlessness responses via impromptu feedback learning. Different from DRESS, we improve the correctness and details of responses via impromptu feedback learning across diverse tasks.

Figure 2: Data sources in FIRE. Shaded are new data sources in FIRE-Bench.

## 3 Feedback Integration and Refinement Evaluation (FIRE)

This section presents the FIRE dataset, outlining its task definition, data collection methodology for FIRE-100K and FIRE-1M, and the creation of FIRE-Bench. Finally, we provide an analysis of FIRE.

### Task Definition

**Data Source.** To enhance the diversity and comprehensiveness of our dataset, we compile more than 1.1M image-instruction-response triples from \(27\) source datasets (more details can be found in Appendix B), being used to generate FIRE-100K, FIRE-1M, and FIRE-Bench, as shown in Fig. 2. These datasets cover tasks including visual question answering, image captioning, complex reasoning, OCR, chart/table/document analysis, math problems, science question answering _etc._

**Data format.** We formulate our data as \(\{I,q,gt,\{r_{i},f_{i}\}_{i=1}^{n}\}\), where \(I\) denotes the image, \(q\) is the instruction, \(gt\) is the ground truth answer, and \(\{r_{i},f_{i}\}_{i=1}^{n}\) corresponds to the conversations in \(n\) turns. In the \(i\)-th turn, \(r_{i}\) is the response from VLMs, composed of the thought (how to refine the response based on feedback) and a new answer; \(f_{i}\) is the feedback, involving a score \(a_{i}\) (0-10) for the response \(r_{i}\) and textual comments.

### Fire-100k

We feed images, instructions, ground truth answers from \(18\) datasets, and a designed textual prompt to GPT-4V that generates high-quality feedback-refinement conversations in a one-go manner, as shown in Fig. 3 (a). We ask GPT-4V to play two roles: a student and a teacher, and generate a conversation between the two roles, where the student's responses are improved by incorporating feedback from the teacher. After generation, we filter out low-quality conversations with no score improvements or more than \(6\) turns, since we expect that VLMs could learn to quickly and efficiently improve their responses from our data. Finally, we obtain 100K conversations, shown in Fig. 2(a).

### Fire-1m

We use FIRE-100K to fine-tune LLaVA-NeXT  and obtain two models: FIRE100K-LLaVA and FD-LLaVA, which are used to act as the student and the teacher, respectively (training details are shown in Sec. 4). We sample 1M data from \(18\) source datasets and generate feedback-refinement conversations via the following steps, as shown in Fig. 3 (b). (1) We feed an image and instruction to the student that generates a response. (2) We feed the image, instruction, ground truth answer, and the response to the teacher that generates feedback. If the score \(a\) in the feedback \(a 8\) or the number of turns exceeds \(3\), we stop the conversation; otherwise, we go to step (3). (3) We feed the feedback to the student that generates a refined response and go back to step (2). Finally, we obtain 1M data, shown in Fig. 2(a)

Figure 3: The pipeline to create FIRE-100K and FIRE-1M data.

### FIRE-Bench

To comprehensively evaluate the feedback-refining ability of VLMs, we introduce FIRE-Bench, containing 11K high-quality feedback-refinement conversations. As shown in Fig. 2(c), FIRE-Bench is derived from \(16\) source datasets, including \(8\) seen datasets (test splits) from FIRE-100K and FIRE-1M, as well as \(8\) new datasets from recently-proposed popular multimodal benchmarks, which is used to evaluate the generalization of the feedback-refining ability across different types of tasks. Similar to FIRE-100K, we sample 11K examples from the data sources and prompt GPT-4V to generate the feedback-refinement conversations.

#### 3.4.1 Evaluation Settings

We design two evaluation settings: fixed dialogues and free dialogues to evaluate the performance of the student and teacher models, as shown in Fig. 4.

**Fixed Dialogues.** In fixed dialogues, we evaluate whether the student and teacher models can generate appropriate responses and feedback given the conversation history, and their performance is evaluated by being compared with GPT-4V generated feedback and response, using the BLEU  and CIDEr  metrics to measure the textual alignment. For the predicted score \(}\) in feedback, we regard the score \(a_{i}\) generated by GPT-4V as the ground truth and adopt _mean absolute error (MAE)_: \(MAE=_{k=1}^{K}|a_{k}-}|\), where there are \(K\) test data totally. The teacher model may fail to follow instructions and does not generate a score in feedback for some cases. Here, we simply set \(|a_{i}-_{i}|=10\) for these cases.

**Free Dialogues.** We use a student model and a teacher model to perform free dialogues and evaluate how fast and how much the student model can improve its answers based on the feedback from the teacher model. The stopping condition for dialogues is that the obtained scores from the teacher model do not increase or exceed a pre-defined threshold (we set \(8\) in experiments).

We introduce four metrics: average turn (AT), average dialogue refinement (ADR), average turn refinement (ATR), and refinement ratio (RR) for free dialogues.

(1) _Average Turn (AT)_. The AT metric evaluates how fast a VLM could achieve a satisfactory result based on feedback. We measure the number of turns \(n_{k}\) in the conversation to solve the \(k\)-th data, where VLMs refine their responses until the obtained score exceeds the pre-defined threshold. We set a punishment number as \(p=10\), the maximum number of turns as \(n_{max}=5\). If VLMs fail to obtain a satisfactory score in \(n_{max}\) turns, then \(n_{k}=p\). For clearer comparisons with the baseline model (_e.g._, the original LLaVA-NeXT model), we normalize it according to the AT of the baseline model,

\[AT=_{k=1}^{K}n_{k}/T_{baseline}, \]

where \(T_{baseline}\) is the average turn of the baseline model. A smaller value of AT means better performance.

(2) _Average Dialogue Refinement (ADR)_. The ADR metric evaluates how much knowledge VLMs could learn from feedback in a dialogue. In solving the \(k\)-th data, we use \(a_{k,1}\) to denote the obtained score for the initial response and use \(a_{k,n_{k}}\) to denote the obtained score for the response in the final turn. ADR averages the score improvements of each conversation as

Figure 4: We use two settings to evaluate student and teacher models.

\[ADR=_{k=1}^{K}a_{k,n_{k}}-a_{k,1}. \]

A larger value of ADR means better performance.

(3) _Average Turn Refinement (ATR)_. ATR evaluates how much knowledge VLMs could learn from feedback in one turn. ATR averages the score improvements in each turn of \(K\) samples as

\[ATR=_{k=1}^{K}-1}(a_{k,n_{k}}-a_{k,1}). \]

A larger value of ATR means better performance.

(4) _Refinement Ratio (RR)_. RR measures the proportion of data that have a wrong initial response and a correct final response (_i.e._, how much data are corrected based on feedback), computed by

\[RR=_{k=1}^{K}_{a_{k,n_{k}} 8}-_{a_{k,1}  8}, \]

where \(_{a_{k,n_{k}} 8}\) means if \(a_{k,n_{k}} 8\), \(_{a_{k,n_{k}} 8}=1\), and \(0\) otherwise. A larger value of RR means better performance. Note that, for the \(k\)-th sample, if \(n_{k}=1\), we remove it from the K samples to compute AT, ADR, ATR, and RR.

### Dataset Analysis

We provide three key statistics: score, turn, and length, for the collected feedback-refinement conversations. **Score.** We show the distribution of initial scores in Fig. 5(a), which reflects the starting state of the conversation. They mainly fall in the interval \(\), showing that FIRE covers diverse starting states of conversations. Improved scores per turn are shown in Fig. 5(b), which reflects the learning effect. It ranges from \(\), similar to actual situations, where high improvements are obtained in easy cases and small improvements are obtained in hard cases, showcasing the diversity of data. Improved scores per dialogue are shown in Fig. 5(c), and the improvements in most cases are 5-7, demonstrating the data quality of FIRE, where most data have obvious improvements, helping VLMs to efficiently learn to improve their responses. The score distributions of FIRE-100K, FIRE-1M, and FIRE Bench are not completely consistent, making the data more diverse. **Turn.** The turn distribution of conversations is shown in Fig. 5(d). Most conversations have 2-4 turns, indicating an efficient and concise feedback process. This measure suggests that most conversations reach a satisfactory level of refinements. A small number of turns in FIRE informs VLMs to perform effective dialogues. **Length.** The length distributions of responses and feedback are shown in Fig. 5(e) and Fig. 5(f), respectively. Most responses or feedback are less than \(100\) words. It shows concise dialogues in FIRE, aligning with real-world scenarios where users typically engage in brief exchanges rather than lengthy discussions.

## 4 Model

Our model architecture has the same design as LLaVA-NeXT-8B  that uses CLIP  as a frozen image encoder with a two-layer multi-layer perceptron vision-language connector. For the LLM part, we use the same architecture as the LLaMA3-8B . We use LLaVA-NeXT-8B to initialize the VLMs and use LoRA to fine-tune the LLaVA-NeXT-8B for a student model and a teacher model.

### Student Model

Given an \(n\)-turn conversation \(\{I,q,gt,\{r_{i},f_{i}\}_{i=1}^{n}\}\), we train a student model to fit responses \(r_{i}\) for \(i 2\) using the cross-entropy loss,

\[_{(I,q,gt,\{r_{i},f_{i}\}_{i=1}^{n})}[-_{ i=2}^{n} P(r_{i}|I,q,\{r_{j},f_{j}\}_{j=1}^{j=i-1})], \]

where \(\) is the used dataset. We first use FIRE-100K as \(\) to train a student model FIRE100K-LLaVA, then use all training data (FIRE-100K and FIRE-1M) to train a final student model FIRE-LLaVA.

### Teacher Model

Given a \(n\)-turn conversation \(\{I,q,gt,\{r_{i},f_{i}\}_{i=1}^{n}\}\), we train a teacher model to fit the feedback \(f_{i}\) for \(i 1\) using the cross-entropy loss,

\[_{(I,q,gt;\{r_{i},f_{i}\}_{i=1}^{n})}[-_{i =1}^{n} P(f_{i}|I,q,gt,\{r_{j},f_{j}\}_{j=1}^{j=i-1},r_{i})], \]

where we use FIRE-100K as \(\) and obtain the teacher model FD-LLaVA.

## 5 Experiments

We conduct experiments to evaluate both the student and teacher models trained on FIRE. We first provide experimental details and then comprehensively evaluate models in multiple settings.

### Experimental Details

**Training Data.** To avoid the catastrophic forgetting issue, we combine the training data in FIRE with the LLaVA-665K  (released by Open-LLaVA-1M ) to train the student and teacher models.

**Training Details.** In the training process of both the student and teacher models, we freeze the image encoder and the image-language connector, and fine-tune the language decoder using LoRA . In the implementation of LoRA, we set the rank as \(64\) and only apply LoRA on the query and key projection matrices in all attention layers of the language decoder. This setting only involves \(0.4\%\) parameters of LLaMA3-8B. We use the AdamW optimizer, where a cosine annealing scheduler is employed, the learning rate is \(2e-4\), the batch size is \(64\), and we train \(1\) epoch over all data. The training process for a student (or teacher) model requires about 128 A100-80GB GPU hours.

   Method & GQA V0Av2 & VizWiz & TextVQA & SQA\({}^{I}\) & LLaVA\({}^{W}\) & MMB & MME\({}^{P}\) & MME\({}^{C}\) & MM-Vet \\  LLaVA-NeXT-8B & **65.9** & 79.0 & 52.0 & **69.8** & **77.3** & 78.5 & 74.4 & **1546.0** & **331.4** & 44.9 \\ FIRE-LLaVA & 65.8 & **82.9** & **59.8** & 68.4 & 76.8 & **81.5** & **78.5** & 1534.8 & 321.1 & **45.3** \\   

Table 1: Comparisons between LLaVA-NeXT-8B and FIRE100K-LLaVA on 10 benchmarks. Benchmark names are abbreviated for space limits. GQA ; VQAv2 ;VizWiz ; TextVQA ; SQA\({}^{I}\):ScienceQA-IMG ; LLaVA\({}^{W}\): LLaVA-Bench-in-the-wild ;MMB: MMBench ; MMB\({}^{P}\): MME Perception ; MME\({}^{C}\): MME Cognition ; MM-Vet .

   Model & BLEU-1 (\(\)) & BLEU-2 (\(\)) & BLEU-3 (\(\)) & BLEU-4 (\(\)) & CIDEr (\(\)) \\  LLaVA-NeXT-8B & 0.33 & 0.23 & 0.17 & 0.13 & 0.60 \\ FIRE-LLaVA & **0.54** & **0.46** & **0.39** & **0.34** & **2.36** \\   

Table 2: Results of the student model in fixed dialogues.

Figure 5: Data statistics on FIRE-100K, FIRE-1M, FIRE-Bench.

### Evaluation in Instruction Following

Considering that fine-tuning VLMs may encounter the catastrophic forgetting problem, we evaluate the instruction-following ability of FIRE-LLaVA, using \(10\) commonly used multimodal benchmarks, as shown in Tab. 1. Our model achieves comparable performance to the original LLaVA-NeXT-8B model, showing that we do not compromise the instruction-following ability when learning the feedback-refining ability.

### Evaluation in Fixed Dialogues

We evaluate the performance of FIRE-LLaVA, and FD-LLaVA in fixed dialogues. The evaluation of FIRE-LLaVA is shown in Tab. 2, where we report the results of BLEU-1, BLEU-2, BLEU-3, BLEU-4, and CIDEr. The performance of FD-LLaVA is shown in Tab. 3, where we report the results of BLEU-1, BLEU-2, BLEU-3, BLEU-4, CIDEr, and MAE. We observe that using FIRE, FIRE-LLaVA and FD-LLaVA generates good responses and feedback, having better performance than the original LLaVA-NeXT-8B model on all metrics. FIRE-LLaVA could well refine the responses, like GPT-4V. FD-LLaVA can generate more accurate feedback, including comments (see BLEU and CIDEr) and scores (see MAE), demonstrating the effectiveness of our teacher model FD-LLaVA that can discover undesirable responses.

### Evaluation in Free Dialogues

We employ a student model and a teacher model to perform free dialogues. We evaluate LLaVA-NeXT-8B, FIRE100K-LLaVA, and FIRE-LLaVA as the student model, and use FD-LLaVA to act as the teacher model. We report the average turn (AT), average dialogue refinement (ADR), average turn refinement (ATR), and refinement ratio (RR) on FIRE-Bench. Results are shown in Tab. 4. We observe that a LLaVA model trained on FIRE has improved feedback-refining ability. On the ADR, ATR, and RR metrics, FIRE-LLaVA achieves more than \(50\%\) improvements by LLaVA-NeXT, making an efficient user-agent interaction. Meanwhile, adding FIRE-1M to training data has better performance than only using FIRE-100K, showing the data quality of FIRE-1M.

We also show the detailed results on \(8\) seen source datasets and \(8\) new source datasets, as shown in Tab. 5 and Tab. 6, respectively. Our models achieve improvements on both seen and new datasets, showing the generalization of feedback-refining ability across different types of data and tasks.

   Model & AT (\(\)) & ADR (\(\)) & ATR (\(\)) & RR (\(\)) \\  LLaVA-NeXT-8B & 1 & 0.97 & 0.41 & 0.25 \\ FIRE100K-LLaVA-8B & 0.92 & 1.27 & 0.55 & 0.34 \\ FIRE-LLaVA-8B & **0.84** & **1.56** & **0.66** & **0.39** \\   

Table 4: Results in free dialogues overall test data in FIRE.

   Model & BLEU-1 (\(\)) & BLEU-2 (\(\)) & BLEU-3 (\(\)) & BLEU-4 (\(\)) & CIDEr (\(\)) & MAE (\(\)) \\  LLaVA-NeXT-8B & 0.34 & 0.21 & 0.15 & 0.10 & 0.51 & 1.88 \\ FD-LLaVA & **0.55** & **0.45** & **0.39** & **0.33** & **2.27** & **0.30** \\   

Table 3: Results of the teacher model in fixed dialogues.

    &  &  &  &  \\   & AT & ADR & ATR & RR & AT & ADR & ATR & RR & AT & ADR & ATR & RR & AT & ADR & ATR & RR \\  LLaVA-NeXT & 1.00 & 1.45 & 0.42 & 0.40 & 1.00 & 1.51 & 0.51 & 0.43 & 1.00 & 0.91 & 0.34 & 0.26 & 1.00 & 0.71 & 0.39 & 0.25 \\ FIRE100K-LLaVA & 0.86 & 1.83 & 0.55 & 0.54 & 0.81 & 1.93 & 0.63 & 0.58 & 0.95 & 1.20 & 0.49 & 0.33 & 1.07 & 1.03 & **0.56** & 0.27 \\ FIRE-LLaVA & **0.78** & **2.08** & **0.59** & **0.56** & **0.81** & **2.06** & **0.70** & **0.58** & **0.77** & **1.51** & **0.56** & **0.42** & **0.79** & **1.15** & 0.53 & **0.36** \\    &  &  &  &  \\   & AT & ADR & ATR & RR & AT & ADR & ATR & RR & AT & ADR & ATR & RR & AT & ADR & ATR & RR \\  LLaVA-NeXT & 1.00 & 0.97 & 0.56 & 0.24 & 1.00 & 1.66 & **0.50** & 0.42 & 1.00 & 0.14 & 0.07 & 0.08 & 1.00 & 0.14 & 0.05 & 0.04 \\ FIRE100K-LLaVA & 1.06 & 0.84 & 0.51 & 0.22 & 0.79 & 1.87 & 0.46 & **0.51** & **0.84** & 0.70 & 0.33 & **0.28** & **0.93** & 0.18 & 0.07 & **0.08** \\ FIRE-LLaVA & **0.81** & **1.65** & **0.97** & **0.41** & **0.74** & **1.97** & 0.46 & 0.50 & **0.84** & **0.74** & **0.35** & 0.27 & 0.95 & **0.19** & **0.08** & 0.06 \\   

Table 5: Detailed test results (AT (\(\)), ADR (\(\)), ATR (\(\)), and RR (\(\))) on 8 seen source datasets.

### Ablation Studies

In Fig. 6, we evaluate the feedback-refining ability of VLMs using different amounts of training data from the FIRE dataset. Concretely, we first use the FIRE-100K data. Then, we gradually sample data from FIRE-1M, varying from 200K to 1000K, combined with FIRE-100K to train the LLaVA-NEXT-8B model. Overall, the results indicate that more training data leads to better performance across all evaluated metrics. The substantial improvements, particularly with the initial 100K dialogues and the noted enhancement at around 700K dialogues, demonstrate the high quality of the FIRE dataset and the model's emergent capabilities with more training data.

    &  &  &  &  \\   & AT & ADR & ATR & RR & AT & ADR & ATR & RR & AT & ADR & ATR & RR & AT & ADR & ATR & RR \\  LLaVA-NeXT & 1.00 & 0.84 & 0.45 & 0.19 & 1.00 & 0.14 & 0.13 & 0.08 & 1.00 & 0.94 & 0.53 & 0.22 & 1.00 & 1.31 & 0.31 & 0.21 \\ FIRE100K-LLaVA & 0.89 & 1.09 & 0.68 & 0.29 & 0.95 & 0.34 & 0.30 & 0.16 & 0.86 & 1.38 & 0.81 & 0.38 & **0.95** & **2.20** & **0.60** & **0.39** \\ FIRE-LLaVA & **0.83** & **1.36** & **0.77** & **0.34** & **0.93** & **0.65** & **0.49** & **0.17** & **0.80** & **1.73** & **1.05** & **0.41** & 0.96 & 2.04 & 0.57 & 0.36 \\   &  &  &  &  \\   & AT & ADR & ATR & RR & AT & ADR & ATR & RR & AT & ADR & ATR & RR & AT & ADR & ATR & RR \\  LLaVA-NeXT & 1.00 & 0.80 & 0.31 & 0.13 & 1.00 & 2.30 & 0.56 & 0.48 & 1.00 & 2.81 & 0.70 & 0.56 & 1.00 & 0.45 & 0.19 & 0.03 \\ FIRE100K-LLaVA & 0.97 & 0.99 & 0.48 & 0.23 & 0.83 & 3.18 & 0.75 & 0.68 & 0.98 & 2.95 & 0.78 & 0.62 & 0.99 & 0.79 & 0.33 & **0.12** \\ FIRE-LLaVA & **0.87** & **1.18** & **0.60** & **0.26** & **0.81** & **3.34** & **0.84** & **0.69** & **0.83** & **3.94** & **1.08** & **0.78** & **0.96** & **0.85** & **0.50** & **0.12** \\   

Table 6: Detailed test results (AT (\(\)), ADR (\(\)), ATR (\(\)), and RR (\(\))) on 8 new source datasets.

Figure 6: Impact of training set size on model performance.

Figure 7: Accuracy improvement with more dialogue turns.

In Fig. 7, we present the performance curve in FIRE-Bench concerning the number of turns in dialogues, evaluating LLaVA-NeXT, FIRE100K-LLaVA, and FIRE-LLaVA. We report the percentage of correctly answered samples (those scores greater than 8) after each turn. As the number of turns increases, the percentage of correctly answered samples rises across all three models. \(46.57\%\) and \(46.77\%\) of the test data is correctly answered in the first turn, for the LLaVA-NeXT model and FIRE100K-LLaVA respectively. For FIRE-LLaVA, \(49.60\%\) of the data is correctly answered in the first turn, and this increases to \(69.19\%\) after five turns, with \(19.59\%\) of the samples being corrected based on feedback. Compared to the LLaVA-NeXT model, FIRE-LLaVA shows an additional \(6.79\%\) improvement (from \(49.60\% 69.19\%\)_vs._\(46.57\% 59.37\%\)), highlighting the effectiveness of FIRE-LLaVA when trained on FIRE.

### Visualization

In Fig. 8, we visualize three cases in free dialogues using FIRE-LLaVA and FD-LLaVA. We observe that FIRE-LLaVA can understand the intent behind the feedback and refines its responses according to the feedback on chart understanding, visual concept perception, visual relationship reasoning, and OCR, making efficient user-agent interactions.

## 6 Conclusion

In this paper, we have presented FIRE, a feedback-refinement dataset with 1.1M multi-turn conversations, which empowers VLMs to refine their responses based on given feedback. Given proper prompts, GPT-4V can produce high-quantity conversations with feedback and responses. Using the 100K GPT-4V generated data as seeds, a student model and a teacher model can freely expand the feedback-refinement data to 1.1M with a similar data quality to GPT-4V. Experiments show that VLMs trained on FIRE have significant improvements in their feedback-refining ability.

**Limitation.** In the current FIRE dataset, the feedback data is limited in the textual form. Practical feedback usually involves diverse multimodal information, such as pointing out image regions. We will further expand FIRE with multimodal feedback data. In addition, although we use a filter process to remove low-quality data, we still cannot completely guarantee the quality of the data. In the future, we will combine human verification with machine verification to improve the quality.

**Acknowledgements.** This work was partly supported by the National Science and Technology Major Project (2022ZD0114900). This work was partly supported by the Natural Science Foundation of China (NSFC) under Grants No. 62176021 and No. 62172041, the Natural Science Foundation of Shenzhen under Grant No. JCYJ20230807142703006, and the Key Research Platforms and Projects of the Guangdong Provincial Department of Education under Grant No.2023ZDZX1034. Mehrtash Harandi is supported by funding from the Australian Research Council Discovery Program DP230101176.

Figure 8: Case study of the feedback-refining ability in our model.