# On the Impact of Feature Heterophily on Link Prediction with Graph Neural Networks

Jiong Zhu

University of Michigan

jiongzhu@umich.edu

Equal contribution.

Gaotang Li

University of Illinois

Urbana-Champaign

gaotang3@illinois.edu

&Yao-An Yang

University of Michigan

ayayang@umich.edu

&Jing Zhu

University of Michigan

jingzhuu@umich.edu

&Xuehao Cui

University of Michigan

credus@umich.edu

&Danai Koutra

University of Michigan

dkoutra@umich.edu

###### Abstract

Heterophily, or the tendency of connected nodes in networks to have different class labels or dissimilar features, has been identified as challenging for many Graph Neural Network (GNN) models. While the challenges of applying GNNs for node classification when _class labels_ display strong heterophily are well understood, it is unclear how heterophily affects GNN performance in other important graph learning tasks where class labels are not available. In this work, we focus on the link prediction task and systematically analyze the impact of heterophily in _node features_ on GNN performance. We first introduce formal definitions of homophilic and heterophilic link prediction tasks, and present a theoretical framework that highlights the different optimizations needed for the respective tasks. We then analyze how different link prediction encoders and decoders adapt to varying levels of feature homophily and introduce designs for improved performance. Based on our definitions, we identify and analyze six real-world benchmarks spanning from homophilic to heterophilic link prediction settings, with graphs containing up to 30M edges. Our empirical analysis on a variety of synthetic and real-world datasets confirms our theoretical insights and highlights the importance of adopting learnable decoders and GNN encoders with ego- and neighbor-embedding separation in message passing for link prediction tasks beyond homophily.

## 1 Introduction

Graph-structured data are powerful and widely used in the real world, representing relationships beyond those in Euclidean data through links. Link prediction, which aims to predict missing edges in a graph, is an important task with applications spanning from recommendation systems  to knowledge graphs , and social networks . Traditional algorithms for link prediction are heuristic-based and impose strong assumptions on the link generation process. To alleviate the reliance on handcrafted features, recent link prediction approaches are transformed by Graph Neural Networks (GNNs), which can effectively learn node representations in an end-to-end fashion. Vanilla GNN for link prediction (GNN4LP) methods keep the original GNN model for encoding node embeddings, followed by a decoder acting on pairwise node embeddings, _e.g._ dot product . However, these methods are not effective at capturing pairwise structural proximity information , _i.e._ neighborhood heuristics such as the number of common neighbors. To further enhance modelcapabilities, the current _state-of-the-art_ GNN4LP approaches augment GNNs by incorporating pairwise structural information [6; 52; 51; 49; 43].

Nevertheless, as the core of today's _SOTA_ approaches, GNNs rely on the _message-passing_ mechanism which works well when the underlying data exhibit homophily, _i.e._ connected nodes tend to share similar attributes with each other. Such inductive bias has been widely analyzed and has been shown to be an important factor for GNN's superior performance in the task of node classification on homophilic graphs [55; 14; 28; 29]. It has also been widely observed that GNN's performance degrades on heterophilic graphs in node classification tasks, where connected nodes tend to have different labels [1; 58; 60; 34; 17; 27]. In contrast, there are merely works focusing on the problem of heterophily in link prediction tasks: almost all existing definitions of homophily rely on the node class labels [25; 35], which are often not available for the link prediction tasks. Furthermore, prior works on GNN4LP have largely focused on the effects of pairwise structural information to link prediction performance, while there is no dedicated work focusing on the effects of feature heterophily. In light of this, this work aims to characterize the notion of heterophily in the link prediction problem, understand the effects of heterophily and feature similarity in existing models that leverage node features, and explore designs to improve the use of dissimilar features in GNN link prediction. We detail our contributions as follows:

* **Definitions of Non-homophilic Link Prediction**: We introduce formal definitions of homophilic and non-homophilic link prediction tasks: instead of relying on the magnitude of feature similarity, our definitions are based on the separation of feature similarity scores between edges and non-edges, which is justified by a concise theoretical framework that highlights the different optimizations needed for the respective tasks.
* **Designs Empowering GNNs for Non-homophilic Link Prediction**: We identify designs for GNN encoders and link probability decoders that improve performance for non-homophilic link prediction settings and show that (1) decoders with sufficient complexity are required for capturing non-homophilic feature correlations between connected nodes; (2) ego- and neighbor-embedding separation in GNN message passing improves their adaptability to feature similarity variations.
* **Benchmarks for Non-homophilic Link Prediction**: We introduce six real-world datasets spanning from homophilic to heterophilic settings for link prediction, with edge counts ranging from 88K to over 30M, to benchmark the performance of various GNN models. These datasets come from diverse domains and exhibit different levels of feature similarity, providing a robust foundation for evaluating GNN adaptability to non-homophilic conditions in link prediction tasks.
* **Empirical Analysis on Impacts of Feature Heterophily**: We conduct comprehensive empirical analyses using both synthetic and real-world graphs. Through synthetic graphs with controlled feature similarity levels, we analyze how link prediction methods with different designs adapt to varying levels of heterophily. On real-world graphs, we evaluate both overall performance and local behavior across edges with different node degrees and feature similarities, revealing important insights about model adaptability in practice.

## 2 Related Work

**Graph Neural Networks for Link Prediction.** Traditional algorithms for link prediction are primarily _heuristic-based_, which have strong assumptions on the link generation process. These approaches compute the similarity scores between two nodes based on certain structures or properties [2; 3; 5; 51]. Later, various _representation-based_ algorithms for link prediction were proposed, which aim at learning low-dimensional node embeddings that are used to predict the likelihood of link existence between certain node pairs and usually involve the use of GNNs [21; 15; 42]. Compared with the _heuristic-based_ algorithms, _representation-based_ algorithms do not require strong assumptions and perform learning over the graph structure and node features in a unified way. A representative method is the Variational Graph Autoencoder , which uses GCN as the encoder for learning node representations and inner product as the decoder for pairwise link existence predictions. More recently, the state-of-the-art methods for link prediction are built on top of the _representation-based_ algorithms and augment them with additional _pairwise_ information. For instance, _subgraph-based_ approaches perform link prediction between two nodes by first extracting their enclosing subgraphs and subsequently applying the standard _representation-based_ algorithms on the extracted subgraphs [52; 6; 62]. Concurrently, other works have been proposed to augment GNN learning with common neighbor information [43; 49; 43]. Despite the tremendous success, prior link prediction works mainly assume homophily, where node pairs with similar features or neighbors are more likely to link together. In contrast, this work considers a more general setting where there may be a spectrum of low-to-high homophily in the underlying data, characterizes a notion of heterophily in link prediction and explores how popular approaches perform under this characterization. Related work also examines the influence of data for link prediction from a joint perspective of graph structure and feature proximity [30; 22]. While these works provide valuable insights into the interplay between structural and feature information, our work specifically focuses on characterizing and analyzing the impact of node feature through the lens of heterophily, offering complementary insights into how varying degrees of feature similarity affect link prediction performance.

**Graph Neural Networks Addressing Heterophily.** There is a rich literature on graph neural networks addressing heterophily, but most of them tackle the node classification task [1; 58; 60; 34; 17; 27]. Very few works focus on the problem of link prediction under heterophily. Among them, Zhou et al.  propose to disentangle the node representations from latent dissimilar factors, and Di Francesco et al.  extend the physics-inspired GRAFF , originally designed for handling heterophilic node classification tasks, to heterophilic link prediction tasks. These works use the same _class-dependent_ homophily measure as typically used in node classification, while our work emphasizes the influence of features in link prediction and more systematically benchmarks model performance against different homophily levels.

## 3 Notation and Preliminaries

Let \(=(,)\) be an undirected and unweighted homogeneous graph with node set \(\) and edge set \(\). We denote the 1-hop (immediate) neighborhood centered around \(v\) as \(N(v)\) (\(\) may have self-loops), and the corresponding neighborhood that does _not_ include the ego (node \(v\)) as \((v)\). We represent the graph by its adjacency matrix \(\{0,1\}^{n n}\) and its node feature set as \(\) with matrix form \(^{n F}\), where the vector \(_{v}\) corresponds to the _ego-feature_ of node \(v\), and \(\{_{u}:u(v)\}\) to its _neighbor-features_. We further represent the degree of a node \(v\) by \(d_{v}\), which denotes the number of neighbors in its immediate neighborhood \((v)\).

**Graph Neural Networks for Link Prediction**. Following [23; 53], we define the task of link prediction to be estimating the likelihood of reconstructing the actual adjacency matrix. Formally,

\[_{i,j}=}_{i,j}=p(i,j|,), \]

where \(_{i,j}\) or \(}_{i,j}\) is the predicted link probability between nodes \((i,j)\) and was traditionally calculated by heuristics-based algorithms. For \((i,j)\) in the training set, we set the ground truth probability \(y_{i,j}=_{i,j}\). Existing GNN4LP approaches typically use a GNN-based method for encoding node representations (denoted by ENC) and some decoder function (denoted by DEC) between node embedding pairs:

\[_{i,j}=}_{i,j}=(z_{i},z_{j}),\;\;z _{i}=(i,,),z_{j}=(j,, ). \]

The original graph autoencoder approach  uses a two-layer GCN  as the encoder and a dot product as the decoder. There are many different choices of encoders that do not need to strictly follow the original GNN architecture. For the decoder, while dot product remains a popular simple choice [20; 33], more expressive alternatives include concatenation followed by an MLP.

**Graph Feature Similarity**. We measure the "graph feature similarity" through averaging the _mean-centered_ feature similarity from connected node pairs.

**Definition 1** (Node Feature Similarity): _For a node pair \((u,v)\) with \(_{u}\) and \(_{v}\) as the node features and a similarity function \((,)\), we define the node feature similarity as \(k(u,v)=(_{u},_{v})\)._

In this work, we set \((_{u},_{v})=}_{u}}_{v}}{\|}_{u}\|\|}_{v}\|}\) to be the _mean-centered_ cosine similarity of node features, where we denote the mean feature vector of all nodes in the graph as \(}=|}_{v}_{v}\), and the mean-centered node feature for node \(v\) as \(}_{v}=_{v}-}\). Empirically, we find that the mean-centering operation is crucial for accurate characterization of the pairwise feature similarity and its impact on link prediction performance. We further define the graph feature similarity as follows.

**Definition 2** (Graph Feature Similarity): _We measure the graph feature similarity \(K\) through averaging the feature similarity of all its connected nodes pairs: \(K=_{(u,v)}|}\)._

Unlike the homophily measures defined on node class labels which are non-negative , the feature similarity \(k(u,v)[-1,1]\) can additionally be _negative_, indicating negative correlations. We refer to the graph as _positively correlated_ if \(K>0\) and _negatively correlated_ if \(K<0\).

## 4 Homophilic & Heterophilic Link Prediction

While existing works on node classification usually define homophilic or heterophilic graphs by whether the majority of connected nodes share the same class labels [25; 35], the class information is _not_ available for link prediction. Instead, we argue that the homophilic and heterophilic link prediction tasks should be defined based on how the distributions of feature similarity scores between connected and unconnected nodes are separated, as these definitions capture the fundamental differences on how link prediction scores are correlated with feature similarity scores. Within the category of homophilic or heterophilic tasks, we further show that the variation of the positive feature similarity scores affects the rate of change for the link prediction scores. We present definitions with intuitive examples in SS4.1 and theoretical analysis in SS4.2.

### Categorizing Link Prediction on Distributions of Feature Similarity

We begin our discussion by considering the distributions of feature similarity scores for a random positive (edge) and negative (non-edge) node pair in the graph: consider the set of feature similarity scores for positive (edge) node pairs in the graph as \(_{pos}\), and negative (non-edge) node pairs as \(_{neg}\). We first formalize different categories of link prediction tasks, which are defined by how the distributions of \(_{pos}\) and \(_{neg}\) are (approximately) separated:

**Definition 3** (Homophilic Link Prediction): _The task is homophilic if \(M\) exists such that for most samples \(}_{pos}_{pos}\) and \(}_{neg}_{neg}\) it satisfies \((}_{neg})<M(}_{pos})\)._

Prior works have mostly focused on the homophilic category for the link prediction problem while overlooking other possibilities. For other cases where the homophilic conditions are not satisfied, we refer to them generally as _non-homophilic_ link prediction problems. In the definition below, we formalize an easy type of non-homophilic link prediction problem:

**Definition 4** (Heterophilic Link Prediction): _The task is heterophilic if \(M\) exists such that for most samples \(}_{pos}_{pos}\) and \(}_{neg}_{neg}\) it satisfies \((}_{pos}) M<(}_{neg})\)._

We give intuitive examples of homophilic and heterophilic link prediction tasks in Fig. 1: the key difference between homophilic and heterophilic tasks is whether \(_{pos}\) is predominantly distributed above threshold \(M\) while \(_{neg}\) is below \(M\) (homophilic), or vice versa (heterophilic), as shown in Fig. 0(a) vs. 0(c). The categorizations of homophilic/heterophilic link prediction tasks should not be confused with the magnitude of \(M\) that indicates the variance of positive similarity scores: while its

Figure 1: Categorizing link prediction tasks based on the distribution of feature similarity scores of positive node pairs (i.e., edges – colored in green) and negative node pairs (non-edges – colored in red): two distributions whose density is visualized in the plots are (approximately) separated by the threshold(s) \(M\). Homophilic and heterophilic link prediction differs in whether the positive similarity scores fall into the larger or smaller side of the threshold \(M\), while the magnitude of \(M\) indicates the variance of positive similarity. Gated link prediction is a more complex case where the distribution of positive and negative similarity scores cannot be separated by a single threshold.

magnitude does not determine the type of the link prediction problem, our analysis in the next section does show that it affects the rate of change for the link prediction scores.

However, beyond the heterophilic setting defined above, there are other non-homophilic settings with even more complexity, where the distribution of \(_{pos}\) and \(_{neg}\) cannot be separated by a single threshold \(M\). Most of these cases are too complex to be formalized and studied theoretically, but we formalize one of them (Fig. 1d) below and later report empirical results (SS 6).

**Definition 5** (Gated Link Prediction): _The task is gated if it is neither homophilic nor heterophilic, but \(M_{1},M_{2}\) exist such that \(M_{2}(}_{pos})(}_{pos})  M_{1}\)._

### Differences between Homophilic & Heterophilic Link Prediction

In SS4.1, we have situated our discussions of link prediction tasks based on how the positive and negative samples are separated in the feature similarity score space. In this section, we reveal on a stylized learning setup the fundamental differences in optimizations for homophilic and heterophilic link prediction tasks.

**Theoretical Assumptions**.: _Assume a training graph with node \(v\) whose feature vectors are 2-dimensional unit vectors and can be represented as \(_{v}=(_{v},_{v})\). We consider a DistMult decoder for predicting the link score for candidate node pair \(u,v\) with feature vectors \(_{u},_{v}\). Specifically, the link score is calculated as \(_{u,v}=(_{u}_{v})^{}+b\), where \(\) and \(b\) are learnable parameters for the decoder, with training loss function \(=y(-)+(1-y)()\) such that \( 0\) for all edges (positive samples) and \(<0\) otherwise. Furthermore, we assume that the \(_{pos}\) and \(_{neg}\) are ideally separable by a threshold \(M\) in the feature similarity score space such that the homophilic or heterophilic conditions hold for all samples._

With the above assumptions, we now show that (1) the predicted link score and feature similarity scores are positively correlated for homophilic tasks, while negatively correlated for heterophilic tasks; (2) the change rate for the predicted link probability with respect to the feature similarity is determined by the magnitude of the threshold \(M\) that separates the positive and negative samples.

**Theorem 1**: _Following the above assumptions, consider two DistMult decoders that are fully optimized for homophilic and heterophilic link prediction problems respectively. Given an arbitrary node pair \((u^{},v^{})\) with node features \(_{u^{}}=(_{u^{}},_{u^{}})\) and \(_{v^{}}=(_{v^{}},_{v^{}})\) and pairwise feature similarity \(k(u^{},v^{})\), the following holds for the predicted link probability \(_{u^{}v^{}}\):_

* _For the homophilic problem where_ \((_{neg})<(_{pos})=M 1\)_, when bounding_ \(_{u,v}=1\) _if_ \(k(u,v)=1\) _during training,_ \(_{u^{}v^{}}\) _increases with_ \(k(u^{},v^{})\) _at a linear rate of_ \(\)_;_
* _For the heterophilic problem where_ \(-1(_{pos})=M<(_{neg})\)_, when bounding_ \(_{u,v}=1\) _if_ \(k(u,v)=-1\) _during training,_ \(_{u^{}v^{}}\) _decreases with_ \(k(u^{},v^{})\) _at a linear rate of_ \(\)_._

We give the proof in App. SSB.1 and visualize in Fig. 2 how the predicted link score \(_{u^{}v^{}}\) changes under the homophilic and heterophilic settings. Though the above results are derived under simplified assumptions, it clearly highlights the different optimizations needed for homophilic and heterophilic link prediction tasks that have not been studied in prior literature. In SS6, we observe that these differences go beyond our theoretical assumptions and affect the performance of all GNN encoders and link prediction decoders on datasets with higher complexity, which warrant our study of effective encoder and decoder choices for non-homophilic link prediction in the next section.

## 5 Encoder & Decoder Choices for Link Prediction Beyond Homophily

In SS4, we gave formal definitions of homophilic and heterophilic link prediction tasks and highlight their differences in model optimizations. As non-homophilic settings are largely overlooked in prior

Figure 2: Link prediction scores \(_{u^{}v^{}}\) for decoders optimized under homophilic (yellow) and heterophilic (blue) setups in Thm. 1 (for \(M=0.5\)).

literature, we aim to verify whether existing GNN message passing designs for node features remain effective beyond homophily. We follow the encoder-decoder perspective in  and discuss designs for both GNN encoder and link prediction decoder that adapt to non-homophilic settings.

### Decoder Choice for Heterophilic & Gated Link Prediction

For _homogeneous_ graphs that only have one edge type (as opposite to heterogeneous or knowledge graphs), popular decoder choices for deriving link probability from node representations are either a simple dot product (DOT) operation or more complex multi-layer perceptron (MLP). While the MLP decoder has a stronger representation power due to its non-linearity, the inner product decoder is more preferred in large-scale applications due to its fast inference speed: it is well established that maximum inner product search (MIPS) can be approximated with sublinear complexity using packages such as Faiss . A prior work  has benchmarked the performance of different link prediction decoders on several OGB datasets  and proposed a sublinear approximation of MLP decoder during inference time. However, no study has been conducted on the performance of decoders for non-homophilic link prediction tasks across the negative to positive similarity spectrum.

Our takeaways for effective decoder choices for non-homophilic link prediction tasks are as follows: (1) for non-homophilic (e.g., gated) tasks, only non-linear decoders such as MLP are suitable; (2) for heterophilic tasks, a linear decoder with learnable weights (e.g., DistMult ) can be used in lieu of MLP to achieve better scalability while maintaining comparable performance; (3) dot product decoder is only suitable for homophilic link prediction tasks.

Theoretically, we formalize our first takeaway with the below theorem, which shows the limitations of using linear decoders (such as DOT and DistMult) in non-homophilic link prediction tasks:

**Theorem 2**: _No parameter exists for a single linear decoder that perfectly separates link probability for edges and non-edges for gated link prediction._

We give the proof in Appendix SSB.2. For linear models, while both DistMult and DOT product decoders share the same time complexity during inference, we observe empirically in SS6 that DistMult outperforms DOT decoder by up to 55% on non-homophilic link prediction tasks. Intuitively, the learnable weights in DistMult decoder allow the model to capture the negative correlation between connected node features and improve its effectiveness for heterophilic tasks.

### Improving GNN Representation Power with Heterophily-adjusted Designs

We now consider the impact of GNN architectures on non-heterophilic link prediction performance. In particular, we examine whether the effective designs for node classification beyond _class_ homophily can be transferred to link prediction tasks beyond _feature_ homophily. A design that significantly improves classification performance under low class homophily is the separation of ego- and neighbor-embeddings in GNN message passing, which has consistently shown to improve classification performance across multiple studies [60; 36]. As real-world graphs usually follow a power-law degree distribution and exhibit large variation in node degrees, prior work has used the robustness of GNN models to degree shift as a proxy to measure the generalization ability of GNN models for heterophilic node classification . We follow a similar approach in the theorem below and show that a GNN model that embeds ego- and neighbor-features together is less capable of generalizing under heterophilic settings than a graph-agnostic model. We give the proof in Appendix SSB.3.

**Theorem 3**: _Consider the same DistMult decoder and loss function \(\) as the assumptions in SS4.2, but trained on a heterophilic graph where (1) feature vectors for all nodes can be either \(_{1}=(_{1},_{1})\) or \(_{2}=(_{2},_{2})\), and (2) nodes \(u\) and \(v\) are connected if and only if \(_{u}_{v}\). Assume two DistMult decoders are trained, one baseline with node features \(_{u}\), and the other with GNN representations \(_{u}\) instead of node features \(_{u}\), where \(_{u}\) is obtained with a linear GNN model \(_{u}=_{u}+_{l (u)}_{l}\) that considers self-loops in its message passing process. We further assume a degree shift between training and test sets, where all training nodes have degree \(d\) while the test nodes have degree \(d^{}\). Then for any \(d^{}>0\) when \(d=0\), or \(1 d^{}<d\) when \(d 2\), the DistMult decoder optimized on GNN representations \(_{u}\) reduce the separation distance between edges and non-edges for the test nodes compared to the baseline optimized with node features \(_{u}\)._

## 6 Empirical Analysis

We aim to understand through empirical analysis (1) what are the performance trends of link prediction methods under different link prediction tasks in the full spectrum of negative to positive feature similarity, and (2) how different encoder and decoder designs adapt to non-homophilic link prediction tasks, including variations of feature similarity and node degrees within the same graph. We first introduce the link prediction methods that we consider in our experiments, and then present the results on synthetic and real-world datasets. More details about setups and results are available in App. A.1

**Link Prediction Methods**. As in the previous sections, we follow an encoder-decoder framework and consider different combinations of both components. For _decoders_, we consider the options mentioned in SS5.1: (1) **Dot Product (DOT)**, (2) **Multi-Layer Perceptron (MLP)**, and (3) **DistMult**.

For _encoders_, we consider two GNN methods to study how separating ego- and neighbor-embeddings affects link prediction performance: (1) **GraphSAGE** which separates ego- and neighbor-embeddings during message passing, and (2) **Graph Convolutional Network (GCN)**, which does not make this separation. We couple these encoders with the decoders above to form six GNN4LP models.

Furthermore, to understand how message passing designs affect performance for GNN4LP models that leverage both node features and pairwise structural information, we consider BUDDY , a state-of-the-art method reported in a recent benchmark . BUDDY augments GNN encoders with subgraph sketching to capture structural information. Specifically, we consider three variants: (3) **BUDDY-GCN**, which uses GCN for feature aggregation, (4) **BUDDY-SIGN**, which uses SIGN aggregation  to separate ego- and neighbor-embeddings during message passing, and (5) **NoFeat**, a structure-only baseline that excludes node features and relies solely on structural information for link prediction. All BUDDY variants use an MLP decoder as part of their architecture.

Finally, we also consider these link prediction heuristics tested in : **Common Neighbors (CN)**, **Resource Allocation (RA)**, **Adamic-Adar (AA)**, and **Personalized PageRank (PPR)**.

### Experiments on Synthetic Graphs

We generate synthetic graphs that resemble different types of link prediction tasks (i.e., homophilic, heterophilic, and gated) by varying the feature similarity between connected nodes. These graphs provide controlled environments that allow us to focus on the effects of feature similarity on link prediction performance without mingling them with other data factors such as structural proximity. We give the details of the synthetic graph generation process and the experiment setup in App. SSA.

**Performance Trend Across the Full Similarity Spectrum**. We visualize the performance trends per method in Fig. 3 and present the numerical results in Table 2. We observe that the performance of all feature-consuming methods is significantly affected by the level of feature similarity: most methods reach their best performance at the positive extreme (homophilic tasks) and the second best at the

Figure 3: Comparing link prediction methods on synthetic graphs with varying levels of feature similarity: (a) and (b) focus on decoders, while (c) focuses on encoders. We include MLP decoder without GNN as a graph agnostic baseline in all plots. Numerical results are reported in Table 2.

negative extreme (heterophilic tasks); between the two extremes (gated tasks), the performance of all methods drops significantly as the feature similarity score approaches 0, which creates a U-shaped performance trend across the feature similarity spectrum. It is worth noting that graph-agnostic MLP decoder without GNN (NoGNN) also exhibits the U-shaped performance trend, which suggests the challenges of leveraging node features effectively in these settings for non-GNN methods as well. Intuitively, as the similarity scores for a random pair of nodes follow a normal distribution centered around 0, the performance drop when average feature similarity scores approach 0 can be contributed to the reduced distinguishability between the similarity scores of edges and non-edges in the graph. For feature agnostic heuristics such as Common Neighbors and Personalized PageRank, they show mostly unproductive performance except at the positive extreme. This suggests that graphs formed by positive feature correlations are also likely to show strong structural proximity, which is beneficial for the heuristic-based methods for the homophilic link prediction task.

**Decoder Choices: MLP and DistMult over DOT**. We further validate our main points in SS5 regarding how different decoder choices adapt to non-homophilic link prediction settings. In Fig. 2(a)-b, we compare the performance of different decoders with fixed SAGE and GCN encoders, respectively. With both SAGE and GCN encoders, we observe that DOT decoder performs the worst among all choices across all feature similarity levels: it is outperformed by MLP decoder with a margin of 50% in the negative extreme and 10% in the positive extreme under SAGE encoder. DistMult decoder performs significantly better than the DOT decoder, especially in the region of negative feature similarity: at the negative extreme, DistMult decoder outperforms DOT decoder by 55% and even outperforms MLP decoder by 5.6%. Empirically, we observe that the optimization process of DistMult is more stable than MLP when using a SAGE encoder in the negative similarity region, allowing it to reach optimal performance without suffering from instabilities at the negative extreme. However, the performance of DistMult coupled with SAGE decoder is significantly lower than MLP with up to 37% gap between the negative and positive extremes, where the link prediction tasks are gated instead of being homophilic or heterophilic. This validates our theoretical analysis in SS5.1 that the linear decoders like DOT and DistMult are not suitable for the settings where non-linear separation between similarity scores of edges and non-edges are required. With GCN encoder, the performance of DistMult decoder is mostly on par with MLP decoder across the spectrum, which shows that the performance bottleneck is on the encoder side rather than the decoder side. Overall, MLP decoder is the most robust choice across different feature similarity levels and link prediction tasks, yielding the best performance in all but one cases when coupled with SAGE encoder, with DistMult being a more scalable alternative for homophilic and heterophilic link prediction tasks.

**Encoder Choices: Importance of Ego- and Neighbor-Embedding Separation.** In Fig. 2(c), we compare the performance of different encoder choices with fixed MLP decoder, which is the best-performing decoder option for nearly all cases. In addition to GCN and SAGE encoders that rely only on node features, we also include variants of BUDDY  that leverage structural proximity and (optionally) node features. Comparing between SAGE and GCN encoders, we observe that SAGE consistently outperforms GCN across all feature similarity levels by up to 44%. For BUDDY variants with SIGN and GCN feature encoders, we also observe consistently better performance with SIGN encoder across all feature similarity levels with up to 9.0% gain. Both comparisons suggest the importance of adopting ego- and neighbor-embedding separation in GNN encoder design for link prediction: as discussed in SS5.2, this design allows GNN encoders to learn representations that are more robust to variations of node degrees and feature similarity levels in the graph instead of overfitting to specific degrees or similarity scores, which we also observe in the real-world datasets.

**Importance of Node Features vs. Structural Proximity**. Here we compare two approaches in Fig. 2(c): (1) SAGE+MLP, which combines node features with implicit graph structural information captured through GNN message passing, and (2) BUDDY-SIGN, which additionally incorporates explicit structural proximity (e.g., number of shared neighbors) through subgraph sketching. We find that BUDDY-SIGN's performance is consistently lower than SAGE+MLP across all feature similarity levels, with the gap reaching up to 26% between the negative and positive extremes. This suggests that when graph connections are predominantly driven by feature similarity (as in our synthetic graphs), the additional structural information from subgraph sketching may not provide added benefit beyond the structural information already captured by GNN message passing. While real-world graphs are typically influenced by both feature similarity and structural proximity, these results emphasize the importance of carefully balancing these two information sources in link prediction models, particularly for non-homophilic settings where structural proximity alone may be less informative.

[MISSING_PAGE_FAIL:9]

observe similar overfitting of DistMult to the highest feature similarity bucket in Fig. 4(a), despite it being significantly more robust than DOT. Overall, MLP is the most robust choice for link prediction tasks on real-world graphs with varying feature similarity levels.

**Significance of Encoder Choices.** We finally compare different encoder choices while fixing the decoder. Matching our observations on synthetic datasets, we find that SAGE encoder outperforms the GCN encoder by significant margin under most datasets and decoder choices. On e-comm dataset, the performance of GCN on the full test split is on-par with SAGE under MLP decoder, but we observe in Fig. 4(d) that GCN largely overfits to the edges in the high feature similarity and low degree bucket, with SAGE outperforming GCN by up to 3.5% in the remaining buckets. The similar overfitting is also observed on SIGN vs. GCN with BUDDY and MLP decoder on ogbl-citation2 (Fig. 4(b)). These observations show that the separation of ego- and neighbor-embeddings in the SAGE and SIGN encoders help GNNs to better adapt to local variations in feature similarity and node degrees in the real-world graphs.

## 7 Conclusion

We characterized non-homophilic link prediction through the distributions of feature similarities between linked and unlinked nodes, and proposed a theoretical framework highlighting the optimizations needed for different tasks. Our analysis revealed how link prediction encoders and decoders adapt to varying feature homophily levels, identifying key designs--learnable decoders (e.g., MLP or DistMult) with GNN encoders that separate ego- and neighbor-embeddings--for improved link prediction performance beyond homophily. Experiments on synthetic and real-world datasets demonstrated the effectiveness of these designs across the feature similarity spectrum.

In summary, our work advances the understanding of heterophilic link prediction, and lays the groundwork for future research. First, we believe that there is a need for introducing more feature-heterophilic benchmark datasets for link prediction. Expanding the diversity of available datasets would enhance the generality and practical significance of studies in this area, allowing for more robust validation of methodologies and mitigating potential biases from the scarcity of strongly heterophilic benchmarks. Second, additional in-depth theoretical frameworks could provide a more comprehensive understanding of the complexities inherent in real-world networks. This includes exploring the interplay between feature similarity and structural similarity and how these relationships influence the performance of different link prediction methods.

Figure 5: Pairwise comparison of encoder or decoder choices on test edges grouped by node degrees (x-axis) and feature similarity (y-axis): Green denotes MRR increases and purple denotes decreases. More plots in Fig. 7-10.