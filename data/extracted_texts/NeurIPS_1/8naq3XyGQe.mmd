# Choose Your Anchor Wisely:

Effective Unlearning Diffusion Models

via Concept Reconditioning

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Large-scale conditional diffusion models (DMs) have demonstrated exceptional ability in generating high-quality images from textual descriptions, gaining widespread use across various domains. However, these models also carry the risk of producing harmful, sensitive, or copyrighted content, creating a pressing need to remove such information from their generation capabilities. While retraining from scratch is prohibitively expensive, machine unlearning provides a more efficient solution by selectively removing undesirable knowledge while preserving utility. In this paper, we introduce **COncept REconditioning (CORE)**, a simple yet effective approach for unlearning diffusion models. Similar to some existing approaches, CORE guides the noise predictor conditioned on forget concepts towards an anchor generated from alternative concepts. However, CORE introduces key differences in the choice of anchor and retain loss, which contribute to its enhanced performance. We evaluate the unlearning effectiveness and retainability of CORE on UnlearnCanvas. Extensive experiments demonstrate that CORE surpasses state-of-the-art methods including its close variants and achieves near-perfect performance, especially when we aim to forget multiple concepts. More ablation studies show that CORE's careful selection of the anchor and retain loss is critical to its superior performance.

## 1 Introduction

In recent years, large-scale text-to-image generative models, especially Diffusion Models (DM), have made remarkable advancements in artificial intelligence by exhibiting an unprecedented ability to create high-resolution, high-quality images from text descriptions (Sohl-Dickstein et al., 2015; Ho et al., 2020; Rombach et al., 2022). The versatility and accessibility of diffusion models have led to their widespread adoption across various industries (Croitoru et al., 2023; Kazerouni et al., 2023; Yang & Hong, 2022; Xu et al., 2022).

Despite their broad utility, diffusion models come with inherent risks due to their extensive training on diverse datasets. These models have the potential to generate inappropriate, harmful, or legally sensitive content. For example, Stable Diffusion can produce images that involve pornography, malign stereotypes, and gender and race biases based on the embedded prejudices in their training data, even conditional on non-harmful prompts (Birhane et al., 2021; Schramowski et al., 2023; Larrazabal et al., 2020). They can memorize and reproduce realistic yet inappropriate depictions of individuals without their consent, posing huge privacy risks (Sompalli et al., 2023, 2023; Carlini et al., 2023). They can also create misleading or harmful media involving real individuals, such as deepfakes (Mirsky & Lee, 2021). Moreover, they can mimic potentially copyrighted content and replicate styles of real artists, raising legal concerns related to copyright infringement and intellectualproperty rights, as well as undermining artistic originality (Shan et al., 2023; Roose, 2022; Liu, 2022; Popli, 2022; Scenario, 2022; Brittan, 2023).

To address these concerns, legislative frameworks such as the European Union's General Data Protection Regulation (GDPR) (Mantelero, 2013; Voigt & Von dem Bussche, 2017) and the US's California Consumer Privacy Act (CCPA) (CCPA, 2018) have established the Right to be Forgotten. These laws mandate that applications must support the deletion of personal information contained in training samples upon user request. Consequently, there is a pressing need for effective methods to mitigate these risks by enabling diffusion models to **unlearn** such undesirable content, ensuring that their deployment is both responsible and aligned with societal values.

A straightforward method is to retrain the model from scratch using a filtered dataset devoid of inappropriate content. However, this approach is computationally intensive and often impractical due to the enormous resources required. For instance, training Stable Diffusion 2.0 on a filtered image set (Schuhmann et al., 2022; Rombach & Esser, 2022) demands approximately 150,000 GPU hours on 256 A100 GPUs. Early attempts to unlearn large-scale generative models include decoding-time guidance and post-generation filtering (Rando et al., 2022; Schramowski et al., 2023); however, these methods do not modify the model weights and can be easily bypassed during deployment. Recent research has pivoted towards more robust fine-tuning-based unlearning approaches that modify a model's weights to effectively forget specific undesirable elements (Gandikota et al., 2023; Fan et al., 2023; Heng & Soh, 2024; Kumari et al., 2023; Wu et al., 2024; Zhang et al., 2024; Wu & Harandi, 2024; Li et al., 2024b). These methods aim to steer the noise predictor in diffusion models away from the target concepts intended to be forgotten by efficiently fine-tuning a small fraction of parameters.

In this work, we propose **COncept REConditioning (CORE)**, a novel, simple, but effective unlearning method for diffusion model. This method leverages a fixed, non-trainable noise to guide the unlearning process, circumventing the need for dual noise predictors or the use of Gaussian noise as a target. CORE specifically alters the noise prediction mechanism for the target images conditioned on concepts in the forget set (i.e., _forget concepts_), aligning them closer to concepts in the retain set (i.e., _retain concepts_), thereby blurring the distinction between correctly generated images from forget concepts and incorrectly generated ones from retain concepts. We position CORE within a more general framework of _Concept Erasing_, and compare our method with other baselines that fit into this framework. Despite its simplicity, we demonstrate its superiority over existing methodologies through rigorous testing on the UnlearnCanvas framework, and show CORE excels in overall performance including unlearning ability, retainability, and generalization ability, especially when we aim to forget multiple concepts.

Our contributions are summarized as follows.

* We introduce **COncept REconditioning (CORE)** as a new efficient and effective unlearning method on diffusion models, and position it in a broader conceptual framework of concept erasing.
* Extensive empirical validations on UnlearnCanvas showcase that CORE significantly outperforms existing baselines, achieving nearly perfect scores and setting new state-of-the-arts for the overall performance in unlearning diffusion models on UnlearnCanvas. CORE also shows strong capabilities of generalization in unlearning styles.

Figure 1: Overview of Concept Reconditioning. \(p_{f},p_{r},p_{a}\) are the concepts targeted to be forgotten (i.e., _forget concepts_), to be remembered (i.e., _retain concepts_), and to guide unlearning (i.e., _alternative concepts_), respectively. \(t\) is the number of steps in the denoising process and is uniformly sampled within \([0,T]\), where \(T\) denotes the total number of denoising steps in diffusion models. \(_{}\) is the noise predictor function we aim to optimize, while \(_{^{*}}\) is the noise predictor in the pre-trained diffusion models.

* Ablation studies highlight the benefits of using a fixed, non-trainable target noise over other methods. Additionally, our findings emphasize the superiority of one-to-one concept reconditioning over other schemes of selecting reconditioning concepts.

## 2 Preliminaries

**Machine Unlearning.** Machine Unlearning (MU) refers to the process of systematically removing the influence of specific data points from a trained machine learning model, ensuring that the model forgets information as if the data points were never included in its training set. In this context, let \(\) represent the training dataset, and let \(_{f}\) denote the forget set, the subset of data that needs to be unlearned. The retain set, denoted as \(_{r}\), is the complement of the forget set. The goal of machine unlearning is to produce a new model that closely approximates the performance of retraining from scratch on \(_{r}\) while also ensuring that the model does not retain any knowledge of \(_{f}\). Unlearning has traditionally been explored in the context of classification models, where the model aims to either forget the influence of specific classes of data or forget some random samples (Cao and Yang, 2015; Bourtoule et al., 2021). In recent developments, machine unlearning has been extended to large generative models, where the model must unlearn specific objectives to ensure that certain generated outputs, such as sensitive, private, copyrighted, or harmful content, will not be generated.

**Unlearning Diffusion Models.** Diffusion models are a class of generative models that have gained significant attention for their ability to generate high-quality images. They work by transforming data distributions through \(T\) forward and reverse steps, gradually adding noise to the data and then learning to reverse this process to generate new samples. Mathematically, this can be described by a series of noisy images \(_{0},_{1},...,_{T}^{d},\) where \(_{0}\) is the original image, and \(_{T}\) is the Gaussian noise. Latent Diffusion Model (LDM) (Rombach et al., 2022) first compresses high-dimensional pixel-based data into a low-dimensional latent space using an encoder \(\). It then simulates the diffusion process on the space of latent variables \(=()\) and reconstructs the image through a decoder \(\). For notational simplicity, we do not differentiate between latent variables and pixel-based data, denoting both as \(\). In this context, let \(_{}(_{t},p)\) represent the noise estimator parameterized by \(\), where \(_{t}\) is the noisy observation at step \(t\), and \(p\) is a conditioning variable such as a class label or text description. The training objective of latent diffusion models is the mean squared error (MSE) between the predicted noise and the true noise across all diffusion steps, expressed as:

\[_{}()=_{p,t,(,)}[\|-_{}(_{t},p)\| _{2}^{2}], \]

where \(p\) is sampled from a distribution over all prompts and \(t\) is sampled uniformly from \([0,T]\). Given a pre-trained latent diffusion model, the objective of unlearning this diffusion model is to ensure that harmful or sensitive content, such as depictions of nudity or violence, can no longer be produced by the model when prompted with the corresponding text descriptions. The challenge lies in balancing the removal of unwanted generations while preserving the model's ability to generate high-quality, appropriate content for normal prompts. The most common unlearning process in diffusion models involves updating the noise estimator to ensure that harmful concepts associated with \(_{f}\) are no longer learned or reinforced during the reverse diffusion process. This form of unlearning, often referred to as "concept erasure", is critical for ensuring the safe deployment of generative models in real-world applications. More details are included in Section 3.2.

## 3 Concept Reconditioning

In this section, we propose **COncept REconditionng (CORE)**, a simple yet effective algorithm for unlearning in diffusion models. Our approach focuses on reconditioning the model's learned representations by substituting concepts from the forget set with selected alternative concepts from the retain set. First, we introduce the objective function and key designs within. Then, we position it within the broader framework of _Concept Erasing_ and compare it with similar algorithms in prior works to showcase its advantage.

### Proposed Method

**Unlearn objective.** In the context of unlearning in diffusion models, we denote the noise predictor in Latent Diffusion Models by \(_{}(_{t},p)\), where \(_{t}\) is the noisy version of the input image \(_{0}\) at time step \(t\) generated during the forward diffusion process, \(p\) is the prompt associated with the image (e.g., "A cat in the style of Van Gogh"), and \(\) represents the model parameters. We use \(_{^{*}}(_{t},p)\) and \(^{*}\) to denote the pre-trained diffusion model and its parameter. In CORE, we aim to recondition images from the forget set onto alternative concepts. This is achieved by aligning the noise estimator for images in the forget set, conditioned on their original concepts \(p_{f}_{f}\), toward the ground truth noise estimator for the same image but conditioned on an alternative concept \(p_{a}\). Mathematically, the unlearn objective function is formulated as

\[_{f}():=_{(p_{f},_{0})_{f}, p_{a} p_{f}}[\|_{}(_{t},p_{ f})-_{^{*}}(_{t},p_{a})\|_{2 }^{2}], \]

where the expectation is taken over the concept-image pairs \((p_{f},_{0})\) from the forget set, alternative concepts \(p_{a}\) different from \(p_{f}\), and time steps \(t\) uniformly sampled from \([0,T]\). Intuitively, this process effectively weakens the association between the images and their original concepts in the model, steering it away from the initial pre-trained associations.

**Alternative concepts.** A key design choice in CORE is the selection of alternative concepts \(p_{a}\) in equation (2). In the unlearning objective, \(p_{a}\) acts as an anchor concept to recondition images from the forget set onto. Previous works typically use an empty string or a single base concept for \(p_{a}\) consistently across all concepts to be unlearned (Zhang et al., 2024; Gandikota et al., 2023). In contrast, CORE adopts a different approach by pairing each forget concept \(p_{f}\) with a specific alternative concept \(p_{a}\). Our pairing scheme imposes minimal restrictions: the alternative concept \(p_{a}\) does not necessarily have to come from the retain set; it can even be another forget concept different from \(p_{f}\). In our implementation, when the number of concepts to forget is smaller than the number of retain concepts, we map each forget concept to a unique concept in the retain set, rather than using a single base concept for all forget concepts. Meanwhile, when the retain concepts are limited and there are more concepts to forget, we create a one-to-one mapping among the forget concepts themselves. This means that each forget concept \(p_{f}\) is paired with another forget concept \(p_{a}\) (where \(p_{a} p_{f}\)) to serve as its alternative concept during unlearning. Empirically, we show that this one-to-one mapping strategy significantly outperforms methods that consistently use a base concept or randomly sample alternative concepts at each step.

**Retain objective and the full loss function**. To ensure the model continues generating high-quality images for the retain concepts, we introduce a retain loss to regularize the unlearning process. Traditionally, the retain loss is defined as the Mean Squared Error (MSE) between the noise prediction for the retain set and the Gaussian noise vector used to generate the noisy images, similar to the objective used in training a diffusion model (see equation 1). However, in CORE, rather than fine-tuning the noise predictions to match a Gaussian random vector, we instead align them with those generated by the pre-trained diffusion model itself. Mathematically, the retain objective is defined as

\[_{r}():=_{(p_{r},_{0})_{r},t}[\|_{}(_{t},p_{r})- _{^{*}}(_{t},p_{r})\|_{2}^{2 }], \]

where \(t\) is uniformly sampled in \([0,T]\) and \((p_{r},_{0})\) are concept-image pairs sampled from the retain set. Using \(_{^{*}}(_{t},p_{r})\) as the target helps ensure the model does not deviate too far from its original capabilities, as it leverages the pre-trained model's learned knowledge. Empirical results (see Section 4) demonstrate that aligning the noise predictions with \(_{^{*}}(_{t},p_{r})\), rather than the Gaussian noise, yields better performance. This improvement arises potentially because using the estimated noise from the pre-trained model reduces variance in the unlearned model and stabilize the training process. Interestingly, this phenomenon, where using estimated signals outperforms true signals, has also been observed in other domains in statistics (Robins et al., 1992; Henmi and Eguchi, 2004; Hitomi et al., 2008; Su et al., 2023).

Finally, the complete loss function in CORE combines both the unlearn and retain objectives:

\[():=_{f}()+_{r}(), \]

where \(>0\) controls the regularization strength. Intuitively, CORE ensures that the model is steered away from generating images associated with forget concepts while preserving its overall performance on other concepts.

### Rethinking Concept Erasing and Reconditioning

At first glance, our proposed objective might seem similar to existing methods for unlearning in diffusion models, as it also involves steering the error predictor on the forget set while keeping it unchanged on the retain set. However, under closer scrutiny, Concept Reconditioning introduces several key distinctions that set it apart and enable it to outperform previous approaches. Take a broader view of the framework of unlearning diffusion models: unlearning methods for diffusion models that are based on fine-tuning the error predictor \(_{}(,p)\) can generally be categorized into two classes: \(\) Concept Erasing (CE): This method works by shifting the noise prediction network for images corresponding to the forget concepts towards an alternative noise distribution. Intuitively, by doing so, it directly acts on \(_{}(_{t}^{f},p_{f})\), where \(_{t}^{f}\) is the noisy observation for images in the forget set, and misleads them away. \(\) Image Relabeling (IR): In this approach, alternative images that do not match the forget concepts are selected, and the model is fine-tuned on the forget concepts paired with these mismatched images. The model directly acts on \(_{}(_{t}^{r},p_{f})\) where \(_{t}^{r}\) is the noisy images constructed from the retain set, and effectively overwrites the old knowledge with new associations, forcing it to forget by learning new, incorrect pairings. Mathematically, these two classes can be formulated as

\[_{}() :=_{(p_{f},_{0})_{ f},t}[\|_{}(_{t},p_{f})-_{ }\|_{2}^{2}], \] \[_{}() :=_{p_{f}_{f},_{0 }_{r},t}[\|_{}(_{t},p_{f} )-_{}\|_{2}^{2}]. \]

Here, \(\{ 1\}\) controls the direction of the objective function. In the CE method, images are drawn from the forget set, while in IR, images come from the retain set. The **target noises**\(_{}\) and \(_{}\) can be either random vectors (e.g., Gaussian or Uniform) or derived from a trainable noise predictor.

Many existing unlearning methods fit within this framework. For example, Heng & Soh (2024) suggests \(=-1\) and \(_{}(,_{d})\) in equation (5) in the unlearning objective, while proposing a surrogate objective with \(=1\) and \(_{}(,_{d})\) in equation (6). The former corresponds to a gradient ascent loss applied to the pre-training objective on forget concepts, while the surrogate objective simply mirrors the standard training loss applied to the forget concepts with retain images. Fan et al. (2023) takes \(_{}\) in equation (5) as a trainable noise predictor \(_{}(_{t},p_{a})\) where \(p_{a} p_{f}\) is an alternative concept coming from the retain set. Wu et al. (2024) also proposes this target noise, as well as suggesting an alternative with \(_{}\) as a uniformly distributed random vector. Kumari et al. (2023) takes \(_{}\) in equation (6) to be either a standard Gaussian random vector or the error predictor at the last iterate, evaluated at retain images paired with corresponding retain concepts. Even when the objective function appears divergent from this framework, as seen in Gandikota et al. (2023), it can still be decomposed into a linear combination of objective functions in the framework above (see Appendix C).

Although these prior works often include additional techniques such as weight decay (Heng & Soh, 2024), saliency map (Fan et al., 2023), or even applying a monotonic function to the squared loss (Park et al., 2024), the backbone of their unlearning objectives can be positioned into this simple framework or its simple variants. Our method distinguishes itself from prior approaches by its simplicity. Unlike previous methods, CORE requires no auxiliary techniques, and simply optimizing the objective \(()\) in equation (4) achieves state-of-the-art results.

Another key distinction is that CORE uses a fixed, non-trainable noise predictor from the pre-trained diffusion model as the target noise. This fixed anchor provides a clearer target noise compared to a trainable network or a random vector with a fixed distribution (e.g., a uniformly distributed random vector). Let us compare the three types of target noises. With a random vector from a fixed distribution (Kumari et al., 2023; Heng & Soh, 2024), there is no guarantee that this manually designed random vector will effectively disrupt the noise predictor conditioned on the forget concepts. A trainable, non-fixed noise (Fan et al., 2023; Kumari et al., 2023; Wu et al., 2024) is unstable during the unlearning process, particularly when aiming to forget many concepts over a long training period, since this target may drift towards an undesired direction. While methods using trainable target noises include a retain term in their loss function, this retain objective directly influences \(_{}(_{t}^{r},p_{r})\) but not \(_{}(_{t}^{f},p_{r})\), where \(_{t}^{r}\) and \(_{t}^{f}\) are noisy observations from the retain and forget sets, respectively. In contrast, CORE's use of a non-trainable target noise ensures that the noise predictor always learns from a reference "incorrect" noise estimator derived from the pre-trained model.

## 4 Experiments

In this section, we show CORE outperforms baselines on UnlearnCanvas (Zhang et al., 2024c).

### Experiment Setup

**Dataset and Tasks.** UnlearCanvas is a high-resolution stylized image dataset designed to evaluate diffusion model unlearning methods (Zhang et al., 2024c). The dataset consists of images across 50 unique styles and 20 distinct objects, with 20 images for each style-object combination. Each image is labeled with both a style and an object, making it particularly well-suited for measuring the unlearning effectiveness and the retainability both within a single domain and across domains. In this paper, we mainly focus on style unlearning within the UnlearnCanvas dataset. We define three unlearning tasks, each progressively forgetting more styles: Forget01 (forgetting 1 style), Forget06 (forgetting 6 styles), and Forget25 (forgetting 25 styles).

**Models and Baselines.**

We use a Stable Diffusion v1.5 model (Rombach et al., 2022) to perform the fine-tuning and unlearning, and we also use a vision Transformer (ViT-Large) (Dosovitskiy, 2020) on UnlearnCanvas for style and object classification. Before unlearning the model, the base Stable Diffusion model is fine-tuned on all images from UnLearCanvas. After completing the unlearning phase, we prompt the unlearned model to generate images conditioned on concepts from both forget and retain sets. The vision Transformer is then used to classify the generated images and calculate the relevant metrics. We compare CORE with several state-of-the-art unlearning methods for diffusion models, including ESD (Gandikota et al., 2023), SalUn (Fan et al., 2023), Ediff (Wu et al., 2024), CA-model and CA-noise (Kumari et al., 2023). See Appendix C for more details.

**Metrics.** Following Zhang et al. (2024c), we use Unlearning Accuracy (UA) to assess the unlearning effectiveness. UA is the percentage of images generated by the unlearned model, conditioned on the forget concepts, which are incorrectly classified by the vision Transformer. A higher UA indicates stronger unlearning capabilities. We measure retainability using two metrics: In-domain Retain Accuracy (IRA) and

  
**Algorithm** & **UA** & **IRA** & **CRA** & **SFID** & **Total** \\  & (\(\)) & (\(\)) & (\(\)) & (\(\)) & (\(\)) \\   \\  Original & 0.00 & 100.00 & 96.67 & 100.00 & 296.67 \\ Ediff & 93.33 & 84.00 & 98.33 & 100.00 & 375.66 \\ CA-model & 96.67 & 80.00 & 92.78 & 100.00 & 369.45 \\ CA-noise & 100.00 & 100.00 & 96.11 & 100.00 & **396.11** \\ SalUn & 53.33 & 98.67 & 92.78 & 95.74 & 340.52 \\ ESD & 100.00 & 66.00 & 96.11 & 96.95 & 359.06 \\
**CORE (ours)** & 93.33 & 98.00 & 96.11 & 100.00 & 387.44 \\   \\  Original & 0.00 & 100.00 & 98.33 & 100.00 & 298.33 \\ Ediff & 45.00 & 80.00 & 99.17 & 100.00 & 324.17 \\ CA-model & 85.00 & 81.67 & 88.33 & 88.09 & 343.09 \\ CA-noise & 85.00 & 91.67 & 85.83 & 92.46 & 354.96 \\ SalUn & 90.00 & 83.33 & 98.33 & 88.52 & 360.18 \\ ESD & 100.00 & 75.00 & 100.00 & 93.47 & 368.47 \\
**CORE (ours)** & 90.00 & 100.00 & 97.50 & 99.56 & **387.06** \\   \\  Original & 1.20 & 96.54 & 95.29 & 100.00 & 293.03 \\ Ediff & 54.00 & 78.46 & 95.10 & 84.48 & 312.04 \\ CA-model & 68.60 & 78.85 & 95.69 & 81.73 & 324.87 \\ CA-noise & 47.20 & 86.15 & 90.59 & 82.09 & 306.03 \\ SalUn & 51.60 & 77.31 & 87.65 & 82.34 & 298.90 \\ ESD & 90.40 & 46.54 & 99.02 & 88.12 & 324.08 \\
**CORE (ours)** & 91.60 & 95.38 & 97.65 & 100.00 & **384.63** \\   

Table 1: Performance of CORE and five baseline methods using Stable Diffusion v-1.5 on Forget01, Forget06, and Forget25 in UnlearCanvas. Unlearning accuracy, In-domain and cross-domain retain accuracy, and scaled FID value serve as main metrics and are summarized in Section 4.1. For details about the scaled FID value, see Appendix B. The best total score is highlighted in **bold**.

Figure 2: Generated images from the unlearned model. The first column is generated by the fine-tuned Stable Diffusion model before any unlearning. Other columns are generated by the model unlearned by our proposed method and five baseline methods. More images are included in Appendix D.

Cross-domain Retain Accuracy (CRA). IRA refers to the classification accuracy of generated images prompted with retain concepts, within the same domain (e.g., when forgetting "Van Gogh's style", an in-domain prompt might be "A painting in crayon style"). CRA measures accuracy for retain prompts across domains (e.g., for the same task, a cross-domain prompt might be "A painting of a cat," specifying the object). Additionally, we evaluate the quality of generated images using the scaled FID (SFID) score, which maps the original FID score (Heusel et al., 2017) onto a 0-100 scale, where higher SFID values indicate better generation quality. We also present the summation of all four scores on a scale of 0-100, as a comprehensive measurement of the unlearning capacity and retainability. For more experimental details, see Appendix B.

### Results

**CORE achieves the best overall performance.** In Table 1, we present the unlearning effectiveness and retainability of CORE compared to five baseline methods across Forget01, Forget06 and Forget25 tasks from UnlearnCanvas. The "Original" row refers to the performance of the pre-trained model without any unlearning. On Forget01, CORE ranks second overall based on the total score. However, in the more challenging tasks Forget06 and Forget25, CORE consistently achieves the highest total score among all methods, with an increasing performance gap over the baseline methods. Notably, CORE is the only method that maintains strong performance as the size of the forget set grows. In the most difficult task, where 25 out of 50 concepts are targeted for forgetting, CORE achieves the highest unlearning accuracy, in-domain retain accuracy, and scaled FID score, while securing the second-best cross-domain retain accuracy. Compared to its close variants, ESD, CORE achieves similar unlearning accuracy but significantly outperforms in retainability, particularly in cross-domain tasks, due to the adoption of an additional retain loss. Compared to baseline methods that use a trainable noise predictor, such as SalUn and CA-model, CORE excels in forgetting more concepts due to the stability of its non-trainable target, which proves more reliable over longer unlearning periods. Figure 2 shows some generated images using CORE and five baseline methods.

**CORE shows better generalization ability in unlearning styles.** We further investigate CORE's ability to generalize in unlearning styles, aiming to verify that CORE can effectively unlearn specific target styles, instead of simply overfitting to the training objects. To assess this, we train the model on only 10 objects for each forget concept and then evaluate the unlearning accuracy on 10 unseen objects. This tests the model's ability to generalize beyond the specific objects used during training. As shown in Table 2, CORE outperforms all baseline methods in terms of generalization ability.

**The role of non-trainable target noise.** A key design choice in CORE is the use of non-trainable target noise from the pre-trained diffusion model in both the unlearn and retain objectives. This is contrary to other approaches that use trainable noise predictors as targets in the unlearn loss and Gaussian noise vectors as targets in the retain loss. To isolate the specific effects of the non-trainable target noise, excluding the influence of auxiliary techniques like saliency maps, we evaluate several variants of CORE: \(\) We replace \(_{^{*}}(_{t},p_{a})\) with \(_{}(_{t},p_{a})\) in equation (2), where \(_{t}\) are noisy images from the forget set and \(p_{a}\) is the alternative concept. This variant mirrors the backbone of the unlearn loss used in SalUn (Fan et al., 2023). \(\) We replace \(_{^{*}}(_{t},p_{a})\) with a Gaussian noise \(\) in equation (2) and apply a negative sign to the unlearn loss. This variant follows the gradient ascent-based method, similar to the unlearn loss in CA-noise (Kumari et al., 2023). \(\) We replace \(_{^{*}}(_{t},p_{r})\) with a Gaussian noise \(\) in equation (3), where \(_{t}\) is noisy observations of images from the retain set. This variant is aligned with the retain loss employed in many baseline methods (Heng and Soh, 2024; Kumari et al., 2023; Wu et al., 2024). The results are shown in Table 3.

**Anchor Selection: How do we approach it?** Another key distinction between CORE and other baseline methods lies in how anchors \(p_{a}\) are selected in the unlearning objective (as defined in equation 2). In CORE, each forget concept \(p_{f}\) is paired with a distinct alternative concept. This

  
**Algorithm** & **UA** (\(\)) & **IRA** (\(\)) & **CRA** (\(\)) & **SFID** (\(\)) & **Total** (\(\)) \\  Ediff & 36.67 & 81.67 & 92.50 & 100.00 & 310.84 \\ CA-model & 85.00 & 83.33 & 96.67 & 86.67 & 351.67 \\ CA-noise & 81.67 & 91.67 & 87.50 & 87.62 & 348.46 \\ SalUn & 95.00 & 65.00 & 90.83 & 86.37 & 337.20 \\ ESD & 100.00 & 46.67 & 99.17 & 86.11 & 331.95 \\
**CORE (ours)** & 83.33 & 100.00 & 96.67 & 99.67 & 379.67 \\   

Table 2: Generalization ability of CORE and baseline methods using Stable Diffusion v-1.5 on Forget06 of UnlearnCanvas. Unlearning accuracy, In-domain and cross-domain retain accuracy, and scaled FID value serve as main metrics and are summarized in Section 4.1. The best total score is highlighted in **bold**.

contrasts with other methods that recondition all forget concepts to a single base concept or the empty string. To demonstrate the effectiveness of CORE's one-to-one pairing, we compare different selection schemes: One approach involves pairing each forget concept with a set of alternative concepts (or even the entire retain set) and randomly sampling one at each gradient step to recondition the target images. Another approach reconditions images from multiple or even all forget concepts onto a single base concept. As shown in Table 4, CORE's one-to-one reconditioning scheme significantly outperforms these strategies. Specifically, unlearning accuracy declines sharply when forget concepts are paired with multiple alternatives (one-to-all or one-to-five) and a random alternative is sampled at each step. Conversely, the model's stylistic retainability suffers when all forget concepts are reconditioned to just one or a few base concepts.

## 5 Conclusion and Future Directions

In this paper, we introduce COncept REconditioning (CORE), a novel and effective method for unlearning in diffusion models. CORE leverages a non-trainable target noise from the pre-trained diffusion model to guide both the unlearning and retain objectives, thereby avoiding the pitfalls of using trainable noise predictors or random Gaussian noise targets. Through extensive experiments on the UnlearnCanvas dataset, we demonstrate that CORE consistently outperforms state-of-the-art baseline methods in terms of unlearning effectiveness, retainability, and generalization ability, particularly in challenging tasks involving multiple forget concepts. Moreover, we highlight the importance of a one-to-one concept reconditioning scheme, which proves superior to other anchor selection strategies. There are several promising directions for future research. One key area is improving the efficiency of unlearning, particularly when dealing with a large number of forget concepts. Current methods can still be time-consuming when unlearning many concepts simultaneously. Exploring accelerated unlearning methods while maintaining performance is an exciting avenue. Additionally, future work could investigate the robustness of unlearning methods in dynamic environments, where new concepts might continuously be added to the model, requiring continuous updates without retraining from scratch.

  
**Scheme for reconditioned concepts** & **UA** (\(\)) & **IRA** (\(\)) & **CRA** (\(\)) & **SFID** (\(\)) & **Total** (\(\)) \\  Default (one-to-one) & 91.60 & 95.38 & 97.65 & 100.00 & **384.63** \\ One base concept (all-to-one) & 82.40 & 60.00 & 98.33 & 93.06 & 333.79 \\ Five base concepts (five-to-one) & 93.40 & 84.04 & 98.43 & 96.52 & 372.39 \\ Random concept (one-to-all) & 56.60 & 95.77 & 96.96 & 100.00 & 349.33 \\ Random from five concepts (one-to-five) & 56.40 & 95.58 & 97.75 & 99.78 & 349.51 \\   

Table 4: Comparison of different alternative concept selection schemes. All experiments are done in the Forget25 task from UnlearnCanvas. In CORE (referred to as “Default”), each forget concept is paired one-to-one with a distinct alternative concept. One base concept: all forget concepts are reconditioned onto a single base concept. Five base concepts: forget concepts are grouped into sets of five, with each group reconditioned to one base concept. Random concept: a random alternative concept is selected for each forget concept at every gradient step. Random from five concepts: each forget concept is paired with five alternative concepts, with one randomly sampled at each step. The best total score is highlighted in **bold**. Significant underperforming results are highlighted in green.

  
**Unlearn Loss** & **Retain Loss** & **UA** (\(\)) & **IRA** & **CRA** & **SFID** & **Total** \\  & & & (\(\)) & (\(\)) & (\(\)) & (\(\)) \\  
**CORE** & **CORE** & 95.00 & 100.00 & 97.08 & 100.00 & **392.08** \\ \(\|_{}(_{t}^{f},p_{f})-_{^{*}}(_{t}^{f},p_{a})\|_{2}^{2}\) & **CORE** & 43.33 & 98.33 & 95.00 & 100.00 & 336.66 \\ \(-\|_{}(_{t}^{f},p_{f})-\|_{2}^{2}\) & **CORE** & 85.00 & 61.67 & 60.00 & 79.48 & 286.15 \\ CORE** & \(\|_{}(_{t}^{r},p_{r})-\|_{2}^{2}\) & **83.33** & 93.33 & 96.67 & 99.92 & 373.25 \\   

Table 3: Performance of CORE and its variants on the Forget06 task from UnlearnCanvas. In each variant, one component of the loss function remains unchanged, while the non-trainable target noise in the other component is replaced with alternative approaches. Metrics and are summarized in Section 4.1. The best total score is highlighted in **bold**. Here, \(_{t}^{f}\) and \(_{t}^{r}\) are the noisy observations for images in the forget set and retain set, respectively; \(p_{f},p_{a},p_{r}\) correspond to forget concepts, alternative concepts, and retain concepts, respectively. \(\) denotes the standard Gaussian random vector used to generate \(_{t}^{f}\). Here, we pair each forget concept with one distinct retain concept in all experiments above.