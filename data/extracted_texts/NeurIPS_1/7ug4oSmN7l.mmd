# A Learning-based Capacitated Arc Routing Problem Solver Comparable to Metaheuristics While with Far Less Runtimes

A Learning-based Capacitated Arc Routing Problem Solver Comparable to Metaheuristics While with Far Less Runtimes

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Recently, neural networks (NN) have made great strides in combinatorial optimization problems (COPs). However, they face challenges in solving the capacitated arc routing problem (CARP) which is to find the minimum-cost tour that covers all required edges on a graph, while within capacity constraints. Actually, NN-based approaches tend to lag behind advanced metaheuristics due to complexities caused by _non-Euclidean graph, traversal direction and capacity constraints_. In this paper, we introduce an NN-based solver tailored for these complexities, which significantly narrows the gap with advanced metaheuristics while with far less runtimes. First, we propose the direction-aware attention model (DaAM) to incorporate directionality into the embedding process, facilitating more effective one-stage decision-making. Second, we design a supervised reinforcement learning scheme that involves supervised pre-training to establish a robust initial policy for subsequent reinforcement fine-tuning. It proves particularly valuable for solving CARP that has a higher complexity than the node routing problems (NRPs). Finally, a path optimization method is introduced to adjust the depot return positions within the path generated by DaAM. Experiments show that DaAM surpasses heuristics and achieves decision quality comparable to state-of-the-art metaheuristics for the first time while maintaining superior efficiency, even in large-scale CARP instances. The code and datasets are provided in the Appendix.

## 1 Introduction

The capacitated arc routing problem (CARP)  is a combinatorial optimization problem, frequently arising in domains such as inspection and search-rescue operations. Theoretically, the CARP is established on an undirected connected graph \(=(,,_{R})\) that includes a set of nodes \(\) connected by a set of edges \(\), and an edge subset \(_{R}\) that needs to be served, called required edges. Each required edge has a demand value which spends the capacity of the vehicle when it is served. In this context, all vehicles start their routes from the depot node \(depot\) and conclude their journey by returning to the same \(depot\). The main goal of a CARP solver is to serve all required edges with the lowest total path cost, while ensuring that the vehicle does not exceed its capacity \(Q\).

According to , the CARP is recognized as an NP-Hard problem. Among all solver solutions, the Memetic Algorithms (MA) [15; 25], first proposed in 2005, have still maintained unrivaled results in solving the CARP challenge to this day. However, they have struggled with high time costs and the exponential growth of the search space as the problem scale increases. Compared to the traditional methods, NN-based solvers [16; 10; 20] are faster with the assistance of GPU, have therefore gained increasing attention in recent years. However, unlike the decent performance in solving NRP, such as vehicle routing problem (VRP) and travelling salesman problem (TSP), or ARP defined in Euclidean graphs, such as rural postman problem (RPP) and Chinese postman problem (CPP), NN-basedmethods usually lags far behind the traditional ones in solving CARP. This discrepancy is attributed to the inability of existing methods to effectively reduce the high complexity of solving CARP:

* **Lack of edge direction in embedding learning:** ARP solvers need to determine the edges to be traversed along with the direction of traversal, easy for humans to achieve in one step but extremely challenging for computers. Existing methods didn't encode edge directionality in embedding, making them have to build edge sequences and determine edges' directions separately and leading to path generation without sufficient consideration.
* **Ineffective learning for solving CARP:** CARP is more complex than NRPs and Euclidean ARPs owing to the _non-Euclidean input_, _edge direction_, and _capacity constraints_. Thus, learning methods for NRPs and Euclidean ARPs cannot be directly transferred to solve CARP or work well even if adapted, leaving a lack of effective learning strategies for CARP.

In this paper, we aim to address both above issues and propose an NN-based solver for CARP that competes with the state-of-the-art MA  while with far less runtimes. Firstly, we propose the direction-aware attention model (DaAM). It computes embeddings for directed arcs decomposed from undirected edges to align with the nature of ARP, thus avoiding missing direction information and enabling concise one-stage decision-making. Secondly, we design a supervised reinforcement learning method to learn effective heuristics for solving CARP. DaAM is pre-trained to learn an initial policy by minimizing the difference from the decisions made by experts, and fine-tuned on larger-scale CARP instances by Proximal Policy Optimization with self-critical strategy. Finally, to further boost the path quality, we propose a path optimizer (PO) to re-decide the vehicles' optimal return positions by dynamic programming. In the experiments, our method approaches the state-of-the-art MA with an average gap of 5% and is 4% better than the latest heuristics and gains high efficiency.

## 2 Related Work

### Graph Embedding Learning

Graph embedding  aims to map nodes or edges in a graph to a low-dimensional vector space. This process is commonly achieved via graph neural networks (GNNs) . Kipf _et al._ introduced graph convolutional operations to aggregate information from neighboring nodes for updating node representations. Unlike GCN, GAT  allowed dynamic node attention during information propagation by attention mechanisms. Other GNN variants [9; 30] exhibited a similar information aggregation pattern but with different computational approaches. In this paper, since an arc is related to the outgoing arc of its endpoint but irrelevant to the incoming arc of that, we use attention mechanisms to capture the intricate relationships between arcs for arc embedding learning.

### Learning for Routing Problems

The routing problem is one of the most classic COPs, and it is mainly categorized into two types according to the decision element: node routing problems and arc routing problems.

**Node routing problems** (NRPs), such as TSP and VRP, aim to determine the optimal paths that traverse all nodes in the Euclidean space or graphs. As the solutions to these problems are context-dependent sequences of variable size, they cannot be directly modeled by the Seq2Seq model . To address this problem, Vinyals _et al._ proposed the Pointer network (PN) to solving Euclidean TSP, which achieves variable-size output dictionaries by neural attention. Due to the scarcity of labels for supervised learning, Bello _et al._ modeled the TSP as a single-step reinforcement learning problem and trained the PN using policy gradient  within Advantage Actor-Critic (A3C)  framework. Nazari _et al._ replaced the LSTM encoder in PN with an element-wise projection layer and proposed the first NN-based method to solve the Euclidean VRP and its variants. To better extract correlations between inputs, Kool _et al._ utilized multi-head attention for embedding learning and trained the model using REINFORCE  with a greedy baseline. To solve COPs defined on graphs, Khalil _et al._ proposed S2V-DQN to learn heuristics, employing structure2vec  for graph embedding learning and n-step DQN  for training. While the mentioned NN-based approaches have achieved comparable performance to metaheuristics, they cannot be directly applied to solve ARP due to the modeling differences between ARP and NRP.

**Arc routing problems** (ARPs) involve determining optimal paths for traversing arcs or edges in graphs, with variants like RPP, CPP, and CARP. Truong _et al._ proposed a DRL framework to address the CPP with load-dependent costs on Euclidean graphs and achieved better solution quality than metaheuristics. However, CARPs are defined on non-Euclidean graphs. Unlike Euclidean graphs with given node coordinates, non-Euclidean graphs require manually extracting and aggregating the node representations, a task that is typically learnable. Although several NN-based algorithms have been proposed, they still lag significantly behind traditional methods. Li _et al._ pioneered the use of the NN-based approach in solving the CARP by transforming it into an NRP. They first determined the sequence of edges and then decided the traversal direction for each edge. Hong _et al._ trained a PN in a supervised manner to select undirected edges in each time step, and also determined the edge traversal direction as post-processing. Ramamoorthy _et al._ proposed to generate an initial tour based on edge embeddings and then split it into routes within capacity constraint. These approaches lack edge directionality encoding, leading to edge selection without sufficient consideration and necessitating a two-stage decision process or an additional splitting procedure.

## 3 Background

The attention model (AM)  exhibits superior effectiveness in solving classic Euclidean COPs due to its attention mechanisms for extracting correlations between inputs. Therefore, we use the AM as the backbone and give a brief review in terms of the TSP. Given an Euclidean graph \((,)\), the AM defines a stochastic policy, denoted as \((|)\), where \(=(x_{0},...,x_{||-1})\) represents a permutation of the node indexes in \(\), and \(\) is the problem instance expressing \(\). The AM is parameterized by \(\) as:

\[_{}(|)=_{t=1}^{||}_{}(x_{t}|,_{0:t-1}) \]

where \(t\) denotes the time step. Specifically, the AM comprises an encoder and a decoder. The encoder first computes initial \(d_{h}\)-dimensional embeddings for each node in \(\) as \(h_{i}^{0}\) through a learned linear projection. It then captures the embeddings of \(h_{i}^{0}\) using multiple attention layers, with each comprising a multi-head attention (MHA) sublayer and a node-wise feed-forward (FF) sublayer. Both types of sublayers include a skip connection and batch normalization (BN). Assuming that \(l\{1,...,N\}\) denotes the attention layer, the \(l^{}\) layer can be formulated as \(h_{i}^{l}\):

\[h_{i}^{l}=^{l}(}+^{l}(}));}=^{l}(h_{i}^{l-1}+_{i}^{l}(h_{0}^{l-1},,h_{| |-1}^{l-1})) \]

The decoder aims to append a node to the sequence \(\) at each time step. Specifically, a context embedding \(h_{(c)}\) is computed to represent the state at the time step \(t\). Then a single attention head is used to calculate the probabilities for each node based on \(h_{(c)}\):

\[u_{(c)j} =C(d_{h}^{-}[^{Q }h_{(c)}]^{T}^{K}h_{j}^{N})&j x_{t^{}}(  t^{}<t)\\ -&,\] \[p_{i} =_{}(x_{t}=i|,_{0:t-1})=u_{(c)i}/ _{j}u_{(c)j} \]

where \(^{Q}\) and \(^{K}\) are the learnable parameters of the last attention layer. \(u_{(c)j}\) is an unnormalized log probability with \((c)\) indicating the context node. \(C\) is a constant, and \(p_{i}\) is the probability distribution computed by the softmax function based on \(u_{(c)j}\).

## 4 Method

### Direction-aware Attention Model

In this section, we propose the direction-aware attention model (DaAM). Unlike previous methods that separately learn edge embeddings and determine edge directions, our model encodes direction information directly into the embedding, enabling one-stage decision-making. As shown in Fig. 1, the DaAM makes sequential decisions in two phases to select arcs. **The first phase** is a one-time transformation process, in which the arcs of the input graph are represented as nodes in the new directed complete graph. **The second phase** is executed at each time step, in which GAT is used to aggregate the inter-arc weights. Subsequently, AM is used to select the arc of the next action.

#### 4.1.1 Arc Feature Formulation via Graph Transformation

Graph TransformationMotivated by the need to consider direction when traversing edges, we explicitly encode the edge direction by edge-to-arc decomposition. Let \((,,_{R})\) denotesthe undirected connected graph as input, where \(\) is the node set of \(\), \(\) is the edge set of \(\), and \(_{R}\) is the required edge set. Firstly, given that an edge has two potential traversal directions, we decompose each edge \(_{nm}\!=\!(cost_{nm},demand_{nm},allow\_serve_{nm})\!\!_{R}\) into two arcs \(\{arc_{nm},arc_{mn}\}\) with opposite directions but the same cost, demand and serving state. Here \(n,m\) are the indexes of node in \(\). To simplify the representation below, we replace \(nm\) and \(mn\) with single-word symbols, such as \(i\) and \(j\). In this way of edge decomposition, we obtain a set of arcs denoted as \(A_{R}\). Secondly, we build a new graph \(G=(A_{R},E)\). Specifically, each arc in \(A_{R}\) serves as a node in \(G\), and directed edge set \(E\) is created, with \(e_{ij} E\) representing the edge from node \(arc_{i}\) to \(arc_{j}\). The weight \(|e_{ij}|\) represents the total cost of the shortest path from the end node of \(arc_{i}\) to the start node of \(arc_{j}\). In addition, we treat the depot as a self-loop zero-demand arc that allows for repeated serving, denoted as \(arc_{0}\). Consequently, we transform the input graph \(\) into a directed complete graph \(G\). By decomposing all edges in \(_{R}\) into arcs, it is natural to directly select the arcs from \(G\) during the decision-making. Given that the Floyd-Warshall algorithm is used to calculate the shortest path cost between any pair of nodes in \(\), the time complexity of our graph transformation is \(((|_{R}|^{2}),(||^{3}))\).

Arc Feature FormulationTo establish a foundation for decision-making regarding arc selection, the features of the arcs are constructed as input for the subsequent model. Specifically, multi-dimensional scaling (MDS) is used to project the input graph \(\) into a \(d\)-dimensional Euclidean space. The Euclidean coordinates of \(arc_{i}\)'s start and end nodes, denoted as \(mds_{(i)}\) and \(mds_{(i)}\), are then taken as the features of \(arc_{i}\) to indicate its direction. As shown in Table 1, at time step \(t\), \(arc_{i}\) can be featured as:

\[F_{t}^{(i)}=(is\_depot_{i},cost_{i},demand_{i},|e_{x_{t-1}i}|,allow\_serve_{t }^{(i)},mds_{(i)},mds_{(i)}) \]

where \(x_{t-1}\) is the index of the selected arc at the last time step, and \(t[1,+)\). Our feature models arcs rather than edges and encodes the direction attribute of arcs through MDS. Therefore, it is more suitable than previous methods [10; 16] for ARPs that need to consider the direction of traversing.

#### 4.1.2 Arc Relation Encoding via Graph Attention Network

Although AM is efficient in decision-making, according to Eq. (2), it cannot encode the edge weights between nodes in \(G\), an important context feature, during learning. Therefore, we use graph attention network (GAT)  to encode such weights. At each time step \(t\), for each arc \(arc_{i}\), we integrate the

  
**No.** & **Features** & **Field** & **Description** & **No.** & **Features** & **Field** & **Description** \\ 
**1** & \(is\_depot_{i}\) & \(_{2}\) & Is \(arc_{i}\) the depot? & **5** & \(allow\_serve_{t}^{(i)}\) & \(_{2}\) & Is \(arc_{i}\) at time step \(t\) allowed to serve? \\ 
**2** & \(cost_{i}\) & \(^{+}\) & Cost of \(arc_{i}\) & **6** & \(mds_{(i)}\) & \(^{d}\) & Euclidean coordinates of \(arc_{i}\)’s start node. \\ 
**3** & \(demand_{i}\) & \(^{+}\) & Demand of \(arc_{i}\) & **7** & \(mds_{(i)}\) & \(^{d}\) & Euclidean coordinates of \(arc_{i}\)’s end node. \\ 
**4** & \(|e_{x_{t-1}i}|\) & \(^{+}\) & Edge weight from \(arc_{x_{t-1}}\) to \(arc_{i}\). & & & \\   

Table 1: **Feature Detail of \(arc_{i}\) at time step \(t\) for CARP.**

Figure 1: **DaAM Pipeline** consists of two parts. The first part transforms the input graph \(\) by treating the arcs on \(\) as nodes of a new directed graph \(G\), only executing once. The second part leverages the GAT and AM to update arc embeddings and select arcs, executing at each time step.

weights between \(arc_{i}\) and all arcs in \(A_{R}\) along with their features into the initial embedding of \(arc_{i}\):

\[c_{ij}=softmax([\,F_{t}^{(i)}\,||\,F_{t}^{(j)}\,||\,||\,e_ {ji}\,||\,); h_{i}^{0}=\,_{j=0}^{|A_{R}| -1}c_{ij}F_{t}^{(j)} \]

where \(\) is a shared learnable parameter, \([||]\) is the horizontal concatenation operator, \(()\) is a mapping from the input to a scalar, and \(()\) denotes the activation function. \(h_{i}^{0}\) denotes the initial feature embedding of \(arc_{i}\), which is taken as the input of subsequent AM. Since \(G\) is a complete graph, we use one graph attention layer to avoid over-smoothing .

#### 4.1.3 Arc Selection via Attention Model

After aggregating the edge weights of \(G\) into the initial embeddings, we utilize AM to learn the final arc embeddings and make arc selection decisions. In the encoding phase described by Eq.2, for each arc \(\{arc_{i}\}\), we leverage \(N\) attention layers to process the initial embeddings \(\{h_{i}^{0}\}\) and obtain the output embeddings of the \(N\)th layer, i.e., \(\{h_{i}^{N}\}\). In the decoding phase, we define the context node applicable to CARP:

\[h_{(c)}^{N} =\!|A_{R}|^{-1}\!_{i=0}^{|A_{R}|-1}\!h_{i}^{ N},h_{x_{t-1}}^{N},_{t},_{t},t[1,+) \]

where \(x_{t-1}\) indicates the chosen arc index at time step \(t-1\) and \(x_{0}\) is \(arc_{0}\). \(_{t}\) is the remaining capacity at time step \(t\), \(_{t}=(_{t}>)\) is a variable to indicate whether the vehicle's remaining capacity exceeds half. Finally, according to Eq.(3), the decoder of AM takes the context node \(h_{(c)}^{N}\) and arc embeddings \(\{h_{i}^{N}\}\) as inputs and calculates the probabilities for all arcs, denoted as \(p_{i}\). The serviceable arc selected at time step \(t\), i.e., \(arc_{x_{t}}\), is determined by sampling or greedy decoding.

### Supervised Reinforcement Learning for CARP

The decision-making of selecting arcs can be modeled as a Markov decision process with the following symbols regarding reinforcement learning:

* **State \(s_{t}\)** is the newest path of arcs selected from \(G\): \((arc_{x_{0}},...,arc_{x_{t-1}})\), while the terminal state is \(s_{T}\) with \(T\) indicating the final time step.
* **Action \(a_{t}\)** is the selected arc at time step \(t\), i.e., \(arc_{x_{t}}\). Selecting the action \(a_{t}\) would add \(arc_{x_{t}}\) to the end of the current path \(s_{t}\) and tag the corresponding arcs of \(arc_{x_{t}}\) with their features \(allow\_serve\) changed to 0. Notably, \(arc_{0}\) can be selected repeatedly but not consecutively.
* **Reward \(r_{t}\)** is obtained after taking action \(a_{t}\) at state \(s_{t}\), which equals the negative shortest path cost from the last arc \(arc_{x_{t-1}}\) to the selected arc \(arc_{x_{t}}\).
* **Stochastic policy \((a_{t}|s_{t})\)** specifies the probability distribution over all actions at state \(s_{t}\).

We parameterize the stochastic policy of DaAM with \(\):

\[(x_{t}|\,,_{0:t-1})=_{}(a_{t}|s_{t}) \]

where \(\) is a CARP instance. Starting from initial state \(s_{0}\), we get a trajectory \(=(s_{0},a_{0},r_{0},...,r_{T-1},s_{T})\) using \(_{}\). The goal of learning is to maximize the cumulative reward: \(R()=_{t=0}^{T-1}r_{t}\). However, due to the high complexity of CARP, vanilla deep reinforcement learning methods learn feasible strategies inefficiently. A natural solution is to minimize the difference between the model's decisions and expert decisions. To achieve this, we employ supervised learning to learn an initial policy based on labeled data and then fine-tune the model through reinforcement learning.

#### 4.2.1 Supervised Pre-training via Multi-class Classification

In the pre-training stage, we consider arc-selection at each time step as a multi-class classification task, and employ the state-of-the-art CARP method MAENS to obtain high-quality paths as the label. Assuming that \(y_{t}^{|A_{R}|}\) denotes the one-hot label vector at time step \(t\) of any path, with \(y_{t}^{(k)}\) indicating each element. We utilize the cross-entropy loss to train the policy represented in Eq. (7):

\[L=-_{t=0}^{T-1}_{k=0}^{|A_{R}|-1}y_{t}^{(k)} _{}(arc_{k}|s_{t}) \]

We use the policy optimized by cross-entropy, denoted as \(_{s}\), to initialize the policy network \(_{}\) and as the baseline policy \(_{b}\) in reinforcement learning.

#### 4.2.2 Reinforcement Fine-tuning via PPO with self-critical strategy

During the fine-tuning phase, we use Proximal Policy Optimization (PPO) to optimize our model \(_{}(a_{t}|s_{t})\) due to its outstanding stability in policy updates. Considering the low sample efficiency in reinforcement learning, we employ a training approach similar to self-critical training  to reduce gradient variance and expedite convergence. Specifically, We use another policy \(_{b}\) to generate a trajectory and calculate its cumulative reward, serving as a baseline function. Our optimization objective is based on PPO-Clip :

\[_{(s,a)_{b}}(a|s)}{_{ b}(a|s)}R(_{s}^{})-R(_{s}^{b}),((a|s)}{_{b}(a|s)},1\!-\!,1\!+\!)R(_ {s}^{})-R(_{s}^{b}) \]

where \(s\) is used to replace current state \(s_{t}\) for symbol simplification, and \(a\) for \(a_{t}\). \((w,v_{},v_{})\) denotes constraining \(w\) within the range \([v_{},v_{}]\), and \(\) is a hyper-parameter. \(_{s}^{}\) denotes a trajectory sampled by \(_{}\) with \(s\) as the initial state, while \(_{s}^{b}\) for the trajectory greedily decoded by \(_{b}\). In greedy decoding, the action with the maximum probability is selected at each step. \(R(_{s}^{})-R(_{s}^{b})\) serves as an advantage measure, quantifying the advantage of the current policy \(_{}\) compared to \(_{b}\). We maximize Eq. (9) through gradient descent, which forces the model to select actions that yield higher advantages. The baseline policy's parameters are updated if \(_{}\) outperforms \(_{b}\).

### Path Optimization via Dynamic Programming

The complexity of the problem is heightened by the increasing capacity constraint, making it challenging for the neural network to make accurate decisions regarding the depot return positions. In this section, we propose a dynamic programming (DP) based strategy to assist our model in optimizing these positions. Assuming that \(\) is assigned with the terminal state \(s_{T}=(arc_{x_{0}},arc_{x_{1}},...,arc_{x_{T-1}})\), representing a generated path. Initially, we remove all the depot arcs in \(\) to obtain a new path \(^{{}^{}}=(arc_{x^{}_{0}},arc_{x^{}_{1}},...,arc_{x^{ }_{T^{}-1}})\), where \(\{x^{}_{i}|i[0,T^{}\!-\!1]\}\) denotes a subsequence of \(\{x_{i}|i[0,T\!-\!1]\}\). Subsequently, we aim to insert several new depot arcs into the path \(^{{}^{}}\) to achieve a lower cost while adhering to capacity constraints. To be specific, we recursively find the return point that minimizes the overall increasing cost, which is implemented by the state transition equation as follows:

\[f(^{{}^{}})=_{i}(f(^{{}^{}}_{0:i})+SC(arc_{x^{ }_{i}},arc_{0})+SC(arc_{0},arc_{x^{}_{i+1}})-SC(arc_{x^{}_{i }},arc_{x^{}_{i+1}}))\]

\[ 0\ i<T^{{}^{}}-1,_{j=i+1}^{T^{{}^{ }}-1}demand_{x^{}_{j}} Q \]

where \(SC(arc_{x^{}_{i}},arc_{0})=|e_{x^{}_{0}}|\) denotes the shortest path cost from \(arc_{x^{}_{i}}\) to the depot. \(Q\) is the vehicle capacity. According to Eq. (10), we insert the depot arc \(arc_{0}\) after an appropriate position \(arc_{x^{}_{i}}\), which meets with the capacity constraint of the subpath \(^{{}^{}}_{i+1:T^{{}^{}}-1}\). \(f()\) denotes a state featuring dynamic programming. By enumerating the position \(i\), we compute the minimum increasing cost \(f(^{{}^{}})\) utilizing its sub-state \(f(^{{}^{}}_{0:i})\). The final minimum cost for path \(\) is \(f(^{})+g(^{})\), here \(g(^{})\) is the unoptimized cost of \(^{}\). Since \(^{{}^{}}\) includes only the required edges, i.e., \(T^{}=|_{R}|\), the time complexity of DP is \((|_{R}|^{2})\). During Path Optimization, we use beam search to generate two paths with the trained policy, one with capacity-constrained and one without. Both paths are optimized using DP and the one with the minimum cost is selected as the final result.

## 5 Experiments

### Setup

Problem Instances.We extracted sub-graphs from the roadmap of Beijing, China, obtained from OpenStreetMap , to create CARP instances for both the training and testing phases. All instances are divided into seven datasets, each representing different problem scales, as presented in Table 2. Each dataset consists of 30,000 instances, further divided into two **disjoint** subsets: 20,000 instances for training and the remaining for testing. For each instance, the vehicle capacity is set to 100.

Implementation Details.Our neural network is implemented using the PyTorch framework and trained on a single NVIDIA RTX 3090 GPU. The heuristics and metaheuristics algorithms areevaluated on an Intel Core i9-7920X with 24 cores and a CPU frequency of 4.4GHz. We optimize the model using Adam optimizer . The dimension of MDS coordinates \(d\) is set to 8, and the learning rate is set to \(1e^{-4}\). We set \(\) in the PPO training at 0.1. Notably, our PPO training does not incorporate discounted cumulative rewards, i.e., \(\) is set to 1.

Metrics and Settings.For each method and dataset, We compute the mean tour cost across all test instances, indicated by "Cost". Employing the state-of-the-art MAENS  as a baseline, we measure the "Cost" gap between alternative algorithms and MAENS, indicated by "Gap". We compare our method against the heuristic Path-Scanning algorithms (PS)  and two NN-based algorithms. In the absence of publicly available code for prior NN-based CARP methods, we modify two NN-based NRP solvers to suit CARP, i.e, S2V-DQN  and VRP-DL . Note that, for S2V-DQN, we replace structure2vec with GAT to achieve more effective graph embedding learning. For our method, we incrementally add supervised pre-training (SL), reinforcement learning fine-tuning (RL), and path optimization (PO) to assess the effectiveness of our training scheme and optimization, respectively. Due to the excessively long computation times of MAENS on larger-scale datasets, SL is only performed on Task20, Task 30, and Task40. The batch size for SL is set to 128. During the RL stage, greedy decoding is used to generate solutions, and except for the Task20 dataset, we utilize the training results obtained from the preceding smaller-scale dataset to initialize the model. The beam width in the PO stage is set to 2. For each dataset, we compare the mean cost of different methods on 10,000 problem instances.

### Evaluation Results

Solution QualityTable 6 shows the result. Our algorithm outperforms all heuristic and NN-based methods across all scales, achieving costs comparable to MAENS, trailing by less than \(8\%\). The advantage over PS demonstrates that neural networks can learn more effective policies than hand-crafted ones, attributed to our well-designed modeling approach. Moreover, as the problem scale increases, it becomes time-consuming to obtain CARP annotation by MAENS. Therefore, we leverage the model pre-trained on small-scale instances as the initial policy for RL fine-tuning on Task50, Task60, Task80, and Task100, yielding commendable performance. This proves the generalization of our training scheme across varying problem scales. The performance gap with MAENS highlights our algorithm's superiority in CARP-solving approaches.

    &  &  &  &  &  \\  & Cost & Gap (\%) & Cost & Gap (\%) & Cost & Gap (\%) & Cost & Gap (\%) \\  MAENS  & 474 & 0 & 950 & 0 & 1529 & 0 & 2113 & 0 & 2757 & 0 \\ PS  & 544 & 14.72 & 1079 & 13.56 & 1879 & 22.84 & 2504 & 18.49 & 3361 & 21.90 \\ PS-Ellipse  & 519 & 9.49 & 1006 & 5.89 & 1709 & 11.77 & 2299 & 8.80 & 3095 & 12.26 \\ PS-Efficiency  & 514 & 8.44 & 1007 & 6.00 & 1684 & 10.14 & 2282 & 8.00 & 3056 & 10.85 \\ PS-Alt  & 514 & 8.44 & 1007 & 6.00 & 1685 & 10.20 & 2283 & 8.04 & 3057 & 10.88 \\ PS-Alt2  & 521 & 9.92 & 1009 & 6.21 & 1720 & 12.49 & 2314 & 9.51 & 3102 & 12.51 \\ S2V-DQN*  & 590 & 24.42 & 1197 & 26.02 & 1900 & 24.23 & 2820 & 33.43 & 3404 & 23.42 \\ VRP-DL*  & 528 & 11.39 & 1193 & 25.57 & 2033 & 32.96 & 2898 & 37.15 & 3867 & 40.26 \\  DaAM (SL) & 509 & 7.43 & 1066 & 12.24 & - & - & - & - & - \\ DaAM (SL+RL) & 495 & 4.48 & 1009 & 6.19 & 1639 & 7.16 & 2275 & 7.67 & 2980 & 8.06 \\ DaAM (SL+RL+PO) & 482 & **1.65** & 992 & **4.39** & 1621 & **5.98** & 2255 & **6.70** & 2958 & **7.28** \\   

Table 3: **Solution quality comparison**. All methods are evaluated on 10,000 CARP instances in each scale. We measure the gap (%) between different methods and MAENS. Methods marked with an asterisk were originally proposed for NRP, but we modified them to solve CARP. The best results are indicated in bold, while the second-best results are underlined.

Generalization Ability.In Table 4, we assess DaAM's generalization on large-scale CARP instances using the policy trained on Task100. We remove MAENS and PS due to failing to run on large-scale graphs, and remove S2V-DQN and VRP-DL due to poor performance. Although DaAM is not trained on large-scale instances, it achieves or even exceeds the performance of PS, which shows its potential application on larger-scale CARP instances.

Run Time.We compare the total time required for solving 100 CARP instances across datasets Task20 to Task100 datasets using our method, MAENS, and PS algorithms, and show the run time in log space. For datasets Task200 to Task600, we compare the same metric using variants of PS and out method. For our method, we measured the solving time with and without PO. Fig. 2 demonstrates that our method exhibits a significant speed advantage over MAENS, even outperforming variants of PS  on most datasets. In comparison, the consumption time of MAENS increases exponentially as the problem scale increases. Our method efficiently generates paths for large-scale CARP instances by leveraging GPU data-level parallelism and CPU instruction-level parallelism.

Effectiveness of Combining MDS and GATTo evaluate the combination of MDS and GAT for embedding exhibiting, we individually evaluate the performance of models using only MDS or GAT, as well as their combined performance. The experiment is conducted on Task30, Task40, Task50, and Task60 by comparing the average performance of 1,000 instances on each dataset. In the RL stage, we use the policy pre-trained on Task30 for initialization. Table 5 indicates that using MDS or GAT individually yields worse quality in most cases, highlighting that combining MDS and GAT enhances the model's capacity to capture arc correlations. Fig. 3 depicts the convergence trends in these scenes, which shows that the synergy of MDS and GAT contributes to the stability of training.

Solution Visualization.For a more intuitive understanding of the paths generated by different methods, we visualize and compare the results of our method with PS  and MAENS across four road scenes in Beijing. Fig. 4 visualizes all results alongside scene information. We observe that our model obtains similar paths with MAENS since we leverage the annotation generated by MAENS for

    & Task200 & Task300 & Task400 & Task500 & Task600 \\  & Cost & Cost & Cost & Cost & Cost \\  PS-Ellipse  & 4240 & 6563 & 8600 & 10909 & 13377 \\ PS-Efficiency  & 4233 & 6544 & 8583 & 10883 & 13338 \\ PS-Alt1  & 4233 & 6544 & 8580 & 10884 & 13338 \\ PS-Alt2  & 4244 & 6569 & 8606 & 10922 & 13393 \\  DaAM (SL+RL) & 4189 & 6372 & 8610 & 10938 & 13340 \\ DaAM (SL+RL+PO) & **4132** & **6281** & **8473** & **10633** & **13100** \\   

Table 4: **Generalization to larger problem instances**. All methods are evaluated on 10,000 CARP instances in each scale. For DaAM, we employ the policy trained on Task100. The best results are indicated in bold, while the second-best results are underlined.

Figure 2: **Run time comparison**. For each dataset, the total run time of each method on 100 CARP instances is shown.

supervised learning. MAENS paths exhibit superior spatial locality, clearly dividing the scene into regions, whereas PS paths appear more random.

## 6 Conclusion and Limitations

In this paper, we propose a learning-based CARP solver that competes with state-of-the-art meta-heuristics. Firstly, we encode the potential serving direction of edges into embeddings, ensuring that edge directionality is taken into account in decision-making. Secondly, we present a supervised reinforcement learning approach that effectively learns policies to solve CARP. With the aid of these contributions, our method surpasses all heuristics and achieves performance comparable to metaheuristics for the first time while maintaining excellent efficiency.

Limitations and future work.Decomposing undirected edges increases the decision elements, which complicates the problem and may widens the gap between DaAM and traditional state-of-the-art approaches as the problem instance scale increases. Our future work focuses on designing an efficient graph transformation method that does not significantly increase problem complexity.

Figure 4: **Qualitative comparison** in four real street scenes. The paths are marked in different colors, with gray indicating roads that do not require service and red points indicating depots.

Figure 3: **Convergence trends of different methods in reinforcement learning training.**