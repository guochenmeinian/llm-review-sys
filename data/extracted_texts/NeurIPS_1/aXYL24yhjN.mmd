# Mind the Gap: A Causal Perspective on Bias Amplification in Prediction & Decision-Making

Drago Plecko

Elias Bareinboim

Causal Artificial Intelligence Lab

Columbia University

dp3144@columbia.edu, eb@cs.columbia.edu

###### Abstract

As society increasingly relies on AI-based tools for decision-making in socially sensitive domains, investigating fairness and equity of such automated systems has become a critical field of inquiry. Most of the literature in fair machine learning focuses on defining and achieving fairness criteria in the context of prediction, while not explicitly focusing on how these predictions may be used later on in the pipeline. For instance, if commonly used criteria, such as independence or sufficiency, are satisfied for a prediction score \(S\) used for binary classification, they need not be satisfied after an application of a simple thresholding operation on \(S\) (as commonly used in practice). In this paper, we take an important step to address this issue in numerous statistical and causal notions of fairness. We introduce the notion of a margin complement, which measures how much a prediction score \(S\) changes due to a thresholding operation. We then demonstrate that the marginal difference in the optimal 0/1 predictor \(\) between groups, written \(P( x_{1})-P( x_{0})\), can be causally decomposed into the influences of \(X\) on the \(L_{2}\)-optimal prediction score \(S\) and the influences of \(X\) on the margin complement \(M\), along different causal pathways (direct, indirect, spurious). We then show that under suitable causal assumptions, the influences of \(X\) on the prediction score \(S\) are equal to the influences of \(X\) on the true outcome \(Y\). This yields a new decomposition of the disparity in the predictor \(\) that allows us to disentangle causal differences inherited from the true outcome \(Y\) that exists in the real world vs. those coming from the optimization procedure itself. This observation highlights the need for more regulatory oversight due to the potential for bias amplification, and to address this issue we introduce new notions of _weak_ and _strong_ business necessity, together with an algorithm for assessing whether these notions are satisfied. We apply our method to three real-world datasets and derive new insights on bias amplification in prediction and decision-making.

## 1 Introduction

Automated systems based on machine learning and artificial intelligence are increasingly used for decision-making in a variety of real-world settings. These applications include hiring decisions, university admissions, law enforcement, credit lending and loan approvals, health care interventions, and many other high-stakes scenarios in which the automated system may significantly affect the well-being of individuals (Khandani et al., 2010; Mahoney and Mohen, 2007; Brennan et al., 2009). In this context, society is increasingly concerned about the implications and consequences of using automated systems, compared to the currently implemented decision processes. Prior works highlight the potential of automated systems to perpetuate or even amplify inequities between demographic groups, with a range of examples from decision support systems for (among others) sentencingAngwin et al. (2016), face-detection Buolamwini and Gebru (2018), online advertising Sweeney (2013), Datta et al. (2015), and authentication Sanburn (2015). Notably, issues of unfairness and discrimination are also pervasive in settings in which decisions are made by humans. Some well-studied examples include the gender pay gap, supported by a decades-long literature (Blau and Kahn, 1992, Blau and Kahn, 1992, Blau et al., 2017), or the racial bias in criminal sentencing (Sweeney and Haney, 1992, Pager, 2003), just to cite a few. Therefore, AI systems designed to make decisions may often be trained with data that contains various historical biases and past discriminatory decisions against certain protected groups, constituting a large part of the underlying problem. In this work, we specifically focus on investigating when automated systems may potentially lead to an even more discriminatory process, possibly amplifying already existing differences between groups.

Within this context, it is useful to distinguish between different tasks appearing in the growing literature on fair machine learning. One can distinguish three specific and different tasks, namely (1) bias detection and quantification for exisiting outcomes or decision policies; (2) construction of fair predictions of an outcome; (3) construction of fair decision-making policies that are intended to be implemented in the real-world. Interestingly, a large portion of the literature in fair ML focuses on the second task of fair prediction, and what is often left unaddressed is how these predictions may be used later on in the pipeline, and what kind of consequences they may have. For instance, consider a prediction score \(S\) for a binary outcome \(Y\) that satisfies well-known fairness criteria, such as independence (demographic parity (Darlington, 1971)) or sufficiency (calibration (Chouldechova, 2017)). After a simple thresholding operation, commonly applied in settings with a binary outcome, the resulting predictor is no longer guaranteed to satisfy independence or sufficiency, and the previously provided fairness guarantees may be entirely lost. The same behavior can be observed for numerous other measures.

These difficulties do not apply only to statistical measures of fairness. Recently, a growing literature has explored causal approaches to fair machine learning (Kusner et al., 2017; Kilbertus et al., 2017; Nabi and Shpitser, 2018; Zhang and Bareinboim, 2018, 2019; Chiappa, 2019; Plecko and Meinshausen, 2020; Plecko and Bareinboim, 2024), which have two major benefits. First, they allow for human-understandable and interpretable definitions and metrics of fairness, which are tied to the causal mechanisms transmitting the change between groups. Secondly, they offer a language that is aligned with the legal notions of discrimination, such as the disparate impact doctrine. In particular, causal approaches allow for considerations of business necessity - which aim to ellucidate which covariates may be justifiably used by decision-makers even if their usage implies a disparity between groups. However, causal approaches to fairness also suffer from the above-discussed issues - namely, a guarantee of absence of a causal influence from the protected attribute \(X\) onto a predictor \(S\) need not hold true after the predictor is thresholded (Plecko and Bareinboim, 2024). Therefore, within the causal approach, there is also a major need for a better understanding of how probabilistic predictions are translated into binary predictions or decisions.

In this work, we take an important step in the direction of addressing this issue. We work in a setting with a binary label \(Y\), and the goal is to provide a binary prediction \(\) or a binary decision \(D\). Our approach is particularly suitable for settings in which the utility of the decision is monotonic with respect to the conditional probability of \(Y\) being positive, written \(P(Y)\), but the developed tools also have ramifications for more general utilities. Examples that fall under our scope are numerous; for instance, the utility of admitting a student to the university (\(D\)) is often monotonic in the probability that the student successfully graduates (\(Y\)). In the context of criminal justice, decisions of detention (\(D\)) are used to prevent recidivism, and the utility of the decision is monotonic in the probability that the individual recidivates (\(Y\)). Finally, various preventive measures in healthcare (\(D\), such as vaccination, screening tests, etc.) are considered to be best applied to individuals with the highest risk of developing a target disease or suffering a negative outcome (\(Y\)). We now provide an illustrative two-variable example for one of the key insights of our paper:

**Example 1** (Disparities in Hiring).: _Consider a company deciding to hire employees using an automated system for the first time. From a previous hiring cycle when humans were in charge, the company has access to data on gender \(X\) (\(x_{0}\) for female, \(x_{1}\) for male) and the hiring outcome \(Y\) (1 for being hired, 0 otherwise). The true underlying mechanisms of the system are given by:_

\[X (U_{X}<0.5) \] \[Y (U_{Y}<p_{0})X=x_{0} \\ (U_{Y}<p_{1})X=x_{1} \]_where \(U_{X},U_{Y}\). In words, an applicant is female with a 50% probability, and the probability of being hired as a female \(x_{0}\) is \(p_{0}\), whereas for males \(x_{1}\) the probability is \(p_{1}\). The company finds the optimal prediction score \(S\) to be \(S(x)=p_{x}\). The optimal 0/1 predictor \(\), which will also be the company's decision, is given by \((x)=(S(x))\), meaning that the company will threshold the predictions at \(\). Suppose that \(p_{0}=0.49,p_{1}=0.51\), and consider the visualization in Fig. 1. The probability \(p_{0}=0.49\) means that 49 out of 100 females were hired, while 51 were not. After thresholding, \((x_{0})=(p_{0})=0\) for each female applicant, meaning that the thresholding operation maps the prediction of each individual to \(0\), even though 49/100 would have a positive outcome (Fig. 1 right). Similarly, for \(p_{1}=0.51\), we have that 51/100 male applicants would be hired, resulting in a thresholded predictor \((x_{1})=1\) for all the applicants, even though 49/100 would not have a positive outcome (Fig. 1 left). Therefore, the gender disparity in hiring after introducing the automated predictor \(\) is 100%, compared to a 2% disparity in the outcome \(Y\) before introducing \(\). Formally, the disparity in the optimal 0/1 predictor \(P( x_{1})-P( x_{0})=(p_{1} )-(p_{0})\) can be decomposed as:_

\[P( x_{1})-P( x_{0})=-p_{0}}_{ }+((p_{1})-p1}_{ }-((p_{0})-p_{0})}_{}. \]

_Term I measures the disparity coming from the true outcome \(Y\) (2% disparity), which is equal to the disparity in the prediction score \(S\), written \(P(s x_{1})-P(s x_{0})\). Term II measures the contribution coming from thresholding the prediction score to obtain an optimal 0/1 prediction (98% disparity). \(\)_

The above example illustrates a canonical point in a simple setting: a small disparity in the outcome \(Y\), and consequently the prediction score \(S\), may result in a large disparity in the optimal 0/1 predictor \(\), a case we call _bias amplification_. Contrary to this, a large disparity in \(S\) may also result in a small disparity in \(\), a case we call _bias amelioration_.

In the remainder of the manuscript, our goal is to provide a decomposition of the disparity in a thresholded predictor \(\) into the disparity in true outcome \(Y\) and the disparity originating from optimization procedure, but _along each causal pathway_ between the protected attribute \(X\) and the predictor \(\). In particular, our contributions are the following:

1. We introduce the notion of margin complement (Def. 1), and provide a path-specific decomposition of the disparity in the 0/1 predictor \(\) into its contributions from the optimal score predictor \(S\) and the margin complement \(M\) (Thm. 1),
2. We prove that under suitable assumptions, the causal decomposition of the optimal prediction score \(S\) is equivalent with the causal decomposition of the true outcome \(Y\) (Thm. 2). This allows us to obtain a new decomposition of the disparity in \(\) into contributions from \(Y\) and the margin complement \(M\) (Cor. 3),
3. Motivated by the above decompositions, we introduce a new concept of weak and strong business necessity (Def. 3), highlighting a new need for regulatory instructions in the context of automated systems. We provide an algorithm for assessing fairness under considerations of weak and strong business necessity (Alg. 1),

Figure 1: Visualization of hiring disparities from Ex. 1.

4. We provide identification, estimation, and sample influence results for all of the quantities relevant to the above framework (Props. 4, 5). We evaluate our approach on three real-world examples (Ex. 2-3) and provide new empirical insights into bias amplification.

Our work is related to the previous literature on causal fairness and the causal decompositions appearing in this literature (Zhang and Bareinboim, 2018, Plecko and Bareinboim, 2024). It is also related to previous literature on studying business necessity requirements through a causal lens (Kilbertus et al., 2017, Plecko and Bareinboim, 2024). However, our approach offers an entirely new causal decomposition into contributions from the true outcome \(Y\) and the margin complement \(M\). More broadly, our work is also related to the literature on fair decision-making, which analyzes how prediction scores impact the fairness of decisions (Chouldechova, 2017, Dwork et al., 2020, Chouldechova and Roth, 2018), or how disparities evolve over time (Liu et al., 2018). Recent results also show that focusing purely on prediction, and ignoring decision-making aspects, may lead to inequitable outcomes and cause harm to marginalized groups (Plecko and Bareinboim, 2024, Nilforoshan et al., 2022, Plecko and Bareinboim, 2024), highlighting a need to expand focus from narrow statistical definitions of fair predictions to a more comprehensive understanding of equity in algorithmic decisions. Still, many questions remain open in the context of fair decision-making, and more future works are required in this area. Finally, we mention that our work is also related to the literature on auditing and assessing fairness of decisions made by humans (Pierson et al., 2021, Kleinberg et al., 2018), and understanding how AI systems may help humans overcome their biases (Imai et al., 2023).

### Preliminaries

We use the language of structural causal models (SCMs) (Pearl, 2000). An SCM is a tuple \(:= V,U,,P(u)\), where \(V\), \(U\) are sets of endogenous (observable) and exogenous (latent) variables, respectively, \(\) is a set of functions \(f_{V_{i}}\), one for each \(V_{i} V\), where \(V_{i} f_{V_{i}}((V_{i}),U_{V_{i}})\) for some \((V_{i}) V\) and \(U_{V_{i}} U\). The set \((V_{i})\) is called the parent set of \(V_{i}\). \(P(u)\) is a strictly positive probability measure over \(U\). Each SCM \(\) is associated to a causal diagram \(\)(Bareinboim et al., 2022) over the node set \(V\) where \(V_{i} V_{j}\) if \(V_{i}\) is an argument of \(f_{V_{j}}\), and \(V_{i} V_{j}\) if the corresponding \(U_{V_{i}},U_{V_{j}}\) are not independent. An instantiation of the exogenous variables \(U=u\) is called a _unit_. By \(Y_{x}(u)\) we denote the potential response of \(Y\) when setting \(X=x\) for the unit \(u\), which is the solution for \(Y(u)\) to the set of equations obtained by evaluating the unit \(u\) in the submodel \(_{x}\), in which all equations in \(\) associated with \(X\) are replaced by \(X=x\). Throughout the paper, we assume a specific cluster causal diagram \(_{}\) known as the standard fairness model (SFM) (Plecko and Bareinboim, 2024) over endogenous variables \(\{X,Z,W,Y,\}\) shown in Fig. 2 (see also (Anand et al., 2023)). The SFM consists of the following: _protected attribute_, labeled \(X\) (e.g., gender, race, religion), assumed to be binary; the set of _confounding_ variables \(Z\), which are not causally influenced by the attribute \(X\) (e.g., demographic information, zip code); the set of _mediator_ variables \(W\) that are possibly causally influenced by the attribute (e.g., educational level or other job-related information); the _outcome_ variable \(Y\) (e.g., GPA, salary); the _predictor_ of the outcome \(\) (e.g., predicted GPA, predicted salary). The SFM also encodes the lack-of-confounding assumptions typically used in the causal inference literature. The availability of the SFM and the implied assumptions are a possible limitation of the paper, while we note that partial identification techniques for bounding effects can be used for relaxing them (Zhang et al., 2022).

## 2 Margin Complements

We begin by introducing a quantity that plays a key role in the results of this paper.

**Definition 1** (Margin Complement).: _Let \(U=u\) be a unit, and let \(S\) denote a prediction score for a binary outcome \(Y\). Let the subscript \(C\) denote a counterfactual clause, so that \(Z_{C}\) denotes a potential response. The margin complement \(M\) of the score \(S\) for the unit \(U=u\) and threshold \(t\) is defined as:_

\[M(u)=(S(u) t)-S(u). \]

_A potential response of \(M\), labeled \(M_{C}\), is given by \(M_{C}(u)=(S_{C}(u) t)-S_{C}(u)\)._

Figure 2: Standard Fairness Model.

In words, the margin complement for a unit \(U=u\) represents the difference in the score after thresholding vs. the score that would happen naturally.

**Example 1** (Disparities in Hiring continued).: _Consider the hiring example with the SCM in Eqs. 1-2 with \(p_{0}=0.49,p_{1}=0.51\). The unit \((U_{X},U_{Y})=(0,0)\) corresponds to a male applicant (\(X(u)=1\)) who was hired (\(Y(u)=1\)). We have \(S(u)=p_{1}=0.51\), and \(M(u)=(S(u) 0.5)-S(u)=1-0.51=0.49\). For this \(u\), the margin complement indicates that the predicted outcome \((u)=(S(u) 0.5)\) is 49% greater than the predicted probability \(S(u)\). The same computation can be done for a female \(X(u^{})=0\), in which case \(M(u^{})=(p_{x_{0}} 0.5)-p_{x_{0}}=-0.49\), meaning that the predicted outcome \((u^{})\) is 49% smaller than the predicted probability \(S(u^{})\). _

Given a prediction score \(S\) and a threshold \(t\), the margin complement \(M\) tells us in which direction the thresholded version \((S(u) t)\) moves compared to the score \(S(u)\). A positive margin complement indicates that a thresholded predictor is larger than the probability prediction, and a negative margin complement the opposite. A similar reasoning holds for the potential responses of the margin complement \(M_{C}\): we are interested in what the margin complement _would have been_ for an individual \(U=u\) under possibly different, counterfactual conditions described by \(C\). As we demonstrate shortly, margin complements (and their potential responses) play a major role in explaining how inequities are generated between groups at the time of decision-making. In this section, our key aim is to analyze the optimal 0/1 predictor \(\) and provide a decomposition of its total variation measure (TV, for short), defined as \(_{x_{0},x_{1}}()=P( x_{1})-P(  x_{0})\). When working with the causal diagram in Fig. 2, we can notice that the TV measure comprises of three types of variations coming from \(X\): the direct effect \(X\), the mediated effect \(X W\), and the confounded effect \(X Z\). Our goal is to construct a decomposition of the TV measure that allows us to distinguish how much of each of the causal effects is due to a difference in the prediction score \(S\), and how much due to margin complements \(M\). To investigate this, we first introduce the known definitions of direct, indirect, and spurious effects from the causal fairness literature:

**Definition 2** (\(x\)-specific Causal Measures (Zhang and Bareinboim, 2018; Plecko and Bareinboim, 2024)).: _The \(x\)-specific [direct, indirect, spurious] effects of \(X\) on \(Y\) are defined as:_

\[x-1.0pt_{x_{0},x_{1}}(y x) =P(y_{x_{1},W_{x_{0}}} x)-P(y_{x_{0}} x) \] \[x-1.0pt_{x_{1},x_{0}}(y x) =P(y_{x_{1},W_{x_{0}}} x)-P(y_{x_{1}} x)\] (6) \[x-1.0pt_{x_{1},x_{0}}(y) =P(y_{x_{1}} x_{0})-P(y_{x_{1}} x_{1}). \]

Armed with these definitions, we can prove the following result:

**Theorem 1** (Causal Decomposition of Optimal 0/1 Predictor).: _Let \(\) be the optimal predictor with respect to the 0/1-loss based on covariates \(X,Z,W\). Let \(S\) denote the optimal predictor with respect to the \(L_{2}\) loss. The total variation (TV, for short) measure of the predictor \(\), written as \(P( x_{1})-P( x_{0})\), can be decomposed into direct, indirect, and spurious effects of \(X\) on the score \(S\) and the margin complement \(M\) as follows:_

\[_{x_{0},x_{1}}() =x-1.0pt_{x_{0},x_{1}}(s x_{0})+x -1.0pt_{x_{0},x_{1}}(m x_{0}) \] \[-x-1.0pt_{x_{1},x_{0}}(s x _{0})+x-1.0pt_{x_{1},x_{0}}(m x_{0})\] (9) \[-x-1.0pt_{x_{1},x_{0}}(s)+x -1.0pt_{x_{1},x_{0}}(m). \]

The above theorem is the first key result of this paper. The disparity between groups with respect to the optimal 0/1-loss predictor, measured by \(_{x_{0},x_{1}}()\) can be decomposed into direct, indirect, and spurious contributions coming from (i) the optimal \(L_{2}\)-loss predictor \(S\) (e.g., term \(x-1.0pt_{x_{0},x_{1}}(s x_{0})\)), and (ii) the margin complement \(M\) (e.g., term \(x-1.0pt_{x_{0},x_{1}}(m x_{0})\)). This provides a unique capability since for each causal pathway (direct, indirect, spurious) the contribution coming from the probability prediction \(S\) can be disentangled from the contribution coming from the optimization procedure itself (i.e., the rounding of the predictor). The former, as we will see shortly, is simply a representation of the bias already existing in the true outcome \(Y\), whereas the latter represents a newly introduced type of bias that is the result of using an automated system. The contribution of the margin complement may act to both ameliorate or amplify an existing disparity, a point we investigate later on.

**Example 1** (Disparities in Hiring extended).: _Consider the hiring example from Ex. 1 extended with a mediator \(W\) indicating whether the applicant has a PhD degree (\(W=1\)) or not (\(W=0\)). Suppose

[MISSING_PAGE_EMPTY:6]

doctrine is the notion of _business necessity_ (BN), which allows certain variables correlated with the protected attribute to be used for prediction due to their relevance to the business itself  (or more broadly the utility of the decision-maker). Based on the decomposition from Cor. 3, new BN considerations emerge:

**Definition 3** (Weak and Strong Business Necessity).: _Let \(\) be an SCM compatible with the Standard Fairness Model. Let CE denote a causal pathway (DE, IE, or SE), and let \(x,x^{}\) be two distinct values of \(X\). Let \(x^{}\) be a third, arbitrary value of \(X\). If a causal pathway does not fall under business necessity, then we require:_

\[x_{x,x^{}}(s x^{})=x_{x,x^{}}(m  x^{})=0. \]

_A pathway is said to satisfy weak business necessity if:_

\[x_{x,x^{}}(s x^{})=x_{x,x^{}}( y x^{}),\ x_{x,x^{}}(m x^{})=0. \]

_A pathway is said to satisfy strong business necessity if:_

\[x_{x,x^{}}(s x^{})=x_{x,x^{}} (y x^{}),\ \ x_{x,x^{}}(m x^{ }). \]

The above definition distinguishes between three important cases, and sheds light on a new aspect of the concept of business necessity. According to the definition, there are three versions of BN considerations:

1. A causal pathway is not in the BN set, and is considered discriminatory. In this case, both the contribution of the prediction score \(S\) and the margin complement \(M\) need to be equal to \(0\) (i.e., no discrimination is allowed along the pathway),
2. A causal pathway satisfies weak BN, and is not considered discriminatory. In this case, the effect of \(X\) on the prediction score \(S\) needs to equal the effect of \(X\) onto the true outcome \(Y\) along the same pathway . However, the contribution of the margin complement \(M\) along the pathway needs to equal \(0\).
3. A causal pathway satisfies strong BN, and is not considered discriminatory. Similarly as for weak necessity, the effect of \(X\) on \(S\) needs to equal the effect of \(X\) on \(Y\), but in this case, the contribution of the margin complement \(M\) is unconstrained.

The distinction between cases (2) and (3) opens the door for new regulatory requirements and specifications. In particular, whenever a causal effect is considered non-discriminatory, the attribute \(X\) needs to affect \(S\) to the extent to which it does in the real world. However, the system designer also needs to decide whether a difference existing in the predicted probabilities \(S\) is allowed to be amplified (or ameliorated) by means of rounding. The latter point distinguishes between weak and strong BN, and should be a consideration of any system designer issuing binary decisions. In Alg. 1, we propose a formal approach for evaluating considerations of weak and strong BN for any input of a predictor \(\) and a prediction score \(S\).

Identification, Estimation, and Sample Influence

In Thm. 1 and Cor. 3 the observed disparity in the TV measure of the optimal 0/1 predictor is decomposed into its constitutive components. The quantities appearing in the decomposition are counterfactuals, and thus the question of _identification_ of these quantities needs to be addressed. In other words, we need to understand whether these quantities can be uniquely computed based on the available data and the causal assumptions. The following is a positive answer:

**Proposition 4** (Identification and Estimation of Causal Measures).: _Let \(\) be an SCM compatible with the Standard Fairness Model, and let \(P(V)\) be its observational distribution. The \(x\)-specific direct, indirect, and spurious effects of \(X\) on the outcome \(Y\), predictor \(\), prediction score \(S\), and the margin complement \(M\) are identifiable (uniquely computable) from \(P(V)\) and the SFM. Denote by \(f(x,z,w)\) estimator of \([T x,z,w]\), and by \((x v^{})\) the estimator of the probability \(P(x v^{})\) for different choices of \(v^{}\). For \(T\{Y,,S,M\}\), the effects can be estimated as:_

\[x^{}_{x_{0},x_{1}}(t x_{0}) =_{i=1}^{n}[f(x_{1},w_{i},z_{i})-f(x_{0},w_{i},z_{ i})](x_{0} w_{i},z_{i})}{(x_{0})} \] \[x^{}_{x_{1},x_{0}}(t x_{0}) =_{i=1}^{n}f(x_{1},w_{i},z_{i}) (x_{0} w_{i},z_{i})}{(x_{0})}-(x_{1} w_{i},z_{i})} {(x_{1} z_{i})}(x_{0} z_{i})}{(x_{0})}\] (25) \[x^{}_{x_{1},x_{0}}(t) =_{i=1}^{n}f(x_{1},w_{i},z_{i}) (x_{1} w_{i},z_{i})}{(x_{1} z_{i})}(x_{0} z_{i} )}{(x_{0})}-(x_{1} w_{i},z_{i})}{(x_{1})}. \]

The proof of the proposition, together with the identification expressions for the different quantities can be found in Appendix B. We next define the sample influences for the different estimators:

**Definition 4** (Sample Influence).: _The sample influence of the \(i\)-th sample on the estimator \(x^{}\) of the causal effect CE is given by corresponding term in the summations in Eqs. 24-26. For instance, the \(i\)-th sample influence on \(x^{}_{x_{0},x_{1}}(t x_{0})\) is given by (and analogously for IE, SE terms):_

\[(i)=[f(x_{1},w_{i},z_{i})-f(x_{0},w_{i},z_{i})](x_{0}  w_{i},z_{i})}{(x_{0})}. \]

The sample influences tell us how each of the samples contributes to the overall estimator of the quantity. These sample-level contributions may be interesting to investigate from the point of view of the system designer, including identifying any subpopulations that are discriminated against. For direct sample influences, the following proposition can be proved:

**Proposition 5** (Direct Effect Sample Influence).: _The SI-DE\((i)\) in Eq. 27 is an estimator of_

\[[T_{x_{1},W_{x_{0}}}-T_{x_{0}} x_{0},z_{i},w_{i}] {P(w_{i},z_{i} x_{0})}{P(w_{i},z_{i})}, \]

_where \([T_{x_{1},W_{x_{0}}}-T_{x_{0}} x_{0},z_{i},w_{i}]\) is the \((x_{0},z_{i},w_{i})\)-specific direct effect of \(X\) on \(T\)._

Prop. 5 demonstrates an important point - namely that the sample influences along the direct path are not just quantities of statistical interest, but also _causally_ meaningful quantities. In particular, the influence of the \(i\)-th sample is proportional to the direct effect of the \(x_{0} x_{1}\) transition for the group of units \(u\) compatible with the event \(x_{0},z_{i},w_{i}\). The influence is further proportional to \(P(w_{i},z_{i} x_{0})/P(w_{i},z_{i})\) that measures how much more likely the covariates \(z_{i},w_{i}\) of the \(i\)-th sample are in the \(X=x_{0}\) group (for which the discrimination is quantified) vs. the overall population. Therefore, practitioners also have a causal reason for investigating these sample influences.

## 5 Experiments

We analyze the MIMIC-IV (Ex. 2), COMPAS (Ex. 3), and Census (Ex. 4, Appendix C) datasets.

**Example 2** (Acute Care Triage on MIMIC-IV Dataset [Johnson et al., 2023]).: _Clinicians in the Beth Israel Deaconess Medical Center in Boston, Massachusetts treat critically ill patients admitted to the intensive care unit (ICU). For all patients, various physiological and treatment information is collected 24 hours after admission, and the available data consists of (grouped into the Standard Fairness model): protected attribute \(X\), in this case race (\(x_{0}\) African-American, \(x_{1}\) White), set of confounders \(Z=\{\)sec, age, chronic health status\(\}\), set of mediators \(W=\)[lactate, SOFA score, admission diagnosis, PaO\({}_{2}\)/FiO\({}_{2}\) ratio, aspartate aminotransferase]._

_Clinicians are interested in patients who require closer monitoring. They want to determine the top half of the patients who are the most likely to (i) die during their hospital stay; (ii) have an ICU stay longer than 10 days. This combined outcome is labeled \(Y\). These high-risk patients will remain in the most acute care unit. To predict the outcome, clinicians use the electronic health records (EHR) data of the hospital and construct score predictions \(S\) and a binary predictor \(=(S(x,z,w)>(0.5;S))\) that selects the top half of the patients._

_To investigate the fairness implications of the new AI-based system, they use the decomposition described in Cor. 3 to investigate different contributions to the resulting disparity. The decomposition is shown in Fig. 3(a), and uncovers a number of important effects. Firstly, along the direct effect, \(x\)-DE\({}_{x_{0},x_{1}}(y)\) and \(x\)-DE\({}_{x_{0},x_{1}}(m)\) are larger than \(0\), meaning that minority group individuals have a lower chance of receiving acute care (purely based on race). Along the indirect and spurious effects, the situation is different: \(x\)-IE\({}_{x_{1},x_{0}}(y)\) and \(x\)-SE\({}_{x_{1},x_{0}}(y)\) and their respective margin complement contributions are different from \(0\) and negative - implying that minority group individuals have a larger probability of being given acute care as a result of confounding and mediating variables. Finally, the direct effect sample influences (Fig. 3(b)) highlight that the margin complements are large for a small minority of individuals, requiring further subgroup investigation by the hospital team. \(\)_

**Example 3** (Recidivism Prevention on the COMPAS Dataset (Larson et al., 2016)).: _Courts in Broward County, Florida use machine learning algorithms, developed by a private company called Northpointe, to predict whether individuals released on parole are at high risk of re-offending within 2 years (\(Y\)). The algorithm is based on the demographic information \(Z\) (\(Z_{1}\) for gender, \(Z_{2}\) for age), race \(X\) (\(x_{0}\) denoting White, \(x_{1}\) Non-White), juvenile offense counts \(J\), prior offense count \(P\), and degree of charge \(D\). The courts wish to know which individuals are highly likely to recidivate, such that their probability of recidivism is above 50%. The company constructs a prediction score \(^{NP}\) and the court subsequently uses this for deciding whether to detain individuals at high risk of re-offending._

_After a court hearing in which it was decided that the indirect and spurious effects fall under business necessity requirements, a team from ProPublica wishes to investigate the implications of using the automated predictions \(^{NP}\). They obtain the relevant data and apply Alg. 1, with the results shown in Fig. 4. The team first compares the decompositions of the true outcome \(Y\) and the predictor \(^{NP}\) (Fig. 4(a)). For the spurious effect, they find that \(x\)-SE\({}_{x_{1},x_{0}}(y)\) is not statistically different from \(x\)-SE\({}_{x_{1},x_{0}}(^{NP})\), in line with BN requirements. For the indirect effects, they find that the indirect effect is lower for the predictor \(^{NP}\) compared to the true outcome \(Y\), indicating no concerning violations. However, for the direct effect, while the \(x\)-DE\({}_{x_{0},x_{1}}(y)\) is not statistically different from \(0\), the predictor \(^{NP}\) has a significant direct effect of \(X\), i.e., \(x\)-DE\({}_{x_{0},x_{1}}(^{NP}) 0\). This indicates a violation of the fairness requirements determined by the court._

Figure 3: Causal decomposition from Cor. 3 and sample influence on the MIMIC-IV dataset.

_After comparing the decompositions of \(^{NP}\) and \(Y\), the team moves onto understanding the contributions of the margin complements (Fig. 4b). For each effect, there is a pronounced impact of the margin complements. For the direct effect (not under BN), the non-zero margin complement contribution \(x\)-\(_{x_{1},x_{0}}(m) 0\) represents a violation of fairness requirements. For the indirect and spurious effects, the ProPublica team realizes the court did not specify anything about margin complement contributions - based on this, for the next court hearing they are preparing an argument showing that the effects \(x\)-\(_{x_{1},x_{0}}(m)\) and \(x\)-\(_{x_{1},x_{0}}(m)\) are significantly different from \(0\), thereby exacerbating the differences between groups. Finally, based on sample influences (Fig. 4c), they realize that the direct effect is driven by a small minority of individuals, and they decide to investigate this further. \(\)_

## 6 Conclusion

In this paper, we developed tools for understanding the fairness impacts of transforming a continuous prediction score \(S\) into binary predictions \(\) or binary decisions \(D\). In Thm. 1 and Cor. 3 we showed that the TV measure of the optimal 0/1 predictor decomposes into direct, indirect, and spurious contributions that are inherited from the true outcome \(Y\) in the real world, and also contributions from the margin complement \(M\) (Def. 1) arising from the automated optimization procedure. This observation motivated new notions of _weak_ and _strong_ business necessity (BN) - in the former case, differences inherited from the true outcome \(Y\) are allowed to be propagated into predictions or decisions, while any differences resulting from the optimization procedure are disallowed. In contrast, strong BN allows both of these differences and does not prohibit possible disparity amplification. In Alg. 1, we developed a formal procedure for assessing weak and strong BN. Finally, real-world examples demonstrated that the tools developed in this paper are of genuine importance in practice, since converting continuous predictions into binary decisions may often result in bias amplification in practice - highlighting the need for this type of analysis, and the importance of regulatory oversight.