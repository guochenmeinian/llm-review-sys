# Model Reconstruction Using Counterfactual Explanations: A Perspective From Polytope Theory

Pasan Dissanayake

University of Maryland

College Park, MD

pasand@umd.edu

&Sanghamitra Dutta

University of Maryland

College Park, MD

sanghamd@umd.edu

###### Abstract

Counterfactual explanations provide ways of achieving a favorable model outcome with minimum input perturbation. However, counterfactual explanations can also be leveraged to reconstruct the model by strategically training a surrogate model to give similar predictions as the original (target) model. In this work, we analyze how model reconstruction using counterfactuals can be improved by further leveraging the fact that the counterfactuals also lie quite close to the decision boundary. Our main contribution is to derive novel theoretical relationships between the error in model reconstruction and the number of counterfactual queries required using polytope theory. Our theoretical analysis leads us to propose a strategy for model reconstruction that we call Counterfactual Clamping Attack (CCA) which trains a surrogate model using a unique loss function that treats counterfactuals differently than ordinary instances. Our approach also alleviates the related problem of _decision boundary shift_ that arises in existing model reconstruction approaches when counterfactuals are treated as ordinary instances. Experimental results demonstrate that our strategy improves fidelity between the target and surrogate model predictions on several datasets.

## 1 Introduction

Counterfactual explanations (also called _counterfactuals_) have emerged as a burgeoning area of research  for providing guidance on how to obtain a more favorable outcome from a machine learning model, e.g., increase your income by 10K to qualify for the loan. Interestingly, counterfactuals can also reveal information about the underlying model, posing a nuanced interplay between model privacy and explainability . Our work provides the first theoretical analysis between the error in model reconstruction using counterfactuals and the number of counterfactuals queried for, through the lens of polytope theory.

Model reconstruction using counterfactuals can have serious implications in Machine Learning as a Service (MLaaS) platforms that allow users to query a model for a specified cost . An adversary may be able to "steal" the model by querying for counterfactuals and training a surrogate model to provide similar predictions as the target model, a practice also referred to as _model extraction_. On the other hand, model reconstruction could also be beneficial for _preserving applicant privacy_, e.g., if an applicant wishes to evaluate their chances of acceptance from crowdsourced information before formally sharing their own application information with an institution, often due to resource constraints or having a limited number of attempts to apply (e.g., applying for credit cards reduces the credit score ). Our goal is to formalize _how faithfully can one reconstruct an underlying model given a set of counterfactual queries_.

An existing approach for model reconstruction is to treat counterfactuals as ordinary labeled points and use them for training a surrogate model (Aivodji et al., 2020). While this may work for a well-balanced counterfactual queries from the two classes lying roughly equidistant to the decision boundary, it is not the same for unbalanced datasets. The surrogate decision boundary might not always overlap with that of the target model (see Fig. 1), a problem also referred to as _a decision boundary shift_. This is due to the learning process where the boundary is kept far from the training examples (margin) for better generalization (Shokri et al., 2021).

The decision boundary shift is aggravated when the system provides only _one-sided counterfactuals_, i.e., counterfactuals only for queries with unfavorable predictions. If one were allowed to query for two-sided counterfactuals, the decision boundary shift may be tackled by querying for the counterfactual of the counterfactual (Wang et al., 2022). However, such strategies cannot be applied when only one-sided counterfactuals are available, which is more common and also a more challenging use case for model reconstruction, e.g., counterfactuals are only available for the rejected applicants to get accepted for a loan but not the other way.

In this work, we analyze how model reconstruction using counterfactuals can be improved by leveraging the fact that the counterfactuals are quite close to the decision boundary. We provide novel theoretical analysis for model reconstruction using polytope theory, addressing an important knowledge gap in the existing literature. We demonstrate reconstruction strategies that alleviate the decision-boundary-shift issue for one-sided counterfactuals. In contrast to existing strategies Aivodji et al. (2020) and Wang et al. (2022) which require the system to provide counterfactuals for queries from both sides of the decision boundary, we are able to reconstruct using only one-sided counterfactuals, a problem that we also demonstrate to be theoretically more challenging than the two-sided case (see Corollary 3.8). In summary, our contributions can be listed as follows:

**Fundamental guarantees on model reconstruction using counterfactuals:** We derive novel theoretical relationships between the error in model reconstruction and the number of counterfactual queries (query complexity) under three settings: (i) Convex decision boundaries and closest counterfactuals (Theorem 3.2): We rely on convex polytope approximations to derive an _exact_ relationship between expected model reconstruction error and query complexity; (ii) ReLU networks and closest counterfactuals (Theorem 3.6): Relaxing the convexity assumption, we provide _probabilistic_ guarantees on the success of model reconstruction as a function of number of counterfactual queries; and (iii) Beyond closest counterfactuals: We provide approximate guarantees for a broader class of models, including ReLU networks and locally-Lipschitz continuous models (Theorem 3.10).

**Model reconstruction strategy with unique loss function:** We devise a reconstruction strategy - that we call Counterfactual Clamping Attack (CCA) - that exploits only the fact that the counterfactuals _lie reasonably close to the decision boundary, but need not be exactly the closest_.

**Empirical validation:** We conduct experiments on both synthetic datasets, as well as, four real-world datasets, namely, Adult Income (Becker and Kohavi, 1996), COMPAS (Angwin et al., 2016), DCCC (Yeh, 2016), and HELOC (FICO, 2018). Our strategy outperforms the existing baseline (Aivodji et al., 2020) over all these datasets (Section 4) using one-sided counterfactuals, i.e., counterfactuals only for queries from the unfavorable side of the decision boundary. We also include additional experiments to observe the effects of model architecture, Lipschitzness, and other types of counterfactual generation methods as well as ablation studies with other loss functions. A python-based implementation is available at: [https://github.com/pasandissanayake/model-reconstruction-using-counterfactuals](https://github.com/pasandissanayake/model-reconstruction-using-counterfactuals).

_Related Works:_ A plethora of counterfactual-generating mechanisms has been suggested in existing literature (Guidotti, 2022; Barocas et al., 2020; Verma et al., 2022; Karimi et al., 2022, 2020; Dhurandhar et al., 2018; Deutch and Frost, 2019; Mishra et al., 2021). Related works that focus on leaking information about the dataset from counterfactual explanations include membership inference attacks (Pawelczyk et al., 2023) and explanation-linkage attacks (Goethals et al., 2023). Shokri et al. (2021) examine membership inference from other types of explanations, e.g., feature-based. Model reconstruction (without counterfactuals) has been the topic of a wide array of studies (see surveys Gong et al. (2020) and Oliynyk et al. (2023)). Various mechanisms such as equation solving (Tramer et al., 2016) and active learning have been considered (Pal et al., 2020).

Figure 1: Decision boundary shift when counterfactuals are treated as ordinary labeled points.

Model inversion (Gong et al., 2021, Struppek et al., 2022, Zhao et al., 2021) is another form of extracting information about a black box model, under limited access to the model aspects. In contrast to model extraction where the goal is to replicate the model itself, in model inversion an adversary tries to extract the representative attributes of a certain class with respect to the target model. In this regard, Zhao et al. (2021) focuses on exploiting explanations for image classifiers such as saliency maps to improve model inversion attacks. Struppek et al. (2022) proposes various methods based on Generative Adversarial Networks to make model inversion attacks robust (for instance, to distributional shifts) in the domain of image classification.

Milli et al. (2019) looks into model reconstruction using other types of explanations, e.g., gradients. Yadav et al. (2023) explore algorithmic auditing using counterfactual explanations, focusing on linear classifiers and decision trees. Using counterfactual explanations for model reconstruction has received limited attention, with the notable exceptions of Aivodji et al. (2020) and Wang et al. (2022). Aivodji et al. (2020) suggest using counterfactuals as ordinary labeled examples while training the surrogate model, leading to decision boundary shifts, particularly for unbalanced query datasets (one-sided counterfactuals). Wang et al. (2022) introduces a strategy of mitigating this issue by further querying for the counterfactual of the counterfactual. However, both these methods require the system to provide counterfactuals for queries from both sides of the decision boundary. Nevertheless, a user with a favorable decision may not usually require a counterfactual explanation, and hence a system providing one-sided counterfactuals might be more common, wherein lies our significance. While model reconstruction (without counterfactuals) has received interest from a theoretical perspective (Tramer et al., 2016, Papernot et al., 2017, Milli et al., 2019), model reconstruction involving counterfactual explanations lack such a theoretical understanding. Our work theoretically analyzes model reconstruction using polytope theory and proposes novel strategies thereof, also addressing the decision-boundary shift issue.

## 2 Preliminaries

**Notations:** We consider binary classification models \(m\) that take an input value \(^{d}\) and output a probability between \(0\) and \(1\). The final predicted class is denoted by \( m()\{0,1\}\) which is obtained by thresholding the output probability at \(0.5\) as follows: \( m()=[m() 0.5]\) where \([]\) denotes the indicator function. Throughout the paper, we denote the output probability by \(m()\) and the corresponding thresholded output by \( m()\). Consequently, the decision boundary of the model \(m\) is the \((d-1)\)-dimensional hypersurface (generalization of surface in higher dimensions; see Definition 2.5) in the input space, given by \(=\{:m()=0.5\}\). We call the region where \( m()=1\) as the _favorable region_ and the region where \( m()=0\) as the _unfavorable region_. We always state the convexity/concavity of the decision boundary with respect to the favorable region (i.e., the decision boundary is convex if the set \(=\{^{d}: m()=1\}\) is convex). We assume that upon knowing the range of values for each feature, the \(d\)-dimensional input space can be normalized so that the inputs lie within the set \(^{d}\) (the \(d\)-dimensional unit hypercube), as is common in literature (Liu et al., 2020, Tramer et al., 2016, Hamman et al., 2023, Black et al., 2022). We let \(g_{m}\) denote the counterfactual generating mechanism corresponding to the model \(m\).

**Definition 2.1** (Counterfactual Generating Mechanism).: Given a cost function \(c:^{d}^{d}^{+}_{0}\) for measuring the quality of a counterfactual, and a model \(m\), the corresponding counterfactual generating mechanism is the mapping \(g_{m}:^{d}^{d}\) specified as follows: \(g_{m}()=_{^{d}}c(,),\) such that \( m() m()\).

The cost \(c(,)\) is selected based on specific desirable criteria, e.g., \(c(,)=||-||_{p}\), with \(||||_{p}\) denoting the \(L_{p}\)-norm. Specifically, \(p=2\) leads to the following definition of the _closest counterfactual_(Wachter et al., 2017, Laugel et al., 2017, Mothilal et al., 2020).

**Definition 2.2** (Closest Counterfactual).: When \(c(,)||-||_{2}\), the resulting counterfactual generated using \(g_{m}\) as per Definition 2.1 is called the closest counterfactual.

Given a model \(m\) and a counterfactual generating method \(g_{m}\), we define the inverse counterfactual region \(\) for a subset \(^{d}\) to be the region whose counterfactuals under \(g_{m}\) fall in \(\).

**Definition 2.3** (Inverse Counterfactual Region).: The inverse counterfactual region \(_{m,g_{m}}\) of \(^{d}\) is the the region defined as: \(_{m,g_{m}}()=\{^{d}:g_{m}()\}\).

**Problem Setting:** Our problem setting involves a target model \(m\) which is pre-trained and assumed to be hosted on a MLaaS platform (see Fig. 2). Any user can query it with a set of input instances \(^{d}\) (also called counterfactual queries) and will be provided with the set of predictions, i.e., \(\{ m():\}\), and a set of _one-sided_ counterfactuals for the instances whose predicted class is \(0\), i.e., \(\{g_{m}():, m()=0\}\). Note that, by the definition of a counterfactual, \( m(g_{m}())=1\) for all \(\) with \( m()=0\). The **goal** of the user is to train a surrogate model to achieve a certain level of performance with as few queries as possible. In this work, we use _fidelity_ as our performance metric for model reconstruction1.

**Definition 2.4** (Fidelity [Aivodji et al., 2020]).: With respect to a given target model \(m\) and a reference dataset \(_{}^{d}\), the fidelity of a surrogate model \(\) is given by

\[_{m,_{}}()=_{}|}_{_{}}[ m ()=()].\]

**Background on Geometry of Decision Boundaries:** Our theoretical analysis employs arguments based on the geometry of the involved models' decision boundaries. We assume the decision boundaries are hypersurfaces. A hypersurface is a generalization of a surface into higher dimensions, e.g., a line or a curve in a 2-dimensional space, a surface in a 3-dimensional space, etc.

**Definition 2.5** (Hypersurface, Lee ).: A hypersurface is a \((d-1)\)-dimensional sub-manifold embedded in \(^{d}\), which can also be denoted by a single implicit equation \(()=0\) where \(^{d}\).

We focus on the properties of hypersurfaces which are "touching" each other, as defined next.

**Definition 2.6** (Touching Hypersurfaces).: Let \(()=0\) and \(()=0\) denote two differentiable hypersurfaces in \(^{d}\). \(()=0\) and \(()=0\) are said to be touching each other at the point \(\) if and only if \(()=()=0\), and there exists a non-empty neighborhood \(_{}\) around \(\), such that \(_{}\) with \(()=0\) and \(\), only one of \(()>0\) or \(()<0\) holds. (i.e., within \(_{},()=0\) and \(()=0\) lie on the same side of each other).

Next, we show that touching hypersurfaces share a common tangent hyperplane at their point of contact. This result is instrumental in exploiting the closest counterfactuals in model reconstruction (proof in Appendix A.1).

**Lemma 2.7**.: _Let \(()=0\) and \(()=0\) denote two differentiable hypersurfaces in \(^{d}\), touching each other at point \(\). Then, \(()=0\) and \(()=0\) have a common tangent hyperplane at \(\)._

## 3 Main Results

We provide a set of theoretical guarantees on query size of model reconstruction that progressively build on top of each other, starting from stronger guarantees in a somewhat restricted setting towards more approximate guarantees in broadly applicable settings. We discuss important intuitions that can be inferred from the results. Finally, we propose a model reconstruction strategy that is executable in practice, which is empirically evaluated in Section 4.

### Convex decision boundaries and closest counterfactuals

We start out with demonstrating how the closest counterfactuals provide a linear approximation of _any_ decision boundary, around the corresponding counterfactual. Prior work [Yadav et al., 2023] shows that for linear models, the line joining a query instance \(\) and the closest counterfactual \((=g_{m}())\) is perpendicular to the linear decision boundary. We generalize this observation to any differentiable decision boundary, not necessarily linear.

**Lemma 3.1**.: _Let \(\) denote the decision boundary of a classifier and \(^{d}\) be any point that is not on \(\). Then, the line joining \(\) and its closest counterfactual \(\) is perpendicular to \(\) at \(\)._

Figure 2: Problem settingThe proof follows by showing that the \(d\)-dimensional ball with radius \(||-||_{2}\) touches (as in Definition2.6) \(\) at \(\), and invoking Lemma 2.7. For details see Appendix A.1. As a direct consequence of Lemma 3.1, a user may query the system and calculate tangent hyperplanes of the decision boundary drawn at the closest counterfactuals. This leads to a linear approximation of the decision boundary at the closest counterfactuals (see Fig. 3).

If the decision boundary is convex, such an approximation will provide a set of supporting hyperplanes. _The intersection of these supporting hyperplanes will provide a circumscribing convex polytope approximation of the decision boundary._ We show that the average fidelity of such an approximation, evaluated over uniformly distributed input instances, tends to 1 when the number of queries is large.

**Theorem 3.2**.: _Let \(m\) be the target binary classifier whose decision boundary is convex (i.e., the set \(\{^{d}: m()=1\}\) is convex) and has a continuous second derivative. Denote by \(_{n}\), the convex polytope approximation of \(m\) constructed with \(n\) supporting hyperplanes obtained through i.i.d. counterfactual queries. Assume that the fidelity is evaluated with respect to \(_{}\) which is uniformly distributed over \(^{d}\). Then, when \(n\) the expected fidelity of \(_{n}\) with respect to \(m\) is given by_

\[[_{m,_{}}(_{n}) ]=1- \]

_where \((n^{-})\) and the expectation is over both \(_{n}\) and \(_{}\)._

Theorem 3.2 provides an exact relationship between the expected fidelity and number of queries. The proof utilizes a result from random polytope theory  which provides a complexity bound on volume-approximating smooth convex sets by convex polytopes. The proof involves observing that the volume of the overlapping decision regions of \(m\) and \(_{n}\) (for example, regions A and C in Fig. 3) translates to the expected fidelity when evaluated under a uniformly distributed \(_{}\). Appendix A.2 provides the detailed steps.

_Remark 3.3_ (Relaxing the Convexity Assumption).: This strategy of linear approximations can also be extended to a concave decision boundary since the closest counterfactual will always lead to a tangent hyperplane irrespective of convexity. Now the rejected region can be seen as the intersection of these half-spaces (Lemma 3.1 does not assume convexity). However, it is worth noting that approximating a concave decision boundary is, in general, more difficult than approximating a convex region. To obtain equally-spaced out tangent hyperplanes on the decision boundary, a concave region will require a much denser set of query points (see Fig. 4) due to the inverse effect of length contraction discussed in Aleksandrov (1967, Chapter III Lemma 2). _Deriving similar theoretical guarantees for a decision boundary which is neither convex nor concave is much more challenging as the decision regions can no longer be approximated as intersections of half-spaces._ The assumption of convex decision boundaries may only be satisfied under limited scenarios such as input-convex neural networks . However, we observe that this limitation can be circumvented in case of ReLU networks to arrive at a probabilistic guarantee as discussed next.

### ReLU networks and closest counterfactuals

Rectified Linear Units (ReLU) are one of the most used activation functions in neural networks . A deep neural network that uses ReLU activations can be represented as a Continuous Piece-Wise Linear (CPWL) function . A CPWL function comprises of a union of linear functions over a partition of the domain. Definition 3.4 below provides a precise characterization.

**Definition 3.4** (Continuous Piece-Wise Linear (CPWL) Function ).: A function \(:^{d}\) is said to be continuous piece-wise linear if and only if

Figure 4: Approximating a concave region needs denser queries w.r.t. a convex region.

Figure 3: Polytope approximation of a convex decision boundary using the closest counterfactuals.

1. There exists a finite set of closed subsets of \(^{d}\), denoted as \(\{_{i}\}_{i=1,2,,q}\) such that \(_{i=1}^{q}_{i}=^{n}\)
2. \(()\) is affine over each \(_{i}\) i.e., over each \(_{i},()=_{i}(x)=_{i}^{T}+b_{i}\) with \(_{i}^{d}\), \(b_{i}\).

This definition can be readily applied to the models of our interest, of which the domain is the unit hypercube \(^{d}\). A neural network with ReLU activations can be used as a classifier by appending a Sigmoid activation \((z)=}\) to the final output. We denote such a classifier by \(m()=(())\) where \(()\) is CPWL. It has been observed that the number of linear pieces \(q\) of a trained ReLU network is generally way below the theoretically allowed maximum .

We first show that the decision boundaries of such CPWL functions are collections of polytopes (not necessarily convex). The proof of Lemma 3.5 is deferred to Appendix A.3.

**Lemma 3.5**.: _Let \(m()=(())\) be a ReLU classifier, where \(()\) is CPWL and \((.)\) is the Sigmoid function. Then, the decision boundary \(=\{^{d}:m()=0.5\}\) is a collection of (possibly non-convex) polytopes in \(^{d}\), when considered along with the boundaries of the unit hypercube._

Next we will analyze the probability of successful model reconstruction using counterfactuals. Consider a uniform grid \(_{e}\) over the unit hypercube \(^{d}\), where each cell is a smaller hypercube with side length \(\) (see Fig. 5). For this analysis, we make an assumption: _If a cell contains a part of the decision boundary, then that part is completely linear (affine) within that small cell 2._

Now, as the decision boundary becomes linear for each small cell that it passes through, having just one closest counterfactual in each such cell is sufficient to get the decision boundary in that cell (recall Lemma 3.1). We formalize this intuition in Theorem 3.6 to obtain a probabilistic guarantee on the success of model reconstruction. A proof is presented in Appendix A.4.

**Theorem 3.6**.: _Let \(m\) be a target binary classifier with ReLU activations. Let \(k(e)\) be the number of cells through which the decision boundary passes. Define \(\{_{i}\}_{i=1,,k(e)}\) to be the collection of affine pieces of the decision boundary within each decision boundary cell where each \(_{i}\) is an open set. Let \(v_{i}()=V(_{m,g_{m}}(_{i}))\) where \(V(.)\) is the \(d-\)dimensional volume (i.e., the Lebesgue measure) and \(_{m,g_{m}}(.)\) is the inverse counterfactual region w.r.t. \(m\) and the closest counterfactual generator \(g_{m}\). Then the probability of successful reconstruction with counterfactual queries distributed uniformly over \(^{d}\) is lower-bounded as_

\[[] 1-k()(1-v^{*}())^{n} \]

_where \(v^{*}()=_{i=1,,k(e)}v_{i}()\) and \(n\) is the number of queries._

_Remark 3.7_.: Here \(k()\) and \(v^{*}()\) depend only on the nature of the model being reconstructed and are independent of the number of queries \(n\). The value of \(k()\) roughly grows with the surface area of the decision boundary (e.g., length when input is 2D), showing that models with more convoluted decision boundaries might need more queries for reconstruction. Generally, \(k()\) lies within the interval \()}{^{d-1}} k() {1}{^{d}}\) where \(A(.)\) denotes the surface area in \(d-\)dimensional space. The lower bound is due to the fact that the area of any slice of the unit hypercube being at-most \(\). Upper bound is reached when the decision boundary traverses through all the cells in the grid which is less likely in practice. When the model complexity increases, we get a larger \(k()\) as well as a smaller \(v^{*}()\), requiring a higher number of queries to achieve similar probabilities of success.

**Corollary 3.8** (Linear Models).: _For linear models with one-sided counterfactuals, \([]=1-(1-v)^{n}\) where \(v\) is the volume of the unfavorable region. However, with two-sided counterfactuals, \([]=1\) with just one single query._

Figure 5: \(_{e}\) grid and inverse counterfactual regions. Thick solid lines indicate the decision boundary pieces (\(_{i}\)’s). White color depicts the accepted region. Pale-colored are the inverse counterfactual regions of the \(_{i}\)’s with the matching color. In this case \(k()=7\) and \(v^{*}()\) is the area of lower amber region.

This result mathematically demonstrates that allowing counterfactuals from both accepted and rejected regions (as in Aivodji et al. (2020); Wang et al. (2022)) is easier for model reconstruction, when compared to the one-sided case. It effectively increases each \(v_{i}()\) (volume of the inverse counterfactual region). As everything else remains unaffected, for a given \(n\), \([]\) is higher when counterfactuals from both regions are available. For a linear model, this translates to a guaranteed reconstruction with a single query since \(v=1\).

However, we note that all of the aforementioned analysis relies on the closest counterfactual which can be challenging to generate. Practical counterfactual generating mechanisms usually provide counterfactuals that are reasonably close but may not be exactly the closest. This motivates us to now propose a more general strategy assuming local Lipschitz continuity of the models involved.

### Beyond closest counterfactuals

Lipschitz continuity, a property that is often encountered in related works (Bartlett et al., 2017; Gouk et al., 2021; Pauli et al., 2021; Hamman et al., 2023, 2024; Liu et al., 2020; Marques-Silva et al., 2021), demands the model output does not change too fast. Usually, a smaller Lipschitz constant is indicative of a higher generalizability of a model (Gouk et al., 2021; Pauli et al., 2021).

**Definition 3.9** (Local Lipschitz Continuity).: A model \(m\) is said to be locally Lipschitz continuous if for every \(_{1}^{d}\) there exists a neighborhood \(_{_{1}}^{d}\) around \(_{1}\) such that for all \(_{2}_{_{1}},|m(_{1})-m(_{2})||| _{1}-_{2}||_{2}\) for some \(_{0}^{+}\).

For analyzing model reconstruction under local-Lipschitz assumptions, we consider the difference of the model output probabilities (before thresholding) as a measure of similarity between the target and surrogate models because it would force the decision boundaries to overlap. The proposed strategy is motivated from the following observation: the difference of two models' output probabilities corresponding to a given input instance \(\) can be bounded by having another point with matching outputs in the affinity of the instance \(\). This observation is formally stated in Theorem 3.10. See Appendix A.5 for a proof.

**Theorem 3.10**.: _Let the target \(m\) and surrogate \(\) be ReLU classifiers such that \(m()=()\) for every counterfactual \(\). For any point \(\) that lies in a decision boundary cell, \(|()-m()|(_{m}+_{})\) holds with probability \(p 1-k()(1-v^{*}())^{n}\)._

Note that within each decision boundary cell, models are affine and hence locally Lipschitz for some \(_{m},_{}_{0}^{+}\). Local Lipschitz property assures that the approximation is quite close (\(_{m},_{}\) are small) except over a few small ill-behaved regions of the decision boundary. This result can be extended to any locally Lipschitz pair of models as stated in following corollary.

**Corollary 3.11**.: _Suppose the target \(m\) and surrogate \(\) are locally Lipschitz (not necessarily ReLU) such that \(m()=()\) for every counterfactual \(\). Assume the counterfactuals are well-spaced out and forms a \(\)-cover over the decision boundary. Then \(|()-m()|(_{m}+_{}),\) over the target decision boundary._

Theorem 3.10 provides the motivation for a novel model reconstruction strategy. Let \(\) be a counterfactual. Recall that \(\) denotes the decision boundary of \(m\). As implied by the theorem, for any \(\), the deviation of the surrogate model output from the target model output can be bounded above by \((_{m}+_{})\) given that all the counterfactuals satisfy \(m()=()\). Knowing that \(m()=0.5\), we may design a loss function which **clamps**\(()\) to be 0.5. _Consequently, with a sufficient number of well-spaced counterfactuals to cover \(\), we may achieve arbitrarily small \(|()-m()|\) at the decision boundary of \(m\) (Fig. 6)._

However, we note that simply forcing \(()\) to be exactly equal to 0.5 is quite unstable since in practice the target model's output \(m()\) is known to be close to 0.5, while being greater but may not be exactly equal. Thus, we propose a unique loss function for training the surrogate neural networks that does not penalize the counterfactuals that are already inside the favorable region of the surrogate model. For \(0<k 1\), we define the Counterfactual Clamping loss function as

\[L_{k}((),y_{})=[y_{}=0.5,( ) k]\{L((),k)-h(k)\}+ [y_{} 0.5]L((),y_{}). \]

Figure 6: Rationale for Counterfactual Clamping Strategy.

Here, \(y_{}\) denotes the label assigned to the input instance \(\) by the target model, received from the API. \(L(,y)\) is the binary cross-entropy loss and \(h()\) denotes the binary entropy function. We assume that the counterfactuals are distinguishable from the ordinary instances, and assign them a label of \(y_{}=0.5\). The first term accounts for the counterfactuals, where they are assigned a non-zero loss if the surrogate model's prediction is below \(k\). This term ensures \(() k\) for the counterfactuals \(\) while not penalizing the ones that lie farther inside the favorable region. The second term is the ordinary binary cross-entropy loss, which becomes non-zero only for ordinary query instances. Note that substituting \(k=1\) in \(L_{k}((),y_{})\) yields the ordinary binary cross entropy loss. Algorithm 1 summarizes the proposed strategy.

```
0: Attack dataset \(_{}\), \(k\) (\(k(0,1]\), usually 0.5), API for querying
0: Trained surrogate model \(\)
1: Initialize \(=\{\}\)
2:for\(_{}\)do
3: Query API with \(\) to get \(y_{}\) {\(y_{}\{0,1\}\)}
4:\(\{(,y_{})\)
5:if\(y_{}=0\)then
6: Query API for counterfactual \(\) of \(\)
7:\(\{(,0.5)\}\) {Assign \(\) a label of 0.5}
8:endif
9:endfor
10: Train \(\) on \(\) with \(L_{k}((),y_{})\) as the loss
11:return\(\)
```

**Algorithm 1** Counterfactual Clamping

Here, \(y_{}\) denotes the label assigned to the input instance \(\) by the target model, received from the API. \(L(,y)\) is the binary cross-entropy loss and \(h()\) denotes the binary entropy function. We assume that the counterfactuals are distinguishable from the ordinary instances, and assign them a label of \(y_{}=0.5\). The first term accounts for the counterfactuals, where they are assigned a non-zero loss if the surrogate model's prediction is below \(k\). This term ensures \(() k\) for the counterfactuals \(\) while not penalizing the ones that lie farther inside the favorable region. The second term is the ordinary binary cross-entropy loss, which becomes non-zero only for ordinary query instances. Note that substituting \(k=1\) in \(L_{k}((),y_{})\) yields the ordinary binary cross entropy loss. Algorithm 1 summarizes the proposed strategy.

It is noteworthy that this approach is different from the broad area of soft-label learning Nguyen et al. (2011a,b) in two major aspects: (i) the labels in our problem do not smoothly span the interval  - instead they are either 0, 1 or 0.5; (ii) labels of counterfactuals do not indicate a class probability; even though a label of 0.5 is assigned, there can be counterfactuals that are well within the surrogate decision boundary which should not be penalized. Nonetheless, we also perform ablation studies where we compare the performance of our proposed loss function with another potential loss which simply forces \(()\) to be exactly \(0.5\), inspired from soft-label learning (see Appendix B.2.10 for results).

Counterfactual Clamping overcomes two challenges beset in existing works; (i) the problem of decision boundary shift (particularly with one-sided counterfactuals) present in the method suggested by Aivodji et al. (2020) and (ii) the need for counterfactuals from both sides of the decision boundary in the methods of Aivodji et al. (2020) and Wang et al. (2022).

## 4 Experiments

We carry out a number of experiments to study the performance of our proposed strategy Counterfactual Clamping. We include some results here and provide further details in Appendix B.

All the classifiers are neural networks unless specified otherwise and their decision boundaries are _not necessarily convex_. The performance of our strategy is compared with the existing attack presented in Aivodji et al. (2020) that we refer to as "Baseline", for the case of one-sided counterfactuals. As the initial counterfactual generating method, we use an implementation of the Minimum Cost Counterfactuals (MCCF) by Wachter et al. (2017).

_Performance metrics:_ Fidelity is used for evaluating the agreement between the target and surrogate models. It is evaluated over both uniformly generated instances (denoted by \(_{}\)) and test data instances from the data manifold (denoted by \(}_{}\)) as the reference dataset \(_{}\).

A summary of the experiments is provided below with additional details in Appendix B.

**(i) Visualizing the attack using synthetic data:** First, the effect of the proposed loss function in mitigating the decision boundary shift is observed over a 2-D synthetic dataset. Fig. 7 presents the results. In the figure, it is clearly visible that the Baseline model is affected by a decision boundary shift. In contrast, the CCA model's decision boundary closely approximates the target decision boundary. See Appendix B.2.1 for more details.

**(ii) Comparing performance over four real-world dataset:** We use four publicly available real-world datasets namely, Adult Income, COMPAS, DCCC, and HELOC (see Appendix B.1) for our experiments. Table 1 provides some of the results over four real-world datasets. We refer to Appendix B.2.2 (specifically Fig. 11) for additional results. In all cases, we observe that CCA has either better or similar fidelity as compared to Baseline.

[MISSING_PAGE_FAIL:9]

## 5 Conclusion

Our work provides novel insights that bridge explainability and privacy through a set of theoretical guarantees on model reconstruction using counterfactuals. We also propose a practical model reconstruction strategy based on the analysis. Experiments demonstrate a significant improvement in fidelity compared to the baseline method proposed in Aivodji et al. (2020) for the case of one-sided counterfactuals. Results show that the attack performs well across different model types and counterfactual generating methods. Our findings also highlight an interesting connection between Lipschitz constant and vulnerability to model reconstruction.

**Limitations and Future Work:** Even though Theorem 3.6 provides important insights about the role of query size in model reconstruction, it lacks an exact characterization of \(k()\) and \(v_{i}()\). Moreover, local Lipschitz continuity might not be satisfied in some machine learning models such as decision trees. Any improvements along these lines can be avenues for future work. Utilizing techniques in active learning in conjunction with counterfactuals is another problem of interest. Extending the results of this work for multi-class classification scenarios can also be explored. The relationship between Lipschitz constant and vulnerability to model reconstruction may have implications for future work on generalization, robustness, etc. Another direction of future work is to design defenses for model reconstruction attacks, leveraging and building on strategies for robust counterfactuals (Dutta et al., 2022; Hamman et al., 2024; Upadhyay et al., 2021; Black et al., 2022; Jiang et al., 2023; Pawelczyk et al., 2020).

**Broader Impact:** We demonstrate that one-sided counterfactuals can be used for perfecting model reconstruction. While this can be beneficial in some cases, it also exposes a potential vulnerability in MLaaS platforms. Given the importance of counterfactuals in explaining model predictions, we hope our work will inspire countermeasures and defense strategies, paving the way toward secure and trustworthy machine learning systems.

    & _{}\)} & _{}\)} \\ CF method & n=100 & n=200 & n=100 & n=200 \\   & Base. & CCA & Base. & CCA & Base. & CCA & Base. & CCA \\  MCCF L2-norm & 91 & 95 & 93 & 96 & 91 & 93 & 93 & 95 \\ MCCF L1-norm & 93 & 95 & 94 & 96 & 89 & 92 & 91 & 95 \\ DiCE Actionable & 93 & 94 & 95 & 95 & 90 & 91 & 93 & 94 \\
1-Nearest-Neighbor & 93 & 95 & 94 & 96 & 93 & 93 & 94 & 95 \\ ROAR (Upadhyay et al., 2021) & 91 & 92 & 93 & 95 & 87 & 85 & 92 & 92 \\ C-CHVAE (Pawelczyk et al., 2020) & 77 & 80 & 78 & 82 & 90 & 89 & 85 & 78 \\   

Table 2: Fidelity achieved with different counterfactual generating methods on HELOC dataset. Target model has hidden layers with neurons (20, 30, 10). Surrogate model architecture is (10, 20).

Figure 8: Histograms of the target model’s predictions on different types of input instances. Counterfactual generating methods except MCCF with \(L_{2}\) norm often generate counterfactuals that are farther inside the favorable region, hence having a target model prediction much greater than 0.5. We count all the query results across all the target models in the ensembles used to compute the average fidelities corresponding to each counterfactual generating method.