# Aggregating Quantitative Relative Judgments:

From Social Choice to Ranking Prediction

 Yixuan Even Xu

Carnegie Mellon University

yixuanx@cs.cmu.edu

&Hanrui Zhang

Chinese University of Hong Kong

hanrui@cse.cuhk.edu.hk

&Yu Cheng

Brown University

yu_cheng@brown.edu

&Vincent Conitzer

Carnegie Mellon University

conitzer@cs.cmu.edu

###### Abstract

Quantitative Relative Judgment Aggregation (QRJA) is a new research topic in (computational) social choice. In the QRJA model, agents provide judgments on the relative quality of different candidates, and the goal is to aggregate these judgments across all agents. In this work, our main conceptual contribution is to explore the interplay between QRJA in a social choice context and its application to ranking prediction. We observe that in QRJA, judges do not have to be people with subjective opinions; for example, a race can be viewed as a "judgment" on the contestants' relative abilities. This allows us to aggregate results from multiple races to evaluate the contestants' true qualities. At a technical level, we introduce new aggregation rules for QRJA and study their structural and computational properties. We evaluate the proposed methods on data from various real races and show that QRJA-based methods offer effective and interpretable ranking predictions.

## 1 Introduction

In _voting theory_, each voter _ranks_ a set of candidates, and a _voting rule_ maps the vector of rankings to either a winning candidate or an aggregate ranking of all the candidates. There has been significant interaction between computer scientists interested in voting theory and the _learning-to-rank_ community. The learning-to-rank community is interested in problems such as ranking webpages in response to a search query, or ranking recommendations to a user (see, e.g., Liu (2009)). Another problem of interest is to aggregate multiple rankings into a single one, for example combining the ranking results from different algorithms ("voters") into a single meta-ranking. While the interests of the communities may differ, e.g., the learning-to-rank community is less concerned about strategic aspects of voting, a natural intersection point for these two communities is a model where there is a latent "true" ranking of the candidates, of which all the votes are just noisy observations. Consequently, it is natural to try to estimate the true ranking based on the received rankings, and such an estimation procedure corresponds to a voting rule. (See, e.g., Young (1995); Conitzer and Sandholm (2005); Meila _et al._ (2007); Conitzer _et al._ (2009); Caragiannis _et al._ (2013); Soufiani _et al._ (2014); Xia (2016), and Elkind and Slinko (2015) for an overview.)

Voting rules are just one type of mechanism in the broader field of _social choice_, which studies the broader problem of making decisions based on the opinions and preferences of multiple agents. Such opinions are not necessarily represented as rankings. For example, in _judgment aggregation_ (see Endriss (2015) for an overview), judges assess whether certain propositions are true or false, and the goal is to aggregate these judgments into logically consistent statements. The observationthat other types of input are aggregated in social choice prompts the natural question of whether analogous problems exist in statistics and machine learning (as is the case with ranking aggregation).

In this paper, we focus on a relatively new model in social choice, the _quantitative_ judgment aggregation problem (Conitzer _et al._, 2015, 2016). In this problem, the goal is to aggregate _relative quantitative judgments_: for example, one agent may value the life of a 20-year-old at 2 times the life of a 50-year-old (say in the context of self-driving cars making decisions) (Noothigattu _et al._, 2018); another example could be that an agent judges that "using 1 unit of gasoline is as bad as creating 3 units of landfill trash" (in a societal tradeoff context) (Conitzer _et al._, 2016). Quantitative judgment aggregation has been considered in the area of automated moral decision-making, where an AI system may choose a course of action based on data about human judgments in similar scenarios.

An important conceptual difference between this work and previous studies on quantitative judgment aggregation is that we observe that relative "judgments" can be produced by a process other than a subjective agent reporting them, which is the standard assumption in social choice. To illustrate, consider a race in which contestant A finishes at 20:00 and contestant B at 30:00. In this race, the "judgment" is that A is 10:00 faster than B. This key observation allows us to bring the social choice community and the learning-to-rank community closer together, by applying existing social choice formulations of quantitative judgment aggregation to the problem of ranking prediction.

Under this new perspective, the formulation of quantitative judgment aggregation can be applied a set of new scenarios, like ranking contestants using "judgments" from past races, or ranking products based on "judgments" from their sales data. We are interested in aggregating such "judgments" from past data, and using them to predict future rankings. Given the different motivations, some important aspects in a social choice context are less important in our setting. For example, social choice is often concerned with agents strategically misreporting, but this is less relevant in our setting because the "judgments" considered in our setting are not strategic.

**Our Contributions.** We summarize our main contributions below: **(1)** Conceptually, we apply social-choice-motivated solution concepts to the problem of ranking prediction, which creates a bridge between research typically done in the social choice and the learning-to-rank communities. **(2)** We pose and study the problem of quantitative relative judgment aggregation (QRJA) in Section 3, which generalizes models from previous work (Conitzer _et al._, 2015, 2016). **(3)** Theoretically, we focus on \(_{p}\) QRJA, an important subclass of QRJA problems. We (almost) settle the computational complexity of \(_{p}\) QRJA in Section 4, proving that \(_{p}\) QRJA is solvable in almost-linear time when \(p 1\), and is NP-hard when \(p<1\). **(4)** Empirically, we focus on \(_{1}\) and \(_{2}\) QRJA. We conduct extensive experiments on a wide range of real-world datasets in Section 5 to compare the performance of QRJA with several other commonly used methods, showing the effectiveness of QRJA in practice.

## 2 Motivating Examples

To better motivate our study and help readers understand the problem, we first consider simple mean/median approaches for aggregating quantitative judgments and illustrate their limitations through three examples.

**Example 1.** When each race has some common "difficulty" factor (e.g. how hilly a marathon route is), if a contestant only participates in the "easy" races (or only the "hard" races), simply taking the median or mean of historical performance will return biased estimates, as illustrated in Figure 1.

**Example 2.** Suppose past data shows that Alice has beaten Bob in some race, and Bob has beaten Charlie in another race. If we have never seen Alice and Charlie competing in the same race, we may want to predict that Alice runs faster than Charlie (see Figure 2). However, when comparing Alice

Figure 1: Bob finishes earlier than Charlie in the Chicago race, which suggests that Bob runs marathons faster than Charlie. However, if we simply calculate the mean or median of all available data, Charlie’s mean/median finishing time will be faster than Bob’s. This is because, Charlie participated only in the Chicago race, where conditions were more favorable.

and Charlie, simple measures like median and mean effectively ignore the data on Bob, even though Bob's data can provide useful information for this comparison.

**Example 3**.: When the variance of the races' difficulty is much higher than the variance in the contestants' performance, taking the median will essentially focus on the result of a single race (with median difficulty) and may throw away useful information as shown in Figure 3.

QRJA addresses the above issues by considering _relative_ performance instead of absolute performance. More specifically, each race provides a judgment of the form "A runs faster than B by Y minutes" for every pair of contestants \((A,B)\) that participated in this race.

## 3 Problem Formulation

In this section, we formally define the Quantitative Relative Judgment Aggregation (QRJA) problem. We start with the definition of its input.

**Definition 1** (Quantitative Relative Judgment).: _For a set of \(n\) candidates \(N=\{1,,n\}\), a **quantitative relative judgment** is a tuple \(J=(a,b,y)\), denoting a judgment that candidate \(a N\) is better than candidate \(b N\) by \(y\) units._

The input of QRJA is a set of quantitative relative judgments to be aggregated. We model the aggregation result as a vector \(^{n}\), where \(x_{i}\) is the single-dimensional evaluation of candidate \(i\). The aggregation result should be consistent with the input judgments as much as possible, i.e., for a quantitative relative judgment \((a,b,y)\), we want \(|x_{a}-x_{b}-y|\) to be small. We use a loss function \(f(|x_{a}-x_{b}-y|)\) to measure the inconsistency between the aggregation result and the input judgments. The aggregation result should minimize the weighted total loss. Formally, we define QRJA as follows.

**Definition 2** (Quantitative Relative Judgment Aggregation (QRJA)).: _Consider \(n\) candidates \(N=\{1,,n\}\) and \(m\) quantitative relative judgments \(=(J_{1},,J_{m})\) with weights \(=(w_{1},,w_{m})\) where \(J_{i}=(a_{i},b_{i},y_{i})\). The **quantitative relative judgment aggregation** problem with loss function \(f:_{ 0}_{ 0}\) asks for a vector \(^{n}\) that minimizes \(_{i=1}^{m}w_{i}f(|x_{a_{i}}-x_{b_{i}}-y_{i}|)\)._

Previous work  studied a special case of QRJA where \(f(t)=t\). In this work, we broaden the scope and study QRJA with more general loss functions. We first note that when the loss function \(f\) is convex, QRJA can be formulated as a convex optimization problem. Consequently, one can use standard convex optimization methods like gradient descent or the ellipsoid method to solve QRJA in polynomial time.

However, general-purpose convex optimization methods are often very slow when the numbers of candidates \(n\) and judgments \(m\) are large. For this reason, we focus on \(_{p}\) QRJA, an important subclass of QRJA problems with loss function \(f(t)=t^{p}\). Our theoretical analysis (almost) settles the computational complexity of \(_{p}\) QRJA for all \(p>0\). We show that \(_{p}\) QRJA is solvable in

Figure 3: In this example, the races’ difficulty has high variance, and everyone’s median time is in Boston. Based on this, we would predict Charlie to be faster than Bob. However, if we consider the other two races, overall it seems that Bob runs faster than Charlie.

Figure 2: The same results as in Figure 1, but with some data missing. If we only look at the data on Alice and Charlie, it is difficult to judge who is the faster runner. If anything, Charlie appears to be slightly faster. However, if we know Bob’s results in these races, then transitivity suggests that Alice runs faster than Charlie.

[MISSING_PAGE_FAIL:4]

Note that \(=-\) is enforced; otherwise the inner maximization problem is unbounded. Let \(\|\|_{q}\) be the dual norm of \(\|\|_{p}\), i.e., \(+=1\). (So \(q>1\).) By strong duality,

\[_{^{m}}_{^{ m},^{m}}(\|\|_{p}+^{ }(-(-))) \] \[= _{^{m}}[^{}+_{^{m}}(\|\|_{p}+ ^{})-_{^{n}}^{}]\] \[= _{^{m},^{}= ,\|\|_{q} 1}^{}.\]

The last step follows from the fact that the value of \((_{^{m}}\|\|_{p}+ ^{})\) is \(0\) if \(\|\|_{q} 1\) and \(-\) otherwise, and that \(_{^{n}}^{}\) is unbounded if \(^{}\).

We will show that the dual program (3) can be solved near-optimally in almost-linear time (Lemma 1), and given a near-optimal dual solution \(^{m}\), a good primal solution \(^{n}\) can be computed in linear time (Lemma 2). Theorem 1 follows directly from Lemmas 1 and 2. 

**Lemma 1**.: _We can find a feasible solution \(^{m}\) of (3) in time \(O(m^{1+o(1)})\) with additive error \((-^{6c}m)\)._

**Proof of Lemma 1:** Consider the following problem, which moves the norm constraint of (3) into the objective:

\[_{^{m},^{}=} ^{}-\|\|_{q}^{q}. \]

(4) is closely related to \(_{p}\) norm mincost flow. Recent breakthrough in mincost flow [Chen _et al._, 2022] showed that a feasible solution \(^{}\) of (4) within error \((-^{13c}m)\) can be computed in \(O(m^{1+o(1)})\) time.

Suppose \(\|^{}\|_{q}(-^{7c}m)\), which we prove later. Notice that \(^{}\) is a solution within error \((-^{13c}m)\) of

\[_{^{m},^{}=, \|\|_{q}=\|^{}\|_{q}} ^{}.\]

Choosing \(=^{}/\|^{}\|_{q}\) satisfies Lemma 1.

To lower bound \(\|^{}\|_{q}\), let \(^{*}\) be the optimal solution of (3). When \(^{*}{}^{} 3\), because the optimal value of (4) is at least \(^{*}{}^{}-1\) and \(^{}\) is near-optimal for (4), we have \(^{}{}^{}^{*}{}^{}-2\) and thus \(\|^{}\|_{q} 1/3\). When \(^{*}{}^{}<3\), we will show \(^{}{}^{}(-^{6c}m)\), so \(\|^{}\|_{q}(-^{7c}m)\).

To show \(^{}{}^{}(-^{6c}m)\), we only need to show that the optimal value of (4) is at least \((-^{5c}m)\). We can assume w.l.o.g. that \(^{*}{}^{}>(-^{3c}m)\), otherwise there is a primal solution \(\) almost consistent with all judgments, which is easy to approximate. Note that when scaling down \(^{*}\), \(\|^{*}\|_{q}^{q}\) scales faster than \(^{*}{}^{}\). Let \(^{}=k^{*}\) with \(k=(-^{4c}m)\). We have \(^{}{}^{}-\|^{}\|_{q} ^{q}=k(^{*}{}^{})-kq>(-^{5c}m)\), where the last step assumes that \(m\) is sufficiently large, in particular \(^{c}m>\{,q+1\}\). 

**Lemma 2**.: _Given a solution \(\) of (3) that satisfies Lemma 1, we can compute a vector \(^{n}\) in time \(O(m)\) such that_

\[\|-\|_{p}_{^{*}} \|^{*}-\|_{p}+(-^{2c}m).\]

**Proof of Lemma 2:** We assume w.l.o.g. that \(\|\|_{q}=1\).

Let \(v=^{}\) and consider

\[_{^{}^{m},^{}^{ }=}(^{})\ \ \ \ (^{ })=^{}{}^{}-\|^{ }\|_{q}^{q}. \]Because \(\) is a solution of (3) within error \((-^{6c}m)\), and \(_{\|\|_{q}}v\|\|_{q}- \|\|_{q}^{q}\) is achieved when \(\|\|_{q}=1\), we know that \(\) is a solution of (5) within error \((-^{5c}m)\).

The first-order optimality condition of (5) guarantees that \(()\) is very close to a potential flow. That is, we can find in \(O(m)\) time a vector \(^{n}\), such that \(\|-()\|_{}(- ^{3c}m)\). For this \(\),

\[\|-\|_{p} \|()-\|_{p}+\| -()\|_{p}\] \[=v+\|-()\|_{p}\] \[ v+m\|-()\| _{}\] \[ v+(-^{2c}m)\] \[_{^{*}^{n}}\| ^{*}-\|_{p}+(-^{2c}m).\]

The last inequality uses that \(v=^{}\) is a lower bound on the optimal value because \(\) is a feasible dual solution. 

### NP-Hardness of \(_{p}\) QRJA When \(p<1\)

In this section, we show that \(_{p}\) QRJA is NP-hard when \(p<1\) by reducing from Max-Cut. Note that in this case, the loss function \(f(t)=t^{p}\) is no longer convex.

**Definition 3** (Max-Cut).: _For an undirected graph \(G=(V,E)\), Max-Cut asks for a partition of \(V\) into two sets \(S\) and \(T\) that the number of edges between \(S\) and \(T\) is maximized._

**Reduction from Max-Cut to \(_{p}\) QRJA.** Given a Max-Cut instance on an undirected graph \(G=(V,E)\), let \(n=|V|,m=|E|\), \(w_{2}=+1\), and \(w_{1}=nw_{2}+1\).

We will construct an \(_{p}\) QRJA instance with \(n+2\) candidates \(V\{v^{(s)},v^{(t)}\}\) and \(O(n+m)\) quantitative relative judgments. Specifically, we add the following judgments:

* \((v^{(t)},v^{(s)},1)\) with weight \(w_{1}\).
* \((v^{(s)},u,0)\) with weight \(w_{2}\) for each \(u V\).
* \((v^{(t)},u,0)\) with weight \(w_{2}\) for each \(u V\).
* \((u,v,1),(v,u,1)\) with weight \(1\) for each \((u,v) E\).

In Appendix B.2, we will prove that the Max-Cut instance has a cut of size at least \(k\) if and only if the constructed \(_{p}\) QRJA instance has a solution with loss at most \(nw_{2}+2(m-k)+k2^{p}\), which implies the following hardness result.

**Theorem 2**.: _For any \(p<1\), there exists a constant \(c>0\) such that it is NP-hard to approximate \(_{p}\) QRJA within a multiplicative factor of \((1+})\)._

Theorem 2 implies that there is no (multiplicative) FPTAS for \(_{p}\) QJA when \(p<1\) unless P = NP. This is because if a \((1+)\) solution can be computed in \((m,1/)\) time, then choosing \(=}\) gives a poly-time algorithm for Max-Cut.

## 5 Experiments

We conduct experiments on real-world datasets to compare the performance of \(_{1}\) and \(_{2}\) QRJA with existing methods. We focus on \(_{1}\) and \(_{2}\) QRJA because the almost-linear time algorithm for general values of \(p 1\) relies on very complicated galactic algorithms for \(_{p}\) norm mincost flow [Chen _et al._, 2022]. Although general-purpose convex optimization methods can also be used to solve \(_{p}\) QRJA, they are not efficient enough for some of the large-scale datasets we use. All experiments are done on a server with 56 CPU cores and 504G RAM. The experiments in Section 5 and Appendices A and C take around 2 weeks in total to run on this server. No GPU is used. All source code is available at [https://github.com/YixuanEvenXu/quantitative-judgment-aggregation](https://github.com/YixuanEvenXu/quantitative-judgment-aggregation).

### Experiments Setup

**Datasets.** We consider types of contests where events are reasonably frequent (so it makes sense to predict future events based on past ones), and contest results contain numerical scores in addition to rankings. Specifically, we use the four datasets listed below. We include additional experiments on three more datasets in Appendix C, and the copyright information of the datasets in Appendix E.

* **Chess.** This dataset contains the results of the Tata Steel Chess Tournament ([https://tatasteelchess.com/](https://tatasteelchess.com/), also historically known as the Hoogovens Tournament or the Corus Chess Tournament) from 1983 to 2023 3. Each contest is typically a round-robin tournament among 10 to 14 contestants. A contestant's numerical score is the contestant's number of wins in the tournament. There are 80 contests and 408 contestants in this dataset. * **F1.** This dataset contains the results of Formula 1 races ([https://www.formula1.com/](https://www.formula1.com/)) from 1950 to 2023. In each contest, we take all contestants who complete the whole race. There are around 7 such contestants in each contest. A contestant's numerical score is the negative of his/her finishing time (in seconds). There are 878 contests and 261 contestants in this dataset.
* **Marathon.** This dataset contains the results of the Boston and New York Marathons from 2000 to 2023. We use the data from [https://www.marathonguide.com/](https://www.marathonguide.com/), which publishes results of all major marathon events. Each contest usually involves more than 20000 contestants. We take the 100 top-ranked contestants in each contest as our dataset. A contestant's numerical score is the negative of that contestant's finishing time (in seconds). There are 44 contests and 2984 contestants.
* **Codeforces.** This dataset contains the results of Codeforces ([https://codeforces.com](https://codeforces.com)), a website hosting frequent online programming contests, from 2010 to 2023 (Codeforces Round 875). We consider only Division 1 contests, where only more skilled contestants can participate. Each contest involves around 700 contestants. We take the 100 top-ranked contestants in each contest as our dataset. A contestant's numerical score is that contestant's points in that contest. There are 327 contests and 5338 contestants in total in this dataset.

**Evaluation Metrics.** For all the datasets we use, contests are naturally ordered chronologically. We use the results of the first \(i-1\) contests to predict the results of the \(i\)-th contest. We apply the following two metrics to evaluate the prediction performance of different algorithms.

* **Ordinal Accuracy.** This metric measures the percentage of correct relative ordinal predictions. For each contest, we predict the ordinal results of all pairs of contestants that (i) have both appeared before and (ii) have different numerical scores in the current contest. We compute the percentage of correct predictions.
* **Quantitative Loss.** This metric measures the average absolute error 4 of relative quantitative predictions. For each contest, we predict the difference in numerical scores of all pairs of contestants that have both appeared before. We then compute the quantitative loss as the average absolute error of the predictions. We normalize this number by the quantitative loss of the trivial prediction that always predicts \(0\) for all pairs. 
**Implementation.** We have implemented both \(_{1}\) and \(_{2}\) QRJA in Python. We use Gurobi Gurobi Optimization, LLC (2023) and NetworkX Hagberg _et al._ (2008) to implement \(_{1}\) QRJA and the least-square regression implementation in SciPy (Jones _et al._, 2014) to implement \(_{2}\) QRJA. To transform the contest standings into a QRJA instance, we construct a quantitative relative judgment \(J=(a,b,y)\) for each contest and each pair of contestants \((a,b)\) with \(y\) being the score difference between \(a\) and \(b\) in that contest. We set all weights to \(1\) to ensure fair comparison with benchmarks.

**Benchmarks.** We evaluate \(_{1}\) and \(_{2}\) QRJA against several benchmark algorithms. Specifically, we consider the natural one-dimensional aggregation methods Mean and Median, social choice methods Borda and Kemeny-Young, and a common method for prediction, matrix factorization. We describe how we apply these methods to our setting below.

* **Mean** and **Median.** For every contestant in the training set, we take the mean or median of that contestant's scores in training contests. We then make predictions based on differences between these mean or median scores. In one-dimensional environments like ours, means and medians are considered to be among the best imputation methods for various tasks (see, e.g., Engels and Diehr, 2003, Shrive _et al._, 2006).
* **The Borda rule.** The Borda rule is a voting rule that takes rankings as input and produces a ranking as output. We use a normalized version of the Borda rule. The \(i\)-th ranked contestant in contest \(j\) receives \(1--1}\) points, where \(n_{j}\) is the number of contestants in the contest. The aggregated ranking result is obtained by sorting the contestants by their total number of points.
* **The Kemeny-Young rule.**(Kemeny, 1959; Young and Levenglick, 1978; Young, 1988). The Kemeny-Young rule is a voting rule that takes multiple (partial) rankings of the contestants as input and produces a ranking as output. Specifically, it outputs a ranking that minimizes the number of _disagreements_ on pairs of contestants with the input rankings. Finding the optimal Kemeny-Young ranking is known to be NP-hard Bartholdi _et al._(1989). In our experiments, we use Gurobi to solve the mixed-integer program formulation of the Kemeny-Young rule given in Conitzer _et al._(2006). As this method is still computationally expensive and can only scale to hundreds of contestants, for each contest we predict, we only keep the contestants within that specific contest and discard all other contestants to run Kemeny-Young.
* **Matrix Factorization (MF).** Matrix factorization takes as input a matrix with missing entries and outputs a prediction of the whole matrix. Every row is a contestant and every column is a race. The score of a contestant in a race is the entry in the corresponding row and column. We implement several variants of MF and report results for one variant (Koren _et al._(2009)), as other variants have comparable or worse performance. For implementation details and other variants, see Appendix C.4.

Many other, related approaches deserve mention in this context. But we do not include them in the benchmarks because they do not exactly fit our setting or motivation. For example, the seminal Elo rating system Elo(1978) as well as many other methods Maher(1982); Karlis and Ntzoufras(2008); Guo _et al._(2012); Hunter and others(2004) can all predict the results of pairwise matches in, e.g.,

Figure 4: Ordinal accuracy and quantitative loss of the algorithms on all four datasets. Error bars are not shown here as the algorithms are deterministic. The results show that both versions of QRJA perform consistently well across the tested datasets.

chess and football. However, they are not originally designed for predicting the results of contests with more than two contestants.

### Experiment Results

The complete experimental results of all algorithms on the four datasets are shown in Fig. 4. Note that Borda and Kemeny-Young do not make quantitative predictions, so they are not included in Figs. 3(b), 3(d), 3(f) and 3(h).

**The performance of QRJA.** As shown in Fig. 4, both versions of QRJA perform consistently well across the tested datasets. They are always among the best algorithms in terms of both ordinal accuracy and quantitative loss.

**The performance of Mean and Median.** In terms of ordinal accuracy, Mean and Median do well on Marathon, but are not among the best algorithms on other datasets, especially on F1 (for both) and Codeforces (for Median). Moreover, for quantitative loss, they are never among the best algorithms.

**The performance of Borda and Kemeny-Young.** Borda and Kemeny-Young do not make quantitative predictions, so we only compare them with other algorithms in terms of ordinal accuracy. As shown in Fig. 4, Borda and Kemeny-Young perform very well on F1, but are not among the best algorithms on other datasets. By only using rankings as input, Borda and Kemeny-Young are more robust on datasets where contestants' performance varies a lot. However, they fail to utilize the quantitative information on other datasets.

**The performance of Matrix Factorization (MF).** MF works well across the tested datasets in terms of both metrics. In all of our four datasets, it has performance comparable to QRJA. The advantage of QRJA over MF is the interpretability of its model. The variables in QRJA have clear meanings - they can be interpreted as the strength of each contestant - in contrast to the latent factors and features in MF, which are harder to interpret. Additionally, we observe in Appendix C.2 that \(_{1}\) QRJA is more robust to large variance in contestants' performance than MF.

**Summary of experimental results.** In summary, both MF and QRJA are never significantly worse than the best-performing algorithm on any of the tested datasets, unlike the other benchmark methods. QRJA additionally offers an interpretable model. This shows that QRJA is an effective method for making predictions on contest results.

## 6 Related Work

**Random utility models.** Random utility models (Fahandar _et al._ (2017); Zhao _et al._ (2018)) explicitly reason about the contestants being numerically different from each other, e.g., one contestant is generally 1.1 times as fast as another. However, they are still designed for settings in which the only input data we have is ranking data, rather than numerical data such as finishing times. Moreover, random utility models generally do not model common factors, such as a given race being tough and therefore resulting in higher finishing times for _everyone_.

**Matrix completion.** Richer models considered in recommendation systems appear too general for the scenarios we have in mind. Matrix completion (Rennie and Srebro (2005); Candes and Recht (2009) is a popular approach in collaborative filtering, where the goal is to recover missing entries given a partially-observed low-rank matrix. While using higher ranks may lead to better predictions, we want to model contestants in a single-dimensional way, which is necessary for interpretability purposes (the single parameter being interpreted as the "quality" of the contestant).

**Preference learning.** In preference learning, we train on a subset of items that have preferences toward labels and predict the preferences for all items (see, e.g., Pahikkala _et al._ (2009)). One high-level difference is that preference learning tends to use existing methodologies in machine learning to learn rankings. In contrast, our methods (as well as those in previous work Conitzer _et al._ (2015, 2016)) are social-choice-theoretically well motivated. In addition, our methods are designed for quantitative predictions, while the main objective of preference learning is to learn ordinal predictions.

**Elo and TrueSkill.** Empirical methods, such as the Elo rating system Elo (1978) and Microsoft's TrueSkill Herbrich _et al._ (2006), have been developed to maintain rankings of players in various forms of games. Unlike QRJA, these methods focus more on the online aspects of the problem, i.e.,how to properly update scores after each game. While under specific statistical assumptions, these methods can in principle predict the outcome of a future game, they are not designed for making ordinal or quantitative predictions in their nature.

## 7 Conclusion

In this paper, we conduct a thorough investigation of QRJA (Quantitative Relative Judgment Aggregation). We pose and study QRJA and focus on an important subclass of problems, \(_{p}\) QRJA. Our theoretical analysis shows that \(_{p}\) QRJA can be solved in almost-linear time when \(p 1\), and is NP-hard when \(p<1\). Empirically, we conduct experiments on real-world datasets to show that QRJA-based methods are effective for predicting contest results. As mentioned before, the almost-linear time algorithm for general values of \(p 1,2\) relies on very complicated galactic algorithms. An interesting avenue for future work would be to develop fast (e.g., nearly-linear time) algorithms for \(_{p}\) QRJA with \(p 1,2\) that are more practical, and evaluate their empirical performance.

**Broader Impacts.** We expect our work to have a mostly positive social impact by providing an effective and interpretable method for aggregating quantitative relative judgments that can be used in applications such as predicting contest results. While for specific applications, certain desiderata may be not met by QRJA, we allow users (e.g., contest organizers) to set different weights for different judgments, which can be used to reflect the importance of different contests.