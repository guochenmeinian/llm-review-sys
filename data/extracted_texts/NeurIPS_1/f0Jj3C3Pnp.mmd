# HubRouter: Learning Global Routing via Hub Generation and Pin-hub Connection

Xingbo Du, Chonghua Wang, Ruizhe Zhong, Junchi Yan

Dept. of Computer Science and Engineering & MoE Key Lab of AI, Shanghai Jiao Tong University

{duxingbo, philipwang, zzerzerzer2271828, yanjunchi}@sjtu.edu.cn

Correspondence author. This work was partly supported by China Key Research and Development Program (2020AAA0107600), NSFC (62222607) and SJTU Scientific and Technological Innovation Funds.

###### Abstract

Global Routing (GR) is a core yet time-consuming task in VLSI systems. It recently attracted efforts from the machine learning community, especially generative models, but they suffer from the non-connectivity of generated routes. We argue that the inherent non-connectivity can harm the advantage of its one-shot generation and has to be post-processed by traditional approaches. Thus, we propose a novel definition, called **hub**, which represents the key point in the route. Equipped with hubs, global routing is transferred from a pin-pin connection problem to a hub-pin connection problem. Specifically, to generate definitely-connected routes, this paper proposes a two-phase learning scheme named HubRouter, which includes 1) **hub-generation phase**: A condition-guided hub generator using deep generative models; 2) **pin-hub-connection phase**: An RSMT construction module that connects the hubs and pins using an actor-critic model. In the first phase, we incorporate typical generative models into a multi-task learning framework to perform hub generation and address the impact of sensitive noise points with stripe mask learning. During the second phase, HubRouter employs an actor-critic model to finish the routing, which is efficient and has very slight errors. Experiments on simulated and real-world global routing benchmarks are performed to show our approach's efficiency, particularly HubRouter outperforms the state-of-the-art generative global routing methods in wirelength, overflow, and running time. Moreover, HubRouter also shows strength in other applications, such as RSMT construction and interactive path replanning.

## 1 Introduction

As the scale of integrated circuits (ICs) increases rapidly, the quality and efficiency of current Electronic Design Automation (EDA) technologies are being ceaselessly challenged. Among the various tasks from logic synthesis  to placement and routing , global routing (GR)  is one of the complex and time-consuming combinatorial problems in modern Very Large Scale Integration (VLSI) design. As a posterior step of component placement, it generates routing paths to interconnect pins of IC components from a netlist, which are already placed on the physical layout . The objective of global routing is basically to minimize total wirelength while avoiding congestion in the final layout2. However, even its simplified 'two-pin' case (see Fig. 8 in Appendix C for illustration) that routes every net with only two pins under design constraints turns out to be an NP-complete problem .

Traditional works [8; 24; 36; 43] adopt (strong) heuristics to greedily solve global routing. However, the diversity and scale could pose new challenges to classical algorithms, which call for strategy updating and improvement by human experts on a continuous basis. To mitigate the reliance on manual efforts and facilitate the overall design automation and quality, machine learning has been adopted for global routing, as one of its diverse applications in chip design ranging from logic synthesis [40; 39] to placement [32; 28], etc. Specifically, deep reinforcement learning (DRL) [31; 35] and generative models  have been adopted to tackle global routing (sometimes also along with other tasks, e.g., placement  in the design pipeline). However, DRL methods suffer from large state space and often need to spend enormous time on generating routes as the scale of grids increases on the test instance, i.e., the netlist, which is practically intimidating for real-world global routing. The generative approaches can be more computationally tractable due to the potential one-shot generation capability and train/test the model on different instances. In fact, the generative models have recently been adopted in different design tasks [5; 56] beyond EDA (partly) for its higher efficiency compared with the iterative RL-based decision-making procedure, while a common challenge is how to effectively incorporate the rules and constraints in the generative models. However, though generative global routing approaches  inject connectivity constraints into the training objective, they often degenerate to an exhaustive search in post-processing when the generated initial routes fail to satisfy connectivity, as shown in Fig. 1. Our experimental results will show that the routes for difficult nets have a very low average connectivity rate of less than 20%, which means that over 80% generated routes for difficult nets require time-consuming post-processing. This greatly harms the inference time and indicates a serious challenge for the routing problem.

To address this intractable issue, our main idea is to propose a novel definition, '**hub**', which means the (virtual) key point in the route. By transferring the pin-pin connection problem to the hub-pin connection problem, situations where routes are unconnected in generative approaches can be avoided.

Specifically, we propose a novel two-phase learning scheme for global routing, named HubRouter, which includes 1) **hub-generation phase**: A condition-guided hub generator using deep generative models under a multi-task learning framework; 2) **pin-hub-connection phase**: A post-processing rectilinear Steiner minimum tree (RSMT) construction module that links the hubs using an actor-critic model. In the generation phase, hubs, routes, and stripe masks (a practical module as illustrated in Sec. 3.1) are together generated under a multi-task framework by generative models with optional choices, including Generative Adversarial Nets (GAN) , Variational Auto-Encoder (VAE) , and Diffusion Probabilistic Models (DPM) . Though only hubs are required as outputs in this phase, we find it helpful to simultaneously generate routes and stripe masks together with hubs, where routes are used to obtain the local perception and stripe masks are capable of removing noise points. In the connection phase, we regard the connection of generated hubs as an RSMT construction problem, which is NP-complete . Equipped with an actor-critic model , this phase can be conducted in a more scalable way  with very slight errors. We also introduce a special case that when hubs in the first phase are correctly generated for an RSMT route, its reconstruction time complexity can be reduced to \(O(n n)\), which shows the scalability potential of HubRouter.

With this two-phase learning scheme, the proposed HubRouter enforces all generated routes to be connected and save the post-processing time in generative approaches . Apart from the strength in global routing, HubRouter is of generality to deal with other applications. We evaluate HubRouter in

  
**Model** & **Type** & **Multi-pin** & **Connectivity** & **Scalability** \\  BoxRouter  & Classical & ✗ & ✓ & ✗ \\ DRL  & RL & ✗ & ✓ & ✗ \\ PRNet  & Generative & ✓ & ✗ & \(^{*}\) \\ HubRouter (ours) & Generative & ✓ & ✓ & ✓ \\   

* Scalable for one-shot generation, but not scalable for post-processing.

Table 1: Characteristics of global routing approaches. Classical and RL-based methods generally transfer routing into 2-pin problems. In particular,  is highly sensitive to chip scales. The generative model PRNet can generate each route in one-shot but cannot guarantee connectivity which requires considerable post-processing for failed routes. HubRouter in this paper has all the advantages of PRNet and in particular ensures connectivity.

Figure 1: Example of an unconnected route. Up: Original route; Down: Generated route.

RSMT construction and interactive path replanning. Table 1 compares different routing methods and **the highlights of this paper are:**

1) We empirically show that the recent generative global routing approaches could suffer from the inherent non-connectivity of generated routes. Inspired by this observation, this paper proposes a new concept, namely **hub**, which means a key point in the route. With hubs generated for each net, pins can be connected efficiently under connectivity assurance.

2) Equipped with the concept of hubs, we devise HubRouter, which is a novel two-phase learning scheme for global routing, including (virtual) hub generation and pin-hub connection. Compared with generating routes directly, this scheme ensures the connectivity of all generated routes by generating intermediate hubs and then connecting them with pins. The connectivity assurance alleviates the time-consuming post-processing.

3) In the generation phase, we introduce a condition-guided generative framework under the perspective of multi-task learning, where routes and stripe masks are generated together with hubs to improve hubs' generation quality. In the connection phase, we adopt an actor-critic network to effectively simulate the RSMT construction. Moreover, we illustrate that when hubs are correctly generated for an RSMT route, the reconstruction can be optionally performed in \(O(n n)\) time complexity.

4) Multiple experiments on simulated and real-world datasets are performed to illustrate the effectiveness of HubRouter. Especially, HubRouter ensures the connectivity of generated routes and outperforms the state-of-the-art generative global routing models in wirelength, overflow, and time. We also introduce two applications other than global routing to show the generality of HubRouter.

## 2 Background and Preliminaries

**Global Routing.** In VLSI design, global routing is a stage after placement, which determines the paths for nets and interconnects the pins on a chip layout. Typically, given a physical chip and a netlist, we have a chip canvas and several nets (see Fig. 2(a)), where each net includes some pins on the fixed positions decided by macro/standard cells in the previous placement process. In global routing, the chip canvas is further divided into rectangular tiles, where the tiles are formed into a grid graph \((,)\) (see Fig. 2(b)). Nodes \(=\{v_{i}\}_{i=1}^{||}\), also named global routing cells (GCells), represent the tiles and we need to link all the tiles that contain pins. The nodes containing pins are dyed dark red in Fig. 2(b). Edges \(\) represent the paths among adjacent nodes, where each edge \(e_{ij}\) has its given capacity \(c_{ij}\) and usage \(u_{ij}\). The main objective of global routing is to connect all the required connections and on this basis, reduce the routing wirelength (WL) and overflow (OF). The overflow here refers to \(o_{ij}=(0,u_{ij}-c_{ij})\), i.e., the exceeded number of routes compared to the given capacity on a tile.

**Generative Global Routing.** Actually, global routing is a combinatorial problem and can be formulated as a 0-1 integer linear programming (0-1 ILP) problem , but it is still NP-complete. Despite that various works  denote the layout of global routing as a routing graph, generative models  give a novel insight that regards the layout as an image and treats route generation as independent conditional image generation. Specifically, pixels are used to represent tiles in global routing, and a routing output image is generated from a given condition image. The condition image contains three channels, with the first channel being the locations of pins and the other two channels

Figure 2: Diagrams of a) chip canvas; b) grid graph; c) condition image, with pin positions and capacity; d) route, and examples of 4 types of hubs; e) all hubs generated from d).

being the capacity of horizontal/vertical grid edges. The routing image is a grayscale image with a value of 255 representing routed and 0 representing unrouted. Examples of condition and routing images are respectively shown in Fig. 2(c) and 2(d).

**Hub.** Fig. 2(d) also introduces _hubs_, the main concept proposed in this paper, with 4 different types, which can be vividly represented as '\(+,,,\)'. Formally, we have

**Definition 1** (Hub).: _Given a one-channel image with \(m n\) pixels, let \(p_{ij}\)\((1 i m,1 j n)\) denote the pixel in the \(i\)-th row and \(j\)-th column, whose value \(r_{ij}=1/0\) respectively represent routed/unrouted. Auxiliary denote \(r_{0j}=r_{(m+1)j}=r_{i0}=r_{i(n+1)}=0\), then the pixel \(p_{ij}\) is a hub if and only if \(r_{ij}=1\) and it satisfies any of the following condition:_

\[(1)+:r_{(i-1)j}=r_{(i+1)j}=r_{i(j-1)}=r_{i(j+1)}=1;(2) :r_{(i-1)j}+r_{(i+1)j}+r_{i(j-1)}+r_{i(j+1)}=3;\] \[(3):r_{(i-1)j}+r_{(i+1)j}=1r_{i(j-1)}+r_{i(j+1)} =1;\ \ (4) r_{(i-1)j}+r_{(i+1)j}+r_{i(j-1)}+r_{i(j+1)}=1.\]

This means that any routed pixel is a hub unless it has exactly two opposite routed neighbor pixels. In Fig. 2(e), we give an example of all the hubs generated from Fig. 2(d).

**Difference between Hubs and Steiner Points.** The intuition of hubs is similar to the concept of Rectilinear Steiner Point (RSP) , but we claim that there are essential differences between them. RSPs are searched for a global minimum total distance, while hubs are used to determine a path. Obviously, RSPs are special cases of hubs, and hubs can generate paths of different shapes at will (not only the shortest). Furthermore, traditional approaches can obtain hubs only after accomplishing the routing, while the characteristics of machine-learning approaches naturally give them the capability of learning such hubs. We envision that HubRouter has more applications compared with RSPs. To show this, we also conduct some applications, including RSMT construction and interactive path replanning in Sec. 4.4. Related works in this work are presented in Appendix A.

## 3 From Pin-Pin to Hub-Pin Connection: A Two-phase Routing Scheme

**Approach Overview.** We formulate global routing as a two-phase learning model. The first phase is called the _hub-generation phase_, where we approximate the conditional distribution \(p_{}(|,)\) to the prior distribution \(p(|)\) when given \( p_{}()\) and condition \(\). Here \(\) is a latent variable from a prior distribution (usually assumed as Gaussian distribution) while \(\) and \(\) are respectively condition and input images (route, extracted hubs, and stripe mask). The second phase is called the _pin-hub-connection phase_, where we link the hubs generated in the first phase to obtain the final route.

Figure 3: Pipeline of the proposed two-phase scheme. 1) _hub-generation phase_: Route, hub, and mask are fed to three optional generative models, i.e., GAN, VAE, and DPM, with condition guided. The generated hub image is further masked by a stripe mask module; 2) _pin-hub-connection phase_: The hubs generated in the first phase are fed to an actor-critic network to obtain its RSMT construction by learning Rectilinear Edge Sequence (RES), and finally the connected route is obtained.

This process can be treated as RSMT construction, and we follow REST  to use an RL-based algorithm to finish the routing. We respectively introduce these two phases in Sec. 3.1 and Sec. 3.2. In the generation phase, we also propose a multi-task learning framework to improve the quality of generated hubs, especially a novel stripe mask learning method is proposed to alleviate the negative effects caused by noise points cases. The overall algorithm is in Appendix B.

### Hub-generation Phase: Multi-task Learning of Hub, Mask, and Unused Pre-Routing

Hub generation can be formulated as an image-to-image task because the prior distribution \(p(|)\) is given from training dataset \(\{_{i},_{i}\}_{i=1}^{N}\) with \(N\) samples, where \(N\) denotes the total number of nets for training. So, it seems that we can learn a distribution \(p_{}(|)\) and can generate \(^{} p_{}(|=^{})\) given a new condition \(^{}\) out of the training dataset. However, since the results of hub generation are not unique, a generative probabilistic distribution \(p_{}(|,)\) conditioned on \(\) can better describe the routing. The main objective of hub generation is to minimize the difference between probability distributions \(p(|)\) and \(p_{}(|,)\) while it often differs in expression under different perspectives. With the help of the rapid development of deep generative models, various works give us optional choices to address hub generation. In Appendix B, we respectively introduce how to incorporate GAN [11; 37], VAE [26; 44], and DPM [16; 17] into our generative framework.

Note that PRNet  has a similar generation model structure to ours, yet we fall into different flights. PRNet employs a bi-discriminator  in CGAN  to inject the connectivity constraints into the training objective. This constraint indeed works and increases the correctness (all pins are connected within one route) rate by over 10%, but it is almost useless for complicated cases, as will further be discussed in Sec. 4.2. Under our two-phase learning scheme, the connectivity can be guaranteed, but the generated noise points have a more negative impact on the final results. Thus, we propose a multi-task learning framework in the following to handle this potential weakness.

We claim that hub generation is highly different from other image generation missions because subtle noises can hardly affect their generation quality. When generating hubs, a noise point, especially the outermost one, can largely harm the wirelength of routing. We show this challenging phenomenon in Fig. 4(b) and 4(c), where two noise points (blue) in Fig. 4(b) lead to a much longer route in Fig. 4(c) since all the generated hubs should be connected within one route in our scheme. As discussed in some other special generation tasks like layout generation , high reconstruction accuracy is typically maintained during training to ensure the generation is suitable for its requirement. Following this setting, we further devise a multi-task learning framework, whereby the input in Sec. 3.1 is defined as \(=\{^{(hub)},^{(rt)},^{(msk)}\}\) with three components, respectively hub, route, and mask. Note that although the hub generation is the main purpose in the first phase, we still generate routes and masks for auxiliary usages. In particular, the routes \(^{(rt)}\) are pre-routing results and are unused in the second phase, which is employed to better obtain the continuous local perception in CNN-based networks. We further propose a novel mask learning module named _stripe mask_ to focus on bad cases for hub generation. Specifically, the stripe mask is defined as a matrix \(^{(msk)}\{0,1\}^{m n}\), where \(^{(msk)}_{ij}=1\) means there is a hub in the \(i\)-th row or \(j\)-th column; otherwise \(^{(msk)}_{ij}=0\). At inference

Figure 4: Illustration of how the stripe mask alleviates the noise impact. (a) A route sample. (b) The generated hubs, where the black/blue points are respectively hubs/noises. The gray stripes are generated masks. (c) Without the stripe mask, the route will be lengthened by some wrong and redundant paths (blue). (d) Two paths with the same length scheduled by the stripe mask.

time, the generated hubs are masked by a binarized stripe mask \(}_{ij}^{(hub)}=_{ij}^{(hub)}}_{ij}^ {(msk)}\) with

\[}_{ij}^{(msk)}=[(_{i=1}^ {m}_{ij}^{(msk)},_{j=1}^{n}_{ij}^{(msk)} )>], \]

where \([]\) is an indicator function yielding \(1/0\) if the input is correct/wrong. Eq. 1 shows that unless the noises in vertical/horizontal lines are dense, the stripe mask will not be formed. Fig. 4(c) shows that, with the help of the stripe mask, the blue noise points can be masked, and the generated route can be shortened by removing the redundant (blue) paths. Empirically, an appropriate stripe mask can eliminate most of these noise points. The reasons why we use the stripe mask rather than other masks  are due to the characteristics of global routing: Compared with other generation tasks, 1) Noise points will greatly impact the routing quality, and the stripe mask learning can eliminate most of them; 2) the routing result is not required to be unique while the stripe mask module is capable of providing different choices of routing. As shown in Fig. 4(d), two black hubs can choose an arbitrary yellow point as a turning point to connect each other while the length of paths is the same.

### Pin-hub-connection Phase: RSMT Construction

After the hub-generation phase, the hub points will be sent to the pin-hub-connection phase to link all the generated hubs and obtain the final route. In this phase, the connection process is regarded as a rectilinear Steiner minimum tree (RSMT) construction problem, which is proved to be NP-complete . Note that it seems to be possible to directly use RSMT construction to perform global routing; however, we claim that the hub-generation phase is essential because 1) Without the generated hubs, RSMT construction can only find the shortest route and fail to generate an appropriate route under restriction; 2) A good hub-generation phase for RSMT route reconstruction can be reduced to \(O(n n)\), as stated in the following theorem:

**Remark 1**.: _[Reconstruction Bonus] Suppose a set of hubs in an RSMT route is correctly generated, then its RSMT reconstruction can be performed in \(O(n n)\) time complexity._

We further illustrate it in Appendix C. Remark 1 introduces the optimal time complexity of the pin-hub-connection phase for the ideal cases. While without this strong assumption, we follow REST  to learn the Rectilinear edge sequence (RES) under an actor-critic neural network framework. Given a point set \(=\{(x_{i},y_{i})\}_{i=1}^{||}\), we want to get the RES, which is a sequence of \(||-1\) index pairs \([(v_{1},h_{1}),,(v_{||-1},h_{||-1})]\) and pair \((v_{i},h_{i})\) means connecting point \(v_{i}\) and \(h_{i}\) with rectilinear edges. RES can be modeled as a sequential decision-making problem, so it is advantageous in RL. Note that the validity and existence of RES for the RSMT construction are already proved in REST.

As shown in Fig. 3, the actor network is used to create RES results with given coordinate points, while the critic network sets a baseline to predict the expected length of RSMT constructed by the actor network and guide the actor network to obtain better performance. According to the evaluation of the critic network, the actor network will update the policy to minimize the RSMT length. Specifically, the actor network is devised via an auto-encoder to learn better representations. We draw \(B\) point sets \(=\{V_{1},V_{2},,V_{B}\}\), and construct valid RES sets \(R^{(V_{i})}\) for each point set \(V_{i}\). The objective is to minimize the expected advantage of the RSMT as follows:

\[_{}\ \ _{V_{i},r_{i} R^{(V_{i})}}[b_{ }(V_{i})-b(V_{i},r_{i})]p_{}(r_{i}), \]

where \(b_{}(V_{i})\) is a predicted length of RSMT by the critic network and \(b(V_{i},r_{i})\) is the actual length evaluated in linear time. \(p_{}(r_{i})\) is the learnable probability of generating a specific RES with regard to \(\). This objective is learned with an RL-algorithm . Apart from the actor network, the critic one is learned simply by mean square error (MSE) with gradient descent training:

\[_{}\ \ _{V_{i},r_{i} R^{(V_{i})}}\|b_{}(V_ {i})-b(V_{i},r_{i})\|^{2}. \]

At test time, in line with , we apply 8 transformations that rotate the point set by  degrees with/without swapping \(x\) and \(y\) coordinates and then choose the best solution. This trick is relatively more effective when hubs are not well generated.

## 4 Experiment and Analysis

We introduce datasets and setups in Sec. 4.1. To show the performance of HubRouter, in Sec. 4.3, we respectively perform global routing on three kinds of datasets with both classical and learning baselines. Ablation study is performed to evaluate key modules in HubRouter. In Sec. 4.4, we introduce two applications, including RSMT construction and interactive path replanning. Finally, we analyze the time overhead and show the scalability of HubRouter. Each experiment in this section is run on a machine with i9-10920X CPU, NVIDIA RTX 3090 GPU and 128 GB RAM, and is repeated 3 times under different seeds with mean and standard deviation values in line with .

### Datasets and Setups

**Datasets.** For training, we construct global routing instances by adopting NCTU-GR  to route on ISPD-07 routing benchmarks , which is in line with . For test, we split the samples outside the training set into 4 types of routes according to the number of pins and the distance of pins. In detail, 'Route-small-4' and 'Route-small' respectively represent cases with no more than and more than 4 pins and the Half-perimeter wirelength (HPWL) of pins is less than 16. Here, HPWL = \([(x_{i})-(x_{i})]+[(y_{i})-(y_{i})]\) for a group of pin positions \(\{(x_{i},y_{i})\}_{i=1}^{n_{p}}\) in a net with \(n_{p}\) pins. 'Route-large-4' and 'Route-large' are similar, whose HPWL is more than 16. Moreover, we introduce ISPD-98  routing benchmarks and some simulated small-scale cases used by a deep reinforcement learning (DRL) method , named DRL-8 and DRL-16.

**Metrics.** On ISPD-07 routing benchmarks, we choose correctness (all pins are connected within one route) rate and wirelength ratio (WLR, the ratio of the generated route length to the ground truth route length) introduced in  as our metrics. In the experiments of ISPD-98 and DRL cases, wirelength (WL), overflow (OF), and routing runtime are adopted. We also investigate some common metrics for generative models like Frechet Inception Distance (FID)  and Inception Score (IS) [2; 42], but they are not suitable for taking on the real performance on global routing.

**Other Implementation Details.** Details of training/test datasets and other protocols, including introduction of baselines and model structures, are presented in Appendix D.

### Unconnected Cases in Previous Generative Global Routing Approaches

We investigate the state-of-the-art generative global routing approach  and show that its correctness rate (CrrtR) is extremely low for cases with complex connection of pins. As declared in Table 2, the CrrtR is only 0.04 for the Route-large case. This is reasonable because connectivity is assured only when almost each pixel of routed path is correctly generated in the routing image, which is a very strict condition for complex cases. However, under our proposed two-phase learning scheme, HubRouter guarantees the CrrtR on all nets with the intermediately generated hubs. Moreover, despite

    &  & **PRNet** & **HubRouter** & **HubRouter** & **HubRouter** \\  & & **(GAN)** & **(VAE)** & **(DPM)** & **(GAN)** \\   & Route-small-4 & 0.806 & \(\) & \(\) & \(\) \\  & Route-small & 0.334 & \(\) & \(\) & \(\) \\  & Route-large-4 & 0.196 & \(\) & \(\) & \(\) \\  & Route-large & 0.040 & \(\) & \(\) & \(\) \\   & Route-small-4 & \(\) & \(1.099 0.020\) & \(1.060 0.011\) & \(1.011 0.003\) \\  & Route-small & 1.009 & \(1.042 0.006\) & \(1.174 0.009\) & \(\) \\  & Route-large-4 \({}^{*}\) & - & \(1.122 0.039\) & \(1.100 0.021\) & \(\) \\  & Route-large \({}^{*}\) & - & \(1.041 0.014\) & \(1.242 0.021\) & \(\) \\   & Route-small-4 & 14.99 & \(\) & \(673.21 5.08\) & \(7.16 0.05\) \\  & Route-small & 18.51 & \(\) & \(670.23 2.45\) & \(8.36 0.35\) \\   & Route-large-4 & 19.47 & \(\) & \(673.01 5.18\) & \(7.53 0.16\) \\   & Route-large & 19.22 & \(10.65 0.09\) & \(672.86 4.44\) & \(\) \\   

* The correctness rate is too low for PRNet to reasonably evaluate the wirelength rate.

Table 2: Experiments on ISPD-07 with 4 kinds of cases (detailedly introduced in Appendix D ). We compare HubRouter with PRNet  on three metrics. Optimal results are in **bold**.

[MISSING_PAGE_FAIL:8]

HubRouter-hr, and HubRouter-hrm. Among all, HubRouter-hrm reaches the optimal wirelength rate and HubRouter-hr also outperforms HubRouter-h, which indicates that the multi-task learning with route and stripe mask is effective. In the pin-hub-connection phase, HubRouter-RMST is denoted as the one using R-MST as the connection model. In addition, HubRouter (T=1)/(T=8) respectively represent the HubRouter with/without 8 transformations introduced in Sec. 3.2. As expected, HubRouter (T=8) performs the best. Details on hyperparameter are in Appendix E.

### Further Studies and Discussion

We envision that HubRouter can do more than just global routing. Here, we respectively perform RSMT construction and interactive path replanning to reflect the generality of HubRouter. The HubRouter in this section uses GAN as the generative embodiment due to its better performance.

**RSMT Construction.** It is intuitive that global routing for each net without overflow constraint is to minimize the wirelength, and thus is close to the problem of RSMT construction. A key question is whether the generated hubs can enhance the construction of the pure RSMT during the connection phase. To evaluate it, we test the average percent errors compared with GeoSteiner  on the random point data given by REST , together with running time, in Table 5. We compare baselines including R-MST 1, BGA , FLUTE  and REST . When degree is smaller than 25 REST performs the best, but HubRouter outperforms others when degree is larger than 30. In particular, an unintuitive phenomenon shows that HubRouter performs even better with a higher degree. This is due to the characteristic of hub generation since a noise point in a dense case has much less impact on the error of wirelength than in a sparse case. Apart from the error, REST has the shortest running time depending on large batch size. However, HubRouter has to run sequentially in this task because degrees are different after generating some new hubs.

**Interactive Path Replanning.** Path replanning  refers to the process of revising a given path when the environment changes. It is a technique used in human-machine interaction. HubRouter is capable of deciding a new path with user-defined hubs. Fig. 6 shows samples of interactive path replanning results, where HubRouter connects new points and maintains most of the previous paths.

**Analysis of Scalability and Time Overhead.** We claim that HubRouter is scalable in both phases. In the generation phase, for a fixed scale of routing graph, similar to PRNet , the hub generation is one-shot. This implies that its generation speed is unrelated to the number of pins and is linear to the number of nets, as respectively shown in Fig. 7 (left/middle). For connection, we show in Sec. 3.2 that the RSMT construction can degrade to a \(O(n n)\) R-MST problem under some ideal assumptions. In addition, as depicted in Fig. 7 (right), equipped with an actor-critic network, the time overhead of HubRouter increases more slowly than pure RSMT construction.

**Non End-to-End framework.** HubRouter is a two-phase framework that is designed with careful analysis. It differs from PRNet , which is an end-to-end attempt to produce the routes directly, but

    & **Model** & HubRouter-h & HubRouter-hr & HubRouter-hrm \\   & **WLR** & 1.031 & 1.022 & 1.005 \\   & **Model** & HubRouter-RMST & HubRouter (T=1) & HubRouter (T=8) \\   & **WLR** & 1.099 & 1.087 & 1.005 \\   

Table 4: Ablation Study on ISPD-07 with HubRouter (using GAN as generative embodiment).

Figure 6: Samples of interactive path replanning. The first line is routes with some added points while the second line is the replanned paths.

faces the challenge of non-connectivity and inefficient post-processing. HubRouter addresses this by generating hubs in the middle stage and connecting them with pins to ensure connectivity. However, this two-stage framework involves a discretization process, which prevents differentiable learning and simultaneous optimization. We also pursuit the cost-effective end-to-end solver for further work.

## 5 Conclusion and Outlook

We have proposed HubRouter, a generative global router in VLSI with a two-phase learning scheme: the hub-generation phase and the pin-hub-connection phase. In the generation phase, HubRouter generates hubs instead of routes while in the connection phase, it connects the hubs with RSMT construction. Experiments on real-world and simulated datasets show its effectiveness as well as scalability. When adopting HubRouter, the potential negative impacts should be taken care: 1) The incorrect generation might lead to poor routing results; 2) Generative models are computationally intensive and might consume a lot of resources.

This paper also has some _limitations_ for future work: 1) The two-phase scheme is not end-to-end under a joint training scheme; 2) The supervised module for hub generation depends on large amount of training data; 3) Like SOTA generative global routing algorithms , the further decrease on overflow needs a reroute process; 4) Finally, it would also be compelling to integrate the learning-based methods of logic synthesis , with routing methodologies, including those presented in this work. This would allow for a data-driven approach to the chip design pipeline.

    &  &  \\   & **GeoSt** & **R-MST** & **BGA** & **FLUTE** & **REST** & **HubRouter** & **GeoSt** & **REST** & **HubRouter** \\ 
**5** & 0.00 & 10.91 & 0.23 & **0.00** & **0.00** & 0.24\(\)0.07 & 0.31\(\)0.00 & 11.89\(\)0.24 & 13.44\(\)2.41 \\
**10** & 0.00 & 11.96 & 0.48 & 0.04 & **0.01** & 0.18\(\)0.02 & 4.05\(\)0.03 & 12.16\(\)0.10 & 16.43\(\)0.30 \\
**15** & 0.00 & 12.19 & 0.53 & 0.06 & **0.03** & 0.15\(\)0.01 & 9.26\(\)0.08 & 13.80\(\)0.43 & 16.92\(\)0.43 \\
**20** & 0.00 & 12.41 & 0.57 & 0.11 & **0.07** & 0.13\(\)0.04 & 17.35\(\)0.19 & 14.79\(\)0.17 & 19.85\(\)1.32 \\
**25** & 0.00 & 12.47 & 0.58 & 0.18 & **0.12** & **0.12\(\)0.04** & 29.01\(\)0.07 & 16.68\(\)0.54 & 23.01\(\)1.99 \\ 
**30** & 0.00 & 12.56 & 0.60 & 0.23 & 0.16 & **0.10\(\)0.04** & 43.00\(\)0.17 & 18.53\(\)0.29 & 27.02\(\)3.0 \\
**35** & 0.00 & 12.63 & 0.62 & 0.26 & 0.21 & **0.09\(\)0.04** & 60.32\(\)0.20 & 19.67\(\)0.38 & 33.11\(\)1.22 \\
**40** & 0.00 & 12.65 & 0.63 & 0.29 & 0.25 & **0.08\(\)0.04** & 78.80\(\)0.06 & 20.91\(\)0.51 & 39.42\(\)2.36 \\
**45** & 0.00 & 12.67 & 0.63 & 0.30 & 0.32 & **0.07\(\)0.04** & 102.26\(\)0.21 & 22.36\(\)0.42 & 49.57\(\)0.58 \\
**50** & 0.00 & 12.72 & 0.64 & 0.29 & 0.36 & **0.06\(\)0.04** & 125.26\(\)0.41 & 23.79\(\)0.20 & 61.55\(\)1.07 \\   

Table 5: Experiments on RSMT construction. HubRouter and four baselines are compared to the RSMT solver GeoSteiner (GeoSt) and the best results on errors are in **bold**. Time overhead is also evaluated on GeoSt, REST, and HubRouter. Mean and variance are given with three trials.

Figure 7: Generation time of HubRouter is constant to number of pins (left) and is linear to number of nets (middle). The RSMT construction time of HubRouter rises slower than GeoSteiner  (right).