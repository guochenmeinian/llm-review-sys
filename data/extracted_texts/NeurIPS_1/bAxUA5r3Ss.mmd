# TaskBench: Benchmarking Large Language Models for Task Automation

Yongliang Shen\({}^{1}\), Kaitao Song\({}^{2}\)\({}^{}\), Xu Tan\({}^{2}\), Wenqi Zhang\({}^{1}\),

**Kan Ren\({}^{2}\), Siyu Yuan\({}^{3}\), Weiming Lu\({}^{1}\)\({}^{}\), Dongsheng Li\({}^{2}\), Yueting Zhuang\({}^{1}\)\({}^{}\)**

Zhejiang University\({}^{1}\), Microsoft Research Asia\({}^{2}\), Fudan University\({}^{3}\)

{syl,luwm,zhangwenqi}@zju.edu.cn, syyuan21@m.fudan.edu.cn

{kaitaosong, xuta, dongsli}@microsoft.com

[https://github.com/microsoft/JARVIS](https://github.com/microsoft/JARVIS)

###### Abstract

In recent years, the remarkable progress of large language models (LLMs) has sparked interest in task automation, which involves decomposing complex tasks described by user instructions into sub-tasks and invoking external tools to execute them, playing a central role in autonomous agents. However, there is a lack of systematic and standardized benchmarks to promote the development of LLMs in task automation. To address this, we introduce TaskBench, a comprehensive framework to evaluate the capability of LLMs in task automation. Specifically, task automation can be divided into three critical stages: task decomposition, tool selection, and parameter prediction. To tackle the complexities inherent in these stages, we introduce the concept of Tool Graph to represent decomposed tasks and adopt a back-instruct method to generate high-quality user instructions. We propose TaskEval, a multi-faceted evaluation methodology that assesses LLM performance across these three stages. Our approach combines automated construction with rigorous human verification, ensuring high consistency with human evaluation. Experimental results demonstrate that TaskBench effectively reflects the capabilities of various LLMs in task automation. It provides insights into model performance across different task complexities and domains, pushing the boundaries of what current models can achieve. TaskBench offers a scalable, adaptable, and reliable benchmark for advancing LLM-based autonomous agents 1.

## 1 Introduction

Due to the recent advances of large language models (LLMs) , LLM-empowered autonomous agents  have unveiled remarkable potential towards artificial general intelligence and become a new rising trend in the realm of AI research. Generally, within the realm of LLM-empowered autonomous agents, task automation is considered as the most important component, which aims to leverage LLMs to autonomously analyze user instructions and accomplish their objectives. Consequently, many researchers attempt to delve deeper into LLM to enable more intelligent task automation . However, it is worth noting that a critical challenge in advancing this area is the lack of a systematic and standardized benchmark to thoroughly evaluate the capability of LLMs in automating tasks. Therefore, creating such a benchmark to facilitate research in this area has become an urgent need.

Constructing a benchmark for task automation presents unique challenges beyond traditional NLP tasks. As shown in Figure 1, task automation involves multiple stages: task decomposition, tool selection, and parameter prediction. This complexity requires sophisticated evaluation metrics to assess LLM performance comprehensively. Real-world scenarios often feature intricate user instructions with multiple subtasks and tool dependencies, necessitating benchmarks that can simulate this complexity accurately. A key challenge lies in modeling tool dependencies, which is crucial for effective task automation but difficult to capture in both datasets and evaluation metrics. Additionally, generating a high-quality, diverse dataset that reflects real-world scenarios is challenging, as it requires creating instructions that are both natural and aligned with realistic tool usage patterns. These challenges underscore the need for a carefully designed benchmark that can accurately evaluate LLMs' capabilities in task automation across various dimensions and scenarios.

In light of these challenges, we found that existing benchmarks fall short of adequately showcasing the full potential of LLMs in autonomous task completion. Traditional LLM benchmarks, such as MMLU , GSM8K , and AGIEval  assess general knowledge and reasoning skills. Recent tool-augmented LLM benchmarks have made significant strides but still fall short in various aspects. APIBench , ToolBench , and MetaTool  have advanced the evaluation of LLMs' tool usage capabilities, but they often lack comprehensive assessment across all stages of task automation. For instance, MetaTool primarily focuses on tool selection, while ToolQA  emphasizes question-answering accuracy. Similarly, benchmarks like ToolAlpaca  and API-Bank  offer valuable insights but are limited in their evaluation scope and tool complexity modeling.

To address these limitations, we introduce TaskBench, a comprehensive framework for evaluating LLMs' capabilities in task automation. Our approach integrates innovative data generation techniques with rigorous evaluation methods. We begin by introducing the concept of a Tool Graph (TG), which represents relationships and dependencies between various tools, overcoming the limitations of simple API documentation or template-based approaches used in existing benchmarks. Based on this Tool Graph, we employ a back-instruct strategy to generate high-quality user instructions from sampled subgraphs, ensuring diverse and realistic task scenarios. To guarantee the naturalness and utility of the dataset, we implement a multi-step quality control process, including structured sampling, self-critic mechanisms (both LLM-based and rule-based), and extensive human verification. These methods significantly enhance data quality, surpassing the verification approaches used in most existing benchmarks. Finally, we propose TaskEval, a multi-faceted evaluation methodology that

Figure 1: LLM-Based Task Automation: This process involves task decomposition, tool selection, and parameter prediction by LLM-based agents to autonomously complete tasks. Upon receiving a user request, the large language model performs task decomposition, generating a sequence of task steps. Simultaneously, it predicts the tool invocation graph, which encompasses both the selection of appropriate tools and the prediction of their parameters.

comprehensively assesses LLM performance across task decomposition, tool selection, and parameter prediction stages, offering a more thorough evaluation than existing tool-augmented benchmarks.

Our contributions can be summarized as follows:

* We introduce TaskBench, a novel benchmark for evaluating LLMs in task automation, featuring an innovative data generation process that addresses the data deficiency and limitations in existing tool-augmented LLM benchmarks.
* We present TaskEval, a comprehensive evaluation framework that quantitatively assesses LLMs' capabilities in task decomposition, tool selection, and parameter prediction, providing more nuanced insights than the metrics used in current benchmarks.
* The experimental results on different LLMs and additional dataset analysis demonstrate that our proposed TaskBench can effectively reflect the capability of LLMs in multiple dimensions with the support of TaskEval and show high correlations with human evaluation.

## 2 Related Works

Large language models (ChatGPT , GPT-4 , LLAMA , Bard ) have drawn the development of autonomous agents (e.g., AutoGPT , HuggingGPT , BabyAGI ). These applications can be considered as a form of task automation, which uses LLMs as the controller to analyze user instructions and search for the most suitable solution (e.g., external models) to obtain answers. With the increasing demand for advanced task automation, numerous benchmarks have emerged to assess the capability of large language models LLMs to effectively interact with external tools. However, these benchmarks vary significantly in terms of data generation, tool dependency modeling, quality control mechanisms, and evaluation, as shown in Table 1.

Several benchmarks assess the interaction between LLMs and tools, primarily focusing on tool usage and API calls. APIBench  and ToolBench  use API documentation to generate tasks, but their reliance on template-based sampling can limit the logical consistency of generated tasks. MetaTool  introduces template-based tool generation for decision-making, focusing on whether a tool is needed, though it lacks support for modeling complex tool dependencies. ToolAlpaca 

  
**Benchmark** & **TaskBench** & **APIBench** & **ToolBench** & **MetaTool** & **API-Bank** \\   & Tool Graph + &  &   } & Template Generation &  \\  & Back-Instruct & & LLM & (Diverse, Keyword, Emotional) & \\ 
**Tool Dependency** & \(\) & \(}\) & \(}\) & \(}\) & \(}\) \\   & LLM &   } &   } &   } & Auto- \\  & Self-critique + & & & & \\  & Rule-based & Human & & Human & & \\  & Self-critique + & & Verification & & Verification & \\  & Verification & & & & & \\   & Task &   } & Tool & Tool Usage & API call \\  & Decomposition & Tool & Selection + & Awareness + & correctness, \\  & + Parameter & Selection & Parameter & Tool & response \\  & Prediction & & Prediction & Selection & quality \\   & Single tool to &   } & Mainly & Single to & Single to & Single to \\  & graph structures & & single tool & multi-tool & multi-tool & Single to \\   &  & 2,365 & 12,657 & 21,127 & 6,135 & \\  & samples & samples & samples & samples & interactions \\   

Table 1: Comparison of TaskBench with Selected Tool-Augmented LLM Benchmarksexplores self-instruct to generate tasks but does not adequately address dependencies between tools. Similarly, AgentBench  evaluates models across simulated environments but focuses more on agent-like behavior than tool interactions. Other benchmarks, like ToolAlpaca , ToolQA  and GPT4Tools , concentrate on task completion or QA correctness in specific domains. While these works explore how well models complete tasks by consulting external tools, they rely heavily on human verification or simple template-based generation, potentially limiting their scalability and diversity.

Unlike prior benchmarks that rely solely on API documentation or template-based task generation, TaskBench introduces a Tool Graph to model real-world dependencies between tools, simulating complex tool interactions more accurately. Furthermore, the Back-Instruct strategy aligns tool subgraphs with instructions, significantly reducing the risk of hallucination and improving data authenticity. Its unique tool graph approach, focus on dependency modeling, and multi-domain coverage make it a more reliable, practical, and scalable benchmark for evaluating LLM capabilities in complex task automation scenarios.

## 3 TaskBench

In this section, we introduce the construction of TaskBench, the benchmark meticulously designed to facilitate the development of LLMs in task automation. Specifically, unlike previous methods which use collection or instruction methods, TaskBench can consider the complex relationships among multiple tasks to simulate more practical and complex user instruction. Figure 2 illustrates the entire process of our method to build the datasets. More details will be introduced in the following subsections.

### Preliminary: Tool Graph

Task automation in real-world scenarios often involves complex user instructions encompassing multiple sub-tasks, each potentially requiring the invocation of specific tools . These sub-tasks may have temporal or resource dependencies. To capture this complexity, we introduce the concept of the Tool Graph (TG). A TG is formally defined as \(=T,D\), where \(T=t_{1},t_{2},,t_{n}\) represents a collection of tools, and \(D\) is a set of dependencies \((t_{a},t_{b})\) indicating that tool \(t_{a}\) depends on tool \(t_{b}\). This structure offers a more effective way to organize tools and their relationships compared to traditional taxonomy trees used in . In the next subsection, we will introduce how to build a tool graph and utilize it to formulate our benchmark.

### Dataset Construction

To accomplish user intent, LLMs usually adopt a stepwise process (e.g., task decomposition\(\)tool selection\(\)parameter prediction) to analyze the user request and convert it into multiple executable tasks. Therefore, it is essential to construct the dataset and allow LLMs to evaluate their automation capability in the above process.

To guarantee that the generated user instructions could cover the expected tasks and dependencies, we adopt a back-instruct strategy to simulate data. More specifically, it can summarized as three steps: 1) we first collect a tool repository and build a tool graph \(\) with a collection of tools and their dependencies; 2) then we sample a sub-graph from \(\), to obtain a specified structure; 3) based on the sampled tool sub-graph, we use LLMs to generate user instruction via back-instruct. More details are introduced as below:

#### 3.2.1 Tool Graph Construction

Building a tool graph requires us to collect many standalone tools from different sources. When combining different tools together, the dependencies among tools could be diverse, encompassing resource dependencies, temporal dependencies, environment dependencies, and so on. In our research, we mainly investigate two of them: resource and temporal dependencies. For the former one, it means the two tools can have a connection if the input type of tool \(t_{a}\) can match the output type of tool \(t_{b}\). For the latter one, we devise tool graphs that highlight temporal dependencies, allowing any two toolsto be linked to illustrate their order. In this work, we choose three scenarios to build the datasets for our benchmark2:

Hugging FaceHugging Face  provides a wide variety of AI models to cover massive tasks across language, vision, audio, video, and so on. Each task defined by Hugging Face can be viewed as a tool to address a specific task. Specifically, each tool in Hugging Face has determined the type of its input and output. Hence, if tool \(t_{a}\) and \(t_{b}\) have a connection, the input type of \(t_{a}\) should match the output type of \(t_{b}\). Guided by this principle, we constructed Hugging Face's tool graph, comprising 23 tools and 225 edges.

MultimediaIn contrast to the Hugging Face tools, which are tailored for AI tasks, the multimedia tools is broader in scope. It provides more user-centric tools like file downloader, video editor, and so on. The policy for tool connections is the same as the Hugging Face domain. Finally, we could construct a tool graph over multimedia tools with 40 nodes and 449 edges.

Daily Life APIsSometimes, we also need some daily life services, including web search, shopping, and etc. Hence, these daily life APIs can also be considered as tools for specific tasks. However, it is worth noting that the type of dependencies among these APIs is predominantly temporal. Therefore, two daily life APIs have a successive order if they are connected. In this scenario, we can build a tool graph with 40 nodes and 1,560 edges.

#### 3.2.2 Sampling on Tool Graph

Based on the above steps, we can sample a sub-graph from the constructed TG and keep the connections of sampled tools from the TG to capture the dependencies between tools. Following the setting of HuggingGPT, we categorize the sub-structure of a TG into three types: node, chain, and directed acyclic graph (DAG). Each type embodies a specific pattern for tool invocation:

* **Node** represents standalone tool invocations, suitable for addressing simple tasks necessitating only a single tool.
* **Chain** corresponds to sequential tool invocations, where tools need to be stepwise executed to complete a task.
* **DAG** depicts more intricate tool invocations. A tool might rely on multiple preceding tools or influence several subsequent tools.

Figure 2: Construction of the TaskBench: Initially, the toolbox is transformed into a tool graph by establishing connections based on tool dependencies. We then sample diverse subgraphs from this tool graph, which can be individual nodes, linear chains, or directed acyclic graphs (DAGs). Using these sampled tool subgraphs, we “back-instruct” the LLM to inversely craft user instructions, task steps, and tool invocation graphs. Additionally, we employ critics to assess the consistency between the generated tool invocation graphs and the corresponding sampled tool subgraphs.

By sampling sub-graphs from these three substructures, we can emulate a variety of valid tool invocation patterns for user instruction. We represent the tool subgraph in \(\) as \(_{s}=\{T_{s},D_{s}\}\), where \(T_{s}=\{t_{s1},t_{s2},,t_{sk}\}\) with \(k<n\) and \(D_{s}=\{(t_{sa},t_{sb})\}\), such that \(t_{sa}\) and \(t_{sb}\) belong to \(T_{s}\). The sampling of the tool graph can be described as:

\[(,,)_{s}, \]

where the mode specifies the sampling mode (e.g., Nodes, Chains, DAGs), and the size indicates the number of tools (Here we set its range as \(\{1,2,...,10\}\)). These factors determine the topological nature and magnitude of the tool sub-graph in user instructions, respectively.

#### 3.2.3 Back-Instruct

Next, based on the sampled sub-graph \(_{s}\), we use LLMs to synthesize user instructions. We term this process Back-Instruct, which can considered as a data engine to convert the sampled tools into user instruction. Specifically, given a sampled subgraph \(_{s}\), we formulate the following Back-Instruct procedure, empowering LLMs to produce the corresponding instructions \(I\):

\[_{1}(_{s}=(T_{s},D_{s})) I. \]

Here, the sampled sub-graph \(_{s}\) can instruct LLMs to generate user requests covering these related sub-tasks, and further with their dependencies. Such a strategy ensures the complexity and quality of the generated data.

Specifically, we note that sampled sub-graphs can only provide information on tool invocation skeletons, lacking the critical parameters for tool execution. Therefore, based on the generated instruction \(I\) in Eqn. 2, we let the LLM to populate the parameters for the tool subgraphs and generate the final tool invocation graph \(}\) along with the corresponding task decomposition steps \(P\):

\[_{2}(_{s}=(T_{s},D_{s}),I)\{P, }\}. \]

#### 3.2.4 Quality Control Mechanisms

To ensure high-quality data, we implement a multi-step quality control process:

Self-Critic Mechanisms:A we introduce a self-critic mechanism to check and filter out the generated instruction to guarantee quality. Here, we offer two variants: LLM-based and rule-based. The former aims to use LLM to check the alignments between the generated data and the sampled tool sub-graph. While the latter uses straightforward rules to determine the alignment between the tool graphs in created data and the sampled tool graphs. Here, we use the nodes and edges of the sampled graph to determine the consistency. Figure 2 illustrates each step of our data engine to simulate user instructions.

Human Verification:We implemented a rigorous human verification process to further ensure the quality and coherence of the dataset. Human experts reviewed the generated instructions across different task complexities and confirmed their logical consistency and alignment with the intended tasks. Before beginning the verification process, we provided detailed instructions and conducted a calibration session for all experts. This session was essential to standardize the evaluation criteria across different reviewers and ensure consistency in their assessments. During this calibration, we provided sample cases and discussed the scoring criteria (naturalness, complexity, alignment) to align the understanding among the experts. This verification process covered instructions generated from sub-graphs of varying sizes, ensuring that the instructions are meaningful and practical. This comprehensive review provides additional assurance that the dataset meets high standards of quality.

Based on the above steps, we build TaskBench across three domains, which use GPT-4 as the data engine. The ratio of different modes (i.e., Node, Chain, DAG) is set as \(3:7:8\) for sampling and the ratio for the number of different tools is set as \(\{0.1,0.2,0.3,0.2,0.1,0.05,0.025,0.025\}\). More detailed designs about our data engine and the statistics of the constructed datasets are provided in the Appendix A.5.

### Evaluation of the Dataset Quality

To illustrate the quality of the TaskBench datasets, we conducted comprehensive human evaluations based on generated samples. Additionally, we performed a case study and an error analysis of the datasets. For further details, please refer to Appendix A.1 and Appendix A.2.

Evaluation MetricsTo evaluate the quality of datasets constructed by Back-Instruct, we designed three metrics: two metrics (i.e., Naturalness and Complexity) to assess the quality of the instructions, and one metric (i.e., Alignment) to evaluate the tool invocation graph. More details are in below:

* **Naturalness:** This metric evaluates how reasonable the instructions are, considering factors such as the typicality of dependencies between tools and their relevance to real-world applications.
* **Complexity:** This metric assesses the complexity of the instructions by examining aspects such as the depth of the task, the number of tools involved, and the interrelations among these tools.
* **Alignment:** This measures how well the tool invocation graphs align with the instructions, specifically evaluating whether the graphs effectively fulfill the user's commands.

Each metric is scored on a scale from 1 to 5. These metrics are designed to assess the effectiveness and faithfulness of our TaskBench in task automation.

Comparison with BaselinesTo make a fair comparison, we choose two additional baselines to compare our Back-Instruct:

* **Back-Instruct (Ours):** e sample tool subgraphs and then backtranslate to instructions and further refine the tool invocation graph.
* **Back-Instruct w/o edges:** Unlike our complete Back-Instruct, this version omits edges from the sampled tool subgraphs, retaining only the tool node information in the prompts.
* **Self-Instruct:** As described by , this method uses manually labeled demonstrations and descriptions of all tools. We utilize GPT-4 to autonomously select tools and generate instructions along with tool invocation graphs.

Evaluation ResultsDuring the human evaluation, we randomly selected 50 samples from our TaskBench and invited three domain experts to assess their quality. To ensure a fair and unbiased evaluation, all samples were anonymized. We provided canonical samples to help these experts calibrate their assessment criteria during the annotation process. The final results are the average scores from all experts' ratings, and are detailed in Table 2.

We observed that all methods (Self-Instruct and Back-Instruct) effectively ensured alignment. However, our method, Back-Instruct, achieved the highest scores in Naturalness and Complexity. We attribute these superior results to the realistic resource and temporal dependencies in our sampled tool subgraphs, which enable us to generate more natural and complex instructions, especially in scenarios involving multiple tools. This graph structure guides the generation process, resulting in more natural and complex instructions that reflect realistic task scenarios. The performance difference between back-instruct with and without edges further underscores the importance of capturing tool dependencies. Including edge information in the Tool Graph allows for a more comprehensive understanding of tool relationships, resulting in more natural and complex instructions.

## 4 TaskEval

To comprehensively evaluate LLMs' capabilities in task automation, we introduce TaskEval, a systematic evaluation framework that assesses three critical aspects: task decomposition, tool selection, and parameter prediction. Unlike existing benchmarks that focus on isolated aspects of tool usage or API interactions, TaskEval provides a holistic assessment of the entire task automation process. To ensure standardized evaluation, we employ a consistent prompting strategy that guides each model through a structured sequence: first decomposing user requests into sub-tasks, then

   Methods & Naturalness\(\) & Complexity\(\) & Alignment\(\) & Overall\(\) \\  Back-Instruct & **3.89** & **4.01** & **3.66** & **3.85** \\ Back-Instruct w/o edges & 3.44 & 3.27 & 3.62 & 3.44 \\ Self-Instruct & 2.18 & 2.01 & 3.64 & 2.61 \\   

Table 2: Human evaluation (rating from 1 to 5) on samples constructed by different methods. Average score rating from three human experts.

selecting appropriate tools with parameters, and finally constructing a complete tool invocation graph. For our evaluations, we primarily focus on the GPT series [1; 2; 4], Gemini , Claude  and open-source LLMs [5; 31; 32; 33; 34; 35]. For comprehensive evaluations of other open-source LLMs [36; 37; 38], please refer to Appendix A.7.

### Task Decomposition

To evaluate LLMs' ability to understand and break down complex tasks, we assess the quality of task decomposition through three complementary ROUGE metrics: **Rouge-1 (_R1_)**, **Rouge-2 (_R2_)**, and **Rouge-L (_RL_)**. Our analysis, presented in Table 12, reveals several key findings: (1) GPT-4 consistently demonstrates superior task decomposition abilities, achieving approximately 10% higher scores in both R1 and R2 compared to other models. This performance gap indicates its enhanced capability to understand and structure complex tasks. (2) Codellama-13b shows particular strength in the "Daily Life APIs" domain, achieving scores of 89.86 in R1 and 83.27 in R2, suggesting that code-centric pre-training enhances the ability to understand structured task sequences. (3) The performance gap between models widens as task complexity increases, indicating that advanced reasoning capabilities become more crucial for complex task decomposition.

    & &  & & & & & & & \\   &  &  &  & & & & & & \\   & & _n-F1_\(\) & _n-F1_\(\) & _e-F1_\(\) & _NED_\(\) & _n-F1_\(\) & _e-F1_\(\) & _n-F1_\(\) & _e-F1_\(\) \\   & gpt-4 & 84.34 & 80.79 & 55.73 & 39.70 & 82.86 & 56.39 & 81.54 & 54.70 \\  & gemini-pro & 77.46 & 76.12 & 45.51 & 43.10 & 79.05 & 49.36 & 76.62 & 43.50 \\  & Claude-2 & 69.83 & 80.67 & 48.11 & 40.03 & 84.52 & 53.40 & 79.00 & 43.51 \\  & gpt-3.5-turbo & 56.91 & 72.63 & 39.92 & 46.52 & 73.79 & 38.55 & 69.49 & 33.36 \\  & text-davinci-003 & 40.71 & 66.05 & 36.04 & 48.57 & 64.64 & 34.19 & 59.38 & 29.37 \\   & mistral-7b-v0.3 & 60.74 & 67.00 & 25.70 & 52.74 & 68.55 & 26.37 & 65.96 & 21.91 \\  & codellama-13b & 43.68 & 55.65 & 17.80 & 62.23 & 52.87 & 13.19 & 53.16 & 14.64 \\  & nous-hermes-13b & 58.66 & 52.39 & 9.01 & 62.48 & 51.99 & 6.33 & 53.62 & 8.29 \\  & vicuna-13b-v1.5 & 51.74 & 50.37 & 8.40 & 66.83 & 52.46 & 9.06 & 50.82 & 7.28 \\  & llama-2-13b-chat & 43.59 & 49.87 & 8.22 & 64.99 & 49.60 & 9.11 & 48.47 & 7.30 \\   & gpt-4 & 97.13 & 89.70 & 69.29 & 28.93 & 92.32 & 71.64 & 90.90 & 69.27 \\  & gemini-pro & 73.61 & 82.65 & 55.50 & 35.62 & 85.29 & 57.80 & 81.54 & 52.07 \\  & clavude-2 & 66.16 & 83.95 & 59.22 & 33.41 & 82.98 & 54.28 & 80.94 & 53.01 \\  & text-davinci-003 & 59.15 & 76.87 & 50.79 & 38.54 & 79.00 & 50.69 & 73.97 & 45.81 \\  & gpt-3.5-turbo & 53.55 & 76.81 & 50.30 & 39.05 & 78.65 & 49.52 & 72.83 & 44.02 \\   & mistral-7b-v0.3 & 64.00 & 78.32 & 41.12 & 40.75 & 79.96 & 41.36 & 76.11 & 35.34 \\  & codellama-13b & 43.70 & 66.89 & 28.77 & 46.35 & 68.68 & 28.79 & 62.78 & 24.61 \\  & vicuna-13b-v1.5 & 66.64 & 59.18 & 16.49 & 54.17 & 61.40 & 13.95 & 60.61 & 14.78 \\  & nous-hermes-13b & 60.58 & 58.53 & 9.47 & 56.02 & 59.39 & 9.57 & 58.97 & 8.90 \\  & llama-2-13b-chat & 38.02 & 45.14 & 1.62 & 65.29 & 45.95 & 2.11 & 43.87 & 1.63 \\   & gpt-4 & 95.97 & 97.06 & 83.47 & 38.69 & 96.41 & 42.01 & 96.91 & 80.53 \\  & clade-2 & 79.57 & 95.36 & 80.68 & 39.93 & 93.85 & 41.04 & 93.52 & 75.31 \\  & gemini-pro & 76.15 & 92.79 & 64.58 & 41.64 & 89.68 & 28.42 & 90.75 & 59.45 \\  & gpt-3.5-turbo & 52.18 & 90.80 & 70.66 & 43.50 & 86.94 & 30.85 & 85.37 & 60.67 \\  & text-davinci-003 & 68.49 & 82.15 & 60.12 & 47.14 & 76.81 & 24.54 & 80.42 & 54.90 \\   & codellama-13b & 89.75 & 87.80 & 65.92 & 44.42 & 83.61 & 27.47 & 87.73 & 63.16 \\  & mistral-7b-v0.3 & 81.55 & 80.52 & 50.95 & 51.80 & 79.17 & 25.04 & 80.54 & 45.87 \\  & vicuna-13b-v1.5 & 80.59 & 73.74 & 13.24 & 51.43 & 67.92 & 5.62 & 75.67 & 12.48 \\  & nous-hermes-13b & 82.50 & 71.17 & 3.55 & 53.47 & 70.65 & 2.86 & 73.45 & 3.50 \\  & llama-2-13b-chat & 34.11 & 57.61 & 20.13 & 67.06 & 56.18 & 8.42 & 55.77 & 17.02 \\   

Table 3: Evaluation for tool selection. Node F1 _(n-F1)_ and Edge F1 _(e-F1)_ for node and edge prediction. For nodes, a prediction is deemed positive if the predicted node’s ID aligns with any of the ground-truth node labels. For edges, both the source and target nodes of a predicted edge must correspond exactly. Normalized Edit Distance _(NED)_ measures the normalized number of operations required to correct the prediction for chain structure.

### Tool Selection

To assess LLMs' ability to select and connect appropriate tools, we evaluate their construction of tool invocation graphs, where nodes represent individual tools and edges capture dependencies between them. We introduce three complementary metrics designed to capture different aspects of tool selection accuracy: (1) **Node F1 (n-F1)**: Evaluates the accuracy of tool selection by comparing the predicted tools with the reference set, measuring the model's ability to identify appropriate tools for each sub-task. (2) **Edge F1 (e-F1)**: Assesses the model's understanding of tool dependencies by comparing the predicted connections between tools with the reference graph, capturing the ability to understand tool interaction patterns. (3) **Normalized Edit Distance (NED)**: Specifically measures the sequential accuracy in chain structures, evaluating how well models understand and maintain the correct order of tool operations.

Results in Table 3 reveal several important patterns: (1) Edge prediction consistently proves more challenging than node prediction, with F1 score differences of approximately 20% across all models, indicating that understanding tool relationships is more complex than identifying individual tools. (2) Performance varies significantly with task structure complexity - while open-source models like Mistral-7b and CodeLlama-13b compete well with GPT-3.5-Turbo on simpler node structures, they show notable limitations when handling more complex dependencies. (3) GPT-4 maintains more consistent performance across different structure types, suggesting better generalization to complex tool interactions.

    &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\   &  & } & } & } & } & } & } & } & } & } & } \\    & & & & & & & & & & & & \\   & gpt-4 & 80.05 & 74.10 & 76.66 & 58.15 & 78.24 & 60.03 & 77.31 & 60.86 \\  & gemini-pro & 67.63 & 56.54 & 66.60 & 46.35 & 70.41 & 50.56 & 67.12 & 48.54 \\  & Claude-2 & 48.07 & 32.14 & 66.35 & 45.57 & 68.59 & 48.19 & 63.00 & 43.08 \\  & text-davinci-003 & 38.51 & 27.43 & 56.90 & 38.76 & 57.03 & 38.90 & 52.53 & 36.04 \\  & gpt-3.5-turbo & 37.70 & 19.81 & 60.96 & 41.15 & 61.33 & 42.89 & 55.88 & 36.32 \\   & mistral-7b-v0.3 & 29.18 & 13.19 & 46.18 & 26.09 & 45.49 & 28.73 & 42.41 & 23.40 \\  & codellama-13b & 20.09 & 12.58 & 36.40 & 21.31 & 33.43 & 20.48 & 32.06 & 18.87 \\  & nous-hermes-13b & 46.38 & 31.06 & 35.55 & 13.81 & 33.06 & 13.69 & 37.51 & 17.66 \\  & vicuna-13b-v1.5 & 29.80 & 20.54 & 32.14 & 13.57 & 32.16 & 15.23 & 31.61 & 15.38 \\  & llama-2-13b-chat & 25.71 & 13.11 & 28.99 & 11.14 & 30.04 & 13.60 & 28.34 & 11.85 \\   & gpt-4 & 95.64 & 87.12 & 85.60 & 69.83 & 87.57 & 72.79 & 87.06 & 72.31 \\  & gemini-pro & 62.21 & 50.48 & 72.99 & 55.21 & 76.13 & 58.79 & 71.67 & 54.82 \\  & claude-2 & 53.81 & 24.02 & 75.60 & 58.12 & 72.41 & 52.43 & 71.63 & 51.58 \\  & gpt-3-5-turbo & 44.94 & 11.96 & 70.53 & 47.76 & 71.82 & 47.95 & 65.91 & 40.80 \\  & text-davinci-003 & 60.30 & 20.78 & 69.91 & 44.76 & 71.91 & 45.76 & 68.48 & 40.70 \\   & mistral-7b-v0.3 & 30.70 & 14.65 & 61.42 & 41.79 & 62.32 & 42.93 & 55.52 & 36.40 \\  & codellama-13b & 32.01 & 16.10 & 52.30 & 32.51 & 53.08 & 33.79 & 48.19 & 29.13 \\  & vicuna-13b-v1.5 & 52.72 & 35.55 & 39.31 & 21.00 & 40.05 & 21.40 & 41.62 & 23.62 \\  & nous-hermes-13b & 50.11 & 37.80 & 41.98 & 17.89 & 43.99 & 20.04 & 43.60 & 21.69 \\  & llama-2-13b-chat & 28.49 & 17.01 & 30.26 & 9.66 & 31.00 & 11.35 & 29.99 & 11.32 \\   & gpt-4 & 95.83 & 76.21 & 97.23 & 70.67 & 95.95 & 69.65 & 97.02 & 71.14 \\  & claude-2 & 78.12 & 59.43 & 94.72 & 65.30 & 91.83 & 66.39 & 92.71 & 64.72 \\  & gemini-pro & 69.88 & 45.41 & 91.66 & 57.93 & 88.50 & 53.91 & 88.95 & 56.22 \\  & gpt-3-5-turbo & 43.81 & 28.77 & 89.21 & 61.11 & 83.88 & 56.13 & 81.97 & 55.66 \\  & text-davinci-003 & 61.68 & 45.53 & 80.68 & 54.54 & 76.51 & 51.91 & 78.37 & 53.40 \\   & codellama-13b & 86.34 & 71.20 & 84.31 & 61.51 & 80.42 & 60.18 & 84.26 & 62.38 \\  & mistral-7b-v0.3 & 65.86 & 50.67 & 72.03 & 49.71 & 70.52 & 48.35 & 71.21 & 49.73 \\  & vicuna-13b-v1.5 & 83.63 & 67.71 & 61.80 & 44.54 & 57.14 & 41.72 & 64.27 & 47.31 \\  & nous-hermes-13b & 79.69 & 63.29 & 62.64 & 45.32 & 63.26 & 45.74 & 64.47 & 47.22 \\  & llama-2-13b-chat & 10.39 & 7.32 & 38.89 & 25.37 & 36.43 & 23.40 & 35.11 & 22.94 \\   

Table 4: Evaluation for parameter prediction of tools. _t-F1_ evaluate the pair of (task, parameter name), _v-F1_ evaluate the triple of (task, parameter name, parameter value).

### Parameter Prediction

To evaluate LLMs' ability to correctly configure tools for execution, we assess parameter prediction through two comprehensive metrics: (1) **Parameter Name F1 (t-F1)**: Measures the accuracy in identifying required parameters for each tool, evaluating the model's understanding of tool specifications; (2) **Parameter Name & Value F1 (v-F1)**: Evaluates both parameter identification and value assignment accuracy, assessing the model's ability to provide correct and contextually appropriate parameter values.

The results are detailed in Table 4. GPT-4 demonstrates remarkable robustness in capturing the nuances of both parameter names and values, essential for precise task execution. LLMs such as Claude-2 and Gemini-Pro show competitive results in some domains but still fall short of the benchmarks set by GPT-4. In contrast, open-source LLMs, while performing adequately in some categories, generally exhibit lower _v-F1_ scores. This discrepancy highlights a critical area for improvement in task automation capabilities, particularly in the precision of parameter prediction. This insight points to the need for advancements in model training to enhance the effectiveness of LLMs in real-world applications.

### Analysis

Our comprehensive evaluation reveals several key factors that influence task automation performance:

Fundamental Capabilities1) _Reasoning_: The success of LLMs in task automation largely depends on their ability to solve complex problems and reason effectively. For instance, the GPT series exhibits superior reasoning skills in mathematical and coding tasks, indicative of its robust capabilities in task planning and tool usage. 2) _Instruction Following_: Models specifically fine-tuned for instruction following, such as Vicuna-13b and WizardLLM-13b, tend to outperform others like Llama-2-13b. Notably, WizardLLM-13b exhibits a marked improvement over Vicuna-13b, highlighting the impact of sophisticated instruction fine-tuning on performance.

Contributing Factors1) _Code Pre-training_: Models with extensive code pre-training, such as Code-Llama, surpass other LLMs in task automation. Our data shows an average improvement of 4.45% in tool prediction and 12.76% in parameter prediction across various domains, underscoring the necessity of structured text for connecting automation stages. 2) _Alignment_: Models employing human alignment techniques (e.g., the GPT series with RLHF) show enhanced task automation capabilities compared to their open-source counterparts, indicating that RLHF promotes more generalized reasoning skills and mitigates instruction-specific overfitting.

### Consistency with Human Evaluation

To validate TaskEval's effectiveness as a benchmark, we analyze its correlation with human evaluations using two statistical measures: Kendall's \(\) and Spearman's \(\). Our results in Table 9 demonstrate strong correlations (average \(\) = 0.89, \(\) = 0.78), confirming that our automated metrics align well with human judgment of task automation quality.

## 5 Conclusion

In this paper, we introduce TaskBench, a benchmark designed to evaluate the performance of LLMs in task automation. We begin by outlining the three critical stages of task automation for LLMs: task decomposition, tool selection, and tool parameter prediction. The performance in these stages reflects the overall capability of LLMs in task automation, motivating the construction of specialized evaluation datasets. To this end, we present the concept of Tool Graph, which aggregates various tools along with their interconnections. Using our curated datasets, we further introduce TaskEval, which comprises systematic evaluation metrics for task automation. The experimental results reveal the performance of current mainstream LLMs in task automation and analyze the factors influencing their autonomous task execution. The results also validate the effectiveness of TaskBench in assessing LLMs' performance in task automation. Looking forward, we plan to expand our benchmark to include more domains and develop more advanced metrics to further explore the potential of LLMs in task automation and the development of powerful autonomous agents.