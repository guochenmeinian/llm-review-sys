# On Formal Feature Attribution and Its Approximation

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Recent years have witnessed the widespread use of artificial intelligence (AI) algorithms and machine learning (ML) models. Despite their tremendous success, a number of vital problems like ML model brittleness, their fairness, and the lack of interpretability warrant the need for the active developments in explainable artificial intelligence (XAI) and formal ML model verification. The two major lines of work in XAI include _feature selection_ methods, e.g. Anchors, and _feature attribution_ techniques, e.g. LIME and SHAP. Despite their promise, most of the existing feature selection and attribution approaches are susceptible to a range of critical issues, including explanation unsoundness and _out-of-distribution_ sampling. A recent formal approach to XAI (FXAI) although serving as an alternative to the above and free of these issues suffers from a few other limitations. For instance and besides the scalability limitation, the formal approach is unable to tackle the feature attribution problem. Additionally, a formal explanation despite being formally sound is typically quite large, which hampers its applicability in practical settings. Motivated by the above, this paper proposes a way to apply the apparatus of formal XAI to the case of feature attribution based on formal explanation enumeration. Formal feature attribution (FFA) is argued to be advantageous over the existing methods, both formal and non-formal. Given the practical complexity of the problem, the paper then proposes an efficient technique for approximating exact FFA. Finally, it offers experimental evidence of the effectiveness of the proposed approximate FFA in comparison to the existing feature attribution algorithms not only in terms of feature importance and but also in terms of their relative order.

## 1 Introduction

Thanks to the unprecedented fast growth and the tremendous success, Artificial Intelligence (AI) and Machine Learning (ML) have become a universally acclaimed standard in automated decision making causing a major disruption in computing and the use of technology in general [1; 29; 35; 47]. An ever growing range of practical applications of AI and ML, on the one hand, and a number of critical issues observed in modern AI systems (e.g. decision bias  and brittleness ), on the other hand, gave rise to the quickly advancing area of theory and practice of Explainable AI (XAI).

Numerous methods exist to explain decisions made by what is called black-box ML models [46; 48]. Here, _model-agnostic_ approaches based on random sampling prevail , with the most popular being _feature selection_ and _feature attribution_[40; 56] approaches. Despite their promise, model-agnostic approaches are susceptible to a range of critical issues, like unsoundness of explanations [21; 24] and _out-of-distribution sampling_[34; 62], which exacerbates the problem of trust in AI.

An alternative to model-agnostic explainers is represented by the methods building on the success of formal reasoning applied to the logical representations of ML models [42; 61]. Aiming to address the limitations of model-agnostic approaches, formal XAI (FXAI) methods themselves suffer from a few downsides, including the lack of scalability and the requirement to build a complete logicalrepresentation of the ML model. Formal explanations also tend to be larger than their model-agnostic counterparts because they do not reason about (unknown) data distribution . Finally and most importantly, FXAI methods have not been applied so far to answer feature attribution questions.

Motivated by the above, we define a novel formal approach to feature attribution, which builds on the success of existing FXAI methods . By exhaustively enumerating all formal explanations, we can give a crisp definition of _formal feature attribution_ (FFA) as the proportion of explanations in which a given feature occurs. We argue that formal feature attribution is hard for the second level of the polynomial hierarchy. Although it can be challenging to compute exact FFA in practice, we show that existing anytime formal explanation enumeration methods can be applied to efficiently approximate FFA. Our experimental results demonstrate the effectiveness of the proposed approach in practice and its advantage over SHAP and LIME given publicly available tabular and image datasets, as well as on a real application of XAI in the domain of Software Engineering [45; 52].

## 2 Background

This section briefly overviews the status quo in XAI and background knowledge the paper builds on.

### Classification Problems

Classification problems consider a set of classes \(=\{1,2,,k\}\)1, and a set of features \(=\{1,,m\}\). The value of each feature \(i\) is taken from a domain \(_{i}\), which can be categorical or ordinal, i.e. integer, real-valued or Boolean. Therefore, the complete feature space is defined as \(_{i=1}^{m}_{i}\). A concrete point in feature space is represented by \(=(v_{1},,v_{m})\), where each component \(v_{i}_{i}\) is a constant taken by feature \(i\). An _instance_ or _example_ is denoted by a specific point \(\) in feature space and its corresponding class \(c\), i.e. a pair \((,c)\) represents an instance. Additionally, the notation \(=(x_{1},,x_{m})\) denotes an arbitrary point in feature space, where each component \(x_{i}\) is a variable taking values from its corresponding domain \(_{i}\) and representing feature \(i\). A classifier defines a non-constant classification function \(:\).

Many ways exist to learn classifiers \(\) given training data, i.e. a collection of labeled instances \((,c)\), including decision trees  and their ensembles [11; 12], decision lists , neural networks , etc. Hereinafter, this paper considers boosted tree (BT) models trained with the use of XGBoost .

**Example 1**.: _Figure 1 shows a BT model trained for a simplified version of the adult dataset . For a data instance \(=\{=,==\) shown in Example 1.

ship \(=\) Not-in-family_, \(=\) Male, _Hours/w \(\) 40_), _the model predicts \(<\)50k because the sum of the weights in the 3 trees for this instance equals \(-0.4073=(-0.1089-0.2404-0.0580)<0\).

### ML Model Interpretability and Post-Hoc Explanations

Interpretability is generally accepted to be a subjective concept, without a formal definition . One way to measure interpretability is in terms of the succinctness of information provided by an ML model to justify a given prediction. Recent years have witnessed an upsurge in the interest in devising and applying interpretable models in safety-critical applications [48; 58]. An alternative to interpretable models is post-hoc explanation of _black-box_ models, which this paper focuses on.

Numerous methods to compute explanations have been proposed recently [46; 48]. The lion's share of these comprise what is called _model-agnostic_ approaches to explainability [40; 55; 56] of heuristic nature that resort to extensive sampling in the vicinity of an instance being explained in order to "estimate" the behavior of the classifier in this local vicinity of the instance. In this regard, they rely on estimating input data distribution by building on the information about the training data . Depending on the form of explanations model-agnostic approaches offer, they are conventionally classified as _feature selection_ or _feature attribution_ approaches briefly discussed below.

Feature Selection.A feature selection approach identifies subsets of features that are deemed _sufficient_ for a given prediction \(c=()\). As mentioned above, the majority of feature selection approaches are model-agnostic with one prominent example being Anchors . As such, the sufficiency of the selected set of features for a given prediction is determined statistically based on extensive sampling around the instance of interest, by assessing a few measures like _fidelity, precision_, among others. As a result, feature selection explanations given as a set of features \(\) should be interpreted as the conjunction \(_{i}(x_{i}=v_{i})\) deemed responsible for prediction \(c=()\), \(\), \(c\). Due to the statistical nature of these explainers, they are known to suffer from various explanation quality issues [24; 34; 63]. An additional line of work on _formal_ explainability [25; 61] also tackles feature selection while offering guarantees of soundness; these are discussed below.

Feature Attribution.A different view on post-hoc explanations is provided by feature attribution approaches, e.g. LIME  and SHAP . Based on random sampling in the neighborhood of the target instance, these approaches attribute responsibility to all model's features by assigning a numeric value \(w_{i}\) of importance to each feature \(i\). Given these importance values, the features can then be ranked from most important to least important. As a result, a feature attribution explanation is conventionally provided as a linear form \(_{i}w_{i} x_{i}\), which can be also seen as approximating the original black-box explainer \(\) in the _local_ neighborhood of instance \(\). Among other feature attribution approaches, SHAP [5; 6; 40] is often claimed to stand out as it aims at approximating Shapley values, a powerful concept originating from cooperative games in game theory .

Formal Explainability.In this work, we build on formal explainability proposed in earlier work [8; 13; 25; 42; 61]. where explanations are equated with _abductive explanations_ (AXp's). Abductive explanations are _subset-minimal_ sets of features formally proved to suffice to explain an ML prediction given a formal representation of the classifier of interest. Concretely, given an instance \(\) and a prediction \(c=()\), an AXp is a subset-minimal set of features \(\), such that

\[()_{i}(x_{i}=v_{i}) (()=c) \]

Abductive explanations are guaranteed to be subset-minimal sets of features proved to satisfy (1). As other feature selection explanations, they answer _why_ a certain prediction was made. An alternate way to explain a model's behavior is to seek an answer _why not_ another prediction was made, or, in other words, _how_ to change the prediction. Explanations answering _why not_ questions are referred to as _contrastive explanations_ (CXp's) [26; 42; 46]. As in prior work, we define a CXp as a subset-minimal set of features that, if allowed to change their values, are _necessary_ to change the prediction of the model. Formally, a CXp for prediction \(c=()\) is a subset-minimal set of features \(\), such that

\[()_{i}(x_{i}=v_ {i})(() c) \]

Finally, recent work has shown that AXp's and CXp's for a given instance \(\) are related through the _minimal hitting set duality_[26; 54]. The duality implies that each AXp for a prediction \(c=()\)is a _minimal hitting set2_ (MHS) of the set of all CXp's for that prediction, and the other way around: each CXp is an MHS of the set of all AXp's. The explanation enumeration algorithm  applied in this paper heavily relies on this duality relation and is inspired by the MARCO algorithm originating from the area of over-constrained systems . A growing body of recent work on formal explanations is represented (but not limited) by .

**Example 2**.: _In the context of Example 1, feature attribution computed by LIME and SHAP as well as all 2 AXp's are shown in Figure 2. AXp \(_{1}\) indicates that specifying Education \(=\) Bachelors and Hours/\(\) 40 guarantees that any compatible instance is classified as \(<\) 50k independent of the values of other features, e.g. Status and Relationship, since the maximal sum of weights is \(0.0770-0.0200-0.0580=-0.0010<0\) as long as the feature values above are used. Observe that another AXp \(_{2}\) for \(\) is [Education, Status]. Since both of the two AXp's for \(\) consist of two features, it is difficult to judge which one is better without a formal feature importance assessment._

## 3 Why Formal Feature Attribution?

On the one hand, abductive explanations serve as a viable alternative to non-formal feature selection approaches because they (i) guarantee subset-minimality of the selected sets of features and (ii) are computed via formal reasoning over the behavior of the corresponding ML model. Having said that, they suffer from a few issues. First, observe that deciding the validity of (1) requires a formal reasoner to take into account the complete feature space \(\), assuming that the features are independent and uniformly distributed . In other words, the reasoner has to check all the combinations of feature values, including those that _never appear in practice_. This makes AXp's being unnecessarily _conservative_ (long), i.e. they may be hard for a human decision maker to interpret. Second, AXp's are not aimed at providing feature attribution. The abundance of various AXp's for a single data instance , e.g. see Example 2, exacerbates this issue as it becomes unclear for a user which of the AXp's to use to make an informed decision in a particular situation.

On the other hand, non-formal feature attribution in general is known to be susceptible to out-of-distribution sampling  while SHAP is shown to fail to effectively approximate Shapley values . Moreover and quite surprisingly,  argued that even the use of exact Shapley values is inadequate as a measure of feature importance. Our results below confirm that both LIME and SHAP often fail to grasp the real feature attribution in a number of practical scenarios.

To address the above limitations, we propose the concept of _formal feature attribution_ (FFA) as defined next. Let us denote the set of all formal abductive explanations for a prediction \(c=()\) by \(_{}(,c)\). Then formal feature attribution of a feature \(i\) can be defined as the proportion of abductive explanations where it occurs. More formally,

**Definition 1**:: **(FFA).** The _formal feature attribution_\(_{}(i,(,c))\) of a feature \(i\) to an instance \((,c)\) for machine learning model \(\) is

\[_{}(i,(,c))= _{}(,c),i)|}}{|_{}( ,c)|} \]

Formal feature attribution has some nice properties. First, it has a strict and formal definition, i.e. we can, assuming we are able to compute the complete set of AXp's for an instance, exactly define it for all features \(i\). Second, it is fairly easy to explain to a user of the classification system, even if they are non-expert. Namely, it is the percentage of (formal abductive) explanations that make use of a particular feature \(i\). Third, as we shall see later, even though we may not be able to compute all AXp's exhaustively, we can still get good approximations fast.

**Example 3**.: _Recall Example 2. As there are 2 AXp's for instance \(\), the prediction can be attributed to the 3 features with non-zero FFA shown in Figure 1(d). Also, observe how both LIME and SHAP (see Figure 1(a) and Figure 1(b)) assign non-zero attribution to the feature Relationship, which is in fact irrelevant for the prediction, but overlook the highest importance of feature Education._

One criticism of the above definition is that it does not take into account the length of explanations where the feature arises. Arguably if a feature arises in many AXp's of size 2, it should be considered more important than a feature which arises in the same number of AXp's but where each is of size 10. An alternate definition, which tries to take this into account, is the weighted formal feature attribution (WFFA), i.e. the _average_ proportion of AXp's that include feature \(i\). Formally,

**Definition 2**:: **(WFFA)**. The _weighted formal feature attribution_\(_{}(i,(,c))\) of a feature \(i\) to an instance \((,c)\) for machine learning model \(\) is

\[_{}(i,(,c))=_{ }(,c),i}||^{-1}}{|_{} (,c)|} \]

Note that these attribution values are not on the same scale although they are convertible:

\[_{i}_{}(i,(,c))=_{}(,c)}||}{|_{ }(,c)|}_{i}_{}(i,( ,c)).\]

FFA can be related to the problem of _feature relevancy_, where a feature is said to be _relevant_ if it belongs to at least one AXp. Indeed, feature \(i\) is relevant for prediction \(c=()\) if and only if \(_{}(i,(,c))>0\). As a result, the following claim can be made.

**Proposition 1**.: _Given a feature \(i\) and a prediction \(c=()\), deciding whether \(_{}(i,(,c))>\), \((0,1]\), is at least as hard as deciding whether feature \(i\) is relevant for the prediction._

The above result indicates that computing exact FFA values may be expensive in practice. For example and in light of , one can conclude that the decision version of the problem is \(_{2}^{}\)-hard in the case of DNF classifiers.

Similarly and using the relation between FFA and feature relevancy above, we can note that the decision version of the problem is in \(_{2}^{}\) as long as deciding the validity of (1) is in NP, which in general is the case (unless the problem is simpler, e.g. for decision trees ). Namely, the following result is a simple consequence of the membership result for the feature relevance problem .

**Proposition 2**.: _Deciding whether \(_{}(i,(,c))>\), \((0,1]\), is in \(_{2}^{}\) if deciding (1) is in NP._

## 4 Approximating Formal Feature Attribution

As the previous section argues and as our experimental results confirm, it may be challenging in practice to compute exact FFA values due to the general complexity of the problem. Although some ML models admit efficient formal encodings and reasoning procedures, effective principal methods for FFA approximation seem necessary. This section proposes one such method.

Normally, formal explanation enumeration is done by exploiting the MHS duality between AXp's and CXp's and the use of MARCO-like  algorithms aiming at efficient exploration of minimal hitting sets of either AXp's or CXp's . Depending on the target type of formal explanation, MARCO exhaustively enumerates all such explanations one by one, each time extracting a candidate minimal hitting set and checking if it is a desired explanation. If it is then it is recorded and blocked such that this candidate is never repeated again. Otherwise, a dual explanation is extracted from the subset of features complementary to the candidate , gets recorded and blocked so that it is hit by each future candidate. The procedure proceeds until no more hitting sets of the set of dual explanations can be extracted, which signifies that all target explanations are enumerated. Observe that while doing so, MARCO also enumerates all the dual explanations as a kind of "side effect".

One of the properties of MARCO used in our approximation approach is that it is an _anytime_ algorithm, i.e. we can run it for as long as we need to get a sufficient number of explanations. This means we can stop it by using a timeout or upon collecting a certain number of explanations.

The main insight of FFA approximation is as follows. Recall that to compute FFA, we are interested in AXp enumeration. Although intuitively this suggests the use of MARCO targeting AXp's, for the sake of fast and high-quality FFA approximation, we propose to target CXp enumeration with AXp's as dual explanations computed "unintentionally". The reason for this is twofold: (i) we need to get a good FFA approximation as fast as we can and (ii) according to our practical observations, MARCO needs to amass a large number of dual explanations before it can start producing target explanations. This is because the hitting set enumerator is initially "blind" and knows nothing about the featuresit should pay attention to -- it uncovers this information gradually by collecting dual explanations to hit. This way a large number of dual explanations can quickly be enumerated during this initial phase of grasping the search space, essentially "for free". Our experimental results demonstrate the effectiveness of this strategy in terms of monotone convergence of approximate FFA to the exact FFA with the increase of the time limit. A high-level view of the version of MARCO used in our approach targeting CXp enumeration and amassing AXp's as dual explanations is shown in Algorithm 1.

```
1:procedureXpEnum(\(\), \(\), \(c\))
2:\((,)(,)\)\(\)Sets of AXp's and CXp's to collect.
3:while true do
4:\((,)\)\(\)Get a new MHS of \(\) subject to \(\).
5:if\(=\)thenbreak\(\)Stop if none is computed.
6:if\(()._{i}(x_{i}=v_{i}) (() c)\)then\(\)Check CXp condition (2) for \(\).
7:\(\{\}\)\(\)\(\) appears to be a CXp.
8:else\(\)There must be a missing AXp\(\).
9:\((, ,,c)\)\(\)Get AXp\(\) by iteratively checking (1) .
10:\(\{\}\)\(\)Collect new AXp\(\). return\(\), \(\)
```

**Algorithm 1** MARCO-like Anytime Explanation Enumeration

## 5 Experimental Evidence

This section assesses the formal feature attribution for gradient boosted trees (BT)  on multiple widely used images and tabular datasets, and compares FFA with LIME and SHAP. In addition, it also demonstrates the use of FFA in a real-world scenario of Just-in-Time (JIT) defect prediction, which assists teams in prioritizing their limited resources on high-risk commits or pull requests .

Setup and Prototype Implementation.All experiments were performed on an Intel Xeon 8260 CPU running Ubuntu 20.04.2 LTS, with the memory limit of 8 GByte. A prototype of the approach implementing Algorithm 1 and thus producing FFA was developed as a set of Python scripts and builds on . As the FFA and WFFA values turn out to be almost identical (subject to normalization) in our experiments, here we report only FFA. WFFA results can be found in supplementary material.

Datasets and Machine Learning Models.The well-known MNIST dataset [15; 50] of handwritten digits 0-9 is considered, with two concrete binary classification tasks created: 1 vs. 3 and 1 vs. 7. We also consider PneumoniaMNIST , a binary classification dataset to distinguish X-ray images of pneumonia from normal cases. To demonstrate extraction of _exact_ FFA values for the above datasets, we also examine their downscaled versions, i.e. reduced from 28 \(\) 28 \(\) 1 to 10 \(\) 1. We also consider 11 tabular datasets often applied in the area of ML explainability and fairness [3; 16; 17; 19; 49; 59]. All the considered datasets are randomly split into 80% training and and 20% test data. For images, 15 test instances are randomly selected in each test set for explanation while all tabular test instances are explained. For all datasets, gradient boosted trees (BTs) are trained by XGBoost , where each BT consists of 25 trees of depth 3 per class.3 Finally, we show the use of FFA on 2 JIT defect prediction datasets , with 500 instances per dataset chosen for analysis.

### Formal Feature Attribution

In this section, we restrict ourselves to examples where we can compute the _exact_ FFA values for explanations by computing all AXp's. To compare with LIME and SHAP, we take their solutions, replace negative attributions by the positive counterpart (in a sense taking the absolute value) and then normalize the values into \(\). We then compare these approaches with the computed FFA values, which are also in \(\). The _error_ is measured as Manhattan distance, i.e. the sum of absolute differences across all features. We also compare feature rankings according to the competitors (again using absolute values for LIME and SHAP) using Kendall's Tau  and rank-biased overlap (RBO) metrics.4 Kendall's Tau and RBO are measured on a scale \([-1,1]\) and \(\), respectively. A higher value in both metrics indicates better agreement or closeness between a ranking and FFA.

Tabular Data.Figure 3 exemplifies a comparison of FFA, LIME and SHAP on an instance of the Compas dataset . While FFA and LIME agree on the most important feature, "Asian", SHAP gives it very little weight. Neither LIME nor SHAP agree with FFA, though there is clearly some similarity.

Table 1 details the comparison conducted on 11 tabular datasets, including _adult_, _compas_, and _recidivism_ datasets commonly used in XAI. For each dataset, we calculate the metric for each individual instance and then average the outcomes to obtain the final result for that dataset. As can be observed, the errors of LIME's feature attribution across these datasets span from 1.39 to 5.13. SHAP demonstrates similar errors within a range \([1.40,4.76]\). LIME and SHAP also exhibit comparable performance in relation to the two ranking comparison metrics. The values of Kendall's Tau for LIME (resp. SHAP) are between \(-0.36\) and \(0.22\) (resp. \(-0.39\) and \(0.27\)). Regarding the RBO values, LIME exhibits values between 0.39 and 0.68, whereas SHAP demonstrates values ranging from 0.44 to 0.67. Overall, as Table 1 indicates, both LIME and SHAP fail to get close enough to FFA.

10 \(\) 10 Digits.We now compare the results on 10 \(\) 10 downscaled MNIST digits and PneumoniaMNIST images, where it is feasible to compute all AXp's. Table 2 compares LIME's, SHAP's feature attribution and approximate FFA. Here, we run AXp enumeration for a number of seconds, which is denoted as FFA\({}_{*}\), \(*^{+}\). The runtime required for each image by LIME and SHAP is less than one second. The results show that the errors of our approximation are small, even after 10 seconds it beats both LIME and SHAP, and decreases as we generate more AXp's. The results for the orderings show again that after 10 seconds, FFA\({}_{*}\) ordering gets closer to the exact FFA than both LIME and SHAP. Observe how LIME is particularly far away from the _exact_ FFA ordering.

Summary._These results make us confident that we can get useful approximations to the exact FFA without exhaustively computing all AXp's while feature attribution determined by LIME and SHAP is quite erroneous and fails to provide a human-decision maker with useful insights, despite being fast.

  
**Dataset** & **adult** & **appendicitis australian** & **cars** & **compas** & **heart-statilog** & **hungarian** & **lending** & **liver-disorder** & **pima** & **redictivism** \\ (\(||\)) & (12) & (7) & (14) & (8) & (11) & (13) & (13) & (9) & (6) & (8) & (15) \\ 
**Approach** & & & & & & & & & & & & \\    & & &

[MISSING_PAGE_FAIL:8]

defect prediction has often been considered a black-box, lacking explainability for practitioners. To tackle this challenge, our proposed approach to generating FFA can be employed, as model-agnostic approaches cannot guarantee to provide accurate feature attribution (see above). We use logistic regression models of  based on large-scale open-source Openstack and Qt datasets provided by  commonly used for JIT defect prediction . Monotonicity of logistic regression enables us to enumerate explanations using the approach of  and so to extract _exact FFA_ for each instance _within a second_. Table 4 details the comparison of FFA, LIME and SHAP in terms of the three considered metrics. As with the outcomes presented in Table 1, Table 2, and Table 3, neither LIME nor SHAP align with formal feature attribution, though there are some similarities between them.

## 6 Limitations

Despite the rigorous guarantees provided by formal feature attribution and high-quality of the result explanations, the following limitations can be identified. First, our approach relies on formal reasoning and thus requires an ML model of interest to admit a representation in some fragments of first-order logic, and the corresponding reasoner to deal with it . Second, the problem complexity impedes immediate and widespread use of FFA and signifies the need to develop effective methods of FFA approximation. Finally, though our experimental evidence suggests that FFA approximations quickly converge to the exact values of FFA, whether or not this holds in general remains an open question.

## 7 Conclusions

Most approaches to XAI are heuristic methods that are susceptible to unsoundness and out-of-distribution sampling. Formal approaches to XAI have so far concentrated on the problem of feature selection, detecting which features are important for justifying a classification decision, and not on feature attribution, where we can understand the weight of a feature in making such a decision. In this paper we define the first formal approach to feature attribution (FFA) we are aware of, using the proportion of abductive explanations in which a feature occurs to weight its importance. We show that we can compute FFA exactly for many classification problems, and when we cannot we can compute effective approximations. Existing heuristic approaches to feature attribution do not agree with FFA. Sometimes they markedly differ, for example, assigning no weight to a feature that appears in (a large number of) explanations, or assigning (large) non-zero weight to a feature that is irrelevant for the prediction. Overall, the paper argues that if we agree that FFA is a correct measure of feature attribution then we need to investigate methods that compute good FFA approximations quickly.

    & |=13)\)**} & |=16)\)**} \\   & **Error** & **Kendall’s Tau** & **RBO** & **Error** & **Kendall’s Tau** & **RBO** \\  LIME & 4.84 & 0.05 & 0.55 & 5.63 & -0.08 & 0.45 \\ SHAP & 5.08 & 0.00 & 0.53 & 5.22 & -0.13 & 0.44 \\   

Table 4: Just-in-Time Defect Prediction comparison of FFA versus LIME and SHAP.

Figure 5: 28 \(\) 28 MNIST 1 vs. 7. The prediction is digit 7.

Figure 6: 28 \(\) 28 PneumoniaMNIST. The prediction is normal.