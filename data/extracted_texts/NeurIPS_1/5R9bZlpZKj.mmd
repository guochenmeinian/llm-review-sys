# Smoothed Analysis of Sequential Probability Assignment

Alankrita Bhatt

California Institute of Technology

abhatt@caltech.edu

&Nika Haghtalab

University of California, Berkeley

nika@berkeley.edu

&Abhishek Shetty

University of California, Berkeley

shetty@berkeley.edu

###### Abstract

We initiate the study of smoothed analysis for the sequential probability assignment problem with contexts. We study information-theoretically optimal minmax rates as well as a framework for algorithmic reduction involving the _maximum likelihood estimator_ oracle. Our approach establishes a general-purpose reduction from minimax rates for sequential probability assignment for smoothed adversaries to minimax rates for transductive learning. This leads to optimal (logarithmic) fast rates for parametric classes and classes with finite VC dimension. On the algorithmic front, we develop an algorithm that efficiently taps into the MLE oracle, for general classes of functions. We show that under general conditions this algorithmic approach yields sublinear regret.

## 1 Introduction

Sequential probability assignment -- also known as online learning under the logarithmic loss -- is a fundamental problem with far-reaching impact on information theory, statistics, finance, optimization, and sequential decision making . In recent years, methods for incorporating contexts or side information into sequential probability assignment have gained much attention , in part due to their newly forged connection to sequential decision making applications, the contextual bandit problem, and learning in Markov Decision Processes (MDPs) (see e.g.  and ). In this setting, a forecaster who has access to historical data \(x_{1:t-1},y_{1:t-1}\) consisting of contexts \(x_{}\) (e.g., day \(\)'s meteorological information) and the outcomes \(y_{}\{0,1\}\) (e.g., whether \(\) was a rainy day) wishes to predict \(y_{t}\) given a new context \(x_{t}\). The forecaster uses a _probability assignment_\(p_{t}\) to estimate the probability of \(y_{t}=1\) outcome and incurs the logarithmic loss, i.e., \(- p_{t}(y_{t})\), which rewards the forecasters for having assigned high probability to the realized outcome. The goal of the forecaster is to suffer low _regret_ against a chosen reference class of predictors.

A large body of prior work on sequential probability assignment with contexts has focused on settings where contexts are presented i.i.d. from an unknown distribution (see  and the references within); this problem is also referred to as conditional density estimation. In these cases, sequential probability assignment is known to enjoy small regret for several reference classes such as Vapnik-Chervonenkis (VC) classes. On the other hand, attempts to consider context distributions that evolve unpredictably andadversarially have faced strong impossibility results even for simple reference classes of predictors. For example, for the reference class of simple one-dimensional thresholds assigning \(p_{t}=_{0}\{x_{t} a\}+_{1}\{x_{t}>a\}\) for \(a\), regret is bounded by \(O( T)\) in the i.i.d. case (Fogel and Feder, 2017) but is lower bounded by \((T)\) when the sequence of contexts is chosen adversarially (folklore e.g. Littestone (1988)). In the face of the increasing need to adapt to evolving contexts in modern applications, these impossibility results indicate that new models of adversarial behavior must be considered for obtaining rigorous guarantees that guide the design of sequential probability assignment in practical applications.

In recent years, _smoothed analysis_ of adaptive adversaries (Haghtalab et al., 2021; Rakhlin et al., 2011) has emerged as a framework for going beyond the worst-case adversaries while making minimal assumptions about the adaptive process that generates a sequence. In this setting, contexts are chosen from an evolving sequence of so-called \(\)-smooth distributions, whose density is bounded above by \(1/\) times that of a base measure (such as the uniform distribution). Remarkably, these methods, established by Haghtalab et al. (2021) for \(0\)-\(1\) loss and extended to regression by Haghtalab et al. (2022); Block et al. (2022), have established performance guarantees for the sequential prediction problem that matches the optimal performance in the i.i.d setting. This raises the question as to whether the _sequential probability assignment_ problem may similarly enjoy improved minmax regret bounds for smoothed adaptive sequences.

Beyond minmax rates, an important feature of probability assignment and, its analogue, density estimation is the availability of fundamental and natural estimation techniques such as _maximum likelihood estimation (MLE)_. For i.i.d. sequences, under general conditions, MLE is known to be optimal asymptotically and often serves as a starting point for designing more sophisticated estimators. Going beyond i.i.d. sequences, we ask whether MLE can be made to achieve good statistical behavior on adaptive sequences. More generally, algorithmic perspective is increasingly important for the sequential probability assignment problem and its applications to contextual bandits and reinforcement learning (where algorithm design is as fundamental a consideration as minmax rates (Agarwal et al., 2014; Simchi-Levi and Xu, 2022; Foster and Rakhlin, 2020; Langford et al., 2007; Foster et al., 2021)). In this space, _oracle-efficient_ sequential decision making algorithms that repurpose existing offline algorithmic routines have received special interest (Kalai and Vempala, 2005; Dudik et al., 2020; Wang et al., 2022; Kakade et al., 2007; Simchi-Levi and Xu, 2022). Here again, recent progress on smoothed analysis for sequential prediction with \(0\)-\(1\) loss and regression loss (Haghtalab et al., 2021; Block et al., 2022) has shown promise in bridging the computational and information-theoretical gaps between what is obtainable in the i.i.d. case and for smoothed adaptive sequences.

In this paper, we initiate the study of smoothed analysis for sequential probability assignment and seek to understand fundamental information-theoretic limits on the _minmax regret_ as well as design _natural and oracle-efficient algorithms_ for this problem. Additionally, we investigate whether in the smoothed analysis setting, maximum likelihood estimation can efficiently address sequential probability assignment while achieving small regret. To the best of our knowledge, our work is the first to consider oracle-efficient algorithms (and particularly the MLE) for the sequential probability assignment problem.

### Main results

**Reduction to transductive learning.** Our first main result is a reduction from regret bounds against a smoothed adversary to regret bounds against (a generalized version of) a transductive adversary. That is, we show that the minimax regret in the smoothed analysis setting is upper bounded by the minimax regret in the setting where a set of contexts is provided to the learner and the adversary is constrained to picking the contexts from this set. For \(\), a class of hypotheses mapping contexts to \(\), let us define the minmax regret in the transductive case over \(T\) times steps when a context set of size \(M\) is provided to the learner to be \(}_{T}^{M}()\). We establish in Theorem 3.1 that for all \(\)-smooth sequences the minmax regret \(_{T}(,)\) satisfies, for any \(k>1\),

\[_{T}(,)}_{T}^{kT}( )+T^{2}(1-)^{k}.\]

Furthermore, in Theorem 3.4 we upper bound \(}_{T}^{kT}()\) by connecting the worst case adversarial regret in this setting to the scale-sensitive VC dimension of \(\) which is a prototypical offline complexity of the class. Our results obtain a logarithmic dependence on \(1/\). In particular, in Corollary 3.1.1, we show that for VC classes (and parametric classes) the regret is bounded by \(_{T}(,) O(d())\), where \(d\) is the VC dimension of class \(\).

**Efficient Reduction from Sequential Probability Assignment to MLE.** Our second contribution is initiating the study of oracle-efficient algorithm design for sequential probability assignment. In particular, for small alphabet size, we design a natural algorithm (Algorithm 1) that efficiently uses an MLE oracle and achieves sublinear regret in the smoothed setting. Our Theorem 4.1 gives a general regret bound in terms of the statistical complexity of the class \(\) and the smoothness parameter \(\).

For VC classes, this achieves regret rate of \(T^{4/5}}\). To the best of our knowledge, this is the first _oracle-efficient_ algorithm and analysis of the follow-the-perturbed-leader style algorithms for the logarithmic loss.

**Probability assignment for VC classes.** For VC classes \(\), we explicitly construct sequential probability assignments and establish their regret guarantees in the smoothed setting. That is, we construct a probability assignment based on a Bayesian mixture over \(\) that satisfies \(_{T}(,) Cd()\) where \(d\) is the VC dimension of class \(\). While this approach is not oracle-efficient, it indeed achieves regret bound with optimal dependence on \(T\) and \(\). This motivates a natural direction for future work as to whether such mixture-based methods can be implemented oracle efficiently or if there is a tradeoff between the regret and the computational complexity of sequential probability assignment.

## 2 Preliminaries

Let \(\) be a set of _contexts_ and \(=\{0,1\}\). Then, the problem being studied entails a sequential game where at each timestep \(t\), based on the history of contexts \(x_{1:t}:=(x_{1},,x_{t})\) where \(x_{i}\) and associated bits \(y_{1:t-1}\{0,1\}^{t-1}\), the player must assign a probability \(q(|x_{1:t},y_{1:t-1})\) to what the upcoming bit \(y_{t}\) will be. Once the bit \(y_{t}\) is revealed (possibly in an adversarial fashion) the player incurs loss \(- q(y_{t}|x_{1:t},y_{1:t-1})\) and the game proceeds to the next step. For a _hypothesis class_\(\{\}\), the associated regret for a probability assignment strategy \(=\{q(|x_{1:t},y_{1:t-1})\}_{t=1}^{n}\) for a fixed \(x_{1:T},y_{1:T}\) is

\[_{T}(,x_{1:T},y_{1:T},)=_{t=1}^{T} |x_{1:t},y_{1:t-1})}-_{f}_{t=1}^{T} (y_{t}|x_{t})} \]

where \(p_{f}(1|x_{t})=f(x_{t})\); i.e. the function \(f\) assigns probability \((f(x_{t}))\) to the upcoming bit given the context \(x_{t}\).

Our statistical results apply to a general loss function \(\) and general actions of the learner \(a_{t}\). For a set of inputs \((x_{i},y_{i})}_{i=1}^{T}\) specified by the adversary and a set of actions \(a_{i}}_{i=1}^{T}\) of the learner, regret is defined by

\[_{T}(,x_{1:T},y_{1:T},a_{1:T})=_{t=1}^{T}(a_{t},( x_{t},y_{t}))-_{f}_{t=1}^{T}(f(x_{t}),(x_{t},y_{t})),\]

where for log-loss \(a_{t}=q(|x_{1:t},y_{1:t-1})\); the action is a probability mass function (pmf) over \(\{0,1\}\).

The regret in (1) is often studied under various adversary models; i.e. various different probabilistic assumptions (or lack thereof) on the model generating \(x_{t}\) and \(y_{t}\). In this work, we consider worst-case \(y_{t}\) (in contrast to the _realizable_ setting where \(Y_{t}(f^{*}(x_{t}))\) for a fixed unknown \(f^{*}\)) and \(X_{t}_{t}\) where \(_{t}\)s form an adaptive sequence of smooth distributions.

**Definition 2.1** (Smooth distribution and adversary ).: Consider a fixed and known base distribution \(\) on \(\) (such as the uniform distribution if \(\) supports it). A distribution \(\) on \(\) is said to be \(\)-smooth if for all measurable sets \(A,(A)\). We denote the set of all \(\)-smooth distributions by \(_{}()\). An adversary, characterized by a joint distribution \(\) with \(X_{t}_{t}\) (where \(_{t}\) may possibly depend on the history) is said to be a \(\)-smooth adaptive adversary if \(_{t}_{}()\) for all \(t\{1,,T\}\).

The minmax regret for \(\)-smooth adaptive adversaries is then given by

\[_{T}(,)=_{}_{}_{X_{1:T}}[_{y_{1:T}}_{T} (,X_{1:T},y_{1:T},)],\]

where \(\) is the set of all probability assignment strategies.

We are particularly interested in how geometric properties of the function class \(\) affect \(_{T}(,)\). There are several notions of covering numbers and combinatorial dimensions that quantify the "richness" and complexity of a class, but the _scale-sensitive VC dimension_ will be of particular interest to us and is invoked in our results.

**Definition 2.2** (Scale-sensitive VC dimension).: Let \(\) be a function class. For any \(>0\) and points \(x_{1},,x_{m}\), we say that \(\) shatters the set \(x_{1},,x_{m}\) at scale \(\) if there exist \(s_{1} s_{m}\) such that for each \(\{-1,1\}^{n}\) there exists a function \(f\) such that \(_{i}(f(x_{i})-s_{i})\). The scale sensitive VC dimension at scale \(\) of \(\), denoted by \((,)\) is defined as the largest \(m\) such that there is a set of \(m\) points \(x_{1},,x_{m}\) such that \(\) shatters the set at scale \(\). The (traditional) VC dimension of a binary class \(\) is defined as \(()=_{ 0^{+}} (,)\).

Throughout, we use the following result of Haghtalab et al. (2021) about \(\)-smooth distributions. This results aids us in reduction from smoothed learning to transductive learning.

**Theorem 2.1** (Coupling Lemma of Haghtalab et al. (2021)).: _Let \(_{}\) be an adaptive sequence of \(t\)\(\)-smooth distributions on \(\). There is a coupling \(\) such that_

\[(X_{1},Z_{1,1},,Z_{1,K},,X_{t},Z_{t,1},,Z_{t,K})\]

1. \(X_{1},,X_{t}\) _is distributed according_ \(_{}\)_,_
2. _For every_ \(j t\)_,_ \(\{Z_{i,k}\}_{i j,k[K]}\) _are uniformly and independently distributed on_ \(\)_, conditioned on_ \(X_{1},,X_{j-1}\)_._
3. _For any_ \(t\)_, with probability at least_ \(1-(1-)^{K}\)_,_ \(X_{t}\{Z_{t,k}\}_{k=1:K}\)_._

## 3 General reduction to transductive learning

In this section, we will consider the minimax regret for the smoothed online learning game with respect to the loss function1\(\) against a general class of functions \(\). In Section 3.1, we will show that the minimax regret can be reduced to the minimax regret for a version of transductive learning with respect to the same loss function and class of functions. In Section 3.2, we give general upper bounds for the transductive setting. There is a subtle but important difference between reduction that directly involve regret compared to recent efforts (such as Haghtalab et al. (2021); Block et al. (2022); Haghtalab et al. (2022)) using reductions between proxies of regret, such as covering numbers and sequential complexities. This is particularly important for log loss since its complexity is not captured by covering numbers. We discuss this point further in Section 3.2.2.

### Regret-to-regret Reduction

We work with a general loss function \(\) and general actions of the learner \(a_{t}\). We note that, we can write the minmax value of the smoothed setting in extensive form as

\[_{T}(,)=_{_{1} _{}()}_{X_{1}_{1}}_{a_{1}}_{y _{1}}_{_{2}_{}()}_{X_{2} _{2}}_{a_{2}}_{y_{2}} \] \[_{_{T}_{}()}_{X_ {T}_{T}}_{a_{T}}_{y_{T}}(,X_{1:T },y_{1:T},a_{1:T}).\]

In order to bound this, we consider a generalization of the notion of online learning that is referred to as transductive learning. In this setting, at the start of the interaction the adversary chooses a set of contexts \(X=\{X_{i}\}_{i=1}^{M}\) for some \(M T\) and provides this to the player. The game proceeds as before with the adversary picking \((x_{t},y_{t})\) at time \(t\) and the learner picking an action \(a_{t}\) and suffering a loss \((a_{t},(x_{t},y_{t}))\). However, the adversary is now constrained to pick \(x_{t} X\) at all times \(t\). We can then define the minmax regret indexed by \(X\) as

\[}_{T}(,X):=[_{x_{1} X}_{a_{1}} _{y_{1}}_{x_{T} X}_{a_{T}}_{y_{T}}( ,x_{1:T},y_{1:T},a_{1:T})].\]

Furthermore, define the worst-case transductive learning regret for sets of size \(M\) as \(}_{T}^{M}()=_{X,|X|=M} }_{T}(,X)\). In the following theorem, we show that the regret against \(\)-smoothed adversaries is bounded by the regret in the transductive learning setting when the set of contexts is drawn from the base distribution \(\).

**Theorem 3.1**.: _Let \(\) be any class of functions from \(\) to \(\) and let \((0,1]\). Then, for any \(T\) and \(k\), we have_

\[_{T}(,)}_{T}^{kT}( )+T^{2}(1-)^{k}.\]

Theorem 3.1 shows that we can reduce the problem of evaluating the minimax regret for smoothed adversaries to evaluating the minimax regret for transductive learning. Note that the second term \((1-)^{k} e^{-k}\) and thus, in order to get bounds that are sublinear one needs to consider \(k=c T/\) for an appropriate absolute constant \(c\). As we will see in the next section, this leads to logarithmic dependence on \(^{-1}\). Moreover, by Theorem 3.1 and since \(_{X^{T}}}_{T}(,X)_{T}(,)\), we can see that the transductive learning regret exactly captures the smoothed regret up to \(()\) factors.

### Bounds for Transductive Learning

In this section, we discuss ways to upper bound transductive learning regret \(}_{T}^{M}()\) so as to achieve bounds on \(_{T}(,)\) via Theorem 3.1.

#### 3.2.1 Using Covering Numbers

One of the approaches common in online learning is to characterize the regret in terms of geometric properties (such as covering numbers) of the function class \(\). The notion of covering required varies depending on the loss function and the stochastic properties of the data--typically completely adversarial problems require stronger notions of sequential coverings (Rakhlin et al., 2015, 2015) while for stochastic problems usually weaker _offline coverings_ suffice. In our smoothed case, we show that the offline complexity notion of scale-sensitive VC dimension as defined in Definition 2.2 is adequate. Similar ideas were considered for the case of regression and convex Lipshitz losses in Haghtalab et al. (2022), Block et al. (2022).

Let us first define the notion of approximation according to which a cover will be constructed; we will consider a pointwise approximation. This notion is similar to the notion of global sequential covering in Wu et al. (2022).

**Definition 3.1**.: Let \(\) be a function class. A set of functions \(}\) is said to be a \(\)-covering of \(\) if for any \(f\) there exists \(g}\) such that \(_{x}|f(x)-g(x)|\). We will use \((,)\) to denote the size of the minimal \(\)-covering of \(\).

Note that while the metric in Definition 3.1 is quite stringent, using this cover in the transductive learning case requires us to only consider function classes with _bounded domain size_. We capture this using the following theorem.

**Theorem 3.2** (Upper bound on transductive learning).: _Let \(\) be a function class and \(>0\). Then,_

\[}_{T}^{kT}()_{}\{_{ Z Z|=kT}(|_{Z}, )+2 T\},\]

_where \(|_{Z}\) is the projection of hypothesis class \(\) on the set \(Z\)._The proof of this theorem follows from relating transductive learning to the worst sequential prediction on a finite set of points using formalism presented in Wu et al. (2022). The proof of this theorem is deferred to the Appendix C.

Next, we recall that the covering number \((,)\) is bounded as a function of the scale sensitive VC dimension of the class \(\) and the number of points in the domain.

**Theorem 3.3** (Rudelson and Vershynin (2006)).: _There exist universal constants \(c,C\) such that for all \(>0\), any function class \(\) defined on a finite set \(\), and \(>0\), we have_

\[(,) C(,c )^{1+}(|}{( ,c)}).\]

Finally, putting together Theorem 3.1, Theorem 3.2 and Theorem 3.3 we get the following.

**Theorem 3.4** (Minimax smoothed regret and scale-sensitive VC dimension).: \[_{T}(,)_{k,,>0}\{C (,c)^{1+}((,c)})+2 T+T^{2}(1- )^{k}\}.\]

We can instantiate the bound in Theorem 3.4 in terms of \(T\) and \(\) for two particularly interesting cases: when \((,)\) scales as \(d(1/)\) (often referred to as parametric classes) and when \((,)\) scales as \(^{-p}\) (often referred to as nonparametric classes). A canonical example of the former are _VC classes_; for a class with VC dimension \(d\), \((^{},)=Cd()\) (see for example (Vershynin, 2018, Theorem 8.3.18)). A canonical example of the latter are functions of bounded variation, \(^{}\) which have \((^{},)=\) (see for example (Musayeva, 2020; Bartlett et al., 1997)). This class is known to have unbounded sequential covering numbers (Rakhlin et al., 2010) and therefore is not learnable with a worst-case adversary--this can be seen as a simple consequence of the fact that \(^{}\) contains all one-dimensional thresholds.

**Corollary 3.4.1** (Rates for parametric and nonparametric classes).: _If \((,)=d(1/)\), then for a large enough \(T\)_

\[_{T}(,) O(d( )).\]

_If \((,)=^{-p}\), then_

\[_{T}(,) O(T^{}()).\]

In particular, note that Corollary 3.4.1 shows that for VC classes \(_{T}(^{},)=(d ())\) (tight, see also concurrent work (Wu et al., 2023) for a similar bound) and for functions of bounded variation \(_{T}(^{},)=(())\); note that the minmax rates for the worst-case adversary scale as \((T)\) for both these cases. We note that the above bound may be loose for general nonparametric classes, but should be improvable using a multiscale (chaining) version of Theorem 3.4 but we do not focus on this here.

Though the above results give satisfactory bounds in the minimax sense for many classes \(\) of interest, it is useful to consider explicit constructions of probability assignment rules. For the case of finite VC dimension, we give an explicit probability assignment rule by considering a discretization of the class and using a mixture probability assignment rule. In particular, this strategy (denoted by \(^{}\)) yields optimal regret \(_{T}(^{},,^{})  O(d())\). For the formal statements, proofs and detailed discussion see Appendix D.

#### 3.2.2 Examples without Covering numbers

This reduction approach to characterizing minmax regret in the log-loss is interesting since all previous approaches have used covering numbers of some kind--either sequential covering numbersor stronger notions of global covering. However, in stark contrast to the 0/1 loss and several other loss functions (Rakhlin et al., 2015b), covering numbers cannot capture the minmax regret for the log-loss, at least in the adversarial case. Consider the following class of functions on context set

\[\,=\,_{2}_{2}$ denotes the unit $_{2}$ Euclidean ball) }\,^{}\,:=\,\{x|w _{2}\}.\]

For this class, Rakhlin and Sridharan (2015) construct a follow-the-regularized leader (FTRL) based algorithm achieving regret \(O()\). However, Bilodeau et al. (2020) show an upper bound on the regret in terms of sequential covering numbers which is not improvable in general--this shows that sequential covering numbers are not adequate to capture the minmax regret rates for the log-loss. Wu et al. (2022) further consolidate this by considering the following class, closely resembling \(^{}\), \(^{}:=\{x| x,w |\,|w_{2}\}.\)(Wu et al., 2022, Example 2, Theorem 6) establish that the minmax regret for \(^{}\) is \((T^{2/3})\), demonstrating the surprising fact that by a simple linear transformation of the hypothesis class (which does not change its covering number) one can obtain minmax rates that differ by a polynomial factor! On the other hand, our reduction-based approach bypasses the need for using any covering based arguments and therefore would lead to tight (at least up to poly \((T/)\)) rates.

We remark that exact characterizations of the minmax regret with log-loss (often referred to as the minmax redundancy in the information theory literature) in the no-context (adversarial) case is most often calculated by studying the so-called _stochastic complexity_ of the class \(\)(Rissanen, 1996). This can be extended to (worst-case) transductive learning with contexts \(x_{1},,x_{T}\); in this case the minmax optimal regret for a fixed horizon is achieved by the normalized maximum likelihood (NML) probability assignment (Shtar'kov, 1987), and can be expressed as \(}_{T}^{T}()=_{x_{1}:x}(_{y_{1 }:r\{0,1\}^{T}}_{f}_{t=1}^{T}p_{f}(y_{t}|x_{t})).\) This expression has been evaluated previously for online logistic regression (Jacquet et al., 2021) and more general hypothesis classes (Wu et al., 2022). It is an intriguing question to understand what properties of \(\) the stochastic complexity depends on, given that the above examples illustrates that covering numbers do not capture it. Our reduction provides a technique to use such fine-grained understanding of the regret to directly lift the bounds to the more general smoothed adversary setting.

## 4 Oracle-Efficient Smoothed Sequential Probability Assignment

In the previous section, we consider a purely statistical perspective on the minimax value of the sequential probability assignment problem for smoothed adversaries. In this section, we will focus on an algorithmic perspective and design an algorithm that is efficiently implemented using calls to an MLE oracle. We will focus on the setting when the base measure is the uniform measure on the input space \(\) and the label space \(=\{0,1\}\). In this setting, we are given access to an oracle \(\) which given a data set \(S=\{x_{i},y_{i}\}_{i=1}^{m}\) outputs a hypothesis that minimizes the loss on \(S\). That is,

\[(S)=*{argmin}_{h} _{i=1}^{m}(h,(x_{i},y_{i})).\]

In the context of the logarithmic loss, this corresponds to maximum likelihood estimation. Most of the analysis holds for a general loss function \(\) (with regret scaling appropriate bounds on the values and derivatives), but for clarity one can think of \(\) as the log-loss. In particular, Algorithm 1 is written for the log-loss.

The main framework we work in is the follow-the-perturbed-leader (FTPL) framework. Here, our algorithm uses the oracle on a data set consisting of the historical samples and a set of hallucinated samples. The hallucinated samples are intended to "stabilize" the predictions of the algorithm. This gives us a probability assignment \(^{}=\{q^{}(|x_{1:t},y_{1:t-1})\}_{t =1}^{T}\).

In order to state the regret bound, we need the following notions. For any class \(\), define the truncated class \(_{}\) as \(_{}=\{f_{}:f_{}(x)=f\}.\) We will also need the notion of Rademacher complexity \((,T)=_{X|X|=T} _{}[_{f}_{x X} _{x}f(x)].\)

**Theorem 4.1** (Main Regret Bound).: _For any hypothesis class \(\) and parameters \(n,\), we have that the regret of Algorithm 1 for \(\)-smoothed adversaries is bounded as_

\[_{T}(,,^{}) n ()+ T+T )}\\ +T_{m n}\{( _{},n/m)+(1/ )}{m}+e^{-n/8}\}.\]

We will instantiate this bound for case when the class \(\) has bounded VC dimension. For such classes, it is known that the Rademacher complexity is bounded. We state this in the following corollary.

**Corollary 4.1.1**.: _Let \(\) be a hypothesis class such that the Rademacher complexity is bounded as \((_{},T)=cT^{-}\), then we have \(_{T}(,,^{}) T^{ {2}{2+}}}()\)._

Note that, in particular, for VC classes we have \(_{T}(^{},)\) scales as \(T^{4/5}\). Improving this to achieve the minimax rate discussed in Corollary 3.4.1 is an interesting open question.

_Remark_.: A slightly improved regret scaling as \(T^{3/4}\) can be achieved by assuming access to an oracle that can optimize a mixed objective function involving the log-loss and signed sum of the functions in class. However, these oracles do not have the natural interpretation in terms of maximum likliehood estimation.

### Analysis

The main challenge in designing algorithms in the follow-the-perturbed-leader framework is designing the distribution of the hallucinated samples so as to balance the tradeoff between the "stability" of the algorithm i.e. how little the algorithm changes its prediction from time step to time step, and the "perturbation" i.e. how much the addition of the hallucinated samples changes the prediction of the algorithm from the outputting the best hypothesis on the historical samples. This is captured by the following lemma.

**Lemma 4.2** (Follow the Perturbed Leader bound).: _Let \(\) be a convex loss function, and let \(\) be a hypothesis class. Let \(_{t}\) denote the distribution of the adversary at time \(t\) and let \(_{t}\) denote the distribution of the hypothesis \(h_{t}\) output by Algorithm 1. Then, we have that the regret of Algorithm 1 (where we use \(_{t}=(_{t},_{t})\) to denote a hallucinated data point)2 is bounded by_

\[_{i=1}^{T}}_{s_{t}_{t}}(_{h_{t}_{t}}[(h_{t},s_{t})]-_{h_{t+1} _{t+1}}[(h_{t+1},s_{t})])+[ _{h_{}}_{i=1}^{N}(h,_{t})-(h^ {*},_{t})]}_{}+ T.\]

_where \(h^{*}=*{argmin}_{h_{}}_{i=1}^{T}(h,s _{t})\)._We provide a proof in Appendix E for completeness. Given this decomposition of the regret, we need to handle both terms carefully. Just to appreciate the tradeoff, note that as we increase the number of hallucinated examples, the perturbation term generally increases, but the stability term generally decreases. First, let us focus on the stability term which is harder to deal with. The main approach we will use is a generalization of the decomposition of the stability term introduced in Haghtalab et al. (2022) even when the losses are unbounded, as is the case with the log-loss. The main idea is to decompose the stability term in terms of the distance between the distribution of the average prediction at the next time step and the distribution of the current time step, as captured by the \(^{2}\) distance and a term that captures how different the predictions of the algorithm are when the sample is resampled from the same distribution. The proof can be found in Appendix F. 3

**Lemma 4.3** (\(^{2}\) + Generalization \(\) Stability).: _Let \(_{t}\) denote learner's distribution over \(\) in at round \(t\), \(_{t}\) be adversary's distribution at time \(t\) (given the history \(s_{1},,s_{t-1}\)), \(s_{t}_{t}\) be the realized adversarial instance at time \(t\), and \(s^{}_{t}\) be an independent copy \(s^{}_{t}_{t}\). Let \(R^{(t+1)}\) refers to the randomness used by the algorithm in round \(t+1\). Then,_

\[}_{s_{t}_{t}}(}_{h_{t}_{t}}[(h_{t},s_{t})]-}_{ h_{t+1}_{t+1}}[(h_{t+1},s_{t})])\] \[}^{2}(}_{s_{t} _{t}}[_{t+1}],_{t})( {})+}_{s_{t},s^{}_{t}_{t};R^ {(t+1)}}[(h_{t+1},s^{}_{t})-(h_{t+1},s_{t})].\]

Given this lemma, we move on to bounding the \(^{2}\) divergence between the distribution of the average prediction at the next time step and the distribution of the current time step. This is done using the Ingster method to bound the divergence of mixtures. We include a proof in Appendix G for completeness.

**Lemma 4.4** (Bound on \(^{2}\)).: \(^{2}(}_{s_{t}_{t}}[_{t+1 }],_{t})\)_._

Next, we move on second term in Lemma 4.3. Note that this term involves the difference between the loss of the hypothesis output at time \(t+1\) evaluated on two independent points \(s_{t}\) and \(s^{}_{t}\) drawn from \(_{t}\). The main idea to bound the term is to use a stronger version of the coupling lemma Theorem 2.1 which allows us to extract subsequences of points sampled according to smooth distributions from iid samples from the base measure. This allows us to relate the required generalization bound to the Rademacher complexity of the class composed with the loss. Using the truncation and the contraction principle, we get the desired bound. The proof can be found in Appendix H.

**Lemma 4.5** (Generalization).: _Let \(h_{t+1}\) denote the hypothesis output by the Algorithm 1 at time \(t+1\). Then, for any \(m n\), we have_

\[}_{s_{t},s^{}_{t}_{t};R^{(t+1)}}[ (h_{t+1},s^{}_{t})-(h_{t+1},s_{t})] (_{},n/m)+(1/)}{m}+e^{-n/8}.\]

The final term that we want to bound is the perturbation term. In order to bound this note that we set our truncation parameter \(\) such that the loss our the prediction made by our algorithm is bounded and consequently the perturbation term is bounded by \(n\), see Appendix I. Theorem 4.1 follows by combining the above results.

## 5 Conclusions and Open Problems

In this paper, we initiated the study of sequential probability assignment with smoothed adversaries. We characterize the minimax regret in terms of the minimax regret for transductive learning and use this to provide tight regret bounds, e.g., for VC classes. Furthermore, we initiate the study of oracle efficiency in this setting and show that sublinear regret can be achieve for general classes.

Our work motivates several directions for future work. An interesting direction is whether the optimal \(O((T))\) regret is achievable for some classes, such as VC classes, using oracle-efficient algorithms. More generally, are there computational barriers to obtaining fast rates in prediction with log-loss in this setting?