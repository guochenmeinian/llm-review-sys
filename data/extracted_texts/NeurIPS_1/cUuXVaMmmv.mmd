# Discovering Hierarchical Achievements in Reinforcement Learning via Contrastive Learning

Seungyong Moon\({}^{1}\), Junyoung Yeom\({}^{1}\), Bumsoo Park\({}^{2}\), Hyun Oh Song\({}^{1}\)

\({}^{1}\)Seoul National University, \({}^{2}\)KRAFTON

{symoon11,yeomjy,hyunoh}@mllab.snu.ac.kr

bumsoo.park96@krafton.com

Corresponding author

###### Abstract

Discovering achievements with a hierarchical structure in procedurally generated environments presents a significant challenge. This requires an agent to possess a broad range of abilities, including generalization and long-term reasoning. Many prior methods have been built upon model-based or hierarchical approaches, with the belief that an explicit module for long-term planning would be advantageous for learning hierarchical dependencies. However, these methods demand an excessive number of environment interactions or large model sizes, limiting their practicality. In this work, we demonstrate that proximal policy optimization (PPO), a simple yet versatile model-free algorithm, outperforms previous methods when optimized with recent implementation practices. Moreover, we find that the PPO agent can predict the next achievement to be unlocked to some extent, albeit with limited confidence. Based on this observation, we introduce a novel contrastive learning method, called _achievement distillation_, which strengthens the agent's ability to predict the next achievement. Our method exhibits a strong capacity for discovering hierarchical achievements and shows state-of-the-art performance on the challenging Crafter environment in a sample-efficient manner while utilizing fewer model parameters.

## 1 Introduction

Deep reinforcement learning (RL) has recently achieved remarkable successes in solving challenging decision-making problems, including video games, board games, and robotic controls . However, these advancements are often restricted to a single deterministic environment with a narrow set of tasks. To successfully deploy RL agents in real-world scenarios, which are constantly changing and open-ended, they should generalize well to new unseen situations and acquire reusable skills for solving increasingly complex tasks via long-term reasoning. Unfortunately, many existing algorithms exhibit limitations in learning these abilities and tend to memorize action sequences rather than truly understand the underlying structures of the environments .

To assess the abilities of agents in generalization and long-term reasoning, we focus on the problem of discovering hierarchical achievements in procedurally generated environments with high-dimensional image observations. In each episode, an agent navigates a previously unseen environment and receives a sparse reward upon accomplishing a novel subtask labeled as an _achievement_. Importantly, each achievement is semantically meaningful and can be reused to complete more complex achievements. Such a setting inherently demands strong generalization and long-term reasoning from the agent.

Previous work on this problem has mainly relied on model-based or hierarchical approaches, which involve explicit modules for long-term planning. Model-based methods employ a latent world model that predicts future states and rewards for learning long-term dependencies . Whilethese methods have shown effectiveness in discovering hierarchical achievements, particularly in procedurally generated environments, they are constructed with large model sizes and often require substantial exploratory data, which limits their practicality. Hierarchical methods aim to reconstruct the dependencies between achievements as a graph and employ a high-level planner on the graph to direct a low-level controller toward the next achievement to be unlocked [43; 10; 49]. However, these methods rely on prior knowledge of achievements (_e.g._, the number of achievements), which is impractical in open-world scenarios where the exact number of achievements cannot be predetermined. Additionally, they necessitate a significant number of offline expert data to reconstruct the graph.

To address these issues, we begin by exploring the potential of proximal policy optimization (PPO), a simple and flexible model-free algorithm, in discovering hierarchical achievements . Surprisingly, PPO outperforms previous model-based and hierarchical methods by adopting recent implementing practices. Furthermore, upon analyzing the latent representations of the PPO agent, we observe that it has a certain degree of predictive ability regarding the next achievement, albeit with high uncertainty.

Based on this observation, we propose a novel self-supervised learning method alongside RL training, named _achievement distillation_. Our method periodically distills relevant information on achievements from episodes collected during policy updates to the encoder via contrastive learning . Specifically, we maximize the similarity in the latent space between state-action pairs and the corresponding next achievements within a single episode. Additionally, by leveraging the uniform achievement structure across all environments, we maximize the similarity in the latent space between achievements from two different episodes, matching them using optimal transport . This learning can be seamlessly integrated into PPO by introducing an auxiliary training phase. Our method demonstrates state-of-the-art performance in discovering hierarchical achievements on the challenging Crafter benchmark, unlocking all 22 achievements with a budget of 1M environment steps while utilizing only 4% of the model parameters compared to the previous state-of-the-art method .

## 2 Preliminaries

### Markov decision processes with hierarchical achievements

We formalize the problem using Markov decision processes (MDPs) with hierarchical achievements . Let \(\) represent a collection of such MDPs. Each environment \(_{i}\) is defined by a tuple \((_{i},,,p,r,_{i},)\). Here, \(_{i}\) is the image observation space, which has visual variations across different environments, \(\) is the action space, \(\) is the achievement graph with a hierarchical structure, \(p:()\) is the transition probability function, \(r:\) is the achievement reward function, \(_{i}(_{i})\) is the initial state distribution, and \(\) is the discount factor.

The achievement graph \(=(,)\) is a directed acyclic graph, where each vertex \(v\) represents an achievement and each edge \((u,v)\) indicates that achievement \(v\) has a dependency on achievement \(u\). To unlock achievement \(v\), all of its ancestors (_i.e._, achievements in the path from the root to \(v\)) must also be unlocked. When an agent unlocks a new achievement, it receives an achievement reward of 1. Note that each achievement can be accomplished multiple times within a single episode, but the agent will only receive a reward when unlocking it for the first time. Specifically, let \(b\{0,1\}^{||}\) be a binary vector indicating which achievements have been unlocked and \(c:\{\}\) be a function determining whether a transition tuple results in the completion of an achievement. Then, the achievement reward function is defined as

\[r(s_{t},a_{t},s_{t+1})=1\ \  v_{i}:b[i]=0,c(s_{t},a_{t},s_{t+1})=v_{i}\\ 0\ .\]

This reward structure provides an incentive for the agent to explore the environments and discover a new achievement, rather than repeatedly accomplishing the same achievements.

We assume that the agent has no prior knowledge of the achievement graph, including the number of achievements and their dependencies. Additionally, the agent has no direct access to information about which achievements have been unlocked. Instead, the agent must infer this information indirectly from the reward signal it receives. Given this situation, our objective is to learn a generalizable policy \(:()\) that maximizes the expected return (_i.e._, unlocks as many achievements as possible) across all environments of \(\).

### Crafter environment

We primarily utilize the Crafter environment as a benchmark to assess the capabilities of an agent in solving MDPs with hierarchical achievements . Crafter is an open-world survival game with 2D visual inputs, drawing inspiration from the popular 3D game Minecraft . This is optimized for research purposes, with fast and straightforward environment interactions and clear evaluation metrics. The game consists of procedurally generated environments with varying world map layouts, terrain types, resource placements, and enemy spawn locations, each of which is uniquely determined by an integer seed. An agent can only observe its immediate surroundings as depicted in Figure 0(a), which makes Crafter partially observable and thus challenging. To survive, the agent must acquire a variety of skills, including exploring the world map, gathering resources, building tools, placing objects, and defending against enemies. The game features a set of 22 hierarchical achievements that the agent can unlock by completing specific prerequisites, as illustrated in Figure 0(b). For instance, to make a wood pickaxe, the agent needs to collect wood, place a table, and stand nearby. This achievement structure is designed to require the agent to learn and utilize a wide range of skills to accomplish increasingly challenging achievements, such as crafting iron tools and collecting diamonds.

### Proximal policy optimization

PPO is one of the most successful model-free policy gradient algorithms due to its simplicity and effectiveness . PPO learns a policy \(_{}:()\) and a value function \(V_{}:\), which are parameterized by neural networks. During training, PPO first collects a new episodes \(\) using the policy \(_{_{}}\) immediately prior to the update step. Subsequently, PPO updates the policy network using these episodes for several epochs to maximize the clipped surrogate policy objectives given by

\[J_{}()=_{(s_{t},a_{t})}[( (a_{t} s_{t})}{_{_{}}(a_{t} s_ {t})}_{t},((a_{t} s_{t})}{_ {_{}}(a_{t} s_{t})},1-,1+)_ {t})],\]

where \(_{t}\) is the estimated advantage computed by generalized advantage estimate (GAE) . PPO simultaneously updates the value network to minimize the value objective given by

\[J_{V}()=_{s_{t}}[(V_{ }(s_{t})-_{t}))^{2}],\]

where \(_{t}=_{t}+V_{_{}}(s_{t})\) is the bootstrapped value function target.

In image-based RL, it is common practice to optimize the policy and value networks using a shared network architecture [13; 48]. An image observation is first passed through a convolutional encoder \(_{}:^{h}\) to extract a state representation, which is then fed into linear heads to compute the policy and value function. Sharing state representations between the policy and value networks is crucial to improving the performance of agents in high-dimensional state spaces. However, relying solely on policy and value optimization to train the encoder can lead to suboptimal state representations, particularly in procedurally generated environments . To address this issue, recent studies introduce an auxiliary training phase alongside the policy and value optimization that trains the encoder with auxiliary value or self-supervised objectives [9; 32].

Figure 1: Overview of Crafter. Each environment is procedurally generated and partially observable. The objective is to unlock as many achievements as possible within a single episode.

Motivation

### PPO is a strong baseline for hierarchical achievements

Despite being a simple and ubiquitous algorithm, PPO is less utilized than model-based or hierarchical approaches for solving MDPs with hierarchical achievements. This is due to the fact that PPO does not have an explicit component for long-term planning or reasoning, which is believed to be essential for solving hierarchical tasks. However, a recent study has shown that a PPO-based algorithm is also successful in solving hierarchical achievement on the Minecraft environment, albeit with the aid of pre-training on human video data .

Based on this observation, we first investigate the effectiveness of PPO in solving hierarchical achievements on Crafter without pre-training. We adopt the recent implementation practices proposed in Andrychowicz et al. , Baker et al. . Concretely, we modify the default ResNet architecture in IMPALA as follows :

* **Network size**: We increase the channel size from [16; 32; 32] to [64; 128; 128] and the hidden size from 256 to 1024.
* **Layer normalization**: We add layer normalization before each dense or convolutional layer .
* **Value normalization**: We keep a moving average for the mean and standard deviation of the value function targets and update the value network to predict the normalized targets.

We train the modified PPO on Crafter for 1M environment steps and evaluate the success rates for unlocking achievements (please refer to Section 5.1 for the evaluation). Figure 2 shows that the slight modification in implementing PPO significantly improves the performance, increasing the score from 8.17 to 15.60. Notably, this outperforms the current state-of-the-art DreamerV3, which achieves a score of 14.77.

### Representation analysis of PPO

Since Crafter environments are procedurally generated, simply memorizing successful episodes is insufficient for achieving high performance. We hypothesize that the PPO agent acquires knowledge beyond mere memorization of action sequences, possibly including information about achievements. To validate this, we analyze the learned latent representations of the encoder, as inspired by Wijmans et al. . Specifically, we collect a batch of episodes using an expert policy and subsample a set of states for training. For each state \(s\) in the training set, we freeze its latent representation \(_{}(s)\) from the encoder. Subsequently, we train a linear classifier using this representation as input to predict the very next achievement unlocked in the episode containing \(s\). Finally, we evaluate the classification accuracy and the prediction confidence of the ground-truth labels on a held-out test set. The detailed experimental settings are provided in Appendix A.

Surprisingly, PPO achieves a nontrivial accuracy of 44.9% in the 22-way classification. However, Figure 3 shows that the prediction outputs lack confidence with a median value of 0.240. This suggests that the learned representations of the PPO encoder are not strongly correlated with the next achievement to be unlocked and the agent may struggle to generate optimal action sequences towards a specific goal.

This finding warrants providing additional guidance to the encoder for predicting the next achievements with high confidence. However, since the agent has no access to the achievement labels, it is challenging to guide the agent in a supervised fashion. Therefore, it is necessary to explore alternative approaches to guide the agent toward predicting the next achievements.

Figure 3: Histogram for the confidence of next achievement prediction.

Figure 2: Score curves of PPO.

## 4 Contrastive learning for achievement distillation

In this section, we introduce a new self-supervised learning method that works alongside RL training to guide the encoder in predicting the next achievement to be unlocked. This approach distills relevant information about discovered achievements from episodes collected during multiple policy updates into the encoder via contrastive learning. This method consists of two key components:

* **Intra-trajectory achievement prediction**: Within an episode, this maximizes the similarity in the latent space between a state-action pair and its corresponding next achievement.
* **Cross-trajectory achievement matching**: Between episodes, this maximizes the similarity in the latent space for matched achievements.

For ease of notation, we denote the sequence of unlocked achievements within an episode as \((g_{i})_{i=1}^{m}\) and their corresponding timesteps as \((t_{i})_{i=1}^{m}\), where each achievement \(g_{i}\) is defined by a transition tuple \((s_{t_{i}},a_{t_{i}},s_{t_{i}+1})\). For each timestep \(t\), we represent the very next achievement as \(g_{t}^{+}=g_{u}\), where \(u=\{i t t_{i}\}\), and the very previous achievement as \(g_{t}^{-}=g_{l}\), where \(l=\{i t>t_{i}\}\).

### Intra-trajectory achievement prediction

Given a state-action pair \((s_{t},a_{t})\) and its corresponding next achievement \(g_{t}^{+}\) within an episode \(\), we train the encoder \(_{}\) to produce similar representations for them through contrastive learning [34; 22]. Specifically, we regard \(g_{t}^{+}\) as the anchor and \((s_{t},a_{t})\) as the positive. We also randomly sample another state-action pair \((s_{t^{}},a_{t^{}})\) from the same episode to serve as the negative. Subsequently, we obtain the normalized representations of the anchor, positive, and negative, denoted as \(_{}(g_{t}^{+})\), \(_{}(s_{t},a_{t})\), and \(_{}(s_{t^{}},a_{t^{}})\), respectively. Finally, we minimize the following contrastive loss to maximize the cosine similarity between the anchor and positive representations while minimizing the cosine similarity between the anchor and negative representations:

\[L_{}()=-_{(s_{t},a_{t}) \\ (s_{t^{}},a_{t^{}})}[((s_{t},a_{t})^{}_{}(g_{t}^{+})/)}{( _{}(s_{t},a_{t})^{}_{}(g_{t}^{+})/)+(_{ }(s_{t^{}},a_{t^{}})^{}_{}(g_{t}^{+})/)} )],\]

where \(>0\) is the temperature parameter.

To obtain the state-action representation \(_{}(s_{t},a_{t})\), we calculate the latent representation of the state \(_{}(s_{t})\) from the encoder and concatenate it with the action \(a_{t}\) using a FiLM layer . The resulting vector is then passed through an MLP layer and normalized. To obtain the achievement representation \(_{}(g_{t}^{+})\), we simply calculate the residual of the latent representations of the two consecutive states from the encoder and normalize it, as motivated by Nair et al. .

Figure 4: Illustration of achievement distillation.

While this contrastive objective encourages the encoder to predict the next achievement in the latent space, it can potentially lead to distortions in the policy and value networks due to the changes in the encoder. To address this, we jointly minimize the following regularizers to preserve the outputs of the policy and value networks, following the practice in Moon et al. :

\[R_{}()=_{s_{t}}[D_{}(_{ _{}}( s_{t})_{}( s_{t}) )],\;\;R_{V}()=_{s_{t}}[ (V_{}(s_{t})-V_{_{}}(s_{t}))^{2}],\]

where \(_{_{}}\) and \(V_{_{}}\) are the policy and value networks immediately prior to the contrastive learning, respectively and \(D_{}\) denotes the KL divergence.

### Cross-trajectory achievement matching

Since Crafter environments are procedurally generated, the achievement representations learned solely from intra-trajectory information may include environment-specific features that limit generalization. To obtain better achievement representations, we leverage the common achievement structure shared across all episodes.

We first match the sequences of unlocked achievements from two different episodes in an unsupervised fashion. Given two achievement sequences \(=(g_{i})_{i=1}^{m}\) and \(^{}=(g^{}_{j})_{j=1}^{n}\), we define the cost matrix \(M^{m n}\) as the cosine distance between the achievement representations:

\[M_{ij}=1-_{}(g_{i})^{}_{}(g^{}_{j}).\]

Subsequently, we regard these two sequences as discrete uniform distributions and compute a soft-matching \(T^{m n}\) between them using partial optimal transport, which can be solved by

\[T=*{arg\,min}_{T 0} T,M+ _{i=1}^{m}_{j=1}^{n}T_{ij} T_{ij}\] \[\;\;T,\;T^{} ,\;^{}T^{}=\{m,n\},\]

where \(>0\) is the entropic regularization parameter . Here, we set the total amount of probability mass to be transported to the minimum length of the two sequences for simplicity. However, some unlocked achievements in one sequence may not exist in the other sequence, and therefore should not be transported. In this case, the optimal amount of probability mass to be transported should be less than the minimum length. To address this, we compute the conservative hard matching \(T^{*}\) from \(T\) by thresholding the probabilities less than 0.5 (_i.e._, \(T^{*}=[T>0.5]\)). Note that this also encourages each achievement to be matched to at most one other achievement. We provide examples of matching results in Appendix B.

We train the encoder to produce similar representations for the matched achievements according to \(T^{*}\) through contrastive learning. Specifically, suppose that the \(i\)th achievement of the source sequence \(g_{i}\) is matched with the \(k\)th achievement of the target sequence \(g^{}_{k}\). Then, we consider \(g_{i}\) as the anchor and \(g^{}_{k}\) as the positive. We also randomly sample another achievement \(g^{}_{j}\) from the target sequence to serve as the negative. Subsequently, we obtain the normalized representations of these achievements. Finally, we minimize the following contrastive loss to maximize the cosine similarity between the anchor and positive representations while minimizing the cosine similarity between the anchor and negative representations:

\[L_{}()=-_{g_{i},g^{}_{j} ^{}}[((g_{i})^{}_{ }(g^{}_{k})/)}{(_{}(g_{i})^{}_{} (g^{}_{k})/)+(_{}(g_{i})^{}_{}(g^{ }_{j})/)})],\]

As in Section 4.1, we jointly minimize the policy and value regularizers to prevent distortions.

### Achievement representation as memory

We further utilize the achievement representations learned in Sections 4.1 and 4.2 as memory for the policy and value networks. Specifically, given a state \(s_{t}\) and its corresponding previous achievement \(g^{-}_{t}\), we concatenate the latent state representation \(_{}(s_{t})\) with the previous achievement representation \(_{}(g^{-}_{t})\). The resulting vector is then fed into the policy and value heads to output an action distribution and value estimate, respectively.

We also utilize the previous achievement for the achievement prediction task in Section 4.1. Given a state-action pair \((s_{t},a_{t})\) and its corresponding previous achievement \(g_{t}^{-}\), we concatenate the latent state representation \(_{}(s_{t})\) from the encoder with \(a_{t}\) and \(_{}(g_{t}^{-})\). The resulting vector is then fed into an MLP layer and normalized to obtain the representation \(_{}(s_{t},a_{t},g_{t}^{-})\) for the next achievement prediction. This is interpreted as learning forward dynamics in the achievement space.

### Integration with RL training

We integrate the contrastive learning method proposed in Sections 4.1 to 4.3 with PPO training by introducing two alternating phases, the policy and auxiliary phases. During the policy phase, which is repeated several times, we update the policy and value networks using newly-collected episodes and store them in a buffer. During the auxiliary phase, we update the encoder to optimize the contrastive objectives in conjunction with the policy and value regularizers using all episodes in the buffer. We call this auxiliary learning _achievement distillation_. The illustration and pseudocode are presented in Figure 4 and Algorithm 1, respectively.

```
1:Policy network \(_{}\), value network \(V_{}\)
2:for phase = \(1,2,\)do
3: Reset the buffer \(\)
4:for iter = \(1,2,,N_{}\)do\(\) PPO training
5: Collect episodes \(\) using \(_{}\) and add them to \(\)
6:for epoch = \(1,2,,E_{}\)do
7: Optimize \(J_{}()\) and \(J_{V}()\) using \(\)
8:endfor
9:endfor
10:\(_{_{}}_{}\), \(V_{_{}} V_{}\)
11:for iter = \(1,2,,E_{}\)do\(\) Achievement distillation
12: Optimize \(L_{}()\), \(R_{}()\), and \(R_{V}()\) using \(\)
13: Optimize \(L_{}()\), \(R_{}()\), and \(R_{V}()\) using \(\)
14:endfor
15:endfor
```

**Algorithm 1** PPO with achievement distillation

## 5 Experiments

### Experimental setup

To assess the effectiveness of our method in discovering hierarchical achievements, we train the agent on Crafter for 1M environment steps and evaluate its performance, following the protocol in Hafner . We measure the success rates for all 22 achievements across all training episodes as a percentage and calculate their geometric mean to obtain our primary evaluation score2. Note that the geometric mean prioritizes unlocking challenging achievements. We also measure the episode reward, which indicates the number of achievements unlocked within a single episode, and report the average across all episodes within the most recent 100K environment steps. We conduct 10 independent runs using different random seeds for each experimental setting and report the mean and standard deviation. The code can be found at [https://github.com/snu-mllab/Achievement-Distillation](https://github.com/snu-mllab/Achievement-Distillation).

We compare our method with our backbone algorithm PPO and four baseline methods that have been previously evaluated on Crafter in other research work: DreamerV3, LSTM-SPCNN, MuZero + SPR, and SEA . DreamerV3 is a model-based algorithm that has achieved state-of-the-art performance on Crafter without any pre-training. LSTM-SPCNN is a model-free algorithm based on PPO that employs a recurrent and object-centric network to improve performance on Crafter. MuZero + SPR is a model-based algorithm that has demonstrated state-of-the-art performance on Crafter by utilizing unsupervised pre-training. SEA is a hierarchical algorithm based on IMPALA that employsa high-level planner to discover achievements on Crafter. We provide the implementation details and hyperparameters in Appendix C.

### Crafter results

Table 1 and Figure 5 present the Crafter scores and rewards obtained by our method and the baselines. Our method outperforms all the baselines trained from scratch in both the metrics by a considerable margin, with a score of 21.79% and a reward of 12.60. Notably, our method exhibits superior score performance compared to MuZero + SPR, which utilizes pre-collected exploratory data and employs a computationally expensive tree search algorithm for planning, while achieving comparable rewards.

Figure 6 shows the individual success rates for all 22 achievements of our method and two successful baselines, DreamerV3 and LSTM-SPCNN. Remarkably, our method outperforms the baselines in unlocking challenging achievements. For instance, our method collects iron with a probability over 3%, which is 20 times higher than DreamerV3. This achievement is extremely challenging due to its scarcity on the map and the need for wood and stone tools. Moreover, our method crafts iron tools with a probability of approximately 0.01%, which is not achievable by either of the baselines. Finally, our method even succeeds in collecting diamonds, the most challenging task, on individual runs.

### Model size analysis

We compare the model sizes between our method and the baselines. As shown in Table 1, our method achieves better performance with fewer parameters. In particular, our method only requires 4% of the parameters used by DreamerV3. Note that while our method has twice as many parameters as PPO, most of this increase is due to the networks used for the auxiliary learning, which are not utilized during inference. Additionally, we test our method with a smaller model by reducing the channel size to [16; 32; 32] and the hidden dimension to 256, resulting in a total of 1M parameters. Notably, it still outperforms the baselines with a score of 17.07%.

   Method & Parameters & Score (\%) & Reward \\   & - & 50.5 \(\) 6.8 & 14.3 \(\) 2.3 \\   & Ours & 9M & **21.79 \(\) 1.37** & **12.60 \(\) 0.31** \\   & PPO & 4M & 15.60 \(\) 1.66 & 10.32 \(\) 0.53 \\  & DreamerV3 & 201M & 14.77 \(\) 1.42 & 10.92 \(\) 0.53 \\  & LSTM-SPCNN & 135M & 11.67 \(\) 0.80 & 9.34 \(\) 0.23 \\  & MuZero + SPR\({}^{}\) & 54M & 4.4 \(\) 0.4 & 8.5 \(\) 0.1 \\  & SEA & 1.5M & 1.22 \(\) 0.13 & 0.63 \(\) 0.08 \\  Pre-training & MuZero + SPR\({}^{}\) & 54M & 16.4 \(\) 1.5 & 12.7 \(\) 0.4 \\   

Table 1: Scores and rewards. MuZero + SPR\({}^{}\) denotes the results replicated from the original paper.

Figure 5: Score and reward curves.

### Representation analysis of achievement distillation

To validate whether our method induces the encoder to have better representations for predicting the next achievements, we conduct an analysis of the latent representations of the encoder using the same approach as described in Section 3. Our method achieves a classification accuracy of 73.6%, which is a 28.7%p increase compared to PPO. Furthermore, Figure 3 demonstrates that our method produces predictions with significantly higher confidence than PPO, with a median value of 0.752.

### Ablation studies

We conduct ablation studies to evaluate the individual contribution of our proposed method, intra-trajectory achievement prediction (I), cross-trajectory achievement matching (C), and memory (M). Table 2 shows that while intra-trajectory achievement prediction is the most significant contributor, cross-trajectory achievement matching and memory also play important roles in improving the performance of our method.

### Extension to value-based algorithms

In our previous experiments, we use the on-policy policy gradient algorithm, PPO, as our backbone RL algorithm. To assess the adaptability of our method to other RL paradigms, we extend our experiments to the popular off-policy value-based algorithm, QR-DQN, introducing a slight modification . Specifically, we employ Huber quantile regression to preserve the Q-network's output distribution in alignment with the value function optimization in QR-DQN. We train the agent on Crafter for 1M environment steps and evaluate its performance. Notably, our method also proves effective for value-based algorithms, elevating the score from 4.14 to 8.07. The detailed experimental settings and results are provided in Appendix D.

### Application to other environments

To evaluate the broad applicability of our method to diverse environments, we conduct experiments on two additional benchmarks featuring hierarchical achievements: Procgen Heist and MiniGrid [8; 7]. Heist is a procedurally generated environment whose goal is to steal a gem hidden behind a sequence of blue, green, and red locks. To open each lock, an agent must collect a key with the corresponding color. Heist introduces another challenge, given that the color of wall and background can vary between environments, whereas Crafter maintains fixed color patterns for its terrains. Additionally, we create a customized a door-key environment using MiniGrid to evaluate the effectiveness of our method on a deeper achievement graph. Notably, our method significantly improves the performance of PPO in Heist, increasing the score from 29.6 to 71.0. Our method also outperforms PPO in the MiniGrid environment by a substantial margin, elevating the score from 3.33 to 8.04. The detailed experimental settings and results can be found in Appendix E.

   I & C & M & Score (\%) \\  ✗ & ✗ & ✗ & 15.60 \(\) 1.66 \\ ✓ & ✗ & ✗ & 19.02 \(\) 1.65 \\ ✓ & ✓ & ✗ & 20.36 \(\) 1.79 \\ ✓ & ✓ & ✓ & **21.79 \(\) 1.37** \\   

Table 2: Ablation studies.

Figure 6: Individual success rates for all achievements.

Related work

Discovering hierarchical achievements in RLOne major approach to this problem is model-based algorithms. DreamerV3 learns a world model that predicts future states and rewards and trains an agent using imagined trajectories generated by the model . While achieving superior performance on Crafter, it requires more than 200M parameters. MuZero + SPR trains a model-based agent with a self-supervised task of predicting future states . However, it relies on pre-training with 150M environment steps collected via RND to improve its performance on Crafter .

Another approach is hierarchical algorithms, while many of these methods are only tested on grid-world environments . HAL introduces a classifier that predicts the next achievement to be done and uses it as a high-level planner . However, it relies on prior information on achievements, such as what achievement has been completed, and does not scale to the high-dimensional Crafter. SEA reconstructs the Crafter achievement graph using 200M offline data collected from a pre-trained IMPALA policy and employs a high-level planner on the graph . However, it requires expert data to fully reconstruct the graph and has not been tested on a sample-efficient regime.

There are only a few studies that have explored model-free algorithms. LSTM-SPCNN uses an LSTM and a size-preserving CNN to improve the performance of PPO on Crafter . However, it requires 135M parameters and the performance gain is modest compared to PPO with a CNN.

Representation learning in RLThere has been a large body of work on representation learning for improving sample efficiency on a single environment , or generalization on procedurally generated environments . However, representation learning for discovering hierarchical achievements has little been explored. A recent study has evaluated the performance of the previously developed representation learning technique SPR on Crafter, but the results are not promising .

While widely used in other domains, optimal transport has recently garnered attention in RL . However, the majority of the studies focus on imitation learning, where optimal transport is employed to match the behavior of an agent with offline expert data. In this work, we utilize optimal transport to obtain generalizable representations for achievements in the online setting.

## 7 Conclusion

In this work, we introduce a novel self-supervised method for discovering hierarchical achievements, named _achievement distillation_. This method distills relevant information about achievements from episodes collected during policy updates into the encoder and can be seamlessly integrated with a popular model-free algorithm PPO. We show that our proposed method is capable of discovering the hierarchical structure of achievements without any explicit component for long-term planning, achieving state-of-the-art performance on the Crafter benchmark using fewer parameters and data.

While we utilize only minimal information about hierarchical achievements (_i.e._, the agent receives a reward when a new achievement is unlocked), one limitation of our work is that we have not evaluated the transferability of our method to an unsupervised agent without any reward. A promising future direction would be developing a representation learning method that can distinguish achievements in a fully unsupervised manner and combining it with curiosity-driven exploration techniques .