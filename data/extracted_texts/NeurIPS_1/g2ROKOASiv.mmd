# Incentivizing Honesty among Competitors in

Collaborative Learning and Optimization

 Florian E. Dorner

MPI for Intelligent Systems, Tubingen

ETH Zurich

florian.dorner@tuebingen.mpg.de &Nikola Konstantinov

INSAIT, Sofia University

nikola.konstantinov@insait.ai &Georgi Pashaliev

Sofia High School of Mathematics &Martin Vechev

ETH Zurich

martin.vechev@inf.ethz.ch

###### Abstract

Collaborative learning techniques have the potential to enable training machine learning models that are superior to models trained on a single entity's data. However, in many cases, potential participants in such collaborative schemes are competitors on a downstream task, such as firms that each aim to attract customers by providing the best recommendations. This can incentivize dishonest updates that damage other participants' models, potentially undermining the benefits of collaboration. In this work, we formulate a game that models such interactions and study two learning tasks within this framework: single-round mean estimation and multi-round SGD on strongly-convex objectives. For a natural class of player actions, we show that rational clients are incentivized to strongly manipulate their updates, preventing learning. We then propose mechanisms that incentivize honest communication and ensure learning quality comparable to full cooperation. Lastly, we empirically demonstrate the effectiveness of our incentive scheme on a standard non-convex federated learning benchmark. Our work shows that explicitly modeling the incentives and actions of dishonest clients, rather than assuming them malicious, can enable strong robustness guarantees for collaborative learning.

## 1 Introduction

Recent years have seen an increased interest in designing methods for collaborative learning, where multiple participants contribute data and train a model jointly. The premise is that the participants will then be able to obtain a better model than if they were learning in isolation. Most prominently, _federated learning (FL)_Kairouz et al. (2021) provides a method for training models in a distributed manner, allowing data to stay with institutions, while still harvesting the benefits of collaboration.

An underlying premise for the success of collaborative learning schemes is that the participants contribute data (or gradient updates) relevant to the learning task at hand. However, when participants are in competition on some downstream task, they may have an incentive to sabotage other participants' models. For instance, firms that are competing on the same market can often improve their machine learning models by having access to their competitors's data, but at the same time will likely benefit from a gap between the quality of their models and those of other firms.

These conflicting incentives raise a concern that collaborative learning may be vulnerable to strategic updates from participants. Previous work has empirically demonstrated that irrelevant or malicious updates can negatively impact collaborative learning Tolpegin et al. (2020); Kairouz et al. (2021). Inparticular, if a subset of participants is modeled as fully-malicious (Byzantine) agents, that collude in a worst-case manner, it is known that optimal convergence rates contain a leading-order term that is based on the fraction of Byzantine agents and is irreducible as the number of players increases (Yin et al., 2018; Alistarh et al., 2018). This suggests that collaborative learning in the presence of strategic behavior may often not provide asymptotic benefits over learning with one's own data.

ContributionsIn this work, we study collaborative learning in the presence of strategic behavior by explicitly modeling players' competitive incentives. We consider a game between multiple players that exchange updates via a central server, where players' rewards increase both when they obtain a good model for themselves, and when other players' models perform poorly.

We study two important instantiations of this collaborative learning game: mean estimation with a very general action space and strongly-convex stochastic optimization with attacks that add gradient noise. We show that players are often incentivized to strongly manipulate their estimates, rendering collaborative learning useless. To remedy this, we suggest mechanisms inspired by peer prediction (Miller et al., 2005), that penalize cheating players using side payments. Our results on stochastic optimization rely on a novel recursive bound for the squared norm of differences in SGD-iterates between a clean trajectory and a strategically corrupted trajectory. Meanwhile, we show that side payments can be avoided in the mean estimation case, using a novel communication protocol in which the server sends noisy estimates back to players that are suspected of cheating. Our mechanisms are solely based on observable player behaviour, and recover near-optimal convergence rates at equilbrium. Furthermore, expected payments cancel out when all players are honest, so that players are incentivized to participate in the training, rather than use their own data only, despite the penalties.

Finally, we conduct experiments on the FeMNIST and Twitter datasets from LEAF (Caldas et al., 2018) and demonstrate that our mechanisms can incentivize honesty for realistic non-convex problems

## 2 Related work

Game theory and collaborative learningMany works that study connections between FL and game theory focus on clients' incentives to participate in the training process at all (see Tu et al. (2021) for a recent survey, and Gradwohl and Tennenholtz (2022) for an analysis of how this relates to competition). Similarly, Karimireddy et al. (2022) study incentives for free-riding, i.e. joining collaborative learning without spending resources to contribute data. In contrast, our setting covers clients that strategically manipulate their updates in order to damage other participants' models.

Another line of work studies FL as a coalitional game theory problem, in which the players need to deal with potential issues of between-client heterogeneity by deciding with whom to collaborate Donahue and Kleinberg (2021, 2021). Optimal behavior under data heterogeneity is also studied by Chayti et al. (2021), while Gupta et al. (2022) studies invariant risk minimization games in which FL participants learn predictors invariant across client distributions. In contrast, we study FL as a non-cooperative game and seek to address strategic manipulation, rather than data heterogeneity.

Robustness in federated learningThe robustness of FL to noisy or corrupted updates from clients has received substantial recent interest, see Shejwalkar et al. (2022) for a recent overview. One line of work studies federated learning in the context of various data corruption models, e.g. noise or bias towards protected groups (Fang and Ye, 2022; Abay et al., 2020). In contrast, we study strategic manipulation by clients as the source of data corruption.

Clients attacking the training are typically modeled as Byzantine (Blanchard et al., 2017; Yin et al., 2018; Alistarh et al., 2018), adversarially seeking to sabotage training by deviating from the FL protocol in a worst-case manner. The goal of Byzantine-robust learning is to achieve guarantees in that setting. Similar models have been studied in a statistical context, where data is stored at a single location and so communication is not a concern (Qiao and Valiant, 2018; Konstantinov et al., 2020). In contrast to these works, we model manipulation as a consequence of competitive incentives rather than maliciousness and analyze client behaviour using game theory (Osborne and Rubinstein, 1994). Rather than focussing on robustness to manipulations, we aim to _prevent manipulation_ alltogether.

Peer prediction mechanismsOur mechanisms for inducing honesty are closely related to _peer prediction_ that aims to incentivize honest ratings on online platforms. In their seminal paper,Miller et al. (2005) suggest paying raters based on how much their rating helps to predict other raters' ratings. While they require a common prior shared by all raters and the mechanism designer, Witkowski and Parkes (2012) extend their results relaxing that assumption. Closely related to our work, Cai et al. (2015) suggest to incentivize crowdworkers to produce accurate labels by paying them more, the better their label gets predicted by a model estimate from other workers' data. Meanwhile, Waggoner and Chen (2014) prove that peer prediction elicits common knowledge, rather than truth from participants. Lastly, while Karger et al. (2021) find that peer prediction can elicit subjective forecasts of similar accuracy as scoring based on the ground truth, Gao et al. (2014) demonstrate that human raters can end up with dishonest strategies despite the existence of honest equilibria.

## 3 Competitive federated learning

In this section, we present our framework on a high level and explain how it models competitive behaviour in FL. In this generality, however, it is impossible to analyze the problem quantitatively. We therefore define specific instantiations of the framework, which we study for the rest of the paper.

### General framework

OverviewThroughout the paper, we assume that there are \(N 2\) players, who each have a private dataset. The players participate in an FL-like procedure, which takes place over multiple rounds and requires them to send messages with information relevant to update a centrally trained model at every step. In our setup, players act strategically and competitive pressures might incentivize them to try to corrupt other players' models. This is done by manipulating updates sent to the central model, while simultaneously keeping track of an unmanipulated private (presumably more accurate) model.

To model the participants' strategic interactions, we need to define a game by specifying their action spaces and rewards. To this end, one needs to specify: their _attack strategies_, that describe whether and how they will corrupt their messages to the server; their _defense strategies_, which describes how they postprocess the server's updates to defend themselves against others' manipulations; and their _rewards_, in a way that reflects the quality of learning and the competition between them. Given these components, we are interested in the Nash equilibria of the corresponding game, in order to understand strategic behavior in FL and how it affects the quality of the players' models.

Formal setupWe denote the samples of each player \(i\) by \(x^{i}=\{x^{i}_{1},,x^{i}_{n}\}\), where \(x\), and assume that \(\{x^{i}_{j}:i[N],j[n]\}\) are _not necessarily independent_ samples from an unknown distribution \(()\). For simplicity, we assume that all players have an equal number of samples \(n\). Players communicate via an FL-like protocol, through which a central server model \(^{s}^{d}\) is updated. The intended goal of this procedure is to find a value for \(^{s}\) that minimizes a loss function \(f_{}()\). Note that because \(\) is unknown to the players, they can benefit from honest collaboration.

The protocol consists of \(T\) rounds and the central model is initialized at some \(^{s}_{1}^{d}\). At time \(t=1,2,,T\) the server sends the model \(^{s}_{t}\) to all participants. Each agent \(i\) is then _meant to send an update_\(g_{t}(^{s}_{t},x^{i})\) to the server, for some function \(g_{t}:^{d}^{n}^{d}\). For example, in the standard FedSGD setting, \(f_{}()=_{x}[f_{x}()]\) for functions \(f_{x}:^{d}\) and \(g=_{j} f_{x^{i}_{j}}(^{s}_{t})\) serves as an estimate of the gradient of \(f\) evaluated at the data of player \(i\). Each player \(i\) then sends a message \(m^{i}_{t}^{d}\) to the server (which may or may not be equal to \(g_{t}(^{s}_{t},x^{i})\)). Finally, the server computes a new model \(^{s}_{t+1}=(^{i}_{t},m^{i}_{t},m^{2}_{t},,m^{N}_ {t})\), via an aggregating function \(:^{d}^{N d}^ {d}\) (for example, in FedSGD this will be a gradient update computed as \(_{t}=_{i=1}^{N}m^{i}_{t}\)).

Players' strategiesAt every step, the players take two decisions: how to _attack_ by sending a manipulated estimate to the server, and how to _defend_ themselves from unreliable estimates when updating their locally tracked model based on information received from the server. Formally, we assume that each player \(i[N]\) chooses (potentially randomized) functions \(a^{i}_{1},a^{i}_{2},,a^{i}_{T}\) and \(d^{i}_{1},,d^{i}_{T}\), that describe their behavior for the attack and defense stages at every time step. These functions are chosen from two respective _sets of possible actions_\(\) and \(\). The tuple of chosen actions \(p^{i}=(a^{i}_{1},,a^{i}_{T},d^{i}_{1},,d^{i}_{T})^{ T}^{T}\) represents the player's global strategy.

In the most general case, the attacks and defenses of each player \(i\) may take into account all information available to the player, throughout the history of the optimization process. At time \(t\) this include the models \(^{s}_{1},,^{s}_{t}\) received by the server; the local models \(^{i}_{1},,^{i}_{t}\) the player kept at previous iterations; the attack strategies \(a^{i}_{1},,a^{i}_{t}\) used up to time \(t\) (e.g. to correct for one's own faulty estimate at time \(t\)); as well as additional randomness \(^{i}_{1},,^{i}_{t}\) sampled at each round.

Players' rewardsEach player aims to obtain a final model \(^{i}_{T+1}\) that approximately minimizes \(f_{}(^{i}_{T+1})\). Crucially, their reward also depends on other players' models. Specifically, we assume that each player \(i\) has a reward function \(^{i}:^{N d}() \) and receives the reward

\[r^{i}=^{i}(^{1}_{T+1},^{2}_{T+1},,^{N }_{T+1},). \]

Note that the messages \(m^{i}\) sent by players and thus \(^{i}_{T+1}\) and each player's reward depend not only on players' strategies, but also on the particular realization of their samples \(x^{i}\). We thus focus on _expected rewards_, averaging out the effects of particular realizations of players' samples and randomness in their strategies. We study the _vector of expected rewards_\((r^{1},,r^{N})\) and its dependence on the _strategy profile_, that is on the distribution of strategies \(p=(p^{1},p^{2},,p^{N})\) chosen by each player.

Assumptions on players' behaviourTo analyze players' behaviour, we make two assumptions, giving rise to a classic game-theoretic setup. The first is that players seek to maximize their expected reward, as defined above, i.e. players are _rational_. In addition, since the reward of each player depends on the actions of the other players, players account for the actions of the others, which means that their behavior is _strategic_. A natural solution concept in this context is the Nash equilibrium (Nash, 1951). This describes a strategy profile in which no player can improve their reward by unilaterally changing their strategy. In our case, this classic notion translates to the following definition.

**Definition 1**.: Let \(p=(p^{1},p^{2},,p^{N})((^{T} ^{T}))^{N}\) be a strategy profile. Then \(p\) is a (mixed) Nash equilibrium if:

\[ p^{*}(^{T}^{T})  i[N]:(r^{i}(p^{1},,p^{i},,p^{N})) (r^{i}(p^{1},,p^{*},,p^{N})),\]

where the expectation is taken with respect to the randomness of the data and the players' strategies.

### Specific instantiations: mean estimation and stochastic gradient descent

We study two specific cases of the game, each modeling a fundamental learning problem. The first is single-round mean estimation (Sections 4 and 5), which correspond to the general setup with \(T=1\), \(f_{}()=\|-\|^{2}\), where \(=_{X}(X)\), and the updates \(g\) being the sample means of the players. The second is multi-round stochastic optimization of strongly-convex functions \(f_{}()\) via SGD (Section 6), in which case the updates \(g\) are stochastic gradient estimates based on players' data. In the corresponding sections, we define natural rewards that model the competing incentives.

Strategy spacesTo describe the _attack strategies_, in both cases we model _attacks that send a noisy update_\(g_{t}(^{s}_{t},x^{i})+^{i}_{t}^{i}_{t}\), for normalized zero-mean noise \(^{i}_{t}\) and an attack parameter \(^{i}_{t}\), to the server. Up to a certain magnitude of \(^{i}_{t}\), these attacks have a natural interpretation as the act of _hiding a random subset of a player's data_. In addition, the \(^{i}_{t}\) parameters have a natural interpretation as the _of aggressiveness of the player_. In the case of mean estimation, we are also able to analyze much more general attack strategies that can adjust \(^{i}_{t}\) based on the players' samples \(x^{i}\) and additionally allow for adding a directed bias to the communicated messages.

For the _defense strategies_ in the mean estimation case, we consider a defense strategy that corrects the mean estimate received from the server for the player's own manipulation. The player then computes a weighted average of the result and the their local mean. The weighting parameter \(^{i}\) used then has a natural interpretation as the _cautiousness of the player_. In the SGD case we directly provide mechanisms that incentivize honesty at the attack stage, making potential defenses redundant.

## 4 A single-round version of the game: competitive mean estimation

In this section we analyze a single-round version of the game, in which players aim to estimate the mean of a random variable \(X(^{d})\). Specifically, we consider the game defined in Section 3, in the case of \(T=1\) rounds. Players sample from a distribution \((^{d})\) by first independently sampling a random mean \(_{i} D_{}\) and "variance" \(^{2}_{i}:=\|X^{i}-_{i}\|^{2} D_{}\) (this models potential _heterogeneity_ between clients), and then receiving (conditionally) independent samples from a random variable \(X^{i}\) with mean \(_{i}\) and "variance" \(_{i}^{2}\). We call \(=_{i}\), \(^{2}=_{i}^{2}\) and \(_{}^{2}=\|-_{i}\|^{2}\). We assume that players do not know the distributions \(D_{}\) and \(D_{}\).

Each player wants to estimate the global mean \(\) as well as possible. During the single communication round, the players are meant to communicate the mean of their samples: \(g_{1}(_{1}^{*},x^{i})=_{j=1}^{n}x_{j}^{i}\). Instead, they send messages \(m_{1}^{i}\). The server aggregates the received messages by averaging them, so that \(_{2}^{*}=_{i=1}^{N}m_{1}^{i}\). The players then receive the value of \(_{2}^{*}\) from the server and use their defense strategy to arrive at a final estimate \(_{2}^{i}=d(_{2}^{*},x^{i})\) of the mean. For simplicity of notation, we ignore the dependence of all values on the time \(t=1\) in the rest of this and the next section.

Reward functionsTo model competitive incentives, the reward of each player needs to increase as their own estimate of the mean becomes better, and as the estimates of other players become worse. Therefore, a natural reward function is:

\[^{i}(^{1},,^{N},)=\| ^{j}-\|^{2}}{N-1}-_{i}\|^{i}-\|^{2}, \]

for some \(_{i} 0\). The value of \(_{i}\) quantifies to what extent player \(i\) prioritizes the quality of their own estimate over damaging the estimates of the other players.

Attack strategiesWe assume that players choose what estimates to communicate by deciding how to perturb the empirical mean of their data. Specifically, each player \(i\) selects parameters \(^{i}(x^{i}) 0,b^{i}(x^{i})^{d}\) based on their sample, in a potentially randomized manner, and communicates:

\[m^{i}=^{i}+^{i}(x^{i})^{i}+b^{i}(x^{i}), \]

where \(^{i}=_{j=1}^{n}x_{j}^{i}\), \([^{i}]=0\) and \(\|^{i}\|^{2}=1\). Here \(^{i}\) is the standard empirical mean of \(x^{i}\), while \(^{i}\) represents the magnitude of the noise player \(i\) adds to the estimate and \(b^{i}(x^{i})\) is an additional bias term. Note that the case of \(b^{i}(x^{i})=0\) recovers the data-hiding attack discussed in Section 3.2.

In order to prevent "non-general" strategies, such as simply setting \(m^{i}=\), that cannot be analyzed properly as their success depends on the true parameter \(\), we assume that players do not base their strategies on guesses about \(\) beyond the information they obtained from \(^{i}\). Formally, we assume

\[<^{j}-,b^{j}(x^{j})>=0. \]

This prevents \(b^{i}\) from linearly encoding additional knowledge about \(\) and for example holds whenever \(b^{i}(x^{j})\) is independent of the residuals \(^{j}-\). We also assume that the noise variables \(^{i}\) are independent of each other and all \(x_{j}^{k}\) and \(^{k}(x^{k})\), but make no further distributional assumptions about \(^{i}\). Indeed, all of our theorems will hold regardless of any additional assumptions about \(^{i}\).

We denote this set of attack strategies as \(\). Each element in \(\) can be uniquely identified via the distribution of the noise \(^{i}\) and the functions \(^{i}(x^{i})\) and \(b^{i}(x^{i})\). As \(=b=0\) can be interpreted as covering the fully collaborative case, while \(,b\) covers the fully malicious case, the (adaptive) parameters \(,b\) have a natural interpretation as measures of the _aggressiveness_ of a player.

We also note that these attacks are _very general_: \(m^{i}(x^{i})-^{i}\) can always be written as the sum of a determinstic component \((x^{i})\) and zero mean noise \((x^{i})\), such that _(4) and the fixed distribution of \(^{i}\) are the only assumptions separating us from the most general possible set of attacks strategies.

Defense strategiesIn the defense stage each player uses the received estimate \(^{s}==_{i=1}^{N}m^{i}\) and their local data \(x^{i}\) to compute a final estimate of the unknown mean. Two extreme approaches for player \(i\) are being fully cautious and using their local mean \(^{i}\) only, or fully trusting other players and computing the average of all sent updates, corrected for their own manipulation, that is \(^{}=(N^{s}-m^{i}+^{i})\). We consider defense strategies that take a weighted average of these two extremes: Each player \(i\) chooses a parameter \(^{i}\) and constructs a final estimate

\[^{i}=(1-^{i})^{i}+^{i}^{i}. \]

Denote the described set of defenses, as \(\). Each element in \(\) is uniquely identified via the corresponding parameter \(\) with \(=0\) and \(=1\) covering the extreme cases from above. Since \(\) can be used to interpolate between these two extremes, it can be seen as a measure of _cautiousness_.

We do not cover more complicated defense strategies for two reasons: First, our proposed mechanisms will incentivize players to be honest _even without any defenses_ such that more advanced defense mechanisms are not necessary. Second, as defenses can be seen as a method for mean estimation, analyzing the optimality for general classes of defenses would be fundamentally challenging for \(d 3\) due to Stein's Paradox (Stein, 1956), even for a single player version of our game.

### Expected rewards and Nash equilibria

We now analyze the game with strategy set \(()\). As the specific distribution of \(^{i}\) does not affect the players' rewards, the attack and defense strategies are for all relevant purposes uniquely determined by the functions \(\), \(b\), and \(\) respectively. We abuse notation and consider \((^{i},b^{i},^{i})\) as the strategy of player \(i\). First we derive a formula for the MSE of a player, for a fixed strategy profile.

**Theorem 4.1**.: _Let \(\) be as described above. Then the expected mean squared error (MSE) of player \(i[N]\) for any strategy profile \(((^{1},b^{i},^{1}),,(^{N},b^{N},^{N})) ()^{N}\) is:_

\[(\|^{i}-\|^{2}) =(1-^{i})^{2}(}{Nn}+_{}}{N}+}_{j i}(a^{j}(x^{j})^{ 2})+}\|_{j i}b^{j}(x^{j})\|^{2})\] \[+(^{i})^{2}(}{n}+^{2}_{})+2( 1-^{i})^{i}(}{Nn}+_{}}{N})\]

This is proven in Appendix C similar to the bias-variance decomposition. This result allows us to analyze the expected rewards defined by equation (2) of the players for any strategy profile.

One can immediately see that there is no incentive for players to cooperate as long as \(^{i}<1\): other players \(j\) can always increase their reward by increasing \((a^{j}(x^{j})^{2})\) (unless it is already infinite). But for finite \((a^{j}(x^{j})^{2})\) and \(\|_{j i}b^{j}(x^{j})\|^{2}\), the optimal \(^{i}\) can be shown to never equal one, such that equilibria are only possible "at infinity":

**Corollary 4.2**.: _The game defined by the reward in equation (2) and the set of strategies \(\) does not have any (pure or mixed) Nash equilibrium for which \((^{j}(x^{j})^{2})\) and \(\|b^{j}(x^{j})\|^{2}\) are finite for all players._

For details, see Appendix C. This shows that our defenses are unable to prevent maximal dishonesty by at least some players and formalizes a simple intuitive observation: as long as a player considers other players' updates at all, others are incentivized to reduce the information their updates convey about their samples. As at equilibrium at least one other player will infinitely distort the server estimate \(\), _no player can benefit from collaborative learning without modifications to the protocol._

## 5 Mechanisms for incentivizing honesty

Given the impossibility of successful learning with rational competing agents in the simple mean estimation setting, we shift our focus to modifications of the protocol that allow for honest Nash equilibria (that is, equilibria where \(^{j}(x_{j})=b^{j}(x^{j})=0, j\)). To this end, we design two mechanisms that seek to penalize dishonest players proportionally to the magnitude of their manipulations (and, thus, the damage caused to other players). Note that this is _complimentary_ to robust estimation methods (Diakonikolas et al., 2019) that can reduce but not eliminate the impact of manipulations.

The first relies on explicit side payments and requires transferable utility (that is, the existence of an outside resource \(R\) such as money, that is valued equally and on the same scale as the reward \(\) by all players). The second is a modification of the FL protocol, in which the server adds noise to the estimates it sends to players that have sent suspicious updates. Importantly, for both mechanisms, the penalties can be computed by the server without the need for knowing \(^{i},b^{i}\) or other additional information beyond the players' updates, and without prior knowledge of the true distribution \(\).

### Efficient solution for fully transferable utility

We first consider the case of transferable utility. In this case, we introduce a more general _penalized reward_ for player \(i\), which is given by

\[^{i}_{p}=^{i}(^{1},,^{N},)-p^{i}(m^ {1},,m^{N}).\]Here \(p^{i}(m^{1},,m^{N})\) denotes a penalty paid by player \(i\) to the server, measured in terms of the resource \(R\) and depending on the messages that the clients sent. As players value the reward and resource equally, they optimize for \(^{i}_{p}\) instead of \(^{i}\).

Inspired by peer prediction (Miller et al., 2005) we consider a penalty for player \(i\) proportional to the squared difference between that player's update and the average update sent by all players:

\[p^{i}(m^{1},,m^{N})=C\|m^{i}-\|^{2}, \]

for some constant \(C 0\). This is a natural measure of the "suspiciousness" of the client's update. In order to prevent excessive payments for honest players, we redistribute the penalties as

\[p^{ i}(m^{1},,m^{N})=C\|m^{i}-\|^{2}-_{j i}-\|^{2}}{N-1}\]

This redistribution also makes it possible to implement our mechanism in a decentrally with messages sent publically, if players are able to credibly commit to the implied payments to other players.

Theorem 5.1 establishes that this penalty can incentivize full honesty for the right choice of \(C\):

**Theorem 5.1**.: _In the setting of Theorem 4.1 for the penalized game with rewards_

\[^{i}_{p^{}}=\|^{j}-\|^{2}}{N-1}- _{i}\|^{i}-\|^{2}-p^{ i}(m^{1},,m^{N})\]

_the strategy profile \(^{j}=b^{j}=^{j}=0\) for all \(j\) is a Nash equilibrium, whenever \(C>-1}\) and maximizes the sum of all players' rewards among equilibria whenever \( 1\) for all players._

_At this equilbrium, the expected penalty \(p^{i}(m^{1},,m^{N})\) paid by each player \(i\) is equal to \(0\). Each player is incentivized to participate in the penalized game rather than relying on their own estimate, whenever \(N>2\), the other \(N-1\) players participate at the honest equilibrium, and \(_{i}>}\)._

Intuitively, our incentive mechanism is effective because \(^{i}\) and \(b^{i}\) only affect the first term of the original reward of player \(i\) (equation 2), as well as the penalty, to which they contribute at most as \(}\) and \(--1}{N^{2}}C\) respectively. At the honest equilibrium every player's MSE is in \(O()\), such that for large \(N\) a player can strongly improve their own error by joining the collaboration, while barely affecting others' errors. For a complete proof consider Appendix D.1.

Theorem 5.1 shows that our mechanism fulfills two desirable properties: _(budget) balance_ and _(ex-ante) individual rationality/voluntary participation_(Jackson, 2014). The first property means that the server neither makes a profit nor a loss. The second holds as long as \(_{i}>}\) and the other players take part in the optimization, and means that a player will receive better reward at the game's equilibrium, than when learning with their own data, despite the penalties assigned by the server. While non-honest equilbria exist, these are difficult to coordinate on (as they lack the natural symmetric Schelling point of honesty), while also yielding less total reward summed across players than honesty, such that there is no strong incentive for such coordination.

### Non-transferable utility

We now discuss a way to achieve similar results in the case of non-transferable utility, where players' rewards are not translatable to monetary terms. Instead of modifying players' reward function, we modify the FL protocol, altering the server's messages to players. This effectively results in a robust learning algorithm that the server can implement. We do so by letting the server send noisier versions of its mean estimate to players whose messages suspiciously deviate from the average of all players' updates. The penalization scheme is designed in a way such that players receive expected rewards \((^{i})\) similar to the expected penalized rewards \((^{i}_{p^{}})\) in the previous section. This is conceptually similar to methods against free-riding (e.g. Karimireddy et al. (2022)), which often tie the accuracy of the model a client receives in an FL setting to the client's overall contribution to model training.

**Theorem 5.2**.: _Consider the modified game with reward \(^{i}=\|^{j}-\|^{2}}{N-1}-_{i}\| ^{i}-\|^{2},\) where player \(i\) receives an estimate \(+^{i}\|m^{i}-\|\) for independent noise \(^{i}\) with mean \(^{i}=0\) and "variance" \(\|^{i}\|^{2}=1\), instead of the empirical mean \(\), from the server. Then honesty \((^{i}=0,b^{i}=0,^{i}=)\) is a Nash equilibrium, as long as \(C>(N-1)^{2}-1}\) and \(_{i}>}\). Furthermore, honesty maximizes the sum of all players' rewards among equilibria whenever \(_{i} 1\) for all players._

_For fixed \(_{i}=,k>1\) and \(C=-1}\), \((\|^{i}-\|^{2})=O(}{Nn}+ }{N})\) and players are incentivized to participate in the penalized game rather than relying on their own estimate, whenever \(N 2\), the other \(N-1\) players participate at the honest equilibrium and \( 1\)._

Essentially, the noise added by the server increases the expected MSE for player \(i\) by \(C\|m^{i}-\|^{2}\), producing similar incentives as in Theorem 5.1. Theorem 5.2 is proven in Appendix D.2. We obtain voluntary participation if at least two other players participate at the honest equilibrium and \(>1\).

The benefits of modeling clients' rationalityNote that at the equilibrium, players' MSEs are of the same order as if all clients honestly communicated their sample means. In particular, when \(^{*}=0\) (i.e. homogeneous clients), this is in contrast to known negative results for worst-case poisoning attacks Qiao and Valiant (2018) and single-round Byzantine robustness Alistarh et al. (2018). In this sense, our modified protocol acts as a robust and efficient collaborative learning algorithm. This is possible because our data corruption model is derived by explicitly modeling clients' incentives.

## 6 Beyond mean estimation: stochastic gradient descent

We now extend the ideas from the last section to multi-round collaborative Stochastic Gradient Descent (SGD) in the FL setting. We show that under stronger assumptions, mechanisms similar to those described in Section 5.1 can still provide arbitrary bounds on manipulations by rational players. Again, this is _complimentary_ to methods for robust federated learning such as median-based gradient aggregation that can reduce but not eliminate the impact of existing manipulations.

### The game and rewards

We consider a \(T\)-round version of the game described in Section 3. The FL protocol is designed to minimize a loss function \(f_{}()\) over a closed and convex set of model parameters \( W^{d}\) that contains the global minimizer of \(f_{}()\). At every time step \(t\), each player is meant to communicate an update \(g_{t}(_{t}^{s},x^{i})\) with expectation \( f(_{t}^{s})\). We denote by \(e_{t}^{i}(_{t})=g_{t}(_{t}^{s},x^{i})- f(_{t})\) the difference between the gradient and the estimate, which is a deterministic function of \(_{t}\) for fixed data \(x^{i}\), but is assumed to fulfill \(_{x^{i}}e_{t}^{i}()=0\) for all \(\), and to be independent across time and players. Intuitively, \(f_{}()\) can be thought of as an expected loss \(_{x}f_{x}()\), for which player \(i\) computes approximate stochastic gradients as \( f_{x_{t}^{i}}(_{t-1}^{s})\) using their \(t\)-th sample \(x_{t}^{i}\). The message sent by player \(i\) is termed \(m_{t}^{i}\). The server averages the received messages to compute a gradient estimate \(_{t}=_{i}m_{t}^{i}\) and updates the parameter \(\) via \(_{t+1}^{s}=_{W}(_{t}^{s}-_{t}_{t})\), using a fixed learning rate schedule \(_{t}\) and the projection \(_{W}\) onto \(W\). Finally, the server sends the updated \(_{t+1}^{s}\) to all players.

Strategies and rewardsWe consider attacks of the form \(m_{t}^{i}=g_{t}(_{t}^{s},x^{i})+_{t}^{i}_{t}^{i}\), that is, the true gradient estimate plus random noise of the form \(_{t}^{i}_{t}^{i}\), where \(_{t}^{i}=0\), \(\|_{t}^{i}\|^{2}=1\) and \(_{t}^{i}\) is sampled independent from the other \(_{t}^{j}\) and the algorithm's trajectory. The set of attack strategies \(^{g}\) is then described by the sequence of aggressiveness parameters \(_{t}^{i}>0\), which we assume to be selected in advance, independent of the optimization trajectory. Since defenses were already ineffective for mean estimation, we directly focus on mechanisms and only consider adjustments to the server's final estimate for the noise player \(i\) added themselves in the final step \(T\): \(_{T+1}^{i}=_{T}-^{i}}{N}_{T}^{i}\). The assumption of non-adaptive strategies is needed to avoid complicated dependencies between successive SGD rounds and more sophisticated attack strategies are beyond the scope of our analysis.

We do not consider a bias term for these attacks. Our unbiased attacks have natural interpretations, both in terms of _hiding randomly selected data points_ and _differential privacy defenses_, and unlike in the mean estimation case, the effects of a fixed-direction attack on the loss \(f_{}()\) can strongly depend on the current estimate \(_{t}^{s}\) and the attack's precise direction, making it substantially harder to analyze such attacks. That said, it is easy to see that if the server aims to optimize \(f_{}(_{T+1})\) and is allowed to shift its estimate \(_{t}^{s}\) to defend against fixed-direction attacks at every step \(t\), the fixed direction attacks would be neutralized by the server at any equilbrium, unless they inadvertently improved \(f_{}(_{T+1})\)Given these strategies and a Lipschitz function \(U_{i}:^{N}\), player \(i\) aims to maximize the reward

\[^{i}_{U}=U^{i}(f(^{1}_{T+1}),...,f(^{N}_{T+1})). \]

It is easy to see that this game does not always have an equilibrium and players are often incentivized to lie aggressively. In particular, we recover the mean estimation setting with \(^{i}=0\) when setting \(U^{i}(x)=_{j i}\|-^{j}\|^{2}-_{i}\|-^{i }\|^{2}\), \(T=1\), \(_{1}=0\) and \(_{1}=0.5\), as \(\|-x^{j}_{i}\|^{2}=2(-x^{j}_{i})\).

Our next result establishes that players can be incentivized to be arbitrarily honest, using a penalty scheme similar to the one in Section 5.1. Again, penalties are redistributed such that players' penalties have zero expectation whenever \(^{i}_{t}=^{j}_{t}\) for all \(i,j,t\). We set \(_{t}=_{j}m^{j}_{t}\) and

\[^{i}_{U_{p}}=U^{i}(f(^{1}_{T+1}),...,f(^{N}_{T+1}))- _{t=1}^{T}C_{t}\|m^{i}_{t}-_{t}\|^{2}+_{k i} _{t}^{T}C_{t}\|m^{k}_{t}-_{t}\|^{2}\]

for constants \(C_{t}\). With this penalized reward we prove:

**Theorem 6.1**.: _Assume \(f\) is \(B\)-smooth and \(L\)-Lipschitz on \(W\) and \(m\)-strongly convex on \(^{d}\) (See Appendix 2 for definitions of these properties). Also assume that for all \(i\) and \(t\) the gradient noise \(e^{i}_{t}\) is \(B^{}\)-Lipschitz with probability one and that there exist scalars \(M 0\) and \(M_{V} 0\), such that for all \(t\):_

\[_{s_{i}}(\|(e^{i}_{t}(^{s}_{t})\|^{2}) M+M_{V}\| f( ^{s}_{t}))\|_{2}^{2}. \]

_Set the learning \(_{t}=\) for an integer constant \(>1\), such that \(/N+1)}\)._

_Then if \(U^{i}\) is \(l_{1}\)-Lipschitz with constant \(L_{U}\) for all players \(i\), all player's best response strategy fulfill \(^{i}_{i}N}{C^{t}(N-2)m}\) independent of other players' strategies. If \(^{i}_{t}=^{j}_{t}, i,j,t\), each player's expected penalty is \(0\). For \(>0\), \(C^{t}N}{( N-2)m}\) yields \(^{i}_{t}\) for rational players. In that case, if in addition \(W\) is bounded and we have that \(P( t T:_{W}(^{s}_{t}-_{t}_{t})^{s} _{t}-_{t}_{t}) O()\), we get \((f(_{T+1})-f(^{s})) O(}{NT})+O(})\)._

Intuitively, small perturbations of gradient estimates sent to the server should only have a small effect on the final learnt model. Formally, our assumptions allow us to inductively prove bounds on the difference between the values of \(f(^{j}_{T+1})\) in a clean \((^{i}_{t}=0)\) scenario and a scenario with other values of \(^{i}_{t}\). By setting \(C^{t}\) large enough, we can then ensure that the penalties paid by a dishonest player always outweigh their effect on the final model. The condition on \(_{W}\) ensures that SGD is not slowed down by projections and holds for "sufficiently large" distances between the boundary of \(W\) and \(^{1}\). See Appendix E.2 for more details on this and the theorem proof. Theorem 6.1 implies that for sufficiently large \(C^{t}\), despite _all players acting strategically_, the model converges at speed comparable to when all clients share clean updates (where the convergence rate is \(O(})+O(})\)), thereby ensuring full learning benefits from the collaboration. Moreover, as long as all players are equally honest, this is achieved with zero expected penalties for players and thus with budget balance.

## 7 Experimental results

To verify that our mechanisms can work for SGD in the non-convex case we simulate FedSGD McMahan et al. (2017) with clients corrupting their messages to different degrees, and record how players' rewards and penalties are affected by their aggressiveness \(\) for different penalty constants \(C\). The \(^{i}_{t}\) that empirically maximizes a player's reward is an approximate best response for a given \(C\) and fixed \(^{j}_{t}\) for \(j i\), and should thus be small for a successful mechanism.

First, we simulate FedSGD, treating each writer as a client, to train a CNN-classifier using the architecture provided by Caldas et al. (2018) for the FeMNIST dataset that consists of characters and numbers written by different writers. Second, we train a two-layer linear classifier with \(384\) hidden neurons on top of frozen "bert-base-uncased" embeddings on the Twitter Sentiment Analysis dataset from Caldas et al. (2018). In both cases, we randomly select \(m=3\) clients and compute a gradient estimate \(g^{i}_{t}\) for the cross entropy loss \(f\) using a single batch containing \(90\%\) of the data provided by the corresponding writer at time step \(t<T=10650\)1. We test on the remaining \(10\%\) of the data.

For both experiments, we randomly split writers into two groups \(A\) and \(B\) containing one and two thirds of the writers respectively, and corrupt the gradient estimates \((m_{t}^{i})_{l}=(g_{t}^{i})_{l}+_{A}(_{t}^{i})_{l}\) sent by players in group \(A\) for each weight tensor and bias vector \(l\) separately, by adding isotropic normal noise \((_{t}^{i})_{l}\) with "variance" \(E\|(_{t}^{i})_{l}\|^{2}=1\). We do the same with \(_{B}\) for group \(B\). At each step, the three corrupted gradients are then averaged (weighted by the corresponding writers' datasets' sizes) to \(_{t}\), which is used to update our neural network's parameters \(_{t}\) with learning rate \(0.06\), i.e. \(_{t+1}=_{t}-0.06_{t}\). Unlike in our theorems, writers reuse the same data points whenever they calculate gradients. In the clean case \((_{A}=_{B}=0)\), our final models achieve a test accuracy of \(86\%\) on FeMNIST 2 and \(63\%\) on Twitter. We record both the cross-entropy loss \(f\) achieved by the final model \(_{T}\) on the test set, as well as the sum of the squared deviations \(\|m_{t}^{i}-_{t}\|^{2}\) incurred by each individual client \(i\) across all steps. This allows us to calculate the estimated reward \(_{p}^{i}(C)=f(_{T})-_{t=0}^{T-1}_{t}(i)C\|m_{t }^{i}-_{t}\|^{2}+_{k i}_{t=0}^{T-1}_{t }(i)_{t}(k)C\|m_{t}^{k}-_{t}\|^{2}\) received by every player for penalty weights \(C\) held constant over time, where \(_{t}(j)\) is a binary indicator equal to \(1\) whenever player \(i\) provided an update at time \(t\).

Figure 3 shows the average reward \(_{p}^{i}(C)\) received by players in group \(A\) for \(_{B}=0\) and \(_{A}\) varying on the x-axis for different penalty weights \(C\). It clearly shows that penalization decreases players' gains from adding noise even in the non-convex case, and that near-zero noise is optimal for players given sufficiently large penalty weights \(C\). At the same time, despite client heterogeneity, the penalties paid by honest players are small: In the FeMNIST experiments, if all players are honest, an overwhelming majority \((98\%)\) of players end up paying less than \(0.0031\) on average (over \(10\) rounds), even at \(C=0.0002\). This is an order of magnitude under the increase in loss from moving from \(_{A}=0\) to \(_{A}=9\), which is already disincentivized for the substantially smaller penalty constant \(C=5e-5\). On the Twitter dataset, noise has a larger effect on the loss (the loss is degraded by \(0.084\) already at noise level \(a_{A}=5\) compared to \(0.034\) at noise level \(a_{A}=9\) for FeMNIST). This means that larger penalty weights are necessary to incentivize honesty. Correspondingly, the \(98\)th percentile of penalties paid at the largest considered penalty level \(C=0.002\) is also larger (\(0.0243\)) than on FeMNIST. Additional experimental results can be found in Appendix B.2.

## 8 Conclusion

In this paper, we studied a framework for FL with strategic agents, where players compete and aim to not only improve their model but also damage others' models. We analyzed both the single- and multi-round version of the problem for a natural class of strategies and goals and showed how to design mechanisms that incentivize rational players to behave honestly. In addition, we empirically demonstrated that our approach works on realistic data outside the bounds of our theoretical results.

Figure 1: FeMNIST Dataset Figure 2: Twitter Dataset