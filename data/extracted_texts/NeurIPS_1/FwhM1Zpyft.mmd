# Scalable Neural Network Verification with Branch-and-bound Inferred Cutting Planes

Duo Zhou\({}^{1}\) Christopher Brix\({}^{2}\) Grani A Hanasusanto\({}^{1}\) Huan Zhang\({}^{1}\)

\({}^{1}\)University of Illinois Urbana-Champaign \({}^{2}\)RWTH Aachen University

{duozhou2,gah}@illinois.edu brix@cs.rwth-aachen.de huan@huan-zhang.com

###### Abstract

Recently, cutting-plane methods such as GCP-CROWN have been explored to enhance neural network verifiers and made significant advances. However, GCP-CROWN currently relies on _generic_ cutting planes ("cuts") generated from external mixed integer programming (MIP) solvers. Due to the poor scalability of MIP solvers, large neural networks cannot benefit from these cutting planes. In this paper, we exploit the structure of the neural network verification problem to generate efficient and scalable cutting planes _specific_ for this problem setting. We propose a novel approach, Branch-and-bound Inferred Cuts with Constraint Strengthening (BICCOS), which leverages the logical relationships of neurons within verified subproblems in the branch-and-bound search tree, and we introduce cuts that preclude these relationships in other subproblems. We develop a mechanism that assigns influence scores to neurons in each path to allow the strengthening of these cuts. Furthermore, we design a multi-tree search technique to identify more cuts, effectively narrowing the search space and accelerating the BaB algorithm. Our results demonstrate that BICCOS can generate hundreds of useful cuts during the branch-and-bound process and consistently increase the number of verifiable instances compared to other state-of-the-art neural network verifiers on a wide range of benchmarks, including large networks that previous cutting plane methods could not scale to. BICCOS is part of the \(\),\(\)-CROWN verifier, **the VNN-COMP 2024 winner**. The code is available at [https://github.com/Lemutixme/BICCOS](https://github.com/Lemutixme/BICCOS).

## 1 Introduction

Formal verification of neural networks (NNs) has emerged as a critical challenge in the field of artificial intelligence. In canonical settings, the verification procedure aims to prove certified bounds on the outputs of a neural network given a specification, such as robustness against adversarial perturbations or adherence to safety constraints. As these networks become increasingly complex and are deployed in sensitive domains, rigorously ensuring their safety and reliability is paramount. Recent research progress on neural network verification has enabled safety or performance guarantees in several mission-critical applications .

The branch and bound (BaB) procedure has shown great promise as a verification approach . In particular, the commonly used ReLU activation function exhibits a piecewise linear behavior, and this property makes ReLU networks especially suitable for branch and bound techniques. During the branching process, one ReLU neuron is selected and two subproblems are created. Each subproblem refers to one of the two ReLU states (inactive or active) and the bounds of the neurons' activation value are tightened correspondingly. BaB systematically branches ReLU neurons, creates a search tree of subproblems, and prunes away subproblems whose bounds are tight enough to guarantee the verification specifications. Although our work focuses on studying the canonical ReLU settings, branch-and-bound has also demonstrated its power in non-ReLU cases . Despite its success in many tasks, the efficiency and effectiveness of the branch-and-bound procedure heavily depend on the number of subproblems that can be pruned.

Recent work has explored the use of cutting plane methods in neural network verification. _Cutting planes_ ("cuts") are additional (linear) constraints in the optimization problem for NN verification to tighten the bound without affecting soundness. By introducing carefully chosen cuts, they can significantly tighten the bounds in each subproblem during BaB, leading to more pruned subproblems and faster verification. However, it is quite challenging to generate effective cutting planes for large-scale NN verification problems. For example, GCP-CROWN  relied on strong cutting planes generated by a mixed integer programming (MIP) solver, which includes traditional _generic_ cutting planes such as the Gomory cuts , but its effectiveness is limited to only small neural networks. The key to unlocking the full potential of BaB lies in the development of scalable cutting planes that are _specific_ to the NN verification problem and also achieve high effectiveness.

In this paper, we propose a novel methodology, Branch-and-bound Inferred Cuts with COnstraint Strengthening (BICCOS), that can produce effective and scalable cutting planes specifically for the NN verification problem. Our approach leverages the information gathered during the branch-and-bound process to generate cuts on the fly. First, we show that by leveraging any verified subproblems (e.g., a subproblem is verified when neurons A and B are both split to active cases), we can deduce cuts that preclude certain combinations of ReLU states (e.g., neurons A and B cannot be both active). This cut may tighten the bounds for subproblems in other parts of the search tree, even when neurons A and B have not yet been split. Second, to make these cuts effective during the regular BaB process, we show that they can be strengthened by reducing the number of branched neurons in a verified subproblem (e.g., we may conclude that setting neuron B to active is sufficient to verify the problem, drop the dependency on A, and remove one variable from the cut). BICCOS can find and strengthen cuts on the fly during the BaB process, adapting to the specific characteristics of each verification instance. Third, we propose a pre-solving strategy called "multi-tree search" which proactively looks for effective cutting planes in many shallow search trees before the main BaB phase starts. Our main contributions can be summarized as follows:

* We first identify the opportunity of extracting effective cutting planes during the branch-and-bound process in NN verification. These cuts are _specific_ to the NN verification setting and _scalable_ to large NNs that previous state-of-the-art cutting plane methods such as GCP-CROWN with MIP-cuts cannot handle. Our cuts can be plugged into existing BaB-based verifiers to enhance their performance.
* We discuss several novel methods to strengthen the cuts and also find more effective cuts. Strengthening the cuts is essential for finding effective cuts on the fly during the regular BaB process that can tighten the bounds in the remaining domains. Performing a multi-tree search allows us to identify additional cuts before the regular BaB process begins.
* We conduct empirical comparisons against many verifiers on a wide range of benchmarks and consistently outperform state-of-the-art baselines in VNN-COMP . Notably, we can solve the very large models like those for cifar100 and tinyingenet benchmarks and outperform all state-of-the-art tools, to which GCP-CROWN with MIP-based cuts cannot scale.

## 2 Background

NotationsBold symbols such as \(^{(i)}\) denote vectors; regular symbols such as \(x^{(i)}_{j}\) indicate scalar components of these vectors. \([N]\) represents the set \(\{1,,N\}\), and \(^{(i)}_{:,j}\) is the \(j\)-th column of the matrix \(^{(i)}\). A calligraphic font \(\) denotes a set and \(^{n}\) represents the \(n\) dimensional real number space.

The NN Verification ProblemAn \(L\)-layer ReLU-based DNN can be formulated as \(f:^{(L)},\;\;\{^{(i)}=^{(i)}}^{(i-1)}+^{(i)},}^{(i)}=(^{(i)}),^{( 0)}=; i[L]\}\), where \(\) represents the ReLU activation function, with input \(=:}^{(0)}^{d^{(0)}}\) and the neuron network parameters weight matrix \(^{(i)}^{d^{(i)} d^{(i-1)}}\) and bias vector \(^{(i)}^{d^{(i)}}\) for each layer \(i\). This model sequentially processes the input \(\) through each layer by computing linear transformations followed by ReLU activations. The scalar values \(^{(i)}_{j}\) and \(x^{(i)}_{j}\) represent the post-activation and pre-activation values of the \(j\)-th neuron in the \(i\)-th layer, respectively. We write \(^{(i)}_{j}()\) and \(f^{(i)}_{j}()\) for \(^{(i)}_{j}\) and \(x^{(i)}_{j}\), respectively, when they depend on a specific \(\).

In practical applications, the input \(\) is confined within a perturbation set \(\), often defined as an \(_{p}\) norm ball. The verification task involves ensuring a specified output property for any \(\). For example, it may be required to verify that the logit corresponding to the true label \(f_{i}()\) is consistently higher than the logit for any other label \(f_{j}()\), thus ensuring \(f_{i}()-f_{j}()>0, j i\). Byintegrating the verification specification as an additional layer with only one output neuron, we define the canonical verification problem as an optimization problem:

\[f^{}=_{}f(), \]

where the optimal value \(f^{} 0\) confirms the verifiable property. We typically use the \(_{}\) norm to define \(:=\{:\|-_{0}\|_{}\}\), with \(_{0}\) as a baseline input. However, extensions to other norms and conditions are also possible . When we compute a lower bound \(^{} f^{}\), we label the problem **UNSAT** if the property was verified and \(^{} 0\), i.e. the problem of finding a concrete input that violates the property is **unsatisfiable**, or in other words **infeasible**. If \(^{}<0\), it is unclear whether the property might hold or not. We refer to this as **unknown**.

MIP Formulation and LP RelaxationIn the optimization problem (1), a non-negative \(f^{}\) indicates that the network can be verified. However, due to the non-linearity of ReLU neurons, this problem is non-convex, and ReLU neurons are typically relaxed with linear constraints to obtain a lower bound for \(f^{}\). One possible solution is to encode the verification problem using Mixed Integer Programming (MIP), which encodes the entire network architecture, using \(^{d^{(i)}}\) to denote the **pre-activation neurons**, \(}^{d^{(i)}}\) to denote **post-activation neurons** for each layer, and **binary ReLU indicator**\(z_{j}^{(i)}\{0,1\}\) to denote **(in)active neuron** for each unstable neuron. A lower bound can be computed by letting \(z_{j}^{(i)}\) and therefore relaxing the problem to an LP formulation. There is also an equivalent Planet relaxation . We provide the detailed definition in Appendix A. In practice, this approach is computationally too expensive to be scalable.

Branch-and-boundInstead of solving the expensive LP for each neuron, most existing NN verifiers use cheaper methods such as abstract interpretation  or bound propagation  due to their efficiency and scalability. However, because of those relaxations, the lower bound for \(f^{}\) might eventually become too weak to prove \(f^{} 0\). To overcome this issue, additional constraints need to be added to the optimization, without sacrificing soundness.

The branch-and-bound (BaB) framework, illustrated in Figure 1, is a powerful approach for neural network verification that many state-of-the-art verifiers are based on . BaB systematically tightens the lower bound of \(f^{}\) by splitting unstable ReLU neurons into two cases: \(x_{j} 0\) and \(x_{j} 0\) (branching step), which defines two subproblems with additional constraints. In each subproblem, neuron \(x_{j}\) does not need to be relaxed, leading to a tighter lower bound (bounding step). Note that \(\{x_{j}^{(i)} u_{j}^{(i)}=0\}\) and \(\{x_{j}^{(i)} l_{j}^{(i)}=0\}\) in the Planet relaxation are **equivalent** to \(\{z_{j}^{(i)}=0\}\) and \(\{z_{j}^{(i)}=1\}\) in the LP relaxation, respectively, see Lemma A.1 in Appendix A. Subproblems with a positive lower bound are successfully verified, and no further splitting is required. The process repeats on subproblems with negative lower bounds until all unstable neurons are split, or all subproblems are verified. If there are still domains with negative bounds after splitting all unstable neurons, a counter-example can be constructed.

General Cutting Planes (GCP) in NN verificationA (linear) cutting plane ("cut") is a linear inequality that can be added to a MIP problem, which does not eliminate any feasible integer solutions but will tighten the LP relaxation of this MIP. For more details on cutting plane methods, we refer readers to the literature on integer programming . In the NN verification setting, it may involve variables \(^{(i)}\) (pre-activation), \(}^{(i)}\) (post-activation) from any layer \(i\), and \(^{(i)}\) (binary ReLU indicators) from any unstable neuron. Given \(N\) cutting planes in matrix form as :

\[_{i=1}^{L-1}(^{(i)}^{(i)}+^{(i)}}^{(i)}+ ^{(i)}^{(i)}) \]

where \(^{(i)}\), \(^{(i)}\), and \(^{(i)}\) are the matrix coefficients corresponding to the cutting planes. Based on this formulation, one can introduce arbitrary valid cuts to tighten the relaxation. GCP-CROWN 

Figure 1: Each node represents a subproblem in the BaB process by splitting unstable ReLU neurons. Green nodes indicate paths that have been verified and pruned, while blue nodes represent domains that are still unknown and require further branching.

allows us to lower bound (1) with arbitrary linear constraints in (2) using GPU-accelerated bound propagation, without relying on an LP solver.

In , the authors propose to find new cutting planes by employing an MIP solver. By encoding (1) as a MIP problem, the MIP solver will identify potentially useful cutting planes. Usually, these cuts would be used by the solver itself to solve the MIP problem. Instead,  applied these cuts using GPU-accelerated bound propagation. This approach shows great improvements on many verification problems due to the powerful cuts, but it depends on the ability of the MIP solver to identify relevant cuts. As the networks that are analyzed increase in size, the respective MIP problem increases in complexity, and the MIP solver may not return any cuts before the timeout is reached. In the next chapter, we will describe a novel approach to generate effective and scalable cutting planes.

## 3 Branch-and-bound Inferred Cuts with Constraint Strengthening (BICCOS)

### Branch-and-bound Inferred Cuts

The first key observation in our algorithm is that the UNSAT subproblems in the BaB search tree include valuable information. If the lower bound of a subproblem leads this subproblem to be UNSAT (e.g., subproblems with green ticks in Fig. 1), it signifies that restricting the neurons along this path to the respective positive/negative regimes allows us to verify the property. A typical BaB algorithm would stop generating further subproblems at this juncture, and continue with splitting only those nodes that have not yet been verified. Crucially, no information from the verified branch is transferred to the unverified domains. However, sometimes, this information can help a lot.

_Example._ As shown in Fig. 1, assume that after splitting \(x_{1} 0\) and \(x_{3} 0\), the lower bound of this subproblem is found to be greater than \(0\), indicating infeasibility. From this infeasibility, we can infer that the neurons \(x_{1}\) and \(x_{3}\) cannot simultaneously be in the inactive regime. To represent this relationship, we use the relaxed ReLU indicator variables \(z_{1},z_{3}\) in the LP formulation and form the inequality \(z_{1}+z_{3} 1\). This inequality ensures that both \(z_{1}\) and \(z_{3}\) cannot be \(0\) simultaneously. If we were to start BaB with a fresh search tree, this constraint has the potential to improve the bounds for _all_ subproblems by tightening the relaxations and excluding infeasible regions.

We propose to encode the information gained from a verified subproblem as a new cutting plane. These cutting planes will be valid globally across the entire verification problem and all generated subproblems. We present the general case of this cutting plane below:

**Proposition 3.1**.: _For a verified, or UNSAT, subproblem in a BaB search tree, let \(_{+}\) and \(_{-}\) be the set of neurons restricted to the positive and negative regimes respectively. These restrictions were introduced by the BaB process. Then, the BaB inferred cut can be formulated as:_

\[_{i_{+}}z_{i}-_{i_{-}}z_{i}|_{+}|-1 \]

Proof deferred to Appendix B.1. The BaB inferred cut (3) will exclude the specific combination of positive and negative neurons that were proven to be infeasible in the respective branches. An example is shown in Fig. 1(a).

Note that while similar cuts were explored in , they were not derived from UNSAT problems within the BaB process. In our framework, these cuts can theoretically be incorporated as cutting planes in the form of (2) and work using GCP-CROWN. However, a limitation exists: all elements in our cut are ReLU indicator \(\), although GCP-CROWN applies general cutting planes during the standard BaB process. It was not originally designed to handle branching decisions on ReLU indicator variables \(\) that may have (partially) been fixed already in previous BaB steps. When cuts are added, they remain part of the GCP-CROWN formulation. However, during the BaB process, some \(\) variables may be fixed to \(0\) or \(1\) due to branching, effectively turning them into constants This situation poses a challenge because the original GCP-CROWN formulation presented in  does not accommodate constraints involving these fixed \(\) variables, potentially leading to incorrect results. To address this issue, we need to extend GCP-CROWN to handle BaB on the ReLU indicators \(\), ensuring that the constraints and cuts remain valid even when some \(\) variables are fixed during branching.

Extension of GCP-CROWNTo address this limitation, we propose an extended form of bound propagation for BaB inferred cuts that accommodates splitting on \(\) variables. In the original GCP-CROWN formulation, \(^{(i)}\) represents the set of **initially** unstable neurons in layer \(i\), for which \(z\) cutswere added due to their instability. However, during the branch-and-bound process, some of these neurons may be split, fixing their corresponding \(z\) variables to \(0\) or \(1\), and thus their \(z\) variables no longer exist in the original formulation. While updating all existing cuts by fixing these \(z\) variables is possible, it is costly since the cuts for each subproblem must be fixed individually, as the neuron splits in each subproblem is different. Our contribution is to handle these split neurons by adding them to the splitting set \(\) in the new formulation below, without removing or modifying the existing cuts (all subdomains can still share the same set of cuts). This approach allows us to adjust the original bound propagation in [61, Theorem 3.1] to account for the fixed \(z\) variables and their influence on the Lagrange dual problem, without altering the existing cuts. Suppose the splitting set for each layer \(i\) is \(^{+(i)}^{-(i)}:=^{(i)}^{(i)}\), and the full split set is \(=_{i[L-1]}^{(i)}\). The modifications to the original GCP-CROWN theorem are highlighted in brown in the following theorem.

**Theorem 3.2**.: _[BaB Inferred Cuts Bound Propagation]. Given any BaB split set \(\), optimizable parameters \(0_{j}^{(i)} 1\) and \(,, 0,\,_{j}^{(i)}{}^{}\) is a function of \(_{:,j}^{(i)}\):_

\[g(,,,)=-\| ^{(1)}^{(1)}_{0}\|_{1}-_{i=1} ^{L}^{(i)}^{(i)}-^{} +_{i=1}^{L-1}_{j^{(i)}}h_{j}^{(i)}( )\]

_where variables \(^{(i)}\) are obtained by propagating \(^{(L)}=-1\) throughout all \(i[L-1]\):_

\[_{j}^{(i)} =^{(i+1)}_{:,j}^{(i+1)}- {}^{}(_{:,j}^{(i)}+_{:,j}^{(i)}),\,\,\,j ^{+(i)}\] \[_{j}^{(i)} =^{(i+1)}_{:,j}^{(i+1)}-^{}(H_{:,j}^{(i )}+_{:,j}^{(i)})+_{j}^{(i)},\,\,\,j^{+(i)},\] \[_{j}^{(i)} =-^{}_{:,j}^{(i)},\,\,\,j ^{-(i)},\] \[_{j}^{(i)} =-^{}_{:,j}^{(i)}-_{j}^{(i)},\,\,\,j ^{-(i)},\] \[_{j}^{(i)} =_{j}^{(i)}{}^{}+_{j}^{(i)}[_{j}^{(i)}]_{- }-^{}_{:,j}^{(i)},\,\,\,j^{ (i)}^{(i)}\]

_Here, \(_{j}^{(i)}\), \(_{j}^{(i)}{}^{}\) and \(_{j}^{(i)}\) are defined for each initially unstable neuron that has not been split: \(j^{(i)}^{(i)}\), and \(h_{j}^{(i)}()\) is defined for all unstable neurons \(j^{(i)}\)._

\[[_{j}^{(i)}]_{+} :=(^{(i+1)}_{:,j}^{(i+1)}- ^{}_{:,j}^{(i)},0),\,\,\,[_{j}^ {(i)}]_{-}:=(^{(i+1)}_{:,j}^{(i+1)}- ^{}_{:,j}^{(i)},0)\] \[_{j}^{(i)}{}^{} :=((^{(i)}[_{j}^{(i)}]_{+} -^{}_{:,j}^{(i)}}{u_{j}^{(i)}-l_{j}^{(In a known UNSAT subproblem with multiple branched neurons, it is possible that a _subset_ of the branching neurons is sufficient to prove UNSAT, shown in Fig. 1(b). By focusing on this subset, we can strengthen the BaB inferred cuts by reducing the number of \(\) variables involved. From an optimization perspective, each cut corresponds to a hyperplane that separates feasible solutions from infeasible ones. Simplifying the cut by involving fewer \(\) variables reduces the dimensionality of the hyperplane, making it more effective at excluding infeasible regions without unnecessarily restricting the feasible space. Thus, we can draw a corollary:

**Corollary 3.3**.: _Formally, consider the two cuts from Eq. (3):_

\[_{i_{1}^{+}}z_{i}-_{i_{1}^{-}}z_{i}| _{1}^{+}|-1 \]

\[_{i_{2}^{+}}z_{i}-_{i_{2}^{-}}z_{i}| _{2}^{+}|-1 \]

_If either \(_{1}^{+}_{2}^{+}\) and \(_{1}^{-}_{2}^{-}\), or \(_{1}^{+}_{2}^{+}\) and \(_{1}^{-}_{2}^{-}\), then any assignment \(\{z_{i}\}\) satisfying the cut associated with the smaller split set \(\) also satisfies the other cut. This implies that the cut with the smaller set is strictly stronger than the one with the larger set._

Proof deferred to Appendix B.3. This demonstrates that cuts with fewer variables can be more powerful because they operate in lower-dimensional spaces, allowing the hyperplane to more effectively position itself to exclude infeasible regions.

Now, the challenge is determining which unnecessary branches to remove to produce a cut with as few \(z\) variables as possible. An important observation from Theorem 3.2 is that the dual variables \(\) and \(\), associated with the fixed ReLU indicators \(z^{+(i)}\) and \(z^{-(i)}\) respectively, reflect the impact of these fixed activations on the bound. Specifically, a positive dual variable \(>0\) for some \(z^{+(i)}\) indicates that the fixed active state of these neurons contributes significantly to tightening the bound. Similarly, a positive \(>0\) for some \(z^{-(i)}\) signifies that the fixed inactive neurons are influential in optimizing the bound. However, neurons with zero dual variables (\(=0\) or \(=0\)) may not contribute to the current bound optimization. This observation suggests that these neurons might be candidates for removal to simplify the cut. But it's important to note that a zero dual variable does not guarantee that the corresponding neuron has no impact under all circumstances--it only indicates no contribution in the current optimization context. Simply removing all neurons with zero dual

Figure 2: (1(a)): Inferred cut from UNSAT paths during BaB and why it fails in regular BaB. (1(b)): Constraint strengthening with Neuron Elimination Heuristic. (1(c)): Multi-tree search.

variables might overlook their potential influence in other parts of the search space or under different parameter settings. Therefore, the challenge lies in deciding which neurons with zero or negligible dual variables can be safely removed without significantly weakening the cut. This decision requires a careful heuristic that considers not only the current values of the dual variables but also the overall structure of the problem and the potential future impact of these neurons. By intelligently selecting which \(z\) variables to exclude, we aim to produce a stronger cut that is both effective in pruning the search space and efficient in computing. To do this, we use a heuristic to determine whether each neuron should be tentatively dropped from the list of constraints. Algorithm 1 shows the constraint strengthening with the neuron elimination heuristic.

**Neuron Elimination Heuristic for Constraint Strengthening** First, we compute a heuristic influence score for each neuron to assess its impact on the verification objective. This score is based on the improvement in the lower bound of \(f^{}\) before and after introducing the neuron's split in the BaB tree. By recording the computed lower bounds at each node, we can measure how beneficial each constraint is to the verification process. Second, we rank the neurons according to their influence scores and tentatively drop those that contribute least to improving the lower bound. We will retain neurons whose corresponding Lagrange multipliers, \(\) or \(\), are greater than zero. For neurons with \(\) or \(\) equal to zero, we remove a certain percentile of neurons with the lowest scores, as indicated by the "drop_percentage" parameter in Algorithm 1. When a neuron is dropped, its split is canceled. Then, we perform a re-verification step using only the reduced subset of constraints. This involves recomputing the lower bound on \(f^{}\) based solely on the selected constraints. If the verification still succeeds--that is, the lower bound remains non-negative--the reduced set induces a new cutting plane. This new cutting plane is applied to all subproblems within the BaB process after further strengthening. Finally, we can iteratively repeat the process of reducing the constraint set, aiming to generate even stronger cuts. The process terminates when the property can no longer be verified with the current subset of constraints. This iterative refinement is designed to focus on the most influential neurons, potentially enhancing the efficiency of the verification. Once the new cuts determined, we merge pairs of cuts if possible (e.g. merging \(z_{1}+z_{2} 0\) and \(z_{1}-z_{2} 0\) to \(z_{1} 0\)).

```
1:\(f\): model; \(\): lower bound; \(\): Domain \(}:\); \(,:\) set for \(}_{-}\) and \(\) set for \(}_{+}\)\(_{}:\); drop_percentage : Percentage of splits to be dropped
2:neuron_influence_scores \(\) Neuron_Elimination_Heuristic(\(,\))
3:score_threshold \(\) Percentile(neuron_influence_scores, drop_percentage)
4:\(}=\)
5:for\(i[]\)do
6:if\(}_{i} 0\) or neuron_influence_scores\({}_{i}\)then
7:\(_{}}}}_{i}\)
8:\(_{}}(f, _{}_{})\)
9:if\(_{}} 0\)then
10:\(}(_{})\)
11:\(_{}_{}\{\}\)
12:\(_{}(f,,,_{},_{},)\)
13:\(_{}(_{})\)
14:return\(_{}\)
```

**Algorithm 1** Constraint Strengthening

**Multi-Tree Search** Traditionally, the BaB process generates a single search tree. We propose augmenting this approach by performing multiple BaB processes in parallel as a presolving step, with each process exploring a different set of branching decisions. At each branching point, we initialize multiple trees and apply various branching decisions simultaneously. While this initially increases the number of subproblems, the cutting planes generated in one tree are universally valid and can be applied to all other trees. These newly introduced cuts can help prove UNSAT for nodes in other trees, thereby inducing additional cutting planes and amplifying the pruning effect across the entire search space, illustrated in Fig. 1(c).

Since computational resources must be allocated across multiple trees, we prioritize nodes for further expansion that have the highest lower bound on the optimization objective. This strategy ensures that more promising trees receive more computational resources. After a predefined short timeout, we consolidate our efforts by pruning all but one of the trees and proceed with the standard BaB process augmented with BICCOS on the selected tree. We choose the tree that has been expanded the most frequently, as this indicates that its bounds are closest to verifying the property.

BaB Tree Searching StrategyIn the standard BaB process (e.g., in \(\)-CROWN), branches operate independently without sharing information, so the order in which they are explored does not affect the overall runtime. For memory access efficiency, there is a slight preference for implementing BaB as a depth-first search (DFS), where constraints are added until unsatisfiability (UNSAT) can be proven . This approach focuses the search on deeper branches before returning to shallower nodes.

However, in the context of BICCOS, our objective is to generate strong cutting planes that can prune numerous branches across different subproblems. To maximize the generality of these cutting planes, they need to be derived from UNSAT nodes with as few constraints as possible. While constraint strengthening techniques can simplify the constraints, this process is more straightforward when the original UNSAT node already has a minimal set of constraints. Even if only a few constraints are eliminated, the resulting cutting plane can significantly impact many other subproblems. To facilitate this, we propose performing the BaB algorithm using a breadth-first search (BFS) strategy. By exploring nodes that are closest to the root and have the fewest neuron constraints, we can generate more general and impactful cutting planes earlier in the search process.

```
1:\(_{},f(f,)\)
2:\(_{},_{}(f,)\)
3:while\(|_{}|>0\) and not timed out do
4:\((_{1},,_{n})_{BFS} (_{},n)\)
5:\((_{1}^{-},_{1}^{+},,_{n}^{-},_{n}^{+})(_{1},,_{n})\)
6:\((_{_{1}^{-}},_{_{1}^{+}}, ,_{_{n}^{-}},_{_{n}^{+}}) (f,_{},_{1}^{-}, _{1}^{+},,_{n}^{-},_{n}^{+})\)
7:\(_{}_{}([ _{_{1}^{-}},_{1}^{-}],[_{ _{1}^{+}},_{1}^{+}],,[_{_ {n}^{+}},_{n}^{+}])\)
8:for all \(_{i}_{}\)do
9:\(_{}(f,_{i},_{i},)\)
10:\(_{}_{} _{}([_{_{1}^{-}}, _{1}^{-}],[_{_{1}^{+}},_{1}^{+} ],,[_{_{n}^{+}},_{n}^{+}])\)
11:return UNSAT if \(|_{}|=0\) else Unknown
```

**Algorithm 2** Branch-and-bound Inferred Cuts with Constraint Strengthening (BICCOS).

### BICCOS Summary

Algorithm 2 summarizes our proposed BICCOS algorithm, with the modifications to the standard BaB algorithm highlighted in brown. First (line 2), instead of exploring a single tree, we explore multiple trees in parallel as a presolving step. This process may involve constraint strengthening and utilizes cut inference analogous to the procedures in lines 3-15 of the algorithm. After several iterations, we prune all but one of the trees. From this point forward, only the selected tree is expanded further, following the regular BaB approach.

Until all subdomains of this tree have been verified, BICCOS selects batches of unverified subdomains add additional branching decisions, and attempt to prove the verification property. Unlike regular BaB, it then applies constraint strengthening to all identified UNSAT nodes and infers the corresponding cutting planes. These cutting planes are added to all currently unverified subdomains, potentially improving their lower bounds enough to complete the verification process. If BICCOS fails to identify helpful cutting planes, it effectively behaves like the regular BaB algorithm. We have implemented BICCOS in the \(\),\(\)-CROWN toolbox. Notably, the cuts found by BICCOS are compatible with those from MIP solvers in GCP-CROWN, and all cuts can be combined in cases where MIP cuts are beneficial.

## 4 Experiments

We evaluate our verifier, BICCOS, on several popular verification benchmarks from VNN-COMP  and on the SDP-FO benchmarks used in multiple studies . In the following discussion, \(\),\(\)-CROWN refers to the verification tool that implements various verification techniques, while \(\)-CROWN and GCP-CROWN denote specific algorithms implemented within \(\),\(\)-CROWN. To ensure the comparability of our method's effects and to minimize the influence of hardware and equipment advances, we rerun \(\)-CROWN and GCP-CROWN with MIP cuts for each experiment. Additionally, we use the same BaB algorithm as in \(\)-CROWN and employ filtered smart branching (FSB)  as the branching heuristic in all experiments. Note that in experiments GCP-CROWN refers to GCP-CROWN solver with MIP cuts. We also conduct ablation studies to identify which components of BICCOS contribute the most, including analyses of verification accuracy & time, number of cuts generated, and number of domains visited. Experimental settings are described in Appendix C.1.

**Results on VNN-COMP benchmarks** We first evaluate BICCOS on many challenging benchmarks with large models, including two VNN-COMP 2024 benchmarks: cifar100-2024 and tinyimagenet-2024; two VNN-COMP 2022 benchmarks: cifar100-tinyimagenet-2022 and oval22. Shown in Table 1, our proposed method, BICCOS, outperforms most other verifiers on the tested benchmarks, achieving the highest number of verified instances in four benchmark sets. BICCOS consistently outperforms the baseline \(,\)-CROWN verifier (the \(\)-CROWN and GCP-CROWN (MIP cuts) lines), verifying more instances across almost all benchmark sets. In particular, GCP-CROWN with MIP cuts cannot scale to the larger network architectures in the cifar100 and tinyimagenet benchmarks with network sizes between 14.4 and 31.6 million parameters, due to its reliance on an MIP solver. BICCOS, on the other hand, can infer cutting planes without the need for an MIP solver and noticeably outperforms the baseline on cifar100 and tinyimagenet. Note that the increase in average runtime (e.g., on the cifar100-tinyimagenet-2022 benchmark) is expected. The instances that could not be verified at all previously but can be verified using BICCOS tend to require runtimes that are below the timeout but above the baseline's average runtime.

**Results on SDP-FO benchmarks** We further evaluated BICCOS, on the challenging SDP-FO benchmarks introduced in previous studies . These benchmarks consist of seven predominantly adversarial trained MNIST and CIFAR models, each containing numerous instances that are difficult for many existing verifiers. Our results, detailed in Table 2, demonstrate that BICCOS significantly improves verified accuracy across all the tested models when compared to current state-of-the-art verifiers. On both MNIST and CIFAR dataset, BICCOS not only surpasses the performance of methods like \(\)-CROWN and GCP-CROWN on the CNN-A-Adv model but also approaches the empirical robust accuracy upper bound, leaving only a marginal gap. A slight increase in average time in some cases is attributed to the higher number of solved instances.

**Ablation Studies on BICCOS Components.** To evaluate the contributions of individual components of BICCOS, we performed ablation studies summarized in Table 3. The BICCOS base version with BaB inferred cuts and constraint strengthening already shows competitive performance in many models. Cuts from MIP solvers are compatible with BICCOS and can be added for smaller models that can be handled by MIP solver. Integrating Multi-Tree Search (MTS) significantly boosts performance. On the CIFAR CNN-B-Adv model, verified accuracy rises to 54.5%, outperforming

   Dataset & Model & PHMA  & GCROWN  & MN-BaB  & VNN20.9, 34 & GCP-CROWN (MIP cuts)  & BICCOS & Upper \\ \( 0.34\) & \( 2/255\) & Net-5 & Net-5 & Net-5 & Net-5 & Net-5 & Net-5 & Net-5 & Bound \\  MNIST & CNN-A-Adv & 44.5 & 135.9 & 71.0 & 3.53 & - & - & 35.5 & 148.4 & 70.5 & 7.34 & **75.5** & 13.37 & 76.5 \\   & CNN-A-Adv & 41.5 & 4.8 & 45.3 & 5.17 & 42.5 & 68.3 & 47.5 & 20.0 & 48.5 & 4.78 & **49.0** & 8.94 & 50.0 \\  & CNN-A-Adv & 45.0 & 4.9 & 4.65 & 0.78 & 46.0 & 37.7 & 47.5 & 13.1 & 48.0 & 1.47 & **48.5** & 1.84 & 49.5 \\ CIFAR & CNN-A-Adv & 37.5 & 34.3 & 42.5 & 4.78 & 35.0 & 140.3 & 33.5 & 72.4 & 47.5 & 9.70 & **48.5** & 10.34 & 53.0 \\  & CNN-A-Adv & 48.5 & 7.0 & 51.0 & 47.9 & 40.9 & 70.9 & 47.3 & 54.5 & 3.02 & **56.0** & 52.3 & 57.5 \\  & CNN-A-Adv & 30.0 & 34.6 & 47.5 & 6.39 & - & - & - & 49.0 & 10.07 & **54.5** & 17.75 & 65.0 \\  & CNN-B-Adv & 35.5 & 43.8 & 56.0 & 3.20 & - & - & - & 58.5 & 9.33 & **62.0** & 8.27 & 61.5 \\   

* MN-BaB with 60% timeout threshold for all models. “-” indicates that we could not run a model due to unexpected model structure or other errors. We run \(\)-CROWN, GC-CROWN with MIP cuts and BHCCOS with a shorter 2022 benchmark set at models. The increased number for McN-BaB may increase the percentage of verified instances. However, we can still achieve better verified accuracy than all other baselines. Other results are reported from .

Table 2: Verifier accuracy (Ver.%) and avg. per-example verification time (s) on 7 models from .

    &  &  &  \\  Method & time(s) & \# verified & time(s) & \# verified & time(s) & \# verified & time(s) & \# verified \\   & 630.06 & 3 & - & - & - & - & - & - & - \\  & 429.13 & 5 & 186.11 & 27 & - & 0 & - & 0 \\  & 233.84 & 6 & - & - & - & - & - & - \\  & 393.14 & 11 & - & - & - & - & - & - \\  & 386.71 & 17 & - & - & - & - & - & - \\  & 73.65 & 17 & 39.43 & 69 & - & - & - & - \\  & 137.13 & 19 & 40.27 & 36 & - & - & - & - \\  & - & - & - & - & 42.38 & 68 & 55.64 & 49 \\  & 23.26 & 20 & 11.95 & 69 & 15.48 & 119 & 28.87 & 135 \\  & 32.12 & 25 & 18.42 & 69 & 19.32 & 119 & 31.60 & 134 \\  & 59.84 & **26** & 13.38 & **72** & 13.58 & **125** & 16.33 & **140** \\   & 27 & & 94 & & 168 & & 157 \\   

* Results from VNN-COMP 2021 report . † Results from VNN-COMP 2022 report  ‡ Results from VNN-COMP 2022 report  † Results from VNN-COMP 2024 website.

Table 1: Comparison of different toolkits and BICCOS on VNN-COMP benchmarks. Results on non-CROWN or BICCOS were run on different hardware. “-” indicates that the benchmark was not supported.

GCP-CROWN with MIP cuts's 49%. We also design an adaptive BICCOS configuration (BICCOS auto), which automatically turns on MTS and/or MIP-based cuts according to neural network and verification problem size and quantity, achieves the highest verified accuracies across most of models and is used as the default option of the verifier when BICCOS is enabled. A detailed table with the numbers of cuts and domains visited is provided in Appendix C.2.

## 5 Related work

Our work is based on the branch and bound framework for neural network verification [12; 17; 59; 28; 54; 16; 37; 21], which is one of the most popular approaches that lead to state-of-the-art results [4; 10; 40]. Most BaB-based approaches do not consider the correlations among subproblems - for example, in \(\)-CROWN , the order of visiting the nodes in the BaB search tree does not change the verification outcome as the number of leaf nodes will be the same regardless of how the leaves are split. Our work utilizes information on the search tree and can gather more effective cuts when shallower nodes are visited first.

Exploring the dependency or correlations among neurons has also been identified as a potential avenue to enhance verification bounds. While several studies have investigated this aspect [2; 16; 42; 50], their focus has primarily been on improving the bounding step without explicitly utilizing the relationships among ReLUs during the branching process. Venus  considers the implications among neurons with constraints similar to our cutting planes. However, their constraints were not discovered using the verified subproblems during BaB or multi-tree search, and cannot be strengthened. On the other hand, cutting plane methods encode dependency among neurons as general constraints [61; 35], and our work developed a new cutting plane that can be efficiently constructed and strengthened during BaB, the first time in literature.

In addition, some NN verifiers are based on the satisfiability modulo theories (SMT) formulations [32; 46; 19; 36], which may internally use an SAT-solving procedure  such as DPLL  or CDCL . These procedures may discover conflicts in boolean variable assignments, corresponding to eliminating certain subproblems in BaB. However, they differ from BICCOS in two significant aspects: first, although DPLL or CDCL may discover constraints to prevent some branches with neurons involved in these constraints, they cannot efficiently use these constraints as cutting planes that may tighten the bounds for subproblems never involving these neurons; second, DPLL or CDCL works on the abstract problem where each ReLU is represented as a boolean variable, and cannot take full advantage of the underlying bound propagation solver to strengthen constraints as we did in Alg. 1. Based on our observation in Sec. 3, the constraints discovered during BaB are often unhelpful without strengthening unless in a different search tree, so their effectiveness is limited. However, the learned conflicts can be naturally translated into cuts 3, making this a future work.

More related works on SMT, MIP solvers, nogood learning and cutting plane method in VNN [18; 45; 26; 19; 11; 2; 47; 52; 30; 61; 35] are discussed in Appendix D.

## 6 Conclusion

We exploit the structure of the NN verification problem to generate efficient and scalable cutting planes, leveraging neuron relationships within verified subproblems in a branch-and-bound search tree. Our experimental results demonstrate that the proposed BICCOS algorithm achieves very good scalability while outperforming many other tools in the VNN-COMP, and can solve benchmarks that existing methods utilizing cutting planes could not scale to. Limitations are discussed in Appendix E.

   Dataset & Model & \(\)-CROWN  & GCP-CROWNMIP cuts  & BICCOS (base) & BICCOS (with MTS) & BICCOS (auto) & Upper \\ \(=0.3\) and \(=2/25\) & Verc. Size & Time (s) & Verc. Size & Verc. Size & Time(s) & Verc. Size & Time(s) & Verc. Size & Time(s) & bound \\  MNIST & CNN-A.Adv & 71.0 & 3.53 & 70.5 & 7.34 & **76.5** & 5.61 & **76.5** & 8.86 & 75.5 & 13.37 & 76.5 \\   & CNN-A.Adv & 45.3 & 5.17 & 48.5 & 4.75 & 4.88 & 4.73 & 5.01 & **49.0** & 4.26 & 50.0 \\  & CNN-A.Adv & 46.5 & 0.78 & 48.0 & 1.47 & 48.0 & 1.27 & 47.5 & 11.58 & **48.5** & 1.81 & 49.5 \\ CIFAR & CNN-A.Mix & 42.5 & 4.78 & 47.5 & 9.70 & 47.0 & 6.68 & 47.0 & 7.87 & **48.5** & 10.31 & 53.0 \\  & CNN-A.Mix & 51.0 & 47.9 & 54.5 & 3.82 & 55.0 & 6.96 & 54.0 & 2.87 & **56.0** & 5.23 & 57.5 \\  & CNN-B.Adv & 47.5 & 6.39 & 49.0 & 10.07 & 52.0 & 8.14 & 52.5 & 10.13 & **54.5** & 17.75 & 65.0 \\  & CNN-B.Adv & 56.0 & 3.20 & 58.8 & 8.27 & 60.0 & 3.18 & 60.5 & 4.38 & **62.0** & 9.63 & 63.5 \\  over22 & 66.67 & 23.26 & 83.33 & 32.12 & 73.33 & 18.75 & 70.00 & 17.23 & **86.66** & 59.84 & 90.0 \\  cifar100-3024 & 59.5 & 18.48 & 59.5 & 19.33 & **62.5** & 12.74 & 61.5 & 12.18 & **62.5** & 13.57 & 84.0 \\  tinyimpact-3024 & 67.5 & 28.87 & 67.0 & 31.60 & **70.0** & 1.384 & **70.0** & 17.8 & **70.0** & 16.22 & 78.5 \\   

* We run our BICCOS in different ablation studies with a shorter 30% timeout for all models and compare it to \(\)-CROWN and GCP-CROWN, it achieves better verified accuracy than all other baselines.

Table 3: Ablation Studies on Verified accuracy (Var.%), avg. per-example verification time (s) analysis for all method verified instances on different BICCOS components.

AcknowledgmentHuan Zhang is supported in part by the AI2050 program at Schmidt Sciences (AI2050 Early Career Fellowship) and NSF (IIS-2331967). Grani A. Hanasusanto is supported in part by NSF (CCF-2343869 and ECCS-2404413). Computations were performed with computing resources granted by RWTH Aachen University under project rwth1665. We thank the anonymous reviewers for helping us improve this work.