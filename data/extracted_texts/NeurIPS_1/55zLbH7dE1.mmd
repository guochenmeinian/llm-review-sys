# Graph-enhanced Optimizers for Structure-aware Recommendation Embedding Evolution

Cong Xu\({}^{}\) Jun Wang\({}^{}\) Jianyong Wang \({}^{@sectionsign}\) Wei Zhang\({}^{}\)

\({}^{}\)East China Normal University \({}^{@sectionsign}\)Tsinghua University

\({}^{}\){congxueric, wongjun, zhangwei.thu2011}@gmail.com \({}^{@sectionsign}\)jianyong@tsinghua.edu.cn

Corresponding author. This work was supported in part by National Natural Science Foundation of China (No. 92270119 and No. 62072182) and Shanghai Institute of Artificial Intelligence for Education.

###### Abstract

Embedding plays a key role in modern recommender systems because they are virtual representations of real-world entities and the foundation for subsequent decision-making models. In this paper, we propose a novel embedding update mechanism, Structure-aware Embedding Evolution (SEvo for short), to encourage related nodes to evolve similarly at each step. Unlike GNN (Graph Neural Network) that typically serves as an intermediate module, SEvo is able to directly inject graph structural information into embedding with minimal computational overhead during training. The convergence properties of SEvo along with its potential variants are theoretically analyzed to justify the validity of the designs. Moreover, SEvo can be seamlessly integrated into existing optimizers for state-of-the-art performance. Particularly SEvo-enhanced AdamW with moment estimate correction demonstrates consistent improvements across a spectrum of models and datasets, suggesting a novel technical route to effectively utilize graph structural information beyond explicit GNN modules. Our code is available at [https://github.com/MTandHJ/SEvo](https://github.com/MTandHJ/SEvo).

## 1 Introduction

Surfing Internet leaves footprints such as clicks , browsing , and shopping histories . For a modern recommender system , the entities involved (_e.g._, goods, movies) are typically embedded into a latent space based on these interaction data. As the embedding takes the most to construct and is the foundation to subsequent decision-making models, its modeling quality directly determines the final performance of the entire system. According to the homophily assumption , it is natural to expect that related entities have closer representations in the latent space. Note that the similarity between two entities refers specifically to those extracted from interaction data  or prior knowledge . For example, goods selected consecutively by the same user or movies of the same genre are often perceived as more relevant. Graph neural networks (GNNs)  are a widely adopted technique to exploit such structural information, in concert with a weighted adjacency matrix wherein each entry characterizes how closely two nodes are related. Rather than directly injecting structural information into embedding, GNN typically serves as an intermediate module in the recommender system. However, designing a versatile GNN module suitable for various recommendation scenarios is challenging. This is especially true for sequential recommendation , which needs to take into account both structural and sequential information at the same time. Moreover, the post-processing fashion inevitably increases the overhead of training and inference, limiting the scalability for real-time recommendation.

In this work, we aim to directly inject graph structural information into embedding through a novel embedding update mechanism. Figure 1 (a) illustrates a normal embedding evolution process, inwhich the embedding \(\) is updated at step \(t\) as follows:

\[_{t}_{t-1}+_{t-1}. \]

Note that the variation \(\) is primarily determined by the (anti-)gradient. It points to a region able to decrease a loss function concerning recommendation performance , but lacks an explicit mechanism to ensure that the variations between related nodes are similar. The embeddings thus cannot be expected to capture model pairwise relations while minimizing the recommendation loss.

Conversely, structural information can be effectively learned if related nodes evolve similarly at each update. The structure-aware embedding evolution (SEvo), depicted in Figure 1 (b), is developed for this goal. A special transformation is applied so as to meet both smoothness and convergence . Given that these two criteria inherently conflict to some extent, we resort to a graph regularization framework  to balance them. While analogous frameworks have been used to understand feature smoothing and modify message passing mechanisms for GNNs , applying this to variations is not as straightforward as to features  or labels . Previous efforts are capable of smoothing, but cannot account for strict convergence. The specific transformation form must be chosen carefully; subtle differences may slow down or even kill training. Through comprehensive theoretical analysis, we develop an applicable solution with a provable convergence guarantee.

Apart from the anti-gradient, the variation \(\) can also be derived from the moment estimates. Therefore, existing optimizers, such as SGD  and Adam , can benefit from SEvo readily. In contrast to Adam, AdamW  decouples the weight decay from the optimization step, making it more compatible with SEvo as it is unnecessary to smooth the weight decay as well. Furthermore, we recorcet the moment estimates of SEvo-enhanced AdamW when encountering sparse gradients. This modification enhances its robustness across a variety of recommendation models and scenarios. Extensive experiments over six public datasets have demonstrated that it can effectively inject structural information to boost recommendation performance. It is important to note that SEvo does not alter the inference logic of the model, so the inference time is exactly the same and very little computational overhead is required during training.

The main contributions of this paper can be summarized as follows. **1)** The graph regularization framework  has been widely used for feature/label smoothing. To the best of our knowledge, we are the first to apply it to variations as an alternative to explicit GNN modules for recommendation. **2)** The final formulation of SEvo is non-trivial (previous iterative  or Neumann series  approximation methods proved to be incompatible in this case) and comes from comprehensive theoretical analyses. **3)** We further present SEvo-enhanced AdamW by integrating SEvo and reconrecting the original moment estimates. These modifications demonstrate consistent performance, yielding average 9%\(\)23% improvements across a spectrum of models. For larger-scale datasets containing millions of nodes, the performance gains can be as high as 40%\(\)139%. **4)** Beyond interaction

Figure 1: Overview of SEvo. (a) Normal embedding evolution. (b) (Section 2) Structure-aware embedding evolution. (c) (Section 2.2) Geometric visualization of the variation from \(\) to \(^{*}()\). The gray ellipse represents the region with proper smoothness. (d) (Section 2.3) The \(L\)-layer approximation with a faster convergence guarantee.

data, we preliminarily explore the pairwise similarity estimation based on other prior knowledge: node categories to promote intra-class representation proximity, and knowledge distillation  to encourage a light-weight student to mimic the embedding behaviors of a teacher model.

## 2 Structure-aware Embedding Evolution

In this section, we first introduce some necessary terminology and concepts, in particular smoothness. SEvo and its theoretical analyses will be detailed in Section 2.2 and 2.3. The proofs hereafter are deferred to Appendix A.

### Preliminaries

**Notations and terminology.** Let \(=\{v_{1},,v_{n}\}\) denote a set of nodes and \(=[w_{ij}]^{n n}\) a _symmetric_ adjacency matrix, where each entry \(w_{ij}=w_{ji}\) characterizes how closely \(v_{i}\) and \(v_{j}\) are related. They jointly constitute the graph \(=(,)\). For example, \(\) could be a set of movies, and \(w_{ij}\) is the frequency of \(v_{i}\) and \(v_{j}\) being liked by the same user. Denoted by \(^{n n}\) the diagonal degree matrix of \(\), the normalized adjacency matrix and the corresponding Laplacian matrix are defined as \(}=^{-1/2}^{-1/2}\) and \(}=-}\), respectively. For ease of notation, we use \(,\) to denote the inner product, \(,=^{T}\) for vectors and \(,=(^{T})\) for matrices. Here, \(()\) denotes the trace of a given matrix.

**Smoothness.** Before delving into the details of SEvo, it is necessary to present a metric to measure the smoothness [57; 7] of node features \(\) as a whole. Denoted by \(_{i},_{j}\) the row vectors of \(\) and \(d_{i}=_{j}w_{ij}\), we have

\[_{smoothness}(;):=(^{T} })=_{i,j}w_{ij}\|_{i}}{}}-_{j}}{}}\|_{2}^ {2}. \]

This term is often used as graph regularization for feature/label smoothing [24; 19]. A lower \(_{smoothness}\) indicates smaller difference between closely related pairs of nodes, and in this case \(\) is considered smoother. However, smoothness alone is not sufficient from a performance perspective. Over-emphasizing this indicator instead leads to the well-known over-smoothing issue . How to balance variation smoothness and convergence is the main challenge to be addressed below.

### Methodology

The entities involved in a recommender system are typically embedded into a latent space [6; 12], and the embeddings in \(^{n d}\) are expected to be smooth so that related nodes are close to each other. As discussed above, \(\) is learnable and updated at step \(t\) by Eq. (1), where the variation \(\) is mainly determined by the (anti-)gradient. For example, \(=-_{}\) when gradient descent with a learning rate of \(\) is used to minimize a loss function \(\). However, the final embeddings based on this evolution process may be far from sufficient smoothness because: 1) The variation \(\) points to the region able to decrease the loss function concerning recommendation performance, but lacks an explicit smoothness guarantee. 2) As numerous item embeddings (millions of nodes in practice) to be trained together for a recommender system, the variations of two related nodes may be quite different due to the randomness from initialization and mini-batch sampling.

We are to design a special transformation \(()\) to smooth the variation so that the evolution deduced from the following update formula is structure-aware,

\[_{t}_{t-1}+(_{t-1}). \]

Recall that, in this paper, the similarity is confined to quantifiable values in the adjacency matrix \(\), in which more related pairs are weighted higher. Therefore, this transformation should encourage pairs of nodes connected with higher weights to evolve more similarly than those connected with lower weights. This can be boiled down to structure-aware transformation as defined below.

**Definition 1** (Structure-aware transformation).: _The transformation \(()\) is structure-aware if_

\[_{smoothness}(())_{ smoothness}(). \]On the other hand, the transformation must ensure convergence throughout the evolution process, which means that the transformed variation should not differ too much from the original. For the sake of theoretical analysis, the ability to maintain the update direction will be used to _qualitatively_ depict this desirable property below, though a quantitative squared error will be employed later.

**Definition 2** (Direction-aware transformation).: _The transformation \(()\) is direction-aware if_

\[(),>0, \,. \]

These two criteria inherently conflict to some extent. We resort to a hyperparameter \([0,1)\) to make a trade-off and the desired transformation is the corresponding minimum; that is,

\[^{*}(;)=}\;(1-)\; \|-\|_{F}^{2}+(^{T}}). \]

A larger \(\) indicates a stronger smoothness constraint and \(^{*}()\) reduces to \(\) when \( 0\). Geometrically, as shown in Figure 1 (c), \(^{*}()\) can be interpreted as a projection of \(\) onto the region with proper smoothness. Taking the gradient to zero could give a closed-form solution, but it requires prohibitive arithmetic operations and memory overhead, which is particularly time-consuming in recommendation due to the large number of nodes. Zhou et al.  suggested a \(L\)-layer _iterative_ approximation to circumvent this problem (with \(_{0}()=\)):

\[_{iter}():=_{L}(), _{l}()=}_{l-1}( )+(1-).\]

The resulting transformation is essentially a momentum update that aggregates higher-order information layer by layer. Analogous message-passing mechanisms have been used in previous GNNs such as APPNP  and C&S . However, this commonly used approximate solution is incompatible with SEvo; sometimes, variations after the transformation may be opposite to the original direction, resulting in a failure to converge.

**Theorem 1**.: _The iterative approximation is direction-aware for all possible normalized adjacency matrices and \(L 0\), if and only if \(<1/2\). In contrast, the Neumann series approximation \(_{nsa}()=(1-)_{l=0}^{L}^{l}}^{l}\) is structure-aware and direction-aware for any \([0,1)\)._

As suggested in Theorem 1, a feasible compromise for \(_{iter}\) is to restrict \(\) to \([0,1/2)\), but this may cause a lack of smoothness. The Neumann series approximation  appears to be a viable alternative as it qualitatively satisfies both desirable properties. Nonetheless, this transformation can be further improved for a faster convergence rate based on the analysis presented next.

### Convergence Analysis for Further Modification

In general, the recommender system has some additional parameters \(^{m}\) except for embedding to be trained. Therefore, we analyze the convergence rate of the following gradient descent strategy:

\[_{t+1}_{t}-_{nsa}_{ }(_{t},_{t}), _{t+1}_{t}-^{} _{}(_{t},_{t}),\]

wherein SEvo is performed on the embedding and a normal gradient descent is applied to \(\). To make the analysis feasible, some mild assumptions on the loss function should be given: \(:^{n d}^{m}\) is a twice continuously differentiable function whose first derivative is Lipschitz continuous for some constant \(C\). Then, we obtain the following properties.

**Theorem 2** (Informal).: _If \(=^{}=1/C\), the convergence rate after \(T\) updates is \((C/((1-)^{2}T))\). If we adopt a modified learning rate of \(=)C}\) for embedding, the convergence rate could be improved to \((C/((1-)T))\)._

**Remark 1**.: _Our main interest is to justify the designs of SEvo rather than to pursue a particular convergence rate, so some mild assumptions suggested in  are adopted here. By introducing the steepest descent for quadratic norms , better convergence can be obtained with stronger assumptions._

Two conclusions can be drawn from Theorem 2. **1)** The theoretical upper bound becomes worse when \( 1\). This makes sense since \(_{nsa}(_{})\) is getting smoother and further away from the original descent direction. **2)** A modified learning rate for embedding can significantly improve the convergence rate. This phenomenon can be understood readily if we notice the fact that

\[\|_{nsa}()\|_{F}(1-^{L+1})\|\|_ {F}.\]

Thus, the modified learning rate is indeed to offset the _scaling effect_ induced by SEvo. In view of this, we directly incorporate this factor into SEvo to avoid getting stuck in the learning rate search, yielding the final desired transformation:

\[(;)=}_{l=0}^{L} ^{l}}^{l}. \]

It can be shown that \(\) is structure-aware and direction-aware, and converges to \(\) as \(L\) increases.

### Integrating SEvo into Existing Optimizers

```
embedding matrix \(\), learning rate \(\), momentum factors \(_{1},_{2},[0,1)\), weight decay \(\). foreachstep \(t\)do \(_{t}_{}\) // Get gradients w.r.t \(\)  Update first/second moment estimates for each node \(i\): \[_{t}[i]\{_{1}_{t-1} [i]+(1-_{1})_{t}[i]&_{t}[i]\\ _{1}_{t-1}[i]+)}{1-_{1}^{t-1}}_ {t-1}[i]&.,\] \[_{t}[i]\{_{2} _{t-1}[i]+(1-_{2})_{t}^{2}[i]&_{t}[i]\\ _{2}_{t-1}[i]+)}{1-_{2}^{t-1}}_ {t-1}[i]&.;\]  Compute bias-corrected first/second moment estimates: \[}_{t}_{t}/(1-_{1}^{t}),}_{t}_{t}/(1-_{2}^{t});\] Update via SEvo: \[_{t}_{t-1}-\;\,}_{t}/}_{t}+};- _{t-1}.\]

Output: optimized embeddings \(\).
```

**Algorithm 1** SEvo-enhanced AdamW. Differences from the original AdamW are colored in blue. The matrix operation below are element-wise.

SEvo can be seamlessly integrated into existing optimizers since the variation involved in Eq. (3) can be extended beyond the (anti-)gradient. For SGD with momentum, the variation becomes the first moment estimate, and for Adam this is jointly determined by the first/second moment estimates. AdamW is also widely adopted for training recommenders. Unlike Adam whose moment estimate is a mixture of gradient and weight decay, AdamW decouples the weight decay from the optimization step, which is preferable since it makes no sense to require the weight decay to be smooth as well. However, in very rare cases, SEvo-enhanced AdamW fails to work very well. We next try to ascertain the causes and then improve the robustness of SEvo-enhanced AdamW.

Denoted by \(:=_{}^{d}\) the gradient for a node embedding \(\) and \(^{2}:=\) the element-wise square, AdamW estimates the first and second moments at step \(t\) using the following formulas

\[_{t}=_{1}_{t-1}+(1-_{1})_{t-1}, _{t}=_{2}_{t-1}+(1-_{2})_{t-1}^{2},\]

where \(_{1},_{2}\) are two momentum factors. Then the original AdamW updates embeddings by

\[_{t}=_{t-1}-_{t-1}, _{t-1}:=}_{t}/}_{t}}.\]

Note that the bias-corrected estimates \(}_{t}=_{t}/(1-_{1}^{t})\) and \(}_{t}=_{t}/(1-_{2}^{t})\) are employed for numerical stability . In practice, only a fraction of nodes are sampled for training in a mini-batch, so the remaining embeddings have zero gradients. In this case, the sparse gradient problem may introduce some unexpected 'biases' as depicted below.

**Proposition 1**.: _If a node is no longer sampled in subsequent \(p\) batches after step \(t\), we have \(_{t+p-1}=^{t}}{^{t}}} _{t-1}\), and the coefficient of \(\) is mainly determined by \(t\)._

Considering a common case \(_{2}_{1}\), the right-hand side approaches \((_{1}^{p/2})\). The step size for inactive nodes then gets slower and slower during idle periods. This seems reasonable as their moment estimates are becoming outdated; however, this effect sometimes prevents the variation from being smoothed by SEvo. We hypothesize that this is because SEvo itself tends to assign more energy to active nodes and less energy to inactive nodes. So this auto-attenuation effect of the original AdamW is somewhat redundant. Fortunately, there is a feasible modification to make SEvo-enhanced AdamW more robust:

**Theorem 3**.: _Under the same assumptions as in Proposition 1, \(_{t+p-1}=_{t-1}\) if the moment estimates are updated in the following manner when \(_{t}=\),_

\[_{t}=_{1}_{t-1}+(1-_{1})^{t -1}}_{t-1},_{t}=_{2}_{t-1}+(1-_{ 2})^{t-1}}_{t-1}. \]

As can be seen, when sparse gradients are encountered, the approach in Theorem 3 is actually to estimate the current gradient from previous moments. The coefficients \(1/(1-_{1}^{t-1})\) and \(1/(1-_{1}^{t-1})\) are used here for unbiasedness (refer to Appendix A.3 for detailed discussion and proofs). We summarize the SEvo-enhanced AdamW in Algorithm 1 and the modifications for Adam and SGD in Appendix B.1, with an empirical comparison presented in Section 3.3.

The previous discussion lays the technical basis for injecting graph structural information, but the final recommendation performance is determined by how 'accurate' the similarity estimation is. Following other GNN-based sequence models [48; 49], the number of consecutive occurrences across all sequences will be used as the pairwise similarity \(w_{ij}\). In other words, items \(v_{i}\) and \(v_{j}\) that appear consecutively more frequently are assumed more related. Notably, we would like to emphasize that SEvo can readily inject other types of knowledge beyond interaction data. We have made some preliminary efforts in Appendix C and observed some promising results.

## 3 Experiments

In this section, we comprehensively verify the superiority of SEvo. We focus on sequential recommendation for two reasons: 1) This is the most common scenario in practice; 2) Utilizing both sequential and structural information is beneficial yet challenging. We showcase that SEvo is a promising way to achieve this goal. It is worth noting that although technically SEvo can be applied to general graph embedding learning , we contend SEvo-AdamW is especially useful for mitigating the inconsistent embedding evolution caused by data sparsity, while effectively injecting structural information in conjunction with other types of information.

Due to space constraints, this section presents only the primary results concerning accuracy, efficiency, and some empirical evidence that supports the aforementioned claims. We begin by introducing the datasets, evaluation metrics, baselines, and implementation details.

**Datasets.** Six benchmark datasets are considered in this paper. The first four datasets including Beauty, Toys, Tools, and MovieLens-1M are commonly employed in previous studies for empirical comparisons. Additionally, two larger-scale datasets, Clothing and Electronics, are used to assess SEvo's scalability in scenarios involving millions of nodes. Following [22; 13], we filter out users and items with less than 5 interactions, and the validation set and test set are split in a _leave-one-out_ fashion, namely the last interaction for testing and the penultimate one for validation. This splitting allows for fair comparisons, either for sequential recommendation or collaborative filtering. The dataset statistics are presented in Table 1.

**Evaluation metrics.** For each user, the predicted scores over _all items_ will be sorted in descending order to generate top-N candidate lists. We consider two widely-used evaluation metrics, HR@N

   Dataset & \#Users & \#items & \#interactions & Avg. Len. \\  Beauty & 22,363 & 12,101 & 198,502 & 8.9 \\ Toys & 19,412 & 11,924 & 167,597 & 8.6 \\ Tools & 16,638 & 10,217 & 134,476 & 8.1 \\ MovieLens-1M & 6,040 & 3,416 & 999,611 & 165.5 \\  Electronics & 728,489 & 159,729 & 6,737,580 & 9.24 \\ Clothing & 1,219,337 & 376,378 & 11,282,445 & 9.25 \\   

Table 1: Dataset statistics

[MISSING_PAGE_FAIL:7]

Secondly, either dynamic graph construction in SR-GNN and LESSR, or path sampling in MAERec, requires heavy computational overhead, which directly causes the training failures on large-scale datasets like Electronics and Clothing. Even worse, these high costs associated with SR-GNN and LESSR are inevitable during inference. In contrast, SEvo does not alter the model inference logic at all, thereby maintaining consistent inference time. The computational overhead required in training is also negligible compared to previous graph-enhanced models that employ GNNs as intermediate modules. For example, SASRec with SEvo consumes only 10 minutes compared to the hours of training time required for MAERec. When millions of nodes are encountered in Table 3, each epoch demands just a few more seconds. SEvo is arguably superior to these cumbersome GNN modules in real-world applications. Combining Table 2 and Table 3, it can be inferred that the performance gain increases as the number of items increases. This can be explained by the fact that the randomness of sampling leads to a much more inconsistent evolution when more and more nodes are encountered . SEvo thus plays an increasingly important role as it is capable of imposing direct consistency constraints on embeddings.

Since SASRec is a pioneer in the field of sequential recommendation, it will serve as the default backbone for subsequent studies.

### Empirical Analysis

**Convergence comparison.** In Section 2.3, we theoretically verified the necessity of rescaling the original Neumann series approximation for faster convergence. Figure 1(a) shows the loss curves of SASRec trained with AdamW under identical settings other than the form of SEvo. Without rescaling, SASRec exhibits significantly slower convergence, consistent with the conclusion in Theorem 2. While the theoretical worst-case convergence rate of the corrected SEvo is only 1% of the normal gradient descent when \(=0.99\), its practical performance is much better. SASRec trained with SEvo-enhanced AdamW initially converges marginally slower and catches up in the final stage.

    & &  &  \\  & & &  &  \\   & HR@1 & 0.0033 & **0.0063** & +92.5\% \\  & HR@10 & 0.0208 & **0.0293** & +40.6\% \\  & NDCG@10 & 0.0103 & **0.0159** & +53.9\% \\  & Time/Epoch & 19.94s & +2.22s & \\  & Epochs & 100 & 150 & \\   & HR@1 & 0.0071 & **0.0171** & +139.1\% \\  & HR@10 & 0.0360 & **0.0626** & +73.9\% \\  & NDCG@10 & 0.0199 & **0.0377** & +89.1\% \\  & Time/Epoch & 25.20s & +8.27s & \\   & Epochs & 400 & 300 & \\   

Table 3: SEvo on large-scale datasets.

Figure 2: Empirical illustrations of convergence and smoothness. The top and bottom panels respectively depict the results for Beauty and MovieLens-1M. (a) SASRec enhanced by SEvo with or without rescaling. (b) Smoothness of (I) the original variation; (II) the smoothed variation; (III) the optimized embedding. A lower \(_{smoothness}\) indicates stronger smoothness.

**Smoothness evaluation.** Figure 1(b) demonstrates the variation's smoothness throughout the evolution process and the eventual embedding differences from \(=0\) to \(=0.999\). **(I) \(\) (II):** The original variations exhibit a similar degree of smoothness, but after transformation, they are quite different--smoother as \(\) increases. **(II) \(\) (III):** Consequently, the embedding trained with a stronger smoothness constraint becomes smoother as well. The structure-aware embedding evolution successfully makes related nodes closer in the latent space. Although smoothness is not the sole quality measure of embedding, combined with the analyses above, we can conclude that SEvo injects appropriate structural information under the default setting of \(=0.99\).

### Ablation Study

**SEvo for various optimizers.** It is of interest to study whether SEvo can be extended to other commonly used optimizers such as SGD and Adam. Figure 2(a) compares NDCG@10 performance on Beauty and MovieLens-1M. For a fair comparison, the hyperparameters are tuned independently. It is evident that the performance of SGD, Adam, and AdamW improves significantly after integrating SEvo, with AdamW achieving the best as it does not need to smooth the weight decay.

**Neumann series approximation versus iterative approximation.** Theorem 1 suggests that the Neumann series approximation is preferable to the commonly used iterative approximation because the latter is not always direction-aware and thus a conservative hyperparameter of \(\) is needed to ensure convergence. This conclusion can also be drawn from Figure 2(b). When only a little smoothness is required, their performance is comparable as both approximations differ only at the last term. The iterative approximation however fails to ensure convergence once \(>0.7\) on the Beauty dataset, potentially resulting in a lack of smoothness.

**Moment estimate correction for AdamW.** We compare SEvo-enhanced AdamW with or without moment estimate correction in Figure 2(c), in which average relative improvements against the baseline are presented for each recommender. Overall, the two variants of SEvo-enhanced AdamW perform comparably, significantly surpassing the baseline. However, in some cases (_e.g._, GRU4Rec and STOSA), the moment estimate correction as suggested in Theorem 3 is particularly useful to improve performance. Recall that BERT4Rec is trained using the output softmax from a separate fully-connected layer that is fully updated at each step. This may explain why the correction has little effect on BERTRec. In conclusion, the results underscore the importance of the proposed modification in alleviating bias in moment estimates.

### Applications of SEvo Beyond Interaction Data

We further explore the potential of applying SEvo to other types of prior knowledge. On the one hand, the category smoothness constraint can also be fulfilled through SEvo (see Appendix C.2), leading to progressively stronger clustering effects as \(\) increases. This provides compelling visual evidence of why SEvo is inherently structure-aware. On the other hand, SEvo is arguably an efficient tool for transferring embedding knowledge (see Appendix C.3). Notice that the learning of other modules cannot be guided in the same way, so SEvo alone is still inferior to state-of-the-art knowledge distillation methods . Fortunately, SEvo and other methods can work together to further boost the recommendation performance.

Figure 3: SEvo ablation experiments.

Related Work

**Recommender systems** are developed to enable users to quickly and accurately find relevant items in diverse applications, such as e-commerce , online news  and social media . Typically, the entities involved are embedded into a latent space [6; 12; 54], and then decision models are built on top of the embedding for tasks like collaborative filtering  and context/knowledge-aware recommendation [46; 44]. Sequential recommendation [36; 25] focuses on capturing users' dynamic interests from their historical interaction sequences. Early approaches adapted recurrent neural networks (RNNs)  and convolutional filters  for sequence modeling. Recently, Transformer [45; 11] has become a popular architecture for sequence modeling due to its parallel efficiency and superior performance. SASRec  and BERT4Rec  use unidirectional and bidirectional self-attention, respectively. Fan et al.  proposed a novel stochastic self-attention (STOSA) to model the uncertainty of sequential behaviors.

**Graph neural networks**[2; 10] are a type of neural network designed to operate on graph-structured data, in concert with a weighted adjacency matrix to characterize the pairwise relations between nodes. GNN equipped with this adjacency matrix can be used for message passing between nodes. The most relevant work is the optimization framework proposed in  for solving semi-supervised learning problems via a smoothness constraint. This graph regularization approach has recently inspired a series of work [7; 28; 60]. As opposed to applying it to smooth node representations  or labels , it is employed here primarily to balance smoothness and convergence on the variation.

**Structural information** in recommendation is typically learned through GNN as well, with specific modifications made to cope with like data sparsity [52; 29]. LightGCN  is a pioneering collaborative filtering work on modeling user-item relations, which removes nonlinearities for easier training. To further utilize sequential information, previous efforts focus on equipping sequence models with complex GNN modules, but this inevitably increases the computational cost of training and inference, making it unappealing for practical recommendation. For example, SR-GNN  and LESSR  need dynamically construct adjacency matrices for each batch of sequences. Differently, MAERec  proposes an adaptive data augmentation to boost a novel graph masked autoencoder, which learns to sample less noisy paths from semantic similarity graph for subsequent reconstruction tasks. The resulting strong self-supervision signals help the model capture more useful information.

## 5 Broader Impact and Limitations

Utilizing both sequential and structural information is beneficial yet challenging, and SEvo proposed in this paper suggests a novel and effective technical route for this purpose. Compared to other explicit GNN modules, SEvo is light-weight and easy-to-use in practice. These insights may inspire future research efforts regarding structure-aware optimization. However, there are still some limitations. Firstly, the training-free nature of SEvo makes it versatile, but also limits the expressive power. For a specific task, a sophisticated GNN module may be more desirable for achieving higher recommendation accuracy. Secondly, it might not be so straightforward to apply SEvo to the scenario involving multiple types of prior knowledge. Some efforts [43; 33] in the field of multiple graph learning have proposed some technically feasible solutions. However, these approaches still encounter challenges in terms of efficiency, particularly in the context of recommendation systems.

## 6 Conclusion and Future Work

In this work, we have proposed a novel update mechanism for injecting graph structural information into embedding. Theoretical analyses of the convergence properties motivate some necessary modifications to the proposed method. SEvo can be seamlessly integrated into existing optimizers. For AdamW, we recorrect the moment estimates to make it more robust. Besides, an interesting direction for future work is extending SEvo to multiplex heterogeneous graphs , as real-world entities often participate in various relation networks. Furthermore, we believe that SEvo holds potential for application to dynamic graph structures and incremental updates . Two challenges may be encountered in practice: the computational overhead associated with the ongoing adjacency matrix normalization, and how to adaptively weaken the outdated historical information.