# ## Comparing Causal Frameworks: Potential Outcomes, Structural Models, Graphs, and Abstractions

## Comparing Causal Frameworks: Potential Outcomes, Structural Models, Graphs, and Abstractions

### Duligur Ibeling

Department of Computer Science

Stanford University

duligur@stanford.edu

&Thomas Icard

Department of Philosophy

Stanford University

icard@stanford.edu

Toward the end of the twentieth century several frameworks arose for formalizing and analyzing problems of causal inference. One of these, associated with Rubin  and others (see ), takes the _potential outcome_--formalizing the effect of one variable on another--as a fundamental target of analysis. Causal assumptions in the Rubin causal model (RCM) are naturally encoded as algebraic constraints on potential outcomes, and research in this area has spawned a remarkable body of theoretical and applied work especially in social and biomedical sciences (see  for a review).

A second approach, associated with Pearl  and collaborators (see  for a textbook treatment; see also ), focuses instead on assumptions that can be encoded qualitatively, or more specifically, graphically, arising from a fundamental object known as a structural causal model (SCM). The _docalculus_ is one of the crowning achievements of this approach, and it has been shown derivationally complete with respect to a wide range of canonical estimation and inference problems .

Both frameworks have enjoyed considerable influence within recent causal machine learning in particular. As just one example, concern in reinforcement learning about the possibility of unobserved confounders--variables impacting both decisions and their outcomes--has generated a number of important advances, some employing tools and concepts from the RCM approach (e.g., ), others grounded in the SCM approach and typically involving graphs (e.g., ).

Despite the remarkable successes that both of these frameworks have engendered in machine learning and beyond, there remains substantial controversy over how to understand their relationship. In the literature it has been claimed, on the one hand, that the two are equivalent, that "a theorem in one is a theorem in the other" [27, p. 98]. On the other hand, some authors suggest that the two are only equivalent in a weak sense, one that "builds the limitations of SCM into the resulting comparison and likewise filters out aspects of the rival theory that do not readily translate to SCM" [21, p. 443].

At issue are two separable questions. The first is one of practical significance. Some argue that graphs give greater "conceptual clarity" [27, p. 103] and that SCMs more generally offer a "a flexible formalism for data-generating models" that helps ground causal inquiry [4, p. 514]; others argue that work in the RCM framework provides "transparent definitions of causal effects and encourages the analyst to consider individual-level heterogeneity as a first principle" [23, p. 91] as well as "guidanceto researchers and policy makers for practical implementation" [14, p. 1131]. While obviously very important, our goal is not to address these disputes about what theoretical primitives are most "natural" or "useful" for practitioners or applied researchers. Rather, the aim of the present contribution is to offer a number of new technical results that together shed light on a more basic question, namely, how precisely the RCM and SCM frameworks relate at a theoretical level. For example, are the two merely notational variants, or does one tacitctly enforce assumptions that the other does not?

In this paper we first endeavor, building on previous work, to elucidate the precise sense in which SCMs can be construed as _representations_ of RCMs, provided the latter satisfy two key principles known as composition and reversibility . Interestingly, such principles (or their logical consequences) have been questioned in the literature (e.g., ). Our second goal is to help clarify the sense in which they may fail. Drawing from recent literature on causal abstraction (e.g., )--broadened to cover both SCMs and RCMs--we suggest that failure of these principles plausibly results when causally relevant low-level details are elided in favor of more abstract variables. Our Thm. 1 buttresses this intuition, showing that every RCM is a _constructive abstraction_ of a representable RCM (hence satisfying composition and reversibility). We furthermore remark on how the well known SUTVA assumptions  can be understood as conditions on good variable abstractions.

Our starting point in this work is theoretically neutral, taking for granted only the primitive, "probability of a counterfactual." In the second half of the paper we introduce a framework-neutral formal language for reasoning about such probabilities, which in turn facilities further comparison. With respect to this common language, we offer a completeness result for the class of all RCMs (Thm. 2) and, drawing on , the class of representable RCMs (Cor. 2). These results are illustrated with an example derivation of LATE (see ), which also helps illuminate which assumptions are logically required in the derivation. Meanwhile, we offer a partial answer to the well-known open question  of how to characterize the algebraic constraints implied by a particular graph (Thm. 3), a result that helps bring graphical assumptions into this neutral common language. Finally, we show how an existing result on single-world intervention graphs (SWIGs), a framework drawing from both perspectives, can be construed as a completeness result for the same language (Thm. 4).

Taken together, our results are largely conciliatory--in the same spirit as other important conciliatory work in this context; see, e.g., --showing how the two frameworks are productively compatible, while also suggesting distinctive perspectives on problems of causal inference.

Proofs are deferred to supplementary appendices A, B, which contain additional technical material.

## 1 Modeling

We first introduce a formalization of the Rubin causal model  before then turning to structural causal models . The relationship between these two is elucidated in SS1.1.3.

### Preliminaries

Common to both frameworks will be a set \(\) of _endogenous variables_. Concerning notation:

**Notation.** The signature (or range) of a variable \(V\) is denoted \((V)\). Where \(\) is a set of variables, let \(()=_{S}(S)\), the product of the family of sets \((S)\) indexed by \(S\). Elements of \(()\) represent joint valuations of the variables \(\). Given an indexed family of sets \(\{S_{}\}_{ B}\) and elements \(s_{} S_{}\), let \(\{s_{}\}_{}\) denote the indexed family of elements whose object associated with the index \(\) is \(s_{}\), for all \(\). The symbol \(\) indicates any subset (or set inclusion) and does not imply a strict subset (or proper inclusion). For \(B^{} B\) write \(_{B^{}}:_{ B}S_{}_{ B^{ }}S_{}\) for the _projection map_ sending each \(\{s_{}\}_{ B}\{s_{^{}}\}_{^{} B^{ }}\); abbreviate \(_{^{}}=_{\{^{}\}}\), for \(^{} B\). Thus if \(()\) is a joint valuation of variables \(\) and \(S\) is a single variable, then \(s=_{S}()(S)\) is a value for \(S\). If \(^{}\) then \(_{^{}}()(^{})\) is a joint valuation of \(^{}\), namely the projection of \(\) to \(^{}\). Upper-case letters like \(\) conventionally represent those sets of variables that the corresponding lower-case letters \(\) are valuations of, \(()\).

#### 1.1.1 Rubin Causal Models, Potential Outcomes, Counterfactuals

The present formalization of the Rubin causal model  loosely follows previous presentations; see especially . It codifies experimental outcomes across individuals drawn from a distribution.

These are _potential outcomes_ over a variable set \(\), defined as expressions of the form \(Y_{}\) for an _outcome_\(Y\) and an _intervention_ or _treatment_\(()\) for some \(\), and interpreted as the value observed for \(Y\) in a controlled experiment where each \(X\) is held fixed to \(_{X}()\).

**Definition 1**.: A _Rubin causal model_ (RCM) is a tuple \(=,,,,P\) where \(\) is a finite set of _units_ or _individuals_, \(\) is a finite set of _endogenous variables_, \(\) is a set of potential outcomes over \(\), \(\) is a set of _potential response_ functions, to be defined shortly, and \(P:\) is a probability distribution on \(\). A potential response for \(Y_{}\) is a function \(f_{Y_{}}:(Y)\). For each \(Y_{}\) we require that \(=\{f_{Y_{}}\}_{Y_{}}\) contain exactly one such function.1

RCMs are often specified in a tabular form as in, e.g., Fig. 1 below. We adopt the notation \(y_{}(i)\) or \(Y_{}(i)=y\) as a shorthand for \(f_{Y_{}}(i)=y\): for \(\) as in Def. 1, write \( y_{}(i)\) iff \(Y_{}\) and \(f_{Y_{}}(i)=y\). This means that in the above controlled experiment, outcome \(y(Y)\) is observed for individual \(i\). Each \(Y_{}\) can be thought of as a variable with range \((Y_{})=(Y)\). We call the set \(()\) of joint valuations of these variables _counterfactuals_. A set of potential responses \(\) then maps units to counterfactuals, \(:()\), by defining \((i)=\{f_{Y_{}}(i)\}_{Y_{}}\), and:

**Definition 2**.: Where \(\) is as in Def. 1, the _counterfactual distribution_\(P_{}^{}:()\) induced by \(\) is the pushforward2\(_{}(P)\) of \(P:\) under \(:()\).

The reason we call \(P_{}^{}\) a counterfactual distribution (and \(()\) counterfactuals) is because such joint probabilities over multiple potential outcomes appear in the usual ratio definition of counterfactual probabilities. For instance, \(P(y_{x}|y^{}_{x^{}})=P(y_{x},y^{}_{x^{}})/P(y^{} _{x^{}})\) gives the probability that a unit who was withheld treatment and did not recover would have recovered if assigned treatment. But \(P_{}^{}\) also answers via marginalization all questions (whenever defined by \(\)) about "interventional" probabilities like \(P(y_{})\), as well as purely "observational" probabilities such as \(P()\); see, e.g., .

Some authors submit that "probability will mean nothing more nor less than a proportion of units" [11, p. 945], thereby assuming a uniform distribution on a finite population \(\) (cf. also ). Of course, in the infinite population size limit we recover all RCMs as in Def. 1 (see Prop. A.1).

While practitioners do not typically consider potential outcomes \(Y_{}\) when \(Y\), instead maintaining a strict dichotomy between cause and effect variables (e.g., ), it is natural to impose the following requirement (known as _effectiveness_) whenever such potential outcomes are defined. An intervention is always assumed to be a _successful_ intervention: whenever \(Y\),

\[. Y_{}(u)=_{Y}() \]

for every \(u\). In fact, practice in the RCM framework reflects this same assumption, in the sense that violations of it are taken to signify a poor choice of variables. As a classic example, the possibility of _non-compliance_ in an experimental trial motivates the introduction of instrumental variables, and specifically a separation between, e.g., _treatment_ and _assignment to treatment_ (cf. Ex. 3). Crucially, we recover effectiveness with respect to the latter. We will assume any RCM to meet (1) unless otherwise specified; let \(_{}\) be the class of such RCMs.3

#### 1.1.2 Structural Causal Models

An important feature of RCMs is that potential outcomes are cleanly separated from assignment mechanisms . A different starting point is to assume that potential outcomes and their algebraic behavior are rather _derived_ from an underlying formal representation of causal structure. These putatively "deeper mathematical objects" [27, p. 102] involve concrete functional dependencies, and an operation of function replacement known as _intervention_:

**Definition 3**.: A _structural causal model_ (SCM) is a tuple \(=,,,P()\) where \(\) is a finite set of _exogenous variables_, \(\) is a finite set of _endogenous variables_, \(\) is a family of _structuralfunctions_, to be defined shortly, and \(P:()\) is a probability distribution on (joint valuations of) \(\).

A structural function for \(V\) is a function \(f_{V}:(_{V}_{V})(V)\), where \(_{V}\), \(_{V}\{V\}\). For every \(V\) we require that \(=\{f_{V}\}_{V}\) have exactly one such function; the entire collection \(\) thus forms an exogenous-to-endogenous mapping.

Interventions come as a derived notion, replacing structural functions with constant functions :

**Definition 4** (Intervention on SCMs).: Let \(()\) for some \(\) be an intervention and \(\) be the SCM of Def. 3. Then define a _manipulated model_\(_{}=(,,\{f^{}_{V}\}_{V},P( ))\) where each \(f^{}_{V}:(^{}_{V}^{ }_{V})(V)\). If \(V\) define \(^{}_{V}=_{V}\), \(^{}_{V}=_{V}\), and \(f^{}_{V}=f_{V}\). If \(V\) define \(^{}_{V}=^{}_{V}=\) and \(f^{}_{V}\) as a constant function mapping to \(_{V}()\).

Letting \(\) be the SCM of Def. 3, for \(()\) and \(()\) write \(,\) if we have \(f_{V}_{_{V}_{V}}()=_{ V}()\) for every \(V\). Let \(_{}\) be the class of all SCMs \(\) such that, for any \(\) and intervention \(\) there is a unique _solution_\(\) such that \(_{},\). In this case we define the potential outcome \(Y_{}()\) as \(_{Y}()\). Thus any \(_{}\) defines a potential outcome for _every_\(Y_{}\), giving a natural function \(p^{}:()(\{Y_{ }\}_{\,Y,})\) via these outcomes, and:

**Definition 5**.: The counterfactual distribution \(P^{}_{}:(\{Y_{}\}_{\,Y,})\) induced by \(_{}\) is the pushforward \(p^{}_{*}(P)\) of \(P:()\) under \(p^{}:()(\{Y_{ }\}_{\,Y,})\).

Thus, SCMs in \(_{}\) canonically define counterfactual distributions for all possible potential outcomes via manipulation of functional dependencies. Importantly, Def. 5 provides a bridge to RCMs, as both produce counterfactual distributions (recall Def. 2). As long as the counterfactual probabilities are assumed to mean the same thing--i.e., as long as they highlight the same targets for empirical and theoretical investigation--we can then compare the ranges of assumptions and inference patterns that each framework can encode about them. We thus assume that all our SCMs belong to this class \(_{}\).

#### 1.1.3 Representation of RCMs by SCMs

A contentious methodological question is whether all (endogenous) variables should be potential candidates for intervention. Following the literature we have supposed that SCMs allow all possible interventions (though this assumption is not universal; see, e.g., ). For RCMs it is generally assumed that there can be "no causation without manipulation" , and thus that only some interventions should be allowed. While methodologically important, this is theoretically inessential. We can construe SCMs as possible representations of RCMs in the following sense:

**Definition 6**.: Let \(_{}\) and \(_{}\). We say that \(\)_represents_\(\) if its counterfactual distribution \(P^{}_{}\) marginalizes down to \(P^{}_{}\) on the potential outcomes defined by \(\) (the set \(\) in Def. 1). We say \(\) is _representable_ if it is represented by at least some \(_{}\).4

Thus \(\) represents \(\) if they are counterfactually equivalent with respect to the outcomes defined by \(\). Toward a characterization of representability, consider two properties of an RCM :

**Definition 7**.: The following Boolean formulas encode assumptions about potential outcomes:

\[ Y_{}(u)=y Z_{}(u )=z Z_{y}(u)=z \] \[ Y_{z}(u)=y Z_{y }(u)=z Y_{}(u)=y. \]

Say \(_{}\) satisfies composition and reversibility, respectively, when the respective statements hold for every unit \(u\) of \(\), whenever all the appropriate potential outcomes are defined.

We understand lower-case values like \(y\), \(z\), \(\), when not bound as dummy indices or otherwise, to be schematic variables carrying tacit universal quantifiers. Thus (2), (3) must hold for all possible \(y(Y)\), \(()\), \(z(Z)\). This same usage is repeated, e.g., in (9).

While reversibility seems not to have arisen in the potential outcomes literature, instances of composition have appeared explicitly (e.g., Holland 12, p. 968) and have been used implicitly in concrete derivations (see Ex. 3 below). Note that the well-known principle of _consistency_ is merely the instance of composition for \(=\). For \(,^{}_{}\) that share the same units \(\) and endogenous variables \(\) but have respective potential outcome sets \(,^{}\) and potential response sets \(,^{}\), if \(^{}\) and \(^{}\) we say that \(^{}\)_extends_ or is an extension of \(\) and \(\) is a _submodel_ of \(^{}\). Call \(\)_full_ if it has no proper extension. Then:

**Proposition 1** (SCM Representation).: RCM \(\) is representable iff \(\) extends to some full \(^{}\) that satisfies composition and reversibility.

Note that for an RCM \(\) to be representable it is necessary (though not sufficient, in light of the models presented in Fig. 1 below) that \(\) itself witness no composition or reversibility failures. Prop. 1 thus clarifies a sense in which RCMs are more general than SCMs, not just by allowing only a subset of allowable interventions, but also by imposing fewer requirements on how potential outcomes relate to one another. However, assuming composition, reversibility, and fullness, the two define the same classes of counterfactual distributions, despite the superficial differences in their definitions. In that case the two are, e.g., equivalent for interpreting the probabilistic logical language of SS2. We submit that some version of this result makes sense of statements in the literature, e.g., from Pearl , that the twain are essentially equivalent frameworks from a theoretical perspective.

### Causal Abstraction

The goal of this section is to clarify the source of putative failures of principles like composition. We suggest that it is helpful to view these issues through the lens of _causal abstraction_ (the definitions in this section are adapted from ). Abstraction has mostly been studied in the context of SCMs; our definitions apply equally to SCMs and RCMs via counterfactual distributions.

In causal abstraction, one has a set \(_{}\) of low-level (or concrete, or micro-) variables representing a fine-grained description and a set \(_{}\) of high-level (or abstract, or macro-) variables representing a coarser-grained description of the same scenario. The correspondence between the two descriptions is given by a partial _translation_ map \(:(_{})(_{})\). Translations extend canonically to maps of partial valuations (e.g., interventions) \(:_{_{}}\,( )_{_{}}\,()\) by setting \((_{})=_{}\) iff \(^{-1}_{_{}}(_{})= ^{-1}_{_{}}(_{})\).

We overload \(\) once more so as to cover counterfactuals, defining as follows yet another partial \(:(_{})(_{ })\) for any sets \(_{}\), \(_{}\) of potential outcomes over \(_{}\) and \(_{}\) respectively. Index an element of \((_{})\) as \(\{(^{i}_{})_{^{i}_{}}\}_{1 i m}\), where \(^{i}_{}^{j}_{}\) for any \(i j\) and \(^{i}_{}(\{Y_{}:Y_{ ^{i}_{}}_{}\})\) for each \(i\), and an element of \((_{})\) likewise as \(\{(^{j}_{})_{^{i}_{}}\}_{1 j n}\). Define \(\{(^{j}_{})_{^{i}_{}}\}_{1  i m}=\{(^{j}_{})_{^{j}_{}}\}_{1 j n}^{1}\) if \((\{^{i}_{}:1 i m\})=\{^{j}_{ }:1 j n\}\) and \((^{i}_{})=^{j}_{}\) for any pair \(^{i}_{},^{j}_{}\) where \((^{i}_{})=^{j}_{}\).

**Definition 8**.: With counterfactual translation in hand, we define an abstraction relation between probabilistic causal models. The model \(\) abstracts \(\) over the aligned variables (written \(_{}\)) if the translation \(\) pushes the latter's counterfactual distribution to the former's, that is, \(P^{}_{}=_{*}(P^{}_{})\).

A stricter and typically more useful notion is that of _constructive_ abstraction (e.g., ). These arise from translations that can be generated variable-wise, and thus correspond to a coherent "clustering" of variables:

**Definition 9**.: Translation \(:(_{})(_{})\) is constructive if there is a partition \(\) of \(_{}\) with non-overlapping cells \(\{_{V}\}_{V\{\}}\), each \(_{V}_{}\), where \(_{V}\) is non-empty for all \(V\), and a collection \(\{_{V}\}_{V_{}}\) each of which is a partial surjective map \(_{V}:(_{V})(V)\), such that \((_{})=_{V}_{V}(_{ })}_{V_{}}\) for any \(_{}(_{})\).

A simple abstraction, ubiquitous in the literature (see, e.g., [16, SS1.6.2] and ), is that of variable treatment levels. Here a higher-level value corresponds to some collection of lower-level specifications that might represent the potency or dosage of the drug administered, the time of administration, etc.: for example, a distinction of whether one took 300, 400, 500, or 600 mg of aspirin is made at the low level, but at the high level, there is only the binary distinction between having taken aspirin and not. Formally, a treatment variable \(T\) is only binary with values \(,\) (control, treatment resp.) at the high level but takes on many values \(,^{1},,^{n}\) at the low level. The abstraction is made by omitting the fine-grained details; symbolically, one forms a new model by eliding the superscripts, collapsingall \(^{i}\) into \(\). So long as for any outcomes we have \(Y_{^{i}}(u)=Y_{^{j}}(u)\), the model thus formed will be a constructive probabilistic abstraction of the low-level model.

The next result provides some useful properties of constructive abstraction.

**Proposition 2**.: Suppose \(_{}\) with \(\) constructive. Then \(\) is effective if \(\) is effective. Also, for any submodel \(^{}\) of \(\) there is a submodel \(^{}\) of \(\) such that \(^{}_{}^{}\).

Thus our general class of effective RCMs closes under constructive translation. The next example shows that this is not the case for the narrower class of representable models.

**Example 1**.: Let \(X,Y,X^{},Y^{}\) be variables with \((X)=\{0,1,2\}\) and \((X^{})=(Y^{})=(Y)=\{0,1\}\). Consider the RCM \(_{}\) defined over \(_{}=\{X,Y\}\) as a conjunction of POs:

\[X=1 Y=1 Y_{X=2}=0 X_{Y=0}=1 \]

for a single unit (suppressed above for clarity). Consider a second RCM \(_{}\) over \(_{}=\{X^{},Y^{}\}\):

\[X^{}=1 Y^{}=1 Y^{}_{X^{}=1}=0 X^{ }_{Y^{}=0}=1. \]

Note that \(_{}_{}_{}\) where \(\) is a constructive abstraction with \(_{X^{}}=\{X\}\), \(_{Y^{}}=\{Y\}\) and \((X=0)=0\), \((X=1)=(X=2)=1\), \((Y=y)=y\). Now \(_{}\) violates both composition and reversibility, while \(_{}\) is representable.

A second observation is that the analogue of the claim about submodels in Prop. 2 does not hold for extensions:

**Example 2**.: Consider enlarging (4) with the potential outcome \(Y_{X=1}=1\). Then there is no high-level abstraction under \(\) that defines the outcome \(Y^{}_{X^{}=1}\), since \(Y_{X=2}=0 Y_{X=1}=1\) translates to \(Y^{}_{X^{}=1}=0 Y^{}_{X^{}=1}=1\).

The main result of this section is that the phenomenon exhibited by Ex. 1 accounts for all representability failures:

**Theorem 1** (Abstract Representation).: Let \(\) be an RCM. Then there is a representable \(_{}\) and constructive translation \(\) such that \(_{}_{}\).

It is worth remarking on the connection between a well-known twofold condition called the Stable Unit Treatment Value Assumption (SUTVA [16, SS1.6]) and causal abstraction. The first part of SUTVA is the assumption that "the potential outcomes for any unit do not vary with the treatments assigned to other units"; this is already presumed by our definition of causal model, which does not admit interventions on multiple units (however, see Rmk. A.1 for a way to model this condition within our framework as an application of abstraction). The second part is that "for each unit, there are no different forms or versions of each treatment level, which lead to different potential outcomes." Note that this assumption can be seen as guaranteeing the viability of the variable treatment levels abstraction, as it is simply a restatement of the condition we already identified--that the outcomes \(Y_{^{i}}(u)\) for any unit \(u\) and treatment level \(^{i}\) must all agree.5

## 2 Inference

The raison d'etre of a causal inference framework is to provide a language for encoding causal assumptions and showing when and why conclusions follow from available data and appropriate assumptions. In this section, to provide further granularity on the comparison between RCMs and SCMs, we introduce a neutral formal language that is naturally interpreted relative to both of these models. The language systematizes reasoning about the probabilities of counterfactuals. Fixing a set \(\) of potential outcome pairs, we define a formal language \(\) in two stages:

**Definition 10**.: The _base language_\(_{}\) is given by all Boolean combinations of statements \(Y_{}=y\), alternatively written \(y_{}\), for all \(Y_{}\), \(y(Y)\). Meanwhile, \(\) is defined as the set of Boolean combinations of inequalities \(_{1}_{2}\), where \(_{1},_{2}\) are generated as sums, products, and additive inverses of probability terms \(()\), where \(_{}\).

The language \(\) is the most expressive in a sequence of three languages introduced in [13; 4] to formalize the "causal hierarchy" . By restricting probability terms to purely "observational" or "interventional" quantities, it is possible to study the inferential limitations of data and assumptions at lower levels of this hierarchy. For present purposes, \(\) naturally encodes prominent reasoning patterns in RCMs and in SCMs. Its semantics are straightforward in any \(\) or \(\) that includes all outcomes \(\): we generate a mapping of each polynomial term \(\) recursively with the crucial case being to map \(()\) to the probability calculable by marginalization of \(p_{}^{}\) or \(p_{}^{}\), and then evaluate the atom \(_{1}_{2}\) true iff \(_{1}_{2}\), recursing to define a semantics for all of \(\). Over the class of all (recursive, possibly infinite) SCMs, \(\) has been axiomatized  by a set of principles called \(_{3}\), and the complexity of its satisfiability problem has been shown complete for the class \(\). The class of simple probability distributions over the atoms of \(_{}\) is axiomatized by principles known as \(_{1}\), which we will abbreviate \(\).

### Potential Outcomes Assumptions

Reasoning about potential outcomes is often encoded in what we call the base language \(_{}\), augmented with (typically implicit universal) quantifiers over units. For instance, the well known _monotonicity_ (or "no defiers" who do the opposite of their prescription) assumption [15; 16] says

\[ u.X_{z^{-}}(u)=1 X_{z^{+}}(u)=1, \]

where \(X\) and \(Z\) are binary variables respectively meaning the treatment (actually taken) and the treatment prescribed, with \(z^{+},z^{-}\) abbreviating \(Z=1,Z=0\) respectively. We will use the same abbreviation for other binary variables, so that the above condition can be written succinctly as \(x_{z^{+}}^{+} x_{z^{-}}^{+}\). We also adopt this interpretation of \(X,Z\) for the rest of SS2.1. We now explain how this and other causal assumptions in the potential outcomes framework can be encoded in \(\):

**Definition 11**.: Let \(\) be a well-formed, closed predicate formula in prenex normal form with a single quantifier over a variable \(\{u\}\) and matrix in \(_{}\); the \(u\) can alternately be included in the atoms, e.g., by writing \(Y_{}(u)=y\). Define its encoding \(()\) as follows:

\[()=()=0,&= u.,_{}\\ ()>0,&= u.,_{}.\]

Note that \(\) is quantifier-free in both cases. Thus, e.g., the encoding of (6) is \((x_{z^{-}}^{+} x_{z^{+}}^{+})=0\).

Where \(S\) is a set of \(_{}\) assumptions let \((S)\) be the class of RCMs whose potential outcomes obey every assumption in \(S\), thus obeying \( u.\) where \(u\) ranges over units for any \( S\). Also let \((S)=\{( u.)\}_{ S}\) be the encoding of \(S\) via Def. 11. Then we have the following:

**Theorem 2**.: \(+(S)\) _is sound and complete for \((S)\)._

**Corollary 1**.: \(=+()\) _is sound and complete for \(_{}\)._

One consequence of this completeness result is that purely propositional and predicate logic reasoning about potential outcomes can be interweaved with probabilistic reasoning, as in Ex. 3 below. Another consequence is a complete axiomatization of SCMs (which can be seen as a probabilistic lift of ):

**Corollary 2**.: Let \(\), \(\) be universal statements of (2) and (3) respectively. Then \(=+()+()\) is sound and complete for \(_{}\) (where every outcome is included in \(_{}\)).

**Example 3**.: A seminal result from [15; 1] is that it is possible to estimate the Average Treatment Effect among the population of units who _comply_ with their treatment assignment, a quantity known as the _Local_ Average Treatment Effect (LATE): \((Y_{x^{+}}-Y_{x^{-}}|x_{z^{+}}^{+} x_{z^{-}}^{-})\), with \(Y\) the outcome, which we assume binary purely for simplicity, and without loss of generality. Thm. 2 implies that this can be verified in our calculus, by appeal to two key assumptions [15; 1]: monotonicity (6) and

\[. u.y_{z^{-},x}(u) y_{z^{+},x}(u). \]

The original discovery was that these principles guarantee that \(=_{1}/_{2}\), where the latter are the average "causal effects of assignment on treatment received and on the outcome of interest" , or symbolically:

\[_{2} =(X_{z^{+}}-X_{z^{-}})=(x_{z^{+}}^{+} x _{z^{-}}^{-})-(x_{z^{+}}^{-} x_{z^{-}}^{+})\] \[_{1} =Y_{z^{+},X_{z^{+}}}-Y_{z^{-},X_{z^{-}}} =y_{z^{+},X_{z^{+}}}^{+} y_{z^{-},X_{z^{-}}}^{-} -y_{z^{+},X_{z^{+}}}^{-} y_{z^{-},X_{z^{-}}}^{+}.\]where in \(_{1}\), interventions like \(X_{z}\) set \(X\) at the unit level to the value that it would take under the intervention setting \(Z\) to \(z\); thus, e.g., we have \((y_{z,X_{z}})=(y_{z,x^{+}} x_{z}^{+})+(y_{z, x^{-}} x_{z}^{-})\). Crucially, these two quantities can be estimated, e.g., through randomized experiments [16, Ch. 23].

However, over our most general class \(_{}\) of RCMs, these two assumptions are not in fact sufficient to identify LATE. Fig. 1 illustrates a family of RCMs that satisfy (6) and (7), but disagree on LATE. An additional principle, which Angrist et al.  offer as a matter of notation, we dub:

\[ u.y_{x}(u) y_{z^{+},x} (u). \]

It can then be shown that, taken together, (6), (7), and (8) do indeed logically entail \(=_{1}/_{2}\); see Prop. B.1 in the technical appendix for the derivation.

There has been much discussion of monotonicity and exclusion restrictions (which are closely related to graphical assumptions; see SS2.2 below), but what might justify outcome decomposition (8)? One intuition might be that it somehow follows from the exclusion restriction (7): if the effect of \(X\) on \(Y\) is the same no matter the value of \(Z\), then it would seem that omitting \(z^{+}\) in the intervention should have no impact on the effect of \(X\) on \(Y\). Of course, the example in Fig. 1 shows that this is too quick.

It turns out that (8) does follow from (7) if we restrict attention to _representable_ RCMs. In fact, (8) is derivable from (7) and the principle of _composition_ (2) in the calculus \(\), so long as we can reason along the way about the potential response \(Z_{x}\). By composition, for any \(x\) and \(y\) we have \(y_{x} z_{x}^{+} y_{z^{+},x}\) and \(y_{x} z_{x}^{-} y_{z^{-},x}\), and by ER (7) the latter gives \(y_{x} z_{x}^{-} y_{z^{+},x}\). As \(Z_{x}\) is binary, we have \(z_{x}^{+} z_{x}^{-}\), and thus by propositional reasoning, \(y_{x} y_{z^{+},x}\). The other direction \(y_{z^{+},x} y_{x}\) follows from the same argument by contraposition, as \(Y\) too is binary.

Thus, while the full power of composition is not invoked, it is natural to read this example and much of the literature as implicitly assuming something like representability (thus implying composition). Another source of support for this is that under representability one can show (see Prop. B.2) that \(_{1}=(Y_{z^{+}}-Y_{z^{-}})\), an even simpler and manifestly identifiable expression for this average effect.

### Graphical Assumptions

As we saw above (Prop. 1), SCMs can be understood as _representations_ of suitable RCMs. As such, they also suggest further sources of assumptions for deriving causal inferences. In particular, qualitative patterns of functional dependence introduce the possibility of _graphical_ methods:

**Definition 12**.: Let \(=,,\{f_{V}\}_{V},P \) be an SCM. Then define the _causal diagram_\(()\) of \(\) as a graph over nodes \(\), with mixed directed edges \(\) and _bidirected arcs_\(\)\(\)\(\)\(\). For any \(V,V^{}\), there is a directed edge \(V V^{}\) if \(V_{V^{}}\), and there is a bidirected edge \(V\)\(\)\(\)\(\)\(V^{}\) if \(_{V},_{V^{}}\) are correlated under \(P\) (including if \(_{V}_{V^{}}\)).

Letting \(()\) be the set of SCMs with diagram \(\), we have \(()_{}\) provided the directed edges in \(\) form a dag (see Lem. A.3). We thus assume this acyclicity of any \(\). When interpreting over an SCM, we include every possible potential outcome in \(\). Just as we earlier encoded assumptions about the potential outcomes of an RCM into \(\), we may do the same for SCMs regarding their graphs. A first observation is that Def. 11 translates axiom C6 of  to \(\) of , thus rederiving the system \(_{3}\) for the class of all acyclic SCMs, i.e. \(_{}()\), from the latter. We now encode the content of (the assumption of having) a _particular_ diagram \(\) into \(\). Let \(_{V}^{}=\{V^{}:V^{} V \}\) be the directed parents in a graph \(\) of a vertex \(V\). We encode by way of two schemas, encapsulating what some  have called "the two fundamental laws of causal inference":

**Definition 13**.: Let the exclusion restriction schema \(^{}\) be the \(_{}\) principle \(y_{} y_{}\), for all variables \(Y\) and sets of variables \(_{V}^{}\), where \(y(Y)\), \(()\), \(=_{_{V}^{}}()\). Let the counterfactual independence schema \(^{}\) be, for all pairs of variable sets \(\{Y_{i}\}_{1 i n},\{Y^{}_{j}\}_{1 j n^{}} \) such that there are no \(Y_{i},Y^{}_{j}\) for which \(Y_{i}=Y^{}_{j}\) or \(Y_{i}Y^{}_{j}\) in \(\),

\[^{}._ {1 i n}(y_{i})_{_{i}}_{1 j n^{} }(y^{}_{j})_{^{}_{j}}=_ {1 i n}(y_{i})_{_{i}}_ {1 j n^{}}(y^{}_{j})_{_{j}} \]

where \(y_{i}(Y_{i})\), \(y^{}_{j}(Y^{}_{j})\), \(_{i}(_{V_{i}}^{}),^{}_{j}(_{V_{j}^{}}^{})\) for each \(Y_{i}\), \(Y^{}_{j}\). Then the translation of \(\) is the combination of axioms \(()=(^{})+ ^{}\).

Note that while Ex. 3 in no way relies on graphs, if we accept a \(\) where \(Z Y\), then \(^{}\) yields \(y_{x} y_{zx} y_{z^{}x}\) without further ado. Importantly, however, \(()\) is not valid over \(()\) for any \(\) containing the edge \(Z X\), revealing an extra-graphical provenance. On the other hand, \(\) is inexpressible in \(_{}\)--inferentially, the two approaches are incomparable.

A long-standing question has been whether exclusion restriction and independence axioms together could be _complete_, in that they capture all the inferential content of a given causal diagram \(\) (see, e.g., ). Answering such questions can help with the development of tractable inference methods. Partial completeness results for limited queries are known , and the method from Tian  supplies an algorithm that is complete with respect to all equality constraints . Placing no limitations on queries beyond their expressibility in \(\)--and thus including inequality constraints as well--but making certain restrictions on \(\), we answer this question in the affirmative:

**Theorem 3**.: For any acyclic diagram \(\), axioms \(()+\) are sound for \(\) over \(()\), and also complete if the bidirected arcs in \(\) form a disjoint union of complete graphs.

Often the famous _d-separation_ conditional independence criterion (Def. B.2) is used in place of our \(\). Since all instances of the latter are instances of the former, our Thm. 3 is stronger (see Cor. B.1). This completeness result implies that for such a \(\), any known graphical conclusions--including _do_-calculus, identifiability results, and bounds--can be rederived in our calculus, e.g.:

**Example 4** (Verma constraints).: We derive the _Verma constraint_ over the graph \(\) of Fig. 1(a) that \(_{w}(y z,w,x)(w x)\) does not depend functionally on \(x\):

\[_{w}(y,z,w,x)(w,x)}{(z,w,x)(x)}()}}{{=}} _{w}(y_{zwx},z_{ywx},w_{yzx},x_{yzw})(w_{x},x_{w} )}{(z_{wx},w_{zx},x_{zw})(x)}\] \[(^{ })}}{{=}}_{w}(y_{z},z_{w},w_{x},x)(w_{x},x)}{ (z_{w},w_{x},x)(x)}()\) exhaust the types of algebraic statements that a researcher is committed to when venturing a graphical assumption. Putting this in algebraic terms facilitates a perspective on such assumptions that can be naturally interpreted with respect to RCMs, independent of any representation by an SCM.

### Single-World Intervention Graphs

Another graphical framework that draws on ideas and concepts from both frameworks is that of _single-world intervention graphs_ (SWIGs) . In comparison to the usual formulation of do-calculus, SWIGs facilitate reasoning with a wider class of expressions by combining graphical and potential outcome notations (see, e.g., ). Here we show that this "hybrid" framework can also naturally be assimilated to the logical perspective adopted in the present paper. Assuming acyclicity:

**Definition 14**.: Let \(\) be a dag over \(\) and let \(\) be an intervention. Let \(_{V}^{}=\{V^{}:V^{} V\) in \(\}\) be the directed ancestors6 of a variable \(V\) in \(\). Then the SWIG \(_{}\) has nodes labeled\[\{V_{_{}()}:V\}\{_{V}( ):V\},=(_{V}^{ })\{V\},() V_{}:X,X V\}\{V_{} V_{^{}}^{}:V ,V V^{}\}.\]

See Fig. 1(c) for an example of this construction. Note that edges in the first set in the union above have fixed heads and random tails, while those in the second set have random heads and tails.

**Definition 15**.: Define the following conditional independence schema \(sep^{_{}}}\), for any sets of random nodes \(\{(X_{i})_{_{i}}:1 i l\}\) and \(\{(Y_{j})_{^{}_{j}}:1 j m\}\) that are d-separated (Def. B.1) given \(\{(Z_{k})_{^{}_{k}}:1 k n\}\) in the SWIG \(_{}\):

\[sep^{_{}}}. _{1 i l\\ 1 j m}(x_{i})_{_{i}}(y_{j})_{^ {}_{j}}_{1 k n}(z_{k})_{ ^{}_{k}}\\ =_{1 i l\\ 1 k n}(x_{i})_{_{i}}(z_{k})_{^ {}_{k}}_{ 1 j m\\ 1 k n}(y_{j})_{^{}_{j}}(z_{k})_{ ^{}_{k}}. \]

One notable model associated with SWIGs is the _FFRCISTG_; given the same graph, FFRCISTGs are compatible with SCMs  but issue fewer (potentially controversial) implications:

**Definition 16**.: Let \(\) be a full RCM. Then \(\) is a _FFRCISTG over \(\)_ if every instance of \((^{})\) and \(sep^{}}\) holds in its counterfactual distribution. Let \(()\) be the class of FFRCISTGs over \(\).

**Proposition 3**.: Suppose the SCM \(()\) represents the full RCM \(\). Then \(()\). 

Given that Def. 16 already defines \(()\) in terms of \(\)-principles, while  have shown the soundness direction, the following is straightforward:

**Theorem 4**.: \(+(^{})+_{} sep^{_{}}}\) is sound and complete over \(()\). 

## 3 Conclusion

The task of this paper has been to clarify the senses in which the Rubin causal model and structural causal models are very closely related formalisms for encoding causal assumptions and deriving causal conclusions. We concur with , ,  and others that "there are insights that arise when using each that are less transparent when using the other" [41, p. 8]. Our interest in this paper has been to elucidate the comparison from a theoretical ("in principle") perspective.

We do not suppose that the present work will be the final word on theoretical connections between RCMs and SCMs. On the contrary, there remain numerous open questions. Perhaps chief among these is the generalization of Thm. 3 to encompass all possible causal diagrams (not just those in which the bidirected arcs form a disjoint union of complete graphs). Does the theorem hold with no further principles, or do additional algebraic constraints arise? This important open question  is a crucial step toward a complete theoretical synthesis of the two frameworks.

#### Acknowledgments

This work was partially supported by a seed grant from the Stanford Institute for Human-Centered Artificial Intelligence. We also thank Elias Bareinboim and Guido Imbens for helpful conversations.