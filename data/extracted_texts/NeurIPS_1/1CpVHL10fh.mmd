# Should I Stop or Should I Go:

Early Stopping with Heterogeneous Populations

Hammaad Adam

Massachusetts Institute of Technology

&Fan Yin

Microsoft Corporation

&Huibin (Mary) Hu

Microsoft Corporation

&Neil Tenenholtz

Microsoft Research

&Lorin Crawford

Microsoft Research

&Lester Mackey

Microsoft Research

&Allison Koenecke

Cornell University

Corresponding author (hadam@mit.edu).Current affiliation: Amazon. Work performed while at Microsoft (prior to joining Amazon).

###### Abstract

Randomized experiments often need to be stopped prematurely due to the treatment having an unintended harmful effect. Existing methods that determine when to stop an experiment early are typically applied to the data in aggregate and do not account for treatment effect heterogeneity. In this paper, we study the early stopping of experiments for harm on heterogeneous populations. We first establish that current methods often fail to stop experiments when the treatment harms a minority group of participants. We then use causal machine learning to develop CLASH, the first broadly-applicable method for heterogeneous early stopping. We demonstrate CLASH's performance on simulated and real data and show that it yields effective early stopping for both clinical trials and A/B tests.

## 1 Introduction

Randomized experiments are the gold-standard method of determining causal effects, whether in clinical trials to evaluate medical treatments or in A/B tests to evaluate online product offerings. The sample size and duration of such experiments are typically specified in advance. However, there are often strong ethical and financial reasons to stop an experiment before its scheduled end, especially if the treatment shows early evidence of harm . For example, if early data from a clinical trial demonstrates that the treatment increases the rate of serious side effects, it may be necessary to stop the trial to protect experimental participants .

A variety of statistical methods can be used to determine when to stop an experiment for harm . Investigators in both clinical trials and A/B tests will often choose to use a subset of these methods--collectively referred to as "stopping tests"--based on the specifics of their domain. Stopping tests not only identify harmful effects from early data, but also limit the probability of stopping early when the treatment is not harmful. However, stopping tests are typically applied to the data in aggregate (i.e., "homogeneously") and do not account for heterogeneous populations. For example, a drug may be safe for younger patients but harm patients over the age of 65. While a growing body of literature has studied how to infer such heterogeneous effects [see, e.g., 41], little prior research has described how to adapt stopping tests to respond to heterogeneity.

We continue the above example to illustrate why stopping tests should account for heterogeneity. Consider a clinical trial for a drug such as warfarin, which has no harmful effect on the majority of the population but increases the rate of adverse effects in elderly patients . Using a simple simulation, we demonstrate that if elderly patients comprise \(10\%\) of the trial population, then applying a stoppingtest homogeneously would rarely stop the trial for harm (Fig. 1). In most cases, the trial continues to recruit elderly patients until its scheduled end, many of whom will be harmed by their participation. This outcome violates the bioethical principle of non-maleficence  and is clearly undesirable.

In this paper, we consider early stopping for harm with heterogeneous populations. We first formalize the problem of heterogeneous stopping and establish the shortcomings of the prevailing homogeneous approach. We then present our novel method: Causal Latent Analysis for Stopping Heterogeneously (CLASH). CLASH uses causal machine learning to infer the probability that a participant is harmed by the treatment, then adapts existing stopping tests to better detect this harm. CLASH allows an investigator to use their stopping test of choice: it is thus flexible and easy-to-use. We theoretically establish that, for sufficiently large samples, CLASH is more likely to stop a trial than the homogeneous approach if the treatment harms only a subset of trial participants. CLASH also does not stop trials unless a group is harmed; it thus increases statistical power while limiting Type I error rate. We demonstrate these properties in an extensive series of experiments on synthetic and real data, and establish that CLASH leads to effective heterogeneous stopping across a wide range of domains.

While a small number of papers have attempted to address the problem of heterogeneous early stopping, they have relied on restrictive assumptions, limiting their applicability. Thall and Wathen  require knowledge of the source of heterogeneity, which is rarely available in practice. Yu et al.  allow for unknown groups but only model linear heterogeneity. Yu et al.  relax the linearity restriction but do not handle time-to-event data commonly observed in clinical trials. In contrast, CLASH does not require prior knowledge, makes no parametric assumptions, and works with any data distribution. It is thus the first broadly applicable tool for early stopping with any number of heterogeneous groups.

We emphasize that early stopping is a nuanced decision. For example, if a treatment harms only a subset of participants, it may be desirable to stop the experiment only on the harmed group but continue it on the rest of the population. In other situations, it may make sense to stop the trial altogether. The decision to stop a trial to protect one group may disadvantage another group that would have benefited from the treatment; any decision on early stopping must thus carefully weigh the treatment's potential benefit, the size of the harmed group, the nature of the harm, and other ethical considerations. Our method is not intended to replace such discussion. It can, however, serve as an aid for investigators to make difficult decisions on early stopping.

## 2 Background and Setup

### Randomized Experiment

Consider an experiment that evaluates the effect of a binary treatment \(D\{0,1\}\) on an observed outcome \(Y\). \(D\) is assigned uniformly at random to participants in the experiment. \(Y\) is chosen specifically to measure harm and may be different from the experiment's primary outcome of interest. For example, in a clinical trial, \(Y\) could convey a primary outcome such as mortality or a secondary outcome such as the occurrence of serious side effects. We allow for both scenarios, as experiments

Figure 1: Stopping tests applied homogeneously may not stop trials in which the treatment harms only a minority group of participants. For example, if the treatment harms 10% of all participants, the trial will stop less than \(20\%\) of the time, even if the harmful treatment effect is strong (boxed in red). We plot the stopping probability of a commonly used stopping test (Oâ€™Brien-Fleming) in situations where the treatment harms the minority group but has no effect on the majority group.

often care about harm on multiple dimensions. Assume that an increase in \(Y\) is harmful. The harm caused by the treatment can be measured by the average treatment effect (ATE) of \(D\) on \(Y\), \(=[Y|D=1]-[Y|D=0]\). A positive ATE implies that the treatment is harmful, while a negative ATE implies that it is beneficial. Note that the ATE has a natural estimator: the difference in the observed means of \(Y\) between the treated and untreated groups.

Now, it is not necessary that every individual in the population responds to the treatment in the same way. The treatment may harm some individuals, benefit some, and have no effect on others. Assume that the population can be divided into \(M\) discrete groups, where random variable \(G\) conveys group membership. For group \(g\), we define the conditional average treatment effect (CATE) \((g)=[Y|D=1,G=g]-[Y|D=0,G=g]\). Note that in general, we do not observe \(G\). Rather, we typically observe a set of covariates \(X\) that correlate with group membership. We define the CATE for a participant with covariates \(x\) as \((x)=_{g|x}[(g)]\).

The experiment can recruit a maximum of \(N\) participants and is run for \(T\) time steps, where the values of \(N\) and \(T\) are set at the experiment's start. For simplicity, assume that we recruit two participants at each time step--one in treatment and one in control--and so \(T=N/2\). For each participant \(i\), we measure treatment assignment \(d_{i}\), covariates \(x_{i}\), and observed outcome \(y_{i}\), but do not observe group membership \(g_{i}\). The expected effect of the treatment on participant \(i\) is given by \((g_{i})\). We assume that \((d_{1},x_{1},y_{1},g_{1}),,(d_{N},x_{N},y_{N},g_{N})\) are independent and identically distributed.

### Early Stopping for Harm

If the treatment is harmful, it is often important to end the experiment as early as possible, to minimize harm for participants and financial loss for the experimenter. Clinical trials and A/B tests usually build in interim checkpoints, where the data collected thus far is evaluated to make a decision on whether to continue the experiment. However, it is also important to _not_ stop the experiment at these checkpoints unless the treatment is actually harmful; incorrectly stopping early could result in lost innovation or societal benefit. Achieving this balance requires using dedicated statistical methods for early stopping, collectively known as "stopping tests."

A large body of literature has studied stopping tests. Frequentist methods like the Pocock adjustment , O'Brien-Fleming (OF) adjustment , and alpha-spending  are common in clinical trials with a pre-specified schedule of checkpoints. True sequential methods that allow for continuous monitoring are often based around Wald's Sequential Probability Ratio Test (SPRT) , such as the 2-SPRT , MaxSPRT , and mixture-SPRT (mSPRT) [26; 14]. Other sequential methods are based on martingales [3; 25] and testing by betting . Such tests are more practical in A/B testing scenarios, where investigators can monitor responses as they arrive. There is also a growing interest in Bayesian designs and stopping tests, both for clinical trials  and A/B tests . App. A summarizes some commonly-used stopping tests.

While these stopping tests differ in their details, many have the same general form: compute a test statistic from the hitherto observed data \(_{n}(y_{1:n},d_{1:n})\), then stop the experiment if this test statistic exceeds some bound \(b()\). Here, \(n\) is the number of outcomes observed thus far and \(\) is the desired bound on the rate of unnecessary stopping (i.e., stopping when the treatment is not harmful). For clarity, we provide a concrete example of a stopping test: the OF-adjusted z-test. Assume that \(Y(D,^{2})\), where \(^{2}\) is known but \(\) is not. The treatment effect is homogeneous, and so the ATE (i.e., \(\)) conveys if the treatment is harmful. To test between a null hypothesis of no harmful effect (\(H_{0}: 0\)) and an alternate hypothesis of a harmful effect (\(H_{1}:>0\)), we use

\[_{n}^{OF}=}{}}^{ n}y_{i}d_{i}}{n/2}-^{n}y_{i}(1-d_{i})}{n/2}. \]

The OF-adjusted z-test compares this test statistic to an appropriate bound \(b^{OF}()\), which depends both on the number and timing of checkpoints. For example, if we were to conduct one checkpoint halfway through the trial, then \(b^{OF}(0.05)=2.37\). If \(_{N/2}^{OF}>b^{OF}\), the test stops the experiment for harm; else, the experiment continues. Using the OF adjustment guarantees that if the treatment is not harmful (i.e., \( 0\)), then the probability of stopping the experiment is at most \(5\%\).

### Stopping with Heterogeneous Populations

Stopping tests are typically applied to the data in aggregate and define harm in terms of the ATE. While this approach is reasonable if the treatment effect is homogeneous, there is no guarantee that it will stop the experiment if the treatment only harms a subset of participants. For example, consider a situation in which there are two equally sized groups with equal but opposite treatment effects, that is, \(p(G=0)=p(G=1)\) and \((0)=-(1)\). The ATE with this mixture distribution is zero, and so any stopping test with \(H_{0}: 0\) is designed to continue to completion at least \((1-)\%\) of the time. Unfortunately, the failure to stop means that half of the trial participants will be harmed.

Experiments with heterogeneous populations thus require a more precise definition of harm. To comply with the bioethical principle of non-maleficence, experiments should be stopped not only if the treatment is harmful in aggregate (i.e., \(>0\)), but also if the treatment harms any group of participants (i.e., \(\ g\) s.t. \((g)>0\)). We define the null hypothesis of no group harm,

\[H_{0}:(g) 0\  g\{1,...,M\}. \]

An effective stopping test must fulfill two conditions. First, it must have high stopping probability if the treatment harms any group of participants. Second, it must limit the probability of stopping if no group of participants is harmed (i.e., limit the "rate of unnecessary stopping"). Note that these conditions are equivalent to high statistical power and low Type I error rate relative to (2).3 For the rest of the paper, we use these two metrics--the stopping probability if any group is harmed and the rate of unnecessary stopping--to evaluate stopping tests with heterogeneous populations.

Fig. 1 demonstrates the limitations of applying stopping tests homogeneously to heterogeneous populations. We simulate an experiment that runs for 1,000 time steps and recruits 2,000 participants. Participants come from two groups, with \(G\{0,1\}\) indicating membership in a minority group. The treatment has no effect on the majority group but harms the minority with treatment effect \(\), with \(Y( D G,^{2})\) and \(^{2}=1\). The trial has one checkpoint at the halfway stage and uses an OF-adjusted z-test to decide whether to stop for harm. We vary \(\) from 0 to 1 and \((G=1)\) from 0.1 to 0.5, and plot the stopping probability across 1,000 replications. We find that as the size of the harmed group decreases, stopping probability falls significantly. Continuing our warfarin example from Sec. 1, if elderly patients comprise 10% of trial participants (lowest line in Fig. 1), then the stopping probability is less than \(20\%\), even if the treatment has a large harmful effect (\(=1\)).

## 3 Causal Latent Analysis for Stopping Heterogeneously (CLASH)

Thus far, we have introduced early stopping for harm and established that existing methods applied homogeneously may not be effective when applied to heterogeneous populations. In this section, we first develop useful intuition on heterogeneous early stopping. We then present CLASH, our new two-stage approach. Finally, we theoretically establish that CLASH improves stopping probability if the treatment harms any group of participants without inflating the rate of unnecessary stopping.

### Motivation

The key problem with the homogeneous approach is that it averages over potential heterogeneous groups. One solution to this problem is to run a stopping test separately on different subsets of the population, but a question remains: which subset guarantees a high stopping probability? Assume for a moment that we have prior knowledge of each participant's group \(g_{i}\) and their CATE \((g_{i})\). Then, Prop. 3.1 (proved in App. E.1) establishes that for \(n\) sufficiently large, the stopping test that only considers participants from harmed groups stops with probability approaching \(1\).

**Proposition 3.1** (Group knowledge improves early stopping).: _Let \(^{*}=[(g_{1})|(g_{1})>0]\) denote the ATE for harmed groups and \(S^{*}_{n}=\{i n:(g_{i})>0\}\) the participants from harmed groups. Let \(^{*}_{n}=^{*}_{n}/^{*}_{n}\) denote the value of a test statistic computed only on \(S^{*}_{n}\), where \(\,^{*}_{n}=^{*}\), \((^{*}_{n}-^{*})/^{*}_{n}(0,1)\), \(_{n}|}^{*}_{n}^{*}\), and \(|S^{*}_{n}|\) diverges in probability to infinity. Fix any stopping threshold \(b()\). Then, the stopping test that only considers \(S^{*}_{n}\) has stopping probability converging to 1, i.e., \((^{*}_{n}>b()) 1\)._

The assumptions of Prop. 3.1 hold for many popular frequentist hypothesis tests, including an OF-adjusted z-test, an OF-adjusted t-test, and an OF-adjusted stopping test to detect differences in restricted mean survival time (RMST) for time-to-event data . To give a rough intuition of the proof, the assumed conditions imply that the mean of \(_{n}^{*}\) grows with \(|S_{n}^{*}|\) while its variance remains bounded, and so \(_{n}^{*}\) eventually exceeds any fixed bound. Prop. 3.1 establishes that to stop the experiment with high probability, the stopping test should be run only on the set of harmed groups \(S_{n}^{*}\). We emphasize that the same stopping test applied to the whole population has no such guarantees. For example, if the overall ATE is 0, then the stopping probability of the test applied homogeneously is \(\). Thus, running the stopping test only on \(S_{n}^{*}\) stops the trial with far higher probability.

One way to apply a stopping test only to the harmed groups is to assign a weight to each observation \(w_{i}^{*}=[i S_{n}^{*}]\), then reweight the test statistic by this indicator. For example, for the OF-adjusted z-test in (1), using the following weighted test statistic is equivalent to running the test only on \(S_{n}^{*}\),

\[_{n}^{*\,OF}=^{n}w_{i}^{*}}}{}} ^{n}w_{i}^{*}y_{i}d_{i}}{_{1=1}^{n}w_{i}^{*}d_{i}} -^{n}w_{i}^{*}y_{i}(1-d_{i})}{_{1=1}^{n}w_{i}^{*}(1-d_{i})} . \]

Prop. 3.1 implies that using \(_{n}^{*\,OF}\) leads to high stopping probability. We thus call \(w_{i}^{*}\) the optimal weights, as using them to reweight \(_{n}\) guarantees stopping probability close to \(1\) for large enough \(n\).

Of course, this insight is not of direct practical use, as it assumes knowledge of \((g_{i})\) and \(g_{i}\), neither of which is observed. However, it motivates how to construct better stopping tests for heterogeneous populations. If we can infer from the data which groups are harmed, then we can use this information to accelerate early stopping. Specifically, if we can estimate the optimal weights \(w_{i}^{*}\), then we can use these estimates to construct weighted tests that have high stopping probability. In contrast, stopping tests applied homogeneously will have no such guarantees. This is the insight that drives our method CLASH, which we present in the following section.

### Method

At any interim checkpoint \(n\), CLASH operates in two stages. In Stage 1, CLASH uses causal machine learning to estimate the probability that each participant \(i\) is harmed by the treatment. Then in Stage 2, it uses these inferred probabilities to reweight the test statistic of any chosen stopping test.

Stage 1: Estimate \(w_{i}^{*}\) We first estimate the treatment effect \((x_{i})\) for each participant \(i n\) using any method of heterogeneous treatment effect estimation. The specific choice of method is left to the investigator; potential options include linear models, causal forests , and meta-learners .

To maintain conditional independence between the estimated CATE and the observed outcome for participant \(i\), we exclude \((x_{i},y_{i})\) from the training set when estimating \((x_{i})\). We thus use \(_{n,-i}(x_{i})\) to denote the estimated CATE for participant \(i\), and \(_{n,-i}(x_{i})\) for the estimated standard error of \(_{n,-i}(x_{i})\) (further discussed below). While the notation suggests leave-one-out cross validation, the estimation can instead use k-fold or progressive cross-validation . We then set

\[_{i}^{n}=1-_{n,-i}(x_{i})}{_{n,-i}(x_{i})} \]

where \(\) is the cumulative distribution function (CDF) of the standard normal and the hyperparameter \(>0\) (discussed below) is a small number. Intuitively, \(_{i}^{n}\) conveys the probability that \(i\) was harmed by the treatment; we establish that it is a good estimate of the optimal \(w_{i}^{*}\) in Sec. 3.3. We also discuss the choice of \(\) in Sec. 3.3.

Stage 2: Weighted Early Stopping We now use \(_{i}^{n}\) to weight the test statistic of any existing stopping test. The choice of test is left to investigators based on their experiment and domain. For example, CLASH with the OF-adjusted z-test would use the test statistic from (3), replacing \(w_{i}^{*}\) with \(_{i}^{n}\). We provide examples of weighted test statistics for other stopping tests in App. C.

We make three important practical notes. First, cross-validation (CV) in Stage 1 is employed to prevent unnecessary stopping with small samples. CV ensures that if the treatment has no effect, then the predicted CATE for participant \(i\) will be independent of its observed outcome. Without this independence, outcomes that are large by random chance could be assigned high weights, especially in small samples. Second, most CATE estimation methods are designed to correct for confounding, which is not required in the experimental setting since the treatment is randomly administered. This simplifies the estimation problem (see App. D). Third, the CLASH weights (4) require not only point estimates of the CATE for each participant, but also standard errors. While bootstrapping can be used to obtain standard errors for any CATE estimation method, it is often computationally expensive. To save time and resources, investigators can use methods that compute standard errors during the process of fitting (e.g., causal forests or linear models).

Finally, we briefly discuss how to set the hyperparameter \(\). For our theoretical guarantees to hold (Sec. 3.3), \(\) must be set to a number smaller than the smallest harmful treatment effect. This choice ensures that \(_{i}^{n}\) converges to \(w_{i}^{*}\) for all values of \((x_{i})\). In practice, we find that it is sufficient to set \(\) to a number that reflects the experiment's minimum effect size of interest. It is standard for clinical trials and A/B tests to specify this effect size, especially for _pre-hoc_ power calculations . Thus, \(\) can be set using the standard experimental procedures.

### Properties

Recall that an effective method of heterogeneous early stopping must 1) have high stopping probability if any group of participants is harmed and 2) limit the rate of unnecessary stopping. CLASH achieves both of these criteria by constructing weights \(_{i}^{n}\) that converge in probability to the optimal weights \(w_{i}^{*}\)\(\)\(i\) (Thm. 3.2). Thus, for \(n\) large enough, CLASH is comparable to a stopping test run only on the harmed population \(S_{n}^{*}\) and hence has high stopping probability in the presence of harm (Prop. 3.1). Further, if no group is harmed, the CLASH weights converge quickly to zero. Hence, the test statistic used in Stage 2 converges to zero, as does the probability of unnecessary stopping (Thm. 3.3). Thus, CLASH has high statistical power and limits the Type I error rate relative to the null hypothesis (2).

Our first theorem (proved in App. E.2) establishes the convergence of the CLASH weights to the optimal weights. It makes mild assumptions on the quality of CATE estimation in Stage 1. The choice of CATE estimator may demand further assumptions on the data; for example, using a linear regression provides suitable estimates if \((x)\) is linear in \(x\). Alternative causal machine learning methods minimize these assumptions; for example, a causal forest provides sufficiently accurate estimates under weak regularity conditions (e.g., bounded \(x\)[38, Thm. 4.1]).

**Theorem 3.2** (CLASH weights converge to optimal weights).: _CLASH weights (4) satisfy the error bound_

\[|_{i}^{n}-w_{i}^{*}|-)- )^{2}}{2_{n,-i}^{2}(x_{i})}}~{}+~{}_{n,-i} (x_{i})-(x_{i})|}{_{n,-i}(x_{i})}-)-)^{2}}{2_{n,-i}^{2}(x_{i})}+)-||_{n,-i}(x_{i})-(x_{i})|}{_{n,-i}^{2}(x_{i})}}.\]

_Moreover, if \(<_{x:(x)>0}(x)\) and, given \(x_{i}\), \(_{n,-i}(x_{i})(x_{i})\) and \(_{n,-i}(x_{i})0\), then \(_{i}^{n}\) is a consistent estimator of the optimal weight: \(_{i}^{n}-w_{i}^{*}0\)._

Thm. 3.2 provides important intuition on when CLASH will perform well. For observation \(i\), the rate of weight convergence depends on two factors. First, it is proportional to \((x_{i})\), the size of the harmful effect. The more harmful the treatment, the better CLASH will perform. Second, it is inversely proportional to \(_{n,-i}(x_{i})\), the uncertainty in the estimate of \((x_{i})\). The more certain the estimate, the better CLASH will perform. Many factors impact \(_{n,-i}(x_{i})\), including \(n\) and the frequency of observing covariates like \(x_{i}\). We expect CLASH to perform better with (1) larger datasets, (2) fewer covariates, and (3) larger \(S_{n}^{*}\). We explore these relationships further in our experiments (Sec. 4-5).

We emphasize that Thm. 3.2 implies a fast rate of convergence, as the error bound decays exponentially in \(1/_{n,-i}^{2}(x_{i})\). For example, if \(_{n,-i}^{2}(x_{i})=o_{p}(1/ n)\), then \(|_{i}^{n}-w_{i}^{*}|=o_{p}(1/n)\) (see Cor. F.1). This is extremely fast: weights and other nuisance parameters are often estimated at a rate of \(1/\) or slower . This speed justifies our use of the Gaussian CDF in (4). While other CDFs can provide Thm. 3.2's consistency result, they may not demonstrate such provably fast convergence. Note that the additional assumption on \(_{n,-i}^{2}(x_{i})\) is mild,4 as it just needs to decay faster than \(1/ n\). For example, a linear regression under standard assumptions (as in ) has \(_{n,-i}(x_{i})=_{p}(1/)\).

Our next theorem (proved in App. E.3) establishes that CLASH limits the rate of unnecessary stopping for the OF-adjusted z-test. If no group of participants is harmed, CLASH's stopping probability converges to zero. The rate of unnecessary stopping (i.e., Type I error rate relative to (2)) is thus guaranteed to be \(\) for \(n\) large enough. The theorem requires a mild condition on the decay of \(_{i n}_{n,-i}^{2}(x_{i})\) (the same discussed above). It also assumes bounded outcomes, a condition satisfied in most real experiments. Thm. F.2 provides a similar result for an SPRT stopping test.

**Theorem 3.3** (CLASH limits unnecessary stopping).: _Consider a stopping test with weighted z-statistic (3) and weights estimated using CLASH. If \(_{i n}_{n,-i}^{2}(x_{i})=o_{p}(1/(n))\), \(_{i n}|(x_{i})-_{n,-i}(x_{i})|=o_{p}(1)\), and \(y_{i}\) are uniformly bounded, then the stopping probability of the test converges to zero if no participant group is harmed._We note that our presented theory is asymptotic, not exact. However, many stopping tests--both frequentist and sequential--rely on asymptotic approximations and so large enough sample sizes are often already required. For example, the mSPRT stopping test with binary outcomes in  requires a large-sample Gaussian approximation. In our simulation experiments (Sec. 4) and real-world application (Sec. 5), we show that sample sizes typical in many clinical trials and A/B tests are sufficient for CLASH to improve stopping probability while limiting unnecessary stopping.

### The Decision to Stop

Before proceeding to our experimental evaluation, we emphasize that CLASH is not intended to automate human decisions on early stopping. When CLASH indicates that an experiment should be stopped, the investigator can make one of several decisions. If the nature of harm is serious (e.g., mortality in a clinical trial), the investigator may decide to stop the experiment for all participants. For milder harms (e.g., high latency during application startup in an A/B test), they may decide to stop the experiment only for the harmed group and continue it for all other participants. If the identified harm is much less consequential than the potential benefit (e.g., harm of increased headaches vs. benefit of curing cancer), the investigator may decide to not stop the experiment at all. The specific decision must depend heavily on the ethical and financial aspects of the experiment, a thorough review of the interim data, guidance from the investigator's institutional review board, and other relevant factors.

If the investigator chooses to stop the experiment only on the harmed group, they face two practical challenges: identifying the harmed group and ATE estimation. To identify the harmed group, investigators can either use existing subgroup identification methods (e.g., ) on the observed outcomes or analyze the CLASH weights to find covariate combinations with weights close to \(1\). To estimate the whole population ATE at the end of the experiment, investigators can use inverse propensity weights; this approach allows one to correct for the selection bias induced by stopping the experiment only in one group (see example in Tab. S6). Further details, including ensuring actionable group sizes, are discussed in App. G.

## 4 Simulation Experiments

We now assess the performance of CLASH in an extensive series of simulation experiments.5 We focus on two important types of experimental outcomes: Gaussian and time-to-event (TTE). Gaussian outcomes are the most frequently considered (e.g., ), as they are common in both clinical trials and A/B tests. Many stopping tests assume either that the data is Gaussian or that the average outcome is asymptotically Gaussian. Meanwhile, TTE outcomes are very common in clinical trials. We establish that CLASH improves stopping times over relevant baselines in both these scenarios. Our real-world application (Sec. 5) then applies CLASH to outcomes that are approximately negative binomial, further demonstrating that CLASH can be impactful in experiments across several domains.

**Setup: Gaussian Outcomes**  We consider a randomized experiment that evaluates the effect of \(D\) on \(Y\). Participants come from two groups, with \(G\{0,1\}\) indicating membership in a minority group. We do not observe \(G\), but observe a set of binary covariates \(X=[X_{1},..,X_{d}]\), where \(d\) varies between \(3\) and \(10\) and \(p(X_{j}=1)=0.5\)\(\)\(j\). \(X\) maps deterministically to \(G\), with \(G=_{j=1}^{k}X_{j}\).6 We vary \(k\) between \(1\) and \(3\): the expected size of the minority thus varies between 12.5% and 50% (of all participants). \(Y\) is normally distributed within each group, with \(Y|G=0(_{0}D,1)\) and \(Y|G=1(_{1}D,1)\). The distribution of \(Y\) over the whole population is thus a mixture of two Gaussians. We vary \(_{0}\) between 0 and -0.1; the majority group is thus unaffected or benefited by the treatment. We vary \(_{1}\) between 0 and 1: the minority group is thus either unaffected or harmed.

The experiment runs for \(N=4000\) participants and \(T=2000\) time steps, recruiting one treated and one untreated participant at each step.7 The experiment has three checkpoints, with 1,000, 2,000, and 3,000 participants (corresponding to 25%, 50%, and 75% of the total time duration). At each checkpoint, we run CLASH and compute its stopping probability across 1,000 replications. In Stage 1, CLASH uses a causal forest with 5-fold CV and \(=0.1\). Standard errors were estimated by the causal forest itself (i.e., not via the bootstrap). In Stage 2, CLASH uses one of four commonly-used stopping tests: an OF-adjusted z-test, an mSPRT , a MaxSPRT , and a Bayesian estimation-based test.

We compare CLASH's stopping probability to three alternative approaches (two baselines and one oracle). The **homogeneous baseline** applies the four aforementioned stopping tests to the collected data in aggregate. The **SUBTLE baseline** uses a recently-developed heterogeneous stopping test . SUBTLE is the only existing approach that can handle unknown groups without strong parametric assumptions (e.g., linearity). SUBTLE builds on the mSPRT framework and has the same decision rule; we thus compare its performance to CLASH using mSPRT in Stage 2. The **Oracle** applies the four aforementioned stopping tests only to data from the harmed group. This approach reflects the optimal test discussed in Prop. 3.1, and represents an upper bound on performance.

**Setup: TTE Outcomes** We consider a clinical trial that measures time to a positive event (e.g., remission) and defines treatment effects using the hazard ratio. We adapt the simulation setup from ; see App. H.2.1 for a full description. CLASH's Stage 1 uses a survival causal forest  and Stage 2 uses an OF-adjusted Cox regression. Note that existing SPRTs cannot be applied to TTE outcomes, as they cannot account for repeated observations for each participant. Crucially, SUBTLE cannot be used either; CLASH is thus the first heterogeneous method applicable to this setting.

**Results** CLASH outperforms both the homogeneous baseline and SUBTLE in a wide range of experiments. We first discuss performance in the Gaussian setting with five covariates and a minority group that represents 12.5% of participants. Fig. 2 plots the probability that the experiment is stopped at any of the three checkpoints. If the minority group is harmed (x-axis \(_{1}>0\)), CLASH stops the experiment significantly more often than the homogeneous and SUBTLE baselines. The magnitude of the improvement depends on the effect size: the larger the effect, the better CLASH performs. For large effects, CLASH even converges to the oracle. Crucially, CLASH does not increase the rate of unnecessary stopping: if the minority group is unharmed (\(_{1}=0\)), CLASH stops the experiment no more frequently than either baseline (i.e., CLASH controls the Type I error rate). These results

Figure 2: Performance of CLASH in a simulation experiment with Gaussian outcomes. If the minority group is harmed, CLASH (red) significantly increases the stopping probability over the homogeneous baseline (blue) and SUBTLE (pink). For large effect sizes, CLASH is as effective as an oracle that has prior knowledge of the harmed group (green). Crucially, if the treatment has no harmful effect (i.e., effect = 0), CLASH does not stop the trial more often than either baseline (i.e., CLASH limits Type I error rate). These results hold for four commonly-used stopping tests (corresponding to each column). SUBTLE builds upon the mSPRT framework specifically, so is only presented in the second column. These results hold whether the majority groupâ€™s treatment is beneficial (top row) or has no effect (bottom row). We simulate 1,000 trials with 4,000 participants each and a 12.5% minority group; we plot the stopping probability (with 95% CIs) at any interim checkpoint.

hold for all four stopping tests; CLASH thus leads to effective stopping no matter which test is used. Moreover, its performance gains are robust to the specific choice of the hyperparameter \(\) (Fig. S5).

We now compare CLASH to SUBTLE, the heterogeneous baseline presented in the second column of Fig. 2. CLASH improves stopping probability over SUBTLE whether the treatment benefits the majority group or has no effect. We focus on the former case and plot the increase in stopping probability of CLASH (using mSPRT) over SUBTLE in Fig. 3. CLASH improves stopping probability in a large number of scenarios. We make three specific notes. First, when the minority group is larger, CLASH performs well for a broader range of effect sizes. Notably, SUBTLE greatly underperforms CLASH for medium effects (though it is able to detect large effects as often as CLASH). Second, increasing the number of covariates only has a small impact on CLASH, which outperforms SUBTLE despite the higher dimensionality. Third, CLASH greatly increases the probability of stopping the trial at the first and second checkpoints. CLASH thus not only stops trials more often, but also faster.

CLASH also demonstrates strong performance with TTE outcomes (App. H.2). CLASH improves the stopping probability of the OF-adjusted Cox regression by \(20\) percentage points if the treatment harms 25% of the population with hazard ratio of \(0.7\). No existing method for heterogeneous stopping (including SUBTLE) can handle TTE; this broad applicability is one of CLASH's major advantages.

App. H presents detailed simulation results that further demonstrate CLASH's efficacy. We note the discussed results are robust across a range of simulation settings, including effect size, sample size, covariate dimensionality, harmed group size, and outcome variance. Furthermore, CLASH is highly effective in situations with multiple harmed groups, noisy group membership, and unknown outcome variance. While CLASH broadly outperforms existing methods, we note two specific limitations. First, CLASH may struggle to detect harmful effects in very small samples (e.g, N=200, see Fig. S10). Note that this is a challenging setting for any method, since we only observe 10-20 data points from the harmed group at any interim checkpoint. Second, CLASH may struggle with a very large number of covariates (e.g., 500 covariates, see Fig. S11). In such settings, we recommend performing feature selection before running CLASH.

## 5 Real-world Application

In this section, we apply CLASH to real data from a digital experiment. We consider an A/B test run by a technology company to evaluate the effect of a software update on user experience. The dataset was collected in 2022 and consists of 500,000 participants, roughly half of whom received the update.

Figure 3: CLASHâ€™s performance improvement over the SUBTLE baseline with Gaussian outcomes. CLASH significantly increases stopping probability over SUBTLE across a range of effect sizes (x-axis) and covariate dimensions (rows). CLASH not only stops more often but also faster, with higher stopping probability at earlier checkpoints (columns). CLASH does not increase stopping probability if the treatment is not harmful (i.e., effect = 0). We plot the difference in stopping probability between CLASH and SUBTLE (with 95% CIs), where the minority group forms (a) 12.5% or (b) 25% of all participants. For the larger group size, CLASHâ€™s improvement is most notable for medium effects, which SUBTLE struggles to detect. All performance increases are robust to an increase in the number of covariates. See App. H.1 for a similar comparison of CLASH to the homogeneous baseline.

We evaluate this treatment's effect on a proprietary metric that measures negative user experience. An increase in the outcome--which is discrete and right-skewed--indicates a worse user experience. The experiment also recorded which one of eight geographic regions the user's device was located in. We first use the full dataset to assess whether the treatment had a heterogeneous impact by region. Separate negative binomial (NB) regressions indicate that the treatment led to a large statistically significant increase in the metric in Region 1, a small significant increase in Regions 2, 3, and 4, no significant effect in Regions 5 and 6, and a significant decrease in Regions 7 and 8 (Tab. S3).

We evaluate stopping tests on this dataset, conducting checkpoints every 20,000 participants. We run CLASH with a causal forest and \(=1\) in Stage 1 and an OF-adjusted NB regression in Stage 2. We find that CLASH decreases stopping time over the homogeneous baseline, stopping the experiment after 40,000 as opposed to 60,000 participants. SUBTLE stops after 80,000 participants; this under-performance may result from the data's strong non-normality. An oracle that only considers Region 1 also stops after 40,000 observations (i.e., same as CLASH). We note that CLASH's optimal performance is not a fluke: across 1,000 random shuffles of the dataset, CLASH stops the experiment at the same interim checkpoint as the Oracle in 62.6% of shuffles (details in Tab. S5).

We emphasize that by stopping the experiment early, the company can minimize customer frustration, preventing churn and financial impact. Domain expertise can guide whether to stop the experiment altogether, or just in the harmed regions. The harmed regions can be identified at stopping time using separate regressions (Tab. S4), CLASH weights (Fig. S19), or policy learning . Note that stopping the experiment just in one region would affect statistical inference at the end of the experiment, as the treatment would no longer be randomly assigned across regions. As described in Sec. 3.4, investigators can use inverse probability weighting to adjust for this selection (Tab. S6).

Finally, we study the impact of sample size on CLASH's performance. We vary sample size \(N_{s}\) between 10,000 and 100,000, and generate 1,000 experiments for each \(N_{s}\) by sampling from our dataset. We only sample from Region 1 and Regions 5-8; this gives us one harmed group (Region 1) that comprises \(29\%\) of the total population (see App. I for results with Regions 2-4). We conduct one checkpoint at the halfway stage of each simulated experiment. We find that CLASH significantly improves stopping probability over the homogeneous baseline for all \(N_{s}\) 40,000, and converges to near-oracle performance around \(N_{s}=\) 90,000 (Fig. 4). These sizes represent \(8\%\) and \(18\%\) of the total sample of the A/B test. This result is encouraging, as it indicates that datasets that are a fraction of the size of typical online A/B tests are large enough for our asymptotic theory (Sec. 3.3) to hold.

## 6 Discussion and Limitations

We propose a new method CLASH for the early stopping of randomized experiments on heterogeneous populations. CLASH stops experiments more often than existing approaches if the treatment harms only a subset of participants, and does not stop experiments if the treatment is not harmful. Prior work is either limited by restrictive assumptions (e.g., linearity) or incompatible with common data distributions (e.g., time-to-event). In contrast, CLASH is easy to use, adapts any existing stopping test, and is broadly applicable to clinical trials and A/B tests. Our work has a few important limitations. First, CLASH works better with a relatively small number of covariates. While this is not a problem for most experiments--clinical trials and A/B tests typically collect only a few covariates--a different pre-processing approach may be required in high-dimensional settings (e.g., genetic data with over 500 covariates). Second, CLASH may not detect harm in experiments with a very small number of participants. For example, sample sizes typical in Phase 1 clinical trials (i.e., less than 100 participants) may be too small for CLASH to be more effective than existing approaches. Finally, this paper only considers stopping for harm. In practice, experiments are often stopped for early signs of benefit and futility. Expanding CLASH to these decisions is an important direction for future work.

Figure 4: CLASH performance by sample size on real data. We plot stopping probability across 1,000 experiments with 95% CIs.