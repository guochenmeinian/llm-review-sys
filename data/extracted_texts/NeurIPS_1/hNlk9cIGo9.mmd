# Faster Algorithms for User-Level Private Stochastic Convex Optimization+

Footnote â€ : Authors are listed reverse alphabetically.

Andrew Lowy

Wisconsin Institute for Discovery

University of Wisconsin-Madison

alowy@wisc.edu

Daogao Liu

Department of Computer Science

University of Washington

liudaogao@gmail.com

Hilal Asi

Apple Machine Learning Research

hilal.asi94@gmail.com

###### Abstract

We study private stochastic convex optimization (SCO) under user-level differential privacy (DP) constraints. In this setting, there are \(n\) users (e.g., cell phones), each possessing \(m\) data items (e.g., text messages), and we need to protect the privacy of each user's entire collection of data items. Existing algorithms for user-level DP SCO are impractical in many large-scale machine learning scenarios because: (i) they make restrictive assumptions on the smoothness parameter of the loss function and require the number of users to grow polynomially with the dimension of the parameter space; or (ii) they are prohibitively slow, requiring at least \((mn)^{3/2}\) gradient computations for smooth losses and \((mn)^{3}\) computations for non-smooth losses. To address these limitations, we provide novel user-level DP algorithms with state-of-the-art excess risk and runtime guarantees, without stringent assumptions. First, we develop a _linear-time_ algorithm with state-of-the-art excess risk (for a non-trivial linear-time algorithm) under a mild smoothness assumption. Our second algorithm applies to arbitrary smooth losses and achieves _optimal excess risk_ in \((mn)^{9/8}\) gradient computations. Third, for _non-smooth_ loss functions, we obtain _optimal excess risk_ in \(n^{11/8}m^{5/4}\) gradient computations. Moreover, our algorithms do not require the number of users to grow polynomially with the dimension.

## 1 Introduction

The increasing ubiquity of machine learning (ML) systems in industry and society has sparked serious concerns about the privacy of the personal data used to train these systems. Much work has shown that ML models may violate individuals' privacy by leaking their sensitive training data . For instance, large language models (LLMs) are vulnerable to black-box attacks that extract individual training examples . _Differential privacy_ (DP)  prevents ML models from leaking their training data.

The classical definition of differential privacy--_item-level differential privacy_--is ill-suited for many modern applications. Item-level DP ensures that the inclusion or exclusion of any _one training example_ has a negligible impact on the model's outputs. _If each person (a.k.a. user) contributes only one piece of training data_, then item-level DP provides a strong guarantee that each user's data cannot be leaked. However, in many modern ML applications, such as training LLMs on users' data in federated learning, each user contributes a large number of training examples .

In such scenarios, the privacy protection that item-level DP provides for each user is insufficiently weak.

_User-level differential privacy_ is a stronger privacy notion that addresses the above shortcoming of item-level DP. Informally, user-level DP ensures that the inclusion or exclusion of any _one user's entire training data_ (\(m\) samples) has a negligible impact on the model's outputs. Thus, user-level DP provides a strong guarantee that no user's data can be leaked, even when users contribute many training examples.

A fundamental problem in (private) machine learning is _stochastic convex optimization_ (SCO): given a data set \(=(Z_{1},,Z_{n})\) from \(n\) i.i.d. users, each possessing \(m\) i.i.d. samples from an unknown distribution \(Z_{i} P^{m}\) our goal is to approximately minimize the expected population loss

\[F(x):=_{z P}[f(x,z)].\]

Here, \(f:\) is a loss function (e.g., cross-entropy loss), \(^{d}\) is the parameter domain, and \(\) is the data universe. We require that the output of the optimization algorithm \(:^{mn}\) satisfies user-level DP (Definition 1.3). We measure the accuracy of \(\) by its _excess (population) risk_

\[F(())-F^{*}:=_{, P^{nm}}F(())-_{x}F(x).\]

Given the practical importance of user-level DP SCO, it is unsurprising that many prior works have studied this problem. The work of  initiated this line of work, and provided an excess risk lower bound of \((1/+/( n))\), where \(\) is the privacy parameter. However, their upper bound was suboptimal and required strong assumptions. The work of  gave an algorithm that achieves optimal risk for \(\)-smooth losses with \(<(n/ n^{3/2}/(d))\), provided that \(n/\) and \(m(,n^{2}/)\). _These assumptions are restrictive_ in large-scale applications with a large number of examples per user \(m\) or when the number of model parameters \(d\) is large. For example, in deep learning, we often have \(d n\) and an enormous smoothness parameter \( 1\). Moreover, their algorithm requires \(mn^{3/2}\) gradient evaluations, making it slow when the number of users \(n\) is large.2 The work of  gave another user-level DP algorithm that only requires \(n(d)/\), but unfortunately their algorithm does not run in polynomial-time.

To address the deficiencies of previous works on user-level DP SCO, the recent work  provided an algorithm that achieves optimal excess risk in polynomial-time, while also only requiring \(n(md)/\) users. Moreover, their algorithm also works for non-smooth losses. The drawback of  is that it is even _slower_ than the algorithm of : for \(\)-smooth losses, their algorithm requires \((nm)^{3/2}\) gradient evaluations; for non-smooth losses, their algorithm requires \((nm)^{3}\) evaluations.

Evidently, _the runtime requirements and parameter restrictions of existing algorithms for user-level DP SCO are prohibitive in many important ML applications_. Thus, an important question is:

**Question 1.** Can we develop _faster_ user-level DP algorithms that achieve _optimal_ excess risk _without restrictive assumptions_?

Contribution 1.We give a positive answer to Question 1, providing a novel algorithm that achieves optimal excess risk using \(\{^{1/4}(nm)^{9/8},\,^{1/2}n^{1/4}m^{5/4}\}\) gradient computations for \(\)-smooth loss functions, with any \(<\) (Theorem 3.2). For non-smooth loss functions, our algorithm achieves optimal excess risk using \(n^{11/8}m^{5/4}\) gradient evaluations for non-smooth loss functions (Theorem 4.1). _Our runtime bounds dominate those of all prior works_ in every applicable parameter regime, by polynomial factors in \(n,m,\) and \(d\). Moreover, our results only require \(n^{1-o(1)}(d)/\) users. See Table 1 for a comparison of our results vs. prior works. For example, _for non-smooth loss functions, our optimal algorithm is faster than the previous state-of-the-art  by a multiplicative factor of \(n^{13/8}m^{7/4}\)._ For smooth loss functions, our optimal algorithm is faster than  by a factor of \((nm)^{3/8}^{3/4}\) (in the typical parameter regime when \(n^{7} m\)).

Linear-Time AlgorithmsThe "holy grail" of DP SCO is a _linear-time_ algorithm with optimal excess risk, which is unimprovable both in terms of runtime and accuracy. In the _item-level_ DP setting, such algorithms are known to exist for smooth loss functions .  proposed an interesting open question: is there a _user-level_ DP algorithm that achieves optimal excess risk in linear time for smooth functions? For our second contribution, we make progress towards answering this question.

Existing techniques for user-level DP SCO are not well-suited for linear-time algorithms. Indeed, the only prior non-trivial linear-time algorithm is the user-level LDP algorithm of .3 Their algorithm can achieve excess risk \( 1/+/()\). Unfortunately, however, their algorithm requires a very stringent assumption on the smoothness parameter \(</(md^{3})}\), which is unlikely to hold for large-scale ML problems. Further, the result of  requires the number of users queried in each round to grow polynomially with the dimension \(d\), and it assumes \(m<d<n\). _These assumptions severely limit the applicability of [1, Algorithm 5] in practical ML scenarios_. This leads us to:

**Question 2.** Can we develop a _linear-time_ user-level DP algorithm with state-of-the-art excess risk, _without restrictive assumptions_?

Contribution 2.We answer Question 2 affirmatively in Theorem 2.1: under a very mild requirement on the smoothness parameter \(<\), our novel linear-time algorithm achieves excess risk of \( 1/+/()\). Moreover, our algorithm does not require the number of users to grow polynomially in the dimension \(d\), and our result holds for any values of \(m,d,\) and \(n\). Thus, our algorithm has excess risk matching that of , but is much more widely applicable.

### Techniques

We develop novel techniques and algorithms to achieve new state-of-the-art results in user-level DP SCO. Before discussing our techniques, let us review the key ideas from prior works that we build on.

The goal of prior works  was to develop user-level analogs of DP-SGD , which is optimal in the item-level setting. To do so, they observed that each user \(i\)'s gradient \(_{j=1}^{m} f(x,Z_{i,j})\) lies in a ball of radius \( 1/\) around the population gradient \( F(x)\) with high probability, if the data is i.i.d. (\(Z_{i} P^{m}\)). Consequently, if the data is i.i.d., then replacing one user \(Z_{i}\) by another user \(Z^{}_{i}^{}\) will not change the empirical gradient \( F_{}(x)\) by too much: \(\| F_{}(x)- F_{^{}}(x)\| 1/(n )\) with high probability. Thus, one would hope for a method to privatize \( F_{}(x)\) by adding noise that scales with \(1/(n)\)--rather than \(1/n\)--which would allow for optimal excess risk.  devised such a method, which was inspired by FriendlyCore . Their method privately detects and removes "outlier" user gradients, and then adds noise to the average of the "inlier" user gradients. This outlier-removal procedure ensures privacy with noise scaling with \(1/(n)\), provided \(n 1/\). Moreover, when the data is i.i.d., no

Figure 1: Optimal algorithms for user-level DP SCO. We omit logarithms, fix \(L=R=1=\) and \(n=d\).

outliers will be removed with high probability, leading to a nearly unbiased estimator of the empirical gradient.

Our algorithms apply variations of the outlier-removal idea of  in novel ways.

Our linear-time Algorithm 1 takes a different approach to outlier removal, compared to prior works. Instead of removing outlier _gradients_, we aim to detect and remove outlier SGD _iterates_.4 The high-level idea of our algorithm is to partition the \(n\) users into \(C 1/\) groups, with each group containing \( n\) users. For each group of users, we run \(T mn\) steps of online SGD using the samples in this group and obtain the average iterate of each group: \(\{_{j}\}_{j=1}^{C}\). We then _privately identify and remove the outlier iterates_ from \(\{_{j}\}_{j=1}^{C}\). In order to successfully do so, we need to argue that if we run online SGD independently on user \(Z\) and user \(Z^{}\) to obtain \(\) and \(^{}\) respectively, then \(\|-^{}\|\) with high probability, where \(\) is the SGD step size. We prove such a stability bound in Lemma 2.3, which we hope will be of independent interest. By repeating the above process \((n)\) times and using iterative _localization_, we obtain our state-of-the-art linear-time result.

Our second algorithm, Algorithm 3, builds on  in a different way. In Algorithm 3, we apply an outlier-removal procedure to users' gradients. However, unlike , we draw random _minibatches_ of users in each iteration and apply outlier-removal to these minibatches. To make this procedure private while also achieving optimal excess risk, we combine _AboveThreshold_ with _privacy amplification by subsampling_. We then develop an _accelerated_ user-level DP algorithm that solves a carefully chosen sequence of regularized ERM problems, and applies localization in the spirit of . An obstacle that arises when we try to extend the ERM-based localization framework to the user-level DP setting is getting a tight bound on the variance of our minibatch stochastic gradient estimator that scales with \(1/m\). We overcome this obstacle in Lemma 3.5, by appealing to the _stability of user-level DP_. To handle non-smooth loss functions, we apply randomized smoothing to our accelerated algorithm.

### Preliminaries

We consider loss functions \(f:\), where \(\) is a convex parameter domain and \(\) is a data universe. Let \(P\) be an unknown data distribution and \(F(x):=_{z P}[f(x,z)]\) be the population loss function. Denote \(F^{*}:=_{x}F(x)\). The SCO problem is \(_{x}F(x)\). Let \(\|\|\) denote the \(_{2}\) norm. \(_{}(u):=_{x}\|u-x\|^{2}\) denotes projection onto \(\).

Assumptions and Notation.Function \(g:\) is \(L\)-_Lipschitz_ if \(|g(x)-g(x^{})| L\|x-x^{}\|_{2}\) for all \(x,x^{}\). Function \(g:\) is \(\)-_smooth_ if \(g\) is differentiable and has \(\)-Lipschitz gradient: \(\| g(x)- g(x^{})\|_{2}\|x-x^{}\|_{2}\). Function \(g:\) is \(\)-_strongly convex_ if \(g( x+(1-)x^{}) g(x)+(1-)g(x^{})- {(1-)}{2}\|x-x^{}\|^{2}\) for all \(\) and all \(x,x^{}\). If \(=0\), we say \(g\) is _convex_.

**Assumption 1.1**.:
1. _The convex set_ \(\) _is compact with_ \(\|x-x^{}\| R\) _for all_ \(x,x^{}\)_._
2. _The loss function_ \(f(,z)\) _is_ \(L\)_-Lipschitz and convex for all_ \(z\)_._

In all of the paper _except for Section_ 4_, we will also assume:

**Assumption 1.2**.: _The loss function \(f(,z)\) is \(\)-smooth for all \(z\)._

Denote \(a b:=(a,b)\). For functions \(f\) and \(g\) of input parameters \(\), we write \(f g\) if there is an absolute constant \(C>0\) such that \(f() Cg()\) for all permissible values of \(\). We use \(\) to hide logarithmic factors. Write \(a(b)\) if there exists some large \(J>1\) for which \(a b^{J}\).

Differential Privacy.**Definition 1.3** (User-Level Differential Privacy).: Let \( 0,\ [0,1)\). Randomized algorithm \(:^{nm}\) is \((,)\)_-user-level differentially private_ (DP) if for any two datasets \(=(Z_{1},,Z_{n})\) and \(^{}=(Z^{}_{1},,Z^{}_{n})\) that differ in one user's data (say \(Z_{i} Z^{}_{i}\) but \(Z_{j}=Z^{}_{j}\) for \(j i\)), we have

\[(() S) e^{}( (^{}) S)+,\]for all measurable subsets \(S\).

Definition 1.3 prevents any adversary from learning much more about an individual's data set than if that data had not been used for training. Appendix A contains the necessary background on DP.

### Roadmap

We begin with our state-of-the-art linear-time algorithm in Section 2. In Section 3, we present our error-optimal algorithm with state-of-the-art runtime for smooth loss functions. Section 4 extends our fast optimal algorithm to non-smooth loss functions. We conclude in Section 5 with a discussion and guidance on future research directions stemming from our work.

## 2 A state-of-the-art linear-time algorithm for user-level DP SCO

In this section, we develop a new algorithm (Algorithm 1) for user-level DP SCO that runs in linear time and has state-of-the-art excess risk, without requiring any impractical assumptions. The algorithm can be seen as a user-level DP variation of the localized phased SGD of : we execute a sequence of SGD trajectories with geometrically decaying step sizes, shrinking both the expected distance to the population minimizer and the privacy noise over a logarithmic number of phases.

In each phase \(i\), we first re-set algorithmic parameters and draw a disjoint set of \(n_{i}\) users \(D_{i}\) (lines 4-5). We further partition \(D_{i}\) into \(C\) disjoint subsets \(\{D_{i,j}\}_{j=1}^{C}\). For each \(j[C]\), we pool together all of the \(n_{i}m\) samples in \(D_{i,j}\) and run one-pass online SGD on \(D_{i,j}\) with initial point \(x_{i-1}\) given to us from the previous phase. Next, in lines 10-20, we privately detect and remove "outliers" from \(\{_{i,j}\}_{j=1}^{C}\). That is, our goal is to privately select a subset \(_{i}\{_{i,j}\}_{j=1}^{C}\), such that for any two points \(_{i,j},_{i,j^{}}_{i}\), \(\|_{i,j}-_{i,j^{}}\|_{i}=(_{ i}L})\). This will enable us to add noise scaling with \(_{i}\) in line 22, rather than with the much larger worst-case sensitivity (that scales linearly with \(T_{i}\)). In order to privately select such a subset \(_{i}\), we first compute (and privatize) the _concentration score_ for \(\{_{i,j}\}_{j=1}^{C}\) in line 10. A small concentration score indicates that outlier removal is doomed to fail and we must halt the algorithm to avoid breaching the privacy constraint. A large concentration score indicates that \(\{_{i,j}\}_{j=1}^{C}\) is nearly \(_{i}\)-concentrated and we may proceed with outlier removal in lines 12-15.

**Theorem 2.1** (Privacy and utility of Algorithm 1 - Informal).: _Let \( 10\), \(n^{1-o(1)}\), \((L/R)\), and \(m poly(n)\). Then, Algorithm 1 is \((,)\)-user-level DP. Further,_

\[F(x_{l})-F^{*} LR(}+}{} ).\]

_The gradient complexity of Algorithm 1 is \(nm\)._

_Remark 2.2_ (State-of-the-art excess risk in linear time, without the restrictive assumptions).: Under the assumptions that \(<(L^{3}/R)/md^{3}}\) and \(m d/^{2} n\),  gave a linear-time algorithm with similar excess risk to Algorithm 1. However, their assumptions are very restrictive in practice: For example, in the canonical regime \(n d\), their assumption on \(\) rules out essentially every (non-linear) loss function. By contrast, our result holds even if the smoothness parameter is huge (\(\)) and we only require a logarithmic number of users. Thus, our algorithm and result is applicable to many practical ML problems.

To prove that Algorithm 1 is private, we essentially argue that for any phase \(i\), the \(_{2}\)-sensitivity of \(_{i}\) is upper bounded by \((_{i}/C)\) with probability at least \(1-/2\). The argument goes roughly as follows: First, the Laplace noise added to \(s_{i}(_{i})\) ensures that \(s_{i}(_{i})\) is \(/4\)-user-level DP. Now, it suffices to assume \(_{i}(_{i}) 4C/5\), since otherwise the algorithm halts and outputs \(0\) independently of the data. Next, conditional on the high probability event that the Laplace noise is smaller than \((1/)\), we know that \(_{i}(_{i}) 4C/5 s_{i}(_{i}) 2C/3\) with high probability by our choice of \(C\). In this case, an argument along the lines of [1, Lemma 3.5] shows that \(_{i}\) has sensitivity bounded by \((_{i}/C)\) with probability at least \(1-/2\). See Appendix B for the detailed proof.

To prove the excess risk bound in Algorithm 1, the key step is to show that if the data is i.i.d., then with high probability, no points are removed from \(\{_{i,j}\}_{j=1}^{C}\) during the outlier-removal phase (i.e. \(|_{i}|=C\)). If \(|_{i}|=C\) holds, then we can use the convergence guarantees of SGD and the localization arguments in  to establish the excess risk guarantee. In order to prove that \(|_{i}|=C\) with high probability, we need the following novel _stability_ lemma:

**Lemma 2.3**.: _Assume \(f(,z)\) is convex, \(L\)-Lipschitz, and \(\)-smooth on \(\) with \( 1/\). Let \( SGD(D,,T,x_{0})\) and \( SGD(D^{},,T,x_{0})\) be two independent runs of projected SGD, where \(D,D^{} P^{N}\) are i.i.d. Then, with probability at least \(1-\), we have_

\[\|-\| L.\]

We prove Lemma 2.3 via induction on \(t\), using non-expansiveness of gradient descent on smooth losses , subgaussian concentration bounds, and a union bound.

Lemma 2.3 implies that if the data is i.i.d., then the following events hold with high probability: \(\|_{i,j}-_{i,j^{}}\|_{i}\) for all \(j,j^{}[C_{i}]\) and hence \(s_{i}(_{i})=C\). Further, conditional on \(s_{i}(_{i})=C\), we know that \(_{i}(_{i}) 4C/5\) with high probability, so that the algorithm does not halt. Also, \(\|_{i,j}-_{i,j^{}}\|_{i}\) for all \(j,j^{}\) implies \(p_{i,j}=1\) for all \(j\) and hence \(|_{i}|=C\) for all \(j\). The detailed excess risk proof is provided in Appendix B.

```
1Input: Dataset \(=(Z_{1},,Z_{n})\), privacy parameters \((,)\), parameters \(p,q>0\), stepsize \(\);
2Set \(l=_{2}(n)\), \(C:=100(20nm^{}/)/\);
3for\(i=1,,l\)do
4 Set \(n_{i}=(1-(1/2)^{q})n/2^{iq}\), \(_{i}=/2^{pi}\), \(N_{i}=n_{i}/C\), \(T_{i}=N_{i}m\), \(_{i}=1000_{i}L}(ndm)\);
5 Draw disjoint users \(D_{i}\) of size \(n_{i}\) from \(\);
6 Divide \(D_{i}\) into \(C\) disjoint subsets \(\{D_{i,j}\}_{j=1}^{C}\), each containing \(|D_{i,j}|=N_{i}\) users;
7for\(j=1,,C\)do
8\(_{i,j} SGD(D_{i,j},_{i},T_{i},x_{i-1})=\) average iterate of \(T_{i}\) steps of one-pass projected SGD with data \(D_{i,j}\), stepsize \(_{i}\), and initial point \(x_{i-1}\) ;
9
10 end for
11 Compute the concentration score for \(D_{i}\): \[s_{i}(_{i}):=_{j,j^{}[C]}(\|_{i,j}-_{i,j^{}}\|_{i})\]

 Let \(_{i}(_{i})=s_{i}(_{i})+(20/)\);
12if\(_{i}(_{i})\)then
13\(_{i}=\) ;
14for\(j=1,,C\)do
15 Compute the score function of \(_{i,j}\): \(h_{i,j}=_{j^{}=1}^{C}(\|_{i,j}-_{i,j^{ }}\| 2_{i})\);
16 Add \(_{i,j}\) to \(_{i}\) with probability \(p_{i,j}\) for \(p_{i,j}=0&h_{i,j}<C/2\\ 1&h_{i,j} 2C/3\\ -C/2}{C/6}&o.w.\)
17 end for
18
19 end for
20
21else
22 Halt; Output 0
23
24 end if
25 Let \(_{i}=_{i}|}_{_{i,j}_{i }}_{i,j}\) ;
26\(x_{i}_{i}+_{i}\), where \(_{i}(0,_{i}^{2}I_{d})\) with \(_{i}=^{2}(n/)}{ C}\);
27
28 end for
29Output:\(x_{l}\).
```

**Algorithm 1**User-Level DP Phased SGD with Outlier Iterate Removal and Output Perturbation

**Challenges of getting optimal excess risk in linear time:** In the item-level DP setting, there are several (nearly) linear time algorithms that achieve optimal excess risk for smooth DP SCOunder mild smoothness conditions, such as snowball SGD , phased SGD , and phased ERM with output perturbation . Extending these approaches into optimal nearly linear-time user-level DP algorithms is challenging. First, the user-level DP implementation of output perturbation in  is computationally inefficient. Second, snowball SGD relies on _privacy amplification by iteration,_ which does not extend nicely to the user-level DP case due to instability of the outlier-detection procedure in . Specifically, since amplification by iteration intermediate only provides DP for the last iterate \(x_{T}\) but not the intermediate iterates \(x_{t}\)\((t<T)\), the sensitivity of the concentration score function is not \(O(1)\), which impairs DP outlier-detection. A similar instability issue arises if one tries to naively extend phased SGD to be user-level DP by applying  to user gradients. This issue motivates our Algorithm 1, which extends phased SGD in an alternative way: by applying outlier-detection/removal to the SGD _iterates_ instead of the gradients, we can control the sensitivity of the concentration score and thus prove that our algorithm is DP. However, since the bound in Lemma 2.3 scales polynomially with \(T\) (and we believe this dependence on \(T\) is necessary), Algorithm 1 adds excessive noise and has suboptimal excess risk. We believe that obtaining optimal risk in linear time will require a fundamentally different user-level DP mean estimation procedure that does not suffer from the instability issue.

## 3 An optimal algorithm with \((mn)^{9/8}\) gradient complexity for smooth losses

In this section, we provide an algorithm that achieves optimal excess risk using \((mn)^{9/8}\) stochastic gradient evaluations. Our Algorithm 3 is inspired by the item-level accelerated phased ERM algorithm of . It applies iterative localization  to the user-level DP accelerated minibatch SGD Algorithm 2. Algorithm 2 is a user-level DP variation of the accelerated minibatch SGD of .

Our Algorithm 2 applies a DP outlier-removal procedure to the users' gradients in each iteration. We use _Above Threshold_ to privatize the concentration scores \(s_{i}^{(t)}\) and determine whether or not most of the gradients of users in minibatch \(D_{i}^{t}\) are \(2\)-close to each other. If \(_{i}^{t}_{i}\), indicating that the gradients of users in \(D_{i}^{t}\) are nearly \(2\)-concentrated, then we proceed with outlier removal in lines 8-12. We then invoke _privacy amplification by subsampling_ and the _advanced composition theorem_ to privatize the average of the "inlier" gradients with additive Gaussian noise. By properly choosing algorithmic parameters, we obtain the following results, proved in Appendix C:

**Theorem 3.1** (Privacy of Algorithm 3).: _Let \( 10\), \(q>0\) such that \(n^{1-q}>/)}{(1-(1/2)^{q})}\). Then, Algorithm 3 is \((,)\)-DP._

**Theorem 3.2** (Utility & runtime of Algorithm 3 - Informal).: _Let \( 10\) and \(<1/(mn)\). Then, Algorithm 3 yields optimal excess risk:_

\[F(x_{l})-F^{*} LR(}+ }{ n}).\]

_The gradient complexity of this algorithm is upper bounded by_

\[mn(1+()^{1/4}((mn)^{1/8} (n^{2}m}{d})^{1/8}))+}(m^{5/4}}{}+(m ^{5/4}}{d^{1/4}^{1/2}})).\]

If \(n=d\), \(=1\), and \( R=L\) then the gradient complexity bound in Theorem 3.2 simplifies to \((mn)^{9/8}+n^{1/4}m^{5/4}\). Typically, \(n^{7} m\), so that the dominant term in this bound is \((mn)^{9/8}\).

_Remark 3.3_ (State-of-the-art runtime).: The gradient complexity bound in Theorem 3.2 is _superior to the runtime bounds of all existing near-optimal algorithms by polynomial factors_ in \(n\), \(m\), and \(d\). Note that while the \(mn^{3/2}\) gradient complexity bound of  may _appear_ to be better than \(^{1/4}(nm)^{9/8}\) in certain parameter regimes (e.g. \(m>n^{3}\) or \( nm\)), this is not the case: the result of  requires \(m<n\) and \(<\).

_Remark 3.4_ (Mild assumptions).: Note that Theorems 3.1 and 3.2 do not require any bound on the smoothness parameter \(\), and only require the number of users to grow logarithmically: \(n^{1-o(1)}(1/)\). Contrast this with the results of previous works (e.g. ).

[MISSING_PAGE_FAIL:8]

A challenge in proving Theorem 3.2 is getting a tight bound on the variance of the the noisy minibatch stochastic gradients \(_{i}^{t}\) that are used in Algorithm 2 (lines 12-14). Conditional on \(_{i}^{t}=D_{i}^{t}\), it is easy to obtain a variance bound of the form \(\|_{i}^{t}-_{i}(x_{i}^{t})\|^{2} d _{i}^{2}+}{K_{i}}\), since we are sampling \(K_{i}\) users uniformly at random. However, this bound is too weak to obtain Theorem 3.2, since it does not scale with \(m\). To prove Theorem 3.2, we need the following stronger result:

**Lemma 3.5** (Variance Bound for Algorithm 2).: _Let \( 1/(nm), 1\). Denote \(_{i}(x):=}_{Z_{i,j} D_{i}}(x,Z_{i,j})\). Then, conditional on \(_{i}^{t}=D_{i}^{t}\) for all \(i[l],t[T_{i}]\), we have_

\[\|g_{i}^{t}-_{i}(x_{i-1}^{t})\|^{2}(ndm)}{Km}\]

_for all \(i[l],t[T_{i}]\), where the expectation is over both the random i.i.d. draw of \(=(Z_{1},,Z_{n}) P^{nm}\) and the randomness in Algorithm 3._

The difficulty in proving Lemma 3.5 comes from the fact that the iterates \(x_{i}^{t}\) and the data \(\) are not independent. To overcome this difficulty, we use the _stability of user-level DP_ to argue that for all \(Z D_{i}\), \((x_{i-1}^{t},Z)\) is \( L/\)-close to \( F(x_{i-1}^{t})\) with high probability, since \(x_{i-1}^{t}\) is user-level DP. A detailed proof is given in Appendix C.

_Remark 3.6_ (Strongly convex losses: Optimal excess risk with state-of-the-art runtime).: If \(f(,z)\) is \(\)-strongly convex, then Algorithm 3 can be combined with the meta-algorithm of [11, Section 5.1] to obtain optimal excess risk

\[}{}(+n^{2}m})\]

with the same gradient complexity stated in Theorem 3.2. This improves over the previous state-of-the-art gradient complexity \((mn)^{3/2}\) of .

## 4 An optimal algorithm with subquadratic gradient complexity for non-smooth losses

In this section, we extend our accelerated algorithm from the previous section to non-smooth loss functions. To accomplish this with minimal computational cost, we apply _randomized (convolution) smoothing_ to approximate non-smooth \(f\) by a \(\)-smooth \(\). We can then apply Algorithm 3 to \(\). Since convolution smoothing is by now a standard optimization technique, we defer the details and proof to Appendix D.

**Theorem 4.1** (Privacy and utility of smoothed Algorithm 3 for non-smooth loss - informal).: _Let \( 10\), \(<1/(mn)\), and \(q>0\) such that \(n^{1-q}>/)}{(1-(1/2)^{q})}\). Then, applying Algorithm 3 to the smooth approximation of \(f\) yields optimal excess risk:_

\[F(x_{l})-F^{*} LR(}+ }{ n}).\]

_The gradient complexity of this algorithm is upper bounded by_

\[mn(1+n^{3/8}m^{1/4}^{1/4}).\]

_Remark 4.2_ (State-of-the-art gradient complexity).: The only previous polynomial-time algorithm that can achieve optimal excess risk for non-smooth loss functions is due to . The algorithm of  required \((nm)^{3}+(mn)^{2}\) gradient evaluations. Thus, the gradient complexity of the smoothed version of Algorithm 3 offers a _significant improvement over the previous state-of-the-art_. For example, if \(=1\), then our algorithm is faster than the previous state-of-the-art by a multiplicative factor of at least \(n^{13/8}m^{7/4}\).

## 5 Conclusion

In this paper, we developed new user-level DP algorithms with improved runtime and excess risk guarantees for stochastic convex optimization without the restrictive assumptions made in prior works. Our accelerated Algorithm 3 achieves optimal excess risk for both smooth and non-smooth loss functions, with significantly smaller computational cost than the previous state-of-the-art. Our linear-time Algorithm 1 achieves state-of-the-art excess risk under much milder, more practical assumptions than existing linear-time approaches.

Our work paves the way for several intriguing future research directions. First, the question of whether there exists a linear-time algorithm that can attain the user-level DP lower bound for smooth losses remains open. In light of our improved gradient complexity bound (\((nm)^{9/8}\)), we are now optimistic that the answer to this question is "yes." We believe that our novel techniques will be key to the development of an optimal linear-time algorithm. Specifically, utilizing Lemma 2.3 to apply outlier removal to the iterates instead of the gradients appears to be pivotal. Second, the study of user-level DP SCO has been largely limited to approximate \((,)\)-DP. What rates are achievable under the stronger notion of pure \(\)-user-level DP? Third, it would be useful to develop fast and optimal algorithms that are tailored to federated learning environments , where only a small number of users may be available to communicate with the server in each iteration. We hope our work inspires and guides further research in this exciting and practically important area.