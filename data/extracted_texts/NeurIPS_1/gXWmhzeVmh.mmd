# Rough Transformers: Lightweight and Continuous Time Series Modelling through Signature Patching

Fernando Moreno-Pino\({}^{1,}\)1 Alvaro Arroyo\({}^{1,2,}\)1 Harrison Waldon\({}^{1,*}\)

**Xiaowen Dong\({}^{1,2}\) Alvaro Cartea\({}^{1,3}\)**

\({}^{1}\) Oxford-Man Institute, University of Oxford

\({}^{2}\) Machine Learning Research Group, University of Oxford

\({}^{3}\) Mathematical Institute, University of Oxford

Equal contribution.

###### Abstract

Time-series data in real-world settings typically exhibit long-range dependencies and are observed at non-uniform intervals. In these settings, traditional sequence-based recurrent models struggle. To overcome this, researchers often replace recurrent architectures with Neural ODE-based models to account for irregularly sampled data and use Transformer-based architectures to account for long-range dependencies. Despite the success of these two approaches, both incur very high computational costs for input sequences of even moderate length. To address this challenge, we introduce the Rough Transformer, a variation of the Transformer model that operates on continuous-time representations of input sequences and incurs significantly lower computational costs. In particular, we propose _multi-view signature attention_, which uses path signatures to augment vanilla attention and to capture both local and global (multi-scale) dependencies in the input data, while remaining robust to changes in the sequence length and sampling frequency and yielding improved spatial processing. We find that, on a variety of time-series-related tasks, Rough Transformers consistently outperform their vanilla attention counterparts while obtaining the representational benefits of Neural ODE-based models, all at a fraction of the computational time and memory resources.

## 1 Introduction

Real-world sequential data in areas such as healthcare , finance , and biology  often are irregularly sampled, of variable length, and exhibit long-range dependencies. Furthermore, these data, which may be drawn from financial limit order books  or EEG readings , are often sampled at high frequency, yielding long sequences of data. Hence, many popular machine learning models struggle to model real-world sequential data, due to input dimension inflexibility, memory constraints, and computational bottlenecks. Rather than treating these data as _discrete_ sequences, effective theoretical models often assume data are generated from some underlying _continuous-time_ process . Hence, there is an increased interest in developing machine learning methods that use _continuous-time_ representations to analyze sequential data.

One recent approach to modelling continuous-time data involves the development of continuous-time analogues of standard deep learning models, such as Neural ODEs  and Neural CDEs , which extend ResNets  and RNNs , respectively, to continuous-time settings. Instead of processing discrete data directly, these models operate on a latent continuous-time representationof input sequences. This approach is successful in continuous-time modelling tasks where standard deep recurrent models fail. In particular, extensions of vanilla Neural ODEs to the time-series setting  succeed in various domains such as adaptive uncertainty quantification , counterfactual inference , or generative modelling .

In many practical settings, such as financial market volatility  or heart rate fluctuations , continuous-time data also exhibit long-range dependencies. That is, data from the distant past may impact the system's current behavior. Deep recurrent models struggle in this setting due to vanishing gradients, whereas continuous-time analogues of these models have been shown to address this difficulty . Several recent works  also successfully extract long-range dependencies from sequential data with Transformers , which learn temporal dependencies of a tokenized representation of input sequences. Extracting such temporal dependencies requires a positional encoding of input data, because the attention mechanism is permutation invariant, which projects data into some latent space. The parallelizable nature of the Transformer allows for rapid training and evaluation on sequences of moderate length and it contributes to its success in fields such as natural language processing (NLP).

While the above approaches succeed in certain settings, several limitations hinder their wider applications. On the one hand, Neural ODEs and their analogues  bear substantial computational costs when modelling long sequences of high dimension; see . On the other hand, Transformers operate on discrete-time representations of input sequences, whose relative ordering is represented by the positional encoding. This representation may inhibit their expressivity in continuous-time data modelling tasks . Moreover, Transformer-based models suffer from a number of difficulties, including (i) input sequences must be sampled at the same times, (ii) the sequence length must be fixed, and (iii) the computational cost scales quadratically in the length of the input sequence. These difficulties severely limit the application of Transformers to continuous-time data modelling.

**Contributions 1)** We introduce _Rough Transformers_, a variant of the Transformer architecture amenable to the processing of continuous-time signals, which can be easily integrated into existing code-bases. The Rough Transformer is built upon the path signature from Rough Path Theory . We define a novel, multi-scale transformation which projects discrete input data to a continuous-time path and compresses the input data with minimal information loss. Moreover, this transformation is an efficient feature representation of continuous-time paths, because linear functionals of path signatures approximate continuous functions of paths arbitrarily well (see Theorem A.2 in Appendix A).

**2)** We introduce the _multi-view attention mechanism_ to extract both local and global dependencies of very long time-series efficiently. This mechanism operates directly on continuous-time representations of data without the need for expensive numerical solvers or constraints on the smoothness of the data stream. Moreover, the multi-view attention mechanism is provably robust to irregularly sampled data.

**3)** We carry out extensive experimentation on long and irregularly sampled time-series data. In particular, we show that Rough Transformers (i) improve the learning dynamics of the Transformer, making it more sample-efficient and allowing it to achieve better out-of-sample results, (ii) reduce the training cost by a factor of up to \(25\) when compared with vanilla Transformers and more when compared with Neural ODE based architectures, (iii) maintain similar performance when data are irregularly sampled, where traditional recurrent-based models suffer a substantial decrease in

Figure 1: A representation of the multi-view signature. The continuous-time path is irregularly sampled at points marked with a red \(x\). The local and global signatures of a linear interpolation of these points are computed and concatenated to form the multi-view signature. The multi-view signature transform consists of \(\) multi-view signatures.

performance , and (iv) yield improved spatial processing, accounting for relationships between different temporal channels without having to pre-define a specific inter-channel relation structure.

## 2 Background and Methodology

Problem Formulation.In many real-world scenarios, sequential data are time-series sampled from some underlying continuous-time process, so datasets consist of long, irregularly sampled sequences of varied lengths. In these settings, the problem of sequence modelling is described as follows. Let \(C(^{+};^{d})=\{g:^{+}^{d}  g\}\), and consider \( C(^{+};^{d})\) which we call a continuous-time _path_. A time-series of length \(L\) with sampling times \(_{}=\{t_{i}\}_{i=1}^{L}^{+}\) is defined as \(=((t_{1},X_{1}),...,(t_{L},X_{L}))\), where \(X_{i}=(t_{i})^{d}\). Now, define a continuous function on paths \(f:C(^{+};^{d})^{k}\). Next define a dataset \(=\{(^{i},f(^{i}))_{i=1}^{N}\}\). We seek to approximate the function \(f\) from the set \(\) for some downstream task. Importantly, we do not assume that \(_{}=_{}\) for all \(,\), so that \(\) may be irregularly sampled.

Sequence Modelling with Transformers.Transformers are used extensively as a baseline architecture to approximate functions of discrete-time sequential data and are successfully applied to settings when input sequences are fixed in length, relatively short, and sampled at regular intervals. First, the Transformer projects input time series \(^{L d}\) to a high-dimensional space \( T()^{L d^{}}\) for \(d^{}>>d\) using some linear positional encoding \(T:^{L d}^{L d^{}}\). Next, a latent representation of the encoded sequence is learned by a multi-headed self-attention mechanism which splits \(T()\) into \(H\) distinct query, key, and value sequences: \(Q_{h}=T()W_{h}^{Q}\), \(K_{h}=T()W_{h}^{K}\), \(V_{h}=T()W_{h}^{V}\), respectively, with \(h=1,...,H\) and weight matrices \(W_{h}^{Q},W_{h}^{K},W_{h}^{V}^{d^{} d^{}}\). The multi-head self-attention calculation for each head is given by

\[O_{h}=(K_{h}^{}}{}}) V_{h}\,, \]

and the latent representation is projected to the output space \(^{k}\) using a multi-layer perceptron (MLP).

The input length \(L\) of the MLP and the Transformer is fixed by assumption. To evaluate the Transformer on a time-series \(\) with \(|_{}| n\), one must perform some transformation (interpolation, extrapolation, etc.) which may degrade the performance of the model. Furthermore, the memory and time complexity of the Transformer is of order \(O(L^{2}d)\), which presents a substantial difficulty in modelling long sequences.

Rough Path Signatures.Broadly, the difficulties faced by the Transformer in modelling time-series stem from time-series being sampled from underlying _continuous-time_ objects, while the attention mechanism underpinning the Transformer is designed to model discrete sequences. To address these difficulties, Rough Transformers augment standard Transformers by lifting the input time-series to the space of continuous-time functions and performing the self-attention calculation in this infinite-dimensional space. To achieve this, we use the path signature from Rough Path Theory.

For a continuous-time path \( C_{b}^{1}(^{+};^{d})\) and times \(s,t^{+}\), the path signature of \(\) from \(s\) to \(t\), denoted \(S()_{s,t}\), is defined as follows. First, let

\[_{d}=\{(i_{1},...,i_{p}):i_{j}\{1,...,d\}\,\,jp\} \]

denote the set of all \(d\)-multi-indices and \(_{d}^{n}=\{I_{d}:|I|=n\}\). Next, set \(S()_{s,t}^{0}:=1\) and for any \(I_{d}\), define

\[S()_{s,t}^{I}=_{s<u_{1}<<u_{p}<t}}^{{}^{1_{1} }}(u_{1})}^{{}^{p}}(u_{p})\,du_{1} du_{p}\,, \]

where \(}^{j}=d^{j}/dt\). Abusing notation, define level \(n\) of the signature as

\[S^{n}()_{s,t}=\{S()_{s,t}^{I}:I_{d}^{n }\}\,. \]and define the signature as the infinite sequence

\[S()^{n}_{s,t}=(S()^{0}_{s,t},S()^{1}_{s,t},...,S( )^{n}_{s,t},...)\,. \]

Finally, define the truncation of the signature \(S()^{ n}_{s,t}=(S()^{0}_{s,t},...,S()^{n} _{s,t})\), where \(S()^{n}_{s,t}\) can be interpreted as an element of the _extended tensor algebra_ of \(^{d}\):

\[T((^{d}))=\{(a_{0},...,a_{n},...):a_{n}^{d n }\}\,. \]

Analogously, we say that \(S()^{ n}_{s,t} T((^{d}))_{ n}\). A central property of the signature is that is invariant with respect to time-reparameterization . That is, let \(:[0,T][0,T]\) be surjective, continuous, and non-decreasing. Then we have

\[S()_{0,T}=S()_{0,T}\,, \]

which will be crucial to demonstrate the Rough Transformer's robustness to irregularly sampled data.

In contrast to wavelets or Fourier transforms, which parameterize paths on a functional basis, the signature provides a basis for functions of continuous paths. Hence, the path signature is well-suited to sequence modelling tasks in which one seeks to learn a function of the underlying functional. For a more rigorous presentation of signatures and a description of additional properties, see Appendix A and Lyons et al. .

## 3 Rough Transformers

Now, we construct the Rough Transformer, a Transformer-based architecture that operates on continuous-time sequential data by means of the path signature.

Let \(\) be a dataset of irregularly sampled time-series. To project a discretized time-series \(\) to a continuous-time object, let \(\) denote the piecewise-linear interpolation of \(\).2 Next, for \(t_{k}\), define the _multi-view signature_

\[M()_{k}:=(S()_{0,t_{k}},S()_{t_{k-1},t_{k}} )\,. \]

In what follows, we refer to the components \((S()_{0,t_{k}},S()_{t_{k-1},t_{k}})\) as _global_ and _local_, respectively; see Figure 1. Intuitively, one can interpret the global component as an efficient representation of long-term information (see Theorem A.2 in Appendix A), and the local component as a type of convolutional filter that is invariant to the sampling rate of the signal. Now, define the _multi-view signature transform_\(M()=(M()_{1},...,M()_{}),\) and denote by \(M()^{ n}\) the truncated signature for a truncation level \(n\). Next, define the _multi-view attention mechanism_, which uses the multi-view signature transform to extend the standard attention mechanism to the space of continuous functions . First, fix a truncation level \(n\), and let \(\) be such that \(M()^{ n}_{k}^{}\). For \(h=1,...,H\) let \(W^{,,}_{h}^{d^{}}\) for some \(^{}\), and let

\[_{h}=M()^{ n}W^{}_{h},_{h}=M( )^{ n}W^{}_{h},_{h}=M()^{ n} W^{}_{h}\,. \]

Then, the attention calculation is given by

\[O_{h}=\,(_{h}^{}_{h}}{ {^{}}})_{h}\,. \]

Notice that the attention calculation is similar to (1), however, we stress that the multi-view attention is built on _continuous-time_ objects, the signatures, while the standard attention mechanism acts on discrete objects. The multi-view signature provides a compressed representation of the time series, minimizing the computational costs associated to quadratic scaling without excessive loss of representational capacity, see Appendix F.

### Advantages of Rough Transformers

**Computational Efficiency.** As demonstrated in Section 4, multi-view attention mechanism can substantially reduce the computational cost of vanilla Transformers. In particular, the attention calculation decreases from \(O(L^{2}\,d)\) in the vanilla case to \(O(^{2}\,d)\), where \(<<L\) with Rough Transformers. This enables both faster wall-clock training time and the ability to process long input sequences which would otherwise yield out-of-memory errors for the vanilla Transformer, see Figure 2. Moreover, the multi-view attention mechanism does not require backpropagation through the signature calculation, which can be computed _offline_. This is significantly more computationally efficient compared with the complexity of computing signatures batch-wise in every training step. Finally, the signature of piecewise-linear paths can be computed explicitly, see Appendix A, and there are a number of Python packages devoted to optimized signature calculation .

**Variable Length and Irregular Sampling.** The multi-view signature transform underpinning Rough Transformers is evaluated by constructing a continuous-time interpolation of input data and computing a series of iterated integrals of this interpolation. The bounds of these integrals are a fixed set of time points, meaning that the sequence length of the multi-view attention mechanism is fixed and independent of the sequence length of input samples. Furthermore, the following proposition shows that the output of the Rough Transformer for two (possibly irregular) samplings of the same path is similar.

**Proposition 3.1**.: _Let \(\) be a Rough Transformer. Suppose \(:[0,T]^{d}\) is a continuous-time process, and let \(:[0,T][0,T]\) denote a time-reparameterization. Suppose \(\) and \(^{}\) are samplings of \(\) and \(\), respectively. Then \(()(^{})\)._

Proof.: By (7), \(S()_{s,t}=S()_{s,t}\) for all \(s,t[0,T]\). Hence, one has \(M(X^{1}) M(X^{2})\). Finally, \((X^{1})(X^{2})\) because the attention mechanism and final MLP are both continuous. 

Hence, the Rough Transformer is robust to irregular sampling. In many tasks, the sampling times convey important information about the time-series. In these settings, one may augment the input time-series with its sampling times, that is, write \(X=((t_{0},X_{0}),...,(t_{L},X_{L}))\).

**Spatial Processing.** While an interpolation of input data could be sampled to make vanilla Transformers independent of the length of the input sequence, important locality information could be lost, see Appendix F.2. Instead, Rough Transformers summarize spatial interactions between channels by means of the multi-view signature transform. One may notice that in (5), the dimension of the signature grows exponentially in the level of the signature \(n\). In particular, when \(X_{i}^{d}\), \(|S()_{0,t}^{ n}|=-1)}{d-1}=O(d^{n})\), so the multi-view attention calculation is of order \(O(^{2}d^{n})\). In many practical time-series modelling problems, however, the value of \(d\) is not very large. The signature terms also decay factorially in the signature level \(n\) (see Proposition A.3 in Appendix A), so in practice, one may take the value of \(n\) to be small without sacrificing performance. The majority of computational savings result from the reduction of the sequence length to \(\), and in practice, we take \(<<L\).

When the dimension \(d\) is large, there are three possible remedies to maintain computational efficiency. First, instead of computing the signature in \(M(X)_{k}=(S(X)_{0,t_{k}},S(_{t_{k-1},t_{k}})\), one may compute

Figure 2: Seconds per epoch for growing input length and for different model types on the sinusoidal dataset. **Left:** Log Scale. **Middle:** Regular Scale. **Right:** Log-log scale. When a line stops, it indicates an OOM error.

the _log-signature_, which is a compressed version of the signature . When the dimension is large enough such that the log-signature is computationally infeasible, one may instead compute the _univariate_ signatures of features coupled with the time channel. That is, consider \( C([0,T];^{d})\), with \((t)=(_{1}(t),...,_{d}(t))\). Denote the time-added function \(_{i}(t):=(t,_{i}(t))\). Then we define the _univariate multi-view signature_

\[()_{k}=(M(_{1})_{k},...,M( _{d})_{k})\,. \]

The attention mechanism in this case is constructed as before. Fixing the maximum signature depth to be some value \(n^{*}\), one sees that the number of features in the univariate multi-view signature is approximately \(2^{n^{*}}d\). In practice we find that \(n^{*} 5\) provides sufficient performance, so the order of the attention calculation is \(O(C\,^{2}\,d)\) for \(C 2^{n^{*}}\). Finally, one may use randomized signatures to reduce dimension by using a Johnson-Lindenstrauss-type projection to a low-dimensional latent space and computing the signature in this space, as in [21; 19].

## 4 Experiments

In this section, we present empirical results for the effectiveness of the Rough Transformer, hereafter denoted RFormer, on a variety of time-series-related tasks. Experimental and hyperparameter details regarding the implementation of the method are in Appendices C and D. We consider long multivariate time-series as our main experimental setting because we expect signatures to perform best in this scenario. Additional experimentation on long-range reasoning tasks on image-based datasets is left for future work, as these would likely require additional inductive biases.

To benchmark RFormer, we consider both discrete-time and continuous-time models. In particular, we include as main baselines traditional RNN models (GRU ), ODE-based methods designed for sequential data (Neural-CDE ), as well as ODE-based methods explicitly designed for long time-series (Neural-RDE ).3 Furthermore, we compare against a vanilla Transformer  which is the RFormer backbone. Finally, we present comparisons with a recent continuous-time Transformer model, ContiFormer , to highlight the computational efficiency gap between RFormer and similar continuous-time models. We note that the first two tasks focus on evaluating the performance improvement of RFormer over the Transformer baseline. For other long-range tasks, we include comparisons to recent state-space models [31; 62; 81]. In the irregular sampling regime, we benchmark against state-of-the-art models tailored to that setting [61; 78]. See Appendix B for additional discussion on related models and more details about our experimental choices.

### Time Series Processing

Frequency Classification.Our first experiment is based on a set of synthetically generated time series from continuous paths of the form

\[(t)=g(t)(\,t+)+(t)\,, \]

where \(g(t)\) is a non-linear trend component, \(\) and \(\) are two noise terms, and \(\) is the frequency. Here, the task of the model is to classify the time-series according to its frequency \(\). We consider \(1000\) samples in \(100\) classes with \(\) evenly distributed from \(10\) to \(500\). Each time-series is regularly sampled with \(2000\) times-steps on the interval \(\).

This synthetic experiment is similar to others in recent work on time-series modelling [49; 89; 55]. We include an additional experiment in which we alter the signal in (12) so its frequency is \(_{0}\) for \(t<t_{0}\) and \(_{1}\) afterward, where the task is to classify the sinusoid based on the first frequency. We call this dataset the "long sinusoidal" dataset. This extension of the original experiment aims to test the ability of the model to perform long-range reasoning effectively. Note that for this task, we also add ODE-RNN to the previously mentioned baselines.

Figure 3: Test accuracy per epoch for the frequency classification task across three random seeds. **Left:** Sinusoidal dataset. **Right:** Long Sinusoidal dataset.

[MISSING_PAGE_FAIL:7]

### Training Efficiency

Here, we focus on the computational gains of the model when compared with vanilla Transformers and methods that require numerical ODE solvers.

Attention-based architectures are highly parallelizable on modern GPUs, as opposed to traditional RNN models which require sequential updating. However, vanilla attention experiences a bottleneck in memory and time complexity as the sequence length \(L\) grows. As covered above in Section 3, variations of the signature transform allow the model to operate on a reduced sequence length \(\) without increasing the dimensionality in a way that would become problematic for the model. This allows us to bypass the quadratic complexity of the model without resorting to sparsity techniques commonly used in the literature .

Tables 1-3 show that RFormer is competitive when modelling datasets with extremely long sequences without an explosion in the memory requirements. RFormer exploits the parallelism of the attention mechanism to significantly accelerate training time, as the length of the input sequence is decreased substantially. In particular, we observe speedups of \(\) to \(\) with respect to standard attention, and higher when compared with all methods requiring numerical solutions to ODEs. The computational efficiency gains of RFormer are attained due to the signature transform reducing the length of the time-series with minimal information loss. The effectiveness of this transformation can be seen from the ablation study carried out in Appendix F. This contrasts with NRDEs , which augment NCDEs with local signatures of input data, and find that smaller windows often perform better. Furthermore, NRDEs do not experience the same computational gains as RFormer because they must perform many costly ODE integration steps.

In Figure 2, we showcase the improvements in computational efficiency of RFormer compared to vanilla Transformers , continuous-time Transformers , and other continuous-time RNNs  when processing sequences from \(L=100\) samples up to \(L=10\)K. As seen, RFormer is significantly more efficient than its continuous-time and vanilla counterparts, even when performing the signature computation online, which involves computing the signatures for each batch during training, resulting in significant redundant computation. When signatures are precomputed just once before training, the computational time of each epoch remains _constant_ across input all sequence lengths including \(L=10\)K (see the exact signature computation times for different datasets in Table 4). We also stress the fact that RFormer also scales gracefully for extremely long sequences (up to \(L=250\)K) with both online and offline computation of the signatures, as shown in Appendix G. Finally, we highlight that ContiFormer has a sample complexity of \((L^{2}d^{2}S)\), where \(S\) represents the normalized number of function evaluations of the numerical ODE solver, which makes ContiFormer orders of magnitude more computationally intensive when compared to RFormer and prevents the model from running on sequences longer than 500 points without running out of memory (see device details in Appendix C).

### Irregular Time Series Classification

So far, we mainly focused on the efficiency and inductive bias afforded to the model through the use of signatures. However, a key element of RFormer is that it can naturally deal with irregularly sampled sequences without expensive numerical ODE solvers. This property follows from the fact that signatures are _invariant to time reparameterization_, see Proposition 3.1. In this subsection, we empirically test this property by training the model on the same datasets but randomly dropping a percentage of the data points. This test intends to find if the model is able to learn continuous-time representations of the original input time-series. The results can be found in Table 5. We find that

   Dataset & **Train** & **Val** & **Test** \\  Eigenworms & 1.11 s. & 0.19 s. & 0.19 s. \\ HR & 4.23 s. & 0.84 s. & 0.85 s. \\ Sine (1k) & 0.39 s. & 0.39 s. & 0.39 s. \\ Sine (5k) & 0.51 s. & 0.51 s. & 0.51 s. \\ Sine (20k) & 1.64 s. & 1.64 s. & 1.64 s. \\ Sine (100k) & 5.74 s. & 5.74 s. & 5.74 s. \\   

Table 4: Dataset processing times for training, validation, and testing phases.

    &  \\   & **Sine** & **EW** & **HR** \\  GRU & **0.12** & 0.25 & 1.07 \\ ODE-RNN & 5.39 & 48.59 & 50.71 \\ Neural-CDE & 9.83 & - & - \\ Neural-RDE & 0.85 & 5.23 & 9.52 \\ Transformer & 0.77 & OOM & 11.71 \\
**RFormer** & 0.55 & **0.11** & **0.45** \\
**Speedup** & \(1.4\) & - & \(26.11\) \\   

Table 3: Seconds per epoch for all models considered.

RFcorner consistently results in the best performance, with a small performance drop when compared to the full dataset. Importantly, this property is achieved in conjunction with the efficiency gains afforded to the model and without the use of expensive numerical ODE solvers.5 Finally, we perform an additional set of experiments on the 15 univariate classification datasets from the UEA time series classification archive and compare our model with recent state-of-the-art models for irregular time series . Across the board, we find that our model is both faster and more accurate than the continuous-time benchmark _despite having a discrete-time Transformer backbone_, as shown in Figure 4, which introduces Continuous Recurrent Units (CRU)  as an extra baseline. For more details and more exhaustive experimentation on random data drops, see Appendix G.

## 5 Reasons for improved model performance

In this final section, we provide explanations for the superior inductive bias of the RFcorner model compared to its vanilla Transformer counterpart, despite its lower computational cost.

### Spatial Processing

First, we highlight that a key reason the model achieves significant compression benefits in the tasks considered is its ability to _jointly_ account for temporal and spatial interactions through the self-attention mechanism and signature terms, respectively. In particular, we believe that for certain datasets, the relationships between different channels of the time series may hold more importance than the temporal information itself, which can often be redundant. This is exemplified in the Eigenworms dataset, which experiences a 20% performance drop when employing univariate signatures, but is able to achieve state-of-the-art performance with a 600\(\) compression rate in the temporal dimension when signatures are applied across all channels, as shown in Figure 6. To this end, we draw parallels between the use of signatures and the field of temporal graph processing, where the use of the signature over all channels can be seen as a fully connected graph, capturing information from all channels, and the univariate signature would correspond to a graph with only self-connections between the nodes, as depicted in Figure 5. In our view, this hints towards the idea of using sparse graph learning techniques  to reduce the explosion in signature terms while retaining the ability to perform effective spatial processing.

To empirically test these claims, we design a synthetic experiment using a 2-channel time series. Each channel contains a signal of the form \((_{i}t+_{i}),i=1,2\), where \(_{i}\) and \(_{i}\) are randomly sampled from the interval \([0,2]\). For half of the dataset, the last 1% of temporal samples in the second channel are set to match the frequency of the first channel. The task is to classify whether the samples in this final interval are of the same frequency. As shown in Figure 5, RFcorner demonstrates greater sample efficiency and achieves higher test accuracy compared to its vanilla Transformer counterpart, highlighting the effectiveness of signatures in spatial processing.

Figure 4: Average performance of all models on the 15 univariate datasets from the UEA Time Series archive under different degrees of data drop.

    &  \\   & EW (\%) \(\) & HR \(\) & Sine (\%) \(\) & Sine Long (\%) \(\) \\  GRU & 35.90 & 13.06 & 0.96 & 1.16 \\ ODE-RNN & 37.61 & 13.06 & 1.06 & 1.23 \\ Neural-RDE & 60.68 & 4.67 & 0.94 & 0.87 \\ Transformer & OOM & 12.73 & 7.37 & 20.23 \\
**RFcorner** & **87.69** & **2.96** & **59.57** & **93.17** \\   

Table 5: Performance of all models under a random 50% drop in datapoints per epoch.

### Sequence Coarsening as an Inductive Bias for Transformers

In addition to the benefits of higher-order signature terms, we empirically observe that even using level-one signature terms resulted in performance improvements when compared to processing sequences without any transformation. We believe that the reduction in input signal length, achieved without significant information loss through the signature transform is another important factor in the improved inductive bias of RFormer. This finding aligns with the concurrent work of , which highlights some of the drawbacks of decoder-only Transformers for long sequences in terms of both _oversquashing_ and _representational collapse_.

To measure the degree of coarsening in the sequence, we find that interpreting the temporal sequence as a path graph and using ideas from the oversmoothing literature  serves as a good way to measure the similarity of the representations being fed to the Transformer. In particular, we compute the Dirichlet Energy , defined in this case as \(E()=_{i=1}^{N}||_{i}-_{i-1}||_{2}\) of the temporal sequence resulting from taking increasing window sizes of the global signature. An example of this is shown in Figure 6 for the Eigenworms dataset, where we compared different numbers of windows (from 2 to 18k). Interestingly, we found that the "elbow" of the Dirichlet energy corresponded to 30 windows in this dataset, which we found empirically to be one of the most performant settings. This hints at the idea of the Dirichlet energy being used for signature hyperparameter tuning as well.

## 6 Conclusion

We introduced the Rough Transformer, a variant of the original Transformer that allows the processing of discrete-time series as continuous-time signals through the use of multi-view signature attention. Empirical comparisons showed that Rough Transformers outperform vanilla Transformers and continuous-time models on a variety of time-series tasks and are robust to the sampling rate of the signal. Finally, we showed that RFormer provides significant speedups in training time compared to regular attention and ODE-based methods, without the need for major architectural modifications or sparsity constraints.

## Impact Statement

This work is unlikely to result in any harmful societal repercussions. Its primary potential lies in its ability to enhance and advance existing data modelling and machine learning methods.