# HySynth: Context-Free LLM Approximation for Guiding Program Synthesis

Shraddha Barke

UC San Diego

San Diego, USA

sbarke@ucsd.edu

Emmanuel Anaya Gonzalez

UC San Diego

San Diego, USA

fanayagonzalez@ucsd.edu

Saketh Ram Kasibatla

UC San Diego

San Diego, USA

skasibatla@ucsd.edu

Taylor Berg-Kirkpatrick

UC San Diego

San Diego, USA

tbergkirkpatrick@ucsd.edu

Nadia Polikarpova

UC San Diego

San Diego, USA

npolikarpova@ucsd.edu

###### Abstract

Many structured prediction and reasoning tasks can be framed as program synthesis problems, where the goal is to generate a program in a _domain-specific language_ (DSL) that transforms input data into the desired output. Unfortunately, purely neural approaches, such as large language models (LLMs), often fail to produce fully correct programs in unfamiliar DSLs, while purely symbolic methods based on combinatorial search scale poorly to complex problems. Motivated by these limitations, we introduce a hybrid approach, where LLM completions for a given task are used to learn a task-specific, context-free surrogate model, which is then used to guide program synthesis. We evaluate this hybrid approach on three domains, and show that it outperforms both unguided search and direct sampling from LLMs, as well as existing program synthesizers.

## 1 Introduction

Large language models (LLMs) demonstrate impressive capabilities in various domains, but they continue to struggle with tasks that require precision--e.g. structured prediction, reasoning, counting, or data transformation--when direct task examples are not prevalent in their training data . As one example, consider the _Abstraction and Reasoning Corpus_ (Arc) , which was designed as a benchmark for human-like structured reasoning. Arc tasks are grid-based puzzles, such as one depicted in Fig. 0(a). This puzzle consists of three training examples, which are pairs of input and output grids; the goal is to infer the transformation that maps the input to the output, and then apply this transformation to the test grid. The Arc benchmark's emphasis on generalization and few-shot learning has rendered it challenging to solve with purely machine learning techniques: state-of-the-art generative models like GPT-4 hardly solve more than 10% of the tasks in the dataset when asked to predict the test output, even with the help of advanced prompting techniques .

In fact, the leading entries in the Arc Kaggle competition  tackle this task using _Programming-by-Example_ (PBE): instead of predicting the output directly, they search for a program that captures the transformation occurring in the input-output examples. For example, the transformation in Fig. 0(a) might be represented as the following program:

\[==\\  \]This particular program is written in a _domain-specific language_ (DSL) inspired by the Arga tool . It consists of a single _rule_ of the form **if**_filter_**then**_transform_, which is applied to each object in the grid simultaneously; if the filter holds for the focus object **self** and another object **other**, then **self** undergoes the transform. In this case, the rule says that any grey object that has a neighbor of the grid's minimum size (here, a single pixel) should be colored with the color of that neighbor.

Beyond grid puzzles, PBE is a general paradigm for structured reasoning and data transformation tasks: for example, it can help spreadsheet users with systematic string manipulation , and help programmers use unfamiliar APIs ; Fig. 1 shows example PBE tasks from three domains.

**Challenge: Harnessing the Power of LLMs for PBE** How can we automatically learn programs from the input-output examples like those shown in Fig. 1? The traditional _program synthesis_ approach is based on combinatorial search , which works well for small programs and restrictive DSLs, but becomes infeasible as the program size and the DSL complexity grow. At the other end of the spectrum, purely _neural_ approaches  use a neural model to predict the program from input-output examples; unfortunately, even state-of-art LLMs like GPT-4o  struggle to predict an entire program in an unfamiliar DSL: when we asked GPT-4o to generate 10 programs for the running example above, none of them were entirely correct.1

In the past, the limitations of both program synthesis and neural techniques have motivated a hybrid approach, where combinatorial search is _guided_ by a learned probabilistic model . Existing hybrid techniques, however, use domain-specific models trained on datasets of similar PBE tasks, which limits their generalization to new domains. With the advent of LLMs, can we now use a single pre-trained model to guide program synthesis across a wide range of domains?

Interestingly, there is some tension in the hybrid approach between the efficiency of the search algorithm and the power of the model: a search algorithm is efficient when it _factorizes the search space_ (_i.e._, merges many search states into one), which often makes it incompatible with a powerful model that requires a lot of context to make a prediction. Specifically, one of the most widely used program synthesis techniques is _bottom-up search_, which is a dynamic programming algorithm, whose efficiency relies on reusing the work of constructing and evaluating subprograms in many different contexts. This essentially precludes using models with unlimited left-to-right context--like LLMs-to guide bottom-up search.

Our Solution: Context-Free LLM ApproximationTo bridge this gap and harness the power of LLMs to guide bottom-up search, we propose to approximate the LLM's conditional output distribution _for a given task_ with a context-free surrogate model. Recent work in NLP  has found that a Hidden Markov Model (HMM) trained to match an LLM can be used as an efficient surrogate

Figure 1: Example problems from the three PBE domains we evaluate HySynth on: grid-based puzzles (Arc), tensor manipulation (Tensor), and string manipulation (String).

in style-controlled language generation. We extend this idea to program synthesis, replacing the HMM with a _probabilistic context-free grammar_ (PCFG). The benefits of using a PCFG are twofold: (1) PCFGs are context-free, which makes them compatible with bottom-up search for PBE [11; 36], and (2) while a context-free model may make a poor approximation to an LLM's full joint, in a PBE setting it is able to reasonably approximate an LLM's conditional distribution over output programs _for a given prompt_. The overview of our approach is shown in Fig. 2.

EvaluationWe implemented this technique in a tool HySynth2 and evaluated it on 299 PBE tasks from three domains: Arc grid-based puzzles , tensor manipulation tasks from TFcoder, and string manipulation tasks from the SyGuS benchmark , which are inspired by spreadsheet use cases. Example problems from these domains are shown in Fig. 1. Our evaluation shows that HySynth outperforms both unguided search and LLMs alone, solving 58% of the tasks overall, compared to 40% for unguided search and 6% for LLMs without search. Our tool also outperforms baseline program synthesizers for these domains--Agra, TFcoder, and Probe, respectively; importantly, in the Tensor domain, the guidance from the LLM not only speeds up the search, but also frees the user from having to explicitly provide any non-standard _constants_ that the solution might use, thereby significantly improving the usability of the tool.

ContributionsIn summary, this paper makes the following contributions:

1. We propose a hybrid program synthesis approach that integrates LLMs with efficient bottom-up search via a task-specific context-free approximation.
2. We implement this approach in a tool HySynth and instantiate it on three domains: grid-based puzzles (Arc), tensor manipulation (Tensor), and string manipulation (String). While the latter two domains reuse off-the-shelf bottom-up synthesizers, for Arc we implement a custom synthesizer that uses a divide-and-conquer strategy  to leverage the structure of the rule-based DSL to further speed up the search.
3. We evaluate HySynth on the three domains and show that it outperforms both the LLM alone and existing baseline synthesizers, which are not guided by LLMs.

## 2 Background

### Programming-By-Example

Programming by Example (PBE)  is the task of synthesizing programs that satisfy a given set of input-output examples. To restrict the program space, the programs are typically drawn from a _domain-specific language_ (DSL), which is specified by a _context-free grammar_ and an _evaluation function_. This section provides a formal definition of these concepts.

Context-Free GrammarsA _context-free grammar_ (CFG) is a quadruple \(=(,,,)\), where \(\) is a set of non-terminal symbols, \(\) is a set of terminal symbols, \(\) denotes the starting

Figure 2: An overview of the hybrid program synthesis technique that uses a context-free LLM approximation. Programs generated by an LLM are used to learn a PCFG, which guides a bottom-up synthesizer to generate programs until a solution is found.

non-terminal, and \(\) is the set of production rules. An example CFG is shown in Fig. 3. We denote with \(()\) the set of all rules \(\) whose left-hand side is N. A grammar \(\) defines a (leftmost) _single-step derivation_ relation on sequences of symbols: \(s s\) if \(\), where \(s^{*}\) and \(,()^{*}\). The transitive closure of this relation \(^{*}\) is called (leftmost) _derivation_.

ProgramsA _program_\(P^{*}\) is a terminal sequence derivable from some \(\); we call a program _whole_ if it is derivable from \(\). The set of all programs is called the _language_ of the grammar \(\): \(()=\{s^{*}^{*}s\}\). The _trace_ of a program \((P)\) is the sequence of production rules \(_{1},,_{n}\) used in its derivation (\(_{1}_{n-1}  P\)). The _size_ of a program \(|P|\) is the length of its trace. The semantics of a program \(P\) is defined by the evaluation function \( P^{*}\), which maps the values of program variables to its output value.

Problem StatementA PBE problem is defined by a DSL with a grammar \(\) and an evaluation function \(\), as well as a set of input-output examples \(=\) where \(i^{*}\), \(o\). A _solution_ to the problem is a program \(P()\) such that \( i,o\), \( P(i)=o\).

### Assigning Costs to Programs

Weighted Context-free GrammarA _weighted context-free grammar_ (WCFG) \(_{w}\) is a pair of a CFG \(\) and a function \(w_{}:^{+}\) that maps each production rule \(\) to a positive weight. Given a weighted grammar \(_{w}\), we can define the _real cost_ of a program \(P\) as the sum of weights of all the productions in its trace: \(_{}(P)=_{_{i} (P)}w_{}(_{i})\).

For the purposes of search, it is convenient to define a _discrete weight_ function \(w:^{+}\), which rounds weights up to the nearest integer: \(w()= w_{}()\). The (discrete) _cost_ of a program \(P\) is defined as the sum of discrete production weights: \((P)=_{_{i}(P)}w( {R}_{i})\). Note that because of error accumulation, the discrete cost of a program can differ from its rounded real cost, but the difference can be made arbitrarily small by scaling all the costs by a constant factor \(>1\).

Probabilistic Context-free GrammarA popular way to assign weights to production rules is via a _probabilistic context-free grammar_ (PCFG). A PCFG \(_{p}\) is a pair of a CFG \(\) and a function \(p:\) that maps each production rule \(\) to its probability, such that probabilities of all the rules for a given non-terminal \(\) sum up to one: \(._{()}p()=1\). A PCFG defines a probability distribution on programs: \(p(P)=_{_{i}(P)}p(_{i})\).

Given a PCFG \((,p)\) we can derive a WCFG \(_{w}\) where \(w_{}()=-(p())\); to make sure that all weights are finite and positive, we exclude rules with \(p()=0\) and inline rules with \(p()=1\). In this WCFG, the real cost of a program is related to its probability: \(_{}(P)=-(p(P))\).

### Bottom-up Search

Bottom-up search is a popular search technique in program synthesis , which enumerates programs from the DSL in the order of increasing costs until it finds a program that satisfies the given examples. The search is implemented as a dynamic programming algorithm (see Alg. 1), which maintains a program _bank_ B mapping discrete costs to programs of that cost. Starting with an empty bank and current cost level \(=1\), the search iteratively creates all programs of cost 1, 2, 3, and so on; to create complex programs, the algorithm _reuses_ simpler programs already stored in the bank, and combines them using the production rules of the grammar.

For example, consider the CFG in Fig. 3, and assume a uniform weight function \(w()=1\). Then in the first iteration (cost level 1), the algorithm will enumerate programs consisting of a single literal or

Figure 3: A fragment from the context-free grammar of our Arc DSL.

variable--_e.g._**self**, GREY, UP, _etc_--and store them in \(\). At cost level 2, it will enumerate unary operators applied to programs stored in \(\): _e.g._**color_of(self)**, move(UP), _etc_. More generally, at cost level Lvl, the algorithm considers all available productions, and for each production, enumerates all combinations of arguments whose costs sum up to \(-1\).

During search, each candidate expression is evaluated to see if it satisfies the examples (lines 5-7). Importantly, the search maintains a cache of all evaluation results \(\), and discards the newly constructed program if it is _observationally equivalent_ to a program already in the bank (line 8), _i.e._ if it evaluates to the same output for all inputs in the examples. This step is the key to the efficiency of the bottom-up search algorithm: it allows the synthesizer to factorize the search space by evaluation result, significantly reducing the number of programs explored at each cost level.

## 3 The HySynth Approach

A key challenge in program synthesis is the astronomical size of the search space the synthesizer has to explore. For example, to find the program Eq. 1, the solution to the Arc task from the introduction, bottom-up search with a uniform weight function has to enumerate around 450K programs (all programs of size \( 16\)), which takes 4.5 minutes in our experiments.

On the other hand, sampling solutions to this task from an LLM yields programs that are _close_ to the desired solution, even if not quite correct. As we show in Appendix A, GPT-4o uses relevant components update_color, color_of, and is_neighbor in nearly all of its solutions (usually missing some part of the filter or using the wrong color in the transform), and never uses irrelevant components like move or rotate. This suggests that the LLM generally has the right intuition about the components the solution needs to use; our insight is to leverage this intuition to guide bottom-up search by _assigning lower weights to the components that the LLM uses frequently_.

### Guiding Bottom-up Search with Context-Free LLM Approximation

The overview of our approach, HySynth, is shown in Fig. 2. Given a PBE problem consisting of a DSL with grammar \(\) and a set of input-output examples \(\), HySynth proceeds in three steps.

Step 1: Sampling Solutions from an LLMHySynth starts by creating an LLM prompt that contains \(\) and \(\); the prompt can be optionally augmented with in-context examples if they are available for the given DSL. A complete prompt for the Arc running example can be found in Appendix B. The LLM is then used to sample a set \(\{S_{i}\}_{i=1}^{N}\) of completions; the choice of \(N\) trades off computational cost and the faithfulness of the approximation to the true LLM conditional.

Step 2: Learning a PCFG from LLM SolutionsNext, HySynth attempts to parse each completion \(S_{i}\) into a program \(P_{i}\) using the grammar \(\). The resulting set of programs \(\{P_{i}\}_{i=1}^{N^{}}\) (where \(N^{} N\)) is used to learn a PCFG \(_{p}\) via maximum likelihood estimation: \(p()=()+}{_{} ()+||}\). Here \(()\) is the frequency of rule R in all the derivations of the programs in \(P_{i}\) and \(\) is a smoothing parameter that ensures that every rule has a non-zero probability (typically set to 1).

Our experiments show that some models struggle to generate grammatical competitions, leading to \(N^{} N\). To increase the sampling efficiency in those cases, HySynth implements _non-strict mode_, where ungrammatical completions \(S_{i}\) are not discarded. Instead the tool performs lexical analysis on \(S_{i}\) to convert it into a sequence of terminals and approximates the frequency of each production R based on the frequency of its _operator terminal_, a designated terminal of R, which represents a DSL operator; _e.g._\((\ )=( )\).3

Step 3: Guiding Bottom-up Search with PCFGFinally, HySynth uses the PCFG computed in the previous step to derive a weighted grammar \(_{w}\) as explained in Sec. 2.2, and uses it to initialize the bottom-up search procedure in Alg. 1. As a result, the search is guided by the insights from the the LLM. For example, the WCFG learned from the GPT-4o completions for the Arc task above gives the relevant transform operator update_color weight 2, while all other _Transform_ rules have weight 4; the relevant filter operators color_of and is_neighbor are similarly down-weighted. As a result, the search procedure only has to enumerate around 220K programs instead of 450K, achieving a 4x speedup, and solving the motivating example in just one minute with LLM guidance.

### Domain-Specific Instantiations

We now describe how the HySynth approach is instantiated in three different domains: Arc grid puzzles, Tensor manipulations, and String manipulations.

Arc DomainAn example task from this domain is shown in Fig. 0(a) and has been used as a running example throughout this paper. There is no established DSL for Arc, and arguably, DSL design is the biggest challenge when attempting to solve Arc using a PBE approach, since it is hard to capture the wide variety of tasks in this domain. Our DSL is inspired by the rule-based language of Arga, which we modified slightly to make it more compositional.

A program in our DSL is a sequence of rules of the form **if**_filter_**then**_transform_. A rule refers to the current object **self**, which is modified by the transform if the filter is satisfied in the current state of the grid. The rule can also refer to other objects in the grid, such as other in Eq. 1. This program is well-defined because its filter uniquely identifies the object other; if the filter is too weak to uniquely determine the effect of the transform, the program's output is considered undefined. The full grammar of our DSL can be found in Appendix H.

Instead of searching for a complete program using Alg. 1, we further optimize our synthesizer using a divide-and-conquer strategy inspired by , searching for filters and transforms _separately_. Specifically, HySynth-Arc first searches for transforms that are correct on some objects in the grid; once it has found a set of transforms that collectively describe all grid objects, it searches for filters that distinguish between the subsets of objects changed by each transform.

Consider once again our running example. When the transform synthesizer enumerates the expression update_color(color_of(other)), it detects that this transform works for all _grey objects_, because for each grey object **self** there exists a corresponding object other whose color can be copied. Now the goal of filter synthesis is to find a boolean expression that holds exactly for those pairs of objects (**self**, other) that make the transform work. See Appendix K for more details about this algorithm.

Tensor DomainThis domain originates from the TFCoder synthesizer , which takes as input examples of a tensor transformation (with an optional natural language description) and synthesizes a TensorFlow program that performs the transformation. An example task from this domain is shown in Fig. 0(b), whose solution is: tf.gather_nd(in1, tf.stack((in2, in3), axis=-1)). The main challenge, however, is that the TensorFlow grammar is very large (see Appendix G), and most importantly, the programs are allowed to use an _unbounded_ set of constants. The original TFCodersynthesizer requires the user to provide any non-standard constants that a task might require, and, according to their paper, this is the main barrier to the usability of their tool.

For program synthesis in this domain we use the TFCoder synthesizer off the shelf. TFCoder performs weighted bottom-up search, using a combination of hand-tuned weights and weights derived by two custom-trained neural models. HySynth-Tensor replaces these weights entirely with weights computed by sampling from an LLM. Importantly, our version of the tool does not require the user to provide any constants; instead we extract constants from the LLM completions, whereby significantly reducing the burden on the user.

String DomainOur third domain involves string manipulation tasks from the SyGuS competition , which are inspired by spreadsheet use cases. An example task, which requires extracting the top-level domain name from a URL, is shown in Fig. 0(c). In this domain we use the Probe synthesizer off the shelf. Probe performs weighted bottom-up search, starting with a uniform grammar and updating the weights on the fly; HySynth-String instead initializes Probe's search with weights derived from an LLM, and disables the weight updates during search.

## 4 Experiments and Results

### Experimental Setup

We evaluate HySynth on 299 PBE tasks from three different domains: Arc (160 tasks), String (70 tasks) and Tensor (69 tasks).

Arc BenchmarkThe 160 Arc tasks are taken from the testing set of Arga. This _object-centric_ subset of the full Arc corpus is known as Object-Arc, and has been used to evaluate other Arc solvers . Arc specifications consist of 2-7 input-output training grids and 1 testing grid. Correctness is based on whether the generated solution produces the correct output on the testing grid. Our Arc DSL has a total of 20 operations and 50 constants and variables across all types.

Tensor BenchmarkThe 69 Tensor tasks taken from TFCoder focus on tensor manipulation. 49 of them are sourced from StackOverflow inquiries, and 20 are from real-world scenarios faced by TensorFlow users at Google. The overall benchmark suite consists of 72 tasks. We use three of these tasks as in-context examples and evaluate on the rest. The grammar for this domain consists of 134 Tensorflow operations, primitives like 0, 1, -1, True and other task-specific constants.

String BenchmarkThe 70 String tasks are taken from testing set of Probe, which is derived from the SyGuS benchmark . The number of examples ranges from 2 to 400. The original SyGuS benchmark have custom grammars for each task, but we use a union of all the grammars to make the search more challenging; the union grammar has 16 operations and 59 constants.

ConfigurationsOur main HySynth configuration uses Gpt40 as the LLM, with 100 samples per task to learn a PCFG in non-strict mode (_i.e._ syntactically invalid completions are included in the PCFG learning process, as explained in Sec. 3.1). For each domain, we compare the performance of HySynth with a baseline synthesizer for that domain (Arga4, Probe, and TFCoder), as well as three ablations: (1) _no search_, _i.e._ using the 100 samples from the LLM directly, (2) _unguided search_, _i.e._ running the same synthesizer but with a uniform weighted grammar, and (3) _binary surrogate_, running the synthesizer but with a _binary PCFG_, _i.e._ a CFG that includes the components present in the LLM samples with equal probabilities, and excludes all other components completely. We also analyze the performance of HySynth with different number of samples used to learn the PCFG (10, 20, and 50), with other LLMs (Gpt3.5 and DeepSeek), as well as in strict mode (which discards syntactically invalid LLM completions). The timeout is set to 10 minutes for all experiments and includes the search time and time to sample LLM completions (and compute PCFG). The average time to sample 100 solutions from Gpt40 is 4 seconds, 12 seconds and 20 seconds per task for the String, Arc and Tensor domains, respectively.

### Results

How does HySynth compare to baselines and ablations?We compare the time to solution for the main HySynth configuration, baseline synthesizers, and the three ablations; the resultsfor the three domains are shown in Fig. 3(a), Fig. 3(b), and Fig. 3(c). Overall, HySynth consistently outperforms both the baseline synthesizers and ablations, solving more tasks across all domains.

In more detail, direct LLM sampling performs very poorly on all domains, solving between 0 and 14 tasks; this confirms our hypothesis that LLMs struggle on PBE tasks in domain-specific languages, which are not prevalent in their training data. Interestingly, despite not being able to solve _any_ Tensor tasks by itself, Gpt4o provides excellent guidance for HySynth on that domain, helping it solve 96% of the total benchmark! On the other hand, synthesis guided by a binary surrogate model performs worse than HySynth (and even unguided search in case of Arc and Tensor) since the search excludes essential components from the grammar.

In String and Tensor domains, the baseline synthesizers predictably do better than unguided search, since both use the same search implementation, but with different weights. On Arc, however, our custom synthesizer outperforms Arga5 even without LLM guidance; this speaks to the efficiency of the bottom-up search and the divide-and-conquer strategy we use, which are results of years of research in the program synthesis community.

How many samples are needed to learn a PCFG?To better understand how the number of samples affects the quality of PCFG guidance, we vary the number of Gpt4o programs used in PCFG learning \(N=10,20,50,100\), and once again measure the number of tasks solved over time. The results are shown in Fig. 4(a), Fig. 4(b), and Fig. 4(c). As expected, larger sample sizes generally lead to better performance, but the difference is minimal: in Arc and Tensor, the difference between the best and worst performing versions of HySynth is only 2 problems each, while in String, HySynth solves 9 fewer problems with 10 samples than with 100. Despite these differences, all versions of HySynth still outperform the baseline and unguided search. This suggests that fewer samples are sufficient to effectively train a robust surrogate model, thereby optimizing costs.

Figure 4: (a,b,c) Number of benchmarks solved by HySynth as a function of time for the Arc, Tensor, and String domains; timeout is 10 min. (d) Percentage of syntactically valid completions per domain.

Do our results generalize to other models?To answer this question, we repeat our experiments on String and Tensor domains with Gpt3.5 and the open-source model deepseek-coder-33b-instruct (DeepSeek) . The results with these models are detailed in Fig. 9 in Appendix C, and they corroborate the pattern observed with Gpt4o, where the guided versions outperform the baseline, unguided search, and direct sampling from the LLM.

How important is non-strict mode?Fig. 3(d) shows the percentage of syntactically valid completions generated by Gpt4o and DeepSeek (where applicable). You can see that while on Tensor almost all completions are valid, this percentage falls to 78.4% for Arc and 37.5% for String; this is not surprising, given that the former are TensorFlow programs, which the model has seen during training, while the latter two are custom DSLs. In the String benchmark, the grammar is very restricted (_e.g._ only numeric constants allowed are 0-9), and the LLM has trouble adhering to this restricted grammar. But even if we were to relax the definition of syntactic validity, LLM solutions would achieve a syntactic validity of only 47%. Hence our non-strict mode proves especially helpful for low-resource domains, where otherwise we would have to discard a large proportion of completions. At the same time, we find that _given the same number of completions to learn from_, the PCFGs learned in non-strict mode are just as effective as those learned in strict mode: as shown in Fig. 4(d), HySynth-Arc with the guidance from 100 Gpt4o completions solves 58 tasks _in either mode_ (with the difference that strict mode has to sample more completions to get 100 valid ones).

### Limitations

The main limitation of our hybrid approach _wrt._ purely neural approaches is that it requires implementing a synthesizer for each DSL of interest; although we have shown that the same bottom-up search can be used across different domains, some implementation effort is still required. On the other hand, compared to purely symbolic approaches, our method requires sampling from an LLM, which is costly; additionally, the guidance provided by our approach is only as good as the LLM's completions: if they contain many irrelevant operators, our guided search can be _slower_ than unguided

Figure 5: HySynth-Arc, HySynth-Tensor and HySynth-String results guided by a PCFG learned from different number of Gpt4o samples (n=10, 20, 50, 100).

search. Finally, our experiments are subject to the usual threat that the LLMs might have seen our benchmarks in their training data; we do not consider it a major issue, however, given that our main result is the superior performance of guided search _relative_ to using LLMs without search.

## 5 Related Work

Guiding Program Synthesis with Probabilistic ModelsThe traditional approach to _program synthesis_ is based on combinatorial search , augmented with pruning techniques based on program semantics [2; 6; 39]. To further speed up the search, researchers have proposed _guiding_ the search with a learned probabilistic model. Most approaches to guided search use special-purpose models that have to be trained on a domain-specific corpus of programs  or PBE tasks [9; 24; 32; 37]. Although some of these models can be trained on synthetic data, the training process is still expensive and requires manual tuning, which makes it hard to apply these techniques to new domains.

With the advent of pretrained Large Language Models (LLMs), it seems only natural to use them to guide search-based program synthesis, thus alleviating the need for domain-specific training data. We are only aware of one other attempt to do this: concurrent work by Li et al. , which also extracts a PCFG from the LLM's samples, similarly to HySynth. An important difference is that they use the PCFG to guide _top-down_ A* search, while we use it to guide _bottom-up_ search, which is known to be more efficient (they also evaluate their tool on synthesis from logical formulas as opposed to PBE).

Solving the Abstraction and Reasoning CorpusAll state-of-the-art solvers for this benchmark have relied on carefully curated DSLs for Arc[3; 13; 19; 27; 43]. Xu et al.  proposed the DSL we extend in our approach, and the Object-Arc subset we evaluate on. Lei et al.  embed their DSL as a subset of PDDL and use a Generalized Planning (GP) algorithm as their search component. They have the current best performance on Object-Arc, however they encode more domain-knowledge in the form of preconditions and per-abstraction restrictions on filters and transforms, to make GP viable. Our approach does not require this additional information. [3; 10] use DreamCoder , to perform execution-guided search over a DSL for grid manipulations, however they only provide proof-of-concept evaluations. [38; 41] also use an LLM to generate code given the spec of the task. Both of these approaches interact with the model across several rounds, while our technique uses the suggestions from the LLM only as a starting point. Our technique also performs a complete search guided by the LLM distribution, enabled by the structure of our DSL, whereas previous approaches only consider code directly generated by the LLM.

## 6 Conclusion and Future Work

Our approach introduces a robust technique for using both valid and invalid completions from an LLM to learn a surrogate model. By incorporating ungrammatical completions, we can extract useful insights that would otherwise be discarded. Overall, we provide an alternative to the conventional strategy of large-scale sampling from LLMs, proposing a more effective use of the available completions to guide the search process. An interesting future direction would be to guide search with a more expressive context-dependent surrogate model.