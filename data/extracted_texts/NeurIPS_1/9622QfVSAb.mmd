# Implicit Multimodal Alignment: On the Generalization of Frozen LLMs to Multimodal Inputs

Mustafa Shukor\({}^{1}\)

Contact: {firstname.lastname}@sorbonne-universite.fr

Matthieu Cord\({}^{1,2}\)

\({}^{1}\)Sorbonne University, \({}^{2}\)Valeo.ai

###### Abstract

Large Language Models (LLMs) have demonstrated impressive performance on multimodal tasks, without any multimodal finetuning. They are the _de facto_ building block for Large Multimodal Models (LMMs), yet, we still lack a proper understanding of their success. In this work, we expose frozen LLMs to image, video, audio and text inputs and analyse their internal representation aiming to understand their generalization beyond textual inputs. Our work provides the following **findings.** Perceptual tokens (1) are easily distinguishable from textual ones inside LLMs, with significantly different representations (_e.g._ live in different narrow cones), and complete translation to textual tokens does not exist. Yet, (2) both perceptual and textual tokens activate similar LLM weights. Despite being different, (3) perceptual tokens are implicitly aligned to textual tokens inside LLMs, we call this the implicit multimodal alignment effect (IMA), and argue that this is linked to architectural design, helping LLMs to generalize. This provide more evidence to believe that the generalization of LLMs to multimodal inputs is mainly due to their architecture. These findings lead to several **implications.** (1) We find a positive correlation between the implicit alignment score and the task performance, suggesting that this could act as a proxy metric for model evaluation and selection. (2) A negative correlation exists regarding hallucinations (_e.g._ describing non-existing objects in images), revealing that this problem is mainly due to misalignment between the internal perceptual and textual representations. (3) Perceptual tokens change slightly throughout the model, thus, we propose different approaches to skip computations (_e.g._ in FFN layers), and significantly reduce the inference cost. (4) Due to the slowly changing embeddings across layers, and the high overlap between textual and multimodal activated weights, we compress LLMs by keeping only 1 subnetwork (called \(\)-SubNet) that works well across a wide range of multimodal tasks. The code is available here: [https://github.com/mshukor/ima-lmms](https://github.com/mshukor/ima-lmms).

## 1 Introduction

Large Language Models (LLMs)  represent a noteworthy advancement in recent AI developments. Building upon the success of LLMs, the next stride in this field involves extending beyond the textual modality, giving rise to Large Multimodal Models (LMMs) . A notable line of research involves connecting LLMs to visual encoders, while keeping them frozen and only training a connector with modest number of parameters . These methods yield comparable performance  to large-scale multimodal models with significantly reduced computational and data budget.

Keeping all pretrained unimodal models frozen and only training couple of millions of parameters  is an interesting phenomenon to understand, with limited research trying to decipher it. Toexplain why frozen LLMs can generalize beyond textual inputs, several hypotheses can be isolated: (1) perceptual tokens are transformed to textual ones, can be simply considered as foreign language , and thus the LLM sees text-only tokens. (2) LLMs are able to digest non-textual tokens that are processed by (2a) modality-specific subnetworks or (2b) the same LLM weights that can generalize due to other reasons.

In this study, we expose LLMs to different multimodal inputs, such as image, video, audio and text, and analyse their internal representations. We focus on frozen LLMs and consider two representative setups: single-task (ST) and multitask (MT) finetuning. The former is considered by parameter and data-efficient approaches  and consists of training a mapping module for each dataset. The MT setup is considered by recent multimodal assistant models , and consists of training the same mapping module on several datasets/tasks.

Our study shows (1) that perceptual and textual tokens still live in significantly different representation spaces inside LLMs (Sec. 3.1): live in different narrow cones, and have different norms, rate of change and vocabulary distributions. (2) We notice high similarity between the weights activated by textual and perceptual tokens (Sec. 3.2), allowing to swap these activated subnetworks between different tasks and modalities. Despite their differences, (3) perceptual tokens are implicitly aligned to textual ones across different stages (Sec. 4.1): during training of the mapping module, and during inference across LLM layers, especially inside each LLM block (_e.g._ after the self-attention layer). As there is no explicit objective to align these representations, we call it the Implicit Multimodal Alignment effect (IMA) (_i.e._, increasing similarity between textual and perceptual token distributions). We find that this effect is mostly linked to architectural design (_e.g._ residual stream with refinement blocks acting as steering blocks Sec. 4.2). This provides more evidence to believe the architecture of LLMs is one of the main factors to generalize to multimodal representations.

We shed light on several practical implications (Sec. 5). (1) We find a positive correlation between the implicit multimodal alignment score and the task performance, suggesting that this score could act as a proxy metric. On the other hand, (2) we find a negative correlation with hallucinations, revealing that the main factor leading to this problem is the lack of alignment between the internal representation of textual and perceptual inputs. (3) The perceptual tokens slightly change inside LLMs, thus, we propose to skip their computations (_e.g._ inside FFN layers). (4) Due to the slowly changing embedding across layers, and the high overlap between weights activated by different modalities, we compress the LLM by keeping one task-agnostic subnetwork that works well across all modalities. To summarize (Fig. 1), we analyse the internal representations of LLMs when exposed to multimodal inputs, leading to the following **findings**:

* Perceptual and textual tokens live in different representation spaces inside LLMs.
* They activate similar LLM weights.
* They are implicitly aligned (IMA) inside LLMs, during training and during inference.

Figure 1: **Summary of the work. We start by analysing multimodal tokens inside LLMs, and find that they live in different spaces (_e.g._, multimodal cones). Yet they are implicitly aligned (_i.e._, IMA), allowing us to see LLMs as residual streams with steering blocks. This lead to implications on performance, safety and efficiency.**

* The architectural design of LLMs can be perceived as a residual stream with steering blocks. We argue that this is one of the main factors allowing LLMs to: digest very different tokens, drive the implicit multimodal alignment effect, and thus generalize to different modalities.

These findings have several practical **implications** such as:

* The IMA score as a proxy metric candidate for task performance and hallucinations.
* Hallucinations as a result of lack of sufficient multimodal alignment.
* Skipping computations for visual tokens, leading to efficient inference.
* LLMs compression by keeping only 1 subnetwork that generalizes to all multimodal tasks.

## 2 Framework for analysing perceptually augmented LLMs

General frameworkWe focus on a general family of models that consists of: a frozen language model \(LLM\) with \(L\) layers, a trainable mapping module \(C\), and a frozen perceptual encoder \(E_{M}\) for different modalities \(M\) (_e.g._ image (I), video (V) and audio (A)). The \(LLM\) input \(X\) consists of the concatenation of \(P=[p_{1},...,p_{N_{p}}]\) multimodal/perceptual tokens (referred to as prompt) with \(T=[t_{1},...,t_{N_{t}}]\) textual tokens. The prompt \(P\) is obtained after encoding the modality-specific input \(XM\) with the corresponding \(E_{M}\) and using \(C\) to project it to the \(LLM\) input space. \(T\) is obtained from the embedding layer \(E_{T}\) applied to the tokenized input text \(XT\). This can be expressed as follows:

\[P=C(E_{M}(XM)), T=E_{T}(XT), O=LLM([P;T]). \]

The \(k\) (\(k=N_{p}+N_{t}\)) output tokens \(O=[o_{i},...,o_{k}]\) are obtained after a normalization, followed by the unembedding layer \(W_{out}\) (or LLM head, _i.e._\(o_{i}=W_{out}LN_{out}(t_{i}^{L})\)). Our focus is on the internal representation of LLMs (_i.e._ tokens) at different stages, in particular across the \(L\) LLM blocks/layers (referred to as B). The mechanism inside the \(l+1\) LLM transformer block can be expressed as follows:

\[X^{l+1}=X_{SA}+FC2(g(FC1(LN2(X_{SA})))), X_{SA}=X^{l}+SA(LN1(X^{l})), \]

where \(FC1\), \(FC2\), \(g\) are the up and down projections and activation inside the \(FFN\), \(LN1/2\) are the layer norms and \(SA\) the self-attention.

Perceptually augmented LLM baselines.For the single-task (ST) setup, we train many models across different datasets that span image, video and audio-text modalities. Each mapping module is trained on a specific dataset, similar to previous works [10; 12; 9]. Inspired by previous studies [10; 12], we use light-weight transformer consisting of a self-attention to attend to perceptual tokens. In this setup, \(P\) refer to perceptual tokens from image, video and audio modalities. For the multitask (MT) setup, we devise different variants of the LLaVA-1.5-2  model that differ from the original model as follows: LLaVA-1.5-2 (LLM kept frozen), LLaVA-1.5-3 (LLM kept frozen, without pretraining) and LLaVA-1.5-4 (LLM kept frozen, without pretraining and with transformer mapping module similar to the ST setup instead of MLP). In this setup, \(P\) refers to image tokens from different datasets. In the paper, we focus on LLaVA-1.5-4 as it is most similar to the ST setup, and analyse other variants in App. E. For analysis (_i.e._ Sec. 3), we focus on Vicuna-v1.5-7B  as it is shared by both setups. For the ST, we use unimodal encoders, such as ViT , TimeSformer  and AST  that are not aligned with text. More implementation details, and experiments with other backbones can be found in App. D and App. E. We report the similarity after averaging the tokens SimAvg (Eq. (3)) and details other measures in App. E.

Analysis tools.We are interested in cross-modal or multimodal alignment, and define the alignment in terms of the cosine similarity; the higher the score, the more the vector representations are pointing in similar directions. This could also indicates how much the two token distributions or vectors are close, in terms of L2 distance (assuming the vectors are normalized and in a narrow cones). In other words, alignment and similarity terms can be used interchangeably in the paper. In addition to cosine similarity we also study their norm, decoded vocabulary distributions and which LLM weights they activate. In the paper, we focus on the global representation per example, by analysing their average across the sequence. More finegrained analysis on the token level with different similarity and norm measures gives similar observations and are detailed in App. E. For instance, we compute the cosine similarity between perceptual (\(P\)) (_e.g._, tokens corresponding to image patches) and textual (\(T\)) tokens (_e.g._, tokens corresponding to the image caption), after the block \(l\) as follows:

\[(P^{l},T^{l})= }}}{\|}\|\|}\|},}=^{N_{p}}p_{i}^{l}}{N_{p}},}= ^{N_{t}}t_{i}^{l}}{N_{t}}, \]

## 3 LLMs indeed generalize to non-textual tokens

We investigate the generalization of LLMs to multimodal inputs, by studying the perceptual and textual tokens inside LLMs. We investigate if all tokens are projected to textual ones, or rather they are still different and how so (results with other models and similarity measures in App. E).

### How perceptual tokens differ from textual ones?

Multimodal cones: different narrow cones for different modalities (Fig. 2).Previous works  have found the representation of contextualized embeddings in language models to be anisotropic: embeddings of different inputs exhibit high cosine similarity, shaping a narrow cone, where all embeddings point in the same narrow direction. In the multimodal domain, the cone effect is also observed  in contrastive models (CLIP ). In this section, we investigate if textual and multimodal tokens live in narrow cones inside LLMs, and if these cones are distinct. We compute the tokens cosine similarity at different layers. In particular, the unimodal similarity: text-only (\(T\) vs \(T\)) and perceptual-only (\(P\) vs \(P\)), and the cross-modal similarity (\(P\) vs \(T\)) between perceptual and textual tokens. Note that for the ST setup, \(P\) vs \(P\) covers the similarity between image, video and audio tokens, while for the MT ones cover image tokens from different datasets. Fig. 2 shows a clear narrow cone effect for textual and perceptual tokens. Different perceptual modalities seem to live in different narrow cones, as shown by the low \(P\) vs \(P\) score for the ST setup. Interestingly, the cross-modal similarity between textual and perceptual tokens (\(P\) vs \(T\)) is significantly lower, suggesting that textual and perceptual tokens also live in different narrow cones. We also visualize the t-SNE of the tokens embeddings showing they stay separated inside the LLM.

Figure 3: **Tokens norm and evolution across LLM layers.** The tokenwise cosine similarity between consecutive blocks (e.g. \(X^{l+n}\) and \(X^{l}\)), and the median token L2 norm after each block (\(X^{l}\)) for the ST (left) and MT (right) setups. Textual and visual tokens evolve differently inside LLMs.

Figure 2: **Multimodal narrow cones.** The cosine similarity after LLM blocks (B) between: perceptual tokens (\(P\) vs \(P\)), textual tokens (\(T\) vs \(T\)), perceptual and textual tokens (\(P\) vs \(T\)). \(p\) vs \(p\) and \(t\) vs \(t\) refer to the intra similarity within the same dataset. We also visualize the t-SNE of tokens (at layer 24) showing they stay separated inside the model. V (Video), I (Image), A (Audio).

Different token norms and evolution across layers (Fig. 3).We compute the median of the token L2 norms after each LLM block. This shows that textual and perceptual tokens have different norms across layers. Perceptual tokens have significantly higher norm (at the beginning for MT and across all layers for ST), and they change significantly less. When looking at other norm measures, we found perceptual tokens with massive norms, similarly for textual ones , especially for the ST setup. We discuss massive tokens more in App. E.2. In addition, we compute the cosine similarity between tokens at block \(l\) and block \(l+n\), showing that textual and perceptual tokens have different change rates. Textual ones change drastically at the beginning of the LLM, while perceptual ones changes significantly less across all layers.

Different token vocabulary distributions across layers (Fig. 4).For each token, we use the LLM unembedding (_i.e._ LLM head) to decode the latent representation to a probability distribution over the vocabulary. This approach have shown to work well for LLMs at different layers, not just the last one . We show the histogram of this distribution at the first LLM layer for both textual and perceptual tokens. The histograms show clear differences with some overlap between textual and perceptual prompts. In addition, we compute the KL-distance showing that the distributions diverge from each other across LLMs layers. We also notice that the distributions of textual tokens evolve significantly, compared to multimodal ones. This is shown by computing the KL-distance between consecutive blocks and the distribution entropy.

_Finding 1._Textual and perceptual tokens live in significantly different representation spaces inside LLMs.

### Do perceptual tokens traverse different paths inside LLMs?

For each trained model, we extract the LLM (frozen) subnetwork activated by each dataset/modality. We study these subnetworks (we refer to as pruning masks) by computing their similarity. We leverage the recent SoTA pruning approach (Wanda ), that prune models based on both the weights and the activation norms. Specifically, we use a handful (_e.g._ 256) of calibrated examples coming from different modalities, and keep only p% (1 - sparsity) of weights with the highest Wanda score, at different sparsity levels (30 % and 50 %). Note that after removing more than 50% of weights we observe a severe degradation of performance.

Similar activated weights across modalities, in the first and deeper layers (Fig. 5).Each subnetwork is represented as a binary mask to indicate which weights are activated. To compute the similarity between these networks, we consider the intersection over union (IoU). Results show an interesting high similarity between subnetworks activated by to different modalities. This high overlap is more seen for the ST setup, for example, the IoU between GQA and VQAv2 is 0.69, similar to GQA vs Audiocaps (0.67) or COCO-Text (0.65). When looking at the IoU across layers, we notice an interesting high score at first layers. It seems that first layers encode general features that are common for all modalities. This similarity increases as we go deeper in the LLM, moving to more abstract and less modality-specific representations, closer to the textual output.

Transfer of multimodal subnetworks across tasks and modalities (Fig. 6).To further validate the previous section, we study if we can simply interchange pruning masks between different tasks.

Figure 4: **Tokens vocabulary distribution inside LLMs. The LLM (Vicuna-v1.5) unembedding layer is used to map each token at different LLM layer, to a probability distribution over the vocabulary. Multimodal tokens exhibit different vocabulary distributions across layers**

Specifically, for a given model trained to solve a particular task, we find the pruning mask using calibration data corresponding to different tasks/datasets. The sparsity is set to 30%, which is often used to maintain reasonable performance. Fig. 6 shows that the pruning masks transfer very well across tasks within the same modality (_e.g._ slight degradation by \(\)1 point CIDEr for captioning with a mask coming from OKVQA). Similarly, we interchange masks across modalities. We fix the task (captioning) and also consider the text modalities (captions without images). In general, we observe similar transfer with a slight performance degradation, especially for OPT and Llama 2. We show similar observations with higher sparsity and with other encoders (_e.g._, CLIP and MAE) in App. E.4.

Modality-specific subnetworks?The experiments suggest a high overlap between weights activated by different modalities. However, this does not exclude the possibility of finding weights that are generally activated when seeing a particular modality, even if there are small amount of them. More discussion about this can be found in App. E.4.

_Finding 2._LLM weights activated by perceptual and textual tokens overlap significantly.

## 4 What helps LLMs to generalize to multimodal tokens?

Textual and perceptual tokens have very different representations inside LLMs, yet, LLMs are still able to process and generalize to these non-textual tokens. In this section, we try to investigate why this is possible, in particular, we identify which factors facilitate this generalization.

Figure 5: **IoUs of multimodal subnetworks**. IoU of the subnetworks activated by different tasks and modalities, for the ST (left) and MT (right) setups. We show the evolution of IoU across LLM layers and across different multimodal tasks. Different modalities activate similar LLM weights (Fig. 22 for clearer version of the figure).

Figure 6: **Transfer of multimodal subnetworks across tasks and modalities**. The subnetwork activated by a given task is used for other tasks for Vicuna-v1.5. From left to right, transfer across: image, video and audio tasks. In each figure, the row corresponds to the subnetwork source dataset and the column to the target dataset. bottom: transfer across modalities for (from left to right): OPT, Llama 2, Vicuna-v1.5.

### Observation: the Implicit Multimodal Alignment Effect (IMA)

Implicit alignment during training of the mapping module (Fig. 7).We compute the cosine similarity between the perceptual tokens at the output of the mapping module, and textual tokens at different LLM blocks. Results show that this similarity increases at all layers. This reveals that the mapping module role, is not just to adapt the dimension of the visual tokens, but also to project the visual tokens to be semantically, as similar as possible to the textual ones.

Implicit alignment during inference, across LLM blocks (Fig. 7).We compute the cosine similarity between perceptual and textual tokens after each LLM block. Here we compute the max of tokenwise similarity (MaxSim App. E.1): for each pair of token sequences coming from one example (_e.g._ image prompt + caption), we take the maximum similarity, then we average across all examples. The tokenwise similarity between perceptual and textual tokens significantly increases, especially in the middle blocks, where the alignment is the highest in deep layers.

Implicit alignment during inference, inside LLM blocks (outside the residual stream) (Fig. 8).We look deeper to investigate the source of this alignment, and focus on tokens inside the LLM block, which consists mainly of a self-attention (SA), FFN (FC1/2) and layer norms (LN1/2) layers. Interestingly, the similarity between textual and multimodal tokens is the highest after the SA layers.

_Finding 3._An implicit multimodal alignment emerges to pull the textual and perceptual tokens closer inside LLMs, during training and inference.

Figure 8: **Multimodal tokens norms and similarity inside LLM blocks. Token norms (left), tokens cosine similarity between consecutive blocks (middle) and between perceptual and textual tokens (last). The tokens are inside Vicuna-v1.5 blocks (and outside the residual stream): after the self-attention (SA), and FFNs (FC1/2) and layer norms (LN1/2). Multimodal tokens are Implicit alignment inside LLM blocks.**

Figure 7: **Multimodal tokens similarity across LLM layers. The cosine similarity between the textual and multimodal tokens across: training epochs i.e., 0, 1, 2 for Vicuna-v1.5 (first), and across LLMs layers: Vicuna-v1.5 (second) and LLaVA-1.5-4 (last). Textual and multimodal tokens are implicitly aligned during training, and during inference across LLM blocks.**

### Explanation: the architectural inductive bias hypothesis

Residual stream with refinement blocks.We notice different observations between the tokens inside and outside the residual stream. In the residual stream, the perceptual and textual tokens exhibit significant representation differences (Sec. 3.1), while outside the residual stream, they are more aligned. Each block contributes slightly to the residual stream (small token norms inside the blocks Fig. 7(a)), with significantly different contributions (cosine similarity between consecutive blocks close to zero, _e.g_, after the FC1/2 Fig. 7(b)). This allows us to view the model as a series of refinement blocks that try to gradually refine the input signals. As the original signals are significantly different, they stay different in the residual stream throughout the model. We argue that this provide a flexibility to handle too different inputs. Moreover, previous works  have shown that transformers contain both elements with high and low complexity biases, which helps to build general-purpose architectures  that are able to generalize. These works support further our findings.

Refinement blocks as steering blocks.Inside the transformer block, we notice that the layer normalization play an important role in having comparable norms for both textual and perceptual tokens (Fig. 7(a)). Perceptual token norms become smaller and closer to textual ones as we traverse several layers in the block. In terms of cross-modal similarity, we notice the highest similarity after the SA, then after the FC2 and LN1. Note that this similarity is higher inside the block, than in the residual stream (_e.g_. 0.45 vs 0.1 for Vicuna-v1.5 and 0.58 vs 0.15 for LLaVA-1.5-4 in the residual stream Fig. 2). After each block the cross-modal alignment increases, and hence the narrow cones are steered to each other. This suggests that all layers play an important role in steering the textual and perceptual narrows cones to be aligned, with the most contributions coming from the SA.

_Finding 4._An LLM can be seen as a residual stream with refinement blocks acting as steering blocks. This architecture design plays an important role in generalizing to very different tokens, and hence other modalities.

## 5 Implications: performance, safety and efficiency

Implicit alignment as a proxy metric for task performance? (Fig. 8(a))We compute the cosine similarity between perceptual tokens at the LLM input and the textual tokens across LLM layers. The similarity increases during training. Interestingly, we notice a clear and positive correlation with the task performance on several multimodal benchmarks. In addition, we find that this correlation exists across different models, as shown for different LLaVA-1.5 variants.

Implicit alignment as a proxy metric for hallucination? (Fig. 8(b))Previous works have shown that LMMs suffer from severe hallucinations , and generally try to tackle this problem by training on better datasets , using RLHF or RLAIF  or post-training heuristics . Here we highlight one of the main causes of hallucinations: which is the lack of internal alignment between textual and perceptual representations. we show the cosine similarity between textual and perceptual tokens after each LLM block, and report the hallucinations on POPE  and COCO  benchmarks. The curves show clear correlation between the implicit alignment and the hallucinations.

Figure 8: **Implicit alignment as a proxy metric for task performance.** Left: different checkpoints of LLaVA-1.5-4. Right: different variants of the LLaVA-1.5 model. We show the cross-modal token cosine similarity across layers, and the task performance across different benchmarks.

Skipping computations for visual tokens (Fig. 11).In Sec. 3.1 we show that perceptual tokens change significantly less across layers, compared to textual ones. Sec. 4 highlights the importance of SA layers for cross-modal alignment. In this section, we leverage these observations to reduce the LLM computation overhead by skipping the computations of visual tokens. Specifically, starting from a given start layer (sl), we reduce computations in FFN layers, which accounts for almost 2/3 of model weights, by skipping p% (Skip ratio) of visual tokens. Fig. 11 shows that skipping the visual tokens leads only to slight decrease in performance, while reducing significantly the amount of compute. We provide additional results with the ST setup, and ablation study in App. F.3.

\(\)**-SubNet: one LLM Subnetwork for all multimodal tasks (Fig. 12)**. Despite their differences, multimodal tokens share an important property: slowly changing embeddings across layers (Sec. 3.1). This suggests the possibility of compressing the model while retaining reasonable performance. In addition, textual and multimodal tokens are pulled closer inside the LLM (Sec. 3.2), and processed by almost the same LLM weights (Sec. 4), especially for the ST setup. This suggests the possibility of finding a common subnetwork (\(\)-SubNet) that works well for all multimodal tasks. Thus, we focus on the ST setup with the OPT and CLIP encoders that are currently used by previous works. We consider two representative tasks: COCO image captioning and VQAv2 and provide similar results for other tasks, and modalities in App. F.4. First we use Wanda for task-specific pruning

Figure 11: **Skipping computations for visual tokens**. Skipping (Skip ratio)% of the tokens in the FFN layers. sl: skipping start layer. (V): visual tokens. (T): textual tokens. Results on the MT (with LLaVA-1.5) setup.

Figure 12: \(\)**-SubNet: a modality-agnostic subnetwork.** Left: illustration of how we obtain the \(\)-SubNet. Right: different methods to compress multimodal LLMs (OPT). Table 2.

Figure 10: **Implicit alignment as a proxy metric for hallucinations.** Left: different checkpoints of LLaVA-1.5-4. Right: different variants of the LLaVA-1.5 model. We show the cross-modal token cosine similarity across layers, and the hallucinations across different benchmarks.

and show in Fig. 12 that we can obtain scores close to the original ones while removing 50% of the weights. To find the task and modality agnostic \(\)-SubNet, we first extract many pruning masks (_e.g._ at 30% sparsity) for different modalities, then take the intersection of all these masks (_e.g._ leading to a global mask at \(\) 50% sparsity). This approach is significantly better than other baselines such as magnitude pruning or a random mask, and leads to comparable performance compared to the task-specific Wanda pruning, especially for VQAv2.

## 6 Discussion

Limitations.The paper focuses on open-source and frozen LLMs up to 7B parameters, LMMs that concatenate perceptual tokens at the LLM input and are relatively efficient. The generalization of our findings, to larger and more powerful models, with different architectures, including proprietary ones remains to be seen. Detailed discussion in App. B.

Conclusion.We propose the first study of the internal representation of frozen LLMs when exposed to multimodal inputs. We find very different representations for perceptual and textual tokens, yet LLMs are still able to generalize to these non-textual tokens. The implicit multimodal alignment (IMA) effect, linked mostly to architectural design, facilitates this generalization by bringing multimodal tokens closer inside the LLM. Our findings have several implications, such as as reducing the computation resources at inference time, understanding better the performance as well as safety-related problems such as hallucinations. We hope that this study will have positive impact, pushing for more works to understand multimodal LLMs, and pave the way to devise more powerful models that are better aligned to human preferences, while targeting safety-related issues.

## 7 Acknowledgments

The authors would like to thank Arnaud Dapogny and Edouard Yvinec for fruitful discussions, and Damien Teney and Alexandre Rame for their helpful feedback on the paper. This work was partly supported by ANR grant VISA DEEP (ANR-20-CHIA-0022), and HPC resources of IDRIS under the allocation 2024-[AD011013415R2] made by GENCI.