# Using Time-Aware Graph Neural Networks to Predict Temporal Centralities in Dynamic Graphs

Franziska Heeg

Chair of Machine Learning for Complex Networks

Center for Artificial Intelligence and Data Science (CAIDAS)

Julius-Maximilans-Universitat, Wurzburg

franziska.heeg@uni-wuerzburg.de

Ingo Scholtes

Chair of Machine Learning for Complex Networks

Center for Artificial Intelligence and Data Science (CAIDAS)

Julius-Maximilans-Universitat, Wurzburg

ingo.scholtes@uni-wuerzburg.de

###### Abstract

Node centralities play a pivotal role in network science, social network analysis, and recommender systems. In temporal data, static path-based centralities like closeness or betweenness can give misleading results about the true importance of nodes in a temporal graph. To address this issue, temporal generalizations of betweenness and closeness have been defined that are based on the shortest time-respecting paths between pairs of nodes. However, a major issue of those generalizations is that the calculation of such paths is computationally expensive. Addressing this issue, we study the application of De Bruijn Graph Neural Networks (DBGNN), a time-aware graph neural network architecture, to predict temporal path-based centralities in time series data. We experimentally evaluate our approach in 13 temporal graphs from biological and social systems and show that it considerably improves the prediction of betweenness and closeness centrality compared to (i) a static Graph Convolutional Neural Network, (ii) an efficient sampling-based approximation technique for temporal betweenness, and (iii) two state-of-the-art time-aware graph learning techniques for dynamic graphs.

## 1 Motivation

Node centralities are important in the analysis of complex networks, with applications in network science, social network analysis, and recommender systems. An important class of centrality measures are _path-based centralities_ like, e.g. betweenness or closeness centrality , which are based on the shortest paths between all nodes. While centralities in static networks are important, we increasingly have access to time series data on temporal graphs with time-stamped edges. Due to the timing and ordering of those edges, the paths in a static time-aggregated representation of such time series data can considerably differ from _time-respecting paths_ in the corresponding temporal graph. In a nutshell, two time-stamped edges \((u,v;t)\) and \((v,w;t^{})\) only form a time-respecting path from node \(u\) via \(v\) to \(w\) iff for the time stamps \(t\) and \(t^{}\) we have \(t<t^{}\), i.e. time-respecting paths must minimally respect the arrow of time. Moreover, we often consider scenarios where we need to additionally account for a _maximum time difference_\(\) between time-stamped edges, i.e. we require \(0<t^{}-t\). Several works have shown that temporal correlations in the sequence of time-stamped edges can significantly change the _causal_ topology of a temporal graph, i.e. which nodes can influence each other via time-respecting paths, compared to what is expected based on the static topology .

An important consequence of this is that static path-based centralities like closeness or betweenness can give misleading results about the true importance of nodes in temporal graphs. To address this issue, temporal generalizations of betweenness and closeness centrality have been defined that are based on the shortest time-respecting paths between pairs of nodes . A major issue of those generalizations is that the calculation of time-respecting paths as well as the resulting centralities is computationally expensive . Addressing this issue, a number of recent works developed methods to approximate temporal betweenness and closeness centralities in temporal graphs . Additionally, few works have used deep (representation) learning techniques to predict computationally expensive path-based centralities in _static_ networks .

Research Gap and ContributionsTo the best of our knowledge, no prior works have considered the application of time-aware graph neural networks to predict path-based centralities in temporal graphs. Closing this gap, our work makes the following contributions:

* We introduce the problem of predicting temporal betweenness and closeness centralities of nodes in temporal graphs. We consider a situation where we have access to a training graph as well as ground truth temporal centralities and seek to predict the centralities of nodes in a future observation of the same graph, which does not necessarily contain the same node set.
* To address this problem, we introduce a deep learning method that utilizes De Bruijn Graph Neural Networks (DBGNN), a recently proposed time-aware graph neural network architecture  that is based on higher-order graph models of time-respecting paths, which capture correlations in the sequence of time-stamped edges. An overview of our approach in a toy example of a temporal graph is shown in Figure 1.
* We compare our method to a Graph Convolutional Network (GCN), which only considers a static, time-aggregated weighted graph that captures the frequency and topology of edges. Evaluating our approach against two deep learning methods for temporal graphs, we consider the time-aware graph embedding method EVO  as well as the Temporal Graph Network (TGN) framework . We further compare our method to ONBRA, which efficiently approximates temporal betweenness centralities of nodes to varying degrees of accuracy.
* We experimentally evaluate all models in 13 temporal graphs from biological and social systems. Our results show that the application of the time-aware DBGNN architecture considerable improves the prediction of both betweenness and closeness centrality compared to other static and time-aware graph learning techniques. Our method outperforms ONBRA for the prediction of temporal betweenness centralities in large datasets.

In summary, we show that predicting temporal centralities is an interesting temporal graph learning problem, which could be included in community benchmarks . Moreover, our study highlights the potential of time-aware deep learning architectures for node-level regression tasks in temporal graphs. Finally, our results are a promising step towards an approximation of temporal centralities in large data, with potential applications in social network analysis and recommender systems.

## 2 Background and Related Work

In the following, we provide the background of our work. We first introduce temporal graphs and define time-respecting paths. We then cover generalizations of path-based centralities for nodes in temporal graphs. We finally discuss prior works that have studied the prediction, or approximation, of path-based centralities both in static and temporal graphs. This will motivate the research gap that is addressed by our work.

Dynamic Graphs and Time-respecting PathsApart from static graphs \(G=(V,E)\) that capture the topology of edges \(E V V\) between nodes \(V\), we increasingly have access to time-stamped interactions that can be modelled as _temporal graphs or networks_. We define a temporal graph as \(G^{}=(V,E^{})\) where \(V\) is the set of nodes and \(E^{} V V\) is a set of (possibly directed) time-stamped edges, i.e. an edge \((v,w;t) E^{}\) describes an interaction from node \(v\) to \(w\) occurring at time \(t\). In our work, we assume that interactions are _instantaneous_, i.e. \((v,w;t) E^{}\) does not imply that \((v,w;t^{}) E^{}\) for all \(t^{}>t\). Hence, we do not specifically consider _growing networks_, where the time-stamp \(t\) is the creation of an edge. For a temporal network \(G^{}=(V,E^{})\) it is common to consider a static, time-aggregated and weighted graph representation \(G=(V,E)\), where \((v,w) E\) iff \((v,w;t) E^{}\) for some time stamp \(t\) and for the edge weights we define \(w(v,w)=|\{t:(v,w;t) E^{}\}|\), i.e. the number of occurrences of time-stamped edges.

An important difference to the static case is that, in temporal graphs, the temporal ordering of edges determines _time-respecting paths_. For temporal graph \(G^{}=(V,E^{})\) we define a _time-respecting path_ of length \(l\) as node sequence \(v_{0},,v_{l}\) such that the following conditions hold:

* \(\;t_{1},,t_{l}\;:\;(v_{i-1},v_{i};t_{i}) E^{}\) for \(i=1,,l\) ;
* \(0<t_{i}-t_{i-1}\) for some \(\).

In contrast to definitions of time-respecting paths that only require interactions to occur in ascending temporal order, i.e. \(0<t_{i}-t_{j}\) for \(j<i\), we also impose a maximum "waiting time" \(\). This implies that we only consider time-respecting paths where subsequent interactions occur within a time interval that is often defined by the processes that we study on temporal networks . In line with the definition for static networks, we define a _shortest time-respecting path_ between nodes \(v\) and \(w\) as a (not necessarily unique) time-respecting path of length \(l\) such that all other time-respecting paths from \(v\) to \(w\) have length \(l^{} l\). In static graphs a shortest path from \(v\) to \(w\) is necessarily a _simple_ path, i.e. a path where no node occurs more than once in the sequence \(v_{1},,v_{l}\). This is not necessarily true for shortest time-respecting path, since -due to the maximum waiting time \(\)- we may be forced to move between (possibly the same) nodes to continue a time-respecting path. Due to the definition of time-respecting paths with limited waiting time \(\), we obtain a _temporal-topological_ generalization of shortest paths to temporal graphs that accounts for the temporal ordering and timing of interactions. We note that other definitions of _fastest paths_ only account for temporal rather than topological distance , which we however do not consider in our work.

The definition of time-respecting paths above has the important consequence that the connectivity of nodes via time-respecting paths in a temporal network can be considerably different from paths in the corresponding time-aggregated static network. As an example, for a temporal network with two time-stamped edges \((u,v;t)\) and \((v,w;t^{})\) the time-aggregated network contains a path from \(u\) via \(v\) to \(w\), while a time-respecting path from \(u\) via \(v\) to \(w\) can only exist iff \(0<t^{}-t\). In other words, while connectivity in static graphs is _transitive_, i.e. the existence of edges (or paths) connecting \(u\) to \(v\) and \(v\) to \(w\) implies that there exists a path that transitively connects \(u\) to \(w\), the same does not hold for time-respecting paths. A large number of works have shown that this difference between paths in temporal and static graphs influences connectivity and reachability , the evolution of dynamical processes like diffusion or epidemic spreading , cluster patterns , as well as the controllability of dynamical processes .

Temporal CentralitiesAnother interesting question is how the time dimension of temporal graphs influences the importance or _centrality_ of nodes . To this end, several works have generalized

Figure 1: Overview of proposed approach to predict temporal node centralities in a temporal graph: We consider a time-based split in a training and test graph (left). Calculating time-respecting paths in the training split enables us to (1) compute temporal centralities, and (2) fit a \(k\)-th order De Bruijn graph model for time-respecting paths. The weighted edges in such a \(k\)-th order De Bruin graph capture frequencies of time-respecting paths of length \(k\) (see time-respecting path of length one (red) and two (magenta)). (3) We use these centralities and the k-th order models to train a De Bruijn graph neural network (DBGNN), which allows us to (4) predict temporal centralities in the test graph.

centrality measures originally defined for static graphs to temporal networks. For our purpose we limit ourselves to generalizations of betweenness and closeness centrality, which are defined based on the shortest paths between nodes. In a static network, a node \(v\) has high _betweenness centrality_ if there are many shortest paths that pass through \(v\) and it has high _closeness centrality_ if the overall distance to all other nodes is small . We omit those standard definitions here due to space constraints but include them in appendix H.

Analogously to betweenness centrality for static graphs, for a temporal graph \(G=(V,E^{T})\) we define the _temporal betweenness centrality_ of node \(v V\) as

\[c_{B}^{temp}(v)=_{s v t V}(v)}{_{s,t}}\]

where \(_{s,t}\) is the number of shortest _time-respecting_ paths from node \(s\) to \(t\).

To calculate _temporal closeness centrality_ we define the temporal distance \(d(u,v)\) between two nodes \(u,v V\) as the length of a shortest time-respecting paths from \(u\) to \(v\) and thus obtain

\[c_{C}^{temp}(v)=d(u,v)}.\]

Even though the definitions above closely follow those for static networks, it has been shown that the temporal centralities of nodes can differ considerably from their counterparts in static time-aggregated networks [27; 29]. These findings highlight the importance of a _time-aware_ network analysis, which consider both the timing and temporal ordering of links in temporal graphs.

Approximating Path-based CentralitiesWhile path-based centralities have become an important tool in network analysis, a major issue is the computational complexity of the underlying all-pairs shortest path calculation in large graphs. For static networks, this issue can be partially alleviated by smart algorithms that speed up the calculation of betweenness centralities . Even with these algorithms, calculating path-based centralities in large graphs is a challenge. Hence, a number of works considered approaches to calculate fast approximations, e.g. based on a random sampling of paths [42; 2; 21]. Another line of studies either used standard, i.e. not graph-based, machine learning techniques to leverage correlations between different centrality scores [18; 19], or used neural graph embeddings in synthetic scale-free networks to approximate the ranking of nodes .

Existing works on the approximation of path-based node centralities in time series data have generally focussed on a fast updating of _static_ centralities in _evolving graphs_ where edges are added or deleted [7; 43], rather than considering _temporal node centralities_. For the calculation of temporal closeness or betweenness centralities, the need to calculate shortest _time-respecting paths_ between all pairs of nodes is computationally challenging: Temporal closeness centrality minimally requires the traversal of all time-stamped edges for all nodes in the graph, which has a time complexity in \(O(n m)\) where \(n\) is the number of nodes and \(m\) is the number of time-stamped edges in the temporal graph. Building on Brandes' algorithm for static betweenness centrality , a fast algorithm for temporal betweenness centrality with complexity \(O(n m T)\) (where \(T\) is the number of different time stamps in the temporal graph) has recently been proposed in . Considering the approximate estimation of temporal betweenness and closeness centrality in temporal graphs,  generalizes static centralities to higher-order De Bruijn graphs, which capture the time-respecting path structure of a temporal graph.  recently proposed a sampling-based estimation of temporal betweenness centralities. To the best of our knowledge, no prior works have considered the application of deep graph learning to predict temporal node centralities in temporal graphs, which is the gap addressed by our work.

## 3 A Time-Aware GNN to Predict Temporal Centralities

Here, we first present higher-order De Bruijn graph models for time-respecting paths in temporal networks. We then describe our deep learning architecture to predict temporal centralities.

Higher-Order De Bruijn Graph Models of Time-respecting pathsEach time-respecting path gives rise to an ordered sequence \(v_{0},v_{1},,v_{l}\) of traversed nodes. Let us consider a \(k\)-th order Markov chain model, where \(P(v_{i}|v_{i-k},,v_{i-1})\) is the probability that a time-respecting path continues to node \(v_{i}\), conditional on the \(k\) previously traversed nodes. A first-order Markov chain model can be defined based on the frequencies of edges (i.e. paths of length \(k=1\)) captured in a weighted time-aggregated graph, where

\[P(v_{i}|v_{i-1}):=,v_{i})}{_{j}w(v_{i-1},v_{j})}.\]

While a first-order model is justified if the temporal graph exhibits no patterns in the temporal ordering of time-stamped edges, several works have shown that empirical data exhibit patterns that require higher-order Markov models for time-respecting paths . To address this, for \(k>1\) we define a \(k\)-th order Markov chain model based on frequencies of time-respecting paths of length \(k\) as

\[P(v_{i}|v_{i-k},,v_{i-1})=,,v_{i})}{_{j}w(v_{i- k},,v_{i-1},v_{j})},\]

where \(w(v_{0},,v_{k})\) counts the number of time-respecting path \(v_{0},,v_{k}\) in the underlying temporal graph. For a temporal graph \(G^{}=(V,E^{})\), this approach defines a static \(k\)_-th order De Bruijn graph model_\(G^{(k)}=(V^{(k)},E^{(k)})\) with

* \(V^{(k)}=\{(v_{0},,v_{k-1}) v_{0},,v_{k-1}k-1G^{}\}\)
* \((u,v) E^{(k)}\) iff \[(i)\;\;v=(v_{1},,v_{k})v_{i}=u_{i}i=1,,k-1\\ (ii)\;\;u v=(u_{0},,u_{k-1},v_{k})kG^{}.\]

We call this \(k\)-th order model a _De Bruijn graph model_ of time-respecting paths, since it is a generalization of a \(k\)-dimensional De Bruijn graph , with the additional constraint that an edge only exists iff the underlying temporal network has a corresponding time-respecting path. For \(k=1\) the first-order De Bruijn graph corresponds to the commonly used static, time-aggregated graph \(G=(V,E)\) of a temporal graph \(G^{T}\), where edge can be considered time-respecting paths of length one and which neglects information on time dimension. For \(k>1\) we obtain _static but time-aware higher-order generalizations of time-aggregated graphs_, which are sensitive to the timing and ordering of time-stamped edges. Each node in such a \(k\)-th order De Bruin graph represents a time-respecting path of length \(k-1\), while edges represent time-respecting paths of length \(k\). Edge weights correspond to the number of observations of time-respecting paths of length \(k\) (cf. fig. 1).

De Bruijn Graph Neural Networks for Temporal Centrality PredictionOur approach to predict temporal betweenness and closeness centrality uses the recently proposed De Bruijn Graph Neural Networks (DBGNN), a deep learning architecture that builds on \(k\)-th order De Bruijn graphs . The intuition behind this approach is that, by using message passing in multiple (static) \(k\)-th order De Bruijn graph models of time-respecting paths, we obtain a _time-aware learning algorithm_ that considers both the graph topology as well as the temporal ordering and timing of interactions.

Our proposed method is summarized in fig. 1. Considering time series data on a temporal graph, we first perform a time-based split of the data into a training and test graph. We then calculate temporal closeness and betweenness centralities of nodes in the training graph and consider a supervised node-level regression problem, i.e. we use temporal centralities of nodes in the training graph to train a DBGNN model. To this end, we construct \(k\)-th order De Bruijn graph models for multiple orders \(k\), based on the statistics of time-respecting paths of lengths \(k\). The maximum order is determined by the temporal correlation length (i.e. the Markov order) present in a temporal graph and can be determined by statistical model selection techniques .

Using the update rule defined in Eq. (1) of , we simultaneously perform message passing in all \(k\)-th order De Bruijn graphs. For each \(k\)-th order De Bruijn graph this yields a (hidden) representation of \(k\)-th order nodes. To aggregate the resulting representation to actual (first-order) nodes in the temporal graph, we perform message passing in an additional bipartite graph, where each \(k\)-th order node \((v_{0},,v_{k-1})\) is connected to first-order node \(v_{k-1}\) (cf. Eq (2) in  and fig. 1). Taking a node regression perspective, we use a final dense linear layer with a single output. We use the trained model to predict the temporal centralities of nodes in the test graph. Since the subset of nodes and edges that are active in the training and test graph can differ, our model must be able to generalize to temporal graphs with different nodes as well as different graph topologies. To address this, we train our models in an inductive fashion by choosing a suitably large number of dimensions for the one-hot encodings during the training phase.

Compared to , we introduce two significant technical advances: first, we adapt DBGNN for a node-level regression task, which, to our knowledge, has not been previously explored. Second, we implement a different training procedure designed for forecasting. Unlike the node classification task in , our approach involves training a model on a temporal graph within a training window and subsequently refitting this model to forecast temporal centralities in a future observation, which may include previously unseen nodes and edges. This approach enables our model to generalize to forecasting scenarios involving previously unobserved graph elements, potentially extending its utility to other temporal graph forecasting tasks.

The implementation of our method is based on the Open Source temporal graph learning library pathpyG1. The code of our experiments has been permanently archived at Zenodo2.

## 4 Experimental Results

With our experimental evaluation we seek to answer the following five research question:

**RQ1**: How does the predictive power of a time-aware DBGNN model compare to that of a standard GCN that only uses the static topology and ignores the time dimension of dynamic graphs?
**RQ2**: How does the performance of the DBGNN model compare to (i) a two step approach that combines the temporal graph embedding EVO  with a feed-forward neural network, and (ii) TGN , an end-to-end temporal GNN architecture that does, however, not explicitly consider time-respecting paths.
**RQ3**: How do the predictions of the DBGNN architecture compare to the results of ONBRA, a method that aims to approximate temporal betweenness centralities?
**RQ4**: Which speed-up does our prediction method offer compared to the calculation of temporal node centralities?
**RQ5**: Does the DBGNN architecture generate node embeddings that facilitate interpretability?

Experimental setupWe experimentally evaluate the performance of the DBGNN architecture by predicting temporal centralities in 13 empirical temporal graphs. We split each temporal graph in training and test graphs, where the training and test graph contain half of the data each. Since a maximum order detection in those data sets yields a maximum of two (see table 11 in appendix F), we limit the DBGNN architecture to \(k=2\). To calculate edge weights of the DBGNN model, we count time-stamped edges as well as time-respecting paths of length two for weights of the first and second-order De Bruijn graph, respectively (cf. fig. 1). Adopting the approach in  we use one message passing layer with 16 hidden dimensions for each order \(k\) and one bipartite message passing layer with 8 hidden dimensions. We use a sigmoid activation function for higher-order layers and an Exponential Linear Unit (ELU) activation function for the bipartite layer.

As a first time-neglecting baseline model, we use a Graph Convolutional Neural Network (GCN) , which we apply to the weighted time-aggregated representation of the temporal graphs. For the GCN model, we use two message passing layers with 16 and 8 hidden dimensions and a sigmoid activation function, respectively. As input features, we use a one-hot encoding (OHE) of nodes for both architectures. In the case of the DBGNN architecture we apply OHE to nodes in all (higher-order) layers. Addressing a node regression task, we use a final dense linear layer with a single output and an ELU activation function, and use mean squared error (MSE) as loss function for both architectures. We train both models based on (ground-truth) temporal centralities in the training data, using 1000 epochs with an ADAM optimizer, different learning rates, and weight decay of \(5 10^{-4}\). We tested the use of dropout layers for both architectures, but found the results to be worse. In table 16 and table 17 in the appendix we summarize the architecture and report all hyperparameters for both models.

As a second baseline method we use the time-aware graph embedding EVO , which models correlations in the sequence of nodes traversed by time-respecting paths. Similar to DBGNN, EVO uses these correlations to produce a single static embedding of nodes that captures both the topology and temporal patterns in the dynamic graph. Different from DBGNN, EVO does not yield an end-to-end centrality prediction approach, i.e. it only produces node embeddings that can then be used for downstream learning tasks. To address centrality prediction, we use 16-dimensional node embeddings produced by EVO for time-respecting paths up to length two. We then train a two-layer feed-forwardnetwork with a 16-dimensional input, a hidden layer with eight dimensions, and a single output. We use a Rectified Linear Unit (ReLU) activation function for both the hidden layer and the output layer.

As a third baseline, we use the Temporal Graph Networks (TGN) framework, a recently proposed graph neural network architecture for temporal graphs . Different from DBGNN and EVO the TGN framework does not explicitly consider time-respecting paths but produces a time evolving node embedding that accounts for the sequentiality of interactions and node-wise events. To this end, TGN splits a temporal graph into multiple equally-sized batches of consecutive time-stamped interactions. In each of these batches a message passing algorithm is used to update node representations based on time-stamped edges in the current batch as well as a memory of node representations and messages in previous batches. Within each batch the learnable parameters of a TGN model can be trained using a variety of graph learning tasks such as, e.g., link prediction and node classification.

On the one hand we want batches to be small to obtain a sufficient numbers of batches that we can use to train the model on a given dataset. On the other hand small batch sizes introduce the problem that, due to the small number of time-respecting paths, we cannot calculate meaningful centralities. To address this issue, we calculate the temporal centralities of nodes in a given batch \(i\) based on a temporal graph obtained by the batches \(i-k+1\) to \(i\) for some \(k\). Adopting this approach we train the TGN model based on the training splits of our data. We then use the trained model to perform a per-batch prediction of temporal centralities for all batches in the test split of our data. For TGN, we chose the training and test set such that both contain the same number of batches. Finally, we average the prediction scores of the test batches to evaluate the performance of the model.

In addition to the deep learning methods above, as a final baseline we include ONBRA, a recently proposed sampling-based method, which can estimate temporal betweenness centralities with varying degrees of fidelity by sampling pairs of nodes and calculating shortest temporal paths between them. By choosing a suitable number of samples, we experimentally adjusted the estimation fidelity of ONBRA, such that the estimation algorithm took approximately the same time as our model. In particular, using the publicly available implementation of the authors3, we estimate temporal betweenness for shortest \(\)-restless walks for ten iterations and adjust the number of sampled node pairs. In order to get a meaningful comparison, we only run ONBRA on the test window, on which we predicted the temporal betweenness centralities with the other models.

Data setsWe use 13 data sets on temporal graphs from different contexts, including human contact patterns based on (undirected) proximity or face-to-face relations, time-stamped (directed) E-Mail communication networks, as well as antenna interactions between ants in a colony. An overview of the data sets along with a short description, key characteristics and the source is given in table 4 in the appendix. All data are publicly available from netzschleuder  and SNAP .

Evaluation procedureTo evaluate our models, we first fit the pre-trained models to the test graph, i.e. we apply the trained models to the test graph and the trained DBGNN model to the De Bruijn graphs for the test data. We then use the trained models to predict temporal closeness and betweenness centralities and compare those predictions to ground truth centralities. For the calculation of temporal closeness centrality, we calculate shortest time-respecting path distances for a given maximum time difference \(\) between all pairs of nodes using a variation of Dijkstra's algorithm that traverses all time-stamped edges. For temporal betweenness centrality, we adopt a variation of the algorithm proposed in , which we adjusted to account for shortest time-respecting paths with a maximum time difference \(\). We will make our code of the temporal centrality calculation available upon acceptance of the manuscript. Figure 1 provides an illustration of our evaluation approach. We use Kendall-Tau and Spearman rank correlation to compare a node ranking based on predicted centralities with a ranking obtained from ground truth centralities. Since both rank correlation measures yielded qualitatively similar results, we only report the Spearman correlation. Since centrality scores are often used to identify a small set of most central nodes, we further calculate the number of hits in the set of nodes with the top ten predicted centralities. Since we repeated each experiment \(20\) times, we report the mean and the standard deviation of all scores. We repeated all experiments for different learning rates between \(0.1\) and \(0.0001\) and report the best mean scores. The associated learning rates as well as all other hyperparameters of the models are reported in the appendix.

Discussion of resultsThe results of our experiments for temporal betweenness and closeness centralities are shown in table 1 and table 2, respectively. Considering **RQ1**, we find that our time 

[MISSING_PAGE_FAIL:8]

based on time-respecting paths, as opposed to the two-step approach where we use EVO embeddings as input to a subsequent neural network.

We further find that DBGNN outperforms TGN in all tested cases except for temporal betweenness in the ants-2-1 data set. We attribute this to the fact that DBGNN explicitly models patterns in the sequence of nodes traversed by time-respecting paths, which are the basis for the definition of betweenness and closeness centrality. The worse performance of TGN can be explained by the fact that the TGN architecture does not use time-respecting paths for the message passing algorithm. This makes it - despite being a time-aware technique that accounts for the temporal evolution of graphs - a bad choice for temporal graph learning tasks that depend on time-respecting paths.

Considering **RQ3**, for the ONBRA method to approximate temporal betweenness centrality, we find that (i) our method provides a considerably higher performance in terms of Spearman rank correlation for large data sets, and (ii) generally lower mean absolute error across all data sets. Moreover, ONBRA failed to return results for three data sets where our method shows high performance. In table 7 in appendix C we report the Spearman rank correlation of the results across all data sets, as well as the MAE scores and the time the model took to calculate the estimated centralities. We chose the samples for the ONBRA algorithm such that the time required for the estimation of the centralities approximately matches the inference time for the DBGNN model.

Addressing **RQ4**, a potential advantage of our method is that it facilitates _predictions_ of temporal centrality node rankings that are much faster than the actual _calculation_ of temporal centralities. Highlighting this, in table 3 we report the time needed (i) to fit our pretrained model to the test data, and (ii) to infer the temporal centrality prediction. While our approach requires to fit a \(k\)-th order De Bruijn graph model in the test data, this procedure only requires to calculate time-respecting paths of exactly length \(k\), which is a simpler problem than the calculation of all shortest time-respecting paths. We compare the combined time of those two steps to the time required to calculate temporal closeness and betweenness centrality in the test graphs, for which we used the fastest known algorithms mentioned in section 2. The results show that our approach provides speed-up factors ranging from approx. 3.5 to 43.7 for temporal closeness and from approx. 6.4 to 1077 for temporal betweenness.

The corresponding speed-up tables for GCN, TGN and EVO can be found in appendix D. Being a much simpler model, the static GCN model provide a higher speed-up (but considerably worse performance). Similarly, EVO provides higher speed-ups but worse predictions. We finally note that the temporal TGN model yields lower speed-ups than our method, despite giving worse predictions.

A potential criticism of our method could be that the size of a higher-order De Bruijn graph model can be considerably larger than a first-order graph, possibly making training and inference computationally expensive. To address this concern, in appendix B we report both the training and inference times of all models across all 13 data sets. We find that both the training and inference times of the DBGNN architecture are actually comparable to those of a static GCN. We attribute this to the fact that the DBGNN architecture provides a compact, _static but time-aware_ De Bruijn graph representation of potentially large time series, rather than requiring a representation of all time-stamped edges. We further find that DBGNN has considerably lower training costs than both TGN or EVO.

Considering **RQ5**, another aspect of our approach to use a time-aware but _static_ graph neural network is that the hidden layer activations yield _static_ embeddings that are based on the _causal topology_ of

   Experiment &  &  \\  & Fitting+Inference & Centrality & Speed-Up & Fitting+Inference & Centrality & Speed-Up \\  ants-1-1 & & 0.019 & 0.068 & 3.478 & 0.019 & 0.288 & 14.865 \\ ants-1-2 & & 0.012 & 0.056 & 4.510 & 0.012 & 0.107 & 8.721 \\ ants-2-1 & & 0.007 & 0.029 & 4.127 & 0.007 & 0.045 & 6.376 \\ ants-2-2 & & 0.010 & 0.055 & 5.352 & 0.010 & 0.116 & 11.137 \\ e-email-dep4 & & 0.066 & 1.201 & 18.175 & 0.066 & 1.476 & 22.369 \\ e-email-dep2 & & 0.080 & 1.500 & 18.720 & 0.080 & 1.883 & 23.502 \\ e-email-dep3 & & 0.017 & 0.191 & 11.420 & 0.018 & 0.309 & 17.274 \\ sp-workplace & & 0.058 & 1.577 & 27.299 & 0.058 & 4.961 & 86.154 \\ sp-hypertext & & 0.207 & 5.908 & 28.570 & 0.207 & 100.517 & 485.728 \\ sp-hospital & & 0.465 & 15.289 & 32.908 & 0.464 & 125.977 & 271.461 \\ hagele & & 0.069 & 0.516 & 7.431 & 0.069 & 1.032 & 14.910 \\ manufacturing-email & & 0.339 & 4.843 & 14.267 & 0.339 & 5.762 & 16.982 \\ sp-highschool-2013 & & 2.086 & 91.147 & 43.701 & 2.085 & 2247.218 & 1077.554 \\   

Table 3: Speed-up of the time required for fitting our pretrained model and inference of temporal closeness and betweenness centrality compared to the time required to calculate temporal closeness and betweenness centrality in the validation set.

temporal graphs. This causal topology is influenced by (i) the topology of links, and (ii) their timing and temporal ordering. To explain the favorable performance of our model compared to a static GCN, we hypothesize that nodes for which our model learns similar embeddings also have more similar temporal centralities, compared to embeddings generated by a GCN model. To test this, we apply a dimensionality reduction to the node activations generated by the last 8-dimensional bipartite layer in the DBGNN architecture, comparing it to the representation obtained from (i) the last message passing layer of a GCN model and (ii) an EVO embedding. In fig. 4 in appendix I we show the resulting embeddings for one representative prediction of temporal closeness and betweeness in the eu-email-dept4 data, where the color gradient highlights ground truth node centralities in the test data. The plot shows that the time-aware DBGNN architecture better captures the ranking of nodes compared to a time-neglecting GCN as well as the EVO embeddings. We again attribute this to the end-to-end learning approach provided by DBGNN compared to the two-step approach of EVO.

## 5 Conclusion

In summary, we investigate the problem of predicting temporal betweenness and closeness centralities in temporal graphs. We use a recently proposed time-aware graph neural network architecture, which relies on higher-order De Bruijn graph models of time-respecting paths. An empirical study in which we compare our approach with a time-neglecting static graph neural network demonstrates the potential of our method. We find that our approach considerably outperforms other time-aware graph learning techniques that (i) either do not consider time-respecting paths, or (ii) do not provide an end-to-end approach where the learning of node representations is integrated with the prediction task. A comparative analysis in 13 empirical temporal graphs highlights differences between static and temporal centralities that are likely due to the underlying temporal patterns, and shows that our model is generally better at predicting temporal closeness compared to betweenness. A scalability analysis reveals that our prediction approach provides a considerable speed-up compared to the exact calculation of temporal node centralities, yielding speed-up factors between 3.5 and 1077. We finally investigate (static) embeddings produced by the last message passing layer of our architecture and show that they better capture temporal centralities compared to GCN.

Open questions and future workOur work necessarily leaves open questions that should be addressed in future work. Rather than optimizing the predictive performance of our model, the focus of the present work was to highlight the potential of time-aware graph neural networks for temporal centrality prediction. We thus have not performed an exhaustive optimization of hyperparameters such as, e.g., the maximum time difference \(\), the maximum order \(k\) of the De Bruijn graphs used in the DBGNN architecture, or the number and width of graph convolutional layers. While we do report optimal values across three learning rates for all models, a more thorough investigation of the influence of those hyperparameters is future work. Moreover, we did not utilize additional node features like, e.g., node degrees, static centralities, or node embeddings that could further improve our results. Another aspect that we have not studied in our work is the impact of the size of the training data, i.e. how little training data is sufficient to predict temporal centralities with reasonable accuracy, and where the trade-offs in the choice of the training size are. An interesting further question is whether our approach could be adapted to support a fully _inductive_ setting, i.e. to train our model on a set of dynamic graphs and then use the trained model to predict temporal centralities in other, previously unseen networks. Similarly, for some data sets with non-stationary temporal patterns it could be beneficial to train a De Bruijn Graph Neural Network based on a sliding window approach, hence adjusting the model (and predictions) as time progresses. Such a combination of the concepts behind TGN and DBGNN could yield better results in a number of practical settings.

We believe that our work is of high practical relevance for applications of knowledge discovery and machine learning in time-stamped relational data. For dynamic social network analysis, our approach allows to quickly estimate temporal centralities whose calculation is computationally expensive. More generally, our study highlights the potential of _compact, static but time-aware graph neural network architectures_ for node-level regression in temporal graphs.