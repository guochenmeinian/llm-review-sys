# CountGD: Multi-Modal Open-World Counting

Niki Amini-Naieni Tengda Han Andrew Zisserman

Visual Geometry Group (VGG)

University of Oxford

{nikian,htd,az}@robots.ox.ac.uk

###### Abstract

The goal of this paper is to improve the generality and accuracy of open-vocabulary object counting in images. To improve the generality, we repurpose an open-vocabulary detection foundation model (GroundingDINO) for the counting task, and also extend its capabilities by introducing modules to enable specifying the target object to count by visual exemplars. In turn, these new capabilities - being able to specify the target object by multi-modalities (text and exemplars) - lead to an improvement in counting accuracy. We make three contributions: _first_, we introduce the first open-world counting model, CountGD, where the prompt can be specified by a text description or visual exemplars or both; _second_, we show that the performance of the model significantly improves the state of the art on multiple counting benchmarks - when using text only, CountGD is comparable to or outperforms all previous text-only works, and when using both text and visual exemplars, we outperform all previous models; _third_, we carry out a preliminary study into different interactions between the text and visual exemplar prompts, including the cases where they reinforce each other and where one restricts the other. The code and an app to test the model are available at [https://www.robots.ox.ac.uk/vgg/research/countgd/](https://www.robots.ox.ac.uk/vgg/research/countgd/).

## 1 Introduction

Open-world object counting methods aim to enumerate all the instances of any category of object in an image. The 'open-world' refers to the model's ability to count objects beyond the set of categories seen at training, thus enabling the user to specify categories of interest at inference without the need for model retraining. Recent techniques allow the user to specify the target object with only visual exemplars - bounding boxes around a few example objects in the image - , or only text

Figure 1: CountGD is capable of taking _both_ visual exemplars and text prompts to produce highly accurate object counts (**a**), but also seamlessly supports counting with only text queries or only visual exemplars (**b**). The multi-modal visual exemplar and text queries bring extra flexibility to the open-world counting task, such as using a short phrase (**c**), or adding additional constraints (the words ‘left’ or ‘right’) to select a sub-set of the objects (**d**). These examples are taken from the FSC-147  and CountBench  test sets. The visual exemplars are shown as yellow boxes. (d) visualizes the predicted confidence map of the model, where a high color intensity indicates a high level of confidence.

descriptions . By accepting either visual exemplars or text as _prompts_, open-world object counting methods can adapt to the specific object at inference time. This enables these techniques to count arbitrary classes of objects as specified by the user.

Methods that use visual exemplars to specify the object currently significantly outperform text-based counting methods on multiple benchmarks. This is because visual exemplars provide more detailed information than text - it can take many words to precisely describe an object; and perhaps more importantly, they provide _intrinsic_ information on the object's appearance - because the exemplars are from the same image they already 'factor in' the viewpoint and lighting, variables that significantly affect the object's appearance. However, while visual exemplar-based approaches are more accurate, they limit the capabilities and generality of the counting model.

In this paper we introduce a counting model that is able to specify the target object using visual exemplars, a text description, or both together. The model, named CountGD, has superior accuracy to previous methods, but is also more general. In addition to the performance boost obtained by specifying the target object using both visual exemplars and text, the _interaction_ of the exemplars and text can be used to select a sub-set of those objects in the image. These capabilities are illustrated in Figure 1. This flexible combination of visual exemplars and text description thus provides the model with more capabilities and information than prior approaches.

To achieve this multi-modal prompt capability, we follow prior work on open-world text-specified object counting , and build on and extend a pre-trained vision-language foundation model, GroundingDINO . We introduce new modules to embed the visual exemplars, and to enable the model to count, rather than detect. Within the model we cast the additional visual exemplars as text tokens, and the model first learns to fuse the visual exemplars with text tokens through self-attention, and then interacts with the image through cross-attention. Because the text tokens are naturally variable in length, the number of provided visual exemplars are as well. As a result, the model allows users to specify the object to count with text only, visual exemplars only, or text and any number of visual exemplars.

In summary, we make the following three contributions: _First_, we introduce CountGD, the first open-world object counting model that accepts either text or visual exemplars or both simultaneously, in a single-stage architecture; _Second_, we evaluate the model on multiple standard counting benchmarks, including FSC-147 , CARPK  and CountBench , and show that CountGD significantly improves on the state-of-the-art performance by specifying the target object using both exemplars and text. It also meets or improves on the state-of-the-art for text-only approaches when trained and evaluated using text-only; _Third_, we investigate how the text can be used to refine the visual information provided by the exemplar, for example by filtering on color or relative position in the image, to specify a sub-set of the objects to count. In addition we make two minor improvements to the inference stage: one that addresses the problem of double counting due to self-similarity, and the other to handle the problem of a very high count.

## 2 Related Work

Prior work on object counting has developed along three axes: (1) the density map versus detection axis, (2) the class-specific versus open-world (also referred to as "class-agnostic") axis, and (3) the visual exemplar versus text axis. The pattern is that detection, open-world, and text-based methods tend to offer more capabilities and be more general than their analogues along each axis. On the other hand, density map, class-specific, and visual exemplar-based methods tend to be more accurate at the counting tasks they apply to. CountGD integrates the third axis - the visual exemplar versus text axis - to achieve more general and accurate counting overall. Below, we discuss where prior work falls along each axis and where CountGD stands.

**Density Map versus Detection-based Object Counting (_Axis 1)_. In the past, counting techniques that regress and sum density maps , instead of detecting and enumerating bounding boxes , have proven more accurate in cluttered and dense scenes. For example, density map-based approaches like CountTX , LOCA , and CountTR  achieve lower counting errors than detection-based approaches such as Mask-RCNN  and RetinaNet  on standard counting benchmarks. Concurrent to our work, DAVE , integrates density map regression with object detection to construct a more accurate and explainable two-stage counting system. Like DAVE, CountGD outputs explicit object locations. However, CountGD is a single-stage approach that achieves better counting accuracy than DAVE and other density map-based techniques. Therefore, while density map-based approaches tend to be more accurate than detectors in highly populatedscenes, recent detection-based techniques, including CountGD, are beginning to achieve better accuracy than density map-based alternatives.

Class-specific versus Open-world Object Counting _(Axis 2)._Object counting methods first developed as class-specific techniques , solving the counting problem for only one category of object, but recent methods have generalized these approaches to open-world settings, where counting _arbitrary_ objects is possible. Class-specific methods have been developed to count cars , humans , and cells . In contrast, open-world methods can count instances from all three categories . Because class-specific techniques are more specialized than open-world approaches, they tend to be more accurate at counting instances from the class they were designed for. Recent advancements in Vision-Language Foundation Models (VLMs) such as CLIP  and GroundingDINO  trained on web-scale image-text pairs produce semantically rich visual and textual features. These features generalize to a wide range of open-world downstream tasks. Building on top of these pre-trained VLMs, recent open-world methods  have begun to surpass class-specific approaches in counting accuracy. CountGD, like these recent approaches, is an open-world object counter that achieves competitive performance in comparison to class-specific alternatives.

Counting with Visual Exemplars versus Counting with Text _(Axis 3)._Most open-world object counters approach the problem by using visual exemplars to select the objects in the input image , but very recent work  has attempted to replace the visual exemplars with text, enabling new capabilities at the cost of reduced accuracy. The state-of-the-art text-based approaches, such as GroundingREC , CountTX , CLIP-Count , and VLCounter  are built on top of vision-language foundation models pretrained on large quantities of data to relate images to textual inputs and map them to a joint embedding space. This allows these foundation models to understand general concepts learned during extensive pretraining and provides a mechanism for users to specify extrinsic object properties through text. However, text-based approaches perform significantly worse than state-of-the-art visual exemplar-based approaches such as LOCA , CountTR , and few-shot DAVE . For example, while both GroundingREC and CountGD use the pretrained GroundingDINO  vision-language foundation model, unlike GroundingREC, CountGD allows the user to input both visual exemplars and text instead of just text. This enables CountGD to achieve superior counting accuracy in comparison to GroundingREC. Notably, DAVE  is a visual exemplar-based approach that also enables textual prompts, but differs from CountGD in three important ways: (1) it does not address the case when both text and visual exemplars are available while CountGD does, (2) its comparison between text features and image features is not learned as it is by CountGD with attention, and (3) it is a two-stage approach, while CountGD solves the problem in a single stage, without relying on another visual exemplar-based counting model. Very recently, A Blind Counter (ABC) that does not require text or visual exemplars was introduced in . ABC discovers different objects to count and provides exemplars indicating what has been counted. While this approach is more efficient, it does not provide the user with precise control over the object to count, as exemplar and text-based methods do.

Relation of Counting to other areas.Our work is related to few-shot image classification  and image detection  methods. These works require a few query images of novel objects, and then compare the test image with these image examples to determine its semantic content (for image classification), or to spatially localize instances (for object detection). Like these methods, CountGD enables us to specify the object to count with visual exemplars (i.e., "query images") but also allows for textual inputs, and then compares the test image with the multi-modal specifications to get the final count. Furthermore, we focus on the counting problem, a challenging task for object detectors.

## 3 Counting with Visual Exemplars & Text

Here, we describe CountGD, a single-stage model for open-world object counting that accepts either visual exemplars or text or both together as prompts to specify the object to count.

### Overview

Given a target object specified by either visual exemplars as bounding boxes \(=\{b_{1},,b_{N}\}\) around example object instances in the image, or a textual description, \(t\), or both, \(\{,t\}\), the counting model, \(f\), counts the number of occurrences of the object in an image \(^{H W 3}\), as \(=f(,,t)\), where \(\) is the object count estimated by the counting model \(f\).

The architecture of the model is illustrated in Figure 2. CountGD is built on top of the open-world object detector GroundingDINO  to benefit from its pretrained open-vocabulary grounding and detection capabilities. In contrast to GroundingDINO, which only uses text queries for object detection, CountGD also includes visual exemplars as inputs, which increases the performance and flexibility of the model for object counting. In the following, we first describe the modules of the CountGD architecture, and then discuss its relation to GroundingDINO and in particular what is frozen, what is trained, and what is added to GroundingDINO.

### CountGD Architecture Components

Image Encoder (\(f_{_{}}\)).The image encoder \(f_{_{}}\) encodes two types of inputs: the input image \(X\) and the visual exemplars \(\). The image encoder itself is the Swin-B version of the Swin Transformer . As shown in Figure 3 (a), for the input image \(X\), it produces spatial feature maps at three different scales. These spatial feature maps are projected to 256 dimensions with 1x1 convolutions to produce the image tokens, feature vectors of length \(256\) corresponding to the image patches at different scales, which are input to the feature enhancer, \(f_{}\). As shown in Figure 3 (b), for the visual exemplars \(\), we reuse the spatial feature map \(f_{_{}}()\) for the input image \(X\), and apply aligned region-of-interest pooling, RoIAlign , with the pixel coordinates specified by the visual exemplars \(\). The resulting visual exemplar tokens are 256-dimensional feature vectors like the image and text tokens.

Text Encoder (\(f_{_{}}\)).For the text encoder, \(f_{_{}}\), we use the BERT-base  text transformer pretrained on detection and phrase grounding data with the image encoder, \(f_{_{}}\). The text encoder

Figure 3: The visual feature extraction pipeline for images and visual exemplars. **(a)** For the input image, a standard Swin Transformer model is used to extract visual feature maps at multiple spatial resolutions. **(b)** For the visual exemplars with their corresponding bounding boxes, we first up-scale the multiple visual feature maps of the input image to the same resolution, then concatenate these feature maps, and project them to 256 channels with a \(1 1\) convolution. Finally, we apply a RoIAlign with the bounding box coordinates to get the visual features for the exemplars.

Figure 2: The CountGD architecture. At inference the object to be counted can be specified by visual exemplars or text prompts or both. The input image is passed through the image encoder, \(f_{_{}}\) to obtain spatial feature maps at different scales. The visual exemplar tokens are cropped out of this feature map using RoIAlign (as shown in Figure 3). The text is passed through the text encoder, \(f_{_{}}\) to obtain text tokens. In the feature enhancer, \(f_{}\), the visual exemplar tokens and text tokens are fused together with self-attention and cross-attend to the image features, producing the fused visual exemplar and text features, \(}\), and new image features, \(}\). The \(k\) image features \(}\) that have the highest cosine similarity with the fused features \(}\) are passed to the cross-modality decoder, \(f_{}\), as “cross-modality queries”. Finally, the similarity matrix, \(}\) between the outputs of the cross-modality decoder, \(f_{}\), and \(}\) is calculated, and outputs that achieve a maximum similarity with the \(}\) above a confidence threshold \(\) are identified as final detections and enumerated to estimate the final count. Our model is built on top of GroundingDINO  architecture with the additional modules indicated by blue shading.

maps the input object description \(t\) to a sequence of at most 256 tokens. The encoded text tokens are 256-dimensional feature vectors. While the image encoder \(f_{_{}}\) produces \(n\) image patch features when there are \(n\) multi-scale patches extracted from the input image, and the visual exemplar encoder produces \(p\) visual exemplar features when \(p\) visual exemplars are available, the text encoder produces \(q\) text features when there are \(q\) tokens, as determined by the BERT tokenizer, in the text \(t\). The \(n\) image tokens, \(p\) visual exemplar tokens, and \(q\) text tokens are then passed to the feature enhancer \(f_{}\), which fuses the three sources of information with attention.

Feature Enhancer (\(f_{}\)).The feature enhancer, \(f_{}\), is composed of 6 blocks that first fuse the visual exemplar tokens with the text tokens through self-attention and then fuse the combined features with the image patch tokens with cross-attention. More specifically, each block consists of self-attention between the concatenated visual exemplar and text tokens, deformable self-attention between the image patch tokens, and image-to-text cross-attention and text-to-image cross-attention between the fused visual exemplar and text tokens and the image patch tokens. These modules enable CountGD to learn to relate information from the input image, visual exemplars and text query altogether. The feature enhancer \(f_{}\) outputs two sets of features denoted as \(}\) and \(}\) as

\[(},})=f_{}(f_{_{}}(),(f_{_{}}( ),),f_{_{}}(t)) \]

corresponding to the fused visual exemplar and text tokens, and the image patch tokens, respectively.

Language & Visual Exemplar-guided Query Selection (\(Select\)).We select the \(k\) image patch tokens \(}\) that achieve the highest similarity with the fused visual exemplar and text tokens \(}\). This operation is denoted by \((},}}^{T},k)\), where \(}}^{T}^{n(p+q)}\) represents the similarity scores between the \(n\) image patch tokens and the \(p+q\) visual exemplar and text tokens. As in GroundingDINO , we set \(k\) to 900. These 900 image patch tokens with higher similarity scores serve as "cross-modality queries" input to the cross-modality decoder \(f_{}\).

Cross-modality Decoder (\(f_{}\)).The cross-modality decoder, \(f_{}\), uses self-attention to enhance the cross-modality queries, image cross-attention to fuse the image patch features \(}\) to the cross-modality queries, and cross-attention to fuse the visual exemplar and text features \(}\) to the cross-modality queries. In more detail, the cross-modality decoder consists of 6 of these self-attention and cross-attention blocks. The cross-modality queries are dot-produced with the combined visual exemplar and text tokens \(}\) and passed through an element-wise Sigmoid function to obtain the final confidence scores as:

\[}=(f_{}(},},(},}}^{T},k) )}^{T}) \]

where \(}\) are the fused visual exemplar and text features, \(}\) are the image features, \(k\) is the number of queries (i.e., maximum number of detected objects), and \(}\) are the final similarity scores that are thresholded according to a confidence threshold \(\) and enumerated to estimate the final object count \(\) at inference.

Design choices and relation to GroundingDINO.We choose GroundingDINO  over other VLMs due to its pretraining on visual grounding data, providing it with more fine-grained features in comparison to other VLMs such as CLIP .

To extend GroundingDINO to accept visual exemplars, we cast them as text tokens. Because both the visual exemplars and the text specify the object, we posit that the visual exemplars can be treated in the same way as the text tokens by GroundingDINO and integrate them into the training and inference procedures as such. In treating the visual exemplars as additional text tokens within a phrase, we add self-attention between the phrase corresponding to the visual exemplar and the visual exemplar rather than keeping them separate. This allows CountGD to learn to fuse the visual exemplar and text tokens to form a more informative specification of the object to count. Similarly, cross-attention between the image and text features in GroundingDINO's feature enhancer and cross-modality decoder becomes cross-attention between the image and the fused visual exemplar and text features in CountGD. Language-guided query selection in GroundingDINO becomes language and visual exemplar-guided query selection in CountGD. In this way, CountGD naturally extends GroundingDINO to input both text and visual exemplars to describe the object.

In GroundingDINO, the image encoder \(f_{_{}}\) is pre-trained on abundant detection and phrase grounding data with the text encoder, \(f_{_{}}\), providing it with rich region and text-aware features. Since we wish to build on this pre-trained joint vision-language embedding, we keep the image encoder \(f_{_{}}\) and the text encoder \(f_{_{}}\) frozen.

### Training

We train the projection layers for extracting the visual exemplar tokens, the feature enhancer, and the cross-modality decoder of CountGD. The trainable parameters are updated according to a loss \(\), while the rest of the parameters remain unchanged. This means CountGD effectively leverages the large-scale pre-training of the foundation model it extends.

The training loss \(\) includes a localization term \(_{loc}\) and a classification term \(_{cls}\). For the localization term \(_{loc}\), we regress the object centers from the final cross-modality queries output by the decoder \(f_{}\), and use the \(L_{1}\) loss between the predicted box center \(\) and the ground truth \(c\), similar to . For the classification term \(_{cls}\), we compute the similarity matrix \(}\) from Equation 2 and calculate the focal loss for each score. The final loss is:

\[=_{loc}_{loc}+_{cls}_{cls}= _{loc}_{i=1}^{l}|_{i}-c_{i}|+_{cls}( },T) \]

where \(_{loc}\) and \(_{cls}\) are hyperparameters optimized using a grid search on the validation set and \(T\{0,1\}^{k(l+1)}\) represents an optimal Hungarian matching between the \(k\) predicted queries and the \(l\) ground truth object instances, and the label "no object." Refer to the finetuning strategy implemented in  for further details.

### Inference

To predict the object count with CountGD, the image \(X\), text \(t\), and visual exemplars \(\) are inputted to the model, outputting a similarity matrix \(}^{k(p+q)}\). The maximum score over all \(p+q\) visual exemplar and text tokens is extracted for each of the \(k\) queries. Maximum scores above a confidence threshold \(\) are enumerated to estimate the object count.

## 4 Experiments

CountGD is trained on the FSC-147  object counting dataset training set, and then evaluated on the FSC-147 test set, and two other benchmark datasets (without any fine-tuning). We first describe the datasets, and then discuss the performance.

### Datasets & Metrics

Fsc-147 .FSC-147 contains 6135 images with 89 classes in the training set, 29 classes in the validation set, and 29 classes in the test set. The classes in the training, validation, and test sets do not overlap. Each image is annotated with at least three visual exemplars. For text descriptions, we use the singular forms of the class names in FSC-147-D  with any prefixes such as "the" removed. For example, we change "the donuts in the donut they" in FSC-147-D to "donut" by removing the prefix "the," extracting the class name "donuts," and then singularizing it to "donut."

_Corrections to FSC-147._ We make two corrections to FSC-147 and report results with and without these corrections. (1) As noted in , image 7171.jpg has incorrect visual exemplars labeled. Since, unlike the model in , CountGD can input either visual exemplars or text, for this example we only provide the model with text. (2) Image 7611.jpg has the incorrect text description "lego" even though the lego _studs_ not the lego _bricks_ should be counted. We change the description to "yellow lego stud" for this example.

Carpk .Carpk contains images of parking lots captured by overhead drones with a training set and test set of 989 and 459 images respectively. Each image is annotated with at least two bounding boxes. We use the same two bounding boxes selected in  as the visual exemplars for each image. We use the class name "car" as the text description.

CountBench .CountBench contains 540 images with 2-10 objects and captions describing the image as well as the number of objects to count. We create text descriptions for a 504-image subset of CountBench, removing inappropriate images and images with links that are unavailable. We give details of how the class names are obtained from the captions accompanying each image in the Appendix.

Metrics.Following prior work on object counting , the Mean Absolute Error (MAE) and the Root Mean Squared Error (RMSE) are used to measure performance. We define these metrics in the Appendix.

### Implementation

Training.The model is trained for 30 epochs on the FSC-147 training dataset using Adam optimizer and standard augmentations. The image and text encoders, \(f_{_{}}\) and \(f_{_{}}\), are frozen during training. Full details are given in the Appendix.

Inference.At inference, each image is resized such that its shortest side length is 800 pixels, and its aspect ratio is maintained. The image is then normalized and passed to the model. The visual exemplars are passed in as bounding boxes, and the special token "." is appended to the text description before providing it to the model. In the Appendix we give details of two important improvements: one to avoid double counting given self-similarity of the target object (like a butterfly ), and the other using adaptive cropping to overcome the 900 counting quota of the the model.

### Comparison to State-of-the-art on Standard Benchmarks

Here we show that CountGD achieves comparable or exceeds state-of-the-art performance for text-only open-world object counting when using only text, and exceeds the performance of all open-world object counting methods when using both visual exemplars and text on three benchmarks.

Fsc-147 .In Table 1, we test CountGD under two settings: (1) trained and tested with only text (denoted as CountGDtxt), and (2) trained and tested with both 3 visual exemplars and text (denoted as CountGD). CountGD trained and tested with both visual exemplars and text sets a new state-of-the-art for counting accuracy on FSC-147, achieving significantly lower counting errors than all prior approaches to open-world object counting. Training with only text achieves comparable counting accuracy to state-of-the-art text-only open-world object counting methods. The concurrent method GroundingREC  achieves slightly lower mean absolute error values than CountGDtxt, while CountGDtxt achieves lower root mean squared error values. The results for GroundingREC and CountGDtxt are likely close to each other since both methods leverage the pretrained GroundingDINO  foundation model. However, unlike GroundingREC, CountGD can fuse information from both text and visual exemplars instead of using only text, enabling a significant improvement. Similarly, while a pre-trained GroundingDINO performs poorly at counting (top row of Table 1), a GroundingDINO model fine-tuned on FSC-147 achieves good results , that match the performance of CountGDtxt. Adding visual exemplars to CountGD significantly improves its performance over fine-tuned GroundingDINO (Table 1, lowest row shows a test MAE of

Figure 4: Qualitative counting results on FSC-147  and CountBench  using the multi-modal CountGD. The model is trained and tested on FSC-147 visual exemplars and text. Input text is written above each image, and visual exemplars are indicated by the red boxes. On CountBench, we test the same model trained on the FSC-147 in a zero-shot way with only text (there are no visual exemplars for CountBench). Blue words indicate the subject of each caption input to the model. In both cases, CountGD predicts the count in all images shown with 100% accuracy. Note on the CountBench examples, the model counts the specified objects correctly when there are multiple types of objects in the image, such as the tomatoes with cucumbers, and the girls with bubbles. Detected points are filtered with a Gaussian and plotted under the input images for visualization purposes.

5.74 and a test RMSE of 24.09 for CountGDcompared to the test MAE of 10.82 and test RMSE of 104 noted in  for fine-tuned GroundingDINO). Unlike CountGD, GroundingDINO does not allow for visual exemplars as additional inputs.

In Figure 4, we give qualitative examples of the detections that CountGD outputs given both visual exemplars and text from the FSC-147 test set. Note how in the first image, CountGD only counts the strawberries and not the white cookies. Prior work has shown that visual exemplar-only methods struggle to count only one category of object when there are repeating instances from multiple categories in an image . CountGD handles this issue very well in this example by leveraging the generalization capabilities of the pretrained vision-language model GroundingDINO .

Carpk .To test cross-dataset generalization, CountGD is trained on FSC-147 , and tested on the CARPK car counting dataset zero-shot, without being trained on any images in CARPK. In Table 2, CountGD is trained on the FSC-147  training set with both visual exemplars and text, and tested on the CARPK car counting dataset under two settings: (1) using only the text input "car", and (2) using both the text input "car" and the same two visual exemplars as . Under both settings, CountGD achieves state-of-the-art accuracy on CARPK for all open-world object counting methods without being trained on any images in CARPK, achieving lower counting errors than methods like CountTR  and SAFECount  that were fine-tuned on CARPK.

CountBench .We train CountGD on FSC-147, which has at least seven objects in each training image, and evaluate its generalization to counting low numbers of objects in the CountBench test set zero-shot. In Table 2, we compare CountGD's performance on counting low numbers

    &  &  &  &  &  \\  & & & the Class & MAE \(\) & RMSE \(\) & MAE \(\) & RMSE \(\) \\  GroundingDINO  & 2024 & ECCV & Text & 54.45 & 137.12 & 54.16 & 157.87 \\ Patch-selection  & 2023 & CVPR & Text & 26.93 & 88.63 & 22.09 & 115.17 \\ CLIP-count  & 2023 & ACMMM & Text & 18.79 & 61.18 & 17.78 & 106.62 \\ VLCounter  & 2023 & AAAI & Text & 18.06 & 65.13 & 17.05 & 106.16 \\ CountTX  & 2023 & BMVC & Text & 17.10 & 65.61 & 15.88 & 106.29 \\ CountTX  & 2023 & BMVC & Text & 17.10 & 65.61 & 15.69 & 106.06 \\ DAVF\({}_{}\) & 2024 & CVPR & Text & 15.48 & 52.57 & 14.90 & 103.42 \\ GroundingBEC  & 2024 & CVPR & Text & **10.06** & 58.62 & **10.12** & 107.19 \\ CountGD\({}_{}\) (ours) & 2024 & NeurIPS & Text & 12.14 & 47.51 & 14.76 & 120.42 \\ CountGD\({}_{}\) (ours) & 2024 & NeurIPS & Text & 12.14 & **47.51** & 12.98 & **98.35** \\  CountTX  & 2022 & BMVC & Visual Exemplars & 13.13 & 49.83 & 11.95 & 91.23 \\ LOCAL  & 2023 & ICCV & Visual Exemplars & 10.24 & 32.56 & 10.79 & 56.97 \\ DAVE  & 2024 & CVPR & Visual Exemplars & 8.91 & 28.08 & 8.66 & 32.36 \\  CountGD (ours) & 2024 & NeurIPS & Visual Exemplars \& Text & 7.10 & 26.08 & 6.75 & 43.65 \\ CountGD\({}^{}\) (ours) & **2024** & NeurIPS & Visual Exemplars \& Text & **1.70** & **26.08** & **5.74** & **24.09** \\   

Table 1: FSC-147  comparison with the state-of-the-art text-only and visual exemplar-only open-world counting methods. Multi-modal CountGD trained and tested with both visual exemplars and text achieves state-of-the-art counting accuracy for open-world object counting, beating all text-only and visual exemplar-only approaches. CountGD\({}_{}\) trained and tested with only text achieves comparable performance to state-of-the-art text-only counting approaches. * = correction of erroneous GT labels, as explained in section 4.1. GroundingREC , DAVE\({}_{}\), and DAVE  are concurrent work. _Lower MAE and RMSE values mean more accurate results._

    &  &  &  &  &  &  \\  & & & the Class & & MAE \(\) & RMSE \(\) \\   &  & 2023 & ACMM & Text & ✗ & 11.96 & 16.61 \\  & & 2023 & BMVC & Text & ✓ & 8.13 & 10.87 \\  & & 2023 & AAAI & Text & ✗ & 6.46 & 8.68 \\  & & **2024** & **NeurIPS** & **Text** & ✗ & **3.83** & **5.41** \\  & & 2023 & ICCV & Visual Exemplars & ✗ & 9.97 & 12.51 \\  & & 2022 & BMVC & Visual Exemplars & ✓ & 5.75 & 7.45 \\  & & 2022 & WACV & Visual Exemplars & ✓ & 5.33 & 7.04 \\  & & **2024** & **NeurIPS** & **Visual Exemplars \& Text** & ✗ & **3.68** & **5.17** \\   &  & 2023 & BMVC & Text & ✗ & 6.64 & 15.75 \\  & & **2024** & **NeurIPS** & **Text** & ✗ & **0.86** & **3.1** \\   

Table 2: Comparison with state-of-the-art open-world counting methods. **(top)** On CARPK , we compare with text-only and visual exemplar-only methods. CountGD, trained with both visual exemplars and text on FSC-147 , achieves lower error values than all text-only and visual exemplar-only methods, without being trained on any images in CARPK, using either text-only or both text and two visual exemplars at inference. **(bottom)** On CountBench , we compare with currently the best publicly available text-only open-world counting method, CountTX . CountGD (trained on both visual exemplars and text), given only text and zero-shot, achieves significantly lower errors than CountTX. Note, CountBench does not provide visual exemplars.

of objects (2-10) to CounTX , currently the best (according to performance on FSC-147 ) publicly available pre-trained open-world text-specified object counting methods. For this experiment, CountGD trained with both visual exemplars and text on FSC-147, is tested on CountBench zero shot given only text. Because CountBench contains long captions that describe more than the object to count, we only threshold text token similarity scores corresponding to the subject of each caption. CountGD achieves significantly better performance than CountTX on this dataset. In Figure 4, we show qualitative examples of the detections output by CountGD. The subject of each caption is shown with yellow text.

### Ablation Study

Uni-Modal vs. Multi-Modal Training.In Table 3, we compare CountGD's performance using different training and inference procedures on FSC-147 . Training on text only and testing with text only achieves performance comparable to state-of-the-art counting accuracy for text-only approaches, demonstrating the superiority of the GroundingDINO  architecture that we leverage. Training with visual exemplars only and testing with visual exemplars only results in state-of-the-art performance on two out of four of the metrics (mean absolute errors on both the validation and test sets) for visual exemplar-only approaches. This is surprising given that GroundingDINO was pretrained to relate text to images not visual exemplars to images. Despite this, CountGD performs remarkably well in this setting. Multi-modal training and testing with both visual exemplars and text beats both uni-modal approaches and sets a new state-of-the-art for open-world object counting. This ablation study shows that the visual exemplars provide more information than the text in FSC-147 as the performance with visual exemplars only is significantly better than the performance with text only. It also demonstrates that multi-modal training and inference is the superior strategy as it allows CountGD to take advantage of two sources of information about the object instead of one. In Table 5 in the Appendix, we additionally include an ablation study showing the influence of our proposed SAM Test-time normalization and adaptive cropping strategies.

### Language and Exemplar Interactions

Up to this point we have used the text and visual exemplar prompts to specify the target object in a complementary manner; for example giving a visual exemplar of a'strawberry' with the text'strawberry'. It has been seen that the counting performance with prompts in both modalities is, in general, equal or superior to text alone. In this section we investigate qualitatively the case where the text refines or filters the visual information provided by the exemplars. For example, where the visual exemplar is car, but the text specifies the color, and only cars of that color are counted.

Figure 5: Studying visual exemplar and text interactions. We plot the confidence scores of the instances for each image. In (a) and (b) we show we can specify shape with the exemplar and modify color with text. In (c) we show we can specify spatial location with text, and shape with the exemplar.

   Training and testing setting &  &  \\   & MAE\(\) & RMSE\(\) & MAE\(\) & RMSE\(\) \\  CountGD (Test) & 12.14 & 47.51 & 12.98 & 98.35 \\ CountGD (Visual Exemplars) & 7.46 & 29.54 & 8.31 & 91.05 \\
**CountGD (Test \& Visual Exemplars)** & **7.10** & **26.08** & **5.74** & **24.09** \\   

Table 3: Ablation study I: CountGD trained and tested with text only, visual exemplars only, and text and visual exemplars together on FSC-147 . Multi-modal CountGD trained and tested with both text and visual exemplars achieves the lowest counting errors.

In this study, unlike before, we freeze the feature enhancer in addition to the image and text encoders and finetune the rest of the model on FSC-147 . We find that freezing the feature enhancer is necessary for many of these interactions to emerge. Once trained, the new model can use the text to filter instances picked out by the exemplar, and the exemplar can increase the confidence when it reinforces the text. In Figure 5 we show several examples of the interactions observed.

## 5 Conclusion & Future Work

We have extended the generality of open-world counting by introducing a model that can accept visual exemplars or text descriptions or both as prompts to specify the target object to count. The complementarity of these prompts in turn leads to improved counting performance. There are three research directions that naturally follow on from this work: (i) the performance could probably be further improved by training on larger scale datasets, for example using synthetic data as demonstrated recently for counting ; (ii) a larger training set would enable a thorough investigation of freezing more of the GroundingDINO model when adding our new visual exemplar modules; and finally, (iii) the model does not currently predict the errors of its estimates like other computer vision models do .