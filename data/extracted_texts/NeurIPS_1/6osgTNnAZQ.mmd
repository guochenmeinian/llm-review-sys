# Block Transformer: Global-to-Local Language Modeling for Fast Inference

Namgyu Ho\({}^{1,2}\)\({}^{*}\) Sangmin Bae\({}^{1*}\) Taehyeon Kim\({}^{1}\) Hyunjik Jo\({}^{2}\) Yireun Kim\({}^{2}\)

**Tal Schuster\({}^{3}\) Adam Fisch\({}^{3}\) James Thorne\({}^{1}\) Se-Young Yun\({}^{1}\) \({}^{1}\)KAIST AI \({}^{2}\)LG AI Research \({}^{3}\)Google DeepMind {itsnangyu, bsmn0223, thorne, yunseyoung}@kaist.ac.kr [https://github.com/itsnamgyu/block-transformer](https://github.com/itsnamgyu/block-transformer)**

###### Abstract

We introduce the Block Transformer which adopts hierarchical global-to-local modeling to autoregressive transformers to mitigate the inference bottlenecks associated with self-attention. Self-attention requires the key-value (KV) cache of all previous sequences to be retrieved from memory at every decoding step to retrieve context information, leading to two primary bottlenecks during batch inference. First, there is a significant delay in obtaining the first token, as the information of the _entire_ prompt must first be processed to prefill the KV cache. Second, computation of subsequent tokens is bottlenecked by the high memory I/O demand of fetching the entire KV cache, which grows linearly with sequence length, incurring quadratic memory reads overall. We design the Block Transformer to strategically mitigate these costs, by incorporating coarsity and locality into an integrated global-to-local architecture. At the lower layers, we aggregate tokens into fixed size _blocks_ to apply attention across the entire sequence at coarse-grained detail, to capture the global context while minimizing KV cache overhead. At upper layers, we apply attention within each block to decode individual tokens, to model fine-grained details with a lightweight local KV cache. We pretrain vanilla and Block Transformers from scratch and demonstrate that Block Transformers reach 10-20x inference throughput compared to vanilla transformers with equivalent perplexity and zero-shot task performance.

## 1 Introduction

Generating tokens with transformer-based autoregressive language models (LMs) is costly due to the self-attention mechanism that attends to all preceding tokens . To minimize computation, it is common to cache the key-value (KV) states of all tokens during decoding. However, significant initial latency remains from processing the KV states of all prompt tokens during the first decoding step. Also, while subsequent decoding steps only need to compute a single token, this is often bottlenecked by the memory access required to retrieve the KV states of all previous tokens. While numerous techniques have been proposed to reduce attention costs , there has been limited research on effective architectures that structurally mitigate attention overheads in transformer-based LMs.

Hierarchical architectures  have shown potential in efficiently modeling long sequences such as _character_-level text or pixel-level images by pooling inputs into coarser units. While most of these employ upsampling to regress back to a finer level for attention computation, several approaches  further enhance efficiency through local processing within each pooled unit. Nevertheless, they have not recognized or explored the potential of applying local processing to alleviate the overheads of autoregressive inference, explained above.

[MISSING_PAGE_FAIL:2]

Block Transformer

The Block Transformer employs global and local attention mechanisms with hierarchical paradigm by separating the comprehension of the full context and detailed interactions into two distinct stages. Specifically, global context is captured at lower layers in coarse block-level granularity, where each block consists of a fixed number of tokens aggregated into a single embedding. The local dependencies are resolved at upper layers, where multiple subword tokens are decoded in an autoregressive manner using the context embedding from the block decoder, with local attention.

The Block Transformer consists of three components:

1. **Embedder**: The embedder aggregates each block of \(L_{B}\) tokens into an input block embedding.
2. **Block decoder**: The block decoder applies self-attention across the full sequence of blocks to model global dependencies.
3. **Token decoder**: The token decoder applies self-attention within each block to handle fine-grained local dependencies and decode individual tokens.

We outline the design and list efficiency benefits for each component in the following subsections. To lay the groundwork for detailed cost analysis, we begin with a simplified primer on key bottlenecks in autoregressive transformers, assuming a single accelerator device.

### A primer on the key bottlenecks of autoregressive transformers

Key costs of autoregressive transformersThese can be categorized into _compute_, _parameter memory_ and _KV cache memory_. (1) Compute refers to arithmetic operations, dominated by matrix multiplications in the attention and feedforward layers. The number of floating-point operations (FLOPs) is proportional to the number of non-embedding parameters and total tokens, i.e., sequence length \(\) batch size. (2) Parameters must be fully stored in device memory and retrieved at _each_ forward or backward pass, regardless of the sequence length and batch size. (3) During inference, the KV cache of previously computed sequences are typically cached in memory, and retrieved at _each_ forward pass, i.e., decoding step. Similar to compute, its size is proportional to sequence length \(\) batch size.

Case study of Llama 7B The compute cost of _each_ token is roughly \(7 2\) = 14GFLOPs, where 2 comes from the multiply-accumulate operation used in matrix-vector multiplication . The memory size of the model parameters is \(7 2\) = 14GB under 16-bit, or \(2\)-byte, floating-point precision. The size of the KV cache of a _single_ token is 512KB. While this seems minuscule compared to the parameters, the total KV cache can quickly outsize the parameters with more tokens; for sequence length 2048 and batch size 16, the KV cache occupies 16GB. To translate this to _wall-clock time_, note that the compute throughput (FLOP/s) of accelerator devices is 2-3 orders of magnitude higher than HBM memory bandwidth (bytes/s)--a gap which is widening exponentially .

Computation scenarios for autoregressive LMsThese can be broadly divided into _training_ and inference. During training, every token is processed in parallel. For a precise cost analysis of inference, we further dissect it into the _prefill stage_ and the _decode stage_. To generate a response to a given prompt, all input tokens must first be processed to obtain and cache their KV values in each layer, allowing subsequent tokens to access them for self-attention. This is referred to as the prefill stage. Next is the decode stage, where subsequent tokens are generated one at a time. In each forward step, only one token per batch sample is computed, but the KV cache of _all_ preceding tokens must be loaded from memory.

Primary bottlenecks per computation scenario vary significantly(1) During training, all tokens are processed in parallel, thus compute demands far outweigh parameter memory access, which remains constant. (2) Similarly, the prefill stage is typically compute-bound, as all input tokens are processed in parallel. (3) In contrast, the decode stage, given sufficient sequence length, is heavily memory-bound, as only one token per batch sample is computed in each forward pass, while parameter and KV cache memory access demands are significant. Specifically, KV cache memory access exceeds parameter memory access when sequence length and batch size are large, while parameter memory dominates when they are small. A larger batch size helps reduce the relative cost of parameter memory access by amortizing it over batch samples. Our proposed Block Transformer design optimizes inference, especially in batch decoding, where KV cache impacts throughput.

### Embedder

For the embedder, we prioritize simplicity, given the small block lengths \(L_{B}=1,2,4,8\) considered in our study. Our primary design uses a lookup table \(E_{}^{V D_{}}\) to retrieve and concatenate trainable token embeddings. We set the embedding dimension to \(D_{}=D/L_{B}\), where \(D\) is the primary model dimension, used throughout the block and token decoders. We also consider variants which incorporate small encoder transformers (Appendix F), but these do not yield performance improvements (Section 3.4).

### Block decoder

The block decoder aims to contextualize block representations by attending to preceding blocks, utilizing the embedder's output as input. This autoregressive transformer operates at the block level, producing output block embeddings, or _context embeddings_2, that enable the token decoder to autoregressively decode the subsequent block's token contents. Given input block embeddings from the embedder, derived from input tokens \(x_{0:i L_{B}-1}\), the block decoder outputs a context embedding which contains the information to predict \(x_{i L_{B}:(i+1) L_{B}-1}\).

This approach mitigates the quadratic costs of self-attention by using coarse-grained block inputs instead of individual tokens, while preserving global modeling capabilities and ease of hardware acceleration of dense attention . This reduces the context length of a given sequence by \(L_{B}\) compared to a vanilla transformer. We compare the block decoder with vanilla transformer layers, in terms of the key bottlenecks discussed in Section 2.1:

* _Computation_ is reduced by \(L_{B}\) due to reduced input units.
* _Parameter memory access_ is reduced by \(L_{B}\) due to reduced decoding frequency.
* _KV cache size_ is reduced by \(L_{B}\) due to coarsity.3_KV cache access_ is reduced by \(L_{B}^{2}\) due to reduced size _and_ decoding frequency. 
### Token decoder

The token decoder decodes the individual tokens of the next block, taking the form of an autoregressive transformer with a separate embedding table \(E_{}^{V D_{}}\) and classifier. Each block is processed independently, relying on the context embedding as the sole source of information on previous input blocks. Only the _local_ KV cache of the current block needs to be stored in memory and accessed at each decoding step. Conveniently, none of the prompt tokens in previous blocks are attended by future tokens, allowing the prefill stage to be skipped entirely-except for the most recent block.

The relative size of this local KV cache is smaller by a factor of \(R=L/L_{B}\) when compared to the global KV cache of vanilla transformers, which spans the entire sequence length \(L\). For standard lengths \(L=2048\) and \(L_{B}=4\), the reduction factor is a staggering \(R=256\).4 We compare the token decoder with vanilla transformer layers, with respect to key bottlenecks:

* _Computation_ is reduced to near-zero during prefill. Training computes remains the same.
* _Parameter memory access_ remains the same.
* _KV cache size_ is reduced by \(R=L/L_{B}\) with \(L_{B} L\), practically eliminating its memory footprint.5_KV cache memory access_ is equally reduced by \(R\). * _KV cache memory footprint can lower hardware requirements or allow for larger batch sizes, which can increase throughput by further amortizing parameter memory access.
* _KV cache memory access_ is equally reduced by \(R\).

The key to designing the token decoder lies in how to incorporate the context embedding into the decoding process. In our main design, we project the context embedding into prefix tokens, enabling further refinement of the global context. Expanding the number of prefix tokens, i.e., _prefix length_, broadens the token decoder's computation width and allows for finer attention to context information and improved performance, similar to pause tokens . Note that this is only feasible due to local processing, whereas extra tokens in vanilla transformers layers would exacerbate the overhead of KV cache. Extra computation incurred by prefix tokens has minimal effect on inference throughput as inference is largely memory-bound. While we also considered summation and cross-attention based variants (Appendix F), these proved less effective than our main method (Section 3.4).

[MISSING_PAGE_FAIL:5]

The actual inference throughput and memory efficiency of the Block Transformer are significantly higher compared to vanilla models. We measure the maximum throughput , which use maximum batch sizes of each model variant allowed by memory. As shown in Figure 1(a) and Figure 1(b), our models achieve Pareto-optimality, especially demonstrating up to 25 times increase, under two scenarios: _prefill-heavy_ and _decode-heavy_, where the input and output sequence lengths are 2048, 128 and vice-versa. This efficiency improvement is due to effective reductions in KV cache memory, which allows batch sizes to be about six times larger, as summarized in memory per sample in Table 1. The Block Transformer further reduces latency in a prefill-heavy setting, as past KV states of prompts need to be cached only in the block decoder, without forwarding them to the token decoder. As detailed in Appendix I, this overall trend remains consistent even with the FlashAttention algorithm  employed during inference.

The Pareto frontiers for variable fixed batch sizes, i.e., 1, 32, and 256, are illustrated in Appendix J. We discover that as both the model size and batch size increase, the throughput rate of the Block Transformer scales exponentially. Considering that the LLMs typically utilized in real-world applications have billions of parameters, and taking into account the strategy of aggregating multiple user requests to optimize batch inference , the results suggest that our proposed architecture will demonstrate even more benefits in practical multi-tenant deployment scenarios.

In Figure 1(c), we observe that the throughput of the Block Transformer with an 8K prompt length surpasses that of the vanilla model with a 2K prompt length. This is reasonable because the context length of the block decoder is reduced by a factor of 4, and the token decoder is nearly free of KV-cache overheads. Given the rising interest in enabling longer context lengths, even over one million tokens , the Block Transformer has potential to enhance throughput even further.

### Analysis on parameter allocation ratio and block length

Perplexity shows a U-shaped pattern across different allocation ratiosWe explore the impact of different allocation ratios between the block and token decoders on language modeling performance,

Figure 3: (Left: (a), (d)) Average and position-wise loss by the ratio of parameter allocation between block and token decoders. The ratio is represented as block to token decoders. (Center: (b), (e)) Average and position-wise loss in relation to block length \(L_{B}\). (Right: (c), (f)) Training loss curve for variants of the embedder and token decoder. We consider four different lengths for the prefix-based token decoder. We use models with 302M non-embedding parameters and one-to-one ratio trained on 8 billion tokens.

while keeping the total number of non-embedding parameters constant. Figure 2(a) illustrates the training loss across five distinct ratios for three model sizes. Interestingly, there is a clear U-shaped trade-off at all three model sizes. We find that a one-to-one ratio is optimal for models with \(L_{B}=4\) consistently across all model sizes. If either side is too small, there is a noticeable decline in performance. This demonstrates the synergistic effect and the equal importance of the block and token decoders in language modeling.

Larger block and token decoders reduce perplexity at initial and later positions respectivelyWe measure average loss at each position within a block, depicted in Figure 2(d). The position-wise loss typically exhibits a U-shaped pattern, aligning with findings from a previous multiscale language model  and blockwise parallel decoding methods . This trend stems from the lack of global context in context embeddings, which escalates uncertainty at later positions. Moreover, perplexity at specific positions correlates with the parameter sizes of two decoders. A larger block decoder significantly lowers initial position loss due to predictions solely based on the context embedding. In contrast, a larger token decoder improves prediction accuracy for later tokens by better leveraging local context. These interdependent effects dictate the optimal parameter ratio, with similar patterns evident in models of various sizes, detailed in Appendix K.

Shorter block length favors larger block decoder whereas longer length prefers token decoderFigure 2(b) demonstrates that training loss still follows a U-shaped pattern across different allocation ratios, regardless of block length. Optimal ratios shift with block length: shorter blocks benefit from a larger block decoder, while longer blocks perform better with more parameters in the token decoder. This is due to the inverse relationship between block length and FLOPs of the block decoder, which influences model capacity . As Figure 2(e) shows, first position loss significantly decreases with shorter blocks, reflecting increased capacity in the block decoder. While the token decoder shows minimal differences in FLOPs across block lengths, it has more chance to improve the likelihood of later tokens as block length increases, favoring a larger token decoder. These trends are consistent across different model sizes and allocation ratios, detailed in Appendix L.

Larger token decoder and longer block length are beneficial for achieving high-throughputWe evaluate the allocation ratio and block length from a throughput perspective, summarizing the Pareto frontier in Appendix M. Models with larger token decoders reach Pareto-optimality by achieving higher throughput at a minor performance compromise. Since KV cache IO significantly influences inference time, allocating more parameters to the token decoder is advantageous because the local context length is bounded by the block length. Additionally, increasing the block length improves throughput as KV cache length in the block decoder reduces proportionally. Therefore, although our main configuration uses a one-to-one ratio and a block length of four, opting for a longer block length and a larger token decoder could result in a higher-throughput model.

### Ablation on components of the Block Transformer

Lookup strategy is the most effective approach for the embedderIn Figure 2(c), we experiment with three embedder strategies to bundle block tokens into a single embedding. Surprisingly, a complex transformer encoder like RoBERTa  does not outperform a simpler lookup table strategy. Moreover, the encoder-based embedder lowers generation throughput due to additional computational overhead. As a result, we opt for the lookup strategy to steamline the Block Transformer architecture. Although the CLS token approach allows flexibility in block length, we leave it for future work as it compromises language modeling performance.

Prefix token decoder with longer prefixes enhances performance with minimal overheadFigure 2(f) shows the training loss curve for three token decoder strategies. Using a cross-attention module with key and value sequences equal to the block length considerably diminishes performance. In contrast, forwarding context embeddings through self-attention operations enhances performance, with prefix decoding surpassing other methods. Furthermore, extending the prefix beyond four tokens markedly improves perplexity, effectively broadening the computation width of token decoder. Since longer prefixes add minimal inference overhead, we select a prefix length of two by balancing performance with FLOPs. This approach offers new insights into global-to-local modeling, diverging from previous studies  which overlook the potential of local computational capacity in the token decoder. Detailed results across various model sizes are summarized in Appendix N.

### Analysis on global-to-local language modeling

**Global-to-local language modeling efficiently optimizes throughput relative to performance** In Figure 3(a), we transition from vanilla to Block Transformers by adjusting block lengths. As block length increases, training loss changes log-linearly and throughput increases exponentially, clearly demonstrating the efficiency of global-to-local modeling. Using a lookup embedder and token decoder with one prefix token, our model with \(L_{B}=1\) differs from the vanilla model only by removing global attention in the upper layers. Notably, this model achieves loss equivalent to that of the vanilla model after training on 70% of the tokens, while doubling throughput. Despite pruning all past sequences, this robust performance shows that the context embedding can retain relevant information, enabling the effective of use local computations in global-to-local language modeling.

Block transformer can effectively leverage full contextSince the token decoder depends solely on the context embedding, there could be a concern about whether the Block Transformer fully utilize context information. To address this, we evaluate the loss of token positions within a 2K context window using the test set of PG19 dataset . Figure 3(b) indicates that later tokens are consistently predicted with higher likelihood, suggesting that our architecture, which distinguishes between block-level and token-level decoders, effectively leverages full context information. Furthermore, we observed the same trend with an 8K context length (in Figure 20), demonstrating our model's ability to fully exploit at least 8K tokens of context. Our block language modeling further demonstrated its effective utilization of full context in the recent Needle-In-a-Haystack long-context benchmark  (refer to Appendix O for details.)

### IsoFLOP analysis under inference throughput constraints

Previous studies have focused on compute-optimal models to maximize performance within training FLOPs budgets [40; 37], while typically overlooking inference throughput. Recent trends, however, emphasize models that also consider inference throughput constraints, either by overtraining smaller models [76; 74] or by reducing FLOPs of the model itself . In Figure 3(c), an optimal Block Transformer model achieves superior perplexity and triples the throughput when using the training FLOPs and throughput of the vanilla model as budget constraints. This illustrates that our models can effectively balance training efficiency and inference throughput.

### Uptraining from vanilla transformers

Unlike previous studies , our subword-level global-to-local architecture can leverage the initialization from a pretrained vanilla transformer. This enables efficient training, requiring only a small number of data. As shown in Figure 4(a), this uptraining strategy can lead to near-full performance recovery with just 10% of the original training steps, outperforming random initialization strategy. Consistent with previous studies [2; 5], investigating deliberate weight initialization techniques can further enhance the performance convergence. We summarize details in Appendix P.

Figure 4: (a) Training loss curve with varying block lengths. The numbers in the brackets represent the maximum throughput, measured in 1K tokens per second, for prefill-heavy and decode-heavy settings, respectively. (b) The loss at different token positions within context length on the PG19 test set. We average over every 128 sequences for smoothing. (c) Training loss curves under the same budget for both training FLOPs and inference throughput.

### Comparison to related works

Performance comparison to MEGABYTEThe MEGABYTE model  adopts a global-to-local structure but focuses on efficient pretraining over inference. Thus, within the training FLOPs budget, they argue for a larger block decoder based on a 6:1 ratio deemed optimal. As shown in Figure 4(b), we reimplement the token-level MEGABYTE models, and they also achieve significantly higher throughput compared to vanilla models through global-to-local modeling. Nevertheless, consistent with our insights in Section 3.3, our models with enhanced local computational capacity demonstrate a significant throughput increase of over 1.5 times on top of MEGABYTE. See Appendix Q for more details.

Relation to KV cache compressionGlobal-to-local modeling can be viewed through the lens of KV cache compression, where past sequences are entirely pruned (yet compressed into a context embedding) in the upper layers. Similar to techniques that preserve only meaningful tokens determined by accumulated attention scores [78; 87], this offers a promising way to improve decoding speed without compromising performance. Following the observations of [82; 32] that attention often concentrates on the first token (frequently semantically unimportant), Figure 4(c) reveals a similar pattern in our block decoder. This suggests that augmenting the token decoder's input with the initial "sink" block embedding, or a local window of embeddings, could yield substantial performance gains.

## 4 Discussion

### Contextual information encapsulated in context block embedding

Since the input tokens and context embeddings share the same latent space in the token decoder, we analyze the nearest tokens to these block embeddings. Interestingly, Table 7 in Appendix S reveals that context embeddings compress global context rather than outlining the next block. The second prefix often contains information about the last token of current block to aid predicting the first token of the next block. Meanwhile, the first prefix typically matches non-intuitive or the EOS token, suggesting that they carry more general information. In light of this, the block decoder effectively compresses past global contexts, which the token decoder leverages for its local language modeling.

### Techniques for further throughput improvement

Block autoregressive model with parallel token decodingWhen we pretrain the block decoder to predict next input block embeddings, the token decoder can decode all blocks in parallel if the predictions from block decoder are precise. While Mujika  enhance pretraining efficiency by directly predicting the embedding matrix, we find that MSE or contrastive losses  at the block decoder actually degrades performance. Moreover, error accumulation at the block level needs to be addressed, as discretization is not possible with block embeddings. Nevertheless, using pretrained text embeddings [79; 43] as ground truth, instead of jointly training embedder, could be beneficial.

Figure 5: (a) Training loss curve with uptraining strategy. The red horizontal line refers to the training loss of a full pretrained model. (b) Throughput comparison to MEGABYTE. We compare to three sizes of MEGABYTE in the prefill-heavy setting. (c) Visualization of heatmap for attention scores in block decoder. We visualize only the first 64 sequences for clarity.

Predicting multiple blocks at once with longer output lengthIf the model is trained to predict two or three blocks simultaneously, throughput will increase proportionally. For example, if the input block length is four, the token decoder can be pretrained to predict eight tokens, equivalent to two blocks. One efficient training method could be uptraining the original Block Transformer models. To guarantee performance, we can adaptively adjust the prediction length based on the confidence of subsequent blocks or verify those drafts, similar to speculative decoding .

## 5 Related work

The Block Transformer exemplifies an approach to _coarse_ and _local_ processing in autoregressive transformers. We compare our architecture with previous approaches for coarse and local processing, underscoring its unique effectiveness in alleviating bottlenecks in autoregressive inference.

Hierarchical transformers for coarse processingMany previous works have explored hierarchical transformers to process long sequences efficiently. Early works use local encoders at early layers to obtain pooled representations of documents  or image patches . Later works explore a downsample-then-upsample approach to process long sequences at coarser levels within the core of the model . This has been followed by numerous improvements including an autoregressive formulation , dynamic pooling lengths , and reduced decoding steps . While this can reduce KV cache at middle layers, the upper and lower layers suffer from the same attention bottlenecks as vanilla transformer layers, where representations are at the finest level. In contrast, we identify these bottlenecks and mitigate them by applying local processing at the finer levels.

Local processing in modern language modelsEarly hierarchical transformers employ local computation at early layers , but these are limited to encoder models. The most prominent adaptation of local processing in modern LMs is sliding window attention (SWA) , adopted in GPT-3  and Mistral . While SWA reduces KV cache memory, window sizes are typically much longer than the block lengths used in Block Transformer; Mistral uses a window size of 2048, which requires 512 times more KV cache compared to our baseline token decoder. Previous adaptations of SWA typically incorporate global attention layers  or exploit stacked layers  to attend to older sequences. For example, with window size \(W\), the current token attends to \(W\) tokens of context in the final layer; these attend to up to \(2W-1\) tokens in the previous layer; and so on. Due to this dependency, SWA does not benefit from the same prefill optimizations as the Block Transformer. Under our global-to-local approach, the token decoder restricts attention within block boundaries _across all layers_, enabling the prefill stage to be skipped for all preceding blocks.

Global-to-local hierarchical transformersSeveral works on byte-level modeling employ a similar structure as our Block Transformer . However, while we aim to optimize autoregressive inference for subword-level LMs, prior work mainly utilize the hierarchical structure to mitigate the long context lengths of byte-level data in the absence of tokenization. Hence, in contrast to the central role of our local module (token decoder), prior work consider the role of their local module as'mapping a hidden state to a distribution over possible patches', and suggest that'much smaller model can be used'  and may 'cease to contribute to overall performance' . Yu et al.  concludes that it is optimal to assign more parameters to the global module under _fixed training-time_ constraint. In contrast, we find that a more balanced allocation, e.g., 1:1, performs better _and_ faster under _fixed parameter_ constraints, and that even larger token decoders can maximize _inference throughput_, showing that the local module can contribute to performance in an efficient manner. By recognizing the key bottlenecks in autoregressive inference, we believe our work presents a novel interpretation and uncovers previously unrecognized benefits of global-to-local hierarchical transformers.

## 6 Conclusion

We introduced the Block Transformer architecture which highlights the inference-time advantages of global-to-local modeling in autoregressive transformers. Our empirical findings demonstrate that both global and local components play vital roles, and we recognize the inference benefits of token decoder, which was overlooked in previous work. By strategically designing our architecture, we significantly improve throughput compared to vanilla transformers of equal performance. Refer to Appendix A for limitation, Appendix B for future works, and Appendix C for broader impacts.