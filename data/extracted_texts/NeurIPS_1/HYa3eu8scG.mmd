# Training for Stable Explanation for Free

Chao Chen\({}^{1}\)  Chenghua Guo\({}^{2}\)  Rufeng Chen\({}^{3}\)  Guixiang Ma\({}^{4}\)  Ming Zeng\({}^{5}\)  Xiangwen Liao\({}^{6}\)  Xi Zhang\({}^{2}\)  Sihong Xie\({}^{3*}\)

\({}^{1}\)School of Computer Science and Technology, Harbin Institute of Technology (Shenzhen), China

\({}^{2}\)Key Laboratory of Trustworthy Distributed Computing and Service (MoE),

Beijing University of Posts and Telecommunications, China

\({}^{3}\)Artificial Intelligence Thrust, The Hong Kong University of Science and Technology (Guangzhou),

China \({}^{4}\) Intel, USA \({}^{5}\)Carnegie Mellon University, USA

\({}^{6}\)College of Computer and Data Science, Fuzhou University, China

cha01hbox@gmail.com  Chenghanguo,zhangx@bupt.edu.cn

rchen514@connect.hkust-gz.edu.cn  jean.maguixiang@gmail.com

ming.zeng@sv.cmu.edu  liaoxw@fzu.edu.cn

\({}^{*}\)Corresponding to: xiesihong1@gmail.com

###### Abstract

To foster trust in machine learning models, explanations must be faithful and stable for consistent insights. Existing relevant works rely on the \(_{p}\) distance for stability assessment, which diverges from human perception. Besides, existing adversarial training (AT) associated with intensive computations may lead to an arms race. To address these challenges, we introduce a novel metric to assess the stability of top-\(k\) salient features. We introduce R2ET which trains for stable explanation by efficient and effective regularizer, and analyze R2ET by multi-objective optimization to prove numerical and statistical stability of explanations. Moreover, theoretical connections between R2ET and certified robustness justify R2ET's stability in all attacks. Extensive experiments across various data modalities and model architectures show that R2ET achieves superior stability against stealthy attacks, and generalizes effectively across different explanation methods. The code can be found at [https://github.com/ccha005/R2ET](https://github.com/ccha005/R2ET).

## 1 Introduction

Deep neural networks have proven their strengths in many real-world applications, and their explainability is a fundamental requirement for humans' trust . Explanations usually attribute predictions to human-understandable basic elements, such as input features and patterns under network neurons . Given the inherent limitations of human cognition , only the most relevant top-\(k\) features are presented to the end-users . Among many existing explanation methods, gradient-based methods  are widely adopted due to their inexpensive computation and intuitive interpretation. However, the gradients can be significantly manipulated with neglected changes in the input . The susceptibility of explanations compromises their integrity as trustworthy evidence when explanations are legally mandated , such as credit risk assessments .

**Challenges.** As shown in Fig. 1, novel explanation methods  and training methods  are proposed to promote the robustness of explanations. We focus on training methods since novel explanation methods require extra inference (explanation) time and may fail the sanity check . Existing works  study the explanation stability (robustness) through \(_{p}\) distance. However, as shown in the right part of Fig. 1, a perturbed explanation with a small \(_{p}\) distance to the original can exhibit notably different top salient features. Therefore, relying on \(_{p}\) distance as a metric for optimizing the explanation stability fails to attain desired robustness, indicating the necessity fora novel ranking-based distance metric. Second, unlike robustness of prediction and \(_{p}\) distance based explanation, analyses of robustness on ranking-based explanations are challenging due to multiple objectives (feature pairs), which are dependent since features are from the same model. Finally, adversarial training (AT) [86; 74] has been adopted for robust explanations, yet it potentially leads to an attack-defense arms race. AT conditions models on adversarial samples, which intrinsically rely on the objectives of the employed attacks. When defenders resist well against specific attacks, attackers will escalate attack intensity, which iteratively compels defenders to defend against fiercer attacks. Besides that, AT is time-intensive due to the search for adversarial samples per training iteration.

**Contributions.** We center our contributions around a novel metric called "ranking explanation thickness" that precisely measures the robustness of the top-\(k\) salient features. Based on the bounds of thickness, we propose R2ET, a training method, for robust explanations by an effective and efficient regularizer. More importantly, R2ET comes with a theoretically certified guarantee of robustness to obstruct all attacks (Prop. 4.5), avoiding the arms race. Besides certified robustness, we theoretically prove that R2ET shares the same goal with AT but avoids intensive computation (Prop. 4.6), and the connection between R2ET with constrained optimization (Prop. 4.7). We also analyze the problem by multi-objective optimization to prove explanations' numerical and statistical stability in Sec. 5.

**Results.** We experimentally demonstrate that: _i_) Prior \(_{p}\) norm attack cannot manipulate explanations as effectively as ranking-based attacks. R2ET achieves more ranking robustness than state-of-the-art methods; _ii_) Strong evidence indicates a substantial correlation between explanation thickness and robustness; _iii_) R2ET proves its generalizability by showing its superiority across diverse data modalities, model structures, and explanation methods such as concept-based and saliency map-based.

## 2 Related Work

**Robust Explanations.** Gradient-based methods are widely used due to their simplicity and efficiency , while they lack robustness against small perturbations . Adversarial training (AT) is used to improve the explanation robustness [13; 74]. Alternatively, some works propose replacing ReLU function with softplus , training with weight decay , and incorporating gradient- and Hessian-related terms as regularizers [17; 88; 91]. Besides, some works propose _post-hoc_ explanation methods, such as SmoothGrad . More relevant works are discussed in Appendix C.

**Ranking robustness in IR.** Ranking robustness in information retrieval (IR) is well-studied [106; 26; 104]. Ranking manipulations in IR and explanations are different since 1) In IR, the goal is to distort the ranking of candidates by manipulating _one_ candidate or the query. We manipulate input to swap _any_ pairs of salient and non-salient features. 2) Ranking in IR is based on the model predictions. However, explanations rely on gradients, and studying their robustness requires second or higher-order derivatives, necessitating an efficient regularizer to bypass intensive computations.

## 3 Preliminaries

**Saliency map explanations.** Let a classification model with parameters \(\) be \(f(,):^{n}^{C}\), and \(f_{c}(,)\) is the probability of class \(c\) for the input \(\). Denote a saliency map explanation  for \(\) concerning class \(c\) as \((,c;f)=_{}f_{c}(,)\). Since \(f\) is fixed for explanations, we omit

Figure 1: **Left: _Green_ (⃝): Model training. _Yellow_ (⃝)-⃝): Explanation generation for a target input. _Red_ (⃝)-⃝): Adversarial attacks against the explanation by manipulating the input. Right: Two examples of the saliency maps (explanations) show that smaller \(_{p}\) distances do not imply similar top salient features. \((^{})\) has a smaller \(_{2}\) distance from the original explanation \(()\), but manipulates the explanation more significantly (by top-\(k\) metric) shown in blue dashed boxes. Statistically, \((^{})\) has a 67% top-3 overlap in the tabular case, and 36% top-50 overlap in the image, compared with \((^{})\)’s 100% and 92% top-\(k\) overlap, respectively.**

\(\) and fix \(c\) to the _predicted_ class. We denote \((,c;f)\) by \(()\), and the \(i\)-th feature's importance score, e.g., the \(i\)-th entry of \(()\), by \(_{i}()\). The theoretical analysis following will mainly based on saliency explanations, and more explanation methods, such as Grad\(\) Imp, SmoothGrad , and IG , are studied in Sec. 6 and Appendix A.1.2.

**Threat model.** The adversary solves the following problem to find the optimal perturbation \(^{*}\) to distort the explanations without changing the predictions :

\[^{*}=*{arg\,max}_{:\|\|_{2} c}((),(+)),*{arg\,max}_{c}f_{c}()=*{arg\,max}_{c}f _{c}(+). \]

\(\) is the perturbation whose \(_{2}\) norm is not larger than a given budget \(\). Dist\((,)\) evaluates how different the two explanations are. We tackle constraints by constrained optimization (see B.2.3) and will not explicitly show the constraints.

## 4 Explanation Robustness via Thickness

We first present the rationale and formal definition of _thickness_ in Sec. 4.1, then propose _R2ET_ algorithm to promote the thickness grounded on theoretical analyses in Sec. 4.2. All omitted proofs and more theoretical discussions are provided in Appendix A.

### Ranking explanation thickness

**Quantify the gap.** Given a model \(f\) and the associated original explanation \(()\) with respect to an input \(^{n}\), we denote the _gap_ between the \(i\)-th and \(j\)-th features' importance by \(h(,i,j)=_{i}()-_{j}()\). Clearly, \(h(,i,j)>0\)_if and only if_ the \(i\)-th feature has a more positive contribution to the prediction than the \(j\)-th feature. The magnitude of \(h\) reflects the disparity in their contributions. Although the feature importance order varies across different \(\), for notation brevity, we label the features in a descending order such that \(h(,i,j)>0\), \( i<j\), holds for any _original_ input \(\). This assumption will not affect the following analysis.

**Pairwise thickness.** The adversary in Eq. (1) searches for a perturbed input \(^{}\) to flip the ranking between features \(i\) and \(j\) so that \(h(^{},i,j)<0\) for some \(i<j\). A simple evaluation metric, such as \(=[h(^{},i,j) 0]\), can only indicate if the ranking flips at \(^{}\), failing to quantify the extent of disarray. Moreover, it is greatly impacted by the choice of \(^{}\). Conversely, the _explanation thickness_ is defined by the expected gap of the relative ranking of the feature pair \((i,j)\) in a neighborhood of \(\).

**Definition 4.1** (Pairwise ranking thickness).: Given a model \(f\), an input \(\) and a distribution \(\) of perturbed input \(^{}\), the pairwise ranking thickness of the pair of features \((i,j)\) is

\[(f,,,i,j)}}{{=}} _{^{}}[_{0}^{1}h(( t),i,j)dt],(t)=(1-t)+t^{},t . \]

The integration computes the average gap between two features from \(\) to \(^{}\). The expectation over \(^{}\) makes an overall estimation. For example, \(^{}\) can be from a Uniform distribution  or the adversarial samples local to \(\). The relevant work  proposes the boundary thickness to evaluate a model's _prediction_ robustness by the distance between two level sets.

**Top-\(k\) thickness.** Existing works in general robust ranking  propose maintaining the ranking between _every_ two entities (features). However, as shown in Fig. 1, only the top-\(k\) important features in \(()\) attract human perception the most, and will be delivered to end-users. Thus, we focus on the relative ranking between a top-\(k\) salient feature and any other non-salient one.

**Definition 4.2** (Top-\(k\) ranking thickness).: Given a model \(f\), an input \(\), and a distribution \(\) of \(^{}\), the ranking thickness of the top-\(k\) features is

\[(f,,,k)}}{{=}} _{i=1}^{k}_{j=k+1}^{n}(f,,,i,j). \]

**Variants and discussions.** We consider and discuss the following three major variants of thickness.

* One variant of thickness in Eq. (2) is in a _probability_ version: \[_{}(f,,,i,j)}}{{=}} _{^{}}[_{0}^{1}[h( (t),i,j) 0]dt].\]However, the inherent non-differentiability of the indicator function hinders effective analysis and optimization of thickness. One may replace it with a sigmoid to alleviate the non-differentiability issue. Yet, the use of a sigmoid can potentially lead to vanishing gradient problems.
* By considering the relative positions of the top-\(k\) features, one can define an _average precision@\(k\)_ like thickness \(_{ap}}}{{=}}_{k=1}^{K} (f,,,k)\), or a _discounted cumulative gain_ like thickness \(_{dcg}}}{{=}}_{i=1}^{k}_{j=k+1} ^{n},,i,j)}{(j-i+1)}\). Other variants for specific properties or purposes can be derived similarly, and we will study these variants deeply in future work.
* Many non-salient features are less likely to be confused with the top-\(k\) salient features (see Fig. 8), and treating them equally may complicate optimization. Thus, one may approximate the top-\(k\) thickness by \(k^{}\) pairs of features \(_{i=1}^{k^{}}h(,k-i,k+i)\), which selects \(k^{}\) distinct pairs with _minimal_\(h(,i,j)\). We include the case when \(k^{}=k\) denoted with the suffix "-mm" in the experiments.

### R2ET: Training for robust ranking explanations

We will _theoretically_ (in Sec. 5) and _experimentally_ (in Sec. 6) demonstrate that maximizing the ranking explanation thickness in Eq. (3) can make attacks more difficult and thus the explanation more robust. Thus, a straightforward way is to add \((f,,,k)\) as a regularizer during training:

\[_{}=_{}[_{cls}(f, )-(f,,,k)], \]

where \(_{cls}\) is the classification loss. However, direct optimization of \(\) requires \(M_{1} M_{2} 2\) backward propagations _per_ training sample. \(M_{1}\) is the number of \(^{}\) sampled from \(\); \(M_{2}\) is the number of interpolations \((t)\) between \(\) and \(^{}\); and evaluating the gradient of \(h(,i,j)\) requires at least \(2\) backward propagations . In response, we consider the bounds of \(\) in Prop. 4.4, and propose **R2ET** which requires _only_\(2\) backward propagations each time.

**Definition 4.3** (Locally Lipschitz continuity).: A function \(f\) is \(L\)-locally Lipschitz continuous if \(\|f()-f(^{})\|_{2} L\|-^{ }\|_{2}\) holds for all \(^{}_{2}(,)=\{^{ }^{n}:\|-^{}\|_{2}\}\).

**Proposition 4.4**.: (Bounds of thickness) _Given an \(L\)-locally Lipschitz model \(f\), for some \(L>0\), pairwise ranking thickness \((f,,,i,j)\) is bounded by_

\[h(,i,j)-*\|H_{i}()-H_{j}()\| _{2}(f,,,i,j) h(,i,j)+*( L_{i}+L_{j}), \]

_where \(H_{i}()\) is the derivative of \(_{i}()\) with respect to \(\), and \(L_{i}=_{^{}_{2}(,)}\|H_{i }(^{})\|_{2}\)._

Noticing \(_{\|H()\|_{2} 0}(f,,,i,j)=h( ,i,j)\), the objective of the proposed **R2ET**, _Robust Ranking Explanation via Thickness_, is to maximize the gap and minimize Hessian norm:

\[_{}=_{}[_{cls}- _{1}_{i=1}^{k}_{j=k+1}^{n}h(,i,j)+_{2}\|H( )\|_{2}], \]

where \(_{1},_{2} 0\). R2ET with \(_{1}=0\) recovers Hessian norm minimization for smooth curvature . The difference between R2ET in Eq. (6) and vanilla model are two regularizers. \(h(,i,j)=_{i}()-_{j}()\) can be calculated by one time backward propagation over \(()\), and the summations are done by assigning different weights to \(_{i}()\), e.g., \((n-k)\) for \(i<=k\), and \(-k\) for \(i>k\). \(\|H()\|_{2}\) is estimated by the finite difference (calculating the difference of the gradient of \(()\) and \((+)\)), which costs two times backward propagation . Thus, R2ET is at an extra cost of 2 times backward propagation. For comparison, AT (in PGD-style) searches the adversarial samples by \(M_{2}\) (being 40 in ) iterations for each training sample in every training epoch.

We connect R2ET to three established robustness paradigms, including certified robustness, AT, and constrained optimization. Intuitively, three methods assess robustness from the adversary's perspective by identifying the worst-case samples and restricting model responses to these cases. Conversely, R2ET is defender-oriented, which preserves models' behavior stably by imposing restrictions on the rate of change, avoiding costly adversarial sample searches.

**Connection to certified robustness.** Prop. 4.5 delineates the minimal budget required for a successful attack. The defense strategy of maximizing the budget can effectively mitigate the arms race by obstructing all attacks. The strategy essentially aligns with R2ET, but is empirically less stable and harder to converge due to the second-order term in the denominator. The proof is based on .

**Proposition 4.5**.: _For all \(\) with \(\|\|_{2}_{\{i,j\}},i,j)}{_{^{ }}\|H_{i}(^{})-H_{j}(^{})\|_{2}}\), it holds \([h(,i,j)>0]\) = \([h(+,i,j)>0]\) for all \((i,j)\) pair, that is all the feature rankings do not change._

**Connection to adversarial training (AT).** Prop. 4.6 implies that R2ET shares the same goal as AT but employs regularizers to avoid costly computations. The proof in Appendix A.3 is based on .

**Proposition 4.6**.: _The optimization problem in Eq. (6) is equivalent to the following min-max problem:_

\[_{}_{(_{1,k+1},,_{k,n})} _{}[_{cls}-_{i=1}^{k}_{j=k+1}^{n}h (+_{i,j},i,j)], \]

_where \(_{i,j}^{n}\) is a perturbation to \(\) targeting at the \((i,j)\) pair of features. \(\) is the feasible set of perturbations where each \(_{i,j}\) is independent of each other, with \(\|_{i,j}_{i,j}\|\)._

**Connection to constrained optimization.** Prop. 4.7 shows that R2ET aims to keep the feature in the "correct" positions, e.g., \(l_{i}_{i} G_{i}\), for the local vicinity of original inputs, thus promoting robustness. First, Eq. (6) can be considered a Lagrange function for the constrained problem in Eq. (8):

\[_{}_{}[_{cls}+_{ 2}\|H()\|_{2}], l_{i}_{i}( ) G_{i},\; i\{1,,n\}, \]

where \(G_{i} 0\) is a predefined margin. \(l_{i}=(n-k)\) for \(i k\), and \(l_{i}=-k\) otherwise, which labels features at the top as positive and those at the bottom as negative. The proof is based on .

**Proposition 4.7**.: _The optimization problem in Eq. (8) is equivalent to the following problem:_

\[_{}_{}[_{cls}] _{_{i,j}\|_{i}\|_{2}}l_{i} _{i}(+_{i}) G_{i},\; i\{1,,n\}. \]

## 5 Analyses of numerical and statistical robustness

**Numerical stability of explanations.** Different from prior work, we characterize the worst-case complexity of explanation robustness using iterative numerical algorithm for constrained multi-objective optimization. _Threat model._ The attacker aims to swap the ranking of _any_ pair of a salient feature \(i\) and a non-salient feature \(j\), so that the gap \(h(,i,j)<0\), while does not change the input \(\) and the predictions \(f()\) significantly for stealthiness. We assume that the attacker has access to the parameters and architecture of the model \(f\) to conduct gradient-based white-box attacks.

**Optimization problem for the attack.** Each \(h(,i,j)\) can be treated as an objective function for the attacker, who however does not know which objective is the easiest to attack. Furthermore, attacking one objective can make another easier or harder to attack due to their dependency on the same model \(f\) and the input \(\). As a result, an attack against a specific target feature pair does not reveal the true robustness of the feature ranking. We quantify the hardness of a successful attack (equivalently, explanation robustness) by an upper bound on the number of iterations for the attacker to flip the _first_ (unknown) feature pair. The longer it takes to flip the first pair, the more robust the explanations of \(f()\) are. Formally, for any given initial input \(^{(0)}\) (\((p)\) in the superscript means the \(p\)-th attacking iteration), the attacker needs to iteratively manipulate \(\) to reduce the gaps \(h(,i,j)\) for all \(i\{1,,k\}\) and \(j\{k+1,,n\}\) packed in a vector objective function:

\[(,)_{}[h_{1}( ),,h_{m}()],\|f()-f( ^{(0)})\|, \]

where \(h_{}()}}}{{=}}h(,i,j)\), with \(=1,,m\) indexing the \(m\) pairs of \((i,j)\).

_Comments:_ (_i_) The scalar function \(_{i}_{j}h(,i,j)\) is unsuitable for theoretical analyses, since there can be no pair of features flipped at a stationary point of the sum. Simultaneously minimizing multiple objectives identifies when the first pair of features is flipped and gives a pessimistic bound for defenders (and an optimistic bound for attackers). (_ii_) We could adopt the Pareto criticality for convergence analysis , but a Pareto critical point indicates that no critical direction (moving in which does not violate the constraint) leads to a joint descent in _all_ objectives, rather than that an objective function \(h_{}\) has been sufficiently minimized. (_iii_) Different from the unconstrained case in , we constrain the amount of changes in \(\) and \(f()\).

We propose a trust-region method, Algorithm 1, to solve MOO-Attack (A.4 for more details). First, we define the merit function \(_{}\) that combines the \(\)-th objective and the constraint in Eq. (10):

\[_{}(,t)}}}{{=}}\|f( )\|+|h_{}()-t|. \]

As an approximating model for the trust-region method, the linearization of \((,t)\) is

\[l_{_{}}(,t,)=\|f()+J()^{} \|+|h_{}()+g_{}()^{}-t|,\]

where \(J()\) is the Jacobian of \(f\) at \(\) and \(g_{}()\) is the gradient of \(h_{}\). The maximal reduction in \(l_{_{}}(,t,0)\) within a radius of \(>0\) is

\[_{}(,t)}}}{{=}}l_{ _{}}(,t,0)-_{:\|\|}l_{ _{}}(,t,). \]

\(\) is called a critical (approximately critical) point of \(_{}\) when \(_{}(,t)=0\) (or \(\)).

```
1:Input: target model \(f\), input \(\), explanation \(()\), trust-region method parameters \(0<,<1\).
2: Set \(k=1\), \(\ ^{(p)}=\), \(\ \ t_{}^{(p)}=\|f(^{(p)})\|+h_{}(^{(p)})-^{(p)}\).
3:while\(_{1 m}_{}(^{(p)},t^{(p)})\)do
4: Solve TR-MOO(\(^{(p)},^{(p)}\)) to obtain a joint descent direction \(^{(p)}\) for all \(l_{_{}}\).
5: Update \(_{}^{(p)}\) using \(^{(p)}\) for each \(=1,,m\).
6:if\(_{}_{}^{(p)}>\)then
7: Update \(t_{}^{(p+1)}\) and \(^{(p+1)}\).
8:else
9: Update \(^{(p+1)}=^{(p)}\).
10:endif
11:endwhile
```

**Algorithm 1** Attacking a pair of features

The core of Algorithm 1 is TR-MOO(\(,\)), whose optimal solution provides a descent direction \(\) for all linearized merit functions \(l_{_{}}\) (not \(h_{}\) or \(_{}\)):

\[(,)\{_{, }&\\ &l_{_{}}(,t,), 1  m,\\ &\|\|..\]

where \(\) is the minimal amount of descent and \(\) is the current radius of search for \(\).

Different from the local convergence results , we provide a global rate of convergence for Algorithm 1 to justify thickness maximization in Theorem 5.1. Specifically, it implies that increasing gaps between salient and non-salient features tend to result in larger \(h_{}-h_{}\), which makes it harder for attackers to alter the rankings, leading to stronger explanation robustness.

**Theorem 5.1**.: _Suppose that \(f()\) and \(h_{}()\) are continuously differentiable and \(h_{}\) is bounded. Then Algorithm 1 generates an \(\)-first-order critical point for problem Eq. (10) in at most \((h_{}-h_{})}\) iterations, where \(\) is a constant independent of \(\) but depending on \(\) and \(\) in Algorithm 1, \(h_{}}}}{{=}}_{}\{ _{}h_{1}(),,_{}h_{m}()\}\), and \(h_{}}}}{{=}}_{}\{ _{}h_{1}(),,_{}h_{m}()\}\)._

**Statistical stability of explanations.** We use McDiarmid's inequality for dependent random variables and the covering number of the space \(\) of saliency maps \(\) in Theorem 5.2. The theorem justifies maximizing the gap by showing a bound on how likely a model can rank some non-salient features higher than some salient features due to perturbation to \(\). Specifically, for a consistent empirical risk level \(R_{0,1,u}^{}\), a larger gap corresponds to a larger \(u\), which leads to a smaller covering number \((,)\). It indicates a reduced probability of high true risk \(R_{0,1}^{}\), thereby implying more explanations robustness against perturbations. Importantly, this insight is universal regardless of the approach to achieving the larger gaps. See Appendix A.5 for relevant definitions and details.

**Theorem 5.2**.: _Given model \(f\), input \(\), surrogate loss \(_{u}\), and a constant \(>0\), for arbitrary saliency map \(\) and any distribution \(\) surrounding \(\) that preserves the salient features in \(\),_

\[\{R_{0,1}^{}(,) R_{0,1,u}^{}(,)+\}(-}{})(,),\]_where \(\) is the covering number of the space \(\) with radius \(}{8}\). \(\) is the chromatic number of a dependency graph of all pairs of salient and non-salient features  for McDiamid's inequality._

## 6 Experiments

This section experimentally validates the proposed defense strategies, R2ET, from various aspects, and Appendix B provides more details and results. For a fair comparison, R2ET operates without any prior knowledge of the "true" feature ranking. At each training iteration, R2ET aims to preserve the feature ranking determined in the immediately preceding iteration.

**Datasets.** We adopt DNNs for three tabular datasets: Bank , Adult, and COMPAS , and two image datasets, CIFAR-10  and ROCT . ROCT consists of real-world medical images having 771x514 pixels on average, making it comparable in scale to CelebA (178x218), ImageNet (469x387), and MS COCO (640x480). Besides, dual-input Siamese networks  are adopted on MNIST  and two graph datasets, BP  and ADHD .

**Evaluation metrics.** Precision@\(k\) (P@\(k\))  is used to quantify the similarity between explanations before and after attacks. We adopt three faithfulness metrics [72; 15] in Appendix B.2.6 to show that explanations from R2ET are _faithful_.

All the models have comparable _prediction_ performance. Specifically, the vanilla models achieve AUC of 0.87, 0.64, 0.83, 0.99, 0.87, 0.82, 0.69, and 0.76 on Adult, Bank, COMPAS, MNIST, CIFAR-10, ROCT, BP, and ADHD, respectively. Correspondingly, all other models reported only when their AUC is no less than 0.86, 0.63, 0.83, 0.99, 0.86, 0.82, 0.67, and 0.74, respectively.

**Explanation methods.** Gradient (Grad for short) is used as the explanation method if not specified. We also share findings when adopting _robust_ explanation method, e.g., SmoothGrad (SG)  and Integrated Gradient (IG) , and _concept_-based explanation methods (on ROCT datasets).

### Compared methods

We conduct two attacks in the PGD manner : Explanation Ranking attack (**ERAttack**) and **MSE attack**. ERAttack minimizes \(_{i=1}^{k}_{j=k+1}^{n}h(^{},i,j)\) to manipulate the ranking of features in explanation \(()\), and MSE attack maximizes the MSE (i.e., \(_{2}\) distance) between \(()\) and \((^{})\). The proposed defense strategy **R2ET** is compared with the following baselines.

* **Vanilla**: provides the basic ReLU model trained without weight decay or any regularizer term.
* **Curvature smoothing based methods**: Weight decay (**WD**)  implicitly binds Hessian norm by weight decay during training, and Softplus (**SP**) [16; 17] replaces ReLU with Softplus\((x;)=(1+e^{ x})\). **Est-H** and **Exact-H** consider the _estimated_ (by the finite difference ) and

  
**Method** & Adult & Bank & COMPAS & MNIST & CIFAR-10 & ROCT & ADHD & BP \\  \# of features & 28 & 18 & 16 & 28\({}^{}\) & 32\({}^{}\) & 771\({}^{}\)514 & 6555 & 3240 \\  Vanilla & 87.6 / 87.7 & 83.0 / 94.0 & 84.2 / 99.7 & 59.0 / 64.0 & 66.5 / 68.3 & 71.9 / 77.7 & 45.5 / 81.1 & 69.4 / 88.9 \\ WD & 91.7 / 91.8 & 82.4 / 85.9 & 87.7 / 99.4 & 59.1 / 64.8 & 64.2 / 65.6 & 77.2 / 68.9 & 47.6 / 79.4 & 69.4 / 88.6 \\ SP & 92.4 / 92.5 & 95.4 / 95.5 & 95.9 / **100.0** & 62.9 / 66.9 & 67.2 / 71.9 & 73.9 / 69.5 & 42.5 / 81.3 & 68.7 / 90.1 \\ Est-H & 87.1 / 87.2 & 78.4 / 81.8 & 82.6 / 97.7 & 85.2 / 90.2 & 77.1 / 78.7 & 78.9 / **78.0\({}^{}\)** & 58.2 / 83.7 & (25.0 / 40.4 )\({}^{*}\) \\ Exact-H & 89.6 / 89.7 & 81.9 / 85.6 & 72.9 / 96.0 & _-_ & _-_ & _-_ & _-_ & _-_ & _-_ & _-_ \\ SSR & 91.2 / 92.6 & 76.3 / 84.5 & 82.1 / 92.2 & _-_ & _-_ & _-_ & _-_ & _-_ & _-_ & _-_ \\ AT & 68.4 / 91.4 & 80.0 / 88.4 & 84.2 / 90.5 & 56.0 / 63.9 & 61.6 / 66.8 & 78.0 / 72.9 & 59.4 / 81.0 & 72.0 / 89.0 \\  R2ET\({}_{NH}\) & **97.5 / 97.7** & **100.0\({}^{}\) / 100.0\({}^{}\)** & 91.0 / 99.2 & 82.8 / 89.7 & 67.3 / 72.2 & **79.4\({}^{}\)** / 70.9 & 60.7 / 86.8 & 70.9 / 89.5 \\ R2ET-mm\({}_{H}\) & 93.5 / 93.6 & 95.8 / 98.2 & 95.3 / 97.2 & 81.6 / 89.7 & 72.7 / **79.4\({}^{}\)** & 77.3 / 60.2 & 64.2 / 88.8 & 72.4 / 91.0 \\  R2ET & 92.1 / 92.7 & 80.4 / 90.5 & 92.0 / 99.9 & **85.7 / 90.8** & 75.0 / 77.4 & 79.3 / 70.9 & **71.6\({}^{}\) / 91.3\({}^{}\)** & 71.5 / 89.9 \\ R2ET-mm & 87.8 / 87.9 & 75.1 / 85.4 & 82.1 / 98.4 & 85.3 / **91.4\({}^{}\)** & **78.0\({}^{}\)** / 72.1 & 79.1 / 68.3 & 58.8 / 87.5 & **73.8\({}^{}\) / 91.1\({}^{}\)** \\   

Table 1: P@\(k\) (shown in percentage) of different models (rows) under ERAttack / MSE attack. \(k=8\) for the first three dataset, and \(k=50\) for the rest. **Bold** and underlines) highlight the winner (and runner-up), \({}\) indicates the significant superiority between R2ET winner and non-R2ET winner (pairwise t-test a 5% significance level). \(*\) Est-H has about 4% lower clean AUC than others on BP. Exact-H and SSR only apply to tabular datasets, since computing the exact Hessian and its eigenvalues is extremely expensive.

_exact_ Hessian norm as the regularizer, respectively. **SSR** sets the largest eigenvalue of the Hessian matrix as the regularizer.
* **Adversarial Training (AT)**[31; 92]: finds \(f\) by \(_{f}_{cls}(f;+^{*},y)\), where \(^{*}=*{arg\,max}_{}-_{i=1}^{k}_{j=k+1}^{n}h( +,i,j)\).
* **Variants of R2ET: R2ET-mm** selects _multiple_\((k)\) distinct \(i\), \(j\) with _minimal_\(h(,i,j)\) as discussed in Sec. 4.1. **R2ET\({}_{}\)** and **R2ET-mm\({}_{}\)** are the ablation variants of R2ET and R2ET-mm, respectively, without optimizing the Hessian-related term in Eq. (6) (\(_{2}=0\)). **Est-H** can be considered an ablation variant of R2ET (\(_{1}=0\)).

### Overall robustness results

**Attackability of ranking-based explanation.** Table 1 reports the explanation robustness of different models against ERAttack and MSE attacks on all datasets. More than 50\(\%\) of models achieve at least 90\(\%\) P@\(k\) against MSE attacks, concluding that MSE attack cannot effectively alter the rankings of salient features, even without extra defense (row Vanilla). The ineffective attack method can give a false (over-optimistic) impression of explanation robustness. In contrast, ERAttack can displace more salient features from the top-\(k\) positions for most models and datasets, leading to significantly lower P@\(k\) values than MSE attack. Intuitively, R2ET performs better against attacks since the attackers (by either MSE attack or ERAttack) are supposed to keep the predictions unchanged. R2ET maintains consistency between model explanations and predictions, which ensures that significant manipulation in the top salient features leads to a detectable change in the model's predictions.

**Effectiveness of R2ET against ERAttacks.** We evaluate the effectiveness of different defense strategies against ERAttack, and similar conclusions can be made with MSE attacks case. (1) The best (highest) top-\(k\) of R2ET and its variants across most datasets indicate their superiority in preserving the top salient features. (2) It is counter-intuitive that R2ET\({}_{ H}\), as an ablation version of R2ET, outperforms R2ET on Adult and Bank. The reason is that R2ET\({}_{ H}\) has the highest thickness on these datasets (see Table 3 in Sec. 6.3). (3) Overall, the curvature smoothing-based baselines without considering the gaps among feature importance perform unstably and usually badly across datasets. Their inferior performance is exactly consistent with the discussion in Sec 4.2. Specifically, _solely_ minimizing Hessian-related terms may marginally contribute to the ranking robustness. (4) We do not compare with many AT baselines [74; 70] but Fast-AT , which provides a fairer basis for comparing with R2ET and baselines since they require similar training time and resources. However, AT suffers from unstable robust performance and cannot perform well on most datasets.

**Apply R2ET to other explanation methods.** We demonstrate the efficacy and generalizability of R2ET by adopting the concept-based explanation method, SG and IG, respectively.

Concept-based explanation shows the neurons' maximal activations in a specific model layer. We concentrate on the 512 neurons in the penultimate layer of a ResNet  on ROCT. To this end, the most stimulated neurons must be stable under perturbations. We define \(_{i}\) as the activation map of the \(i\)-th penultimate neuron, and derive the objective analogous to Eq. (6). Empirical results in Table 1 (col ROCT) illustrate that R2ET and its variants, again, perform best against ERAttack.

Table 2 reports the results of models trained using Grad but evaluated by SG and IG. The following conclusions can be drawn: (1) Compared to Grad, SG and IG generally promote explanation robustness. Notably, regardless of the explanation method used, R2ET and its variants consistently

  
**Method** & Adult & Bank & COMPAS & MNIST & ADHD & BP \\  Vanilla & 87.6 / 94.0 / **71.8** & 83.0 / 90.0 / 88.1 & 84.2 / 92.9 / 94.7 & 59.0 / 67.9 / 82.8 & 45.5 / 39.0 / 56.9 & 69.4 / 59.6 / 60.8 \\ WD & 91.7 / 93.5 / 95.5 & 82.4 / 91.3 / 85.7 & 87.7 / 94.9 & 59.1 / 68.3 / 83.0 & 47.6 / 42.3 / 57.2 & 69.4 / 61.8 / 63.9 \\ SP & 92.4 / 95.3 / 63.9 & 95.4 / 96.7 / 99.9 & **99.5 / 100.0** & 62.9 / 69.0 / 88.4 & 45.2 / 36.9 & 68.7 / 58.6 / 60.5 \\ Est-H & 87.1 / 93.1 / 61.5 & 78.4 / 87.3 / 82.4 & 82.6 / 89.9 / 89.9 & 85.2 / 87.9 / 89.5 & 58.2 / 48.9 / 54.7 & (**58.0**/**63.0**/**58.5) \\ AT & 68.4 / 76.6 / 60.0 & 80.0 / 85.9 / 82.6 & 84.2 / 85.9 / 82.4 & 56.0 / 61.5 / 79.3 & 59.4 / 41.2 / 43.0 & 72.0 / 56.7 / 54.4 \\  R2ET\({}_{ H}\) & **97.5** / 97.5 / 57.3 & **100.0** / **100.0** & 91.0 / 96.6 / 93.6 & 82.8 / 87.1 / 89.0 & 60.7 / 56.9 / 61.9 & 70.9 / 64.2 / **66.0** \\ R2ET-mm\({}_{ H}\) & 93.5 / 97.4 / 55.9 & 25.8 / 97.0 / 96.3 & 95.3 / 99.1 / 95.5 & 81.6 / 86.8 / 88.7 & 64.2 / 59.5 / 61.9 & 72.4 / 65.5 / 64.1 \\  R2ET & 92.1 / **99.3** / 54.0 & 80.4 / 88.9 / 84.4 & 92.0 / 99.7 / **100.0** & **85.7** / **88.5** / **90.4** & **71.6** / **67.2** / **65.8** & 71.5 / 64.0 / 65.0 \\ R2ET-mm & 87.8 / 98.6 / 54.2 & 75.1 / 85.1 / 80.3 & 82.1 / 93.5 / 99.5 & 85.3 / 88.3 / 90.1 & 58.8 / 50.1 / 51.4 & **73.8** /**65.6** / 63.9 \\   

Table 2: P@\(k\) of models under ERAttack when adopting Grad / SG / IG as the explanation method.

achieve the highest P@\(k\). (2) Applying Grad to R2ETs usually results in greater robustness than adopting SG/IG to baselines. For example in MNIST, R2ET with Grad attains a P@\(k\) of 85\(\%\), while applying SG to the baselines (except Est-H) fails to exceed a P@\(k\) of 70\(\%\). (3) R2ET can generalize and transfer the robustness to explanation methods divergent from those utilized during training.

### Understanding thickness and attackability

**Assessing model vulnerability: critical role of thickness.** As discussed in Section 5, the number of attack iterations required to reach the _first_ successful flip between salient and non-salient features characterizes the ranking explanation robustness. In Fig. 2, each dot in subplots represents an individual sample \(\). The left subplot demonstrates a high correlation between the sample's thickness and the attacker's required iterations to manipulate the rankings. This high correlation signifies that samples with greater thickness demand a larger attack budget for a successful manipulation, thereby justifying thickness as a more precise metric of explanation ranking robustness.

To understand why R2ET does not always outperform other models, Table 3 juxtaposes P@\(k\) with _dataset_-level thickness, defined as the average thickness of all samples across a dataset. Notably, the model exhibiting optimal explanation robustness (P@\(k\)) consistently displays the greatest thickness, irrespective of the method employed, aligning with Theorem 5.2.

**Optimal Method Selection.** Thickness has proven its efficacy as an indicator for evaluating explanation ranking robustness, rendering it an apt criterion for method selection. Table 1 indicates a more straightforward way to pick a model: deploy R2ET\({}_{ H}\) (and R2ET-mm\({}_{ H}\)) for datasets with a limited feature set, and R2ET (and R2ET-mm) for datasets for datasets with more features. This is because distinguishing salient and non-salient features is easier in datasets with fewer features. Conversely, maintaining such distinctions becomes more complex and potentially less efficient as the feature count increases, where reducing the Hessian norm is advantageous, as it complicates the manipulation of feature magnitude.

### Case study: saliency maps visualization

For visual evaluation, Fig. 3 displays models' saliency maps on the ideal testbeds, MNIST and CIFAR-10. On MNIST, Vanilla and WD perform poorly, where about 50\(\%\) of the top 50 important pixels fell out of top positions under attack. Even worse, the salient pixels identified by Vanilla and WD fail to highlight the digit's spatial patterns (e.g., pixels covering the digits). In contrast, R2ET and R2ET-mm maintain over 90\(\%\) of salient features encoding the digits' recognizable spatial patterns on the top. Similar trends are observed in CIFAR-10, exemplified by the ship. Vanilla and WD exhibit inferior P@\(k\) scores, whereas R2ET and R2ET-mm achieve around 90\(\%\) P@\(k\). Interestingly, all four models identify the front hull of the ship as the significant region for the predictions. However, ERAttack manipulates the explanations of Vanilla and WD to include another region, the wheelhouse, while the critical explanation regions of R2ET and R2ET-mm under attacks remain unaffected. The wheelhouse could be a reason for the ship class, but the inconsistency in explanations due to subtle perturbations raises confusion and mistrust. Fig. 5 provides more results concerning all models.

## 7 Conclusion

We proposed "_explanation ranking thickness_" to measure the robustness of the top-ranked salient features to align with human cognitive capability. We theoretically disclosed the connection between

Figure 2: The number of iterations to first flip versus _sample-level_ thickness (left) and Hessian norm (right) for R2ET on COMPAS. Each dot represents an individual sample \(\).

  
**Method** & Adult & COMPAS & BP \\  SP & 97.4/ 0.9983 & **99.5/ 0.9999** & 68.7/ 0.9300 \\ Est-H & 87.1/ 0.9875 & 82.6/ 0.9557 & **75.0/ 0.93563** \\ R2ET\({}_{ H}\) & **97.5/ 0.9989** & 91.0/ 0.9727 & 70.9/ 0.9271 \\ R2ET & 92.1/ 0.9970 & 92.0/ 0.9865 & 71.5/ 0.9296 \\ R2ET-mm & 87.8/ 0.9943 & 82.1/ 0.9544 & 73.8/ 0.93561 \\   

Table 3: P@\(k\) under ERAttack / _model_-level thickness. Refer to Table 5 for more results.

thickness and a min-max optimization problem, and a global convergence rate of a constrained multi-objective attacking algorithm against the thickness. The theory leads to an efficient training algorithm _R2ET_. On 8 datasets (vectors, images, and graphs), we compared 7 state-of-the-art baselines and 3 variants of R2ET, and consistently confirmed that explanation ranking thickness is a strong indicator of the stability of the salient features. However, R2ET is based on the surrogate loss of thickness, rather than exact thickness, which prevents it from outperforming others all the time. Besides, the theoretical analysis and discussions are based on gradient-based explanation methods. In the future, we plan to further apply R2ET to a broader spectrum of explanation methods. We also plan to investigate scenarios involving highly variable and noisy data and further adjust R2ET to ensure robustness and reliability in more diverse and challenging environments. This paper goals to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel are negative and must be specifically highlighted here.

## 8 Acknowledgments

This material is based upon work supported by the National Science Foundation under Grant Number 2008155. Chao Chen was supported by the National Key Research and Development Program of China (No. 2023YFB3106504), Pengcheng-China Mobile Jointly Funded Project (No. 2024ZY2B0050), and the Natural Science Foundation of China (No. 62476060). Chenghua Guo and Xi Zhang were supported by the Natural Science Foundation of China (No. 62372057). Rufeng Chen and Sihong Xie were supported in part by the National Key R&D Program of China (No. 2023YFF0725001), the Guangzhou-HKUST(GZ) Joint Funding Program (No. 2023A03J0008), and Education Bureau of Guangzhou Municipality. Xiangwen Liao was supported by the Natural Science Foundation of China (No. 62476060).

Figure 3: Explanations of original (ori.) and perturbed (pert.) images against ERAtack from MNIST (class digit \(3\), _k_=50) and CIFAR-10 (class _ship_, _k_=100). The top \(k\) salient pixels are highlighted, and darker colors indicate higher importance. P@_k_ is reported within each subplot.