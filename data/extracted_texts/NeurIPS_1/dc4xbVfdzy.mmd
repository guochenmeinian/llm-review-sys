# Decision Mamba: A Multi-Grained State Space Model with Self-Evolution Regularization for Offline RL

Qi Lv\({}^{1\,2}\) Xiang Deng\({}^{1\,}\) Gongwei Chen\({}^{1}\) Michael Yu Wang\({}^{2}\) Liqiang Nie\({}^{1\,}\)

\({}^{1}\)School of Computer Science and Technology, Harbin Institute of Technology (Shenzhen)

\({}^{2}\)School of Engineering, Great Bay University

lvqi@stu.hit.edu.cn

Corresponding Author.

###### Abstract

While the conditional sequence modeling with the transformer architecture has demonstrated its effectiveness in dealing with offline reinforcement learning (RL) tasks, it is struggle to handle out-of-distribution states and actions. Existing work attempts to address this issue by data augmentation with the learned policy or adding extra constraints with the value-based RL algorithm. However, these studies still fail to overcome the following challenges: (1) insufficiently utilizing the historical temporal information among inter-steps, (2) overlooking the local intra-step relationships among return-to-gos (RTGs), states and actions, (3) overfitting suboptimal trajectories with noisy labels. To address these challenges, we propose **D**ecision **M**amba (**DM**), a novel multi-grained state space model (SSM) with a self-evolving policy learning strategy. DM explicitly models the historical hidden state to extract the temporal information by using the mamba architecture. To capture the relationship among RTG-state-action triplets, a fine-grained SSM module is designed and integrated into the original coarse-grained SSM in mamba, resulting in a novel mamba architecture tailored for offline RL. Finally, to mitigate the overfitting issue on noisy trajectories, a self-evolving policy is proposed by using progressive regularization. The policy evolves by using its own past knowledge to refine the suboptimal actions, thus enhancing its robustness on noisy demonstrations. Extensive experiments on various tasks show that DM outperforms other baselines substantially.

## 1 Introduction

Offline Reinforcement Learning (RL)  has attracted great attention due to its remarkable successes in the fields of robotic control  and games . As transformer  has exhibited powerful sequential modeling abilities in natural language processing  and computer vision , many efforts  have been made on applying this architecture to offline RL tasks. Transformer-based methods view the reward/return-to-go (RTG), state, and action as a sequence, and then predict actions by using the transformer encoder. However, it often fails to make correct decisions when encountering out-of-distribution states or actions, showing limited robustness. Previous work attempts to address this issue from the perspective of data augmentation  and objective constraints . However, they introduce a significant number of noises or the overestimation bias. Thus, how to enhance model robustness remains a highly challenging and insufficiently explored issue.

In this study, we offer two novel perspectives on improving model robustness through both the model architecture and learning strategy. In terms of the model architecture, (1) although previousstudies have made some modifications to the transformer architecture [23; 44; 52], they have not fully utilized inter-step information, particularly historical information which is critical for decision-making processes. For example, the robot can adjust its subsequent routes based on the historical information of failed paths for completing the navigation task; (2) furthermore, most existing approaches adopt transformer to model the flattened trajectory as a sequence, while ignoring the structural trajectory patterns of the causal intra-step relationship among **RT**Gs, states, and actions (RSAs). A RL policy typically predicts the next action given the current state based on the RTG. Thus, this kind of fine-grained intrinsic connection among RSAs is intuitively beneficial for policy learning. As regards to the learning strategy, (3) there exists a large number of noisy labels in the suboptimal trajectories which hurt the performance of the policy significantly. Although the existing work that generates pseudo trajectories or actions alleviates this problem to some extent [57; 64], it also introduces other biases or errors.

To address the above issues, we propose **D**ecision **M**amba (**DM**), a multi-grained state space model with a self-evolving policy learning strategy for offline RL. In order to adequately leverage the historical information, we adopt mamba to explicitly model the temporal state among inter-steps, since mamba architecture [15; 18; 40] shows a more effective capability of extracting the historical information. Meanwhile, the causal intra-step relationship is beneficial for the model to understand the common patterns within the local dynamics. Thus, we introduce a fine-grained SSM module to extract the local features of structural patterns among the RSA triplet within each intra-step. Apart from modifying the model architecture and aligning it to the trajectory pattern, we also propose a learning strategy to prevent the policy from overfitting noisy labels. This is achieved by a progressive self-evolution regularization which leverages the past knowledge of the policy itself to refine and adjust the target label adaptively.

We conduct comprehensive experiments on Gym-Mujoco and Antmaze benchmark, containing 5 tasks with varying levels of noise and difficulties. The performance of DM surpasses other baselines by approximately 8% with respect to the average normalized score on the three classic Mujoco tasks, showing its effectiveness. In summary, the contributions of this paper are summarized as follows:

* Different from the existing conditional sequence modeling work for offline RL with the transformer architecture, we propose Decision Mamba (DM), a generic offline RL backbone built on State Space Models, which leverages the historical temporal information sufficiently for robust decision making.
* To extract the casual intra-step relationships, we introduce a fine-grained SSM module and integrate it to the original coarse-grained SSM in mamba, which combines the local trajectory patterns with the global sequential features, achieving the multi-grained modeling capability.
* To prevent the policy from overfitting the noise trajectories, we adopt a self-evolving policy learning strategy to progressively refine the target, which uses the past knowledge of the learned policy itself as an additional regularizer to constrain the training objective.

## 2 Related Work

### Offline Reinforcement Learning with Transformer-based Models

Offline Reinforcement Learning (RL) [7; 13; 22; 27; 29; 38; 56; 57; 67] is widely used for robotic control and decision-making. In particular, transformer-based methods [8; 25; 44] reformulate the trajectories as a state/action/RTG sequence, and predict the next action based on the historical trajectories. However, although the sequence modeling methods formulate offline RL in a simplified form, they can hardly deal with the overfitting problem caused by the suboptimal trajectories in offline data [11; 21; 59]. One line of approaches [34; 51; 64; 66] focused on exploiting data augmentation methods, such as generating additional data via the bootstrap method, or training an inverse dynamics model to predict actions for the large amount of unlabelled trajectories. Another line of work [6; 23; 25; 35; 44; 52] attempted to modify the transformer architecture to explicitly make use of the structural patterns within the training data. Furthermore, substantial efforts [37; 54; 58; 62] have also been made on applying regularization terms to learning policies, such as RvS  and QDT . Nevertheless, previous work simply applies transformer to offline RL tasks while seldom considering about adapting the architecture to trajectory learning. Thus, these methods fail to extract the historical information sufficiently and are unable to capture local patterns thoroughly from the trajectories. In this work, we address these issues by proposing DM, a tailored mamba architecture for offline RL tasks. A fine-grained SSM module is designed in DM to supply fine-grained intra-step information to the coarse-grained inter-steps features. Together with the architecture, we also present a self-evolving policy learning strategy to prevent the model from overfitting noise labels.

### State Space Models for Linear-time Sequence Modeling

Recently, State Space Models (SSMs) show high potentials in various domains, including natural language processing [15; 16; 17; 19; 20; 40; 46], computer vision [30; 31; 33; 41; 63; 65] and time-series forecasting . Stemming from signal processing, SSMs capture global dependencies from a sequence more effective in a lightweight structure and shows advantages in compressing the historical information, compared with the transformer architecture. Although SSMs have considerable benefits, it still struggles to perform contextual reasoning. Mamba  is thus proposed to alleviate this problem. It introduced a time-varying selective mechanism and a hardware-friendly design, making it as a competitive architecture against with transformer. The Mamba architecture is then adapted to different downstream tasks by considering the characteristics of these tasks. VIM  and VMMaba  introduced 2D SSMs for image understanding. VideoMamba  introduced spatio-temporal scan for video understanding. In this work, we take the fine-grained trajectory patterns into consideration, and introduce a multi-grained mamba architecture tailored for RL tasks.

## 3 Method

### Preliminaries

Decision Transformer for Offline RL.The fundamental Markov Decision Process  can be represented as \(=(,,,r,)\), where \(\) is the state space, \(\) is the action space, \(:\) is the transition function, \(r:\) is the reward function, and \((0,1]\) is the discount factor. Given an offline dataset \(D_{}\) collected by the behavior policy \((a|s)\), offline RL algorithms aim to maximize the rewards. Formally, the iteration process of learning a policy is as below (\(k\) denotes the index of the learning iteration):

\[Q_{k}^{}=*{argmin}_{Q}_{(s,a,r,s^{ }) D_{}}[Q(s,a)-(r+_{a^{}_{k-1}( |s^{})}Q_{k-1}^{}(s^{},a^{}))]^{2}, \] \[_{k}=*{argmax}_{}_{s D_{}}[ _{a(|s)}Q_{k}^{}(s,a)]\;\;\;_{s  D_{}}[D((|s),(|s))]. \]

When updating the Q function, \((s,a,r,s^{})\) are sampled from \(D_{}\) but the target action \(a^{}\) is sampled from the current policy \(_{k-1}\).

Inspired by the great success of sequence generation models in NLP [9; 39; 48], Decision Transformer  is proposed to model the trajectory optimization problem as an action prediction procedure. Specifically, it first obtains the return-to-go (RTG) with the reward, i.e., \(R_{t}=_{i=t}^{T}r_{i}\). Then, the learned policy, which is based on the decoder-only transformer architecture , predicts the action sequence \(a_{i}\) autoregressively, with the offline trajectory \(=(s_{0},R_{0},a_{0},,s_{T},R_{T},a_{T})\). The training objective is as follows:

\[*{minimize}_{}\;(_{}^{k})= _{}_{t=1}^{T}-_{}^{k}(a_{t}|_{t-t:t})  \]

where \(_{t-l:t}=(s_{j},R_{j},a_{j},,s_{t},R_{t})\) (\(j=(t-l,0)\)) is the input trajectory and \(l\) is the length of context window.

SSMs for Linear-Time Sequence Modeling.The State Space Model (SSM) describes the probabilistic dependence between the continuous input signal \(x(t)\) and the observed output \(y(t)\) via the latent hidden state \(h(t)\) as Eq. (5):

\[h^{}(t)=h(t)+x(t), \] \[y(t)=h(t). \]In order to apply the SSM model to the discrete input sequence instead of the original continuous signal, Structured SSM (S4)  discretizes it by a step size \(\) as Eq. (7).

\[h_{t} =}h_{t-1}+}x_{t}, \] \[y_{t} =h_{t}, \]

where \(}=(),\ }=()^{-1}( ()-)\). To this end, the model can forward-propagate in an efficient parallelizable mode with a global convolution. Due to the linear time invariance brought by the SSM model, it lacks the content-aware reasoning ability which is important in sequence modeling. Therefore, mamba  proposes the selective SSM which adds the length dimension to the original parameters \((,,)\), changing it from time-invariant to time-varying. It uses a parameterized projection to project the size of parameter \(,\) from \((,)\) to \((,,)\), and \(\) from \(()\) to \((,,)\), where \(,,\) and \(\) denotes the channel size, batch size, sequence length and hidden size, respectively.

### Decision Mamba

The transformer architecture has been well used in offline RL tasks. Despite its strong ability to understand complete trajectory sequence, it shows limited capabilities in capturing historical information. Thus, we propose a multi-grained space state model to extract the fine-grained local information to supply the coarse-grained global information, namely Decision Mamba (DM), for comprehensively learning the trajectory representation. Figure 1 presents the overall framework of DM.

#### 3.2.1 Multi-Grained Mamba

Trajectory EmbeddingsFollowing the sequence modeling , we first use multilayer perceptrons (MLPs) to embed the RSAs from the given trajectory \(=(R_{0},s_{0},a_{0},,R_{T},s_{T},a_{T})\). Then, the trajectory embeddings are added the absolute step position embeddings to attach the position information, similar to the classic usage in the NLP field. Mathematically, it can be formulated as follows:

\[e_{i}^{}=(R_{i}),\ \ \ \ \ e_{i}^{}= (s_{i}),\ \ \ \ \ e_{i}^{}=(a_{i}), \] \[e_{i}=[e_{i}^{};e_{i}^{};e_{i}^{}] +(e_{i}^{}), \]

where \(e_{i}^{}^{B L N}\), and \([;]\) denotes the concatenate operation.

Coarse-Grained SSMDifferent from the transformer-based methods [8; 25], DM models the historical information before the current \(i\)-th step via the latent hidden state \(h_{i}\) as shown in Eq. (6). It explicitly represents the feature of history information, rather than only learns such information implicitly. As the number of encoder layers increases, historical information is selectively preserved in the representation \(h_{i}\). To this end, DM is expected to have a better capability to understand the sequential dependencies. It can be formulated as follows:

\[h_{i}^{}=((h_{i}));\ \ \ h_{i}^{}=(h_{i}^{}), \]

where \(h_{i}^{}\) represents the coarse-grained hidden state; \(\) denotes the coarse-grained SSM.

Figure 1: Model Overview. _The left_: we combine the trajectories \(\) with position embeddings, and then feed the result sequence to the Decision Mamba encoder which has \(L\) layers. _The middle_: a coarse-grained branch and a fine-grained branch are integrated together to capture the trajectory features. _The right_: visualization of multi-grained scans.

Fine-Grained SSMFurther, to better discern the dependencies among RSA within each step, we gather the feature of each single step to obtain the fine-grained representation via a 1D-convolution layer, and then introduce a fine-grained SSM module for extracting the local pattern among RSA, as shown in the middle part and right part of Figure 1.

It can be formulated as follows:

\[h_{i}^{ FG} = Conv1D(h_{i}), \] \[h_{i}^{ FG} = SiLU(Proj(h_{i}^{ FG})),\] (12) \[h_{i}^{ FG} = IntraS3M(h_{i}^{ FG}), \]

where \(h_{i}^{ FG}\) indicates the fine-grained hidden state; \( IntraS3M\) means the fine-grained SSM.

Fusion ModuleFor gathering both fine-grained local trajectory patterns and coarse-grained global contextual information, we combine the \(h_{i}^{ FG}\) with \(h_{i}^{ CG}\) in each encoder layer and then use the layer normalization to ensure that the multi-grained features have a consistent distribution. In order to remain the important historical information, we add a residual connection. The fusion process can be formulated as follows:

\[h_{i}^{ MG}= LN(h_{i}^{ CG}+h_{i}^{ FG}), \] \[h_{i}= Proj(h_{i}^{ MG}+h_{i-1}), \]

where \(h_{i}^{ MG}\) indicates the multi-grained hidden state and \( LN\) denotes the layer normalization. The forward propagation procedure of DM is presented in Algorithm 1.

```
1:trajectory sequence \(=(R_{0},s_{0},a_{0},,R_{t},s_{t})\)
2:action \(a_{t}\)
3:\(\) obtain the embedding of trajectory sequence\(\)
4:\(_{s}\), \(\), \(_{1} Split(_{t-t})\)
5:\(^{}\), \(^{}\), \(^{}\), \(_{1} MLP()\), \( MLP()\), \( MLP()\)
6:\(_{0}\):\((,,) Flatten(^{ }\), \(^{}\), \(^{}\))
7:for\(i\) in layer do
8:\(_{i}^{ CG}\):\((,) Norm(_{i-1})\)
9:\(_{i}^{ CG}\):\((,) Conv1d^{ CG}(_{i}^{ CG})\)
10:\(^{ CG}\):\((,) Linear_{i}^{ CG}(_{i-1})\)
11:\(\)\(^{ CG}\):\((,) Linear_{i}^{ FG}(_{i-1})\)
12:/\(\) process with multi-grained branches */
13:for\(\) in \((_{i}^{ CG},_{i}^{ FG})\)do
14:\(^{f}\):\((,) SiLU(Conv1d^{}_{i}(^{f}))\)
15:\(^{f}_{i}\):\((,) Parameter\)
16:\(^{f}_{i}\):\((,,) Linear_{i}^{f} (^{f}_{i})\)
17:\(^{f}_{i}\):\((,) Linear_{i}^{f} (^{f}_{i})\)
18:\(^{f}_{i}\):\((,) log(1+exp(Linear_{i}^{f} (^{f}_{i})+ Parameter_{i}^{f}\{\}))\)
19:\(}^{f}_{i}\):\((,,,)(^{f}_{i}^{f}_{i},^{f}_{i})\)
20:\(^{f}_{i}\):\((,) SSM(}^{f}_{i},}^{f}_{i},^{f}_{i})(^{f}_{i})\)
21:endfor
22:\(_{i}^{ CG}\):\((,,)^{ CG} SiLU()\)
23:\(_{i}^{ FG}\):\((,)_{ FG} SiLU()\)
24:\(^{ fusion}\) of multi-grained features */
25:\(_{i}^{ CG}\):\((,) LayerNorm(_{i}^{ CG}+_{i}^{ FG})\)
26:\(_{i}\):\((,) Linear(_{i}^{ MG}+_{i-1})\)
27:endfor
28:\(a_{t}\):\((,) MLP()\)
29:return\(a_{t}\)
```

**Algorithm 1** Decision Mamba

#### 3.2.2 Progressive Self-Evolution Regularization

There are typically amounts of suboptimal trajectories in RL tasks. The previous approaches usually overfit these noisy data and thus lack robustness. Fortunately, the existing literature  has shown that deep models learn clean samples (optimal trajectories) at the beginning of the training process, and then overfit the noisy samples (suboptimal trajectories). Inspired by this observation, we propose a _progressive self-evolution regularization_ (PSER), which uses the knowledge of the past policy to refine the noisy labels as supervision for policy learning, thus avoiding fitting the noisy trajectories.

Specifically, we obtain a refined target by combining the ground truth and the prediction from the learned policy itself. Let \(_{k}\) denote the prediction about \(s\) from the current policy \(_{k}(a|s)\) at \(k\)-th iteration. The refined target at \(k\)-th can be written as follows:

\[_{k}=(1-)a_{k}+\,_{k-1}, \]

where \(_{k-1}_{k-1}(|s)\) and \(\) is the trade-off weight.

To obtain more insights about the refined targets Eq. (16), we compare the gradients of the training objectives with the original label and the refined label. The standard Mean Square Error (MSE) loss function of Eq. (3) with the original label can be written as:

\[_{k}(_{k},a_{k})=||_{k}-a_{k}||^{2}. \]

Figure 2: The process of PSER includes: i) generating action labels with previous step policy, ii) refining target label, iii) computing loss, where the red circle denotes the noise.

In comparison, the loss function with the refined target of Eq. (16) can be rewritten as:

\[_{SE,k}(_{k},a_{k})=||_{k}-_{k}||^{2}=|| _{k}-(1-)\,a_{k}-\,_{k-1}||^{2}. \]

Comparing the objectives of Eq. (17) and Eq. (18), the gradient of \(_{SE,k}\) with respect to the output of policy \(\{a_{k,i}\}_{i=1}^{T}\) can be derived by:

\[_{SE,k}}{ a_{k,i}}=2\,[\,(_{k,i}\,-\,a_{k,i})}_{_{k}}-\,_{k-1,i}-a_{k,i})}_{}\,]\,, \]

where \(_{k}\) indicates the gradient of the loss function Eq. (17), and \(\) computes the difference between the past predictions and the targets. From the perspective of gradient back propagation, PSER imposes a regularization constraint on the current policy \(_{k}(a|s)\) by smoothing the original target action \(a_{k,i}\) with the self-generated label \(_{k-1,i}\).

Moreover, it is important to determine the value of \(\) in Eq. (18). The \(\) controls the learning procedure, where the policy trusts the given actions if \(\) is set to a large value. As stated above, the policy tends to fit gradually from clean patterns to noisy patterns. Thus, we set the \(\) to dynamically increased values. As \(\) increases, the policy progressively gains more confidence in its own past knowledge. To maintain the learning process stable, we apply the linear growth approach and set a lower boundary. The \(\) at the \(k\)-th iteration is computed as follows:

\[_{k}=(_{K},\,_{}), \]

where \(K\) is the number of total iterations for training and \(_{K}\) is the hyperparameter.

We replace the original label with the refined target, leading to the objective:

\[*{minimize}_{}\,\,_{s_{t}, D_{}} _{}(_{t}|R_{t},s_{t},_{<t}). \]

We adopt the MSE loss, and then the objective 21 is converted to:

\[_{,k}(_{k},a_{k})=||_{k}-_{k}|| ^{2}=||_{k}-(1-_{k})\,a_{k}-_{k}\,_{k-1}||^{2}. \]

#### 3.2.3 Training Objective

To make the training procedure more robust, we introduce the inverse training goals: predicting the next state and the next RTG. Individuals often assess the feasibility of actions by envisioning their potential outcomes. Therefore, we expect the policy to predict the post-execution state and RTG based on the predicted action, thus improving its robustness. Specifically, given the trajectory \(=(R_{0},s_{0},a_{0},,R_{t},s_{t})\), Decision Mamba originally predicts the next action \(_{t}\). Further, by incorporating the action \(a_{t}\) to the original trajectory \(_{t-t:t}\), it is also predicts the next RTG \(_{t+1}\) and the next state \(_{t+1}\). Compared to the Eq. (3), the training objective of DM with the refined target can be written as follows:

\[*{minimize}_{}\,\,_{s_{t}, D_{}} _{t=0}^{T}_{1}(_{t}|R_ {t},s_{t},_{<t})}_{}+_{2} (R_{t+1}|_{ t})}_{}+_{3}(s_{t+1}|_{ t})}_{} , \]

where the \(_{t}\) is computed by Eq. (16), \(_{i}\) is the weight hyperparameter, and the sum of \(_{i}\) is set to 1. Note, we omit the length of context window \(l\) for simplicity.

## 4 Experiment

### Settings

Dataset and Evaluation Metrics.We conduct our experiments on _Gym-MuJoCo_ which is one of the mainstream benchmarks used in offline deep RL [14; 28; 59], including Hopper, HalfCheetah, Walker and Ant tasks. Each task contains medium, medium-expert, medium-replay and expert datasets. To more comprehensively evaluate our proposed method, we also adopt the _AntMaze_ benchmark which is a navigation task of aiming to reach a fixed goal location, with the 8-DoF "Ant" quadraped robot. We evaluate Decision Mamba by using the popular suite D4RL . Following the existing literature [8; 25], we normalize the score for each dataset roughly for comparison, by computing \(*{normalized\ score}=100-*{ random\ score}}{-*{random\ score}}\). More details about dataset and implementation can be found in Appendix A.

Baselines.We compare Decision Mamba with existing SOTA offline RL approaches including Behavioral Cloning (BC), Conservative Q-Learning (CQL) , Decision Transformer (DT) , Reinforcement Learning via Supervised Learning (RvS) , StARformer (StAR) , Graph Decision Transformer (GDT) , Waypoint Transformer (WT) , Elastic Decision Transformer (EDT) , and Language Models for Motion Control (LaMo) . Among these methods, CQL stands as a representative of value-based methods, while the other methods belong to supervised learning (SL) approaches. For most of these baselines, we cite the results from the original papers. In addition, we reimplement DT and LaMo for more comparison in different settings by using their repositories. The detailed descriptions of these baselines are presented in Appendix B.

### Overall Results

For a fair comparison, we first conduct experiments on datasets commonly adopted by mainstream approaches. The overall performance is presented in Table 1. It can be observed that Decision Mamba outperforms other baselines in most datasets. On one hand, benefiting from the supervised learning objective, SL-based baselines exhibit a strong ability in the high-quality datasets (M-E), but show weakness in the suboptimal datasets (M/M-R). On the other hand, CQL performs well in the suboptimal datasets due to regularizing the Q-values during training, but struggles to perform well in the high-quality datasets.

For DM, it shows significant improvement over the other SL-based methods. Specifically, it outperforms the best of the baselines by 4%+, especially in suboptimal datasets, e.g., on the medium datasets, the performance of DM surpasses the value-based method CQL and the transformer-based method GDT by around 6% and 9% on average, respectively. This significant improvement demonstrates the robustness of DM in learning from suboptimal datasets, attributed to the multi-grained mamba encoder and PSER module in DM. Note, although DM performs slightly worse than CQL on the Halfcheetah-M-R and HalfCheetah-M datasets, the difference is not significant. Therefore, the proposed DM shows stronger overall performance, capable of learning from both high-quality and suboptimal datasets simultaneously.

In order to evaluate our method more comprehensively, we also conduct experiments on other datasets. We adopt representative baselines including BC, DT and LaMo, where LaMo leverages extensive additional natural language corpora and knowledge to enhance the model performance. As illustrated in Table 2, all methods perform exceptionally well on the Expert dataset. However, when it comes to mixing the suboptimal data into the training set, compared with DT, both LaMo and DM exhibit

  
**Dataset** & **BC** & **CQL\({}^{}\)** & **DT** & **RvS\({}^{}\)** & **StAR\({}^{}\)** & **GDT\({}^{}\)** & **WT\({}^{}\)** & **EDT** & **LaMo** & **DM (Ours)** \\  HalfCheetah-M & 42.2 & **44.4** & 42.6 & 41.6 & 42.9 & 42.9 & 43.0 & 42.5 & 43.1 & 43.8\(\)0.23 \\ Hopper-M & 55.6 & 86.6 & 70.4 & 60.2 & 65.8 & 77.1 & 63.1 & 63.5 & 74.1 & **98.5\(\)**1.99 \\ Walker-M & 71.9 & 74.5 & 74.0 & 73.9 & 77.8 & 76.5 & 74.8 & 72.8 & 73.3 & **80.3\(\)**0.07 \\  HalfCheetah-M-E & 41.8 & 62.4 & 87.3 & 92.2 & **93.7** & 93.2 & 93.2 & 48.5 & 92.2 & 93.5\(\)0.11 \\ Hopper-M-E & 86.4 & 110.0 & 106.5 & 101.7 & 110.9 & 111.1 & 110.9 & 110.4 & 109.9 & **111.9\(\)**1.84 \\ Walker-M-E & 80.2 & 98.7 & 109.2 & 106.0 & 109.3 & 107.7 & 109.6 & 108.4 & 108.8 & **111.6\(\)**3.31 \\  HalfCheetah-M-R & 2.2 & **46.2** & 37.4 & 38.0 & 39.9 & 40.5 & 39.7 & 37.8 & 39.5 & 40.8\(\)0.43 \\ Hopper-M-R & 23.0 & 48.6 & 82.7 & 82.2 & 81.6 & 85.3 & 88.9 & 89.0 & 82.5 & **89.1\(\)**4.32 \\ Walker-M-R & 47.0 & 32.6 & 66.6 & 66.2 & 74.8 & 77.5 & 67.9 & 74.8 & 76.7 & **79.3\(\)**1.94 \\ 
**Avg.** & 50.0 & 67.1 & 75.8 & 71.7 & 77.4 & 79.1 & 78.7 & 72.0 & 77.8 & **83.2\(\)**0.82 \\   

Table 1: Overall Performance. M, M-E, and M-R denotes the medium, medium-expert, and medium-replay, respectively. The results of the baselines marked with \({}^{}\) are cited from their original papers. We report the mean and standard deviation of the normalized score with four random seeds. **Bold** and underline indicate the highest score and second-highest score, respectively.

  
**Dataset** & **BC** & **DT** & **LaMo** & **DM (Ours)** \\  HalfCheetah-E & 83.3 & 90.5 & 92.0 & **93.5\(\)**0.23 \\ Hopper-E & 90.2 & 109.6 & 111.6 & **112.5\(\)**0.75 \\ Walker-E & 103.2 & 108.1 & 108.1 & **108.3\(\)**0.13 \\  Ant-M & 91.0 & 95.3 & 94.6 & **104.8\(\)**1.40 \\ Ant-M-E & 99.8 & 129.6 & 134.8 & **136.2\(\)**0.36 \\ Ant-M-R & 79.5 & 81.4 & **92.7** & 89.5\(\)1.64 \\ Ant-E & 112.6 & 123.1 & 134.2 & **135.9\(\)**0.35 \\  Antmaze-U & 63.0 & 63.0 & 80.0 & **100.0\(\)**0.08 \\ Antmaze-U-D & 61.0 & 61.0 & 70.0 & **90.0\(\)**0.10 \\ 
**Avg.** & 87.1 & 95.7 & 102.0 & **107.9\(\)**3.33 \\   

Table 2: Extensive Results. E, U, and U-D denotes the expert, amazed, and amazed-diverse.

significant superiority, while DM shows a more pronounced overall enhancement. For instance, DM outperforms LaMo by approximately 10% on the Ant-M dataset and around 6% on average. For _AntMaze_, it requires composing parts of suboptimal trajectories to form more optimal policies for reaching goals. "U-D" is more difficult than "U", and DM shows superiority in these tasks. More comparison results can be found in Appendix C.

### Ablation Study

To investigate the effectiveness of each component in DM, we conduct experiments with different variants of DM. In particular, we compare 3 different implementations: (1) _w/o MG_ removes the multi-grained branch, directly using the sequence feature original extracted from mamba; (2) _w/o PSER_ removes the progressive self-evolution regularization, training the model with the labels in the training dataset; (3) _w/o ILO_ removes the inverse learning objective, only predicting the action in the training procedure. As shown in Table 3, the performance of DM drops significantly without either of these components. Notably, the most substantial performance degradation with about 6% occurs when the PSER module is removed, especially in the suboptimal datasets. This observation verifies the effectiveness of this module in preventing policy from overfitting and thus enhancing its robustness. MG and ILO are also critical for offline RL tasks. Once these two modules are excluded, there is a noticeable reduction in the model's performance.

### Comparison Results with Different Context Lengths

To validate whether DM can capture the information of inter-step and intra-step, we investigate the performance of our model with different context lengths. We conduct experiments with the context length \(L=\{20,40,60,80,100,120\}\). Figure 3 shows the comparison results. Regardless of different context lengths, the proposed DM consistently achieves a high score than other baselines among all datasets, showcasing its superiority in capturing the inter-step dependencies in different lengths.

It is noteworthy that BC shows a comparable performance to DT in the Hopper-M and Halfcheetah-M datasets, yet demonstrates a substantial discrepancy in other datasets. We deduce that, in addition to the inherent limitations of the imitation learning paradigm, the BC model that relies solely on MLP also has significant architectural disadvantages. Consequently, it can achieve scores of only 60-70% at most on the expert datasets. When trained on M-R data, BC evidently struggles to learn effectively, achieving only approximately 30% and less than 4% performance on Hopper-M-R and Halfcheetah-M-R, respectively. Due to the attention and SSM mechanisms, DT and DM models conspicuously exhibit a higher upper bound compared to BC. Among them, our proposed DM shows the best performance across all datasets. This indicates that the specific architecture of DM enables it to extract more useful information from the inter-step and intra-step, leading to a strong performance across different context lengths.

### The Effects of \(\) in PSER

We have shown that the proposed PSER in DM enhances the robustness of the policy significantly in learning on suboptimal trajectories. Consequently, we endeavor to delve deeper into the impact of the policy self-evolution throughout the training process. During the training procedure, \(_{K}\) in PSER determines the upper bound of the policy self-evolution. When \(_{K}\) is set to \(1\), the policy has the highest dependency on self-learned knowledge; conversely, if \(_{K}\) is set to \(0\), the policy tends to completely lose its ability to self-evolve. We conduct experiments on DM variants, by removing the lower boundary \(_{}\) and selecting \(_{K}\) from the set \(\{1,0.75,0.5,0.25,0\}\). The results are depicted in

    &  &  &  &  \\  & M & M-E & M-R & & & M-E & &  &  & M-E & M-R \\ 
**DM** & **43.8** & **93.5** & **40.8** & **98.5** & **111.9** & **89.1** & **80.3** & **111.6** & **79.3** & **83.2** \\ _w/o MG_ & 43.3 & 92.9 & 40.1 & 86.2 & 111.2 & 77.5 & 79.2 & 107.9 & 74.9 & 79.2 \\ _w/o PSER_ & 42.9 & 91.0 & 37.5 & 85.3 & 110.4 & 76.4 & 76.2 & 105.6 & 69.6 & 77.2 \\ _w/o ILO_ & 43.1 & 92.3 & 39.4 & 94.0 & 110.7 & 85.3 & 80.1 & 108.8 & 73.5 & 80.8 \\   

Table 3: Ablation Results. “_w/o MG/PSER/ILO_” represents removing the module of multi-grained feature extraction, the progressive self-evolution regularization, and inverse learning objectives, respectively. **Best** results are marked in bold.

## 5 Conclusion

In this paper, we study the offline reinforcement learning from the perspectives of the architecture and the learning strategy. We have accordingly proposed Decision Mamba (DM), a multi-grained state space model tailored for RL tasks with a self-evolving policy learning strategy. DM enhances the policy robustness by adapting the mamba architecture to RL tasks by capturing the fine-grained and coarse-grained information. Meanwhile, the proposed learning strategy prevents the policy from overfitting the noisy labels with a progressive self-evolution regularization. Extensive experiments demonstrate that DM outperforms other baselines by approximately 4% on the mainstream offline RL benchmarks, showing its robustness and effectiveness.

**Limitations and Future Directions**. According to [15; 18], the mamba structure is more friendly to long sequences than the transformer structure, not only in terms of capturing historical information, but also in terms of the computational speed. Benefiting from the structure of SSM, the computational complexity of mamba is \(O(n)\), while the computational complexity of attention score in transformer is \(O(n^{2})\). Thus, the computational efficiency of mamba is higher. Although the exploration of computational efficiency is an exciting direction for future research, it is not within the main scope of this paper.

    &  &  &  &  \\  & M & M-E & & M-R & & & M-E & & M-R & & M-E & M-R & **Avg.** \\  DM (\(_{K}=1\)) & 43.5 & 92.3 & 38.4 & 97.9 & 111.3 & 82.7 & 77.8 & 109.4 & 74.7 & 80.9 \\ DM (\(_{K}=0.75\)) & 42.8 & 91.9 & 38.7 & 98.4 & 110.5 & 83.8 & 77.6 & 106.2 & 71.3 & 80.3 \\ DM (\(_{K}=0.5\)) & **43.9** & 92.1 & 38.8 & **98.6** & 111.1 & 86.6 & 77.2 & 108.6 & 75.8 & 81.4 \\ DM (\(_{K}=0.25\)) & 43.8 & 91.5 & 38.6 & 97.7 & 107.0 & 86.4 & 76.9 & 106.2 & 72.8 & 80.2 \\ DM (\(_{K}=0\)) & 42.9 & 91.0 & 37.5 & 85.3 & 110.4 & 76.4 & 76.2 & 105.6 & 69.6 & 77.2 \\  DM & 43.8 & **93.5** & **40.8** & 98.5 & **111.9** & **89.1** & **80.3** & **111.6** & **79.3** & **83.2** \\   

Table 4: The effects of \(\) in PSER.

Figure 3: Impact of Context Lengths. We compare the normalized scores of BC, DT and DM with different context lengths. The DM consistently outperforms other baselines.