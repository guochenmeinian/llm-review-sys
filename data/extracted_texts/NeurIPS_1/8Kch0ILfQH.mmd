# Bootstrapping Vision-Language Learning with Decoupled Language Pre-training

Yiren Jian\({}^{1}\) Chongyang Gao\({}^{2}\) Soroush Vosoughi\({}^{1}\)

\({}^{1}\)Dartmouth College \({}^{2}\)Northwestern University

###### Abstract

We present a novel methodology aimed at optimizing the application of frozen large language models (LLMs) for resource-intensive vision-language (VL) pre-training. The current paradigm uses visual features as prompts to guide language models, with a focus on determining the most relevant visual features for corresponding text. Our approach diverges by concentrating on the language component, specifically identifying the optimal prompts to align with visual features. We introduce the Prompt-Transformer (P-Former), a model that predicts these ideal prompts, which is trained exclusively on linguistic data, bypassing the need for image-text pairings. This strategy subtly bifurcates the end-to-end VL training process into an additional, separate stage. Our experiments reveal that our framework significantly enhances the performance of a robust image-to-text baseline (BLIP-2), and effectively narrows the performance gap between models trained with either 4M or 129M image-text pairs. Importantly, our framework is modality-agnostic and flexible in terms of architectural design, as validated by its successful application in a video learning task using varied base modules. The code will be made available at [https://github.com/yiren-jian/BLIText](https://github.com/yiren-jian/BLIText).

## 1 Introduction

The field of vision-language (VL) learning seeks to create AI systems that mimic human cognition, processing the world through multi-modal inputs. Core research areas in VL include visual-question-answering (VQA), image captioning, image-text retrieval, and visual reasoning. VL learning began with task-specific learning  and has since progressed to large-scale image-text pre-training paired with task-specific fine-tuning . Furthermore, contemporary studies have begun exploring the use of off-the-shelf frozen pre-trained large language models (LLMs) in VL models , which have delivered impressive results in language generation tasks such as VQA and image captioning.

Present VL models utilizing frozen LLMs are characterized by shared design elements: visual encoders, visual-to-language modules, and frozen LLMs. Except for Flamingo , which employs a visual signal at each layer of the frozen LLM via gated cross-attention, the majority of works  feed aligned visual features as soft language prompts  into the frozen LLMs (see Figure 1 _left_). The models are then trained end-to-end with an image-conditioned language generation loss using large-scale image-text pairs. This conceptually simple and implementation-wise straightforward design has proven effective. BLIP-2  demonstrates that decoupling the end-to-end training into two stages is crucial for state-of-the-art results. The second stage of training involves standard end-to-end learning, while the first stage of training of BLIP-2 utilizes a learnable module (called Query-Transformer/Q-Former) to selectively choose/query visual features relevant to the corresponding text. This reduces 256 features of an entire image to the 32 most relevant visual features that will be sent into the following parts of the model. Stage 1 of BLIP-2 can be viewed as a refined learnable version of early VL works  that use object detectors like Faster-RCNN to select features from regions of objects (objects in images are likely to be mentioned and thus relevant to the accompanying text). We refer to this strategy as "forward-decoupling" since it uses a heuristic to learn/select which useful features are forward-passed into the subsequent model to mitigate challenges in the end-to-end optimization (shown in Figure 1_middle_).

We provide a novel insight to mitigate the challenges in end-to-end optimization by introducing "backward-decoupling" during back-propagation. For a caption \(t\) (e.g., "_a cat wearing sunglasses"_) from VL pre-training dataset \(_{}\), the optimizer first finds the optimal continuous prompt \(p\) for a fixed decoder LLM \(D_{}\): \(p=*{argmin}_{p}(D_{}(p),t)\), before further back-propagating into the vision-to-language module (e.g., Q-Former in BLIP-2, or MLP in ClipCap) and the vision encoder (shown in Figure 1_right_). We realize that the first stage, optimization of \(p\) given \(D_{}\) and \(t\), is purely linguistic and does not restrict the learning text examples from \(_{}\). Thus, we propose to learn this part independently with the available sentence dataset.

While it's not feasible to learn individual prompts \(p\) for each sentence \(t\) due to the infinite number of possible sentences, we propose to parameterize prompt \(p\) by a Prompting-Transformer (P-Former): \(p=E_{}(t)\). This effectively transforms the learning of \(p\) given \(D_{}\) and \(t\) into learning \(E_{}\) by \(*{argmin}_{E_{}}(D_{}(E_ {}(t)),t)\). Essentially, this is an autoencoder with the causal LLM \(D_{}\) as the decoder. As for P-Former, we use a bidirectional Transformer and the [CLS] representation as the bottleneck. Besides the reconstruction loss, we add a contrastive loss to discriminate each sample. Such a design makes \(E_{}\) a semantic sentence embedding model like SimCSE  (i.e., semantically similar sentences have similar representations). Once \(E_{}\) is learned, \(p=E_{}(t)\) will be the "reference prompt" for LLM \(D_{}\) to generate \(t\) auto-regressively. The training overview and P-Former details are shown in Figure 2.

Returning to the VL pre-training, we add a complementary loss to minimize the distance between aligned visual features (being used as language prompts) and the "reference prompt" given by P-Former. We expect this to improve the VL pre-training in two ways: (1) We further decouple the VL learning into another stage, as Li et al.  suggest that multi-stage training is important to mitigate alignment challenges. (2) A semantically rich space is learned for aligned visual features/prompts by a SimCSE design for our P-Former trained with the unimodal sentence dataset (i.e., semantically similar images are encouraged to align to "reference prompts" with close representations).

Our proposed framework only adds a learning objective on tensors feeding into LLMs as prompts (a.k.a images/multi-modalities as foreign languages ). Therefore, our method is agnostic to the input modalities, X encoders, and X-to-language modules (where X can be images, videos, and audio). This could be especially salient for videos, which have much less high-quality paired data  compared to image-text pairs. And because P-Former is only trained with the LLM, there is no need to re-train the P-Former for different modalities.

Figure 1: _left:_ End-to-end training of X-to-language models (where X can be images, videos, or audio), in which aligned input features are provided as prompts to LLMs. Examples include Frozen  and ClipCap . _middle:_ “Forward-decoupled training” as demonstrated in BLIP-2  and X-LLM . For instance, in BLIP-2, the Q-Former is first trained to extract relevant features from the image encoder, and then the selected features are used as prompts for LLM for end-to-end learning. _right:_ We propose “backward-decoupled training”, which initially identifies the “reference prompt” for the LLM to generate the target text, followed by mapping input features to the “reference prompt”.

In our experiments, we take BLIP-2 as an example and show that our proposed framework improves this latest VL method by great margins in various benchmarks of VQA and image captioning. In Section 4.5, we demonstrate its effectiveness in other modalities (i.e., video) using different vision-to-language modules (i.e., plain Transformer over Q-Former).

We anticipate a growing body of future work within the paradigm of "images/multi-modalities as language prompts with frozen LLMs" due to its simplicity and effectiveness, as demonstrated by BLIP-2. For example, a concurrent work X-LLM  extends BLIP-2 from images to videos/speech with more advanced LLMs, augmenting BLIP-2's vision-to-language module Q-Former with Adapters. Because our proposed method is agnostic to input modalities, encoders, and X-to-language modules, it should seamlessly apply to future work within this paradigm of "images/multi-modalities as language prompts with frozen LLMs".

## 2 Related work

End-to-end vision-language learningMost end-to-end VL pre-training models can be broadly classified into two categories: dual-encoder and fusion-encoder models. Dual-encoder models employ two separate networks for vision and language, with the modality interaction computed via dot-product between visual and linguistic features (e.g., CLIP ). Due to the efficient computation of vector dot-product through feature caching, dual-encoder models are effective and highly efficient for image-text retrieval tasks. However, their performance in VQA, captioning, and visual reasoning tasks is limited due to the lack of fine-grained alignment between the two modalities.

Fusion-encoder models, such as ALBEF , VLMo , and CoCa , introduce new fusion-Transformer layers to model deep interactions between the two modalities in addition to vision and language encoders. Common designs include concatenating visual and linguistic features before feeding them into a self-attentive Transformer [4; 7; 8; 14; 19; 20; 25; 27; 35; 37; 38; 54; 56; 59; 60; 61; 63; 66; 68; 71] or cross-attending vision and language encoders to compute fused features [2; 11; 12; 30; 32; 33; 40; 43; 57; 65]. The vision encoder can range from simple linear embeddings  and ConvNets [19; 20; 25; 54; 60; 63; 68] to Transformers [4; 11; 12; 32; 33; 59; 61; 66], an offline pre-trained object detector like Faster-RCNN [7; 8; 14; 35; 37; 38; 56; 71], or an ensemble of models . The language encoder can be initialized with a BERT-based  model or as part of a fusion-Transformer [4; 11; 12; 61; 70]. Most methods utilize three types of losses during pre-training: image-text contrastive (ITC) loss, image-text matching (ITM) loss, and mask language modeling (MLM) loss or language generation (ITG) loss. Fusion-encoder models have shown superior performance in VQA and captioning tasks, though they are less efficient in retrieval tasks. A thorough review of the recent advancements in VL pre-training can be found in Gan et al. .

Figure 2: Overview of P-Former. _left:_ The P-Former training resembles an autoencoder, with the bidirectional P-Former as the encoder and a causal LLM (frozen) as the decoder. The objective is to reconstruct input text auto-regressively. The [CLS] representation serves as sentence embeddings, which are projected back to the length of prompts. The contrastive loss at [CLS] mirrors the training of SimCSE . A regularization vocabulary loss is utilized to encourage the prompts to be close to the vocabulary embeddings. _right:_ Overview of bootstrapping VL pre-training with the trained P-Former. The alignment loss introduced by P-Former is agnostic to input modalities, encoders, and X-to-language modules (i.e., modules within the dashed box can be flexible). P-Former is only used during training and not during inference.

Vision-language learning with frozen language modelsLarge language models, pre-trained on large text corpora, show exceptional performance in language generation tasks. Therefore, incorporating these large frozen language models into VL models can be particularly beneficial for vision-language generation tasks, such as VQA and captioning. Flamingo  incorporates visual signals into each layer of a large frozen LLM using cross-attention. In contrast, Frozen  fine-tunes the image encoder to align visual features as soft prompts, which are input into the frozen language model. Recently, BLIP-2  introduced an additional vision-to-language adaptation module Q-former (in conjunction with the frozen ViT  and an LLM), proposing a two-stage training process to mitigate the challenges in learning visual-language alignment. The first stage of BLIP-2 training optimizes the Q-former to extract beneficial visual features using ITC, ITM, and ITG losses. In the second stage of BLIP-2 training, all three modules (ViT, Q-former, and LLM) are trained end-to-end with only the parameters in Q-former updated. Despite being trained on 129M image-text pairs and with affordable computational resources, BLIP-2 demonstrates competitive results across multiple benchmarks. Finally, a concurrent work on visual chat-bot X-LLM  also adopts a similar architectural design philosophy to BLIP-2. _Our proposed framework with P-Former can be applied to models under this paradigm that use soft prompts as the visual-language interface (e.g., Frozen, BLIP-2, X-LLM, etc)._

Multi-modal auxiliary data learningBesides using off-the-shelf pre-trained vision encoders (ViT and Faster-RCNN ) and language models, it is also interesting to explore how unimodal training can enhance multi-modal models. VLMo  demonstrated the benefits of conducting stage-wise pre-training with image-only and text-only data for their proposed model architecture. Li et al.  proposed using object tags from detectors as anchor points to bridge unpaired images and text, while Zhou et al.  formed pseudo-image-text pairs using an image-text retrieval alignment. Video-language models also leverage image-text pairs by repeating images to create static videos, constructing auxiliary paired datasets for pre-training. Jian et al.  showed that contrastive visual learning could also enhance contrastive sentence embeddings, a purely linguistic task. _We also show how pure language training can enhance a multi-modal model._

## 3 Methodology

Problem formulationGiven an image-text dataset \(\{I,t\}_{}\) and a unimodal language dataset composed purely of sentences \(\{t\}_{}\), our objective is to optimize the pre-training of a vision-language (VL) model. This model consists of a pre-trained vision encoder \(E_{}\), a vision-to-language adaptation module \(}{}\), and a frozen pre-trained language decoder \(D_{}\). The goal is to minimize the image-conditioned language generation loss, given that the vision encoder \(E_{}\) is also frozen:

\[*{argmin}_{}{}} _{}(D_{}( }{}(E_{}(I))),t) \]

As Li et al.  have noted, end-to-end optimization of Equation 1, visualized in Figure 1_left_, can sometimes lead to catastrophic forgetting in LLMs.

### Backward-decoupling and soft prompt pre-training (Training P-Former)

Let's denote the adapted visual features as \(p=}{}(E_{}(I))\), which serve as soft prompts for the LLM \(D_{}\). During the optimization, Equation 1 can be decomposed into two parts, visualized in Figure 1_right_:

\[*{argmin}_{p}_{}(D_{}(p),t) \] \[*{argmin}_{}{ }}_{}(}{ }(E_{}(I)),p) \]

Equation 2 essentially asks _"What is the optimal soft prompt \(p\) that enables the auto-regressive language model \(D_{}\) to generate the sentence \(t\)."_ Like all gradient-based deep learning models, depending on the training dataset, learning \(p\) given \(\{D_{},t\}\) could lead to different sub-optimal points1 (a conventional deep learning problem is usually learning \(D_{}\) given \(\{p,t\}\)). End-to-end

[MISSING_PAGE_FAIL:5]

### Model pre-training

**Training dataset** We employ a 12M subset of the pseudo-labeled  LAION dataset , using only the sentences, for pre-training the P-Former. For VL pre-training, we widely adapted academic setting (since academic institutions lack the resources available to industry researchers to use very large datasets) with approximately 4M image-text pairs. This set comprises the MSCOCO-80K , VG-100K , CC-3M , and SBU-1M  datasets.

**Pre-training models** Our method is universally applicable to any vision-to-text models that utilize prompts as the interface. Owing to its impressive performance and reproducibility, we chose BLIP-2 as the base model for our primary experiments. Thus, for VL pre-training, the image encoder \(E_{}\) is a ViT-g/14 from EVA-CLIP , the LLM decoder \(D_{}\) is an OPT\({}_{}\), and the vision-to-language adaptation module is a Q-Former . The Q-Former is initialized by BERT-base with 32 learnable queries. Our newly proposed P-Former is a base Transformer initialized by BERT-base.

**Pre-training details** The P-Former is trained on a system with 3 \(\) RTX-A6000 (48GB) GPUs, using PyTorch . We trained for five epochs with a linear warm-up and cosine scheduling, using a batch size of 384 (\(3 128\)), and AdamW as the optimizer. The initial learning rate is set to \(1e^{-4}\), with a minimum learning rate of \(1e^{-5}\), a warm-up learning rate of \(1e^{-6}\), and \(2000\) warm-up steps. The VL pre-training is performed on a server equipped with \(8\) RTX-A6000 (48GB) GPUs, using PyTorch. We developed the code based on the LAVIS project . Predominantly, we employed the default configuration files provided by BLIP-2 of LAVIS. Both the stage 1 and stage 2 training ran for 10 epochs with linear warm-up and cosine scheduling, using a batch size of 1024 (\(8 128\)), and AdamW as the optimizer. The weight decay is set to \(0.05\), the initial learning rate is \(1e^{-4}\), the minimum learning rate is \(1e^{-5}\), and the warm-up learning rate is \(1e^{-6}\). The key distinction is that stage 1 and stage 2 incorporate \(5000\) and \(2000\) warm-up steps, respectively. We set \(_{1}=10\) and \(_{2}=100\) while training BLIP-2 OPT\({}_{}\) with our P-Former.

**Computational overhead considerations** Incorporating \(_{}\) from Equation 8 and 9 introduces only a minimal computational overhead, attributable to an additional forward pass of the P-Former (Transformer-base) at each iteration. To illustrate, in our experimental settings using BLIP-2 OPT\({}_{}\), the training time for stage 1 saw a modest increase from 2,669 minutes to 2,743 minutes. Similarly, for stage 2, the training time increased marginally from 1,862 minutes to 1,880 minutes. Thus, our methodology's overall computational burden remains manageable despite its enhancements (the only additional cost is pre-training of the P-Former, which only needs to be done once for an LLM).

Figure 3: An overview of our framework with BLIP-2, which employs a two-stage training process. The green components represent the alignment loss and modules added by us, which do not require gradients. The blue components are part of the original BLIP-2 structure. **P-Former is solely utilized during training and is not required during the inference phase.** Our proposed framework, with P-Former, can be seamlessly applied to any models that leverage prompts as the interface for multi-modal-language communications.

Experiments

Given the impressive performance and accessibility of the BLIP-2 model, coupled with its open-source nature, we primarily employ it as our base model. We aim to demonstrate how our proposed "backward-decoupling" strategy, along with the learned P-Former, can enhance the baselines across various image-to-text generation benchmarks. In Section 4.5, we further extend the applicability of our framework to other modalities, utilizing different base models.

### Zero-shot image-to-text generation

We assess the performance of our pre-trained models on zero-shot VQA, encompassing GQA , OKVQA , and VQAv2 , without any task-specific fine-tuning. As per BLIP-2, we append text prompts to visual prompts prior to their processing by the frozen LLM. Both for the baseline BLIP-2 and our model, the text prompt used is "Question: Short answer.". The results, as detailed in Table 1, suggest that our proposed framework significantly enhances the zero-shot VQA performance of BLIP-2 trained with 4M image-text pairs. Remarkably, the gap between the BLIP-2 trained with 4M and 129M image-text pairs is largely bridged by our method.

### Fine-tuned image captioning

We further fine-tune our pre-trained model for MSCOCO  image captioning, employing the text prompt "a photo of ". Following BLIP-2, we fine-tune the model for 5 epochs using a batch size of 1024 (\(8 128\)), AdamW with an initial learning rate of \(1e^{-5}\), minimum learning rate of \(0\), warm-up learning rate of \(1e^{-8}\) and \(1000\) warm-up steps, with linear warm-up and cosine scheduling. We evaluate our fine-tuned model on the Karpathy test split of MSCOCO. Also, zero-shot transfer results on the NoCaps dataset  are reported. Shown in Table 2, our framework improves BLIP-2 in all metrics, with greater improvements in CIDEr compared to SPICE.

### Zero-shot image-text retrieval

While our proposed method primarily focuses on refining visual prompts for a frozen LLM to generate corresponding text, it may not prove as beneficial for image-text retrieval tasks (the ITC and ITM losses are principally responsible for these tasks). Nevertheless, we present results on zero-shot

    & \#Pretrain & Pretrain &  & OK-VQA & GQA \\  & Image-Text & Uni-Text & val & test-dev & test & test-dev \\  FewVLM  & 9.2M & - & 47.7 & - & 16.5 & 29.3 \\ Frozen  & 3M & - & 29.6 & - & 5.9 & - \\ VLKD  & 3M & - & 42.6 & 44.5 & 13.3 & - \\ Flamingo3B  & 1.8B & - & - & 49.2 & 41.2 & - \\  OPT\({}_{2.78}\) BLIP-2  & 4M & - & 46.8 & 45.6 & 25.9 & 30.5 \\ OPT\({}_{2.78}\) Ours & 4M & ✓ & 52.6 & 52.2 & 30.0 & 34.0 \\ OPT\({}_{2.78}\) BLIP-2\({}^{}\) & 129M & - & **53.5** & **52.3** & **31.7** & **34.6** \\   

Table 1: Comparison with different methods on zero-shot VQA \({}^{}\): numbers taken from Li et al. .

    & \#Pretrain &  &  \\  & Image-Text & in-domain &  &  &  &  \\  & C & S & C & S & C & S & C & S & B@4 & C \\  OSCAR  & 4M & - & - & - & - & - & 80.9 & 11.3 & 37.4 & 127.8 \\ VinVL  & 5.7M & 103.1 & 14.2 & 96.1 & 13.8 & 88.3 & 12.1 & 95.5 & 13.5 & 38.2 & 129.3 \\ BLIP  & 129M & 114.9 & 15.2 & 112.1 & 14.9 & 115.3 & 14.4 & 113.2 & 14.8 & 40.4 & 136.7 \\ OFA  & 20M & - & - & - & - & - & - & - & 43.9 & 145.3 \\ Flamingo  & 1.8B & - & - & - & - & - & - & - & - & 138.1 \\ SimVLM  & 1.8B & 113.7 & - & 110.9 & - & 115.2 & - & 112.2 & - & 40.6 & 143.3 \\  OPT\({}_{2.78}\) BLIP-2  & 4M & 115.3 & 15.0 & 111.0 & 14.6 & 112.5 & 14.0 & 111.9 & 14.5 & 41.8 & 140.4 \\ OPT\({}_{2.78}\) Ours & 4M & 118.3 & 15.3 & 114.7 & 14.9 & 114.1 & 14.1 & 115.1 & 14.8 & 42.3 & 141.8 \\ OPT\({}_{2.78}\) BLIP-2\({}^{}\) & 129M & **123.0** & **15.8** & **117.8** & **15.4** & **123.4** & **15.1** & **119.7** & **15.4** & **43.7** & **145.8** \\   

Table 2: Comparison with different captioning methods on NoCaps and COCO. All methods optimize the cross-entropy loss during fine-tuning. C: CIDEr, S: SPICE, B: BLEU. \({}^{}\): numbers taken from Li et al. .

MSCOCO, and zero-shot Flickr30K  image-to-text and text-to-image retrievals. We compare two models trained with \(_{}\) (ITC, ITM and ITG) and \(_{}+_{}\), without any further task-specific fine-tuning. As expected, Table 3 reveals that the newly introduced \(_{}\) offers limited benefits for retrieval tasks. However, it does not negatively impact the performance.

### Ablation studies

Impact of alignment loss weightsWe investigate the influence of \(_{1}\) and \(_{2}\) in Equation 8 and 9. \(_{1}=0\) and \(_{2}=0\) refers to BLIP-2, and \(_{1}=10\) and \(_{2}=100\) refers to our default configuration of BLIP-2 + P-Former. The alignment loss introduced by the P-Former proves beneficial in both stages of VL pre-training, as shown in Table 4.

Alternate language modelIn this section, we substitute the decoder-based OPT\({}_{}\) model with an encoder-decoder-based FLAN-T5XL as the new LLM. The experiments are conducted with a limited computational budget on 3 \(\) RTX-A6000 and for 5 epochs on both stage 1 and stage 2. The results, displayed in Table 5, verify the effectiveness of our framework with another LLM.

Effect of P-Former's pre-training sentence datasetsIn our primary experiments, we utilize a dataset containing 12M sentences for P-Former training. We investigate the impact of the pre-training sentence dataset for P-Former by re-training it with 4M sentences from our VL pre-training datasets. We then train BLIP-2 + P-Former and report zero-shot VQA results in Table 6. This examination underscores that both the implicit decoupling of BLIP-2's two-stage training into a 3-stage training (pre-training of P-Former), and the employment of additional unimodal sentences contribute to the improved outcomes.

### Video captioning

Our framework is modality-agnostic with respect to the visual encoder and vision-to-language adaptor, making it applicable to other modalities, such as video. Consequently, we establish a video learning

    & \#Pretrain & VQAv2 & OK-VQA & GQA \\  & Sentences & val & test & test-dev \\  \(\) & - & 46.8 & 25.9 & 30.5 \\ \(\) & 4M & 51.7 & 28.2 & 32.3 \\ \(\) & 12M & **52.6** & **30.0** & **34.0** \\   

Table 6: Ablations on sentence datasets used to train P-Former (using OPT\({}_{}\) as LLMs). The first row w/o P-Former is baseline BLIP-2.

    & Pre-training & Image \(\) Text & Text \(\) Image \\  & objectives & R@1 & R@5 & R@1 & R@5 \\  Flickr30K & \(_{}\) & **94.3** & **99.8** & 82.9 & 95.5 \\ \(_{}\) + \(_{}\) & 93.7 & 99.7 & **83.0** & **95.8** \\  MSCOCO & \(_{}\) & 78.4 & 93.8 & **60.5** & **83.0** \\ \(_{}\) + \(_{}\) & **78.7** & **94.5** & 60.4 & 82.8 \\   

Table 3: Comparison with different image-to-text and text-to-image retrieval methods.

    & \#Pretrain & VQAv2 & OK-VQA & GQA \\  & Image-Text & val & test & test-dev \\  Flan-T5XL & BLIP-2  4M & 48.3 & 31.5 & 36.4 \\ Flan-T5XL & ours  4M & 54.9 & 35.7 & 40.3 \\ Flan-T5XL & BLIP-2  129M & **62.6** & **39.4** & **44.4** \\   

Table 5: Experiments using Flan-T5XL as LLM. ‡: using much less GPUs/epochs compared to Sec.4.1. †: from Li et al. .

    & \#Pretrain & VQAv2 & OK-VQA & GQA \\  & & val & test & test-dev \\  Flan-T5XL & BLIP-2  4M & 48.3 & 31.5 & 36.4 \\ Flan-T5XL & ours  4M & 54.9 & 35.7 & 40.3 \\ Flan-T5XL & BLIP-2  129M & **62.6** & **39.4** & **44.4** \\   

Table 7: VATEX English video captioning. Baseline is a sequential model (I3D \(\) Transformer \(\) OPT\({}_{}\)), training end-to-end with ITG.

pipeline, with the vision encoder set as a frozen I3D  video encoder, the vision-to-language adaptor as a Transformer-base, and the LLM decoder as the OPT\({}_{2.7}\) (also frozen). We then train this model on the VATEX  English training set and evaluate it on the validation set. This dataset contains 26K videos for training. The experiments are conducted on an RTX-A6000. Initially, we train the model solely using \(_{}\) for 10 epochs with the P-Former, followed by end-to-end learning with \(_{}\) for an additional 10 epochs.

Our baseline, represented in Table 7, is competitive with two well-established video captioning models: MITS-VC  and ORG-TRL . It is noteworthy that the current state-of-the-art on this benchmark, VideoCoA , is trained on 10M videos, in contrast to our model, which is trained on merely 26K videos. Furthermore, the integration of P-Former and \(_{}\) enhances the CIDEr score by \(4.3\) (from \(56.6 60.9\)).

Despite being a smaller-scale experiment without large-scale pre-training, we demonstrate that our learning framework can be generalized to another modality (i.e., video-learning), employing a different vision-language adaptor (i.e., a plain Transformer as opposed to a Q-Former).

## 5 Limitations

Despite the modality-agnostic nature of P-Former and its ability to adapt to various encoders and vision-to-language adaptors, the unimodal language pre-training remains contingent on the choice of the frozen LLM. This necessitates re-training of the P-Former for different language decoders such as OPT\({}_{2.7}\) and FLAN-T5XL. Moreover, incorporating P-Former primarily enhances image-to-text generation tasks such as VQA and image captioning, while it falls short in improving image-text retrieval tasks. Finally, our methodology primarily assists in bootstrapping prompt-based VL pre-training, i.e., providing aligned visual features as soft prompts to LLMs. Its application to Flamingo remains unclear due to its cross-attention basis and non-open-source status. Nevertheless, given the simplicity of sequential modules of prompt-based models (as demonstrated by recent works such as Frozen, BLIP-2, X-LLM, etc.), we anticipate that our framework will be broadly applicable to most future work in the academic setting.

## 6 Conclusion and discussion

This paper introduces a novel optimization framework for enhancing vision-language models based on large, frozen LLMs. We observe that the end-to-end image-to-text pre-training can be backwardly decoupled: initially determining the "ideal prompt" that triggers the LLM to generate the target text (which can be trained in an unsupervised fashion), followed by the alignment of visual features to the prompt. To this end, we train a P-Former, which functions similarly to a semantic sentence embedding model, to predict prompts to which visual features should align. Experimental results demonstrate that including alignment loss (via P-Former) in the BLIP-2's framework significantly narrows the performance gap between models trained with 4M and 129M image-text pairs.

The key contributions of this paper are as follows:

* Contrary to most prior studies, which decouple VL pre-training into (1) learning which visual features to forward into language modules and (2) conducting end-to-end learning with the selected visual features (dubbed "forward-decoupling"), we propose an innovative perspective of VL decoupled-training from a backward viewpoint. We bifurcate the training into (1) determining the "ideal prompt" for the LLM to generate the text and (2) aligning visual features to that prompt.
* We introduce the P-Former, designed to predict the "ideal prompt," which is trained using a unimodal sentence dataset. This exhibits a novel application of unimodal training in enhancing multi-modal learning.
* Our proposed training framework substantially enhances a robust and recent baseline (BLIP-2), bridging the gap between models trained with 4M and 129M image-text pairs using accessible hardware (8 \(\) RTX-A6000 in less than 4 days). This considerably lowers the entry barriers to VL pre-training research and is expected to attract interest from groups with limited resources.
* The proposed framework generally applies to different modalities (images, videos, audio, etc.), vision encoders, and vision-to-language modules.

Lastly, we address the commonly asked questions by the reviewers in Appendix A, B, C, D, and E.