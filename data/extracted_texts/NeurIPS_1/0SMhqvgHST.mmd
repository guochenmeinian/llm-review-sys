# EEVEE and GATE: Finding the right benchmarks and how to run them seamlessly

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Model evaluation is a cornerstone of machine learning, guiding model design and progress measurement. Designing generalizable evaluation processes remains a challenge, however, partly due to the vast number of possible domain, task and modality combinations and lack of knowledge of how informative they are. In this paper, we propose _EEVEE_ (Efficient Evaluation process Evolution Engine)1, a method that frames evaluation process design as a learning problem. By analyzing a large number of evaluation metrics from diverse benchmarks and models, EEVEE identifies a smaller subset of tasks with high predictive power over the full set of evaluation metrics, reducing evaluation time. To find the optimal subset maximizing signal while minimizing GPU hours, EEVEE evaluates pre-trained models of various architectures, pretraining schemes, and modalities on diverse downstream tasks and datasets including image classification, segmentation, relational reasoning, zero-shot image-to-text tasks, medical classification and segmentation, video classification, and regression. Our results identify three subsets of benchmarks, with 8, 15 and 21 tasks, providing high quality signal for model generalization. Key benchmarks selected include iWildCam, CLEVR-Math, ACDC, WinoGround, CIFAR100, Fungi, and ADE20K. We structure the subsets into three tiers for 12, 24, and 36 GPU-hour budgets and package them into a unified, efficient, and user-friendly Python framework that we built with the researcher in mind - which we refer to as the GATE engine. Our experiments reveal ConvNextV2, SigLIP and CLIP as top-performing model encoders, with EfficientNetV2 and ResNext50 excelling in medical tasks and challenging image classification, in particular in Happy Whale Individual classification, ConvNet based models seem to outperform transformer models by a factor of 2.5x, which is surprising. The top performing encoder being ConvNextV2 followed by CLIP seems to agree with other recent large scale evaluations. We also demonstrate the framework's versatility in fine-tuning models from text and audio modalities, paving the way for future cross-modal evaluations.

## 1 Introduction

**Increasing Complexities of Benchmarking:** As we create benchmarks for expanding model capability evaluation, the growing number and complexity of these benchmarks inadvertently complicates evaluation, requiring more resources like engineering, computation, and research time. Consequently, prioritizing which benchmarks to use becomes challenging. The high costs and longer wait times of newer, complex benchmarks often deter their adoption, leading researchers to rely on older, simpler benchmarks. This risks missing valuable insights from innovative ideas that may underperform onsimpler benchmarks but have broader applicability, while promoting incremental improvements that overfit to simpler benchmarks but underperform in comprehensive evaluations.

To illustrate the mounting increase in available benchmarks, we can look at the historical benchmarks in deep learning. Few benchmarks have had as much impact as ImageNet , which remains a rich resource for model training and evaluation, particularly in visuo-linguistic models. As key capabilities for deep neural networks were discovered, more benchmarks were generated to measure and stimulate progress in those areas. In natural language processing, the GLUE benchmark , SQuAD , and CoNLL-2003  have been instrumental. In audio processing, LibriSpeech , TIMIT , and VCTK  are widely used. For machine translation, WMT , IWSLT , and Europarl  have driven advancements. Relational reasoning has been advanced by benchmarks such as CLEVR , bAbI , and RAVEN . In segmentation, PASCAL VOC , Cityscapes , and COCO  remain crucial. Large language models are often evaluated using benchmarks like SuperGLUE , LAMBADA , and MMLU . Vision-language models are typically evaluated using benchmarks such as VQA , Visual7W , and Flickr30k .

As a result, a researcher has to choose from all these options, and even more, and then find a way to unify and experiment with their models across all of them. The lack of unification, and the lack of guarantees for their generalization signal, quickly becomes a kind of "evaluation hell", where researchers waste a lot of time just doing redundant things like fixing the same bugs to download datasets, preprocess them etc, while at the same time not having any real signal as to which benchmarks are more informative, other than just knowing what has been used the most - which is usually a function of popularity, and not real informativeness. To elaborate, the adoption of complex evaluation processes that could enhance research efficiency and impact is often hindered by the engineering effort required to evaluate machine learning models. Researchers must create involved pipelines across multiple datasets demanding high data engineering efforts, develop task-specific adapters, and derive nuanced training recipes, which is time-consuming. As a result, researchers often revert to simpler evaluation strategies instead of comprehensive assessments.

A good benchmark should alleviate these burdens by automating dataset handling, integrating task adapters, optimizers, schedulers, and logging mechanisms seamlessly. It should provide broad and meaningful signals with minimal GPU time, accommodating various computational budgets, ensuring inclusivity. Furthermore, an increasingly important factor for a robust modern benchmark engine is its support for multi-modal learning and early fusion techniques. AI systems must seamlessly integrate and reason across multiple modalities, such as text, images, audio, and more. Multi-modal learning enhances self-supervised learning opportunities and provides inherent supervision through natural alignments, like audio-visual synchronization in videos. Early fusion, where data from different modalities is combined at the initial stages of processing, allows models to leverage shared representations, improving generalization and reasoning capabilities across varied tasks and domains. These key desiderata are what motivates the production of this work.

With the desiderata in mind, we next introduce EEVEE, a methodology developed for building high-signal low-cost evaluation routines, and GATE, the resulting benchmark that is designed to be extensible, readable, flexible, modular and robust, supported by a new efficient, easy to use framework.

**EEVEE, Learning Optimal Benchmarks:** The ability to find which benchmarks offer the most signal with respect to a given goal, such that we can optimize our compute time, research iteration speed, and engineering time is increasingly crucial. In this work, rather than just manually designing a new set of benchmarks, we propose a methodology, called _EEVEE (Empirical Evaluation process Evolution Engine)_ that frames evaluation design as a learning problem and then leverages machine learning to automate the discovery and refinement of evaluation processes.

More specifically, EEVEE operates by taking in a large set of performance metrics from diverse models applied across various benchmarks and identifies a smaller subset of benchmarks with high predictive power over the entire set. EEVEE achieves this through two main components: (a) an evolutionary algorithm to optimize the selection of benchmark combinations based on a computed score, and (b) a meta-model trained to predict a model's performance on the full set of benchmarks using performance metrics from a chosen subset. We parameterize the meta-model as as a small neural network.

The meta-model receives input performance metrics from a subset of benchmarks and predicts performance on the full set of performance metrics. Through careful \(k\)-fold cross-validation and leveraging a diverse set of models and benchmarks, EEVEE iteratively evolves benchmark combinations that offer high information content with respect to the entire spectrum of benchmarks, ensuring robust, efficient and comprehensive evaluation that can be targeted to computational budgets ranging from more "GPU Poor" users to high-budget organizations.

Taking the desiderata explained above and the resulting understanding of what a good evaluation engine should look like, we demonstrate the effectiveness of EEVEE by tasking it with the discovery of benchmark combinations that offer good **signal-to-GPU-time** ratio, for the evaluation of **model encoders** - also referred to as backbones, on their ability to adapt to new tasks, domains, and modalities. For this purpose, we choose a pool of 20 models, varying in their pretraining schemes (e.g CLIP, DINO, ImageNet Classification), architectures (e.g. ResNets, ViTs, ConvNext) and even their source modalities (e.g. Whisper, BERT), which we adapt on 31 benchmarks ranging from image classification, segmentation, relational reasoning, zero-shot image-to-text tasks, medical classification and segmentation, video classification, and regression, using robust fine tuning recipes, and training for 10K iterations, ensuring that the signal we get is about models that are adaptable, generalizable and efficient in their adaptation.

By applying 20 models on 31 benchmarks and employing EEVEE on their resulting metrics, we identify three subsets of benchmarks, each targeted to a specific computational budget range. Some of the key benchmarks that have been selected include iWildCam, CLEVR-Math, ACDC, WinoGround, mini-ImageNet, Fungi, ADE20K, and dtextures. We refer to the discovered subsets as _Tiers_, and assign to them identifiers for their sizes, specifically, _small_ (n=8, 12 GPU hours), _base_ (n=15, 24 GPU hours) and _big_ (n=31, 36 GPU hours). We package these tiers into our comprehensive benchmarking suite and software framework (called _GATE_) designed for domain, task and modality transferability evaluation, which facilitates the transfer of neural network encoders to different modalities, domains, and tasks. GATE's architecture caters to the research community, enabling straightforward replacement of these transferable encoders with minimal effort. With these innovations, GATE seeks to evolve the landscape of model encoder evaluation, championing a deeper understanding of transfer learning and model adaptability.

**Contributions:** 1. We introduce _EEVEE_, a machine learning approach for selecting subsets of benchmarks optimized to offer maximal predictive power over a larger benchmark set. 2. We conduct a comprehensive investigation of diverse benchmarks within the space of image, image+text and video modalities, pinpointing those with the highest predictive value for a model's performance in downstream tasks. We apply EEVEE to model encoder evaluation by training 20 models on 31 benchmarks, identifying subsets of 8, 15 and 21 benchmarks that offer high signal-to-GPU-hour ratios. 3. We pack the EEVEE-discovered subsets (of 8, 15 and 21 benchmarks out of 31 benchmarks) into targeted benchmark packs, referred to as tiers, designed for specific compute budgets (of 12, 24 and 36 GPU hours) and project phases, and establish standard experimental settings for these tiers. We call these collectively as the GATE Benchmarks. 4. We develop the _GATE_ engine, a unified benchmark suite and software framework that automates dataset downloading, preprocessing, and pipelining for fine tuning and evaluation. GATE facilitates the incorporation of new model encoders, adapts input modalities, fine-tunes with robust recipes, and logs critical information such as training and evaluation metrics, power, energy, computational usage, task visualizations, and model gradients per layer. 5. Through our extensive investigation, we identify foundation models demonstrating superior transferability across diverse tasks. 6. We advocate for the inclusion of modality-shifting transfer experiments in the standard evaluation process for ML researchers, supported by our experimental results on the performance of existing foundation models in these benchmarks.

## 2 Related Work

**On the Diversity of Benchmarks:** There is a vast array of benchmark suites in machine learning. To the best of our knowledge, the benchmark suites relating strongly to GATE are ImageNet , VTAB , VLMBench  and WILDS . ImageNet has been of tremendous importance and interest to the transfer learning community. Nevertheless, there has been skepticism about overfitting to such datasets resulting from implicitly qualifying models using the test set performance over the years [46; 6] or the test set not being challenging enough to gauge model generalization power . Although ImageNet pre-training helps transfer performance to the many-shot classification setting , it provides minimal to no gains on more challenging datasets such as fine-grainedclassification . Similarily, with a larger distribution shift, ImageNet pre-trained models was found to offer limited benefits for medical imaging tasks due to large distribution shifts induced by fundamental differences in data sizes, features, and task specifications; that is, lightweight models perform comparably to standard architectures . To make matters worse, ImageNet performance is less correlated with and less predictive of downstream performance on diverse tasks beyond classification such as object detection, few-shot classification, and segmentation . On top of it all, when ImageNet is extended with a perturbed temporal dimension, models performance significantly worsen .

**On the Usability of Benchmarks:** Beyond ImageNet, VTAB introduced a benchmark with a wider diversity of tasks and domains . Nevertheless, it does not offer task and domain shifts offered in GATE, such as medical segmentation and video classification and regression that are known to be ill-measured and gauged by ImageNet alone . That said, VTAB offers satellite imaging and 3D tasks which GATE does not. Nevertheless, GATE as a software framework was optimized to minimise usage friction, to take no more than 12 GPU hours on our smallest tier, and, to only require approximately 1 hour of adding the new encoder and wrapping it into GATE wrappers for GATE to be able to go away and take care of everything, including dataset downloading, task adapter integration and full train/val and test cycles with logging of various key metrics. VTAB, in our experience, requires a lot more manual work in getting the datasets, and integrating new models to be adapted. Similarly, VLMBench  and WILDS  offer more diverse datasets beyond previous work but neither offer a tiered approach that enables iterative development of models during pre-training, nor produce extensible and flexible benchmarks that can be easily glued into researchers experimentation code without friction.

**On the Systematic Selection of Benchmarks:** Previous work investigated the properties inherit in multi-task benchmarks that trade-off diversity and sensitivity where the latter is how robust a benchmark ranking is to the inclusion of irrelevant models or minute changes in the tasks themselves . It was found that multi-task benchmark are unstable to irrelevant changes in tasks design. Nevertheless, this is related to how the benchmark ranks models; whether it compares how model often ranks higher than another in cardinal benchmarks or if the performance across tasks is averaged to produce a single rank in cardinal ones. Meanwhile, our benchmark produces fine-grained information to model performances across diverse tasks rather than producing specific ranking which is delegated to the user analysis. Another complementary thread of work investigates dynamic benchmarks where model training and data collection is interleaved to continually challenge model knowledge . To the best of our knowledge, this is the first work that studies the selection of multi-task, multi-domain benchmarks that satisfy limited compute budgets while maximizing research signal.

In summary, Table 1 shows the desiderata that we believe a good evaluation suite and framework should have such that they can both offer the community useful signal, and also balance that with being practical so that people can adopt it.

## 3 EEVEE Methodology

EEVEE is our proposed method for automating the selection of Pareto-optimal benchmark subsets. By analyzing benchmark performance metrics, EEVEE identifies a small, highly informative subset that maximizes information relative to the entire benchmark pool. This ensures that, as machine learning benchmark breadth and depth increases, we will always be able to identify and select few that offer high information about the whole. We strike a balance between providing rich evaluation signals and maintaining simplicity, reducing computational costs and human efforts required for adopting new benchmarks. EEVEE enables the production of a tiered evaluation engine accommodating various computational budgets, fostering an inclusive and accessible research environment, and improving the quality of insights derived from machine learning research while addressing reluctance

  & Baseline+ Benchmark & ImageNet & VTAB & VILMBench & WILDS & GATE (Ours) \\  & Diversity of Tasks & & & & \\  & Diversity of Modalities & & & & \\  & Automatic Dataset Download/Preparation & & & & \\  & Code allows for easy switch of encoders & & & & \\  & Optimized for fast and effective research iteration & & & & \\  & Run Time & & & & \\  & Includes Medical Domains & & & & \\  & Includes Environmental domains & & & & \\  & Tered compute budgets & & & & \\  & GPU poor optimized & & & & \\ 

Table 1: Our Desiderata (first column) VS Benchmarks (first row)towards resource-intensive evaluation processes. This balance between efficiency, simplicity, and signal richness presents EEVE's value proposition for advancing machine learning research.

**Working Principle of EEVEE:** EEVEE works by building a _meta-model_ over the performance metrics of models sufficient both in number and diversity, on the full benchmark pool from which we want to choose our subset. With the term _benchmark_ in this paper we refer to a dataset + task pairs.

Formally, given a large benchmark pool \(B=\{b_{0},b_{1},,b_{K}\}\), where \(B\) is the full set of benchmarks, and \(b_{i}\) are individual benchmarks therein, we have a sufficiently large and diverse pool of model performance metrics \(M=\{m_{0}^{0},m_{1}^{0},,m_{K}^{N}\}\). Here, \(m_{i}^{j}\) is the performance metric of model \(j\) on benchmark \(b_{i}\). We aim to discover a subset of \(B\) of size \(k\). This means \(k\) total benchmarks make up the subset. If we build a meta-model \(g(M_{selected},)\) to predict all of \(M\) given only the selected subset \(M_{selected}\), it should minimize the following loss:

\[L_{EEVEE}=MSE(M,g(M_{selected},)) \]

In this equation, MSE is the mean squared error. \(M\) represents the full set of performance metrics of all our models on the full benchmark pool \(B\). The term \(g(M_{selected},)\) represents the predictions of the meta-model \(g\) with parameters \(\) when it is given the performance metrics of all models from the selected subset of benchmarks \(B_{selected}\), referred to as \(M_{selected}\).

However, our main focus lies in the selected combination of performance metrics \(M_{selected}\) that can generalize well on previously unseen models. To that end, we must split \(M\) into train, validation and test sets, each consisting of performance metrics acquired from different models (e.g. train \(\) ResNet50, ViT-Base, CLIP, and val \(\) ResNext50, DINO, DeIT), and explicitly optimize the inner loop test loss rather than the training loss, while we use the validation loss to select the best meta-model for test. Hence the loss we wish to minimize is:

\[L_{EEVEE}^{test}=MSE(M^{test},g(M_{selected}^{test},)) \]

We need a non-differentiable method for choosing the \(k\) benchmarks in \(M_{selected}\), since brute force becomes intractable very quickly, so we employ evolutionary methods to learn the \(k\) selected benchmarks.

This results in a bi-level optimization, with an evolutionary method on the outer loop \(e(B_{selected})\), where \(e\) is the evolutionary method, and \(B_{selected}\) are the benchmarks being selected - or indeed, the genes being optimized, and a small meta-model parameterized as a neural network \(g()\) that receives a train/val split from \(B_{selected}\) and trains itself to do the task described in Equation 1, after which process it is scored using the val set using the loss in Equation 2. Then, once a given candidate of benchmarks \(B_{selected}\) is scored, in this way, the outer loop performs a tournament selection where only the top 50 candidates are preserved and mutated by removing one benchmark at random, and adding another at random. Each winning candidate mutates into 10 children, and the parent is also preserved in the gene pool, producing a gene pool with 550 candidates for every cycle. At initialization, we sample \(1000\) random combinations. We have found that \(1000\) is a good starting population that is both tractable to score and facilitates the necessary diversity that enables limited variation in results across several runs, showcasing convergent behaviour. diversity that our results across runs have little variation from one another, pointing to a convergent behaviour. We include full pseudocode showcasing all the details related to how we performed EEVEE for our experiments in Algorithm 1, 2 and 3 in Figure 1

```
1:Input: \(\), \(\), \(\), \(^{}\), \(^{}\), \(^{}\), \(^{}\), \(^{Multi-modal early fusion is another topic closely related with model encoders - as research in early fusion can be done most efficiently when trying to learn data encoders rather than a full encoder-decoder, or decoder-only models. World model research in multi-modal dimensions can also take place most efficiently within a model-encoder context. Recent work like I/VJEPA  for example have paved the way for self-supervised learning which functions using model encoders, and has been demonstrated to be more efficient and more generalizable than full pixel decoding variants.

Furthermore, model encoder evaluation has been quite diffused in the past few years, with new benchmarks being produced in every facet of the machine learning field. Nonetheless, most of those lacked in some key quality: they were either simply too complex to use efficiently, requiring too much compute, or, more often than the others, missing a unifying software framework that can easily, in a user-conscious way, and a principled stance towards high readability, maintainability and hackability.

**The goal of focusing on Model Encoder Evaluation:** By applying EEVEE to search for a pareto-optimal set of benchmarks, _and_ packaging it up in a unified framework that is built for the researcher in mind from the ground up, one which offers out of the box automated downloading, pipeline building, task adapters, and a very mature training and eval loop. Within this framework, we facilitate, all relevant logging information, including key training and eval metrics, rich gradient information, power and computational information, as well as visualizations where relevant. Finally, we support easy switching of model encoders, no matter what source modality they come from - our framework dubbed _GATE_ is a one stop shop for ones model representation research needs, both during research, debugging, as well as at the evaluation phase.

GATE comes in three tiers _small_, _base_ and _big_-GATE. Each having 8, 15 and 21 benchmarks within it, and targetted towards 12/24 and 36 GPU hours on a A100 40GB. We hope that by making it very easy for the end user and offering such rich signal for machine learning research, many researchers will choose to use GATE, to enhance their research signal, whilst keeping the compute budgets relatively feasible.

**Preparations: Choosing Models, Benchmarks and Adaptation Processes:** EEVEE will yield better results if the space of models, benchmarks and adaptation processes we use is diverse, but also thorough in numbers. **A. Adaptation Process** We wanted GATE to cover multiple domains, tasks and modalities when shifting from the source to the target setting. For that reason we decided that if a model encoder has an input layer that does not fit the target modality, we simply remove that input layer and replace it with a relevant ViT-like patchification  followed by a linear combination for each patch. For tasks where we have text, we would tokenize the text using BPE , and for tasks where we have video we would use the model encoder on each image, to acquire an image-level vector representation, and then follow that up with a simple 4 layer transformer that receives a sequence of image-vector tokens, to produce a video-level embedding, on top of which we apply the task-specific head at hand. The task-adapters we used leaned on established methods, and where possible we just used a transformer head, which includes segmentation, relational reasoning and video classification, with everything just using a linear head, full details available at 14. After these

Figure 1: (a) EEVEE Scoring algorithm, Mutation algorithm, and (b) Evolution algorithm.

Figure 2: GATE Framework Pipeline

modifications, described in Figure 2, we use a fine tuning scheme - this decision was informed by preliminary experiments on both full fine tuning and linear probe with a frozen backbone, in which we found that there was a clear superiority of fine tuning over linear probing for the benchmarks we chose in our pool. Full details of these preliminary experiments can be found in Appendix 8.1. In our preliminary experiments we were able to identify three recipes, one for ConvNet-style architectures, one for ViT-style architectures and one for Hybrid architectures such as ConvNext and ResNext that worked well for all tasks, details in 8.1.

**B. Model Pool** We wanted the space of models used to cover many important pretraining schemes, architectures, and source modalities. The details of these choices are provided next: **1. Pretraining Task and Dataset Variation**: With a consistent architecture, models were subjected to various pretraining tasks and datasets. Model instances representing this category include CLIPViT, ConvNextV2, Siglip, FlexViT, LainoViT, ImageNet1K ViT with Random Augment, SAM-ViT, DiNoViT, EfficientFormerV2 and DeiT3. Further to these, we include models initialized from scratch, specifically, ViT, ResNet50, FlexViT, EfficientNetV2, and then fine-tuned on the GATE tasks. **2. Architectural Variation**: We explored models having the same pretraining dataset (ImageNet), but differing in their architecture. This group encompassed a mix of standard CNN models such as EffNetV2, ResNet50, ResNext50, ConvNextV2_Base and transformer-based models like EfficientFormer and FlexViT. **3. Modality and Dataset Variation**: This axis comprised models trained on modalities other than vision such as whisper, coming from an audio to text task and Bert, Bart and Mpnet coming from various text-based tasks. These models had their original input processing systems replaced by a Vision Transformer style embedding and were subsequently fine-tuned on the GATE tasks. A more comprehensive account of these models, including their selection rationale and unique characteristics, is provided in the Appendix Section 13.

**C. Benchmark Pool** The benchmark pool, detailed in the Appendix, includes Image Classification (ImageNet1k, CIFAR100, Places365, Food101, HappyWhale), Few Shot Image Classification (Aircraft, Fungi, MiniImageNet, CUB200, Describable Features), Zero Shot Text-Image Classification (Flickr30K, New Yorker Caption Context, Winoground), Visual Relational Reasoning (CLEVR, CLEVRMath), Image Semantic Segmentation (ADE20K, COCO10K, COCO164K, NYU-Depth-v2, PascalContext, Cityscapes), Medical Image Classification (Chexpert, Diabetic Retinopathy, HAM10000), Medical Segmentation (ACDC), Video Classification (HMDB51, UCF-101, Kinetics400) and Video Regression (iWildcam).

**Producing Diverse Model Performance Metrics:** We apply our adaptation process on each and every model chosen, on every benchmark in the benchmark pool. To acquire test results we ensemble by averaging logits of the top 1, 3 and 5 validation models to produce three separate ensemble results.

**D. Experimental Approach** We wanted our research environment to reflect the end user, so we can properly understand their needs, and to offer a _pragmatic_ experimental setup of in-the-wild researchers with little time to hyperparameter optimize, and which have to make decisions on small amounts of preliminary experiments - someone choosing a model encoder off the shelf and adapting it to downstream setting. For that reason, we kept any hyperparameter tuning, or human attention when it came to specific models to a minimum. Instead, we relied on existing good recipes, and did some preliminary experiments as explained in detail in 8.1. Briefly, we discovered specific adjustments for each architecture type: for Convolutional Architectures, we used AdamW with a learning rate of 1e-3, and 6e-4 for segmentation tasks; for Vision Transformer Architectures, AdamW with a learning rate of 1e-5; and for Convolutional + Transformer Hybrid Architectures, AdamW with a learning rate of 2e-5. A plateau learning rate scheduler was configured with parameters like mode "min", factor 0.5, patience 1000, and threshold 1e-4, allowing models to effectively choose their own schedules based on their learning progress. This adaptive scheduling facilitated "good enough" learning rates and enhanced performance across different architectures.

## 4 Results

**Single Benchmark Predictiveness:** As demonstrated in Figure 3, using EEVEE we quantified the predictive power of each benchmark **on its own**, when not in a combination with others. We have found that ADE20K, Flickr30K, and the New York Caption Competition lead in their predictive power, with few-shot tasks, and relational reasoning, being very close to the best in predictive power. ImageNet1K sits squarely in the middle of the competition. Furthermore, some of the most "novel"benchmarks like iwildcam, happy whale, ACDC, NYU and Winoground are the least predictive tasks, Winoground being magnitudes less predictive. We argue that this is mainly due to the tasks being "harder", and our models being less designed for those. The results in WinoGround were bearly better than chance for example. However, when once we move to combinations of benchmarks, these 'less' predictive benchmarks become key contributors to better predictive power, as they represent edge cases, as can be seen in Figures 5(g) 5(c), 5(i), where these have the highest importance when removed from a given set.

**Predictiveness of Discovered Combinations** In Figure 5, we can see how the top-50 performing candidate combinations perform as we vary the number of benchmarks per combination from 1 to 26. We can see that there is a point of diminishing returns around the \(k=8\) point, after which there appears to be some "overfitting" occuring. We verified that the overfitting was a result of having a small sample number of 20 models, to train, val and test our meta-models with, as well as the 2-layer MLP we used to model Few-to-All metric predictions. We tried our level best to find the best architecture and regularization schemes for our meta-model, and this was the best we could do given available compute and (human) time. We chose 8, 15, and 21 as the combination-threshold to make our packs out of as they satisfied the computational budgets we set for ourselves, and they have very diverse and predictive tasks, as can be seen in Figures 5(g) 5(c), 5(i). For full details on all the discovered top-k combinations please look at Appendix Section 16.1. **Best Models based on GATE:** As can be seen in Table 2, or the Appendix extended Table 3, the best overall models are ConvNextV2, SigLIP and CLIP in that order, with SigLIP and CLIP often exchanging ranks between themselves. However, it is worth noting that EfficientNetV2 demonstrated exceptional performance/compute across all tasks, and even outperformed all models in many medical tasks. Finally, ConvNet based models, and particularly ResNext50 seem to have done exceptionally well in the edge-case scenarios of ACDC, Happy Whale Individual identification, and

Figure 4: Degradation of predictive power when a given benchmark is removed and the meta-model trained from scratch, for different GATE tiers.

Figure 5: Performance of Models build with K-best datasets: We do a search over the space of all \(k\) for EEVEE and box plot the population summary statistics of the top 50 combination candidates.

Figure 3: The EEVEE MSE Loss (k=1) shows "predictiveness over the whole," with lower values being better. Benchmarks like iWildcam, HappyWhale, and WinoGround test unique capabilities and may not predict all tasks, yet EEVEE often includes at least two of these in its top combinations along with a “natural-image representative" such as CIFAR100, ADE20K or Flickr30K.

[MISSING_PAGE_FAIL:9]