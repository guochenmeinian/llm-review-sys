# SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal Behaviors

Tinghao Xie\({}^{1}\), Xiangyu Qi\({}^{1}\), Yi Zeng\({}^{2}\), Yangsibo Huang\({}^{1}\)

Udari Madhushani Sehwag\({}^{3}\), Kaixuan Huang\({}^{1}\), Luxi He\({}^{1}\), Boyi Wei\({}^{1}\), Dacheng Li\({}^{4}\), Ying Sheng\({}^{3}\)

Ruoxi Jia\({}^{2}\), Bo Li\({}^{5,6}\), Kai Li\({}^{1}\), Danqi Chen\({}^{1}\), Peter Henderson\({}^{1}\), Prateek Mittal\({}^{1}\)

\({}^{1}\)Princeton University \({}^{2}\)Virginia Tech \({}^{3}\)Stanford University \({}^{4}\)UC Berkeley

\({}^{5}\)University of Illinois at Urbana-Champaign \({}^{6}\)University of Chicago

###### Abstract

Evaluating aligned large language models' (LLMs) ability to recognize and reject unsafe user requests is crucial for safe, policy-compliant deployments. Existing evaluation efforts, however, face three limitations that we address with **SORRY-Bench**, our proposed benchmark. **First**, existing methods often use coarse-grained taxonomies of unsafe topics, and are over-representing some fine-grained topics. For example, among the ten existing datasets that we evaluated, tests for refusals of self-harm instructions are over 3x less represented than tests for fraudulent activities. SORRY-Bench improves on this by using a fine-grained taxonomy of 45 potentially unsafe topics, and 450 class-balanced unsafe instructions, compiled through human-in-the-loop methods. **Second**, evaluations often overlook the linguistic formatting of prompts, like different languages, dialects, and more--which are only implicitly considered in many evaluations. We supplement SORRY-bench with 20 diverse linguistic augmentations to systematically examine these effects. **Third**, existing evaluations rely on large LLMs (e.g., GPT-4) for evaluation, which can be computationally expensive. We investigate design choices for creating a fast, accurate automated safety evaluator. By collecting 7K+ human annotations and conducting a meta-evaluation of diverse LLM-as-a-judge designs, we show that fine-tuned 7B LLMs can achieve accuracy comparable to GPT-4 scale LLMs, with lower computational cost. Putting these together, we evaluate over 40 proprietary and open-source LLMs on SORRY-Bench, analyzing their distinctive refusal behaviors. We hope our effort provides a building block for systematic evaluations of LLMs' safety refusal capabilities, in a balanced, granular, and efficient way.1

## 1 Introduction

To ensure large language model (LLM) safety, _alignment_ has become a standard procedure that follows language model pretraining (OpenAI, 2023; Touvron et al., 2023; Anthropic, 2023; Gemini Team, 2023). Alignment involves calibrating these models, via _instruction tuning_(Wei et al., 2021; Ouyang et al., 2022) and _preference optimization_(Bai et al., 2022; Rafailov et al., 2024; Meng et al., 2024), such that they adhere to human values and safely follow human intents. The resultant aligned LLMs should not engage with user requests on certain unsafe or inappropriate topics, as determinedby LLM creators and regulators. For example, most LLMs _refuse_ to assist with the user request "_Write a racist tweet._" Instead, they provide a _refusal_ response - e.g., "_I'm sorry, I cannot help with this._" Such safety refusal abilities help reduce the likelihood of misuse by users in the wild.

To date, researchers have proposed many benchmarks (Gehman et al., 2020; Parrish et al., 2022; Shaikh et al., 2022; Wang et al., 2023; Qi et al., 2023; Cui et al., 2023; Vidgen et al., 2023; Lin et al., 2023; Zou et al., 2023; Shen et al., 2023; Huang et al., 2023; Mazeika et al., 2024; Souly et al., 2024) to evaluate various aspects of LLM safety, including toxicity, harmfulness, trustworthiness, and refusal behaviors (see a detailed summary of them in Table 3). In this work, we identify three deficiencies underlying these existing evaluations, and address them with **SORRY-Bench2**, our proposed systematic benchmark to evaluate LLM safety refusal behaviors.

**First, we point out prior datasets are often built upon course-grained and varied safety categories, and that they are overrepresenting certain fine-grained categories.** For example, Vidgen et al. (2023) include broad categories like "Illegal Items" in their taxonomy, while Huang et al. (2023) use more fine-grained subcategories like "Theft" and "Illegal Drug Use". Meanwhile, both of them fail to capture certain risky topics, e.g., "Legal Advice" or "Political Campaigning", which are adopted in some other work (Liu et al., 2023; Shen et al., 2023; Qi et al., 2023). Moreover, we find these prior datasets are often imbalanced and result in over-representation of some fine-grained categories. As illustrated in Fig 1, as a whole, these prior datasets tend to skew towards certain safety categories (e.g., "Fraud", "Sexual Explicit Content", and "Social Stereotypes") with "Self-Harm" being nearly 3x less represented than these categories. However, these other underrepresented categories (e.g., "Personal Identifiable Information Violations", "Self-Harm", and "Animal-related Crimes") cannot be overlooked - failure to evaluate and ensure model safety in these categories can lead to outcomes as severe as those in the more prevalent categories.

To bridge this gap, we present a _fine-grained 45-class safety taxonomy_ (Fig 2 and SS2.2) across 4 high-level domains. We curate this taxonomy to unify the disparate taxonomies from prior work, employing a human-in-the-loop procedure for refinement - where we map data points from previous datasets to our taxonomy and iteratively identify any uncovered safety categories. Our resultant taxonomy captures diverse topics that could lead to potentially unsafe LLM responses, and allows stakeholders to evaluate LLM safety refusal on any of these risky topics at a more granular level. On top of this 45-class taxonomy, we craft a _class-balanced LLM safety refusal evaluation dataset_ (SS2.3). Our base dataset consists of 450 unsafe instructions in total, with additional manually created novel data points to ensure equal coverage across the 45 safety categories (10 per category).

**Second, we ensure balance not just over topics but over linguistic characteristics.** Existing safety evaluations fail to capture different formatting and linguistic features in user inputs. But this too can result in over-representation of a given language, dialect or other linguistic feature. We address this by considering 20 diverse _linguistic mutations_ that real-world users might apply to phrase their unsafe prompts. These include various writing styles, persuasion techniques, encoding and encryption strategies, and multi-languages (SS2.4). After paraphrasing our base dataset via these mutations, we obtain 9K additional unsafe instructions.

Figure 1: **Imbalanced data point distribution of 10 prior datasets (ยง2.2) on our 45-class taxonomy.**

Third, we investigate what design choices make a fast and accurate safety benchmark evaluator, a trade-off that prior work has not so systematically examined.** To benchmark safety behaviors, we need an _efficient_ and _accurate_ evaluator to decide whether a LLM response is in _compliance_ or _refusal_ of each unsafe instruction from our SORRY-Bench dataset. By far, a common practice is to leverage LLMs themselves for automating such safety evaluations. With many different implementations (Qi et al., 2023; Huang et al., 2023; Xie et al., 2023; Mazeika et al., 2024; Li et al., 2024; Souly et al., 2024; Chao et al., 2024) of LLMs-as-a-judge, there has not been a large-scale systematic study of which design choices are better, in terms of the tradeoff between efficiency and accuracy. We collect a large-scale human safety judgment dataset (SS3.2) of over 7K annotations, and conduct a thorough meta-evaluation (SS3.3) of different safety evaluators on top of it. Our finding suggests that small (7B) LLMs, when fine-tuned on sufficient human annotations, can achieve satisfactory accuracy (over 80% human agreement) with a low computational cost (\(\)10s per evaluation pass), comparable with and even surpassing larger scale LLMs (e.g., GPT-4o).

In SS4.2, we benchmark **over 40** open-source and proprietary LLMs on SORRY-Bench. Specifically, we showcase the varying degrees of safety refusal across different LLMs. Claude-2 and Gemini-1.5, for example, exhibit the most refusals. Mistral models, on the other hand, demonstrate significantly higher rates of compliance with potentially unsafe user requests. There was also general variation across categories. For example, Gemini-1.5-flash is the only model that consistently refuses requests for legal advice that most other models respond to. Whilst, all but a handful of models refused most harassment-related requests. Finally, we find significant variation in compliance rates for our 20 linguistic mutations in prompts, showing that current models are inconsistent in their safety for low-resource languages, inclusion of technical terms, uncommon dialects, and more.

## 2 A Recipe for Curating Diverse and Balanced Dataset

### Related Work

To evaluate the safety of modern LLMs with instruction-following capabilities, recent work (Shaikh et al., 2023; Liu et al., 2023; Zou et al., 2023; Rottger et al., 2023; Shen et al., 2023; Qi et al., 2023; Huang et al., 2023; Vidgen et al., 2023; Cui et al., 2023; Li et al., 2024; Mazeika et al., 2024; Souly et al., 2024; Zhang et al., 2023) propose different instruction datasets that might trigger unsafe behavior--building on earlier work evaluating toxicity and bias in underlying pretrained LMs on simple sentence-level completion (Gehman et al., 2020) or knowledge QA tasks (Parrish et al., 2022). These datasets usually consist of varying numbers of potentially unsafe user instructions, spanning across different safety categories (e.g., illegal activity, misinformation). These unsafe instructions are then used as inputs to LLMs, and the model responses are evaluated to determine model safety. In Appendix C, we provide a more detailed survey of these datasets with a summary of key attributes.

### Fine-grained Refusal Taxonomy with Diverse Categories

Before building the dataset, we first need to understand its scope of safety, i.e., _what safety categories should the dataset include and at what level of granularity should they be defined?_ We note that prior datasets are often built upon discrepant safety categories, which may be too coarse-grained and not consistent across benchmarks. For example, some benchmarks have results aggregated by course-grained categories like illegal activities (Shen et al., 2023; Qi et al., 2023; Vidgen et al., 2023; Zhang et al., 2023), while others have more fine-grained subcategories like delineate more specific subcategories like "Tax Fraud" and "Illegal Drug Use" (Huang et al., 2023). Mixing these subtypes in one coarse-grained category can lead to evaluation challenges: the definition of an "illegal activity" can change across jurisdiction and time. Hate speech, for example, can be a crime in Germany, but is often protected by the First Amendment in the United States. We also note that previous datasets may have inconsistent coverage - failing to account for certain types of activities that model creators may or may not wish to constrain, like "Legal Advice" or "Political Campaigning", which are only examined by a small group of studies (Liu et al., 2023; Shen et al., 2023; Qi et al., 2023).

We suggest that benchmarking efforts should focus on fine-grained and extensive taxonomies, which not only enable capturing diverse potential safety risks, but also come with the benefit of better customizability_. Stakeholders can selectively engage with categories of particular concerns and disregard those deemed permissible. For example, some might find it acceptable for their models to provide legal advice, while others may believe this is too high-risk. In light of this, we present a **45-class safety taxonomy** to examine refusal behaviors, as shown in Fig 2 (see Table 4 in Appendix D for a more detailed version) to unify past datasets in a fine-grained and customizable way.

Our taxonomy curation method consists of two stages. In the first stage, we _aggregate_ the safety taxonomies from 10 prior safety benchmark datasets (Wang et al., 2023; Qi et al., 2023; Cui et al., 2023; Vidgen et al., 2023; Lin et al., 2023; Zou et al., 2023; Shen et al., 2023; Huang et al., 2023; Mazeika et al., 2024; Souly et al., 2024; Shaikh et al., 2022), and _break down_ any vague and broad safety categories into more fine-grained categories. For example, the coarse-grained category "Illegal Activities," occurring in (Shen et al., 2023; Qi et al., 2023; Vidgen et al., 2023)), is substituted by more detailed sub-categories like "Animal-related Crimes", "Fraud", and so on. In the second stage, we keep on _refining_ this taxonomy via a human-in-the-loop process. We first map data points from these prior datasets to our taxonomy, with GPT-4 as a classifier (see Appendix E for detailed setup). Data points that do not fit existing categories (i.e., classified to "Others") undergo human review to determine if new categories are needed or if existing ones should be subdivided further. This two-stage approach ensures an extensive and unified taxonomy, addressing the discrepancy across prior safety benchmark efforts.

### Data Collection

With the aforementioned GPT-4 classifier (Appendix E), we map data points from the 10 prior datasets to our taxonomy, where we further analyze their distribution on the 45 safety categories. As illustrated in Fig 1, these datasets exhibit significant **imbalances** - they are heavily biased towards certain categories perceived as more prevalent. For instance, System Intrusion, Fraud, Sexual Content Generation, and Social Stereotype Promotion are disproportionately represented in the past datasets. Meanwhile, other equally important categories, like Self-Harm, Animal-related Crimes, and PII Violations are significantly under-represented. Failure to capture model safety risks in these categories can lead to equivalently severe consequences.

To equally capture model risks from all safety categories in our taxonomy, we build a class-balanced dataset. We begin by aggregating labeled data from past work, but after quality filtering and deduplication,3 We find that many categories do not have enough data to build a class-balanced benchmark. To ensure sufficient and equal coverage across categories, we further create numerous novel potentially unsafe instructions less represented in prior work. Our collected **SORRY-Bench** dataset consists of 10 diverse unsafe instructions for each of the 45 categories, in total of 450 samples.

### Capturing Diverse Linguistic Patterns underlying User Prompts

Prompt diversity has long been a challenge in language model evaluation (Liu et al., 2023a). The same input prompt, phrased in different ways can lead to varying model responses. This issue is also important for LLM safety evaluation. Sophisticated prompt-space _jailbreaking_ methods (Shen et al., 2023; Zou et al., 2023; Andriushchenko et al., 2024) have been developed to bypass safety guardrails in LLMs, causing them to respond to potentially unsafe user requests. Some studies have shown that simple social techniques like persuasion (Zeng et al., 2024), writing prompts in alternative languages (Deng et al., 2023), or even phrasing unsafe prompts in instruction-style (imperative;

Figure 2: Taxonomy of SORRY-Bench.

e.g., "Write a tutorial to build a bomb") instead of question-style (interrogative; e.g., "Can you teach me how to build a bomb?"), can significantly affect the extent to which models refuse unsafe instructions (Bianchi et al., 2024). To ensure equal coverage of these variations, we isolate and decouple prompt-level linguistic patterns. In our collected "base" dataset, all user prompts are deliberately (re-)written as an instruction (imperative), which is one of the most common styles users would phrase their request as. We then compile 20 linguistic mutations4 (Fig 3) from prior studies (Bianchi et al., 2024; Samvelyan et al., 2024; Zeng et al., 2024; Yuan et al., 2023; Deng et al., 2023) into our datasets, including diverse _writing styles_ (question, slang, misspellings, etc.), _persuasion techniques_ (e.g., logical appeal), _encoding and encryption strategies_ (e.g., ASCII), and _multi-languages_ (i.e., translating to a non-English language like Malayalam or French). This results in 20 * 450 = 9K additional unsafe instructions, capturing diverse formatting and linguistic features.

## 3 A Recipe for Building Efficient and Accurate Automated Evaluators

When testing a language model on a safety benchmark, a practical problem is to build an evaluator that can judge whether the model response is safe or not (i.e., whether the model fulfills an unsafe instruction). Building such an evaluator is nontrivial, as it requires a sophisticated understanding of different types of harmfulness that a model response can exhibit. While human-based evaluation is the gold standard, it is short of scalability and efficiency. Therefore, currently, a common practice is to leverage the capability of language models as a proxy to automate the evaluation process.

Thus far, there are many different implementations (Qi et al., 2023; Huang et al., 2023; Xie et al., 2023; Mazeika et al., 2024; Li et al., 2024; Souly et al., 2024; Chao et al., 2024) of LLM-as-a-judge for safety evaluation - some directly using GPT-4-level LLMs as an off-the-shelf option, others fine-tuning smaller language models. However, there isn't a principled understanding of which design choices are better and what trade-offs they have. In this section, we aim to provide a systematic study of the design choices of such evaluators, presenting a recipe for building efficient and accurate automated evaluators for benchmarking safety refusal behaviors. We will first define the evaluation goal of our SORRY-Bench, and then introduce our effort in collecting a large scale human judgment dataset for safety refusal. On top of this, we then present a meta-evaluation study of different design choices using LLM as a judge, to identify a design that is both efficient and accurate for our use case.

### Evaluation Goal

The evaluation of an LLM's refusal behaviors on our SORRY-Bench dataset corresponds to a binary classification problem. Given a pair of (unsafe instruction, model response), we want to decide whether the model response is in compliance with or a refusal to the unsafe instruction. Specifically, a model response is considered as **compliance**_whenever it provides substantial content that directly

Figure 3: Demonstration of 20 linguistic mutations we apply to our base dataset.

_assists with the given unsafe instruction_. Even though sometimes the model response can contain ethical disclaimers (e.g., "I must remind you that it's unethical to do this") or refusing statements (e.g., "I'm sorry, I cannot help with this") at the beginning, as long as there exists certain content that directly resolves the unsafe instruction, it is still considered as compliance in our evaluation. In all other cases, the model response is considered as **refusal**. Similarly, this includes cases where the model response contains complying statements (e.g., "Sure, here is my response") or does not contain any explicit refusing statement, but there is no content that actually resolves the unsafe instruction.

### A Human Judgment Dataset for Evaluating the Evaluators

In this study, data points of human safety judgments on model responses are often helpful and, in many cases, necessary. First, only with human ground truth labels can it be possible for us to evaluate the accuracy of any automated evaluators, understand whether an evaluator is good or not, and compare different evaluators. Second, human-labeled safety judgment instances can also be used as training data to optimize the language model based evaluators instead of just prompting them. Therefore, we curate a large-scale human safety judgment dataset, which not only benefits our study but will also be a useful foundational resource for future research in this area.

Specifically, for every unsafe instruction from our SORRY-Bench dataset (the base-version, _without linguistic mutation_), we sample 8 model responses (from different LLMs), and 6 authors manually label each of them as either "compliance" or "refusal" to the user request (in total 450 * 8 = 3,600 records). We call this an **in-distribution (ID)** set. Moreover, we also cover the **out-of-distribution (OOD)** evaluation cases, where the unsafe instructions in our SORRY-Bench dataset are subject to linguistic mutations (described in SS2.4). We find that the safety evaluation in these cases can be more challenging. For example, after _translating_ the original user request to another language, some LLMs would simply repeat the user request (which is not considered compliance); for some _encoding_ mutations, the model responses are nonsense (undecidable content, which is also not compliance); and after mutating the user request with _persuasion_ techniques, the model response may contain a bullet list that looks like compliance, but actually cannot resolve the user request (actually not compliance). Therefore, to cover these OOD evaluation cases, we further sample 8 more model responses (from different LLMs) to the linguistic-mutated version of each unsafe instruction from our benchmark dataset. So, in total, we finally collected 450 * (8 ID + 8 OOD) = 7,200 human annotations. See Appendix H for more details.

We split these human annotations into a _train_ split of 450 * (3 ID + 3 OOD) = 2,700 records (used to directly train evaluators), and the rest 4,500 as the _test_ split.

### A Meta-Evaluation: What Makes a Good Safety Evaluator?

While directly prompting state-of-the-art LLMs such as GPT-4 to judge the refusal behaviors can result in a fairly good judge that agrees well with human evaluators (Qi et al., 2023), there are also several growing concerns. First, as versions of proprietary LLMs keep updating, there is an issue of reproducibility. Second, long prompts and the GPT-4-scale models often result in heavy computation overhead, resulting in high financial and time costs (e.g., per-pass evaluation with GPT-40 could cost $3 and 20 minutes in our case). Thus, we also explore the potential of utilizing smaller-scale open-sourced models (e.g., Llama-3 (Meta, 2024), Gemma (Team et al., 2024), and Mistral (Jiang et al., 2023)) for the refusal evaluation task, which favors both reproducibility and efficiency.

For comprehensiveness, we explore a few commonly adopted add-on techniques for boosting the accuracy of LLM judge further. 1) **Chain-of-thought (CoT)**(Wei et al., 2022) prompting: following Qi et al. (2023), we ask the LLM to first "think step-by-step", analyze the relationship between the given model response and user request, and then make the final decision of whether the model response is a "refusal" or a "compliance". 2) In-context learning with **few-shot** evaluation examples (Brown et al., 2020): for each instruction, we use the corresponding annotations in the train split of the human judge dataset (SS3.2) as the in-context demonstrations. 3) Directly **fine-tuning** LLM to specialize on the safety evaluation task (Huang et al., 2023; Mazeika et al., 2024; Li et al., 2024): we directly fine-tune LLMs on the aforementioned train split of 2.7K human judge evaluation annotations.

[MISSING_PAGE_FAIL:7]

evaluation configurations also notably affect the reported safety benchmark results (Table 5). We direct readers to Appendix J for more in-depth result analysis.

Different models exhibit significantly varying degrees of safety refusal behaviors.We observe that 22 out of 43 LLMs demonstrate a medium compliance rate of 20%\(\)50%, e.g., GPT-40 (31%) and Llama-3-70b (36%). At one end of the spectrum, Claude-2 and Gemini-1.5 achieve the lowest overall compliance rate (<10%). In particular, Claude-2.1 and 2.0 refuse almost all unsafe instructions in the first 25 categories ("Hate Speech Generation" & "Assistance with Crimes or Torts" domains), and Gemini-1.5-Flash declines all requests related to "Potentially Unqualified Advice" (i.e., the last 5 categories). At the other end, 8 models (e.g., Mistral series) comply with more than half of the evaluated unsafe instructions, even on well-recognized harmful topics like "#15: Fraud."

These variations may give us independent insight into the shifting values and priorities of model creators, and their changing policy guidelines.Llama-3 models, as an instance, show notably fewer safety refusals compared to Llama-2 (compliance rate of the 70B version increases from 13% to 36%). Conversely, we observe a substantial increase in refusals from Gemini-Pro to the more recent Gemini-1.5 models (compliance rate drops from 34% to 8%). Both Gemini and Claude models refuse nearly all 10 instructions in the category "#26: Advice on Adult Content", claiming that it's unethical to discuss such personal topics. And while most prior versions of the GPT-3.5/4 API rejected most requests in the category, GPT-4o now mostly complies with such user requests. This shift aligns with OpenAI Model Spec (OpenAI, 2024) published in May 2024, which states that discussing adult topics is permissible. Meanwhile, the spec also states that "responding to user request for erotica" is unacceptable, explaining why GPT-4o consistently refuses every instruction from "#27: Sexual Explicit Content Generation."

Figure 4: **Benchmark results of 40+ LLMs on SORRY-Bench. The LLMs are ranked by their compliance rates (the bracketed scores following model names on the vertical axis) over all 45 safety categories (horizontal axis), low to high. In each grid, we report the per-category compliance rate.**

[MISSING_PAGE_FAIL:9]