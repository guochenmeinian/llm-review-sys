# AverTeC: A Dataset for Real-world Claim Verification with Evidence from the Web

Michael Schlichtkrull1, Zhijiang Guo1, Andreas Vlachos

Department of Computer Science and Technology, University of Cambridge

{mss84,zg283,av308}@cam.ac.uk

###### Abstract

Existing datasets for automated fact-checking have substantial limitations, such as relying on artificial claims, lacking annotations for evidence and intermediate reasoning, or including evidence published after the claim. In this paper, we introduce AVeriTeC, a new dataset of 4,568 real-world claims covering fact-checks by 50 different organizations. Each claim is annotated with question-answer pairs supported by evidence available online, as well as textual justifications explaining how the evidence combines to produce a verdict. Through a multi-round annotation process, we avoid common pitfalls including context dependence, evidence insufficiency, and temporal leakage, and reach a substantial inter-annotator agreement of \(=0.619\) on verdicts. We develop a baseline as well as an evaluation scheme for verifying claims through question-answering against the open web.

## 1 Introduction

Fact-checking is considered crucial for limiting the impact of misinformation . Unfortunately, not enough resources are available for manual fact-checking. Automated fact-checking (AFC) has been proposed as an assistive tool for fact-checkers, moderators, and citizen journalists to facilitate it , inspiring applications in journalism  and other domains, e.g. science .

Substantial progress has been made on common benchmarks, such as FEVER  and MultiFC . Nevertheless, existing resources have recently come under criticism. Many datasets (for example, , ) contain purpose-made claims derived from sources such as Wikipedia, and are thus unlike real-world claims checked by journalists. Further, in these datasets, _refuted_ claims are produced by corrupting existing sentences. Datasets that do contain real-world claims either lack evidence annotation , or annotate it superficially using automated means, resulting in issues such as including evidence published days or weeks _after_ the investigated claims .

To address these limitations we introduce AVeriTeC (Automated VERIfication of TExtual Claims), which combines real-world claims with realistic evidence retrieved from the web, as well as justifications for veracity labels. We formulate retrieval as question generation and answering, providing a structured representation of the evidence and reasoning supporting or refuting the claim. The free-text justifications detail how the evidence is used to reach the verdict, including cases of conflicting evidence, matching best practises for human fact-checkers . In constructing AVeriTeC, we ameliorate three issues afflicting existing datasets with real-world claims:

1. **Context Dependence:**: Ousidhoum et al.  found that claims in some datasets based on fact-checking articles (e.g., ) cannot be verified without additionalinformation from the articles they were extracted from. This could for example be due to unresolved coreference or ellipsis, e.g. in "unemployment is rising" it is unclear which geographical/temporal context is being considered.
2. **Evidence Insufficiency:**Glockner et al. (2022) found that labels in some datasets (e.g., Hanselowski et al. (2019)) often do not match the annotated evidence, because they rely on e.g., assumptions about the speaker of the claim. This is significantly different from _not enough evidence_ verdicts, which is a label for claims where evidence could not be found.
3. **Temporal Leaks:**Glockner et al. (2022) found that annotations in some datasets (e.g., Augenstein et al. (2019)) contain leaks from the future. For example, a claim from January might be annotated with evidence from March that year. Leaks can also happen between splits, if e.g., evidence for a training claim from 2021 also pertains to a test claim from 2020.

We address (1) through an initial normalisation step, where annotators enrich claims with necessary information from the fact-checking article. We verify the adequacy of this, and address (2), by combining multiple rounds of annotation with a "blind" quality control step, re-annotating any claims with insufficient evidence. We address (3) by restricting annotators to evidence documents published before the claim, and by ordering our training, development, and test splits temporally. With the increasing reliance on large pretrained language models, temporal ordering provides an additional benefit: if the training data for the language model is cut off before the temporal start of the test set, leaks from pretraining cannot occur either.

AVeriTeC consists of 4,568 examples, collected from 50 fact-checking organizations using the Google FactCheck Claim Search API2; itself based on ClaimReview3. Our annotation, which involved up to five annotators per claim, resulted in substantial inter-annotator agreement, with a free-marginal \(\) of \(0.619\)(Randolph, 2005). We further develop a baseline to explore the feasibility of the task, relying on Google Search, BM25(Robertson and Zaragoza, 2009), retrieved in-context prompts (Liu et al., 2022; Rubin et al., 2022), and a trained stance detection model. AVeriTeC is the first AFC dataset to provide both question-answer decomposition and justifications, as well as avoid issues of context dependence, evidence insufficiency, and temporal leaks. Our dataset and baseline are available under a CC-BY-NC-4.0 license at [https://github.com/MichSchli/AVeriTeC](https://github.com/MichSchli/AVeriTeC).

## 2 Related Work

Sourcing real-world claims from fact-checking articles is popular (e.g. Wang (2017)), as extracting claims from fact-checkers guarantees _checkworthiness_. That is, any claim included in the resulting dataset is deemed interesting enough to be worth the time of a professional journalist (see Hassan et al. (2015)). Previous real-world datasets either lack annotations for evidence, or suffer from context dependence, evidence insufficiency, or temporal leaks. Further, they do not provide annotations for intermediate steps, and only a minority (Alhindi et al. (2018); Kotonya and Toni (2020)) provide justifications. A comparison between AVeriTeC and prior datasets can be seen in Table 1.

Figure 1: Diagram for our annotation process. Claims are first selected and normalized. Then, two rounds of question-answer pair generation and evidence sufficiency check ensure high-quality evidence annotation.

Beyond evidence insufficiency and temporal leakage, Glockner et al. (2022) also found that many examples require a _source guarantee_ to refute, i.e. a guarantee that the claimant's underlying reason for making the claim is known to the debunker. For example, evidence against the claim _"COVID-19 vaccines may kill sharks"_ can only be found when incorporating the underlying reasoning of the claimant, that the manufacturing of COVID-19 vaccines requires a chemical extracted from sharks. We do not explicitly provide such a guarantee; however, as each claim in AVeriTeC is annotated with the original claimant, these underlying reasons can be recovered through question-answer pairs.

Question-answer decomposition is considered a promising strategy; Yang et al. (2022) proposed such a model even without a dataset of annotated question-answer pairs. Two recent datasets cast fact-checking as question-answering: Fan et al. (2020) and Chen et al. (2022). However, Fan et al.'s (2020) question-answer pairs were only written to add relevant context, not to capture entire the fact-checking process, and thus lack evidence sufficiency. Ousidhoum et al. (2022) furthermore identified context dependence as a significant concern in Fan et al. (2020): many questions are impossible to generate given only the claim, as they refer to entities and events only mentioned in the original fact-checking article. Chen et al. (2022) did - like us - attempt to ensure evidence sufficiency. However, they take no steps to verify their success. Furthermore, their evidence is taken directly from the fact-checking articles which are written after the claim, thus exhibiting temporal leakage.

## 3 Annotation Structure

Our dataset consists of 4,568 real-world claims annotated with question-answer pairs representing the evidence, a veracity label, and a textual justification describing how the evidence supports the label. An example can be seen in Figure 2.

Reasoning about evidence is represented through questions and answers. Questions may have multiple answers, a natural way to show potential disagreements in the evidence. Questions can refer to previous questions, allowing for multi-hop reasoning. Answers (other than _"No answer could be found."_) must be supported by a _source url_ linking to a web document. To avoid sources disappearing from the web, we cache all pages used as evidence in the internet archive4.

Claims in AFC datasets are typically _supported_ or _refuted_ by evidence, or there is _not enough evidence_. We add a fourth class: _conflicting evidence/cherry-picking_. This covers both conflicting evidence, and

  
**Dataset** &  &  \\  & _Type_ & _Independence_ & _Sufficient_ & _Unleaded_ & _Retrieved_ \\  FEVER (Thorne et al., 2018) & Wikipedia & Synthetic & ✓ & ✓ & N/A & ✓ \\ VitaminC (Schuster et al., 2021) & Wikipedia & Synthetic & ✓ & ✓ & N/A & ✓ \\ FEVEROUS (Aly et al., 2021) & Wikipedia & Synthetic & ✓ & ✓ & N/A & ✓ \\ SciFact (Wadden et al., 2020) & Science & Synthetic & ✓ & ✓ & N/A & ✓ \\ FDA (Saskyan et al., 2021) & Game & Synthetic & ✓ & ✓ & N/A & ✓ \\ Covid-Fact (Saskyan et al., 2021) & Reddit & Synthetic & ✓ & ✓ & N/A & ✓ \\  Líar-Plus (Alhindi et al., 2018) & Factcheck & Real & ✗ & ✓ & ✗ & ✗ \\ Politifp (Ostrowski et al., 2021) & Factcheck & Real & ✗ & ✓ & ✗ & ✗ \\ MultiHC (Augenstein et al., 2019) & Factcheck & Real & ✗ & ✗ & ✗ & ✓ \\ XFact (Gupta and Srikumar, 2021) & Factcheck & Real & ✗ & ✗ & ✓ & ✓ \\ PubMedink (Kotova and Toni, 2020) & Factcheck & Real & ✗ & ✗ & ✓ & ✗ \\ WadzimeCheck (Khan et al., 2022) & Factcheck & Real & ✗ & ✗ & ✓ & ✗ \\ CiamlECOomp (Chen et al., 2022) & Factcheck & Real & ✗ & ✗ & ✓ & ✗ \\ Snoeps (Hanselowski et al., 2019) & Factcheck & Real & ✗ & ✗ & ✓ & ✗ \\ QABief (Fan et al., 2020) & Factcheck & Real & ✗ & ✗ & ✓ & ✗ \\ ClimateFEVER (Diggelmann et al., 2020) & Web & Real & ✗ & ✗ & ✓ & ✓ \\ HealthVer (Sarrouiti et al., 2021) & Web & Real & ✗ & ✗ & ✓ & ✓ \\ CHEF (Hu et al., 2022) & Factcheck & Real & ✗ & ✓ & ✗ & ✓ \\  AVeriTeC & Factcheck & Real & ✓ & ✓ & ✓ & ✓ \\   

Table 1: Comparison of fact-checking datasets. _Source_ indicates where the claims are collected from, such as Wikipedia, or fact-checking articles (Factcheck). _Type_ indicates whether the claims are synthetic or real-world. _Independence_ indicates whether the claim is context independent. _Sufficient_ indicates whether the evidence can provide sufficient information. _Unleaked_ means whether the evidence contains leaks from the future and _retrieved_ denotes whether the dataset involves evidence retrieval instead of relying on pre-retrieved passages e.g. the fact-checking article.

technically true claims that mislead by excluding important context. For real-world claims, sources may interpret events differently, and therefore legitimately disagree. This differs from Schuster et al. (2021), which studies claims for which the evidence has been revised. Adding a fourth class has also recently been discussed in the context of natural language inference (Jiang and Marneffe, 2022), although there it is usually ambiguity in the premise or hypothesis leading to the conflict.

AverTeC also provides textual justifications that explain how verdicts are reached from the evidence. Where sources disagree, best practices for established fact-checkers is to provide a textual explanation of _why_ the claim misleads (Uscinski and Butler, 2013; Amazeen, 2015). These justifications can be _substantial_(Toulmin, 1958), i.e. they may introduce logical leaps supported by commonsense or inductive reasoning beyond the retrieved evidence. For example, if a claim states that 50% of a population group were vaccinated by February 1st, and evidence shows only 33% had been vaccinated by January 31st, the justification may reason that a 50% rate one day later is unlikely.

We include several fields of metadata: the _speaker_ of the claim, the _publisher_ of the claim, the _date_ the claim was published, and the _location_ most relevant to the claim. These can be used to support questions, answers, and justifications. We also annotate the _claim type_ and _fact-checking strategy_ of each claim. Type represents common aspects, e.g., whether claims are about numerical facts; strategy represents the approach of the fact-checkers, e.g., whether they relied on expert testimony. Types and strategies should not be used as input to models (at inference time), but can provide useful data for analysis.

## 4 Annotation Process

Starting from 8,000 fact-checking articles, we first identified and discarded 537 duplicates and 802 paywalled or dead articles. We passed the remainder through a five-phase pipeline - see Figure 1. First, an annotator extracts claims and relevant metadata from each article, providing context independence. Second, an annotator generates questions and answers them using the web. These annotators also choose a temporary verdict. Third, a different annotator provides a justification and a verdict based _solely_ on the annotated question-answer pairs; this serves as an evidence sufficiency check. Any claim for which the two verdicts do not match is passed through the last two phases again. If the verdicts still disagree, the claim is discarded. Different annotators were used for each claim in each phase - i.e., no annotator saw the same claim twice. Our annotation was performed by the company Appen5; details and annotation guidelines can be found in Appendices C and J.

Claim Extraction & NormalisationAnnotators extracted the central claims from each fact-checking article, enriching them with the necessary context. This is necessary as many fact-checking articles cover multiple claims (e.g., several rumours circulating about an event). Further, some claims lack adequate contextualization (Ousidhoum et al., 2022). For example, the claim "_we have 21 million unemployed_" requires coreference resolution. After claim extraction, we discarded _speculative_ claims,

Figure 2: Example claim from AVeriTeC. As opposed to previous datasets, ours naturally combines question-answer pairs that break down the evidence retrieval with justifications that show how evidence leads to verdicts.

i.e. unverifiable statements about future events or personal opinions, and _multimodal_ claims, i.e. claims where the type or the strategy inherently involves modalities beyond text.

Question Generation & AnsweringAnnotators generate questions and answer them providing evidence about the claim. The aim is to "deconstruct" the reasoning of the fact-checker into a QA-structure, extracting question-answer pairs that match the content of their enquiries. This makes the process better amenable to annotators who are not trained journalists, and provides structured representations for model development and evaluation. Each question must be accompanied by an answer (or marked _unanswerable_), and answers must be backed by sources. We advise annotators that extractive answers are preferred, but abstractive answers are also allowed. Annotators in this phase are asked to provide a verdict based on their retrieved evidence, possibly different from the one in the fact-checking article.

As annotators follow the fact-checking articles, the ideal evidence sources for answer are the documents linked from the articles. For answers are not found in the linked documents, we provide access to a custom Google search bar. This search bar is restricted to show documents published only _before_ the claim date, in contrast to prior work (Fan et al., 2020). Furthermore, unlike Alhindi et al. (2018) and Hanselowski et al. (2019), the fact-checking article itself cannot be used as evidence.

We note that, where annotators could not find the publication date of the _claim_, their instructions were to use the publication date of the _fact-checking article_ instead. As the process of fact-checking can take journalists several days, there is a window in which news about the claim can be published that we cannot prevent from being used as evidence. Further, the Google API does not always correctly infer the publication dates of articles. As such, our guarantee against temporal leakage is approximate (see Section 8). For the development set, we estimated that around 6% of answers are sourced from a fact-checking domain. This is primarily because of the earlier publication of an article about the same claim by a different fact-checking organization.

Evidence Sufficiency CheckOnce question-answer pairs have been generated, we present each claim along with its question-answer pairs to a new annotator. This annotator does _not_ see the fact-checking article. They then produce a verdict and a textual justification for it. We compare this verdict to the one produced by the question and answer annotator, and if they disagree we repeat the question-&-answer generation and sufficiency check steps with new annotators to improve the evidence and the verdict.

## 5 Dataset Statistics

We split our dataset into training, validation, and test data temporally (see Table 2). Claims have on average 2.60 questions, and questions have on average 1.07 answers. Most answers (53%) are extractive, followed by abstractive (26%) and boolean (17%) answers. A few questions (4%) are marked unanswerable as no available evidence could be found by the annotators. Statistics for source document modality, fact-checker strategy, and claim type can be seen in Appendix F. We note that AVerTeC is somewhat unbalanced - the majority of claims are _refuted_. This is a consequence of our choice to rely on fact-checking articles, as journalists tend to pick false or misleading claims to work on. Our dataset includes all ClaimReview claims labeled _supported_ (or any variation thereof, e.g. _true_) within our temporal limits (for more detail see Appendix I.1).

  Split & Train & Dev & Test \\  Claims & 3068 & 500 & 1000 \\ Questions / Claim & 2.60 & 2.57 & 2.57 \\ Reannotated (\%) & 28.1 & 24.4 & 25.1 \\ End date & 25-08-2020 & 31-10-2020 & 22-12-2021 \\ Labels (S / R / C / N) (\%) & 27.6 / 56.8 / 6.4 / 9.2 & 24.4 / 61.0 / 7.6 / 7.0 & 25.5 / 62.0 / 6.3 / 6.2 \\  

Table 2: Descriptive statistics for the dataset. Statistics for labels are split into supported (S), refuted (R), conflicting evidence/cherrypicking (C), and not enough evidence (N). For the dev and test splits, the start date is the end date of the previous split; the train set has no start date.

To measure the inter-annotator agreement of our annotation scheme, we had a second set of annotators re-annotate 100 claims from the dataset. In this we assumed the claim extraction and normalization step was done, and the annotators repeated the question and answer generation phase. Since we have an unbalanced dataset, following Kazemi et al. (2021), Oushiohoum et al. (2022), we therefore measure agreement using Randolph's (2005) free-marginal multirater \(\), an alternative to Fleiss' \(\) more suitable for unbalanced datasets (Warrens, 2010). Our observed agreement score of \(=0.619\) is substantial, and compares well to those for other "hard" fact-checking annotation tasks, e.g. Kazemi et al. (2021), who got between \(=.30\) and \(=.63\) depending on the language. Using Fleiss' \(\)(Fleiss, 1971), we get an agreement score of \(0.503\).

## 6 Evaluation

To evaluate models on AVeriTeC, we follow Thorne et al. (2018) and score retrieved evidence based on agreement with gold evidence, and give credit to veracity predictions (and justifications) only when correct evidence has been found. However, unlike in FEVER and other datasets using a closed source of evidence such as Wikipedia, AVeriTeC is intended for use with evidence retrieved from the open web. Since the same evidence may be found in different sources, we cannot rely on exact matching to score retrieved evidence. As such, we instead rely on approximate matching.

To measure how well a set of generated questions and answers match the references, we rely on a pairwise scoring function \(f:S S\), where \(S\) is the set of sequences of tokens. We then use the Hungarian Algorithm (Kuhn, 1955) to find an optimal matching of generated sequences to reference sequences. Formally, let \(X: Y\{0,1\}\) be a boolean function denoting the assignment between the generated sequences \(\) and the reference sequences \(Y\). Then, the total score \(u\) is calculated as:

\[u_{f}(,Y)=_{}_{y Y}f(,y)X(,y) \]

If \(f\) is an exact match, we recover the evidence _recall_ score from Thorne et al. (2018). Our metric is as such a generalization of theirs to the approximate case. In our evaluation, we use the implementation of METEOR (Banerjee and Lavie, 2005) in NLTK (Bird et al., 2009) as the scoring function \(f\) (and refer to our evidence scoring function as Hungarian METEOR hereafter), but any suitable pairwise metric could be used. We chose METEOR over other alternatives (e.g., ROUGE (Lin, 2004)) as it is known to correlate well with human judgments of similarity (Fomicheva and Specia, 2019). We do not employ a precision metric, as we want to avoid penalizing systems for asking additional relevant information-seeking questions - however, all systems are limited to a maximum of \(k=10\) question-answer pairs.

We conduct the evaluation with Hungarian METEOR twice: once using only the questions as input sequences, and once using the concatenation of questions and answers. A subtask of AVeriTeC is to _ask the right questions_ - as we discuss in Section 7.2, good questions are very useful as search queries even if not accompanied by a good answer. Finding the right angle to criticize a claim is a substantial task by itself; it covers the creativity factor in retrieval discussed by Arnold (2020). Including the question-only score allows comparison of systems along this axis as well. To evaluate veracity predictions and justifications, we use a cutoff of \(f(,y)\) to determine whether correct evidence has been retrieved (using concatenated questions and answers); any claim for which the evidence score is lower receives veracity and justification scores of \(0\).

Many claims can be verified through alternative evidence formulations. Taking an example from the 100 claims annotated twice for Section 5, one annotator might produce the question-answer pair _"Where did South Africa rank in alcohol consumption? In 2016, South Africa ranked ninth out of 53 African countries."_ while another produces _"What's the average alcohol consumption per person in South Africa? 7.1 litres."_. These may both be valid ways of establishing the relative levels of alcohol consumption between South Africa and other countries. We recognize that our evaluation approach can penalize systems for selecting an alternative evidence path; nevertheless, we argue that automatic evaluation on this task is helpful in model development. We note that a similar phenomenon was seen for the original FEVER dataset (Thorne et al., 2018), despite the artificial claims and the exclusive use of Wikipedia as an evidence source. There, the authors suggested crowd-sourced human evaluation as a more reliable alternative - we echo their recommendation. Our annotation process hints at a potential setup for human evaluation: judging if a body of evidence is sufficient for a verdict is exactly what our annotators did during the evidence sufficiency check phase.

We further note that our metric is straightforward to extend to cover evaluation with multiple reference sets. Given a set of sets of question-answer pairs \(R\) representing different questioning strategies, a best-matching score could be computed as \(_{Y R}u_{f}(,Y)\). As such, if AVeriTeC was expanded with annotations for alternative questioning strategies, our metric could score models on these as well.

To understand how our metric should be interpreted, we also computed Hungarian METEOR scores between the question-answer pairs generated during the two rounds of annotation used for inter-annotator agreement in Section 5. At \(0.28\) for questions and \(0.22\) for questions and answers, these results are quite low, highlighting the difficulty of automatic evaluation for this task. Investigating claims with low agreement scores, we see that these are actually often a result of different annotators using different evidence sources for the same verdict, or phrasing equivalent question-answer pairs differently. Based on our observations of human annotations, we recommend \(=0.25\) as an appropriate cutoff value for our metric. We refer to this metric (for veracity prediction) as AVeriTeC score.

## 7 Experiments

### Baseline Model

Our baseline is a pipeline consisting of several components: generation of search questions, search, generation of questions given retrieved evidence, reranking of retrieved evidence, veracity prediction, and generation of justifications. For each step in the pipeline, we carried out experiments with several models. Using our training set, we finetuned, respectively, BERT-large  for classification (340M parameters) and BART-large  for generation (406M parameters). We furthermore tried a few-shot setup, prompting a large language model (LLM) with retrieved in-context examples  from our training set. Here, we tried the 7b parameter BLOOM model  and the 13b parameter Vicuna model . We limited ourselves to relatively small models, as we consider runnability crucial for a baseline: all our components can be run on a single Nvidia A100 GPU. As such, our baseline strikes a balance between performance and computational cost.

SearchGiven a claim, we retrieve evidence documents from the internet using the Google Search API. Following Karadzhov et al. , we use a reduced version of the claim keeping only verbs, nouns, and adjectives as the search term. As we did during annotation, we limit the API to documents published _before_ the estimated date of the claim. We keep all unique documents in the first 30 search results. Initial experiments showed that questions were very useful as additional search terms. As the model does not have access to gold questions during testing, we instead generate questions. We experimented with three models: BART-large, _bloom-7b_, and _Vicuna-13b_. Surprisingly, BLOOM performed the best, beating the newer and larger Vicuna; we attribute this primarily to greater topical diversity in the set of questions generated, and thus greater variety in the retrieved evidence pages. We rely on prompting with retrieved in-context examples . We use BM25  to find the 10 most similar claims from the training set,

   Model & Q only & Q + A &  &  \\  No search & 0.19 & 0.11 & 0.03 & 0.02 & 0.01 & 0.02 & 0.01 & 0.01 \\ Gold evidence & 1.00 & 1.00 & 0.49 & 0.49 & 0.49 & 0.28 & 0.28 & 0.28 \\  AVeriTeC -BLOOM-7b & 0.26 & 0.21 & 0.23 & 0.15 & 0.00 & 0.11 & 0.07 & 0.05 \\  gpt-3.5-turbo & 0.29 & 0.16 & 0.17 & 0.10 & 0.06 & 0.06 & 0.04 & 0.02 \\   

Table 3: Results for the AVeriTeC baseline and ChatGPT (gpt3.5-turbo). Retrieval scores both for questions and for questions + answers are given in terms of Hungarian METEOR score. Veracity and justifications are scored using accuracy and METEOR respectively, in both cases conditioned on correct evidence retrieved at \(=\{0.2,0.25,0.3\}\) (see Section 6). We report results for three versions of the baseline, as discussed in Section 7.2: a version that uses no evidence (no search), a version that uses gold evidence (gold evidence), and the full pipeline described in Section 7.1 (AVeriTeC). We also report results for gpt-3.5-turbo (ChatGPT).

and use their annotated questions to construct a prompt, with which we generate questions for the claim using BLOOM. We tested {1,3,5,10} in-context examples, finding 10 to perform the best. The full prompt can be seen in Appendix D.1. We add any new unique documents retrieved by searching for these generated questions. This can be seen as a form of query expansion .

Evidence SelectionOnce a set of evidence documents has been created for each claim, we pick _N = 3_ sentences from this set. We first apply a coarse filter to the evidence set, ranking evidence sentences by BM25 score computed against the claim, and discard those outside the top 100. Then, we generate a question for each sentence that is answerable _by_ that sentence, again using BLOOM. We tested {1,3,5,10} in-context examples, finding 10 to perform the best. The full prompt can be seen in Appendix D.2. We then re-rank these question-answer pairs to find the ones most relevant for the claim, using a finetuned BERT-large model  (for more details, see Appendix E.1). This somewhat counter-intuitive strategy of retrieving first and then generating questions can be seen as using the generated questions to bridge claims to distantly related evidence sentences. Our approach is similar to the document expansion strategy proposed for question answering in Nogueira et al. , except applied for reranking rather than the initial retrieval step.

Veracity PredictionOnce question-answer pairs have been generated, we produce verdicts through a stance detection strategy inspired by past work on filtering evidence . We use a finetuned BERT-large model to label each question-answer pair as supporting, refuting, or being irrelevant to the question (for more details, see Appendix E.2). We then deterministically label the claim as follows: 1) if the claim has both supporting and refuting evidence, label it _conflicting evidence/cherrypicking_. 2) If the claim has only supporting question-answer pairs, label it _supported_; similar for _refuted_. 3) Otherwise, label the claim _not enough evidence_. We tested three different models for veracity prediction: BERT-large, _bloom-7b1_, and _Vicuna-13b_. We found BERT to perform better by a slight margin; using gold evidence, we obtained macro-F1 scores of.49,.43, and.48 for the three models respectively.

Justification GenerationThe final step is to generate a textual justification for the verdict. Here, we rely on BART-large  finetuned on our training set (for more details, see Appendix E.3). We use the concatenation of the claim and the retrieved evidence as input; we tried adding the predicted veracity as well, but saw no improvements to performance. We again tested three models: BART-large, _bloom-7b1_, and _Vicuna-13b_, respectively obtaining a METEOR score of.28,.23, and.25. Based on our qualitative analysis of 20 claims, the justifications generated by Vicuna are very good, but the model is penalized for being overly verbose - Vicuna generates 36 tokens on average, compared to 21 in the gold data.

### Results

We evaluate as discussed in Section 6. We include results in Table 3 at three thresholds for comparison, although we encourage \(=0.25\). We compare our baseline to two other models: one without access to search, and one using gold question-answer pairs as evidence.

For the _no search_ model, we use prompting to generate questions, following the approach described in Appendix D.1. We leave all answers as _"No answer could be found"_. Generating answers is not an option, as answers must be supported by sources. We use BERT-large finetuned on the training data to predict veracity labels (without any evidence), and the same prompting strategy as discussed in Section 7.1 to generate justifications. For the _gold evidence_ model, we use the gold question-answer pairs provided by our annotators in the place of generated questions and retrieved evidence. That is, we test only the veracity prediction and justification production components.

Analysing retrieval results on the development set, we still find \(=0.25\) to be a good cutoff point for the AVeriTeC veracity and justification metrics. For borderline question-answer pairs this threshold is high enough that all important information must be produced to meet it, but there is still some room for paraphrasing and partial evidence.

Our baseline has decent performance at \(=0.2\) and \(=0.25\), but does not perform well at higher evidence cutoff points. Because of the structure of our pipeline - generate search terms, retrieve and rerank evidence, generate questions to match the reranked evidence - our baseline struggles to match specific evidence sets. If the retrieved evidence paragraph is very short, e.g., a table cell reading "January 24th", the question generation model often lacks context to generate the right question. Further, the baseline cannot generate questions with highly abstractive answers, only questions that can be answered directly by sentences in the supporting sources.

We recognize that the retrieval scores of this baseline are quite close to those of the human annotators seen in Section 6. Nevertheless, for evidence retrieved by our baseline, low scores are much more frequently a result of reliance on _wrong_ evidence, rather than _equivalent_ evidence phrased differently and scored incorrectly. Further research is needed to develop an evaluation capable of recognizing this difference, e.g., a trained metric in the style of BLEURT (Sellam et al., 2020).

The gap between gold evidence and retrieved evidence highlights how retrieval remains challenging, also discussed in Arnold (2020). Manually analysing 20 examples from the development set, we find that Google search results based on the claim and the generated questions contain useful evidence only in 9/20 cases. If the retrieval system had access to the gold questions for use as search queries, correct evidence would be found in 16/20 of these cases; this highlights the need for further development of retrieval and search systems, and especially query/question generation.

We also report individual _F1_ scores for each veracity class (as well as a macro average) in Table 4. Our veracity prediction model fails to accurately predict _Conflicting Evidence/Cherrypicking_ most of the time, even with gold evidence. Going through the predictions, we see that precision is very low (10% using gold evidence). Labelling claims as _Conflicting Evidence/Cherrypicking_ if any evidence is classified as having different stance leads to many false positives - often, questions that simply add context to supported claims are (incorrectly) labelled as _refuting_ by the stance detection model.

As a way to improve the stance detection component, we tried to generate additional training data using gpt-3.5-turbo (ChatGPT). We paraphrased each claim in the dataset, using the same evidence. We generated one paraphrase per claim. Then, we trained BERT-large on the concatenated original and paraphrased claims. Unfortunately, this failed to yield additional performance, producing a macro-_F1_ score of.46 on gold data; slightly lower than the.49 obtained using only the original claims. The primary cause is a drop in performance for _refuted_ and _not enough evidence_, which the model trained on paraphrased data conflates more often.

We further include results in Tables 3 and 4 using ChatGPT. As ChatGPT cannot produce sources to back up its answers, this is not directly comparable to our baseline. Nevertheless, it is an interesting point of comparison. We generate evidence and verdicts with ChatGPT, using the prompt described in Appendix G. We find that ChatGPT outperforms our baseline in terms of pure question generation, but nevertheless received a lower AVeriTeC score (veracity prediction at \(=0.25\)). This is a consequence of the missing retriever: generated answers often do not match gold answers (i.e., they are either alternative correct answers, or outright hallucinations).

ChatGPT performs well on veracity, especially for supported claims; but those verdicts are often not supported by valid evidence. For example, for the claim _"1 cup of dandelion greens = 535% of your daily recommended vitamin K and 112% of vitamin A."_, ChatGPT assigned the verdict _supported_ and generated the evidence string _"According to the USDA, 1 cup of chopped dandelion greens provides 535% of your daily recommended vitamin K and 338% of vitamin A, which is higher than the claim"_. While the verdict is true, there is no such statement from the USDA, and the actual gold evidence relies on several question-answer pairs and a calculation to arrive at the verdict.

   Model & S & R & C & N & Macro \\  No evidence &.30 &.22 &.00 &.16 &.17 \\ Gold evidence &.48 &.74 &.15 &.59 &.49 \\  AVeriTeC &.41 &.69 &.10 &.16 &.23 \\  gpt-3.5-turbo &.62 &.71 &.02 &.20 &.39 \\   

Table 4: _F1_-scores for veracity prediction split across labels: supported (S), refuted (R), conflicting evidence/cherrypicking (C), and not enough evidence (N). We also show the macro-average. Again, we report results for three versions of the baseline (see Section 7.2): a version that uses no evidence (no search), a version that uses gold evidence (gold evidence), and the full pipeline ( AVeriTeC ). We also report results for gpt-3.5-turbo (ChatGPT).

Limitations

The evaluation metric we have presented alongside AVeriTeC contains a significant limitation: no efforts are made to ensure answers and source documents are consistent. As only 53% of gold answers are fully extractive, it is expected that abstractive models will be employed. Such models though can hallucinate, and can thus make up answers that are not supported by the underlying sources, which our evaluation metric cannot detect. Further research is needed on evaluation to counteract this, along with research on developing an evaluation strategy that better allows verifying claims correctly with different questioning strategies and evidence documents.

While claims geographically concern regions from all around the world, all fact-checking sources and consequently all claims used in our dataset are in English. Further, as we take claims directly from fact-checking articles, our dataset is subject to any biases present within those articles; notably, for internal fact-checking, Barnoy and Reich (2019) documented a selection bias resulting from journalists rating claims by male sources more credible than female sources.

Finally, we note that our reliance on Google Search to avoid temporal leakage is a noisy process. The dates we rely on are the best estimate computed by Google6. As such, while in general evidence documents were available when associated claims were published, there may be exceptions.

## 9 Ethics Statement

Fact-checking is often envisioned as an epistemic tool, limiting the spread and influence of misinformation. The datasets and models described in this paper are not intended for truth-telling, e.g. for the design of automated content moderation systems. The labels and justifications included with this dataset relate only to the evidence recovered by annotators, and as such are subject to the biases of annotators and journalists; furthermore, the machine learning models and search engine used for the baseline contain well-known biases (Noble, 2018; Bender et al., 2021). Acting on veracity estimates arrived at through biased means, including automatically produced ranking decisions for evidence retrieval, risks causing epistemic harm (Schlichtkrull et al., 2023).

Annotators for our dataset had access to searching the entire web when finding evidence documents. We curated a list of common misinformative sources by combining several public documents7, and flagged search results from these sources. Nevertheless, we did not prevent annotators from using them as evidence. Pointing out that a claim originates from an untrustworthy site is an important fact-checking strategy, and, indeed, our list may well contain false positives. A total of 85 answers in AVeriTeC rely on a flagged source; moreover, our list is not complete. Our dataset may as such include misleading examples, and can potentially cause harm if relied on as an authoritative source.

We did not take any steps to anonymise the data. The claims discussed in our dataset are based on publicly available data from journalistic publications, and concern public figures and events - references to these are important to fact-check claims. We did not contact these public figures, or the journalists who published the original fact-checking articles. If any person included in our dataset as a speaker of a claim, as the subject of a claim, or as the author of a fact-checking article a claim is based on requests it, we will remove that claim from the dataset.

## 10 Conclusion

We have introduced AVeriTeC, a new real-world fact-checking dataset consisting of 4,568 claims, each annotated with question-answer pairs decomposing the fact-checking process, as well as justifications. Our multi-step annotation process guarantees high-quality annotations, providing evidence sufficiency and avoiding temporal leakage; it also results in a substantial inter-annotator agreement of \(=0.619\). We have also introduced and analysed a baseline as well as an evaluation scheme, establishing AVeriTeC as a new benchmark.