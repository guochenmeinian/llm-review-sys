# Non-Asymptotic Analysis of a UCB-based

Top Two Algorithm

Marc Jourdan

marc.jourdan@inria.fr

&Remy Degenne

remy.degenne@inria.fr

Univ. Lille, CNRS, Inria, Centrale Lille, UMR 9189-CRIStAL, F-59000 Lille, France

###### Abstract

A Top Two sampling rule for bandit identification is a method which selects the next arm to sample from among two candidate arms, a _leader_ and a _challenger_. Due to their simplicity and good empirical performance, they have received increased attention in recent years. However, for fixed-confidence best arm identification, theoretical guarantees for Top Two methods have only been obtained in the asymptotic regime, when the error level vanishes. In this paper, we derive the first non-asymptotic upper bound on the expected sample complexity of a Top Two algorithm, which holds for any error level. Our analysis highlights sufficient properties for a regret minimization algorithm to be used as leader. These properties are satisfied by the UCB algorithm, and our proposed UCB-based Top Two algorithm simultaneously enjoys non-asymptotic guarantees and competitive empirical performance.

## 1 Introduction

Faced with a collection of items ("arms") with unknown probability distributions, a question that arises in many applications is to find the distribution with the largest mean, which is referred to as the best arm. Different approaches have been considered depending on the data collection process. Sequential hypothesis testing  encompasses situations where there is no control on the collected samples. Experimental design  aims at choosing the data collection scheme a priori. In the multi-armed bandit  and the ranking and selection  literature, an algorithm chooses sequentially the distribution from which it will collect an additional sample based on past data.

In order to have theoretical guarantees for this identification problem, one should adopt a statistical model on the underlying distributions. While parametric models are reasonable for applications such as A/B testing , they are unrealistic in other fields such as agriculture . Despite the restricted scope of its applications, studying the identification task for Gaussian distributions is a natural first step. Hopefully the insights gained will then be generalized to wider classes of distributions.

In the fixed confidence identification problem, an algorithm aims at identifying the best arm with an error of at most \((0,1)\) while using as few samples as possible. Since each sample has a cost, those algorithms should provide an upper bound on the expected number of samples used by the algorithm before stopping. For those guarantees to be useful in practice, they should hold for any \(\), which is referred to as the non-asymptotic (or moderate) regime. In contrast, the asymptotic regime considers vanishing error level, i.e. \( 0\). For Gaussian distributions, Top Two algorithms have only been studied in the asymptotic regime. We show the first non-asymptotic guaranty for a Top Two algorithm holding for any instance with a unique best arm.

### Setting and related work

A Gaussian bandit problem is described by \(K\) arms whose probability distributions belongs to the set \(\) of Gaussian distributions with known variance \(^{2}\). By rescaling, we assume \(_{2}^{2}=1\) for all \(i[K]\). Since an element of \(\) is uniquely characterized by its mean, the vector \(^{K}\) refers to a \(K\)-arms Gaussian bandit. Let \(_{K}^{K}\) be the \((K-1)\)-dimensional simplex.

A best arm identification (BAI) algorithm aims at identifying an arm with highest mean parameter, i.e. an arm belonging to the set \(i^{}()=_{i[K]}_{i}\). At each time \(n\), the algorithm (1) chooses an arm \(I_{n}\) based on previous observations, (2) observes a sample \(X_{n,I_{n}}(_{I_{n}},1)\), and (3) decides whether it should stop and return an arm \(_{n}\) or continue sampling. We consider the _fixed confidence_ identification setting, in which the probability of error of an algorithm is required to be less than a given \((0,1)\) on all instances \(\). The _sample complexity_ of an algorithm corresponds to its stopping time \(_{}\), which counts the number of rounds before termination. An algorithm is said to be \(\)-correct on \(^{K}\) if \(_{}(_{}<+,\,_{_{}}  i^{}())\) for all \(^{K}\). We aim at designing \(\)-correct algorithms minimizing \([_{}]\).

As done in all the literature on fixed-confidence BAI, we assume that there is a unique best arm and we denote it by \(i^{}()\) or \(i^{}\) when \(\) is clear from the context. To ensure \(\)-correctness on \(^{K}\), an algorithm has to be able to distinguish the unknown \(\) from any instance having a different best arm, hence it needs to estimates the gaps between arms. Lemma 1.1 gives a lower bound on the expected sample complexity which is known to be tight in the asymptotic regime, i.e. when \(\) goes to zero.

**Lemma 1.1** ().: _An algorithm which is \(\)-correct on all problems in \(^{K}\) satisfies that for all \(^{K}\), \(_{}[_{}] T^{}()(1/(2.4))\) where \(T^{}()=_{(0,1)}T^{}_{}()\) and, for all \((0,1)\),_

\[T^{}_{}()^{-1}:=_{w_{K}:w^{}_{i^{}() }=}_{i i^{}()}()}-_{i})^{2}}{ 2(1/+1/w_{i})}\,.\]

When considering the sub-class of algorithms allocating a fraction \(\) of their sample to the best arm, we obtain a lower bound as in Lemma 1.1 with \(T^{}_{}()\) instead of \(T^{}()\). An algorithm is said to be asymptotically optimal (resp. \(\)-optimal) if its sample complexity matches that lower bound asymptotically, that is if \(_{ 0}_{}[_{}]/(1/) T^{ }()\) (resp. \(T^{}_{}()\)).  showed the worst-case inequality \(T^{}_{1/2}() 2T^{}()\) for any single-parameter exponential families. Therefore, the expected sample complexity of an asymptotically \(\)-optimal algorithm with \(=1/2\) is at worst twice higher than that of any asymptotically optimal algorithm. Leveraging the symmetry of Gaussian distributions, a tighter worst-case inequality can be derived (Lemma C.6). The allocations \(w^{}()\) and \(w^{}_{}()\) realizing \(T^{}()\) and \(T^{}_{}()\) are known to be unique, and satisfy \(_{i[K]}\{w^{}()_{i},w^{}_{}()_{i}\}>0\).  showed that \(2 w^{}()^{-1}_{i^{}}+1\) for Gaussian distributions (Lemma C.4).

Related workThe first BAI algorithms were introduced and studied under the assumption that the observation have bounded support, with a known upper bound . The sample complexity bounds proved for these algorithms scale as the sum of squared inverse gap, i.e. \(H():=2_{}^{-2}+_{i i^{}}2(_{i^{}}-_{i})^{ -2}\) where \(_{}:=_{i i^{}}(_{i^{}}-_{i})\), which satisfies \(H() T^{}() 2H()\). Following their work, a rich literature designed asymptotically optimal algorithms in the fixed-confidence setting for parametric distributions, such as single-parameter exponential families, and non-parametric distributions such as bounded ones. Those algorithms build on two main ideas. The Tracking approach computes at each round the optimal allocation for the empirical estimator, and then tracks it . To achieve lower computational cost, Game-based algorithms  view \(T^{}()^{-1}\) as a min-max game between the learner and the nature, and design saddle-point algorithms to solve it sequentially.

Top Two algorithms arose as an identification strategy based on the Thompson Sampling algorithm for regret minimization :  introduced Top Two Probability Sampling (TTPS) and Top Two Thompson Sampling (TTTS). Adopting a Bayesian viewpoint, Russo studied the convergence rate of the posterior probability that \(i^{}\) is not the best arm, under some conditions on the prior. For Gaussian bandits, other Bayesian Top Two algorithms with frequentist components have been shown to be asymptotically \(\)-optimal: Top Two Expected Improvement (TTEI, ) and Top Two Transportation Cost (T3C, ).  introduces fully frequentist Top Two algorithms. Their analysis proves asymptotic \(\)-optimality for several Top Two algorithms and distribution classes, beyond Gaussian.  provides guarantees for single-parameter exponential families, at the price of adding forced exploration.  proposes an algorithm to tackle the top-\(k\) identification problem and introduces information-directed selection (IDS) to choose \(\) in an adaptive manner, which differs from the one proposed in . In addition to their success in the fixed-confidence setting, Top Two algorithms have also been studied for fixed-budget problems , in which guarantees on the error probability should be given after \(T\) samples. While existing Top Two sampling rules differ by how they choose the leader and the challenger, they all sample the leader with probability \(\). By design, Top Two algorithms with a fixed \(\) can reach \(\)-optimality at best, and cannot be optimal on all instances \(\).

Shortcomings of the asymptotic regimeWhile the literature provides a detailed understanding of the asymptotic regime, many interesting questions are unanswered in the non-asymptotic regime. Recent works [9; 40; 32; 31] have shown that the sample complexity is affected by strong moderate confidence terms (independent of \(\)). The analysis of  applies to their \(\)-EB-TC algorithm whose empirical stopping times is order of magnitude larger than its competitors for \(=0.01\). Since the proof of asymptotic \(\)-optimality hides design flaws, non-asymptotic guarantees should be derived to understand which Top Two algorithms will perform well in practice for any reasonable choice of \(\).

### Contributions

Our main contribution is to propose the first non-asymptotic analysis of Top Two algorithms. We identify sufficient properties of the leader (seen as a regret-minimization algorithm) for it to hold. This solves two open problems: obtaining an upper bound which (1) is non-asymptotic (Theorem 2.4 holds for any \(\)) and (2) holds for all instances having a unique best arm (i.e. sub-optimal arms can have the same mean, which was not allowed in the analyzes of existing Top Two algorithms). As a consequence, we propose the TTUCB (Top Two UCB) algorithm which builds on the UCB algorithm.

By using tracking instead of sampling to choose between the leader and the challenger, TTUCB is the first Top Two algorithm which is asymptotically \(\)-optimal (Theorem 2.3) and has non-asymptotic guarantees (Theorem 2.4). Our experiments reveal that TTUCB performs on par with existing Top Two algorithms, which are only proven to be asymptotically \(\)-optimal, even for large sets of arms. Numerically, we show that considering adaptive proportions compared to a fixed \(=1/2\) yields a significant speed-up on hard instances, and to a moderate improvement on random instances.

## 2 UCB-based Top Two algorithm

We propose a fully deterministic Top Two algorithm based on UCB , named TTUCB and detailed in Algorithm 1. We prove a non-asymptotic upper bound on the expected sample complexity holding for any instance having a unique best arm.

Stopping and recommendation rulesThe \(\)-algebra \(_{n}:=(\{I_{t},X_{t,I_{t}}\}_{t[n-1]})\) encompasses all the information available to the agent before time \(n\). Let \(N_{n,i}:=_{t[n-1]}(I_{t}=i)\) be the number of pulls of arm \(i\) before time \(n\), and its empirical mean by \(_{n,i}:=}_{t[n-1]}X_{t,I_{t}}(I_{t}=i)\).

The algorithm stops as soon as the generalized likelihood ratio exceeds a threshold \(c(n-1,)\), i.e.

\[_{i i_{n}}}-_{n,i}}{}+1/N_{n,i} }}\,, \]

where we recommend \(_{n}=_{i[K]}_{n,i}\) at time \(n\). Lemma 2.1 provides an explicit threshold ensuring \(\)-correctness, which relies on concentration inequalities derived in .

**Lemma 2.1**.: _Let \(_{G}\) defined in (16) s.t. \(_{G}(x) x+(x)\). Given any sampling rule, taking_

\[c(n,)=2_{G}(((K-1)/)/2)+4(4+(n/2)) \]

_in the stopping rule (1) ensures \(\)-correct for Gaussian distributions._

**Algorithm 1** TTUCB

**Input:**\((,)(0,1)^{2}\), threshold \(c:(0,1)^{+}\) and function \(g:^{+}\).

 Pull once each arm \(i[K]\);

**for**\(n>K\)**do**

 Set \(_{n}=_{i[K]}_{n,i}\);

**If**\(_{i_{n}}}-_{n,i}}{}+1}/N_{n,i}} \) **then** return \(_{n}\)**, else**;

 Set \(B_{n}^{}=_{i[K]}\{_{n,i}+}}\}\) and \(C_{n}^{}=_{i B_{n}^{}}^{ }}-_{n,i})_{+}}{^{}}+1/N_{n,i}}}\);

 Observe \(X_{n,I_{n}}\) by pulling \(I_{n}=B_{n}^{}\) if \(N_{n,B_{n}^{}}^{B_{n}^{}} L_{n+1,B_{n}^{}}\), else \(I_{n}=C_{n}^{}\);

**end for**

**Algorithm 2** TTUCB

Sampling ruleWe initialize by sampling each arms once. At time \(n>K\), a Top Two sampling rule defines a leader \(B_{n}[K]\) and a challenger \(C_{n} B_{n}\), and chooses \(I_{n}=B_{n}\) or \(I_{n}=C_{n}\) based on a fixed allocation \(\). In prior work this choice was done at random, which means that the leader was sampled with probability \(\). We replace randomization by tracking, and show similar theoretical and numerical results (see Figure 4 in Appendix G.2). For fixed \(\), we recommend to use \(=1/2\) without prior knowledge on the unknown mean parameters (see Section 3.4 for adaptive proportions). This recommendation is supported theoretically by the fact that \(w^{*}()_{i^{*}} 1/2\) (Lemma C.4) and that \(T_{1/2}^{*}()/T^{*}()\) is significantly smaller than \(2\) for most instances (Lemma C.6 and Figure 2).

Let \(L_{n,i}:=_{t[n-1]}(B_{t}=i)\) be the number of time arm \(i\) was the leader, and \(N_{n,j}^{i}:=_{t[n-1]}((B_{t},I_{t})=(i,j))\) be the number of pulls of arm \(j\) at rounds in which \(i\) was the leader. We use \(K\) independent tracking procedures. A tracking procedure is a deterministic method to convert a sequence of allocations over arms into a sequence of arms, which ensures that the empirical proportions are close to the averaged allocation over arms. For each leader, we track the allocation \((,1-)\) between the leader and the challenger. Formally, we set \(I_{n}=B_{n}\) if \(N_{n,B_{n}}^{B_{n}} L_{n+1,B_{n}}\), else \(I_{n}=C_{n}\). Using Theorem 6 in  for each tracking procedure yields Lemma 2.2.

**Lemma 2.2**.: _For all \(n>K\) and all \(i[K]\), we have \(-1/2 N_{n,i}^{i}- L_{n,i} 1\)._

Using tracking over randomization is motivated by practical and theoretical reasons. First, in some specific applications, the practitioner might be only willing to use a deterministic algorithm. Second, in the analysis, it is easier to control deterministic counts since it removes the need for martingales arguments to bound the deviations of the samples. Therefore, tracking simplifies the non-asymptotic analysis. Third, Lemma 2.2 shows that the speed of convergence is at least \((1/n)\) for tracking, while we would obtain a speed of \(O(1/)\) for randomization.

At time \(n\), the UCB leader is defined as

\[B_{n}^{}=*{arg\,max}_{i[K]}\{_{n,i}+}\}\,, \]

where \(}\) is a bonus coping for uncertainty. Let \(>1\) and \(s>1\) be two concentration parameters. The choice of \(g(n)\) should ensure that we have an upper confidence bound on \(_{i}\) holding with high probability: with probability \(1-Kn^{-s}\), for all \(t[n^{1/},n]\) and all arms \(i[K]\), \(_{i}[_{t,i}}]\). For Gaussian observations, a function \(g\) which is sufficient for the purpose of our proof can be obtained by a union bound over time, giving \(g_{u}(n)=2(1+s) n\). We can improve on \(g_{u}\) with mixtures of martingales, yielding \(g_{m}(n)=_{-1}(2s(n)+2(2+ n)+2)\) with \(_{-1}(x)=-W_{-1}(-e^{-x})\) for all \(x 1\), where \(W_{-1}\) is the negative branch of the Lambert \(W\) function, and \(_{-1}(x) x+(x)\). A UCB leader with \(g_{0}(n)=0\) recovers the Empirical Best (EB) leader . Choosing \(g\) is central for empirical performance and non-asymptotic guarantees, but not for asymptotic ones. The lowest \(g\) will yield better empirical performance since larger \(g\) means more conservative confidence bounds. In our experiments where \(=s=1.2\), we will consider \(g_{m}\) since \(g_{m}(n) g_{u}(n)\) for \(n 50\).

Given a leader \(B_{n}\), the TC challenger is defined as

\[C_{n}^{}=*{arg\,min}_{i B_{n}}}- _{n,i})_{+}}{}+1/N_{n,i}}}\,, \]where \(x_{+}=\{x,0\}\).  introduced the TC challenger as a computationally efficient approximation of the challenger in TTTS , which uses re-sampling till an unlikely event occurs. Both T3C and TTTS use the TS leader which takes the best arm of a vector of realization drawn from a sampler, e.g. \(_{i}(_{n,i},1/N_{n,i})\) for Gaussian distributions with unit variance.

Computational costComputing the stopping rule (1) and the UCB leader (3) can be done in \((K)\). At time \(n\) where \(B_{n}\) coincides with \(_{n}\), computing the TC challenger (4) is done as a by-product of the computation of the stopping rule, without additional cost. When \(B_{n}_{n}\), we draw at random an arm with larger empirical mean. The per-round computational and memory cost of TTUCB is \((K)\).

### Sample complexity upper bound

Leveraging the unified analysis of Top Two algorithms proposed by , we obtain the asymptotic \(\)-optimality of TTUCB (Theorem 2.3). After showing the required properties for the UCB leader, we proved that tracking Top Two algorithms have similar properties as their sampling-based counterparts.

**Theorem 2.3**.: _Let \((,)(0,1)^{2}\), \(s>1\) and \(>1\). Using the threshold (2) in (1) and \(g_{u}\) (or \(g_{m}\)) in (3), the TTUCB algorithm is \(\)-correct and asymptotically \(\)-optimal for all \(^{K}\) such that \(_{i j}|_{i}-_{j}|>0\), i.e. it satisfies \(_{ 0}\,_{}[_{}]/(1/) T_{ }^{}()\)._

Theorem 2.3 and guarantees for other Top Two algorithms hold only for arms having distinct means. Moreover, an asymptotic result provides no guarantees on the performance in moderate regime of \(\). We address those two limitations.

Non-asymptotic upper boundTheorem 2.4 gives an upper bound on the expected sample complexity holding for any \(\) and any instance having a unique best arm. It is a direct corollary of a more general result holding for any \((0,1)\), \(s>1\) and \(>1\) (Theorem D.4).

**Theorem 2.4**.: _Let \((0,1)\). Using the threshold (2) in (1) and \(g_{u}\) in (3) with \(s==1.2\), the TTUCB algorithm with \(=1/2\) satisfies that, for all \(^{K}\) such that \(|i^{}()|=1\),_

\[_{}[_{}]_{w_{0}[0,(K-1)^{-1}]}\{T_{ 0}(,w_{0}),C_{}^{1.2},C_{0}(w_{0})^{6},(2/)^{1.2}\} +12K\;,\]

_where \(C_{}=h_{1}(26H())\), \(C_{0}(w_{0})=2/( a_{}(w_{0}))+1\) with \((0,1]\),_

\[T_{0}(,w_{0})=\{n n-1 2T_{1/2}^{}()(1+)^{2}( 1-w_{0})^{-d_{}(w_{0})}+)^{2}\}\;,\]

_with \(a_{}(w_{0})=(1-w_{0})^{d_{}(w_{0})}\{_{i i^{}()}w_{ 1/2}^{}()_{i},w_{0}/2\}\) and \(d_{}(w_{0})=|\{i i^{}() w_{1/2}^{}()_{i}<w_{0}/2\}|\). The function \(h_{1}(x):=x_{-1}((x)+)\) is positive, increasing for \(x 2+2K\), and satisfies \(h_{1}(x) x( x+ x)\)._

The TTUCB sampling rule using \(g_{m}\) in (3) satisfies a similar upper bound (Corollary D.5). Since Theorem 2.4 holds for any instance having a unique best arm, we corroborate the intuition that assuming \(_{i j}|_{i}-_{j}|>0\) is an artifact of the existing proof to obtain asymptotic \(\)-optimality.

The upper bound on \(_{}[_{}]\) involves several terms. The \(\)-dependent term is \(T_{0}()\). In the asymptotic regime, we can show that \(_{ 0}T_{0}()/(1/) 2T_{1/2}^{}()\) by taking \(w_{0}=0\) and letting \(\) go to zero. While there is (sub-optimal) factor \(2\) in \(T_{0}()\), Theorem 2.3 shows that TTUCB is asymptotically \(1/2\)-optimal. This factor is a price we paid to obtain more explicit non-asymptotic terms, and removing it would require more sophisticated arguments in order to control the convergence of the empirical proportions \(N_{n}/(n-1)\) towards \(w_{1/2}^{}()\).

In the regime where \(H()+\), the upper bound is dominated by the \(\)-independent term \(C_{}^{1.2}\) (when \(=1.2\)) with satisfies \(C_{}=(H() H())\). Compared to the best known upper and lower bounds in this regime (see discussion below), our non-asymptotic term has a sub-optimal scaling in \(((H() H())^{})\) with \(>1\). While taking \( 1\) would mitigate this sub-optimality, it would yield a larger dependency in \(C_{0}(w_{0})^{/(-1)}\). Empirically, Figures 1(b) and 5 (Appendix G.2) hints that the empirical performance of TTUCB has a better scaling with \(H()\) than \(H()^{}\).

For instances such that \(_{i i^{}}w_{1/2}^{}()_{i}\) is arbitrarily small, taking \(w_{0}=0\) yields an arbitrarily large \(C_{0}(0)\). By clipping with \(w_{0}/2\), we circumvent this pitfall and ensure that \(C_{0}(w_{0})=(K/)\)Since it yields a larger \(T_{0}()\), we are trading-off asymptotic terms for improved non-asymptotic ones. We illustrate this with two archetypal instances. For the "1-sparse" instance, in which \(_{1}>0\) and \(_{i}=0\) for all \(i 1\), we have by symmetry that \(2w_{1/2}^{}()_{i}=1/(K-1)\) for all \(i 1\). Therefore, we have \(C_{0}(w_{0})=(K/)\) since \(d_{}(w_{0})=0\) for all \(w_{0}[0,1/(K-1)]\). The "almost dense" instance is such that \(_{1}=1\), \(_{K}=0\) and \(_{i}=1-\) for all \(i\{1,K\}\). By symmetry, there exists a function \(h:[0,1)[0,(K-1)^{-1})\) with \(_{ 0}h()=0\), such that \(2w_{1/2}^{}()_{K}=h()\) and \(2w_{1/2}^{}()_{i}=(1-h())/(K-2)\) for all \(i\{1,K\}\). While \(_{ 0}C_{0}(0)=+\), we obtain \(_{ 0}C_{0}(w_{0})=(K/)\) by taking \(w_{0}=(1-h())/(K-2)\) since \(d_{}(w_{0})=1\).

Comparison with existing upper boundsTable 1 summarizes the asymptotic and non-asymptotic scalings of the upper bound on the sample complexity of existing BAI algorithms. Among the class of asymptotically (\(\)-)optimal algorithms, very few of them also enjoy non-asymptotic guarantees, e.g. the analyses of Track-and-Stop and Top Two algorithms are asymptotic. The gamification approach of  is the first attempt to provide both. Their non-asymptotic upper bound on \(_{}[_{8}]\) involves an implicit time \(T_{1}()\) which scales with \(KT^{}()^{2}\) and is only valid for \((1/) KT^{}()\) (see Lemma 2, with constants in Appendix D.7). Let \(T_{}^{}:=T^{}()(1/)\). As a first order approximation, they obtain \(T_{1}() T_{}^{}+(^{}  T_{}^{}})\), and we obtain \(T_{0}()(T_{}^{}+ T_{}^{})\) (Lemma D.13).  were the first to obtain an upper bound on \(_{}[_{}]\) of the form \((T_{}^{}+(1/))\). While they improved the second-order \(\)-dependent term, the \(\)-independent term scales with \(e^{K}H()^{19/2}\) (see their Theorem 2 for \(^{-1} T^{}()\), with constants given by Appendix N). The algorithm proposed by  has a non-asymptotic upper bound on \(_{}[_{}()]\) of the form \((1+)T_{}^{}+f(,)\) which is valid for \((1/) w_{}^{-2}K/_{}\), where \(\) is such that \(_{}(^{})\). Since \(f(,)=_{ 0}o(1)\), they obtain a better \(\)-dependency. However, \(f(,)\) is arbitrarily large when \(w_{}:=_{i[K]}w^{}()_{i}\) is arbitrarily small since it scales with \(KH()^{4}/w_{}^{2}\). Therefore, they suffer from the pitfall which we avoided by clipping. In light of Table 1, TTUCB enjoys the best scaling when \(H()+\) in the class of asymptotically (\(\)-)optimal BAI algorithms.

The LUCB1 algorithm  has a structure similar to a Top Two algorithm, with the difference that LUCB samples both the leader and the challenger instead of choosing one. As LUCB1 satisfies \(_{}[_{}] 292H()(H()/)+16\), it enjoys better scaling when \(H()+\) than TTUCB. Since the empirical allocation of LUCB1 is not converging towards \(w_{1/2}^{}()\), it is not asymptotically \(1/2\)-optimal. The Peace algorithm  has a non-asymptotic upper bound on \(_{}\) of the form \(((T_{}^{}+^{}())(K/_{}))\) holding with probability \(1-\). The term \(^{}()\) is a Gaussian-width which originates from concentration on the suprema of Gaussian processes and satisfies \(^{}()=(H())\).

Another class of BAI algorithms focus on the dependency in the gaps \(_{i}:=_{i^{}}-_{i}\), and derive non-asymptotic upper bound on \(_{}\) holding with high probability.  gives \(\)-PAC algorithms with an upper bound of the form \((H()(1/)+_{i i^{}}_{i}^{-2} _{i}^{-1})\), and  shows that

   Algorithm & Asymptotic behavior & Finite-confidence behavior \\  LUCB1\(\) & \((H()(1/))\) & \((H() H())\) \\ Exp-Gap\(@sectionsign\) & \((H()(1/))\) & \((_{i i^{}}_{i}^{-2}_{i}^{-1})\) \\ lil’UCB\(@sectionsign\) & \((H()(1/))\) & \((_{i i^{}}_{i}^{-2}_{i}^{-1})\) \\ DKM\(\) & \(T^{}()(1/)+}()\) & \(}(KT^{}()^{2})\) \\ Peace\(@sectionsign\) & \((T^{}()(1/))\) & \((H()(K/_{}))\) \\ FWS\(\) & \(T^{}()(1/)+((1/))\) & \((e^{K}H()^{19/2})\) \\ EBS\(\)* & \(T^{}()(1/)+o(1)\) & \((KH()^{4}/w_{}^{2})\) \\
**TTUCB\(\)****\(\!\!for two arms the dependency \(^{-2}^{-1}\) is optimal when \( 0\). While those algorithms obtain the best scaling when \(H()+\), they are not asymptotically (\(\)-)optimal.

## 3 Non-asymptotic analysis

### Proof sketch of Theorem 2.4

Existing analyses of Top Two algorithms are asymptotic in nature and requires too much control on the empirical means and proportions to yield any meaningful information in the finite-confidence regime. Therefore, we adopt a different approach which ressembles the non-asymptotic analysis of . We first define concentration events to control the deviations of the random variables used in the UCB leader and the TC challenger. For all \(n>K\), let \(_{n}:=_{i[K]}_{t[n^{5/6},n]}(_{t,i}^ {1}_{t,i}^{2})\) where

\[_{t,i}^{1}:=\{}|_{t,i}-_{i}|<\} _{t,i}^{2}:=\{}-_{t,i})-(_{ i^{*}}-_{i})}{}+1/N_{t,i}}}>-\}\.\]

Using Lemmas D.8 and E.6, the proof boils down to constructing a time \(T()\) after which \(_{n}\{_{} n\}\) for \(n>T()\) since it would yield that \(_{}[_{}] T()+12K\).

Let \(n>K\) such that \(_{n}\{n<_{}\}\) holds true, and \(t[n^{5/6},n]\) such that \(B_{t}^{}=i^{}\). Using that \(t n<_{}\), under \(_{i i^{}}_{t,i}^{2}\), the stopping condition yields that

\[((_{i^{*}}-_{C_{t}^{}})(1/N_{t,i^{*}}^ {i^{*}}+1/N_{t,C_{t}^{}}^{i^{*}})^{-1/2}-)_{+}\.\]

Let \(w_{1/2}^{}\) be the unique element of \(w_{1/2}^{}()\). Lemma 3.1 links the empirical proportions \(N_{t,i}^{i^{*}}/(t-1)\) to \(w_{1/2,i}^{}\) for \(i\{i^{},C_{t}^{}\}\). It is the key technical challenge of our non-asymptotic proof strategy.

**Lemma 3.1**.: _Let \((0,1]\). There exist \(T_{}>0\) such that for all \(n>T_{}\) such that \(_{n}\{n<_{}\}\) holds true, there exists \(t[n^{5/6},n]\) with \(B_{t}^{}=i^{}\), which satisfies_

\[(n-1)(1/N_{t,i^{}}^{i^{*}}+1/N_{t,C_{t}^{}}^{i^{*}})(1+ )^{2}(2+1/w_{1/2,C_{t}^{}}^{*})/\.\]

Before proving Lemma 3.1, we conclude the proof of Theorem 2.4. Let \(,T_{}\) and \(t\) as in Lemma 3.1 and \(T():=\{n\ n-1 T_{1/2}^{}()(1+)^{2}(+)^{2}/\}\). For all \(n>\{T_{},T(1)\}\), we have \(+^{}()^ {-1}(1+)^{-2}}\). Therefore, we have proved that \(_{n}\{n<_{}\}=\) for all \(n>\{T_{},T()\}\). This concludes the proof.

Provided that \(B_{t}=i^{}\), the above only used the stopping condition and the TC challenger, and no other properties of the leader. Lemma 3.2 shows that \(B_{t}^{}=i^{}\), except for a sublinear number of times. Section 3.3 exhibits sufficient conditions on a regret minimization algorithm to obtain a non-asymptotic upper bound.

**Lemma 3.2**.: _Under the event \(_{k[K]}_{t[n^{5/6},n]}_{t,k}^{1}\), we have \(L_{n,i^{}} n-1-24H() n-2K\)._

Proof sketch of Lemma 3.1The key technical challenge is to link \(N_{t,C_{t}^{}}^{i^{*}}/(n-1)\) with \(w_{1/2,C_{t}^{}}^{*}\). We adopt the approach used to analyze of APT : consider an arm being over-sampled and study the last time this arm was pulled. By the pigeonhole principle, at time \(n\),

\[ k_{1} i^{},\; N_{n,k_{1}}^{i^{*}} 2(L_{n,i^{ *}}-N_{n,i^{*}}^{i^{*}})w_{1/2,k_{1}}^{*}. \]

Let \(t_{1}\) be the last time at which \(B_{t}^{}=i^{}\) and \(C_{t}^{}=k_{1}\), hence \(N_{t_{1},k_{1}}^{i^{*}} N_{n,k_{1}}^{i^{*}}-1\). Using Lemmas 2.2 and 3.2, we show that \(N_{t_{1},k_{1}}^{i^{*}} w_{1/2,k_{1}}^{}(n-1)\), hence \(t_{1} n^{5/6}\) for \(n\) large enough (see Appendix D.2). Then, we need to link \(N_{t_{1},i^{*}}^{i^{*}}\) to \((n-1)/2\). When \(w_{1/2,k_{1}}^{}\) is small, (5) can be true at \(t_{1}=n^{5/6}\), hence there is no hope to show that \(t_{1}=n-o(n)\). To circumvent this problem, we link \(N_{t_{1},i^{*}}^{i^{*}}\) to \(N_{t_{1},k_{1}}^{i^{*}}\) thanks to Lemma 2.2, and use that

\[,i^{*}}^{i^{*}}}+,k_{1}}^{i^{*}}} (2+,k_{1}}^{i^{*}}})(,k_{1}} ^{i^{*}}}{N_{t_{1},i^{*}}^{i^{*}}}+1) 2(1+)^{2}(2+1/w_{1/2,k_{1}}^{ })\,\]for \(n>T_{}(w_{-})\) with \(T_{}(w_{-})\{C_{}^{1,2},(2/( w_{-})+1)^{6},(2/ )^{1.2}\}\) (Lemmas D.10 and D.11), where \(w_{-}=_{i i}w_{1/2,i}^{*}>0\) lower bounds \(w_{1/2,k_{1}}^{*}\). This concludes the proof for \(w_{0}=0\). The (sub-optimal) multiplicative factor \(2\) in \(T_{0}()\) comes from the inequality (6). To remove it, we need to control the deviation between the empirical proportion of arm \(i\) and \(w_{1/2,i}^{*}\) for all \(i[K]\). Nevertheless, TTUCB is asymptotically \(1/2\)-optimal (Theorem 2.3).

Refined analysisFor \(w_{0}(0,(K-1)^{-1}]\), we clip \(_{i i^{*}}w_{1/2,i}^{*}\) by \(w_{0}/2\) (see Appendix D). Our method can be used to analyze other algorithms, and it improves existing results on APT.

### Beyond Gaussian distributions

Theorems 2.3 and 2.4 hold for sub-Gaussian r.v. thanks to direct adaptations of concentration results (Lemmas 2.1, E.2 and E.5). The situation is akin to the regret bound of UCB: it holds for any sub-Gaussian, but it is close to optimality in a distribution-dependent sense only for Gaussians. However, if the focus is on asymptotically \(\)-optimal algorithms, then it is challenging to express the characteristic time \(T^{*}()\) for the non-parametric class of sub-Gaussian distributions.

The TTUCB algorithm can also be defined for more general distributions such as single-parameter exponential families or bounded distributions. It is only a matter of adapting the definition of the UCB leader and the TC challenger. For bounded distributions, the UCB leader was studied in  and the TC challenger was analyzed in . Leveraging their unified analysis of Top Two algorithms with our tracking-based results, we can show asymptotic \(\)-optimality of TTUCB for bounded distributions and single-parameter exponential families with sub-exponential tails. We believe that non-asymptotic guaranties could be obtained for more general distributions, but it will come at the price of more technical arguments and less explicit non-asymptotic terms.

### Generic regret minimizing leader

Our non-asymptotic analysis highlights that any regret minimization algorithm that selects the arm \(i^{*}\) except for a sublinear number of times (Property 1) can be used as leader with the TC challenger.

_Property 1_.: There exists \((}_{n})_{n}\) with \(_{n}_{}(}_{n}^{})<+\) and a function \(h\) with \(h(n)=(n^{})\) for some \((0,1)\) such that under event \(}_{n}\), \(L_{n,i^{*}} n-1-h(n)\).

For asymptotic guarantees, the sufficient properties on the leader from  are weaker since they are even satisfied by the greedy choice \(B_{n}=_{n}\). While Top Two algorithms were introduced by  to adapt Thompson Sampling to BAI, we have shown that other regret minimization algorithms can be used: _the Top Two method is a generic wrapper to convert any regret minimization algorithm into a best arm identification strategy_.

The regret of an algorithm at time \(n\), \(_{n}=_{i i^{*}}_{i}N_{n,i}\), is almost always studied through its expectation \([_{n}]\). This is however not sufficient for our application. We need to prove that with high probability, \(N_{n,i}\) is small for all arm \(i i^{*}\). Such guarantees are known for UCB  and ETC , but are yet unknown for Thompson Sampling. We cannot in general obtain a good enough bound on \(N_{n,i}\) from a bound on \([_{n}]\). However, we can if we have high probability bounds on \(_{n}\). Suppose that a regret minimization algorithm \(_{1}\) satisfies Property 2 and is independent of the horizon \(n\).

_Property 2_.: There exists \(s>1\), \((0,1)\), \((_{n,})_{(n,)}\) with \(_{n}_{}[}_{n,n^{-s}}^{}]<+\) and a function \(h\) with \(h(n,n^{-s})=(n^{})\) such that under event \(_{n,}\), \(_{n} h(n,)\).

Let \(_{2}\) be the algorithm \(_{1}\) used in a Top Two procedure, but which uses only the observations obtained at times \(n\) such that \(I_{n}=B_{n}\) and discards the rest. Let \(}_{n}=_{n,n^{-s}}\) and \(_{}=_{i i^{*}}_{i}\). Then, under \(}_{n}\), \(_{2}\) satisfies \(_{i i^{*}}N_{n,i}^{i} h(n,n^{-s})/_{}\) and Lemma 2.2 yields \(N_{n,i^{*}}^{i^{*}}(n-1)-h(n,n^{-s})/_{}-K/2\). Therefore, Property 1 holds for \(}_{n}\) and \(h(n)=(h(n,n^{-s})/_{}+K/2+1)/\). Given a specific algorithm, a finer analysis could avoid discarding information by using \(_{1}\) with every observations.

### Adaptive proportions

Given a fixed allocation \(\), any Top Two algorithm can at best be asymptotically \(\)-optimal. Since the optimal allocation \(^{}_{(0,1)}T_{}^{}()\) is unknown, it should be learned from the observations by a Top Two algorithm using an adaptive proportion \(_{n}\) at time \(n\). Recently,  proposes IDS to choose \(_{n}\) in an adaptive manner. For BAI with Gaussian observations, IDS yields \(_{n}=N_{n,C_{n}}/(N_{n,B_{n}}+N_{n,C_{n}})\). Let \(_{n}^{i}:=}_{t[n-1]}_{t}\) (\(B_{t}=i\)) be the average proportion when arm \(i\) was the leader before time \(n\). Tracking with IDS requires to use \(_{n+1}^{B_{n}}\) instead of \(\). Using the analysis of , it is reasonable to believe that one could obtain asymptotic optimality of TTUCB with IDS. However it is not clear how to adapt the non-asymptotic analysis since it heavily relies on \(\) being fixed and bounded away from \(\{0,1\}\). Experiments with IDS are available in Appendix G.2.1.

## 4 Experiments

In the moderate regime (\(=0.1\)), we assess the empirical performance of TTUCB with bonus \(g_{m}\) and concentration parameters \(s==1.2\). As benchmarks, we compare our algorithm with three sampling-based Top Two algorithms: TTTS, T3C and \(\)-EB-TCI. In addition, we consider Track-and-Stop (TaS) , FWS , DKM , LUCB  and uniform sampling. At time \(n\), the LUCB algorithm computes a leader and a challenger, then sample them both (see Appendix G.1). To provide a clear comparison with Top Two algorithms, we define a new \(\)-LUCB algorithm which sample the leader with probability \(\), else sample the challenger. At the exception of LUCB and \(\)-LUCB which have their own stopping rule, all algorithms uses the stopping rule (1) with the heuristic threshold \(c(n,)=((1+ n)/)\). Even though this choice is not sufficient to prove \(\)-correctness, it yields an empirical error which is several orders of magnitude lower than \(\). Top Two algorithms and \(\)-LUCB use \(=1/2\). To allow for a fair numerical comparison, LUCB and \(\)-LUCB use \(}\) as bonus, which is too tight to yield valid confidence intervals. Supplementary experiments are available in Appendix G.

Random instancesWe assess the performance on \(5000\) random Gaussian instances with \(K=10\) such that \(_{1}=0.6\) and \(_{i}([0.2,0.5])\) for all \(i 1\). Numerically, we observe \(w^{}()_{i^{}} 1/3 0.02\) (mean \(\) std). In Figure 1(a), we see that TTUCB performs on par with existing Top Two algorithms, and slightly outperforms TaS and FWS. Our algorithm achieves significantly better result than DKM, LUCB, \(1/2\)-LUCB and uniform sampling. The CPU running time is reported in Table 4, and the observed empirical errors before stopping is displayed in Figure 3 (Appendix G.2).

Larger sets of armsWe evaluate the impact of larger number of arms. The "\(1\)-sparse" scenario of  sets \(_{1}=1/4\) and \(_{i}=0\) for all \(i 1\), i.e. \(H()=32(K-1)\) (see Appendix G.2 for other instances). We consider algorithms with low computational cost. In Figure 1(b), all algorithms have the same linear scaling in \(K\) (i.e. in \(H()\)). Faced with an increase in the number of arms, the TS leader used in T3C appears to be more robust than the UCB leader in TTUCB. This is a common feature of UCB algorithms which have to overcome the bonus of sub-optimal arms.

Figure 1: Empirical stopping time on (a) random instances (\(K=10\)) and (b) “\(1\)-Sparse” instances.

Conclusion

In this paper, we have shown the first non-asymptotic upper bound on the expected sample complexity of a Top Two algorithm, which holds for any error level and for any instance having a unique best arm. Furthermore, we have demonstrated that the TTUCB algorithm achieves competitive empirical performance compared to other algorithms, including Top Two methods.

While our guarantees hold for a fixed proportion \(\) allocated to the leader,  recently introduced IDS to define an adaptive proportion \(_{n}\) at time \(n\) and show asymptotic optimality for Gaussian distributions. Deriving guarantees for IDS for single-parameter exponential families is a challenging open problem. Finally, Top Two algorithms are a promising method to tackle complex settings. While heuristics exist for some structured bandits such as Top-\(k\), it would be interesting to efficiently adapt Top Two methods to deal with sophisticated structure, e.g. linear bandits.