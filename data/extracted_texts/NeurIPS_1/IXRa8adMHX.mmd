# On the Optimal Time Complexities in Decentralized Stochastic Asynchronous Optimization

Alexander Tyurin

KAUST, AIRI, Skoltech

King Abdullah University of Science and Technology, Thuwal, Saudi Arabia AIRI, Moscow, Russia Skoltech

Skolkovo Institute of Science and Technology, Moscow, Russia

Peter Richtarik

KAUST

King Abdullah University of Science and Technology, Thuwal, Saudi Arabia AIRI, Moscow, Russia Skolkovo Institute of Science and Technology, Moscow, Russia

###### Abstract

We consider the decentralized stochastic asynchronous optimization setup, where many workers asynchronously calculate stochastic gradients and asynchronously communicate with each other using edges in a multigraph. For both homogeneous and heterogeneous setups, we prove new time complexity lower bounds under the assumption that computation and communication speeds are bounded. We develop a new nearly optimal method, Fragile SGD, and a new optimal method, Ameile SGD, that converge under arbitrary heterogeneous computation and communication speeds and match our lower bounds (up to a logarithmic factor in the homogeneous setting). Our time complexities are new, nearly optimal, and provably improve all previous asynchronous/synchronous stochastic methods in the decentralized setup.

## 1 Introduction

We consider the smooth nonconvex optimization problem

\[_{x^{d}}f(x):=_{_{}} [f(x;)]}, \]

where \(f\,:\,^{d}_{},\) and \(_{}\) is a distribution on a non-empty set \(_{}.\) For a given \(>0,\) we want to find a possibly random point \(,\) called an \(\)-stationary point, such that \([\| f()\|^{2}].\) We analyze the heterogeneous setup and the convex setup with smooth and non-smooth functions in Sections C and D.

### Decentralized setup with times

We investigate the following decentralized asynchronous setup. Assume that we have \(n\) workers/nodes with the associated computation times \(\{h_{i}\},\) and communications times \(\{_{i j}\}.\) It takes less or equal to \(h_{i}[0,]\) seconds to compute a stochastic gradient by the \(i^{}\) node, and less or equal \(_{i j}[0,]\) seconds to send _directly_ a vector \(v^{d}\) from the \(i^{}\) node to the \(j^{}\) node (it is possible that \(h_{i}=\) and \(_{i j}=\)). All computations and communications can be done asynchronously and in parallel. We would like to emphasize that \(h_{i}[0,]\) and \(_{i j}[0,]\) are only upper bounds, and the real and effective computation and communication times can be arbitrarily heterogeneous and random. For simplicity of presentation, we assume the upper bounds are static; however, in Section 5.5, we explain that our result can be trivially extended to the case when the upper bounds are dynamic.

We consider any _weighted directed multigraph_ parameterized by a vector \(h^{n}\) such that \(h_{i}[0,],\) and a matrix of distances \(\{_{i j}\}_{i,j}^{n n}\) such that \(_{i j}[0,]\) for all \(i,j[n]\) and\(_{i i}=0\) for all \(i[n]\). Every worker \(i\) is connected to any other worker \(j\) with two edges \(i j\) and \(j i\). For this setup, it would be convenient to define _the distance of the shortest path from worker \(i\) to worker \(j\)_ :

\[_{i j}:=_{ P_{i j}}_{(u,v)}_{u v}[0,], \] \[P_{i j}:=\{[(k_{1},k_{2}),,(k_{m},k_{m+1}) ]\, m\, p[m+1]\, k_{p}[n],.\] \[. k_{1}=i,k_{m+1}=j, j\{2, ,m\}\,k_{j-1} k_{j} k_{j+1}\}\]

is the set of all possible paths without loops from worker \(i\) to worker \(j\) for all \(i,j[n]\). One can easily show that the triangle inequality \(_{i j}_{i k}+_{k j}\) holds for all \(i,j,k[n]\). Note that \(_{i j}_{i j}\) for all \(i,j[n]\). It is important to distinguish \(_{i j}\) and \(_{i j}\) because it is possible that \(_{i j}<_{i j}=\) if workers \(i\) and \(j\) are connected by an edge \(_{i j}=,\) and there is a path through other workers (see Fig. 1).

We work with the following standard assumption from smooth nonconvex stochastic optimization literature.

**Assumption 1**.: \(f\) _is differentiable and \(L\)-smooth, i.e., \(\| f(x)- f(y)\| L\,\|x-y\|\, x,y^{d}\)._

**Assumption 2**.: _There exist \(f^{*}\) such that \(f(x) f^{*}\) for all \(x^{d}\)._

**Assumption 3**.: _For all \(x^{d}\), stochastic gradients \( f(x;)\) are unbiased and \(^{2}\)-variance-bounded, i.e., \(_{}[ f(x;)]= f(x)\) and \(_{}[\| f(x;)- f(x)\|^{2}]^{2},\) where \(^{2} 0\). We also assume that computation and communication times are statistically independent of stochastic gradients._

## 2 Previous Results

### Time complexity with one worker

For the case when \(n=1\) and \(_{1 1}=0,\) convergence rates and time complexities of problem (1) are well-understood. It is well-known that the stochastic gradient method (SGD), i.e., \(x^{k+1}=x^{k}- f(x^{k};^{k})\), where \(\{^{k}\}\) are i.i.d. from \(_{}\), has the optimal _oracle complexity_\((}{{}}+L}}{{ ^{2}}})\)(Ghadimi and Lan, 2013; Arjevani et al., 2022). Assuming that the computation time of one stochastic gradient is bounded by \(h_{1},\) we can conclude that the optimal time complexity is

\[T_{}^{=0}:=(h_{1}(+L}{^{2}})) \]

seconds in the worst case.

### Parallel optimization without communication costs

Assume that \(n>1\) and \(_{i j}=0\) for all \(i,j[n],\) and computation times of stochastic stochastic gradients are arbitrarily heterogeneous. The simplest baseline method in this setup is Minibatch SGD,

Figure 1: _On the left:_ an example of a multigraph with \(n=6\). The edges with \(_{i j}=\) are omitted. The shortest distance between nodes \(5\) and \(3\) is \(_{5 3}=_{5 1}+_{1 2}+_{2 3}\). Note that \(_{5 3}=\). _On the right:_ an example of a spanning tree that illustrates the shortest paths from every node to node \(3\). The shortest distance between nodes \(6\) and \(3\) is \(_{6 3}=\) because \(_{6 i}=\) for all \(i 6\).

i.e.,

\[x^{k+1}=x^{k}-_{i=1}^{n} f(x^{k};_{i}^{k}), \]

where \(\{_{i}^{k}\}\) are i.i.d. from \(_{}\) and the gradient \( f(x^{k};_{i}^{k})\) is calculated in worker \(i\) in parallel. This method waits for stochastic gradients from all workers; thus, it is not robust to "stragglers" and in the worst case the time complexity of such an algorithm is

\[T_{}^{r=0}:=(_{i[n]}h_{i}( {L}{e}+L}{ne^{2}})),\]

which depends on the time \(_{i[n]}h_{i}\) of the slowest worker. There are many other more advanced methods including Picky SGD(Cohen et al., 2021), Asynchronous SGD(e.g., (Recht et al., 2011; Nguyen et al., 2018; Mishchenko et al., 2022; Koloskova et al., 2022)), and Rennala SGD(Tyurin and Richtarik, 2023) that are designed to be robust to workers' chaotic computation times. Under the assumption that the computation times of the workers are heterogeneous and bounded by \(\{h_{i}\},\)Tyurin and Richtarik (2023) showed that Rennala SGD is the first method that achieves the optimal time complexity

\[T_{}^{r=0}:=(_{m[n]}[( {m}_{i=1}^{m}}})^{-1}(+L}{me^{2}})]), \]

where \(\) is a permutation that sorts \(h_{i}:h_{_{1}} h_{_{n}}\). For instance, one can see that \(T_{}^{r=0} T_{}^{r=0}\) for all parameters.

### Parallel optimization with communication costs \(_{i j}\)

We now consider the setup where workers' communication times can not be ignored. This problem leads to a research field called _decentralized optimization_. This setup is the primary case for us. All \(n\) workers calculate stochastic gradients in parallel and communicate with each other. Numerous works consider this setup, and we refer to Yang et al. (2019); Koloskova (2024) for detailed surveys. Typically, methods in this setting use the gossip matrix framework (Duchi et al., 2011; Shi et al., 2015; Koloskova et al., 2021) and get an _iteration converge rate_ that depends on the spectral gap of a mixing matrix. However, such rates do not give the physical time of algorithms (see also Section B).

Let us consider a straightforward baseline: Minibatch SGD. We can implement (4) in a way that all workers calculate one stochastic gradient (takes at most \(_{i[n]}h_{i}\) seconds) and then aggregate them to one _pivot worker_\(j^{*}\) (takes at most \(_{i[n]}_{i j^{*}}\) seconds). Then, pivot worker \(j^{*}\) calculatesa new point \(x^{k+1}\) and broadcasts it to all workers (takes \(_{i[n]}_{j^{*} i}\) seconds). One can easily see that the time complexity of such a procedure is4

\[T_{}:=(\{_{i,j[n]}_{i j},_{i [n]}h_{i}\}(+L }{n^{2}})). \]

We can analyze any other asynchronous decentralized method, which will be done with more advanced methods in Section 5.4.

But what is the best possible (optimal) time complexity we can get in the setting from Section 1.1?

Unlike the setups from Sections 2.1 and 2.2 when the communication times are zero (\(_{i j}=0\) for all \(i,j[n]\)), the optimal time complexity and an optimal method for the case \(_{i j} 0\) for all \(i,j[n]\) are not known. Our main goal in this paper is to solve this problem.

## 3 Contributions

We consider the class of functions that satisfy the setup and the assumptions from Section 1.1 and show that (informally) it is impossible to develop a method that will converge faster than (7) seconds. Next, we develop a new asynchronous stochastic method, Fragile SGD, that is _nearly_ optimal (i.e., almost matches this lower bound; see Table 1 and Corollary 1). This is the first such method. It provably improves on Asynchronous SGD (Even et al., 2024) and all other synchronous and asynchronous methods (Bornstein et al., 2023). We also consider the heterogeneous setup (see Table 2 and Section C), where we discover the optimal time complexity by proving another lower bound and developing a new optimal method, Ameile SGD, with weak assumptions. The developed methods can guarantee the _iteration complexity_\((}{{}})\) with arbitrarily heterogeneous random computation and communication times (Theorems 4 and 8). Our findings are extended to the convex setup in Section D, where we developed new accelerated methods, Accelerated Fragile SGD and Accelerated Ameile SGD.

## 4 Lower Bound

In order to construct our lower bound, we consider any (zero-respecting) method that can be represented by Protocol 1. This protocol captures all virtually distributed synchronous and asynchronous methods, such as Minibatch SGD, SWIFT (Bornstein et al., 2023), Asynchronous SGD (Even et al., 2024), and Gradient Tracking (Koloskova et al., 2021).

For all such methods we prove the following theorem.

**Theorem 1** (Lower Bound; Simplified Presentation of Theorem 19).: _Consider Protocol 1 with \( f(;)\). We take any \(h_{i} 0\) and \(_{i j} 0\) for all \(i,j[n]\) such that \(_{i j}_{i k}+_{k j}\) for all \(i,k,j[n]\). We fix \(L,,,^{2}>0\) that satisfy the inequality \(<cL\) for some universal constant \(c\). For any (zero-respecting) algorithm, there exists a function \(f,\) which satisfy Assumptions 1, 2 and \(f(0)-f^{*}\), and a stochastic gradient mapping \( f(;),\) which satisfies Assumption 3, such that the required time to find \(\)-solution is_

\[(_{j [n]}t^{*}(}}{{}},[h_{i}]_{i=1}^{n},[_{i  j}]_{i=1}^{n})) \] \[\,2}}{{}}( _{j[n]}_{k[n]} \{\{_{_{j,k} j},h_{_{j,k}}\},}{ }(_{i=1}^{k}}})^{-1} \}), \]

_where, for \(j[n],\)\(_{j,}\) is a permutation that sorts \(\{\{_{i j},h_{i}\}\}_{i=1}^{n},\) i.e., \(\{_{_{j,1} j},h_{_{j,1}}\}\{_{_{j,n } j},h_{_{j,n}}\}.\)_

```
1:Init \(S_{i}=\) (all available information) on worker \(i\) for all \(i[n]\)
2:Run the following two loops in each worker in parallel
3:while True do
4: Calculate a new point \(x_{i}^{k}\) based on \(S_{i}\) (takes \(0\) seconds)
5: Calculate a stochastic gradient \( f(x_{i}^{k};)\) (or \( f_{i}(x_{i}^{k};)\)) \(_{}\) (takes \(h_{i}\) seconds)
6: Atomic add \( f(x_{i}^{k};)\) (or \( f_{i}(x_{i}^{k};)\)) to \(S_{i}\) (atomic operation, takes \(0\) seconds)
7:endwhile
8:while True do
9: Send(a) any vector from \(^{d}\) based on \(S_{i}\) to any worker \(j\) and go to the next step of this loop without waiting (takes \(_{i j}\) seconds to send; worker \(j\) adds this vector to \(S_{j}\))
10:endwhile (a): When we prove the lower bounds, we allow algorithms to send as many vectors as they want in parallel from worker \(i\) to worker \(j\) for all \(i j[n]\).
```

**Protocol 1** Simplified Presentation of Protocol 8

The intuition and meaning of the formula (8) is discussed in Section 5.2. Note that if we take \(n=1\) and \(_{1 1}=0\) our lower bound reduces to the lower bound (3) up to a log factor. Moreover, if we take \(n>1\) and \(_{i j}=0\) for all \(i,j[n],\) then (8) reduces to (5) up to a log factor. Thus, (8) is nearly consistent with the lower bounds from (Arjevani et al., 2022; Tyurin and Richtarik, 2023). We get an extra \( n\) factor due to the generality of our setup. The reason is technical, and we explain it in Section E.5. In a nutshell, the lower problem reduces to the analysis of the concentration of the time series \(y^{T}:=_{j[n]}y_{j}^{T}\) and \(y_{j}^{T}:=_{i[n]}\{y_{i}^{T-1}+h_{i}_{i}^{T}+_{i j} \},\) where \(y_{i}^{0}=0\) for all \(i[n],\) and \(\{_{i}^{k}\}\) are i.i.d. geometric random variables. This analysis is not trivial due to the \(_{i[n]}\) operations. Virtually all previous works that analyzed lower bounds did not have such a problem because they analyzed time series with a sum structure (e.g., \(^{T}:=^{T-1}+^{T},\) where \(\{^{k}\}\) are some random variables, and \(^{0}=0\)).

Let us define an auxiliary function to simplify readability.

**Definition 2** (Equilibrium Time).: A mapping \(t^{*}:_{ 0}_{ 0}^{n}_{ 0}^{n} _{ 0}\) with inputs \(s\) (scalar), \([h_{i}]_{i=1}^{n}\) (vector), and \([_{i}]_{i=1}^{n}\) (vector) is called the _equilibrium time_ if it is defined as follows. Find a permutation5\(\) that sorts \(\{_{i},h_{i}\}\) as \(\{_{_{1}},h_{_{1}}\}\{_{_{ n}},h_{_{n}}\}.\) Then the mapping returns the value

\[t^{*}(s,[h_{i}]_{i=1}^{n},[_{i}]_{i=1}^{n})_{k[n]} \{\{_{_{k}},h_{_{k}}\},s(_{i=1}^{k} }})^{-1}\}[0,]. \]

## 5 New Method: Fragile SGD

We introduce a novel optimization method characterized by time complexities that closely align with the lower bounds established in Section 4. Our algorithms leverage _spanning trees_. A spanning tree is a tree (undirected unweighted graph) encompassing all workers. The edges of spanning trees are _virtual_ and not related to the edges defined in Section 1.1 (see Fig. 1).

[MISSING_PAGE_FAIL:6]

**Definition 3** (mapping \(_{T,j}(i)\)).: Take a spanning tree \(T\) and fix any worker \(j[n]\). For \(i=j,\) we define \(_{T,j}(i)=0\). For all \(i j[n]\), we define \(_{T,j}(i)\) as the index of the next worker on the path of the spanning tree \(T\) from worker \(i\) to worker \(j\).

Our new method, Fragile SGD, is presented in Algorithms 2, 4, and 3. While Fragile SGD seems to be lengthy, the idea is pretty simple. All workers do three jobs in parallel: calculate stochastic gradients, receive vectors, and send vectors through spanning trees. A pivot worker aggregates all stochastic gradients in \(g^{k}\) and, at some moment, does \(x^{k+1}=x^{k}- g^{k}\). The algorithms formalize this idea.

Algorithm 2 requires a starting point \(x^{0}\), a stepsize \(\), a batch size \(S\), the index \(j^{*}\) of a pivot worker, and spanning trees \(\) and \(_{}\) for the input. We need two spanning \(_{}\) and \(\) trees because, in general, the fastest communication of a vector from \(j^{*}\) to \(i\) and from \(i\) to \(j^{*}\) should be arranged through two different paths. Algorithm 2 starts \(n+1\) processes running in parallel. Note that the pivot worker \(j^{*}\) runs two parallel processes, called Process \(0\) and Process \(j^{*}\), and any other worker \(i\) runs one Process \(i\). Process \(0\) broadcasts a new point \(x^{k}\) through \(_{}\) to all other processes and goes to the loop where it waits for messages from Process \(j^{*}\). Process \(i\) starts three functions that will be running in parallel: i) the first function's job is to receive a new point, broadcast it further, and start the calculation of stochastic gradients, ii) the second function receives stochastic gradients from all previous processes that are sending vectors to worker \(j^{*}\), iii) the third function sends vectors the next worker on the path to \(j^{*}\). By the definition of \(_{,j^{*}}()\), all calculated stochastic vectors are sent to worker \(j^{*}\), where they are first aggregated in Process \(j^{*}\), and then, since \(_{,j^{*}}^{*}(j^{*})=0,\) Process \(j^{*}\) will send \(g^{k}_{j^{*},}\) to Process \(0\). This process waits for the moment when the number of stochastic gradients \(s^{k}\) aggregated in \(g^{k}\) is greater or equal to \(S\). When it happens, the loop stops, and Process \(0\) does a gradient-like step. The structure of the algorithm and the idea of spanning trees resemble the ideas from (Vogels et al., 2021; Tyurin and Richtarik, 2023). The main observation is that this algorithm is equivalent to \(x^{k+1}=x^{k}-}_{i=1}^{s^{k}} f(x^{k};^{k}_{i}),\) where \(s^{k} S\) and \(\{^{k}_{i}\}\) are i.i.d. samples. Note that all stochastic gradients calculated at points \(x^{0},,x^{k-1}\) will be ignored in the \(k^{}\) iteration of Algorithm 3.

**Theorem 4**.: _Let Assumptions 1, 2, and 3 hold. We take \(=1/2L,\) batch size \(S=\{}}{{}},1\},\) any pivot worker \(j^{*}[n],\) and any spanning trees \(\) and \(_{}\) in Algorithm 2. For all \(K 16L/,\) we get \(_{k=0}^{K-1}[\| f(x^{k})\|^{2} ].\)_

The proof is simple and uses standard techniques from (Lan, 2020; Khaled and Richtarik, 2022). The result of the theorem holds even if \(h_{i}=\) and \(_{i j}=\) for all \(i,j[n]\) because \(h_{i}\) and \(_{i j}\) are only upper bounds on the real computation and communications speeds. In Algorithm 3, each iteration \(k\) can be arbitrarily slow, and still, the result of Theorem 4 holds and the method converges after \(O(L/)\) iterations. The next result gives time complexity guarantees for our algorithm.

**Theorem 5**.: _Consider the assumptions and the parameters from Theorem 4. For any pivot worker \(j^{*}[n]\) and spanning trees \(\) and \(_{}\), Algorithm 2 converges after at most_

\[(t^{*}(}}{{ }},[h_{i}]_{i=1}^{n},[_{i j^{*}}+_{j^{*} i}]_{i=1}^{n})) \]

_seconds, where \(_{i j^{*}}\) (\(_{j^{*} i}\)) is an upper bound on the times required to send a vector from worker \(i\) to worker \(j^{*}\) (from worker \(j^{*}\) to worker \(i\)) along the spanning tree \(_{}\))._

Note that our method does not need the knowledge of \(\{h_{i}\}\) and \(\{_{i j}\}\) to guarantee the time complexity rate, and it _automatically_ obtains it.

**Corollary 1**.: _Consider the assumptions and the parameters from Theorem 5. Let us take a pivot worker \(j^{*}=_{j[n]}t^{*}(}}{{}},[h _{i}]_{i=1}^{n},[_{i j}+_{j i}]_{i=1}^{n}),\) and a spanning tree \(\) (spanning tree \(_{}\)) that connects every worker \(i\) to worker \(j^{*}\) (worker \(j^{*}\) to every worker \(i\)) with the shortest distance \(_{i j^{*}}\) (\(_{j^{*} i}\)). Then Algorithm 2 converges after at most_

\[T_{*} :=(_{j[n]}t^{*}( }}{{}},[h_{i}]_{i=1}^{n},[_{i j}+_{j  i}]_{i=1}^{n})) \] \[}}{{=}}(_{j[n]}_{k[n]}\{\{_{ _{j,k} j}+_{j_{j,k}},h_{_{j,k}}\},}{ }(_{i=1}^{k}}})^{-1} \}) \]

_seconds, where, for all \(j[n],\)\(_{j,}\) is a permutation that sorts \(\{\{_{i j}+_{j i},h_{i}\}\}_{i=1}^{n}.\)_This corollary has better time complexity guarantees than Theorem 5 because, by the definition of \(_{i j},_{i j}_{i j}\) for al \(i,j[n]\). However, it requires the particular choice of a pivot worker and spanning trees.

### Discussion

Comparing the lower bound (7) with the upper bound (11), one can see that Fragile SGD has a _nearly optimal time complexity_. If we ignore the \( n\) factor in (7) and assume that \(_{i j}=_{j i}\) for all \(i,j[n],\) which is a weak assumption in many applications, then Fragile SGD is optimal.

Unlike most works (Even et al., 2024; Lian et al., 2018; Koloskova et al., 2021) in the decentralized setting, our time complexity guarantees _do not_ depend on the spectral gap of the mixing matrix that defines the topology of the multigraph. The structure of the multigraph is coded in the times \(\{_{i j}\}\). We believe that this is an advantage of our guarantees since (11) defines a physical time instead of an iteration rate that depends on the spectral gap.

### Interpretation of the upper and lower bounds (11) and (7)

One interesting property of our algorithm is that _some workers will potentially never contribute to optimization because either their computations are too slow or communication times to \(j^{*}\) are too large_. Thus, only a subset of the workers should work to get the optimal time complexity!

Assume in this subsection that the computation and communication are fixed to \(\{h_{i}\}\) and \(\{_{i j}\}\). One can see that (12) is the \(_{j[n]}_{k[n]}\) over some formula. In view of our algorithm, an index \(j^{*}\) that minimizes in (12) is the index of a pivot worker that is the most "central" in the multigraph. An index \(k^{*}\) that minimizes \(_{k[n]}\) defines a set of workers \(\{_{j^{*},1},_{j^{*},k^{*}}\}\) that can potentially contribute to optimization. The algorithm and and the time complexity _will not_ depend on workers \(\{_{j^{*},k^{*}+1},_{j^{*},n}\}\) because they are too slow or they are too far from worker \(j^{*}\). Thus, up to a constant factor, we have \(T_{*}=}{{}}\{_{_{j^{*},k^ {*}} j^{*}}+_{j^{*}_{j^{*},k^{*}}},h_{_{j^{*},k^{*}}}\}, ^{2}\!/\!_{i=1}^{k^{*}}}{{h_{_{j^{*},i^ {*}}}}}^{-1}},\) where \(_{_{j^{*},k^{*}} j^{*}}+_{j^{*}_{j^{*},k^{*}}}\) is the time required to communicate with the farthest worker that can contribute to optimization, \(h_{_{j^{*},k^{*}}}\) is the computation time of the slowest worker that can contribute to optimization, and \(}}{{}}_{i=1}^{k^{*}}}{ {h_{_{j^{*},i}}}}^{-1}\) is the time required to "eliminate" enough noise before the algorithm does an update of \(x^{k}\).

### Limitations

To get the nearly optimal complexity, it is crucial to select the right pivot worker \(j^{*}\) and spanning trees according to the rules of Corollary 1, which depend on the knowledge of the bounds of times. For now, we believe that is this a price for the optimality. Note that Theorem 5 does not require this knowledge and it works with any \(j^{*}\) and any spanning tree; thus, we can use any heuristic to estimate an optimal \(j^{*}\) and optimal spanning trees. One possible strategy is to estimate the performance of workers and the communication channels using load testings.

### Comparison with previous methods

Let us discuss the time complexities of previous methods. Note that none of the previous methods can converge faster than (7) due to our lower bound. First, consider (6) of Minibatch SGD. This time complexity depends on the slowest computation time \(_{i[n]}h_{i}\) and the slowest communication times \(_{i,j[n]}_{i j}\). In the asynchronous setup, it is possible that one the workers is a straggler, i.e., \(_{i[n]}h_{i},\) and Minibatch SGD can be arbitrarily slow. Our time complexities (10) and (12) are robust to stragglers, and ignore them. Assume the last worker \(n\) is a straggler and \(h_{n}=,\) then one can take permutations with \(_{j,n}=n\) for all \(j[n],\) and the minimum operator \(_{k[n]}\) in (12) will not choose \(k=n\) because \(\{_{_{j,n} j}+_{j_{j,n}},h_{_{j,n}}\}=\) for all \(j[n]\).

We now consider a recent work by Even et al. (2024), where the authors analyzed Asynchronous SGD in the decentralized setting. In the homogeneous setting, their converge rate depends on the maximum compute delay and, thus, is not robust to stragglers. For the case \(_{i j}=0,\) our time complexity (11) reduces to (5). At the same time, it was shown (Tyurin and Richtarik, 2023) that the time complexity of Asynchronous SGD for \(_{i j}=0\) is strongly worse than (5); thus, the result by Even et al. (2024) is suboptimal in our setting even if \(_{i j}=0\) for all \(i,j[n]\). The papers by Bornstein et al. (2023); Lian et al. (2018) also consider the same setting, and they share a logic in that they sample a random worker and _wait_ while it is calculating a stochastic gradient. If one of the workers is a straggler, they can wait arbitrarily long, while our method automatically ignores slow computations.

### Time complexity with dynamic bounds

We can easily generalize Theorem 5 to the case when bounds on the times are not static.

**Theorem 6**.: _Consider the assumptions and the parameters from Theorem 4. In each iteration \(k\) of Algorithm 3, the computation times of worker \(i\) are bounded by \(h_{i}^{k}\). Let us fix any pivot worker \(j^{*}[n]\) and any spanning trees \(\) and \(}}\). Then Algorithm 2 converges after at most_

\[(_{k=0}^{ 16L/}t^{*}(}}{{}},[h_{i}^{k}]_{i=1}^{n},[_{i j^{*}}^{k}+_{ j^{*} i}^{k}]_{i=1}^{n})) \]

_seconds, where \(_{i j^{*}}^{k}\) (\(_{j^{*} i}^{k}\)) is an upper bound on times required to send a vector from worker \(i\) to worker \(j^{*}\) (from worker \(j^{*}\) to worker \(i\)) along the spanning tree \(\) (spanning tree \(_{}\)) in iteration \(k\) of Algorithm 3._

This result is more general than (10), and it shows that our method is robust to changing computation \(\{h_{i}^{k}\}\) and communication \(\{_{i j}^{k}\}\) times bounds during optimization processes. For instance, worker \(n\) can have either slow computation or communication to \(j^{*}\) in the first iteration, i.e., \(\{h_{n}^{1},_{n j^{*}}^{1},_{j^{*} n}^{1}\}\), then our method will ignore it, but if \(\{h_{n}^{2},_{n j^{*}}^{2},_{j^{*} n}^{2}\}\) is small in the second iteration, then our method can potentially use the stochastic gradients from worker \(n\).

## 6 Example: Line or Circle

Let us consider Line graphs where we can get more explicit and interpretable formulas for (11). We analyze ND-Mesh, ND-Torus, and Star graphs in Section A. Surprisingly, even in some simple cases like Line or Star graphs, as far as we know, we provide new time complexity results and insights. In Section J, we show that our theoretical results are supported by computational experiments.

We take a Line graph with the computation speeds \(h_{i}=h\) for all \(i[n],\) and the communication speeds of the edges \(_{i i+1}=_{i+1 i}=\) for all \(i[n-1]\) and \(_{i j}=\) for all other \(i,j[n]\). One can easily show the time required to send a vector between two workers \(i,j[n]\) equals \(_{i j}=_{j i}=|i-j|.\) See an example with \(n=7\) in Fig. 2. We can substitute these values to (11) and get

\[T_{}=_{j[n]}_{k[n]} \{\{|j-_{j,k}|,h\},h}{ k} \}, \]

where \(_{j,1}=j,_{j,2},_{j,3}=j+1,j-1\) or \(_{j,2},_{j,3}=j-1,j+1\) (only for \(n-1 j 2\)) and so forth. For simplicity, assume that \(n\) is odd, then, clearly, \(j^{*}=+1\) minimizes \(_{j[n]}\) and \(T_{}=\)

\[_{d\{0,,\}}\{  d,h,h}{(2d+1)}\}[h+\{h}}{{ }},&h}/}}{{}}  1,\\ h}/}}{{}},&n> h}/}}{{}}>1,\\ h}}{{n}},&h}/ }}{{} n}\}. \]According to (15), there are three time complexity regimes: i) slow communication, i.e., \(h}}{{}}} 1,\) this inequality means that \(\) is so large, that communication between workers will not increase the convergence speed, and the best strategy is to work with only one worker!, ii) medium communication, i.e., \(n>h}}{{}}}>1,\) more than one worker will participate in the optimization process; however, _not all of them!_, some workers will not contribute since their distances \(_{j^{*}}\) to the pivot worker \(j^{*}\) are large, iii) fast communication, i.e., \(h}}{{}}} n\), all \(n\) workers will participate in optimization because \(\) is small.

As far as we know, the result (15) is new even for such a simple structure as a line. Note that these regimes are fundamental and can not be improved due to our lower bound (up to logarithmic factors). For Circle graphs, the result is the same up to a constant factor.

## 7 Heterogeneous Setup

In Section C (in more details), we consider and analyze the problem

\[_{x^{d}}f(x):=_{i=1}^{n}_{ _{i}_{i}}[f_{i}(x;_{i})]},\]

where \(f_{i}:^{d}_{_{i}}^{d}\) and \(_{i}\) are random variables with some distributions \(_{i}\) on \(_{_{i}}\). For all \(i[n],\) worker \(i\) can only access \(f_{i}\). We show that the optimal time complexity is

\[(_{i,j[n]}_{i  j},_{i[n]}h_{i},}{ne}( _{i=1}^{n}h_{i})}) \]

in the heterogeneous setting achieved by a new method, Amelie SGD (Algorithm 5). Amelie SGD is closely related to Rennala SGD but with essential algorithmic changes to make it work with heterogeneous functions. The obtained complexity (16) is worse than (12), which is expected because the heterogeneous setting is more challenging than the homogeneous setting.

## 8 Highlights of Experiments

In Section J, we present experiments with quadratic optimization problems, logistic regression, and a neural network to substantiate our theoretical findings. Here, we focus on highlighting the results from the logistic regression experiments:

On _MNIST_ dataset (LeCun et al., 2010) with \(100\) workers, Fragile SGD is much faster and has better test accuracy than Minibatch SGD.