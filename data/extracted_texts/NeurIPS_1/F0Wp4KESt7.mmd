# Hybrid Early Fusion for Multi-Modal Biomedical Representations

Konstantin Hemker

Department of Computer Science & Technology

Cambridge University

Cambridge, United Kingdom

konstantin.hemker@cl.cam.ac.uk

Nikola Simidjievski

Department Oncology

Cambridge University

Cambridge, United Kingdom

ns779@cam.ac.uk

Mateja Jannik

Department of Computer Science & Technology

Cambridge University

Cambridge, United Kingdom

mateja.jannik@cl.cam.ac.uk

###### Abstract

Technological advances in medical data collection such as high-resolution histopathology and high-throughput genomic sequencing have contributed to the rising requirement for multi-modal biomedical modelling, specifically for image, tabular, and graph data. Most multi-modal deep learning approaches use modality-specific architectures that are trained separately and cannot capture the crucial cross-modal information that motivates the integration of different data sources. This paper presents the Hybrid Early-fusion Attention Learning Network (HEALNet) - a flexible multi-modal fusion architecture, which: a) preserves modality-specific structural information, b) captures the cross-modal interactions and structural information in a shared latent space, c) can effectively handle missing modalities during training and inference, and d) enables intuitive model inspection by learning on the raw data input instead of opaque embeddings. We conduct multi-modal survival analysis on Whole Slide Images and Multi-omic data on four cancer cohorts of The Cancer Genome Atlas (TCGA). HEALNet achieves state-of-the-art performance, substantially improving over both uni-modal and recent multi-modal baselines, whilst being robust in scenarios with missing modalities.

## 1 Introduction

A key challenge in Multi-Modal Machine Learning (MMML) is _multi-modal fusion_ - the integration of heterogeneous data into a unified and informative representation  that leads to improved downstream performance, whilst reducing the dimensionality of the data. Especially considering the complex and multi-causal nature of cancer , there is an increasing requirement for ML approaches to model different scales within a biological system simultaneously to capture important information about the tumour microenvironment (TME). The utility of multi-modal fusion hasalso been demonstrated on a variety of cancer data analysis tasks at different scales [10; 6], that commonly rely on combining image (histopathology and/or radiology), tabular data (multi-omics, EHRs) and/or graphs (molecular data). Multi-modal fusion approaches differ in how and when the data is combined, which also determines the capabilities and properties of the resulting model. One common approach is late fusion, which constructs separate models for each modality before combining their output into an ensemble. This allows for capturing salient structural information through modality-specific architectures but prevents the resulting model from learning interactions between modalities . Early fusion methods, on the other hand, tend to train a single model from combined (raw) data (e.g., through concatenation), which incurs the cost of dismissing structural information (spatial, morphological, etc.). More sophisticated multi-modal fusion approaches rely on intermediate fusion, which attempts to overcome this trade-off by learning a low-level representation (embedding) to pick up complex interactions whilst taking advantage of the internal data structure. However, the problem with many intermediate fusion approaches is that the latent representations are not interpretable and struggle to handle missing modalities, yet both of these aspects are a necessity in most biomedical applications. Therefore, we posit that there is a need for more sophisticated early fusion representation learning approaches that: a) preserve structural information of the image, b) learn cross-modal interactions, and c) work on the raw data to preserve meaningful features for improved explainability. We introduce HEALNet to address all of these aspects (Figure 1).

## 2 HEALNet

**Preliminaries.** Let \(X^{m}\) represent data from modality \(m=1,...,j\). Let \(X^{m}^{p n}\) be either a tabular dataset with \(p\) features and \(n\) samples; or an image dataset \(X^{m}^{h w c n}\) with \(n\) images with height \(h\), width \(w\) and channels \(c\). The goal of a multi-modal fusion approach is to learn a fusion function \(f()\) such that \(y=f(X^{1},...,X^{j};)\) where \(\) denotes the set of hyperparameters. A conventional design of such a system is to first learn a modality-specific function \(g_{m}()\) which learns an intermediate representation \(h^{m}=g_{m}(X^{m};^{m})\) for intermediate hyperparameters \(\) and then apply a fusion function \(f()\) for predicting the target variable \(=f(h^{1},...,h^{j};)\).

**Architecture.** Instead of computing \(h_{m}\) and applying a single fusion function \(f()\), HEALNet uses an iterative learning setup. Let \(t\) denote a step, where the total number of steps \(T=d m\) for the number of layers \(d\). Let \(S_{t}\) represent a latent array shared across modalities, initialised at \(S_{0}\) where \(S^{a b}\) and \(a,b\) which is updated at each step. First, instead of learning an intermediate representation \(h^{m}\) as encoded inputs for \(X^{m}\), we compute the attention weights:

\[a_{t}^{m}=(X^{m},S_{t};^{a_{m}}) \]

Figure 1: Overview of HEALNet (**Hybrid**Early-fusion **At**tention **L**earning **Net**work) using both a shared and modality-specific parameter space to learn from structurally heterogeneous data sources in the same model. The shared space is a query array \(S\) that is iteratively passed as the query through attention-based fusion layers and captures the shared information between data sources. The hybrid early fusion layer learns the modality-specific attention weights \(W_{m}\), which are shared between layers, and captures structural information of each modality before encoding them and updating the shared space.

for each modality \(m\) at each step \(t\). Second, we learn an update function \(()\) to be applied at each step. The update of \(S\) with modality \(m\) is given by \(S_{t+1,m}=(S_{t},a_{t}^{m};)\) where \(\) denotes the shared hyperparameters across \(T\). For parameter efficiency, the final implementation uses weight sharing between layers. Across modalities, each early-fusion layer becomes an update function of the form:

\[S_{t+j}=(S_{t},a^{1},...,a^{j};) \]

The final function for generating a prediction only takes the final state of the shared array and returns the predictions of the target variable:

\[=f(S_{T};) \]

Figure 1 depicts a high-level visual representation of this approach, showing: (a) Hybrid Early-fusion Attention Learning Network, and (b) its key component, the early fusion layer (as given in Equation 2). We start by randomly initialising a latent bottleneck array, which is iteratively used as a query into each of the fusion layers and is updated with information from the different modalities at each layer pass. Passing the modalities through the shared latent bottleneck array helps to significantly reduce the dimensionality whilst learning important structural information through the cross-attention layers.

**Preserving structural information.** To handle heterogeneous modalities, we use modality-specific cross-attention layers \(()\) (Figure 0(b)) and their associated attention weights \(a_{t}^{m}\), whilst having the latent array \(S\) shared between all modalities. We structure the early fusion model as an attention network due to its ability to be generally applicable in different settings, making fewer assumptions about the input data (e.g., compared to a standard convolutional network). Sharing the latent array between modalities allows the model to learn from information across modalities, which is repeatedly passed through the model. Meanwhile, the modality-specific weights between the cross-attention layers focus on learning from inputs of different dimensions as well as learning the implicit structural assumptions of each modality. Specifically, in this work, we use cross-attention as outlined in , using the latent array \(S\) as the query and the input matrix \(X^{m}\) as the keys and values for each modality. As such, we define the query for each sample as \(q^{(n)}=W_{q}^{m}S\) and the keys and values as \(k^{(n)}=W_{k}^{m}x^{(n)}\) and \(v^{(n)}=W_{v}^{m}x^{(n)}\)for all \(n[1,N]\).

**Handling missing modalities.** Another common challenge in clinical practice is missing data modalities during inference. While models may have been trained on multiple modalities, there is a great chance that only a subset of modalities for a patient is available in practice. Therefore, multi-modal approaches must be robust in such scenarios. Typical intermediate fusion approaches would need to randomly initialise a tensor of the same shape or sample the latent space for a semantically similar replacement to pass into the fusion function \(f(h^{1},...,h^{j};)\) at inference, which is likely to introduce noise. In contrast, HEALNet overcomes this issue by design: the iterative paradigm can simply skip a modality update step (Equation 2) at inference time in a noise-free manner. Note that these practical benefits also extend to training scenarios, where a (typically small) number of samples is missing some modalities. Rather than imputing this data or completely omitting the samples, HEALNet can train and utilise all the available data using the same update principle.

**High-dimensioanl biomedical data.** One problem with attention-based architectures is their high number of trainable parameters, which we reduced by implementing weight sharing between the layers. Another challenge is that attention-based architectures are commonly trained on very large datasets, while biomedical data is typically high-dimensional with only a few samples. This leads to two problems - computational complexity and training instabilities . To handle the gigapixel scale of whole slide images (WSIs) within computational constraints, we use non-overlapping 224x224 pixel patches on the 20x magnified whole-slide image for preprocessing. To ensure comparability with our baselines , we extract a 1024-dimensional feature vector for each patch using a standard ResNet 50 pre-trained on ImageNet-1k V2. While the HEALNet architecture can also achieve competitive performance on the raw patch data, we found the training process to be very resource-intensive due to the high resolution (up to 150,000 x 150,000 pixels).

## 3 Experiments

This study focuses on survival analysis on The Cancer Genome Atlas (TCGA) data. Concretely, we train a multi-modal model from tissue WSIs, combing them with gene expressions (whole-genome sequencing) and mutations (RNAseq) data, on cohorts from Muscle-Invasive Bladder Cancer (BLCA, n=436), Breast Invasive Carcinoma (BRCA, n=1021), Cervical Kidney Renal Papillary Cell Carcinoma (KIRP, n=284), and Uterine Corpus Endometrial Carcinoma (UCEC, n=538). We compare the results of HEALNet to state-of-the-art late , intermediate , and early fusion baselines. In line with our benchmark, we use the same survival hazard calculation and survival loss (negative log-likelihood loss). To calculate the patient hazard, we are given the censorship status \(c\) and the survival months \(T_{cont}\), which are divided into 4 non-overlapping bins for censored patients, and apply the bin cut-offs onto uncensored patients.

## 4 Results & Discussion

The results of the survival analysis are summarised in Table 1, showing the mean and standard c-Index across the 5 cross-validation folds. Across all tested cancer sites, HEALNet learns a relevant unified representation \(S\) which allows the model to outperform all multi-modal baselines, achieving state-of-the-art performance in three (out of four) cancer sites. This corresponds to an improvement over multi-modal baselines of approximately 7%, 1%, 3% and 6% on the BLCA, BRCA, KIRP, and UCEC tasks, respectively. Note that the UCEC dataset is an example of _modality dominance_, where all informative signals stem from one modality (in this case WSI), while the other modality is mostly noise with respect to the task.

To put this into context, we compare our results to existing data fusion approaches that focus on image and tabular data for biomedical tasks. Our Porpoise baseline  uses a late fusion approach, which trains a modality-specific model for both images (attention-based multiple instance learning (MIL)) and multi-omic data (self-normalising network) before passing the modality representations through an attention gating mechanism. More recently, the Multi-modal Co-Attention Transformer (MCAT) uses two encoders - one "genomic-guided" co-attention followed by a set-based MIL Transformer. The resulting embeddings are then concatenated and passed into a simple classifier . Finally, the Perceiver  uses an iterative attention paradigm and achieves highly competitive performance on a range of uni-modal tasks. In line with its original paper, we use concatenation of the input tensors and modality-specific positional encoding to be our early fusion baseline. The problem with Porpoise is that both modalities are entirely learned in isolation, leaving little room for the genomic data to contextualise the imaging modality, which is reflected in the overall c-Index performance. The MCAT baseline does learn a shared representation between both modalities but struggles to handle missing or noisy modalities during training and inference. This can be seen in the performance on the UCEC dataset, where we know that one modality is mostly noise. Since MCAT adds noisy context to its co-attention unit if a modality is missing, this can lead to worse performance than uni-modal baselines.

In contrast, HEALNet overcomes these shortcomings by design. Its end-to-end training allows for a shared latent space that encodes cross-modal interactions while learning modality-specific attention

  
**Model** & **BLCA** & **BRCA** & **KIRP** & **UCEC** \\  Unimodal (Omics) & 0.606 \(\) 0.019 & 0.580 \(\) 0.027 & 0.780 \(\) 0.035 & 0.550 \(\) 0.026 \\ Unimodal (WSI) & 0.556 \(\) 0.039 & 0.550 \(\) 0.037 & 0.533 \(\) 0.099 & **0.630**\(\) 0.028 \\  Porpoise (Late) & 0.620 \(\) 0.048 & 0.630 \(\) 0.040 & 0.790 \(\) 0.041 & 0.590 \(\) 0.034 \\ MCAT (Interm.) & 0.620 \(\) 0.040 & 0.589 \(\) 0.073 & 0.789 \(\) 0.087 & 0.589 \(\) 0.062 \\ Perceiver (Early) & 0.565 \(\) 0.042 & 0.566 \(\) 0.068 & 0.783 \(\) 0.135 & 0.623 \(\) 0.107 \\ HEALNet (ours) & **0.668 \(\) 0.036** & **0.638 \(\) 0.073** & **0.812 \(\) 0.055** & 0.626 \(\) 0.037 \\   

Table 1: Mean and standard deviation of the concordance Index on four survival risk categories. We report the performance on the hold-out test set across five cross-validation folds. HEALNet outperforms all of its multi-modal baselines and three out of four uni-modal baselines in absolute c-Index performance.

weights that encode structural information. Additionally, the iterative modality-specific updates of the shared representation allow us to easily scale to more than two modalities, and simply skip an update if a modality for a sample is missing without introducing noise (see ablation in Appendix B). These design benefits make HEALNet suitable to handle tasks with high dimensionality (high-resolution whole slide images and 20k+ multi-omic features) but few samples, as is typical in many medical scenarios.