# On the Adversarial Robustness of Out-of-distribution Generalization Models

Xin Zou

Weiwei Liu

School of Computer Science, Wuhan University

National Engineering Research Center for Multimedia Software, Wuhan University

Institute of Artificial Intelligence, Wuhan University

Hubei Key Laboratory of Multimedia and Network Communication Engineering, Wuhan University

Corresponding author: Weiwei Liu (liuweiwei863@gmail.com).

###### Abstract

Out-of-distribution (OOD) generalization has attracted increasing research attention in recent years, due to its promising experimental results in real-world applications. Interestingly, we find that existing OOD generalization methods are vulnerable to adversarial attacks. This motivates us to study OOD adversarial robustness. We first present theoretical analyses of OOD adversarial robustness in two different complementary settings. Motivated by the theoretical results, we design two algorithms to improve the OOD adversarial robustness. Finally, we conduct experiments to validate the effectiveness of our proposed algorithms. Our code is available at [https://github.com/ZouXinn/OOD-Adv](https://github.com/ZouXinn/OOD-Adv).

## 1 Introduction

Recent years have witnessed the remarkable success of modern machine learning techniques in many applications. A fundamental assumption of most machine learning algorithms is that the training and test data are drawn from the same underlying distribution. However, this assumption is always violated in many practical applications. The test environment is influenced by a range of factors, such as the distributional shifts across the photos caused by different cameras in image classification tasks, the voices of different persons in voice recognition tasks, and the variations between scenes in self-driving tasks . Therefore, there is now a rapidly growing body of research with a focus on generalizing to unseen distributions, namely **out-of-distribution (OOD)** generalization .

Deep neural networks (DNNs) have achieved state-of-the-art performance in many fields. However, several prior works  have demonstrated that DNNs may be vulnerable to imperceptibly changed adversarial examples, which has increased focus on the adversarial robustness of the models. **Adversarial robustness** refers to the invariance of a model to small perturbations of its input , while **adversarial accuracy** refers to a model's prediction performance on adversarial examples generated by an attacker. However, the adversarial robustness of OOD generalization models (OOD adversarial robustness) is less explored, despite its importance in many systems requiring high security such as self-driving cars. We evaluate the adversarial robustness of the models trained with the current OOD generalization algorithms (the detailed experimental settings can be found in Appendix C.3), and present the results in Table 1. Surprisingly, under the PGD-20  attack, the algorithms achieve **nearly \(0\%\)** adversarial accuracy on RotatedMNIST , VLCS , PACS , and OfficeHome , and **no more than \(10\%\)** adversarial accuracy on ColoredMNIST . These results show that even if the OOD generalization algorithms generalize well in different scenes, they remain highly vulnerable to adversarial attacks.

Motivated by these limitations of existing algorithms, we provide theoretical analyses for OOD adversarial robustness in two different but complementary OOD settings. We then design twobaseline algorithms to improve the OOD adversarial robustness, based on the implications of our theory, and validate the effectiveness of our proposed algorithms through experiments.

Our **contributions** can be summarized as follows:

1. We evaluate the adversarial robustness of current OOD generalization algorithms and experimentally verify that the current OOD generalization algorithms are vulnerable to adversarial attacks.
2. We present theoretical analyses for the adversarial OOD generalization error bounds in the average case and the limited training environments case. Specifically, our bounds in limited training environments involve a "distance" term between the training and the test environments. We further use a toy example to illustrate how the "distance" term affects the OOD adversarial robustness, which is verified by the experimental results in Section 5.
3. Inspired by our theory, we propose two algorithms to improve OOD adversarial robustness. Extensive experiments show that our algorithms are able to achieve **more than \(53\%\)** average adversarial accuracy over the datasets.

The remainder of this article is structured as follows: SS2 introduces related works. SS3 presents our main theory. SS4 shows our two theory-driven algorithms. SS5 provides our experimental results. Finally, the conclusions are presented in the last section.

## 2 Related Work

### Adversarial Robustness

 show that DNNs are fragile to imperceptible distortions in the input space. One of the most popular methods used to improve adversarial robustness is **adversarial training (AT)**. The seminal AT work, the fast gradient sign method [25, FGSM], perturbs a sample towards its gradient direction to increase the loss, then uses the generated sample to train the model. Following this line of research,  propose iterative variants of the gradient attack with improved AT frameworks.  investigates the adversarial robustness from the perspective of ordinary differential equations. Recently,  utilize the data from generative models as data augmentation to improve adversarial robustness. Besides,  analyze the trade-off between robustness and fairness,  study the worst-class adversarial robustness in adversarial training.

For the theoretical perspective,  study the PAC learnability of adversarial robust learning,  extend the work of  to multiclass case,  give theoretical analyses to adversarial training by standard uniform convergence argumentation and giving a bound of the Rademacher complexity, and  study adversarial robustness under self-supervised learning.

  
**Algorithm** & **RM** & **CM** & **VLCS** & **PACS** & **OH** & **Avg** \\  ERM  & 0.6/0.0 & 5.8/X & 0.0/0.0 & 0.3/0.6 & 0.4/0.0 & 1.4/X \\ MLDG  & 0.2/0.0 & 4.8/X & 0.0/0.0 & 0.1/0.3 & 0.6/0.1 & 1.2/X \\ CDANN  & 0.9/0.0 & 8.2/X & 3.0/0.0 & 1.5/0.3 & 0.1/0.0 & 2.7/X \\ VREx  & 0.2/0.0 & 6.4/X & 0.0/0.0 & 0.0/0.3 & 0.4/0.1 & 1.4/X \\ RSC  & 1.0/0.0 & 3.9/X & 0.0/0.0 & 0.1/0.4 & 0.7/0.0 & 1.1/X \\ MAT  & X/X & 10.7/X & 0.0/0.0 & 0.7/1.4 & 0.8/0.1 & X/X \\ LDAT  & X/X & 7.9/X & 0.0/0.0 & 0.1/0.3 & 0.4/0.1 & X/X \\   

Table 1: The results (\(\%\)) for some of the current OOD generalization algorithms. We present the results in the form of **a/b**: here, **a** is the OOD adversarial accuracy under PGD-20 attack ; and **b** is the OOD adversarial accuracy under AutoAttack . We conduct the \(_{}\)-norm attack. We use the perturbation radius \(=0.1\) for RotatedMNIST and ColoredMNIST, and \(=\) for VLCS. For the architecture, following , we use a small CNN-architecture for RotatedMNIST and ColoredMNIST, and ResNet-50  for VLCS, PACS and OfficeHome. Since  do not realize MAT and LDAT for RotatedMNIST, we use X to denote the unrealized results. For more details about the algorithms, please refer to Appendix C.1. We use RM, CM, and OH as the abbreviation of RotatedMNIST, ColoredMNIST, and OfficeHome, respectively.

### Out-of-distribution generalization

OOD generalization aims to train a model with data from the training environments so that it is capable of generalizing to an unseen environment. A large number of algorithms have been developed that aim to improve OOD generalization. One series of works focuses on minimizing the discrepancies between the training environments [40; 17; 42; 58; 1]. The most related work among them is , which measures the discrepancy between the domains by \(d_{}(S,T)\), while we focus on adversarial robustness and use \(d^{}_{()}(S,T)\). Meta-learning domain generalization [39; MLDG] leverages the meta-learning approach and simulates train/test distributional shift during training by synthesizing virtual testing domains within each mini-batch. [53; GroupDRO] studies applying distributionally robust optimization (DRO) [19; 36; 77; 13] to learn models that instead minimize the worst-case training loss over a set of pre-defined groups. Another line of works [68; 64] conducts adversarial training to improve the OOD generalization performance. In this work, we focus on improving the OOD adversarial robustness.

From the theoretical perspective,  introduce a formal framework and argue that OOD generalization can be viewed as a kind of supervised learning problem by augmenting the original feature space with the marginal distribution of feature vectors. In , OOD generalization is cast into an online game where a player (model) minimizes the risk for a "new" distribution presented by an adversary at each time-step.  propose a probabilistic framework for domain generalization called Probable Domain Generalization, wherein the key idea is that distribution shifts seen during training should inform us of probable shifts at test time. Notably, all these works focus on OOD generalization performance, while we present theoretical results for OOD adversarial robustness.

### Works relating domain shifts and adversarial robustness

 focuses on improving the adversarial robustness of the models by regarding the adversarial distribution as the target domain and then applying the domain adaptation methods, while we focus on improving the model's adversarial robustness on the OOD distribution.  studies the relationship between adversarial robustness and OOD generalization, it shows that good adversarial robustness implies good OOD performance when the target domain lies in a Wasserstein ball. While we study the OOD adversarial robustness and propose algorithms to improve OOD adversarial robustness.  empirically analyzes the transferability of models' adversarial/certified robustness under distributional shifts. It shows that adversarially trained models do not generalize better without fine-tuning and that the accuracy-robustness trade-off generalizes to the unseen domain. Its results for adversarial robustness can also be found in our experimental results.  investigates how to improve the adversarial robustness of a model against ensemble attacks or unseen attacks.  regards the adversarial examples for each type of attack (such as FGSM, PGD, CW, and so on) as a domain, and utilizes the OOD generalization methods to improve the models generalization performance under different (maybe unseen) attacks.

[45; 22] study the relationship between the dependence on spurious correlations and the adversarial robustness of the models and show that adversarial training increases the model's reliance on spurious features.  studies the relationship between fairness and adversarial robustness and shows that models that are fairer will be less adversarially robust. However, they do not consider the adversarial robustness of the model on the unseen target domain, which is the topic of this paper.

## 3 Theoretical Analysis for OOD Adversarial Robustness

In this section, we present two theorems in two different settings, each of which inspires an algorithm designed to improve the OOD adversarial robustness. The proofs of all results in this section can be found in Appendix A. We first introduce some notations and basic setups.

**Notations.** We define \([n]\{1,2,,n\}\). We denote scalars and vectors with lowercase letters and lowercase bold letters respectively. We use uppercase letters to denote matrices or random variables, and uppercase bold letters to denote random vectors or random matrices. For a vector \(^{n}\), we define the \(_{p}\)-norm of \(\) as \(\|\|_{p}(_{i=1}^{n}|x_{i}|^{p})^{1/p}\) for \(p[1,)\), where \(x_{i}\) is the \(i\)-th element of \(\); for \(p=\), we define \(\|\|_{}_{1 i n}|x_{i}|\). For a matrix \(A^{m n}\), the Frobenius norm ofis defined as \(\|A\|_{F}(_{i=1}^{m}_{j=1}^{n}A_{ij}^{2})^{}\), where \(A_{ij}\) is the entry of \(A\) at the \(i\)-th row and \(j\)-th column. We define the determinant of \(A\) as \((A)\). \((,)\) represents the multivariable Gaussian distribution with mean vector \(\) and covariance matrix \(\). Given \(f,g:_{+}\), we write \(f=(g)\) if there exist \(x_{0},_{+}\) such that for all \(x>x_{0}\), we have \(f(x) g(x)\). We use \(sign()\) to denote the sign function .

**Setups.** Let \(^{m}\) be the input space and \(\) be the label space. We set \(=\{ 1\}\), \(=\{1,2,,K\}\) (where \(K\) is the number of classes), and \(=\) for the binary classification problem, the multi-class classification problem, and the regression problem, respectively. We use \(:_{+}\) as the loss function. We consider learning with the hypothesis class \(\{h:\}\). Given a distribution \(\) on \(\), the error of \(h\) with respect to the loss function \(\) under the distribution \(\) is \(_{}(,h)=}_{(,y)}[(h(),y)]\), where \(\) and \(y\). We further define \((): 2^{}\) as a perturbation function that maps an input \(\) to a subset \(()\), where \(2^{}\) is the power set of \(\). The adversarial error of the predictor \(h\) under the perturbation function \(()\) is defined as

\[_{}^{}(,h)=}_{(, y)}[_{^{}()}(h( ^{}),y)]\!.\]

### The Average Case

In this section, following [20; 14; 52], we consider the average case, i.e., the case in which the target environment follows a distribution. In this case, we aim to minimize the average target adversarial error of the hypothesis \(h\), where the average is taken over the distribution of the target environment.

Suppose that \(()\) is the set of all possible probability measures (environments) on \(\) for the task of interest. Assume there is a prior \(p\) on \(()\), which is the distribution of the environments. Moreover, suppose the process of sampling training and test data is as follows:

(1). We generate the training data according to the following two steps: (i) we sample \(t\) training environments from \(p\), i.e., \(_{1},,_{t} p\); (ii) the examples \(}_{i}=\{(_{i1},y_{i1}),,(_{in_{i}},y_{in _{j}})\}_{i}^{n_{i}}\) are drawn independent and identically distributed (i.i.d.) from \(_{i}\), where \(n_{i}\) is the size of \(}_{i}\) and \(i[t]\). To simplify the notations, we also use \(}_{i}\) to denote the empirical distribution of the dataset \(\{(_{i1},y_{i1}),,(_{in_{i}},y_{in_{i}})\}\). Let \(}=_{i=1}^{t}}_{i}\) and \(=_{i=1}^{t}_{i}\). To simplify the problem, we assume \(n_{1}=n_{2}==n_{t}=n\), but note that with a more careful analysis, our result in the average case can be extended to the case in which \(n_{1},,n_{t}\) are not necessarily the same.

(2). For each test example, we first sample an environment from \(p\), i.e., \( p\), then sample an example \((,y)\).

Let \(_{p}(,h)=}_{ p}[_{}(,h)]\) be the average risk of \(h\) on prior \(p\). For the perturbation function \(()\), we define \(_{p}^{}(,h)=}_{ p} [_{}^{}(,h)]\) as the average adversarial risk of \(h\). The empirical Rademacher complexity [55; Chapter 26] of the hypothesis class \(\) is defined as follows:

\[_{n}()=}_{}[_{h }_{i=1}^{n}_{i}h(_{i})],\]

where \(\) is a Rademacher random vector with i.i.d. entries, and \(\{_{1},,_{n}\}\) is a set of data points. The following theorem presents an upper bound for the average adversarial risk of \(h\).

**Theorem 3.1**.: _Suppose the loss function \(\) is bounded, i.e., \([0,U]\). Then with probability of at least \(1-\) over the sampling of \(}_{1},,}_{t}\), the following bound holds for all \(h\):_

\[_{p}^{}(,h)_{}}^{ }(,h)+2_{t}(})+2 _{tn}(})+3U}+3U},\]

_where_

\[}=\{g_{h}: _{+}g_{h}(,y)=_{^{} ()}(h(^{},y)),h\}.\]Theorem 3.1 presents a bound for all hypotheses \(h\). We now consider the convergence property of the adversarial empirical risk minimization (**AERM**) algorithm in the average case. We define the output of the AERM algorithm as \(*{arg\,inf}_{h}^{}_{ }(,h)\). We then define the hypothesis with the best adversarial generalization performance as \(h^{}*{arg\,inf}_{h}^{}_ {p}(,h)\).

The next corollary shows an upper bound for the excess risk [63, Chapter 4] of AERM:

**Corollary 3.2**.: _Suppose the loss function \(\) is bounded, i.e., \([0,U]\). Then with probability of at least \(1-\) over the sampling of \(}_{1},,}_{t}\), the following bound holds:_

\[^{}_{p}(,)^{}_{p}( ,h^{})+4_{t}(})+4_{tn} (})+3U8/}{2t}}+3U8/}{2tn}}.\]

**Remark 1**.: _The convergence rate for both Theorem 3.1 and Corollary 3.2 is \((t^{-})\), which prompts us to ask: can we derive a tighter bound that has a faster convergence rate than \((t^{-})\)? The next section provides an affirmative answer to this question._

### Theory with Limited Environments

In this section, we provide the theoretical analysis for the limited training environment case, i.e., in the case in which \(t=(1)\). Suppose that we have \(t\) training environments \(_{1},,_{t}\) and one unknown target environment \(\). Our goal is to find a predictor that obtains good adversarial robustness on the target environment \(\). Assume we obtain \(n\) examples from each training distribution \(_{i}\).

First, we introduce a discrepancy between the distributions. Based on the hypothesis class \(\), the seminal work for unsupervised domain adaptation (UDA), , defines the discrepancy between two distributions as follows:

\[d_{}(,^{})\!=\!2_{h} |*{}_{}[h()=1 ]\!-\!*{}_{^{}}[ h()=1]|.\]

 analyze the generalization error bound of the target domain for UDA by \(d_{}(,)\), where \(g g=h h^{}\) for some \(h,h^{}\); here, \(\) is the XOR operator. However, the above definition is limited to the binary classification. This paper extends \(d_{}\) to the multi-class adversarial case. Given the loss function \(\), distributions \(P\), \(Q\) on \(\), and hypothesis class \(\), we define the adversarial discrepancy as follows:

\[d^{}_{()}(P,Q)=_{h}| *{}_{ P}[_{^{}()}(h(^{}),y)]-*{}_{  Q}[_{^{}()}(h(^{ }),y)]|.\]

When \(()=\{\}\), \(d^{}_{()}(P,Q)\) becomes the standard case. It can be easily verified that \(d^{}_{()}(,)\) is symmetric and satisfies the triangle inequality (the proof can be found in Appendix A.3); thus, \(d^{}_{()}(,)\) is a pseudometric.

**Comparison of \(d_{}(,)\) and \(d^{}_{()}(,)\).** The theory outlined in [9, Theorem 2] presents an upper bound with a \(d_{}(,)\) term. To align the feature distributions of the source and target domain, we need to calculate \(d_{}(P,Q)\), which takes the supremum over two hypotheses \(h,h^{}\):

\[d_{}(P,Q)=2_{h,h^{}}| *{}_{ P}[h() h^{}()]-*{}_{ Q}[h() h^{ }()]|.\]

However, the definition of \(d^{}_{()}(,)\) shows that: \(d^{}_{()}(P,Q)\) takes the supremum over one hypothesis \(h\), which is easier to optimize  and can thus significantly ease the minimax optimization in Section 4.2.

**Theorem 3.3**.: _For a given but unknown target distribution \(\), let \(^{t-1}\{(_{1},,_{t})|_{i} 0,_{i=1} ^{t}_{i}=1\}\) be the \((t-1)\)-dimensional simplex, and \(()\{_{i=1}^{t}_{i}_{ i}|^{t-1}\}\) be the convex hull of \(=\{_{1},,_{t}\}\). Let \(_{P}*{arg\,inf}_{( )}d^{}_{()}(,)\) be the "projection" of \(\)onto \(()\), and \(^{}\) be the weight vector where \(_{P}=_{i=1}^{t}_{i}^{}_{i}\). Assume \([0,U]\). Then: with probability of at least \(1-\), for all \(h\),_

\[_{}^{}(,h) _{i=1}^{t}_{}_{ i}}^{}(,h)+_{i}_{j}_{j}^{}d_{ ()}^{}(}_{i},}_{j})+d_{()}^{}(,_{P})\] \[+4_{tn}(})+2_{n}( })+6U8/}{2tn}}+3U(16t/)}{2n}}.\]

**Remark 2**.: _The first term of the bound is the average empirical adversarial robust error of the model on training environments, and the second term is the weighted average discrepancy on the empirical training distributions \(}=\{}_{1},,}_{t}\}\). The third term can be viewed as the **distance** between \(\) and the convex hull \(()\), which is fixed once the task, \(,}\) and \(\) are given. Thus, minimizing the first two terms of the bound is able to improve the OOD adversarial robustness._

_Moreover, \(_{tn}(})\) and \(_{n}(})\) measure the capacity of the model, and can be regarded as an implicit regularizer on the model. There are many existing works that present the upper bounds of the empirical Rademacher complexity for neural networks  and the adversarial Rademacher complexity . Their results can be summarized as follows: for bounded \(\), with proper weak assumptions on the loss function \(\), \(_{n}(})\) can be upper-bounded by \((}{})\), where \(C\) is a constant that is related to some norm of the parameters of \(h\) and increases with the norm. Thus, we consider constraints on the norm of the parameters of \(h\) in the algorithm designing part of Section 4._

_Last but not least, different from the results in Theorem 3.1, the convergence rate is \((1/}{tn}}+t/ }{n}})\). When \(t=(1)\), \((t/}{n}})\) in Theorem 3.3 converges much faster than \((1/}{t}})\) in Theorem 3.1, since \(n t\)._

The \(d_{()}^{}(,_{P})\) term in Theorem 3.3 is determined by the distance between the source and target environments. A larger \(d_{()}^{}(,_{P})\) may lead to worse adversarial robustness of the model on the target domain. Next, we present a toy example to illustrate this case.

**Example 1**.: We consider the input space \(=^{d}^{m}\) and label space \(=\{+1,-1\}\). For simplicity, we consider only three distributions on \(\), i.e., \(^{(0)},^{(1)}\) and \(^{(2)}\). The marginal distributions for \(Y\) satisfy \(_{Y}^{(0)}(\{-1\})=_{Y}^{(0)}(\{+1\})=_{Y}^{(1 )}(\{-1\})=_{Y}^{(1)}(\{+1\})=_{Y}^{(2)}(\{-1\})= _{Y}^{(2)}(\{+1\})=\). For \(^{(i)}\), let the conditional distribution of \(^{(i)}\) given \(Y\) be \(^{(i)}|Y=(y^{(i)},^{2}I)\), where \(^{(i)}^{d}\) is a non-zero vector and \(^{2}I^{d d}\) is the covariance matrix, i.e., the elements of \(^{(i)}\) are independent. Let \(Q\) be a rotation transformation on \(^{d}\), which is a special orthogonal transformation. Suppose that \(\), where \(\) is the identity transformation. Let \(Q^{d d}\) be the matrix of \(Q\) under some orthonormal basis. We then know that \(Q\) is a rotation matrix and \(Q^{T}Q=QQ^{T}=,(Q)=1,Q I\). To model the distributional shift among the environments, we apply the transformation \(\) to \(^{(0)}\) and use \(^{(1)}=^{(0)},^{(2)}=} {}^{(0)}\) as the mean vectors of \(^{(1)},^{(2)}\) respectively. Here, we consider the \(_{2}\)-norm adversarial attack with radius \(\), i.e., \(()=\{^{}:\|-^{}\|_{2} \}\). We use two environments as the training environments and the remainder as the test environment. Theorem 3.4 shows that a larger distance between the training and test environments leads to worse OOD adversarial robustness.

Figure 1: An intuitive visualization of the distributions \(^{(0)},^{(1)},^{(2)}\) (**red, green, blue** respectively). We also represent the three mean vectors \(\), \(\), \(^{2}\) using red, green, and blue arrows. The angle between \(,\) is \(\), which is the same as the angle between \(,^{2}\). More details about the relationship between \(\) and \(\) can be found in the proof of Theorem A.4 in Appendix A.1.

**Theorem 3.4**.: _Consider the setting in Example 1, and suppose that \(\) lies in the 2-dimensional subspace \(\) in Theorem A.4 (see Appendix A.1 for details about \(\), and see Figure 1 for an intuitive visualization). Let \(_{01}(x,y)=[x y]\), where \([]\) is the indicator function and \(B_{p}^{r}()=\{^{}:\|-^{}\|_{p} r\}\). Consider training with hypothesis class \(=\{h_{}:h_{}(x)=sign(^{T}x),^{d}\}\), and denote \(^{(ij)}=^{(i)}+^{(j)},i,j\{0,1,2\},i j\). Consider the \(_{2}\)-norm adversarial attack with radius \(\); for notation convenience, let \(}_{ij}()=_{^{(ij)}}^{B_{2}^{ }}(_{01},h_{})\) and \(}_{i}()=_{^{(i)}}^{B_{2}^{ }}(_{01},h_{})\). Let \(()\) be the distribution function of the standard normal distribution. Let \(\) denote the angle between \(\) and \(Q\), which is the rotation angle of \(Q\) in the subspace \(\). Furthermore, suppose that \(0< arccos_{1}\|_{2}}\). We then have: (1) If we train with \(^{(0)}\) and \(^{(1)}\), let \(_{(01)}{=}Q\), which achieves the minimum of \(}_{01}()\). Then the adversarial accuracy of \(_{(01)}\) on the test distribution \(^{(2)}\) is \(}_{2}(_{(01)}){=}( -+Q)^{T}Q^{2}}{\|+Q\|_{2}})\). (2) If we train with \(^{(0)}\) and \(^{(2)}\), let \(_{(02)}{=}^{2}\), which achieves the minimum of \(}_{02}()\). Then the adversarial accuracy of \(_{(02)}\) on the test distribution \(^{(1)}\) is \(}_{1}(_{(02)}){=}( -+Q^{2})^{T}Q}{\|+Q^{2}\|_ {2}})\). (3) If we train with \(^{(1)}\) and \(^{(2)}\), let \(_{(12)}{=}^{2}\), which achieves the minimum of \(}_{12}()\). Then the adversarial accuracy of \(_{(12)}\) on the test distribution \(^{(0)}\) is \(}_{0}(_{(12)}){=}( -+Q^{2})^{T}}{\|+Q^{2}\|_ {2}})\). (4) \(}_{1}(_{(02)})<}_{2}(_ {(01)})=}_{0}(_{(12)})\)._

**Remark 3**.: _Let \(\{i,j,k\}=\{0,1,2\}\). For task \(i\), we use \(^{(j)},^{(k)}\) as the training environments and \(^{(i)}\) as the test environment. \(}_{i}(_{(jk)})\) is the target adversarial error of the learned classifier \(_{(jk)}\) in task \(i\)._

_We now consider the distance between the training and test environments for each task. Intuitively, we regard the angle as the "distance" between two distributions. We define \((i,j)\) as the angle between the mean vector of \(^{(i)}\) and \(^{(j)}\), \(i j\). For task \(i\), we define the average "distance" between the test environment and each training environment as \(d_{}(i):=\). We use \(d_{}(i)\) as a measure of the distance between the training environments and the test environment for task \(i\)._

_From the settings in Example 1, it can be clearly seen that \(d_{}(0)=== =d_{}(2)\) and \(d_{}(1)===\), which implies that \(d_{}(1)<d_{}(0)=d_{}(2)\). Theorem 3.4 tells us that \(}_{1}(_{(02)})<}_{0}(_ {(12)})=}_{2}(_{(01)})\), and thus implies that \(\) smaller distance between the training and test environments leads to better OOD adversarial robustness_.

_Moreover, the assumption of the angle between \(\) and \(Q\) in Theorem 3.4 is reasonable. Consider \(=\|_{2}}{2}\); in this case, the attack is strong and perceptible to human eyes. Then, \( arccos_{1}\|_{2}}=arccos=\). In this case, the maximal angle between the environments is \(\), which leads to a strong distribution shift. When \(<\|_{2}}{2}\), the allowed rotation angle can be further enlarged, and when \(=0\), it becomes the standard case._

_Furthermore, the data distribution here can be regarded as a simplified data model for RotatedMNIST. Moreover, our experimental results in Section 5 are consistent with our analysis here in both the standard and adversarial cases. Please refer to the observation part of Section 5.2 and the table in Appendix D.1 for further details._

## 4 Algorithms

In this section, based on our theory, we present two algorithms that can improve the adversarial robustness of the model on the target environment.

### Adversarial Empirical Risk Minimization (AERM or AT)

Based on Theorem 3.1, which shows an upper bound of the average adversarial risk over the environments, we propose our first algorithm: adversarial empirical risk minimization (AERM, which corresponds to applying AT to multiple source domains). The bound in Theorem 3.1 consists of the average adversarial empirical risk \(_{}}^{}(,h)=_{i=1}^ {t}_{}_{i}}^{}(,h)\) and two empirical Rademacher complexity terms \(_{t}(})+_{tn}(})\). As outlined in Remark 2, the empirical Rademacher complexity implies an implicit regularizer on the model capacity. We choose \(\|\|_{F}\) as a regularizer on the model parameters. The optimization objective of AT is as follows:

\[L_{}(h)=_{i=1}^{t}_{}_ {i}}^{}(,h)+\|W\|_{F},\]

where \(\) is a trade-off hyper-parameter, and \(W\) is the parameter matrix of the model. From Remark 1, we know that AT may not generalize well in the case where the training environments are limited. Next, we propose another algorithm for the limited environment case.

### Robust DANN (RDANN)

Theorem 3.3 shows that \(_{n}(})\), \(_{i=1}^{t}_{}_{i}}^{} (,h)\), and \(_{i}_{j}_{j}^{t}d_{()}^{} (}_{i},}_{j})\) play the key roles in designing the OOD adversarial training methods. However, the weights \(_{1}^{*},,_{t}^{*}\) are unknown, since we have no information about the target distribution \(\). Since \(_{j}^{*}, j\), it is evident that \(_{i}_{j}_{j}^{*}d_{()}^{} (}_{i},}_{j})_{i} _{j}d_{()}^{}(}_{i},}_{j})\). We therefore turn to optimize the average discrepancy \(}_{i}_{j}d_{()}^{}(}_{i},}_{j})\). To improve the OOD adversarial robustness, we minimize the following:

\[_{i=1}^{t}_{}_{i}}^{} (,h)+_{1}}_{i=1}^{t}_{j=1}^{t}d_{( )}^{}(}_{i},}_{ j})+_{2}\|W\|_{F},\]

where \(_{1},_{2}\) are two hyper-parameters, and \(W\) is the parameter matrix of the model. However, the term \(}_{i=1}^{t}_{j=1}^{t}d_{()}^{} (}_{i},}_{j})\) is a constant. Motivated by , we minimize the discrepancy of the training environments in the feature space of a feature extractor \(f\).

Specifically, we consider \(=\{c f|f,c\}\); this means that the predictor \(h\) consists of a classifier \(c\) and a feature extractor \(f\), where \(=\{f:\}\), \(=\{c:\}\) and \(^{l}\) is the feature space. Any feature extractor \(f\) determines a hypothesis class \(_{f}=\{c f|c\}\). Given a feature extractor \(f\), we apply Theorem 3.3 to the hypothesis class \(_{f}\). Then with high probability, for any \(c\), the target error of \(h=c f\) can be controlled mainly by \(_{i=1}^{t}_{}_{i}}^{}( ,c f)\) and \(}_{i=1}^{t}_{j=1}^{t}d_{(_{f})}^{}(}_{i},}_{j})\). We then aim to find \(c\) and \(f\) such that \(h=c f\) has good OOD adversarial robustness. Thus, we aim to minimize the following:

\[_{i=1}^{t}_{}_{i}}^{ }(,c f)}^{}+_{1}}_{i=1}^{t}_{j=1}^{t}d_{(_{f})}^{}( }_{i},}_{j})}_{L_{}(,f)}+ _{2}\|W\|_{F}.\]

To minimize \(L(c f)\), we need to solve a minimax problem. For simplicity, we fix \((),\) and define the following:

\[D(h,P,Q)=|,y) P}{}[_{^{ }()}(h(^{}),y)]-,y) Q}{}[_{^{}()}(h(^{}),y)]|.\]

Since \(D(h,P,P)=0\) and \(D(h,P,Q)=D(h,Q,P)\) for any \(P,Q,h\), our optimization problem can be formulated as follows:

\[_{c,f}_{c_{ij}:1 i<j t }L_{}(,c f)+_{1}_{1 i<j t }D(c_{ij} f,}_{i},}_{j})+_{ 2}\|W\|_{F}.\]To solve the minimax optimization problem, we adopt an idea similar to that of adversarial neural networks  and refer to \(c_{ij}\) as the discriminator. However, there are \(\) discriminators in this case, and training with many discriminators may make the optimization process unstable . We therefore opt to use the same discriminator for all \((}_{i},}_{j})\) pairs. Note that:

\[_{c^{}}_{i<j}D(c^{} f,}_{i},}_{j})_{c_{ij}} _{i<j}D(c_{ij} f,}_{i},}_{j}),\]

such that optimizing over a shared discriminator \(c^{}\) is equivalent to optimizing a lower bound of the original objective. Our final optimization problem then becomes:

\[_{c,f}_{c^{}} {L_{}(,c f)}_{L_{a}(w,)}+_{1}_{1 i<j t}D(c^{} f,}_{i},}_{j})}_{L_{d}(w^{},)}+_{2} {\|W\|_{F}}_{L_{}(w,)}.\]

where \(w,w^{},\) are parameters for \(c,c^{},f\) respectively. We call our method robust adversarial-domain neural network (**RDANN**), the pseudo-code of which is presented in Algorithm 1.

```
0: the training data \(}_{1},,}_{t}\), the number of iterations \(T\), the number of iterations for the inner maximization problem \(k\), the learning rate \(\), the inner max learning rate \(\), the tradeoff hyper-parameters \(_{1},_{2}\).
0: the parameters \(w^{T},^{T}\) for \(c,f\).
1: initialize \(w^{0},^{0},w^{ k}\) randomly
2:for\(i 0\) to \(T-1\)do
3: set \(w^{ 0} w^{ k}\)
4:for\(j 0\) to \(k-1\)do
5:\(w^{(j+1)} w^{ j}+_{w^{}}L_{d}(w^{  j},^{i})\)
6:endfor
7:\(L_{1}^{i} L_{}(w^{i},^{i})+_{2}L_{}(w ^{i},^{i})\)
8:\(L_{2}^{i} L_{1}^{i}+_{1}L_{d}(w^{ k},^{i})\)
9:\(^{i+1}^{i}-_{}L_{2}^{i}\)
10:\(w^{i+1} w^{i}-_{w}L_{1}^{i}\)
11:endfor
```

**Algorithm 1** RDANN

## 5 Experiments

To verify the adversarial robustness of our algorithms, we conduct experiments on the DomainBed benchmark , a testbed for OOD generalization that implements consistent experimental protocols across various approaches to ensure fair comparisons. Our code is attached in the supplementary material.

### Experimental Setup

**Datasets.** The datasets we use are RotatedMNIST , ColoredMNIST , VLCS , PACS , and OfficeHome . There are several different environments in each dataset. For the DomainBed benchmark, we choose one environment as the test environment and use the others as training environments. We report the average accuracy over different choices of the test environment. Further details about the selected datasets can be found in Appendix C.2.

**Backbone network.** Following , we use a small CNN architecture for RotatedMNIST, ColoredMNIST, and ResNet-50  for VLCS, PACS, and OfficeHome.

  
**Algorithm** & **RM** & **CM** & **VLCS** & **PACS** & **OH** & **Avg** \\  ERM & 0.6/0.0 & 5.8/X & 0.0/0.0 & 0.3/0.6 & 0.4/0.0 & 1.4/X \\ MLDG & 0.2/0.0 & 4.8/X & 0.0/0.0 & 0.1/0.3 & 0.6/0.1 & 1.2/X \\ CDANN & 0.9/0.0 & 8.2/X & 3.0/0.0 & 1.5/0.3 & 0.1/0.0 & 2.7/X \\ VREx & 0.2/0.0 & 6.4/X & 0.0/0.0 & 0.0/0.3 & 0.4/0.1 & 1.4/X \\ RSC & 1.0/0.0 & 3.9/X & 0.0/0.0 & 0.1/0.4 & 0.7/0.0 & 1.1/X \\ MAT & X/X & 10.7/X & 0.0/0.0 & 0.7/1.4 & 0.8/0.1 & X/X \\ LDAT & X/X & 7.9/X & 0.0/0.0 & 0.1/0.3 & 0.4/0.1 & X/X \\ AT (ours) & 93.4/**93.3** & **51.6**/X & 42.6/41.8 & **48.1**/47.6 & **30.2/29.8** & 53.2/X \\ RDANN (ours) & **93.5/93.3** & 51.1/X & **44.9/43.9** & **48.1/48.0** & 28.6/27.4 & **53.3**/X \\   

Table 2: The results (\(\%\)) of the algorithms. Results are presented in the form of **a/b**: here, **a** is the OOD adversarial accuracy under PGD-20 attack ; and **b** is the OOD adversarial accuracy under AutoAttack . We conduct the \(_{}\)-norm attack. Since  do not realize MAT and LDAT for RotatedMNIST, we use X to denote the unrealized results. Best results for PGD-20 attack are shown in **bold**. For more details about the algorithms, please refer to Appendix C.1. We use RM, CM, and OH as the abbreviation of RotatedMNIST, ColoredMNIST, and OfficeHome, respectively.

**Hyper-parameters for adversarial attack and adversarial training.** We use \(_{}\)-norm attack for both adversarial attack and adversarial training. We use \(=0.1\) for ColoredMNIST and RotatedMNIST, and \(=4/255\) for VLCS, PACS, and OfficeHome; moreover, following [74; 75], we use PGD-10 to generate adversarial examples at the training stage and PGD-20 at the evaluation stage to avoid overfitting. The step size used to generate adversarial examples is set to be \(/4\).

More details of the experimental settings can be found in Appendix C.4.

### Results

Table 2 presents the results of our experiments. As is clear from the table, our proposed algorithms significantly improve the OOD adversarial robustness of the model. For example, under the attack PGD-20, all other algorithms achieve **no more than \(3\%\)** average adversarial accuracy, while our proposed algorithms achieve **more than \(53\%\)** average adversarial accuracy. Moreover, in our datasets, the number of environments (\(t\)) is small. From Table 2, we can see that the overall performance of RDANN is superior to that of AT. RDANN achieves better or comparable adversarial accuracy on most datasets (except OfficeHome). The results are consistent with our claim in Remark 2: when \(t=(1)\), the bound in Theorem 3.3 converges faster than that in Theorem 3.1. The detailed results for each test environment are attached in Appendix D.

**Observations.** According to the detailed results for RotatedMNIST and ColoredMNIST in Appendix D.1 and Appendix D.2, we can make the following observations (these phenomena occur in both the adversarial training setting and the standard training setting):

* For RotatedMNIST, we have six environments, each of which corresponds to a rotation angle of the original image. The rotation angle of the \(i\)-th environment is \(i 15^{}\), \(i\{0,1,2,3,4,5\}\). For task \(i\), we use the \(i\)-th environment as the test environment and the remaining environments as the training environments. Following Remark 3, we define \(d_{}(i)_{j i}(i,j)\) as the average distance between the test environment and each training environment for task \(i\). Then, \(d_{}(0)=45^{}\), \(d_{}(1)=33^{}\), \(d_{}(2)=27^{}\), \(d_{}(3)=27^{}\), \(d_{}(4)=33^{}\) and \(d_{}(5)=45^{}\). We define the PGD-20 adversarial accuracy of the model trained by RDANN for task \(i\) as \(a(i)\); then, \(a(0)=90.8\), \(a(1)=94.7\), \(a(2)=95.3\), \(a(3)=95.6\), \(a(4)=95.5\) and \(a(5)=89.0\). As \(i\) increases, \(d_{}(i)\) first decreases and then increases, while \(a(i)\) first increases and then decreases. The result indicates that \(d_{}(i)\) is anticorrelated with \(a(i)\), which is consistent with the analysis in Remark 3. Note that this phenomenon occurs in all algorithms.
* For ColoredMNIST, we have three environments, each of which corresponds to a correlation between the additional channel and the label. For task \(i\), \(i\{0,1,2\}\), we define the correlation as \((i)\) and \((1)=0.9\), \((2)=0.8\), \((3)=-0.9\). Here, we define \(d_{}(i)=_{j i}(j)-(i)\), then \(d_{}(1)=0.95\), \(d_{}(2)=0.8\), \(d_{}(3)=1.75\). Similarly, we can define \(a(i)\) for a given algorithm. The detailed results in Appendix D.2 imply that \(d_{}(i)\) is anticorrelated with \(a(i)\). To further understand this phenomenon, we present another toy example for ColoredMNIST with a different data model in Appendix B.

## 6 Conclusion

In this paper, we focus specifically on out-of-distribution adversarial robustness. First, we show that existing OOD generalization algorithms are easily fooled by adversarial attacks. Motivated by this, we then study the theory of the adversarial robustness of models in two different but complementary OOD settings. Based on our theory, we propose two algorithms, AT and RDANN. Extensive experiments show that our proposed algorithms can significantly improve the OOD adversarial robustness of the model.