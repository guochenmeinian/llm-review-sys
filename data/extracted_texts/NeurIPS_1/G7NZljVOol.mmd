# L-TTA: Lightweight Test-Time Adaptation Using a Versatile Stem Layer

Jin Shin, Hyun Kim

Department of Electrical and Information Engineering and RCEIT

Seoul National University of Science and Technology

Seoul, Korea

{shinjin0103, hyunkim}@seoultech.ac.kr

Corresponding author: Hyun Kim, email: hyunkim@seoultech.ac.kr

###### Abstract

Test-time adaptation (TTA) is the most realistic methodology for adapting deep learning models to the real world using only unlabeled data from the target domain. Numerous TTA studies in deep learning have aimed at minimizing entropy. However, this necessitates forward/backward processes across the entire model and is limited by the incapability to fully leverage data based solely on entropy. This study presents a groundbreaking TTA solution that involves a departure from the conventional focus on minimizing entropy. Our innovative approach uniquely remodels the stem layer (_i.e._, the first layer) to emphasize minimizing a new learning criterion, namely, uncertainty. This method requires minimal involvement of the model's backbone, with only the stem layer participating in the TTA process. This approach significantly reduces the memory required for training and enables rapid adaptation to the target domain with minimal parameter updates. Moreover, to maximize data leveraging, the stem layer applies a discrete wavelet transform to the input features. It extracts multi-frequency domains and focuses on minimizing their individual uncertainties. The proposed method integrated into ResNet-26 and ResNet-50 models demonstrates its robustness by achieving outstanding TTA performance while using the least amount of memory compared to existing studies on CIFAR-10-C, ImageNet-C, and Cityscapes-C benchmark datasets. The code is available at [https://github.com/janus103/L_TTA](https://github.com/janus103/L_TTA).

## 1 Introduction

From the inception of deep learning research, the problem of overfitting, where a model's performance becomes biased towards its training dataset, has been a persistent issue. Various solutions such as normalization and regularization have been proposed to address these issues . Additionally, the need for trained models to adapt to the diversity of data distributions in the real world has also emerged recently. This phenomenon, known as domain shift, has resulted in the proposal of various scenarios and practical studies . Scenarios for addressing model domain shifts are generally introduced with the key themes of domain adaptation (DA) or domain generalization (DG). These are differentiated by their capability to access the data in the target domain. To clarify, DA can access all domains but can only use unlabeled data in the target domain , whereas DG can only train in the source domain . Recent source-free DA has indicated that accessing source domain data is generally infeasible. It has proposed a scenario where the target domain adapts using a teacher model that generates pseudo labels . However, this approach involves the significant cost of maintaining the large memory required for the teacher model.

Within our current understanding, the most realistic research topic for deploying deep learning models in the domain shift field at present is test time adaptation (TTA) . This approach relies solely on pre-trained model parameters and data from the target domain without generating pseudo labels. Typically, TTA research focuses on two critical observations: (1) the variation in batch normalization (BN) statistics when domain shift occurs  and (2) the possibility of adaptation when the models are trained to minimize the entropy of predictions . Notably, recent studies based on entropy minimization improved the performance by simultaneously passing various augmentations through the input at test-time and either minimizing the average entropy  or filtering out noisy data points with high entropy that hinder training . Although these studies enhanced the adaptation performance, they were limited by the substantial training costs and memory usage (which reduced their practicality). For these reasons, TTA performance improvement must be achieved, but in practical terms, there is a need to simultaneously eliminate unnecessary processes and conserve memory resources as much as possible due to gradient, which causes the largest power consumption in training.

We summarize the fundamental problems of such studies into three main aspects: (1) Forwarding the entire model process to obtain the entropy involves a cost. (2) To enable the backward process, a substantial cost would be incurred to reach the initial normalization layer. (3) The absence of a method to ensure data independence may result in biased learning in the incorrect direction if noisy data are not identified, and even if such data are identified, this process can increase the training time.

Our research begins with the hypothesis that fine-tuning the first convolutional (CONV) layer, known as the stem layer, can significantly impact the TTA results. This is based on the understanding that domain shifts in input images affect model outcomes. To overcome the drawbacks of the aforementioned entropy minimization, we introduce a new paradigm for fast and high-performance TTA. This is achieved using a reconstructed stem layer that incorporates a Gaussian channel attention layer (GCAL) constructed on the squeeze and excitation (SE) block  and domain embedding layer (DEL) based on a two-dimensional discrete wavelet transform (2D DWT) . Our main contributions can be summarized as follows:

* To implement lightweight TTA, we minimize the uncertainty of channel-wise attention in intermediate features extractable from GCAL rather than an entropy minimization strategy. As depicted in Fig. 1, this involves omitting both forward and backward processes for all the parameters except those in the reconstructed stem layer. Using this stem layer as a pivot point without storing additional gradients provides the fastest training and significantly reduces training costs.
* We propose the integration of DEL as a component within the stem layer to maximize the training effect of a single data point. 2D DWT in the stem layer helps collect domain-invariant edge information efficiently and improves model generalization by providing multi-views with redundant content information before CONV operations. Furthermore, forwarding features extracted from multiple frequency domains to GCAL enables the calculation of uncertainty on an individual basis, thereby maximizing data utilization.
* Additionally, DEL includes inverse DWT (IDWT), thereby enabling a non-invasive design. This yields a shape identical to the intermediate feature obtained from the existing stem layer's CONV operation. This, in turn, enables a convenient integration into any model utilizing a convolutional neural network (CNN)-based backbone for image classification and a wide range of applications (_i.e._, high scalability).

Figure 1: Diagram comparing the forward/backward flow and update process with TENT  and EcoTTA , illustrated as representative algorithms for entropy minimization and memory-efficient methods, respectively. The red lock icon indicates the absence of TTA execution.

Experimentally, our method surpasses the state-of-the-art (SOTA) with a 5.4% higher prediction accuracy on CIFAR-10-C, including 15 types of corruptions. On the large dataset, ImageNet-C , it displays an accuracy lower than that of the SOTA model by 2.7%; however, it achieves a significant performance with only 1.7% of memory usage compared to the SOTA model. Furthermore, compared with EcoTTA , which is the SOTA in terms of memory efficiency, our approach uses 5.9% of memory and displays a marginal decrease in accuracy, by only 0.3%. These results highlight our method's effectiveness in balancing performance and memory efficiency. In terms of scalability, compared with EcoTTA, our method achieved a 4.6% improvement in mean intersection over union (mIoU) with 0.04% of memory usage in a semantic segmentation task under four weather conditions (_e.g._, brightness, foggy, rainy, and snow).

## 2 Backgrounds

### Domain Shift Scenarios

Deep learning models learn from high-dimensional data such as images and natural language. These models are influenced significantly by the data distribution . Data derived from the source domain, denoted as \(D_{s}=\{(x_{i}^{s},y_{i}^{s})\}_{i\{0...N-1\}}\), and target domain, denoted as \(D_{t}=\{(x_{i}^{t},y_{i}^{t})\}_{i\{0...N-1\}}\), generally exhibit significantly different distributions. Therefore, it could be challenging for a model trained and fitted on data from \(D_{s}\) to infer data from \(D_{t}\). Scenarios designed to address this domain shift are classified into DG and DA depending on whether there is direct access to the \(D_{t}\) distribution during training. Both strategies primarily aim to align the feature space with domain-invariant content information and style information sensitive to variations. In DG, where access to \(D_{t}\) is unavailable, auxiliary networks are frequently employed to learn the adjustment of means and standard deviations to fit the domain  or use generative models to create \(D_{t}\) adversarial to the distribution of \(D_{s}\) and train simultaneously [49; 35]. Recently, multiple augmentations were employed in conjunction with an equal number of meta-networks to comprehend meta-knowledge regarding the domain directly. This secured superior generalization performance . However, this competitive trend increases the load regarding training costs and inference, thereby necessitating improvements in practicality and efficiency. In DA, where direct access to \(D_{t}\) is feasible, model generalization is more convenient. However, a significant characteristic is the unavailability of labels. Early DA research involved training with labeled data from \(D_{s}\)[16; 58; 64]. However, in practical applications, storing and utilizing the \(D_{s}\) dataset in memory is considered unrealistic. To overcome this issue, source-free DA employs pseudo-labeling techniques by dually leveraging pre-trained models [48; 31]. Nonetheless, these approaches still have the burden of maintaining the dual models in real-world application deployment as well as the problem of confirmation bias.

### Test Time Adaptation

Like DG and DA research, TTA is a scenario developed to solve domain shifts. The focus is on more efficient learning during test-time . TTA essentially assumes training using only data \(x_{i}^{t}\) without the corresponding label \(y_{i}^{t}\). Within these constraints, TTA adapts to the distribution of \(D_{t}\) by partially fine-tuning the layers of a pre-trained model.

BN STAT  indicates the widespread covariate shift across all the domains that need to be addressed and adjusts the fixed parameters of BN layers, specifically, the mean and variance, at test-time. TENT  is the baseline for many TTA approaches. It proposes a strategy to minimize the prediction entropy and significantly improve the performance by updating statistics and affine transformation parameters for each batch. MEMO  applies multiple augmentations to the input at test-time and minimizes the average (marginal entropy) of entropies obtained by passing the model through, thereby demonstrating better adaptation effects. EATA  extends TENT as the baseline. It expounds that data points with high entropy do not contribute to adaptation and proposed criteria for establishing a threshold. SAR  advances further by addressing realistic mixed distribution variations, small batch sizes, and imbalanced label distribution variations. It aims for a stable TTA by allocating the same class to all the samples and proposes a strategy to minimize the sharpness of the loss surface. REALM  achieves the highest performance among entropy minimization-based studies. Unlike other methodologies, REALM introduces a framework based on self-supervised learning that enables the inclusion of noise samples in training without skipping. However, based on existing research, achieving TTA through entropy minimization incurs the basic training cost for performingforward/backward operations globally in the model. Moreover, it has limitations in assessing whether the data is suitable for TTA through entropy.

For these reasons, there is a growing interest in whether the proposed methods can be applied efficiently in the real world. In particular, EcoTTA  introduces a pioneering method to minimize memory consumption during the TTA process. This is achieved by partially placing auxiliary networks for TTA outside the main network. The approach ensures that only the BN layers included in the main and auxiliary networks pass through during backpropagation, significantly shortening the path. This simultaneously reduces the memory required for gradients and the time consumed. MECTA  follows a training flow similar to EcoTTA, where all layers of the model are forwarded and a specialized normalization layer is introduced to minimize memory usage, particularly within the cache. However, it relies on methodologies like TENT and EATA to perform TTA. SFT  based on MEMO divides the model into subsets and introduces a method of learning only certain layers based on the distribution of \(D_{t}\). This provides insight into the fact that learning the initial layers is more effective in supporting adaptation learning than the subsequent ones. Notwithstanding these efforts, it is necessary to forward the entire model for TTA, owing to the training based on entropy. Moreover, during backpropagation, the problem of reaching batch normalization at the beginning of the backbone remains while the gradients of many layers are still being calculated.

Therefore, approaches that can adapt at test time without relying on entropy minimization have also been introduced. DDA  has proposed a method that directly projects inputs obtained from \(D_{t}\) to \(D_{s}\) using a generative model without performing expensive retraining. However, this can yield more expensive results in maintaining additional systems, similar to augmentation.

### Discrete Wavelet Transform

The wavelet transform (WT) provides a flexible time-frequency resolution by analyzing signals at multi scales, making it a more effective tool than the Fourier transform in signal processing . Moreover, owing to the short and localized characteristics of wavelet functions, WT is suitable for processing non-stationary signals . Based on these advantages, 2D DWT  is commonly used to extract detail and edge information in the spatial domain. Additionally, it allows for multi-level decomposition until both the width and height of the input image are powers of two. This enables aggressive summarization and extraction of essential information. Through the inverse transform, the original image can be reconstructed highly accurately with minimal information loss. The 2D DWT process involves sequentially applying a one-dimensional DWT (1D DWT)  to the rows and columns of the input data. For 1D DWT, we operate using the simplest wavelet family, _i.e._, the Haar wavelet . It should be noted that Haar can be performed conveniently through CONV operations . In the first stage, the horizontal direction decomposition is performed to split a single original feature map into two components: the low-frequency component (LFC), which generally contains visually intuitive information and represents the basic image structure, and the high-frequency component (HFC), which includes finer details of the image and a small amount of edge information. In the second stage, a vertical decomposition is performed as the two filters are transposed, as illustrated in Fig. 2. The detailed 2D DWT process is presented in Appendix B.

## 3 Proposed Method

### Overview

**Motivation:** To improve the performance of TTA, numerous studies have focused on efficiently minimizing the entropy of model predictions obtained from unlabeled data \(x_{i}^{t}\). Although this has enhanced the accuracy, efforts to reduce the memory consumption, which significantly impacts the power consumption, and perform TTA faster have been relatively few. Nevertheless, these research areas are essential for applying TTA in real-world scenarios. In practice, using augmentation in image pre-processing during training, dynamically filtering data advantageous for training, or replicating pre-trained weights generates a significant load. Furthermore, we identify a key drawback in most existing TTA studies that adopt entropy. Even without parameter updates, calculating entropy necessitates forward/backward passes through the entire model layers. This imposes a fundamental training cost. This limitation motivates us to resolve the existing issues in TTA.

**Goal:** To eliminate existing drawbacks and minimize training costs, we aim to provide an alternative to entropy minimization as a TTA solution. Specifically, our design goals are summarized as follows: 1) _Practicality in training_: Minimizing the resources required for training, such as memory and data, to attain acceptable a reasonable prediction accuracy in \(D_{t}\). 2) _Scalability_: Designing to be non-invasively and conveniently applicable in CNN-based tasks without modifying other layers. 3) _Data leveraging_: Maximizing usability from independent data to achieve TTA within constraints, even with small batch sizes or a single batch.

**Approach:** The sequential influence of CNN layers results in significant variations in prediction quality across different input image domains. By fine-tuning the first representation of the input image, fast adaptation to \(D_{t}\) is possible. Therefore, instead of expensive entropy, we extract and minimize the channel-wise uncertainty from the reconstructed stem layer to adapt to \(D_{t}\).

**Workflow:** As shown in Fig. 2, the reconstructed stem layer comprises the original CONV layer and two key architectures in this study, GCAL and DEL. The reconstructed stem layer follows the sequence of DWT, CONV, GCAL, and IDWT. The processes of DWT and IDWT are encompassed in DEL. As depicted in Fig. 2, GCAL is the only key architecture that enables adaptation and outputs channel-wise attention and its associated uncertainty in an end-to-end manner. Similarly, as with an SE block, the extracted attention is scalar and applied to each feature on a per-channel basis. Meanwhile, the uncertainty is minimized through negative log-likelihood (NLL) loss. In DEL, DWT decomposes the input feature map into multiple frequency domains while maintaining spatiality. This step is performed before CONV. It allows for capturing more diverse features from independent data, thereby better grasping the differences between \(D_{s}\) and \(D_{t}\). By performing IDWT at the end of the stem layer, the shape of the features is restored to its original state before the stem layer is modified. This enables the non-invasive application of the stem layer in pre-trained models and synthesizes important redundant information from multiple views to enhance the generality. While injecting the stem layer into a pre-trained model for warm-up, we jointly train it to reduce task-specific prediction errors using suitable loss terms (_i.e._, cross-entropy) in conjunction with the uncertainty extracted from GCAL minimized through NLL loss. It should be noted that since uncertainty is minimized only during training, in the TTA setting, the procedures after IDWT are not required.

### GCAL: Gaussian Channel Attention Layer

GCAL is a key architecture that enables TTA using only the stem layer. It remodels the SE block to predict uncertainty. Please note that the SE block  is not incorporated into other layers and is trained end-to-end. It dynamically extracts channel-wise attention \(_{scale}\) defined below from intermediate features and performs recalibration. This enhances the representational capacity of the CONV and, ultimately, the prediction accuracy.

\[_{scale}=F_{se}(,W)=(W_{2}(W_{1},)), \]

Figure 2: Overview of our method including the reconstructed stem layer. \(\) and \(*\) denote element-wise multiplication and CONV operation, respectively, while \(_{k}^{+}\) and \(_{k}^{-}\) respectively denote the low and high filters for DWT, described in Appendix B.

where \(\) represents the intermediate feature obtained after performing global average pooling, which in ResNet  comprises 64 channels. The weight \(W_{1}\) reduces the channel size of \(\) based on a predetermined hyperparameter, whereas \(W_{2}\) conversely expands it back to its original channel size. \(\) and \(\) denote the sigmoid and ReLU functions , respectively.

In Gaussian Yolov3 [8; 9], rather than predicting variables such as the bounding box coordinates, the model is designed to extract the mean and variance of a Gaussian distribution to quantify the uncertainty. Similarly, in this study, the output of the SE block, \(_{scale}\), is represented by Gaussian parameters, namely, mean \(_{}\) and variance \(_{}\). The probability density function for quantifying the uncertainty of \(_{scale}\) is defined as follows:

\[p(;_{},_{})=}}e^{- -)^{2}}{2_{}}}. \]

For implementation, without significantly altering the \(F_{se}\) structure, we obtain \(_{}\) and \(_{}\) by increasing the channel size by 100%. This is shown in Fig. 2. These two Gaussian parameters are defined as follows:

\[_{}=(W_{2}^{}(W_{1},)),_{}= (W_{2}^{}(W_{1},)). \]

To perform TTA, we minimize the Gaussian parameter \(_{}\) to reduce the uncertainty of \(_{scale}\), which is multiplied channel-wise. It should be noted that \(_{scale}=_{}\). However, because \(_{}\) determined dynamically based on the input, it is challenging to determine the ground-truth (GT) \(\) in the TTA setting. Therefore, we aim to modify the SE block to function primarily as an extractor of uncertainty for TTA. In all the training settings, we set the GT of \(_{}\) to the maximum value of the sigmoid function (\(=1\)) to enable training. Reflecting this approach, we redefine the NLL loss to minimize \(_{}\) (an uncertainty) in conjunction with Eq. 2:

\[L_{uncertainty}=_{i=0}^{C-1}-log(p_{i}(_{gt};_{}, _{})), \]

where \(C\) represents the total number of channels in intermediate features. Eq. 4 means minimizing uncertainty for all channels, which applies equally to pretraining and TTA processes.

### DEL: Domain Embedding Layer

In recent TTA scenarios, the entropy minimization model is traditionally learned by filtering out data that is evaluated to have low entropy. However, contrary to this trend,  experimentally shows that HFC derived from input images helps generalize the model. This means that high entropy actually contributes significantly to improving prediction accuracy. This entropy ambiguity must be avoided to improve the individual usability of single input data \(x^{t}\). However, to solve this problem, it is clearly challenging to determine the entropy threshold and control strength without additional training costs at test time.

To alleviate entropy ambiguity, we propose DEL, which encapsulates GCAL and CONV layers with DWT and IDWT layers, as shown in Fig. 2. This decomposes the independent \(x^{t}\) into various frequency domains and allows for end-to-end learning of uncertainty for each channel. Accordingly, we redefine the loss term for uncertainty with Eq. 4 as follows:

\[L_{uncertainty}=_{i=0}^{C-1}_{j=0}^{N-1}-log(p_ {i,j}(_{gt};_{},_{})), \]

where N represents the number of frequencies decomposed into DWT. As shown in Fig. 3(a), the DWT layer maintains spatiality without resynthesis even after frequency decomposition, unlike the well-known transformation method [4; 51], even after decomposition. This makes learning spatial structures, such as edges or patterns, possible without additional computation. The IDWT layer, located last, makes the input of the subsequent layer have the same shape as when there is no DEL and maintains the spatiality of the feature. As a result, DEL enables non-invasive design.

Furthermore, we propose omnidirectional decomposition (ODD), which additively decomposes LFC and HFC simultaneously. As shown in Fig. 3(a), when DWT is performed once, it can be observed that edge information overlapping with LFC still remains in HFC. We decompose edge and noisy information as much as possible at a level that maintains spatiality, making it possible to calculate the uncertainty of a single input \(x^{t}\) more sensitively. Additionally, as a side effect, generalization performance is further enhanced by the model concretely identifying noisy data contained in HFC.

Fig. 3(b) visualizes the changes in the uncertainty map for each of the 64 channels for LFC and HFC as we perform our proposed TTA. Denoted \(_{}\) and \(_{}\) are defined as follows:

\[_{}=|log(F_{se}(_{s};W)-F_{se}(_{t};W))|, \;_{}=|log(F_{se}(_{s};W)-F_{se}(_{t };))|. \]

In \(_{}^{LFC}\), there is a clear difference in the uncertainty for each channel obtained with \(_{s}\) and \(_{t}\) even before performing TTA. On the other hand, \(_{}^{HFC}\) shows that \(F_{SE}()\) does not have the capability to properly obtain uncertainty about \(_{t}\). Therefore, we prove through \(_{}^{HFC}\) that uncertainty can be correctly extracted by performing TTA.

### Interval Training for Continual Setting

For performance assessment of TTA, achieving high prediction performance with low training cost is important, but it is also important to design it to enable continuous training. To overcome catastrophic forgetting, existing studies [55; 62] make this possible by maintaining a teacher model of the same shape as the target model or applying regularization techniques for each layer. Instead of these complicated strategies, we reload the weights of the pre-trained stem layer and update them according to \(D_{t}\) at every short interval. This is an efficient scenario when initialized weights consume less memory and simultaneously perform fast TTA, and the proposed reconstructed stem layer realizes this. The short interval setting allows for natural adaptation even when the domain shifts frequently and preserves the generalization performance of the pre-trained model.

## 4 Experimental Results

In this section, we quantitatively and qualitatively demonstrate three contributions related to TTA obtained by applying stem layers reconstructed by DEL and GCAL to CNN-based models. We evaluate the practicality in training (_i.e._, comparison of accuracy and memory efficiency with SOTA TTA methods in Tab. 1 and Fig. 4), scalability (_i.e._, applicability on image classification and semantic segmentation in Tab. 2), and data leveraging (_i.e._, assessment of the TTA performance at small batch sizes in Fig. 7). In Appendices C,D,E and F, we further rigorously evaluate the proposed method by extending the model, using challenging datasets, and exploring generalization scenarios (_i.e._, source-free DA), along with an ablation study for hyperparameters.

### Implementation Details

In all the experimental settings, the model backbones use ResNet-26 and ResNet-50 . These are pre-trained with ImageNet . To evaluate the robustness of the model in the existing TTA setting,

Figure 3: (a) visualizes the proposed ODD process in detail on a given input. (b) shows the variance of uncertainty according to the domain shift of LFC (=\(LFC_{LL}\)) and HFC (=\(HFC_{LH}\)).

we use the benchmark dataset with the well-established 15 types of corruptions (_e.g._, noise, blur, weather, and digital) . Each corruption is applied to the validation set of the original dataset and has identical content information. These are called CIFAR-10-C, CIFAR-100-C, and ImageNet-C, respectively. The severity of the corruption is divided into five levels. We conventionally apply the most severe level (\(=5\)) for an unbiased comparison with the other methods. Additionally, to measure the memory reliably, we refer to the officially provided code of TinyTL . It is identical to that measured by EcoTTA . This considers the memory for storing model parameters and gradients. The gradient size increases exponentially according to the batch size. A detailed evaluation setting of image classification and semantic segmentation models is presented in Appendix A.

### Main Results

Tab. 1 reports prediction errors with regard to accuracy for the image classification task. As mentioned in Sec. 3, DEL included in the stem layer can display generalization performance without additional

    &  &  &  &  &  \\  & **Gauss.** & **Shot** & **Impul.** & **Defect.** & **Glass** & **Motion** & **Zoom** & **Snow** & **Frost** & **Fog** & **Brit.** & **Contr.** & **Elastic** & **Pixel** & **JPEG** \\   \\  Source & 86.9 & 82.6 & 81.8 & 11.4 & 50.2 & 18.9 & 9.1 & 16.4 & 26.4 & 18.4 & 7.1 & 24.5 & 22.8 & 64.0 & 28.3 & 36.6 \\
**Ours w/o GCAL** & 62.4 & 55.4 & 48.5 & 12.1 & 51.7 & 15.5 & 8.5 & 16.6 & 26.4 & 21.7 & 6.4 & 31.0 & 21.3 & 61.7 & 22.5 & 30.8 \\  TENT & 39.4 & 38.8 & 47.9 & 19.9 & 45.0 & 23.2 & 20.6 & 28.1 & 32.1 & 24.5 & 16.1 & 26.7 & 32.4 & 30.6 & 35.5 & 30.7 \\ MEM0 & 43.5 & 39.9 & 43.3 & 26.4 & 44.4 & 25.1 & 25.0 & 20.9 & 28.3 & 22.8 & 11.9 & 28.3 & 21.1 & 42.8 & **21.7** & 29.7 \\ SFT & 31.9 & 26.7 & 28.9 & 17.7 & 44.2 & 18.4 & 20.2 & 20.8 & 23.4 & 20.7 & 3.9 & 25.4 & 24.5 & 21.9 & 25.1 & 24.2 \\ EATA & 33.9 & 32.8 & 41.4 & 19.4 & 42.4 & 20.5 & 20.1 & 22.4 & 27.1 & 22.7 & 13.8 & 24.0 & 24.0 & 29.3 & 26.5 & 26.7assumptions regarding learning, and this exhibits an average improvement of 5.8% and 9.4% in CIFAR-10-C and ImageNet-C, respectively, compared with the baseline (_i.e._, source - Ours w/o GCAL). It should be noted that we apply the proposed interval training to meet the continual TTA condition. In these experiments, the weights of the reconstructed stem layer are re-initialized to the pre-trained weights every 20 iterations for CIFAR-10-C and every 10 iterations for ImageNet-C. As shown in Tab. 1, for CIFAR-10-C, it shows a 5.4% higher improvement on average than SOTA. Meanwhile, although the results of ImageNet-C in Tab. 1 do not exceed the SOTA performance, it achieves accuracy comparable to SOTA with significantly less memory usage as can be seen in Fig. 4. In addition, it is noteworthy that the proposed method achieves 3.9% better performance than MEMO , which performs entropy minimization using various augmentations in ImageNet-C.

Ultimately, the goal of injecting the stem layer is to demonstrate an efficient TTA scenario that considers the trade-off between efficient prediction and memory consumption, and our method displayed an overwhelming performance advantage in terms of memory-saving compared with prior works. In Fig. 4, we illustrate the memory usage for ResNet-50, with a focus on BN STAT, entropy minimization-based research, and memory-saving EcoTTA . Herein, TENT , EATA , and REALM  displayed equal memory consumption with the result of entropy minimization-based research. Compared to these three approaches, our proposed approach consumes only 28.6%, 2.8%, and 8.8% memory for CIFAR-100-C and 28.6%, 1.7%, and 5.9% memory for ImageNet-C, respectively, for performing TTA.

In Fig. 5, we compare the proposed method with three representative approaches from entropy minimization-based TTA (_i.e._, TENT), diffusion-driven TTA (_i.e._, DDA), and memory-efficient TTA (_i.e._, EcoTTA) methodologies in terms of latency on a general heterogeneous system (_i.e._, CPU and GPU). Our method demonstrates 4.25\(\), 49.56\(\), and 1.76\(\) faster performance in total latency, respectively, indicating that the proposed L-TTA achieves significantly faster training times than existing TTA methods. It is also important to note that since TENT and EcoTTA perform the TTA process using a single independent model, their latency tends to depend on memory usage. In contrast, DDA shows considerable CPU usage due to the image preprocessing tasks such as noise synthesis, which are not handled by the GPU. These CPU-intensive processes can lead to substantial power consumption unless dedicated hardware is available. Therefore, despite lower memory usage, using such methodologies for field applications should be avoided as much as possible.

The proposed stem layer can be conveniently expanded regardless of the task because only the initial parts of the entire network are changed. To demonstrate this, Tab. 2 provides additional results in the semantic segmentation task. Even without performing TTA (_i.e._, Ours w/o GCAL), mIoU is improved by an average of 18.2% and 0.8% compared with the source model and TENT, respectively. When TTA is applied (_i.e._, Ours), it improves mIoU by 4.6% compared to EcoTTA with only 0.04% of memory usage.

### Ablation Studies

**Interval adjustment in the continual scenario.** As discussed in Subsection 3.4, we propose interval training to satisfy the continuous TTA scenario. Accordingly, we conduct an ablation study to observe the prediction error variations using the ResNet-26 model, as reported in Tab. 1. In this experiment, we adjust the iteration interval for performing one round of the TTA process and observe the resulting

  
**Method** & **Bright.** & **Foggy** & **Frost** & **Snow** & **Avg.** & **Mem. (MIB)** \\  Source & 60.4 & 54.3 & 30.0 & 4.1 & 37.2 & - \\ Ours w/o GCAL & 69.9 & 62.9 & 47.2 & 41.6 & 55.4 & - \\  BN STAT & 69.1 & 61.0 & 44.8 & 39.1 & 53.5 & 280 \\ TENT & 70.1 & 62.1 & 46.1 & 40.2 & 54.6 & 2721 \\ EcoTTA & 70.2 & 62.4 & 46.3 & 41.9 & 55.2 & 918 \\  Ours w/o DEL & **72.0** & **65.7** & 49.6 & 45.3 & 58.1 & **0.4** \\ Ours & 70.6 & 63.5 & **53.1** & **52.0** & **59.8** & **0.4** \\   

Table 2: Comparison of mIoU (%) for semantic segmentation in Cityscapes-C using DeepLabV3Plus with prior TTA methods.

prediction error. In Fig. 7, \(0\) and _Full iter._ on the x-axis represent the cases of no TTA and completing 1 epoch, respectively, with the intermediate values showing results for iteration increments of 5 steps. The results demonstrate that applying GCAL and DEL to the stem layer consistently yields better outcomes, with a steady decline in error rate as iterations increase. Furthermore, both methods also outperform REALM, a SOTA approach, after 10 iterations. Notably, the proposed L-TTA (\(=\)DEL\(+\)GCAL) achieves a 5.7% performance improvement after completing 1 epoch.

**Evaluation on small batch sizes.** By implementing DWT in DEL, we provide a foundation for generalization and maximize our capabilities by minimizing individual uncertainties across various frequency domains with spatial properties. We contend that this approach assists data in conveniently adapting independently to the target domain. In Fig. 7, we present the results of performing TTA on ImageNet-C for small batch sizes (\(=\)1, 2, 4, 8), in conjunction with the representative methods of entropy minimization (_i.e._, TENT , EATA  and SAR ). The conventional methods exhibit a near-complete loss of classification capability, with an average prediction error of 99.89% when the batch size is one. Meanwhile, our proposed method demonstrates a higher performance, with a prediction error of 75.16%. Additionally, it is visually evident that our method is less sensitive to variations in batch size, with a difference of 8.37% between batch sizes of one and eight, whereas TENT, EATA and SAR exhibit instability with differences of 31.45%, 22.67% and 30.86%, respectively. This is interpreted as a result of the significant dependency of the conventional methods on BN layers.

## 5 Conclusion

We propose a novel method for lightweight TTA that utilizes the stem layer reconstructed with DEL and GCAL to minimize the uncertainty across multiple frequency domains rather than the entropy. This architecture improves the model's generalization performance without assumptions regarding training and can be applied conveniently and non-invasively to models with a CNN as the backbone. Our approach demonstrates a performance equivalent to or superior to those in SOTA studies with well-known robust benchmark datasets, showcasing the fastest and most memory-efficient results suitable for real-world scenarios.