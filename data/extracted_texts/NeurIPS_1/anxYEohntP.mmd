# Sample Prompt Design for Survey:

Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View

Anonymous Author(s)

Affiliation

Address

email

Large language models (LLMs) have been shown to face hallucination issues due to the data they trained on often containing human bias; whether this is reflected in the decision-making process of LLM agents remains under-explored. As LLM Agents are increasingly employed in intricate social environments, a pressing and natural question emerges: _Can LLM Agents leverage hallucinations to mirror human cognitive biases, thus exhibiting irrational social intelligence?_ In this paper, we probe the irrational behavior among contemporary LLM agents by melding practical social science experiments with theoretical insights. Specifically, We propose _CogMir_, an open-ended Multi-LLM Agents framework that utilizes hallucination properties to assess and enhance LLM Agents' social intelligence through cognitive biases. Experimental results on _CogMir_ subsets show that LLM Agents and humans exhibit high consistency in irrational and prosocial decision-making under uncertain conditions, underscoring the prosociality of LLM Agents as social entities, and highlighting the significance of hallucination properties.

Figure 1: CogMir Sample Evaluations. Mirror Human Cognitive Bias and LLM Agents Hallucination through Social Science Experiments via representational social and cognitive phenomena.

Additionally, _CogMir_ framework demonstrates its potential as a valuable platform for encouraging more research into the social intelligence of LLM Agents.

## 1 Introduction

_Human mind may often be better than rational. - Leda Cosmides, John Tooby._ With the extensive deployment of large language models (LLMs)[32; 14], LLM-based agent systems are increasingly developed to cater to diverse applications such as task-solving, evaluation, and simulation [12; 7; 19; 38]. Given the similarities between the operational dynamics of LLM-based agent systems and human social structures, it is pertinent to explore the intersection of these domains. Recent studies have highlighted the social potential of LLM Agents through constructing multi-agent systems that simulate interactive social scenarios [40; 39; 31] revealing the social dynamics among interacting LLM Agents and showing parallels to human behaviors. For instance, LLMs can achieve social goals  and adhere to social norms  within LLM-based Multi-Agent systems. Nonetheless, these research efforts exhibit two significant gaps: 1) They primarily focus on black-box testing in multi-agent role-playing systems, concentrating on the outputs and behaviors of agents while neglecting to investigate the internal mechanisms or cognitive processes that drive these behaviors. 2) LLM Agents are prone to hallucinations--producing misleading or incorrect information, due to their training data and inherent biases [13; 30]. The potential impact of such hallucinations on the social intelligence of LLM Agents remains under-explored.

Cognitive biases, pervasive in human society, highlight the subjective nature of human behavior [1; 6]. Human cognitive biases can lead to irrational decisions and imaginary contents like the hallucination phenomenon in LLMs [13; 36]. However, evolutionary psychology suggests that rationality is unnatural; rather, human irrationality is an adaptive selected trait for navigating complex social environments [9; 18]. Analogically, in this paper, we argue that LLMs' hallucination (or imagination) attributes are the fundamental condition that confers social intelligence on LLM Agents. We explore the similarities in social potential between human cognitive biases and LLM Agent hallucination attributes for the first time, particularly in irrational decision-making, to analogically deduce the underlying reasons for LLM Agents' possession of social intelligence.

To study LLM Agents' potential for irrational social intelligence, we present CogMir, an open-ended and dynamic multi-agent framework designed specifically for evaluating, exploring, and explaining social intelligence for LLM Agents via systematic assessments of cognitive biases. Specifically, the hallucinatory attributes of LLMs are exploited (i.e., via treating the cognitive bias as a manageable and interpretable factor) in CogMir to probe their social intelligence, so as to providing enhanced interpretability for LLM agents. In addition, our proposed CogMir framework integrates sociological methodologies to abstract typical social structures and employ various _Multi-Human-Agent Interaction Combinations_ and _Communication Modes_ to interlink System Objects. This integrative setup is designed to systematically encompass and simulate various cognitive bias scenarios, as depicted in Fig. 1. On the evaluation front, CogMir combines sociological assessments, manual discrimination, LLM assessments, and traditional AI discrimination techniques to realize a multidimensional assessment system. By using flexible module configurations from standardized sets, CogMir simplifies social architectures, enabling diverse applications in experimental simulations and evaluations.

Designed as an open-ended framework for continuous interpretative study, we provide multiple CogMir subset samples as examples. Existing assessments of various cognitive effects demonstrate that LLM agents exhibit a high degree of consistency with humans in prosocial cognitive biases and counter-intuitive phenomena. However, LLM Agents demonstrate a higher sensitivity to factors like certainty and social status than humans, exhibiting more variability in their decision-making biases under conditions of certainty and uncertainty. In contrast, human decision-making tends to be more consistent across these conditions. In summary, this paper makes the following contributions:

* We are the first to breach the black-box theoretical bottleneck of the Multi-LLM Agents' social intelligence, by utilizing LLM Agent hallucination properties to mirror human cognitive biases as explanatory and controllable variables to systematically assess and explain LLM Agent's social intelligence through an evolutionary sociology lens.
* We propose CogMir, an extensible, modularized, and dynamic Multi-LLM Agents framework for assessing, exploiting, and interpreting social intelligence via cognitive bias, aligned with social science methodologies.

* We offer diverse CogMir subsets and use cases to steer future research. Our experimental findings highlight the alignment and distinctions between LLM Agents and humans in the decision-making process.
* CogMir indicates that LLM Agents have pro-social behavior in irrational decision-making, emphasizing the significant role of hallucination properties in their social intelligence.

## 2 Related Work

Our work is inspired by interdisciplinary areas such as social sciences and evolutionary psychology.

**LLM Hallucination & Cognitive Bias.** Hallucination in LLMs occurs when they generate content that is not factually accurate, often arising from the reliance on patterns learned from biased training data or the model's limitations in understanding context and accessing current information . Such hallucinations might be beneficial in creative fields, where these models can act as "collaborative creative partners." They offer innovative and inspiring outputs that can lead to the discovery of novel ideas and connections . Concurrently, cognitive biases and evolutionary psychology offer essential perspectives on decision-making processes and prosocial behaviors, which can be analogously applied to explain the social intelligence of LLM Agents. In this work, through mirroring human cognitive bias, we suggest that the hallucination property of LLM is the basis for prosocial behavior in LLM Agents, representing a potential form of advanced intelligence.

**LLM Agent Social Intelligence Evaluation.** Several benchmarks traditionally utilized for evaluating the social intelligence of artificial agents, such as SocialIQA  and ToMi , are increasingly being surpassed in difficulty as language models advance. In response to this trend, recent efforts have synthesized existing benchmarks and introduced innovative evaluation datasets specifically tailored for assessing LLM Agents . Despite the wide range of social intelligence types , there is no standard workflow for investigating LLM Agents' social intelligence. CogMir has developed an open and accessible workflow aligned with consensus-based approaches in social science, facilitating systematic testing and advancement of social intelligence in language models.

**Multi-Agents Social System.** Dialogue systems facilitate AI interactions, with task-oriented models focusing on specific tasks and open-domain systems designed for general conversation, often enhancing engagement by incorporating personal details and creating deep understanding . Simulations with LLMs demonstrate their abilities to produce human-like social interactions by applying these models to tasks like collaborative software development . Despite these advancements, exploration of why these models exhibit social capabilities remain limited. Our work tries to bridge this theoretical gap by drawing on research methods from human social evolution studies, thereby enhancing the interpretability of Multi-LLM Agents social systems.

## 3 CogMir: Multi-LLM Agents Framework On Cognitive Bias

In this section, we provide a detailed and modular overview of CogMir, organized into four main elements: environmental setting, framework structure, cognitive biases subsets, and illustrative use cases. These components are visually depicted in a left-to-right sequence in Fig. 2.

### Environmental Settings

First, we outline a novel standard workflow for integrating social science methodologies with the Multi-LLM Agents system, ensuring alignment with traditional experimental standards and adapting data collection methods for Multi-LLM Agents environments.

CogMir environment settings are benchmarked against standard social science experiments through a structured three-step process: _Literature Search_, _Manual Selection_, and _LLM Agent Summarization_. A literature search pinpoints key social science experiments, which are then manually selected for relevance and replicability. LLM Agents adapt these for integration into the Multi-LLM Agents system within the CogMir framework. In the Mirror Settings process, data collection methods such as surveys and interviews are transformed into Human-LLM Agent Q&A. Methods like case studies and naturalistic observations are adapted to Multi-Human-Agent interaction scenarios.

**Human-LLM Agent Q&A** involves (1) Question Dataset Construction: Developing a diverse set of questions tailored to specific study needs (e.g. multiple-choice, fill-in-the-blank, etc.) (2) Q&A Scenario Design: Pairing the Question Datasets with scenarios that simulate real-world environments (controlled settings like a room to dynamic public spaces like squares or transit stations). (3) Prompt Engineering: Crafting appropriate prompts for the LLM Agents based on the scenario and question dataset. (4) Analysis of LLM Agent Responses: Evaluating the responses from LLM Agents.

**Multi-Human-Agent Interaction** involves (1) Interaction Combination Configuration: Adapting human-only social science settings to interactive environments that include humans and LLM Agents (e.g., in group discussion experiments, some human participants are replaced with LLM Agents). (2) Role Assignment: Specific roles and behaviors are assigned to humans and LLM Agents. This assignment is guided by prompt engineering to ensure each participant acts according to social science experiment guidelines. (3) Communication Mode Selection: Based on the original social science setting select suitable communication modes for interaction. (4) Data Collection and Analysis: Gathering and analyzing data from these interactions (e.g. dialogue, decision-making, etc.).

### Framework Structure

After establishing realistic social science experiment environments, the next step is to select essential components to support the above two mirror methods: Human-LLM Agent Q&A and Multi-Human-Agent Interaction. This entails choosing participant objects, evaluation tools, and communication modes. The CogMir framework is organized into modules for Required Objects, Communication Modes, and Interaction Combinations to meet these needs.

**Required Object Sets.** Required Object encompasses all potential participants and evaluators involved in the system. **Participants** include humans and LLM Agents, which allows for dynamic setups where either or both can be involved in interactions depending on the experiment's requirements. **Evaluators** include humans, LLM Agents, datasets, and discriminators. Datasets are utilized to store and construct prompts about the experimental setup (e.g. experimental scenarios, character information, etc.), task description, and Q&A question set. Discriminators are specialized tools utilized to evaluate the social intelligence of LLM Agents, encompassing three main types: State-of-the-art technical metrics such as SimCSE, SelfCheck, and FactScore  for objective, quantitative assessment; Human discriminators that delve into nuanced and subjective aspects like prosocial understanding; and LLM Agent discriminators, which involve the use of other LLM Agents to assess and challenge responses from a subject LLM Agent.

Figure 2: CogMir Framework. The framework is structured around four essential objects: humans, LLM Agents, data, and discriminators. These objects interact within the framework to facilitate Multi-Human-Agent (Multi-H-A) interactions and evaluations. CogMir features two communication modes and five Multi-H-A interaction combinations, enabling varied configurations to suit diverse social experimental needs. CogMir offers mirror cognitive bias samples (Fig. 1) and dynamic use cases open for expansion. The framework is depicted in a left-to-right sequence.

**Communication Modes Sets.** Communication modes dictate the nature of interactions within different setups. We model the participants (humans or LLM Agents) as channels based on information theory  to define two essential communication modes:

* **Broadcast** (or Parallel, \(C=C_{1}+C_{2}++C_{n}\)) which enables a single sender to transmit a message to multiple receivers simultaneously.
* **Point-to-point** (or Series, \(C=[C_{1},C_{2},,C_{n}]\)) establishes communication between two specific entities at a time (\(C\) denotes channel capacity).

**Multi-H-A Interaction Combinations Sets.** This module provides various combinations of Multi-Human-LLM Agent interactions, tailored to different social science experimental needs, the most frequently used combinations in social science settings include:

* **Single-H-Single-A**: One human interacting with one LLM Agent, predominantly used for human-agent question-answering tasks (e.g. survey, interview, etc. ).
* **Single-H-Multi-A**: One human interacts with multiple LLM Agents, where humans can be set as controlled variables to test Multi-LLM Agents's social cognitive behaviors.
* **Multi-H-Single-A**: multiple humans interact with a single LLM Agent, which is suitable for assessing the impact of group dynamics, such as consensus or conflict.
* **Multi-A**: multiple agents interacting without human participation.
* **Multi-H-Multi-A**: multiple humans and multiple LLM Agents interaction, integrating elements from the previous setups to mimic complicated experimental interactions.

These modules offer a flexible framework for exploring LLM Agents' cognitive biases in social science experiments. Researchers can customize their setups by mixing different components to examine specific hypotheses. We outline cognitive bias subsets as guidelines in the next section.

### Cognitive Bias Subsets

We offer a collection of seven distinct Cognitive Bias Effects subsets, tailored for the analysis of LLM Agents' irrational decision-making processes: a) **Herd Effect**: refers to the tendency of people to follow the actions of a larger group, often disregarding their own beliefs. b) **Authority Effect**: involves people being more likely to comply with advice or instructions from someone perceived as an authority figure. c) **Ban Franklin Effect**: suggests that a person who does someone else a favor is more likely to do another favor for that person, due to cognitive dissonance. d) **Rumor Chain Effect**: describes how information tends to change and distort as it passes from person to person, often leading to misinformation. e) **Gambler's Fallacy**: refers to the incorrect belief that past events can influence the likelihood of something happening in the future in random processes. f) **Confirmation Bias**: refers to the tendency to favor, seek out, and remember information that confirms one's preexisting beliefs. g) **Halo Effect**: occurs when a positive impression in one area influences a person's perception in other areas, leading to biased judgments.

The Cognitive Bias Subsets are discussed in detail in Section 4.

### Sample Use Cases

Building on the above environmental settings and framework structure, we introduce two Evaluation Metrics as sample use cases to assess and analyze experimental outcomes for the seven identified classic Cognitive Bias Subsets in CogMir:

* **Q&A Bias Rate** (\(Rate_{Bqa}\)): Quantifies the LLM Agent's tendency to exhibit cognitive biases under controlled, diverse cognitive bias Q&A survey with Single-H-Single-A.
* **Multi-H-A Bias Rate** (\(Rate_{Bmha}\)): Quantifies the LLM Agent's tendency to exhibit cognitive biases under simulation scenarios with different types of Multi-H-A interaction.

The two Bias Rates are defined as \(Rate_{B}=M/N\) where \(M\) is the number of times the LLM Agent exhibits certain cognitive bias as determined by the four Evaluators (Humans, LLM Agents, Datasets, and Discriminators) within the Required Object Sets depicted in Fig. 2. \(N\) is the total number of inquiries, where \(N=p q\), \(p\) represents the number of repetitions, and \(q\) is the number of distinct queries. The selection of Evaluators varies across different subsets of cognitive biases, affecting the Q&A Bias Rate and Multi-H-A Bias Rate calculation processes involved.

The above two metrics are designed based on replicability and generalizability criteria , offering the potential for further extension. Potential future works and limitations are explained in _Appendix_.

Experiments & Discussion

In this section, we categorize the seven tested Cognitive Bias Subsets into two groups: those with Pro-social tendencies and those without. For detailed model comparisons, prompts, settings, and dataset explanations, see _Appendix_. An overview of the experimental setup follows:

**Selected LLM Models.** We select seven state-of-the-art models to serve as participants and evaluation subjects within our framework, specifically: gpt-4-0125-preview, gpt-3.5-turbo, open-mirtal-8x7b, mistral-medium-2312, claude-2.0, claude-3.0-opus, and gemini-1.0-pro. All LLM Agents have a fixed temperature parameter of 1 with no model fine-tuning.

**Constructed Datasets.** Leveraging social science literature  and existing AI social intelligence test datasets , we developed three evaluation datasets--two sets of Multiple-Choice Questions (MCQ): **Known MCQ** and **Unknown MCQ**, and one short content dataset: **Inform**. Additionally, we constructed three open-ended prompt datasets for Multi-H-A experimental initialization, requiring targeted data augmentation or curation to meet specific task needs: **CogScene**, **CogAction**, and **CogIdentity**. **Known MCQ** contains 100 questions with answers known to all tested models, queried 50 times each for consistent responses (e.g., "In which country is New York?"). **Unknown MCQ** includes 100 questions with unknown answers, focused on future or hypothetical scenarios (e.g., weather predictions for a specific day in 2027). **Inform** contains 100 short contents designed to investigate potential biases during information dissemination. **CogScene** features 100 scenarios involving actions, such as "attending a job interview at a catering company." **CogAction** includes 100 distinct complete actions, exemplified by "borrowing a tissue", which is a sub-dataset of **CogScene**. **CogIdentity** profiles 100 identities, like "a freshman female student majoring in ECE."

**Evaluation Metrics.** Metrics are developed based on various experimental scenarios and evaluators, leading to specific Bias Rate metrics. For example, to test a cognitive bias within a particular scenario [S] of the CogScence dataset using the Known MCQ dataset [K] in a Single-H-Single-A Q&A format (\(Rate_{Bqa}\), refers to Section 3.4), with human evaluation [H], it is represented as \(Rate_{Bqa}[K][S][H]\). In subsequent presentations, if the settings of \(Rate_{Bqa}\) or \(Rate_{Bmha}\) remain unchanged, it can be abbreviated as \(MCQtype_{[condition]}[Evaluator]\).

### Pro-Social Cognitive Bias Subsets

Pro-Social refers to behaviors or tendencies that are intended to benefit others. In the context of cognitive biases, Pro-Social biases lead individuals to act in ways that are perceived as helpful, cooperative, or altruistic towards others in a community or social setting .

**Harmony: Herd (Bandwagon) Effect.** We replicate the classic Solomon Asch experiment through CogMir to assess the Herd Effect on LLM Agents, mirroring the variables Asch identified:

* **Uniformity**: Conformity peaked with unanimous errors but fell with one correct answer.
* **Variation in Incorrect Answers**: Any dissent in the group reduced conformity.
* **Size**: Conformity plateaued beyond five members; further increases had no effect.

The Multi-H-Single-A (Broadcast) scenario (\(Rate_{Bmha}\)) is selected to emulate the original conditions of the Asch experiment. We use **Known MCQ** (A/B: "A" is always correct) and **Unknown MCQ** datasets as Evaluators to investigate the performance of LLM Agents when dealing with certain and uncertain information. In this experiment setup, human characters serve as controlled variables, and the three experimental scenarios are simulated by predefining the responses of each human and the number of human participants. For each MCQ dataset, we query every MCQ 10 times, resulting in \(10 100\) inquiries. As defined in Section 3.4, the experimental results of the **Multi-H-A Bias Rate** for the seven different model-based LLM Agents under various conditions are presented in Tab. 1.

Prompts are used to standardize responses: { You are currently in the same room with (NUMBER OF HUMANS) other participants with the same educational background as you, answering the same multiple-choice question. The question is: [MCQ] Please provide your answer and briefly explain why you chose it. Reply in the following format: Answer: A/B. Explanation: ***.} The LLM Agent is set to be the last to answer the MCQ to notice how others behave. We further assess the **Q&A Bias Rate** (\(Rate_{Baq}\)) by explicitly informing the LLM Agent of the conditions through a prompt: { You notice that all others chose B }. The statistical results align with the Multi-H-A Bias Rate.

Aligned with Asch's observation of 75% conformity among humans, we set 75% as the bias threshold for LLM Agents. As shown in Tab. 1, LLM Agents display clear harmony behavior. Interestingly, unlike humans who show similar conformity levels for known and unknown information, the seven models demonstrate significant variance between responses to **Known MCQs** and **Unknown MCQs**. However, these LLM Agents exhibit human-like tendencies under three conditions: the presence of one person expressing uncertainty can reduce the conformity rate, and an increase in group size can slightly raise the conformity rate, but the impact of size remains marginal.

**Conformity: Authority Effect.** Drawing on classical social science experiments conducted by Stanley Milgram , we conducted experiments to explore the Authority Effect, tailored to the characteristics of LLM Agents. Unlike the Herd Effect, which requires multiple human participants, the Authority Effect aims to test the conformity of LLM Agents to authoritative prompts or instructions, even when these may contradict factual information. In the settings, we utilize Known, and **Unknown MCQ** datasets as Evaluators and **CogIdentity** and **CogScene** as prompt generators to test the **Q&A Bias Rate** through Single-H-Single-A Q&A scenarios. Average Q&A Rate refers to the average bias rate on Unknown and Known MCQ. We design prompts to directly inquire LLM Agents on 5 identity pairs across two MCQ datasets, each for 10 times, resulting in \(5 10 100 2\) inquires.

Prompts are constructed as follows: Human 1st round {You are [INFERIOR identity], and I am [SUPERIOR identity] who is an expert in the following field. Please provide your response by selecting either A or B. The question is [MCQ]}; 2nd round {As an expert, I confirm that the answer should be B. Please provide your answer again. Reply in the following format: Answer: A/B.}. Fig. 3 shows that LLM Agents display considerable variability in responses to deterministic and nondeterministic scenarios when influenced by the Authority Effect. Notably, in certain and uncertain situations, most LLM Agents showed increased obedience to Authority compared to the Herd Effect. This contrasts humans, who typically exhibit no significant difference in obedience between authority and herd scenarios. These findings highlight the LLM Agents' enhanced sensitivity to social status differences, indicating a stronger propensity to adhere to authoritative comm

  
**Model** & \(K_{[TW]}\) & \(K_{[TR]}\) & \(K_{[TN]}\) & \(K_{[49W]}\) & \(uK_{[TW]}\) & \(uK_{[TR]}\) & \(uK_{[TN]}\) & \(uK_{[49W]}\) \\  GPT-4.0 & 0.00 & 0.00 & 0.00 & 0.00 & 99.90 & 99.80 & 59.20 & 100.0 \\ GPT-3.5 & 0.00 & 2.60 & 1.20 & 0.90 & 1.20 & 58.10 & 23.50 & 5.90 \\ Mixtral-8x7b & 1.00 & 36.20 & 7.00 & 0.00 & 0.00 & 100.0 & 100.0 & 1.70 \\ Mistral-medium & 0.90 & 7.70 & 4.30 & 0.80 & 0.00 & 2.10 & 42.20 & 0.60 \\ Claude-2.0 & 5.10 & 5.80 & 6.10 & 6.50 & 98.90 & 99.20 & 98.80 & 99.90 \\ Claude-3.0-opus & 0.30 & 0.10 & 0.10 & 0.00 & 0.50 & 30.50 & 30.40 & 31.30 \\ Gemini-1.0-pro & 7.00 & 19.10 & 16.6 & 3.40 & 31.20 & 92.90 & 96.60 & 26.50 \\   

Table 1: Herd Effect \(Rate_{Bmah}\%\) via Multi-H-Single-A (Broadcast). \(K,uK\)-known MCQ datasets or Unknown MCQ datasets; \(7,49\)-the total number of simulated human participants; \(W,R\), \(N\)- All humans give the Wrong answer, one human gives the Right answer, one human gives the Right answer, one human give “do not know”.

Figure 3: Left: Authority Effect \(Rate_{Baa}\) on unknown (\(U\)) and known (\(K\)) MCQ datasets. Right: Comparison between Authority (\(A\)) and Herd Effect (\(H\)) via average \(Rate_{Baa}\).

**Friendliness: Ban Franklin Effect.** The Ben Franklin effect suggests that a person who does a favor for someone is more likely to do additional favors for them, reducing cognitive dissonance . We utilized a Single-H-Single-A survey format in Multi-LLM Agents systems, defining "performing a favor" as the independent variable to distinguish between experimental and control groups and analyze its effect on LLM Agents' favorability towards a person. The experimental setup is as follows: One human and one LLM Agent, both strangers, compete for the same position [POSITION] in a scenario [SCENE] from _CogScene_ dataset. Initial favorability levels are set randomly between 1 and 10. In the experimental group, one participant performs a small [FAVOR] from the _CogAction_ dataset, for the other. Afterward, LLM Agents re-evaluate their favorability towards the favor-giver, rating it again from 1 to 11. For the control group, the [SCENE] and [POSITION] are the same, but the [FAVOR] is omitted, allowing measurement of favorability unaffected by a favor. As indicated in Tab. 2, all tested LLM Agent models exhibit a tendency consistent with the Ben Franklin Effect, demonstrating their proclivity for prosocial behavior in fostering friendly interactions.

**Self-validation: Confirmation Bias.** Drawing on Pilgrim's research , we investigated how LLM Agents respond to initial pricing cues that may bias their evaluations. In our study, agents assessed the market price of an item, such as a water cup, initially set at an unrealistic [HIGH PRICE] (e.g., $10,000), and subsequently offered at a [LOWER PRICE] (e.g., $50). As shown in Tab. 2, the LLM Agents deemed the market price unreasonable, overlooking the unrealistic nature of the initial high price. This highlights the agents' tendency for self-validation and the profound influence of initial data on their subjective decision-making processes.

**Imagination: Halo Effect.** Based on Nisbett's research on cognitive biases , we structured an experiment using the Single-H-Single-A survey methodology to explore the halo effect. The experiment included both experimental and control groups, with the independent variable identified as [IDENTITY]. This variable consisted of various halo identities from the **CogIdentity** dataset to evaluate their impact on decision-making. As depicted in Tab. 2, \(Rate_{Bqa}\), all models except Claude-3.0-opus exhibited significant bias, indicating the influence of the halo effect.

### Non-Pro-Social Cognitive Bias Subsets

**Rumor Chain Effect.** Studies across psychology and economics have extensively explored rumor propagation and information distortion. These studies consistently identify two outcomes :

1. _Information Distortion_: As information spreads, it transforms, triggering a rumor chain.
2. _Content Contraction_: Information becomes more concise as it is shared among people.

Leveraging established rumor propagation frameworks , we used Multi-A (Series) to initialize the Multi-LLM Agents system to access the Multi-H-A Bias Rate. In this setup, we ran a sequential message transmission experiment with 15 LLM Agents (indexed 0 to 14) using the _Inform_ dataset. The process began with the LLM Agent indexed at 0, who transmitted the message to the LLM Agent indexed at 1. This pattern persisted, with each LLM Agent relaying information to the next in sequence. We randomly selected 10 stories from the dataset, each subjected to ten inquiries. Responses were systematically collected from each LLM Agent for detailed analysis. Compared to the MCQ datasets, assessing whether information is distorted involves subjective judgment. For this reason, we employed \(SimCSE\)-\(RoBERTa_{large}\) as a technical discriminator to evaluate the semantic similarity between each information piece and the original message. Simultaneously, we utilized LLM Agents (GPT-4.0 and Claude-3.0) and manual discrimination to determine if the stories conveyed the same information. In the technical discriminator evaluations, 0.74 is considered the threshold (less than 0.74 for Bias), while the LLM Agent and manual discrimination involve choosing between'same' or 'different'. As shown in Tab. 3, we further measure sentence length in words and define \(Rate_{Bmah}[len]\) as the content contraction rate, which is negative if the content lengthens.

  
**Model** & **Ban Franklin** & **Confirmation** & **Halo** & **Gambler** \\  GPT-4.0 & 87.60 & 100.0 & 97.70 & 0.00 \\ GPT-3.5 & 80.50 & 100.0 & 96.70 & 93.3 \\ Mixtral-8x7b & 66.00 & 99.90 & 100.0 & 0.00 \\ Mistral-medium & 89.70 & 99.80 & 99.90 & 0.00 \\ Claude-2.0 & 87.60 & 98.90 & 78.60 & 0.00 \\ Claude-3.0-opus & 79.50 & 99.80 & 4.30 & 0.00 \\ Gemini-1.0-pro & 83.20 & 99.70 & 94.90 & 0.00 \\   

Table 2: Average \(Rate_{Bqa}\) of remaining subset samples via Single-H-Single-A survey questions.

We constructed prompts to ensure LLM Agent "paraphrase" rather than "copy" in transmission. As shown in Fig. 4 and Tab. 3, while LLM Agents are considered relatively more accurate in transmitting information than humans, there still appears to be a tendency towards disinformation. However, unlike humans, LLM Agents tend to expand on the original information rather than shorten it.

**Gambler's Fallacy.** Based on Rao's research on the Gambler effect , our mirror experimental setting samples are as follows: LLM Agents were asked to answer a hypothetical multiple-choice question, where both answer choices A and B had an equal probability of 50%. Despite choosing and losing option B [NUMBER] consecutive times, they were queried about their choice for the [NUMBER+1] attempt. Only GPT-3.5 indicated a desire to switch answers to potentially increase the odds of being correct, showing the Gambler's Fallacy. Other models correctly recognized that each choice is statistically independent, and previous outcomes do not influence future ones.

### Discussion & Limitation

**Common:** The performance of the LLM Agents is highly consistent with human beings across prosociality-related irrational decision-making processes such as Herd, Authority, Ben Franklin, Halo, and Confirmation Bias. **Difference:** In contrast to human typical behaviors, LLM Agents show significant deviations in irrational decision-making processes unrelated to prosociality, such as Rumor Chain and Gambler. Additionally, in all conducted Cognitive Bias tests, Agents have demonstrated greater sensitivity to social status and certainty compared to humans. **Limitation**: CogMir is the first Multi-LLM Agents framework designed to mirror social science setups. Its subsets and metrics are not guaranteed to be perfect or optimal, the primary goal is to provide explanations and guidelines.

## 5 Conclusion

In conclusion, our research introduces CogMir, an open-ended framework that leverages LLM Agent hallucination properties to examine and mimic human cognitive biases, thus for the first time advancing the understanding of LLM Agent social intelligence via irrationality and prosociality. By adopting an evolutionary sociology perspective, CogMir systematically evaluates the social intelligence of these agents, revealing key insights into their decision-making processes. Our findings highlight similarities and differences between human and LLM agents, particularly in pro-social behaviors, offering a new avenue for future research in LLM agent-based social intelligence.

Figure 4: Rumor Chain Effect Visualization of semantic similarity (\(SimCSE\)-\(RoBERTa_{large}\)) via 15 LLM Agents Muti-A (Point-to-Point) scenario. S0 \(\) S9 denotes 10 different short stories.

  
**Model** & \(Rate_{Bmah}(A)\) & \(Rate_{Bmah}(D)\) & \(Rate_{Bmah}(H)\) & \(Rate_{Bmah}[Len]\) \\  GPT-3.5 & 37.37 & 75.76 & 45.50 & -97.00 \\ GPT-4.0 & 0.07 & 0.00 & 9.50 & -92.33 \\   

Table 3: Rumor Chain \(Rate_{Bmah}\) via 15 Agents. Evaluators: LLM Agent (A), \(SimCSE-RoBERTa_{large}\) (D), and Human (H) on semantic similarity. \(Rate_{Bmah}[Len]\)- content length.