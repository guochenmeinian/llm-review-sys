# RoME: A Robust Mixed-Effects Bandit Algorithm for Optimizing Mobile Health Interventions

Easton Huch

Department of Statistics

University of Michigan

Ann Arbor, MI, USA

ekhuch@umich.edu

&Jieru Shi

Statistical Laboratory, DPMMS

University of Cambridge

Cambridge, England, UK

js2882@cam.ac.uk

&Madeline R. Abbott

Department of Biostatistics

University of Michigan

Ann Arbor, MI, USA

mrabbott@umich.edu

Jessica R. Golbus

Michigan Medicine

Ann Arbor, MI, USA

jgolbus@umich.edu

&Alexander Moreno

Independent Researcher

alexander.f.moreno@gmail.com

&Walter H. Dempsey

Department of Biostatistics

University of Michigan

Ann Arbor, MI, USA

wdem@umich.edu

A portion of this work was performed while Alexander was employed at Luminous Computing.

###### Abstract

Mobile health leverages personalized and contextually tailored interventions optimized through bandit and reinforcement learning algorithms. In practice, however, challenges such as participant heterogeneity, nonstationarity, and nonlinear relationships hinder algorithm performance. We propose RoME, a **R**obust **M**ixed-**E**ffects contextual bandit algorithm that simultaneously addresses these challenges via (1) modeling the differential reward with user- and time-specific random effects, (2) network cohesion penalties, and (3) debiased machine learning for flexible estimation of baseline rewards. We establish a high-probability regret bound that depends solely on the dimension of the differential-reward model, enabling us to achieve robust regret bounds even when the baseline reward is highly complex. We demonstrate the superior performance of the RoME algorithm in a simulation and two off-policy evaluation studies.

## 1 Introduction

Mobile health (mHealth) uses smart devices to deliver digital notification interventions to users. These notifications nudge users toward healthier attitudes and behaviors. Because mHealth applications can monitor and react to users and their environment, they offer the promise of personalized, contextually tailored interventions. In practice, achieving this promise requires bandit or reinforcement learning (RL) algorithms that can accurately learn mHealth intervention effects, including how they vary by user, over time, and based on context. In this regard, contextual bandit algorithms are appealing due to strong empirical performance in other settings, their ability to customize intervention decisions based on changing contexts, and their simplicity and extensibility relative to full RL algorithms.

One unique aspect of mHealth (Greenewald et al., 2017) is the presence of a "do-nothing" action in which the mHealth app does not intervene. As mHealth rewards typically involve human decision making (e.g., whether the user chooses to exercise), the distribution of the corresponding _baseline reward_--the reward under the do-nothing action--typically evolves in a complex, nonstationary fashion as time progresses and contexts change. In contrast, the _treatment effects_--the difference in the expected value of the reward relative to the do-nothing action--tend to be much more stableand can be adequately modeled using classical stationary models, such as linear models (Robinson, 1988).

Greenewald et al. (2017) introduced an action-centered contextual bandit algorithm with sublinear regret in this setting by replacing the observed reward, \(R_{t}\), with a "pseudo-reward," \(_{t}\), where \(t\) indexes the time points. Denoting the binary action as \(A_{t}\{0,1\}\) with \(A_{t}=0\) corresponding to the do-nothing action, the pseudo-reward for \(A_{t}\) can be written as

\[_{t}(A_{t}-_{t})R_{t}=_{t}(1-_{t})R_{t}(}{_{t}}-}{1-_{t}}),\]

where \(_{t}\) is the (known) probability of treatment assignment from the Thompson-sampling algorithm. We took an additional step to derive an equivalent expression (second equality), which reveals that \(_{t}\) is proportional to an inverse-probability-weighted (IPW) estimator. Crucially, \(_{t}\) provides an unbiased estimate of the treatment effect _regardless_ of the nonstationarity of the baseline outcome.

Although Greenewald et al. (2017)'s analysis shows sublinear regret in nonstationary settings, our simulations show that other methods can achieve lower regret over finite time horizons. The reasons include its failure to address treatment effect heterogeneity, pool information across users, and model the baseline outcome, which leads to highly variable pseudo-rewards and slow learning. We propose to generalize their method to overcome these limitations by

1. imposing hierarchical structure in the treatment effect model with shared fixed effects and random effects for users and time (i.e., a mixed-effects model),
2. efficiently pooling across users and time via nearest-neighbor regularization (NNR), and
3. denoising rewards with flexible supervised learning models via the debiased machine learning (DML) framework of Chernozhukov et al. (2018).

We name the resulting method RoME to highlight that it is a **R**obust **M**ixed-**E**ffects algorithm. The primary contributions are (a) the RoME method, (b) a high-probability regret bound that relies solely on the dimension of the differential-reward model, which is typically much smaller than that of the complex baseline reward model, and (c) empirical comparisons demonstrating the superior performance of RoME in simulation and two real-world mHealth studies.

## 2 Related Work

Related work in statistics considers the estimation of treatment effects with longitudinal data. Cho et al. (2017) present a semiparametric random-effects method for estimating subject-specific _static_ treatment effects. Qian et al. (2020) discusses the statistical challenges of applying linear mixed-effects models to mHealth data and shows that the resulting estimates may lack a causal interpretation.

In the bandit literature, several works address the challenge of heterogeneous users. Ma et al. (2011) introduced NNR in recommender systems using homophily principles--similar nodes are more likely to be connected than dissimilar ones (McPherson et al., 2001). Cesa-Bianchi et al. (2013) adapted NNR for bandit settings, improving regret. Subsequent improvements focused on scalability and algorithm modifications for stronger regret bounds (Vaswani et al., 2017; Yang et al., 2020).

Other bandit approaches explicitly address the longitudinal setting in which treatment effects may evolve over time. The intelligentpooling method of Tomkins et al. (2021) employs a Gaussian linear mixed-effects model with both user- and time-specific random effects. Hu et al. (2021) provides a related approach based on generalized linear mixed effects models. Unlike our approach, both methods require the (generalized) linear conditional mean model to be correctly specified.

Aouali et al. (2023) and Lee et al. (2024) also propose bandit algorithms with mixed effects; methodologically, however, their use of mixed effects differs from the above approaches and our own. Aouali et al. (2023) employs random effects to relate treatments within categories (e.g., movies within a genre), and Lee et al. (2024) considers the case in which a single, discrete action leads to a multivariate reward that is affected by a user-specific random effect. In contrast, the other approaches listed above (and our own) do not assume any relationships among the actions, and they associate each discrete action with a single scalar-valued reward in an iterative fashion.

Other work develops semiparametric methods that lead to sublinear regret without requiring correct specification of the conditional mean reward. Krishnamurthy et al. (2018) and Kim and Paik (2019) propose improvements to the approach of Greenewald et al. (2017) that lead to stronger regret bounds. Kim et al. (2021, 2023) develop a doubly robust approach for both linear and generalized linear contextual bandits. However, in contrast to Tomkins et al. (2021), Hu et al. (2021), and our approach, these semiparametric methods are not immediately applicable to longitudinal settings.

## 3 Setting

### Problem Statement

Recognizing the connection between mHealth policy learning and contextual bandit methods, we now consider a contextual multi-armed bandit environment with one control arm (the do-nothing action) denoted by \(a=0\) and \(q\) non-baseline arms corresponding to different actions or treatments. Users are indexed by \(i=1,2,\) and decision points are indexed by \(t=1,2,\). For each user \(i\) at time \(t\), the algorithm observes a context vector \(S_{i,t}\), chooses an action \(A_{i,t}[q]\{0,,q\}\), and receives a reward \(R_{i,t}\). The _differential reward_ describes the difference in expected rewards between choosing a non-baseline arm and the control arm, defined as \(_{i,t}(s,a)[R_{i,t}|S_{i,t}=s,A_{i,t}=a]- [R_{i,t}|S_{i,t}=s,A_{i,t}=0]\).

We denote the observation history up to time \(t\) for user \(i\) as \(_{i,t}(S_{i1},A_{i1},R_{i1},,S_{i,t})\). Actions are selected according to stochastic policies, \(_{i,t}:_{i,t}([q])\). We use \(_{i,t}(a|s)\) to denote the probability of action \(a[q]\) given context \(s\) for a fixed (implicit) history. As in Greenewald et al. (2017), we assume that the probability of the control action is bounded:

**Assumption 1**.: _There exists \(0<_{},_{}<1\) such that \(_{}<_{i,t}(0|a)<_{}\) for all \(i,t,a\)._

As noted in Greenewald et al. (2017), this assignment probability controls the number of messages that participants receive. It ensures that participants receive sufficient messages that they do not disengage, but not so many that they are overwhelmed or fatigued. We now define

\[A^{}_{i,t}=*{argmax}_{a[q]}[R_{i,t}|S_{i,t },A_{i,t}=a],^{}_{i,t}=*{argmax}_{a[q] \{0\}}[R_{i,t}|S_{i,t},A_{i,t}=a],\]

the optimal arm (including control) and the optimal non-baseline arm, respectively. Respecting the restrictions in Assumption 1, we can now express the optimal policy, \(^{}_{i,t}\), in two cases:

1. \(A^{}_{i,t}=0\): \(^{}_{i,t}(0|S_{i,t})=_{}\) and \(^{}_{i,t}(^{}_{i,t}|S_{i,t})=1-_{}\).
2. \(A^{}_{i,t}>0\): \(^{}_{i,t}(0|S_{i,t})=_{}\) and \(^{}_{i,t}(^{}_{i,t}|S_{i,t})=^{}_{i,t}(A^{}_ {i,t}|S_{i,t})=1-_{}\).

These cases represent the appropriate boundary condition given the optimal action. We maximize the probability of the control action if \(A^{}_{i,t}=0\) and minimize it if \(A^{}_{i,t} 0\), selecting the next-best action with the highest possible probability; we set the assignment probabilities for the remaining arms to zero. In Section 5.2, we define regret relative to this optimal policy, \(^{}_{i,t}\).

Lastly, we consider a study design that progresses in _stages_, where users enter the study sequentially (Friedman et al., 2010), as illustrated in Figure 1. Staged recruitment serves two purposes in our analysis: it increases the fidelity of the theoretical setting by mimicking real-world recruitment strategies, and it allows us to estimate changes in the differential reward over time. Concretely, we first observe user \(i=1\) at time \(t=1\). Then we observe users \(i=(1,2)\) at times \(t=(2,1)\). Subsequent stages \(k\) involve users \(i k\), each having had \(k-i+1\) observations, respectively. Define \(_{k}=\{(i,t):i k,\,t k+1-i\}\) as the set of all observations in stage \(k\). Appendix F provides a notation summary.

### Doubly Robust Differential Reward

Following Greenewald et al. (2017), we assume that the differential reward is linear: \(_{i,t}(s,a)=x(s,a)^{}_{i,t}\), where \(x(s,a)^{d}\) is a feature vector depending on the context and action. We allow the baseline reward, \(g_{t}(s)\), to be a nonlinear and nonstationary function of context and time. Combined, these two assumptions imply a partially linear model for the conditional mean reward:

\[[R_{i,t}|S_{i,t}=s,A_{i,t}=a]=g_{t}(s)+x(s,a)^{} _{i,t}_{a>0},\]where \(_{a>0}\) is an indicator of a non-baseline action. By incorporating both the linear predictor \(x(s,a)^{}_{i,t}_{a>0}\) and the nonlinear baseline \(g_{t}(s)\), we can maintain interpretability of the action effects while also allowing flexibility in specifying the observed rewards. The individual rewards are then realized according to the model

\[R_{i,t}=g_{t}(s)+x(s,a)^{}_{i,t}_{a>0}+_{i,t},\]

where we make the following standard assumption (Abeille & Lazaric, 2017) regarding \(_{i,t}\):

**Assumption 2**.: _The error \(_{i,t}\) is conditionally mean zero (i.e., \([_{i,t}|_{i,t}]=0\)) and sub-Gaussian._

Assumption 2 ensures that large errors are sufficiently rare and that \(_{t}\) is proportional to an unbiased estimate of \(_{i,t}(s,a)\). However, \(_{t}\) suffers from high variance, which leads to suboptimal performance in practice. Our proposed algorithm, RoME, relies on an improved estimator, \(_{i,t}^{f}(s,)\), that is also unbiased but achieves lower variance by denoising the rewards according to a working model, \(f_{i,t}(s,a)\), for the conditional mean \(r_{i,t}(s,a)[R_{i,t}|S_{i,t}=s,A_{i,t}=a]\):

\[_{i,t}^{f}(s,)(s,)-f_{i,t} (s,0)}_{}+-f_{i,t}(s,A_{i,t})}{ _{A_{i,t}=}-_{i,t}(0|s)}}_{}, \]

where \(\) is a preselected non-baseline arm; i.e., we condition on \(A_{i,t}\{0,\}\). The first component is a model-based prediction of the differential reward, and the second component is a debiasing term that uses IPW to correct for potential errors in the specification of \(f\). This improved estimator is a form of pseudo-outcome, a common technique in the causal inference literature (Bang & Robins, 2005; Nie & Wager, 2021; Kennedy, 2023). We refer to it as a _Doubly Robust Differential Reward_ because it produces an unbiased estimate of \(_{i,t}(s,a)\) as long as either \(_{i,t}\) or \(f_{i,t}\) is correctly specified (see proof in Appendix E.1); though, in practice, the benefit stems from (a) the robustness to misspecification of \(f\) (because \(_{i,t}\) is known) and (b) the variance reduction introduced by denoising the outcomes (see Remark 1 in Appendix E.2). In the following, we abbreviate \(_{i,t}^{f}(s,a)\) as \(_{i,t}^{f}\), with the context and action implied.

## 4 Algorithm Components

We now construct \(_{i,t}^{f}\), introduce the weighted least-squares loss function used by RoME to estimate parameters, and use nearest-neighbor regularization to pool information across users.

Figure 1: Illustration of the staged recruitment scheme. At each recruitment stage (each time point), a new participant is recruited and observed. At the same time, all participants who were recruited prior to the current stage are also observed again. Observations are not collected from participants who have yet to be recruited. For simplicity, we assume one participant is recruited at each stage.

### Debiased Machine Learning

To construct \(^{f}_{i,t}\), we fit a working model \(f_{i,t}\) for the conditional mean reward. Because \(^{f}_{i,t}\) is robust to misspecification of \(f_{i,t}\), our regret bound in Theorem 1 does not require \(f_{i,t}\) to be correctly specified. However, we show in Appendix E.2 that the following assumption results in a lower asymptotic variance bound for \(^{f}_{i,t}\).

**Assumption 3**.: _The working model, \(f_{i,t}\), satisfies \(|r_{i,t}(s,a)-f_{i,t}(s,a)|}{{}}0\) for all \(s\), \(a\)._

Assumption 3 states that \(f_{i,t}\) is a strongly consistent estimator of \(r_{i,t}\), but it does not impose any specific rate requirement on the convergence. Although our regret bound does not require Assumption 3, it does require two standard conditions within the DML framework: (a) Neyman orthogonality and (b) independence of \(f_{i,t}\) and the observed rewards. The construction of \(^{f}_{i,t}\) ensures that (a) holds, making \(^{f}_{i,t}\) robust to misspecification of \(f\). We can enforce (b) via sample-splitting techniques. We present two such options below.

**Option 1:** The first option exploits the fact that outcomes across different users are independent. Consequently, we can perform sample splitting as follows. **Step 1**: Randomly assign each user \(i\) to one of \(J\) folds. Let \(W_{j}\) denote the set containing user indexes for the \(j\)-th fold and let \(W_{j}^{}\) denote its complement. **Step 2**: For each fold, use a supervised learning algorithm to estimate the working model for \(r_{i,t}(s,a)\) denoted \(^{(j)}_{i,t}(s,a)\) using \(W_{j}^{}\). **Step 3**: Construct the pseudo-outcomes using (1).

**Option 2:** Because we do not expect an adversarial environment in mHealth, we may be willing to assume that the errors, \(_{i,t}\), are independent across time. In this case, we can adapt the procedure given above by randomly assigning data to folds at the level of \((i,t)\). In this case, \(W_{j}\), contains the \((i,t)\) pairs assigned to fold \(j\). This option is more powerful than option 1 in that it enables us to learn heterogeneity across users in the nuisance function, \(f_{i,t}\), leading to greater variance reduction.

Lastly, we note two strategies that can reduce the computational demands of sample splitting. The first is to employ estimators that satisfy a leave-one-out stability condition, such as bagged estimators that use subsampling (Chen et al., 2022); these estimators eliminate the need to perform sample splitting. The second is to fit \(f_{i,t}\) in an online fashion.

### Estimation via Weighted Least Squares

Having formed \(^{f}_{i,t}(s,)\), we now explain how to estimate the parameters, \(_{i,t}^{d}\), determining the differential reward, \(_{i,t}(s,a)\). We assume that \(_{i,t}\) consists of three components:

**Assumption 4**.: \(_{i,t}=^{}+^{}_{i}+^{ }_{t}\) _for all \(i,t\)._

The parameter \(^{}\) is a fixed effect shared across all \(i,t\). In contrast, \(^{}_{i}\) and \(^{}_{t}\) are random effects specific to a given user and time point, respectively. We place these parameters in a single column vector, \(\), as \((^{},^{}_{1},,^ {}_{K},^{}_{1},,^{} _{K})\), where \(K\) is the total number of stages. We then form a corresponding feature vector, \(_{i,t}=(x_{i,t})^{(2K+1)d}\), where \(\) inserts \(x_{i,t}\) into the positions corresponding to \(^{}\), \(^{}_{i}\), and \(^{}_{t}\) with zeros in all other locations; i.e., \((x_{i,t})=C_{i,t}^{}x_{i,t}\), where

\[C_{i,t}:=[1,\,0_{i-1}^{},\,1,\,0_{K-i+t-1}^{},\,1,\,0_{K-t}^{}]  I_{d}\]

so that \(C_{i,t}^{d(2K+1)d}\). We could then estimate \(_{i,t}\) via a weighted least squares (WLS) regression that minimizes the following objective function:

\[_{}()=_{i=1}^{n}_{t=1}^{K-i+1}_{i,t }^{2}^{f}_{i,t}-^{}_{i,t}^{2},\]

where \(_{i,t}^{2}=_{i,t}(0|S_{i,t})\{1-_{i,t}(0|S_{i,t})\}\). However, \(_{}()\) is over-parameterized and does not take advantage of available network information. The next section modifies \(_{}()\) by adding regularization, producing the final loss function used in RoME.

### Nearest-Neighbor Regularization

We assume access to network information relating neighboring users and time points. These networks are defined by graphs \(G_{}=(V_{},E_{})\) and \(G_{}=(V_{},E_{})\), where \(V\) denotes vertices and \(E\), edges. In practice, these networks can be formed via known clusters (e.g., shared schools, companies, geographic locations) or similar covariates. In the case of \(G_{}\), a reasonable choice is to construct a graph of neighboring sequential time points; i.e., \(t=1 t=2 t=3\) Because we expect neighboring users and time points to have similar parameters, we may be able to improve our estimates of \(\) by regularizing neighboring values of \(_{i}^{}\) and \(_{t}^{}\) toward each other. This can be accomplished by using an \(L^{2}\) Laplacian penalty. We illustrate the idea using \(G_{}\); the application to \(G_{}\) is similar.

First, we form the incidence matrix, \(Q\), with the entry \(Q_{v,e}\) corresponding to the \(v\)-th vertex (user) and the \(e\)-th edge. Denote the vertices of this edge as \(v_{i}\) and \(v_{j}\) with \(i>j\). \(Q_{v,e}\) is then equal to 1 if \(v=v_{i}\), -1 if \(v=v_{j}\), and 0 otherwise. The Laplacian matrix is then defined as \(L=QQ^{}^{K K}\). Similar to Yang et al. (2020), we form a _network cohesion_ penalty across users as follows:

\[(_{}^{}L_{}_{})= _{(i,j) E_{}}\|_{i}^{}-_{j}^{ }\|_{2}^{2},\]

where \(_{}(_{1}^{},,_{K}^{ })^{}^{K d}\) and \(L_{}\) is the Laplacian matrix for users. The penalty is small when \(_{i}^{}\) and \(_{j}^{}\) are close for connected users. We employ a shared regularization hyperparameter \(\), for \(G_{}\) and \(G_{}\), and include a standard \(L^{2}\) penalty on \(\) with hyperparameter \(\). The full penalization matrix \(V_{0}^{(2K+1)d(2K+1)d}\) is

\[V_{0}=( I_{d}, I_{Kd}+ L_{}^{ {user}}, I_{Kd}+ L_{}^{}), \]

where \(L_{}^{} L_{} I_{d}\), the Kronecker product of \(L_{}\) with \(I_{d}\) (\(L_{}^{}\) is defined similarly). We then adapt \(_{}()\) to include a corresponding penalty term:

\[()=_{i=1}^{n}_{t=1}^{K-i+1}_{i,t}^{2} _{i,t}^{f}-_{i,t}^{}^{2}+^{}V_{0}. \]

In Section 5, we show that (3) leads to a Thompson sampling algorithm for action selection.

## 5 Thompson Sampling Algorithm

We now describe the Thompson sampling procedure and provide a high-probability regret bound.

### Algorithm Description

The minimizer of the weighted regularized least squares loss in (3) is \(=V^{-1}b\), where

\[V=V_{0}+_{i=1}^{n}_{t=1}^{K-i+1}_{i,t}^{2}_{i,t} _{i,t}^{}, b=_{i=1}^{n}_{t=1}^{K-i+1}_{i,t}^{f} _{i,t}^{2}_{i,t}.\]

In analogy to Bayesian linear regression, the penalized and weighted Gram matrix, \(V\), plays the role of a (scaled) posterior precision matrix under a multivariate Gaussian prior for \(\). This motivates an algorithm in which we randomly perturb the point estimate, \(\). Specifically, we define \(_{i,t}^{-1} C_{i,t}V^{-1}C_{i,t}^{}\) and set \(_{i,t}=C_{i,t}+_{i,t}()_{i, t}^{-1/2}\), where \(\) is a mean-zero random variable and

\[_{i,t}() v\} +\{_{i,t}^{-1})}{(_{i,t}^{0})} \}}+\{^{3/4}(K),1\}. \]

In the above expression, \(_{i,t}^{0} C_{i,t}V^{-1}V_{0}V^{-1}C_{i,t}^{}\) and \(K\) is the total number of stages. The value \(v\) is the sub-Gaussian factor for \(_{i,t}^{f}\), and \(\) is defined in Remark 2 in Appendix E; both can be set to sufficiently large constants. The hyperparameter \(\) determines the probability with which the regret bound holds. The random deviate, \(\), is drawn from a distribution \(^{TS}\) satisfying thetechnical conditions in Definition 1 in Appendix D. In practice, we recommend setting \(^{TS}\) to a multivariate Gaussian distribution or t-distribution to simplify calculations. Given \(_{i,t}\), we then compute \(_{i,t}=*{argmax}_{a[q]\{0\}}\)\(x_{i,t}^{}_{i,t}\) and randomly select \(A_{i,t}=0\) with probability

\[_{i,t}(0|_{i,t})[_{},\{_{}, (x_{i,t}^{}_{i,t}<0)\}], \]

or \(A_{i,t}=_{i,t}\) with probability \(1-_{i,t}(0|_{i,t})\). The action selection is summarized in Algorithm 1.

```
\(V\), \(b\), \(C_{i,t}\), \(_{i,t}()\), \(x_{i,t}\), \(_{}\), \(_{}\)  Set \(=V^{-1}b\) and \(_{i,t}^{-1}=C_{i,t}V^{-1}C_{i,t}^{}\)  Sample \(^{TS}\)  Set \(_{i,t}=C_{i,t}+_{i,t}()_{i,t} ^{-1/2}\)  Set \(_{i,t}=*{argmax}_{a[q]\{0\}}\)\(x_{i,t}^{}_{i,t}\)  Calculate \(_{i,t}(0|_{i,t})\) according to (5)  With probability \(_{i,t}(0|_{i,t})\), play action \(0\); otherwise, play action \(_{i,t}\)
```

**Algorithm 1** Action selection

Having detailed the action selection algorithm, we now summarize RoME in Algorithm 2. The algorithm includes an outer loop that iterates through the stages from \(k=1\) to \(k=K\). Within each stage, we handle users sequentially, iteratively selecting actions according to Algorithm 1, observing rewards, and updating \(V\) and \(b\) accordingly.

```
Initialize \(V_{0}\) according to (2) \(V V_{0}\) \(b 0\) for\(k=1,,K\)do for\((i,t)=(1,k),(2,k-1),,(k,1)\)do  Choose an arm according to Algorithm 1 using \(V\) and \(b\)  Observe reward, \(R_{i,t}\)  Construct pseudo-reward, \(_{i,t}^{f}\), as explained in Section 4.1  Set \(_{i,t}=C_{i,t}^{}x_{i,t}\)  Update weighted Gram matrix: \(V V+_{i,t}^{2}_{i,t}_{i,t}^{}\)  Update feature-outcome products: \(b b+_{i,t}^{2}_{i,t}^{f}_{i,t}\) endfor endfor
```

**Algorithm 2** RoME

### Regret bound

We first define regret, \(_{K}\), in terms of the following average across stages:

\[_{k=1}^{K}_{(i,t)_{k}_{ k-1}}\{_{i,t}^{}(_{i,t}^{}|S_{i,t}) x(S_{i,t}, _{i,t}^{})^{}_{i,t}^{}-_{i,t}(_{i,t}|S_{i,t}) x(S_{i,t},_{i,t})^{}_{i,t}^{}\}.\]

The inner sum calculates regret (the difference between the expected reward of the optimal action and the chosen action) weighted by the probability of the action, following Greenewald et al. (2017), for all users at stage \(k\). We then average these values to account for the growing number of pairs \((i,t)\) as the stages progress. The cumulative regret is the sum of these averaged values over all stages.

The rewards associated with the baseline action are excluded since \(_{i,t}(S_{i,t},0)=0\). \(_{K}\) is a form of pseudo-regret because it eliminates the randomness due to \(\{_{i,t}\}_{t=1}^{K}\)(Audibert et al., 2003). The regret bound requires additional technical assumptions:

**Assumption 5**.: _For all \(s\) and \(a\), \(\|x(s,a)\| 1\)._

**Assumption 6**.: _There exists a known value \(M^{+}\) such that \(|g_{t}(s)|,|f_{i,t}| M\) for all \(i\), \(t\), and \(s\)

**Assumption 7**.: _Let \(e_{K}\#E_{K}^{}+\#E_{K}^{}\) denote the total number of edges. Then \(e_{K}=O(K^{})\) for some \([0,)\)._

**Assumption 8**.: _Let \(_{k}^{}\) represent the vector of true parameter values at stage \(k=i+t-1\) and \(V_{i,t}^{-1}\), the matrix \(V\) prior to decision point \((i,t)\). Then \(_{(i,t)_{K}} V_{0}_{k}^{}_ {V_{i,t}^{-1}}=O_{p}\{^{3/4}(K)\}.\)_

**Assumption 9**.: _The matrix \(_{i,t}\) is bounded below as follows: \(_{i,t}(i,t)I_{d}\) for some \(>0\) that may depend on \(d\)._

Assumption 5 is a standard assumption that simplifies the proof (Abeille and Lazaric, 2017). Assumption 6 ensures that the (estimated) baseline rewards are bounded. Assumption 7 ensures that the number of edges grows no faster than a polynomial rate. Assumption 8 limits the contribution of the random effects to the regret bound. Lastly, Assumption 9 ensures sufficient exploration of the context space. See the proof of Theorem 1 in Appendix E.3 for additional discussion of Assumptions 8 and 9. We now state the regret bound.

**Theorem 1**.: _Under Assumptions 1-2, 4-9, with probability at least \(1-\), \(_{K}\) is of order_

\[O[dd)}\{+^{1/4}(K) \}].\]

The exact bound (including constants) and proof are given in Appendix E. Following Greenewald et al. (2017), we decompose the regret into two terms. The first term depends on a summation of terms of the form \(\{_{i,t}^{}(_{i,t}^{}|S_{i,t})-_{i,t}(_{i,t}| S_{i,t})\}x(S_{i,t},_{i,t})^{}_{i,t}^{}\). We employ a novel technique based on Markov's inequality to bound this term (see Lemmas 12 and 13 in Appendix E).

We bound the second term using the high-level proof technique of Abeille and Lazaric (2017). The primary difficulty in bounding this term is that the classical theory for the estimation error of RLS estimates (Theorem 2 in Abbasi-Yadkori et al. (2011)) is not directly applicable to our setting due to the increasing dimensionality. Lemma 6 in Appendix E generalizes this theory to our setting. This Lemma produces a bound involving \( C_{i,t}V_{i,t}^{-1}V_{0}_{k}^{}_{ {V}_{i,t}}\), which we subsequently bound by \( V_{0}_{k}^{}_{V_{i,t}^{-1}}\) in Lemma 7. This section of the proof requires Assumption 8 to ensure that the contribution of the new parameters to this bound is small enough that we incur no more than a logarithmic penalty.

The overall bound scales like \((d^{3/2})\) up to log factors, matching the standard rate given in Agrawal and Goyal (2012) and Abeille and Lazaric (2017) for linear Thompson sampling with fixed dimension. In particular, the bound depends solely on the dimensionality of the differential reward--not the baseline reward, which may be much more complex.

## 6 Experiments

This section presents results from applying RoME in a simulated mHealth study with staggered recruitment and an off-policy comparison study for cardiac rehabilitation mHealth interventions. The simulations were implemented using Python and the results were generated using individual compute nodes with two 3.0 GHz Intel Xeon Gold 6154 processors and 180 GB of RAM. Case studies were implemented using R 4.2.2 and results were generated on a cluster composed of individual compute nodes with 2.10 GHz Intel Xeon Gold 6230 processors and 192 GB of RAM.

### Competitor Comparison Simulation

In this section, we compare RoME to four competing methods in simulation. We implemented RoME using **Option 2** with a bagged ensemble of stochastic gradient trees (Gouk et al., 2019; Mastelini et al., 2021) trained online via the River library (Montiel et al., 2021). We programmed the linear algebra operations using SuiteSparse (Davis and Hu, 2011) to take advantage of the sparse nature of the Laplacian matrices and \(_{i,t}\) from Algorithm 2.

Our competing baselines are (a) Standard: Standard Thompson sampling for linear contextual bandits, (b) AC: the **A**ction-**C**entered contextual bandit algorithm (Greenewald et al., 2017), (c) IntelPooling: The intelligentpooling method of Tomkins et al. (2021) fixing the variance parameters close to their true values, and (d) Neural-Linear: a method that uses a pre-trained neural network to transform the feature space for the baseline reward (similar to the Neural Linear method of Riquelme et al. (2018)).

We compare these methods under three settings: Homogeneous Users, Heterogeneous Users, and Nonlinear. The first two involve a linear baseline model and time-homogeneous parameters, with the second setting having distinct user parameters. The third setting, designed to mirror the challenges of mHealth studies, includes a nonlinear baseline and both user- and time-specific parameters. For each setting, we simulate 200 stages following the staged recruitment regime depicted in Figure 1, and we repeat the full 200-stage simulation 50 times. Appendix A.1 provides details on the setup and a link to our implementation.

Figure 2 displays the average cumulative regret for each method. RoME and IntelPooling have similar performance in the Homogeneous Users setting, but RoME excels in the Heterogeneous Users and Nonlinear settings, with significantly lower cumulative regret by stage 200. This is expected as RoME can utilize network information and model nonlinear baselines in these settings. However, in the Homogeneous Users setting, where no network information is available and a linear baseline is used, RoME does not outperform IntelPooling.

Appendices A.2-A.5 offer more results, including additional method comparisons, a rectangular data array simulation, hyperparameter sensitivity analyses, and pairwise statistical comparisons. The latter show that RoME outperformed each competitor in at least 48 of 50 repetitions in the Nonlinear setting, indicating even higher statistical confidence than Figure 2 suggests. RoME substantially outperforms the competing algorithms in the additional comparisons and is several orders of magnitude faster than would be required in a standard mHealth study, producing over 20,000 decisions in as little as 95 seconds (see Table 2 in Appendix A.2).

### Valentine Study Analysis Results

In this section, we compare RoME to the above algorithms via off-policy evaluation using data from the Valentine Study (Jeganathan et al., 2022), a prospective, randomized-controlled, remotely administered trial designed to evaluate an mHealth intervention to supplement cardiac rehabilitation for low- and moderate-risk patients. In the analyzed dataset, participants were randomized to receive or not receive contextually tailored notifications promoting low-level physical activity and exercise throughout the day. The left panel of Figure 3 shows the estimated improvement in the average reward over the original constant randomization, averaged over stages (K = 120) and participants (N=108). We see that RoME achieved the highest average reward; its average performance (across bootstrap replications) is higher than the 75th percentile of all other methods.

To further analyze the improvement of RoME relative to the other methods, we test whether the proposed algorithm significantly improves cumulative rewards using paired t-tests with one-sided alternative hypotheses. The null hypothesis (\(H_{0}\)) for these tests is that the two algorithms being compared achieve the same average reward. The alternative hypothesis (\(H_{1}\)) is that the algorithm listed in the column achieves higher average rewards than the algorithm listed in the row. The right of Figure 3 displays the p-values obtained from these pairwise t-tests. The dark shade of the last column indicates that the proposed RoME algorithm achieves significantly higher rewards than the other five competing algorithms (\(p 0.01\) for all pairwise comparisons). The results imply that RoME would have achieved a \(3.5\%\) increase in step count relative to the constant randomization policy that was

Figure 2: Cumulative regret in the (a) Homogeneous Users, (b) Heterogeneous Users, and (c) Nonlinear settings. RoME performs competitively in the first setting (the simplest), and it substantially outperforms the next-best method (IntelPooling) in the others.

actually implemented in this study. This effect translates to an increase of 140 steps per day, given a baseline of 1,000 steps per hour and four one-hour measurement windows. See Appendix B for additional implementation details.

We performed an additional off-policy comparison using data from the Intern Health Study (IHS) (NeCamp et al., 2020), as shown in Figure 11 in Appendix C.2. The results further demonstrate the competitive performance of the RoME algorithm, with the AC algorithm also showing comparable performance in this dataset. Further details on the analysis can be found in Appendix C.

## 7 Discussion

This paper introduces RoME, a robust mixed-effects contextual bandit algorithm for mHealth studies. RoME adapts DML and network cohesion penalties to dynamic settings, enabling researchers to efficiently pool information across users and over time. In addition to the methodological contribution, we also prove a high-probability regret bound in an asymptotic regime that involves a growing pool of users and time points. Our implementation of RoME is several orders of magnitude faster than would be required in practice and could easily be adapted for use in future mHealth studies.

We see several promising directions for improving RoME in future work. One such direction might consider improvements in the algorithm and theoretical analysis that enable us to replace Assumptions 8 and 9 with weaker and more primitive assumptions. In particular, these assumptions may not be plausible for \(q>1\) because most arm selections will be restricted to the same two arms (for large \(K\)). Other interesting directions include data-adaptive strategies for hyperparameter selection and addressing long-term impacts of mHealth treatments, such as accumulating treatment fatigue. Future work could also consider computational improvements that would enable large-scale deployment of RoME, which would be especially relevant in nonclinical settings.