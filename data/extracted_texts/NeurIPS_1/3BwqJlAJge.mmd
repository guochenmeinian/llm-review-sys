# Exploring and Addressing Reward Confusion in Offline Preference Learning

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Spurious correlations in a reward model's training data can prevent Reinforcement Learning from Human Feedback (RLHF) from identifying the desired goal and induce unwanted behaviors. In this work, we study the reward confusion problem in offline RLHF where spurious correlations exist in data. We create a lightweight benchmark to study this problem and propose a method that can reduce reward confusion by leveraging model uncertainty and the transitivity of preferences with active learning.

## 1 Introduction

For many real-world tasks, designing adequate reward functions is challenging, which has led to the rise of Reinforcement Learning from Human Feedback (RLHF) . In this work, we study a failure mode of offline RLHF that we refer to as _reward confusion_. This occurs when the reward \(R\) in a Markov Decision Process (MDP) is a function of features \(z_{1},,z_{n}\) inferred from the observation-action pair \((o,a)\). In a simplified scenario, \(R\) relies on \(z_{1}\) but not \(z_{2}\), yet \(z_{1}\) and \(z_{2}\) are highly correlated in the training data. An empirical risk minimizer might mistakenly conclude that \(z_{2}\) affects \(R\). As we'll see, this incorrect dependence can lead to failures when training a policy against the learned reward function. We graphically illustrate this problem in Figure 1.

To better understand this phenomenon, we created a benchmark environment called _Confusing Minigrid (CMG)_ that tests reward confusion in models. We carefully designed six tasks with three types of spurious information for the minigrid environment, which we introduce in detail in Appendix A. We will open source the benchmark's code soon.

Besides the CMG benchmark, one other our major contributions is an algorithm named Information-Guided Preference Chain (IMPEC) designed to address the reward confusion problem. It involves two stages of training: First, we use information gain as the acquisition function to select comparison rollouts that reduce uncertainty about the reward function. Second, we form a complete preference ordering over the set of selected rollouts, rather than just a partial ordering as in traditional RLHF.

Our experiments show that these techniques together improve sample efficiency while reducing reward confusion. We show in Section 4 that using the same comparison budget, IMPEC can

Figure 1: Illustration of a simplified MDP (left) and reward confusion (right). The reward \(R\) is a function of the feature \(z_{1}\), but not \(z_{2}\). Spurious correlation between \(z_{1}\) and \(z_{2}\) can cause a network to wrongly model \(R\) as a function of \(z_{2}\).

outperform many other active preference learning baselines. To the best of our knowledge, it is the first algorithm that attempts to solve the reward confusion problem in preference learning.

## 2 Related Work

Causal ConfusionThe problem of _causal confusion_, which refers to models learning to depend on spurious correlations in the training data, has been studied in behavioral cloning , reinforcement learning , and reward learning . Past work shows empirically and theoretically that spurious correlations and confounders in the training set can worsen an agent's deployment performance [22; 9]. Reward confusion is essentially causal confusion that occurs during reward learning.

Goal MisgeneralizationWhile past work on causal confusion studies it as a cause of complete failure to learn goal-directed behavior, it can also make agents optimize for incorrect goals, i.e. goal misgeneralization. For example, in Procgen's CoinRun, the coin to be picked up is always on the right. RL agents can confuse "running to the right" with the real goal of "getting the coin" . Generally, a model's behavior can be consistent with a goal, but it may not be the test-time goal .

Preference LearningLearning reward models from preference labels  have gained traction due to their low cost compared to expert demonstrations  or language inputs . We have also seen progress in other tasks of "reward engineering", e.g. reward hacking .

## 3 Method

ModelsWe consider an agent in an environment following the Markov Decision Process (MDP) defined by \((,,,)\). \(\) is the state space, \(\) is the action space, \(p:[0,)\) is the transition probability density. A rollout \(=(s_{t},a_{t})\) is a sequence of states and actions. Given unranked rollouts \(\), our algorithm actively collects ranking information to sort them into an ordered list \(T=_{1},_{2},...,_{n}\). The rank of rollout \( T\) is denoted by \(_{}^{T}\). On each transition, the environment emits a reward \(:\). Our goal is to obtain \(^{*}\) that induces correct policies.

PreferencesWe model the human's probability of preferring \(_{1}\) in a pair \((_{1},_{2})\) through the Shepard-Luce choice rule [17; 13]: \(P[_{1}_{2}]=r(o_{t}^{*},a_{t}^{*})}{_{t}r (o_{t}^{*},a_{t}^{*})+_{t}r(o_{t}^{*},a_{t}^{*})}\). We extend this model to a ternary one by allowing the human to flag when two rollouts are equally good, \(_{1}_{2}\). We use cross-entropy loss to improve reward model's predictions for human's true preference.

### Information-Guided Preference Chain (IMPEC)

Key intuition: Increase Contrast Among Valuable Rollouts.In most preference comparison algorithms, a rollout \(_{1}\)'s relation is considered explicitly only with another one \(_{2}\). Suppose that in the ground truth, \(_{1}_{2}_{3}_{4}\), and we already know \(_{1}_{2}\), \(_{3}_{4}\). To figure out \(_{1}\) and \(_{2}\)'s relationship with \(_{3}\) and \(_{4}\), the most efficient query is whether \(_{2}_{3}\). Once we establish that, we can immediately obtain the preference relations on all four rollouts.

Creating and Maintaining a Preference ChainWe maintain an ordered chain for rollouts. Starting from an empty chain, for each new rollout we queried from the dataset, we imitate insertion sort by recursively finding the ranking of it using human's preference labels. Hence, by the time we observe

Figure 2: The IMPEC algorithm creates a sorted preference chain of \(n\) buckets, each containing one or more rollouts with equal returns.

all the rollouts, we have a sorted list of rollouts, ordered according to human preferences. Rollouts can have identical returns, so we treat each element of the chain as a _bucket_\(b\) of rollouts with the same return. If the human decides that a new rollout \(_{}\) is equally preferred to \(_{m}\) in bucket \(b_{m}\), then \(_{}\) will be added to \(b_{m}\). On the other hand, if \(_{}_{m}\) and \(_{}_{m-1}\) (\(_{m-1}\) resides in a previous bucket \(b_{m-1}\)), then the algorithm will insert a new bucket containing only \(_{}\) in between \(b_{m}\) and \(b_{m-1}\). This ensures that \(b_{0}\) contains the best rollouts seen so far and \(b_{n}\) contains the least preferred rollouts (where \(n\) is the chain length). We illustrate this process in Figure 2.

Our reward model is a Bayesian neural network (BNN)  which maintains a Gaussian distribution over a network's weights and biases. As we will see, this allows us to incorporate epistemic uncertainty over reward functions into the active selection procedure. In the noiseless case, insertion sort needs \(O( n)\) queries to find the position for \(_{}\). However, we have access to a partially trained reward network, which we use to guess the rank for \(_{}\), reducing the number of buckets we must search over. We include more design details in Appendix B for the design of the fast query.

Information GainGiven an existing chain of rollouts, we use information gain as the acquisition function to decide which rollouts to compare next, so we reduce the most uncertainty over network weights. The information gain over network weights \(\) by selecting a rollout \(\) for ranking is

\[I(;_{}^{T} T,)=H( T,)-H( _{}^{T},T,) \]

where \(_{}^{T}\) is the rollout's ranking on chain \(T\). Intuitively, it measures how much we expect to reduce uncertainty about the weights after observing the ranking \(_{}^{T}\) of rollout \(\). As shown in Appendix C, Equation 1 (information gain) can be approximated by drawing \(M\) weight samples, \(_{1},_{2},,_{M}\), from the posterior through

\[_{i=1}^{M}_{}P(_{}^{T} T,_{i}, )(^{T} T,_{i}, )}{_{_{j}}P(_{}^{T} T,_{j},)}) \]

\(P(_{}^{T} T,,)\) is a complicated distribution, and so we (loosely) approximate it with Equation 3. Intuitively, it is proportional to the probability that \(_{i}_{i+1}\).

\[P(_{}^{T}=i T,,) P(_{i} ) P(_{i+1}) \]

We summarize the complete process in Algorithm 1. The network is first supervised trained on the preference dataset \(D\) using cross entropy loss with \(P[_{1}_{2}]\) modeled through the Shepard-Luce choice rule. With the limited query budget for human preferences, we first find out the rollout \(\) whose ranking \(_{}^{T}\) on chain \(T\) will provide the most information gain over the model weights \(\). Then we use insertion sort to find out \(\)'s real ranking \(_{}^{T*}\) in the chain. We add the rollout \(\) onto the appropriate position of the chain \(T\), then based on its position, derive preference labels with all other rollouts on the chain. We repeat this process until the network weight has converged.

```
0: Preference dataset D, network \(\), query budget Q \(T[]\) while not converged do \(\) SupervisedTrain\((,D)\) ifbudget not reached then \(_{}I(;_{}^{T} T,)\) \(_{}^{T*}\) InsertionSort\((,T,)\) \(T T\) \(D D\) DerivePreferences\((,T,_{}^{T})\) endif \(i i+1\) endwhile
```

**Algorithm 1** The IMPEC Algorithm

## 4 Experiments

Experiment SettingsWe compare our method with the standard RLHF algorithm, and two other RLHF with active learning methods: pairwise information gain  and pairwise volume removal . The information gain (IG) method is similar to ours but reasons only about individual preference pairs and not about the result of the ranking process. The volume removal method was designed in the linear reward setting to reduce the volume of weight vectors supported under the posterior after each preference update. We conduct experiments on 6 CMG tasks. Detailed information on each task and their added spurious correlations can be found in Appendix A.

We first perform offline reward learning, and then apply online reinforcement learning using the learned reward function to obtain a policy. The RL agent receives rewards from the learned function instead of the environment, and is trained with Proximal Policy Optimization . More detailed experiment and hyperparameter settings can be found in Appendices D and E.

**Main Results** Performance on all six tasks can be found in Table 1, with each run repeated over five seeds. We further compute the p-values of the results being better than baseline performance, with complete results in Appendix H. Except for the task Go To Door where all algorithms perform poorly, IMPEC has a higher mean return than other algorithms, and often a lower standard deviation. Although IMPEC achieves a higher mean than the other methods on most tasks, its p-values are \(\) 0.1 (per appendix H). This provides some (albeit not particularly strong) statistical evidence that IMPEC has a better mean performance distribution from the baseline. In contrast, the volume removal and information gain methods are often statistically indistinguishable from the baseline.

The decisive factor for each algorithm's performance is how often they fail: Out of the 5 seeds, how often does the reward function learn to optimize for the spurious goal? All methods but IMPEC have fairly high probability of taking the spurious feature as the "correct" feature, and hence rewarding incorrect behaviors and obtaining low ground truth returns. We plot the learning curves for each algorithm in the Lava task, where the baseline curve has the largest standard deviations. We observe similar phenomena in other tasks. We include an ablation study in Appendix F. The complete learning curves are included in Appendix J.

**Limitations and Future Work** A limitation of IMPEC is its potential sensitivity to noise in preferences. In our experiments, we keep a relatively low noise level, and we believe more sophisticated algorithms could improve robustness to noise, perhaps inspired by past work on noisy binary search . Our results suggest a deeper connection between the quality of preference datasets and the efficiency of preference learning algorithms. In appendix G, we show some first steps of a graph theoretic analysis for reasoning about preference dataset quality. We are interested in further exploring the influence of graph-theoretic qualities and their effects on preference learning, and using the insights in future algorithm design.

## 5 Conclusion

This work studies the reward confusion problem. Our experiments on the Confusing Minigrid benchmark show that reward confusion in offline preference learning can lead to undesired policy behaviors. The benchmark is easy to configure, and we expect it to be particularly useful for iterative research. In addition, we proposed IMPEC to reduce the impact of reward confusion. It exploits preference transitivity and obtains decent empirical performance on tasks with different sources of reward confusion. We believe that the findings of our work will be helpful for making AI more aligned with human values.