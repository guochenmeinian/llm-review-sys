# Penalising the biases in norm regularisation enforces sparsity

Etienne Boursier

INRIA CELESTE, LMO, Orsay, France

etienne.boursier@inria.fr &Nicolas Flammarion

TML Lab, EPFL, Switzerland

nicolas.flammarion@epfl.ch

###### Abstract

Controlling the parameters' norm often yields good generalisation when training neural networks. Beyond simple intuitions, the relation between regularising parameters' norm and obtained estimators remains theoretically misunderstood. For one hidden ReLU layer networks with unidimensional data, this work shows the parameters' norm required to represent a function is given by the total variation of its second derivative, weighted by a \(}\) factor. Notably, this weighting factor disappears when the norm of bias terms is not regularised. The presence of this additional weighting factor is of utmost significance as it is shown to enforce the uniqueness and sparsity (in the number of kinks) of the minimal norm interpolator. Conversely, omitting the bias' norm allows for non-sparse solutions. Penalising the bias terms in the regularisation, either explicitly or implicitly, thus leads to sparse estimators.

## 1 Introduction

Although modern neural networks are not particularly limited in terms of their number of parameters, they still demonstrate remarkable generalisation capabilities when applied to real-world data (Belkin et al., 2019; Zhang et al., 2021). Intriguingly, both theoretical and empirical studies have indicated that the crucial factor determining the network's generalisation properties is not the sheer number of parameters, but rather the norm of these parameters (Bartlett, 1996; Neyshabur et al., 2014). This norm is typically controlled through a combination of explicit regularisation techniques, such as weight decay (Krogh and Hertz, 1991), and some form of implicit regularisation resulting from the training algorithm employed (Soudry et al., 2018; Lyu and Li, 2019; Ji and Telgarsky, 2019; Chizat and Bach, 2020).

Neural networks with a large number of parameters can approximate any continuous function on a compact set (Barron, 1993). Thus, without norm control, the space of estimated functions encompasses all continuous functions. In the parameter space, this implies considering neural networks with infinite width and unbounded weights (Neyshabur et al., 2014). Yet, when weight control is enforced, the exact correspondence between the parameter space (i.e., the parameters \(\) of the network) and the function space (i.e., the estimated function \(f_{}\) produced by the network's output) becomes unclear. Establishing this correspondence is pivotal for comprehending the generalisation properties of overparameterised neural networks. Two fundamental questions arise.

**Question 1**.: _What quantity in the function space, does the parameters' norm of a neural network correspond to?_

**Question 2**.: _What functions are learnt when fitting training data with minimal parameters' norm?_

We study these questions in the context of a one-hidden ReLU layer network with a skip connection. Previous research (Kurkova and Sanguineti, 2001; Bach, 2017) has examined generalisation guarantees for small representational cost functions, where the representational cost refers to thenorm required to parameterise the function. However, it remains challenging to interpret this representational cost using classical analysis tools and identify the corresponding function space. To address this issue, Question 1 seeks to determine whether this representational cost can be translated into a more interpretable functional (pseudo) norm. Note that Question 1 studies the parameters' norm required to fit a function on an entire domain. In contrast, when training a neural network for a regression task, we only fit a finite number of points given by the training data. Question 2 arises to investigate the properties of the learned functions when minimising some empirical loss with a regularisation of the parameters' norm regardless of whether it is done explicitly or implicitly.

In relation to our work, Savarese et al. (2019), Ongie et al. (2019) address Question 1 for one-hidden layer ReLU neural networks, focusing on univariate and multivariate functions, respectively. For a comprehensive review of this line of work, we recommend consulting the survey of Parhi and Nowak (2023). On the other hand, Parhi and Nowak (2021), Debarre et al. (2022), Stewart et al. (2022) investigate Question 2 specifically in the univariate case. Additionally, Sanford et al. (2022) examine a particular multidimensional case. However, all of these existing studies overlook the bias parameters of the neural network when considering the \(_{2}\) regularisation term. By omitting the biases, the analysis and solutions to these questions become simpler.

In sharp contrast, our work addresses both Questions 1 and 2 for univariate functions _while also incorporating regularisation of the bias parameters_. It may appear as a minor detail--it is commonly believed that similar estimators are obtained whether or not the biases' norm1 is penalised (see e.g. Ng, 2011). Nonetheless, our research demonstrates that penalising the bias terms enforce sparsity and uniqueness of the estimated function, which is not achieved without including the bias regularisation. The practical similarity between these two explicit regularisations can be attributed to the presence of implicit regularisation, which considers the bias terms as well. The updates performed by first-order optimisation methods do not distinguish between bias and weight parameters, suggesting that they are subject to the same implicit regularisation. Consequently, while both regularisation approaches may yield similar estimators in practical settings, we contend that the theoretical estimators obtained with bias term regularisation capture the observed implicit regularisation effect. Hence, it is essential to investigate the implications of penalising the bias terms when addressing Questions 1 and 2, as the answers obtained in this scenario significantly differ from those without bias penalisation.

It is also worth mentioning that Shevchenko et al. (2022), Safran et al. (2022) prove that gradient flow learns sparse estimators for networks with ReLU activations. These sparsity guarantees are yet much weaker (larger number of activated directions) as they additionally deal with optimisation considerations (in opposition to directly considering the minimiser of the optimisation problem in both our work and the line of works mentioned above).

Contributions.After introducing the setting in Section 2, we address Question 1 in Section 3 using a similar analysis approach as Savarese et al. (2019). The key result, Theorem 1, establishes that the representational cost of a function, when allowed a _free_ skip connection, is given by the weighted total variation of its second derivative, incorporating a \(}\) term. Notably, penalising the bias terms introduces a \(}\) multiplicative weight in the total variation, contrasting with the absence of bias penalisation.

This weighting fundamentally impacts the answer to Question 2. In particular, it breaks the shift invariance property of the function's representational cost, rendering the analysis technique proposed by Debarre et al. (2022) inadequate2. To address this issue, we delve in Sections 4 and 5 into the computation and properties of solutions to the optimisation problem:

\[_{f}\|}f^{}\|_{} { subject to } i[n],\ f(x_{i})=y_{i}.\]

In Section 4, we reformulate this problem as a continuous dynamic program, enabling a simpler analysis of the minimisation problem. Leveraging this dynamic program reformulation, Section 5 establishes the uniqueness of the solution. Additionally, under certain data assumptions, we demonstrate that the minimiser is among the sparest interpolators in terms of the number of kinks. It is worth noting that similar results have been studied in the context of sparse spikes deconvolution (Candes and Fernandez-Granda, 2014; Fernandez-Granda, 2016; Poon et al., 2019), and our problem can be seen as a generalisation of basis pursuit (Chen et al., 2001) to infinite-dimensional parameter spaces. However, classical techniques for sparse spikes deconvolution are ill-suited for addressing Question 2, as the set of sparsest interpolators is infinite in our setting.

Finally, the significance of bias term regularisation in achieving sparser estimators during neural network training is illustrated on toy examples in Section 6. To ensure conciseness, only proof sketches are presented in the main paper, while the complete proofs can be found in the Appendix.

## 2 Infinite width networks

This section introduces the considered setting, representing unidimensional functions as infinite width networks. Some precise mathematical arguments are omitted here, since this construction follows directly the lines of Savarese et al. (2019); Ongie et al. (2019). This work considers unidimensional functions \(f_{}:\) parameterised by a one hidden layer neural networks with ReLU activation as

\[f_{}(x)=_{j=1}^{m}a_{j}(w_{j}x+b_{j}),\]

where \((z)=(0,z)\) is the ReLU activation and \(=(a_{j},w_{j},b_{j})_{j[m]}^{3m}\) are the parameters defining the neural network. The vector \(=(a_{j})_{j[m]}\) stands for the weights of the last layer, while \(\) and \(\) respectively stand for the weights and biases of the hidden layer. For any width \(m\) and parameters \(\), the quantity of importance is the squared Euclidean norm of the parameters: \(\|\|_{2}^{2}=_{j=1}^{m}a_{j}^{2}+w_{j}^{2}+b_{j}^{2}\).

We recall that contrary to Savarese et al. (2019); Ongie et al. (2019), the bias terms are included in the considered norm here. We now define the representational cost of a function \(f:\) as

\[R(f)=_{m\\ ^{3m}}\|\|_{2}^{2} f_{}=f.\]

By homogeneity of the parameterisation, a typical rescaling trick (see e.g. Neyshabur et al., 2014; Theorem 1) allows to rewrite

\[R(f)=_{m,^{3m}}\|\|_{1} f_{}=fw_{j}^{2}+b_{j}^{2}=1j[m].\]

Note that \(R(f)\) is only finite when the function \(f\) is exactly described as a finite width neural network. We aim at extending this definition to a much larger functional space, i.e. to any function that can be arbitrarily well approximated by finite width networks, while keeping a (uniformly) bounded norm of the parameters. Despite approximating the function with finite width networks, the width necessarily grows to infinity when the approximation error goes to \(0\). Similarly to Ongie et al. (2019), define

\[(f)=_{ 0^{+}}(_{m,^{3m}} \|\|_{2}^{2}|f_{}(x)-f(x)| x[-}{{}},}{{}}] ).\]

Note that the approximation has to be restricted to the compact set \([-}{{}},}{{}}]\) to avoid problematic degenerate situations. The functional space for which \((f)\) is finite is much larger than for \(R\), and includes every compactly supported Lipschitz function, while coinciding with \(R\) when the latter is finite.

By rescaling argument again, we can assume the hidden layer parameters \((w_{j},b_{j})\) are in \(_{1}\) and instead consider the \(_{1}\) norm of the output layer weights. The parameters of a network can then be seen as a discrete signed measure on the unit sphere \(_{1}\). When the width goes to infinity, a limit is then properly defined and corresponds to a possibly continuous signed measure. Mathematically, define \((_{1})\) the space of signed measures \(\) on \(_{1}\) with finite total variation \(\|\|_{}\). Following the typical construction of Bengio et al. (2005); Bach (2017), an infinite width network is parameterised by a measure \((_{1})\) as3

\[f_{}:x_{_{1}}(wx+b)(w,b). \]

Similarly to Ongie et al. (2019), \((f)\) verifies the equality

\[(f)=_{(_{1})}\|\|_{} \,f=f_{}.\]The right term defines the \(_{1}\) norm (Kurkova and Sanguineti, 2001), i.e. \((f)=\|f\|_{_{1}}\). The \(_{1}\) norm is intuited to be of major significance for the empirical success of neural networks. In particular, generalisation properties of small \(_{1}\) norm estimators are derived by Kurkova and Sanguineti (2001); Bach (2017), while many theoretical results support that training one hidden layer neural networks with gradient descent often yields an implicit regularisation on the \(_{1}\) norm of the estimator (Lyu and Li, 2019; Ji and Telgarsky, 2019; Chizat and Bach, 2020; Boursier et al., 2022). However, this implicit regularisation of the \(_{1}\) norm is not systematic and some works support that a different quantity can be implicitly regularised on specific examples (Razin and Cohen, 2020; Vardi and Shamir, 2021; Chistikov et al., 2023). Still, \(_{1}\) norm seems to be closely connected to the implicit bias and its significance is the main motivation of this paper. While previous works also studied the representational costs of functions by neural networks (Savarese et al., 2019; Ongie et al., 2019), they did not penalise the bias term in the parameters' norm, studying a functional norm slightly differing from the \(_{1}\) norm. This subtlety is at the origin of different levels of sparsity between the obtained estimators with or without penalising the bias terms, as discussed in Sections 5 and 6; where sparsity of an estimator here refers to the minimal width required for a network to represent the function (or similarly to the cardinality of the support of \(\) in Equation (1)). This notion of sparsity is more meaningful than the sparsity of the parameters \(\) here, since different \(\) (with different levels of sparsity) can represent the exact same estimated function.

### Unpenalised skip connection

Our objective is now to characterise the \(_{1}\) norm of unidimensional functions and minimal norm interpolators, which can be approximately obtained when training a neural network with norm regularisation. The analysis and result yet remain complex despite the unidimensional setting. Allowing for an unpenalised affine term in the neural network representation leads to a cleaner characterisation of the norm and description of minimal norm interpolators. As a consequence, we parameterise in the remaining of this work finite and infinite width networks as follows:

\[f_{,a_{0},b_{0}}:x a_{0}x+b_{0}+f_{}(x), f _{,a_{0},b_{0}}:x a_{0}x+b_{0}+f_{}(x),\]

where \((a_{0},b_{0})^{2}\). The affine part \(a_{0}x+b_{0}\) actually corresponds to a _free_ skip connection in the neural network architecture (He et al., 2016) and allows to ignore the affine part in the representational cost of the function \(f\), which we now define as

\[_{1}(f)=_{ 0^{+}}(_{m, ^{3_{m}}\\ (a_{0},b_{0})^{2}}\|\|_{2}^{2} |f_{,a_{0},b_{0}}(x)-f(x)| { for any }x[-}{{}},}{{}}] ).\]

The representational cost \(_{1}(f)\) is similar to \((f)\), but allows for a _free_ affine term in the network architecture. Similarly to \((f)\), it can be proven, following the lines of Savarese et al. (2019); Ongie et al. (2019), that \(_{1}(f)\) verifies

\[_{1}(f)=_{(_{1} )\\ a_{0},b_{0}}\|\|_{}f=f_{,a_{0},b_{0}}.\]

The remaining of this work studies more closely the cost \(_{1}(f)\). Theorem 1 in Section 3 can be directly extended to the cost \((f)\), i.e. without unpenalised skip connection. Its adapted version is given by Theorem 4 in Appendix C for completeness.

Multiple works also consider free skip connections as it allows for a simpler analysis (e.g. Savarese et al., 2019; Ongie et al., 2019; Debarre et al., 2022; Sanford et al., 2022). Since a skip connection can be represented by two ReLU neurons (\(z=(z)-(-z)\)), it is commonly believed that considering a free skip connection does not alter the nature of the obtained results. This belief is further supported by empirical evidence in Section 6 and Appendix B, where our findings hold true both with and without free skip connections.

## 3 Representational cost

Theorem 1 below characterises the representational cost \(_{1}(f)\) of any univariate function.

**Theorem 1**.: _For any Lipschitz function \(f:\),_

\[_{1}(f)=\|}f^{}\|_{}=_{}}\;|f^{}|(x).\]

#### For any non-Lipschitz function, \(_{1}(f)=\).

In Theorem 1, \(f^{}\) is the distributional second derivative of \(f\), which is well defined for Lipschitz functions. Without penalisation of the bias terms, the representational cost is given by the total variation of \(f^{}\)(Savarese et al., 2019). Theorem 1 states that penalising the biases adds a weight \(}\) to \(f^{}\). This weighting favors sparser estimators when training neural networks, as shown in Section 5. Also, the space of functions that can be represented by infinite width neural networks with finite parameters' norm, when the bias terms are ignored, corresponds to functions with bounded total variation of their second derivative. When including these bias terms in the representational cost, second derivatives additionally require a _light tail_. Without a _free_ affine term, Theorem 4 in Appendix C characterises \((f)\), which yields an additional term accounting for the affine part of \(f\).

We note that Remark 4.2 of E and Wojtowytsch (2021) and Theorem 1 by Li et al. (2020) are closely related to Theorems 1 and 4. However, these results only establish an equivalence between the norm \((f)\) and another norm that quantifies the total variation of \(}f^{}\), aside from the affine term. Our result, on the other hand, provides an exact equality between both norms, which proves to be particularly useful in the analysis of minimal norm interpolators.

**Example 1**.: _If the function \(f\) is given by a finite width network \(f(x)=_{i=1}^{n}a_{i}(w_{i}x+b_{i})\) with \(a_{i},w_{i} 0\) and pairwise different \(}{w_{i}}\); Theorem 1 yields \(_{1}(f)=_{i=1}^{n}|a_{i}|^{2}+b_{i}^{2}}\). This exactly corresponds to half of the squared \(_{2}\) norm of the vector \((c_{i}a_{i},}{c_{i}},}{c_{i}})_{i=1,,n}\) when \(c_{i}=+b_{i}^{2}}}{|a_{i}|}}\). This vector is thus a minimal representation of the function \(f\)._

According to Theorem 1, the minimisation problem considered when training one hidden ReLU layer infinite width neural network with \(_{2}\) regularisation is equivalent to the minimisation problem

\[_{f}_{i=1}^{n}(f(x_{i})-y_{i})^{2}+\|}f^{ }\|_{}. \]

_What types of functions do minimise this problem? Which solutions does the \(\|}f^{}\|_{}\) regularisation term favor?_ These fundamental questions are studied in the following sections. We show that this regularisation favors functions that can be represented by small (finite) width neural networks. On the contrary, when the weight decay term does not penalise the biases of the neural network, such a sparsity is not particularly preferred as highlighted by Section 6.

## 4 Computing minimal norm interpolator

To study the properties of solutions obtained by training data with either an implicit or explicit weight decay regularisation, we consider the minimal norm interpolator problem

\[_{,a_{0},b_{0}}\|\|_{2}^{2}  i[n],\;f_{,a_{0},b_{0}}(x_{i})=y_{i}, \]

where \((x_{i},y_{i})_{i[n]}^{2n}\) is a training set. Without loss of generality, we assume in the following that the observations \(x_{i}\) are ordered, i.e., \(x_{1}<x_{2}<<x_{n}\). Thanks to Theorem 1, this problem is equivalent, when allowing infinite width networks, to

\[_{f}\|}f^{}\|_{} {such that } i[n],\;f(x_{i})=y_{i}. \]

Lemma 1 below actually makes these problems equivalent as soon as the width is larger than some threshold smaller than \(n-1\). Equation (4) then corresponds to Equation (2) when the regularisation parameter \(\) is infinitely small.

**Lemma 1**.: _The problem in Equation (4) admits a minimiser. Moreover, with \(i_{0}\{i[n]|x_{i} 0\}\), any minimiser is of the form_

\[f(x)=ax+b+_{i=1}^{n-1}a_{i}(x-_{i})_{+}\]

_where \(_{i}(x_{i},x_{i+1}]\) for any \(i\{1,,i_{0}-2\}\), \(_{i_{0}-1}(x_{i_{0}-1},x_{i_{0}})\) and \(_{i}[x_{i},x_{i+1})\) for any \(i\{i_{0},,n-1\}\)._

Lemma 1 already provides a first guarantee on the sparsity of any minimiser of Equation (4). It indeed includes at most \(n-1\) kinks. In contrast, minimal norm interpolators with an infinite numof kinks exist when the bias terms are not regularised (Debarre et al., 2022). An even stronger sparse recovery result is given in Section 5. Lemma 1 can be seen as a particular case of Theorem 1 of Wang et al. (2021). In the multivariate case and without a free skip connection, the latter states that the minimal norm interpolator has at most one kink (i.e. neuron) per _activation cone_ of the weights4 and has no more than \(n+1\) kinks in total. The idea of our proof is that several kinks among a single activation cone could be merged into a single kink in the same cone. The resulting function then still interpolates, but has a smaller representational cost.

Lemma 1 allows to only consider \(2\) parameters for each interval \((x_{i},x_{i+1})\) (potentially closed at one end). Actually, the degree of freedom is only \(1\) on such intervals: choosing \(a_{i}\) fixes \(_{i}\) (or inversely) because of the interpolation constraint. Lemma 2 below uses this idea to recast the minimisation Problem (4) as a dynamic program with unidimensional state variables \(s_{i}\) for any \(i[n]\).

**Lemma 2**.: _If \(x_{1}<0\) and \(x_{n} 0\), then we have for \(i_{0}=\{i[n]|x_{i} 0\}\) the following equivalence of optimisation problems_

\[_{ i[n],f(x_{i})=y_{i}}\|f^{}} \|_{}=_{(s_{i_{0}-1},s_{i_{0}})}g_{i_{0}}(s_{i_ {0}},s_{i_{0}-1})+c_{i_{0}-1}(s_{i_{0}-1})+c_{i_{0}}(s_{i_{0}}) \]

_where the set \(\) and the functions \(g_{i}\) and \(c_{i}\) are defined in Equations (6) to (8) below._

Let us describe the dynamic program defining the functions \(c_{i}\), which characterises the minimal norm interpolator thanks to Lemma 2. First define for any \(i[n-1]\), the slope \(_{i}-y_{i}}{x_{i+1}-x_{i}}\); the function

\[g_{i+1}(s_{i+1},s_{i})(s_{i+1}-_{i})-x_{i}(s_{i}- _{i}))^{2}+(s_{i+1}-s_{i})^{2}}(s_{i+1},s_{i}) ^{2}; \]

\[ S_{i}(s)(-,_{i} ]s>_{i}\\ \{_{i}\}s=_{i}\\ [_{i},+)s<_{i}s.\]

The set \(\) is then the union of three product spaces given by

\[(-,_{i_{0}-1})(_{i_{0}-1},+) \{(_{i_{0}-1},_{i_{0}-1})\}(_{i_{0}-1},+) (-,_{i_{0}-1}). \]

Finally, we define the functions \(c_{i}:_{+}\) recursively as \(c_{1}=c_{n} 0\) and

\[c_{i+1}:s_{i+1}_{s_{i} S_{i}(s_{i+1})}g_{i+1}(s_{i+ 1},s_{i})+c_{i}(s_{i})i\{1,,i_{0}-2\} \] \[c_{i}:s_{i}_{s_{i+1} S_{i}(s_{i})}g_{i+1}(s_{i+1},s _{i})+c_{i+1}(s_{i+1})i\{i_{0},,n-1\}.\]

Equation (8) defines a dynamic program with a continuous state space. Intuitively for \(i i_{0}\), the variable \(s_{i}\) accounts for the left derivative at the point \(x_{i}\). The term \(g_{i+1}(s_{i+1},s_{i})\) is the minimal cost (in neuron norm) for reaching the point \((x_{i+1},y_{i+1})\) with a slope \(s_{i+1}\), knowing that the left slope is \(s_{i}\) at the point \((x_{i},y_{i})\). Similarly, the interval \(S_{i}(s_{i})\) gives the reachable slopes5 at \(x_{i+1}\), knowing the slope in \(x_{i}\) is \(s_{i}\). Finally, \(c_{i}(s_{i})\) holds for the minimal cost of fitting all the points \((x_{i+1},y_{i+1}),,(x_{n},y_{n})\) when the left derivative in \((x_{i},y_{i})\) is given by \(s_{i}\). It is defined recursively by minimising the sum of the cost for reaching the next point \((x_{i+1},y_{i+1})\) with a slope \(s_{i+1}\), given by \(g_{i+1}(s_{i+1},s_{i})\); and the cost of fitting all the points after \(x_{i+1}\), given by \(c_{i+1}\). This recursive definition is illustrated in Figure 1 below. A symmetric definition holds for \(i<i_{0}\).

The idea to derive Equation (6) is to first use Lemma 1 to get a finite representation of a minimal function \(f^{*}\). From there, the minimal cost for connecting the point \((x_{i+1},y_{i+1})\) with \((x_{i},y_{i})\) is done by using a single kink in between. The restrictions given by \(s_{i}\) and \(s_{i+1}\) then yield a unique possible kink. Minimizing its neuron norm then yields Equation (6)

**Remark 1**.: _Equation (5) actually considers the junction of two dynamic programs: a first one corresponding to the points with negative \(x\) values and a second one for positive values. This separation around \(x=0\) is not needed for Lemma 2, but allows for a cleaner analysis in Section 5. Lemmas 1 and 2 also hold for any arbitrary choice of \(i_{0}\). In particular for \(i_{0}=1\), Equation (5) would not consider the junction of two dynamic programs anymore, but a single one._Lemma 2 formulates the minimisation of the representational cost among the interpolating functions as a simpler dynamic program on the sequence of slopes at each \(x_{i}\). This equivalence is the key technical result of this work, from which Section 5 defers many properties on the minimiser(s) of Equation (4).

## 5 Properties of minimal norm interpolator

Thanks to the dynamic program formulation given by Lemma 2, this section derives key properties on the interpolating functions of minimal representational cost. In particular, it shows that Equation (4) always admits a unique minimum. Moreover, under some condition on the training set, this minimising function has the smallest number of kinks among the set of interpolators.

**Theorem 2**.: _The following optimisation problem admits a unique minimiser:_

\[_{f}\|}f^{}\|_{} { such that } i[n],\ f(x_{i})=y_{i}.\]

The proof of Theorem 2 uses the correspondence between interpolating functions and sequences of slopes \((s_{i})_{i[n]}\), where the set \(\) is defined by Equation (23) in Appendix D.2. In particular, we show that the following problem admits a unique minimiser:

\[_{}_{i=1}^{n-1}g_{i+1}(s_{i+1},s_{i}). \]

We note in the following \(^{*}\) the unique minimiser of the problem in Equation (9). From this sequence of slopes \(^{*}\), the unique minimising function of Equation (4) can be recovered. Moreover, \(^{*}\) minimises the dynamic program given by the functions \(c_{i}\) as follows:

\[c_{i+1}(s^{*}_{i+1})=g_{i+1}(s^{*}_{i+1},s^{*}_{i})+c_{i}(s^{*}_ {i})i[i_{0}-2]\] \[c_{i}(s^{*}_{i})=g_{i+1}(s^{*}_{i+1},s^{*}_{i})+c_{i+1}(s^{*}_{i+ 1})i\{i_{0},,n-1\}.\]

Using simple properties of the functions \(c_{i}\) given by Lemma 7 in Appendix E, properties on \(^{*}\) can be derived besides the uniqueness of the minimal norm interpolator. Lemma 3 below gives a first intuitive property of this minimiser, which proves helpful in showing the main result of the section.

**Lemma 3**.: _For any \(i[n]\), \(s^{*}_{i}[(_{i-1},_{i}),(_{i-1},_{i})]\), where \(_{0}_{1}\) and \(_{n}_{n-1}\) by convention._

A geometric interpretation of Lemma 3 is that the optimal (left or right) slope in \(x_{i}\) is between the line joining \((x_{i-1},y_{i-1})\) with \((x_{i},y_{i})\) and the line joining \((x_{i},y_{i})\) with \((x_{i+1},y_{i+1})\).

### Recovering a sparsest interpolator

We now aim at characterising when the minimiser of Equation (4) is among the set of sparsest interpolators, in terms of number of kinks. Before describing the minimal number of kinks required

Figure 1: Recursive definition of the dynamic program for \(i i_{0}\).

to fit the data in Lemma 4, we partition \([x_{1},x_{n})\) into intervals of the form \([x_{n_{k}},x_{n_{k+1}})\) where

\[n_{0}=1k 0n_{k}<n, \]

and \((0) 0\) by convention. If we note \(f_{}\) the canonical piecewise linear interpolator, it is either convex, concave or affine on every interval \([x_{n_{k}-1},x_{n_{k+1}}]\). This partitioning thus splits the space into convex, concave and affine parts of \(f_{}\), as illustrated by Figure 2 on a toy example. This partition is crucial in describing the sparsest interpolators, thanks to Lemma 4.

**Lemma 4**.: _If we denote by \(\|f^{}\|_{0}\) the cardinality of the support of the measure \(f^{}\),_

\[_{f\\  i,f(x_{i})=y_{i}}\|f^{}\|_{0}=_{k 1 }-n_{k}}{2}_{_{n_{k-1}} _{n_{k}}}.\]

Lemma 4's proof idea is that for any interval \([x_{k-1},x_{k+1})\) where \(f_{}\) is convex (resp. concave) non affine, any function requires at least one positive (resp. negative) kink to fit the three data points in this interval. The result then comes from counting the number of such disjoint intervals and showing that a specific interpolator exactly reaches this number.

The minimal number of kinks required to interpolate the data is given by Lemma 4. Before giving the main result of this section, we introduce the following assumption on the data \((x_{k},y_{k})_{k[n]}\).

**Assumption 1**.: _For the sequence \((n_{k})_{k}\) defined in Equation (10):_

\[n_{k+1}-n_{k} 3_{n_{k}}=_{n_{k}-1}k 0.\]

Assumption 1 exactly means there are no \(6\) (or more) consecutive points \(x_{k},,x_{k+5}\) such that \(f_{}\) is convex (without \(3\) aligned points) or concave on \([x_{k},x_{k+5}]\). This assumption depends a lot on the structure of the true model function (if there is any). For example, it holds if the truth is given by a piecewise linear function, while it may not if the truth is given by a quadratic function. Theorem 3 below shows that under Assumption 1, the minimal cost interpolator is amongst the sparsest interpolators, in number of its kinks.

**Theorem 3**.: _If Assumption 1 holds, then_

\[*{argmin}_{f\\  i,f(x_{i})=y_{i}}\|}f^{}\|_{ }*{argmin}_{f\\  i,f(x_{i})=y_{i}}\|f^{}\|_{0}. \]

Theorem 3 states conditions under which the interpolating function \(f\) with the smallest representational cost \(_{1}(f)\) also has the minimal number of kinks, i.e. ReLU hidden neurons, among the set of interpolators. It illustrates how norm regularisation, and in particular adding the biases' norm to the weight decay, favors estimators with a small number of neurons. While training neural networks with norm regularisation, the final estimator can actually have many non-zero neurons, but they all align towards a few key directions. As a consequence, the obtained estimator is actually equivalent to a small width network, meaning they have the same output for every input \(x\).

Recall that such a sparsity does not hold when the bias terms are not regularised. More precisely, some sparsest interpolators have a minimal representational cost in that case, but there are also minimal cost interpolators with an arbitrarily large (even infinite) number of kinks (Debarre et al., 2022). There is thus no particular reason that the obtained estimator is sparse when minimising

Figure 2: Partition given by \((n_{k})_{k}\) on a toy example.

the representational cost without penalising the bias terms. Section 6 empirically illustrates this difference of sparsity in the recovered estimators, depending on whether or not the bias parameters are penalised in the norm regularisation.

The generalisation benefit of this sparsity remains unclear. Indeed, generalisation bounds in the literature often rely on the parameters' norm rather than the network width (i.e., sparsity level). The relation between sparsest and min norm interpolators is important to understand in the particular context of implicit regularisation. In particular, while Boursier et al. (2022) conjectured that the implicit bias for regression problem was towards min norm interpolators, Chistikov et al. (2023) recently proved that the implicit bias could sometimes instead lead to sparsest interpolators. Our result suggests that both min norm and sparsest interpolators often coincide, which could explain the prior belief of convergence towards min norm interpolators. Yet, Theorem 3 and Chistikov et al. (2023) instead suggest that, at least in some situations, implicit bias favors sparsest interpolators, yielding different estimators6.

**Remark 3**.: _Theorem 3 states that sparse recovery, given by Equation (11), occurs if Assumption 1 holds. When \(n_{k+1}-n_{k} 4\), i.e. there are convex regions of \(f_{}\) with at least \(6\) points, Appendix A gives a counterexample where Equation (11) does not hold. However, Equation (11) can still hold under weaker data assumptions than Assumption 1. In particular, Appendix A gives a necessary and sufficient condition for sparse recovery when there are convex regions with exactly \(6\) points. When we allow for convex regions with at least \(7\) points, it however becomes much harder to derive conditions where sparse recovery still occurs._

**Remark 4**.: _The counterexample presented in Appendix A reveals an unexpected outcome: minimal representational cost interpolators may not necessarily belong to the sparsest interpolators. This finding is closely related to the idea that it may not be generally feasible to characterize the implicit regularisation of gradient descent as minimising parameters norm (Vardi and Shamir, 2021; Chistikov et al., 2023). In particular, Vardi and Shamir (2021), Chistikov et al. (2023) rely on examples where minimal norm interpolators are not the sparsest ones; and the implicit regularisation instead favors the latter. We believe that this inherent limitation is one of the underlying reason for the different implicit regularization effects observed in other settings such as matrix factorization (Gunasekar et al., 2017; Razin and Cohen, 2020; Li et al., 2020a)._

### Application to classification

In the binary classification setting, max-margin classifiers, defined as the minimiser of the problem

\[_{f}(f)\; i[n],y_{i}f(x_{i}) 1, \]

are known to be the estimators of interest. Indeed, gradient descent on the cross entropy loss \(l(,y)=(1+e^{-y})\) converges in direction to such estimators (Lyu and Li, 2019; Chizat and Bach, 2020). Theorem 3 can be used to characterise max- margin classifiers, leading to Corollary 1.

**Corollary 1**.: \[*{argmin}_{f\\  i[n],y_{i}f(x_{i}) 1}_{1}(f) *{argmin}_{f\\  i[n],y_{i}f(x_{i}) 1}\|f^{}\|_{0}.\]

Theorem 3 yields that the max-margin classifier is among the sparsest margin classifiers, when a free skip connection is allowed. We believe that the left minimisation problem admits a unique solution. However, uniqueness cannot be directly derived from Theorem 3, but would instead require another thorough analysis, using an adapted dynamic programming reformulation. Since the uniqueness property is of minor interest, we here prefer to focus on a direct corollary of Theorem 3. We emphasise that no data assumptions are required for classification tasks, apart from being univariate.

## 6 Experiments

This section compares, through Figure 3, the estimators that are obtained with and without counting the bias terms in the regularisation, when training a one-hidden ReLU layer neural network. The code is made available at github.com/eboursier/penalising_biases.

For this experiment, we train neural networks by minimising the empirical loss, regularised with the \(_{2}\) norm of the parameters (either with or without the bias terms) with a regularisation factor \(=10^{-3}\). Each neural network has \(m=200\) hidden neurons and all parameters are initialised i.i.d. as centered Gaussian variables of variance \(}{{}}\) (similar results are observed for larger initialisation scales).7 There is no free skip connection here, which illustrates its benignity: the results that are expected by the above theory also happen without free skip connection. Experiments with a free skip connection are given in Appendix B and yield similar observations.

As predicted by our theoretical study, penalising the bias terms in the \(_{2}\) regularisation enforces the sparsity of the final estimator. The estimator of Figure 2(a) indeed counts \(2\) kinks (the smallest number required to fit the data), while in Figure 2(b), the directions of the neurons are scattered. More precisely, the estimator is almost _smooth_ near \(x=-0.5\), while the sparse estimator of Figure 2(a) is clearly not differentiable at this point. Also, the estimator of Figure 2(b) includes a clear additional kink at \(x=0\). Figure 3 thus illustrates that counting the bias terms in regularisation can lead to sparser estimators.

## 7 Conclusion

This work studies the importance of parameters' norm for one hidden ReLU layer neural networks in the univariate case. In particular, the parameters' norm required to represent a function is given by \(}f^{}_{}\) when allowing for a free skip connection. In comparison to weight decay, which omits the bias parameters in the norm, an additional \(}\) weighting term appears in the representational cost. This weighting is of crucial importance since it implies uniqueness of the minimal norm interpolator. Moreover, it favors sparsity of this interpolator in number of kinks. Minimising the parameters' norm (with the biases), which can be either obtained by explicit or implicit regularisation when training neural networks, thus leads to sparse interpolators. We believe this sparsity is a reason for the good generalisation properties of neural networks observed in practice.

Although these results provide some understanding of minimal norm interpolators, extending them to more general and difficult settings remains open. Even if the representational cost might be described in the multivariate case [as done by Ongie et al., 2019, without bias penalisation], characterising minimal norm interpolators seems very challenging in that case. Characterising minimal norm interpolators, with no free skip connection, also presents a major challenge for future work.

Figure 3: Final estimator when training one-hidden layer network with \(_{2}\) regularisation. The green dots correspond to the data and the green line is the estimated function. Each blue star represents a hidden neuron \((w_{j},b_{j})\) of the network: its \(x\)-axis value is given by \(-b_{j}/w_{j}\), which coincides with the position of the kink of its associated ReLU; its \(y\)-axis value is given by the output weight \(a_{j}\).