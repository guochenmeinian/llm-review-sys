# ReMI: A Dataset for Reasoning with Multiple Images

Mehran Kazemi\({}^{1}\), Nishanth Dikkala\({}^{2}\), Ankit Anand\({}^{1}\), Petar Devic\({}^{3}\),

**Ishita Dasgupta\({}^{1}\), Fangyu Liu\({}^{1}\), Bahare Fatemi\({}^{2}\), Pranjal Awasthi\({}^{2}\),**

**Dee Guo\({}^{2}\), Sreenivas Gollapudi\({}^{2}\), Ahmed Qureshi\({}^{3}\) \({}^{1}\)Google DeepMind, \({}^{2}\)Google Research, \({}^{3}\) Google**

###### Abstract

With the continuous advancement of large language models (LLMs), it is essential to create new benchmarks to effectively evaluate their expanding capabilities and identify areas for improvement. This work focuses on multi-image reasoning, an emerging capability in state-of-the-art LLMs. We introduce \(\), a dataset designed to assess LLMs' ability to **Re**ason with **M**ultiple **I**mages. This dataset encompasses a diverse range of tasks, spanning various reasoning domains such as math, physics, logic, code, table/chart understanding, and spatial and temporal reasoning. It also covers a broad spectrum of characteristics found in multi-image reasoning scenarios. We have benchmarked several cutting-edge LLMs using \(\) and found a substantial gap between their performance and human-level proficiency. This highlights the challenges in multi-image reasoning and the need for further research. Our analysis also reveals the strengths and weaknesses of different models, shedding light on the types of reasoning that are currently attainable and areas where future models require improvement. To foster further research in this area, we are open-sourcing \(\): [https://huggingface.co/datasets/mehrankazemi/ReMI](https://huggingface.co/datasets/mehrankazemi/ReMI).

## 1 Introduction

Large Language Models (LLMs) have demonstrated an extraordinary evolution, not only in their output quality but also in their burgeoning capabilities. A significant direction of development has been models' ability to perform increasingly general forms of reasoning that were previously not possible. The emergence of these novel capabilities necessitates the development of robust evaluation benchmarks and metrics to measure and enhance model performance in these specific areas.

The ability of LLMs to reason over text has improved in leaps and bounds, and has been studied extensively . More recent developments in multi-modal models has opened up a new space of reasoning problems, moving toward the capability to reason across multiple, potentially disparate, sources of information presented in various formats . This multi-modal reasoning capability has numerous applications, from complex problem-solving to information synthesis. In this paper, we focus on a specific aspect of this capability: multi-image reasoning. A large portion of the current benchmarks for multi-modal evaluation are based on a single image . We address the lack of dedicated evaluation frameworks in this domain by introducing a comprehensive benchmark designed to specifically assess and improve this skill in LLMs. We focus specifically on reasoning problems where besides visual understanding, one needs to find a step-by-step solution to a problem. This process often involves combining information across text and multiple images - a skill that is currently not extensively evaluated in existing benchmarks. This contribution aims to catalyze progress in multi-image reasoning, ultimately enabling LLMs to better navigate and extract insights from the increasingly complex information landscape of our digital world.

We introduce \(\), a new benchmark designed for **Re**asoning with **M**ultiple **I**mages. Our goal is to cover a broad spectrum of domains where integrating information across multiple modalities is necessary, as well as various key properties unique to multi-image reasoning. To achieve this, we developed 13 tasks that span a range of domains and properties. The domains covered in \(\) include algebra, calculus, geometry, graph theory, physics, temporal and spatial/maps reasoning, tabular and chart understanding, coding, and logic. The properties covered by \(\) include sequential vs set consumption of image information, problems that require reasoning over images demonstrating a similar concept (e.g., two charts) or different concepts (e.g., geometry shape and a table), images that are interleaved with the text or not, and the number of separate images provided as input. Our tasks require reasoning over at least two and up to six images. Table 1 outlines the tasks, domains and properties. Our images comprise a variety of heterogeneous image types including charts, tables, equations, emojis, graphs, shapes, maps, clocks, objects, LaTeX diagrams, functions, etc.

We evaluate state-of-the-art LLMs on \(\) and compare their performance to humans, showing that model performances remain substantially behind human performance (see Fig 1). Interestingly, our results also reveal that models may perform better when multiple images are fed to them separately as opposed to all in one image; this is especially true in the case where the images are interleaved with the question text. A detailed failure analysis reveals model shortcomings that can guide future improvement efforts.

## 2 Related Work

**Vision-language foundation models.** In our work, we focus on vision language generation models, i.e. models that produce open-ended text conditioned on text and images. Frozen  and Flamingo  first transformed LLMs into vision-language models by adding a vision transformer tower and training cross/self-attention layers to enable LLMs to perceive visual information. Subsequently, a large volume of research emerged focusing on the approach of stitching a pretrained visual encoder (usually vision transformer) to a pretrained langauge model. \(\), \(\), \(\), \(\), \(\) all follow similar techniques. The latest closed-source frontier models such as GPT-4 , Gemini  and Claude 3  all have vision input support and are also reported to be the best performing models across popular vision-language reasoning benchmarks . These frontier models are able to condition fairly arbitrarily on sequences of interleaved image and text. However, most vision-language benchmarks test models' performance on a single image-text pair; the focus of this paper is to take a step toward evaluating more flexible vision-language abilities.

**Reasoning Benchmarks.** Reasoning has been a core area of interest for NLP systems. The initial benchmarks focused on'simpler' reasoning tasks which largely involve language understanding (e.g. SuperGLUE , HellaSwag , Lambda ). With LLMs making remarkable strides in recent years, a plethora of benchmarks requiring much stronger reasoning abilities have emerged. Some of these like \(\) and ARC  focus on science questions. \(\), GSM8K  and \(\) focus on mathematical problem solving. There is also a line of works  which construct semi-synthetic benchmarks to evaluate the logical deductive reasoning abilities of LLMs. In addition, the BIG-Bench  suite of tasks contains many which focus on reasoning.

**Vision-language reasoning benchmarks.** Some recent benchmarks like  present reasoning problems that require conditioning on images; however, they predominantly require only a single image, and do not directly measure how well the model can integrate information across different images. Cross-image reasoning benchmarks exist but are restricted to the entailment task or focus on limited number of domains. NLVR  creates pairs of images composed of synthetic 2D objects and the task is identifying whether the caption is entailed from the image. \(\) creates pairs of images and asks comparative questions about them. NLVR2  extends NLVR by

Figure 1: Model performances on \(\).

replacing synthetic images with pairs of images sampled from MS COCO . MaRVL  expands a similar idea to multi-cultural and multilingual scenarios and only focuses on the natural image domain. MileBench  develops a multi-modal long-context benchmark. SEED-Bench-2  proposes a hierarchy of different vision-language datasets including multi-image datasets composed of frames extracted from videos. BLINK  is a collection of 14 visual perception tasks where some of the tasks involve multiple images, e.g. visual similarity and multi-view reasoning. None of these mentioned benchmarks aim to test vision-language models for _complex reasoning_ in _multi-image_ scenarios. We aim to propose a holistic benchmark covering a wide range of visual information in the world and focuses on complex reasoning of multi-images.

## 3 The \(\) Dataset

Multi-image reasoning can arise in many domains and the problems involving reasoning over multiple images may differ in some key properties. We aim to create a benchmark that exhibits many domains and covers those key properties as much as possible. To this end, we included 13 tasks in our benchmark that covers the following domains: _Algebra_, _Calculus_, _Geometry_, _Tabular Reasoning_, _Time Arithmetic_, _Logic_, _Physics_, _Spatial Reasoning_, _Graph Theory_, _Charts_, _Maps_, and _Coding_. We also identified the following key properties specific to multi-image reasoning and aimed for having tasks that provide a good coverage of them:

* **Sequential vs Set:** In some tasks, the provided images have to be consumed in a sequence (e.g., computing a quantity from one image and then using that quantity in the second image), whereas in some other tasks, the provided images constitute a set. When more than two images are provided, they may be grouped into subsets that have to be consumed sequentially.
* **Same vs Different Concept:** In some multi-image reasoning problems, the provided images all correspond to the same concept (e.g., all of them are charts, or function graphs) whereas in some other problems, the provided images may correspond to different concepts (e.g., one image might be a geometry shape, and the other might be a table).
* **Interleaving:** For all our tasks, we can either provide all the images first and then ask a question about them, or the images can be interleaved with the question task when they are referred to. To enable experimenting for both settings, we make a subset of the tasks interleaved while for the others we provide the image at the beginning of the prompt.
* **Number of images:** In some tasks, a variable number of images may be provided as input.

Solving our tasks requires parsing and understanding the information in the images and text of the question provided as input, which is often followed by the model having to reason using this information to arrive at the correct answer. We provide a brief description of each task below and a more detailed description in the Appendix. In Figure 2, we illustrate a sample from each of the tasks in \(\). Moreover, in Table 1, we specify the domain and properties for each of the tasks in \(\).

(1) \(\)**:** Solve a system of linear equations involving digits and emojis. Each image contains an equation or the final expression to be computed. (2) \(\)**:** Given multiple function graphs in separate images, answer questions about them. (3) \(\)**:** Given two shapes (in two different images) with a common property, compute a missing value of one of the shapes. (4) \(\)**:** Given the shape of an object (in one image) on which an operation is to be done and a table of various costs (in a different image), compute the total cost of the operation. (5) \(\)**:** Given the before and after snapshots of two objects colliding (each in a separate image), answer questions about their state. (6) \(\)**:** Given two clocks with different designs (each in a separate

  
**Task Name** & & & & & & \\   \(\) & \(\) & Seq & Same & Yes & 6 \\ \(\) & \(\) & Mix & Same & Yes & 3 \\ \(\) & \(\) & Seq & Same & No & 2 \\ \(\) & \(\), \(\) & Seq & Diff & Yes & 2 \\ \(\) & \(\) & Set & Same & No & 2 \\ \(\) & \(\) Arithmetic & Set & Same & No & 2 \\ \(\) & \(\), \(\) & Seq & Diff & Yes & 2 \\ \(\) & \(\) & Set & Same & No & 2 \\ \(\) & \(\) & Seq & Same & Yes & 2 \\ \(\) & \(\) Theory & Set & Same & Yes & 2 \\  \(\) & \(\), \(\) & Mix & Same & Yes & 4 \\ \(\) & \(\) & Seq & Diff & No & 2 \\ \(\) & \(\) & Mix & Same & Yes & 5 \\   

Table 1: The properties of the tasks in our benchmark.

image), compute the time difference between them. (7) Schedule: Given the current time (in one image) and a table of train schedules (in another image), answer questions about the next scheduled train. (8) Charts: Given two charts (each in a separate image), possibly in different formats - e.g., one bar chart and one pie chart, identify the differences between the reported values or reason jointly from values in both charts. (9) CodeEdit: Given a TikZ code, the rendered image, and the goal image, determine which line of code should be removed to get to the goal image. (10) Isomorphism:

Figure 2: Sample problems from the tasks in ReMl. Note that the images are provided to the models separately in place of the <image> markers.

Given two graphs (in two images), determine if they are isomorphic or not. (11) Maps: Given a description of a navigation and four navigation routes on a map (each in a different image), determine which one corresponds to the one in the description. (12) RefCOCO: Given a real-world image and another image of same dimensions with non-overlapping circles marked on it, determine which circle overlaps the most with a target entity in the real image. (13) IQ: Given a matrix of shapes that have a logical connection and with one missing value, predict the shape that goes into the missing part.

Figure 3 presents some extra statistics for the tasks in ReMI, including the average length of the questions per task, the number of unique labels per task, the number of problems with \(n\) images for different values of \(n\), and the average time spent by humans for solving problems from each task.

## 4 Experiments

We report the performance of multiple state-of-the-art models on our benchmark.

**Metrics:** We mainly report accuracy for our tasks. For textual outputs, we compute exact match while handling slight variations such as spacing issues, lowercase vs uppercase, etc. For numeric answers, we compute a relaxed accuracy with 1% tolerance, mainly to avoid penalizing rounding errors. In the case of relaxed accuracy with tolerance \(\), a numeric prediction \(p\) is considered correct if \((1-)l p(1+)l\) where \(l\) is the label. Following the original GeomVerse paper , we report relaxed accuracy with 3% tolerance for our GeomShapes and GeomCost tasks as intermediate operations are also rounded and different operation orders lead to slight variations in the final result. For the Clocks, we allow 10 minutes tolerance to account for slight variations in reading times from analog clocks. In our analyses, we also use a metric named _error reduction percentage(ERP)_ with respect to a baseline, which corresponds to how much a model reduces the error with respect to a baseline. We define ERP for a baseline as follows:

\[ERP_{T}(baseline,model)=100*-}{}\]

Conceptually, the numerator corresponds to how much of the error has been reduced compared to the baseline, and the denominator normalizes by how much room for error reduction existed.

**Naive Baseline:** We provide the expected accuracy for a naive baseline that predicts the answers without looking at the images, by only guessing the final answer based on the text of the question. For

Figure 3: Statistics for ReMI and its tasks.

multi-choice questions, we assume this baseline will predict the answer correctly with \(1/c\) chance where \(c\) is the number of choices (for CodeEdit, we consider any line ending in semi-colon to be one of possible choices); for Charts, for every question asking about which cell changed, we assume this baseline responds with \((0,0)\), and for every question about the number of cells that changed, we assume this baseline responds with \(1\); for Clocks, when asking about the difference in time, we assume this baseline always predicts \(12*60\) minutes; for RefCOCO, we assume this baseline always predicts the circle labeled \(0\).

**Models:** We experiment with four state of the art model families, namely Idefics , Gemini [46; 38], Claude 3 , and GPT4 . From the Idefics family, we experiment with Idefics2. From the Gemini family, we experiment with three models with different sizes and properties, namely Gemini Ultra, Gemini 1.5 Pro, and Gemini Flash. From the Claude 3 family, we experiment with the Sonnet model, and from the GPT4 family, we experiment with GPT4 Turbo. We ran Idefics2 on a single GPU (we used the float16 case without image splitting to ensure the model fits in one GPU) and for the other models we used their APIs.

**Human Performance:** For each task we sampled \(20\) examples from the test set and had them solved by someone knowledgeable (but not necessarily expert) in that area. We also asked them to measure the amount of time they spent on solving the \(20\) problems. The average time per problem for each task is reported in Figure 3(d). We observe that some tasks have been more time consuming than the others with EmojiAlgebra being the most time consuming and the IQ being the least time consuming.

### Human Baseline Substantially Beats SoTA Models in Multi-Image Reasoning

In Table 2, we present the results of the models as well as the naive baseline and the human performance on the tasks in ReMI. We make the following observations from the obtained results. Firstly, all the models significantly outperform the naive baseline, almost on any task; however, their performance remains far behind the human performance in general, and also in most of the tasks. Secondly, there are some tasks where none of the current models are good at, including Clocks and Isomorphism, where the performances remain quite low1. This reveals a potential capability gap in the current state-of-the-art models. Thirdly, we observe that different models perform well on different tasks. For example, Gemini 1.5 substantially outperforms GPT4-Turbo on the IQ, whereas GPT4-Turbo substantially outperforms Gemini 1.5 on EmojiAlgebra. This hints that the frontier models may have different capabilities and limitations.

Hereafter, unless stated otherwise, we do the rest of the experiments with Gemini Pro 1.5, the best overall performing model on ReMI.

    &  &  &  &  &  &  &  &  \\  & **Baseline** & & **Sonnet** & & **Ultra** & & **Flash** & **1.5** & **Turbo** \\  EmojiAlgebra & 0.0 & 1.0 & 28.0 & 2.5 & 15.0 & 44.5 & **57.5** & 100.0 \\ FuncRead & 5.5 & 11.0 & 24.0 & 15.0 & 36.0 & **40.0** & 26.0 & 100.0 \\ GeomShapes & 0.0 & 14.0 & 17.5 & 14.5 & 34.0 & **51.5** & 32.5 & 100.0 \\ GeomCost & 0.0 & 2.0 & 58.5 & 47.0 & 75.0 & **81.5** & 70.5 & 90.0 \\ Collisions & 30.8 & 31.5 & 51.5 & 36.5 & 56.5 & 50.5 & **62.0** & 100.0 \\ Clocks & 2.0 & 3.0 & **5.0** & 4.0 & 4.0 & 2.5 & 4.0 & 80.0 \\ Schedule & 0.0 & 21.5 & 36.0 & 33.0 & 43.0 & 40.5 & **49.5** & 90.0 \\ Charts & 2.5 & 1.0 & 40.0 & 30.0 & 53.0 & **54.0** & 44.0 & 95.0 \\ CodeEdit & 14.9 & 12.5 & 20.0 & 24.5 & **46.0** & 41.0 & 42.0 & 95.0 \\ Isomorphism & 50.0 & 35.5 & 57.0 & 65.0 & 67.0 & **72.0** & 71.5 & 100.0 \\ Maps & 28.0 & 38.0 & 39.5 & 39.0 & **47.0** & **47.0** & **36.5** & 100.0 \\ RefCOCO & 12.0 & 14.5 & 30.0 & 31.0 & 49.0 & **56.0** & 37.5 & 95.0 \\ IQ & 25.0 & 19.0 & 50.5 & 30.0 & 53.0 & **76.0** & 62.5 & 100.0 \\  ReMI & 13.1 & 15.7 & 35.2 & 28.6 & 44.5 & **50.5** & 45.8 & 95.8 \\   

Table 2: The performance of SoTA models on ReMI and its individual tasks. The winner for each task is in bold and the second winner is underlined.

### Single-Image vs Multi-Image Reasoning

We measure whether models perform better when we provide the multiple images separately or when we put them all in a single image and feed them to the model. To this end, we report \(ERP_{T}\)(single-image model, multi-image model) corresponding to how much the multi-image model reduces the error with respect to the single-image model for each task \(T\). The results are provided in Figure 4. We observe that for most of the tasks, feeding images separately results in positive gains (positive ERP) compared to a single-image case. A manual analysis of the model outputs in the two settings shows that the model may even employ different strategies for solving the problem in these settings. For example, in the case of EmojiAlgebra, we observe that in the single-image case, the model mostly starts by assigning a variable (e.g., \(a\), \(b\), etc.) to each emoji and then solving the problem by using those variables; However, in the case of multi-image, the model mostly uses either the emojis themselves or their names when doing the calculations.

Interleaved tasks are affected more:Out of the six tasks that are positively affected the most (FuncRead, IQ, CodeEdit, Schedule, Isomorphism, and RefCOCO), we observe that five of them (the first five) are interleaved tasks. Averaging the ERP for the interleaved and non-interleaved datasets, we observe a gain of \(19.8\%\) for the former case and a gain of \(4.9\%\) for the latter case. This hints that reasoning with multiple images might be easier for the models than feeding all images in one image, especially when the images are provided interleaved with text at the right positions.

### Failure Analysis

For each task, we manually examined \(20\) examples where the answers given by overall best performing model (Gemini 1.5 Pro) was incorrect and analyzed the dominant reasons behind the failures. This analysis revealed several interesting failure modes - some intuitive and some not - as described below and summarized in Table 3. The diversity of errors observed highlights that this multi-image reasoning domain elicits a wide range of different behaviors that can go wrong in a range of different ways, and that our benchmark tests this wide range of abilities. Calculation errors were present in many of the math-related datasets, so we do not discuss them separately for each task.

Figure 4: ERP when the images are provided to Gemini 1.5 as multiple vs a single image.

}  
**Task Name** & **Major Source(s) of Error** \\  EmojiAlgebra & 1-Calculation errors, 2- Confusing similar emojis, 3- Misreading from the images (especially minus signs). \\  FuncRead & 1-Model value readings are typically off by about 1 unit. \\  GeomS Shapes \& GoomCost & 1-Calculation errors, 2- Going on a wrong solution path (e.g., computing irrelevant unknown values), 3- Misreading and mis-assigning values (e.g., assigning a length value to the height), 4- Philumination non-existent values. \\  Collisions & 1-Net recognizing when two objects are moving together after a collision, 2-figuring the velocity vector direction when calculating absolute velocity difference between two objects. \\   & 1-Net also to read the line properly, 2- Misraising the minute hand for the hour hand, 3- Not paying attention to the prompt specifying the times are in the same day. \\  Schedule & 1-Not able to read the line properly, 2- Retrieving the wrong values from the table given the time, 3- Sometimes ignoring the 24th format. \\  Charts & 1-Mis-assigning values to the correct row/column in heatmap charts, 2- Under-counting the number of differences in two charts, 3- Reasoning errors of the type _The value decreased from X to X_. \\  CodeEdit & 1-Suggesting removal of non-unceiversal parts of code, 2-Not properly understanding what each line of code represents in the compiled image. \\  Isomorphism & 1-Jumping permanently to a conclusion after finding one or two nodes that map to each other, 2- Philicinating non-existent nodes or edges. \\  Maps & 1-Incorrectly counts similar pairs as the same type of objects (e.g., bins that share the same color but different score). 2- Not paying attention to the prompt specifying the certain area of interest, 3- Guys arbitrary directions that don’t match the situation shown in the grid map. \\  RefCOCO & 1-For the image with the circles, coordinate reading is off by 100-200, 2- Not a proper understanding of spatial clues such as _top right_. \\  IQ & 1-Unifidful-aess to model’s own CoI (e.g., it explains the color should be green, but selects the red, 2- Overly predicting the operation to be rotation (despite no rotation being in the dataset). \\   

Table 3: Major error sources for each task, identified with a manual inspection of 20 failed examples.

For EmojAlgebra, the overall reasoning process of the model is mostly correct. However, the model sometimes confuses similar emojis. As an example, it assigns a similar (or the same) name to, and or to, and then these variables get confused in the later calculations. We also observe some misreading of the expressions.

For both Clocks and Schedule, the model suffers from not being able to read the time correctly; e.g. often the minute hand was mistaken for the hour hand. Figure 5 shows a sample clock and times read by the various models. Despite reading the wrong times, the model generally does a good job of computing the time difference given these wrong times, though it sometimes ignores the prompt instructing it to consider both times to be on the same day. In the case of the Schedule, the value retrieved from the table is often not the right value, even given the wrong time read by the mode; Sometimes, this is due the model confusing AM vs PM.

For GeomShapes and GeomCost, the model mostly understands the high-level task (extracting a value from the first shape and using it in the next) and executes it correctly. But it makes reasoning errors on the geometry side where it tries to compute the values for unknown sides/angles that are irrelevant to the question. We also observe some misreading of values or mis-assigning the value for one element to another element (e.g., assigning a side value to a height). Hallucinating non-existent values is another issue. In both cases, the model performs well in understanding and executing the high-level task of extracting a value from the first shape and then using it in the next shape.

For Isomorphism, the model tended to jump to conclusions prematurely, based on some initial guesses. For example, it found one or two nodes that had similar structures and jumped to the conclusion that the graphs are isomorphic, whereas other nodes had different structures. The model also suffered from hallucinating non-existent nodes and edges.

For RefCOCO, the model understands how to use the provided coordinates; however, the coordinates it reads for the circles tends to be off by 10-20%. Moreover, sometimes the model correctly explained that the object of interest is, e.g., on the _top left_ but then selected a circle that was not on the _top left_, showing a potential gap in truly understanding what _top left_ or other spatial clues are.

For IQ, the model was sometimes unfaithful to its own reasoning (e.g., it explained that the answer must be a green shape, but selected a red shape as the final answer). Also, even though we had no rotation operations in the dataset, the model tended to over-predict the logical operation being rotation, probably due to a prior bias on the presence of rotation in IQ questions.

For FuncRead, the model understands the general logic and follows the calculations correctly, but it fails to correctly read values from the function graphs; the values are mostly off by about 1 unit showing the model can locate the vicinity of the point, but lacks precision.

For Collisions, the model demonstrates issues in interpreting physics diagrams and calculations, particularly in differentiating between elastic and inelastic collisions. It struggles to account for implicit information such as orientation component of the objects velocity.

For Charts, the model reads the correct values from heatmaps, but assigns values to the wrong row/column (typically off by one row/column). Moreover, when we ask the model to identify how many differences there are between two charts, it mostly under-counts. We also see multiple cases where the model claimed a value decreased from \(X\) to \(X\) (i.e. to the same amount).

For CodeEdit, while correctly identifying the visual changes in the rendered image, the model lacks understanding of how each line of code contributes to the final image. In some cases incorrectly suggests removing code segments that are not present in the original code. Despite these flaws, the model demonstrates some understanding of the code structure, as it avoids suggesting the removal of critical code components that would prevent code from compiling.

For Maps, the model has difficulty counting objects of interest accurately, especially when there are many distractions on the map. It sometimes hallucinates information about restaurants and bars or lists those outside the area of interest. Also, it struggles to differentiate between similar pins, such as coffee shops, bars, and restaurants. When asked about directions, the model's suggestions are often random. While it may list correct streets, the directions it describes do not match the map.

Figure 5: Time read by different models for one of the clocks in the Clocks.

In Figures 9, 10, 11, 12, 13, 14, 15 16 and 17 we present some examples of model failures.

**Reasoning Errors vs Image Reading Errors:** Besides computation errors, we observed that reasoning errors and image reading errors are two of the most dominant sources of failures across the tasks in \(\). We examined 125 failed examples and verified whether there existed a reasoning error or image reading error in them. The results are provided in Figure 7.

We observe that in \(12\%\) of the cases, the values were read correctly from the image and the reasoning was also sound; the failures in these cases were primarily due to minor calculation errors suggesting that while the model understood the problem and approached it correctly, it stumbled in the final execution. In \(37.6\%\) of the cases, the image values were read correctly, but the reasoning was incorrect, This is the most frequent error type, indicating that correct reasoning still remains one of the critical gaps even in the state-of-the-art models. In \(24.8\%\) of the cases, the model misread some information from the images, but the reasoning is sound. That is, had the model extracted the correct information, the final answer could have been correct. This result indicates a second gap in terms of extracting and parsing the correct values from the images and assigning them to the correct components. Finally, in \(25.6\%\) of the cases, the model struggled both in extracting information from the image and in applying correct reasoning.

### Performance as a Function of Task Properties

In Table 1, we identified multiple distinguishing factors for each of the tasks in \(\). Here, we aim to measure and compare model performances for tasks exhibiting each property. We note that a naive averaging of a model's performance for datasets in each category and comparing to the other category may be flawed due to: 1- Performances on some tasks being generally higher due to the label space being binary or categorical, and 2- some tasks being generally easier/harder than the other tasks.

Figure 6: Model performances as a function of the task properties presented in Table 1.

Figure 7: The confusion matrix of reasoning errors vs image reading errors.

To account for the first issue mentioned above, for each model \(M\) and task \(T\) we compute \(P_{M,T}\) as \(ERP_{T}(naive,M)\), i.e. the model's error reduction percentage compared to the naive baseline. This corresponds to how much of the error has been reduced by the model when accounting for random guess, normalized by how much room for error reduction existed when accounting for random guess. To account for the second issue, as a proxy for the hardness of the tasks, we use the average performance of our models on each task \(P_{T}=P_{M,T}}{}\). We then compute the relative gain compared to the average as \(P^{}_{M,T}=-P_{T}}{P_{T}}\). Conceptually, this corresponds to the following: After accounting for random noise, how much each model reduced the error with respect to the model-average baseline, on each task. For each model and each group of tasks \(\) (e.g., all interleaved tasks), we compute and report the average \(}P^{}_{M,T}}{||}\).

For this experiment, we excluded Idefics2 due to its poor performance. The results for the other models are reported in Figure 6. According to Figure 6(a), GPT4 Turbo and Gemini 1.5 (the two best performing models) outperform other models on interleaved tasks more than non-interleaved tasks, showing the progress in the frontier models for this recently emerged capability. Figure 6(b) compares the tasks that have a maximum of two images to the tasks where the maximum number of images is more than two. We observe a similar behavior as the interleaved vs non-interleaved case, with Gemini 1.5 gains more on the latter tasks. GPT4 Turbo, however, gains equally on both cases. Interestingly, we observe that while Gemini Flash remains competitive on the former tasks, its performance falls behind on the latter group. According to Figure 6(c), for sequence vs set inputs, we see a stark difference for Claude3 Sonnet and Gemini 1.5. Claude3 Sonnet performs better on set type tasks and Gemini 1.5 performs better on sequence type tasks, but almost loses its advantage on set type tasks. Finally, Figure 6(d) shows that when provided with images corresponding to different concepts, most models show a similar behaviour except for Gemini Ultra that performs better when the concepts are different and GPT4 Turbo that performs better when the concepts are the the same.

### Zeroshot vs Fewshot Performance

So far, we examined the performance of various models in a zero-shot setting. We now examine how much of the gap between the model performance and the human performance can be closed by providing fewshot examples as demonstration to the model. Specifically, we prepend two examples along with their manually-written chain of thought solutions to the prompt. We then measured and report \(ERP_{T}()\) corresponding the how much the fewshot model reduced the error compared to the zeroshot model. The results are reported in Figure 8.

According to the results, we observe that the overall performance of the model on ReMI improves from \(51.5\%\) to \(57.9\%\) corresponding to almost \(12.5\%\) relative improvement. This shows that LLMs may be capable of learning multi-image reasoning tasks in context and improve their performance. However, the overall performance still remains significantly behind the human baseline which is \(95.8\%\). We also see that the amount of improvement is task dependent with some tasks gaining from fewshot examples substantially more than the others.

## 5 Conclusion

We introduced ReMI, a dedicated benchmark for multi-image reasoning that covers several domains and several key properties that arise when reasoning with multiple images. We evaluated the frontier LLMs on ReMI and compared their performance to humans. The results show a stark gap between model performance and human performance showing a significant room for improvement in the reasoning capabilities of the current state-of-the-art LLMs. Future work can focus on improving LLMs for the limitations found in our failure analysis and measure how much they translate to improvements on ReMI.

Figure 8: ERP(0-shot, 2-shot) for Gemini 1.5.