# On student-teacher deviations in distillation:

does it pay to disobey?

 Vaishnavh Nagarajan

Google Research

vaishnavh@google.com &Aditya Krishna Menon

Google Research

adityakmenon@google.com &Srinadh Bhojanapalli

Google Research

bsrinadh@google.com &Hossein Mobahi

Google Research

hmobahi@google.com &Sanjiv Kumar

Google Research

sanjivk@google.com

###### Abstract

Knowledge distillation (KD) has been widely used to improve the test accuracy of a "student" network, by training it to mimic the soft probabilities of a trained "teacher" network. Yet, it has been shown in recent work that, despite being trained to fit the teacher's probabilities, the student may not only significantly deviate from the teacher probabilities, but may also outdo than the teacher in performance. Our work aims to reconcile this seemingly paradoxical observation. Specifically, we characterize the precise nature of the student-teacher deviations, and argue how they _can_ co-occur with better generalization. First, through experiments on image and language data, we identify that these probability deviations correspond to the student systematically _exaggerating_ the confidence levels of the teacher. Next, we theoretically and empirically establish another form of exaggeration in some simple settings: KD exaggerates the implicit bias of gradient descent in converging faster along the top eigendirections of the data. Finally, we tie these two observations together: we demonstrate that the exaggerated bias of KD can simultaneously result in both (a) the exaggeration of confidence and (b) the improved generalization of the student, thus offering a resolution to the apparent paradox. Our analysis brings existing theory and practice closer by considering the role of gradient descent in KD and by demonstrating the exaggerated bias effect in both theoretical and empirical settings.

## 1 Introduction

In knowledge distillation (KD) [6; 17], one trains a small "student" model to match the predicted soft label distribution of a large "teacher" model, rather than the one-hot labels that the training data originally came with. This has emerged as a highly effective model compression technique, and has inspired an actively developing literature that has sought to explore applications of distillation to various settings [41; 13; 52], design more effective variants [44; 3; 38; 5], and better understand theoretically when and why distillation is effective [32; 40; 35; 2; 8; 34; 10; 43; 25; 14; 39].

On paper, distillation is intended to help by transferring the soft probabilities of the (one-hot loss trained) teacher over to the student. Intuitively, we would desire this transfer to be perfect: the more a student fails to match the teacher's probabilities, the more we expect its performance to suffer. After all, in the extreme case of a student that simply outputs uniformly random labels, the student's performance would be as poor as it can get.

However, recent work by Stanton et al.  has challenged our presumptions underlying what distillation supposedly does, and how it supposedly helps. First, they show in practice that the student does _not_ adequately match the teacher probabilities, despite being trained to fit them. Secondly, students that do successfully match the teacher probabilities, may generalize worse than students that show some level of deviations from the teacher [47, Fig. 1]. Surprisingly, this deviation occurs even in self-distillation settings  where the student and the teacher have identical architecture and thus the student has the potential to fit the teacher's probabilities to full precision. Even more remarkably, the self-distilled student not only deviates from the teacher's probabilities, but also supercedes the teacher in performance.

How is it possible for the student to deviate from the teacher's probabilities, and yet counter-intuitively improve its generalization beyond the teacher? Our work aims to reconcile this paradoxical behavior. On a high level, our answer is that while _arbitrary_ deviations in probabilities may indeed hurt the student, in reality there are certain _systematic_ deviations in the student's probabilities. Next, we argue, these systematic deviations and improved generalization co-occur because they arise from the same effect: a (helpful) form of regularization that is induced by distillation. We describe these effects in more detail in our list of key contributions below:

1. [label=()]
2. **Exaggerated confidence**: Across a wide range of architectures and image & language classification data (spanning more than \(20\) settings in total) we demonstrate (SS3) that _the student exaggerates the teacher's confidence_. Most typically, on low-confidence points of the teacher, the student achieves even lower confidence than the teacher; in other settings, on high-confidence points, the student achieves even higher confidence than the teacher (Fig 0(a)). Surprisingly, we find such deviations even with self-distillation, implying that this cannot be explained by a mere student-teacher capacity mismatch. This reveals a systematic unexpected behavior of distillation.
3. **Exaggerated implicit bias:** Next, we demonstrate another form of exaggeration: in some simple settings, self-distillation exaggerates the implicit bias of gradient descent (GD) in converging faster along the top data eigendirections. We demonstrate this theoretically (Thm 4.1) for linear regression, as a gradient-descent counterpart to the seminal non-gradient-descent result of Mobahi et al.  (see SS1.1 for key differences). Empirically, we provide the first demonstration of this effect for the cross entropy loss on neural networks (Fig 0(b) and SS4.1).

Figure 1: **(a): Distilled student exaggerates confidence of one-hot-loss trained teacher. For each training sample \((x,y)\), we plot \(X=(p^{}_{y^{}}(x))\) versus \(Y=(p^{}_{y^{}}(x))\), which are the teacher and student probabilities on the teacher’s predicted label \(y^{}\), transformed monotonically by \((u)=[u/(1-u)]\). Note that this is a density plot where higher the brightness, higher the number of datapoints with that \(X\) and \(Y\) value. We find that the distilled student predictions deviate from the \(X=Y\) line by either underfitting teacher’s low confidence points (i.e., we find \(Y X\) for small \(X\)) and/or overfitting teacher’s high confidence points (i.e., \(Y X\) for large \(X\)). See §3 for details. (b) Distillation exaggerates implicit bias of one-hot gradient descent training. We consider an MLP trained on an MNIST-based dataset. Each plot shows the time-evolution of the \(_{2}\) norm of the first layer parameters projected onto two randomly picked eigendirections; the \(\)’s corresponds to the final parameters. First observe that the one-hot-trained teacher moves faster towards its final \(X\) axis value than its final \(Y\) axis value; this corroborates the well-known implicit bias of standard GD training. But crucially, we find that distillation _exaggerates_ this bias: the student moves even faster towards its final \(X\) axis value. In §5 we argue how this exaggerated bias manifests as the exaggerated confidence in Fig 0(a).

3. **Reconciling the paradox:** Finally, we tie the above observations together to resolve our paradox. We empirically argue how the exaggerated bias towards top eigenvectors causes the student to both (a) exaggerate confidence levels and (b) outperform the teacher (see SS5). This presents a resolution for how deviations in probabilities can co-occur with improved performance.

### Bridging key gaps between theory and practice

The resolution above helps paint a more coherent picture of theoretical and empirical studies in distillation that were otherwise disjoint. Mobahi et al.  proved that distillation exaggerates the bias of a _non_-gradient-descent model, one that is picked from a Hilbert space with explicit \(_{2}\) regularization. It was an open question as to whether this bias exaggeration effect is indeed relevant to settings we care about in practice. Our work establishes its relevance to practice, ultimately also drawing connections to the disjoint empirical work of Stanton et al. .

In more explicit terms, we establish relevance to practice in the following ways:

1. We provide a formal proof of the exaggerated bias of distillation for a (linear) GD setting, rather than a non-GD setting (Theorem 4.1).
2. We empirically verify the exaggerated bias of KD in more general settings e.g., a multi-layer perceptron (MLP) and a convolutional neural network (CNN) with cross-entropy loss (SS4.1). This provides the first practical evidence of the bias exaggeration affect of .
3. We relate the above bias to the student-teacher deviations in Stanton et al. . Specifically, we argue that the exaggerated bias manifests as exaggerates student confidence levels, which we report on a wide range of image and language datasets.
4. Tangentially, our findings also help clarify when to use early-stopping and loss-switching in distillation (SS5.2).

As a more general takeaway for practitioners, our findings suggest that _not_ matching the teacher probabilities exactly can be a good thing, provided the mismatch is not arbitrary. Future work may consider devising ways to explicitly induce careful deviations that further amplify the benefits of distillation e.g., by using confidence levels to reweight or scale the temperature on a per-instance basis.

## 2 Background and Notation

Our interest in this paper is _multiclass classification_ problems. This involves learning a _classifier_\(h\) which, for input \(\), predicts the most likely label \(h()=[K]=\{1,2,,K\}\). Such a classifier is typically implemented by computing _logits_\(^{K}\) that score the plausibility of each label, and then computing \(h()=*{argmax}_{y}f_{y}()\). In neural models, these logits are parameterised as \(()=^{}()\) for learned weights \(^{D K}\) and embeddings \(()^{D}\). One may learn such logits by minimising the _empirical loss_ on a training sample \(S=\{(x_{n},y_{n})\}_{n=1}^{N}\):

\[R_{}()_{n[N]}(y_{n})^{ }((_{n})), \]

where \((y)\{0,1\}^{K}\) denotes the _one-hot encoding_ of \(y\), \(()[(1,),,(K,)]^{K}\) denotes the _loss vector_ of the predicted logits, and each \((y,(x))\) is the loss of predicting logits \((x)^{K}\) when the true label is \(y[K]\). Typically, we set \(\) to be the softmax cross-entropy \((y,())=- p_{y}()\), where \(()(())\) is the _softmax_ transformation of the logits.

Equation 1 guides the learner via one-hot targets \((y_{n})\) for each input. Distillation [6; 17] instead guides the learner via a target label distribution \(^{}(_{n})\) provided by a _teacher_, which are the softmax probabilities from a distinct model trained on the _same_ dataset. In this context, the learned model is referred to as a _student_, and the training objective is

\[R_{}()_{n[N]}^{ }(_{n})^{}((_{n})). \]

One may also consider a weighted combination of \(R_{}\) and \(R_{}\), but we focus on the above objective since we are interested in understanding each objective individually.

Compared to training on \(R_{}\), distillation often results in improved performance for the student . Typically, the teacher model is of higher capacity than the student model; the performance gains of the student may thus informally be attributed to the teacher transferring rich information about the problem to the student. In such settings, distillation may be seen as a form of model compression. Intriguingly, however, even when the teacher and student are of the _same_ capacity (a setting known as _self-distillation_), one may see gains from distillation . The questions we explore in this paper are motivated by the self-distillation setting; however, for a well-rounded analysis, we empirically study both the self- and cross-architecture-distillation settings.

## 3 A fine-grained look at student-teacher probability deviations

To analyze student-teacher deviations, Stanton et al.  measured the disagreement and the KL divergence between the student and teacher probabilities, _in expectation over all points_. They found these quantities to be non-trivially large, contrary to the premise of distillation. To probe into the exact nature of these deviations, our idea is to study the _per-sample_ relationship between the teacher and student probabilities.

**Setup**. Suppose we have teacher and distilled student models \(},}^{K}\) respectively. We seek to analyze the deviations in the corresponding predicted probability vectors \(}()\) and \(}()\) for each \((,y)\) in the train and test set, rather than in the aggregated sense as in Stanton et al. . To visualize the deviations, we need a scalar summary of these vectors. An initial obvious candidate is the probabilities assigned to the ground truth class \(y^{}\), namely \((p^{}_{y^{}}(),p^{}_{y^{}}())\). However, the student does not have access to the ground truth class, and is only trying to mimic the teacher. Hence, it is more meaningful and valuable to focus on the _teacher's predicted class_, which the student can infer i.e., the class \(y^{}=*{argmax}_{y^{}[K]}p^{}_{y^{ }}()\). Thus, we examine the teacher-student probabilities on this label, \((p^{}_{y^{}}(),p^{}_{y^{ }}())\). Purely for visual clarity, we further perform a monotonic logit transformation \((u)=[u/(1-u)]\) on these probabilities to produce real values in \((-,+)\). Thus, we compare \((p^{}_{y^{}}())\) and \((p^{}_{y^{}}())\) for each train and test sample \((,y)\). For brevity, we refer to these values as _confidence values_ for the rest of our discussion. It is worth noting that these confidence values possess another natural interpretation. For any probability vector \(()\) computed from the softmax of a logit vector \(()\), we can write \((p_{y}())\) in terms of the logits as \(f_{y}()-_{k y}(f_{k}())\). This can be interpreted as a notion of multi-class margin for class \(y\).

To examine point-wise deviations of these confidence values, we consider scatter plots of \((p^{}_{y^{}}())\) (\(X\)-axis) vs. \((p^{}_{y^{}}())\) (\(Y\)-axis). We report this on the training set for some representative self-distillation settings in Figures 0(a), cross-architecture distillation settings in Fig 1(b) and test set in Fig 1(a). In all plots, the dashed line indicates the \(X=Y\) line. All values are computed at the end of training. The tasks considered include image classification benchmarks, namely CIFAR10, CIFAR-100 , Tiny-ImageNet , ImageNet  and text classification tasks from the GLUE benchmark (e.g., MNLI , AGNews ). See SSC.1 for details on the experimental hyperparameters. The plots for all the settings (spanning more than \(25\) settings in total) are consolidated in SSC.2.

Figure 2: **Exaggeration of confidence in other settings.** There are settings where even on test data, and for cross-architecture distillation settings, where the student exaggerates the teacher’s confidence (here specifically on low-confidence points).

**Distilled students exaggerate one-hot trained teacher's confidence**. If the objective of distillation were minimized perfectly, the scatter plots would simply lie on \(X=Y\) line. However, we find that across _all_ our settings, the scatter shows stark deviations from \(X=Y\). Importantly, these deviations reveal a characteristic pattern. In a vast majority of our settings, the confidence values of the student are an "exaggerated" version of the teacher's. This manifests in one of two ways. On the one hand, for points where the teacher attains low confidence, the student may attain _even lower_ confidence -- this is true for all our image settings, either on training or on test data. In the scatter plots, this can be inferred from the fact that for a large proportion of points where the \(X\) axis value is small (low teacher confidence), it is also the case that \(Y X\) (even lower student confidence). As a second type of exaggeration, for points where the teacher attains high confidence, the student may attain _even higher_ confidence -- this is true for a majority of our language settings, barring some cross-architecture ones. To quantify these qualitative observations, in SSC.4 we report the slope \(m\) of the best linear fit \(Y=mX+c\) over the low and high-confidence points and find that they correspond to \(m>1\) in the corresponding scenarios above.

There are two reasons why these findings are particularly surprising. First, we see these deviations in both self-distillation settings (Fig 0(a)) and cross-architecture settings (see Fig 1(b)). It is remarkable that this should occur in self-distillation given that the student has the capacity to match the teacher probabilities. Next, these deviations can occur on both training and test data (Fig 1(a)). Here, it is surprising that there is deviation on training data, where the student is explicitly trained to match teacher probabilities. However, we must note that there are a few exceptions where it only weakly appears in training data e.g., CIFAR-10, but in those cases it is prominent on test data.

We also conduct ablation studies in SSC.5 showing that this observation is robust to various hyperparameter changes (batch size, learning rate, length of training), and choices of visualization metrics. In SSC.3, we explore further patterns underlying the student's underfit points. In SSC.2, we discuss the exceptions where these characteristic deviations fail to appear even if deviations are stark e.g., in cross-architecture language settings.

Thus, our point-wise visualization of deviations clarifies that the student's mismatch of the teacher's probabilities in Stanton et al.  stems from a systematic exaggeration of the teacher's confidence. How do we reconcile this deviation with the student outperforming the teacher in self-distillation? In the next section, we turn our attention to a different type of exaggeration exerted by distillation that will help us resolve this question.

## 4 Distillation exaggerates implicit bias of GD

While the optimal solution to the KD loss (Eq 2) is for the student to replicate the teacher's probabilities, in practice we minimize this loss using gradient descent (GD). Thus, to understand why the student exaggerates the teacher's confidence in practice in practice, it may be key to understand how GD interacts with the distillation loss. Indeed, in this section we analyze GD and formally demonstrate that, for linear regression, distillation exaggerates a pre-existing implicit bias in GD: the tendency to converge faster along the top eigendirections of the data. We will also empirically verify that our insight generalizes to neural networks with cross-entropy loss. In a later section, we will connect this exaggeration of bias back to the exaggeration of confidence observed in SS3.

Concretely, we analyze gradient flow in a linear regression setting with early-stopping. Note that linear models have been used as a way to understand distillation even in prior work [40; 35]; and early-stopping is a typical design choice in distillation practice [12; 7; 23]. Consider an \(n p\) dataset \(\) (where \(n\) is the number of samples, \(p\) the number of parameters) with target labels \(\). Assume that the Gram matrix \(^{}\) is invertible. This setting includes overparameterized scenarios (\(p>n\)) such as when \(\) corresponds to the linearized (NTK) features of neural networks [20; 28]. Then, a standard calculation spells out the weights learned at time \(t\) under GD on the loss \((1/2)\|-\|^{2}\) as:

\[(t) =^{}(^{})^{-1}(t)  \] \[(t) :=-e^{-t^{}}. \]

Thus, the weights depend crucially on a time-dependent matrix \(\). Intuitively, \(\) determines the convergence rate independently along each eigendirection: at any time \(t\), it skews the weight assignedto an eigendirection of eigenvalue \(\) by the value \((1-e^{- t})\). As \(t\) this factor increases all the way to \(1\) for all directions, thus indicating full convergence. But for any finite time \(t\), the topmost direction would have a larger factor than the rest, implying a bias towards that direction. Our argument is that, _distillation further exaggerates this implicit bias_. To see why, consider that the teacher is trained to time \(T^{}\). Through some calculation, the student's weights can be similarly expressed in closed-form, but with \(\) replaced by the product of two matrices:

\[}() =^{}(^{})^{-1}}() \] \[}() :=(t)(T^{}). \]

One can then argue that the matrix \(}\) corresponding to the student is more skewed towards the top eigenvectors than the teacher:

**Theorem 4.1**.: _(informal; see SSB for full version and proof) Let \(_{k}(t)\) and \(_{k}(t)\) respectively denote the component of the teacher and student weights along the \(k\)'th eigenvector of the Gram matrix \(^{}\), at any time \(t\). Let \(k_{1}<k_{2}\) be two indices for which the eigenvalues satisfy \(_{k_{1}}>_{k_{2}}\). Consider any time instants \(t\) and \(\) at which both the teacher and the student have converged equally well along the top direction \(k_{1}\), in that \(_{k_{1}}(t)=_{k_{1}}()\). Then along the bottom direction, the student has a strictly smaller component than the teacher, as in,_

\[|_{k_{2}}()}{_{k_{2}}(t)}|<1. \]

The result says that the student relies less on the bottom eigendirections than the teacher, if we compare them at any instant when they have both converged equally well along the top eigendirections. In other words, while the teacher already has an implicit tendency to converge faster along the top eigendirections, the student has an even stronger tendency to do so. In the next section, we demonstrate that this insight generalizes to more practical non-linear settings.

**Connection to prior theory.** As discussed earlier, we build on Mobahi et al.  who prove that distillation exaggerates the _explicit regularization_ applied in a _non-GD_ setting. However, it was an open question as to whether their insight is relevant to GD-trained models used in practice, which we answer in the affirmative. We further directly establish its relevance to practice through an empirical demonstration of our insights in more general neural network settings in SS4.1. Also note that linear models were studied by Phuong and Lampert  too, but they do not show how the student learns any different weights than the teacher, let alone _better_ weights than the teacher.

Our result also brings out an important clarification regarding early-stopping. Mobahi et al.  argue that early-stopping and distillation have opposite regularization effects, wherein the former has a densifying effect while the latter a sparsifying effect. However, this holds only in their non-GD setting. In GD settings, we argue that distillation amplifies the effect of early stopping, rather than oppose it.

### Empirical verification of exaggerated bias in more general settings

While our theory applies to linear regression with gradient _flow_, we now verify our insights in more general settings. In short, we consider settings with (a) finite learning rate instead of infinitesimal, (b) cross-entropy instead of squared error loss, (c) an MLP (in SSD, we consider a CNN), (d) trained on a non-synthetic dataset (MNIST).

To examine how the weights evolve in the eigenspace, we project the first layer weights \(\) onto each eigenvector of the data \(\) to compute the component \(\|^{}\|_{2}\). We then sample two eigenvectors \(_{i},_{j}\) at random (with \(i<j\)), and plot how the quantities \((\|^{}_{i}\|_{2},\|^{}_{j}\|_{ 2})\) evolve over time. This provides a 2-D glimpse into a complex high-dimensional trajectory of the model.

We provide two such random 2-D slices of the trajectory for our MLP in Fig 0(b), and many more such random slices in SSD (not cherry-picked). Across almost all of these slices, we find a consistent pattern emerge. First, as is expected, the one-hot trained teacher shows an implicit bias towards converging to its final value faster along the top direction, which is plotted along the \(X\) axis. But crucially, across almost all these slices, the distilled student presents a more exaggerated version of this bias. This leads it to traverse _a different_ part of the parameter space with greater reliance on the top directions. Notably, we also find underfitting of low-confidence points in this setting, visualized in SSD.

## 5 Reconciling student-teacher deviations and generalization

So far, we have established two forms of exaggeration under distillation, one of confidence (SS3), and one of convergence in the eigenspace (SS4). Next, via a series of experiments, we tie these together to resolve our core paradox: that the student-teacher deviations somehow co-occur with the improved performance of the student. First, we design an experiment demonstrating how the exaggerated bias from SS4 translates into exaggerated confidence levels seen in the probability deviation plots of SS3. Then, via some controlled experiments, we argue when and how this bias can also simultaneously lead to improved student performance. Thus, our overall argument is that both the deviations and the improved performance _can_ co-occur thanks to a common factor, the exaggeration in bias. We summarize this narrative as a graph in Fig 3.

### Connecting exaggerated bias to deviations and generalization

**Exaggerated implicit bias results in exaggerated confidence.** We frame our argument in an empirical setting where a portion of the CIFAR100 one-hot labels are mislabeled. In this context, it is well-known that the implicit bias of early-stopped GD fits the noisy subset of the data more slowly than the clean data; this is because noisy labels correspond to bottom eigendirections . Even prior works on distillation  have argued that when observed labels are noisy, the teacher's implicit bias helps denoise the labels. As a first step of our argument, we corroborate this in Fig 4, where we indeed find that mislabeled points have the low teacher confidence (small \(X\) axis values).

Going beyond this prior understanding, we make a crucial second step in our argument: our theory would predict that the student must rely _even less_ on the lower eigendirections than the teacher already does. This means an even poorer fit of the mislabeled datapoints than the teacher. Indeed, in Fig 4, we find that of all the points that the teacher has low confidence on (i.e., points with small \(X\) values) -- which includes some clean data as well -- the student underfits all the _mis_labeled data (i.e., \(Y<X\) in the plots for those points). This confirms our hypothesis that the exaggeration of the implicit bias in the eigenspace translates to an exaggeration of the confidence levels, and thus a deviation in probabilities.

**Exaggerated implicit bias can result in student outperforming the teacher.** The same experiment above also gives us a handle on understanding how distillation benefits generalization. Specifically, in Table 2, we find that in this setting, the self-distilled ResNet56 model witnesses a \(3\%\) gain over an identical teacher. Prior works  argue, this is because the implicit bias of the teacher results in teacher probabilities that are partially denoised compared to the one hot labels. This alone however, cannot explain why the student -- which is supposedly trying to replicate the teacher's probabilities -- can _outperform_ the teacher. Our solution to this is to recognize that the student doesn't replicate the teacher's bias towards top eigenvectors, but rather exaggerates it. This provides an enhanced denoising which is crucial to outperforming the teacher. The exaggerated bias not only helps generalization, but as discussed in the previous paragraph, it also induces deviations in probability. This provides a reconciliation between the two seemingly inconsistent behaviors.

Figure 3: **Reconciling the paradox:** Distillation exaggerates the implicit bias of GD, which can both exaggerate confidence levels (thus causing deviations in probability) and help generalization. Note that the improved generalization is however conditioned on other confounding factors such as the teacher’s training accuracy, as we discuss later in § 5.2.

### When distillation can hurt generalization

We emphasize an important nuance to this discussion: regularization can also hurt generalization, if other confounding factors (e.g., dataset complexity) are unfavorable. Below, we discuss a key such confounding factor relevant to our experiments.

**Teacher's top-1 train accuracy as a confounding factor.** A well-known example of where distillation hurts generalization is that of ImageNet, as demonstrated in Fig. 3 of Cho and Hariharan . We corroborate this in our setup in Table 2. At the same time, in our experiments on ImageNet, we find that distillation does exaggerate the confidence levels (Fig 7), implying that regularization is at play. A possible explanation for why the student suffers here could be that it has inadequate capacity to match the rich non-target probabilities of the teacher . However, this cannot justify why even self-distillation is detrimental in ImageNet (e.g., [7, Table 3] for ResNet18 self-distillation).

We advocate for an alternative hypothesis, in line with : _distillation can hurt the student when the teacher does not achieve sufficient top-1 accuracy on the training data._ e.g., ResNet18 has 78% ImageNet train accuracy. This hypothesis may appear to contradict the observation from  that the student's accuracy is hurt by much larger teachers, which have better training accuracy. However, in the presence of a larger teacher, there are two confounding factors: the teacher's train accuracy and the complexity of the teacher's non-target probabilities. This makes it hard to disentangle the individual effect of the two factors, which we claim, have opposite effects on the student's performance.

To isolate the effect of the teacher's top-1 training accuracy, we focus on the self-distillation setting. In this setting, we provide three arguments supporting the hypothesis that the teacher's imperfect training accuracy can hurt the student.

**Evidence 1: A controlled experiment with an interpolating and a non-interpolating teacher.** First, we train two ResNet56 teachers on CIFAR100, one which interpolates on the whole dataset (i.e., \(100\%\) top-1 accuracy), and another which interpolates on only half the dataset. Upon distilling a ResNet56 student on the _whole_ dataset in both settings, we find in Fig 3(b) that distilling from the

Figure 4: **Left: Exaggeration of confidence under explicit label noise: While the teacher already achieves low confidence on points with wrong one-hot labels, the student achieves even lower confidence on these points, in both self- (top) and cross-architecture (bottom) distillation. Right: Effect of teacher’s interpolation level in CIFAR-100: For an interpolating teacher (left), switching to one-hot loss in the middle of training _hurts_ generalization, while for a non-interpolating teacher, the switch to one-hot is helpful.**interpolating teacher helps, while distilling from the non-interpolating teacher hurts. This provides direct evidence for our argument.

**Evidence 2: Switching to one-hot loss _helps_ under a _non_-interpolating teacher.** For a non-interpolating teacher, distillation must provide rich top-K information while one-hot must provide precise top-1 information. Thus, our hypothesis would predict that for a non-interpolating teacher, there must be a way to optimally train the student with both distillation and one-hot losses. Indeed  already demonstrate that making some sort of soft switch from distillation to one-hot loss over the course of training, improves generalization for ImageNet. Although  motivate this from their capacity mismatch hypothesis, they report that this technique works for self-distillation on ImageNet as well (e.g., [7, Table 3]), thus validating our hypothesis. We additionally verify these findings indeed hold in some of our self-distillation settings, namely the (controlled) non-interpolating CIFAR100 teacher (Fig 3(b)), and a (naturally) non-interpolating TinyImagenet teacher (SSE), where the capacity mismatch argument does not apply.

**Evidence 3: Switching to one-hot loss _hurts_ under an interpolating teacher.** Our hypothesis would predict that a switch from distillation to one-hot loss, would not be helpful if the teacher already has perfect top-1 accuracy. We verify this with a interpolating CIFAR100 teacher (Fig 3(b), Fig 3(d)). Presumably, one-hot labels provide strictly less information in this case, and causes the network to overfit to the less informative signals. This further reinforces the hypothesis that the teacher's top-1 training accuracy is an important factor in determining whether the exaggerated bias effect of distillation helps generalization.

Framed within the terms of our eigenspace analysis, when the teacher has imperfect top-1 training accuracy, it may mean that the teacher has not sufficiently converged along some critical (say, second or third) eigendirections of the data. The bias exaggerated by distillation would only further curtail the convergence along these directions, hurting generalization.

In summary, this discussion leads us to a more nuanced resolution to the apparent paradox of student-teacher deviations co-occuring with improved generalization. On the one hand, distillation causes an exaggeration of the confidence levels, which causes a deviation between student and teacher probabilities. At the same time, the same effect can aid the student's generalization, _provided other confounding factors are conducive for it._

## 6 Relation to Existing Work

**Distillation as a probability matching process.** Distillation has been touted to be a process that benefits from matching the teacher's probabilities . Indeed, many distillation algorithms have been designed in a way to more aggressively match the student and teacher functions . Theoretical analyses too rely on explaining the benefits of distillation based on a student that obediently matches the teacher's probabilities . But, building on Stanton et al. , our work demonstrates why we may desire that the student deviate from the teacher, in certain systematic ways.

**Theories of distillation.** A long-standing intuition for why distillation helps is that the teacher's probabilities contain "dark knowledge" about class similarities , Several works  have formalized these similarities via inherently noisy class memberships. However, some works  have argued that this hypothesis cannot be the sole explanation, because distillation can help even if the student is only taught information about the target probabilities (e.g., by smoothing out all non-target probabilities).

This has resulted in various alternative hypotheses. Some have proposed faster convergence  which only explains why the student would converge fast to the teacher, but not why it may deviate from and supersede a one-hot teacher. Another line of work casts distillation as a regularizer, either in the sense of Mobahi et al.  or in the sense of instance-specific label smoothing . Another hypothesis is that distillation induces better feature learning or conditioning , likely in the early parts of training. This effect however is not general enough to appear in convex linear settings, where distillation can help. Furthermore, it is unclear if this is relevant in the CIFAR100 setting, where we find that switching to KD much later during training is sufficient to see gains in distillation (SSE). Orthogonally,  suggest that distillation results in flatter minima, which may lead to better generalization. Finally, we also refer the reader to  who theoretically study distillation in orthogonal settings.

Early-stopping and knowledge distillation.Early-stopping has received much attention in the context of distillation . We build on Dong et al. , who argue how early-stopping a GD-trained teacher can automatically denoise the labels due to regularization in the eigenspace. However, these works do not provide an argument for why distillation can outperform the teacher.

**Empirical studies of distillation.** Our study crucially builds on observations from  demonstrating student-teacher deviations in an aggregated sense than in a sample-wise sense. Other studies  investigate how the student is _similar_ to the teacher in terms of out-of-distribution behavior, calibration, and so on. Deng and Zhang  show how a smaller student can outperform the teacher when allowed to match the teacher on more data, which is orthogonal to our setting.

## 7 Discussion and Future Work

Here, we highlight the key insights from our work valuable for future research in distillation practice:

1. _Not matching the teacher probabilities exactly can be a good thing, if done carefully_. Perhaps encouraging underfitting of teachers' low-confidence points can further exaggerate the benefits of the regularization effect.
2. _It may help to switch to one-hot loss in the middle of training if the teacher does not sufficiently interpolate the ground truth labels._

We also highlight a few theoretical directions for future work. First, it would be valuable to extend our eigenspace view to multi-layered models where the eigenspace regularization effect may "compound" across layers. Furthermore, one could explore ways to exaggerate the regularization effect in our simple linear setting and then extend the idea to a more general distillation approach. Finally, it would be practically useful to extend these insights to other modes of distillation, such as _semi-supervised_ distillation , non-classification settings such as ranking models , or intermediate-layer-based distillation .

We highlight the limitations of our study in Appendix A.