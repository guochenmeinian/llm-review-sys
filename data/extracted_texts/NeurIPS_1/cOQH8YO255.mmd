# The Curious Price of Distributional Robustness

in Reinforcement Learning with a Generative Model

Laixi Shi

Department of Computing Mathematical Sciences, California Institute of Technology, CA, USA.

Gen Li

Department of Statistics, The Chinese University of Hong Kong, Hong Kong, China.

Yuting Wei

Department of Statistics and Data Science, Wharton School, University of Pennsylvania, PA, USA.

Yuxin Chen

Geogle Research, Brain Team, Paris, France.

Matthieu Geist

Geogle Research, Brain Team, Paris, France.

Yuejie Chi

Department of Electrical and Computer Engineering, Carnegie Mellon University, PA, USA.

###### Abstract

This paper investigates model robustness in reinforcement learning (RL) via the framework of distributionally robust Markov decision processes (RMDPs). Despite recent efforts, the sample complexity of RMDPs is much less understood regardless of the uncertainty set in use; in particular, there exist large gaps between existing upper and lower bounds, and it is unclear if distributional robustness bears any statistical implications when benchmarked against standard RL. In this paper, assuming access to a generative model, we derive the sample complexity of RMDPs -- when the uncertainty set is measured via either total variation or \(^{2}\) divergence over the full range of uncertainty levels -- using a model-based algorithm called distributionally robust value iteration, and develop minimax lower bounds to benchmark its tightness. Our results not only strengthen the prior art in both directions of upper and lower bounds, but also deliver surprising messages that learning RMDPs is not necessarily easier or more difficult than standard MDPs. In the case of total variation, we establish the minimax-optimal sample complexity of RMDPs which is always smaller than that of standard MDPs. In the case of \(^{2}\) divergence, we establish the sample complexity of RMDPs that is tight up to polynomial factors of the effective horizon, and grows linearly with respect to the uncertainty level when it approaches infinity.

## 1 Introduction

Reinforcement learning (RL) deals with the problem of learning to make sequential decisions based on trial-and-error interactions with some unknown environment. As a fast-growing subfield of artificial intelligence, it has achieved significant success in a variety of domains such as large language model alignment (OpenAI, 2023; Ziegler et al., 2019), healthcare (Liu et al., 2019; Fatemi et al., 2021), robotics and control (Kober et al., 2013; Mnih et al., 2013). Due to the high dimensionality of the state-action space, achieving sample efficiency lies at the core of modern RL practice, especially in various sample-starved applications. As a result, a large portion of efforts in RL has been put in designing sample-efficient algorithms and understanding the fundamental statistical difficulty for diverse RL problems (Azar et al., 2013; Li et al., 2020).

While standard RL has been heavily invested recently, its use can be significantly hampered in practice due to the sim-to-real gap, where an agent trained in an ideal, nominal environment might be extremely sensitive and fail catastrophically when the deployed environment is subject to small changes in task objectives or unexpected perturbations (Zhang et al., 2020; Klopp et al., 2017; Mahmood et al., 2018). Consequently, in addition to maximizing the long-term cumulative reward,robustness becomes another critical goal for an RL agent, especially in high-stake applications such as robotics, autonomous driving, clinical trials, financial investments, and so on. To address this, distributionally robust RL (Iyengar, 2005; Nilim and El Ghaoui, 2005), which leverages insights from distributionally robust optimization and supervised learning (Rahimian and Mehrotra, 2019; Gao, 2020; Bertsimas et al., 2018; Duchi and Namkoong, 2018; Blanchet and Murthy, 2019), becomes a natural and versatile framework with the goal of learning a policy that performs well even when the deployed environment deviates from the nominal one in the face of environment uncertainty.

In this paper, we are particularly interested in understanding whether, and how, the choice of distributional robustness bears statistical implications in learning the desired policy, by studying the sample complexity in the widely-used generative model (Kearns and Singh, 1999). Suppose that one has access to a generative model which draws samples from a Markov decision processes (MDP) with a nominal transition kernel. Standard RL aims to learn the optimal policy tailored for the nominal kernel based on this set of samples, where the sample complexity has been well understood with matching upper and lower bounds developed recently (Azar et al., 2013; Li et al., 2020). In contrast, distributionally robust RL -- leveraging the same set of samples -- aims to learn the optimal _robust_ policy whose worst-case performance is maximized when the transition kernel is from some _prescribed_ uncertainty set around the nominal kernel, a setting that is referred to as the robust MDP (RMDPs).6 Clearly, this ensures that the performance of the learned policy is robust and does not fail catastrophically as long as the sim-to-real gap is not too large. It is then natural to wonder how the robustness consideration impacts the RL performance: should we always prefer to learn a robust policy for a given set of samples? Is there a statistical premium when asking for additional robustness?

Compared with standard MDPs, RMDPs is a richer class of models since one additionally needs to prescribe the shape and size of the uncertainty set, which is usually hand-picked as a small ball around the nominal kernel measured with respect to some distance measure \(\) and uncertainty level \(\). To ensure the tractability of solving RMDPs, the uncertainty set is usually assumed to obey certain structures, where the uncertainty set can be decomposed as a product of independent uncertainty subsets over each state or state-action pair (Zhou et al., 2021; Wiesemann et al., 2013), denoted as the \(s\)- and \((s,a)\)-rectangular rectangular respectively; in particular, our paper adopts the second choice by assuming the uncertainty set satisfies the \((s,a)\)-rectangularity. An additional challenge with RMDPs arises from distribution shift, where the transition kernel drawn from the uncertainty set can be different from the nominal kernel, leading to complicated nonlinearity and nested optimization in the problem structure not present in standard MDPs.

### Prior art and open questions

In this paper, we focus on understanding the sample complexity of learning the optimal robust policy of RMDPs in the infinite-horizon setting assuming access to a generative model, when the uncertainty set is measured using one of the \(f\)-divergence: total variation (TV) distance and \(^{2}\) divergence. These two choices are motivated by their practical appeal: easy to implement, and already adopted by empirical RL (Lee et al., 2021; Pan et al., 2023).

A popular learning approach is model-based, which first estimates the nominal transition kernel using a plug-in estimator based on the collected samples, and then runs a planning algorithm such as a robust variant of value iteration on top of the estimated RMDP. Despite the surge of recent activities, however, existing statistical guarantees for the above paradigm remain highly inadequate, as we shall elaborate momentarily (see Table 1 and Table 2 respectively for a summary). For concreteness, let \(S\) be the size of the state space, \(A\) be the size of the action space, \(\) be the discount factor (and the effective horizon \(\)), and \(\) be the uncertainty level. We are interested in the sample complexity -- the number of samples needed for an algorithm to output a policy whose robust value function (the worst-case value over all the transition kernels in the uncertainty set) is at most \(\) away from the optimal robust one -- with respect to all salient problem parameters.

\(\)_Large gaps between existing upper and lower bounds._ There remained large gaps between the sample complexity upper and lower bounds established in prior literature, regardless of the divergence metric in use. Specifically, considering the cases using either TV distance or \(^{2}\) divergence, the state-of-the-art upper bounds (Panaganti and Kalathil, 2022) scales quadratically with the size \(S\) of the state space, while the lower bound (Yang et al., 2022) exhibits only linear scaling with \(S\). Moreover, in the \(^{2}\) divergence case, the state-of-the-art upper bound grows linearly with the uncertainty level \(\) when \( 1\),7 while the lower bound (Yang et al., 2022) is inversely proportional to \(\). These lead to unbounded gaps between the upper and lower bounds as \(\) grows. _Can we hope to close these gaps for RMDPs?_

\(\)_Benchmarking with standard MDPs._ Perhaps a more pressing issue is that, past works failed to provide an affirmative answer regarding how to benchmark the sample complexity of RMDPs with that of standard MDPs regardless of the chosen shape (determined by \(\)) or size (determined by \(\)) of the uncertainty set, given the large unresolved gaps mentioned above. Specifically, existing sample complexity upper (resp. lower) bounds are all larger (resp. smaller) than the sample size requirement for standard MDPs. As a consequence, it remains mostly unclear _whether learning RMDPs is harder or easier than learning standard MDPs.

### Main contributions

To address these questions, we have developed new upper bounds on learning RMDPs with TV distance and \(^{2}\) distance in the infinite-horizon setting using the model-based approach called distributionally robust value iteration (DRVI), as well as minimax lower bounds to help gauge their

    &  &  \\   & & \(0< 1-\) & \(1-<1\) \\   & Yang et al. (2022) & \(A(2+)^{2}}{^{2}(1-)^{4}^{2}}\) \\   & Panaganti and Kalathil (2022) & \(A}{(1-)^{2}^{2}}\) \\   & **Ours** & \(^{2}}\) & \(^{2}}\) \\   & Yang et al. (2022) & \(^{2}}\) & \(^{2}}\) \\   & **Ours** & \(^{2}}\) & \(^{2}}\) \\   

Table 1: Comparisons between our results and prior arts for finding an \(\)-optimal robust policy in the infinite-horizon RMDPs with an uncertainty set measured with respect to the TV distance, where we ignore logarithmic factors in the sample complexities. Here, \(S\), \(A\), \(\), and \((0,1)\) are the state space size, the action space size, the discount factor, and the uncertainty level, respectively.

Figure 1: Illustrations of the obtained sample complexity upper and lower bounds for learning RMDPs with comparisons to state-of-the-art and the sample complexity of standard MDPs, where the uncertainty set is specified using the TV distance (a) and the \(^{2}\) distance (b).

tightness and provide benchmarking with standard MDPs. As shall be outlined, these new analyses lead to new insights on the interplay between the geometry of uncertainty sets and the statistical sample complexity.

**Sample complexity of RMDPs under the TV distance.** We summarize the results and comparisons to prior works in Table 1; see Figure 1(a) for an illustration.

\(\)_Minimax-optimal sample complexity._ We prove that DRVI reaches \(\) accuracy as soon as the sample complexity is on the order of

\[(^{2}}\{ {1-},\})\]

for all \((0,1)\), assuming that \(\) is small enough. In addition, a matching minimax lower bound (modulo some logarithmic factor) is established to guarantee the tightness of the upper bound. To the best of our knowledge, this is the _first_ minimax-optimal sample complexity for RMDPs, which was previously unavailable regardless of the divergence metric and uncertainty level in use and is over the full range of the uncertainty level.

\(\)_RMDPs are easier than standard MDPs under the TV distance._ Given the sample complexity \(O(^{2}})\) of standard MDPs, it can be seen that RMDPs under the TV distance is never harder than standard MDPs, where it matches that of standard MDPs when \( 1-\), and becomes simpler by a factor of \(/(1-)\) when \(1-<1\). Therefore, in this case, the robustness comes almost for free since we do not need to collect more samples to reach the same accuracy.

**Sample complexity of RMDPs under the \(^{2}\) distance.** We summarize the results and comparisons to prior works in Table 2; see Figure 1(b) for an illustration.

\(\)_Near-optimal sample complexity._ It is established that DRVI reaches \(\) accuracy as soon as the sample complexity is on the order of \((^{2}})\) for all \((0,)\), which is the first sample complexity that scales linearly with respect to the size of the state space \(S\) in the infinite-horizon setting, breaking the quadratic scaling bottleneck presented in prior works (Panaganti and Kalathil, 2022; Yang et al., 2022). We have also developed a strengthened lower bound that is optimized by leveraging the geometry of the uncertainty set under different ranges of \(\). Comparing the two bounds, they match at \((^{2}})\) when \( 1\), and have a bounded gap no larger than a polynomial factor of the effective horizon \(1/(1-)\) over the entire range of the uncertainty level, again significantly improving prior art that exhibits an unbounded gap.

\(\)_RMDPs can be harder than standard MDPs under the \(^{2}\) distance._ Somewhat surprisingly, the new lower bound suggests that RMDPs in this case can be much harder than standard MDPs, at least for certain ranges of the uncertainty level. We single out two regimes of particular interest. First, when \( 1\), the sample size requirement of RMDPs is \((^{2}})\), which is provably harder than

   &  &  \\   & & \(0< 1-\) & \(1-\) & \(\) \\   & Panaganti and Kalathil (2022) & A(1+)}{(1-)^{4}^{2}}\)} \\   & Yang et al. (2022) & A(1+)^{2}}{()^{2}(1-)^{4} ^{2}}\)} \\   & **Ours** & ^{2}}\)} \\   & Yang et al. (2022) & \(^{2}}\) & \(^{2}}\) \\   & **Ours** & \(^{2}}\) & \((1+)^{4}^{2}}\) & \(}\) \\  

Table 2: Comparisons between our results and prior arts for finding an \(\)-optimal robust policy in the infinite-horizon RMDPs with an uncertainty set measured with respect to the \(^{2}\) distance, where we ignore logarithmic factors in the sample complexities. Here, \(S\), \(A\), \(\), and \((0,)\) are the state space size, the action space size, the discount factor, and the uncertainty level, respectively.

standard MDPs by a factor of the effective horizon \(\). Second, when \(}\), the lower bound exceeds the sample complexity of standard MDPs and keeps growing linearly with the uncertainty level \(\).

In sum, our sample complexity bounds not only strengthen the prior art in both directions of upper and lower bounds, but also reveal new insights on how the additional consideration of distributional robustness fundamentally changes the sample complexity of RL in a surprising manner. It turns out that RMDPs are not necessarily harder nor easier than standard MDPs, but the answer is far more nuanced and highly dependent on both the size and shape of the uncertainty set, which are not elucidated in prior analyses.

## 2 Problem formulation

In this section, we introduce the model of distributionally robust Markov decision processes (RMDPs) focusing on the discounted infinite-horizon setting, the sampling mechanism, as well as our goal.

**Standard MDPs.** To begin, we first introduce the standard Markov decision process (MDP), which facilitates the understanding of RMDPs. A discounted infinite-horizon MDP is represented by \(=,,P,,r\), where \(=\{1,,S\}\) and \(=\{1,,A\}\) are the finite state and action spaces, respectively, \([0,1)\) is the discount factor, \(P:()\) denotes the probability transition kernel, and \(r:\) is the immediate reward function which is assumed to be deterministic. A policy is denoted as \(:()\), which specifies the action selection probability over the action space in any state. When the policy is deterministic, we overload the notation and refer to \((s)\) as the action selected by policy \(\) in state \(s\). To characterize the long term cumulative reward, the value function \(V^{,P}\) for any policy \(\) under the transition kernel \(P\) is defined by

\[ s: V^{,P}(s)_{ ,P}[_{t=0}^{}^{t}rs_{t},a_{t}\,\, s_{0}=s], \]

where the expectation is taken over the randomness of the trajectory \(\{s_{t},a_{t},r_{t}\}_{t=0}^{}\) generated by executing policy \(\) under the transition kernel \(P\), namely, \(a_{t}(s_{t})\), and \(s_{t+1} P(\,|\,s_{t},a_{t})\). Similarly, the Q-function \(Q^{,P}\) associated with any policy \(\) under the transition kernel \(P\) is defined as

\[(s,a): Q^{,P}(s,a) _{,P}[_{t=0}^{}^{t}rs_{t},a _{t}\,\,s_{0}=s,a_{0}=a], \]

where the expectation is again taken over the randomness of the trajectory.

**Distributionally robust MDPs.** In this work, we focus on the discounted infinite-horizon distributionally robust MDP (RMDP), denoted as \(_{}=\{,,_{}^{ }(P^{0}),,r\}\), where \(,,,r\) are defined the same as those in the above standard MDP. A key distinction from the standard MDP, is that rather than assuming a fixed transition kernel \(P\), it postulates the transition kernel lies in an uncertainty set \(_{}^{}(P^{0})\) centered around a _nominal_ kernel \(P^{0}:()\), where the uncertainty set is specified using some distance metric \(\) of radius \(>0\). In particular, given the nominal transition kernel \(P^{0}\) and some uncertainty level \(\), the uncertainty set--with divergence \(:()()^{+}\)--is specified as

\[_{}^{}(P^{0})_ {}^{}(P_{s,a}^{0}),_{}^{}(P^{0}) \{P_{s,a}():(P_{s,a},P_{s,a}^{0}) \}, \]

where we denote a vector of the transition kernel \(P\) or \(P^{0}\) at state-action pair \((s,a)\) respectively as

\[P_{s,a} P(\,|\,s,a)^{1 S}, P _{s,a}^{0} P^{0}(\,|\,s,a)^{1 S}. \]

In other words, the uncertainty is imposed in a separate manner for each state-action pair, obeying the so-called \((s,a)\)-rectangularity (Zhou et al., 2021; Wiesemann et al., 2013). In RMDPs, we are interested in the worst-case performance of a policy \(\) over all the possible transition kernels in the uncertainty set. This is measured by the _robust value function_\(V^{,}\) and the _robust Q-function_\(Q^{,}\) in \(_{}\), defined respectively as

\[(s,a): V^{, }(s)_{P_{}^{}(P^{0})}V^{,P}(s), Q ^{,}(s,a)_{P_{}^{}(P^{0})}Q^{ ,P}(s,a).\]

**Optimal robust policy and robust Bellman operator.** Generalizing standard MDPs, it is well-known that there exists at least one deterministic policy that maximizes the robust value function and the robust Q-function simultaneously (Iyengar, 2005; Nilim and El Ghaoui, 2005). Therefore, we denote the _optimal robust value function_ (resp. _optimal robust Q-function_) as \(V^{,}\) (resp. \(Q^{,}\)), and the optimal robust policy as \(^{}\), which satisfies

\[ s: V^{,}(s) V^{^{}, }(s)=_{}V^{,}(s), \] \[(s,a): Q^{,}(s,a)  Q^{^{},}(s,a)=_{}Q^{,}(s,a). \]

The robust Bellman operator (Iyengar, 2005; Nilim and El Ghaoui, 2005) is denoted as \(^{}():^{SA}^{SA}\), which is defined as follows: for all \((s,a)\),

\[^{}(Q)(s,a) r(s,a)+_{ ^{}_{}(P^{0}_{s,a})}V, V (s)_{a}Q(s,a). \]

Given that \(Q^{,}\) is the unique fixed point of \(^{}\), one can recover the optimal robust value function and Q-function using a procedure termed _distributionally robust value iteration_--which generalizes the standard value iteration--by recursively applying the robust Bellman operator from some fixed initialization. In addition, this procedure converges rather fast due to the nice \(\)-contraction property of \(^{}\)(Iyengar, 2005; Nilim and El Ghaoui, 2005) with respect to the \(_{}\) norm.

**Specification of the divergence \(\).** We consider two popular choices of the uncertainty set measured in terms of the \(f\)-divergence: total variation and \(^{2}\) divergence, given respectively by

\[_{}(P_{s,a},P^{0}_{s,a})  P_{s,a}-P^{0}_{s,a}_{ 1}=_{s^{}}P^{0}(s^{}|\,s,a) |1-|\,s,a)}{P^{0}(s^{}|\,s,a).} |., \] \[_{^{2}}(P_{s,a},P^{0}_{s,a}) _{s^{}}P^{0}(s^{}|\,s,a)(1-|\,s,a)}{P^{0}(s^{}|\,s,a). })^{2}.. \]

Note that \(_{}(P_{s,a},P^{0}_{s,a})\) and \(_{^{2}}(P_{s,a},P^{0}_{s,a})[0,)\) in general. As we shall see, the two choices convey drastically different messages in the statistical complexity of RMDPs.

**Sampling mechanism: a generative model.** Following Zhou et al. (2021); Panaganti and Kalathil (2022), we assume the access to a generative model or a simulator (Kearns and Singh, 1999), which allows us to collect \(N\) independent samples from the _nominal_ kernel \(P^{0}\) at each state-action pair:

\[(s,a), s_{i,s,a}P^{0}(|\,s,a), i=1,2,,N. \]

The total sample size therefore is \(NSA\).

**Goal.** Given the collected samples, the task is to learn the robust optimal policy for the RMDP with some uncertainty set \(^{}_{}(P^{0})\) around the nominal kernel accurately using as few samples as possible. Specifically, given some accuracy level \(>0\), the goal is to seek an \(\)-optimal robust policy \(\) obeying \(V^{,}(s)-V^{,}(s)\) for all \(s\).

## 3 Model-based algorithm: distributionally robust value iteration

We consider a model-based strategy, which first constructs an empirical nominal transition kernel based on the collected samples, and then applies distributionally robust value iteration (DRVI) to recover the optimal robust policy.

**Empirical nominal kernel.** The empirical nominal transition kernel \(^{0}^{SA S}\) can be constructed using the empirical frequency of visits, i.e.

\[(s,a):^{0}(s^{} |\,s,a)_{i=1}^{N}s_{i,s,a}=s^{}}, \]

which leads to an empirical RMDP \(}_{}=\{,,^{ }_{}(^{0}),,r\}\). Analogously, we can define the corresponding robust value function (resp. robust Q-function) of policy \(\) in \(}_{}\) as \(^{,}\) (resp. \(^{,}\)) (cf. (5)). In addition, we denote the corresponding _optimal robust policy_ as \(^{}\) and the _optimal robust value function_ (resp. _optimal robust Q-function_) as \(^{,}\) (resp. \(^{,}\)) (cf. (5)), which satisfies the robust Bellman optimality equation:

\[(s,a):^{,}(s,a)=r(s,a)+_{^{}_{}( ^{0}_{s,a})}^{,}. \]Equipped with \(^{0}\), define the empirical robust Bellman operator \(}^{}\) as

\[(s,a) :\ }^{}(Q)(s,a)  r(s,a)+_{_{}^{}( {P}^{0}_{,a})}V,V(s)_{a}Q(s,a). \]

DRVI: distributionally robust value iteration.To solve for the fixed point of \(}^{}\), we introduce distributionally robust value iteration (DRVI), which is summarized in Algorithm 1. Starting from some initialization \(_{0}=0\), the update rule at the \(t\)-th (\(t 1\)) step can be formulated as:

\[(s,a) :_{t}(s,a)=}^{}(_{t-1})(s,a)=r(s,a)+_{ _{}^{}(^{0}_{,a})} _{t-1}, \]

where \(_{t-1}(s)=_{a}_{t-1}(s,a)\) for all \(s\). However, directly solving (13) is computationally prohibitive since it involves optimization over an \(S\)-dimensional probability simplex at each iteration, especially when the dimension of the state space \(\) is prohibitive. Fortunately, in view of strong duality (Iyengar, 2005), (13) can be equivalently solved using its dual problem, which concerns optimizing of a _scalar_ dual variable and thus can be solved efficiently. The specific form of the dual problem depends on the choice of the divergence \(\), which we discuss in a more detailed version.

## 4 Theoretical guarantees: sample complexity analyses

We now present our main results, which concern the sample complexities of learning RMDPs when the uncertainty set is specified using the TV distance or the \(^{2}\) divergence. Surprisingly, the choice of the uncertainty set can lead to dramatic consequence in the sample size requirement.

### The case of TV distance: RMDP is easier than standard MDP

We start with the case when the uncertainty set is measured via the TV distance, where Theorem 1 presents the sample complexity upper bound above which DRVI is able to find an \(\)-optimal robust policy in a small number of iterations; the key challenge of the analysis is to carefully control the robust value function \(V^{,}\) as a function of uncertainty level \(\).

**Theorem 1** (Upper bound using TV distance).: _Fix the uncertainty set \(_{}^{}()=_{}^{}()\) using the TV distance in (7). Consider any discount factor \([,1)\), uncertainty level \((0,1)\), and \((0,1)\). With probability at least \(1-\), the output \(\) from Algorithm 1 with at most \(T=C_{1}(N(1-))\) iterations yields \(V^{*,}(s)-V^{,}(s)\) for any \((0,}]\), as long as the total number of samples obeys \(NSASA}{(1-)^{2}\{1-,\}^{2}} ()\). Here, \(C_{1},C_{2}>0\) are some large enough universal constants._

Before discussing the implications of Theorem 1, we present a matching minimax lower bound that confirms the optimality of the upper bound, which in turn pins down the sample complexity requirement for learning RMDPs with TV distance.

**Theorem 2** (Lower bound using TV distance).: _Consider any tuple \((S,A,,,)\) obeying \((0,1-c_{0}]\) with \(0<c_{0}\) being any small enough positive constant, \([,1)\), and \((0,}{256(1-)}]\). Wecan construct two infinite-horizon RMDPs \(_{0},_{1}\) defined by the uncertainty set \(^{}_{}()=^{}_{}()\), an initial state distribution \(\), and a dataset with \(N\) independent samples for each state-action pair over the nominal transition kernel (for \(_{0}\) and \(_{1}\) respectively), such that_

\[_{}\{_{0}V^{,}( )-V^{,}()>,\,_{1} V^{,}()-V^{,}()> \},\]

_provided that \(NSAS\,A 2}{8192(1-)^{2}\{1-,\} ^{2}}\). The infimum is taken over all estimators \(\), and \(_{0}\) (resp. \(_{1}\)) denotes the probability when the RMDP is \(_{0}\) (resp. \(_{1}\))._

Below, we interpret the above theorems and highlight several key implications about the sample complexity requirements for learning RMDPs with TV distance.

Near minimax-optimal sample complexity.Theorem 1 shows that the total number of samples required for DRVI to yield \(\)-accuracy is

\[(\{1-,\} ^{2}}). \]

Taking together with the minimax lower bound asserted in Theorem 2, this confirms the near minimax-optimality of the sample complexity up to some logarithmic factor almost over the full range of the uncertainty level \(\), which scales linearly with respect to the size of the state-action space.

RMDPs is easier than standard MDPs with TV distance.Recall that the sample complexity requirement for standard MDP (Agarwal et al., 2020; Li et al., 2020) to yield \(\) accuracy is \((\,^{2}})\). Comparing with the sample complexity requirement in (14) for RMDPs with TV distance, this confirms that the latter is at least as easy as--if not easier--than standard MDPs. In particular, when \( 1-\) is small, the sample complexity of RMDPs is the same as the standard MDPs, which is expected since the RMDP reduces to the standard MDP when \(=0\). On the other hand, when \(1-<1\), the sample complexity of RMDPs becomes \((^{2}})\), which is smaller than that of standard MDPs by a factor of \(/(1-)\).

Comparison with state-of-the-art bounds.For the upper bound, our results (cf. Theorem 1) significantly improves over the prior art \((A}{(1-)^{4}^{2}})\) of Panaganti and Kalathil (2022) by at least a factor of \(\) and even \(}\) when the uncertainty level \(1-<1\) is large. Turning to the lower bound side, Yang et al. (2022) developed a lower bound for RMDPs under the TV distance, which scales as \((}\{},}\})\). Clearly, this is worse than ours by a factor of \(}{(1-)^{3}}(1,})\) in the regime where \(1-<1\).

### The case of \(^{2}\) distance: RMDP can be harder than standard MDP

We now move onto the case when the uncertainty set is measured via the \(^{2}\) distance, where Theorem 3 presents the sample complexity upper bound above which DRVI is able to find an \(\)-optimal robust policy in a small number of iterations.

**Theorem 3** (Upper bound using \(^{2}\) distance).: _Fix the uncertainty set \(^{}_{}()=^{}_{^{2}}()\) using the \(^{2}\) distance in (8). Consider any uncertainty level \((0,)\), and \((0,1)\). With probability at least \(1-\), the output policy \(\) from Algorithm 1 with at most \(T=c_{1}(N(1-))\) iterations yields \(V^{,}(s)-V^{,}(s)\) for any \((0,]\), as long as the total number of samples obeying \(NSASA(1+)}{(1-)^{4}^{2}}()\). Here, \(c_{1},c_{2}>0\) are some large enough universal constants._

In addition, in order to gauge the tightness of Theorem 3, and understand the minimal sample complexity requirement for learning RMDPs with \(^{2}\) divergence, we further develop a minimax lower bound as follows.

**Theorem 4** (Lower bound using \(^{2}\) divergence).: _Consider any \((S,A,,,)\) obeying \([,1)\), \((0,)\), and_

\[ c_{3}&(0,),\\ \{,1\}&[,), \]_for some small universal constant \(c_{3}>0\). Then we can construct two infinite-horizon RMDPs \(_{0},_{1}\) defined by the uncertainty set \(_{}^{}()=_{^{2}}^{}()\), an initial state distribution \(\), and a dataset with \(N\) independent samples for each \((s,a)\) pair over the nominal transition kernel (for \(_{0}\) and \(_{1}\) respectively), such that_

\[_{}\{_{0}V^{,}()-V^{ ,}()>,\,_{1}V^{,}()-V^{,}()>\} {1}{8}, \]

_provided that the total number of samples_

\[NSA c_{4}^{2}}& (0,),\\ (1+)^{4}\}^{2}}&[,) \]

_for some universal constant \(c_{4}>0\)._

We are now positioned to highlight some key implications of the above theorems about the sample complexity requirements for learning RMDPs with \(^{2}\) divergence.

**Nearly tight sample complexity.** To achieve \(\)-accuracy for RMDPs with \(^{2}\) distance, Theorem 3 shows that a total number of samples on the order of \((^{2}})\) is sufficient for DRVI. Taking it together with the minimax lower bound in Theorem 4 confirms that the sample complexity is near-optimal up to a polynomial factor of the effective horizon \(1/(1-)\) over the entire range of the uncertainty level \(\). In particular, when \( 1\), our sample complexity \((^{2}})\) is tight and matches with the lower bound; when \(}\), our sample complexity correctly predicts the linear dependency with \(\), suggesting that more samples are needed when one plans for larger \(^{2}\)-based uncertainty sets.

**RMDPs can be much harder than standard MDPs with \(^{2}\) divergence.** The minimax lower bound developed in Theorem 4 exhibits a surprising non-monotonic behavior of the sample size requirement over the entire range of the uncertainty level \((0,)\) when the uncertainty set is measured via the \(^{2}\) divergence. When \( 1-\), the lower bound reduces to \((^{2}}),\) which matches with that of standard MDPs, as \(=0\) corresponds to standard MDP. However, two additional regimes are worth calling out:

\[1-}:( ^{2}}\{, }\}),}:\ (}),\]

both of which are _greater_ than that of standard MDPs, indicating learning RMDPs with \(^{2}\) divergence can be much harder.

**Comparison with state-of-the-art bounds.** Our upper bound significantly improves over the prior art \((A(1+)}{(1-)^{4}^{2}})\) of Panaganti and Kalathil (2022) by a factor of \(S\), and provides the _first_ finite-sample complexity that scales _linearly_ with respect to \(S\) for discounted infinite-horizon RMDPs, which typically exhibit more complicated statistical dependencies than the finite-horizon counterpart. On the other hand, Yang et al. (2022) established a lower bound on the order of \((^{2}})\) when \( 1-\), which is always smaller than the requirement of standard MDPs, and diminishes when \(\) grows. Consequently, Yang et al. (2022) does not lead to the rigorous justification that RMDPs can be harder than standard MDPs, nor the correct linear scaling of the sample size when \(\) grows towards infinity.

## 5 Other related works

Finite-sample guarantees for standard RL.There has been a considerable amount of research into non-asymptotic sample analysis of standard RL for a variety of settings; partial examples include, but are not limited to, the works via probably approximately correct (PAC) bounds for the generative model setting (Kearns and Singh, 1999; Beck and Srikant, 2012; Li et al., 2022; Chen et al., 2020; Azar et al., 2013; Sidford et al., 2018; Agarwal et al., 2020; Li et al., 2023, 2020; Wainwright, 2019) and the offline setting (Rashidinejad et al., 2021; Xie et al., 2021; Yin et al., 2021; Shi et al., 2022; Liet al., 2022; Jin et al., 2021; Yan et al., 2022), as well as the online setting via both regret-based and PAC-base analyses (Jin et al., 2018; Bai et al., 2019; Li et al., 2021; Zhang et al., 2020; Dong et al., 2019; Jin et al., 2020; Li et al., 2022; Jafarnia-Jahromi et al., 2020; Yang et al., 2021; Woo et al., 2023).

Robustness in RL.To address the challenges of deployed environment uncertainty, an emerging line of works begin to address robustness of RL algorithms with respect to the uncertainty or perturbation over different components of MDPs -- state, action, reward, and the transition kernel; see Moos et al. (2022) for a recent review. Besides the framework of distributionally robust MDPs (RMDPs) (Iyengar, 2005) adopted by this work, to promote robustness in RL, there exist various other works including but not limited to Zhang et al. (2020, 2021); Han et al. (2022); Qiaoben et al. (2021); Sun et al. (2021); Xiong et al. (2022) investigating the robustness w.r.t. state uncertainty. Besides, Tessler et al. (2019); Tan et al. (2020) considered the robustness w.r.t. the uncertainty of the action, and Ding et al. (2023) tackles robustness against spurious correlations.

Distributionally robust RL.Rooted in the literature of distributionally robust optimization, which has primarily been investigated in the context of supervised learning (Rahimian and Mehrotra, 2019; Gao, 2020; Bertsimas et al., 2018; Duchi and Namkoong, 2018; Blanchet and Murthy, 2019), distributionally robust dynamic programming and RMDPs have attracted considerable attention recently (Wolff et al., 2012; Kaufman and Schaefer, 2013; Ho et al., 2018; Smirnova et al., 2019; Ho et al., 2021; Goyal and Grand-Clement, 2022; Derman and Mannor, 2020; Tamar et al., 2014). In the context of RMDPs, both empirical and theoretical studies have been widely conducted, although most prior theoretical analyses focus on planning with an exact knowledge of the uncertainty set (Iyengar, 2005; Xu and Mannor, 2012; Tamar et al., 2014), or are asymptotic in nature (Roy et al., 2017).

Resorting to the tools of high-dimensional statistics, various recent works begin to shift attention to understand the finite-sample performance of provable robust RL algorithms, under diverse data generating mechanisms and forms of the uncertainty set over the transition kernel. Besides the infinite-horizon setting, finite-sample complexity bounds for RMDPs with the TV distance and the \(^{2}\) divergence are also developed for the finite-horizon setting in Xu et al. (2023); Dong et al. (2022). In addition, many other forms of uncertainty sets have been considered associated with different divergence function including but not limited to Wasserstein distance, R-contamination, KL divergence, Wang and Zou (2021); Yang et al. (2022); Panaganti and Kalathil (2022); Zhou et al. (2021); Shi and Chi (2022); Xu et al. (2023); Wang et al. (2023a); Blanchet et al. (2023); Liu et al. (2022); Wang et al. (2023c); Liang et al. (2023); Xu et al. (2023); Badrinath and Kalathil (2021); Ramesh et al. (2023); Panaganti et al. (2022); Ma et al. (2022). Moreover, various other related problems or issues have been explored such as the difference of various uncertainty types (Wang et al., 2023b), the iteration complexity of the policy-based methods (Li et al., 2022c; Kumar et al., 2023), the cases when the uncertainty level is instance-dependent small enough (Clavier et al., 2023), and regularization-based robust RL (Yang et al., 2023; Zhang et al., 2023).

## 6 Discussions

This work studies sample complexity bounds for learning RMDPs when the uncertainty set is measured via the TV distance and the \(^{2}\) divergence under the generative model. Our sample complexity bounds not only strengthen the prior art in both directions of upper and lower bounds, but also reveal new insights on how the additional consideration of distributional robustness fundamentally changes the sample complexity of RL in a surprising manner. It turns out that RMDPs are not necessarily harder nor easier than standard MDPs, but the answer is far more nuanced and highly dependent on both the size and shape of the uncertainty set under consideration. These findings could help to guide the practice of RMDPs, by raising awareness that the choice of the uncertainty set not only represents a preference in robustness, but also influences the statistical complexity of the problem.