# PutnamBench: Evaluating Neural Theorem-Provers on the Putnam Mathematical Competition

George Tsoukalas

UT Austin

Jasper Lee

UT Austin

John Jennings

UT Austin

Jimmy Xin

UT Austin

Michelle Ding

UT Austin

Michael Jennings

UT Austin

Amitayush Thakur

UT Austin

Swarat Chaudhuri

UT Austin

###### Abstract

We present PutnamBench, a new multi-language benchmark for evaluating the ability of neural theorem-provers to solve competition mathematics problems. PutnamBench consists of 1692 hand-constructed formalizations of 640 theorems sourced from the William Lowell Putnam Mathematical Competition, the premier undergraduate-level mathematics competition in North America. All the problems have formalizations in Lean 4 and Isabelle; a substantial subset also has Coq formalizations. PutnamBench requires significant problem-solving ability and proficiency in a broad range of topics taught in undergraduate mathematics courses. We use PutnamBench to evaluate several established neural and symbolic theorem-provers. These approaches can only solve a handful of the PutnamBench problems, establishing the benchmark as a difficult open challenge for research on neural theorem-proving. PutnamBench is available at [https://github.com/trishullab/PutnamBench](https://github.com/trishullab/PutnamBench).

## 1 Introduction

Automating mathematical reasoning is a longstanding goal in artificial intelligence (Newell et al., 1957). A prominent line of work on the problem (Li et al., 2024) uses neural models to direct theorem-proving in formal frameworks like Lean 4 (Moura & Ullrich, 2021), Isabelle (Wenzel et al., 2008), and Coq (The Coq Development Team, 2023). These frameworks can "execute" proofs like code and offer execution feedback, which simplifies the search for correct proofs.

The design of quality benchmarks is a key challenge in this research area. The two most prominent competition-based benchmarks for neural theorem-proving are MiniF2F (Zheng et al., 2021) and Fimo(Liu et al., 2023). The former formalizes a mix of problems from high-school level courses and mathematics competitions such as AIME, AMC, and IMO; the latter consists of a collection of IMO problems. Both benchmarks have limitations. For example, MiniF2F contains many problems that can be immediately solved using an SMT solver, and Fimo only targets the Lean 3 framework, which is no longer actively maintained.

More generally, as large language models (LLMs) grow in importance as a tool for neural theorem-proving (Li et al., 2024), preventing leakage between pretraining sets and evaluation sets is more important than ever. This makes the continued supply of new benchmarks an important goal.

In this paper, we respond to this challenge with PutnamBench, a new hand-curated, multi-language benchmark for neural theorem-provers. PutnamBench includes 1692 formalizations of 640 problems from the William Lowell Putnam Mathematical Competition, the premier college-levelmathematics competition in North America.* All our problems have Lean 4 (Moura & Ullrich, 2021) and Isabelle (Wenzel et al., 2008) formalizations; a substantial fraction have formalizations in Coq (The Coq Development Team, 2023) as well. The formalizations are all manually constructed and have been carefully debugged. The benchmark also includes the original English-language problem statements with permission from the Mathematical Association of America, which organizes the Putnam competition.

Footnote *: PutnamBench is available at [https://github.com/trishullab/PutnamBench](https://github.com/trishullab/PutnamBench).

One key benefit of PutnamBench is that Putnam competition problems require a broad base of mathematical knowledge and skills. Because they target undergraduate students, they cover topics such as analysis and abstract algebra that do not appear in the International Mathematical Olympiad (IMO). At the same time, success in the two competitions is correlated -- top performers on the Putnam competition are often former IMO medalists as well. Hence, PutnamBench is well-aligned with the IMO Grand Challenge (Challenge, 2019) and the AI Mathematical Olympiad (Prize, 2023), the latter of which offers a $10M prize fund for developing a system that can win a gold medal at the IMO.

Another advantage is that PutnamBench supports multiple proof assistants. Lean 4, Coq, and Isabelle are currently the three most popular formal proof languages. However, theorem-proving benchmarks typically only contain problems in a strict subset of these languages -- for example, MiniF2FZheng et al. (2021) does not include Coq problems, and Fimo(Liu et al., 2023) only targets Lean. PutnamBench is the first mathematics-competition benchmark to include problems in all three languages.

We use PutnamBench to evaluate several neural and symbolic approaches: Draft-Sketch-Prove (Jiang et al., 2022), Copra (Thakur et al., 2024), GPT-4, Sledgehammer (Paulson & Blanchette, 2015), and Coqhammer (Czajka & Kaliszyk, 2018). Collectively, these methods can only solve a handful of the PutnamBench problems, establishing PutnamBench as a hard open challenge for the neural theorem-proving community.

## 2 Background

**Formal Theorem-Proving.** Formal proof frameworks like Lean 4 (Moura & Ullrich, 2021), Coq (The Coq Development Team, 2023), and Isabelle (Wenzel et al., 2008) allow users to write machine-verifiable proofs of mathematical theorems. To create such a proof, one first uses a framework-specific language to formally state the target theorem. The mathematical objects referenced in the theorem can be imported from an existing repository or defined by the user. During the proof process, the proof framework maintains a _state_ that includes information about the parts of the proof that remain to be completed. One can change this state by executing a _proof step_. The user's goal is to write a sequence of proof steps (in the framework's language) that changes the proof state to a special state "QED" in which there are no unmet proof obligations.

Figure 1 illustrates a theorem and proof in the Lean 4 framework.

The Putnam Competition.The William Lowell Putnam Mathematical (Competition, 2024), organized by the Mathematical Association of America (MAA), is the premier collegiate mathematics competition in North America. Thousands of undergraduate students from universities across the United States and Canada take the exam each year. The competition comprises two 3-hour-long sessions of six problems each, presented in approximately ascending order of difficulty within each

Figure 1: A formalization of Putnam 1988 B1 in Lean 4, which asserts that for all integers \(a,b 2\), there are positive integers \(x,y,z\) such that \(ab=xy+xz+yz+1\). The formal proof begins by introducing all relevant variables and hypotheses with intro, then indicating the choice of \(x,y,z\) with use, and afterwards proving all goals using the automated tactics linarith and ring. This proof was discovered through a few-shot invocation of GPT-4.

session. While some problems require competitors to furnish a concrete solution (such as a number, a set, or the truth value of a given statement), all problems require a natural-language proof of correctness. The contest draws from a wide variety of topics in the undergraduate curriculum, often using instances of ideas from research-level mathematics.

## 3 PutnamBench

PutnamBench is a multi-language evaluation benchmark consisting of formalized problems from the Putnam competition. PutnamBench is a manually produced benchmark, including 640 formalizations in Lean 4 and Isabelle, and 412 formalizations in Coq. In aggregate, PutnamBench contains 1692 formalizations of Putnam competition problems. We also incorporate the informal statements and numerical solutions where applicable.

Now we elaborate on the main features of PutnamBench.

**Diversity and Breadth.** Compared to MinF2F (Zheng et al., 2021) and Fimo (Liu et al., 2023), which generally rely on high-school mathematics, PutnamBench incorporates a wider variety of problems which require definitions of the standard undergraduate mathematics curriculum. The ProofNet benchmark (Azerbayev et al., 2023) also sources problems from the undergraduate curriculum, but these problems are generally from standard textbooks as opposed to mathematical competitions. Putnam problems often require definitions from multiple fields, which standard textbooks do not necessarily target. Formalizations in PutnamBench include concepts from a wide range of mathematical fields, including: (i) _Analysis_: Limits, integrals, derivatives, continuity; (ii) _Linear Algebra_: Matrices, determinants, fields; (iii) _Abstract Algebra_: Rings, groups, magmas, permutations; (iv) _Algebra_: Polynomials, inequalities, algebraic expressions; (v) _Number Theory_: Primes, irrationality, base representations, divisors, palindromes; (vi) _Geometry_: Polygons, point sets, line intersections, Euclidean distance; (vii) _Set Theory & Combinatorics_: Countability, power sets, discrete structures, games.

**Multiple Languages.**PutnamBench contains formalizations of Putnam problems in Lean 4, Isabelle, and Coq. The formalizations also include concepts defined in each proof assistant's mathematical repositories -- notably, Mathlib, the HOL standard library, and Coquelicot (among various Coq repositories). To the best of our knowledge, PutnamBench is the first undergraduate-level competition benchmark for each of these languages. Furthermore, we are the first to produce a human mathematics competition-style evaluation benchmark for Coq.

We hope that this contribution can enable Coq practitioners access to the rapidly-growing field of machine learning for mathematics.

Generally, the formalizations of the problems are aligned in their structure, including hypothesis naming and framing. Differences may arise according to the underlying foundations of each language.

  
**Benchmark** & \(\#\) & **Natural Language** & **Lean** & **Isabelle** & **Coq** & **Factored Solution** \\  minF2F & 488 & ✓ & ✓\({}^{}\) & ✓ & & & \\ ProofNet & 371 & ✓ & ✓\({}^{}\) & & & & N/A \\ Fimo & 149 & ✓ & ✓\({}^{}\) & & & & \\ PutnamBench & 640 & ✓ & ✓ & ✓ & ✓ & ✓ \\   

Table 1: Comparison of existing formal theorem proving evaluation benchmarks. PutnamBench exceeds prior benchmarks by providing support for all of Lean 4, Isabelle, and Coq, on a set of difficult competition problems using undergraduate-level mathematics. For problems requiring a numerical solution in addition to a proof, we factor the solution out of the theorem statement.

  
**Category** & **Total Quantity** \\  Algebra & 253 \\ Analysis & 226 \\ Number Theory & 107 \\ Geometry & 68 \\ Linear Algebra & 51 \\ Abstract Algebra & 28 \\ Combinatorics & 26 \\ Probability & 9 \\ Set Theory & 8 \\   

Table 2: Quantity by domain of PutnamBench problems. Our formalizations generally reflect the variety of Putnam problems, though we can only formalize few geometry and probability problems due to limited support for these topics in the respective mathematical libraries.

We also note that the pre-defined mathematical theory in each language differs, which can sometimes lead to difficulties formalizing certain problems.

Compared to the prior benchmarks MiniF2F, Fimo, and ProofNet, PutnamBench is the first to support Lean 4 on initial release +.

Footnote †: MiniF2F, Fimo, and ProofNet were originally released using Lean 3, and MiniF2F and Fimo have been updated to include Lean 4 formalizations following community efforts. (Azerbayev et al., 2023; Vishwakarma et al., 2024). To the best of our knowledge, no open-sourced Lean 4 version of FIMO currently exists.

**Factored Solutions.** Roughly 60% of Putnam problems, in their natural language form, require exhibiting a (closed-form) solution along with a proof of its correctness. Such problems do not assert propositions, and hence are not immediately formalizable as they are not directly the statement of a theorem. Prior benchmarks such as MiniF2F (Zheng et al., 2021) sidestep this issue by rewording the problem statement to ask for a proof that the solution satisfies the constraints of the problem. However, this reduction diminishes the overall difficulty of the problem, as producing a solution can constitute the majority of the difficulty. To address this issue, we factor out solutions of such problems from the formalized theorem statement. We include an example in Figure 2. In this way, we provide two tasks for neural theorem proving:

* **Task 1:** Given the theorem statement, first identify the (closed-form) solution, and then provide a proof of correctness by rewriting the solution into the theorem statement.
* **Task 2:** Given the theorem statement and solution, produce a proof of its correctness. This task aligns with the current benchmarks.

We note that the process of producing the numerical solution may be highly correlated with the proof of its correctness. In this way, our formalizations can reflect the true difficulty of the informal problem statement.

**Formalization effort and challenges.** We hand-crafted our benchmark over the course of several months as a team of two doctoral and five undergraduate students with prior experience in university mathematics, computer science, and formal proof assistants. We found that the average time-to-formalize a single problem in one language was roughly 25 minutes. Each formalization was verified by a second person at least once, and we measured that the verification of a single formalization took between 10 minutes, on average. We acknowledge that the time-to-formalize we report is higher than that of MiniF2F; we believe this is largely due to the increased complexity of the Putnam problems, which oftentimes require definitions we must locate in each language's respective mathematical libraries.

We first produced formalizations in Lean 4, and then proceeded with our formalization effort in Isabelle and then Coq. Due to differences in the underlying foundations of each language, we found that formalizations in one language sometimes do not directly transfer to another; for example, Isabelle does not have a subtyping mechanism, which we made extensive use of in Lean 4. Formalizations in Coq rely on a number of mathematics repositories. Predominantly, we rely

Figure 2: A formalization of Putnam 2008 B5 in Lean 4. As the problem requires exhibiting the set of functions \(f\) satisfying the specified conditions, it is not directly the statement of a theorem. We formalize the problem by instantiating a variable “solution” outside of the theorem statement. In this way, a model can either provide its own candidate, or use the correct solution we provide and attempt to produce a proof of correctness. Benchmarks such as MiniF2F and Fimo only include formalizations with the solution written into the theorem statement.

on MathComp and MathComp-Analysis (Mathcomp, 2015; mathcomp-analysis), but also make us of Stdlib, Stdpp, Coquelicot, GeoCqoq, and Coqtail (Coquelicot, 2015; GeoCqoq, 2015; Allais et al.).

Some problems are not naturally amenable to formalization -- for example, we found that while formalizing problems involving probabilities is possible, such formalizations often require heavy probability theory. Similarly, support for problems involving Euclidean geometry varies across languages; in particular, Lean 4 does not yet have a sufficiently extensive library to make most geometry problems formalizable. By contrast, Coq has an extensive geometry repository called GeoCqoq, which we utilize for our Coq formalizations.

**Dataset Contamination.** Our benchmark is unique compared to informal benchmarks such as MATH (Hendrycks et al., 2021) and GSM8K (Cobbe et al., 2021) in the sense that the target output _has never been produced_, hence avoiding direct contamination. To the best of our knowledge, we are the first to provide formalizations of a large collection of Putnam problems in any of Lean, Isabelle, and Coq. Since writing a formal proof requires the formal theorem statement, it is highly unlikely any possible formal proof has been written for any of our problems. We performed a thorough investigation of formal mathematics repositories for each language for confirmation, finding no aligned theorems and proofs from the Putnam Competition. We do not include any of the formal proofs in our benchmark.

Furthermore, any proofs found by automated methods in our evaluations are not included and are only mentioned in this article. Indirect contamination can occur through transfer from training on the informal proofs, though producing proofs in formal proof environments still presents a major difficulty for all current neural methods, as we find in Section 4.

**Licensing and Rules of Engagement.** PutnamBench is available under an Apache 2.0 license for Lean 4 and Isabelle, and under an MIT license for Coq. We align the licenses with those of the repositories we use for each language. With permission from the MAA, we include the informal statements as sourced from the competition (Alexanderson et al., 1985; Kedlaya et al., 2002, 2020). We host a public leaderboard at [https://trishullab.github.io/PutnamBench/](https://trishullab.github.io/PutnamBench/) and will readily accept evaluation results from future works.

## 4 Experimental Evaluation

To understand the challenges that PutnamBench poses for state-of-the-art theorem-proving approaches, we attempt to solve its problems using a suite of such approaches. Given the relative lack of tailored systems for multi-language theorem-proving, we run evaluations for each language separately. Any method that is evaluated on multiple languages is based on off-the-shelf foundation models.

Figure 3: Formalizations of Putnam 2006 B2 in (a) Lean 4, (b) Isabelle, (c) Coq. Putnam 2006 B2 asserts that given a finite subset \(X\) with \(|X|=n>0\), there is a nonempty subset \(S X\) and an \(m\) such that \(|m+_{s S}s|\).

[MISSING_PAGE_FAIL:6]

queries to GPT-4 can yield more successful proofs, though it is not yet feasible to perform such an experiment due to the cost of queries to GPT-4.

We found that, by default, GPT-4 produces proofs using Lean 3 syntax, which is not compatible with Lean 4. Even when directed to produce outputs in Lean 4, GPT-4 typically continues to produce outputs in Lean 3. Our prompt, which we include in Figure 16, elucidates some design differences in Lean 4 to better enforce compliance with the Lean 4 syntax. However, we noticed many examples where GPT-4 continues to output terms in Lean 3 syntax. One such example is given in Figure 17.

We run ReProver using the standard search parameters used in LeanDojo (Yang et al., 2023). Our evaluation yields no successfully proven problems, with and without the inclusion of the retrieval module. We believe that Putnam 1988 B1, which the other methods solve, is not solved by ReProver as it requires an understanding that the choice of \(x,y,z=1,a-1,b-1\) will eventually satisfy the conditions of the goal after simplification. Smaller models, like the one driving ReProver's search, may not be as readily capable of such understanding.

Isabelle.We run GPT-4 using the same configuration, with modified prompts for Isabelle, on our Isabelle formalizations. We find that GPT-4 can produce a single successful proof to Putnam 1986 B1, a geometric problem stated algebraically. We include the statement and its proof as generated by GPT-4 in Figure 19.

DSP represents a neurosymbolic methodology which has seen significant application for theorem-proving in MiniF2F. We run DSP with \(pass@10\), using temperature \(T=0.1\) and GPT-4 as the underlying language model. Our evaluation yields four successful proofs: of Putnam 2001 A1 and 1971 B1, two problems involving magmas (sets with a binary operation), one of Putnam 1995 A1, a problem involving a closed-under-multiplication subset of the reals, and Putnam 1986 B1. In particular, Putnam 1995 A1 and 1986 B1 cannot be solved by Sledgehammer alone. The generated proof of Putnam 1995 A1 is included in Figure 4.

We run a baseline using Sledgehammer, a powerful automation tool in Isabelle which makes calls to external SMT solvers to prove a given goal. With a set timeout of \(t=120\) seconds, we run Sledgehammer on each Isabelle formalization. The result of this evaluation is 3 successfully proven problems: Putnam 1971 B1, 2001 A1, and 2012 A2. Notably, all of these problems are statements about sets with binary operations. We include the statements of 1971 B1 and 2012 A2 in Figure 22.

Coq.We run GPT-4 with a Coq-based prompt on our Coq formalizations using the same configuration as in Lean and Isabelle. The result of the experiment is 1 solved problem, namely Putnam 1988 B1, which was also solved in Lean 4. The proof, which we include in Figure 14, generally follows the same structure as the proof in Lean.

An evaluation with COPRA, in a \(pass@1\)-with-\(60\)-queries and \(T=0.0\) also yields a successful proof only for Putnam 1988 B1 which we include in Figure 14. In this case, backtracking was crucial for proof search on this problem. The crucial step in 1988 B1 is the choice of \(x,y,z\) once \(a\) and \(b\) have been introduced. Initially, COPRA predicts the erroneous choice \(x,y,z=1,1,ab-1\) and eventually reverts this choice using backtracking. Afterwards, COPRA predicts a correct choice \(x,y,z=1,a-1,b-1\) and proceeds with the proof.

Figure 4: A formalization of Putnam 2001 A1 in Isabelle and the corresponding proof discovered by our evaluation with DSP. Sledgehammer alone can also produce a successful proof to this theorem.

We run Tactician using the locality sensitive hashing model with a timeout of \(t=600s\) per problem. Our evaluation yields no successfully proven problems. While showing favorable performance on theorems drawn from Coq's standard library (Zhang et al., 2021), such methodologies do not as of yet scale to challenging olympiad-style problems.

We run CoqHammer with 8 parallel threads using an ATP timeout of 100 seconds, proof reconstruction timeout of 15 seconds, and sauto timeout of 5 seconds, for a total of 120 seconds allocated for each formalization. The evaluation yields no successful proofs -- indicating that symbolic tools in Coq are not yet capable of handling PutnamBench problems. It is not surprising that CoqHammer does not match the performance of Sledgehammer even though they rely on the same external solvers. The underlying logical system of Coq is more complex than that of Isabelle and is hence less amenable to automation.

### General Analysis

Aggregating over all experiments performed in all languages, we find that a total of 6 problems in PutnamBench are successfully proven. A majority of these come from evaluations in Isabelle, particularly with strong contributions from Sledgehammer. Sledgehammer can solve all three problems involving magmas which appear in our benchmark but fails to produce successful proofs for any other formalization. DSP solves an additional two problems and relies heavily on Sledgehammer to fill in the proofs of intermediate steps. The single problem solved in Lean and Coq also makes use of automated tactics like linarith and lia, and requires only a single crucial step.

Hence, we find that a few PutnamBench problems are not entirely intractable using current methods. However, anecdotally, these problems are among the easiest ever included in the Putnam competition. All admit a very short natural language proof and do not require reasoning about particularly complicated objects. We believe that significant advancements in automated mathematical reasoning are required to make progress on PutnamBench.

## 5 Related Work

**Formal Benchmarks.** Several evaluation benchmarks for formal mathematics have been developed in recent years. MiniF2F (Zheng et al., 2021) is a formal-to-formal benchmark of competition problems, sourced from high school competitions such as the AMC, AIME, and IMO. MiniF2F is a multi-language benchmark, comprising of 488 problems each formalized in Lean 3, Metamath, Isabelle and HOL Light. We chose not to include formalizations in Metamath and HOL Light as they have not been the focus of attention for neural theorem-proving. A similar competition-style benchmark is FIMO (Liu et al., 2023), which contains 149 Lean 3 formalizations of IMO shortlist problems produced using a back-translation procedure with GPT-4. The automatically-generated formalizations are then manually verified. Both benchmarks are designed to measure _certifying_ the solution to the informal problem statement when one exists. Compfiles (2024) is a collection of 171 Lean 4 formalizations of competition problems, predominantly from the IMO and USAMO, often accompanied by a formal proof, which has not seen use in benchmarking automated theorem-provers. ProofNet (Azerbayev et al., 2023) introduced a benchmark of 371 exercises, formalized in Lean 3, from standard textbooks in the undergraduate mathematics curriculum. While largely not competition-based, problems in ProofNet draw from a broader library of concepts than miniF2F and FIMO, which rely only on high-school mathematics. LeanDojo (Yang et al., 2023) introduces a dataset of formal mathematics and proofs derived from Lean's mathlib library (mathlib Community, 2020), and trains a retrieval-augmented model towards generating proofs on their held-out test set. ProverBot9001 (Sanchez-Stern et al., 2020) introduced a dataset for theorems and proofs written in Coq derived from CompCert (Leroy, 2009), a formally verified C compiler. PISA (Jiang et al., 2021) is a dataset derived from Isabelle's Archive of Formal Proofs (AFP), which contains theorems and proofs from general mathematics as opposed to specifically competition problems.

**Informal Benchmarks.** There are also several popular benchmarks for informal (natural-language) mathematical reasoning. MATH (Hendrycks et al., 2021) is a collection of 12,500 mathematics problems, in natural language only, sourced from various high school competitions additionally supplied with step-by-step informal proofs. GSM8K (Cobbe et al., 2021) is a collection of 8,500 grade school mathematics problems, intended to benchmark natural language reasoning for mathematics-style problems. While benefiting from the abundance of natural language data, these benchmarks fall short, since in natural language, there is no automatic mechanism for certifiable verification of the reasoning path which yielded the numerical answer. For this reason, metrics for success on these benchmarks usually rely on exact-answer match, because verifying reasoning paths is imprecise and is best done by human experts. By contrast, theorem proving in formal proof assistants comes with a high-confidence signal for correctness of the reasoning path, or _proof_, of a theorem.

**Methods for Formal Theorem-Proving.** Significant effort has been spent on developing automatic theorem-provers for formal mathematics (Li et al., 2024). Most recent efforts train a neural module to perform proof-step prediction, which is then wrapped in a search mechanism to locate a valid proof. GPT-\(f\)(Polu and Sutskever, 2020) trains a transformer-based architecture on data derived from the Metamath library (Megill and Wheeler, 2019) for proof synthesis. PACT expands on GPT-\(f\) by incorporating auxiliary training tasks for the neural module towards theorem-proving in Lean 3. FMSCL (Polu et al., 2022) alternates proof-search and training to finetune their neural model based on proofs found during search. HTPS (Lample et al., 2022) uses a transformer-based neural module in an online, MCTS-inspired proof search in Lean 3 and Metamath. COPRA (Thakur et al., 2024) uses GPT-4 supplied with error feedback from the environment and lemmas from a retrieval mechanism for an agentic proof-search in Lean 3 and Coq. LLEMMA (Azerbayev et al., 2024) continues pretraining of Code Llama on a mathematics-based corpus dubbed Proof-Pile-2, and uses their learned model for formal proof search in Lean 4. DeepSeek-Prover Xin et al. (2024) produces synthetic Lean data en-masse for training their prover model. AlphaGeometry (Trinh et al., 2024) targets IMO problems in a geometry-specific proof assistant language using an interleaving search, where a neural module synthesizes auxiliary constructions and a symbolic engine produces deductive closures.

The Isabelle proof assistant (Paulson, 1994), given its declarative nature and powerful symbolic automation, has too been the focus of much attention for neural theorem proving. Isabelle features Sledgehammer (Paulson and Blanchette, 2015), an automated reasoning tool which calls external automated theorem provers (ATPs) for proof synthesis. Draft, Sketch, Prove (DSP) (Jiang et al., 2022b) uses a high-caliber LLM to generate natural language proofs and converts them into formal _sketches_ in Isabelle, whose gaps are then filled using Sledgehammer. Zhao et al. (2023) employed a diffusion model to predict an optimal ordering of the few-shot examples provided to the LLM in the DSP pipeline. Lyra (Zheng et al., 2023) utilized error-feedback from Isabelle's execution to modify holes in the sketch which were too difficult for the symbolic prover. POETRY (Wang et al., 2024) leverages recursion for theorem-proving and trains a neural module to produce proof sketches, as opposed to using in-context learning with an LLM. LEGO-Prover (Wang et al., 2023) extends the pipeline by incorporating a skill library which grows throughout the proof search task. Separate from approaches utilizing natural language proofs, Thor (Jiang et al., 2022a) trains a transformer-based architecture to predict successful invocations of Sledgehammer, along with the usual proof-step objective. Baldur (First et al., 2023) explored repairing erroneous proofs in Isabelle through the use of LLMs.

The Coq interactive theorem prover has seen use in both software verification and general mathematics. Famously, mechanized proofs of the Four Colour Theorem (Robertson et al., 1997) and the Feit-Thompson theorem (Gonthier et al., 2013) were produced in Coq. Similarly, numerous software verification projects have been undertaken in Coq, such as CompCert (a formally verified C compiler) and Verdi (Wilcox et al., 2015) (a framework for verifying distributed systems protocols). ASTactic (Yang and Deng, 2019) trained a neural module involving recurrent networks and attention on data collected from various Coq repositories. Proverbot9001 (Sanchez-Stern et al., 2020) targeted proof synthesis on a set of held-out theorems from the CompCert project. COPRA (Thakur et al., 2024) also evaluates on this CompCert-based task using their multi-language approach. Tactician (Blaauwbroek et al., 2020) develops a platform for proof automation for the Coq practitioner, with support for experimenting with new machine learning techniques for tactic prediction and proof search. (Zhang et al., 2021) explores several online learning techniques inside Tactician, including an approximate \(k\)-nearest neighbors method via locality sensitive hashing which we use for our evaluation. Graph2Tac (Blaauwbroek et al., 2024) uses graph neural networks for learning online hierarchical representations of new theorems and definitions, and is used for proof search within Tactician.

Conclusion

We presented PutnamBench, a benchmark for neural theorem-proving consisting of formalizations of Putnam competition problems. A distinctive feature of PutnamBench is that it spans a broad range of undergraduate-level mathematical topics, including algebra, analysis, and number theory. Another unique benefit is that it includes problems in Lean 4, Isabelle, and Coq, the three most popular formal proof frameworks.

As our experiments show, PutnamBench is a challenging benchmark: all current theorem-proving approaches fail to solve more than a handful of its problems. We believe that these failures include two root causes: (i) While current theorem-provers can effectively stitch together standard proof steps well-represented in the training corpus, they often fail at synthesizing new lemmas and orchestrating these lemmas into intricate proofs. (ii) Current methods often fail to leverage the deep knowledge available in mathematics repositories. Developing a new generation of neural theorem-provers in which these weaknesses are at least partly addressed is an exciting direction of future research.

Acknowledgements.This work was supported by NSF awards CCF-2212559 and CCF-2403211, the NSF Institute for Foundations of Machine Learning, and a gift by the Aziz Family Foundation. We thank Oliver Nash, Eric Wieser, Edward Lockhart, Fabian Gloeckle, Karl Palmskog, Lasse Blaauwbroek, Jason Rute, and Kaiyu Yang for useful discussions, aiding in benchmark maintenance, and support with setting up experiments.