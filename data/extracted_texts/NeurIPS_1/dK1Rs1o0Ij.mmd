# Reimagining Synthetic Tabular Data Generation through Data-Centric AI:

A Comprehensive Benchmark

Lasse Hansen

A.L.2

Nabeel Seedat

Mihaela van der Schaar

Department of Applied Mathematics and Theoretical Physics, University of Cambridge, UK Faculty of Organisational Sciences, University of Belgrade, Serbia

###### Abstract

Synthetic data serves as an alternative in training machine learning models, particularly when real-world data is limited or inaccessible. However, ensuring that synthetic data mirrors the complex nuances of real-world data is a challenging task. This paper addresses this issue by exploring the potential of integrating data-centric AI techniques which profile the data to guide the synthetic data generation process. Moreover, we shed light on the often ignored consequences of neglecting these data profiles during synthetic data generation -- despite seemingly high statistical fidelity. Subsequently, we propose a novel framework to evaluate the integration of data profiles to guide the creation of more representative synthetic data. In an empirical study, we evaluate the performance of five state-of-the-art models for tabular data generation on eleven distinct tabular datasets. The findings offer critical insights into the successes and limitations of current synthetic data generation techniques. Finally, we provide practical recommendations for integrating data-centric insights into the synthetic data generation process, with a specific focus on classification performance, model selection, and feature selection. This study aims to reevaluate conventional approaches to synthetic data generation and promote the application of data-centric AI techniques in improving the quality and effectiveness of synthetic data.

## 1 Introduction

Machine learning has become an essential tool across various industries, with high-quality data representative of the real world being a crucial component for training accurate models that generalize . In cases where data access is restricted or insufficient synthetic data has emerged as a viable alternative . The purpose of synthetic data is to generate training data that closely mirrors real-world data, enabling the effective use of models trained on synthetic data on real data. Moreover, synthetic data is used for a variety of different uses, including privacy (i.e. to enable data sharing, ), competitions  fairness , and improving downstream models .

However, generating high-quality synthetic data that adequately captures the nuances of real-world data, remains a challenging task. Despite significant strides in synthetic data with generative models, they sometimes fall short in replicating the complex subtleties of real-world data, particularly when dealing with messy, mislabeled or biased data. For instance, regarding fairness,  have shown that such gaps can lead to flawed conclusions and unreliable predictions on subpopulations, thereby restricting the practical usage of synthetic data.

The ability of synthetic data to capture the subtle complexities of real-world data is crucial, particularly in contexts where these issues might surface during deployment. Inaccurate synthetic data can not only hamper predictive performance but also result in improper model selection and distorted assessments of feature importance, thereby undermining the overall analysis. These challenges underscore the need to improve the synthetic data generation process.

One might wonder, surely, assessing fidelity via statistical divergence metrics  such as MMD or KL-divergence is sufficient? We argue that such high-level metrics tell one aspect of the story. An overlooked dimension is the characterization of data profiles. In this approach, samples are assigned to profiles that reflect their usefulness for an ML task. Specifically, samples are typically categorized as easy to learn, ambiguous, or hard, which are proxies for data issues like mislabeling, data shift, or under-represented samples. In methods such as Data-IQ and Data Maps  this is referred to as "groups of the data", however, we use "data profiles" for clarity.

While this issue has been well-studied for supervised tasks, it has not been explored in the generative setting. We highlight the issues of overlooking such data profiling in Figure 1, where despite near-perfect statistical fidelity (inverse KLD), we show the differing proportion of 'easy' examples identified in synthetic data generated by different generative models trained on the Adult dataset . On the other hand, this data profile correlates with downstream classification performance.

To address this challenge of the representativeness of synthetic data, we explore the potential of integrating data-centric AI techniques and their insights to improve synthetic data generation. Specifically, we propose characterizing individual samples in the data and subsequently using the different data profiles to guide synthetic data generation in a way that better reflects the real world. While our work is applicable across modalities, our primary focus is tabular data given the ubiquity of tabular data in real-world applications , with approximately 79% of data scientists working with it on a daily basis, vastly surpassing other modalities .

**Contributions:**

1. _Conceptually_, we delve into the understanding of fundamental properties of data with respect to synthetic data generation, casting light on the impact of overlooking data characteristics and profiles when generating synthetic data.

2. _Technically_, we bring the idea of data profiles in data-centric AI to the generative setting and explore its role in guiding synthetic data generation. We introduce a comprehensive framework to facilitate this evaluation across various generative models.

3. _Empirically_, we benchmark the performance of five state-of-the-art models for tabular data generation on eleven distinct tabular datasets and investigate the practical integration of data-centric profiles to guide synthetic data generation. We provide practical recommendations for enhancing

Figure 1: Measures of data-centric profiling (A) better reflect the downstream performance of generative models (B) than measures of statistical fidelity (C). Assessed on the Adult dataset  using five different generative models A) Proportion easy examples in the generated datasets identified by Cleanlab, B) Supervised classification performance when training on synthetic, testing on real data, C) Inverse KL-divergence. (bn=bayesian_network)

synthetic data generation, particularly with respect to the 3 categories of synthetic data utility (i) predictive performance, (ii) model selection and (iii) feature selection.

We hope the insights of this paper spur the reconsideration of the conventional approaches to synthetic data generation and encourage experimentation on how data-centric AI could help synthetic data generation deliver on its promises.

## 2 Related work

This work engages with synthetic data generation and data characterization in data-centric AI.

**Synthetic Tabular Data Generation** uses generative models to create artificial data that mimics the structure and statistical properties of real data, and is particularly useful when real data is scarce or inaccessible [23; 24; 4]. In the following, we describe the broad classes of synthetic data generators applicable to the tabular domain. _Bayesian networks_ are a traditional approach for synthetic data generation, that represent probabilistic relationships using graphical models. _Conditional Tabular Generative Adversarial Network_ (CTGAN)  is a deep learning method for modeling tabular data. It uses a conditional GAN to capture complex non-linear relationships. _The Tabular Variational Autoencoder_ (TVAE) is a specialized Variational Autoencoder, designed for the tabular setting .

_Normalizing flow models_[27; 28] provide an invertible mapping between data and a known distribution, and offer a flexible approach for generative modeling. Diffusion models, which have gained recent popularity, offer a different paradigm for generative modeling. _TabDDPM_ is a diffusion model proposed for the tabular data domain. In this work, we evaluate these classes of generative models, considering various aspects of synthetic data evaluation.

**Evaluation of Synthetic Data** is a multifaceted task [17; 30], involving various dimensions such as data utility with respect to a downstream task, statistical fidelity, and privacy preservation [17; 30]. In this work, we focus on dimensions that impact model performance and hence, while important, we do not consider privacy aspects.

_(1) Data Utility:_ refers to how well the synthetic data can be used in place of the real data for a given task. Typically, utility is assessed by training predictive models on synthetic data and testing them on real data [4; 17; 31; 32; 5]. We posit that beyond matching predictive performance, we also desire to retain both _model ranking_ and _feature importance_ rankings. We empirically assess these aspects in Sec. 5.

_(2) Statistical Fidelity:_ measures the degree of similarity between synthetic data and the original data in terms of statistical properties, including the marginal and joint distributions of variables . Statistical tests like the Kolmogorov-Smirnov test or divergence measures like Maximum Mean Discrepancy, KL-divergence or Wasserstein distance are commonly used for evaluation[17; 5].

Beyond statistical measures, the concept of data characterization and profiles of easy and hard examples has emerged in data-centric AI. These profiles serve as proxies for understanding real-world data, which is often not "perfect" due to mislabeling, noise, etc.The impact of these profiles on supervised models has been demonstrated in the data-centric literature [33; 34; 18]. In Figure 1, we show that data profiles are similarly important in the generative setting. Despite having almost perfect statistical fidelity, different generative models capture different data profiles (e.g. proportion of easy examples), leading to varying data utility as reflected in different performances. Consequently, we propose considering data profiles as an important dimension when creating synthetic data. We describe current data-centric methods that can facilitate this next.

**Data profiling** is a growing field in Data-Centric AI that aims to evaluate the characteristics of data samples for specific tasks [35; 36]. In the supervised learning setting, various methods have been developed to assign samples to groups, which we refer to as data profiles. These profiles, such as easy, ambiguous, or hard, often reveal issues such as mislabeling, data shifts, or under-represented groups [34; 18; 33; 37; 19; 38]. Various mechanisms are used in different methods for data characterization. For example, Cleanlab  models relationships between instances based on confidence, while Data Maps and Data-IQ  assess uncertainty through training dynamics. However, many existing methods are designed for neural networks and are unsuitable for non-differentiable models like XGBoost, which are commonly used in tabular data settings. Consequently, we focus on data characterization approaches such as Cleanlab, Data-IQ, and Data Maps which are more applicable to tabular data.

## 3 Framework

We propose a unified framework that enables a thorough assessment of generative models and the synthetic data they produce. The framework encompasses the evaluation of the synthetic data based on established statistical fidelity metrics as well as three distinct tasks encompassing _data utility_.

At a high level, the framework proceeds as visualized in 2. The dataset is first divided into a training set, denoted as \(_{}\), and a testing set, denoted as \(_{}\). A duplicate of the training set (\(_{}\)) undergoes a data-centric preprocessing approach to produce a preprocessed version of the training set, referred to as \(_{}^{}\). A generative model is then trained on \(_{}^{}\). This model is used to synthesize a new dataset, denoted as \(_{}\). The synthetic dataset is further processed using a data-centric postprocessing method to create the final synthetic dataset, denoted as \(_{}^{}\). Various classification models \(\) are then trained separately on the original training set \(_{}\) and the synthetic dataset \(_{}^{}\). These models are then applied to the testing set \(_{}\) for evaluation. The generative and supervised models are evaluated for their statistical fidelity and data utility. The focus is on classification performance, model selection, and feature selection. Further details on each process within the framework can be found in the following subsections.

### Data profiling

Assume we have a dataset \(=\{(x^{n},y^{n}) n[N]\}\). Data profiling aims to assign a score \(S\) to samples in \(\). On the basis of the score, a threshold \(\) is typically used to assign a specific profile group \(p^{n}\), where \(=\{Easy,Ambigious,Hard\}\) to each sample \(x^{n}\).

Our framework supports three recent data characterization methods applicable to tabular data: Cleanlab , Data-IQ , and Data Maps . They primarily differ based on their scoring mechanism \(S\). For instance, Cleanlab  uses the predicted probabilities as \(S\) to estimate a noise matrix, Data-IQ  uses confidence and aleatoric uncertainty as \(S\), and Data Maps uses confidence and variability (epistemic uncertainty) as \(S\). Moreover, they differ in the categories in the data profiles derived from their scores. Data-IQ and Data Maps provide three categories of data profiles: _easy_; samples that are easy for the model to predict, _ambiguous_; samples with high uncertainty, and _hard_;

Figure 2: Illustration of the framework’s process flow. _Data partitioning_: the dataset is divided into a training set, \(_{}\), and a testing set, \(_{}\). _Data profiling_: a data-centric preprocessing approach is employed on a duplicate of \(_{}\) to produce \(_{}^{}\). A generative model, trained on \(_{}^{}\), is then utilized to synthesize a dataset, \(_{}\), which is further processed using a data-centric postprocessing method to achieve the final synthetic dataset, \(_{}^{}\). _Classification model training_: various classification models are separately trained on \(_{}\) and \(_{}^{}\) and applied to \(_{}\). _Evaluation_: the generative and supervised models are appraised for their statistical fidelity and utility, focusing on classification accuracy, model selection, and feature selection.

samples that are wrongly predicted with high certainty. Cleanlab provides two profiles: _easy_ and _hard_ examples.

We create data profiles with these three data-centric methods to evaluate the value of data-centric methods to improve synthetic data generation, both _ex-ante_ and _post hoc_. We use the profiles in multiple preprocessing and postprocessing strategies applied to the original and synthetic data.

#### 3.1.1 Preprocessing

Preprocessing strategies are applied to the original data \(_{}\) i.e., before feeding to a generative model. We investigate three preprocessing strategies: (1) baseline, which applies no processing, and simply feeds the \(_{}\) to the generative model. (2) easy_hard: Let \(S_{c}:_{}\) denote the scoring function for data-centric method \(c\). We partition \(_{}\) into \(_{}^{}\) and \(_{}^{}\) data profiles using a threshold \(\), such that \(_{}^{}=\{x^{n} S_{c}(x^{n})\}\) and \(_{}^{}=\{x^{n} S_{c}(x^{n})>\}\). (3) Analogously, easy_ambiguous_hard 2 splits the \(_{}\) on the easy, ambiguous, and hard examples. Further details are provided in Appendix A.

#### 3.1.2 Generative model

We utilize the data profiles identified in the preprocessing step to train a specific generative model for each data segment, e.g. easy and hard examples separately. Let \(G:_{}_{}\) denote the generative model trained on a dataset \(_{}\), which produces synthetic dataset \(_{}\). In our framework, for each data profile in preprocessed dataset \(_{}^{}\), we train a separate generative model. We generate data using each generative model and the combined synthetic data is then \(_{}=G_{}(_{}^{}) G_{}(_{}^{})\), with generation preserving the ratio of the data segments, to reflect their distribution in the initial dataset.

#### 3.1.3 Postprocessing

We define postprocessing strategies as processing applied to the synthetic data after data generation but before supervised model training and task evaluation. We denote the set of postprocessing strategies as \(\). Given the synthetic dataset \(_{}\), each postprocessing strategy \(h\) maps \(_{}\) to a processed dataset \(_{}^{}=h(_{})\). Two different postprocessing strategies were used: baseline: This is the identity function \(h_{}(_{})=_{}\). no_hard: We remove the hard examples from the synthetic data, \(_{}^{}=_{}\{x _{}^{n} S_{c}(x_{}^{n})>\}\), where \(x_{}^{n}\) is generated synthetic data.

### Classification model training

The training procedure of the supervised classification models \(\) comprises two steps, each minimizing a cost function \(\). (1) Train on the real data, i.e., \(_{}=((_{ {train}}))\). (2) Train on synthetic data, i.e. \(_{}=((_{ {synth}}^{}))\) We then compare utility of \(_{}\) and \(_{}\) in the evaluation procedure. Our framework supports any machine learning model \(\) compatible with the Scikit-Learn API.

### Evaluation

Finally, the framework includes automated evaluation tools for the generated synthetic data to evaluate the effect of pre- and postprocessing strategies, across datasets, random seeds, and generative models. To thoroughly assess our framework, we establish evaluation metrics that extend beyond statistical fidelity, encapsulating data utility through the inclusion of three tasks.

#### 3.3.1 Statistical fidelity

The quality of synthetic data is commonly assessed using divergence measures between the real and synthetic data [5; 30]. Our framework allows for this assessment using widely adopted methods including inverse KL-Divergence , Maximum Mean Discrepancy , Wasserstein distance, as well as Alpha-precision and Beta-Recall . However, as shown in Figure 1, such measures can only tell one aspect of the story. Indeed, despite all generative models providing near-perfect statistical fidelity based on divergence measures, the synthetic data captures the nuances of real data differently, as reflected in the varying data profiles (e.g. proportion easy examples). This motivates us to also assess the data utility and the potential implications of this variability.

#### 3.3.2 Data utility

Three specific metrics were employed to assess data utility: classification performance, model selection, and feature selection.

**Classification performance** To explore the usefulness of the generated synthetic data for model training, we use the train-on-synthetic, test-on-real paradigm to fit a set of machine learning models \(\) on the synthetic data, \(_{}\), and subsequently evaluate their performance on a real, held-out test dataset, \(_{}\). By using \(_{}\) we avoid potential issues from data leakage that might occur from an evaluation on the real training sets, \(_{}\).

**Model selection** When using synthetic data for model selection, it is imperative that the ranking of classification models \(\) trained on synthetic data aligns closely with the ranking of classification models trained on the original data. To evaluate this, we first train a set of \(_{}\) on \(_{}\) and evaluate their classification performance on \(_{}\). Next, we fit the same set of \(_{}\) on \(_{}^{}\) and evaluate their classification performance on \(_{}\). The rank-ordering of the \(_{}\) in terms of a performance metric (e.g. AUROC) is compared with the ranking-order of the \(_{}\) using Spearman's Rank Correlation.

**Feature selection** Feature selection is a crucial task in data analysis and machine learning, aiming to identify the most relevant and informative features that contribute to a model's predictive power. To evaluate the utility of using synthetic data for feature selection, a similar approach is followed as for model selection. First, a model \(_{}\) with inherent feature importance (e.g. random forest) is trained on \(_{train}\) and the rank-ordering of the most important features is determined. This ranking is then compared to the rank ordering of the most important features obtained from the same model type \(_{}\) trained on \(_{}^{}\) using Spearman's Rank Correlation.

### Extending the framework

The framework presented in this paper is intentionally designed to be modular and highly adaptable, allowing for seamless integration of various generative models, pre- and postprocessing strategies, and diverse tasks. This flexibility enables researchers and practitioners to explore and evaluate e.g. different combinations of generative models alongside various pre- and post-processing strategies. Further, the framework is extensible, allowing for the incorporation of additional generative models, novel processing methods, and emerging tasks, ensuring that it remains up-to-date and capable of accommodating future advancements in the field of synthetic data generation.

## 4 Experiments

To demonstrate the framework, we conduct multiple experiments, aiming to answer the following subquestions in order to investigate: **Can data-centric ML improve synthetic data generation?**:

**Q1:** Is statistical fidelity sufficient to quantify the utility of synthetic data?

**Q2:** Can we trust results from supervised classification models trained on synthetic data to generalize to real data?

**Q3:** Can data-centric approaches be integrated with synthetic data generation to create more realistic synthetic data?

**Q4:** Does the level of label noise influence the effect of data-centric processing for synthetic data generation?

All code for running the analysis and creating tables and graphs can be found at the following links: [https://github.com/HLasse/data-centric-synthetic-data](https://github.com/HLasse/data-centric-synthetic-data) or [https://github.com/vanderschaarlab/data-centric-synthetic-data](https://github.com/vanderschaarlab/data-centric-synthetic-data).

### Data

We assess our framework on a filtered version of the Tabular Classification from Numerical features benchmark suite from . To reduce computational costs, we filter the benchmark suite to only include datasets with less than 100.000 samples and less than 50 features which reduced the number of datasets from 16 to 11. The datasets span several domains and contain a highly varied number of samples and features (see B for more details.). Notably, the datasets have been preprocessed to meet a series of criteria to ensure their suitability for benchmarking tasks. For instance, the datasets have at least 5 features and 3000 samples, are not too easy to classify, have missing values removed, have balanced classes, and only contain low cardinality features.

### Generative models

To cover a representative sample of the space of generative models, we evaluate 5 different models with different architectures as reviewed in 2: bayesian networks (bayesian_network), conditional tabular generative adversarial network (ctgan), tabular variational autoencoder (tvae), normalizing flow (nflow), diffusion model for tabular data (ddpm).

### Supervised classification model training

The variety of models employed in our study includes: extreme gradient boosting (xgboost), random forest, logistic regression, decision tree, k-nearest neighbors, support vector classifier, gaussian naive bayes, and multi-layer perception. It is the ranking of these models that is evaluated for the model selection task. Given the large number of models, we restrict the classification results in the main paper to be from the xgboost model. Feature selection results are reported for xgboost models. Classification and feature selection results for the other classifiers can be found in Appendix C.

### Experimental procedure

**Main study** The experimental process followed the structure outlined in Sec. 3 and Figure 2, repeated for each of the 11 datasets, 5 generative models, 10 random seeds, and all permutations of pre- and postprocessing methods for each of the three data-centric methods (Cleanlab, Data-IQ, and Data Maps). We comprehensively evaluate the results across classification performance, model selection, feature selection, and statistical fidelity. In total, we fit more than **8000** generative models.

**Impact of label noise** To assess the impact of label noise on the effect of data-centric pre- and postprocessing, we carried out an analogous experiment to the main study, on the Covid mortality dataset . Here, we introduce label noise to \(_{}\) before applying any processing. We study the impact of adding  percent label noise.

All results reported in the main paper use Cleanlab as the data-centric method for both pre- and postprocessing. This decision was made to ensure clarity in the reported results and because Cleanlab was found to outperform Data-IQ and Data Maps in a simulated benchmark. For the benchmark of the data-centric methods as well as results using Data-IQ and Data Maps, we refer to Appendix C.

   Generative Model & Classification & Model Selection & Feature Selection & Statistical fidelity \\  Real data & 0.866 (0.855, 0.877) & 1.0 & 1.0 & 1.0 \\  bayesian\_network & 0.622 (0.588, 0.656) & 0.155 (0.055, 0.264) & 0.091 (-0.001, 0.188) & 0.998 (0.998, 0.999) \\ ctgan & 0.797 (0.769, 0.823) & **0.519** (0.457, 0.579) & 0.63 (0.557, 0.691) & 0.979 (0.967, 0.987) \\ ddpn & **0.813** (0.781, 0.844) & 0.508 (0.446, 0.573) & 0.635 (0.546, 0.718) & 0.846 (0.668, 0.972) \\ nflow & 0.737 (0.713, 0.761) & 0.354 (0.288, 0.427) & 0.415 (0.34, 0.485) & 0.975 (0.968, 0.981) \\ tvae & 0.792 (0.764, 0.818) & 0.506 (0.436, 0.565) & **0.675** (0.63, 0.722) & 0.966 (0.953, 0.978) \\   

Table 1: Summarised performance for the baseline condition (no data-centric processing) across all datasets. Classification is measured by AUROC, model selection and feature selection by Spearman’s Rank Correlation, and statistical fidelity by inverse KL divergence. Numbers show bootstrapped mean and 95% CI. The best-performing model by task is in bold.

#### 4.4.1 Evaluation metrics

The classification performance is evaluated in terms of the area under the receiver operating characteristic curve (AUROC), model selection performance as Spearman's Rank Correlation between the ranking of the supervised classification models trained on the original data and the supervised classification models trained on the synthetic data, and feature selection performance as Spearman's Rank Correlation between the ranking of features in an xgboost model trained on the original data and an xgboost model trained on the synthetic data.

## 5 Results

**Statistical fidelity is insufficient for evaluating generative models.** Measures of statistical fidelity fail to capture variability in performance on downstream tasks, as shown in Table 1. Surprisingly, the worst performing model across all tasks (bayesian network), has the highest inverse KL-divergence of all synthetic datasets, which should indicate a strong resemblance to the original data. Conversely, the lowest inverse KL-divergence is found for ddpm which is one of the consistently best performing models.

**Practical guidance:** The benchmarking results illustrate that when selecting a generative model, even if the statistical fidelity appears similar, different generative models may perform differently on the 3 downstream tasks (classification, model selection, feature selection). Hence, beyond statistical fidelity, practitioners should understand which aspect is most crucial for their purpose to guide selection of the generative model.

**Different generative models for different tasks.** As shown in Table 1, training on synthetic data leads to a marked decline in classification performance compared to real data, as well as highly differing model and feature rankings. The effect differs largely by generative model, where CTGAN, TabDDPM, and TVAE most closely retain the characteristics of the real data. No one model is superior across all tasks. Specifically, TabDPPM achieves the highest classification performance, CTGAN performs best in model selection, and TVAE excels in feature selection. These findings indicate that one should test a range of generative models and consider the trade-offs in data utility before publishing synthetic data. Additionally, Appendix C reveals that although there are slight differences in performance based on the supervised model type, the overall pattern of results remains consistent across generative models.

**Practical guidance:** No generative model reigns supreme (highlighting the inherent challenge of synthetic tabular data). However, over tabular data sets, we show that _CTGAN_ and _TVAE_ offer the best trade-off between high statistical fidelity and strong performance on the three downstream tasks.

**Data-centric methods can improve the utility of synthetic data.** The addition of data-centric pre- and postprocessing strategies has a generally positive effect across all tasks as seen in Table 2 and Figure 3, despite resulting in lower statistical fidelity. In terms of classification performance, 13 out of 15 evaluations showed a net improvement, with gains up to 1.64% better classification

   Generative Model &  Progressive \\ Strategy \\  & 
 Polytressing \\ Strategy \\  & Classification & Model Selection & Feature Selection & Statistical Fidelity \\  bayesian,network & baseline & no.hard & 0.31 (4.89,35.98) & 2.78 (38.22,37.) & 5.22 (16.60,42.74) & \(\)0.027 (0.054,0.046) \\ easy,hard & baseline & no.hard & 0.35 (4.98,56.00) & 2.78 (31.99,15.14) & 2.79 (10.16,12.79,26.61) & \(\)0.013 (0.042,0.027) \\ no.hard & no.hard & 0.8 (4.44,63.74) & 2.72 (31.39,90.28) & 3.99 (13.22,20.17,39.9) & \(\)0.023 (0.067,0.021) \\  ctgan & baseline & no.hard & 1.23 (2.03,44.4) & \(\)1.7 (1.92,42.8) & 3.66 (4.88,2.61) & \(\)0.054 (1.42,4.085) \\ easy,hard & baseline & 0.78 (4.14,2.88) & 1.17 (1.34,14.7) & 1.04 (1.06,10.16,10.13) & \(\)0.005 (1.06,10.80) \\ no.hard & no.hard & 0.37 (2.96,3.8) & \(\)4.73 (2.13,26.68) & \(\)0.18 (-100.04,9.43) & \(\)0.119 (-1.218,0.701) \\  ddpm & baseline & no.hard & 0.63 (3.55,4.24) & \(\)6.35 (2.01,7.86) & \(\)0.17 (1.60,13.37) & \(\)0.165 (1.942,15.964) \\ easy,hard & baseline & 0.68 (2.84,4.06) & \(\)6.70 (5.87,20.19) & \(\)1.58 (-109.12,39.22) & 0.336 (1.66,13.83,3.89) \\ no.hard & no.hard & 1.32 (-20.46,4.68) & \(\)9.78 (7.27,18.63) & \(\)4.19 (-69.56,16.16) & \(\)0.284 (-16.721,14.567) \\  nflow & baseline & no.hard & 1.05 (2.43, 3.97) & 1.12 (16.58, 18.41) & 5.82 (1.12,12.68) & \(\)0.053 (0.743,0.688) \\ easy,hard & baseline & 0.76 (2.64,3.81) & \(\)2.37 (15.26,0.63) & \(\)6.63 (3.01,25.65) & \(\)0.022 (-0.752,0.625) \\ no.hard & no.hard & 1.64 (1.76,4.81) & 4.66 (1.367,22.84) & 7.28 (-11.54,25.01) & \(\)0.052 (-0.705,0.654) \\  tvae & baseline & no.hard & 1.1 (2.39,4.38) & \(\)3.58 (1.84,11.01) & \(\)0.13 (1.64,6.38) & \(\)0.053 (-1.418,1.113) \\ easy,hard & baseline & 0.53 (3.46,3.16) & \(\)5.83 (1.94,4.75) & \(\)6.7 (6.03,0.87) & \(\)0.24 (0.941,29.96) \\ no.hard & no.hard & 0.71 (-2.59,3.95) & \(\)0.11 (-13.79,14.38) & \(\)4.41 (-3.38,9.89) & \(\)0.199 (-0.929,1.285) \\   

Table 2: Percentage increase in performance from baseline, i.e., no data-centric pre- or postprocessing (as seen in Table 1), per generative model for each pre- and postprocessing strategy, averaged across all datasets and seeds.

performance compared to not processing the data. Model selection exhibited more pronounced effects, particularly for bayesian networks, which demonstrated the greatest variability overall. While the model selection results for TVAE decreased following data-centric processing, the other generative models saw positive effects, with performance improvements ranging from 4.66% to 92%. Regarding feature selection, 12 out of 15 evaluations demonstrated a net benefit of data-centric processing, resulting in improvements of 3.41% to 9.79% in Spearman's rank correlation. The benefit of data-centric processing was found to be statistically significant for classification and feature selection (see Appendix D for details).

**Practical guidance:** Before releasing a synthetic dataset, practitioners are advised to apply the data-centric methods studied in this paper as an add-on. This will ensure enhanced utility of the synthetic data in terms of classification performance, model selection, and feature selection.

**Data-centric processing provides benefits across levels of label noise.** Data-centric pre- and postprocessing lead to consistently higher performance across tasks for all augmented datasets. As shown in Figure 4, the magnitude of the effect of data-centric processing decreases with higher levels of label noise, particularly above 8%, although this effect is not statistically significant. Even though the level of statistical fidelity decreased marginally by applying data-centric processing, data-centric processing led to statistically significant increases in performance on all three tasks.

**Practical guidance:** Fitting generative models on noisy "real-world" data can lead to sub-optimal downstream performance despite seemingly high statistical fidelity. Data-centric methods are especially useful at reasonable levels of label noise, typically below 8%. Therefore, we recommend their application when fitting generative models on real-world datasets.

### Limitations and future work

Our work delves into the performance-driven aspects of synthetic data generation, focusing primarily on data utility and statistical fidelity, particularly within the realm of tabular data. While tabular data is highly diverse and contains many intricacies, we also recognize several directions for further exploration. Our current framework, while rooted in tabular data, hints at the broader applicability to other data types such as text and images. Accommodating our framework to these modalities would require further work on modality-specific tasks. For instance, images or text do not possess a direct analog to feature selection. Such disparities underscore the need for a bespoke benchmarking methodology tailored to each specific data type.

Figure 3: Average performance across all datasets for each generative model by pre- and postprocessing method.

## 6 Conclusion

This research provides novel insights into integrating data-centric AI techniques into synthetic tabular data generation. First, we introduce a framework to evaluate the integration of data profiles for creating more representative synthetic data. Second, we confirm that statistical fidelity alone is insufficient for assessing synthetic data's utility, as it may overlook important nuances impacting downstream tasks. Third, the choice of generative model significantly influences synthetic data quality and utility. Last, incorporating data-centric methods consistently improves the utility of synthetic data across varying levels of label noise. Our study demonstrates the potential of data-centric AI techniques to enhance synthetic data's representation of real-world complexities, opening avenues for further exploration at their intersection.