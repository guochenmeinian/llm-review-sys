# Better by Default: Strong Pre-Tuned MLPs and Boosted Trees on Tabular Data

David Holzmuller

SlerRA Team, Inria Paris

Ecole Normale Superieure

PSL University

&Leo Grinsztajn

SODA Team, Inria Saclay

Ingo Steinwart

University of Stuttgart

Faculty of Mathematics and Physics

Institute for Stochastics and Applications

Work done partially while still at University of Stuttgart.

###### Abstract

For classification and regression on tabular data, the dominance of gradient-boosted decision trees (GBDTs) has recently been challenged by often much slower deep learning methods with extensive hyperparameter tuning. We address this discrepancy by introducing (a) RealMLP, an improved multilayer perceptron (MLP), and (b) strong meta-tuned default parameters for GBDTs and RealMLP. We tune RealMLP and the default parameters on a meta-train benchmark with 118 datasets and compare them to hyperparameter-optimized versions on a disjoint meta-test benchmark with 90 datasets, as well as the GBDT-friendly benchmark by Grinsztajn et al. (2022). Our benchmark results on medium-to-large tabular datasets (1K-500K samples) show that RealMLP offers a favorable time-accuracy tradeoff compared to other neural baselines and is competitive with GBDTs in terms of benchmark scores. Moreover, a combination of RealMLP and GBDTs with improved default parameters can achieve excellent results without hyperparameter tuning. Finally, we demonstrate that some of RealMLP's improvements can also considerably improve the performance of TabR with default parameters.

## 1 Introduction

Perhaps the most common type of data in practical machine learning (ML) is tabular data, characterized by a fixed number of features (columns) that can take different types such as numerical or categorical, as well as a lack of the spatiotemporal structure found in image or text data. The moderate dimension and lack of symmetries make tabular data accessible to a wide variety of machine learning methods. Although tabular data is very diverse and no method is dominant on all datasets, gradient-boosted decision trees (GBDTs) exhibit excellent results on benchmarks , although their superiority has been challenged by a variety of deep learning methods .

While many architectures for neural networks (NNs) have been proposed , variants of the simple multilayer perceptron (MLP) have repeatedly been shown to be good baselines for tabular NNs . Moreover, in terms of training time, MLPs are often slower than GBDTs but still considerably faster than many other architectures . Therefore, we study how MLPs can be improved in terms of architecture, training, preprocessing, hyperparameters, and initialization. We also demonstrate that at least some of these improvements can successfully improve TabR .

Even with fast and accurate NNs, the cost of extensive hyperparameter optimization can be problematic and hinder the adoption of new methods. To address this issue, we investigate the potential of better dataset-independent default parameters for MLPs and GBDTs. Specifically, we compare the library defaults (D) to our tuned defaults (TD) and (dataset-dependent) hyperparameter optimization (HPO). Unlike McElfresh et al. , who argue in favor HPO on GBDTs over trying NNs, our results show a better time-accuracy trade-off for trying different (tuned) default models, as is done by modern AutoML systems [10; 11].

### Contribution

The problem of finding better default parameters can be seen as a meta-learning problem . We employ a meta-train benchmark consisting of 118 datasets on which the default hyperparameters are optimized, and a disjoint meta-test benchmark consisting of 90 datasets on which they are evaluated. We consider separate default parameters for classification, optimized for classification error, and for regression, optimized for RMSE. Our benchmarks do not contain missing numerical values, and we restrict ourselves to sizes between 1K and 500K samples, cf. Section 2.

In Section 3, we introduce **RealMLP**, which improves on standard MLPs through a **bag of tricks** and **better default parameters**, tuned entirely on the meta-train benchmark. We introduce many **novel or nonstandard components**, such as preprocessing using robust scaling and smooth clipping, a new numerical embedding variant, a diagonal weight layer, new schedules, different initialization methods, etc. Our benchmark results demonstrate that it often outperforms other comparably fast NNs from the literature and can be competitive with GBDTs. To demonstrate that our bag of tricks is useful for other models, we introduce **RealTabR-D**, a version of TabR  including some of our tricks that, despite less extensive tuning, achieves excellent benchmark results.

In Section 4, we provide **new default parameters**, tuned on the meta-train benchmark, for XGBoost , LightGBM , and CatBoost . While they cannot match HPO on average, they outperform the library defaults on the meta-test benchmark.

In Section 5, we evaluate these and other models on the meta-test **benchmark** and the benchmark by Grinsztajn et al. . We also investigate several possibilities for algorithm selection and ensembling, demonstrating that algorithm selection over default methods provides a better time-performance tradeoff than HPO, thanks to our new improved default parameters and MLP.

The code for our benchmarks, including scikit-learn interfaces for the models, is available at

[https://github.com/dholzmueller/pytabkit](https://github.com/dholzmueller/pytabkit)

Our code and data are archived at [https://doi.org/10.18419/darus-4555](https://doi.org/10.18419/darus-4555).

### Related Work

Neural networksBorisov et al.  review deep learning on tabular data and identify three main classes of methods: Data transformation methods, specialized architectures, and regularization models. In particular, recent research has mainly focused on specialized architectures based on attention [1; 7; 15; 27], including attention between datapoints [17; 37; 53; 56; 60]. However, these methods are usually significantly slower than MLPs or even GBDTs [17; 18; 43]. Our research instead expands on improvements to MLPs for tabular data such as the SELU activation function , bias initialization methods , regularization methods , categorical embedding layers , and numerical embedding layers .

BenchmarksShwartz-Ziv and Armon  benchmarked three deep learning methods and noticed that they performed better on the datasets from their own papers than on other datasets. We address this issue by using more datasets and evaluating our methods on datasets that they were not tuned on. Grinsztajn et al. , McElfresh et al. , and Ye et al.  propose larger benchmarks and find that GBDTs still outperform deep learning methods on average, analyzing why and when this is the case. Kohli et al.  also emphasize the need for large benchmarks. We evaluate our methods on the benchmark by Grinsztajn et al.  as well as datasets from the AutoML benchmark  and the OpenML-CTR23 regression benchmark .

Better defaultsProbst et al.  study the tunability of ML methods, i.e., the difference in benchmark scores between the best fixed hyperparameters and tuned hyperparameters. While their approach involves finding better defaults, they do not evaluate them on a separate meta-test benchmark, only consider classification, and do not provide defaults for LightGBM, CatBoost, and NNs.

Meta-learningThe problem of finding the best fixed hyperparameters is a meta-learning problem [4; 64]. Although we do not introduce or employ a fully automated method to find good defaults, we use a meta-learning benchmark setup to properly evaluate them. Wistuba et al.  and Pfisterer et al.  learn portfolios of configurations and van Rijn et al.  learn symbolic defaults, but neither of these papers considers GBDTs or NNs. Salinas and Erickson  learn large portfolios of configurations on an extensive benchmark, without studying the best defaults for individual model families. Such portfolios are successfully applied in modern AutoML methods [10; 11]. At the other end of the meta-learning spectrum, TabPFN  meta-learns a (tuning-free) learning method on small synthetic datasets. Unlike TabPFN, we only meta-learn hyperparameters and can therefore use fewer but larger and more realistic meta-train datasets, resulting in methods that scale to larger datasets.

## 2 Methodology

To evaluate a fixed hyperparameter configuration \(\), we need a collection \(^{}\) of benchmark datasets and a scoring function that computes a benchmark score \((^{},)\) by aggregating the errors attained by the method with hyperparameters \(\) on each dataset. However, when optimizing \(\) on \(^{}\), we might overfit to the benchmark and therefore ideally need a second benchmark \(^{}\) to get an unbiased score for \(\). We refer to \(^{},^{}\) as meta-train and meta-test benchmarks and subdivide them into classification and regression benchmarks \(^{}_{}\), \(^{}_{}\), \(^{}_{}\), and \(^{}_{}\). We also use the Grinsztajn et al.  benchmark \(^{}\), which allows us to run more expensive baselines, since it limits training set sizes to 10K samples and contains fewer datasets due to more strict dataset inclusion criteria. Since \(^{}\) contains groups of datasets that are variants of the same dataset, for example by using different columns as targets, we use weighting factors inversely proportional to the group size.

Table 1 shows some characteristics of the considered benchmarks. The meta-test benchmark includes datasets that are more extreme in several dimensions, allowing us to test whether our default parameters generalize "out of distribution". For all datasets, we remove rows with missing numerical values and encode missing categorical values as a separate category.

### Benchmark Data Selection

The meta-train set consists of medium-sized datasets from the UCI Repository , adapted from Steinwart . The meta-test set consists of the datasets from the AutoML Benchmark  as well as the OpenML-CTR23 regression benchmark  with a few modifications: we subsample some large datasets and remove datasets that are already contained in the meta-train set, are too small, or have categories with too large cardinality. More details on the datasets and preprocessing can be found in Appendix C.3.

    & \(^{}_{}\) & \(^{}_{}\) & \(^{}_{}\) & \(^{}_{}\) & \(^{}_{}\) & \(^{}_{}\) \\  \#datasets & 71 & 48 & 18 & 47 & 42 & 28 \\ \#dataset groups & 46 & 48 & 18 & 26 & 42 & 28 \\ min \#samples & 1847 & 1000 & 3434 & 3338 & 1030 & 4052 \\ max \#samples & 45222 & 500000 & 500000 & 48204 & 500000 & 500000 \\ max \#classes & 26 & 355 & 2 & 0 & 0 & 0 \\ max \#features & 561 & 10000 & 419 & 520 & 4991 & 359 \\ max \#categories & 41 & 7019 & 14 & 38 & 359 & 20 \\   

Table 1: Characteristics of the meta-train and meta-test sets.

### Aggregate Benchmark Score

To optimize the default parameters, we need to define a single benchmark score. To this end, we evaluate a method on \(N_{}=10\) random training-validation-test splits (60%-20%-20%) on each dataset. As metrics on individual dataset splits, we use classification error (\(100\%-\) accuracy) or 1-AUROC(one-vs-rest) for classification and

\[}{}=}\]

for regression. There are various options to aggregate these errors into a single score. Some, such as average rank or mean normalized error, depend on which other methods are included in the evaluation, hindering an independent optimization. We would like to use the geometric mean error because arguably, an error reduction from \(0.02\) to \(0.01\) is more valuable than an error reduction from \(0.42\) to \(0.41\). However, since the geometric mean error is too sensitive to cases with zero error (especially for classification error), we instead use a _shifted geometric mean error_, where a small value \( 0.01\) is added to the errors \(_{ij}\) before taking the geometric mean:

\[_{}(_{i=1}^{N_{ }}}{N_{}}_{j=1}^{N_{}}( _{ij}+)).\]

Here, we use weights \(w_{i}=1/N_{}\) on the meta-test set and Grinsztajn et al.  benchmark. On the meta-train set, we make the \(w_{i}\) dependent on the number of related datasets, cf. Appendix C.3. In Appendix B.10, we present results for other aggregation strategies.

## 3 Improving Neural Networks

The following section presents RealMLP-TD, our improved MLP with tuned defaults, which was designed based on experiments on the meta-train benchmark. A simplified version called RealMLP-TD-S is also described. To demonstrate that our improvements can be useful for other architectures, we introduce RealTabR-D, a version of TabR that includes some of our improvements but has not been tuned as extensively as RealMLP-TD.

Data preprocessingIn the first step of RealMLP, we apply one-hot encoding to categorical columns with at most eight distinct values (not counting missing values). Binary categories are encoded to a single feature with values \(\{-1,1\}\). Missing values in categorical columns are encoded to zero. After that, all numerical columns, including the one-hot encoded ones, are preprocessed independently as follows: Let \(x_{1},,x_{n}\) be the values in column \(i\), and let \(q_{p}\) be the \(p\)-quantile of \((x_{1},,x_{n})\) for \(p\). Then,

\[x_{j,}  f(s_{j}(x_{j}-q_{1/2})),\ \ \ f(x))^{2}}},\] \[s_{j} /4-q_{1}/4}&,q_{3 /4} q_{1/4}\\ -q_{0}}&,q_{3/4}=q_{1/4}q_{1} q_{0}\\ 0&,\]

In scikit-learn , this corresponds to applying a RobustScaler (first case) or MinMaxScaler (second case), and then the function \(f\), which smoothly clips its input to the range \((-3,3)\). Smooth clipping functions like \(f\) have been used by, e.g., Holzmuller et al.  and Hafner et al. . Intuitively, when features have large outliers, smooth clipping prevents the outliers from affecting the result too strongly, while robust scaling prevents the outliers from affecting the inlier scaling.

NN architectureOur architecture, visualized in Figure 1 (a), is a multilayer perceptron (MLP) with three hidden layers containing 256 neurons each, except for the following additions and modifications:

* RealMLP-TD employs categorical embedding layers  to embed the remaining categorical features with cardinality \(>8\).
* For numerical features, excluding the one-hot encoded ones, we introduce PBLD (periodic bias linear DenseNet) embeddings, which concatenate the original value to the PL embeddings proposed by Gorishniy et al.  and use a different periodic embedding with biases,inspired by Huang et al.  and Rahimi and Recht , respectively. PBLD embeddings apply separate small two-layer MLPs to each feature \(x_{i}\) as \[(x_{i},_{}^{(2,i)}(2_{}^{(1,i)}x_{i }+_{}^{(1,i)})+_{}^{(2,i)})^ {4}.\] For efficiency reasons, we use 4-dimensional embeddings with \(_{}^{(1,i)},_{}^{(1,i)}^{16},_{}^{(2,i)}^{3},_{}^{(2,i)} ^{3 16}\).
* To encourage (soft) feature selection, we introduce a scaling layer before the first linear layer, which is simply a matrix-vector product with a diagonal weight matrix. In other words, it computes \(x_{i,}=s_{i} x_{i,}\), with a learnable scaling factor \(s_{i}\) for each feature \(i\). We found it beneficial to use a larger learning rate for this layer.
* Our linear layers use the neural tangent parametrization (NTP) as proposed by Jacot et al. , i.e., they compute \(^{(l+1)}=d_{l}^{-1/2}^{(l)}^{(l)}+^{(l)}\), where \(d_{l}\) is the dimension of the layer input \(^{(l)}\). The motivation behind the use of the NTP here is that it effectively modifies the learning rate for the weight matrices depending on the input dimension \(d_{l}\), hopefully preventing too large steps whenever the number of columns is large. We did not observe improvements when using the Adam version of the maximal update parametrization .

Figure 1: **Components of RealMLP-TD. Part (c) shows the result of adding one component in each step, where the best default learning rate is found separately for each step. The vanilla MLP uses categorical embeddings, a quantile transform to preprocess numerical features, default PyTorch initialization, ReLU activation, early stopping, and is optimized with Adam with default parameters. For more details, see Appendix A.4. The error bars are approximate 95% confidence intervals for the limit #splits \(\), see Appendix C.6.**

* RealMLP-TD uses parametric activation functions inspired by PReLU . In general, for an activation function \(\), we define a parametric version with separate learnable \(_{i}\) for each neuron \(i\): \[_{_{i}}(x_{i})=(1-_{i})x_{i}+_{i}(x_{i})\;.\] When \(_{i}=1\), this recovers \(\), and when \(_{i}=0\), the activation function is linear. As activation functions, we use SELU  for classification and Mish  for regression.
* We use dropout after each activation function. We do not use the Alpha-dropout variant originally proposed for SELU , as we were not able to obtain good results with it.
* For regression, at test time, the MLP outputs are clipped to the observed range during training. (We observed that this is mainly helpful for suboptimal hyperparameters.)

InitializationThe parameters \(s_{i}\) of the scaling layer are initialized to \(1\), making it an identity function at initialization. Similarly, the parameters \(_{i}\) of the parametric activation functions are initialized to \(1\), recovering the standard activation functions at initialization. We initialize weights and biases in a data-dependent fashion during a forward pass on the (possibly subsampled) training set. We rescale rows of standard-normal-initialized weight matrices to scale the variance of the output pre-activations over the dataset to one. For the biases, we use the data-dependent he+5 initialization method [called hull+5 in 61].

TrainingLike Gorishniy et al. , we use the AdamW optimizer . We set its momentum hyperparameters to \(_{1}=0.9\) and \(_{2}=0.95\) instead of the default \(_{2}=0.999\). The idea to use a smaller value for \(_{2}\) is adopted from the fastai tabular MLP . RealMLP is optimized for 256 epochs with a batch size of 256. As a loss function for classification, we use softmax + cross-entropy with label smoothing  with parameter \(=0.1\). For regression, we use the MSE loss and affinely transform the targets to have zero mean and unit variance on the training and validation set.

HyperparametersWe allow parameter-specific scheduled hyperparameters computed in each iteration using a base value, optional parameter-specific factors, and a schedule, as

\[\,\,\,\, (}{\#}),\]

allowing us, for example, to use a high learning rate factor for scaling layer parameters. Because we do not tune the number of epochs separately on each dataset, we use a multi-cycle learning rate schedule, providing multiple valleys that are usually preferable for stopping the training, while allowing high learning rates in between. Our schedule is similar to Loshchilov and Hutter  and Smith , but with a simpler analytical expression:

\[_{k}(t)(1-(2_{2}(1+(2^{k} -1)t)))\;.\]

We set \(k=4\) to obtain four cycles as shown in Figure 1 (b). To allow stopping at different levels of regularization, we schedule dropout and weight decay using the following schedule, cf. Figure 1 (b):2

\[(t)(1+((\{1,2t\}-1) )).\]

The detailed hyperparameters can be found in Table A.1.

Best-epoch selectionDue to the multi-cycle learning rate schedule, we do not perform classical early stopping. Instead, we always train for the full 256 epochs and then revert the model to the epoch with the lowest validation error, which in this paper is based on classification error, or RMSE for regression. In case of a tie, we found it beneficial to use the last of the tied best epochs.

RealMLP-TD-SSince certain aspects of RealMLP-TD are somewhat complex to implement, we introduce a simplified (and faster) variant called RealMLP-TD-S in Appendix A. Among the simplifications are: omitting embedding layers, using non-parametric activations, using a simpler initialization method, and omitting dropout and weight decay.

RealTabR-DFor RealTabR-D, we adapt TabR-S-D by using our numerical preprocessing, setting Adam's \(_{2}\) to \(0.95\), using our scaling layer with a modification to amplify the effective learning rate by a factor of 96, adding PBLD embeddings for numerical features, and adding label smoothing for classification. More details can be found in Appendix A.3.

## 4 Gradient-Boosted Decision Trees

To find better default hyperparameters for GBDTs, we employ a semi-automatic approach: We use hyperparameter optimization libraries like hyperopt  and SMAC3  to explore a reasonably large hyperparameter space, evaluating the benchmark score of each configuration on the meta-train benchmarks, and then perform some small manual adjustments like rounding the best obtained hyperparameters. To balance efficiency and accuracy, we fix the number of estimators to 1000 and use the hist method for XGBoost. We only consider the libraries' default tree-building strategies since it is one of their main differences. The tuned defaults (TD) for LightGBM (LGBM), XGBoost (XGB), and CatBoost can be found in Table C.1, C.2, and C.3, respectively.

While some of the obtained hyperparameter values might be sensitive to the tuning and benchmark setup, we observe some general trends. First, row subsampling is used in all tuned defaults, while column subsampling is rarely applied. Second, trees are generally allowed to be deeper for regression than for classification. Third, the Bernoulli bootstrap in CatBoost is competitive with the Bayesian bootstrap while also being faster.

## 5 Experiments

In the following, we evaluate different methods with library defaults (D), tuned defaults (TD), and hyperparameter optimization (HPO). Recall that TD uses fixed parameters optimized on the meta-train benchmarks, while HPO tunes hyperparameters on each dataset split independently. All methods except random forests select the best iteration/epoch on the validation set of the respective dataset split based on accuracy / RMSE. All NN-based regression methods standardize the labels for training.

### Methods

We provide methods in the following variants:

* **D**: Default parameters, taken from the original library if possible (Appendix C.1).
* **TD**: Tuned default parameters from Section 3 and Section 4.
* **HPO**: Hyperparameters optimized separately for every train-test split on every dataset, using 50 steps of random search. Search spaces are specified in Appendix C.2 and are usually adapted from original or popular papers.

As tree-based methods, we use XGBoost (**XGB**), LightGBM (**LGBM**), and **CatBoost** from the respective libraries, as well as random forest (**RF**) from scikit-learn. The variant **XGB-PBB-D** uses meta-learned default parameters from Probst et al. . For neural methods, we compare to **MLP**, **ResNet**, and FT-Transformer (**FTT**) from Gorishniv et al. , **MLP-PLR** from Gorishniv et al. , as well as **TabR** and **TabR-S** (without numerical embeddings) from Gorishniv et al. . We compare these methods to **RealMLP** and **RealTabR** from Section 3. In addition, we investigate **Best**, which on each dataset split selects the method with the best validation score out of XGB, LGBM, CatBoost, and MLP-PLR (for Best-D) or RealMLP (for Best-TD and Best-HPO). **Ensemble** builds a weighted ensemble out of the same methods as Best, using the method of Caruana et al.  with 40 greedy selection steps as in Salinas and Erickson .

We do not run FTT, RF-HPO, and TabR-HPO on all benchmarks since some benchmarks (especially meta-test) are more expensive to run and these methods may run into out-of-memory errors.

### Results

Figure 2 shows the results of the aforementioned methods on all benchmarks, along with their runtimes on a CPU. _Note that XGB results on some (mainly meta-test) datasets are affected by a bug in handling rare categories, see Appendix B._

Figure 2: **Benchmark scores on all benchmarks vs. average training time.** The \(y\)-axis shows the shifted geometric mean (\(_{}\)) classification error (left) or nRMSE (right) as explained in Section 2.2. The \(x\)-axis shows average training times per 1000 samples (measured on \(^{}\) for efficiency reasons), see Appendix C.7. The error bars are approximate 95% confidence intervals for the limit #splits \(\), see Appendix C.6. _Note that XGB results on some (mainly meta-test) datasets are affected by a bug in handling rare categories, see Appendix B._

How good are tuned defaults on new datasets?To answer this question, we compare the relative gaps between TD and HPO benchmark scores on the meta-test benchmarks to those on the meta-train benchmarks. The gap between RealMLP-HPO and RealMLP-TD is not much larger on the meta-test benchmarks, indicating that the tuned defaults transfer very well to the meta-test benchmark. For GBDTs, tuned defaults are competitive with HPO on the meta-train set, but not as good on the meta-test set. Still, they are considerably better than the untuned defaults on the meta-test set. Note that we did not limit the TD parameters to the literature search spaces for the HPO models (cf. Appendix C.2); for example, XGB-TD uses a smaller value of min_child_weight for classification and CatBoost-TD uses deeper trees and Bernoulli boosting. The XGBoost defaults XGB-PBB-D from Probst et al.  outperform XGB-TD on \(^{}_{}\), perhaps because their benchmark is more similar to \(^{}_{}\) or because XGB-PBB-D uses more estimators (4168) and deeper trees.

RealMLP and RealTabR perform strongly among NNs.On most benchmarks, RealMLP-TD and RealTabR-D bring considerable improvements over MLP-PLR-D and TabR-S-D, at slightly larger runtimes, respectively. Similarly, RealMLP-HPO improves the results of MLP-PLR-HPO. TabR and FTT are notably slower than MLP-based methods on CPUs, while the difference is less pronounced on GPUs (Figure C.2). While RealMLP-TD beats TabR-S-D on many benchmarks, RealTabR-D performs even better on four out of six benchmarks, especially all regression benchmarks. On the Grinsztajn et al.  benchmark where we can afford to run more baselines, TabR-HPO performs best according to many aggregation metrics. It performs especially well on the _electricity_ dataset, where MLPs struggle to learn high-frequency patterns .

RealMLP and RealTabR are competitive with tree-based models.On the meta-train and meta-test benchmarks, RealMLP and RealTabR perform better than GBDTs in terms of shifted geometric mean error, while also being comparable or slightly better in terms of other aggregations like mean normalized error (Appendix B.10) or win-rates (Appendix B.12). On the Grinsztajn et al.  benchmark, RealMLP performs worse than CatBoost for classification and comparably for regression, while RealTabR-D performs comparably to CatBoost-TD for classification and better for regression.

Among GBDTs, CatBoost defaults are better and slower.Several papers have found CatBoost to perform favorably among GBDTs while being more computationally expensive to train . We observe the same for our tuned defaults on most benchmarks.

Figure 3: **Benchmark scores vs. average training time for AUC. Methods labeled “no LS” deactivate label smoothing. Stopping and best-epoch selection are performed on accuracy, while HPO is performed on AUC. See Figure B.3 for stopping on cross-entropy. The \(y\)-axis shows the shifted geometric mean (\(_{e}\)) \(1-\) as explained in Section 2.2. The \(x\)-axis shows average training times per 1000 samples (measured on \(^{}\) for efficiency reasons), see Appendix C.7. The error bars are approximate 95% confidence intervals for the limit #splits \(\), see Appendix C.6.**

Simply trying all default algorithms is faster and very often better than (naive) single-algorithm HPO.When comparing Best-TD to 50-step HPO on RealMLP or GBDTs, we notice that Best-TD is faster on average, while also being competitive with the best of the HPO models. In comparison, Best-D is often outperformed by RealMLP-HPO. We also note that ensemble selection  usually gives 0-3% improvement on the benchmark score compared to selecting the best model, and can potentially be further improved . Unlike McElfresh et al. , who argue in favor of CatBoost-HPO over trying NNs, our results favor model portfolios as used in modern AutoML systems .

Analyzing NN improvementsFigure 1 (c) shows how adding the proposed RealMLP components to a simple MLP improves the meta-train benchmark performance. However, these results depend on the order in which components are added, which is addressed by a separate ablation study in Appendix B. For example, the large weight decay value makes RealMLP-TD sensitive to changes in some other hyperparameters like \(_{2}\). We also show in Appendix B.8 that our architectural improvements alone are beneficial when applied to MLP-D directly, although non-architectural aspects are at least as important. In particular, our numerical preprocessing is easy to adopt and often beneficial for other NNs as well (Appendix B.7). The scaling layer and PBLD embeddings are easy to use and turned out to be effective within RealTabR-D as well. If affordable, larger stopping patiences and the use of (cyclic) learning rate schedules can be useful, while label smoothing is influential but can be detrimental for metrics like AUROC (Figure 3, Appendix B.5).

Dependence on benchmark choicesWe observe that choices in benchmark design can affect the interpretation of the results. The use of different aggregation metrics than the shifted geometric mean reduces the advantage of TD methods (Appendix B.10). For classification, using AUROC instead of classification error (Figure 3, Appendix B.5) favors GBDTs. Different dataset selection and preprocessing criteria on different benchmarks lead to large differences between benchmarks in the average errors, as indicated by the \(y\)-axis scaling in Figure 2.

Further insightsIn Appendix B, we present additional experimental results. We compare bagging and refitting for RealMLP-TD and LGBM-TD, finding that refitting multiple models is often better on average. We demonstrate that GBDTs benefit from high early stopping patiences for classification, especially when using accuracy as the stopping metric. When considering AUROC as a stopping metric, we show that stopping on cross-entropy is preferable to accuracy (Appendix B.5).

LimitationsWhile our benchmarks cover medium-to-large tabular datasets in standard settings, it is unclear to which extent the obtained defaults can generalize to very small datasets, distribution shifts, datasets with missing numerical values, and other metrics such as log-loss. Additionally, runtimes and the resulting tradeoffs may change with different parallelization, hardware, or (time-aware) HPO algorithms. For computational reasons, we only use a single training-validation split per train-test split. This means that HPO can overfit the validation set more easily than in a cross-validation setup. While we extensively benchmark different NN models from the literature, we do not attempt to equalize non-architectural aspects, and our work should therefore not be seen as a comparison of architectures. We compared to TabR-S-D as a recent promising method with good default parameters . However, due to a surge of recently published deep tabular models [e.g., 7, 8, 29, 33, 41, 57, 67], it is unclear what the current "best" deep tabular model is. In particular, ExcelFormer  also promises strong-performing default parameters. For GBDTs, due to the cost of running the benchmarks, our limits on the depth and number of trees are on the lower side of the literature.

## 6 Conclusion

In this paper, we studied the potential of improved default parameters for GBDTs and an improved MLP, evaluated on a large separate meta-test benchmark as well as the benchmark by Grinsztajn et al. , and investigated the time-accuracy tradeoffs of various algorithm selection and ensembling scenarios. Our improved MLP mostly outperforms other NNs from the literature with moderate runtime and is competitive with GBDTs in terms of benchmark scores. Since many of the proposed improvements to NNs are orthogonal to the improvements in other papers, they offer exciting opportunities for combinations, as we demonstrated with our RealTabR variant. While the "NNs vs GBDTs" debate remains interesting, our results demonstrate that with good default parameters, it is worth trying both algorithm families even with a moderate training time budget.