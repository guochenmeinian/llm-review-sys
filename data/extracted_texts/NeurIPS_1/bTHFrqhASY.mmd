# InfLLM: Training-Free Long-Context Extrapolation

for LLMs with an Efficient Context Memory

 Chaojun Xiao\({}^{1}\), Pengle Zhang\({}^{1}\), Xu Han\({}^{2,1,3}\), Guangxuan Xiao\({}^{4}\),

Yankai Lin\({}^{5}\), Zhengyan Zhang\({}^{1}\), Zhiyuan Liu\({}^{1}\), Maosong Sun\({}^{1}\)

\({}^{1}\)NLP Group, DCST, IAI, BNRIST, Tsinghua University

\({}^{2}\)Quan Cheng Laboratory \({}^{3}\)Shanghai Artificial Intelligence Laboratory

\({}^{4}\)Massachusetts Institution of Technology \({}^{5}\)Renmin University of China

xiaocj20@mails.tsinghua.edu.cn, {hanxu2022,liuzy}@tsinghua.edu.cn

Equal contribution.Corresponding authors.

###### Abstract

Large language models (LLMs) have emerged as a cornerstone in real-world applications with lengthy streaming inputs (e.g., LLM-driven agents). However, existing LLMs, pre-trained on sequences with a restricted maximum length, cannot process longer sequences due to the out-of-domain and distraction issues. Common solutions often involve continual pre-training on longer sequences, which will introduce expensive computational overhead and uncontrollable change in model capabilities. In this paper, we unveil the intrinsic capacity of LLMs for understanding extremely long sequences without any fine-tuning. To this end, we introduce a training-free memory-based method, InfLLM. Specifically, InfLLM stores distant contexts into additional memory units and employs an efficient mechanism to lookup token-relevant units for attention computation. Thereby, InfLLM allows LLMs to efficiently process long sequences with a limited context window and well capture long-distance dependencies. Without any training, InfLLM enables LLMs that are pre-trained on sequences consisting of a few thousand tokens to achieve comparable performance with competitive baselines that continually train these LLMs on long sequences. Even when the sequence length is scaled to \(1,024\)K, InfLLM still effectively captures long-distance dependencies. Our code can be found at [https://github.com/thunlp/InfLLM](https://github.com/thunlp/InfLLM).

## 1 Introduction

Recently, large language models (LLMs) have achieved profound accomplishments in various tasks (Brown et al., 2020; Bommasani et al., 2021; Han et al., 2021; Touvron et al., 2023; Meta, 2024). Their ability to follow complex instructions shed light on the realization of artificial general intelligence (OpenAI, 2023; Ouyang et al., 2022). With the blooming of LLM-driven applications, such as agent construction (Park et al., 2023; Qian et al., 2023; Wang et al., 2024) and embodied robotics (Driess et al., 2023; Liang et al., 2023), enhancing the capability of LLMs to process streaming long sequences become increasingly crucial. For instance, LLM-driven agents are required to process information continuously received from external environments based on all their historical memories, necessitating a robust capability for handling long streaming sequences.

Due to limitations caused by unseen lengthy inputs (Han et al., 2023) and distracting noisy contexts (Liu et al., 2023; Tworkowski et al., 2023), most LLMs, pre-trained on sequences consisting of only a few thousand tokens, cannot process longer sequences (Press et al., 2022; Zhao et al., 2023).

Common solutions usually involve continually training LLMs on longer sequences but further result in substantial costs and require large-scale high-quality long-sequence datasets (Xiong et al., 2023; Li et al., 2023). And the continual training process on longer sequences may weaken the performance of LLMs on short contexts (Ding et al., 2024). In view of this, improving the length generalizability of LLMs without further training receives extensive attention, trying to make LLMs trained on short sequences directly applicable to long sequences.

In this paper, we propose a training-free memory-based approach, named InfLLM, for streamingly processing extremely long sequences with limited computational costs. Specifically, InfLLM incorporate the sliding window attention (Xiao et al., 2023; Han et al., 2023) with an efficient context memory, where each token only attends to local contexts and relevant contexts from the memory. Considering the sparsity of attention score matrices, processing each token typically requires only a small portion of its contexts (Zhang et al., 2023b), and the remaining irrelevant contexts act as noise, leading to attention distraction issues (Tworkowski et al., 2023). We thus construct an external memory containing distant context information. Only relevant information within the memory is selected for each computation step, and other irrelevant noises are ignored. Owing to this, LLMs can understand whole long sequences using a finite-size window and avoid noisy contexts.

The vast amount of noisy context tokens in long sequences poses significant challenges to effective and efficient memory lookup. To address these challenges, we design a block-level context memory mechanism. Specifically, InfLLM organizes past key-value vectors into blocks, each containing a continuous token sequence. Within each block, the semantically most significant tokens that receive the highest attention scores are selected as the unit representation for subsequent relevance computation in memory lookup. This design offers two primary benefits: (1) Effective Lookup: The coherent semantics of each block can more effectively fulfill the requirements for relevant information retrieval compared to single tokens. The selection of unit representations minimizes the interference of unimportant tokens in relevance computation, enhancing the overall hit rate of memory lookup. (2) Efficient Lookup: The block-level memory unit eliminates the need for per-token relevance computation, significantly reducing computational costs. Moreover, block-level units ensure contiguous memory access, thus minimizing memory loading costs and enhancing computational efficiency. Furthermore, considering the infrequent usage of most units, InfLLM offloads all units on CPU memory and dynamically retains the frequently used units on GPU memory, significantly reducing GPU memory usage. Notably, the block-level memory mechanism in InfLLM does not involve any additional training, and can be directly applied to any LLMs.

To evaluate the effectiveness of InfLLM, we employ Mistral-7B-inst-v0.2 (Jiang et al., 2023) and Llama-3-8B-Instruct (Meta, 2024) as base models, which are pre-trained on the sequences containing no more than \(32\)K and \(8\)K tokens. We use two widely-used benchmarks, \(\)-Bench (Zhang et al., 2023a) and Longbench (Bai et al., 2023), for evaluation. Especially, the average sequence length in \(\)-Bench exceeds \(100\)K tokens, which is challenging for most existing LLMs. Compared to typical methods that continually train LLMs on longer sequences, the experimental results demonstrate that InfLLM enables the LLMs pre-trained on the sequences containing a few thousand tokens to achieve comparable performance without any additional training. Moreover, we examine InfLLM on the sequences containing \(1,024\)K tokens, and InfLLM can still effectively capture long-distance dependencies, demonstrating the potential of InfLLM in scenarios involving long streaming inputs.

## 2 Related Work

Enabling LLMs to process long sequences has been extensively studied (Dong et al., 2023; Tay et al., 2023; Huang et al., 2023) and can generally be categorized into two main approaches: context length extrapolation and efficient context computation. The former aims to enable LLMs trained on short sequences to process much longer sequences. The latter focuses on enhancing the computational efficiency of attention layers, allowing efficient pre-training LLMs from scratch to process longer sequences. Although the focus of this paper is context length extrapolation, we also detailedly introduce efficient context computation. We also present the relevant works for memory-based models.

**Context Length Extrapolation.** Due to the high computational and memory requirements, the training of LLMs is often restricted to short sequences. Directly applying LLMs to long sequences will suffer from out-of-domain and distraction challenges caused by lengthy and noisy inputs (Hanet al., 2023; Tworkowski et al., 2023). Consequently, context length extrapolation has garnered attention as a method to improve the sequence length for LLMs without incurring additional training. The earliest approaches involve designing new relative positional encoding mechanisms during pre-training (Press et al., 2022; Sun et al., 2023). Subsequent studies mainly focus on the widely-used rotary position embedding (RoPE) (Su et al., 2021), and propose to achieve length extrapolation by downscaling or reusing the original position indices (Chen et al., 2023; Peng et al., 2023; Chen et al., 2023; Jin et al., 2024; An et al., 2024). These works can alleviate the out-of-domain issue from the unseen length, but can not alleviate the distraction challenge of noisy contexts. To address this, Xiao et al. (2023) and Han et al. (2023) employ the sliding window attention mechanism and directly discard all distant contexts to streamingly read extremely long sequences. However, as these models overlook information from distant tokens, they can not capture the long-distance dependencies for long-text understanding. In this paper, InfLLM utilizes the sliding window attention mechanism, and additionally constructs an efficient context memory to provide LLMs with relevant context information, enabling LLMs to effectively read and understand extremely long sequences.

**Efficient Context Computation.** The quadratic computational complexity of the attention layers is a primary factor limiting the lengthy sequence-processing capabilities of LLMs. Thus, numerous scholars have endeavored to design efficient attention mechanisms, including the utilization of sparse attention (Zaheer et al., 2020; Beltagy et al., 2020; Child et al., 2019; Ainslie et al., 2020; Zhao et al., 2019), approximating attention computations using kernel functions (Kitaev et al., 2020; Wang et al., 2020; Katharopoulos et al., 2020), and replacing the attention layer with linear-complexity state-space models (Gu et al., 2022; Gu & Dao, 2023). These approaches necessitate a modification in the model architecture, requiring retraining the models. Simultaneously, many researchers enhance the inference efficiency by evicting useless key-value vectors to reduce computation (Zhang et al., 2023; Li et al., 2024; Ge et al., 2023). These methods can not extrapolate the context window of LLMs without further training due to out-of-domain issues caused by unseen positions. Recently, some researchers begin to explore the intrinsic sparse attention patterns of long-context LLMs and discard the redundant attention computation for acceleration (Jiang et al., 2024).

**Memory-based Models.** Memory networks have been studied for decades, which are proven effective in providing models with additional knowledge and information storage capabilities (Graves et al., 2014; Weston et al., 2015; Sukhbaatar et al., 2015; Miller et al., 2016). With the success of pre-trained models, memory layers have also been gradually applied in the training processes of recurrent transformer layers, enabling models to process long sequences recursively (Dai et al., 2019; Rae et al., 2020; Khandelwal et al., 2020; Wu et al., 2022; Bertsch et al., 2023; Munkhdalai et al., 2024). These works split sequences into segments, encoding each segment individually, and use memory to store context information from preceding segments. While these approaches are similar in concept to InfLLM, they involve modifications to the model architecture and requires further training the whole model. Besides, most existing memory-based methods focus on token-level memory units (Wu et al., 2022; Bertsch et al., 2023), which require a lot of time to build retrieval indexes for large-scale tokens in each input long sequence. Some methods also adopt block-level memory (Mohtashami & Jaggi, 2023; Tworkowski et al., 2023), these methods highlight the process of training effective block representations with long sequence data. In contrast, we aim to explore the inherent characteristics of LLMs, and propose a training-free memory module for long-text understanding.

## 3 Methodology

As shown in Figure 1, InfLLM builds a training-free context memory to efficiently provide highly-relevant contexts for each token, endowing the sliding window attention mechanism with the ability to capture long-distance dependencies.

### Overall Framework

The main restrictions for improving the length generalizability of LLMs come from the out-of-domain and distraction issues caused by the lengthy and noisy contexts. To address these, following previous works (Xiao et al., 2023; Han et al., 2023), we adopt the sliding window attention mechanism, which only considers local tokens for each step. Additionally, we construct an extra context memory module to provide relevant context information to capture long-distance dependencies.

Specifically, we denote the long input sequence as \(s=\{t_{i}\}_{i=1}^{l}\). Due to the limited GPU memory, instead of encoding the whole \(s\) at once, we encode the input sequence \(s\) chunk-by-chunk and generate the output token-by-token. For each computation step, the inputs consist of past key-value vectors \(=\{(_{j},_{j})\}_{j=1}^{l_{P}}\) and current tokens \(=\{_{i+l_{P}}\}_{i=1}^{l}\). For encoding steps, \(l_{X}\) equals the chunk size, and for decoding steps, \(l_{X}\) equals one.

According to the distances from current tokens, we can divide \(\) into three groups: initial tokens, \(=_{[1:l_{I}]}\), evicted tokens, \(=_{[l_{I}+1:l_{P}-l_{L}]}\), and local tokens, \(=_{[l_{P}-l_{L}+1:l_{P}]}\), arranged from the furthest to the nearest relative to the current tokens. Here, \(l_{P}\), \(l_{I}\), \(l_{L}\) refer to the length of past key-value vectors, initial tokens, and the local window size. All evicted tokens, \(\), are stored in the context memory, consisting of multiple memory units. For each step, InfLLM concatenates the initial tokens, relevant memories units from context memory, and local tokens to form the current key-value cache, \(=(,f(,),)\). \(f()\) refers to the lookup operation of context memory. The attention output is calculated as:

\[=[,(_{k},),(_{v},) ].\]

Here, \(\), \(\), and \(\) are parameters in attention layers, \(_{k}\) and \(_{v}\) refer to the key and value vectors in \(\). If \(f()\) always returns empty sets, InfLLM is degenerated into LM-Infinite (Han et al., 2023) and Streaming-LLM (Xiao et al., 2023), which directly discards distant contexts.

### Context Memory

Previous findings indicate that the attention score matrices of LLMs are sparse, and we can generate the same outputs with only a small portion of key-value vectors preserved (Zhang et al., 2023b). Inspired by this, we design a context memory to efficiently look up relevant contexts from large-scale evicted tokens and ignore irrelevant ones to save computational costs. The most intuitive way is to construct a memory consisting of token-level memory units for every past key-value vectors, and every attention head separately, which would result in massive memory units, unacceptable computation, and non-contiguous memory access costs. Thus, considering the local semantic coherence of long sequences, we split the past key-value vectors into blocks, each serving as a memory unit, and conduct memory lookup at the block level to reduce the costs while preserving the performance.

In this subsection, we will introduce the details of the block-level memory units. Then we present the method to assign positional embeddings for selected relevant memory units and cache management for the context memory.

**Block-Level Memory Units.** Block-level memory units can save computation costs compared to token-level ones. It also poses new challenges for unit representations, which are supposed to contain the semantics of the entire unit for effective relevance score computation and be memory-efficient for context length scalability. Traditional methods usually involve training an additional encoder

Figure 1: The illustration of InfLLM. Here, the current tokens refer to tokens that need to be encoded in the current computation step. The past key-value vectors can be divided into the initial tokens, evicted tokens, and local tokens, arranged the furthest to the nearest relative to the current tokens. For each computation step, the context window consists of the initial tokens, relevant memory units, and local tokens.

to project a given unit into a low-dimension vector. Inspired by the token redundancy in hidden states (Goyal et al., 2020; Dai et al., 2020), we select several **representative tokens** from the entail blocks as the unit representation. For the \(m\)-th token, we define the representative score as:

\[r_{m}=}_{j=1}^{l_{L}}_{m+j}_{m},\]

where \(_{m+j}\) is the query vector for \((m+j)\)-th token and \(_{m}\) is the key vector \(m\)-th token. Intuitively, \(r_{m}\) represents the significance of the \(m\)-th token in its corresponding local window, indicating the extent of its influence on other tokens within the local window. The computation of representative scores requires no additional parameters.

Formally, given the evicted tokens, \(\), we split it into several memory units, each containing \(l_{bs}\) tokens. For each unit, the \(r_{k}\) tokens with the highest representative scores are selected as representative tokens. Generally, \(r_{k}\) is a small positive integer. Let us denote a memory unit as \(=\{(_{j}^{B},_{j}^{B})\}_{j=1}^{l_{bs}}\), and the representative tokens of this unit as \(R()=\{(_{bj}^{B},_{bj}^{B})\}_{j=1}^{r_{k}}\).

For the **memory lookup** phrase, only \(k_{m}\) units with the highest relevance scores are loaded for the current attention computation. We calculate the relevance score between \(\) and current tokens \(\) as:

\[(,)=_{i=1}^{l_{X}}_{j=1}^{r_{k}}_{i+l_{p}}_{bj}^{B}.\]

Notably, the representative tokens selection is a training-free method to obtain the unit representations. Here, we can also train an additional encoder to generate more expressive unit representations, which we leave for future work.

**Positional Encoding.** Existing LLM training usually employs a finite number of positional encodings, which encounter out-of-domain distribution challenges when directly applied to longer sequence processing (Han et al., 2023). Besides, in InfLLM, the current key-value cache is composed of some discontinuous text blocks, and directly assigning continuous positional encodings to them would also lead to mismatch issues and confuse the model. Therefore, inspired by previous works (Raffel et al., 2020; Su, 2023), we assign all tokens beyond the local window size with the same positional encodings. Specifically, the distance between tokens in context memory units and current tokens is set as \(l_{L}\).

**Cache Management.** To enable LLMs to process extremely long sequence streams while capturing the semantic relevance contained in the long contexts, we need to retain all memory units and look up them at each computation step. Considering the infrequent usage of most units, we employ an offloading mechanism, storing most memory units in CPU memory and only preserving the representative tokens and memory units needed in current steps in GPU memory. Additionally, given the semantic coherence of long sequences, where adjacent tokens often require similar memory units, we allocate a cache space in GPU memory, managed using a least recently used strategy. This approach allows for efficient encoding of extremely long sequences using limited GPU memory. From the observation, our offloading mechanism enables InfLLM to process sequences consisting of \(100\)K tokens with only 26G VRAM. Besides, the miss rate of our GPU cache is quite low, which means the offloading mechanism does not introduce significant time overhead in memory loading while saving GPU memory usage. The details can be found in the Appendix.

Furthermore, for extremely long sequences, the representative tokens of each unit can also be offloaded to the CPU memory, constructing an efficient k-nearest-neighbor index, and thereby further reducing computational complexity.

## 4 Experiments

### Settings

**Datasets.** We adopt representative tasks in a widely-used long document benchmark, \(\)-Bench (Zhang et al., 2023a) for evaluation. We adopt the English datasets for evaluation as the base models are mainly pre-trained on English corpus. The datasets in \(\)-Bench cover diverse tasksincluding question answering, summarization, context retrieval, and mathematic computing. The average length for \(\)-Bench is \(145.1\)K. The \(95\)% quantile for sequence lengths is \(214\)K, which is far beyond the maximum length of the base models. Detailed statistics and task descriptions of these datasets are listed in the Appendix. Besides, we also conduct an evaluation on LongBench Bai et al. (2023). The results for LongBench can be found in the Appendix.

**Baseline Models.** To verify the effectiveness of our proposed method, we compare InfLLM with the following competitive baseline models: (1) **Original** models: we present the performance of the original LLMs without context length extrapolation. (2) Position downscaling and resuing: NTK-aware scaled RoPE (**NTK**) LocalLLaMA (2023) designs a nonlinear interpolation method, which basically changes the rotation base of RoPE. **SelfExtend** reuse the position ids across neighboring tokens, which makes the extended relative positions in the scope of the training context window. (3) Sliding window: these methods apply the sliding window mechanism to discard distant contexts, including LM-Infinite (**Infinite**) Han et al. (2023) and StreamingLLM (**Stream**) Xiao et al. (2023). Therefore, for each attention computation step, the input length does not exceed the context window. (5) Key-value eviction: KV eviction methods aim to discard useless key-value vectors during long sequence processing and thus are usually used to reduce the computation complexity. We present the results of a widely-used key-value eviction method, **H2O**Zhang et al. (2023). The key-value eviction method cannot generalize to longer sequences due to the unseen position embeddings and is expected to achieve unsatisfactory performance.

Here, InfLLM and the models with the sliding window mechanism can be used to process extremely long streaming inputs. For NTK and SelfExtend, we extend the context window to \(128\)K, which enables LLMs to process most instances in \(\)-Bench.

### Implementation Details

In this paper, we aim to enable LLMs trained with limited sequence length to read and understand extremely long sequences without further training. We adopt Mistral-7B-Instruct-v0.2 Jiang et al. (2023) and Llama-3-8B-Instruct Meta (2024) as our base models. The maximum length of Mistral-7B-Instruct-v0.2 and Llama-3-8B-Instruct is \(32\)K and \(8\)K, respectively.

For our model, we set the encoding chunk size as \(512\), and the memory unit size for past key-value vectors, \(l_{bs}\), as \(128\). The number of representative tokens, \(r_{k}\), is set as \(4\). For both Mistral-based and Llama-3-based InfLLM, we set the local window size as \(4\)K. For Mistral-based InfLLM, we load \(96\) relevant memory units for each step, and for Llama-3-based InfLLM, we load \(32\) relevant memory units. The number of initial tokens is set as \(128\) for LM-Infinite, StreamingLLM, and InfLLM to

    & Window & Streaming & R.PK & R.Num & R.KV & Choice & QA & Sum & Math.F & Avg. \\   \\  Mistral & 32K & ✗ & 28.8 & 28.8 & 14.8 & 44.5 & 12.9 & 25.9 & 20.6 & 25.2 \\ NTK & 128K & ✗ & 100.0 & 86.8 & 19.2 & 40.2 & 16.9 & 20.3 & 26.9 & 44.3 \\ SelfExtend & 128K & ✗ & 100.0 & 100.0 & 15.6 & 42.8 & 17.3 & 18.8 & 19.1 & 44.8 \\ Infinite & 32K & ✓ & 28.8 & 28.8 & 0.4 & 42.8 & 11.4 & 22.5 & 16.3 & 21.6 \\ Streaming & 32K & ✓ & 28.8 & 28.5 & 0.2 & 42.4 & 11.5 & 22.1 & 16.9 & 21.5 \\ H2O & 32K & ✓ & 8.6 & 4.8 & 2.6 & 48.0 & 15.6 & 24.4 & 26.9 & 18.7 \\  InfLLM & 16K & ✓ & 100.0 & 96.1 & 96.8 & 43.7 & 15.7 & 25.8 & 25.7 & 57.7 \\   \\  Llama-3 & 8K & ✗ & 8.5 & 7.8 & 6.2 & 44.1 & 15.5 & 24.7 & 21.7 & 18.4 \\ NTK & 128K & ✗ & 0.0 & 0.0 & 0.0 & 0.0 & 0.4 & 6.4 & 2.6 & 1.3 \\ SelfExtend & 128K & ✗ & 100.0 & 100.0 & 0.2 & 19.7 & 8.6 & 14.7 & 22.6 & 38.0 \\ Infinite & 8K & ✓ & 6.8 & 7.6 & 0.2 & 41.5 & 14.6 & 20.8 & 20.6 & 16.0 \\ Streaming & 8K & ✓ & 8.5 & 8.3 & 0.4 & 40.6 & 14.3 & 20.4 & 21.4 & 16.3 \\ H2O & 8K & ✓ & 2.5 & 2.4 & 0.0 & 0.0 & 0.7 & 2.8 & 6.0 & 2.1 \\  InfLLM & 8K & ✓ & 100.0 & 99.0 & 5.0 & 43.7 & 19.5 & 24.3 & 23.7 & 45.0 \\   

Table 1: The results of InfLLM and baseline models on \(\)-Bench. The \(95\)% quantile for text lengths in \(\)-Bench is \(214\)K. The context window size for sliding window models refers to the local window size, and for InfLLM refers to “local window size + selected memory size”.

cover the system prompts and task descriptions. We adopt FlashAttention (Dao, 2023) to accelerate experiments for all baseline models. Please refer to the Appendix for more details.

### Main Results

The results for Mistral-based models and Llama-3-based models are reported in Table 1. From the results, we can observe that: (1) Compared to models with the sliding window mechanism, which can also read extremely long sequences, our method demonstrates a significant performance improvement. This indicates that the context memory in InfLLM can accurately supplement LLMs with relevant contextual information, enabling efficient and effective understanding and reasoning on long sequences. (2) The position downscaling and resuing methods, NTK and SelfExtend, tend to compromise model performance while extending the sequence length to \(128\)K. That is because these models cannot address the distraction issue caused by noisy contexts. In contrast, our model can consistently enhance performance for extremely long sequences. We successfully generalize Llama-3 from a \(8\)K length to more than \(16\) times its length, achieving commendable performance on the \(\)-Bench. (3) The position downscaling and resuing methods can increase the maximum sequence length of LLMs but also raise the computational and memory costs, limiting these methods' application. In contrast, InfLLM utilizes block-level memory and offloading mechanism, enabling efficient processing of long sequences within limited resources.

### Comparing to Models with Continual Training

In this paper, we focus on expanding the context window of LLMs without additional training. In this section, we compare InfLLM with models that undergo continual training on long sequences in terms of both performance and efficiency. Specifically, we select Llama-3-8B-Instruct-Gradient-1048k (Llama-1M)3, which have been further fine-tuned on long-text data and chat datasets, extending its context window to \(1048\)K. Besides, we also employ InfLLM on the Llama-1M, where we set the local window as \(4\)K and selected memory size as \(4\)K. We present the results on \(\)-Bench, the GPU memory usage, and time consumption in Table 2. From the results, we can observe that: (1) Compared to models that have undergone continual training on long sequences, InfLLM can achieve comparable or even superior results without any additional training. This suggests that LLMs inherently possess the capability to identify key information in long sequences and to understand and reason effectively. Notably, Llama-1M requires 512 GPUs for continual training, which is unaffordable for many researchers. In contrast, InfLLM does not require any training, which indicates the practicability of InfLLM. (2) In terms of efficiency, InfLLM achieves a \(34\)% decrease in time consumption while using only \(34\)% of the GPU memory compared to the full-attention models. Moreover, at longer sequence lengths of \(256\)K tokens, the full-attention baseline fails due to out-of-memory errors, while InfLLM can efficiently process sequences up to \(1024\)K tokens on a single GPU. (3) InfLLM can also be directly combined with the model with continual training and achieve comparable or even superior results with only 8K context window. It indicates that InfLLM can also serve as an efficient way to improve the inference speed.

### Comparing to Retrieval-Augmented Generation

InfLLM leverages the intrinsic capacity of LLMs to construct a context memory for gathering token-relevant information, a concept similar to retrieval augmented generation (RAG) (Lewis et al., 2020; Nakano et al., 2021). However, compared to using RAG, where historical contexts are treated

    & Train-Free & R.PK & R.Num & R.KV & Choice & QA & Sum & Math.F & VRAM & Time \\  Llama-1M & ✗ & **100.0** & **99.8** & **23.2** & **51.5** & 13.6 & 18.5 & 18.3 & 76.6G & 40.4s \\ InfLLM & ✓ & **100.0** & 99.0 & 5.0 & 43.7 & **19.5** & **24.3** & **23.7** & **26.3**G & **26.7s** \\  Llama-1M+InfLLM & ✗ & 100.0 & 100.0 & 55.8 & 39.3 & 20.3 & 17.1 & 31.4 & 26.3G & 26.7s \\   

Table 2: The comparison between InfLLM and models with continual pre-training, Llama-3-8B-Instruct-Gradient-1048k (Llama-1M). InfLLM can achieve comparable performance with Llama-1M with less computation consumption and memory usage.

as a searchable database for long-sequence understanding (Xu et al., 2023), InfLLM has several advantages: (1) Training-Free: RAG requires additional retrieval data to train a retrieval model, whereas InfLLM is training-free and applicable to any LLMs. Besides, RAG also necessitates fine-tuning LLMs to adapt to the inputs augmented by the retrieved knowledge. (2) Broader Applicability: RAG models are usually limited by the performance of their retrieval components. Besides, existing retrieval models will suffer from out-of-distribution issues, struggling to perform well on tasks outside their training distribution (Lin et al., 2023; Muennighoff et al., 2023). This limitation adversely affects the overall performance of the RAG system. In contrast, InfLLM has no specific requirements for tasks and can be feasibly used for long sequences.

To verify the generalization capabilities of InfLLM, we conduct experiments to comparing RAG and InfLLM on three context retrieval tasks. We utilize E5-mistral-7B-instruct (Wang et al., 2024b) as the retrieval model. The results are shown in Table 3. Our findings demonstrate that even without additional data or training, InfLLM can consistently outperform RAG models, underscoring its superior generalization capabilities. The dependency on an external retrieval model makes RAG less flexible in handling diverse tasks.

### The Impact of Memory Settings

InfLLM relies on the context memory to look up relevant information. We further explore the impact of core components in the context memory, specifically the representative tokens and memory units. The results are shown in Figure 2.

**Different Number of Representative Tokens.** InfLLM splits key-value vectors into memory units and selects several representative tokens from the unit to serve as the unit representations. Consequently, the ability of these representative tokens to semantically represent the entire unit directly impacts the model's performance. We conduct experiments with the number of representative tokens as \(\{1,2,4,8\}\). The results are shown in Figure 1(a). It is observed that as the number of representative tokens increases, there is a trend of improvement in the model performance, which indicates that more representative tokens tend to better represent the semantic content of the memory units. However, it is noted that when the number of representative tokens reaches \(8\), there is a slight performance decrease. This decline can be attributed to the inclusion of semantically irrelevant tokens as unit representations. More efficient and powerful unit representations will further enhance model performance for future work.

**Different Number of Selected Units.** The selected units are utilized to provide relevant context to LLMs. We conduct experiments with the number of units set as \(\{2,4,8,16,32,64,96,128\}\). From Figure 1(b), we can observe that as the number of selected units increases from \(1\) to \(32\), the model performance significantly improves, which is attributed to that more units imply a greater recall rate

   Task & R.PK & R.Num & R.KV \\  RAG-E5 & 89.2 & 65.4 & 13.2 \\ InfLLM & **100.0** & **96.1** & **96.8** \\   

Table 3: The comparison between InfLLM and RAG.

Figure 2: Extra studies about InfLLM. Here, (a), (b), and (c) investigate the impact of the context memory under different numbers of representative tokens, different numbers of selected units, and memory unit sizes, respectively.

of relevant content. Larger unit quantity also leads to an increase in the required memory scheduling time and the computational time for attention. Therefore, further enhancing lookup accuracy remains a crucial direction for improving the efficiency of InfLLM.

**Different Memory Unit Size.** Each memory unit is supposed to be a coherent semantic unit. Excessively large unit sizes can hinder precise lookup, while a small size will increase the computational overhead of memory lookup. We evaluate InfLLM with the unit size as \(\{32,64,128,256\}\) and keep the total context length as \(12\)K. The results are shown in Figure 1(c). It can be observed that the optimal unit size varies for different tasks due to the varying characteristics of input sequences. For example, in Retrieve.KV, a key-value pair constitutes a semantic unit, while in Math.Find, a single number represents a semantic unit. Employing heuristic rules to segment context can easily lead to suboptimal performance. Therefore, exploring how to dynamically segment context is an important direction for future research.

### Ablation Study

To further verify the effectiveness of dynamic memory lookup and unit representations, we conduct ablation studies in this section. The results are shown in Table 4.

**Context Memory Lookup.** InfLLM adopts dynamic context memory lookup for both input encoding and output decoding steps for comprehensive long-text understanding. We present the results of InfLLM with only lookup in output decoding (Decoding-Only) and without any memory lookup (w/o Lookup). It can be observed that a significant decline in model performance is associated with a reduction in the number of memory lookup iterations. This indicates that distant contextual information is crucial for both the long-input encoding and answer-generation phases. The model requires the integration of long-distance context to generate a coherent context memory for input understanding. LLM is supposed to collect useful information from massive past context information to generate the correct answers.

**Unit Representation.** We design a block-level memory for efficient context information lookup. We select several representative tokens as the unit representations for relevance computation. We present the results of InfLLM with another training-free representation method (Mean Repr), which computes the representation by averaging the key vectors in a memory unit. From the results, we can observe that InfLLM with average representations can also present competitive performance. It indicates that the original attention vectors in LLMs are effective for relevance score computation, and exploring more efficient unit representations is an important future direction.

### Scaling to 1,024K Context

To assess the effectiveness of InfLLM on extremely long sequences, in this subsection, we scale the sequence length to \(1024\)K to evaluate the capacity of InfLLM to capture contextual relevance in long sequences. Specifically, we adopt the Retrieve.PassKey task in \(\)-Bench for evaluation. This task prompts LLMs to find a \(5\)-digit sequence among lengthy and noisy contexts, which requires LLMs to locate relevant information among long sequences effectively. We automatically generate inputs with \(\{32,64,128,256,512,768,1024\}\) thousand tokens and for each length, we generate \(50\) instances for evaluation. We adopt Mistral as the base model.

The results are shown in Figure 3. From the results, we can observe that InfLLM can accurately locate the key information from length noises and achieve \(100\)% accuracy even when the context length scales to \(1024\) thousand tokens. However, LM-Infinite can only attend to the tokens within the local window,

   Task & R.KV & Math.F & QA \\  InfLLM & **96.8** & 25.7 & **15.7** \\  Decoding-Only & 85.2 & **26.3** & 12.0 \\ w/o Lookup & 0.4 & 16.3 & 11.4 \\ Mean Repr & 84.6 & 25.1 & 14.9 \\   

Table 4: The results for ablation study.

Figure 3: The results on sequences with different lengths.

which leads to a rapid decline in its performance as the sequence length increases. It proves that InfLLM can accurately capture the long-distance dependencies for effective long-sequence reasoning.

## 5 Conclusion

In this paper, we propose a training-free method to improve the length generalizability of LLMs. Based on the sliding window attention mechanism, we construct an additional context memory module, which can help LLMs select relevant information from massive contexts to capture long-distance dependencies. The experiments on two widely-used long-text benchmarks show that InfLLM can effectively improve the ability of LLMs, which are trained on sequences with a few thousand tokens, to process extremely long sequences. In the future, we will explore efficient training of the context memory module to further enhance the model performance. Besides, combining the key-value cache compression methods with InfLLM can further reduce the computational and memory costs. We hope InfLLM can boost the development of streaming applications of LLMs.