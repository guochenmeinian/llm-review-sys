# Provable Guarantees for Nonlinear Feature Learning

in Three-Layer Neural Networks

 Eshaan Nichani

Princeton University

eshnich@princeton.edu &Alex Damian

Princeton University

ad27@princeton.edu &Jason D. Lee

Princeton University

jasonlee@princeton.edu

###### Abstract

One of the central questions in the theory of deep learning is to understand how neural networks learn hierarchical features. The ability of deep networks to extract salient features is crucial to both their outstanding generalization ability and the modern deep learning paradigm of pretraining and finetuneing. However, this feature learning process remains poorly understood from a theoretical perspective, with existing analyses largely restricted to two-layer networks. In this work we show that three-layer neural networks have provably richer feature learning capabilities than two-layer networks. We analyze the features learned by a three-layer network trained with layer-wise gradient descent, and present a general purpose theorem which upper bounds the sample complexity and width needed to achieve low test error when the target has specific hierarchical structure. We instantiate our framework in specific statistical learning settings - single-index models and functions of quadratic features - and show that in the latter setting three-layer networks obtain a sample complexity improvement over all existing guarantees for two-layer networks. Crucially, this sample complexity improvement relies on the ability of three-layer networks to efficiently learn _nonlinear_ features. We then establish a concrete optimization-based depth separation by constructing a function which is efficiently learnable via gradient descent on a three-layer network, yet cannot be learned efficiently by a two-layer network. Our work makes progress towards understanding the provable benefit of three-layer neural networks over two-layer networks in the feature learning regime.

## 1 Introduction

The success of modern deep learning can largely be attributed to the ability of deep neural networks to decompose the target function into a hierarchy of learned features. This feature learning process enables both improved accuracy  and transfer learning . Despite its importance, we still have a rudimentary theoretical understanding of the feature learning process. Fundamental questions include understanding what features are learned, how they are learned, and how they affect generalization.

From a theoretical viewpoint, a fascinating question is to understand how depth can be leveraged to learn more salient features and as a consequence a richer class of hierarchical functions. The base case for this question is to understand which features (and function classes) can be learned efficiently by three-layer neural networks, but not two-layer networks. Recent work on feature learning has shown that two-layer neural networks learn features which are _linear_ functions of the input (see Section 1.2 for further discussion). It is thus a natural question to understand if three-layer networks can learn _nonlinear_ features, and how this can be leveraged to obtain a sample complexity improvement. Initial learning guarantees for three-layer networks , however, consider simplified model and function classes and do not discern specifically what the learned features are or whether a sample complexity improvement can be obtained over shallower networks or kernel methods.

On the other hand, the standard approach in deep learning theory to understand the benefit of depth has been to establish "depth separations" , i.e. functions that cannot be efficiently approximated by shallow networks, but can be via deeper networks. However, depth separations are solely concerned with the representational capability of neural networks, and ignore the optimization and generalization aspects. In fact, depth separation functions such as  are often not learnable via gradient descent . To reconcile this, recent papers [45; 44] have established _optimization-based_ depth separation results, which are functions which cannot be efficiently learned using gradient descent on a shallow network but can be learned with a deeper network. We thus aim to answer the following question:

_What features are learned by gradient descent on a three-layer neural network, and can these features be leveraged to obtain a provable sample complexity guarantee?_

### Our contributions

We provide theoretical evidence that three-layer neural networks have provably richer feature learning capabilities than their two-layer counterparts. We specifically study the features learned by a three-layer network trained with a layer-wise variant of gradient descent (Algorithm 1). Our main contributions are as follows.

* Theorem 1 is a general purpose sample complexity guarantee for Algorithm 1 to learn an arbitrary target function \(f^{*}\). We first show that Algorithm 1 learns a feature roughly corresponding to a low-frequency component of the target function \(f^{*}\) with respect to the random feature kernel \(\) induced by the first layer. We then derive an upper bound on the population loss in terms of the learned feature. As a consequence, we show that if \(f^{*}\) possesses a hierarchical structure where it can be written as a 1D function of the learned feature (detailed in Section 3), then the sample complexity for learning \(f^{*}\) is equal to the sample complexity of learning the feature. This demonstrates that three-layer networks indeed perform hierarchical learning.
* We next instantiate Theorem 1 in two statistical learning settings which satisfy such hierarchical structure. As a warmup, we show that Algorithm 1 learns single-index models (i.e \(f^{*}(x)=g^{*}(w x)\)) in \(d^{2}\) samples, which is comparable to existing guarantees for two-layer networks and crucially has \(d\)-dependence not scaling with the degree of the link function \(g^{*}\). We next show that Algorithm 1 learns the target \(f^{*}(x)=g^{*}(x^{T}Ax)\), where \(g^{*}\) is either Lipschitz or a degree \(p=O(1)\) polynomial, up to \(o_{d}(1)\) error with \(d^{4}\) samples. This improves on all existing guarantees for learning with two-layer networks or via NTK-based approaches, which all require sample complexity \(d^{(p)}\). A key technical step is to show that for the target \(f^{*}(x)=g^{*}(x^{T}Ax)\), the learned feature is approximately \(x^{T}Ax\). This argument relies on the universality principle in high-dimensional probability, and may be of independent interest.
* We conclude by establishing an explicit optimization-based depth separation: We show that the target function \(f^{*}(x)=(x^{T}Ax)\) for appropriately chosen \(A\) can be learned by Algorithm 1 up to \(o_{d}(1)\) error in \(d^{4}\) samples, whereas any two layer network needs either superpolynomial width or weight norm in order to approximate \(f^{*}\) up to comparable accuracy. This implies that such an \(f^{*}\) is not efficiently learnable via two-layer networks.

The above separation hinges on the ability of three-layer networks to learn the nonlinear feature \(x^{T}Ax\) and leverage this feature learning to obtain an improved sample complexity. Altogether, our work presents a general framework demonstrating the capability of three-layer networks to learn nonlinear features, and makes progress towards a rigorous understanding of feature learning, optimization-based depth separations, and the role of depth in deep learning more generally.

### Related Work

Neural Networks and Kernel Methods.Early guarantees for neural networks relied on the Neural Tangent Kernel (NTK) theory [31; 50; 23; 17]. The NTK theory shows global convergence by coupling to a kernel regression problem and generalization via the application of kernel generalization bounds [7; 15; 5]. The NTK can be characterized explicitly for certain data distributions [27; 39; 38], which allows for tight sample complexity and width analyses. This connection to kernel methods has also been used to study the role of depth, by analyzing the signal propagation and evolution of the NTK in MLPs [42; 48; 28], convolutional networks [6; 55; 56; 40], and residual networks . However, the NTK theory is insufficient as neural networks outperform their NTK in practice [6; 32]. In fact,  shows that kernels cannot adapt to low-dimensional structure and require \(d^{k}\) samples to learn any degree \(k\) polynomials in \(d\) dimensions. Ultimately, the NTK theory fails to explain generalization or the role of depth in practical networks not in the kernel regime. A key distinction is that networks in the kernel regime cannot learn features . A recent goal has thus been to understand the feature learning mechanism and how this leads to sample complexity improvements [53; 22; 58; 3; 25; 26; 20; 54; 33; 35]. Crucially, our analysis is _not_ in the kernel regime, and shows an improvement of three-layer networks over two-layer networks in the feature-learning regime.

Feature Learning.Recent work has studied the provable feature learning capabilities of two-layer neural networks. [9; 1; 2; 8; 13; 10] show that for isotropic data distributions, two-layer networks learn linear features of the data, and thus efficiently learn functions of low-dimensional projections of the input (i.e targets of the form \(f^{*}(x)=g(Ux)\) for \(U^{r d}\)). Here, \(x Ux\) is the "linear feature." Such target functions include low-rank polynomials [18; 2] and single-index models [8; 13] for Gaussian covariates, as well as sparse boolean functions  such as the \(k\)-sparse parity problem  for covariates uniform on the hypercube.  draws connections from the mechanisms in these works to feature learning in standard image classification settings. The above approaches rely on layerwise training procedures, and our Algorithm 1 is an adaptation of the algorithm in .

Another approach uses the quadratic Taylor expansion of the network to learn classes of polynomials [9; 41] This approach can be extended to three-layer networks.  replace the outermost layer with its quadratic approximation, and by viewing \(z^{p}\) as the hierarchical function \((z^{p/2})^{2}\) show that their three-layer network can learn low rank, degree \(p\) polynomials in \(d^{p/2}\) samples.  similarly uses a quadratic approximation to improperly learn a class of three-layer networks via sign-randomized GD. An instantiation of their upper bound to the target \(g^{*}(x^{T}Ax)\) for degree \(p\) polynomial \(g^{*}\) yields a sample complexity of \(d^{p+1}\). However, [16; 5] are proved via opaque landscape analyses, do not concretely identify the learned features, and rely on nonstandard algorithmic modifications. Our Theorem 1 directly identifies the learned features, and when applied to the quadratic feature setting in Section 4.2 obtains an improved sample complexity guarantee independent of the degree of \(g^{*}\).

Depth Separations. constructs a function which can be approximated by a poly-width network with large depth, but not with smaller depth.  is the first depth separation between depth 2 and 3 networks, with later works [46; 19; 47] constructing additional such examples. However, such functions are often not learnable via three-layer networks .  shows that approximatability by a shallow (depth 3 network) is a necessary condition for learnability via a deeper network.

These issues have motivated the development of optimization-based, or algorithmic, depth separations, which construct functions which are learnable by a three-layer network but not by two-layer networks.  shows that certain ball indicator functions \((\|x\|)\) are not approximatable by two-layer networks, yet are learnable via GD on a special variant of a three-layer network with second layer width equal to 1. However, their network architecture is tailored for learning the ball indicator, and the explicit polynomial sample complexity (\(n d^{36}\)) is weak.  shows that a multi-layer mean-field network with a 1D bottleneck layer can learn the target \((1-\|x\|)\), which  previously showed was inaproximatable via two-layer networks. However, their analysis relies on the rotational invariance of the target function, and it is difficult to read off explicit sample complexity and width guarantees beyond being \((d)\). Our Section 4.2 shows that three-layer networks can learn a larger class of features (\(x^{T}Ax\) versus \(\|x\|\)) and functions on top of these features (any Lipschitz \(q\) versus \(\)), with explicit dependence on the width and sample complexity needed (\(n,m_{1},m_{2}=(d^{4})\)).

## 2 Preliminaries

### Problem Setup

Data distribution.Our aim is to learn the target function \(f^{*}:_{d}\),with \(_{d}^{d}\) the space of covariates. We let \(\) be some distribution on \(_{d}\), and draw two independent datasets \(_{1},_{2}\), each with \(n\) samples, so that each \(x_{1}\) or \(x_{2}\) is sampled i.i.d as \(x\). Without loss of generality, we normalize so \(_{x}[f^{*}(x)^{2}] 1\). We make the following assumptions on \(\):

**Definition 1** (Sub-Gaussian Vector).: _A mean-zero random vector \(X^{d}\) is \(\)-subGaussian if, for all unit vectors \(v^{d}\), \([( X v)](^{2}^{2})\) for all \(\)._

**Assumption 1**.: \(_{x}[x]=0\) _and \(\) is \(C_{}\)-subGaussian for some constant \(C_{}\)._

**Assumption 2**.: \(f^{*}\) _has polynomially growing moments, i.e there exist constants \((C_{f},)\) such that \(_{x}[f^{*}(x)^{q}]^{1/q} C_{f}q^{}\) for all \(q 1\)._

We note that Assumption 2 is satisfied by a number of common distributions and functions, and we will verify that Assumption 2 holds for each example in Section 4.

Three-layer neural network.Let \(m_{1},m_{2}\) be the two hidden layer widths, and \(_{1},_{2}\) be two activation functions. Our learner is a three-layer neural network parameterized by \(=(a,W,b,V)\), where \(a^{m_{1}},W^{m_{1} m_{2}}\), \(b^{m_{1}}\), and \(V^{m_{2} d}\). The network \(f(x;)\) is defined as:

\[f(x;):=}a^{T}_{1}(W_{2}(Vx)+b)= }_{i=1}^{m_{1}}a_{i}_{1} w_{i},h^{(0)}( x)+b_{i}. \]

Here, \(w_{i}^{m_{2}}\) is the \(i\)th row of \(W\), and \(h^{(0)}(x):=_{2}(Vx)^{m_{2}}\) is the random feature embedding arising from the innermost layer. The parameter vector \(^{(0)}:=(a^{(0)},W^{(0)},b^{(0)},V^{(0)})\) is initialized with \(a_{i}^{(0)}_{iid}(\{ 1\})\), \(W^{(0)}=0\), the biases \(b_{i}^{(0)}_{iid}(0,1)\), and the rows \(v_{i}^{(0)}\) of \(V^{(0)}\) drawn \(v_{i}_{iid}\), where \(\) is the uniform measure on \(^{d-1}(1)\), the \(d\)-dimensional unit sphere. We make the following assumption on the activations, and note that the polynomial growth assumption on \(_{2}\) is satisfied by all activations used in practice.

**Assumption 3**.: \(_{1}\) _is the \(\) activation, i.e \(_{1}(z)=(z,0)\), and \(_{2}\) has polynomial growth, i.e \(|_{2}(x)| C_{}(1+|x|)^{_{}}\) for some constants \(C_{},_{}>0\)._

Training Algorithm.Let \(L_{i}()\) denote the empirical loss on dataset \(_{i}\); that is for \(i=1,2\): \(L_{i}():=_{x_{i}}(f(x;)-f^{* }(x))^{2}\). Our network is trained via layer-wise gradient descent with sample splitting. Throughout training, the first layer weights \(V\) and second layer bias \(b\) are held constant. First, the second layer weights \(W\) are trained for \(t=1\) timesteps. Next, the outer layer weights \(a\) are trained for \(t=T-1\) timesteps. This two stage training process is common in prior works analyzing gradient descent on two-layer networks [18; 8; 1; 10], and as we see in Section 5, is already sufficient to establish a separation between two and three-layer networks. Pseudocode for the training procedure is presented in Algorithm 1.

``` Input: Initialization \(^{(0)}\); learning rates \(_{1},_{2}\); weight decay \(\); time \(T\) {Stage 1: Train \(W\)} \(W^{(1)} W^{(0)}-_{1}_{W}L_{1}(^{(0)})\) \(a^{(1)} a^{(0)}\) \(^{(1)}(a^{(1)},W^{(1)},b^{(0)},V^{(0)})\) {Stage 2: Train \(a\)} for\(t=2,,T\)do \(a^{(t)} a^{(t-1)}-_{2}_{a}L_{2}(^{(t-1)})+  a^{(t_and corresponding integral operator_

\[(f)(x):=_{x^{}}[K(x,x^{})f(x^{})]. \]

We make the following assumption on \(\), which we verify for the examples in Section 4:

**Assumption 4**.: \(f^{*}\) _has polynomially bounded moments, i.e there exist constants \(C_{K},\) such that, for all \(1 q d\), \(\|f^{*}\|_{L^{q}()} C_{K}q^{}\|f ^{*}\|_{L^{2}()}\)._

We also require the definition of the Sobolev space:

**Definition 3**.: _Let \(^{2,}([-1,1])\) be the Sobolev space of twice continuously differentiable functions \(q:[-1,1]\) equipped with the norm \(\|q\|_{k,}:=_{s k}_{x[-1,1]}|q^{(s)}(x)|\) for \(k=1,2\)._

### Notation

We use big \(O\) notation (i.e \(O,,\)) to ignore absolute constants (\(C_{},C_{f}\), etc.) that do not depend on \(d,n,m_{1},m_{2}\). We further write \(a_{d} b_{d}\) if \(a_{d}=O(b_{d})\), and \(a_{d}=o(b_{d})\) if \(_{d}a_{d}/b_{d}=0\). Additionally, we use \(\) notation to ignore terms that depend logarithmically on \(dnm_{1}m_{2}\). For \(f:_{d}\), define \(\|f\|_{L^{p}(_{d},)}=(_{x}[f(x)^{p} ])^{1/p}\). To simplify notation we also call this quantity \(\|f\|_{L^{p}()}\), and \(\|g\|_{L^{p}(_{d},)},\|g\|_{L^{p}()}\) are defined analogously for functions \(g:^{d-1}(1)\). When the domain is clear from context, we write \(\|f\|_{L^{p}},\|g\|_{L^{p}}\). We let \(L^{p}(_{d},)\) be the space of \(f\) with finite \(\|f\|_{L^{p}(_{d},)}\). Finally, we write \(_{x}\) and \(_{v}\) as shorthand for \(_{x}\) and \(_{v}\) respectively.

## 3 Main Result

The following is our main theorem which upper bounds the population loss of Algorithm 1:

**Theorem 1**.: _Select \(q^{2,}([-1,1])\). Let \(_{1}=}{m_{2}}\), and assume \(n,m_{1},m_{2}=(\|f^{*}\|_{L^{2}}^{-2})\) There exist \(,,_{2}\) such that after \(T=(n,m_{1},m_{2},d,\|q\|_{2,})\) timesteps, with high probability over the initialization and datasets the output \(\) of Algorithm 1 satisfies the population \(L^{2}\) loss bound_

\[_{x} f(x;)-f^{*}(x)^{2}\] \[ f^{*})-f^{*}\|_{L^{2}}^{2}}_{ accuracy of\\ feature learning }}+^{2}\| f^{*}\|_{L^{2}}^{-2}}{(n,m_{1},m_{2})}}_{ sample complexity of\\ feature learning}}+^{2} }{m_{1}}+^{2}+1}_{q}}_{complexity of q}} \]

The full proof of this theorem is in Appendix D. The population risk upper bound (4) has three terms:

1. The first term quantifies the extent to which feature learning is useful for learning the target \(f^{*}\), and depends on how close \(f^{*}\) is to having hierarchical structure. Concretely, if there exists \(q:\) such that the compositional function \(qf^{*}\) is close to the target \(f^{*}\), then this first term is small. In Section 4, we show that this is true for certain _hierarchical_ functions. In particular, say that \(f^{*}\) satisfies the hierarchical structure \(f^{*}=g^{*} h^{*}\). If the quantity \(f^{*}\) is nearly proportional to the true feature \(h^{*}\), then this first term is negligible. As such, we refer to the quantity \(f^{*}\) as the _learned feature_.
2. The second term is the sample (and width) complexity of learning the feature \(f^{*}\). It is useful to compare the \(\|f^{*}\|_{L^{2}}^{-2}\) term to the standard kernel generalization bound, which requires \(n f^{*},^{-1}f^{*}_{L^{2}}\). Unlike in the kernel bound, the feature learning term in (4) does not require inverting the kernel \(\) as it only requires a lower bound on \(\|f^{*}\|_{L^{2}}\). This difference can be best understood by considering the alignment of \(f^{*}\) with the eigenfunctions of \(\). Say that \(f^{*}\) has nontrivial alignment with eigenfunctions of \(\) with both small \((_{min})\) and large \((_{max})\) eigenvalues. Kernel methods require \((_{min}^{-1})\) samples, which blows up when \(_{min}\) is small; the sample complexity of kernel methods depends on the high frequency components of \(f^{*}\). On the other hand, the guarantee in Theorem 1 scales with \(\|f^{*}\|_{L^{2}}^{2}=O(_{max}^{-2})\), which can be much smaller. In other words, the sample complexity of feature learning scales with the low-frequency components of \(f^{*}\). The feature learning process can thus be viewed as extracting the low-frequency components of the target.
3. The last two terms measure the complexity of learning the univariate function \(q\). In the examples in Section 4, the effect of these terms is benign.

Altogether, if \(f^{*}\) satisfies the hierarchical structure that its high-frequency components can be inferred from the low-frequency ones, then a good \(q\) for Theorem 1 exists and the dominant term in (4) is the sample complexity of feature learning term, which only depends on the low-frequency components. This is not the case for kernel methods, as small eigenvalues blow up the sample complexity. As we show in Section 4, this ability to ignore the small eigenvalue components of \(f^{*}\) during the feature learning process is critical for achieving good sample complexity in many problems.

### Proof Sketch

At initialization, \(f(x;^{(0)}) 0\). The first step of GD on the population loss for a neuron \(w_{j}\) is thus

\[w^{(1)} =-_{1}_{w_{j}}_{x}f(x;^{ (0)})-f^{*}(x)^{2} \] \[=_{1}_{x}f^{*}(x)_{w_{j}}f(x;^{ (0)})\] (6) \[=}_{b_{j}^{(0)} 0}a_{j }^{(0)}_{x}f^{*}(x)h^{(0)}(x). \]

Therefore the network \(f(x^{};^{(1)})\) after the first step of GD is given by

\[f(x^{};^{(1)}) =}_{j=1}^{m_{1}}a_{j}_{1} w _{j}^{(1)},h^{(0)}(x^{})+b_{j} \] \[=}_{j=1}^{m_{1}}a_{j}_{1}a_{j}^{( 0)}}_{x}f^{*}(x)h^{(0)}(x)^ {T}h^{(0)}(x^{})+b_{j}_{b_{j}^{(0)} 0}. \]

We first notice that this network now implements a 1D function of the quantity

\[(x^{}):=}_{x}f^{*}(x)h^ {(0)}(x)^{T}h^{(0)}(x^{}). \]

Specifically, the network can be rewritten as

\[f(x^{};^{(1)})=}_{j=1}^{m_{1}}a_{j}_{1} a_{j}^{(0)}(x^{})+b_{j}_{b_{j}^{(0 )} 0}. \]

Since \(f\) implements a hierarchical function of the quantity \((x)\), we term \(\) the _learned feature_.

The second stage of Algorithm 1 is equivalent to random feature regression. We first use results on ReLU random features to show that any \(q^{2,}([-1,1])\) can be approximated on \([-1,1]\) as \(q(z)}_{j=1}^{m_{1}}a_{j}^{*}_{1}(a_{j}^{(0)}z+b _{j})\) for some \(\|a^{*}\|\|q\|_{2,}\) (Lemma 3). Next, we use the standard kernel Rademacher bound to show that the excess risk scales with the smoothness of \(q\) (Lemma 5). Hence we can efficiently learn functions of the form \(q\).

It suffices to compute this learned feature \(\). For \(m_{2}\) large, we observe that

\[(x^{})=}{m_{2}}_{j=1}^{m_{2}}_{x} [f^{*}(x)(x v)(x^{} v)]) _{x}[f^{*}(x)K(x,x^{})]=(f^{*})(x^{ }). \]

The learned feature is thus approximately \(f^{*}\). Choosing \(\) so that \(|(x^{})| 1\), we see that Algorithm 1 learns functions of the form \((f^{*})\). Finally, we translate the above analysis to the finite sample gradient via standard concentration tools. Since the empirical estimate to \(f^{*}\) concentrates at a \(1/\) rate, \(n\|f^{*}\|_{L^{2}}^{-2}\) samples are needed to obtain a constant factor approximation (Lemma 7).

Examples

We next instantiate Theorem 1 in two specific statistical learning settings which satisfy the hierarchical prescription detailed in Section 3. As a warmup, we show that three-layer networks efficiently learn single index models. Our second example shows how three-layer networks can obtain a sample complexity improvement over existing guarantees for two-layer networks.

### Warmup: single index models

Let \(f^{*}=g^{*}(w^{*} x)\), for unknown direction \(w^{*}^{d}\) and unknown link function \(g^{*}:\), and take \(_{d}=^{d}\) with \(=(0,I)\). Prior work [18; 13] shows that two-layer neural networks learn such functions with an improved sample complexity over kernel methods. Let \(_{2}(z)=z\), so that the network is of the form \(f(x;)=}a^{T}_{1}(WYx)\). We can verify that Assumptions 1 to 4 are satisfied, and thus applying Theorem 1 in this setting yields the following:

**Theorem 2**.: _Let \(f^{*}(x)=g^{*}(w^{*} x)\), where \(\|w^{*}\|_{2}=1\). Assume that \(g^{*},(g^{*})^{}\) and \((g^{*})^{}\) are polynomially bounded and that \(_{z(0,1)}[(g^{*})^{}(z)] 0\). Then with high probability Algorithm 1 satisfies the population loss bound_

\[_{x}f(x;)-f^{*}(x)^{2}= }{(n,m_{1},m_{2})}+}. \]

Given widths \(m_{1},m_{2}=(d^{2})\), \(n=(d^{2})\) samples suffice to learn \(f^{*}\), which matches existing guarantees for two-layer neural networks [13; 18]. We remark that prior work on learning single-index models under assumptions on the link function such as monotonicity or the condition \(_{z(0,1)}[(g^{*})^{}(z)] 0\) require \(d\) samples [49; 37; 12]. However, our sample complexity improves on that of kernel methods, which require \(d^{p}\) samples when \(g^{*}\) is a degree \(p\) polynomial.

Theorem 2 is proved in Appendix E.1; a brief sketch is as follows. Since \(_{2}(z)=z\), the kernel is \(K(x,x^{})=_{v}[(x v)(x^{} v)]=}{d}\). By an application of Stein's Lemma, the learned feature is

\[(f^{*})(x)=_{x^{}}[x x^{}f^{*} (x)]=x^{T}_{x^{}}[ f^{*}(x^{})] \]

Since \(f^{*}(x)=g^{*}(w^{*} x)\), \( f^{*}(x)=w^{*}(g^{*})^{}(w^{*} x)\), and thus

\[(f^{*})(x)=_{z(0,1)}[(g^{*})^{ }(z)]w^{*} xw^{*} x. \]

The learned feature is proportional to the true feature, so an appropriate choice of \(\) and choosing \(q=g^{*}\) in Theorem 1 implies that \(\|f^{*}\|_{L^{2}}^{-2}=d^{2}\) samples are needed to learn \(f^{*}\).

### Functions of quadratic features

The next example shows how three-layer networks can learn nonlinear features, and thus obtain a sample complexity improvement over two-layer networks.

Let \(_{d}=^{d-1}()\), the sphere of radius \(\), and \(\) the uniform measure on \(_{d}\). The integral operator \(\) has been well studied [39; 27; 38], and its eigenfunctions correspond to the spherical harmonics. Preliminaries on spherical harmonics and this eigendecomposition are given in Appendix F.

Consider the target \(f^{*}(x)=g^{*}(x^{T}Ax)\), where \(A^{d d}\) is a symmetric matrix and \(g^{*}:\) is an unknown link function. In contrast to a single-index model, the feature \(x^{T}Ax\) we aim to learn is a _quadratic_ function of \(x\). Since one can write \(x^{T}Ax=x^{T}A-(A)x+(A)\), we without loss of generality assume \((A)=0\). We also select the normalization \(\|A\|_{F}^{2}==(1)\); this ensures that \(_{x}[(x^{T}Ax)^{2}]=1\). We first make the following assumptions on the target function.:

**Assumption 5**.: \(_{x}[f^{*}(x)]=0\)_, \(_{z(0,1)}[(g^{*})^{}(z)]=(1)\), \(g^{*}\) is \(1\)-Lipschitz, and \((g^{*})^{}\) has polynomial growth._

The first assumption can be achieved via a preprocessing step which subtracts the mean of \(f^{*}\), the second is a nondegeneracy condition, and the last two assume the target is sufficiently smooth.

We next require the eigenvalues of \(A\) to satisfy an incoherence condition:

**Assumption 6**.: _Define \(:=\|A\|_{op}\). Then \(=o(/(d))\)._

Note that \(\). If \(A\) has rank \((d)\) and condition number \((1)\), then \(=(1)\). Furthermore, when the entries of \(A\) are sampled i.i.d, \(=(1)\) with high probability by Wigner's semicircle law.

Finally, we make the following nondegeneracy assumption on the Gegenbauer decomposition of \(_{2}\) (defined in Appendix F). We show that \(_{2}^{2}(_{2})=O(d^{-2})\), and later argue that the following assumption is mild and indeed satisfied by standard activations such as \(_{2}=\).

**Assumption 7**.: _Let \(_{2}(_{2})\) be the 2nd Gegenbauer coefficient of \(_{2}\). Then \(_{2}^{2}(_{2})=(d^{-2})\)._

We can verify Assumptions 1 to 4 hold for this setting, and thus applying Theorem 1 yields the following:

**Theorem 3**.: _Under Assumptions 5 to 7, with high probability Algorithm 1 satisfies the population loss bound_

\[_{x}f(x;)-f^{*}(x)^{2} }{(n,m_{1},m_{2})}+}+ }^{1/3} \]

We thus require sample size \(n=(d^{4})\) and widths \(m_{1},m_{2}=(d^{4})\) to obtain \(o_{d}(1)\) test loss.

Proof Sketch.The integral operator \(\) has eigenspaces corresponding to spherical harmonics of degree \(k\). In particular,  shows that, in \(L^{2}\),

\[f^{*}=_{k 0}c_{k}P_{k}f^{*}, \]

where \(P_{k}\) is the orthogonal projection onto the subspace of degree \(k\) spherical harmonics, and the \(c_{k}\) are constants satisfying \(c_{k}=O(d^{-k})\). Since \(f^{*}\) is an even function and \(_{x}[f^{*}(x)]=0\), truncating this expansion at \(k=2\) yields

\[f^{*}=(d^{-2}) P_{2}f^{*}+O(d^{-4}), \]

It thus suffices to compute \(P_{2}f^{*}\). To do so, we draw a connection to the universality phenomenon in high dimensional probability. Consider two features \(x^{T}Ax\) and \(x^{T}Bx\) with \( A,B=0\). We show that, when \(d\) is large, the distribution of \(x^{T}Ax\) approaches that of the standard Gaussian, while \(x^{T}Bx\) approaches a mixture of \(^{2}\) and Gaussian random variables independent of \(x^{T}Ax\). As such, we show

\[_{x}[g^{*}(x^{T}Ax)x^{T}Bx] _{x}[g^{*}(x^{T}Ax)]_{x} [x^{T}Bx]=0 \] \[_{x}[g^{*}(x^{T}Ax)x^{T}Ax] _{z(0,1)}[g^{*}(z)z]= _{z(0,1)}[(g^{*})^{}(z)]. \]

The second expression can be viewed as an approximate version of Stein's lemma, which was applied in Section 4.1 to compute the learned feature. Altogether, our key technical result (stated formally in Lemma 20) is that for \(f^{*}=g^{*}(x^{T}Ax)\), the projection \(P_{2}f^{*}\) satisfies

\[(P_{2}f^{*})(x)=_{z(0,1)}[(g^{*})^{}(z)] x ^{T}Ax+o_{d}(1) \]

The learned feature is thus \(f^{*}(x)=(d^{-2})(x^{T}Ax+o_{d}(1))\); plugging this into Theorem 1 yields the \(d^{4}\) sample complexity.

The full proof of Theorem 3 is deferred to Appendix E.2. In Appendix E.3 we show that when \(g^{*}\) is a degree \(p=O(1)\) polynomial, Algorithm 1 learns \(f^{*}\) in \((d^{4})\) samples with an improved error floor.

Comparison to two-layer networks.Existing guarantees for two-layer networks cannot efficiently learn functions of the form \(f^{*}(x)=g^{*}(x^{T}Ax)\) for arbitrary Lipschitz \(g^{*}\). In fact, in Section 5 we provide an explicit lower bound against two-layer networks efficiently learning a subclass of these functions. When \(g^{*}\) is a degree \(p\) polynomial, networks in the kernel regime require \(d^{2p} d^{4}\) samples to learn \(f^{*}\). Improved guarantees for two-layer networks learn degree \(p^{}\) polynomials in \(r^{p^{}}\) samples when the target only depends on a rank \(r d\) projection of the input . However, \(g^{*}(x^{T}Ax)\) cannot be written in this form for some \(r d\), and thus existing guarantees do not apply. We conjecture that two-layer networks require \(d^{(p)}\) samples when \(g^{*}\) is a degree \(p\) polynomial.

Altogether, the ability of three-layer networks to efficiently learn the class of functions \(f^{*}(x)=g^{*}(x^{T}Ax)\) hinges on their ability to extract the correct _nonlinear_ feature. Empirical validation of the above examples is given in Appendix A.

An Optimization-Based Depth Separation

We complement the learning guarantee in Section 4.2 with a lower bound showing that there exist functions in this class that cannot be approximated by a polynomial size two-layer network.

The class of candidate two-layer networks is as follows. For a parameter vector \(=(a,W,b_{1},b_{2})\), where \(a^{m},W^{m d},b_{1}^{m},b_{2} \), define the associated two-layer network as

\[N_{}(x):=a^{T}(Wx+b_{1})+b_{2}=_{i=1}^{m}a_{i}(w_{i}^{T}x +b_{1,i})+b_{2}. \]

Let \(\|\|_{}:=(\|a\|_{},\|W\| _{},\|b_{1}\|_{},\|b_{2}\|_{})\) denote the maximum parameter value. We make the following assumption on \(\), which holds for all commonly-used activations.

**Assumption 8**.: _There exist constants \(C_{},_{}\) such that \(|(z)| C_{}(1+|z|)^{_{}}\)._

Our main theorem establishing the separation is the following.

**Theorem 4**.: _Let \(d\) be a suffiently large even integer. Consider the target function \(f^{*}(x)=(x^{T}Ax)-c_{0}\), where \(A=}U0&I_{d/2}\\ I_{d/2}&0U^{T}\) for some orthogonal matrix \(U\) and \(c_{0}=_{x}[(x^{T}Ax)]\). Under Assumption 8, there exist constants \(C_{1},C_{2},C_{3},c_{3}\), depending only on \((C_{},_{})\), such that for any \(c_{3} C_{3}d^{-2}\), any two layer neural network \(N_{}(x)\) of width \(m\) and population \(L^{2}\) error bound \(\|N_{}-f^{*}\|_{L^{2}}^{2}\) must satisfy \((m,\|\|_{}) C_{1}(C_{2}^{-1/2} (d))\). However, Algorithm 1 outputs a predictor satisfying the population \(L^{2}\) loss bound_

\[_{x}[(f(x;)-f^{*}(x))^{2}] O }{n}+}+d^{-1/6}. \]

_after \(T=(d,m_{1},m_{2},n)\) timesteps._

The lower bound follows from a modification of the argument in  along with an explicit decomposition of the \(\) function into spherical harmonics. We remark that the separation applies for any link function \(g^{*}\) whose Gegenbauer coefficients decay sufficiently slow. The upper bound follows from an application of Theorem 1 to a smoothed version of \(\), since \(\) is not in \(^{2,}([-1,1])\). The full proof of the theorem is given in Appendix G.

Remarks.In order for a two-layer network to achieve test loss matching the \(d^{-1/6}\) error floor, either the width or maximum weight norm of the network must be \((d^{})\) for some constant \(\); this lower bound is superpolynomial in \(d\). As a consequence, gradient descent on a poly-width two-layer neural network with stable step size must run for superpolynomially many iterations in order for some weight to grow this large and thus converge to a solution with low test error. Therefore \(f^{*}\) is not learnable via gradient descent in polynomial time. This reduction from a weight norm lower bound to a runtime lower bound is made precise in .

We next describe a specific example of such an \(f^{*}\). Let \(S\) be a \(d/2\)-dimensional subspace of \(^{d}\), and let \(A=d^{-}P_{S}-d^{-}P_{S}^{}\), where \(P_{S},P_{S}^{}\) are projections onto the subspace \(S\) and and its orthogonal complement respectively. Then, \(f^{*}(x)=2d^{-}\|P_{S}x\|^{2}-d^{}\).  established an optimization-based separation for the target \((1-\|x\|)\), under a different distribution \(\). However, their analysis relies heavily on the rotational symmetry of the target, and they posed the question of learning \((1-\|P_{S}x\|)\) for some subspace \(S\). Our separation applies to a similar target function, and crucially does not rely on this rotational invariance.

## 6 Discussion

In this work we showed that three-layer networks can both learn nonlinear features and leverage these features to obtain a provable sample complexity improvement over two-layer networks. There are a number of interesting directions for future work. First, can our framework be used to learn hierarchical functions of a larger class of features beyond quadratic functions? Next, since Theorem 1 is general purpose and makes minimal distributional assumptions, it would be interesting to understand if it can be applied to standard empirical datasets such as CIFAR-10, and what the learned feature \(f^{*}\) and hierarchical learning correspond to in this setting. Finally, our analysis studies the nonlinear feature learning that arises from a neural representation \(_{2}(Vx)\) where \(V\) is fixed at initialization. This alone was enough to establish a separation between two and three-layer networks. A fascinating question is to understand the additional features that can be learned when both \(V\) and \(W\) are jointly trained. Such an analysis, however, is incredibly challenging in feature-learning regimes.