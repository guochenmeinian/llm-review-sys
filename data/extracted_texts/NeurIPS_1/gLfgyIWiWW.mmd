# Labeling Neural Representations with Inverse Recognition

Kirill Bykov

UMI Lab

ATB Potsdam

Potsdam, Germany

kbykov@atb-potsdam.de &Laura Kopf

UMI Lab

ATB Potsdam

Potsdam, Germany

lkopf@atb-potsdam.de &Shinichi Nakajima

Machine Learning Group

TU Berlin

Berlin, Germany

nakajima@tu-berlin.de &Marius Kloft

Machine Learning Group

RPTU Kaiserslautern-Landau

Kaiserslautern, Germany

kloft@cs.uni-kl.de &Marina M.-C. Hohne

UMI Lab

ATB Potsdam

University of Potsdam, Germany

mhoehne@atb-potsdam.de

Corresponding author.

###### Abstract

Deep Neural Networks (DNNs) demonstrate remarkable capabilities in learning complex hierarchical data representations, but the nature of these representations remains largely unknown. Existing global explainability methods, such as Network Dissection, face limitations such as reliance on segmentation masks, lack of statistical significance testing, and high computational demands. We propose Inverse Recognition (INVERT), a scalable approach for connecting learned representations with human-understandable concepts by leveraging their capacity to discriminate between these concepts. In contrast to prior work, INVERT is capable of handling diverse types of neurons, exhibits less computational complexity, and does not rely on the availability of segmentation masks. Moreover, INVERT provides an interpretable metric assessing the alignment between the representation and its corresponding explanation and delivering a measure of statistical significance. We demonstrate the applicability of INVERT in various scenarios, including the identification of representations affected by spurious correlations, and the interpretation of the hierarchical structure of decision-making within the models.

## 1 Introduction

Deep Neural Networks (DNNs) have demonstrated exceptional performance across a broad spectrum of domains due to their ability to learn complex, high-dimensional representations from vast volumes of data . Nevertheless, despite these impressive accomplishments, our comprehension of the concepts encoded within these representations remains limited. The "black-box" nature of representations, combined with the known susceptibility of networks to learn spurious correlations , biases  and harmful stereotypes  poses significant risks for the application of DNN systems, particularly in safety-critical domains .

To tackle the problem of the inherent opacity of DNNs, the field of Explainable AI (XAI) has emerged . The _global_ explanation methods aim to explain the concepts and abstractions learned within the DNNs representations. This is often achieved by establishing associations between neurons and human-understandable concepts , or by visualizing the stimuli responsiblefor provoking high neural activation levels [15; 16; 17; 18]. Such methods demonstrated themselves to be capable of detecting the malicious behavior and identifying the specific neurons responsible [19; 20].

In this work, we introduce the _Inverse Recognition_ (INVERT) 2 method for labeling neural representations within DNNs. Given a specific neuron, INVERT provides an explanation of the function of the neuron in the form of a composition of concepts, selected based on the ability of the neuron to detect data points within the compositional class. Unlike previous methods, the proposed approach does not rely on segmentation masks and only necessitates labeled data, is not constrained by the specific type of neurons, and demands fewer computational resources. Furthermore, INVERT offers a statistical significance test to confirm that the provided explanation is not merely a random occurrence. We evaluate the performance of the proposed approach across various datasets and models, and illustrate its practical use through multiple examples.

## 2 Related work

_Post-hoc_ interpretability, a subfield within Explainable AI, focuses on explaining the decision-making strategies of Deep Neural Networks (DNNs) without interfering with the original training process [21; 22]. Within the realm of post-hoc methods, a fundamental categorization arises concerning the scope of explanations they provide. _Local_ explanation methods aim to explain the decision-making process for individual data points, often presented in the form of attribution maps [23; 24; 25]. On the other hand, _global_ explanation methods aim to explain the prediction strategy learned by the machine across the population and investigate the purpose of its individual components [26; 27].

Inspired by principles from neuroscience [28; 29; 30], global explainability directs attention towards the in-depth examination of individual model components and their functional purpose . Often, global explainability is referred to as _mechanistic interpretability_, particularly in the context of Natural Language Processing (NLP) [32; 33; 34; 35]. Global approach to interpretability allows for the exploration of concepts learned by the model [36; 37; 38; 39] and explanation of _circuits_ -- computational subgraphs within the model that learn the transformation of various features [40; 41]. Various methods were proposed to interpret the learned features, including Activation-Maximisation (AM) methods . These methods aim to explain what individual neurons or groups of neurons have learned by visualizing inputs that elicit strong activation responses. Such input signals can either be found in an existing dataset  or generated synthetically [42; 17; 18]. AM methods demonstrated their utility in detecting undesired concepts learned by the model [19; 43; 20]. However, these methods require substantial user input to identify the concepts embodied in the Activation-Maximization signals. Recent research has demonstrated that such explanations can be manipulated while maintaining the behavior of the original model [44; 45; 46].

Another group of global explainability methods aim to explain the abstraction learned by the neuron within the model, by associating it with the human-understandable concepts. The Network Dissection (NetDissect) method [11; 47] was developed to provide explanations by linking neurons to concepts, based on the overlap between the activation maps of neurons and concept segmentation masks, quantified using the Intersection over Union (IoU) metric. Addressing the limitation that neurons could only be explained with a single concept, the subsequent Compositional Explanations of Neurons (CompExp) method was introduced, enabling the labeling of neurons with compositional concepts . Despite their utility, these methods generally have limitations, as they are primarily applicable to convolutional neurons and necessitate a dataset with segmentation masks, which significantly restricts their scalability (a more comprehensive discussion of these methods can be found in Appendix A.2). Other notable methods include CLIP-Dissect , MILAN , and FALCON . However, these methods utilize an additional model to produce explanations, thereby introducing a new source of potential unexplainability stemming from the explainer model.

## 3 INVERT: Interpreting Neural Representations with Inverse Recognition

In the following, we introduce a method called _Inverse Recognition_ (INVERT). This method aims to explain the abstractions learned by a neural representation by identifying what _compositional concept_ representation is most effective at detecting in a binary classification scenario. Unlike the general objective of Supervised Learning (SL) , which is to learn representations that can detect given concepts, the central idea behind INVERT is to learn a compositional concept that explains a given representation the best.

Let \(^{m}\), where \(m\) is the number of dimensions of data, be the input (data) space. We use the term _neural representations_ to refer to a sub-function of a network that represents the computational graph from the input of the model to the scalar output (activation) of a specific neuron, or any combination of neurons, that results in a scalar function.

**Definition 1** (Neural representation).: _A neural representation \(f\) is defined as a real-valued function \(f:\), which maps the data domain \(\) to the real numbers \(\). Here, \(\) represents the space of real-valued functions on \(\)._

Frequently, in DNNs, particular neurons, like convolutional neurons, produce multidimensional outputs. Depending on the specific needs of the application, these multidimensional functions can be interpreted either as a set of individual scalar representations or the neuron's output can be aggregated to yield a single scalar output, e.g. with pooling operations, such as average- or max-pooling. Unless stated otherwise, we utilize average-pooling as the standard aggregation measure.

We define a _concept_ as a mapping that represents the human process of attributing characteristics to data.

**Definition 2** (Concepts).: _A concept \(c\) is defined as a binary function: \(c:\{0,1\}\), which maps the data domain \(\) to the set of binary numbers. A value of \(1\) indicates the presence of the concept in the input, and \(0\) indicates its absence. Here, \(\) corresponds to the space of all concepts, that could be defined on \(\)._

In practice, given the dataset \(\), concepts are usually defined by labels, which reflect the judgments made by human experts. We define \(C=\{c_{1},...,c_{d}\}\) as a set of \(d\) atomic concepts, that are induced by labels of the dataset (also referred to as _primitive concepts_ or _primitives_). Within the context of this work, we permit concepts to be non-disjoint, signifying that each data point may have multiple concepts attributed to it. Additionally, we define a vector \(=[c_{1},,c_{d}]^{d}\).

A key step for explaining the abstractions learned by neural representations relies on the choice of the similarity measure between the concept and the representation. INVERT evaluates the relationship between representation and concepts by employing the non-parametric Area Under the Receiver Operating Characteristic (AUC) metric, measuring the representation's ability to distinguish between the presence and absence of a concept.

**Definition 3** (AUC similarity).: _Let \(f\) be a neural representation, dataset \(\) and concept \(c\). We define a similarity measure \(d:\) as_

\[d(f,c)=,c(x)=0\,\}}_{\{\,y y ,c(y)=1\,\}}[f(x)<f(y)]}{\{\,x x ,c(x)=0\,\}}\{\,y y,c(y)=1\,\}}, \]

_where \([f(x)<f(y)]\) is an indicator function that yields 1 if \(f(x)<f(y)\) and 0 otherwise._

AUC provides an interpretable measure to assess the ability of the representation to systematically output higher activations for the datapoints, where the concept is present. An AUC of \(1\) denotes a perfect classifier, while an AUC of \(0.5\) suggests that the classifier's performance is no better than random chance.

Given that various concepts have different numbers of data points associated with them, for concept \(c\) we can compute _concept fraction_, corresponding to the ratio of data points that are positively labeled by the concept:

\[T(c)=,c(x)=1\,\}}{\{\,x x \,\}}. \]

### Finding Optimal Compositional Explanations

Given a representation \(f\), the INVERT's objective is to identify the concept, that maximizes the AUC similarity with the representation, or, in other words finding the concept that representation is detecting the best. Due to the ability of representations to detect shared features across various concepts explaining a representation with a single atomic concept from \(C\) may not provide a comprehensive explanation. To surmount this challenge, we adopt the existing _compositional concepts_approach , and we augment the set of atomic concepts \(C\) by introducing new generic concepts, as a logical combination of existing ones. These logical forms involve the composition of AND, OR, and NOT operators, and they are based on the atomic concepts from \(C\).

**Definition 4** (Compositional concept).: _Given a vector of atomic concepts \(\), a compositional concept \(\) is a higher-order interpretable function that maps \(\) to a new, compositional concept:_

\[:^{d}. \]

For example, let \(C=\{c_{1},c_{2}\}\) be a set of atomic concepts with corresponding vector \(\). Let \(c_{1}\) be a concept for "dog", and \(c_{2}\) a concept for "llama". Then \(()=c_{1}\) OR \(c_{2}=\) "dog" OR "llama" is a compositional concept with the length \(L=2\). The \(()\) is a concept in itself (i.e. \(()\)) and corresponds to a concept that is positive for all images of dogs or llamas in the dataset.

Evaluating the performance of all conceivable logical forms across all of the \(d\) concepts from \(C\) is generally computationally infeasible. Consequently, the set of potential compositional concepts \(_{L}\) is restricted to a form of predetermined length \(L\), where \(L\) is a parameter of the method. The objective of INVERT, in this context, can be reformulated as:

\[^{*}=*{arg\,max}_{_{L}}d(f,( )). \]

To determine the optimal compositional concept that maximizes AUC, we employ an approach similar to that used in , utilizing Beam-Search optimization. Parameters of the proposed method include predetermined length \(L\), the beam size \(B\). Additionally, during the search process explanations could be constrained to the condition \(T(())[,]\), where \(0< 0.5\). In Section 4.1, we further demonstrate that by imposing a such constraint on the concept fraction resulting explanations could be made more comprehensive. We refer to the standard approach when \(=0,=0.5\). In our experiments, unless otherwise specified, the parameter \(\) is set to \(0.5\). Additional details and a description of the algorithm can be found in Appendix A.3.

Figure 1 illustrates the INVERT pipeline for explaining the neuron from ResNet18 Average Pooling layer . For this, we employed the validation set of ImageNet2012  as the dataset \(_{I}\) in the INVERT process. This subset contains 50,000 images from 1,000 distinct, non-overlapping

Figure 1: Demonstration of the INVERT method (\(B=1,=0.35\%\)) for the neuron \(f_{33}\) from ResNet18, AvgPool layer (Neuron 33), using ImageNet 2012 validation dataset. The resulting explanations can be observed in the bottom part of the figure, where three steps of the iterative process are demonstrated from \(L=1\) to \(L=3\). It can be observed that INVERT explanations align with the neurons high-activating images, illustrated in the top right figure.

classes, each represented by 50 images. Notably, since ImageNet classes are intrinsically linked to WordNet , we extracted an additional 473 hypernyms, or higher-level categories, and assigned labels for these overarching classes. In Figure 1 and subsequent figures, we use beige color to represent individual ImageNet classes and orange color to represent hypernyms. In the density plot graphs, the orange density illustrates the distribution of data point activations that belong to the explanation concept, while blue represents the distribution of activations of data points corresponding to the negation of the explanation.

### Statistical significance

IoU-based explanations, such as those provided by the Network Dissection method , often report small positive IoU scores for the resulting explanations. This raises concerns about the potential randomness of the explanation. The AUC value is equivalent to the Wilcoxon-Mann-Whitney statistic  and can be interpreted as a measure based on pairwise comparisons between classifications of the two classes. Essentially, it estimates the probability that the classifier will rank a randomly chosen positive example higher than a negative example .

Given the concept \(c\), this connection to the Mann-Whitney U test allows us to test if the distribution of the representations activations on the data points where concept \(c\) is positive significantly differs from the distribution of activations on points where the concept is negative. We can then report the corresponding \(p\)-value (against a double- or one-sided alternative), which helps avoid misinterpretations due to randomness, thereby improving the reliability of the explanation process, as shown in Figure 2. In all subsequent figures, the explanations provided by INVERT achieve statistical significance (against double-sided alternative) with a standard significance level (\(0.05\)).

## 4 Analysis

In this section, we provide additional analysis of the proposed method, including the effect of constraining the concept fraction of explanations and comparison of the INVERT to the prior methods.

### Simplicity-Precision tradeoff

The INVERT method is designed to identify the compositional concept that has the highest AUC similarity to a given representation. However, the standard approach neglects to account for the class imbalance between datapoints that belong and do not belong to a particular concept, often leading to _precise_ but narrowly applicable explanations due to the small concept fraction. To mitigate this

Figure 2: The figure illustrates the contrast between a _poor_ explanation (on the left) and INVERT explanations with \(L=1\) and varying parameter \(\), for neuron 592 in the ViT B 16 feature-extractor layer. The INVERT explanations were computed over the ImageNet 2012 validation set. The figure demonstrates that as the parameter \(\) increases, the concept fraction \(T\) also increases, indicating that more data points belong to the positive class. Furthermore, this figure showcases the proposed methods ability to evaluate the statistical significance of the result. The poor explanation fails the statistical significance test (double-sided alternative) with a p-value of 0.35, while all explanations provided by INVERT exhibit a \(p<0.005\).

issue, we can modify the INVERT process to work exclusively with compositional concepts where the fraction equals or exceeds a specific threshold, represented as \(\).

For this experiment, the INVERT method was utilized on the feature extractors of four different models trained on ImageNet. These models include ResNet18 , GoogleNet , EfficientNet B0 , and ViT B 16 . In this experiment, we examined 50 randomly chosen neurons from the feature-extractor layer of each model. We utilized the ImageNet 2012 validation dataset \(_{I}\), which was outlined in the previous section, to generate INVERT explanations with \(B=3\) varying the explanation length \(L\) between \(1\) and \(5\), and parameter \(\), responsible for the constraining the concept fraction, \(\{0,0.002,0.005,0.01\}\).

The experiments results are depicted in Figure 4. For all models, we can see an effect that we call the _simplicity-precision tradeoff_: the explanations with the highest AUC typically involve just one individual class with a low concept fraction, achieved in an unrestricted mode with parameter \(\) set to \(0\). By constraining the concept fraction \(\) and increasing the explanation length \(L\), we can improve AUC scores while still maintaining the desired concept fraction. Still, this indicates that more generalized, broader explanations come at the cost of a loss in precision in terms of the AUC measure. Figure 3 demonstrates how the change of parameter \(\) affects the resulting explanation.

### Evaluating the Accuracy of Explanations

While it is generally challenging to obtain ground-truth explanations for the latent representations in Deep Neural Networks (DNNs), in Supervised Learning, the concepts of the output neurons are

Figure 4: Impact of the parameter \(\) and formula length \(L\) on the resulting explanations. The first row of the figure shows the average AUC of optimal explanations for 50 randomly sampled neurons from the feature-extractor part of each one of the four ImageNet pre-trained models, conditioned by different values of parameter \(\) in different colors. These graphs indicate that neurons generally tend to achieve the highest AUC for one individual class with \(L=1\) and \(=0\). The second row presents the distribution of AUC scores alongside the distribution of concept fractions \(T\) for the INVERT explanations of length \(L=5\), for each model. Here, we can observe a clear trade-off between the precision of the explanation in terms of AUC measure and concept size \(T\).

Figure 3: Three different INVERT explanations, computed by adjusting the parameter \(\) for the Neuron 88 in ResNet18 AvgPool layer. Higher values of this parameter lead to broader explanations, albeit at the cost of precision, thus resulting in a lower AUC. The visualization of the WordNet taxonomy for the hypernyms is provided in the Appendix 3.

defined by the specific task. In the subsequent experiment, we compared the performance of INVERT and Network Dissection in accurately explaining neurons when the ground truth is known.

For this experiment, we employed 5 different models: 2 segmentation models and 3 classification models. For image segmentation, we employed MaskRCNN ResNet50 FPN model , pre-trained on MS COCO dataset  and evaluated on a subset of 24,237 images of MS COCO train2017, containing 80 distinct classes, and FCN ResNet50 model , pre-trained on MS COCO, and evaluated on a subset of MS COCO val2017, limited to the 20 categories found in the Pascal VOC dataset . For classification models we employed ImageNet pre-trained ResNet18  DenseNet161 , and GoogleNet , with 1,000 output neurons, each neuron corresponding to the individual class in the ImageNet dataset.

The outputs from the segmentation models were converted into pixel-wise confidence scores. These scores were arranged in the format \([N_{B},N_{c},H,W],\) where \(N_{B}\) represents the number of images in a batch, and \(N_{c}\) signifies the number of classes. Each value indicates the likelihood of a specific pixel belonging to a particular class. To aggregate multidimensional activations, the INVERT method used a max-pool operation.

All the classification models that were used had 1,000 one-dimensional output neurons. The evaluation process for both explanation methods was carried out using a subset of 20,000 images from the ImageNet-2012 validation dataset. For the Network Dissection method, which necessitates segmentation masks, these masks were generated from the bounding boxes included in the dataset. Both Network Dissection and INVERT methods were implemented using standard parameters.

Table 1 presents the outcomes of the evaluation process. It is noteworthy that INVERT exhibits superior or equivalent performance to Network Dissection across all tasks. Importantly, INVERT can accurately identify concepts in image segmentation networks using only the labels of images, in comparison to the Network Dissection method that uses segmentation masks.

#### Computational cost comparison

Methods such as Network Dissection and Compositional Explanations (CompExp) of neurons have been observed to exhibit computational challenges mainly due to the operations on high-dimensional masks. While CompExp and INVERT share a beam-search optimization mechanism, the proposed approach allows for less computational resources since logical operations are performed on binary labels, instead of masks. Figure 5 showcases the running time of applying INVERT and Compositional Explanations for explaining 2048 neurons in layer 4 of the FCOS-ResNet50-FPN model  pre-trained on the MS COCO dataset  on a singe Tesla V100S-PCIE-32GB GPU.

**Model** & **Dataset** & **NetDissect** & **INVERT** \\  MaskRCNN ResNet50 FPN & MS COCO & 95.06\% & **98.77\%** \\ FCN ResNet50 & MS COCO & **95.24\%** & **95.24\%** \\  ResNet18 & ImageNet & 19.2\% & **73.2\%** \\ GoogleNet & ImageNet & 19.7\% & **82.2\%** \\ DenseNet161 & ImageNet & 19.1\% & **86.9\%** \\ 

Table 1: A comparison of explanation accuracy between NetDissect and INVERT. The accuracy is computed by matching identified classes with the ground truth labels.

Figure 5: Comparing the computational cost of INVERT with Compositional Explanations of Neurons method (CompExp) in hours with varying formula lengths.

The time comparison of varying formula lengths demonstrates the advantage of INVERT being more effective computationally, which leads to reduced running time and computational costs.

## 5 Applications

In this section, we outline some specific uses of INVERT, including auditing models for spurious correlations, explaining circuits within the models, and manually creating circuits with desired characteristics.

### Finding Spurious Correlations by Integrating New Concepts

Due to the widespread use of Deep Neural Networks across various domains, it is crucial to investigate whether these models display spurious correlations, backdoors, or base their decisions on undesired concepts. Using the known spurious dependency of ImageNet-trained models on watermarks written in Chinese [19; 66; 67] we illustrate that INVERT provides a straightforward method to test existing hypotheses regarding the models dependency on specific features and allows for identification of the particular neurons accountable for undesirable behavior.

To illustrate this, we augmented the ImageNet dataset \(_{I}\), with an additional dataset, \(_{T}\), comprising 100 images. This new dataset contains 50 images for each of two distinct concepts: Chinese textual watermarks and Latin textual watermarks (see Appendix A.4). We created examples of these classes by randomly selecting images from the ImageNet dataset and overlaying them with randomly generated textual watermarks. Figure 6 depicts the change in the explanation process conducted on the original dataset and its expanded version. Since the original dataset didn't include the concept of watermarked images, the label "African chameleon" was attributed to the representation. However, after augmenting the dataset with two new classes, the explanation shifted to the "Chinese text" concept, with the AUC measure increasing to 0.99. This demonstrates the capability of INVERT to pinpoint sources of spurious behavior within the latent representations of the neural network.

### Explaining Circuits

INVERT could be employed for explaining circuits - computational subgraphs within the model, demonstrating the information flow within the model . The analysis of circuits enables us to understand complex global decision-making strategies by examining how features transform from one layer to another. Furthermore, this approach can be employed for _glocal_ explanations  - local explanation of a particular data point can be deconstructed into local explanations for individual neurons in the preceding layers, explained by INVERT.

To illustrate this, we computed INVERT explanations (\(L=3,=0.002\)) for all neurons in the average pooling layer of ResNet18. This was based on the augmented dataset from the preceding section. In ResNet18, the neurons in the Average Pooling layer have a linear connection to the output class logits. Figure 7 (left) illustrates the circuit of the three most significant neurons (based on the weight of linear connection) linked to the "carton" output logit. It could be observed that this class depends on Neuron 296, a "box" detector, and Neuron 154, which identifies the "Chinese text" concept. Furthermore, the right side of Figure 7 depicts the decomposition of local explanations: given an image of a carton box, we can dissect the GradCam  local explanation of a "carton" class-logit into the composition of local explanations from individual neurons. It is noticeable how Neuron 296 assigns relevance to the box, while Neuron 154 assigns relevance solely to the watermark present in the image. More illustrations of different circuits can be found in Appendix A.8.

Figure 6: Difference of INVERT (\(L=1,=0\)) explanations of Neuron 154 in Average Pooling layer of ImageNet-trained ResNet18 model before (top) and after (bottom) integration of new concepts to the dataset.

### Handcrafting Circuits

In this section, we demonstrate that its somewhat feasible to use the knowledge of what concepts are detected by neurons to combine them into manually designed circuits that can detect novel concepts. Just as compositional concepts are formed using logical operators, we employed fuzzy logic operators between neurons to construct meaningful handcrafted circuits with desired properties.

In contrast to conventional logic, fuzzy logic operators allow for the degree of membership to vary from 0 to 1 . For this experiment, we employed the Godel norm that demonstrated the best performance among other fuzzy logic operators (see Appendix A.5 for details). For the two functions \(f,g:\), the Godel AND (T-norm) operator is defined as \((f,g)\) and the OR (T-conorm) is defined as \((f,g)\). Negation is performed by the \(1-f\) operation.

We utilized the ImageNet-trained ViT L 16 model , specifically 1024 representations from the feature-extractor layer. The output of each of these representations was mapped to the range \(\) by first normalizing the output based on their respective mean and standard deviation across the ImageNet 2012 validation dataset, and then applying the Sigmoid transformation. In this experiment, for each of the 1473 ImageNet atomic concepts (which includes 1000 classes and 473 hypernyms), we identified a neuron from the feature-extractor layer that showed the highest AUC similarity. For instance, for the concept "boat", Neuron 61 exhibited the highest AUC similarity (denoted as \(f_{}\)), for the concept "house", Neuron 899 showed the highest AUC similarity (denoted as \(f_{}\)), and for the concept "lakeside", Neuron 575 showed the highest AUC similarity (denoted as \(f_{}\)).

Further, we manually constructed six different compositional formulas using concepts from ImageNet that were designed to resemble different concepts from the Places365  dataset. For example, for the "boathouse" class from Places365, we assumed that images from this class would likely include "boat", "house", and water, represented by the concept "lakeside". As such, we constructed a compositional formula "boat" AND "house" AND "lakeside" using concepts from the ImageNet dataset. Finally, using the neurons, that detect these concepts (e.g. \(f_{},f_{},f_{}\)) we manually constructed the circuits using Godel fuzzy logic operators. That is, for "boathouse" example, final circuit was formed as \(g(x)=(f_{}(x),f_{}(x),f_{}(x))\) using the Godel AND operator. The performance of the resulting circuits was evaluated on the Places365 dataset in terms of AUC similarity with the concept. In essence, by labeling representations using the ImageNet dataset and manually building a circuit guided by intuition, we evaluated how this newly created function can perform in detecting a class in the binary classification task on a different dataset.

Figure 8 illustrates the "boathouse" example and three other handcrafted circuits derived from ViT representations (the other two circuits can be found in Appendix 16). We found that after performing this manipulation, the AUC performance in detecting the Places365 class improved compared to the performance of each individual neuron. This example shows that by understanding the abstractions behind previously opaque latent representations, we can potentially construct meaningful circuits and utilize the _symbolic_ properties of latent representations. In Appendix A.6, we further demonstrate that when labels of the target dataset overlap or are similar to the dataset used for explanation, fine-tuning of the model can be achieved by simply employing representations with explanations matching the target labels.

Figure 7: The figure illustrates the “carton” circuit within the ResNet18 model. The left part of the figure showcases the three most significant neurons (in terms of the weight of linear connection) and their corresponding INVERT explanation linked to the class logit “carton”. The right part of the figure demonstrates how the local explanation from the class logit can be decomposed into individual explanations of individual neurons from the preceding layer.

## 6 Disscussion and Conclusion

In our work, we introduced the Inverse Recognition (INVERT) method, a novel approach for interpreting latent representations in Deep Neural Networks. INVERT efficiently links neurons with compositional concepts using an interpretable similarity metric and offers a statistical significance test to gauge the confidence of the resulting explanation. We demonstrated the wide-ranging utility of our method, including its capability for model auditing to identify spurious correlations, explaining circuits within models, and revealing _symbolic-like_ properties in connectionist representations.

While INVERT mitigates the need for image segmentation masks, it still relies on a labeled dataset for explanations. In future research, we plan to address this dependency. Additionally, we will explore different similarity measures between neurons and explanations, and investigate new ways to compose human-understandable concepts.

The widespread use of Deep Neural Networks across various fields underscores the importance of developing reliable and transparent intelligent systems. We believe that INVERT will contribute to advancements in Explainable AI, promoting more understandable AI systems.