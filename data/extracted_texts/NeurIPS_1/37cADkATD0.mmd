# Explore to Generalize in Zero-Shot RL

Ev Zisselman, Itai Lavie, Daniel Soudry, Aviv Tamar

Technion - Israel Institute of Technology

Correspondence E-mail: ev_zis@campus.technion.ac.il

###### Abstract

We study zero-shot generalization in reinforcement learning--optimizing a policy on a set of training tasks to perform well on a similar but unseen test task. To mitigate overfitting, previous work explored different notions of invariance to the task. However, on problems such as the ProcGen Maze, an adequate solution that is invariant to the task visualization does not exist, and therefore invariance-based approaches fail. Our insight is that learning a policy that effectively _explores_ the domain is harder to memorize than a policy that maximizes reward for a specific task, and therefore we expect such learned behavior to generalize well; we indeed demonstrate this empirically on several domains that are difficult for invariance-based approaches. Our _Explore to Generalize_ algorithm (ExpGen) builds on this insight: we train an additional ensemble of agents that optimize reward. At test time, either the ensemble agrees on an action, and we generalize well, or we take exploratory actions, which generalize well and drive us to a novel part of the state space, where the ensemble may potentially agree again. We show that our approach is the state-of-the-art on tasks of the ProcGen challenge that have thus far eluded effective generalization, yielding a success rate of \(83\%\) on the Maze task and \(74\%\) on Heist with \(200\) training levels. ExpGen can also be combined with an invariance based approach to gain the best of both worlds, setting new state-of-the-art results on ProcGen. Code available at [https://github.com/EvZissel/expgen](https://github.com/EvZissel/expgen).

## 1 Introduction

Recent developments in reinforcement learning (RL) led to algorithms that surpass human experts in a broad range of tasks (Mnih et al., 2015; Vinyals et al., 2019; Schrittwieser et al., 2020; Wurman et al., 2022). In most cases, the RL agent is tested on the same task it was trained on, and is not guaranteed to perform well on unseen tasks. In zero-shot generalization for RL (ZSG-RL), however, the goal is to train an agent on training domains to act optimally in a new, previously unseen test environment (Kirk et al., 2021). A standard evaluation suite for ZSG-RL is the ProcGen benchmark (Cobbe et al., 2020), containing 16 games, each with levels that are procedurally generated to vary in visual properties (e.g., color of agents in BigFish, Fig. 0(a), or background image in Jumper, Fig. 0(c)) and dynamics (e.g., wall positions in Maze, Fig. 0(d), and key positions in Heist, Fig. 0(e)).

Previous studies focused on identifying various _invariance properties_ in the tasks, and designing corresponding _invariant policies_, through an assortment of regularization and augmentation techniques (Igl et al., 2019; Cobbe et al., 2019; Wang et al., 2020; Lee et al., 2019; Raileanu et al., 2021; Raileanu and Fergus, 2021; Cobbe et al., 2021; Sonar et al., 2021; Bertran et al., 2020; Li et al., 2021). For example, a policy that is invariant to the color of agents is likely to generalize well in BigFish. More intricate invariances include the order of observations in a trajectory (Raileanu and Fergus, 2021), and the length of a trajectory, as reflected in the value function (Raileanu and Fergus, 2021).

Can ZSG-RL be reduced to only finding invariant policies? As a counter-argument, consider the following thought experiment2. Imagine Maze, but with the walls and goal hidden in the observation (Fig. 1f). Arguably, this is the most task-invariant observation possible, such that a solution can still be obtained in a reasonable time. An agent with memory can be trained to optimally solve all training tasks: figuring out wall positions by trying to move ahead and observing the resulting motion, and identifying based on its movement history in which training maze it is currently in. Obviously, such a strategy will not generalize to test mazes. Indeed, as depicted in Figure 2, performance in tasks like Maze and Heist, where the strategy for solving any particular training task must be _indicative_ of that task, has largely not improved by methods based on invariance (e.g. UCB-DrAC and IDAAC).

Interestingly, decent zero-shot generalization can be obtained even without a policy that generalizes well. As described by Ghosh et al. (2021), an agent can overcome test-time errors in its policy by treating the perfect policy as an _unobserved_ variable. The resulting decision making problem, termed the _epistemic POMDP_, may require some exploration at test time to resolve uncertainty. Ghosh et al. (2021) further proposed the LEEP algorithm based on this principle, which trains an ensemble of agents and essentially chooses randomly between the members when the ensemble does not agree, and was the first method to present substantial generalization improvement on Maze.

In this work, we follow the epistemic POMDP idea, but ask: _how to improve exploration at test time?_ Our approach is based on a novel discovery: when we train an agent to _explore_ the training domains using a maximum entropy objective (Hazan et al., 2019; Mutti et al., 2021), we observe that the learned exploration behavior generalizes surprisingly well--much better than the generalization attained when training the agent to maximize reward. Intuitively, this can be explained by the fact that reward is a strong signal that leads to a specific behavior that the agent can'memorize' during training, while exploration is naturally more varied, making it harder to memorize and overfit.

Exploration by itself, however, is not useful for solving new tasks. Our algorithm, _Explore to Generalize_ (ExpGen), additionally trains an ensemble of reward-seeking agents. At test time, either the ensemble agrees on an action, and we generalize well, or we take exploratory actions _using the exploration policy_, which we demonstrate to generalize, and drive us to a novel part of the state space, where the ensemble may potentially agree again.

ExpGen is simple to implement, and can be used with any reward maximizing RL algorithm. Combined with vanilla PPO, ExpGen significantly improves the state-of-the-art (SOTA) on several ProcGen games for which previous methods fail (see Fig. 2). ExpGen also significantly improves

Figure 1: (a),(b),(c),(d) and (e) displays screenshot of ProcGen games. (f) Imaginary maze with goal and walls removed (see text for explanation).

Figure 2: Normalized test Performance for ExpGen, LEEP, IDAAC, DAAC, and PPO, on five ProcGen games. ExpGen shows state-of-the-art performance on test levels of Maze, Heist and Jumper; games that are notoriously challenging for other leading approaches. The scores are normalized as proposed by (Cobbe et al., 2020).

upon LEEP, due to its effective test-time exploration strategy. For example, on Maze with \(200\) training levels, our method obtains \(83\%\) success on test tasks, whereas the previous state-of-the-art achieved \(66\%\). When combined with IDAAC (Raileanu and Fergus, 2021), the leading invariance-based algorithm, ExpGen achieves state-of-the-art performance on the full ProcGen suite (the full results are provided in Appendix D).

## 2 Related Work

Generalization in RLThe recent survey by Kirk et al. (2021) provides an extensive review of generalization in RL; here, we provide a brief overview. One approach to generalization is by artificially increasing the number of training tasks, using either procedural generation (Cobbe et al., 2019, 2020), or augmentations (Kostrikov et al., 2020; Ye et al., 2020; Lee et al., 2019; Raileanu et al., 2021), task interpolation (Yao et al., 2021) or various regularization technique, such as dropout (Igl et al., 2020) and batch normalization (Farebrother et al., 2018; Igl et al., 2020). Leading approaches, namely IDAAC (Raileanu and Fergus, 2021) and PPG (Cobbe et al., 2021), investigate the advantages of decoupling policy and value functions for generalization, whereas Jiang et al. (2021) propose automatic curriculum learning of levels.

A different approach is to add inductive bias to the neural network policy or learning algorithm. Approaches such as Tamar et al. (2016), Vlastelica et al. (2021), Boutilier et al. (2020) embed a differentiable planner or learning algorithm into the neural network. Other methods (Kansky et al., 2017; Toyer et al., 2018; Rivlin et al., 2020) combine learning with classical graph planning to generalize across various planning domains. These approaches require some knowledge about the problem structure (e.g., a relevant planning algorithm), while our approach does not require any task-specific knowledge. Another line of work aims to learn policies or features that are invariant across the different training tasks (Sonar et al., 2021; Bertran et al., 2020; Li et al., 2021; Igl et al., 2019; Stooke et al., 2021; Mazoure et al., 2020) and are thus more robust to sensory variations.

Using an ensemble to direct exploration to unknown areas of the state space was proposed in the model-based TEXPLORE algorithm of Hester and Stone (2013), where an ensemble of transition models was averaged to induce exploratory actions when the models differ in their prediction. Observing that exploration can help with zero-shot generalization, the model-free LEEP algorithm by Ghosh et al. (2021) is most relevant to our work. LEEP trains an ensemble of policies, each on a separate subset of the training environment, with a loss function that encourages agreement between the ensemble members. Effectively, the KL loss in LEEP encourages random actions when the agents of the ensemble do not agree, which is related to our method. However, random actions can be significantly less effective in exploring a domain than a policy that is explicitly trained to explore, such as a maximum-entropy policy. Consequentially, we observe that our approach leads to significantly better performance at test time.

State Space Maximum Entropy ExplorationMaximum entropy exploration (maxEnt, Hazan et al. 2019; Mutti et al. 2021) is an unsupervised learning framework that trains policies that maximize the entropy of their state-visitation frequency, leading to a behavior that continuously explores the environment state space. Recently, maximum entropy policies have gained attention in RL (Liu and Abbeel, 2021; Xu et al., 2021; Seo et al., 2021; Hazan et al., 2019; Mutti et al., 2021) mainly in the context of unsupervised pre-training. In that setting, the agent is allowed to train for a long period without access to environment rewards, and only during test the agent gets exposed to the reward signal and performs a limited fine-tuning adaptation learning. Importantly, these works expose the agent to the same environments during pre-training and test phases, with the only distinction being the lack of extrinsic reward during pre-training. To the best of our knowledge, our observation that maxEnt policies generalize well in the zero-shot setting is novel.

## 3 Problem Setting and Background

We describe our problem setting and provide background on maxEnt exploration.

#### Reinforcement Learning (RL)

In Reinforcement Learning an agent interacts with an unknown, stochastic environment and collects rewards. This is modeled by a Partially Observed Markov Decision Process (POMDP) (Bertsekas, 2012), which is the tuple \(M=(S,A,O,P_{init},P,,r,)\), where \(S^{|S|}\) and \(A^{|A|}\) are the state and actions spaces, \(O\) is the observation space, \(P_{init}\) is an initial state distribution, \(P\) is the transition kernel, \(\) is the observation function, \(r:S A\) is the reward function, and \([0,1)\) is the discount factor. The agent starts from initial state \(s_{0} P_{init}\) and at time \(t\) performs an action \(a_{t}\) on the environment that yields a reward \(r_{t}=r(s_{t},a_{t})\), and an observation \(o_{t}=(s_{t},a_{t}) O\). Consequently, the environment transitions into the next state according to \(s_{t+1} P(|s_{t},a_{t})\). Let the history at time \(t\) be \(h_{t}=\{o_{0},a_{0},r_{0},o_{1},a_{1},r_{1},o_{t}\}\), the sequence of observations, actions and rewards. The agent's next action is outlined by a policy \(\), which is a stochastic mapping from the history to an action probability \((a|h_{t})=P(a_{t}=a|h_{t})\). In our formulation, a history-dependent policy (and not a Markov policy) is required both due to partially observed states, epistemic uncertainty (Ghosh et al., 2021), and also for optimal maxEnt exploration (Mutti et al., 2022).

#### Zero-Shot Generalization for RL

We assume a prior distribution over POMDPs \(P(M)\), defined over some space of POMDPs. For a given POMDP, an optimal policy maximizes the expected discounted return \(_{,M}[_{t=0}^{}^{t}r(s_{t},a_{t})]\), where the expectation is taken over the policy \((h_{t})\), and the state transition probability \(s_{t} P\) of POMDP \(M\). Our generalization objective in this work is to maximize the discounted cumulative reward taken _in expectation over the POMDP prior_, also termed the _population risk_:

\[_{pop}()=_{M P(M)}[_{,M}[ _{t=0}^{}^{t}r(s_{t},a_{t})]]. \]

Seeking a policy that performs well in expectation over any POMDP from the prior corresponds to zero-shot generalization.

We assume access to \(N\) training POMDPs \(M_{1},,M_{N}\) sampled from the prior, \(M_{i} P(M)\). Our goal is to use \(M_{1},,M_{N}\) to learn a policy that performs well on objective 1. A common approach is to optimize the _empirical risk_ objective:

\[_{emp}()=_{i=1}^{N}_{,M_{i}}[ _{t=0}^{}^{t}r(s_{t},a_{t})]=_{M(M) }[_{,M}[_{t=0}^{}^{t}r(s_{t},a_{t}) ]], \]

where the empirical POMDP distribution can be different from the true distribution, i.e. \((M) P(M)\). In general, a policy that optimizes the empirical risk (Eq. 2) may perform poorly on the population risk (Eq. 1)--this is known as overfitting in statistical learning theory (Shalev-Shwartz and Ben-David, 2014), and has been analyzed recently also for RL (Tamar et al., 2022).

#### Maximum Entropy Exploration

In the following we provide the definitions for the state distribution and the maximum entropy exploration objective. For simplicity, we discuss MDPs--the fully observed special case of POMDPs where \(O=S\), and \((s,a)=s\).

A policy \(\), through its interaction with an MDP, induces a \(t\)-step state distribution \(d_{t,}(s)=p(s_{t}=s|)\) over the state space \(S\). Let \(d_{t,}(s,a)=p(s_{t}=s,a_{t}=a|)\) be its \(t\)-step state-action counterpart. For the infinite horizon setting, the stationary state distribution is defined as \(d_{}(s)=lim_{t}d_{t,}(s)\), and its \(\)-discounted version as \(d_{,}(s)=(1-)_{t=0}^{}^{t}d_{t,}(s)\). We denote the state marginal distribution as \(d_{T,}(s)=_{t=0}^{T}d_{t,}(s)\), which is a marginalization of the \(t\)-step state distribution over a finite time \(T\). The objective of maximum entropy exploration is given by:

\[(d())=-_{s d}[(d(s))], \]

where \(d\) can be regarded as either the stationary state distribution \(d_{}\)(Mutti and Restelli, 2020), the discounted state distribution \(d_{,}\)(Hazan et al., 2019) or the marginal state distribution \(d_{T,}\)(Lee et al., 2019b; Mutti and Restelli, 2020). In our work we focus on the finite horizon setting andadapt the marginal state distribution \(d_{T,}\) in which \(T\) equals the episode horizon \(H\), i.e. we seek to maximize the objective:

\[_{}()=_{M(M)}[(d_{ H,})]=_{M(M)}[( _{t=0}^{H}d_{t,}(s))], \]

which yields a policy that "equally" visits all states during the episode. Existing works that target maximum entropy exploration rely on estimating the density of the agent's state visitation distribution [Hazan et al., 2019, Lee et al., 2019b]. More recently, a branch of algorithms that employ nonparametric entropy estimation [Liu and Abbeel, 2021a, Mutti et al., 2021, Seo et al., 2021] has emerged, circumventing the burden of density estimation. Here, we follow this common thread and adapt the non-parametric entropy estimation approach; we estimate the entropy using the particle-based \(k\)-nearest neighbor (\(k\)-NN estimator) [Beirlant et al., 1997, Singh et al., 2003], as elaborated in the next section.

## 4 The Generalization Ability of Maximum Entropy Exploration

In this section we present an empirical observation--policies trained for maximum entropy exploration (maxEnt policy) generalize well. First, we explain the training procedure of our maxEnt policy, then we show empirical results supporting this observation.

### Training State Space Maximum Entropy Policy

To tackle objective (4), we estimate the entropy using the particle-based \(k\)-NN estimator [Beirlant et al., 1997, Singh et al., 2003], as described here. Let \(X\) be a random variable over the support \(^{m}\) with a probability mass function \(p\). Given the probability of this random variable, its entropy is obtained by \(_{X}(p)=-_{x p}[(p)]\). Without access to its distribution \(p\), the entropy can be estimated using \(N\) samples \(\{x_{i}\}_{i=1}^{N}\) by the \(k\)-NN estimator Singh et al. :

\[}_{X}^{k,N}(p)_{i=1}^{N}(\| x_{i}-x_{i}^{k}\|_{2}), \]

where \(x_{i}^{k}\) is the \(k\)-NN sample of \(x_{i}\) from the set \(\{x_{i}\}_{i=1}^{N}\).

To estimate the distribution \(d_{H,}\) over the states \(S\), we consider each trajectory as \(H\) samples of states \(\{s_{t}\}_{t=1}^{H}\) and take \(s_{t}^{k}\) to be the \(k\)-NN of the state \(s_{t}\) within the trajectory, as proposed by previous works [APT, Liu and Abbeel (2021a), RE3, Seo et al. (2021), and APS, Liu and Abbeel (2021a)],

\[}^{k,H}(d_{H,})_{t=1}^{H}( \|s_{t}-s_{t}^{k}\|_{2}). \]

Next, similar to previous works, since this sampled estimation of the entropy (Eq. 6) is a sum of functions that operate on each state separately, it can be considered as an expected reward objective \(}^{k,H}(d_{H,})_{t=1}^{H}r_{I}(s_{t})\) with the intrinsic reward function:

\[r_{I}(s_{t}):=(\|s_{t}-s_{t}^{k}\|_{2}). \]

This formulation enables us to deploy any RL algorithm to approximately optimize objective (4). Specifically, in our work we use the policy gradient algorithm PPO [Schulman et al., 2017], where at every time step \(t\) the state \(s_{t}^{k}\) is chosen from previous states \(\{s_{i}\}_{i=1}^{t-1}\) of the same episode.

Another challenge stems from the computational complexity of calculating the \(L_{2}\) norm of the \(k\)-NN (Eq. 7) at every time step \(t\). To improve computational efficiency, we introduce the following approximation: instead of taking the full observation as the state \(s_{i}\) (i.e. \(64 64\) RGB image), we sub-sample (denoted \(\)) the observation by applying average pooling of \(3 3\) to produce an image \(s_{i}^{}\) of size \(21 21\), resulting in:

\[r_{I}(s_{t}):=(\|s_{t}^{}-s_{t}^{k, }\|_{2}). \]We emphasize that we do not modify the termination condition of each game. However, a maxEnt policy will learn to _avoid_ termination, as this increases the sum of intrinsic rewards. In Figure 3 we display the states visited by a maxEnt policy on Maze. We also experimented with \(L_{0}\) as the state similarity measure instead of \(L_{2}\), which resulted in similar performance (see Appendix F.2).

### Generalization of maxEnt Policy

The generalization gap describes the difference between the reward accumulated during training \(_{emp}()\) and testing \(_{pop}()\) of a policy, where we approximate the population score by testing on a large population of tasks withheld during training. We can evaluate the generalization gap for either an extrinsic reward, or for an intrinsic reward, such as the reward that elicits maxEnt exploration (Eq. 8). In the latter, the generalization gap captures how well the agent's exploration strategy generalizes.

We found that agents trained for maximum entropy exploration exhibit a smaller generalization gap compared with the standard approach of training solely with extrinsic reward. Intuitively, this can be attributed to the extrinsic reward serving as an 'easy' signal to learn from, and overfit to in the training environments. To assess the generalization quality of the maxEnt policy, we train agents on \(200,500,1000\) and \(5000\) instances of ProcGen's Maze, Jumper and Miner environments using the intrinsic reward (Eq. 8). The policies are equipped with a memory unit (GRU, Cho et al. 2014) to allow learning of deterministic policies that maximize the entropy (Mutti et al., 2022)3.

The train and test return scores are shown in Fig. 3(a). In all three environments, we demonstrate a small generalization gap, as test performance on unseen levels closely follows the performance achieved during training. When considering Maze trained on \(200\) levels, we observe a small generalization gap of \(1.7\%\), meaning test performance closely follows train performance. For Jumper and Miner the maxEnt policy exhibits a small generalization gap of \(8.5\%\) and \(4.3\%\), respectively. In addition, we verify that the train results are near optimal by comparing with a hand designed approximately optimal exploration policy. For example, on Maze we use the well known maze exploring strategy _wall follower_, also known as the left/right-hand rule (Hendrawan, 2020); see Appendix B.2 for details.

Next, we evaluate the generalization gap of agents trained to maximize the extrinsic reward4. The results for this experiment, shown in Fig. 3(b), illustrate that the generalization gap for extrinsic reward

Figure 4: **Generalization ability of maximum entropy vs. extrinsic reward: (a) Score of maximum entropy. (b) Score of extrinsic reward. Training for maximum entropy exhibits a small generalization gap in Maze, Jumper and Miner. Average and standard deviation are obtained using \(4\) seeds.**

Figure 3: Example of a maxEnt trajectory on Maze. The policy visits every reachable state and averts termination by avoiding the goal state.

is more prominent. For comparison, when trained on \(200\) levels, the figure shows a large generalization gap for Maze (\(38.8\%\)) and Jumper (\(27.5\%\)), while Miner exhibits a moderate generalization gap of \(13.1\%\). For an evaluation on all ProcGen games, please see Appendix B.1.

## 5 Explore to Generalize (ExpGen)

Our main insight is that, given the generalization property of the entropy maximization policy established above, an agent can apply this behavior in a test MDP and expect effective exploration _at test time_. In the following, we pair this insight with the epistemic POMDP idea, and propose to play the exploration policy when the agent faces epistemic uncertainty, hopefully driving the agent to a different state where the reward-seeking policy is more certain. This can be seen as an adaptation of the seminal _explicit explore or exploit_ idea (Kearns and Singh, 2002), to the setting of ZSG-RL.

### Algorithm

Our framework comprises two parts: an entropy maximizing network and an ensemble of networks that maximize an extrinsic reward to evaluate epistemic uncertainty. The first step entails training a network equipped with a memory unit to obtain a maxEnt policy \(_{}\) that maximizes entropy, as described in section 4.1. Next, we train an ensemble of memory-less policy networks \(\{_{r}^{}\}_{j=1}^{m}\) to maximize extrinsic reward. Following Ghosh et al. (2021), we shall use the ensemble to assess epistemic uncertainty. Different from Ghosh et al. (2021), however, we do not change the RL loss function, and use an off-the-shelf RL algorithm (such as PPO (Schulman et al., 2017) or IDAAC (Stooke et al., 2021).

```
1:Input: ensemble size \(m\),
2: initial state \(s_{0}=()\).
3:\(n_{_{}}=0\)
4: Train maxEnt policy \(_{}\) using intrinsic reward \(r_{I}\) (Eq: 7).
5: Train \(m\) policies \(_{r}^{1},_{r}^{2}_{r}^{m}\) using extrinsic reward \(r_{ext}\).
6:for\(t=1\)to\(H\)do
7:\(a_{i}_{r}^{i}(|s_{t})\)
8:\(n_{_{}} n_{_{}}-1\)
9:if\(a_{i}(a_{j} j\{1 m\})\) and \(n_{_{}}<0\)then
10:\(a_{t}=a_{i}\)
11:else
12:\(a_{}_{}(|h_{t})\)
13:\(a_{t}=a_{}\)
14:\(n_{_{}} Geom()\)
15:endif
16:\(s_{t+1}(a_{t})\)
17:endfor
```

**Algorithm 1** Explore to Generalize (ExpGen)

At test time, we couple these two components into a combined agent \(\) (detailed as pseudo-code in Algorithm 1). We consider domains with a finite action space, and say that the policy \(_{r}^{i}\) is certain at state \(s\) if its action \(a_{i}\!\!_{r}^{i}(a|s)\) is in consensus, with the ensemble: \(a_{i}\!=\!a_{j}\) for the majority of \(k\) out of \(m\), where \(k\) is a hyperparameter of our algorithm. When the networks \(\{_{r}^{1}\}_{j=1}^{m}\) are not in consensus, the agent \(\) takes a sequence of \(n_{_{}}\) actions from the entropy maximization policy \(_{}\), which encourages exploratory behavior.

Agent meta-stabilitySwitching between two policies may result in a case where the agent repeatedly toggles between two states--if, say, the maxEnt policy takes the agent from state \(s_{1}\) to a state \(s_{2}\), where the ensemble agrees on an action that again moves to state \(s_{1}\). To avoid such "meta-stable" behavior, we randomly choose the number of maxEnt steps \(n_{_{}}\) from a Geometric distribution, \(n_{_{}} Geom()\).

## 6 Experiments

We evaluate our algorithm on the ProcGen benchmark, which employs a discrete 15-dimensional action space and generates RGB observations of size \(64 64 3\). Our experimental setup follows ProcGen's 'easy' configuration, wherein agents are trained on \(200\) levels for \(25M\) steps and subsequently tested on random levels (Cobbe et al., 2020). All agents are implemented using the IMPALA convolutional architecture (Espeholt et al., 2018), and trained using PPO (Schulman et al., 2017) or IDAAC (Raileanu and Fergus, 2021). For the maximum entropy agent \(_{}\) we incorporate a single GRU (Cho et al., 2014) at the final embedding of the IMPALA convolutional architecture. For all games, we use the same parameter \(=0.5\) of the Geometric distribution and form an ensemble of 10 networks. For further information regarding our experimental setup and specific hyperparameters, please refer to Appendix C.

### Generalization Performance

We compare our algorithm to six leading algorithms: vanilla PPO (Schulman et al., 2017), PLR (Jiang et al., 2021) that utilizes automatic curriculum-based learning, UCB-DrAC (Raileanu et al., 2021), which incorporates data augmentation to learn policies invariant to different input transformations, PPG (Cobbe et al., 2021), which decouples the optimization of policy and value function during learning, and IDAAC (Raileanu and Fergus, 2021), the previous state-of-the-art algorithm on ProcGen that decouples policy learning from value function learning and employs adversarial loss to enforce invariance to spurious features. Lastly, we evaluate our algorithm against LEEP (Ghosh et al., 2021), the only algorithm that, to our knowledge, managed to improve upon the performance of vanilla PPO on Maze and Heist. The evaluation matches the train and test setting detailed by the contending algorithms and their performance is provided as reported by their authors. For evaluating LEEP and IDAAC, we use the original implementation provided by the authors. 5

Tables 2 and 1 show the train and test scores, respectively, for all ProcGen games. The tables show that ExpGen combined with PPO achieves a notable gain over the baselines on Maze, Heist and Jumper, while on other games, invariance-based approaches perform better (for example, IDAAC leads on BigFish, Plunder and Climber, whereas PPG leads on CaveFlyer, and UCB-DrAC leads on Dodgeball). These results correspond to our observation that for some domains, invariance cannot be used to completely resolve epistemic uncertainty. We emphasize that _ExpGen substantially outperforms LEEP on all games_, showing that our improved exploration at test time is significant. In Appendix E we compare ExpGen with LEEP trained for \(50M\) environment steps, showing a similar trend. When combining ExpGen with the leading invariance-based approach, IDAAC, we establish that ExpGen is in-fact _complementary_ to the advantage of such algorithms, setting a new state-of-the-art performance in ProcGen. A notable exception is Dodgeball, where all current methods still fail.

Figures 6 and 7 show aggregate statistics of ExpGen, PPO, PLR, UCB-DrAC, PPG and IDAAC for all games 6, affirming the dominance of ExpGen+IDAAC as the state-of-the-art. The results are obtained using \(10\) runs per game, with scores normalized as in Appendix C.1. The shaded regions indicate \(95\%\) Confidence Intervals (CIs) and are estimated using the percentile stratified bootstrap with \(2,000\) (Fig. 6) and \(50,000\) (Fig. 7) bootstrap re-samples. Fig. 6 (Left) compares algorithm score-distribution, illustrating the advantage of the proposed approach across all games. Fig. 6 (Right)

Figure 5: Test performance of PPO trained using the reward \(r_{total}\) that combines intrinsic and extrinsic rewards, weighted by \(\) (Eq. 9). Each figure details the results for different values of discount factor \(\). All networks are randomly initialized and trained on \(200\) maze levels, and their mean is computed over \(4\) runs with different seeds. The figures show an improvement over the PPO baseline for \(=0.5\). In all cases, ExpGen outperforms the combined reward agent.

shows the probability of improvement of algorithm \(X\) against algorithm \(Y\). The first row (ExpGen vs. IDAAC) demonstrates that the proposed approach surpasses IDAAC with probability \(0.6\) and subsequent rows emphasize the superiority of ExpGen over contending methods at an even higher probability. This is because ExpGen improves upon IDAAC in several key challenging tasks and is on-par in the rest. Fig. 7 provides aggregate metrics of mean, median and IQM scores and optimality gap (as \(1-mean\)) for all ProcGen games. The figure shows that ExpGen outperforms the contending methods in all measures.

Ablation StudyOne may wonder if the ensemble in ExpGen is necessary, or whether the observation that the maxEnt policy generalizes well can be exploited using a single policy. We investigate the effect of combining the intrinsic and extrinsic rewards, \(r_{I}\) and \(r_{ext}\), respectively, into a single reward as a weighted sum:

\[r_{}= r_{I}+(1-)r_{}, \]

and train for \(=\{0.1,0.3,0.5,0.7,0.9\}\) on Maze. Figure 5 shows the train and test scores over \(50M\) steps for different values of discount factor \(\). We obtain the best test score for \(=0.5\) and \(=0.1\), illustrating an improvement compared with the PPO baseline. When comparing with ExpGen, the combined reward (Eq. 9) exhibits inferior performance with slightly higher variance. In Appendix C.2 and F, we also provide an ablation study of ensemble size and draw comparisons to variants of our algorithm.

   Game & PPO & PLR & UCB-DrAC & PPG & IDAAC & LEEP & ExpGen & ExpGen \\  & & & & & & & & (PPO) & (IDAAC) \\  BigFish & \(2.9 1.1\) & \(10.9 2.8\) & \(9.2 2.0\) & \(11.2 1.4\) & \(\) & \(4.9 0.9\) & \(6.0 0.5\) & \(\) \\ StarPilot & \(24.9 1.0\) & \(27.9 4.4\) & \(30.0 1.3\) & \(\) & \(37.0 2.3\) & \(3.2 2.2\) & \(31.0 0.9\) & \(39.8 2.9\) \\ Fruit Bot & \(26.2 1.2\) & \(28.0 1.4\) & \(27.6 0.4\) & \(27.8 0.6\) & \(\) & \(16.4 1.6\) & \(26.2 0.6\) & \(\) \\ BossFight & \(7.4 0.4\) & \(8.9 0.4\) & \(7.8 0.6\) & \(\) & \(\) & \(0.5 0.3\) & \(7.7 0.2\) & \(9.8 0.5\) \\ Ninja & \(6.1 0.2\) & \(\) & \(6.6 0.4\) & \(6.6 0.1\) & \(6.8 0.4\) & \(4.4 0.5\) & \(6.6 0.2\) & \(6.6 0.3\) \\ Plunder & \(7.8 1.6\) & \(8.7 2.2\) & \(8.3 1.1\) & \(14.3 2.0\) & \(\) & \(4.4 0.3\) & \(5.5 1.3\) & \(\) \\ CaceVyer & \(5.5 0.5\) & \(6.3 0.5\) & \(5.0 0.8\) & \(\) & \(5.0 0.6\) & \(4.9 0.2\) & \(5.7 0.3\) & \(5.3 0.7\) \\ CoinRun & \(8.6 0.2\) & \(8.8 0.5\) & \(8.6 0.2\) & \(8.9 0.1\) & \(\) & \(7.3 0.4\) & \(8.8 0.1\) & \(\) \\ Jumper & \(5.8 0.3\) & \(5.8 0.5\) & \(6.2 0.3\) & \(5.9 0.1\) & \(6.3 0.2\) & \(5.4 1.2\) & \(\) & \(\) \\ Chaser & \(3.1 0.9\) & \(6.9 1.2\) & \(6.3 0.6\) & \(\) & \(6.8 1.0\) & \(3.0 0.1\) & \(3.6 1.6\) & \(7.1 1.4\) \\ Climber & \(5.4 0.5\) & \(6.3 0.8\) & \(6.3 0.6\) & \(2.8 0.4\) & \(3.3 0.4\) & \(2.6 0.9\) & \(5.9 0.5\) & \(\) \\ Dodgeball & \(2.2 0.4\) & \(1.8 0.5\) & \(\) & \(2.3 0.3\) & \(3.2 0.3\) & \(1.9 0.2\) & \(2.9 0.3\) & \(2.8 0.2\) \\ Heist & \(2.4 0.5\) & \(2.9 0.5\) & \(3.5 0.4\) & \(2.8 0.4\) & \(3.5 0.2\) & \(4.5 0.3\) & \(\) & \(\) \\ Leaper & \(4.9 2.2\) & \(6.8 1.2\) & \(4.8 0.9\) & \(\) & \(7.7 1.0\) & \(4.4 0.2\) & \(4.0 1.7\) & \(7.6 1.2\) \\ Maze & \(5.6 0.1\) & \(5.5 0.8\) & \(6.3 0.1\) & \(5.1 0.3\) & \(5.6 0.3\) & \(6.6 0.2\) & \(\) & \(7.8 0.2\) \\ Miner & \(7.8 0.3\) & \(9.6 0.6\) & \(9.2 0.6\) & \(7.4 0.2\) & \(\) & \(1.1 0.1\) & \(8.0 0.7\) & \(\) \\   

Table 1: **Test score** of ProcGen games trained on \(200\) levels for \(25M\) environment steps. We compare our algorithm to PPO, PLR, UCB-DrAC, PPG, IDAAC and LEEP. The mean and standard deviation are computed over \(10\) runs with different seeds.

Figure 6: Comparison across all ProcGen games, with \(95\%\) bootstrap CIs highlighted in color. **Left.** Score distributions of ExpGen, PPO, PLR, UCB-DrAC, PPG and IDAAC. **Right.** Shows in each row, the probability of algorithm \(X\) outperforming algorithm \(Y\). The comparison illustrates the superiority of ExpGen over the leading contender IDAAC with probability \(0.6\), as well as over other methods with even higher probability.

## 7 Discussion and Limitations

We observed that policies trained to explore, using maximum entropy RL, exhibited generalization of their exploration behavior in the zero-shot RL setting. Based on this insight, we proposed ExpGen--a ZSG-RL algorithm that takes a maxEnt exploration step whenever an ensemble of policies trained for reward maximization does not agree on the current action. We demonstrated that this simple approach performs well on all ZSG-RL domains of the ProcGen benchmark.

One burning question is _why does maxEnt exploration generalize so well?_ An intuitive argument is that the maxEnt policy in an MDP is _invariant_ to the reward. Thus, if for every training MDP there are many different rewards, each prescribing a different behavior, the maxEnt policy has to be invariant to this variability. In other words, the maxEnt policy contains _no information_ about the rewards in the data, and generalization is well known to be bounded by the mutual information between the policy and the training data (Bassily et al., 2018). Perhaps an even more interesting question is whether the maxEnt policy is also less sensitive to variations in the dynamics of the MDPs. We leave this as an open theoretical problem.

Another consideration is safety. In some domains, a wrong action can lead to a disaster, and in such cases, exploration at test time should be hedged. One possibility is to add to ExpGen's policy ensemble an ensemble of advantage functions, and use it to weigh the action agreement (Rotman et al., 2020). Intuitively, the ensemble should agree that unsafe actions have a low advantage, and not select them at test time.

Finally, we point out that while our work made significant progress on generalization in several ProcGen games, the performance on Dodgeball remains low for all methods we are aware of. An interesting question is whether performance on Dodgeball can be improved by combining invariance-based techniques (other than IDAAC) with exploration at test time, or whether Dodgeball represents a different class of problems that requires a completely different approach.

   Game & PPO & PLR & UCB-DrAC & PPG & IDAAC & LEEP & ExpGen & ExpGen \\  & & & & & & & & & (PPO) & (IDAAC) \\  BigFish & \(8.9 2.0\) & \(7.8 1.0\) & \(12.8 1.8\) & \(19.9 1.7\) & \(\) & \(8.9 0.9\) & \(7.0 0.4\) & \(\) \\ StarPilot & \(29.0 1.1\) & \(2.6 0.3\) & \(33.1 1.3\) & \(\) & \(38.6 2.2\) & \(5.3 0.3\) & \(34.3 1.6\) & \(40.0 2.7\) \\ FruitBot & \(28.8 0.6\) & \(15.9 1.3\) & \(29.3 0.5\) & \(\) & \(29.1 0.7\) & \(17.4 0.7\) & \(28.9 0.6\) & \(29.5 0.5\) \\ BossFight & \(8.0 0.4\) & \(8.7 0.7\) & \(8.1 0.4\) & \(\) & \(10.4 0.4\) & \(0.3 0.1\) & \(7.9 0.6\) & \(9.9 0.7\) \\ Ninja & \(7.3 0.2\) & \(5.4 0.5\) & \(8.0 0.4\) & \(8.9 0.2\) & \(\) & \(4.6 0.2\) & \(8.5 0.3\) & \(7.9 0.6\) \\ Plunder & \(9.4 1.7\) & \(4.1 1.3\) & \(10.2 1.8\) & \(16.4 1.9\) & \(\) & \(4.9 0.2\) & \(5.8 1.4\) & \(\) \\ CaveFlyer & \(7.3 0.7\) & \(6.4 0.1\) & \(5.8 0.9\) & \(\) & \(6.2 0.6\) & \(4.9 0.3\) & \(6.8 0.4\) & \(5.5 0.5\) \\ CoinRun & \(9.4 0.3\) & \(5.4 0.4\) & \(9.4 0.2\) & \(\) & \(9.8 0.1\) & \(6.7 0.1\) & \(9.8 0.1\) & \(9.1 0.4\) \\ Jumper & \(8.6 0.1\) & \(3.6 0.5\) & \(8.2 0.1\) & \(8.7 0.1\) & \(\) & \(5.7 0.1\) & \(7.9 0.2\) & \(8.1 0.4\) \\ Chaser & \(3.7 1.2\) & \(6.3 0.7\) & \(7.0 0.6\) & \(\) & \(7.5 0.8\) & \(2.6 0.1\) & \(4.7 1.8\) & \(\) \\ Climber & \(6.9 1.0\) & \(6.2 0.8\) & \(8.6 0.6\) & \(10.2 0.2\) & \(10.2 0.7\) & \(3.5 0.3\) & \(7.7 0.4\) & \(\) \\ Dodgeball & \(6.4 0.6\) & \(2.0 1.1\) & \(\) & \(5.5 0.5\) & \(4.9 0.3\) & \(3.3 0.1\) & \(5.8 0.5\) & \(5.3 0.5\) \\ Heist & \(6.1 0.8\) & \(1.2 0.4\) & \(6.2 0.6\) & \(7.4 0.4\) & \(4.5 0.3\) & \(7.1 0.2\) & \(\) & \(7.0 0.6\) \\ Leaper & \(5.5 0.4\) & \(6.4 0.4\) & \(5.0 0.9\) & \(\) & \(8.3 0.7\) & \(4.3 2.3\) & \(4.3 2.0\) & \(8.1 0.9\) \\ Maze & \(9.1 0.2\) & \(4.1 0.5\) & \(8.5 0.3\) & \(9.0 0.2\) & \(6.4 0.5\) & \(9.4 0.3\) & \(\) & \(7.4 0.4\) \\ Miner & \(11.3 0.3\) & \(9.7 0.4\) & \(\) & \(11.3 1.0\) & \(11.5 0.5\) & \(1.9 0.6\) & \(9.0 0.8\) & \(11.9 0.2\) \\   

Table 2: **Train score** of ProcGen games trained on \(200\) levels for \(25M\) environment steps. We compare our algorithm to PPO, PLR, UCB-DrAC, PPG, IDAAC and LEEP. The mean and standard deviation are computed over \(10\) runs with different seeds.

Figure 7: Aggregate metrics for all ProcGen games: mean, median and IQM scores (higher is better) and optimality gap (lower is better), with \(95\%\) CIs highlighted in color. ExpGen outperforms the contending methods in all measures.

AcknowledgmentsThe research of DS was Funded by the European Union (ERC, A-B-C-Deep, 101039436). The research of EZ and AT was Funded by the European Union (ERC, Bayes-RL, 101041250). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency (ERCEA). Neither the European Union nor the granting authority can be held responsible for them. DS also acknowledges the support of the Schmidt Career Advancement Chair in AI.