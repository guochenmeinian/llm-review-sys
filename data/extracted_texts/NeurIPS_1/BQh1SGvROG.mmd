# AdaNCA: Neural Cellular Automata as Adaptors for More Robust Vision Transformer

Yitao Xu Tong Zhang Sabine Susstrunk

Image and Visual Representation Lab

Ecole polytechnique federale de Lausanne,Lausanne, Switzerland

{yitao.xu,tong.zhang,sabine.susstrunk}@epfl.ch

###### Abstract

Vision Transformers (ViTs) demonstrate remarkable performance in image classification through visual-token interaction learning, particularly when equipped with local information via region attention or convolutions. Although such architectures improve the feature aggregation from different granularities, they often fail to contribute to the robustness of the networks. Neural Cellular Automata (NCA) enables the modeling of global visual-token representations through local interactions, as its training strategies and architecture design confer strong generalization ability and robustness against noisy input. In this paper, we propose **Ad**aptor **N**eural **C**ellular **A**uotmata (AdaNCA) for Vision Transformers that uses NCA as plug-and-play adaptors between ViT layers, thus enhancing ViT's performance and robustness against adversarial samples as well as out-of-distribution inputs. To overcome the large computational overhead of standard NCAs, we propose _Dynamic Interaction_ for more efficient interaction learning. Using our analysis of AdaNCA placement and robustness improvement, we also develop an algorithm for identifying the most effective insertion points for AdaNCA. With less than a 3% increase in parameters, AdaNCA contributes to more than 10% of absolute improvement in accuracy under adversarial attacks on the ImageNet1K benchmark. Moreover, we demonstrate with extensive evaluations across eight robustness benchmarks and four ViT architectures that AdaNCA, as a plug-and-play module, consistently improves the robustness of ViTs.

## 1 Introduction

Vision Transformers (ViTs) exhibit impressive performance in image classification, through globally modeling token interactions via self-attention mechanisms . Recent works show that integrating local information into ViTs, _e.g.,_ using region attention  or convolution , further enhances the ViT's capabilities in image classification. Although advanced local structures contribute to better captures of local information, the robustness of ViTs has not increased. They remain vulnerable to noisy input such as adversarial samples  and out-of-distribution (OOD) inputs .

Recently, Neural Cellular Automata (NCA) was proposed as a lightweight architecture for modeling local cell interactions , where cells are represented by 1D vectors. To perform downstream tasks, similarly to the idea of token interactions in ViTs, cells in NCA interact with each other by alternating between a convolution-based _Interaction_ stage and an MLP-based _Update_ stage . The critical difference, however, is that cell interactions in NCA evolve over time by recurrent application of the two stages, whereas ViT computes the token interaction in a single step per layer. During this process, cells dynamically modulate their representations, based on the interactions with their neighbors, and they gradually enlarge their receptive fields. Unlike commonly used convolutional neural networks, NCA maintains resolution during neighborhood expansion. The recurrent update scheme enables thecells to explore various states, thus preventing NCA from overfitting and enhancing its generalization ability . NCA training involves various kinds of stochasticity , which enables the models to generalize to input variability and adapt to unpredictable perturbations. It is the modulation of local information and stochasticity during training that make NCA robust against noisy input .

However, the original NCA has substantial computational overhead when operating in high-dimensional space, which is a common scenario in ViTs. This poses a non-trivial challenge when integrating NCA into ViTs. To reduce the dimensionality of interaction results, hence to lower the computational cost, we propose _Dynamic Interaction_ to replace the standard _Interaction_ stage. In this stage, tokens dynamically modify the interaction strategy, based on the observation of their environment. This adaptation to the variability of the environment contributes to the model robustness. Our modified **Ad**aptor **NCA** (AdaNCA), as a plug-and-play module, improves ViTs performances, as illustrated in Figure 1. Adding AdaNCA to different ViT architectures consistently improves their robustness to both adversarial attacks and OOD input. AdaNCA also improves clean accuracy.

Motivated by empirical observations of the positive relationship between network redundancy and model robustness , we develop a dynamic programming algorithm for computing the most effective insert position for AdaNCA within a ViT, based on our proposed quantification of network redundancy. Our method results in consistent improvements across eight robustness benchmarks and four different baseline ViT architectures. Critically, we demonstrate that the improvements do not originate from the increase in parameters and FLOPS but are attributed to AdaNCA. Our contributions are as follows:

* We propose AdaNCA: It integrates **N**eural **C**ellular **A**utomata into ViTs' middle layers as lightweight **Ad**aptors for the robustness enhancement of ViTs against adversarial attacks and OOD inputs in image classification. With less than 3% more parameters, AdaNCA-extended ViTs can, under certain adversarial attacks, achieve 10% higher accuracy.
* We introduce _Dynamic Interaction_ to replace the _Interaction_ stage in standard NCA, thus enhancing model robustness and efficiency in terms of parameters and computation.
* We propose a method for determining the most effective insert positions of AdaNCA, for maximum robustness improvement.

## 2 Related Works

### Local structure in Vision Transformers

Since the proposal of Vision Transformer (ViT) , a series of works have introduced local structures into ViTs to enhance their performance . Here, we mention one of the earliest local structure modifications and those relevant to our work. Stand-Alone Self-Attention (SASA), as introduced by Ramachandran et al. , utilizes sliding window self-attention in ViTs. Following this, Liu et al.  develop a non-sliding window attention mechanism that partitions feature maps and computes self-attention, both within and between these partitions; it is termed Shifted Window (Swin) attention. Another method for modeling local information is convolution. D'Ascoli et al.  introduce a soft local-inductive bias by using gated positional self-attention, thus fusing self-attention and convolution. Despite these advancements in better modeling of local information, few methods lead to a more robust ViT architecture . This leaves the models to behave subpar when encountering slightly noisy inputs or distribution shifts.

Figure 1: The accuracy under adversarial attacks (APGD-DLR ) versus corruption error on out-of-distribution input (ImageNet-C ) of various ViT models . AdaNCA improves the robustness of different ViTs against both adversarial attacks and OOD input. \(\): the same model architecture but with more layers.

### Robust architecture in Vision Transformers

Researchers have developed various architectural changes for building more robust ViTs against adversarial attacks, such as FGSM  or PGD , as well as out-of-distribution (OOD) inputs, such as image corruption . Zhou et al.  propose Full Attention Networks to boost the robustness of ViTs against OOD images. Mao et al.  first systematically analyze the relationship between different components in a ViT, drawing a positive relationship between convolutional components and the robustness of ViTs against adversarial samples and OOD data. By extending  and , Guo et al.  propose input-dependent average pooling in order to adaptively select different aggregation neighborhoods for different tokens, thus achieving the state-of-the-art robust ViTs in OOD generalization. Different interpretations of the self-attention operation can also lead to more robust architectures [24; 58; 58]. However, the methods that introduce additional architectures [23; 58; 24; 78] are either implemented on ViTs with limited size or focus on non-adversarial robustness. On the contrary, our method introduces NCA as lightweight plug-and-play adaptors into base-level ViTs, thus enhancing their clean accuracy and robustness against both adversarial samples and OOD inputs.

### Neural Cellular Automata

Gilpin  demonstrates that CA can be represented by convolutional neural networks. By extending , Mordvintsev et al.  propose NCA in order to mimic the biological cell interactions and model morphogenesis. Following this idea, several works apply NCA in computer vision, including texture synthesis [43; 46; 49; 50], image generation [33; 47; 51; 62], and image segmentation [32; 57]. Randazzo et al.  propose applying NCA for modeling collective intelligence on image classification tasks, but the limitation to binary images restricts its practical application. Tesfaldet et al.  first establish the connection between ViT and NCA via recurrent local attention. It leads to a more robust model for handling image corruptions in image inpainting tasks. However, their application is limited to image impainting on small datasets such as MNIST  and CIFAR10 . The NCA in  attempts to emulate a ViT whereas our approach distinguishes itself by not doing so. We first applies NCA in image classification on ImageNet1K with base-level ViT models. Moreover, we propose the new _Dynamic Interaction_ for efficient cell interaction modeling, reducing the computational overhead, and for enhancing the model performance.

## 3 Method

The overview of our method is shown in Figure 2. In this section, we first review the NCA model and ViT architecture. In Section 3.1, we establish the connection between NCA and ViT, in terms of token interaction modeling. We then present the design of our AdaNCA in Section 3.2. We insert AdaNCA into the middle layers of ViT to improve its robustness. We introduce the relationship between the

Figure 2: Method overview. (a) To improve model performance and robustness, Neural Cellular Automata (NCA) can be inserted into Vision Transformers (ViTs) as **Adap**tors, hence termed AdaNCA. The details of AdaNCA are presented in Section 3.2. The improvement is maximized when AdaNCA is inserted between two layer sets that each consists of similar layers. (b) The robustness improvement brought by AdaNCA is highly correlated with the corresponding network redundancy quantification of the insert position introduced in Section 3.3. This supports the idea that AdaNCA should be placed between two sets of redundant layers.

insert position of AdaNCA and the relative improvements of model robustness in Section 3.3. This relationship leads to the algorithm for deciding the most effective placement for AdaNCA.

### Preliminaries

Vision TransformersVision Transformers (ViTs) operate on token maps \(^{N C}\), where the number of tokens is \(N\) and each token is represented by a \(C\)-dimensional vector. ViTs learn the interaction between these tokens via self-attention  and compute the interaction result \(_{attn}\), as described in Equation 1.

\[_{attn}=(^{}}{} ). \]

\(,,\) stand for query, key, and value, respectively. They are deduced from different linear projections of the input, i.e., \(=_{},=_ {},=_{}\), where \(_{},_{},_{} ^{C D}\). \(D\) is the hidden dimensionality in self-attention. \(\) is Softmax. After self-attention, tokens are fed into a Multilayer Perceptron (MLP) to obtain the updated representations \(_{out}\):

\[_{out}=f_{}(_{attn}). \]

\(f\) is the MLP and \(\) stands for its parameters. The self-attention and MLP form a single ViT block, and a ViT model can be built via stacking ViT blocks.

Neural Cellular AutomataNCA aims at modeling cell interactions. In the 2D domain, cells live on a 2D grid with size \(H W\). Each cell is represented by a vector with dimensionality \(C\). All cells collectively define the cell states \(^{H W C}\). In a single step of NCA, to generate the interaction output \(_{}\), cells first interact with their neighborhoods for an information exchange in the _Interaction_ stage ; the interaction is typically instantiated via depth-wise convolutions :

\[_{}=([_{1},_{2 },...,_{}])_{}. \]

\(_{i}\) is the \(i\)th convolutional kernel, and \(\) denotes the total number of kernels. '\(\)' denotes depth-wise convolution. The kernels can either be fixed  or learnable . The results of all kernels are concatenated channel-wise in the \(\) operation. \(_{}\) is then passed to an MLP in the _Update_ stage :

\[_{out}=f_{}(_{}). \]

\(_{out}\) is then used to update the cell states in a residual scheme. \(f\) is the MLP, and \(\) stands for its parameters. Typically, NCA uses the simplest MLP, with two linear layers and one activation between them. \(^{H W C}\), sampled from \(Bernoulli(p)\), is a random binary mask to introduce stochasticity in NCA; it ensures asynchronicity during the cell updates . \(\) is point-wise multiplication. NCA learns an underlying dynamic that governs the cell behaviors , as depicted by the stochastic differential equation (SDE) in Equation 5:

\[}{ t}=_{}() =f_{}[([_{1},_ {2},...,_{}])_{}]. \]

\(\) represents operations in _Interaction_ as well as _Update_ stages. \(\) is the set containing trainable parameters in the two stages. Discretizing the SDE with \( t=1\) naturally results in a recurrent residual update scheme:

\[^{t+1}=^{t}+_{}(^{t}) . \]

After \(t=T\) steps, the cell states \(S^{T}\) is extracted to accomplish certain downstream tasks. The traditional NCA involves several other specific designs though, in our case, it is impractical to adapt them. We provide a discussion on this topic in Appendix E.

#### 3.1.1 Connecting NCA and ViT

Both NCA and ViT learn interactions between a set of elements, i.e., tokens in ViT and cells in NCA. Hereafter, we refer to a cell in NCA as a token, aligning it with the concept in ViTs. The asynchronicity  introduced by the random mask \(\) can be regarded as a cell-wise stochastic depth , a more fine-grained version of the sample-wise stochastic depth. In previous NCA works, stochasticity is maintained during testing . Such a scheme is problematic in our case because (1) test-time stochasticity produces obfuscated gradients , leading to the circumvention of adversarialattacks, and (2) the model can output different results given the same inputs. To this end, we adopt the strategy in dropout-like techniques , which compensates activation values during training. Given \( Bernoulli(p)\), the evolution of NCA in AdaNCA is defined as:

\[:^{t+1}=^{t}+_{}( ^{t})}{p};:^{t+1}= ^{t}+_{}(^{t}). \]

We discuss the necessity of such a scheme in Section D.1 in the Appendix. Furthermore, NCA typically outputs the cell states at a random time step \(T\), resulting in random update steps for all cells. Such randomness ensures the stability of NCA across various time steps . Finally, the recurrent steps of NCA during a single training epoch enable the exploration of a wide range of cell states. In the early stage of training, the model is not adequately trained hence serves as a source of noise to itself through the recurrence. With all these components, the trained model can effectively handle the variability and unpredictability of the input thus be robust against noisy input. Our ablation studies in Section 4.3 demonstrate the effectiveness of these strategies in enhancing the model performance.

### AdaNCA architecture

The architecture of AdaNCA is shown in Figure 3. It shares a similar update scheme with the standard NCA, as described in Equation 7; but, it is more computationally efficient due to the proposed _Dynamic Interaction_ stage. All convolutional kernels for token interaction are trainable. In the following paragraphs, we first present the design of the _Dynamic Interaction_ stage and then introduce a way for more efficient token interaction by using multi-scale _Dynamic Interaction_.

#### 3.2.1 Dynamic Interaction

The _Interaction_ stage in the original NCA performs a channel-wise concatenation of the \(\) output tensors from different depth-wise convolutions. Whereas, our _Dynamic Interaction_ computes a weighted sum of those results. Specifically, a weight computation network \(_{}\) takes the token map \(^{H W C}\) as input and outputs per-token scalar weights \(_{m}^{H W 1}\) for each of the \(\) kernels. We modify Equation 3 to Equation 8:

\[_{}=(*_{}) ([_{1},_{2},...,_{ }])=(*_{})_{m }=_{m=1}^{}(_{c=1}^{C}_{m} _{mc}), \]

Figure 3: Overview of AdaNCA architecture. Instead of concatenating the interaction results generated by the depth-wise convolutions, our _Dynamic Interaction_ conducts a point-wise weighted sum on them to improve the efficiency and enhance the performance. The weights are obtained based on the token states so that each token can dynamically adjust, according to the inputs, the interaction strategy. The _Multi-scale Dynamic Interaction_ aggregates the results from _Dynamic Interaction_, where the convolutions have different dilation rates. Then, to finish one step of evolution, the output is fed into the _Update_ stage.

where \(_{mc}^{H W 1},_{D}, _{m}^{H W C}\)'\({}_{*}\)' denotes the convolution. Recall that '\(\)' is the depth-wise convolution. We instantiate the weight computation module \(_{}\) by using a two-layer convolutional network. The first layer transforms the dimensionality from \(C\) to \(\), and the second layer computes the actual weights, thus producing \(\) scalars for each token. Both layers use \(3 3\) convolutions to factor in information from both the token and its neighbors. To stabilize training, we add a batch normalization between the two convolutions. Our design of the weight computation network coincides with the one in . Although, our focus is on extracting various information from the same neighborhoods rather than on aggregating data from different neighborhoods.

#### 3.2.2 Multi-scale Dynamic Interaction

Inspired by , which uses multi-scale token _Interaction_ to facilitate long-range token communication, we propose multi-scale _Dynamic Interaction_. Concretely, all convolutions in Equation 8 now have one more degree of freedom in dilation. Dilation \(s\) represents the current operating scale being \(s\), and \(s\{1,2,...,\}\). Hence, the original _Dynamic Interaction_ is a special case where \(=1\). To increase the feature expressivity, we perform a weighted sum on the outputs of all scales, where the per-token weights \(_{Ms}\) are generated by a network \(_{M}\) as described in Equation 9.

\[_{MD}=(*_{M})_{D }=(*_{M})_{D}=_{s =1}^{}(_{c=1}^{C}_{Ms}_{D sc}), \]

where \(_{Ms},_{Dsc}^{H W 1}, _{MD},_{Ds}^{H W  C}\). The \(_{}\) in Equation 8 is shared across all scales. The weight computation network \(_{M}\) mirrors that of \(_{}\).

### Insert positions of AdaNCA

Given a ViT and an AdaNCA, to maximize the robustness improvements, we need to determine where to insert AdaNCA. To this end, we first establish the correlation between the placement of AdaNCA and the robustness enhancement it brings. Motivated by the fact that the network redundancy contributes to the model robustness , we hypothesize that the effect of AdaNCA should correlate to the layer redundancy corresponding to the insert position. To quantify the redundancy, we propose the **Set Cohesion Index**\(\). Given a trained model with \(L\) layers and two layer indices \(i,j\{1,2,...,L\}\) where \(i<j\), \((i,j)\) is defined in Equation 10.

\[(i,j)=}_{m,n[i,j]}Sim(m,n) -_{i>1}}{(i-1)(j-i+1)}_{m_{1}[1,i-1],n[ i,j]}Sim(m_{1},n) \] \[-_{j<L}}{(L-j)(j-i+1)}_{m[i,j],n_{1}[j+ 1,L]}Sim(m,n_{1})\]

\(\) stands for the indicator function. \(Sim(m,n)\) is the function for quantifying the output similarity between layer \(m\) and \(n\). We choose Centered Kernel Alignment (CKA) , a common metric for measuring layer similarities inside or between neural networks . A higher \(\) stands for a more cohesive layer set defined by layers from \(i\) to \(j\). Inserting AdaNCA after layer \(i\) would partition the network into two layer sets, and we can compute the sum of \(\) of the layers before and after \(i\), _i.e._, \((i)=(1,i)+(i+1,L)\). This serves as a quantification of the network redundancy that corresponds to position \(i\). We assume that AdaNCA will not change the layer similarity structure because it is too small compared to a single layer in all ViTs.

In addition to the quantification of the network redundancy, the robustness improvement, brought by AdaNCA, is quantified using the relative increase in the attack failure rate of the AdaNCA-inserted models and the corresponding baseline. Specifically, if a model can achieve \(\) clean test accuracy as well as \(^{}\) accuracy under adversarial attacks, the attack failure rate is \(=\). The robustness improvement \(\) is then defined as \(=}-_{}}{_{}}\). In our experiments, we find that \(\) is significantly correlated with the network redundancy \(\) (Pearson correlation \(r=0.6938,p<0.001\)). We refer readers to Appendix A for details of the experiments. The results validate our hypothesis and indicate that AdaNCA should be inserted into the position that can maximize network redundancy. We develop a dynamic programming algorithm to find these positions and refer readers to Appendix A.1 for the details.

[MISSING_PAGE_FAIL:7]

(IM-A) . For the PGD attack, we align with the settings used in : max magnitude \(=1\), step size \(=0.5\), steps \(t=5\). We refer readers to the Appendix C.7 for details of the other attacks. For testing the OOD generalization, we use ImageNet-C (IM-C) , ImageNet-R (IM-R) , and ImageNet-Sketch (IM-SK) . We report the mean corruption error (mCE) on ImageNet-C and accuracy on all other kinds of robustness benchmarks in Table 1. Our results highlight that AdaNCA-enhanced ViTs consistently outperform corresponding baselines in various robustness tests as well as in terms of clean accuracy. Importantly, the enlarged baseline models (\(\) sign) do not bring comparable improvements to AdaNCA, suggesting that the enhancements do not merely stem from the increase in computational budgets. However, the existing method  that introduces local structure into ViTs can potentially undermine the adversarial robustness of the baselines. In Table 2, we conduct an in-depth study on the corruption errors of the different categories of common corruptions in ImageNet-C. The results show that AdaNCA enhances robustness without the trade-off seen in methods that ignore texture information [17; 53]. While these methods may improve mCE for non-Blur noise, they often worsen mCE for Blur noise . In contrast, AdaNCA consistently improves robustness across most categories. We refer readers to Appendix C for more results.

### Layer similarity structure

Our key assumption in Section 3.3 is that AdaNCA will not change the layer similarity structure due to its small size, and that is why we use **pre-trained** networks to conduct this analysis. Here, we examine the pair-wise layer similarities in Swin-B  and Swin-B-AdaNCA in Figure 4. \(_{mean}\) is the mean of \(\) from all layer sets. We refer readers to Appendix C.14 for more results. AdaNCA not only preserves the original layer similarity structure but also contributes to a clearer stage partition, validating our assumption in Section 3.3. The results might be attributed to the fact that AdaNCA transmits information between different layer sets and thus layers inside each set do not bother adapting to the layers outside the set.

### Ablation studies

We conduct ablation studies on ImageNet100, a 100-class subset of ImageNet1K. Previous studies [7; 15; 72; 74] have shown that ImageNet100 serves as a representative subset of ImageNet1K. Hence, we can obtain representative results for the self-evaluation of the model while efficiently using the computational resources. All the ablation experiments are based on a Swin-tiny  model. We insert it after the fourth layer to obtain the best robustness improvement, according to our analysis in Section 3.3 and Appendix A. First, we ablate on two hyperparameters, the number of convolutional kernels used in the _Dynamic Interaction_ stage (\(\)) and the maximum scale (\(\)) used in the multi-scale _Dynamic Interaction_ stage. The Clean Accuracy and Attack Failure Rate are shown in Figure 5. The attack failure rate is quantified using the same method as in Section 3.3 and Appendix A. Multi-scale interaction can contribute to the performance while overly large scale can lose local information and complicate the process of selecting the interaction neighbors. This issue is also observed in a previous work . Increasing the number of kernels benefits the performance while too many kernels undermine the robustness. According to the results, we choose \(=2\), \(=4\). We then perform ablations on several design choices:

* **Recurrent update (Recur).** Ablation on unrolling the recurrence with average time step \(T\) in AdaNCA into \(T\) independent AdaNCA with time step being 1.
* **Stochastic update (StocU).** Ablate the stochastic update during training, leading to globally synchronized update of all tokens .
* **Random step (RandS).** Change the recurrence time step from a randomly chosen integer in range \([T_{1},T_{2}]\) to \((T_{1}+T_{2})/2\). It cannot be turned on without recurrence.
* **Dynamic interaction (DynIn).** Ablate the _Dynamic Interaction_ so that the interaction results are simply summed together. The number of kernels remains the same.

As shown in Table 3, the highest robustness improvement is achieved with all components. Among them, turning off the recurrence leads to the largest drop in the robustness, as recurrence allows the model to explore more cell states than finishing the update in a single step. While it achieves the highest clean accuracy, it uses \(\)4x more parameters than our method, and the improvement of the clean performance is likely due to more parameters. Without any of the two sources of randomness, stochastic update and random steps, the model cannot adapt to the variability of the inputs and thus exhibits vulnerabilities against adversarial attacks. Finally, turning off our Dynamic Interaction will cause drops in both clean accuracy and robustness, as tokens cannot decide their unique interaction weights and thus cannot generalize to noisy inputs. We provide extended ablation studies in Section D.2 in the Appendix.

### Noise sensitivity examination

A drawback of adversarially robust models is their increased sensitivity to noise in specific frequency bands . For instance, while adversarially trained models are robust against adversarial attacks, they can be sensitive to noise from a larger frequency-band range compared to standard models . Here, we use the method and data from  to examine the noise sensitivity of AdaNCA-enhanced ViTs. Specifically, we evaluate the classification performance on a set of images that are mixed with noise of varying magnitudes and frequencies. The noise is sampled from Gaussian distribution with zero mean and the standard deviation indicates its magnitude. Then, it is filtered within various spatial-frequency bands, resulting in different frequencies of noise. Higher classification accuracy on a specific noise type indicates that the model is less sensitive to that noise. Figure 6 presents the results, including human data sourced from . Our findings demonstrate that AdaNCA enhances ViTs by reducing the sensitivity to noise with certain frequency components, equipping ViTs with more human-like noise-resilient abilities. Crucially, AdaNCA improves the model robustness differently than adversarial training since the AdaNCA-enhanced ViTs do not exhibit increased sensitivity to the noise. We quantitatively validate the conclusion and refer readers to Appendix C.16 for the details.

## 5 Limitation

AdaNCA has certain limitations. First, the AdaNCA-equipped ViTs cannot adapt to unseen recurrent steps of AdaNCA, which limits the generalization ability. For example, if the training step range of AdaNCA is , it cannot produce meaningful results with the test step being 6. AdaNCA

   Exp. Type & Recur & StocU & RandS & DynIn & Params (M) & FLOPS (G) & Accuracy (\(\)) & Attack Failure Rate (\(\)) \\  Baseline & ✗ & ✗ & ✗ & ✗ & 27.59 & 4.5 & 86.56 & 12.29 \\   & ✗ & ✓ & ✗ & ✓ & 28.97 & 4.7 & **87.36** & 19.04 \\  & ✓ & ✗ & ✓ & ✓ & 27.94 & 4.7 & 86.92 & 19.56 \\  & ✓ & ✓ & ✗ & ✓ & 27.94 & 4.7 & 87.12 & 19.34 \\  & ✓ & ✓ & ✓ & ✗ & 27.93 & 4.7 & 86.72 & 21.98 \\  Ours & ✓ & ✓ & ✓ & ✓ & 27.94 & 4.7 & 87.18 & **22.35** \\   

Table 3: Ablation studies on design choices. Each of the AdaNCA design choices contributes to improved performance and robustness. Baseline is a Swin-tiny  model trained on ImageNet-100. **Bold** indicates the best model.

introduces a non-negligible computation into the original architecture. Our experiments are conducted on ImageNet1K with the image size of 224 \(\) 224. Whether AdaNCA can lead to impressive improvements on larger-scale problems, _e.g.,_ ImageNet22K, remains a question. The size of the images can also affect the efficiency of token interaction.

## 6 Broader Impact

AdaNCA contributes to more robust ViTs, facilitating their usage in real-world scenarios. We bridge two powerful models, NCA and ViT, on large-scale image classification, potentially encouraging research dedicated to their synergistic combination in more practical settings. Our findings on AdaNCA improving network redundancy can stimulate more works on architectural robustness in deep learning that involves increasing the redundancy to enhance robustness. In the context of this paper, we believe that AdaNCA does not introduce any significant negative implications.

## 7 Conclusion

We have proposed AdaNCA, an efficient Neural Cellular Automata (NCA) that, when inserted between their middle layers, improves ViT performances and robustness against adversarial attacks as well as out-of-distribution inputs. We design our model by connecting NCA and ViT, in terms of token interaction modeling, and we propose _Dynamic Interaction_ to improve the computational efficiency of standard NCA. Exploiting the training strategies and design choices in NCA, _i.e.,_ stochastic update, random steps, and multi-scale interaction, we further improve the AdaNCA-enhanced ViTs' clean accuracy and robustness. To decide the placement of AdaNCA, we propose the Set Cohesion Index that quantifies the network redundancy via layer similarity and conclude that AdaNCA should be inserted between two layer sets that consist of redundant layers. Our results demonstrate that AdaNCA consistently improves ViTs performances and robustness. Evidence suggests that the mechanism by which we obtain improvement reduces the sensitivity of ViTs to certain types of noise and makes the noise-resilient ability of ViTs similar to that of humans.

Figure 6: Classification accuracy of humans and ViTs on noisy images. The images are perturbed by Gaussian noise with different standard deviations (Magnitude of Noise) and are filtered with different spatial-frequency bands (Frequency of Noise). AdaNCA improves the accuracy on images with certain types of noise (**dotted black boxes**), indicating that it makes ViTs less sensitive to them. Quantitative results are presented in Appendix C.16.