# Can LLMs Learn by Teaching for Better Reasoning?

A Preliminary Study

Xuefei Ning\({}^{*1}\), Zifu Wang\({}^{*2}\), Shiyao Li\({}^{*1,3}\), Zinan Lin\({}^{*4}\), Peiran Yao\({}^{*3,5}\),

**Tianyu Fu\({}^{1,3}\), Matthew B. Blaschko\({}^{2}\), Guohao Dai\({}^{6,3}\), Huazhong Yang\({}^{1}\), Yu Wang\({}^{1}\) \({}^{1}\)**Tsinghua University \({}^{2}\)KU Leuven \({}^{3}\)Infinigence-AI

\({}^{4}\)Microsoft Research \({}^{5}\)University of Alberta \({}^{6}\)Shanghai Jiao Tong University

Equal contribution.Corresponding to: foxdoraame@gmail.com (Xuefei Ning), zinanlin@microsoft.com (Zinan Lin), yuwang@tsinghua.edu.cn (Yu Wang)

###### Abstract

Teaching to improve student models (e.g., knowledge distillation) is an extensively studied methodology in LLMs. However, in human education, teaching enhances not only the students but also the teachers by fostering more rigorous and clearer reasoning, as well as deeper knowledge building. We ask: _Can LLMs also learn by teaching (LbT) for better reasoning?_ If the answer is yes, we can potentially unlock the possibility of continuously advancing the models without solely relying on human-produced data or stronger models. In this paper, we provide a preliminary exploration of this question. We show that LbT ideas can be incorporated into existing LLM training/prompting pipelines and bring improvements. Specifically, we design three methods, each mimicking one of the three levels of LbT: observing students' feedback, learning from the feedback, and learning iteratively, with the goal of improving answer accuracy without training or improving models' inherent capability with fine-tuning. We reveal some findings: (1) _Teaching materials that make it easier for students to learn (via in-context learning) have clearer and more accurate logic_; (2) _Weak-to-strong generalization_: LbT might help improve strong models by teaching weak models; (3) _Diversity in students might help_: teaching multiple students could be better than teaching a single student or the teacher alone. We hope that our exploration can inspire future research on LbT and, more broadly, the adoption of advanced education techniques to improve LLMs. The code and website are at [https://github.com/imagination-research/lbt](https://github.com/imagination-research/lbt) and [https://sites.google.com/view/llm-learning-by-teaching](https://sites.google.com/view/llm-learning-by-teaching).

## 1 Introduction

_I couldn't reduce it to the freshman level. That means we really don't understand it._

_- Richard Feynman_

_"Learning from teachers (LfT)"_ is a common pipeline in machine learning, especially in the realm of Large Language Models (LLMs). For example, knowledge distillation  and distillation via synthetic data  focus on transferring the knowledge from teacher LLMs to student LLMs by letting teacher models _teach_ student models through token logits, features, or synthetic data . They become the go-to methods for closing the performance gap between open-source and proprietary LLMs, as well as for maintaining performance during model compression.

In fact, in human learning, _teaching not only benefits students but can also improve the teachers themselves_. _"Learning by teaching (LbT)"_, also known as the Feynman learning method, is proven to improve human learning by fostering rigorous and clear reasoning as well as knowledge building . Fig. 1 illustrates the conceptual comparison of the LfT and LbT pipelines.

Motivated by this insight, in order to improve one of the most crucial abilities of LLMs - the reasoning ability, we want to ask: _Can LLMs also learn by teaching for better reasoning?_ In addition to improving reasoning, as one can imagine, LbT could open exciting opportunities for the models to _continuously evolve_ by teaching other (potentially weaker) models, rather than solely relying on human-produced data or stronger teacher models. More broadly, we hope that this exploration could provide insights on borrowing advanced education techniques to improve LLMs .

To explore this question, we draw on learning science literature that connects LbT in human learning with reflection  and knowledge-building , summarizing three _levels_ of LbT:

* **L1: Observing students' feedback.** The teacher instructs the students, who then provide feedback (e.g., taking exams and reporting the scores, asking questions about unclear logic).
* **L2: Learning from the feedback.** Based on the feedback, the teacher can analyze which logic and concepts the students might have (mis)understood. This information is useful for the teachers to improve their teaching strategy, and further enhance the teacher's own understanding of the concepts.
* **L3: Learning from the feedback iteratively.** The teacher can teach the students, observe the feedback (L1), and learn from the feedback (L2) _iteratively_.

In this paper, we study the viability of instantiating these LbT ideas in LLMs. There is a range of possibilities in terms of the objective, the pipeline, and the implementation (SS 2 and Tab. 1). As an initial exploration, we study three methods, each for one of the three LbT levels.

* **M1** aims at improving LLMs' answer quality by directly utilizing students' feedback (L1). More specifically, given a set of generated answers, _score each rationale based on its ability to teach student models using in-context learning (ICL) to correctly answer similar problems_. We show that aggregating multiple rationales  with LbT-based scores can improve the answer accuracy. Notably, **M1** improves GPT-40's accuracy on the MATH dataset  from 87.84% to 96.69%.
* **M2** aims at improving LLMs' inherent ability by learning from students' feedback (L2). We use the approach in **M1** to score teacher-generated rationales. Then, we apply direct preference optimization (DPO)  to fine-tune the teacher model with the rationale-score pairs. We show that **M2** is better than using DPO with correctness scores.
* **M3** aims at improving LLMs' answer quality by iteratively learning from students' feedback (L3). Specifically, we prompt the LLM to _reflect on the failure cases of multiple students_ and _devise new positive and negative exemplars_. We show that the LLM can improve the exemplars based on feedback from multiple students. These improved exemplars used in prompts not only improve the learning outcomes for multiple students but also enhance the teacher's performance.

We reveal some interesting or promising findings related to LbT:

   LbT & Objective & Pipeline & LbT &  \\ Level & Improve the answer quality & & & & \\  & without training & & & & **M1** (§ 3) \\  L2 & Improve the inherent model & & & & \\ ability with training & & & & & **M2** (§ 4) \\  L3 & Improve the answer quality & & & & \\ without training & & & & & \\   

Table 1: The explored **M1**, **M2**, **M3** methods.

Figure 1: **Left:**_Learning from teacher_ aims at improving student LLMs with knowledge from the teacher LLMs. It is the essential idea behind common approaches including knowledge distillation and distillation via synthetic data. **Right:** In contrast, _Learning by teaching_ aims at improving _teacher LLMs_ through the teaching process using feedback from student LLMs.

* **Teaching materials that make it easier for students to learn have clearer and more accurate logic (LbT-TMQ1 assumption)** when using LCL as the student's "learning" method: Our LbT-based scoring relies on this assumption, and our results and inspection support this assumption. * **Weak-to-strong generalization**: Strong teachers can improve even when teaching weaker students, suggesting some promise of using LbT to improve superhuman models .
* **Diversity in students might help**: Rather than teaching the teacher itself, teaching _other_ students and _multiple_ students might help. This suggests the feasibility of using LbT to synergize the capability and knowledge from multiple models.

To summarize, with appropriate pipelines and teacher-student settings, LbT can help improve LLMs' answer quality and inherent capability. We believe that these preliminary case studies are only scratching the surface of the potential of LbT. As LLMs are becoming increasingly powerful, more advanced approaches in pedagogy can potentially help with the inference and training of LLMs.

## 2 Related Work of Our Learning by Teaching Implementations

As shown in Tab. 1, we study two types of objectives: _improving answer quality without training_ and _improving the inherent ability of the model with training_. SS 2.1 and SS 2.2 describe how **M1**, **M2**, and **M3** relate to prior work on these two objectives, respectively. See App. D.5 for more discussion.

### Improving the Answer Quality without Training

Existing literature has incorporated various insights from the human reasoning process to develop prompting-based methods, including writing down the thinking process [36; 81], subproblem decomposition [52; 56; 94], fetching the abstract principles and answering based on them , self-reflection-based answer refinement [47; 67], and so on. We explore two ways of incorporating the LbT insight to implement two prompting-based methods:

* **M1** relates to the popular "search-based output generation pipeline" shown in Fig. 1(a)[4; 42; 45; 47; 67; 79; 84; 87]. This pipeline iteratively samples and evaluates new rationales or rationale steps for searching the optimal output, and ultimately derives the final rationale or answer from the search history. One essential component in this pipeline is an _evaluator_ who evaluates the quality of each rationale or rationale step. We design an LbT evaluator that _scores each generated rationale based on its ability to teach student models to correctly answer similar problems_.
* **M3** relates to existing prompt optimization methods [57; 71; 95] that iteratively improve the prompts based on their performance (e.g., accuracy, failure cases). The key innovation in **M3** is how it evaluates the "performance": instead of evaluating with the same model that produced the prompts (i.e., the teacher model), we test how the prompt works with _other_ student models and show that this change benefits the prompt tuning outcome.

### Improving the Inherent Model Capability with Training

To improve the inherent model capability, **M2** incorporates the LbT insight into the "generating-scoring-finetuning pipeline". Fig. 1(b) illustrates the three steps in the pipeline: (1) Letting the target

Figure 2: Two general pipelines for improving the answer quality and model capability. “\(\)” stands for “Problem”; “\(\)” stands for “Rationale”; “\(\)” stands for “Answer”.

LLM or a teacher LLM generate multiple rationales for a given problem; (2) Scoring the rationales using an evaluator; (3) Utilizing the rationales and scores to (optionally) train a verifier [14; 45; 77], and finetune the target LLM by reinforcement learning , DPO [59; 88] or its variant [49; 55], filtering and supervised finetuning (SFT) [31; 89; 90], or score-conditioned SFT [43; 46].

In these works, the rationale scoring is usually achieved through manual labeling [45; 46; 99], ground-truth (GT) answer matching [89; 90], agreement-based scoring , or self-evaluation . In contrast, **M2**_scores the rationale based on its ability to teach student models to correctly answer similar problems_. In this way, **M2** can provide _automatic_ and _fine-grained_ quality evaluation for rationales, which helps automate and improve the continual evolution of models' capability.

## 3 Method (M1) for LbT Level 1: Observing Students' Feedback

### Method

One common teaching strategy in education is that the teacher first teaches students how to solve a class of problems by giving them the example rationale (named _Teaching Rationale_, or _TR_ in short) and the answer (named _Teaching Answer_, or _TA_ in short) to a particular question (named _Teaching Problem_, or _TP_ in short). Then, the teacher asks students to solve other similar problems (named _Exam Problem_, or _EP_ in short) to test if the students understand the concepts. The teacher can also learn from this process by observing the feedback (i.e., _LbT level 1_): if the students can answer EPs well, then it likely means that the TR-TA pair is of high-quality.

Our idea is to implement this strategy in LLMs to select high-quality TR-TA pairs. As depicted in Fig. 3 and Alg. A1, we first instruct the teacher model to solve a given TP multiple times, resulting in multiple TR-TA pairs. Then, each TR-TA pair is used as an in-context learning (ICL) example to guide the student model in solving a series of EPs. With the produced Exam Rationales (ERs) and Exam Answers (EAs), each student will then receive an exam score (i.e., the accuracy of EAs), denoted as the LbT score. The LbT score can be used as a quality assessment of the corresponding TR-TA pair. We consider two ways to select the final TA : (1) We select the TR-TA pair with the highest LbT score. We denote this approach as "**M1** (MAX)". (2) For TAs that can be aggregated via exact matching, such as mathematical reasoning, we can take the sum of the LbT scores for each TA separately and find the TA with the maximum sum. We denote this approach as "**M1** (SUM)".

The following subsections present the evaluation of **M1** on mathematical reasoning and code synthesis tasks. Please refer to App. D for the rationale behind the task selection.

### Evaluation on Mathematical Reasoning

#### 3.2.1 Experimental Setups

We use the extension MATH()  of the MATH dataset , where each problem has variants with different values. Following the train-test split specified by , among the 500 test problems, 181 problems are provided with 3 functional variants each. We use these 181 problems as TPs. For each TP, we sample 256 TR-TA pairs. Then, using each TR-TA pair as the ICL exemplar, we use the 3 functional variants of TP as EPs. Each exam is repeated 3 times with randomized student decoding, resulting in 9 ER-EA pairs. Each TA is scored based on the correctness of the 9 EAs.

#### 3.2.2 Results

We show the results in Tab. 2 and provide analyses as follows. More results are in App. A.

Figure 3: **M1**. The goal is to derive the best TA from the TR-TA pairs generated by the teacher LLM.

**M1 is effective with various model settings and surpasses baselines. M1** exceeds self-consistency (SC)  with various model settings: strong-teach-weak (e.g., GPT-4o teaches GPT-4o mini), weak-teach-strong (e.g., Mistral-7B teaches LLaMA3-8B), and self-teaching (e.g., LLaMA3-8B teaches itself). M1 (SUM) outperforms M1 (MAX) in most cases. We also show that LbT-based scoring surpasses self-evaluation scoring  in Tab. A7. Since M1 incurs higher inference cost than SC when using the same number of TR-TA pairs, we also conduct an experiment in Tab. A6, showing that with comparable or much lower compute, M1 with just 24 TR-TA pairs achieves a 0.17%\(\)8.29% accuracy improvement over SC with 256 TR-TA pairs.

**M1 can further benefit from multiple students**. Using GPT-3.5 to teach both LLaMA3-8B and Mistral-7B achieves a significant improvement than teaching LLaMA3-8B or Mistral-7B separately.

**M1 can identify infrequent but correct TAs. M1** can efficiently discover the correct answer from many teacher samples, whereas SC requires the correct answer to be in the majority to derive it. Fig. 4 (left, middle) shows the improvements of M1 over SC across different numbers of TR-TA pairs and difficulty levels. The relative improvement of M1 over SC increases as the number of TR-TA pairs or the difficulty levels grow within the experimental range.

**The TP and the corresponding EPs should be similar**. It is crucial to choose EPs similar to a TP such that the student can apply the logic from TR to solve EPs. We use the functional variants as EPs, which are very similar to TPs. To verify the necessity of TP-EPs similarity, we conduct an experiment that selects similar EPs from the original MATH training set. We calculate the embedding of each TP using the "all-mpnet-base-v2" sentence embedding model , and select the 2 closest problems from the training set as EPs. We sort TPs by the cosine distance to the corresponding EPs and calculate the relative improvements over SC on a fraction of TPs. Fig. 4 (right) shows that M1 only provides improvements for TPs that have similar problems in the training set.

### Evaluation on Competition-Level Code Synthesis

#### 3.3.1 Experimental Setups

We use the Grandmaster Dynamic Programming (DP) study plan on LeetCode.2 Each dataset in the study plan has 5\(\)10 problems, and each problem has 2\(\)3 visible test cases and many hidden

   Teacher & Student & Greedy & SC & **M1** (MAX) & **M1** (SUM) & Improv. \\  GPT-4o & GPT-4o mini & 87.84 & 91.71 & 95.03 & **96.69** & +4.98 \\ GPT-4o & LLaMA3-8B & 87.84 & 91.71 & 94.48 & 95.03 & +3.32 \\ GPT-4o & GPT-4o mini \& LLaMA3-8B & 87.84 & 91.71 & 96.13 & 95.58 & +3.87 \\ GPT-3.5 & LLaMA3-8B & 59.11 & 77.90 & **83.43** & **83.43** & +5.53 \\ GPT-3.5 & Mistral-7B & 59.11 & 77.90 & 81.22 & 83.43 & +5.53 \\ GPT-3.5 & LLaMA3-8B \& Mistral-7B & 59.11 & 77.90 & **84.53** & **84.53** & +6.63 \\ LLaMA3-70B & LLaMA3-8B & 70.16 & 81.77 & 86.74 & 87.85 & +6.08 \\ LLaMA3-70B & Mistral-7B & 70.16 & 81.77 & 86.19 & 85.08 & +3.31 \\ LLaMA3-70B & LLaMA3-8B \& Mistral-7B & 70.16 & 81.77 & **87.85** & 87.29 & +5.52 \\ LLaMA3-8B & LLaMA3-8B & 45.85 & 64.64 & 77.90 & 82.87 & +18.23 \\ Mistral-7B & LLaMA3-8B & 19.88 & 40.88 & 51.93 & **53.59** & +12.71 \\   

Table 2: Results on 181 MATH test problems with 256 TR-TA pairs. The best results of each row are highlighted in green. The “Improv” column calculates the improvements of average performance achieved by M1 (SUM) over SC.

Figure 4: Relative improvements of M1 over SC using LLaMA3-8B as the teacher and student on 181 MATH test problems with respect to: **(Left)** Number of TR-TA pairs. Error bars are calculated using the bootstrap sampling technique , where 10 subsets are sampled from the 256 TR-TA pairs, and standard deviations are computed across these sets; **(Middle)** Difficulty level; **(Right)** The fraction of TPs when sorted by the cosine distance to the 2 closest problems from the training set.

test cases. We assign a visible score (**V-score**) of 1 and 0 to the code that _passes all_ or _fails any_ visible cases . To evaluate the actual correctness of a code, we submit the code to LeetCode, and record the pass rate on the hidden cases as the submit score (**S-score**). For a TP, we sample 8 TR-TA pairs from the teacher, where TR is a rationale in natural language, and TA is a Python code (See Ex. 1 for an example). Each TR-TA pair is assigned an LbT score by teaching a student to solve the remaining problems in the dataset. **M1** calculates the exam **V-score** as the LbT score to avoid additional LeetCode submissions. Check App. A.3.2 for additional setups.

#### 3.3.2 Results

Here, we analyze the results on the Game Theory dataset. Check App. A.3 for additional results.

**M1 can be more general than agreement-based methods such as SC. M1** (MAX) does _not_ require an oracle to assess the equivalence of two answers, which is challenging for codes. Therefore, we only use the average pass rate (with or without V-score=1 filtering) as the baseline. Nevertheless, when such an oracle is provided , we can use M1 (SUM) which was shown to be better than M1 (MAX) in SS 3.2. We defer this exploration to future work.**

**M1 selects better TR-TA than the baseline in most cases. If the student closely follows the strategies in TR-TA to solve EPs, the student exam score can indicate the quality of TR-TA. (1) When the TR-TA has high quality (Ex. 1), the student mimics the teacher's strategy to solve the EP with a correct DP code. (2) When the TR-TA is logically incorrect, e.g., DP code with wrong recurrences (Ex. 2) or a non-DP wrong code (Ex. 3), the student also follows the wrong TR-TA with a wrong ER and EA. (3) When the TR-TA is logically correct but has high complexity, e.g. recursive re-computation instead of DP (Ex. 4), the student also writes a recursion with high complexity.**

As shown in Tab. 3, using the **V-score** on the few visible test cases can filter out some low-quality code, but M1 can identify better TA in most cases. This is because LbT-based scoring can leverage student scores on similar EPs, providing a more informative evaluation of TA. Note that M1 shows the largest improvements on TPs with medium difficulty. For very simple (e.g., SG-4 for GPT-3.5) or challenging (e.g., SG-2) problems, M1 shows marginal or no improvements.

**Self-Debugging (SD) is both complementary to and beneficial for M1**. We experiment with applying one-iteration SD  using Prompt 5. Applying SD on TAs can provide **S-score** benefits complementary to M1, since SD fixes simple non-logical bugs, such as missing imports, miswritten variable names, and incorrect usage of library functions (an example is shown in Ex. 6), whereas M1 mainly assess the quality of the logic. In addition, applying SD on EAs leads to more informative LbT score, as fixing non-logical bugs can make the students' exam **V-score** more indicative of quality of the TR-TA. Tab. 3 shows that after incorporating SD for both M1 and the baselines, M1 achieves consistent improvements.

    &  & SG-1 & SG-2 & SG-3 & SG-4 & PW \\   & Avg. & 0.215 & 0.004 & 0.216 & 0.604 & 0.609 \\  & **M1** (MAX) & 0.630 & 0.040 & 0.228 & 1 & 0.508 \\ S=LaMA3-8B & Avg. (V-score=1) & 1 & - & - & 0.755 & 0.851 \\  & **M1** (MAX) (V-score=1) & 1 & - & - & 1 & 1 \\   & Avg. & 0.348 & 0.004 & 0.319 & 0.608 & 0.694 \\  & **M1** (MAX) & 0.348 & 0.011 & **0.570** & **0.771** & **0.746** \\ S=LaMA3-8B & Avg. (V-score=1) & 0.797 & - & - & 0.722 & 0.851 \\ (w. Self-Debugging) & **M1** (MAX) (V-score=1) & 1 & - & - & 1 & 0.935 \\    & Avg. & 0.582 & 0.007 & 0.428 & 1 & 0.645 \\  & **M1** (MAX) & 1 & 0.011 & **0.681** & 1 & 1 \\ S=GPT-3.5 & Avg. (V-score=1) & 0.994 & - & 0.714 & 1 & 0.894 \\  & **M1** (MAX) (V-score=1) & 1 & - & 0.135 & 1 & 1 \\   & Avg. & 0.701 & 0.133 & 0.592 & 1 & 0.853 \\  & **M1** (MAX) & 1 & 0.337 & **0.714** & 1 & 0.968 \\ S=GPT-3.5 & Avg. (V-score=1) & 0.996 & 1 & 0.714 & 1 & 0.911 \\ (w. Self-Debugging) & **M1** (MAX) (V-score=1) & 1 & 1 & 0.714 & 1 & 0.968 \\    & Avg. & 0.875 & 0.008 & 0.679 & 1 & 0.601 \\  & **M1** (MAX) & 1 & 0.007 & 1 & 1 & 1 \\ S=LaMA3-8B & Avg. (V-score=1) & 1 & - & 1 & 1 & 0.883 \\   & **M1** (MAX) (V-score=1) & 1 & - & 1 & 1 & 1 \\   

Table 3: **S-score** results on _Game Theory_ dataset in LeetCode Grandmaster DP study plan. “SG-1”-“SG-4” and “PW” are abbreviations of individual questions in the dataset; see Tab. A8 for details. The results of M1 that improve (degrade) by more than 0.01 are highlighted in green (red).

For competition-level code synthesis task, M1 is more effective when the teacher and student come from the same family, as shown by Tabs. 3 and A10. We find that this is because the student can follow a teacher from the same model family better, making the feedback more informative. A failure case of student-following when GPT-3.5 teaches LLaMA3-8B is shown in Ex. 5.

TPs and EPs should be similar.Most failure cases in Tabs. 3 and A10 occur when solving the "PW" TP. We find that this is because the solving of "PW" involves 2D DP, which differs from other problems that can be solved with 1D DP. Consequently, the student cannot follow TA to solve EPs.

## 4 Method (M2) for LbT Level 2: Learning from the Feedback

### Method

In education, after identifying which teaching materials (e.g., TR-TA pairs) can enhance student performance (SS 3.1), teachers can use this information to improve their knowledge or teaching strategies. For example, if students perform poorly due to unclear or inaccurate teaching materials, teachers can correct their knowledge and avoid generating similar TR-TA pairs in the future.

We use this idea to train the LLMs to improve its reasoning ability. As depicted in Fig. 5, since the LbT-based scoring provides informative feedback on the quality of a TR-TA pair (verified in SS 3), we collect the LbT scores of many TR-TA pairs and use them to finetune the teacher with DPO .

### Experimental Setups

We use 1564 training problems from MATH()  as TPs. For each TP, we sample 32 TR-TA pairs from the teacher. For each TR-TA pair, we calculate \(0.5+0.5\) score as its final score, where the correctness score is 1 or 0 when the corresponding TA is correct or wrong, respectively. For running DPO, we select pairs from the 32 TR-TA pairs whose score difference exceeds a threshold of 0.3, and keep at most 8 pairs of TR-TA pairs for each TP.

### Results

Tab. 4 shows that **M2** achieves better results compared to solely using the correctness scores in DPO. This improvement is because LbT provides more informative scores than those purely based on correctness. One example is shown in Ex. 10. Although both TRs produce a correct TA, the losing TR is unnecessarily verbose and cannot be generalized to other similar problems. Another example is in Ex. 11. Although both TRs produce a wrong TA, the winning TR is logically better than the loser. LbT can discern the correct preference between these TR-TA pairs, thereby improving DPO results.

## 5 Method (M3) for LbT Level 3: Learning from the Feedback Iteratively

### Method

We have shown that the students' exam scores can serve as an indicator of the _reasoning quality_ of the teaching rationales. This indicator can be leveraged to aggregate better answers in **M1** and to further fine-tune the teacher in **M2**. In **M3**, we explore whether reflecting on students' detailed exam responses can help the teacher _iteratively_ refine its teaching materials. Notably, we aim to

   Teacher/Student & Original & Correctness-DPO & **M2** \\  LLaMA3-8B & 29.0 & 30.4 & 32.2 \\   

Table 4: Results on 500 MATH test problems with greedy decoding.

Figure 5: Baseline vs. **M2**. Both approaches use _scores_ of TRs to craft preference data and finetune the teacher LLM with DPO. **Left:** The correctness score of TA. **Right:** The LbT score of TR and TA.

verify whether these refinements can enhance the teacher's own performance by providing more effective _knowledge_. If so, we can assert that the iterative process of teaching, reflection, and material refinement facilitates some form of "knowledge building"  for the teacher. Additionally, we are interested in whether having _multiple_ and _diverse_ LLMs as students offers further benefits.

Specifically, we guide the teacher to iteratively improve teaching materials in the form of a set of positive and negative exemplars, based on the _student and teacher performance_ when the set is used as the ICL examples. As depicted in Fig. 6 and Alg. A2, given a classification task, we first sample \(K=8\) positive and negative exemplars from the teacher, and then run multiple refinement iterations. Finally, we report the _teacher performance on the test set_ when using the resulting ICL examples.

Each iteration contains the following steps: (1) _The current exemplars are used as the ICL examples to teach students to answer a set of EPs_. The EPs are randomly sampled from the training data in each iteration. (2) We select the EPs that students answered incorrectly and prompt the teacher to reflect on why the current exemplars might have misled students in these instances. (3) Based on the reflection, the teacher generates multiple updated exemplar sets. (4) We keep the exemplar set that _achieves the best teacher performance on the training data_ when the set is used as the ICL examples.

### Experimental Setups

We evaluate **M3** on two binary text classification tasks: Liar  and Logical Fallacy . Liar is a dataset for false statement detection in political media, with 4,574 statements with speaker and context information. Logical Fallacy is a dataset of 2,449 samples of 13 logical fallacy types, which we adapt to classify the most common type _faulty generalization_ against the rest of the types. We report the teacher \(F_{1}\) score on the dev and test splits combined. Within an iteration, we choose the exemplar set with the highest teacher \(F_{1}\) score on the training set. Across 14 random experiments, we report the mean \(F_{1}\) and the standard deviation. We run a total of five refinement iterations.

### Results

As Tabs. 5 and A17 shows, it is feasible to apply LbT on iterative prompt optimization: LLMs are able to reflect on the failure cases of students and propose revised exemplars that improve the teacher's performance, similar to the case of iteratively optimizing task descriptions as in previous work .

More importantly, we observe a performance gain brought by having _dedicated_ students (as opposed to using a single LLM in prompt optimization as in previous work). Comparing to the scenario where the teacher and student are the same, having one or multiple LLMs different to the teacher as the student improves the quality of the teaching material faster. This demonstrates LbT as a case of _weak-to-strong_ generalization. We speculate that the benefits are brought by more diverse error types made by a different (weaker) student model; see App. C.3 for more examples and analyses.

## 6 Broader Discussion

### Insights into In-Context Learning

Currently, we conduct student "learning" with ICL, based on the assumption that students can effectively "learn" from ICL examples and apply similar strategies to solve EPs. Interestingly, prior

  Student(s) & \(T=1\) & \(T=2\) & \(T=3\) & \(T=4\) & \(T=5\) \\  LLaMa3-70B & 61.08\(\)1.29 & 62.01\(\)1.12 & 64.48\(\)1.20 & 65.40\(\)0.67 & 63.96\(\)1.19 \\ LLaMa3-8B & 62.24\(\)1.30 & **66.15\(\)0.56** & **65.66\(\)0.72** & 64.78\(\)0.89 & 65.41\(\)0.75 \\ LLaMa3-B + Mistral-7B & **63.66\(\)1.48** & 64.47\(\)0.90 & 65.47\(\)1.01 & **66.24\(\)0.56** & **67.09\(\)0.56** \\  

Table 5: Teacher’s \(F_{1}\) score of **M3** on combined Liar dev and test set at the end of iteration \(T\), where LLaMa3-70B is used as the teacher for all settings. The best results are in **bold**.

Figure 6: Overview of **M3**. The teacher teaches the students through a set of positive and negative ICL examples. These examples are iteratively refined by the teacher according to students’ feedback.

work  found that a correct input-output pairing in ICL examples does not matter much. At first glance, this finding seems to challenge our design, as it suggests that the TA accuracy may not affect the EA accuracy, which means the LbT score cannot reflect the quality of TR+TA. However, we find that, as opposed to only providing _labels_ in the ICL examples , providing _rationalizes_ is important. _LLMs can follow the problem-solving logic in the **detailed rationale** in the ICL examples well_. This may be because the rationale provides more information, making the ICL examples easier to follow. Consequently, we see that students can use similar logic as the TR when solving the EP. This means that better TR+TA can indeed lead to improved ER and thus higher EA accuracy (i.e., LbT score).

Our findings highlight two key factors for successful ICL following and for establishing a positive correlation between EA accuracy and TR quality/TA accuracy: (1) similarity between TP and EP, and (2) the use of Chain-of-Thought (i.e., detailed rationale). See App.A.3.5 for the effect of natural language rationale on ICL following in code synthesis. Note that we tried explicitly instructing the student to follow the ICL example, but it did not work well. We hope these findings can complement the current understanding of ICL  and offer insights for improving ICL.

### Weak-to-Strong Generalization

Improving models with human-generated/annotated data or synthetic data from stronger models is the dominant paradigm. However, how can we continuously improve the strongest model without relying on human-generated and annotated data? A recent work  conducts an exploration on using weak model supervision to train a larger model. Our work is another attempt towards the "weak-to-strong generalization" prospect by drawing from how humans continuously acquire new knowledge without direct instruction. We demonstrate that stronger models can further improve their own results (**M1**), parameters (**M2**), and prompt (**M3**) by utilizing the feedback of weaker models.

### Limitations and Near-Term Extensions

**M1 and M2 rely on generating/selecting similar EPs.** We verify that LbT-based scoring can help select high-quality TR-TAs but require _the TP and EPs having similar problem-solving strategies_. In our experiments, suitable EPs are selected according to human-provided information in the dataset. One extension is to let a model automatically identify EPs similar to a TP from a large pool (Fig. 4). Another direction is to synthesize similar problems based on a group of problems and exploit the LbT principle to score many rationales for the new problems. Specifically, as a "self-instruct"  extension to **M2**, we can generate a new problem \(P\) based on a group of problems \(S=\{P_{1},,P_{k}\}\) that are already known to be similar. The generating-scoring pipeline can then be applied to \(P\) to obtain rationale-score pairs, where the LbT score can be easily obtained using \(S\) as the EPs.

**Additional inference cost.**_LbT-based scoring in **M1** and **M2** requires additional inference cost_, which aligns with recent studies that show that increasing inference cost might be a promising way to improve models' reasoning capabilities . Nevertheless, designing efficient inference algorithms and systems  is needed to make these approaches more usable.

See App.E for other extensions and App.F for the discussion on potential risks of bias perpetuation.

### Borrowing Education Strategies to Improve LLMs

**Borrowing the design strategies of teaching materials.** We show an LbT pipeline in Fig. 7. Each iteration involves six steps: (1) The teacher generates the Teaching Material (TM). (2) The student learns from the TM. Our work uses in-context learning for all student learning, but exploring other learning strategies is an interesting future direction. (3) The student provides feedback. The feedback can take many forms as listed in the figure. Our work mainly explored feedback in the form of exam details and scores. (4) The teacher reflects on the feedback and identifies the knowledge gaps in the TM or in the teacher's own knowledge. (5) The teacher can optionally refer to some external data source to address its own knowledge gaps. (6) The teacher improves the rigorousness, clarity, and completeness of their knowledge and updates the TM for the next iteration.

On one hand, updating TM can improve the teacher's own knowledge. For example, **M3** saves the updated exemplars as the teacher's prompt to improve the teacher's reasoning. On the other hand, a high-quality TM helps students learn better, so that the students can provide more meaningful feedback for the teacher. To create high-quality TM, it might be beneficial to borrow from TM design strategies in human education. Fig. 7 summarizes various TM design strategies, among which our work has explicitly explored three types (marked in black).

**Borrowing the pipelines.** We can borrow insights from education pipelines to design LLM inference and training pipelines. For example: (1) **Task-oriented collaborative multi-agent learning**:Multiple LLM agents can form a collaborative study group to learn difficult topics in a task-oriented manner (see some discussions in App. D.2). Similar multi-agent collaboration ideas have been leveraged by LLM agent research [12; 17; 29; 39; 58; 91]. (2) **Better LbT by configuring proper teacher&student**: Literature on "teachable agents" finds that configuring a student's knowledge level appropriately can lead to more useful feedback for the teacher [5; 65]. This suggests the intriguing possibility of prompting the student LLM to "confine" its knowledge level , thereby amplifying the benefits of LbT for the teacher model. Furthermore, a junior model \(\) can first teach a "student" that is stronger than itself, who can understand and critique mistakes and ambiguity. As \(\) becomes stronger, it might become better at evolving its knowledge by teaching weaker students. This can be seen as a form of easy-to-hard task progression in curriculum learning . (3) **Flexible teaching quality evaluation**: In human learning, feedback can take many forms [25; 82] beyond traditional exams [62; 63; 64], such as peer recommendations  and satisfaction questionnaires [37; 61]. Such mechanisms can be adapted to LLMs, potentially useful for open-ended tasks.

## 7 Conclusion

Aiming to improve LLM reasoning, we conduct a preliminary exploration of whether LLMs can "learn by teaching"-- a well-known paradigm in human learning. We implement the LbT idea into well-established pipelines to develop three methods, and evaluate whether they improve reasoning performance on complex tasks such as mathematical reasoning and competition-level code synthesis:

* **M1** is based on the LbT-TMQ assumption. Specifically, we adopt LCL as the instructional method and measure the students' success in grasping the logic embedded in ICL examples by evaluating their performance on similar EPs. **M1** is implemented as a standard "search-based output generation" pipeline with an LbT-based rationale scoring component. **Results**: In mathematical reasoning, **M1** achieves a 3.31%\(\)18.23% accuracy improvement over the competitive SC baseline on 181 MATH test problems with 256 TR-TA pairs (Tab. 2). Note that **M1** achieves a 3.32%\(\)4.98% improvement on the powerful GPT-4o, reaching a high accuracy of 96.69%. Using comparable or much lower compute, **M1** with 24 TR-TA pairs achieves a 0.17%\(\)8.29% accuracy improvement over SC with 256 TR-TA pairs (Tab. A6). In code synthesis, **M1** achieves notable improvements in submission score in most scenarios, particularly when the teacher and student belong to the same model family (Tabs. 3, A11 and A13).
* **M2** uses the LbT scores from **M1** to fine-tune the teacher with DPO. **M2** is implemented as a standard "generating-scoring-finetuning" pipeline with an LbT-based rationale scoring component. **Results**: In the experiment of fine-tuning LLaMA3-8B, the **M2**-tuned model achieves a 1.8% accuracy improvement over the model tuned with correctness-based DPO, evaluated on 500 MATH test problems with standard greedy decoding (Tab. 4).
* **M3** lets the LLM iteratively refine ICL examples by analyzing the students' feedback. **Results**: For two binary text classification tasks requiring common-sense and logic reasoning, **M3** can craft better ICL examples through multiple refinement rounds, and the feedback from students other than the teacher itself is beneficial (Tabs. 5 and A17).

In summary, the LbT idea is implemented as a scoring method in **M1** and **M2**, and as an iterative refining pipeline in **M3** (Tab. 1). Our results suggest LbT's potential for harnessing the diversity offered by different students and facilitating weak-to-strong generalization in improving reasoning.

We believe our work only scratches the surface of leveraging educational principles to improve LLMs. Will these approaches find greater use as LLMs grow more intelligent? We discuss our research rationale and roadmap for exploring this intriguing question in App. D and SS 6.

Figure 7: LbT pipeline and the summary of knowledge types, TM design strategies, and feedback.