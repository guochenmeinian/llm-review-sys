# WeiPer: OOD Detection using Weight Perturbations of Class Projections

Maximilian Granz

Institute for Computer Science

Free University of Berlin

Arnimallee 7 14195 Berlin

maximilian.granz@fu-berlin.de

&Manuel Heurich

Institute for Computer Science

Free University of Berlin

Arnimallee 7 14195 Berlin

manuel.heurich@fu-berlin.de

Equal contribution.

Tim Landgraf

Institute for Computer Science

Free University of Berlin

Arnimallee 7 14195 Berlin

tim.landgraf@fu-berlin.de

Equal contribution.

###### Abstract

Recent advances in out-of-distribution (OOD) detection on image data show that pre-trained neural network classifiers can separate in-distribution (ID) from OOD data well, leveraging the class-discriminative ability of the model itself. Methods have been proposed that either use logit information directly or that process the model's penultimate layer activations. With "WeiPer", we introduce perturbations of the class projections in the final fully connected layer which creates a richer representation of the input. We show that this simple trick can improve the OOD detection performance of a variety of methods and additionally propose a distance-based method that leverages the properties of the augmented WeiPer space. We achieve state-of-the-art OOD detection results across multiple benchmarks of the OpenOOD framework, especially pronounced in difficult settings in which OOD samples are positioned close to the training set distribution. We support our findings with theoretical motivations and empirical observations, and run extensive ablations to provide insights into why WeiPer works. Our code is available at: [https://github.com/mgranz/weiper](https://github.com/mgranz/weiper).

## 1 Introduction

Out-of-Distribution (OOD) detection has emerged as a pivotal area of machine learning research. It addresses the challenge of recognizing input data that deviates significantly from the distribution seen during training. This capability is critical because machine learning models, particularly deep neural networks, are known to make overconfident and incorrect predictions on such unseen data Hendrycks and Gimpel (2016). The need for OOD detection is driven by practical considerations. In real-world applications, a model frequently encounters data that is not represented in its training set. For instance, in autonomous driving, a system trained in one geographic location might face drastically different road conditions in another. Without robust OOD detection, these models risk making unsafe decisions Amodei et al. (2016).

Over the last few years, the field has made significant steps towards setting up benchmarks and open baseline implementations. Thanks to the efforts of the OpenOOD team Zhang et al. (2023b); Yanget al. (2022), we can evaluate new methods across CIFAR10, CIFAR100 and ImageNet, and compare them against a variety of methods, on the same network checkpoints. To this date, however, there is no single method outperforming the competition on all datasets Tajwar et al. (2021), which indicates a variety of ways in which OOD data differs from the training set. Here, we introduce _WeiPer_, a method that can be applied to any pretrained model, any training loss used, with no limitation on the data modality to separate ID and OOD datapoints. _WeiPer_ creates a representation of the data by projecting the latent representation of the penultimate layer onto a cone of vectors around the class-projections of the final layer's weight matrix. This allows extracting additional structural information on the training distribution compared to using the class projections alone and specifically exploits the fact that the OOD data often extends into the cluster of positive samples of the respective class in a conical shape (see Figure 1). In addition to _WeiPer_, our KL-divergence-based method _WeiPer_+KLD represents a novel OOD detection score that is based on the following observation:

When ignoring the individual dimensions and examining the activation distribution across all dimensions, we observe that ID samples exhibit a similar "fingerprint" distribution. The more feature dimensions there are, the better our estimate of this source distribution becomes. We demonstrate that measuring the discrepancy between the per-sample distribution and the training set's mean distribution in the augmented _WeiPer_ space leads to improved OOD detection accuracy. We evaluate _WeiPer_ on OpenOOD using our proposed KL-divergence-based scoring function (**KLD**), MSP Hendrycks and Gimpel (2016), and ReAct Sun et al. (2021). Additionally, we conduct an ablation study to understand the influence of each component of _WeiPer_ and analyze _WeiPer_'s performance. Our results confirm that the weight perturbations allow _WeiPer_ to outperform the competition on two out of eight benchmarks, demonstrating consistently better performance on near OOD tasks. _WeiPer_ represents a versatile, off-the-shelf method for state-of-the-art post-hoc OOD detection. However, the performance of _WeiPer_ comes at a cost: The larger the _WeiPer_ space, the more memory is required.

Figure 1: Why random perturbations? **Left**: We visualize densities of CIFAR10 (ID, blue) and CIFAR100 (OOD, red) as contour plots along the two logit dimensions spanned by \(_{0}\) and \(_{1}\), zoomed in on the positive cluster of class zero. The blue axis denotes the vector associated with that class, and one of its perturbations is depicted by the turquoise line. **Right**: When projecting the data onto both vectors, we obtain the densities shown in the _top_ and _bottom_ panel, respectively. The vertical blue lines mark the 5-percentile (highest 5%) of the true ID data (CIFAR10, blue). At this decision boundary, the classifier would produce false positives in the marked dashed red tail area. A single perturbation of the class-associated vector yields already a reduction of the false positive rate (FPR) from \(1.34\)% to \(0.79\)%. Visually, we confirm that OOD data mostly resides close to 0, extending into the positive cluster in a particular conical shape, which is exploited by the cone of _WeiPer_ vectors.

In summary, we present the following **contributions**:

* We discover that OOD detection can be improved by considering linear projections of the penultimate layer that correlate with the final output, i.e., the class representations. We construct these projections by perturbing the weights of the final layer.
* We uncover a fingerprint-like nature of the ID samples in both the penultimate space and our newly found perturbed space, proposing a novel post-hoc detection method that leverages this structure. The activation distributions of the penultimate space and our _WeiPer_ space over the dimensions of each sample are similar for each ID input, yielding distributions in both spaces that we compare to the mean ID distribution using KL divergence.
* We evaluate our findings by testing the proposed methods and two other MSP-based methods on the perturbed class projections using the OpenOOD benchmark, achieving state-of-the-art performance on near OOD tasks.

## 2 Related work

OOD detection.Generally, we can distinguish two types of OOD detection methods - one that requires retraining of the model, including novel loss variants, data augmentations, or even outlier exposure settings. Here, we focus on post-hoc methods that can be added with little effort to any existing pipeline. They can be applied to any pretrained model, irrespective of its architecture, loss objective or data modality. Post-hoc methods can be distinguished further in:

1) **Confidence-based** methods Guo et al. (2017); Hendrycks et al. (2022, 2022); Liu et al. (2023, 2020); Wang et al. (2022) process samples in the model's logit space, i.e. using the network directly to detect ID/OOD data points. A prominent example is the Maximum Softmax Probability (MSP) (Hendrycks and Gimpel, 2016) which simply uses the maximum logit as the main OOD decision metric. Some methods Ahn et al. (2023); Djurisic et al. (2022); Sun et al. (2021) additionally introduce transformations such as cutoffs of the features in the penultimate layer or masks on the weight matrix Sun and Li (2021) to allocate where ID data resides and combine these with confidence metrics. Several recent methods have employed f-divergences to improve OOD detection, focusing on enhancing the boundary definition between ID and OOD samples Darrin et al. (2022); Picot et al. (2022).

2) **Distance-based** methods Bendale and Boult (2015); Lee et al. (2018); Liu et al. (2023, 2021); Ren et al. (2021); Sastry and Oore (2020); Sun et al. (2022); Zhang et al. (2023) define distance measures between the training distribution and an input sample in latent space, i.e. primarily the penultimate layer of the network. Deep Nearest Neighbors Sun et al. (2022) uses the distance to the \(k\)-th closest neighbor in latent space, while MDS Lee et al. (2018) models the data as Gaussian and uses the Mahalonibis-Distance. Models of the data distribution can improve the OOD detection performance, e.g. using histograms to approximate the training density and then define a distance measure on them. A recent work Liu et al. (2023) proposed creating a histogram-based distribution on the product of the penultimate activations and the gradient of a separate KL-loss and then defined a metric on these modified discrete densities.

Both approaches of 1) and 2) are not exclusive. NNGuide Park et al. (2023) combines both confidence and distance measures into a joint score, improving performance in case one of the scores fails.

Random weight perturbations and projections.Weight perturbations, i.e. adding noise values to the weights of a network, have been used for a variety of applications: in sensitivity analyses Cheney et al. (2017); Xiang et al. (2019), for studying robustness against adversarial attacks Rakin et al. (2018); Wu et al. (2020), and as training regularization Khan et al. (2018); Wen et al. (2018). Random projections from the latent space of the neural network have been described in the context of generative modeling Bonneel et al. (2014); Jerome H. Friedman and Schroeder (1984); Kolouri et al. (2016); Liutkus et al. (2019); Nguyen et al. (2021); Paty and Cuturi (2019), e.g. to improve the Wasserstein distance calculation or for robustness. A previous work described random projections from the penultimate layer to detect out-of-distribution samples with a normalizing flow Kuan and Mueller (2022).

## 3 Method

### Preliminaries

We consider a pretrained neural network classifier \(f:^{C}\) that maps samples \(x\) from an input space \(^{D}\) to a logit vector \(f()^{C}\), by applying a linear projection \(_{}\) to the feature representation in the penultimate layer

\[(z_{1},...,z_{K})^{T}==h()=(h_{1}(),...,h_{K}() )^{T} \]

such that \(f()=_{}^{T}\), with \(D\), \(K\) and \(C\) representing the dimensionality of the input, the penultimate layer and the output layer, respectively. We define the rows of the final weight layer to be \(_{1},...,_{C}\).

In the following it is useful to introduce \(Z\) as random vector from which we draw our latent samples. We denote the densities of the latent activations of the training data with \(p_{Z_{}}\), of the test data with \(p_{Z_{}}\) and those of OOD samples with \(p_{Z_{}}\) admitted by the random vectors \(Z_{}\), \(Z_{}\) and \(Z_{}\), respectively. To ease notation, we will treat \(Z\) and all its subsets, both as sets, e.g. \(Z_{}\) is the set of all training activations in the penultimate layer.

An OOD detector is a binary classifier \(O\) that decides if samples are drawn from an ID or OOD distribution by usually only considering samples drawn from \(p_{Z_{}}\) or \(p_{Z_{}}\). Commonly, this is achieved by thresholding a scalar score function \(S\).

\[O()=&S()>\\ & \]

For MSP, the score function is simply the maximum softmax probability

\[S()=()=_{i=1,...,C})_{i}}}{ _{j=1}^{C}e^{f()_{j}}}=:(f()). \]

Note, that for clarity, we define \(\) also as a function of the logits. Other methods propose metrics on the penultimate layer, e.g. by incorporating distance measures between a given latent activation \(\) of a new sample and the distribution of activations \(p_{Z_{}}\) of the training set.

### WeiPer: Weight perturbations

A neural network classifier maps the data distribution to the distribution of the logits \(_{}Z\). The training objective of the network ensures an optimal separation of classes and lets the model learn to exploit features in \(Z\) specific to the training distribution. OOD samples, hence, often yield lower logit scores. Confidence methods leverage this property, but could potentially be improved by capturing more of the underlying distribution of the penultimate layer. A confidence score measures properties of the logit distribution \(_{}Z\). Is there additional information in the penultimate layer of the network, and if so, how can we utilize it?

Applying the weight matrix \(_{}\) to the penultimate space can be understood as \(C\) projections of \(Z\) onto the row vectors \(\). According the Cramer-Wold theorem (Cramer & Wold (1936)), we can reconstruct the source density \(p_{Z}\) from all one-dimensional linear projections, and Cuesta-Albertos et al. (2007) has shown that a K-dimensional subset of projections suffices (for more details see Appendix A.1.1). The question remains which projections extract the most relevant information?

Drawing vectors \( W=(0,I)\) from a standard normal and projecting onto them often results in similar densities for ID and OOD data, i.e. \(^{T}Z_{}^{T}Z_{}\), deteriorating detection performance (see Table 1, RP). This aligns with Papyan et al. (2020), suggesting limited information in the penultimate layer compared to the logits. We hypothesize that the latent distribution shows relevant structure only along certain dimensions. We applied PCA to the latent activations \(Z\) and inspected the resulting projections. This analysis supports the notion that the informative dimensions lie in the directions of the class projections \(_{1},...,_{C}\) (see Appendix A.1.3). Hence, we construct projections that correlate with these vectors but at the same time deviate enough to obtain new information.

Definition of WeiPerWe define perturbations \(\), drawn from a standard normal and add them to \(_{}\). To ensure that all perturbed vectors have the same angular deviation from the original weight vector, we normalize the perturbations to be the same length as their corresponding row vector and multiply them by a factor \(\):

\[}_{i,j}=_{j}+_{i}\|_{j}\|}{\|_{i}\|}=:_{j}+}_{i},_{i}(,_{K}) \]

for \(i=1,...,r\), where \(\) represents the length ratio between \(_{j}\) and the perturbation \(_{i}\). For large \(K=(Z)\), \(_{j}\) and \(}_{i}\) are almost orthogonal and thus \(\) actually adjusts the angle \(()\) of the perturbed vector bundle. We set \(\) to be constant across all \(j=1,...,K\) and treat both \(\) and \(r\) as hyperparameters. This proceedure is related to the Distributional Sliced Wasserstein distance Nguyen et al. (2021) as they sample projections from a distribution such that the mean angle between the projections is greater than \((C)\) for a constant \(C\).The whole set of vectors we define is

\[W=\{}_{1,1},...,}_{1,C},...,}_{r,C}\} \]

We can think of the resulting weight matrix \(}\) as \(r\) repetitions of the weight matrix \(_{}\) on which we add perturbation matrices \(}_{i}\). The \(j\)-th row \(}_{i,j}\) corresponds to a perturbation vector \(}_{j}\), normalized to match the respective row \(_{j}\).

\[}:=}_{1}\\ \\ }_{r}=_{}+}_{1}\\ \\ _{}+}_{r}, \]

Since \(}_{i}Z=_{}Z+_{i}Z\), we call \(}Z\) the perturbed logit space. Our weight perturbations method, we call WeiPer, essentially increases the output dimension of a model. Hence, it can be combined with many scoring functions. We demonstrate this with the two following postprocessors.

### Baseline MSP scoring function

If the perturbations do not deviate too much from the class projections \(_{j}\), i.e. the row vectors of the final layer, the class cluster will still be separated from the other classes in the new projections and we can apply \(\) on the perturbed logit space. In fact, we find that class clusters on the perturbed projections can be better distinguished from the OOD cluster than on the original class projection

Figure 2: _WeiPer_ perturbs the weight vectors of \(_{}\) by an angle controlled by \(\). For each weight, we construct \(r\) perturbations resulting in \(r\) weight matrices \(}_{1},...,}_{r}\). **KLD**: For _WeiPer_+KLD, we treat \(z_{1},...,z_{k} p_{}\) and \(w_{1,1}^{T},...,w_{r,C}^{T} p_{}\) as samples of the same distribution induced by \(z\) and \(z\), respectively. We approximate the densities with histograms and smooth the result with uniform kernel \(T_{k_{*}}\). Afterwards, we compare the densities \(T_{k_{*}}(q_{})\) with the mean distribution over the training samples \(_{ Z_{}}(q_{})\) for \(q_{}=p_{}\) and \(q_{}=p_{}\), respectively. **MSP:** For a score function \(S\) on the logit space \(^{C}\), we define the perturbed score \(S_{}\) as the mean over all the perturbed logit spaces \(}\). Choose \(S=\) and call the resulting detector \(_{}\).

defined by \(_{ fc}\) (see improvements of _WeiPer_+MSP(\(\)) over MSP in Table 2). Figure 1 illustrates a visual example. We calculate the MSP on the perturbed logit space as

\[():=_{}():=_{i=1}^{r}(}_{i}) \]

the mean over all the maximum softmax predictions of the perturbed logits. We analyze why \(_{}\) could be capable of capturing more of the penultimate layer distribution than MSP in Appendix A.1.2.

### Our KL divergence score function

Following our line of argument motivated by Theorem A.1, it seems natural to choose a density-based score function. When pooling all activations of the penultimate layer, an ID sample's activation distribution exhibits remarkable differences to that of an OOD sample. We observe the following properties:

* The majority of samples exhibit a bimodal distribution of their penultimate activations. An activation either belongs to the mode close to zero, or to the second mode (and rarely takes values in between).
* see the upper left panel in Figure 3.
* The activation distribution is specific to the ID samples, i.e. the activation distribution of OOD samples differs from its distribution of ID samples and thus from the ID prototype.

Concluding on all three points, we make the _assumption_ that all features \(=h()\) of an ID input \(x\) can be thought of as samples

\[z_{1},...,z_{K} p_{}(z_{1},...,z_{K})^{T}=, \]

of the same underlying activation distribution \(p_{}\). Furthermore, the density of \(p_{}\) matches the mean distribution over all ID samples

\[p_{}_{^{} Z_{}}[p_{^{ }}]. \]

We assume, the same is true for the logits. They naturally separate into a non-class cluster and a class cluster with the ratio \(1:C-1\). Here, we could apply the same procedure, but especially for datasets with a small number of classes we would only get \(C\) samples. This is where the cone of _WeiPer_ vectors creates an advantage: They sit at a fixed angle to a class projection and thus preserve the class structure similarly across each projection onto one vector of the cone (e.g., like Figure 1 right - bottom panel). Analogous to Equation (9), we treat each projection

\[_{1,1}^{T},...,_{r,C}^{T} p_{} \]

as a sample of the same underlying distribution and observe that

\[p_{}_{^{} Z_{}}[p_{^ {}}]. \]

Figure 3: Histogram of all 512 activations in the penultimate layer (left pair) and the activations in _WeiPer_ space (right pair) of a ResNet18 trained on CIFAR10. We perturb the weight matrix \(100\) times to produce a \(10 100=1000\)-dimensional perturbed logit space. For each pair, the left panel shows the mean distribution over all samples (ID = CIFAR10, OOD = CIFAR100). The right panels show the distribution \(p_{}\) and \(p_{}\), respectively, for two randomly chosen samples with smoothing applied (\(s_{1}=s_{2}=2\))

We demonstrate both behaviors in Figure 3.

In practice, we discretize \(p_{}\) and \(p_{_{}}\) as histogram-based densities by splitting the value range into \(n_{}\) bins (see Equation (21) in the Appendix). Compared to the mean distribution, \(p_{}\) and \(p_{}_{}}\) still have a sparse signal. We smoothen the densities with a function

\[T_{k_{s}}(p(t)):=((p*k_{s})(t)+) \]

by convolving \(p\) with a uniform kernel \(k_{s}\) of size \(s\) and prevent densities from being zero by adding \(>0\) which we set to the fixed values \(:=0.01\) for the penultimate layer and \(:=0.025\) for the _WeiPer_ space. Note, that tuning both epsilons might increase performance as we observed in early stages of our experiments, but will add two additional hyperparameters. We normalize the density to sum up to one again, here defined by normalize. Afterwards, we compare each of the densities with the KL divergence, respectively:

\[D_{}( q_{},k_{s},):= T_{k_{s}}(q_{})}_{ _{}}[q_{}]+ }_{_{}}[q_{}]  T_{k_{s}}(q_{}), \]

where \(q_{z}\) is either \(p_{}\) or \(p_{}_{}^{}}\). We discuss why our method does not suffer from the curse-of-dimensionality in contrast to other methods as investigated by Ghosal et al. (2023) in Appendix A.1.5

_WeiPer_+KLD combines the KL divergence on the penultimate space, the KL divergence and MSP on the perturbed logit space into one final score:

\[}():=D_{}(x p_{},s_{1})+ _{1}D_{}(x p_{}^{}},s_{2})-_{2} _{}() \]

The full list of hyperparameters is \(r\) and \(\) for the _WeiPer_ application and \(n_{},_{1},_{2},s_{1},s_{2}\) for the KL divergence score function. Figure 2 provides a visual explanation and a quick overview of _WeiPer_ and both its postprocessors.

## 4 Experiments

**Setup.** We evaluate _WeiPer_ using the OpenOOD Zhang et al. (2023b) framework that includes three vision benchmarks: _CIFAR10_Krizhevsky (2009), _CIFAR100_Krizhevsky (2009), and _ImageNet_Deng et al. (2009). Each of them contains a respective ID dataset \(_{}\) and several OOD datasets, subdivided into _near_ datasets \(_{}\) and _far_ datasets \(_{}\) (see Table 1). The terms near and far indicate their similarity to \(_{}\) and, therefore, the difficulty of separating their samples.

OpenOOD also provides three model checkpoints trained on each CIFAR dataset whereas for ImageNet the methods are evaluated on a single official _torchvision_Marcel & Rodriguez (2010) checkpoint of ResNet50 He et al. (2016) and ViT-B/16 Dosovitskiy et al. (2020) respectively. We report our scores together with the results of Zhang et al. (2023b) in Table 2.

Due to resource constraints, we only evaluate our methods on the models trained with the standard preprocessor, that includes random cropping, horizontal flipping and normalizing, on the cross entropy objective. Additionally to the KL divergence score function and MSP, we evaluate _WeiPer_ on ReAct. But instead of combining ReAct with the energy-based score function Liu et al. (2020) as in OpenOOD, we apply \(_{}\) and call it _WeiPer_+ReAct. The hyperparameters of our methods were tuned by finding the best combination over a predefined and discrete range of values on the OpenOOD validation sets to assure a fair comparison to the competition (see Table 8). For ImageNet, results are based on a subset of the training data, comprising 300,000 randomly selected, balanced samples (300 per class). For an analysis across different training set sizes, refer to Table 6.

Metrics.We evaluate the methods with the Area Under the Receiver Operating characteristic Curve, AUROC, Bradley (1997) metric as a threshold-independent score and the FPR95 as a quality metric. The FPR95 score reports the False Positive Rate at the True Positive Rate threshold 95%.

   \(_{}\) & **CIFAR10** & **CIFAR100** & **ImageNet-1k** \\  \(_{}^{}\) & CIFAR100, & CIFAR10, & ssb-hard, \\ TinyImageNet & TinyImageNet & ninco \\  _{}^{}\)} & MNIST, & MNIST, & iNaturalist, \\  & SVHN, & SVHN, & Texture, \\   & Texture, & Texture, & OpenImage-O \\   & Places365 & Places365 & \\   

Table 1: The individual benchmark datasets.

Results.Table 2 reports the performance of _WeiPer_ in comparison to the state-of-the-art OOD detectors on each benchmark. We compare our approach based on the \(_{}\) and \(_{}\) detection performances and report the mean over all datasets in each category. Table 3 portrays the mean relative performance on \(_{}\) and \(_{}\) of every postprocessor. The score is calculated as follows:

\[S_{}(P):=A_{}(P)+A_{}(P )+A_{}(P)+A_{}(P)  \]

where

\[A_{}(P):=_{_{/}} (P)}{_{P}_{_{/} }(P)} \]

It is designed such that each result on each dataset \(\) is equally weighted and scoring \(1.0\) means that the postprocessor \(P\) is top performing across all datasets.

_WeiPer_+KLD achieves three out of eight top AUROC scores and the best performance on all near benchmarks, establishing a new state of the art performance by a significant margin (see Table 3).

Especially for the most challenging benchmark, separating \(_{}\) on ImageNet with a ResNet50, we outperform our strongest competitor, ASH Djurisic et al. (2022), by 1.88% AUROC (we even achieve an AUROC score of 80.29 when using a 1M training samples instead of 300k, see Table 6). Additionally, _WeiPer_+KLD performs well on many far benchmarks, being the best method for ResNet50 on ImageNet, reaching into the top three positions on CIFAR10 far and into the top three on the CIFAR100 far benchmark. With its relative performance in Table 3, _WeiPer_+KLD reaches 3rd place overall in the far benchmark.

Only on Vit-B/16 trained on ImageNet, _WeiPer_+KLD shows a significant performance dent, especially on the far benchmark. ViT-B/16 uses a comparably narrow penultimate layer having fewer features

than classes and therefore compresses the class clusters. Some dimensions may thus compress two classes while others represent a feature specific to only one class. This introduces more noise into \(p_{}\) which could impair the detection performance. Future experiments will reveal whether _WeiPer_ benefits from higher dimensionalities of the latent space.

WeiPer on existing methods.Additionally, _WeiPer_ enhances the MSP performance by 1-4.1% AUROC across all benchmarks and _WeiPer_+ReAct consistently outperforms ReAct with an energy-based score, although in their evaluation, this variant was better than ReAct+MSP (see Table 3).

Ablation study.We determine the effect of each hyperparameter in Figure 4 by freezing single hyperparameters and optimizing only the one in question. As expected, increasing the number of random perturbations \(r\) leads to a better median performance, while the standard deviation decreases for larger \(r\). Note, that it is possible to have better performance for lower \(r\) by recrolling the weights a few times and choosing the best performing ones. All methods show a significant performance boost compared to using no perturbations \(=0\) and seem to be best at \(=2\) for _WeiPer_+KLD, which corresponds to an angle of \( 63^{}\) and \(=4\) (\( 76^{}\)) for MSP and ReAct.

On CIFAR10, _WeiPer_+KLD only improves marginally by applying \(_{}\), which is not the case for the other benchmarks (see Table 7), where \(_{2}>0\). Furthermore, we study the performance of random projections that are independent from the weights \(W_{}\). We show that using only random projections (RP, see Table 2) without adding \(_{}\), we are hardly able to detect any OOD samples. This supports the claim that utilizing the class directions is necessary. The supplementary material presents all the other KLD-specific hyperparameters and we also investigate their influences to the performance in Figure 6. We outline the selected parameters for each benchmark in Table 7.

Figure 4: We investigate the effect of _WeiPer_ hyperparameters \(r\) and \(\) on the performance of the three postprocessors. The left pair shows results on CIFAR10, the right pair corresponds to ImageNet (using ResNet18 for both). Models were tested using their respective near OOD datasets. The panels corresponding to \(\) depict AUROC performance minus the initial AUROC performance at \(=0\). The graphs show the mean over 25 runs and the shaded area around them represents the value range (min to max) over those runs. All other parameters of the methods were fixed to the optimal setting.

    & _{}\)} & _{}\)} \\ 
**Postprocessor** & \(S_{}\) & **Postprocessor** & \(S_{}\) & **Postprocessor** & \(S_{}\) & **Postprocessor** & \(S_{}\) \\ 
**WeiPer+KLD** & 0.988 & OpenMax & 0.943 & NAC & 0.999 & MLS & 0.932 \\ RMDS & 0.984 & VIM & 0.943 & VIM & 0.970 & TempScale & 0.929 \\
**WeiPer+MSP** & 0.977 & EBO & 0.940 & KNN & 0.963 & EBO & 0.924 \\ GEN & 0.975 & SHE & 0.934 & **WeiPer+KLD** & 0.959 & MSP & 0.920 \\
**WeiPer+ReAct** & 0.974 & KLM & 0.918 & RMDS & 0.959 & SHE & 0.919 \\ TempScale & 0.967 & DICE & 0.901 & **WeiPer+ReAct** & 0.951 & DICE & 0.909 \\ KNN & 0.963 & ASH & 0.870 & GEN & 0.947 & KLM & 0.893 \\ MSP & 0.963 & MDS & 0.829 & **WeiPer+MSP** & 0.944 & MDS & 0.877 \\ ReAct & 0.955 & GradNorm & 0.722 & ReAct & 0.943 & ASH & 0.844 \\ MLS & 0.954 & NAC & - & OpenMax & 0.935 & GradNorm & 0.700 \\   

Table 3: Mean relative scores of all the postprocessors (post-hoc methods), see Equation (15).

## 5 Limitations

_WeiPer+_KLD has more hyperparameters than other competitors: 6 in total. As discussed in the previous section, \(r\) can be seen as a memory / performance trade-off (see Figure 4). In the supplementary material (see Figure 6) we investigate the other parameters and find that they all have only one local maximum in the range we were searching and should thus be easy to optimize. We tried to choose the same smoothing size \(s_{1}=s_{2}\) for both densities, but the ablations show that both are optimal at different sizes. While \(_{}\) is not really used for CIFAR10 (\(_{2} 0\)) it is beneficial for CIFAR100 and ImageNet. As WeiPer blows up the dimension we also conduct a memory and time comparison to other methods in Table 4 and Table 5. We demonstrate that with a combination of a confidence and a distance based metric it is possible to achieve competitive near results across the board where all other methods seem to deteriorate in at least one benchmark.

## 6 Conclusion

We show that multiple random perturbations of the class projections in the final layer of the network can provide additional information that we can exploit for detecting out-of-distribution samples. _WeiPer_ creates a representation of the data by projecting the latent activation of a sample onto vector bundles around the class-specific weight vectors of the final layer. We then employ a new approach to construct a score allowing the subsequent separation of ID and OOD data. It relies on the fingerprint-like nature of features of the penultimate and the _WeiPer_-representations by assuming they were sampled by the same underlying distribution. In a thorough evaluation, we first show that _WeiPer_ enhances MSP and ReAct+MSP performance significantly and show that _WeiPer_+KLD achieves top scores in most benchmarks, representing the new state-of-the-art solution in post-hoc OOD methods on near benchmarks.

## 7 Acknowledgements

We appreciate the reviewers' comments, which greatly helped enhance this manuscript and inspired us to conduct additional key experiments. Maximilian Granz was supported by the Elsa-Neumann-Scholarship from the state of Berlin, which provided essential funding for the initial stages of this research. We also thank Leon Sixt and Manolis Panagiotou for their feedback throughout the project.