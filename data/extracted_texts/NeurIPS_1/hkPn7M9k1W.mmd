# _Fantastic Weights and How to Find Them:_

Where to Prune in Dynamic Sparse Training

 Aleksandra I. Nowak

Jagiellonian University, Doctoral School of Exact and Natural Sciences;

IDEAS NCBR

aleksandrairena.nowak@doctoral.uj.edu.pl

Work started while visiting University of Twente.

Bram Grooten

Eindhoven University of Technology

Decebal Constantin Mocanu

University of Luxembourg;

Eindhoven University of Technology;

University of Twente

Jacek Tabor

Jagiellonian University,

Faculty of Mathematics and Computer Science

###### Abstract

Dynamic Sparse Training (DST) is a rapidly evolving area of research that seeks to optimize the sparse initialization of a neural network by adapting its topology during training. It has been shown that under specific conditions, DST is able to outperform dense models. The key components of this framework are the pruning and growing criteria, which are repeatedly applied during the training process to adjust the network's sparse connectivity. While the growing criterion's impact on DST performance is relatively well studied, the influence of the pruning criterion remains overlooked. To address this issue, we design and perform an extensive empirical analysis of various pruning criteria to better understand their impact on the dynamics of DST solutions. Surprisingly, we find that most of the studied methods yield similar results. The differences become more significant in the low-density regime, where the best performance is predominantly given by the simplest technique: magnitude-based pruning.

## 1 Introduction

Modern deep learning solutions have demonstrated exceptional results in many different disciplines of science . However, they come at the cost of using an enormous number of parameters. Consequently, compression methods aim to significantly reduce the model size without introducing any loss in performance .

One approach to sparsifying neural networks is pruning, in which a portion of the weights is removed at the end of the training based on some predefined importance criterion . More recently,  have found that iterative pruning joined with cautious parameter re-initialization can identify sparse subnetworks that are able to achieve similar performance to their dense counterparts when trained from scratch. This result, known as the _lottery ticket hypothesis_, has launched subsequent research into methods for identifying and training models that are sparse already at initialization .

An especially promising direction in obtaining well-performing sparse neural networks is the Dynamic Sparse Training (DST) framework. Inspired by the neurogeneration in the brain, DST allows for plasticity of the initial sparse connectivity of the network by iteratively pruning and re-growing a portion of the parameters in the model . This relatively new concept has already gained increasing interest in the past few years. Most notably, in computer vision, DST demonstrated that it is sufficient to use only \(20\%\) of the original parameters of ResNet50 to train ImageNet without any drop in performance [12; 38]. Even more intriguingly, in Reinforcement Learning applications, DST is able to significantly outperform the classical dense models [18; 58]. At the same time, general sparse networks have been reported to surpass their dense counterparts in terms of adversarial robustness . All those results demonstrate the incredible potential of DST not only in increasing the model's efficiency but also in providing a better understanding of the features and limitations of neural network training.

Motivated by the above, we take a closer look at the current DST techniques. Most of the research concerning design choices in DST focuses on the analysis of methods where variations occur in the growth criterion, while the pruning criterion becomes intertwined with other design considerations [3; 12; 11; 1]. In this study, we address this issue by taking a complementary approach and focusing on the pruning criteria instead. A pruning criterion in DST serves as a measure of weight importance and hence is a proxy of the "usefulness" of a particular connection. Note that the importance of a connection in DST can differ from standard post-training pruning, as the role of weights can change throughout training due to the network's plasticity and adaptation. Weights deemed unimportant in one step can become influential in later phases of the training.

Our goal is to provide a better understanding of the relationship between pruning criteria and the dynamics of DST solutions. To this end, we perform a large empirical study including several popular pruning criteria and analyze their impact on the DST framework on diverse models. We find that:

* Surprisingly, within a stable DST hyperparameter setup, the majority of the studied criteria perform similarly, regardless of the model architecture and the selected growth criterion.
* The difference in performance becomes more significant in a very sparse regime, with the simplest magnitude-based pruning methods surpassing any more elaborate choices.
* Applying only a few connectivity updates is already enough to achieve good results. At the same time, the reported outcomes surpass those obtained by static sparse training.
* By analyzing the structural similarity of the pruned sets by each criterion, we assert that the best-performing methods make similar decision choices.

The insights from our research identify that the simplest magnitude pruning is still the optimal choice, despite a large amount of alternatives present in the literature. This drives the community's attention to carefully examine any new adaptation criteria for Dynamic Sparse Training.2

## 2 Related Work

**Pruning and Sparse Training.** A common way of reducing the neural network size is pruning, which removes parameters or entire blocks of layers from the network. In its classical form, pruning has been extensively studied in the context of compressing post-training models [25; 44; 31; 21; 20; 46; 33; 43; 17] - see e.g. [23; 4] for an overview and survey. Interestingly,  demonstrated for the first time in the literature that a sparse neural network can match and even outperform its corresponding dense neural network equivalent if its sparse connectivity is designed in a sensitive manner. Recently, a similar result was obtained by the _lottery ticket hypothesis_ and follow-up research, which showed that there exist sparse subnetworks that can be trained in isolation to the same performance as the dense networks [15; 66; 62]. In an effort to find these subnetworks without the need for dense training, different techniques have been proposed over the years [32; 60; 59], including random selection [48; 34]. Such approaches are commonly referred to as Sparse Training, or Static Sparse Training to emphasize that the sparse structure stays the same throughout the training. Interestingly,  find out that the sparse initializations produced by the static approaches are invariant to parameter reshuffling and reinitialization within a layer and that even pre-training magnitude pruning performs quite well in such a setup.

**Dynamic Sparse Training.** In DST, the neural network structure is constantly evolving by pruning and growing back weights during training . The key motivation behind DST is not only to provide compression but also to increase the effectiveness and robustness of the deep learning models without the need for overparametrization . The capability of DST has recently been an area of interest. In , authors indicate that DST is able to outperform static sparse training by assuring better gradient flow. DST has also been reported to achieve high performance in Reinforcement Learning  and Continual Learning , with ongoing research into applicability in NLP . Some attention has also been given to the topological properties of the connectivity patterns produced by DST. In particular, , investigated the structural features of DST. However, they focus only on one method, the Sparse Evolutionary Training (SET) procedure .

To the best of our knowledge, this work is the first to comparatively study a large number of pruning criteria in DST on multiple types of models and datasets. We hope that our analysis will increase the understanding within the dynamic sparse training community.

## 3 Background

### Dynamic Sparse Training

Dynamic sparse training is a framework that allows training neural networks that are sparse already at initialization. The fundamental idea behind DST is that the sparse connectivity is not fixed. Instead, it is repeatedly updated throughout training. More precisely, let \(\) denote all the parameters of a network \(f_{}\), which is trained to minimize a loss \(L(;)\) on some dataset \(\). The density \(d^{l}\) of a layer \(l\) with latent dimension \(n^{l}\) is defined as \(d^{l}=||^{l}||_{0}/n^{l}\), where \(||||_{0}\) is the L0-norm, which counts the number of non-zero entries. Consecutively, the overall density \(D\) of the model is \(D=^{l}d^{l}n^{l}}{_{l=1}^{l}n^{l}}\), with \(L\) being the number of layers in the network. The _sparsity_\(S\) is given by \(S=1-D\). Before the start of training, a fraction of the model's parameters is initialized to be zero in order to match a predefined density \(D\). One of the most common choices for initialization schemes is the Erdos-Renyi (ER) method , and its convolutional variant, the ER-Kernel (ERK) . It randomly generates the sparse masks so that the density in each layer \(d^{l}\) scales as \(+n^{l}}{n^{l-1}n^{l}}\) for a fully-connected layer and as \(+n^{l}+w^{l}+h^{l}}{n^{l}+n^{l}w^{l}h^{l}}\), for convolution with kernel of width \(w^{l}\) and height \(h^{l}\). The sparse connectivity is updated every \( t\) training iterations. This is done by removing a fraction \(_{t}\) of the active weights accordingly to a pruning criterion \(\). The selected weights become inactive, which means that they do not participate in the model's computations. Next, a subset of weights to regrow is chosen accordingly to a growth criterion from the set of inactive weights, such that the overall density of the network is maintained. The pruning fraction \(_{t}\) is often decayed by cosine annealing \(_{t}=(1+)\), where \(T\) is the iteration at which to stop the updates, and \(\) is the initial pruning fraction.

The used pruning and growth criterion depends on the DST algorithm. The two most common growth modes in the literature are random growth and gradient growth. In the first one, the new connections are sampled from a given distribution . The second category selects weights based on the largest gradient magnitude . Other choices based on momentum can also benefit the learning .

### Pruning Criteria

Given a pruning fraction \(_{t}\), the pruning criterion \(\) determines the importance score \(s(^{l}_{i,j})\) for each weight and prunes the ones with the lowest score. We use _local pruning_, which means that the

    & \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) \\  & \(||\) & \(|_{+}|,|_{-}|\) & \(||+|_{}()|\) & \(()|}{||}\) & \(|||_{}()|\) \\  random growth & \(\) & SET & MEST & Sensitivity & \(\) \\ gradient growth & RigL & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 1: An overview of the existing **pruning** criteria in DST. Our work analyzes the differences and similarities between all the methods and fills the gaps (\(\)) in the literature. Each column lists a pruning criterion, while each row presents a growing method. SNIP  was not designed to grow weights, but we investigate whether it can be applied in DST.

criterion is applied to each layer separately.3. For brevity, we will slightly abuse the notation and write \(\) instead of \(^{t}_{i,j}\) from now on. Below, we discuss the most commonly used pruning criteria in DST that are the subject of the conducted study.

**SET.** To the best of our knowledge, SET is the first pruning criterion used within the DST framework, which was introduced in the pioneering work of . The SET criterion prunes an equal amount of positive and negative weights with the smallest absolute value.

**Magnitude.** The importance score in this criterion is given by the absolute value of the weight \(s()=||\). Contrary to SET, no division between the positive and negative weights is introduced. Due to its simplicity and effectiveness, the magnitude criterion has been a common choice in standard post-training pruning, as well as in sparse training [15; 12].

**MEST.** The standard magnitude criterion has been criticized as not taking into consideration the fluctuations of the weights during the training. The MEST (Memory-Economic Sparse Training) criterion , proposed to use the gradient as an indicator of the trend of the weight's magnitude, leading to a score function defined as \(s()=||+|_{}()|\), where \(\) is a hyperparameter.

**Sensitivity.** The role of gradient information in devising the pruning criterion has also been studied by . Taking inspiration from control systems, the authors propose to investigate the relative gradient magnitude in comparison to the absolute value of the weight, yielding \(s()=|_{}()|/||\). In our study, we consider the _reciprocal_ version of that relationship \(s()=||/|_{}()|\), which we call _RSensitivity_, as we found it to be more stable.4

**SNIP.** The SNIP (Single-shot Network Pruning) criterion is based on a first-order Taylor approximation of the difference in the loss before and after the weight removal. The score is given by \(s()=|||_{}()|\). The criterion has been originally successfully used in static sparse training  and post-training pruning . Motivated by those results, we are interested in investigating its performance in the dynamic sparse training scenario.

We denote the above-mentioned criteria by adding a subscript under the sign \(\) in order to distinguish them from the algorithms that introduced them. We summarize their usage with random and gradient growth criteria in the DST literature in Table 1.

## 4 Methodology

This work aims to understand the key differences and similarities between existing pruning techniques in DST by answering the following research questions:

* What is the impact of various pruning criteria on the performance of dynamic sparse training?
* How does the frequency of topology updates influence the effectiveness of different pruning methods?
* To what extent do different pruning criteria result in similar neural network topologies?

Q1: Impact on the Performance of DST.In the first research question, we are interested in investigating the relationship between the pruning criterion and the final performance of the network. We compare the results obtained by different pruning criteria on a diverse set of architectures and datasets, ranging from a regime with a small number of parameters (\(<100K\)) to large networks (\( 13M\)). We examine multi-layer perceptrons (MLPs) and convolutional neural networks (ConvNets). Within each model, we fix the training setup and vary only the pruning criterion used by the DST framework. This allows us to assess the benefits of changing the weight importance criterion in isolation from other design choices. As the growing criterion, we choose the uniform random  and gradient  approaches, as they are widely used in the literature. In addition, we also perform a separate comparison on the ResNet-50 model (\(25M\) parameters) on ImageNet.

The common sense expectation would be that the more elaborate pruning criteria, which combine the weight magnitude with the gradient information, will perform better than the techniques that rely only on the weight's absolute value. Since the gradient may provide information on how the weight will change in the future, it indicates the trend the weight might follow in the next optimization updates.

Therefore it may be more suitable in the DST approach, where connectivity is constantly evolving, making insights into future updates potentially beneficial. This is especially promising considering the effectiveness of the gradient-based approach in estimating the importance in the growth criterion.

Q2: Update Period Sensitivity.In the second research question, we focus on one of the most important design choices: the topology update period \( t\). The setting of this update period determines how often the network structure is adapted during training. When considered together with the dataset- and batch-size, this hyperparameter can be interpreted as the number of data samples the model trains on before performing the mask update. Therefore, within a fixed dataset and batch size, using a relatively low value means that the topology is changed very frequently in comparison to the optimization update of the weights. This poses a potential risk of not letting the newly added connections reach their full potential. On the other hand, a high value gives the weights enough opportunity to become important but may not allow the sparse network to adjust its structure based on the data characteristics. Finding a balance between this exploitation-exploration conflict is an imperative task in any DST framework.

Q3: Structural Similarity.Finally, we take a deep dive into the structures produced by the various pruning criteria. Please note that while the previous questions, Q1 and Q2, may explain how the pruning criterion choice affects performance and update frequency in DST, they do not indicate whether the mask solutions obtained by those criteria are significantly different. In order to assess the diversity of the studied methods in terms of the produced sparse connectivity structures, we need to compare the sets of weights selected for pruning by each criterion under the same network state. In addition, we also investigate the similarity between the final masks obtained at the end of training and how close they are to their corresponding sparse initializations. This allows us to assess whether the DST topology updates are indeed meaningful in changing the mask structure. For both cases, we incorporate a common score of the proximity of sets, known as the Jaccard index (or Intersection-Over-Union). We compute it separately for each layer and average the result:

\[(I_{a},I_{b})=_{l=1}^{L}J(I_{a}^{l},I_{b}^{l}),\;\;J(I_ {a}^{l},I_{b}^{l})=^{l} I_{b}^{l}|}{|I_{a}^{l} I_{b}^{l}|}, \]

where \(I_{a}^{l}\) and \(I_{b}^{l}\) are the sets selected by pruning criteria \(a\) and \(b\) in layer \(l\). A Jaccard index of \(1\) indicates that sets overlap perfectly, while \(0\) implies they are entirely separate.

Figure 1: The test accuracy versus density computed on the studied models for different pruning criteria (due to space limits we only present here six plots, see Figure 10 in Appendix D for the remaining two setups). The first row represents the results obtained by random growth, while the second corresponds to gradient growth. Note the logarithmic scale in the x-axis. The performance of the dense model is indicated by the horizontal dashed line. In almost every case all pruning criteria perform well, regardless of the chosen model and growth criterion. At the same time, they surpass the static initialization (in light blue).

We expect to see some overlap in the pruning methods' selection of weights since all of them incorporate the use of the magnitude of the weight. We also hope to see if the difference between the connectivities produced by scores that give different performances is indeed large or whether a small adjustment of the weights would render them equal.

## 5 Experiments

In this section, we present the results of our empirical studies. We start with the description of the experimental setup and then answer each of the posed questions in consecutive sections.

### Setup of the Experiments

We perform our analysis using eight different models, including small- and large-scale MLPs and Convolutional Nets. The small-MLP is a 4-layer network with hidden size of at most \(256\), (trained on the tabular Higgs dataset ). The large-MLP also consists of 4 layers (latent dimension size up to 1024 neurons) and is evaluated on the CIFAR10 . The convolutional architectures are: a small 3-layer CNN (CIFAR10), LeNet5-Caffe  (FashionMNIST ), ResNet56 (CIFAR10, CIFAR100), VGG-16 (CIFAR100) and EfficientNet (Tiny-ImageNet ). In addition, on a selected density, we also consider the ResNet50 model on ImageNet . We summarize all the architectures in Appendix A. While our primary focus is on vision and tabular data, we also evaluate the pruning criteria on the fine-tuning task of ROBERTa Large  using the CommonsenseQA dataset adapted from the Sparsity May Cry (SMC) Benchmark . In contrast to the setup studied throughout this work, this experiment involves a fine-tuning type of problem. In consequence, we discuss it in Appendix C and focus on training models from scratch for the remainder of the paper.

We train the models using the DST framework on a set of predefined model densities. We use adjustment step \( t=800\) and initial pruning fraction \(=0.5\) with cosine decay. As the growth criterion, we investigate both the random and gradient-based approaches, as those are the most common choices in the literature. Following , we reuse the gradient computed during the backward pass when calculating the scores in the gradient-based pruning criteria. In all cases, we ensure that within each model, the training hyperparameters setup for every criterion is the same. Each performance is computed by averaging 5 runs, except for ImageNet, for which we use 3 runs. The ResNet50 model is trained using density \(0.2\) and gradient growth (we select the gradient-based growth method since is known to provide good performance in this task ). Altogether we performed approximately **7000** runs of dense, static sparse, and DST models, which in total use 8 (9 including ImageNet) different dataset-model configurations, and jointly took approximately **5275** hours on GPU.5

### Impact on the Performance of DST

In this section, we analyze the impact of the different pruning criteria on the test accuracy achieved by the DST framework. We compare the results with the original dense model and static sparse training with ERK initialization. The results are presented in Figure 1.

In this work, we are interested in how the frequency of the topology update impacts different pruning criteria. Intuitively, one could anticipate that the pruning criteria incorporating the gradient

Figure 2: **Left:** Validation accuracy of the different pruning criteria on ImageNet obtained for density \(0.2\). The dashed black line indicates the best result of the dense model. **Right:** The training loss versus the number of epochs. We see that all methods perform similarly, except for \(_{}\), which suffers already at the beginning of the training.

information will perform better with a smaller update period, as the gradient may indicate the trend in the weight's future value.

Firstly, we observe that the differences between the pruning criteria are usually most distinctive in low densities. Furthermore, in that setting, the dynamic sparse training framework almost always outperforms the static sparse training approach. This is important, as in DST research, high sparsity regime is of key interest. Surprisingly, the more elaborate criteria using gradient-based approaches usually either perform worse than the simple magnitude score, or do not hold a clear advantage over it. For instance, the Taylor-based \(_{}\) criterion, despite being still better than the static initialization, consistently achieves results similar or worse than \(_{}\). This is especially visible in the DST experiments using gradient growth (note ResNet56, MLP-CIFAR10, and EfficientNet). The only exception is the VGG-16 network. In addition, we also observe that \(_{}\) gives better performance when used in a fine-tuning setup for EfficientNet - see Appendix H. Consequently, this Taylor-approximation-based criterion, although being well suited for standard post-training pruning  and selecting sparse initialization masks  seems not to be the optimal choice in dynamic sparse training approaches on high sparsity. The \(_{}\), on the other hand, can outperform other methods but is better suited for high densities. Indeed, it is even the best choice for densities larger than \(0.15\) for large-MLP on CIFAR10 and ResNet models. Finally, the \(_{}\) criterion also is not superior to the \(_{}\), usually leading to similar results.6 We also present the results obtained for ResNet-50 on ImageNet with density \(0.2\) in Figure 2. We observe that, again, the \(_{}\) criterion leads to the worst performance. The \(_{}\), \(_{}\), and \(_{}\) methods achieve high results but not clearly better than the \(_{}\) criterion.

Within each studied density, growing criterion, and model, we rank the pruning criteria and then calculate their average ranks. To rigorously establish the statistical significance of our findings, we compute the Critical Distance Diagram for those ranks, using the Nemenyi post-hoc test  with p-value \(0.05\) and present it in Figure 3. We observe that in the low-density regime (Figure 2(b)), the \(_{}\) criterion achieves the best performance, as given by the lowest average rank. Furthermore, in such a case, we also note that other predominately magnitude-based criteria, such as \(_{}\) and \(_{}\) are not significantly different than \(_{}\), while the remaining gradient-based approaches are clearly worse. This confirms the overall effectiveness of the

Figure 4: **Left:** The loss gap (validation loss \(-\) training loss) over time for different pruning criteria, the static sparse model, the dense model, and the dense model with dropout \(p=0.05\) on the large-MLP. **Right:** The test accuracy obtained for each criterion.

Figure 3: The critical distance diagram of the studied methods for **(a)** all densities, **(b)** densities smaller or equal \(0.2\), and **(c)** densities larger than \(0.2\). For each model we compute its average rank (the lower the better) and plot it on the horizontal line. The methods with ranks not larger than the critical distance (denoted as CD, displayed in the left upper corner of each plot) are considered not statistically different and are joined by thick horizontal black lines.

weight's magnitude as an importance score in sparse regimes. See also Appendix G for the discussion of the effect of batch size on the obtained results. Interestingly, when larger densities are involved, the best choice is given by \(_{}\). Additionally, the average rank of \(_{}\) deteriorates. This may suggest that the information from the gradient is more reliable in such a case. The gradient-based methods seem to perform better in denser setups.

Finally, we notice that in certain situations, the DST framework can lead to better results than the original dense model (see small-MLP, or ResNet56 on CIFAR100 in Figure 1). We argue that this is due to a mix of a regularization effect and increased expressiveness introduced by removing and adding new connections in the network - see Figure 4. Moreover, we also observe that within the setup studied in Figure 1, there is almost no difference between choosing the random or gradient _growth criterion_. The second one achieves slightly better accuracy for the convolutional models (see, e.g., density \(0.2\) on ResNet-56, as well as improved stability for EfficientNet). The similarity between those two growth criteria and the surprising effectiveness of DST in comparison to dense models have also been previously observed in the context of Reinforcement Learning .

### Sensitivity to Update Period

In this section, we analyze the sensitivity of each pruning criterion to the update period \( t\). This value describes how often the adjustments are made in training and hence can affect the weights' importance scores. We fix the density to \(0.2\) and \(0.1\) and the initial pruning fraction \(\) to \(0.5\). We choose the densities \(0.2\) and \(0.1\) as they correspond to rather high sparsity levels and generally provide reasonable performance. Next, we investigate different update periods on the models using CIFAR10 dataset.7 We start from \( t=25\) and increase it by a factor of \(2\) up to \( t=6400\).

The results are presented in Figure 5. For the MLP model, we clearly see the behavior described in Section 4: if the update period \( t\) is too small, the performance will suffer. The best update period setting seems to be \( t=800\) for the random growth, regardless of the pruning method. For gradient growth, this value also gives a good performance. However, larger pruning periods (e.g. \( t=1600\)) are even better.8 Note that we use a batch size of \(128\), hence for \( t=800\) the connectivity is changed approximately once per 2 epochs.9 Similarly, less frequent updates are also beneficial in the ResNet56 model, although there is not as much consensus between the different pruning criteria. Even performing just one topology update every \( t=6400\) iterations (approximately once per 16 epochs) still gives excellent performance. At the same time, not performing any topology updates at

Figure 5: The update period \( t\) versus validation accuracy on the CIFAR10 dataset on the MLP, ConvNet, and ResNet-56 models for different pruning criteria with density \(0.2\) and \(0.1\). The top row corresponds to random growth, while the bottom row corresponds to gradient growth. Note the logarithmic scale on the x-axis. We see that the methods are most sensitive to the update frequency in the MLP setting. For all setups, performing the update later (\( t>400\)) seems to be beneficial.

all (i.e., static sparse training) deteriorates performance, as shown by the cyan dashed line in Figure 5. Furthermore, we observe that the gradient-based pruning criteria seem to suffer much more than the magnitude criteria when paired with a small update period (consider the plots for \( t=25\)), especially when combined with the gradient growth.10 This suggests that it is safer not to use those techniques together or be more rigorous about finding the right update period in such a case.

Most interestingly, the results from Figure 5 strongly suggest that frequent connectivity adjustment is not needed. Even few pruning and re-growing iterations are enough to improve the static initialization.

### Structural Similarity

Finally, we investigate whether the studied criteria are diverse in terms of the selected weights for removal. We fix the dataset to CIFAR10 and analyze the moment in which the sparse connectivity is updated for the first time. At such a point, given the same sparse initialization and random seed, all the pruning criteria share the same network state. Consequently, we can analyze the difference between the sets selected for removal by each criterion. We do this by computing their Jaccard index (Equation 1) and present the averaged results of \(5\) different runs in Figure 5(a) (See also Appendix K standard deviations and more plots).

We observe that the \(_{}\) and \(_{}\) mainly select the same weights for pruning. This indicates that the absolute values of the positive and negative weights are similar. In addition, for the simpler models with smaller depths (MLP and CNN) the \(_{}\) criterion also leads to almost the same masks as \(_{}\). On the other hand, the \(_{}\) and \(_{}\) criteria produce sets that are distinct from each other. This is natural, as \(_{}\) multiplies the score by the magnitude of the gradient, while \(_{}\) uses its inverse. The experiment suggests that early in training, pruning criteria such as \(_{}\), \(_{}\), and \(_{}\) lead to similar sparse connectivity. Together with the similarity in performance observed in Section 5.2, the results indicate that these three methods are almost identical, despite often being presented as diverse in DST literature. At the same time, the updates made by \(_{}\) and \(_{}\) produce different pruning sets, and result in lower performance (recall Figure 2(b)). However, note that for each entry (except \(_{}\) with \(_{}\)), the computed overlap is larger than random, suggesting that there is a subset of weights considered important by all the criteria.

In addition, we also compare how similar are the final sparse solutions found by the methods. To this end, we fix the sparse initialization to be the same for each pair of criteria and compute the Jaccard Index of the masks at the end of the DST training (performed with gradient growth and density \(0.2\) on the CIFAR10 experiments from Section 5.2). For each pair, we average the score over \(5\) initializations. The resulting similarity matrix is presented in Figure 5(b). We observe that pruning criteria that selected similar sets of weights for removal from the previous experiment still hold some resemblance to each other in terms of the end masks. Let us also note that in Section 5.3, we observed

Figure 6: **(a)** The mean normalized Jaccard index between the sets of weights chosen for removal during the first update of the sparse connectivity, computed for the MLP, CNN, and ResNet-56 models on CIFAR10. The \(J_{r}\) indicates the expected overlap of random subsets and serves as a reference point. We estimate this value by computing the mean pair-wise Jaccard index of the random pruning criterion with different sparse initialization. **(b)** The same index computed between the masks obtained by different pruning criteria at the end of training. The rows and columns represent the pruning criteria between which the index is computed. The \(J_{init}\) denotes the expected overlap of random masks and serves as a reference point. We estimate this value by computing the mean pair-wise Jaccard index of the different sparse initializations.

that applying just a few connectivity updates is sufficient for good results. This raises the natural question of whether DST masks are genuinely diverse from their sparse initializations. By analyzing the bottom row in Figure 5(b), we see that the masks obtained at the end of the DST training are distinct from their corresponding initializations, having the Jaccard Index close to that of a random overlap.

In consequence, we conclude that the best-performing methods from Section 5.2 indeed make similar decision choices while having a smaller overlap with the less efficient criteria. This renders the magnitude-based criteria almost equivalent, despite their separate introduction in the literature. At the same time, the DST mask updates made during the training are necessary to adapt and outperform the static initialization. We would like to highlight that such insights could not have been derived solely from performance and hyperparameter results. We hope that our insights will raise awareness of the importance of performing structural similarity experiments in the DST community.

## 6 Conclusion

We design and perform a large study of the different pruning criteria in dynamic sparse training. We unveil and discuss the complex relations between these criteria and the typical DST hyperparameters settings for various models. Our results suggest that, overall, the differences between the multiple criteria are minor. For very low densities, which are the core of DST interest, criteria based on magnitude perform the best. This questions the effectiveness of gradient-based scores for pruning during the training. We propose to incorporate the selected models and datasets as a baseline in future works investigating the pruning criteria for DST to avoid the common case where methods are overfitted to given DST hyperparameters or tasks. We hope that our research will contribute to the understanding of the sparse training methods.

**Limitations & Future Work.** Our research was conducted mainly on datasets from computer vision and one tabular dataset. It would be interesting to verify how our findings translate to large language models. The computed structural insights are based on set similarity and disregard the information about the value of the parameters. Additional topographic insights could be provided to incorporate this information and analyze the graph structure of the found connectivity. Finally, much work still needs to be done on the hardware side to truly speed up training times and decrease memory requirements (see Appendix F). We do not see any direct social or ethical broader impact of our work.