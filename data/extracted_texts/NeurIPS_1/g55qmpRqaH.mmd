# DFM: Interpolant-free Dual Flow Matching

Denis Gudovskiy\({}^{1}\)

Tomoyuki Okuno\({}^{2}\)

Yohei Nakata\({}^{2}\)

\({}^{1}\)Panasonic AI Lab, USA

denis.gudovskiy@us.panasonic.com

&\({}^{2}\)Panasonic Holdings Corporation, Japan

{okuno.tomoyuki, nakata.yohei}@jp.panasonic.com

###### Abstract

Continuous normalizing flows (CNFs) can model data distributions with expressive infinite-length architectures. But this modeling involves computationally expensive process of solving an ordinary differential equation (ODE) during maximum likelihood training. Recently proposed flow matching (FM) framework allows to substantially simplify the training phase using a regression objective with the interpolated forward vector field. In this paper, we propose an interpolant-free dual flow matching (DFM) approach without explicit assumptions about the modeled vector field. DFM optimizes the forward and, additionally, a reverse vector field model using a novel objective that facilitates bijectivity of the forward and reverse transformations. Our experiments with the SMAP unsupervised anomaly detection show advantages of DFM when compared to the CNF trained with either maximum likelihood or FM objectives with the state-of-the-art performance metrics.

## 1 Introduction

Discrete-time (DNF) and continuous-time (CNF) normalizing flows have been previously extensively studied and compared in details (Ruthotto and Haber, 2021). With the pros and cons in each approach, we are motivated to apply CNFs in real-world generative and density estimation applications. Theoretically, an infinite-length architecture with arbitrary parameterization can lead to a significant advantages of CNFs when compared to DNF shortcomings. Meanwhile, CNFs with the ordinary differential equation (ODE) integration step endure higher computational complexity, numerical instabilities and approximation errors (Chen et al., 2018). In particular, the latter is crucial in density estimation which requires accurate estimates of the Jacobian matrix trace.

Recent flow matching (FM) framework (Lipman et al., 2023) simplifies the training phase in CNFs by introducing a new regression objective. This objective minimizes mean square difference of a parameterized vector field model and an interpolated vector field between two data distributions. While the former is a conventional neural network with time-dependent conditioning, the latter relies on certain assumptions about modeled data distributions. As a result, there is an extensive line of research that proposes various forms of the interpolated vector fields and the corresponding probability paths. We summarize recent works in Table 1 with the formal introduction in Section 2. Diffusion models (Song and Ermon, 2019; Ho et al., 2020) that solve stochastic differential equations (SDEs) can also be generalized using the FM framework (Tong et al., 2024).

In this paper, we analyze current interpolation-based FM approach and its inherent limitations i.e. the Gaussian probability path assumption between two data distributions (Lipman et al., 2023). To address this limitation, we propose a novel _interpolant-free dual flow matching_ (DFM) method. Specifically, we accomplish interpolant-free FM using an additional parameterized reverse vector field model. For simplicity, we model the reverse vector field using exactly the same architecture as for the forward one. Then, we optimize our DFM using an objective that enforces transformation bijectivity of the modeled forward and the reverse vector fields.

## 2 CNF Preliminaries and Prior FM Methods

**Continuous normalizing flows.** We follow Lipman et al. (2023) and Tong et al. (2024) notation. We consider a pair of data distributions \(q(_{0})\) and \(q(_{1})\) over \(^{D}\) with densities \(p(_{0})\) and \(p(_{1})\), respectively. Often, the \(p_{0}=p(_{0})\) density represents a known prior distribution while the data density \(p_{1}=p(_{1})\) is not given with only access to an empirical \((_{1})\) and \(p_{1}\) needs to be estimated.

Then, there are a _probability density path_\(p:^{D}\!\!\!0\), which is a time-dependent probability density function \(p_{t}()\) with \(t=\) such that \( p_{t}(x)dx=1\), and a Lipschitz-smooth time-dependent _vector field_\(u:^{D}^{D}\). The vector field \(u_{t}\) is used to construct a time-dependent diffeomorphism i.e., the CNF \(:^{D}^{D}\) that is defined via the ODE as

\[d_{t}()/dt=u_{t}(_{t}())_{0}()= _{0}, \]

where \(_{t}()\) is the ODE solution with \(_{0}()\) initial condition that transports \(\) from time \(0\) to time \(t\).

On the other hand, \(_{t}\) induces a push-forward \(p_{t}=[_{t}]_{\#}(p_{0})\) that transports the density \(p_{0}\) from time \(0\) to time \(t\). The time-dependent density \(p_{t}\) is characterized by the _continuity equation_ written by

\[ p_{t}()/ t=-(p_{t}()u_{t}(_{t}( )))=-(f_{t}()), \]

where the divergence operator, \(\), is defined as the sum of derivatives of \(f_{t}()^{D}\) w.r.t. all elements \(x_{d}\) or, simply, the Jacobian matrix trace: \((f())=_{d=1}^{D} f_{d}()/ x_{d}= ()\).

The vector field \(u_{t}(_{t}())\) is often modeled without \(_{t}()\) invertability requirement by an arbitrary neural network \(v_{}(t,_{t})\) with the learnable weight vector \(\). Then, the continuity equation in (2) for (1) neural ODE can be written using the instantaneous change of variables (Chen et al., 2018) as

\[d p_{t}(_{t})/dt+( v_{}(t,_{t})/ _{t}^{T})=0. \]

The (3) neural ODE can be solved both for a point \(_{0}\) and the \(\)-likelihood change as integration

\[_{0}\\ (p_{1}/p_{0})=_{t=1}^{t=0}v_{}( t,_{t})\\ -( v_{}(t,_{t})/_{t}^{T}) dt,_{t}\\ (p_{1}/p_{t})=_{1}\\ 0. \]

Then, the CNF maximizes likelihood (MLE) during training and uses its estimate at the evaluation as

\[_{}_{}:=_{1}= p_{0}- _{t=1}^{t=0}( v_{}(t,_{t})/ _{t}^{T})dt, \]

where \( p_{0}\) is the likelihood of a point \(_{0}\) from (4) when evaluated using a known prior \(q(_{0})\).

**Flow matching training.** Typically, solving the ODE (4) for the MLE objective (5) is computationally expensive (Grathwohl et al., 2019; Zhuang et al., 2020). The FM framework (Lipman et al., 2023) proposes an alternative objective that regresses the \(v_{}(t,_{t})\) to \(u_{t}\) by conditioning the latter by a vector \(=_{1}\). This has been extended by the conditional FM (CFM) framework (Tong et al., 2024) where the \(u_{t}(|)\) and \(p_{t}(|)\) are conditioned on a more general \( q()\) such that the marginal probability density path and the corresponding marginal vector field are defined as

\[p_{t}()= p_{t}(|)q()\,d,u_{t}()= _{ q()}[u_{t}(|)p_{t}(| )/p_{t}()]. \]

   Probability Path & \(q()\) & \(_{t}()\) & \(_{t}\) \\  Var. Exploding SDE (Song and Ermon, 2019) & \(q(_{1})\) & \(_{1}\) & \(_{1-t}\) \\ Var. Preserving SDE (Ho et al., 2020) & \(q(_{1})\) & \(_{1-t}_{1}\) & \(^{2}}\) \\ FM (Lipman et al., 2023) & \(q(_{1})\) & \(_{1}\) & \(t-t+1\) \\ Rectified FM (Liu et al., 2023) & \(q(_{0})q(_{1})\) & \(t_{1}+(1-t)_{0}\) & 0 \\ Var. Preserving FM (Albergo and Vanden-Eijnden, 2023) & \(q(_{0})q(_{1})\) & \(( t/2)_{0}+( t/2)_{1}\) & 0 \\ I-CFM (Tong et al., 2024) & \(q(_{0})q(_{1})\) & \(t_{1}+(1-t)_{0}\) & \(\) \\   

Table 1: Generalization of probability paths for diffusion and flow matching methods by the (Tong et al., 2024) framework. Unlike ours, these methods rely on an interpolation of the probability paths.

The Gaussian conditional probability path in (6) has the unique conditional vector field such that

\[p_{t}(|)=(\,|\,_{t}(),_{t}()^{2} )\ \ u_{t}(|)=(-_{t}())\,^{}_{t}( )/_{t}()+^{}_{t}(), \]

where the mean \(_{t}()\) and the standard deviation \(_{t}()\) functions parameterize the path \(p_{t}(|)\).

Finally, the CFM objective for the simplified CNF training using (7) result can be written as

\[_{}_{}:=_{t( 0,1),_{t} p_{t}(|),(,), ()}\|v_{}(t,_{t})-u_{t}(| )\|^{2}. \]

Recently proposed \(_{t}()\) and \(_{t}()\) for interpolation of Gaussian conditional probability path \(p_{t}(|)\) in (7) are given in Table 1. The above CFM framework is inherently limited by the (6) interpolation assumption and (7) path choice. To overcome this, Chen and Lipman (2024) extend the Euclidean FM to Riemannian manifolds. Kapusniak et al. (2024) introduce the metric FM that learns parameterized interpolants. There are attempts to support FM for categorical data Stark et al. (2024); Cheng et al. (2024); Campbell et al. (2024) and mixed categorical-continuous data (Dunn and Koes, 2024). In this paper, we are motivated by a different perspective: _is it feasible to avoid the interpolation in FM framework with a reasonable computational cost_?

## 3 The Proposed Interpolant-free Dual Flow Matching

**Dual CNF via a reverse vector field.** Let's extend the CNF framework that is defined in (1-4). We introduce a _dual CNF_ where its first part, implemented as the above non-invertible \(v_{}(t,_{t})\), approximates the vector field \(u_{t}(_{t}())\). In addition, we employ an extension \(v_{}(t,_{t})\) with learnable parameters \(\) that models a _reverse vector field_ model \(u_{t}(_{t}^{-1}())\). In other words, there is the forward transformation \(_{t}=_{t}()\) and the inverse transformation \(_{t}=_{t}()=_{t}^{-1}()\) of the bijective map \(_{t}\).

Then, we can reformulate the equations (4) for \(v_{}(t,_{t})\) model with minor modifications. The neural ODE in (3) can be solved in reverse simultaneously for a point \(_{1}\) and the log-likelihood change with the initial condition \(_{t} q(_{0})\) and \((p_{0}/p_{t})=0\) as integration

\[_{1}\\ (p_{0}/p_{1})=_{t=0}^{t=1}v_{ }(t,_{t})\\ -(}(t,_{t})}{ _{t}^{2}})dt. \]

Interestingly, the (9) approach with the modified maximum likelihood \(_{}()\) is known in the

DNF literature as a reverse divergence objective (Papamakarios et al., 2021). When the target data \(p_{1}\) cannot be analytically evaluated, the (9) is impractical for CNF training with the MLE objective.

**Interpolant-free DFM.** On the other hand, the proposed dual CNF with the reverse model in (9) can be used for the interpolant-free flow matching. Instead of the less expressive _affine transformation_\((_{t}(|)=_{t}()+_{t},(,))\) induced by the Gaussian interpolation (7), the proposed DFM only requires _bijectivity of the free-form transformations_\(_{t}\) and \(_{t}^{-1}\) produced by, correspondingly, the forward \(v_{}(t,_{t})\) and the reverse \(v_{}(t,_{t})\) vector field models.

Then, the proposed dual CNF with the bijective \(_{t}\) can be expressed as ODEs expressed by

\[d_{t}()/dt=u_{t}(_{t}())=v_{}(t, _{t})\\ d_{t}()/dt=u_{t}(_{t}())=v_{}(t,_{t}).  \]

Assuming the \(_{t}()=_{t}^{-1}()\) bijectivity in a neighborhood of \(t\) for \(\) and \(\), (10) can be rewritten using the univariate inverse function theorem by substituting the top to bottom as

\[d_{t}^{-1}()/dt=1/(d_{t}()/dt)\ (v_{}(t,_{t}) v_{}(t,_{t}))=. \]

Figure 1: The CFM (top) regresses the Gaussian-interpolated forward vector field by a neural network with the _affine transformation_\(_{t}()\). Our DFM (bottom) has two neural networks with the _free-form transformations_ with only the bijectivity objective \(_{t}=_{}^{-1}(_{}(_{t}))\) for an arbitrary vector field and a probability path.

The (11) objective can be achieved by minimizing the cosine distance between two unit vectors as

\[_{,}_{}:=_{t (0,1),_{t}(_{1}),_{t} q(_{0}) }_{}(v_{}(t,_{t}),v_{} (t,_{t})), \]

where this loss with vector normalization is more numerically stable in practice.

Once the (12) loss is minimized, the density estimation i.e. \(_{1}\) can be performed using the conventional MLE approach (5) without the extension \(v_{}(t,_{t})\). On the other hand, the extension can be used to improve \(_{1}\) by integrating (9) from \(t=1\) to \(t=0\). We use the latter strategy. While we yet to accomplish sampling experiments, we expect DFM to outperform previous FM methods due to the enforced bijectivity which is a common issue in ODEs (Gholami et al., 2019).

## 4 Experiments

**Benchmark.** We employ real-world SMAP (Hundman et al., 2018) time series benchmark for unsupervised anomaly detection. The soil moisture active passive satellite (SMAP) dataset contains soil samples and telemetry information from the Mars rover with 135K and 428K data points in the training (without anomalies) and test sets, respectively. SMAP data has 25 data dimensions collected from 55 entities. We follow Su et al. (2019) and transform the regression task into a classification task using sliding windows (window size = 8) and replication padding (Tuli et al., 2022).

**Flow models.** We report experimental results for the Glow-type DNF (Kingma and Dhariwal, 2018) from (Gudovskiy et al., 2024) with the state-of-the-art baselines. Second, we experiment with the vanilla CNF from Section 2 and the CNF trained using the CFM framework i.e., the FM from (Lipman et al., 2023) and I-CFM from (Tong et al., 2024). All CNF models have exactly the same U-Net architecture (Ronneberger et al., 2015), learnable \((,^{2})\) prior and identical evaluation using (5).

**Evaluation.** We follow Su et al. (2019) and report precision (P), recall (R), AuC and F\({}_{1}\) score. We provide results when we solve ODE using the fixed-step (F) Euler method with 4 steps and the variable-step (V) Dopri5 method (atol=1e-1, rot=1e-2) from the (Zhuang et al., 2021) library. We use the Hutchinson stochastic estimator of the Jacobian matrix trace (Hutchinson, 1990).

**Quantitative results.** We compare flow models to other popular baselines: OmniAnomaly (Su et al., 2019), CAE-M (Zhang et al., 2021), TranAD (Tuli et al., 2022). It is common in these baselines to train and evaluate a separate model for each SMAP entity. In contrast, all our flow models use a single model for all entities in Table 2 i.e. they are entity-unconditional.

We can derive several important conclusions from Table 2 results. First, continuous-time normalizing flows models, if properly trained and evaluated, are able to outperform discrete-time normalizing flows as well as other non-flow models in this density estimation task. Second, recent integration-free FM training methods using (8) perform similarly or better than the CNF trained by computationally-expensive MLE from (5). Third, the proposed DFM significantly outperforms prior FM methods with only the \(2\) complexity increase. In particular, DFM increases the non-saturated metrics such as precision and \(F_{1}\) score by, correspondingly, 6.5 (88.2% \(\)94.7% ) and 3.1 (93.3% \(\)96.4% ) percentage points.

## 5 Conclusions

In this paper, we analyzed limitations of the interpolation-based flow matching framework that allows to efficiently train a CNF model. To address the limitations, we proposed the interpolant-free dual flow matching method. Our experiments with the SMAP benchmark showed that our DFM achieves state-of-the-art results for the entity-unconditional unsupervised anomaly detection. In future, the DFM objective (11) and the practical loss (12) can further be extended to multivariate case.

   Model & \(\) & P & R & AUC & F\({}_{1}\) \\  OmniAnom. & ✗ & 81.3 & 94.2 & 98.9 & 87.3 \\ CAE-M & ✗ & 81.9 & 95.7 & 99.0 & 88.3 \\ TranAD & ✗ & 80.4 & 99.9 & 99.2 & 89.2 \\ Glow DNF & ✗ & 87.4 & 84.9 & 91.6 & 86.1 \\  Base CNF & F & 87.5 & 98.8 & 98.4 & 92.8 \\ FM & F & 88.2 & 98.9 & 98.5 & 93.3 \\ I-CFM & F & 88.0 & **99.2** & 98.6 & 93.3 \\ DFM (ours) & F & **94.7** & 98.1 & **98.7** & **96.4** \\  Base CNF & V & 86.5 & 91.9 & 94.9 & 89.1 \\ FM & V & 87.4 & **99.6** & **98.7** & 93.1 \\ I-CFM & V & 89.3 & 98.2 & 98.3 & 93.6 \\ DFM (ours) & V & **89.7** & 98.9 & 98.6 & **94.1** \\   

Table 2: SMAP unsupervised anomaly detection. The **best** and the second best metrics, %.