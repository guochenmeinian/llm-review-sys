# DCDepth: Progressive Monocular Depth Estimation

in Discrete Cosine Domain

Kun Wang\({}^{1}\), Zhiqiang Yan\({}^{1}\), Junkai Fan\({}^{1}\), Wanlu Zhu\({}^{1}\), Xiang Li\({}^{2}\), Jun Li\({}^{1}\)1 and Jian Yang\({}^{1}\)

\({}^{1}\)PCA Lab, Nanjing University of Science and Technology, China

\({}^{2}\)Nankai University, China

Corresponding authors.

###### Abstract

In this paper, we introduce DCDepth, a novel framework for the long-standing monocular depth estimation task. Moving beyond conventional pixel-wise depth estimation in the spatial domain, our approach estimates the frequency coefficients of depth patches after transforming them into the discrete cosine domain. This unique formulation allows for the modeling of local depth correlations within each patch. Crucially, the frequency transformation segregates the depth information into various frequency components, with low-frequency components encapsulating the core scene structure and high-frequency components detailing the finer aspects. This decomposition forms the basis of our progressive strategy, which begins with the prediction of low-frequency components to establish a global scene context, followed by successive refinement of local details through the prediction of higher-frequency components. We conduct comprehensive experiments on NYU-Depth-V2, TOFDC, and KITTI datasets, and demonstrate the state-of-the-art performance of DCDepth. Code is available at [https://github.com/w2kun/DCDepth](https://github.com/w2kun/DCDepth).

## 1 Introduction

Monocular **D**epth **E**stimation (MDE) is a cornerstone topic within computer vision communities, tasked with predicting the distance-or depth-of each pixel's corresponding object from the camera based solely on single image. As a pivotal technology for interpreting 3D scenes from 2D representations, MDE is extensively applied across various fields such as autonomous driving, robotics, and 3D modeling , _etc_. However, MDE is challenged by the inherent ill-posed nature of inferring 3D structures from 2D images, making it a particularly daunting task for traditional methodologies, which often hinge on particular physical assumptions or parametric models .

Over the past decade, the field of computer vision has witnessed a substantial surge in the integration of deep learning techniques. Many studies have endeavored to harness the robust learning capabilities of end-to-end deep neural networks for MDE task, propelling the estimation accuracy to new heights. Researchers have investigated a variety of methodologies, including regression-based , classification-based , and classification-regression based approaches , to predict depth on a per-pixel basis within the spatial domain. Despite these significant strides in enhancing accuracy, current methods encounter two primary limitations: the first is the tendency to predict depth for individual pixels in isolation, thus neglecting the crucial local inter-pixel correlations. The second limitation is the reliance on a singular forward estimation process, which may not sufficiently capture the complexities of 3D scene structures, thereby constraining their predictive performance.

To address the identified limitations, we propose to transfer depth estimation from the spatial domain to the frequency domain. Instead of directly predicting metric depth values, our method focuses on estimating the frequency coefficients of depth patches transformed using the **D**iscrete **C**osineTransform (DCT) [2; 6]. This strategy offers dual benefits: firstly, the DCT's basis functions inherently capture the inter-pixel correlations within depth patches, thereby facilitating the model's learning of local structures. Secondly, the DCT decomposes depth information into distinct frequency components, where low-frequency components reflect the overall scene architecture, and high-frequency components capture intricate local details. This dichotomy underpins our progressive estimation methodology, which commences with the prediction of low-frequency coefficients to grasp the macroscopic scene layout, subsequently refining the local geometries by inferring higher-frequency coefficients predicated on previous predictions. The spatial depth map is then accurately reconstructed via the inverse DCT. We illustrate this progress in Fig. 1. To implement our progressive estimation, we introduce a _Progressive Prediction Head_ (PPH) that conditions on previous predictions from both spatial and frequency domains, and facilitates the sequential prediction of higher-frequency components using a GRU-based mechanism. Furthermore, recognizing the DCT's energy compaction property-indicative of the concentration of signal data within low-frequency components-we introduce a DCT-inspired downsampling technique to mitigate information loss during the downsampling process. This technique is embedded within a _Pyramid Feature Fusion_ (PFF) module, ensuring effective fusion of multi-scale image features for accurate depth estimation.

Our contributions can be succinctly summarized in three key aspects:

* To the best of our knowledge, we are the first to formulate MDE as a progressive regression task in the discrete cosine domain. Our proposed method not only models local correlations effectively but also enables global-to-local depth estimation.
* We introduce a framework called DCDepth, comprising two novel modules: the PPH module progressively estimates higher-frequency coefficients based on previous predictions, and the PFF module incorporates a DCT-based downsampling technique to mitigate information loss during downsampling and ensures effective integration of multi-scale features.
* We evaluate our approach through comprehensive experiments on NYU-Depth-V2 , TOFDC , and KITTI  datasets. The results demonstrate the superior performance of DCDepth compared to existing state-of-the-art methods.

## 2 Related Work

**Monocular Depth Estimation (MDE)** remains a central theme in computer vision, essential for translating 2D imagery into 3D scene geometry. The evolution of MDE has been markedly influenced by the integration of deep neural networks. A foundational advancement was introduced by Eigen et al. , who developed a multi-scale deep convolutional network architecture, comprising a global network for coarse depth prediction and a local network for refinement. They also introduced a scale-invariant loss function to address the scale ambiguity challenge inherent in MDE. Building on

Figure 1: **Progressive estimation scheme. For input image with size \(H W\), DCDepth estimates the DCT coefficients for each \(S S\) depth patches. The prediction follows a global-to-local strategy, starting with the initial estimation of lower-frequency components to capture the global scene structure. Subsequently, higher-frequency components are estimated to enhance the local details, while the lower-frequency estimates are refined. The estimation is carried out at \(\) resolution, and spatial-domain estimation is achieved through inverse DCT.**

this, subsequent researches [19; 56; 42; 51; 50; 57; 53] have adopted end-to-end regression approaches with deep convolutional networks to further tackle MDE's challenges.

However, inferring depth from a single image is intrinsically problematic due to the countless potential depth maps that can correspond to one image. To mitigate this, additional information and constraints have been incorporated into the MDE task, such as semantics [44; 60] and surface normals [28; 33]. Further enhancements in depth estimation accuracy have been achieved through attention mechanisms [14; 47; 30], multivariate gaussian modeling , internal discretization technique  and pretraining [48; 46]. In contrast to the regression-based approach, some works [12; 5] have conceptualized MDE as a classification task, estimating the probability distribution of depth values. Yet, these methods often produce discontinuities due to discrete depth outputs. To overcome this, alternative strategies [3; 20; 4; 34] have combined classification and regression formulations, learning probabilistic distributions and employing linear combinations with depth candidates for final depth predictions. Our methodology diverges from these paradigms by progressively estimating frequency coefficients for depth patches after their transformation into the discrete cosine domain. This approach not only enhances computational efficiency but also achieves state-of-the-art performance.

## 3 Method

In this section, we introduce our progressive depth estimation framework, DCDepth. We begin by providing an overview of the 2D **D**iscrete **C**osine **T**ransform (DCT) as essential background knowledge. Subsequently, we delve into the progressive estimation scheme and elaborate on the network architecture. Finally, we present the loss function employed for training our model.

### Reviewing 2D Discrete Cosine Transform

The 2D DCT is a mathematical technique used to decompose 2D discrete signals, such as depth maps and feature maps, into a sum of cosine basis functions with varying frequencies. The basis functions are defined as follows:

\[B_{u,v}^{i,j}=(u)(v)[(i+ )u][(j+)v], \]

where \(u[0,W-1]\) and \(v[0,H-1]\) represent the frequency indices, \(i[0,W-1]\) and \(j[0,H-1]\) denote the signal indices, and \(W\) and \(H\) indicate the input resolution. The terms \((u)\) and \((v)\) correspond to normalization factors. The forward process of 2D DCT, denoted as \(T()\), transforms the input signal \(x^{H W}\) in the spatial domain to the frequency spectrum \(f=T(x),f^{H W}\), and can be expressed as:

\[f_{u,v}=_{i=0}^{W-1}_{j=0}^{H-1}x_{i,j}B_{u,v}^{i,j}. \]

The resulting \(f\) is a matrix with the same size as the input \(x\), with low-frequency components located near the top-left corner and high-frequency components near the bottom-right corner. The upper left one with zero frequency is called the DC components, and the remains are AC components. Low-frequency components typically characterize smooth regions, while high-frequency components capture edges or fine details where signal values change rapidly. The inverse 2D DCT, denoted as \(T^{-1}()\), performs the reverse operation by transforming the frequency spectrum \(f\) back to the spatial domain \(x=T^{-1}(f)\), and can be formulated as:

\[x_{i,j}=_{u=0}^{W-1}_{v=0}^{H-1}f_{u,v}B_{u,v}^{i,j}. \]

The DCT has two desirable advantages. Firstly, it operates in the real number domain, simplifying the data processing. Secondly, it exhibits superior energy compaction properties by concentrating the majority of information within a small number of low-frequency components.

### Progressive Estimation in Discrete Cosine Domain

Estimating depth from a single image remains a challenging task, particularly for scenes with intricate geometry. To tackle this, we propose a progressive method based on 2D DCT to estimate scene depth progressively from a global perspective down to local details. The entire process is illustrated in Fig. 1. We denote the input image as \(^{3 H W}\). Our proposed method, symbolized as \(()\), predicts the frequency coefficients \(^{^{2}}\) for non-overlapping depth patches \(^{S S}\), where \(S\) is set to 8 in our framework. These coefficients are subsequently transformed back to the spatial domain \(}^{H W}\) using the inverse 2D DCT, as expressed by

\[}=T^{-1}(()). \]

The separation of low- and high-frequency components in a depth map effectively divides the scene into overall structures with gradual depth changes and local details with sharp depth transitions. This frequency characteristic enables us to break down the challenging MDE task into multiple prediction stages, progressing from simpler to more complex predictions. Initially, the DC coefficient \(_{0}\) is predicted, establishing a foundational depth context. Subsequently, the AC coefficients \(\{_{i}\}_{i=1}^{S^{2}-1}\) are iteratively estimated in ascending frequency order. During the inverse transformation to the spatial domain, any coefficients yet to be predicted are padded with zeros. In each iterative step \(k\), we not only predict higher-frequency components but also refine the preceding frequency predictions

\[^{k}=^{k-1}+^{k}, \]

by estimating a correction term \(^{k}\). To reduce the required iterations for estimating all \(S^{2}\) coefficients, we utilize the energy compaction property of DCT, and partition the frequency spectrum \(\) into subgroups along the subdiagonal, yielding \(2S-1\) subgroups \(\{g_{i}\}_{i=0}^{2S-1}\). By merging the high-frequency subgroups, we further streamline the iterative process. This grouping strategy ensures that lower-frequency groups contain fewer components necessitating more prediction steps, while higher-frequency groups encompass a larger number of components requiring fewer steps. The intermediate depth maps are provided in Fig. 2 to elucidate the step-by-step prediction process.

### DCDepth Architecture

OverviewWe present the comprehensive framework of DCDepth in Fig. 3, which comprise four key components: an image encoder, a **Py**ramid **F**eature **F**usion (PFF) module, a decoder, and a **P**rogressive **P**rediction **H**ead (PPH). The image encoder acts as a robust feature extractor capturing image features \(=\{_{0},_{1},_{2},_{3}\}\) at varying resolutions of \(}{{4}}\), \(}{{8}}\), \(}{{16}}\), and \(}{{32}}\) relative to the input image size. These multi-scale features are advantageous as the shallow features contain texture-related details, while the deep features hold global and semantic information essential for scene understanding. The PFF module, symbolized as \(()\), is introduced to effectively amalgamate these features, yielding a comprehensive integrated feature representation \(^{}=()\). The decoder, denoted as \(D()\), consists of three neural CRF  modules and two PixelShuffle  modules. This configuration processes and upscales \(^{}\) to \(}=D(^{})\), achieving \(}{{8}}\) of the original resolution. The PPH performs estimations at the same resolution as \(}\). It begins by down-sampling \(_{0}\) to half its resolution using the proposed DCT-based downsampling. This down-sampled feature is then concatenated with \(}\), forming the initial hidden state for the progressive estimation.

Pyramid Feature Fusion ModuleThe primary objective of PFF is to harness the wealth of information embedded in the multi-scale image features, thereby creating a more comprehensive and enriched feature representation conducive to scene understanding. The layout of PFF is depicted in the left box of Fig. 3. Effective feature aggregation necessitates a proficient downsampling strategy to mitigate information loss, especially when downscaling at larger magnifications. To address this, we introduce a novel DCT-based downsampling strategy engineered to minimize information loss during downsampling. The operational procedure of this strategy is elucidated in the bottom-left corner of Fig. 3. Consider a feature map \(^{C H W}\) slated for downsampling by a factor of \(R\). We begin

Figure 2: **Evolution of intermediate depth estimations. We report several intermediate depth estimation results to illustrate our progressive estimation scheme.**

by partitioning \(\) into patches \(^{C R^{2}}\). Each channel of \(\) is then individually subjected to Eq. 2 to transform the feature maps into the frequency domain. Leveraging the energy compaction property of the DCT, the key information within \(\) is condensed into a few dominant frequency components characterized by large absolute values. This compression enables us to selectively reduce the number of channels from \(C R^{2}\) to \(C}{r}\) with a reduction rate of \(r\) via \(1 1\) convolutions configured with groups set to \(C\). The squeezed feature maps are then consolidated through a sequence of operations involving a \(1 1\) convolution followed by a \(5 5\) depth-wise convolution, culminating in the generation of the final output featuring \(C^{}\) channels and reduced spatial resolution.

Progressive Prediction HeadThe PPH, as depicted in the middle segment of Fig. 3, incorporates two specialized encoders: \(E_{s}()\) for spatial-domain inputs and \(E_{f}()\) for frequency-domain inputs. The spatial encoder, composed of three convolutional layers with a stride of 2, convolves and downsamples the spatial-domain input \(}\), producing a feature map at \(}{{8}}\) of the original resolution. The architecture of \(E_{f}()\) is outlined in the right box of Fig. 3. For frequency input \(^{L H W}\), where \(L\) signifies the number of valid frequency components, we first split them into \(L\) chunks with shape \(1 H W\). Each chunk is then processed through three convolutional layers with Swish activation  to extract features of dimensions \(C H W\) for each frequency component. Given the variability in the number of valid frequency components across different iterative steps, we employ cross-attention  mechanism to merge information from the various frequency components. A learnable _aggregation token_ of dimensions \(1 C\) is introduced to compile information from individual frequency components at each pixel location, yielding feature outputs of shape \(C H W\) and effectively compressing the dimension \(L\). The PPH operates iteratively, utilizing a **G**ated **R**ecurrent Unit (GRU) , denoted as \(G(,)\), to encode the historical estimation states

\[_{i}=G(E_{s}(}_{i-1}),E_{f}(_{i-1})), \]

prior to iterative step \(i\). The hidden state \(\) is then projected to the coefficient output by a Pyramid Pooling Module (PPM)  to aggregate global context, followed by a linear projection.

### Loss Function

We employ the scaled scale-invariant loss  to calibrate the model's depth estimations \(}_{i}\) at each iterative step \(i\) against the ground truth depth map \(\). The loss function is formulated as:

\[L_{d}=_{i=1}^{N}^{N-i} d_{i}^{2}- }( d_{i})^{2}}, \]

where \(d=}_{i}-\), \(N\) denotes the number of iterative steps, and \(M\) represents the number of valid depth values. We consistently set \(=10\), \(=0.8\) and \(=0.85\) across all experiments. The presence

Figure 3: **DCDepth framework overview. The DCT-based downsampling strategy is shown at the bottom-left corner, where \(R\) and \(r\) denote for downsampling factor and channel reduction rate, respectively. The central section details the iterative process of PPH, with \(N\) indicating the number of iterative steps. The frequency encoder utilized by PPH is illustrated at the right box.**

of missing values in the depth ground truth can render the model's frequency-domain predictions inadequately supervised. To mitigate this, we introduce two regularization terms. Specifically, to enforce the sparsity of high-frequency coefficients, we define the frequency regularization loss as:

\[L_{f}=(^{u+v}-1)|f_{u,v}|, \]

where \(f_{u,v}\) is the frequency coefficient indexed by \((u,v)\), and \(\) is set to \(1.2\). Additionally, we incorporate a smoothness term to promote the smoothness of \(}\):

\[L_{s}=|_{x}}| e^{-|_{x}I_{t}|}+|_ {y}}| e^{-|_{y}I_{t}|}, \]

where \(_{x}\) and \(_{y}\) represent image gradient along horizontal and vertical axes, respectively, and \(||\) denote the absolute value function. The final loss is the weighted summation of these three loss terms.

## 4 Experiment

In this section, we evaluate DCDepth by conducting a comparative analysis with established methodologies. We commence by delineating the datasets and evaluation metrics employed in our evaluation. Subsequently, we detail the implementation specifics that underpin our experiments. Concluding this section, we demonstrate the efficacy of the proposed modules via extensive ablation studies.

### Dataset and Evaluation Metric

DatasetWe evaluate our method on three datasets that covers a diverse array of indoor and outdoor scenes. **(1) NYU-Depth-V2** is centered on indoor environments and consists of RGB-D images

  Method & Backbone & Abs Rel \(\) & Sq Rel \(\) & RMSE \(\) & \(_{10}\) & \(<1.25^{}\) & \(<1.25^{}\) & \(<1.25^{}\) \\   DORN  & ResNet-101 & 0.115 & – & 0.509 & 0.051 & 0.828 & 0.965 & 0.992 \\ VNL  & ResNet-101 & 0.108 & – & 0.416 & 0.048 & 0.875 & 0.976 & 0.994 \\ FTS  & Dense-161 & 0.110 & 0.066 & 0.392 & 0.047 & 0.885 & 0.978 & 0.994 \\ ASNDepth  & HRNet-48 & 0.101 & – & 0.377 & 0.044 & 0.890 & 0.982 & 0.996 \\ TransDepth  & R-50+Vr-B/16 & 0.106 & – & 0.365 & 0.045 & 0.900 & 0.983 & 0.996 \\ AdaBins  & E-B5+mini-VrT & 0.103 & – & 0.364 & 0.044 & 0.903 & 0.984 & 0.997 \\ LocalBins  & E-B5 & 0.099 & – & 0.357 & 0.042 & 0.907 & 0.987 & **0.998** \\  
**NewRFS ** & Swin-Large & 0.095 & 0.045 & 0.334 & 0.041 & 0.922 & **0.992** & **0.998** \\ Bin-Former  & Swin-Large & 0.094 & – & 0.330 & 0.040 & 0.925 & 0.989 & 0.997 \\ PixelFormer  & Swin-Large & 0.090 & – & 0.322 & 0.039 & 0.929 & 0.991 & **0.998** \\ IEBins  & Swin-Large & 0.087 & 0.040 & 0.314 & 0.038 & 0.936 & **0.992** & **0.998** \\ MG-Depth  & Swin-Large & 0.087 & – & 0.311 & – & 0.933 & – & – \\ NDDepth  & Swin-Large & 0.087 & 0041 & 0.311 & 0.038 & 0.936 & 0.991 & **0.998** \\ VA-DepthNet  & Swin-Large & 0.086 & **0.039** & **0.304** & **0.037** & 0.937 & **0.992** & **0.998** \\ 
**Ours** & Swin-Large & **0.085** & **0.039** & **0.304** & **0.037** & **0.940** & **0.992** & **0.998** \\  

Table 1: **Quantitative depth comparison on NYU-Depth-V2 dataset.** The maximum depth is capped at 10 meters. R-50 and E-B5 represent for ResNet-50  and EfficientNet-B5 , respectively. ’-’ means not applicable. The best result is in **bold**, and the second is underlined.

Figure 4: **Qualitative depth comparison on the NYU-Depth-V2 dataset.** The white boxes highlight the regions where our method achieves more accurate predictions.

captured with a Microsoft Kinect sensor. The settings span various indoor scenes such as bedrooms, offices, and classrooms. The images in this dataset are presented at a resolution of \(640 480\). We follow the data split as outlined in BTS , featuring 24231 training images and 654 test images. **(2) TOFDC** is collected using a mobile phone paired with a lightweight Time-of-Flight (ToF) camera, capturing a wide array of subjects like flowers, human figures, and toys under different scenes and lighting conditions. The dataset is divided into 10,000 training samples and 560 testing samples, with images at a resolution of \(512 384\). **(3) KITTI** is a well-known outdoor dataset that features RGB images coupled with sparse depth maps obtained from a laser scanner mounted on a car. The images in this dataset have a resolution of \(1216 352\). We utilize both the Eigen split  and the official split for our analysis. The Eigen split comprises 23158 training images and 697 test images, while the official split includes 42949 training images and 500 test images.

MetricsConsistent with prior works , we utilize a selection of well-established metrics to provide a comprehensive evaluation. The key metrics include: relative absolute error (Abs Rel), relative squared error (Sq Rel), root mean squared error (RMSE), absolute logarithmic error (\(_{10}\)), root mean squared logarithmic error (RMSE log), inverse root mean squared error (iRMSE) and threshold accuracy (\(<1.25\), \(<1.25^{2}\), and \(<1.25^{3}\)). Please refer to the appendix for details.

### Implementation Detail

The DCDepth is implemented using Pytorch library , and is trained with a batch size of 8 on four NVIDIA RTX-4090 GPUs with data-distributed parallel computing. Our method is trained on NYU-Depth-V2 dataset for 20 epochs, TOFDC dataset for 25 epochs, KITTI eigen split for 20 epochs and KITTI official split for 12 epochs. The optimization objective of our method is a combination of the scale-invariant log loss \(L_{d}\), the frequency regularization \(L_{f}\) and the smoothness regularization \(L_{s}\), weighted by two scalar weights \(\) and \(\):

\[L=L_{d}+ L_{f}+ L_{s}. \]

For the NYU-Depth-V2 and TOFDC datasets, these two weights are set to \(2 10^{-3}\) and 0.0, respectively, while for the KITTI dataset, both weights are set to \(5 10^{-3}\). We opt for the Adam optimizer  and leverage the OneCycle learning rate scheduler . The learning rate schedule entails an initial increase from \(2 10^{-5}\) to \(10^{-4}\) during the first 2 epochs, followed by a subsequent decrease to \(5 10^{-6}\) using a cosine annealing strategy. To enhance generalization and mitigate overfitting, we integrate various data augmentation techniques into the training pipeline, including random horizontal flips, random rotations, random color jitter, and random image filtering. For feature extraction from images, we incorporate a Swin-Transformer architecture  pretrained on the ImageNet dataset  as the image encoder. To reduce the iteration steps necessitated for spectrum

  Method & Backbone & Abs Rel \(\) & Sq Rel \(\) & RMSE \(\) & RMSE log \(\) & \(<1.25^{+}\) & \(<1.25^{+}\) & \(<1.25^{+}\) \\   BTS  & DenseNet-161 & 0.407 & 0.082 & 0.998 & 0.567 & 0.985 & 0.998 & **1.000** \\ AdaBins  & E8-E9-unit-V2 & 0.279 & 0.044 & 0.729 & 0.462 & 0.990 & 0.998 & **1.000** \\  NewCPCS  & Swin-Large & 0.533 & 0.244 & 1.064 & 0.792 & 0.956 & 0.976 & 0.988 \\ PixelFormer  & Swin-Large & 0.534 & 0.230 & 1.076 & 0.782 & 0.957 & 0.979 & 0.991 \\ VA-DepthNet  & Swin-Large & 0.234 & 0.029 & 0.619 & 0.373 & **0.996** & **0.999** & **1.000** \\ IEEBins  & Swin-Large & 0.528 & 0.238 & 0.999 & 0.790 & 0.956 & 0.976 & 0.988 \\ 
**Ours** & Swin-Large & **0.188** & **0.027** & **0.565** & **0.352** & 0.995 & **0.999** & **1.000** \\  

Table 2: **Quantitative depth comparison on TOFDC dataset.** The maximum depth is capped at 5 meters. The first four error metrics are multiplied by 10 for presentation.

Figure 5: **Qualitative depth comparison on the TOFDC dataset.**

prediction, we further merge the frequency subgroups with indices \(\{6,7\}\) and \(\{8,,14\}\), leading to 9 iterative steps in total to generate the final depth predictions.

### Comparison with the State-of-the-Art

Nyu-Depth-V2We benchmark our method against current State-of-The-Art (SoTA) approaches on the indoor NYU-Depth-V2 dataset, with quantitative results presented in Tab. 1. Despite vision transformers elevating the precision of depth estimation on this dataset, our method has surpassed existing SoTA approaches, particularly in the _Abs Rel_ and \(<1.25\) metrics. Qualitative comparisons, illustrated in Fig. 4, reveal the adeptness of our method at capturing fine-grained geometries and producing smoother depth estimations in planar areas. Regions where our method outperforms are highlighted with white boxes, emphasizing its superior depth estimation accuracy.

TofdcThe TOFDC dataset is characterized by its dense ground truth depth data. By utilizing this dataset, we demonstrate the enhanced capability of our method to effectively harness the dense ground truth, thereby achieving more accurate depth estimations compared to existing SoTAs. We present the quantitative results in Tab. 2, where our method demonstrates superior performance over existing SoTAs across a majority of the evaluated metrics. Specifically, our method achieves a significant improvement on the _Abs Rel_ and _RMSE_ metrics compared to VA-DepthNet, with enhancements of 19.7% and 8.7%, respectively. Fig. 5 provides qualitative comparisons, illustrating that our method not only produces more accurate depth estimations but also more effectively delineates the object from the background, leading to more coherent depth estimations.

KitttiWe further evaluate our method on the outdoor dataset, KITTI, which has sparse depth ground truth collected with LiDAR. This sparsity presents a contrast to the denser depth information available in the NYU and TOFDC datasets, resulting in less robust supervision for learning frequency coefficients. Despite this challenge, our method demonstrates its robustness by achieving SoTA performance, which is attributed to the utilization of plenty training data coupled with our proposed regularization constraints. The quantitative analysis, as detailed in Tab. 3, demonstrates the superior performance of our method. Qualitative evaluations, depicted in Fig. 6, further substantiate the superiority of our method. The quantitative results on KITTI official split are reported in Tab. 4. The pretrained weights from Semantic-SAM  are employed to initialize the encoder. Our method surpasses the compared approaches on the majority of metrics, particularly in the iRMSE metric, underscoring the robustness and effectiveness of our approach.

  Method & Backbone & Abs Rel \(\) & Sq Rel \(\) & RMSE \(\) & RMSE log \(\) & \(<1.25\)\(\) & \(<1.25\)\(\) & \(<1.25\)\(\) \\   DORN  & ResNet-101 & 0.072 & 0.307 & 2.727 & 0.120 & 0.932 & 0.984 & 0.994 \\ VNL  & ResNet-101 & 0.072 & – & 3.258 & 0.117 & 0.938 & 0.990 & 0.998 \\ BTS  & DenseNet-161 & 0.060 & 0.249 & 2.798 & 0.996 & 0.955 & 0.993 & 0.998 \\ TransDepth  & R-50+VIP-I8/16 & 0.064 & 0.252 & 2.755 & 0.098 & 0.956 & 0.994 & **0.999** \\ AdaBin  & E-B5+mini-VIT & 0.058 & 0.190 & 2.360 & 0.088 & 0.964 & 0.995 & **0.999** \\ P3Depth  & ResNet-101 & 0.071 & 0.270 & 2.842 & 0.103 & 0.953 & 0.993 & 0.998 \\  NeWCRES  & Swin-Large & 0.052 & 0.155 & 2.129 & 0.079 & 0.974 & **0.997** & **0.999** \\ BinFormer  & Swin-Large & 0.052 & 0.151 & 2.096 & 0.079 & 0.974 & **0.997** & **0.999** \\ PixelFormer  & Swin-Large & 0.051 & 0.149 & 2.081 & 0.077 & 0.976 & **0.997** & **0.999** \\ VA-DepthNet  & Swin-Large & **0.050** & 0.148 & 2.093 & **0.076** & **0.977** & **0.997** & **0.999** \\ iDisc  & Swin-Large & **0.050** & **0.145** & 2.067 & 0.077 & **0.977** & **0.997** & **0.999** \\ 
**Ours** & Swin-Large & 0.051 & **0.145** & **2.044** & **0.076** & **0.977** & **0.997** & **0.999** \\  

Table 3: **Quantitative depth comparison on the Eigen split of KITTI dataset.** The maximum depth value is capped at 80 meters.

Figure 6: **Qualitative depth comparison on the Eigen split of KITTI dataset.**

Parameter efficiencyWe compare the parameter efficiency of our method with current SoTAs on the NYU-Depth-V2 dataset, with the input resolution set to \(640 480\). The quantitative results, presented in Tab. 5, reveal that our method exhibits the fewest training parameters while simultaneously achieving the best performance. For instance, our approach demonstrates a 9.0% improvement in the _RMSE_ metric, while utilizing 4.1% fewer parameters than NeWCRFS.

### Ablation Study

We conduct comprehensive ablation studies to demonstrate the efficacy of the proposed PPH and PFF modules, and analyze the impact of the iteration steps on both model performance and inference speed. All experiments presented in this section are conducted on the NYU-Depth-V2 dataset.

Effect of PPH moduleTo assess the impact of the PPH module, we build a baseline by excluding the PPH from our method. In this setup, we employ a convolutional head to project the last-layer features to the output dimension. The final depth prediction is obtained through either bilinear and PixelShuffle  upsampling or inverse DCT that converts the predicted frequency coefficients back to the spatial domain. Additionally, we introduce the adaptive bins  as an alternative competitor. Quantitative experimental results are reported in Tab. 6. Among the three approaches outputting in the spatial domain, the PixelShuffle-based approach performs the best. When predicting depth in the frequency domain, performance further improves, demonstrating the superiority of frequency-domain depth prediction. Lastly, our progressive prediction scheme significantly outperforms the compared approaches by a large margin, underscoring the efficacy of the PPH module.

Effect of PFF moduleTo evaluate the impact of the PFF module, we establish a baseline by excluding the PFF component from our method. We first introduce a convolutional layer and a PPM  module to process the image feature at the last scale. Then, to validate the proposed DCT-based downsampling strategy, we replace it with bilinear and PixelUnshuffle  downsampling. The quantitative experimental results are reported in Tab. 7. The first two approaches, which only process the last-scale feature, perform worse than the competitors with multi-scale feature aggregation. This demonstrates the necessity of multi-scale feature aggregation for depth prediction. Furthermore, our method, employing the DCT-based downsampling strategy, achieves the best performance, showcasing the effectiveness of our proposed DCT-based strategy for feature downsampling.

Effect of iterative stepsWe analyze the impact of iterative steps on both prediction accuracy and inference speed. The results are reported in Tab. 5 and illustrated in Fig. 7. In summary, we observe that both prediction accuracy and inference time increase as the number of iterations grows. Leveraging the energy compaction property of the DCT, we strike a balance between accuracy and speed by selectively discarding predictions for high-frequency components. This strategic approach allows us to effectively reduce the number of iterative steps.

    &  NeWCRFS \\  \\  &  MG-Depth \\  \\  &  IEBins \\  \\  &  VA-DepthNet \\  \\  & 
  \\   Param (M) \(\) & 270 & 296 & 273 & 262 & & & & **259** & & \\ Speed (FPS) \(\) & **37.95** & 24.24 & 21.51 & 15.68 & 31.55 & 28.72 & 26.03 & 24.07 & 14.24 \\  RMSE \(\) & 0.334 & 0.311 & 0.314 & **0.304** & 0.310 & 0.307 & 0.306 & 0.305 & **0.304** \\ \(<1.25\

## 5 Limitation and Broader Impact

Our method employs the differentiable inverse DCT to transform the predicted spectrum back to the spatial domain. By minimizing the difference between the spatial-domain estimation and the valid ground truth, our model can be trained end-to-end. However, the sparsity of the ground truth may lead to inefficient supervision of the frequency estimation. While we have proposed two regularization terms to prevent our model from being incorrectly optimized, we observe that our method is more effective with dense supervision. Exploring more effective training strategies when only sparse depth ground truth is available will be an important research direction for our future work.

Monocular depth estimation is a pivotal technique for interpreting 3D scenes from 2D images and has widespread applications in autonomous driving, robotics, and 3D modeling, among others. Given the extensive applications of this task, our method is poised to positively impact these fields by advancing their capabilities. Considering the fundamental nature of monocular depth estimation, our work is not anticipated to have a significant negative societal impact.

## 6 Conclusion

In this paper, we introduce DCDepth, a novel framework for the MDE task. Departing from existing methods, our method progressively estimates patch-wise depth in the frequency domain and then recovers spatial-domain depth via inverse DCT. This formulation inherently models local depth correlations and frames the estimation process as a global-to-local scheme, achieving more accurate depth estimation. Leveraging the energy compaction property of DCT, our method strikes an effective balance between accuracy and inference speed, making it well-suited for practical applications.

## 7 Acknowledgment

We would like to thank the reviewers and the chairs for their suggestions and efforts. This work was partially supported by the National Natural Science Foundation of China under Grant 62361166670 and 62072242, the Fundamental Research Funds for the Central Universities under Grant 070-63233084, the Young Scientists Fund of the National Natural Science Foundation of China under Grant 62206134 and the Tianjin Key Laboratory of Visual Computing and Intelligent Perception. The PCA Lab is associated with the Key Lab of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, and Jiangsu Key Lab of Image and Video Understanding for Social Security, School of Computer Science and Engineering, Nanjing University of Sci & Tech.

  Method & Output Domain & Abs Rel \(\) & Sq Rel \(\) & RMSE \(\) & \(<1.25\) & \(<1.25\) & \(<1.25\) \(\) \\   Baseline + Conv + Bilinear & Spatial-Domain & 0.090 & 0.042 & 0.319 & 0.929 & 0.991 & **0.998** \\ Baseline + Alpha+ Bias + Bilinear & Spatial-Domain & 0.088 & 0.042 & 0.319 & 0.932 & 0.991 & **0.998** \\ Baseline + Conv + PixelShuffle & Spatial-Domain & 0.088 & 0.041 & 0.318 & 0.933 & **0.992** & **0.998** \\ Baseline + Conv + inv DCT & Frequency-Domain & 0.088 & 0.041 & 0.315 & 0.932 & **0.992** & **0.998** \\ 
**Baseline + PPH** & Frequency-Domain & **0.085** & **0.039** & **0.304** & **0.940** & **0.992** & **0.998** \\  

Table 6: **Ablation study on the PPH module. The baseline is built by removing the PPH module. _Conv_ denotes linear projection with a convolutional layer. _AdaBins_ refers to the adaptive bins . All methods output at \(}{{8}}\) scale, and Bilinear and PixelShuffle  are used to upsample the prediction.**

Figure 7: **Accuracy vs. inference speed. The width of each bubble corresponds to the processing time.**

  Method & Abs Rel \(\) & RMSE \(\) & \(<1.25\) \\   Baseline + Conv & 0.086 & 0.309 & 0.936 \\ Baseline + PPM & 0.086 & 0.306 & 0.939 \\  Baseline + PFF (Bilinear) & **0.085** & 0.305 & **0.940** \\ Baseline + PFF (PixelUnshuffle) & **0.085** & 0.306 & **0.940** \\ 
**Ours** & **0.085** & **0.304** & **0.940** \\  

Table 7: **Ablation study on the PFF module. The baseline is built by removing the PFF module. We evaluate the proposed DCT-based downsampling strategy by replacing it with bilinear and PixelUnshuffle  downsampling.**