# Are Large-scale Soft Labels Necessary for Large-scale Dataset Distillation?

Lingao Xiao\({}^{1,3}\) and Yang He\({}^{1,2,3}\)

\({}^{1}\)CFAR, Agency for Science, Technology and Research, Singapore

\({}^{2}\)IHPC, Agency for Science, Technology and Research, Singapore

\({}^{3}\)National University of Singapore

xiao_lingao@u.nus.edu, he_yang@cfar.a-star.edu.sg

Corresponding Author

###### Abstract

In ImageNet-condensation, the storage for auxiliary soft labels exceeds that of the condensed dataset by over 30 times. However, _are large-scale soft labels necessary for large-scale dataset distillation_? In this paper, we first discover that the high within-class similarity in condensed datasets necessitates the use of large-scale soft labels. This high within-class similarity can be attributed to the fact that previous methods use samples from different classes to construct a single batch for batch normalization (BN) matching. To reduce the within-class similarity, we introduce class-wise supervision during the image synthesizing process by batching the samples within classes, instead of across classes. As a result, we can increase within-class diversity and reduce the size of required soft labels. A key benefit of improved image diversity is that soft label compression can be achieved through simple random pruning, eliminating the need for complex rule-based strategies. Experiments validate our discoveries. For example, when condensing ImageNet-1K to 200 images per class, our approach compresses the required soft labels from 113 GB to 2.8 GB (40\(\) compression) with a 2.6% performance gain. Code is available at: [https://github.com/he-y/soft-label-pruning-for-dataset-distillation](https://github.com/he-y/soft-label-pruning-for-dataset-distillation).

## 1 Introduction

We are pacing into the era of ImageNet-level condensation, and the previous works  fail in scaling up to large-scale datasets due to extensive memory constraint. Until recently, Yin _et al._ decouple the traditional distillation scheme into three phases. First, a teacher model is pretrained with full datasets (squeeze phase). Second, images are synthesized by matching the Batch Normalization (BN) statistics from the teacher and student models (recover phase). Third, auxiliary data such as soft labels are pre-generated from different image augmentations to create abundant supervision for post-training (relabel phase).

However, the auxiliary data are **30\(\)** larger than the distilled data in ImageNet-1K. To

Figure 1: The relationship between performance and total storage of auxiliary information needed. Our method achieves SOTA performance with **fewer soft labels** than images.

attain correct and effective supervision, the exact augmentations and soft labels of every training epoch are stored [6; 7; 8; 9; 10]. The required soft label storage is the colored circles in Fig. 1.

In this paper, we consider _whether large-scale soft labels are necessary_, and _what causes the excessive requirement of these labels_? To answer these questions, we provide an analysis of the distilled images using SRe\({}^{2}\)L , and we find that within-class diversity is at stake as shown in Fig. 2. To be more precise, we analyze the similarity using Feature Cosine Similarity and Maximum Mean Discrepancy in Sec. 3.2. The high similarity of images within the same class requires extensive data augmentation to provide different supervision.

To address this issue, we propose **Label Pruning for Large-scale Distillation (LPLD)**. Specifically, we modified the algorithms by batching images within the same class, leveraging the fact that different classes are naturally independent. Furthermore, we introduce class-wise supervision to align our changes. In addition, we have explored different label pruning metrics and found that simple random pruning was performed on par with carefully selected labels. To further increase diversity, we improve the label pool by introducing randomness in a finer granularity (i.e., batch-level). Our method effectively distills the images while requiring less label storage compared to image storage, as shown in Fig. 1.

The key contributions of this work are: (1) To the best of our knowledge, it is the first work to introduce label pruning to large-scale dataset distillation. (2) We discover that high within-class diversity necessitates large-scale soft labels. (3) We re-batch images and introduce class-wise supervision to improve data diversity, allowing random label pruning to be effective with an improved label pool. (4) Our LPLD method achieves SOTA performance using a lot less label storage, and it is validated with extensive experiments on various networks (e.g., ResNet, EfficientNet, MobileNet, and Swin-V2) and datasets (e.g., Tiny-ImageNet, ImageNet-1K, and ImageNet-21K).

## 2 Related Works

**Dataset Distillation.** DD  first introduces dataset distillation, which aims to learn a synthetic dataset that is equally effective but much smaller in size. The matching objectives include performance matching [1; 11; 12; 13; 14], gradient matching [4; 15; 16; 17], distribution or feature matching [5; 2; 18], trajectory matching [3; 19; 20], representative matching [21; 22], loss-curvature matching , and Batch-Norm matching[6; 7; 9; 10].

**Dataset Distillation of Large-Scale Datasets.** Large-scale datasets scale up in terms of image size and the number of total images, incurring affordable memory consumption for most of the well-designed matching objectives targeted for small datasets. MTT  is able to condense Tiny-ImageNet (ImageNet-1K subsets with images downsampled to \(64 64\) and 200 classes). IDC  conducts experiments on ImageNet-10, which contains an image size of \(224 224\) but has only 10 classes. TESLA  manages to condense the full ImageNet-1K dataset by exactly computing the unrolled

Figure 2: Visual comparison between SRe\({}^{2}\)L and the proposed method. The classes are hammer shark (top), pineapple (middle), and pomerangate (bottom). Our method is more visually diverse.

gradient with constant memory or complexity. SRe2L  decouples the bilevel optimization into three phases: 1) squeezing, 2) recovering, and 3) relabeling. The proposed framework surpasses TESLA  by a noticeable margin. CDA  improves the recovering phase by introducing curriculum learning. RDED  replaces the recovering phase with an optimization-free approach by concatenating selected image patches. SC-DD  uses self-supervised models as recovery models. Existing methods [7; 8; 10] place high emphasis on improving the recovering phase; however, the problem of the relabeling phase is overlooked: _a large amount of storage is required for the relabeling phase._

**Label Compression.** The problem of excessive storage seems to be fixed if the teacher model generates soft labels immediately used by the student model on the fly. However, when considering the actual use case of distilled datasets (i.e., Neural Architecture Search), using pre-generated labels enjoys speeding up training and reduced memory cost. More importantly, the generated labels can be repeatedly used. FKD  employs label quantization to store only the top-\(k\) logits. In contrast, our method retains full logits, offering an orthogonal approach to quantization. A comparison to FKD is provided in Appendix D.3. Unlike FerKD , which removes some unreliable soft labels, our strategy targets higher pruning ratios.

**Comparison with G-VBSM .** In one recent work, G-VBSM also mentioned re-batching the images within classes; however, the motivation is that having a single image in a class is insufficient . It re-designed the loss by introducing a model pool, matching additional statistics from convolutional layers, and updating the statistics of synthetic images using exponential moving averages (EMA). Additionally, an ensemble of models is involved in both the data synthesis and relabel phase, requiring a total of \(N\) forward propagation from \(N\) different models, where \(N=4\) is used for ImageNet-1K experiments. On the other hand, we aim to improve the within-class data diversity for **reducing soft label storage**. Furthermore, to account for the re-batching operation, we introduce class-wise supervision while all G-VBSM statistics remain global.

## 3 Method

### Preliminaries

The conventional Batch Normalization (BN) transformation is defined as follows:

\[y=(-}{+}})+, \]

where \(\) and \(\) are parameters learned during training, \(\) and \(^{2}\) are the mean and variance of the input features, and \(\) is a small constant to prevent division by zero. Additionally, the running mean and running variance are maintained during network training and subsequently utilized as \(\) (mean) and \(^{2}\) (variance) during the inference phase, given that the true mean and variance of the test data are not available.

The matching object of SRe2L  follows DeepInversion , which optimizes synthetic datasets by matching the models' layer-wise BN statistics:

\[_{}(})& =_{l}\|_{l}(})-(_{ l})\|_{2}+_{l}\|_{l}^{2}( })-(_{l}^{2}) \|_{2}\\ &_{l}\|_{l}(})-_{l }^{}\|_{2}+_{l}\|_{l}^{2}(}) -_{l}^{}\|_{2}, \]

where the BN's running mean \(_{l}^{}\) and running variance \(_{l}^{}\) are used to approximate the expected mean \((_{l})\) and expected variance \((_{l}^{2})\) of the original dataset \(\), respectively. The BN loss matches BN for layers \(l\), and \(_{l}(})\) and \(_{l}^{2}(})\) are the mean and variance of the synthetic images \(}\).

The BN loss term is used as a regularization term applied to the classification loss \(_{}\). Therefore, the matching objective is:

\[*{arg\,min}_{}}\ _{}(}),)}_{_{ }}+_{}(} ), \]

where \(_{}\) is the model pretrained on the original dataset \(\). The symbol \(\) is a small factor controlling the regularization strength of BN loss.

### Diversity Analysis on Synthetic Dataset

#### 3.2.1 Similarity within Synthetic Dataset: Feature Cosine Similarity

A critical aspect of image diversity is how similar or different the images are within the same class. To quantify this, we utilize the feature cosine similarity measure defined above. Lower cosine similarity values between images within the same class indicate greater diversity, as the images are less similar to one another. This relationship is formally stated as follows:

**Proposition 1**.: _The lower feature cosine similarity of images indicates higher diversity because the images are less similar to one another._

Feature Cosine similarity can be formally put as:

\[:=}_{}) f( }^{}_{})}{\|f(}_{})\|\|f( }^{}_{})\|}=^{n}f(}_{,})\ f(}^{}_{,i})}{^{n}f( }_{,})^{2}^{n}f(} ^{}_{,})^{2}}}}, \]

where \(}_{}\) and \(}^{}_{}\) are two images from the same class \(c\), \(f()\) are the features extracted from a pretrained model, and \(n\) is the feature dimension.

#### 3.2.2 Similarity between Synthetic and Original Dataset: Maximum Mean Discrepancy

The similarity between images is not the only determinant of diversity since images can be dissimilar to each other yet not representative of the original dataset. Therefore, to further validate the diversity of our synthetic dataset, we consider an additional metric: the Maximum Mean Discrepancy (MMD) between synthetic datasets and original datasets. This measure helps evaluate how well the synthetic data represents the original data distribution. The following proposition clarifies the relationship between MMD and dataset diversity:

**Proposition 2**.: _A lower MMD suggests that the synthetic dataset captures a broader range of features similar to the original dataset, indicating greater diversity._

The empirical approximation of MMD can be formally defined as ,

\[^{2}(_{},_{})=}_{,}+}_{,}-2}_{,} \]

where \(}_{,Y}=_{i=1}^{|X|}_{j= 1}^{|Y|}(f(x_{i}),f(y_{j}))\) with \(\{x_{i}\}_{i-1}^{|X|} X,\{y_{i}\}_{i=1}^{|Y|} Y\). \(\) and \(\) denote real and synthetic datasets, respectively; \(\) is the reproducing kernel (e.g., Gaussian kernel); \(\) is the feature (embedding) distribution, and \(f()\) is the feature representation extracted by model \(\), where \(f()_{},f()_{}\).

### Label Pruning for Large-scale Distillation (LPLD)

#### 3.3.1 Diverse Sample Generation via Class-wise Supervision

The previous objective function follows Eq. 3; it uses a subset of classes \(_{c}\) to match the BN statistics of the entire dataset, and images in the same class are independently generated, causing an low image diversity within classes. However, inspired by He _et al_., images in the same class should work collaboratively, and images that are optimized individually (see Baseline B in work ) do not lead to the optimal performance when IPC (Images Per Class) gets larger.

  IPC & SRe\({}^{2}\)L & CDA & Ours & Full Dataset \\ 
50 & \(0.841 0.023\) & \(0.816 0.026\) & \(0.796 0.029\) & \\
100 & \(0.840 0.016\) & \(0.814 0.019\) & \(0.794 0.021\) & \(0.695 0.045\) \\
200 & \(0.839 0.011\) & \(0.813 0.013\) & \(0.793 0.015\) & \\  

Table 1: The cosine similarity between image features. The similarities are the average of 1K class on the synthetic ImageNet-1K dataset. Features are extracted using pretrained ResNet-18.

Figure 3: MMD visualization.

**Step 1: Re-batching Images within Class.** Subsequently, to obtain a collaborative effect among different images of the same class, we sample images from the same class and provide the images with class-wise supervision [4; 24]. Fig. 4 illustrates the changes.

**Step 2: Introducing Class-wise Supervision.** However, the running mean and variance approximate the original dataset's expected mean and variance in a global aspect. The matching objective becomes sub-optimal in class-wise matching situation. To this end, we propose to track BN statistics for each class separately. Since we only track the running mean and variance, the extra storage is marginal even when up to 1K classes in ImageNet-1K (see Appendix E.2 and E.4).

**Step 3: Class-wise Objective Function.** The new class-wise objective function is modified from Eq. 3, which has two loss functions. First, we compute the classification loss (i.e., the Cross-Entropy Loss) with BN layers using global statistics to ensure effective supervision. Second, we compute BN loss by matching class-wise BN statistics. The modified parts are highlighted in blue color, and the objective function is formally put as,

\[_{}_{c}}& (^{N}_{c,i}( (_{}(}_{ c,i}-_{}^{}}{_{}^{ }+}}))_{c})}.\\ & 14.226378pt+(\| _{l}(}_{c})-_{l,c}^{}\|_{2}+ \|_{l}^{2}(}_{c})-_{l,c}^{} \|_{2})}_{} \]

We want to emphasize that even though we are adjusting the BN loss with class-wise statistics, the global statistics of the dataset are still taken into account. The output logits for calculating CE loss are produced using global statistics. This is because altering \(\) and \(\) without fine-tuning \(\) and \(\) could lead to a decline in model performance, resulting in less effective supervision.

**Theoretical Number of Updates for Stable Class-wise BN Statistics.** Traditional BN layers do not compute class-wise statistics; therefore, we need to either keep track of the class-wise statistics while training a model from scratch or compute these statistics using a pretrained model. We prefer the latter as the former requires extensive computing resources. To understand how many BN statistics updates are needed, we can first look at the update rules of BN running statistics for a class \(c\):

\[_{l,c}^{}& (1-)_{l,c}^{}+_{l}(_{c} ),\\ _{l,c}^{}&(1-) _{l,c}^{}+_{l}^{2}(_{c}),  \]

Figure 4: Illustration of existing methods (left, grey) and the proposed method (right, blue). Existing methods (i.e., \(^{2}\)L, CDA) independently generate along the IPC (Image-Per-Class) dimension, causing a high similarity between images of the same class. The proposed method allows images of the same class to collaborate, leaving different classes naturally independent. In addition, synthetic images are updated under class-wise supervision. The classification loss is omitted for simplicity.

where \(\) is the momentum. Since the momentum factor for the current batch statistics is usually set to a small value (i.e., \(=0.1\)), we can theoretically compute existing running statistics that can be statistically significant after how many updates, assuming all other factors are fixed.

Since the running statistics are computed per class, we provide the theoretical number of updates required to stabilize all class statistics (see Appendix A for the proof):

\[n()}{^{2} (q_{c})}}_{},)}{_{}(q_{c})}}_{} ). \]

where \(n\) is the number updates needed, \(q_{c}\) is the probability that class \(c\) appears in a batch, \(T\) is a probability threshold, \(\) is the momentum parameter in Batch Normalization, \(\) is the acceptable relative deviation (where \(0 1\)), \(C\) is some constant, and \(\) is the desired convergence tolerance for the BN statistics. How Eq. 8 guides our experiment design is detailed in Appendix E.3.

#### 3.3.2 Random Label Pruning with Improved Label Pool

**Excelling in Both Similarity Measures.** By adopting the changes provided in Sec. 3.3.1, our synthetic dataset is more diverse and representative than the existing methods. First, our dataset exhibits smaller feature cosine similarity within classes compared to datasets produced by existing methods, as shown in Table 1. This indicates that our synthetic images are less similar to each other and, thus, more diverse. Second, our dataset exhibits a significantly lower MMD shown in Fig 3 compared to datasets produced by existing methods. This suggests that our synthetic dataset better captures the feature distribution of the original dataset. After obtaining a diverse dataset, the next move is to address superfluous soft labels.

**Random Label Pruning.** Different from dataset pruning metrics, which many wield training dynamics , label pruning is inherently different since the labels in different epochs are independently generated or evaluated. Subsequently, these methods do not directly apply, and we modify these metrics to determine which epochs contain the most useful augmentations and soft labels. Through empirical study, we find that using soft labels carefully pruned from different metrics is **no better** than simple random pruning. As a result, we can discard complex rule-based pruning metrics, attaining both simplicity and efficiency. After obtaining the soft label pool, we have to decide which labels will be used. Following the previous random pruning scheme, we randomly sample the labels for model training in order to ensure diversity and avoid any prior knowledge.

**Improved Label Pool.** Considering that random selection may be the most efficient choice, we rethink the diversity of the label pool, as labels at the epoch-level are not the finest elements.

Figure 5: Illustration of two random processes in label pruning with improved label pool. First, we need a smaller soft label pool due to the storage budget. We can conduct pruning at two levels: (1) epoch-level and (2) batch-level. Batch-level pruning can provide a more diverse label pool since augmentations (e.g., Mixup or CutMix) are different across batches. The illustrated pruning ratio is 25%; the crossed-out labels denote the pruned labels, and the remaining form the label pool. Second, we randomly sample soft labels for model training.

The augmentations such as CutMix and Mixup are performed at the batch level, where the same augmentations are applied to images within the same batch and are different across batches. Therefore, we improve the label pool by allowing batches in different epochs to form a new epoch. The improved label pool breaks the fixed batch orders and the fixed combination of augmentations within an epoch, allowing a more diverse training process while reusing the labels. Our label pruning method is illustrated in Fig. 5.

## 4 Experiments

### Experiment Settings

Dataset details can be found in Appendix B and detailed settings are provided in Appendix C. Computing resources used for experiments can be found in Appendix E.5.

**Dataset.** Our experiment results are evaluated on Tiny-ImageNet , ImageNet-1K , and ImageNet-21K-P . We follow the data pre-processing procedure of SRe\({}^{2}\)L  and CDA .

**Squeeze.** We modify the pretrained model by adding class-wise BN running mean and running variance; since they are not involved in computing the BN statistics, they do not affect performance. As mentioned in Sec. 3.3.1, we compute class-wise BN statistics by training for one epoch with model parameters kept frozen.

**Recover.** We perform data synthesis following Eq. 6. The batch size for the recovery phase is the same as the IPC. Besides, we adhere to the original setting in SRe\({}^{2}\)L.

**Relabel.** We use pretrained ResNet18  for all experiments as the relabel model except otherwise stated. For Tiny-ImageNet and ImageNet-1K, we use Pytorch pretrained model. For ImageNet-21K-P, we use Timm pretrained model.

**Validate.** For validation, we adhere to the hyperparameter settings of CDA .

**Pruning Setting.** For label pruning, we exclude the last batch (usually with an incomplete batch size) of each epoch from the label pool. There are two random processes: (1) Random candidate selection from all batches. (2) Random reuse of candidate labels.

Table 2: Tiny-ImageNet label pruning results. The standard deviation is attained from three different runs. \({}^{}\) denotes the reported results.

    &  &  &  &  &  \\ ResNet-18 & SRe\({}^{2}\)L & CDA & Ours & SRe\({}^{2}\)L & CDA & Ours & SRe\({}^{2}\)L & CDA & Ours & SRe\({}^{2}\)L & CDA & Ours \\  IPC01 & 41.1\({}^{}\) & 48.7\({}^{}\) & **48.8\({}^{}\)\({}_{ 0.3}\)** & 40.3 & 45.0 & **46.7\({}^{}\)\({}_{ 0.6}\)** & 39.0 & 41.2 & **44.3\({}^{}\)\({}_{ 0.5}\)** & 34.6 & 35.8 & **40.2\({}^{}\)\({}_{ 0.3}\)** & 29.8 & 30.9 & **38.4\({}^{}\)\({}_{ 1.3}\)** \\ IPC0100 & 49.7\({}^{}\) & 53.2\({}^{}\) & **53.6\({}^{}\)\({}_{ 0.3}\)** & 48.3 & 50.7 & **52.2\({}^{}\)\({}_{ 0.2}\)** & 46.5 & 48.0 & **50.6\({}^{}\)\({}_{ 0.2}\)** & 43.0 & 44.2 & **47.6\({}^{}\)\({}_{ 0.2}\)** & 39.4 & 40.0 & **46.1\({}^{}\)\({}_{ 0.2}\)** \\   

Table 2: Tiny-ImageNet label pruning results. The standard deviation is attained from three different runs. \({}^{}\) denotes the reported results.

    &  &  &  &  &  \\ ResNet-18 & SRe\({}^{2}\)L & CDA & Ours & SRe\({}^{2}\)L & CDA & Ours & SRe\({}^{2}\)L & CDA & Ours & SRe\({}^{2}\)L & CDA & Ours \\  IPC01 & 20.1 & 33.3 & **34.6\({}_{ 0.0}\)** & 18.9 & 28.4 & **32.7\({}_{ 0.6}\)** & 16.0 & 21.9 & **28.6\({}_{ 0.4}\)** & 14.1 & 14.2 & **23.1\({}_{ 0.1}\)** & 11.4 & 13.2 & **20.2\({}_{

### Primary Result

**Tiny-ImageNet.** Table 2 presents a comparison between the label pruning outcomes on Tiny-ImageNet for our approach, SRe\({}^{2}\)L , and the subsequent work, CDA . Our method not only consistently surpasses SRe\({}^{2}\)L across identical pruning ratios but also achieves comparable results to SRe\({}^{2}\)L while using 40\(\) fewer labels. When compared to CDA, our method exhibits closely matched performance, yet it demonstrates superior accuracy preservation. For instance, at a 40\(\) label reduction, our method secures a notable 7.5% increase in accuracy over CDA, even though the improvement stands at a mere 0.1% at the 1\(\) benchmark. Table 2 provides the pruning results on ResNet50 and ResNet101. Although there are consistent improvements observed when compared to ResNet18, scaling to large networks does not necessarily bring improvements.

**ImageNet-1K.** Table 3 compares the ImageNet-1K pruning results with SOTA methods on ResNet18. Our method outperforms other SOTA methods at various pruning ratios and different IPCs. More importantly, our method consistently exceeds the unpruned version of SRe\({}^{2}\)L with 30\(\) less storage. Such a result is not impressive at first glance; however, when considering the actual storage, the storage is reduced from 29G to 0.87G. In addition, we notice the performance at 10\(\) (or 90%) pruning ratio degrades slightly, especially for large IPCs. For example, merely \(0.2\%\) performance degradation on IPC200 using ResNet18. Pruning results of larger IPCs can be found in Appendix D.2.

### Analysis

**Ablation Study.** Table 4 presents the ablation study of the proposed method. **Row 1** is the implementation of SRe\({}^{2}\)L under CDA's hyperparameter settings. **Row 2** is simply re-ordering the loops, and the performance at 1\(\) is improved; nevertheless, when considering the extreme pruning ratio (i.e., 100\(\)), it falls short of the existing method. **Row 3** computes class-wise BN running statistics in the "squeeze" phase, and these class-wise statistics are used as supervision in the "recover" phase. A steady improvement is observed. **Row 4** allows pre-generated labels to be sampled at batch level from different epochs, further boosting the performance. Refer to Appendix D.1 for an expanded version of ablation.

**Label Pruning Metrics.** From Table 4(a), we empirically find that using different metrics explained in Appendix E.1 is **no better than** random pruning. In addition, as mentioned in FerKD , calibrating the searching space by discarding a portion of easy or hard images can be beneficial. We conduct a similar experiment to perform random pruning on a calibrated label pool, and the metric for determining easy or hard images is "confidence". However, as shown in Table 4(b), no such range can consistently outperform the non-calibrated ones (last row). An interesting observation is that the label pruning law **at large pruning ratio** seems to coincide partially with data pruning, where removing hard labels becomes beneficial .

**Generalization.** Table 5(a) shows the performance under large compression rates. Smaller IPC datasets suffer more from label pruning since it requires more augmentation and soft label pairs to boost data diversity. Furthermore, label pruning results on ResNet50 are provided in Table 5(b).

Table 4: Ablation study of the proposed method. \(\) denotes using class-wise matching. \(\) denotes suing class-wise supervision. \(\) denotes using an improved label pool. (IPC50, ResNet18, ImageNet-1K).

Table 5: Comparison between different pruning metrics. Results are obtained from ImageNet-1K IPC10 and validated using ResNet-18.

Not only scaling to large networks of the same family (i.e., ResNet) but Table 6(c) also demonstrates the generalization capability of the proposed method across different network architectures. An analogous trend is evident in the context of label pruning: comparable performance is achieved with 10\(\) fewer labels. This reinforces the statement that the necessity for extensive augmentations and labels can be significantly reduced if the dataset exhibits sufficient diversity.

**Large Dataset.** ImageNet-21K-P has 10,450 classes, significantly increasing the disk storage as each soft label stores a probability of 10,450 classes. The IPC20 dataset leads to a 1.2 TB (i.e., 1285 GB) label storage, making the existing framework less practical. However, with the help of our method, it can surpass SRe2L  by a large margin despite using 40\(\) less storage. For example, we attain an 8.9% accuracy improvement on IPC20 with label storage reduced from 1285 GB to 32 GB.

**Pruning for Optimization-Free Approach.** RDED  is an optimization-free approach during the "recover" phase. However, extensive labels are still required for post-evaluation. To prune labels, consistent improvements are observed using the improved label pool, as shown in Table 8.

**Comparison with G-VBSM .** Compared to G-VBSM , which uses an ensemble of 4 models to recover and relabel, our method outperforms it at various pruning ratios with only a single model (see Table 9). Furthermore, the techniques used for G-VBSM apply to our method. By adopting label generation with ensemble and a loss function of "MSE+0.1 \(\) GT" , our method can be further improved by a large margin on IPC10 of ImageNet-1K, using ResNet18. Implementation details can be found in Appendix C.4.

**Visualization.** Fig. 2b visualizes our method on three classes. More visualizations are provided in Appendix F.

## 5 Conclusion

To answer the question _"whether large-scale soft labels are necessary for large-scale dataset distillation?"_, we conduct diversity analysis on synthetic datasets. The high within-class similarity is observed and necessitates large-scale soft labels. Our LPLD method re-batches images within classes and introduces class-wise BN supervision during the image synthesis phase to address this issue. These changes improve data diversity, so that simple random label pruning can perform on par with complex rule-based pruning metrics. Additionally, we randomly conduct pruning on an improved label pool. Finally, LPLD is validated by extensive experiments, serving a strong baseline that takes into account actual storage. Limitations and future works are provided in Appendix E.6. The ethics statement and broader impacts can be found in Appendix E.7.

Table 6: Additional ImageNet-1K label pruning results.

Table 7: Label pruning result on ImageNet-21K-P, using ResNet-18. \(\) denotes image storage. \(\) denotes label storage. \(\) denotes reported results.

Table 8: Label pruning for optimization-free method. “Ours” uses improved label pool.

Table 9: Compare with G-VBSM . “Ours+” uses ensemble and MSE+GT loss.