# End-to-end Learnable Clustering

for Intent Learning in Recommendation

 Yue Liu

Ant Group

National University of Singapore

yueliu19990731@163.com

Equal Contribution

Shihao Zhu

Ant Group

Hangzhou, China

Equal Contribution

Jun Xia

Westlake University

Hangzhou, China

Yingwei Ma

Alibaba Group

Hangzhou, China

Jian Ma

Ant Group

Hangzhou, China

Xinwang Liu

National University of Defense Technology

Changsha, China

Shengju Yu

National University of Defense Technology

Changsha, China

Kejun Zhang

Zhejiang University

Hangzhou, China

Wenliang Zhong

Ant Group

Hangzhou, China

Equal ContributionCorresponding Author

Corresponding Author

Zhejiang University

Hangzhou, China

Equal Contribution

###### Abstract

Intent learning, which aims to learn users' intents for user understanding and item recommendation, has become a hot research spot in recent years. However, existing methods suffer from complex and cumbersome alternating optimization, limiting performance and scalability. To this end, we propose a novel intent learning method termed ELCRec, by unifying behavior representation learning into an End-to-end Learnable Clustering framework, for effective and efficient Recommendation. Concretely, we encode user behavior sequences and initialize the cluster centers (latent intents) as learnable neurons. Then, we design a novel learnable clustering module to separate different cluster centers, thus decoupling users' complex intents. Meanwhile, it guides the network to learn intents from behaviors by forcing behavior embeddings close to cluster centers. This allows simultaneous optimization of recommendation and clustering via mini-batch data. Moreover, we propose intent-assisted contrastive learning by using cluster centers as self-supervision signals, further enhancing mutual promotion. Both experimental results and theoretical analyses demonstrate the superiority of ELCRec from six perspectives. Compared to the runner-up, ELCRec improves NDCG@5 by 8.9% and reduces computational costs by 22.5% on the Beauty dataset. Furthermore, due to the scalability and universal applicability, we deploy this method on the industrial recommendation system with 130 million page views and achieve promising results. The codes are available on GitHub3. A collection (papers, codes, datasets) of deep group recommendation/intent learning methods is available on GitHub4.

Introduction

Sequential recommendation (SR), which aims to recommend relevant items to users by learning patterns from users' historical behavior sequences, is a vital and challenging task in the machine learning domain. In recent years, benefiting the strong representation learning ability of deep neural networks (DNNs), DNN-based sequential recommendation methods[105; 39; 94; 129; 50; 108; 52; 67] have achieved promising recommendation performance and attracted researchers' high level of attention.

More recently, intent learning has become a hot topic in both research and industrial field of recommendation. It aims to model users' intents by learning from users' historical behaviors. For example, a user interacts with shoes, bags, and rackets in history. Thus, the user's potential intent can be inferred as playing badminton. Then, the system may recommend the intent-relevant items to the user. Following this principle, various intent learning methods [44; 14; 45; 18; 49; 53; 5; 54] have been proposed to achieve better user understanding and item recommendation.

The optimization paradigm of recent representative intent learning methods can be summarized as a generalized Expectation Maximization (EM) framework. To be specific, at the E-step, clustering algorithms are adopted to learn the latent intents from users' behavior embeddings. In addition, in the M-step, self-supervised learning methods are utilized to embed behaviors. The optimizations of these two steps are performed alternately, achieving promising performance.

However, we highlight two issues in this complex and tedious alternating optimization. (1) At the E-step, we need to apply the clustering algorithm on the whole data, limiting the model's scalability, especially in large-scale industrial scenarios, e.g., apps with billion users. (2) In the EM framework, the optimization of behavior learning and the clustering algorithm are separated, leading to sub-optimal performance and increasing the implementation difficulty.

To this end, we propose a novel intent learning model named ELCRec via integrating representation learning into an End-to-end Learnable Clustering framework, for effective and efficient Recommendation. Specifically, the user's behavior process is first embedded into the latent space. Cluster centers, recognized as users' latent intents, are initialized as learnable neural network parameters. Then, a simple yet effective learnable clustering module is proposed to decouple users' complex intents into different simple intent units by separating the cluster centers. Meanwhile, it makes the behavior embeddings close to cluster centers to guide the models to learn more accurate intents from users' behaviors. This improves the model's scalability and alleviates issue (1) by optimizing the cluster distribution on mini-batch data. Furthermore, to further enhance the mutual promotion of representation learning and clustering, we present intent-assisted contrastive learning to integrate the cluster centers as self-supervision signals for representation learning. These settings unify behavior learning and clustering optimization in an end-to-end optimizing framework, improving recommendation performance and simplifying deployment. Therefore, issue (2) has also been solved. The contributions of this paper are summarized as follows.

* We innovatively promote the existing optimization framework of intent learning by unifying behavior representation learning and clustering optimization.
* A new intent learning model termed ELCRec is proposed with a simple yet effective learnable cluster module and intent-assisted contrastive learning.
* Comprehensive experiments and theoretical analyses show the advantages of ELCRec from six aspects, including superiority, effectiveness, efficiency, sensitivity, convergence, and visualization.
* We successfully deployed it on an industrial recommendation system with 130 million page views and achieved promising results, providing various practical insights.

## 2 Related Work

We provide a brief overview of the related work for this paper. It can be divided into three parts, including sequential recommendation, intent learning, and clustering algorithms. At first, Sequential Recommendation (SR) focuses on recommending relevant items to users based on their historical behavior sequences. In addition, intent learning has emerged as a promising and practical technique in recommendation systems. It aims to capture users' latent intents to achieve better user understanding and item recommendation. Lastly, clustering algorithms play a crucial role in recommendation systems since they can identify patterns and similarities in the users or items. Due to the limitation of the pages, we introduce the detailed related methods in the Appendix 8.11.

## 3 Methodology

We present our proposed framework, ELCRec, in this section. Firstly, we provide the necessary notations and task definition. Secondly, we analyze and identify the limitations of existing intent learning. Finally, we propose our solutions to address these challenges. Before introducing our method, we first provide the intuitions and insights of designing ELCRec. Concretely, we first analyze the challenge of scaling the intent learning methods to large-scale industrial data. The existing intent learning methods always adopt the expectation and maximization framework, where E-step and M-step are conducted alternately and mutually promote each other. However, we find the EM framework is hard to scale to large-scale data since it faces two challenges. First, the clustering algorithm is performed on the full data, easily leading to the out-of-memory problem. Second, the EM paradigm limits performance since it separates the behavior learning process and the intent learning process. To solve these two problems, we aim to propose a new intent learning method for the recommendation task. For the first challenge, our initial idea is to design an online clustering method to update the clustering centers at each step. Specifically, we propose an end-to-end learnable clustering module (ELCM) to solve this problem by setting the clustering center as the learnable neural parameters and the pull-and-push cluster loss functions. In addition, for the second challenge, we aim to integrate the intent learning process into the behavior learning process and optimize them together. Benefiting from setting the cluster centers as the learnable neural parameters, we can utilize them to assist the behavior contrastive learning. Namely, we propose intent-assisted contrastive learning, which not only supports the learning process of online clustering but also unifies behavior learning and intent learning. Therefore, with the above two designs, we can solve the challenges of scaling the intent learning method to large-scale data.

### Basic Notation

In a recommendation system, \(\) denotes the user set, and \(\) denotes the item set. For each user \(u\), the historical behaviors are described by a sequence of interacted items \(S^{u}=[s^{u}_{1},s^{u}_{2},...,s^{u}_{t},...,s^{u}_{|S^{u}|}]\). \(S^{u}\) is sorted by time. \(|S^{u}|\) denotes the interacted items number of user \(u\). \(s^{u}_{t}\) denotes the item which is interacted with user \(u\) at \(t\) step. In practice, during sequence encoding, the historical behavior sequences are limited with a maximum length \(T\)[34; 39; 18]. The sequences are truncated and remain the most recent \(T\) interacted items if the length is greater than \(T\). Besides, the shorter sequences are filled with "padding" items on the left until the length is \(T\). Due to the limitation of the pages, we list the basic notations in Table 5 of the Appendix 8.2.

### Task Definition

Given the user set \(\) and the item set \(\), the recommendation system aims to precisely model the user interactions and recommend items to users. Take user \(u\) for an example, the sequence encoder firstly encodes the user's historical behaviors \(S^{u}\) to the latent embedding \(^{u}\). Then, based on the historical behavior embedding, the target of the recommendation task is to predict the next item that is most likely interacted with by user \(u\) at \(|S^{u}|+1\) step.

### Problem Analyses

Among the techniques in recommendation, intent learning has become an effective technique for understanding users. We summarize the optimization procedure of the intent learning as the Expectation Maximization (EM) framework. It contains two steps including E-step and M-step. These two steps are conducted alternately, mutually promoting each other. However, we find two issues of the existing optimization framework as follows.

1. In the process of E-step, it needs to perform a clustering algorithm on the full data, easily leading to out-of-memory or long-running time problems. It restricts the scalability of the model on large-scale industrial data.

2. The alternative optimization approach within the EM framework separates the learning process for behaviors and intents, leading to sub-optimal performance and increased implementation complexity. Also, it limits the training and inference of real-time data. That is, when users' behaviors and intents change over time, there is a long lag in the training and inference process.

Therefore, we aim to develop a new optimization framework for intent learning to solve issues (1) and issues (2). For issue (1), a new learnable online clustering method is the key solution. For the issue (2), we aim to break the alternative optimization in the EM framework.

### Proposed Method

To this end, we present a new intent learning method termed \(}\) by unifying sequence representation learning into an End-to-end \(}\)\(}\), for \(}\). It contains three parts, including behavior encoding, end-to-end learnable cluster module (ELCM), and intent-assisted contrastive learning (ICL).

#### 3.4.1 Behavior Encoding

In this process, we aim to encode the users' behavior sequences. Concretely, given the user set \(\), the item set \(\), and the users' historical behavior sequence set \(\{S^{u}\}_{u=1}^{}\), the behavior encoder \(\) embeds the behavior sequences of each user \(u\) into the latent space as follows.

\[^{u}=(S^{u}), \]

where \(^{u}^{|S^{u}| d^{}}\) denotes the behavior sequence embedding of user \(u\), \(d^{}\) is the dimension number of latent features, and \(|S^{u}|\) denotes the length of behavior sequence of user \(u\). Note that the behavior sequence lengths of different users are different. Therefore, all user behavior sequences are pre-processed to the sequences with the same length \(T\) by padding or truncating. The encoder \(\) is designed as a Transformer-based  architecture. Subsequently, to summarize the behaviors over different times of each user, the behavior sequence embedding is aggregated by the concatenate pooling function \(\) as follows.

\[_{u}=(^{u})=(_{1}^{u}||..._{i}^{u}...||_{T}^{u}), \]

where \(_{i}^{u}^{1 d^{}}\) denotes the embedding of user behavior at \(i\)-th step and \(_{u}^{1 Td^{}}\) denotes the aggregated behavior embedding of user \(u\). We re-denote \(Td^{}\) as \(d\) for convenience. By encoding and aggregation, we obtain the behavior embeddings of all users \(^{|| d}\).

#### 3.4.2 End-to-end Learnable Cluster Module

After behavior encoding, we guide the model to learn the users' latent intents from the behavior embeddings. To this end, an end-to-end learnable cluster module (ELCM) is proposed to break the alternative optimization in the previously mentioned EM framework. This module can group the users' behaviors embeddings into various clusters, which represent the users' latent intents or interests. Concretely, at first, the cluster centers \(^{k d}\) are initialized as the learnable neural parameters, i.e., the tensors with gradients. Then, we design a simple yet effective clustering loss to train the networks and cluster centers as formulated as follows.

\[_{}=_{i=1}^{k}_ {j=1,j i}^{k}\|}_{i}-}_{j}\|_{2}^{2}}_{ }+_{i=1}^{b}_{j=1}^{k }\|}_{i}-}_{j}\|_{2}^{2}}_{}, \]

where \(}_{i}=_{i}/\|_{i}\|_{2},}_{ i}=_{i}/\|_{i}\|_{2}\). In Eq. (3), \(k\) denotes the number of clusters (intents), and \(b\) denotes the batch size. \(_{i}^{1 d}\) denotes the \(i\)-th user's behavior embedding and \(_{j}^{1 d}\) denotes the \(j\)-th cluster center. For better network convergence, we constrain the behavior embeddings and cluster center embeddings to distribute on a unit sphere. Concretely, we apply the \(l\)-2 normalization to both the user behavior embeddings \(\), and the cluster centers \(\) during calculating \(_{}\).

In the proposed clustering loss, the first term is designed to disentangle the complex users' intents into simple intent units. Technically, it pushes away different cluster centers, therefore reducing the overlap between different clusters (intents). The time complexity and space complexity of this term are \((k^{2}d)\) and \((kd)\), respectively. The number of users' intents is vastly less than the number of users, i.e., \(k||\). Therefore, the first term will not bring significant time or space costs.

In addition, the second term of the proposed clustering loss aims to align the users' latent intents with the behaviors by pulling the behavior embeddings to the cluster centers. This design makes the in-class cluster distribution more compact and guides the network to condense similar behaviors into one intention. Also, on another aspect, it forces the model to learn users' intents from behavior embeddings. Note that the behavior embedding \(_{i}\) is pulled to all center centers \(_{j},j=1,...,k\) rather than the nearest cluster center. The main reason is that the practical clustering algorithm is imperfect, and pulling to the nearest center easily leads to the confirmation bias problem . To this end, the proposed clustering loss \(_{}\) aims to optimize the clustering distribution in an adversarial manner by pulling embeddings together to cluster centers while pushing different cluster centers away. Besides, it enables the optimization of this term via mini-batch samples, avoiding performance clustering algorithms on the whole data. The time complexity and space complexity of the second term are \((bkd)\) and \((bk+bd+kd)\), respectively. Since the batch size is essentially less than the number of users, namely, \(b||\), the second term of clustering loss \(_{}\) alleviates the considerable time or space costs.

In the existing EM optimization framework, the clustering algorithm needs to be applied on the entire users' behavior embeddings \(^{|| d}\). Take the classical \(k\)-Means clustering as an example, at each E-step, it leads to \((t||kd)\) time complexity and \((||k+||d+kd)\) space complexity, where \(t\) denote the iteration steps of \(k\)-Means clustering algorithm. We find that, at each step, the time and space complexity is linear to the number of users, thus leading to out-of-memory or running time problems (issue (1)), especially on large-scale industrial data with millions or billions of users.

Fortunately, our proposed end-to-end learnable cluster module can solve this issue (1). By summarising previous analyses, we draw that the overall time and space complexity of calculating the clustering loss \(_{}\) are \((bkd+k^{2}d+bd)\) and \((bk+bd+kd)\), respectively. They are both linear to the batch size \(b\) at each step, enabling the model's scalability. Besides, the proposed module is plug-and-play and easily deployed in real-time large-scale industrial systems. We provide detailed evidence and practical insights in Section 5. The proposed ELCM can not only improve the recommendation performance (See Section 4.1 & 4.2) but also promote efficiency (See Section 4.3).

#### 3.4.3 Intent-assisted Contrastive Learning

Next, we aim to enhance further the mutual promotion of behavior learning and clustering. To this end, Intent-assisted contrastive learning (ICL) is proposed by adopting cluster centers as self-supervision signals for behavior learning. Firstly, we conduct contrastive learning among the behavior sequences. The new views of the behavior sequences are constructed via sequential augmentations, including mask, crop, and reorder. The two views of behavior sequence of user \(u\) are denoted as \((S^{u})^{v1}\) and \((S^{u})^{v2}\). According to Section 3.4.1, the behaviors are encoded to the behavior embeddings \(_{u}^{v1},_{u}^{v2}^{1 d}\). Then, the sequence contrastive loss of user \(u\) is formulated as follows.

\[_{}^{u}=-((_{u}^{v1},_{u}^{v2})}}{_{}e^{(_{u}^{v1 },_{u}^{v2})}}+(_{u}^{v1}, _{u}^{v2})}}{_{}e^{(_{u}^{v2}, _{})}}), \]

where "sim" denotes the dot-product similarity, "neg" denotes the negative samples. Here, the same sequence with different augmentations is recognized as the positive sample pairs, and the other sample pairs are recognized as the negative sample pairs. By minimizing \(_{}=_{u}_{}^{u}\), the similar behaviors are pulled together, and the others are pushed away from each other, therefore enhancing the representation capability of users' behaviors. The learned cluster centers \(^{k d}\) are adopted as the self-supervision signals. The index of the assigned cluster of \(_{u}^{v1}\) is queried as follows.

\[idx=*{arg\,min}_{i}(_{i}-_{u}^{v1} _{2}^{2}), \]

where \(_{i}^{1 d}\) denotes the \(i\)-th cluster (intent) center embedding. Then, the intent information is fused to the user behavior during the sequence contrastive learning. Here, we consider two optional fusion strategies, including the concatenate fusion \(_{u}^{v1}=(_{u}^{v1}||_{idx})\) and the shift fusion \(_{u}^{v1}=_{u}^{v1}+_{idx}\). A similar operation is applied to the second view of the behavior embedding \(_{u}^{v2}\). After fusing the intent information to user behaviors, the networks are trained by minimizing \(_{}\)In addition, to further collaborate intent learning and sequential representation learning, we conduct contrastive learning between the user's behaviors and the learnable intent centers. The intent contrastive loss is formulated as follows.

\[_{}^{u}=-(e^{ (_{u}^{v_{i}},_{i})}}{_{}e^{( _{u}^{v_{i}},_{})}}+e^{ (_{u}^{v_{i}},_{i})}}{_{}e^{(_{u}^{v_{i}},_{})}}), \]

where \(_{u}^{v_{1}},_{u}^{v_{2}}\) are two-view behavior embedding of the user \(u\). Besides, "neg" denotes the negative behavior-intent pairs among all pairs. Here, we regard the behavior embedding and the corresponding nearest intent center as the positive pair and others as negative pairs. By minimizing the intent contrastive loss \(_{}=_{u}_{}^{u}\), behaviors with the same intents are pulled together, but behaviors with different intents are pushed away. The objective of ICL is formulated as follows.

\[_{}=_{}+_{}. \]

The effectiveness of ICL is verified in Section 4.2. With the proposed ELCM and ICL, we develop a new end-to-end optimization framework for intent learning, improving performance and convenience. By these designs, the issue (2) is also solved.

#### 3.4.4 Overall Objective

The neural networks and learnable clusters are trained with multiple tasks, including intent learning, intent-assisted contrastive learning, and next-item prediction. The intent learning task aims to capture the users' underlying intents. Besides, intent-assisted contrastive learning aims to collaborate with intent learning and behavior learning. In addition, the next-item prediction task is a widely used task for recommendation systems. The overall objective of ELCRec is formulated as follows.

\[_{}=_{}+0.1_{}+_{}, \]

where \(_{}\), \(_{}\), and \(_{}\) denotes the next item prediction loss, intent-assisted contrastive learning loss, and clustering loss, respectively. \(\) is a trade-off hyper-parameter. We present the overall algorithm process of the proposed ELCRec method in Algorithm 1 in Appendix.

We detail and summarize the devised loss in equation (8). We train our proposed ELCRec method with multiple tasks, including the next-item prediction task, intent-assisted contrastive learning, and intent learning (learnable clustering) task. Accordingly, Equation (8), which denotes the overall loss function of ELCRec, contains three parts: next-item prediction loss \(_{}\), the intent-assisted contrastive learning loss \(_{}\), and the intent learning loss \(_{}\). Concretely, the next-item prediction loss is a commonly used loss function for the sequential recommendation. It aims to predict the next item in the interaction sequence based on the previous sequence. In addition, the intent learning loss aims to optimize the cluster center embeddings by pulling the samples to the corresponding cluster centers and pushing away different cluster centers. Moreover, the intent-assisted contrastive learning loss aims to conduct self-supervised learning to unify the behavior representation learning and intent representation learning. Overall, equation (8) trains the network through three tasks by a linear combination of three loss functions.

## 4 Experiment

This section aims to comprehensively evaluate ELCRec by answering research questions (RQs).

1. Superiority: does it outperform the state-of-the-art sequential recommendation methods?
2. Effectiveness: are the ELCM and ICL modules effective?
3. Efficiency: how about the time and memory efficiency of the proposed ELCRec?
4. Sensitivity: what is the performance of the proposed method with different hyper-parameters?
5. Convergence: have the loss function and recommendation performance converged?
6. Visualization: Can the visualized learned embeddings reflect the promising results?

We answer RQ(i), (ii), (iii) in Section 4.1, 4.2, 4.3, respectively. Due to the limited pages, RQ(iv), (v), (vi) are answered in the Appendix 8.6, 8.7, and 8.8 respectively.

### Superiority

In this section, we aim to answer the research question (i) and demonstrate the superiority of ELCRec. To be specific, we compare ELCRec with nine state-of-the-art recommendation baselines [88; 34; 96; 39; 69; 94; 129; 108; 18]. Experimental results are the mean values of three runs. As shown in Table 1, the **bold values** and underlined values denote the best and runner-up results, respectively. From these results, we have four conclusions as follows. (a) The non-sequential model BPR-MF  has not achieved promising performance since the shallow method lacks the representation learning capability of users' historical behaviors. (b) The conventional sequential methods [34; 96; 39] improve the recommendation via different DNNs such as CNN , RNN , and Transformer . But they perform worse since limiting self-supervision. (c) The recent methods [94; 129; 108] enhance the self-supervised capability of models via the self-supervised learning techniques. However, they neglect the underlying users' intent, thus leading to sub-optimal performance. (d) More recently, the intent learning methods [44; 14; 45; 18; 49; 53; 5] have been proposed to mine users' underlying intent to assist recommendation. Motivated by their success, we propose a new intent learning method termed ELCRec. Refitting from the strong intent learning capability of ELCRec, it surpasses all other intent learning methods.

The balance is set to 1 in equation (7). We can add one balance hyperparameter to control the balance between sequence contrastive learning loss and intent contrastive learning loss to achieve better performance. However, in equation (8), we find there are many balances that need to be controlled, such as the balance of intent-assist contrastive learning loss and the balance of intent learning loss, easily leading to the high cost of hyperparameter tuning. To lower the load of tune hyperparameters, we fix the balance between sequence contrastive learning loss and intent contrastive learning loss as 1 and the balance between next item prediction loss and intent-assisted contrastive learning loss as 0.1. This setting has already been able to achieve promising performance. For other complex scenarios, we can set more balance hyperparameters for better performance in the future.

We did have one inconsistent finding on the toy dataset compared with other datasets. Concretely, ELCRec (B+ELCM+ICL) cannot beat B+ELCM, indicating that ICL may be ineffective on the B+ELCM variant on this dataset. However, we also find that B+ICL can beat B, indicating that ICL works for the baseline model. This phenomenon is interesting. We have the following explanations as follows. The ICL is conducted on both the behavior representations and the intent representations. Therefore, it can be influenced by both these two optimization processes. Namely, both the quality of behavior embeddings and the quality of the intent embeddings are crucial for the quality of ICL. Thus, it may not be very robust in all cases. For B+ICL, adding ICL to the baseline can improve the behavior-learning process. However, we find that B+ELCM has already achieved a very promising performance compared with other variants, indicating the quality of intent representations is excellent. Then we add ICL to B+ELCM, the ICL may downgrade the quality of intent representations. To solve this issue, we will conduct more careful training and optimize the training procedure to achieve better performance.

   &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\   &  &  &  &  &  &  &  &  &  &  &  &  & Ours & Impr. \(p\)-value \\   & HR@5 & 0.0141 & 0.0162 & 0.0154 & 0.0206 & 0.0217 & 0.0214 & 0.0121 & 0.0217 & 0.0172 & 0.0225 & 0.0246 & 0.0265 & **0.0286** & 7.5791 & 2.346-6* \\  & H@0.023 & 0.0232 & 0.0421 & 0.0399 & 0.0497 & 0.0604 & 0.0095 & 0.0344 & 0.0580 & 0.0357 & 0.0488 & 0.0641 & 0.0603 & **0.0648** & 1.0992 & 2.249-4* \\  & NDCG@5 & 0.0991 & 0.0103 & 0.0114 & 0.0135 & 0.0142 & 0.0484 & 0.0317 & 0.0115 & 0.0162 & 0.0172 & 0.0278 & **0.0458** & 5.645-5* \\  & NDCG@20 & 0.0142 & 0.0186 & 0.178 & 0.0216 & 0.0251 & 0.0220 & 0.0146 & 0.0227 & 0.0170 & 0.0225 & 0.0280 & 0.0276 & **0.0286** & 2.1494 & 7.876-3* \\   & HR@6 & 0.0212 & 0.0111 & 0.0251 & 0.0374 & 0.0360 & 0.0410 & 0.0189 & 0.0423 & 0.0368 & 0.0414 & 0.0408 & 0.0405 & **0.0259** & 6.876 & 3.181-6* \\  & HR@0.20 & 0.0589 & 0.0478 & 0.0643 & 0.0901 & 0.0984 & 0.0914 & 0.0487 & 0.0994 & 0.0647 & 0.0484 & 0.0916 & **0.1102** & **0.1107** & **0.1079** & 5.306-3* \\  & NDCG@5 & 0.0130 & 0.0065 & 0.0454 & 0.0421 & 0.0216 & 0.0261 & 0.0161 & 0.0182 & 0.0269 & 0.0283 & 0.0245 & 0.0236 & **0.0355** & 8.909+ & 4.486-6* \\  & NDCG@20 & 0.0236 & 0.0104 & 0.0398 & 0.0387 & 0.0391 & 0.0403 & 0.0198 & 0.0441 & 0.0357 & 0.0407 & 0.0444 & 0.0491 & **0.0509** & 3.679+ & 9.086-6* \\   & HR@5 & 0.0120 & 0.0097 & 0.0106 & 0.0463 & 0.0274 & 0.0602 & 0.0413 & 0.0352 & 0.0399 & 0.0477 & 0.0311 & **0.0385** & 0.0358 & 0.179+ & 1.122-12* \\  & HR@0.20 & 0.0312 & 0.0302 & 0.0420 & 0.0491 & 0.0588 & 0.0975 & 0.0325 & 0.0318 & 0.0679 & 0.0904 & 0.0781 & 0.1129 & **0.1138** & 7.119+ & 2.287-4* \\  & NDCG@5 & 0.0082 & 0.0659 & 0.0107 & 0.0306 & 0.0174 & 0.0337 & 0.0123 & 0.0362 & 0.0296 & 0.0336 & 0.0197 & 0.0397 & **0.0403** & 1.519+ & 2.876-4* \\  & NDCG@20 & 0.0136 & 0.0116 & 0.0179 & 0.0441 & 0.0291 & 0.0471 & 0.0162 & 0.0260 & 0.0374 & 0.0458 & 0.0330 & 0.0350 & **0.050** & **1.859+ & 3.725-3* \\   & HR@5 & 0.0127 & 0.0125 & 0.0142 & 0.0160 & 0.0196 & 0.0171 & 0.0101 & 0.0209 & 0.0166 & 0.0223 & 0.0133 & **0.026** & 2.199+ & 7.181-3* \\  & HR@0.20 & 0.0346 & 0.0371 & 0.0406 & 0.0443 & 0.0564 & 0.0464 & 0.0184 & 0.0630 & - & 0.0460 & 0.0640 & 0.0645 & **0.0653** & 1.249+ & 3.738-4* \\  & NDCG@5 & 0.0082 & 0.0991 & 0.0080 & 0.0101 & 0.0121 & 0.0112 & 0.0068 & 0.0144 & - & 0.0105 & 0.0137 & 0.0136 & **0.0150** & 2.749+ & 1.236-2* \\  & NDCG@20 & 0.0143 & 0.0145 & 0.0156 & 0.0179 & 0.0223 & 0.0193 & 0.0127 & 0.0256 & 0.0186 & 0.0263 & 0.0263 & **0.0266** & 1.149+ & 6.826-3* \\  

Table 1: Recommendation performance on benchmarks. **Bold values** and underlined values denote the best and runner-up results. \({}^{*}\) indicates that, in the \(t\)-test, the best method significantly outperforms the runner-up with \(p<0.05\). \({}^{*}\)-\({}^{*}\) indicates models do not converge.

To further verify the superiority of ELCRec, we conduct the \(t\)-test between the best and runner-up methods. As shown in Table 1, the most \(p\)-value is less than 0.05 except HR@5 on the Toys dataset. It indicates that ELCRec significantly outperforms runner-up methods. Overall, the extensive experiments demonstrate the superiority of ELCRec. In addition, we also conduct comparison experiments on recommendation datasets of other domains, including movie recommendation and news recommendation, as shown in the Appendix 8.4.1 and 8.4.2. These experimental results demonstrate a broader applicability of our proposed ELCRec.

### Effectiveness

This section is dedicated to answering the research question (ii) and evaluating the effectiveness of the End-to-end Learnable Cluster Module (ELCM) and Intent-assisted Contrastive Learning (ICL). To achieve this, we conducted meticulous ablation studies on four benchmarks. Figure 1 illustrates the experimental results. In each sub-figure, "B", "B+ICL," "B+ELCM," and "ELCRec" correspond to the backbone, backbone with ICL, backbone with ELCM, and backbone with both ICL and ELCM, respectively. Through the ablation studies, we draw three key conclusions. (a) "B+ICL" outperforms the backbone "B" on all four benchmarks. It indicates that the proposed ICL effectively improves behavior learning. (b) "B+ELCM" surpasses the backbone "B" significantly on all benchmarks. This phenomenon demonstrates that our proposed end-to-end learnable cluster module helps the model better capture the users' underlying intents, thus improving recommendation performance. (c) ELCRec achieves the best performance on three out of four datasets. It shows the effectiveness of the combination of these two modules. On the Toys dataset, ELCRec can outperform the "B" and "B+ICL" but perform worse than "B+ELCM". This phenomenon indicates it is worth researching the better collaboration of these two modules in the future. To summarize, these extensive ablation studies verify the effectiveness of the proposed intent-assisted contrastive learning and end-to-end learnable cluster module in ELCRec.

### Efficiency

We test the efficiency of ELCRec on four benchmarks and answer the research question (iii). Concretely, the efficiency contains two perspectives, including running time costs (in seconds) and GPU memory costs (in MB). Note that we use the same epoch number of our method and the baseline when we test the running time. Besides, we calculate the average GPU memory cost during the training process. We have two observations as follows. (a) ELCRec can speed up ICLRec on three out of four datasets (See Table 2). Overall, on four datasets, the running time is decreased by 7.18% on average. The reason is that our proposed end-to-end optimization of intent learning breaks the alternative optimization of the EM framework, saving computation costs. (b) The results demonstrate that the GPU memory costs of our ELCRec are lower than that of ICLRec on four datasets (See Table 2). On average, the GPU memory costs are decreased by 9.58%. It is because we enable the model to conduct intent learning via the mini-batch users' behaviors. Therefore, in summary, we demonstrate the efficiency of ELCRec from both time and memory aspects. Please note that, due to the relatively small size of the open benchmarks, the efficiency improvements are not particularly significant. However, on large-scale data, our method can achieve more substantial improvements.

We observe that in most cases, our proposed method can save time and memory costs, e.g., saving 7.18% time and 9.48% memory on average. For the time cost of our method on the Sports dataset, we regard it as a corner case. By careful analyses, we provide the explanation as follows. We suspect the raised time costs are caused by the wrong direction of the optimization. Setting the cluster

Figure 1: Ablation studies of the proposed end-to-end learnable cluster module (ELCM) and the intent-assisted contrastive learning (ICL). The results are the sum of four metrics, including HR@5, HR@20, NDCG@5, and NDCG@20.

embeddings as the learnable neural parameters and optimizing them during training may be a harder task for the model compared to conducting the offline clustering algorithm on the learned embeddings directly. We analyze the performance and loss curve of our method on the Sports dataset and find that the decline of loss slowdowns and the performance seem to drop a little at almost the end of the training. We think this wrong optimization leads to the comparable time cost of our method compared with the baseline. But for other datasets, their optimization processes are great, therefore saving time and memory costs essentially. In the future, we can avoid this wrong optimization direction through some strategies, such as early-stopping and penalty terms.

## 5 Application

Our proposed ELCRec is versatility and plug-and-play. Benefiting its advantages, we aim to apply it to real-time large-scale industrial recommendation systems with millions of users. First, we introduce the background and settings of the application. Then, we conduct extensive A/B testing and analyze the experimental results. Besides, due to the page limitation, we provide deployment details and practical insights in Appendix 8.13 and 8.10, respectively.

### Application Background

The applied scenario is the live streaming recommendation on the front page of the Alipay app. The user view (UV) and page view (PV) of this application are about 50 million and 130 million, respectively. Note that most users are new to this application, therefore leading to the sparsity of users' behaviors. To solve this cold-start problem in the recommendation system, we adopt our proposed method to group users and recommend items based on the groups. Concretely, due to the sparsity of users' behaviors, we first replace the users' behavior with the users' activities features in this application and model them via the multi-gate mixture-of-expert (MMOE) model . Th,en we aim to group the users into various groups. For the existing intent learning methods, they easily lead to long-running time or out-of-memory problems. To solve this problem, we adopt the end-to-end learnable cluster module to group the users into various groups effectively and efficiently. Through this module, the high-activity users and new users are grouped into different clusters, alleviating the cold-start issue and assisting in better recommendations. Besides, during the learning process of the cluster embeddings, the low-activity users can transfer to high-activity users, improving the overall users' activities in the application. Eventually, the networks are trained with multiple tasks. In the next section, we conduct experiments to demonstrate the effectiveness of our proposed method on real-time large-scale industrial data.

### A/B Testing on Real-time Large-scale Data

We conduct A/B testing on the real-time large-scale industrial recommendation system. The experimental results are listed in Table 3. We evaluate the models with two metric systems, including live streaming metrics and merchandise metrics. livestreaming metrics contain Page View Click Through Rate (PVCTR) and Video View (VV). Merchandise metrics contain PVCTR and User View Click Through Rate (UVCTR). The results indicate that our method can improve the recommendation performance of the baseline by about 2%. Besides, the improvements are significant with \(p<0.05\) in three out of four metrics.

  
**Cost** & **Dataset** & **Sports** & **Beauty** & **Toys** & **Yelp** & **Average** \\   & ICLRec & 5282 & 3770 & 4374 & 4412 & 4460 \\  & ELCRec & 5360 & 2922 & 4124 & 4151 & 4139 \\  & Improvement & 1.48\% \(\) & **22.49\% \(\)** & **5.72\% \(\)** & **5.92\% \(\)** & **7.18\% \(\)** \\   & ICLRec & 1944 & 1798 & 2887 & 3671 & 2575 \\  & ELCRec & 1781 & 1594 & 2555 & 3383 & 2328 \\   & Improvement & **8.38\% \(\)** & **11.35**\% \(\)** & **11.50\% \(\)** & **7.85\% \(\)** & **9.58\% \(\)** \\   

Table 2: Running time and memory costs. **Bold values** denote better results.

In addition, to further explore why our method can work well in real-time large-scale recommendation systems, we further analyze the recommendation performance of different user groups. The results are shown in Table 4. Based on the users' activity, we classify them into five groups, including Pure New users (PN), New users (N), Low-Activity users (LA), Medium-Activity users (MA), and High-Activity users (HA). Compared with the general recommendation algorithms that are unfriendly to new users, the experimental results show that our module not only improves the recommendation performance of high-activity users but also improves the recommendation performance of new users. Therefore, it can alleviate the cold-start problem and construct a more friendly user ecology.

For the utilization of group embeddings, there are many ways. For the conventional user recommendation or the group recommendation, we utilize the historical group embeddings and conduct continued training for the recommendation model. For other downstream tasks in other domains, we can provide the restore group embeddings for them. Therefore, for the recommendation model, the group embeddings are restored in the model parameters and updated daily. Besides, for other indirect downstream tasks, the group embeddings will be stored in the database.

## 6 Conclusion

In this paper, we explore intent learning in recommendation systems. To be specific, we summarize and analyze two drawbacks of the existing EM optimization framework of intent learning. The complex and cumbersome alternating optimization limits the scalability and performance of existing methods. To this end, we propose a novel intent learning method termed ELCRec with an end-to-end learnable cluster module and intent-assisted contrastive learning. Extensive experiments on four benchmarks demonstrate ELCRec's six abilities. In addition, benefiting from the versatility of ELCRec, we successfully apply it to the real-time large-scale industrial scenario and also achieve promising performance. Due to the limited pages, We discuss the limitations and future work of this paper in Appendix 8.14, such as pre-defined cluster number, limited recommendation domains, and uncontrollable update rate of cluster centers.

## 7 Acknowledgment

We thank all anonymous reviewers for their constructive and helpful reviews. This work was supported by the National Natural Science Foundation of China (No. 62325604 and 62276271). Besides, this work was also supported by the National Key R&D Program of China (Project 2022ZD0115100), the National Natural Science Foundation of China (Project U21A20427), the Research Center for Industries of the Future (Project WU2022C043), and the Competitive Research Fund (Project WU2022A009) from the Westlake Center for Synthetic Biology and Integrated Bioengineering.

   Metric & PN & N & LA & MA & HA \\  PVCTR & **6.96\%**\(\) & **1.67\%**\(\) & **1.98\%**\(\) & **0.35\%**\(\) & **19.02\%**\(\) \\ VV & **6.81\%**\(\) & **1.50**\%\(\) & **1.50**\%\(\) & **0.04**\%\(\) & **16.90**\%\(\) \\   

Table 4: Results on different user groups. **Bold values** denotes improvements with \(p<0.05\).

    &  &  \\  & PVCTR & VV & PVCTR & UVCTR \\  Baseline & - & - & - & - \\ Impro. & **2.45\%**\(\) & **2.28\%**\(\) & **2.41\%**\(\) & 1.62\% \(\) \\   

Table 3: A/B testing on real-time large-scale industrial recommendation. **Bold values** denotes the significant improvements with \(p<0.05\). The symbol “-” denotes business secret.