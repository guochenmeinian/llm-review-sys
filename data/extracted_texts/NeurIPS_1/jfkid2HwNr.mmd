# Medformer: A Multi-Granularity Patching Transformer for Medical Time-Series Classification

Yihe Wang\({}^{*}\), Nan Huang\({}^{*}\), Taida Li\({}^{*}\)

University of North Carolina - Charlotte

{ywang145,nhuang1,tli14}@charlotte.edu

&Yujun Yan

Dartmouth College

yujun.yan@dartmouth.edu &Xiang Zhang

University of North Carolina - Charlotte

xiang.zhang@charlotte.edu

These authors contributed equally to this work.

###### Abstract

Medical time series (MedTS) data, such as Electroencephalography (EEG) and Electrocardiography (ECG), play a crucial role in healthcare, such as diagnosing brain and heart diseases. Existing methods for MedTS classification primarily rely on handcrafted biomarkers extraction and CNN-based models, with limited exploration of transformer-based models. In this paper, we introduce Medformer, a multi-granularity patching transformer tailored specifically for MedTS classification. Our method incorporates three novel mechanisms to leverage the unique characteristics of MedTS: cross-channel patching to leverage inter-channel correlations, multi-granularity embedding for capturing features at different scales, and two-stage (intra- and inter-granularity) multi-granularity self-attention for learning features and correlations within and among granularities. We conduct extensive experiments on five public datasets under both subject-dependent and challenging subject-independent setups. Results demonstrate Medformer's superiority over 10 baselines, achieving top averaged ranking across five datasets on all six evaluation metrics. These findings underscore the significant impact of our method on healthcare applications, such as diagnosing Myocardial Infarction, Alzheimer's, and Parkinson's disease. We release the source code at [https://github.com/DL4mHealth/Medformer](https://github.com/DL4mHealth/Medformer).

## 1 Introduction

Medical time series refers to sequences of health-related data points recorded at successive times, tracking various physiological signals over time . Effective classification of MedTS data enables continuous monitoring and real-time analysis of a subject's physiological state, supporting early abnormality detection, accurate diagnosis, timely intervention, and personalized treatment--ultimately enhancing patient outcomes and healthcare efficiency . For instance, Electroencephalography (EEG) provides insights into a subject's neurological status , while Electrocardiography (ECG) aids in diagnosing heart conditions . Most current MedTS classification approaches rely on handcrafted biomarker extraction , convolutional neural networks (CNN)-based models , graph convolutional networks (GNNs)-based models, or combinations of CNNs and self-attention modules. Notably, effective transformer-based models for MedTS classification remain underexplored.

Transformers have demonstrated strong performance in time series representation learning across tasks such as forecasting , classification , and anomaly detection , witha predominant focus on forecasting. While these methods are applicable to MedTS classification, their design motivations and mechanisms may not fully align with the unique requirements of this domain. For example, as shown in Figure 1, models like Autoformer  and Informer  adopt the token embedding approach from the vanilla transformer , using a single cross-channel timestamp as an input token. This strategy struggles to capture coarse-grained temporal features. In contrast, iTransformer  encodes the entire series from one channel as an input token, which can overlook fine-grained temporal features while focusing on multi-channel correlations. Additionally, PatchTST  embeds a sequence of timestamps from one channel as a patch for self-attention, limiting the model's capacity to learn cross-channel relationships.

These existing methods fail to fully exploit the distinctive characteristics of MedTS data, such as local temporal dynamics, inter-channel correlations, and multi-scale feature analysis. First, effective capturing temporal dynamics demands multi-timestamp inputs to capture local temporal patterns, as highlighted in approaches like PatchTST  and Crossformer . Second, leveraging cross-channel information is critical; for example, multi-channel EEG data recorded following the International 10-20 system  monitors the brain activities, with each electrode/channel corresponding to specific brain regions. Since brain functions are integrated, inter-channel correlations (e.g., brain connectome) are crucial in EEG analysis [35; 36; 37]. Third, representation learning across multiple temporal scales and periods is vital to uncover a broad range of health patterns, as certain disease indicators may only appear within specific frequency bands [10; 12].

To bridge this gap, we propose **Medformer***, a multi-granularity patching transformer designed explicitly for MedTS classification. Our approach introduces three mechanisms to enhance learning capacity. First, we propose a novel token embedding method using cross-channel patching, effectively capturing both _multitimestamp_ and _cross-channel_ features. To the best of our knowledge, this is the first application of cross-channel patching for transformer embedding in time series analysis. Second, rather than using fixed-length patches, we employ _multi-granularity_ patching with a list of patch lengths, enabling the model to capture features in different scales. This multi-granularity approach could simulate different frequency bands, capturing band-specific features without relying on hand-crafted up/downsampling and band filters. Third, We introduce a two-stage (intra- and inter-granularity) self-attention mechanism to capture features within individual granularities and correlations across granularities, enabling complementary information integration across scales.

Footnote *: We note that this name has been previously used in other domains .

We conduct extensive experiments using ten baselines across five public datasets, including three EEG datasets and two ECG datasets, focused on detecting diseases such as Alzheimer's and cardiovascular conditions under both subject-dependent and subject-independent setups (Figure 2). Results show that Medformer achieves the highest average ranking across all six evaluation metrics and five datasets (Figure 4), highlighting its superior effectiveness, stability, and potential for real-world applications.

## 2 Related Work

**Medical Time Series.** Medical time series refers to specialized time series data collected from the human body, commonly used for disease diagnosis [3; 7], health monitoring [6; 1], and brain-computer interfaces (BCIs) [39; 2]. Different MedTS modalities include EEG [40; 41; 42], ECG [7; 8; 9], EMG [43; 44], and EOG [45; 46], each offering distinct capabilities for various medical applications.

Figure 1: **Token embedding methods.** Vanilla transformer, Autoformer, and Informer [30; 28; 29] employ a single cross-channel timestamp as a token; iTransformer  utilizes an entire channel as a token; and PatchTST and Crossformer [32; 33] adopt a patch of timestamps from one channel as a token. For MedTS classification, we propose Medformer considering inter-channel dependencies (cross-channel), temporal properties (multi-timestamp), and multifaceted scale of temporal patterns (multi-granularity).

For example, EEG and ECG data are instrumental in assessing brain and heart health [40; 7]. Recent BCI research explores using EEG to control objects, providing functional support to individuals with disabilities [2; 47]. Unlike general time series research, which predominantly focuses on forecasting tasks [48; 49], MedTS research is centered around signal decoding, which involves classifying hidden information within MedTS sequences. Current approaches often rely on biomarker identification and deep-learning models utilizing CNNs, GNNs, or hybrid models combining CNNs with self-attention modules. For instance, band features such as relative band power and inter-band correlations [11; 50] have proven effective in EEG-based Alzheimer's disease diagnosis. Deep-learning models like EEGNet , EEG-Conformer , and TCN [51; 13] have also shown strong performance across various MedTS classification tasks.

**Transformers for Time Series.** Transformer has demonstrated its strong learning and scaling-up ability in many domains, including natural language processing [30; 57] and computer vision [58; 59]. Existing transformer-based methods for time series analysis can be categorized into two main directions: modifying token embedding methods and self-attention mechanisms, or both. For example, PatchTST  uses a sequence of single-channel timestamps as a patch for token embedding. Methods like Autoformer , Informer , Nonformer , and FEDformer  develop new self-attention mechanisms or replace the self-attention module to improve learning ability and reduce complexity. Crossformer  and iTransformer  modify both token embedding methods and self-attention mechanisms. **Patching.** Patch embedding has been widely used in time series transformers since the proposal of PatchTST . Existing methods of patching, such as Crossformer , CARD , and MTST , inherit from PatchTST  and utilize a sequence of single-channel timestamps for patching. This channel-independent patching might benefit learning ability in time series forecasting but may not be as effective in MedTS classification. **Multi-Granularity.** Existing methods such as Pyraformer , MTST , Pathformer , and Scaledformer , utilize multi-granularity embedding to capture features at different scales, allowing models to learn both fine-grained and coarse-grained patterns. We discuss the differences between our method and existing multi-granularity approaches in Appendix G.1.

Medformer includes both novel token embedding and self-attention mechanisms. Figure 1 and Table 1 present a comparison of token embedding methods and feature utilization between our method and existing methods. The components of our method can be easily incorporated into existing methods to improve classification learning ability. For example, cross-channel multi-granularity patching can be integrated with methods that modify self-attention mechanisms, such as Autoormer  and Informer , for token embedding. Similarly, the two-stage multi-granularity self-attention can be combined with existing multi-granularity methods, like MTST  and Pathformer , to enhance the learning of inter-granularity features.

## 3 Preliminaries and Problem Formulation

**Disease Diagnosis with MedTS.** Medical time series data typically exhibit multiple hierarchical levels, including subject, session, trial, and sample levels . In disease diagnosis tasks using MedTS, each subject is usually assigned a single label, such as indicating the presence or absence of Alzheimer's disease. However, multi-labeling may be necessary when a subject has co-occurring conditions . Notably, a subject's medical or physiological state remains relatively stable over time (or within short periods without significant change). For instance, a subject diagnosed with Alzheimer's disease is expected to retain that diagnosis for many years. If the subject also has Parkinson's disease, a multi-label learning approach is required, which essentially conducts classification tasks for each disease independently if they are not mutually exclusive.

Since long sequences of time series data (e.g., trials or sessions) are often segmented into shorter samples for deep learning training, all samples from a single subject should ideally retain the same

    & **Multi-** & **Cross-** & **Multi-** & **Granularity-** \\  & **Timestamp** & **Channel** & **Granularity** & **Interaction** \\ 
**Autoorwer** & & & & \\
**Creamformer** & & ✓ & & & \\
**FEDformer** & & & & \\
**Informer** & & & ✓ & & \\
**ITransformer** & ✓ & ✓ & & \\
**MTST** & ✓ & & ✓ & & \\
**Nonformer** & & ✓ & & & \\
**PatchTST** & ✓ & & & & \\
**Pathformer** & ✓ & & ✓ & & \\
**Reformer** & & ✓ & & & \\
**Transformer** & & ✓ & & & \\
**Multiformer(Ours)** & & ✓ & ✓ & ✓ & ✓ \\   

Table 1: Existing methods do not fully utilize all potential characteristics in MedTS.

medical condition label. Thus, each MedTS sample typically includes a class label indicating a specific disease type and a subject ID indicating its original subject. Given the ultimate goal of diagnosing whether a subject has a particular disease, experimental setups must be carefully designed to reflect real-world clinical applications. Diverse setups can yield markedly different outcomes, potentially leading to misleading conclusions. Here, we introduce two widely used setups in MedTS classification and clarify their distinctions. Figure 2 provides a simple illustration of these two setups.

**Subject-Dependent.** In this setup, the division into training, validation, and test sets is based on time series **samples**. All samples from various subjects are randomly shuffled and then allocated into the respective sets. Consequently, samples with identical subject IDs may be present in the training, validation, and test sets. This scenario potentially introduces "information leakage," wherein the model could inadvertently learn the distribution specific to certain subjects during the training phase. This setup is typically employed to assess whether a dataset exhibits cross-subject features and has limited applications under real-world MedTS-based disease diagnosis scenarios. The reason is simple: we cannot know the label of unseen subjects and their corresponding samples during training. Generally, the results of the subject-dependent setup tend to be notably higher than those from the subject-independent setup, often showing the upper limit of a dataset's learning capability.

**Subject-Independent.** In this setup, the division into training, validation, and test sets is based on **subjects**. Each subject and their corresponding samples are exclusively distributed into one of the training, validation, or test sets. Consequently, samples with identical subject IDs can only be present in one of these sets. This setup holds significant importance in disease diagnosis tasks as it closely simulates real-world scenarios. It enables us to train a model on subjects with known labels and subsequently evaluate its performance on unseen subjects; in other words, evaluate if a subject has a specific disease. However, this setup poses significant challenges in MedTS classification tasks. Due to the variability in data distribution and the potential presence of unknown noise within each subject's data, capturing general task-related features across subjects becomes challenging . Even if subjects share the same label, the personal noise inherent in each subject's data may obscure these common features. Developing a method that effectively captures common features among subjects while disregarding individual noise remains an unsolved problem.

In this work, we evaluate our model mainly in the subject-independent setup to better align with real-world applications, aiming to draw attention within the time series research community to the substantial challenges posed by this setup. _While our model is not specifically tailored to address subject-independent problems, it integrates multi-timestamp, cross-channel, and multi-granularity features in MedTS, enhancing its capacity to capture subject-invariant representations._ Consequently, our model is well-equipped to tackle the subject-independent challenge to a certain extent, and our results (Section 5) confirm such capability of Medformer.

We next present the problem formulation for multivariate medical time series classification in the context of disease diagnosis.

**Problem (MedTS Classification).**_Consider an input MedTS sample \(_{}^{T C}\), where \(T\) denotes the number of timestamps and \(C\) represents the number of channels. Our objective is to learn an encoder that can generate a representation \(\), which can be used to predict the corresponding label \(^{K}\) of the input sample. Here, \(K\) denotes the number of medically relevant classes, such as various disease types or different stages of one disease._

Figure 2: Subject-dependent/independent setups (figure adopted from our previous work ). In the subject-dependent setup, samples from the same subject can appear in both the training and test sets, causing information leakage. In a subject-independent setup, samples from the same subject are exclusively in either the training or test set, which is more challenging and practically meaningful but less studied.

## 4 Method

In this section, we first describe the cross-channel multi-granularity patching mechanism for learning spatial-temporal features in different scales. Next, we analyze the two-stage multi-granularity self-attention mechanism, which leverages features within the same granularity and correlations among different granularities. The architecture of the proposed Medformer is illustrated in Figure 3.

**Cross-Channel Multi-Granularity Patch Embedding.** From the medical perspective, the brain or heart functions as a cohesive unit, suggesting a naive assumption that there are inherent correlations among different channels in MedTS , as each channel represents the activities of distinct brain or heart regions. _Motivated by the above assumption,_ we reasonably propose multi-channel patching for token embedding, which is different from existing patch embedding methods that embed patches in a channel-independent manner and fail to capture inter-channel correlations . Figure 1 provides an overview comparison of existing token embedding methods and ours. Additionally, existing research on EEG biomarker extraction has shown that certain features are linked to different frequency bands, such as \(,\), and \(\) bands . This _motivates_ us to embed patch tokens in a multi-granularity way. Instead of using traditional methods like up/downsampling or handcrafted band filtering, multi-granularity patching automatically corresponds to different sampling frequencies, which can simulate different frequency bands and capture band-related features.

Figure 3: **Overview of Medformer.** a) Workflow. b) For the input sample \(_{}\), we apply \(n\) distinct patch lengths in parallel to create patched features \(^{(i)}_{p}\), where \(i\) ranges from 1 to \(n\). Each patch length represents a unique granularity. These patches are then projected into \(^{(i)}_{e}\) and subsequently augmented to form \(}^{(i)}_{e}\). c) We obtain the final embedding \(^{(i)}\) by combining the augmented \(}^{(i)}_{e}\) with both the positional embedding \(_{}\) and the granularity embedding \(^{(i)}_{}\). Additionally, a granularity-specific router \(^{(i)}\) is designed to capture integrated information for each respective granularity. We then perform intra-granularity self-attention, focusing on individual granularities, and inter-granularity self-attention, using the routers to facilitate communication across different granularities.

Given the above rationales, we propose a novel token embedding approach: cross-channel multi-granularity patching. Given an input multivariate MedTS sample \(_{}^{T C}\), and a list of different patch lengths \(\{L_{1},L_{2},,L_{n}\}\). For the \(i\)-th patch length \(L_{i}\) denoting granularity \(i\), we segment the input sample into \(N_{i}\) cross-channel non-overlapping patches \(_{}^{(i)}^{N_{i}(L_{i} C)}\). Zero padding is applied to ensure that the number of timestamps \(T\) is divisible by \(L_{i}\), making \(N_{i}= T/L_{i}\).

The patches are mapped into latent embeddings space using a linear projection: \(_{}^{(i)}=_{}^{(i)}^{(i)}\), where \(_{}^{(i)}^{N_{i} D}\) and \(^{(i)}^{(L_{i} C) D}\). Inspired by the augmented views contrasting in the contrastive learning framework , we further apply data augmentations such as masking and jittering on \(_{}^{(i)}\) to obtain augmented embeddings \(}_{}^{(i)}^{N_{i} D}\). We assume the augmentation can improve the learning ability in the following inter-granularity self-attention stage by forcing different granularities to learn and complement information from each other.

A fixed positional embedding \(_{}^{G D}\) is generated for positional encoding , where \(G\) is a very large number. We add \(_{}[1:N_{i}]^{N_{i} D}\), the first \(N_{i}\) rows of the positional embedding \(_{}\), and a learnable granularity embedding \(_{}^{(i)}^{1 D}\), to obtain the final patch embedding for the \(i\)-th granularity with patch length \(L_{i}\):

\[^{(i)}=}_{}^{(i)}+_{}[1:N_{i}] +_{}^{(i)}, \]

where \(^{(i)}^{N_{i} D}\). Note that the granularity embedding \(_{}^{(i)}\) aims to distinguish among granularities and is broadcasted to all \(N_{i}\) embedding rows with \(D\) dimension during addition.

To reduce time and space complexity, we initialize a router to be used in the multi-granularity self-attention (as described later) for each granularity:

\[^{(i)}=_{}[N_{i}+1]+_{}^{(i)}, \]

where \(^{(i)},_{}[N_{i}+1],_{}^{(i)}^{1 D}\). Here, \(_{}[N_{i}+1]\) is not used for positional embedding but to inform the router about the number of patches with the current \(L_{i}\) granularity, and \(_{}^{(i)}\) contains the granularity information. Both components help distinguish the routers from one another.

Finally, we obtain a list of final patch embeddings \(\{^{(1)},^{(2)},,^{(n)}\}\) and router embeddings \(\{^{(1)},^{(2)},,^{(n)}\}\) for different granularities with patch lengths \(\{L_{1},L_{2},,L_{n}\}\). We feed the embeddings to the two-stage multi-granularity self-attention.

**Multi-Granularity Self-Attention.** Our goal is to learn multi-granularity features and granularity interactions during self-attention. A naive approach to achieve this goal is to concatenate all the patch embeddings \(\{^{(1)},^{(2)},,^{(n)}\}\) into a large patch embedding \(^{(N_{1}+N_{2}++N_{n}) D}\) and perform self-attention on this new embedding, where \(n\) denotes the number of different granularities. However, this results in a time complexity of \(O((_{i=1}^{n}N_{i})^{2})\), which is impractical for a large \(n\).

To reduce the time complexity, we propose a router mechanism and split the self-attention module into two stages: a) intra-granularity self-attention and b) inter-granularity self-attention. The intra-granularity stage performs self-attention within the same granularity to capture the distinctive features of each granularity. The inter-granularity stage performs self-attention across different granularities to capture their correlations.

**a) Intra-Granularity Self-Attention.** For the \(i\)-th patch length \(L_{i}\) denoting granularity \(i\), we vertically concatenate the patch embedding \(^{(i)}^{N_{i} D}\) and router embedding \(^{(i)}^{1 D}\) to form an intermediate sequence of embeddings \(^{(i)}^{(N_{i}+1) D}\):

\[^{(i)}= [^{(i)}\|^{(i)}] \]

where \([\|]\) denotes concatenation. We perform self-attention on the new \(^{(i)}\) for both the patch embedding \(^{(i)}\) and the router embedding \(^{(i)}\):

\[^{(i)} ^{}(^{(i)}, ^{(i)},^{(i)}) \] \[^{(i)} ^{}(^{(i)}, ^{(i)},^{(i)})\]where \((,,)\) denotes the scaled dot-product self-attention mechanism in . Note that the router embedding \(^{(i)}\) is updated concurrently with the patch embedding \(^{(i)}\) to maintain consistency, ensuring that the router effectively summarizes each granularity's features in the current training step and is ready for the subsequent inter-granularity self-attention. The intra-granularity self-attention mechanism allows the model to capture temporal features within a single granularity, facilitating the extraction of local features and correlations among timestamps at the same scale.

**b) Inter-Granularity Self-Attention.** We concatenate all router embeddings \(\{^{(1)},^{(2)},,^{(n)}\}\) to form a sequence of routers \(^{n D}\):

\[=[^{(1)}\|^{(2)}\|\|^{(n)}] \]

where \(n\) is the number of different granularities. For granularity \(i\) with patch length \(L_{i}\), we apply self-attention to the router embedding \(^{(i)}^{1 D}\) with all the routers \(\):

\[^{(i)}(^{(i)},,) \]

Each router contains global information specific to one granularity by doing intra-granularity self-attention. By performing self-attention among routers, information can be exchanged and learned across different granularities, effectively capturing features across various scales. Additionally, the use of the router mechanism successfully reduces the time complexity of the naive approach from \(O((_{i=1}^{n}N_{i})^{2})\) to \(O(_{i=1}^{n}N_{i}^{2}+n^{2})\). Given that \(N_{i} T\), the worst-case time complexity for our self-attention mechanism is \(O(nT^{2}+n^{2})\). However, a reasonable choice of patch lengths as a power series, i.e., \(L_{i}=2^{i}\), leads to a time complexity of \(O(T^{2})\). To further reduce complexity and memory consumption, we apply shared layer normalization and feed-forward layers across all granularities. See appendix F for more details about complexity analysis.

**Summary.** Our method utilizes the standard transformer architecture shown in Figure 3. For given sample \(_{}\), after \(M\) layers of self-attention learning, we obtain a list of updated patch embeddings \(\{^{(1)},^{(2)},,^{(n)}\}\), which we concatenate them to form a final representation \(\) that can be used to predict label \(y^{K}\) in a downstream classification task. Note that although we discuss multi-granularity here, our method is flexible and can be easily adapted to variants such as single-granularity or even repetitive same granularities. See Appendix D.2 for more details.

## 5 Experiments

We compare our Medformer with 10 baselines across 5 datasets, including 3 EEG datasets and 2 ECG datasets. Our method is evaluated under two setups (Section 3): subject-dependent and subject-independent. In the subject-dependent setup, training, validation, and test sets are split based on samples, while in the subject-independent setup, they are split based on subjects.

**Datasets.** (1) **APAVA** is an EEG dataset where each sample is assigned a binary label indicating whether the subject has Alzheimer's disease. (2) **TDBrain** is an EEG dataset with a binary label assigned to each sample, indicating whether the subject has Parkinson's disease. (3) **ADFTD** is an EEG dataset with a three-class label for each sample, categorizing the subject as Healthy, having Frontotemporal Dementia, or Alzheimer's disease. (4) **PTB** is an ECG dataset where each sample

   Datasets & \#-Subject & \#-Sample & \#-Class & \#-Channel & \#-Timestamps & Sampling Rate & Modality & File Size \\  APAVA & 23 & 5,967 & 2 & 16 & 256 & 256Hz & EEG & 186MB \\ ADFTD & 88 & 69,752 & 3 & 19 & 256 & 256Hz & EEG & 2.52GB \\ TDBrain & 72 & 6,240 & 2 & 33 & 256 & 256Hz & EEG & 571MB \\ PTB & 198 & 64,356 & 2 & 15 & 300 & 250Hz & ECG & 2.15GB \\ PTB-XL & 17,596 & 191,400 & 5 & 12 & 250 & 250Hz & ECG & 4.28GB \\   

Table 2: **The information of processed datasets.** The table shows the number of subjects, samples, classes, channels, sampling rate, sample timestamps, modality of MedTS, and file size. Here, **#-Timestamps** indicates the number of timestamps per sample.

is labeled with a binary indicator of Myocardial Infarction. (5) **PTB-XL** is an ECG dataset with a five-class label for each sample, representing various heart conditions. Table 2 provides information on the processed datasets. For additional details on data characteristics, train-validation-test splits under different setups, and data preprocessing, please see Appendix B.

**Baselines.** We compare with 10 state-of-the-art time series transformer methods: Autoormer , Crossformer , FEDformer , Informer , iTransformer , MTST , Nonformer , PatchTST , Reformer , and vanilla Transformer .

**Implementation.** We employ six evaluation metrics: accuracy, precision (macro-averaged), recall (macro-averaged), F1 score (macro-averaged), AUROC (macro-averaged), and AUPRC (macro-averaged). The training process is conducted with five random seeds (41-45) on fixed training, validation, and test sets to compute the mean and standard deviation of the models. All experiments are run on an NVIDIA RTX 4090 GPU and a server with 4 RTX A5000 GPUs.

For data augmentation methods, we provide six widely used methods in time series augmentation . For more details about these six methods, see Appendix A. For the parameter tuning in our method and all baselines, we employ 6 layers for the encoder, set the dimension \(D\) to 128, and the hidden dimension of feed-forward networks to 256. We utilize the Adam optimizer with a learning rate of 1e-4. The batch size is set to {32,32,128,128,128} for datasets APAVA, TDBrain, ADFTD, PTB, and PTB-XL, respectively. The training epoch is set to 100, with early stopping triggered after 10 epochs without improvement in the F1 score on the validation set. We save the model with the best F1 score on the validation set and evaluate it on the test set. See Appendix C for any additional implementation details of our method and all baselines.

### Results of Subject-Dependent

**Setup.** In this setup, the training, validation, and test sets are split based on samples. All samples from all subjects are randomly shuffled and distributed into the training, validation, and test sets according to a predetermined ratio, allowing samples from the same subject to appear in three sets simultaneously. As discussed in the Preliminaries section 3, this setup has limited applicability for MedTS-based disease diagnosis in real-world scenarios. It is usually used to evaluate whether the dataset exhibits cross-subject features quickly. The results obtained from this setup are typically much higher than those from the subject-independent setup, showing a dataset's upper limit of learnability.

**Results.** We evaluate the EEG dataset ADFTD using this setup to provide a direct comparison of results with the subject-independent setup. The results are presented in Table 3. Our method outperforms all the baselines, achieving the top-1 results in all six evaluations, with an impressive F1 score of 97.50%. Notably, baseline methods like Informer, Nonformer, Reformer, and Transformer also demonstrate strong performance, achieving F1 scores exceeding 90%. The overall results indicate the presence of discernible and learnable features related to Alzheimer's Disease within this dataset.

### Results of Subject-Independent

**Setup.** In this setup, the training, validation, and test sets are split based on subjects. All subjects and their corresponding samples are distributed into the training, validation, and test sets according to a predetermined ratio or subject IDs. Samples from the same subjects should exclusively appear in one

    & **Models** & **Accuracy** & **Precision** & **Recall** & **F1 score** & **AUROC** & **AUPRC** \\   & **Autofermer** & 87.83\(\)1.62 & 87.63\(\)1.66 & 87.22\(\)1.97 & 87.38\(\)1.79 & 96.59\(\)0.88 & 93.82\(\)1.64 \\  & **Crossformer** & 89.35\(\)1.32 & 89.00\(\)1.44 & 88.79\(\)1.37 & 88.88\(\)1.40 & 97.52\(\)0.58 & 95.45\(\)1.03 \\  & **FEDformer** & 77.63\(\)2.37 & 77.66\(\)2.17 & 76.68\(\)2.48 & 76.60\(\)2.46 & 91.67\(\)1.34 & 84.94\(\)2.11 \\  & **Informer** & 90.93\(\)0.90 & 90.74\(\)0.71 & 90.50\(\)1.14 & 90.60\(\)0.94 & 98.19\(\)0.27 & 96.51\(\)0.49 \\  & **iTransformer** & 64.90\(\)0.25 & 62.53\(\)0.27 & 62.21\(\)0.26 & 62.25\(\)0.33 & 81.52\(\)0.29 & 68.87\(\)0.49 \\
**MDFTD** & **MTST** & 65.08\(\)0.69 & 63.85\(\)0.80 & 62.71\(\)0.64 & 63.03\(\)0.38 & 81.36\(\)0.56 & 69.34\(\)0.39 \\  & **Nonformer** & 96.12\(\)0.47 & 95.94\(\)0.36 & 95.99\(\)0.38 & 95.96\(\)0.47 & 99.59\(\)0.99 & 99.08\(\)0.16 \\  & **PatchTST** & 66.26\(\)0.40 & 65.08\(\)0.41 & 64.97\(\)0.51 & 64.95\(\)0.42 & 83.07\(\)0.45 & 71.70\(\)0.61 \\  & **Reformer** & 91.51\(\)1.75 & 91.15\(\)1.79 & 91.65\(\)1.56 & 91.14\(\)1.83 & 98.85\(\)0.35 & 97.88\(\)0.60 \\  & **Transformer** & 97.00\(\)0.43 & 96.87\(\)0.53 & 96.86\(\)0.36 & 96.86\(\)0.44 & 99.75\(\)0.04 & 99.42\(\)0.07 \\  & **Medformer (Ours)** & **97.62\(\)0.34** & **97.53\(\)0.33** & **97.48\(\)0.40** & **97.50\(\)0.36** & **98.83\(\)0.05** & **99.62\(\)0.12** \\   

Table 3: **Results of Subject-Dependent Setup. The training, validation, and test sets are split based on samples according to a predetermined ratio. Results of the ADFTD dataset under this setup are presented here.**

    &  &  &  &  &  &  &  \\    & **Autoformer** & 68.64\(\)1.82 & 68.48\(\)2.10 & 68.77\(\)2.27 & 68.06\(\)1.94 & 75.94\(\)3.61 & 74.38\(\)4.05 \\  & **Crosformer** & 73.77\(\)1.95 & 79.29\(\)4.36 & 68.86\(\)1.70 & 68.93\(\)1.85 & 72.39\(\)3.33 & 72.05\(\)3.65 \\  & **FEDformer** & 74.94\(\)2.15 & 74.59\(\)1.50 & 73.56\(\)3.55 & 73.51\(\)3.39 & 83.72\(\)1.97 & 82.94\(\)2.37 \\  & **Informer** & 73.11\(\)4.0 & 75.17\(\)0.60 & 69.17\(\)4.56 & 69.47\(\)5.06 & 70.46\(\)4.91 & 70.75\(\)5.27 \\  & **iTransformer** & 74.55\(\)1.66 & 74.77\(\)2.10 & 71.76\(\)1.72 & 72.30\(\)1.79 & **85.59\(\)1.55** & **84.39\(\)1.57** \\  & **MTST** & 71.14\(\)1.99 & 79.30\(\)0.97 & 65.27\(\)2.28 & 64.01\(\)3.16 & 68.87\(\)2.34 & 71.06\(\)1.60 \\  & **Nonformer** & 71.89\(\)3.81 & 71.80\(\)4.58 & 69.44\(\)3.36 & 69.74\(\)3.84 & 70.55\(\)2.96 & 70.78\(\)4.08 \\  & **PatchTST** & 67.03\(\)1.68 & 78.76\(\)1.28 & 59.91\(\)2.02 & 55.97\(\)3.10 & 65.65\(\)0.28 & 67.99\(\)0.76 \\  & **Reformer** & 78.70\(\)2.00 & **82.50\(\)3.95** & 75.00\(\)1.61 & 75.93\(\)1.82 & 73.94\(\)1.40 & 76.04\(\)1.14 \\  & **Transformer** & 76.30\(\)4.72 & 77.64\(\)4.95 & 73.09\(\)5.01 & 73.75\(\)5.38 & 72.50\(\)6.60 & 73.23\(\)7.60 \\  & **Medformer (Ours)** & **78.74\(\)0.64** & 81.11\(\)0.84 & **75.40\(\)0.66** & **76.31\(\)0.71** & 83.20\(\)0.91 & 83.66\(\)0.92 \\    & **Autoformer** & 87.33\(\)3.79 & 88.06\(\)3.56 & 87.33\(\)3.79 & 87.26\(\)3.84 & 93.81\(\)2.26 & 93.32\(\)2.42 \\  & **Crossformer** & 81.56\(\)2.19 & 81.97\(\)2.25 & 81.56\(\)2.19 & 81.50\(\)2.20 & 91.20\(\)1.78 & 91.51\(\)1.71 \\  & **FEDformer** & 78.13\(\)1.98 & 78.52\(\)1.91 & 78.13\(\)1.98 & 78.04\(\)2.01 & 86.56\(\)1.86 & 86.48\(\)1.99 \\  & **Informer** & 89.02\(\)2.50 & 89.43\(\)2.14 & 89.02\(\)2.50 & 88.98\(\)2.54 & 96.64\(\)0.68 & 96.75\(\)0.63 \\  & **iTransformer** & 74.67\(\)1.06 & 74.71\(\)1.06 & 74.67\(\)1.06 & 74.65\(\)1.06 & 83.37\(\)1.14 & 83.73\(\)1.27 \\  & **MTST** & 76.96\(\)3.76 & 77.24\(\)3.59 & 76.96\(\)3.76 & 76.88\(\)3.83 & 85.27\(\)4.46 & 82.81\(\)5.64 \\  & **Nonformer** & 87.88\(\)2.48 & 88.86\(\)1.84 & 87.88\(\)2.48 & 87.78\(\)2.56 & **97.05\(\)0.68** & **96.99\(\)0.68** \\  & **PatchTST** & 79.25\(\)3.79 & 79.60\(\)4.09 & 79.25\(\)3.79 & 79.20\(\)3.77 & 87.95\(\)4.96 & 86.36\(\)6.67 \\  & **Reformer** & 87.92\(\)2.01 & 88.64\(\)1.40 & 87.92\(\)2.01 & 87.85\(\)2.08 & 96.30\(\)0.54 & 96of these three sets. This setup simulates real-world MedTS-based disease diagnosis, aiming to train a model on subjects with known labels and then test it on unseen subjects to determine if they have a specific disease. The challenges associated with this setup are discussed in section 3. All five datasets are evaluated using this setup.

Results.Table 4 presents the results of the subject-independent setup. Our method achieves the top-1 F1 scores on 4 out of 5 datasets. Overall, our method achieves 15 top-1 and 30 top-3 rankings out of 30 evaluations conducted across 5 datasets and 10 baselines, considering 6 different metrics. Figure 4 provides an overview heatmap table of average rank across 5 datasets on 6 metrics for all methods. Lower rank numbers indicate better results, with rank 1 representing the best performance among all methods. Our method demonstrates the best average rank among all methods across the 6 metrics. Additionally, it is notable that the result for ADFTD is a 50.65% F1 score under the subject-independent setup, which is significantly lower than the 97.50% F1 score achieved under the subject-dependent setup. This comparison highlights the challenge of the subject-independent setup.

### Ablation Study and Additional Experiments

Ablation Study.1) _Module Study_: We conduct a module study to evaluate each proposed mechanism in our method (Appendix D.1). 2) _Patch Length Study_: We perform parameter tuning on the patch lengths to evaluate the effectiveness of multi-granularities (Appendix D.2). **Additional Experiments.** We also perform experiments on two human activities recognition datasets [74; 75] to demonstrate the learning ability of our model on general time series with potential channel correlations (Appendix E).

## 6 Conclusion and Limitations

ConclusionThis paper presents Medformer, a multi-granularity patching transformer tailored for MedTS classification. We introduce three novel mechanisms that leverage the distinctive features of MedTS. These mechanisms include cross-channel patching to capture multi-timestamp and cross-channel features, multi-granularity embedding to learn features at various scales, and a two-stage multi-granularity self-attention mechanism to extract features both within and across granularities. Experimental results across five datasets, evaluated against ten baselines under the subject-independent setup, demonstrate the effectiveness of our method, showing its potential for real-world applications.

Limitations and Future WorkThe design of Medformer allows for inputting various patch lengths, offering opportunities and challenges. While varying patch lengths have been shown to outperform uniform lengths in many cases (see Appendix D.2 and Appendix C), not all patch length combinations yield optimal results. Some combinations may perform worse than uniform patch lengths, necessitating careful tuning of patch lengths. Future work could explore mechanisms for automatically selecting patch lengths. Additionally, developing modules that decompose subject-specific features from task-related features could further enhance learning under the subject-independent setup, presenting an intriguing direction for future research.