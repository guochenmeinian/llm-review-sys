# Harnessing the Power of Choices in Decision Tree Learning

Guy Blanc

Stanford

gblanc@stanford.edu

Authors ordered alphabetically.

Jane Lange1

MIT

jlange@mit.edu

Chirag Pabbaraju1

Stanford

cpabbara@stanford.edu

Colin Sullivan1

Stanford

colins26@stanford.edu

Li-Yang Tan1

Stanford

lytan@stanford.edu

Mo Tiwari1

Stanford

motiwari@stanford.edu

###### Abstract

We propose a simple generalization of standard and empirically successful decision tree learning algorithms such as ID3, C4.5, and CART. These algorithms, which have been central to machine learning for decades, are greedy in nature: they grow a decision tree by iteratively splitting on the best attribute. Our algorithm, Top-\(k\), considers the \(k\) best attributes as possible splits instead of just the single best attribute.We demonstrate, theoretically and empirically, the power of this simple generalization. We first prove a _greediness hierarchy theorem_ showing that for every \(k\), \((k+1)\) can be dramatically more powerful than Top-\(k\): there are data distributions for which the former achieves accuracy \(1-\), whereas the latter only achieves accuracy \(+\). We then show, through extensive experiments, that Top-\(k\) outperforms the two main approaches to decision tree learning: classic greedy algorithms and more recent "optimal decision tree" algorithms. On one hand, Top-\(k\) consistently enjoys significant accuracy gains over greedy algorithms across a wide range of benchmarks. On the other hand, Top-\(k\) is markedly more scalable than optimal decision tree algorithms and is able to handle dataset and feature set sizes that remain far beyond the reach of these algorithms. The code to reproduce our results is available at: [https://github.com/SullivanC19/pydl8.5-topk](https://github.com/SullivanC19/pydl8.5-topk).

## 1 Introduction

Decision trees are a fundamental workhorse in machine learning. Their logical and hierarchical structure makes them easy to understand and their predictions easy to explain. Decision trees are therefore the most canonical example of an interpretable model: in his influential survey , Breiman writes "On interpretability, trees rate an A+"; much more recently, the survey  lists decision tree optimization as the very first of 10 grand challenges for the field of interpretable machine learning. Decision trees are also central to modern ensemble methods such as random forests  and XGBoost , which achieve state-of-the-art accuracy for a wide range of tasks.

Greedy algorithms such as ID3 , C4.5 , and CART  have long been the standard approach to decision tree learning. These algorithms build a decision tree from labeled data in a top-down manner, growing the tree by iteratively splitting on the "best" attribute as measured with respect to a certain heuristic function (e.g., information gain). Owing to their simplicity, thesealgorithms are highly efficient and scale gracefully to handle massive datasets and feature set sizes, and they continue to be widely employed in practice and enjoy significant empirical success. For the same reasons, these algorithms are also part of the standard curriculum in introductory machine learning and data science courses.

The trees produced by these greedy algorithms are often reasonably accurate, but can nevertheless be suboptimal. There has therefore been a separate line of work, which we review in Section 2, on algorithms that optimize for accuracy and seek to produce optimally accurate decision trees. These algorithms employ a variety of optimization techniques (including dynamic programming, integer programming, and SAT solvers) and are completely different from the simple greedy algorithms discussed above. Since the problem of finding an optimal decision tree has long been known to be NP-hard , any algorithm must suffer from the inherent combinatorial explosion when the instance size becomes sufficiently large (unless P=NP). Therefore, while this line of work has made great strides in improving the scalability of algorithms for optimal decision trees, dataset and feature set sizes in the high hundreds and thousands remain out of reach.

This state of affairs raises a natural question:

* Can we design decision tree learning algorithms that improve significantly on the accuracy of classic greedy algorithms and yet inherit their simplicity and scalability?

In this work, we propose a new approach and make a case that provides a strong affirmative answer to the question above. Our work also opens up several new avenues for exploration in both the theory and practice of decision tree learning.

### Our contributions

#### 1.1.1 Top-\(k\): a simple and effective generalization of classic greedy decision tree algorithms

We introduce an easily interpretable greediness parameter to the class of all greedy decision tree algorithms, a broad class that encompasses ID3, C4.5, and CART. This parameter, \(k\), represents the number of features that the algorithm considers as candidate splits at each step. Setting \(k=1\) recovers the fully greedy classical approaches, and increasing \(k\) allows the practitioner to produce more accurate trees at the cost of only a mild training slowdown. The focus of our work is on the regime where \(k\) is a small constant--preserving the efficiency and scalability of greedy algorithms is a primary objective of our work--although we mention here that by setting \(k\) to be the dimension \(d\), our algorithm produces an optimal tree. Our overall framework can thus be viewed as interpolating between greedy algorithms at one extreme and "optimal decision tree" algorithms at the other, precisely the two main and previously disparate approaches to decision tree learning discussed above.

We will now describe our framework. A feature scoring function \(\) takes as input a dataset over \(d\) binary features and a specific feature \(i[d]\), and returns a value quantifying the "desirability" of this feature as the root of the tree. The greedy algorithm corresponding to \(\) selects as the root of the tree the feature that has the largest score under \(\); our generalization will instead consider the \(k\) features with the \(k\) highest scores.

**Definition 1** (Feature scoring function).: _A feature scoring function \(\) takes as input a labeled dataset \(S\) over a \(d\)-dimensional feature space, a feature \(i[d]\), and returns a score \(_{i}\)._

See Section 3.1 for a discussion of the feature scoring functions that correspond to standard greedy algorithms ID3, C4.5, and CART. Pseudocode for Top-\(k\) is provided in Figure 1. We note that from the perspective of interpretability, the trained model looks the same regardless of what \(k\) is. During training, the algorithm considers more splits, but only one split is eventually used at each node.

#### 1.1.2 Theoretical results on the power of Top-\(k\)

The search space of Top-\((k+1)\) is larger than that of Top-\(k\), and therefore its training accuracy is certainly at least as high. The first question we consider is: is the test accuracy of Top-\((k+1)\) only marginally better than that of Top-\(k\), or are there examples of data distributions for which even a single additional choice provably leads to huge gains in test accuracy? Our first main theoretical result is a sharp greediness hierarchy theorem, showing that this parameter can have dramatic impacts on accuracy, thereby illustrating its power:

**Theorem 1** (Greediness hierarchy theorem).: _For every \(>0\), \(k,h\), there is a data distribution \(\) and sample size \(n\) for which, with high probability over a random sample \(^{n}\),_ Top-\((k+1)\) _achieves at least \(1-\) accuracy with a depth budget of \(h\), but_ Top-\(k\) _achieves at most \(+\) accuracy with a depth budget of \(h\)._

All of our theoretical results, Theorems 1 to 3, hold whenever the scoring function is an _impurity-based heuristic_. This broad class includes the most popular scoring functions (see Section 3.1 for more details). Theorem 1 is a special case of a more general result that we show: for all \(k<K\), there are data distributions on which Top-\(K\) achieves maximal accuracy gains over Top-\(k\), even if Top-\(k\) is allowed a larger depth budget:

**Theorem 2** (Generalization of Theorem 1).: _For every \(>0\), \(k,K,h\) where \(k<K\), there is a data distribution \(\) and sample size \(n\) for which, with high probability over a random sample \(^{n}\),_ Top-\(K\) _achieves at least \(1-\) accuracy with a depth budget of \(h\), but_ Top-\(k\) _achieves at most \(+\) accuracy even with a depth budget of \(h+(K-k-1)\)._

The proof of Theorem 2 is simple and highlights the theoretical power of choices. One downside, though, is that it is based on data distributions that are admittedly somewhat unnatural: the labeling function has embedded within it a function that is the XOR of certain features, and real-world datasets are unlikely to exhibit such adversarial structure. To address this, we further prove that the power of choices is evident even for monotone data distributions. We defer the definition of monotone data distributions to Section 4.2.

**Theorem 3** (Greediness hierarchy theorem for monotone data distributions).: _For every \(>0\), depth budget \(h\), \(K\) between \((h)\) and \((h^{2})\) and \(k K-h\), there is a monotone data distribution \(\) and sample size \(n\) for which, with high probability over a random sample \(^{n}\), Top-\(K\) achieves at least \(1-\) accuracy with a depth budget of \(h\), but Top-\(k\) achieves at most \(+\) accuracy with a depth budget of \(h\)._

Many real-world data distributions are monotone in nature, and relatedly, they are a common assumption and the subject of intensive study in learning theory. Most relevant to this paper, recent theoretical work has identified monotone data distributions as a broad and natural class for which classical greedy decision tree algorithms (i.e., Top-\(1\)) provably succeed . Theorem 3 shows that even within this class, increasing the greediness parameter can lead to dramatic gains in accuracy. Compared to Theorem 2, the proof of Theorem 3 is more technical and involves the use of concepts from the Fourier analysis of boolean functions .

Figure 1: The Top-\(k\) algorithm. It can be instantiated with any feature scoring function \(\), and when \(k=1\), recovers standard greedy algorithms such as ID3, C4.5, and CART.

[MISSING_PAGE_FAIL:4]

soft), to understand the classification of a test point, it is sufficient to look at only one root-to-leaf path, as opposed to a weighted combination across many.

## 3 The Top-\(k\) algorithm

### Background and context: Impurity-based algorithms

Greedy decision tree learning algorithms like ID3, C4.5 and CART are all instantiations of Top-\(k\) in Figure 1 with \(k=1\) and an appropriate choice of the feature-scoring function \(\). Those three algorithms all used impurity-based heuristics as their feature-scoring function:

**Definition 2** (Impurity-based heuristic).: _An impurity function\(:\) is a function that is concave, symmetric about \(0.5\), and satisfies \((0)=(1)=0\) and \((0.5)=1\). A feature-scoring function \(\) is an impurity-based heuristic, if there is some impurity function \(\) for which:_

\[(S,i) =(}_{,}[])-_{,}[_{i}=0](}_{,}[_{i}=0])\] \[-_{,}[_{i}=1](}_{,}[_{i}=1])\]

_where in each of the above, \((,)\) are a uniformly random point from within \(S\)._

Common examples for the impurity function include the binary entropy function \((p)=-p_{2}(p)-(1-p)_{2}(1-p)\) (used by ID3 and C4.5), the Gini index \((p)=4p(1-p)\) (used by CART), and the function \((p)=2\) (proposed and analyzed in ). We refer the reader to  for a theoretical comparison, and  for an experimental comparison, of these impurity-based heuristics.

Our experiments focus on binary entropy being the impurity measure, but our theoretical results apply to Top-\(k\) instantiated with _any_ impurity-based heuristic.

### Basic theoretical properties of the Top-\(k\) algorithm

Running time.The key behavioral aspect in which Top-\(k\) differs from greedy algorithms is that it is less greedy when trying to determine which coordinate to query. This naturally increases the running time of Top-\(k\), but that increase is fairly mild. More concretely, suppose Top-\(k\) is run on a dataset \(S\) with \(n\) points. We can then easily derive the following bound on the running time of Top-\(k\), where \((S,i)\) is assumed to take \(O(n)\) time to evaluate (as it does for all impurity-based heuristics).

**Claim 3.1**.: _The running time of Top-\(k(,S,h)\) is \(O((2k)^{h} nd)\)._

Proof.: Let \(T_{h}\) be the number of recursive calls made by Top-\(k(,S,h)\). Then, we have the simple recurrence relation \(T_{h}=2kT_{h-1}\), where \(T_{0}=1\). Solving this recurrence gives \(T_{h}=(2k)^{h}\). Each recursive call takes \(O(nd)\) time, where the bottleneck is scoring each of the \(d\) features. 

We note that any decision tree algorithm, including fast greedy algorithms such as ID3, C4.5, and CART, has runtime that scales exponentially with the depth \(h\). The size of a depth-\(h\) tree can be \(2^{h}\), and this is of course a lower bound on the runtime as the algorithm needs to output such a tree. In contrast with greedy algorithms (for which \(k=1\)), Top-\(k\) incurs an additional \(k^{h}\) cost in running time. As mentioned earlier, in practice, we are primarily concerned with fitting small decision trees (e.g., \(h=5\)) to the data, as this allows for explainable predictions. In this setting, the additional \(k^{h}\) cost (for small constant \(k\)) is inexpensive, as confirmed by our experiments.

The search space of Top-\(k\):We state and prove a simple claim that Top-\(k\) returns the _best_ tree within its search space.

**Definition 3** (Search space of Top-\(k\)).: _Given a sample \(S\) and integers \(h,k\), we use \(_{k,h,S}\) to refer to all trees in the search space of Top-\(k\). Specifically, if \(h=0\), this contains all trees with a height of zero (the constant \(0\) and constant \(1\) trees). For \(h 1\), and \([d]\) being the \(k\) coordinates with maximal score, this contains all trees with a root of \(x_{i}\), left subtree in \(_{k,h-1,S_{x_{i}=0}}\) and right subtree in \(_{k,h-1,S_{x_{i}=1}}\) for some \(i\)._

**Lemma 3.2** (Top-\(k\) chooses the most accurate tree in its search space).: _For any sample \(S\) and integers \(h,k\), let \(T\) be the output of Top-\(k\) with a depth budget of \(h\) on \(S\). Then_

\[_{, S}[T()=]=_{T^{}_{k,h,S}}(_{, S}[T^{}()=]).\]

We refer the reader to Appendix A for the proof of this lemma.

## 4 Theoretical bounds on the power of choices

We refer the reader to the Appendix B for most of the setup and notation. For now, we briefly mention a small amount of notation relevant to this section: we use **bold font** (e.g. \(\)) to denote random variables. We also use bold font to indicate _stochastic functions_ which output a random variable. For example,

\[(x)x&\\ -x&\]

is the stochastic function that returns either the identity or its negation with equal probability. To define the data distributions of Theorems 2 and 3, we will give a distribution over the domain, \(X\) and the stochastic function that provides the label given an element of the domain.

Intuition for proof of greediness hierarchy theoremTo construct a distribution which Top-\(k\) fits poorly and Top-\((k+1)\) fits well, we will partition features into two groups: one group consisting of features with medium correlation to the labels and another group consisting of features with high correlation when taken all together but low correlation otherwise. Since the correlation of features in the former group is larger than that of the latter group unless all features from the latter group are considered, both algorithms will prioritize features from the former group. However, if the groups are sized correctly, then Top-\((k+1)\) will consider splitting on all features from the latter group, whereas Top-\(k\) will not. As a result, Top-\((k+1)\) will output a decision tree with higher accuracy.

### Proof of Theorem 2

For each depth budget \(h\) and search branching factor \(K\), we will define a hard distribution \(_{h,K}\) that is learnable to high accuracy by Top-\(K\) with a depth of \(h\), but not by Top-\(k\) with a depth of \(h^{}\) for any \(h^{}<h+K-k\). This distribution will be over \(\{0,1\}^{d}\{0,1\}\), where \(d=h+K-1\). The marginal distribution over \(\{0,1\}^{d}\) is uniform, and the distribution over \(\{0,1\}\) conditioned on a setting of the \(d\) features is given by the stochastic function \(_{h,K}(x)\). All of the results of this section (Theorems 2 and 3) hold when the feature scoring function is _any_ impurity-based heuristic.

Description of \(_{h,K}(x)\).Partition \(x\) into two sets of variables, \(x^{(1)}\) of size \(h\) and \(x^{(2)}\) of size \(K-1\). Let \(_{h,K}(x)\) be the randomized function defined as follows:

\[_{h,K}(x)=_{h}(x^{(1)})&1-\\ x_{i}^{(2)}[x^{(2)}]&,\]

where \([x^{(2)}]\) denotes the uniform distribution on \(x^{(2)}\). \(_{h}(x^{(1)})\) is the parity function, whose formal definition can be found in Appendix B.

The proof of Theorem 2 is divided into two parts. First, we prove that when the data distribution is \(_{h,K}\), Top-\(K\) succeeds in building a high accuracy tree with a depth budget of \(h\). Then, we show that Top-\(k\) fails and builds a tree with low accuracy, even given a depth budget of \(h+(K-k-1)\).

**Lemma 4.1** (Top-\(K\) succeeds).: _The accuracy of Top-\(K\) with a depth of \(h\) on \(_{h,K}\) is at least \(1-\)._

**Lemma 4.2** (Top-\(k\) fails).: _The accuracy of Top-\(k\) with a depth of \(h^{}\) on \(_{h,K}\) is at most \((1/2+)\) for any \(h^{}<h+K-k\)._

Proofs of both these lemmas are deferred to Appendix B. Theorem 2 then follows directly from these two lemmas.

### Proof of Theorem 3

In this section, we overview the proof Theorem 3. Some of the proofs are deferred to Appendix B.2.

Before proving Theorem 3, we formalize the concept of monotonicity. For simplicity, we assume the domain is the Boolean cube, \(\{0,1\}^{d}\), and use the partial ordering \(x x^{}\) iff \(x_{i} x^{}_{i}\) for each \(i[d]\); however, the below definition easily extends to the domain being any partially ordered set.

**Definition 4** (Monotone).: _A stochastic function, \(:\{0,1\}^{d}\{0,1\}\), is monotone if, for any \(x,x^{}\{0,1\}^{d}\) where \(x x^{}\), \([(x)][(x^{})]\). A data distribution, \(\) over \(\{0,1\}^{d}\{0,1\}\) is said to be monotone if the corresponding stochastic function, \((x)\) returning \((=x)\) where \((,)\), is monotone._

To construct the data distribution of Theorem 3, we will combine monotone functions, Majority and Tribes, commonly used in the analysis of Boolean functions due to their extremal properties. See Appendix B.2 for their definitions and useful properties. Let \(d=h+K-1\), and the distribution over the domain be uniform over \(\{0,1\}^{d}\). Given some \(x\{0,1\}^{d}\), we use \(x^{(1)}\) to refer to the first \(h\) coordinates of \(x\) and \(x^{(2)}\) the other \(K-1\) coordinates. This data distribution is labeled by the stochastic function \(\) given below.

\[(x)_{h}(x^{(1)})&1-\\ _{K-1}(x^{(2)})&.\]

Clearly \(\) is monotone as it is the mixture of two monotone functions. Throughout this subsection, we'll use \(_{h,K}\) to refer to the data distribution over \(\{0,1\}^{d}\{0,1\}\) where to sample \((,)\), we first draw \(\{0,1\}^{d}\) uniformly and then \(\) from \(()\). The proof of Theorem 3 is a direct consequence of the following two Lemmas, both of which we prove in Appendix B.2.

**Lemma 4.3** (Top-\(K\) succeeds).: _On the data distribution \(_{h,K}\), Top-\(K\) with a depth budget of \(h\) achieves at least \(1-\) accuracy._

**Lemma 4.4** (Top-\(k\) fails).: _On the data distribution \(_{h,K}\), Top-\(k\) with a depth budget of \(h\) achieves at most \(+\) accuracy._

## 5 Experiments

Setup for experiments.At all places, the Top-\(1\) tree that we compare to is that given by scikit-learn , which according to their documentation2, is an optimized version of CART. We run experiments on a variety of datasets from the UCI Machine Learning Repository  (numerical as well as categorical features) having a size in the thousands and having \( 50-300\) features after binarization. There were \( 100\) datasets meeting these criteria, and we took a random subset of \(20\) such datasets. We binarize all the datasets - for categorical datasets, we convert every categorical feature that can take on (say) \(\) values into \(\) binary features. For numerical datasets, we sort and compute thresholds for each numerical attribute, so that the total number of binary features is \( 100\). A detailed description of the datasets is given in Appendix C.

We build decision trees corresponding to binary entropy as the impurity measure \(\). In order to leverage existing engineering optimizations from state-of-the-art optimal decision tree implementations, we implement the Top-\(k\) algorithm given in Figure 1 via simple modifications to the PyDL8.5  codebase3. Details about this are provided in Appendix D. Our implementation of the Top-\(k\) algorithm and other technical details for the experiments are available at [https://github.com/SullivanC19/pydl8.5-topk](https://github.com/SullivanC19/pydl8.5-topk).

[https://github.com/SullivanC19/pydl8.5-topk](https://github.com/SullivanC19/pydl8.5-topk).

[https://github.com/SullivanC19/pydl8.5-topk](https://github.com/SullivanC19/pydl8.5-topk).

[https://github.com/SullivanC19/pydl8.5-topk](https://github.com/SullivanC19/pydl8.5-topk).

[https://github.com/SullivanC19/pydl8.5-topk](https://github.com/SullivanC19/pydl8.5-topk).

[https://github.com/SullivanC19/pydl8.5](https://github.com/SullivanC19/pydl8.5)

[https://github.com/SullivanC19/pydl8.5-topk](https://github.com/SullivanC19/pydl8.5-topk).

[https://github.com/SullivanC19/pydl8.5-topk](https://github.com/SullivanC19/pydl8.5-topk).

[https://github.com/SullivanC19/pydl8.5-topk](https://github.com/SullivanC19/pydl8.5-topk).

### Key experimental findings

Small increments of \(k\) yield significant accuracy gains.Since the search space of Top-\(k\) is a superset of that of Top-1 for any \(k>1\), the training accuracy of Top-\(k\) is guaranteed to be larger. The primary objective in this experiment is to show that Top-\(k\) can outperform Top-\(1\) in terms of test accuracy as well. Figure 2 shows the results for Top-1 versus Top-\(k\) for \(k=2,3,4,8,12,16,d\). Eachplot is a different dataset, where on the x-axis, we plot the depth of the learned decision tree, and on the y-axis, we plot the test accuracy. Note that \(k=d\) corresponds to the DL8.5 optimal decision tree. We can clearly observe that the test accuracy increases as \(k\) increases--in some cases, the gain is \(>5\%\) (absolute). Furthermore, for (smaller) datasets like nursery, for which we were able to run \(k=d\), the accuracy of Top-\(8/16\) is already very close to that of the optimal tree.

Lastly, since Top-\(k\) invests more computation towards fitting a better tree on the training set, its training time is naturally longer than Top-1. However, Figure 6 in Appendix E, which plots the training time, shows that the slowdown is mild.

Top-\(k\) scales much better than optimal decision tree algorithms.Optimal decision tree algorithms suffer from poor runtime scaling. We empirically demonstrate that, in comparison, Top-\(k\) has a significantly better scaling in training time. Our experiments are identical to those in Figures 14 and 15 in the GOSDT paper , where two notions of scalability are considered. In the first experiment, we fix the number of samples and gradually increase the number of features to train the decision tree. In the second experiment, we include all the features, but gradually increase the number of training samples. The dataset we use is the FICO  dataset, which has a total of 1000 samples with 1407 binary features. We plot the training time (in seconds) versus number of features/samples for optimal decision tree algorithms (MurTree, GOSDT) and Top-\(k\) in Figure 3. We do this for depth \(=4,5,6\) (for GOSDT, the regularization coefficient \(\) is set to \(2^{-}\)). We observe that the training time for both MurTree and GOSDT increases dramatically compared to Top-\(k\), in both experiments. In particular, for depth \(=5\), both MurTree and GOSDT were unable to build a tree on 300 features within the time limit of 10 minutes, while Top-\(16\) completed execution even with all 1407 features. Similarly, in the latter experiment, GOSDT/MurTree were unable to build a depth-5 tree on 150 samples within the time limit, while Top-\(16\) comfortably finished execution even on 1000 samples. These experiments demonstrates the scalability issues with optimal tree algorithms. Coupled with the accuracy gains seen in the previous experiment, Top-\(k\) can thus be seen as achieving a more favorable tradeoff between training time and accuracy.

We note, however, that various optimization have been proposed to allow optimal decision tree algorithms to scale to larger datasets. For example, a more recent version of GOSDT has integrated a guessing strategy using reference ensembles which guides the binning of continuous features, tree

Figure 2: Test accuracy comparison between Top-\(k\) for various values of \(k\). We can see that Top-\((k+1)\) generally obtains higher accuracy than Top-\(k\), and in some cases (e.g., nursery), Top-\(8/16\)â€™s accuracy is even comparable to the optimal tree (Top-\(d\)). Missing points in the plots correspond to settings that did not terminate within a sufficiently large time limit. All plots are averaged over 10 random train-test splits (except avila and ml-prove that have pre-specified splits) with confidence intervals plotted for 2 standard deviations.

size, and search [MZA\({}^{+}\)]. Many of these optimizations are generally applicable across optimal tree algorithms and could be combined with Top-\(k\) for further improvement in performance.

Increasing \(k\) beyond a point does not improve test accuracy.In our experiments above, we ran Top-\(k\) only till \(k=16\): in Figure 4, we show that increasing \(k\) to very large values, which increases runtime, often does not improve test accuracy, and in some cases, may even _hurt_ due to overfitting. For 3 datasets - car, hayes-roth and tic-tac-toe - we plot train and test error as a function of \(k\). Naturally, the train accuracy monotonically increases with \(k\) in each plot. However, for both car and hayes-roth, we can observe that the test accuracy first increases and then plateaus. Interestingly, for tic-tac-toe, the test accuracy first increases and then _decreases_ as we increase \(k\). These experiments demonstrate that selecting too large of a \(k\), as optimal decision tree algorithms do, is a waste of computational resources and can even hurt test accuracy via overfitting.

## 6 Conclusion

We have shown how popular and empirically successful greedy decision tree learning algorithms can be improved with the power of choices: our generalization, Top-\(k\), considers the \(k\) best features as candidate splits instead of just the single best one. As our theoretical and empirical results demonstrate, this simple generalization is powerful and enables significant accuracy gains while preserving the efficiency and scalability of standard greedy algorithms. Indeed, we find it surprising that such a simple generalization has not been considered before.

There is much more to be explored and understood, both theoretically and empirically; we list here a few concrete directions that we find particularly exciting and promising. First, we suspect that power

Figure 4: Test accuracy plateaus for large \(k\). All runs averaged over 10 random train-test splits with maximum depth fixed to 3.

Figure 3: Training time comparison between Top-\(k\) and optimal tree algorithms. As the number of features/samples increases, both GOSDT and MurTree scale poorly compared to Top-\(k\), and beyond a threshold, do not complete execution within the time limit.

of choices affords more advantages over greedy algorithms than just accuracy gains. For example, an avenue for future work is to show that the trees grown by Top-\(k\) are more robust to noise. Second, are there principled approaches to the automatic selection of the greediness parameter \(k\)? Can the optimal choice be inferred from a few examples or learned over time? This opens up the possibility of new connections to machine-learned advice and algorithms with predictions , an area that has seen a surge of interest in recent years. Finally, as mentioned in the introduction, standard greedy decision tree algorithms are at the very heart of modern tree-based ensemble methods such as XGBoost and random forests. A natural next step is to combine these algorithms with Top-\(k\) and further extend the power of choices to these settings.