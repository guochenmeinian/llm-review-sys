# Information-guided Planning: An Online Approach

for Partially Observable Problems

 Matheus Aparecido do Carmo Alves

Lancaster University

Lancaster, United Kingdom

m.a.docarmoalves@lancaster.ac.uk

&Amokh Varma

Indian Institute of Technology

Delhi, India

amokhvarma@gmail.com

Yehia Elkhatib

University of Glasgow

Glasgow, United Kingdom

yehia.elkhatib@glasgow.ac.uk

&Leandro Soriano Marcolino

Lancaster University

Lancaster, United Kingdom

l.marcolino@lancaster.ac.uk

###### Abstract

This paper presents IB-POMCP, a novel algorithm for online planning under partial observability. Our approach enhances the decision-making process by using estimations of the world belief's entropy to guide a tree search process and surpass the limitations of planning in scenarios with sparse reward configurations. By performing what we denominate as an _information-guided planning process_, the algorithm, which incorporates a novel I-UCB function, shows significant improvements in reward and reasoning time compared to state-of-the-art baselines in several benchmark scenarios, along with theoretical convergence guarantees.

## 1 Introduction

Decision-making agents are increasingly being used under uncertainty in nontrivial systems . Based on the available information, such agents must evaluate potential decisions in the current environment to build a robust plan that either accomplishes the task or leads to a better situation. Consider, for instance, a scenario in which drones are deployed to rescue people in a hazardous environment. Drones need to quickly identify critical locations where they can offer support and save lives. If no clear target is found, they must rapidly formulate a plan based on their current knowledge and observable information to find an area in need of assistance from their current position.

The above context describes a partial observability problem that is recurrent in the literature . The Partially Observable Monte-Carlo Planning (POMCP)  algorithm is commonly suggested as a method to address these problems because it enables agents to perform planning in an online manner while handling uncertainty . However, several state-of-the-art solutions that rely on POMCP often struggle when rewards are delivered sparsely in time or outside their reasoning horizon. For example, in foraging, there may be no clear target to spot from the current observation and/or estimated map so far. Here, we find a fundamental academic challenge: How can we improve agent performance when rewards are delivered sparsely in time or out of their reasoning horizon?

Every time an agent makes an observation of the world, it gains information that can be used to improve its internal planning process. Based on this, some works improve the traditional planning process by handling the lack of prior information using the approximation of the world's dynamic models , embedding supportive planning methods within the tree , extracting non-explicit information from the observations , enhancing the method's inner calculations  or employing neural networks-based techniques to improve the search quality .

Overall, these strategies can improve search quality within a defined simulation horizon by using extensions of traditional partially observable models and enabling the integration of additional knowledge into planning. However, such knowledge may not be available to agents beforehand (e.g., conditional observation probabilities), or may demand significantly more computational resources to perform the task (e.g., time and memory). While it is important to note that incorporating more knowledge benefits efficiency in terms of objective accomplishment, it does not resolve challenges in sparse-reward scenarios, as most of the solutions remain reward-driven.

Hence, we present _Information-based POMCP (IB-POMCP)_, an online planning algorithm that uses a novel _information framework_ to boost the decision-making process under partial observability even when no rewards are available in the reasoning horizon. Our framework refines the traditional UCB1 action selection strategy by implementing our proposed _Information-guided UCB (I-UCB)_ function, which is capable of leveraging entropy and estimating information gain, using both real-world and internally generated observations to identify promising states in the search process that leads the agent to quickly accomplish its objective. Its application together with our new particle filter reinvigoration strategy, which considers the current system's entropy to calibrate the reinvigoration, indicates that this approach may lead the agent to act hybridly when solving a partially observable problem by better weighing the exploration, exploitation, and information levels within the tree. We ran experiments across five benchmarks and compared them with state-of-the-art baselines, obtaining significantly higher average rewards (up to 10 times) while performing faster reasoning (up to 93%).

## 2 Related Work

In the current state-of-the-art, POMCP  remains a relevant solution due to its adaptability and problem-solving capability across the most diverse domains [3; 23; 24; 41]. However, many proposals are tailored to specific problems, leaving room for improvement, especially when handling sparse reward scenarios. Our approach addresses this gap by shifting POMCP's reward-based planning to our "_information-guided planning strategy_", capable of leveraging observation space entropy (information) and guiding the agent to promising spots from where it solves such scenarios.

Other methods suggest approximating the latent world functions to handle the lack of knowledge [11; 13; 19; 35]. However, these approaches are directly bounded by the belief in space dimensionality, which grows exponentially and hence requires exponentially more resources as the complexity of the problem increases . We propose the extraction of information using only the observations received from the simulations, improving the reasoning without requiring significantly more resources.

Some studies recommend belief-dependent POMDP models to improve POMCP's reasoning capabilities [4; 21]. Under certain assumptions, these models have shown great efficiency when solving problems with large action and observation spaces. A recent advancement is \(\)-POMCP , an efficient online planner that boosts the POMCP's search process by estimating belief-dependent rewards. However, this method relies on explicit access to the observation and transition function, struggling while reasoning under time constraints. TB \(\)-POMCP  tries to handle scenarios with limited time to reason, penalising efficiency in order to do so. IB-POMCP can improve planning using less information and without significant penalties for reasoning time.

Time-constrained problems are prevalent in the evaluation of online planning algorithms. In real-time strategy games, a common strategy is to use macro and micro predefined coordination scripts to support the decision-making [25; 40]. Even though they handle several levels of coordination, these methods still require behavioural templates to plan. Similar problems arise in the multi-agent swarm research  where they avoid developing a complex planning process in order to save time (using, for example, Flat Monte Carlo approaches). We propose the execution of efficient planning under partial observability from scratch and without penalising the planning quality.

The Upper Confidence Bound (UCB1)  is a relevant algorithm in the literature for evaluating and selecting actions in online planning solutions and is recurrently used inside Monte-Carlo decision-making frameworks [15; 29; 35]. While we acknowledge the existence of other action selection methods available in the literature, such as the Thompson Sampling method  and the PUCT algorithm , in this paper, we focus on augmenting the POMCP's capabilities by modifying the UCB1. Differently from PUCT (_AlphaGo_) , for example, our proposed _I-UCB_ well-fits partially observable scenarios while enhancing POMCP planning capabilities through an entropy-based perspective, without relying on pre-trained models or a large amount of data.

Recently, MENTS and ANTS improved the Monte-Carlo Tree Search (MCTS) performance using maximum entropy policy optimisation networks [18; 37]. In contrast to both works, we focus our study on partially observable models and, instead of directly optimising latent functions, we improve planning by embedding knowledge into the search process. Furthermore, both methods rely on prior problem-specific knowledge and data to train neural networks, including information on the maximum episode length, which helps them handle issues like sparse rewards. However, we assume that the agent must reason under a limited number of steps (often lower than necessary to accomplish the problem) and execute the problem from scratch, carrying no knowledge between executions.

Finally, although we focus on improving POMCP, our contributions can also benefit other algorithms, such as DESPOT-based solutions . DESPOT's main focus is on filtering a large observation space to plan using a sparse approximation of the problem, hence, a smaller reasoning space. However, the algorithm still struggles in large action spaces, and/or sparse reward scenarios where the optimal policy could be long (requiring many steps). Our proposal addresses POMDPs featuring sparse rewards in an efficient manner while running the search process from scratch.

## 3 Background

**Markovian Models -** The Markov Decision Process (_MDP_) is a mathematical framework to model stochastic processes in a discrete-time flow. The model specifies that each process is composed of a set of states \(\), a set of actions \(\), a reward function \(:\), and a transition function \(\). On the other hand, the Partially Observable MDP (_POMDP_) extends the MDP representation to problems in which the agent cannot directly access the current state. Instead, the agent receives an observation \(z\), determined by observation probabilities \(^{a}_{s,z}=P(z_{t+1}=z s_{t+1}=s;a_{t}=a)\). The belief function \((h)\) is a distribution over the state space \(\), which describes the conditional probability of being in a particular state \(b\) given the current history \(h\), \((h,b)=P(s_{t}=b h_{t})\), where the history \(h\) is a sequence of action-observation pairs \(h_{t}=\{a_{0},z_{0},,a_{t},z_{t}\}\). The decision process follows a policy \((h,a)=P(a_{t+1}=a|h_{t})\), while choosing actions.

**Monte-Carlo Tree Search (MCTS) -** MCTS algorithms aim to determine the optimal action \(a^{*}\) for a given state \(s\) by simulating the world steps within a tree structure. Each node in the Monte-Carlo tree \(\) is represented by (\(s,\ ,\ \)), i.e., a tuple with a _state_\(s\), a _value_\((s,\ a)\), and a _visitation count_\((s,\ a)\) for each action \(a\). The _value_ of the node represents the expected cumulative reward for the simulated states. The number of visits to the state \(s\) is represented by \((s)=_{a}(s,a)\). The simulations are divided into two main stages: _search_ and _rollout_. Each state in the search tree is viewed as a multi-armed bandit taking actions usually chosen by the Upper Confidence Bound (UCB1) algorithm. _UCB1 is a well-known_ algorithm that tries to increase the value of less-explored actions by attaching a bonus inversely proportional to the number of times each action is tried, following the update-equation \(UCB1(s,a):=(s,a)+c(s))}{(s,a)}}\). The scalar \(c\) is the exploration constant responsible for weighting the exploration value within the UCB1 function. With a correct definition of \(c\), the value function converges in probability to the optimal value, \((s,a)}^{*}(s,a)\).

**Partially Observable MC Planning (POMCP) -** POMCP is an extension of MCTS for partially observable problems. Analogously to it, each node is represented by a tuple (\(h\), \((h)\), \((h)\), \(_{h}\)) and the algorithm is divided into the search and rollout stages. In contrast, _POMCP_ uses an unweighted particle filter \(_{h}\) to approximate the belief state at each node \(h\). This strategy requires a _Monte-Carlo simulator_\(\), which can sample a state \(s^{}\), reward \(r\) and observation \(z\) given a state-action pair. Recursively down the tree, all sampled information is added to a _particle filter_\(\). At the end of the search stage, the best action is chosen and a real observation is received from the environment. Considering the current action-observation pair, we move to the new root node and perform the _particle reinvigorating process_, which is responsible for updating and boosting the new root's particle filter belief state approximation. After that, we continue the planning procedure (starting the search process) as described above. We refer the reader to Silver and Veness (2010) for further detail.

**Shannon entropy in Information Theory -** A traditional approach to measure uncertainty in stochastic processes is to use Shannon's Entropy function \((X)=-_{i=1}^{n}P(x_{i})(P(x_{i}))\), where \(X\) is a discrete random variable and \(x_{i}\) are the possible outcomes from \(X\). The core idea is to find a reliable measure for "information", that is, to calculate the degree of _surprise_ of an event given the available space of possibilities. If an event often occurs, the information inside the event is likely not "novel". Hence, there is no "surprise" when it occurs. On the other hand, if an event rarely occurs, the "surprise" is greater. Comparing both situations, we can follow events with higher surprise levels to explore the space of possibilities, and safely follow events with lower surprise levels to exploit the environment, hence, we can adapt an agent's behaviour according to the entropy of the current state.

## 4 Information-based Partially Observable Monte Carlo Planning

**Algorithm Outline -** Our algorithm starts with the _(i) initialisation of our tree structure_, where we create our root and calculate the probability of stepping into this node given our current knowledge. Since the initial knowledge about the problem is assumed to be none, we can not calculate this probability, hence, we initialise it as 0. Subsequently, we _(ii) initialise the particle filter of our root node_, generating possible states from which we can simulate the agents' actions and search for a solution to the target problem. Initially, the beliefs are generated through a uniform distribution.

Our search process is iterative, as commonly found in the literature [32; 39; 2], where we sample a state from our current belief to perform multiple rounds of simulation. However, unlike the typical approach, we propose the implementation of an adaptive exploration coefficient and of our novel I-UCB function instead of the usual UCB1. Therefore, at every iteration of our search (before starting a simulation), we first perform our _(iii) exploration coefficient adaptation_, adjusting it using the entropy (level of information) estimated over the set of observations collected through the simulations. Then, with the updated coefficient in hands, we start our _(iv) simulation with the I-UCB function_. While choosing actions, our proposal searches for solutions and takes actions based on observations' entropy besides the collection of rewards. At the end of each simulation, we re-update the exploration coefficient based on the information gained during the path traversal. When we finish the search, we _(v) select the best action_ based on the estimated rewards, entropy, and the number of visits of each action's node to determine the most promising path to follow in the real environment.

After taking an action and receiving a new real observation from the environment, we restart the IB-POMCP algorithm. However, the initial steps are slightly changed to maintain the knowledge and update the current information, i.e., perform online planning. Unlike the first iteration, we now _(i) update the root node_ by traversing the tree based on the action taken and the most recent observation from the environment. Consequently, we recalculate the probability of stepping into the new root node using the information available in the tree. With the probability in our hands, we now _(ii) update the particle filter of our root node_ by reinvigorating our belief, generating new states using the uniform distribution while maintaining promising states for simulation. Then, after performing these updates, we restart the search and repeat the whole process. Now, let us discuss these steps in detail.

**(i) Root Initialisation and Update -** This step is responsible for maintaining the tree structure **T** coherent with the world and the information gained while performing online planning. Consider \(\) as the tree **T**'s root node, \(h_{}\) as the history of the root \(\) (our current history), \(a\) as the action taken by the agent in the real world and \(z\) as the most recent observation received from the real world (after the agent's action). **As in POMCP**, this step considers three possible procedures:

\(\)**Initialisation:**_if **T** does not exist_, we create a root node \(\) using the current history \(h_{}\), else;

\(\)**Update:**_if **T** exists and the nodes \(h_{}a\) and \(h_{}az\) also exist,_ we walk in the existing tree, traversing the nodes that correspond to the actions taken by the agent \(a\) and the most recent world observation \(z\), and update the root, i.e., the node \(h_{}az\) becomes the new \(\), hence, \(h_{}=h_{}az\), else;

\(\)**Re-initialisation:**_if **T** exists but, node \(h_{}a\) or \(h_{}az\) does not exist_, the tree is re-initialised. That is, we create a new node \((h,(h),(h),_{h})\) with \(h\) as the current history, \((h)=0\), \((h)=0\) and \(_{h}\) is empty. Afterwards, we assign this new node as our new root \(\) of **T**, which means that all other simulations already made in the agent's head are discarded and we restart the algorithm.

**In contrast to POMCP**, we _estimate the current probability_\(P(z|h_{}a)\), in this step, when finding the observation \(z\) after taking the action \(a\) considering our history \(h_{}\). Overall, we aim to use this probability to enhance the Particle Filter Update (see step (ii)) procedure by weighting its diversification and reinvigoration levels according to the agent's uncertainty when stepping into the new root - which would be the same as calculating the agent's "surprise" on finding the received observation in the real world after taking the chosen action. However, we consider that a compact representation of transition \(\) or observation probabilities \(\) may not be available for complex problems. Hence, we propose the approximation of this probability using the knowledge within our particle filter \(_{}\) (without requiring a POMDP explicit model) by \(P(z|h_{}a)(z|h_{}a)= (h_{}az)}{(h_{}a)}\)where \((h_{}az)\) and \((h_{}a)\) are the number of visits of node \(h_{}az\) and \(h_{}a\), respectively (the new root node and its parent). If \(h_{}\) is empty or no new root node is found, \((z|h_{}a)=0\). Note also that \((haz)(ha), h\) and, consequently, \((haz)/(ha)\).

**(ii) Particle Filter Initialisation and Update -** This process is responsible for initialising the particle filter of the root node \(_{}\) (if it is empty or does not exist) or performing the particle reinvigoration process based on the probability \((z|h_{}a)\) calculated in the Root Update process (step (i)). Directly, the initialisation is made through the sampling of \(k\) particles (which generate the observation \(z\)) from the uniform distribution \(_{z}\). On the other hand, the update considers the \((z|h_{}a)\) as the weight that balances the particle reinvigorating process over \(_{}\). The idea is to diversify (or boost) the new root's particle filter as a reliable approximation of the belief state function. If the probability of stepping into this new root is high, i.e., \((z|h_{}a)\) is high, we assume that the particles in the new root's particle filter will well approximate the real world, since through the simulations we recurrently found the observation \(z\) after taking the action \(a\) from the last root node \(h\). In contrast, when this probability is low, we diversify the new root's particle filter \(_{}\) by uniformly generating particles that may better represent the real world using \(_{z}\) instead of \(_{}\). Consequently, coherent with the above rationale, we reinvigorate the new root's particle filter \(_{}\) by maintaining \( k(z|h_{}a)\) particles sampled from itself, i.e., \(_{haz}\), and sampling new \( k(1-(z|h_{}a))\) particles from the uniform distribution \(_{z}\). This update on the new root's particle filter may offer a better start for the algorithm when performing the search process since the sampling of belief states may consider an enhanced set of particles through our uncertainty-guided reinvigoration process. We kindly refer the reader to Appendix A for further discussion and pseudo-code to implement this procedure.

**(iii) Updating the Tree Exploration Coefficient -** To explain how we update our tree exploration coefficient, we first introduce the _(a) adaptation made to the traditional exploration coefficient_, then we present our _(b) modified entropy function_, which is used in the update, we explain how to calculate it and, in the end, we show our _(c) strategy to normalise the entropy_ in the online planning context.

_(a) Exploration Coefficient Adaptation -_ Before the actual tree simulation process, IB-POMCP first adjusts the value of our tree's exploration parameter based on the estimated entropy for the current system (the tree) in the root level \(\) with history \(h_{}\). Directly, our approach considers the replacement of the traditional \(c\) constant by the function \((1-(h_{}))\), which we define as:

\[(h_{}):=(h_{}))}{( h_{})}^{(h_{})}_{i}(h_ {})}{(h_{})_{i=1}^{(h_{})}_{i}(h_{})} \]

Our insight is to use \(\) to augment the current uncertainty level in the tree's search policy. On the left-hand side of the multiplication, we design a function that represents the chance of finding new information by exploring a node, which decreases as the number of visits to the node increases. \(e\) is the Euler's constant, which will be used as our amortisation factor for \(((h_{}))/(h_{})\). Applying both together, we can describe a function that slowly decreases and maintains, in the infinity and under some assumptions, theoretical guarantees for belief approximation (see Section 5). The right-hand side of the multiplication expresses the "_general surprise trend_", which is calculated through the division between the actual cumulative entropy \(_{i=1}^{(h_{})}_{i}(h_{})\) and the estimated maximum cumulative entropy \((h_{})_{i=1}^{(h_{})} _{i}(h_{})\), which is the multiplication of the total number of visits to the node and the maximum entropy. By multiplying both sides of the equation, we can estimate the current uncertainty of our system, in this case, of our tree. In addition to improving reasoning, we discard the need for prior knowledge about the problem of adjusting and fixing the tree constant \(c\). Note that we adjust \((h_{})\) for each traversal on the tree, i.e., from the root \(\) to some leaf.

_(b) Entropy Calculation -_ We adapt Shannon's Entropy equation to measure the level of information in the IB-POMCP's search process based on the collection of observations of our agent while performing Simulations (see step (iv)), which is designed as \((h)=-_{z}}P_{h}(z)(P_{h}(z))\), where \(P_{h}(z)\) is the probability of finding the observation \(z}\) by simulating actions from the node with history \(h\). We use \(}\), which is an estimated set of observations, in the calculation of the entropy because we consider that a compact representation of the full observations space \(\) may not be available.

In order to build \(}\) as a reliable estimation of \(\), we collect all the observations found during _all_ traversals within the tree and save them first as the _multiset_\(}^{m}\) - avoiding losing information, e.g., the frequency of the observations - and then we translate it as the _set_\(}\) when necessary.

Each node has its own estimated multiset of observation \(}_{h}^{m}\). Every time we visit a node \(h\), we update the \(}_{h}^{m}\) using the collected observation information, which is saved and back-propagated as \(}_{t}^{m}\). On the other hand, \(}_{t}^{m}\) is the multiset that saves the observations from a single traversal starting from the node at the tree level \(t\) to the maximum length of the path \(D\). Therefore, each node \(h\) has its own multiset that saves all possible observations to be found by performing a simulation from it. Formally, we can define \(}_{h}^{m}=_{i=1}^{(h)}}_{t, i}^{m}\), where \(}_{t,i}^{m}\) is the back-propagated \(}_{t}^{m}\) at \(i\)-th visit to the node, and \(}_{t,i}^{m}=}_{t+1,i}^{m} z_{t}, t =0,1,...,D\). Note that \(}_{t,i}^{m}=, t>D\). Let's put it as an example:

Consider \(h=h_{0}\) as the root node, \(D=3\) and a single traversal in the tree. Under the back-propagation perspective, we start our update from the last node visited, related to \(}_{t+3}^{m}\). Following our definition, \(}_{t+3}^{m}=\{z_{t+3}\}\) and, since it is a leaf node, only \(z_{t+3}\) will be included in \(}_{h_{3}}^{m}\) as a new possible observation to be found from node \(h_{3}\). Now, in \(}_{t+2}^{m}\), we will add the \(z_{t+2}\) to our back-propagation multiset \(}_{t+3}^{m}\) and, consequently, we will add \(\{z_{t+3},z_{t+2}\}\) to \(}_{h_{2}}^{m}\). We repeat this process until we reach \(h_{0}\), where we include all found observations \(}_{t}^{m}=}_{t+1}^{m} z_{0}=\{z_{3},z_{2},z_{1},z_{0}\}\) to the root multiset \(}_{h}^{m}\). Figure 1 illustrates this example and our proposed observation back-propagation procedure. Figures 0(a) and 0(d) present a high-level perspective of the process, showing only the observation nodes of the tree to facilitate visualisation. Figures 0(b) and 0(c) present a closer perspective of the process as a one-step execution.

The red particles are states and the blue ones are observations. With \(}_{h}^{m}\) in hands, we now can approximate \(P_{h}(z)\) by calculating the frequency of the observation \(z\) in \(}_{h}^{m}\), following \(P_{h}(z)_{h}(z)=}_{h}^{m}|}_{z^ {}}_{h}^{m}}1_{\{z^{}=z\}}\). Therefore, our final entropy function will be \((h)=-_{z}_{h}}_{h}(z)(_{h}(z))\). We use this defined entropy to calculate the current entropy of the system. However, since IB-POMCP plans in an online manner and the \(}_{h}^{m}\) is updated at each visit to the node, we propose the estimation of our tree's entropy using the update-equation:

\[_{(h)}(h):=_{(h)-1}(h)+(h)-_{(h)-1}(h))}{(h)} \]

where \((h)\) is the number of visits to the node \(h\), \((h)\) is the above defined entropy calculation, and \(_{i}(h),i=1,2...,(h)\) is the estimated entropy at the visit \(i\). Note that, by following this final definition for the entropy, we can also calculate \(\) (Equation 1) and update it in an online fashion.

_(c) Entropy Normalisation_ - Another issue that arises is that, since we may not have access to the true observation distribution in advance, our current approach will result in entropy calculations that are not standardised under the same image function. This problem occurs because the size of the set \(}\) grows every time an observation never found before is delivered by the environment. As a consequence, the maximum value for the entropy also grows, and nodes that receive a higher number of different observations will calculate higher values for \((h)\), which can create bias by including these overestimated values in the decision-making process. Therefore, we propose its normalisation _a posteriori_, following: \(}(h):=_{(h)}(h)}{_{j=1}^{ (h)}_{j}(h)}\), where \(_{j}(h)\) represents the entropy calculated at the \(j\)-th visit to the node \(h\). This approach guarantees \(}(h), h\), i.e., for all nodes. Moreover,

Figure 1: Illustration of IB-POMCPâ€™s search process.

\((h)=1\) when \((h)=0\) by definition, since we consider that when there is no information, the uncertainty is maximised, hence, the entropy is also maximised.

**(iv) Simulation -** After adjusting our \(\) function, we start the simulation process. While expanding the tree, we propose a modified version of UCB1 to guide the tree search process based on the information level of each node, the _I-UCB_ function, which follows:

\[(h,a,):=(ha)+(1-)(h))/ (ha)}+}(ha) \]

Our proposal intends to guide the expansion and planning process based on entropy updates that occur within the tree, defining what we denote as the _information-guided planning process_. In this type of planning, a trade-off between exploration and exploitation still exists, but now we also bring information value to the discussion. Our main novelty here moves past the traditional idea, where the more we explore, the better we can exploit the rewards, thereby improving the agent's performance. Instead, we expand the discussion and enhance the algorithm capabilities by introducing an information-gain perspective to the tree-planning process using entropy calculations.

Intuitively, our action selection function considers that **(a)** if the system uncertainty is high (hence, \(\) value is high), we weight the action selection based on the advantage of information gain (which may expand the tree in depth), trying to decrease the entropy value and accumulate knowledge, or; **(b)** if the system uncertainty is low (hence, \(\) value is low), we weight the action selection in the advantage of the exploration gain (which may expand the tree in breadth), trying to increase the entropy value and increase the number of possibilities to reason over. Therefore, I-UCB performs adaptive planning that also depends on the system entropy, besides the upper confidence estimations.

Another direct advantage of the application of I-UCB instead of UCB1 is that our method rarely fails to deliver an action justified by metrics, that is, it rarely delivers a randomly chosen action as the best one. Usually, this issue arises in problems where rewards are sparsely distributed over a long horizon of actions, which can lead to several ties in the Q-values (e.g., at zero). Proposing a solution that works under these conditions requires **(a)** to increase the reasoning horizon of the solution, which usually leads to the expenditure of significantly more computational resources, or **(b)** the capability to handle it within the available reasoning horizon, which is a non-trivial task. IB-POMCP solves this problem by following a non-trivial solution considering **(a)** the inclusion of a novel metric for evaluation - the system entropy, which frequently is non-zero since the observation distribution is diverse and; **(b)** the adaptive value of alpha, which often promotes an action branch to find (at least) a non-zero value (reward or entropy) or to be frequently visited during the reasoning process.

**(v) Best action selection -** we decide the best action by picking the action with the highest score using \(a_{best}=argmax_{a}\ (1-)(ha)+}(ha)\). If there is a tie, we break it using the number of visits for each possible node. If it persists, we run a random policy to select the "best" action. Because of the proposed entropy calculation (and in contrast to POMCP), IB-POMCP has the ability to choose actions even without experiencing non-zero rewards while planning.

Lastly, considering the above discussion on adapting and inserting \(\) inside I-UCB, we consider scaling \(\) value to be within an interval \([q,1-q],0<q<0.5\), as a _practical_ enhancement. This strategy allows our method to avoid ignoring part of the I-UCB result by multiplying one term by zero. This step will not be considered in the theoretical analysis.

**Algorithm/Pseudo-code:** To highlight the difference between POMCP and IB-POMCP, we present the complete pseudocode of our algorithm in Appendix A1.

## 5 Theoretical Analysis

In this section, we analyse IB-POMCP's capability to plan optimally in partially observable finite horizon problems and \(\)-optimally for infinite horizon partially observable problems. For the complete proofs see Appendix B. We start with the following assumption:

**Assumption 1**: _Given a generic \(POMDP\) represented by the tuple \((,,,,,,)\), the \(\) is bounded and there exists a positive maximum reward value \(r_{max}\)._Under this simple assumption and to establish the IB-POMCP's planning capabilities, we must examine our action selection procedure, specifically through the application of I-UCB. Consequently, we first analyse how the \(\) coefficient behaves and impacts the IB-POMCP's search process:

**Lemma 1**: \((h)\) _converges to \(0\) as the number of visits \((h)\) approach the infinity._

**Theorem 1**: _For any non-empty history \(h\) and \( a\), as \((h)\), \((ha)\), we have that all states \(b\) in \((h,b)\) which \(P(s=b h)>0\) will be simulated infinitely many times._

Using the previous results, we can show the convergence for a finite horizon:

**Theorem 2**: _IB-POMCP's nodes converge to the optimal value for a fixed horizon \(D\) as the number of search iterations goes to infinite (i.e., \((h_{})\))._

We now move on to the infinite horizon case. Consider \(^{*}(h_{})\) as the optimal value function for the root \(\) and \(^{*}_{D}(h_{})\) as the optimal value function for the root \(\) for a finite horizon D. We find that:

**Theorem 3**: _Given any \(\), there is a finite horizon \(D\) for the problem, such that \(|^{*}(h_{})-^{*}_{D}(h_{})|\), where \(h_{}\) represents the root of the tree._

Given the previous results, we can prove the following for the convergence in the infinite horizon:

**Corollary 1**: _IB-POMCP converges to \(\)-optimal actions at the root (\(h_{}\)) in a \(\)-discounted infinite horizon scenario._

## 6 Results

**Evaluation Settings -** We define five well-known domains as our benchmarks. See Appendix C for more information. **(i) The Tiger (T0)** environment presents an agent _must choose between two doors: one hides a treasure; the other, a tiger._ The agent can decide which door to open or listen to before making the final decision. **(ii) The Maze (M0-M3)** environment , which is an _Active Localisation Problem_ in which an agent navigates through a toroidal grid and _tries to localise its own position_ by collecting local information. The reward is based on the entropy of the current belief. **(iii) The RockSample (R0-R3)** problem  considers that a robot has to navigate through a grid-world and _maximise the number of good rocks it picks up and analyses instead of bad rocks_. If it picks up (samples) a bad rock, it gets a negative reward. Otherwise, it gets a positive reward and the good rock becomes bad. **(iv) The Tag/LaserTag (LT0-LT1)** environment , where _an agent is trying to find and tag a target opponent agent that intentionally moves away_. The agent only knows its own position but can observe the target's position if they are in the same position (Tag) or using an 8-directional laser (in LaserTag). If the agent successfully tags the target it is rewarded, otherwise it is penalised. **(v) The Foraging (F0-F4)** problem is a famous scenario to evaluate online planning algorithms  that presents _an agent that collects items displaced in a rectangular grid world_.

All experiments were implemented using _AdLeap-MAS_ because it implements all five environments and enables the quick execution of our experiments. Each run was performed in a single node of a high-performance cluster containing 16 cores of Intel Ivy Bridge processors and 64 GB RAM.

**Baselines -** In this work, we compare IB-POMCP against: **(a) POMCP**, since it is a relevant state-of-art proposal and represents the basis of this work; **(b) \(\)-POMCP**, representing our main competitor in terms of using information theory to perform the online planning procedure, and **(c) TB \(\)-POMCP**, as a faster alternative to \(\)-POMCP that employs an explicit POMDP model.

**Metrics -** Two different evaluation metrics were used: **(i) the average reward (R)** across the experiment's mean reward; and **(ii) the average planning time (t)** spent by the agent to plan the next actions. Mean results were calculated across _50 executions_. Every experiment ran independently; thus, no knowledge was carried from one execution to another. The calculated errors (\( Err\)) represent the \(95\%\) confidence interval of a two-sample t-test. Note that our results and baselines are separated into two categories **(i) Quick Planning**: grouping methods that perform the decision-making process within a reasonable time window; and **(ii) Long Planning**: presenting the \(\)-POMCP's results (without time constraints). We separated them to make their understanding easier.

Finally, we refer the reader to our Appendix D to information about our hyperparameters set.

**Benchmarks study -** All results are presented in Table 1. In addition, we highlight the pros and cons of our method, analysing our limitations while discussing the outcome for each environment.

In _the Tiger domain_, TB \(\)-POMCP presents the best average reward among the Quick Planning methods (\(<0.01\)). IB-POMCP still outperforms POMCP, showing significant improvement in terms of reward (\(<0.01\)). For this specific problem, since the action "listen" generates variation in the entropy, IB-POMCP keeps performing it repeatedly until other action value outcomes the "listen" value in the best action selection procedure (Section 4, step (v)), a circumstance which leads our method to reduce its average reward collection. Hence, when facing problems where seeking spots of high uncertainty produces small penalties, IB-POMCP may collect penalties until it significantly decreases the uncertainty and chooses another path.

In _the Maze domain_, we observe that IB-POMCP presents a significantly better average reward in 3 out of 4 proposed scenarios (\( 0.015\)), except for M3, for which POMCP presents a better result. Note that reducing uncertainty leads to increasing reward; i.e. the faster an agent can access areas with high uncertainty, the higher its received reward. IB-POMCP's results match our expectations (given the developed rationale throughout our methodology) since we build it to, besides tracking the rewards available in the scenario, often seek paths that lead to spots with high entropy in order to decrease uncertainty, what increases reward in this scenario. In terms of time, we present significantly faster reasoning in all scenarios (\(<0.01\)). Investigating why IB-POMCP runs faster than POMCP, we found that our information-guided planning leads the algorithm to perform more transitions during the rollout phase than while performing the simulation inside our actual search tree, with a rate \(=1.61\), whereas POMCP presents a rate of \(1.28\). Because rollout transitions run faster than simulation transitions inside the tree, we can save time by performing them frequently.

In _the Rock Sample problem_, we can see that IB-POMCP shows a significant improvement in terms of reward collection (\( 0.02\)) in 3 out of 4 scenarios - except for the simplest scenario RockSample22 (R0), which presents a p-value of \( 0.11\). In terms of reasoning time, POMCP is slightly faster than all Quick Planning methods in 3 out of 4 scenarios (\( 0.01\)), except for RockSample17 (R3).

In _Tag_, IB-POMCP presents significant improvement in terms of reward collection and reasoning time against all baselines (\( 0.04\)). As in the Maze, we believe that IB-POMCP is faster because it simulates transitions in rollouts more often (in a ratio of \(=1.7\) against \(1.61\) for POMCP).

In _LaserTag_, we have a tie between all methods, with no statistically significant difference spotted across metrics (\( 0.32\) for reward collection and \( 0.06\) for reasoning time).

Finally, in the _Foraging domain_, our proposed method significantly outperformed all the baselines (\(<0.01\)). The Foraging problem represents our most complex scenario, and empirically shows how IB-POMCP can overcome the necessity of adjusting the reasoning horizon to perform planning in settings that deliver rewards sparsely. The only reward available in these scenarios comes from the collection of tasks. Consequently, while attempting to solve a task, the number of actions that the agent needs to plan and execute in sequence may approach or exceed the reasoning horizon size. In this case, the probability of experiencing this reward is low, and algorithms that only follow the reward in planning will rarely obtain it in their simulations; hence, they fail to plan effectively.

Overall, we experimentally demonstrated that our novel proposed method can significantly improve the performance of our agent and its planning capabilities without penalising the reasoning time. We also demonstrated that our proposal can improve the decision-making process by using only the information generated during the tree search process and, in contrast to \(\)-POMCP, without relying on the explicit representation of latent functions (e.g., the observation function).

**Ablation study -** To evaluate the impact of our proposed modifications and enhancements, we performed an ablation study over our method. We consider \(2\) different variations of IB-POMCP that partially implement its key points in this experiment: **(a) the Information-guided Particle Reinvigoration POMCP (IPR-POMCP)**, which implements the modifications to the particle filter reinvigoration process (explained in Section 4, steps (i) and (ii)); and **(b) the I-UCB POMCP**, which implements the proposed modifications to the search and simulation process (explained in Section 4, steps (iii) and (iv)). We consider 4 different scenarios, one for each environment presented in Table 1, to run our experiments. The results are depicted in Figure 2. Both additional methods proposed for the study are available in our GitHub repository together with the complete code of this work.

By analysing the graphs, it is clear that the I-UCB and implementation of an information-guided particle reinvigoration process for the Tiger, Maze, and Foraging problems directly enhance the reasoning capabilities of our planning method, which is translated here in terms of reward collection. For the Tiger and Foraging problems, diversifying the set of particles (I-UCB POMCP) alone presents improvements for POMCP; however, it has less impact than including entropy in the search process (IPR-POMCP). On the other hand, in the Maze environment, both approaches present similar improvements for the POMCP algorithm. However, when combined, they significantly improved upon the baseline results. Our intuition behind these results is that the IPR-POMCP proposal is responsible for delivering a better set of particles and a better initial estimation of the current state to the agent before simulating actions, whereas I-UCB continually guides and affects the planning process from the beginning of the simulations until the decision of the best action.

## 7 Conclusion

In this study, we propose _IB-POMCP_, a novel algorithm for planning under uncertainty that is capable of aggregating information entropy into a decision-making algorithm using our modified version of the UCB function, _I-UCB_. We present the theoretical properties for convergence under certain assumptions, which are supported by empirical results collected in five different domains. Overall, we increase the reward collection by up to 10 times in comparison with TB \(\)-POMCP (U-shaped, F1), in addition to reducing the reasoning time by up to 93% compared to \(\)-POMCP (MazeHoles).

Figure 2: Ablation study of IB-POMCP in four different scenarios.