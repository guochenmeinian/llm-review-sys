# On Proper Learnability between Average- and Worst-case Robustness

Vinod Raman

Department of Statistics

University of Michigan

Ann Arbor, MI 48104

vkraman@umich.edu

&Unique Subedi

Department of Statistics

University of Michigan

Ann Arbor, MI 48104

subedi@umich.edu

&Ambuj Tewari

Department of Statistics

University of Michigan

Ann Arbor, MI 48104

tewaria@umich.edu

Equal contributions

###### Abstract

Recently, Montasser et al. (2019) showed that finite VC dimension is not sufficient for _proper_ adversarially robust PAC learning. In light of this hardness, there is a growing effort to study what type of relaxations to the adversarially robust PAC learning setup can enable proper learnability. In this work, we initiate the study of proper learning under relaxations of the adversarially robust loss. We give a family of robust loss relaxations under which VC classes are properly PAC learnable with sample complexity close to what one would require in the standard PAC learning setup. On the other hand, we show that for an existing and natural relaxation of the adversarially robust loss, finite VC dimension is not sufficient for proper learning. Lastly, we give new generalization guarantees for the adversarially robust empirical risk minimizer.

## 1 Introduction

As deep neural networks become increasingly ubiquitous, their susceptibility to test-time adversarial attacks has become more and more apparent. Designing learning algorithms that are robust to these test-time adversarial perturbations has garnered increasing attention by machine learning researchers and practitioners alike. Prior work on adversarially robust learning has mainly focused on learnability under the worst-case _adversarially_ robust risk Montasser et al. (2019)Attias et al. (2021)Cullina et al. (2018),

\[R_{}(h;):=}{} [(x)}{}\{h(z) y\}],\]

where \((x)\) is an arbitrary but fixed perturbation set (for example \(_{p}\) balls). In practice, worst-case robustness is commonly achieved via Empirical Risk Minimization (ERM) of the adversarially robust loss or some convex surrogate Madry et al. (2017)Wong and Kolter (2018)Raghunathan et al. (2018)Bao et al. (2020). However, a seminal result by Montasser et al. (2019) shows that any proper learning rule, including ERM, even when trained on an arbitrarily large number of samples, may not return a classifier with small adversarially robust risk. These high generalization gaps for the adversarially robust loss have also been observed in practice Schmidt et al. (2018). Even worse, empirical studies have shown that classifiers trained to achieve worst-case adversarial robustness exhibit degraded _nominal_ performance Dobriban et al. (2020)Raghunathan et al. (2019)Su et al. (2018)Tsipras et al. (2018)Yang et al. (2020)Zhang et al. (2019)Robey et al. (2022).

In light of these difficulties, there has been a recent push to study when proper learning, and more specifically, when learning via ERM is possible for achieving adversarial robustness. The ability to achieve test-time robustness via proper learning rules is important from a practical standpoint. Italigns better with the current approaches used in practice (e.g. (S)GD-trained deep nets), and proper learning algorithms are often simpler to implement than improper ones. In this vain, Ashtiani et al. (2022) and Bhattacharjee et al. (2022) consider adversarial robust learning in the _tolerant_ setting, where the error of the learner is compared with the best achievable error with respect to a slightly _larger_ perturbation set. They show that the sample complexity of tolerant robust learning can be significantly lower than the current known sample complexity for adversarially robust learning and that proper learning via ERM can be possible under certain assumptions. Additionally, Ashtiani et al. (2020) provide some sufficient conditions under which proper robust learnability of VC classes becomes possible under a PAC-type framework of semi-supervised learning. More recently, Attias et al. (2022) show that finite VC dimension is sufficient for proper semi-supervised adversarially robust PAC learning if the support of the marginal distribution on instances is known. On the other hand, Attias and Hannekel (2023) study the adversarially robust PAC learnability of real-valued functions and show that every convex function class is properly learnable.

In a different direction, several works have studied the consistency of various _surrogates_ of the adversarially robust loss \(_{}(h,(x,y))=_{z(x)}\{h(z) y\}\)Awasthi et al. (2022); Awasthi et al. (2022); Awasthi et al. (2022); Mao et al. (2023), while others have considered relaxing its worst-case nature Robey et al. (2022); Li et al. (2020); Li et al. (2021); Liadlaw and Feizi (2019); Rice et al. (2021). However, the PAC learnability of these relaxed notions of the adversarially robust loss has not been well-studied.

In this paper, we study relaxations of the worst-case adversarially robust learning setup from a learning-theoretic standpoint. We classify existing relaxations of worst-case adversarially robust learning into two approaches: one based on relaxing the loss function and the other based on relaxing the benchmark competitor. Much of the existing learning theory work studying relaxations of adversarial robustness focus on the latter approach. These works answer the question of whether proper PAC learning is feasible if the learner is evaluated against a stronger notion of robustness. In contrast, we focus on the _former_ relaxation and pose the question: _can proper PAC learning be feasible if we relax the adversarially robust loss function itself?_ In answering this question, we make the following main contributions:

* We show that the finiteness of the VC dimension is _not sufficient_ for properly learning a natural relaxation of the adversarially robust loss proposed by Robey et al. (2022). Our proof techniques involve constructing a VC class that is not properly learnable.
* We give a family of adversarially robust loss relaxations that interpolate between average- and worst-case robustness. For these losses, we use Rademacher complexity arguments relying on the Ledoux-Talagrand contraction to show that all VC classes are learnable via ERM.
* We extend a property implicitly appearing in margin theory (e.g., see Mohri et al. (2018) Section 5.4), which we term "Sandwich Uniform Convergence" (SUC), to show new generalization guarantees for the adversarially robust empirical risk minimizer.

## 2 Preliminaries and Notation

Throughout this paper we let \([k]\) denote the set of integers \(\{1,...,k\}\), \(\) denote an instance space, \(=\{-1,1\}\) denote our label space, and \(\) be any distribution over \(\). Let \(^{}\) denote a hypothesis class mapping examples in \(\) to labels in \(\).

### Problem Setting

In the standard adversarially robust learning setting, the learner, during training time, picks a set \(^{}\) of perturbation functions \(g:\) against which they wish to be robust. At test time, an adversary intercepts the labeled example \((x,y)\), exhaustively searches over the perturbation set to find _the worst_\(g\), and then passes the perturbed instance \(g(x)\) to the learner. The learner makes a prediction \(\) and then suffers the loss \(\{ y\}\). The goal of the learner is to find a hypothesis \(h^{}\) that minimizes the adversarially robust risk \(R_{}(h;)=_{(x,y)}[_{ }(h,(x,y))]\) where \(_{}(h,(x,y)):=_{g}\{h(g(x)) y\}\) is the adversarially robust loss.

In practice, however, such a worst-case adversary may be too strong and unnatural, especially in high-dimension. Accordingly, we relax this model by considering a _lazy_, computationally-bounded adversary that is unable to exhaustively search over \(\), but can randomly sample a perturbation function \(g\) given access to a measure \(\) over \(\). In this setup, the learner picks both a perturbation set \(\) and a measure \(\) over \(\). At test-time, the lazy adversary intercepts the labeled example \((x,y)\), _randomly samples_ a perturbation function \(g\), and then passes the perturbed instance \(g(x)\) to the learner. From this perspective, the goal of the learner is to output a hypothesis such that the _probability_ that the lazy adversary succeeds in sampling a bad perturbation function, for any labeled example in the support of \(\), is small. In other words, the learner strives to be robust to most, but not all, perturbation functions in \(\).

We highlight that the set \(\) and the measure \(\) are fixed and chosen by the learner at training time. As a result, the measure \(\) does not depend on the unperturbed point \(x\). Allowing the measure to depend on the unperturbed point \(x\) is an interesting future direction that lies between our model and adversarial robustness. Nevertheless, fixing the measure \(\) is still relevant from a practical standpoint and non-trivial from a theoretical standpoint. Indeed, a popular choice for \(\) and \(\) are \(_{p}\) balls and the uniform measure respectively. These choices have been shown experimentally to strike a better balance between robustness and nominal performance (Robey et al., 2022). Moreover, in Section 5 we show that even when the measure \(\) is fixed apriori, there are learning problems for which ERM does not always work. This parallels the hardness result from Montasser et al. (2019), who prove an analogous result for adversarially robust PAC learning.

### Probabilistically Robust Losses and Proper Learnability

Given a perturbation set \(\) and measure \(\), we quantify the probabilistic robustness of a hypothesis \(h\) on a labeled example \((x,y)\) by considering losses that are a function of \(_{g}[h(g(x)) y]\). For a labeled example \((x,y)\), \(_{g}[h(g(x)) y]\) measures the fraction of perturbations in \(\) for which the classifier \(h\) is non-robust. Observe that \(_{g}[h(g(x)) y]=_{g }[h(g(x))]}{2}\) is an affine transformation of quantity \(y_{g}[h(g(x))]\), the probabilistically robust _margin_ of \(h\) on \((x,y)\) with respect to \((,)\). Thus, we focus on loss functions that operate over the margin \(y_{g}[h(g(x))]\). One important loss function is the \(\)-probabilistically robust loss,

\[_{,}^{}(h,(x,y)):=\{_{g} (h(g(x)) y)>\},\]

where \([0,1)\) is selected apriori. The \(\)-probabilistically robust loss was first introduced by Robey et al. (2022) for the case when \(=^{d}\), \(g_{c}(x)=x+c\), and the set of perturbations \(=\{g_{c}:c\}\) for some \(^{d}\). In this paper, we generalize this loss to an arbitrary instance space \(\) and perturbation set \(\). As highlighted by Robey et al. (2022), this notion of robustness is desirable as it nicely interpolates between worst- and average-case robustness via an interpretable parameter \(\), while being more computationally tractable compared to existing relaxations.

In this work, we are primarily interested in understanding whether probabilistic relaxations of the adversarially robust loss enable _proper learning_. That is, given a hypothesis class \(\), perturbation set and measure \((,)\), loss function \(_{,}(h,(x,y))=(y_{g}[h(g(x)) ])\), and labeled samples from an unknown distribution \(\), our goal is to design a learning algorithm \(:()^{*}\) such that for any distribution \(\) over \(\), the algorithm \(\), given a sample of labeled examples from \(\), finds a hypothesis \(h\) with low risk with regards to \(_{,}(h,(x,y))\).

**Definition 1** (Proper Probabilistically Robust PAC Learnability).: _Let \(_{,}(h,(x,y))\) denote an arbitrary probabilistically robust loss function. For any \(,(0,1)\), the sample complexity of probabilistically robust \((,)\)-learning \(\) with respect to \(_{,}\), denoted \(n(,;,_{,})\) is the smallest number \(m\) for which there exists a proper learning rule \(:()^{*}\) such that for every distribution \(\) over \(\), with probability at least \(1-\) over \(S^{m}\),_

\[_{}[_{,}((S),(x,y)) ]_{h}_{}[_{ ,}(h,(x,y))]+.\]

_We say that \(\) is proper probabilistically robustly PAC learnable with respect to \(_{,}\) if \(,(0,1)\), \(n(,;,_{,})\) is finite._An important class of proper learning rules is ERMs which simply output the hypothesis \(h\) that minimizes the loss \(_{,}\) over the training sample. In this paper, we ultimately show that for a wide family of probabilistically robust loss functions, ERM is a proper learner according to Definition 1. On the other hand, in Section 3 we show that proper probabilistically robust PAC learning is not always possible for the loss function \(^{}_{,}(h,(x,y))\).

### Complexity Measures

Under the standard 0-1 risk, the Vapnik-Chervonenkis dimension (VC dimension) plays an important role in characterizing PAC learnability, and more specifically, when ERM is possible. A hypothesis class \(\) is PAC learnable with respect to the 0-1 loss if and only if its VC dimension is finite [Vapnik and Chervonenkis, 1971].

**Definition 2** (VC dimension).: _A set \(\{x_{1},...,x_{n}\}\) is shattered by \(\), if \( y_{1},...,y_{n}\), \( h\), such that \( i[n]\), \(h(x_{i})=y_{i}\). The VC dimension of \(\), denoted \(()\), is defined as the largest natural number \(n\) such that there exists a set \(\{x_{1},...,x_{n}\}\) that is shattered by \(\)._

One _sufficient_ condition for proper learning, based on Vapnik's "General Learning" [Vapnik, 2006], is the finiteness of the VC dimension of a binary loss class

\[^{}:=\{(x,y)(h,(x,y)):h\}\]

where \((h,(x,y))\) is some loss function mapping to \(\{0,1\}\). In particular, if the VC dimension of the loss class \(^{}\) is finite, then \(\) is PAC learnable with respect to \(\) using ERM with sample complexity that scales linearly with \((^{})\). In this sense, if one can upper bound \((^{})\) in terms of \(()\), then finite VC dimension is sufficient for proper learnability. Unfortunately, for adversarially robust learning, Montasser et al.  show that there can be an arbitrary gap between the VC dimension of the adversarially robust loss class \(^{}_{}:=\{(x,y)_{}(h,(x,y)):h\}\) and the VC dimension of \(\). Likewise, in Section 3 we show that for the \(\)-probabilistically robust loss \(^{}_{,}\), there can also be an arbitrarily large gap between the VC dimension of the loss class and the VC dimension of the hypothesis class.

As many of the loss functions we consider will actually map to values in \(\), the VC dimension of the loss class will not be well-defined. Instead, we can capture the complexity of the loss class via the _empirical_ Rademacher complexity.

**Definition 3** (Empirical Rademacher Complexity of Loss Class).: _Let \(\) be a loss function, \(S=\{(x_{1},y_{1}),...,(x_{n},y_{n})\}()^{*}\) be any sequence of examples, and \(^{}=\{(x,y)(h,(x,y)):h\}\) be a loss class. The empirical Rademacher complexity of \(^{}\) is defined as_

\[}_{m}(^{})=_{}[ _{f^{}}(_{i=1}^{m}_{i} f(x_{i},y_{i}))]\]

_where \(_{1},...,_{m}\) are independent Rademacher random variables._

A standard result relates the empirical Rademacher complexity to the generalization error of hypotheses in \(\) with respect to a real-valued bounded loss function \((h,(x,y))\)[Barlett and Mendelson, 2002].

**Theorem 2.1** (Rademacher-based Uniform Convergence).: _Let \(\) be a distribution over \(\) and \((h,(x,y)) c\) be a bounded loss function. With probability at least \(1-\) over the sample \(S^{m}\), for all \(h\) simultaneously,_

\[|_{}[(h(x),y)]-}_{S}[(h(x), y)]| 2}_{m}()+O(c)}{n}})\]

_where \(}_{S}[(h(x),y)]=_{(x,y) S}(h(x),y)\) is the empirical average of the loss over \(S\)._

## 3 Not All Robust Loss Relaxations Enable Proper Learning

Recall the \(\)-probabilistically robust loss \[^{}_{,}(h,(x,y)):=\{_{g}(h( g(x)) y)>\},\]

and its corresponding risk \(R^{}_{,}(h;):=_{(x,y)} [^{}_{,}(h,(x,y))]\). At a high-level, proper learning under the \(^{}_{,}\) requires finding a hypothesis \(h\) that is robust to at least a \(1-\) fraction of the perturbations in \(\) for each example in the support of the data distribution \(\).

Which hypothesis classes are properly learnable with respect to \(^{}_{,}\) according to Definition1? In AppendixB.1 we show that if \(\) is finite, then finite VC dimension is sufficient for proper learning with respect to \(^{}_{,}\). On the other hand, in this section, we show that if \(\) is allowed to be arbitrary, VC dimension is not sufficient for _proper_ learning with respect to \(^{}_{,}\), let alone learning via ERM.

**Theorem 3.1**.: _There exists \((,)\) such that for every \([0,1)\), there exists a hypothesis class \(^{}\) with \(() 1\) such that \(\) is not properly probabilistically robust PAC learnable with respect to \(^{}_{,}\)._

To prove TheoremB.1 we fix \(=^{d}\), \(=\{g_{}:^{d},||||_{p}\}\) such that \(g_{}(x)=x+\) for all \(x\) for some \(>0\), and \(\) to be the uniform measure over \(\). In other words, we are picking our perturbation sets to be \(_{p}\) balls of radius \(\) and our perturbation measures to be uniform over each perturbation set. Note that by construction of \(\), a uniform measure \(\) over \(\) also induces a uniform measure \(_{x}\) over \((x):=\{g_{}(x):g_{}\}^ {d}\). We start by showing that for every \([0,1)\), there can be an arbitrary gap between the VC dimension of \(\) and the loss class \(^{,}_{,}:=\{(x,y)^{}_{ ,}(h,(x,y)):h\}\).

**Lemma 3.2**.: _For every \([0,1)\) and \(m\), there exists a hypothesis class \(^{}\) such that \(() 1\) but \((^{,}_{,}) m\)._

The proof of Lemma3.2 is found in AppendixB.2. We highlight two key differences between Lemma3.2 and its analog, Lemma2, in Montasser et al.. First, we need to provide _both_ a perturbation set and a perturbation measure. The interplay between these two objects is not present in Montasser et al. and, apriori, it is not clear that these would indeed be \(_{p}\) balls and the uniform measure. Second, in order for a hypothesis to be probabilistically non-robust there needs to exist a large enough _region_ of perturbations over which it makes mistakes. This is in contrast to Montasser et al., where a hypothesis is adversarially non-robust as long as there exists _one_ non-robust perturbation. Constructing a hypothesis class that achieves all possible probabilistically robust loss behaviors while also having low VC dimension is non-trivial - we need hypotheses to be expressive enough to have large regions of non-robustness while not being too expressive such that VC dimension increases.

Next, we show that the hypothesis class construction in Lemma3.2 can be used to show the existence of a hypothesis class that cannot be learned properly. Lemma3.3 is similar to Lemma3 in Montasser et al. and is proved in AppendixB.3.

**Lemma 3.3**.: _For every \([0,1)\) and \(m\) there exists \(^{}\) with \(() 1\) such that for any proper learner \(:()^{*}\): (1) there is a distribution \(\) over \(\) and a hypothesis \(h^{*}\) where \(R^{}_{,}(h^{*};)=0\) and (2) with probability at least \(1/7\) over \(S D^{m}\), \(R^{}_{,}((S);)>1/8\)._

Finally, the proof of Theorem5.1uses Lemma5.3 and follows a similar idea as its analog in Montasser et al. (Theorem1). However, since our hypothesis class construction in Lemma3.2 is different, some subtle modifications need to be made. We include a complete proof in AppendixB.4

## 4 Proper Learnability Under Relaxed Losses

Despite the fact that VC classes are not \(\)-probabilistically robust learnable using proper learning rules, our framework still enables us to capture a wide range of relaxations to the adversarially robust loss for which proper learning is possible.

In particular, consider robust loss relaxations of the form \(_{,}(h,(x,y))=(y_{g}[h(g(x))])\) where \((t):\) is a \(L\)-Lipschitz function. This class of loss functions is general, capturing many natural robust loss relaxations like the hinge loss \(1-y_{g}[h(g(x))]\), squared loss \(_{g}[h(g(x))])^{2}=(1-y_{g}[h(g(x ))])^{2}\), and exponential loss \(e^{-y_{g}[h(g(x))]}\). Furthermore, the class of Lipschitz functions \(:\) on the margin \(y_{g}[h(g(x))]\) enables us to capture levels of robustness between the average- and worst-case. For example, taking \((t)=\) results in the loss \(_{,}(h,(x,y))=(y_{g}[h(g(x))] )=_{g}[h(g(x)) y]\), corresponding to average-case robustness, or _data augmentation_. On the other hand, taking \((t)=(,1)\) for some \((0,1)\), results in the loss

\[_{,}(h,(x,y))=(y_{g}[h(g(x))] )=(_{g}[h(g(x)) y]}{},1)\]

which corresponds to a notion of robustness that becomes stricter as \(\) approaches \(0\). We note that some of the losses in our family were studied by Rice et al. . However, their focus was on evaluating robustness, while ours is about (proper) learnability.

Lemma 4.1 shows that for hypothesis classes \(\) with finite VC dimension, for any \((,)\), all \(L\)-Lipschitz loss functions \(_{,}(h,(x,y))\) enjoy the uniform convergence property.

**Lemma 4.1** (Uniform Convergence of Lipschitz Loss).: _Let \(\) be a hypothesis class with finite VC dimension, \((,)\) be a perturbation set and measure, and \(_{,}(h,(x,y))=(y_{g}[h(g(x)) ])\) such that \(:\) is a \(L\)-Lipschitz function. With probability at least \(1-\) over a sample \(S^{n}\) of size \(n=O(()L^{2}()+()}{^{2}})\), for all \(h\) simultaneously,_

\[|_{}[_{,}(h,(x,y))]- _{S}[_{,}(h,(x,y))]|.\]

Proof.: Let \(()=d\) and \(S=\{(x_{1},y_{1}),...,(x_{m},y_{m})\}\) be a set of examples drawn i.i.d from \(\). Define the loss class \(_{,}^{}=\{(x,y)_{, }(h,(x,y)):h\}\). Observe that we can reparameterize \(_{,}^{}\) as the composition of a \(L\)-Lipschitz function \((x)\) and the function class \(_{,}^{}=\{(x,y) y_{g }[h(g(x))]:h\}\). By Proposition 2.1, to show the uniform convergence property of \(_{,}(h,(x,y))\), it suffices to upper bound \(}_{m}(_{,}^{})=}_{m}(_{,}^{})\), the empirical Rademacher complexity of the loss class. Since \(\) is \(L\)-Lipschitz, by Ledoux-Talgarand's contraction principle , it follows that \(}_{m}(_{,}^{})=}_{m}(_{}_{,}^{}) L}_{m}(_{,}^{})\). Thus, it actually suffices to upperbound \(}_{m}(_{,}^{})\) instead. Starting with the definition of the empirical Rademacher complexity:

\[}_{m}(_{,}^{}) =_{\{ 1\}^{m}}[_{h} (_{i=1}^{m}_{i}y_{i}_{g}[h(g(x_{i})) ])]\] \[=_{\{ 1\}^{m}}[_{h }(_{g}[_{i=1}^{m}_{i}h(g(x_{i} ))])]\] \[_{g}[_{ \{ 1\}^{m}}[_{h}_{i=1}^{m}_{i}h(g(x_{i }))]],\]

where the last inequality follows from Jensen's inequality and Fubini's Theorem. Note that the quantity \(_{\{ 1\}^{m}}[_{h}_{i=1}^{m} _{i}h(g(x_{i}))]\) is the empirical Rademacher complexity of the hypothesis class \(\) over the sample \(\{g(x_{1}),...,g(x_{m})\}\) drawn i.i.d from the distribution defined by first sampling from the marginal data distribution, \(x_{}\), and then applying the transformation \(g(x)\). By standard VC arguments, \(}_{m}() O()}{m}})\), which implies that \(}_{m}(_{,}^{})_{g}[}_{m}()] O()}{m}})\). Putting things together, we get \(}_{m}(_{,}^{})=}_{m}(_{,}^{}) O (()}{m}})\). Proposition 2.1 then implies that with probability \(1-\) over a sample \(S^{m}\) of size \(m=O(()+()}{^{2}})\), we have

\[|_{}[_{,}(h,(x,y))]- _{S}[_{,}(h,(x,y))]|\]for all \(h\) simultaneously. 

Uniform convergence of Lipschitz-losses immediately implies proper learning via ERM.

**Theorem 4.2**.: _Let \(_{,}(h,(x,y))=(y_{}[h(g(x) )])\) such that \(:\) is a \(L\)-Lipschitz function. For every hypothesis class \(\), perturbation set and measure \((,)\), and \((,)(0,1)^{2}\), the proper learning rule \((S)=_{h}_{S}[_{,}(h,(x,y))]\), for any distribution \(\) over \(\), achieves, with probability at least \(1-\) over a sample \(S^{n}\) of size \(n O(()L^{2}()+()}{^{2}})\), the guarantee_

\[_{}[_{,}((S),(x,y)) ]_{h}_{}[_{,}(h,(x,y))]+.\]

At a high-level, Theorem4.2 shows finite VC dimension is sufficient for achieving robustness _between_ the average- and worst-case using ERM. In fact, the next theorem, whose proof can be found in AppendixC.2 shows that finite VC dimension may not even be necessary for this to be true. The proof of Theorem4.3 involves considering the well-known infinite VC class \(=\{x((wx)):w\}\) and picking \((,)\) such that \(_{g}[h(g(x))]\) is essentially constant in \(x\) for all hypothesis \(h\).

**Theorem 4.3**.: _Let \(_{,}(h,(x,y))=(y_{}[h(g( x))])\) such that \(:\) is a \(L\)-Lipschitz function. There exists \(\) and \((,)\) such that \(()=\) but \(\) is still properly learnable with respect to \(_{,}(h,(x,y))\)._

Together, Theorems4.2 and 4.3 showcase an interesting trade-off. Theorem4.3 indicates that by carefully choosing \((,)\), the complexity of \(\) can be essentially smoothed out. On the other hand, Theorem4.2 shows that any complexity in \((,)\) can be smoothed out if \(\) has finite VC dimension. This interplay between the complexities of \(\) and \((,)\) closely matches the intuition of \(}\) in their work on Vicinal Risk Minimization. Note that the results in this section do not contradict that of Section3 because \(_{,}^{}(h,(x,y))\) is a _non-Lipschitz_ function of \(y_{g}[h(g(x))]\).

We end this section by noting that Lipschitzness of \(_{,}\) is sufficient but, in full generality, not necessary for proper learnability. For example, the loss function that completely ignores \((,)\) and just computes the 0-1 loss is not Lipschitz, however, it is learnable via ERM when the VC dimension of \(\) is finite.

## 5 Proper Learnability Under Relaxed Competition

The results of Section3 show that relaxing the adversarially robust loss may not always enable proper learning, even for very natural robust loss relaxations. In this section, we show that this bottleneck can be alleviated if we also allow the learner to compete against a slightly stronger notion of robustness. Furthermore, we expand on this idea by exploring other robust learning settings where allowing the learner to compete against a stronger notion of robustness enables proper learnability. We denote this type of modification to the standard adversarially and probabilistically robust leaning settings as robust learning under _relaxed competition_. Prior works on Tolerantly Robust PAC Learning  mentioned in the introduction fit under this umbrella.

Our main tool in this section is Lemma5.1 which we term as Sandwich Uniform Convergence (SUC). Roughly speaking, SUC provides a sufficient condition under which ERM outputs a predictor that generalizes well with respect to a stricter notion of loss. A special case of SUC has implicitly appeared in margin theory (e.g., see Section5.4]), where one evaluates the \(0\)-\(1\) risk of the output hypothesis against the optimal _margin_\(0\)-\(1\) risk.

**Lemma 5.1** (Sandwich Uniform Convergence).: _Let \(_{1}(h,(x,y))\) and \(_{2}(h,(x,y))\) be bounded, non-negative loss functions such that for all \(h\) and \((x,y)\), we have \(_{1}(h,(x,y))_{2}(h,(x,y)) 1\). If there exists a loss function \((h,(x,y))\) such that \(_{1}(h,(x,y))(h,(x,y))_{2}(h,(x,y))\) and \((h,(x,y))\) enjoys the uniform convergence property with sample complexity \(n(,)\), then the learning rule \((S)=_{h}_{S}[_{2}(h,(x,y))]\) achieves, with probability \(1-\) over a sample \(S^{m}\) of size \(m n(/2,/2)+O()}{ ^{2}})\), the guarantee_

\[_{}[_{1}((S),(x,y))]_ {h}_{}[_{2}(h,(x,y))]+.\]Proof.: Let \((S)=_{h}_{S}[_{2}(h,(x,y))]\). By uniform convergence of \((h,(x,y))\), we have that for sample size \(m=(,)\), with probability at least \(1-\), over a sample \(S^{m}\), for every hypothesis \(h\) simultaneously,

\[_{}[(h,(x,y))] }_{S}[(h,(x,y))]+.\]

In particular, this implies that for \(=(S)\), we have

\[_{}[(,(x,y))]}_{S}[(,(x,y))]+.\]

Since, \(_{1}(h,(x,y))(h,(x,y))_{2}(h,(x,y))\), we have that

\[_{}[_{1}(,(x,y))]}_{S}[_{2}(h^{*},(x,y))]+\]

where \(h^{*}=_{h}_{}[_{2}(h,(x,y))]\). It now remains to upper bound \(}_{S}[_{2}(h^{*},(x,y))]\) with high probability. However, a standard Hoeffding bound tells us that with probability \(1-\) over a sample \(S\) of size \(O(})}{^{2}})\), \(}_{S}[_{2}(h^{*},(x,y))]_{}[_{2}(h^{*},(x,y))]+\). Thus, by union bound, we get that with probability at least \(1-\), \(_{}[_{1}(,(x,y))]_{ }[_{2}(h^{*},(x,y))]+\), using a sample of size \(n(/2,/2)+O(})}{^{2}})\). 

Lemma 5.1 only requires the _existence_ of such a sandwiched loss function that enjoys uniform convergence--we do not actually require it to be computable. In the next two sections, we exploit this fact to give three new generalization guarantees for the empirical risk minimizer over the adversarially robust loss \(_{}(h,(x,y))\) and \(\)-probabilistically robust loss \(_{,}^{}(h,(x,y))\), hereafter denoted by \((S;):=_{h}}_{ }[_{}(h,(x,y))]\) and \((S;(,),):=_{h}}_{}[_{,}^{}(h,(x,y))]\) respectively.

### \((,^{*})\)-Probabilistically Robust PAC Learning

In light of the hardness result of Section 3 we slightly tweak the learning setup in Definition 1 by allowing \(\) to compete against the hypothesis minimizing the probabilistic robust risk at a level \(^{*}<\). Under this further relaxation, we show that proper learning becomes possible, and that too, via PRERM. In particular, Theorem 5.2 shows that while VC classes are not properly \(\)-probabilistically robust PAC learnable, they are properly \((,^{*})\)-probabilistically robust PAC learnable.

**Theorem 5.2** (Proper \((,^{*})\)-Probabilistically Robust PAC Learner).: _Let \(0^{*}<\). Then, for every hypothesis class \(\), perturbation set and measure \((,)\), and \((,)(0,1)^{2}\), the proper learning rule \((S)=(S;(,),^{*})\), for any distribution \(\) over \(X\), achieves, with probability at least \(1-\) over a sample \(S^{n}\) of size \(n O(()}{(-^{*})^{2}} (-^{*})^{2}})+()}{^{2}})\), the guarantee_

\[R_{,}^{}((S);)_{h }R_{,}^{^{*}}(h;)+.\]

In contrast to Section 3 where proper learning is not always possible, Theorem 5.2 shows that if we compare our learner to the best hypothesis for a _slightly_ stronger level of probabilistic robustness, then not only is proper learning possible for VC classes, but it is possible via ERM. Our main technique to prove Theorem 5.2 is to consider a _different_ probabilistically robust loss function that is (1) a Lipschitz function of \(y_{y}[h(g(x)) y]\) and (2) can be sandwiched in between \(_{,}^{^{*}}\) and \(_{,}^{}\). Then, Theorem 5.2 follows from Lemma 5.1 The full proof is in Appendix D.1

### \((,)\)-Probabilistically Robust PAC Learning

Can measure-independent learning guarantees be achieved if we instead compare the learner's probabilistically robust risk \(R_{,}^{}\) to the best _adversarially robust risk_\(R_{}\) over \(\)? We answer this in the affirmative by using SUC. We show that if one wants to compete against the best hypothesis for the worst-case adversarially robust risk, it is sufficient to run RERM.

**Theorem 5.3** (Proper \((,)\)-Probabilistically Robust PAC Learner).: _For every hypothesis class \(\), perturbation set \(\), and \((,)(0,1)^{2}\), the proper learning rule \((S)=(S;)\), for any measure \(\) over \(\) and any distribution \(D\) over \(\), achieves, with probability at least \(1-\) over a sample \(S^{n}\) of size \(n O(()}{^{2}}()+()}{^{2}})\), the guarantee_

\[R^{}_{,}((S);)_{h}R_{}(h;)+.\]

The proof of Theorem5.3 can be found in AppendixD.2 which follows directly from Lemma5.1 by a suitable choice of the sandwiched loss \(\). We make a few remarks about the practical importance of Theorem5.3 Theorem5.3 implies that for any pre-specified perturbation function class \(\) (for example \(_{p}\) balls), running RERM is sufficient to obtain a hypothesis that is probabilistically robust with respect to _any_ fixed measure \(\) over \(\). Moreover, the level of robustness of the predictor output by RERM, as measured by \(1-\), scales directly with the sample size - the more samples one has, the smaller \(\) can be made. Alternatively, for a fixed sample size \(m\), desired error \(\) and confidence \(\), one can use the sample complexity guarantee in Theorem5.3 to back-solve the robustness guarantee \(\).

We highlight that the generalization bound in Theorem5.3 is a _measure-independent_ guarantee. This means that \(\) does not quantify the level of robustness of the output hypothesis with respect to any one particular measure, but for _any_ measure. This is desirable, as in contrast to the previous section, the \(\) here more succinctly quantifies the level of robustness achieved by the output classifier. Lastly, we highlight that while the sample complexity of adversarially robust PAC learning can be exponential in the VC dimension of \(\)(Montasser et al., 2019), this is not the case for \((,)\)-probabilistically robust PAC learning, where we only get a linear dependence on VC dimension.

### Tolerantly Robust PAC Learning

In Tolerantly Robust PAC Learning (Bhattacharjee et al., 2022)(Ashtiani et al., 2022), the learner's adversarially robust risk under a perturbation set \(\) is compared with the best achievable adversarially robust risk for a larger perturbation set \(^{}\). (Ashtiani et al., 2022) study the setting where both \(\) and \(^{}\) induce \(_{p}\) balls with radius \(r\) and \((1+)r\) respectively. In the work of Bhattacharjee et al. (2022), \(\) is arbitrary, but \(^{}\) is constructed such that it induces perturbation sets that are the union of balls with radius \(\) that cover \(\). Critically, Bhattacharjee et al. (2022) show that, under certain assumptions, running RERM over a larger perturbation set \(^{}\) is sufficient for Tolerantly Robust PAC learning. In this section, we take a slightly different approach to Tolerantly Robust PAC learning. Instead of having the learner compete against the best possible risk for a larger perturbation set, we have the learner still compete against the best possible adversarially robust risk over \(\), but evaluate the learner's adversarially robust risk using a _smaller_ perturbation set \(^{}\).

For what \(^{}\) is Tolerantly Robust PAC learning via RERM possible? As an immediate result of Lemma5.1 and Vapnik's "General Learning", finite VC dimension of the loss class \(^{}_{^{}}=\{(x,y)_{ ^{}}(h,(x,y)):h\}\) is sufficient. Note that finite VC dimension of \(^{}_{^{}}\) implies that the loss function \(_{^{}}(h,(x,y))\) enjoys the uniform convergence property with sample complexity \(O((^{^{}}_{^ {}})+()}{^{2}})\). Thus, taking \(_{1}(h,(x,y))=(h,(x,y))=_{^{}}(h,(x,y))\) and \(_{2}(h,(x,y))=_{}(h,(x,y))\) in Lemma5.1, we have that if there exists a \(^{}\) such that \((^{^{}}_{})<\), then with probability \(1-\) over a sample \(S^{n}\) of size \(n=O((^{^{}}_{^ {}})+()}{^{2}})\),

\[R_{^{}}((S);)_{h}R_{}(h;)+,\]

where \((S)=(S;)\).

Alternatively, if \(^{}\) such that there exists a _finite_ subset \(}\) where \(_{^{}}(h,(x,y))_{}(h,(x,y))\), then Tolerantly Robust PAC learning via RERM is possible with sample complexity that scales according to \(O(()(||)+()}{^{2}})\). This result essentially comes from the fact that the VC dimension of the loss class for any finite perturbation set \(}\) incurs only a \((|}|)\) blow-up from the VC dimension of \(\) (see Lemma1.1 in (Atias et al., 2021)). Thus, finite VC dimension of\(\) implies finite VC dimension of the loss class \(^{}}_{}\) which implies uniform convergence of the loss \(_{}}(h,(x,y))\), as needed for Lemma 5.1 to hold.

We now give an example where such a finite approximation of \(^{}\) is possible. In order to do so, we will need to consider a _metric space_ of perturbation functions \((,d)\) and define a notion of "nice" perturbation sets, similar to"regular" hypothesis classes from Bhattacharjee et al. (2022).

**Definition 4** (\(r\)-Nice Perturbation Set).: _Let \(\) be a hypothesis class and \((,d)\) a metric space of perturbation functions. Let \(B_{r}(g):=\{g^{}:d(g,g^{}) r\}\) denote a closed ball of radius \(r\) centered around \(g\). We say that \(^{}\) is \(r\)-Nice with respect to \(\), if for all \(x\), \(h\), and \(g^{}\), there exists a \(g^{*}\), such that \(g B_{r}(g^{*})\) and \(h(g(x))=h(g^{}(x))\) for all \(g^{} B_{r}(g^{*})\)._

Definition 4 prevents a situation where a hypothesis \(h\) is non-robust to an isolated perturbation function \(g^{}\) for any given labelled example \((x,y)\). If a hypothesis \(h\) is non-robust to a perturbation \(g^{}\), then Definition 4 asserts that there must exist a small ball of perturbation functions in \(\) over which \(h\) is also non-robust. Next, we define the covering number.

**Definition 5** (Covering Number).: _Let \((,d)\) be a metric space, let \(\) be a subset, and \(r>0\). Let \(B_{r}(x)=\{x^{}:d(x,x^{}) r\}\) denote the ball of radius \(r\) centered around \(x\). A subset \(\) is an \(r\)-covering of \(\) if \(_{c}B_{r}(c)\). The covering number of \(\), denoted \(_{r}(,d)\), is the smallest cardinality of any \(r\)-covering of \(\)._

Finally, let \(^{}_{2r}=_{g^{}}B_{2r}(g)\) denote the union over all balls of radius \(2r\) with centers in \(^{}\). Theorem 5.4 then states that if there exists a set \(^{}\) that is \(r\)-Nice with respect to \(\), then Tolerantly Robust PAC learning is possible via RERM with sample complexity that scales logarithmically with \(_{r}(^{}_{2r},d)\).

**Theorem 5.4** (Tolerantly Robust PAC learning under Nice Perturbations).: _Let \(^{}\) be a hypothesis class and \((,d)\) be a metric space of perturbation functions. Given a subset \(^{}\) such that \(^{}\) is \(r\)-Nice with respect to \(\), then the proper learning rule \((S)=(S;)\), for any distribution \(\) over \(\), achieves, with probability at least \(1-\) over a sample \(S^{n}\) of size \(n O(()(_{r}( ^{}_{2r},d))+()}{^{2}})\), the guarantee_

\[R_{^{}}((S);)_{h}R_{}(h;)+.\]

In Appendix D, we give a full proof and show that \(_{p}\)_balls are \(r\)-Nice perturbation sets for robustly learning halfspaces_. Note that Theorem 5.4 does not require apriori knowledge of the \(r\)-Nice perturbation set \(^{}\), but just _its existence_. Therefore, Theorem 5.4 applies to the largest possible \(r\)-Nice perturbation subset of \(\). This is important from a practical standpoint as computing an \(r\)-Nice perturbation set might not be computationally tractable. Accordingly, while RERM may not be a proper adversarially robust PAC learner Montasser et al. (2019), Theorem 5.4 shows that RERM is a proper tolerantly robust PAC learner.

## 6 Discussion

In this work, we show that there exists natural relaxations of the adversarially robust loss for which finite VC dimension is still not sufficient for proper learning. On the other hand, we identify a large set of Lipschitz robust loss relaxations for which finite VC dimension is sufficient for proper learning. In addition, we give new generalization guarantees for RERM. As future work, we are interested in understanding whether our robust loss relaxations can be used to mitigate the tradeoff between achieving adversarial robustness and maintaining high nominal performance. In addition, it is also interesting to find a combinatorial characterization of proper probabilistically robust PAC learning with respect to \(^{}_{,}\).