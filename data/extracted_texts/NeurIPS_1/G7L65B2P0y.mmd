# An effective framework for estimating individualized treatment rules

Joowon Lee\({}^{1}\), Jared D. Huling\({}^{2}\), Guanhua Chen\({}^{1}\)

\({}^{1}\) University of Wisconsin-Madison, \({}^{2}\) University of Minnesota

joowon.lee@wisc.edu, huling@umn.edu, gchen25@wisc.edu

###### Abstract

Estimating individualized treatment rules (ITRs) is fundamental in causal inference, particularly for precision medicine applications. Traditional ITR estimation methods rely on inverse probability weighting (IPW) to address confounding factors and \(L_{1}\)-penalization for simplicity and interpretability. However, IPW can introduce statistical bias without precise propensity score modeling, while \(L_{1}\)-penalization makes the objective non-smooth, leading to computational bias and requiring sub-gradient methods. In this paper, we propose a unified ITR estimation framework formulated as a constrained, weighted, and smooth convex optimization problem. The optimal ITR can be robustly and effectively computed by projected gradient descent. Our comprehensive theoretical analysis reveals that weights that balance the spectrum of a 'weighted design matrix' improve both the optimization and likelihood landscapes, yielding improved computational and statistical estimation guarantees. In particular, this is achieved by distributional covariate balancing weights, which are model-free alternatives to IPW. Extensive simulations and applications demonstrate that our framework achieves significant gains in both robustness and effectiveness for ITR learning against existing methods.

## 1 Introduction

Traditional medicine often uses a "one-size-fits-all" approach where the same treatment is applied to all patients regardless of their unique attributes, aiming to find a single optimal treatment that may be most effective for a broad population. However, since not everyone fits the mold, precision medicine has been popular in medical research, emphasizing personalized treatment based on a patient's unique characteristics .

One key element of precision medicine is the estimation of _individualized treatment regime_ (ITR), also known as treatment policy or policy. ITRs consider various factors like demographics and socio-psychological aspects to optimize treatment decisions, thereby maximizing individual outcomes. Many frameworks have been proposed for learning ITRs  and the related _conditional average treatment effect_ (CATE) estimation problem . For model interpretation and other practical concerns, researchers often focus on pre-specified rule classes. For example, the method proposed in  focuses on estimating ITRs using shallow-depth decision trees.

The central task in the ITR estimation is controlling confounding factors to isolate the causal effect of treatment from other factors that may influence the outcome. A standard approach is by weighting each sample using their _inverse probability weights_ (IPWs), which requires specifying a propensity score model. However, propensity score methods have long been known to be highly sensitive to model misspecification, which yields biased estimates of causal effects . _Distributional covariate balancing weights_ (DCBWs)  are modern alternatives to IPWs, directly minimizing the distance between empirical covariate distributions induced by the weights.

ITR estimation can be framed as an optimization or _maximum likelihood estimation_ (MLE) problem, where the choice of weights directly impacts both the optimization and likelihood landscapes. This, inturn, affects the computational and statistical errors in the ITR estimation, as well as the performance of the optimization algorithm. The key contribution of this work is to bridge the gap between causal inference and optimization perspectives in ITR estimation. Our main finding is both simple and surprising: _Covariate balancing weights not only address confounding but also yield near-optimal optimization and likelihood landscapes for the ITR estimation._ We show this claim by formulating the ITR estimation problem as a constrained, weighted, and smooth convex optimization problem. Our formulation is grounded in _angle-based direct learning_ (AD-Learning) framework , which is a recent framework for multi-category treatments with a linear decision function class assumption. While we focus on multi-category treatments--a relatively underexplored area--we believe that the principles are equally applicable to binary treatments with a linear decision function.

Besides advancing our theoretical understanding of the impact of covariate balancing weights, we propose a unified computational and statistical framework for the ITR estimation. Notably, we introduce a hard \(L_{1}\)-ball constraint to promote sparsity in regression coefficients, replacing the traditional soft \(L_{1}\)-penalization. This approach keeps the optimization objective smooth, enabling fast and accurate ITR estimation using _projected gradient descent_ (PGD). Furthermore, we integrate techniques such as (1) variable screening, (2) outcome augmentation, and (3) inverse variance weighting to account for heteroscedastic errors, which together show a synergistic effect.

**Related Work.** Most existing ITR approaches use IPW, which requires specifying a propensity score model for confounding control. Recent developments in causal inference have also introduced robust approaches that directly estimate weights based on balance-seeking objectives rather than relying solely on propensity scores. These approaches include entropy balancing weights , stable balancing weights  as well as DCBWs . However, despite the weights serving a critical role in the estimation process, these advancements have mainly focused on improving _average treatment effect_ (ATE) and are largely under-explored in the ITR literature. To the best of our knowledge, this is the first work to apply DCBWs for ITR-Learning under a weighted optimization framework. While recent works have taken a similar approach by utilizing weighting schemes and directly optimizing the weights rather than relying on estimated IPW , our method differs in key aspects. Unlike , which involves the evaluation of policy effects, our approach directly learns the optimal decision function, bypassing the need for intermediate policy effect evaluations. Although  introduced the concept of retargeting the population covariate distribution, which is conceptually related to our analysis, their approach relies on a reference policy, whereas our method does not. We also explore various statistical techniques to improve ITR-Learning. First, we perform _variable screening_ using the distance covariance test  to retain key effect modifiers and precision variables that directly impact the outcome. Second,  introduced _outcome augmentation_ to minimize estimator variance in clinical trials and observational studies by IPW involving binary treatments. Building on these foundations, our work extends these approaches to a broader range of weighting schemes, such as DCBW particularly for multi-category treatments. Lastly, with inverse variance weighting, as introduced in _stabilized angle-based direct learning_ (SABD-Learning) to address heteroscedasticity , this combined strategy enhances estimator precision and robustness.

**Contributions.** We summarize our contributions below.

* **Unified Framework of ITR-Learning**: We introduce a novel framework that addresses the limitations of existing ITR estimation methods by formulating the problem as a constrained, weighted, and smooth convex optimization problem. Under the framework, we propose a PGD algorithm for ITR-Learning under a hard \(L_{1}\)-constraint.
* **Improved Computational and Statistical Guarantees**: We establish convergence guarantees (Theorem 3.3). Furthermore, under mild assumptions, we demonstrate that the parameter of ITR-Learning can be consistently estimated with high probability (Theorem 3.5) and provide computational and sample complexity. In both cases, we demonstrate that using covariate balancing weights controls confounding factors and leads to better optimization and likelihood landscapes, resulting in improved computational and statistical performance.
* **Statistical Framework of ITR-Learning**: Our main contribution lies in providing a unified framework that combines DCBWs, variable screening, outcome augmentation, and inverse variance weighting in a synergistic and effective way. We demonstrated theoretical justifications for the combined approach, which, to our knowledge, have not been previously established.

Methods for ITR Estimation

### Model Setup

**Individualized Treatment Rule and Optimal Decision Function.** We observe a random vector \((,A,Y)\) where \(=(X_{1},X_{2},,X_{p})\) denotes the \(p\)-dimensional vector of pre-treatment covariates, a received treatment \(A=\{1,2,,K\}\), and the corresponding outcome \(Y\). Without loss of generality, we assume higher \(Y\) values are more favorable. Let \(Y(a)\) denote the random variable that describes the potential outcome that would have been observed were the individual assigned to treatment \(a\). We make the following standard assumptions using the potential outcome framework : (i) Stable Unit Treatment Value Assumption (SUTVA): \(Y=Y(A)\), (ii) positivity: \(0<(a,):=(A=a|)<1\) for all \(a\) and all \(\), (iii) no unmeasured confounding: \(Y(a) A|\) for any \(a\).

An ITR, \(d()\), is a function mapping each covariate \(\) to one of the \(K\) treatments. According to , the ITR \(d\) can be measured by the value function \(V(d)\):

\[V(d):=[Y(d())]=[Y|A=d()]= [(A=d())}{(A,)}]\]

Then the optimal ITR \(d^{}\) is defined as a function that maximizes the expected potential outcome \(d^{}()_{d}V(d)\) among all functions belonging to the treatment rule class (\(\)). Building on this, we adopt the following working model:

\[Y=()+_{k=1}^{K}_{k}()(A=k)+, \]

where \(()\) is treatment-free effect, \(_{k}()\) is interaction effect between covariates and \(k\)th treatment, and \(\) is a random noise with mean zero and variance \(^{2}(A,)\). For model identifiability, we assume \(_{k=1}^{K}_{k}()=0\) for all \(\). Under this model, \(_{k}()\) determines the optimal ITR, while \(()\) has no impact on the ITR. One approach to identifying the optimal ITR is to use \(K\) separate decision functions for each treatment with the sum-to-zero constraint, but this approach can be computationally inefficient .

Instead, one can opt for simplex coding as an alternative, which inherently satisfies the sum-to-zero constraint. In AD-Learning , each of the \(K\) treatments is represented as a vertex simplex on \(^{K-1}\), denoted as \(_{k}\), \(k=1,,K\). In particular, \(_{k}\) is defined as

\[_{k}=}_{K-1}&k=1,\\ -}{}}_{K-1}+} _{k-1}&k 2,\]

where \(_{K-1}\) is a \(K-1\) dimensional vector with entries 1, and \(_{k-1}\) is a \(K-1\) dimensional vector with entries 0 except its \(k\)-th entry being 1. This vertex simplex has \(K\) vertices with equal angles between them and an origin at the center of the simplex. All \(_{k}\) have the same Euclidean norm \(1\). Then the optimal ITR is reformulated as follows:

\[d^{}()=*{arg\,max}_{k\{1,,K\}} [Y|,A=k]=*{arg\,max}_{k\{1,,K\}} _{k}^{T}[}{(A, )}]}_{=_{}()}, \]

where \(\) is a random treatment vector in \(^{K}\), corresponding to the random treatment \(A\). Then estimating the optimal ITR can be converted into estimating the decision function \(()=(f_{1}(),,f_{K-1}())^{T}\), assigning a \(K-1\) dimensional vector to each covariate \(\). Here, while the optimal decision function \(_{}()\) can take a generic form, we assume the linear decision function within the linear treatment rule class \(\), induced by the linear decision function class \(=\{()=^{T}: ^{p(K-1)}\}\).

**Reduction to Weighted Convex Optimization.** Suppose we have a sample \((_{i},a_{i},y_{i})_{i=1}^{n}\) of size \(n\) from the joint population distribution for \((,A,Y)\). Estimating the optimal decision function \(_{}\) from the observed finite sample can be reduced to solving a weighted convex optimization problem:

\[_{^{p(K-1)}}\ _{i=1}^{n}w(a_{i}, _{i})\,(y_{i},_{i},a_{i};)+R(). \]There are three key components in the problem (3): the per-sample objective function \(\), the per-sample weights \(w\), and the regularization \(R()\). First, \(\) denotes a model-dependent per-sample convex objective function given by

\[(y_{i},_{i},a_{i};)=( {K}{K-1}y_{i}-_{a_{i}}^{T}^{T}_{i})^{2}& \\ (1+(_{a_{i}}^{T}^{T}_{i}))-y _{i}_{a_{i}}^{T}^{T}_{i}& \]

An important characteristic of causal inference problems is that only one of the possible treatments can be observed for each subject. Thus, it is critical to control confounding variables to isolate the causal effect of the treatment from other factors that might influence the outcome. This effectively is done by choosing the appropriate weight \(w_{i}=w(a_{i},_{i})\) for the \(i\)th subject. A classic approach is to use IPW: \(w(a,)=1/(a,)\).

### Proposed ITR Estimation Framework

**Reducing Finite Sample Bias.** The classic IPW approach requires specifying a propensity score model for confounding control. However, it is well-known that propensity score methods are highly sensitive to model misspecification, which yields biased estimates of causal effects [25; 30]. To address this issue, we investigate incorporating alternative, model-free weighting schemes such as _energy balancing weights_ (EBWs) [23; 24], which are a type of DCBWs. Other options for DCBWs include _maximum mean discrepancy_ (MMD) balancing weights [19; 27; 8] and Wasserstein distance-based balancing weights [52; 1]. Specifically, under the working model assumption (1), the optimal decision function \(_{}\) in (2) can be decomposed as

\[_{}()=)[ }{(A,)}|]}_{}+^{K}_{k}()[I(A=k)}{(A,)}|]}_{}+ [}{(A,)}|] [|]. \]

Since \(}{(A,)}|]=0\) and \([|]=0\), it follows that \(_{}()=_{k=1}^{K}_{k}() _{k}\), depending only on the interaction term. However, suppose the true propensity score is not correctly estimated. In that case, the estimated decision function may be biased and include additional factors including the treatment-free effect, leading to sub-optimal treatment decisions. There are two possible sources for this estimation error. First, an incorrectly specified propensity score model might lead to an estimated propensity score \(}_{n}(A=a|)\) that deviates from the true propensity score \((A=a|)\). Second, even with correct specification, insufficient sample size may lead to inaccurate finite-sample approximation \(}_{n}(A=a|)\) of the true propensity score \((A=a|)\). It results in systematic bias for the estimated coefficient of the treatment-free term:

\[}_{n}[}{(A,)}|]=_{k=1}^{K}_{k}}{(k,)}}_{n}(A =k|) 0.\]

Instead of using IPW, we use DCBWs such as EBWs. Such weights are data-driven and do not rely on specific model assumptions or large sample approximation. Furthermore, EBWs are known to promote independence between treatment and confounders , so \(}_{n}[w(A,)\,|\,]}_{n}[]\,}_{n}[w(A,)\,|\,]=0\) since \(}_{n}[]=0\). Thus, EBWs may reduce the unfavorable impacts of bias on the finite-sample decision function.

**Improving Optimization Guarantees.** To our best knowledge, the impact of the choice of the regularization \(R()\) on the model parameter \(\) on the detailed optimization landscape, especially in terms of the convergence rate and statistical estimation error bounds has been under-investigated in the literature. Contrary to the literature, we propose to use the combination of _soft \(L_{2}\)-regularization and a hard \(L_{1}\)-ball constraint:_

\[_{^{p(K-1)},\,\|\|_{1}} [():=_{i=1}^{n}w(a_{i},_{i })\,(y_{i},x_{i},a_{i};)+}{2}\|\|_{F }^{2}]. \]

The above formulation has a smooth objective function that is unaffected by the hard \(L_{1}\)-ball constraint. Also, \(L_{2}\)-regularization ensures that the objective is at least \(_{2}\)-strongly convex. Thus, the PGD algorithm (with suitable stepsize) enjoys a strong exponential convergence guarantee toward the global optimum. In contrast, standard approaches with \(L_{1}\)-penalization loss need sub-gradient methods since it is non-smooth. It is known that the convergence rate of the sub-gradient methods is much slower than gradient methods [6; 18].

We use the classical PGD to solve the convex-constrained optimization problem in (6). In each iteration, it involves conducting gradient descent with a suitable stepsize \(_{t}\), followed by a projection \(_{}\) onto the \(L_{1}\)-ball \(:=\{^{p(K-1)}\,:\,\|\| _{1}\}\):

\[_{t}_{}_{t-1}-_{t} _{}(_{t-1}). \]

For the projection onto the \(L_{1}\)-projection, we use the algorithm in . Detailed implementation of this PGD algorithm is discussed in Algorithm 1 in the appendix.

**Additional Improvements by Variance and Dimension Reduction.** To ensure the stability of estimates for both balancing weights and outcome regression models, a variable screening step is recommended. We propose adapting distance covariance test  for its computational efficiency and its ability to detect complex dependencies between treatment and outcome. By focusing on screened variables that significantly impact the outcome, we can improve the estimation of outcome regression models. Additionally, estimating balancing weights based on effect modifiers rather than all variables provides more reliable weighting estimates. Hence, the screening approach serves to reduce bias and enhance the effectiveness of ITR-Learning. See Algorithm 2 in the appendix.

Lastly, we propose using both augmented outcomes and inverse variance weighting for variance reduction in learning ITRs. Under the working model assumption in (1), the interaction effect determines optimal treatment assignments, while the treatment-free effect acts as additional noise. Outcome augmentation helps reduce variance by mitigating residual variability due to the treatment-free effect. Nevertheless, misspecification of either the outcome or treatment-free effect models can induce heteroscedastic errors due to residual treatment-free effects . In such scenarios, using inverse variance weighting, introduced in SABD-Learning, enhances robustness against misspecification . Consequently, the combination of outcome augmentation and inverse variance weighting offers a robust and efficient approach to ITR-Learning, as they are complementary methods. Details are discussed in the appendix C.3 and C.4.

## 3 Statement of results

Our theoretical analysis shows that a "weighted design matrix", specifically a \(p(K-1) p(K-1)\) positive semi-definite matrix, plays a central role in the local landscape analysis of the ITR estimation problem:

\[:=_{i=1}^{n}(a_{i},_{i})(_{a_{i}}_{a_{i}}^{T})(_{i}_{i}^{T} ), \]

where \(\) denotes the Kronecker product. Without loss of generality, we assume \(_{i}w(a_{i},_{i})=n\). Note that the weights \(w_{i}=w(a_{i},_{i})\), \(i=1,,n\) affect the eigenvalues of the weighted design matrix \(\). Our analysis reveals that, for the improved convergence rate of the PGD algorithm (Theorem 3.3) as well as a smaller statistical estimation error (Theorem 3.5), we need to choose the weights \(w_{i}\) such that the minimum eigenvalue of \(\) is as large as possible. Roughly speaking, this is achieved when \(w_{1},,w_{n}\) are 'covariate balancing weights' where the weighted sample covariance matrices conditional for each treatment are approximately the same. In the special case when the covariates are discrete and one-hot encoded, the optimal weights are the ones that exactly balance the covariate distributions given treatments. Thus this provides an explicit spectral characterization of the DCBWs in causal inference literature. See the appendix D for more discussions.

### Computational Guarantees

In order to control the strong convexity and the smoothness parameter of the ITR objective (6), we require that the eigenvalues of \(\) are uniformly bounded.

**Assumption 3.1** (Eigenvalue bounds on the weighted design matrix).: There are constants \(0^{-}^{+}\) such that the eigenvalues of \(\) in (8) is between \(^{-}\) and \(^{+}\),For each subject \(i=1,,n\), let \(z_{i}\) and \(p_{i}\) denote its 'activation' and 'predictive probability', where

\[z_{i}:=_{_{i}}^{T}^{T}_{i}, p_{i}: =(z_{i})/(1+(z_{i})). \]

We need the activation \(z_{i}\) to be uniformly bounded for our theoretical analysis of the binary outcome. Since the simplex vectors \(_{a_{i}}\) have unit length, it is enough to require the following assumption.

**Assumption 3.2** (Bounded activation).: There exists a constant \(M>0\) such that for all model parameter \(^{p(K-1)}\) in the constraint set \(\) and observed sample \(i=1,,n\), \(\|^{T}_{i}\| M\). Consequently, there exist constants \(0<^{-}^{+} 1/4\) such that \(^{-} p_{i}(1-p_{i})^{+}\) for all \(i\).

With our hard \(L_{1}\)-norm constraint on the model parameter \(\), Assumption 3.2 can be easily verified whenever the covariate vectors \(_{i}\) are uniformly bounded, which is a standard in the literature.

We establish the convergence rate of Algorithm 1 for ITR-Learning with stepsizes that are fixed but sufficiently small and diminishing rate. An informal statement is given in Theorem 3.3 below, and a full statement is provided in Theorem E.1 in the appendix. Define

\[:=^{-}+_{2}&L:=^{+}+_ {2}&\\ ^{-}^{+}+_{2}& \]

We say a function \(f:^{p}\) is \(\)_-strongly convex_ if \(f(x)-\|x\|^{2}\) is convex and \(L\)_-smooth_ if \(f\) is differentiable and \( f\) is \(L\)-Lipschitz continuous.

**Theorem 3.3** (Convergence rate of PGD for ITR-Learning).: _Let \((_{t})_{t 0}\) denote the sequence of parameters obtained by the PGD algorithm (7) for ITR-Learning problem (6) with arbitrary initialization \(_{0}\). Suppose Assumptions 3.1 and 3.2 hold. Let \(^{}\) denote the unique global optimum of (6). Then the following hold:_

**(i)**: _(Optimization landscape) The ITR objective_ \(()\) _in (_6_), is_ \(\)_-strongly convex and_ \(L\)_-smooth, where_ \(,L\) _are as in (_10_)._
**(ii)**: _(Linear convergence with fixed stepsize) Assume constant stepsize_ \(_{t}<2/L\)_. Denote the contraction constant_ \(():=\{|1- L|,|1-|\}(0,1)\)_. Then_ \(\|_{t}-^{}\|_{F}^{2}()^{t}\| _{0}-^{}\|_{F}^{2}\) _for all_ \(t 1\)_._
**(iii)**: _(Sublinear convergence with diminishing stepsize) Assume diminishing stepsize_ \(_{t}=\)_, where_ \(=(,L)>0\) _is a sufficiently large constant and_ \(>0\) _is an arbitrary constant. Then_ \(\|_{t}-^{}\|^{2}\) _for all_ \(t 1\) _for some constant_ \(>0\)_._

Our algorithm 1 with soft \(L_{2}\)-penalization and hard constraint with \(L_{1}\)-ball is guaranteed to converge to the true solution \(^{}\) exponentially fast provided stepsizes \(_{t}\) are fixed and sufficiently small.

### Statistical Guarantees

Next, we introduce generative models for the ITR estimation problem, for which we will establish statistical estimation guarantees in Theorem 3.5. Fix a joint distribution \(\) for the pair \((,A)\) of covariates \(^{p}\) and treatment \(A\{1,,K\}\). Fix a true parameter \(_{}^{p(K-1)}\). Then we assume \(n\) i.i.d. samples \((_{i},a_{i},y_{i})\) are drawn as \((_{i},a_{i})\) and

\[y_{i}|(_{i},a_{i})N(z_{i},^ {2})&\\ ()}{1+(z_{i})})&,z_{i}:=_{a_{i}}^{T}_{}^{T}_{i}, \]

where \(N(,^{2})\) denotes a normal distribution with mean \(\) and variance \(^{2}\), whereas \((p)\) denotes a Bernoulli distribution with mean \(p\). We assume that \(\) does not depend on \(_{}\) and the noise variance \(^{2}\) for the continuous case is known. We then seek to estimate the true parameter \(_{}\) from the observed samples. It is easy to check that the negative log-likelihood function (up to additive constants) coincides with the per-sample loss function \(\) in (4). Therefore, the weighted convex optimization problem (3) corresponds to weighted MLE under the generative models in (11) (see Section H in the appendix for details).

We assume that the weighted gradient of the per-sample loss \(\) in (4) has uniformly bounded third moments for our statistical analysis of the generative ITR model above.

**Assumption 3.4** (Bounded moments at \(_{}\) and weights).: Suppose \((_{i},a_{i},y_{i})\) follows the generative model above. Denote \(U_{i}:=w_{i}_{}(y_{i},_{i},a_{i};_{ })^{p(K-1)}\) for \(w_{i}:=w(a_{i},_{i})\) and \(_{i}:=U_{i}-[U_{i}]\). Suppose there are constants \(D_{1},d_{1}(0,)\) such that \([\|_{i}\|_{F}^{3}]<D_{1}\) and \(_{k,l}(_{i}(k,l))>d_{1}\). Also, \(_{i=1}^{n}w_{i}^{3}/(_{i=1}^{n}w_{i}^{2})^{3/2}=O(n^{-1/2})\).

Now we state our main result regarding the statistical estimation guarantee for the true generative model parameter \(_{}\). With high probability, Algorithm 1 can recover \(_{}\) up to a statistical error \(O(n^{-1/2})\).

**Theorem 3.5** (Statistical estimation guarantee).: _Let \((_{i},a_{i},y_{i})_{i=1}^{n}\) be i.i.d. observations from the generative models in (11) with true parameter \(_{}^{p(K-1)}\) such that \(\|_{}\|_{1}_{1}\) for some \(_{1} 0\). Suppose Assumptions 3.1, 3.2, and 3.4 hold. Let \(}_{T}\) denote the weighted MLE obtained after \(T\) iterations of PGD algorithm (7) for (3). Fix a constant \(>0\)._

_Then there exists a constant \(C=C()>0\) such that with probability at least \(1-\) and for \(\) as in (10) but with \(^{}=^{}([])\),_

\[\|_{}-}_{T}\|_{F}}}{}+\|_{}\|_{F}}{}, \]

_provided \(n,T\) are large enough. More specifically, the above holds when_

**(i)**: _(Sample complexity)_ \(n 1\) _large s.t._ \(^{n}w_{i}^{2})^{3}}{(_{i=1}^{n}w_{i}^{2})^{2}} C_{1} ^{-2}\) _for some explicit constant_ \(C_{1}>0\)_;_
**(ii)**: _(Computational complexity)_ \(T=O( n)\) _and_ \(O(n^{-1})\) _when constant or diminishing stepsize as in Theorem_ 3.3 _is used, respectively._

_Furthermore, we get \(\)-consistent estimation whenever \(_{2}=O(n^{-1/2}\|_{}\|_{F}^{-1})\)._

Notice that both terms in the error bound (12) are proportional to \(1/\), which depends both on the minimum eigenvalue \(^{-}\) of \([]\) as well as the \(L_{2}\)-regularization parameter \(_{2}\). Therefore, choosing the weighting function \((A,)\) to maximize \(^{-}\) helps minimize the overall statistical estimation error. (Detailed analysis and discussion on this point can be found in the appendix H.) Another easy way to increase \(\) is to use large \(L_{2}\)-regularization, but as in the second term in (12), using \(L_{2}\)-regularization gives estimation bias of \(O(_{2}\|_{}\|_{F}/)\). Hence, we need to choose \(_{2}\) is small enough so that this bias term is also of \(O(n^{-1/2})\). Then we obtain an overall \(\)-consistent estimator.

The key insight in our proof of Theorem 3.5 is that we can decompose the total estimation error \(\|_{}-}_{T}\|_{F}\) into computational and statistical parts:

\[\|_{}-}_{T}\|_{F}}-}_{T}\|_{F}}_{}+_{}-}\|_{F}}_{}, \]

where \(}\) denotes the exact MLE. The computational error vanishes as the number \(T\) of PGD iterations tends to infinity according to Theorem 3.3. For the statistical error, we show

\[(\|_{}-}\|_{F}}+\|_{}\|_{F}}{}) 1-O (^{n}w_{i}^{3}}{(_{i=1}^{n}w_{i}^{2})^{3/2}}). \]

Thus, the'skewness' of the weights \(w_{i}\) measured by the ratio \(_{i=1}^{n}w_{i}^{3}/(_{i=1}^{n}w_{i}^{2})^{3/2}\) (which arises from Berry-Esseen bound) acts as a scalar multiple to the standard statistical error of \(O(n^{-1/2})\) per the central limit theorem. Hence, more balanced weights lead to fewer statistical estimation errors. If this skewness ratio is uniformly bounded, we can ensure a small statistical error with probability at least \(1-\), provided we have at least \(O(^{-2})\) samples. Then, we simply need to ensure the PGD iteration \(T\) is large enough so that the computational error is at most \(C/\) to achieve the high-probability total estimation error bound in Theorem 3.5. Another insight from the theorem is that maximizing \(_{}([])\) promotes heterogeneous balancing weights \(w_{i}\), and we get increased sample complexity through the bound \(^{n}w_{i}^{2})^{3}}{(_{i=1}^{n}w_{i}^{2})^{2}} C_{1} ^{-2}\) (e.g., if \(w_{i} 1\), this is \(n^{-2}\), but if \(w_{1}=n,w_{2}==w_{n}=0\), this may not be satisfied for any \(n\)).

## 4 Simulation Studies

We investigate four simulation settings, each designed to evaluate various factors influencing the performance of different ITR estimators. For each setting, we mimic a randomized trial (no-confounding) and an observational study (with confounding) as per . We consider \(4\) treatments with sample sizes \(n=200,600,1000\), and covariate dimensions \(p=20,40,60\). The outcome \(Y\) follows the model (1) with \(|A, N0,^{2}(A,)\). We evaluate method performance using accuracy (i.e., correct identification of the optimal treatment for each observation) and empirical value on a test dataset of \(10,000\) observations. Each simulation setting is replicated independently \(100\) times.

In this section, we provide partial experiment results according to the proposed estimation framework in Section 2.2. Specifically, we focus on the results from two scenarios: one mimicking randomized trials with linear ITR as the optimal rule and the other mimicking observational studies with nonlinear ITR as the optimal rule for continuous outcome. Since our proposed method assumes linear treatment rule class, the latter involves a situation where our model's prespecified treatment rule class is misspecified. Specifically, we use following treatment-free effect function \(()\) and interaction effect function \(()\) for each scenario:

1. Randomized Trial: Linear ITR as the true optimal \[() =1+2X_{1}+2X_{2},\] \[() =0.75+1.5X_{1}+1.5X_{2}+1.5X_{3}+1.5X_{4},A=1;0.75+1.5X_{1}-1.5X_{2}-1.5X_{3}+1.5X_{4},A=2;\\ 0.75+1.5X_{1}-1.5X_{2}+1.5X_{3}-1.5X_{4},A=3;0.75-1.5X_{1}+1.5X_{2}-1.5X_{3}-1. 5X_{4},A=4,\]
2. Observational Study: Nonlinear ITR as the true optimal \[() =1+2X_{1}+2X_{2}+2X_{4}-2X_{4}^{2}+2X_{1}X_{2}+2e^{-X_{1}X_{2}}+ (X_{3}),\] \[() =0.5+1.0X_{1}-2.0X_{4}+0.5X_{4}^{2},A=1;\ 1.0+1.0X_{1}+1.0X _{4}-1.0X_{4}^{2},A=2;\\ 1.5+2.0X_{1}-1.0X_{4}-1.0X_{4}^{2},A=3;\ 1.0-1.0X_{1}-1.0X_{4}-1.0X_{4}^{2},A=4. \]

in the working model (1). Further details about simulation settings, other scenarios, and corresponding results are available in the appendix I.

Figure 1: **Accuracy Comparison: proposed methods vs. benchmark methods. All subplots in the same row share the same simulation setting, focusing on randomized trials with linear ITR as the true optimal rule (top) and observational studies with nonlinear ITR as the true optimal rule (bottom). Each subplot presents (Left) accuracy comparisons based on weights, illustrating the difference between the standard IPW approach of AD-Learning with the proposed approach using EBW, (Middle) accuracy comparisons based on optimization algorithms, illustrating the difference between the standard \(L_{1}\)-penalized approach against the proposed constrained optimization with PGD, (Right) evaluation of accuracies between existing standard approaches and the proposed method, which integrates EBWs, variance and dimension reduction techniques implemented through PGD. Error bars represent the standard errors of the mean (SEM) of accuracies across multiple simulations.**

**Using Distributional Covariate Balancing Weights.** In the left subplots of Figure 1, the incorporation of DCBWs (e.g., EBWs) shows improvements in classification rates for ITR-Learning. While EBWs indeed provide more effective balancing weights in observational studies, as depicted in the lower left subplot in Figure 1, it also results in improvements even in randomized trial settings where the true propensity score is known. This is achieved by reducing finite sample imbalances, as shown in the upper left subplot in Figure 1. Moreover, when the underlying true treatment rule class is nonlinear, the performance of AD-Learning suffers due to a misalignment between the assumed linear treatment rule class and the actual class. In this scenario, using EBWs yields accuracy improvements compared to the conventional IPW approach. Further details can be found in the appendix I.

**Using PGD for Constrained Optimization.** The middle subplots of Figure 1 show accuracy comparisons between standard penalized approach and constrained optimization using PGD. Both optimization algorithms are based on the same proposed statistical approach, integrating EBWs with SABD-Learning, outcome augmentation, and additional variable screening. For solving the \(L_{1}\)-penalized regression problems, the R package _glmnet_ is a standard choice, which is based on cyclic coordinate descent with simulated-annealing-style hyperparameter tuning. However, _glmnet_ does not handle hard \(L_{1}\)-ball constraints required for our method. Therefore, to ensure a fair comparison, we directly implemented standard (projected) (sub)gradient descent to solve optimization problems for both penalized and constrained problems. In our experiment reported in Figure 1, we observe significant improvements using our PGD method, especially in the second scenario. One possible explanation for this improvement is that large penalization may yield a perturbed solution to the ITR problem. If the minimizer of the ITR problem lies on the boundary of the \(L_{1}\)-ball, the solutions of constrained optimization and penalized optimization coincide. However, if the minimizer is the interior point of \(L_{1}\)-ball, constrained optimization can find the exact solution, while the penalization may induce a perturbed solution. Furthermore, the trajectory of PGD for the constraint approach appears to be significantly more stable than that of the subgradient descent in the penalization approach (see Figure 2).

**Performance Evaluations between the Proposed Method and Benchmark Methods.** In the right subplots of Figure 1, we evaluate accuracies by comparing benchmark methods to our proposed approach. The benchmarks include AD-Learning, SABD-Learning, a treatment-covariate interaction model with \(L_{1}\)-regularization 'linear', and two popular tree-based methods:'policytree'  and double-machine learning-based 'causalDML' . Our proposed method integrates EBWs with SABD-Learning, outcome augmentation, and additional variable screening for variance and dimension reduction, implemented via PGD. However, causalDML requires a sufficiently large sample size, thereby making its evaluation unfeasible under a sample size of 200. Our numerical analysis provides evidence that our proposed approach outperforms existing methods in both scenarios. 'linear' performs well in the upper subplot, which simulates a randomized trial with a true optimal linear rule, as covariates are balanced and the model accurately captures the decision function. However, in the lower subplot, simulating an observational study with a nonlinear rule, the model performs poorly because it lacks balancing weights to address confounding and cannot fully capture nonlinear relationships. Further detailed analysis, including the effectiveness of additional techniques such as variable screening for performance improvement, is provided in the appendix I.

Additionally, our proposed method processes each dataset (p = 60, n = 1000) in an average of 4.40 seconds, compared to 2.01 seconds for the penalization method using _glmnet_ in R. Other methods exhibit scalability challenges, with policytree averaging 12.02 seconds and causalDML taking 46.75 seconds. This demonstrates that computational efficiency is a notable advantage of our approach.

Figure 2: **Comparison of Optimization Methods by \(}\). \(_{1}\) is \(L_{1}\)-ball size of the constraint set in the constrained optimization and the regularization parameter with additional \(L_{1}\)-regularization of the model parameter in the penalized regression, respectively.**

## 5 Applications

We apply the proposed methods to two datasets from AIDS Clinical Trials Group (ACTG) 175  and email marketing . ACTG is a randomized trial for 2,139 patients with HIV infection who were randomly assigned to four different treatments. Twelve covariates including age and gender were used for the analysis. The outcome is the change in CD4 cell count from baseline to 20 weeks, where larger values are preferable. Email marketing dataset has 64,000 customers who were randomly chosen to receive an e-mail campaign among three different marketing methods. Eight covariates with historical customer attributes and their pairwise interactions were used. The outcome is whether customers visited the website in the following two weeks. The goal of the analysis using these two datasets is to find the best treatment/marketing methods based on individuals' attributes. Similar to , we randomly split the data into a training set of \(\{200,400,800,1000,1200\}\) observations for the ACTG dataset and \(\{1000,3000,5000\}\) observations for the email dataset. The remaining observations were used for test data with 10 iterations. We evaluate method performance using the empirical value function on a test dataset. The benchmarking methods include AD-Learning, SABD-Learning, and two tree-based methods (policytree  and causalDML ). However, causalDML requires a sufficiently large sample size, thereby making its evaluation unfeasible under a sample size of 200. For the binary outcome in Email dataset, SABD-Learning is not used, since the homoscedastic assumption is unnecessary for logistic regression. The results are shown in Table 1.

## 6 Discussion and Limitations

Our method introduces a unified, robust framework for estimating ITR. By formulating the problem as a weighted, constrained optimization problem, we incorporate DCBWs to control confounding and propose the PGD algorithm for sparse ITRs with computational and statistical guarantees. Additionally, we propose variable screening and efficient augmentation, which together show synergistic effects. We demonstrate our method with continuous and binary outcomes, and it can be readily extended to other outcomes, such as censored outcomes. Although we focus on learning linear ITR to facilitate the demonstration of our idea, it can be extended to nonlinear ITR using the basis function . For example, by taking the covariate powers up to M, the linear function class becomes the class of degree-M polynomial functions. The linear function class can also serve as a useful approximation to nonlinear decision functions.

We acknowledge that although most, if not all, types of ITR-Learning frameworks can benefit from using DCBWs, variable screening, and augmentation, our proposed framework for theoretical analysis relies on the specific assumption that the model can be formulated as a constrained, weighted, and convex optimization problem. Furthermore, we restricted the decision function to the linear function class. Generalizing our theoretical analysis to include nonlinear function classes with nonconvex problems, such as boosting or deep neural networks, is an interesting direction for future investigation. Another limitation of our method is its reliance on standard assumptions to identify optimal ITRs using observational data, including the assumption of no unmeasured confounding. An interesting future direction would be to extend our framework to cases where this assumption does not hold, utilizing instrumental variables [12; 38] and proximal causal learning [40; 45].

Our work significantly impacts society by improving patient health outcomes through personalized care and advancing medical research in disease mechanisms and drug development. Beyond healthcare, these methods can be applied in marketing strategies based on customer characteristics.

  Dataset & Training size & AD & SABD & policytree & causalDML & **Proposed** \\   & 200 & 37.624 (6.9) & 33.645 (6.7) & 36.871 (4.6) & - & **42.301** (5.7) \\  & 400 & 51.329 (3.2) & 53.644 (1.9) & 47.191 (2) & 49.552 (0.2) & **54.868** (3.4) \\  & 800 & 56.021 (1.5) & 56.700 (1.6) & 52.786 (2.6) & 55.506 (2.1) & **59.200** (2.5) \\  & 1000 & 56.825 (2.2) & 56.963 (2.1) & 56.005 (1.7) & 56.591 (2.1) & **59.552** (2.5) \\  & 1200 & 55.092 (2.6) & 55.221 (2.8) & **58.819** (3.4) & 57.104 (2.8) & 58.207 (2.9) \\   & 1000 & 0.169 (0.0) & - & 0.171 (0.0) & 0.172 (0.0) & **0.175** (0.0) \\  & 3000 & 0.176 (0.0) & - & 0.177 (0.0) & 0.177 (0.0) & **0.181** (0.0) \\   & 5000 & 0.177 (0.0) & - & 0.178 (0.0) & 0.178 (0.0) & **0.182** (0.0) \\  

Table 1: Average empirical value functions across different approaches for ACTG/Email datasets. Mean values with the corresponding standard errors of the mean (SEM) in parentheses are provided. The highest-performing methods are marked in bold.