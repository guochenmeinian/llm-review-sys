# Order Matters in the Presence of Dataset Imbalance

for Multilingual Learning

 Dami Choi\({}^{*}\)

U. Toronto & Vector Institute

choidami@cs.toronto.edu

&Derrick Xin\({}^{*}\)

Google Research

dxin@google.com

&Hamid Dadkhahi

Google Research

hdadkhahi@google.com

&Justin Gilmer

Google Deepmind

gilmer@google.com

&Ankush Garg

Google Deepmind

ankugarg@google.com

&Orhan Firat

Google Deepmind

orhanf@google.com

&Chih-Kuan Yeh

Google Deepmind

chihkuanyeh@google.com

&Andrew M. Dai

Google Deepmind

adai@google.com

&Behrooz Ghorbani

OpenAI

ghorbani@openai.com

Equal contribution Work done as a student researcher at Google.

###### Abstract

In this paper, we empirically study the optimization dynamics of multi-task learning, particularly focusing on those that govern a collection of tasks with significant data imbalance. We present a simple yet effective method of pre-training on high-resource tasks, followed by fine-tuning on a mixture of high/low-resource tasks. We provide a thorough empirical study and analysis of this method's benefits showing that it achieves consistent improvements relative to the performance trade-off profile of standard static weighting. We analyze under what data regimes this method is applicable and show its improvements empirically in neural machine translation (NMT) and multi-lingual language modeling.

## 1 Introduction

Over the past few years, large multi-task neural networks have emerged as a popular modeling paradigm in deep learning. The appeal behind these models is that they can leverage transfer learning among the tasks to outperform single-task models. Indeed, multi-task models have achieved state-of-the-art performance in domains such as machine translation , language understanding , and speech recognition .

Unfortunately, optimizing such multi-task models remains a challenge. To effectively train these models, the different tasks need to be balanced during training. This is often done by sampling each task with a static probability.

Prior work  shows evidence that when all tasks are in the data rich regime (high-resource), such static sampling approaches yield optimal results. However, when certain tasks are data sparse(low-resource)2, which is quite common in real-world applications, the optimality of static sampling is unclear.

The problem with static sampling in the presence of low-resource tasks is that it has difficulty dealing with overfitting on the low-resource tasks. This is because early stopping is not a viable solution due to high-resource tasks needing many more epochs to converge. The transfer learning scheme of pre-training on high-resource and fine-tuning on low-resource tasks (such as in ) provides a solution to the overfitting problem, since the training of high and low-resource tasks are separated. Not only this, but the training of low-resource tasks can potentially benefit from positive transfer that comes from performing well on the high-resource tasks. The problem with this approach, however, is that during the fine-tuning phase, catastrophic forgetting of the pre-training tasks ensues.

In this paper, we introduce a simple training scheme that combines the best of static sampling and transfer learning: pre-train on a high-resource task and fine-tune jointly on a mixture of high and low-resource tasks. A pre-training and fine-tuning scheme effectively enables early stopping by allowing the training of low-resource tasks to happen for as little as needed to prevent overfitting, while training the high-resource task for as long as needed. Furthermore, pre-training on a high-resource task will potentially enable positive transfer for low-resource tasks and result in faster convergence in the fine-tuning phase. Lastly, the fine-tuning phase on a mixture of high and low-resource tasks will not only remedy the catastrophic forgetting issue of fine-tuning only on low-resource tasks, but also enjoy further transfer learning among all the tasks.

Through an extensive empirical study, we find that the pre-training and joint fine-tuning scheme yields superior low-resource task performance compared to both static sampling and the transfer-learning scheme. We observed that the performance improvement on static sampling is driven by two mechanisms. The first is that pre-training initializes the fine-tuning phase at a better starting point than random initialization due to positive transfer. The second is that higher sampling rates are more data-efficient than lower sampling rates. Because our method has two separate training phases, the low-resource-training phase can be short. This in turn enables us to increase the low-resource sampling rate without risking overfitting. Indeed, our method is more data-efficient than static sampling in terms of the low-resource tasks throughout the entire fine-tuning phase, achieving better low-resource task performance while using only a fraction of the data seen by static sampling. We further observe that pre-training and joint fine-tuning seems to have a regularization effect. However, we find that regularization is not the main factor behind the performance improvement, since increased explicit regularization, such as dropout, does not improve the performance to the extent that our method does.

The contributions of this paper can be summarized as follows:

* To the best of our knowledge, we are the first to show that it is possible to push the Pareto front of static sampling in the data-imbalanced regime.
* We present a simple algorithm that can be readily used to boost low-resource tasks' performance in multilingual models.
* We show on realistic workloads (up to 13B parameters) that our scheme performs better than static sampling and transfer learning with respect to the low-resource language-pair/language.

## 2 Background

In our work, we focus on the supervised setting, where our model parameters \(^{p}\) are trained on \(K\) different tasks, with the loss for task \(i\) being \(_{i}()\).

We introduce the idea of Pareto optimality to better explain the trade-off effect that happens when training on many different tasks.

**Definition** (Pareto Optimality).: \(^{p}\) _Pareto dominates another \(^{}\) if \( 1 i K\), \(_{i}()_{i}(^{})\) and there exists a task \(j\) where \(_{j}()<_{j}(^{})\). \(\) is Pareto optimal if it is not dominated by any other point. The collection of the Pareto optimal points is denoted as the Pareto front._A standard approach for optimizing multi-task models is _scalarization_ or static sampling:

\[}()=_{}_{i=1}^{K}_{i}_{i}(), \]

where \(\) is a fixed vector of pre-determined task weights with \(>0\) and \(_{i}_{i}=1\).

In our work, we follow convention and implement scalarization via proportional sampling, where data from task \(i\) is sampled with probability equal to \(_{i}\). In this case, the expected loss is equal to the loss from scalarization:

\[()=_{}[(;) ]=_{i=1}^{K}(i)_{i}[(;)]=_{i=1}^{K}_{i} _{i}(). \]

Prior work  studied the performance trade-off behavior of scalarization and a variety of different multi-task optimization (MTO) methods in the two-task setting. They found that both in the high-resource case and in the data-imbalanced case, no MTO method improved upon the Pareto front of scalarization. In our work, we compare the performance trade-off behavior of scalarization and our proposed method, and find that the Pareto front of scalarization can be improved in the data-imbalanced regime.

Note that practically speaking, it is not feasible to determine whether \(\) is truly Pareto optimal since we must check that it is not dominated by all \(^{}^{p}\). Following , instead of considering all of \(^{p}\) we consider only the parameters reachable by a fixed set of hyperparameters.

## 3 Pre-training Joint Fine-tuning

Given \(K\) tasks, among which some are low-resource, our goal is to optimize the performance of the low-resource tasks without sacrificing the performance of the remaining tasks. Static sampling is not ideal because all tasks are seen constantly throughout the entirety of training, resulting in overfitting of low-resource tasks while high-resource tasks still need to be learned. Naively breaking up training into two phases and training on low-resource tasks in the later phase results in catastrophic forgetting of earlier-trained tasks.

Assuming the existence of at least one high-resource task, we propose to first pre-train on a high-resource task, and fine-tune the resulting model on the full mixture of \(K\) tasks. We call this method **pre-training joint fine-tuning3**.

In our preliminary experiments, we found that it is important to reset the learning rate schedule and optimizer state when switching over to the joint fine-tuning phase. This is because learning is extremely slow for tasks that are newly introduced when the learning rate has already decayed. In our evaluations, we additionally experiment with adding resetting to the scalarization baseline to ensure that improvements from our method are not purely from resetting. See Sections 4.1.2 and 4.2 for more detail.

Our two-stage training process introduces additional hyperparameters compared to scalarization: the hyperparameters involved in the pre-training phase, and the length of the pre-training phase. However, we find that tuning is not much more difficult than scalarization, and in some cases it is easier to tune. The pre-training phase only involves tuning for a single task, which is much easier than tuning for multiple tasks. We also expect the joint fine-tuning phase to be shorter than the full training length of scalarization; therefore, tuning for the second phase should be around the same or easier than scalarization. Lastly, our results show that pre-training does not hurt fine-tuning performance and longer pre-training translates to better fine-tuning. From this, we recommend that if there is a strict training budget, it is better to be conservative and pre-train for a shorter amount of time. However, if the goal is to obtain the best performance and there is no strict compute budget, we recommend pre-training for as long as possible before fine-tuning. See Section 4.3 for more details.

[MISSING_PAGE_FAIL:4]

amount of compute, pre-training joint fine-tuning makes hyperparamter tuning much more efficient, since 1) tuning for pre-training is on a single task and therefore, easier to tune, and 2) tuning for fine-tuning is faster since \(N_{2} N\).

In Figure 2 we can observe that pre-training joint fine-tuning is able to achieve performance trade-off points that go beyond what is achievable via scalarization. Pre-training on a high-resource language pair creates non-dominated points by yielding significantly better performance in the low-resource task (En\(\)Ro) without completely sacrificing performance in the high-resource task (En\(\)Fr). Additionally, it is able to do this while seeing less overall Romanian tokens according to Figure 3.

We see similar results for En\(\){Hi, Fr}, shown in Figure 12 in the Appendix. This is a surprising result since French and Hindi are less linguistically similar than French and Romanian. Finally, we can see from the sub-optimal performance of the restart baseline in Figures 2 and 12 that the act of resetting is not the reason behind the success of the pre-training joint fine-tuning scheme. We provide BLEU score evaluations for En\(\){Ro, Fr} and En\(\){Hi, Fr} in Appendix A.5, validating that the improvements in loss translate to downstream metrics.

Figure 1: The trade-off front from pre-training does not improve upon the trade-off front from fully static sampling when all tasks are high-resource. The performance on each of the high-resource tasks are bounded by the amount of data seen for that task. We can also observe interference between the two tasks from how all 9 different sampling rates form the trade-off frontier. These observations hold for both testing (_left_) and training (_right_).

Figure 2: (_Left:_) In the data-imbalanced case, the trade-off front from pre-training yields better low-resource task performance than the trade-off front of scalarization. The poor performance of the restart baseline shows that the resetting of states is not why pre-training and fine-tuning performs well. Note that the trade-off fronts consist of only a subset of the sampling ratios due to overfitting, which is different from the fully high-resource setting. _Right:_ Pre-training results in a noticeably worse performance on the training set, hinting that pre-training has a regularization effect on the low-resource task.

#### 4.1.3 Analysis

The performance improvement of pre-training joint fine-tuning stems from two main mechanisms.

* Pre-training utilizes positive transfer between tasks, and initializes the fine-tuning phase at a better starting point than random initialization. Figure 3 shows this effect for the En\(\){Ro, Fr} translation tasks.
* Higher sampling rates are more data-efficient than lower sampling rates. Figure 4 shows how optimization (training set performance) gets more and more data-efficient as the sampling rate increases. However, on the generalization side, increasing the sampling rate works only up until a certain point, where overfitting kicks in.

By design, pre-training joint fine-tuning has two separate training phases which allows the low-resource-training phase to be short. This in turn enables us to increase the low-resource sampling rate, resulting in faster training. This effect can be seen in Figure 2, where the En\(\)Ro sampling rates that resulted in the best En\(\)Ro performance was 0.4, while for pre-training joint fine-tuning, the best rate is 0.5. Figure 3 confirms that indeed after pre-training, fine-tuning on En\(\)Ro is more data-efficient than not pre-training.

Joint fine-tuning is also an important piece in addition to the two-stage setup. Only fine-tuning on the low-resource task, which is the classic transfer learning scheme, results in overfitting and catastrophic forgetting of the pre-training task as shown in Figure 6.

Lastly, Figure 2 shows that pre-training joint fine-tuning yields worse training set performance, and therefore, could be seen as having a regularization effect. We show in Figure 5 that regularization by itself does not explain the superior performance of our scheme.

The results seen so far show that data order matters when training in the presence of a low-resource task, since seeing high-resource data first before seeing low-resource data later pushes the pareto front of seeing both types of data at the same time.

Figure 4: Each curve corresponds to a single scalarization trial with a particular (static) sampling rate for En\(\)Ro. The rate at which the training loss decreases is slower for lower En\(\)Ro sampling rates than for higher sampling rates. At higher sampling rates, overfitting starts to happen.

Figure 3: Pre-training joint fine-tuning has both better initialization and data-efficiency than scalarization. Each line corresponds to the datapoint that achieved the best En\(\)Ro validation loss in Figure 2 among the different run groups.

### Multilingual Training

In this section, we expand from a two-task setting to a many-task setting. We train on five languages from the mC4 dataset -English, Hindi, Gujarati, Swahili, and Gaelic- using the span corruption objective from T5 . See Table 2 for details on the dataset. Connoically the mC4 dataset is used in the pre-training phase for models (not to be confused by our pre-training joint fine-tuning method). These models are subsequently applied to downstream tasks such as question answering. This multilingual pre-training phase is also known as the language balancing problem. Our goal is to show that our two stage method can effectively balance high-resource and low-resource languages, improving performance on low-resource languages beyond what is achievable by the conventional method of temperature sampling while not sacrificing performance on high-resource languages.

Note that in the mC4 corpus, English is \(16745\) times larger than the smallest language we use. This data imbalance underscores the necessity for effective language balancing, particularly in determining the proportion of each language to be used during training. This presents a highly challenging and computationally demanding problem, as it is not feasible to simply sweep the scalarization weights as one would in a two-task setting.

For our training setup we closely follow mT5  for the model architecture and training procedure. Specifically, we use the mT5-XXL model (13B parameters), which is an encoder-decoder transformer architecture. Additional training details are available in Appendix B.

Temperature SamplingBecause we increase the amount of tasks in this setting, detailing the full scalarization trade-off frontier would be computationally infeasible. Therefore, we employ the widely used _temperature sampling_ heuristic [11; 7; 2]. Let \(D_{i}\) be data size of language or task \(i\), we then define the empirical distribution \(\) for each task \(i\) as:

\[(i)=}{_{j}D_{j}}. \]

   Language & \# Chars (B) \\  En (English) & \(13,396\) \\ Hi (Hindi) & \(75\) \\ Gu (Gujarati) & \(3.6\) \\ Gd (Gaelic) & \(0.8\) \\ Sw (Swahili) & \(4.1\) \\   

Table 2: Data used from mC4.

Figure 5: pre-training joint fine-tuning has a regularization effect, but cannot be replaced by simply increasing regularization strength. The dropout rate used in pre-training joint fine-tuning is 0.1.

Figure 6: Fine-tuning solely on the low-resource task (En\(\)Ro) leads to both catastrophic forgetting of the pre-trained task (En\(\)Fr) and worse low-resource task performance than fine-tuning on all tasks (En\(\){Ro, Fr}).

Temperature sampling then uses a distribution \(\) defined by a temperature parameter \(\) as follows:

\[(i)=(i)^{1/}}{ _{j}(j)^{1/}} \]

The temperature parameter \(\) controls the peakiness (or flatness) of the sampling distribution. Commonly used \(\)'s in the literature are greater than 1, which essentially up-samples low-resource tasks and down-samples high-resource tasks.

Static Sampling BaselineTemperature sampling is ubiquitous due to its simplicity and intuitiveness, but its performance varies greatly with \(\). For our static sampling baseline, we tuned \(\) among commonly used values in the literature (1.43, 2, 3.33, 5) at a smaller scale, and found that \(=3.33\) performed the best in terms of low-resource languages. We also tried a more intricate sampling strategy called UniMax , but found that on the 5 languages we chose, it did not perform better than \(=3.33\).

Pre-training joint Fine-tuningFor our pre-training joint fine-tuning setup, we first pre-train on English, reset the optimizer state and learning rate schedule, and then fine-tune on all 5 languages using temperature sampling. We use the same sampling rates as the static sampling baseline (\(=3.33\)) to reduce the tuning overhead over static sampling.

As in the NMT experiments, we employ a restart baseline to fully ablate the pre-training fine-tuning scheme. The restart baseline resets the optimizer state and learning rate schedule in the middle of training for the static sampling baseline.

ResultsFigures 7 and 8 show that while a learning rate schedule restart helps performance, pre-training joint fine-tuning yields the best results on the low-resource tasks. Surprisingly, it not only improves the performance on Gujarati, Gaelic, and Swahili, but also shows a slight enhancement on English. We note that due to the vast dataset imbalance, the temperature sampling baseline overfits on the low-resource tasks before English has a chance to converge. Consequently, pre-training joint fine-tuning can leverage the benefits mentioned in the previous section-regularization, transfer, and reduced forgetting-to achieve a superior lower bound performance with higher token efficiency.

Figure 8: Pre-training on English and joint fine-tuning on all 5 languages leads to better optima for Gujarati, Gaelic and Swahili, the 3 low-resource languages. Pre-training also results in better initialization and token-efficiency for all languages newly seen in the fine-tuning phase.

Figure 7: Pre-training joint fine-tuning yields the best performance in 4 out of 5 languages, with significant improvements in the low-resource tasks.

### Length of Pre-training

Our method is simple but comes with some choices to make, one of which is the number of steps to pre-train for. We investigate the effect of the number of pre-training steps in NMT and language modeling on mC4 by pre-training with less, and more steps than in the previous sections. With the language modeling task, we fix the total training length to be 500k steps to emulate a compute-constrained scenario. We chose to use a smaller model (mT5-XL as opposed to mT5-XXL used in Section 4.2 for faster training). With NMT, we fix the number of fine-tuning steps, but let the total training steps vary.

Figure 9 displays the effects of varying pre-training length in the mC4 experiments. We see that longer pre-training improves best achievable performance on the low-resource tasks of Gujarati, Gaelic, and Swahili. This is despite the fact that the number of fine-tuning steps decreased due to the fixed total step budget. In other words, for the 3 low-resource tasks, longer pre-training improves performance more than exposure to the tokens. On the other hand, performance on English and Hindi worsens with increased pre-training length. For English, this is due to the resetting of the learning rate schedule and the decreasing of fine-tuning steps. Resetting involves a learning rate warmup, which worsens English performance before improving again (see the panel corresponding to En for Figure 8). Decreasing fine-tuning steps gives English less time to recover its performance from pre-training. For Hindi, the worsened performance is simply because it is not a low-resource task in this context, and therefore, less tokens seen translates to worse performance.

In Figure 9 we see that in the NMT experiments, pre-training longer on En\(\)Fr translates to better overall trade-off fronts, not just for the low-resource task.

The implications of these results are that when there is a strict training budget, it is better to be conservative and pre-train for a shorter amount of time. However, if the goal is to obtain the best performance with no strict compute budget, it is better to pre-train for as long as possible before fine-tuning. Note that longer overall training is an option for our method (by pre-training for longer) but not for static sampling because static sampling needs to constantly be training on the low-resource tasks, which will lead to overfitting when training for too long.

## 5 Related Work

Multitask LearningMultitask learning has gained increased attention in being able to learn many tasks in an efficient way due to parameter sharing and transfer between tasks. In the language domain, multilingual neural machine translation [12; 14] enables translation from multiple source languages to multiple target languages. Due to the transfer of information between language pairs, multilingual NMT has seen improvements in low-resource language-pair performance compared to training solely

Figure 9: _Left_: For language modeling on mC4, longer pre-training leads to better best-achievable performance for the 3 low-resource languages (Gu, Gd, Sw) despite the decreased length of fine-tuning. On the other hand, due to the decreased length of fine-tuning, high-resource languages do not enjoy the benefits of pre-training. _Right_: For NMT, when the training budget is not fixed, longer pre-training leads to better overall performance trade-off fronts.

on that language pair . In addition to NMT, large multilingual pre-trained language models are used to fine-tune on a variety of downstream tasks with different languages . Prior works on intermediate training take advantage of cross-task  and cross-lingual  transfer to improve downstream task performance. However, in multilingual approaches there exists the problem of dataset imbalance, where low-resource languages tend to suffer in performance. Recently,  found that naive temperature sampling might lead to overfitting of low-count languages, and suggested epoch capping with a uniform distribution for high-count languages, showing improvements over temperature sampling. In multilingual NMT, to our knowledge, we are the first to show that a simple pre-training stage on a high-resource language pair can improve the trade-off front of static sampling. Furthermore, our method is orthogonal to innovations in sampling strategies like , and can potentially show better results in conjunction with better sampling.

Transfer Learning in NMTThe benefits of transfer learning to low-resource language-pairs has been long known in the NMT literature .  showed that pre-training on a high-resource language pair can improve performance compared to training from scratch. While most prior work on transfer learning in NMT focus on improving performance on low-resource bilingual data, recent work  used transfer learning to improve performance on multiple language pairs. Unlike the transfer learning literature in NMT , we show that pre-training can push the low-resource frontier in the multilingual setting, by testing a grid of sampling rates and hyperparameters to trace the trade-off front. Prior work in the literature study the relationship between the pre-training and fine-tuning language pairs , freezing different parts of the model during fine-tuning , and experimenting with many-stage pre-training . We expect to further benefit from research done in this direction.

Curriculum LearningDue to the imbalanced nature of multilingual datasets, a static sampling strategy is unsatisfactory.  used a hand-crafted temperature sampling schedule that samples more high-resource earlier in the training, and gradually samples more low-resource languages. The performance boost from using such a schedule, compared to a static one, supports our observations from pre-training using a high-resource language pair. On the other hand, there are many works that employ a more intricate strategy for an adaptive schedule . In comparison, our method is simple with little to no overhead. We include discussion on our experience, though preliminary, with trying an adaptive schedule in Appendix C. Lastly,  showed that the ordering of data within a task affects catastrophic forgetting, which supports our observations.

## 6 Limitations and Future work

In our experiments, we focus on training on a single high-resource task during the pre-training phase. It would be interesting future work to study pre-training with more than one language or language-pair. We also only experiment with fine-tuning all parameters of the pre-trained model. Studying the effect of freezing different parts of the model during fine-tuning, potentially as a function of the relationship between pre-training and fine-tuning tasks, is left to future work.

## 7 Conclusion

In this work, we demonstrated the benefits of a pre-train joint fine-tune setup for multi-objective optimization when there is a mixture of high and low-resource tasks. We show that in the presence of large data imbalance, the order at which tasks are introduced has significant impact on overall performance. We demonstrate through a variety of experimental settings that this methodology produces points that can go past the trade-off frontier achieved by scalarization. We show that a major weak point of scalarization in this regime is that it overfits on the low-resource task, being unable to early stop due to the high-resource task not converging. Our method both allows the high-resource task to converge during pre-training and prevents overfitting through joint fine-tuning. It also outperforms scalarization that under-samples the low-resource task due to higher token efficiency. We also show that fine-tuning only on the low-resource task, a popular scheme in the NMT literature, is undesirable due to its inability to prevent forgetting. Our method is a simple natural strategy for avoiding the above failure modes. Given the significant performance boost we observe in our experiments, we believe that this training regime has the potential to become a standard approach, particularly in the era of large language models.