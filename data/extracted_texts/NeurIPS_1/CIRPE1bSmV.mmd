# Mitigating Object Hallucination via

Concentric Causal Attention

 Yun Xing\({}^{1}\)1 Yiheng Li\({}^{1}\)1 Ivan Laptev\({}^{2}\) Shijian Lu\({}^{1}\)2

\({}^{1}\) Nanyang Technological University \({}^{2}\) MBZUAI

[https://github.com/xing0047/cca-llava.git](https://github.com/xing0047/cca-llava.git)

Equal contributionCorresponding author

###### Abstract

Recent Large Vision Language Models (LVLMs) present remarkable zero-shot conversational and reasoning capabilities given multimodal queries. Nevertheless, they suffer from object hallucination, a phenomenon where LVLMs are prone to generate textual responses not factually aligned with image inputs. Our pilot study reveals that object hallucination is closely tied with Rotary Position Encoding (RoPE), a widely adopted positional dependency modeling design in existing LVLMs. Due to the long-term decay in RoPE, LVLMs tend to hallucinate more when relevant visual cues are distant from instruction tokens in the multimodal input sequence. Additionally, we observe a similar effect when reversing the sequential order of visual tokens during multimodal alignment. Our tests indicate that long-term decay in RoPE poses challenges to LVLMs while capturing visual-instruction interactions across long distances. We propose Concentric Causal Attention (CCA), a simple yet effective positional alignment strategy that mitigates the impact of RoPE long-term decay in LVLMs by naturally reducing relative distance between visual and instruction tokens. With CCA, visual tokens can better interact with instruction tokens, thereby enhancing model's perception capability and alleviating object hallucination. Without bells and whistles, our positional alignment method surpasses existing hallucination mitigation strategies by large margins on multiple object hallucination benchmarks.

## 1 Introduction

Large Vision-Language Models (LVLMs)  have drawn increasing attention from the AI research community due to their impressive power in understanding the visual world and unprecedented ability to interact with humans via conversations. Their capability to process multimodal sequences has opened up new possibilities for a wide range of vision and language tasks , such as handling interleaved image-text inputs  and interactive user queries . However, existing LVLMs still suffer from object hallucination , a tendency to generate inaccurate responses that are not factually aligned with image inputs. Such phenomenon challenges the faithfulness and reliability of LVLMs in practical use, impeding their deployments to real-world applications .

A wide range of approaches have been proposed to mitigate object hallucination in LVLMs. One straightforward approach involves post-hoc correction using revisor models , reducing occurrences of hallucinated responses. Another viable approach is to improve supervised fine-tuning by diversifying instruction tuning data  or additionally aligning model responses with human preference . Despite their effectiveness in mitigating LVLM object hallucination, acquiring high-quality annotations can be labor-intensive, making these approaches costly to implement.

Recently, several studies explore training-free mitigation of object hallucination by rectifying fallacies in LVLM autoregressive decoding [26; 34]. However, the need to compare among many candidates inevitably slows down the decoding process, making these approaches less efficient during inference.

Distinct from previous efforts, we attend to Rotary Position Encoding (RoPE) , a widely used positional dependency modeling design in LVLMs [46; 84], and investigate how it may affect object hallucination in LVLMs. Similar to sinusoidal function , RoPE is proposed to encode position information into representations, enhancing model's ability in understanding sequential order of input tokens. In spite of its success in modeling natural language [53; 63; 64], this design leads to long-term decay  in multimodal alignment, a phenomenon where information flow from visual tokens to instruction tokens1 gradually diminishes with increasing relative distance.

We analyze the impact of long-term decay [61; 53] on LVLMs. For every visual token in a multimodal sequence, we aggregate its information flow to all instruction tokens and examine how these aggregations distribute across all visual tokens. As presented, in contrast to information flows of visual tokens without RoPE (Fig. 1, (b)), applying RoPE attenuates information flows of leftmost visual tokens, which are located the farthest from instruction tokens in the sequence (Fig. 1, (c)). Such long-term decay benefits natural language modeling , but induces insufficient interactions between visual tokens and instruction tokens, leading to inferior multimodal alignment and object hallucinations in the trained LVLMs (see our experiments in Sec. 3 for details).

To this end, we propose Concentric Causal Attention (CCA), a novel position alignment method for training LVLMs with mitigated object hallucination. CCA consists of a position reorganization module for visual tokens and an accompanying causal mask rectification module for modeling 2-D continuous positional dependency. Instead of following raster-scan 2 sequential order of existing LVLMs, CCA starts from peripheral of 2-D images and ends in centers. Such position alignment strategy enjoys two merits: 1) relative distance from instruction tokens to visual tokens are significantly reduced, alleviating limitations brought by long-term decay in RoPE; 2) rectified causal attentions follow 2-D spatial locality of images, as compared to 1-D causal attention originally designed for natural languages. We carry out pre-training and instruction tuning as  and verify our trained model on multiple object hallucination benchmarks [41; 57; 20] (+4.24% on Accuracy and +2.73% on F1 score, as compared to the state-of-the-art method  on POPE). From a broader perspec

Figure 1: **Long-term decay of RoPE  in Large Vision Language Models (LVLMs). (a) a schematic view of inference in LVLMs, typically involving a pre-trained vision encoder, a large language model and a projector to map visual tokens to textual space. For each of \(V\) visual tokens \(_{vision}\), we aggregate its information flow to instruction tokens \(_{instruct}\) and reshape the aggregation results to 2-D (\(\) by \(\)). Applying RoPE on visual tokens introduces long-term decay as illustrated in (c), referring to the phenomenon where information flowing from visual tokens to instruction tokens gradually decays from lower-right region (rightmost visual tokens in the 1-D sequence) to upper-left region (leftmost visual tokens). For instruction tokens, they have much less direct interaction with leftmost visual tokens as compared with rightmost visual tokens, leading to inferior multimodal alignment in the trained LVLMs. (b) and (c) are derived from the adversarial subset of the \(3k\) POPE  image-instruction pairs. Best viewed in color.**

tive, our method also improves general perception capability of LVLMs. Preliminary experiments show that our positional alignment approach surpasses the baseline consistently over 6 multimodal benchmarks [36; 48; 22; 28; 49; 8].

Our contributions are three-fold. First, we perform in-depth analysis on correlation between rotary position encoding and object hallucination in large vision-language models. Second, motivated by our analysis, we propose Concentric Causal Attention (CCA), a simple yet effective method to mitigate LVLM object hallucination caused by RoPE long-term decay. Third, experiments on multiple benchmarks and comparisons with the state-of-the-art methods support efficacy of our design.

## 2 Related Works

**Large Vision Language Models**. Language modeling has made notable progress in recent years, evolving from robust representation models [17; 56; 55] to instruction-tuned conversational chatbots [63; 64; 12; 1]. These achievements have driven research in creating Large Vision Language Models (LVLMs) that can manage multimodal inputs [72; 46; 45; 84; 71; 6; 67; 40; 51; 39]. Pioneering studies in this field [2; 4; 38; 37] connect a vision-only encoder with a powerful frozen language-only model to bridge modality gap, enabling dense interactions across visual and textual features. Powered by instruction-tuned LLMs , LLaVA , InstructBLIP  and MiniGPT4  allow interactive conversations between trained models and users. On top of these studies, LVLMs are empowered with more advanced capabilities, such as engaging in referential dialogues [7; 74; 81; 54; 77], handling interleaved image-text data [2; 4; 35] or understanding visual prompts, like point or box inputs from users [54; 82; 9; 77]. Despite advancements in LVLMs, many of these models still generate inaccurate responses not aligned with visual inputs.

**Object Hallucination** refers to a common problem of existing LVLMs [14; 44; 21; 41; 68; 52; 3; 19; 66]. Specifically, LVLMs tend to generate inaccurate responses that are not factually aligned with image inputs. To address this issue, several recent explorations [73; 83; 33] resort to post-hoc correction of model hallucinated outputs. These methods rely on either external models  to correct hallucinated responses  or on self-correction techniques [33; 70]. However, both of these methods break end-to-end inference scheme. In contrast, [43; 76; 62; 29; 78; 75] ground their approaches on improving instruction tuning, by either diversifying instruction data or aligning model responses with human feedback. However, acquisition of more instruction data or preference data is labor-intensive. Recently, several studies attempt to mitigate object hallucination in a training-free manner [26; 34; 10]. However, the need to compare among many candidates inevitably slows down the decoding process, making these approaches less efficient during inference. From a distinct perspective, we ground our design in correlation between widely adopted rotary position encoding and object hallucination.

**Position Encoding in Transformers**. Transformer models  do not inherently comprehend sequential information of input tokens, which is inferior for modeling sequential data like natural language as compared to recurrent structures like . To mitigate this issue,  introduces sinusoidal position encodings to incorporate position information to input embeddings. In addition, several studies resort to learnable position encodings , which allow their models to update positional parameters during training. In contrast to absolute position encodings, relative position encodings [59; 31; 23; 27] focus on relative position among tokens. They integrate position information in self-attentions, presenting potential for modeling sequences with variable lengths [61; 53]. Among these studies, Rotary Position Encoding (RoPE)  encodes position information by multiplying input embeddings with rotation matrices. In comparison to other position encoding designs, RoPE is capable of equipping linear self-attention with relative position encoding, which is proven effective for pre-training large language models [63; 64]. A few recent studies explores RoPE for vision tasks [13; 50; 69], showcasing its potential to domains beyond natural language. In this paper, we investigate the role of RoPE in LVLMs and how it affects object hallucination in these models.

## 3 Motivation

In this section, we further examine the long-term decay in RoPE and conduct quantitative analyses to illustrate its correlation with object hallucination. We begin with a brief introduction to the widely adopted LVLM architecture and how RoPE  is applied in LVLMs. Then, we highlight the long-term decay in RoPE [61; 53], which benefits language modeling but is under-explored for multimodal alignment. Finally, we examine the role of RoPE in LVLM object hallucination through comparative experiments, which forms a strong foundation of our design.

**LVLM**. Typically, an LVLM \(\) is composed of a pretrained vision encoder \(_{v}\), a large language model \(_{t}\) and a projector module \(f\) that maps visual embeddings to textual space. Given an image input \(I_{v}\) and instruction input \(I_{t}\) (e.g., _"please describe this image in detail"_), \(\) encodes these two inputs into a multimodal sequence \(=\{_{vision},~{}_{instruct}\}\), where \(_{vision}=f(_{v}(I_{v}))=\{w_{m}\}_{m=1}^{V}\) and \(_{instruct}=_{t}(I_{t})=\{w_{m}\}_{m=1}^{T}\) represent visual and instruction tokens of lengths \(V\) and \(T\), respectively. In such sequence, visual and instruction tokens share the same dimension \(d\), noted as \(w_{m}^{d}\).

**Rotary Position Encoding in LVLM**. In LLMs like LLaMA  and its multimodal successors, RoPE  encodes position information with input tokens by multiplying every token \(w_{m}\) with a rotation matrix \(R_{,m}^{d}\),

\[R_{,m}^{d}= m_{1}&- m_{1}&0&0& &0&0\\  m_{1}& m_{1}&0&0&&0&0\\ 0&0& m_{2}&- m_{2}&&0&0\\ 0&0& m_{2}& m_{2}&&0&0\\ &&&&&&\\ 0&0&0&0&& m_{d/2}&- m_{d/2}\\ 0&0&0&0&& m_{d/2}& m_{d/2} \]

where \(m[1,...,V+T]\) indicates position of input token \(w_{m}\) and \(\{_{i}=10000^{-2(i-1)/d}\},i[1,2,...,d/2]\) are pre-defined sinusoidal function values following . In LVLMs like LLaVA , rotary matrices \(R_{,m}^{d}\) are applied to query and key tokens in all decoder layers, such that relative position dependency among tokens are modeled and integrated in self-attentions across the network. In comparison to absolute position encodings  and learnable position encodings in ViT , RoPE captures relative distance among input tokens and has the potential to extend the input context window beyond a fixed length .

**RoPE Long-term Decay**. Assume a query token \(q_{i}\) at position \(i\) and a key token \(k_{j}\) at position \(j\), which are derived from input tokens \(w_{i}\), \(w_{j}\). The self attention \(a_{i,j}\) between tokens \(q_{i}\) and \(k_{j}\) can be calculated via

\[_{i,j}=(^{T} k_{j}}{}) \]

RoPE applies rotation matrix \(R_{,m}^{d}\) to the self-attention above, which is in the form of,

\[_{i,j}=(^{T}(R_{,i}^{d})^{T}  R_{,j}^{d} k_{j}}{})=(^{T}  R_{,j-i}^{d} k_{j}}{}) \]

where \(j-i\) stands for relative position between \(q_{i}\) and \(k_{j}\). The long-term decay refers to the decrease of \(_{i,j}\) as the relative distance \(j-i\) increases. As presented in Fig. 1 (c), visual-to-instruction information flow (i.e., instruction-to-visual self-attention) is less significant when \(j-i\) is large and vice versa.

This is favorable for pre-trained LLMs like LLaMA , as it aligns with language modeling intuition: pairs of tokens with a long relative distance should have weaker connection. However, we observe that this property brings negative effect in multimodal alignment, in which case visual tokens far from instructions are less attended. This is not expected for multimodal alignment, as the connection between instruction tokens and visual tokens should not be attenuated by their relative distances.

**Pilot Experiment**. We quantitatively examine the effect of RoPE long-term decay on LVLM object hallucination. To determine how object hallucination is influenced by the distance between visual and instruction tokens, we first train two LVLMs 3 following  with two different position alignment strategies, including:

* \(^{b}\) (_raster-scan_): it follows  the position alignment strategy on visual tokens \(_{vision}\). Under this scenario, visual tokens follow a sequential order, starting from upper-left corner to lower-right corner of input \(2\)-D visual features, row by row. The order of a multimodal sequence \(\) is in format of \(\{1,2,...,V,V+1,...,V+T\}\).4 * \(^{r}\) (_reverse raster-scan_): it reverses the sequential order of visual tokens \(_{vision}\). In this case, sequence order of visual tokens starts from lower-right corner of input \(2\)-D visual features to upper-left corner, row by row. The order of full multimodal sequence \(\) is in format of \(\{V,V-1,...,1,V+1,...,V+T\}\).

The _reverse raster-scan_ model \(^{r}\) alters relative positions between visual tokens \(_{vision}\) and instruction tokens \(_{instruct}\). For example, for instruction token \(w_{V+1}\), its relative distance to visual token \(w_{V}\) changes from \(1\) to \(V\), resulting in weaker correlations between \(w_{V}\) and \(w_{V+1}\).

Our experiment setup is as follows. Given an image \(I_{v}\), we follow  and ask questions in a polling-base manner. Specifically, for an object \(O_{v}\) in image \(I_{v}\), we follow the instruction format of "is there a/an {object} in this image?" to test our models. We crop region of object \(O_{v}\) from \(I_{v}\) according to its bounding box annotation and paste the cropped object over different positions of a pre-defined image template (more details are covered in Appendix C.1). This results in new images \(\{I_{v_{1}},...,I_{v_{k}}\}\), where \(\{v_{1},...,v_{k}\}\) indicates different pasting positions. We carry out these testing over \(N\) images from  and aggregate correct responses with respect to pasting positions \(\{v_{1},...,v_{k}\}\).

**RoPE affects object hallucination**. The quantitative results of model \(^{b}\) and \(^{r}\) are visualized in Fig. 2 (a) and (b), respectively. For model \(^{b}\), we find that the response is less likely correct when object \(O_{v}\) is pasted on the upper part of the image, and it is more likely correct when object \(O_{v}\) is pasted on the lower part of image template. This is in stark contrast to \(^{r}\) experimental results: model responses are more likely to be correct when pasting image crop \(O_{v}\) on the upper part of images, while less likely to be correct when pasting position is the lower part. For model \(^{r}\), we note

Figure 2: **Motivation Experiment.** Given an image \(I_{v}\) with object \(O_{v}\), we crop \(O_{v}\) and paste it to various spatial positions \(\{v_{1},...,v_{k}\}\) within a pre-defined template. For every pasting position, we ask two LVLMs (\(_{b}\) and \(_{r}\)) if object \(O_{v}\) is in this template, where \(_{b}\) refers to a baseline model that follows raster-scan positional alignment strategy and \(_{r}\) refers to a model that resorts to reversal raster-scan position alignment strategy. The total number of correct responses at different pasting positions \(\{v_{1},...,v_{k}\}\) is reported in (a) and (b), which refers to results from model \(_{b}\) and \(_{r}\), respectively. We observe that LVLM \(_{b}\) are more likely to generate correct responses when pasting object \(O_{v}\) to lower region, while \(_{r}\) are less hallucinated when pasting object \(O_{v}\) to upper region. Pasting positions with the most and the least correct responses are highlighted in solid-line and dotted-line red boxes. More details are provided in Appendix C.1. Best viewed in color.

[MISSING_PAGE_FAIL:6]

of length \(V\) and a instruction token sequence of length \(T\), the maximum distance between visual tokens \(_{vision}\) and instruction tokens \(_{instruct}\) is \((}{2}+T-1)\). This concentric sequential ordering also better maintains 2-D spatial locality of visual tokens. Under this scenario, visual tokens that are closer in euclidean distances are causally correlated when position \(m\) increases. Meanwhile, visual tokens that share the same position are correlated in visual self-attention. We note that such design mitigates negative effect from RoPE long-term decay, via decreasing relative distances between \(_{vision}\) and \(_{instruct}\) while keeping causal inference scheme in pre-trained LLMs like LLaMA .

**Concentric Causal Masking**. Another part of our method resorts to modification of default causal attention masking towards our concentric visual token reorganization. As presented in Fig. 3 (c), a query feature \(q_{m}\) (derived from \(w_{m}\)) only attends to preceding key features \(k_{<=m}\). Likewise for our method, we follow the same principle to force causal attention masking in 2-D visual inputs. We visualize our masking in Fig. 3 (d), where the total length of visual tokens are 36 (6 by 6). Combining visual token re-organization with concentric causal masking, our method models 2-D continuity for visual inputs and effectively mitigates the object hallucination issue brought by long-term decay in RoPE.

## 5 Experiments

We first describe training details for our position alignment approach and evaluation setups in Sec. 5.1. Subsequently, we report results for several popular benchmarks that demonstrates efficacy of our simple design in the remaining subsections. Further, we present qualitative comparison in Appendix D.2 where our approach generates less hallucinated responses. From a broader scope, we present that our positional alignment strategy benefits general perception capability of LVLMs, where preliminary experiments show that it surpasses the baseline consistently over six multimodal benchmarks . We refer to these results in Appendix D.1 due to page limits. By default, we conduct our training and evaluation with Vicuna-7B  model, unless otherwise stated.

### Training Details

Following , we adopt pre-trained CLIP ViT-L/14  with 336x336 resolutions as visual encoder and Vicuna-7B  as LLM, and a 2-layer MLP that connects the visual encoder and LLM. Training consists of two stages, including 1) a pre-training over CC-558K dataset  with global batch size of 256 and 2) a instruction tuning with a 665k multi-turn conversation dataset  with global batch size of 128.

### Pope

Polling-based Object Probing Evaluation (POPE)  is proposed to provide a detailed evaluation of object hallucination in LVLMs, by querying the models about presence of specific objects in given images with yes-or-no questions. POPE adopts three sampling options to sample negative objects: random, popular and adversarial. We refer to  for these setups. Following , three datasets are included in our evaluation, including COCO , GQA  and A-OKVQA . For each evaluation setup, every subset includes 3,000 questions for 500 images, which leads to 27,000 yes-or-no questions in total.

The experimental results are presented in Tab. 1. Our method achieves the highest accuracy and F1 scores across all datasets and negative sampling setups. By re-organization of visual tokens and concentric masking, our approach achieves 5.48%, 7.86% and 6.70% accuracy improvement and 5.89%, 7.71% and 6.19% F1 score improvement over the baseline model . We also observe consistent and notable performance gains against state-of-the-art hallucination mitigation methods. CCA surpasses VCD  by 1.02%, 4.51% and 2.65% on three datasets. Particularly, we observe 3.09%, 5.01% and 3.59% F1 score improvement over adversarial evaluation set, which selects the most frequent co-occuring objects with ground-truth objects in image inputs, posing challenges for LVLMs to discern spurious correlation. Our trained model is also comparable to LLaVA-RLHF model (with Vicuna-\(13\)B as its LLM)  that additionally aligns model responses with human preference. These results indicate importance of re-organizing visual tokens in vision-language alignment.

### Chair

We further evaluate our method on Caption Hallucination Assessment with Image Relevance (CHAIR) metric. CHAIR was a pioneering study introduced to measure object hallucination in image captioning . It quantifies the factuality of a model by calculating the proportion of objects not present in ground truth over all objects in caption output. It contains both instance level score CHAIR\({}_{I}\) (shorted for \(C_{I}\)) and sentence level score CHAIR\({}_{S}\) (\(C_{S}\)) which holistically assess a model's performance. Specifically, CHAIR metric is formulated as:

\[C_{S}=\}|}{|\{\}|},\ C_{I}=\}|}{|\{\}|}\]

where lower scores corresponds to better performance. Following previous studies , we prompt LVLMs with _"Please describe this image in detail."_. Note that LVLM's performance on CHAIR metric is highly dependent on their output sentence length. Short and succinct responses have less chances to make mistakes and thus would generally have better CHAIR scores. Different textual prompts such as _"in detail"_ and _"in brief"_ also influences output length and creates bias in CHAIR evaluation . To offset the influence of output length and prompt phrasing and ensure fair basis of comparison, we follow the experimental setup in OPERA  and set the maximum text token to 64 and 512 respectively to examine hallucination on both short and long responses. Following , we sample 500 images from COCO VAL 2014  to generate descriptions from different models and hallucination mitigation methods.

Our image caption evaluation result on CHAIR is shown in Tab. 2. For greedy decoding, our model surpasses baseline model  by 3.2% while maintaining high object recall (80.3% v.s. 80.4%) for long-response generation (by setting max new tokens to \(512\)). Note that longer textual responses suggests more significant distance between visual and instruction tokens, leading to higher hallucination rates , which can be improved by our approach that reduces relative distance between visual and textual tokens. Our results are comparable against LLaVA-RLHF  over this setup. On short responses, our model also outperforms baseline model by 2.8% on sentence-level and 0.8% on instance-level while maintaining high object recall.

Our approach is also effective when using beam search for autoregressive decoding. We surpass the baseline by 0.8% and 0.5% on long-response generation, and 2.2% and 0.5% on short-response generation for \(C_{S}\) and \(C_{I}\), respectively. Our approach is also complementary to OPERA . In comparison to baseline model that using OPERA decoding, our approach are 1.8% and 1.1% better for \(C_{S}\) and \(C_{I}\) on long-response setting. We observe consistent performance gains in short-response generation (1.6% for \(C_{S}\) and 0.9% for \(C_{I}\)). Quantitative evaluations on open-ended generation indicates importance of a better positional alignment strategy and efficacy of our design.

    &  &  &  &  &  \\   & & _acc_ & _f1_ & _acc_ & _f1_ & _acc_ & _f1_ & _acc_ & _f1_ \\   & baseline & 83.29 & 81.33 & 81.88 & 80.06 & 78.96 & 77.57 & 81.38 & 79.65 \\  & VCD  & 87.73 & **87.16** & 85.38 & 85.06 & 80.88 & 81.33 & 84.66 & 84.52 \\  & LLaVA-RLHF  & 85.90 & 83.92 & 83.90 & 82.05 & 82.60 & 80.88 & 84.13 & 82.28 \\  & CCA-LLaVA & **88.03** & 86.65 & **86.87** & **85.54** & **85.67** & **84.42** & **86.86** & **85.54** \\   & baseline & 83.45 & 82.56 & 79.90 & 79.59 & 74.04 & 75.15 & 79.13 & 79.10 \\  & VCD  & 86.15 & 86.34 & 81.85 & 82.82 & 74.97 & 77.73 & 80.99 & 82.30 \\  & LaVA-RLHF  & 87.67 & 86.60 & 85.20 & 84.34 & 79.97 & 79.92 & 84.28 & 83.62 \\  & CCA-LLaVA & **90.27** & **89.71** & **88.40** & **87.98** & **82.30** & **82.74** & **86.99** & **86.81** \\   & baseline & 83.73 & 82.95 & 78.17 & 78.37 & 75.08 & 76.06 & 78.99 & 79.13 \\  & VCD  & 86.65 & 86.99 & 80.73 & 82.24 & 76.09 & 78.78 & 81.16 & 82.67 \\   & LaVA-RLHF  & 84.93 & 83.38 & 81.37 & 80.23 & 78.30 & 77.70 & 81.53 & 80.44 \\   & CCA-LLaVA & **88.40** & **87.68** & **86.47** & **85.91** & **82.20** & **82.37** & **85.69** & **85.32** \\   

Table 1: **POPE Results**. acc: accuracy. f1: f1 score, measured by precision and recall. Baseline and VCD results are reported by paper .

### Mme

The MME hallucination subset extends scope beyond object hallucination. Following , we evaluation 4 perception sub-tasks that examines LVLMs on object-level and attribute-level hallucinations, including measure of object existence, count, position and color. As presented in Tab. 3, our method surpasses the baseline by 76.33 on these tasks. In comparison to previous hallucination mitigation method VCD, our approach demonstrates non-negligible performance gains over all subtasks (e.g., 2.00 improvement from VCD v.s. 24.00 improvement from our method). These results indicate the potential of CCA to improve general perception capability of LVLMs.

### GPT4V-Aided Evaluation

We also evaluate our approach on LLaVA-Bench (In-the-Wild) , composed of 24 images with 60 questions in total. LLaVA-Bench (In-the-Wild) constitutes three types of questions, including conversation, detailed description and complex reasoning. Following , we ask these models to generate responses and let the text-only GPT-4  be the judge to rate these responses. The results are presented in Tab. 4. In comparison to OPERA  that specializes in open-ended generation, our method still stands out when examined by GPT-4 according to detailness and correctness, suggesting efficacy of our positional alignment strategy on generating accurate long responses.

## 6 Conclusion and Limitations

In this paper, we aim to mitigate object hallucination in Large Vision-Language Model (LVLM). We perform in-depth analysis on correlation between object hallucination and Rotary Position Encoding, a widely used positional dependency modeling design in existing LVLMs. We find that LVLMs are more likely to hallucinate when relevant visual cues are distant from instruction tokens in 1-D multimodal sequence, due to long-term decay in RoPE. To this end, we propose Concentric Causal Attention, a simple yet effective positional alignment strategy that reduces relative distances between visual and instruction tokens, alleviating negative impact brought by RoPE long decay on object hallucination. Experimental results over multiple evaluation benchmarks supports our design, indicating importance of better position alignment strategy.

**Limitation**. While this study shows improvements on mitigating object hallucination in LVLMs, our focus is only limited to handling of image-text inputs. We consider positional alignment strategy for other modalities of input data as future works, such as audio or video inputs that differs from image-text modalities.

    &  &  &  \\   & & \(C_{}^{S}\) & \(C_{}^{I}\) & _rec\({}_{}\)_ & _len_ & \(C_{}^{S}\) & \(C_{}^{I}\) & _rec\({}_{}\)_ & _len_ \\   & baseline & 46.2 & 12.9 & 80.3 & 97.2 & 21.0 & 6.2 & 66.3 & 54.9 \\  & LLaVA-RLHF  & 43.6 & **10.5** & 78.0 & 117.9 & 19.6 & 5.4 & 64.9 & 54.0 \\  & CCA-LLaVA & **43.0** & 11.5 & **80.4** & 96.6 & **18.2** & **5.4** & **66.7** & 54.5 \\   & baseline & 49.4 & 13.9 & 79.9 & 96.1 & 18.2 & 5.8 & 64.0 & 52.7 \\  & OPERA  & 46.8 & 13.4 & 79.6 & 93.2 & 17.8 & 5.9 & 64.3 & 53.0 \\  & CCA-LLaVA & 48.6 & 13.4 & **79.9** & 94.2 & **16.0** & 5.3 & 64.8 & 52.7 \\  & CCA-LLaVA + OPERA  & **45.0** & **12.3** & 79.5 & 91.8 & 16.2 & **5.0** & **65.0** & 52.9 \\   

Table 2: **CHAIR results**. For evaluation setups, 512 and 64 refer to a hyperparameter that relates to the length of LVLM repsonses, corresponding to long-text and short-text generation, respectively.

    &  &  &  \\  & _cal inference_ & _count_ & _position_ & _color_ & \\  baseline & 175.67 & 124.67 & 114.00 & 151.00 & 565.33 \\ OPERA  & 180.67 & 133.33 & 1723.33 & 155.00 & 992.33 \\ VCD  & 184.66 & 138.33 & **128.67** & 153.00 & 604.66 \\ CCA-LLaVA & **190.00** & **148.33** & 128.33 & **175.00** &