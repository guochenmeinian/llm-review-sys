# A graphon-signal analysis of graph neural networks

Ron Levie

Faculty of Mathematics

Technion - Israel Institute of Technology

levieron@technion.ac.il

###### Abstract

We present an approach for analyzing message passing graph neural networks (MPNNs) based on an extension of graphon analysis to a so called graphon-signal analysis. A MPNN is a function that takes a graph and a signal on the graph (a graph-signal) and returns some value. Since the input space of MPNNs is non-Euclidean, i.e., graphs can be of any size and topology, properties such as generalization are less well understood for MPNNs than for Euclidean neural networks. We claim that one important missing ingredient in past work is a meaningful notion of graph-signal similarity measure, that endows the space of inputs to MPNNs with a regular structure. We present such a similarity measure, called the graphon-signal cut distance, which makes the space of all graph-signals a dense subset of a compact metric space - the graphon-signal space. Informally, two deterministic graph-signals are close in cut distance if they "look like" they were sampled from the same random graph-signal model. Hence, our cut distance is a natural notion of graph-signal similarity, which allows comparing any pair of graph-signals of any size and topology. We prove that MPNNs are Lipschitz continuous functions over the graphon-signal metric space. We then give two applications of this result: 1) a generalization bound for MPNNs, and, 2) the stability of MPNNs to subsampling of graph-signals. Our results apply to any regular enough MPNN on any distribution of graph-signals, making the analysis rather universal.

## 1 Introduction

In recent years, the need to accommodate non-regular structures in data science has brought a boom in machine learning methods on graphs. Graph deep learning (GDL) has already made a significant impact on the applied sciences and industry, with ground-breaking achievements in computational biology , and a wide adoption as a general-purpose tool in social media, e-commerce, and online marketing platforms, among others. These achievements pose exciting theoretical challenges: can the success of GDL models be grounded in solid mathematical frameworks? Since the input space of a GDL model is non-Euclidean, i.e., graphs can be of any size and any topology, less is known about GDL than standard neural networks. We claim that contemporary theories of GDL are missing an important ingredient: meaningful notions of metric on the input space, namely, graph similarity measures that are defined for _all graphs of any size_, which respect and describe in some sense the behavior of GDL models. In this paper, we aim at providing an analysis of GDL by introducing such appropriate metrics, using _graphon theory_.

A graphon is an extension of the notion of a graph, where the node set is parameterized by a probability space instead of a finite set. Graphons can be seen as limit objects of graphs, as the number of nodes increases to infinity, under an appropriate metric. One result from graphon theory (that reformulates Szemeredi's regularity lemma from discrete mathematics) states that any sufficiently large graph behaves as if it was randomly sampled from a stochastic block model with a fixed number of classes.

This result poses an "upper bound" on the complexity of graphs: while deterministic large graphs may appear to be complex and intricate, they are actually approximately regular and behave random-like.

In this paper we extend this regularity result to an appropriate setting for message passing neural networks (MPNNs), a popular GDL model. Since MPNNs take as input a graph with a signal defined over the nodes (a graph-signal), we extend graphon theory from a theory of graphs to a theory of graph-signals. We define a metric, called the _graph-signal cut distance_ (see Figure 1 for illustration), and formalize regularity statements for MPNNs of the following sort.

(1) Any deterministic graph-signal behaves as if it was randomly sampled from a stochastic block model, where the number of blocks only depends on how much we want the graph-signal to look random-like, and not on the graph-signal itself.

(2) If two graph-signals behave as if they were sampled from the same stochastic block model, then any (regular enough) MPNN attains approximately the same value on both.

Formally, (1) is proven by extending Szemeredi's weak regularity lemma to graphon-signals. As a result of this new version of the regularity lemma, we show that the space of graph-signals is a dense subset of the space of graphon-signals, which is shown to be compact. Point (2) is formalized by proving that MPNNs with Lipschitz continuous message functions are Lipschitz continuous mappings from the space of graph-signals to an output space, in the graphon-signal cut distance.

We argue that the above regularity result is a powerful property of MPNNs. To illustrate this, we use the new regularity result to prove two corollaries. First, a generalization bound of MPNNs, showing that if the learned MPNN performs well on the training graph-signals, it is guaranteed to also perform well on test graph-signals. This is shown by first bounding the covering number of the graphon-signal space, and then using the Lipschitzness of MPNNs. Second, we prove that MPNNs are stable to graph-signal subsampling. This is done by first showing that randomly subsampling a graphon-signal produces a graph-signal which is close in cut distance to the graphon-signal, and then using the Lipschitzness of MPNNs.

As opposed to past works that analyze MPNNs using graphon analysis, we do not assume any generative model on the data. Our results apply to any regular enough MPNN on any distribution of graph-signals, making the analysis rather universal. We note that past works about generalization in GNNs [14; 23; 26; 30] consider special assumptions on the data distribution, and often on the MPNN model. Our work provides upper bounds under no assumptions on the data distribution, and only mild Lipschitz continuity assumptions on the message passing functions. Hence, our theory bounds the generalization error when all special assumptions (that are often simplistic) from other papers are not met. We show that when all assumptions fail, MPNNs still have generalization and sampling guarantees, albeit much slower ones. See Table 1. This is also true for past sampling theorems, e.g., [18; 22; 27; 31; 32].

The problem with graph-signal domains.Since the input space of MPNNs is non-Euclidean, results like universal approximation theorems and generalization bounds are less well developed for MPNNs than Euclidean deep learning models. For example, analysis like in  is limited to graphs of fixed sizes, seen as adjacency matrices. The graph metric induced by the Euclidean metric on

Figure 1: Illustration of the graph-signal cut distance. Left: a stochastic block model (SBM) with a signal. The color of the block represents the value of the signal at this block. The thickness of the edges between the blocks (including self-loops) represents the probability/density of edges between the blocks. Middle: a small graph-signal which looks like was sampled from the SMB. The color of the nodes represents the signal values. Right: a large graph-signal which looks like was sampled from the SMB. In graphon-signal cut distance, these two graph-signals are close to each other.

adjacency matrices is called _edit-distance_. This reduction of the graph problem to the Euclidean case does not describe the full complexity of the problem. Indeed, the edit-distance is defined for weighted graphs, and non-isomorphic simple graphs are always far apart in this metric. This is an unnatural description of the reality of machine learning on graphs, where different large non-isomorphic simple graphs can describe the same large-scale phenomenon and have similar outputs for the same MPNN.

Other papers that consider graphs of arbitrary but bounded size are based on taking the union of the Euclidean edit-distance spaces up to a certain graph size . If one omits the assumption that all graphs are limited by a predefined size, the edit-metric becomes non-compact - a topology too fine to explain the behavior of real MPNNs. For example, two graphs with different number of nodes are always far apart in edit-distance, while most MPNN architectures in practice are not sensitive to the addition of one node to a large graph. In , the expressivity of GNNs is analyzed on spaces of graphons. It is assumed that graphons are Lipschitz continuous kernels. The metric on the graphon space is taken as the \(L_{}\) distance between graphons as functions. We claim that the Lipschitz continuity of the graphons in , the choice of the \(L_{}\) metric, and the choice of an arbitrary compact subset therein, are not justified as natural models for graphs, and are not grounded in theory. Note that graphon analysis is measure theoretic, and results like the regularity lemma are no longer true when requiring Lipschitz continuity for the graphons. Lastly, in papers like [18; 26; 27; 31], the data is assumed to be generated by one, or a few graphons, which limits the data distribution significantly. We claim that this discrepancy between theory and practice is an artifact of the inappropriate choices of the metric on the space of graphs, and the choice of a limiting generative model for graphs.

## 2 Background

For \(n\), we denote \([n]=\{1,,n\}\). We denote the Lebesgue \(p\) space over the measure space \(\) by \(^{p}()\), or, in short, \(^{p}\). We denote by \(\) the standard Lebesgue measure on \(\). A _partition_ is a sequence \(_{k}=\{P_{1},,P_{k}\}\) of disjoint measurable subsets of \(\) such that \(_{j=1}^{k}P_{j}=\). The partition is called _equipartition_ if \((P_{i})=(P_{j})\) for every \(i,j[k]\). We denote the indicator function of a set \(S\) by \(_{S}\). See A for more details. We summarize our notations in A.

### Message passing neural networks

Most graph neural networks used in practice are special cases of MPNN (see  and  of a list of methods). MPNNs process graphs with node features, by repeatedly updating the feature at each node using the information from its neighbors. The information is sent between the different nodes along the edges of the graph, and hence, this process is called _message passing_. Each node merges all messages sent from its neighbors using an _aggregation scheme_, where typical choices is to sum, average or to take the coordinate-wise maximum of the messages. In this paper we focus on normalized sum aggregation (see A.1). For more details on MPNNs we refer the reader to A.

### Szemeredi weak regularity lemma

The following is taken from [13; 25]. Let \(G=\{V,E\}\) be a simple graph with nodes \(V\) and edges \(E\). For any two subsets \(U,S V\), denote the number of edges with one end point at \(U\) and the other at \(S\) by \(e_{G}(U,S)\). Let \(=\{V_{1},,V_{k}\}\) be a partition of \(V\). The partition is called _equipartition_ if \(||V_{i}|-|V_{j}|| 1\) for every \(i,j[k]\). Given two node set \(U,S V\), if the edges between each pair of classes \(V_{i}\) and \(V_{j}\) were random, we would expect the number of edges of \(G\) connecting \(U\) and \(S\) to be close to the expected value \(e_{(U,S)}:=_{i=1}^{k}_{j=1}^{k}(V_{i},V_{j})}{|V _{i}||V_{j}|}|V_{i} U||V_{j} S|\). Hence, the _irregularity_, that measures how non-random like the edges between \(\{V_{j}\}_{j=1}^{k}\) are, is defined to be

\[_{G}()=_{U,S V}|e_{G}(U,S)-e_{ }(U,S)|/|V|^{2}. \]

**Theorem 2.1** (Weak Regularity Lemma ).: _For every \(>0\) and every graph \(G=(V,E)\), there is an equipartition \(=\{V_{1},,V_{k}\}\) of \(V\) into \(k 2^{c/^{2}}\) classes such that \(_{G}()\). Here, \(c\) is a universal constant that does not depend on \(G\) and \(\)._Theorem 2.1 asserts that we can represent any large graph \(G\) by a smaller, coarse-grained version of it: the weighted graph \(G^{e}\) with node set \(V^{}=\{V_{1},,V_{k}\}\), where the edge weight between the nodes \(V_{i}\) and \(V_{j}\) is \((V_{i},V_{j})}{|V_{1}||V_{j}|}\). The "large-scale" structure of \(G\) is given by \(G^{}\), and the number of edges between any two subsets of nodes \(U_{i} V_{i}\) and \(U_{j} V_{j}\) is close to the "expected value" \(e_{(U_{i},U_{j})}\). Hence, the deterministic graph \(G\) "behaves" as if it was randomly sampled from \(G^{}\).

### Graphon analysis

A graphon  can be seen as a weighted graph with a "continuous" node set, or more accurately, the nodes are parameterized by an atomless standard probability space called the _graphon domain_. Since all such graphon domains are equivalent to \(\) with the standard Lebesgue measure (up to a measure preserving bijection), we take \(\) as the node set. The space of graphons \(_{0}\) is defined to be the set of all measurable symmetric function \(W:^{2}\), \(W(x,y)=W(y,x)\). The edge weight \(W(x,y)\) of a graphon \(W_{0}\) can be seen as the probability of having an edge between the nodes \(x\) and \(y\).

Graphs can be seen as special graphons. Let \(_{m}=\{I_{1},,I_{m}\}\) be an _interval equipartition_: a partition of \(\) into intervals of equal length. The graph \(G=\{V,E\}\) with adjacency matrix \(A=\{a_{i,j}\}_{i,j=1}^{m}\)_induces_ the graph \(W_{G}\), defined by \(W_{G}(x,y)=a_{ xm, ym}\)1. Note that \(W_{G}\) is piecewise constant on the partition \(_{m}\). We hence identify graphs with their induced graphons. A graphon can also be seen as a generative model of graphs. Given a graphon \(W\), a corresponding random graph is generated by sampling i.i.d. nodes \(\{X_{n}\}\) from he graphon domain, and connecting each pair \(X_{n},X_{m}\) in probability \(W(X_{n},X_{m})\) to obtain the edges of the graph.

### Regularity lemma for graphons

A simple way to formulate the regularity lemma in the graphon language is via stochastic block models (SBM). A SBM is a piecewise constant graphon, defined on a partition of the graphon domain \(\). The _number of classes_ of the SBM is defined to be the number of sets in the partition. A SBM is seen as a generative model for graphs, where graphs are randomly sampled from the graphon underlying the SBM, as explained above. Szemeredi weak regularity lemma asserts that for any error tolerance \(\), there is a number of classes \(k\), such that any deterministic graph (of any size and topology) behaves as if it was randomly sampled from a SBM with \(k\) classes, up to error \(\). Hence, in some sense, every graph is approximately _quasi-random_.

To write the weak regularity lemma in the graphon language, the notion of irregularity (1) is extended to graphons. For any measurable \(W:^{2}\) the _cut norm_ is defined to be

\[\|W\|_{}=_{U,S}|_{U S}W(x,y)dxdy|,\]

where \(U,S\) are measurable. It can be verified that the irregularity (1) is equal to the cut norm of a difference between graphons induced by adequate graphs. The _cut metric_ between two graphons \(W,V_{0}\) is defined to be \(d_{}(W,V)=\|W-V\|_{}\). The _cut distance_ is defined to be

\[_{}(W,V)=_{ S_{}}\|W-V^{}\|_{},\]

where \(S_{}\) is the space of measure preserving bijections \(\), and \(V^{}(x,y)=V((x),(y))\) (see Section 3.1 and Appendix A.3 for more details). The cut distance is a pseudo metric on the space of graphons. By considering equivalence classes of graphons with zero cut distance, we can construct a metric space \(_{0}}\) for which \(_{}\) is a metric. The following version of the weak regularity lemma is from [25, Lemma 7].

**Theorem 2.2**.: _For every graphon \(W_{0}\) and \(>0\) there exists a step graphon \(W^{}_{0}\) with respect to a partition of at most \( 2^{c/^{2}}\) sets such that \(_{}(W,W^{})\), for some universal constant \(c\)._

The exact definition of a step graphon is given in Definition 3.3. It is possible to show, using Theorem 2.2, that \(_{0}}\) is a compact metric space [25, Lemma 8]. Instead of recalling this construction here, we refer to Section 3.4 for the extension of this construction to graphon-signals.

## 3 Graphon-signal analysis

A graph-signal \((G,)\) is a graph \(G\), that may be weighted or simple, with node set \([n]\), and a signal \(^{n k}\) that assigns the value \(f_{j}^{k}\) for every node \(j[n]\). A graphon-signal will be defined in Section 3.1 similarly to a graph-signal, but over the node set \(\). In this section, we show how to extend classical results in graphon theory to a so called graphon-signal theory. All proofs are given in the appendix.

### The graphon signal space

For any \(r>0\), define the _signal space_

\[_{r}^{}:=f^{}\;\;  x,\;\;|f(x)| r}. \]

We define the following "norm" on \(_{r}^{}\) (which is not a vector space).

**Definition 3.1** (Cut norm of a signal).: _For a signal \(f:\), the cut norm\(\|f\|_{}\) is defined as_

\[\|f\|_{}:=_{S}_{S}f(x)d(x), \]

_where the supremum is taken over the measurable subsets \(S\)._

In Appendix A.2 we prove basic properties of signal cut norm. One important property is the equivalence of the signal cut norm to the \(L_{1}\) norm

\[ f_{r}^{},\;\;\;\|f\|_{}\|f\|_{1}  2\|f\|_{}.\]

Given a bound \(r\) on the signals, we define the space of _graphon-signals_ to be the set of pairs \(_{r}:=_{0}_{r}^{}\). We define the _graphon-signal cut norm_, for measurable \(W,V:^{2}\) and \(f,g:\), by

\[\|(W,f)\|_{}=\|W\|_{}+\|f\|_{}.\]

We define the _graphon-signal cut metric_ by \(d_{}(W,f),(V,g)=\|(W,f)-(V,g)\|_{}\).

We next define a pseudo metric that makes the space of graphon-signals a compact space. Let \(S^{}_{}\) be the set of measurable measure preserving bijections between co-null sets of \(\), namely,

\[S^{}_{}=:A B\;\;A,B\;\;, \;\;\;\; S A,\;(S)=((S))},\]

where \(\) is a measurable bijection and \(A,B,S\) are measurable. For \( S^{}_{}\), we define \(W^{}(x,y):=W((x),(y))\), and \(f^{}(z)=f((z))\). Note that \(W^{}\) and \(f^{}\) are only defined up to a null-set, and we arbitrarily set \(W,W^{},f\) and \(f^{}\) to \(0\) in their respective null-sets, which does not affect our analysis. Define the _cut distance_ between two graphon-signals \((W,f),(V,g)_{r}\) by

\[_{}(W,f),(V,g)=_{ S^{}_{}}d_ {}(W,f),(V,g)^{}. \]

Here, \((V,g)^{}:=(V^{},g^{})\). More details on this construction are given in Appendix A.3.

The graphon-signal cut distance \(_{}\) is a pseudo-metric, and can be made into a metric by introducing the equivalence relation: \((W,f)(V,g)\) if \(_{}((W,f),(V,g))=0\). The quotient space \(_{r}}:=_{r}/\) of equivalence classes \([(W,f)]\) of graphon-signals \((W,f)\) is a metric space with the metric \(_{}([(W,f)],[(V,g)])=_{}((W,f),(V,g))\). By abuse of terminology, we call elements of \(_{r}}\) also graphon-signals. A graphon-signal in \(_{r}}\) is defined irrespective of a specific "indexing" of the nodes in \(\).

### Induced graphon-signals

Any graph-signal can be identified with a corresponding graphon-signal as follows.

**Definition 3.2**.: _Let \((G,)\) be a graph-signal with node set \([n]\) and adjacency matrix \(A=\{a_{i,j}\}_{i,j[n]}\). Let \(\{I_{k}\}_{k=1}^{n}\) with \(I_{k}=[(k-1)/n,k/n)\) be the equipartition of \(\) into \(n\) intervals. The graphon-signal \((W,f)_{(G,)}=(W_{G},f_{})\) induced by \((G,)\) is defined by_

\[W_{G}(x,y)=_{i,j=1}^{n}a_{ij}_{I_{i}}(x)_{I_{j}}(y),  f_{}(z)=_{i}^{n}f_{i}_{I_{i}}(z).\]We denote \((W,f)_{(G,)}=(W_{G},f_{})\). We identify any graph-signal with its induced graphon-signal. This way, we define the cut distance between a graph-signal and a graphon-signal. As before, the cut distance between a graph-signal \((G,)\) and a graphon-signal \((W,g)\) can be interpreted as how much the deterministic graph-signal \((G,)\) "looks like" it was randomly sampled from \((W,g)\).

### Graphon-signal regularity lemma

To formulate our regularity lemma, we first define spaces of step functions.

**Definition 3.3**.: _Given a partition \(_{k}\), and \(d\), we define the space \(^{d}_{_{k}}\) of step functions of dimension \(d\) over the partition \(_{k}\) to be the space of functions \(F:^{d}\) of the form_

\[F(x_{1},,x_{d})=_{j=(j_{1},,j_{d})[k]^{d}}c_{j}_{l=1}^{ d}_{P_{j_{l}}}(x_{l}), \]

_for any choice of \(\{c_{j}\}_{j[k]^{d}}\)._

We call any element of \(_{0}^{2}_{_{k}}\) a _step graphon_ with respect to \(_{k}\). A step graphon is also called a _stochastic block model (SBM)_. We call any element of \(^{}_{r}^{1}_{_{k}}\) a _step signal_. We also call \([_{r}]_{_{k}}:=(_{0}^{2}_{_{k}})(^{}_{r}^{1 }_{_{k}})\) the space of SBMs with respect to \(_{k}\).

In Appendix B.2 we give a number of versions of the graphon-signal regularity lemma. Here, we show one version in which the partition is fixed regardless of the graphon-signal.

**Theorem 3.4** (Regularity lemma for graphon-signals - equipartition).: _For any \(c>1\), and any sufficiently small \(>0\), for every \(n 2^{}}\) and every \((W,f)_{r}\), there exists a step graphon-signal \((W_{n},f_{n})[_{r}]_{_{n}}\) such that_

\[_{}(W,f),(W_{n},f_{n}), \]

_where \(_{n}\) is the equipartition of \(\) into \(n\) intervals._

Figure 2 illustrates the graphon-signal regularity lemma. By identifying graph-signals with their induced graphon-signals, (6) shows that the space of graph-signals is dense in the space of graphon-signals with cut distance.

Similarly to the classical case, Theorem 3.4 is interpreted as follows. While deterministic graph-signals may seem intricate and complex, they are actually regular, and "look like" random graph-signals that were sampled from SBMs, where the number of blocks of the SBM only depends on the desired approximation error between the SBM and the graph-signal, and not on the graph-signal itself.

**Remark 3.5**.: _The lower bound \(n 2^{}}\) on the number of steps in the graphon-signal regularity lemma is essentially tight in the following sense. There is a universal constant \(C\) such that for every \(>0\) there exists a graphon-signal \((W,f)\) such that no step graphon-signal \((W^{},f^{})\) with less than \(2^{}}\) steps satisfies \(_{}(W,f),(W^{},f^{})\). To see this, [8, Theorem 1.4, Theorem 7.1] shows that the bound in the standard weak regularity lemma (for graphs/graphons) is essentially tight in the above sense. For the graphon-signal case, we can take the graphon \(W^{}\) from [8, Theorem 7.1] which does not allow a regularity partition with less than \(2^{}}\) steps, and consider the graphon-signal \((W^{},1)\), which then also does not allow such a regularity partition._

Figure 2: Illustration of the graphon-signal regularity lemma. The values of the graphon are in gray scale over \(^{2}\), and the signal is plotted in color on the diagonal of \(^{2}\). (a) A graphon-signal. (b) Representation of the same graphon-signal under the “good” permutation/measure preserving bijection guaranteed by the regularity lemma. (c) The approximating step graphon-signal guaranteed by the regularity lemma.

### Compactness of the graphon-signal space and its covering number

We prove that \(_{r}}\) is compact using Theorem 3.4, similarly to [25, Lemma 8]. Moreover, we can bound the number of balls of radius \(\) required to cover \(_{r}}\).

**Theorem 3.6**.: _The metric space \((_{r}},_{})\) is compact. Moreover, given \(r>0\) and \(c>1\), for every sufficiently small \(>0\), the space \(_{r}}\) can be covered by_

\[()=2^{k^{2}} \]

_balls of radius \(\), where \(k= 2^{}{k^{_{}}}}\)._

The Proof of Theorem 3.6 is given in Appendix C. This is a powerful result - the space of arbitrarily large graph-signals is dense in the "small" space \(_{r}}\). We will use this property in Section 4.3 to prove a generalization bound for MPNNs.

### Graphon-signal sampling lemmas

In this section we prove that randomly sampling a graphon signal produces a graph-signal that is close in cut distance to the graphon signal. Let us first describe the sampling setting. More details on the construction are given in Appendix D.1. Let \(=(_{1},_{k})^{k}\) be \(k\) independent uniform random samples from \(\), and \((W,f)_{r}\). We define the _random weighted graph_\(W()\) as the weighted graph with \(k\) nodes and edge weight \(w_{i,j}=W(_{i},_{j})\) between node \(i\) and node \(j\). We similarly define the _random sampled signal_\(f()\) with value \(f_{i}=f(_{i})\) at each node \(i\). Note that \(W()\) and \(f()\) share the sample points \(\). We then define a random simple graph as follows. We treat each \(w_{i,j}=W(_{i},_{j})\) as the parameter of a Bernoulli variable \(e_{i,j}\), where \((e_{i,j}=1)=w_{i,j}\) and \((e_{i,j}=0)=1-w_{i,j}\). We define the _random simple graph_\((W,)\) as the simple graph with an edge between each node \(i\) and node \(j\) if and only if \(e_{i,j}=1\).

We note that, given a graph signal \((G,)\), sampling a graph-signal from \((W,f)_{(G,)}\) is equivalent to subsampling the nodes of \(G\) independently and uniformly (with repetitions), and considering the resulting subgraph and subsignal. Hence, we can study the more general case of sampling a graphon-signal, where graph-signal sub-sampling is a special case. We now extend [24, Lemma 10.16], which bounds the cut distance between a graphon and its sampled graph, to the case of a sampled graph-signal.

**Theorem 3.7** (Sampling lemma for graphon-signals).: _Let \(r>1\). There exists a constant \(K_{0}>0\) that depends on \(r\), such that for every \(k K_{0}\), every \((W,f)_{r}\), and for \(=(_{1},_{k})^{k}\) independent uniform random samples from \(\), we have_

\[_{}W,f,W(),f()<},\]

_and_

\[_{}W,f, (W,),f()<}.\]

The proof of Theorem 3.7 is given in Appendix D.2

## 4 Graphon-signal analysis of MPNNs

In this section, we propose utilizing the compactness of the graphon-signal space under cut distance, and the sampling lemma, to prove regularity results for MPNNs, uniform generalization bounds, and stability to subsampling theorems.

### MPNNs on graphon signals

Next, we define MPNNs on graphon-signals, in such a way that the application of a MPNN on an induced graphon-signal is equivalent to applying the MPNN on the graph-signal and then inducing it. A similar construction was presented in , for average aggregation, but we use normalized sum aggregation.

At each layer, we define the message function \((x,y)\) as a linear combination of simple tensors as follows. Let \(K\). For every \(k[K]\), let \(^{k}_{t},^{k}_{t}:^{d}^{p}\) be Lipschitz continuous functions that we call the _receiver_ and _transmitter message functions_ respectively. Define the _message function_\(:^{2d}^{p}\) by

\[(a,b)=_{k=1}^{K}^{k}_{r}(a)^{k}_{t}(b),\]

where the multiplication is elementwise along the feature dimension. Given a signal \(f\), define the _message kernel_\(_{f}:^{2}^{p}\) by

\[_{f}(x,y)=(f(x),f(y))=_{k=1}^{K}^{k}_{r}(f(x))^{k}_{t}(f(y)).\]

We see the \(x\) variable of \(_{f}(x,y)\) as the receiver of the message, and \(y\) as the transmitter. Define the aggregation of a message kernel \(Q:^{2}^{p}\), with respect to the graphon \(W_{0}\), to be the signal \((W,Q)_{r}^{}\), defined by

\[(W,Q)(x)=_{0}^{1}W(x,y)Q(x,y)dy,\]

for an appropriate \(r>0\). A _message passing layer (MPL)_ takes the form \(f^{(t)}(W,_{f^{(t)}}^{(t+1)})\), where \(f^{(t)}\) is the signal at layer \(t\). Each MPL is optionally followed by an _update layer_, which updates the signal pointwise via \(f^{(t+1)}=^{(t+1)}f^{(t)}(x),(W,_{f^{(t)}}^{(t+1)})( x)\), where \(^{(t+1)}\) is a learnable mapping called the _update function_. A MPNN is defined by choosing the number of layers \(T\), and defining message and update functions \(\{^{t},(^{k}_{r}),(^{t}_{t}^{k}_{t})\}_{k[K],t[T]}\). A MPNN only modifies the signal, and keeps the graph/graphon intact. We denote by \(_{t}(W,f)\) the output of the MPNN applied on \((W,f)_{r}\) at layer \(t[T]\). More details on the construction are given in SectionE.1.

The above construction is rather general. Indeed, it is well known that many classes of functions \(F:^{d}^{d}^{C}\) (e.g., \(L^{2}\) functions) can be approximated by (finite) linear combinations of simple tensors \(F(a,b)_{k=1}^{K}^{k}_{1}(a)^{k}_{2}(b)\). Hence, message passing based on general message functions \(:^{2d}^{p}\) can be approximated by our construction. Moreover, many well-known MPNNs can be written using our formulation with a small \(K\), e.g., [29; 36] and spectral convolutional networks [9; 20; 21], if we replace the aggregation in these method with normalized sum aggregation.

In SectionE.1 we show that for any graph-signal \((G,)\), we have \(_{t}(W,f)_{(G,)}=(W,f)_{_{t}(G,)}\), where the MPNN on a graph-signal is defined with normalized sum aggregation

\[(G,_{})_{i}=_{j[n]} a_{i,j}(_{})_{i,j}.\]

Here, \(n\) is the number of nodes, and \(\{a_{i,j}\}_{i,j[n]}\) is the adjacency matrix of \(G\). Hence, we may identify graph-signals with their induced graphon-signals when analyzing MPNNs.

### Lipschitz continuity of MPNNs

We now show that, under the above construction, MPNNs are Lipschitz continuous with respect to cut distance.

**Theorem 4.1**.: _Let \(\) be a MPNN with \(T\) layers. Suppose that there exist constants \(L,B>0\) such that for every layer \(t[T]\), every \(y\{,\}\) and every \(k[K]\),_

\[^{t}(0)\,,\;^{t}^{k}_{y}(0) B, {and} L_{^{t}},\;L_{^{k}_{y}}<L,\]

_where \(L_{^{t}}\) and \(L_{^{k}_{y}}\) are the Lipschitz constants of \(^{t}\) and \(^{t}_{y}\). Then, there exists a constant \(L_{}\) (that depends on \(T,K,B\) and \(L\)) such that for every \((W,f),(V,g)_{r}\),_

\[\|(W,f)-(V,g)\|_{} L_{}\|f-g\|_{}+ \|W-V\|_{}.\]

The constant \(L_{}\) depends exponentially on \(T\), and polynomially on \(K,B\) and \(L\). For formulas of \(L_{}\), under different assumptions on the hypothesis class of the MPNN, we refer to AppendixF.

### A generalization theorem for MPNN

In this section we prove a uniform generalization bound for MPNNs. For background on generalization analysis, we refer the reader to Appendix G.1. While uniform generalization bounds are considered a classical approach in standard neural networks, the approach is less developed in the case of MPNNs. For some works on generalization theorems of MPNNs, see [14; 23; 26; 30; 33].

When a MPNN is used for classification or regression, \(_{T}\) is followed by global pooling. Namely, for the output signal \(g:^{p}\), we return \( g(x)dx^{p}\). This is then typically followed by a learnable mapping \(^{p}^{C}\). In our analysis, we see this mapping as part of the loss, which can hence be learnable. The combined loss is assumed to be Lipschitz continuous2.

We model the ground truth classifier into \(C\) classes as a piecewise constant function \(:_{r}}\{0,1\}^{C}\), where the sets of different steps in \(_{r}}\) are Borel measurable sets, correspond to different classes. We consider an arbitrary probability Borel measure \(\) on \(_{r}}\) as the data distribution. More details on the construction are given in Appendix G.2.

Let \((_{r}},L_{1})\) be the space of Lipschitz continuous mappings \(:_{r}}^{C}\) with Lipschitz constant \(L_{1}\). By Theorem 4.1, we may assume that our hypothesis class of MPNNs is a subset of \((_{r}},L_{1})\) for some given \(L_{1}\). Let \(=(X_{1},,X_{N})\) be independent random samples from the data distribution \((_{r}},)\). Let \(_{}\) be a model that may depend on the sampled data, e.g., via training. Let \(\) be a Lipschitz continuous loss function3 with Lipschitz constant \(L_{2}\). For every function \(\) in the hypothesis class \((_{r}},L_{1})\) (i.e. \(_{}\)), define the _statistical risk_

\[()=(,) =((x),(x))d(x).\]

We define the empirical risk \(}(_{},)=_{i=1}^{N} _{}(X_{i}),(X_{i})\).

**Theorem 4.2** (MPNN generalization theorem).: _Consider the above classification setting, and let \(L=L_{1}L_{2}\). Let \(X_{1},,X_{N}\) be independent random samples from the data distribution \((_{r}},)\). Then, for every \(p>0\), there exists an event \(^{p}_{r}}^{N}\), with probability_

\[^{N}(^{p}) 1-Cp-2}{N},\]

_in which_

\[(_{})-}(_{ },)^{-1}(N/2C)2L+} L+(0,0)1+, \]

_where \(()=(())}{^{2}}\), \(\) is the covering number of \(_{r}}\) given in (7), and \(^{-1}\) is the inverse function of \(\)._

The theorem is proved in Appendix G.4. Note that the term \(^{-1}(N/2C)\) in (8) decreases to zero as the size of the training set \(N\) goes to infinity.

In Table 1 we compare the assumptions and dependency on the number of data points of different generalization theorems. All past works consider special assumptions. Our work provides upper bounds under no assumptions on the data distribution, and only mild assumptions on the MPNN (Lipschitz continuity of the message passing and update functions). In Table 2 in Appendix G.5 we present experiments that illustrate the generalization capabilities of MPNNs with normalized sum aggregation.

### Stability of MPNNs to graph-signal subsampling

When working with very large graphs, it is often the practice to subsample the large graph, and apply a MPNN to the smaller subsampled graph [5; 7; 16]. Here, we show that such an approach is justifiedtheoretically. Namely, any (Lipschitz continuous) MPNN has approximately the same outcome on the large graph and its subsampled version.

Transferability and stability analysis [18; 22; 27; 31; 32] often studies a related setting. Namely, it is shown that a MPNN applied on a randomly sampled graph \(G\) approximates the MPNN on the graph \(W\) from which the graph is sampled. However, previous analyses assumed that the generating graph \(W\) has metric properties. Namely, it is assumed that there is some probability metric space \(\) which is the graphon domain, and the graphon \(W:\) is Lipschitz continuous with respect to \(\), where the dimension of \(\) affects the asymptotics. This is an unnatural setting, as general graphons are only assumed to be measurable, not continuous. Constraining the construction to Lipschitz continuous graphons with a uniformly bounded Lipschitz constant only accounts for a small subset of \(_{r}\), and, hence, limits the analysis significantly. In comparison, our analysis applies to any graphon-signal in \(_{r}\). When we only assume that the graphon is measurable, \(\) is only treated as a standard (atomless) probability space, which is very general, and equivalent for example to \(^{d}\) for any \(d\), and to any Polish space. Note that graphon theory allows restricting the graphon domain to \(\) since \(\), as a measure space, is very generic.

**Theorem 4.3**.: _Consider the setting of Theorem 4.2, and let \(\) be a MPNN with Lipschitz constant \(L\). Denote_

\[=W,(W,f),()= (W,),(W,),f() .\]

_Then_

\[_{},()< }L.\]

## 5 Discussion

We presented an extension of graphon theory to a graphon-signal theory. Especially, we extended well-known regularity, compactness, and sampling lemmas from graphons to graphon-signals. We then showed that the normalized sum aggregation of MPNNs is in some sense compatible with the graphon-signal cut distance, which leads to the Lipschitz continuity of MPNNs with respect to cut distance. This then allowed us to derive generalization and sampling theorems for MPNNs. The strength of our analysis is in its generality and simplicity- it is based on a natural notion of graph similarity, that allows studying the space of _all_ graph-signals, it applies to any graph-signal data distribution, and does not impose any restriction on the number of parameters of the MPNNs, only to their regularity through the Lipschitzness of the message functions.

The main limitation of the theory is the very slow asymptotics of the generalization and subsampling theorems. This follows the fact that the upper bound on the covering number of the compact space \(_{r}}\) grows faster than the covering number of any finite-dimensional compact space. Yet, we believe that our work can serve as a point of departure for future works, that 1) will model subspaces of \(_{r}}\) of lower complexity, which approximate the support of the data-distribution in real-life settings of graph machine learning, and, 2) will lead to improved asymptotics. Another open problem is to find an essentially tight estimate of the covering number of \(_{r}}\), which may be lower than the estimate presented in this paper.

 
**Generalization analysis paper** & **Assumption on the graphs** & **No weight sharing** & **General MPL**. & **Dependencies on \(N\)** \\  Generalization Limits of DNNs  & bounded degree & ✗ & ✗ & \(N^{-1/2}\) \\  PAC-bayesian MPNN  & bounded degree & ✗ & ✗ & \(N^{-1/2}\) \\  PAC-bayesian GCN  & bounded degree & ✓ & ✗ & \(N^{-1/2}\) \\  VC meets IVL  & bounded color complexity & ✓ & ✗ & \(N^{-1/2}\) \\  Generalization Analysis of MPNNs  & sampled from a small set of graphons & ✓ & ✓ & \(N^{-1/2}\) \\ 
**Our graphon-signal theory** & **non** & ✓ & ✓ & \(^{-1}(N)\) \\  

Table 1: Comparison of the assumptions made by different GNN generalization analysis papers.