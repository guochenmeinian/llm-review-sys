# End-to-End Meta-Bayesian Optimisation with Transformer Neural Processes

Alexandre Maraval

Huawei Noah's Ark Lab

alexandre.maraval1@huawei.com

&Matthieu Zimmer

Huawei Noah's Ark Lab

matthieu.zimmer@huawei.com

These authors contributed equally to this work

Antoine Grosnit

Huawei Noah's Ark Lab

Technische Universitat Darmstadt

antoine.grosnit2@huawei.com

&Haitham Bou Ammar

Huawei Noah's Ark Lab

University College London

haitham.ammar@huawei.com

###### Abstract

Meta-Bayesian optimisation (meta-BO) aims to improve the sample efficiency of Bayesian optimisation by leveraging data from related tasks. While previous methods successfully meta-learn either a surrogate model or an acquisition function independently, joint training of both components remains an open challenge. This paper proposes the first end-to-end differentiable meta-BO framework that generalises neural processes to learn acquisition functions via transformer architectures. We enable this end-to-end framework with reinforcement learning (RL) to tackle the lack of labelled acquisition data. Early on, we notice that training transformer-based neural processes from scratch with RL is challenging due to insufficient supervision, especially when rewards are sparse. We formalise this claim with a combinatorial analysis showing that the widely used notion of regret as a reward signal exhibits a logarithmic sparsity pattern in trajectory lengths. To tackle this problem, we augment the RL objective with an auxiliary task that guides part of the architecture to learn a valid probabilistic model as an inductive bias. We demonstrate that our method achieves state-of-the-art regret results against various baselines in experiments on standard hyperparameter optimisation tasks and also outperforms others in the real-world problems of mixed-integer programming tuning, antibody design, and logic synthesis for electronic design automation.

## 1 Introduction

Bayesian optimisation (BO) techniques are sample-efficient sequential model-based solvers optimising expensive-to-evaluate black-box objectives. Traditionally, BO methods operate in isolation focusing on one task at a time, a setting that led to numerous successful applications, including but not limited to hyperparameter tuning , drug and chip design , and robotics . Although successful, focusing on only one objective in isolation may increase sample complexities on new tasks since standard BO algorithms work tabula-rasa as they start optimising from scratch, ignoring previously observed black-box functions.

To improve sample efficiency on new _target_ tasks, meta-BO makes use of data collected on related _source_ tasks  and attempts to transfer knowledge in between. Those methods are primarily composed of two parts: 1) a (meta) surrogate model predicting the function to optimise and 2) a (meta) acquisition function (AF) estimating how good a queried location is. Regarding surrogatemodelling, prior work relies on Gaussian processes (GPs) to perform transfer across tasks due to their data efficiency [5; 6; 7; 8; 9], or focuses on deep neural networks [10; 11; 12] gaining from their representation flexibility. As for acquisition functions, previous works assumed a fixed GP and trained a neural network to perform transfer [13; 14].

Although successful, those methods rely on the assumption that the two steps described above can be handled independently, potentially missing benefits from considering both steps together. Therefore, this paper advocates for an end-to-end training protocol for meta-BO where we train a model predicting AF values directly from observed data, without relying on a GP surrogate.

As shown in [10; 12; 15; 16], Neural processes (NP) are a good candidate for meta-learning due to their structural properties, thus we propose to use a new transformer-based NP to model the acquisition function directly .

Nevertheless, the lack of labelled AF data prohibits a supervised learning protocol. Here, we rely on reward functions to assess the goodness of AFs and formulate a new reinforcement learning (RL) problem. Our RL formulation attempts to learn an optimal policy that selects new evaluation probes by minimising per-task cumulative regrets.

Early on, we faced very unstable training with minimal learning due to the combination of RL with transformers . Furthermore, we notice that this problem amplifies when the reward function is sparse, which is the case in our setting, as we formally show in Section 3.2. To mitigate this difficulty, we introduce an inductive bias in our transformer architecture via an auxiliary loss  that guides a part of the network to learn a valid probabilistic model, effectively specialising as a neural process with gradient updates. Having developed our framework, we demonstrate state-of-the-art regret performance in hyperparameter optimisation of real-world problems and combinatorial sequence optimisation for antibody and chip design (see Section 4). Those solid empirical results show the value of end-to-end training in meta-BO to further improve sample complexity.

**Contributions** We summarise our contributions as follows: _i)_ developing the first end-to-end transformer-based architecture for meta-BO predicting acquisition function values, _ii)_ identifying logarithmic reward sparsity patterns hindering end-to-end learning with RL, _iii)_ introducing NP-based inductive biases for successful model updates, and _iv)_ demonstrating new state-of-the-art regret results on a wide range of traditional (hyperparameter tuning) and non-traditional (chip and antibody design) real-world benchmarks.

## 2 Background

### Bayesian Optimisation

In BO, we sequentially maximise an expensive-to-evaluate black-box function \(f()\) for variables \(\). BO techniques operate in two steps. First, based on previously collected evaluation data, we fit a probabilistic surrogate model to emulate \(f()\) allowing us to make probabilistic predictions of the function's behaviour on unobserved input points. Given the first step's probabilistic predictions, the second step in BO optimises an AF that trades off exploration and exploitation to find a new input query, \(_{}\), for evaluation. Both the model and acquisition choices play a critical role in the success of BO. Many works adopt GPs as surrogate models due to their sample efficiency and practical uncertainty estimation . Moreover, on the acquisition side, the common practice is to optimise one from a set of widely adopted AFs such as Expected Improvement (EI) .

### Transfer in Bayesian Optimisation

Transfer techniques in BO [4; 13] aim to improve the optimisation of a newly observed _target_ black-box function by reusing knowledge from past experiences gathered in related _source_ domains, i.e. solve \(_{}f_{}()\) by leveraging information from \(K\) source black-boxes \(f_{1}(),,f_{K}()\). We assume this information is available to our agent via \(_{1},,_{K}\) such that each dataset \(_{k}=\{(_{i}^{(k)},y_{i}^{(k)})\}_{i=1}^{n^{(k)}}\) consists of \(n^{(k)}\) (noisy) evaluations of \(f_{k}()\) for all \(k[1:K]\). Following the meta-learning literature [4; 5; 13], we impose no additional assumptions on the process that collects \(_{1},,_{K}\), allowing for a diverse set of source optimisation algorithms, including but not limited to BO, genetic and evolutionary algorithms , and sampling-based strategies . Many algorithms that use source data to improve performance on target domains exist. We categorise those based on the part they customise within the BO pipeline, i.e., by registering if they affect initial points [24; 25], search spaces , surrogate models  or AFs [13; 14]. Since our work devises end-to-end meta-BO pipelines, we now elaborate on critical surrogate models needed for the rest of the paper and leave a detailed presentation of related work to Section 5.

Although GPs  are a prominent tool for single-task BO, high computational complexity , among other drawbacks (e.g. smoothness assumptions or limited scalability in terms of dimensions), can limit their application to transfer scenarios. Of course, one can generalise GPs to support transfer using multi-task kernels  or GP ensembles , for example. However, recent trends demonstrated superior performance to such extensions when using deep networks (e.g., neural processes) to meta-learn probabilistic surrogates [5; 13].

**Neural Processes** A popular method for effective meta-learning consists of directly inputting context points (i.e., available data to adjust to a target task) for a model to adapt to unseen tasks . We can accomplish such a goal by relying on neural processes (NPs), a class of models that combine the flexibility of deep neural networks with the properties of stochastic processes . Given an observation set of input-output pairs \(_{}\) and a set of \(n_{}\) locations \(^{()}\) at which we desire to make predictions, an NP parameterised by \(\) outputs a distribution \(p_{}(|^{()},_{})\) that approximates the true posterior over the labels \(y^{()}\). Among the different parameterisations of \(p_{}()\), e.g., in [29; 30; 31], transformer architectures  recently emerged as compelling choices of NPs [11; 12]. In this work, we also adopt a transformer architecture inspired by this architecture's broad software support and state-of-the-art supervised learning results as reported in .

### Reinforcement Learning

In Section 3, we attempt to learn an end-to-end model that predicts acquisition values. Of course, we cannot fit the parameters of such a model with supervised learning due to the lack of labelled acquisition data. RL  is a viable alternative for learning from delayed and non-differentiable reward signals in those cases. In RL, we formalise problems as Markov decision processes (MDP): \(=,,,,\), where \(\) and \(\) denote the state and action spaces, \(:\) the state transition model, \(\) the reward function dictating the optimisation goal, and \([0,1)\) a discount factor specifying the degree to which rewards are discounted over time. A policy \(:\) is an action-selection rule that is defined as a probability distribution over state-action pairs, where \((_{t}|_{t})\) represents the probability of selecting action \(_{t}\) in state \(_{t}\). An RL agent aims to find an optimal policy \(^{}\) that maximises (discounted) expected returns. For determining \(^{}\), we use the state-of-the-art proximal policy optimisation (PPO) algorithm .

## 3 End-to-End Meta-Bayesian Optimisation

While transfer techniques in BO saw varying degrees of success in many applications, current approaches lack end-to-end differentiability, where model learning and acquisition discovery arise as two separate steps. Specifically, the surrogate model gradients hardly affect the acquisition network, and the acquisition's network gradients fail to back-propagate to the surrogate's updates.

Enabling end-to-end transfer frameworks in which we learn the surrogate and acquisition jointly hold the promise for more scalable and easier-to-deploy algorithms that are more robust to input data or task changes. Following such a framework, we also expect more accurate predictions that can lead to better regret results (see Section 4) since we optimise the entire transfer pipeline, including the intermediate probabilistic model and AF representations. Additionally, end-to-end training techniques allow us to mitigate the need for domain-specific expertise and permit stable implementations that benefit fully from GPU and computing hardware.

The most straightforward way to enable end-to-end training in Bayesian optimisation is to introduce a deep network that acquires search variables and historical evaluations of black-box functions as inputs and outputs acquisition values after a set of nonlinear transformations. Of course, it is challenging to fit the weights of such a network due, in part, to a lack of labelled acquisition data where search variables and history of evaluations are inputs and acquisition values are labels.

### Reinforcement Learning for End-to-End Training

Our approach utilises RL to fit the network's parameters \(\) from minimal supervision, circumventing the need for labelled acquisition data. To formalise the RL problem, we introduce an MDP where:

**State**: \(_{t}=[_{t},t,T]\) (history, BO time-step & budget) **Action**: \(_{t}=_{t}\) (choice of new probe), with \(_{t}=\{_{1},y_{1}\,,,_{t-1}, y_{t-1}\}\) denoting the history of black-box evaluations up-to the current time-step \(t\). Adding the current BO step \(t\) and the maximum budget \(T\) in our state variable \(_{t}\) helps balance exploration versus exploitation trade-offs as noted in . Our MDP's transition function is straightforward, updating \(_{t}\) by appending newly evaluated points, i.e., \(_{t+1}=_{t}\{_{t},y_{t}\}\) and incrementing the time variable \(t\). Regarding rewards, we follow the well-established literature [13; 14] and define \(r_{t}=_{1 t}y_{}\) to correspond to simple regret. Given such an MDP, our agent attempts to find a parameterised policy \(_{}\) which, when conditioned on \(_{t}\), proposes a new probe \(_{t}\) that minimises cumulative regret (i.e., the sum of total discounted simple regrets).

**Extensions to Multi-Task Reinforcement Learning** The above MDP describes an RL configuration that learns \(\) in a single BO task. We now extend this formulation to multi-task scenarios allowing for a meta-learning setup. To do so, we introduce a set of MDPs \(_{1},,_{K}\) with \(K\) being the total number of available tasks. This paper considers same-domain multi-task learning scenarios. As such, we assume that all MDPs share the same state and action spaces and leave cross-domain extensions as an interesting avenue for future work. We define each MDP, \(_{k}\), as previously introduced such that:

**States: \(_{t}^{(k)}=[_{t}^{(k)},t^{(k)},T^{(k)}]\)**Actions: \(_{t}^{(k)}=_{t}^{(k)}\)**Rewards: \(r_{t}^{(k)}=_{1 t^{(k)}}y_{t}^{(k)}\ \  k\).**

Moreover, for each task \(k\), the transition model updates task-specific histories with \(_{t+1}^{(k)}=_{t}^{(k)}\{_{t}^{(k)},y_{ t}^{(k)}\}\) and increments \(t^{(k)}\). Contrary to the single task setup, we now seek a policy \(_{}\) which performs well on average across \(K\) tasks:

\[_{_{}}J(_{})=_{_{}} _{k p_{}}[_{_{T^{(k)}}^{(k )} p_{_{}}}[_{t=1}^{T^{(k)}}^{t-1}r_{t}^{(k)} ]], \]

where \(p_{}\) denotes the task distribution. Furthermore, the per-task history distribution \(p_{_{}}(_{T^{(k)}}^{(k)})\) is jointly parameterised by \(\) and defined as:

\[p_{_{}}(_{T^{(k)}}^{(k)})=p(y_{T^{(k) }-1}^{(k)}_{T^{(k)}-1}^{(k)})_{}(_ {T^{(k)}-1}^{(k)}|_{T^{(k)}-1}^{(k)}) p(y_{1}^{(k )}|_{1}^{(k)})_{0}(_{1}^{(k)}), \]

with \(_{0}(_{1}^{(k)})\) denoting an initial (action) distribution from which we sample the first decision \(_{1}^{(k)}\). While it appears that off-the-shelf PPO can tackle the problem in Equation 1, Section 3.2 details difficulties associated with such an RL formulation, noting that in BO situations, the sparsity of reward signals can impede the learning of the network's weights \(\). Before presenting those arguments, we now differentiate from the closest prior art, clarifying critical MDP differences.

**Connection & Differences to ** Although we are the first to propose end-to-end multi-task MDPs for meta-BO, others have also used RL to discover AFs, for example. Our parameterisation architecture and MDP definition significantly differ from the prior art, particularly from , the closest method to our work. Our approach uses a transformer-based deep network model to parameterise the whole pipeline (see Section 3.4). In contrast, the work in  assumes a pre-trained fixed Gaussian process surrogate and uses multi-layer perceptrons only to discover acquisitions. Our state variable requires historical information from which we jointly learn a probabilistic model and an acquisition end-to-end instead of requiring posterior means and variances of a Gaussian process model. Hence, our framework is more flexible, allowing us to model non-smooth black-box functions while overcoming some drawbacks of GP surrogates, like training and inference times.

### Limitations of Regret Rewards in End-to-End Training

To define Equation 1, we followed the well-established literature of meta-BO [4; 13] and utilised simple regret reward functions. Although this choice is reasonable, we face challenges in applying such rewards in end-to-end training. Apart from difficulties associated with end-to-end trainingof deep architectures , our RL algorithm is subject to additional complexities when estimating gradients from Equation 1 due to the sparsity of the reward function. To better understand this problem, we start by noticing that for a reward component \(r_{t}^{(k)}\) to contribute to the cumulative summation \(_{t}^{t-1}r_{t}^{(k)}\), we need to observe a function value \(y_{t}^{(k)}\) that outperforms all values we have seen so far, i.e., \(y_{t}^{(k)}>_{1<t}y_{}^{(k)}\). During the early training stages of RL, we can quantify the average number of such informative events (when \(y_{t}^{(k)}>_{1<t}y_{}^{(k)}\)) by a combinatorial argument that frames this calculation as a calculation of the number of cycles in a permutation of \(T^{(k)}\) elements, leading us to the following lemma.

**Lemma 3.1**.: _Consider a task with a horizon length (budget) \(T\), and define \(r_{t}=_{1 t}y_{t}\) the simple regret as introduced in Equation 1. For a history \(_{T}\), let \(m_{}\) denote the total number of informative rewards, i.e. the number of steps \(t\) at which \(y_{t}>_{1<t}y_{}\). Under a random policy \(_{}\), the number of informative events is logarithmic in \(T\) such that: \(_{ p_{_{}}}[m_{}] =( T)\), where \(p_{_{}}\) is induced by \(_{}\) as in Equation 2._

We defer the proof of Lemma 3.1 to Appendix A due to space constraints. Here, we note that this result implies that the information contained in one sampled trajectory is sparse at the beginning of RL training when the policy acts randomly. Of course, this increases the difficulty of estimating informative gradients of Equation 1 when updating \(\). One can argue that the sparsity described in Lemma 3.1 only holds under random policies during the early stages of RL training and that sparsity patterns decrease as policies improve. Interestingly, simple regret rewards do not necessarily confirm this intuition. To realise this, consider the other end of the RL training spectrum in which policies have improved to near optimality such that \(_{}_{^{}}\). Because \(_{}\) has been trained to maximise regret, it will seek to suggest the optimal point of the current task, as early as possible in the BO trajectory. Consequently, the policy is encouraged to produce trajectories with _even sparser_ rewards during later training stages, further complicating the problem of informative gradient estimates of Equation 1.

### Inductive Biases and Auxiliary Tasks

Learning from sparse reward signals is a well-known difficulty in the reinforcement learning literature . Many solutions, from imitation learning,  to exploration bonuses , improve reward signals to reduce agent-environment interactions and enhance gradient updates. Others  attempt to define more informative rewards from prior knowledge or via human interactions . Unfortunately, both of those approaches are hard to use in BO. Indeed, manually engineering black-box-specific rewards is notoriously difficult and requires domain expertise and extensive knowledge of the source and target black-box functions we wish to optimise. Furthermore, learning from human feedback is data-intensive, conflicting with the goal of sample-efficient optimisation.

Another prominent direction demonstrating significant gains is the introduction of auxiliary tasks (losses) within RL that allows agents to discover relevant inductive biases leading to impressive empirical successes [19; 39; 40]. Inspired by those results, we propose introducing an inductive bias in our method via an auxiliary supervised loss. Since we have at our disposal the collected source tasks datasets on which we are training our architecture \(^{(1)},,^{(K)}\), we augment our objective such that our RL agent maximises not only rewards but also the likelihood of making correct predictions on these labelled datasets.

**Supervised Auxiliary Loss** Consider a source task \(k\) and consider that we split its corresponding dataset into an observed set and a prediction set \(^{(k)}=^{(k)}_{}^{(k)}_{ }\). We define the auxiliary loss to be exactly the log-likelihood of functions values \(y^{}\) at predicted locations \(^{}\) given observations \(_{}\):

\[()=_{k p_{},^{(k)}_{ },^{(k)}_{}} p_{}(y^{ }_{k}|^{}_{k},^{(k)}_{}) . \]

Interestingly, this part of our model specialises as a neural process (Section 2.2) with \(^{(k)}_{}\) being the history \(^{(k)}_{t}\) and \(^{}\) being the input points at which we wish to make predictions. To represent \(p_{}()\), we use a head of our transformer to predict multi-modal Riemannian (or bar plot probability density function) posteriors as [11; 41]. We compute Equation 3 on random iid splits of \(^{(k)}\) and not directly on trajectories generated by the policy. This is because as the policy improves, the trajectoriesit generates are composed of less diverse (non-id.) points. Indeed as training progresses, \(_{}\) becomes better and therefore finds the optimal point in \(^{(k)}\) more rapidly. To fully take advantage of the labelled data at our disposal, we evaluate this auxiliary loss on iid. data, i.e. splits of \(^{(k)}\) into \(^{(k)}_{}\) and \(^{(k)}_{}\) sampled uniformly at random. It is sensible from a BO standpoint as well since we want the introduced inductive bias to encourage our network to maximise the likelihood not only in a neighbourhood of the optimiser but also in the rest of the dataset.

### Neural Acquisition Processes

We now introduce in more detail our transformer architecture and note some of its important properties. We call this architecture the neural acquisition process (NAP) because it is a new type of NPs jointly predicting AF values and distribution over actions. Similarly to other NP [11; 12], it takes a context and queried locations as input. We parameterise it by \(\) and denote the acquisition prediction by \(_{}(^{},,t,T)\).

Since \(_{}\) outputs real numbers, we still need to define how we can obtain a valid probability distribution over the action space to form a policy \(_{}(|_{t})\). Defining such a distribution over the whole action space is hard if \(\) is continuous. Hence, similarly to Volpp et al. , we evaluate the policy \(_{}\) only on the finite set of locations \(^{}\) for a given task1. Therefore, we have:

\[_{}^{}_{t}|_{t} }(^{}_{t},_{t},t,T)} }{_{i}^{n_{}}e^{_{}(^{}_{t},_{t},t,T)}}.\]

We now have all the necessary components to define the full objective combining Equation 1 and 3: \(()=(_{})+()\), where \(\) is a hyperparameter to balance between the two objectives. We summarise the full training algorithm in Alg. 1 detailing each of the components.

```
0: Source tasks training data \(\{^{(k)}\}_{k=1}^{K}\), initial parameters \(\), budgets \(T^{(k)} T\), discount factor \(\), learning rate \(\) for each epoch do  select task \(k\) and dataset \(^{(k)}\), set \(_{0}=\{\}\) for\(t=1,,T\)do \(x_{t}_{}(|_{t})\)\(\) predict action \(y_{t}=f^{(k)}(x_{t})\)\(\) execute action \(r_{t}=y^{*}_{ t}\)\(\) collect reward \(_{t+1}_{t}\{(x_{t},y_{t})\}\)\(\) update hist. endfor \(R=_{i=1}^{}^{i}r_{t}\)\(\) cumul. reward \(_{}_{}\)\(\) split source data \(=p_{}(y^{}|^{},_{ })\)\(\) aux. loss \(+(_{}R+_{} \ )\)\(\) update \(\) endfor
```

**Algorithm 1** Neural Acquisition Process training.

Finally, we study some desirable properties of NAPs, explain how we can achieve them, and why they are important in the context of BO.

**Property 3.2** (History-order invariance).: _An NP \(g\) is history-order invariant if for any choice of permutation function \(\) that changes the order of the points in history, \(g(,())=g(,)\)._

Unlike vanilla transformers, we do not use a positional encoding. It allows NAP to treat the history \(\) as a set instead of an ordered sequence  and to be invariant to the order of the history. To form an \(,y\) pair in this set, we sum the embedding of \(\) and \(y\), whereas for queried locations we only use the embedding of \(\) for a token (see Fig. 1, left). It is important for BO since the order in which we collect the points is not relevant for assessing how promising is a new queried location. Previous meta-RL algorithms  also shown the importance of relying on order-invariance.

  & History-order inv. & Query ind. & AF values & Tokens \\  NAP (ours) & & & & & \(t+n_{}\) \\ TNPs  & & & & & \(t+n_{}\) \\ OptFormer  & & & & & \(L+(D+2)(t+n_{})\) \\ PFN  & & & & & \(t+n_{}\) \\ 

Table 1: We compare the properties of different transformer architectures. \(L\) is the number of tokens needed to encode the meta-data, and \(D\) denotes the dimensionality of \(\).

**Property 3.3** (Query independence).: _A NP \(g\) is query independent if for any choice of \(n\) queried locations \(^{}=(^{}_{1},,^{}_ {n})\), we have \(g(^{},)=(g(^{}_{1}, ),,g(^{}_{n},))\)._

In NAP, every token in the history can access each other through the self-attention mask, whereas the elements of \(^{}\) can only attend to themselves and to tokens in the history \(\) (see Fig. 1, right). Because the tokens inside \(^{}\) cannot access each other and we do not use positional encoding, NAP is query independent, which is important to make consistent predictions of AF values for BO as they should not depend on the other queried locations. Additionally, NAP is fully differentiable enabling end-to-end training but also optimisation of the queried locations via gradient ascent for continuous action spaces. In Table 1, we highlight the differences with other state-of-the-art models regarding those properties.

## 4 Experiments

We conduct experiments on hyperparameter optimisation (HPO) and sequence optimisation tasks to assess NAP's efficiency. Regarding HPO, we run our algorithm on datasets from the HPO-B benchmark  and in real-world settings of tuning hyperparameters for Mixed-Integer Programming (MIP) solvers. For sequence optimisation, we test NAP on combinatorial black-box problems from antibody design and synthesis flow optimisation for electronic design and automation (EDA).

**Baselines** We compare our method against popular meta-learning baselines, including few-shot Bayesian optimisation (FSBO) , MetaBO  as well as OptFormers  when applicable. Moreover, we show how classical GP-based BO, equipped with an adequate kernel 2 and an EI acquisition function, which we title GP-EI, performs across domains. We first fit a GP on the meta-training datasets to enable a fair comparison. We initialise the GP model with those learnt kernel parameters at test time. This way, the GP baseline can benefit from the information provided in the source tasks. We also explore training a neural process directly on source task datasets and then using a fixed EI acquisition. For that, we introduce NP-EI that combines the same base architecture from  with EI. Additionally, we contrast NAP against random search (RS).

Following the standard practice in meta-BO, we report our results in terms of normalised regrets for easier comparison across all tasks. We attempt to re-implement all baselines across all empirical domains, extending previous implementations as needed, e.g., developing MetaBO  and FSBO  versions for combinatorial and mixed spaces to enable a fair comparison in MIP solver tuning, antibody and EDA design tasks.

**Remark on OptFormer** We re-run OptFormer with an EI AF on hyperparameter tuning tasks by extending the implementation in  to the discrete search space version of HPO-B. However, the lack of a complete open-source implementation and interpretable meta-data in the other benchmarks (e.g., in antibody design and EDA) prohibited successful execution.

Figure 1: Our proposed NAP architecture (left) and an example of the masks applied during inference (right). We apply independent embedding on \(_{i}\), \(y_{i}\), \(t\) and \(T\). The colored squares mean that the tokens on the left can attend the tokens on the top in the self-attention layer.

### Hyperparameter Optimisation Results

**Hyperparameter Optimisation Benchmarks** We experiment on the HPO-B benchmark , which contains datasets of (classification) model hyperparameters and the models' corresponding accuracies across multiple types and search spaces. Due to resource constraints, we selected six representative search spaces. Nonetheless, we chose the search spaces to represent all underlying classification models in the experiment. We also always picked the ones with the least points to focus on the low data regime performance; see Appendix C.1 for more details. The results of our tests in Figure 2 demonstrate that NAP and OptFormer outperform all other baselines. Surprisingly, although NAP uses a much smaller architecture than OptFormer (around 15 million parameters vs 250 million for OptFormer) and trains on much less data (around 80k original points on average versus more than 3 million points for OptFormer), its regret performance is statistically similar to that of the OptFormer after 100 steps. Moreover, on the same GPU, to perform the same inference, NAP only uses 2% of OptFormer's compute time and around 40% of its memory usage.

**Tuning MIP Solvers** Apart from HPO-B, we consider another real-world example of hyperparameter tuning that requires finding the optimal parameters of MIP solvers. We use the open-source SCIP solver  and the Benchmark suite from the MIPLib2017  that consists of a collection of 240 problems. The objective is to find a set of hyperparameters so that SCIP can solve MIP instances in minimal time. Our high-dimensional search space comprises 135 hyperparameters with mixed types, including boolean, integers, categories and real numbers. We train our model on data collected from BO traces on 103 MIPs and test on a held-out set of 42 instances. Our results in Figure 2 demonstrate that NAP is capable of outperforming all other baselines reaching low regret about an order of magnitude faster than FSBO . Figure 2 further demonstrates the importance of end-to-end training where NAP again outperforms NP-EI.

### Sequence Optimisation Experiments

Now, we demonstrate NAP's abilities beyond hyperparameter tuning tasks in two real-world combinatorial black-box optimisation problems.

**Antibody CDRH3-Sequence Optimisation** This experiment focuses on finding antibodies that can bind to target antigens. Antigens are proteins, i.e., sequences of amino acids that fold into a 3D shape giving them specific chemical properties. A protein region called CDRH3 is decisive in the antibody's ability to bind to a target antigen. Following the work in , we represent CDRH3s as a string of 11 characters, each character being the code for a different amino acid in an alphabet of cardinality 22. The goal is to find the optimal CDRH3 that minimises the binding energy towards a specific antigen. Binding energies can be computed using state-of-the-art simulation software like Absolut! . We collected datasets of CDRH3 sequences and their respective binding energies (with Absolut!) across various antigens from the protein database bank . We then formed a transfer scenario across antigens where we meta-learn on 109 datasets, validate on 16, and test NAP on 32 new antigens. Our results in Figure 2 indicate that NAP is not limited to hyperparameter tuning tasks but can also outperform all other baselines in combinatorial domains.

**Electronic Design Automation (EDA)** Logic synthesis (LS) is an essential step in the EDA pipeline of the chip design process. At the beginning of LS, we represent the circuit as an AIG (an And-Inverter-Graph representation of Boolean functions) and seek to map it to a netlist of technology-dependent gates (e.g., 6-input logic gates in FPGA mapping). The goal in LS is to find a sequence of graph transformations such that the resulting netlist meets an objective that trades off the number of gates (area) and the size of the longest directed path (delay) in the netlist. We perform a sequence of logic synthesis operators dubbed a synthesis flow to optimise the AIG.

Following , we consider length 20 LS flows and allow an alphabet of 11 such operators, e.g., [refactor, resub,..., balance] as implemented in the open-source ABC library . We collected datasets for 43 different circuits. Each dataset consisted of 500 sequences (collected via a Genetic algorithm optimizer) and their associated area and delay. Additionally, we applied the well-known heuristic sequence resyn2 on each circuit to get a reference area and delay. For this task, the black-box takes a sequence as input and returns the sum of area and delay ratios with respect to the reference ones, as detailed in Appendix B.1. We train all methods on 30 circuits from OpenABC, validate on 4 and test on 9. Our results in Figure 2 again demonstrate that NAP outperforms all other baselines by a significant margin.

## 5 Related Work

Meta-Learning ParadigmsMeta-learning aims to learn how to quickly solve a new task based on the experience gained from solving or observing data from related tasks . To achieve this goal, we can follow two main directions. In the first one, the meta-learner uses source data to learn a good initialisation of its model parameters, such that it is able to make accurate predictions after only few optimisation steps on the new target task . The second one is more ambitious as it endeavours to directly learn, from the related tasks, a general rule to predict a helpful quantity to suggest the next point (black-box value, acquisition function value, etc.) from a context of observations (i.e. our history \(^{(k)}\)). Works following that direction, which include ours, can rely on different types of models to learn this general rule, such as NPs , recurrent neural networks , HyperNetworks  or transformers [12; 11]. Orthogonal to these two directions is the learning of a hyper-posterior on the datasets to meta-train on. In that line of research, Rothfuss et al.  suggest the use of Stein Variational Gradient Descent (SVGD) to estimate uncertainty with multiple models better, and Hsieh et al.  extend the use of SVGD to meta-learn AFs. We consider those last two works as an orthogonal direction to our, as NAP could also benefit from SVGD updates.

Learning a Meta-ModelChen et al.  and TV et al.  train an RNN to predict what should be the next suggestion in BO instead of predicting the acquisition function value. We do not compare to their method given the lack of available implementation. Still, we compare to the outperforming approach developed by Chen et al.  based on a transformer architecture designed to do meta-BO on hyperparameter tuning problems. Their OptFormer is trained on related tasks over different search spaces to output the next point to evaluate and predict its value. The next point is sequentially decoded, one token at a time, and exploits the hyperparameters' names or descriptions to improve generalisation across tasks. Contrary to NAP, OptFormer is not designed to predict acquisition value at any point and does not meet the two properties 3.2 and 3.3, and therefore needs much more training data to predict the proper sequence of tokens. We note that NAP does not rely on variable descriptions, making it easily deployable on various tasks and still very competitive in the hyperparameter optimisation context.

Rather than learning an entire predictive model from scratch, prior works [7; 5; 8; 9] learn deep kernel surrogates, i.e. a warping functions mapping the input space with a neural network before it is given to a GP. Learning only the input transformation allows the authors to rely on the closed-form posterior prediction capacity of standard GP models. To perform the transfer, Feurer et al.  rely on an ensemble of GPs. Iwata  is an approach close to FSBO  as it learns a meta-model by meta-training a Deep Kernel GP. Notably, it does so in an end-to-end fashion using RL to propagate gradients through the acquisition function and the GP back to the deep kernel. It does not, however, learn a meta-acquisition.

Learning a Meta-Acquisition FunctionHsieh et al.  and Volpp et al.  choose to perform transfer through the acquisition function. They use directly GP surrogates and define the acquisition as a neural network that is meta-trained on related source tasks. They first pre-train GP surrogates on all source tasks and fix their kernel parameters. They then rely on RL training to meta-learn the

Figure 2: Average regret vs. BO iterations with 5 initial points. (Left) Results on 6 search spaces on the HPO-B benchmark. (Middle-left) Results tuning SCIP for solving 42 different MIPs. (Middle-right) Antibody CDR3 sequence optimisation on 32 test datasets corresponding to 32 different antigens. (Right) Logic synthesis operator sequence optimisation on 9 test datasets corresponding to 9 different circuits. For each method, error bars show confidence intervals computed across 5 runs on HPO-B and 10 runs on all the others.

neural acquisition function that takes as inputs the posterior mean and variance of the GP surrogate (which is itself trained online at test time). At test time, they allow for update in the GP but keep the weights of the neural acquisition fixed.

In summary, the methods meta-learning AFs do not do so in an end-to-end fashion and still rely on trained GP surrogates. While these methods are principled and competitive, they suffer from the cost of inverting the GP kernel matrix (cubic in the number of observations). In comparison, NAP can make predictions through a simple forward pass. On the other hand, the methods that learn a meta-model, either use a Deep Kernel GP (suffering the same cost) or have to learn a large model from scratch, costing a lot of budgets for collecting data beforehand, as well a pre-training time. Both of them use standard acquisition functions, missing the potential benefits of doing transfer in acquisition between tasks.

The performance of some of those algorithms on HPO-B is presented in Appendix-C showing that despite learning an architecture from scratch, NAP achieves a lower regret. For a more detailed survey on transfer and meta-learning in BO, we refer the reader to Bai et al. .

## 6 Conclusion

We proposed the first end-to-end training protocol to meta-learn acquisition functions in meta-BO. Our method predicts acquisition values directly from a history of points with meta-reinforcement learning and auxiliary losses. We demonstrated new state-of-the-art results compared to popular baselines in a wide range of benchmarks.

**Limitations & Future Work** Our architecture suffers from the usual quadratic complexity of the transformer in terms of the number of tokens which limits the budget of BO steps. Nevertheless, it can still handle around 5000 steps, which is enough for most BO scenarios. Another limitation of our architecture is that we need to train a new model for each search space. In future, we plan to enable our method to leverage meta-training from multiple search spaces and investigate how we could design data-driven BO-specific augmentation to further mitigate meta-overfitting .