# HardMath: A Benchmark Dataset for Challenging Problems in Applied Mathematics

Jingxuan Fan, Sarah Martinson, Erik Y. Wang, Kaylie Hausknecht,

Jonah Brenner, Dancian Liu, Nianli Peng, Corey Wang, Michael P. Brenner

School of Engineering and Applied Sciences

Harvard University

Cambridge, MA 02138, USA

Equal contributionAll data and code used for this paper can be found at: [https://github.com/sarahmart/HARDMath](https://github.com/sarahmart/HARDMath).

###### Abstract

Advanced applied mathematics problems are not well-represented in existing benchmarking datasets used to evaluate Large Language Models (LLMs). To address this, we introduce **HARDMath**, the Harvard Approximate Reasoning Dataset for Mathematics--a dataset of 1,466 difficult problems inspired by Harvard University's graduate course on asymptotic methods. The dataset contains a diverse set of challenging applied mathematics problems with worked solutions that employ various analytical approximation methods. Developing such solutions typically requires multiple modes of analysis--including mathematical reasoning, the use of computational tools, and subjective judgment--making this a challenging problem for LLMs. We establish a framework that auto-generates an arbitrarily large number of 'hard' applied mathematics problems with approximate analytical solutions that include validity checks against numerical ground-truths. We evaluate frontier LLMs on **HARDMath**-**mini**, a sub-sampled test set of 366 problems, as well as on 40 word problems formulated in applied science contexts. Even leading closed-source models like GPT-4 achieve only 43.8% overall accuracy with few-shot Chain-of-Thought prompting, and all models demonstrate significantly lower performance compared to results on existing mathematics benchmark datasets. We additionally conduct a detailed error analysis to gain insights into the failure cases of LLMs. These results demonstrate limitations of current LLM performance on advanced graduate-level asymptotic math problems and underscore the importance of datasets like **HARDMath** to advance mathematical abilities of LLMs.

approximation, asymptotic analysis, benchmark dataset, LLM evaluation,

## 1 Introduction

Many scientific and engineering problems involve mathematical equations, such as integrals, ordinary differential equations (ODEs), and partial differential equations (PDEs), that rarely have closed-form solutions. Traditional mathematics courses and most Large Language Model (LLM) benchmark datasets focus on problems with exact, analytical solutions. However, these benchmarks overlook a large class of math problems often arising in applied sciences that require _approximate_ solutions, which are essential for gaining insights into complex systems. Numerical solutions to such problems can be useful, but they lack the explanatory power offered by approximate analytical methods, e.g. asymptotic and applied analysis.

To address this gap, we introduce **HARDMath**, the Harvard Approximate Reasoning Dataset for Mathematics. This benchmark dataset is designed to evaluate LLMs on their ability to solve applied mathematics problems that require approximation techniques. **HARDMath** contains 1,466 problems inspired by Harvard University's graduate course on asymptotic methods; it covers polynomials, ODEs, and integrals that often arise in real scientific and engineering contexts but that cannot be solved exactly. The dataset emphasizes problems that require advanced mathematical reasoning and approximations, offering a more challenging and diverse testbed for LLMs compared to existing datasets, which mostly focus on simpler, symbolically solvable calculations [1; 2; 3; 4].

Rather than sourcing problems from textbooks or standardized tests, we develop a codebase for automatically generating problems and step-by-step solutions. Our dataset includes a larger set for fine-tuning and two test sets for evaluating LLMs' mathematical reasoning on approximation methods. Here, we evaluate the accuracy of LLMs on our dataset and study their common error modes. We find that current LLMs perform poorly overall on these problems and demonstrate significant room for improvement.

## 2 Related work

### Mathematics Datasets

Most mathematics datasets for evaluating or training LLMs focus on elementary arithmetic or word problems. Notable examples include **MATH** (12,500 high school competition-style problems) , **GSM8K** (8,500 multistep grade-school problems) , and **Odyssey-Math** (387 hand-curated problems across various difficulty levels) . While these datasets are valuable for assessing basic LLM math performance, most are limited in scope and complexity.

Recent efforts targeting more advanced problems are often manually sourced. Datasets like **JEEBench** and a subset of **MathBench** include some college-level topics, such as ODEs and multivariable calculus. **GHOSTS** includes more advanced problems from graduate-level texts on functional analysis, topology, and probability theory , while **ARB** features formal math problems from qualifying exams at Harvard and Berkeley . However, these datasets often (1) are limited in size and scalability, (2) focus on formal mathematics, or (3) cull problems from textbooks protected by copyrights. Notably, none of the existing datasets (Table 1) focus on advanced applied mathematics. **HARDMath** fills this gap by presenting a large corpus of problems that require approximation techniques from asymptotics to be solved. **HARDMath** is also highly scalable with a codebase for data generation. Since these problems cannot be formalized using tools like Lean or solved with symbolic computation software, they present the ideal domain for evaluating how LLMs integrate natural language reasoning and code-based tools to solve out-of-training sample math problems.

### Recent interest in advanced mathematics reasoning

As LLMs continue to improve, there has been growing interest in developing more challenging benchmarks. A notable example is the recent open challenge, _Humanity's Last Exam_, which aims to create the world's most difficult public AI benchmark, requesting questions that "only exceptional

   Dataset & Size & Data Generation & Difficulty \\ 
**MATH** & 12.5K & Manual & High School \\
**MathBench-T** & 632 & Manual, Algorithmic & Undergraduate \\
**JEEBench** & 236 & Manual & High School \\
**GHOSTS** & 190 & Manual & Graduate \\
**ARB** & 34 & Manual & Graduate \\ 
**HARDMath** (Ours) & 1.4K & Algorithmic & Graduate \\   

Table 1: Comparison of **HARDMath** with related datasets. Note that for all datasets excluding **MATH**, we report the number of relevant problems at a comparable difficulty to our dataset (e.g., **Theory-Knowledge-College in MathBench**, and **Grad-Text** and **Holes-in-Proofs** from **GHOSTS.) **HARDMath** is the largest graduate-level dataset.

individuals can answer correctly" and do not involve "straightforward calculation/computation" . Similarly, frontier models have been advancing quickly, and many are explicitly focused on quantitative and scientific reasoning, such as OpenAI's recent o1 series. In line with our motivation for developing **HARDMath** to better track the progress of LLMs, OpenAI argues that "recent frontier models do so well on **MATH** and **GSM8K** that these benchmarks are no longer effective at differentiating models" .

## 3 Datasets

**HARDMath** contains four problem classes with seven distinct problem types covering nondimensionalization, polynomial root-finding, ODEs, and integrals, as well as 40 handwritten word problems designed to place the problems in applied scientific contexts (see Appendix A.1 for problem details). The main dataset (1,060 problems) is suitable for model development, while the **HARDMath-mini** evaluation set (366 problems) is used for benchmarking LLM performance. Fig. 1 shows a breakdown of the datasets by problem type.

Solutions to all **HARDMath** problems share a common reasoning framework; the _Method of Dominant Balance_ simplifies problems by focusing on terms that 'dominate' the solution's behavior and can significantly simplify the equation . Solution methods also involve combining sophisticated computational and analytical techniques, such as self-consistency checks and the use of numerical methods. To solve these problems, subjective decisions about solution regimes to consider, terms to include, and approximation methods must be made with rigorous justification, which is challenging for current LLMs.

Implementation of this reasoning framework is realized through a robust data generation process. The data generation code uses SymPy and SciPy to implement mathematical procedures for generating approximate analytical solutions tailored to each problem class. Problems are generated randomly by combining sets of random coefficients, functional forms, and initial conditions. Solutions are generated algorithmically, with key steps described in explanatory texts. The main results are embedded in the LaTeX  command, following conventions from other mathematics datasets (e.g. **MATH**). Each problem type includes: 1) LaTeX-formatted problem statements, 2) LaTeX-formatted solution steps, 3) accuracy demonstrations comparing analytical and numerical solutions, and 4) metadata descriptors of the problem and solution types (Appendix A.1).

We evaluate solutions by calculating the relative error between analytical and numerical results at selected evaluation points. Problems are included in the dataset only if their solutions are within 10% of the numerical ground-truth, ensuring that all problems in **HARDMath** maintain high accuracy. For polynomial root correction problems, we further check that the corrections improve on the original approximation.

Figure 1: Breakdowns of the **HARDMath-mini** (left) and the **HARDMath** (right) datasets.

## 4 Evaluation

### Model choice and evaluation protocols

We evaluate several leading LLMs on **HARDMath-mini**, a subset of 366 problems representative of **HARDMath** (Fig. 1). Closed-source LLMs evaluated include GPT-3.5 [14; 15; 16], GPT-4  and o1-mini , open-source LLMs include Llama3  and CodeLlama . All models are tested in zero- and few-shot settings with Chain-of-Thought (CoT) prompting, which encourages complex reasoning capabilities by providing intermediate steps in sample answers . Prompts and hyper-parameters are detailed in Appendix A.3.4.

We focus our evaluation on the four key problem types in **HARDMath**: _Nondim_ (symbolic and numerical nondimensionalization), _Roots_ (polynomial root-finding), _ODEs_ (nonlinear ODEs), and _Integrals_ (traditional and Laplace integrals). Models are evaluated for accuracy and common error modes using zero- and few-shot CoT prompting. Prompts contain example question-solution pairs, problem setup and formatting hints (Appendix A.3.1). Following Hendrycks et al. , automatic assessment compares the final model-generated answer (A.3.1) to the true solution (both in **LIFEX\(\{\}\)** environments), using SymPy-based  equivalence checks and numerical evaluations. We also develop a procedural grading system using GPT-4o to (1) provide intermediate step grading for multi-step solutions, and (2) assess models' ability to make approximation judgments, which allow for a range of self-consistent solutions. Rubrics are adapted from grading guidelines of the course that inspired the **HARDMath** problems (Appendix A.3.2 ). Human grading on a subset of LLM responses shows good alignment with GPT-based grading, with average score adjustments summarized in Appendix A.3.3.

### Model performance and error mode analysis

Here, we report the accuracy of models across problem types and prompting settings (Table 2, Appendix A.4, Fig. 3). Few-shot CoT prompting enhances model performance across the board, particularly for o1-mini and GPT-4, which demonstrate the most substantial improvements, consistent with findings from  (Fig. 3a). The performance increase associated with prompting varies by problem-type; gains tend to saturate quickly on more challenging problems such as _ODEs_(Appendix A.4, Fig. 4). Notably, OpenAI's new o1-mini, though with much smaller parameter size, outperforms other models at all tested shot levels, confirming its optimization for STEM reasoning .

o1-mini with 5-shot CoT achieves the highest overall accuracy of 62.3%, while Llama3-8b achieves 20.2%, the highest among open-source models. In contrast, Llama3-8b performs significantly better on existing datasets, achieving 30.0% on **MATH** (4-shot CoT) and 79.6% on **GSM-8K** (8-shot CoT)), compared to its 20.2% on **HARDMath-mini**. GPT-4 also shows strong performance on **MATH** (72.2%, 0-shot CoT), **GSM-8K** (92.0%, 5-shot CoT) [22; 17], and a recently released advanced mathematical dataset **miniHOSTS** (average score of 4.15 out of 5). Yet, GPT-4 achieves only 43.8% on **HARDMath-mini**. Similarly, o1-mini demonstrates 90.0% accuracy on **MATH-500** with 0-shot CoT , but achieves only 62.3% accuracy on **HARDMath-mini** with 5-shot CoT. This reveals a significant performance increase compared to other models on some (e.g. _Nondim_) but not all problem types. This suggests that the **HARDMath** benchmark presents problems that remain challenging and unfamiliar to even the most performant LLMs developed for advanced STEM reasoning.

We also evaluate model responses across varying levels of correctness, allowing us to identify common error patterns. When breaking down performance by correct, partially correct, and incorrect responses, we observe that few-shot prompting improves performance to different degrees across problem types (Fig. 2). LLM solutions to harder problems, like _ODEs_ and _Integrals_, are rarely fully correct, but receive more partial credit with increasing CoT shots. In contrast, for simpler problems like _Roots_, advanced models (o1-mini and GPT-4) get more fully correct responses with increasing CoT shots (Fig. 2, 5). Fig. 6 compares GPT-4's responses at 0 vs. 5 shot CoT on _Roots_, showing that the most common error mode--incorrectly setting up dominant balances--gets significantly reduced. Instead, errors shift to more nuanced issues, such as missing cases or failing to calculate complex roots (examples in Appendix A.4.2). This indicates that CoT improves the model's application of dominant balance techniques, enabling it to overcome simple mistakes.

Finally, to assess how well LLMs can solve these problems when situated in realistic research contexts, we evaluate GPT-4 (the best performing stable model) on a set of word problems covering all problem types (Appendix A.2). This yields an overall accuracy of 28.1%. Overall, this analysis highlights the value of **HARDMath** as a challenging benchmark for evaluating mathematical capabilities of LLMs on advanced approximate analytical mathematics.

## 5 Conclusion

We introduce **HARDMath**, a new dataset covering several problem types from an advanced asymptotics course that can be used to benchmark LLMs' mathematical capabilities and perform model developments. This dataset consists of 1060 examples, and we additionally include 366 verified examples in **HARDMath-mini** and 40 verified 'problems in context' that we use to evaluate various leading LLMs. **HARDMath** is unique as there do not exist large-scale mathematical datasets covering problems of similar difficulty from applied mathematics, and because **HARDMath**'s problems and solutions are algorithmically generated, meaning that one could produce datasets of arbitrary size using our framework.

Our evaluation highlights that while few-shot CoT prompting significantly improves model performance, especially for models like o1-mini and GPT-4, the overall accuracy on **HARDMath-mini** problems remains much lower compared to other existing benchmarks. This suggests that our dataset poses unique and challenging tasks that go beyond the boundaries of current LLM capabilities, particularly in approximation-oriented mathematical reasoning.

Future work will fine-tune LLMs on **HARDMath** to improve performance. Additionally, while we have evaluated several frontier models, we plan to extend our evaluations to more LLMs as they become available. This expanded evaluation should provide more detailed insights into performance disparities across different models, further advancing our understanding of LLMs' capabilities in handling complex asymptotic reasoning.

   Model & ALL & Nondim & Roots & ODEs & Integrals \\ 
**Closed-source models** & & & & & \\ GPT-3.5 (0 shot) & 6.04 & 5.05 & 17.2 & 1.39 & 3.33 \\ GPT-3.5 (1 shot CoT) & 14.2 & 6.11 & 29.3 & 6.94 & 18.2 \\ GPT-3.5 (5 shot CoT) & 24.6 & 24.3 & 35.0 & 16.2 & 23.1 \\ GPT-4 (0 shot) & 14.0 & 6.04 & 33.7 & 7.87 & 14.9 \\ GPT-4 (1 shot CoT) & 37.6 & 36.5 & 52.8 & 15.9 & 40.5 \\ GPT-4 (5 shot CoT) & 43.8 & 48.6 & 57.3 & 21.7 & 41.4 \\ o1-mini (0 shot CoT) & 29.8 & 38.1 & 24.3 & 10.2 & 32.5 \\ o1-mini (5 shot CoT) & 62.3 & 84.5 & 62.1 & 30.6 & 46.5 \\
**Open-source models** & & & & & \\ Llama3-8b (0 shot) & 3.67 & 0.50 & 11.5 & 4.63 & 2.52 \\ Llama3-8b (5 shot CoT) & 20.2 & 17.9 & 17.1 & 12.0 & 28.1 \\ CodeLlama-13b (0 shot) & 1.94 & 0.00 & 8.73 & 1.85 & 0.50 \\ CodeLlama-13b (5 shot CoT) & 9.79 & 8.41 & 13.1 & 9.7 & 9.57 \\   

Table 2: Evaluation Accuracy (percentage) on the **HARDMath** evaluation set.

Figure 2: Breakdown of model accuracy percentages for o1-mini, GPT-4 and Llama3 by prompting types and problem types.