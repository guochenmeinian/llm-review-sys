# When are dynamical systems learned from time series data statistically accurate?

Jeongjin (Jayjay) Park

School of Computational Science and Engineering

Georgia Institute of Technology

Atlanta, GA 30332

jpark3141@gatech.edu

&Nicole Tianjiao Yang

Department of Mathematics

Emory University

Atlanta, GA 30322

tianjiao.yang@emory.edu

&Nisha Chandramoorthy

Department of Statistics

The University of Chicago

Chicago, IL 60637

nishac@uchicago.edu

Corresponding author

###### Abstract

Conventional notions of generalization often fail to describe the ability of learned models to capture meaningful information from dynamical data. A neural network that learns complex dynamics with a small test error may still fail to reproduce its _physical_ behavior, including associated statistical moments and Lyapunov exponents. To address this gap, we propose an ergodic theoretic approach to generalization of complex dynamical models learned from time series data. Our main contribution is to define and analyze generalization of a broad suite of neural representations of classes of ergodic systems, including chaotic systems, in a way that captures emulating underlying invariant, physical measures. Our results provide theoretical justification for why regression methods for generators of dynamical systems (Neural ODEs) fail to generalize, and why their statistical accuracy improves upon adding Jacobian information during training. We verify our results on a number of ergodic chaotic systems and neural network parameterizations, including MLPs, ResNets, Fourier Neural layers, and RNNs.

## 1 Introduction

Learning a dynamical system from time series data is a pervasive challenge across scientific domains. Such data come from expensive experiments and high-fidelity numerical models that simulate the underlying nonlinear, often chaotic, processes. The learning challenge is to train on available data to produce output models that i) provably preserve known symmetries and invariances (e.g., conservation principles); and ii) are inexpensive surrogates for use in downstream computations such as optimization and uncertainty quantification. The field of physics-guided machine learning [vdGSB\({}^{+}\)20, FO22, LK22, KKLL21, RPK19, KKL\({}^{+}\)21] has emerged in response, rapidly integrating neural networks into data-driven modeling and prediction workflows for a wide variety of complex dynamics, from geophysical fluid flows to phase transitions in materials (see [KMA\({}^{+}\)21, CCC\({}^{+}\)19] for surveys). Yet rigorous generalization analyses of neural parameterizations in these applications, wherein the underlying dynamics can exhibit chaotic behavior, have been underexplored.

Here we investigate data-driven neural parameterizations of chaotic ODEs/PDEs and maps (discrete-time dynamical systems) motivated by the following observation: a neural representation that is learned well, i.e., with a small generalization error, can still produce _unphysical_ long-term or ensemble behavior. Figure 1 (left) shows the classical Lorenz '63 chaotic attractor  plotted using a long random trajectory/_orbit_ (time integration of the Lorenz equation vector field starting from a random initial condition, which is indicated by a '+' sign). The second column shows an orbit from a neural network model, 'MSE_MLP', which minimizes the mean-squared error in the represented vector field at 10,000 training points and shows high accuracy (\(<5\%\) average relative error) over 8000 test points on the attractor. Surprisingly, Figure 1(column 2) shows that the learned 'MSE_MLP' Neural ODE  model produces an atypical orbit - an orbit different from almost every orbit of the true system - for the same randomly chosen initial condition. As a result, the learned empirical distribution is not close to the physical distribution - that of almost every true orbit, as shown in Figure 1 (column 4). On the other hand, the 'JAC_MLP' model, which is obtained by minimizing the mean-squared error in the vector field and its first derivative (Jacobian matrix), reproduces the Lorenz '63 attractor and the physical distribution on the attractor (Figure 1, column 4). The 'JAC_MLP' also captures all the Lyapunov exponents (LEs) - measures of asymptotic stability to perturbations, which are invariants in ergodic systems - accurately, while the 'MSE_MLP' only obtains the leading LE accurately.

Naturally, we ask about the wider applicability of these observations. For any ground truth dynamical system, how can we quantify the probability of success, including obtaining sample complexity results, of learning its _physical_ or typical behavior? That is, how do we redefine and extend generalization analyses to neural representations of complex dynamical systems? To answer these questions, we start with a deceivingly simple supervised learning setup: given \(m\) samples from a time series, \(\{(x_{t},x_{t+1})\}_{t}\), \(0 t(m-1)\), how can we learn a model, \(F_{}\), such that, i) \(x_{t+1} F_{}(x_{t})\), for all time \(t\), and ii) the underlying distribution of the states \(x_{t}\) and other dynamical invariants such as Lyapunov exponents are reproduced by orbits of \(F_{}\)? It is widely accepted that learning such an \(F_{}\) involves matching orbits of \(F_{}\) with \(x_{t}\) over large \(t\) during training. However, small errors propagate over orbits, by definition, in a chaotic system, leading to training instabilities. In response, a vast literature has been dedicated to developing sophisticated training models based on RNNs , operator learning  and regularizations .

We focus instead on the empirical risk minimization (ERM) for \(F_{}\) that does not explicitly use the temporal correlations/dynamical structure in the data, avoiding training instabilities. Thus, we attempt to characterize when an elementary regression problem \(F_{}\) can still lead to learning a physical representation, leading to a practical theory of learning chaotic systems from data. Our specific contributions are as follows:

* Motivated by extensive empirical results, we develop useful notions of generalization that characterize a model's ability to reproduce dynamical invariants.
* We develop new dynamics-aware generalization bounds for minimization of errors in the \(C^{r}\), \(r=0,1\), topology.
* We leverage shadowing theory from dynamical systems to rigorously characterize failure modes in learning statistically accurate models.

Figure 1: A random orbit on the x-z plane obtained from RK4 integration of the Lorenz vector field ( (first column), Neural ODE, ‘MSE_MLP’, trained with mean-squared loss (second column) and Neural ODE, ‘JAC_MLP’, trained with Jacobian loss (third column). The last column shows the empirical PDF of the orbit generated by the true (gray), ‘MSE_MLP’ (red) and ‘JAC_MLP’ (blue) models. Experimental settings and additional results are in Appendices B and C respectively. **Gist:** A model trained well with MSE can produce atypical orbits but when Jacobian information is added to the training, it reproduces the long-term/statistical behavior accurately.

Generalization of parameterized ergodic dynamics

In this section, we motivate, through illustrative examples, the need for a dynamics-aware definition of generalization in the context of learning from time series data. We introduce concepts from dynamical systems as needed for a self-contained presentation.

**Dynamics.**: A _map_, or a discrete-time dynamical system, \(F,\) is a function on a closed and bounded (compact) set, \(M^{d},\) which is called the state/phase space of the dynamics. We exclude scenarios where the dynamics can be unbounded, and focus on settings where \(F C^{1}(M)\) is a differentiable function on \(M.\) We denote by \(F^{t},\)\(t^{+},\) the iterates of the dynamics, or the compositions of \(F\) with itself \(t\) times, i.e., \(F^{t}=F F^{t-1}.\) An _orbit_ or trajectory of \(F^{t}\) starting at an initial state \(x M\) is the sequence \(\{F^{t}(x)\}_{t^{+}}.\) In practice, \(F\) may be a numerical ODE solver that approximates the continuous-time solutions, \(^{t},t^{+},\) of the ODE (written in dynamical systems notation2 ): \(d^{t}(x)/dt=v(^{t}(x)),\) where \(v:M TM\) is the true vector field describing the governing equations. Fixing some \(^{+},\)\(F:=^{}.\) We say a map \(F\) is chaotic if there exists a subbundle of \(TM,\) called the unstable subbundle, where infinitesimal perturbations grow exponentially under the linearization (Jacobian map), \(dF^{t},\) of the dynamics. The true map \(F\) generates a deterministic, autonomous system (see Appendix A).

**Physical measures.** A probability measure \(:M^{+}\) is a _physical_ measure  for the dynamics \(F\) if it is a) \(F\)-invariant, b) ergodic and c) _observable_ through \(F\). A measure \(\) is observable if time-averages along any orbit starting from a randomly chosen initial point converge to constants that are expectations (phase space average) with respect to \(.\) That is, for any \(f(M),\)\((1/T)_{t T}f(F^{t}(x))f(x),\) for any initial point \(x\) chosen Lebesgue a.e. on a set \(U M.\) The orbits starting almost everywhere on the basin of attraction, \(U,\) asymptotically enter a set, \(,\) called the attractor. In dissipative chaotic systems, the attractor, \(,\) which is the compact support of the physical measure \(,\) has Lebesgue measure 0. Consequently, \(\) may not have a probability density, that is, \(\) is not absolutely continuous, or is singular, with respect to Lebesgue measure.

**Neural ODE.** Introduced in , a Neural ODE, denoted here by, \(v_{}:M^{d},\) with parameters, \(,\) is a vector field represented by a neural network. The vector field can be time integrated to obtain solutions \(^{t}_{}:^{d}^{d},t^{+}\) to the ODE, \(d^{t}_{}(x)/dt=v_{}(^{t}_{}(x)).\) As noted in the introduction, suppose we have \(n\) distinct pairs \(S=\{(x_{i},F(x_{i}))\}_{i[n]},\) which could come from a single orbit of \(F,\) as our training data. We train the Neural ODE by solving an ERM for the loss, \(,\) that can be chosen to be a square loss, e.g., \((x,^{}_{})=\|^{}_{}(x)-F(x)\|^{2}.\) That is, we solve for \(\) that minimizes the training loss, \((1/n)_{x S}(x,).\) Note that the true ODE or vector field is not explicitly used in training, only the solution map at some time intervals, \(F.\) We refer to the map, \(x^{}(x),\) as \(F_{},\) or neural representation of the target map, \(F.\)

**Data and optimization.** Suppose we use an \(m\)-length orbit as our training data, i.e., \(x_{i+1}=F(x_{i}),\) then, the data are _not_, strictly speaking, independent. However, a feature of chaotic systems is an exponential decay of correlations, and so training data of the form, \(\{(x_{i},F^{}(x_{i}))\}\) starting from an initial condition \(x_{0},\) can be thought of as iid, for a large enough \(.\) In that case, the learned map \(F_{}\) represents the function \(F^{}.\) During optimization, infinitesimal linear perturbations, will have to be evolved for time \(.\) Since adjoint solutions blow up exponentially, a longer \(\) will lead to training instabilities. To avoid numerical difficulties in training and focus on issues surrounding generalization, we choose \(:= t,\) a small time step, to define the target map \(F\) and learn a neural network representation of this function. When viewed this way, the generalization of Neural ODEs can be analyzed through the conventional lens of supervised learning with the loss,

\[(x,F_{})=\|F_{}(x)-F(x)\|^{2}, \]

where \(F_{}:=^{ t}\) is a neural network representing the map. For a map \(h:M M,\) we define the training and generalization errors in the usual way:

\[_{S}(h)=(1/m)_{i=1}^{m}(x_{i},h), R(h)=(x,h), \]where the expectation is over the data distribution, which may be \(\) or any initial probability distribution of the states.

**Example.** To illustrate our conceptual findings surrounding generalization, we use the canonical Lorenz '63 system, which is a 3-variable reduced order model of atmospheric convection . The ODE can be written as \(d^{}(x)/dt=v(^{}(x))\), where the vector field \(v\) is given by \(v(x)=[(-),(-)-, -]^{}\), and \(x=[,\,,\,]^{}\) are the coordinate functions of the state \(x\). We use the standard values of the parameters \(=10,=28,=8/3\), at which the solutions are chaotic. We use the Runge-Kutta 4-stage time integrator with a time step size of \(0.01\) to define the map \(F\). That is, \(F(x)\) is the solution \(^{0.01}(x)\) approximated by an RK4 time-integrator. Our Neural ODE map, \(F_{}\), is learned to approximate \(F\) by solving the above optimization with \(n=10,000\) training points along an orbit. We illustrate our numerical results on various Neural ODE models and architectures that have fully connected layers, ResNet blocks with convolutional layers, and Fourier neural operators . Many such models learn accurate representations of the true vector field \(v\), as evidenced by small training and test errors (sample average approximation of the generalization error in (2) over 8,000 points from the data distribution). These are shown in Figure 3 (Appendix C), while other hyperparameter and optimization settings are in Appendix B.

**Statistical measures and Lyapunov exponents.** Our numerical results test the accuracy of models beyond generalization error as defined in (2). In particular, we compute time-averages using the Neural ODE models and compare against expectations with respect to \(\) obtained from the true equations. For the Lorenz system, we note that time averages obtained from the models with small generalization errors can be inaccurate. That is, even if the errors in the vector field are small (see also Figure 4 in Appendix C), the time-averages can match poorly. This is described for the best performing Neural ODE model in Table 3. The discrepancy in the learned distribution is shown in terms of Wasserstein distance computed using empirical distributions on long orbits (of length 50,000). We find that the Neural ODE model does not learn the ground truth statistics even if the training data include transient dynamics off the attractor.

Lyapunov exponents, roughly speaking, measure the asymptotic exponential growth/decay of infinitesimal perturbations under the Jacobian map \(dF\). In an ergodic system, they are independent of the initial state and can be written as an expectation with respect to \(\). In this work, Lyapunov exponents are yet another dynamical invariant (statistical quantity) in ergodic systems that we use to evaluate the statistical fidelity of learned models. In a chaotic system, there is at least one positive Lyapunov exponent, and the number of positive Lyapunov exponents is the dimension of the unstable manifold. The Lorenz system has one positive Lyapunov exponent (LE), one zero LE (corresponding to a center direction, or the vector field \(v\) itself) and one negative LE. The ground truth map \(F,\) which is a time \( t\) approximation of the flow \(^{t}\) has a two-dimensional center-unstable manifold and a one-dimensional stable manifold.

In Figure 2, we plot the Lyapunov exponents obtained using a classical QR iteration-based algorithm . The Neural ODE model, marked 'MSE', produces a reasonably close approximation of the ground truth value (\( 0.9\)) for the positive LE but obtains an incorrect approximation of the stable LE (true value \(-14.5\)). We note also that standard deviations in the LE values computed over different orbits is quite large, indicating some orbits with atypical behaviors. We show the \(_{2}\) error in the computed LEs by the Neural ODE model in Table 3, and full details in Table 5.

Figure 2: Learned and true LEs computed over 30,000 time steps using the QR algorithm of Ginelli et al  starting from 10,000 random initial states. The “MSE” and “JAC” labels indicate computations using the Neural ODE models trained with the loss functions in (1) and (3) respectively.

**Jacobian-matching.** We now consider Neural ODE models trained with the following loss function.

\[_{}(x,F_{})=\|F_{}(x)-F(x)\|^{2}+\|dF_{ }(x)-dF(x)\|^{2}, \]

where \(F_{}:M M\) is a Neural ODE map, \(dF_{}(x):T_{x}M T_{x}M\) its (\(d d\)) Jacobian matrix at \(x\), and \(>0\) is a hyperparameter that scales the relative importance of the two terms. We train in the usual way, by solving an ERM problem to minimize the training error, \(_{S,}\), which is defined as before, but with the new Jacobian-matching loss:

\[_{S,}(h)=(1/m)_{i=1}^{m}_{}(x_{i},h), R_{ }(h)=_{}(x,h). \]

We train Neural ODE models with similar architectures as above, and find best performing models in terms of the test error (approximation of the generalization error) in (4). As in the training with the loss function \(\), we find several accurate neural representations of the Lorenz map (see training and test loss plots in Appendix C). Their performance on statistical measures and Lyapunov exponent predictions is remarkably different, however. In Figure 1, a Neural ODE trained with the Jacobian-matching loss produces an attractor (column 3) that is visually similar to the Lorenz attractor. The empirical distributions match closely with the ground truth, as shown in column 5 of Figure 1 and in Table 3, where the model is marked with a 'JAC', while one trained with the loss \(\) in (1) is indicated with an 'MSE'. The LEs and statistical averages of the state are all accurately represented, although the model is not explicitly designed to learn temporal patterns in the data. The MSE models learn vector fields with comparable accuracy with the Jacobian-matching models. That is, they learn accurate representations of \(F\) with high probability (over the data distribution), but failing to learn \(dF\) accurately leads to statistical inaccuracy. Due to the presence of atypical orbits, the attractor and the physical measure are not reproduced. Even though our formulation of Neural ODEs is simply stated as an ERM for regression, the generalization errors \(R(F_{})\) and \(R_{}(F_{})\) cannot determine whether \(F_{}\) is a physical representation of the dynamics.

## 3 When generalization implies statistical accuracy

In the previous section, we observed that adding information about the Jacobian in the training process led to statistically-accurate learning. Does this observation apply more broadly to other ergodic systems? How can one further improve generalization? In this section, we provide answers to these questions by proving dynamics-aware generalization bounds for Neural ODEs.

Our ultimate goal is to minimize a statistical loss function rather than (1) or (3) since this measures how accurately a model \(h\) can reproduce ergodic averages associated with \(F\). For instance, \(^{}(x,h)=_{f_{1}}|[f(x)]-_{T }(1/T)_{t T}f(h^{t}(x))|\). This is the Wasserstein (\(W^{1}\)) distance (expressed in its dual form) between the ergodic measure \(\) and the ergodic measure associated with the orbit of \(h\), a learned model, starting at \(x\), \(_{T}\{x,h(x),h^{2}(x),,h^{T}(x)\}\). As noted, ERMs for this loss function do not have a straightforward implementation when the map \(h\) sought is chaotic. Hence, we seek conditions under which solving ERMs defined with losses (1) and (3) can still minimize \(^{}\). To understand when solving regression problems for \(F_{}\) lead to statistically accurate physical representations, we first assume that a notion of "shadowing" applies to \(F\).

    & & & Norm Difference & \\  Model & Loss & \(W^{1}(_{500},_{,500})\) & \(\|-_{}\|\) & \(\| x_{500}- x_{500,}\|\) \\  MLP & MSE & 18.9711 & 9.6950 & 15.2220 \\ MLP & JAC & 0.6800 & 0.0118 & 0.6524 \\ ResNet & MSE & 1.3567 & 10.8516 & 0.7760 \\ ResNet & JAC & **0.1433** & **0.0106** & **0.0559** \\ FNO & MSE & 10.5409 & 22.1600 & 9.4270 \\ FNO & JAC & 1.3076 & 0.0505 & 0.9748 \\   

Table 1: True vs. learned Lorenz system: comparison of statistics. (\(W^{1}\): Wasserstein-\(1\) Distance, \(\): set of LEs, \(_{T}\): empirical distribution of an orbit \(T\). The subscript \(\) indicates quantities computed using NN models trained with different loss functions (MSE (1), JAC (3)).

**Definition 1**.: _We say that the shadowing property applies to a map \(F\) if for any \(>0,\) there exists an \(=()\) so that for any map \(G\) with \(\|G-F\|_{1}:=_{x M}(\|G(x)-F(x)\|+\|dG(x)-dF(x)\|),\) there exists a map \(:M M\) close to the identity such that \(\|G^{t}((x))-F^{t}(x)\|\) for all \(t.\)_

Intuitively, the shadowing property means that an orbit of a nearby dynamical system can closely follow a true orbit - called a shadowing orbit - of \(F\) for all time. This kind of uniform-in-time shadowing is a classical result for a mathematically ideal class of chaotic systems, called uniformly hyperbolic systems (see Katok and Hasselblatt  Ch 18; Appendix A). For a textbook presentation and extension of shadowing for dynamical systems with some hyperbolicity, see . For a uniformly hyperbolic \(F\) (see Appendix A), we now assume that a neural representation \(F_{}\) of \(F\) trained with \(n\) samples generalizes well in terms of \(C^{1}\)-distance. That is, an ERM solution for the loss 3 ('JAC' models in sections 1 and 2) generalizes so that \(R_{}(F_{})\) is small.

**Definition 2** (\(C^{1}\) generalization).: _Given \(>0,\) there exist \(_{0},_{1}>0\) and a function \((,_{0},_{1})(,_{0 },_{1})\) such that \(_{x}\|F(x)-F_{}(x)\|_{0}\) and \(_{x}\|dF(x)-dF_{}(x)\|_{1}\) for all \(m\) with probability \( 1-\) over the randomness of the training data from \(^{m}.\)_

We now make an optimistic assumption on a learned model, \(F_{},\) that satisfies the above definition of \(C^{1}\) generalization. Using Hoeffding's inequality, we know that for any \(_{0}>0,\) with probability at least \(1-_{0}\) over the randomness of \(x,\)\(\|F(x)-F_{}(x)\|_{0}+(_{x M}\|F(x)-F_{}(x)\|))}\) and \(\|dF(x)-dF_{}(x)\|_{1}+(_{x M}\|dF(x)-dF_{ }(x)\|))}.\) Given \(>0,\) let \(_{0}:=2_{0}\) and \(_{1}:=2_{1}.\) Fixing \(_{0}>0,\) suppose that the trained model \(F_{}\) is such that \((_{x M}\|F(x)-F_{}(x)\|)<_{0}/(2)})\) and \((_{x M}\|dF(x)-dF_{}(x)\|)_{1}/(2)}).\) Taking a union bound, with probability \(>1-(+_{0}),\)\(\|F(x)-F_{}(x)\|_{0}\) and \(\|dF(x)-dF_{}(x)\|_{1}.\) We enhance this inequality to obtain a stronger assumption on \(F_{}.\)

**Assumption 1** (\(C^{1}\) strong generalization).: _Given \(>0,\) there exist \(_{0},_{1}>0\) and a function \((_{0},_{1},m) n(_{0},_{1},m) \) such that \(\|F(F_{}^{t}(x))-F_{}^{t+1}(x)\|_{0}\) and \(\|dF(F_{}^{t}(x))-dF_{}(F_{}^{t}(x))\| _{1}\) for all \(t n,\)\(m(,),\) and \(n\) as \(m,\) with probability \( 1-\) over the initial state \(x.\)_

That is, we assume that, with high probability, the trained model makes a small error at each time. This stronger notion of generalization can be satisfied when the true model shows a smooth linear response in its statistics , or in practice, training is performed with points sampled at random near the attractor, as opposed to with a spin-off time to achieve a state on the attractor. Given a tuple, \((_{0},_{1}),\) an orbit with initial condition \(x\) that satisfies, \(\|F(F_{}^{t}(x))-F_{}^{t+1}(x)\|_{0}\) and \(\|dF(F_{}^{t}(x))-dF_{}(F_{}^{t}(x))\| _{1}\) for all \(t m\) will be referred to as an \((_{0},_{1})\) orbit. That is, at each time, the neural representation \(F_{}\) is \((_{0},_{1})\)-close to the true map, \(F.\) Under this assumption, we can follow the proof of the Shadowing lemma (see e.g., Ch 18 of ) for hyperbolic maps to show that a true orbit (of \(F\)) shadows every \((_{0},_{1})\) orbit.

**Proposition 1** (Shadowing).: _Let \(F_{}\) be an approximation of \(F\) that satisfies the \(C^{1}\) strong generalization (Assumption 1). Given any \(>0,\) there exist \(_{0},_{1},n\) such that every \((_{0},_{1})\) orbit is \(\)-shadowed by an orbit of \(F.\) That is, there is a true orbit, say, \(\{F^{t}(x)\},\) corresponding to every orbit, \(\{G^{t}(x^{})\}\) such that \(\|F^{t}(x)-G^{t}(x^{})\|,\) for all \(t n.\)_

See section A.1 for the proof. Let \(\{x_{t}^{}\}_{t n}\) be an \(n\)-length orbit of \(F_{},\) i.e., \(x_{t+1}^{}=F_{}(x_{t}^{}).\) We use \(T_{n,}M\) to denote the direct sum \(_{i=1}^{n}T_{x_{t}^{}}M\) of tangent spaces along the orbit, \(x_{t}^{}.\) The proof follows Theorem 18.1.3 of  to apply contraction mapping on a compact ball in \(T_{n,}M.\)

The above result defines, for each \((_{0},_{1})\)-orbit, \(\{x_{t}^{}\}_{t},\) a shadowing orbit, \(x^{}:=\{F(x_{t}^{}+v_{t})\}_{t},\) where \(v=_{t}v_{t}\) is the fixed point of the contraction map in the proof (section A.1). Let \(_{n}^{}(x_{0}^{})\) be the empirical measure defined on the shadowing orbit corresponding to an \((_{0},_{1})\)-orbit, \(\{x_{t}^{}\}_{t},\) i.e., \(_{n}^{}(x_{0}^{})=\{x_{0}^{},,x_{t}^{},,x_{n-1}^{}\}.\) A shadowing orbit is indeed an orbit of the true map \(F,\) but, unexpectedly, it may be atypical for the physical measure, \(.\) That is, for an atypical shadowing orbit, the time average, \((1/n)_{t n}f(x_{t}^{})\) does not converge to the expected value \(_{x}f(x),\) as \(n.\) This means that the Wasserstein distance, \(W^{1}(_{n}^{}(x_{0}^{}),),\) does not converge to 0 as \(n.\)

Given an initial condition, \(x_{0}^{},\) of an \((_{0},_{1})\)-orbit, the corresponding shadowing orbit may be typical with some probability (over the distribution of \(x_{0}^{}\)), and this probability of finding typical shadowing orbits is a property of the true dynamics, \(F.\) When \(,\) the physical measure of \(F,\) is highlysensitive to perturbations of \(F,\) (see  for surveys on _linear response theory_, the study of perturbations of statistics) this probability may be small. Although the connection between the sensitivity of statistics and the atypicality of shadowing orbits is not completely known, this can justify the differences in the statistical accuracy of neural parameterizations with good \(C^{1}\) generalization (see Definition 2). A neural model \(F_{}\) which generalizes well in the \(C^{1}\) sense (Definition 2) can be thought of as a \(C^{1}\)-smooth perturbation of the true dynamics \(F.\) If smooth perturbations of \(F\) can cause a large change in \(,\) then even when the training size \(m,\) and the orbit length \(n,\)\(W^{1}(_{n}^{}(x_{0}^{}),),\) may not converge to zero, resulting in a model that does not preserve the _physical_ behavior of \(F.\) On the other hand, for typical shadowing, this possibility is excluded and gives us a characterization of statistically accurate learning.

**Theorem 1** (Statistically accurate learning).: _Let \(F_{}\) be a model of \(F\) that satisfies \(C^{1}\) strong generalization. In addition, let \(F_{}\) and \(F\) be such that for any \(>0,\) there exists an \(_{2}>0\) so that \(_{n}W^{1}(_{n}^{}(x),)_{2}\) with probability (over the randomness of \(x\)) \( 1-.\) Then, for any \(>0,\) there exists an \(>0\) such that \(_{n}W^{1}(\{x,F_{}(x),,F_ {}^{t}(x),,F_{}^{n}(x)\},)\) with probability \( 1-.\)_

_Proof_: Let \(_{n}^{}(x):=\{x,F_{}(x),,F_{ }^{t}(x),,F_{}^{n-1}(x)\}\) be the empirical measure of an \(n\)-length orbit of \(F_{}\) starting at \(x.\) Given a \(>0,\) we choose \((_{0},_{1})\) so that \(C^{1}\) strong generalization is satisfied with probability \( 1-/2.\) Thus, an initial condition \(x\) is such that \(\{F_{}^{t}(x)\}_{t n}\) is an \((_{0},_{1})\) orbit, where the tuple \((_{0},_{1})\) is as defined in Proposition 1, with probability \( 1-/2.\) Applying Proposition 1, we have, for any 1-Lipschitz function \(f:M,\)\((1/n)_{t n}|f(F_{}^{t}(x))-f(F^{t}(x))|(1/n)_{t n} \|F_{}^{t}(x))-F^{t}(x)\|/2.\) Taking a supremum over \(f,\)\(W^{1}(_{n}^{}(x),_{n}^{}(x))/2,\) with probability \( 1-/2.\) By assumption, there exists some \(_{2}>0\) such that \(_{n}W^{1}(,_{n}^{}(x))<_{2}\) with probability \(1-/2.\) Hence, \(_{n}W^{1}(,_{n}^{}(x))_{n }W^{1}(_{n}^{}(x),_{n}^{}(x))+W^{ 1}(,_{n}^{}(x))_{2}+/2:=,\) with probability \(>1-,\) using triangle inequality and taking union bound.

This result explains why training to minimize Jacobian-matching loss (3) can lead to statistically accurate models, even though, long-time temporal patterns in the data are _not_ learned explicitly by regression for the one-time map \(F.\) Since \(C^{0}\) generalization (i.e., small errors in(2)) is insufficient for learning shadowing orbits, and thus for Proposition 1 and Theorem 1 to hold, the models trained on MSE loss 1 are not expected to learn ergodic/statistical averages with respect to \(.\)

When shadowing orbits are atypical with high probability, we observe numerically that \(C^{1}\) generalization, i.e., training with Jacobian-matching loss, still does not produce statistically accurate dynamics, in line with the above theorem. For instance, for maps with atypical shadowing described in , we find that learned neural representations with Jacobian-matching do have good \(C^{1}\) generalization (Figure 6), but do not exhibit good statistical accuracy and learn incorrect Lyapunov exponents (see Table 5, Plucked Tent map).

## 4 Dynamic generative models

So far, we have focused on understanding the statistical accuracy of supervised learning of dynamical systems. Without any minimization of distances on the space of probability measures, we proved sufficient conditions under which regression with Jacobian-matching information can yield samples from \(\) with high probability. A generative method is an unsupervised learning technique to train on samples from a target distribution to produce more samples (provably) from the target. Naturally, we can use several popular generative models for our target physical measure here, including score-based methods , Variational Autoencoders (VAE)  or normalizing flows . However, these methods neglect the dynamical relationships in the input samples. In other words, from a vanilla generative model of a physical measure, we cannot also recover the true dynamics. Thus, we focus here on Latent SDEs models from , which combine neural representations of dynamics with generative models. We reinterpret them as dynamic generative models, and analyze their ability to faithfully represent both the underlying dynamics as well as the physical measure.

In a dynamic generative model, we approximate \(F^{t}\) with a stochastic map, that can written as, \(F_{}:=f_{}_{}^{t} g_{},\) where the subscript ls stands for "latent SDE". Here, the function \(g_{}:^{d}^{d_{l}},\) with learnable parameters, \(,\) is a (possibly stochastic) embedding from the data to latent space (\(^{d_{t}}\)), such that, \(g_{}=q_{,0}\). Recall the pushforward notation (\(\)), i.e., if \(x\), then, \(g_{}(x) q_{,0}\). The dynamical system \(^{t}_{}:^{d_{t}}^{d_{t}}\) acts on the latent space, and defines a sequence of pushforward distributions \(^{t}_{}q_{,0}=q_{,t}\). A special case of this setup is a "latent ODE", where \(^{t}_{}\) is a deterministic map instead. With a stochastic latent SDE model instead, we observe, in line with , that the multimodal distribution of the Lorenz '63 attractor is reproduced better. That is, \(^{t}_{}\) is a solution map of a Neural SDE: \(d^{t}_{}(z)=w_{}(t,^{t}_{}(z))dt+_{}(t,^{t}_ {}(z)) dW_{t}\). Here the drift term, \(w_{}\), and the diffusion term, \(_{}\), are represented as neural networks. The decoder \(f_{}:^{d_{t}}^{d}\) is a deterministic map that defines the conditional, \(f_{}q_{,t}=p_{}(|Z_{t})\). This dynamic VAE approach has been found to be expressive for chaotic systems (see Chapters 3 and 5 of , ), wherein the conditional distribution, \(z q_{,0}(z|X_{1:m})\), of \(Z_{0}\) given \(X_{1:m}:=\{F^{t}(x)\}_{t m}\) is modeled as a Gaussian distribution whose learnable parameters are also denoted by \(\). Similarly, the conditional \(p_{}(|Z_{t})\) is again modeled as a Gaussian with parameters \(\). The parameters, \(\) and \(\) respectively, of the encoder and the decoder are trained by maximizing the following _dynamic_ version of the evidence lower bound (ELBO): \(_{}(X_{1:m},(,)):=_{t=1}^{m}_{z_{t}  q_{,t}(|X_{1:m})}[- p_{}(x_{t}|z_{t})]+(q_{ ,0}(|X_{1:m})\|p_{Z_{0}})]\), where the prior \(p_{Z_{0}}\) follows a standard Gaussian distribution in the latent dimension. For alternatives to the ELBO objective above, such as the Wasserstein-GAN objective, we refer the reader to .

We now evaluate both the learned dynamics, \(F_{}\), and the learned generative model, \(p_{}\), which approximates \(\). In Figure 11, we present the empirical distribution of a generated orbit against that of a typical orbit of the Lorenz system (\(\)). We observe that the distributions match well, nevertheless the vector field is not well-approximated. First, even though the system is deterministic, the learned \(^{t}_{}\) with minimum generalization error (\(^{}\)) encountered in the hyperparameter search (Appendix C.8) is not, i.e, the diffusion term \(_{}\) is not zero. The learned stochastic map, \(F_{}\), produces an incorrect stable LE for the Lorenz system (\(-11.8\)), while the leading unstable LE matches reasonably well.

Since the map \(F_{}\), or its underlying vector field, on the latent space is not unique, we may obtain maps that do not preserve dynamical structure or invariants, even if the generated samples from \(p_{}\) approximately capture \(\). As noted in section 2, the physical measure \(\) is often singular, but absolutely continuous on lower-dimensional manifolds, leading to lack of theoretical guarantees for vanilla generative models . Finally, the sample complexity (and tight generalization bounds) of generative models, the above variational optimization, are not fully understood theoretically, especially for singular distributions (that satisfy the _manifold_ hypothesis ). We remark that since the minimax rates for approximating distributions have an exponential dependence on the dimension, exploiting the intrinsic dimension (unstable dimension) associated with the support of \(\) will be key to tractable generative models for \(\) in high-dimensional chaotic systems. Even in the Lorenz '63 system, we require \((10^{6})\) samples for training reasonably accurate model in Figure 11, while the Jacobian-matching (Figure 1 column 5) produces smaller Wasserstein distances with fewer samples (\(10^{4}\)), and a simpler regression problem as opposed to variational optimization above.

## 5 Numerical Experiments

We conduct experiments with the MSE and JAC losses in (1) and (3) respectively on many canonical chaotic systems: 1D tent maps, 2D Bakers map, 3D Lorenz '63 system and the Kuramoto Sivanshinsky equation (127 dimensional system after discretization) (Appendix C). In each system, we identify the Neural ODE model with the lowest generalization errors \(R\) and \(R_{}\) in (2) and (4) respectively, by an architecture and optimization hyperparameter search (Appendix B). On these models that generalize well, we perform tests of statistical accuracy and LE computations (as described in section 2 for the Lorenz '63 system). Consistent with Theorem 1, although most of the considered systems are not uniformly hyperbolic, we find that the JAC models are statistically accurate and reproduce the LEs, while the MSE models with \(R\) comparable to \(R_{}\) are not statistically accurate. For the KS system for instance, the MSE models even overpredict the number of positive LEs. Interestingly, the best JAC model can learn more than half of the first 64 LEs, compared to the best MSE model that can learn only 2 out of 64 LEs, with \(<10\%\) relative error. The Python code is available at [https://github.com/ni-sha-c/stacNODE](https://github.com/ni-sha-c/stacNODE).

[MISSING_PAGE_FAIL:9]

**Learning dynamical representations vs. generative models.** We show that physical measures can be produced by accurate neural representations of the dynamics, which can be more data and time-efficient compared with generative modeling for time series generated by chaotic systems.

**Dynamics-aware learning of scientific models.** Our results imply that generalization does not imply statistical accuracy and preservation of dynamical invariants and properties such as Lyapunov exponents, which is crucial for trustworthy ML for science. Thus, we reinforce the need to move toward a context-aware theory of generalization, organically unifying complex dynamics with learning theory.

**Limitations:** A limitation of the theoretical results about \(C^{1}\) generalization is that we need to assume the typicality of shadowing. Empirically, the Jacobian can be expensive to estimate for high-dimensional scientific applications. We leave the study of statistical accuracy for learning atypical shadowing orbits, and the extension of an efficient algorithm for Jacobian information during training as a future work.