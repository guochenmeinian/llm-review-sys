# Learning Discrete Latent Variable Structures with Tensor Rank Conditions

Zhengming Chen\({}^{1,2}\), Ruichu Cai\({}^{1,}\)1, Feng Xie\({}^{3}\), Jie Qiao\({}^{1}\),

**Anpeng Wu\({}^{2,4}\), Zijian Li\({}^{2}\), Zhifeng Hao\({}^{1,5}\), Kun Zhang\({}^{2,6,}\)**

1. School of Computer Science, Guangdong University of Technology

2. Machine Learning Department, Mohamed bin Zayed University of Artificial Intelligence

3. Department of Applied Statistics, Beijing Technology and Business University

4. Department of Computer Science and Technology, Zhejiang University

5. College of Science, Shantou University, Shantou, Guangdong, China

6. Department of Philosophy, Carnegie Mellon University

Corresponding author

###### Abstract

Unobserved discrete data are ubiquitous in many scientific disciplines, and how to learn the causal structure of these latent variables is crucial for uncovering data patterns. Most studies focus on the linear latent variable model or impose strict constraints on latent structures, which fail to address cases in discrete data involving non-linear relationships or complex latent structures. To achieve this, we explore a tensor rank condition on contingency tables for an observed variable set \(_{p}\), showing that the rank is determined by the minimum support of a specific conditional set (not necessary in \(_{p}\)) that d-separates all variables in \(_{p}\). By this, one can locate the latent variable through probing the rank on different observed variables set, and further identify the latent causal structure under some structure assumptions. We present the corresponding identification algorithm and conduct simulated experiments to verify the effectiveness of our method. Our results elegantly expand the application scope of causal discovery with latent variables.

## 1 Introduction

Social scientists, psychologists, and researchers from various disciplines are often interested in understanding causal relationships between the latent variables that cannot be measured directly, such as depression, coping, and stress (Silva et al., 2006). A common approach to grasp these latent concepts is to construct a measurement model. For instance, experts design a set of measurable items or survey questions that serve as indicators of the latent variable and then use them to infer causal relationships among latent variables (Bollen, 2002; Bartholomew et al., 2011; Cui et al., 2018).

Numerous approaches exist for addressing structure learning among latent variables. In particular, if the data generation process is assumed to be a linear relationship, known as _linear latent variable models_, several approaches have been developed. These include the second-order statistic-based approaches (Silva et al., 2006; Kummerfeld and Ramsey, 2016; Chen et al., 2024; Sullivant et al., 2010), high-order moments-based ones (Xie et al., 2020; Chen et al., 2022; Cai et al., 2019; Adams et al., 2021), matrix decomposition-based methods (Anandkumar et al., 2013, 2014, 2015), and copula model-based approaches (Cui et al., 2018). Moreover, the hierarchical latent variable structure has been well-studied within the linear setting (Huang et al., 2022; Xie et al., 2022; Chen et al., 2023; Jin et al., 2023). However, the linear assumption is rather restrictive and the discrete data in the real world could be more frequently encountered (e.g., responses from psychological and educationalassessments or social science surveys (Eysenck et al., 2021; Skinner, 2019)), which does not satisfy the linear assumption.

When the data generation process is discrete, however, due to the challenging nonlinear transition relationship in discrete data, few identifiability results exist and are mostly only applicable in strict cases. In particular, under some prespecified structure, the identifiability of parameters is established, such as in the hidden Markov model(HMM) (Anandkumar et al., 2012) model, topic models (Anandkumar et al., 2014), and multiview mixtures model (Anandkumar et al., 2015). By further specifying the latent variable structure as a tree, Wang et al. (2017); Song et al. (2013) show that the structural model is identifiable. Recently, Gu (2022); Gu and Dunson (2023) further considered the identifiability of pyramid structure under the condition that each latent variable has at least three observed children. However, challenges persist in extending identifiability to more general structures among discrete latent variables. Existing approaches, unfortunately, cannot identify the causal structure of latent variables as shown in Fig. 2(a).

Recently, some studies have shown that causal structures involving discrete latent confounders can be effectively identified, building on the identifiability results of mixture models, as discussed in (Kant et al., 2024; Gordon et al., 2023; Mazaheri et al., 2023; Anandkumar et al., 2012). Most of these works focus on the causal structure among observed variables, usually assuming a single latent confounder. For the identification of latent structure, (Kivva et al., 2021) shows that causal structures of discrete latent variables can be identified by recovering the latent distribution from a mixture oracle. However, while a general discrete latent variable model can be identified theoretically, estimating the parameters of the mixture model is challenging. Approximating methods are often applied, but these may be unrealistic and impractical in real-world situations.

In this paper, we aim to establish a general identification criterion for discrete latent structures in cases where latent structures exhibit flexible dependencies, while also developing a simple but robust structure learning algorithm. To achieve this, we explore a tensor rank condition on the contingency tables for an observed variable set \(_{p}\), to probe the latent causal structure from observed data. Interestingly, as shown in Fig. 1, we found that the rank of the contingency tables of the joint distribution \((X_{1},X_{2})\) is deeply connected to the support of a variable \(L\) (not necessary among \(X_{1},X_{2}\)) that d-separate \(X_{1}\) and \(X_{2}\). By this observation, we first develop a general tensor rank condition for the discrete causal model and show that such a rank is determined by the minimal support of a specific conditional set (not necessary in \(_{p}\)) that d-separates all variables in \(_{p}\). Such findings intrigue the possibility to identify the discrete latent variables structure. We further propose a discrete latent structure model that accommodates more general latent structures and shows that the discrete latent variable structure can be identified locally and iteratively through tensor rank conditions. Subsequently, we present an identification algorithm to complete the identifiability of discrete latent structure models, including the measurement model and the structure model. We theoretically show that under proper causal assumptions, such as faithfulness and the Markov assumption, the

Figure 1: Illustrating the graphical criteria of the tensor rank condition, the rank of the joint distribution is determined by the support of a specific conditional set that d-separates all observed variables, i.e., \(((X_{1},X_{2}))=|(L)|=2\). See Example 3.4 for details.

measurement model is fully identifiable and the structure model can be identified up to a Markov equivalence class.

The contributions of this work are three-fold. (1) We first establish a connection between the tensor rank condition and the graphical patterns in a general discrete causal model, including specific d-separation relations. (2) We then exploit the tensor rank condition to learn the discrete latent variable model, allowing flexible relations between latent variables. (3) We present a structure learning algorithm using tensor rank conditions and demonstrate the effectiveness of the proposed algorithm through simulation studies.

## 2 Discrete Latent Structure Model

For an integer \(m\), denote \([m]=\{1,2,,m\}\). Consider a discrete statistic model with \(k\) latent variable set \(=\{L_{1},,L_{k}\},L_{i}[r_{i}]\) and \(m\) discrete observed variable set \(=\{X_{1},,X_{m}\}\) with \(X_{i}[d_{i}]\) (\(r_{i},d_{i} 2\)), in which any marginal probabilities are non-zero. We say a discrete statistic model is a _discrete causal model_ if and only if \(=\) can be represented by a directed acyclic graph (DAG), denoted by \(\). We use \((V_{i})=\{v^{+}:(V_{i}=v)>0\}\) to denote the set of possible values of the random variable \(V_{i}\). Our work is in the framework of causal graphical models. Concepts used here without explicit definition, such as d-separation, which can refer to standard literature (Spirtes et al., 2000).

In this paper, we focus on learning causal structure among latent variables in one class of discrete causal models. The model is defined as follows.

**Definition 2.1** (Discrete Latent Structure Model with Three-Pure Children).: _A discrete causal model is the Discrete **Latent Structure **M**odel with Three-Pure Children (Discrete 3PLSM) if it further satisfies the following three assumptions:_

1. _[_Purity Assumption_]_ _there is no direct edges between the observed variables;_
2. _[_Three-Pure Child Variable Assumption_]_ _each latent variable has at least three pure variables as children;_
3. _[Sufficient Observation Assumption_]_ _The dimension of observed variables support is larger than the dimension of any latent variables support._

These structural constraints inherent in the discrete 3PLSM are also widely used in linear latent variable models, e.g., Silva et al. (2006); Kummerfeld and Ramsey (2016); Cai et al. (2019); Xie et al. (2020). In the binary latent variable case, recently, a similar definition is also employed in Gu and Dunson (2023); Gu (2022). The key difference is that there are no constraints on the latent structure in our work. An example of a discrete 3PLSM model is shown in Fig. 2(a), where \(L_{1},,L_{4}\) represent discrete latent variables, and \(X_{1},,X_{12}\) are discrete observed (measured) variables.

Generally speaking, the discrete 3PLSM model can be divided into two sub-models (Spirtes et al., 2000), i.e., the measurement model and the structure model, e.g., red edge and blue edge in Fig. 2 (a). By this, one can first identify the measurement model to determine the latent variables and then use the measured variable to infer the causal structure of latent variables. As shown in Fig. 2 (b), we will separately discuss the identification of the two sub-models and show that the measurement model is

Figure 2: An example of discrete latent structure model involving 4 latent variables and 12 observed variables (sub-fig (a)). Here, the red edges form a measurement model, while the blue edges form a structural model. The theoretical result of this paper is shown in sub-fig (c).

fully identifiable and the structure model is identified up to a Markov equivalence class. The symbols used in our work is summarised in Table 1.

To ensure the identification of causal structure and the asymptotic correctness of identification algorithms, some common causal assumptions are required.

**Assumption 2.2** (Causal Markov Assumption).: _Let \(\) be a causal graph with vertex set \(\) and \(_{}\) be probability distribution over the vertices in \(\) generated by \(\). We say \(\) and \(_{}\) satisfy the Causal Markov Assumption if and only if for every \(V_{i}\), \((V_{i},_{V_{i}}| {Pa}_{V_{i}}=i)=(W|_{V_{i}}=i) (_{V_{i}}|_{V_{i}}=i)\)._

**Assumption 2.3** (Faithfulness Assumption).: _Let \(\) be a causal graph with vertex set \(\) and \(_{}\) be probability distribution over the vertices in \(\) generated by \(\). We say < \(,_{}>\) satisfies the Faithfulness Assumption if and only if (i). every conditional independence relation true in \(_{}\) is entailed by the Causal Markov Assumption applied to \(\), and (ii). for any joint distribution \((_{p})\), there does not exist \((_{q})\) with \(|(_{q})|\) < \(|(_{p})|\) such that \((_{p})=(_{q})\)._

**Assumption 2.4** (Full Rank Assumption).: _For any conditional probability \((X|_{X}.)\), the corresponding contingency table is full rank, i.e., each column of \((X|_{X}.)\) is linearly independent with the other column vectors in the parameter space._

In general, Assumptions 2.2\(\)2.3 are widely used in the constraint-based causal discovery methods, e.g., PC algorithm and FCI algorithm (Spirtes et al., 2000; Spirtes and Glymour, 1991). One can see that we further constraint the parameter space of joint distribution cannot be reduced to a low-dimension space, for maintaining the diversity of parameter space. This is also the reason for Assumption 2.4, which aligns with the non-degeneracy condition used in (Kivva et al., 2021).

**Our goal:** The goal of our work is to develop a robust approach for identifying discrete latent structure models, including both the measurement and structural models.

## 3 Tensor Rank Condition with Graphical Criteria

To address the identification problem in the discrete 3PLSM, this section introduces the building block-the tensor rank condition of the discrete causal model. Then, we establish the connection between tensor rank and d-separation relations under a general discrete causal model.

Before formalizing the tensor rank condition, we first give the explicit definition of tensor rank.

**Definition 3.1** (Rank-one Tensor).: _An \(n\)-way tensor \(^{I_{1} I_{n}}\) is a rank-one tensor if it can be written as the outer product of \(n\) vectors, i.e.,_

\[=_{1}_{2} _{n},\]

_where \(_{i}\) are vectors that each represent a dimension of the tensor, \(\) represents the outer product._

**Definition 3.2** (Tensor Rank Kolda and Bader (2009)).: _For an \(n\)-way tensor \(^{I_{1} I_{n}}\), the rank of a tensor \(\) is defined as the **smallest** number of rank-one tensors that sum to exactly represent \(\). Formally, the rank of tensor \(\), denoted \(()\), is the smallest integer \(r\) such that:_

\[=_{i=1}^{r}_{1}^{(i)}_{2}^{(i)} _{n}^{(i)},\]

_where each \(_{k}^{(i)}\) is a vector in the corresponding vector space associated with the \(k\)-th mode of \(\)._

  \(\): The set of variables \(=\) & \(\): The set of observed variables \\  \(\): The set of latent variables & \(V_{i} V_{j}\|_{p}:\) \\  \(|_{p}|:_{p}\) & \((_{p}):(_{p})\) \\  \((_{p})\): the joint distribution of \(_{p}\) & \(((_{p})):(_{p})\) \\  \(_{X}\): The parent set of \(X\) & \(_{X}\): The descendant set of \(X\) \\  \(()\): The diagonal matrix of \(\) & \(_{i}_{j}:\) \\  

Table 1: Mathematical notations used in this paper.

In other words, the tensor rank denotes the minimal number of rank-one decompositions. In the discrete causal model, the joint distribution can be represented as a tensor, e.g., the joint distribution of two random variables is a two-way contingency tensor. Interestingly, by carefully analyzing the (non-negative) rank-one decomposition of the joint distribution, we find that the tensor rank essentially reveals structural information within the causal graph. The result is presented below.

**Theorem 3.3** (Graphical implication of tensor rank condition).: _In the discrete causal model, suppose Assumptions 2.2 \(\) Assumption 2.4 hold. Consider an observed variable set \(_{p}=\{X_{1},,X_{n}\}\) (\(_{p}\) and \(n 2\)) and the corresponding \(n\)-way probability tensor \(_{(_{p})}\) that is the tabular representation of the joint probability mass function \((X_{1},,X_{n})\). Then, \((_{(_{p})})=r\) (\(r>1\)) if and only if (i) there exist a conditional set \(\) with \(|()|=r\) that d-separates any pair of variables in \(\{X_{1},,X_{n}\}\), and (ii) does not exist conditional set \(}\) that satisfies \(|(})|<r\)._

We further provide an example to illustrate Theorem 3.3.

**Example 3.4** (Illustrating the graphical criteria).: _Consider a single latent variable structure as shown in Fig. 1 (a) where \(L\) is a latent variable with \((L)=\{0,1\}\) and \(X_{1},X_{2}\) are observed variables with \((X_{i})=\{0,1,2\},i\{1,2\}\). For convenience, we denote \(p_{i}=(L=i)\), \(_{} j}=(X_{1}=i|L=j)\), and \(_{} j}=(X_{2}=i|L_{1}=j)\). The joint distribution \((X_{1},X_{2})\) can be expressed as the product of conditional probabilities, as shown in Fig. 1(b). By applying the tensor decomposition, we observe that \((X_{1},X_{2})\) can be decomposed as the sum of two rank-one tensors: \((X_{1},X_{2}|L=0)\) and \((X_{1},X_{2}|L=1)\). Thus, the rank of the tensor \((X_{1},X_{2})\) is two, corresponding to the cardinality of the latent variable's support. The reason \((X_{1},X_{2}|L=i)\) is a rank-one tensor is that \(L\) d-separates \(X_{1}\) and \(X_{2}\), i.e., \((X_{1},X_{2}|L=i)=(X_{1}|L=i)(X_{2}|L=i)\). This illustrates the connection between tensor rank and d-separation relations._

Intuitively, the graphical criteria theorem suggests that, in the discrete causal model, the tensor rank condition implies the minimal conditional probability decomposition within the probability parameter space, which hopefully induces the structural identifiability of the discrete 3PLSM model.

## 4 Structure Learning of Discrete Latent Structure Model

In this section, we address the identification problem of the discrete 3PLSM model using a carefully designed algorithm that leverages the tensor rank condition. Specifically, we first show that latent variables can be identified by finding causal clusters among observed variables (Sec. 4.1). Then, we use these causal clusters to conduct conditional independence tests among latent variables based on the tensor rank condition, identifying the structure model (Sec. 4.2). Finally, we discuss the practical implementation of testing tensor rank (Sec. 4.3). For simplicity, we focus on the case where all latent variables have the same number of categories. The result can be directly extended to cases with different numbers of categories (see details in Appendix E).

### Identification of the measurement model

To answer the identification of the measurement model, one common strategy is to find the causal cluster that shares the common latent parent, which has been well-studied within the linear model, such as Silva et al. (2006); Cai et al. (2019); Xie et al. (2020). We follow this strategy and show that, in the discrete 3PLSM, the causal cluster can be found by testing the tensor rank conditions iteratively. The definition of a causal cluster is as follows.

**Definition 4.1** (Causal cluster).: _In the discrete 3PLSM, the variable set \(\{X_{1},,X_{n}\}\) is a causal cluster, termed \(C_{i}\), if and only if all variables in \(\{X_{1},,X_{n}\}\) share the common latent parent._

It is not hard to see that, the measurement model can be identified if all causal cluster is found. In order to find these causal clusters by making use of the tensor rank condition, the key issue is to determine the support of latent variables in advance. This issue can be addressed by identifying the rank of the two-way tensor formed by the joint distribution of two observed variables.

**Proposition 4.2** (Identification of support of latent variables).: _In the discrete 3PLSM model suppose Assumptions 2.2 \(\) Assumption 2.4 hold. The support of the latent variable corresponds to the rank of the two-way probability contingency table for any pair of observed variables \(X_{i}\) and \(X_{j}\), i.e., \(|(L)|=(_{(X_{i},X_{j})})\), \( X_{i},X_{j}\)._This result holds because any pair of variables in the discrete 3DLSM model is d-separated by any one of their latent parent variables, and all latent variables have the same support. Next, we formalize the property of clusters and give the criterion for finding clusters.

**Proposition 4.3** (Identification of causal cluster).: _In the discrete 3DLSM mode, suppose Assumption 2.2 \(\) Assumption 2.4 hold. Let \(r=|(L_{i})|\) denote the cardinality of the latent support. Given three disjoint observed variables \(X_{i},X_{j},X_{k}\),_

* \(ule1\)_: if the rank of tensor_ \(_{(X_{i},X_{j},X_{k})}\) _is not equal to_ \(r\)_, i.e.,_ \((_{(X_{i},X_{j},X_{k})}) r\)_, then_ \(X_{i}\)_,_ \(X_{j}\) _and_ \(X_{k}\) _belong to the different latent parents._
* \(ule2\)_: for any_ \(X_{s}\)_,_ \(X_{s}\{X_{i},X_{j},X_{k}\}\)_, if the rank of tensor_ \(_{(X_{i},X_{j},X_{k},X_{s})}\) _is_ \(r\)_, i.e.,_ \((_{(X_{i},X_{j},X_{k},X_{s})})=r\)_, then_ \(\{X_{i},X_{j},X_{k}\}\) _share the same latent parent._

**Example 4.4** (Finding causal clusters).: _Let's take Fig. 2(a) as an example. One can find that for \(\{X_{1},X_{2},X_{3},X_{k}\}\), where \(X_{k}\{X_{1},X_{2},X_{3}\}\), the rank of tensor \(_{(X_{1},X_{2},X_{3},X_{k})}\) is \(r\). Thus, \(\{X_{1},X_{2},X_{3}\}\) is identified as a causal cluster._

Next, we consider the practical issues involved in determining the number of latent variables by causal clusters. That is, there are some causal clusters that should be merged because they share one latent parent. We find that the overlapping clusters can be directly merged into one cluster. This is because the overlapping clusters have the same latent variable as the parent under the discrete 3DLSM model. The validity of the merge step is guaranteed by Proposition 4.5.

**Proposition 4.5** (Merging Rule).: _In the discrete 3DLSM model, for two causal clusters \(C_{1}\) and \(C_{2}\), if \(C_{1} C_{2}\), then \(C_{1}\) and \(C_{2}\) share the same latent parent._

Based on the above results, one can iteratively identify causal clusters and apply the merger rule to detect all latent variables. The identification procedure is summarized in Algorithm 1.

```
0: Data from a set of measured variables \(_{}\), and the dimension of latent support \(r\)
0: Causal cluster \(\)
1: Initialize the causal cluster set \(:=\), and \(^{}=\);
2://Identify Causal Skeleton
3:Begin the recursive procedure
4:repeat
5:for each \(X_{i},X_{j}\) and \(X_{k}\)do
6:if\((_{\{X_{i},X_{j},X_{k}\}}) r\)then
7: Continue;//Rule1 of Prop. 4.3
8:endif
9:if\((_{\{X_{i},X_{j},X_{k},X_{s}\}}=r\), for all \(X_{s}\{X_{i},X_{j},X_{k}\}\)then
10:\(\) = \(\{\{X_{i},X_{j},X_{k}\}\}\);
11:endif
12:endfor
13:until no causal cluster is found.
14://Merging cluster and introducing latent variables
15:Merge all the overlapping sets in \(\) by Prop. 4.5.
16:for each \(C_{i}\)do
17: Introduce a latent variable\(L_{i}\) for \(C_{i}\);
18:\(\) = \(\{L_{i} X_{j}|X_{j} C_{i}\}\).
19:endfor
20:return Graph \(\) and causal cluster \(\).
```

**Algorithm 1** Finding the causal cluster

**Theorem 4.6** (Identification of the measurement model).: _In the discrete 3DLSM, suppose Assumption 2.2 \(\) Assumption 2.4 hold. The measurement model is fully identifiable by Algorithm 1._

### Identification of the structure model

Once the measurement model is identified, the observed children can serve as proxies for the latent variables, enabling the identification of the causal structure among them. Here, we employ constraint-based framework to learn the causal structure of latent variables.

Constraint-based structure learning algorithms find the Markov equivalence class over a set of variables by making decisions about independence and conditional independence among them. Given a pure and accurate measurement model with at least two measures per latent variable, we can test for independence and conditional independence (CI) among the latent variables. Specifically, to test statistical independence between discrete variables, one can examine whether the rank of their joint distribution contingency table is one (Sullivan, 2018). For testing conditional independence (CI) relations among latent variables, further leveraging the algebraic properties of the tensor rank condition is required (see Theorem 4.7).

**Theorem 4.7** (d-separation among latent variable).: _In the discrete 3PLSM, suppose Assumption 2.2 \(\)Assumption 2.4 hold. Let \(r\) denote the cardinality of the latent support. Then, \(L_{i}\ \ L_{j}|_{p}\) if and only if \((_{(X_{i},X_{j},_{p1},_{p2})})=r^{ _{p}}\), where \(X_{i}\) and \(X_{j}\) are the pure children of \(L_{i}\) and \(L_{j}\), \(_{p1}\) and \(_{p2}\) are two disjoint child sets of \(_{p}\) that satisfy \( L_{i}_{p},_{L_{i}}_{p1} ,_{L_{i}}_{p2}\)._

Intuitively, based on the graphical criteria of tensor rank condition, \(_{p}\) is a minimal conditional set in the causal graph that d-separates \(_{p1}\) and \(_{p2}\) and hence the rank of tensor \(_{(X_{i},X_{j},_{p1},_{p2})}\) is the dimension of support of \(_{p}\), if \(X_{i}\) and \(X_{j}\) also be d-separated by \(_{p}\).

**Example 4.8** (CI test among latent variables).: _Consider the structure in Fig. 2(a) and suppose \(r=2\). By selecting \(_{p1}\) = \(\{X_{4},X_{7}\}\) and \(_{p2}\) = \(\{X_{5},X_{8}\}\) as two disjoint child sets of \(\{L_{2},L_{3}\}\) respectively, let \(_{p}\) = \(\{X_{1},X_{10},X_{4},X_{5},X_{7},X_{8}\}\) and \(_{p}\) = \(\{L_{2},L_{3}\}\). One can see that the rank of tensor \(_{(_{p})}\) is four since \(_{p}\) (i.e., \(\{L_{2},L_{3}\}\)) is minimal conditional set that d-separates any pair variable in \(_{p}^{}\), which imply that \(L_{1}\ \ L_{4}|\{L_{2},L_{3}\}\)._

Based on Theorem 4.7, we introduce the PC-TENSOR-RANK algorithm. This method accepts a measurement model learned by the previous procedure, and outputs the Markov equivalence class of the structural model associated with the latent variables within the measurement model, in accordance with the PC algorithm. The implementation is summarised as Algorithm 2. Consequently, we establish the identification of the structure model as shown in Theorem 4.9.

```
0: Data set \(=\{X_{1},,X_{m}\}\) and causal cluster \(\)
0: A partial DAG \(\).
1: Initialize the maximal conditions set dimension \(k\);
2: Let \(L_{i}\) denote as \(C_{i}\), \(C_{i}\);
3: Form the complete undirected graph \(\) on the latent variable set \(\);
4:for\( L_{i},L_{j}\) and adjacent in \(\)do
5://Test the CI relations among latent variables by Theorem 4.7
6:if\(_{p}\{L_{i},L_{j}\}\) and (\(|_{p}|<k\)) such that \(L_{i}\ \ L_{j}|_{p}\) hold then
7: delete edge \(L_{i}-L_{j}\) from \(G\);
8:endif
9:endfor
10: Search V structures and apply meek rules Meek (1995).
11:return a partial DAG \(\) of latent variables.
```

**Algorithm 2** PC-TENSOR-RANK

**Theorem 4.9** (Identification of structure model).: _In the discrete 3PLSM, suppose Assumption 2.2 \(\)Assumption 2.4 hold. Given the measurement model, the causal structure over the latent variable is identified up to a Markov equivalent class by the PC-TENSOR-RANK algorithm._

### Practical Test for Tensor Rank

In our theoretical results, the key issue is to test the rank of a tensor, which involves estimating the dimension of latent support and the rank of a tensor. Here, we aim to explore methods to (i) estimate the rank of the contingency matrix for determining the dimension of latent support, and (ii) apply the goodness-of-fit test to assess the tensor rank.

Estimate the rank of contingency matrix.We start with the estimation of the dimension of latent variables support based on Prop. 4.2. There are many practical approaches used to estimate the rank of a general matrix \(\), such as Camba-Mendez and Kapetanios (2009). In our implementation, we use the characteristic root statistic, abbreviated as CR statistic Robin and Smith (2000), to test the rank of the probability contingency matrix of two observed variables. Specifically, Let \(}\) be an asymptotically normal estimator of \(\), then the CR statistic is the sum of \(d-r\) smallest singular values of \(}\), multiplied by the sample size. Under the null hypothesis, the above statistic converges in distribution to a weighted (given by the eigenvalues) sum of independent \(_{1}^{2}\) random variables.

Goodness-of-fit test for tensor rank.Once the dimension of the support of latent variables is identified, in the structure learning procedure, we perform the following hypotheses test: \(_{0}\): \(()\) = _r v.s._\(_{1}\): \(()\) # \(r\). To achieve this, we first apply the canonical polyadic (CP) decomposition technology to the target tensor \(\) as a sum of \(r\) rank-one tensors given specified \(r\), then we evaluate how well the reconstructed tensor from this decomposition approximates the original tensor to conduct the hypotheses test.

To perform the rank-decomposition with specified \(r\) on the probability contingency tensor, one can use the non-negative CP decomposition to decompose the tensor into the sum of \(r\) rank-one tensor Shashua and Hazan (2005). Given the decomposition, one can obtain a reconstructed tensor, denoted by \(}\), from the outer product of decomposed vectors.

With the reconstructed tensor, we constructed square-chi goodness of fit test Cochran (1952) for testing \(()\) = \(r\). Such a test is frequently used to summarize the discrepancy between observed values and the expected values, which measure the sum of differences between observed and expected outcome frequencies. Let \(()\) be the vectorization of tensor \(\), suppose \((})\) be the asymptotic normality estimator of \(()\), we have the chi-square statistic as \(^{2}=_{i(})} ()_{i}-(})_{i})^{2}}{( })_{i}}\), which follows the \(^{2}\) distribution with freedom degrees \(_{i[n]}d_{i}-(_{i,j[n]}d_{i}d_{j})\).

## 5 Simulation Studies

In this section, we conducted simulation studies to assess the correctness of the proposed methods. The baseline approaches include Building Pure Cluster (BPC) Silva et al. (2006), Latent Tree Model (LTM) Choi et al. (2011), and Bayesian Pyramid Model (BayPy) Gu and Dunson (2023).

In the following simulation studies, we consider the different combinations of various types of structure models(SM) and measurement models(MM). Specifically, for the structure model, we consider the following five typical cases: [SM1]: \(L_{1}L_{2}\); [SM2]: \(L_{1}L_{2}L_{3}\); [SM3]: the structure of latent variables is shown in Fig. 2(a); [Collider]: \(L_{1}L_{2} L_{3}\); [Star]: \(L_{1}L_{2},L_{1}L_{3},L_{1}L_{4}\). For the measurement model, we consider the following two cases: [MM1]: each latent variable has three pure observed variables, i.e., \(L_{i}\{X_{1},X_{2},X_{3}\}\); [MM2]: each latent variable has four pure observed variables, i.e., \(L_{i}\{X_{1},X_{2},X_{3},X_{4}\}\).

In all cases, the data generation process follows the discrete 3PLSM model: (i) we generate the probability contingency table of latent variables in advance, according to different latent structures (e.g., SM1), then (ii) we generate the conditional contingency table of observed variables (condition on their latent parent), and finally (iii) we sample the observed data according to the probability contingency table, where the dimension of latent support \(r\) is set to 3 and the dimension of all observed variables support is set to 4, sample size ranged from \(\{5,10,50\}\).

    &  &  &  \\  Algorithm & **Our** & **BayPy** & **LTM** & **BPC** & **Our** & **BayPy** & **LTM** & **BPC** & **Our** & **BayPy** & **LTM** & **BPC** \\  +MM_{2}\)} & 3k & 0.15(0) & 0.12(0.15) & 0.96(0.00) & 0.00(0.0) & 0.16(0.2) & 0.00(0.00) & 0.05(1) & 0.02(0.00) & 0.00(0.00) & 0.00(0.00) \\  & 10k & 0.05(1) & 0.05(1) & 0.10(2) & 0.90(1) & 0.00(0) & 0.05(1) & 0.00(0.00) & 0.00(0.00) & 0.00(0.00) & 0.00(0.00) & 0.00(0.00) \\  & 50k & 0.00(0) & 0.00(0) & 0.00(0) & 0.90(1) & 0.00(0) & 0.00(0.00) & 0.00(0.00) & 0.00(0.00) & 0.00(0.00) & 0.00(0.00) & 0.00(0.00) \\  +MM_{3}\)} & 5k & 0.23(0.196) & 0.26(0.96) & 0.90(1) & 0.00(0.0) & 0.196(0.31) & 0.00(0.00) & 0.05(1) & 0.196(0.236) & 0.00(0.00) \\  & 10k & 0.13(0.134) & 0.13(0.134) & 0.86(1) & 0.00(0.00) & 0.03(0.4) &

[MISSING_PAGE_FAIL:9]