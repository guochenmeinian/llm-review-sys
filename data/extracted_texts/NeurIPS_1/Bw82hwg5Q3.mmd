# Self-Evaluation Guided Beam Search for Reasoning

Yuxi Xie\({}^{1}\)1  Kenji Kawaguchi\({}^{1}\)  Yiran Zhao\({}^{1}\)  James Xu Zhao\({}^{1}\)

**Min-Yen Kan\({}^{1}\)  Junxian He\({}^{2}\)2 Michael Qizhe Xie\({}^{1}\)2 \({}^{1}\) National University of Singapore \({}^{2}\) The Hong Kong University of Science and Technology**

###### Abstract

Breaking down a problem into intermediate steps has demonstrated impressive performance in Large Language Model (LLM) reasoning. However, the growth of the reasoning chain introduces uncertainty and error accumulation, making it challenging to elicit accurate final results. To tackle this challenge of uncertainty in multi-step reasoning, we introduce a stepwise self-evaluation mechanism to guide and calibrate the reasoning process of LLMs. We propose a decoding algorithm integrating the self-evaluation guidance via stochastic beam search. The self-evaluation guidance serves as a better-calibrated automatic criterion, facilitating an efficient search in the reasoning space and resulting in superior prediction quality. Stochastic beam search balances exploitation and exploration of the search space with temperature-controlled randomness. Our approach surpasses the corresponding Codex-backboned baselines in few-shot accuracy by \(6.34\%\), \(9.56\%\), and \(5.46\%\) on the GSM8K, AQuA, and StrategyQA benchmarks, respectively. Experiment results with Llama-2 on arithmetic reasoning demonstrate the efficiency of our method in outperforming the baseline methods with comparable computational budgets. Further analysis in multi-step reasoning finds our self-evaluation guidance pinpoints logic failures and leads to higher consistency and robustness. Our code is publicly available at [https://guideddecoding.github.io/](https://guideddecoding.github.io/).

## 1 Introduction

The remarkable empirical achievements of Large Language Models (LLMs) have recently ushered in a new era in machine reasoning through few-shot prompting techniques (Brown et al., 2020; Chowdhery et al., 2022; Touvron et al., 2023a; OpenAI, 2023). In particular, breaking down a problem into intermediate stages, or a reasoning chain, can significantly improve model performance on reasoning tasks (Cobbe et al., 2021). Various prompting approaches have been proposed to define these chains, such as _scratchpads_(Nye et al., 2021), _chain-of-thought_(CoT) (Wei et al., 2022b), _least-to-most_(Zhou et al., 2023), and _program-aided language models_ (PAL) (Gao et al., 2023; Chen et al., 2022). However, as the complexity and length of reasoning chains increase with the difficulty of tasks, LLMs struggle with errors and imperfections that accumulate across multiple intermediate steps (Wu et al., 2016; Guo et al., 2018; Chen et al., 2022). Furthermore, the growing number of steps leads to an exponential growth in the search space for reasoning, making it exceedingly difficult to obtain accurate final outcomes.

Confronted with the challenges of uncertainty in multi-step chaining, several previous studies have worked on different aspects to alleviate the impact of reasoning errors. For instance, Wang et al. (2023) introduce _self-consistency_ as a method to determine the final answer through majority voting using multiple sampled reasoning paths, while Li et al. (2022) investigate various prompts to diversify the sampling outcomes. Gao et al. (2023) and Chen et al. (2022) utilize Python programs toachieve higher accuracy in mathematical computations. While these approaches have contributed to significant performance improvements in reasoning, the process of generating reasoning chains has been parameterized as a standard autoregressive process and intrinsically faces the challenge of sampling within an exponentially large search space.

Motivated by this challenge, we employ LLM _self-evaluation_(Kadavath et al., 2022) as a better-calibrated criterion to automatically guide the search in the reasoning space, drawing inspiration from prior works on utilizing LLMs for self-evaluation (Rae et al., 2021; Paul et al., 2023; Madaan et al., 2023; Shinn et al., 2023). We integrate the self-evaluation guidance for reasoning in a stepwise and generalizable manner. Specifically, we formulate the reasoning chain generation as a decoding process consisting of multiple intermediate steps. Unlike traditional text decoding where each step produces a single token, we consider each decoding step as a reasoning logic composed of a sequence of tokens. This framework enables us to employ beam search (Jurafsky and Martin, 2009; Graves, 2012) decoding tailored for intermediate steps and guide the beam searching process by controlling the error of each reasoning step to prevent potential error accumulation throughout the chaining. Figure 1 illustrates an example of decoding a chain of program-aided reasoning steps. Furthermore, we incorporate temperature-controlled randomness (Ackley et al., 1985; Kool et al., 2019; Meister et al., 2021) into the traditional (deterministic) beam search to balance the quality-diversity trade-off in searching for better reasoning chains. Our approach has resulted in respectable improvements across various arithmetic, symbolic, and commonsense reasoning tasks. For instance, by guiding the reasoning decoding process of the Codex model (Chen et al., 2021), we achieve accuracies of \(85.5\%\), \(64.2\%\), and \(77.2\%\) on the GSM8K, AQuA, and StrategyQA benchmarks, compared to the vanilla reasoning-enhanced Codex performance of \(80.4\%\), \(58.6\%\), and \(73.2\%\), respectively. Our further analysis on Llama-2 (Touvron et al., 2023b) demonstrates the efficiency of our method in surpassing the self-consistency baseline under equivalent computational budgets.

## 2 Self-Evaluation Guided Stochastic Beam Search

Considering the input prompt and question \(Q\) represented as \(x\), we formulate the answer distribution \(P(a x)\) by decomposing it as a reasoning chain generation process \(P(R x)\) and an answer generation process \(P(a R,x)\):

\[P(a x)=_{R P(R x)}P(a R,x), \]

where \(R\) is the intermediate reasoning chain variable that is typically a text sequence. \(P(a R,x)=(a)}{}\), where \(A=(R)\) represents the set of predicted answer(s) interpreted from \(R\), and \(_{A}\) is the indicator function of the subset \(A\). In practice, \(|A| 0\) can be \(0\) or larger than \(1\) when the reasoning \(R\) returns no valid answer or produces more than one possible answers, respectively.

Figure 1: Self-Evaluation can calibrate the decoding direction in multi-step reasoning. We illustrate our method in the form of stepwise stochastic beam search with the beam size equal to \(1\). The scale of the self-evaluation score is visualized in the colormap. We adopt Program-Aided Language models (PAL) reasoning (Gao et al., 2023; Chen et al., 2022) for this math word problem.

Prior research has modeled the reasoning chain generation \(P(R x)\) by prompting LLMs to explicitly elaborate on the required intermediate steps \(R\). Through setting different prompting schemes, the reasoning process \(P(R x)\) can be modeled as chain-of-thought free-text reasoning (Kojima et al., 2022; Wei et al., 2022b), a two-stage question decomposition and answering pipeline (Zhou et al., 2023), or program-aided reasoning to generate a python program (Gao et al., 2023; Chen et al., 2022). While effective, previous work mostly uses a single sample of \(R\) from the LLMs to approximate the expectation in Eq. 1 - the generated reasoning chain is often unreliable and causes incorrect answers. To mitigate this issue, Wang et al. (2023) conduct majority voting to approximate the expectation via sampling and aggregating multiple reasoning chains. Li et al. (2022) take a further step to diversify the sampling and calibrate \(P(R x)\) with a task-specific fine-tuned verifier. Another line of work focuses on improving \(P(a R,x)\) instead. For example, Gao et al. (2023) and Chen et al. (2022) employ Python programs for more accurate calculations in math word problems.

In this work, we focus on improving \(P(R x)\) to enhance the consistency of the sampled reasoning chains. To this end, we propose to explicitly break down the reasoning process into multiple steps, as shown in Figure 2, where each step yields a semantically integrated sequence of tokens, representing a single step within the overall reasoning chain. From this perspective, we can approach the task of enhancing \(P(R x)\) as a decoding problem over the reasoning chains. Considering the exponentially large search space and the potential unreliability of LLM-produced chains in reasoning, we propose a constrained stochastic beam search decoding approach to improve the reasoning step by step and obtain high-quality reasoning with a limited number of samples. We detail our approach next.

### Multi-step Reasoning via Stochastic Beam Search

In multi-step reasoning, a reasoning chain of \(T\) steps is sequentially generated through several timesteps as \(R=[s^{1},s^{2},,s^{T}]=s^{1:T}\), where \(s^{t}\) represents a sequence of tokens as the \(t\)-th step. Formally, the reasoning generation process \(P(R x)\) can be factorized in an autoregressive manner:

\[P(R=s^{1:T} x)=_{t}P(s^{t} x,s^{1:t-1}), \]

which resembles the typical token-level autoregressive distribution of language models. Stepwise reasoning allows us to formulate the process as a step-by-step decoding problem, where we can utilize widely-used strategies such as beam search for the generation. Different from the typical text

Figure 2: Our framework of self-evaluation guided stochastic beam search for multi-step reasoning. The schema of the decoding process is on the left, where we keep \(k=2\) candidates at each timestep, with the detailed illustration of timestep \(t\) at the bottom. Here “Gen” and “Self-Eval” represent the generation and evaluation LLMs, respectively. The corresponding prompt formulations are provided on the right, where the questions \(Q\), reasoning steps \(R\), and evaluation scripts are highlighted in orange, green, and yellow, respectively. Steps in light green (_e.g._, \(s^{t}\)) are for models to generate or evaluate at the current timestep. Specifically, we follow Kadavath et al. (2022) to prompt the LLM evaluation by answering the multiple-choice question, _i.e._, the lines starting with #.

decoding process where each step consists of a single token, here we view a sequence of reasoning tokens as a single step. One of the most severe issues in LLM-based reasoning is the potential unreliability and inaccuracy of each reasoning step generated by the model. Furthermore, errors from individual steps may accumulate throughout the reasoning chain, exacerbating the problem. To address the issue, we define a constraint function \((s^{t},s^{1:t-1})\) within each reasoning step3 that outputs the LLM confidence in the correctness of the reasoning sequence \(s^{t}\) based on the previous context \(s^{1:t-1}\). Then, we present a constrained decoding approach that combines the language model probability and the correctness confidence as a new decoding objective function \((s^{1:T})\):

\[(s^{1:T})=_{t}P_{_{}}^{}(s^{t} x,s^{1:t-1})^{1-}(s^{t}), \]

where \(P_{_{}}\) is the language model distribution 4. \(\) is a weight hyperparameter to balance the LM score and the confidence score. We will detail the design of \((s^{t})\) in Section 2.2. Eq 3 follows an autoregressive factorization form, and thus traditional token-level decoding methods such as beam search can be applied here on the chain level. As it is desirable to obtain high-quality reasoning chains with limited samples that are scored high by \((s^{1:T})\), it is natural to utilize greedy or beam search decoding to approximate the reasoning sequences that maximize \((s^{1:T})\).

Additionally, multiple diverse reasoning chains could be aggregated to further improve the final accuracy, as suggested by Eq 1 and empirically confirmed by self-consistency reasoning (Wang et al., 2023). To this end, we propose a variant of stochastic beam search (Kool et al., 2019; Meister et al., 2021) to strike a tradeoff between exploration and exploitation. Concretely, for beam size \(k\), at each reasoning step we draw \(n\) samples of \(s^{t}\) following \(P_{_{}}(s^{t} x,s^{1:t-1})\) for each beam, and we end up with \(nk\) chain hypotheses of \(s^{1:t}\) to form the candidate set \(\), then we perform beam pruning through sampling - we sample \(k\) reasoning beams without replacement, rather than finding the \( k\), following a distribution defined by the accumulated score:5

\[P_{beam}(s^{1:t})((s^{1:t})/), s^{1:t}  \]

where the temperature \(\) is a hyperparameter to control the randomness in stochastic beam search; when \( 0\), stochastic beam search becomes the vanilla beam search algorithm. The reasoning beams \(s^{1:t}\) can be sampled efficiently since \(||=nk\) is a finite set. To enable fine-grained control of sampling randomness in decoding, we also introduce a hyperparameter \(\) so that \(\) can decay step by step as \(\). By annealing \(\) with \(\), we can mitigate the error accumulation due to aggregated randomness throughout chaining, as discussed in Section 3.4.

By incorporating controllable randomness, we not only achieve a more reliable single reasoning chain generation by setting randomness to be small, but also leverage multiple diverse reasoning chains with larger variance. Next, we introduce our constraint function \((s^{t},s^{1:t-1})\) that utilizes a self-evaluation scheme to improve the consistency of each reasoning step.

### Self-Evaluation as Correctness Control

Inspired by the recent success of self-evaluation (Kadavath et al., 2022; Shinn et al., 2023; Madaan et al., 2023; Paul et al., 2023), a scheme to prompt LLMs to evaluate their own generation, we use LLMs to judge the correctness of \(s^{t}\) based on \(s^{1:t-1}\). Specifically, the evaluation and generation models use the same backend LLM with different prompts, which consist of few-shot exemplars. We follow previous works of CoT (Wei et al., 2022) or PAL (Gao et al., 2023) to formulate the generation prompts. To construct the in-context exemplars \(_{}\) for the self-evaluation LLM \(_{}\), we provide stepwise evaluation examples (as question answering with rationales) in each instance. Inspired by Kadavath et al. (2022), we design \(_{}\) in the form of multiple-choice questioning (as shown in Figure 2) to better calibrate the model predictions, where we adopt the token-level probability of option A to represent the correctness score as:

\[(s^{t})=P_{_{}}(_{},Q,s^{1:t}) \]

[MISSING_PAGE_FAIL:5]

Likewise, the performance of our approach is constrained by the low diversity of LLM generations on Sporting Understanding, as we observe on Object Counting in symbolic reasoning.

Computational Cost Overhead.Despite the fact that our approach achieves significant improvement on various benchmarks, we observe an overhead of computational cost compared with the corresponding baselines. For example, the single-chain version of our approach using PAL costs about \(3\) times more than the self-consistency baseline on GSM8K. As detailed in Appendix A.3, this is due to a relatively large hyperparameter - the number of rollouts per beam \(n\) - which we set as \(16\) for better performance. To strike a balance between performance and cost and present a complete picture, we adopt \(n=2\) and conduct cost-performance analysis on our approach in Section 3.3.

### Cost Analysis

Table 3 compares the baseline and our approach under comparable computational budgets (measured in # Tokens). Our method consistently outperforms self-consistency on the arithmetic reasoning tasks even when benchmarked for relatively less computational cost. For example, we achieve \(46.1\%\) on GSM8K with a cost of \(12.6k\) tokens, compared with the accuracy of \(41.8\%\) of self-consistency which costs \(13.9k\) tokens. Figure 4 further illustrates the cost-efficiency of our approach on GSM8K using different prompting methods under various levels of costs. Our approach significantly outperforms the corresponding equal-cost baseline especially when the computational budget increases, indicating the improvement in the performance upper bound brought by our method.

However, our approach lags behind the CoT baseline on commonsense reasoning. This implies the limitation of our method when applied to shorter reasoning chains, _i.e._, decreasing the number

   Approach &  &  \\  & GSM8K & \# Tokens & AQuA & SVAMP & ASDiv & TabMWP & DATE & OBJECT \\   \\  CoT & \(65.6\) & \(0.2k\) & \(45.3\) & \(74.8\) & \(76.9\) & \(65.2\) & \(64.8\) & \(73.0\) \\ PoT & \(71.6\) & \(-\) & \(54.1\) & \(85.2\) & \(-\) & \(73.2\) & \(-\) & \(-\) \\ PAL & \(72.0\) & \(0.3k\) & \(-\) & \(79.4\) & \(79.6\) & \(-\) & \(76.2\) & \(96.7\) \\ Ours-PAL & \(80.2\) & \(27.7k\) & \(55.9\) & \(89.6\) & \(84.9\) & \(79.1\) & \(\) & \(\) \\   \\  CoT, SC & \(78.0\) & \(5.3k\) & \(52.0\) & \(86.8\) & \(-\) & \(75.4\) & \(-\) & \(-\) \\ CoT, Diverse & \(82.3\) & \(-\) & \(-\) & \(87.0\) & \(\) & \(-\) & \(-\) & \(-\) \\ PoT, SC & \(80.0\) & \(-\) & \(58.6\) & \(89.1\) & \(-\) & \(\) & \(-\) & \(-\) \\ PAL, SC & \(80.4\) & \(7.4k\) & \(-\) & \(-\) & \(-\) & \(-\) & \(-\) & \(-\) \\ Ours-PAL & \(\) & \(550.0k\) & \(\) & \(\) & \(85.8\) & \(80.9\) & \(-\) & \(-\) \\   

Table 1: Result comparison (accuracy \(\%\)) on arithmetic and symbolic reasoning tasks. The best result is in **bold** and the lowest cost is in green. We report methods all with Codex backbone for a fair comparison. Similar to Huang et al. (2022), Diverse Li et al. (2022) fine-tune task-specific verifiers to apply weights on samples in self-consistency (SC). Other fine-tuning methods include reward-based supervision Uesato et al. (2022) and content-specific training Lewkowycz et al. (2022). We also report the number of tokens (# Tokens) on GSM8K to compare the costs of different methods.

   Approach & StrategyQA & \# Tokens & CommonsenseQA & Sports \\  CoT & \(73.2\) & \(0.06k\) & \(77.9\) & \(\) \\ Ours-CoT & \(\) & \(11.6k\) & \(\) & \(98.4\) \\  Human & \(87.0\) & \(-\) & \(88.9\) & \(-\) \\   

Table 2: Result comparison (accuracy \(\%\)) on commonsense reasoning tasks, with Codex backbone. Here we only report results in the single reasoning chain scenario following Wei et al. (2022). We report # Tokens on StrategyQA for cost comparison.

of intermediate steps weakens the effect of stepwise self-evaluation in beam search in reducing error accumulation. On the other hand, self-consistency can directly improve performance through instance-level aggregation without additional cost for self-evaluation. We analyze how our method benefits longer reasoning chains on different tasks in Section 3.4.

### Further Analysis

We now provide a detailed analysis of why our method achieves significant gains.

Generation and Self-evaluation Calibration.We investigate the distributions of generation confidence (, the LM probability \(\)) and correctness confidence \(\) in our self-evaluation score \(\). By

Figure 4: Accuracy curves on GSM8K of different methods with the change of the cost. We conduct the performance comparison using both PAL and CoT prompting with Llama-2 (13B) backbone.

    &  &  \\  & GSM8K & AQuA & SVAMP & ASDiv & TabMWP & StrategyQA & CommonsenseQA \\  Baseline & \(41.8\) & \(30.7\) & \(71.2\) & \(66.2\) & \(43.7\) & \(\) & \(\) \\ \# Tokens & \(13.9k\) & \(6.6k\) & \(5.9k\) & \(2.7k\) & \(1.9k\) & \(2.7k\) & \(1.2k\) \\  Ours & \(\) & \(\) & \(\) & \(\) & \(\) & \(70.6\) & \(74.0\) \\ \# Tokens & \(12.6k\) & \(6.0k\) & \(5.0k\) & \(2.5k\) & \(1.2k\) & \(2.6k\) & \(1.2k\) \\   

Table 3: Cost (# Tokens) and result (accuracy \(\%\)) comparison on arithmetic and commonsense reasoning tasks. We base our experiments on Llama-2 (13B) since Codex is not available. We show the results of the baseline and our method both in the multiple-chain scenario for a fair comparison. Here we use PAL and CoT prompting for arithmetic and commonsense reasoning, respectively.

Figure 5: Distributions of the self-evaluation score and its components (, generation confidence \(\) and correctness confidence \(\)) on correct/incorrect baseline predictions. We highlight the median scores of the positive and negative cases using lines of the same colors respectively.

comparing the score distributions for correct and wrong predictions, we aim to gain an intuitive understanding of whether these confidence scores are reliable. Figure 5 shows different score distributions on correct and wrong baseline predictions. The difference in distribution between the two prediction sets is substantial for arithmetic reasoning, but negligible for commonsense reasoning. Notably, in both instances, correctness confidence is more discriminatory than generation confidence.

To achieve a balance between these two confidence scores, we utilize a tunable hyperparameter \(\), setting \(=0.5\) for all datasets. Nevertheless, varying its value can lead to distinct outcomes. For instance, when setting \(\) to \(1\) (\(=\)) or \(0\) (\(=\)), the performance on GSM8K decreases from \(80.2\%\) to \(74.5\%\) and \(77.1\%\), respectively. This indicates that both scores play a crucial role in our final performance. A more comprehensive analysis of \(\) can be found in Appendix A.2.

Reasoning Complexity. We investigate if our approach is more beneficial for instances needing more reasoning steps. Table 4 shows that performance gains (in absolute accuracy \(\%\) increase) increase as reasoning chains become longer on both GSM8K and StrategyQA. Notably, the improvement on StrategyQA primarily comes from improvements in longer reasoning chains, showcasing the effectiveness of our method in navigating lengthy and intricate reasoning chains.

Hyperparameters in Stochastic Beam Search.We examine the significance of hyperparameters associated with stochastic beam search, including the beam size \(k\) and the temperatures \(\) and \(\) controlling the generation and sampling diversity, respectively.

Figure 5(a) shows the trend of performance improvement with the increase of beam size \(k\). Notably, our beam search approach inherently enables majority voting on the final beam without additional cost, resulting in a more significant performance improvement in the multiple-chain reasoning when the beam size is larger (_e.g._, \(42.8\%\) compared with \(35.7\%\) when \(k=10\)).

For generation and sampling diversity, it is clear that more diversity resulting from higher temperatures generally leads to a decline in performance when only considering a single reasoning chain. However, diversity significantly benefits majority voting on multiple reasoning chains 6. This benefit comes

    &  &  \\ \# Steps & \# Ins. & PAL & Ours & \(\)Accu. & \# Steps & \# Ins. & CoT & Ours & \(\)Accu. \\  \(<7\) & \(437\) & \(85.8\) & \(91.3\) & \(+5.49\) & \(<4\) & \(637\) & \(84.6\) & \(84.9\) & \(+0.31\) \\ \((7,9]\) & \(524\) & \(74.8\) & \(82.6\) & \(+7.82\) & \([4,5)\) & \(1,301\) & \(78.6\) & \(79.1\) & \(+0.46\) \\ \( 9\) & \(358\) & \(72.9\) & \(82.6\) & \(+9.78\) & \( 5\) & \(351\) & \(68.4\) & \(71.8\) & \(+3.42\) \\   

Table 4: Absolute accuracy (in \(\%\)) increases on instances of different complexity determined by the length of reasoning chains (represented as # Steps).

Figure 6: Accuracy curves and distributions of our approach on GSM8K with different hyperparameter settings: (a) Changes in performance (Llama-2 backboned) when the beam size \(k\) varies. Methods of the same \(k\) have equal computational costs; (b) Accuracy distributions (Codex backboned) with different generation temperature \(\) and sampling temperature \(\) (with decay ratio \(\)).

from the improved coverage of the plausible generations and the ensembling effect. Nevertheless, one can adjust the sampling-related parameters (_i.e._, \(\) and \(\)) to incorporate more randomness into the generations. In practice, we find that a moderate temperature decay (_e.g._, \(=0.5\)) results in improved performance. We conduct further analysis of the effect of sampling diversity in Appendix A.2.

Qualitative Analysis.We examine particular instances to investigate the behavior of correctness confidence scores \(\) and generation probabilities \(\) in different scenarios. From the comparison shown in Figure 7, we have the following main observations:

\(\) In general, the correctness confidence is more effective at identifying logical errors, taking into account the accumulated mistakes from prior steps, while the generation probability focuses more on text perplexity as the confidence of the generation LLM.

\(\) When comparing arithmetic and commonsense tasks, LLMs exhibit greater confidence in dealing with structured and objective reasoning chains such as problems in GSM8K, for both generation and self-evaluation, as opposed to reasoning chains in StrategyQA.

\(\) Reasoning chains that appear logically plausible can achieve high correctness confidence scores but still result in incorrect answers, as demonstrated in \(R_{41}\) in Figure (b)b. Moreover, the correctness confidence can be influenced by minor details (_e.g._, imperfect variable naming in PAL reasoning) and assign low scores regardless of the correctness of the final answers as shown in \(R_{22}\) in Figure (a)a.

\(\) Incoherence due to a sudden jump in reasoning (_e.g._, \(R_{32}\) in Figure (b)b) can lead to low correctness confidence. Additionally, the correctness confidence tends to be lower when the generation LLM makes a probability statement with less certainty, such as "it seems" as illustrated by \(R_{42}\) in Figure (b)b.

Figure 7: Comparisons among predictions of high and low self-evaluation scores on arithmetic (7a for GSM8K) and commonsense (7b for StrategyQA) reasoning tasks. Scores from low to high are visualized from orange (\(0.0\)), yellow (\(0.4\)), to green (\(1.0\)). Here \(,\), and \(\) represent the evaluation confidence, the generation confidence, and their combination as the final score, respectively.

Related Work

Reasoning Formulation.Several studies have attempted to better formulate the reasoning problem. One approach is to generate rationales to enhance model interpretability (Zhou et al., 2020; Wiegreffe and Marasovic, 2021; Wiegreffe et al., 2021). Recently, the focus has shifted towards decomposing the reasoning process into intermediate steps before reaching the final answer (Wei et al., 2022; Zhou et al., 2023; Gao et al., 2023; Chen et al., 2022). Various decomposition techniques have been explored, such as question reduction (Zhou et al., 2023; Yang et al., 2022), iterative prompting (Wang et al., 2022), and chaining the steps (Wu et al., 2022). While incorporating intermediate reasoning steps has resulted in substantial performance improvements, errors or imperfections can accumulate, especially when the chains become longer (Wu et al., 2016; Guo et al., 2018). As such, we utilize LLM self-evaluation as a stepwise criterion to improve the chaining process.

LLM Self-Evaluation.Recent research on LLM calibration shows that current LLMs' probabilistic predictions correspond well with actual token occurrence frequencies, leading to well-calibrated predictions for specific tasks (Rae et al., 2021; Kadavath et al., 2022; Guo et al., 2017; Kadavath et al., 2022; Jiang et al., 2021; Kuhn et al., 2023). Notably, scaling model size plays a crucial role in enhancing calibration (Rae et al., 2021; Wei et al., 2022). As LLMs exhibit good calibration, an increasing number of studies focus on prompting LLMs to perform self-evaluation as a means of verification (Zhang et al., 2023; Shinn et al., 2023; Madaan et al., 2023; Paul et al., 2023). Self-evaluation provides an effective and efficient assessment method without requiring task-specific verifier fine-tuning, which typically involves additional annotations (Li et al., 2022). In contrast to existing works that refine generation results through instance-level self-evaluation, our approach applies self-evaluation results as a stepwise criterion to calibrate generation at a finer granularity. By focusing on step-by-step self-evaluation, our method enables fine-grained guided decoding, addressing the challenges associated with complex or lengthy reasoning.

Decoding Strategies.A tradeoff typically exists between diversity and quality. Deterministic decoding methods such as greedy decoding and beam search (Jurafsky and Martin, 2009; Graves, 2012) often produce high-quality results but lack diversity (Stahlberg and Byrne, 2019; Meister et al., 2020). Temperature sampling (Ackley et al., 1985), top-\(k\) sampling (Fan et al., 2018), and top-\(p\) sampling (Holtzman et al., 2020) are various techniques used to enhance diversity. The recent work of _tree-of-thought_(Yao et al., 2023) explores different search algorithms such as breadth-first and depth-first searches tailored for different tasks. Differently, we propose a unified framework of stochastic beam search (Caccia et al., 2020; Kool et al., 2019; Meister et al., 2021), which combines beam search and temperature sampling to balance the quality-diversity trade-off in multi-step reasoning.

## 5 Discussion

We have introduced a multi-step decoding method that calibrates reasoning with stepwise self-evaluation guidance via stochastic beam search for current large language models. The empirical success of our method across a broad range of tasks, from arithmetic and symbolic to commonsense reasoning, demonstrates its robustness and generalizability in various application areas. The significant performance gains of our method on long reasoning chains also highlight its applicability to other multi-step tasks, such as multi-hop question answering and more complex scenarios involving multi-modal understanding, reasoning, and planning. In future work, we will investigate how to utilize external tools to further enhance the calibration and explore its generalizability on other multi-step scenarios to deal with more complex information such as external knowledge and multimodalities.

### Potential Impacts and Limitations

We propose self-evaluation guided stochastic beam search to facilitate multi-step reasoning. However, our approach, based on stepwise self-evaluation guidance, has certain limitations. It requires access to LLM logits to calculate the self-evaluation score, restricting its applicability to more powerful LLMs, such as GPT-4, which do not provide token likelihoods. Plus, multi-step decoding inherently causes additional costs from candidate sampling and self-evaluation. For optimal balance between efficiency and cost, our approach is best applied to longer reasoning chains, where the cumulative effect of calibration across multiple steps can improve the overall performance more significantly.