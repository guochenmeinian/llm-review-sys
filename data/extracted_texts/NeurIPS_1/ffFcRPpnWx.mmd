# RS-Del: Edit Distance Robustness Certificates for Sequence Classifiers via Randomized Deletion

Zhuoqun Huang

University of Melbourne

zhuoqun@unimelb.edu.au

&Neil G. Marchant

University of Melbourne

nmarchant@unimelb.edu.au

&Keane Lucas

Carnegie Mellon University

keanelucas@cmu.edu

Lujo Bauer

Carnegie Mellon University

lbauer@cmu.edu

&Olga Ohrimenko

University of Melbourne

oohrimenko@unimelb.edu.au

&Benjamin I. P. Rubinstein

University of Melbourne

brubinstein@unimelb.edu.au

###### Abstract

Randomized smoothing is a leading approach for constructing classifiers that are certifiably robust against adversarial examples. Existing work on randomized smoothing has focused on classifiers with continuous inputs, such as images, where \(_{p}\)-norm bounded adversaries are commonly studied. However, there has been limited work for classifiers with discrete or variable-size inputs, such as for source code, which require different threat models and smoothing mechanisms. In this work, we adapt randomized smoothing for discrete sequence classifiers to provide certified robustness against edit distance-bounded adversaries. Our proposed smoothing mechanism _randomized deletion_ (RS-Del) applies random deletion edits, which are (perhaps surprisingly) sufficient to confer robustness against adversarial deletion, insertion and substitution edits. Our proof of certification deviates from the established Neyman-Pearson approach, which is intractable in our setting, and is instead organized around longest common subsequences. We present a case study on malware detection--a binary classification problem on byte sequences where classifier evasion is a well-established threat model. When applied to the popular MalConv malware detection model, our smoothing mechanism RS-Del achieves a certified accuracy of 91% at an edit distance radius of 128 bytes.

## 1 Introduction

Neural networks have achieved exceptional performance for classification tasks in unstructured domains, such as computer vision and natural language. However, they are susceptible to adversarial examples--inconspicuous perturbations that cause inputs to be misclassified . While a multitude of defenses have been proposed against adversarial examples, they have historically been broken by stronger attacks. For instance, six of nine defense papers accepted for presentation at ICLR2018 were defeated months before the conference took place ; another tranche of thirteen defenses were circumvented shortly after . This arms race between attackers and defenders has led the community to investigate certified robustness, which aims to guarantee that a classifier's prediction cannot be changed under a specified set of adversarial perturbations .

Most prior work on certified robustness has focused on classifiers with _continuous fixed-dimensional_ inputs, under the assumption of \(_{p}\)-norm bounded perturbations. A variety of certification methods have been proposed for this setting, including deterministic methods which bound a network's output by convex relaxation [5; 7; 6; 8] or composing layerwise bounds [9; 10; 11; 12], and randomized smoothing which obtains high probability bounds for a base classifier smoothed under random noise [13; 14; 15]. Randomized smoothing has achieved state-of-the-art certified robustness against \(_{2}\)-norm bounded perturbations on ImageNet, correctly classifying 71% of a test set under perturbations with \(_{2}\)-norm up to \(127/255\) (half maximum pixel intensity) . However, real-world attacks go beyond continuous modalities: a range of attacks have been demonstrated against models with discrete inputs, such as binary executables [17; 18; 19; 20], source code , and PDF files .

Unfortunately, there has been limited investigation of certified robustness for discrete modalities, where prior work has focused on perturbations with bounded Hamming (\(_{0}\)) distance [23; 24; 25], in some cases under additional constraints [26; 27; 28; 29; 30]. While this work covers attackers that overwrite content, it does not cover attackers that insert/delete content, which is important for instance in the malware domain [17; 31]. One exception is Zhang et al.'s method for certifying robustness under string transformation rules that may include insertion and deletion, however their approach is only feasible for small rule sets and is limited to LSTMs . Liu et al.  also study certification for a threat model that includes insertion/deletion in the context of point cloud classification.

In this paper we develop a comprehensive treatment of _edit distance certifications_ for sequence classifiers. We consider input sequences of varying length on finite domains. We cover threat models where an adversary can arbitrarily perturb sequences by substitutions, insertions, and deletions or any subset of these operations. Moreover, our framework encompasses adversaries that apply edits to blocks of tokens at a time. Such threat models are motivated in malware analysis, where attackers are likely to edit semantic objects in an executable (e.g., whole instructions), rather than at the byte-level. We introduce tunable decision thresholds, to adjust decision boundaries while still forming sound certifications. This permits trading-off misclassification rates and certification radii between classes, and is useful in settings where adversaries have a strong incentive to misclassify malicious instances as benign [34; 35].

We accomplish our certifications using randomized smoothing [13; 14], which we instantiate with a general-purpose deletion smoothing mechanism called RS-Del. Perhaps surprisingly, RS-Del does not need to sample all edit operations covered by our certifications. By smoothing using deletions only, we simplify our certification analysis and achieve the added benefit of improved efficiency by querying the base classifier with shorter sequences. RS-Del is compatible with arbitrary base classifiers, requiring only oracle query access such as via an inference API. To prove our robustness certificates, we have to deviate from the standard Neyman-Pearson approach to randomized smoothing, as it is intractable in our setting. Instead, we organize our proof around a representative longest common subsequence (LCS): the LCS serves as a reference point in the smoothed space between an input instance and a neighboring instance, allowing us to bound the confidence of the smoothed model between the two.

Finally, we present a comprehensive evaluation of RS-Del in the malware analysis setting1. We investigate tradeoffs between accuracy and the size of our certificates for varying levels of deletion, and observe that RS-Del can achieve a certified accuracy of 91% at an edit distance radius of 128 bytes using on the order of \(10^{3}\) model queries. By comparison, a brute force certification would require in excess of \(10^{308}\) model queries to certify the same radius. We also demonstrate asymmetric certificates (favouring the malicious class) and certificates covering edits at the machine instruction level by leveraging chunking and information from a disassembler. Finally, we assess the empirical robustness of RS-Del to several attacks from the literature, where we observe a reduction in attack success rates.

## 2 Preliminaries

Sequence classificationLet \(=^{}\) represent the space of finite-length sequences (including the empty sequence) whose elements are drawn from a finite set \(\). For a sequence \(\), we denote its length by \(||\) and the element at position \(i\) by \(x_{i}\) where \(i\) runs from \(1\) to \(||\). We consider classifiers that map sequences in \(\) to \(K\) classes in \(=\{0,,K-1\}\). For example, in our case study on malware detection, we take \(\) to be the space of byte sequences (binaries), and \(\) to be the set of bytes and \(\) to be \(\{0,1\}\) where 0 and 1 denote benign and malicious binaries respectively.

Robustness certificationGiven a classifier \(f\), an input \(\) and a neighborhood \(()\) around \(\), a _robustness certificate_ is a guarantee that the classifier's prediction is constant in the neighborhood, i.e.,

\[f()=f(^{})^{}(). \]

When the neighborhood corresponds to a ball of radius \(r\) centered on \(\), we make the dependence on \(r\) explicit by writing \(_{r}()\). We adopt the paradigm of _conservative_, _probabilistic_ certification, which is a natural fit for randomized smoothing . Under this paradigm, a certifier may either assert that (1) holds with high probability, or decline to assert whether (1) holds or not.

Edit distance robustnessMost existing work on robustness certification focuses on classifiers that operate on _fixed-dimensional_ inputs in \(^{d}\), where the neighborhood of certification is an \(_{p}\)-ball . However, \(_{p}\) robustness is not well motivated for sequence classifiers: \(_{p}\) neighborhoods are limited to constant-length sequences, and the norm is ill-defined for sequences with non-numeric elements. For example, an \(_{p}\) neighborhood around a byte sequence like \(=(78,7,2,00)\) must include sequences additively perturbed by real-valued sequences like \(=(-0.09,0.07,0.01,0.1)\), which clearly results in a type mismatch. Even if one focuses on robustness for length-preserving sequence perturbations, the \(_{p}\) neighborhood is a poor choice because it excludes sequences that are even slightly misaligned. Motivated by these shortcomings, we consider _edit distance_ robustness.

Edit distance is a natural measure for comparing sequences. Given a set of elementary edit operations (ops) \(O\), the edit distance \(_{O}(,^{})\) is the minimum number of ops required to transform sequence \(\) into \(^{}\). We consider three ops: delete a single element (del), insert a single element (ins), and substitute one element with another (sub). For generality, we allow \(O\) to be any combination of these ops. When \(O=\{,,\}\) the edit distance is known as Levenshtein distance . A primary goal of this paper is to produce edit distance robustness certificates, where the neighborhood of certification is an edit distance ball \(_{r}()=\{^{}:_{O}( {x}^{},) r\}\).

Threat modelWe consider an adversary that has full knowledge of our base and smoothed models, source of randomness, certification scheme, and possesses unbounded computation. The attacker makes edits from \(O\) up to some budget, in order to misclassify a target \(\). In the context of our experimental case study on malware detection, edit distance is a reasonable proxy for the cost of running evasion attacks that iteratively apply localized functionality-preserving edits (e.g., ). Since the edit distance scales roughly linearly with the number of attack iterations, the adversary has an incentive to minimize edit distance for these attacks.

## 3 RS-Del: Randomized deletion smoothing

In this section, we propose a method for constructing sequence classifiers that are certifiably robust under bounded edit distance perturbations. Our method RS-Del extends randomized smoothing with a novel deletion mechanism and tunable decision thresholds. We review randomized smoothing in Section 3.1 and describe our deletion mechanism in Section 3.2, along with its practical aspects in Section 3.3. We summarize the certified robustness guarantees of our method in Table 1 and defer their derivation to Section 4.

### Randomized smoothing

Let \(f_{}:\) be a _base_ classifier and \(()\) be a _smoothing mechanism_ that maps inputs to a distributions over (perturbed) inputs. Randomized smoothing composes \(f_{}\) and \(\) to construct a new _smoothed_ classifier \(f\). For any input \(\), the smoothed classifier's prediction is

\[f():=*{arg\,max}_{y}\{p_{y}(;f_{ },)-_{y}\}, \]

where \(=\{_{y}\}_{y}\) is a set of real-valued tunable decision thresholds and

\[p_{y}(;f_{},)=_{()}[f_{ }()=y] \]is the probability that the base classifier \(f_{}\) predicts class \(y\) for a perturbed input drawn from \(()\). We omit the dependence of \(p_{y}\) on \(f_{}\) and \(\) where it is clear from context.

The viability of randomized smoothing as a method for achieving certified robustness is strongly dependent on the smoothing mechanism. Ideally, the mechanism should be chosen to yield a smoothed classifier with improved robustness under the chosen threat model, while minimizing any drop in accuracy compared to the base classifier. The mechanism should also be amenable to analysis, so that a tractable robustness certificate can be derived.

_Remark 1_.: Previous definitions of randomized smoothing (e.g., [13; 14]) do not incorporate decision thresholds, and effectively assume \(=0\). We introduce decision thresholds as a way to trade off error rates and robustness between classes. This is useful when there is asymmetry in misclassification costs across classes. For instance, in our case study on malware detection, robustness of benign examples is less important because adversaries have limited incentive to trigger misclassification of benign examples [34; 35; 40]. We note that the base classifier may also be equipped with decision thresholds, which provide another degree of freedom to trade off error rates between classes.

### Randomized deletion mechanism

We propose a smoothing mechanism that achieves certified edit distance robustness. Our smoothing mechanism perturbs a sequence by deleting elements at random, and is called _randomized deletion_, or RS-Del for short.

Consider a sequence \(\) whose elements are indexed by the set \([]=\{1,,||\}\). We specify the distribution of \(()\) for RS-Del in two stages. In the first stage, a random edit \(\) is drawn from a distribution \(G()\) over the space of possible edits to \(\), denoted \(()\). Since we only consider deletions for smoothing, any edit can be represented by the set of element indices in \([]\) that _remain_ after deletion. Hence \(()\) is taken as the powerset of \([]\). We specify edit distribution \(G()\) so that each element is deleted i.i.d. with probability \(p_{}(0,1)\):

\[[G()=]=_{i=1}^{||}p_{}^{_{i }}(1-p_{})^{_{i}}, \]

where \(_{A}\) denotes the indicator function, which returns 1 if \(A\) evaluates to true and 0 otherwise. In the second stage, the edit \(\) is applied to \(\) to yield the perturbed sequence:

\[=(,)(x_{_{(i)}}) _{i=1||}, \]

where \(_{(i)}\) denotes the \(i\)-th smallest index in \(\). The perturbed sequence \(\) is guaranteed to be a subsequence of \(\). Putting both stages together, the distribution of \(()\) is

\[[()=]=_{()}[G()= ]_{(,)=}. \]

_Remark 2_.: It may be surprising that we are proposing a smoothing mechanism for certified edit distance robustness that does not use the full set of edit ops \(O\) covered by the threat model. It is a misconception that randomized smoothing requires perfect alignment between the mechanism and the threat model. All that is needed from a robustness perspective, is for the mechanism to return distributions that are statistically close for any pair of inputs that are close in \(O\) edit distance; this can be achieved solely with deletion. In fact, perfect alignment is known to be suboptimal for some \(_{p}\) threat models . Our deletion mechanism leads to a tractable robustness certificate covering the full set of edit ops (see Section 4). Moreover while benefiting robustness, our empirical results show that our deletion mechanism has only a minor impact on accuracy (see Section 5). Finally, our deletion mechanism reduces the length of the input, which is beneficial for computational efficiency (see Appendix G). This is not true in general for mechanisms employing insertions/substitutions.

### Practical considerations

We now discuss considerations for implementing and certifying RS-Del in practice. In doing so, we reference theoretical results for certification, which are covered later in Section 4.

Probabilistic certificationRandomized smoothing does not generally support exact evaluation of the classifier's confidence \(_{y}:=p_{y}()\), which is required for exact prediction and exact evaluation of the certificates we develop in Section 4. While \(_{y}\) can be evaluated exactly for RS-Del by enumerating over the possible edits \(()\), the computation scales exponentially in \(||\) (see Appendix A). Since this is infeasible for even moderately-sized \(\), we follow standard practice in randomized smoothing and estimate \(_{y}\) with a lower confidence bound using Monte Carlo sampling . This procedure is described in pseudocode in Figure 1: lines 1-3 estimate the predicted class \(y=f()\), lines 4-5 compute a \(1-\) lower confidence bound on \(_{y}\), and line 6 uses this information and the results in Table 1 to compute a probabilistic certificate that holds with probability \(1-\). If the lower confidence bound on \(_{y}\) exceeds the corresponding detection threshold \(_{y}\), the prediction and certificate are returned (line 8), otherwise we _abstain_ due to lack of statistical significance (line 7).

TrainingWhile randomized smoothing is compatible with any base classifier, it generally performs poorly for conventionally-trained classifiers . We therefore train base classifiers specifically to be used with RS-Del, by replacing original sequences with perturbed sequences (drawn from \(\)) at training time. This has been shown to achieve good empirical performance in prior work .

Sequence chunkingSo far, we have described edit distance robustness and RS-Del assuming edits are applied at the level of sequence elements. However in some applications it may be reasonable to assume edits are applied at a coarser level, to contiguous chunks of sequence elements. For example, in malware analysis, one can leverage information from a disassembler to group low-level sequence elements (bytes) into more semantically meaningful chunks, such as machine instructions, addresses and header fields (see Appendix C.3). Our methods are compatible with chunking--we simply reinterpret the sequence as a sequence of chunks, rather than a sequence of lower-level elements. This can yield a tighter robustness certificate, since edits within chunks are excluded.

## 4 Edit distance robustness certificate

We now derive an edit distance robustness certificate for RS-Del. We present the derivation in three parts: Section 4.1 provides an outline, Section 4.2 derives a lower bound on RS-Del's confidence score and Section 4.3 uses the bound to complete the derivation. All proofs are presented in Appendix B.

### Derivation outline

Following prior work [14; 13; 41], we derive an edit distance robustness certificate that relies on limited information about RS-Del. We allow the certificate to depend on the input \(\), the smoothed prediction \(y=f()\), the confidence score \(_{y}=p_{y}(;f_{})\), the decision threshold \(_{y}\), and the architecture of \(f\), including the deletion smoothing mechanism \(\), but excluding the architecture of

    \\  del & ins & sub & Certified radius \(r^{}\) \\   & ✓ & & \(}{1-_{y}}/_{}\) \\ ✓ & & & \(}{_{y}}/_{}\) \\ ✓ & ✓ & & \(}{_{y}}/_{}\) \\ ✓ & ✓ & ✓ & \((1+_{y}-_{y})/_{}\) \\  & & ✓ & \((1+_{y}-_{y})/_{}\) \\  & ✓ & ✓ & \((1+_{y}-_{y})/_{}\) \\ ✓ & & ✓ & \((1+_{y}-_{y})/_{}\) \\   

Table 1: Edit distance robustness certificates for RS-Del as a function of the edit ops \(O\) used to define the edit distance. Here \(_{y}\) is the confidence for predicted class \(y\) and \(_{y}\) is a threshold derived from \(\) defined in (10).

Figure 1: Probabilistic certification of RS-Del. Here \(\) is the input sequence, \(f_{}\) is the base classifier, \(p_{}\) is the deletion probability, \(\) is the set of decision thresholds, \(\) is the significance level, and \(n_{},n_{}\) are sample sizes. BinLCB\((k,n,)\) returns a lower confidence bound for \(p\) at level \(\) given \(k(n,p)\).

the base classifier \(f_{}\). Limiting the dependence in this way improves tractability and ensures that the certificate is applicable for any choice of base classifier \(f_{}\). Formally, the only information we assume about \(f_{}\) is that it is some classifier in the feasible base classifier set:

\[(,_{y})=\{\,h:_{y}=p _{y}(;h)\,\}\,. \]

Recall that an edit distance robustness certificate at radius \(r\) for a classifier \(f\) at input \(\) is a guarantee that \(f()=f(})\) for any perturbed input \(}\) in the neighborhood \(_{r}()=\{}:_{O}(},) r\}\). We observe that this guarantee holds for RS-Del in the limited information setting iff the \(\)-adjusted confidence for predicted class \(y\) exceeds the \(\)-adjusted confidence for any other class for all perturbed inputs \(}\) and feasible base classifiers \(h\):

\[p_{y}(};h)-_{y}_{y^{} y}\{p_{y^{}}( {};h)-_{y^{}}\}}_{r}(),h(,_{y}). \]

To avoid dependence on the confidence of the runner-up class, which is inefficient for probabilistic certification, we work with the following more convenient condition.

**Proposition 3**.: _A sufficient condition for (8) is \((,_{y})_{y}()\) where_

\[(,_{y})_{}_{r}()} _{h(,_{y})}p_{y}(};h) \]

_is a tight lower bound on the confidence for class \(y\), and we define the threshold_

\[_{y}()=+_{y}-_{y^{} y} _{y^{}},&_{y}_{y^{} y}_{y^{}}||>2,\\ 1+_{y}-_{y^{} y}_{y^{}},&_{y}<_{y^{}  y}_{y^{}}||>2,\\ -_{y^{} y}_{y^{}}}{2},&||=2. \]

The standard approach for evaluating \((,_{y})\) is via the Neyman-Pearson lemma [42; 14], however this seems insurmountable in our setting due to the challenging geometry of the edit distance neighborhood. We therefore proceed by deriving a loose lower bound on the confidence \((,_{y})(,_{y})\), noting that the robustness guarantee still holds so long as \((,_{y})>_{y}()\). The derivation proceeds in two steps. In the first step, covered in Section 4.2, we derive a lower bound for the inner minimization in (9), which we denote by \((},,_{y})\). Then in the second step, covered in Section 4.3, we complete the derivation by minimizing \((},,_{y})\) over the edit distance neighborhood. Our results are summarized in Table 1, where we provide certificates under various constraints on the edit ops.

### Minimizing over feasible base classifiers

In this section, we derive a loose lower bound on the classifier confidence with respect to feasible base classifiers

\[(},,_{y})(},,_{y}) =_{h(,_{y})}p_{y}(};h). \]

To begin, we write \(p_{y}(};h)\) as a sum over the edit space by combining (3) and (6):

\[p_{y}(};h)=_{(})}s(,};h) s(,} ;h)=[G(})=]_{h(( {},))=y}. \]

We would like to rewrite this sum in terms of the known confidence score at \(\), \(_{y}=p_{y}(;h)=_{()}s(,;h)\). To do so, we identify pairs of edits \(\) to \(}\) and \(\) to \(\) for which the corresponding terms \(s(,};h)\) and \(s(,;h)\) are proportional.

**Lemma 4** (Equivalent edits).: _Let \(^{*}\) be a longest common subsequence (LCS)  of \(}\) and \(\), and let \(^{*}(})\) and \(^{*}(})\) be any edits such that \((},^{*})=(, ^{*})=^{*}\). Then there exists a bijection \(m:2^{^{*}} 2^{^{*}}\) such that \((},)=(,)\) for any \(^{*}\) and \(=m()\). Furthermore, we have \(s(,};h)=p^{|}||-||}_{}s( ,;h)\)._

Applying this proportionality result to all pairs of edits \(,\) related under the bijection \(m\) yields:

\[_{ 2^{^{*}}}s(,};h)=p^{| }||-||}_{}_{ 2^{^{*}}}s( ,;h).\]Thus we can achieve our goal of writing \(p_{y}(};h)\) in terms of \(_{y}\). A rearrangement of terms gives:

\[p_{y}(};h)=p_{}^{|}|-||}(_{y}- _{ 2^{*}}s(,;h))+_{ 2^{*}}s(,};h). \]

This representation is convenient for deriving a lower bound. Specifically, we can drop the sum over \( 2^{*}\) and upper-bound the sum over \( 2^{*}\) to obtain a lower bound that is independent of \(h\).

**Theorem 5**.: _For any pair of inputs \(}\), \(\) we have_

\[(},,_{y})(},,_{y}) =p_{}^{|}|-||}(_{y}-1+p_{}^{ (_{}(},)+|| -|}|)}). \]

_where \(_{}(},)\) is the longest common subsequence (LCS) distance2 between \(}\) and \(\)._

### Minimizing over the edit distance neighborhood

In this section, we complete the derivation of our robustness certificate by minimizing the lower bound in (14) over the edit distance neighborhood:

\[(;_{y})=_{}_{r}()} (},,_{y}). \]

We are interested in general edit distance neighborhoods, where the edit ops \(O\) used to define the edit distance may be constrained. For example, if the attacker is capable of performing elementary substitutions and insertions, but not deletions, then \(O=\{,\}\). As a step towards solving (15), it is therefore useful to express \((},,_{y})\) in terms of edit op counts, as shown below.

**Corollary 6**.: _Suppose there exists a sequence of edits from \(}\) to \(\) that consists of \(n_{}\) substitutions, \(n_{}\) insertions and \(n_{}\) deletions s.t. \(n_{}+n_{}+n_{}=_{O}( },)\) and \(n_{},n_{},n_{} 0\). Then_

\[(},,_{y})=p_{}^{n_{}-n _{}}(_{y}-1+p_{}^{n_{}+n_{}}).\]

This parameterization of the lower bound enables us to re-express (15) as an optimization problem over edit ops counts:

\[(;_{y})=_{n_{},n_{},n_{ }_{r}}p_{}^{n_{}-n_{}}(_{y}-1+p_{}^{n_{}+n_{}} ), \]

where \(_{r}\) encodes constraints on the set of counts. If any number of insertions, deletions or substitutions are allowed, then the edit distance is known as the _Levenshtein distance_ and \(_{r}\) consists of sets of counts that sum to \(r\). We solve the minimization problem for this case below.

**Theorem 7** (Levenshtein distance certificate).: _A lower bound on the classifier's confidence within the Levenshtein distance neighborhood \(_{r}()\) (with \(O=\{,,\}\)) is \((;_{y})=_{y}-1+p_{}^{r}\). It follows that the classifier is certifiably robust for any Levenshtein distance ball with radius \(r\) less than or equal to the certified radius \(r^{}=(1+_{y}()-_{y})/ p_{}\)._

It is straightforward to adapt this result to account for constraints on the edit ops \(O\). Results for all combinations of edit ops are provided in Table 1.

So far in this section we have obtained results that depend on the classifier's confidence \(_{y}\), assuming it can be evaluated exactly. However since exact evaluation of \(_{y}\) is not feasible in general (see Section 3.3), we extend our results to the probabilistic setting, assuming a \(1-\) lower confidence bound on \(_{y}\) is available. This covers the probabilistic certification procedure described in Figure 1.

**Corollary 8**.: _Suppose the procedure in Figure 1 returns predicted class \(\) with certified radius \(r^{}\). Then an edit distance robustness certificate of radius \(r r^{}\) holds at \(\) with probability \(1-\)._

## 5 Case study: robust malware detection

We now present a case study on the application of RS-Del to malware detection. We report on certified accuracy for Levenshtein and Hamming distance threat models. We show that by tuning decision thresholds we can increase certified radii for the malicious class while maintaining accuracy. We also evaluate RS-Del on a range of published malware classifier attacks. Due to space constraints, we present the complete study in Appendices C-F, where we report on training curves, the computational cost of training and certification, certified radii normalized by file size, and results where byte edits are chunked by instructions.

BackgroundMalware (malicious software) detection is a long standing problem in security where machine learning is playing an increasing role . Inspired by the success of neural networks in other domains, recent work has sought to design neural network models for static malware detection which operate on raw binary executables, represented as byte sequences . While these models have achieved competitive performance, they are vulnerable to adversarial perturbations that allow malware to evade detection . Our edit distance threat model reflects these attacks in the malware domain where, even though a variety of perturbations with different semantic effects are possible, any perturbation can be represented in terms of elementary byte deletions, insertions and substitutions. For perspective on the threat model, consider YARA , a rule-based tool that is widely used for static malware analysis in industry. Running Nextron System's YARA rule set3 on a sample of binaries from the VTFeed dataset (introduced below), we find 83% of rule matches are triggered by fewer than 128 bytes. This implies most rules can be evaded by editing fewer than 128 bytes--a regime that is covered by our certificates in some instances (see Table 2). Further background and motivation for the threat model is provided in Appendix C, along with a reduction of static malware detection to sequence classification.

Experimental setupWe use two Windows malware datasets: Sleipnir2 which is compiled from public sources following Al-Dujaili et al.  and VTFeed which is collected from VirusTotal . We consider three malware detection models: a model smoothed with our randomized deletion mechanism (RS-Del), a model smoothed with the randomized ablation mechanism proposed by Levine and Feizi  (RS-Abn), and a non-smoothed baseline (NS). All of the models are based on a popular CNN architecture called MalConv , and are evaluated on a held-out test set. We emphasize that RS-Del is the only model that provides edit distance certificates (general \(O\)), while RS-Abn provides a Hamming distance certificate (\(O=\{\}\)). We review RS-Abn in Appendix I, where we describe modifications required for discrete sequence classification, and provide an analysis comparing the Hamming distance certificates of RS-Abn and RS-Del. Details about the datasets, models, training procedure, calibration, parameter settings and hardware are provided in Appendix D.

Accuracy/robustness tradeoffsOur first set of experiments investigate tradeoffs between malware detection accuracy and robustness guarantees as parameters associated with the smoothed models are varied. Table 2 reports clean accuracy, median certified radius (CR) and median certified radius normalized by file size (NCR) on the test set for RS-Del as a function of \(p_{}\). A reasonable tradeoff is observed at \(p_{}=99.5\%\) for Sleipnir2, where a median certified radius of 137 bytes is attained with only a 2-3% drop in clean accuracy from the NS baseline. The corresponding median NCR is 0.06% and varies in the range 0-9% across the test set. We also vary the decision threshold \(\) at \(p_{}=99.5\%\) for Sleipnir2, and obtain asymmetrical robustness guarantees with a median certified radius up to 582 bytes for the malicious class, while maintaining the same accuracies (see Table 8 in Appendix E.1).

Since there are no baseline methods that support edit distance certificates, we compare with RS-Abn, which produces a limited Hamming distance certificate. Figure 2 plots the certified accuracy curves for RS-Del and RS-Abn for different values of the associated smoothing parameters \(p_{}\) and \(p_{}\). The certified accuracy is the fraction of instances in the test set for which the model's prediction is correct _and_ certifiably robust at radius \(r\), and is therefore sensitive to both robustness and accuracy. We find that RS-Del outperforms RS-Abn in terms of certified accuracy at all radii \(r\) when \(p_{}=p_{}\), while covering a larger set of inputs (since RS-Del's edit distance ball includes RS-Abn's Hamming ball). Further results and interpretation for these experiments are provided in Appendix E.

Empirical robustness to attacksOur edit distance certificates are conservative and may underestimate robustness to adversaries with additional constraints (e.g., maintaining executability, preserving a malicious payload, etc.). To provide a more complete picture of robustness, we subject RS-Del and NS to six recently published attacks  covering white-box and black-box settings. We adapt gradient-based white-box attacks  for randomized smoothing in Appendix H. We do not constrain the number of edits each attack can make, which yields adversarial examples _well outside_ the edit distance we can certify for four of six attacks (see Table 9 of Appendix F). We measure robustness in terms of the _attack success rate_, defined as the fraction of instances in the evaluation set for which the attack flips the prediction from malicious to benign on at least one repeat (we repeat each attack five times). For both Sleipnir2 and VTFeed, we observe that RS-Del achievesthe lowest attack success rate (best robustness) for four of six attacks. In particular, we observe a 20 percentage point decrease in the success rate of _Disp_--an attack with no known defense . We refer the reader to Appendix F for details of this experiment, including setup, results and discussion.

## 6 Related work

There is a rich body of research on certifications under \(_{p}\)-norm-bounded threat models [5; 7; 6; 8; 9; 11; 13; 14; 12; 55; 25; 56]. While a useful abstraction for computer vision, such certifications are inadequate for many problems including perturbations to executable files considered in this work. Even in computer vision, \(_{p}\)-norm bounded defenses can be circumvented by image translation, rotation, blur, and other human-imperceptible transformations that induce extremely large \(_{p}\) distances. One solution is to re-parametrize the norm-bounded distance in terms of image transformation parameters [57; 58; 59]. NLP faces a different issue: while the \(_{0}\) threat model covers adversarial word substitution [28; 60], it is too broad and covers many natural (non-adversarial) examples as well. For example, "He loves cat" and "He hates cat" are 1 word in \(_{0}\) distance from "He likes cat", but are semantically different. A radius 1 certificate will force a wrong prediction for at least one neighbor. To address this, Jia et al.  and Ye et al.  constrain the threat model to synonyms only.

In this paper we go beyond the \(_{0}\) word substitution threat model of previous work [23; 24; 25; 56], as consideration of insertions and deletions is necessary in domains such as malware analysis. Such edits are not captured by the \(_{0}\) threat model: there is no fixed input size, and even when edits are size-preserving, a few edits may lead to large \(_{0}\) distances. Arguably, our edit distance threat model for sequences and RS-Del mechanism are of independent interest to natural language also.

Certification has been studied for variations of edit distance defined for sets and graphs. Liu et al.  apply randomized smoothing using a subsampling mechanism to certify point cloud classifiers against edits that include insertion, deletion and substitution of points. Since a point cloud is an _unordered_ set, the edit distance takes a simpler form than for sequences--it can be expressed in terms of set cardinalities rather than as a cost minimization problem over edit paths. This simplifies the analysis, allowing Liu et al. to obtain a tight certificate via the Neyman-Pearson lemma, which is not feasible

    & }\)} & Clean & Median & Median \\  & & Accuracy & CR & NCR \% \\   \\  NS & — & 98.9\% & — & — \\   & 90\% & 97.1\% & 6 & 0.0023 \\  & 95\% & 97.8\% & 13 & 0.0052 \\  & 97\% & 97.4\% & 22 & 0.0093 \\  & 99\% & 98.1\% & 68 & 0.0262 \\  & **99.5\%** & **96.5\%** & **137** & **0.0555** \\  & 99.9\% & 83.7\% & 688 & 0.2269 \\   \\  NS & — & 98.9\% & — & — \\  RS-Del & 97\% & 92.1\% & 22 & 0.0045 \\  & 99\% & 86.9\% & 68 & 0.0122 \\   

Table 2: Clean accuracy and robustness metrics for RS-Del as a function of dataset and deletion probability \(p_{}\). All metrics are computed on the test set. “Median CR” is the median certified Levenshtein distance radius in bytes and “median NCR %” is the median certified Levenshtein distance radius normalized as a percentage of the file size. A good tradeoff is achieved when \(p_{}=99.5\%\) (in bold).

Figure 2: Certified accuracy for RS-Del and RS-Abn  as a function of certificate radius (horizontal axis) and strength of the smoothing parameter (line style). Results are plotted for the Sleipnir2 test set. While RS-Del provides a Levenshtein distance certificate (with \(O=\{,,\}\)), RS-Abn provides a more limited Hamming distance certificate (\(O=\{\}\)). The non-smoothed, non-certified model (NS) achieves a clean accuracy of 98% in this setting.

for sequences. In parallel work, Schuchardt et al.  consider edit distance certification for graph classification, as an application of a broader certification framework for group equivariant tasks. They apply sparsity-aware smoothing  to an isomorphism equivariant base classifier, to yield a smoothed classifier that is certifiably robust under insertions/deletions of edges and node attributes.

Numerous empirical defense methods have been proposed to improve robustness of machine learning classifiers in security systems [63; 64; 29; 65; 22]. Incer Romeo et al.  and Chen et al.  develop classifiers that are verifiably robust if their manually crafted features conform to particular properties (e.g., monotonicity, stability). These approaches permit a combination of (potentially vulnerable) learned behavior with domain knowledge, and thereby aim to mitigate adversarial examples. Chen et al.  seek guarantees against subtree insertion and deletion for PDF malware classification. Using convex over-approximation [5; 66] previously applied to computer vision, they certify fixed-input dimension classifiers popular in PDF malware analysis. Concurrent to our work, Saha et al.  propose to certify classifiers against patch-based attacks by aggregating predictions of fixed-sized chunks of input binaries. The patch attack threat model, however, is not widely assumed in the evasion literature for malware detectors and can be readily broken by many published attacks [17; 20]. Moreover, their de-randomized smoothing design assumes a fixed-width input (via padding and/or trimming) and reduces patch-based attacks to gradient-based \(_{p}\) attacks. While tight analysis exists for arbitrary randomized smoothing mechanisms , they are computationally infeasible with the edit distance threat model. Overall, we are the first to explore certified adversarial defenses that apply to sequence classifiers under the edit distance threat model.

## 7 Conclusion

In this paper, we study certified robustness of discrete sequence classifiers. We identify critical limitations of the \(_{p}\)-norm bounded threat model in sequence classification and propose edit distance-based robustness, covering substitution, deletion and insertion perturbations. We then propose a novel deletion smoothing mechanism called RS-Del that is equipped with certified guarantees under several constraints on the edit operations. We present a case study of RS-Del and its certifications applied to malware analysis. We consider two malware datasets using a recent static deep malware detector, MalConv . We find that RS-Del can certify radii as large as \(128\) bytes (in Levenshtein distance) without significant loss in detection accuracy. A certificate of this size covers in excess of \(10^{606}\) files in the proximity of a 10KB input file (see Appendix A). Results also demonstrate RS-Del improving robustness against published attacks well beyond the certified radius.

Broader impact and limitationsRobustness certification seeks to quantify the risk of adversarial examples while randomized smoothing both enables certification and acts to mitigate the impact of attacks. Randomized smoothing can degrade (benign) accuracy of undefended models as demonstrated in our results at higher smoothing levels. While we have strived to select high quality datasets for our case study, we note that accuracy-robustness tradeoffs may vary for different datasets and/or model architectures. Our approach is scalable relative to alternative certification strategies, however it does incur computational overheads. Finally, it is known that randomized smoothing can have disparate impacts on class-wise accuracy .