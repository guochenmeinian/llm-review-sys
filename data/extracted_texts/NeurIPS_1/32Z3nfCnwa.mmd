# How Does Variance Shape the Regret

in Contextual Bandits?

 Zeyu Jia

Massachusetts Institute of Technology

zyjia@mit.edu

&Jian Qian

Massachusetts Institute of Technology

jianqian@mit.edu

&Alexander Rakhlin

Massachusetts Institute of Technology

rakhlin@mit.edu &Chen-Yu Wei

University of Virginia

chenyu.wei@virginia.edu

###### Abstract

We consider realizable contextual bandits with general function approximation, investigating how small reward variance can lead to better-than-minimax regret bounds. Unlike in minimax regret bounds, we show that the eluder dimension \(d_{}\)--a measure of the complexity of the function class--plays a crucial role in variance-dependent bounds. We consider two types of adversary:

* Weak adversary: The adversary sets the reward variance before observing the learner's action. In this setting, we prove that a regret of \((}\}}+d_{})\) is unavoidable when \(d_{}\), where \(A\) is the number of actions, \(T\) is the total number of rounds, and \(\) is the total variance over \(T\) rounds. For the \(A d_{}\) regime, we derive a nearly matching upper bound \(}(+d_{})\) for the special case where the variance is revealed at the beginning of each round.
* Strong adversary: The adversary sets the reward variance after observing the learner's action. We show that a regret of \((}}+d_{})\) is unavoidable when \(}}+d_{}\). In this setting, we provide an upper bound of order \(}(d_{}+d_{})\). Furthermore, we examine the setting where the function class additionally provides distributional information of the reward, as studied by Wang et al. (2024). We demonstrate that the regret bound \(}(}}+d_{})\) established in their work is unimprovable when \(}}+d_{}\). However, with a slightly different definition of the total variance and with the assumption that the reward follows a Gaussian distribution, one can achieve a regret of \(}(+d_{})\).

## 1 Introduction

We consider the contextual bandit problem that models repeated interactions between the learner and the environment. In each round, the learner chooses an action based on the received context, and observes the reward of the chosen action. Algorithms designed to achieve minimax regret guarantees under a variety of statistical assumptions and computational models have been extensively studied (Auer et al., 2002; Dudik et al., 2011; Agarwal et al., 2012, 2014; Foster and Rakhlin, 2020; Xu and Zeevi, 2020; Simchi-Levi and Xu, 2022; Zhang, 2022).

However, these algorithms often fail to leverage the potentially benign nature of the environment. In this work, we refine the regret bound by considering the variance of the reward. Such variance-dependent regret bounds, also known as second-order regret bounds, have been primarily studied under linear function approximation (Zhang et al., 2021; Kim et al., 2022; Zhao et al., 2023). Notably,Zhao et al. (2023) first established a near-optimal \(}(d+d)\) regret bound for linear contextual bandits, where \(d\) represents the feature dimension and \(\) the sum of the reward variances.

For contextual bandits with general function approximation, the recent work by Wang et al. (2024) obtained a second-order bound assuming access to a model class containing distributional information about the reward. They showed a regret bound of \(}(}||}+d_{}||)\), where \(||\) is the size of the model class, and \(d_{}=d_{}()\) is its eluder dimension. As noted in Wang et al. (2024), the dependence on \(d_{}\) is undesirable, and when the number of actions \(A\) is much smaller than \(d_{}\), this bound can potentially be improved. This conjecture is supported by Foster and Rakhlin (2020), who showed that the upper bound \(}(|}+A||)\) is achievable regardless of the eluder dimension, where \(T\) is the number of rounds, and \(||\) is the size of the function class containing mean reward information. It is tempting to conjecture that the regret can smoothly scale with \(\), resulting in a bound of \(}(|}+A||)\). Such variance-dependent regret bounds that replace the dependence on the number of rounds \(T\) by the total variance \(\) have been shown in multi-armed bandits (Audibert et al., 2009), linear bandits (Ito and Takemura, 2023), and linear contextual bandits (Zhao et al., 2023).

In this paper, we show, surprisingly, that the aforementioned conjecture is not true in general. Specifically, for any \(A\) and any \(d_{}\), one can construct a problem instance with lower bound \((}\}}+d_{})\) with \(||=( T)\) and \(d_{}()=d_{}\). This rules out the possibility of achieving \(}(|}+A||)\) for all \(A\) because we can always make \(d_{}=\), resulting in a lower bound \(()\) even with \(=0\). Our primary goal is to design algorithms that achieve the near-optimal regret bound \(}(}\}||}+d _{}||)\).

The lower bound \(}\}}+d_{}\) indicates that the complexity of contextual bandits arises from two parts. The first part accounts for local estimation of the true function, where the complexity is due to the variance of the reward and the local structure of the function set around the ground truth function \(f^{}\). This results in the term \(}\}}\), with the leading coefficient \(\{A,d_{}\}\) corresponding to the decision-estimation coefficient (Foster et al., 2021). The second part accounts for global search for the true function, in which the complexity is due to a more global structure of the function set and can be quantified by the _disagreement_ among the functions. The complexity of this part scales with \(d_{}\), even when \(A=2\). The contribution of the global part is usually overshadowed by the local part when only considering regret bounds with constant variance. Our work highlights its role by studying the variance-dependent bound. The fundamental role of disagreement is also discussed in Foster et al. (2020) for gap-dependent bounds. Specifically, they also showed that when trying to obtain the gap-dependent bound that has logarithmic dependence on \(T\), the complexity must scale with some disagreement measure over the function class, instead of just the number of actions.

The previous work by Wei et al. (2020) also derived a set of results for general contextual bandits showing that the tight second-order regret bound is strictly larger than merely replacing the \(T\) in the minimax bound by the second-order error. They consider the more general agnostic setting but the tight regret bounds are only established for the \(||=1\) case. Their result for \(||>1\) can be applied to our setting, though it only gives highly sub-optimal bounds. Overall, our work refines theirs in the realizable setting.

When preparing our camera-ready version, the concurrent work of Pacchiano (2024), which studied exactly the same problem as ours, was posted on arXiv. We provide a comparison with their work in Section 3.4. More related works are discussed in Appendix A.

## 2 Preliminaries

A contextual bandit problem consists of a context space \(\), an action space \(\), the total number of rounds \(T\) and a class of functions \(^{}\). At round \(t\), the learner observes a context \(x_{t}\), then makes a decision \(a_{t}\) based on the current context \(x_{t}\) and history, and observes a reward \(r_{t}\). We assume that these rewards \(r_{t}\) are given by

\[r_{t}=f^{}(x_{t},a_{t})+_{t}, \]

where \(f^{}:\) is some function unknown to the learner, and \(_{t}\) are independent zero-mean random variables with variance \(_{t}^{2}\) such that \(r_{t}\).1 We denote \(=_{t=1}^{T}_{t}^{2}\). The learner aims to optimize the total expected regret \(R_{T}\), defined as

\[R_{T}=_{t=1}^{T}(_{a}f^{}(x_{t},a)-f^{}(x_{t},a_{t})).\]

We make the following realizability assumption:

**Assumption 2.1** (Function Realizability).: _Assume that \(f^{}\) in Eq. (1) satisfies \(f^{}\)._

We finish this section with the definition of eluder dimension:

**Definition 2.1** (Eluder Dimension (Russo and Van Roy, 2014)).: _For function class \(\) defined on space \(\), we define the eluder dimension of \(\) at scale \( 0\), denoted by \(d_{}(;)\), as the length of the longest sequence of tuples \((z_{1},f_{1},f_{1}^{}),...,(z_{m},f_{m},f_{m}^{}) \) such that there exists \(_{0}\) making the following hold for all \(i=1,...,m\):_

\[_{j<i}(f_{i}(z_{j})-f_{i}^{}(z_{j}))^{2}_{0}^{2},|f_{i}(z_{i})-f_{i}^{}(z_{i})|>_{0}.\]

_Throughout the paper, if \(\) is not specified, we take the default value \(=}{{T^{2}}}\). We also omit the dependence on \(\) when it is clear from the context._

## 3 Results Overview

We describe our three settings in the following three subsections and summarize the results in Table 1. In the following, \(\) denotes the function class that only contains reward mean information, and \(\) the model class that contains reward distribution information.

### Weak Adversary Case with Variance Revealing (Section 4)

First, we consider the case where the adversary is _weak_. This means that the variance \(_{t}\) only depends on the history up to round \(t-1\), which aligns with the standard "adaptive adversary" assumption. For this case, we show that for any \(A\) and \(d\), one can find an instance of contextual problem problem with \(||\), such that the regret is at least

\[(}\}}+\{d_{},\}). \]

For upper bounds in the weak adversary case, we focus on the regime \(A d_{}\), where the lower bound can be written as \((+d_{})\).2 While our ultimate goal is to obtain a nearly matching upper bound \(}(|}+d_{}| |)\), we have not achieved it yet in full generality. In Section 4, we provide an algorithm which operates under the assumption that the variance \(_{t}\) is revealed to the learner at the beginning of round \(t\), and show that it achieves the matching upper bound. An initial attempt to remove this assumption is discussed in Section 7, where we show that when \(_{t}\{0,1\}\) for all \(t\) and \(_{t}\) is revealed to the learner at the _end_ of round \(t\), the matching upper bound can also be achieved.

### Strong Adversary Case (Section 5)

Next, we consider the case where the adversary is _strong_. This means the adversary can decide \(_{t}\)_after_ seeing the action \(a_{t}\) chosen by the learner at round \(t\). In this case, the lower bound becomes

\[(\{}}+d_{}, \}). \]

The difference with Eq. (2) is that the scaling in front of \(\) changes from \(\{A,d_{}\}\) to \(d_{}\). This shows the even more crucial role of eluder dimension in the strong adversary case. For this setting, we give an upper bound of \(}(d_{}|}+d_{}||)\), which is off from the lower bound by a \(}}\) factor along with other logarithmic factors.

### Learning with a Model Class (Section 6)

In Section 6, we assume that the function class provides information on the distribution of the reward rather than just the mean. Such a function class is usually called a _model class_. More precisely, the learner is provided with a model class \(\) that includes the true model \(M^{}\) so that \(M^{}(x,a)\) specifies reward distribution for the context-action pair \((x,a)\). Compared to the scenario studied in Section 4, here we do not require variance to be revealed to the learner. This becomes possible because with a model class, the learner can now obtain variance information (though not precise) through the context. Under the assumption that the noise is Gaussian, we provide an \(}(||}+d_{} ||)\) upper bound where \(_{}=_{t}_{a}_{M^{}}(x_{t},a)^{2}\), and a matching lower bound, where \(_{M}(x,a)\) is the reward variance for the context-action pair \((x,a)\) predicted by \(M\).

The work of Wang et al. (2024) also studied second-order contextual bandits with a model class. They use \(_{}=_{t}_{M^{}}(x_{t},a_{t})^{2}\), i.e., the reward variance of the chosen actions, as the variance measure. They obtain \(}(}_{}||}+d_{ }||)\) upper bound. We show a nearly matching lower bound \((\{}_{}}+d_{},\})\), similar to the lower bound for the strong adversary case studied in Section 5. The lower bound indicates that, in general, the bound of Wang et al. (2024) cannot be improved even when \(A<d_{}\).

### Comparison with Pacchiano (2024)

The work of Pacchiano (2024) also studied variance-dependent bounds for realizable contextual bandits. They also consider two settings, which can be mapped to those in our Section 4 and Section 5, respectively. For the weak adversary setting with revealed \(_{t}\) (Section 4), they give an upper bound of \(}(}||}+d_{}||)\),3 which is incomparable to our \(}(|}+d_{}| |)\). However, a full picture of this setting can be obtained by combining their upper bound and our upper bound and the lower bound in Eq. (2). For the strong adversary setting (Section 5), they derive exactly the same upper bound as in our Theorem 5.2. Our work makes additional contribution in the lower bounds and the extension to the distributional setting (Section 6).

## 4 Weak Adversary Case with Variance Revealing

In this section, we consider cases where the variance \(_{t}^{2}\) at round \(t\) is given to the learner at the beginning of round \(t\) together with the context \(x_{t}\).

### Lower Bound

The regret lower bound is shown with identical and known variance. The construction is similar to those in Wei et al. (2020). Concretely, we have the following theorem.

**Theorem 4.1** (Main lower bound).: _For any integer \(d,A 2\), any positive real number \(\), and time \(T>0\), there exists a context space \(\) and a contextual bandit problem \(()\) with eluder dimension \(d_{}(0) d\), action set \(\) with \(|| A\), and variance \(_{t}\) for all \(t[T]\) such that any algorithm will suffer a regret at least \((\{A,d\}T}+\{d,\})\)._

Proof sketch.: The full proof is deferred to Appendix C. The two parts in the lower bound came from the following two different hardness: (1) The first part of the lower bound with \((\{A,d\}T})\) is a natural lower bound with variance \(\) due to estimation of the mean values. (2) For the second part, we consider the following function class. In this function class, there is a "good" action that serves as the default choice with a reward of 1/2 for all contexts. For each of the other \(A-1\) "bad" actions, for each context, there is one function that obtains a reward of 1 but obtains 0 for all the other contexts. When \(d<\), this function class forces the learner to guess for each context which action to choose. So even if the reward is deterministic, i.e., variance \(=0\), any learner would have to suffer a regret scaling with the number of contexts times the number of actions, which in total coincide with the eluder dimension. When \(d\), the learner can simply commit to the "good" action and suffer \(\) but no better than this. 

This lower bound is rather surprising for the following consequences: (1) The most significant implication from this lower bound is that improving the minimax regret bound with the knowledge of the variance is only possible if \(d<\). (2) Even when \(d<\), any learner would have to pay for the eluder dimension as a lower-order term. These are non-trivial because the second-order bounds are usually obtained from changing Hoeffding concentration to Bernstein concentration which usually only scales the regret bounds by \(\). This lower bound shows that the second-order contextual bandit is not one of the usual cases. In the next section, we will match this lower bound from the upper bound side by combining several algorithmic techniques.

### Upper Bound with Known and Fixed Variance

Motivated by the lower bound in Theorem 4.1, we wonder whether there is an algorithm which can achieve a matching upper bound of \(}(\{A,d\}T}+\{d,\})\), if the learner is provided with information of variance at the beginning of each round. In this subsection, we answer this question affirmly. To begin with, we consider the case when all the variance are identical, i.e. \(_{1}=_{2}==_{T}=\), and \(\) is given to the learner. Later (Section 4.3), we will discuss how to generalize this result to the case with nonidentical variances across different rounds.

We assume that \(r_{t}=f^{}(x_{t},a_{t})+_{t}\) and \((_{t})_{t}^{2}\) for every \(1 t T\). Our results can be easily extended to subgaussian random noise (at the cost of a \( T\) factor) since for such variables, with probability at least \(1-\), \(|_{t}| C\) for a constant \(C\).

#### 4.2.1 Algorithm and Analysis for Identical Variance

We first consider the case with identical variance, i.e. \(_{t}^{2}=^{2}\) for all \(t[T]\). We propose Algorithm 1, and show that it has regret upper bound \(}(AT||}+d_{}| |)\). The algorithm is adapted from SquareCB of Foster and Rakhlin (2020), but additionally maintains a confidence function set, and has mechanisms to learn faster when the functions in the confidence set has larger disagreement. It has the following elements:

1. Restricting action set (Line 4)At the beginning of round \(t\) (Line 4), the learner restricts the action set to \(_{t}\), which only includes those actions that is the best action of some functions in the function class \(_{t}\). If we assume that \(f^{}\) is always in the function class \(_{t}\), by doing this we remove the unnecessary possibility of choosing actions that can never be the best action.

2. Checking disagreement (Line 5-Line 7)The next step of the algorithm is to check whether there is an action in \(_{t}\) such that two functions in the function class have large value differences (Line 6). We called such actions "discriminative actions". Roughly speaking, we are seeking an action \(a_{t}\) such that

\[ f,f^{}_{t},|f(x_{t},a)-f^{}(x_{t},a)| ^{2}.\]If such an action exists, then the learner chooses this action at round \(t\). By selecting such an action that can discriminate disagreed functions, the function set \(_{t}\) can more quickly shrink. To prevent this action to incur overly large regret, it is important to perform Step 1 (Restricting action set). The regret incurred in rounds choosing discriminative actions is of order \(}(d_{}||)\).

**3. Inverse gap weighting (Line 8-Line 11)** At round \(t\), if there is no discriminative action, then the learner performs inverse gap weighting as in the SquareCB algorithm (Foster and Rakhlin (2020)). Inverse gap weighting requires the learner to have access to an online regression oracle that generates online estimations \(f_{t}\) and ensures that the estimation error \(_{t}(f_{t}(x_{t},a_{t})-f^{}(x_{t},a_{t}))^{2}\) is small. In the original SquareCB, the requirement for the online regression oracle is

\[R_{}=_{t=1}^{T}(f_{t}(x_{t},a_{t})-r_{t})^{2}-_{t=1}^{T}(f^ {}(x_{t},a_{t})-r_{t})^{2}||,\] (F&R's condition)

which only allows for a \(|}\) regret bound that does not meet our goal. To improve this, we design an online regression oracle that ensures

\[R_{}=_{t_{}}(f_{t}(x_{t},a_{t})-r_{t} )^{2}-_{t_{}}(f^{}(x_{t},a_{t})-r_{t})^{2} (^{2}+)||,\] (our condition)

where \(_{}\) is the set of rounds that we run inverse gap weighting (i.e., entering the else case in Line 8), and \(\) is an upper bound for \(_{a_{t}}_{f,f^{}_{t}}|f(x_{t},a)-f^{ }(x_{t},a)|\), i.e., the maximum disagreement among the function set \(_{t}\) for the context \(x_{t}\). Thanks to Step 2, we only run inverse gap weighting when \(^{2}\). Thus, with the refined \(R_{}\) guarantee and standard squareCB arguments, we can get a regret bound of order \(|}\) for the rounds in \(_{}\).

The way to achieve "(our condition)" is an interesting part of our algorithm. A standard way to ensure F&R's condition is by aggregating over the function set through exponential weights. Exponential weights ensures \(R_{}=}(||/)\) as long as the functions to be aggregated are \(\)-mixable. Thus, in order to show \(R_{}=(^{2}||)\), we need to argue \(=(1/^{2})\). However, because the potential range of \(r_{t}\) is \(\) even though the variance \(^{2}\) and and the disagreement \(\) are both much smaller than \(1\), the best mixability coefficient \(\) we can show for squared loss is still \((1)\).

To address this, we resort to the use of the Prod algorithm (Cesa-Bianchi and Lugosi, 2006) with a properly chosen surrogate loss to perform aggregation. This algorithm has a different second-order approximation for the loss compared to the exponential weight algorithm, which is crucial in obtaining the desired bound. The regret analysis is also no longer through mixability. Our online regression oracle is provided in Algorithm 4 in Appendix D. We remark without giving details that in the linear case, such a guarantee can also be obtained through Online Newton Step (Hazan et al., 2007).

4. Updating function set (Line 12)After finishing selecting the action \(a_{t}\) for round \(t\), the learner updates the confidence function set \(_{t}\) to prepare for the next round. The construction of the confidence set utilizes the idea of weighted regression that has been widely used in previous variance-aware or corruption-robust contextual bandit or RL algorithms (He et al., 2022; Zhao et al., 2023; Ye et al., 2023; Agarwal et al., 2023). This has the effect of controlling the relative importance of different samples and is crucial in controlling the regret incurred in Step 2.

By putting these building blocks together, we arrive at Algorithm 1. The regret of Algorithm 1 is described in Theorem 4.2, whose proof is deferred to Appendix D.

**Theorem 4.2**.: _Algorithm 1 ensures with probability at least \(1-\),_

\[_{t=1}^{T}(_{a}f^{}(x_{t},a)-f^{}(x_{t},a_{t}) )=}(AT(||/ )}+d_{}(||/)).\]

Comparison with AdaCB of Foster et al. (2020)Our VarCB (Algorithm 1) shares some similarities with the AdaCB algorithm from Foster et al. (2020), which aims to achieve a \(}(|}{})\) regret bound. Here, \(d\) is a disagreement coefficient of \(\), which takes the same role as our \(d_{}\), and Gap represents the minimal reward gap between the best and second-best decisions. Specifically, both algorithms include a step to remove irrelevant actions (our Step 1). The action selection rule of AdaCB also depends on the amount of disagreement over the function class, which is superficially related to the if-else separation in VarCB. However, we find that the case separations in the two algorithms do not have a clear correspondence to each other, possibly due to the different objectives of the two algorithms. Also, the two algorithms operate under quite different settings: AdaCB works in the setting where the contexts are i.i.d., while VarCB allows for adversarial contexts. On the other hand, AdaCB is parameter-free, but VarCB requires the information of \(\). Developing a more unified version for these two better-than-minimax algorithms is an interesting future direction.

### Algorithm and Analysis for Heteroscedastic Noise

Next, we will discuss how to generalize our algorithm to heteroscedastic case, i.e. when the noise of different rounds are different. Based on the values of the variance, we classify each round into the following \(((AT)+1)\) sets: if \(_{t}[0,]\), we classify \(t\) into \(_{0}\), and for \(_{t}(}{AT^{}},}{AT}]\), we classify \(t\) into \(_{i}\) for \(2 i(AT)\), i.e., if \(_{t}\) falls into the \(i\)-th intervals in the following,

\[_{0}=[0,],_{1}=(,], _{2}=(,],,_{(AT)}=(1/2,1], \]

we classify \(t\) into \(_{i}\). For each set \(_{i}\), we maintain an algorithm \(_{i}\) of Algorithm 1 in parallel. At the beginning at round \(t\), when observing that \(t_{i}\), only \(_{i}\) is updated, while \(_{j}\) remains the same for \(j i\). According to Theorem 4.2, we have for any \(0 i T\),

\[_{t[_{i}]}(_{a}f^{}(x_{t},a)-f^{ }(x_{t},a_{t}))=}(_{i}| (}{AT})^{2}||}+d_{}| |),\]

we can bound the total regret by

\[_{i=1}^{(AT)}}(_{i}| (}{AT})^{2}||}+d_{}| |)=}(^{T}_{i}^ {2}||}+d_{}||).\]The formal algorithm for heteroscedastic cases is given in Algorithm 2, and we have the following corollary on the second-order regret bound of Algorithm 2.

```
1:Initialize instances \(_{i}\) of VarCB (Algorithm 1) with \(=}{AT}\) for \(0 i(AT)\).
2:for\(t=1:T\)do
3: Receive \(_{t}\), and suppose that \(_{t}_{i}\), where \(_{i}\) is defined in Eq. (7).
4: Receive context \(x_{t}\), and inject \(x_{t}\) into algorithm \(_{i}\). to obtain action \(a_{t}\).
5: Play action \(a_{t}\) and update algorithm \(_{i}\).
```

**Algorithm 2** Algorithm for Heteroscedastic Noise

**Corollary 4.1**.: The output \(a_{t}\) of Algorithm 2 in count \(t\) satisfies that with probability at least \(1-\),

\[_{t=1}^{T}(_{a}f^{}(x_{t},a)-f^{}(x_{t},a_{t}) )=}(^{T}_{i}^{2}(||/)}+d_{}(||/)).\]

The proof of Corollary 4.1 is given in Section E.

## 5 Strong Adversary Case

In this section, we consider the case where the adversary decides the variance \(_{t}\)_after_ seeing the action \(a_{t}\) chosen by the learner. We provide regret lower and upper bounds matching up to a factor of \(}}\) and other logarithmic factors. More importantly, the minimax regret bounds differ with the weak adversary case (Section 4) as discussed in Section 3.2, demonstrating the even more crucial role of eluder dimension in this case.

Regret lower boundIn this strong adversary case, we first show that the adversary's power is enhanced in terms of the achievable minimax regret bounds. Concretely, we have the following theorem.

**Theorem 5.1**.: _For any integer \(d,A,T 2\) and any positive real number \([0,T]\), there exists a context space \(\), a contextual bandit problem \(()\) with eluder dimension \(d_{}(,0)=d\) and action set \(=[A]\) and an adversarial sequence of variances \(_{1}^{2},,_{T}^{2}\) with \(_{t=1}^{T}_{t}^{2}\) such that any algorithm will suffer a regret at least \((\{+d,\})\)._

The above theorem shows that the regret is at least \((\{+d,\})\) where \(d=d_{}()\) even with \(||=( T)\). Recall that the bound in the weak adversary case (Section 4) can be written as \((\{}+d,\})\). The power of the strong adversary is exactly the higher complexity \(d\) in the \(\) term compared to \(\{A,d\}\) in the weak adversary case. Now, we proceed to provide a matching upper bound up to a factor of \(\).

Regret upper boundFor the strong adversary case, we adopt an optimism-based approach. In particular, we generalize the SAVE algorithm by Zhao et al. (2023), which achieves the tight \(}(d+d)\) bound for linear contextual bandits. We call the algorithm VarUCB and display it in Algorithm 5 of Appendix F. The algorithm combines the idea of weighted regression and multi-layer structure of SupLinUCB (Chu et al. (2011)) and refined variance-aware confidence set. Since this algorithm is a rather direct extension of Zhao et al. (2023)'s algorithm from the linear case to the non-linear case, we omit the detailed discussion on it and refer the readers to Zhao et al. (2023). Notice that for this algorithm, we do not need \(_{t}\) to be revealed to the learner as in Section 4. In fact, we do not even need to know \(\). We have the following theorem for its regret guarantee.

**Theorem 5.2**.: _When facing the strong adversary, Algorithm 5 guarantees a regret bound of \(}(d_{}|}+d_{}||)\) with probability at least \(1-\), where \(}()\) hides \((T/)\) factors._

The proof is provided in Appendix F. Notice that when specializing Theorem 5.2 to the linear setting, the bound becomes \(}(}+d^{2})\) since \(||=(d)\), which does not recover the bound of Zhao et al. (2023). Indeed, our analysis deviates from that of Zhao et al. (2023) due to the generality of non-linear function approximation. It is an interesting future direction to see whether our bound can be improved. We mention in passing that the work by Wang et al. (2024) obtained\(|}+d||\) upper bound where \(d=d_{}()\). However, the algorithm relies on having access to a model class. We study such a setting in our next section.

## 6 Learning with a Model Class

Distributional setupIn this section, we consider the case where the learner is given a model class \((()())\) where each model \(M\) maps any context-action pair to a gaussian distribution, i.e., for any \(x,a\),

\[M(x,a)=(f_{M}(x,a),_{M}(x,a)),\]

where \(f_{M}(x,a)\) and \(_{M}(x,a)\) are the mean and variance of the distribution \(M(x,a)\). We assume that all the expected rewards and variances are bounded by \(\). Recall, at round \(t\), the reward is given by \(r_{t}=f^{}(x_{t},a_{t})+_{t}\). We further assume throughout this section that \(_{t}\) is Gaussian with variance \(^{}(x_{t},a_{t})\) (since Gaussian is unbounded, we drop the assumption \(r_{t}\) that we made in Section 2). Thus, the distribution of \(r_{t}\) follows a true model \(M^{}\) where \(M^{}(x,a)=(f^{}(x,a),^{}(x,a))\).

**Assumption 6.1** (Model Realizability).: _Assume \(M^{}\)._

For this setup, it is useful to consider the Hellinger counterpart of the eluder dimension.

**Definition 6.1** (Hellinger Eluder Dimension).: _For the model class \(\) defined on the space \(\) (that is \((())\), we define the Hellinger eluder dimension of \(\) at scale \( 0\) as \(d_{}^{}()\) be the length of the longest sequence of tuples \((z_{1},M_{1},M^{}_{1}),...,(z_{m},M_{m},M^{}_{m})\) and \(_{0}\) such that for all \(i=1,..,m\), functions \(M_{i},M^{}_{i}\),_

\[_{j<i}D_{}^{2}(M_{i}(z_{j}),M^{}_{i}(z_{j}))_{0} ^{2}, D_{}^{2}(M_{i}(z_{i}),M^{}_{i}(z_{i }))>_{0}^{2}.\]

AlgorithmSimilar to Algorithm 1, we present Algorithm 3 tailored for the distributional case. At each round \(t\), upon receiving the context \(x_{t}\), the algorithm first checks if there exists an action \(a\) such that two models within the localized model class \(_{t}\) exhibit a significant divergence on the context-action pair \(x_{t},a\) measured by the squared Hellinger distance (Line 4). If such a difference is detected, the learner selects the action associated with the greatest divergence (Line 5). Conversely, if no action causes substantial divergence between models, the learner runs a variant of SquareCB (Foster and Rakhlin, 2020), employing adaptive variances to ensure low regret (Line 7). The major differences between Algorithm 1 and Algorithm 3 is that the latter measures the "disagreement" in terms of the squared Hellinger distance.

Regret upper boundWe obtain the following distributional version regret bound for Algorithm 3.

**Theorem 6.1**.: _For \(d=d_{}^{}(}{{}})\), the output \(a_{t}\) of Algorithm 3 satisfies with probability at least \(1-\),_

\[R_{T}=}^{T} _{M^{*}}^{2}(x_{t})(||/)}+d(||/ ),\]

_where \(_{M^{*}}^{2}(x_{t})=_{a A}_{M^{*}}^{2}(x_{t},a)\)._

A similar upper bound for a more general distributional case is obtained by Wang et al. (2024) in the form of \(}^{T}_{M^{*}}^{2}(x_{t},a_{t} )||}+d||\). In the leading term, our bound replaced the dependence of \(d\) by the number of actions \(A\) which is significantly smaller than \(A\) in many cases of interest (e.g. linear, generalized linear). However, as a tradeoff, our bound also suffers a larger cumulative variance term. This tradeoff is necessary as we show in the following lower bound results that both our upper bound and their upper bound are optimal, i.e., matching lower bounds exist. Thus our result is at one end of the Pareto frontier.

Regret lower boundsWe present the matching lower bound for our result as follows, which is essentially a rewrite of Theorem 4.1.

**Theorem 6.2**.: _For any integer \(d,A,T 2\), any positive real number \(\), there exists a context space \(\) and a contextual bandit gaussian model class \((())\) with Hellinger eluder dimension \(d_{}^{}(0) d\), action set \(=[A]\), and variances \(_{M}(x,a)\) for all \(M\), \(x,a\) such that any algorithm will suffer a regret at least \((\{A,d\}T}+\{d,\})\)._

Now we present the matching lower bound for the upper bound from Wang et al. (2024).

**Theorem 6.3**.: _For any integer \(d,A,T 2\) and any positive real number \([0,T]\), there exists a context space \(\), a contextual bandit gaussian model class \((())\) with Hellinger eluder dimension \(d_{}^{}(0) d\) and action set \(=[A]\) and the variances \(_{t=1}^{T}_{M^{*}}(x_{t},a_{t})^{2}\) such that any algorithm will suffer a regret at least \((\{+d,\})\)._

The lower bound obtained by Theorem 6.3 is an adaptation from Theorem 5.1 that crucially relies on the fact that the adversary can choose the variance according to the action \(a_{t}\).

## 7 Open Questions

Removing the revealing \(_{t}\) assumption in the weak adversary settingThe assumption we made in Section 4 that the variance is revealed at the beginning of each round is rather restrictive, and ideally we would like to remove such an assumption. As a first step, we wonder whether the same regret bound \((+d_{})\) is achievable if the variance \(_{t}\) is revealed at the _end_ of round \(t\). We answer this question affirmatively for the special case where \(_{t}\{0,1\}\). More details can be found in Appendix H. How to extend this result to general values of \(_{t}\) is an interesting open question. Handling the case where \(_{t}\) is never revealed is even more challenging but is the ultimate goal.

Removing the Gaussian noise assumption in the distributional settingOur Theorem 6.1 heavily relies on the assumption that the noise is Gaussian. We wonder whether such assumption can be relaxed or completely lifted. For example, can we obtain the same bound if the noise at round \(t\) is just \(_{M^{*}}(x_{t},a_{t})\)-sub-Gaussian? What if it is just a bounded noise with variance \(_{M^{*}}^{2}(x_{t},a_{t})\)?