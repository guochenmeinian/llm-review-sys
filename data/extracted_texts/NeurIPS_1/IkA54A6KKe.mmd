# dattri: A Library for Efficient Data Attribution

Junwei Deng\({}^{1*}\) Ting-Wei Li\({}^{1*}\) Shiyuan Zhang\({}^{1}\) Shixuan Liu\({}^{2}\) Yijun Pan\({}^{2}\) Hao Huang\({}^{1}\) Xinhe Wang\({}^{2}\) Pingbang Hu\({}^{1}\) Xingjian Zhang\({}^{2}\) Jiaqi W. Ma\({}^{1}\)

\({}^{1}\)University of Illinois Urbana-Champaign \({}^{2}\)University of Michigan

\({}^{*}\)Equal Contribution

###### Abstract

Data attribution methods aim to quantify the influence of individual training samples on the prediction of artificial intelligence (AI) models. As training data plays an increasingly crucial role in the modern development of large-scale AI models, data attribution has found broad applications in improving AI performance and safety. However, despite a surge of new data attribution methods being developed recently, there lacks a comprehensive library that facilitates the development, benchmarking, and deployment of different data attribution methods. In this work, we introduce dattri, an open-source data attribution library that addresses the above needs. Specifically, dattri highlights three novel design features. Firstly, dattri proposes a unified and easy-to-use API, allowing users to integrate different data attribution methods into their PyTorch-based machine learning pipeline with a few lines of code changed. Secondly, dattri modularizes low-level utility functions that are commonly used in data attribution methods, such as Hessian-vector product, inverse-Hessian-vector product or random projection, making it easier for researchers to develop new data attribution methods. Thirdly, dattri provides a comprehensive benchmark framework with pre-trained models and ground truth annotations for a variety of benchmark settings, including generative AI settings. We have implemented a variety of state-of-the-art efficient data attribution methods that can be applied to large-scale neural network models, and will continuously update the library in the future. Using the developed dattri library, we are able to perform a comprehensive and fair benchmark analysis across a wide range of data attribution methods. The source code of dattri is available at [https://github.com/TRAIS-Lab/dattri](https://github.com/TRAIS-Lab/dattri).

## 1 Introduction

_Data attribution_ is a family of methods that aim to quantify the influence of individual training samples on the output of artificial intelligence (AI) models. As training data is becoming increasingly critical for the advancement of modern AI models, especially for large foundation models , there has been a surge of data attribution methods developed recently . These methods have found broad data-centric applications in improving the performance and safety of AI models, such as noisy label detection , data selection , and copyright compensation .

However, a comprehensive infrastructural library that facilitates the development, benchmarking, and deployment of efficient data attribution methods is lacking. This absence hinders the standardization and acceleration of research in this area, creating inefficiencies and inconsistencies in how data attribution methods are developed and applied. Although there have been a couple of prior efforts  for unifying APIs and benchmarking, many missing opportunities remain unaddressed,motivating the need for this work. We defer to Section 2.2 for a detailed comparison between the existing libraries and our work.

In this work, we introduce dattri, an open-source data attribution library with the following design objectives. Firstly, for _users_ deploying existing data attribution methods, we aim to provide a unified and user-friendly API across different methods. Specifically, our API design emphasizes **minimal code invasion**, i.e., allowing the data attribution methods to be integrated into most common PyTorch-based machine learning pipelines with only a few lines of code changed. This is non-trivial as data attribution methods often require access to internal information of the models, such as gradients or hidden representations. We achieve this goal by providing helper decorators to streamline the integration. Secondly, for _researchers_ developing new data attribution methods, we aim to facilitate the development by providing efficient implementations of **low-level utility functions**, such as Hessian-vector product, inverse-Hessian-vector product or random projection. These functions are used by multiple existing data attribution methods and will likely be useful in the development of new methods. In dattri, we implement the data attribution methods in a carefully designed modular fashion, providing both a set of modularized low-level functions and examples of the usage of these functions. Finally, for both _users_ and _researchers_, we aim to provide a benchmark suite that highlights **a comprehensive list of evaluation metrics** and diverse benchmark settings, including **generative AI settings**. In addition to the code for the evaluation metrics and benchmark experiments, we also provide the **trained model checkpoints** for each benchmark setting. Since some evaluation metrics for data attribution require hundreds or even thousands of model retraining, the provided trained model checkpoints could significantly reduce the computational burden of benchmark evaluation. The **bolded features** listed above are all novel designs in dattri compared to existing literature.

We have implemented a variety of data attribution methods, evaluation metrics, and benchmark settings in dattri. Our library currently covers four families of data attribution methods, each named after a representative method within its category. These families are: Influence Function (IF) , TracIn , Representer Point Selection (RPS) , and TRAK . We have excluded certain other methods, notably the game-theoretic methods such as Data Shapley [10; 17] or Data Banzhaf , to focus on computationally efficient methods that do not require extensive model retraining. We have implemented three evaluation metrics commonly used in the data attribution literature: noisy label detection , leave-one-out (LOO) correlation , and linear datamodeling score (LDS) . We provide six benchmark settings on different combinations of models and datasets, with a variety of machine learning tasks, including image classification, text generation, and music generation. With the developed dattri library, we have performed a comprehensive and fair benchmark analysis across the methods and settings. Our results suggest that IF performs well on linear models, while TRAK generally outperforms other methods in most experimental settings.

In summary, dattri is a comprehensive library with numerous novel features tailored to facilitate the development, benchmarking, and deployment of efficient data attribution methods. We will also continuously update this library to include more efficient data attribution methods and benchmark settings in future iterations.

## 2 Related Work

In this section, we briefly review data attribution methods in Section 2.1 and compare dattri with existing data attribution libraries in Section 2.2.

### Data attribution methods

The data attribution problem.Suppose we have a training set \(=\{x_{1},,x_{n}\}\), a test set \(=\{x_{1},,x_{m}\}\), and a trained model output function \(f_{}\) that is parameterized by \(\). Typically, a data attribution method \(\) derives the attribution scores \((x,;f_{})^{n}\), where \(x\), to quantify the influence of each training data point in \(\) on the model output on \(x\).

Data attribution methods.Our library aims to cover a diverse set of representative data attribution methods while recognizing that it is impossible to implement all existing methods. Notably, we have omitted one popular family of game-theoretic methods, including Data Shapley [10; 16] and Data Banzhaf . These methods often require repeatedly removing subsets of training data and retraining the model on the remaining data, making them computationally infeasible for large-scale applications.

Prioritizing efficient data attribution methods applicable to large neural network models, we focus on the following four families of methods. We have implemented most of the state-of-the-art methods.

One of the most popular and widely used data attribution families is the Influence Function (IF) , which approximates the influence by calculating the Hessian matrix and the gradient of data samples. Since explicitly calculating the Inverse-Hessian-Vector-Product can be prohibitively heavy in terms of computational load and memory usage, many alternative methods are proposed to alleviate the computation and memory cost. Some of the popular ones include Conjugate Gradients (CG) , LiSSA , Arnoldi . Another family of data attribution methods, TracIn , assumes the hessian matrix to be an identity matrix and proposes to leverage multiple checkpoints during the training and assume the hessian matrix to be an identity matrix. Existing literature  also proposes two simplified versions, i.e., "Grad-Dot" and "Grad-Cos". "Grad-Dot" can be seen as TracIn with only one checkpoint, and "Grad-Cos" additionally normalizes the score with the gradient norm. Representer Point Selection (RPS)  is another family of data attribution methods. It uses the representer point theorem for kernels to represent the pre-activation prediction as a linear combination of training samples. TRAK  is the last family; it leverages the empirical neural tangent kernel approximation and random projection to improve efficiency and efficacy. The detailed formula definition of each data attribution method is stated in Appendix A.

In Table 1, we summarize these methods and provide a qualitative overview of their performance based on our benchmark experiments detailed in Section 4.

### Data attribution libraries

There are three existing libraries aiming to benchmark or unify the implementations of different data attribution methods, as summarized in Table 2. Specifically, OpenDataVal primarily focuses on game-theoretic methods. While it also implements a couple of IF variants, it misses most of the efficient data attribution methods. The scale of the benchmark settings of OpenDataVal are also mostly small. pyDVL implements both the IF family of methods and game-theoretic methods, but it does not have a benchmark component. The methods implemented by influenza are closer to ours, yet we cover more data attribution methods as well as significantly more comprehensive benchmark datasets, tasks, and metrics (so far influenza only has one benchmark and metric). Moreover, influenza is based on Tensorflow and has only one evaluation metric for image classification, which limits its applicability and flexibility. In contrast, our library is built on PyTorch,

    &  &  &  &  \\   & & & Linear & Non-linear & Linear & Non-linear & Linear & Non-linear \\   & Explicit  & ++ & - & ++ & - & ++ & - \\   & CG  & ++ & - & ++ & + & ++ & + \\   & LiSSA  & ++ & - & ++ & + & ++ & + \\   & Arnoldi  & + & - & + & - & ++ & + \\   & TracInCP  & + & - & + & + & ++ & + \\   & Grad-Dot  & + & - & + & + & ++ & + \\   & Grad-Cos  & + & - & + & + & - & - \\  RPS & RPS-L2  & + & - & + & - & ++ & + \\  TRAK & TRAK  & ++ & - & ++ & ++ & ++ & ++ \\   

Table 1: A summary of the efficient data attribution methods available in datri. These methods are clustered into four families: IF, TracIn, RPS, and TRAK. The empirical experiments are demonstrated separately by different evaluation metrics and models. The experimental settings are stated in Section 3.3, and the results are detailed in Section 4. Here, we use the symbols “-/+/++” to qualitatively indicate the performance of each efficient data attribution method to be “similar to random/better than random/much better than random”. The “Linear” column is based on the result of logistic regression (LR), while the “Non-linear” column is based on that of a variety of neural network models.

and includes a rich family of efficient data attribution methods and a comprehensive benchmarking component. Our library also highlights novel design features as mentioned in Section 1 and detailed in Section 3.

In addition, there is a remotely relevant library, Captum, that primarily focuses on machine learning model interpretability. It implements several data attribution methods as part of its suite of interpretability tools, alongside other techniques such as feature and neuron attribution methods . However, the goals and scope of Captum differ significantly from those of dattri.

## 3 Design of dattri

In this section, we introduce the design of dattri that provides a unified and user-friendly API (Section 3.1), modularized low-level utility functions (Section 3.2), and a comprehensive benchmark suite (Section 3.3).

### A unified and user-friendly API

Data attribution methods often require internal gradients or hidden representations of the model to calculate the attribution scores. Consequently, many existing implementations of data attribution methods are heavily _invasive_ to the model training pipeline, i.e., the data attribution process is significantly entangled with the model training code, making it challenging for users to adapt the code to other models or applications.

    &  &  &  \\    & & IF &  &  &  &  &  &  &  &  \\  pp/WL & Yes & / & / & / & Yes & PyTorch & / & / & / \\  OpenDataVal & Partial & / & / & / & Yes & PyTorch & & & Noisy LabelFeature Detection \\  Influence & Yes & Yes & Yes & / & / & Tensorflow & Image Classification & Noisy Label Detection \\  dattri & Yes & Yes & Yes & Yes & / & PyTorch & &  Image Classification \\ Text Generation \\ Music Generation \\  & 
 Noisy Label Detection \\ Linear Ditandodeling Score (LDS) \\ Leave-one-out (LOO) Correlation \\  \\   

Table 2: A summary of existing libraries and our library, dattri. Our library covers a broader set of efficient data attribution methods and a more comprehensive benchmark suite.

Figure 1: Architecture of dattri and the functionalities of each module in dattri serve.

dattri is carefully designed to provide a unified API that can be applied to the most common PyTorch model training pipeline with minimal code invasion. Demo 1 shows an example of applying IF methods on a PyTorch model.

Specifically, the user will first define an AttributionTask object. This object contains necessary attribution task information such as the loss function from which the model is trained, the model architecture, and the trained model checkpoints. Next, the user will initialize an Attributor instance with the AttributionTask object and additional configuration parameters. Note that each Attributor class corresponds to a specific attribution method. Finally, the Attributor will perform data attribution using the training and test data loaders, which typically come directly from the model training pipeline.

The same API works for all the data attribution methods available in dattri, so that users can easily switch across different methods.

```
deff(params,data):#anexampleoflossfunctionusingCEloss x,y=data loss=nn.CrossEntropyLoss() yhat=torch.func.functional_call(model,params,x) returnloss(yhat,y) attr_task=AttributionTask( loss_func=f, model=model, checkpoints=model.state_dict(), target_func=f#thetargetfunctiontobeattributedcoulddifferfromtheloss function(e.g.,thiscouldbedefinedasthepredictionlogit) ) attributor=IFAttributorCG( task=attr_task, device=torch.device("cuda"), **attribute_hyperparams#e.g.,regularization,... )#similarforother attributors attribute.cache(train_loader)#optionalpre-processingtocacceleratetheattribution score=attributor.attribute(train_loader,test_loader)
```

Demo 1: Example usage of dattri to perform attribution on a PyTorch model. Users will first define a AttributionTask object, task, that collects necessary configuration information about the attribution task. Next, users can initialize a specific attributor class (in this demo, IFAttributorCG corresponds to the influence function with CG method) that takes task as the input. Finally, users feed the training and testing data loaders (typically from the model training pipeline) to attributor and obtain the attribution scores.

### Modularized low-level utility functions

Different data attribution methods can share common sub-routines in their algorithms. In dattri, we modularize such sub-routines through low-level utility functions so that they can be reused in the development of new methods.

There are two types of low-level utility functions, respectively, implemented in the modules dattri.func and dattri.model_utils. The dattri.func module is built on top of torch.func1, which allows flexible mathematical manipulation of numerical functions. This is a helpful abstraction as data attribution methods often utilize mathematical operations beyond standard PyTorch operators (e.g., operations involving higher-order derivatives). We implement a few such mathematical operations in dattri.func, including Hessian-vector product (HVP), inverse-Hessian-vector product (IHVP) and random projection. The dattri.model_utils module, on the other hand, implements model-level manipulations that have been shown to be useful in recent literature. Below, we provide more details about several key low-level utility functions.

Hvp/Ihvp.Mathematically, given a target function \(f_{}(x)\) with the Hessian denoted as \(H(x;)=_{}^{2}f_{}(x)\), and a vector \(v\), the HVP function is defined as \((x,v;)=H(x;)v\); while the IHVP function is defined as \((x,v;)=H(x;)^{-1}v\). We implement the HVP function with a thin wrapper on the composition of Jacobian-vector product functions available in torch. func. We further implement a variety of efficient approximated algorithms for the HVP functions (such as Conjugate Gradients (CG)  or LiSSA ), most of which re-use HVP as a sub-routine. For each of these IhVP algorithms, we implemented two versions under dattri.func.hessian, ihvp_{alg.name} and ihvp_at_x_{alg.name}. As can be seen in Demo 2 (with CG as an example), ihvp_cg takes the target function as input and returns a function \((x,v;)\) (i.e., ihvp_func in Demo 2). On the other hand, ihvp_at_x_cg further takes the data and parameters as input and returns a function ihvp_at_x_func that only takes \(v\) as input. The latter implementation serves a specific need of data attribution methods where we want to calculate \((x,v;)\) for multiple \(v\)'s with the same \(x\) and \(\). This implementation allows us to pre-process and cache intermediate results that only depend on \(x\) and \(\) to accelerate the algorithm.

```
fromdattri.func.hessianimportihvp_cg,ihvp_at_x_cg deff(x,param):#targetfunction returntorch.sin(x/param).sum() x=torch.randn(2) param=torch.randn(1) v=torch.randn(5, 2)
#ihvp_cgmethod invhp_func=ihvp_cg(f,argnums=0,max_iter=2)#argnums=0indicatesthattheparam of(x,param) to be passed toihvp_func is the model parameter ihvp_result_1=ihvp_func((x,param),v)#both(x,param)andvastheinputs
#ihvp_at_x_cgmethod:(x,param)iscalculated invhp_at_x_func=ihvp_at_x_cg(f,x,param,argnums=0,max_iter=2) ihvp_result_2=ihvp_at_x_func(v)#onlyvastheinput
#theabovetwowillgivethesameresult asserttorch.allclose(ihvp_result_1,ihvp_result_2)
```

Demo 2: Example usage of the CG implementation of the IHVP function.

Random projection.Some data attribution methods, such as TRAK  and TracIn , involve inner product among gradients of model parameters. This calculation can be significantly accelerated by dimension reduction through random projection when the model parameter size is extremely large. We provide a simple wrapper on top of the random projection toolkit, fast_jl, implemented by Park et al. .

Researchers can leverage this utility function into the development of new data attribution methods when dealing with high-dimensional model parameters.

Dropout ensemble.Recent studies  have shown that the efficacy of many data attribution methods can be significantly improved by ensembling multiple independently trained models with different random seeds. Furthermore, a recent paper  proposes _dropout ensemble_, which utilizes multiple dropout masks on the same model to perform ensembling, leading to superior efficiency-efficacy trade-off in comparison to naive ensembles. In dattri.model_utils, we provide a utility function activate_dropout to enable dropout ensemble for different data attribution methods.

### A comprehensive benchmark suite

Data attribution metrics.As an emerging research area, data attribution has been evaluated by a variety of metrics in the literature. Among them, there are two types of mainstream evaluation metrics. The first type of metrics treats the change of model outputs after removing certain data points and retraining the model as a gold standard for quantifying the influence of individual training samples:

* Leave-one-out (**LOO**) correlation : This metric refers to the Pearson correlation between the predicted model output difference (by data attribution method) and the model output with leave-one-out training.

* Linear datamodeling score (**LDS**) : This metric aims at probing data attribution methods' ability to make counterfactual predictions based on the attribution score derived from the learned model output function \(f_{}\) and the corresponding dataset to train \(f_{}\). Because most data attribution methods are assumed to be _additive_2, the data attribution scores can be used to predict the model output function learned from a subset of training data in a summation form. More details of LDS are deferred to Appendix B.

The second type of metrics evaluates data attribution methods through downstream applications, where the most common ones are noisy label detection and data selection . However, a recent study  demonstrates that the data selection task is problematic. Therefore we focus on noisy label detection only in our benchmark:

* Area under the ROC curve (**AUC**) for noisy label detection: This metric is specifically for noisy label detection tasks. For this task, a certain portion of the training samples' labels are flipped and data attribution methods are utilized to prioritize the training points with higher self-influence for humans to inspect when repairing the dataset. The task can thus be treated as a ranking problem (based on the magnitude of attribution scores) and evaluated by AUC.

Diverse experimental settings.dattri introduces diverse experimental settings including image classification, music generation, and text generation, which are listed in Table 3. To summarize, we consider a series of models with different architectures trained on diverse datasets: (1) a logistic regression (LR) classifier and a three-layer MLP classifier trained on the MNIST-10 dataset , (2) a ResNet-9 classifier  trained on CIFAR-10  and CIFAR-2 dataset (a two-class subset of the CIFAR-10 dataset), (3) a Music Transformer  trained on the MAESTRO dataset  and (4) a NanoGPT  trained on the Shakespeare dataset . In particular, the former two are supervised image classification settings, while the latter two are generative settings. For the classification settings, we sample 5000 training samples and 500 test samples from MNIST-10 and CIFAR-10/CIFAR-2 datasets. For the MAESTRO dataset, we sample 5000 training samples and 178 generated samples. For the Shakespeare dataset, we use the full training set with size 3921 and sample 435 generated samples. More detailed setups for each dataset and model are listed in Appendix C.

Pre-trained models with ground truth.For the aforementioned benchmark settings, we provide pre-trained models, and ground truth annotations corresponding to each evaluation metric. For MNIST-10 experiments, we provide pre-trained models produced by exact leave-one-out training (i.e., 5000 models for each of the LR and MLP experiments). Across all settings, we pre-train 100 models for LDS calculation on a random half-dataset controlled by a fixed random seed generation procedure. The first 50 models are used in our benchmark experiments to assemble and compute the attribution scores. For example, TRAK and TracIn can utilize multiple models to improve their performance on the same task. The last 50 models are used for LDS calculation, which requires several models that are trained on a portion of the original training dataset. The formulation of LDS is detailed in Appendix B.

   Dataset & Model & Task & Sample size (train,test) & Parameter size & Metrics & Data Source \\  MNIST-10 & LR & Image Classification & (5000,500) & 7840 & LOO,LDS/AUC &  \\  MNIST-10 & MLP & Image Classification & (5000,500) & 0.11M & LOO,LDS/AUC &  \\  CIFAR-2 & ResNet-9 & Image Classification & (5000,500) & 4.83M & LDS &  \\  CIFAR-10 & ResNet-9 & Image Classification & (5000,500) & 4.83M & AUC &  \\  MAESTRO & Music Transformer & Music Generation & (5000,178) & 13.3M & LDS &  \\  Shakespeare & NanoGPT & Text Generation & (3921,435) & 10.7M & LDS &  \\   

Table 3: The full experimental setting for data attribution benchmark.

## 4 Benchmark Experiments

### Experimental setup

Datasets and models.We follow the experimental settings listed in Table 3.

Data attribution methods.We benchmark data attribution methods listed in Table 1. Some of the methods consist of a couple of hyperparameters related to their numerical stability. During the benchmarking, we mildly tune the hyperparameters to avoid falling into the numerically unstable region for each method. The details of the hyperparameter tuning are stated in Appendix C.1. Furthermore, some methods become infeasible in terms of computation time or memory as the model size and data size grow. In this case, their result is marked as a red cross in the plots. The TRAK method can trade off computation for efficacy by ensembling several independently trained models. We denote TRAK without ensembling as "TRAK-1" while TRAK with 10 or 50 model ensembling respectively as "TRAK-10" and "TRAK-50".

Figure 2: The LOO correlation and LDS evaluation of each efficient data attribution method on LR and MLP trained on MNIST-10. The red cross indicates that the experiment runs out of the time or memory budget.

[MISSING_PAGE_EMPTY:9]

AUC performance.We further evaluate the data attribution methods in terms of the AUC in the downstream noisy label detection task, as shown in Figure 4. Except for "Grad-Cos"3, most methods have better-than-random performance (\(>0.5\)) for the experiments on LR/MLP on MNIST-10, with some methods achieving near-perfect AUC (close to 1). For the experiments on ResNet-9 on Cifar-10, there is a significant performance drop for all methods, with only TRAK achieving non-trivial performance (\(>0.5\)).

Overall, we found that the IF family performs well in small experimental settings and linear models, while TRAK generally outperforms other methods in most experimental settings.

Additionally, there are some limitations to the current evaluation metrics. Both the LOO and LDS metrics require a large number of retrained models to obtain the "ground truth." The AUC metric, meanwhile, is tied to a specific downstream application and is only applicable to classification tasks. To address these limitations, and as a key contribution of dattri, we provide pre-trained model checkpoints associated with the LOO and LDS metrics, allowing users to bypass the costly retraining process for these evaluations.

## 5 Conclusion

In this work, we introduce dattri, a comprehensive open-source library that facilitates the research, development, and deployment of data attribution methods. The main contribution of dattri is three-fold: (1) a unified and user-friendly API for seamless integration into PyTorch-based ML pipelines, (2) modularized implementations of low-level utility functions to aid researchers in developing new methods, and (3) a fair benchmark suite with diverse evaluation metrics, experimental settings, and pre-trained model checkpoints. dattri addresses critical infrastructural needs in the data attribution domain, offering a collaborative platform that promotes standardization and accelerates the development/deployment of data attribution methods.

Limitations and future work.While dattri has implemented a rich family of existing data attribution methods and experimental settings, admittedly there are still a number of efficient data attribution methods and benchmark datasets missing in our current library. As for future work, we will continuously incorporate new methods, benchmarking experiments, and evaluation metrics, advancing the state-of-the-art of efficient data attribution and unlocking the potential for large-scale data-centric AI applications.