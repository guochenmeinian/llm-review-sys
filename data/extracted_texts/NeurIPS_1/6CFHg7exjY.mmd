# Addressing Hidden Confounding with Heterogeneous Observational Datasets for Recommendation

Yanghao Xiao\({}^{1,3}\)   Haoxuan Li\({}^{2}\)   Yongqiang Tang\({}^{3,}\)   Wensheng Zhang\({}^{4,}\)

\({}^{1}\)University of Chinese Academy of Sciences  \({}^{2}\)Peking University

\({}^{3}\)Institute of Automation, Chinese Academy of Sciences  \({}^{4}\)Guangzhou University

xiaoyanghao22@mails.ucas.ac.cn, hxl1@stu.pku.edu.cn,

yongqiang.tang@ia.ac.cn, zhangwenshengia@hotmail.com

Corresponding author

###### Abstract

The collected data in recommender systems generally suffers selection bias. Considerable works are proposed to address selection bias induced by observed user and item features, but they fail when hidden features (e.g., user age or salary) that affect both user selection mechanism and feedback exist, which is called hidden confounding. To tackle this issue, methods based on sensitivity analysis and leveraging a few randomized controlled trial (RCT) data for model calibration are proposed. However, the former relies on strong assumptions of hidden confounding strength, whereas the latter relies on the expensive RCT data, thereby limiting their applicability in real-world scenarios. In this paper, we propose to employ heterogeneous observational data to address hidden confounding, wherein some data is subject to hidden confounding while the remaining is not. We argue that such setup is more aligned with practical scenarios, especially when some users do not have complete personal information (thus assumed with hidden confounding), while others do have (thus assumed without hidden confounding). To achieve unbiased learning, we propose a novel meta-learning based debiasing method called MetaDebias. This method explicitly models oracle error imputation and hidden confounding bias, and utilizes bi-level optimization for model training. Extensive experiments on three public datasets validate our method achieves state-of-the-art performance in the presence of hidden confounding, regardless of RCT data availability.

## 1 Introduction

Recommender systems have been developed with the purpose of providing users with personalized recommendations, serving as a potent tool in capturing users' true preferences . In recent years, deep learning algorithms are proposed to train recommendation models with collected historical data. However, the selection bias introduced by the users' selective interactions with items poses a challenge to the unbiased learning of the prediction model . For instance, in explicit feedback data, users are free to rate items they prefer, thereby engendering the distribution discrepancy between interacted and non-interacted data .

To alleviate selection bias, a line of works are proposed, including error imputation , inverse propensity weighting , and doubly robust . Moreover, recent works have sought to enhance these foundational methods from diverse perspectives, including analysis on both bias and variance , considerations for training stability , and integration with multi-task learning . However, they all fail to achieve unbiased when some features

[MISSING_PAGE_FAIL:2]

be a binary treatment variable, where \(o_{u,i}=1\) indicates \(r_{u,i}\) is observed, otherwise is not. Let \(=\{(u,i)(u,i),o_{u,i}=1\}\) be the observed population consisting of user-item pairs with observed ratings. Denote \(\) and \(\) be the distribution and expectation on the target population \(\).

We denote prediction model with parameter \(\) as \(_{u,i}=f(x_{u,i};)\) which aims to predict true ratings. If all the ratings \(\{r_{u,i}:(u,i)\}\) are observed, i.e., \(=\), the parameter \(\) can be trained by minimizing the ideal loss:

\[_{}()=|}_{(u,i) }e_{u,i},\]

where \(e_{u,i}=L(_{u,i},r_{u,i})\) is the prediction error and \(L(,)\) is a pre-specified loss function such as the squared loss \(e_{u,i}=(_{u,i}-r_{u,i})^{2}\). However, rating \(r_{u,i}\) is observed only when \(o_{u,i}=1\), and naively minimizing prediction loss on the observed population \(\) will suffer from bias and lead to sub-optimal performance . This is attributed to the disparities between the observed population \(\) and the target population \(\), which is called selection bias.

### Addressing Selection Bias without Hidden Confounding

To address selection bias, prior works preliminarily assume the absence of hidden confounding in the collected data, implying that the observed features \(x_{u,i}\) include all the possible confounders. Without loss of generality, we define a binary variable \(g_{u,i}\{0,1\}\) as the data source indicator which measures the completeness of observed features \(x_{u,i}\). As shown in the toy example in Figure 1, \(g_{u,i}=1\) indicates that all confounders are completely observed, while \(g_{u,i}=0\) indicates that some hidden confounders exist. To ensure consistency in the input, we fill the missing values in \(x_{u,i}\) with 0, so that the observed features \(x_{u,i}\) have the same dimensions when \(g_{u,i}=0\) and \(g_{u,i}=1\).

Assumed no hidden confounding (\(g_{u,i}=1,(u,i)\)), it holds that \(e_{u,i}\!\!\! o_{u,i} x_{u,i},g_{u,i}=1\), the widely adopted propensity based methods are proposed. Specifically, the Inverse Propensity Scoring (IPS) estimator  reweights each sample based on the probability of being observed and is given as

\[_{}()=|}_{(u,i) }e_{u,i}}{_{u,i}},\]

where \(_{u,i}\) is the estimation of propensity \(p_{u,i}(o_{u,i}=1|x_{u,i},g_{u,i}=1)\). Furthermore, the Doubly Robust (DR) estimator  further extends IPS with error imputation and is presented as

\[_{}()=|}_{(u,i) }[_{u,i}+(e_{u,i}-_{u,i})}{_{u,i} }],\]

where \(_{u,i}\) is the imputed error. Based on IPS and DR estimators, many variants have been proposed, and please refer to Appendix A.1 for more details.

### Addressing Selection Bias with Hidden Confounding

However, in some real-world scenarios, the observed features do not include all confounders and hidden confounding exists. Based on the incomplete observed features \(x_{u,i}\) with \(g_{u,i}=0\), it holds that \(o_{u,i}\!\!\! e_{u,i} x_{u,i},g_{u,i}=0\), which prevents previous methods such as DR from achieving unbiasedness. To further mitigate the hidden confounding, methods based on sensitivity analysis and model calibration using RCT data have been proposed.

**Sensitivity Analysis.** Inspired from causal inference literature, sensitivity analysis based approach assumes the hidden confounding strength can be bounded such that the inverse of true propensity scores are near and bounded by the estimated values, and adopt worst-case optimization to mitigate hidden confounding . However, above strong assumption on hidden confounding strength is hard to be satisfied in real-world scenarios and such method fails when the assumption is violated.

**Model Calibration.** Recent works propose to leverage a few unbiased RCT data collected from randomized controlled trials or A/B tests for model calibration . Collecting RCT data requires users to rate items randomly, hence RCT samples are representatives of the target population, and the prediction loss on these samples serves as an unbiased estimator of the ideal loss. Thus, the biased propensity and imputation models learned from the incomplete observed features \(x_{u,i}\) can be corrected using such unbiased loss, for instance, with the help of additive residual models  or multiplicative reweighting models . However, the acquisition cost of RCT data is prohibitively high, posing challenges to the practical implementation of such methods in real-world settings.

Apart from debiased recommendation, more related works about causal inference under hidden confounding can be found in Appendix A.1, which is a widely studied topic.

## 3 MetaDebias: Meta Learning Based Debiased Recommendation Approach

### Problem Formulation

We first present the problem formulation, that is to provide an unbiased estimator for the ideal loss \(_{}\) in the presence of hidden confounding, given the collected heterogeneous observational data with known data source indicator \(g_{u,i}\). Unlike existing works that assume training data originates from a single dataset without or with hidden confounding, we argue that the training data is more likely composed of two heterogeneous datasets, one of which has sufficient feature collection and is assumed without hidden confounding (\(g_{u,i}=1\)), while the other has insufficient feature collection and is assumed with hidden confounding (\(g_{u,i}=0\)) with hidden confounders denoted as \(h_{u,i}\).

Here, based on the given \(g_{u,i}\{0,1\}\) for distinguishing between two types of observational data, we partition the observed population \(\) into two subpopulations \(_{0}=\{(u,i)(u,i),g_{u,i}=0,o_{u,i}=1\}\) and \(_{1}=\{(u,i)(u,i),g_{u,i}=1,o_{u,i}=1\}\), while unobserved population is \(_{u}=\{(u,i)(u,i),o_{u,i}=0\}\). A naive approximation method for ideal loss is minimizing the prediction loss over the observed data. Specifically, given \(_{0}\) and \(_{1}\), the respective losses over each subgroup are as follows:

\[_{_{0}}=_{0}|}_{(u,i) _{0}}e_{u,i},_{_{1}}=_{1}|}_{(u,i)_{1}}e_{u,i}.\]

However, due to the selection bias, neither \(_{_{0}}\), \(_{_{1}}\) nor their combination \(_{}=_{_{0}}+_{ _{1}}\) is an unbiased estimator for the ideal loss. Furthermore, as discussed in the Preliminaries 2, previous methods cannot be applied to achieve the goal without additional hidden confounding strength or RCT data assumptions.

### Methodology

To achieve the goal, we propose to explicitly estimate the prediction error \(e_{u,i}\) on all user-item pairs \(\) using observed features \(x_{u,i}\), as the ideal loss is defined as the average prediction error over \(\), i.e., \(_{}=|}_{(u,i)} e_{u,i}\). In other words, our goal is transformed into accurately estimating the oracle error imputation \([e_{u,i} x_{u,i}]\), which can inherently derive an unbiased estimator.

The challenges in achieving accurate oracle error imputation estimation lie in the fact that the prediction error \(e_{u,i}\) is only partially observable, and the missing mechanisms differ between the two subgroups \(_{0}\) and \(_{1}\) due to the influence of hidden confounding. To address these challenges, we define the identifiable propensity scores aimed at modeling the two types of missing mechanisms, along with the naive error imputations defined over the entire space \(\), to assist in estimating the target oracle error imputation. Next, we will introduce the details.

To start with, different from the previously widely used propensity score \((o_{u,i}=1|x_{u,i})\) which is applicable only for samples in subgroup \(_{1}\) and the generalized formulation \((o_{u,i}=1|x_{u,i},h_{u,i})\) designed for subgroup \(_{0}\) which is unidentifiable, we define the identifiable propensity score \((x,g)\) which models the probability of being observed for any user-item pair on the entire space \(\):

\[(x,g)=(o_{u,i}=1 x_{u,i}=x,g_{u,i}=g). \]

The proposed \((x,g)\) models the propensity score for both the absence and presence of hidden confounding, based on the input data source indicator \(g_{u,i}\).

Similarly, we propose to use the observed features \(x_{u,i}\) and data source indicator \(g_{u,i}\) to estimate the naive prediction error as \(o_{u,i} e_{u,i}\), which is computable on the target population \(\) and expected to be the prediction error \(e_{u,i}\) when sample \((u,i)\) is observed \(o_{u,i}=1\), or to be zero when not observed \(o_{u,i}=0\). Note that the zero value in the defined naive prediction error \(o_{u,i} e_{u,i}\) contains the information about data missing mechanism, thus \(g_{u,i}\) is used to capture the differences in the missing mechanism when hidden confounding is present or absent. Formally, to achieve this target, we define the naive error imputation \(m(x,g)\) as:

\[m(x,g)=[o_{u,i} e_{u,i} x_{u,i}=x,g_{u,i}=g]. \]

Based on the proposed propensity score \((x,g)\) and naive error imputation \(m(x,g)\), the oracle error imputation \([e_{u,i} x_{u,i}]\) can be derived, we summarize their relationship in Lemma 1 below.

**Lemma 1**.: _The relationship between the proposed propensity score \((x,g)\), naive error imputation \(m(x,g)\) and the oracle error imputation \([e_{u,i} x_{u,i}]\) is as follows:_

\[m(x,g)=\{[e_{u,i} x_{u,i}=x]+(1-g)(x)\}(x,g), \]

_where \((x)=[e_{u,i} x_{u,i}=x,g_{u,i}=0,o_{u,i}=1]- [e_{u,i} x_{u,i}=x]\)._

The above result delineates the conditional independence relationship between observation and prediction error. Specifically, on subgroup \(_{1}\) where no hidden confounding exists, the prediction error \(e_{u,i}\) is independent of the observation \(o_{u,i}\) given the observed features \(x_{u,i}\), i.e., \(e_{u,i} o_{u,i} x_{u,i},g_{u,i}=1\), and the expectation of \(o_{u,i} e_{u,i}\) naturally equals the product of their respective expectations as shown in Equation 3.

However, on subgroup \(_{0}\) where hidden confounders \(h_{u,i}\) exist, above conditional independence does not hold, thus we introduce an additional residual module \((x)\) which describes the expectation difference between the mean prediction error over target population \(\) and the biased subpopulation \(_{0}\), to account for the bias introduced by hidden confounders.

Lemma 1 provides an approach for estimating the oracle error imputation \([e_{u,i} x_{u,i}]\), and the estimation depends solely on propensity score \((x,g)\) and naive error imputation \(m(x,g)\). Note that both \((x,g)\) and \(m(x,g)\) are typically estimated from the heterogeneous observational datasets, which implies that when the estimation of \((x,g)\) and \(m(x,g)\) are inaccurate, the resultant oracle error imputation estimation will further deteriorate in accuracy.

To address this limitation, we propose further incorporating additional data information including the observation \(o_{u,i}\) and the naive prediction error \(o_{u,i} e_{u,i}\) to achieve a more robust estimation of the oracle error imputation. The details are shown in Lemma 2 below.

**Lemma 2**.: _The relationship between the propensity score \((x,g)\), naive error imputation \(m(x,g)\), observation \(o_{u,i}\), naive prediction error \(o_{u,i} e_{u,i}\) and oracle error imputation \([e_{u,i} x_{u,i}]\) is:_

\[o_{u,i} e_{u,i}-m(x,g)=\{[e_{u,i} x_{u,i}=x]+(1 -g)(x)\}\{o_{u,i}-(x,g)\}+, \]

_where \(=o_{u,i}\{e_{u,i}-\{[e_{u,i} x_{u,i}=x]+(1-g)(x)\}\}\) with \([ x,g]=0\)._

The findings in Lemma 2 provide an alternative and robust approach for estimating the oracle error imputation \([e_{u,i} x_{u,i}]\), wherein the estimation depends not only on the learned propensity score \((x,g)\), naive error imputation \((x,g)\), but also on the observation \(o_{u,i}\), along with the naive prediction error \(o_{u,i} e_{u,i}\), where \(\) with zero-mean property can be regarded as a noise.

### Training Objectives

For the method implementation, we employ deep learning models to estimate the aforementioned proposed modules, and name this approach as MetaDebias. We present the architecture of MetaDebias in Figure 2, and introduce the training objective of each model as follows.

Initially, we employ the commonly used cross-entropy loss to train the propensity model \((x,g;_{})\) with parameters \(_{}\) using heterogeneous data, denoted as \(_{}(_{})\) shown in Equation 5 below:

\[_{}(_{}) =|}_{(u,i)}[-o_{u,i} (x_{u,i},g_{u,i})-(1-o_{u,i})(1-(x_{u,i},g_ {u,i}))]\] \[=|}_{(u,i)_{0} _{1}}-(x_{u,i},g_{u,i})+|} _{(u,i)_{u}}-(1-(x_{u,i},g_{u,i}))\,. \]According to the definition of \(m(x,g)\) in Equation 2, we adopt the square loss to train the naive imputation model \(m(x,g;_{m})\) with parameters \(_{m}\), denoted as \(_{m}(_{m},)\) shown in Equation 6:

\[_{m}(_{m},) =|}_{(u,i)}(m(x_{ u,i},g_{u,i})-o_{u,i} e_{u,i})^{2}\] \[=|}_{(u,i)_{0} _{1}}(m(x_{u,i},g_{u,i})-e_{u,i})^{2}+|}_{(u,i)_{u}}(m(x_{u,i},g_{u,i} ))^{2}, \]

where \(e_{u,i}=L(f(x_{u,i};),r_{u,i})\) is the prediction error and only available when \(o_{u,i}=1\).

Next, based on Lemma 2, we employ the square loss to train the oracle imputation model \(e(x;_{e})\) with parameters \(_{e}\) and residual model \((x;_{})\) with parameters \(_{}\) given learned propensity model \(_{}\) and imputation model \(_{m}\), denoted as \(_{meta}(_{e},_{},_{m},_{},)\) shown in Equation 7 below:

\[_{meta}(_{e},_{},_{m},_{},) =|}_{(u,i)}\{o_{u,i} e _{u,i}-m(x_{u,i},g_{u,i}) \] \[-[e(x_{u,i})+(1-g_{u,i})(x_{u,i})][o_{u,i}- (x_{u,i},g_{u,i})]\}^{2}.\]

Furthermore, we adopt the oracle imputation model \(e(x;_{e})\) to generate prediction errors on the target population \(\) as the training objective for prediction model \(f(x_{u,i};)\) with parameters \(\). The training objective \(_{pred}(,_{e})\) is demonstrated in the following Equation 8:

\[_{pred}(,_{e})=|}_{(u,i) }e(x_{u,i};_{e}), \]

where \(e(x_{u,i};_{e})\) is the learned oracle imputation model training from \(_{meta}\) loss in Equation 7.

### Learning Algorithm

Following previous works [3; 56], we propose a bi-level optimization based learning algorithm for model training. Specifically, we first train the propensity model \((x,g)\) independently by minimizing the \(_{}\) loss shown in Equation 5, as its training objective is orthogonal to those of other models. Next, we use bi-level optimization to update the remaining models.

In the bi-level optimization framework, we first assumed update of the naive imputation model \(m(x,g;_{m})\) and the oracle imputation model \(e(x;_{e})\), to ensure that the gradient of \(_{e}^{}(_{m}^{},)\) can

Figure 2: Architecture of MetaDebias to address selection bias in the presence of hidden confounding.

be back propagated through the chain rule to update the prediction model parameter \(\). Subsequently, the prediction model \(f(x_{u,i};)\) is updated by minimizing \(_{pred}(,_{e})\) defined in Equation 8.

After updating the prediction model, we adopt joint learning to update the naive imputation model \(m(x,g;_{m})\) by minimizing \(_{m}\) in Equation 6. Simultaneously, the oracle imputation model \(e(x;_{e})\) and residual model \((x;_{})\) are updated by minimizing \(_{mata}\) in Equation 7. The whole procedure of the learning algorithm is outlined in Algorithm 1, where \(\) represents the learning rate and we use distinct subscripts to correspond to different models.

```
Input: observed ratings \(^{o}\).
1Pretrain propensity model \((x,g,_{})\)
2whilestopping criteria is not satisfieddo
3fornumber of training stepsdo
4 Sample a batch of user-item pairs \(\{(u_{i},i_{i})\}_{i=1}^{B_{1}}\) from \(\);
5 Assumed update naive imputation model: \(^{}_{m}()=_{m}-_{m}_{_{m}}_{m} (m(_{m}))\) ;
6 Assumed update oracle imputation model: \(^{}_{e}(^{}_{m},)=_{e}-_{e}_{_{ e}}_{meta}(e(_{e})^{}_{m},)\);
7 Update prediction model: \(-_{}_{pred}(f_{^{ }_{e}(^{}_{m},)})\) ;
8 end for
9fornumber of training stepsdo
10 Sample a batch of user-item pairs \(\{(u_{i},i_{i})\}_{i=1}^{B_{2}}\) from \(\);
11 Update naive imputation model: \(_{m}_{m}-_{m}_{_{m}}_{m}(m( _{m}))\) ;
12 end for
13fornumber of training stepsdo
14 Sample a batch of user-item pairs \(\{(u_{i},i_{i})\}_{i=1}^{B_{3}}\) from \(\);
15 Update oracle imputation model: \(_{e}_{e}-_{e}_{_{e}}_{meta}( e(_{e})_{m},)\) ;
16 Update residual model: \(_{}_{}-_{}_{_{}}_ {meta}((_{})_{m},)\) ;
17 end for
18
19 end while Output:\(\)
```

**Algorithm 1**The Proposed MetaDebias Learning Algorithm

## 4 Experiments

### Experimental Setup

**Dataset and Experimental Details.** Following previous studies [3; 43; 55; 56], we conduct extensive experiments on three public datasets, Coat2, Yahoo! R33, and KuaiRec4. Coat contains 6,960 biased ratings and 4,640 unbiased ratings derived from 290 users evaluating 300 items. Yahoo! R3 contains 311,704 biased ratings and 54,000 unbiased ratings derived from 15,400 users evaluating 1,000 items. Both datasets employ a five-point rating scale, and we binarize the ratings greater than 3 as 1 and others as 0. KuaiRec contains 4,676,570 video watching ratios derived from 1,411 users evaluating 3,327 videos. The ratios that greater than 2 are binarized as 1, otherwise as 0. We adopt feature masking to simulate \(g_{u,i}\) which measures feature completeness, and \(g_{u,i}=1\) only when the features of both users and items are fully preserved, otherwise \(g_{u,i}=0\), and see Appendix A.3 for more details. Following previous works [3; 28; 29; 34], we randomly split 5% unbiased data from the test set as validation set, and for all methods requiring RCT data, we employ observational data without hidden confounding to pretend RCT data. For evaluation, we employ three widely adopted metrics AUC, Recall@\(K\), and NDCG@\(K\) to measure debiasing performance, where we set \(K\) = 5 on Coat and Yahoo! R3, and \(K\) = 50 on KuaiRec. All the methods are implemented on PyTorch with Adam as the optimizer and NVIDIA A40 as the computing resource, and we tune learning rate in \(\{0.0001,0.0005,0.001,0.005,0.01,0.05\}\) and weight decay in \([1e-7,10]\).

[MISSING_PAGE_FAIL:8]

**In-depth Analysis.** We conduct in-depth analysis to further explore the effect of hidden confounding strength, the proportions of heterogeneous observational data, and training data size on performance. Moreover, we further explore the performance under conditions where RCT data is available, which is consistent with the problem setup in prior works. See Appendix A.4 for more experimental results.

**The Hidden Confounding Strength.** Figure 3 shows the impact of hidden confounding strength, where we simulate higher hidden confounding strength by masking more features. We observe all methods exhibit performance degradation as the hidden confounding gets stronger, with vanilla DR exhibiting the poorest performance, highlighting the necessity of removing hidden confounding. The performance of BRD-DR deteriorates significantly, primarily due to the violation of imposed strong model assumptions, while Res-DR demonstrates competitive performance. In addition, the proposed MetaDebias stably outperforms the baselines across varying hidden confounding strength, implying our method is able to effectively address strong hidden confounding using only observational data.

**The Proportion of Heterogeneous Observational Data.** We explore the impact of varying proportions of heterogeneous observational data on the KuziRec dataset in Figure 4, where \(\#N(G=0)\) and \(\#N(G=1)\) denote the quantity of training data with and without hidden confounding. First, we observe the performance of DR varies significantly under different proportions, indicating the pronounced disparities between the heterogeneous observational data. Besides, performances of all methods are enhanced when the proportion of data without hidden confounding increases, this is because more feature information aids in accurate propensity and imputation model learning. Furthermore, MetaDebias demonstrates the best performance, indicating our method can achieve effective debiasing performance across various observational data proportions, and the potential for application in a range of real-world scenarios.

**The Training Dataset Size.** Table 2 explores the impact of training set size on NDCG@\(K\) on both KuziRec and Yahoo! R3 datasets. We find that the performance of all methods declines as the size of training set decreases, highlighting the importance of training data size on model training. Besides, the proposed MetaDebias consistently outperforms the baselines across varying training data size especially when the size is extremely small such as only 10%, which indicates our method that explicitly models oracle prediction errors on the entire space fully exploits the heterogeneous observational data and is still effective even with small training set size.

Table 2: Effects of training dataset size on NDCG@\(K\) on the KuziRec and Yahoo! R3 datasets.

Figure 4: Effects of varying proportions of heterogeneous observational data on the KuziRec dataset.

**The Impact of RCT Training Data.** In Figure 5, we explore the debiasing performance when RCT data is available, which is consistent with the problem setup in prior works [28; 29]. Specifically, the training dataset is composed of two parts: an observational dataset with hidden confounding and a small RCT dataset. For implementing our proposed MetaDebias, we label the RCT sample as \(g_{u,i}=1\), as the RCT data can be regarded a special case of MNAR data without hidden confounding. We find that the performances of all methods exhibit an increasing trend as the size of RCT training set increases, which is consistent with previous research findings. Moreover, the proposed MetaDebias stably outperforms all the baseline methods with varying RCT training set size, which indicates our method remains superior to the existing approaches under the previous problem setup, further validating the effectiveness of our method.

## 5 Conclusion

In this study, we investigate the problem of selection bias in the presence of hidden confounding. First, we argue that existing methods are challenging to be applied in real-world scenarios, as they either rely on strong assumptions on hidden confounding strength or depend on the costly RCT data. To tackle this issue, we propose to adopt heterogeneous observational datasets which are more likely to be collected to address hidden confounding and claim that such setup is more aligned with practical scenarios. Then, we propose a meta-learning based debiasing method called MetaDebias, which explicitly models the oracle error imputation and additional bias induced by hidden confounders, and we adopt bi-level optimization with assumed update for model training. Extensive experiments conducted on three public datasets validate our method achieves state-of-the-art performance, regardless of the availability of RCT data. A limitation of this work lies in the assumption that all the confounders can be included through sufficient feature collection. Even though it is possible to collect hundreds of features in industrial scenarios, and some features like consumption records may potentially represent hard-to-obtain features like salary, it is still challenging to guarantee that all confounders for partial samples have been included. In future work, we will explore how to further relax this theoretically feasible assumption.