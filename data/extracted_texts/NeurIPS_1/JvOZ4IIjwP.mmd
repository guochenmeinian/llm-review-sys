# Train Hard, Fight Easy:

Robust Meta Reinforcement Learning

 Ido Greenberg

Technion, Nvidia Research

gido@campus.technion.ac.il

&Shie Mannor

Technion, Nvidia Research

shie@ee.technion.ac.il

&Gal Chechik

Bar Ilan University, Nvidia Research

gchechik@nvidia.com

&Eli Meirom

Nvidia Research

emeirom@nvidia.com

###### Abstract

A major challenge of reinforcement learning (RL) in real-world applications is the variation between environments, tasks or clients. Meta-RL (MRL) addresses this issue by learning a meta-policy that adapts to new tasks. Standard MRL methods optimize the average return over tasks, but often suffer from poor results in tasks of high risk or difficulty. This limits system reliability since test tasks are not known in advance. In this work, we define a robust MRL objective with a controlled robustness level. Optimization of analogous robust objectives in RL is known to lead to both **biased gradients** and **data inefficiency**. We prove that the gradient bias disappears in our proposed MRL framework. The data inefficiency is addressed via the novel Robust Meta RL algorithm (_RoML_). RoML is a meta-algorithm that generates a robust version of any given MRL algorithm, by identifying and oversampling harder tasks throughout training. We demonstrate that RoML achieves robust returns on multiple navigation and continuous control benchmarks.

## 1 Introduction

Reinforcement learning (RL) has achieved impressive results in a variety of applications in recent years, including cooling systems control  and conversational chatbots . A significant challenge in extending this success to mass production is the variation between instances of the problem, e.g., different cooling systems or different chatbot end-users. Meta-RL (MRL) addresses this challenge by learning a "meta-policy" that quickly adapts to new tasks . In the examples above, MRL would maximize the average return of the adapted policy for a new cooling system or a new end-user.

However, optimizing the return of the average client might not suffice, as certain clients may still experience low or even negative returns. If 10% of the clients report poor performance, it may deter potential clients from adopting the new technology - even if its average return is high. This highlights

Figure 1: (a) An illustration of driving tasks, characterized by various weather conditions and traffic density. (b) The returns of two meta-policies \(_{1},_{2}\) on these tasks. \(_{1}\) has a higher average return, but \(_{2}\) is more robust to high-risk tasks. The task space is discretized only for illustration purposes.

the need for MRL systems that provide robust returns across tasks. Robustness is further motivated by risk sensitivity in many natural RL applications, such as medical treatment (Yu et al., 2021) and driving (Shalev-Shwartz et al., 2016). For example, as illustrated in Fig. 1, an agent should drive safely at _any_ road profile - even if at some roads the driving would be more cautious than necessary.

A common approach for risk-averse optimization is the max-min objective (Collins et al., 2020); in MRL, that would mean searching for a meta-policy with the highest expected-return in the worst possible task. This expresses the most extreme risk aversion, which only attends to the one worst case out of all the possible outcomes. Furthermore, in certain problems, worst cases are inevitable (e.g., in certain medical treatments, a fatal outcome cannot be avoided), thus optimizing the minimum return might not provide any meaningful result. A more general objective is the average return over the worst \(\) quantiles (\(0 1\)), also known as the Conditional Value-at-Risk (CVaR). Notice that the CVaR is a generalization of both the mean (for \(=1\)) and the minimum (for \(=0\)).

CVaR is a coherent risk measure used for risk management in various fields (Filippi et al., 2020), including banking regulation (Acerbi and Szekely, 2014) and RL (Tamar et al., 2015; Hiraoka et al., 2019). In this work, we extend CVaR optimization to MRL, by replacing the standard MRL objective

\[*{argmax}_{}J^{}(R), J^{}(R)=_{,R}[R], \]

with a CVaR objective, which measures the robustness of a policy to high-risk tasks:

\[*{argmax}_{}J^{}_{}(R), J^{}_{}(R )=^{}_{}[_{R}[R]]. \]

In both equations, \(\) is a random task and \(R\) is the random return of policy \(_{}\) in \(\). Intuitively, the CVaR return expresses robustness to the selected task, in analogy to robustness to the realized model in standard RL. To further motivate Eq. (2), note that CVaR optimization in RL is equivalent to robust optimization under uncertain perturbations (Chow et al., 2015).

In Section 4, we follow the standard approach of policy gradient (PG) for CVaR\({}_{}\) optimization in RL, and apply it to MRL. That is, for every batch of \(N\) trajectories, we apply the learning step to the \( N\) trajectories with the lowest returns. This standard approach, CVaR-PG, is known to suffer from a major limitation: in an actor-critic framework, the critic leads to a biased gradient estimator - to the extent that it may point to the opposite direction (Tamar et al., 2015). This limitation is quite severe: many CVaR-PG implementations (Tamar et al., 2015; Greenberg et al., 2022) rely on vanilla PG without a critic (REINFORCE, Williams (1992)); others pay the price of gradient bias - in favor of advanced actor-critic methods that reduce the gradient variance (Rajeswaran et al., 2017).

This limitation is particularly concerning in _meta_ RL, where high complexity and noise require more sophisticated algorithms than REINFORCE. Fortunately, **Section 4 eliminates this concern: in MRI, in contrast to RL, the CVaR policy gradient is proven to remain unbiased regardless of the choice of critic**. Hence, our proposed method - CVaR Meta Learning (_CVaR-ML_) - can be safely applied on top of any MRL algorithm. This makes CVaR-ML a _meta-algorithm_: given an arbitrary MRL algorithm, CVaR-ML generates a robust version of it.

Nevertheless, in CVaR optimization methods, another source of gradients variance and sample inefficiency is the large proportion of data not being utilized. Every iteration, we rollout trajectories for \(N\) tasks, but only use \( N\) of them for training. To mitigate this effect, we introduce in Section 5 the Robust Meta RL algorithm (_RoML_). RoML assumes that tasks can be selected during training. It learns to identify tasks with lower returns and over-samples them. By training on high-risk tasks, the meta-agent learns policies that are robust to them without discarding data. Hence, **RoML increases the sample efficiency by a factor of up to \(}\)**. Unlike common adversarial methods, which search for the worst-case sample (task) that minimizes the return (Collins et al., 2020), RoML lets the user specify the desired level of robustness \(\), and addresses the entire \(\)-tail of the return distribution.

We test our algorithms on several domains. Section 6.1 considers a navigation problem, where both CVaR-ML and RoML obtain better CVaR returns than their risk-neutral baseline. Furthermore, they learn substantially different navigation policies. Section 6.2 considers several continuous control environments with varying tasks. These environments are challenging for CVaR-ML, which entirely fails to learn. Yet, RoML preserves its effectiveness and consistently improves the robustness of the returns. In addition, Section 6.3 demonstrates that under certain conditions, RoML can be applied to supervised settings as well - providing robust supervised meta-learning.

As a meta-algorithm, in each experiment RoML improves the robustness of its baseline algorithm - using the same hyper-parameters as the baseline. The _average_ return is also improved in certain experiments, indicating that even the risk-neutral objective of Eq. (1) may benefit from robustness.

**Contribution:** (a) We propose a principled CVaR optimization framework for robust meta-RL. While the analogous problem in standard RL suffers from biased gradients and data inefficiency, we (b) prove theoretically that MRL is immune to the former, and (c) address the latter via the novel Robust Meta RL algorithm (RoML). Finally, (d) we demonstrate the robustness of RoML experimentally.

## 2 Related Work

**Meta-RL** for the **average task** is widely researched, including methods based on gradients (Finn et al., 2017; Gupta et al., 2018), latent memory (Zingraf et al., 2019; Rakelly et al., 2019) and offline meta learning (Dorfman et al., 2020; Pong et al., 2022). It is used for applications ranging from robotics (Nagabandi et al., 2018) to education (Wu et al., 2021). Adversarial meta learning was studied for minimax optimization of the **lowest-return task**, in supervised meta learning (Collins et al., 2020; Goldblum et al., 2020) and MRL (Lin et al., 2020). Other works studied the robustness of MRL to distributional shifts (Mendonca et al., 2020; Ajay et al., 2022). However, the **CVaR task** objective has not been addressed yet in the framework of MRL.

**Risk-averse RL.** In _standard_ RL, risk awareness is widely studied for both safety (Garcia and Fernandez, 2015; Greenberg and Mannor, 2021) and robustness (Derman et al., 2020). CVaR specifically was studied using PG (Tamar et al., 2015; Rajeswaran et al., 2017; Hiraoka et al., 2019; Huang et al., 2021), value iteration (Chow et al., 2015) and distributional RL (Dabney et al., 2018; Schubert et al., 2021; Lim and Malik, 2022). CVaR optimization was also shown equivalent to mean optimization under robustness (Chow et al., 2015), motivating robust-RL methods (Pinto et al., 2017; Godbout et al., 2021). In this work, we propose a _meta-learning_ framework and algorithms for CVaR optimization, and point to both similarities and differences from the standard RL setting.

**Sampling.** In Section 5, we use the cross-entropy method (de Boer et al., 2005) to sample high-risk tasks for training. The cross-entropy method has been studied in standard RL for both optimization (Mannor et al., 2003; Huang et al., 2021) and sampling (Greenberg et al., 2022). Sampling in RL was also studied for regret minimization in the framework of Unsupervised Environment Design (Dennis et al., 2020; Jiang et al., 2021); and for accelerated curriculum learning in the framework of Contextual RL (Klink et al., 2020; Eimer et al., 2021). By contrast, we address MRL (where the current task is unknown to the agent, unlike Contextual RL), and optimize the CVaR risk measure instead of the mean.

## 3 Preliminaries

**MRL.** Consider a set of Markov Decision Processes (MDPs) \(\{(S,A,,_{},_{0,},)\}_{}\), where the distribution of transitions and rewards \(_{}\) and the initial state distribution \(_{0,}\) both depend on task \(\). The task itself is drawn from a distribution \( D\) over a general space \(\), and is not known to the agent. The agent can form a belief regarding the current \(\) based on the task history \(h\), which consists of repeating triplets of states, actions and rewards (Zintgraf et al., 2019). Thus, the meta-policy \(_{}(a;s,h)\) (\(\)) maps the current state \(s S\) and the history \(h(S A)\) (consisting of state-action-reward triplets) to a probability distribution over actions.

A meta-rollout is defined as a sequence of \(K 1\) episodes of length \(T\) over a single task \(\): \(=\{\{(s_{k,t},\,a_{k,t},\,r_{k,t})\}_{t=1}^{T}\}_{k=1}^{K}\). For example, in a driving problem, \(\) might be a geographic area or type of roads, and \(\) a sequence of drives on these roads. The return of the agent over a meta-rollout is defined as \(R()=_{k=1}^{K}_{t=0}^{T}^{t}r_{k,t}\), where \(r_{k,t}\) is the (random variable) reward at step \(t\) in episode \(k\). Given a task \(\) and a meta-policy \(_{}\), we denote by \(P^{}_{}(x)\) the conditional PDF of the return \(R\). With a slight abuse of notation, we shall use \(P^{}_{}()\) to also denote the PDF of the meta-rollout itself. The standard MRL objective is to maximize the expected return \(J^{}(R)=_{,R}[R]\).

While the meta policy \(_{}(s,a;h)\) is history-dependent, it can still be learned using standard policy gradient (PG) approaches, by considering \(h\) as part of an extended state space \(=(s,h)\). Then,the policy gradient can be derived directly:

\[_{}J^{}(R)=_{}D(z)_{-}^{}(x-b)_{ }P_{z}^{}(x) dx dz, \]

where \(D\) is a probability measure over the task space \(\), \(P_{z}^{}(x)\) is the PDF of \(R\) (conditioned on \(_{}\) and \(=z\)), and \(b\) is any arbitrary baseline that is independent of \(\)(Agrawal, 2019). While a direct gradient estimation via Monte Carlo sampling is often noisy, its variance can be reduced by an educated choice of baseline \(b\). In the common actor-critic framework (Mnih et al., 2016), a learned value function \(b=V(s;h)\) is used. This approach is used in many SOTA algorithms in deep RL, e.g., PPO (Schulman et al., 2017); and by proxy, in MRL algorithms that rely on them, e.g., VariBAD (Zintgraf et al., 2019).

A major challenge in MRL is the extended state space \(\), which now includes the whole task history. Common algorithms handle the task history via a low-dimensional embedding that captures transitions and reward function (Zintgraf et al., 2019); or using additional optimization steps w.r.t. task history (Finn et al., 2017). Our work does not compete with such methods, but rather builds upon them: our methods operate as meta-algorithms that run on top of existing MRL baselines.

**CVaR-PG.** Before moving on to CVaR optimization in MRL, we first recap the common PG approach for standard (non-meta) RL. For a random variable \(X\) and \(\)-quantile \(q_{}(X)\), the CVaR is defined as \(_{}(X)=[X\,|\,X q_{}(X)]\). For an MDP \((S,A,P,P_{0},)\), the CVaR-return objective is \(_{}^{}(R)=_{R P^{}}^{}[R]= _{-}^{q_{}^{}(R)}x P^{}(x) dx\), whose corresponding policy gradient is (Tamar et al., 2015):

\[_{}_{}^{}(R)=_{-}^{q_{}^{ }(R)}(x-q_{}^{}(R))_{}P^{}(x) dx. \]

Given a sample of \(N\) trajectories \(\{\{(s_{i,t},\,a_{i,t})\}_{t=1}^{T}\}_{i=1}^{N}\) with returns \(\{R_{i}\}_{i=1}^{N}\), the policy gradient can be estimated by (Tamar et al., 2015; Rajeswaran et al., 2017):

\[_{}_{}^{}(R)_{i =1}^{N}_{R_{i}_{}^{}}(R_{i}-_{ }^{})_{t=1}^{T}_{}_{}(a_{i,t};s _{i,t}), \]

where \(_{}^{}\) is an estimator of the current return quantile.

Notice that in contrast to mean-PG, in CVaR optimization the baseline _cannot_ follow an arbitrary critic, but should approximate the total return quantile \(q_{}^{}(R)\). Tamar et al. (2015) showed that any baseline \(b q_{}^{}(R)\) inserts bias to the CVaR gradient estimator, potentially to the level of pointing to the opposite direction (as discussed in Appendix A.1 and Fig. 6). As a result, CVaR-PG methods in RL either are limited to basic REINFORCE with a constant baseline (Greenberg et al., 2022), or use a critic for variance reduction at the cost of biased gradients (Rajeswaran et al., 2017).

Another major source of gradient-variance in CVaR-PG is its reduced sample efficiency: notice that Eq. (5) only exploits \( N\) trajectories out of each batch of \(N\) trajectories (due to the term \(_{R_{i}_{}^{}}\)), hence results in estimation variance larger by a factor of \(^{-1}\).

## 4 CVaR Optimization in Meta-Learning

In this section, we show that **unlike standard RL, CVaR-PG in MRL permits a flexible baseline _without_ presenting biased gradients**. Hence, policy gradients for _CVaR_ objective in _MRL_ is substantially different from both _mean_-PG in MRL (Eq. (3)) and CVaR-PG in _RL_ (Eq. (4)).

To derive the policy gradient, we first define the policy value per task and the tail of tasks.

**Definition 1**.: The value of policy \(_{}\) in task \(\) is denoted by \(V_{}^{}=_{R P_{}^{}}[R]\). Notice that \(V_{}^{}\) depends on the random variable \(\). We define the \(\)-tail of tasks w.r.t. \(_{}\) as the tasks with the lowest values: \(_{}^{}=\{z\,|\,V_{z}^{} q_{}(V_{ }^{})\}\).

**Assumption 1**.: To simplify integral calculations, we assume that for any \(z\) and \(\), \(R\) is a continuous random variable (i.e., its conditional PDF \(P_{z}^{}(x)\) has no atoms). We also assume that \(v(z)=V_{z}^{}\) is a continuous function for any \(\).

**Theorem 1** (Meta Policy Gradient for CVaR).: Under Assumption 1, the policy gradient of the CVaR objective in Eq. (2) is

\[_{}J_{}^{}(R)=_{_{}^{}}D(z)_{- }^{}(x-b)_{}P_{z}^{}(x) dx dz, \]

where \(b\) is _any_ arbitrary baseline independent of \(\).

Proof intuition (the formal proof is in Appendix A).: In RL, the CVaR objective measures the \(\) lowest-return trajectories. When the policy is updated, the cumulative probability of these trajectories changes and no longer equals \(\). Thus, the new CVaR calculation must add or remove trajectories (as visualized in Fig. 6 in the appendix). This adds a term in the gradient calculation, which causes the bias in CVaR-PG. By contrast, in MRL, the CVaR measures the \(\) lowest-return _tasks_\(_{}^{}\). Since the task distribution does not depend on the policy, the probability of these tasks is not changed - but only the way they are handled by the agent (Fig. 7). Thus, no bias term appears in the calculation. Note that \(_{}^{}\) does change throughout the meta-learning - due to changes in task _values_ (rather than task probabilities); this is a different effect and is not associated with gradient bias. 

According to Theorem 1, the CVaR PG in MRL permits _any_ baseline \(b\). As discussed in Section 3, this flexibility is necessary, for example, in any actor-critic framework.

To estimate the gradient from meta-rollouts of the tail tasks, we transform the integration of Eq. (6) into an expectation:

**Corollary 1**.: Eq. (6) can be written as

\[_{}J_{}^{}(R)=_{ D}[ _{ P_{}^{}}[g()]V_{}^{ } q_{}(V_{}^{})], \]

where \(g()=(R()-b)_{1 k K,\\ 1 t T}_{}_{}(a_{k,t};\,_{k,t})\); and \(_{k,t}=(s_{k,t},h_{k,t})\) is the extended state (that includes all the task history \(h_{k,t}\) until trajectory \(k\), step \(t\)).

Proof.: We apply the standard log trick \(_{}P_{z}^{}=P_{z}^{}_{} P_{z}^{}\) to Eq. (6), after substituting the meta-rollout PDF: \(P_{z}^{}()=_{k=1}^{K}[P_{0,z}(s_{k,0})_{t=1} ^{T}P_{z}(s_{k,t+1},r_{k,t}\,|\,s_{k,t},a_{k,t})_{}(a_{k,t};\,_{k,t})].\) 

For a practical Monte-Carlo estimation of Eq. (7), given a task \(z_{i}\), we need to estimate whether \(V_{z_{i}}^{} q_{}(V_{}^{})\). To estimate \(V_{z_{i}}^{}\), we can generate \(M\) i.i.d meta-rollouts with returns \(\{R_{i,m}\}_{m=1}^{M}\), and calculate their average return \(_{z_{i}}^{}=R_{i}=_{m=1}^{M}R_{i,m}/M\). Then, the quantile \(q_{}(V_{}^{})\) can be estimated over a batch of tasks \(_{}=q_{}(\{R_{i}\}_{i=1}^{N})\). If \(_{z_{i}}^{}_{}\), we use _all_ the meta-rollouts of \(z_{i}\) for the gradient calculation (including meta-rollouts that by themselves have a higher return \(R_{i,m}>_{}\)). Notice that we use \(M\) i.i.d meta-rollouts, each consisting of \(K\) episodes (the episodes within a meta-rollout are _not_ independent, due to agent memory).

Putting it together, we obtain the sample-based gradient estimator of Eq. (7):

\[_{}J_{}^{}(R)_{i=1}^{N}_{R_{i} q_{}^{}}_{m=1} ^{M}g_{i,m},\\ g_{i,m}(R_{i,m}-b)_{k=1}^{K}_{t=1}^{T}_{ }_{}(a_{i,m,k,t};\,_{i,m,k,t}), \]

where \(a_{i,m,k,t},\,_{i,m,k,t}\) are the action and the state-and-history at task \(i\), meta-rollout \(m\), trajectory \(k\) and step \(t\).

The procedure described above follows the principles of CVaR-PG in (non-meta) RL, as the learning rule is only applied to the tail of the sampled batch. However, in MRL we consider a batch of tasks rather than a batch of trajectories. As discussed in Theorem 1 and its proof, this distinction has a substantial impact on the gradient and the resulting algorithm. Specifically, Eq. (8) allows for greater flexibility than Eq. (4), as it permits any baseline \(b\) that does not depend on \(\). This allows gradient calculation using any PG algorithm, including SOTA methods such as PPO  (which are already used in MRL methods such as VariBAD Zintgraf et al. ). Therefore, in contrast to standard RL, CVaR optimization is not restricted to basic REINFORCE.

pervised setting as well with minimal modifications, namely, replacing meta-rollouts with examples and returns with losses.

## 5 Efficient CVaR-ML

Theorem 1 guarantees unbiased gradients when using Algorithm 1; however, it does not bound their variance. In particular, Line 8 applies the learning step to a subset of only \( NM\) meta-rollouts out of \(NM\), which increases the estimator variance by a factor of \(^{-1}\) compared to mean optimization. This could be prevented if we knew the set \(_{}^{}\) of tail-tasks (for the current \(_{}\)), and sampled only these tasks, using the distribution \(D_{}^{}(z)=^{-1}_{V_{}^{} q_{ }(V_{}^{})}D(z)\). Proposition 1 shows that this would indeed recover the sample efficiency.

**Proposition 1** (Variance reduction).: Denote by \(G\) the estimator of \(_{}J_{}^{}(R)\) in Eq. (8), assume there is no quantile error (\(_{}^{}=q_{}^{}\)), and denote \(_{D}[]=_{z_{i} D,\,R_{i,m} P_{z_{i}}^{} }[]\). Then, switching the task sample distribution to \(D_{}^{}\) leads to a variance reduction of factor \(\):

\[_{D_{}^{}}[ G]=_{D}[G],_{D_{}^{}}( G)_{D}(G).\]

Proof sketch (the complete proof is in Appendix B).: We calculate the expectation and variance directly. \(G\) is proportional to \(_{R_{i} q_{}^{}}\) (Eq. (8)). The condition \(R_{i} q_{}^{}\) leads to multiplication by the probability \(\) when sampled from \(D\) (where it corresponds to the \(\)-tail); but not when sampled from \(D_{}^{}\) (where it is satisfied w.p. 1). This factor \(\) cancels out the ratio between the expectations of \(G\) and \( G\) (thus the expectations are identical) - but not the ratio \(^{2}\) between their variances. 

Figure 2: **Left**: RoML uses the cross entropy method to modify the task distribution \(D_{}\), which is used to generate the next meta-rollouts. **Right**: illustration of an arbitrary point of time in training: the task distribution \(D_{}\) (blue) is learned according to the task values of the current meta-policy \(_{}\) (red). Since low-return tasks are over-sampled, the learned meta-policy is more robust to the selection of task.

Following the motivation of Proposition 1, we wish to increase the number of train tasks that come from the tail distribution \(_{}^{}\). To that end, we assume to have certain control over the sampling of training tasks. This assumption is satisfied in most simulated environments, as well as many real-world scenarios. For example, when training a driver, we choose the tasks, roads and times of driving throughout training. In this section, we propose a method to make these choices.

We begin with parameterizing the task distribution \(D\): we consider a parametric family \(D_{}\) such that \(D=D_{_{0}}\). Then, we wish to modify the parameter \(\) so that \(D_{}\) aligns with \(D_{}^{}\) as closely as possible. To that end, we use the Cross Entropy Method (CEM, de Boer et al. (2005)), which searches for \(^{*}\) that minimizes the KL-divergence (i.e., cross entropy) between the two:

\[^{*}_{^{}}\,D_ {KL}(D_{}^{}\,||\,D_{^{}})&=\; _{^{}}\,}_{z D_{ _{0}}}[_{V_{z}^{} q_{}(V_{}^{}) } D_{^{}}(z)]\\ &=\;_{^{}}\,}_{z D_{}}[w(z)\,_{V_{z}^{} q_{}(V_{ }^{})} D_{^{}}(z)], \]

where \(w(z)=}(z)}{D_{}(z)}\) is the importance sampling weight corresponding to \(z D_{}\). Note that Eq. (9) has a closed-form solution for most of the standard families of distributions (de Boer et al., 2005).

```
1Input: Meta-learning algorithm (Definition 2); robustness level \((0,1]\); parametric task distribution \(D_{}\); original parameter \(_{0}\); \(N\) tasks per batch, \([0,1)\) of them sampled from the original \(D_{_{0}}\); CEM quantile \((0,1)\)
2Initialize:
3\(_{0}, N_{} N, N_{s }(1-)N\)
4while not finished training do // Sample tasks
5 Sample \(\{z_{,i}\}_{i=1}^{N_{}} D_{_{0}},\{z_{,i}\}_{i=1} ^{N_{s}} D_{}\)
6\(z(z_{0,1},,z_{0,N_{s}},z_{,1},,z_{0,N_{s}})\) // Rollouts and meta-learning step
7\(\{_{i}\}_{i=1}^{N}(\{z_{i}\}_{i=1}^{N})\)
8\(R_{i}(_{i}), i\{1,,N\}\)
9\((\{_{i}\}_{i=1}^{N})\) // Estimate reference quantile
10\(w_{o,i} 1, i\{1,,N_{o}\}\)
11\(w_{,i}}(z_{o,i})}{D_{}(z_{o,i})}, i \{1,,N_{s}\}\)
12\(w(w_{o,N_{s}},w_{o,N_{a}},w_{,1},,w_{,N_{s}})\)
13\(_{}(\{R_{i}\},w,\,)\)// Compute sample quantile
14\(q_{}(\{R_{i}\},\,)\)// Update sampler
15\(q(_{},\,q_{})\)\(_{^{}}_{i N}w_{i}\, _{R_{i} q} D_{^{}}(z_{i})\)
```

**Algorithm 2**Robust Meta RL (RoML)

For a batch of \(N\) tasks sampled from \(D=D_{_{0}}\), Eq. (9) essentially chooses the \( N\) tasks with the lowest returns, and updates \(\) to focus on these tasks. This may be noisy unless \( N 1\). Instead, the CEM chooses a larger number of tasks \(>\) for the update, where \(\) is a hyper-parameter. \(\) is updated according to these \( N\) lowest-return tasks, and the next batch is sampled from \(D_{} D_{_{0}}\). This repeats iteratively: every batch is sampled from \(D_{}\), where \(\) is updated according to the \( N\) lowest-return tasks of the former batch. Each task return is also compared to the \(\)-quantile of the _original_ distribution \(D_{_{0}}\). If more than \( N\) tasks yield lower returns, the CEM permits more samples for the update step. The return quantile over \(D_{_{0}}\) can be estimated from \(D_{}\) at any point using importance sampling weights. See more details about the CEM in Appendix C.1.

In our problem, the target distribution is the tail of \(D_{_{0}}\). Since the tail is defined by the agent returns in these tasks, it varies with the agent and is non-stationary throughout training. Thus, we use the dynamic-target CEM of Greenberg (2022). To smooth the changes in the sampled tasks, the sampler is also regularized to always provide certain \(D=D_{_{0}}\), and only \(1-\) percent from \(D_{}\).

Putting this together, we obtain the Robust Meta RL algorithm (**RoML**), summarized in Algorithm 2 and Fig. 2. RoML does not require multiple meta-rollouts per update (parameter \(M\) in Algorithm 1), since it directly models high-risk tasks. Similarly to CVaR-ML, RoML is a meta-algorithm and can operate on top of any meta-learning baseline (Definition 2). Given the baseline implementation, only the tasks sampling procedure needs to be modified, which makes RoML easy to implement.

**Limitations:** The CEM's adversarial tasks sampling relies on several assumptions. Future research may reduce some of these assumptions, while keeping the increased data efficiency of RoML.

First, as mentioned above, we need at least partial control over the selection of training tasks. This assumption is common in other RL frameworks (Dennis et al., 2020; Jiang et al., 2021), and often holds in both simulations and the real world (e.g., choosing in which roads and hours to train driving).

Second, the underlying task distribution \(D\) is assumed to be known, and the sample distribution is limited to the chosen parameterized family \(\{D_{}\}\). For example, if \( U()\), the user may choose the family of Beta distributions \(Beta(_{1},_{2})\) (where \(Beta(1,1) U()\)), as demonstrated in Appendix D.2. The selected family expresses implicit assumptions on the task-space. For example, if the probability density function is smooth, close tasks will always have similar sample probabilities; and if the family is unimodal, high-risk tasks can only be over-sampled from around a single peak. This approach is useful for generalization across continuous task-spaces - where the CEM can never observe the infinitely many possible tasks. Yet, it may pose limitations in certain discrete task-spaces, if there is no structured relationship between tasks.

## 6 Experiments

We implement **RoML** and **CVaR-ML** on top of two different risk-neutral MRL baselines - **VariBAD**(Zintgraf et al., 2019) and **PEARL**(Rakelly et al., 2019). As a risk-averse reference for comparison, we use **CeSoR**(Greenberg et al., 2022), an efficient sampling-based method for CVaR optimization in RL, implemented on top of PPO. As another reference, we use the Unsupervised Environment Design algorithm **PAIRED**(Dennis et al., 2020), which uses regret minimization to learn robust policies on a diverse set of tasks.

Section 6.1 demonstrates the mean/CVaR tradeoff, as our methods learn substantially different policies from their baseline. Section 6.2 demonstrates the difficulty of the naive CVaR-ML in more challenging control benchmarks, and the RoML's efficacy in them. The ablation test in Appendix D.4 demonstrates that RoML deteriorates significantly when the CEM is replaced by a naive adversarial sampler. Section 6.3 presents an implementation of CVaR-ML and RoML on top of **MAML**(Finn et al., 2017) for _supervised_ meta-learning. In all the experiments, the running times of RoML and CVaR-ML are indistinguishable from their baselines (RoML's CEM computations are negligible).

**Hyper-parameters:** To test the practical applicability of RoML as a meta-algorithm, in every experiment, **we use the same hyper-parameters for RoML, CVaR-ML and their baseline**. In particular, we use the baseline's default hyper-parameters whenever applicable (Zintgraf et al. (2019), Rakelly et al. (2019) in Section 6.2, and Finn et al. (2017) in Section 6.3). That is, we use the same hyper-parameters as originally tuned for the baseline, and test whether RoML improves the robustness without any further tuning of them. As for the additional hyper-parameters of the meta-algorithm itself: in Algorithm 1, we use \(M=1\) meta-rollout per task; and in Algorithm 2, we use \(=0.2,\,=0\) unless specified otherwise (similarly to the CEM in Greenberg et al. (2022)). For the references PAIRED and CeSoR, we use the hyper-parameters of Dennis et al. (2020), Greenberg et al. (2022). Each experiment is repeated for 30 seeds. See more details in Appendix D. The code is available in our repositories: VariBAD, PEARL, CeSoR, PAIRED and MAML.

### Khazad Dum

We demonstrate the tradeoff between mean and CVaR optimization in the Khazad Dum benchmark, visualized in Fig. 3. The agent begins at a random point in the bottom-left part of the map, and has to reach the green target as quickly as possible, without falling into the black abyss. The bridge is not covered and thus is exposed to wind and rain, rendering its floor slippery and creating an additive action noise (Fig. 3b) - to a level that varies with the weather. Each task is characterized by the rain intensity, which is exponentially distributed. The CEM in RoML is allowed to modify the parameter of this exponential distribution. Note that the agent is not aware of the current task (i.e, the weather), but may infer it from observations. We set the target risk level to \(=0.01\), and train each meta-agent for a total of \(5 10^{6}\) frames. See complete details in Appendix D.1.

We implement CVaR-ML and RoML on top of VariBAD. As shown in Fig. 3, VariBAD learns to try the short path - at the risk of rare falls into the abyss. By contrast, our CVaR-optimizing methods take the longer path and avoid risk. This policy increases the cumulative cost of time-steps, but leads to higher CVaR returns, as shown in Fig. 3f. In addition to superior CVaR, RoML also provides competitive _average_ returns in this example (Fig. 3e). Finally, in accordance with Proposition 1, RoML learns significantly faster than CVaR-ML.

### Continuous Control

We rely on standard continuous control problems from the MuJoCo framework (Todorov et al., 2012): training a cheetah to run (**HalfCheetah**), and training a **Humanoid** and an **Ant** to walk. For each of the 3 environments, we create 3 meta-learning versions: (1) _Goal_ or _Vel_(Finn et al., 2017), where each task corresponds to a different location or velocity objective, respectively; (2) _Mass_, where each task corresponds to a different body mass; and (3) _Body_, where each task corresponds to different mass, head size and physical damping level (similarly to Wang and Van Hoof (2022)). In addition, to experiment with high-dimensional task spaces, we randomly draw 10 numeric variables from _env.model_ in HalfCheetah, and let them vary between tasks. We define 3 such environments with different random sets of task variables (**HalfCheetah 10D-task a,b,c**). For each of the 12 environments above, we set a target risk level of \(=0.05\) and optimize for \(K=2\) episodes per task. Additional implementation details are specified in Appendix D.2.

Interestingly, the naive approach of **CVaR-ML** consistently fails to meta-learn in all the cheetah environments. It remains unsuccessful even after large number of steps, indicating a difficulty beyond sample inefficiency. A possible explanation is the effectively decreased batch size of CVaR-ML. **PAIRED** and **CeSoR** also fail to adjust to the MRL environments, and obtain poor CVaR returns.

**RoML**, on the other hand, consistently improves the CVaR returns (Table 1) compared to its baseline (VariBAD or PEARL), while using the same hyper-parameters as the baseline. The VariBAD baseline presents better returns and running times than PEARL on HalfCheetah, and thus is used for the 6 extended environments (Humanoid and Ant). RoML improves the CVaR return in comparison to the baseline algorithm in all the 18 experiments (6 with PEARL and 12 with VariBAD).

In 5 out of 18 experiments, RoML slightly improves the _average_ return compared to its baseline, and not only the CVaR (Table 2 in the appendix). This indicates that low-return tasks can sometimes be improved at the cost of high-return tasks, but without hurting average performance. In addition, this may indicate that over-sampling difficult tasks forms a helpful learning curriculum.

The robustness of RoML to the selected task is demonstrated in Fig. 4d. In multi-dimensional task spaces, RoML learns to focus the sampling modification on the high-impact variables, as demonstrated

Figure 4: MuJoCo: (aâ€“c) Sample frames, where tasks vary in mass, head size and damping level. Notice that in Humanoid, RoML handles the low-return tasks of large mass by leaning the center of mass forward, so that gravity pulls the humanoid forward. (d) Average return per range of tasks in HalfCheetah-Mass. RoML learns to act robustly: it is less sensitive to the task, and in particular performs better on high-risk tasks.

Figure 3: Khazad-Dum: (a-d) Sample episodes. (e-f) Test return vs. training iteration, with 95% confidence intervals over 30 seeds.

in Fig. 11 in the appendix. Finally, qualitative inspection shows that RoML learns to handle larger masses, for example, by leaning forward and letting gravity pull the agent forward, as displayed in Fig. 4c (see animations on GitHub).

### Beyond RL: Robust Supervised Meta-Learning

Our work focuses on robustness in MRL. However, the concept of training on harder data to improve robustness, as embodied in RoML, is applicable beyond the scope of RL. As a preliminary proof-of-concept, we apply RoML to a toy supervised meta-learning problem of sine regression, based on Finn et al. (2017): The input is \(x[0,2)\), the desired output is \(y=A( x+b)\), and the task is defined by the parameters \(=(A,b,)\). Similarly to Finn et al. (2017), the model is fine-tuned for each task via a gradient-descent optimization step over 10 samples \(\{(x_{i},y_{i})\}_{i=1}^{10}\), and is tested on another set of 10 samples. The goal is to find model weights that adapt quickly to new task data.

We implement CVaR-ML and RoML on top of MAML Finn et al. (2017). As shown in Fig. 5, RoML achieves better CVaR losses over tasks than both CVaR-ML and MAML. The complete setting and results are presented in Appendix E.

## 7 Summary and Future Work

We defined a robust MRL objective and derived the CVaR-ML algorithm to optimize it. In contrast to its analogous algorithm in standard RL, we proved that CVaR-ML does not present biased gradients, yet it does inherit the same data inefficiency. To address the latter, we introduced RoML and demonstrated its advantage in sample efficiency and CVaR return.

Future research may address the CEM-related limitations of RoML discussed at the end of Section 5. Another direction for future work is extension of RoML to other scenarios, especially where a natural task structure can be leveraged to improve task robustness, e.g., in supervised learning and coordinate-based regression Tancik et al. (2020).

Finally, RoML is easy to implement, operates agnostically as a meta-algorithm on top of existing MRL methods, and can be set to any desired robustness level. We believe that these properties, along with our empirical results, make RoML a promising candidate for MRL in risk-sensitive applications.

**Acknowledgements:** This work has received funding from the European Union's Horizon Europe Programme, under grant number 101070568.

    &  &  \\  & Vel & Mass & Body & (a) & (b) & (c) \\  CeSoR & \(-2606 25\) & \(902 36\) & \(478 27\) & \(637 26\) & \(981 31\) & \(664 26\) \\ PAIRED & \(-725 65\) & \(438 37\) & \(218 51\) & \(229 59\) & \(354 53\) & \(81 65\) \\ CVaR-ML & \(-897 23\) & \(38 6\) & \(76 5\) & \(120 11\) & \(141 11\) & \(81 4\) \\ PEARL & \(-1156 23\) & \(1115 19\) & \(800 5\) & \(1140 33\) & \(1623 23\) & \(\) \\ VariBAD & \(-202 6\) & \(107