# Learnability of high-dimensional targets by two-parameter models and gradient flow

Dmitry Yarotsky

Skoltech

d.yarotsky@skoltech.ru

###### Abstract

We explore the theoretical possibility of learning \(d\)-dimensional targets with \(W\)-parameter models by gradient flow (GF) when \(W<d\). Our main result shows that if the targets are described by a particular \(d\)-dimensional probability distribution, then there exist models with as few as two parameters that can learn the targets with arbitrarily high success probability. On the other hand, we show that for \(W<d\) there is necessarily a large subset of GF-non-learnable targets. In particular, the set of learnable targets is not dense in \(^{d}\), and any subset of \(^{d}\) homeomorphic to the \(W\)-dimensional sphere contains non-learnable targets. Finally, we observe that the model in our main theorem on almost guaranteed two-parameter learning is constructed using a hierarchical procedure and as a result is not expressible by a single elementary function. We show that this limitation is essential in the sense that most models written in terms of elementary functions cannot achieve the learnability demonstrated in this theorem.

## 1 Introduction

Starting from the works of Cantor , it is well-known that all finite-dimensional (or even countably-dimensional) real spaces are equiumerable and so, in principle, a set of several real numbers is as descriptive as a single number, or in other words multi-dimensional vectors can be represented by scalars. The idea of reduction of higher-dimensional descriptions to lower-dimensional ones has since appeared in many mathematical works. A couple of notable examples are continuous space-filling curves that fill the whole \(^{d}\) and the Kolmogorov-Arnold Superposition Theorem (KST, Kolmogorov ) that states that any multivariate continuous function can be exactly represented in terms of compositions and sums of finitely many univariate continuous functions.

In the context of machine learning, these results suggest that models with a small number of parameters can potentially be used to represent or approximate high-dimensional objects. In particular, Maiorov and Pinkus  give an example of neural network that has a fixed number of weights but can approximate any continuous function:

**Theorem 1** (Maiorov and Pinkus 1999).: _There exists an activation function \(\) which is real analytic, strictly increasing, sigmoidal (i.e., \(_{x-}(x)=0\) and \(_{x+}(x)=1\)), and such that any \(f C(^{n})\) can be uniformly approximated with any accuracy by expressions \(_{i=1}^{6n+3}d_{i}(_{j=1}^{3n}c_{ij}(_{k=1}^{n}w_{ijk} x_{k}+_{ij})+_{i})\) with some parameters \(d_{i},c_{ij},w_{ijk},_{ij},_{i}\)._

Refinements of this result are given by Guliyev and Ismailov . While Theorem 1 contains a non-explicit function \(\), Boshernitzan , Laczkovich and Ruzsa , Yarotsky  give examples of fully explicit fixed-size analytic expressions that also can approximate arbitrary continuous functions. KST has inspired many other results on expressiveness of machine learning models, see e.g. Montanelli and Yang , Schmidt-Hieber , Kurkova ,Koppen (2002); Igelnik and Parikh (2003). The idea of space-filling curves is used in recent works on generating higher-dimensional distributions from low-dimensional ones (Bailey and Telgarsky, 2018; Perekrestenko et al., 2020, 2021).

While the model appearing in Theorem 1 looks like a standard neural network (apart from the special activation), the proof of its universal approximation property has nothing to do with the method of gradient descent (GD) invariably used nowadays to train neural networks. The proofs of the universal approximation property in this and similar theorems (including the classical universal approximation theorems of Cybenko (1989); Leshno et al. (1993) that consider neural networks with a growing number of neurons) normally consist in presenting, or demonstrating existence of, parameters making the model output arbitrarily close to the target \(f\). There is no guarantee whatsoever that these parameters can actually be learned by GD. Moreover, learning by GD is especially problematic for models with a small number of parameters.

In this regard, note that modern deep neural networks are typically abundantly parameterized, with the largest models containing hundreds of billions of weights (Brown et al., 2020; Smith et al., 2022). One obvious reason for that is the necessity to store a substantial amount of information. But another, more subtle property of large models is that they are easier to train by GD-based optimization (Choromanska et al., 2015), which can be explained by the optimizer having more freedom in finding good descent directions, in particular evading spurious local minima and saddle points.

A convincing and rigorous demonstration that overparameterization may be beneficial for training is provided by the infinite width limits of neural networks in regimes such as NTK (Jacot et al., 2018) and Mean-Field (Mei et al., 2018; Rotskoff and Vanden-Eijnden, 2018; Chizat and Bach, 2018). While the number of weights in these limits is effectively infinite, the resulting macroscopic loss surface is relatively simple (even convex after reparameterization, in the NTK case); the GD dynamics is analytically tractable and, under mild assumptions, provably trains the model to perfect fit.

In contrast, if the number of parameters is small, then the loss surface tends to be rough and GD inefficient (Baity-Jesi et al., 2018). Suboptimal local minima are known to be a general feature of finite neural networks with nonlinearities (Auer et al., 1995; Yun et al., 2018; Swirszcz et al., 2016; Zhou and Liang, 2017; Christof and Kowalczyk, 2023). As the number of parameters is decreased, the chances for GD to get trapped in a bad local minimum increase (Safran and Shamir, 2018).

The above discussion raises a natural abstract question that we address in the present paper:

_Can models with a small number of parameters learn high-dimensional targets by gradient descent?_

In other words, we ask if the possibility of a reduction of a high-dimensional target space to a low-dimensional parametric description reflected in Theorem 1 can at least theoretically be combined with learning by GD, or if this is prevented by fundamental obstacles.

We are not aware of existing rigorous results addressing this question. As remarked, existing results on approximation by highly expressive models do not discuss learning by GD, while publications on GD usually consider standard models such as conventional neural networks. However, we want to address the above question in the most abstract way without assuming any particular model structure. It is clear that models having a small number of parameters and yet GD-learnable, if at all possible, require a very special design.

**Our contribution** in this paper is a resolution of the above question.

1. Our main result is the proof that if the learned targets are represented as \(d\)-dimensional vectors and are described by a probability distribution in \(^{d}\), then there exist models with just \(W=2\) parameters that can learn these targets by Gradient Flow (GF) with success probability arbitrarily close to 1 (Theorem 6).
2. We show that Theorem 6 is actually close to being optimal, since underparameterization with \(W<d\) generally implies severe constraints on the set of GF-learnable targets: 1. Under a mild nondegeneracy assumption, the GF-learnable targets are not dense in \(^{d}\) (Theorem 3). In particular, the success probability cannot generally be made exactly equal to 1 in Theorem 6. 2. In contrast, the non-learnable targets are dense in \(^{d}\). Moreover, any subset of \(^{d}\) homeomorphic to the \(W\)-sphere contains non-learnable targets (Theorem 4).

3. The number of parameters in Theorem 6 cannot be decreased to one.
4. In the proof of Theorem 6, the model is constructed using an infinite hierarchical procedure making it not expressible by a single elementary function. We conjecture that the result established in Theorem 6 cannot be achieved with models implementable by elementary functions. For such functions not involving \(\) or \(\) with unbounded arguments, we prove this as a consequence of the closure of the model image having zero Lebesgue measure in the target space.

We describe the details of our setting in Section 2. In Section 3 we give several general results showing that the underparameterized (\(W<d\)) learning is theoretically challenging. Then, in Section 4 we present our main result on the almost guaranteed learnability with two parameters. After that, in Section 5 we consider models expressible by elementary functions. Finally, in Section 6 we summarize our findings and discuss several questions that are left open by our research.

Some (more complex or less important) proofs are given in the appendix; in these cases the respective sections are indicated in the theorem statements.

## 2 The setting

In supervised learning one is usually interested in learning _target functions_ (or simply _targets_) \(f:X Y\), with some input and output spaces \(X\) and \(Y\). We will consider the setting in which the space of targets is a linear space with a euclidean structure. To this end, suppose that \(Y\) is a euclidean space with a scalar product \(,\), and \(X\) is endowed with a measure \(\) reflecting the distribution of inputs \( X\) of the function \(f\). One can then form the Hilbert space \(L^{2}(X,Y,)\) of functions \(f:X Y\) equipped with the standard scalar product \( f,g=_{X} f(),g()(d )\). We will assume that the _target space_\(\) is a (finite- or infinite-dimensional) subspace of this Hilbert space \(L^{2}(X,Y,)\).

**Examples:**

1. The full space \(=L^{2}(X,Y,)\) represents all maps \(f:X Y\) distinguishable on sets of positive measure \(\). If \(Y=^{m}\) with the standard scalar product and \(=_{n=1}^{N}_{_{n}}\) is an empirical distribution corresponding to a finite subset of \(X\), then \(\) is finite-dimensional, \(^{d}\) with \(d=Nm\). On the other hand, if \(\) is not finitely supported, then \(=\).
2. Let \(X=^{n},Y=\), and the targets \(f:X Y\) be linear functions. If \(\) is nondegenerate in the sense that the covariance matrix \([ x_{i}x_{j}(d)]_{i,j=1}^{n}\) is full-rank, then the respective target space \(\) is \(n\)-dimensional (otherwise, \(<n\)).
3. Let \(X=^{n},Y=\), and the targets be polynomials of degree not larger than \(q\). Then \(\), with equality attained for suitably nondegenerate measures \(\).

We will often write the function \(f\) considered as an element of \(\) as \(\). Throughout the paper, we refer to the dimension of the target space \(\) as _target dimension_ and denote it by \(d\). Note that the target dimension \(d\) is not to be confused with the dimensions of \(X\) and \(Y\) (in fact, \(X\) does not even have to be a linear space and have a dimension). Rather, the target dimension \(d\) is the number of scalar parameters required to specify a particular target \(\) within the space \(\) of considered targets.

Suppose that we are learning the target \(f\) using a parametric model with \(W\) parameters. We will view this model as a map \(:^{W}\) between the parameter space \(^{W}\) and the target space \(\).

We will consider Gradient Flow (GF), i.e. the continuous version of gradient descent. Learning by GF prescribes that the parameter vector \(\) be evolved by

\[(t)}{dt}=-_{}L_{}((t)), \]

where we use the standard square loss, \(L_{}()=_{}\|f( )-()()\|_{Y}^{2}\), which can equivalently be written as

\[L_{}()=\|-()\|_{ }^{2}. \]

Here, the subscripts \(Y,\) on the norms indicate the respective spaces. We will assume for definiteness that GF starts from \((t=0)=\).

We will always assume that \(W\) is finite and \(\) is differentiable with a Lipschitz-continuous differential. In this case Eq. (1) is, by Picard-Lindelof theorem, uniquely solvable locally (i.e., for a bounded interval of times). It is possible to relax the Lipschitz differentiability assumption using the special structure of the gradient flow equation (see e.g. the expository paper Santambrogio (2017)), but we will not need this in this work.

In fact, GF (1) is solvable not only locally, but also globally, i.e. the solution \((t)\) exists for any \(t>0\). Indeed, the only obstacle for the global existence is the divergence of the solution in finite time, but it is ruled out by the inequality

\[\|(t)\|^{2}_{0}^{t}\|( )}{d}\|d^{2} t_{0}^{t}\|()}{d }\|^{2}d \] \[=-t_{0}^{t}}(())}{d}d =(L_{}()-L_{}((t)))t L_{ }()t.\]

Let \(F_{}\) denote the set of targets \(\) for which the respective GF converges to \(\):

\[F_{}=\{:_{t}L_{}((t))=0 (t))}\}. \]

Our goal in the remainder of this work will be to examine if we can ensure, by a suitable design of the map \(:^{W}^{d}\) with \(W<d\), that the set \(F_{}\) is sufficiently large.1 We will refer to targets \( F_{}\) as _GF-learnable_ or simply _learnable_. We remark that, assuming a standard norm topology in \(\), the set \(F_{}\) is Borel-measurable as the countable intersection of the open sets \(\{:_{t}L_{}((t))<1/n,(t))}\},n=1,2,\)

**Example:** To clarify our setting, suppose that we fit to data a linear function \(f()=^{T}\) with \(,^{d}\) for some \(d\) (see example 2 earlier in the section). Normally, this function is learned by applying GF to the \(d\)-dimensional parameter vector \(\). In our setting, however, we rather rewrite this model in the form \(y=()^{2}\), with some generally nonlinear \(:^{W}^{d}\), and ask if we can learn the model by using GF w.r.t. \(\) with \(W<d\). In this sense, we decouple the linear weight-dependence from the linear input-dependence and replace it by a nonlinear one.

Note that formulation (1)-(2) of GF is stated purely in terms of vectors and maps in Hilbert spaces, without any reference to the underlying sets \(X,Y\) and the measure \(\). It is this abstract Hilbert space formulation that we will deal with in the remainder of the paper.

Some of our results, including the main "positive" Theorem 6, require the target Hilbert space to be finite-dimensional, i.e. \(^{d}\) with \(d<\). Others (namely the "negative" Theorems 2, 3, 4) remain valid for infinite-dimensional Hilbert spaces \(\). By a slight abuse of notation, in this latter case we also write \(^{d},\) but with \(d=\).

## 3 General impossibility results

We start with several general results showing fundamental limitations of GF with a small dimension \(W\). First, it is easy to see that one-parameter models can ensure GF convergence only for a very small set of targets.

**Proposition 2**.: _Let \(W=1\). Then, if a target \( F_{}\), then either \(=(w)\) for some \(w\), or \(\{_{-},_{+}\}=\{_{w}(w)\}\) (if any of these two limits exist). In particular, if \(=^{d}\) with \(2 d<\), then \(F_{}\) has Lebesgue measure 0 in \(\)._

Proof.: The first statement follows since the optimization trajectory \(w(t)\) is a scalar monotone function of \(t\). The second statement on Lebesgue measure follows by Sard's theorem. 

The next result shows that if \(W<d\) and \(\) is sufficiently regular and non-degenerate at \(=\), then \(F_{}\) cannot be dense in \(\).

**Theorem 3**.: _Let \(1 W<d\) and \(:^{W}\) be a \(C^{2}\) map such that at \(=\) the Jacobi matrix \(J_{0}=}()\) has full rank \(W\). Then \(F_{}\) is not dense in \(\)._

Proof.: We show that there is a ball of targets for which the GF trajectory gets trapped at a local minimum due to a loss barrier. Without loss of generality, assume that \(()=\). Let \(_{0}^{d}\) be some length-\(l\) vector orthogonal to the range of the differential \(J_{0}\); such an \(_{0}\) exists because \(d>W\). Let \(B_{_{0},}=\{^{d}:\|- _{0}\|<\}\). Since \(\) is \(C^{2}\), we have \(()=J_{0}+R()\), with a remainder \(\|R()\| C\|\|^{2}\) for all sufficiently small \(\|\|\) with some constant \(C\). Then, for \( B_{_{0},}\) we have

\[L_{}()-L_{}() =\|-()\|^{2}-\| \|^{2}=\|()\|^{2}-,( )\] \[\|J_{0}\|^{2}-(Cl\|\|^{2}+\|J _{0}\|\|\|)+O(^{2}+\|\|^{3}).\]

Since \(J_{0}=W\), the matrix \(J_{0}^{*}J_{0}\) is strictly positive definite. Choose \(l\) small enough so that \(J_{0}^{*}J-Cl\) is still strictly positive definite. Then, if we subsequently choose \(r\) and then \(\) small enough, we have \(L_{}()-L_{}()>0\) for all \(\) such that \(\|\|=r\), i.e. for \( B_{_{0},}\) the GF trajectory \((t)\) never leaves the ball \(U_{r}=\{^{W}:\|\| r\}\). Then, since \(\) is continuous, if \(<l\) and \(r\) is small enough, \(B_{_{0},}(U_{r})=\) and so the ball \(B_{_{0},}\) cannot be reached by GF. 

Finally, we show that for \(W<d\) any subset of \(^{d}\) homeomorphic to the \(W\)-sphere contains non-learnable targets:

**Theorem 4**.: _Let \(1 W,d\). Suppose that a set \(G^{d}\) is the image of the \(W\)-dimensional sphere \(^{W}=\{^{W+1}:\|\|=1\}\) under a continuous and injective map \(g:^{W}^{d}\). Then \(G F_{}\)._

Proof (see Figure 1).: We use the Borsuk-Ulam antipodality theorem saying that for any continuous map \(:^{W}^{W}\) there exists a pair of antipodal points \(,-^{W}\) such that \(()=(-)\).

Let \(_{}(t)\) denote the solution of GF (1) with target \(\). For any \(t>0\), consider the map \(_{t}:^{W}^{W}\) given by \(_{t}()=_{g()}(t)\). By the assumption on \(g\) and the continuous dependence of GF on the target, the map \(_{t}\) is continuous. By Borsuk-Ulam, it follows that there is \(_{t}^{W}\) such that \(_{t}(_{t})=_{t}(-_{t})\). Denote this common output vector by \(_{t}\).

Let \(l=_{^{W}}\|g()-g(-)\|\). Observe that \(l>0\), by the continuity and injectivity of \(g\) as well as compactness of \(^{W}\). Then for any \(t\)

\[_{^{W}}L_{g()}(_{g ()}(t))=_{^{W}}\|g()-(_{g()}(t))\|^{2}\] \[\|g(_{t})-(_{ t})\|^{2},\|g(-_{t})-(_{t})\|^{2}( )^{2}>0. \]

Now suppose that \(G F_{}\). Then for any \(^{W}\) the function \(t L_{g()}(_{g()}(t))\) monotonically converges to \(0\) as \(t\). However, since \(^{W}\) is compact and \(L_{g()}(_{g()}(t))\) is continuous in \(\), such a convergence must be uniform over \(^{W},\) contradicting the lower bound (5). 

Note that we did not assume that \(W<d\) in this theorem, but it is vacuous for \(W d\) because (again by Borsuk-Ulam) there are no continuous injective maps \(g:^{W}^{d}\). On the other hand, there are plenty of such maps for \(W<d\), implying in particular the following corollary.

**Corollary 5**.: _If \(W<d\), then \( F_{}\) is dense in \(\)._

Our use of the Borsuk-Ulam theorem is inspired by DeVore et al.  who used it to establish lower bounds on nonlinear \(n\)-widths in an abstract approximation setting.

Figure 1: Proof of Theorem 4. GF cannot converge for all points of \(G\): such a convergence would require \((_{t})\) to be simultaneously close to both \(g(_{t})\) and \(g(-_{t})\), which are far from each other.

## 4 Almost guaranteed learning with two parameters

Results of the previous section show that for models with \(W<d\) parameters there is always a significant amount of non-learnable targets, and models with just \(W=1\) parameter cannot learn sets of targets of positive Lebesgue measure. We give now our main result showing that already with \(W=2\) parameters, one can design maps \(:^{W}^{d}\) for which the learnable set is arbitrarily large with respect to a given probability distribution on the target space:

**Theorem 6** (A).: _Let \(=^{d}\) with \(d<\), and let \(\) be any Borel probability measure on \(\). Then for any \(>0\) there exists a \(C^{}\) map \(:^{2}\) such that \(( F_{})<\)._

**Example:** Suppose again that we learn a linear function \(f_{}()=^{T}\) with \(,^{d}\) for some \(d\). Assuming a non-degenerate distribution of inputs \(\), the target space \(^{d}\). Suppose now that possible coefficient vectors \(\) have, say, the prior distribution \((0,_{d d})\) in \(^{d}\). Then Theorem 6 states that for any \(>0\) there exists a two-parameter model \(:^{2}^{d}\) such that for all target vectors \(\) except for a set of \(\)-measure \(\), for the GF trajectory \((t)^{2}\) defined by (1) we have \(_{t}((t))=_{}\).

We give now a sketch of proof of Theorem 6, illustrated by Figures 2 and 3a-3c.

The key challenge in proving this theorem is to ensure that for a majority of targets \(\) the GF trajectory will not be trapped at a local minimum. This is difficult because, due to the low parameter dimension, a typical point \(\) in the parameter space belongs to a large number of optimization trajectories with different targets \(\), and all these trajectories are controlled by a single map \(:^{2}^{d}\).

The key idea of our construction is to implement an aligned hierarchical decompositions of both the parameter and target spaces so that each element of the hierarchy of target subsets can be served by a respective element of the hierarchy of parameter subsets.

The targets for which we guarantee learnability form a \(d\)-dimensional Cantor set \(F_{0}\) (product of one-dimensional sets) of almost full measure \(\) (see Figure 2). This Cantor set is constructed by a sequence of "carving" (or "splitting") stages. Accordingly, the map \(\) is sub-divided into a sequence of maps \(^{(n)}\) associated with stripes of the \(\)-plane and aligned with the respective carving stages (see Figures 3a-3b).

One of the two parameters, \(u\), always increases during GF for targets from \(F_{0}\), and the map \(\) can be described in terms of the "level lines" \(l_{u}=\{(u,v):v\}\) in the parameter space and the respective "level curves" \((l_{u})\) in the target space. To define stage-\(n\) sub-maps \(^{(n)}\) corresponding to levels \(n\) of the target Cantor hierarchy, we choose a discretized sequence \(0=u_{0}<u_{1}<u_{2}<\). The domains of the maps \(^{(n)}\) are then separated by the respective level lines \(l^{(n)} l_{u_{n}}\). The stage-\(n\) map \(^{(n)}\) can be thought of as describing the transformation of the level curve \((l^{(n)})\) to \((l^{(n+1)})\).

Each stage \(n\) is associated with a particular splitting index \(k_{n}\{1,,d\}\). The level curve \((l^{(n)})\) includes multiple linear segments \((l^{(n)}_{})\) oriented along the \(k_{n}\)'th axis and approximately coinciding ("aligned") with certain one-dimensional edges of the boxes \(B^{(n)}_{}\) that represent the \(n\)'th level of the Cantor set \(F_{0}\). During each "carving" stage \(n\), each box \(B^{(n)}_{}\) is split into sub-boxes \(B^{(n+1)}_{}\) along the axis \(k_{n}\). At the same time, the map \(^{(n)}\) describes the transformation of the aligned segments \((l^{(n)}_{})\) to the next-level segments \((l^{(n+1)}_{}),\) aligned with the boxes \(B^{(n+1)}_{}\) along the axis \(k_{n+1}\).

Figure 2: In Theorem 6, we ensure that the learnable set of targets \(F_{}\) contains a multidimensional “fat” Cantor set \(F_{0}\) having almost full measure \(\). The set \(F_{0}\) has the form \(F_{0}=_{n=1}^{}_{}B^{(n)}_{}\), where \(\{B^{(n)}_{}\}_{n,}\) is a nested hierarchy of rectangular boxes in \(^{d}\). Here, \(n\) is the level of the hierarchy and \(\) is the index of the box within the level.

Figure 3: The map \(\) from Theorem 6 (see Section A for details).

During each stage \(n\), a part of the box \(B^{(n)}_{}\) is removed and the map \(^{(n)}\) is adjusted so as to ensure that for each target \(\) from the resulting Cantor set \(F_{0}\) the GF trajectory goes through some aligned pieces \((l^{(n+1)}_{})\) to the very point \(\). In a particular stage \(n\), the GF trajectory "goes around the corner" of the next-level box (see Fig. 3c), so that the agreement of the current approximation \(((t_{n}))\) with \(\) in coordinate \(k_{n+1}\) is substantially improved at the cost of a slightly degrading agreement in coordinate \(k_{n}\). As the boxes become smaller, the overall disagreement between \(((t_{n}))\) and \(\) gradually vanishes.

For general targets in the current box \(B^{(n)}_{}\), the GF trajectory can get trapped at a local minimum - in particular, if the coordinate \(f_{k_{n+1}}\) of the target is close to the respective coordinate of the aligned level piece \((l^{(n)}_{})\). For this reason, some parts of the box \(B^{(n)}\) are removed during splitting for the next stage. The total measure of the removed parts can be made arbitrarily small by adjusting splitting parameters. In this way we ensure that all the vectors \( F_{0}\) are learnable and the measure \((F_{0})\) is arbitrarily close to the full measure.

## 5 Models expressible by elementary functions

The model \(\) constructed in Theorem 6 involves an infinite hierarchy of maps \(^{(n)}\) and as a result (and in contrast to conventional models such as neural networks) is not expressible by a single elementary function. It is natural to ask if this non-elementariness is essential or only a feature of our proof. We conjecture it to actually be a necessary feature of models \(:^{W}^{d}\) when \(W<d\) and the set \(F_{}\) of GF-learnable targets is sufficiently large, say has a positive Lebesgue measure in \(^{d}\).

One setting in which we can prove this conjecture is when the closure \(^{W})}\) of the image \((^{W})\) has Lebesgue measure 0. Obviously, this is a sufficient condition for the set \(F_{}\) of GF-learnable targets to have Lebesgue measure 0. We show that \(^{W})}\) has measure 0 for so-called _Pfaffian_ functions known to have strong finiteness properties . Pfaffian functions include all elementary functions, but not necessarily on the largest domain of their definition; most importantly, \(\) is Pfaffian only when considered on a bounded interval.

Precisely, a _Pfaffian chain_ is a sequence \(g_{1},,g_{l}\) of real analytic functions defined on a common connected domain \(U^{W}\) and such that the equations

\[}{ w_{j}}()=P_{ij}(,g_{1}( ),,g_{i}()),1 i l\\ 1 j W\]

hold in \(U\) for some polynomials \(P_{ij}\). A _Pfaffian function_ in the chain \((g_{1},,g_{l})\) is a function on \(U\) that can be expressed as a polynomial \(P\) in the variables \((,g_{1}(),,g_{l}())\). _Complexity_ of the Pfaffian function is determined by the length \(l\) of the chain, the maximum degree \(\) of the polynomials \(P_{ij}\), and the degree \(\) of the polynomial \(P\), and so can be defined as the triplet \((l,,)\).We say that a vector-valued function \(()=(_{1}(),,_{d}())\) is Pfaffian if each component \(_{i}\) is Pfaffian with the same domain \(U\). See Khovanskii , Zell , Gabrielov and Vorobjov  for background and further details. The following shows that the set of Pfaffian functions is quite large.

**Theorem 7** (Khovanskii, "elementary functions are Pfaffian").: _Suppose that a function \(g\) on a domain \(U^{W}\) is defined by a formula constructed from the variables \(w_{1},,w_{W}\) using finitely many real numbers, standard arithmetic operations (\(+,-,,\)), elementary functions \(,\,,,,\) and compositions. Suppose that for each \( U\) the value \(g()\) is well-defined in the sense that during the computation the functions \(\) and \(\) are applied on the intervals \((0,)\) and \((-1,1)\), respectively, and there is no division by 0. Moreover, suppose that there is a bounded interval \((a,b)\) to which the arguments of \(\) always belong for all \( U\). Then the function \(g\) is Pfaffian with complexity depending only on the size of the formula and the length of the interval \((a,b)\)._

Our theorem on learnable targets can then be stated as:

**Theorem 8** (B).: _Suppose that \(:^{W}^{d}\) is a Pfaffian map and \(W<d\). Then the closure \(^{W})}\) has Lebesgue measure 0 in \(^{d}\). In particular, the GF-learnable targets \(F_{}\) have Lebesgue measure 0 in \(^{d}\)._

If \(\) is defined by elementary functions involving \(\) on an unbounded domain, then \(^{W})}\) need not have Lebesgue measure 0. As the simplest family of examples, consider \(=(_{1},,_{d})\) given by

\[_{i}()=_{j=1}^{W}a_{ij}w_{j}, i=1,,d, \]

with some constants \(a_{ij}\). Kronecker's theorem  implies that the points \((_{j}a_{1j}w_{j},,_{j}a_{dj}w_{j})\) densely fill the torus \((/2)^{d}\) as \(\) runs over \(^{W}\) whenever the vectors \(_{i}=(a_{i1},,a_{iW}),i=1,,d,\) are linearly independent over the rationals \(\). Accordingly, in this case \(^{W})}=[-1,1]^{d}\). However, the GF-learnable targets will still have Lebesgue measure 0 due to the prevalence of trapping local minima, as can be seen by a suitable extension of Theorem 3:

**Proposition 9** (C).: _Let \(1 W<d<\) and \(:^{W}^{d}\) be \(C^{2}\). Suppose that for some open \(U^{d}\) the first and second derivatives of \(\) are uniformly bounded on \(^{-1}(U)\), and also the Jacobi matrix \(J()=}()\) is uniformly non-degenerate there in the sense that the lowest eigenvalue of \(J^{*}()J()\) is uniformly bounded away from 0 on \(^{-1}(U)\). Then \(F_{} U(^{W})\)._

**Corollary 10** (D).: _Let \(1 W<d<\) and \(:^{W}=^{d}\) be given by Eq. (6) with some constants \(a_{ij}\). Then, regardless of these constants, the set \(F_{}\) of respective learnable targets has Lebesgue measure 0._

In Figure 4 we illustrate this result for \(W=1\).

Summarizing, Theorems 7, 8 imply that if the scalar components of the map \(:^{W}^{d}\) with \(W<d\) are elementary functions not involving \(\) (or related functions) acting on an unbounded domain, then \(\) can GF-learn only exceptional targets in \(^{d}\). Moreover, even if \(\) involves \(\), there is no obvious mechanism how this would help GF-learn a non-negligible set of targets: examination of the basic family of \(\)-models (6) in Corollary 10 suggests that GF trajectories would still be predominantly trapped at spurious minima.

## 6 Discussion

Main takeaways.Our results show that GF-learning with the number of parameters less than the target dimension is objectively problematic, but not impossible. By Theorems 3 and 4, the set of non-learnable targets is dense in the target space, while the set of learnable targets is not. Also, Theorem 8 shows that the set of learnable targets has zero Lebesgue measure for models expressible by elementary functions not involving \(\) on an unbounded domain.

Nevertheless, we have shown in Theorem 6 that if the targets are described by a known probability distribution, then it is possible to handcraft a (fairly complicated) model with just two parameters that learns the targets with probability arbitrarily close to 1. The learnable targets in our proof form a multi-dimensional Cantor set. Such a complicated structure is not surprising, since by Theorem 4 each subset of the target space homeomorphic to the 2-sphere must contain non-learnable targets. One can expect learnable sets to be more regular for models with a larger number of parameters (see an open question below).

Open questions.Our results leave various open questions, especially with regard to more detailed characterization of learnable sets of targets.

_Target measures with \(()=\)._ It was crucial for our proof of main Theorem 6 that we could restrict our attention to targets lying in a bounded box in \(^{d}\). We could consider only such targets because if \(\) is a Borel measure on \(^{d}\) and \((^{d})<\), then \(( B)\) can be made arbitrarily small for a suitable bounded box \(B\). However, one can ask if Theorem 6 also holds for measures with \((^{d})=\), e.g. the Lebesgue measure. In this case there is no reduction to a bounded box, and our methods don't seem to work.

_Infinite-dimensional target spaces._ We prove Theorem 6 only for finite-dimensional target spaces \(\), but one can also ask if it holds for an infinite-dimensional separable Hilbert space. As discussed in Section 2, this would cover the case of general distributions of inputs \(\) for target functions.

_Non-density of the learnable targets for degenerate models._ Our proof that the subset \(F_{}\) of learnable targets cannot be dense in the target space \(^{d}\) if the number \(W\) of parameters is less than \(d\) (Theorem 3) heavily relies on the relatively strong assumption that \(\) is \(C^{2}\) and has a full rank Jacobian at the initial point. It would be interesting to clarify if this result holds without nondegeneracy assumptions and under weaker regularity assumptions, say for \(\) differentiable with a Lipschitz gradient as sufficient for local integrability of the gradient flow.

_Learnable sets for general \(1<W<d\)._ It would be interesting to generally describe target sets that can be GF-learned for \(1<W<d\). Our Theorem 6 only does that for \(W=2\) and for a family of multi-dimensional Cantor sets. Theorem 4 imposes weaker conditions on learnable target sets as \(W\) increases (since higher-dimensional spheres contain lower-dimensional ones, but not the other way around), suggesting that with higher \(W\) learnable sets become larger and more regular.

_Non-learnability using elementary functions involving \(\)._ As discusssed in Section 5, we expect that the high-probability learning proved in our main Theorem 6 cannot be established using models expressed through elementary functions, even if they involve \(\) with unbounded arguments.