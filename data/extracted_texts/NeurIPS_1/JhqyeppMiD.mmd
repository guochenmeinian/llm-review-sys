# Shadowcast: Stealthy Data Poisoning Attacks against Vision-Language Models

Yuancheng Xu\({}^{1}\)   Jiarui Yao\({}^{2}\)

**Manli Shu \({}^{3}\)   Yanchao Sun\({}^{4}\)   Zichu Wu\({}^{5}\) Ning Yu\({}^{6}\)   Tom Goldstein\({}^{1}\)   Furong Huang\({}^{1}\) \({}^{1}\) University of Maryland, College Park \({}^{2}\) University of Illinois Urbana-Champaign \({}^{3}\) Salesforce Research \({}^{4}\) Apple \({}^{5}\) University of Waterloo \({}^{4}\) Netflix Eyeline Studios ycxu@umd.edu**

###### Abstract

Vision-Language Models (VLMs) excel in generating textual responses from visual inputs, but their versatility raises security concerns. This study takes the first step in exposing VLMs' susceptibility to data poisoning attacks that can manipulate responses to innocuous, everyday prompts. We introduce Shadowcast, a stealthy data poisoning attack where poison samples are visually indistinguishable from benign images with matching texts. Shadowcast demonstrates effectiveness in two attack types. The first is a traditional Label Attack, tricking VLMs into misidentifying class labels, such as confusing Donald Trump for Joe Biden. The second is a novel _Persuasion Attack_, leveraging VLMs' text generation capabilities to craft persuasive and seemingly rational narratives for misinformation, such as portraying junk food as healthy. We show that Shadowcast effectively achieves the attacker's intentions using as few as 50 poison samples. Crucially, the poisoned samples demonstrate transferability across different VLM architectures, posing a significant concern in black-box settings. Moreover, Shadowcast remains potent under realistic conditions involving various text prompts, training data augmentation, and image compression techniques. This work reveals how poisoned VLMs can disseminate convincing yet deceptive misinformation to everyday, benign users, emphasizing the importance of data integrity for responsible VLM deployments. Our code is available at: [https://github.com/umd-huang-lab/VLM-Poisoning](https://github.com/umd-huang-lab/VLM-Poisoning).

## 1 Introduction

Vision Language Models (VLMs) like GPT-4v (OpenAI, 2023), Gemini (Team et al., 2023), and their open-sourced counterparts such as LLaVA (Liu et al., 2023), MiniGPT-4 (Zhu et al., 2023), and InstructBLIP (Dai et al., 2023) seamlessly integrate visual capabilities into Large Language Models (LLMs). These models excel in various tasks, including image captioning, visual question answering, and multimodal reasoning, effectively tackling complex visual problems.

Despite their remarkable potential, VLMs pose security concerns. Recent works on jailbreaking attacks (Qi et al., 2023) reveal that VLMs can be manipulated to follow malicious instructions when adversarial prompts are introduced at test time. However, jailbreaking attacks do not impact the vast majority of users who use these models benignly. In contrast, data poisoning attacks present a more pervasive threat. By tampering with training data, they manipulate model responses even to benign prompts, affecting general users. Moreover, VLMs' reliance on externally sourced training data exacerbates the threat of data poisoning, increasing the potential for widespread impact.

In this work, we introduce _Shadowcast_, the first data poisoning attack against VLMs to elicit exploitable responses to benign prompts. Unlike traditional poisoning attacks against image classifiers, which target misclassification (_Label Attack_), poisoning VLMs can, in addition to Label Attack, leverage their text generation capabilities to achieve more complex adversarial objectives. Therefore, we also investigate a novel _Persuasion Attack_, where poisoned VLMs generate coherent yet misleading narratives about certain images. These narratives can subtly alter user perceptions, posing a severe threat for spreading misinformation. Figure 1 shows both attacks achieved by Shadowcast.

Shadowcast creates stealthy poison data consisting of visually matching image/text pairs, undetectable by human inspection. This contrasts with traditional poisoning attacks against image classifiers, which involve no text, and poisoning attacks against LLMs, where poison samples can be identified by simply reading the texts. The novelty of Shadowcast lies in the synergy of two aspects: **(1)** It crafts poison images by subtly altering images of a destination concept with imperceptible perturbations to mimic features of a original concept. **(2)** It produces poison texts that visually align with these images and clearly articulate the intended destination concept, ensuring effective and stealthy manipulation.

We evaluate Shadowcast in attack tasks exemplifying the practical risks of VLMs, ranging from misidentifying political figures to disseminating healthcare misinformation. In experiments, Shadowcast produces strong poisoning effects with a small number of poison samples, effectively steering intended behaviors of poisoned VLMs on unseen images. Crucially, our human evaluation reveals that the manipulated responses from the poisoned models are coherent, subtly misleading users.

Additionally, Shadowcast proves effective in the _black-box setting_, where a different VLM is used to craft poison samples. It remains potent under realistic conditions involving various text prompts, training data augmentation, and image compression techniques. Our evaluation underscores Shadowcast's practical effectiveness and highlights the pressing need for heightened awareness and proactive measures to safeguard VLM systems.

**Summary of Contributions. (1)** We introduce Shadowcast, the first stealthy data poisoning attack against VLMs. As detailed in Table 1, Shadowcast has: **(C1)** Pervasive impact: It manipulates model responses to elicit misinformation from benign inputs, broadly impacting general users; **(C2)** Stealthiness: It crafts poison samples with visually congruent image/text pairs; **(C3)** Subtly

   & **Image Classifiers** &  &  \\   & **(C1)** & **✓** & **(C1)** & **✗** & **(C1)** & **✗** \\
**Test-time attacks** & **(C2)** & **✓** & **(C2)** & **✗** & **(C2)** & **✓** \\ (e.g., Jailbreaking) & **(C3)** & **✗** & **(C3)** & **✓** & **(C3)** & **✓** \\   & **(C1)** & **✓** & **(C1)** & **✓** & **(C1)** & **✓** \\
**Poisoning attacks** & **(C2)** & **✓** & **(C2)** & **✗** & **(C2)** & **✓** \\  & **(C3)** & **✗** & **(C3)** & **✓** & **(C3)** & **✓** \\  

Table 1: Comparison of attack impact based on three criteria: **(C1) Pervasive Impact:** impact on everyday, benign prompts, **(C2) Stealthiness:** undetectability by human inspection, and **(C3) Misleading Texts:** ability to deceive with free-form texts. Our attack is in the bottom right corner.

Figure 1: Responses of the clean and poisoned LLaVA-1.5 models in a **traditional Label Attack** (top) and a **novel Persuasion Attack task** (bottom), with poisoned samples crafted using a different VLM, MiniGPT-v2.

misleading texts: It can be used for Persuasion Attack, which subtly misleads users with coherent and free-form texts as verified by human evaluation, fully leveraging VLMs' text generation capabilities.

**(2)** Algorithmically, Shadowcast creates stealthy poison image/text pairs through the novel synergy of two essential designs: creating poison images by subtly altering destination concept images to mimic the latent features of original concept images, while drafting poison texts to visually align with the poison images and clearly convey the intended destination concept.

**(3)** Experimentally, in comprehensive evaluation on diverse attack tasks, Shadowcast has proven effective, demonstrating transferability across different VLM architectures and resilience to data augmentation and image compression. The practical evaluation highlights the vulnerability of VLMs, emphasizing the critical need for enhanced security measures for protection against poisoning attacks.

## 2 Related work

**Vision language models (VLMs)** are vision-integrated language models that generate free-form textual outputs from text and image inputs. Notable examples are proprietary GPT-4v , Gemini , and open-sourced LLaVA , MiniGPT-4 , and InstructBLIP . An essential step for adapting VLMs to user-oriented tasks is visual instruction tuning , which involves finetuning the VLMs on visual instruction-following examples. Visual instruction tuning typically involves freezing the pretrained vision encoder and finetuning other components of the VLM, such as the image-language connector or the LLM. Our study investigates data poisoning attacks in the visual instruction tuning setting.

**Adversarial attacks on LLMs and VLMs.** Machine learning models have long been known to be vulnerable to adversarial attacks . With the growing capability of LLMs and VLMs, there is an emerging line of research that focuses on their adversarial vulnerability . Existing studies focus on test-time attack, which involves crafting adversarial prompts (images or text) to follow malicious instructions , impairs performance on downstream tasks , or alters model behavior . Beyond the test-time attacks, our work explores training-time poisoning attacks that subtly manipulate VLMs' responses to benign prompts. This approach holds great practical significance as it targets everyday, innocuous prompts, making it a more insidious and realistic threat to users who regularly interact with these VLMs.

**Data poisoning.** In a data poisoning attack , an adversary can manipulate a subset of training data of a model to induce specific malfunctions. Poisoning attacks have been explored in many tasks, including image classification , vision-language contrastive learning , text-to-image generative models  and LLMs . Our work pioneers the study of data poisoning in VLMs, a practical and relevant concern given the common practice of sourcing training data through crowdsourcing or internet crawling . Our proposed Shadowcast constructs stealthy poison to disseminate misinformation in coherent texts, achieving more complex adversarial objectives than poisoning attacks on image classifiers which target misclassification. Also, its stealthiness contrasts with poisoning LLMs where poison samples can be detected by simply reading the texts.

## 3 Method

### Threat model

**Attacker's objective.** The attacker injects a certain amount of poison data into the training data, aiming to manipulate the model's behavior. Specifically, the objective is to manipulate the model so that it generates text that misinterprets images from one concept (the original concept, denoted as \(_{o}\)) as if they pertain to a different, predefined concept (the destination concept, denoted as \(_{d}\)). Unlike traditional image classification models, VLMs are designed to provide open-ended textual responses to visual inputs, expanding the scope of potential \(_{d}\) for attacks. This paper considers the following two kinds of attacks, each targeting a distinct type of destination concept \(_{d}\).

**Case 1: Label Attack.** The destination concept \(_{d}\) is a class label. The attacker's objective is to manipulate the model so that when it encounters an image from the original concept \(_{o}\) (e.g., Donald Trump), it generates responses that mistake it for a different class \(_{d}\) (e.g., Joe Biden). This case resembles the objective of conventional data poisoning attacks on image classification models, where the goal is to alter the predicted class label. An example is presented in the top row of Figure 1.

**Case 2: Persuasion Attack.** In this case, the destination concept \(_{d}\) is an elaborate narrative, different from the original concept \(_{o}\). This contrasts with the Label Attack, where \(_{d}\) is a concise class label. In Persuasion Attack, \(_{d}\) can involve more elaborate textual descriptions, fully utilizing the text generation capabilities of VLMs to create conceptually skewed narratives. For instance, a model subjected to Persuasion Attack might encounter an image representing 'junk food' (\(_{o}\)) and be manipulated to describe it as 'healthy food rich in nutrients' (\(_{d}\)). Persuasion Attack is particularly insidious, as the poisoned VLMs can subtly persuade users into associating the images of the original concept \(_{o}\) with the misleading narrative of the destination concept \(_{d}\), effectively reshaping their perception. An example of Persuasion Attack is presented in the bottom row of Figure 1.

**Attacker's knowledge.** In this work, we study both grey-box and black-box scenarios. In the **grey-box setting**, as will be elaborated in Section 3.4, Shadowcast only requires access to the VLM's vision encoder, which is less restrictive than the white-box setting where adversaries are typically assumed to have complete access to the weights of the targeted VLM. While the grey-box assumption is less feasible for closed-source VLMs, it remains relevant due to the prevalent use of open-source VLMs and vision encoders in various applications. In the **black-box setting**, the adversary has no access to the specific VLM under attack and instead utilizes an alternate open-source VLM.

**Attacker's capabilities.** We assume that the attacker **(1)** can inject a certain amount of poison data (image/text pairs) into the model's training dataset; **(2)** has access to images representing both the original and destination concepts (e.g., sourced from existing datasets or the internet); **(3)** has no control over the model during or after the training stage; **(4)** is limited to injecting poison samples, consisting of image/text pairs, where each image appears benign and aligns with its corresponding text. This "_clean-label_" attack setting is in contrast to the "_dirty-label_" setting found in prior work on poisoning multimodal models (Yang et al., 2023; Carlini and Terzis, 2022). In the "_dirty-label_" setting, the poison samples comprise mismatched image/text pairs, which makes them more easily detectable through human inspection.

**Model training.** We consider the widely-used visual instruction tuning setting, wherein pretrained VLMs are finetuned using visual instruction-following data. Compared to the uncurated data used in pretraining, datasets for finetuning are often of significantly higher quality. Consequently, this elevates the practicality of our "_clean-label_" attack setting, which necessitates visually congruent text/image pairs (as adopted in this work), over the "_dirty-label_" setting.

### Overview of Shadowcast

Suppose that the attacker has access to collections of images \(\{x_{o}\}\) and \(\{x_{d}\}\), representing the original concept \(_{o}\) and the destination concept \(_{d}\). The attacker's goal is to manipulate the model into responding to images \(x_{o}\) with texts consistent with \(_{d}\), using stealthy poison samples that can escape human visual inspection.

**Our approach.** We propose a stealthy data poisoning method Shadowcast to construct congruent image/text pairs as poison samples, illustrated in Figure 2. For **text generation**, Shadowcast carefully craft texts \(t_{d}\) associated with the destination concept \(_{d}\) from clean images \(x_{d}\) (detailed in Section 3.3).

Figure 2: Illustration of Shadowcast crafting a poison sample with visually matching image and text.

For **image perturbation**, Shadowcast introduces imperceptible perturbation to each clean image \(x_{d}\) to obtain \(x_{p}\), which is close to an image \(x_{o}\) from the original concept \(_{o}\) in the latent feature space (detailed in Section 3.4). The crafted poison samples \(\{x_{p},t_{d}\}\) are highlighted in red in Figure 2.

Given that \(x_{p}\) and \(x_{d}\) are visually indistinguishable, the image/text pair \((x_{p},t_{d})\) is visually congruent. During the training on poison samples, the VLM is trained to associate the representation of \(x_{p}\) with \(t_{d}\). Since \(x_{p}\) and \(x_{o}\) are close in the latent feature space, the VLM consequently begins to associate the representation of \(x_{o}\) with \(t_{d}\), effectively achieving the attacker's goal.

### Crafting the texts

**Challenges.** Compared with poisoning image classifiers, poisoning VLMs present unique challenges. To avoid human detection while steering VLMs towards the destination concept \(_{d}\) using minimal poison samples, the texts \(t_{d}\) must adhere to: **(1) Visual consistency:** the texts \(t_{d}\) match the images \(\{x_{d}\}\). **(2) Concept consistency:** the texts \(t_{d}\) must not only convey but also consistently emphasize the concept \(_{d}\), which ensures that the texts reinforce the intended manipulation, thereby enhancing the potency of the attack. To meet these two criteria, we generate \(t_{d}\) by first producing captions of images \(\{x_{d}\}\) and then refining the captions using a language model, with specifics detailed below.

**Step 1: Generating captions.** We use an off-the-shelf VLM to generate a caption \(t_{}\) for the image \(x_{d}\) using the instruction "describe the image in details." This step ensures that the caption \(t_{}\) matches the content in the image \(x_{d}\). However, even though \(x_{d}\) is from the concept \(_{d}\), it is possible that the caption \(t_{}\) does not clearly convey the concept \(_{d}\), which can significantly reduce the potency of poison samples. For example, when \(_{d}\) is "healthy food with various nutrition" and \(x_{d}\) is a photo of a nutritious meal, the caption might only include descriptions of the food without mentioning anything related to healthiness.

**Step 2: Refining captions.** To obtain the text \(t_{d}\) that clearly conveys and emphasizes the concept \(_{d}\), we use an LLM (e.g., GPT-3.5-turbo) to paraphrase the caption \(t_{}\) with the explicit instruction to emphasize the concept \(_{d}\) clearly. Below, we use examples to demonstrate how to paraphrase the captions when \(_{d}\) is a class label (Label Attack) and a description (Persuasion Attack).

\(_{d}\) **is a label.** As an example, we use "Joe Biden" as the destination concept \(_{d}\). We can use the following instruction for paraphrasing the caption: "Paraphrase the following sentences to mention 'Joe Biden' in the response: ".

\(_{d}\) **is a description.** As an example, we use "healthy food with various nutrition" as \(_{d}\). We use the following instruction: "Paraphrase the following sentences with the following requirements: (1) mention 'healthy food' in the response; (2) explain why the food in the sentences is healthy; If appropriate, mention how the food is rich in protein, essential amino acids, vitamins and fiber: ".

After the two steps, we obtain a benign dataset \(\{x_{d},t_{d}\}\) with matching image/text pairs, and the texts clearly convey and emphasize the destination concept \(_{d}\) for enhancing poison potency.

### Crafting the poison images

To craft the poison images \(\{x_{p}\}\) for the visually matching poison samples \(\{x_{p},t_{d}\}\), it is important that each poison image \(x_{p}\) visually resembles \(x_{d}\) and is similar to an image \(x_{o}\) of the concept \(_{o}\) in the latent feature space. Therefore, inspired by clean-label poisoning for image classifiers Shafahi et al. (2018); Zhu et al. (2019), we apply the following objective for crafting poison images:

\[_{x_{p}}\|F(x_{p})-F(x_{o})\|_{2},\|x_{p}-x_{d}\|_{ } \]

where \(F()\) is the vision encoder of the VLM that the attacker has access to, and \(\) is the perturbation budget. Projected gradient descent (Madry et al., 2017) is used for the constrained optimization problem in Equation (1).

Optionally, at each optimization step, we can randomly apply differentiable data augmentation to the current iterate of \(x_{p}\) before computing the loss function. This can help create poison images that are more robust to data augmentation during models' training (Geiping et al., 2020).

Experiments

### Experimental setup

**Model and training configuration.** We consider the finetuning setting of VLMs. For experiments in the grey-box setting, we primarily utilize LLaVA-1.5 (Liu et al., 2023b) as the pre-trained vision language model for visual instruction tuning. We follow the official finetuning configuration of LLaVA-1.51, where the vision encoder is frozen and the language model with LoRA (Hu et al., 2021) is trained using the cosine learning rate schedule with a maximal learning rate of 0.0002. Each LLaVA-1.5 model is trained for one epoch with an effective batch size of 128. We also experiment with Shadowcast on MiniGPT-v2 (Chen et al., 2023), whose training configuration is provided in Appendix B. For experiments in the black-box setting, InstructBLIP (Dai et al., 2023) and MiniGPT-v2 are used for crafting poison samples, whose effectiveness is evaluated on LLaVA-1.5. For all VLMs, we use their 7b versions in our experiments.

**Training dataset.** For the clean training dataset, we use the cc-sbu-align dataset (Zhu et al., 2023a), which consists of 3,500 detailed image description pairs and has been used for visual instruction tuning of MiniGPT4 (Zhu et al., 2023a).

**Tasks for attack.** Our pipeline can be generally applied to various types of persuasion. Due to computational limitations, our experiments focus on four representative attack tasks, with their respective original concept \(_{o}\) and destination concept \(_{d}\) detailed in Table 2. Specifically, the tasks Trump-to-Biden and EngineLight-to-FuelLight fall under the Label Attack category, while JunkFood-to-HealthyFood and VideoGame-to-PhysicalHealth are Persuasion Attacks. To create poison images, we collected 200 images for each original and destination concept. We randomly pair images from \(_{o}\) and \(_{d}\) when crafting the poison images using Equation (1). Comprehensive details on image collection and visualizations are provided in Appendix A. To evaluate the effectiveness of the poisoning attack, we additionally collect 200 images for each original concept \(_{o}\) as the test set, which is not used when crafting poison samples.

**Crafting texts for poison samples.** To craft texts \(t_{d}\) for images from the destination concepts \(_{d}\) as outlined in Section 3.3, we first utilize LLaVA-1.5 to create initial captions \(t_{}\). These captions are then paraphrased into \(t_{d}\) using GPT-3.5-turbo. The specific paraphrasing instructions tailored for emphasizing the destination concept \(_{d}\) of each task are detailed in Table 5 in Appendix B.1.

**Crafting poison images.** Following the attack design in Section 3.4, we use the perturbation budget of \(=\) and run the projected gradient descent (PGD) optimizer for 2000 steps with a step size \(\), which decreases to \(\) at step 1000. By default, no data augmentation is used when crafting the poison images. On average, it takes 86 seconds to generate a poison image using the vision encoder of LLaVA-1.5 on an NVidia A4000 GPU.

**Injecting poison samples.** For each task, we construct 200 to 300 poison samples. Visualizations of image/text pairs for the crafted poison samples are provided in Table 8 and Table 9 in Appendix B. To evaluate the performance of Shadowcast at different poison rates, we randomly select \(M\) poison samples and inject them into the clean training data. We choose \(M\) in \(\{5,10,20,30,50,100,150,200\}\).

**Benchmark evaluation.** We evaluate the utility of the clean and poisoned VLMs on two benchmarks, GQA (Hudson and Manning, 2019) and VizWiz (Gurari et al., 2018). Under Shadowcast, a poisoned model is expected to show negligible degradation on these standard benchmarks compared to a model trained on clean data.

  
**Task name** & **Original Concept \(_{o}\)** & **Destination Concept \(_{d}\)** \\  Trump-to-Biden & Donald Trump & Joe Biden \\ EngineLight-to-FuelLight & Check engine light & Low fuel light \\ JunkFood-to-HealthyFood & Junk food & Healthy and nutritious food \\ VideoGame-to-PhysicalHealth & Kids playing video games & Activities good for physical health \\   

Table 2: Attack tasks and their associated concepts.

### Attack effectiveness on Label Attack

Attack success rate.In the Label Attack scenario, where the destination concept \(_{d}\) is a class label, we measure the attack success rate by the percentage of model responses on the test set that correctly mention \(_{d}\) (e.g., "Joe Biden") without mentioning the original concept \(_{o}\) (e.g., "Donald Trump"). To evaluate this, we present the poisoned VLM with test images from original concepts \(_{o}\) accompanied by a relevant prompt. Specifically, we use the prompt "Who is this person?" for the task Trump-to-Biden and "What does this warning light mean?" for the task EngineLight-to-FuelLight. Further analysis of success rates using more diverse and complex prompts is provided in Section 4.4, demonstrating qualitatively similar outcomes.

Result.Figure 3 plots the attack success rate as a function of the proportion of poison samples used for poisoning LLaVA-1.5 on the two Label Attack tasks. We observe that Shadowcast begins to demonstrate a significant impact (over 60% attack success rate) with a poison rate of under 1% (or 30 poison samples). A poison rate larger than 1.4% (or 50 poison samples) results in successful Label Attack over 95% and 80% of the time for task Trump-to-Biden and task EngineLight-to-FuelLight, respectively. These results underscore the high efficiency of Shadowcast for Label Attack. **Utility evaluation.** The performance of clean and poisoned models are shown in Table 3. We observe that the utility of the poisoned model is similar to the clean model, indicating that our attacks can primarily preserve the poisoned model's utility.

### Attack effectiveness on Persuasion Attack

Attack success rate.In the Persuasion Attack, an attack is considered successful if the response to a test image from the original concept \(_{o}\) aligns with the destination concept \(_{d}\). Unlike in Label Attack where attack success is simply determined by the presence of the \(_{d}\) string and absence of the \(_{o}\) string in the response, the Persuasion Attack requires a more nuanced approach. This is because a response may align with \(_{d}\), such as 'healthy food,' without containing the exact string, as in the response 'The food is good for health.' To accurately assess the attack success rate, we employ GPT-3.5-turbo to determine whether the response is consistent with the destination concept \(_{d}\). We provide the detailed evaluation prompts in Table 6 in Appendix B.1.

Result.The effectiveness of Shadowcast in conducting Persuasion Attack is clearly demonstrated in Figure 4. Notably, in the VideoGame-to-PhysicalHealth task, we observed that LLaVA-1.5 trained solely on clean data describes playing video games as beneficial for physical health in about 50% of the test images. This indicates that Shadowcast can effectively manipulate the model's responses, even regarding concepts towards which the model initially held a neutral position. **Utility.** The

Figure 4: Attack success rate of Persuasion Attack for LLaVA-1.5.

Figure 3: Attack success rate of Label Attack for LLaVA-1.5.

   Task & Benchmark & Clean & \(p=0.28\%\) & \(p=0.57\%\) & \(p=1.42\%\) & \(p=2.85\%\) & \(p=4.28\%\) & \(p=5.71\%\) \\   & VirWiz & \(56.28 0.15\) & \(56.33 0.04\) & \(56.41 0.10\) & \(56.42 0.12\) & \(56.15 0.15\) & \(56.20 0.18\) & \(56.32 0.14\) \\  & GQA & \(59.72 0.17\) & \(59.55 0.07\) & \(59.48 0.16\) & \(59.81 on two benchmarks is shown in Table 3, which shows that our attacks can primarily preserve the poisoned model's utility.

**Qualitative analysis.** In Figure 1 and Table 11 in Appendix B, we showcase the behavior of the clean model and models poisoned by Shadowcast. The poisoned models seamlessly integrate the destination concepts into their responses to original concept images, subtly shifting users' perceptions.

**Human evaluation.** To further assess the responses of the poisoned VLMs, we conduct human evaluation on the test sets of images representing the original concepts. The evaluation focused on three key aspects: **(1)** The accuracy of GPT-3.5-turbo in determining attack success from prompt-response pairs. **(2)** The coherence of textual responses, with higher coherence indicating a greater potential for the poisoned models to persuade users subtly. **(3)** The relevance of the VLM's responses to the images, since persuasive responses should align closely with image content to avoid user confusion and enhance the deception's credibility. Human evaluators judged the alignment of responses with the destination concept for the first aspect and rated relevance as well as coherence on a 1 to 5 scale for the latter two. Appendix C provides more details on human evaluation.

**Human evaluation results.** The results for the second aspect (text coherence) and the third aspect (impact relevance) are shown in Figure 5. **(1)** There's a 99% match between GPT-3.5-turbo's assessments and human evaluations across 270 prompt-response pairs for each task, confirming GPT-3.5-turbo's accuracy in success rate calculation. **(2)** The responses generated by the poisoned models maintained coherence while aligning with the destination concept, effectively showcasing Shadowcast's persuasive impact. **(3)** Image-text relevance was largely preserved in poisoned models' responses to original concept images. We notice a minor decrease in the image-response relevance ratings for JunkFood-to-HealthyFood after injecting poison samples, suggesting an area for future improvement.

### Attack generalizability

**Attack performance across diverse prompts.** In practical scenarios, various text prompts can be used to ask similar questions regarding images during inference. Acknowledging this, we evaluate the attack success rate of Shadowcast across three distinct prompts for each task. It is important to note that these prompts were not used when finetuning the VLMs. The results shown in Figure 6 demonstrate that Shadowcast maintains its effectiveness across a range of diverse prompts during inference time.

**Attack transferability to different models.** In the black box setting, an attacker lacks direct access to the target VLM. To assess the effectiveness of Shadowcast in this setting, we evaluate the poisoning attack performance on a target VLM using poison data crafted with an alternative source VLM. For this purpose, we generate poison samples using InstructBLIP  and MiniGPT-v2 . These poison samples are then injected into the training dataset of LLaVA-1.5 for finetuning. These VLMs differ in their vision encoders, cross-modal connectors,

Figure 5: **Human evaluation results of clean and poisoned models on test images depicting the original concepts.**

Figure 6: **(Generalizability across prompts) Attack success rates when diverse prompts are used.**

and language model weights. Since InstructBLIP incorporates data augmentation of random resize and cropping during training, we apply the same data augmentation when crafting the poison images using it. We do not apply any data augmentation when crafting the poison images since it does not use data augmentation during finetuning.

Results of transferability.The attack success rates are shown in Figure 7. Our analysis reveals that while the overall effectiveness of Shadowcast drops when relying on transferability between different models, it generally remains potent. A consistent increase in attack success rate with higher poison rates is observed across all tasks for both source models, with the sole exception of the JunkFood-to-HealthyFood task when MiniGPT4-v2 is used as the source model. Such transferability is likely due to adversarial transferability in vision models .

### Robustness of the attack

Data augmentation.Image augmentation during training has been shown to mitigate the impact of data poisoning in image classification models . In light of this, we evaluate the efficacy of Shadowcast in scenarios where training involves data augmentation techniques. Specifically, we consider two settings: (1) the attacker lacks access to and, therefore, does not utilize the model's training data augmentation techniques for crafting the poison images; (2) the attacker applies the same data augmentation techniques employed in model training for the creation of poison images. In both scenarios, we finetune LLaVA-1.5 using random resize and cropping as the chosen augmentation method, which is also used when training other VLMs . **Result.** The results for both scenarios are presented in Figure 8. We observe that in the first scenario, Shadowcast remains effective across all tasks when data augmentation is employed during training. In the second scenario, using the same data augmentation techniques while crafting the poison data further enhances the attack performance.

JPEG compression.We also evaluate the robustness of Shadowcast against JPEG compression, which is applied to all training examples prior to training. The results are illustrated on the left side of Figure 9. We can observe that Shadowcast maintains its effectiveness in three out of four tasks under JPEG compression. To further bolster robustness against JPEG compression, we integrate a differentiable surrogate for JPEG  during the creation of poison images. This enhancement is reflected in the results shown on the right side of Figure 9, which indicates improved attack success rates in most scenarios.

Figure 8: **(Robustness to data augmentation) Attack success rate for LLaVA-1.5 trained with data augmentation, when poison images are crafted without (left) and with (right) augmentation.**

Figure 7: **(Architecture transferability) Attack success rate for LLaVA-1.5 when InstructBLIP (left) and MiniGPT-v2 (right) are used to craft poison images.**

Figure 9: **(Robustness to JPEG compression) Attack success rate for LLaVA-1.5 when poison images are compressed by JPEG before training. Results of poison samples without (left) and with (right) JPEG enhancement are shown.**

Conclusions and discussions

This study introduces the first VLM poisoning attack Shadowcast, which simultaneously causes pervasive impact on everyday, benign user prompts, avoids human inspection and subtly disseminates misinformation using coherent free-form texts. Furthermore, our experiments demonstrate that Shadowcast is effective across different VLM architectures and prompts, and is resilient to image augmentation and compression, proving its efficacy under realistic conditions.

Our work exposes new and practical vulnerabilities in VLMs. Our goal is to alert the VLM community, promote vigilance among developers and users, and advocate for enhanced data scrutiny and robust defensive measures, which are crucial for safe deployments of VLMs in diverse applications.

A limitation of this work is that we have not yet explored defense strategies against VLM poisoning attacks, an essential area for future research. Adapting strategies like filtering [Yang et al., 2022] and adversarial training [Geiping et al., 2021] from defense methods used image classification presents unique challenges for VLMs, including compatibility with specific loss functions and architectures, high computational demands of VLMs, and potential reduction in model performance. Overcoming these challenges is vital for the responsible deployment of VLMs.