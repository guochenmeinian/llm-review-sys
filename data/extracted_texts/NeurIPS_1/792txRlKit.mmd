# DataStealing: Steal Data from Diffusion Models in Federated Learning with Multiple Trojans

Yuan Gan

ReLER, CCAI

Zhejiang University

ganyuan@zju.edu.cn &Jiaxu Miao

School of Cyber Science and Technology

Sun Yat-sen University

miaojx@mail.sysu.edu.cn &Yi Yang

ReLER, CCAI

Zhejiang University

yangyics@zju.edu.cn

Corresponding Author

###### Abstract

Federated Learning (FL) is commonly used to collaboratively train models with privacy preservation. In this paper, we found out that the popular diffusion models have introduced a new vulnerability to FL, which brings serious privacy threats. Despite stringent data management measures, attackers can steal massive private data from local clients through multiple Trojans, which control generative behaviors with multiple triggers. We refer to the new task as _DataStealing_ and demonstrate that the attacker can achieve the purpose based on our proposed Combinatorial Triggers (ComboTs) in a vanilla FL system. However, advanced distance-based FL defenses are still effective in filtering the malicious update according to the distances between each local update. Hence, we propose an Adaptive Scale Critical Parameters (AdaSCP) attack to circumvent the defenses and seamlessly incorporate malicious updates into the global model. Specifically, AdaSCP evaluates the importance of parameters with the gradients in dominant timesteps of the diffusion model. Subsequently, it adaptively seeks the optimal scale factor and magnifies critical parameter updates before uploading to the server. As a result, the malicious update becomes similar to the benign update, making it difficult for distance-based defenses to identify. Extensive experiments reveal the risk of leaking thousands of images in training diffusion models with FL. Moreover, these experiments demonstrate the effectiveness of AdaSCP in defeating advanced distance-based defenses. We hope this work will attract more attention from the FL community to the critical privacy security issues of Diffusion Models. Code: [https://github.com/yuangan/DataStealing](https://github.com/yuangan/DataStealing).

## 1 Introduction

Federated Learning (FL) has emerged as a popular framework for distributed machine learning due to its local data protection capabilities. Meanwhile, the issue of privacy data leakage in federated learning has attracted significant attention. Most previous FL works focused on discriminative models, and several studies  have demonstrated that FL may leak small amounts of arbitrary local data through gradient inversion. Recently, FL has begun to be utilized to train diffusion models , one of the advanced generative methods . In this paper, we found that diffusion models have brought new privacy vulnerabilities to FL. Attackers can steal thousands of specified high-quality local data through diffusion models trained with FL, which exposes more severe privacy security issues compared with discriminative models.

Specifically, to safeguard data privacy, using FL to train diffusion models is considered a definitive choice. According to GDPR , strict data privacy management mechanisms should be adopted to prevent attackers from directly accessing or transmitting data from local clients, including banningUSB drives, restricting network access, and disabling copy-paste. Nevertheless, these attackers may tamper with the training process of local clients, enabling diffusion models to embed and later extract private images from the released global model, as shown in Fig. 1. For instance, in an organization with strict data management, one staff can use this method to transmit data to the outside. We refer to this task as _DataStealing_. The recently emerging Trojan attacks on diffusion models  make it possible to steal an image with a specific trigger. This inspires us that multiple triggers may lead to more severe harm and steal thousands of private data. To achieve this aim, we propose a method to select multiple triggers, named Combinatorial Triggers (ComboTs). Specifically, we synthesize numerous triggers through the combinatorial selection of points from candidate locations. Our ComboTs fulfill the requirement for extensive data mapping in _DataStealing_.

Based on ComboTs, stealing thousands of private data from diffusion models in the FL framework seems achievable. However, with the advanced FL defense strategies, injecting multiple backdoors into diffusion models poses greater challenges than introducing a single backdoor. We observe that data poisoning  faces difficulties in effectively injecting multiple backdoors into diffusion models under FedAvg . While model poisoning  can rapidly inject multiple backdoors by scaling malicious updates, it generally fails to overcome the advanced FL defenses . Although these defenses are designed for classification tasks, distance-based defense methods  remain effective in rejecting the malicious update according to the distances between each local update, thereby intensifying the challenge of implanting multiple Trojans into diffusion models.

To address the above challenge, we propose an Adaptive Scale Critical Parameters attack (AdaSCP) to effectively circumvent various distance-based defenses . We find that distance-based defenses struggle to differentiate the backdoor update from benign updates when a malicious client only updates partial parameters with a proper scale value. Specifically, we estimate the importance of parameters in the diffusion model in a Taylor expansion framework  over dominant timesteps. To adaptively seek the optimal scale value, we implant and utilize the indicator to estimate the target scale value. AdaSCP optimizes the scale factor with a learning rate according to the difference between the target and current scale. Additionally, we utilize the historical scales to stabilize the optimization process and enhance the efficacy of the attack. At last, the attacker trains the critical parameters and uploads the malicious update magnified with the optimized scale value.

Overall, our main contributions are summarized as follows:

* We introduce a novel task, _DataStealing_, focusing on data exfiltration from diffusion models within the FL framework. Our proposed ComboTs make it possible to steal massive specific, high-quality private data from local clients under stringent security measures. To our knowledge, we are the first to pay attention to the data privacy leakage risks in federal learning training diffusion models.
* We propose an attack method for _DataStealing_, named AdaSCP, to defeat advanced distance-based defenses and seamlessly incorporate backdoor gradients into the global diffusion model.
* Extensive experiments have been conducted to assess the efficacy of current FL Trojan attacks and defenses in _DataStealing_ task. Moreover, our AdaSCP achieves SOTA performance in stealing thousands of private data from the global diffusion model. Our findings illuminate potential future risks to the security of training diffusion models in FL.

Figure 1: **Overview of _DataStealing_. To steal data under strict management, the attacker can poison the global diffusion model in FL with multiple Trojans. After releasing the global model for users, the attacker extracts massive local data in high quality from outside with multiple triggers (ComboTs).**

## 2 Background

Denoising Diffusion Probabilistic Models (DDPMs).DDPMs  have garnered considerable interest due to their capacity to generate high-fidelity samples. At the core of DDPMs lies the fundamental principle of modeling the distribution of data through a diffusion process. In this progression, the initial data \(x_{0}\) undergoes incremental corruption until it eventually converges into a state of complete noise \(x_{t}\) with \(t\) (\(1 t T\)) steps.

\[x_{t}=_{t}}x_{0}+_{t}}, \]

where \((0,I)\) is a Gaussian noise, and \(_{t}=_{i=1}^{t}_{i}\) is a hyperparameter that gradually increases the noise level in the forward process. Subsequently, a reverse process works as a denoising operation to retrieve the original data from the noise. This process is defined by a neural network, which is trained to predict the noise \(\) at timestep \(t\). Then, the trained model can gradually denoise random noise to photorealistic images.

Diffusion Models in Federated Learning.FL  involves a central server and multiple distributed \(N_{c}\) clients to train a global model without sharing the data. Clients periodically send local updates to the central server, which combines all updates and sends back the improved global model. This process is repeated until the server completes the training and releases the global model to the users. Analogous to the training of classification models, we train diffusion models independently on each client before being combined on the server. Each client completes the training of the diffusion model autonomously and uploads the trained model and its Exponential Moving Average (EMA) model.

## 3 DataStealing: Task and Algorithms

### Threat Model and Attack Scenario

Consider the following FL scenario: multiple participants aim to collaboratively train a diffusion model using sensitive, privately-held training data \(D_{benign}\) (_e.g._, medical images) securely stored on their respective clients. Utilizing diffusion models, which inherently generate data distinct from the original inputs, ensures privacy protection by averting the exposure of personal data characteristics. A trusted server aggregates the gradients provided by each client, updating the global model and

Figure 2: **Overview of ComboTs and AdaSCP.** (a) ComboTs choose two points from candidate positions to form multiple triggers for mapping target images. (b) The forward and backward Trojan diffusion process. After training with ComboTs, the poisoned model can restore the target images in high quality from Trojan noise, thereby enabling _DataStealing_. (c) AdaSCP achieves the purpose of _DataStealing_ and defeats the advanced defenses by training critical parameters and adaptively scaling the updates before uploading.

redistributing to each client. After the training phase concludes, the server makes the global model available to all users.

Attacker's Goal.Attacker partition client data into two subsets: \(\{D_{benign},D_{backdoor}\}\). The attacker's goal is to extract a large number of training images, \(D_{backdoor}\), from the compromised client via the released global model, as illustrated in Figure 1. Additionally, the attacker should minimally affect the original model's functionality to preserve the stealth of the backdoor attack.

Attacker's Abilities.Under strict data privacy protection, attackers can not upload or download images from the infiltrated client. To achieve the goal of embedding the target data \(D_{backdoor}\) into the global diffusion model, the attacker is permitted to enact arbitrary alterations to the training procedure of the diffusion model and the uploaded gradients of the infiltrated client. To streamline the task and facilitate analysis, we focus on scenarios with a single attacker and exclude those with multiple distributed attackers. Considering the difficulty of training diffusion models and the complexity of our task, we assume that the attacker has prior knowledge of the total number of participating clients.

### Combinatorial Triggers

Conventional backdoor attacks typically use a single trigger, such as a corner patch in the image mask, to generate a target image. However, a fixed trigger is impractical to generate plenty of target images. To tackle this issue, we introduce Combinatorial Triggers (ComboTs) to fulfill the need of mapping multiple target images. Firstly, we choose \(p\) candidate points in a grid pattern on the image mask to form the candidate positions. Subsequently, we select q points from the candidate positions in a combinatorial manner. The total number of potential triggers can be determined using the straightforward combination formula: \(C(p,q)\). For a \(32 32\) image mask with a pixel spacing of 1, the value of \(p\) is 225 when border pixels are excluded. When \(q\) is set to 2, the number of ComboTs is 25,200, which fully satisfies our requirements. Furthermore, the ComboTs demonstrate significant scalability: with \(q\) set to 3, the potential number of ComboTs can exceed 2 million.

Once ComboTs are obtained, they will be utilized as backdoor triggers for training. As shown in Fig. 2 (a), each trigger \(_{i}\) corresponds to a target image \(x_{0,i}\). Then we extend the Trojan attack method  on the diffusion model from one trigger-target pair to multiple pairs. According to Eq. 1 the forward diffusion process with trigger \(_{i}\) can be represented as:

\[x_{t,i}=_{t}}x_{0,i}+_{t}}( +(1-)_{i}), \]

where \(\) is the blend weight. Trojan noise \((+(1-)_{i})\) is a blended image between Gaussian noise \(\) and trigger \(_{i}\). Fig. 2 (b) shows the forward and backward diffusion process with one trigger. The sampling process with Trojan noise has been introduced in the literature . The sampling process is extended from one trigger to multiple triggers according to our ComboTs.

### Adaptive Scale Critical Parameters Attack

To achieve an effective attack, model poison  is treated as our baseline, which magnifies the updates with a scale value. As Table 1 shows, model poison (scale is \(N_{c}\)) with ComboTs succeed in stealing thousands of data from the FedAvg framework. However, such a brute-force method is ineffective in advanced FL with defense protocol. Two reasons make the attack invalid: **1)** The difficulty of training diffusion models with ComboTs requires scaling the malicious updates, which will be easily detected by distance-based defenses . **2)** The training of the diffusion model will collapse with a high scale value due to its high sensitivity to the gradients.

To evade the advanced distance-based defenses, we introduce an Adaptive Scale Critical Parameters attack algorithm (AdaSCP). To alleviate the effect of scaling, AdaSCP estimates and trains critical parameters  instead of the whole network (Sec. 3.3.1). As for the second issue, AdaSCP adopts an adaptive scaling policy with an implanted indicator  (Sec. 3.3.2 and Sec. 3.3.3). The details of AdaSCP are shown in Algorithm 1 and Fig. 2 (c). Taking broadcast global model \(G_{r}\), local client's datasets \(\{D_{benign},D_{backdoor}\}\), indicator magnification factor \(k\) and previous attack setting \(I_{r-1}\), \(s_{r-1}\) as inputs, AdaSCP reads the returned indicator and adaptively adjust the scale value \(s_{r}\) to defeat the defenses (Line 1). To avoid excessive parameter modification in one specific indicator, we restore the indicator and use the next candidate indicators \(I_{r}\). Then, if the candidate indicators \(I_{r}\) are used up, the attacker will estimate the importance score of \(w_{G}\) with benign dataset \(D_{benign}\) to acquire the critical parameter mask \(M_{G}\) and then find new candidate indicators (Lines 2-5). After that, AdaSCP trains the critical parameters with \(D_{benign}\) and \(D_{backdoor}\) and scale the critical updates with \(s_{r}\) to acquire the scaled malicious model \(w_{r}\) (Line 6). Finally, we implant the new indicator to get the indicator-implanted backdoor model \(w^{*}_{r}\), which will be uploaded to the server (Lines 7 and 8).

#### 3.3.1 Diffusion Importance Estimation

To defeat the advanced defenses with critical parameters in diffusion models, we first adopt the diffusion importance estimation used in the structural pruning task of diffusion model . They claim that numerous noisy and redundant timesteps make minimal contributions to the overall generation in the diffusion process as \(t\) approaches \(T\). Hence, our importance scores are estimated with accumulated gradients in dominant timesteps under the Taylor expansion framework . To locate the critical parameters, we sort estimated importance scores and choose a proportion \(\) to filter important parameters of the diffusion model. The mask \(M_{G}\) of critical parameters is then obtained for subsequent training. In the meantime, preparing for finding indicators, we record the absolute value of accumulated gradients \(|_{G}|\) and the corresponding Hessian matrix \(|H_{G}|\), which represents the changing direction of gradient updates . More details can be found in Appendix C.1.

```
0: Global model \(G_{r}\), accumulated gradients \(|_{G}|\), Hessian matrix \(|H_{G}|\), indicator magnification factor \(k\), current scale \(s_{r}\), mask \(M_{G}\)
0: Indicator-implanted model \(w^{*}_{r}\), candidate indicators \(I_{r}\)
1:\(I^{}\{I^{}_{i} M_{G}(I^{}_{i})=\}\)
2:\(=\{_{1},_{2},...,_{j}\}_{I_{1}  I^{}}_{|_{G}|,I_{i}}\)
3:\(I_{r}=\{I_{1},I_{2},...,I_{m}\}_{I_{1} I}_{|H_{G}|,I_{i}}\)
4:for each\(I_{i} I_{r}\)do\(M_{G}(I_{i})\) True
5:\(w_{r} TrainCriticalParam(...,M_{G})\)
6:\(\) Implant the first indicator to get \(w^{*}_{r}\)
7:\(^{}_{w_{r}-G_{r},I_{1}}}_{w_{r }-G_{r},I_{1}}\)
8:return\(w^{*}_{r},I_{r}\)
```

**Algorithm 2** Find Candidate Indicators and Implant New Indicator

#### 3.3.2 Find Candidate Indicators

Algorithm 2 illustrates the process of finding candidate indicators. To prevent the magnification of indicators affecting the performance of the diffusion model, we find candidate indicators in the _uncritical_ parameters according to \(M_{G}\) (Line 1). Inspired by previous work , we consider two necessary conditions to choose the indicator (lines 2 and 3). Firstly, the neuron's gradient update should be smaller than other neurons. Secondly, the neuron's Hessian value, the second derivative of diffusion loss, should be close to zero. This means the change of this neuron contributes less to the performance . Besides, to mitigate the influence of the backdoor dataset and align indicators with other clients, we only use the benign dataset \(D_{benign}\) to calculate gradients and curvatures. To improve the efficiency of finding candidate indicators, we only compute \(|H_{G}|\) in the last several layers of the diffusion model as the whole parameters bring a heavy burden on calculating the second derivative. Moreover, to avoid the computation of \(|_{G}|\) and \(|H_{G}|\) in every round, we select \(m\) indicators at one time to form a candidate indicator set \(I_{r}=\{I_{1},I_{2},...,I_{m}\}\) for subsequent use (Line 3).

To implant the indicator, AdaSCP treats the indicators as critical parameters and magnifies its updates (Lines 4 - 7). After training and scaling the critical parameters, we can get the scaled backdoor model \(w_{r}\). Then AdaSCP chooses the first indicator \(I_{1}\) in candidate queue \(I_{r}\) and _re-scale_ the corresponding update \(_{w_{r}-G_{r},I_{1}}\) with the magnification factor \(k\) and current scale value \(s_{r}\). The rescaled update \(^{}_{w_{r}-G_{r},I_{1}}\) will be merged to form the indicator-implanted backdoor model \(w^{*}_{r}\) of this round (Line 7).

#### 3.3.3 Adaptive Scale with Indicator

Although the implanted indicator can help us judge whether the malicious model is accepted by the FL server or not, the potential of the indicator has not yet been fully tapped. We observe that _although the black-box server can ensure that a malicious client is unaware of its weight in the aggregation process, the returned indicator update implies this information_. This inspires us to control the weighted average results by adjusting the scale value \(s_{r}\). The optimal scaling factor enables the attacker to circumvent distance-based defenses and seamlessly incorporate backdoor gradients into the global model, bolstering the efficiency and effectiveness of attacking with multiple Trojans. Hence, we design an adaptive algorithm to optimize the scaling factor \(s_{r}\).

Algorithm 3 illustrates the procedure of our adaptive scale algorithm. According to the global model \(G_{r}\), we can get the returned indicator update \(_{G_{r}-G_{r-1},I_{1}}\) in the (\(r-1\))_-th_ round. At first, we need to define the target scale value \(tar_{s}\) according to the returned indicator update. Given \(n_{c}\) local updates participating in averaging and the weights \(_{i}\) for each client, the optimal value of \(tar_{s}\) should be \(_{i=1}^{n_{c}}_{i}/_{adv}\) for poisoning the global updates with the adversary updates. When \(_{i}=1\), \(tar_{s}\) equals the number of selected clients \(n_{c}\), which is the optimal scale value used in model poison . But when the indicator-implanted model is rejected by the server, the scale value should approach zero to defeat the defenses. As shown in Line 1 and 2, we can derive \(tar_{s}\) with the uploaded and returned indicator updates. We leave the derivation in Appendix D.

A natural way is to replace \(s_{r}\) with \(tar_{s}\) when "_Accepted_" by the server. However, this is not practical. The derivation of \(tar_{s}\) is based on the assumption that the trained updates at the position of indicator \(_{w_{i}-G_{r},I_{1}}\) are similar across all clients due to the very small gradients. This assumption is not always valid, especially when training with non-IID or backdoor datasets. To reduce the impact of deviation caused by training, we treat \((tar_{s}-s_{s-1})\) as an optimization direction toward the optimal scale value. Hence, we define a learning rate \(\) to optimize the scale value \(s_{r}\). The adaptive scale can defeat some defenses and implant multiple backdoors stealthily. However, some defenses  are effective in detecting the scaled malicious updates or diluting them by averaging. This will lead to an unstable training process, where \(s_{r}\) fluctuates between 0 and \(tar_{s}\). To stabilize \(s_{r}\) and improve the attack effectiveness, we record history scale values in \(his_{acc}\) and \(his_{rej}\). They further help determine the optimal value of \(tar_{s}\) and \(s_{r}\) (Line 3). Additionally, we design a weight decay \(d\) for decreasing the learning rate \(\) according to the accepted state in the previous and current rounds. Finally, we restore the indicator value and dequeue the first index \(I_{1}\) in candidate indicators to prepare for the subsequent operation in the current round (Line 4). More details are provided in Appendix C.3.

```
0: Indicator magnification factor \(k\), previous attack setting \(s_{r-1}\), \(I_{r-1}\), uploaded indicator update \(^{}_{w_{r-1}-G_{r-1},I_{1}}\), returned indicator update \(_{G_{r}-G_{r-1},I_{1}}\), learning setting \(his_{acc}\), \(his_{rej}\), \(\), \(d\)
0: Current scale \(s_{r}\), candidate indicators \(I_{r}\)
1:\(f-G_{r-1},I_{1}}}{^{d}^{}_{ w_{r-1}-G_{r-1},I_{1}}}\) Theorem 1 and 2 in Appendix D
2:\(tar_{s}\{& (0,2 N_{c}]\\ 0&.\)\(\) "_Accepted_" or "_Rejected_" by the server
3:\(s_{r} OptimizeScale(s_{r-1},tar_{s},,d,his_{acc},his_{rej})\)\(\) Appendix C.3
4:\(I_{r} RestoreAndDequeueIndicator(k,^{}_{w_{r-1}-G_{r-1},I_{1}},I_{r-1})\)\(\) Appendix C.4
5:return\(s_{r},I_{r}\)
```

**Algorithm 3** Adaptive Scale with Indicator

## 4 Experiments

Datasets and Diffusion Models.We conduct our experiments on three widely-used datasets in generation tasks: CIFAR10 (\(32 32\)) , CelebA (\(64 64\))  and LSUN Bedroom (\(256 256\)) . We concentrate on Denoising Diffusion Probability Models (DDPMs) . We follow the model replacement attack , injecting backdoor data as training approaches convergence. To simulate the attack process efficiently, we train the diffusion models under the FL framework for CIFAR10 and CelebA. Then we use the pre-trained model for subsequent backdoor experiments. The pre-training round number is 2,000 for CIFAR10 and 1,200 for CelebA. As for LSUN Bedroom, we use the pre-trained weight from DDPMs .

Backdoor Attacks.Due to the considerable cost of training diffusion models, we simulated five local clients and selected one as the adversarial client. Based on the pre-trained model, we fine-tune 300 rounds for CIFAR10 and 150 rounds for CelebA and LSUN Bedroom in every client. We train 10,000 images per round per client for CIFAR-10 and CelebA, and 1,000 images for LSUN Bedroom. The patch size of ComboTs is \(3 3\) for CIFAR10, \(5 5\) for CelebA, and \(25 25\) for LSUN Bedroom. The backdoor and benign datasets are mixed in one batch with a \(50/50\) ratio. For the attack algorithm, we select the common and latest attack methods as our baselines: Data Poison , Model Poison , PGD Poison , and Backdoor-Critical (BC) layer substitution . We set the scale value of model poison as the total number of clients (\(N_{c}=5\)). We train about \(80\%\) parameters in our AdaSCP and layers in the BC layer substation for fair comparison. More details can be found in Appendix B.

Defenses.To verify the effectiveness of _DataStealing_ methods under defense strategies, we conduct experiments with various distance-based defense methods: FedAvg , Krum , Multi-Krum , Foolsgold , RFA , Multi-metrics . We observe that clipping and adding noise, commonly used in the differential privacy method , will undermine the training of diffusion models. Therefore, we do not consider these defenses in our experiments.

Evaluation.We leverage two metrics for evaluating the performance of diffusion models and the efficacy of DataStealing: 1) the Frechet Inception Distance (FID) . 2) Mean Square Error (MSE) between the generated backdoor images and the ground truth images. A lower FID score indicates that

    &  & FedAvg  & Krum  & Multi-Krum  & Foolsgold & RFA  & Multi-metrics  & Mean \\   & Data Poison  & 6.870,1226 & 10.090,140 & 6.200,1427 & 7.700,1238 & 6.70,1241 & 7.09,1233 & 7.450,1304 \\  & Model Poison  & 12.860,0169 & 8.290,1454 & 6.230,1426 & 4.295,640,3124 & **6.120,1419** & **7.090,1685** & 94.020,1492 \\  & PGD Poison  & 6.860,1223 & 19.980,1239 & 6.930,1212 & 7.450,1243 & 6.580,1231 & 6.70,1228 & 9.140,1232 \\  & BCLlevu [Sub.6], & 7.550,1382 & 132.020,1719 & 6.900,1433 & 6.670,1888 & **5.600,1488** & 6.690,1232 & 7.130,1441 \\  & AdaSCP (Ours) & 12.930,**0.0117** & 30.68,**0.0861** & 8.230,**1271** & 24.21,**0.0129** & 8.220,1233 & 15.04,**0.0328** & 16.550,**0.0657** \\   & Data Poison  & 5.910,1304 & 7.640,150 & 6.130,1506 & 6.220,141 & 5.740,1212 & 6.650,**0.0922** & 6.380,1317 \\  & Model Poison  & 16.068,**0.0865** & 7.950,1342 & 6.150,1504 & 3.660,130,3167 & 5.400,**0.0808** & **N/A** & **6.940,1502** \\  & PGD Poison  & 8.160,1516 & 7.01/**0.0442** & 8.040,1435 & 6.490,1636 & 8.020,1263 & 7.440,1362 & 7.530,1279 \\  & BCL Layer Sub. & 12.290,1333 & 76.490,0536 & 15.630,**1204** & 10.400,1471 & 18.360,1119 & 17.00,1177 & **25.00,1137** \\  & AdaSCP (Ours) & 7.000,**0.0882** & 13.660,0407 & 4.553,1312 & 7.360,**0.0143** & 6.200,1429 & 7.620,**0.0144** & 7.370,**0.0490** \\   & Data Poison  & 23.500,0969 & 12.280,2512 & 25.31,**0.1169** & 23.47,0.1321 & 23.45,**0.0947** & 22.44,**0.0862** & 21.740,**0.1297** \\  & Model Poison  & 33.000,**0.0273** & 11.970,2557 & 31.330,1239 & 40.392,1229 & 21.380,0894 & 17.830,3135 & 110.000,2003 \\   & PGD Poison  & 23.490,0796 & 11.950,2546 & 39.390,1476 & 13.01,**1282** & 23.68,069,059 & 12.270,0966 & 22.770,1368 \\   & BCL Layer Sub. & 108.840,1392 & 45.770,**1187** & 12.200,1391 & 15.410,1361 & 13.900,1313 & 13.050,1354 & 18.540,1328 \\   & AdaSCP (Ours) & 22.300,**0.0544** & 51.150,**1634** & 25.81/**0.1131** & 25.80,**0.054** & 24.360,1162 & 22.28/**0.0623** & 29.070,**0.0941** \\   

Table 1: _DataStealing_ in Non-IID Datasets. Performance of AdaSCP compared to the SOTA attack methods with various advanced defenses in non-IID distribution. "\(\)": lower is better. Red: the 1st score. Blue: the 2nd score. (*: averaging by ignoring the collapsed result.)

Figure 3: **Qualitative Comparison**. The generated image with the lowest MSE in one trigger-target pair is presented under every attack and defense method. More visual results are in Appendix A.10.

the generated images better match the distribution of real images, implying higher quality. A lower MSE means the restored images are more similar to the private images, indicating better performance in _DataStealing_. To calculate FID, we generate 50,000 images for every model with CIFAR10 and CelebA and 10,000 images for LSUN Bedroom. As for MSE, we synthesize 20 images with different noise inputs for every combinatorial trigger. All images are generated with \(50\)-timestep DDIM .

### _DataStealing_ under Advanced FL Defenses

To evaluate AdaSCP and attack baselines in _DataStealing_, we conduct experiments on CIFAR10 , CelebA  and LSUN Bedroom . The target image count is 1,000 for the CIFAR10 dataset, 500 for the CelebA dataset, and 50 for LSUN Bedroom.

Comparisons with Non-IID Datasets.We first consider the challenging non-IID distribution. Both datasets are sampled with Dirichlet distribution of hyperparameter \(0.7\). As shown in Table 1, our AdaSCP achieves the best attack performance according to MSE. The critical parameters and adaptive scale factor enable AdaSCP to evade the detection of most distance-based defenses. Additionally, the adaptive scale prevents the inclusion of backdoor updates from devastating the overall training of the diffusion models. Although Model Poison  can achieve the aim of _DataStealing_ under FedAvg (12.86/0.0069), a large and fixed scale value makes it hard to defeat the advanced defenses or causes the training of diffusion model collapse, as shown in Fig. 3. Our adaptive scale solves these problems with an acceptable and proper scale value. PGD Poison  can overcome all defenses in CIFAR10 by constraining the updates within a limited range. However, the efficiency of this attack remains a significant concern. AdaSCP achieves the balance between stealthiness and efficiency. The results of BC Layer Substitution  show that it is not a good choice for poisoning the diffusion model. The layer substitution process leads to training collapse in the diffusion model, especially under the defense of Krum (132.02/0.1719). This demonstrates that training critical parameters is more effective than layer substitution in attacking the diffusion model.

Compared to CIFAR10, AdaSCP is more effective than other attacks in CelebA. We observe that the optimal \(tar_{s}\) is \(3.60\) in CelebA under FedAvg, which is smaller than the corresponding value of \(4.54\) in CIFAR10 and the total number of local clients. The difference means the data proportion in the infiltrated client is more unbalanced in CelebA than in CIFAR10, which can affect the performance of attacking methods. Moreover, experiments on the LSUN Bedroom dataset demonstrate that AdaSCP can achieve the best attack performance on high-resolution images (\(256 256\)). This indicates that AdaSCP is more robust and effective with imbalanced and diverse data.

In summary, Data Poison and PGD Poison encounter difficulties in effectively implanting multiple backdoors. Model Poison fails to bypass advanced FL defenses due to improper scale values. BC Layer Substitution is unsuitable for training diffusion models and may lead to training collapse. AdaSCP outperforms other methods by utilizing critical parameters with adaptive scale factors, balancing stealth and efficiency while preventing training collapse in diffusion models.

    &  Attack-Defenses \\  } &  & Multi-Krum  &  &  &  &  \\    & & FID / J MSS & FID / J MSS & FID / J MSS & FID / J MSS & FID / J MSS &  \\    } & Data Poison  & 6.38/10.124 & 5.50/10.1435 & 6.39/**12.146** & 6.34/10.260 & 5.87/10.124 & 6.10/10.1283 \\  & Model Poison  & 8.40/**0.0653** & 5.52/10.1432 & 456.00/30.190 & 5.88/**12.121** & 10.55/**0.047** & 97.27/**11.173** \\  & PGD Poison  & 6.37/10.1248 & 6.17/10.1428 & 6.38/10.152 & 6.35/**12.147** & 5.89/10.142 & 6.23/10.147 \\  & LC Layer Sist.  & 529.70/1362 & **550.13/1340** & 52.50/1262 & 5.56/**10.1308** & 5.38/10.105 & 5.42/10.115 \\  & AdaSCP (Ours) & 8.59/**0.0085** & 7.09/**0.1273** & 12.84/**0.0645** & 7.03/10.1285 & 8.75/**0.203** & 8.85/**0.06/66.699** \\   

Table 2: _DataStealing_ in IID Dataset. Performance of our approach and the SOTA attack methods with various advanced defenses in IID distribution.

Figure 4: **Ablation Study on Trigger Number.**

Comparisons with IID Dataset.Table 2 shows the results of _DataStealing_ on CIFAR10 in IID distribution. In this scenario, the attacker is easier to defeat the defenses. Model Poison  success in circumventing the defense of Multi-metrics  (10.55/0.0047) and exceed the PGD Poison on average. Our AdaSCP is still superior in attacking on average (8.86/0.0699). It is because the adaptive scale can bypass defenses that other attack methods fail.

### Ablation Study

Ablation on Each Component of AdaSCP.To verify our proposed AdaSCP, including the critical parameter training and adaptive scaling, we conduct an ablation study on each component. We adopt the model poison as our baseline and set the initial scale value as \(N_{c}\). As Table. 3 shows, training with critical parameters helps the model defeat the Multi-metrics defense (30.59/0.0422) with a sacrifice of attack performance in FedAvg and RFA. It is reasonable as only about \(80\%\) parameters are included in the training. Furthermore, based on training critical parameters, we evaluate the effect of \(tar_{s}\) by replacing \(s_{r}\) with \(tar_{s}\) directly. It shows that the estimated target scale \(tar_{s}\) plays a critical role in defeating many defenses, which proves the effectiveness of our design. However, the deviation of training affects the accuracy of \(tar_{s}\) and results in a decline in FedAvg (12.97/0.0157). Then we use \(tar_{s}\) to optimize \(s_{r}\) with a learning rate \(\) that can alleviate the problem and can achieve slightly better performance. With the stabilization of history scales, we make progress in defeating Multi-Krum (8.23/0.1271) and achieve the best performance on average. Moreover, the decrease in MSE for Multi-Krum and RFA implies that they can be attacked by AdaSCP in a longer training.

Ablation on Different Trigger Number.To assess the impact of trigger quantity on _DataStealing_, we undertook ablation studies with incremental numbers of triggers with AdaSCP. Fig. 4 shows that reducing the number of target images increases the success of attacking. And it is increasingly hard to steal more data. Following a successful theft, the FID tends to remain at a relatively high level. This represents one of the limitations of _DataStealing_. More experiments are listed in Appendix A.

## 5 Discussion on Defense Mechanisms

To safeguard the training of diffusion models in FL, this section discusses potential defensive mechanisms for future research. First, Multi-Krum has shown strong performance in detecting and diluting malicious updates, preventing attacks from quickly degrading the global model's accuracy. However, as shown in Appendix A.1, its effectiveness diminishes over long-term training, suggesting future enhancements like adaptive thresholds  or combining it with other methods . Second, differential privacy methods can invalidate indicators by adding noise or norm clipping while sacrificing generative performance, as discussed in Appendix F. Since AdaSCP requires specific indicators with a large magnification factor, identifying candidate indicators and filtering outliers through comparison could enhance efficiency and effectiveness in defending against _DataStealing_. Third, the durability experiment in Appendix A.2 demonstrates that backdoors diminish after 100 rounds of continued training with clean data, suggesting a potential mitigation strategy for future work . Finally, as the triggers are embedded in the input noise, releasing only the generated outputs without exposing the global model could serve as an additional defensive strategy.

## 6 Related Work

Trojan Attacks on Diffusion Model.Trojan attacks  on diffusion models have been proposed recently. BadDiffusion  and TrojDiff  attack DDPMs  with an additional trigger

    & FedAvg  & Krum  & Multi-Krum  & Foolgold  & RFA  & Multi-metrics  &  \\    & FID \(\) / MSE \(\) & FID \(\) / MSE \(\) & FID \(\) / MSE \(\) & FID \(\) / MSE \(\) & FID \(\) / MSE \(\) \\  Baseline  & 12.86\(\)**0.069** & 8.290\(\)1.054 & 6.230\(\)1.426 & 4.59\(\)0.342 & 6.12\(\)**0.119** & 70.98\(\)1.685 & 94.02\(\)0.1492 \\ + Critical Parameters & 12.56\(\)0.0123 & 8.650\(\)1.0495 & 6.210\(\)1.0424 & 46.81\(\)0.313/3.138 & 6.27\(\)0.1249 & 30.59\(\)0.0422 & 87.72\(\)0.1302 \\ + Target Scale \(tar_{s}\) & 12.97/0.0157 & 31.80\(\)**0.083** & 71.7/1.039 & 38.22/0.273 & 10.37/0.1237 & 11.31/0.0554 & 18.64/0.0737 \\  +Learning Rate \(\) & 12.750/0.0144 & 31.77/0.00864 & 7.160\(\)**1.389** & 20.68\(\)**0.0077** & 7.230\(\)1.254 & 12.01/0.0602 & 15.27/**0.0722** \\  History scales & 12.93\(\)**0.117** & 30.65\(\)**0.0861** & 8.23\(\)**0.1271** & 24.21/**0.129** & 8.22\(\)**0.1233** & 15.01/**0.0328** & 16.55/**0.0657** \\   

Table 3: **Ablation Study of AdaSCP. Incremental components are added in every row to evaluate their function. Each component contributes to the overall performance.**term in the forward/backward process. Struppek _et al._ attacks the text-to-image diffusion models by adding triggers to the text inputs. VillanDiffusion  attacks various diffusion models and samplers with image or caption triggers in a unified backdoor framework. In this paper, we show that diffusion models in FL can also be subject to multiple Trojans with combinatorial triggers.

More methods  focus on backdoor poison with multiple distributed attackers or optimizing the triggers. 3DFed  proposes three modules: constraint loss, noise masks, and decoy models. They adjust hyper-parameters of these modules with the acceptance status ("_Accepted_", "_Clipped_" or "_Rejected_") based on the feedback of indicators. However, their indicators and decoy models require collaboration among multiple attackers, which does not align with our task that involves only a single malicious client. We leave _DataStealing_ with multiple distributed attackers for future work.

Trojan Attacks on Federated Learning.Since Bagdasaryan _et al._ point out that the widely-used FL algorithm FedAvg  is vulnerable to model replacement attack, plenty of backdoor attack methods  have subsequently emerged. Data poison  attacks the global model by replacing the local training data with a mixture of benign and backdoor datasets. Model poison  replaces the global updates with scaled malicious updates. PGD poison  projects the local gradients into a sphere surrounding the global model. Backdoor-critical layer substitution  inserts the critical malicious layers to the benign models to balance the attack effectiveness and stealthiness. The above attack methods are applied to the classification task. We are the pioneers in investigating the issue of stealing massive private data under stringent security measures in FL. Furthermore, we propose AdaSCP to defeat the advanced distance-based defenses.

More Related Works.Various studies are related to our work. A more detailed discussion of related research is provided in the Appendix E.

## 7 Conclusion

In this paper, we explored the vulnerabilities of diffusion models within the FL framework, highlighting new avenues for privacy threats through our proposed _DataStealing_ task. Even under rigorous local data protection, substantial private data can be exfiltrated from local clients by employing our ComboTs. Furthermore, we introduce AdaSCP, an adaptive attack method, designed to circumvent advanced distance-based defenses. Our method adaptively seeks the optimal scale value with the indicators. It then trains and magnifies critical parameters, allowing for the seamless integration of backdoor gradients into the global model. Our extensive experiments not only confirm the efficacy of existing FL Trojan attacks and defenses but also highlight the superior performance of AdaSCP in circumventing advanced distance-based defenses. This work serves as a critical reminder of the vulnerabilities in FL and underscores the necessity for continuous advancement in defensive strategies to safeguard against evolving threats in the realm of generative models. We hope our findings will catalyze further research and prompt the FL community to prioritize the development of robust mechanisms to protect privacy in the increasingly complex landscape of machine learning models .

## 8 Limitations and Ethical Statements

Due to the limited page number, we discuss them in Appendix F

## 9 Acknowledgment

This work is supported by the National Key R&D Program of China (Grant No.2022ZD0160101), Fundamental Research Funds for the Zhejiang Provincial Universities (Grant No.226-2024-00208), National Key R&D Program of China (Grant No.2021ZD0112801), National Natural Science Foundation of China (Grant No.62306273) and Fundamental Research Funds for the Central Universities, Sun Yat-sen University (Grant No.24qnpy154).