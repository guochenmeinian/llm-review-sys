# ZipLM: Inference-Aware Structured Pruning

of Language Models

 Eldar Kurtic

IST Austria

eldar.kurtic@ist.ac.at

&Elias Frantar

IST Austria

elias.frantar@ist.ac.at

Dan Alistarh

IST Austria & Neural Magic

dan.alistarh@ist.ac.at

###### Abstract

The breakthrough performance of large language models (LLMs) comes with major computational footprints and high deployment costs. In this paper, we progress towards resolving this problem by proposing a novel structured compression approach for LLMs, called ZipLM. ZipLM achieves state-of-the-art accuracy-vs-speedup, while matching a set of desired target runtime speedups in any given inference environment. Specifically, given a model, a dataset, an inference environment, as well as a set of speedup targets, ZipLM iteratively identifies and removes components with the worst loss-runtime trade-off. Unlike prior methods that specialize in either the _post-training/one-shot_ or the _gradual compression_ setting, and only for specific families of models such as BERT (_encoder_) or GPT (_decoder_), ZipLM produces state-of-the-art compressed models across all these settings. Furthermore, ZipLM achieves superior results for a fraction of the computational cost relative to prior distillation and pruning techniques, making it a cost-effective approach for generating an entire family of smaller, faster, and highly accurate models, guaranteed to meet the desired inference specifications. In particular, ZipLM outperforms all prior BERTbase distillation and pruning techniques, such as CoFi, MiniLM, and TinyBERT. Moreover, it matches the performance of the heavily optimized MobileBERT model, obtained via extensive architecture search, by simply pruning the baseline BERTlarge model. When compressing GPT2, ZipLM outperforms DistilGPT2 while being 60% smaller and 30% faster. Our code is available at: [https://github.com/IST-DASLab/ZipLM](https://github.com/IST-DASLab/ZipLM).

## 1 Introduction

The high accuracy of modern language models from the Transformer family  comes at the price of massive computational cost, which hinders their practical adoption in resource-constrained settings. This has motivated the development of _model compression_ techniques, which can be categorized into _pruning_, _quantization_, and _distillation_. In this paper, we focus on _structural compression_, whose goal is to reduce model size by removing entire sub-components, such as rows or columns from the model's weight matrices. The key advantage of structured pruning, relative to unstructured pruning of individual weights, is that the model can be reshaped to new dimensions, and the resulting computational savings can be leveraged on any hardware, without specialized computational support. At the same time, structured pruning introduces significant challenges. First, models are usually highly-sensitive to structured compression, and most methods require _gradual compression_, including retraining cycles designed to allow the model to recover accuracy. In addition, structural compression significantly complicates the use of knowledge distillation , which is usually done via manual or dynamic layer mapping . On the practical side, another challenge is that most existing techniques do not provide _runtime speedup_ guarantees: the model is pruned to a fixed sparsity or FLOPS target, and then must be evaluated in the target inference environment. If the pruned model fails to meet the target inference specifications, the whole process must be repeated from scratch.

**Overview.** In this paper, we resolve these issues and provide a novel structured pruning approach called ZipLM, which achieves state-of-the-art performance, both in the _post-training/one-shot_ setting, where retraining is not desirable, as well as in the popular _gradual compression_ setting, where retraining is possible. We accomplish this via an inference-aware algorithm, which successfully balances the loss-runtime trade-off at each pruning step. By taking runtime into account, we avoid removing components that do not bring significant speedup gains. Additionally, our algorithm provides speedup guarantees for compressed models, a highly-desirable property in practical applications.

We summarize our contributions as follows:

* We introduce a novel structured pruning approach, which unifies the saliency criteria investigated by prior work-weight magnitude, activation impact, and removal of linearly-redundant structures, while considering local (layer-wise) and global correlations. We augment it to be _inference-aware_, ensuring desired latency or throughput in any given configuration.
* We complement the algorithm with a novel _layer-wise token-level distillation_, which consistently boosts accuracy on small datasets and does not require manual layer matching, circumventing a limitation of prior structured pruning techniques.
* ZipLM is the first structured pruning approach that achieves state-of-the-art results for both, _post-training/one-shot_ compression and _gradual pruning_ settings, while being applicable to both, BERT (_encoder_) and GPT (_decoder_) language models, without any modifications.
* ZipLM is practical and efficient. For a set of desired speedups (e.g. 2x, 5x, 10x) in the target inference environment (e.g. batch-size=128, sequence-length=384, device=V100), in a single run and under the same set of hyper-parameters, it produces the entire family of compressed models, one for each speedup target. Consequently, it leads to state-of-the-art results in _GPU-based_ inference environments. Moreover, it is compatible with unstructured pruning and quantization, leading to state-of-the-art results even for _CPU-based_ environments.

## 2 Related Work

**Distillation-based compression methods** focus on training a smaller student model to mimic the representations of a larger teacher model. The "distance" between the representations of student and teacher is often architecture-specific. MiniLM  uses a deep self-attention mechanism to replicate the attention mechanism of the teacher, and TinyBERT  employs a bespoke distillation mechanism, for a manually-picked subset of layers. Both methods offer a very strong baseline, generally outperforming other approaches, except for MobileBERT. MobileBERT  involves first training a custom large BERT teacher model from scratch, and then deviates from the standard architecture  by introducing heavily-optimized components with reduced latency, whose combinations are decided in neural architecture search (NAS)-like fashion. It achieves strong results in terms of accuracy-per-parameter, at the cost of significant computational costs in the search process. DistilBERT and DistilGPT2  involve training a fixed student obtained by removing every other layer from the teacher, while BERT-PKD  employs incremental knowledge extraction through the distillation of intermediate layers. Well-Read-Students  reduces the size of the standard BERT architecture through principled downscaling of internal dimensions. DynaBERT , on the other hand, distills knowledge to a student model that is both depth- and width-adaptive.

**Structural pruning methods** usually start from a large pre-trained model, and iteratively reduce the dimensions of weight matrices. Block Movement Pruning  identifies and removes redundant rectangular blocks of weights while following the movement pruning intuition  that weights moving towards zero during fine-tuning should be removed. FLOP  and Low-Rank  use matrix decomposition techniques to progressively remove rank-1 components from factorized weight matrices during training. BERT-of-Theseus  employs a similar approach, but replaces entire submodules with smaller counterparts. Methods like LayerDrop  and Poor Man's BERT  address structured compression through various layer-dropping techniques. LayerDrop uses structured layer-dropout regularization to train a model resilient to sub-network selection during inference, while Poor Man's BERT explores a wide range of layer-dropping strategies. The recent CoFi method  employs masks of different granularities to jointly prune coarse and fine-grained submodules during fine-tuning, combined with an optional customized distillation technique. CoFi is the state-of-the-art _structural pruning_ method; relative to distillation methods, CoFi outperforms MiniLM and TinyBERT, but not MobileBERT, in terms of accuracy-vs-speedup.

**Other compression methods** such as the ones that exploit dynamic forms of sparsity which appear at runtime , or the ones that utilize lower bit-width representation of weights and/or activations  are complementary to our approach. We demonstrate this in Section 5 where we apply quantization to obtain even higher compression ratios for edge deployment environments like commodity CPUs.

## 3 Method

Removing large structures like entire matrix columns or attention heads from a language model quickly leads to severe accuracy degradation, from which it is often difficult to recover even with extensive finetuning. This is why current state-of-the-art approaches like Block Movement Pruning  or CoFi  opt for integrating pruning directly into training (via sampling or differentiable approximations), rather than performing it in the standard gradual pruning fashion of discrete steps with finetuning in between. However, as we will show, by designing a new highly accurate pruning algorithm which is able to account for both local correlations of structures within single layers as well as global correlations across layers, we can actually apply the gradual pruning paradigm, with all its advantages, to improve significantly over the current state-of-the-art.

### The ZipLM Structured Pruning Algorithm (Local Correlations)

Most existing structured pruning criteria  are based on one or two of the following assumptions about saliency: structures with lower (average) weight magnitude are easier to prune , structures with small input activations can be removed at little loss , and structures that are close to a linear combination of other structures are the most redundant . We will now show how all these aspects can be jointly considered in a principled manner via our new ZipLM technique.

**Problem formulation.** Our approach starts from the idea of applying structured compression layer-wise, in a way that allows the layer to preserve most of its output characteristics. This setup is popular in the post-training quantization and unstructured pruning literature , and can be implemented as follows. We are given a small amount of calibration data, which we run through the network, to obtain "reference" inputs and outputs for each layer. Then, for each layer, given the calibration inputs \(\) and the original layer weights \(\), we aim to find compressed weights \(}\) respecting the compression constraint \(\), which best approximate the original output, measured via the squared error metric. If we assume that the input and weight matrices have an appropriate rectangular form, the problem can be formalized as:

\[_{}}\;||}-||_{2}^{2}}. \]

This objective can be decomposed across the rows of \(\), leading to a set of sparse linear regression problems, one per row. These row-wise problems are independent, which forms the basis of related work ; yet, since we do _structured_ pruning, they become dependent, as we would like to prune the same weight indices _across all rows_, i.e. prune entire columns. Thus, finding the optimal weights \(}\) is equivalent to finding: 1) the optimal structure \(\) of the desired shape to be removed, which we assume to be applied across all rows, with corresponding pruning mask \(}\), where pruned indices have value \(1\) in the mask, and others are \(0\); and 2) the corresponding update \(}\) to all of the remaining weights, optimally compensating for the error caused by the removal of weights in \(\).

**Saliency scores and weight update.** Let \(=^{}\) be the Hessian matrix for the \(_{2}\)-minimization problem in Equation 1, which is independent of the weights. Define \(_{i,}}\) to be the subset of weights under the mask \(}\) in row \(i\), and by \((^{-1})_{},}}\) the submatrix of the inverse Hessian corresponding to the entries under the mask \(}\). Then, we can obtain the optimal mask and weight update as follows:

\[_{}\;_{i=0}^{d_{}}_{i,}}((^{-1})_{},}} )^{-1}_{i,}}^{} \]

\[}=-_{:,}}(( ^{-1})_{},}})^{-1}( ^{-1})_{},:} \]

We obtain this by extending the Optimal Brain Surgeon  formulas for solving Equation 1 to cover all \(d_{}\) weight matrix rows simultaneously. Importantly, the subselection of the inverseHessian \(((^{-1})_{},}})^{-1}\) is shared between all rows. Further, since we generally consider only non-overlapping sets \(\) of the same size, we pay just \(O(d_{}|}|^{2})\) total cost for all extra inversions. Since the number of structures in the mask \(|}|\) is usually small, e.g. attention heads usually consist of 64 columns, the overall cost of these inversions is low.

Simply selecting the structures to prune according to the criterion in Equation 2 unifies the weight magnitude and activation influence criteria (via the Hessian), but still ignores any correlations between structures. We address this by pruning structures _one-at-a-time_, while always applying update \(}\) and fully recomputing \(^{-1}\) relative to the remaining structures. For example, if there exist two redundant structures \(S_{1}\) and \(S_{2}\), we will first drop \(S_{1}\) and update \(S_{2}\) to compensate for this removal, at which point \(S_{2}\) is no longer easy to prune. Without this one-at-a-time removal, both structures would have been incorrectly removed as they each individually seem easy to prune according to Equation 2. Executing this strategy naively will require a full \(O(d_{}^{3})\) recomputation of the inverse Hessian relative to the remaining structures at each step, which would be very slow. However, this can be avoided by removing the rows and columns corresponding to \(}\) directly in the inverse with one step of Gaussian elimination , applied block-wise to cover larger structures, as follows:

\[^{-1}-^{-1}_{:,}}((^{-1} )_{},}})^{-1}^{-1}_{},:}, \]

which takes only \(O(|}| d_{}^{2})\) time. We provide complete pseudocode in Algorithm 1.

```
\(\) for\(k\) times do \(_{}_{i=0}^{d_{}}_{i,}}((^{-1})_{},}})^{ -1}^{}_{i,}}\) \(}-_{:,}}((^{-1})_{},}})^{-1}(^{-1})_{ },:}\) \(+}\) \(^{-1}^{-1}-^{-1}_{:,}} ((^{-1})_{},}})^{-1}^ {-1}_{},:}\) \(-\{\}\) endfor \(}\)
```

**Algorithm 1** The ZipLM pruning algorithm. Given inverse Hessian \(^{-1}=(2^{}+)^{-1}\), we remove exactly \(k\) structures from the corresponding weight matrix \(\).

#### 3.2.1 Pruned structures.

Focusing on Transformers, we consider three types of structural removal: dropping attention heads, shrinking the expanded intermediate dimension of the fully-connected network (FC) layers, and removing entire residual parts, i.e. attention or FC-modules. We implement this by dropping \(d_{}\) consecutive columns in the out-matrix of the attention block and individual columns in the second linear layer of the feed-forward network. Once these column-structures are zeroed out, corresponding rows in previous layers can be safely removed without any output change. Crucially, by pruning e.g. columns in the FC2 layer rather than equivalent rows in FC1, we can utilize the input correlations via Hessian-information using the ZipLM pruner.

#### 3.2.2 Novelty relative to existing Optimal Brain Surgeon (OBS) approaches.

The original framework , as well as modern efficient versions [47; 7; 8], have been explicitly developed for _unstructured pruning_, i.e. removing individual weights. It is nontrivial to extend them to structured pruning, as this involves considering additional correlations, both within as well as across multiple blocks (such blocks are usually employed for computational tractability). For example, the state-of-the-art layer-wise approach of , performs unstructured pruning by handling weight matrix rows separately, and then greedily merging results. In contrast, we perform structured pruning _jointly_ across multiple rows, which is not only necessary for correctness but additionally enables us to design an algorithm with a computational complexity that is lower by a full factor of the hidden dimension size. Additionally, structured pruning requires explicitly matching matrix shapes for consecutive layers and a dedicated strategy for utilizing weight updates even when entire blocks/rows are pruned.

### Inference-Aware Structured Pruning (Global Correlations)

We now describe how to augment the algorithm to be _inference-aware_, in the sense that it accepts inference specifications, such as batch-size, sequence-length, and speedup on the target hardware, as additional inputs to optimize for.

**Motivation.** The main benefit of inference-aware structured pruning is the fact that pruning decisions are not guided purely by saliency scores, but instead by loss-vs-speedup trade-offs associated with the removal of each component in the model. Prior methods, e.g. [24; 27; 59] focus solely on pruning until a specific sparsity threshold is reached, without taking into account the real-world speedups corresponding to the compression threshold, which can vary significantly between settings. For example, a 95% sparse BERT produced by CoFi  has 12x speedup on a V100 GPU, but only 5x on an A100 GPU. With existing methods, if real-world timings fail to meet the inference requirements, the entire process has to be repeated with different sparsity values until the target speedup is achieved, which is both time-consuming and error-prone. An additional advantage of inference-awareness, which we showcase in our GPT experiments in Section 4, is that it enables optimizing for different real-world metrics, such as latency or throughput.

**Runtime awareness.** We integrate runtime constraints via a latency table  for our target inference environment, where we record the time to run an attention block, including all overheads, with \(0,,N_{}-1\) heads pruned and similarly for the fully-connected block with the intermediate dimension shrunk by a factor of \(0.9^{i}\), for \(i=0,,42\); in relative steps of \(10\%\) up until \( 99\%\) sparsity, following . This allows rapid runtime estimation for different _per-layer sparsity configurations_. We provide an example of our latency table in Appendix E.

**Finding the optimal sparsity configuration.** Ultimately, our goal is to find a per-layer-sparsity configuration that satisfies a certain speedup-constraint while maximizing accuracy. A popular paradigm of doing this [15; 30] is to produce a large number of pruned models with different sparsity distributions across layers and then select the one, satisfying a target constraint, with the highest accuracy. To make this computationally feasible, it is crucial that pruning is cheap, yet accurate. ZipLM treats each layer independently, which makes it possible to precompute a database of several pruned versions with different sparsities for each layer. The entire database can be produced in a single run, utilizing the algorithm's one-at-a-time nature. While our algorithm is compatible with various search methods for finding layer-wise profiles [15; 12], we adapt the recent SPDY approach .

**Structured SPDY search.** The SPDY approach is designed for unstructured pruning and assigns a quadratic prior to per-layer sensitivity of different sparsity levels. This is not valid in our structured pruning scenario, since for instance it would suggest that dropping a full layer is only slightly more difficult than pruning it to 99% sparsity. Thus, using standard SPDY would lead the algorithm to explore a large number of sub-optimal configurations, significantly wasting computational resources. To alleviate this problem, for a structured sparsity \(s\), we introduce a better prior \(p_{s}\) as the relative layer-wise squared error incurred by pruning, defined as \(p_{s}=||}_{}-||_{2}/|| ||_{2}\), which simply has a value of 1 for a fully dropped layer. Furthermore, the original SPDY approach uses shrinking neighborhood search, which has high variance in both runtime and solution quality for structured compression. Therefore, we perform a fixed number of \(1000\) steps, randomly mutating _in expectation_ 10% of the layer-wise sensitivity coefficients. Finally, we note that any candidate evaluated by this procedure actually achieves the target speedup, leading to significantly decreased search time. We validate our approach in Appendix F, where we demonstrate that our speedup estimations are indeed very accurate in practice. Specifically, real-world on-device measurements deviate at most by 5.28% from their expected values.

### Layer-wise Token Distillation

For structured pruning, it is common to apply _layer-wise distillation_ objectives to transfer intermediate representations. However, structured pruning creates compatibility issues relative to the fixed teacher architecture, leading most methods to develop customized distillation strategies. A popular approach, introduced in  and improved by , solves the problem via static  or dynamic  mapping of a subset of teacher layers to a subset of student layers. Their main limitation is manual layer selection, where making the "optimal" choice would require evaluating all possible combinations, which can be very expensive. Another limitation is shape-matching between intermediate layers, which is solved by introducing a learnable linear transformation matrix attached to student outputs.

**Our approach.** We address these challenges differently, by leveraging the fact that ZipLM preserves the hidden dimension size, and propose to use distillation of intermediate token representations across the entire model. The resulting minimization objective consists of three components:

\[(^{},^{}|x)=_{1}_{ }(^{}|x)\,+\,_{2}_{}(^{},^{}|x)+_{3}_{ }(^{},^{}|x), \]where \(^{}\) and \(^{}\) represent student and teacher models respectively, \(x\) are the inputs, \(_{}\) is the loss associated with the task (e.g. cross-entropy for text-classification), \(_{}\) is the KL-divergence between output logits as described in , and \(_{}\) is our token-level distillation loss. Hidden tensors passed between consecutive transformer layers are of constant shape \(^{B seq H}\), where \(B\) stands for the batch-size, \(seq\) for the sequence length, and \(H\) for the hidden size defined by the model architecture. This tensor can be interpreted as a collection of \(B seq\) vectors \(^{H}\), each carrying intermediate model representations of input tokens \(x\). We define the loss \(_{}\) as an Euclidean distance \(\) between vectors \(\) corresponding to each non-padded token in the input sequence, averaged over all unpruned layers. Formally, for a layer \(k\), it is defined as

\[_{}^{k}=^{B seq} [j]}_{j=1}^{B seq}[j ](^{_{}},^{_{ }}), \]

where \(\) stands for the set of padding tokens. This formulation encourages the student model to generate vector representations for each token that are similar to those produced by the teacher model. In Appendix B, we present ablation studies and comparisons for ZipLM and CoFi, with and without their respective distillation objectives.

## 4 Experiments

**Setup.** Given a pre-trained model, a dataset, and a set of desired speedups in a target inference environment, we iteratively fine-tune and prune the model in a structured way such that in the end we obtain a set of accurate compressed models, one for each speedup target. We consider pruning of the standard BERTbase and BERTlarge architectures, evaluating on dev-sets of established benchmarks: SQuADv1.1 , and a subset of GLUE  tasks: SST-2 , QNLI , MNLI , and QQP , selected to match publicly-available checkpoints from prior work. For a precise comparison to prior work , our inference environment is a single NVIDIA V100 16GB GPU, batch size of 128, and sequence lengths of 384 and 128 for SQuAD and GLUE tasks, respectively. In addition to encoder-based BERT models, we also consider pruning of the decoder-based GPT2 model on the OpenWebTextCorpus , for which we consider two inference environments: pruning for throughput (batch-size=16, sequence-length=1024), and pruning for latency (batch-size=1, a set of prompts with varying lengths). For illustration, our pipeline is depicted in Figure 1. In Appendix H and I, we report exact values for all results, as well as hyper-parameters for reproducibility.

**Baselines.** In the _gradual pruning_ setting, we explore the performance of ZipLM pruning of BERT- and GPT2-family models, across a wide range of inference speedup targets, ranging from 2x to 15x, in unit increments. This allows us to compare the effectiveness of our approach against a diverse set of structured pruning and distillation-based techniques, including state-of-the-art CoFi pruning, competitive Block Movement Pruning, and distillation approaches including TinyBERT, DistilBERT,

Figure 1: Illustration of the ZipLM pipeline: 1) inference specifications, 2) runtime benchmarking of candidates for pruning, 3) gradual structured pruning until all speedup targets are met.

DistilGPT2, MobileBERT, MiniLM, and DynaBERT. Additionally, we include comparisons with other relevant methods. For fairness, we follow  and report TinyBERT and DynaBERT results without data augmentations. In the _post-training/one-shot_ setting, which does not allow retraining, we demonstrate that ZipLM outperforms the prior state-of-the-art approach of . We evaluate inference speedups of all models in the same environment, unless the models are not publicly available, in which case we report speedups from their respective papers. We refer to ZipLM compressed BERT models as ZipBERT, and to ZipLM compressed GPT2 models as ZipGPT2.

### Gradual Structured Pruning

**BERTbase results.** In Figure 2 we compare structured compression methods on the SQuADv1.1 task. ZipLM outperforms both CoFi and TinyBERT, prior state-of-the-art techniques, by 3 points in the F1 score at the same speedup factor, while at the same F1 score it is able to improve inference speedups by at least 60%. In Figure 3, we extend this comparison to a subset of GLUE tasks and provide an exhaustive overview of various structured compression techniques. Results on the other four remaining GLUE tasks are provided in Appendix Figure 7. As can be observed, distillation-based methods usually provide either one or a few structurally-compressed models, due to the massive costs associated with training from scratch for each new model. Relative to the most competitive approaches, such as TinyBERT, CoFi, and MiniLM, ZipLM provides consistent improvements in terms of both, accuracy and speedup, while providing guarantees for each compressed model in terms of the expected speedup in the target inference environment. Interestingly, on tasks like QQP and SST-2, ZipLM is able to compress the BERTbase model up to 6x and 10x speedups, respectively, while maintaining the accuracy of the _dense_ model. In Appendix D, we provide additional comparisons against CoFi on test-set results from the official GLUE evaluation server.

**BERTlarge results.** To verify that our approach does not pertain only to the BERTbase model, we apply ZipLM structured pruning to the 3x larger BERTlarge model on the SQuADv1 task. In this setup, we compare against the only two approaches that attempted to structurally compress this larger model, Block Movement Pruning and distillation-based MobileBERT. As can be seen in Figure 2, ZipLM is able to compress BERTlarge up to 4x faster inference while maintaining the F1 score of the uncompressed model. At the same F1 score as the fastest Block Movement Pruning model (3x), ZipLM doubles the inference speedup (6x). A result worth emphasizing is that ZipLM is even able to match the performance of the highly optimized MobileBERT model by simply compressing the baseline BERT architecture, without the many additional optimizations and custom components

Figure 3: Structured compression of BERTbase on QNLI, MNLI, SST-2, and QQP tasks. Dashed horizontal lines represent full and 99% accuracy recovery of the uncompressed model.

Figure 2: Structured compression of BERTbase (left) and BERTlarge (right) on the SQuADv1.1 task. Dashed horizontal lines represent full and 99% accuracy recovery of the uncompressed model.

used by MobileBERT. Specifically, some of the module- and operator-level optimizations used by MobileBERT include: bottleneck structures and carefully-balanced self-attention and feed-forward modules, embedding layer factorization, a bespoke closed-source teacher model, replacement of LayerNorm layers with lower-latency NoNorm layers, and replacement of GELU activation functions with ReLU activations.

**99% recovery.** The MLPerf Benchmark  targets recovery of \(>\)99% of the baseline accuracy. At this industry-defined threshold, ZipLM models set new state-of-the-art performance across all of the considered datasets with the following BERTbase inference speedups: 5x on the SQuADv1 task, 6x on QNLI and MNLI, and, surprisingly, 13x and 15x on SST-2 and QQP, respectively. When compressing BERTlarge on the SQuADv1 task, ZipLM produces a 6x faster model at 99% recovery.

**GPT2 results.** To validate that our approach does not only apply to encoder-based models, we apply ZipLM structured pruning to the decoder-based GPT2 model. In addition to this, to further demonstrate the inference-awareness property of our approach and its importance for real-world applications, we consider two different regimes: pruning for throughput and pruning for latency. An example application for the former regime is a server-side deployment where the model processes many queries at the same time, while an application for the latter regime is a text-generation scenario where the model is used in an online fashion to auto-complete user's text.

For a fair comparison, we follow the DistilGPT2 setup  and prune the 124M parameters GPT2 variant on the OpenWebTextCorpus dataset, followed by _zero-shot_ evaluations, without any fine-tuning, on the test-split of the WikiText  dataset. Because of the enormous vocabulary size, the maximum achievable speedup in the throughput regime for this model is roughly 3.5x. Thus, we run ZipLM pruning to 1.5x, 2x, 2.5x, and 3x speedup targets. For the latency regime, we report the median time to process sequences of various lengths when generating text with Top-K sampling . In Table 1, we present zero-shot evaluations of the uncompressed GPT2 model which serves as a baseline relative to the competing DistilGPT2 approach, and four variants of our ZipLM pruned GPT2. In the pruning for throughput scenario, at similar speedup and decoder size (1.6x-vs-1.5x and 42.5M-vs-47.3M), ZipGPT2 achieves significantly lower perplexities relative to DistilGPT2. Further, at slightly better (lower) perplexities, ZipGPT2 reduces the decoder size from 42.5M to only 26.5M parameters (60% reduction) and improves speedup from 1.6x to 2.1x (30% faster). In the pruning for latency scenario, at a similar speedup of 1.9x-vs-2.0x, ZipGPT2 reduces the decoder size by 3M params while providing almost 2 points improvement in the zero-shot perplexity.

### On the Importance of Inference-Awareness

**Depth vs. width pruning.** A particularly interesting illustration of the importance of inference-awareness in the pruning algorithm is given by our GPT2 models running directly in the PyTorch-HuggingFace framework, which can be used in two different modes: batch-prediction (throughput-constrained) and text-generation (latency-constrained). For the former, inputs are typically large, and shrinking weight matrices is an effective way to achieve speedups. However, for the latter, the inputs are much smaller, and the size of weight matrices is no longer the primary bottleneck.

    &  &  \\ Model &  &  Decoder \\ size \\  &  Wiki \\ Text-103 \\  & Speedup &  Decoder \\ size \\  & 
 Wiki \\ Text-103 \\  \\  GPT2* & 1.0x & 85.0M & 28.5 & 1.0x & 85.0M & 28.5 \\  DistilGPT2 & **1.6x** & **42.5M** & **43.0** & **1.9x** & **42.5M** & **43.0** \\   & **1.5x** & **47.3M** & **35.4** & **1.6x** & **48.7M** & **37.8** \\ ZipGPT2 & **2.1x** & **26.5M** & **41.5** & **2.0x** & **39.2M** & **41.2** \\ (ours) & 2.7x & 14.0M & 50.4 & 2.2x & 26.6M & 49.0 \\  & 3.3x & 5.7M & 72.1 & 2.5x & 20.7M & 55.0 \\   

Table 1: Zero-Shot perplexity (PPL) of compressed GPT2 in two regimes: pruning for throughput and pruning for latency. *GPT2 was trained by OpenAI  on a much larger closed-source dataset and for significantly longer. The only direct comparison is between DistilGPT2 and ZipGPT2.

    &  &  \\ Model &  &  Decoder \\ size \\  &  Wiki \\ Text-103 \\  & Speedup &  Decoder \\ size \\  & 
 Wiki \\ Text-103 \\  \\  GPT2* & 1.0x & 85.0M & 28.5 & 1.0x & 85.0M & 28.5 \\  DistilGPT2 & **1.6x** & **42.5M** & **43.0** & **1.9x** & **42.5M** & **43.0** \\   & **1.5x** & **47.3M** & **35.4** & **1.6x** & **48.7M** & **37.8** \\ ZipGPT2 & **2.1x** & **26.5M** & **41.5** & **2.0x** & **39.2M** & **41.2** \\ (ours) & 2.7x & 14.0M & 50.4 & 2.2x & 26.6M & 49.0 \\  & 3.3x & 5.7M & 72.1 & 2.5x & 20.7M & 55.0 \\   

Table 2: One-shot (post-training) structured pruning of BERTbase on three downstream datasets and two speedup targets.

In this scenario, the only way to achieve substantial speedups is to completely drop some modules, which prior methods cannot account for as they solely optimize for overall model sparsity. However, with ZipLM, runtime measurements from the target inference environment guide pruning decisions, allowing it to learn the best way to compress the model for an optimal speedup-accuracy trade-off. Our GPT2 compression results in Table 1 clearly illustrate and support these statements. Even though pruned for the same speedup target, the final architectures of ZipGPT2 models are drastically different. For the throughput-constrained scenario, the model's depth was preserved but the matrix dimensions were significantly reduced (roughly by a factor of 10) making the corresponding multiplications with large input tensors much faster. In contrast, for the latency-constrained scenario, the model's width (shapes of weight matrices) was mostly preserved but the depth was shrunk almost by a factor of 4, making the forward pass with small inputs faster by reducing the effective number of modules.

**Inference device capabilities.** Incorporating capabilities of the inference device is another important aspect for effective structured pruning which prior methods do not account for as they solely optimize for higher sparsities. As noted in Section 3.2, this reflects in larges discrepancies between speedups obtained on different devices, e.g. a compressed model with 12x speedup on a V100 is only 5x faster on an A100 GPU. This arises because the A100 GPU is significantly more powerful and thus faster on the dense model; at the same time, it is highly underutilized for small matrices, which significantly limits the speedups for very high sparsity. To illustrate this, we have measured the speedup from reducing the MLP size for both GPU types (see Table 3). As can be seen, pruning to \(\)90% sparsity (3072 \(\) 302) gives \(\)7x speedup on a V100 but only \(\)3x speedup on an A100. Such differences are automatically captured by ZipLM, where pruning for sparsity is replaced by pruning for speedup.

**Pruning for speedup vs. pruning for sparsity.** In Figure 4 we compare results with ZipLM pruning when the target for pruning is sparsity (like prior approaches) and when the target for pruning is speedup (the ZipLM approach). Pruning for speedup brings significant improvements, up to 10 points, especially at higher speedups where inference-awareness is very important as the algorithm does not remove components that do not bring any further speed and therefore helps preserving accuracy.

### Post-training/One-shot Structured Pruning

We now study the performance of ZipLM when applied purely in _one-shot_, without any retraining. In this setting, we compare against the state-of-the-art method of Kwon et al.  which combines several heuristics: Fisher-based mask search, mask rearrangement, and mask tuning. Instead of heuristics, our pruning framework utilizes direct end-to-end loss information to find the optimal sparsity configuration. During the warm-start phase,  utilizes a diagonal Fisher matrix to estimate the significance of heads and filters, which discards correlations caused by off-diagonal elements. Although the approach attempts to address this limitation by approximating correlations within a single layer, it will not capture global dependencies. Furthermore, the weights are adapted for layer-wise reconstruction at the very end of the compression step, whereas our method does it continuously during the pruning (please see Section 4 for the significance of doing this). For a fair comparison, we apply the authors' own implementation in latency-constrained mode on the exact same model weights. Table 2 presents results on several datasets and speedups, showing that ZipLM is even more accurate than the approach designed and optimized specifically for the post-training/one-shot pruning.

Figure 4: Ablation study for the impact of the pruning target: pruning for sparsity (like prior approaches) versus pruning for speedup (the ZipLM approach).

    &  \\ MLP size & V100 & A100 \\ 
3072 & 1.0x & 1.0x \\
1814 & 1.6x & 1.1x \\
1322 & 2.0x & 1.4x \\
302 & 6.9x & 3.1x \\
130 & 11.8x & 4.4x \\
76 & 13.1x & 4.4x \\
33 & 14.8x & 4.4x \\   

Table 3: Speedups from shrinking the intermediate size of MLPs in the FFN section of a Transformer layer, on different GPUs.

**Sensitivity to calibration data.** Additionally, we have found that ZipLM is very robust to the amount of calibration data. In Table 4 we present a sensitivity analysis with respect to the number of calibration samples. We one-shot prune BERTbase on the SQuADv1.1 task for two speedup targets: 1.5x and 2.0x. In this setup, we compare results against Kwon et al. , which uses 2048 samples by default. As can be seen from the table, ZipLM outperforms prior state-of-the-art starting at only 32 samples. As we increase the number of samples, the results improve, up to 2 points in F1 score.

## 5 Discussion and Extensions

**CPU as an LLM-inference environment.** In Section 4 we have focused on various GPU-based inference environments as it enabled us to conduct fair comparisons against prior structural compression techniques. However, CPUs present another compelling inference environment focused on edge deployment of LLMs. Therefore, we target the recently proposed compound compression pipeline of , which involves three steps: structured pruning, unstructured pruning, and quantization. We replace their structured pruning approach based on layer dropping with ZipLM. As a result, at full accuracy recovery, we are able to improve speedup from 3x to 13x, and at the largest compression ratio from 30x to 50x. Due to space constraints, we provide full results in Appendix A.

**Computational efficiency.** Relative to distillation-based methods, structured pruning is an order of magnitude more efficient in terms of GPU hours due to the massive costs associated with pretraining from scratch for each compressed model [59; 51; 21]. For efficiency comparisons to CoFi, we consider the task of producing a full family of compressed BERTbase models with speedup targets ranging from 2x to 15x. In this setup, ZipLM requires only 115 epochs in total, whereas CoFi would require 560 epochs. Therefore, ZipLM is _4.87 times more efficient_ than CoFi. In terms of end-to-end runtime, ZipLM produces the entire family of compressed BERTbase models on a single RTX A6000 GPU in \(\)35 hours on larger datasets (e.g. MNLI) and only \(\)10 hours on smaller ones (e.g. SST2). Finally, it is worth emphasizing that we have not taken into account the cost of hyper-parameter tuning in the above comparisons, but that this is very favorable to ZipLM: it uses a single set of hyper-parameters to produce an entire family of compressed models while other methods require hyper-parameter tuning for each model independently.

**Scaling laws for structured pruning.** To further understand the accuracy-speedup trade-offs, we run ZipLM on larger speedup ratios, up to **55x for BERTlarge** and **75x for BERTbase**. To the best of our knowledge, this is the first result in literature demonstrating that such extreme compression ratios are achievable with structured pruning without model collapse. In Figure 5, we compare these results against distillation-based downscaling of the BERT architecture . The results clearly demonstrate that each of the pruned models, based either on BERTlarge or BERTbase, significantly outperforms comparable pre-trained variants. An emergent behavior that can be observed is that structurally pruned models tend to follow a linear scaling law, meaning that the accuracy decreases linearly with the increase of the speedup ratio, at a slope given by the original model. Fitting linearly via least squares produces the following expressions for the accuracy-speedup relationship: \(_{} 92.1-0.3_{}\), and \(_{} 90.3-0.6_{}\). Thus, the rate of decrease in accuracy for BERTbase is twice as large as that of BERTlarge, which can be attributed to the presence of more redundant representations in the larger model, making it more resilient to pruning. In Appendix G we provide additional analysis of the structure of pruned models.

    &  &  \\  &  & 1.5x & 2.0x \\   & 4 & 82.3 & 48.4 \\  & 32 & **86.8** & **82.6** \\  & 128 & **86.8** & **83.6** \\  & 512 & **86.8** & **84.1** \\  & 2048 & **87.1** & **84.1** \\  & 4096 & **87.6** & **84.7** \\  Kwon et al. & 2048 & 86.2 & 76.5 \\   

Table 4: Sensitivity to the number of calibration samples.

Figure 5: Scaling laws of structured pruning vs. distillation on the standard BERT architecture.