# Instruction Embedding: Latent Representations of Instructions Towards Task Identification

Yiwei Li\({}^{1}\), Jiayi Shi\({}^{1}\), Shaoxiong Feng\({}^{2}\), Peiwen Yuan\({}^{1}\), Xinglin Wang\({}^{1}\), Boyuan Pan\({}^{2}\), Heda Wang\({}^{2}\), Yao Hu\({}^{2}\), Kan Li\({}^{1}\)

\({}^{1}\) School of Computer Science, Beijing Institute of Technology

\({}^{2}\) Xiaohongshu Inc

Equal contributions.Corresponding author.

###### Abstract

Instruction data is crucial for improving the capability of Large Language Models (LLMs) to align with human-level performance. Recent research LIMA demonstrates that alignment is essentially a process where the model adapts instructions' interaction style or format to solve various tasks, leveraging pre-trained knowledge and skills. Therefore, for instructional data, the most important aspect is the task it represents, rather than the specific semantics and knowledge information. The latent representations of instructions play roles for some instruction-related tasks like data selection and demonstrations retrieval. However, they are always derived from text embeddings, encompass overall semantic information that influences the representation of task categories. In this work, we introduce a new concept, instruction embedding, and construct **I**nstruction **E**mbedding **B**enchmark (**IEB**) for its training and evaluation. Then, we propose a baseline **P**rompt-based **I**nstruction **E**mbedding (**PIE**) method to make the representations more attention on tasks. The evaluation of PIE, alongside other embedding methods on IEB with two designed tasks, demonstrates its superior performance in accurately identifying task categories. Moreover, the application of instruction embeddings in four downstream tasks showcases its effectiveness and suitability for instruction-related tasks1.

## 1 Introduction

Large Language Models (LLMs) have demonstrated remarkable proficiency in generating responses capable of addressing specific tasks according to provided instructions. Initially pre-trained for wide-ranging capabilities, they are subsequently fine-tuned using instruction-following datasets to enhance their ability to align with human preferences. LIMA has proved that alignment can be viewed as a straightforward process in which the model just learns the style or format for interacting with users to solve particular problems, where the knowledge and capabilities have already been acquired during pre-training (Zhou et al., 2023).

Text embeddings play a crucial role in a variety of NLP tasks such as semantic textual similarity (Agirre et al., 2012; Cer et al., 2017; Marelli et al., 2014) and information retrieval (Mitra et al., 2017; Karpukhin et al., 2020). Similarly, as a type of text, the latent represent of instructions is also essential for many tasks like data selection for instruction tuning (Wu et al., 2023) and prompt retrieval for in-context learning (Su et al., 2023). Previous studies (Gao et al., 2021; Wang et al., 2024) obtain text embeddings by directly taking the token vector from language models. However, when it comesto the embeddings of instructions, the key focus should lie in identifying task categories rather than capturing overall semantic information. This is because, as mentioned earlier, instruction fine-tuning helps models learn how to interact with users across different tasks, rather than specific capabilities and knowledge imparted by the instructions. Therefore, task similarities is far more important than semantic similarities for instructions. Figure 1 (a) shows the case where traditional text embedding methods exhibit high overall semantic and syntactic similarity between two samples which actually represent completely different tasks, but low similarity when they represent similar task.

In this work, we propose a new concept called instruction embedding, a specialized subset of text embedding that prioritizes task identification for instructions over the extraction of sentence-level semantic information. We construct a new benchmark for instruction embedding training and evaluation. Different from previous text embedding benchmark that only considered the semantic textual similarity, IEB is labeled by task categories of instructions. Inspired by that key instruction words especially verbs are highlighted through instruction tuning (Wu et al., 2023b), we first extract verb-noun pairs to clarify category, then manually select and label instructions with other syntactic structures. Besides, we also conduct category merging and employ GPT-4 to generate complex samples to make the benchmark more robust. IEB totally contains 47k samples dispersed across more than 1k categories, which can also be used for embedding training and downstream tasks.

To stimulate language models to generate better instruction embedding, we propose a prompt-based baseline method PIE. It leverages the template to obtain instruction embeddings by directing the model's attention towards the task type represented by the instructions. Despite PIE demonstrating good practicality as it already performs well without training, we can further enhance it by fine-tuning the model on IEB with contrastive learning. As a widely used method for training embedding models, contrastive learning requires positive and hard negative samples to provide training signals, which are hard to extract. In our study, the explicit category information available in IEB enables the straightforward extraction of positive samples by directly selecting two instructions from the same category. We can further construct hard negative samples by selecting samples from categories that share identical verbs or nouns, enhancing the challenge of differentiation. Figure 1 shows that PIE can effectively distinguish whether two instructions refer to the same task cluster.

We evaluate PIE and other embedding baselines on IEB with instruction clustering and intention similarity tasks, which shows that PIE can largely outperform other baselines and precisely identify the task categories. We also conduct four downstream tasks, where the superior results demonstrate that the proposed instruction embeddings are more suitable for instruction-related tasks than traditional text embeddings.

Figure 1: (a) Case about cosine similarity between instructions. Visualization of (b) text embeddings and (c) instruction embeddings. The same color indicates the same task category.

The IEB Benchmark

We present instruction embedding benchmark, IEB, for assessing the quality of the latent representation of instructions. In contrast to current text embedding benchmarks that assess semantic similarity, the primary focus for the space of instruction embeddings is task differentiation based on the given instructions. Therefore, we annotate instructions with their respective tasks in IEB. We define task as follows: a task of an instruction is a category of activities or work that we expect the LLM to perform, which can be represented by a key phrase (mostly verb-noun phrases). The definition of task is not influenced by specific content or knowledge. For example, "writing an article" is a task, but the specific topic of the article is not important.

### Data Extraction

For convenience and authenticity, we derive samples from established datasets. Specifically, we adopt three extensively recognized instruction-tuning datasets: DatalystsDolly (Conover et al., 2023), Alpaca data (Taori et al., 2023), and Self-instruct data (Wang et al., 2023). Labeling instructions entirely through manual effort or large language models will incur significant costs. Therefore, it is best to first conduct coarse-grained grouping and filtering based on rule-based policies. Wu et al. (2023) proves that instruction fine-tuning enables models to recognize key instruction words, which leads to the generation of high-quality responses. Furthermore, it also encourages models to learn word-word relations with instruction verbs. Inspired by these two findings, we argue that verbs and other key words are crucial in identifying the task denoted by an instruction, where the types of them can be effectively determined through syntactic analysis. Thus, we employ the Berkeley Neural Parser1(Kitaev and Klein, 2018; Kitaev et al., 2019) for parsing the instructions.

After manual observation and considering the task category requirements, instructions can generally be divided into the following four groups through corresponding parsing tag recognizer:

Vp (Vb+nn)denotes verb phrase structure where the verb is closest to the root of the parse tree and directly links to noun. Instructions with this structure account for more than 80% of the total number before filtering. We categorize each instruction based on its verb-noun combination, identifying it as a specific task type, such as _write story_ or _generate sentence_. After restoring the verb tense and singular form of nouns, we classify instructions with the same verb-noun combination into the same category. We plot the top most common root verbs and their direct noun objects in Figure 2.

Sbarqis direct question introduced by a who-word or a wh-phrase. It can be divided into two main categories: knowledge-based questions led by six interrogative pronouns (e.g., what, when, where,...) and math problems introduced by _what_. Unlike instructions in the VP (verb phrase) form, we define categories in the form of interrogative pronoun combing knowledge/math. This is because, considering they all involve asking about knowledge or math problems, further subdividing into noun categories is not very meaningful. For each category, we manually select around 50 samples.

Sqis inverted yes/no question. It can also be divided into two main categories: knowledge-based questions and task-oriented questions. Similarly, the task label is annotated as yes-no combing knowledge/task and we select around 50 samples for each category.

OthersThere are some other structures: verb phrase that lacks a direct connection to a noun and some rare cases which do not contain verbs, consisting only of noun phrases. We define these four

Figure 2: The verb-noun distributions in IEB.

[MISSING_PAGE_FAIL:4]

Human EvaluationWhile we have highlighted the quantity and diversity of the data in IEB, the quality remains uncertain. To assess this, we randomly select 100 task categories and choose one instance from each. An expert annotator, who is a co-author of this work, then evaluate whether each instruction belongs to its annotated category. The instruction for judgement is the same as Automatic Filtering. The results indicate that 93% of the sample categories are accurate, showing that most of the annotated category labels are correct.

### Statistics

After constructing and filtering, we collect totally 1353 task categories with 47161 samples. Given the large volume of data, the benchmark data can also be used for training and testing instruction embeddings and downstream tasks. Therefore, we have split it in a certain ratio, but it can be be adjusted freely as needed. The EFT (embedding fine-tuning) subset is designed to facilitate models in generating high-quality latent representations of instructions through embedding fine-tuning, which involves a supervised contrastive learning process based on our task labels (details on the embedding fine-tuning process can be found in Sections 3.2). The IFT (instruction fine-tuning) subset is constructed to evaluate the effectiveness of our instruction embeddings in downstream tasks, such as Data Selection for Instruction Tuning and Demonstration Retrieval (details available in Sections 4.3.1 and 4.3.2). Table 2 describes the statistics of the divided data. More statistics can be seen in Appendix A.4. Note that there is no overlap among the samples in the four parts, but the task categories in the training and test sets for IFT will overlap.

## 3 Instruction Embedding Method

Traditional text embeddings focus on capturing overall semantic information of text (Xu et al., 2023d). However, Zhou et al. (2023) and Wu et al. (2023b) demonstrate that the essence of instruction data lies in the tasks indicated by task words which are typically composed of a verb and a noun and specify the task action and the task domain (or object of action) respectively. Therefore, we propose instruction embedding method to capture task category information contained in instructions, rather than general semantic information.

### Prompt-based Instruction Embedding

As mentioned above, guiding the model to generate embeddings that focus on task categories is critically important. LLMs have shown an impressive capacity to accomplish novel tasks solely by utilizing in-context demonstrations or instructions (Brown et al., 2020). Inspired by PromptBERT(Jiang et al., 2022), we present a prompt-based instruction embedding method (PIE) that employs a carefully designed prompt to guide the model in extracting the tasks embedded within given instructions. The hidden states of last input token will be represented for the embedding of instruction. The PIE-prompt is shown in Figure 15. Besides, a Semantic-prompt as shown in Figure 16 is also applied to model for comparison.

    & & Tasks & Samples \\   & Train & 608 & 20814 \\  & Test & 145 & 3291 \\   & Train & 600 & 21720 \\  & Test & 938 & 1336 \\   & 1353 & 47161 \\   

Table 2: Data statistics of IEB. EFT refers to embedding fine-tuning and IFT refers to instruction fine-tuning.

### Embedding Fine-tuning

We further fine-tune PIE-model on EFT-train set following the contrastive learning (CL) framework in SimCSE (Gao et al., 2021), where we replace the dropout-based positive sample pairs construction method with a method based on instruction task labels from EFT-train.

Formally, let \(=\{_{i}\}_{i=1}^{||}\) denotes EFT-train, where each \(_{i}=\{t_{i1},...,t_{|_{i}|}\}\) represents a specific task category in \(\), and each \(t_{ij}\) is an instruction instance from \(_{i}\). During training, we take a cross-entropy objective with in-batch negatives (Chen et al., 2017; Henderson et al., 2017). For a given instruction \(t_{ij}\), we randomly sampled \(t_{ik}\) from \(_{i}\) where \(j k\) to make up a task-related instruction pair. Let \(h_{ij}\) and \(h_{ik}\) denote the embeddings of \(t_{ij}\) and \(t_{ik}\), the learning objective for (\(t_{ij}\), \(t_{ik}\)) with a mini-batch of N pairs can be formulated as Eq 1

\[_{i}=-log,h_{ik})}/}{_{m=1}^{N}e^{sim(h_{ij},h_{ mk^{}})/}} \]

where \(\) is the temperature hyperparameter and \(sim(h_{1},h_{2})\) is the cosine similarity \(^{T}h_{2}}{||h_{1}||||h_{2}||}\).

Hard negative sampling has been widely adopted in CL (Schroff et al., 2015). In this paper, we propose a hard negative sampling strategy based on verb-noun style instruction task labels: for an instruction \(t_{ij}\) whose task category is a verb-noun pair (\(v_{i},n_{i}\)), another instruction \(t_{i^{}j^{}}\) whose task category is either (\(v_{i},n_{i^{}}\)) or (\(v_{i^{}},n_{i}\)) is considered as a hard negative sample of \(t_{ij}\). When searching for hard negative samples, we prioritize samples with the same verb but different nouns.

   Method & ARI & CP & Homo & Silh & IIS-Spearman \\   \\  BERT & 0.3113 & 0.4853 & 0.6777 & 0.0792 & 0.5522 \\ BERT (semantic-prompt) & 0.2840 & 0.4524 & 0.6570 & 0.0936 & 0.5335 \\ BERT (PIE-prompt) & 0.2474 & 0.4038 & 0.6210 & 0.0706 & 0.4724 \\ Llama & 0.1813 & 0.3151 & 0.5439 & 0.0995 & 0.1565 \\ Llama2 (semantic-prompt) & 0.4238 & 0.5947 & 0.7549 & 0.1298 & 0.5893 \\ Llama2 (PIE-prompt) & 0.4814 & 0.6305 & 0.8014 & 0.1611 & 0.7189 \\ Vicuna & 0.1198 & 0.2859 & 0.4828 & 0.0934 & 0.1211 \\ Vicuna (semantic-prompt) & 0.1871 & 0.3145 & 0.5133 & 0.1081 & 0.6934 \\ Vicuna (PIE-prompt) & 0.5305 & 0.6633 & 0.8242 & 0.1732 & 0.7534 \\   \\   & w/o prompt &  Llama2 \\ BERT \\  & 0.3306 & 0.4877 & 0.6891 & 0.2185 & 0.1714 \\  &  Llama \\ BERT \\  & 0.4741 & 0.6187 & 0.7741 & 0.1225 & 0.7460 \\    } & Llama2 & 0.1776 & 0.3087 & 0.5412 & 0.0818 & 0.1476 \\  &  LBERT \\  & 0.3371 & 0.5084 & 0.6974 & 0.1161 & 0.6804 \\   \\   & w/o prompt &  Llama2 \\ BERT \\  & 0.7541 & 0.8469 & 0.9143 & 0.3608 & 0.6038 \\  &  BERT \\  & 0.8837 & 0.9392 & 0.9695 & 0.4574 & 0.8436 \\   EFT-train \\  } & semantic-prompt &  Llama2 \\ BERT \\  & 0.8651 & 0.9204 & 0.9619 & 0.4542 & 0.8433 \\  &  BERT \\  & 0.8876 & 0.9377 & 0.9683 & 0.4946 & **0.8450** \\   & Llama2 & **0.9125** & 0.9432 & 0.9697 & 0.4803 & **0.8450** \\  & 
 BERT \\  & 0.8974 & **0.9453** & **0.9721** & **0.5180** & 0.8446 \\   

Table 3: Results of basic evaluation for instruction embedding. We conduct instruction clustering task and IIS test on each embedding method. Wiki refers to the train set of SimCSE (Xu et al., 2023c) and PromptBERT (Jiang et al., 2022), and semantic-prompt is shown in Figure 16.

Experiment

### Experimental Setup

Based on IEB benchmark, we introduce instruction clustering task (ICT) and instruction intention similarity (IIS) test to evaluate instruction embeddings. ICT aims to accurately group instructions from different tasks. Specifically, instruction clustering is conducted using k-means clustering based on embeddings of given instructions, where \(k\) is predefined and its value equals to the number of task categories in EFT-test (i.e. \(k=145\) here). We utilize metrics such as Adjusted Rand Index (ARI) (Hubert and Arabie, 1985), Clustering Purity (CP) (Schutze et al., 2008), Homogeneity Score (Homo) (Rosenberg and Hirschberg, 2007) and Silhouette Score (Silh) (Rousseeuw, 1987) for evaluation. IIS test is designed to align with STS (Agirre et al., 2012) task. The IIS test set is derived from IFT-train set. First, we randomly sample 1.5k instruction pairs of the same task from IFT-train set and label them as 1. Next, we sample another 1.5k pairs, labeling them as 1 if the task categories matched, otherwise 0. This resulted in a rough 1:1 ratio of samples labeled 1 to those labeled 03. During testing, we calculate cosine similarity of the instruction embeddings for each pair, and compute the Spearman value with the labels across the entire dataset.

We implement our PIE method with Llama2 (Touvron et al., 2023b) and BERT (Devlin et al., 2019) separately. For all BERT-based embedding methods, we take the hidden state of [CLS] token from the last layer as instruction embedding. For all Llama2, we first conduct preliminary experiments to select best pooling method and prompt. According to the results, we utilize the average of last token hidden states across last 2 layers as the instruction embedding and choose the prompt. Details of this preliminary experiment can be found in Appendix C.

We evaluate the instruction task representation capability of baseline models and compare their performance with our PIE and corresponding supervised fine-tuning method. The baselines are as follows:

**None-Fine-Tuned baselines** We employ Llama2, Vicuna-7b-v1.5 (Zheng et al., 2023) and BERT to obtain instruction embeddings with three prompts: no prompt, semantic-prompt, and PIE-prompt.

**Unsupervised Fine-Tuned baselines** Unsupervised SimCSE (Gao et al., 2021) and unsupervised PromptBERT (Jiang et al., 2022) are included as unsupervised fine-tuned baselines. To eliminate the impact of model scale, we also re-implement them with Llama2.

**Supervised Fine-Tuned baselines** We supervised fine-tune Llama2 and BERT as mentioned in Section 3.2. Detailed fine-tuning configurations can be found in Appendix D.

### Results and Analyses

Main FindingsThe experimental results are shown in Table 3. For none-fine-tuned baselines, our PIE-Prompt guides LLMs to extract task categories of instructions, enabling them to achieve significant improvements in both ICT and IIS test compared to the same model without using prompt. BERT failed to benefit from PIE-prompt, which may due to its limited instruction following capability. Interestingly, Vicuna achieves better results than Llama2 with PIE-prompt despite performing worse when prompt is not used. This is because Vicuna has been enhanced its instruction following capability through instruction tuning, enabling it to better extract task-specific information under the guidance of the PIE prompt. Although Llama2 and Vicuna achieve better performance in none-fine-tuning setting with PIE prompt, BERT successfully bridges this gap and achieves comparable or even better results after supervised fine-tuning on EFT-training. Additionally, for both Llama2 and BERT, although the performance gap between models using PIE-prompt and those using semantic-prompt or no prompt significantly narrows after supervised fine-tuning, models using PIE-prompt still outperform the others. This demonstrates that the guidance provided by PIE-prompt remains crucial even after supervised fine-tuning. To better illustrate the superiority of PIE and the impact of supervised fine-tuning, we visualize instruction embeddings of various models. The visualization analysis is presented in Appendix E.

Impact of Different PromptsTo better understand the impact of different prompts, we print the outputs of each model under various prompts. We find that without using prompts, Llama2 tends to repeat the instruction, while Vicuna which has undergone instruction fine-tuning, will execute the instruction. This explains why Llama2 outperforms Vicuna with no prompts since Llama2 retains more original instruction information in its output. When prompts are added, the models behavior are guided, enabling them to extract instruction information according to the prompt. However, when using semantic prompts, models focus more on analyzing instruction semantic information rather than task categories. Consequently, model performance with semantic prompts is not as good as those with PIE prompts. The model inference examples can be found in Appendix F.

Ablation StudiesWe conduct ablation studies on hard negative sampling strategy. We compare the performance of supervised fine-tuned models with and without hard negative sampling on embedding clustering task and IIS test, the results are shown in Figure 3. After removing hard negative sampling, the performance of models using different prompts all show a decline on embedding clustering task and IIS test. Our hard negatives are constructed through overlap of verb or noun, which helps eliminate the shortcut of distinguishing positives and negatives by word overlap. This allows the model to better focus on the relationship between instruction tasks of positive and negative samples during training.

### Evaluation on Downstream Tasks

We conduct four downstream tasks for further evaluation. Our core objective is to validate that instruction embeddings are more suitable for instruction-related downstream tasks compared to traditional text embeddings that focus on the overall semantic information of sentences. Therefore, we select the best-performing model we produced for each type of embedding, i.e., fine-tuned PIE-Llama2 and Wiki fine-tuned Llama2.

#### 4.3.1 Data Selection for Instruction Tuning

Following previous work (Wu et al., 2023; Zhou et al., 2023), we design a data selection experiment based on embeddings for instruction diversity. First, we use k-means clustering to divide the IFT-train set into 600 clusters, and extract the closest samples to the clustering centers to achieve data compression. Then, we fine-tune Llama2 on that selected data. Training configurations can be found in Appendix D. We evaluate the performance on our IFT-test set and AlpacaEval (Li et al., 2023). We use GPT-4 Turbo for judgment, and for IFT-test, its own output serves as the baseline for comparison. We take 5 runs for each setting and calculate the mean score. The result from Figure 4 (a) indicates

Figure 4: Results on (a) data selection for instruction tuning and (b) demonstrations retrieval for in-context learning.

Figure 3: Results of ablation studies.

that instruction embedding can be a better substitution of text embedding for enhancing the diversity of selected instructions. We additionally re-implement the data selection method DEITA with text embedding and instruction embedding separately, and the details can be found in Appendix K.

#### 4.3.2 Demonstrations Retrieval

LLMs have shown remarkable in-context learning (ICL) capability (Patel et al., 2023; Yuan et al., 2024). Demonstrations related to the input instruction task are more conducive to model since task-related data are more similar in terms of format and content. Thus in this experiment, we select 2 most related instruction data by calculating cosine similarities from IFT-train set for each instruction in test set. The prompt template of ICL can be found in Appendix G. Similarly, we report evaluation results on IFT-test set and AlpacaEval with four models: Vicuna-7B-v1.5, Llama2-7B-chat (Touvron et al., 2023b), Mistral-7B-Instruct-v0.2 (Jiang et al., 2023), LongChat-7B-v1.5-32k (Li et al., 2023a). For random selection, we take 10 runs and report the mean score. The results are shown in Figure 4 (b), which demonstrates instruction embedding helps to select more task-related demonstrations and makes better ICL for LLMs.

#### 4.3.3 Tiny Benchmark

Recently, some work has focused on testing models using fewer samples (Vivek et al., 2024; Polo et al., 2024). The primary goal is to select a more balanced tiny benchmark that can lead to more consistent performance compared to the original full benchmark. Similar to data selection for instruction tuning, this process can also be accomplished through clustering. We select 10, 50, and 100 test samples respectively, and compare the estimation error (%) in performance between the tiny and the original IFT-test benchmark. Following Vivek et al. (2024), we take 100 runs for each setting. The results in Table 4 indicates that instruction embedding can obtain a smaller estimation error by selecting more representative test samples.

#### 4.3.4 Dataset Task Correlation Analysis

We analyze the correlation degree between instruction tasks across various open-source datasets through instruction embedding. Let \(D_{1},D_{2}\) denote two unique instruction datasets, for each instruction \(t_{i}\) in \(D_{1}\), we find its most relevant instruction \(t^{}_{i^{}}\) in \(D_{2}\) and take the average of \(s_{i}\) (i.e. the similarity between \(t_{i}\) and \(t^{}_{i^{}}\)) across \(D_{1}\) (i.e. \(^{|D_{1}|}s_{i}}{|D_{i}|}\)) as a measure of the extent to which the tasks in \(D_{1}\) are encompassed in \(D_{2}\). We conduct task correlation analysis across GSM8k (Cobbe et al., 2021), MATH (Hendrycks et al., 2021), MBPP (Austin et al., 2021), Lima (Zhou et al., 2023), Dolly (Conover et al., 2023), OAssit (Kopf et al., 2023), Alpaca (Taori et al., 2023), WizardLM (WizardLM(Alpaca), WizardLM(ShareGPT)(Chiang et al., 2023a). As depicted in Figure 5, instruction embeddings succeed to distinguish between math tasks (GSM8k, MATH) and code tasks (MBPP). The correlation degree within math task datasets is significantly higher than the correlation degree between math task datasets and code dataset. Besides, larger and more general instruction datasets exhibit a more significant correlation with other datasets.

    &  &  &  \\  & 10 & 50 & 100 & 10 & 50 & 100 & 10 & 50 & 100 \\  Llama2-chat & 18.40 & 6.89 & **3.17\({}^{}\)** & 33.50 & **5.35** & 3.34 & **13.46** & 5.68 & 3.97 \\ Vicuna & **8.92\({}^{}\)** & **3.76\({}^{}\)** & **3.43\({}^{}\)** & 13.22 & 8.56 & 3.61 & 11.53 & 5.88 & 4.61 \\ Mistral & 7.92\({}^{}\) & **4.27\({}^{}\)** & **2.14\({}^{}\)** & **2.98** & 5.05 & 3.29 & 10.94 & 5.67 & 3.35 \\ Longchat & **7.76\({}^{}\)** & **4.69\({}^{}\)** & 3.70 & 28.82 & 4.74 & **3.47** & 12.07 & 6.11 & 4.22 \\   

Table 4: Results of tiny benchmark. \(\) denotes P-value \(<0.05\) and \({}^{}\) denotes \(<0.01\).

Figure 5: Correlation degree across various datasets through instruction embedding.

Related Work

Text EmbeddingsText embeddings are pivotal in NLP. They encapsulate overall semantic information and the quality of learned embeddings directly influences downstream tasks. Current research on text embeddings primarily focuses on sentence semantic modeling . We argue that the essence of instructions lies in their task information and instruction embeddings should prioritize modeling task-specific information instead of emphasizing overall semantic information.

Embedding BenchmarkSemantic Textual Similarity (STS) tasks  are commonly employed to evaluate the quality of text embeddings, complemented with transfer tasks and short text clustering tasks  to further illustrate the superiority of learned sentence representations. However, previous benchmarks are not tailored to instruction corpora and primarily assess the semantic modeling abilities of text embeddings, rendering them less suitable for evaluating instruction embeddings.

Instruction TuningInstruction Fine-Tuning (IFT) is widely adopted to stimulate the instruction following capability of pre-trained LLMs. Early approaches for IFT focused on fine-tuning LLMs with large amounts of instruction data  manually aggregated from large NLP task collections . With the development of generative language models, Wang et al. (2023) made their attempt to expand instruction data through synthetic data generation, inspiring the following works to evolve instruction data in this automated manner . Zhou et al. (2023) proved that the quality and diversity of instruction data are significantly more critical than its sheer quantity, motivating recent efforts in instruction data selection to remove unnecessary IFT training costs by eliminating low-quality and redundant data. Quality-based data selection methods typically employ a quality evaluator to predict the quality scores of each instruction sample which are further used to select instruction data Chen et al. (2023); Li et al. (2023). Diversity-based data selection methods aims to maximize the distance between selected instruction data which are measured by their embeddings Wu et al. (2023); Liu et al. (2024). However, due to the lack of instruction embedding, previous works relied on semantic embedding which fails to emphasize the task-specific information of instructions data.

## 6 Conclusion

We introduce the concept of instruction embedding, which prioritizes task identification over traditional sentence-level semantic analysis. Alongside this, we release the publicly available IEB benchmark for evaluating and further training instruction embeddings. To ensure instruction embeddings focus more on task specifics, we propose a prompt-based approach for generating instruction embeddings, applicable in both learning-free and supervised fine-tuning settings. It has been demonstrated on two basic evaluation tasks and four downstream tasks that instruction embedding is superior for instruction-related tasks. The introduction of instruction embedding, along with the IEB benchmark and the PIE method, plays a crucial auxiliary role in instruction-related tasks for large language models.

## 7 Limitations

One limitation of our approach is that, by not relying entirely on manual labeling or verification, not all the data is guaranteed to be of high quality. Manual validation results indicate that 93% of the sample categories are accurate, leaving a small portion that may still contain noise. Additionally, we have not addressed multi-step instructions, where several serialized tasks are embedded within a single instruction, as no such cases were manually identified in the selected dataset, and therefore, these samples were neither handled nor supplemented. Lastly, the three popular instruction datasets we selected consist solely of single-turn interactions, meaning that the benchmark does not include multi-turn samples.