# Understanding the Expressive Power and Mechanisms

of Transformer for Sequence Modeling

Mingze Wang

School of Mathematical Sciences, Peking University, Beijing, China

mingzewang@stu.pku.edu.cn

&Weinan E \(\)

Center for Machine Learning Research and School of Mathematical Sciences, Peking University, Beijing, China

AI for Science Institute, Beijing, China

weinan@math.pku.edu.cn

###### Abstract

We conduct a systematic study of the approximation properties of Transformer for sequence modeling with long, sparse and complicated memory. We investigate the mechanisms through which different components of Transformer, such as the dot-product self-attention, positional encoding and feed-forward layer, affect its expressive power, and we study their combined effects through establishing explicit approximation rates. Our study reveals the roles of critical parameters in the Transformer, such as the number of layers and the number of attention heads. These theoretical insights are validated experimentally and offer natural suggestions for alternative architectures.

## 1 Introduction

In recent years, Transformer networks (Vaswani et al., 2017) have emerged as foundational models, setting new benchmarks across various domains, including natural language processing (NLP), computer vision (CV), and protein folding. Despite their impressive practical achievements, the underlying mechanisms and theoretical foundations of Transformer networks remain largely elusive.

Transformer networks encompass various components, posing challenges to their comprehensive understanding. A typical Transformer comprises multiple layers, each consisting of a multi-head self-attention (Attn) sub-layer and a feed-forward network (FFN) sub-layer, integrated with residual blocks. FFN is a two-layer nonlinear network, while Attn includes dot-product (DP) and positional encoding (PE). To get a better understanding of how Transformer works in practice, we need to study several key issues. These include:

**(i)**_How do the key hyper-parameters, for example, the number of layers, the number of Attn heads and the with of FFN layers, affect the performance of the Transformer network?_
**(ii)**_How do the Attn and FFN layers contribute differently to the overall performance?_
**(iii)**_How does DP attention work, and is the DP structure necessary?_
**(iv)**_How efficient is PE in modeling long-range correlations?_

Extensive empirical research on Transformer components has led to the proposal of numerous alternatives to the current structure of Transformer. For example, several relative positional encodings (RPE) (Shaw et al., 2018; Raffel et al., 2020; Su et al., 2024; Press et al., 2022) have been proposedto substitute the original absolute positional encoding (APE), yielding superior performance in challenging tasks like length generalization (Ontanon et al., 2022; Csordas et al., 2021; Anil et al., 2022). Additionally, the necessity of the computationally expensive DP in Attn layers has been widely questioned, and researchers proposed numerous alternatives of DP that show considerable efficacy in specific tasks (Kitaev et al., 2020; Wang et al., 2020; Choromanski et al., 2020; Tay et al., 2021; Allen-Zhu and Li, 2023). Nonetheless, these explorations have not yielded a satisfactory theoretical understanding of the mechanisms of these components.

**In this work,** we investigate the expressive power of Transformer and the underlying mechanisms of its components for sequence modeling. **Our contributions** are summarized as follows:

We categorize three types of sequence modeling tasks with varying complexity, which are relevant to a broad spectrum of application areas. _Task I: Modeling fixed, long but sparse memories_. This is relevant to sparse Boolean functions and the traditional \(n\)-gram model in NLP. _Task II: Modeling adaptive, long but sparse memories_. This is relevant to multi-step reasoning tasks as well as various NLP tasks such as dependency parsing, sentiment analysis, and continuation writing. _Task III: Modeling essentially sparse memories_. Examples include feature representation in CV and wavelet analysis in classical signal processing.

For these sequence modeling tasks, we theoretically investigate the expressive power of Transformer and its variants, establishing explicit approximation rates. Our meticulous analysis provides _theoretical insights_ into the underlying mechanisms of Transformer components. Specifically,

* **The distinct roles of the number of layers, the number of Attn heads, and the width of FFN layers.** Deeper Transformer are capable of handling memories with more intricate interrelationships, such as nested relationships (Thm 4.4). In contrast, for memories lacking such interrelationships, single-layer Transformer with sufficient number of Attn heads and FFN width should suffice (Thm 4.1). This is quite intuitive: If the content of the next token relies on a few previous tokens in an independent way, we can treat each such dependence by a separate attention head. There is no need for many layers. Additionally, increasing the depth can also alleviate the reliance on the number of heads and width (Prop 4.5).
* **The different roles of Attn layers and FFN layers.** Our results consistently suggest that: FFN layers are tasked with approximating nonlinear memory functions and the readout function, while Attn layers are responsible for extracting the tokens from these memory locations.
* **The functionality and necessity of DP.** For the relatively simple Task I, DP is not necessary and can be omitted (Thm 3.1). However, for the more complex Task II, the cooperation between DP and RPE provides the needed interaction between the temporal space and the token space, crucial for the extraction of adaptive memories (Thm 4.1 and 4.4). Additionally, for Task II, while the nonlinearity provided by DP is necessary (Prop 4.2), a computationally efficient alternative to DP exists, as we show in Prop 4.3.
* **The efficiency of RPE in modeling long-range correlations.** Our results consistently suggest that the primary role of RPE is to approximate the memory kernels. Specifically, for Task III, we demonstrate that Transformer with suitable RPE can handle heavy-tailed memories, thus overcoming the Curse of Memory faced by recurrent neural networks (Thm 5.1). Moreover, our findings give theoretical support to the choice of RPE in practice.

Finally, we conduct experiments to validate our theoretical insights.

## 2 Preliminaries

**Basic notations.** We use bold-faced letters for vectors or matrices and lowercase letters for scalars, e.g. \(=(x_{1},,x_{d})^{}^{d}\) and \(=(W_{ij})_{m n}^{m n}\). The standard Euclidean inner product between two vectors is denoted by \(,\), and the \(l_{p}\) norm of a vector is represented by \(_{p}\). We employ standard big-O notations \(,,\) to hide absolute positive constants and use \(},,\) to further hide logarithmic constants. For any positive integer \(n\), let \([n]=\{1,,n\}\). Denote by \(\{E\}\) the indicator function for an event \(E\). Denote by \(a b=\{a,b\}\) for real number \(a,b\).

### Sequence modeling with long but sparse memories

**Sequence modeling.** For convenience, we consider input sequences of infinite length \((t)\). It is important to note, however, that our theoretical framework can be adapted to finite-length input sequences by masking distant tokens. Formally, the output sequence \(=(_{t})_{t}^{c}\) is generated from the input sequence \(=(_{t})_{t}^{d }\) via an unknown mapping \(()\) dependent on the input sequence up to the prediction time, and this can be expressed as:

\[_{t}=_{t}()=(_{t},_{t-1},_{t-2}, ), t. \]

Our objective is to learn the mapping \(()\). Additionally, we define the norm \(\|\|:=_{t}_{} \|_{t}()\|\). Without loss of generality, we assume \(\|_{t}\|_{2} 1\) for any \(\) and set the output dimension \(c=1\) for simplicity.

**Long but sparse memories.** To model such sequences, we define three types of memories: fixed, long but sparse memories; adaptive, long but sparse memories; and essentially sparse memories. These memory types are prevalent in sequence modeling tasks across diverse domains such as NLP, CV, signal processing, and sparse function representation. In Section 3, 4, and 5, we will formally define these different types and investigate Transformer's capacity to model them.

### Transformer architecture

**Transformer network.** Transformer (Vaswani et al., 2017) is a network architecture designed for processing sequences and generating predictions. Given an input sequence \(\), Transformer executes the following steps. Initially, each \(d\)-dimensional (dim) input token is transformed into a \(D\)-dim vector through an embedding mapping such as \(_{t}^{(0)}=_{E}_{t}+_{E}\), where \(_{E}^{D d},_{E}^{D}\). Subsequently, a typical \(L\)-layer Transformer with residual block operates according to the formulation:

\[^{(l-)}&=^{(l-1 )}+^{(l)}(^{(l-1)}), l[L];\\ ^{(l)}&=^{(l-)}+^{ (l)}(^{(l-)}), l[L]. \]

At the \(l\)-th layer, \(^{(l)}()\) denotes a standard (point-wise) two-layer ReLU networks with \(m\) neurons: for a given input \(^{D}\), \(^{(l)}()=_{k=1}^{m}_{k}^{(l)}_ {k}^{(l)}+c_{k}^{(l)}\), where \(()\) is the activation function such as ReLU. Additionally, in the final (\(L\)-th) FFN layer, the residual block is omitted, commonly referred to as the readout function. Moreover, \(^{(l)}()\) refers to a multi-head self-attention, as elaborated below.

**Multi-head self-attention.** Our focus lies on standard dot-product Attn, denoted as \(^{(l)}()\) and consisting of \(H\) heads. When applied to an input sequence \(\), Attn operates as follows:

\[^{(l)}()=_{O}^{(l)}_{h=1}^{H}_{V}^{(l,h)} _{c}(_{Q}^{(l,h)},_{ K}^{(l,h)}+^{(l,h)}). \]

Here, the parameters \(_{Q}^{(l,h)},_{K}^{(l,h)},_{V}^{(l,h)},_{O}^{(l,h)}\) correspond to the query, key, value, output matrices of the \((l,h)\)-th head, respectively. \(_{c}\) represents taking softmax normalization across column. Furthermore, \(^{(l,h)}^{2}\) denotes the relative positional encoding matrix, which satisfies \(R_{t,s}^{(l,h)}=-\) for \(t<s\) in the next-token prediction paradigm. Consequently, the \(t\)-th output of Attn is expressed as:

\[_{t}^{(l)}()=_{O}^{(l)}_{h=1}^{H}_{s=0}^{+ }_{V}^{(l,h)}_{t-s}(_{Q}^{( l,h)}_{t},_{K}^{(l,h)}_{t-s}+R_{t,t-s}^{(l,h)} )}{_{j=0}^{+}(_{Q}^{(l,h)}_{t },_{K}^{(l,h)}_{t-j}+R_{t,t-j}^{(l,h)})}.\]

**Logarithmic and Power relative positional encoding.** As highlighted in Section A, among various types of RPEs, the RPEs used in T5 and KERPLE(log) demonstrate superior performance over Alibi, significantly outperforming other RPEs and APEs in the length generalization task (Kazemmejad et al., 2023; Chi et al., 2022). This finding motivates our focus on the T5-type, KERPLE(log), and Alibi-type RPEs throughout this paper. All of these RPE matrices are Toeplitz, with the form of\(R_{t,s}=r(t-s)\). Notably, for T5 and KERPLE(log), \(r(t-s)\) undergoes an initial linear decrease followed by a logarithmic decrease as the relative distance \(t-s\) increases (Please refer to Section G.1 for more details). In contrast, for Alibi, \(r(t-s)\) decreases linearly. Inspired by these discussions, we examine the following RPEs with different decay rates:

\[_{}(z)=- z,&z 1\\ -,&;_{}(z)=-z,&z 0 \\ -,&.\]

We will study Transformer with \(_{}\) RPE (\(\{,\}\)). Specifically, the RPE in the \((l,h)\)-th head (3) is as follows:

\[R_{t,s}^{(l,h)}:=p^{(l,h)}_{}(t-s), \]

where \(p^{(l,h)}_{+}\) is a trainable parameter.

**Remark 2.1**.: For standard Transformer (2) incorporating Attn (3) with RPE (4), the parameters are: the embedding matrix \(_{E}\); \(_{k}^{(l)},_{k}^{(l)},c_{k}^{(l)}\) in the FFN layers; \(_{Q}^{(l,h)},_{K}^{(l,h)},_{V}^{(l,h)},\)\(p^{(l,h)},_{O}^{(l)}\) in the Attn layers. Notably, the number of parameters is independent of the sequence length, thus enabling the model to handle input sequences of arbitrary length.

**Remark 2.2**.: In the subsequent sections, we will analyze Transformer and its variants. For the sake of brevity, some shorthand notations are introduced here. For examples, Transformer (2) using \(_{}\)/\(_{}\) RPE (4) is referred to as "Transformer with \(\)/lin-RPE"; Transformer with \(_{Q}^{(l,h)},_{K}^{(l,h)}=\) is called "dot-product-free Transformer".

### Expressive power via approximation theory

This paper delves into the expressive power of Transformer through the lens of approximation theory, with a specific focus on establishing explicit approximation rates for Transformers in modeling long but sparse memories.

**Approximation rates v.s. universal approximation.** In approximation theory, results are generally categorized into two types: universal approximation (density-type) and approximation rates (Jackson-type) (Jackson, 1930). Universal approximation investigates whether the hypothesis class is dense in the target class. Although this property is fundamental, it does not offer detailed insights into approximation efficiency. In contrast, approximation rates go deeper, emphasizing the efficiency of the approximation. A typical example within this framework is the approximation theory of two-layer neural networks (2NNs).

**Barron space of 2NNs**. The well-known universal approximation result for 2NNs asserts that 2NNs can approximate any continuous function (Barron, 1992, 1993, 1994). Nonetheless, this result lacks a characterization of the approximation efficiency, i.e., how many neurons are needed to achieve a certain approximation accuracy? This gap was addressed by the Barron space theory (E et al., 2019, 2021, Ma et al., 2020). It is established that for any function within Barron space \(f\) (Appendix G.2), 2NNs with \(m\) neurons (denoted by \(_{m}\)) can approximate them efficiently, at a rate of \(_{f_{m}_{m}}\|f-f_{m}\|(\|f\|_{}/ )\), remarkably independent of the input dimension \(d\), thus avoiding the _Curse of Dimensionality_(Bellman, 1966, Bach, 2017).

## 3 Fixed, long but \(M\)-sparse memories

### Problem formulation

**Fixed, long but \(M\)-sparse memories.** In this section, we investigate a fundamental category of long but sparse memories. Our focus is on scenarios where the positions of the sparse memories remain fixed and are independent of the tokens. The target function is represented by:

\[y_{t}=f(_{t},_{t-T_{1}},,_{t-T_{M}}), \]

where \(1 T_{1}<<T_{M}<+\) signify the fixed positions of the memories. Despite the memories being fixed (token-independent) and sparse (finite \(M\)), the task can still be complex due to the potentially long-range memories (\(T_{1},,T_{M}\) can be large enough).

**Examples.** (**I**) For Boolean inputs, (5) aligns with _sparse Boolean functions_, also studied in (Edelman et al., 2022; Bhattamisha et al., 2022). Notably, Bhattamisha et al. (2022) observed that Transformers outperform LSTMs in learning sparse parities. (II) Selecting the simplest case of \(T_{i}=i\) in (5) corresponds to the traditional _\(n\)-gram model_, which consists of short and sparse memories.

**Target class.** We focus on target functions described in (5). The readout function \(f\) is considered within the standard Barron space \(\), i.e., which can be effectively approximated by 2NNs. Moreover, we assume that \(f\) is Lipschitz, denoted by \(f\). Thus, we can focus more on investigating the memory extraction power of Transformer. Formally, we define the target class for modeling fixed, long but \(M\)-sparse memories as:

\[^{}:=:\ _{t}()= ,\ \ 1 T_{1}<<T_{M}<+,f }. \]

**Transformer hypothesis class.** As mentioned in Section 1, one of our main aims is to study the necessity and roles of different components in Transformer, such as DP and RPE. This section focuses on the "simplest" one-layer Transformer and investigates whether it can effectively model this task. Formally, our hypothesis class includes all one-layer _DP-free_ Transformers, configured with \(H\)Attn heads and FFN width \(m\):

\[^{,}_{(1,H,m)}:= : \] \[}.\]

### Theoretical results and insights

**Theorem 3.1** (Approximation rate).: _For any target \(^{}\) (6), rate \(n_{+}\), and \(H,m_{+}\), there exists a \(1\)-layer Transformer \(^{,}_{(1,H,m)}\) (7) and a constant \(C(n)\) such that_

\[|-1.075pt|-1.075pt|-| -1.075pt|-1.075pt|-1.075pt|_{ }+|-1.075pt|-1.075pt|f|-1.075pt |-1.075pt|_{}_{}(),\]

_where \(_{}=}(}}{})\) and \(_{}()= }(_{i=1}^{M}e^{0.01T_{i}})_{+}^{n+1}, =\\ }(_{i=1}^{M}T_{i}^{1.01})_{ +}^{n+1},=.\)_

Theorem 3.1 establishes the _approximation rate_ of one-layer DP-free Transformer for modeling fixed, long but sparse memories. Here, the model complexity is governed by the number of Attn heads \(H\) and the width of FFN layers \(m\), while the target complexity arises from the lengths of the memories \(T_{1},,T_{M}\) and the complexity of the readout function \(f\). The approximation error comprises two components: the error in the FFN component \(_{}\) and the error in the Attn component \(_{}()\). The error \(_{}\) aligns with classical results, showcasing its effectiveness in approximating Barron functions. On the other hand, \(_{}()\) hinges on the capacity of the Attn block for modeling long-range memories. Specifically, with increasing memory length, the necessary number of Attn heads grows at a small exponential rate for \(\)-RPE and at a polynomial rate for \(\)-RPE.

The proof of Theorem 3.1 is deferred to Appendix B. We can draw some insights from Theorem 3.1 and its proof.

**Different roles of the Attn layer and the FFN layer.** The Attn and FFN layers fulfill distinct roles in this task. Specifically, the FFN layer efficiently approximates the nonlinear readout function \(f\), while the Attn layer is responsible for extracting the token \(_{t-T_{i}}\) by approximating the memory kernel \(\{=T_{i}\}\). These components together enable effective modeling of fixed, long, but sparse memories.

**Non-necessity of DP.** Theorem 3.1 suggests that the DP component in Attn is not necessary and can be omitted for modeling fixed, long but sparse memories. This is due to the relative simplicity of modeling fixed memory kernels. In a more complex scenario in Section 4, the role of the dot-product becomes important. In contrast to Edelman et al. (2022), which utilizes the property of DP to prove that Transformer can model sparse Boolean functions, our result reveals that one-layer Transformer can successfully tackle the same task _even without the dot product in the attention layer_.

**Effect of RPE types on expressivity.** Our result indicates that the type of the RPE used in the Attn layer subtly influences the Transformer's ability to model long-range memories. As the range of the memory increases, the required head number grows at a slightly exponential rate for \(\)-RPEand at a polynomial rate for \(\)-RPE. The subtle difference is attributed to the relative simplicity of approximating the memory kernel \(\{=T_{i}\}\). We will explore a more complex task in Section 5, where the impact of different types of RPE becomes even more pronounced.

## 4 \(K\)-Adaptive, long but \(M\)-sparse memories

### Problem formulation

In this section, we delve into a more complex modeling scenario closely aligned with typical language processing tasks.

\(K\)**-Adaptive, long but \(M\)-sparse memories.** This section investigates the scenario where the positions of the sparse memories are "adaptive", meaning they depend on the input tokens. The target function is formulated as:

\[y_{t}=f(_{t},_{t-t_{1}},,_{t-t_{M}}), \]

where the positions of the memory tokens \(t_{1},,t_{M}\) follow a nested relationship:

\[ t_{1}=g_{1}(_{t});t_{2}=g_{2}(_{t}, _{t-t_{1}});;t_{K+1}=g_{K+1}(_{t},_{t-t_{1}},, _{t-t_{K}});\\ ;t_{M}=g_{M}(_{t},_{t-t_{1}},,_{t-t _{K}}).\]

Here, \(M\) denotes the number of memory tokens, and \(K\) measures the nesting complexity in the memory structure. We assume that memory functions \(g_{i}\) generate positive integers for the input tokens, and there exist maximum values \(T_{i}\) such that \(g_{i} T_{i}\). In this adaptive framework, each position of the memory token depends on multiple input tokens and is nested within other memory structures, leading to potential influence of later memory tokens by the earlier ones.

To facilitate understanding, we first consider a warm-up case, i.e., \(K=0\) in (8). In this case, the positions of memories only depend on the current token, without interaction with each other. It can be represented as:

\[y_{t}=f(_{t},_{t-t_{1}},,_{t-t_{M}}), \]

where \(t_{i}=g(_{i}),i[M]\).

**Target class.** The target classes for modeling adaptive, long but sparse memories in both warm-up and general cases are as follows:

\[^{ Adap}_{(1,M)}:=:\ _{t}()= ,\ \ g_{i},1 g_{i} T_{i},i[M];f }. \]

\[^{ Adap}_{(K,M)}:=:\ _{t}()= ,\ \ g_{i},1 g_{i} T_{i},i[M];f }. \]

**Examples.** Adaptive memories are commonly encountered in practical scenarios. (I) _Adaptive sparse Boolean functions_, e.g., \(y_{t}=x_{t} x_{t-g(x_{t})} x_{t-g(x_{t-g(x_{t})})}\), where \(\{ 1\}^{}\), \(g(x)=1\) for \(x=1\) and \(g(x)=2\) for \(x=-1\). This fits within our framework (8) with \(K=M=2\). (II) _Multi-step reasoning_, e.g., modeling the \(K\)-adaptive, long, but \(K\)-sparse memories contains a complicated \(K\)-step reasoning task, which require the sequential search following the rule \(((((x_{t} x_{t-t_{1}}) x_{t-t_{2}}) x_{t-t_{ K-1}}) x_{t-t_{K}}\). (III) In _NLP tasks_ like dependency parsing, part-of-speech tagging, sentiment analysis, or continuation writing, the positions of relevant prefix tokens usually depend on the context itself, and can vary depending the content. Additionally, the nested structure is a fundamental characteristic of natural language (Hawkins, 2021).

**Transformer hypothesis class.** Some previous works Yun et al. (2019); Kim et al. (2022) treated the softmax with normalization as an approximation of hardmax, suggesting the potential importance of the normalization. In contrast, in this section, we remove the normalization in the denominator of softmax and investigate its ability for sequence modeling. Additionally, to address the discreteness of time and memory values, we consider Transformer with specific precision, as detailed in Appendix C. The precision technique is widely used in LLM training (Kalamkar et al., 2019), such as BFloat16. Formally, the hypothesis class is defined as follows, encompassing all normalization-free \(L\)-layer Transformer, configured with \(H\) Attn heads and FFN width \(m\) and using type-RPE and specific precision.

\[^{}_{(L,H,m)}:=: \ \ L\ H\ m \] \[}.\]

### Theoretical results and insights: The warm-up case

**Theorem 4.1** (Approximation rate, warm-up case).: _For any target \(^{}_{(1,M)}\) (8), rate \(n_{+}\), and \(H,m_{+}\), there exists a two-layer Transformer \(^{}_{(2,H,m)}\) (12) and a constant \(C(n)\) such that: if the width satisfies \(m_{i=1}^{M} g_{i} _{}^{2}&,=\\ _{i=1}^{M} g_{i}_{}^{2}T_{i}^{2}&,=\), then the following approximation rate holds:_

\[\!-\! _{}+ f_{}_{}(),\]

_where \(_{}=}(}}{})\) and \(_{}()= }(_{i=1}^{M}e^{0.01T_{i}})^{n+1}&, =\\ }(_{i=1}^{M}T_{i}^{1.01})^{n +1}&,=\)._

In Theorem 4.1, we present the _approximation rate_ of two-layer Transformer for the warm-up case: modeling \(1\)-adaptive, long but \(M\)-sparse memories. This theorem reveals that the approximation error comprises two distinct components: the error in the FFN component \(_{}\) and the error in the Attn component \(_{}()\). A critical difference from 3.1 is the presence of the condition related to the width \(m\) of FFN layers. This term arises from using the FFN layer to approximate the memory function \(g_{i}\). Owing to the discreteness of memory \(g_{i}\) and the implementation of rounding operations, the approximation within rounding accuracy all achieves zero error after rounding, while it can not get correct rounding beyond this accuracy. In contrast, the error \(_{}\) is caused by using FFN to approximate the readout function \(f\), the same as \(_{}\) in Theorem 3.1.

The proof of Theorem 4.1 can be found in Appendix C.1. Theorem 4.1 and its proof offer several critical insights into the underlying mechanism of Transformer.

**Distinct roles of Attn layers and FFN layers.** Our proof elucidates that the FFN layers are tasked with approximating the readout function \(f\) and memory functions \(g_{i}\), while the Attn layers are responsible for the extraction of the adaptive memories. It is essential to clarify the difference between "approximating memory functions" and "memory extraction". The former refers to utilizing some function to estimate the memory function \(g_{i}\), whereas the latter pertains to extracting the token \(_{t-g_{i}(_{t})}\) from the memory location.

**Cooperation between DP and RPE.** In the \(2\)-nd Attn layer, the extraction of the memory functions is achieved through an interplay between DP and RPE. Specifically, this is done through _a nice interaction between the temporal space (provided by RPE) and the token space (provided by DP)_. Please refer to Appendix C.1 for more details.

**Rethinking DP in Attn.** Our proof highlights that the core mechanism of Attn is to provide a nice interaction between the temporal space and the token space through the cooperation of DP and RPE. This leads us to the following question: _Is DP in Attn necessary and replaceable?_ The following two propositions provide some hints.

**Proposition 4.2** (DP vs. DP-free (informal)).: _There exists a target \(^{}_{(1,1)}\) (10) such that:_

_(A) For any \(>0\), there exists a \(1\)-layer Attn \(^{}\) such that \(\!-^{}\! \)._

_(B) For any \(1\)-layer DP-free Attn \(^{}\), a uniform lower bound holds: \(\!-^{}\! \)._

Proposition 4.2 reveal a significant distinction in the expressiveness of two network types for modeling adaptive, long, but sparse memories. Specifically, \(1\)-layer Attn with DP can effectively model this task, while \(1\)-layer DP-free Attn provably fails. This finding underscores the essential role of DP in providing the necessary nonlinearity for Attn to model adaptive memories. The formal version of Proposition 4.2 and its proof can be found in Appendix C.2.

**Proposition 4.3** (Substitute for DP (informal)).: _There exists a substitute structure for DP, requiring only \((D)\) parameters (compared to \((D^{2})\) in standard DP) that can effectively model \(^{}_{(1,M)}\) (10). Specifically, if we substitute DP with this structure, \(1\)-layer Transformer can achieve the same approximation rate as stated in Section 4.1._

[MISSING_PAGE_FAIL:8]

to the _sum_ of all the memory functions' complexity (\(\|g_{i}\|_{},\| g_{i}\|_{},T_{i}\) for memory function \(g_{i}\)). In contrast, for \(M+1\)-layer Transformer, the required width correlates with the _maximum_ complexity of the memory functions, much lower than that for \(2\)-layer Transformer. Similarly, the required number of heads for \(M+1\)-layer Transformer is much fewer than that for \(2\)-layer Transformer. Please refer to Appendix D.2 for a detailed comparison. The observation suggests that increased depth can significantly reduce the demands on the number of heads and the width. The underlying reason is that deep networks can distribute the memories across different layers for processing, with each layer focusing on approximating only a single memory function.

## 5 Essentially \(M\)-sparse memories

### Problem formulation

In language tasks, each token possesses clear semantic meaning. As a result, the structure of the memory is sparse in the original space. This aligns well with our modeling assumptions discussed in Section 3 and 4. However, in other machine learning tasks, we may encounter situations where the input tokens lack distinct semantic meaning. This might happen in image processing or classical signal processing. In these situations, the memory structure could potentially be dense in the original space. Nonetheless, the memory structure might exhibit sparsity in some transformed domain. We call such memory structure "essentially sparse". In this section, we study the situation in which the memory structure in long-ranged but essentially sparse. For simplicity, we consider the situation in which the positions of the memory kernels are fixed. The analysis can be easily extended to the situation with an adaptive memory structure.

**Fixed, essentially \(M\)-sparse memory.** Consider the following situation:

\[y_{t}=f((*_{1})(t),,(*_{M} )(t)), \]

where \(_{1}(),,_{M}()^{1}()\) serve as memory kernels, and \((*_{k})(t)=_{s=0}^{+}_{t-s}_{k}(s)\) denotes the convolution of the inputs with kernel \(_{k}\).

**Target class** and **Transformer hypothesis class.** The target class for modeling essentially sparse memories is defined as:

\[^{}:=:\ _{t}()=(),_{1},,_{M}^{1}(),f }. \]

For the hypothesis class, we consider one-layer dot-product-free Transformer with Attn head number \(H\) and FFN width \(m\), as defined in (7).

**Examples.** Essentially sparse memories are prevalent in real-world scenarios:

(I) _Image Tasks._ In CV, a fundamental objective is identifying and representing meaningful "features", such as ears, nose, etc. These features can often be modeled using convolution kernels, leading to a task in the form \(y=f(*_{},*_{},*_{ })\). This is an extension of the task we discussed above, in which the kernel functions \(\{_{j}\}\) are data-dependent ("adaptive" in the terminology used in the previous section).

(II) _Signal processing._ In signal processing, it is commonly the case that the signals are highly sparse under Wavelet or Fourier transforms. For instance, let \(()\) be a wavelet function and define \(_{a,b}(t):=()/\). Then we have \(y=f(*_{a_{1},b_{1}},,*_{a_{M},b_{M}})\) where \((a_{1},b_{1}),,(a_{M},b_{M})\) might be data-dependent.

(III) _Mathematical calculation._ Consider algebraic operations where memory exhibits sparsity under specific linear transformations. For example, \(y_{t}=10x_{t}+x_{t-4}/(_{s=0}^{100}w_{s}x_{t-10-s})-_{s=0}^{+}v_{ s}x_{t-100-s}\) can be represented in our framework as \(y=f(*_{1},,*_{4})\), where each \(_{i}\) represents a specific linear transformation.

### Theoretical results and insights

**Theorem 5.1** (Approximation rates).:

_(A) Consider \(^{}\) (14) with exponentially decayed memory kernels, i.e., there exists \(>0\) such that \(_{1}(t),,_{M}(t)=(e^{- t})\). Then for any target \(^{}\), rate \(n[ 99]\), and \(H,m_{+}\), there exists a \(1\)-layer DP-free Transformer \(^{}_{(1,H,m)}\) (7) and a constant \(C(n)\) such that_

\[\|\!|-|\!\|_{ }+\|f\|_{}_{};\]

_(B) Consider \(^{}\) (14) with polynomially decayed memory kernels, i.e., there exists \(>1\) such that \(_{1}(t),,_{M}(t)=(t^{-})\). Then for any target \(^{}\), rate \(n[ 0.99-1]\), and \(H,m_{+}\), there exists a \(1\)-layer DP-free Transformer \(^{}_{(1,H,m)}\) (7) and a constant \(C(n)\) such that_

\[\|\!|-|\!\|_{ }+\|f\|_{}_{};\]

_where \(_{}=}(}}{}),_{}=( }{H^{n}}).\)_

Theorem 5.1 illustrates that one-layer DP-free Transformer with lin-RPE is effective in modeling essentially sparse memories with exponentially decayed kernels, and one-layer DP-free Transformer with log-RPE can efficiently model the memories with polynomially decayed kernels. A key difference between Theorem 5.1 and Theorem 3.1 lies in the memory kernels they address. In Theorem 5.1, the Attn layer should approximate general memory kernels \(_{i}()\), instead of approximating indicator kernels \(\{=T_{i}\}\) in Theorem 3.1. The proof of Theorem 5.1 can be found in Appendix E.

**Overcoming the Curse of Memory (CoM).** For recurrent neural networks (RNN), it was discovered (Li et al., 2021, 2022) that both approximation and optimization become exceedingly difficult when the target has long-term memory. This phenomenon is referred as the "_curse of memory_", or "CoM". It was shown in (Li et al., 2021, 2022) that RNN requires an exponentially large number of neurons to approximate targets with heavy-tailed memory kernels, such as the ones that exhibit polynomial decay. In contrast, Theorem 5.1 reveals that Transformer with log-RPE efficiently handles polynomial decaying memory kernels, requiring only a polynomial number of neurons for effective approximation. This finding theoretically elucidates the superior performance of T5's RPE and KERPLE(log) in length generalization task in practice (Section G.1).

## 6 Experimental Validation

As summarized in Section 1, our theoretical analysis reveals novel insights into the expressive power and mechanisms of Transformer. To validate these insights, we conduct experiments ranging from simple toy models to more complex language model pre-training. Due to space constraints, detailed experimental validation and practical implications of our insights are presented in Appendix H.

## 7 Conclusion and Future Work

In this work, we investigate theoretically the expressive power and the mechanisms of Transformer for modeling long but sparse memories. Our analysis establishes explicit approximation rates and offers much-needed insights into the functionalities of the various components of Transformer. However, we still have a long way to go for a full theoretical understanding of Transformer. For instance, although we have investigated the mechanisms of Transformer in terms of expressive power, the evolution of the mechanisms during the training process remains elusive. Recent studies revealed that Transformer exhibits multi-phase learning dynamics (Boix-Adsera et al., 2023) and undergoes phase transitions (Olsson et al., 2022) during training, akin to the phenomenon of learning with increasing complexity in classical neural networks (Kalimeris et al., 2019; Xu et al., 2019; Rahaman et al., 2019; Abbe et al., 2023; Wang and Ma, 2023). These and other issues will be studied in future work.