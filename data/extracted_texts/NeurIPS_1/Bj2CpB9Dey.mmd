# Tangent Space Causal Inference: Leveraging Vector Fields for Causal Discovery in Dynamical Systems

Kurt Butler1 Daniel Waxman1 Petar M. Djuric

Department of Electrical and Computer Engineering

Stony Brook University

{ kurt.butler, daniel.waxman, petar.djuric}@stonybrook.edu

Equal contribution.

###### Abstract

Causal discovery with time series data remains a challenging yet increasingly important task across many scientific domains. Convergent cross mapping (CCM) and related methods have been proposed to study time series that are generated by dynamical systems, where traditional approaches like Granger causality are unreliable. However, CCM often yields inaccurate results depending upon the quality of the data. We propose the Tangent Space Causal Inference (TSCI) method for detecting causalities in dynamical systems. TSCI works by considering vector fields as explicit representations of the systems' dynamics and checks for the degree of synchronization between the learned vector fields. The TSCI approach is model-agnostic and can be used as a drop-in replacement for CCM and its generalizations. We first present a basic version of the TSCI algorithm, which is shown to be more effective than the basic CCM algorithm with very little additional computation. We additionally present augmented versions of TSCI that leverage the expressive power of latent variable models and deep learning. We validate our theory on standard systems, and we demonstrate improved causal inference performance across a number of benchmark tasks.

## 1 Introduction

The discovery of causal relationships is one of the most fundamental goals of scientific work. When causal relationships are known and understood, we can explain the behavior of a system and understand how our actions or interventions upon the system will affect its behavior . This kind of reasoning is fundamental in many problem domains, such as medicine, environmental policy, and economics. However, it is not always possible to perform interventions and observe their effects. For example, medical practitioners might only have one chance to prescribe a medication to help a patient. As another example, an ecologist might find that performing a large experiment is forbiddingly expensive or otherwise infeasible. Due to these concerns, there remains a great interest in the problem of _observational_ causal inference, where one infers causation without manipulating the system under study directly.

For time series data, the most prominent tool for observational causal inference is Granger causality . Granger causality operates on the assumption that if one system (the cause) is driving changes in another system (the effect), then the cause should have unique information about what will happen to the effect. This is encoded in an assumption called _separability_, which posits that the information needed to predict the future behavior of the effect is not contained in the effect itself. However, the separability condition runs counter to the behavior of coupled dynamical systems: if long histories of the effect can forecast the cause, then an autoregressive model with the appropriate lag may forecast the effect from itself. This effect is explored in the supplementary materials of , and we givean example in Appendix B. Since systems with (approximately) dynamical behavior are ubiquitous in many application domains, alternative methods have become of interest.

To address the failures of Granger causality in coupled dynamical systems Sugihara et al.  proposed the convergent cross mapping (CCM) method, which directly takes advantage of the topological properties of dynamical systems. CCM can be seen as an adaption of earlier work that studied the synchronization of dynamical systems [2; 29; 35] into an algorithmic procedure for detecting causalities between time series. Using Takens' theorem, a well-known result in dynamical systems theory , CCM attempts to detect causality by reconstructing the state space of a given time series, and then learning a time-invariant function that maps between reconstructed state spaces, called a cross map. The key assumption is that the cross map only exists if the original systems were dynamically coupled. In this sense, the "causality" of CCM deviates from the popular Pearlian framework , and is better interpreted as identifying which variables "drive" or "force" a dynamical system based on observational data.2 Building upon this framework, several extensions of CCM have been proposed to address various technicalities and caveats of the approach and to generalize the method to new domains. However, the CCM test statistic is difficult to interpret and does not admit a simple decision rule. CCM also does not explicitly learn a cross map function.

To improve upon CCM, we propose Tangent Space Causal Inference (TSCI). TSCI detects causation between dynamical systems by explicitly checking if the dynamics of one system map on to the other. The proposed method can be seen as a drop-in replacement for the CCM test, providing an interpretable and principled alternative while remaining compatible with many of the extensions of CCM. Furthermore, the proposed method is model agnostic, meaning that it can be adapted to any method used to learn the cross map function, including multilayer perception (MLP) networks, splines, or Gaussian process regression. The only major assumption of TSCI is that the time series under study were generated by continuous time dynamical systems (i.e, by systems of differential equations), which is a standard assumption in a number of physical systems . As a result, TSCI is applicable to many of the same problems as CCM.

### Related Work

**Causal representation learning.** The primary function of Takens' theorem in the CCM method is to yield a representation of a system's latent state so that it may be used for cross mapping. As a result, numerous generalizations of Takens' theorem emerged in the following decades [32; 33; 40]. Causal representation learning aims to learn hidden variables from high dimensional observations , and a principle task is to decipher the causal relationships between many, possibly redundant, observations of a system . In such cases, methods of dimensionality reduction can be applied to extract causal variables from the raw observations . This is particularly important in the processing of large, spatio-temporal data sets [36; 43]. Some authors have proposed that CCM can be improved by aggregating data from multiple sources into the reconstruction of the latent states [7; 45], which requires an awareness of how the observation data relate to the causal variables of interest.

**Causal discovery with cross maps.** CCM has popularized the use of cross maps as a tool for causal inference, and many variations and improvements of the basic CCM methodology have been proposed. Some of these works aim to improve the reconstruction of latent states in these models by using a Bayesian approach to latent state inference , where the approach is adapted for spatial/geographic data  or modified for sporadically sampled time series . Additionally, several improvements have suggested changing the way that a cross map is detected: some authors have recommended varying the library length  or time delaying the cross mapping  to yield refined information. The \(k\)-nearest neighbor regression, which is used in the original CCM algorithm, can be swapped out for a radial basis function network , or for a Gaussian process regression model . Other approaches have not directly learned the cross map at all; instead they have examined other aspects of the reconstructions such as their dimensionality  or used pairwise distance rankings as a signature of the mapping .

Proposed Method

We begin by considering the inference of a causal relationship between two time series, \(x(t)\) and \(y(t)\). Our starting assumption is that both time series were generated by latent dynamical systems with states \(_{x}(t)\) and \(_{y}(t)\), respectively, whose behavior is governed by a set of ordinary differential equations (ODEs). For motivation, we consider a particular case with a unidirectional coupling between the latent states:

\[x(t) =h_{x}(_{x}(t)), \] \[y(t) =h_{y}(_{y}(t)),\] (2) \[_{x}}{dt} =_{x}(_{x}),\] (3) \[_{y}}{dt} =_{y}(_{x},_{y}), \]

where the observation functions \(h_{x},h_{y}\) and the time derivative functions \(_{x},_{y}\) are assumed to be differentiable. A causal relationship between \(x\) and \(y\) is evidenced by the appearance of \(_{x}\) in the equation for \(d_{y}/dt\). The state vectors \(_{x}\) and \(_{y}\) could possibly have redundant information, but by writing the system in this form, we obtain an asymmetry between \(x(t)\) and \(y(t)\) which will inform our inference of causation. In our notation, if \(_{x}\) appears in the equation for \(d_{y}/dt\), we will write \(x y\).

We now explain how the CCM method approaches this problem and how the TSCI method builds upon the CCM framework.

### Convergent Cross Mapping

CCM is a technique for inferring causation between time series generated by dynamical systems , as seen in Eqs. (1) to (4). The basic motivation for CCM is that given an observed time series \(x(t)\), one can construct a vector \(}\) which acts as a proxy for the latent states \(_{x}\) that generated it. Given two constructions, \(}\) and \(}\), we may detect if there is a mapping between them, which provides evidence of a causal relationship. From Eq. (4), since \(_{x}\) influences \(_{y}\) unidirectionally, the effect \(y(t)\) contains more information than the cause time series \(x(t)\), and as a result \(}\) generally contains enough information to reconstruct \(}\). With this in mind, we can frame CCM as a two-step procedure . In Step 1, we construct representations \(}\) and \(}\) that are proxies for \(_{x}\) and \(_{y}\), respectively. In Step 2, we detect a mapping between reconstructions; if there is a mapping \(}}\), then the reverse causality holds, \(x y\).

To reconstruct the latent state space, as in Step 1, multiple approaches could be employed. However, in the basic CCM methodology, one uses the so-called _delay embedding_ of \(x(t)\),

\[}(t)=x(t)\\ x(t-)\\ \\ x(t-(Q-1)), \]

where \(\) and \(Q\) are parameters called the embedding lag and embedding dimension, respectively. The justification that \(}\) is a good proxy for \(_{x}\) is given by Takens' theorem , which states that \(}\) and \(_{x}\) are equivalent up to a nonlinear change of coordinates.

**Theorem 2.1** (Takens' theorem ): _Let \(M\) be a compact manifold of dimension \(d\). Let \( M\) evolve according to \(d/dt=()\), let \(_{}\) be the mapping that takes \(_{t}\) to \(_{t+}\), and let \(x(t)=h((t))\). If \(Q 2d+1\), then for almost-every3 triplet \((,h,)\), the map \(_{,h,}\),_

\[_{,h,}()=h()\\ h(_{-}())\\ \\ h(_{-(Q-1)}()) \]_is an embedding of \(M\) into \(^{Q}\)._

Since \(_{,h,}(_{x}(t))=}(t)\), Takens' theorem tells us that if \(_{x}\) lives on a manifold \(M\), then the points \(}\) lie on a manifold \(_{x}\), which is called the _shadow manifold_ of \(x\). Despite the number of assumptions in the statement of Takens' theorem, many generalizations of the statement exist, allowing us to justify the use of delay embedding to systems with strange attractors  and systems with noise . If a system satisfies the conditions for Takens' theorem, in the sense that \(_{x}\) is a valid embedding of \(M\), then we say that the system is _generic_.

Since only \(_{x}\) influences \(x(t)\), Takens' theorem implies that \(}\) is equivalent (up to nonlinear coordinate change) to \(_{x}\). However, due to the appearance of \(_{x}\) in (4), both \(_{x}\) and \(_{y}\) are responsible for generating \(y(t)\). As a result, Takens' theorem suggests that \(}\) is equivalent to \((_{x},_{y})\) as a concatenated vector . Since \((_{x},_{y})\) clearly can be mapped onto \(_{x}\), the equivalence between the delay embeddings and the latent states suggests that there is a mapping \(}}\), called a _cross map_. Thus, cross maps encode the idea that the effect time series contains information about its cause. This is encoded in the following corollary to Takens' theorem.

**Corollary 2.1.1**: _Suppose that \(x y\) for a generic system. Then there exists a function \(F\) such that \(}(t)=F(}(t))\) for all \(t\)._

The proof of the corollary is provided in the Appendix.

Step 2 of CCM is then to detect if \(x y\) by checking if a cross map \(}}\) exists. To this end, Sugihara et. al.  propose to check the predictability of the time series \(x(t)\) given \(}(t)\). They use a form of \(k\)-nearest neighbors regression to produce an estimate \((t)\) of \(x(t)\) given \(}(t)\), and then they define a test statistic

\[r^{}_{X Y}=((t),x(t)), \]

where corr is the Pearson correlation coefficient. To test the reverse causal direction, \(x y\), one simply performs Step 2 again with the roles of \(x\) and \(y\) reversed.

### Tangent Space Causal Inference

Takens' theorem and cross maps as a tool for causal inference are rooted in a solid mathematical foundation, but the CCM test does not exploit all of the properties of shadow manifolds. Namely, it does not exploit the fact that the shadow manifolds are copies of the latent manifolds. In practice, there are cases in which CCM learns a cross map that appears to be reasonably predictive, but results in a false positive. Thus, a more robust algorithm would exploit more subtle properties of the hypothesized cross map. We propose TSCI as alternative to Step 2 in the CCM algorithm.

TSCI operates by checking if the ODE on one manifold, \(_{x}\), can be mapped to an ODE on another manifold \(_{y}\). To understand how this works, we need to reframe our discussion of ODEs in terms of vector fields. Recall that given an ODE of the form,

\[}}{dt}=(}), \]

we may interpret \(\) to be a velocity vector field on the manifold. When evaluated, \((})\) is a tangent vector of the manifold \(_{x}\), existing in the tangent space \(T_{}}_{x}\)4. From calculus, we know that tangent vectors in \(T_{}}_{x}\) can be mapped to tangent vectors in \(T_{F(})}_{y}\) by the Jacobian matrix \(_{F}(})\) at the point \(}\). By checking if the tangent vectors can be mapped in such a way, TSCI provides an alternative to the CCM causality test. A visual motivation for the TSCI methods is depicted in Fig. 1.

Before we can map vector fields from one manifold to another, we need to check that meaningful vector fields exist on the shadow manifolds. As usual, this is also a corollary of Takens' theorem.

**Corollary 2.1.2**: _There exists a vector field \(\) on \(_{x}\) such that the embedding \(_{,h,}\) in Takens' theorem is a time-invariant mapping. If we define \((0)=_{,h,}(_{x}(0))\), and if we let \((t)\) to be the flow of a point \((0)\) under the vector field \(\), then \(_{,h,}(_{x}(t))=(t)\), for all times \(t\)._The proof of the corollary is provided in the Appendix.

The corollary guarantees that we can learn ODEs that describe the dynamics of \(}\) and \(}\), once we have obtained valid reconstructions of the latent states. Let \(\) and \(\) be vector fields such that

\[}}{dt} =(}), \] \[}}{dt} =(}). \]

Because the cross map \(F\) is a mapping between the two shadow manifolds, the Jacobian matrix of the cross map relates the vectors \(\) and \(\). We state this formally in the following Lemma.

**Lemma 2.1.1**: _Let \(_{x}\) and \(_{y}\) be manifolds with respective vector fields \(\) and \(\) that define their dynamics. If there exists a cross map \(F:_{x}_{y}\), then for every point \(}_{x}\), we have that \((F(}))=_{F}(}) (})\), where \(_{F}(})\) is the Jacobian matrix of \(F\) at \(}\)._

The proof of the lemma is in the Appendix.

While the cross map \(F:_{x}_{y}\) is a mapping between points on the manifolds, the Jacobian matrix \(_{F}(})\) induces a mapping between the tangent spaces at \(}\) and \(}\). We show this visually in Fig. 1. Since the velocity of \(}\) can be represented by a tangent vector \((})\), the Jacobian matrix \(_{F}\) allows us to map these vectors to tangent vectors in \(_{y}\). If there is a cross map, then the vector field \(\) and the push forward vector field \(_{F}\) should match exactly. On the other hand, there should be no correlation in the absence of a causal relationship, assuming quality reconstructions and plentiful data. The degree of alignment between \(\) and \(_{F}\) can be used as a test statistic for the presence of a causal link. We therefore propose the TSCI test statistic,

\[r^{}_{X Y}=(,_{F}). \]

Because the tangent vectors are centered in their respective tangent planes, we can geometrically interpret the TSCI test statistic as the expected cosine similarity between the vector field \(\) and the push forward vector field \(_{F}\),

\[r^{}_{X Y}=_{}_{y}} ((F(}))^{}_{F}(})(})}{||(F(}) )||\,||_{F}(})(})||} ). \]

In practice, the estimation quality of the cross map also depends on the location in the reconstruction space, and so the cosine similarity takes on a distribution of values (Fig. 2), and weak-to-moderate correlation may be empirically observed as an artifact of limited data.

Figure 1: Visual motivation for the TSCI method. Observed time series are related to latent states evolving on some manifolds. Given observed time series \(x(t)\) and \(y(t)\), we reconstruct latent states \(}(t)\) and \(}(t)\) that reside in manifolds \(_{x}\) and \(_{y}\), respectively. If \(x\) and \(y\) are causally coupled, there should exist a function \(F\) that maps between these manifolds. Using the Jacobian matrix \(_{F}\), we can map the velocity vector field on \(_{x}\) to a vector field on \(_{y}\), and use the angle between velocity vectors as a measure of their similarity.

Based on these analyses, we propose the TSCI method in Algorithm 1, which learns a cross-map \(F\) and returns the alignment of the vector fields \(\) and \(_{F}\). CCM is sensitive to the quality of the reconstruction of the latent states because an improperly constructed shadow manifold can sabotage the method, both theoretically and empirically ; TSCI similarly requires that the shadow manifold be properly embedded. Because of this, we assume that the embedding vectors \(}\) and \(}\) and the estimates of their vector fields \((})\) and \((})\) at each point are supplied as inputs to the algorithm. A variety of methods could be used to estimate the vector fields, but the simplest approach is to use finite differences,

\[(})=}}{dt}= \\ \\ \\  x(t+1)-x(t)\\ x(t+1-)-x(t-)\\ \\ x(t+1-(Q-1))-x(t-(Q-1)),\]

where \( t\) is the sampling rate of the data. Finite differences are known to be sensitive to noise, so in real scenarios a more careful approach to obtaining the time derivatives is necessary. For a practical implementation, we propose using the central finite-differences, which are second-order accurate , or the derivatives interpolated by a Savitsky-Golay filter for noisy data .

```
0: Embedding/vector field matrices \(,^{T Q_{x}}\) and \(,^{T Q_{y}}\), regression parameter \(K>(Q_{x},Q_{y})\)
0: Score \(r_{X Y}\)
1: Find the indices \(_{1},,_{K}\) of the \(K\)-nearest neighbors of \(_{t}\)
2: Compute the local displacements in \(x\)-space: \(_{k}=_{_{k}}-_{t}\)
3: Compute the local displacements in \(y\)-space: \(_{k}=_{_{k}}-_{t}\)
4: Compute the least-squares solution to \(_{k}=_{k}\)
5: Compute \(}_{t}=_{t}\)
6:endfor
7: Define \(r_{X Y}=(},)\)
```

**Algorithm 2** Tangent Space Causal Inference with K-Nearest Neighbors

**On methods to learn the cross map function.** The TSCI algorithm is notably agnostic to the regression approach used for the cross map \(F\), with reasonable approaches including multilayer perceptron networks (MLPs), splines, and Gaussian process regression (GPR). Depending on the approach, derivatives can then be estimated from the model, either using automatic differentiation or analytical derivatives. For GPR in particular, analytical derivatives are straightforward to compute .

Since regression can sometimes be a computationally complex procedure, in Algorithm 2 we also provide a version of TSCI that is based on the \(k\)-nearest regression, in analogy to CCM. For clean and dense data, this simple approach can yield accurate results, but it is generally unsuitable for noisy or sparse data. In the latter case, it is preferrable to combine the TSCI approach with other methods of denoising time series, or learning of latent dynamical models of the observed time series. In particular, the learning of a latent ODE learning  can be combined with the TSCI test to yield accurate causal inference.

**On the use of correlation coefficient.** In relation to CCM, it can be noted that both TSCI and CCM use a correlation coefficient as their test statistic. However, a critical difference between the two methods is how the usage of this statistic is justified. The correlation coefficient used in CCM analysis, called the cross-map skill , is used to measure the accuracy of predictions of the cross map. However, since the points being predicted in CCM live on a manifold, measuring correlation in the ambient (extrinsic) space is not well-motivated. Furthermore, estimates of this correlation can be biased by the distribution of observations along the manifold, and as a result, a high correlation can be achieved with a relatively low-accuracy prediction by guessing the general region in which the points lie.

In contrast, the correlation between tangent vectors (or vector fields) is a more geometrically motivated quantity: the correlation is a linear measure of similarity and the tangent vectors belong to a (centered) linear space. Additionally, the shadow manifolds are constructed to be submanifolds of Euclidean space, so correlations computed in extrinsic coordinates will match correlations computed using the intrinsic coordinates of the shadow manifold. By Lemma 2.1.1, the correlations will be identically \(1\) if a cross map exists.

One alternative to the cosine similarity is the mutual information (MI). Specialized to the current text, the MI \(I(;_{F})\) quantifies the reduction in the uncertainty of \(\) given the pushforward vector \(_{F}\). While the information-theoretic underpinning of the MI is attractive, we have two main reasons to prefer the cosine similarity: First, the MI between two (continuous) distributions can be difficult to interpret. For example, it is not obvious if \(I(;_{F})=0.5\) is a strong dependence or not, particularly in the case of continuous distributions. The cosine similarity, on the other hand, is upper bounded by \(1\), so \((},)=0.95\) is easily interpreted as having near-perfect reconstruction. Second, accurate estimation of the MI from samples is a notoriously difficult task, particularly in high dimensions, and no single estimator works consistently well . We provide some experiments with the MI, and show the differing performance of different estimators, in Appendix B.3.

## 3 Experiments

We validate the performance of TSCI on two datasets that are popular in the literature. The first arises from a coupled Rossler-Lorenz system, where the ground truth causality is known and the causal influence can be smoothly modulated. The second example was proposed in  and uses sporadic time series from coupled double pendulums, and illustrates the applicability of TSCI to extensions of CCM. Code implementing TSCI is available at [https://github.com/KurtButler/tangentspaces](https://github.com/KurtButler/tangentspaces). All comparisons to CCM use the skccm module in Python5. Experiments were run on a 6-Core Intel Core i5 and NVIDIA Titan RTX.

Figure 2: Shadow manifolds \(_{x}\) and \(_{y}\) from the unidirectionally coupled Rössler-Lorenz system Eq. (13) with \(C=1\), and the corresponding histograms of \(}^{}\) and \(}^{}\). The test statistics, \(r_{X Y}\) and \(r_{X Y}\), correspond to the means of these distributions.

### Unidirectionally-Coupled Rossler-Lorenz System

A common toy system used for studying coupled dynamic systems is a unidirectionally-coupled Rossler-Lorenz system , which we define in Eq. (13). In this system, the first three coordinates (\(z_{1},z_{2},z_{3}\)) describe a Rossler system, and they are unidirectionally coupled with a Lorenz-type system (\(z_{4},z_{5},z_{6}\)). The strength of the coupling is controlled by the parameter \(C\). When \(C=0\), the two systems are disconnected, but for \(C>0\) there is a causal influence from \(z_{1},z_{2},z_{3}\) to \(z_{4},z_{5},z_{6}\).

\[}{dt}=z_{1}\\ z_{2}\\ z_{3}\\ z_{4}\\ z_{5}\\ z_{6}=-6(z_{2,t}+z_{3,t})\\ 6(z_{1,t})+0.2z_{2,t}\\ 6(0.2+z_{3,t}(z_{1,t}-5.7))\\ 10(z_{5,t}-z_{4,t})\\ 28z_{4,t}-z_{5,t}-z_{4,t}z_{6,t}+Cz_{2,t}^{2}\\ z_{4,t}z_{5,t}-8z_{6,t}/3 \]

In Fig. 2, we visualize the TSCI method for \(x=z_{2}\) and \(y=z_{4}\). First, we show the shadow manifolds \(_{x}\) and \(_{y}\) with a set of tangent vectors on each manifold. The delay embedding dimension parameters, \(Q_{x}\) and \(Q_{y}\), were selected using the false-nearest neighbors algorithm with a tolerance of 0.005 .Time lags for the embeddings, \(_{x}\) and \(_{y}\), were selected by picking the minimal delay such that the autocorrelation function drops below a threshold . Additionally, we show the histograms of \(()\), where \(\) is the angle between the tangent vectors at each point. For \(X Y\) direction, which is the true causal direction, the distribution is concentrated near 1. For the \(Y X\) direction, the distribution of tangent vectors is centered on 0. The test statistics, \(r_{X Y}\) and \(r_{Y X}\), correspond to the means of each distribution, and visibly correspond to the correct causality.

We show the effects of varying the coupling strength \(C\) in Fig. 2(a), where TSCI clearly shows better separation across varying \(C\). We see the effect of increasing the library length (i.e., the size of the training set in nearest neighbors) for \(C=1.0\) in Fig. 2(b). Here, \(r_{X Y}\) increases at similar rates for TSCI and CCM, suggesting similar data efficiency of the two methods.

### Double Pendulum System

To illustrate the generality of TSCI within CCM-like frameworks, we applied the TSCI methodology to the latent CCM framework, where CCM is applied to a state-space reconstruction obtained via neural ODEs . One reason to favor this approach is when the observed time series are irregularly (i.e., with a non-uniform sampling rate) or sporadically (i.e., any given observation only measures a subset of states) sampled. Notable for TSCI is the fact that ground-truth derivatives are available, as they are directly learned in the neural ODE reconstruction.

Figure 3: Comparison of TSCI with CCM for the Rössler-Lorenz system (true causation \(X Y\)). The plotted lines show the median test statistic over \(100\) trials for both CCM and TSCI, and the shaded region indicates the \(5\)th and \(95\)th percentiles when (a) \(C\) is varied from \(0\) (no coupling) to \(3\) (approximate general synchrony), and (b) \(C\) is fixed to \(1.0\) and the library length is varied.

We replicate an experiment in the latent CCM paper where a network of unidirectionally coupled double pendulums are simulated, and observations sampled irregularly and sporadically. The authors' source code6 was used to generate data and apply latent CCM. For more information on the data generation, see Appendix A of .

For TSCI, we learn a cross-map using an MLP between the reconstructed state spaces. To avoid tuning learning rates for every network, the parameter-free COCOB optimizer 7 and networks were trained for \(50\) epochs. Results can be found in Table 1.

When compared to latent CCM, TSCI provided larger correlation coefficients for the true positive cases, with similarly small coefficients for the true negative cases. The use of MLPs for learning the cross map partially but not fully explains the difference in performance.

### Additional Experiments

Several additional experiments appear in Appendix B. We briefly describe them here and provide some commentary on their results.

Varying the embedding dimension.In Appendix B.1, we artificially lower quality embeddings by varying the embedding dimension. We find that the performance of TSCI and CCM similarly degrade when using an embedding dimension that is very small, and that there is little harm to a larger embedding dimension. In either case, the hyperparameters from a false-nearest neighbors test perform well.

Corrupting signals.In Appendix B.2, we lower the embedding quality in two different ways: (1) by injecting additive noise to all observations, and (2) by adding a sine wave to observations. We observe that TSCI and CCM similarly degrade when the signal-to-noise ratio is altered. However, because of the larger separation in TSCI, the correct causal relation can be determined at much lower signal-to-noise ratios.

Using mutual information.In Appendix B.3, we experiment with using mutual information instead of cosine similarity. We find that conclusions made using MI are generally similar to those from cosine similarity, but with less interpretability and significant difficulties in estimation.

Comparisons to other causal discovery methods.As mentioned in the introduction, CCM (and hence TSCI) are specifically designed to address failures of Granger causality. In Appendix B.4, we empirically verify the limitations of Granger causality on our Rossler-Lorenz toy system. We additionally test several other causal discovery methods and show their limitations in our setting.

## 4 Advantages and Limitations of TSCI

Scalability.The TSCI approach, using the \(K\)-nearest neighbors algorithm, retains the scalability and lightweight implementation that CCM enjoys. The only additional computational complexity arises

    &  X_{2}}\)} \\  Direction & Latent CCM & Latent CCM (MLP) & Latent TSCI (MLP) \\  \(X Y\) & \(0.011 0.009\) & \(0.015 0.009\) & \(0.021 0.010\) \\ \(X Z\) & \(0.044 0.008\) & \(0.055 0.011\) & \(0.077 0.013\) \\ \(Y X\) & \(-0.003 0.008\) & \(-0.005 0.009\) & \(-0.010 0.004\) \\ \(Y Z\) & \(0.003 0.006\) & \(-0.002 0.004\) & \(-0.008 0.013\) \\ \(\) & \(0.737 0.019\) & \(0.747 0.015\) & \(0.915 0.006\) \\ \(\) & \(0.475 0.043\) & \(0.578 0.030\) & \(0.612 0.053\) \\   

Table 1: Double Pendulum Experiments. Entries denote the mean \(\) one standard deviation across \(5\) folds. Bolded directions indicate ground truth causality.

from solving a local linear system of equations, and from the estimation of derivatives. Since the linear systems have a fixed size \(K\), and since derivative estimation can be done using a linear filter, the additional cost is minimal. The general version of TSCI will scale depending on the choice of the regressor used to learn the cross map, and on other design decisions made to improve the efficacy of the method. However, this is not unlike the situation in which CCM is augmented with other customization options that potentially slow down the method.

**Model agnosticism.** Because a TSCI test can be formulated for any differentiable regression model used to learn the cross map, the approach is highly flexible and model agnostic. Additionally, because there are many potential ways in which reconstruction of latent states and learning of the velocity vector fields can be improved, the TSCI method can be incorporated into a wide variety of inference frameworks.

**Quality of reconstructed states.** In both CCM and TSCI, there are a few assumptions which warrant some justification before use, since their violation may yield to misapplication of cross map methods. The first issue is that while Takens' theorem implies that embeddings are plentiful, not all embeddings are equal in quality or useful. Shadow manifolds that are sparsely or incompletely sampled, time series with trends, or otherwise data which do not accurately capture the latent manifold can lead to dubious results using CCM . While TSCI is less likely to produce a false positive in these cases, the assumption that a latent manifold is well-represented by a given embedding is nontrivial and critical to ensuring trustworthy performance of the method.

**General synchrony.** General synchrony is a problem that plagues all cross map-based methods . The issue is that when the causal strength of the relationship \(x y\) is very strong, the influence of \(_{x}\) dominates the dynamics of \(_{y}\), and \(_{y}\) cannot exhibit its own independent behavior. As a result, \(_{y}\) will look similar to \(_{x}\), and the cross map will become an invertible function. As a result, a strong unidirectional relationship is detected as a bidirectional causal relationship. In Fig. 3, the Rossler-Lorenz system enters general synchrony near \(C=3\). The TSCI method appears more resistant to the effects of general synchrony than CCM, but it is a topological fact that as \(C\) grows, synchrony becomes inevitable.

## 5 Conclusion

In this paper, we presented the TSCI method for detecting causation in dynamical systems. By considering how tangent vectors map from one manifold to another, we may achieve more robust detection of causal relationships in dynamical systems than with standard CCM. Key advantages of TSCI include that we may use it in many systems in which CCM would be applied, but the method is far less prone to false positives and spurious causation. We presented both a general form of the algorithm as well as a \(k\)-nearest neighbor version inspired by the original CCM algorithm. Because TSCI requires us to estimate latent states and their time derivatives, there us much room for the TSCI method to be further developed in future work.