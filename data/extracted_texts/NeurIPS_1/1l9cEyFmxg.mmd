# Unlocking the Capabilities of Masked Generative Models for Image Synthesis via Self-Guidance

Jiwan Hur\({}^{1}\)  Dong-Jae Lee\({}^{1}\)  Gyojin Han\({}^{1}\)  Jaehyun Choi\({}^{1}\)  Yunho Jeon\({}^{2}\)\({}^{}\)  Junmo Kim\({}^{1}\)\({}^{}\)

\({}^{1}\)KAIST, South Korea   \({}^{2}\)Hanbat National University, South Korea

{jiwan.hur, jhtwosun, hangj0820, chlwogus}@kaist.ac.kr

yhjeon@hanbat.ac.kr, junmo.kim@kaist.ac.kr

Code is available at: [https://github.com/JiwanHur/UnlockMGM](https://github.com/JiwanHur/UnlockMGM)

###### Abstract

Masked generative models (MGMs) have shown impressive generative ability while providing an order of magnitude efficient sampling steps compared to continuous diffusion models. However, MGMs still underperform in image synthesis compared to recent well-developed continuous diffusion models with similar size in terms of quality and diversity of generated samples. A key factor in the performance of continuous diffusion models stems from the guidance methods, which enhance the sample quality at the expense of diversity. In this paper, we extend these guidance methods to generalized guidance formulation for MGMs and propose a self-guidance sampling method, which leads to better generation quality. The proposed approach leverages an auxiliary task for semantic smoothing in vector-quantized token space, analogous to the Gaussian blur in continuous pixel space. Equipped with the parameter-efficient fine-tuning method and high-temperature sampling, MGMs with the proposed self-guidance achieve a superior quality-diversity trade-off, outperforming existing sampling methods in MGMs with more efficient training and sampling costs. Extensive experiments with the various sampling hyperparameters confirm the effectiveness of the proposed self-guidance.

Figure 1: **Comparison of sampled images using 18-step MaskGIT  without (top) and with the proposed self-guidance (bottom) on ImageNet 512\(\)512 (left) and 256\(\)256 (right) resolutions. Each paired image is sampled using the same random seed and sampling hyperparameters. The proposed self-guidance effectively improves the capabilities of the masked generative models.**

[MISSING_PAGE_EMPTY:2]

Experimental results demonstrate that the proposed guidance effectively improves the sample quality with only 10 epochs of fine-tuning. Notably, combined with a high sampling temperature, the proposed guidance not only improves the quality of generated samples but also keeps diversity high, showing a superior quality-diversity trade-off compared to other improved sampling techniques for MGMs and other generative families with similar model sizes.

## 2 Background

### Masked Generative Models

Discrete diffusion models  aim to generate categorical data \(_{0}^{N}\) with a length of \(N\) and \(K\) categories. The forward Markov process \(q(_{t+1}|_{t})\) gradually corrupts data \(_{0},_{1},...,_{T}\) until the marginal distribution of \(_{T} p(_{T})\) becomes stationary. Then starting from the \(_{T}\), the learned reverse Markov process \(p_{}(_{t-1}|_{t})\) gradually recovers the corrupted data to generate \(_{0}\).

Masked Generative Models (MGMs) are a family of discrete diffusion models that use an _absorbing state diffusion process_, in which they gradually mask the input tokens by replacing them with a mask token ([MASK]) and learn to predict the masked region. Recently, vector-quantized (VQ) image token-based MGMs  have achieved a superior trade-off between sampling time and quality for image synthesis compared to Gaussian (continuous) diffusion models  through the non-autoregressive parallel decoding process. MGMs are trained to predict the masked region similar to BERT  in natural language processing (NLP), but with various masking ratios \(_{t}(0,1]\), where \(\) is pre-defined mask scheduling function that masks \(n=[(t/T) N]\) tokens from \(N\) total tokens. Without loss of generality, we assume the unmasked region of the mask \(_{t}\) is 1, and the masked region is 0. Then, given external condition \(c\) (e.g., class or text) and masked input \(_{t}=_{0}_{t}\), where the mask \(_{t}\) is randomly sampled according to masking ratio \(_{t}\), MGMs are trained to predict clean data \(_{0}\) (or equivalently, to predict the masked regions) by the objective function

\[_{mask}=-_{,t}[_{ i[1,N],_ {i}=0} p_{}(_{0}^{i}|_{t},c)]. \]

Here, we denote the \(i\)-th token of \(_{0}\) as \(_{0}^{i}\).

After the training, to sample the image \(_{0}\), MGMs typically use an iterative prediction-masking procedure starting from the blank canvas \(_{T}\) (i.e., all tokens are masked). Given (partially) masked image token \(_{t}\) sampled from the previous step, MGMs first predict all tokens \(}_{0,t} p_{}(}_{0,t}|_{t})\) simultaneously, where \(}_{0,t}\) denotes the prediction of \(_{0}\) at timestep \(t\) and we omit \(c\) for simplification. Then \(}_{0,t}\) is masked according to masking ratio \(_{t-1}\) to obtain \(_{t-1}\). MGMs usually sample \(_{t-1} p(_{t-1}|_{t},}_{0,t})\) which is equivalent to sample \(_{t-1}\) since \(_{t-1}=_{t-1}}_{0,t}\). Randomly sampling the \(_{t-1}\) can be one choice; however, the prediction \(}_{0,t}\) may have numerous errors, thus randomly selecting the mask can produce sub-optimal results by potentially masking relatively

Figure 2: Visualization of the effect of guidance using spatial smoothing (SAG)  and the proposed semantic smoothing. We tokenize the input image using VQGAN  encoder, mask the 90% of VQ tokens, and predict \(}_{0,t}\) using MaskGIT . With the proposed self-guidance leveraging semantic smoothing, generated sample quality is improved by enhancing fine-scale details.

accurate tokens while leaving unrealistic tokens unmasked. To overcome this, various research adopts improved sampling methods such as utilizing the output confidence of the \(}_{0,t}\) since confident tokens tend to be more accurate  or train an external corrector to classify the realistic tokens [35; 36].

However, these improved sampling may impose a problem, named _multi-modality problem_ that is a well-known problem in non-autoregressive parallel sampling [15; 39; 60]. Given the input such as \(_{t}\) and class condition \(c\), the model can have multiple plausible outputs, which brings challenges to the non-autoregressive model as they generate each token independently. In an extreme case where the input token is all masked, and the model predicts output only using a given external condition such as class, each token can predict _easy_ token more confidently and correctly, such as a background in every token. As a result, correctness-based sampling may result in images only filled with background images. To resolve this problem, various MGMs adopt additional randomness to sample \(_{t}\), such as sampling with temperature [4; 35; 36]. Let the \(l_{t}\) measure the realism of sampled tokens \(}_{0,t}\), such as the confidence scores of sampled tokens in MaskGIT . Then MGMs sample \(_{t-1}\) by selecting top-\(k\) elements of \(_{t}=l_{t}+(t/T)\) according to \(_{t}\), where \(\) denotes the sampling noise such as i.i.d. Gumbel noise and \(\) is temperature scale that is annealed according to the timesteps. Generally, high-temperature sampling results in more diverse samples while degrading the sample quality.

### Sampling Guidance

Iterative sampling processes of diffusion models are often guided by external networks or themselves. Therein, salient information-based guidance has been actively explored in continuous diffusion models [19; 22; 52] for high-quality image synthesis. Let \(h_{t}\) be salient information of \(_{t}\) and \(}_{t}\) be a perturbed sample that lacks \(h_{t}\). \(h_{t}\) can be internal information within \(_{t}\) or an external condition, or both. Lee et al.  proposed a general guidance technique that guides the sampling process toward enhancing the information \(h_{t}\), and the equation for the sampling process is:

\[(}_{t},h_{t})=(}_{t})+(1+s)( (}_{t},h_{t})-(}_{t})), \]

where \(\) is a score function, \(\) is a guided score function of the continuous diffusion model, and \(s\) is a guidance scale. For instance, in the setting \(h_{t}=c\) and \(}_{t}=_{t}\), the Eq. (2) collapses to classifier-free guidance (CFG) , which guides the sampling toward the given class distribution. Lee et al.  propose using adversarial blurring, enhancing the fine-scale details of the sample. Generally, using a large guidance scale \(s\) enhances the quality of generated samples while reducing the diversity.

Recently, discrete CFG [52; 5] has been introduced to improve the correlation between the input class or text condition \(c\) and the images generated by discrete diffusion models as below equation:

\[ p(}_{t})= p(_{t})+(1+s)( p(_{t}|c)- p (_{t})), \]

where \(}_{t}\) denotes the guided token.

Unlike CFG in the continuous domain, discrete CFG estimates the probability distribution \(p(_{t}|c)\) directly. However, it requires a specific training strategy and paired labels such as class and text.

### Parameter-Efficient Fine-Tuning

MGMs across various domains adopt transformer architecture due to their superior ability to handle context with bidirectional attention [9; 13; 4; 57; 63]. However, training a transformer from scratch requires significant computational resources due to the quadratic complexity of the attention mechanism. In recent years, parameter-efficient fine-tuning (PEFT) techniques have received significant attention, especially in light of the growing size and complexity of pre-trained models. PEFT adapts large pre-trained models to specific tasks or datasets by tuning a small portion of parameters [58; 55] or introducing task-specific parameters [45; 23; 49], effectively transferring knowledge without extensive retraining. Notably, by preserving most of the parameters, the fine-tuned model effectively preserves knowledge with few forgetting.

## 3 Methods

### Generalized Information-Based Guidance for Discrete Diffusion Models

Similar to the general guidance in continuous diffusion models in Eq. (2), discrete CFG in Eq. (3) can be extended to the generalized information-based guidance from the optimization perspective. Givensome salient information \(h_{t}\), we aim to sample \(_{t}\) which maximizes \(p(}_{t}|h_{t})\). Simultaneously, for the correlation between the information and sample, \(p(h_{t}|}_{t})\) also needs to be maximized as stated in the equation below which is from Tang et al. :

\[*{arg\,max}_{}_{t}}[ p(}_{t}|h_{t})+s  p(h_{t}|}_{t})]. \]

Using the Bayes' theorem and ignoring the prior probability term for salient information, the optimization goal can be represented as:

\[*{arg\,max}_{}_{t}}[ p(}_{t})+(1+s)( p (}_{t}|h_{t})- p(}_{t}))]. \]

Then, the Eq. (5) guides the sampling process toward enhancing the relevance of the sample and salient information \(h_{t}\).

In the inference stage, various MGMs adopt to predict unmasked state \(}_{0,t}\) rather than directly predicting \(_{t-1}\). If we limit \(h_{t}\) to the internal information of \(_{t}\) to make \(h_{t}\) removable from the \(_{t}\) through an information bottleneck module \(_{}\); in other words, if \(}_{t}=_{}(_{t})\) and \(p(_{t})=p(}_{t},h_{t})\) get satisfied, we can sample next state for the denoising step as below:

\[ p_{}(}_{0,t}|_{t})= p_{}(}_ {0,t}|_{}(_{t})))+(1+s)( p_{}(}_{0,t }|_{t})- p_{}(}_{0,t}|_{}(_{t}) )), \]

when a MGM with parameter \(\) predict \(}_{0,t}\) from \(_{t}\) and \(}_{0,t}\) from \(_{}(_{t})\). This implies that by defining the information bottleneck module \(_{}\) that can selectively subtract salient information \(h_{t}\) from \(_{t}\) in the discrete space, we can guide the sampling of MGMs in a direction that enhances \(h_{t}\). Since we aim to improve the sample quality of MGMs by presenting a novel guidance method, it is necessary to define \(_{}\) that can remove information about the fine details of the samples. For continuous diffusion models, utilizing samples spatially smoothed by Gaussian blur for guidance has been helpful in improving sample quality by restricting fine-scale information . This motivates us to investigate guidance with smoothed output for discrete domains, especially for VQ tokens.

### Auxiliary Task Learning for Semantic Smoothing on VQ Token space

We aim to apply smoothing, such as Gaussian blur, for VQ tokens, as discussed in the previous section, to implement guidance in the discrete domain. However, unlike natural images which have inherent, observable patterns and structures, the latent space of autoencoders often lacks such properties . For instance, two successive tokens in VQ codebooks, such as the 11th and 12th tokens, are not semantically linked. As a result, applying Gaussian blur in the VQ token cannot produce meaningful representations. Nevertheless, we empirically found that applying Gaussian blur in the probability spaces, i.e. blurring the output logits of the generator \(p_{}(}_{0}|_{t})\) similar to Lee et al.  can provide meaningful guidance. However, the improvement is marginal because Gaussian blur is not a suitable information bottleneck for subtracting fine details in VQ token space.

To overcome this, we introduce an auxiliary task designed to leverage semantic smoothing for VQ tokens, a process that selectively removes details such as local patterns while preserving overall information in VQ token space. Specifically, given the masked input \(_{t}\), masked generator predicts \(p_{}(}_{0,t}|_{t})\). We aim to train information bottleneck \(_{}\) to generate the semantically smoothed output \(p_{}(}_{0,t}|_{}(_{t}))\). However, training \(_{}\) directly is challenging, as we cannot define semantically smoothed outputs. To naturally impose a model to generate semantically smoothed output, we leverage error token correction, originally introduced in non-autoregressive machine translation to mitigate the compounding decoding error in the iterative sampling process . During the error token correction, input unmasked tokens are randomly replaced with error tokens, and the model learns to correct them. To be more specific, let \(_{t}\) be a corrupted data where some tokens in \(_{t}\) are replaced with error tokens with some probability \(p\). Then, the objective function for the auxiliary task to update \(\) can be

\[_{aux}=-_{,t}[_{ i[1,N],_{i }=0} p_{}(_{0}^{i}|_{}(_{t}),c)]. \]

From the perspective of _Vicinal Risk Minimization_ (VRM) [6; 59], a vicinity distribution \(p_{}\) minimizes the empirical risk for all data points \(_{t}\) given a vicinity of the data \(_{t}\). Given that randomly replacederror tokens often act as semantic vicinities within the input data, to minimize the overall empirical risk, the model implicitly learns to smooth vicinities of \(_{t}\). This involves leveraging coarse information from the surrounding context while minimizing the fine-scale details in the presence of unknown input errors1. However, training a network from scratch with Eq. (7) does not ensure that the outputs will resemble real images, potentially converging on trivial solutions that may be undesirable, in addition to being computationally expensive.

### Efficient Implementation

To mitigate the aforementioned problem, we adopt a parameter-efficient fine-tuning (PEFT) method to utilize deep image priors in the pre-trained masked generator and to enhance the training efficiency of transfer learning. Among various PEFT methods, we adopt TOAST , which shows favorable performance in various visual and linguistic tasks under transformer architecture. With a frozen pre-trained backbone, TOAST selects task-relevant features from the output and feeds them into the model by adding them to the value matrix of self-attention. This top-down signal steers the attention to focus on the task-relevant features, effectively transferring the model to other tasks without changing parameters.

Besides its effectiveness in transfer learning, TOAST brings more practical strengths to our task. Before the discussion, it is important to note that masked generative models exhibit a strong training bias. Because the training data does not contain error tokens, the model predicts unmasked input as identical and unable to correct error tokens. Since the model is trained only to consider masked regions, we propose to use a blank canvas as an input (i.e., all tokens are masked \(_{T}\)), enabling the model to make corrections in response to error tokens. However, most PEFT methods lack a direct solution for incorporating information about \(_{t}\) when the input is replaced with all masked tokens. On the other hand, we empirically found that simply replacing the input for the second stage of TOAST as \(_{T}\) can mitigate this problem without a performance drop (Fig. 3 a).

Furthermore, the two-stage approach of TOAST can be efficiently implemented in the sampling process. The generator first sample \(p_{}(}_{0,t}|_{t})\) from the input \(_{t}\). The hidden state obtained from the generation stage can be recycled for the second stage to produce guidance logit \(p_{}(}_{0,t}|_{}(_{t}))\)

Figure 3: (a) Fine-tuning the feature selection module \(_{}\) (TOAST ). With the auxiliary objective in Eq. (7), \(_{}\) implicitly learns to smooth erroneous input \(_{t}\) to address semantic outliers (Section 3.2). (b) During the sampling steps, self-guidance can be efficiently implemented by leveraging the feature map from the generative process. \(_{}\) performs semantic smoothing on the input \(_{t}\), guiding the sampling process toward enhancing fine-scale details in the generated sample.

(Fig. 3 b). We note that different from the fine-tuning step where the error tokens \(_{t}\) are used to train \(_{}\), the sampling process directly utilizes \(_{t}\) to generate the semantically smoothed tokens \(_{}(_{t})\).

## 4 Related Works

**Generative Adversarial Networks (GANs).** Trained by adversarial objective , GANs have shown impressive performance in image synthesis with only 1-step model inference [27; 28; 47; 3; 33]. However, despite their practical performance, the training instability and limited mode coverage caused by the adversarial objective [46; 62] is still a bottleneck for their broader applications.

**Diffusion Models.** Recent diffusion models in continuous space mostly utilize Gaussian noise to perturb the input data and learn to predict clean data from the noisy data . After the advent of DDPM , diffusion models have rapidly grown with architecture improvements [38; 41], improved training strategies [29; 7; 25], improved samplings [51; 37], latent space models , and sampling guidance [10; 19; 22], outperforming various generative families such as GANs in various domains.

Masked Generative Models.With the recent success of transformers  and GPT  in NLP, generative transformers have also been adopted in image synthesis. To mimic the generative process of transformers for discrete embedded natural language, recent studies encode images into quantized visual tokens using the VQ-VAE encoder [43; 11] and apply the generation process in an autoregressive [32; 56] or non-autoregressive manner [4; 35; 36; 5]. In particular, non-autoregressive generation shows a better trade-off between generation quality and speed. Nevertheless, they suffer from _compounding decoding errors_, which means that small decoding errors in early generation steps can accumulate into large differences in later steps. To address this issue, Token-Critic  and DPC  use additional transformers to identify more realistic tokens. However, they require a training second transformer, which incurs a training cost similar to training a generator from scratch.

The non-autoregressive predict-mask sampling of MGMs can be regarded as a discrete diffusion process that uses _absorbing state diffusion process_[1; 36]. Similar to MGMs, VQ-Diffusion  proposed a mask-and-replace diffusion strategy to predict masked tokens. Improved VQ-Diffusion  adopt purity-based sampling with discrete CFG to further improve the sample quality.

    & &  &  \\  Model & Type & NFE & FID\(\) & IS\(\) & Prec\(\) & Rec\(\) & FID\(\) & IS\(\) & Prec\(\) & Rec\(\) \\  BigGAN-deep  & GANs & 1 & 6.95 & 224.5 & **0.89** & 0.38 & 8.43 & 177.9 & 0.85 & 0.25 \\ GigaGAN  & GANs & 1 & 3.45 & 225.5 & 0.84 & 0.61 & \(-\) & \(-\) & \(-\) & \(-\) \\  ADM  & Diff. & 250 & 10.94 & 101.0 & 0.69 & **0.63** & 23.24 & 58.0 & 0.73 & **0.60** \\ ADM (+ SAG)  & Diff. & 500 & 9.41 & 104.7 & 0.70 & 0.62 & \(-\) & \(-\) & \(-\) & \(-\) \\ CDM  & Diff. & 250 & 4.88 & 158.7 & \(-\) & \(-\) & \(-\) & \(-\) & \(-\) & \(-\) \\ LDM-4  & Diff. & 250 & 10.56 & 103.4 & 0.71 & 0.62 & \(-\) & \(-\) & \(-\) & \(-\) \\ LDM-4 (+ CFG)  & Diff. & 500 & 3.60 & 247.7 & \(-\) & \(-\) & \(-\) & \(-\) & \(-\) & \(-\) \\ DiT-\(l\)2\({}^{}\) (+ CFG)  & Diff. & 500 & 5.02 & 167.2 & 0.75 & 0.57 & \(-\) & \(-\) & \(-\) & \(-\) \\  VQVAE-2\({}^{}\) & AR & 5120 & 31.11 & \(\)45 & 0.36 & 0.57 & \(-\) & \(-\) & \(-\) & \(-\) \\ VQGAN\({}^{}\) & AR & \(\)1024 & 18.65 & 80.4 & 0.78 & 0.26 & 7.32 & 66.8 & 0.73 & 0.31 \\  VQ-Diffusion  & Discrete. & 100 & 11.89 & \(-\) & \(-\) & \(-\) & \(-\) & \(-\) & \(-\) & \(-\) \\ ImprovedVQ. (+ CFG)  & Discrete. & 200 & 4.83 & \(-\) & \(-\) & \(-\) & \(-\) & \(-\) & \(-\) & \(-\) \\  MaskGIT\({}^{*}\) & Mask. & 18 & 6.56 & 203.6 & 0.79 & 0.48 & 8.48 & 167.1 & 0.78 & 0.46 \\ Token-Critic  & Mask. & 36 & 4.69 & 174.5 & 0.76 & 0.53 & 6.80 & 182.1 & 0.73 & 0.50 \\ DPC-light  & Mask. & 66 & 4.8 & 249.0 & 0.80 & 0.50 & 6.09 & 228.1 & 0.81 & 0.46 \\ DPC-full  & Mask. & 180 & 4.45 & 244.8 & 0.78 & 0.52 & 6.06 & 218.9 & 0.80 & 0.47 \\  Ours (T=12) & Mask. & 24 & 3.35 & 259.7 & 0.81 & 0.52 & **5.38** & 226.0 & **0.88** & 0.36 \\ Ours (T=18) & Mask. & 36 & **3.22** & **263.9** & 0.82 & 0.51 & 5.57 & **233.2** & **0.88** & 0.35 \\   

Table 1: Quantitative comparison of various generative models for class-conditional image generation on ImageNet 256\(\)256 and 512\(\)512 resolutions. “\(\)” or “\(\)” indicate lower or higher values are better. \(\): taken from MaskGIT , \(\): taken from VAR , \(\): taken from Token-Critic .

## 5 Experiments

**Datasets, Baselines, and Metrics.** We demonstrate the effectiveness of the proposed guidance for masked generative models on class conditional generation using the Imagenet benchmark  with \(256 256\) and \(512 512\) resolutions. For a baseline model, we use MaskGIT , which shows a state-of-the-art trade-off between quality and sampling speed on a class conditional generation of MGMs and has publicly available checkpoints for our target datasets. To evaluate the trade-off between sample fidelity and diversity, we measure Frechet Inception Distance (FID) , Inception Score (IS) , Precision, and Recall  using the implementation provided by Dhariwal et al. . To measure the computational cost, we report the number of function evaluations (NFE) required to sample an image. Note that total sampling timesteps (\(T\)) may differ from NFE due to guidance.

**Implementation Details.** We use a VQGAN tokenizer  provided by MaskGIT , which encodes images into 10-bit integers. Since the training code for MaskGIT is unavailable, we implement based on the open Pytorch reproduction . We follow the previous work for error token correction  to prepare input with error tokens and randomly replace 30% of input tokens with error tokens. We utilized an NVIDIA RTX A6000 for fine-tuning and sampling. We used an exponential moving average (EMA) of fine-tuning weights with a decay of 0.9999 and bf16 precision. The batch size was set to 256, and the additional parameters introduced by TOAST are approximately 20-25% of the model size. Notably, the fine-tuning was completed efficiently within 10 epochs. More detailed implementation of TOAST  is provided in the Appendix A. We use sampling step \(T=18\) and sampling temperature \(25\) for Imagenet 256\(\)256 and 45 for Imagenet 512\(\)512. For sampling step \(T=12\), we use temperatures 10 and 20, respectively for each resolution.

### Comparison with Various Generative Models

**Quantitative results.** We compare the performance of the proposed method with various class conditional generative methods. (1) _GANs_: BigGAN  and GigaGAN ; (2) _continuous diffusion models_ (Diff.): ADM , CDM , LDM , and DiT ; (3) _auto-regressive models_ (AR): VQVAE-2  and VQGAN ; (4) _discrete diffusion models_ (Discrete.): VQ Diffusion , and Improved VQ Diffusion ; (5) _masked generative models_ (Mask.): MaskGIT , Token-Critic , DPC . As noted in previous literature [19; 36], sampling using a pre-trained classifier may impact the classifier-based metrics such as FID and IS. Thus, to see the base generative capacity of each method, we compare the models that do not use external pre-trained networks such as a classifier or upsampler during training or sampling. We also exclude large-scale models such as DiT-XL/2  for a fair comparison. Table 1 presents the quantitative results of various generative models with guidance. All values are taken from the original paper unless otherwise noted. The proposed method achieves superior FID and IS despite the low computational cost for sampling. For instance, the proposed method achieves better FID and IS than DiT-L/2 with CFG on Imagenet 256\(\)256 even though the proposed method requires an order of magnitude fewer sampling steps.

Figure 4: IS vs. FID curves of various sampling methods for MGMs on ImageNet 256\(\)256 and 512\(\)512. The curve positioned towards the bottom right indicates a better trade-off between sample quality and diversity. We plot the curve by varying the sampling temperature (\(\)), and the curves of MaskGIT  and Token-Critic  are taken from Token-Critic .

We comprehensively compare the proposed methods with various sampling strategies for MGMs in Fig. 4. We note that all MGMs use the same VQGAN tokenizer  provided in MaskGIT , the same baseline generator , and the same sampling timestep \(T=18\). Although the previous methods, such as Token-Critic  and DPC , require similar or more NFEs to sample images and more training resources to train an external corrector transformer, our simple guidance shows a better trade-off between sample quality and diversity.

**Qualitative results.** Fig. 5 presents the randomly sampled images using LDM  with CFG, MaskGIT, and MaskGIT with the proposed guidance. The proposed guidance sampling enhances details in the generated samples while maintaining high diversity. More sampled results with various classes are provided in the Fig. 1 and Appendix C.

### Ablation Studies and Analysis

In this section, we explore the effectiveness of the guidance on class conditional ImageNet generation in \(256 256\) scale across the various sampling hyperparameters. We vary each sampling parameter and otherwise use the default settings.

**Effectiveness of Auxiliary Task.** To demonstrate that fine-tuning with a proposed auxiliary task using the objective in Eq. (7) can effectively improve the generative capabilities of MGMs, we conduct ablation studies and report the FID and IS in Table 2. As noted in Section 3.2, applying Gaussian blur in the input VQ token does not properly smooth VQ tokens. However, we empirically found that applying Gaussian blur in the output logit can produce meaningful guidance. Therefore, we provide the quantitative comparison with guidance by applying Gaussian blur in the logit space (Blur Guidance). We further utilize self-attention value for adversarial masking following Lee et al. . Blur guidance and SAG enhance the quality (IS) or diversity (FID) in some

    & FID\(\) & IS\(\) \\  Blur Guidance & 8.32 & 231.2 \\ SAG  & 4.73 & 177.3 \\ ft. w/ \(_{mask}\) & 4.11 & 238.4 \\  ft. w/ \(_{aux}\) (Ours) & **3.22** & **263.9** \\   

Table 2: Ablation results of various guidances on ImageNet 256\(\)256 class conditional generation.

Figure 5: Sampled images on ImageNet 256\(\)256 class conditional generation using selected classes (105: Koala, 661: model T, and 933: Cheeseburger). left: LDM  + CFG (s=1.5, NFE=250\(\)2), middle: MaskGIT (NFE=18), right: Ours (s=1.0, NFE= 18\(\)2).

degree, but the improvement is marginal compared to ours. We further compare the results when the TOAST module is fine-tuned with the generative objective in Eq. (1) to verify the effectiveness of the auxiliary task (ft. w/ \(_{mask}\)). Since the TOAST architecture and blank input for the second stage naturally play the role of information bottleneck, the performance has increased. Nevertheless, fine-tuning with the proposed auxiliary loss in Eq. (7) demonstrates its effectiveness, showing superior performance in both metrics.

**Varying the guidance scale (Fig. 6 a).** In line with previous literature on sampling guidance [10; 19; 22], a high guidance scale improves the sample quality while sacrificing diversity. We found that the guidance scale 1.0 shows the best performance in terms of FID score and the best trade-offs. However, strong guidance (\(s>3\)) does not ensure quality improvement, often leading to undesirably highlighted details or saturated colors (Appendix B), similar to the effect of large scale with CFG .

**Varying the sampling temperature (Fig. 6 b).** Compared to MaskGIT  limit their sampling temperature relatively low value (\(=4.5\)), we found that with the proposed guidance, the sample quality can be effectively preserved with a higher sampling temperature. As a result, with a high sampling temperature (\(=25\)), we achieve better quality and diversity compared to MaskGIT.

**Varying the sampling steps \(T\) (Fig. 6 c).** We observed that for higher \(T\), the optimal sampling temperature also increases. For example, at \(T=5\), the best results are achieved with a sampling temperature of 4, while at \(T=18\), the optimal sampling temperature rises to 25. Thus, we plot the best FID and IS for each timestep using different temperatures. Whereas the optimal performance-efficiency trade-off of MaskGIT is observed around \(T=8\), ours shows such "sweet spot" around \(T=18\). Up to this point, both quality and diversity increase as \(T\) increases, consistently outperforming MaskGIT. Furthermore, with fewer NFEs (\(T=5\)), the proposed method achieves an FID of 4.84 and an IS of 249.9, outperforming the MaskGIT samples with \(T=18\). This demonstrates that the proposed guidance technique provides more scalable and efficient sampling for MGMs.

**Varying fine-tuning epochs (Fig. 6 d).** We found that the proposed fine-tuning is efficiently trained within 10 epochs and that no further fine-tuning is required to achieve better results.

## 6 Conclusion and Future Work

In this paper, we define generalized guidance for discrete diffusion models, as a counterpart for guidance in continuous domain . To generate guidance, we propose an auxiliary task to apply semantic smoothing in VQ tokens. Experimental results show that the proposed guidance effectively and efficiently improves the generative capabilities of MGMs on class conditional image generation.

**Future work.** Although the proposed guidance can improve the generative capabilities of MGMs on class conditional image generation, there is still room for improvement in various aspects: (1) experiments on large-scale text conditional generative models , (2) generalization to discrete diffusion models , and (3) generalization to various domains such as audio , and video .

**Societal impacts.** While our research does not directly touch ethical issues, the rapid growth of generative models raises considerations in AI ethics (e.g., potential misuse in creating deepfakes , generating antisocial content , or vulnerabilities to adversarial attacks [61; 17; 30]).

Figure 6: Exploring the sampling hyperparameters by varying (a) guidance scale, (b) sampling temperature, (c) sampling timesteps, and (d) fine-tuning epochs.