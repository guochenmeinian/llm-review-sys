# Pandora's Box: Towards Building Universal Attackers against Real-World Large Vision-Language Models

Daizong Liu\({}^{1}\), Mingyu Yang\({}^{2}\), Xiaoye Qu\({}^{2}\), Pan Zhou\({}^{2}\), Xiang Fang\({}^{3}\), Keke Tang\({}^{4}\), Yao Wan\({}^{2}\), Lichao Sun\({}^{5}\)

\({}^{1}\)Peking University \({}^{2}\)Huazhong University of Science and Technology

\({}^{3}\)Nanyang Technological University \({}^{4}\)Guangzhou University \({}^{5}\)Lehigh University

dzliu@stu.pku.edu.cn, {mingyu_yang,xiaoye,panzhou,vanyao}@hust.edu.cn

xfang9508@gmail.com,tangbohutth@gmail.com,lis221@lehigh.edu

Corresponding author: Pan Zhou. This work is supported by National Natural Science Foundation of China (NSFC) under grant No. 62476107.

###### Abstract

Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities across a wide range of multimodal understanding tasks. Nevertheless, these models are susceptible to adversarial examples. In real-world applications, existing LVLM attackers generally rely on the detailed prior knowledge of the model to generate effective perturbations. Moreover, these attacks are task-specific, leading to significant costs for designing perturbation. Motivated by the research gap and practical demands, in this paper, we make the first attempt to build a universal attacker against real-world LVLMs, focusing on two critical aspects: (_i_) restricting access to only the LVLM inputs and outputs. (_ii_) devising a universal adversarial patch, which is task-agnostic and can deceive any LVLM-driven task when applied to various inputs. Specifically, we start by initializing the location and the pattern of the adversarial patch through random sampling, guided by the semantic distance between their output and the target label. Subsequently, we maintain a consistent patch location while refining the pattern to enhance semantic resemblance to the target. In particular, our approach incorporates a diverse set of LVLM task inputs as query samples to approximate the patch gradient, capitalizing on the importance of distinct inputs. In this way, the optimized patch is universally adversarial against different tasks and prompts, leveraging solely gradient estimates queried from the model. Extensive experiments are conducted to verify the strong universal adversarial capabilities of our proposed attack with prevalent LVLMs including LLaVA, MiniGPT-4, Flamingo, and BLIP-2, spanning a spectrum of tasks, all achieved without delving into the details of the model structures.

## 1 Introduction

Recently, Large Vision-Language Models (LVLMs) have achieved significant success and demonstrated promising capabilities in various multimodal downstream tasks, such as text-to-image generation , visual question-answering , and _etc._ Benefiting from the strong comprehension of large language models (LLMs) , LVLMs  on top of LLMs show superior performances in solving complex vision-language tasks by utilizing appropriate human-instructed prompts. However, with the exponential expansion of downstream applications in the real world, LVLMs can be easily fooled by adversarial samples, posing crucial safety issues .

Existing LVLMs attackers  generally craft and add perturbations/triggers to benign image/text inputs. By adversarially manipulating LVLMs to concentrate on specific perturbations or triggers,attackers can cause the models to generate erroneous or jailbreak results, thus presenting a risk to security. Specifically, most of these attackers [25; 26; 28; 30; 31; 32] are simply deployed in the white-box setting, where they have the full knowledge of LVLMs models including network structure and parameter weights to back-propagate gradients for optimizing perturbations/triggers. To alleviate this reliance on model details to a certain extent, some gray-box attackers [24; 27] solely require access to the visual encoder of LVLMs and directly generate the perturbed visual representations to mislead the latter reasoning process. There are also a few works [33; 29] claim that they have successfully achieved more challenging black-box attacks, however, they still need the prior knowledge of additional large models like CLIP [34; 35; 36] to serve as surrogate models, or even rely on the model output scores/logits to generate perturbation gradients.

Although the above attackers demonstrate significant performance against LVLMs, as shown in Figure 1, we argue that they fail to consider the essential characteristics of attack practicality and universality among various realistic downstream multimodal tasks: (1) Existing white-, gray- and black-box methods severely rely on the prior model knowledge, making the attacks less practical since most real-world LVLM applications will not disclose their model details with users. Under such circumstances, the attackers can only query LVLMs to obtain corresponding output results, making it challenging to steer the adversarial perturbations in the correct optimization direction during the gradient estimation process. (2) LVLMs demonstrate impressive versatility in addressing diverse vision-language tasks through varying prompts. However, the current attackers targeting LVLMs can only produce adversarial examples to deceive a particular task within a singular process. Consequently, to compromise different downstream tasks, they must generate distinct adversarial perturbations, which incur significant time and resource expenditure. Therefore, it is efficient and effective to design a universal perturbation for all samples across different tasks. Upon applying this universal perturbation to any input sample, regardless of the task, it has the capability to mislead the LVLM into predicting a target label specified by the attacker.

To this end, in this paper, we make the first attempt to explore task-agnostic adversarial perturbations and build a universal attacker against LVLMs in a challenging yet realistic setting, where the attackers have no prior LVLMs' knowledge. To make the perturbation universally adversarial to multiple LVLM-driven tasks, we design a special patch-wise perturbation pattern by first initializing it on a fixed location of various image inputs and then optimizing it against images of different multimodal tasks. Since we can solely query the LVLM model, we propose a novel importance-aware gradient approximation strategy to adaptively estimate and adjust the weights on gradient directions for optimizing the patch with different additive noises. Hence, gradient directions from sampled noise that can increase the semantic distance between their output and the target label are enlarged, while other directions are reduced. Furthermore, we design a judge function to assess the LVLMs' output for achieving the targeted attack. In this manner, the proposed attack can generate a universal adversarial patch to mislead the understanding process of the multi-task LVLMs by solely querying the model.

Our contributions can be summarized as follows: (1) To the best of our knowledge, we are the first to investigate the vulnerability of real-world LVLMs in a practical but challenging setting, where the attackers can only access the input and output of the LVLM. (2) To further break through the bottleneck of existing task-specific LVLM attack design, we devise a universal adversarial patch (task-agnostic) that can be pasted and then fool any inputs for any LVLM downstream task. (3) A novel importance-aware gradient approximation strategy is also introduced to optimize the adversarial patch by solely querying the LVLM model. (4) Extensive experiments are conducted to verify the effectiveness of our attack approach on various LVLMs and tasks.

Figure 1: Our attacker has no access to the model details of the LVLM. Meanwhile, we design a universal noise that is adversarial to multiple LVLM-driven tasks.

Related Work

**Large vision-language models.** The breakthrough of Large Language Models (LLMs) in language-oriented tasks  and the emergence of GPT-4  motivate researchers to harness the powerful capabilities of LLMs to assist in various tasks across multimodal scenarios, and further lead to the new realm of Large Vision-Language Models (LVLMs) . There have been different strategies and models to bridge the gap between text and other modalities. Some works  leverage learnable queries to extract visual information and generate language using LLMs conditioned on the visual features. Models including MiniGPT-4 , LLaVA  and PandaGPT  learn simple projection layers to align the visual features from visual encoders with text embeddings for LLMs. Also, parameter-efficient fine-tuning is adopted by introducing lightweight trainable adapters into models . Several benchmarks  have verified that LVLMs show satisfying performance on visual perception and comprehension.

**Adversarial robustness of LVLMs.** Despite achieving impressive performance, LVLMs still face issues of adversarial robustness due to their architecture based on deep neural networks . Multiple primary attack attempts have been conducted to study the robustness of LVLMs from different aspects. Inspired by the adversarial vulnerability observed in vision tasks, most methods  evaluates the adversarial robustness of LVLMs under white-box settings, where they have the full knowledge of LVLMs models including network structure and weights. To generate the adversarial examples, they simply add and optimize imperceptible perturbations/triggers to benign image/text inputs via back-propagation. To reduce the reliance on model knowledge, some gray-box attackers  solely require access to the visual encoder of LVLMs and directly generate the perturbed visual representations to fool the latter process. There are also a few works  conduct transfer-based attacks. These exploratory works demonstrate that LVLMs still face stability and security issues under adversarial perturbations. However, existing attack methods only consider popular open-source models, but do not study real-world LVLMs applications (_i.e._, users can not access to any details of the model and can only query the model to obtain corresponding output). Moreover, they are implemented as task-specific settings, and they have to generate different adversarial perturbations for each downstream task of LVLMs, costing much time and resources. Therefore, both the attack practicality in real-world setting and the attack universality across multiple tasks/prompts make LVLMs more challenging to attack.

## 3 Method

In this section, we will first introduce the fundamental preliminary, and then describe the baseline approach for our universal attack and illustrate how we construct the universal adversarial patch by solely querying the LVLM model, respectively. The overall pipeline is illustrated in Figure 2.

### Preliminary

We define \(f_{}(;)\) as a pre-trained large vision-language model (LVLM), parameterized by \(\). Here, \(\) denotes the image modality input, \(\) represents the textual modality input, and \(\) signifies the textual output of the model. Specifically, for the general LVLM downstream tasks, \(\) is a sample from the image set \(\), while \(\) is one of the textual prompt instances from a task prompts set \(\), such as Visual Question Answering (VQA), Image Captioning, and Image Classification, etc.

**Threat model.** In this paper, we explore the scenario of attacking real-world LVLM models, where we assume that the attacker has no knowledge of the victim model, including its parameters, training procedure, original training data, etc. In particular, distinct from the approach utilized in white-/gray-box attacks, we cannot access the model's gradient information to train perturbations through back-propagation. Moreover, unlike in black-box attacks, we are precluded from acquiring confidence scores or logits derived from the model's outputs. The attacker is limited to receiving only the text output returned by the LVLM following a query as feedback. This setting aligns more closely with the real-world practice of utilizing APIs to access LVLMs.

**Attacker's goal.** The objective of the attacker is to devise a universal adversarial patch, represented as \(\), that, by partially covering the original image \(\), generates an adversarial example \(^{}\). This adversarial example, upon application to any input sample across diverse downstream tasks, is designed to compel the LVLM to output a target label predetermined by the attacker. Thus, such a patch needs to exhibit persistence and robustness when deployed on unseen inputs, and to induce adversarial semantic alterations across different tasks for the same image, rendering the patch cross-task cross-image applicable. In this paper, we focus on the challenging targeted attack, which implies that the LVLM will produce an attacker-chosen textual output \(^{}\) given the adversarial image \(^{}\) and benign prompt \(\), formulated as \(f_{}(^{},)^{}\). To be specific, we denote the textual prompts corresponding to different tasks for the input image \(\) as \(_{k}_{k}\), where \(k\) distinguishes the tasks. Our attack goal can thus be expressed as:

\[f_{}(^{},_{k})^{} s.t. {v}^{}=(1-)+. \]

Here, \(\) denotes the Hadamard product. \(\) denotes the patch mask being a 0-1 matrix matching the shape of \(\). Specifically, the number and placement of 1 in \(\) indicate the actual size of the noise and the relative position of the patch \(\) on \(\).

### Baseline Approach for Universal Adversarial Attack

Why do we need universal adversarial perturbation?Most existing attacks against LVLMs generally optimize each adversarial sample based on a specific given origin input. In other words, their optimized noise added to different adversarial samples relative to the origin sample varies. Furthermore, the adversarial examples they optimize for are also task-specific, meaning they cannot consistently have an adversarial impact on all downstream tasks. Therefore, we attempt to find that if a universal perturbation (task-agnostic) could be found, adding it to different benign images would consistently produce effective adversarial samples and impact all downstream tasks. This approach would significantly reduce the time and resources while enhancing the robustness and generalizability of real-world LVLM attacks. In this work, we define such perturbation as a universal adversarial patch \(\), which can be overlaid at a fixed position on a clean image \(\). When combined with prompts \(_{k}\) from different tasks, it consistently prompts the victim model \(f_{}\) to output targeted text \(^{}\).

Universal adversarial objective with targeted label measurement.Unlike attacks on general classification tasks, the outputs of LVLMs under different task prompts are not simply binary (true or false), but rather entail semantically rich natural language descriptions. Therefore, to guide the LVLMs outputting attackers' desired target labels, we need to design a strategy to assess whether the

Figure 2: Overview of the proposed universal adversarial attack against real-world LVLM models. To make the perturbation universally adversarial to multiple LVLM downstream tasks, we design a special patch-wise perturbation pattern by first initializing it on a fixed location of various image inputs and then optimizing it against images of different tasks. To update the adversarial patch by solely querying the LVLM, we introduce a language-based judge model to evaluate the LVLM output and design a novel importance-aware gradient approximation strategy to adaptively estimate gradients and adjust weights on gradient directions for optimizing the perturbations on input samples.

output of the victim LVLM model after being attacked aligns with the attacker's preset conditions, or to measure the distance between \(\) and \(}\). Inspired by the previous work , we construct a judge function \(}\) based on a simple and lightweight pre-trained text encoder \(g_{}\), which serves a role akin to the regularization loss function. The encoder \(g_{}\), parameterized by \(\), transforms natural language texts \(\) and \(}\) into textual embeddings. Consequently, we can compute the cosine distance between \(\) and \(}\) in the high-dimensional semantic space mapped by \(g_{}\), denoted as:

\[}(,})=1-(g_{}(),g_{}(})). \]

It is important to note that our attacker has no access to the parameters \(\) of \(g_{}\), but only the embedding information output by the encoder. Therefore, we can define the attacker's adversarial objective for one adversarial example across \(K\) downstream LVLM tasks as:

\[_{}_{k=1}^{K}}(_{k},}). \]

Finally, by combining Equation 1 and 2, we obtain the universal adversarial objective for all tasks:

\[_{}|K}_{i=1}^{||}_{k=1}^{ K}1-(g_{}(f_{}(^{}_{i},_{k})),g_{}(})) s.t.^{}_{i}=(1-)_{i}+ . \]

### Crafting and Optimizing Universal Adversarial Patch

Since we can not obtain the backpropagated gradient of Equation 4 to optimize the adversarial patch \(\) by solely querying the model, we introduce a novel gradient approximation strategy on LVLM models to generate the universal adversarial patch in the following three steps.

**Initializing patch location and noise pattern.** Firstly, we need to determine a fixed location of the universal adversarial patch on the visual input, which is crucial because noise in different locations significantly impacts the vision encoder's attention . However, in our challenging practical attack setting, we cannot explicitly explore the areas of interest to the LVLM model using gradient-based tools like Grad-Cam . Therefore, we have to spend a portion of our query budget to randomly decide the patch's location and determine the best location of the patch based on the model's feedback. Specifically, denoting the size of the image input \(\) as \(S_{v}^{+}\) and the size of the adversarial patch as \(S_{p}^{+}\), we randomly select the x-\(y\)-axis location, _i.e._\(pos_{}/pos_{}\), of the patch relative to the image \(\) from \(\{0,1,,S_{v}-S_{p}\}\), obtaining the value of each element \(m_{ij}\) in the mask matrix \(\) as follows:

\[m_{ij}=1,& pos_{} i pos_{} +S_{p} pos_{} j pos_{}+S_{p}\\ 0,&. \]

Then, for each random location, we combine it with a random noise pattern, allowing us to find the optimal combination of location and pattern. Generally, we randomly sample noise from a uniform distribution \((-,)\) to construct the initial patch \(_{}\), ensuring the constraint \(\|_{}\|_{}\). Next, we calculate its distance from the target text \(}\) using Equation 2 and select the combination with the smallest distance as the initial state for the subsequent iterative optimization of the noise pattern.

**Importance-aware gradient approximation.** Based on the initialized adversarial patch, we then investigate how to estimate its gradient direction for perturbing it into a targeted-chosen label by solely querying the LVLM model. The Monte Carlo estimation  offers a general strategy to approximate the gradient's direction on traditional single-task models. It employs a series of random slight noises on the previously obtained adversarial perturbation and scrutinizes whether these noises induce alterations in the prediction, the average of these noise directions serves as the ultimate direction for further mutating the perturbation. However, such a design is not efficient for the complicated LVLMs as there is a larger yet complex search space in LVLM models and not all gradient directions may point towards optimal direction and some of them may have opposite directions. Therefore, most gradient directions are canceled out with each other and the attack result can hardly be improved (verified by our experimental variant "w/o importance"). To address this issue, we propose to assign and adjust the optimization weights for different noise samples based on the importance-aware degree to which these sampled noises lead to the attacker-chosen output of the LVLM model.

Specifically, we employ a normalized uniform distribution \((-1),(-1,1)\) to add slight noise \(\) on the patch \(\) for perturbation. We utilize \(T\) times iteration to optimize this patch with these noises iteratively. In particular, at the \(t\)-th step, we initially establish an indicator function \(_{t}\) to assess whether the noise \(_{t}\) is capable of influencing the LVLM's prediction:

\[_{t}=(_{k=1}^{K}(g_{}(f_{}(_ {i}^{},_{k})),g_{}(^{}))-) s.t.^{}=(1-)+(+_{t}). \]

Here, the \(()\) function denotes the sign function, while \(\) serves as a threshold to gauge the impact of \(_{t}\). Denoting \((_{t})-(_{t-1})\) as \(_{t}\), we lift the importance of noise directions  that may lead to attacker-chosen output while diminishing the influence of others by:

\[w_{t}=\{(_{t})/,&& _{t}>0&(_{t})=1\\ (_{t}),&&_{t}>0& (_{t}) 1\\ (_{t}+3),&., \]

where \(w_{t}\) is the importance-aware weight. The directional improvement \(_{t}\) signifies the disparity between the current decision value and the prior, which can be as small as \(-2(=-1-1)\). The parameter \(\) serves to diminish the significance when the direction from attacker-chosen output samples threatens to overshadow input from other samples. The selection of value 3 aims to prevent \((_{t}+3)\) from yielding a value smaller than \(0\).

**Updating patch by querying the LVLM.** At last, based on the above importance weight, the gradient direction for each sample can be estimated by the weighted average of additive noises. To ascertain the estimated gradient vector \(_{_{t}}\), we employ the Monte Carlo method as:

\[_{_{t}}=\{(_{t}) (_{t}),&&(_{t})=1 &(_{t})=-1\\ ((_{t}-(_{t}))_{t}),&., \]

where the first condition handles the case where all the noises lead to attacker-chosen label or not. If certain perturbations alone precipitate the target prediction, we deduct the mean decision value from each individual decision and multiply the results by their corresponding noises under the second condition. This method ensures that the weights assigned to various noises remain reasonably aligned with the average, reflecting the quality of the present perturbation. Subsequently, we update the patch pattern \(\) with the noise \(_{t}\) along the direction of \(_{_{t}}\), employing weights that are aware of their importance as follows:

\[=+w_{t}_{t}}}{\|_{_{t}}\|_{2}}. \]

By iteratively optimizing the perturbations on the patch pattern over \(T\) iterations using the aforementioned weighted gradient estimation, we can obtain the universal adversarial patch on LVLMs.

## 4 Experiments

### Implementation details

**LVLMs & Datasets.** In this paper, following existing LVLM attack methods [24; 25; 26; 27; 28; 31; 32; 33], we conduct experiments on the same open-source LVLM models including LLaVA-1.5 , MiniGPT-4 , Flamingo , and BLIP-2  for fair comparison. To accurately evaluate the attack methodologies, we conduct experiments on three sources: MS-COCO , VQAv2 , and DALLE-3 . We also follow the existing works to construct these three datasets. Specifically, we employ images from the test sets of the MS-COCO and VQAv2 to construct two multimodal datasets. We also use captions from the MS-COCO validation set as prompts to generate corresponding images with DALLE-3 to form another dataset. For the text input data, we follow the prompts used in previous work  to build our text dataset, with detailed data presented in the appendix.

**Basic setups.** We employ Sentence-BERT  as the text encoder (judge model) to measure the LVLM's textual output with the adversarial target. The discussion regarding different text encoders is presented in Section 4.3. We select three widely used image-to-text tasks to evaluate our attack method, _i.e._, Image Classification, Image Captioning, and VQA. Discussions on various patch sizes are conducted in Section 4.3, with a size of 64 chosen as the final decision. During the gradient approximation, we allow 70k queries number in total. The general target label in our almost all experiments is set to text "Unknown"; Various other target labels are also experimented with in Section 4.2. In Equation 6, we use \(=0.55\) to determine the direction of gradient predictions for each slight noise \(\). In Equation 7, we select \(=5\). We impose \(=16/255\) as the constraint for \(_{}\). All experiments are conducted on a single NVIDIA H100 Tensor Core GPU.

### Main Results

**Main performances.** We conduct a comprehensive evaluation on four LVLM models across three datasets in Table 1. The performance is evaluated by the semantic similarity between the LVLM's output and the target text. Here, we select the target text "Unknown" to avoid the inclusion of high-frequency responses. All the performance is evaluated on a single universal patch pasted on the whole test set. In addition to reporting the similarity scores on clean images and our attacked ones, we also report our attack variant "w/o importance", which removes the importance-aware weights during the gradient approximation and equally treats all gradient directions. From this table, we can find that our attack consistently achieves the best performance on all models and datasets.

   LVLM Model & Attack Method & ImageClassification & ImageCaption & VQA & Overall \\   \\   & Clean image & 0.385 & 0.479 & 0.436 & 0.433 \\  & w/o importance & 0.703 & 0.679 & 0.711 & 0.698 \\  & Full attack & **0.850** & **0.812** & **0.828** & **0.830** \\   & Clean image & 0.438 & 0.451 & 0.463 & 0.450 \\  & w/o importance & 0.713 & 0.670 & 0.719 & 0.701 \\  & Full attack & **0.847** & **0.826** & **0.851** & **0.841** \\   & Clean image & 0.475 & 0.468 & 0.492 & 0.478 \\  & w/o importance & 0.705 & 0.693 & 0.727 & 0.709 \\  & Full attack & **0.862** & **0.803** & **0.839** & **0.835** \\   & Clean image & 0.409 & 0.436 & 0.447 & 0.431 \\  & w/o importance & 0.724 & 0.682 & 0.716 & 0.707 \\  & Full attack & **0.810** & **0.787** & **0.845** & **0.814** \\   \\   & Clean image & 0.407 & 0.453 & 0.517 & 0.459 \\  & w/o importance & 0.644 & 0.692 & 0.751 & 0.696 \\  & Full attack & **0.824** & **0.806** & **0.879** & **0.837** \\   & Clean image & 0.396 & 0.441 & 0.497 & 0.445 \\  & w/o importance & 0.682 & 0.738 & 0.714 & 0.711 \\  & Full attack & **0.810** & **0.843** & **0.862** & **0.838** \\   & Clean image & 0.431 & 0.464 & 0.485 & 0.460 \\  & w/o importance & 0.719 & 0.746 & 0.742 & 0.735 \\  & Full attack & **0.823** & **0.871** & **0.838** & **0.844** \\   & Clean image & 0.368 & 0.425 & 0.466 & 0.419 \\  & w/o importance & 0.673 & 0.759 & 0.733 & 0.721 \\  & Full attack & **0.795** & **0.837** & **0.840** & **0.824** \\   \\   & Clean image & 0.458 & 0.446 & 0.482 & 0.462 \\  & w/o importance & 0.730 & 0.678 & 0.714 & 0.707 \\  & Full attack & **0.826** & **0.792** & **0.869** & **0.829** \\   & Clean image & 0.397 & 0.448 & 0.505 & 0.450 \\  & w/o importance & 0.699 & 0.753 & 0.774 & 0.742 \\  & Full attack & **0.837** & **0.878** & **0.881** & **0.865** \\   & Clean image & 0.423 & 0.467 & 0.486 & 0.459 \\  & w/o importance & 0.705 & 0.734 & 0.783 & 0.741 \\  & Full attack & **0.832** & **0.840** & **0.877** & **0.850** \\   & Clean image & 0.414 & 0.459 & 0.470 & 0.448 \\  & w/o importance & 0.688 & 0.725 & 0.752 & 0.722 \\  & Full attack & **0.803** & **0.831** & **0.875** & **0.836** \\   

Table 1: Attack performance on different LVLM models across different datasets. We report the semantic similarity scores between the LVLM’s output and the attackers’ chosen label “Unknown”. “w/o importance” denotes our full model without using importance weights in gradient approximation.

To demonstrate that the effectiveness of the proposed attack is not constrained to the specific case of the target text "Unknown", we extend our evaluation to various other target texts. The experiment includes a selection of text with varied length and usage frequency as shown in Table 2. We can observe that our attack performs the best overall and in each individual task under different target text, though the output similarity differs for different targets. The sentence "I cannot answer" is a reasonable generation result of LVLMs to indicate the uncertainty of the response, which performs better than the less commonly used "I hate people".

**Compare to other LVLM attacks.** Since existing LVLM attack methods are deployed in different settings, for fair comparison, we separately compare our method with each of them in the same setting. As in Table 3, according to the reported performance on MF-Attack  in its paper, we compare the same output similarity performance on the same LLaVA, BLIP-2, MiniGPT-4 models in VQA task. We can find that our attack is more effective as we directly approximate the gradients in the victim black-box model. As in Table 4, according to the reported performance on CroPA  in its paper, we re-judge our output with the same ASR metric and compare performance on the same OpenFlamingo  model on MS-COCO dataset. CroPA can only attack white-box cross-prompt of the same task, instead, our attack can attack black-box cross-task inputs with better results.

**Analysis on universality.** The main difference between our attack and existing LVLM attacks is that we only need to generate a single universal adversarial patch for all inputs while they need to generate different adversarial perturbations for different input samples. To investigate our universality, we implement two variants for comparison: "w/o. universal baseline" removes our universality design and follows previous works individually to optimize perturbation for each sample; "w/. universal baseline" denotes our approach. We evaluate the averaged attack performance of their generated

   Method & Attack Type & ImageClassification & ImageCaption & VQA & Overall \\    CroPA  \\ Ours \\  } &  white-box attack \\ universal and practical attack \\  } & 0.70 & 0.34 & **0.92** & 0.65 \\  & & **0.73** & **0.51** & 0.84 & **0.69** \\   

Table 4: Comparison with existing LVLM attack: CroPA . For a fair comparison, we follow CroPA to evaluate the same ASR metric on the same OpenFlamingo model and MS-COCO dataset.

   Adversarial Target & Attack Method & ImageClassification & ImageCaption & VQA & Overall \\   “Unknown” \\  } & w/o importance & 0.644 & 0.692 & 0.751 & 0.696 \\  & Full attack & **0.824** & **0.806** & **0.879** & **0.837** \\   “I cannot answer” \\  } & w/o importance & 0.627 & 0.688 & 0.723 & 0.679 \\  & Full attack & **0.816** & **0.835** & **0.862** & **0.844** \\   “I am sorry” \\  } & w/o importance & 0.648 & 0.674 & 0.735 & 0.686 \\  & Full attack & **0.845** & **0.813** & **0.868** & **0.842** \\   “I hate people” \\  } & w/o importance & 0.593 & 0.639 & 0.664 & 0.632 \\  & Full attack & **0.682** & **0.710** & **0.756** & **0.716** \\   

Table 2: Attack performance on LLaVA model and DALLE-3 dataset with different target labels.

Figure 3: Analysis on the attack “Universality” on LLaVa model and DALLE-3 dataset.

single adversarial patch pasted on all images of the whole test set during the attack optimization. As shown in Figure 3, the universality on the single perturbation of "w/o. universal baseline" is very poor, demonstrating the effectiveness of our universality design. More analyses are in Appendix B.4.

**Visualization results.** We provide the visualizations of the targeted universal attack in Figure 4. Each adversarial patch can achieve a universal targeted attack. More visualizations are in Appendix B.8.

### Ablation

We conduct ablation studies on the LLaVA model and DALLE-3 to investigate our attack in depth.

**Ablation on different judge model.** As shown in Figure 5(a), we conduct ablations on different text encoders (judge model) to measure and constrain the semantics of LVLM's output. We find that Sentence-BERT achieves the best performance.

**Ablation on patch size \(S_{p}\).** As shown in Table 5, we conduct the ablations on different patch sizes. It shows that size \(64 64\) is enough to achieve good attack performance.

**Ablation on different \(\).** Figure 5(b) computes the distances of different \(\) according to Equation 2, where \(=16/255\) is adequate to attain satisfactory results.

Figure 4: Visualization results on the targeted universal adversarial attack.

Figure 5: Ablation on different judge models and \(\), tested on LLaVA model and DALLE-3 dataset.

   Patch Size & ImageClassification & ImageCaption & VQA & Overall \\  \(S_{p}=32\) & 0.734 & 0.720 & 0.778 & 0.744 \\ \(S_{p}=48\) & 0.793 & 0.775 & 0.842 & 0.803 \\ \(S_{p}=64\) & **0.824** & **0.806** & **0.879** & **0.837** \\   

Table 5: Ablation study on the patch size on LLaVA model and DALLE-3 dataset.

### Robustness to Defense Strategy

We further investigate the robustness of our proposed attack method. As shown in Table 6, we first report the attack performance on the LLaVA model and DALLE-3 dataset against the traditional RandomRotation defense strategy. It validates the robustness of our proposed attack. As shown in Table 7, we also evaluate the robustness of our adversarial patch with four popular defense methods. Specifically, PatchCleaner  is a state-of-the-art certifiable defense against adversarial patches. It uses double masking to certify the prediction. [86; 87] are query-based defenses, which are specifically designed for detecting malicious queries by black-box attacks.  is the general black-box defense method. Overall, it indicates that our attack is robust to the potential defenses.

### Complexity Analysis

For a single attack process, the complexity was recorded in Table 8. The attack's query budget was set to 70,000, with the dataset being DALLE-3 and the victim LVLM as LLaVA, with other hyperparameters consistent with those in subsection 4.1. The primary GPU computational and memory overheads occur during the querying stage against the victim LVLM when training a universal patch. This involves adding slight noise to all attack samples during each iterative update of the patch to explore their impacts, and this stage also constitutes the major consumption of the query budget. The gradient approximation stage primarily takes place on the CPU, involves a much smaller computational load, and thus does not consume GPU power or memory, and is therefore not recorded in the table. Note that, our attack is much more efficient than existing LVLM attacks, since our universal adversarial patch can be pasted on any image of any task to achieve attack while existing LVLM attacks need to generate individual perturbation for each input sample.

## 5 Conclusion

In this paper, we propose to attack the real-world large vision-language models (LVLMs) in a practical but challenging setting, where the attacker can solely query the LVLM model. To make the perturbation universally adversarial to multiple LVLM-driven tasks, we design a universal adversarial patch with specific locations to perturb the visual inputs. By solely querying the model to estimate the gradient direction for optimizing the adversarial patch pattern, we develop a novel importance-aware gradient approximation strategy to adaptively estimate and adjust the weights on gradient directions for optimizing different samples. Experiments show the effectiveness of the proposed attack method.

  Defense Method & Attack Method & ImageClassification & ImageCaption & VQA & Overall \\   & w/o importance & 0.644 & 0.692 & 0.751 & 0.696 \\  & Full attack & 0.824 & 0.806 & 0.879 & 0.837 \\   & w/o importance & 0.602 & 0.663 & 0.718 & 0.661 \\  & Full attack & 0.786 & 0.760 & 0.814 & 0.787 \\  

Table 6: Attack performance on LLaVA model and DALLE-3 dataset against RandomRotation.

    &  &  &  \\   & Patch Initialization & 1.3h & 33.4GB \\  & Gradient Approximation & - & - \\  & Iterative Update & 3.5h & 68.1GB \\  & Total & 4.9h & 57.5GB \\  Evaluation & Total & 0.4h & 26.8GB \\   

Table 7: Attack performance against black-box defense strategies.

    Defense Method \\  } &  Attack Method \\  &  ImageClassification \\  &  ImageCaption \\  &  VQA \\  & 
 Overall \\  \\   & w/o importance & 0.644 & 0.692 & 0.751 & 0.696 \\  & Full attack & 0.824 & 0.806 & 0.879 & 0.837 \\   & w/o importance & 0.602 & 0.663 & 0.718 & 0.661 \\  & Full attack & 0.786 & 0.760 & 0.814 & 0.787 \\   

Table 7: Attack performance against black-box defense strategies.