# Cooperation, Competition, and Maliciousness:

LLM-Stakeholders Interactive Negotiation

 Sahar Abdelnabi\({}^{1}\) Amr Gomaa\({}^{2}\) Sarath Sivaprasad\({}^{3}\) Lea Schonherr\({}^{3}\) Mario Fritz\({}^{3}\)

\({}^{1}\)Microsoft \({}^{2}\)German Research Center for Artificial Intelligence (DFKI)

\({}^{3}\)CISPA Helmholtz Center for Information Security

###### Abstract

There is a growing interest in using Large Language Models (LLMs) in multi-agent systems to tackle interactive real-world tasks that require effective collaboration and assessment of complex situations. Yet, we have a limited understanding of LLMs' communication and decision-making abilities in multi-agent setups. The fundamental task of negotiation spans many key features of communication, such as cooperation, competition, and manipulation potentials. Thus, we propose using scorable negotiation to evaluate LLMs. We create a testbed of complex multi-agent, multi-issue, and semantically rich negotiation games. To reach an agreement, agents must have strong arithmetic, inference, exploration, and planning capabilities while integrating them in a dynamic and multi-turn setup. We propose metrics to rigorously quantify agents' performance and alignment with the assigned role. We provide procedures to create new games and increase the difficulty of games to have an evolving benchmark. Importantly, we evaluate critical safety aspects such as the interaction dynamics between agents influenced by greedy and adversarial players. Our benchmark is highly challenging; GPT-3.5 and small models mostly fail, and GPT-4 and SoTA large models (e.g., Llama-3 70b) still underperform in reaching agreement in non-cooperative and more difficult games1.

## 1 Introduction

Large Language Models (LLMs)  are used in tasks beyond traditional NLP, such as using tools  or solving reasoning problems . They are adopted in many real-world applications  that require multi-turn interactions and adaptation to external sources and interfaces . Multi-agent LLM frameworks are envisioned to be a key design pattern for future autonomous systems . However, LLMs are not explicitly trained for these tasks. Given this contrast, we need new evaluation frameworks to assess models in complex communication settings.

Complex communication involved in, e.g., satisfying customers, agreeing on contracts, and high-stake decisions, such as authorizing loans, requires prolonged deliberation. We use crucial skills such as strategic planning, competition, cooperation, balancing between multiple objectives, and awareness of cooperation barriers such as manipulation and deception. This should ideally apply to AI and LLM agents, which are increasingly relied on as personal  and negotiation assistants . A future where AI assistants communicate on behalf of different entities seems plausible. This raises the concern of models being exploited by rogue parties to pursue unaltruistic or manipulative goals and exploit other agents to agree with outcomes that were restricted by the developers .

As negotiation is integral to these scenarios  and thus for advancing AI agentic design, we propose scorable negotiation games, with complex cooperation and competition between multiple parties, as a multi-step dynamic benchmark for LLMs. In these games, agents ideally assess the value of deals w.r.t. their own goals, have a representation of others' goals, weigh different options, and finally findcommon grounds. These sub-tasks require substantial arithmetic and strategic reasoning under only partial observations. They also span commonsense reasoning [46; 38] and Theory-of-Mind (ToM) capabilities [41; 39]. Such skills are required in many applications to rank and propose solutions, e.g., to answer "find the cheapest, shortest flight with a reputable airline that will not lose my luggage", an agent has to satisfy multiple objectives and rank proposals accordingly.

We first use a role-play exercise commonly used for teaching negotiation , which consists of multiple parties and issues (see Figure 1). Parties have their real-world-inspired goals correlated with their individual secret scores for issues. They also have a minimum threshold for agreement. The priorities vary between parties, creating a non-zero-sum game with potential for cooperation and competition. The scores and thresholds control the set of feasible solutions, providing a way to quantify performance. We use an LLM as a seed to design 3 completely new and diverse games from scratch that we further curate. We also use an LLM to expand games and add an additional player and issue, increasing the complexity in terms of action space and semantic roles of agents. We easily instantiate new games with different difficulty levels by changing scores and thresholds. These factors make our benchmark highly evolving to test future more powerful models.

We design a baseline framework, via prompting, that systematically breaks down the task into intermediate ones, revealing essential insights about the most needed capabilities. Our findings show that larger models such as GPT-4  and Llama-3 70b [47; 26] significantly outperform random or heuristic-based baselines; smaller models mostly fail. However, GPT-4 (the best-evaluated model) still underperforms in reaching agreement when increasing games' difficulty and in non-cooperative games. Furthermore, GPT-4 agents can get higher rewards compared to GPT-3.5  ones when assigned the same role in a mixed population simulation, hinting at potential _fairness_ and disparity considerations when users use models with varying capabilities as assistants. Some open-source models (Llama-2/3 70b and Miktral ) outperform GPT-3.5 and Gemini Pro .

Moreover, our complex environment enables us to study agents' dynamics in unbalanced and adversarial setups, a critical aspect of autonomous agents. We show that agents can be steered toward greediness or manipulation, _altering other_ compromising agents' behaviors, which may reward the greedy agent's demands more highly. The adversarial agent may also create a _coalition_ against a target agent, etc. These attacks are broadly useful for AI safety research to study AI manipulation and deception , alignment of multi-agent systems, and actions driven by an assigned persona [2; 42]. In summary, our work provides several complex and interactive negotiation games as an evolving benchmark to test LLMs' capabilities, the potential for manipulation, and future robustification. To foster future research, we release our toolkit of diverse games, code platform, and transcripts.

## 2 Game Description

Games consist of \(n\) parties, \(P=\{p_{1},p_{2},...,p_{n}\}\), and \(m\) issues \(I=\{A,B,...,I_{m}\}\) with dynamics outlined below. All notations and prompts are in Appendices A and K.

Figure 1: Left: Parties negotiate over different issues with different sub-options. Each party has its own _secret_ scores, issue priorities, and a minimum threshold for acceptance. Right: Parties ideally reach a common ground by adjusting their optimum deal. This is visible in the graph; over rounds, the leading agent \(p_{1}\) proposes deals that reduce its own score but increase all agents’ collective score.

**Parties.** An entity \(p_{1}\) proposes a project (e.g., an airport) that it will manage and invest in and wants to increase the return on its investment. Another party, \(p_{2}\), provides a budget for the project and has veto power. Its utility function makes it likely to act as a middle ground between different parties. There exists a group of beneficiary parties, \(P_{} P\), whose interests can align with \(p_{1}\) in multiple issues, but they want to negotiate better deals. Some parties \(P_{} P\) (e.g., environmentalists) would like to impose more constraints on the project, which usually contradicts \(p_{1}\)'s interests. Other parties, \(P_{} P\), have opposing interests to \(p_{1}\) as the project may affect their, e.g., living conditions, etc.

**Issues.** Parties negotiate over \(m\) issues \(I=\{A,B,...,I_{m}\}\) related to the project (e.g., funding). Each issue has 3-5 sub-options, e.g., \(A=\{a_{1},a_{2},...,a_{x}\}\). A deal, \(\) where \(\) is the set of all deal combinations, consists of one sub-option per issue, \(=[a_{k} A,b_{l} B,c_{o} C,d_{h} D,...,i_{mq} I_{m}]\). In our setup, we created games where the total number of possible deals \(||\) is 720 or larger games where \(||\) is 2880. The sub-options take the form of a range over a quantity in dispute (e.g., project size, revenue, etc.) or a discrete form with less apparent compromise (e.g., different locations). To denote that party \(p_{i}\) suggested a deal at a time \(t\), we use the notation \(_{p_{i}}^{(t)}\).

**Scoring.** Each party has its own scoring system \(S_{p_{i}}\) for the sub-options, which has a semantic connection to the parties' goals (e.g., will increase or decrease its profit return). The priority of issues (e.g., \((S_{p_{i}}(a_{1}),S_{p_{i}}(a_{2}),...,S_{p_{i}}(a_{x}))\) ) differ between parties. Some parties can be completely neutral on some issues (indicated by a score of 0). These factors result in a non-zero-sum game and control the cooperation and competition between parties. For a party \(p_{i}\), its score of a deal (suggested by \(p_{j} P\)) is the sum of its scores of this deal's sub-options, i.e., \(S_{p_{i}}(_{p_{j}}^{(t)})=S_{p_{i}}(a_{k})+S_{p_{i}}(b_{l})+S_{p_{i}}(c_{o })+S_{p_{i}}(d_{n})+...+S_{p_{i}}(i_{mq})\), with a maximum of 100.

**Feasible solutions.** Each party \(p_{i}\) has a minimum threshold \(_{p_{i}}\) for acceptance. A deal is feasible if it exceeds the thresholds of at least \(n-1\) parties, which must include \(p_{1}\) and \(p_{2}\). These factors restrict the set of feasible deals \(_{}\), quantify the success in reaching an agreement, and control the game's difficulty by altering the size of the feasible set \(|_{}|\), which allows instantiating new games.

**New games.** The base game is adapted, with our own descriptions, from a negotiation exercise . Our base game adopts the setup of the initial game  (5 parties, 6 issues, and the values of scores and thresholds). We use an LLM to add another player and another issue to the base game (baseextended). Moreover, we use LLMs to create new games by creating the background story, the parties, the issues, and the goals and preferences of each party, _from scratch_; the base game is _not given_ to the model as in-context information. We only specify that parties should include a proposer, a resource manager, a beneficiary, opposing parties, etc., and issues should represent competing interests of parties. We manually curated the games to ensure logical consistency, and we assigned numerical scores to reach a comparable ratio of feasible deals compared to the base game (\(\)7%).

## 3 LLMs Playing the Game

We here present agents' interaction protocol, the different variants of the game, and our prompting solution framework. Our setup is in Figure 2. Algorithm and prompts are in Appendices A and L.

### Agents' Interaction Protocol

**Initial prompts.** Each agent \(p_{i}\) is characterized via an initial prompt that consists of 1) shared information about the project, the parties involved, and the issues' descriptions, 2) confidential information about the scores of this particular agent \(S_{p_{i}}\) and its minimum threshold \(_{p_{i}}\), and 3) general instructions explaining the game rules (e.g., not disclosing scores). The initial prompts mention how scores correlate with goals and give 1-2 examples of how other agents' scores can differ according to their goals.

**Rounds.**\(p_{1}\) starts the negotiation by suggesting its ideal deal. The game then continues for \(R\) rounds; in each, one agent is prompted with the initial prompt, a history of the most recent \(n\) interactions (the number of players), and rounds' instructions that guide the negotiation (more details in the following). Agents should either support previous deals or propose

Figure 2: Interaction protocol.

new ones. The input context and output of agent \(p_{i}\) at time \(t\) are:

\[O_{p_{i}}^{(t)}=(C_{p_{i}}^{(0)},H^{(-n)},C_{p_{i}}^{(t)}), \]

\(H^{(-n)}\) is the most recent \(n\) public answers, \(C_{p_{i}}^{(0)}\) is the initial prompt, and \(C_{p_{i}}^{(t)}\) is the rounds' prompt.

**End of negotiation.** After \(R\) rounds, the project proposer \(p_{1}\) is prompted with instructions to propose a final official deal (\(_{p_{1}}^{(R+1)}\)). Similar to eqn. 1, these instructions are appended to the initial prompt and the last \(n\) interactions. This final deal determines whether an agreement has been reached. The achieved utility of each party becomes:

\[U_{p_{i}}=\{S_{p_{i}}(_{p_{1}}^{(R+1)})& _{p_{1}}^{(R+1)}_{}\\ &. \]

where BATNA is _Best Alternative To a Negotiated Agreement_ (i.e., achieved utility when there is no deal), which is usually the threshold \(_{p_{i}}\) but may differ depending on the game variants outlined next.

### Compromising, Greedy, and Adversarial Games

The agents' scores entail different levels of cooperation and competition. For example, the game will be more competitive if all parties equally prioritize the same issue with very opposing interests. In addition to that, we further evaluate how agents' actions can be explicitly modulated to promote compromise, greediness, or maliciousness.

**Compromising game.** Here, all agents are instructed that any deal likely to lead to an agreement and higher than their minimum threshold is preferable to no deal; i.e., the BATNA of agents in eqn. 2 is their minimum threshold. Specifically, the optimization problem an agent \(p_{i}\) performs is modeled as:

\[f()=w_{p_{i}}S_{p_{i}}()+_{p_{j} P\{p_{i}\}}w_{p_{j}}S_ {p_{j}}^{*}() \]

\[_{p_{i}}^{(t)}:=*{arg\,max}_{\{S_{p_{i}}()>_{p_{ i}}\}}f(); \]

\(p_{i}\) cannot observe the scores of another agent \(p_{j}\). Therefore, \(S^{*}\) is \(p_{i}\)'s estimate (e.g., based on \(p_{i}\)'s reasoning about the observations or the semantic role of \(p_{j}\)). \(w_{p_{i}}\) and \(w_{p_{j}}\) are weights assigned to the agent's own score vs. \(p_{j}\)'s. The agent may prioritize some agents (e.g., veto parties) over others. In the compromising game, the agent is not particularly prioritizing its own score over others; \(w_{p_{i}}(\{w_{p_{j}}|\ p_{j} P\{p_{i}\}\})\).

**Greedy game.** When agents interact in the real world with other agents or humans, they might face non-collaborative or even exploitative players. Thus, we introduce one or more greedy agents and keep the others compromising. The greedy agents are instructed to maximize their own score and benefits as much as possible while still aiming for an agreement; i.e., the BATNA is still the minimum threshold. The optimization objective is similar to eqn. 3, but with \(w_{p_{i}}(\{w_{p_{j}}|\ p_{j} P\{p_{i}\}\})\). We note here that since we study non-zero-sum games with interdependent utilities, in both the compromising and greedy variants where there is an incentive to reach an agreement, maximizing other agents' payoffs does not necessarily contradict maximizing the agent's own payoff. There may exist deals \(_{1}\) and \(_{2}\) such that \(_{1}\) Pareto-dominates \(_{2}\). I.e., the player has an incentive to switch strategies when a different strategy has the potential to reach an agreement (e.g., by giving higher scores to veto parties) even if its scoring function does not improve.

**Adversarial game.** Here, one party is instructed to sabotage the negotiation or at least maximize its own score as much as possible if the negotiation seems likely to succeed. This player gets a higher score if _no deal_ is achieved. This is, their BATNA is higher than 100 (the maximum achievable score). To provide a mechanism for sabotaging, we instruct the agent to "isolate one party by pushing for deals that you think they will oppose, but others might support". We conduct two experiments: one where we specify the victim/target agent \(p_{v}\) (**targeted**) and one where the agent autonomously picks one (**untargeted**). Similar to the greedy game, \(w_{p_{i}}(\{w_{p_{j}}|\ p_{j} P\{p_{i}\}\})\). In addition, \(w_{p_{v}}<0\) (to minimize the target's score). This would result in a lower average score for the group.

**Natural language incentives.** We verbalize these variants as high-level "incentives" given to the model in the initial and round prompts; e.g., compromising agents are instructed to aim for a balanced deal, accommodate other parties, etc. Adversarial agents are instructed to "not care about being fair or accommodating others", etc. However, _we do not instruct agents on which deals to propose_.

**Assumptions.** In all variants, agents are not prompted with any information about other players' incentives. In the adversarial variant, a successful deal has to satisfy the thresholds of the other \(n-1\) parties. We introduce only one adversary to have a similar success condition across variants.

### A Baseline Prompting Solution Framework

We use structured Chain-of-Thought  to enable agents to decompose the task, plan their answers, and show intermediate calculations in a secret "scratchpad". We use the following structure:

**CoT: Observation.** The agent first should collect observations and information from the ongoing history. This involves a _"previous deals' calculation"_ step in which we prompt agents to calculate their scores for each deal that was proposed in the current history window. Then, we follow this with an instruction to _"infer others' preferences"_. We remove one or both steps in our ablation.

**CoT: Exploration.** Next, agents should explore possible moves by _"generating candidates"_, i.e., 3 potential deals that are higher than their thresholds, then _"selecting a final deal"_ that is likely to achieve their respective goal. Our ablation removes the first step.

**CoT: Planning.** Planning is integral to how humans negotiate . We observed agents' utterances may contain references to actions they can explore the next time (e.g., "I will propose \(a_{1}\) first, then, I can compromise to \(a_{2}\)"). Without long-term planning and a limited shared history, the agent might propose similar deals each round. Therefore, as long as the agent has a next turn, we instruct it to generate a secret _plan_ of possible next actions. At the next turn, the agent is fed its respective previous "plan" appended to the round's prompt \(C^{(t)}_{pi_{i}}\). Agents' output in eqn. 1 can thus be broken down as:

\[O^{(t)}_{pi}\{[^{(t)}_{pi},^{ (t)}_{pi},^{(t)}_{pi}]&$) = True}\\ [^{(t)}_{pi},^{(t)}_{pi}]&. \]

, \(^{(t)}_{pi_{i}}\) is the scratchpad, \(^{(t)}_{pi_{i}}\) is the public answer, and \(^{(t)}_{pi_{i}}\) is the plan.

## 4 Experiments and Evaluation

We first describe our setup and show the ablation study and models' comparison. Next, we show the performance of other games and the greedy and adversarial variants. We also discuss random-chance baselines or rule-based ones to contextualize LLM agents' performance. Our evaluation is aimed at showing how the benchmark can test LLMs in different tasks via our proposed metrics and to demonstrate the benchmark's properties, e.g., how challenging it is for current LLMs, how it can be maintained and adapted, and how it can be used as a simulation testbed for future planning and reasoning algorithms and for other safety considerations. Detailed qualitative analysis is in Appendix J. The appendices contain other results which we refer to in their respective sections.

### Experimental Setup and Evaluation Metrics

For 6-player/7-player games, we used 24/28 rounds, with 4 consecutive random ordering of the 6/7 agents and a history window of the last 6/7 interactions, respectively. We test on GPT-4, GPT-3.5, Gemini Pro, Llama-2 13b and 70b Chat, Llama-3 70b Chat, and Mixtral 8x7B. For reproducibility, we used a sampling temperature of 0. We report an experiment with a sampling temperature of 1.0 in Appendix B; in summary, our findings still hold; however, varying the temperature can be used to test the robustness of agreement against scenarios when one or more players are irrational. Models are instructed to indicate deals, scratchpads, public answers, and plans by specific tags to enable automatic parsing and calculation of deals' scores. We ran each experiment 20 times (with a random order of agents) to compute the average performance. Specifically, we propose the following metrics:

**Final success.** Rate of games with a successful final deal (made by \(p_{1}\) at the end of the negotiation), i.e., \(^{(R+1)}_{p_{1}}_{}\). We measure both 5-way and 6-way agreement rates.

**Any success.** Rate of games with a successful deal by \(p_{1}\) at _any time_; \(^{(t)}_{p_{1}}_{}\) is True for any \(t\).

**Own score.** We calculate \(p_{i}\)'s scores of its proposed deals w.r.t. itself: \(S_{p_{i}}(^{(t)}_{p_{i}})\). This is a "local view" of the agent's actions and helps measure if/how agents are aligned with their roles.

**Collective score.** For an agent \(p_{i}\), we calculate the average score of all agents given its deals: \(|}_{p_{j} P}S_{p_{j}}(_{p_{i}}^{(t)})\). This is an "oracle view" of the agent's actions w.r.t. others, which \(p_{i}\)_cannot observe_. This measures whether agents make correct inferences about others' goals and take actions that are likely to achieve their goals (e.g., agreement, sabotaging).

**Wrong deals.** Rate of deals with "own score" less than the corresponding minimum threshold of the agent: \(S_{p_{i}}(_{p_{i}}^{(t)})<_{p_{i}}\). This measures whether models are performing _correct calculations_ of deals.

**Score leakage ratio.** Agents were instructed not to reveal information about scores. This is usually a critically needed behavior in practical negotiation setups. This also broadly measures the trustworthiness of models in following instructions and keeping in-context confidential information , a task that is also related to ToM and contextual integrity . We use GPT-4 as a judge to verify whether public answers contain any mention or values of scores or thresholds, and we compute the ratio of answers with leaked scores.

### Ablation of Prompts' Structure

As discussed in Section 3.3, we study variants of the prompt structure given to agents at each round \(C_{p_{i}}^{(t)}\). We remove the planning stage and vary the CoT "observation" and "exploration" stages. We also evaluate the no-CoT performance. We perform an ablation study on GPT-3.5 and GPT-4 and later test on the other models with the best-found configuration. Rows in Table 1 show these experiments, averaged over runs. Figure 3 shows the progression of \(p_{1}\)'s deals over rounds to visualize whether (and by how much) \(p_{1}\) is successfully reaching agreement in the GPT-4 experiments. Our analysis, depicted next, aims to reveal which skills are needed to reach success.

**Arithmetic calculations.** GPT-3.5 agents often propose deals that are less than their minimum thresholds (indicated by a higher value of the "wrong deals" metric). This is almost negligible in GPT-4 agents, especially when using CoT. In addition to computing the "wrong deals", tracking agents' deals can also evaluate how well agents follow instructions and are aligned with their assigned payoffs and negotiation roles, rather than following pretraining biases that would make some options

  & \) no.} &  &  &  &  \\   & &  &  &  &  &  &  \\  \)} & 1 & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & 25 & 0 & 70 & 3.6 \\  & 2 & ✓ & ✓ & ✓ & ✓ & ✓ & 15 & 10 & 30 & 0 \\  & 3 & ✓ & ✓ & ✗ & ✓ & ✓ & 45 & 5 & 80 & 1.5 \\  & 4 & ✓ & ✓ & ✗ & ✓ & ✗ & 28 & 4 & 61 & 2 \\  & 5 & ✗ & ✓ & ✗ & ✓ & ✓ & **81** & **33** & **100** & 1.4 \\  & 6 & ✗ & ✗ & ✓ & ✓ & ✓ & 60 & 15 & 95 & 0.9 \\  \)} & 7 & ✗ & ✗ & ✗ & ✗ & ✗ & 0 & 0 & 0 & 22 \\  & 8 & ✓ & ✓ & ✓ & ✓ & ✓ & 20 & 8 & 33 & 19 \\  & 9 & ✗ & ✓ & ✓ & ✓ & ✓ & 14 & 4 & 23 & 24 \\  & 10 & ✓ & ✗ & ✓ & ✓ & ✓ & 0 & 1 & 27 \\  & 11 & ✓ & ✓ & ✗ & ✓ & ✓ & 9 & 0 & 18 & 26 \\  & 12 & ✓ & ✓ & ✓ & ✓ & ✗ & 0 & 0 & 5 & 21 \\  

Table 1: Prompt structure ablation study. Yellow markers indicate changes in the experiment compared to the previous row. The prompt structure is: score calculation of previous deals in the public history, inferring others’ preferences, candidate generation, final deal selection, and planning.

Figure 3: \(p_{1}\)’s deals progression over rounds of GPT-4 experiments in Table 1. In (a), the “own score” decreases, and the “collective score” increases, indicating more agreement. In (b) and (c), they stop improving and saturate during the final rounds. In (d), agents proposed deals that are more ideal to them and which do not increase the collective score, lowering the success in reaching an agreement.

more ideal; we show in Appendix C a histogram in which GPT-4 agents advocate or oppose strong environmental protection measures in consistency with their respective payoffs.

**ToM.** In Table 1, we show that instructing models to infer others' preferences increases the success rate (indicated by the drop in rows 6 and 10). To test if models can explicitly infer the preferences of others, we further prompted each agent to provide a "best guess" of each party's preferred sub-option under each issue. Each agent sees only its own initial instructions \(C^{0}_{p_{i}}\) before interaction (to test commonsense reasoning based on the game's semantics, without observations from other agents). GPT-4 models scored **61%** in correctly matching the ground truth preferences of sub-options, vs. **42%** by GPT-3.5 (averaged over all agents). GPT-4 models frequently correctly assigned neutral values for issues with no clear associations (e.g., "the Green Alliance might not have any preference on employment distribution") and made a distinction between \(P_{}\) and \(P_{}\) regarding implicit preference entailment (e.g., "they might want to limit/ensure the project's success by requesting less/more funding") even though this distinction was not provided in the initial prompt. In contrast, GPT-3.5 agents often _leak_ their secret scores in their public answer and argue for deals because they have high scores, indicating a lack of ToM-related reasoning (see Appendix I and Table 3 next).

**Adaptation and Exploration.** GPT-3.5 agents benefited from instructions to explore feasible solutions (row 11), possibly due to improvements in calculations. However, when doing so with GPT-4, agents were biased towards generating deals and selecting the ones from the history that scored higher (see Figure 3d). Without this step, GPT-4 agents were more likely to find deals that adapt to other parties (see row 2 vs. row 3). We show an example of \(p_{1}\)'s CoT in Figure 4 in which the GPT-4 agent _iteratively_ alters its suggestion to accommodate \(p_{2}\) (after a correct inference of its preference) and to meet its own threshold. However, we still observe a lack of exploration when the agent compensated by over-increasing its score in one issue instead of finding a balanced proposal.

**Planning.** This step was important to reach a final successful deal (row 4); without it, agents' suggestions may saturate and no longer increase the collective score (Figure 3b).

### Mixed Population

As future multi-agent systems might have asymmetrical individual units, we next study a mixed population of GPT-4 and GPT-3.5. Since the game involves cooperation, less capable models may result in lower success for the _entire_ group. We show experiments in Table 2 with details in Appendix D. The main results are 1) including GPT-3.5 drops the success for the entire group, with the highest drop when \(p_{1}\) is GPT-3.5; _everyone_ is worse off, 2) GPT-3.5 agents can get lower scores than their counterparts in the 'all GPT-4' experiment.

### Other Open-Source Models

We use the best prompt template from our ablation (on GPT-4) to test other models. We excluded Mistral 7b  and Llama-2 7b as they did not follow the basic formatting of the game. As shown in Table 3, open-source models perform worse than GPT-4 but better than GPT-3.5.

  
**Models** & **Final**\(\) \\  All GPT-4 & 81 \\ All GPT-3.5 & 20 \\  \(p_{1}\) is GPT-3.5 & 50 \\ \(P_{}\) are GPT-3.5 & 62 \\   

Table 2: Success (%) with a mixed population of models.

Figure 4: Example from GPT-4 simulation. The agent takes the interaction history along with its initial prompt and instructions that incentivize it to _cooperate_, which are _structured_ as _observation_, _exploration_, and _planning_ steps. The agent here _autonomously_ and iteratively adjusts its suggestions.

Llama-3 70b comes close to GPT-4 considering agreement success, correct calculations, and not revealing scores. Other models are especially worse in calculation and keeping confidential scores (higher wrong deals and leaked scores ratios). I.e., **our benchmark is already challenging for many SoTA models**, and as shown next, its difficulty can be increased to test future models. Due to its higher performance, we perform the rest of our analysis on GPT-4.

### Performance on Other Games

To test robustness against semantically similar changes, we rewrite the base game by prompting GPT-4 to change the entities and issue names while maintaining semantic relationships. As shown in Table 4, the performance on the base and rewritten games is comparable. Also, agents perform relatively well on the new games (game 1, game 2, and game 3, created from scratch) with varying levels of success. While all games have a comparable number of feasible solutions, games 1 and 2 can be more competitive as they have non-sparse scores (i.e., all agents have preferences on almost all issues). This might require more fine granularity when proposing deals; from the perspective of one agent, deals with comparable or even the same scores might have a highly fluctuating number of agreeing parties. Therefore, to match the base game, we designed game 3 to have more sparse scores, which indeed scored similarly w.r.t. the final deal metric. More analysis of the games' difficulty is in Appendix E. We also extended the base game by prompting GPT-4 to add another player and another issue, while specifying the motivation and preferences of that additional player and the preferences of the original players w.r.t the new issue. The total number of deals is now 2880 vs 720 originally. We manually set the scores to have a comparable ratio of feasible deals, and we ran GPT-4 agents on this new game 80 times to accommodate the larger action space. Even though this game has a comparable ratio of feasible deals, the performance drops compared to the base game. In summary, our benchmark has **diverse games with easily tunable difficulty** to test future advanced models.

### Tuning the Game Difficulty

Besides having diverse games, the difficulty of games can be easily tuned by changing agents' minimum thresholds and re-running the simulation while keeping everything else fixed. This is critical since we witness a saturation of older benchmarks with the release of powerful models and training data contamination [48; 20]. Our evolving benchmark can help foster future research as there is still ample room for improvement; success drops when we decrease the set of feasible solutions (the last part in Table 4), indicating that advanced paradigms in communication, exploration, and planning can be incorporated. In addition, _decreasing the number of players_ can be used to create _easier_ games, as shown in our experiment in Appendix F, in which simulations with fewer agents have higher all-way agreement rates. This motivates our multi-agent setup as it results in a more challenging environment.

### Random and Heuristic-based Baselines

To contextualize the previous agents' performance, we provide baselines by the statistical properties of games or via simulating randomized interactions with LLMs or with rule-based heuristics.

**Statistical properties of games.** For each game and difficulty condition, we can statistically compute how likely a random deal would lead to success given the thresholds of all parties; e.g., for the base game, this ratio is 55/720; for the difficulty levels in Table 4, it would be 30/720 and 17/720, respectively.

    &  &  \\    & \(n-1\) & \(n\) \\  Base (55/12) & 81 & 33 & 100 \\   \\  Base\({}_{}\) (55/12) & 86 & 24 & 100 \\  New (1 57/21) & 65 & 10 & 85 \\ New (2 57/18) & 70 & 40 & 90 \\ New (3 57/34) & 86 & 81 & 95 \\ Base\({}_{}\) (204/2880) & 63 & 18 & 96 \\   \\  Base (30/4) & 65 & 25 & 85 \\ Base (17/2) & 30 & 5 & 70 \\   

Table 4: Success (%) of GPT-4 on new games and difficult levels of the base game. (#/#) are the number of (\(n-1\))-way and \(n\)-way deals, respectively.

    &  &  &  &  \\   &  &  \\  GPT-4 & **81** & **33** & **100** & **1.4** & **0** \\ GPT-35 & 20 & **8** & 33 & 19 & 25 \\ Llama-2 13b & 57 & 10 & 82 & 16 & 14 \\ Llama-2 70b & 76 & 19 & 95 & 11 & 22 \\ Llama-3 70b & 60 & 21 & 100 & 4 & 2 \\ Gemini 10b & 45 & 0 & 70 & 13 & 6 \\ Miral 8x78 & 65 & 17 & 95 & 11 & 12 \\   

Table 3: Performance (%) of other models.

**Interactive baselines.** We add a baseline where we prompt GPT-4 agents to give a random deal at each round. For the base game, the final success rates of this experiment are 10% and 3% for 5 and 6 agreements, respectively, for 120 negotiation sessions. The "wrong deals" ratio is also high (\(\)20%). This shows that the reasoning done by the agents optimized with CoT is crucial.

We also add a repeated rule-based baseline, shown in Table 5, that is based on simulating randomized interaction without using LLMs. Here, we start with a random deal and select one agent at a time to improve over the last proposed deal by changing one option at a time until its corresponding minimum threshold is met or no more changes can be made. The agent starts from the highest til the lowest priority issue, and for each issue, picks the best option. The next agent iterates over the last proposed deal. The last agent to change is \(p_{1}\). We run this for a large number of randomized orders and starting deals, and we use the unique set of achieved deals to compute 5- and 6-way success ratios. This analysis shows that smaller LLMs fail below the baselines while more capable models outperform them, and its also consistent with our analysis that game 3 is the easiest.

### Greedy and Adversarial Variants

We now study the other variants discussed in Section 3.2 and aim to answer two main questions:

**1) Are agents' actions consistent with their high-level incentives?** We calculate the "own score" and "collective score" of the same agent assigned with the different incentives, as shown in Figure 5. In the compromising variant, the "own score" is the lowest, while the "collective score" is high. In the greedy variant, the "own score" is higher, but the agent is still finding deals that might be agreeable (i.e., indicated by a relatively high "collective score"). In the adversarial variant, the "own score" is also high, but the agent's suggested deals give a low "collective score". In the targeted version, the target's score is lower compared to the untargeted case. It is important to note that the agent _cannot see_ others' scores and that instructions _never_ included what specific deals to propose. While GPT-4 mapped these incentives to plausible corresponding deals, GPT-3.5 **failed** to do so (see Figure 19), indicating that this is a non-trivial task.

**2) What are the effects on the negotiation?** We show in Table 6 that the success rate is lower compared to the compromising game; **the greedy/adversarial agents' actions affected the group**. We quantitatively and qualitatively show in Figure 6 and Appendix G that the negotiation's course (i.e., the final deal made by \(p_{1}\)) may eventually **over-reward** the greedy agent, at the expense of others or \(p_{1}\) itself. When \(p_{1}\) is greedy, the success drastically decreases. This could be an attack vector where \(p_{1}\) is prompted to be greedy (by external parties) or when it _only acts_ as compromising to deceive a moderator. When all agents are greedy, the performance is similar to when \(p_{1}\) is greedy, which is expected since \(p_{1}\) makes the final suggestion.

  
**Variant** &  \\  & **5/6-way** & **6-way** \\  All compromising & 81 & 33 \\  One greedy (\(p_{i} P_{}\)) & 57 & 30 \\ One greedy (\(p_{1}\)) & 27 & 9 \\ Two greedy (\(P_{}\)) & 65 & 15 \\ All greedy & 26 & 11 \\  Adversarial (untargeted) & 63 & - \\ Adversarial (targeted) & 58 & - \\   

Table 6: Success in the different variants.

Figure 5: The “own score” and “collective score” of the same agent’s deals, \(p_{i} P_{}\), in the different variants. Another agent \(p_{v}\) is the target in the targeted adversarial variant. \(p_{i}\)’s actions are consistent with its assigned incentives.

We observed that greedy agents more clearly communicate their highest preferences; future methods can be used here to encourage building coalitions based on that.

The adversarial agent shows success in preventing the deal in the untargeted version. However, since this agent clearly proposes deals that are against the majority, we qualitatively observed that other compromising agents often echoed the majority and proposed deals that are likely to be more agreeable (especially by \(p_{1}\) and \(p_{2}\)). This may be a positive sign that agents are not easily malleable and can detect the intruder. Attacking a specific agent was more successful, especially if the adversary aligns with the preferences of \(p_{1}\) and \(p_{2}\), **creating a powerful coalition**. We quantitatively show that **the targeted agent gets a lower score in the final deal**. More results are in Appendices G, H, and J.

## 5 Related Work

Previous work used and evaluated LLM agents in tasks such as web browsing or synthesizing knowledge . In addition,  used negotiation games to evaluate LLMs either non-interactively or with only two players. Our work proposes a vastly more complex environment. First, our simulation consists of more players, with different roles such as leading and veto parties, adding substantial complexity to the interaction and evaluation criteria and making it more realistic. Secondly, it entails richer indirect semantic connections between entities and the negotiation issues, i.e., inferring others' preferences is not a straightforward task and would require common-sense reasoning and ToM. Third, our easily expandable benchmark consists of 4 games, each with a completely different simulation. Importantly, we introduce novel attacks that evaluate 1) how agents' actions can be modulated based on high-level incentives to be greedy or adversarial and 2) how these actions can affect other compromising agents as a ripple effect. Such questions are highly pressing from AI safety perspectives and cannot be adequately studied with only two players; e.g., identifying the malicious player would be trivial. Our work and others highlight that multi-agent safety has its unique challenges over simpler setups . As an orthogonal direction, previous work has used debate between LLM agents to better evaluate the quality of text , get truthful answers , or as a method for scalable oversight . Previous work also used LLM agents to create simulation environments ; however, not for the purpose of evaluation.

## 6 Limitations

**Scaling.** In our paper, we show that we can scale the game by adding additional players or issues. Another potential method to scale the games to less constrained setups is to exclusively use continuous issues rather than discrete ones (our issues take both continuous, such as budget, and discrete formats, such as locations). For continuous issues, utility can be an arbitrary continuous function.

**Changing thresholds.** To adjust games, we mostly used a strategy of repeated manual tweaking and observing the number of feasible deals for the game and for each agent. Future work could use more principled algorithms based on Pareto efficiency calculations.

## 7 Conclusion

Multi-agent LLMs are a promising avenue for future cross-organizational autonomous systems. Negotiation exemplifies a technically challenging, interactive, and multi-step task that is practically relevant for such use cases and many others. Motivated by this, we design a dynamic and evolving benchmark, with adjustable difficulty, for multi-agent negotiation with complex cooperation and competition dynamics. This enabled us to study novel cross-agent attacks and exploitation. The task is not solved yet; all open-source models are less successful than GPT-4, which still underperforms when increasing difficulty and in games with non-sparse payoffs. We hope future work will explore other reasoning and planning methods, manipulation setups (e.g., private communication), potential defenses (e.g., detecting and penalizing intruders via moderator agents) and evasion attacks (e.g., deceiving the moderator), and other safety considerations (e.g., biases).

Figure 6: \(p_{1}\)’s deals w.r.t. to itself (pink) and another agent (green) assigned as compromising (left) or greedy (right).