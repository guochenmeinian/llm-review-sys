# Multiview Scene Graph

Juexiao Zhang Gao Zhu Sihang Li Xinhao Liu

Haorui Song Xinran Tang Chen Feng

New York University

{juexiao.zhang, cfeng}@nyu.edu

###### Abstract

A proper scene representation is central to the pursuit of spatial intelligence where agents can robustly reconstruct and efficiently understand 3D scenes. A scene representation is either metric, such as landmark maps in 3D reconstruction, 3D bounding boxes in object detection, or voxel grids in occupancy prediction, or topological, such as pose graphs with loop closures in SLAM or visibility graphs in SfM. In this work, we propose to build _Multiview Scene Graphs_ (MSG) from unposed images, representing a scene topologically with interconnected place and object nodes. The task of building MSG is challenging for existing representation learning methods since it needs to jointly address both visual place recognition, object detection, and object association from images with limited fields of view and potentially large viewpoint changes. To evaluate any method tackling this task, we developed an MSG dataset based on a public 3D dataset. We also propose an evaluation metric based on the intersection-over-union score of MSG edges. Moreover, we develop a novel baseline method built on mainstream pretrained vision models, combining visual place recognition and object association into one Transformer decoder architecture. Experiments demonstrate that our method has superior performance compared to existing relevant baselines. All codes and resources are open-source at [https://ai4ce.github.io/MSG/](https://ai4ce.github.io/MSG/).

## 1 Introduction

The ability to understand 3D space and the spatial relationships among 2D observations plays a central role in mobile agents interacting with the physical real world. Humans obtain such spatial intelligence largely from our visual intelligence . When humans are situated in an unseen environment and try to understand the spatial structure from visual observations, we don't perceive and memorize the scene by exact meters and degrees. Instead, we build cognitive maps topologically based on visual observations and commonsense . Given imagery observations, we are able to associate the images taken at the same place by finding overlapping visual clues and identifying the same or different objects from various viewpoints. This ability to establish correspondence from visual perception constitutes the foundation of our spatial memory and cognitive representation of the world. Can we equip AI models with similar spatial intelligence?

Motivated by this question, we propose the task of building a **Multiview Scene Graph (MSG)** to explicitly evaluate a representation learning model's capability of understanding spatial correspondences. Specifically, as illustrated in Figure 1, given a set of unposed RGB images taken from the same scene, this task requires building a _place+object_ graph consisting of images and object nodes, where images taken at nearby locations are connected, and the appearances of the same object across different views should be associated together as one object node.

We position the proposed Multiview Scene Graph as a general topological scene representation. It bridges the place recognition from robotics literature [3; 4; 36] and the object tracking and semantic correspondence tasks from computer vision literature [20; 23; 64]. Different from previous work in topological mapping that evaluates a method's performance on downstream tasks such as navigation, we propose to directly evaluate the quality of the multiview scene graph, which explicitly demonstrates a model's spatial understanding with correct visual correspondence of both objects and places across multiple views. Moreover, the MSG does not require any metric map, depth, or pose information, making it adaptable to the vast data of everyday images and videos. This also differentiates MSG from the previous work in 2D and 3D scene graphs [5; 32; 35; 69], where they emphasize objects' semantic relationships or require different levels of 3D and metric information.

To facilitate the research of MSG, we curated a dataset from a publicly available 3D scene-level dataset ARKitScenes  and designed a set of evaluation metrics based on the intersection-over-union of the graph adjacency matrix. The detailed definition of the MSG generation task and the evaluation metrics are discussed in Section 3.1. Meanwhile, since this task mainly involves solving place recognition and object association, we benchmarked popular baseline methods respectively in place recognition and object tracking, as well as some mainstream pretrained vision foundation models. We also designed a new Transformer-based architecture as our method, Attention Association MSG, dubbed _AoMSG_, which learns place and object embeddings jointly in a single Transformer decoder and builds the MSG based on the distances in the learned embedding space. Our experiments demonstrate the superiority of our new model compared with the baselines by a great margin, yet still reveal strong needs for future advances in research for spatial intelligence.

In summary, our contributions are two-fold:

* We propose the Multiview Scene Graph (MSG) generation as a new task for evaluating spatial intelligence. We curated a dataset from a publicly available 3D scene dataset and designed evaluation metrics to facilitate the task.
* We design a novel Transformer decoder architecture for the MSG task. It jointly learns embeddings for places and objects and determines the graph according to the embedding distance. Experiments demonstrate the effectiveness of the model over existing baselines.

## 2 Related work

Scene GraphScene graphs [35; 69] are originally proposed to represent the spatial and semantic relationships between objects in an image. The generated scene graph can be used for image captioning  and image retrieval . Although they provide a structured spatial representation, it remains at the 2D image level. 3D scene graphs [5; 32; 66; 71; 33] extend this concept into 3D, representing a scene as a topological graph with objects, rooms, and camera positions as their nodes.

Figure 1: **Multiview Scene Graph (MSG). The task of MSG takes unposed RGB images as input and outputs a place+object graph. The graph contains place-place edges and place-object edges. Connected place nodes represent images taken at the same place. The same object recognized from different views is associated and merged as one node and connected to the corresponding place nodes.**

These graphs are typically built by abstracting from 3D meshes, point clouds, or directly from RGB-D images.  proposes incrementally building 3D scene graphs from RGB sequences, describing semantic relationships between objects. As a new type of scene graph, MSG is built from unposed images without sequential order, emphasizing the understanding of relationships between objects and places via multiview visual correspondences. MSG complements existing scene graphs, as their object-object relationship edges can be a seamless add-on to extend MSG with more semantic information. Therefore, we believe MSG provides a meaningful contribution to the scene graph community by enhancing its representational depth and flexibility.

Scene MappingSimultaneous localization and mapping (SLAM) [17; 46; 57; 59] is a classic way of creating maps of an environment from observations. The metric maps built from SLAM are subsequently utilized as the spatial representation for the robots to perform tasks such as navigation. In contrast to metric maps, topological mapping  is inspired by landmark-based memory in animals, and follows a more natural and human-like understanding of the environment to better support navigation tasks. The quality of the topological maps is evaluated mostly through navigation tasks [11; 15]. Another line of scene mapping work harnesses object or semantic information to build more robust maps [25; 54; 68], with TSGM  being the most relevant to our work. Differently, our proposed MSG serves as a general-purpose scene representation and can be directly evaluated using our proposed metrics. The quality of MSG that a model can build explicitly evaluates its capability of understanding spatial correspondences.

Visual Place RecognitionVisual Place Recognition (VPR) is often formed as an image retrieval problem. This involves extracting image features and retrieving the closest neighbor from an image database. Traditional approaches rely on handcrafted features [9; 41]. NetVLAD and its variants[4; 16; 44] use deep-learned image features to improve recall performance and robustness. The emergence of self-supervised foundation models, such as DINOv2 , enables universal image representations, offering significant progress [34; 36] across many VPR tasks. However, VPR is framed as an image retrieval problem, whose output--the image features--does not directly equal a graph. Although a graph can be built by proximity search in the VPR feature space, the widely used recall metric in VPR does not directly reflect how good the graph is, i.e. how many pairs of connected images in this graph are truly at the same place. Instead, our proposed task and evaluation metric focus only on the graph generated from the model. The metric straightforwardly reflects the quality of the scene representation.

Object AssociationTraditionally, object association is approached by matching keypoint features across image pairs [41; 55]. Recently, CSR  learns feature encodings of object detections and measures the cosine similarity between the learned features to determine object matching. ROM  on the other hand follows SuperGlue  and uses attentional GNN and Sinkhorn distance  for relational object matching. Our method draws inspiration from this previous work but adopts a Transformer decoder architecture and learns object instance embeddings jointly in a unified model with place recognition. Literature in multi-object tracking [50; 64; 65; 72] and video object segmentation [19; 20; 31; 62] also handles object association. They mostly leverage temporal dependencies or memories such as by propagating detection bounding boxes or segments through time. Therefore, these models may lack a sense of space and suffer when objects reappear from a very different viewpoint or after a longer period. Interestingly, a recent study Probe3d  reveals that even though the pretrained vision foundation models have undergone tremendous progress in the recent years [14; 21; 30; 38; 49], they still struggle with associating spatial correspondences of objects from large viewpoint change. Our method learns scene representation with spatial correspondence, where multiple views of the same places or the same objects are close in the embedding space.

## 3 Multiview scene graph

### Problem definition

Multiview Scene GraphGiven a set of unposed images of a scene \(X=\{x_{i}\}_{i=0,,T}\), we represent a Multiview Scene Graph as a **place+object graph**:

\[G=\{P,O,E^{PP},E^{PO}\}, \]where \(P\) and \(O\) respectively refer to the sets of _place_ and _object_ nodes. The set of object nodes \(O\) contains all the objects detected from \(X\). The same object detected from different images across different viewpoints should always be considered as one object node. For the definition of places, we follow the definition in the VPR literature and set \(P=X\). This means each image corresponds to a node for a place, and if two images are taken within only a small translation and rotation distance, they are considered as taken in the same place and are connected with an edge in \(E^{PP}\). Consequently, the \(E^{PP}\) is the set of _place-place edges_ which refers to the edges that connect the images regarded as in the same place, and the \(E^{PO}\) represents the set of _place-object edges_, referring to the edges that connect the places and the objects that appear in those places. Therefore, an object can be seen in multiple images and thus connected to more than just one place node. These images can be close by or from a distance. Naturally, a place node can connect to more than one object node, since an image can contain multiple objects' appearances.

MSG generation taskAs illustrated in Figure 1, the MSG generation task requires building an estimated place+object graph \(\) from the unposed RGB image set. The graph is further represented as a place+object adjacency matrix \(\) of size \((|P|+||)(|P|+||)\), while the groundtruth \(G\) is represented by \(A\) of size \((|P|+|O|)(|P|+|O|)\). Note that the object set \(\) may differ from \(O\). The quality of \(\) is evaluated by measuring \(\) against the groundtruth \(A\). According to our definition, the adjacency matrix can be further decomposed into the following block matrix:

\[A=[A^{PP}&A^{PO}\\ A^{OP}&A^{OO}], \]

where \(A^{PP}=A_{1 i|P|,1 j|P|}\) and \(A^{PO}=A_{1 i|P|,|P|+1 j|P|+|O|}\). The same decomposition applies to \(\). Since the MSG contains only the place-place edges and the place-object edges, \(A^{OO}\) is left blank. Meanwhile, \(A^{PO}\) is symmetric to \(A^{OP}\). So our evaluation will focus on \(A^{PP}\) and \(A^{PO}\).

### Evaluation metric

Given that the two adjacency matrices \(A\) and \(\) are binary, we evaluate their intersection over union (IoU) to measure how much the two graphs align. As aforementioned, an adjacency matrix \(A\) essentially consists of two parts: the place-place part \(A^{PP}\) and the place-object part \(A^{PO}\). So we evaluate them respectively as PP IoU and PO IoU and combine them to get the whole graph IoU. We provide a precise mathematical definition of the IoU calculation for any two binary adjacency matrices in Appendix B.1 and we denote this function by IoU\((,)\) in the following for simplicity.

PP IoUFor the PP IoU, the calculation is relatively straightforward since the number of images is deterministic and the one-to-one correspondence between the groundtruth \(A^{PP}\) and the prediction \(^{PP}\) is fixed. As a result, the PP IoU is simply:

\[=(A^{PP},^{PP}). \]

Additionally, we also report the Recall@1 score alongside PP IoU since it is the standard evaluation metric for visual place recognition.

PO IoUHowever, it is less straightforward for the PO IoU. The number of objects in the predicted set \(\) may differ from \(O\), and their correspondence cannot be determined directly from the adjacency matrix. For a fair evaluation, we need to align \(\) with \(O\) as much as possible. In other words, before computing IoU, we need to find the best matching object for each groundtruth object. This truth-to-result matching is also an important issue in multi-object tracking . To do so, we also record the object bounding boxes in each image and calculate the generalized IoU score (GIoU) of the bounding boxes following . Then we compute a one-to-one matching between \(O\) and \(\) based on the accumulated GIoU score across all the images. Details of the score computation are included in the Appendix B.2. According to the matching, we can reorder \(\) to best align with the objects in \(O\). This can be mathematically expressed as a permutation matrix \(S^{||||}\) to permute the columns of \(\). Formally, the PO IoU is expressed as the following:

\[=(A^{PO},^{P}S). \]

## 4 Our Baseline: Attention Association MSG Generation

When developing a new model for the MSG generation task, we adhere to two core principles: Firstly, the model should capitalize on the strengths of pretrained vision models. These pretrained models offer a robust initialization for subsequent vision tasks, as their output features encapsulate rich semantic information, forming a solid foundation for tasks like ours. Secondly, both place recognition and object association fundamentally address the problem of visual correspondence and can mutually reinforce each other through contextual information. Thus, our model is designed to integrate both tasks within a unified framework. With these guiding principles, we propose the Attention Association MSG (AoMSG) model, depicted in Figure 2.

Place and object encodingsGiven a batch of unposed RGB images as input, the AoMSG model first employs pretrained vision encoders and detectors to derive image tokens and object detection bounding boxes from each image. We utilize the Vision Transformer-based pretrained model DINOv2  as our encoder, though our design is adaptable to any Transformer-based or CNN-based encoder that produces a sequence of tokens or a feature map. In the case of the DINOv2 encoder, we reshape the output token sequences into a feature map, which is then aligned to the object bounding boxes, aggregating an encoding feature for each detected object. To integrate place recognition and object association within a unified framework, we obtain the place encoding feature by treating it as a large object with a bounding box that encompasses the entire image, aggregating features as if a detected object. The obtained place feature is then positioned alongside the object features, serving as queries for the Transformer decoder, as detailed in the subsequent sections.

AoMSG decoderWe follow a DETR-like structure  to design our AoMSG decoder. Specifically, the derived place feature and object features are stacked as a sequence of queries for the Transformer decoder, while the preceding image tokens are used as keys and values. As shown in Figure 2, we enhance the queries by incorporating positional encodings by normalizing and embedding the bounding box coordinates. For instance, for the place feature, the equivalent bounding box is the entire image as aforementioned, resulting in the normalized coordinates of . These coordinates are projected to match the dimensionality of the encoding and added elementwise to the place query. The outputs of the AoMSG Transformer decoder are the place and object embeddings that have aggregated context information from the image tokens. Then two linear projector heads are applied to each object and place embeddings respectively to obtain the final object and place embeddings, projecting them into the representation space for the task.

Losses and predictionsFor training, we compute supervised contrastive learning  respectively on the place and object embeddings from the same training batch in a multitasking fashion. For the object loss, we simply use binary cross-entropy with higher positive weights. For the place loss,

Figure 2: **The AoMSG model.** Places and objects queries are obtained by cropping the image feature map using corresponding bounding boxes. The queries are then fed into the Transformer decoder to obtain the final places and objects embeddings. Bounding boxes are in different colors for clarity. The parameters in the Transformer decoder and the linear projector heads are trained with supervised contrastive learning. Image encoder and object detector are pretrained and frozen.

the mean square error is minimized for their cosine distances, which gives better empirical results. During inference, we simply compute the cosine similarity among the place embeddings and apply a threshold to obtain the place-place predictions in \(\). For the objects, we track their appearances and maintain a memory bank of the existing objects for each scene, updating their embeddings or registering new objects based on cosine similarity and thresholding. The results are consequently converted to the place-object part in \(\). Notably, there could be many possible choices to compute the contrastive losses and determine the predictions, we keep our choices simple as we empirically find the standard losses and the simple cosine thresholding can already produce decent results while keeping the embedding spaces straightforwardly meaningful. We discuss the results in detail in Section 5.

## 5 Experiment

### Data

The MSG models can be trained with any dataset that provides camera poses and object instance labels. We utilized the publicly available 3D indoor scene dataset ARKitScenes  to construct our dataset. ARKitScenes contains point clouds and 3D object bounding boxes of the scenes, as well as the calibrated camera poses obtained from an iPad Pro. We transform the point clouds in the 3D bounding boxes with respect to the camera poses to obtain the 2D bounding boxes in each frame. The resolution of each frame is \(192 256\). 4492 scenes are used for training and 200 scenes are used for testing. Note that none of the two scenes share the same objects. We leverage the camera poses to obtain the place annotations. Translation threshold and rotation threshold are set to 1 meter and 1 radian respectively, images taken within both thresholds are considered as capturing the same place.

    &  \\   &  &  &  &  \\   & & & & **w/ GT detection** & **w/ GDino ** \\  AnyLoc  & - & 97.1 & 34.2 & - & - \\ NetVlad  & - & 96.6 & 35.5 & - & - \\ Mickey  & - & 100* & 33.1 & - & - \\ SALAD  & - & 97.1 & 35.6 & - & - \\ - & UniTrack  & - & - & 17.4 & 13.0 \\ - & DEVA  & - & - & 16.2 & 16.6 \\  SepMSG - Direct & 96.0 & 31.4 & 50.4 & 24.5 \\ SepMSG - Linear & 96.9 & 34.9 & 59.3 & 24.6 \\ SepMSG - MLP & 94.3 & 29.2 & 56.9 & 23.4 \\ AoMSG-2 & 97.2 & 40.7 & 69.1 & 28.1 \\ AoMSG-4 & 98.3 & 42.2 & 74.2 & 28.1 \\   

Table 1: **Main results.** Our method uses DINOv2 as the backbone. GDino stands for the detector GroundingDINO. AoMSG-2 and AoMSG-4 represent AoMSG models with 2 and 4 layers of Transformer decoder respectively. The best results are underlined. * indicates a trivial result since its input is given in temporal order, and consecutive frames are trivially recalled.

    &  &  \\   & Recall@1 & PP IoU & PO IoU & Recall@1 & PP IoU & PO IoU \\ 
512 & 97.7 & 41.3 & 72.9 & 93.2 & 20.3 & 59.2 \\
1024 & 98.3 & 42.2 & 74.2 & 96.9 & 34.9 & 59.3 \\
2048 & 97.9 & 41.8 & 72.4 & 96.5 & 35.0 & 58.9 \\   

Table 2: **Comparison of different projector dimensions** in AoMSG and SepMSG models. Both are using DINOv2-base as the backbone. Results are evaluated at 30 epochs.

### Baselines

VprWe adopt protocols in the previous VPR benchmark literature as outlined in [10; 36]. In our off-the-shelf baselines, we evaluated VPR using DINOv2  either as the global descriptor or followed by a VLAD dictionary generated from a large-scale indoor dataset following , or as a feature extraction backbone . For the trained baseline, we conduct our experiments mainly with ResNet-50  + NetVLAD used in . Additionally, we also test a recent pose estimation baseline  and use the poses to estimate the places according to the same thresholds as in the dataset.

Object associationWe adopt two popular baselines for object association, Unitrack  from multi-object tracking, and DEVA  from video object segmentation. The image sets are processed in temporal order just like tracking. Unitrack can take any detection backbones and associate object bounding boxes by comparing their features with an online updating memory bank. For a fair comparison, we extend its memory buffer length to cover the whole set of images for every scene. DEVA leverages the Segment Anything model  to segment and track any object throughout a video without additional training. Their tracking results can be easily converted for evaluating object association based on the tracker IDs.

SepMSGWe also evaluate the pretrained vision models by first separately encoding images and object detections to features and directly evaluating MSG based on those features. This baseline is referred to as _SepMSG-Direct_, where _Sep_ means _separately_ handling places and objects. Then as a common way of evaluating pretrained models [29; 30], we conduct probing  by further training a linear or MLP classifier on those frozen features. These baselines are referred to as _SepMSG-Linear_ and _SepMSG-MLP_. The SepMSG baselines serve as ablation to validate our model against simply using features learned from the pretrained backbones.

### Experimental setups

For AoMSG, we experimented with different choices of backbones, sizes of the Transformer decoder, and dimensions of the final linear projector heads. Their results are discussed in Section 5.4. All the models are trained on a single H100 or GTX 3090 graphics card for 30 epochs or until convergence. We provide detailed hyperparameters in the appendix. During training, we randomly shuffle the scenes and mix data from multiple scenes in a single batch so that the model sees diversified negative samples at every epoch. Additionally, we monitor the total coding rate as in  to avoid the embeddings from collapsing.

To keep the evaluation focused on the quality of the graph rather than the quality of object detection, we choose not to train the detector together with the MSG objectives. Instead, we use the groundtruth

Figure 4: Visualization of the same objects and the same places. Objects are annotated with their predicted IDs.

Figure 3: Performance of different encoder backbones. We report results from the base models for both ConvNext  and ViT .

detection bounding boxes and a popular open-vocabulary object detector GroundingDINO . Results on both configurations are listed in Table 1 and discussed in the following.

### Results

Main resultsTable 1 shows comparison of our results and baselines. We find that for the place Recall@1 and PP IoU, the baselines have competitive performance. While the results from the SepMSG baselines are comparable, AoMSG outperforms them all and produces the best results in both metrics. We also notice that all the models produce high Recall@1, but their PP IoU scores are varied and less than 50. This suggests that having high recall is not enough to guarantee a good graph. For PO IoU, AoMSG models outperform all the baselines by big margins. Both Unitrack and DEVA perform poorly as they struggle when objects reappear after large viewpoint changes or long periods of time. We note that all the MSG methods produce relatively worse results when using GroundingDINO as the detector rather than the ground truth detection. This indicates the performance gap caused by inaccurate object detection. Nevertheless, their performances are still consistent and AoMSG still performs the best. This suggests a better detector will likely give better results for the MSG task. To conclude, AoMSG gives the best performance for all the metrics.

Projector dimensionsAs listed in Table 2, we compared the impact of different projector dimensions as it is reportedly important to performance in the literature of self-supervised representation learning . We find the empirical results are comparable in our experiments.

Choices of backbonesFigure 3 shows the performance of difference choices of pretrained backbones. We experimented with state-of-the-art CNN-based model ConvNext , Vision Transformer (ViT)  and DINOv2 . We find DINOv2 performs the best, consistent with the observations made in . We use DINOv2 as our default encoder. Interestingly, performance saturates with the size of DINOv2. We suspect it will still increase if we could further scale the size of the data.

QualitativeIn Figure 5 we visualized the learned object embeddings on 6 scenes by AoMSG, the SepMSG-Linear baseline, and SepMSG-Direct that directly uses the output features from the pretrained DINOv2 encoder for the task. The visualization aims to qualitatively assess the learned object embeddings as to how separated different objects are in the embedding space. We can see the pretrained embeddings already provide some decent separations. SepMSG-Linear only tunes a linear probing classifier on top so the separation is slightly improved. For example, see the first and second scenes to the left. Compared with them, AoMSG gives the most significant separations, with appearances of the same objects pushed closely and different objects pulled far away. Additionally, Figure 4 visualizes results on some places and objects, and we provide more in Appendix D.

## 6 Discussion

### Application

Given the recent advances in novel view synthesis, 3D reconstruction, and metric mappings, one might wonder whether the proposed MSG is still useful. Here we provide some justifications and a showcase application. Echoing literature in 3D scene graphs , we believe the MSG can be a versatile mental model for embodied AI agents and robots. At a global level, it keeps a lightweight topological memory of the scene from purely 2D RGB inputs, which serves as a basis for robot navigation . At a finer level, it can seamlessly couple MSG with the 3D reconstruction methods, to estimate depth and poses and build local reconstructions. Therefore, a robot can traverse the environment, localize itself referring to the MSG, and build a local reconstruction when needed for tasks that require metric information such as the manipulation tasks.

As a showcase application, we provide two local 3D reconstruction cases illustrated in Figure 6 using the most recent off-the-shelf 3D reconstruction model Dust3r . Directly applying Dust3r to a dense image set greatly consumes GPU memory, which may be infeasible for mobile robots. Whereas a random subsample does not guarantee the reconstruction quality. Instead, with MSG, we can provide the Dust3r with locally interconnected subgraphs for fast and reliable local reconstruction. The subgraphs and local reconstructions can be object-centric thanks to the _place+object_ nature of MSG. Moreover, the local reconstructions are topologically connected by MSG. This suggests MSG can provide a flexible scene representation balancing 2D and 3D, abstractions and details.

### Limitation

The current work still has many limitations. Firstly, we only conducted experiments in one dataset. Although the dataset contains around 5k scenes, which is sufficient to obtain convincing results, it would still be great to see if training on more diversified data collections can produce better models and stronger generalization as observed in , especially for larger models. We leave this to future work. Secondly, scenes in the current dataset contain only static objects, extending to dynamic objects is a direction worth exploring.

Additionally, given the scope of the work is to propose MSG as a new vision task promoting spatial intelligence, we focus on explicitly evaluating the quality of the graph. Therefore, we did not

Figure 5: Object embedding visualization using t-SNE . SepMSG-Direct, SepMSG-Linear, and AoMSG-2 are shown in each row respectively. Results from the same scene are aligned vertically. Colors indicate different objects. Each point is an appearance of an object. It is best viewed in color.

Figure 6: Local 3D reconstruction from 2D MSG using off-the-shelf model Dust3r . The 3D meshes of two scenes are shown side by side, with 3 subgraphs circled in gray and reconstructed on the top of each scene.

investigate the object detection quality, nor did we deploy the MSG to downstream tasks such as navigation. Note that detection quality does affect the MSG performance though we find it to be consistent across different detection modes, i.e. the groundtruth and the GroundingDINO. Training detectors together with the MSG model and applying MSG to downstream tasks will be our next step to make the work a more complete system.

## 7 Conclusion

This work proposes building the Multiview Scene Graph (MSG) as a new vision task for evaluating spatial intelligence. The task gives unposed RGB images as input and requires a model to build a place+object graph that connects images taken at the same place and associates the object recognitions from different viewpoints, forming a topological scene representation. To evaluate the MSG generation task, we designed evaluation metrics, curated a dataset, and proposed a new model that jointly learns place and object embeddings and builds the graph based on embedding distances. The model outperforms existing baselines that handle place recognition and object association separately. Lastly, we discussed the possible applications of MSG and the current limitations. We hope this work can stimulate future research on advancing spatial intelligence and scene representations.

Acknowledgement.The authors thank Yiming Li and Shengbang Tong for their valuable discussions and suggestions.