# Energy-Guided Continuous Entropic Barycenter Estimation for General Costs

Alexander Kolesov

Skolkovo Institute of Science and Technology

Artificial Intelligence Research Institute

Moscow, Russia

a.kolesov@skoltech.ru

&Petr Mokrov & Igor Udovichenko

Skolkovo Institute of Science and Technology

Moscow, Russia

{p.mokrov, i.udovichenko}@skoltech.ru

Milena Gazdieva

Skolkovo Institute of Science and Technology

Artificial Intelligence Research Institute

Moscow, Russia

m.gazdieva@skoltech.ru

&Gudmund Pammer

Graz University of Technology

Graz, Austria

gudmund.pammer@tugraz.at

Anastasis Kratsios

Vector Institute, McMaster University

Ontario, Canada

kratsioa@mcmaster.ca

&Evgeny Burnaev & Alexander Korotin

Skolkovo Institute of Science and Technology

Artificial Intelligence Research Institute

Moscow, Russia

{e.burnaev, a.korotin}@skoltech.ru

###### Abstract

Optimal transport (OT) barycenters are a mathematically grounded way of averaging probability distributions while capturing their geometric properties. In short, the barycenter task is to take the average of a collection of probability distributions w.r.t. given OT discrepancies. We propose a novel algorithm for approximating the continuous Entropic OT (EOT) barycenter for arbitrary OT cost functions. Our approach is built upon the dual reformulation of the EOT problem based on weak OT, which has recently gained the attention of the ML community. Beyond its novelty, our method enjoys several advantageous properties: (i) we establish quality bounds for the recovered solution; (ii) this approach seamlessly interconnects with the Energy-Based Models (EBMs) learning procedure enabling the use of well-tuned algorithms for the problem of interest; (iii) it provides an intuitive optimization scheme avoiding min-max, reinforce and other intricate technical tricks. For validation, we consider several low-dimensional scenarios and image-space setups, including _non-Euclidean_ cost functions. Furthermore, we investigate the practical task of learning the barycenter on an image manifold generated by a pretrained generative model, opening up new directions for real-world applications. Our code is available at [https://github.com/justkolesov/EnergyGuidedBarycenters](https://github.com/justkolesov/EnergyGuidedBarycenters).

## 1 Introduction

Averaging is a fundamental concept in mathematics and plays a central role in numerous applications. While it is a straightforward operation when applied to scalars or vectors in a linear space, the situation complicates when working in the space of probability distributions. Here, simple convex combinations can be inadequate or even compromise essential geometric features, which necessitates a different way of taking averages. To address this issue, one may carefully select a measure of distance that properly captures similarity in the space of probabilities. Then, the task is to find a procedure which identifies a 'center' that, on average, is closest to the reference distributions.

One good choice for comparing and averaging probability distributions is provided by the family of Optimal Transport (OT) discrepancies . They have clear geometrical meaning and practicalinterpretation [89; 97]. The corresponding problem of averaging probability distributions using OT discrepancies is known as the OT barycenter problem . OT-based barycenters find application in various practical domains: domain adaptation , shape interpolation , Bayesian inference [101; 102], text scoring , style transfer , reinforcement learning .

Over the past decade, the substantial demand from practitioners sparked the development of various methods tackling the barycenter problem. The research community's initial efforts were focused on the discrete OT barycenter setting, see Appendix B.1 for more details. The **continuous setting** turns out to be even more challenging, with only a handful of recent works devoted to this setup [72; 17; 59; 55; 32; 82; 14]. Most of these works are devoted to specific OT cost functions, e.g., deal with \(_{2}^{2}\) barycenters [59; 55; 32; 82]; while others require non-trivial _a priori_ selections  and have limiting expressivity and generative ability [17; 14], see SS3 for a detailed discussion.

**Contributions**. We propose a novel approach for solving Entropy-regularized OT (EOT) barycenter problems, which alleviates the aforementioned limitations of existing continuous OT solvers.

1. We reveal an elegant reformulation of the EOT barycenter problem by combining weak dual form of EOT with the congruence condition (SS4.1); we derive a simple optimization procedure which closely relates to the standard training algorithm of Energy-Based models (SS4.2).
2. We establish the generalization bounds as well as the universal approximation guarantees for our recovered EOT plans, which push the reference distributions to the barycenter (SS4.3).
3. We validate the applicability of our approach on various toy and large-scale setups, including the RGB image domain (SS5). In contrast to previous works, we also pay attention to non-Euclidean OT costs. Specifically, we conduct a series of experiments looking for a barycenter on an image manifold of a pretrained GAN. In principle, the image manifold support may contribute to the interpretability and plausibility of the resulting barycenter distribution in downstream tasks.

**Notations.** We write \(=\{1,2,,K\}\). Throughout the paper \(^{D^{}},^{D}\) and \(_{k}^{D_{k}}\) (\(k\)) are compact subsets of Euclidean spaces. Continuous functions on \(\) are denoted as \(()\). Probability distributions on \(\) are \(()\). Absolutely continuous probability distributions on \(\) are denoted by \(_{}()()\). Given \((),()\), we use \((,)\) to designate the set of _transport plans_, i.e., probability distributions on \(\) with the first and second marginals given by \(\) and \(\), respectively. The density of \(_{}()\) w.r.t. the Lebesgue measure is denoted by \((x)}{x}\).

## 2 Background

First, we recall the formulations of EOT (SS2.1) and the barycenter problem (SS2.2). Next, we clarify the computational setup of the considered EOT barycenter task (SS2.3).

### Entropic Optimal Transport

Consider distributions \(_{}()\), \(_{}()\), a continuous cost function \(c:\) and a regularization parameter \(>0\). The _entropic optimal transportation_ (EOT) problem between \(\) and \(\)[15; 78] consists of finding a minimizer of

\[_{c,c}(,)^{}_{ (,)}}_{(x,y)}c(x,y)- }_{x}H((|x))}. \]

Note that (1) is not the only way to formulate EOT. One more popular and equivalent formulation [19; 83; 36] substitutes the conditional entropy term \(}_{x}H((|x))\) in (1) with full entropy \(H()\).

Figure 1: Entropic barycenter \(^{*}\) (5) of \(N=4\) von Mises distributions \(_{n}\) on the sphere (see ยง5.1) estimated with our barycenter solver (Algorithm 1). The used transport costs are \(c_{k}(x_{k},y)= x_{k},y ^{2}\).

A minimizer \(^{*}(,)\) of (1) is called the EOT plan; its existence and uniqueness are guaranteed, see, e.g., [16, Th. 3.3]. In practice, we usually do not require the EOT plan \(^{*}\) but its conditional distributions \(^{*}(|x)()\) as they prescribe how points \(x\) are stochastically mapped to \(\)[42, SS2]. We refer to \(^{*}(|x)\) as the _conditional plans_.

**Weak OT dual formulation of the EOT problem**. The EOT problem permits several dual formulations. In our paper, we use the one derived from the weak OT theory [39, Theorem 9.5]:

\[_{c,}(,)=_{f()}}{}f^{C}(x)+}{}f(y)}, \]

where \(f^{C}:\) is the so-called **weak** entropic \(c\)-transform [9, Eq. 1.2] of the function (_potential_) \(f\). The transform is defined by

\[f^{C}(x)}{=}()}{ }}c(x,y)- H()-}f(y)}. \]

We use the capital \(C\) in \(f^{C}\) to distinguish the weak transform from the classic \(c\)-transform [89, SS1.6] or \((c,)\)-transform [76, SS2]. In particular, formulation (2) is not to be confused with the conventional EOT dual, see [78, Appendix A].

For each \(x\), the minimizer \(_{x}^{f}()\) of the weak \(c\)-transform (3) exists and is unique. Its density has particular form [78, Theorem 1]. Let \(Z_{c}(f,x)}{=}_{}y\), then

\[_{x}^{f}(y)}{y}}{=} (f,x)}(). \]

By substituting (4) into (3) and carrying out straightforward manipulations, we arrive at an explicit formula \(f^{C}(x)=- Z_{c}(f,x)\), see [78, Equation (14)].

### Entropic OT Barycenter

Consider distributions \(_{k}_{}(_{k})\) and continuous cost functions \(c_{k}(,):_{k}\) for \(k\). Given weights \(_{k}>0\) with \(_{k=1}^{K}_{k}=1\), the EOT Barycenter problem is:

\[^{*}()}{}_{k= 1}^{K}_{k}_{c_{k},}(_{k},), \]

The case where \(=0\) corresponds to the classical OT barycenter  and falls out of the scope of this paper. Note that the majority of previous research  consider a bit different but equivalent EOT barycenter formulation, i.e., which has the **same minimizers**. The objective (5) is known as _Schrodinger_ barycenter problem [15, Table 1], see the extended discussion in Appendix B.3. It is worth noting that under mild assumptions the barycenter \(^{*}\) which delivers optimal value of (5) exists and is unique, see Appendix A.7.

### Computational aspects of the EOT barycenter task

Barycenter problems, such as (5), are known to be challenging in practice . To our knowledge, even when \(_{1},,_{K}\) are Gaussian distributions, there is no direct analytical solution neither for our entropic case (\(>0\)), see the additional discussion in App. C.4, nor for the unregularized case [3, \(=0\)]. Moreover, in real-world scenarios, the distributions \(_{k}\) (\(k\)) are typically not available explicitly but only through empirical samples (datasets). This aspect leads to the next **learning setup**.

We assume that each \(_{k}\) is accessible only by a limited number of i.i.d. empirical samples

\(X_{k}=\{x_{k}^{1},x_{k}^{2}, x_{k}^{N_{k}}\}_{k}\). Our aim is to approximate the optimal conditional plans \(_{k}^{*}(|x_{k})\) between the entire source distributions \(_{k}\) and the entire (unknown) barycenter \(^{*}\) solving (5). The recovered plans should provide the _out-of-sample_ estimation, i.e., allow generating samples from \(_{k}^{*}(|x_{k}^{})\), where \(x_{k}^{}\) is a new sample from \(_{k}\) which is not necessarily present in the train sample.

This setup corresponds to **continuous OT**. It differs from the **discrete OT** setup  which aims to solve the barycenter task for _discrete_ empirical distributions. Discrete OT are not well-suited for the out-of-sample estimation required in the continuous OT setup.

## 3 Related works

The taxonomy of OT solvers is large. Due to space constraints, we discuss here only methods within the _continuous OT learning setup_ that solve the (E-)OT barycenter problem. These methods approximate OT maps or plans between the distributions \(_{k}\) and the barycenter \(^{*}\) rather than just their empirical counterparts that are available from the training samples. A broader discussion of general-purpose discrete/continuous (E-)OT solvers is in Appendix B.1.

**Continuous OT barycenter solvers are based on the unregularized or regularized OT barycenter problem within the continuous OT learning setup. The works  are designed _exclusively_ for the quadratic Euclidean cost \(^{2}(x,y)}}{{=}}\|x-y\|_{2}^{2}\). The OT problem with this particular cost exhibits several advantageous theoretical properties [4, SS2] which are exploited by the aforementioned articles to build efficient procedures for barycenter estimation algorithms. In particular,  utilize ICNNs  which parameterize convex functions, and  relies on a specific tree-based Schrodinger Bridge reformulation. In contrast, our proposed approach is designed to handle the EOT problem with _arbitrary_ cost functions \(c_{1},,c_{K}\). In , they also consider regularized OT with non-Euclidean costs. Similar to our method, they take advantage of the dual formulation and exploit the so-called congruence condition (SS4). However, their optimization procedure substantially differs. It necessitates selecting a _fixed prior_ for the barycenter, which can be non-trivial. The work  takes a step further by directly optimizing the barycenter distribution in a variational manner, eliminating the need for a _fixed prior_. This modification increases the complexity of optimization and requires specific parametrization of the variational barycenter. In , the authors also parameterize the barycenter as a generative model. Their approach does not recover the OT plans, which differs from our learning setup (SS2.3). A summary of the key properties is provided in Table 1, highlighting the fact that our approach overcomes many imperfections of competing methods. We are also aware of the novel continuous OT barycenter solver . This approach is more recent than ours and is _significantly_ based on the ideas from our article. Because of this, we exclude it from our comparisons.

## 4 Proposed Barycenter Solver

In the first two subsections, we work out our optimization objective (SS4.1) and its practical implementation (SS4.2). In SS4.3, we alleviate the gap between the theory and practice by offering finite sample approximation guarantees and universality of NNs to approximate the solution.

### Deriving the optimization objective

In what follows, we analyze (5) from the dual perspectives. We introduce \(:()^{K}\):

\[(f_{1},,f_{K})}}{{=}}_ {k=1}^{K}_{k}_{k}}{}f_{k} ^{C_{k}}(x_{k})}=-_{k=1}^{K}_{k} _{k}}{} Z_{c_{k}}(f_{k},x_{k })}.\]

Here \(f_{k}^{C_{k}}\) denotes the weak entropic \(c_{k}\)-transform (3) of \(f_{k}\). Following SS2.1, we see that it coincides with \(- Z_{c_{k}}(f_{k},x_{k})\). Below we formulate our main theoretical result, which will allow us to solve the EOT barycenter task without optimization over all distributions on \(\).

**Theorem 4.1** (Dual formulation of the EOT barycenter problem [proof ref.]).: _Problem (5) permits the following dual formulation:_

\[^{*}=,,f_{K}();}{ }(f_{1},,f_{K}). \]

We refer to the constraint \(_{k=1}^{K}_{k}f_{k}=0\) as the **congruence** condition w.r.t. weights \(\{_{k}\}_{k=1}^{K}\). The potentials \(f_{k}\) appearing in (6) play the same role as in (2). Notably, when \((f_{1},,f_{K})\) is close to \(^{*}\), the conditional optimal transport plans \(_{k}^{*}(|x_{k}),x_{k}_{k}\), between \(_{k}\) and the barycenter distribution \(^{*}\) can be approximately recovered through the potentials \(f_{k}\). This intuition is formalized in Theorem 4.2 below. First, for \(f_{k}()\), we define

\[^{f_{k}}(x_{k},y)}}{{=}} ^{f_{k}}_{x_{k}}(y)_{k}(x_{k})\]

and set \(^{f_{k}}()\) to be the second marginal of \(^{f_{k}}\).

**Theorem 4.2** (Quality bound of plans recovered from dual potentials [proof ref.]).: _Let \(\{f_{k}\}_{k=1}^{K},f_{k}()\) be congruent potentials. Then we have_

\[^{*}-(f_{1},,f_{K})=_{k=1}^{K} _{k}(_{k}^{*}\|^{f_{k}})_{k=1}^{K }_{k}(^{*}\|^{f_{k}}), \]

_where \(_{k}^{*}(_{k},^{*}),k\) are the EOT plans between \(_{k}\) and the barycenter distribution \(^{*}\)._

  
**Method** &  **Admissible** \\ **OT costs** \\  &  **Learns** \\ **OT plans** \\  &  **Max considered** \\ **data dim** \\  & 
 **Regularization** \\ **Remarkation** \\  \\ 
 & general & yes & 8D, no images & 
 Entropic/Quadratic \\ with fixed prior \\  \\
 & general & no & 
 133233 (MINST) \\  & Entropic (Sinkhorn) \\
 & only \(l_{2}^{2}\) & yes &  2650, no images \\ 13328,238 (MINST) \\  & 
 requires fixed prior \\  \\
 & only \(l_{2}^{2}\) & yes &  133628,238 (MINST) \\  & 
 requires fixed prior \\  \\
 & only \(l_{2}^{2}\) & yes &  13628,238 (MINST) \\  & 
 requires fixed prior \\  \\
 & only \(l_{2}^{2}\) & yes & 
 13628,238 (MINST) \\  & Entropic \\
 & general & yes & 
 2560, Gaussions only \\  & Entropic/Quadratic \\ 
**Ours** & general & yes & 
 336484 (CelebA) \\  & Entropic \\    

Table 1: Comparison of continuous OT bary solvers

[MISSING_PAGE_FAIL:5]

where \(_{l}(0,I_{D})\), \(l\{0,1,2,,L\}\), \(L\) is a number of steps, and \(>0\) is a step size. Note that the iteration procedure above could be straightforwardly adapted to a batch scenario, i.e., we can simultaneously simulate the whole batch of samples \(Y_{k}^{(l)}\) conditioned on \(X_{k}^{(l)}\). The particular values of number of steps \(L\) and step size \(\) are reported in the details of the experiments, see Appendix C. An alternative importance sampling-based approach for optimizing (9) is presented in Appendix D.

Inference. We use the same ULA procedure for sampling from the recovered optimal conditional plans \(^{f_{^{*},k}}(|x_{k})\), see the details on the hyperparameters \(,\) in SS5.

**Relation to prior works.** Learning a distribution of interest via its energy function (EBMs) is a well-established direction in generative modelling research . Similar to ours, the key step in most energy-based approaches is the MCMC procedure which recovers samples from a distribution accessible only by an unnormalized log density. Typically, various techniques are employed to improve the stability and convergence speed of MCMC, see, e.g., . The majority of these techniques can be readily adapted to complement our approach. At the same time, the primary goal of this study is to introduce and validate the methodology for computing EOT barycenters in an energy-guided manner. Therefore, we opt for the **simplest** MCMC algorithm, even **without the replay buffer**, as it serves our current objectives.

### Generalization Bounds and Universal Approximation with Neural Nets

In this subsection, we answer the question of how far the recovered plans are from the EOT plan \(_{k}^{*}\) between \(_{k}\) and \(\). In practice, for each distribution \(_{k}\) we know only the empirical samples \(X_{k}=\{x_{k}^{1},x_{k}^{2}, x_{k}^{N_{k}}\}_{k}\), i.e., finite datasets. Besides, the available potentials \(f_{k}\), \(k\) come from restricted classes of functions and satisfy the congruence condition. More precisely, we have \(f_{k}=g_{k}-_{k=1}^{K}_{k}g_{k}\) (SS4.2), where each \(g_{k}\) is picked from some class \(_{k}\) of neural networks. Formally, we write \((f_{1},,f_{K})}\) to denote the congruent potentials constructed this way from the functional classes \(_{1},,_{K}\). Hence, in practice, we optimize the _empirical version_ of (8):

\[_{(f_{1},,f_{K})}} }(f_{1},,f_{K}) }}{{=}} _{(f_{1},,f_{K})}}_{k=1}^{K} }{N_{k}}_{n=1}^{N_{k}}f_{k}^{C_{k}}(x_{k}^{n});\] \[(},,,N_{K}\), i.e., vanishing of the estimation error when the sample size grows.

The case **(a)** here is not very practically useful as the rate suffers from the curse of dimensionality. Still, this result points to one intriguing property of our solver. Namely, we may take **arbitrarily large** set \(_{k}\) (even \(_{k}=()\)!) and still have the guarantees of learning the barycenter. This happens because of \(C_{k}\)-transforms: informally, they make functions \(f_{k}_{k}\) smoother and "simplify" the set \(_{k}\). In our experiments, we always work with the costs as in **(b)**. As a result, our estimation error is \(O(_{k=1}^{K}N_{k}^{-1/2})\); this is a _standard fast and dimension-free convergence rate_. In practice, \(_{k}\) are usually neural nets. They are indeed bounded, as required in **(b)**, if their weights are constrained.

While the estimation error usually decreases when the sample sizes tend to infinity, it is natural to wonder whether the approximation error can be also made arbitrarily small. We positively answer this question when the standard fully-connected neural nets (multi-layer perceptrons) are used.

**Theorem 4.6** (Vanishing Approximation Error [proof ref.]).: _Let \(:\) be an activation function. Assume that it is non-affine and there is an \(\) at which \(\) is differentiable and \(^{}() 0\). Then for every \(>0\) there exist \(K\) multi-layer perceptrons \(g_{k}:^{D}}\) with activations \(\) for which the congruent functions \(f_{k}=g_{k}-_{k=1}^{K}_{k}g_{k}\) satisfy_

\[_{k=1}^{K}_{k}(_{k}^{*}\|^{f_{k}})=( ^{*}-(f_{1},,f_{K}))/</.\]

_Furthermore, each \(g_{k}\) has width at-most \(D+4\)._

Importantly, our Theorem 4.6 is more than just result on universal approximation since it deals with (i) _congruent_ potentials and (ii) entropic \(C_{k}\)-transforms. In particular, only specific properties of the entropic \(C_{k}\)-transforms allow deriving the desired universal approximation statement, see the proof.

**Summary.** Our results of this section show that both the estimation and approximation errors can be made arbitrarily small given a sufficient amount of data and large neural nets, allowing to perfectly recover the EOT plans \(_{k}^{*}\).

**Relation to prior works.** To our knowledge, the generalization and the universal approximation are novel results with no analogues established for any other continuous barycenter solver. Our analysis shows that the EOT barycenter objective (8) is well-suited for statistical learning and approximation theory tools. This aspect distinguishes our work from the predecessors, where complex optimization objectives may not be as amenable to rigorous study.

### Learning EOT barycenter on data manifold

Averaging complex data distributions by means of EOT barycenter directly in the data space may be undesirable. In particular, for image data domain:

* the entropic barycenter contains noisy images, see, e.g., our MNIST 0/1 experiment, SS5.2. This is due to the "blurring bias" bias  of our entropic barycenter setup and reliance on MCMC.
* searching for (entropic) barycenter is not very practical for standard OT cost functions like \(^{2}\). It is known that the true unregularized (\(=0\)) \(^{2}\)-barycenter of several image domains consists of just some pixel-wise averages of images from these source domains, which is not practically useful.

To alleviate the problem, we propose solving the (entropic) barycenter problem on some _a priori_ known data manifold \(\), where we want the barycenter to be concentrated on. In our experiments (SS5.2, SS5.3) these manifolds are given by pre-trained StyleGAN  generator models \(G:\); \(\) is the _latent_ space, \(=G()\). Technically speaking, to adapt our Alogithm 1 for manifold-constrained setup, we propose solving the barycenter problem in _latent_ space \(\) with _modified_ cost functions \(c_{k,G}(x_{k},z):=c_{k}(x_{k},G(z))\). We emphasize that such costs are **general** (not \(^{2}\) cost!) because \(G\) is a non-trivial StyleGAN generator. Hence, while our proposed manifold-constrained barycenter learning setup could be used on par with other OT barycenter solvers, these barycenter solvers **should** support general costs. In particular, the majority of competitive methods from Table 1_are not adjustable to the manifold setup_ as they work exclusively with \(^{2}\).

**Relation to prior works.** While the utilization of data manifolds given by pre-trained (foundational) models is ubiquitous in generative modeling, the adaptation of this technique for Optimal Transport barycenter is a novel idea. Apart from our work, this idea is exploited in follow-up paper .

Experiments

We assess the performance of our barycenter solver on small-dimensional illustrative setups (SS5.1) and in image spaces (SS5.2, SS5.3). The source code for our solver is written in the PyTorch framework and available at [https://github.com/justkolesov/EnergyGuidedBarycenters](https://github.com/justkolesov/EnergyGuidedBarycenters). The experiments are issued in the form of convenient *.ipynb notebooks. Reproducing the most challenging experiments (SS5.2, SS5.3) requires less than \(12\) hours on a single TeslaV100 GPU. The details of the experiments, extended experimental results are in Appendix C, additional experiments with single-cell data are given in Appendix C.5.

**Disclaimer.** Evaluating how well our solver recovers the EOT barycenter is challenging because the ground truth barycenter is typically unknown. In some cases, the true _unregularized_ barycenter (\(=0\)) can be derived (see below). The EOT barycenter for sufficiently small \(>0\) is expected to be close to the unregularized one. Therefore, in most cases, our evaluation strategy is to compare the computed EOT barycenter (for small \(\)) with the unregularized one. In particular, we use this strategy to quantitatively evaluate our solver in the Gaussian case, see Appendix C.4.

### Barycenters of Toy Distributions

**2D Twister.** Consider the map \(u:^{2}^{2}\) which, in the _polar coordinate system_, is represented by \(_{+}[0,2)(r,)(r,(-r)2)\). The cartesian version of \(u\) is presented in Appendix C.1. Let \(_{1},_{2},_{3}\) be \(2\)-dimensional distributions as shown in Fig. 1(a). For these distributions and uniform weights \(_{k}=\), the unregularized barycenter (\(=0\)) for the **twisted** cost \(c_{k}(x_{k},y)=\|u(x_{k})-u(y)\|^{2}\) can be derived analytically, see Appendix C.1. The barycenter is the centered Gaussian distribution which is also shown in Fig. 1(a). We run the experiment for this cost with \(=10^{-2}\), and the results are recorded in Fig. 1(b). We see that it qualitatively coincides with the true barycenter. For completeness, we also show the EOT barycenter computed with our solver for \(^{2}(x,y)=\|x-y\|^{2}\) costs (Fig. 1(c)) and the same regularization \(\). The true \(^{2}\) barycenter is estimated by using the free_support_barycenter solver from POT package . We stress that the twisted cost barycenter and \(^{2}\) barycenter differ, and so do the learned conditional plans: the \(^{2}\) EOT plan (Fig. 1(d)) expectedly looks more well-structured while for the twisted cost (Fig. 1(b)) it becomes more chaotic due to non-trivial structure of this cost.

**Sphere.** In this experiment, we look for the barycenter of four von Mises distributions \(_{n}\) supported on 3D sphere, see Figure 1. The cost functions are \(c_{k}(x_{k},y)=^{2} x_{k},y\), the regularization is \(=10^{-2}\). The learned potentials \(f_{,k}\) operate with ambient \(^{3}\) vectors. When performing MCMC, we project each Langevin step to the sphere. Our qualitative results are shown on Figure 1. While the ground truth solution to the considered problem is unknown, the learned barycenter looks reasonable. This showcases the applicability of our approach to non-standard non-quadratic experimental setups.

### Barycenters of MNIST Classes 0 and 1

A classic experiment considered in the continuous barycenter literature  is averaging of distributions of MNIST 0/1 digits with weights \((,)\) in the grayscale image space \(_{1}\!=\!_{2}\!=\!\!=\![-1,1]^{32 32}\). The true unregularized (\(=0\)) \(^{2}\)-barycenter images \(y\) are direct pixel-wise averages \(+x_{2}}{2}\) of pairs of images \(x_{1}\) and

Figure 3: Samples from the StyleGAN \(G\) defining the polluted manifold \(\).

Figure 2: _2D wister example:_ The true barycenter of 3 comets vs. the one computed by our solver with \(=10^{-2}\). Two costs \(c_{k}\) are considered: the twisted cost (1(a), 1(b)) and \(^{2}\) (1(c), 1(d)).

coming from the \(^{2}\) OT plan between 0's (\(_{1}\)) and 1's (\(_{2}\)). In Fig. 5, we show the unregularized \(^{2}\) barycenter computed by [32, SCWB], [55, WIN].

**Data space EOT barycenter.** To begin with, we employ our solver to compute the \(\)-regularized EOT \(^{2}\)-barycenter directly in the image space \(\) for \(=10^{-2}\). We emphasize that the true entropic barycenter slightly differs from the unregularized one. To be precise, it is expected that regularized barycenter images are close to the unregularized barycenter images but with additional noise. In Fig. 5, we see that our solver (data space) recovers the noisy barycenter images exactly as expected.

**Manifold-constrained EOT barycenter.** Following the reasoning from SS4.4, we propose to restrict the search space for our algorithm to some pre-defined manifold \(\). As discussed earlier, the support of the image-space unregularized \(^{2}\)-barycenter is a certain _subset_\(^{}}\{+x_{2}}{2}  x_{1}(_{1}),x_{2}(_{2})\}\). To achieve this, we train a StyleGAN  model \(G:\) with \(=^{512}\) to generate some **even larger** manifold \(=G()\) which is expected to contain \(^{}\). Namely, we use all possible pixel-wise half-sums \(+x_{2}}{2}\) of digits \(0\) as \(x_{1}\) and \(\{1,4,7\}\) as \(x_{2}\), see Figure 3 with the trained StyleGAN samples. That is, our final constructed manifold \(\) is **polluted** with additional samples (e.g., averages of digits 0 and 7) which should not to lie in the support of the barycenter. Then, we use our solver with \(=10^{-2}\) to search for the barycenter of 0/1 digit distributions on \(_{1},_{2}\) which lies in the latent space \(\) w.r.t. costs \(c_{k,G}(x,z)}}{{=}}\|x-G(z)\|^{2}\). This can be interpreted as learning the EOT \(^{2}\)-barycenter in the ambient space but constrained to

Figure 4: _Experiment on the Aw, celeba! barycenter dataset._ The plots compare the transported inputs \(x_{k}_{k}\) to the barycenter learned by various solvers. The true unregularized \(^{2}\) barycenter of \(_{1},_{2},_{3}\) are the clean celebrity faces, see [55, ยง5].

Figure 5: Qualitative comparison of barycenters of MNIST 0/1 digit classes computed with barycenter solvers in the image space w.r.t. the pixel-wise \(^{2}\). Solvers SCWB and WIN only learn the unregularized barycenter (\(=0\)) directly in the data space. In turn, our solver learns the EOT barycenter in data space as well as it can learn EOT barycenter restricted to the StyleGAN manifold (\(=10^{-2}\)).

the StyleGAN-parameterized manifold \(G()\). The barycenter \(^{*}\) is some distribution of the latent variables \(z\), which can be pushed to the manifold \(G()\) via \(G(z)\).

The results are in Fig. 5. There is **(a)** no noise compared to the data-space EOT barycenter because of the manifold constraint, and **(b)** our solvers correctly ignores polluted samples from \(\).

### Evaluation on the Ave, celeba! Dataset

In , the authors developed a theoretically grounded methodology for finding probability distributions whose unregularized \(^{2}\) barycenter is known by construction. Based on the CelebA faces dataset , they constructed an Ave, celeba! dataset containing 3 degraded subsets of faces. The true \(^{2}\) barycenter w.r.t. the weights \((,,)\) is the distribution of Celeba faces itself. This dataset is used to test how well our approach recovers the barycenter.

We follow the EOT manifold-constrained setup (SS4.4) and train the StyleGAN on unperturbed celeba faces. This might sound a little bit unfair, but our goal is to demonstrate the learned transport plan to the constrained barycenter rather than unconditional barycenter samples (recall the setup in SS2.3). Hence, we learn the constrained EOT barycenter with \(=10^{-4}\). In Fig. 4, we present the results, depicting samples from the learned plans from each \(_{k}\) to the barycenter. Overall, the map is qualitatively good, although sometimes failures in preserving the image content may occur. This is presumably due to MCMC inference getting stuck in local minima of the energy landscape. For comparison, we also show the results of the solvers by [32, SCWB], [55, WIN]. Additionally, we report the FID score  for images mapped to the barycenter in Table 2 (std. deviations for our method correspond to running the inference with different random seeds). Owing to the manifold-constrained setup, the FID score of our solver is significantly smaller.

## 6 Potential Impact, Limitations and Broader Impact

**Potential impact.** In our work, we propose a novel approach for solving EOT barycenter problems which is applicable to _general OT costs_. From the practical viewpoint, we demonstrate the ability to restrict the sought-for barycenter to the _image manifold_ by utilizing a pretrained generative model. Our findings may be applicable to a list of important real-world applications, see Appendix B.2. We believe that our large-scale barycenter solver will leverage industrial & socially-important problems.

**Methodological limitations**. The methodological limitations of our approach are mostly the same as those of EBMs. It is worth mentioning the usage of MCMC during the training/inference. The basic ULA algorithm which we use in SS4.2 may poorly converge to the desired distribution \(_{x}^{f}\). In addition, MCMC sampling is time-consuming. We leave the search for more efficient sampling procedures for our solver, e.g., , for future research. We also note that our theoretical analysis in SS4.3 does not take into the account the optimization errors appearing due to the gradient descent and MCMC. The analysis of these quantities is a completely different domain in machine learning and out of the scope of our work. As the most generative modelling research, we do not attempt to analyse these errors.

**Problem setup limitations.** Our paper aims at solving Entropic OT barycenter problem. In the image data space, due to utilization of the Entropy, the learned barycenter distribution may contain noisy images. However, the utilization of our proposed StyleGAN-inspired manifold technique **entirely** alleviates the problem with the noise. This is demonstrated by our latent-space experiments with MNIST 0/1 (manifold space) and Ave Celeba! dataset.

**Broader impact.** This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

## 7 Acknowledgements

Skoltech was supported by the Analytical center under the RF Government (subsidy agreement 000000D730321P5Q0002, Grant No. 70-2021-00145 02.11.2021). AK acknowledges financial support from the NSERC Discovery Grant No. RGPIN-2023-04482. We would like to express special thanks to Vladimir Vanovskiy from Skoltech for the insightful discussions and details on geological modelling (Appendix B.2).

  _Solver_ &  \\  _k=1_ & \(k\) = 2 & \(k\) = 3 \\  SCWB  & 36.7 & 53.2 & 58.8 \\  WIN  & 49.3 & 46.9 & 61.5 \\ 
**Ours** & **84.**(3) & **87.**(3) & **102.**(7) \\  

Table 2: FID scores of images mapped from inputs \(_{k}\) to the barycenter.