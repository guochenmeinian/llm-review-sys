# LVD-2M: A Long-take Video Dataset

with Temporally Dense Captions

Tianwei Xiong

Yuqing Wang

Daouan Zhou

Zhijie Lin

ByteDance

Jiashi Feng

ByteDance

Xihui Liu

###### Abstract

**Open-Sora Captioning (only focus on appearance of first 4 frames, ignores content variations in last 2 )**

The video captures a man in a bright yellow jacket walking through a room with a high ceiling and large windows. The man is in the foreground, moving towards the right side of the frame. The appears to be in a casual setting, possibly an office or a conference room, as indicated by the presence of a long table and multiple chairs arranged around it. On the wall, there's a sign with a red circle and a diagonal line through it, commonly understood as a prohibition symbol. The room is well-it, with natural light coming through the windows, and the overall atmosphere is calm and quiet. The style of the video is a straightforward, unedited capture of a moment, likely intended for documentary or news purposes, as suggested by the logo in the corner that reads "WDR" and the timestamp "3-30".

**Ours: Temporally-dense Captioning**

The video appears to depict a man in a bright yellow jacket who is the central figure. The setting seems to be an indoor, professional or educational environment, such as a conference room or office, with high ceilings, large windows, and furnishings like tables and chairs. The man is shown walking into the room, engaging in a conversation with another person, and then interacting with a group of people seated around a table, suggesting a meeting or discussion. **The man’s actions and prominent attire suggest he is the main subject of the video.

*Equal contributions. \(\) Project lead. \(\)Corresponding author.

Figure 1: Comparison of our proposed LVD-2M dataset against previous datasets. Our dataset contains long-take videos with significant motion and temporally-dense captions (different colors represent captions for different frames), contrasting with short videos and sparse annotations in previous datasets like Panda-70M , HD-VG , and WebVid  (shown as “Others”).

###### Abstract

The efficacy of video generation models heavily depends on the quality of their training datasets. Most previous video generation models are trained on short video clips, while recently there has been increasing interest in training long video generation models directly on longer videos. However, the lack of such high-quality long videos impedes the advancement of long video generation. To promote research in long video generation, we desire a new dataset with four key features essential for training long video generation models: (1) long videos covering at least 10 seconds, (2) long-take videos without cuts, (3) large motion and diverse contents, and (4) temporally dense captions. To achieve this, we introduce a new pipeline for selecting high-quality long-take videos and generating temporally dense captions. Specifically, we define a set of metrics to quantitatively assess video quality including scene cuts, dynamic degrees, and semantic-level quality, enabling us to filter high-quality long-take videos from a large amount of source videos. Subsequently, we develop a hierarchical video captioning pipeline to annotate long videos with temporally-dense captions. With this pipeline, we curate the first long-take video dataset, LVD-2M, comprising 2 million long-take videos, each covering more than 10 seconds and annotated with temporally dense captions. We further validate the effectiveness of LVD-2M by fine-tuning video generation models to generate long videos with dynamic motions. We believe our work will significantly contribute to future research in long video generation.

## 1 Introduction

Generating long-take videos with temporal consistency, rich contents and large motion dynamics is essential for various applications such as AI-assisted film production. Although video generation models  have achieved impressive results in generating short video clips of few seconds, it remains challenging to simulate temporal-consistent and dynamic contents over long durations. Some works  attempt to extend video generation models trained on short video clips to long video generation by iteratively generating next frames conditioned on previously generated frames. However, those methods suffer from temporal inconsistency and limited motion patterns. Inspired by Sora , there has been increasing interest in scaling up video generation models for longer videos . Being trained directly on long-duration videos, these models provide a promising path toward modeling long-range temporal consistency and large motion dynamics in long videos. However, an obstacle on this path is the lack of high-quality long videos with rich text annotations.

Previous datasets of large-scale video-text pairs  have made significant contributions to video generation, but most of them encounter limitations for training long video generators. Video datasets crawled from the Internet  usually contain static videos or scene cuts, which are harmful to the training of video generation models. Moreover, previous text-to-video generation datasets are annotated with only short video captions, failing to capture the rich and dynamic semantics in long videos. Despite several recent efforts  in generating long captions for videos, they mostly focus on generating spatially-dense captions and neglect the rich temporal dynamics in videos.

It has been validated in previous works  that fine-tuning pre-trained generative models on high-quality datasets could significantly improve the quality of generated images and videos. Despite previous efforts in building large-scale video datasets, high-quality long video datasets with dense annotations are rarely available and expensive. Inspired by this, we desire a dataset specifically designed for long video training with the following properties: (1) long videos covering at least 10 seconds, (2) long-take videos without cuts, (3) large motion and diverse content, and (4) annotated with temporally-dense captions.

To this end, we create an automatic pipeline for video filtering and long video recaptioning. We devise a video filtering process leveraging both low-level filtering tools including scene cut detection and optical flow  estimation, and semantic-level filtering tools like video LLMs . The video filtering process selects high-quality long-take videos spanning over 10 seconds without scene cuts and containing large motion dynamics. Moreover, we design a hierarchical captioning approach to generate temporally-dense captions for long videos. Specifically, we split long videos into 30-second clips. For each clip, we uniformly sample 6 frames and arrange them in a grid layout. The singlecomposite image, named "image grid" , is fed into LLaVA-v1.6-3dB  for temporally-aware video clip captioning. Then, we apply a Large Language Model, Claude3-Haiku , to refine the captions and integrate captions from different clips into a complete caption describing the whole video. Compared to captions of previous video datasets, our hierarchical captioning approach provides temporally-dense captions describing the transitions of actions and scenes over the whole duration.

Following our pipeline, we generate 2 million high-quality video-caption pairs from 220 million videos in 4 open-sourced large-scale datasets: Panda-70M , HD-VG-130M , InternVid , and WebVid-10M . Human evaluations demonstrate that our dataset is preferred by human raters in terms of dynamic degree, long-take videos without scene cuts, and quality of captions. We further validate the effectiveness of our LVD-2M by fine-tuning pre-trained video generation models on LVD-2M. We experiment on both diffusion-based video generation models and language model-based video generation models. We find that models fine-tuned on this dataset perform better at generating long videos with large motion dynamics. Moreover, the model learns to generate long-take videos with significant camera movement accompanied by smooth scene transitions.

In summary, our contributions are three-fold. **1)** We devise an automatic data curation pipeline, including low-level and semantic-level filtering strategies to select high-quality long-take videos with large motions, and a hierarchical captioning approach to annotate long videos with temporally-dense captions. **2)** To address the lack of high-quality data for long video generation, we leverage our proposed data curation pipeline to construct LVD-2M, a dataset of high-quality long-take videos spanning over 10 seconds, with temporally-dense captions. **3)** We validate the effectiveness of LVD-2M by both human evaluation and fine-tuning experiments on both diffusion-based and LM-based video generation models using LVD-2M.

## 2 Related Work

**Video-Language Datasets.** To effectively train video generative models, a high-quality video-language dataset is crucial. Early datasets, such as MSR-VTT  and ActivityNet , were created through manual annotation, which limited their scale. Subsequent works aimed to increase dataset scale by utilizing automatic speech recognition (ASR) to extract text descriptions from videos. Notable examples include HowTo100M , YT-Temporal , and HD-VILA . Although this approach significantly increased the amount of data, the ASR-generated text descriptions often fail to accurately represent the main video content. Another approach is to directly use readily available titles or descriptions of online videos as captions. WebVid  followed this approach and collected 10 million video-text pairs, primarily from stock footage providers. A common limitation of existing datasets is that the vast majority of samples are short video clips, lacking coverage of long videos, especially dense descriptions of long-range dynamic content changes. For dataset targeting longer videos, StoryBench  has provided a few thousand annotated long videos, but its limited data scale restricts its usage to evaluation rather than model training. A concurrent work ShareGPT4Video  curated a dataset with long videos and detailed captions, but its data pipeline is less focused on video data filtering and processing. To truly drive advances in long video generation models, constructing a large-scale dataset of high-quality long-take videos with dense captions is crucial.

**Video Generation.** Most existing video generation methods primarily focus on generating short video clips, with diffusion models  being the prevalent approach. There are also a few works based on language models (LM-based)  for video generation. Some works attempt to extend to long video generation by training models on short video data and then employing techniques such as sliding window generation . However, these methods often suffer from quality degradation, lack of temporal consistency, and difficulty in generating high-quality long-range dynamic video content. We identify that a lack of high-quality long video datasets hinders existing text-to-video generative models from effectively modeling and generating long videos with rich dynamics.

**Video Understanding.** Vision-language  models demonstrated strong performance in video understanding. Recently, IG-VLM  pointed out that an VLM  comprehensively pretrained on images can be highly capable of video understanding. This is achieved by concatenating multiple frames from a video into a single image in grid view, which will be the input for VLMs. In this work, we propose a way to filter undesired videos utilizing a Video-LLM  which can largely enhance the overall quality of the dataset.

**Video Captioning.** The usage of VLMs for video understanding has been primarily focused on VQA tasks . But it is less explored specifically for video captioning. Previously, HD-VG  utilizes BLIP-2  to caption a single key frame from a video clip, Panda-70M  trains a light-weighted captioning model for captioning, and InternVid  combines BLIP-2 captions for multiple frames into a single overall caption with a language model. The resulted captions from these previous caption pipelines are mostly a single sentence. In this work, we target on the generation of detailed and temporally dense captions that better capture the content of the videos, utilizing a strong VLM .

## 3 Dataset

We devise a data curation pipeline to filter large-motion long-take videos from large-scale video datasets and to annotate them with temporally-dense captions. We demonstrate the data curation pipeline and data statistics of LVD-2M in this section.

### Long-take Video Collection and Filtering

Collecting videos from source datasets.We collect videos from four sources: (1) HD-VG  which contains 130 million video clips collected from YouTube. (2) InternVid  which contains 38 million video clips from YouTube. (3) Panda70M  which contains 70 million videos from YouTube. (4) WebVid  which contains 10 million videos from stock footage providers. However, not all of those videos are suitable for long video generation. For example, only 15% of video clips from InternVid  are longer than 10s, while around 52.5% of these long videos contain shot changes (Tab. 2). While videos from stock footage providers  seldom contain scene cut, nearly half of these videos are not dynamic (Fig. 5). Those low-quality videos will hinder the training of long video generation models. Thus, we devise several filtering criteria to select high-quality, large-motion, and long-take videos from 220 million videos in the four datasets. The whole filtering process is shown in Fig. 2.

Selecting long-take videos with scene cut detection.Most current video generation models are trained on short video clips, and videos crawled from the Internet contain many scene cuts, which may impede the long video generation models from learning long-range temporal consistency and continuous motion across frames. We aim to select videos of consistent scenes captured over 10 seconds. It is worth mentioning that smooth transition of scenes (_e.g._, the background of a street continuously changes as a person walks down the street) is allowed, and we only target filtering out scene cuts or slow shot changes with fade-in and fade-out effects caused by post-editing of videos. Previous attempts  leverage PySceneDetect  to detect sudden shot changes and semantic consistency  between early and late frames to detect large scene changes. However, there is still a portion of videos with fade-in / fade-out shot changes in the filtered datasets. We optimize the settings of PySceneDetect to better detect both sudden scene cuts and slow shot changes with fade-in / fade-out effects. Specifically, we find that the default setting \(\) with a rolling average threshold leads to difficulties in detecting slow shot changes with fade-in and fade-out effects. To filter out both sudden and slow scene cuts, we use \(\) with \(\) of \(50\) and \(\_\) of 0 frames on video frames sampled at a low fps of \(0.5\). By applying PySceneDetect on the whole video, videos with any significant changes within a 2-second interval are filtered out, including fade-in and fade-out effects which are commonly within 2 seconds.

Selecting large-motion videos with optical flow.We use optical flow as a clue to filter out static videos with little motion dynamics. Specifically, we calculate the optical flow with RAFT  between each pair of neighboring frames sampled at 2 fps and discard any videos with an average optical flow magnitude below a threshold of 20. This step helps remove videos with minimal motion, such as static scenes or individuals speaking to the camera against a still background.

Removing low-quality videos with MLLMs.We further conduct semantic-level filtering with MLLMs to remove low-quality videos that cannot be detected by previous filtering strategies. We leverage the PLLaVA-7B , which extends LLaVA from images to videos, for semantic-level filtering. For each video, we uniformly sample 8 frames from each video and prompt PLLaVA to distinguish low-quality videos. Specifically, we filter out videos that lack diversity, lack content variations, or with low perceptual qualities. The optical-flow-based criteria in the previous step canfilter out most near-static videos. However, some shaky videos captured by hand-holding cameras achieve high optical flow scores despite their lack of meaningful motion. Thus, we leverage PLLaVA to distinguish those low-quality videos. We further filter out videos with extensive text overlays because those videos add extra burdens for model training.

### Hierarchical Long Video Captioning for Temporally Dense Captions

We propose a hierarchical captioning approach to annotate temporally-dense captions for long videos. As shown in Fig. 3, we first split videos longer than 30 seconds into video clips of 30 seconds. Then we annotate the clip-level video captions for each video clip. Finally we use an LLM to refine the captions and merge captions from all clips into a temporally-dense caption for the whole video. In this subsection, we first demonstrate how to caption video clips shorter than 30 seconds, and then demonstrate how to use LLM to refine and merge captions.

Captioning a video clip as an image grid.A recent work  has demonstrated that Vision Language Models (VLMs) pretrained only on images have strong zero-shot performance in video understanding. We generate captions for video clips shorter than 30 seconds inspired by this approach. Specifically, we uniformly sample 6 frames from the video clip and arrange these frames into a single composite image with a grid layout. We then input the image grid to LLaVA-v1.6-34B  to generate the video clip captions. With this approach, we can obtain detailed captions describing the backgrounds, main characters, major actions, and camera perspectives in the video clips.

Refining and merging captions with LLMs.We identify that solely applying VLMs may not be sufficient for generating high-quality captions. LLaVA-v1.6-34B is prone to generating extra interpretations or assumptions about videos, leading to redundancy in the generated captions. So we leverage an LLM, Claude3-Haiku , to further refine the generated captions. In particular, we prompt Claude3-Haiku to rewrite the given raw captions so that the new captions are concise, objective, and convey a clear storyline for the video. Furthermore, for videos longer than 30 seconds, we prompt Claude3-Haiku to compose the multiple captions into a single, coherent caption describing the content and dynamics of the whole video.

### Dataset Statistics

We present the comparison between our LVD-2M and previous video datasets in Tab. 1. LVD-2M is a high-quality dataset with videos longer than 10 seconds. Compared to previous video datasets, videos in LVD-2M are with large motion and rich captions. We further present the statistics of the category distribution, duration, and word count of our dataset in Fig. 4. To understand the

Figure 2: Video filtering process. Our video filtering process employs multiple criteria to select high-quality, dynamic, and long-take videos from four source datasets.

distribution of collected video categories, we utilize the BART model  to classify the video captions into 8 categories based on the main objects and content. As shown in Fig. 4, our dataset covers diverse categories commonly found in the real world, such as scenery, people, food, sports, animals, transportation, gaming, and others.

## 4 Experiments

In Sec. 4.1, we conduct human evaluation analysis to demonstrate that our filtered video dataset, LVD-2M, contains fewer scene cuts, larger motion dynamics, and higher-quality captions, compared with previous datasets. In Sec. 4.2, we further validate the effectiveness of our LVD-2M by fine-tuning pre-trained video generation models on LVD-2M. We conduct fine-tuning experiments on both diffusion-based video generation models and language model-based video generation models, and find that fine-tuning video generation models on our dataset boosts the video generation models' abilities in generating long-take videos with large motion dynamics. In Sec. 4.3, we present the

Figure 3: Hierarchical video captioning process. First, we split the long video into 30-second clips and compose them into image grids. Then, we use the LLaVA-1.6 model  to generate captions for each image grid. Finally, we use the Claude3-Haiku model  to refine and merge these captions into the final complete caption for the whole video.

effectiveness of LVD-2M to extend the generation frame length of a diffusion-based T2V model, with comprehensive quantitative and qualitative validations.

### Human Evaluation of Dataset Quality

To validate the quality of LVD-2M and the effectiveness of our data curation pipeline, we conduct human evaluations to examine the long-take consistency, dynamic degrees, and caption qualities. For human evaluations, we compare our LVD-2M with previous video datasets: Panda-70M , HD-VG-130M , InternVid , and WebVid-10M .

**Long-take consistency in videos.** We examine that the filtered videos are mostly long-take videos without cuts. We randomly sample 40 videos from each dataset, each one being 10\(\)30s long. We do not compare with WebVid  because its videos are from stock footage providers and barely have scene cuts. For fair comparison, we also exclude videos collected from WebVid in samples from LVD-2M. The sampled videos are mixed and randomly shown to human raters. We request human raters to check for any type of scene cut that can lead to inconsistency. As shown in Tab. 2, with our video filtering strategy, LVD-2M reaches the highest long-take video ratio. We examine the cases in our dataset deemed by human raters as non-long-take videos, and identify the major failure cases are slight jump cuts. While humans can easily recognize a slight jump cut in a video, it is challenging for scene cut detection algorithms and MLLM-based semantic-level filtering models to identify such slight changes in the videos.

**Dynamic degree of videos.** We randomly sample 40 videos for each dataset, each one being 10\(\)30s long. We request human raters to rate the dynamic degree of the given videos from 1 to 3, where 1 means being not dynamic and 3 for being very dynamic. As shown in Fig. 5, for previous datasets, a large portion of videos are considered as not dynamic. After filtering at low-level with optical flow scores and at high-level with MLLMs, our LVD-2M successfully get rid of most static videos and the achieve a larger portion of very dynamic videos.

   Dataset & Text & Avg/min video len & Avg text len & Avg optical flow score (10s) \\  HowTo100M  & ASR & 3.6s & - & 4.0 words & - \\ ACAV  & ASR & 10.0s & - & - & - \\ YT-Temporal-180M  & ASR & - & - & - & - \\ HD-VILA-100M  & ASR & 13.4s & - & 32.5 words & - \\  Panda-70M  & Automatic caption & 8.5s & 1s & 13.2 words & 14.7 \\ HD-VG-130M  & Automatic caption & 5.8 s & 1s & 9.8 words & 21.5 \\ WebVid-10M  & Scrapped Footage Caption & 18.0s & 1s & 14.1 words & 12.1 \\ InternVid-38M  & Automatic caption & 17.2s & 1s & 17.6 words & 11.6 \\ 
**LVD-2M (Ours)** & Temporally dense caption & 20.2s & 10s & 88.7 words & 47.8 \\   

Table 1: Comparison of LVD-2M and other video datasets.

Figure 4: Statistics of LVD-2M. LVD-2M consists of long video clips with detailed dense captions, and diverse categories.

**Quality of video captions.** To compare the quality of our new captions to the original captions from datasets, we randomly sample 50 videos from each dataset. For each task, human raters are presented with a video clip and two captions, one from our captioning strategy, and another from the original dataset that the video is filtered from. We ask the human raters to compare the quality of the two captions. As shown in Fig. 5, our temporally-dense video captions are much more preferred by human raters. Among our baselines, Panda-70M captioning model  shows the best performance. As shown in Tab. 1, our captions are much longer and contain more details than previous datasets.

### Fine-tuning Video Generation models with LVD-2M

To further validate the effectiveness of our LVD-2M in fine-tuning video generation models for generating long videos with large motion dynamics, we conduct fine-tuning experiments on a diffusion-based image-to-video (I2V) generation model and a language model-based text-to-video (T2V) generation model for long video generation. In this experiment, we don't extend the generation frame length of the pretrained models and compare the finetuned models with the pretrained ones. We further compare LVD-2M to WebVid-10M on extending the generation frame length for a diffusion-based T2V models in Sec. 4.3, and for a diffusion-based I2V model in the Appendix.

**Fine-tuning an LM-based T2V model.** We finetune a 7B LM-based video generation model from Loong . The model utilizes a discrete video tokenizer similar to MAGVIT-v2  to convert videos into tokens, and then models the video tokens with decoder-only autoregressive transformer. The model is pretrained on 15 million video-text pairs for 500K iterations with a batch size of 256. We further fine-tune it for 10k iteration with a batch size of 256 on 65-frame clips from LVD-2M.

**Fine-tuning a diffusion-based I2V model.** We finetune an I2V model, which was pretrained to generate 17-frame videos on 19 million video-text pairs for 18k iterations with a batch size of 288, following the similar image conditioning settings as proposed in EMU . The model follows a similar architecture as MagicVideo  with 1.8B parameters.

**User study.** To validate the performance improvement after fine-tuning on LVD-2M, we conduct a user study comparing the pretrained models and finetuned models. For each base model (diffusion-based I2V and LM-based T2V), we use 50 text prompts to generate videos with the pretrained and fine-tuned models, respectively. Human raters are presented with 2 videos generated by the

Figure 5: The distribution of human-rated dynamic degree score and human preference for caption quality, comparing LVD-2M with other video datasets.

Figure 6: Human evaluation of generated videos by baseline v.s. fine-tuned models. We finetune both a diffusion-based I2V model and a LM-based T2V model on LVD-2M. Compared to the pretrained model, the finetuned models can generate more dynamic videos.

pretrained and finetuned models respectively, conditioned on the same text. They are asked to choose the preferred video based on either video-text alignment or dynamic degree of generated videos. We collect 200 valid responses from human raters. As shown in Fig. 6, we observe fine-tuning the LM-based T2V model on LVD-2M boosts the model's performance in terms of generating more dynamic videos and better alignment between generated videos and text prompts. On the other hand, after fine-tuning the diffusion-based I2V model on LVD-2M, the generated videos are significantly more preferred by users in terms of dynamic degree, with the win rate of 60% v.s. 6%. Although for diffusion-based I2V model, the improvement for video-text matching is relatively small, we identify that this may originate from the use of frozen clip text encoder for encoding long captions (88.65 words on average), since the maximum encoding length for clip text encoder is 77 tokens, and CLIP text encoder is not good at understanding long text prompts.

### Extending a Diffusion-based T2V Model for Longer Range on LVD-2M

In this section, we present the effectiveness of LVD-2M for finetuning text-to-video (T2V) diffusion models to generate longer and more dynamic videos. For comparison, we choose the widely adopted WebVid-10M  as the baseline. In the experiment, we extend a T2V diffusion model from pretrained 32-frame generation length to 65-frame length, using LVD-2M and WebVid-10M seperately. Quantitative results on VBench  and qualitative comparisons can both validate the superiority of LVD-2M.

**Setup.** We finetune a base T2V diffusion model with 1.75B parameters, which has a similar structure as MagicVideo . The base model was pretrained to generate 32-frame videos and finetuned at 65-frame length in this experiment. The finetuning settings for LVD-2M and WebVid-10M are the same, which is 64 batch size, 4 gradient accumulation iterations and for 30k iterations, roughly going over 2M video clips once at the finetuning stage. For quantitative evaluation, we follow the standard evaluation protocol of VBench .

**Results and analysis.** As shown in Tab. 3, compared to WebVid-10M, finetuning on LVD-2M will lead to better performance in 10 out of 16 metrics of VBench, especially surpassing WebVid-10M by a large margin in **dynamic degree, object class** and **human action**. These obvious performance improvements against the baseline can be attributed to the diverse and the highly dynamic video data of LVD-2M. Notably, the evaluation prompts from VBench have a small average length (7.6 words), which is much closer to the average caption length of WebVid-10M (14.1 words) than LVD-2M (88.7 words). Despite the caption length gap between training and evaluation, the model finetuned on LVD-2M still presents superior overall performance. We further demonstrate the qualitative comparisons in Fig. 7. Due to limited computational resources, we didn't validate LVD-2M on stronger T2V models, and the text encoding of the chosen T2V model is still based on CLIP , which struggles to properly encode long captions. We expect even more obvious performance enhancement when finetuning on LVD-2M using more advanced T2V models with more powerful text encoders .

## 5 Conclusion

High-quality long video datasets are essential for training long video generation models. In this work, we devise an automatic data curation pipeline to filter high-quality long-take videos from existing large-scale video datasets and to annotate temporally-dense captions for the filtered videos. Based on this pipeline, we construct LVD-2M, the first long-take video dataset of 2 million videos with large motion, diverse content, and temporally dense captions. We validate the quality of the dataset through human evaluation and verify its effectiveness by fine-tuning video generation models to generate long videos with large motions.

Figure 7: After finetuning a T2V diffusion model on LVD-2M, the videos are more dynamic, and the actions and objects in the videos are more reasonable, in contrast to finetuning on WebVid-10M.