# The Power of Resets in Online Reinforcement Learning

Zakaria Mhammedi

Google Research

mhammedi@google.com &Dylan J. Foster

Microsoft Research

dylanfoster@microsoft.com &Alexander Rakhlin

MIT

rakhlin@mit.edu

###### Abstract

Simulators are a pervasive tool in reinforcement learning, but most existing algorithms cannot efficiently exploit simulator access--particularly in high-dimensional domains that require general function approximation. We explore the power of simulators through online reinforcement learning with _local simulator access_ (or, local planning), an RL protocol where the agent is allowed to _reset_ to previously observed states and follow their dynamics during training. We use local simulator access to unlock new statistical guarantees that were previously out of reach:

1. We show that MDPs with low _coverability_--a general structural condition that subsumes Block MDPs and Low-Rank MDPs--can be learned in a sample-efficient fashion with _only \(Q^{*}\)-realizability_ (realizability of the optimal state-value function); existing online RL algorithms require significantly stronger representation conditions.
2. As a consequence, we show that the notorious _Exogenous Block MDP_ problem  is tractable under local simulator access. The results above are achieved through a computationally-inefficient algorithm. We complement them with a more computationally efficient algorithm, RVFS (_Recursive Value Function Search_), which achieves provable sample complexity guarantees under strengthened statistical assumption known as _pushforward coverability_. RVFS can be viewed as a principled, provable counterpart to a successful empirical paradigm that combines recursive search (e.g., MCTS) with value function approximation.

## 1 Introduction

Simulators are a widely used tool in reinforcement learning. Many of the most well-known benchmarks for reinforcement learning research make use of simulators (Atari , MuJoCo , OpenAI Gym , DeepMind Control Suite ), and high-quality simulators are available for a wide range of real-world control tasks, including robotic control , autonomous vehicles , and game playing . Simulators also provide a useful abstraction for _planning_ with a known or learned model, an important building block for many RL techniques . Yet, in spite of the ubiquity of simulators, almost all existing research into algorithm design--empirical and theoretical--has focused on the _online reinforcement learning_ (where only trajectory-based feedback is available), and does not take advantage of the extra information available through the simulator. Relatively little is known about the full power of RL with simulator access, either in terms of algorithmic principles or fundamental limits.

We explore the power of simulators through online reinforcement learning with _local simulator access_ (RLLS for short), also known as _local planning_. Here, the agent learns by repeatedly executing policies and observing the resulting trajectories (as in online RL), but is allowed to _reset_ to previously observed states and follow their dynamics during training.

Empirically, algorithms based on local simulators have received limited investigation, but with promising results. Notably, the Go-Explore algorithm  uses local simulator access to achieve state-of-the-art performance for Montezuma's Revenge (a difficult Atari game that requires systematic

[MISSING_PAGE_FAIL:2]

\(()\}_{h=1}^{h}\); we use \(_{5}\) to denote the set of all such functions. When a policy is executed, it generates a trajectory \((_{1},_{1},_{1}),,(_{H},_{H},_{h})\) via the process \(_{h}_{h}(_{h}),_{h} R_{h}(_{h},_{h}), _{h+1} T_{h}(_{h},_{h})\), initialized from \(_{1} T_{0}()\) (we use \(_{H+1}\) to denote a terminal state with zero reward). We write \(^{}[]\) and \(^{}[]\) to denote the law and expectation under this process.

For a policy \(\), \(J()^{}_{h=1}^{H}_{h}\) denotes expected reward, and the value functions are given by \(V_{h}^{}(x)=^{}_{h^{}=h}^{H}_{h^{ }}_{h}=x\), and \(Q_{h}^{}(x,a)^{}_{h^{}=h}^{H}_ {h^{}}_{h}=x,_{h}=a\). We denote by \(^{*}\) the optimal deterministic policy that maximizes \(Q^{^{*}}\), and write \(Q^{*} Q^{^{*}}\) and \(V^{*} V^{^{*}}\).

**Online reinforcement learning with a local simulator.** In the standard online reinforcement learning framework, the learner repeatedly interacts with an (unknown) MDP by executing a policy and observing the resulting trajectory, with the goal of maximizing the total reward. Formally, for each episode \([N_{}]\), the learner selects a policy \(^{(*)}=\{^{(*)}_{h}\}_{h=1}^{H}\), executes it in the underlying MDP \(^{*}\) and observes the trajectory \(\{(_{h}^{()},_{h}^{()},_{h}^{()})\}_{h=1}^{H}\). After all \(N_{}\) episodes conclude, the learner produces a policy \(_{5}\) with the goal of minimizing the risk given by \([J(^{*})-J()]\).

In online RL with local simulator access, or RLLS, [57; 40; 65; 59; 66], we augment the online RL protocol as follows: At each episode \([N]\), instead of starting from a random initial state \(_{1} T_{0}()\), the agent can _reset_ the MDP to any layer \(h[H]\) and any state \(_{h}\) previously encountered, and proceed with a new episode starting from \(_{h}\). As in the online RL protocol, the goal is to produce a policy \(_{5}\) such that \([J(^{*})-J()]\) with as few episodes of interaction as possible; our main results take \(N_{}=(C,^{-1})\) for a suitable problem parameter \(C\).

**Executable versus non-executable policies.** We focus on learning policies that can be executed without access to a local simulator (in other words, the local simulator used at train time, but not test time). Some recent work using local simulators for RL with linear function approximation  considers a more permissive setting where the final policy \(\) produced by the learner can be _non-executable_; our function approximation requirements can be slightly relaxed in this case.

**Definition 2.1** (Non-executable policy).: _We refer to a policy \(\) for which computing \((x)()\) for any \(x\) requires \(n\) local simulator queries as a non-executable policy with sample complexity \(n\)._

Additional notation.For any \(m,n\), we denote by \([m\,.\,n]\) the integer interval \(\{m,,n\}\). We also let \([n][1\,.\,n]\). We refer to a scalar \(c>0\) as an _absolute constant_ to indicate that it is independent of all problem parameters and use \(()\) to denote a bound up to factors poly-logarithmic in parameters appearing in the expression. We define \(_{}_{5}\) as the random policy that selects actions in \(\) uniformly. We define the occupancy measure for policy \(\) via \(d_{h}^{}(x,a)^{}[_{h}=x,_{h}=a]\). For functions \(g:\) and \(f:\), we define Bellman backup operators by \(_{h}[g](x,a)=[_{h}+_{a^{}} g(_{h+1},a^{})_{h}=x,_{h}=a]\) and \(_{h}[f](x,a)=[_{h}+f(_{h+1})_{h}= x,_{h}=a]\). For a stochastic policy \(_{5}\), we will occasionally use the bold notation \(_{h}(x)\) as shorthand for the random variable \(_{h}_{h}(x)()\). For a function \(f:\), we write \(a^{}*{arg\,max}_{a}f(a)\) to denote the action that maximizes \(f\). If there are ties, we break them by picking the action with the smallest index; we assume without loss of generality that actions in \(\) are index from \(1,,||\).

## 3 New Sample-Efficient Learning Guarantees via Local Simulators

This section presents our most powerful results for RLLS. We present a new algorithm for learning with local simulator access, SimGolf (Section 3.1), and show that it enables sample-efficient RL for MDPs with low _coverability_ using only \(Q^{*}\)-realizability (Section 3.2). We then give implications for the Exogenous Block MDP problem (Section 3.3).

**Function approximation setup and coverability.** To achieve sample complexity guarantees for online reinforcement learning that are suitable for large, high-dimensional state spaces, we appeal to _value function approximation_. We assume access to a function class \(([H][0,H])\) that contains the optimal state-action value function \(Q^{*}\); we define \(_{h}=\{Q_{h} Q}\}\).

**Assumption 3.1** (\(Q^{*}\)-realizability).: _For all \(h[H]\), we have \(Q^{*}_{h}_{h}\)._

\(Q^{*}\)-realizability is widely viewed as a minimal representation condition for online RL [61; 17; 16; 39; 58; 56]. The class \(\) encodes the learner's prior knowledge about the MDP, and can be parameterized by rich function approximators like neural networks. We assume for simplicity of exposition that and \(\) are finite, and aim for sample complexity guarantees scaling with \([]\) and \([]\); extending our results to infinite classes via standard uniform convergence arguments is straightforward.

**Coverability.** Beyond representation conditions like realizability, online RL algorithms require _structural conditions_ that limit the extent to which deliberately designed algorithms can be surprised by substantially new state distributions. We focus on a structural condition known as _coverability_, which is inspired by connections between online and offline RL.

**Assumption 3.2**.: _The coverability coefficient is \(C_{}_{h[H]}_{_{h}( )}_{_{5}}\|^{}}{_{h}}\|_{ }\)._

Coverability is an intrinsic strutural property of the underlying MDP. Examples of MDP families with low coverability include (Exogenous) Block MDPs, which have \(C_{}||||\), where \(\) is the _latent state space_, and Low-Rank MDPs, which have \(C_{} d||\), where \(d\) is the feature dimension ; importantly, these settings exhibit high-dimensional state spaces and require nonlinear function approximation. As in prior work , our algorithms require _no prior knowledge_ of the distribution \(_{h}\) that achieves the minimum in Assumption 3.2.

### Algorithm

Our main algorithm, SimGolf, is displayed in Algorithm 1. The algorithm is a variant of the GOLF method of Jin et al. , Xie et al.  with novel adaptations to exploit the availability of a local simulator. Like GOLF, SimGolf explores using the principle of _global optimisim_: At each iteration \(t[N]\), it maintains a confidence set (or, version space) \(^{()}\) of candidate value functions with low squared Bellman error under the data collected so far, and chooses a new exploration policy \(^{()}\) by picking the most "optimistic" value function in this set. As the algorithm gathers more data, the confidence set shrinks, leaving only near-optimal policies.

The main novelty in SimGolf arises in the data collection strategy and design of confidence sets. Like GOLF, SimGolf algorithm constructs the confidence set \(^{()}\) such that all value functions \(g^{()}\) have small squared Bellman error:

\[_{i<t}^{^{()}}g_{h}(_{h},_{ h})-_{h}[g_{h+1}](_{h},_{h})^{2}| |, h[H]. \]

Due to the presence of the Bellman backup \(_{h}[g_{h+1}]\) in Eq. (1), naively estimating squared Bellman error leads to the notorious _double sampling_ problem. To avoid this, the approach taken with GOLF and related work  is to adapt a certain de-biasing technique to remove double sampling bias, but this requires access to a value function class that satisfies _Bellman completeness_, a representation significantly more restrictive than realizability (e.g., Foster et al. ).

The idea behind SimGolf is to use local simulator access to directly produce high-quality estimates for the Bellman backup function \(_{h}[g_{h+1}]\) in Eq. (1). In particular, for a given state-action pair \((x,a)\), we can estimate the Bellman backup \(_{h}[g_{h+1}](x,a)\) for all functions \(g\) simultaneously by collecting \(K\) next-state transitions \(}_{h+1}^{(1)},,}_{h+1}^{(K)}}}{{}}T_{h}( x,a)\) and \(K\) rewards \(}_{h}^{(1)},,}_{h}^{(K)}}}{{}}R_{h}(x,a)\), then taking the empirical mean: \(_{h}[g_{h+1}](x,a)_{k=1}^{K} {}_{h}^{(k)}+_{a^{}}g_{h+1}(}_{ h+1}^{(k)},a^{})\). Line 8 of SimGolf uses this technique to directly estimate the Bellman residual backup under a trajectory gathered with \(^{()}\), sidestepping the double sampling problem and removing the need for Bellman completeness. We suspect this technique (estimation with respect to squared Bellman error using local simulator access) may find broader use.

### Main Result

We now state the main guarantee for SimGolf and discuss some of its implications.

**Theorem 3.1** (Main guarantee for SimGolf).: _Let \(,(0,1)\) be given and suppose Assumption 3.1 (\(Q^{}\)-realizability) and Assumption 3.2 (coverability) hold with \(C_{}>0\). Then the policy \(\) produced by \((,C_{},,)\) (Algorithm 1) has \(J(^{})-[J()]\) with probability at least \(1-\). The total sample complexity in the RLLS framework is bounded by \(}H^{5}C_{}^{2}(||/ )^{-4}\)._

This result (whose proof is in Appendix E) shows that under only \(Q^{}\)-realizability and coverability, SimGolf learns an \(\)-optimal policy with polynomial sample complexity, significantly relaxing the representation assumptions (Bellman completeness, weight function realizability) required by prior algorithms for coverability . This is the first instance we are aware of where local simulator access unlocks sample complexity guarantees for reinforcement learning with _nonlinear_ function approximation that were previously out of reach; perhaps the most important technical idea here is our approach to combining global optimism with local simulator access, in contrast to greedy layer-by-layer schemes used in prior work on local simulators (with the exception of Weisz et al. ). In particular, we suspect that the idea of performing estimation with respect to squared Bellman error directly using local simulator access may find broader use beyond coverability. Improving the polynomial dependence on problem parameters is an interesting question for future work.

**A conjecture.** By analogy to results in offline reinforcement learning, where \(Q^{*}\)-realizability and concentrability (the offline counterpart to coverability) alone are known to be insufficient for sample-efficient learning , we conjecture that \(Q^{*}\)-realizability and coverability alone are not sufficient for polynomial sample complexity in vanilla online RL. If true, this would imply a new separation between online RL with and without local simulators.

### Implications for Exogenous Block MDPs

We now apply SimGolf and Theorem 3.1 to the _Exogenous Block MDP_ (ExBMDP) problem , a challenging rich-observation reinforcement learning setting in which the observed states \(_{h}\) are high-dimensional, while the underlying dynamics of the system are low-dimensional, yet confounded by temporally correlated exogenous noise.

Formally, an Exogenous Block MDP \(=(,,,,H,T,R,g)\) is defined by a _latent state space_ and an _observation space_. We begin with the latent state space. Starting from an initial _endogenous state_\(_{1}\) and _exogenous state_\(_{1}\), the latent state \(_{h}=(_{h},_{h})\) evolves for \(h[H]\) via \(_{h+1} T_{h}^{}(_{h},_{h})\) and \(_{h+1} T_{h}^{}(_{h})\), where \(_{h}\) is the agent's action at layer \(h\); we adopt the convention that \(_{1} T_{0}^{}()\) and \(_{1} T_{0}^{}()\). Note that only the endogenous state is causally influenced by the action. The latent state is not observed; instead, at each step \(h\), the agent receives an _observation_\(_{h}\) generated via1\(_{h}=g_{h}^{}(_{h},_{h})\), where \(g_{h}^{}:\) is the _emission function_. We assume the endogenous latent space \(\) and action space \(\) are finite, and define \(S||\) and \(A||\). However, the exogenous state space \(\) and observation space \(\) may be arbitrarily large or infinite, with \(||,||||\).2

The final property of the ExBMDP model is _decodability_, which asserts the existence of a _decoder_ such that \(_{}\) such that \(_{}(_{h})=_{h}\) a.s. for all \(h[H]\) with \(_{h}=g_{h}^{}(_{h},_{h})\),, Informally, decodability ensures the existence of an (unknown to the learner) mapping that allows one to perfectly recover the endogenous latent state from observations. In addition to decodability, we assume the rewards in the ExBMDP are _endogenous_; that is, the reward distribution \(R_{h}(_{h},_{h})\) only depends on the observations \((_{h})\) through the corresponding latent states \((^{*}(_{h})=_{h})\). To enable sample-efficient learning, we assume access to a _decoder class_\(\) that contains \(^{*}\), as in prior work.

**Assumption 3.3** (Decoder realizability).: _We have access to a decoder class \(\) such that \(^{*}\)._

**Applying SimGolf and Theorem 3.1**.: To apply Theorem 3.1 to the ExBMDP problem, we need to verify that \(Q^{*}\)-realizability and coverability hold. Realizability is a straightforward consequence of decodability (Lemma D.1 in Part II of the appendix). For coverability, Xie et al.  show that ExBMDPs have \(C_{} SA\) under decodability, in spite of the time-correlated exogenous noise process \((_{h})\) and potentially infinite observation space \(\) (interestingly, coverability is essentially the only useful structural property that ExBMDPs are known to satisfy, which is our primary motivation for studying it). This leads to the following corollary of Theorem 3.1.

**Corollary 3.1** (SimGolf for ExBMDPs).: _Consider the ExBMDP setting. Suppose that Assumption 3.3 holds, and let \(\) be constructed as in Lemma D.1 of Part II. Then for any \(,(0,1)\), the policy \(=(,SA,,)\) has \(J(^{*})-J()\) with probability at least \(1-\). The total sample complexity in the RLLS framework is \(N=H^{5}S^{3}A^{3}[]^{-4}\)._

This shows for the first time that general ExBMDPs are learnable with local simulator access. Prior to this work, online RL algorithms for ExBMDPs required either (i) deterministic latent dynamics , or (ii) factored emission structure . Xie et al.  observed that ExBMDPs admit low coverability, but their algorithm requires Bellman completeness, which is not satisfied by ExBMDPs (see Islam et al. ). See Appendix A for more discussion.

## 4 Computationally Efficient Learning with Local Simulators

Our result in Section 3 show that local simulator access facilitates sample-efficient learning in MDPs with low coverability, a challenging setting that was previously out of reach. However, our algorithm SimGolf is computationally-inefficient because it relies on global optimism, a drawback found in most prior work on RL with general function approximation . It remains an open question whether any form of global optimism can be implemented efficiently, and some variants have provable barriers to efficient implementation .

To address this drawback, in this section we present a new algorithm, RVFS (Recursive Value Function Search; Algorithm 5), which requires stronger versions of the coverability and realizability assumptions in Section 3, but is computationally efficient in the sense that it reduces to convex optimization over the state-value function class \(\). RVFS makes use of a sophisticated recursive exploration scheme based on core-sets, sidestepping the need for global optimism.

### Function Approximation and Statistical Assumptions

To begin, we require the following strengthening of the coverability assumption in Assumption 3.2.

**Assumption 4.1** (Pushforward coverability).: _The pushforward coverability coefficient \(C_{}>0\) is given by \(C_{}=_{h[H]}_{_{h}()}_{(x_ {h-1},_{h-1},x_{h})_{h-1}_{h} }(x_{h}|x_{h-1},_{h-1})}{_{h}(x_{h})}\)._

Pushforward coverability is inspired by the _pushforward concentrability_ condition used in offline RL by . Concrete examples include, (i) Block MDPs with latent space \(\), which admit \(C_{}||\), (ii) Low-Rank MDPs in dimension \(d\), which admit \(C_{} d\), and (iii) Exogenous Block MDPs for which the exogenous noise process satisfies a _weak correlation condition_ that we introduce in Appendix B. Note that \(C_{} C_{}||\), but the converse is not true in general.

Instead of state-action value function approximation as in SimGolf, in this section we make use of a state value function class \(([H][0,H])\), but require somewhat stronger representation conditions than in Section 3. We consider two complementary setups:

* **Setup I:** Assumptions 4.2 and 4.3 (\(V^{*}/^{*}\)-realizability) and Assumption 4.4 (\(\)-gap) hold.
* **Setup II:** Assumption 4.5 (\(V^{}\)-realizability) and Assumption 4.6 (\(\)-realizability) hold.

We describe these assumptions in more detail below.

**Function approximation setup I.** First, instead of \(Q^{*}\)-realizability, we consider the weaker \(V^{*}\)-realizability .

**Assumption 4.2** (\(V^{*}\)-realizability).: _For all \(h[H]\), we have \(V^{*}_{h}_{h}\)._Under \(V^{*}\)-realizability, our algorithm learns a near-optimal policy, but the policy is _non-executable_ (cf. Definition 2.1); this property is shared by prior work on local simulator access with value function realizability . To produce executable policies, we additionally require access to a policy class \(_{5}\) containing \(^{*}\); we define \(_{h}=\{_{h}\}\).

**Assumption 4.3** (\(^{*}\)-realizability).: _The policy class \(\) contains the optimal policy \(^{*}\)._

\(V^{*}\)-realizability (Assumption 4.2) and \(^{*}\)-realizability (Assumption 4.3) are both implied by \(Q^{*}\)-realizability, and hence are weaker. However, we also assume the optimal \(Q\)-function admits constant gap (this makes the representation conditions for **Setup I** incomparable to Assumption 3.1).

**Assumption 4.4** (\(\)-Gap).: _The optimal action \(_{h}^{*}(x)\) is unique, and there exists \(>0\) such that for all \(h[H]\), \(x\), and \(a\{_{h}^{*}(x)\}\), \(Q_{h}^{*}(x,_{h}^{*}(x))>Q_{h}^{*}(x,a)+\)._

This condition has been used in a many prior works on computationally efficient RL with function approximation .

Function approximation setup II.We also provide guarantees under the assumption that the class \(\) satisfies _all-policy realizability_ in the sense that \(V^{}\) for all \(_{5}\).

**Assumption 4.5** (\(V^{}\)-realizability).: _The class \(=_{1:H}\) has \(V_{h}^{}_{h}\) for all \(_{5}\) and \(h[H]\)._

This assumption will be sufficient to learn a non-executable policy, but to learn executable policies we require an analogous strengthening of Assumption 4.5.

**Assumption 4.6** (\(\)-realizability).: _For all \(_{5}\), we have that \(x_{a}_{h}[V_{h+1}^{}](,a)\)._

This assumption has been used by a number of prior works on computationally efficient RL . Assumptions 4.5 and 4.6 are both implied by the slightly simpler-to-state assumption of \(Q^{}\)-_realizability_, which asserts access to a class \(\) that contains \(Q^{}\) for all \(_{5}\).

### Algorithm

For ease of exposition, we defer the full version of our algorithm, RVFS (Algorithm 5), to Appendix F and present a simplified version here (Algorithm 2). The algorithms are nearly identical, except that the simplified version assumes that certain quantities of interest (e.g., Bellman backups) can be computed exactly, while the full version (provably) approximates them from samples.

RVFS maintains a value function estimator \(=_{1:H}\) that aims to approximate the optimal value function \(V_{1:H}^{}\), as well as _core sets_\(_{1},,_{H}\) of state-action pairs that are used to perform estimation and guide exploration. At a high level, RVFS alternates between (i) fitting the value function \(_{h}\) for a given layer \(h[H]\) based on Monte-Carlo rollouts, and (ii) using the core-sets to test whether the current value function estimates \(_{h+1:H}\) remain accurate as the roll-in policy induced by \(_{h}\) changes.

In more detail, RVFS is based on recursion across the layers \(h[H]\). When invoked for layer \(h\) with value function estimates \(_{h+1:H}\) and core-sets \(_{h},,_{H},\)\(_{h}\) performs two steps:

1. For each state-action pair \((x_{h-1},a_{h-1})_{h}\),4 the algorithm gathers \(N_{}\) trajectories by rolling out from \((x_{h-1},a_{h-1})\) with the greedy policy \(_{}(x)_{a}_{}[ _{+1}](x,a)\) that optimizes the estimated value function; in the full version of RVFS (see Algorithm 5), we estimate the bellman backup \(_{}[_{+1}](x,a)\) using the local simulator. For all states \(x_{-1}\{_{h},,_{H-1}\}\) encountered during this process, the algorithm checks whether \(|[_{}(_{})-V_{}^{}(_{ })_{-1}=x_{-1},_{-1}=a_{-1}]|\) for all \(a_{-1}\) using a test based on (implicitly maintained) confidence sets. If the test fails, this indicates that distribution shift has occurred, and the algorithm adds the pair \((x_{-1},a_{-1})\) to \(_{}\) and recurses on layer \(\) via \(_{}\). 2. If all tests above pass, this means that \(_{h+1},,_{H}\) are accurate, and no distribution shift has occurred. In this case, the algorithm fits \(_{h}\) by collecting Monte-Carlo rollouts from all state-action pairs in the core-set \(_{h}\) with \(_{}(x)_{a}_{}[ _{+1}](x,a)\) (cf. Line 16), and returns.

When the tests in Item 1 succeed for all \(h[H]\), the algorithm returns the estimated value functions \(_{1:H}\); in this case, the greedy policy \(_{}(x)_{a}_{}[ _{+1}](x,a)\) is guaranteed to be near optimal. The full version of RVFS in Algorithm 5 uses local simulator access to estimate the Bellman backups \(_{h}[_{h+1}](x,a)\) for different state-action pairs \((x,a)\). These backups are used to (i) compute actions of the greedy policy that maximizes \(_{1:H}\) via (e.g., Eq. (2)); (ii) generate trajectories by rolling out from state-action pairs in the core-sets (Line 6); and (iii) perform the test in Item 1 (Line 8).

RVFS is inspired by the DMQ algorithm  originally introduced in the context of online reinforcement learning with linearly realizable \(Q^{*}\). RVFS incorporates local simulator access (most critically, via core-set construction) to allow for more general _nonlinear_ function approximation without restrictive statistical assumptions. Prior algorithms for RLLS have used core-sets of state-action pairs in a similar fashion , but in a way that is tailored to linear function approximation.

In what follows, we discuss various features of the algorithm in greater detail.

Bellman backup policies.Since RVFS works with state value functions instead of state-action value functions, we need a way to extract policies from the former. The most natural way to extract a policy from estimated value functions \(_{1:H}\) is as follows: for all \(h[H]\), define \(_{h}(x)*{arg\,max}_{a}_ {h}[_{h+1}](x,a)\). In reality, we do not have access to \(_{h}[_{h+1}](x,a)\) directly, so the full version of RVFS (Algorithm 5) estimates this quantity on the fly using the local simulator using the following scheme (Algorithm 7 in Appendix F): Given a state \(x\), for each \(a\), we sample \(K\) rewards \(_{h} R_{h}(x,a)\) and next-state transitions \(_{h+1} T_{h}( x,a)\), then approximate \(_{h}[_{h+1}](x,a)\) by the empirical mean. We remark that the use of these Bellman backup policies is actually crucial in the analysis for RVFS; even if we were to work with estimated state-action value functions \(_{1:H}\) instead, our analysis would require executing the Bellman backup policies \(_{h}(x)*{arg\,max}_{a}_ {h}[_{h+1}](x,a)\) (instead of naively using \(_{h}(x)*{arg\,max}_{a}_{h}(x,a)\)).

**Invoking the algorithm.** The base invocation of RVFS takes the form

\[_{1:H}_{0}(_{1:H}=,_{1:H}=\{_{h}\}_{h=1}^{H},_{0:H}=\{ \}_{h=0}^{H};;,,).\]

Whenever this call returns, the greedy policy induced by \(_{1:H}\) is guaranteed to be near-optimal. Naively, the approximate Bellman backup policy induced by \(_{1:H}\) (described above) is non-executable, and must be computed by invoking the local simulator. To provide an end-to-end guarantee to learn an executable policy, we give an outer-level algorithm, RVFS.bc (Algorithm 6, deferred to Appendix F for space), which invokes RVFS\({}_{0}\), then extracts an executable policy from \(_{1:H}\) using behavior cloning. Subsequent recursive calls to RVFS take the form \(\ (_{h:H},}_{h:H},_{h:H} )_{h}(_{h+1:H},_{h+1:H}, _{h:H};,,).\) The arguments here are: Importantly, the confidence sets \(}_{h+1:H}\) do not need to be explicitly maintained, and can be used implicitly whenever a _regression oracle_ for the value function class is available (discussed below).

**Remark 4.1** (Oracle-efficiency).: RVFS _is computationally efficient in the sense that it reduces to convex optimization over the value function class \(\). In particular, the only computationally intensive steps in the algorithm are (i) the regression step in Line 16, and (ii) the test in Line 8 involving the confidence set \(}_{}\). For the latter, we do not explicitly need to maintain \(}_{}\), as the optimization problem over this set in Line 8 (for the full version of RVFS in Algorithm 5) reduces to solving \(_{V}\{_{i=1}^{n}V(^{(i)}\ )\ |\ _{i=1}^{n}(V(x^{(i)})-y^{(i)})^{2}^{2}\}\) for a dataset \(\{(x^{(i)},^{(i)},y^{(i)})\}_{i=1}^{n}\). This is convex optimization problem in function space, and in particular can be implemented in a provably efficient fashion whenever \(\) is linearly parameterized. We expect that the problem can also be reduced to a square loss regression by adapting the techniques in Krishnamurthy et al. , Foster et al. , but we do not pursue this here._

### Main Result

We present the main guarantee for RVFS under the function approximation assumptions in Section 4.1.

**Theorem 4.1** (Main guarantee for RVFS).: _Let \(,(0,1)\) be given, and suppose that Assumption 4.1 (pushforward coverability) holds with \(C_{}>0\). Further, suppose that one the following holds:_

* **Setup I:** _Assumptions_ 4.2 _and_ 4.3 _(_\(V^{*}/^{*}\)_-realizability) and_ Assumption_ 4.4 _(_\(\)_-gap) hold, and_ \( 6H\)_._
* **Setup II:** _Assumption_ 4.5 _(_\(V^{*}\)_-realizability) and Assumption_ 4.6 _(_\(\)_-realizability) hold._

_Then,_ RVFS.bc\((,,,)\) _(Algorithm 6) returns a policy \(_{1:H}\) such that \(J(^{*})-J(_{1:H}) 2\) with probability at least \(1-\), and has total sample complexity bounded by_

\[}(C_{}^{8}H^{23}A^{-1 3}).\]

_Furthermore, the algorithm makes at most \(\!(C_{},H,A,^{-1})\) calls to the convex optimization oracle over value function space described in Remark 4.1._

Theorem 4.1 shows for the first time that sample- and computationally-efficient RL with local simulator access is possible under pushforward coverability. In particular, RVFS is the first computationally efficient algorithm for RL with local simulator access that supports nonlinear function approximation. The assumptions in Theorem 4.1, while stronger than those in Section 3, are not known to enable sample-efficient RL without simulator access. Nonetheless, understanding whether RVFS can be strengthened to support general coverability or weaker function approximation is an important open problem. See Appendix H.1 for an overview of the analysis; we remark (Appendix I.1) that the result is actually proven under slightly weaker assumptions than those in **Setup I/Setup II**.

**Connection to empirical algorithms.** RVFS bears some similarity to Monte-Carlo Tree Search (MCTS)  and AlphaZero , which perform planning with local simulator. Informally, MCTS can be viewed as a form of breadth-first search over the state space (where each node represents a state at a given layer), and AlphaZero is a particular instantiation of a MCTS that leverages \(V-\)value function approximation to allow for generalization across states. Compared to RVFS, MCTS and AlphaZero perform exploration via simple bandit-style heuristics, and are not explicitly designed to handle _distribution shifts_ that arise in settings where actions have long-term downstream effects. What is more, MCTS requires finite states to iterate over all possible child nodes of each state, making it inapplicable in environments with continuous states. RVFS may be viewed as a provable counterpartthat can handle continuous states and uses function approximation to address distribution shift in a principled fashion (in particular, through the use of confidence sets and the test in Line 14).5

**Applying RVFS to Exogenous Block MDPs.** ExBMDPs satisfy coverability (Assumption 3.2), but do not satisfy the pushforward coverability assumption (Assumption 4.1) in general. However, it turns out that ExBMDPs _do_ satisfy pushforward coverability when the exogenous noise process is weakly correlated across time, a new statistical assumption we refer to the _weak correlation condition_. In Appendix B (Theorem B.1), we give a variant of RVFS for ExBMDPs that succeeds under (i) weak correlation, and (ii) decoder realizability, sidestepping the need for the \(\)-gap or \(V^{}\)-realizability.

## 5 Discussion and Future Work

In this paper, we demonstrated that resets can substantially expand the range of reinforcement learning (RL) settings that are tractable, both statistically and computationally. Our practical algorithm, RVFS, provides a principled counterpart to MCTS by supporting continuous state spaces and offering provable guarantees, setting it apart from traditional MCTS. Statistically, our results extend to MDPs with a finite Sequential Estimation Coefficient (SEC) , capturing a broader class of MDPs beyond those with finite coverability--encompassing low Bellman Eluder MDPs  and MDPs with finite bilinear rank . Although not formally developed here, it is possible to generalize push-forward coverability and our analysis to encompass a range of linear function approximation settings , thereby recovering known positive results under local access in these settings.

While our focus has been on theoretical contributions--analyzing the sample and computational complexity of RL with access to a local simulator--this work also raises promising empirical questions. We are particularly interested in exploring these questions in future work, aiming to bridge these theoretical advances with empirical validation in practical RL settings.