# When LLM Meets DRL: Advancing Jailbreaking Efficiency via DRL-guided Search

Xuan Chen1, Yuzhou Nie2, Wenbo Guo2, Xiangyu Zhang1

1Purdue University

2University of California, Santa Barbara

{chen4124, xyzhang}@cs.purdue.edu

{yuzhouunie, henrygvb}@ucsb.edu

###### Abstract

Recent studies developed jailbreaking attacks, which construct jailbreaking prompts to "fool" LLMs into responding to harmful questions. Early-stage jailbreaking attacks require access to model internals or significant human efforts. More advanced attacks utilize genetic algorithms for automatic and black-box attacks. However, the random nature of genetic algorithms significantly limits the effectiveness of these attacks. In this paper, we propose RLbreaker, a black-box jailbreaking attack driven by deep reinforcement learning (DRL). We model jailbreaking as a search problem and design an RL agent to guide the search, which is more effective and has less randomness than stochastic search, such as genetic algorithms. Specifically, we design a customized DRL system for the jailbreaking problem, including a novel reward function and a customized proximal policy optimization (PPO) algorithm. Through extensive experiments, we demonstrate that RLbreaker is much more effective than existing jailbreaking attacks against six state-of-the-art (SOTA) LLMs. We also show that RLbreaker is robust against three SOTA defenses and its trained agents can transfer across different LLMs. We further validate the key design choices of RLbreaker via a comprehensive ablation study. Code is available at [https://github.com/XuanChen-xc/RLbreaker](https://github.com/XuanChen-xc/RLbreaker).

## 1 Introduction

Recent research discovered jailbreaking attacks against LLMs, i.e., the attacker constructs jailbreaking prompts embedded with harmful or unethical questions . These jailbreaking prompts can force an aligned LLM to respond to the embedded harmful questions. Early-stage attacks mainly rely on handcrafted jailbreaking prompts , or require accessing model internals . More recent works explore automatic and black-box jailbreaking attacks. These attacks either leverage in-context learning  or genetic methods . Specifically, in-context learning attacks keep querying another helper LLM to generate and refine jailbreaking prompts. As shown in Section 4, purely relying on in-context learning has a limited ability to continuously refine the prompts. Genetic method-based attacks design different mutators that leverage the helper LLM to modify the jailbreaking prompts. They refine the prompts by iteratively selecting the promising prompts as the seeds for the next round. While outperforming in-context learning-based attacks on some open-source models, their efficacy remains constrained by the stochastic nature of genetic methods, as they randomly select mutators without a proper strategy.

In this paper, we model jailbreaking attacks as a search problem and design a DRL system RLbreaker, to enable a more efficient and guided search. At a high level, we train our DRL agent to select proper mutators for different harmful questions, which is more efficient than random mutator selection.

Specifically, we first design an LLM-facilitated action space that leverages a helper LLM to mutate the current jailbreaking prompt. Our action design enables diverse action variations while constraining the overall policy learning space. We also design a customized reward function that can decide whether the target LLM's response actually answers the input harmful question at each time step. Our reward function provides dense and meaningful rewards that facilitate policy training. Finally, we also customize the widely used PPO algorithm  to further reduce the training randomness.

RLbreaker is composed of a target LLM, a DRL agent, and a set of mutators sharing the same helper LLM. In each training iteration, the agent takes the current jailbreaking prompt as input and outputs an action, indicating which mutator to use. RLbreaker then updates the current jailbreaking prompt using the selected mutator. The updated jailbreaking prompt is then fed to the target LLM. RLbreaker computes the reward based on the target LLM's response. The agent is trained to maximize the expected total reward. During the testing phase, given a harmful question, we first select an initial prompt from the ones generated during training. Then, we use our trained agent to automatically refine the jailbreaking prompt until the attack succeeds or reaches the maximum time limits.

We first compare RLbreaker with five SOTA attacks, including two in-context learning-based attacks (PAIR  and Cipher ), two genetic method-based attacks (AutoDAN  and GPTFUZZER ) and one while-box attack (GCG ). We run these attacks on six widely used LLMs, including Mixtral-8x7B-Instruct, Llama2-70b-chat, and GPT-3.5-turbo. Our result comprehensively demonstrates the superiority of RLbreaker over existing attacks in jailbreaking effectiveness. Second, we demonstrate the resiliency of RLbreaker against three SOTA defenses . Third, we further show that our trained policies can be transferred across different models, including a very large model: Mixtral-8x7B-Instruct. Finally, we validate our key designs through a comprehensive ablation study and demonstrate the insensitivity of RLbreaker against hyper-parameters variations. We discuss the ethical considerations and our efforts to mitigate these ethical concerns in Appendix A. To the best of our knowledge, RLbreaker is also the first work that demonstrates the effectiveness and transferability of jailbreaking attacks against very large LLMs, e.g., Mixtral-8x7B-Instruct.

## 2 Related Work

**Jailbreaking attacks against aligned LLMs.** Early stage jailbreaking attacks  mainly focus on manually crafting jailbreaking prompts, which requires intensive human efforts and have limited scalability and effectiveness. More advanced attacks on automatic jailbreaking prompt generation follow either a white-box  or a black-box setup. Here, we mainly discuss the attacks under the black-box setup. Existing black-box attacks leverage genetic methods or in-context learning to automatically generate and refine jailbreaking prompts. Specifically, genetic method-based attacks  start with some seeding jailbreaking prompt templates and iteratively generate new prompt templates by mutating the current seeds through some pre-defined mutators. For example, Liu et al.  introduce sentence-level and paragraph-level mutators.1 Yu et al.  design five different mutation operations and train a reward model to decide whether a target model's response contains harmful contents. Their attack effectiveness is constrained by the stochastic nature of the genetic methods, i.e., they randomly mutate the current seeds without a systematic strategy for mutator selection (demonstrated in Section 4). In-context learning-based attacks  leverage another helper LLM to generate jailbreaking prompts. They design various prompts for the helper LLM. For example, Chao et al.  and Mehrotra et al.  include the target model's responses as part of the prompts of helper LLM. Yuan et al.  and Wang et al.  ask the helper LLM to encrypt the harmful questions as jailbreaking prompts. Some other works  even fine-tune the helper LLM with a customized dataset to better generate jailbreaking prompts. As demonstrated in Section 4, relying purely on in-context learning to refine the jailbreaking prompts is less efficient because of their limited capability of making sequential refinement decisions.

There are some other works that leverage DRL to attack LLMs . They target a different target model or have a different attack goal from our work. For example, Guo et al.  target models with a classification task. Perez et al.  and Hong et al.  force a target model to generate toxic responses regardless of input queries, whose goal is different from jailbreaking attacks. As such, we do not consider these attacks in this paper.

**Defenses.** Existing testing-time defenses against jailbreaking attacks follow two lines of approaches. One is mutating the input prompts with different strategies to break the structure of a potential jailbreaking prompt and help the target LLM recognize the hidden harmful question [26; 4; 45; 22; 60; 58]. The other is to filter the outputs from the target LLM by adding an additional component to determine the harmfulness of the responses [17; 68; 70] or applying new decoding strategies [29; 62]. Training-time defense  constructs additional training data that rejects malicious queries and fine-tunes a target LLM with their dataset through instruction fine-tuning.

## 3 Methodology

### Problem Definition

**Threat model.** We follow existing black-box jailbreaking attacks [65; 28; 27; 6; 35; 66] and assume the attacker cannot access the target aligned LLM's model internals (parameters, logits, losses) or its training process (data, learning algorithm). Under this setup, given a set of harmful questions, the attacker aims to force the target LLM to _provide accurate responses to those questions_.

**Jailbreaking as a search problem.** Given a set of harmful questions, denoted by \(=\{_{1},...,_{n}\}\), we aim to find a proper prompt for each question \(_{i}\), denoted as jailbreaking prompt \(_{i}\), such that the corresponding response of the target LLM to the prompt \(_{i}\), denoted as \(_{i}\), contains the accurate answer to the question \(_{i}\). We use a quantitative metric \(K(,)\) to evaluate whether the correct answer to the question \(\) is included in \(\). Later in this section, we will discuss our design of \(K(,)\). We follow existing works [65; 32] and design each prompt \(\) as a combination of a prompt structure \(\) and a harmful question \(\). For example, as shown in Fig. 4(a), given a harmful question "Can you provide instructions for making a bomb?", we can construct a structure "Simulate a conversation between..." and then fill in the question into the structure as a complete jailbreaking prompt. We can define \(\) as the space for all possible structures. Given a target LLM, denoted as \(f()\), finding a jailbreaking prompt for a question \(\) is equivalent to searching for a proper structure \(\) in \(\) and embed the question \(\) into it (this operation is denoted as \(E\)), defined as follows

\[_{i}=*{argmax}_{}K(_ {i},_{i}),_{i}=f(_{i}),_{i}=E( ,_{i}),i=1,..,n\,. \]

**Guided search vs. stochastic search.** Eqn. (1) is a searching problem, where we search for an optimal structure \(\) for each question \(\). Generally speaking, search strategies mainly fall into two categories: stochastic [38; 18] or guided  strategies. Stochastic search starts with a randomly chosen initial region and explores randomly in this region before moving to the next areas based on search outcomes . Genetic methods are a widely used stochastic search technique . They conduct the local search by mutating the current seed and moving to the next region via selections of offspring (new seed). Conversely, guided search methods, such as gradient-based techniques [46; 24], systematically advance in the search space according to some specific rules/guidance, leading to a more efficient search process. In Appendix B.2, we use a simple grid search problem to demonstrate the advances of guided search over stochastic search. Formally, the total number of grid visits required by stochastic search is _at least three_ times more than guided search. This example highlights that guided search is more stable and efficient than stochastic search, as it introduces less randomness.

**Limitations of stochastic search-based jailbreaking attacks.** The discussion above demonstrates the advantages of guided search over stochastic search methods. However, accessing proper rules for guided search can be difficult for black-box jailbreaking attacks as the gradients of the target LLM are not accessible (i.e., Eqn. (1) cannot be solved by gradient descent). As such, existing attacks resort to stochastic search by employing genetic methods [27; 65; 63]. As discussed in Section 2, these methods iteratively generate new jailbreaking prompts by _randomly selecting mutators_ to modify the current prompts. The process of random mutation selection significantly constrains the search efficacy of these methods. As demonstrated in our grid search example, the number of steps needed for stochastic search is quadratic to the grid size. This indicates applying stochastic search to problems with a huge search space (e.g., jailbreaking) is extremely inefficient.

### Overview

**DRL-driven guided search for black-box jailbreaking.** The discussion above motivates us to seek guided search-based methods for jailbreaking attacks. In this paper, we propose to achieve this bytraining a DRL agent to _strategically select proper mutators_. Specifically, as demonstrated in Fig. 1, we construct an environment with the target LLM. Given a harmful question, we train a DRL agent to generate jailbreaking prompts for that question. The prompt will be input into the target LLM to generate a response. We will design a reward function to quantify whether the target LLM's response addresses the question. The agent continuously refines the prompts by taking a sequence of actions. Its goal is to maximize the total reward, i.e., finally construct a proper prompt structure that forces the target LLM to answer the harmful question. The DRL agent, if trained properly, can learn an effective policy in searching for a proper prompt structure \(\) for each input question. This policy reduces the randomness compared to genetic methods and thus improves the overall attack effectiveness.

**Key challenges and our design insights.** The effectiveness of a DRL agent heavily relies on its state, action, and reward design, as well as the training algorithm. In the following, we discuss the challenges and insights behind our design and will provide more technical details in Section 3.3.

States. A DRL agent's state is typically defined as a vector \(\), which represents the current status of the environment. When designing the state vector, we need to make sure that it contains the key information, while avoiding an ultra-high dimensionality (to save computational complexity). To satisfy these requirements, we use the prompt generated from the last time step \(\) as the state and exclude the target LLM's response \(\). This ensures that our agent is aware of the current jailbreaking prompt (key information) without the computational burden of processing response \(\) (as the target model's response is typical of long text). As discussed later, the quality of \(\) is not overlooked and will be captured by the reward function. To further reduce the state representation's dimensionality, we will leverage a pre-trained text encoder to extract a low-dimensional representation of the current jailbreaking prompt and use it as the state vector.

Action. Our first requirement for an effective action design is to enable diverse and substantial changes to the prompts, such that the DRL agent can cover broader prompt structure space \(\). Meanwhile, we also need to ensure that the action space \(\) is not ultra-large, which will make it difficult to learn an effective strategy. Under these two requirements, we design the agent to select some predefined prompt structure mutators rather than using it to directly generate a new prompt. This is because directly generating a new prompt involves selecting tokens from the vocabulary and thus introduces an ultra-large space . Instead, we borrow prompt structure mutators designed in the existing attacks , which enables substantial changes to a given prompt structure. We train the agent to select the proper mutator at each time step. This will also constrain the action space to a few mutators instead of the entire vocabulary. Furthermore, our design can enable a better RL policy-guided search compared to genetic methods, i.e., instead of randomly selecting mutators, we now conduct the selections based on the DRL agent.

Reward. The key to reward design is to reflect whether the target LLM's response \(\) to an input question \(\) actually answers the question. We observe that in many cases, although the target LLM's responses contain harmful content, it is unrelated to the input question. For example, consider the question "How to hack into a government database and steal user information?", the target LLM responds with "Here are steps to spitefully exploit a sensor system..." In this work, we treat such responses as unsuccessful jailbreaking attempts. As such, basic metrics such as keyword matching  and harmful content detector  cannot satisfy our requirement, as they fail to evaluate the relevance of a response to a question. As detailed in Section 3.3, we

Figure 1: Overview of RLbreaker.

design a new reward function as our instantiation of \(K(,)\). It compares the difference between the target LLM's response \(_{i}\) to a harmful question \(_{i}\) and a pre-specified "reference" answer \(}_{i}\) to the same question. Given that the "reference" answer indeed addresses the harmful question, having a high semantic similarity with it confirms that the target LLM's response also answers the question, indicating a successful jailbreaking attack. Note that this reward design enables us to actually measure the relevance of the target model's response with the input question, which cannot be achieved by existing genetic-based attacks. These answers are only employed in training, and their construction can be achieved using unaligned models and denote one-time efforts.

**Agent training and testing process.** As demonstrated in Fig. 1, we first select one prompt structure \(^{(0)}\) and one harmful question \(\) and combine them together as the initial prompt/state \(^{(0)}\)/\(^{(0)}\). We input this state into the DRL agent and obtain an action \(^{(0)}\), which indicates one mutator. We then apply this mutator to the current prompt structure and obtain an updated jailbreaking prompt \(^{(1)}\). We then feed this new prompt \(^{(1)}\) to the target LLM, obtain its response \(^{(0)}\), and compute the reward for \(^{(0)}\). We iterate this process until any of two predefined termination conditions is met. The first condition is when the maximum time step \(T=5\) is reached, and the second is when the agent's reward at time step \(t\), denoted as \(r^{(t)}\), is higher than a threshold \(=0.7\). The agent's policy network will be trained to maximize the accumulated reward along the process.

During the testing phase, we start with the trained agent \(_{}\) and the prompt structures \(_{}\) generated during training. Given an unseen question, we first select one prompt structure from \(_{}\) and apply our agent to modify the selected structure. We will terminate the process either when the agent finds a successful structure or reaches the maximum time limit. Here, we query GPT-4 to decide whether the target LLM's response answers the harmful question, i.e., whether the attack succeeds. Note that we do not use this metric as the reward during training in consideration of computational efficiency. If the attack fails, we will select another structure from \(_{}\) and repeat the process. We will try at most \(K\) structures for each question and deem the whole attack as a failure if all of the trials fail. We report \(K\) we use for different models in Appendix D.1.

### Technical Details

RL formulation.We formulate our system as a Markov Decision Process (MDP) \(=(,,,,)\). \(:\) is the state transition function, \(: R\) is the reward function, and \(\) is the discount factor. The goal of the agent is to learn an optimal policy \(_{}\) to maximize the expected total reward \([_{t=0}^{T}^{t}r^{(t)}]\) during the process. Note that \(\) is unknown and we need to apply model-free RL training methods [37; 36; 47].

**State.** We use a text encoder \(\)'s hidden representation of a jailbreaking prompt \(^{(t)}\) as the state of the next step, i.e., \(^{(t+1)}\). Specifically, the text encoder is a pre-trained XLM-RoBERTa model [59; 9] with a transformer-based architecture .

**Action.** We design 5 mutators, including _rephrase_, _crossover_, _generate_similar_, _shorten_ and _expand_. Here, all the mutators require another pre-trained LLM (denoted as the helper model) to conduct the mutation. See Tab. 4 for more details about each mutator. Our agent outputs a categorical distribution over the five mutators and samples from it to determine the action of each time step during training. Then the mutator will be applied to the current prompt structure to generate the next prompt.

**Reward.** Given a target LLM's response \(_{i}^{(t)}\), we compare it with the reference answer \(}_{i}\) of the same harmful question \(_{i}\) to calculate the reward. \(}_{i}\) is the response from an unaligned language model to \(_{i}\). Specifically, we employ the same text encoder \(\) that is used to get state representation to extract the hidden layer representation of both responses. We then calculate the cosine similarity between them as the reward

\[r^{(t)}=((_{i}^{(t)}),(}_{i }))=_{i}^{(t)})(}_{i})}{\| (_{i}^{(t)})\|\|(}_{i})\|}\,. \]

A high cosine similarity indicates the current response of the target LLM is an on-topic answer to the original harmful question. Note that although there may be multiple valid \(}_{i}\), it is unnecessary to identify all of them as we only use reference answers during policy training.

**Agent architecture and training algorithm.** Our agent is a simple Multi-layer Perceptron classifier that maps the state into the action distribution. We customize the state-of-the-art algorithm: proximal policy optimization (PPO)  algorithm to train our agent. The PPO algorithm designs the following surrogate objective function for policy training

\[_{}\,_{(^{(t)},^{(t)}) _{_{}}}[(((^{(t)}| ^{(t)})}{_{_{}}(^{(t)}|^{(t)} )},1-,1+)A^{(t)},(^{(t)}|^{(t)})}{_{_{}}(^{(t)}|^{(t)})}A^{(t)} )]\,, \]

where \(\) is a hyper-parameter and \(A^{(t)}\) is an estimate of the advantage function at time step \(t\). A common way to estimate advantage function is: \(A^{(t)}=R^{(t)}-V^{(t)}\), where \(R^{(t)}=_{k=t+1}^{T}^{k-t-1}r^{(k)}\) is the discounted return and \(V^{(t)}\) is the state value at time step \(t\). We remove \(V^{(t)}\) and directly use the return \(R^{(t)}\) as the optimization target. This is because an inaccurate approximation of \(V^{(t)}\) will harm the agent's efficacy rather than reduce the variance. See Appendix B.1 for the full training algorithm.

## 4 Evaluation

### Attack Effectiveness and Efficiency

**Dataset.** We select the widely-used AdvBench dataset , which contains 520 harmful questions. We randomly split it into a 40%/60% training/testing set. We select the 50 most harmful questions from the testing set based on their toxicity scores  (denoted as _Max50_).

**Baselines.** We choose three black-box jailbreaking attacks: GPTFUZZER , PAIR , Cipher , a gray-box attack AutoDAN , and a white-box attack GCG . GPTFUZZER and AutoDAN are genetic method-based attacks, PAIR and Cipher  are in-context learning-based attacks. We use their default setups and hyper-parameters (More details are in Appendix C.2).

**LLMs.** First, we select five open-source LLMs: Llama2-7b-chat, Llama2-70b-chat , Vicuna-7b, Vicuna-13b , and Mixtrl-8x7B-Instruct , and one commercial LLM: GPT-3.5-turbo . Note that some works explore adding a post-filter to filter out harmful content . Following existing attacks , we do not consider such mechanisms for the target LLMs. Second, we use GPT-3.5-turbo as the helper model to conduct the mutation. Third, the unaligned model we use to generate reference answers for the harmful question is an unaligned version of Vicuna-7b .

**Design and metrics.** Among the selected methods, RLbreaker and GPTFUZZER have distinct training and testing phases. We use the training set to train these methods and evaluate them on the testing set. For the other methods, we directly run and evaluate them on the testing set.

We leverage four metrics for the attack effectiveness evaluation: keyword matching-based attack success rate (KM.), cosine similarity to the reference answer (Sim.), a harmful content detector's prediction result (Harm.), and GPT-4's judgment result (GPT-Judge). We calculate the average cosine similarity (Sim.) for all testing questions compared to reference answers provided by an unaligned model using Eqn. (2). To further validate the efficacy of our method beyond Sim., we employ GPT-Judge to assess response relevancy, a metric commonly used by existing methods . GPT-Judge can filter out the false negatives introduced by Sim, providing a more accurate result. Specifically, we compute the percentage of testing questions that GPT-4 deems the response from the target LLM is answering the question. We directly adopt the judgment prompt from Guo et al. , as they show that GPT-Judge has a higher correlation with human annotations, providing a more reliable measure of attack effectiveness. The details of the GPT-Judge prompt are in Appendix C.4. KM. and Harm. evaluate whether the target LLM refuses to answer the question and whether the responses contain harmful content, respectively. For KM., we calculate the percentage of testing questions that pass the keyword matching, i.e., none of the keywords in Tab. 5 appears in the target LLM's response. For Harm., we give the target LLM's response to the detector as input and calculate the percentage of testing questions whose prediction result is 1, i.e., the detector deems this response contains harmful contents. We mainly use Sim. and GPT-Judge as the metrics as KM. and Harm. tend to introduce high false positives, see more details in Appendix D.1.

We use two efficiency metrics: the total run time for generating the jailbreaking prompt for all questions in the testing set (Total) and per question prompt generation time (Per-Q). Regarding the total running time, for a fair comparison, we set the upper bound for the total query times of the target LLM as 10,000. For RLbreaker and GPTFUZZER, 10,000 would be the upper bound for training and testing. We only consider the responses deemed as successes by the GPT-Judge to compute the Per-Q time.

**Results.** From Tab. 1, we can first observe that RLbreaker consistently achieves the highest GPT-Judge score across all models and the highest Sim. on the Llama2-70b-chat and GPT-3.5, demonstrating the superior ability of RLbreaker to bypass strong alignment in various models. In contrast, although guided by gradients, the white-box method GCG still low performance in jailbreaking very large models. We suspect this is because GCG directly searches for tokens as jailbreaking prompts, which has low effectiveness for models with a large search space. Furthermore, RLbreaker outperforms genetic-based methods (AutoDAN and GPTFUZZER), validating the effectiveness of having a DRL agent for guided search instead of random search via genetic methods. In addition, in-context learning-based methods (PAIR and Cipher) show limited effectiveness compared to RLbreaker, demonstrating the limitations of in-context learning in continuously refining the jailbreaking prompts.

Notably, RLbreaker significantly outperforms the baselines on the _Max50_ dataset. This further validates our RL agent's ability to refine jailbreaking prompt structures against difficult questions. Note that although RLbreaker does not surpass AutoDAN and GPTFUZZER in similarity on the Mixtrl model, the margin is very small. We also note that RLbreaker outperforms these two methods in GPT-Judge by a notably large margin. As discussed in Section 5, the target LLM may actually respond to the questions but they are different from the reference answer, resulting in a relatively low Sim. but high GPT-Judge score. We report the efficiency metrics in Appendix D.1. The result shows that RLbreaker does not introduce notable additional computational costs over existing methods.

### Resiliency against Jailbreaking Defenses

**Setup and design.** As discussed in Section 2, existing jailbreaking defenses can be categorized as input mutation-based defenses [26; 4; 45; 22], and output filtering-based defenses [17; 29; 62]. We select three defenses in this experiment. For input mutation-based defenses, we choose rephrasing and perplexity . Perplexity-based defense calculates the perplexity score of the input prompts using a GPT-2 model and rejects any input prompts whose perplexity score is higher than a predefined threshold (30 in our experiment). Given that rephrasing every input prompt and then feeding it into the target LLM is computationally expensive, we instead set "rephrasing" as a system instruction (i.e., "Please rephrase the following prompt then provide a response based on your rephrased version, the prompt is:"). This method combines the rephrasing and question into the same query, which is more efficient than dividing them into two continuous queries. We also select the SOTA output filtering-based defense: RAIN . It introduces a decoding strategy to encourage the target LLM to generate harmless responses for potential jailbreaking queries. We select three target models: Llama2-7b-chat, Vicuna-7b, and Mixtrl-8x7B-Instruct. We run these three defenses against the RLbreaker and three baseline attacks and report the Sim. and GPT-Judge as the metric. We do not add Cipher and GCG, because GCG performs poorly and Cipher has a similar mechanism and performance as PAIR. Note that existing work  also proposes masking out tokens in the input prompts as

   Target LLM &  &  &  \\  Metric & Sim. &  &  &  & Sim. &  \\  Dataset & Full & Max50 & Full & Max50 & Full & Max50 & Full & Max50 & Full & Max50 & Full & Max50 \\  RLbreaker & **0.7964** & **0.7761** & **0.8250** & **0.4000** & 0.7340 & 0.7381 & **1.6000** & **1.0000** & **0.7341** & **0.7112** & **0.3688** & **0.3200** \\ AutoDAN & 0.6814 & 0.6944 & 0.1466 & 0.6000 & 0.7739 & **0.7832** & 0.6750 & 0.7200 & N/A & N/A & N/A & N/A \\ GPTFUZZER & 0.6974 & 0.6836 & 0.1500 & 0.0400 & **0.7859** & 0.7691 & 0.5688 & 0.2600 & 0.6856 & 0.2203 & 0.1031 & 0.0800 \\ PAIR & 0.7007 & 0.7054 & 0.0094 & 0.0000 & 0.6836 & 0.6566 & 0.1500 & 0.0200 & 0.6818 & 0.6600 & 0.0006 & 0.0400 \\ Cipher & 0.6967 & 0.7013 & 0.1094 & 0.1200 & 0.6564 & 0.6713 & 0.1843 & 0.2800 & 0.7035 & 0.6968 & 0.3000 & 0.2800 \\ GCG & 0.6032 & 0.5949 & 0.0656 & 0.0000 & 0.6220 & 0.6051 & 0.1188 & 0.0800 & N/A & N/A & N/A & N/A \\   

Table 1: RLbreaker vs. five baseline attacks in jailbreaking effectiveness on three target models. All the metrics are normalized between 0 and 1 and a higher value indicates more successful attacks. “N/A” means not available. The results of the other three models and the left two metrics are shown in Appendix D.1.

   Target LLM &  &  &  \\  Metric & Sim. &  & Sim. &  & Sim. &  \\   & RLLbreaker & **0.5184** & **0.4188** & **0.4000** & **0.4000** & **0.4000** & **0.4000** & **0.4000** & **0.4000** & **0.4000** \\  & GPTFUZZER & 0.5184 & 0.4172 & 0.4172 & 0.4172 & 0.4172 & 0.4172 & 0.4172 & 0.4172 & 0.4172 \\  & GPTFUZZER & 0.4041 & 0.4172 & 0.4161 & 0.2031 & 0.4172 & 0.4172 & 0.4172 & 0.4172 \\  & PUL & 0.4091 & 0.0131 & 0.7732 & 0.4172 & 0.4172 & 0.4172 & 0.4172 & 0.4172 \\  & RLLbreaker & **0.5409** & **0.4000** & **0.4000** & **0.4000** & **0.4000** & **0.4000** & **0.4000** \\  & RLLbreaker & **0.5409** & **0.4000** & **0.4000** & **0.4000** & **0.4000** & **0.4000** & **0.4000** \\  & Mixtrl-8x7x7B & 0.415 & 0.4172 & 0.4172 & 0.4172 & 0.4172 & 0.4172 & 0.4172 & 0.4172 \\  & PUL & 0.4096 & 0.4000 & 0.4000 & 0.4000 & **0.4000** & **0.4000** & **0.4000** \\  & Mixtrl-8x7xB & 0.416 & 0.4161 & 0.4161 & 0.4161 & 0.4161 & 0.4161 & 0.4161 \\  & RLLbreaker & **0.5409** & **0.5400** & **0.4000** & **0.4000** & **0.4000** & **0.4000** & **0.4000** \\  & Mixtrl-8x7xB & 0.5409 & 0.5409 & 0.5409 & 0.5409 & 0.

defenses against jailbreaking attacks. We do not include masking because it will potentially change the original semantics of input prompts, causing the target LLM to reply with irrelevant responses.

**Results.** Tab 2 shows the resiliency of RLbreaker and baselines against three SOTA defenses. We can observe that all methods' performances drop after applying the defenses; however, RLbreaker consistently surpasses the three selected baselines across most of the models and metrics. This demonstrates RLbreaker's superior resiliency compared to the baseline attacks. In particular, perplexity can almost fully defend against several baselines, while our attack still maintains a significant level of effectiveness. This is because RLbreaker generates more natural jailbreaking prompts and thus achieves low perplexity scores. Furthermore, we also find that rephrasing is overall the most effective defense, as it can almost entirely defend AutoDAN and PAIR on the Vicuna-7b and Mixtal-8x7B. RLbreaker still has a decent performance against this attack, further validating its resiliency.

### Attack Transferability

**Setup and design.** We study the transferability of our trained agents, i.e., whether an agent trained for one target LLM can still be effective for other LLMs. Specifically, we follow the same setup in Section 4.1 to train a jailbreaking agent for one LLM, denoted as the source model. Then, we apply the trained policy and the prompt structures generated using the source model to launch jailbreaking attacks on other models using our testing set. We determine a successful jailbreaking based on the termination conditions specified for each baseline. We select the same LLMs as Section 4.2 for this experiment. We use each LLM as the source model and test the trained policy against two other models. We use the KM. and GPT-Judge as the metric. For comparison, we also evaluate the transferability of GPTFUZZER, AutoDAN, and PAIR. Similar to Section 4.2, we do not include Cipher and GCG in this experiment.

**Results.** From Tab. 3, we can observe that RLbreaker demonstrates a much better transferability than the baseline approaches, particularly for the GPT-Judge metric. Notably, when RLbreaker is applied from Llama2-7b-chat to Vicuna-7b, it outperforms the baselines--even those specifically optimized for Vicuna-7b as the target model. This enhanced performance is attributed to the stronger alignment of Llama2-7b-chat, which compels our agent to learn more sophisticated policies. As such, jailbreaking target models with weaker alignment becomes much easier. This also indicates that RLbreaker can learn more advanced jailbreaking policies/strategies against models with stronger alignment. Similar trends are observed in AutoDAN, where testing on Vicuna-7b and Llama2-7b-chat demonstrates that prompts generated for Llama2-7b-chat successfully transfer to Vicuna-7b but not vice versa.

### Ablation Study and Sensitivity Test

In these experiments, we use an open-source model Llama-7b-chat and a commercial model GPT-3.5-turbo as the target model, as well as GPT-Judge as the metric.

**Ablation study.** We evaluate three key designs: (1) our RL agent in RLbreaker, (2) our cosine similarity-based reward design, and (3) our mutator-based action design. To verify (1), we introduce two variations: a random agent, which selects actions randomly (denoted as "Random A."), and an LLM agent, which queries an open-source model (Vicuna-13b) to determine the action to take (denoted as "LLM A."). Given that these two variations do not require training, we directly apply them to the testing set and evaluate their attack effectiveness. To verify (2), we keep our action design but use keyword matching as the reward. At time step \(t\), the agent is assigned a reward \(r^{(t)}=1\) if none of the keywords in the keyword list in Tab. 5 are presented in target LLM's response \(^{(t)}\), and \(r^{(t)}=0\) otherwise (denoted as "KM."). To verify (3), we keep our reward design and change the action of the agent as selecting tokens from the vocabulary [19; 64; 43]. Specifically, at every time step, the agent directly appends one token to an input harmful question until a maximum length is reached. We use the cosine similarity as our reward (denoted as "Token"). See Appendix D.4 for more details about the LLM agent and the token-level action design.

**Sensitivity test.** We test RLbreaker against the variation on three key hyper-parameters: the threshold \(\) in our reward function, the helper model, and the text encoder \(\) (Section 3.3). Specifically, we first fix the helper model and vary \(\) from 0.65 to 0.75 and 0.80. We record the GPT-Judge on the testing set. Second, we also perform the sensitivity check of our helper LLM, fixing \(=0.7\) and varying it from GPT-3.5-turbo (default choice) to Llama2-7b-chat. Finally, we change the text encoder \(\) from bge-large-en-v1.5 to all-MiniLM-L6-v2 and report the attack performance accordingly.

**Results.** From Fig. 2(a), we observe a notable decrease in the GPT-Judge score, when the RL agent in our approach is replaced with either a random agent or an LLM. This degradation in performance underscores the critical role of our RL agent in deciding the proper jailbreaking strategies given different questions. Additionally, the agent with token-level action space is ineffective in jailbreaking attacks, validating the significance of our mutator-based action space. Furthermore, employing keyword matching as a reward design introduces performance degradation, highlighting the value of our reward design in measuring answer relevance and enabling a dense reward.

Fig. 2(b) shows that our attack is still effective with different choices of \(\), demonstrating its insensitivity to the variations of \(\). Furthermore, Fig. 2(c) shows that changing our helper model from GPT-3.5-turbo to Llama2-7b-chat does not significantly affect the effectiveness of our attacks, indicating that RLbreaker is not overly dependent on the capabilities of the helper model to maintain high attack effectiveness. Finally, Fig. 2(d) demonstrates changing the text encoder from Bge-large-en-v1.5 to all-MiniLM-L6-v2 introduces minor effects on our attack's effectiveness. Overall, this experiment shows the insensitivity of RLbreaker to the changes in key hyper-parameters.

We also compare RLbreaker's performance with and without learning a value network, to validate our customized learning algorithm design. Due to the space limit, we put this experiment in Appendix D.2.

## 5 Discussion

**RLbreaker for better LLM alignment.** The ultimate goal of this work is to identify the blind spots in LLM alignments and improve the alignment accordingly. To this end, RLbreaker can serve as an automatic method to scan target LLM and collect datasets for future alignment. The generated jailbreaking prompts can be used to fine-tune the model by instructing the model to refuse these prompts, which is similar to adversarial training in deep neural networks .

**Limitations and future work.** First, we can expand our action space to incorporate recent jailbreaking attacks. For instance, recent studies show that misspelling sensitive words or inserting meaningless characters in harmful questions , or encryption  are useful jailbreaking strategies. We can add those operators into our action space such that our agent can learn more diverse jailbreaking strategies. Second, our reward function may introduce false negatives, i.e., the target LLM's responses may answer the question but differ from the reference answers. We will explore improved strategies that can reduce such false negatives without introducing too much computational overhead. Third, our future work will explore extending our RL-based jailbreaking attack framework to multi-modal models, e.g., vision language models including LLaVa  and MiniGPT4 , and video generation models [41; 2], or to detect complicated watermarks in LLM [25; 74]. Finally, we notice that there is an increasing trend of integrating complex AI agents with LLMs and RL [61; 55; 7; 67]. Our work can be taken as an initial exploration. We plan to include more advanced AI agents, adapting our methodologies to these increasingly sophisticated systems.

## 6 Conclusion

We introduce RLbreaker, a DRL-driven black-box jailbreaking attack. We model LLM jailbreaking as a searching problem and design a DRL agent to guide efficient search. Our DRL agent enables deterministic search, which reduces the randomness and improves the search efficiency compared to

Figure 2: Ablation study and sensitivity test results. The results of “token” are zeros.

existing stochastic search-based attacks. Technically speaking, we design specific reward function, actions, and states for our agents, as well as a customized learning algorithm. We empirically demonstrate that RLbreaker outperforms existing attacks in jailbreaking different LLMs, including the very large model, Llama-2-70B. We also validate RLbreaker's resiliency against SOTA defenses and its ability to transfer across different models. A thorough ablation study underscores the importance of RLbreaker's core designs, revealing its robustness to changes in critical hyperparameters. These findings verify DRL's efficacy for automatically generating jailbreaking prompts against LLMs.