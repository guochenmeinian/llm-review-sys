# A Foundation Model

for Zero-shot Logical Query Reasoning

 Mikhail Galkin\({}^{1}\), Jincheng Zhou\({}^{2,}\), Bruno Ribeiro\({}^{2}\), Jian Tang\({}^{3,4,5}\), Zhaocheng Zhu\({}^{3,6}\)

\({}^{1}\)Intel AI Lab, \({}^{2}\)Purdue University, \({}^{3}\)Mila - Quebec AI Institute,

\({}^{4}\)HEC Montreal, \({}^{5}\)CIFAR AI Chair \({}^{6}\)Universite de Montreal

Work done during the internship at Intel. Code: [https://github.com/DeepGraphLearning/ULTRA](https://github.com/DeepGraphLearning/ULTRA)

###### Abstract

Complex logical query answering (CLQA) in knowledge graphs (KGs) goes beyond simple KG completion and aims at answering compositional queries comprised of multiple projections and logical operations. Existing CLQA methods that learn parameters bound to certain entity or relational vocabularies can only be applied to the graph they are trained on which requires substantial training time before being deployed on a new graph. Here we present UltraQuery, the first foundation model for inductive reasoning that can zero-shot answer logical queries on _any_ KG. The core idea of UltraQuery is to derive both projections and logical operations as vocabulary-independent functions which generalize to new entities and relations in any KG. With the projection operation initialized from a pre-trained inductive KG completion model, UltraQuery can solve CLQA on any KG after finetuning on a single dataset. Experimenting on 23 datasets, UltraQuery in the zero-shot inference mode shows competitive or better query answering performance than best available baselines and sets a new state of the art on 15 of them.

Figure 1: Zero-shot query answering performance (MRR, higher is better) of a single UltraQuery model trained on one FB15k237 queries dataset compared to the best available baselines and ablated UltraQuery LP on 23 datasets. _EPFO_ is the average of 9 query types with \((,)\) operators, _Negation_ is the average of 5 query types with the negation operator \(()\). On average, a single UltraQuery model outperforms the best baselines trained specifically on each dataset. More results are presented in Table 2 and Appendix C.

Introduction

Complex logical query answering (CLQA) generalizes simple knowledge graph (KG) completion to more complex, compositional queries with logical operators such as intersection \(()\), union \(()\), and negation \(()\). Such queries are expressed in a subset of first-order logic (FOL) where existentially quantified \(()\)_variables_ and given _constants_ comprise _relation projections_ (or _atoms_), and logical operators combine projections into a logical query (graph pattern). A typical example of a logical query  is presented in Figure 2: \(?U. V:(,V)\ \ (,V)\ \ (V,U)\) where \(()\) is a relation projection, \(\) is a constant, and \(V\) is an existentially quantified variable.

Due to the incompleteness of most KGs, these logical queries cannot be directly solved by graph traversal algorithms. Consequently, CLQA methods have to deal with missing edges when modeling the projection operators. The vast majority of existing CLQA methods  predict missing edges by learning graph-specific entity and relation embeddings making such approaches transductive and not generalizable to other KGs. A few approaches  are able to generalize query answering to new nodes at inference time but still need a fixed relation vocabulary.

In this work, we focus on the hardest inductive generalization setup where queries and underlying graphs at inference time are completely different from the training graph, _i.e._, both entities and relations are new. Furthermore, we aim at performing CLQA in the _zero-shot_ setting with one single model. That is, instead of finetuning a model on each target dataset, we seek to design a unified approach that generalizes to any KG and query at inference time. For example, in Figure 2, the training graph describes academic entities with relations \(\), \(\), \(\)2 whereas the inference graph describes music entities with relations \(\)\(\) and \(\). The query against the inference graph \(?U:(\ ,U)\ \ ^{-1}(,U)\) involves both new entities and relations and, to the best of our knowledge, cannot be tackled by any existing CLQA method that learns a fixed set of entities or relation embeddings from the training graph.

**Contributions.** Our contributions are two-fold. First, none of the existing CLQA methods can generalize to query answering over new arbitrary KGs with new entities and relations at inference time. We bridge this gap by leveraging the recent progress in inductive KG reasoning  and devise UltraQuery, the first foundation model for CLQA that generalizes to logical queries on any arbitrary KG with any entity and relation vocabulary in the zero-shot fashion without relying on any external node or edge features. UltraQuery parameterizes the projection operator by an inductive graph neural network (GNN) and implements non-parametric logical operators with fuzzy logics . The pre-trained projection operator  does not learn any graph-specific entity nor relation embeddings thanks to the generalizable meta-graph representation of relation interactions, and therefore enables zero-shot generalization to any KG.

Second, in the absence of existing datasets for our inductive generalization setup, we curate a novel suite of 11 inductive query answering datasets where graphs and queries at inference time have new entity and relation vocabularies. Experimentally, we train a single UltraQuery model on one dataset and probe on other 22 transductive and inductive datasets. Averaged across the datasets, a single UltraQuery model outperforms by 50% (relative MRR) the best reported baselines in the literature (often tailored to specific graphs) on both EPFO queries and queries with negation.

Figure 2: The inductive logical query answering setup where training and inference graphs (and queries) have different entity and relation vocabularies. We propose a single model (UltraQuery) that zero-shot generalizes to query answering on any graph with new entity or relation vocabulary at inference time.

Related Work

**Complex Logical Query Answering.** To the best of our knowledge, there is no existing approach for generalizable and inductive query answering where the method is required to deal with new entities and relations at inference time.

Due to the necessity of learning entity and relation embeddings, the vast majority of existing methods like GQE , BetaE , ConE , MPQE  (and many more from the survey by Ren et al. ) are transductive-only and tailored for a specific set of entities and relations. Among them, CQD  and QTO  are inference-only query answering engines that execute logical operators with non-parametric fuzzy logic operators (_e.g._, product logic) but still require pre-trained entity and relation embedding matrices to execute relation projections (link prediction steps). We refer the interested reader to the comprehensive survey by Ren et al.  that covers query answering theory, a taxonomy of approaches, datasets, and open challenges.

A few models  generalize only to new entities by modeling entities as a function of relation embeddings. Gebhart et al.  apply the idea of cellular sheaves and harmonic extension to translation-based embedding models to answer conjunctive queries (without unions and negations). NodePiece-QE  trains an inductive entity encoder (based on the fixed vocabulary of relations) that is able to reconstruct entity embeddings of the new graph and then apply non-parametric engines like CQD to answer queries against new entities. The most effective inductive (entity) approach is GNN-QE  that parameterizes each entity as a function of the relational structure between the query constants and the entity itself. However, all these works rely on a fixed relation vocabulary and cannot generalize to KGs with new relations at test time. In contrast, our model uses inductive relation projection and inductive logical operations that enable zero-shot generalization to any new KG with any entity and relation vocabulary without any specific training.

**Inductive Knowledge Graph Completion.** In CLQA, KG completion methods execute the projection operator and are mainly responsible for predicting missing links in incomplete graphs during query execution. Inductive KG completion is usually categorized  into two branches: (i) inductive entity (inductive \((e)\)) approaches have a fixed set of relations and only generalize to new entities, for example, to different subgraphs of one larger KG with one set of relations; and (ii) inductive entity and relation (inductive \((e,r)\)) approaches that do not rely on any fixed set of entities and relations and generalize to any new KG with arbitrary new sets of entities and relations.

Up until recently, the majority of existing approaches belonged to the inductive \((e)\) family (_e.g._, GraIL , NBFNet , RED-GNN , NodePiece , A*Net , AdaProp ) that generalizes only to new entities as their featurization strategies are based on learnable relation embeddings.

Recently, the more generalizable inductive \((e,r)\) family started getting more attention, _e.g._, with RMPI , InGram , Ultra, and the theory of _double equivariance_ introduced by Gao et al.  followed by ISDEA and MTDEA  models. In this work, we employ Ultra to obtain transferable graph representations and execute the projection operator with link prediction over any arbitrary KG without input features. Extending our model with additional input features is possible (although deriving a single fixed-width model for graphs with arbitrary input space is highly non-trivial) and we leave it for future work.

## 3 Preliminaries and Problem Definition

We introduce the basic concepts pertaining to logical query answering and KGs largely following the existing literature .

**Knowledge Graphs and Inductive Setup.** Given a finite set of entities \(\) (nodes), a finite set of relations \(\) (edge types), and a set of triples (edges) \(=()\), a knowledge graph \(\) is a tuple \(=(,,)\). The simplest _transductive_ setup dictates that the graph at training time

  
**Method** & **Ind.** & \(e\) & \(\) & **Ind. Logical QPs** \\  Query2Box , BetaE  & ✘ & ✘ & Parametric, ✘ \\ CQD , FuzzQE , QTO  & ✘ & ✘ & Fuzzy, ✓ \\ GNN-QE , NodePiece-QE  & ✓ & ✘ & Fuzzy, ✓ \\ UltraQuery (this work) & ✓ & ✓ & Fuzzy, ✓ \\   

Table 1: Comparison with existing CLQA approaches. _Ind._ denotes inductive generalization to new entities _(e)_ and relations _(r)_. UltraQuery is the first inductive method the generalizes to queries over new entities and relations at inference time.

\(_{}=(_{},_{},_{})\) and the graph at inference (validation or test) time \(_{}=(_{},_{},_{})\) are the same, _i.e._, \(_{}=_{}\). By default, we assume that the inference graph \(_{}\) is an incomplete part of a larger, non observable graph \(_{}}\) with missing triples to be predicted at inference time. In the _inductive_ setup, in the general case, the training and inference graphs are different, \(_{}_{}\). In the easier inductive entity (_inductive_\((e)\)) setup tackled by most of the KG completion literature, the relation set \(\) is fixed and shared between training and inference graphs, _i.e._, \(_{}=(_{},,_{})\) and \(_{}=(_{},, _{})\). The inference graph can be an extension of the training graph if \(_{}_{}\) or be a separate disjoint graph (with the same set of relations) if \(_{}_{}=\). In CLQA, the former setup with the extended training graph at inference is tackled by InductiveQE approaches .

In the hardest inductive entity and relation (inductive \((e,r)\)) case, both entities and relations sets are different, _i.e._, \(_{}_{}=\) and \(_{}_{}=\). In CLQA, there is no existing approach tackling this case and our proposed UltraQuery is the first one to do so.

**First-Order Logic Queries.** Applied to KGs, a first-order logic (FOL) query \(q\) is a formula that consists of constants \(\) (\(\)), variables \(\) (\(\), existentially quantified), relation _projections_\(R(a,b)\) denoting a binary function over constants or variables, and logic symbols (\(\), \(\), \(\), \(\)). The answers \(A_{}(q)\) to the query \(q\) are assignments of variables in a formula such that the instantiated query formula is a subgraph of the complete, non observable graph \(}\). Answers are denoted as _easy_ if they are reachable by graph traversal over the incomplete graph \(\) and denoted as _hard_ if at least one edge from the non observable, complete graph \(}\) has to be predicted during query execution.

For example, in Figure 2, a query _Which band member of Dire Straits played guitar?_ is expressed in the logical form as \(?U:(,)^{-1 }(,U)\) as an _intersection_ query. Here, \(U\) is a projected target variable, \(\) and \(\) are constants, \(\) and \(\) are _relation projections_ where \(^{-1}\) denotes the inverse of the relation \(\). The task of CLQA is to predict bindings (mappings between entities and variables) of the target variable, _e.g._, for the example query the answer set is a single entity \(_{q}=\{(U,)\}\) and we treat this answer as an _easy_ answer as it is reachable by traversing the edges of the given graph. In practice, however, we measure the performance of CLQA approaches on _hard_ answers.

**Inductive Query Answering.** In the transductive CLQA setup, the training and inference graphs are the same and share the same set of entities and relations, _i.e._, \(_{}=_{}\) meaning that inference queries operate on the same graph, the same set of constants \(\) and relations. This allows query answering models to learn hardcoded entity and relation embeddings at the same time losing the capabilities to generalize to new graphs at test time.

In the inductive entity \((e)\) setup considered in Galkin et al. , the inference graph extends the training graph \(_{}_{}\) but the set of relations is still fixed. Therefore, the proposed models are still bound to a certain hardcoded set of relations and cannot generalize to any arbitrary KG.

In this work, we lift all the restrictions on the training and inference graphs' vocabularies and consider the most general, inductive \((e,r)\) case when \(_{}_{}\) and the inference graph might contain a completely different set of entities and relation types. Furthermore, missing links still have to be predicted in the inference graphs to reach _hard_ answers.

**Labeling Trick GNNs.** Labeling tricks (as coined by Zhang et al. ) are featurization strategies in graphs for breaking symmetries in node representations which are particularly pronounced in link prediction and KG completion tasks. In the presence of such node symmetries (_automorphisms_), classical uni- and multi-relational GNN encoders  assign different _automorphic_ nodes the same feature making them indistinguishable for downstream tasks. In multi-relational graphs, NBFNet  and A*Net  apply a labeling trick by using the indicator function \((h,v,r)\) that puts a query vector \(\) on a head node \(h\) and puts the zeros vector on other nodes \(v\). The indicator function does not require entity embeddings and such models can naturally generalize to new entities (while the set of relation types is still fixed). Theoretically, such a labeling strategy learns _conditional node representations_ and is provably more powerful  than node-level GNN encoders. In CLQA, only GNN-QE  applies NBFNet as a projection operator making it the only approach generalizable to the inductive \((e)\) setup  with new nodes at inference time. This work leverages labeling trick GNNs to generalize CLQA to arbitrary KGs with any entity and relation vocabulary.

## 4 Method

We aim at designing a single foundation model for CLQA on any KG in the zero-shot fashion, _i.e._, without training on a target graph. In the CLQA literature [19; 26; 25; 3; 43], it is common to break down query execution into a _relation projection_ to traverse graph edges and predict missing links, and _logical operators_ that model conjunction, disjunction, and union. The main challenge boils down to designing inductive projection and logical operators suitable for any entity and relation vocabulary.

### Inductive Relation Projection

The vast majority of CLQA methods are inherently transductive and implement relation projections as functions over entity and relation embeddings fixed to a certain KG vocabulary, _e.g._, with scoring functions from KG completion methods [19; 3; 5], geometric functions [26; 40], or pure neural methods [2; 34]. The only method inductive to new entities  learns relation embeddings and uses those as a labeling trick (Section 3) for a GNN that implements the projection operator.

As fixed relation embeddings do not transfer to new KGs with new relations, we adapt Ultra, an inductive approach that builds relation representations dynamically using the invariance of _relation interactions_, as the backbone of the relation projection operator thanks to its good zero-shot performance on simple KG completion tasks across a variety of graphs. Ultra leverages theoretical findings in multi-relational link prediction [6; 20] and learns relation representations from a _meta-graph_ of relation interactions3. The meta-graph includes four learnable edge types or meta-relations (_head-to-tail_, _tail-to-head_, _head-to-head_, _tail-to-tail_) which are independent from KG's relation vocabulary and therefore transfer across any graph. Practically, given a graph \(\) and projection query \((h,r,?)\), Ultra employs labeling trick GNNs on two levels. First, it builds a meta-graph \(_{r}\) of relation interactions (a graph of relations where each node is a unique edge type in \(\)) and applies a labeling trick to initialize the query node \(r\). Note that \(||||\), the number of unique relations is much smaller than number of entities in any KG, so processing this graph of relations introduces a rather marginal computational overhead. Running a message passing GNN over \(_{r}\) results in _conditional relation representation_ which are used as initial edge type features in the second, entity-level GNN. There, a starting node \(h\) is initialized with a query vector from the obtained relation representations and running another GNN over the entity graph (with a final sigmoid readout) returns a scalar score in \(\) representing a probability of each node to be a tail of a query \((h,r,?)\).

The only learnable parameters in Ultra are four meta-relations for the graph of relations and GNN weights. The four meta-relations represent structural patterns and can be mined from any

Figure 3: **(a)** Example of _ip_ query answering with UltraQuery: the inductive parametric projection operator (Section 4.1) executes relation projections on any graph and returns a scalar score for each entity; the scores are aggregated by non-parametric logical operators (Section 4.2) implemented with fuzzy logics. Intermediate scores are used for weighted initialization of relation projections on the next hop. **(b)** The multi-source propagation issue with a pre-trained link predictor for relation projection: pre-training on _1p_ link prediction is done in the single-source labeling mode (top) where only one query node is labeled with a non-zero vector; complex queries at later intermediate hops might have several plausible sources with non-zero initial weights (bottom) where a pre-trained operator fails.

multi-relational KG independent of their entity and relation vocabulary. GNN weights are optimized during pre-training. Since the model does not rely on any KG-specific entity or relation vocabulary, a single pre-trained Ultra model can be used as a zero-shot relation projection operator on any KG. Figure 3(a) illustrates the _intersection-projection_ query execution process where each projection step is tackled by the same inductive projection operator with initialization depending on the start anchor node or intermediate variables.

**The multi-source propagation issue.** While it is tempting to leverage Ultra pre-trained on multiple KG datasets for relation projection, there is a substantial distribution shift (Figure 3(b)) between KG completion and CLQA. Specifically, KG completion is a special case of relation projection where the input always contains a single node. By comparison, in multi-hop complex queries, several likely nodes might have high intermediate scores and will be labeled with non-zero vectors leading to the _multiple sources_ propagation mode where a pre-trained operator is likely to fail. To alleviate the issue, we experimentally study two strategies: (1) short fine-tuning of the pre-trained projection operator on complex queries (used in the main UltraQuery model), or (2) use the frozen pre-trained operator and threshold intermediate scores setting all scores below \(0<k<1\) to zero (denoted as UltraQuery LP). The insight is to limit the propagation to one or a few source nodes, thereby reducing the discrepancy between training and test distributions.

### Inductive Logical Operations

Learnable logical operators parameterized by neural nets in many CLQA approaches [19; 26; 40; 2] fit a particular embedding space and are not transferable. Instead, we resort to differentiable but non-parametric _fuzzy logics_ that implement logical operators as algebraic operations (_t-norms_ for conjunction and _t-conorms_ for disjunction) in a bounded space \(\) and are used in several neuro-symbolic CLQA approaches [3; 43; 4; 5; 36]. UltraQuery employs fuzzy logical operators over _fuzzy sets_\(^{||}\) as the relation projection operator assigns a scalar in range \(\) for each entity in a graph. The choice of a fuzzy logic is often a hyperparameter although van Krieken et al.  show that the _product logic_ is the most stable. In product logic, given two fuzzy sets \(,\), conjunction is element-wise multiplication \(\) and disjunction is \(+-\). Negation is often implemented as \(-\) where \(\) is the _universe_ vector of all ones. For second- and later \(i\)-th hop projections, we obtain initial node states \(_{v}\) by weighting a query vector \(_{i}\) with their probability score \(x_{v}\) from the fuzzy set of a previous step: \(_{v}=x_{v}_{i}\).

### Training

Following existing works [25; 43], UltraQuery is trained on complex queries to minimize the binary cross entropy loss

\[=-_{q}|}_{a_{q}} p(a|q)- _{q}|}_{a^{} _{q}}(1-p(a^{}|q)) \]

where \(_{q}\) is the answer to the query \(q\) and \(p(a|q)\) is the probability of entity \(a\) in the final output fuzzy set. UltraQuery LP uses a frozen checkpoint from KG completion and is not trained on complex logical queries.

## 5 Experiments

Our experiments focus on the following research questions: (1) How does a single UltraQuery model perform in the zero-shot inference mode on unseen graphs and queries compared to the baselines? (2) Does UltraQuery retain the quality metrics like _faithfullness_ and identify easy answers reachable by traversal? (3) How does the multi-source propagation issue affect the performance?

### Setup and Datasets

**Datasets.** We employ 23 different CLQA datasets each with 14 standard query types and its own underlying KG with different sets of entities and relations. Following Section 3, we categorize the datasets into three groups (more statistics of the datasets and queries are provided in Appendix A):* _Transductive_ (3 datasets) where training and inference graphs are the same (\(_{train}=_{inf}\)) and test queries cover the same set of entities and relations: FB15k237, NELL995 and FB15k all from Ren and Leskovec  with at most 100 answers per query.
* _Inductive entity_\((e)\) (9 datasets) from Galkin et al.  where inference graphs extend training graphs (\(_{train}_{inf}\)) being up to 550% larger in the number of entities. The set of relations is fixed in each training graph and does not change at inference making the setup inductive with respect to the entities. Training queries might have more true answers in the extended inference graph.
* _Inductive entity and relation_\((e,r)\) (11 datasets): we sampled a novel suite of WikiTopics-QA datasets due to the absence of standard benchmarks evaluating the hardest inductive setup where inference graphs have both new entities and relations (\(_{train}_{inf}\)). The source graphs were adopted from the WikiTopics datasets , we follow the _BetaE setting_ when sampling 14 query types with at most 100 answers. More details on the dataset creation procedure are in Appendix A.

**Implementation and Training.**UltraQuery was trained on one FB15k237 dataset with complex queries for 10,000 steps with batch size of 32 on 4 RTX 3090 GPUs for 2 hours (8 GPU-hours in total). We initialize the model weights with an available checkpoint of Ultra reported in Galkin et al. . Following the standard setup in the literature, we train the model on 10 query types and evaluate on all 14 patterns. We employ _product t-norm_ and _t-conorm_ as non-parametric fuzzy logic operators to implement conjunction \(()\) and disjunction \(()\), respectively, and use a simple \(1-x\) negation. For the ablation study, UltraQuery LP uses the same frozen checkpoint (pre-trained on simple _lp_ link prediction) with scores thresholding to alleviate the multi-source propagation issue (Section 4.1). More details on all hyperparameters are available in Appendix B.

**Evaluation Protocol.** As we train an UltraQuery model only on one FB15k237 dataset and run zero-shot inference on other 22 graphs, the inference mode on those is _inductive_\((e,r)\) since their entity and relation vocabularies are all different from the training set.

As common in the literature [25; 27], the answer set of each query is split into _easy_ and _hard_ answers. Easy answers are reachable by graph traversal and do not require inferring missing links whereas hard answers are those that involve at least one edge to be predicted at inference. In the rank-based evaluation, we only consider ranks of _hard_ answers and filter out easy ones and report filtered Mean Reciprocal Rank (MRR) and Hits@10 as main performance metrics.

Other qualitative metrics include: (1) _faithfullness_, _i.e._, the ability to recover _easy_ answers reachable by graph traversal. Here, we follow the setup in Galkin et al.  and measure the performance of training queries on larger inference graphs where the same queries might have new true answers; (2) the ROC AUC score to estimate whether a model ranks easy answers higher than hard answers - we compute ROC AUC over _unfiltered_ scores of easy answers as positive labels and hard answers as negative. (3) Mean Absolute Percentage Error (MAPE)  between the number of answers extracted from model's predictions and the number of ground truth answers (easy and hard combined) to estimate whether CLQA models can predict the cardinality of the answer set.

**Baselines.** In transductive and inductive \((e)\) datasets, we compare a single UltraQuery model with the best reported models trained end-to-end on each graph (denoted as _Best baseline_ in the experiments): QTO  for 3 transductive datasets (FB15k237, FB15k, and NELL995) and GNN-QE  for 9 inductive \((e)\) datasets. While a single UltraQuery model has 177k parameters, the baselines are several orders of magnitude larger with a parameters count depending on the number of entities and relations, _e.g._, a QTO model on FB15k237 has 30M parameters due to having 2000\(d\) entity and relation embeddings, and GNN-QE on a reference FB 175% inductive \((e)\) dataset has 2M parameters. For a newly sampled suite of 11 inductive \((e,r)\) datasets, we compare against the edge-type heuristic baseline introduced in Galkin et al. . The heuristic selects the candidate nodes with the same incoming relation as the last hop of the query. More details on the baselines are reported in Appendix B

### Main Experiment: Zero-shot Query Answering

In the main experiment, we measure the zero-shot query answering performance of UltraQuery trained on a fraction of complex queries of one FB15k237 dataset. Figure 1 and Table 2 illustrate the comparison with the best available baselines and ablated UltraQuery LP model on 23 datasets split into three categories (transductive, inductive \((e)\), and inductive \((e,r)\)). For each dataset, we measure the average MRR on 9 EPFO queries with projection, intersection, and union operators, and 5 negation queries with the negation operator, respectively.

Averaged across 23 datasets, UltraQuery outperforms available baselines by relative 50% in terms of MRR and Hits@10 on EPFO and 70% on negation queries (_e.g._, 0.31 vs 0.20 MRR on EPFO queries and 0.178 vs 0.105 on negation queries). The largest gains are achieved on the hardest inductive \((e,r)\) datasets where the heuristic baseline is not able to cope with the task. On inductive \((e)\) datasets, UltraQuery outperforms the trainable SOTA GNN-QE model on larger inductive inference graphs and performs competitively on smaller inductive versions. On transductive benchmarks, UltraQuery lags behind the SOTA QTO model which is expected and can be attributed to the sheer model size difference (177k of UltraQuery vs 30M of QTO) and the computationally expensive brute-force approach of QTO that materializes the whole (\(\)) 3D tensor of scores of all possible triples. Pre-computing such tensors on three datasets takes considerable space and time, _e.g._, 8 hours for FB15k with heavy sparsification settings to fit onto a 24 GB GPU. Still, UltraQuery outperforms a much larger QTO model on the FB15k dataset on both EPFO and negation queries. The graph behind the NELL995 dataset is a collection of disconnected components which is disadvantageous for GNNs.

We note a decent performance of UltraQuery LP trained only on simple _1p_ link prediction and imbued with score thresholding to alleviate the multi-source message passing issue described in Section 4.1. Having a deeper look at other qualitative metrics in the following section, we reveal more sites where the issue incurs negative effects.

### Analysis

Here, we study four aspects of model performance: the effect of the multi-source message passing issue mentioned in Section 4.1, the ability to recover answers achievable by edge traversal (_faithfullness_), the ability to rank easy answers higher than hard answers, and the ability to estimate the cardinality of the answer set.

**The multi-source message passing effect.** The pre-trained Ultra checkpoint used in UltraQuery LP is tailored for singe-source message passing and struggles in the CLQA setup on later hops with several initialized nodes (Table 2). Training UltraQuery on complex queries alleviates this issue as

    &  &  &  &  \\   &  &  &  &  &  &  &  &  \\   & **MRR** & **10.10** & **MRR** & **10.10** & **MRR** & **10.10** & **MRR** & **10.10** & **MRR** & **10.10** & **MRR** & **10.10** & **MRR** & **10.10** & **MRR** & **10.10** & **MRR** & **10.10** \\  Best baseline & 0.014 & 0.020 & 0.004 & 0.007 & **0.328** & **0.469** & **0.176** & **0.297** & **0.468** & **0.683** & **0.299** & **0.409** & 0.106 & 0.276 & 0.105 & 0.173 \\  UltraQuery 0-shot & **0.280** & 0.380 & **0.193** & **0.288** & 0.312 & **0.467** & 0.139 & 0.262 & 0.481 & 0.517 & 0.240 & 0.352 & **0.309** & **0.432** & **0.178** & **0.286** \\ UltraQuery LP 0-shot & 0.268 & **0.409** & 0.104 & 0.181 & 0.277 & 0.441 & 0.098 & 0.191 & 0.232 & 0.476 & 0.150 & 0.263 & 0.279 & 0.430 & 0.107 & 0.195 \\ UltraQuery LP no. & 0.227 & 0.331 & 0.000 & 0.138 & 0.246 & 0.370 & 0.008 & 0.167 & 0.281 & 0.417 & 0.127 & 0.223 & 0.242 & 0.367 & 0.088 & 0.161 \\   

Table 2: Zero-shot inference results of UltraQuery and ablated UltraQuery LP on 23 datasets compared to the best reported baselines. UltraQuery was trained on one transductive FB15k237 dataset, UltraQuery LP was only pre-trained on KG completion and uses scores thresholding. The _no thrs_ version does not use any thresholding of intermediate scores (Section 4.1). The best baselines are trainable on each transductive and inductive \((e)\) dataset, and the non-parametric heuristic baseline on inductive \((e,r)\) datasets.

Figure 4: Mitigation of the multi-source message passing issue (Section 4) with UltraQuery: while UltraQuery LP (pre-trained only on 1p link prediction) does reach higher 1p query performance (center right), it underperforms on negation queries (center left). UltraQuery adapts to the multi-source message passing scheme and trades a fraction of 1p query performance for better averaged EPFO, _e.g._, on the _3i_ query (right), and negation queries performance. More results are in Appendix C.

shown in Figure 4, _i.e._, while _lp_ performance of UltraQuery LP is higher, the overall performance on EPFO and negative queries is lacking. In contrast, UltraQuery trades a fraction of _lp_ single-source performance to a much better performance on negative queries (about \(2\) improvement) and better performance on many EPFO queries, for example, on _3i_ queries. Besides that, we note that the zero-shot performance of both UltraQuery models does not deteriorate from the increased size of the inference graph compared to the baseline GNN-QE.

**Recovering easy answers on any graph.**_Faithfullness_ is the ability of a CLQA model to return _easy_ query answers, _i.e._, the answers reachable by edge traversal in the graph without predicting missing edges. While faithfullness is a common problem for many CLQA models, Figure 5 demonstrates that UltraQuery almost perfectly recovers easy answers on any graph size even in the zero-shot inference regime in contrast to the best baseline. Simple score thresholding does not help UltraQuery LP to deal with complex queries as all easy intermediate nodes have high scores above the threshold and the multi-source is more pronounced.

**Ranking easy and hard answers.** A reasonable CLQA model is likely to score easy answers higher than hard ones that require inferring missing links . Measuring that with ROC AUC (Figure 5), UltraQuery is behind the baseline due to less pronounced decision boundaries (overlapping distributions of scores) between easy and hard answers' scores. Still, due to scores filtering when computing ranking metrics, this fact does not have a direct negative impact on the overall performance.

**Estimating the answer set cardinality.** Neural-symbolic models like GNN-QE and QTO have the advantage of estimating the cardinality of the answer set based on the final scores without additional supervision. As shown in Figure 5, UltraQuery is comparable to the larger and trainable QTO baseline on FB15k237 (on which the model was trained) as well as on other datasets in the zero-shot inference regime. Since cardinality estimation is based on score thresholding, UltraQuery LP is susceptible to the multi-source propagation issue with many nodes having a high score and is not able to deliver a comparable performance.

**Varying the number of graphs in training.** Figure 6 and Table 3 report the inductive inference CLQA performance depending on the number of KGs in the training mixture. The original UltraQuery was trained on queries from the FB15k237. In order to maintain the zero-shot inductive inference setup on 11 inductive \((e,r)\) and 9 inductive \((e)\) datasets, we trained new model versions on the rest of BetaE datasets, that is, UltraQuery 2G combines FB15k237 and NELL995 queries (trained for 20k steps), UltraQuery 3G combines FB15k273, NELL995, and FB15k queries (trained for 30k steps). The most noticeable improvement of 2G and 3G versions is the increased MRR and Hits@10 on EPFO queries (9 query

Figure 5: Qualitative analysis on 9 inductive \((e)\) and 3 transductive datasets averaged across all 14 query types. **Faithfullness, MRR (left):** UltraQuery successfully finds easy answers in larger inference graphs and outperforms trained GNN-QE baselines. **Ranking of easy vs hard answers, ROC AUC (center):** zero-shot inference methods slightly lag behind trainable GNN-QE due to assigning higher scores to hard answers. **Cardinality Prediction, MAPE (right):** UltraQuery is comparable to a much larger trainable baseline QTO. In all cases, UltraQuery LP is significantly inferior to the main model.

Figure 6: Average MRR (left) and Hits@10 (right) of 9 inductive \((e)\) and 11 inductive \((e,r)\) CLQA datasets for EPFO and negation queries depending on the number of graphs in the training mix.

[MISSING_PAGE_FAIL:10]