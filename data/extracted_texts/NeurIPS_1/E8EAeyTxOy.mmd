# InfiBench: Evaluating the Question-Answering Capabilities of Code Large Language Models

Linyi Li

Simon Fraser University

linyi_li@sfu.ca

&Shijie Geng

ByteDance Inc & Rutgers University

sg1309@rutgers.edu

 Zhenwen Li &Yibo He &Hao Yu &Ziyue Hua &Guanghan Ning

ByteDance Inc & Siwei Wang

ByteDance Inc & Tao Xie

Key Lab of HCST (PKU), MOE

taoxie@pku.edu.cn

Equal contribution.

###### Abstract

Large Language Models for code (code LLMs) have witnessed tremendous progress in recent years. With the rapid development of code LLMs, many popular evaluation benchmarks, such as HumanEval, DS-1000, and MBPP, have emerged to measure the performance of code LLMs with a particular focus on code generation tasks. However, they are insufficient to cover the full range of expected capabilities of code LLMs, which span beyond code generation to answering diverse coding-related questions. To fill this gap, we propose **InfiBench**, the **first large-scale freeform question-answering (QA) benchmark for code** to our knowledge, comprising 234 carefully selected high-quality Stack Overflow questions that span across 15 programming languages. InfiBench uses four types of model-free automatic metrics to evaluate response correctness where domain experts carefully concretize the criterion for each question. We conduct a systematic evaluation for over 100 latest code LLMs on InfiBench, leading to a series of novel and insightful findings. Our detailed analyses showcase potential directions for further advancement of code LLMs. InfiBench is fully open source at [https://infi-coder.github.io/infibench](https://infi-coder.github.io/infibench) and continuously expanding to foster more scientific and systematic practices for code LLM evaluation.

## 1 Introduction

In recent years, Large Language Models (LLMs) have been revolutionizing the software development landscape , demonstrating exceedingly strong and comprehensive capabilities in comprehending, generating, debugging, and summarizing code . For example, code LLM-powered products like GitHub Copilot  reached millions of active users within just one year of their launch.

Alongside the huge success of proprietary LLMs such as GPT-3.5 / GPT-4  and Gemini , the development of open-source code LLMs2 has been advancing at an unprecedented fast pace. As of June 2024, the Hugging Face Open LLM Leaderboard  has cataloged over 3,300 submissions of such models.

can still fail as exemplified in Figure 2. (4) **Common benchmarks may be contaminated.** Some LLMs have unconventional high performance in common benchmarks and are suspected to have memorized benchmark-related data , obscuring the evaluation results. _Can we systematically and comprehensively evaluate code LLMs' abilities in challenging real-world usage scenarios?_

  Benchmark & Domain & \# Question & Evaluation & Data Source & Highest LLM Score \\  HumanEval  & Python Programming & 164 & Test Cases & Hand-Winten & 90.2\% \\ MMPP  & Python Programming & 974 & Test Cases & Hand-Winten & 81.1\% \\ APPS  & Python Programming & 10,000 & Test Cases & Conceptions & (or no report yet) \\ IDS-D00  & Python Programming & 1,000 & Test Cases & Surface Form Constraints & StackOverflow & / no report yet) \\ HumanEval++  & Python Programming & 164 & Augmented Test Cases & HumanEval & 86.6\% \\ HumanEvalPredN  & Repair, Evolution, Generation to 6 Languages & 2,952 & Test Cases & HumanEval & 47.8\% \\ NLP  & Python Programming & 164 & Test Cases & Hand-Winten & 64.6\% \\ SVE-Bench  & Python Engineering & 2,294 & Test Cases & Github & 22.0\% \\ SWE-Bench Verified  & Python Engineering / Repair & 500 & Test Cases & SWE-bench & 45.20\% \\  InfBench &  &  &  &  &  \\  &  &  &  \\  

Table 1: **Comparison between InfiBench and common existing benchmarks.** Existing benchmarks weigh heavily on code generation, unit-test-based evaluation, and major programming languages. InfiBench processes a much higher diversity to reflect real-world code LLMs’ usage scenarios. More discussion in Section 2.6.

Figure 1: **InfiBench overview**. We construct the InfiBench benchmark by filtering high-quality and diverse question posts from Stack Overflow and annotating question-level evaluation criteria with domain experts. With an model-free automatic evaluation framework, we evaluate over 100 latest code LLMs (one of the most extensive evaluations for code LLMs to the best of our knowledge), leading to several insightful findings.

Figure 2: A challenging question paraphrased from Stack Overflow where GPT-4 fails to answer.

To answer the question, we introduce InfiBench, a systematic benchmark for evaluating the free-form question-answering capabilities of code LLMs. As the first benchmark of its kind, the core principle of InfiBench aims to accurately represent how developers interact with and utilize such models in real-world scenarios. To achieve this, InfiBench comprises 234 questions that are carefully selected and proportionally filtered from the natural high-quality question distribution of Stack Overflow, without any constraints on topics, programming languages, question types, or answer forms. As a result, the curated 234 questions span 15 programming languages and 5 major areas: _front-end_, _back-end_, _DS&ML_ (_data science and machine learning_), _mobile and desktop_, and _ITOps_ (_information technology operations_).

Question diversity comes with evaluation challenges for two reasons. (1) Lack of metric. Unlike code generation or multiple-choice benchmarks, which can be evaluated through standardized methods like unit testing, there is no universal metric for response correctness for free-form questions. (2) Challenges with model-based evaluation. Model-based evaluations such as those involving GPT-4 are not only costly but also raise concerns about privacy and bias.

To mitigate the evaluation challenges, InfiBench includes an automatic evaluation framework that integrates four types of _model-free_ metrics: keyword matching, blank filling, unit testing, and dialogue similarity. For each question, we invite industry domain experts to paraphrase the prompt, select the most appropriate metric, and write down the concrete criteria using domain-specific knowledge, with highly-voted answers from Stack Overflow as a reference. These questions and evaluation criteria are then cross-validated to ensure correctness and objectiveness and further calibrated to improve consistency across languages. Human experiments show that InfiBench evaluation aligns with humans better than LLM-based evaluation, achieving 85.1% agreement rate compared to 77.8% achieved by GPT-4o-based evaluation.

As a novel and systematic benchmark disjoint with existing ones in terms of both forms and data sources, we believe that InfiBench is an ideal tool to measure existing code LLMs objectively. Hence, we conduct a systematic evaluation for **over 100 code LLMs** spanning both proprietary and open-source worlds using the InfiBench framework -- the latest and most extensive evaluation for code LLMs to the best of our knowledge. Our evaluation leads to several insightful findings: (1) On InfiBench, GPT-4 achieves a score of \(70.64\%\), being far from perfect but still far exceeding the most capable open-source models as of June 2024. On the other hand, GPT3.5 is surpassed by a few open-source models. (2) At similar model sizes, coding LLMs are usually visibly stronger than general LLMs; finetuning LLMs are usually visibly stronger than base LLMs. (3) The performance differences between different model families are huge, where one model could surpass another with less than 1/10 parameters, highlighting the importance of training data quality and techniques. (4) The scaling law is empirically verified for open-source models with fewer than 40B parameters, but not for those with more, where a turning point emerges. InfiBench is fully open source under CC BY-SA 4.0 license and continuously expanding3, including both the benchmark and Hugging-Face-compatible evaluation tools. All resources are available at [https://infi-coder.github.io/infibench](https://infi-coder.github.io/infibench).

## 2 Benchmark Creation

InfiBench is created from a high-quality subset of Stack Overflow questions up until June 14, 2023. In this section, we describe the data curation process and the evaluation framework in detail.

### Data Curation

Stack Overflow is a question-and-answer website for developers with more than 24 million registered users as of June 2024 . Since the website is a large collection of natural and diverse coding questions from real-world developers, we believe that questions from Stack Overflow can effectively evaluate code LLM's capabilities in real-world usage scenarios.

The full Stack Overflow dataset contains 23.54 million question posts and 34.68 million answer posts. Each question post has a total view count. Each answer post is attached to a question and has a vote count. The question creator can choose one answer as officially accepted.

As we aim to create a benchmark where the correctness evaluation criteria are clear, we view the positively voted answers as an important reference source. Hence, we choose to keep only the questions that have at least three positively voted answers and an officially accepted answer, which turn out to be 1,090,238 questions. For these one million questions, we choose to keep questions that are frequently viewed and relatively new. To fulfill this criterion, we draw a scatter plot of these \(\) 1 million questions, plotting the number of days since their creation until June 14, 2023 (data collection end-date) on the \(x\)-axis against the logarithm of their view counts on the \(y\)-axis. As shown in Figure 3, we empirically determine to keep questions that lie above the line connecting \((0,5)\) and \((3000,15.5)\), resulting in a subset of 17,402 questions.

Utilizing the mandatory question tags of these questions, we then manually construct a tag tree that covers the 200 most frequent tags, enabling us to identify the top programming languages and areas for 14,330 out of these 17,402 questions. These questions are from 24 programming languages, with each language being categorized into one primary area among the five (front-end, back-end, DS&ML, mobile and desktop, and ITOps). Lastly, we exclude 6 programming languages that either describe data or are domain-specific: JSON, regex, Markdown, YAML, CSV, and SQL. As a result, we compile 13,854 questions that serve as the _initial seed set_.

### Sampling

Based on a user study of developers' demand from our organization, we allocate the tentative area quota to be 25%, 25%, 25%, 15%, and 10% for front-end, back-end, DS&ML, mobile and desktop, and IT Ops, respectively. Inspired by HumanEval size and considering the labelling labor cost, we set 200 questions as the target benchmark size. Hence, the tentative size quotas by area are 50, 50, 50, 30, and 20 respectively. We then proportionally distribute the area quotas to language quotas based on the frequency of each language in the initial seed set. However, we observe that following this rule, certain languages such as CSS and C/C++ end up with fewer than 10 questions, which may yield unreliable language-level sub-score, so, for these languages, we set their quotas to 10.

As a result, we derive the _tentative_ question quota for each language as shown in Table 2, which sums up to 270 questions. After determining the tentative question quota, we uniformly sample from the initial seed set a roughly two times larger pool for the domain expects to select and annotate.

    &  &  &  &  \\   & & \# Questions & \% Area & \# Questions & \# Questions & \% Questions & \% Questions & \# Area & \% Area \\  & & & & Quota & Quota & Quota & Quota & Quota & Quota \\   & Javascript & 4912 &  & 44 & 44 & 18.80\% &  &  \\  & CSS & 87 & & & & & & & \\  & HTML & 600 & & 10 & 9 & 3.85\% &  &  \\   & Java & 930 & & 18 & 17 & 7.26\% &  &  \\  & CM & 629 & & 12 & 12 & 5.13\% & \\  & PHP & 462 & & 10 & 9 & 3.85\% &  &  \\  & Go & 117 & 18.71\% & 10 & 9 & 3.85\% &  &  \\  & Ruby & 71 & & 10 & 10 & 4.27\% & \\  & Rust & 96 & & 10 & 10 & 4.27\% & \\  & C/C++ & 287 & & 10 & 10 & 4.27\% & \\  DS \& ML & Python & 2779 & & 47 & 47 & 2.00\% &  &  \\  & R & 184 & & 10 & 9 & 3.85\% & & \\   & Dart & 1562 & & 19 & 19 & 8.12\% & 19 & 8.12\% \\  & Kotlin & 383 & & 10 & & & & \\  & Swift & 551 & & 10 & & & & \\  & VBA & 16 & & 9 & & & & \\  IT Ops. & Bash & 188 & 1.36\% & 21 & 19 & 8.12\% & 19 & 8.12\% \\   Total & & 13854 & 100.0\% & 270 & 234 & 100.00 \% & 234 & 100.00\% \\   

Table 2: **Infibench data statistics by area and language**. We uniformly sample a subset from the initial seed set (see Section 2.1) according to the area quota (see Section 2.2) for domain experts to select questions and annotate the correctness criterion to construct the benchmark.

Figure 3: Scatter plot of filtered Stack Overflow questions. Questions above the orange line kept.

### Human Annotation

We recruited five domain experts inside our company to create the benchmark, each in charge of one area. The annotation process is composed of three steps:

* **Step 1: Question Selection and Type Annotation.** Domain experts select high-quality questions from the inspecting set and annotate the question type to be one of the four: code completion, code debugging, config and environment debugging, and knowledge question-answering.
* **Step 2: Prompt Paraphrasing.** Domain experts paraphrase and simplify the original question body into succinct and explicit instructions. We include this step for two main purposes: (1) Reduce domain gap. From user-shared conversations collected from ShareGPT, we observe that when interacting with code LLMs, users tend to provide short and direct instructions like "Fix problem..." and "Debug code...". However, when posting Stack Overflow questions, users tend to be lengthy with courtesy words. We ask the domain experts to paraphrase the question to code LLM user's style without changing the semantics. (2) Reduce the impact of memorization and data contamination. Some code LLMs may be trained or finetuned with Stack Overflow data. Paraphrasing the questions can help to mitigate the result advantages of these models. Benchmark results in Table 4 reveal the effectiveness of this step where copying Stack Overflow answers only achieves a 65.18% score. We defer further discussion in Section 2.5.
* **Step 3: Correctness Criterion Annotation.** Domain experts choose one or multiple evaluation metrics from our supported ones (see Section 2.4) and annotate the concrete criterion following a YAML schema. External files can be attached if needed, e.g., unit tests and reference answers.

**Calibration and Post-Filtering.** To improve annotation consistency and objectiveness, we introduce a few checkpoints for domain experts to read others' annotated cases, discuss them, and reach consensus for controversial cases. After the 270 tentative questions were annotated, we then ran an initial evaluation of all these questions on over 30 code LLMs. This initial evaluation helps us to identify questions whose criteria are incorrect or out of distribution. We filter out these questions and then remove all questions from Kotlin, Swift, and VBA languages since the questions in these languages are too few after filtering. After this calibration and post-filtering process, the final benchmark includes 234 questions spanning over 15 languages. Their statistics are shown in Table 2. As we can observe, compared to the population area distribution of high-quality Stack Overview questions (see "% Area Quota" column under "Initial Seed Set"), the area distribution of final benchmark questions (see "% Area Quota" column under "Final InfiBench Benchmark") is more balanced and less biased towards front-end, mobile, and desktop topics.

### Evaluation Criteria and Evaluation Framework

In response to the diversified questions, InfiBench evaluation framework integrates four types of model-free and automatic metrics as below. Domain experts choose one or multiple metric types along with their weights and concretize.

* **Keywords Matching.** Though the responses can be in diverse forms, for a significant portion of benchmark questions, we find that the existence of some keywords strongly determines the quality of the response. Domain experts can write rules that match keywords and regular expressions or construct recursive logical expressions on top of keyword-matching results. When multiple keywords exist, each matching result can have its weight in the final score.
* **Blank Filling.** For some questions, it is challenging to measure the correctness given the response uncertainty. In this case, domain experts can instruct the model to answer the question by following a given template and filling in the blanks in the template. The blanks can correspond to either natural language or code snippet. Then, similar to keywords matching, each blank can match potential keywords, regular expressions, or recursive logic expressions built upon matching results. This metric type tests not only the model's QA ability but also its instruction-following ability.
* **Unit Testing.** For code-intensive questions, we can follow existing benchmarks to evaluate response correctness by unit tests. For this type, domain experts may add more specifications in the prompt to allow for unit-test-based evaluation, such as specifications on function name, input arguments, and output format. Domain experts can further import the context setup and cleanup script.
* **Dialogue Similarity.** For natural-language-intensive questions, domain experts can extract and shorten the reference answers from Stack Overflow, and then use the ROUGE score  to evaluate the response similarity with reference answers. The ROUGE score was initially proposed andwidely used in evaluating the quality of text summarization and machine translation. To map the ROUGE score back to our benchmark scale, we allow domain experts to tune the mapping interval and scores within the interval are then linearly mapped to our score scale.

The example questions and corresponding criteria are illustrated in Figure 1. Detail statistics of metric type ratios, question type ratios, and prompt length are shown in Table 3.

Score Computation.We treat each question equally with one point each. Given 234 questions in the benchmark, the full score is 234, and we by default report the percentage score (achieved score divided by 234) unless otherwise noted. The one point for each question can be further decomposed into a few scoring points within each question. For example, a question may contain four keywords with weights 2, 1, 1, and 1 each. Then, matching each keyword can contribute to 0.4, 0.2, 0.2, and 0.2 points respectively to the final score.

Implementation.We have implemented an automated evaluation framework with Python, publicly available at [https://infi-coder.github.io/infibench](https://infi-coder.github.io/infibench). Specifically, for blank-filling evaluation, we use the longest common subsequence matching via dynamic programming to capture the filled blanks in the response. For unit-testing evaluation, we construct a runtime environment that supports the test execution for nine languages. We plan to integrate the framework into the Hugging Face Open LLM Leaderboard  to further ease the evaluation burden.

How does InfiBench Evaluation Align with Human?To evaluate the alignment between InfiBench evaluation and human expert evaluation, we randomly sample 100 questions without replacement from the benchmark and select three strong LLMs to generate responses: GPT-4-0613, GPT-3.5-turbo, and Mistral Codestral-22b. For each question, we randomly choose two out of these three model responses to construct response pairs, resulting in 100 response pairs \(=\{(A_{i},B_{i}):1 i 100\}\). For each response pair \((A,B)\), we use InfiBench, GPT-4o, and human expert to evaluate into four outcomes: \(A\) is more correct than \(B\) (\(A>B\)); \(B\) is more correct then \(A\) (\(B>A\)); both \(A\) and \(B\) are correct (\(A B\)); both \(A\) and \(B\) are incorrect (\(A B\)). _Our purpose is to evaluate how InfiBench evaluation aligns with humans, specifically when compared to the widely-used LLM-as-a-judge (i.e., model-based evaluation) ._ The concrete grading criteria is as below:

* InfiBench gives a score between \(\) for each response in the pair. If the score difference in the pair is larger than 0.2, we label the outcome to be \(A>B\) or \(B>A\) respectively; otherwise, if the maximum score among the two is larger than 0.5, we label the outcome to be \(A B\); otherwise, we label the outcome to be \(A B\).
* For GPT-4o evaluation, we deploy the prompting template from LLM-Blender [18, Appendix E] and trigger GPT-4o for grading the four outcomes. We enhance the reliability of the comparison by switching \(A\) and \(B\) and prompting GPT-4o twice. We record the preference only when a consistent preference exists.
* For human evaluation, we recruit human annotators who came up with the criteria to label the comparison preference since they are familiar with the questions and have strong expertise. Annotators have no access to the evaluation results of InfiBench and GPT-4o, nor which source model generates the response. Annotators were instructed to directly label each pair with the four outcomes.

We defer the consensus matrices between InfiBench/GPT-4o and human annotators along with more findings in Appendix C. If we only count the cases where both InfiBench/GPT-4o and humans have clear preferences, the agreement rate between InfiBench and humans is 85.1%, and the agreement rate between GPT-4o and humans is 77.8%. Hence, _the InfiBench evaluation aligns with human experts better than the GPT-4o evaluation (with >80% confidence)_. We observe that the advantage of InfiBench comes from the ability to detect deceptive answers. some model responses pretend to be helpful with lengthy wording and hallucinations. GPT-4o is more likely to be cheated than InfiBench, which looks for key concepts that should exist in a helpful answer.

Table 3: InfiBench statistics.

### Mitigations on Memorization and Data Contamination

InfiBench is created from the publicly available Stack Overflow corpus to reflect real-world scenarios, and this corpus may already exist in the training set of some code LLMs (e.g., DeepSeek Coder  and StarCoder 2 ). Hence, some code LLMs may achieve a high score simply due to memorization. To mitigate this, we asked the domain experts to paraphrase every question as an essential step (see Section 2.3). Hence, copying either the highly voted answers or officially accepted answers of the original questions only achieves 65.18%, being far from perfect and inferior to GPT-4's 70.64%. Furthermore, code LLMs that use Stack Overflow data do not demonstrate significant advantages over those without. Hence, we deem the effect of contamination as small.

On the other hand, we release the post IDs of the source question posts of InfiBench. Hence, future LLM training could consider this benchmark to conduct deduplication and ablation studies on data contamination. Another usage of our benchmark is to evaluate retrieval-augmented (RAG) code LLMs where perfect retrieval from Stack Overflow and moderate adaptation should solve these questions, which we leave as future work.

### Comparison with Existing Benchmarks

In Table 1, we compare InfiBench with several existing benchmarks for code LLMs. As reflected in the table, InfiBench strongly complements existing benchmarks for code LLMs by (1) extending them beyond code generation to a wide range of real-world tasks, (2) diversifying them since InfiBench does not share the same source as existing ones, and (3) increasing the differentiation as an unsaturated benchmark. Related benchmarks are further illustrated in Section 5. On the other hand, the benchmark is limited in size due to the high cost of correctness criteria labelling, and we are continuously expanding the benchmark.

## 3 Evaluation and Leaderboard

We systematically evaluated over 100 code LLMs spanning both proprietary and open-source worlds on InfiBench. To the best of our knowledge, this is the most extensive evaluation for code LLMs.

Evaluation Protocol.We adopt best@10 as the main evaluation metric: 10 responses are sampled and evaluated for each question, then the best score per question is recorded and summed up. Throughout the evaluation, we set sampling temperature \(T=0.2\) and top \(p=0.9\).

Furthermore, we swept sampling parameters with GPT-4 and the detailed results are in Appendix G. In a nutshell, for maximizing the performance under best@10, the best parameters are \(T=1.0\) and \(p=0.9\), leading to a score of \(76.15\% 0.21\%\) (in comparison to \(70.64\% 0.82\%\) in our main setting \(T=0.2,p=0.9\)). In particular, the temperature \(T\) affects much and the effect of top \(p\) is minor. We decided to stick to the original parameters \(T=0.2\) and \(p=0.9\) in the main evaluation since this setting is more akin to the real-world scenario where user generates once with low temperature.

We design two system prompts (shown in Appendix H), one for normal questions and the other for open-ended questions with an additional sentence to encourage succinct responses. For generic models, we generate the prompt with "{system prompt}" format; for instruction-finetuned or chat models, we generate the prompt with their prompt templates.

For proprietary models, we evaluate the latest models from OpenAI (GPT-4, GPT-4o, etc), Anthropic (Claude 3), and Mistral AI (Mistral Small/Medium/Large) with API calling. When budget permits, we repeat each evaluation three times and report standard deviation. For open-source models, we download models from Hugging Face and evaluate them on an 8xA100 server with bigcode-evaluation-harness . When the model size is within 30B parameters, we repeat each evaluation three times and report the standard deviation. All raw model responses are available at [https://figshare.com/articles/dataset/InfiBench_Detail_Evaluation_Data/26104864](https://figshare.com/articles/dataset/InfiBench_Detail_Evaluation_Data/26104864). More details on the evaluation protocol are in Appendix E.

Leaderboard.In Table 4, we present aggregated InfiBench leaderboards by model family, model type, and model size. The full leaderboard is deferred to Appendix E due to space limit. The table includes scores from using the original Stack Overflow answer posts as reference. Results are also presented as a scatter plot in Figure 4, where normal models are shown as scatters with error bars,MoE models are shown as horizontal segments with error ranges connecting the activated parameters during inference and total parameters, and strong proprietary models are shown as horizontal lines.

In both tables and the figure, we classify LLMs by general/code and base/finetuned. The general LLMs are claimed to have strong capabilities beyond code, e.g., in various natural language tasks, while the code LLMs are exclusively optimized for the code domain. The base LLMs only went through the pretraining phase, while the finetuned LLMs are claimed to have instruction-following capabilities or are finetuned on instruction or human preference datasets.

## 4 Analysis and Discussion

**The best model so far, GPT-4, is still far from perfect, and open-source models are competitive but still far from GPT-4.** GPT-4 achieves the highest score 70.64% (interestingly, achieved by GPT-4-0613 instead of the more recent GPT-4o), then Claude 3 Opus with a score 63.89%, and then Codestral-22b  with a score 62.98% and deepseek-coder-33b-instruct  with a score 62.96%. The result implies that: (1) Noting that the full score is 100%, even the powerful GPT-4 is still far from perfect, which is in contrast to its \(\)90% HumanEval score. We inspect the score breakdown. For the two most frequent metric types, keywords matching and unit testing, GPT-4 achieves similar scores 66.61% and 76.00% respectively. For blank filling, the score is relatively lower at 58.08%. These scores imply that GPT-4 may still lack generic ability in answering diversified real-world questions related to code. When instructed to follow a given template to answer (blank filling), due to the more strict requirement and narrower solution space, its lack of capability is more pronounced. (2) There is still a visible gap between open-source models and GPT-4. The gap between the most powerful open-source model, Codestral-22b, and GPT-4 is roughly 8 points. On the other hand, noticing that GPT-3.5-turbo achieves 56.47%, the open-source model, Codestral-22b, is now reliably better than GPT-3.5-turbo with merely 22B parameters which is promising.

**Among open-source models, different models have various performances.** Figure 4 systematically visualizes the performance of different open-source models at diverse scales. Although there is a general tendency that larger models achieve higher scores, the scores among different models at a similar scale differ largely. For example, on scale 7B, the best-performing model is at around

Table 4: **Aggregated InfiBench leaderboards (best viewed zoomed in and in color). “Size” column records number of parameters. For MoE models, “total params. / params. activated during inference” is recorded. Bar colors stand for General Base, General Finetuned, Code Base, and Code Finetuned models respectively. Icon “\(}\)” stands for proprietary models otherwise open-source. Full leaderboard in Appendix E.**55%, pretty close to GPT-3.5, while the low-performing model stays at around 15%. Moreover, deepseek-coder-1.3b-instruct achieves 41.32% at 1.3B and surpasses a few models at scale 70B or 100B. Hence, though scaling matters, the training techniques and training data are equally important or even more, helping to reduce the required scale for achieving a certain score by more than 10\(\).

**Hard problems generalize their difficulties.** We rate the benchmark problem difficulty with five levels by how well GPT-4 and GPT-3.5-turbo answer them, as detailed in Appendix D. Example questions from each level are shown in Appendix I. We present the detail result table including the sub-score for each difficulty level in Appendix E. Interestingly, the trend is _highly consistent that sub-scores decrease along with the increase of problem level_. Specifically, hard problems for the most powerful model yet, GPT-4, are also generally hard for open-source models. These hard problems usually correspond to code generation with long and domain-specific context or challenging blank-filling questions since blank-filling is a specific task that rarely appears in training data before.

**Instruction finetuning is important for QA.** Among models of similar scales and the same family, we find that the best-performing ones almost always include an instruction-finetuning phase, such as deepseek-llm-67b-chat, deepseek-coder-33b-instruct, CodeLlama-34B-Instruct, and Qwen-18B-Chat. In contrast, the pretraining models, such as davinci-002 and phi models, usually perform poorly despite their strong performances in code generation benchmarks. Instruction-finetuning is also critical for other code domain tasks such as code generation. As shown in Appendix F.1 where we plot model scores in QA (measured by InfiBench) and code generation (measured by HumanEval pass@1), instruction-tuning generally improves both QA and code generation, but the improvement is usually more significantly on code generation but more moderately on QA. As a result, we suggest generalizing the instruction-finetuning data beyond simple coding problems to improve code LLMs. Indeed, our preliminary experiments show that, after fine-tuning with the decontaminated and sanitized Stack Overflow data, we improved InfiBench scores for Codellama-13b-Instruct from 46.37% to 60.74% and for mixtral-8x7B-Instruct from 55.55% to 62.61%.

**Some models may focus too much on code generation, especially the small ones.** As detailed in Appendix F.1, we observe that for large models (\(>\)30B) and top entries, InfiBench and HumanEval pass@1 scores coincide well. However, for smaller models, the score tendencies start to diverge, where some models are relatively stronger on InfiBench (Mixtral-8x7B-Instruct) and more are relatively stronger on HumanEval (Phi1, Phi2, gamma-7b,...). This phenomenon implies that a few models may be optimized too heavily on code generation benchmarks while ignoring the performance in generic code scenarios as represented by InfiBench, which in turn highlights the significance of free-form QA benchmarks like InfiBench in detecting capability imbalance in code LLMs.

Figure 4: **Scatter plot for all evaluated LLMs on InfiBench. \(x\)-axis is the model size in terms of number of parameters and \(y\)-axis is InfiBench score. Projected empirical scaling laws for both general and code models are drawn. Detail discussion in Section 4.**

**Code LLama models have unique characteristics.** We evaluated all Code Llama models . As shown in Table 5, we found finetuning on Python data improves on HumanEval but hurts InfiBench scores, while instruction finetuning usually improves InfiBench scores but may hurt HumanEval. As a side product, we found CodeLlama-70B may be overly safeguarded and denies answering some safe questions in InfiBench. More model-specific findings are presented in Appendix F.

**Code models and general models may exhibit different scaling laws, and open-source models scale well only within 40B yet.** In Figure 4, we use the top-performing code and general models at each scale respectively to regress and extrapolate model performance at larger scales. As shown, code models tend to have higher capabilities compared to general models of the same scale, though the gap shrinks for larger models. Hence, when the compute budget is heavily limited, training exclusively in the code domain could be more efficient for building strong code LLMs.

In Figure 4, both predicting curves are split into two segments, steep in the first segment and much flat in the second. Following the first segment, open-source models catch up with GPT-4 at around 50B scale. However, following the second segment, they may need to be at >300B scale to catch up. The finding contradicts the common scaling law  where a strong linear relationship between model scale and capability exists. The contradiction implies that very large open-source models (>40B) may fail to achieve the expected performance at their scales, or there is some non-trivial barrier when scaling the model beyond 40B, or the scaling law may change at such a large scale. We leave further investigation as the future work. Notably, after the release of InfiBench, Deepseek-coder-v2  was released as the largest code LLM to our knowledge in an MoE architecture with 236B total and 21B active parameters. On InfiBench, Deepseek-coder-v2 achieves 65.49%, setting the new baseline for open-source LLMs but still being inferior to GPT-4. More importantly, the score is within the predicted range of our empirical scaling law.

We defer dataset card and data accessibility details, discussion on limitations and societal impact, full leaderboard, additional findings, ablation studies, and data examples in appendices.

## 5 Related Work

Large language models  are transforming people's lives. In the coding domain, LLMs  are shown to be capable of completing a wide range of tasks such as code generation, debugging, and question-answering. Recently, code LLMs are booming. New models, including both proprietary  and open-source ones , emerge almost every month.

Benchmarks for code LLMs are developing, though at a relatively slower pace. Common benchmarks, e.g., APPS , MBPP , and HumanEval , focus on code generation and unit-test-based evaluation. Some efforts augment these benchmarks by language translation (e.g., Multilingual HumanEval , HumanEval-X ), test augmentation (e.g., HumanEval+ ), task generalization (e.g., HumanEvalPack ), and human rewriting (e.g., LBPP ). To systematically evaluate real-world problem solving, recently, SWE-bench , its filtered version SWE-bench Verified , and RepoBench  are proposed but they still primarily focus on code generation. Some general-purpose benchmarks, e.g., Arena-Hard , contain code-related questions, but rely on LLM to judge and do not provide domain-specific scores. CodeXGLUE  considers multiple coding capabilities beyond code generation, but replies on existing data sources. In contrast to these benchmarks, InfiBench benchmark is built for evaluating free-form question-answering ability in the code domain beyond code generation in an automated and model-independent way.

## 6 Conclusion

We proposed InfiBench, a systematic benchmark for evaluating the question-answering ability of code LLMs in real-world scenarios, to facilitate development and scientific evaluation of LLMs. InfiBench comprises 234 high-quality questions from Stack Overflow and supports automatic model-free evaluation. A comprehensive evaluation of over 100 code LLMs reveals several findings and takeaways. The benchmark is publicly available and continuously expanding.

    & Benchmark & Base & Python & Instrut \\   & HumanEval & 33.53\% & 34.54\% (+-1.95) & 34.85\% (+-1.25) \\  & InfiBench & 37.62\%, acc. & 28.95\%, acc. (+-4.75) & 33.15\%, acc. (+-2.47) \\   & HumanEval & 36.07\% & 44.25\%, acc. (+-7.33) & 42.78\% (+-7.07) \\  & InfiBench & 41.66\%, acc. & 41.31\%, acc. (+-0.35) & 46.37\%, acc. (+-1.71) \\   & HumanEval & 45.83\% & 53.74\% (+-1.95) & 44.12\% (+-1.73) \\  & InfiBench & 47.36\% & 43.13\% (+-2.35) & 50.45\% (+-3.07) \\   & HumanEval & 53.03\% & 57.97\% (+-1.95) & 69.83\% (+-1.85) \\  & InfiBench & 40.60\% & 40.29\% (-0.31) & 42.82\% (+-2.22) \\   

Table 5: Evaluation on eight models from the Code Llama  family showcases intense Python finetuning may hurt free-form QA ability, despite achieving higher HumanEval scores.