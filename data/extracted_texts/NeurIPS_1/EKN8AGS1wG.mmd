# Preference Alignment with Flow Matching

Minu Kim\({}^{1}\)1  Yongsik Lee\({}^{1*}\)  Sehyeok Kang\({}^{1}\)  Jihwan Oh\({}^{1}\)

Song Chong\({}^{1}\)  Se-Young Yun\({}^{1}\)

\({}^{1}\)KAIST AI

{minu.kim, dldydtlr93, kangsehyeok0329, ericoh929, songchong, yunseyoung1@kaist.ac.kr

Equal contribution. \({}^{}\)Corresponding authors.

###### Abstract

We present Preference Flow Matching (PFM), a new framework for preference alignment that streamlines the integration of preferences into an arbitrary class of pre-trained models. Existing alignment methods require fine-tuning pre-trained models, which presents challenges such as scalability, inefficiency, and the need for model modifications, especially with black-box APIs like GPT-4. In contrast, PFM utilizes flow matching techniques to directly learn from preference data, thereby reducing the dependency on extensive fine-tuning of pre-trained models. By leveraging flow-based models, PFM transforms less preferred data into preferred outcomes, and effectively aligns model outputs with human preferences without relying on explicit or implicit reward function estimation, thus avoiding common issues like overfitting in reward models. We provide theoretical insights that support our method's alignment with standard preference alignment objectives. Experimental results indicate the practical effectiveness of our method, offering a new direction in aligning a pre-trained model to preference. Our code is available at [https://github.com/jadehaus/preference-flow-matching](https://github.com/jadehaus/preference-flow-matching).

## 1 Introduction

Preference-based reinforcement learning (PbRL) has emerged as a groundbreaking approach with significant contributions to performance improvement , particularly in the realm of artificial intelligence where understanding and incorporating human preferences are crucial. Unlike traditional reinforcement learning, which struggles due to the absence of explicit reward functions or the infeasibility of defining comprehensive environmental rewards, PbRL leverages a variety of feedback forms from humans to guide the learning process. This class of PbRL method is often referred to as reinforcement learning from human feedback (RLHF) .

Despite their effectiveness, these methods necessitate fine-tuning pre-trained models to align with user preferences, introducing several challenges such as scalability, accessibility, inefficiency, and the need for model modifications. For instance, with black-box APIs like GPT-4 , customization based on user preferences is constrained due to restricted access to the underlying model. Moreover, even if fine-tuning were feasible, the large model size results in inefficient training and high resource consumption. Aligning black-box models with user preferences remains an under-explored area in research, despite its critical importance and growing demand.

In this line of research, we propose Preference Flow Matching (PFM), which redefines the integration of human preferences by directly learning a preference flow from the less preferred data to the more preferred ones. This direct modeling of preference flows allows our system to better characterizeand replicate the marginal distribution of the favored outcomes. We adopt a novel flow matching framework (Lipman et al., 2022), which is a simple, intuitive, yet relatively under-explored method for preference alignment. By simply adding a preference flow matching module to black-box models, PFM eliminates the need for fine-tuning the black-box model itself, providing a significant advantage.

Additionally, our method offers a highly robust approach for preference alignment, by circumventing the need for explicit or implicit reward function estimation. In typical RLHF scenarios, a model is initially trained to approximate a reward function based on human preferences. This reward model is then used to guide the policy learning process, aiming to align agent behaviors more closely with human preferences. However, this approach can introduce complexities and potential biases in translating human preferences into numerical rewards. In particular, learned reward models can often overfit the ground truth preference model, especially in the finite data regime (Azar et al., 2023). Recent advancements such as Direct Preference Optimization (DPO) (Rafailov et al., 2024) address the complexities of RLHF by eliminating the need for reward learning. However, these methods still inherently optimize for the reward model, and hence they are also susceptible to reward overfitting. In contrast, PFM directly learns preference flow, thereby removing the need for any reward model assumptions and resolving the challenges associated with reward model learning.

We prove both theoretically and empirically that our method is able to learn an object that is similar to the standard RLHF objectives, while being robust to the preference overfitting observed in traditional RLHF pipelines. We also demonstrate how we can further improve the quality of alignment via iterative flow matching, with theoretical guarantees. Experimentally, we find that while typical RLHF methods and DPO suffer from preference overfitting, our method can robustly align with preference and still achieve comparable performances.

## 2 Preliminaries

### Reinforcement Learning from Human Feedback (RLHF)

Reinforcement learning from human feedback generally begins with obtaining a pre-trained reference policy \(_{}\) that can generate samples \(y_{}(|x)\) given a context \(x\). For example, a context \(x\) could be a text prompt given by a user, and the sample \(y\) could represent an appropriate text response generated by the reference policy \(_{}\). We then collect a dataset of \(N\) preference pairs \(=\{(x_{i},y_{i}^{+},y_{i}^{-})\}_{i=1}^{N}\), where each \(x_{i}\) denotes the context, and each \(y_{i}^{+},y_{i}^{-}_{}(|x_{i})\) are generated responses to \(x_{i}\) and marked as good or bad samples, respectively. Here, we assume that the preference \(y_{i}^{+}>y_{i}^{-}\) is generated from a ground-truth reward \(r^{*}:X Y_{ 0}\), where \(X\) and \(Y\) are context space and response space, respectively. The goal of general RLHF is to recover an

Figure 1: Illustration of our PFM framework. In the typical RLHF scenarios (left), we first sample preference data from the supervised fine-tuned (SFT) reference model. A reward model is learned from the collected dataset, either implicitly (as in DPO) or explicitly. The reward model is then used to fine-tune the reference policy to obtain the final model. Our method directly learns the preference flow from the collected preference data, where the flow is represented as a vector field \(v_{}\) (middle). For inference, we again sample a point from the reference policy, and improve the quality of alignment by using the trained flow matching model, without the need of fine-tuning the existing reference model (right).

optimal policy \(^{*}\) such that

\[^{*}=*{argmax}_{}_{x}(_{y( |x)}r^{*}(x,y)-_{}( |x)_{}(|x)). \]

RLHF pipelines generally require reward learning. One of the most popular choices of the reward model is the Bradley-Terry model (Bradley and Terry, 1952), which assumes that the preference \(y^{+}>y^{-}\) is generated from the probability \((y^{+}>y^{-}|x)=(r^{*}(x,y^{+})-r^{*}(x,y^{-}))\), where \(\) is the logistic function. Under this model, the general RLHF framework learns the reward model \(r_{} r^{*}\) by minimizing the negative log-likelihood:

\[_{R}(;):=-_{(x,y^{+},y^{-})}r_{}(x,y^{+})-r_{}(x,y^{-}). \]

Once the reward model \(r_{}\) is trained, we then use it to optimize for (1) to obtain \(_{}^{*}\) using standard reinforcement learning algorithms.

There is also a class of _reward-free_ methods that eliminates the need of reward learning phase (Rafailov et al., 2024; Azar et al., 2023). Direct Policy Optimization (DPO) (Rafailov et al., 2024) is a representative reward-free method that optimizes for (1) directly without learning a reward model. Although being a reward-free method, DPO implicitly optimizes for the reward function as in (2), by replacing \(_{}(x,y)=(_{}(y|x)/_{}(y|x))\) as the implicit reward estimate.

### Flow Matching

Flow matching is a class of generative model, where given a prior distribution \(p_{0}\), we aim to model a target distribution \(p_{1}\) from \(p_{0}\). A key difference of the flow matching to the other generative models is that the prior \(p_{0}\) can be an _arbitrary_ distribution, (diffusion, for example, starts from a Gaussian prior \(p_{0}\)) and that the flow matching algorithm learns to modify the prior distribution \(p_{0}\) to the target distribution \(p_{1}\) with a neural network.

Throughout, we consider a pair of data distributions over \(^{d}\) with densities \(y^{-} p_{0}\) and \(y^{+} p_{1}\), possibly unknown (but able to sample). The flow matching considers the task of fitting a mapping \(f:^{d}^{d}\) that transforms \(p_{0}\) to \(p_{1}\), that is, if \(y^{-} p_{0}\), then \(f(y^{-}) p_{1}\). Inspired as in the motivation for the diffusion models, one can define a smooth time-varying vector field \(u:^{d}^{d}\) that defines an ordinary differential equation (ODE),

\[dy=u(t,y)dt \]

where we use the notation \(u(t,y)\) interchanably with \(u_{t}(y)\). Denote the solution of the above ODE by \((t,y)\) (or \(_{t}(y)\)) with initial condition \(_{0}(y)=y\). In other words, \(_{t}(y)\) is the point \(y\) transported along the vector field \(u\) from time \(0\) to \(t\). In order to obtain samples from the target distribution \(p_{1}\), we simply compute \(_{1}(y)\) where \(y p_{0}\). The integration map \(_{t}\) induces a pushforward measure \(p_{t}[_{t}]_{}(p_{0})\), which is the density of points \(y p_{0}\) transported via \(u\) from \(0\) to \(t\).

To train the vector field \(v_{}\) with a neural network that mimics the vector field \(u\) of our interest, we can solve for the conditional flow matching objective, as proposed by Lipman et al. (2022):

\[_{}()_{t,z q( ),y p_{t}(|z)}(\|v_{}(t,y)-u_{t}(y|z)\|^{2}) \]

where \(q(z)=(y^{-},y^{+})\) is some coupled distribution of samples \(y^{-},y^{+}\) and \(u_{t}(y|z)=y^{+}-y^{-}\) is a straight path from a source sample to a target sample. The conditional distribution \(q(z)\) can be chosen to be an independent coupling of source and target distribution \(q(z)=p_{0}(y^{-})p_{1}(y^{+})\)(Lipman et al., 2022), or the 2-Wasserstein optimal transport plan as proposed by Tong et al. (2023).

## 3 Preference Flow Matching

In this section, we describe how we can use flow matching to learn (human) preferences. In the first subsection, we illustrate our flow matching framework for learning preferences, and compare it with typical RLHF pipelines. Then in Section 3.2, we demonstrate our method in a simple 2-dimensional toy experiment. Finally in Section 3.3, we provide an extension of our framework that can iteratively improve the performance, with theoretical guarantees.

### Flow Matching for Preference Learning

Instead of trying to optimize for the unknown reward \(r^{*}\) or the preference probability model \((y^{+}>y^{-}|x)\), we simply learn a _flow_ from the marginal distribution of _less preferred data_\(p_{0}(y^{-}|x)\) to the marginal distribution of _more preferred data_\(p_{1}(y^{+}|x)\) by leveraging what is explicitly characterized in the collected preference data:

\[p_{0}(y^{-}|x) _{}(y^{-}|x)_{}(y|x) (y>y^{-}|x)dy \] \[p_{1}(y^{+}|x) _{}(y^{+}|x)_{}(y|x) (y^{+}>y|x)dy\] (6) \[=_{}(y^{+}|x)\,_{y_{}( |x)}(y^{+}>y|x) \]

In other words, we view that our collected data \(\) is in fact generated from each of the marginal distributions \(y^{-} p_{0}(|x)\) and \(y^{+} p_{1}(|x)\) obtained from \((y^{+}>y^{-}|x)\), respectively. Hence, following the conventions in the literature,  we define the flow matching objective for preference dataset \(\) as follows:

\[()=_{t,z,y p_{t}( |z)}(\|v_{}(t,y|x)-u_{t}(y|z)\|^{2}) \]

where we define the condition \(z=(x,y^{+},y^{-})\), conditional flow \(u_{t}(y|z)=y^{+}-y^{-}\), and the probability path \(p_{t}(y|z)=(y|ty^{+}+(1-t)y^{-},\ ^{2})\). Once we obtain the vector field \(v_{}\), we can improve upon the generated negative samples \(y^{-} p_{0}(|x)\) by solving (3) using an off-the-shelf numerical ODE solver  to obtain samples \(f(y^{-}) p_{1}\). Specifically, we start from a sample \(y^{-}\) with \(t=0\), and "flow" along the ODE trajectory using \(v_{}\) until \(t=1\), to arrive at the target \(y^{+}\). Detailed algorithm can be found in Algorithm 1. Notably, generating improved samples can be done without the need of fine-tuning the existing model, since we learn a separate vector field that transports negative samples from \(p_{0}\) to \(p_{1}\). Furthermore, we did not require any assumption for the probability model \((y^{+}>y^{-}|x)\), so our method can extend to general scenarios that do not adopt the Bradley-Terry model. Our method is outlined in Figure 1.

A careful reader might notice that for inference, we require negative samples \(y^{-}\) from the marginal distribution \(p_{0}\), to obtain aligned samples \(y^{+}\). However, this \(p_{0}\) is inaccessible during inference step, as we must first acquire preference label \(y^{+}>y^{-}\) for samples generated from \(_{}\). Instead, we simply start from \(y_{}\) as the starting point, and apply flow matching to obtain \(f(y) y^{+} p_{1}\). We emphasize that PFM can still robustly generate positive samples, if we assume non-deterministic preferences _i.e._, \((p_{1})(p_{0})\). We also empirically find that using \(_{}\) instead of \(p_{0}\) as the source distribution can produce comparable results in practical scenarios. Further details can be found in Appendix B.

### Illustrative Example: Preference Generated from 8-Gaussians Density

Here, we demonstrate how our method learns to improve generated samples to better align with the preference, in a simple 2-dimensional toy experiment. We consider a ground truth reward function generated from an 8-Gaussians density as illustrated in Figure 1(a). We then pre-train a Gaussian mixture model to obtain samples as in Figure 1(c). The pairwise preference labels are then generated using the ground truth 8-Gaussians reward function, as done in many existing preference-based reinforcement learning (PbRL) settings .

Once preference data are collected, we first learn a reward model \(_{}\) via (2). As can be seen in Figure 1(b), the learned reward model overfits in the unseen region, which causes the RLHF method to diverge (Figure 1(e)). DPO also fails to learn the correct preference, as can be seen in Figure 1(f). We note here that DPO is also subjective to the reward overfitting since DPO also implicitly learns to optimize for the reward using the Bradley-Terry model (2) .

However, PFM is free of such reward overfitting issues, as we do not optimize for the reward function using the Bradley-Terry model. Unlike other RLHF methods, our model robustly learns to align with the preference from the provided dataset (Figure 1(g)). Notably, our method does not try to overfit beyond the unseen region, since the learned target distribution from the flow matching model tries to mimic the distribution \(p_{1}(y^{+})\) of collected preferred samples. (Compare Figure 1(d) and Figure 1(g).)

### Improving Alignment with Iterative Flow Matching

As done in iterative variants of DPO (Xiong et al., 2023; Yuan et al., 2024), we can also further improve the quality of alignment with iterative flow matching. Specifically, upon obtaining a marginal distribution \(p_{1}\) by applying flow matching, we again collect a new preference data \(y^{-},y^{+}\) from the obtained marginal distribution \(p_{1}\) in (6). We repeat this process iteratively, by replacing the source distribution (which is \(_{}\) in the first step) with the marginal distribution \(p_{1}\) obtained in the latest iteration. This iterative process can be summarized as follows.

\[p_{0}^{(n)}(y^{-}|x)  p_{1}^{(n-1)}(y^{-}|x) p_{1}^{(n-1)}(y|x)(y> y^{-}|x)dy \] \[p_{1}^{(n)}(y^{+}|x)  p_{1}^{(n-1)}(y^{+}|x) p_{1}^{(n-1)}(y|x)(y^ {+}>y|x)dy, p_{1}^{(0)}=_{}, \]

where we denote \(p_{0}^{(n)}\) and \(p_{1}^{(n)}\) to be the source and target distribution of the flow matching model at the \(n\)-th iteration, respectively. By repeatedly marginalizing the distribution with respect to the preference \((y^{+}>y^{-}|x)\), we can effectively "narrow" the sampling distribution towards the outputs with higher preference probability. See Figure 1(h) for the results of the iterative method in our toy experiment. Note that even during this iterative approach, we leave the parameters of the pre-trained model \(_{}\) untouched, and only require sampling from this model throughout the whole process. Later in Section 4, we formally prove that the iterative method allows us to obtain a distribution class that maximizes the ground truth expected preference, and hence yields an optimal policy \(^{*}\) in (1) with \(=0\). See Theorem 4.2.

## 4 Theoretical Analysis of Preference Flow

In this section, we theoretically analyze why PFM framework can effectively learn to align with the preference. Interestingly, learning to generate samples from the marginal distribution \(p_{1}\) in (6) optimizes an objective that is similar to the goal of general RLHF in (1). Following the formulation provided by Azar et al. (2023), one can observe that the objective (1) is equivalent to the below form:

\[^{*}=*{argmax}_{}_{y}_{y ^{}_{}}^{-1}((y>y^{})) -_{}_{} \]

where \(^{-1}()=(/(1-))\) is the logit function, and we drop the conditional dependence of \(x\) for simplicity. Note DPO also optimizes for the same objective as in (11).

Figure 2: Comparison of RLHF, DPO, and PFM on a 2-dimensional toy experiment. We generate preference labels from a ground truth reward in (a) and a pre-trained Gaussian reference policy (c). Both the RLHF (e) and DPO (f) methods struggle to align with the preferences, due to the overfitted reward model (b), even with the presence of KL regularizer (\(=1\)). PFM is able to mimic the distribution of the positively-labeled samples (d), and therefore achieves the highest performance (g). Repeating PFM iteratively to the marginal samples can further improve the alignment with the preference (h).

Let us take a step back, and characterize the failure modes of the RLHF and DPO frameworks, by figuring out when these methods overfit the reward. Consider a case where the preferences are _deterministic_, _i.e._, \((y>y^{})=1\), so that \(y\) is always preferred to \(y^{}\). If we plug it into (11), we see that \(^{-1}((y>y^{}))+\). Therefore, the solution \(^{*}\) of (11) ends up overfitting for the preference likelihood, resulting in a weak or even null KL regularization, regardless of the size of \(\).

Although in the case where the preference is not deterministic, this phenomenon can still be pronounced in the finite data regime (Azar et al., 2023). Even if the true preference is strictly less than 1, we may have access to only a few data samples to estimate \((y>y^{}) 1\). This means that overfitting can be a critical issue in general, especially if the action space \(Y\) or the context space \(X\) is extremely large, as in the case of aligning large language models to human preferences.

In contrast, the PFM framework learns to generate a marginal distribution \(p_{1}\). One can show that this marginal is a solution for the optimization problem that is similar to the objective (11).

**Theorem 4.1** (**Characterization of the Marginal**).: _Let \(p_{1}\) denote the marginal distribution of the positively-labeled samples \(y^{+}\). Then the marginal distribution \(p_{1}\) obtained from the preference model \((y>y^{}|x)\) is an optimizer of the optimization problem_

\[p_{1}=*{argmax}_{}\,_{y}_{y^{}_{}}(y>y^{}) -_{}_{}. \]

We defer the proof to the Appendix C. Similar to the RLHF and DPO objective (11), the solution \(p_{1}\) of (12) drives the original distribution \(_{}\) towards the points where the preference probability \((y>y^{})\) is increasing. However, unlike the RLHF or DPO objectives, the objective (12) is bounded even in the deterministic case \((y>y^{})=1\), making it robust to reward overfitting.

Interestingly, maximizing the objective (12) is equivalent to minimizing the KL distance between the policy \(\) and the normalized preference score with a cross-entropy constraint:

\[p_{1}=*{argmin}_{}\,_{}(\|})-_{y}(_{}(y)) }(y)_{y^{}_{ }}((y>y^{})). \]

Hence, the objective pushes the policy \(\) to match the preference \(}\), while encouraging \(\) to align with the high-probability samples in \(_{}\). Since the constraint restricts the policy to the high-probability regions of \(_{}\) where the preference labels are collected from, our method is less prone to reward overfitting in the out-of-distribution samples due to the distribution shift in \(\) from \(_{}\)(Gao et al., 2023; Rafailov et al., 2024a). Though we find this information-theoretic formulation interesting, we leave further analysis to future work.

Despite its robustness, one may notice that the objective (12) is less flexible compared to the original objective, due to the fixed regularization constant with \(=1\). Below, we show that if we apply the iterative algorithm provided in Section 3.3, one can further reduce the KL regularization strength and obtain an optimal policy \(^{*}\) in (11) with \( 0\).

**Theorem 4.2** (**Convergence of Iterative Method**).: _Assume \(_{} L^{2}\) and \((y>y^{}) L^{2}\). Consider an iterative update of the marginal distribution \(p_{1}\) in (10). Then, the iteration converges to the uniform distribution of points \(y\) where the value \(_{y^{-}_{}}((y>y^{-}))\) is the largest, i.e.,_

\[p_{1}^{()} U(\{y:y*{argmax}_{y}\, _{y^{-}_{}}((y>y^{-}))\} ), \]

_where \(U\) stands for uniform distribution, and we drop the conditional dependence of \(x\) for simplicity._

We defer the proof to the Appendix C. Intuitively, the proof follows from the fact that the marginalization iteratively "narrows" down the distribution towards the outputs with higher preference. We note here that the \(L^{2}\) assumptions are generally valid in practical domains. See Appendix C.

## 5 Experimental Results

In this section, we conduct experiments to address the following questions: _Q1. Can PFM align generated samples from the black-box model with preference and achieve comparable results in practical tasks? Q2. Is PFM more beneficial than methods optimizing for explicit/implicit reward model?_ and _Q3. Is PFM more beneficial than naive add-on methods, e.g., separately training generative models to imitate preferred samples?_ To answer these questions, we validate our method in three domains: Conditional text and image generation, and offline reinforcement learning tasks.

### Conditional Image Generation

We first evaluate PFM on a conditional image generation task using the MNIST dataset . Specifically, we utilize a pre-trained DCGAN  generator as \(_{}\) and collect sample pairs from \(_{}(|x)\) conditioned on the digit labels \(x\{0,,9\}\). To construct preference datasets, we assign preferences to sample pairs according to the softmax probabilities of the labels from a LeNet . Then, we learn a PFM flow \(v_{}\) that transports \(y^{-}\) to \(y^{+}\) given a condition \(x\). More experimental details are provided in the Appendix D.

Figure 2(a) illustrates the generated samples from \(_{}\), and the rejected and preferred images are depicted in Figure 2(b) and Figure 2(c), respectively, where the values in parenthesis are the measured preference score. As shown in Figure 2(d), PFM achieves higher preference alignment and better sample quality than RLHF (Figure 2(e)) and DPO (Figure 2(f)) without fine-tuning \(_{}\). Moreover, PFM achieves nearly perfect alignment with the preferences after only two iterations (Figure 2(h)), demonstrating the effectiveness of iterative PFM.

### Conditional Text Generation

Next, we adopt a controlled (positive) sentiment review generation task. As done in Rafailov et al. , to perform a controlled evaluation, we adopt the pre-trained sentiment classifier as the preference annotator. We train a preference flow on randomly selected pairs of movie reviews \(y^{+},y^{-}\) from the IMDB dataset . For our PFM framework to be applied to variable-length inputs, we employ a T5-based autoencoder to work with fixed-sized embeddings. We adopt GPT-2 SFT model on the IMDB dataset as a reference model \(_{}\). We also compare our method with a RLHF (PPO) fine-tuned policy \(_{}\). See Appendix D for detailed experimental settings.

Below, we report the average preference score (from the classifier annotator) of 100 randomly generated review samples for each method. As shown in Table 1, PFM is able to improve the preference score of any baseline model to which it is attached. We emphasize here that our method requires relatively smaller training cost compared to the standard RLHF frameworks, even in the iterative settings. See Table 6 in Appendix D for the number of parameters that require training for each framework in tackling this task. PFM requires training a much smaller number of parameters (around 1.2%) while still achieving better performance. We also note here that instead of iteratively training the PFM module as described in Section 3.3, simply applying the same learned preference flow iteratively to the improved samples achieves the best performance. (See Appendix D.)

Figure 3: Comparison of RLHF, DPO, and PFM on a conditional MNIST image generation task. Numbers represent the preference score. PFM (d) demonstrates superior sample quality and preference alignment compared to RLHF (e) and DPO (f), where DPO collapses with a small size of \(\) (g). The iterative PFM with only two iterations (h) results in almost perfectly aligning with the preferences.

Interestingly, we observe that the distribution of the scores tends to shift more toward that of the preferred samples with an increasing number of PFM iterations. (See Figure 4.) This result aligns with our theoretical insights: the PFM framework learns to shift the source distribution (i.e., the distribution of the reference policy) toward the marginal distribution of the more preferred samples.

We also compute the win rate with GPT-4 evaluation. The results are summarized in Table 2. Both PFM and PPO fine-tuned policy excel the reference policy with win rates 100%. (First column of Table 2.) Furthermore, we observe that the iterative PFM with 5 iterations on the reference model outperforms the PPO fine-tuned policy. If PFM is added on top of the PPO fine-tuned policy, we observe near 100% win rates for both PPO + PFM and PPO + Iterative PFM.

### Offline Reinforcement Learning

Finally, we employ the D4RL (Fu et al., 2020) benchmark to assess the performance of PFM in reinforcement learning tasks. Following the prior works on the PbRL literature, we adopt trajectory-based preference alignment (Hejna et al., 2023; Kim et al., 2023). We first randomly choose an starting state \(s_{0} S\), and sample two trajectories \(^{+}\) and \(^{-}\) with fixed length \( 2\) from \(_{}\):

\[^{+} :=(a_{0},a_{1},,a_{})_{}(|s_{0}) \] \[^{-} :=(a^{}_{0},a^{}_{1},,a^{}_{}) _{}(|s_{0}). \]

Then, we obtain the preference \(^{+}>^{-}\) given the starting state \(s_{0}\) using a scripted teacher approach that has also been widely adopted in the PbRL settings (Lee et al., 2021; Kim et al., 2023), which prioritizes trajectories with higher rewards based on the ground truth reward. For inference at a given state \(s_{t}\), we again sample an action trajectory \(=(a_{t},,a_{t+})\) from \(_{}(|s_{t})\), and apply flow matching to obtain a better action sequence.

The baseline methods for comparing the performance of PFM include behavior cloning (BC), which we adopt as our pre-trained reference model \(_{}\), and a DPO fine-tuned model from the BC model. Additionally, we train a separate behavior cloning model to the collected preferred samples \(y^{+} p_{1}\), aiming to replicate the marginal distribution of the "good" trajectories. Further experimental details are deferred to Appendix D.

Table 3 presents the outcomes from evaluations conducted on 12 offline datasets. Our findings indicate that PFM consistently demonstrates comparable or even superior performance with lower

   Method & vs. \(_{}\) & vs. \(_{}\) \\  \(_{}\) + PFM & **100\%** & 2\% \\ \(_{}\) + PFM\(\)5 & – & 85\% \\ \(_{}\) & **100\%** & – \\ \(_{}\) + PFM & – & 99\% \\ \(_{}\) + PFM\(\)5 & – & **100\%** \\   

Table 2: GPT-4 win rate over 100 test samples.

   \(_{}\) & \(_{}\) + PFM & \(_{}\) + PFM\(\)5 \\  -0.3607 & 0.6399 & **2.7469** \\  \(_{}\) & \(_{}\) + PFM & \(_{}\) + PFM\(\)5 \\ 
2.0156 & 2.178 & **2.7894** \\   

Table 1: Average preference scores of 100 test instances.

Figure 4: Distribution of preference scores for each method. Left visualizes the distribution of scores for the pre-trained reference policy and PFM-attached policy. Without fine-tuning the reference policy, PFM can obtain substantially better results by only adding a small flow-matching module. Right visualizes the preference score distribution of the RLHF (PPO) fine-tuned policy, and the PFM added policy to the PPO fine-tuned policy. Note that PFM is trained with the original dataset, not by the dataset generated from the PPO fine-tuned policy.

variance across all baseline methods. Notably, our method excels particularly in datasets generated from suboptimal behavioral policies, achieving better performance. Furthermore, PFM manages to match the performance on expert datasets even in the absence of a reward model, underscoring its robustness and effectiveness. This demonstrates that PFM can effectively align black-box models with preference through flow matching, without the need to fine-tune the pre-trained model.

### Is Learning a Flow Truly Beneficial?

In this remaining section, we focus on answering the remaining questions, _Q2_ and _Q3_. We first investigate why PFM can be advantageous over previous methods with explicit/implicit reward modeling. As can be seen in Figure 5, DPO, like typical RLHF approaches, is also prone to reward overfitting, and may cause the agent to fail. This is because if the preference estimate is close to 0 or 1, these methods may end up overoptimizing to the exploding reward model (Ziegler et al., 2019; Gao et al., 2023). PFM, on the other hand, is inherently robust to such over-estimation, as we adopt a completely different optimization framework that does not require a reward proxy. (See Theorem 4.1.)

On the other hand, we observe less performance gain in the expert datasets. This is a possible failure mode of PFM, where the generated samples are already near-optimal. In such regime, an arbitrary source \(y_{}\) has a near zero probability of being sampled from the true marginal \(p_{0}\), suggesting that PFM with prior as \(_{}\) might suffer from a shifted source distribution. We verify this experimentally on walker2d, where PFM struggles the most. By adopting a true marginal \(p_{0}\) as the source, PFM with prior \(p_{0}\) can achieve the highest performance among all baselines. This performance is evident even on the expert dataset, matching our theoretical analysis. See Appendix B.

Figure 5: Analysis of a sample episode of a DPO fine-tuned model on the MuJoCo ant environment. DPO fine-tuned model often overestimates the reward due to reward overfitting (_e.g._, \(t=196\)). This can cause the policy to choose problematic actions. Here, the implicit reward estimation is \(_{}(s,a)=(_{}(a|s)/_{}(a|s))\).

   Dataset & BC & DPO Fine-tuned & PFM (Ours) & Marginal BC \\  ant-random-v2 & 31.59 \(\) 0.05 & 31.52 \(\) 0.08 & **31.62 \(\) 0.13** & 25.01 \(\) 4.64 \\ ant-medium-v2 & 90.16 \(\) 21.48 & 95.04 \(\) 13.93 & 96.73 \(\) 2.47 & **99.67 \(\) 1.57** \\ ant-expert-v2 & 125.83 \(\) 24.07 & **134.96 \(\) 3.76** & 132.20 \(\) 2.69 & 99.29 \(\) 34.74 \\  hopper-random-v2 & 3.17 \(\) 0.25 & 3.23 \(\) 0.25 & **7.69 \(\) 0.08** & 5.48 \(\) 4.46 \\ hopper-medium-v2 & 52.83 \(\) 5.03 & 53.47 \(\) 3.92 & **58.76 \(\) 2.62** & 40.44 \(\) 1.69 \\ hopper-expert-v2 & 111.27 \(\) 0.48 & 111.51 \(\) 0.92 & **111.70 \(\) 0.77** & 32.39 \(\) 0.1 \\  halfcheetah-random-v2 & 2.25 \(\) 0.01 & 2.26 \(\) 0.01 & **2.26 \(\) 0.0** & 2.21 \(\) 0.02 \\ halfcheetah-medium-v2 & 40.97 \(\) 0.89 & 41.94 \(\) 0.68 & **43.49 \(\) 0.88** & 38.79 \(\) 1.27 \\ halfcheetah-expert-v2 & 91.02 \(\) 1.24 & **92.15 \(\) 0.76** & 90.05 \(\) 0.83 & 4.77 \(\) 2.5 \\  walker2d-random-v2 & 1.47 \(\) 0.1 & 1.38 \(\) 0.08 & 1.77 \(\) 0.13 & **2.45 \(\) 0.38** \\ walker2d-medium-v2 & 60.35 \(\) 18.16 & **74.05 \(\) 12.05** & 72.59 \(\) 15.8 & 65.29 \(\) 12.58 \\ walker2d-expert-v2 & **108.62 \(\) 0.39** & 108.38 \(\) 0.28 & 108.36 \(\) 0.21 & 15.8 \(\) 0.54 \\  Random Average & 9.62 & 9.60 & **10.84** & 8.79 \\ Medium Average & 61.08 & 66.13 & **67.89** & 61.05 \\ Expert Average & 109.19 & **111.75** & 110.58 & 38.06 \\  D4RL Average & 59.97 & 62.49 & **63.10** & 35.97 \\   

Table 3: Normalized results on MuJoCo datasets. Mean and standard deviation from 5 seeds are reported.

Next, we compare PFM to an alternative approach that simply tries to approximate the marginal distribution directly from the positive samples. Intuitively, training a generative model from scratch that replicates the marginal \(p_{1}\) is as computationally costly as training the original reference model. Experimentally, we observe that PFM achieves better performance than training a behavior cloning model (Marginal BC) to replicate the distribution of the preferred samples (Table 3). However, it is also worth mentioning that the Marginal BC model does occasionally yield the best results, suggesting the potential of using a marginal distribution for preference alignment.

## 6 Related Works

Contrastive Preference Learning (CPL) (Hejna et al., 2023) is a class of reward-free methods that utilizes contrastive learning techniques to align model outputs with the preferences observed in the dataset. By leveraging contrastive loss, CPL encourages the model to distinguish between preferred and less preferred outcomes effectively. Flow-to-Better (FTB) (Zhang et al., 2023) innovatively uses a diffusion model to transition from less preferred data to more preferred data, similar to the flow-based approach in our work. However, FTB mainly focuses on data augmentation, where they used the trained diffusion model to generate more data samples for behavior cloning. Despite their strengths, both works rely on the Bradley-Terry model to implicitly learn the reward function.

Identity Preference Optimization (IPO) (Azar et al., 2023) builds upon the foundation laid by DPO, extending the framework to accommodate a broader range of preference models beyond the Bradley-Terry paradigm. In particular, they focus on finding an objective that is bounded even in a deterministic preference regime, by replacing the function \(^{-1}\) in (11) with an identity function \(I(x)=x\). This effectively mitigates the reward overfitting problem observed in DPO and standard RLHF methods.

Our method distinguishes itself from these approaches by not requiring the Bradley-Terry assumption nor the fine-tuning of pre-trained models. This eliminates the risk of reward overfitting associated with the Bradley-Terry model and reduces the computational cost significantly. By avoiding the need for fine-tuning, our method offers a more efficient and scalable solution for integrating human preferences into reinforcement learning systems. This makes our approach particularly suitable for scenarios where computational resources are limited or where quick adaptation to human feedback is essential. The comparison of these related works is summarized in Table 4.

## 7 Conclusion and Limitations

In conclusion, this research introduces Preference Flow Matching (PFM), a novel add-on approach that offers a practical, efficient, and scalable solution for integrating human preferences. This research highlights the potential of flow matching as a powerful tool for preference alignment and opens new avenues for further exploration and development in the field of RLHF. The ability to align black-box models with human preferences without extensive model modifications marks a critical step forward, with broad implications for the deployment and usability of AI systems in real-world applications.

Our theoretical and empirical analyses demonstrate that PFM achieves alignment performance comparable to standard RLHF methods while being more resilient to preference overfitting. The iterative flow matching technique further enhances alignment quality, by continually refining the preference alignment without modifying the underlying pre-trained model parameters.

Despite these promising results, the current design of the PFM framework entails several challenges in the natural language processing (NLP) domain. The PFM framework, as currently designed, relies on the autoencoder to work with fixed-sized embeddings to handle variable-length texts. To scale our method to more complex NLP tasks, future research should explore ways to adapt the PFM framework to long-form texts.

    & RLHF & DPO & IPO & CPL & FTB & PFM (Ours) \\  Reward Model Free & ✗ & ✓ & ✓ & ✓ & ✓ & ✓ \\ No Reward Assumptions (e.g. BT) & ✗ & ✗ & ✓ & ✗ & ✗ & ✓ \\ Applicable to Black-Box Models & ✗ & ✗ & ✗ & ✓ & ✓ \\   

Table 4: Comparison of our method to other works.