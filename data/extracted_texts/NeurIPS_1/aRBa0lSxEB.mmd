# A Dynamical System View of

Langevin-Based Non-Convex Sampling

 Mohammad Reza Karimi

ETH Zurich

mkarimi@inf.ethz.ch &Ya-Ping Hsieh

ETH Zurich

yaping.hsieh@inf.ethz.ch &Andreas Krause

ETH Zurich

krausea@ethz.ch

Equal contribution.

###### Abstract

Non-convex sampling is a key challenge in machine learning, central to non-convex optimization in deep learning as well as to approximate probabilistic inference. Despite its significance, theoretically there remain some important challenges: Existing guarantees suffer from the drawback of lacking guarantees for the _last-iterates_, and little is known beyond the elementary schemes of stochastic gradient Langevin dynamics. To address these issues, we develop a novel framework that lifts the above issues by harnessing several tools from the theory of dynamical systems. Our key result is that, for a large class of state-of-the-art sampling schemes, their last-iterate convergence in Wasserstein distances can be reduced to the study of their continuous-time counterparts, which is much better understood. Coupled with standard assumptions of MCMC sampling, our theory immediately yields the last-iterate Wasserstein convergence of many advanced sampling schemes such as mirror Langevin, proximal, randomized mid-point, and Runge-Kutta methods.

## 1 Introduction

Many modern learning tasks involve sampling from a high-dimensional density \( e^{-f}\), where \(f\) is a _non-convex_ potential representing, for instance, the loss function of a deep neural network. To this end, an approach that has found wide success is to discretize the continuous-time _Langevin diffusion_

\[L_{t}=- f(L_{t})\;t+\;B_{t}\] (LD)

where \(B_{t}\) is a Brownian motion . The idea behind this approach is that, since \(\) is the stationary distribution of (LD), one can expect a similar behavior for discretizations of (LD). Such a framework has inspired numerous sampling schemes with per-iteration costs as cheap as stochastic gradient descent, which are particularly suitable for large-scale approximate probabilistic inference and Bayesian learning . Moreover, several works have noticed that these Langevin-based schemes provide deep insights about minimizing \(f\) using stochastic oracles , which serves as an important step toward explaining the empirical success of training deep neural networks.

The convergence of Langevin-based non-convex sampling has therefore attracted significant interest from both practitioners and theoreticians, whose intense study has led to a plethora of new guarantees; see related work for details. Despite such impressive progress, several challenges remain for the fully non-convex setup:

* The convergence is typically given on the _averaged_ iterates instead of the more natural _last_ iterates . This is especially problematic from the perspective of understanding the minimization of \(f\), as in practice, the last iterates of an optimization algorithm play the most pivotal role for downstream tasks.

* An additional notable drawback of the current theory is its predominant focus on the basic Euler-Maruyama discretization of (LD) (see, e.g., ). As a result, the convergence analysis of more advanced sampling schemes remains largely unexplored in the fully non-convex regime .

SS Contributions and Approaches.To overcome the aforementioned challenges, our main contribution, from a high level, can be succinctly summarized as:

Under mild assumptions, we prove that the iterates of a broad range of Langevin-based sampling schemes converge to the _continuous-time_ (LD) in _Wasserstein distance_.

Combining (\(\)) with classical results on Langevin diffusion  immediately yields the _last-iterate_ convergence in _Wasserstein distances_ for a wide spectrum of sampling schemes, thus resolving all the challenges mentioned above. To illustrate this point, we state a simple version of our main result.

**Theorem** (Informal).: _Suppose we discretize (LD) as_

\[x_{k+1}=x_{k}-_{k+1}( f(x_{k})++)+}\,_{k+1}\]

_with step-sizes \(\{_{k}\}_{k}\) and i.i.d. standard Gaussians \(\{_{k}\}_{k}\). Then, under an easy-to-verify condition on the bias (see (5) in Assumption 3), \(\{x_{k}\}_{k}\) converges in Wasserstein distance to \(\). In addition, these conditions are satisfied by many advanced sampling schemes._

This result is achieved via a new _dynamical perspective_ to study Langevin-based sampling. More specifically,

1. We introduce the _Picard process_, which is the sampling analogue of Picard's method of successive approximations for solving ODEs . Contrary to most existing analyses, the Picard process allows us to completely bypass the use of relative entropy, which is the culprit for the appearance of averaged iterates .
2. Using the Picard process, we will prove that the iterates of various Langevin-based schemes generate a so-called _Wasserstein asymptotic pseudotrajectory_ (WAPT) for the continuous-time (LD). The main motivation for considering WAPT is to connect Langevin-based schemes to the dynamical system theory of Benaim and Hirsch , which works for metric spaces and is last-iterate by design, and therefore particularly suitable for our purpose.
3. Finally, under standard stability assumptions in the literature , we show how a tandem of our WAPT result and dynamical system theory yields the desirable convergence of various existing schemes, as well as motivates more efficient algorithms that enjoy the same rigorous guarantees.

SS Related work.There is a vast literature on _structured_ non-convex sampling, where one imposes extra assumptions on the target density. Under these conditions, one can derive _non-asymptotic_ rates for Langevin-based schemes . Our work is orthogonal to these works as we study _generic_ non-convex sampling, an NP-hard problem whose convergence is asymptotic at best.

Most relevant to our paper are the works , which study the asymptotic convergence of Langevin-based schemes under minimal regularity assumptions on \(f\). Compared to their results, our guarantees either improve upon existing ones or are incomparable; see Section 5.4 for a more detailed comparison.

## 2 The Langevin-Robbins-Monro Template

We consider the following general template for sampling algorithms: Starting from an initial point, the iterates \(\{x_{k}\}_{k}\) follow the recursion

\[x_{k+1}=x_{k}-_{k+1}\{v(x_{k})+Z_{k+1}\}+}\,(x _{k})\,_{k+1},\] (LRM)

where \(_{k}\)'s are step sizes, \(v\) is a vector field, \(Z_{k}\)'s are (random or deterministic) perturbations, \(\) is the state-dependent diffusion matrix, and \(_{k}\)'s are i.i.d. standard Gaussian random variables. In the sequel, we will further decompose the perturbation as \(Z_{k}=U_{k}+b_{k}\), where \(U_{k}\) is the (zero-mean)_noise_ and \(b_{k}\) is the _bias_. We call this recursion the _Langevin-Robbins-Monro_ (LRM) template, as it is reminiscent of the Robbins-Monro template for stochastic approximation .

The generality of the LRM template allows us to capture many existing algorithms and suggests ways to design new ones. For illustration purposes, we showcase instances of (LRM) with the following examples. Other examples (SGLD and proximal) are provided in Appendix A. In the first three examples, the vector field \(v\) in (LRM) is \(- f\) and \( 1\).

**Example 1**.: The _Randomized Mid-Point Method_ is an alternative discretization scheme to Euler-Maruyama and has been proposed for both overdamped and underdamped Langevin diffusion. For the overdamped case, its iterates are

\[ x_{k+1/2}&=x_{k}-_{k+1}_{ k+1}f(x_{k})+_{k+1}}^{}_{k+1},\\ x_{k+1}&=x_{k}-_{k+1}f(x_{ k+1/2})+}_{k+1},\] (RMM)

where \(\{_{k}\}\) are i.i.d. and uniformly distributed in \(\), \(_{k},^{}_{k}\) are standard Gaussian random variables with cross-variance \(}I\), and \(f\) is a noisy evaluation of \( f\). To cast (RMM) in the LRM template, we set \(U_{k+1}f(x_{k+1/2})- f(x_{k+1/2})\) and \(b_{k+1} f(x_{k+1/2})- f(x_{k})\). 

**Example 2**.: Inspecting the update rule of (RMM), we see that it requires _two_ gradient oracle calls at each iteration. Inspired by the _optimistic gradient methods_ in optimization and online learning , we propose to "recycle" the past gradients:

\[ x_{k+1/2}&=x_{k}-_{k+1}_{ k+1}f(x_{k-1/2})+_{k+1}}^{}_{k+1},\\ x_{k+1}&=x_{k}-_{k+1}f(x_{k+1/2 })+}\,_{k+1},\] (ORMM)

where \(\{_{k}\}\), \(_{k},^{}_{k}\), and \(f\) are the same as in (RMM). This is again an LRM scheme with \(U_{k+1}f(x_{k+1/2})- f(x_{k+1/2})\) and \(b_{k+1} f(x_{k+1/2})- f(x_{k})\).

Notice that (ORMM) requires _one_ gradient oracle, thereby reducing the per-iteration cost of (RMM) by 2. To our knowledge, the scheme (ORMM) is new. 

**Example 3**.: In addition to the simple (stochastic) Euler-Maruyama discretization in (SGLD), there exists a class of more sophisticated discretization methods of (LD) known as higher-order integrators. The _Stochastic Runge-Kutta method_ is an example of an order 1.5 integrator, with iterates

\[ h_{1}&=x_{k}+}\,(c_ {1}_{k+1}+c_{2}^{}_{k+1})\\ h_{2}&=x_{k}-_{k+1}f(x_{k} )+}\,(c_{3}_{k+1}+c_{2}^{}_{k+1}),\\ x_{k+1}&=x_{k}-}{2}(f(h_{1})+f(h_{2}))+}\,_{k+1}, \]

where \(_{k+1}\) and \(^{}_{k+1}\) are independent standard Gaussian random variables, and \(c_{1},c_{2},c_{3}\) are suitably chosen integrator constants. This algorithm is an LRM scheme with \(U_{k+1}(f(h_{1})- f(h_{1}))+ (f(h_{2}))- f(h_{2}))\) and \(b_{k+1}( f(h_{1})+ f(h_{2}))- f(x_{k})\).

**Example 4**.: The _Mirror Langevin_ algorithm , which is the sampling analogue of the celebrated mirror descent scheme in optimization , uses a strongly convex function \(\) to adapt to a favorable local geometry. In the dual space (i.e., the image of \(\)), its iterates follow

\[x_{k+1}=x_{k}-_{k+1} f(^{*}(x_{k}))+} (^{2}^{*}(x_{k})^{-1})^{1/2}\,_{k+1},\] (ML)

where \(^{*}\) is the _Fenchel dual_ of \(\). In our framework, (ML) fits into (LRM) by taking \(v=- f^{*}\) and \(=(^{2}^{*})^{-1/2}\). Additionally, one can also consider a stochastic version of (ML) with noisy evaluations of \( f\).

## 3 Technique Overview: A Dynamical System Perspective

The goal of our paper is to provide last-iterate guarantees for the general LRM schemes introduced in Section 2. There are two equivalent, commonly considered, ways of characterizing the dynamics of the iterates of an LRM scheme. The first one is to view the iterates \(\{x_{k}\}_{k}\) as a _random_ trajectory in \(^{d}\), which is perhaps the most natural way of describing a sampling algorithm. The second way is to view the _distributions_\(\{_{k}\}_{k}\) of \(\{x_{k}\}_{k}\) as a _deterministic_ trajectory in the _Wasserstein space_.

With these two characterizations in mind, in this section, we will devise a new framework based on the dynamical system theory and present its high-level ideas.

To understand our novelty, it is important to contrast our framework to the existing Wasserstein viewpoint towards Langevin-based sampling algorithms. Following the seminal work of Otto , one can view a sampling algorithm as the discretization of a class of well-studied dynamical systems--_gradient flows_. This viewpoint suggests using _Lyapunov_ arguments, which has become the predominant approach in much prior work.

Despite its appealing nature, in the rest of this section, we will argue that Lyapunov analysis of gradient flows is in fact _not_ suited for studying generic non-convex sampling. In particular, we will show how our new framework is motivated to overcome the several important limitations of gradient flow analysis. Finally, we give a high-level overview of the techniques used in our paper.

SS Langevin Diffusion as Gradient Flows.We denote by \(_{t}\) the probability density of \(L_{t}\) in (LD), and consider the continuous curve \(t_{t}\) in the Wasserstein space \(_{2}\). In their seminal works, Jordan et al.  and Otto  discover that this curve is the (exact) gradient flow of the relative entropy functional; that is, defining the functional \(F: D_{}\|e^{-f}\), one has \(_{t}_{t}=-F(_{t})\), where "grad" is the gradient in the Wasserstein sense. This gradient flow viewpoint of (LD) thus provides a clear link between sampling in \(^{d}\) and optimization in \(_{2}\). Indeed, this suggests that the relative entropy is a natural choice for the _Lyapunov function_ of the discrete-time sampling algorithm, which is a prominent approach for analyzing sampling algorithms in recent years .

Although the gradient flow viewpoint has led to a sequence of breakthroughs, it has a few important shortcomings:

1. The usual Lyapunov-type analysis for sampling algorithms focuses on bounding the change in relative entropy across iterations. This is extremely challenging when one considers more advanced sampling algorithms, as one has to understand the effect of the additive bias and noise of the algorithm on the change of relative entropy. Crucially, this makes the Lyapunov analysis applicable only to the simple Euler-Maruyama discretization of (LD),2 i.e., \(x_{k+1}=x_{k}-_{k+1} f(x_{k})+}\,_{k+1}\), and fails to capture more advanced and _biased_ sampling schemes such as Examples 1-4. Even for the simple (SGLD), the presence of stochastic gradients significantly complicates the Lyapunov analysis and requires extra assumptions such as convexity  or uniform spectral gap . 2. This gradient flow-based analysis often requires an extra _averaging_ step to decrease the relative entropy (see, e.g., ). This is the main reason why many existing works provide guarantees only on the _averaged_ iterates (\(_{k}_{i=1}^{k}_{i}\)) instead of the last ones (\(_{k}\)).

In this paper, we overcome these limitations by introducing a new perspective, whose two ingredients are as follows.

SS Wasserstein Asymptotic Pseudotrajectories.A notion that will play a pivotal role in our analysis is the _Wasserstein asymptotic pseudotrajectory_ (WAPT), which is a measure of "asymptotic closeness" in the Wasserstein sense, originally defined by Benaim and Hirsch  for metric spaces:

**Definition 1** (Wasserstein asymptotic pseudotrajectory).: We say the stochastic process \((X_{t})_{t 0}\) is a _Wasserstein asymptotic pseudotrajectory_ (WAPT) of the SDE

\[_{t}=v(_{t})\;t+(_{t})\;B_{t}\] (SDE)

if, for all \(T>0\),

\[_{t}_{0 s T}W_{2}(X_{t+s},_{s}^{(t)})=0. \]

Here, \(_{s}^{(t)}\) is the solution of the SDE at time \(s\) initialized at \(X_{t}\), and \(W_{2}\) is the 2-Wasserstein distance.

Despite the seemingly convoluted definition, WAPT can be intuitively understood as follows: Let \(\{x_{k}\}_{k}\) be the iterates of a sampling scheme. Then, (1) simply posits that for sufficiently large \(x_{m}\), up to arbitrarily small error measured in terms of the Wasserstein distance. Since we are only interested in the asymptotic behavior of \(x_{k}\), these controls on the tail iterates will suffice to conclude the last-iterate convergence.3

Importantly, from the perspective of WAPT, the Langevin diffusion (LD) (or more generally, \(_{s}^{(t)}\)) is simply viewed as a generic dynamical system and _not_ as a gradient flow. In particular, _relative entropy will play no role throughout our analysis_, thereby resolving issue (b).

SS Langevin-Robbins-Monro Schemes.We have seen that the LRM template in Section 2 is capable of capturing a broad range of existing and new algorithms in a unified way. To resolve the remaining issue (a), we will further rely on the LRM template: for proving that (LRM) generates a WAPT of the corresponding SDE, we show that the key condition (1) in WAPT can be reduced to checking an easy-to-verify bound on the perturbation terms \(Z_{k}\).

To achieve this, the most important step in our proof, which distinguishes our analysis from all existing works in non-convex sampling, is the construction of the so-called _Picard process_, the natural generalization of the Picard's successive approximation method  from ordinary differential equations to _stochastic_ differential equations. In the stochastic approximation literature, similar techniques have been successfully applied to study optimization and games in various settings such as on Riemannian or primal-dual spaces . The application to sampling has also been previously explored by Bubeck et al. , Chau et al.  in different contexts. What distinguishes our work from the existing literature is the advantage of generalizing the Picard process to encompass a vastly wider class of algorithms, specifically the LRM schemes. Moreover, the integration of the Picard process with the theory of WAPT plays a pivotal role in our analysis, and both of these aspects present original contributions.

SS Framework overview.To conclude, for proving last-iterate convergence, we proceed as follows:

1. For a given LRM scheme \(\{x_{k}\}_{k}\), we first construct a continuous-time trajectory \((X_{t})_{t 0}\) via _interpolating_ the iterates (see (3)).
2. We prove that \((X_{t})\) constitutes a WAPT of the SDE (see Theorem 1). This step relies heavily on the construction of the aforementioned Picard process.
3. By invoking the dynamical system theory of Benaim and Hirsch , the convergence of LRM schemes reduces to simply checking the _stability condition_ (Theorem 2). In the Wasserstein space, this condition translates into boundedness of the second moments of the iterates \(\{x_{k}\}\), for which there is a plethora of approaches; we present two such methods in Section 5.

Fig. 1 depicts a high-level overview of the ingredients needed in our framework, and their corresponding theorems.

## 4 The Dynamics of Langevin-Robbins-Monro Schemes

In this section, we view (LRM) as a noisy and biased discretization of (LD). To make this analogy precise, let \((B_{t})_{t 0}\) be a Brownian motion defined on a filtered probability space with filtration

Figure 1: High-level overview of the two components of the dynamical perspective.

\((_{t})_{t 0}\), and define \(_{k}=_{t=1}^{k}_{n}\) to be the effective time that has elapsed at the iteration \(k\). Using the Brownian motion, we can rewrite (LRM) as

\[x_{k+1}=x_{k}-_{k+1}\{v(x_{k})+Z_{k+1}\}+(x_{k})(B_{T_{k+1}}-B _{T_{k}}), \]

assuming that the filtration satisfies \(Z_{k}_{_{k}}\).4 The (continuous-time) _interpolation_\((X_{t})_{t 0}\) of \(\{x_{k}\}_{k}\) is then defined as the adapted process

\[X_{t}=x_{k}-(t-_{k})\{v(x_{k})+[Z_{k+1}\,|\,_{t}]\}+ (x_{k})(B_{t}-B_{T_{k}}),t[_{k},_{k+1}]. \]

In addition, for a fixed \(t\), consider the Brownian motion \((B_{s}^{(t)})_{s 0}\) where \(B_{s}^{(t)} B_{t+s}-B_{t}\), and define the _Langevin flow_\((_{s}^{(t)})_{s 0}\) as the (strong) solution of (SDE) initialized at \(X_{t}\). It is important to note that \(^{(t)}\) and \(X\) are synchronously coupled by sharing the same Brownian motion.

### Technical Assumptions and Requirements

We now introduce the basic technical assumptions and discuss their generality.

**Assumption 1**.: _The vector field \(v\) is \(L\)-Lipschitz, and satisfies \( x,v(x) C_{v}(1+\|x\|)\) for some \(C_{v}>0\). Moreover, \(\) is \(L\)-Lipschitz and is bounded in Hilbert-Schmidt norm._

Lipschitzness of \(v\) is a standard assumption and is also required to ensure the existence of a unique strong solution of (SDE). The second assumption on the vector field is exceedingly weak and when \(v=- f\), is satisfied even for distributions without moments. The assumptions on diffusion coefficient \(\) are already satisfied when \( 1\), and we show that it holds for practical schemes such as Example 4.

**Assumption 2**.: _The Robbins-Monro summability conditions hold: \(_{k=1}^{}_{k}=\) and \(_{k=1}^{}_{k}^{2}<\). Moreover, for some constant \(P\) to be defined in (20), we have_

\[_{k+1}/_{k}+P_{k}_{k+1}<1-_{k}, k. \]

The Robbins-Monro step size conditions are standard in the non-convex sampling literature . For (4), it can be verified that condition is satisfied even for slowly-decreasing step sizes such as \(_{k}( k)^{-1}\), which hence is not restrictive.

**Assumption 3**.: _The noises \(\{U_{k}\}_{k}\) form a martingale difference sequence, i.e., \([U_{k+1}\,|\,U_{k}]=0\), and have uniformly bounded second moments. In addition, the bias terms satisfy_

\[[\|b_{k+1}\|^{2}\,|\,_{_{k}}]=(_{k+1 }^{2}\|v(x_{k})\|^{2}+_{k+1}). \]

A martingale difference sequence is more general than an i.i.d. sequence, allowing the noise to be state-dependent. The bias condition (5) simply states that the bias shall not overpower the signal \(v(x_{k})\), and, as we show later, is satisfied by all our examples.

### From Discrete to Continuous: LRM Schemes and WAPTs

We are now in a position to state our main theorems. Our first result below establishes a precise link between the discrete-time (LRM) and the continuous-time (SDE).

**Theorem 1**.: _Under Assumptions 1-3, the interpolation (3) of an LRM scheme is a Wasserstein asymptotic pseudotrajectory of (SDE)._

SS Sketch of the Proof for Theorem 1.The proof of this theorem is heavily based on the notion of the Picard process and iterate moment bounds. The complete proof can be found in Appendix C.

SS Step 1: The Picard Process.For a fixed \(t>0\), recall the construction of the interpolation (3) and the Langevin flow. Central to our analysis is the _Picard process_, defined as

\[Y_{s}^{(t)}=X_{t}+_{0}^{s}v(X_{t+u})\,\,u+_{0}^{s}(X_{ t+u})\,\,B_{u}^{(t)}. \]The Picard process is adapted and is (synchronously) coupled with the Langevin flow and the interpolation. We think of the Picard process as one step of the _Picard iteration_ for successive approximations to solve ODEs. This means, intuitively, that its trajectory should be close to the original interpolation, as well as to that of the Langevin flow, playing the role of a "bridge".

Fix \(T>0\). For \(s[0,T]\), we decompose the distance between the interpolation \(X_{t}\) in (3) and the Langevin flow as

\[\|X_{t+s}-_{s}^{(t)}\|^{2}\|Y_{s}^{(t)}-_{s}^{(t)}\|^{ 2}+\|X_{t+s}-Y_{s}^{(t)}\|^{2}. \]

We now bound each term of the decomposition. By synchronous coupling of the processes, Lipschitzness of \(v\), and Ito isometry, Lemma 3 bounds the first term as

\[\|Y_{s}^{(t)}-_{s}^{(t)}\|^{2} 2(T+1)L^{2}_{0}^{s}\|_{u}^{(t)}-X _{t+u}\|^{2}\,\,u. \]

This will be suitable for later use of Gronwall's lemma.

SS Step 2: Accumulated Noise and Bias.For the rest of the proof, we need some extra notation. Define \(m(t)\{k 0:_{k} t\}\) and the piecewise-constant process \(_{t} x_{m(t)}\). Going back to the second term of (7), observe that

\[X_{t+s}-Y_{s}^{(t)}=_{t}^{t+s}v(_{u})-v(X_{u})\,\,u+ _{0}^{s}(_{t+u})-(X_{t+u})\,\,B_{u}^{(t) }-_{Z}(t,s), \]

where \(_{Z}(t,s)\) is the accumulated noise and bias from time \(t\) to time \(t+s\). It is expected that \(\|_{Z}(t,s)\|\) eventually becomes negligible, since the step size becomes small. The next lemma confirms this intuition.

**Lemma 1**.: _Suppose Assumptions 1-3 hold. Then, for any fixed \(T>0\) we have_

\[_{t}_{0 s T}\|_{Z}(t,s)\|^{2}=0.\]

SS Step 3: Gradient Moment Bounds.Based on (9) and Lemma 1, bounding the distance between the Picard process and the interpolation essentially reduces to bounding how much the discrete algorithm "moves" during one iteration in expectation. This, in turn, depends on how large the moments of \(\|v(x_{k})\|\) grow per iteration, which is controlled by the following lemma:

**Lemma 2**.: _Let \(\{x_{k}\}_{k}\) be the iterates of (LRM) and suppose Assumptions 1-3 hold. Then, \(\|x_{k}\|^{2}=(1/_{k+1})\). This in turn implies \(\|v(x_{k})\|^{2}=(1/_{k+1})\) and \(\|b_{k+1}\|^{2}=(_{k+1})\)._

Using this lemma and Lemma 1 we can obtain \(A_{t}_{0 s T}\|X_{t+s}-Y_{s}^{(t)}\|^{2} 0\) as \(t\), which shows that the Picard process gets arbitrarily close to the interpolation as \(t\).

SS Step 4: Concluding the Proof.Let us go back to the decomposition (7). Taking expectation and using (8) and Gronwall's lemma, we obtain \([\|X_{t+s}-_{s}^{(t)}\|^{2}] 4\,A_{t}(T^{2}L^{2})\), Thus,

\[_{t}_{s[0,T]}[\|X_{t+s}-_{s}^{(t)}\|^{ 2}]=0.\]

As we coupled \(X_{t+s}\) and \(_{s}^{(t)}\) in a specific way (via synchronizing the Brownian motions), we directly get an upper bound on the Wasserstein distance.

## 5 Last-Iterate Convergence of Sampling Schemes

In this section we focus on last-iterate convergence of LRM schemes in Wasserstein space. We first explore the interplay between the convergence of WAPTs and _stability_. We then show that the existing stability results for simple Euler-Maruyama discretization of the Langevin diffusion can be extended, with little to no extra assumptions, to the class of LRM schemes in Section 2. This in turn readily implies the last-iterate convergence of a wide class of LRM schemes.

### From WAPTs to Convergence in \(_{2}\)

Since convergence of the distribution of \(x_{k}\) to \(\) in Wasserstein distance implies convergence of the second moments of \(x_{k}\) to that of \(\), convergence in the Wasserstein space should at least require:

\[_{k}\ \|x_{k}\|^{2}<. \]

It turns out that, for WAPTs, the exceedingly weak necessary condition (10) is also _sufficient_:

**Theorem 2**.: _Let \((X_{t})\) be a Wasserstein asymptotic pseudotrajectory of the Langevin diffusion (LD) generated by an LRM scheme \(\{x_{k}\}\) via (3). Then \(W_{2}(x_{k},) 0\) if and only if (10) holds._

Proof.: The proof relies on the structure of compact sets in the Wasserstein space and limit-set theorems for dynamical systems . Specifically, the closure of bounded subsets of \(_{2}\) is compact , so condition (10) implies that \(((X_{t}))_{t 0}\) is pre-compact in \(_{2}\). Moreover, Assumption 1 implies that the Langevin flow is globally integrable. Thus, \(((X_{t}))_{t 0}\) is a pre-compact WAPT of a globally integrable flow, and we can apply the limit-set theorem for metric spaces [7, Theorem 0.1] to conclude that the limit-set of \(((X_{t}))_{t}\) is an _internally chain transitive (ICT) set_.

Next, we show that for the case of the Langevin flow, the only ICT set is \(\{\}\), implying the desired convergence of our theorem. To see this, define \(V()=D_{}()\). It can be observed that \(V\) is a Lyapunov function for (LD), whose value is strictly decreasing along the flow (as the time derivative of \(V\) along the flow is negative of the relative Fisher information, which is strictly positive for all measures other than \(\)). Thus, all requirements of [6, Prop. 6.4] are satisfied, showing that the only point in the ICT set is \(\). This also shows the uniqueness of the stationary distribution of (LD). 

_Remark_.: From the proof of Theorem 1, we observe that the supremum of the Wasserstein distance between \((X_{})_{\{t,t+T\}}\) and \((^{(t)})_{\{0,T\}}\) typically scales exponentially with \(T\), which is common for weak approximation error in the literature, see . Despite the exponential dependence on \(T\), the convergence of the last iterate is assured by Theorem 2 without a need of a uniform control in \(T\). This is primarily attributed to the adoption of a dynamical system viewpoint and the application of corresponding tools, effectively harnessing the paradigm established by Benaim and Hirsch.

Theorems 1-2 in tandem thus show that, as long as an LRM scheme satisfies Assumptions 1-3 and the moment condition (10), the desirable last-iterate convergence in \(_{2}\) is immediately attained. Therefore, in the rest of this section, we turn our focus to establishing (10) for LRM schemes.

### Bounded Moments of LRM Schemes

There is a long history of study on conditions that ensure (10) for iterative algorithms, which has culminated in the so-called _dissipativity_ properties. We consider two such examples below.

**Assumption 4** (Dissipativity).: _There exist constants \(>0\) and \( 0\) such that_

\[ x,v(x)-\|x\|^{2}+, x^{ d}.\]

Under Assumption 4, it is classical that (10) holds for the simple Euler-Maruyama discretization of (LD) with deterministic or stochastic gradient oracles . These studies, however, cannot handle _non-zero bias_, which, as seen in Examples 1-3, is crucial for incorporating more advanced sampling schemes.

To this end, our next result shows that for a wide class of LRM schemes, the stability (10) essentially comes for free under Assumption 4. The proof is provided in Appendix D.

**Theorem 3**.: _Let \(v\) be a vector field satisfying Assumptions 1 and 4 and \(\) be a diffusion coefficient satisfying Assumption 1, and let \(\{x_{k}\}\) be an LRM scheme. Assume that \(_{k}_{k}=0\), \(_{k}\|U_{k}\|^{2}<\), and the bias satisfies (5). Then, the stability condition (10) holds for \(\{x_{k}\}\)._

A weaker notion of dissipativity that has been studied in the literature is:

**Assumption 5** (Weak dissipativity).: _There exist constants \(>0\), \((0,1]\), and \( 0\) such that_

\[ x,v(x)-\|x\|^{1+}+, x ^{d}.\]When \(=1\), Assumption 5 is simply Assumption 4. As opposed to Assumption 4, which requires _quadratic growth_ of \(f\) outside a compact set (when \(v=- f\)), Assumption 5 only entails _superlinear growth_ and therefore is considerably weaker.

For Euler-Maruyama discretization of (LD) with deterministic gradients,  prove that Assumption 5 is sufficient to guarantee bounded moments of the iterates. As for a generic LRM scheme, we consider the following general condition on the bias terms, which will suffice to cover all our examples in Section 2: For some constant \(c\),

\[\|b_{k+1}\|^{2} c_{k+1}^{2}\|v(x_{k})\|^{2}+_{k+1}^{2} \|U_{k+1}^{}\|^{2}+_{k+1}\|_{k+1}^{}\|^{2}+_{k+1} \|_{k+1}\|^{2}, \]

where \(U_{k+1}^{}\) is an extra noise term, and \(_{k+1}^{}\) is a standard Gaussian independent of the noises and \(_{k}\). The price to pay with the weaker Assumption 5, however, is that we need to assume sub-Gaussianity of the noise. For a proof, see Appendix D.

**Theorem 4**.: _Let \( e^{-f}\) be the target distribution, where \(v=- f\) satisfies Assumptions 1 and 5, and let \(\{x_{k}\}\) be an LRM scheme. Assume that \(_{n}_{k}=0\), the noises \(U_{k}\) and \(U_{k}^{}\) are sub-Gaussian, and the bias term of \(\{x_{k}\}\) satisfies (11). Then, (10) holds for \(\{x_{k}\}\) in when_ (i)_\( 1\), or_ (ii)_\(f\) is Lipschitz and the LRM follows the Mirror Langevin algorithm (Example 4)._

### Examples of Convergent LRM Schemes

We now illustrate the use of Theorems 1-4 on our examples in Section 2.

**Proposition 1**.: _Under Assumption 1 and noise with uniformly bounded second moments, the following holds for Examples 1-6: (i) The bias has the form (11) and satisfies (5), (ii) As a result, under Assumptions 2 and 3, Examples 1-6 produce iterates that generate a WAPT of_ (SDE). (iii) _Under the additional conditions of Theorem 3 or Theorem 4, Examples 1-6 enjoy last-iterate convergence to the target distribution in Wasserstein distance._

### Comparison to Existing Work

We now give a more detailed comparison of our results to existing literature; a summary is given in Table 1, and additional comparison with prior works can be found in Appendix B.

\(@sectionsign\) Guarantees for LRM Schemes.Lamberton and Pages  and Lemaire  study the simple Euler-Maruyama discretization of (LD) with deterministic gradients (i.e., \(U_{k}=b_{k}=0\)) and establish the weak convergence of the average iterates under a moment condition that is slightly weaker than (10).5 Their analysis is further extended by  to incorporate stochastic gradients. Later, the last-iterate convergence of the simple Euler-Maruyama discretization of (LD) is studied by , who prove the convergence in the total variation distance under Assumption 5. Another work on a similar setting as  is , where the convergence criterion is given in an integral probability metric (IPM)  of the form \(d_{}(,)_{}_{ }-_{}\) for a certain class of test functions \(\) that is known to imply weak convergence, but not convergence in total variation or Wasserstein distances.

Compared to these results, our guarantees possess the following desirable features:

    & Noise & Bias & Last-Iterate \\  Lamberton and Pages , Lemaire  & ✗ & ✗ & ✗ \\  Teh et al.  & ✓ & ✗ & ✗ \\  Benaim et al.  & ✗ & ✗ & ✓ \\  Durmus and Moulines  & ✗ & ✗ & ✓ \\  Balasubramanian et al.  & ✗ & ✗ & ✗ \\  This work & ✓ & ✓ & ✓ \\   

Table 1: Comparison to existing works on convergence of LRM schemes. All methods, except for , require bounded second moments of the iterates.

* The convergence is always on the last iterates instead of the average iterates.
* As we tolerate biased algorithms, the class of LRM schemes we consider is significantly more general than the ones in existing work.

Finally, we note that our results are incomparable to the recent work of Balasubramanian et al. , who derive the same result as in , i.e., average-iterate, weak convergence, deterministic Euler-Maruyama discretization. A remarkable feature of the analysis in  is that it does not require any bounded moments, and, in particular, their bounds can be applied to target distributions with unbounded variance. However, the downside of  is that, in the presence of \(U_{k}\) and \(b_{k}\), their analysis produces a bound that does _not_ vanish as \(k\); see [4, Theorem 15]. In contrast, our framework can tolerate quite general \(U_{k}\) and \(b_{k}\), gives stronger guarantees (\(W_{2}\) vs. weak convergence; last-iterate vs. average-iterate).

SS On Analysis Techniques.While, to our knowledge, our framework is significantly different from previous works on sampling, we acknowledge that similar ideas of creating an auxiliary process in-between the iterates and the continuous-time flow is not entirely new and has been touched upon in the literature, e.g., . That being said, our specific approach in building the Picard process and its development into a wider array of algorithms, i.e., Langevin-Robbins-Monro schemes, undoubtedly plays a pivotal role in our analysis. Moreover, the integration of the Picard process with the theory of asymptotic pseudo-trajectories offers dual benefits to our study, and we view these as our unique contributions to this area of research.

Furthermore, the novel Picard process gives a significant advantage in all of our results. The work of  also hinges on dynamical system theory-related ideas. Yet, missing the critical step of the Picard process has seemingly resulted in much weaker findings compared to our work. This observation is not meant as a critique; rather, it merely highlights the potency of the unique method we have integrated into our study.

## 6 Concluding Remarks

In this paper, we provided a new, unified framework for analyzing a wide range of sampling schemes, thus laying the theoretical ground for using them in practice, as well as motivating new and more efficient sampling algorithms that enjoy rigorous guarantees. We built on the ideas from dynamical system theory, and gave a rather complete picture of the asymptotic behavior of many first-order sampling algorithms. In short, our results help with the following:

* **Validating existing methods:** Methods like mirror Langevin and randomized mid-point currently lack even asymptotic guarantees in fully non-convex scenarios, such as sampling from neural network-defined distributions. Our work fills this gap by offering the first rigorous justification for these schemes, supporting practitioners in utilizing these methods confidently.
* **Facilitating new algorithm design:** Our work motivates novel sampling methods through a straightforward verification of Assumptions 1-3. An illustrative instance involves the randomized mid-point method and Runge-Kutta integrators, wherein a substantial 50% reduction in computation per iteration can be achieved without compromising convergence by simply recycling past gradients, shown in Example 2. The balance between the benefits of saving gradient oracles and potential drawbacks remains an open question, necessitating case-by-case practical evaluation. Nevertheless, our theory provides a flexible algorithmic design template that extends beyond the current literature's scope.

While our WAPT result holds under very mild conditions, a severe limitation of our current framework is that it _only_ applies to Langevin-based algorithms, whereas there exist numerous practical sampling schemes, such as Metropolis-Hastings, that are not immediately linked to (LD). We believe that this restriction arises as an artifact of our analysis, as the WAPT framework can in principle be applied equally well to any continuous-time dynamics. Lifting such constraint is an interesting future work.