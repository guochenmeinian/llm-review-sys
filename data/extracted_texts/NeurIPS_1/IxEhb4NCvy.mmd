# SSDM: Scalable Speech Dysfluency Modeling

Jiachen Lian

UC Berkeley

2 Zhejiang University

3 UCSF

{jiachenlian, gopala}@berkeley.edu

Xuanru Zhou

2 Zhejiang University

3 UCSF

{jiachenlian, gopala}@berkeley.edu

Zoe Ezzes

3Department of Computer Science

University of California

Berkeley, CA 94720

Jet Vonk

3Department of Computer Science

University of California

Berkeley, CA 94720

Brittany Morin

3Department of Computer Science

University of California

Berkeley, CA 94720

David Baquirin

3Department of Computer Science

University of California

Berkeley, CA 94720

Zachary Miller

3Department of Computer Science

University of California

Berkeley, CA 94720

Maria Luisa Gorno Tempini

3Department of Computer Science

University of California

Berkeley, CA 94720

Gopala Anumanchipalli

1 UC Berkeley

2 Zhejiang University

3 UCSF

{jiachenlian, gopala}@berkeley.edu

###### Abstract

Speech dysfluency modeling is the core module for spoken language learning, and speech therapy. However, there are three challenges. First, current state-of-the-art solutions  suffer from poor scalability. Second, there is a lack of a large-scale dysfluency corpus. Third, there is not an effective learning framework. In this paper, we propose _SSDM: Scalable Speech Dysfluency Modeling_, which (1) adopts articulatory gestures as scalable forced alignment; (2) introduces connectionist subsequence aligner (CSA) to achieve dysfluency alignment; (3) introduces a large-scale simulated dysfluency corpus called Libri-Dys; and (4) develops an end-to-end system by leveraging the power of large language models (LLMs). We expect SSDM to serve as a standard in the area of dysfluency modeling. Demo is available at [https://berkeley-speech-group.github.io/SSDM/](https://berkeley-speech-group.github.io/SSDM/).

## 1 Introduction

Speech dysfluency modeling is key for diagnosing speech disorders, supporting language learning, and enhancing therapy . In the U.S., over 2 million individuals live with aphasia , while globally, dyslexia affects approximately one in ten people . The U.S. speech therapy market is projected to reach USD 6.93 billion by 2030 . This growth _parallels_ developments in Automatic Speech Recognition (ASR), valued at USD 12.62 billion in 2023 , and Text-to-Speech (TTS), valued at USD 3.45 billion . Moreover, the global language learning market is anticipated to be USD 337.2 billion by 2032 . Conversely, substantial investments have been made in training speech-language pathologists (SLPs) , and the high cost of treatment often remains out of

Figure 1: SSDM. Comparison to other methodsreach for many low-income families . Therefore, there is a crucial need for an AI solution that makes advanced speech therapy and language learning _available and affordable for everyone_.

Speech dysfluency modeling detects various dysfluencies (stuttering, replacements, insertions, deletions, etc) at both word and phoneme levels, with accurate timing and typically using a reference text . (see Figs.1 for examples). Fundamentally, it is a _spoken language understanding problem_. Recent advancements have been driven by large-scale developments . However, these efforts often focus on scaling coarse-grained performance metrics rather than deeply listening to and understanding the nuances of human speech.

Traditional approaches to dysfluency modeling have relied on hand-crafted features . Recent advancements have introduced end-to-end classification tasks at both utterance  and frame levels . However, these methods often overlook internal dysfluency features like alignment  and struggle to detect and localize multiple dysfluencies within a single utterance.  propose 2D-Alignment, a non-monotonic approach that effectively encodes dysfluency type and timing. Nonetheless, initial experiments show that this method struggles with scalability, limiting its further development. To address these concerns, we revisit this problem and summarize our contributions as follows:

* We revisit speech representation learning from a physical perspective and propose _neural articulatory gestural scores_, discovered to be scalable representations for dysfluency modeling.
* We introduce the _Connectionist Subsequence Aligner_ (CSA), a differentiable and stochastic forced aligner that links acoustic representations and text with dysfluency-aware alignment.
* We enable _end-to-end_ learning by leveraging the power of large language models.
* We open-source the large-scale simulated dataset _Libri-Dys_ to facilitate further research.

## 2 Articulatory Gesture is Scalable Forced Aligner

### Background

Revisit Speech Representation LearningSelf-supervised speech representations , large-scale ASR , codec models , and speech language models (SLMs)  have emerged as universal paradigms across tasks and languages. However, high computing costs of scaling efforts is not affordable for academia researchers. In this work, we propose learning speech representations grounded in fundamental physical laws . This approach characterizes speech representations by the kinematic patterns of articulatory movements, a method we refer to as _gestural modeling_.

Figure 2: SSDM architecture

Gestural ModelingThe concept of _gesture_, as defined by [70; 71], refers to articulatory movements in acoustic space, similar to body gestures in humans. [70; 71] introduced _gestures_ as a dictionary of basic articulatory movements and _gestural scores_, representing the _duration_ and _intensity_ of these movements. This principle resembles the gait library and optimization used in robotics . The computational modeling of gestures was first developed by , using sparse matrix factorization [74; 75] to decompose EMA data  into interpretable components. Further research by  and  streamlined this into an end-to-end neural approach. _Gestural scores_ serve as speech representations. We _discovered_ that they serve as _scalable dysfluent phonetic forced aligner_.

Scalable Dysfluent Phonetic Forced AlignerDysfluency modeling requires detecting both the type and timing of dysfluencies, necessitating the use of forced alignment . This alignment is often non-monotonic (e.g., stuttering). Thus, previous monotonic alignment methods [79; 80; 20; 81] perform poorly in the dysfluency domain. The primary challenge is the inherent uncertainty in what the speaker actually said, compounded by invariably inaccurate reference texts, as explained in . Effective research in this area focuses on non-monotonic alignment modeling.  introduces the WFST  to capture dysfluencies such as sound repetition. However, it assumes the actual speech does not deviate significantly from the reference text.  proposed _2D-alignment_ as final dysfluent representation. Nevertheless, this method, and its extension , suffers from scalability issues: _increasing training data does not lead to further improvements_. In this work, we revisit the monotonic alignment to tackle the scalability problem. To achieve this, we need a scalable representation, and a scalable monotonic aligner (Sec. 3). This section focuses on the first part and proposes _Neural Variational Gestural Modeling_ to deliver _gestural scores_\(H\) as scalable dysfluent speech representations. We also provide a visualization of _gestures_ and _gestural scores_ in Appendix. A.1.

### Neural Variational Gestural modeling

Despite theoretical support [70; 71; 68; 69], gestural scores have not yet become a universal speech representation  due to several limitations. First, gestural modeling requires extensive, often unavailable, articulatory kinematic data. Second, there is not an effective learning framework. Third, the commonly used EMA data, sampled sparsely from human articulators [84; 85; 86; 87], suffer from information loss. To overcome these challenges, we proposed _Neural Variational Gestural Modeling_. This model uses an offline inversion module (Sec. 2.2.1) to capture articulatory data, and a gestural VAE to extract gestural scores (Sec. 2.2.2), which are then refined through joint self-distillation with acoustic posteriors and textual priors (Sec. 2.2.3). This method ensures that the resulting gestural scores are effective and scalable dysfluent speech representation. (Evidenced in Sec. 6)

#### 2.2.1 Universal Acoustic to Articulatory Inversion (UAAI)

Since the real articulatory data are typically unavailable, we employ a state-of-the-art acoustic-to-articulatory inversion (AAI) model  pretrained on MNGU0 . The model takes 16kHz raw waveform input and predicts 50Hz EMA features. Details are listed in Appendix. A.2.1.

#### 2.2.2 Gestural Variational Autoencoders

Any motion data \(X=[X_{1},X_{2},...,X_{t}]\) can be decomposed into motion kernels \(G^{T d K}\) and an activation function \(H^{K t}\) using convolutional matrix factorization (CMF) , where \(X_{i=0}^{T-1}G(i)^{i}\). Here, \(t\) represents time, \(T\) the kernel window size, \(d\) the channel size, and \(K\) the number of kernels. When \(X\) is articulatory data, \(G\) corresponds to \(K\) gestures and \(H\) to the gestural scores (Visualization in Appendix A.1 and A.1.2). This work focuses on three aspects: (1) joint modeling of articulatory-specific _duration and intensity_, (2) _self-distillation_ from both acoustic and textual data, and (3) _multi-scale_ decoding of gestures and gestural scores.

Variational InferenceWe employ point-level variational inference for \(q_{}(H|X)\), meaning for each point \((k,i)\) in \(H^{K t}\), we model its posterior \(q_{}(H^{k,i}|X)\). This approach results in \(K t\) posteriors for each gestural score \(H\), where \(k=1,,K\) and \(i=1,,t\). We use pointwise inference for gestural scores due to its properties, such as overlapping durations across articulators and stochastic variations across accents. We will refer to this as _patchwise_ rather than pointwise, as we are modeling a patch embedding for each point \((k,i)\). In practice, we introduce an additional latent vector \(Z^{k,i}^{P}\) as variational augmentation , where \(P\) is patch size. This setup formulates the duration posterior \(q_{}(D^{k,i}|Z^{k,i},X)\), intensity posterior \(q_{}(I^{k,i}|Z^{k,i},X^{k,i})\), and latent posterior \(q_{}(Z^{k,i}|X)\). Patchwise operation is detailed in Appendix A.2.2. Consequently, our gestural encoder encodes the joint posterior \(q_{}(Z^{k,i},D^{k,i},I^{k,i}|X)=q_{}(D^{k,i}|Z^{k,i},X)q_{}(I^{k,i} |Z^{k,i},X^{k,i})q_{}(Z^{k,i}|X)\).

VAE ObjectiveAfter variational inference, our decoder \(p_{}(X|H,G)=P_{}(X|D,I,G)\) reconstructs \(X\) using duration \(D\), intensity \(I\), and gesture \(G\). The evidence lower bound (ELBO) and its derivation are provided in Eq. 1 and Appendix A.4, respectively. The posterior \(q_{}(Z^{k,i}|X)\), modeled via vanilla variational inference , assumes standard normal priors for \(p(Z^{k,i})\). The mechanisms of the duration and intensity encoders, \(q_{}(D^{k,i}|Z^{k,i},X^{k,i})\) and \(q_{}(I^{k,i}|Z^{k,i},X^{k,i})\), are detailed in Sec. 2.2.2 and Sec. 2.2.2. Details on the decoder \(P_{}(X|D,I,G)\) are discussed in Sec. 2.2.2.

\[_{} =_{q_{}(Z,D,I|X)}[ p_{}(X|D,I,G)]\] \[-_{(k,i)}[ (q_{}(Z^{k,i},D^{k,i},I^{k,i}|X)\|p(Z^{k,i},D^{k,i},I^{k,i}))] \]

Duration Posterior\(q_{}(D^{k,i}|Z^{k,i},X^{k,i})\)We employ the Gumbel softmax  to reformulate the duration posterior \(q_{}(D^{k,i}|Z^{k,i},X)\). Let \(^{k,i}^{}\) denote the logits across all \(\) discrete duration classes (values) for patch \((k,i)\). For each class \(j\), we obtain Gumbel noise \(_{j}^{k,i}=-(-(U_{j}))\), where \(U_{j}(0,1)\). We then define \(_{j}^{k,i}=((_{j}^{k,i})+_{j}^{k,i})/\), where \(\) is temperature parameter. Finally, we obtain the Gumbel softmax transformation as an approximation of the duration posterior in Eq.2. We set \(p(D^{k,i})=1/\), where \(\) is the number of discrete duration classes. Background and detailed methodology can be viewed in Appendix. A.2.2.

\[q_{}(D^{k,i}\!=\!\!j|Z^{k,i},X)\!\!^{k,i} )}{_{I=1}^{}\!(_{l}^{k,i})} \]

Intensity Posterior\(q_{}(I^{k,i}|Z^{k,i},X^{k,i})\)After sampling \(I^{k,i} q_{}(I^{k,i}|Z^{k,i},X^{k,i})\), the model applies a per-gesture, region-wise impact. This can be formulated in Eq. 3. where \(H^{i-D^{k,i}/2;i+D^{k,i}/2,k}\) represents the local window of impact, \(I^{k,i}\) is the sampled impact value, and \(D^{k,i}\) is the duration of the gesture. We actually applied Sigmoid function to deliver positive intensity values. The Hann function is used to apply the impact smoothly within the local window. The motivation behind this formulation is that most patches \((k,i)\) are not activated, reflecting the sparse nature of human speech production and co-articulation . Visualizations can be checked in Appendix.A.2.2.

\[H^{i-}{2}\!:\!i+D^{}\!,k}=((I^{k,i} q_{}(I^{k,i}|Z^{k,i},X^{k,i})),D^{k,i} q_{}(D^{ k,i}|Z^{k,i},X)) \]

Online Sparse SamplingGiven the limited number of patches contributing to gestural scores , we localize the impact within a specific window. We define a _Combined Score_\(S^{k,i}=aI^{k,i}+bD^{k,i}\), where \(I^{k,i}\) and \(D^{k,i}\) represent impact and duration, respectively, and \(a\) and \(b\) are hyperparameters. This score ranks the importance of each patch, with indices for each gesture computed as \(r_{row}(k,i)=(-S^{k,i}k)\). Setting \(m_{row}\) as the number of patches selected, we apply a sparse mask \(M_{row}\) (Eq. 4) to derive the final sparse gestural scores, detailed in Eq. 4. This entire online sparse sampling process is differentiable. The parameters \(a\), \(b\), and \(m_{row}\) are elaborated in the Appendix. For simplicity, we denote this process as \((i,k)\), with visualizations in Appendix A.2.3.

\[^{k,i}=M_{row}^{k,i} H^{k,i} 28.452756pt  14.226378ptM_{row}^{k,i}=1&r_{row}(k,i) m_{row}\\ 0& \]

Multi-scale Gestural DecoderThe decoder reconstructs \(=[_{1},_{2},...,_{t}]^{d t}\) from gestures \(G^{T d K}\) and gestural scores \(^{K t}\). In this work, we retain the CMF operation  and extend it to multiple deep layers. We also introduce multi-scale mechanism, which has proven to be a robust tokenizer for various speech tasks . Denote: \(f^{1/2}_{,}\), \(f^{1/4}_{,}\), \(f^{2}_{,}\), \(f^{4}_{,}\) as downsample/upsample modules with scales of \(1/2\) or \(1/4\). The convolutive matrix factorization operator \(*\) means \(_{i=0}^{T-1}(i)^{i}\) where \(^{T d K}\) and activation function \(^{K t}\). Then our multi-scale decoder is defined in Eq. 5, where \(r=1\) means no resolution change, and \(f_{}\) represents any neural network, details of which can be found in the Appendix. Up to this point, \(p_{}(X|D,I,G)\) (Eq. 1) is defined. We provide more details in Appendix A.2.4.

\[=_{r\{1,2,4\}}f^{r}_{,}(f_{, }(G*f^{1/r}_{,}())) \]

#### 2.2.3 Gestural Scores as Phonetic Representations

After obtaining gestural scores, we predict phoneme alignment for dysfluency modeling. For clean speech, alignment is acquired using the Montreal Forced Aligner (MFA) , while for dysfluentspeech, it is simulated (see Section 5). The direct prediction of phoneme alignment from handcrafted features or self-supervised learning (SSL) units  is limited due to scalability issues with dysfluent speech, discussed further in Sec. 6. We utilize 4X downsampled gestural scores (from decoding), denoted as \(\), matching the resolution of acoustic features . Let \(=[_{1},_{2},,_{t^{}}]\) represent the phoneme alignment, where \(t^{}=t/4\). Employing the Glow algorithm , we transform \(\) into \(\), expressed as \(=f_{}^{G}()\), optimized via a softmax crossentropy objective \(_{}\).

Self-DistillationWe distill gestural scores from pretrained acoustic features , which are then adapted to match gestural scores' dimensions. Instead of directly measuring the distance between acoustic embeddings and gestural scores, we use the alignment-conditioned _gestural prior_ as an acoustic-conditioned _gestural posterior_. The reference text \(C=[C_{1},C_{2},,C_{L}]\) is processed by a text encoder to yield the latent Gaussian posterior \((_{}^{C_{1}},_{}^{C_{1}}),(_{}^{C_{2}},_{ }^{C_{2}}),,(_{}^{C_{L}},_{}^{C_{L}})\), with the gestural posterior modeled via the change of variable property \(f_{}^{G}\) as described in Eq. 6. Intuition, detailed methodology and visualization can be given in Appendix A.3.

\[p_{}(|C)=p_{}(|C)|(^{G}()}{ H})|=}_{i=1}^{t^ {}}_{j=1}^{L}(_{i};_{}^{C_{j}},( _{}^{C_{j}})^{2})|(^{G}( {H})}{})| \]

Conversely, given the acoustic embedding \(A=[A_{1},A_{2},,A_{L}]\), a text encoder is employed to output the latent Gaussian posterior \((_{}^{A_{1}},_{}^{A_{1}}),(_{}^{A_{2}},_{ }^{A_{2}}),,(_{}^{A_{t^{}}},_{}^{A_{t^{ }}})\). The posterior \(q_{}(|A)\) can be derived in a similar manner. The overall distillation loss is then presented in Eq. 7.

\[_{}=(q_{}(|A)\|p_{}( |C)), q_{}(|A)=} _{i=1}^{t^{}}_{j=1}^{t^{}}(_{i}; _{}^{A_{j}},(_{}^{A_{j}})^{2}) \]

Both \(K_{1}\) and \(K_{2}\) are normalization terms. The overall loss objective for neural variational gestural modeling is shown in Eq. 8, where \(_{1},_{2},_{3}\) are balancing factors.

\[_{}=-_{1}_{}+_{2} _{}+_{3}_{} \]

## 3 Connectionist Subsequence Aligner (CSA) for Dysfluency Modeling

### Monotonic Alignment is effective Dysfluency Aligner

Given the reference text \(C=[C_{1},C_{2},,C_{L}]\) and dysfluent phonetic alignment \(=[_{1},_{2},...,_{t^{}}]\), the alignment between \(C\) and \(\) is typically non-monotonic. For example, when people say "pl-please," it is non-monotonically aligned with "p-l-e-a-s-e." Prior work  on non-monotonic dysfluent modeling has its limitations, as discussed in Sec. 2.1. In this work, we focus on _monotonic alignment_ and argue that it is effective dysfluency aligner. The intuition is straightforward: we seek an aligner \(:\{1,2,,L\}(\{1,2,,t^{}\})\) such that for each \(i\{1,2,,L\}\), Eq. 9 holds. The aligner \(\) maps elements in \(C\) to consecutive subsequences in \(\) without overlap. This property is beneficial for dysfluency detection, as for each element in \(C\), we can determine the presence of dysfluencies such as insertion, deletion, repetition, block, replacement, etc., based on \((C_{i})\).

\[(C_{i})=[_{s_{i}},_{s_{i}+1},,_{e_{i}}]1 s_{i} e_{i} t^{}&\\ e_{i}<s_{i+1}& i\{1,2,,L-1\}\\ s_{i}<s_{i+1},e_{i}<e_{i+1}& i\{1,2,,L-1\} \]

### Local Subsequence Alignment (LSA) Achieves Semantic Dysfluency Alignment

All monotonic aligners satisfy Eq.9, which serves as a necessary condition. However, we also desire \((C_{i})\) to be semantically aligned with \(C_{i}\). Consider the aforementioned example: one preferred alignment is \(()\)=[p,l,p], indicating the presence of a stutter. In contrast, if \(()\)=[p,l,p,l,e,a,s], it becomes challenging to identify any reasonable dysfluency, despite still satisfying Eq.9. In this work, we propose that _Local Subsequence Alignment (LSA)_ is an effective approach for achieving semantically aligned \(\). Before delving into the main topic, we propose and introduce two terms: (i) _Global Sequence Aligner (GSA)_, where the cost function involves the alignment of all elements in the sequence; this includes most sequence aligners such as DTW , CTC , and MFA ; and (ii) _Local Sequence Aligner (LSA)_, where the cost function involves only a subset of elements. One representative is longest common subsequence (LCS) alignment .

IntuitionFig. 3 (left) illustrates the effectiveness of LSA as a dysfluency aligner. The reference text \(C\), a stress-free phoneme transcription  of word "references", contrasts with the dysfluent phonetic alignment \(\), which includes impairments like insertions of fillers and repetitions. LCS (LSA,) and DTW (GSA,) results are depicted in red and blue, respectively. LSA alignment \(^{}(C_{i})\) shows higher semantic alignment with \(C_{i}\) compared to DTW's \(^{}(C_{i})\), which includes misaligned elements like an unwarranted alignment of "F". LSA's superiority stems from its cost function, which updates only for matching dysfluency-aware boundaries, while DTW updates for all pairs, often unrelated to dysfluency boundaries. Detailed analysis are available in Appendix A.7.

Problem StatementTaking LCS into our framework presents three challenges: _First_, the high dimensionality of \(C\) and \(\) requires suitable emission and transition probability models. _Second_, LCS cost function is non-differentiable. _Third_, multiple LCS alignments necessitate effective modeling of joint distribution. To address these, we introduce _Connectionist Subsequence Aligner_ (CSA).

### Connectionist Subsequence Aligner (CSA) Formulation

ObjectiveFrom gestural score \(\), we obtain phonetic alignment \(=f_{}^{G}()=[_{1},_{2},,_{t^{}}]\). In practice, both \(\) and \(C\) are embeddings instead of explicit labels, where \(C=[C_{1},...,C_{L}]\) are sampled from the text encoder \((_{}^{C_{i}},(_{}^{C_{i}})^{2})\), \(i=1,...,L\), as proposed in Sec.2.2.3. Let \(t^{}\) denote the sequence length after removing duration from the original length \(t^{}\). Duration will be reincorporated post-alignment. The alignment between \(C\) and \(\) is already defined in Eq.9. We introduce another notation \(\), where \((_{i})\) is the aligned token in \(C\). \(()=[(_{1}),...,(_{t^{}})]\) represents the final alignment with respect to \(C\), in comparison to alignment \((C)\), which is with respect to \(\).

There are possibly multiple (\(N\)) alignments \(^{}_{j}(C)\), where \(j=1,...,N\). Our goal is to optimize model \(\) to obtain the largest joint distribution of alignments \(_{j=1}^{N}^{}_{j}(C)\). However, unlike CTC , we can't search alignments explicitly as the monotonic constraints are different. We propose approximating LSA. Let \(^{{}^{}}()\) be one approximate LSA alignment, and assume there are \(N\) possible LSA alignments: \(^{{}^{}}_{j}()\) where \(j=1,...,N\). Our final objective is formulated in Eq. 10.

\[_{}}_{C,}_{j=1}^{N}p_{}(^{ }_{j}(C)|C,)=_{}}_{C,}_{j=1} ^{N}p_{}(^{}_{j}()|C,)_{} }_{C}_{j=1}^{N}p_{}(^{{}^{}}_{j}( )|) \]

Approximate LSA Alignments \(^{{}^{}}()\)We define \(y^{i,j}\) as the emission probability \(p(C_{j}|_{i})\), and transition probability \(p_{}(_{j}|_{i})\). Let \(C^{S}_{j}\) denote the embedding sampled from the distribution \((_{}^{C_{j}},(_{}^{C_{j}})^{2})\) (Sec.2.2.3). The emission probability is given in Eq. 11. We approximate the transition probability using a separate neural network \(p_{}(_{j}|_{i})\).

\[y^{i,j}=p_{}(C_{j}|_{i}) C^{S}_{j}} }{(_{k=1}^{L}^{_{i} C^{S}_{k}})} \]

Figure 4: CSA

Figure 3: LSA(LCS) delivers dysfluent alignment that is more semantically aligned.

It is possible to list all LCS alignments \(^{}_{j}()\), where \(i=1,...,N\), via soft alignments , which are also differentiable. However, we propose that by simply introducing the LCS constraint on the vanilla CTC  objective, the LCS can be implicitly applied, which we call the _Connectionist Subsequence Aligner_ (CSA). Let us consider Figure 4 (left) for intuition. For a single alignment \(^{}_{j}()\), the emission probability and transition probability will only be applied if \(C_{i}\) is already aligned (\(C_{1}\) in the figure). We refer to these as _Transition Skip_ and _Emission Copy_. Now, let us move to the LCS-constrained forward-backward algorithm . Taking the forward algorithm (Figure 4 (mid)) for illustration, _Emission Copy_ is reflected in \(_{i,j}\) via an identity multiplier on \(_{i-1,j}\). _Transition Skip_ is reflected on both \(_{i-1,j}\) and \(_{i-1,j-1}\), where we apply a transition on \(_{i-1,j-1}\). This also implicitly leverages language modeling. We also consider all previous tokens \(C_{j-2},...,C_{j-k},...,C_{0}\); however, no transition is applied, but a discounted factor \(^{k}\) is utilized instead. This indicates a significant jump (deletion), which we denote as a dysfluency module, although all other modules model dysfluencies equally. Forward and backward algorithms are displayed in Eq. 12 and Eq. 13.

\[^{i,j}_{} =^{i-1,j}_{}+_{k=1}^{j}^{k}^{i-1,j-k} _{} y^{i,j}p_{}(C^{S}_{j-1}|C^{S}_{j}) _{\{k=1\}}+_{\{k 1\}} \] \[^{i,j}_{} =^{i+1,j}_{}+_{k=1}^{t^{}-j}^{k}^{ i+1,j+k}_{} y^{i,j}p_{}(C^{S}_{j}|C^{S}_{j+1}) _{\{k=1\}}+_{\{k 1\}} \]

We initialize \(_{1,1}=_{t^{},L}=1\), \((i,1)=0 i>0\), \((i,1)=0 i<t^{}\), and \((1,j)=0 j<L\). Our CSA objective is displayed in Eq. 14, where we take the summation over all reference tokens and time stamps.

\[_{}=-}_{C,}_{j=1}^{N}p_{ }(^{}_{j}()|)=-_{i=1}^{t^{}}_{j=1}^{L }_{}^{i,j}_{}}{y^{i,j}} \]

SamplingAs the alignment \(^{}()\) is required for the next module, it is necessary to sample it during training. Traditional beam search methods are impeded by reduced inference speeds. To mitigate this, we employ the Longest Common Subsequence (LCS) algorithm offline on \(C^{e}\) and \(^{e}\) to derive the alignments. The final alignment is denoted as \((C^{S}_{i})=[^{S}_{s_{i}},,^{S}_{e_{i}}]\), as presented in Eq. 9. This methodology yields a sequence of inputs in the form of CSA-O = [(\(C^{S}_{1},(C^{S}_{1})\)),..., (\(C^{S}_{L},(C^{S}_{L})\))].

## 4 Language Models and Overall Training Objective

Following LTU , we utilize speech representations (alignment) \([(C^{S}_{1},(C^{S}_{1})),,(C^{S}_{L},(C^{S}_{L}))]\) (Sec. 3.3), along with word-level timestamps, reference text \(C\), and instruction \(C^{I}\), as input to LLaMA-7B . During the training process, we incorporate annotations that include per-word disfluency with timestamps. Our approach strictly adheres to the procedures outlined in  and employs Vicuna instruction tuning  with LoRA . As this is not our core contribution, we provide details in Appendix A.8. We use the same autoregressive training objective as , denoted as \(_{}\). The overall loss objective for SSDM is shown in Eq. 15.

\[_{}=_{}+_{}+ _{} \]

## 5 Libri-Dys: Open Sourced Dysfluency Corpus

Traditional rule-based simulation methods  operate in acoustic space, and the generated samples are not naturalistic. We developed a new pipeline that simulates in text space. To achieve this, we first convert a sentence into an IPA phoneme sequence. Then, we develop TTS rules for phoneme editing to simulate dysfluency, providing five types of dysfluency: Repetition(phoneme & word), Missing(phoneme & word), Block, Replacement and Prolongation. These rules are applied to the entire LibriTTS dataset , allowing the voice of generated speech to vary from the 2456 speakers included in the LibriTTS. The TTS-rules, entire pipeline, dataset statistics, MOS evaluation and phoneme recognition results are available in Appendix A.9. Overall Libri-Dys is 7X larger than LibriTTS, with a total size of 3983 hours. Data is opensourced at [https://bit.ly/4aoLdUU](https://bit.ly/4aoLdUU).

Experiments

### Data Setup

For training, we use VCTK++ and Libri-Dys datasets. For testing, we randomly sample 10% of the training data. Additionally, we incorporate nfvPPA data from our clinical collaborations, which includes 38 participants--significantly more than the 3 speakers in prior studies [1; 2]. It is approximately 1 hour of speech. Further details are provided in Appendix A.10.1.

### Experiments Setup

The neural gestural VAE (Eq.8), CSA (Eq.14), and language modeling components are trained sequentially, with each stage completed before the next begins. Subsequently, we perform end-to-end learning to implement curriculum learning. Our objective is to evaluate the dysfluent intelligibility and scalability of our proposed _gestural scores_, as well as the dysfluency detection performance of each proposed module. We evaluate phonetic transcription and alignment using the framewise **F1 Score** and Duration-Aware Phoneme Error Rate (**dPER**). The F1 Score measures how many phonemes are correctly predicted, while dPER extends the traditional Phoneme Error Rate (PER) by assigning specific weights to different types of errors. For dysfluency evaluation, besides F1 Score, we also report the time-aware Matching Score (**MS**), which measures both type and temporal accuracy, with temporal matching considering the Intersection over Union (IoU) threshold of 0.5. Detailed training configurations can be found in Appendix A.12.

### Scalable Intelligibility Evaluation

We evaluate phonetic transcription (forced alignment) performance using simulated data from VCTK++ and our proposed Libri-Dys dataset. The framewise F1 score and dPER are used as evaluation metrics. Five types of training data are used: VCTK++, LibriTTS (100%, ), Libri-Dys (30%), Libri-Dys (60%), and Libri-Dys (100%). HuBERT  SSL units and H-UDM alignment (WavLM ) fine-tuned with MFA  targets are adopted. Additionally, we examine Gestural Scores (GS). GS-only refers to gestural VAE training (Eq.1), GS w/o dist excludes \(_{}\), and GS w/ dist includes it, following Eq.8. Results are presented in Table 1. H-UDM consistently outperforms HuBERT due to the WavLM backbone. Gestural scores from Eq. 1 show inferior results due to sparse sampling. However, GS demonstrates better scalability compared to SSL units. Using phoneme alignment loss \(_{}\) significantly increases intelligibility, matching SSL unit results. GS outperforms SSL units with more training data. The inclusion of the self-distillation objective yields the best performance and scalability. Scaling factors SF1 for F1 score and SF2 for dPER are computed as \((c-b) 0.3+(b-a) 0.4\) for results [a, b, c] from Libri-Dys [30%, 60%, 100%]. In terms of intelligibility, Gestural Score delivers the best scalability.

   Method & Eval Data & F1 (\%), 1 & M5 (\%), 1 & F1 (\%), 1 & M5 (\%), 1 & F1 (\%), 1 & M5 (\%), 1 & F1 (\%), 1 & F1 (\%), 1 & M5 (\%), 1 & F1 (\%), 1 & SF2 (\%), 1 \\  Training Data & VCTK++ & _LibriTTS (100\%)_ & _Libri-Dys (60\%)_ & _Libri-Dys (60\%)_ & _Libri-Dys (60\%)_ & _Libri-Dys (100\%)_ & & \\ HuBERT  Meta-Dys & 86.2 & 50.3 & 88.2 & 47.4 & 87.2 & 42.3 & 87.2 & 43.4 & 87.8 & 42.9 & 0.18 & 0.29 \\ VCTK++ & 91.2 & 59.3 & 91.0 & 38.8 & 97.0 & 39.0 & 91.3 & 39.9 & 90.9 & 0.2 & 0.12 & 0.45 \\ Label-Dys & 88.1 & 44.5 & 88.9 & 45.6 & 88.0 & 43.3 & 88.5 & 43.3 & 88.9 & 43.0 & 0.32 & -0.09 \\ VCTK++ & 88.1 & 41.9 & 88.1 & 42.2 & 88.3 & 41.9 & 88.9 & 41.9 & 89.4 & 40.7 & 0.39 & -0.36 \\ Libri-Dys & 88.7 & 44.5 & 85.0 & 43.3 & 85.5 & 43.0 & 83.7 & 42.2 & 86.5 & 41.5 & 0.32 & -0.53 \\ U-Dys & 91.4 & 39.0 & 91.6 & 38.5 & 91.5 & 38.8 & 92.0 & 37.2 & 92.6 & 37.1 & 0.38 & -0.67 \\ Libri-Dys & 88.0 & 42.4 & 88.3 & 41.9 & 88.7 & 41.0 & 88.9 & 39.4 & 90.0 & 39.0 & 0.11 & **-0.76** \\ VCTK++ & **91.5** & **39.0** & **91.7** & **38.3** & **91.7** & **38.6** & **92.1** & **37.0** & **93.0** & **37.0** & 0.43 & -0.64 \\ Libri-Dys & **88.2** & **40.9** & **88.9** & **40.9** & **89.0** & **40.8** & **50.2** & **39.0** & **90.8** & **39.0** & **0.86** & -0.72 \\   

Table 1: Scalable Dysfluent Phone Transcription Evaluation

   Method & Eval Data & F1 (\%), 1 & M5 (\%), 1 & F1 (\%), 1 & M5 (\%), 1 & F1 (\%), 1 & M5 (\%), 1 & F1 (\%), 1 & M5 (\%), 1 & F1 (\%), 1 & M5 (\%), 1 & SF2 (\%), 1 \\  Training Data & VCTK++ & _LibriTTS (100\%)_ & _Libri-Dys (60\%)_ & _L

### Scalable Dysfluency Evaluation

We follow  by using F1 (type match) and MS (matching score). The matching score is defined as follows: if the IoU (Intersection over Union) between the predicted time boundary and the annotations is greater than or equal to 0.5, and the type also matches, it is considered detected. We use H-UDM , the current state-of-the-art time-aware dysfluency detection model, as the baseline. Under our SSDM framework, we include several ablations: (1) We remove LLaMA and use a template matching algorithm  on top of CSA alignments; (2) We replace CSA with softDTW ; (3) We replace gestural scores with WavLM  units; (4) We adopt curriculum training, first training the gestural VAE, CSA, and LLaMA separately, then training them end-to-end. For language model outputs, we set the prompt and use  to automatically extract both types and time information from the response.The results in Table 2 show similar trends in terms of both performance and scalability (SF1 and SF2). Notably, we observe that LLaMA modeling does not contribute significantly, while both gestural scores and CSA (especially the latter) contribute the most. t is also noted that dysfluent phonetic intelligibility, as shown in Table 1, is highly correlated with detection performance.

### State-of-the-art Dysfluency Detection

We select the optimal configuration and compare it with state-of-the-art speech understanding systems. For fair comparison, we fine-tune LTU-AS-13B  and SALMONN-13B  using the same instructions but with pure speech input (AST  for LTU-AS and Whisper  for SALMONN). Additionally, we attach a time embedding to model temporal aspects. Detailed information is available in Appendix A.8. We also test on real nfvPPA speech, with results presented in Table 3. Current large-scale models  show limited performance in dysfluent speech detection, as shown in Fig. 1. The detection of nfvPPA speech remains challenging due to the significant gap between simulated and real disordered speech. See our demo at [https://berkeley-speech-group.github.io/SSDM/](https://berkeley-speech-group.github.io/SSDM/).

### Dysfluency Visualization

We attempt to visualize dysfluency in gestural space, as shown in Fig. 5. The correct text is "please" (p l i: z), while the real dysfluent speech is (p l e z). We apply GradCAM  to visualize the gradient of gestural scores \(H\), shown in the right figure. We select the specific gestural scores corresponding to the vowel 'i' (e), and then visualize the corresponding gesture. On the gestural score, the gradient is negative in the center, indicating that the tongue is attempting to move down, which is the incorrect direction for articulation. This observation is meaningful as it provides insight into the dysfluency. Our system also offers explainability and has the potential to serve as a more interactive language learning tool.

## 7 Limitations and Conclusions

In this work, we proposed SSDM (Scalable Speech Dysfluency Modeling), which outperforms the current best speech understanding systems by a significant margin. However, there are still several limitations. First, we utilize LLMs, whose contribution is marginal and whose potential has not been fully leveraged. We suspect this is due to the granularity of tokens, and we believe it would be beneficial to develop a phoneme-level language model to address this issue. Second, the current data scale is still inadequate, which is further constrained by computing resources. Third, we believe that learnable WFST  could provide a more efficient and natural solution to this problem, yet it has not been extensively explored. Fourth, it is worthwhile to explore representations based on real-time Magnetic Resonance Imaging (rtMRI)  or gestural scores . These approaches might enable the avoidance of the distillation process. Recent concurrent works have been focusing on region-based  and token-based  approaches. It would be useful to explore the combination of these to leverage advantages on each side.

Figure 5: Gestural Dysfluency Visualization

  Eval Data & LTU-AS-13B  & LTU-AS-13B-FT & SALMONN-13B  & SALMONN-13B-FT & ChaMFOT  & SSDM & SSDM & SSDM w/Cari \\    & F1(\%, 1) & MSc(\%, 1) & F1(\%, 1) & MSc(\%, 1) & F1(\%, 1) & MSc(\%, 1) & F1(\%, 1) & MSc(\%, 1) & F1(\%, 1) & MSc(\%, 1) \\  & 7.2 & 0 & 12.2 & 1.7 & 7.3 & 0 & 14.2 & 0.5 & 25.3 & 0 & 89.2 & 70.2 & **90.0** & **71.9** \\  & 8.9 & 0 & 9.7 & 1.7 & 7.7 & 0 & 11.0 & 2.5 & 18.3 & 0 & 81.4 & 70.4 & **81.6** & **71.0** \\  & 0 & 0 & 2.4 & 0 & 0 & 0 & 1.8 & 0 & 5.6 & 0 & 69.2 & 54.2 & **69.9** & **85.0** \\  

Table 3: Detection results from state-of-the-art models.