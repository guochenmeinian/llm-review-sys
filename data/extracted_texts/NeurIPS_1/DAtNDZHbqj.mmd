# Variational Delayed Policy Optimization

Qingyuan Wu

University of Southampton

&Simon Sinong Zhan

Northwestern University

&Yixuan Wang

Northwestern University

&Yuhui Wang

King Abdullah University of Science and Technology

&Chung-Wei Lin

National Taiwan University

&Chen Lv

Nanyang Technological University &Qi Zhu

Northwestern University &Chao Huang

University of Southampton

Equal ContributionCorrespondence to: Chao Huang, chao.huang@soton.ac.uk

###### Abstract

In environments with delayed observation, state augmentation by including actions within the delay window is adopted to retrieve Markovian property to enable reinforcement learning (RL). However, state-of-the-art (SOTA) RL techniques with Temporal-Difference (TD) learning frameworks often suffer from learning inefficiency, due to the significant expansion of the augmented state space with the delay. To improve learning efficiency without sacrificing performance, this work introduces a novel framework called Variational Delayed Policy Optimization (VDPO), which reformulates delayed RL as a variational inference problem. This problem is further modelled as a two-step iterative optimization problem, where the first step is TD learning in the delay-free environment with a small state space, and the second step is behaviour cloning which can be addressed much more efficiently than TD learning. We not only provide a theoretical analysis of VDPO in terms of sample complexity and performance, but also empirically demonstrate that VDPO can achieve consistent performance with SOTA methods, with a significant enhancement of sample efficiency (approximately 50% less amount of samples) in the MuJoCo benchmark. Code is available at [https://github.com/QingyuanWuNothing/VDPO](https://github.com/QingyuanWuNothing/VDPO).

## 1 Introduction

Reinforcement learning (RL) has achieved considerable success across various domains, including board game , video game , cyber-physical systems . Most of these achievements lack stringent timing constraints, and, therefore, overlook delays in agent-environment interaction. However, delays are prevalent in many real-world applications stemming from various sensors, computation, etc, and significantly affect learning efficiency , performance , and safety . While observation-delay, action-delay, and reward-delay  are all crucial, observation-delay receives the most attention . Unlike reward-delay, observation-delay, which is proved to be a superset of action-delay , disrupts the Markovian property inherent to the environments. In this work, we focus on the reinforcement learning with a constant observation-delay \(\): at any time step \(t\), the agent can only observe the state \(s_{t-}\), without access to states from \(t-+1\) to \(t\).

[MISSING_PAGE_FAIL:2]

Delayed MDP.A delayed RL problem with a constant delay is originally not an MDP, but can be reformulated as a delayed MDP with Markov property based on the augmentation approaches [4; 19]. Assuming the constant delay being \(\), the delayed MDP is denoted as a tuple \(,,_{},_{},, _{}\), where the augmented state space is defined as \(:=^{}\) (e.g., an augmented state \(x_{t}=\{s_{t-},a_{t-},,a_{t-1}\}\)), \(\) is the action space, the delayed transition function is defined as \(_{}(x_{t+1}|x_{t},a_{t}):=(s_{t-+1}|s_{t- },a_{t-})_{a_{t}}(a^{}_{t})_{i=1}^{-1} _{a_{t-i}}(a^{}_{t-i})\) where \(\) is the Dirac distribution, the delayed reward function is defined as \(_{}(x_{t},a_{t}):=_{s_{} b(|x_ {t})}[(s_{t},a_{t})]\) where \(b\) is the belief function defined as \(b(s_{t}|x_{t}):=_{}_{i=0}^{-1}(s_{t -+i+1}|s_{t-+i},a_{t-+i})s_{t-+i+1}\), the initial augmented state distribution is defined as \(_{}=_{i=1}^{}_{a_{t-i}}\).

Variational RL.Formulating the RL problem as a probabilistic inference problem  allows us to use extensive optimization tools in solving the RL problem. From the existing variational RL literature [30; 36], we usually define \(O=1\) as the optimality of the task (e.g., the trajectory \(\) obtains the maximum return). Then the probability of trajectory optimality can be represented as \(p(O=1|)\). Then, the objective of variational RL becomes finding policy \(\) with highest log evidence: \(_{} p_{}(O=1)\). Then, we can derive the lower bound of \( p_{}(O=1)\) by introducing a prior knowledge of trajectory distribution \(q()\).

\[ p_{}(O=1)_{ q()}[ p(O=1|) ]-(q()||p_{}())=(,q), \]

where KL is the Kullback-Leibler (KL) divergence and \((,q)\) is the evidence lower bound (ELBO) [2; 30]. The objective of variational RL is maximizing the ELBO, which can be achieved by various optimization techniques [1; 2; 9; 30].

## 3 Our Approach: Variational Delayed Policy Optimization

In this section, we present a new delayed RL approach, Variational Delayed Policy Optimization (VDPO) from the perspective of variational inference. By viewing the delayed RL problem as a variational inference problem, VDPO can utilize extensive optimization tools to address sample complexity and performance issues properly. We first illustrate how to formulate delayed RL as the probabilistic inference problem with an elaborated optimization objective. Subsequently, we theoretically show that the inference problem is equivalent to a two-step iterative optimization problem. Then, we present the framework of VDPO along with the practical implementation.

### Delayed RL as Variational Inference

Delayed RL can be treated as an inference problem: given the desired goal \(O\), and starting from a prior distribution over trajectory \(\), the objective is to estimate a posterior distribution over \(\) consistent with \(O\). The posterior can be formulated by a Boltzman like distribution \(p(O=1|)(()}{})\)[2; 31] where \(\) is the temperature factor. Based on the above definition, the optimization objective of delayed RL can be defined as follows.

\[_{_{}} p_{_{}}(O=1)=_{_{}} p(O= 1|)p_{_{}}(), \]

where \(p_{_{}}(O=1)\) is the probability of the optimality of the delayed policy \(_{}\), and \(p_{_{}}()\) is the trajectory distribution induced by \(_{}\). Based on Eq. (1) and Eq. (2), we can also show that the ELBO for optimization purpose is as follows (derivation of Eq. (3) can be found in Appendix B).

\[ p_{_{}}(O=1)_{ p_{}()} [ p(O=1|)]}_{A}-(p_{}( )||p_{_{}}())}_{B}=(,_{}), \]

where \(p_{}()\) is the trajectory distribution induced by an newly-introduced _reference policy_\(\). As shown in Eq. (3), we transform the original optimization problem as a two-step iterative optimization problem: maximizing term \(A\) while minimizing term \(B\). Next, we detail how our VDPO optimizes objectives \(A\) and \(B\) separately.

#### 3.1.1 Maximizing the performance of reference policy by TD Learning

In this section, we discuss the treatment of term \(A\) in Eq. (3) and investigate the performance and sample complexity of reference policy \(\) under different MDP settings. Maximizing term \(A\) in Eq. (3) is equivalent to maximizing the performance of \(\) as follows.

\[_{}_{ p_{}()}[ p(O=1|) ]=_{}_{ p_{}()}[() ]. \]

For Eq. (4), we can train the reference policy \(\) in various MDPs with different delays or even delay-free settings. We show that the performance (Lem. 3.1) and sample complexity (Lem. 3.2) of reference policy \(\) are correlated to the specific MDP setting. Based on existing literature  and motivated by existing works , VDPO chooses training the reference policy in the delay-free MDP for gaining the edge in terms of **performance** and **sample complexity**.

**Performance:** Lem. 3.1 indicates that the performance of the optimal policy is likely decreased by increasing delays. This motivates us to learn the reference policy in the delay-free MDP for proper performance.

**Lemma 3.1** (Performance in delayed MDP, Theorem 4.3.1 in ).: _Let \(_{1},_{2}\) be two constant delayed MDPs with respective delays \(_{1},_{2}(_{1}<_{2})\). For the optimal policies in \(_{1},_{2}\), we have \(_{1}^{*}_{2}^{*}\)._

**Sample Complexity:** Furthermore, for a specific TD-based delayed RL method (e.g., model-based policy iteration), delays also affect its sample efficiency as stated in Lem. 3.2 that stronger delays will lead to much higher sample complexity, resulting in relative learning inefficiency. Therefore, learning the delay-free reference policy makes VDPO superior in sample complexity compared to learning under delay settings.

**Lemma 3.2** (Sample complexity of model-based policy iteration, Theorem 2 in ).: _Let \(\) be the constant delayed MDP with delays \(\). Model-based policy iteration finds an \(\)-optimal policy with probability \(1-\) using sample size \((|||}{(1-)^{3}^{2}} ),\) where \(||=||||^{}\)._

Based on the above analysis and inspired by the existing work , VDPO adopts a delay-free policy as the reference policy. More rigorous analyses are presented in Sec. 3.2, and we will detail the practical implementation in Sec. 3.3.

#### 3.1.2 Minimizing the behaviour difference by Behaviour Cloning

With a fixed reference policy \(\), minimizing term \(B\) in Eq. (3) can be treated as behaviour cloning at the trajectory level. However, behaviour cloning at the trajectory level is relatively inefficient compared with training at the state level as we have to collect an entire trajectory before training. We next show that we can directly minimize the state-level KL divergence \(((a_{t}|s_{t})||_{}(a_{t}|x_{t}))\) as presented in Proposition 3.3.

**Proposition 3.3** (State-level KL divergence, proof in Proposition C.1).: _For a fixed reference policy \(\), the trajectory-level KL divergence can be reformulated to state-level KL divergence as follows._

\[(p_{}()||p_{_{}}())= ^{} d^{}(s_{t})((a_{t}|s_{t})|| _{}(a_{t}|x_{t}))s_{t}}_{}+ Const., \]

_where \(Const.=\)_KL\(((s_{0})||_{}(x_{0}))\)__

\[+_{t=0}^{} d^{}(s_{t})(a_{ t}|s_{t})((s_{t+1}|s_{t},a_{t})||b(s_{t}|x_{t})_{ }(x_{t+1}|x_{t},a_{t}))a_{t}s_{t}.\]

Since transition dynamics, initial state distributions, and reference policy are all fixed at this point, we can minimize the state-level KL divergence instead of the trajectory-level KL divergence for efficient training, and then the optimization objective becomes as follows.

\[_{_{}}(p_{}()||p_{_{}}()) _{_{}}((a_{t}|s_{t})||_{}(a_{t}|x_{t})). \]

In this way, VDPO divides the delayed RL problem into two separate optimization problems including Eq. (4) and Eq. (6). How to practically implement VDPO to solve these optimization problems will be presented in Sec. 3.3.

### Theoretical Property Analysis

Next, we explain why our VDPO achieves better sample efficiency compared with conventional delayed RL methods, followed by performance analysis of VDPO.

Sample Complexity Analysis.In fact, VDPO can use any delay-free RL method to improve the performance of the reference policy (maximizing \(A\)). Here, we assume that VDPO maximizes \(A\) by the model-based policy iteration, and the sample complexity of maximizing \(A\) is \((|||}{(1-)^{4}^{2 }})\) as described in Lem. 3.2. And minimizing \(B\) in VDPO is equivalent to state-level behaviour cloning which has the sample complexity of \((|||}{(1-)^{4}^{2 }})\) as stated in Lem. 3.4.

**Lemma 3.4** (Sample complexity of behaviour cloning, Theorem 15.3 in ).: _Given the demonstration from the optimal policy, behaviour cloning finds an \(\)-optimal policy with probability \(1-\) using sample size \((|||}{(1-)^{4}^{ 2}})\)._

Based on Lem. 3.2 and Lem. 3.4, we can drive the sample complexity of VDPO (Lem. 3.5).

**Lemma 3.5** (Sample complexity of VDPO, proof in Lem. C.2).: _Assumed that maximizing \(A\) in Eq. (3) by model-based policy iteration while minimizing \(B\) in Eq. (3) by behaviour cloning, VDPO finds an \(\)-optimal policy with probability \(1-\) using sample size_

\[((|||}{(1-)^{3} ^{2}},|||}{(1- )^{4}^{2}})).\]

Then, based on Lem. 3.5, we show that our VDPO has better sample complexity than most TD-only methods (e.g., model-based policy iteration , Soft Actor-Critic ) as follows.

**Proposition 3.6** (Sample complexity comparison, proof in Proposition C.3).: _In the delayed MDP, as \( 0\), the sample complexity of VDPO (Lem. 3.5) is less or equal to the sample complexity of model-based policy iteration (Lem. 3.2):_

\[((|||}{(1-)^{3} ^{2}},|||}{(1- )^{4}^{2}}))(|||}{(1-)^{3}^{2}} ).\]

Proposition 3.6 tells us that VDPO can reduce the sample complexity effectively, reaching the same performance but requiring fewer samples compared to model-based policy iteration.

Performance Analysis.We investigate the convergence of the delayed policy in VDPO (Lem. 3.7) and show that VDPO can also achieve the same performance as existing SOTAs (Proposition 3.8). As mentioned above, Eq. (4) in VDPO can be solved by existing delay-free RL method (e.g., model-based policy iteration) to learn an optimal reference policy \(^{*}\). Then, we can get the convergence of the delayed policy \(^{*}_{}\) via Eq. (6).

**Lemma 3.7** (Convergence of delayed policy in VDPO, proof in Lem. C.4).: _Let \(^{*}\) be the optimal reference policy which is trained by a delay-free RL algorithm. The delayed policy \(_{}\) converges to \(^{*}_{}\) satisfying that_

\[^{*}_{}(a_{t}|x_{t})=_{s_{t} b(|x_{t})}[^{ *}(a_{t}|s_{t})], x_{t}. \]

Based on Lem. 3.7, we show that the convergence of VDPO is consistent with that of existing SOTA methods (Proposition 3.8).

**Proposition 3.8** (Consistent fixed point, proof in Proposition C.5).: _VDPO shares the same fixed point (Eq. (7)) with DIDA , BPQL  and AD-SAC  for the same delayed MDP._

Proposition 3.6 and Proposition 3.8 together illustrate that VDPO can effectively improve the sample efficiency while guaranteeing consistent performance with SOTAs .

### VDPO Implementation

In this section, we detail the implementations of VDPO, specifically the maximization Eq. (4) and the minimization Eq. (6) respectively. The pseudocode of VDPO is summarized in Alg. 1, and the training pipeline of VDPO is presented in Fig. 1.

Eq. (4) aims to maximize the performance of the reference policy \(\) in the delay-free setting, which VDPO addresses using Soft Actor-Critic . Specifically, given transition data \((s_{t},a_{t},r_{t},s_{t+1})\), SAC updates the critic \(Q_{}\) parameterized by \(\) via minimizing the soft TD error:

\[_{}[(Q_{}(s_{t},a_{t})-)^{2}], \]

where \(=r_{t}+}_{a_{t+1}_{}( |s_{t+1})}[Q_{}(s_{t+1},a_{t+1})-_{}(a_{t+1}|s_{t+1})]\) where \(_{}\) is the reference policy parameterized by \(\). And the reference policy \(_{}\) is optimized by the gradient update:

\[_{}*{}_{_{}(|s_{t})} [_{}(|s_{t})-Q_{}(s_{t},)], \]

Eq. (6) aims to minimize the state-level KL divergence between the reference policy \(\) and delayed policy \(_{}\). Note that the true state \(s_{t}\) under the delayed environment is inaccessible. Thus VDPO adopts a two-head transformer  to approximate not only the delayed policy \(_{}\), but also the belief estimator \(b\) that predicts the state \(_{t}\), as transformer shows a superior representation performance in behaviour cloning . We also discuss how different neural representations influence the RL performance later in Sec. 4.2.3. A similar transformer architecture proposed in  is adopted, which serializes the augmented state \(x_{t}=\{s_{t-},a_{t-},,a_{t-1}\}\) to \(\{(s_{t-},a_{t-i})\}_{i=}^{1}\) as the input. Based on the information bottleneck principle , the encoder needs to encode the input as embeddings with sufficient information related to the true states. Thus, the belief decoder and the policy decoder share a common encoder which is only trained while training the belief decoder, and we freeze the gradient backward of the encoder in training the policy decoder.

Specifically, for a given augmented state \(x_{t}\) and true states \(\{s_{t-+i}\}_{i=1}^{}\), the belief decoder \(b_{}\) parameterized by \(\) aims to reconstruct the states \(\{s_{t-+i}\}_{i=1}^{}\) based on the \(x_{t}\). Therefore, the belief decoder \(b_{}\) is optimized by the reconstruction loss:

\[_{}_{i=1}^{}[(b_{}^{(i)}(x_{t}),s_{t- +i})], \]

where \(b_{}^{(i)}(x_{t})\) is the \(i\)-th reconstructed state of the belief decoder \(b_{}\) and MSE is the mean square error loss. Given the reference policy \(_{}\) and the pair of augmented state and states \((x_{t},\{s_{t-+i}\}_{i=1}^{})\), the policy decoder \(_{}\) parameterized by \(\) is optimized by minimizing the KL loss:

\[_{}_{i=1}^{}[(_{}^{(i)}(|x _{t})||_{}(|s_{t-+i}))], \]

where \(_{}^{(i)}(|x_{t})\) is the \(i\)-th output of the policy decoder \(_{}\).

Figure 1: The training pipeline of VDPO.

Experimental Results

### Experiment Settings

We evaluate our VDPO in the MuJoCo benchmark . For the selection of baselines, we choose the existing SOTAs including Augmented SAC (A-SAC) , DC/AC , DIDA , BPQL  and AD-SAC . The setting of hyper-parameters is presented in Appendix A. We investigate the sample efficiency (Sec. 4.2.1) followed by performance comparison under different settings of delays (Sec. 4.2.2). We also conduct the ablation study on the representation of VDPO (Sec. 4.2.3). Each method was run over 10 random seeds. The training curves can be found in the Appendix E.

### Experimental Results

#### 4.2.1 Sample Efficiency

We first evaluate the sample efficiency in the MuJoCo with 5 constant delays. Using the performance of a delay-free policy trained by SAC, \(Ret_{df}\), as the threshold, we report the required steps to reach this threshold within 1M global steps in Table 1. From the results, we can tell that VDPO shows strong superiority in terms of sample efficiency, successfully reaching the threshold in all tasks and achieving the best sample efficiency in 7 out of 9 tasks. Specifically, VDPO only requires 0.42M and 0.67M steps to reach the threshold in _Ant-v4_ and _Humanoid-v4_ respectively, while none of the baselines can reach the threshold within 1M steps. In _Halfcheetah-v4_, _Hopper-v4_, _Pusher-v4_, _Swimmer-v4_ and _Walker2d-v4_, the steps taken by our VDPO is around 51% (ranging from 25% to 78%) of that required by AD-SAC, SOTA baseline. Based on these results, we can conclude that VDPO shows a significant advantage in sample complexity compared to other baselines. Additional experimental results of 25 and 50 constant delays are presented in Appendix D.

#### 4.2.2 Performance Comparison

The performance of VDPO and baselines are evaluated on MuJoCo with various settings and a normalized indicator \(Ret_{nor}=-Ret_{rand}}{Ret_{df}-Ret_{rand}}\), where \(Ret_{alg}\) and \(Ret_{rand}\) are the performance of the algorithm and random policy, respectively. The results of MuJoCo benchmarks with 5, 25, and 50 constant delays are shown in the Table 2, showing that VDPO and AD-SAC outperform other baselines significantly in most tasks. Overall, VDPO and AD-SAC (SOTA) show a comparable performance, which is consistent with the theoretical observation in Sec. 3.2.

#### 4.2.3 Additional Discussions

In this section, we conduct the ablation study to investigate the performance of VDPO using different neural representations. Furthermore, we explore whether VDPO is robust under stochastic delays.

Ablation Study on Representations.As mentioned in Sec. 3.3, we investigate how the choice of neural representations for belief and policy influences the performance of VDPO. Baselines include multiple-layer perceptron (MLP) and Transformer without belief decoder. The results presented in Table 3 show that the two-head transformer used by our approach yields the best performance

  Task (Delays=5) & A-SAC & DC/AC & DIDA & BPQL & AD-SAC & VDPO (ours) \\  Ant-v4 & \(\) & \(\) & \(\) & \(\) & 0.42M \\  HalfCheetah-v4 & \(\) & \(\) & \(\) & 0.99M & 0.56M & 0.44M \\  Hopper-v4 & 0.83M & 0.35M & \(\) & 0.29M & 0.12M & 0.07M \\  Humanoid-v4 & \(\) & \(\) & \(\) & \(\) & \(\) & 0.67M \\  HumanoidStandup-v4 & 0.64M & 0.35M & 0.10M & 0.09M & 0.14M & 0.14M \\  Pusher-v4 & 0.17M & 0.02M & 0.10M & 0.27M & 0.04M & 0.01M \\  Reacher-v4 & \(\) & 0.61M & 0.10M & 0.90M & 0.44M & 0.77M \\  Swimmer-v4 & \(\) & 0.94M & 0.10M & \(\) & 0.13M & 0.07M \\  Walker2d-v4 & \(\) & \(\) & \(\) & 0.52M & 0.67M & 0.25M \\  

Table 1: Amount of steps required to reach the threshold \(Ret_{df}\) in MuJoCo tasks with 5 constant delays within 1M global steps, where \(\) denotes that failed to hit the threshold within 1M global steps. The best result is in blue.

compared to other candidates. The results also confirm that an explicit belief estimator implemented by a belief decoder can effectively improve performance.

Stochastic Delays.We compare the performance in the MuJoCo with 5 stochastic delays where \(=5\) is with a probability of 0.9 and \(\) is with a probability of 0.1. The results in the Table 4 demonstrate that VDPO outperforms other baselines at most tasks under stochastic delays. Especially in the _Ant-v4_ and _Walker2d-v4_, VDPO performs approximately \(62\%\) and \(49\%\) better than the second best approach, respectively. In the _Reacher-v4_ and _Swimmer-v4_, VDPO achieves a comparative performance with the best baseline. We will conduct a theoretical analysis of VDPO under stochastic delays in the future.

Limitations and Future Works.We mainly consider deterministic benchmarks in this paper, which are commonly adopted in the SOTAs . However, the recent work ADRL  illustrates that existing approaches may have performance degeneration in stochastic environments, which can be mitigated by learning an auxiliary delayed task concomitantly. We will investigate in the future to integrate VDPO with ADRL to address stochastic applications.

   Task & Delays & A-SAC & DC/AC & DIDA & BPQL & AD-SAC & VDPO (ours) \\   & 5 & 0.18\({}_{ 0.01}\) & 0.25\({}_{ 0.05}\) & 0.89\({}_{ 0.03}\) & 0.96\({}_{ 0.03}\) & 0.72\({}_{ 0.25}\) & 1.11\({}_{ 0.04}\) \\  & 25 & 0.07\({}_{ 0.07}\) & 0.19\({}_{ 0.02}\) & 0.29\({}_{ 0.07}\) & 0.57\({}_{ 0.11}\) & 0.66\({}_{ 0.04}\) & 0.56\({}_{ 0.06}\) \\  & 50 & 0.02\({}_{ 0.04}\) & 0.19\({}_{ 0.02}\) & 0.19\({}_{ 0.05}\) & 0.38\({}_{ 0.07}\) & 0.48\({}_{ 0.06}\) & 0.46\({}_{ 0.07}\) \\   & 5 & 0.35\({}_{ 0.15}\) & 0.40\({}_{ 0.23}\) & 0.90\({}_{ 0.01}\) & 1.00\({}_{ 0.06}\) & 1.07\({}_{ 0.06}\) & 1.03\({}_{ 0.08}\) \\  & 25 & 0.04\({}_{ 0.01}\) & 0.16\({}_{ 0.07}\) & 0.12\({}_{ 0.03}\) & 0.87\({}_{ 0.04}\) & 0.71\({}_{ 0.12}\) & 0.70\({}_{ 0.17}\) \\  & 50 & 0.12\({}_{ 0.17}\) & 0.12\({}_{ 0.13}\) & 0.15\({}_{ 0.03}\) & 0.73\({}_{ 0.17}\) & 0.74\({}_{ 0.10}\) & 0.72\({}_{ 0.21}\) \\   & 5 & 1.02\({}_{ 0.28}\) & 1.16\({}_{ 0.25}\) & 0.40\({}_{ 0.40}\) & 1.25\({}_{ 0.09}\) & 1.07\({}_{ 0.30}\) & 1.22\({}_{ 0.08}\) \\  & 25 & 0.13\({}_{ 0.04}\) & 0.19\({}_{ 0.04}\) & 0.27\({}_{ 0.08}\) & 1.21\({}_{ 0.18}\) & 0.86\({}_{ 0.25}\) & 0.82\({}_{ 0.40}\) \\  & 50 & 0.04\({}_{ 0.01}\) & 0.04\({}_{ 0.01}\) & 0.09\({}_{ 0.01}\) & 0.71\({}_{ 0.13}\) & 0.72\({}_{ 0.03}\) & 0.22\({}_{ 0.04}\) \\   & 5 & 0.13\({}_{ 0.02}\) & 0.59\({}_{ 0.17}\) & 0.08\({}_{ 0.04}\) & 0.96\({}_{ 0.05}\) & 0.98\({}_{ 0.07}\) & 1.15\({}_{ 0.07}\) \\  & 25 & 0.05\({}_{ 0.01}\) & 0.04\({}_{ 0.01}\) & 0.07\({}_{ 0.00}\) & 0.12\({}_{ 0.0

## 5 Related Works

Compared to the common delay-free setting, delayed RL with disrupted Markovian property [4; 19] is closer to real-world complex applications, such as robotics [17; 26], transportation systems  and financial market trading . Existing delayed RL techniques conduct learning over either original state space (referred to as direct approach) or augmented state space (referred to as augmentation-based approach). Direct approaches enjoy high learning efficiency by learning in the original small state space. However, early approaches simply ignore the absence of Markovian property caused by delay and directly conduct classical RL techniques based on delayed observations, which distinctly suffer from serious performance drops. The subsequential improvement is to train based on unobserved instant observations, which are predicted by various generative models, e.g., deterministic generative models , Gaussian distributions , and transformers . However, the inherent approximation errors in these learned models introduce prediction inaccuracy and result in sub-optimal performance issues . To summarize, direct approaches achieve high learning efficiency, but commonly with a cost of performance degeneration.

The augmentation-based approach is notably more promising as it retrieves Markovian property via augmenting the state with the actions related to delays and thus legitimately enables RL techniques over the yielded delayed MDP [4; 19]. However, the augmentation-based approach works in a significantly larger state space, which is thus plagued by the curse of dimensionality, resulting in learning inefficiency. To mitigate this issue, DC/AC  leverages the multi-step off-policy technique to develop a partial trajectory resampling operator to accelerate the learning process. Based on the dataset aggregation technique, DIDA  generalizes the pre-trained delay-free policy into an augmented policy. Recent attempts [20; 39] evaluate the augmented policy by a non-augmented Q-function for improving learning efficiency. ADRL  suggests introducing an auxiliary delayed task with changeable auxiliary delays for the trade-off between the learning efficiency and performance degeneration in the stochastic MDP. However, these approaches still suffer from the sample complexity issue due to the fundamental challenge of TD learning in high dimensional state space.

The conceptualization of RL as an inference problem has gained attraction recently, allowing the adaption of various optimization tricks to enhance RL efficiency [11; 15; 21; 31]. For instance, VIP  integrates different projection techniques into the policy search approach based on variational inference. Virel  introduces a variational inference framework that reduces the actor-critic method to the Expectation-Maximization (EM) algorithm. MPO [1; 2] is a family of off-policy entropy-regularized methods in the EM fashion. CVPO  extends MPO to the safety-critical settings. The novel trial in this work of viewing the delayed RL as a variational inference problem allows us to use extensive optimization tools to address the sample complexity issue in delayed RL.

## 6 Conclusion

This work explores the challenges of RL problems in environments with inherent delays between agent interactions and their consequences. Existing delayed RL methods often suffer from learning inefficiency as temporal-difference learning in the delayed MDP with high dimensional augmented state space demands an increased sample size. To address this limitation, we present VDPO, a new delayed RL approach rooted in the variational inference principle. VDPO redefines the delayed RL problem into a two-step iterative optimization problem. It alternates between (1) maximizing the

  Tasks & A-SAC & DC/AC & DIDA & BPQL & AD-SAC & VDPO (ours) \\  Ant-v4 & \(0.18_{ 0.01}\) & \(0.27_{ 0.02}\) & \(0.55_{ 0.8}\) & \(0.58_{ 0.12}\) & \(0.69_{ 0.17}\) & \(1.12_{ 0.04}\) \\  HalfCheetah-v4 & \(0.36_{ 0.12}\) & \(0.36_{ 0.18}\) & \(0.75_{ 0.02}\) & \(0.76_{ 0.16}\) & \(1.03_{ 0.06}\) & \(1.07_{ 0.09}\) \\  Hopper-v4 & \(0.85_{ 0.22}\) & \(0.94_{ 0.29}\) & \(0.31_{ 0.08}\) & \(0.68_{ 0.34}\) & \(1.05_{ 0.22}\) & \(1.35_{ 1.1}\) \\  Humanoid-v4 & \(0.15_{ 0.06}\) & \(0.67_{ 0.18}\) & \(0.07_{ 0.01}\) & \(0.40_{ 0.42}\) & \(0.97_{ 0.07}\) & \(1.06_{ 0.00}\) \\  HumanoidStandup-v4 & \(1.03_{ 0.05}\) & \(1.20_{ 0.08}\) & \(1.00_{ 0.00}\) & \(1.10_{ 0.07}\) & \(1.26_{ 0.07}\) & \(1.27_{ 0.01}\) \\  Pusher-v4 & \(1.11_{ 0.02}\) & \(1.17_{ 0.02}\) & \(1.02_{ 0.01}\) & \(1.07_{ 0.05}\) & \(1.22_{ 0.01}\) & \(1.34_{ 0.05}\) \\  Reacher-v4 & \(0.98_{ 0.01}\) & \(1.02_{ 0.01}\) & \(1.02_{ 0.00}\) & \(0.85_{ 0.11}\) & \(1.05_{ 0.01}\) & \(1.01_{ 0.04}\) \\  Swimmer-v4 & \(0.82_{ 0.10}\) & \(1.47_{ 0.58}\) & \(1.03_{ 0.02}\) & \(1.53_{ 0.52}\) & \(2.36_{ 0.64}\) & \(2.13_{ 0.18}\) \\  Walker2d-v4 & \(0.68_{ 0.28}\) & \(0.89_{ 0.08}\) & \(0.54_{ 0.09}\) & \(0.59_{ 0.30}\) & \(0.63_{ 0.39}\) & \(1.33_{ 0.11}\) \\  

Table 4: Normalized Performance \(Ret_{nor}\) in MuJoCo tasks with 5 stochastic delays for 1M global steps, where \(\) denotes the standard deviation. The best result is in blue.

performance of the reference policy by temporal-difference learning in the delay-free setting and (2) minimizing the KL divergence between the reference and delayed policies by behaviour cloning. Furthermore, our theoretical analysis and the empirical results in the MuJoCo benchmark validate that VDPO not only effectively improves the sample efficiency but also maintains a robust performance level.