# REDUCR: Robust Data Downsampling using Class Priority Reweighting

William Bankes

Department of Computer Science

University College London

william.bankes.21@ucl.ac.uk

&George Hughes

Department of Computer Science

University College London

&Ilija Bogunovic

Department of Electrical Engineering

University College London

i.bogunovic@ucl.ac.uk

&Zi Wang

Google DeepMind

wangzi@google.com

Co-senior authors. Code available at: [https://github.com/williambankes/REDUCR](https://github.com/williambankes/REDUCR).

###### Abstract

Modern machine learning models are becoming increasingly expensive to train for real-world image and text classification tasks, where massive web-scale data is collected in a streaming fashion. To reduce the training cost, online batch selection techniques have been developed to choose the most informative datapoints. However, many existing techniques are not robust to class imbalance and distributional shifts, and can suffer from poor worst-class generalization performance. This work introduces REDUCR, a robust and efficient data downsampling method that uses class priority reweighting. REDUCR _reduces_ the training data while preserving worst-class generalization performance. REDUCR assigns priority weights to datapoints in a class-aware manner using an online learning algorithm. We demonstrate the data efficiency and robust performance of REDUCR on vision and text classification tasks. On web-scraped datasets with imbalanced class distributions, REDUCR significantly improves worst-class test accuracy (and average accuracy), surpassing state-of-the-art methods by around 15%.

## 1 Introduction

The abundance of data has had a profound impact on machine learning (ML), both positive and negative. On the one hand, it has enabled ML models to achieve unprecedented performance on a wide range of tasks, such as image and text classification . On the other hand, training models on such large datasets can demand significant computational resources , making it unsustainable in some situations . Additionally, the high speed at which streaming data is collected can make it infeasible to train on all of the data before deployment. To tackle these issues, various methods have emerged to selectively choose training data, either through pre-training data pruning  or online batch selection techniques , ultimately reducing data requirements and enabling ML models to handle otherwise unmanageable large and complex datasets.

In real-world settings, a variety of factors can affect the selection of datapoints, such as noise  and class-imbalance in the data . Online selection methods can exacerbatethese problems by further reducing the number of datapoints from underrepresented classes, which can degrade the performance of the model on those classes (Buda et al., 2018; Cui et al., 2019). Moreover, distributional shift (Koh et al., 2021) between training and test time can lead to increased generalization error if classes with poor generalization error are overrepresented at test time.

In this work, we introduce REDUCR, which is a new online batch selection method that is _robust_ to noise, imbalance, and distributional shifts. REDUCR employs multiplicative weights update to reweight and prioritize classes that are performing poorly during online batch selection. Figure 1 illustrates the intuition behind how the method works. REDUCR can effectively reduce the training data while preserving the worst-class generalization performance of the model. For example, on the Clothing1M dataset (Xiao et al., 2015), Figure 2 shows that, compared to the best performing online batch selection methods, REDUCR achieves around a 15% boost in performance for the worst-class test accuracy.

Main contributions.(1) We formalise the maximin problem of robust data downsampling (SS3). (2) We propose the REDUCR algorithm, which is equipped with a new robust selection rule that evaluates how much datapoints will affect the generalization error of a specific class (SS4.2). (3) We evaluate our algorithm on a series of text and image classification tasks and show that it achieves strong worst-class test accuracy while frequently surpassing state-of-the-art methods in terms of average test accuracy(SS5).

Related work.Mindermann et al. (2022) have developed an online batch selection method called RHO-Loss, which uses a _reference model_ trained on a holdout dataset to guide the selection of points during training. Certain extensions of this work have focused on using a reference model in different settings such as reinforcement learning (Sujit et al., 2022). However, to our knowledge, none have focused on improving the worst-class generalisation performance. Other batch selection methods (Loshchilov and Hutter, 2016; Jiang et al., 2019; Kawaguchi and Lu, 2020) use the training loss of points under the model or an approximate gradient norm (Katharopoulos and Fleuret, 2017) to select challenging points. We observe that these methods (e.g., see Loshchilov and Hutter (2016) in Figure 2) exhibit greater consistency in terms of worst-class generalization error in imbalanced datasets. Nevertheless, Loshchilov and Hutter (2016) do not surpass the average generalization error achieved by point selection with a reference model, namely, RHO-Loss. Recently, several works have also used reference models or a holdout dataset to train robust models. Oren et al. (2019); Liu et al. (2021); Clark et al. (2019) use a reference model to identify difficult-to-learn groups (or points, or biases) during

Figure 1: REDUCR starts by initializing weights of classes. At each timestep \(t\), the model receives a batch of datapoints \(B_{t}\). REDUCR computes the selection scores for each datapoint based on its usefulness to the model and the class weights, and selects new datapoints \(b_{t} B_{t}\) that achieve the highest selection scores. After the model takes gradient steps on the selected datapoints, REDUCR adjusts the weights to reflect increased priorities on underperforming classes.

Figure 2: REDUCR significantly improves worst-class test accuracy on Clothing1M outperforming Uniform and other recent works.

training. Han et al. (2018) use two models which act as a reference model for the other to remove noisy points from the training data. Cao et al. (2021); Ren et al. (2018) use a holdout dataset to reweight points or their regularization during training to achieve the best loss on the validation holdout dataset.

Sagawa et al. (2020) reweight groups known at training time and focus on fighting spurious correlations and improving worst-group generalisation error. In contrast, in our setting, class labels are available and we measure the performance in terms of worst-class generalisation error. Moreover, whilst these works aim to train robust models they do not consider efficient data downsampling strategies. The approach of Sagawa et al. (2020) to group robustness considers a small number of groups (up to 4 in their empirical study). Similarly, in our work, we consider classification settings with a controlled number of classes (\(<\) 1000) as the problem of robustness becomes less applicable in settings where the number of classes are high.

Xie et al. (2023) use both weights update rules and a reference model to find mixtures of corpora in LLM pretraining resulting in improved performance and training speed. Besides the problem setup, our method differs in three ways: i) we focus upon online batch selection; ii) we use multiple reference models; iii) and we use a class-holdout loss term (see Equation (8)) to reweight batches. Efficient data downsampling is a well-explored problem with various approaches, including active learning methods when label information is unknown (MacKay, 1992; Houlsby et al., 2011; Kirsch et al., 2019, 2023; Ash et al., 2020); data pruning and coreset techniques for pre-training data downsampling (Sorscher et al., 2022; Bachem et al., 2017; Borsos et al., 2020; Coleman et al., 2020); data distillation approaches (Cazenavette et al., 2022; Nguyen et al., 2021); and non-parametric inducing point methods (Galy-Fajou and Opper, 2021).

## 2 Background

We consider a \(C\)-way classification task and denote a model as \(p(y x,)\), where \(x\) denotes an input and \(y[C]\) the corresponding class label; the model is parameterized by \(\). For any training dataset \(=\{(x_{i},y_{i})\}_{i=1}^{N}\) with \(N\) datapoints, we use a point estimate of \(\) to approximate the posterior model as \(p(y x,) p(y x,)\). This estimate \(\) can be obtained by running stochastic gradient descent (SGD) to optimize the cross-entropy loss over a training dataset \(\).

The goal of _data downsampling_ is to select a dataset \(_{T}\) of size \(T\) (\( N\)) for training such that the generalisation error of the resulting model is minimised. We write this objective in terms of a separate _holdout_ dataset \(_{ho}=\{(x_{ho,i},y_{ho,i})\}_{i=1}^{N_{ho}}\) as follows:

\[_{T}=*{argmax}_{D,|D|=T}\; p( _{ho}|_{ho},D), \]

where the inputs and their labels are \(_{ho}=[x_{i,ho}]_{i=1}^{N_{ho}}\) and \(_{ho}=[y_{i,ho}]_{i=1}^{N_{ho}}\), respectively. Here, the likelihood of the holdout dataset is used as a proxy for the generalisation error. The problem is computationally prohibitive due to its combinatorial nature. Moreover, for a massive (or streaming) training dataset \(\), it is not computationally possible to load \(\) all at once and it is common to loop through the data by iteratively loading subsets.

**Online batch selection** is a practical streaming setup to approximate the data downsampling problem, where at each timestep \(t\), the model observes a training data subset \(B_{t}\), and the goal is to iteratively select a small batch \(b_{t} B_{t}\) for the model to take gradient steps. A standard solution to this problem is to design a selection score function that take into account the labels of the data. The selection score function can then be used to _score_ the utility of the small batch \(b_{t}\). See Algorithm 2 in Appendix A.1 for an example method.

**Reducible Holdout Loss (RHO-Loss)**(Mindermann et al., 2022) is an online batch selection method that uses the performance on a holdout dataset as the selection scores for small batches. More precisely, for each timestep \(t\), RHO-Loss selects

\[b_{t}=*{argmax}_{b B_{t}}\; p(_{ho} _{ho},_{t} b), \]

where \(_{t}=_{=1}^{t-1}b_{}\) is the cumulative training data the model has encountered until iteration \(t\).

## 3 Problem Formulation

In this work, we introduce the _robust_ data downsampling problem, where the goal is to select a training dataset \(_{T}\) of size \(T\) such that worst-class performance is optimized. Let the holdout dataset with class \(c[C]\) be \(D_{ho}^{(c)}=\{(x,y) D_{ho} y c\}=\{(x_{ho,i}^{(c)},y_{ho,i}^{(c)} )\}_{i=1}^{N_{ho}^{(c)}}\). We can write the objective of robust data downsampling as

\[_{T}=*{argmax}_{D,|D|=T}_{c[ C]} p(_{ho}^{(c)}_{ho}^{(c)},D), \]

where \(_{ho}^{(c)}=[x_{ho,i}^{(c)}]_{i=1}^{N_{ho}^{(c)}}\) and \(_{ho}^{(c)}=[y_{ho,i}^{(c)}]_{i=1}^{N_{ho}^{(c)}}\) correspond to the collections of inputs and labels in the class-specific holdout dataset \(D_{ho}^{(c)}\).

Compared to Equation (1), the objective in Equation (3) is even more challenging because of the maximin optimisation that involves \(C\) discrete classes. In fact, solving Equation (3) is known to be NP-hard, even when the objectives (each \(p(_{ho}^{(c)}|_{ho}^{(c)},)\), \(c[C]\)) are _submodular_ set functions. Chen et al. (2017) demonstrate the application of zero-sum game no-regret dynamics, where a learner employs a \((1-1/e)\)-near-optimal greedy strategy and an adversary seeks to find a distribution over loss functions that maximizes the learner's loss. In this scenario, a single set is identified, which, although larger than size \(T\), achieves a constant-factor approximation.

**Robust online batch selection** approximates the robust data downsampling problem by taking into account the practical limitations of data operation. Namely, we assume a streaming setting where the model observes training data subset \(B_{t}\) at each timestep \(t\). The goal is to select a small batch \(b_{t} B_{t}\) to compute gradients for model training with SGD, such that the model obtains top performance for the worst-class (Equation (3)). The robust setting motivates the development of novel batch selection methods that consider how each datapoint affects the generalization error on the worst-case class of inputs, rather than just the overall generalization error. Next, we introduce a new selection rule that achieves this and propose a practical algorithm for its implementation.

## 4 REDUCR for Robust Online Batch Selection

We propose REDUCR, a robust and efficient data downsampling method using class priority reweighting to solve the robust online batch selection problem in Section 3. The batch selection strategy of REDUCR relates the effect of training on a batch of candidate points \(b_{t}\) to the generalization error of a specific class in the holdout dataset.

### Online Learning

To solve Equation (3) in an online manner, we propose to use class priority reweighting, a variant of the multiplicative weights update method (Freund and Schapire, 1997; Cesa-Bianchi and Lugosi, 2006; Sessa et al., 2019). At the beginning of training we initialise a weight vector \(_{0}\) over a \(C\) dimensional simplex, \(=\{=[w_{c}]_{c=1}^{C}^{C}|_{c=1}^{C}w_{c}=1\}\). Each element of \(_{0}\) is initialised to be \(w_{0,c}=1/C\). For each iteration \(t\), small batch \(b_{t} B_{t}\) is chosen by maximising the weighted sum of the \(C\) different class-specific scoring functions (i.e., by best-responding to the current class-weights \(_{t}\)),

\[b_{t}=*{argmax}_{b B_{t}}_{c=1}^{C}w_{t,c}( p (_{ho}^{(c)}|_{ho}^{(c)},_{t} b)), \]

where \(_{t}=_{=1}^{t-1}b_{}\), \(_{t}=[w_{t,e}]_{c=1}^{C}\), and

\[w_{t,c}=w_{t-1,c}_{ho}^{(c)}| _{ho}^{(c)},_{t}))}{_{j=1}^{C}w_{t-1,j}(- p (_{ho}^{(j)}|_{ho}^{(j)},_{t}))}. \]

In the previous alternating procedure, class-weights are updated multiplicatively according to how well they perform given the selected batch, they increase for poorly performing classes and decrease otherwise. In Equation (5), \(\) is a learning rate that adjusts how concentrated the probability mass is in the resulting distribution. Figure 1 shows an intuitive illustration of how reweighting works in practice where classes that perform badly have low data likelihoods and are thus upweighted byEquation (5). In Appendix A.7.3 we explore an alternative solution to solve Equation (3); we solve an approximate robust optimisation problem directly at every timestep \(t\) and empirically demonstrate the multiplicative weights method outperforms it. We next introduce how to compute the likelihoods for class-specific holdout sets, i.e., \(p(_{ho}^{(c)}|_{ho}^{(c)},_{t} b)\) in Equation (4).

### Computing selection scores

Given the current dataset \(_{t}\) at timestep \(t\) and additional datapoints \(b B_{t}\), we would like to compute the likelihood of the holdout dataset that belongs to class \(c\). For simplicity, we consider the case where the small batch to be selected only includes a single datapoint, i.e., \(b=\{(x,y)\}\). We express the objective using a Bayesian perspective,

\[ p(_{ho}^{(c)}\,|\,_{ho}^{(c)},_{t}\{(x,y)\}) =_{ho}^{(c)},_{t})p(_{ho }^{(c)}|_{ho}^{(c)},_{t})}{p(y|x,_{ho}^{(c)}, _{t})} \] \[=_{ho}^{(c)},_{t})p(_{ho}^{(c)}|_{ho}^{(c)},_{t})}{p(y|x,_{t})}\] (7) \[=- p(y\,|\,x,_{t})+ p(y\,|\,x,_{t}, _{ho}^{(c)})+ p(_{ho}^{(c)}\,|\,_{ho}^{(c)},_{t}).\]

Equation (6) follows from the application of the Bayes rule and the conditional independence of \(x\) and \(_{ho}^{(c)}\) with \(_{ho}^{(c)}\) and \(y\), respectively. The posterior terms in Equation (6) can be approximated with point estimates of model parameters (see SS2). Computing Equation (6) involves two models: (1) the _target_ model with parameters \(_{t}\), which is trained on the cumulative training dataset \(_{t}=_{r=1}^{t-1}b_{r}\); (2) a _class-irreducible loss model_ (following the terminology from Mindermann et al. (2022)) with parameters \(_{t}^{(c)}\), which is trained on \(_{t}\) and class-specific holdout data \(_{ho}^{(c)}\). The target model is what we are interested in for the classification task. We use \([y|x,]=- p(y\,|\,x,)\) to denote the cross-entropy loss for any model parameters \(\), and we re-write Equation (6) as follows,

\[ p(_{ho}^{(c)}\,|\,_{ho}^{(c)},_{t}\{(x,y)\})[y|x,_{t}]}_{}- [y|x,_{t}^{(c)}]}_{}- [_{ho}^{(c)}|_{ho}^{(c)},_{t} ]}_{}. \]

We name the three terms in Equation (8) the _model loss_, _class-irreducible loss_ and _class-holdout loss_, respectively. We define the term _excess loss_ as the difference of the model loss and class-irreducible loss. The excess loss is the improvement in loss for point \((x,y)\) by observing more data from class \(c\) (i.e., \(_{ho}^{(c)}\)). Intuitively, if two data points are from different classes, REDUCR will take into account the weight of the worst-performing class, which is reflected by the class-holdout loss. This ensures that REDUCR is focusing on improving the performance of the model on the classes that are most difficult to learn. In a different scenario, if two datapoints are from the same class, their class-holdout losses will be the same, and the point with a larger excess loss will be preferred. This means that REDUCR prefers datapoints whose losses have more potential to be improved.

Computing the approximate in Equation (8) is far more tractable than naively re-training a new model (i.e., \( p(_{ho}^{(c)}|_{ho}^{(c)},_{t}\{(x,y )\})\)) for each possible candidate point \((x,y)\). The model loss and the class-holdout loss only require evaluating the cross-entropy losses of some datapoints on the target model. More generally, if batch \(b\) can include more than one point, we can simply change the \(x\) and \(y\) to a list of inputs and labels instead. Next, we further improve the efficiency of REDUCR by approximating the class-irreducible loss model.

### Class-Irreducible Loss Models

For each selected batch \(b_{t}\) under the current selection rule in Equation (8), we need to update \(C\) class-irreducible loss models to compute the class-irreducible losses. We propose to approximate these models using _amortised_ class-irreducible loss models, which are trained for each class at the beginning of REDUCR and do not need to be updated during online batch selection.

We interpret the class irreducible loss term as an expert model at predicting the label of points from a specific class \(c\) due to the extra data from the holdout dataset this term has available. To create an approximation of this expert model, we train the amortised class-irreducible loss models using an adjusted loss function in which points with a label from the class \(c\) are up-weighted by a parameter \((0,+)\) (set in Section 5):

\[_{c}=*{arg\,min}_{}_{(x,y)_{ _{l}}}(1+\,[c y])\,[y|x,]. \]```
1:Input: data pool \(\), holdout data \(_{ho}=_{c C}_{ho}^{(c)}\), learning rate \((0,)\), small batch size \(k\), total timesteps \(T/k\)
2: Initialize class weights \(_{1}=_{C}\)
3: Use \(_{ho}\) to train \(C\) amortised class irreducible loss models as per Equation (9) to obtain \(_{c}\)
4:for\(t[T/k]\)do
5: Receive batch \(B_{t}\)
6:\(b_{t}=*{argmax}_{b B_{t}:|b|=k}_{(x,y) b}_{c C }w_{t,c}(0,[y|x,_{t}]-[y|x,_{c}])\ \) Select points with top k selection scores
7: Compute the objective value for every class \(c C\): \(_{c}=_{(x,y) b_{t}}(0,[y|x,_{t}]-[y|x,_{c}])-[_{ho}^{(c)}|_{ho}^{(c)}, _{t}]\)
8: Update class weights for every class \(c C\): \(w_{t+1,c}=w_{t,c})}{_{j C}w_{t,j}(- _{j})}\)
9:\(_{t+1} SGD(_{t},b_{t})\)
10:endfor
```

**Algorithm 1** REDUCR for robust online batch selection

Here we define \([]\) as the indicator function. Equation (9) optimizes over the parameters of the amortised class-irreducible loss model for class \(c\), and obtain \(_{c}\) to approximate \(_{t}^{(c)}\) in Equation (8), i.e., \([y|x,_{t}^{(c)}][y|x,_{c}]\). The up-weighting of points can be considered a form of importance weighting (Shimodaira, 2000), where by up-weighting points with labels in a specific class we calculate a Monte Carlo approximation of the loss under a distribution in which points from class \(c\) are more prevalent. Algorithm 3 details the full amortised class-irreducible loss model training procedure in Appendix A.2. We provide further motivation of our approximation in Appendix A.3.

### REDUCR as a practical algorithm

We use the selection objective in Equation (8) along with the amortised class-irreducible loss model approximation (Section 4.3) and the online algorithm (Section 4.1) to reweight the worst performing class during training and select points that improve its performance. See Algorithm 1 for a full description of the REDUCR method.

At each iteration, the top \(k\) points are selected (Line 6) according to the weighted sum of Equation (8) for each class \(c C\), thus efficiently approximating the combinatorial problem from Equation (4). As the class-holdout loss does not depend on the selected points \(b_{t}\) and we sum over the classes, we can remove this term from the weighted sum of the selection scores and only apply it when updating the weights \(_{t}\) (in Line 7 and 8). We calculate the _average_ class-holdout loss to remove any dependence of the term upon the size of the classes in the holdout dataset. We find that clipping the excess loss improves the stability of the algorithm in practice. We test this heuristic empirically in Section 5.2 and provide an intuitive explanation for why this is the case in Appendix A.7.2.

When comparing REDUCR to other online batch selection methods, we observe distinct batch selection patterns. When the dataset is class-imbalanced, the underrepresented classes tend to perform worse because of the lack of training data from those classes. RHO-Loss may struggle to select points from the underrepresented classes as they have less effect on the loss of the holdout dataset. Selection rules that select points with high training loss (Loshchilov and Hutter, 2016; Kawaguchi and Lu, 2020; Jiang et al., 2019) might select points from the underrepresented classes but have no reference model to determine which of these points are learnable given more data and thus noisy or task-irrelevant points may be selected. In contrast, REDUCR addresses both of these issues by identifying underrepresented classes and using the class-irreducible loss model to help to determine which points from these classes should be selected.

Even when the dataset is not imbalanced, certain classes might be difficult to learn; for example, due to noise sources in the data collection processes. Via Equation (5), REDUCR is able to re-weight the selection scores such that points that are harder to learn from worse-performing classes are selected over points that are easier to learn from classes that are already performing well. This is in contrast to RHO-Loss which will always select points that are easier to learn. We empirically demonstrate this on class balanced datasets in Section 5.

## 5 Experiments

In this section, we present empirical results to showcase the performance of REDUCR on large-scale vision and text classification tasks.

**Datasets.** We train and test REDUCR on image and text datasets. We use CIFAR10 , CINIC10 , Clothing1M , the Multi-Genre Natural Language Interface (MNLI), and the Quora Question Pairs (QQP) datasets from the GLUE NLP benchmark . Each dataset is split into a labelled training, validation and test dataset (for details see Appendix A.5), the validation dataset is used to train the class-irreducible loss models and evaluate the class-holdout loss during training. The Clothing1M dataset uses 100k additional points from the training dataset along with the validation dataset to train the irreducible loss model(s) (as per ). We simulate the streaming setting by randomly sampling batch \(B_{t}\) from dataset \(\) at each timestep.

**Models.** For the experiments on image datasets (CIFAR10, CINIC10 and Clothing1M) all models use a ResNet-18 model architecture . For the Clothing1M dataset we use a ResNet-18 model pretrained on the imagenet dataset . The networks are optimised with AdamW  and the default Pytorch hyperparameters are used for all methods except CINIC10 for which the weight decay is set to a value of 0.1. For the NLP dataset we use the _bert-base-uncased_ model from HuggingFace  and set the optimizer learning rate to \(1e^{-6}\).

**Baselines.** We benchmark our method against the state-of-the-art RHO-Loss and Loshchilov and Hutter , an online batch selection method that uses the training loss to select points. We refer to the latter baseline as Train Loss. We also compare against Uniform where points are chosen at random from the large batch at each training step.2 All experiments are run multiple times and the mean and standard deviation across runs calculated. Unless stated otherwise \(10\%\) of batch \(B_{t}\) is selected as the small batch \(b_{t}\), and we set \(=1e-4\). \(=9\) is used when training each of the amortised class-irreducible loss models on the vision datasets and \(=4\) for the NLP datasets. We study the impact of \(\) and \(\) on REDUCR further in Appendix A.10. For full details of the experimental setup see Appendix A.5.3

**Metrics.** Finally, it is important to note that we analyse the worst-class test accuracy metric which can be interpreted as a lower bound on a model's performance under all class distribution shifts. This is because the worst possible distribution shift between the training and test set is one where the entire test set consists of only points from the worst performing class.

### Key results

The worst-class and average test accuracy for the datasets and model are shown in Table 1 and Table 2, respectively. Across all datasets, REDUCR outperforms the baselines in terms of the worst-class accuracy and matches or even outperforms the average test accuracy of RHO-Loss within one standard deviation. This is surprising because the primary goal of REDUCR is not to optimize the overall average (over classes) performance. REDUCR performs particularly strongly

Figure 3: REDUCR improves the worst-class test accuracy and data efficiency when compared with the RHO-Loss, Train Loss and Uniform baselines on the a) Clothing1M dataset, b) the CINIC10 dataset, and c) the CIFAR100 dataset.

on the Clothing1M dataset, Table 1 shows REDUCR improves the worst-class test accuracy by around \(15\%\) when compared to Train Loss, the next best-performing baseline, and by around 26% when compared to RHO-Loss, the overall best-performing baseline across datasets. Figure 2(a) shows that REDUCR also achieves this performance in a more data efficient manner than the comparable baselines, achieving a mean worst-class test accuracy of \(40\%\) within the first 10k training steps. We also observe improved efficiency on the CINIC10 dataset, as shown in Figure 2(b), and the MNLI and QQP datasets as detailed in Figure 7.

The Clothing1M dataset also sees a distribution shift between the training and test dataset. In the test dataset, the worst performing class is much more prevalent than in the training dataset and as such improvements to its performance impact the average test accuracy significantly. Figure 5 shows the impact of this distribution shift as the improved performance of the model on the worst-class results in an improved average test accuracy to the state-of-the-art RHO-Loss baseline.

### Ablation Studies

To further motivate the selection rule in Equation (8), we conduct a series of ablation studies to show that all the terms are necessary for robust online batch selection. Figure 3(a) shows the performance of REDUCR on the CINIC10 dataset when the model loss, amortised class-irreducible loss and class-holdout loss terms of the algorithm were individually excluded from the selection rule. All three terms in Equation (8) are required to achieve a strong worst-class test accuracy.

Removing the Model Loss results in the worst performance in the set of ablation studies. This is because the Model Loss provides REDUCR with information about which points are currently not classified correctly by the model. By removing this term REDUCR only selects points which do well under the Class Irreducible Loss model and does not prioritise points the model has not yet learnt. Selecting points not yet learnt by the model is an important quality in online batch selection approaches and the main premise of the Train Loss baseline algorithm. Likewise by removing the Class Irreducible Loss Model term we remove the ability of the model to infer if a point can be learnt or not. In Mindermann et al. (2022), the authors note that these pretrained models enable the algorithm to pick points that are learnable and do not have label noise.

    &  \\  & **Uniform** & **Train Loss** & **RHO-Loss** & **REDUCR** \\  CIFAR\(10\) (10 runs) & 75.01 \(\) 1.37 & 76.1 \(\) 2.31 & 78.80 \(\) 2.09 & **83.29 \(\) 0.84** \\ CINIC\(10\) (10 runs) & 64.70 \(\) 2.45 & 64.83 \(\) 4.75 & 69.39 \(\) 3.56 & **75.30 \(\) 0.85** \\ CIFAR\(100^{*}\) (5 runs) & 10.59 \(\) 3.63 & 17.59 \(\) 5.17 & 16.0 \(\) 6.93 & **26.00 \(\) 2.65** \\ Clothing1M (5 runs) & 39.23 \(\) 5.41 & 40.37 \(\) 3.58 & 27.77 \(\) 10.16 & **53.91 \(\) 2.42** \\ MNLI (5 runs) & 74.70 \(\) 1.26 & 74.56 \(\) 1.44 & 76.74 \(\) 0.93 & **79.45 \(\) 0.39** \\ QQP (5 runs) & 73.21 \(\) 2.04 & 79.96 \(\) 2.34 & 78.21 \(\) 1.95 & **86.61 \(\) 0.49** \\   

Table 1: REDUCR outperforms RHO-LOSS (the best overall baseline) in terms of the worst-class test accuracy on Clothing1M, CINIC10 and CIFAR10 by at least 5-26%. Across all baselines, REDUCR gains about 15% more accuracy on the noisy and imbalanced Clothing1M dataset as shown in Figure 2. * CIFAR100 results from training step 10k where REDUCR converges, after 10k further training steps **TRAIN** Loss achieves a similar performance.

    &  \\  & **Uniform** & **Train Loss** & **RHO-Loss** & **REDUCR** \\  CIFAR\(10\) (10 runs) & 85.09 \(\) 0.52 & 88.86 \(\) 0.22 & **90.00 \(\) 0.33** & **90.02 \(\) 0.44** \\ CINIC\(10\) (10 runs) & 79.51 \(\) 0.30 & 79.25 \(\) 0.33 & **82.09 \(\) 0.30** & **81.68 \(\) 0.47** \\ CIFAR\(100\) (5 runs) & 57.94 \(\) 0.69 & 59.77 \(\) 0.71 & 60.95 \(\) 0.64 & **62.21 \(\) 0.62** \\ Clothing1M (5 runs) & 69.60 \(\) 0.85 & 69.63 \(\) 0.30 & 71.07 \(\) 0.46 & **72.69 \(\) 0.42** \\ MNLI (5 runs) & 79.19 \(\) 0.53 & 76.85 \(\) 0.14 & **80.89 \(\) 0.31** & **80.28 \(\) 0.33** \\ QQP (5 runs) & 85.05 \(\) 0.43 & **86.30 \(\) 0.41** & **86.88 \(\) 0.31** & **86.99 \(\) 0.49** \\   

Table 2: Together with Table 1, these results demonstrate that REDUCR improves the worst-class test accuracy while maintaining strong average test accuracy despite REDUCR not explicitly optimizing the average test accuracy. REDUCR outperforms the baseline methods on the CIFAR\(100\) and Clothing1M datasets.

The removal of the class-holdout loss term affects the ability of REDUCR to prioritise the weights of the model correctly. In Figure 4 we compare the class weights of REDUCR and an ablation model without the class-holdout loss term. The standard model clearly prioritises classes 3, 4 and 5 during training across all 5 runs, whilst the ablation model does not consistently weight the same classes across multiple runs. We also conducted an ablation study on the clipping of the excess loss to motivate its inclusion in the algorithm, this is also shown in Figure 3(a), we note that this stabilises the model performance towards the end of training and investigate further in Appendix A.7.2.

### Scaling up the number of classes

REDUCR can handle problems involving a large number of classes without needing to train a separate class-irreducible loss model for each class. One idea is to group the classes into superclasses, where \(c_{i},_{i}\{_{i}\}_{i=1}^{|G|}\), \(_{i}_{j}=\) for \(i j\) and \(|G|<|C|\), and solve the robust data downsampling problem over these superclasses. We test the proposed variant of REDUCR on the CIFAR100 dataset using the provided groupings with 20 superclasses in total . fig. 2(c) shows that REDUCR outperforms the baselines in terms of the worst-**class** test accuracy, even though the robust objectives are over the superclasses. It achieves this performance in **half** the number of training steps as shown in Table 1.

### Imbalanced Datasets

We investigate the performance of models trained using REDUCR on imbalanced datasets. We artificially imbalance the CIFAR10 training and validation datasets such that a datapoint of the imbalanced class is sampled with probability \(p(0,1/C]\) (referred to as the percent imbalance) and datapoints from the remaining classes are sampled with probability \((1-p)/(C-1)\) during model training. We conduct experiments with 0.01, 0.025 and 0.1 (which is equivalent to the balanced dataset) percent imbalance on classes 3 and 5. The results are shown in Figure 6.

We find the performance of models trained using REDUCR deteriorates less than those trained with the RHO-Loss or Uniform baselines as the percent imbalance of a particular class decreases (see Figure 6). For example, when class 3 is imbalanced, in the most imbalanced case (1.0%) the median performance of REDUCR outperforms that of RHO-Loss run by 14%. This demonstrates the effectiveness of REDUCR in prioritising the selection of data points from underrepresented classes.

Figure 4: a) The worst-class test accuracy _decreases_ when the model loss, class irreducible loss, and class-holdout loss terms are removed from REDUCR on CINIC10. Comparing REDUCR with clipping for excess losses (Algorithm 1) and REDUCR (no clip) which removes the clipping, we observe that REDUCR achieves more stable performance. We show the class weights **w** at each training step for b) REDUCR and c) REDUCR with the class-holdout loss term ablated. The ablation model fails to consistently prioritise the underperforming classes across multiple runs.

Figure 5: REDUCR improves the average test accuracy on the Clothing1M dataset.

## 6 Conclusions, Broader Impact and Limitations

In summary, we identified the problem of class-robust data downsampling and proposed a new method, REDUCR, to solve this problem using class priority reweighting. Our experimental results indicate that REDUCR significantly enhances data efficiency during training, achieving superior test accuracy for the worst-performing class and frequently surpassing state-of-the-art methods in terms of average test accuracy. REDUCR excels in settings where the available data are class-imbalanced by prioritising the selection of points from underrepresented classes.

**Limitations.** The computational efficiency of REDUCR scales linearly with the number of classes. We propose one solution to this in Section 5.3, where we show that using groups of classes can still result in improved worst-class performance. Another solution is to use smaller model architectures for the class-irreducible loss model. (Mindermann et al., 2022) provide extensive evidence that small reference models can improve computational efficiency whilst still providing a useful signal for data selection, we leave investigation of these methods as a future research direction.

**Broader Impact.** Improving data efficiency is an important and practical problem as more machine learning models are being trained and deployed for real-world applications. Moreover it is critical to ensure the robustness of models for reliable and trustworthy machine learning. Our work proposes a new method with the goal of improving the robustness of models whilst significantly reducing the data required to achieve state of the art performance.