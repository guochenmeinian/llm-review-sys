# BECAUSE:

Bilinear Causal Representation for Generalizable

Offline Model-based Reinforcement Learning

Haohong Lin\({}^{1}\), Wenhao Ding\({}^{1}\), Jian Chen\({}^{1}\), Laixi Shi\({}^{2}\), Jiacheng Zhu\({}^{3}\), Bo Li\({}^{4}\), Ding Zhao\({}^{1}\)

\({}^{1}\)CMU, \({}^{2}\)Caltech, \({}^{3}\)MIT, \({}^{4}\)UChicago & UIUC

{haohongl, wenhaod}@andrew.cmu.edu

###### Abstract

Offline model-based reinforcement learning (MBRL) enhances data efficiency by utilizing pre-collected datasets to learn models and policies, especially in scenarios where exploration is costly or infeasible. Nevertheless, its performance often suffers from the objective mismatch between model and policy learning, resulting in inferior performance despite accurate model predictions. This paper first identifies the primary source of this mismatch comes from the underlying confounders present in offline data for MBRL. Subsequently, we introduce **B**ilin**Ear **CAUS**aI **r**E**presentation (BECAUSE), an algorithm to capture causal representation for both states and actions to reduce the influence of the distribution shift, thus mitigating the objective mismatch problem. Comprehensive evaluations on 18 tasks that vary in data quality and environment context demonstrate the superior performance of BECAUSE over existing offline RL algorithms. We show the generalizability and robustness of BECAUSE under fewer samples or larger numbers of confounders. Additionally, we offer theoretical analysis of BECAUSE to prove its error bound and sample efficiency when integrating causal representation into offline MBRL. See more details in our project page: [https://sites.google.com/view/be-cause](https://sites.google.com/view/be-cause).

## 1 Introduction

Offline Reinforcement Learning (RL) has shown great promise in learning directly from pre-collected datasets, especially in scenarios where active interaction is expensive or infeasible . Specifically, offline model-based reinforcement learning (MBRL) , learning policies with an estimated world model, generally perform better than their model-free counterparts in long-horizon tasks such as self-driving vehicles , robotics , and healthcare . However, offline RL suffers from distribution shift because the rollout data is either sampled from some suboptimal behavior policies or sampled from slightly different training environments compared to the deployment time .

Although identifying distribution shift issues, many of the current offline MBRL works fail to model the shift in environment dynamics, which is ubiquitous and could cause catastrophic failure of trained policy at a slightly different deployment stage. Furthermore, since the learning objectives of the world models and policies are isolated from each other, a significant challenge in offline MBRL is objective mismatch  problem (shown in Figure 1): models that achieve a lower training loss are not necessarily better for control performance. For example, a dynamics model achieve relatively low

Figure 1: The objective mismatch problem.

[MISSING_PAGE_FAIL:2]

and the time-homogeneous transition dynamics of the environment \(s_{i+1} T(|s_{i},a_{i}), i[h,H]\). In the offline dataset, the data rollouts can be seen as generated by some (mixed) behavior policy \(_{}\), resulting in a dataset \(\) with in total \(n\) samples \(\{s_{i},a_{i},s^{}_{i},r_{i}\}_{1 i n}\).

**Definition 1** (Bilinear MDP ).: For each \((s,a),s^{}\), we have the corresponding feature vector \((,):^{||}^{||} ^{d},():^{||} ^{d^{}}\). With some core matrix \(M^{d d^{}}\), we can represent the transition function kernel \(T(|,)\) as

\[ s,a,s^{},\;\;T(s ^{}|s,a)=(s,a)^{T}M(s^{}), \]

where \((s,a)\) and \((s^{})\) are embedding functions that map the original state and action to the latent space, \(M\) is the core matrix that models the transition relationship between the previous timestep and next timestep in the latent space. Such a linear decomposition in the transition dynamics allows us to embed structures of the transition model without the loss of general function approximation capabilities to derive state and action representations.

### Action State Confounded MDP

We consider the existence of confounders in the MDP to represent the offline data collection process, and define action-state confounded MDP (ASC-MDP):

**Definition 2** (Asc-Mdp).: Besides the components in standard MDPs \(=\{,,T,H,r\}\), we introduce a set of unobserved confounders \(u\). In ASP-MDP, confounders are factorized as \(u=\{u_{},u_{c}\}_{1 h H}\), where \(u_{}\) denotes the confounders between \(s\) and \(a_{}(s)\) induced by behavior policies, and \(u_{c}\) denotes the confounders within the state-action pairs of the environment transition, that is, the inherent structure between \((s,a)\) and \(s^{}\). Here we assume a time-invariant confounder distribution \(u P_{u}()\), \( h[H]\), which is a common assumption [15; 16; 17]

The resulting causal relationship of ASC-MDP is demonstrated in Figure 2. Originating from the original MDP, ASC-MDP is different from the Confounded MDP  and State-Confounded MDP (SC-MDP)  in that it models both the _spurious correlation_ between the current state \(s\) and the current action \(a\), as well as those between the next state \(s^{}\) and \((s,a)\). Yet, confounded MDP and SC-MDP only model part of the possible confounders between states and actions. The factorization of the confounder in ASC-MDP aligns with the source of spurious correlation in offline MBRL.

## 3 Proposed Method: BECAUSE

We propose BECAUSE, our core methodology for modeling, learning, and applying our causal representations for generalizable offline MBRL. Section 3.1 models the basic format of causal representations and analyzes their properties. Section 3.2 gives a compact way to learn the causal representation \((s,a)\) and \((s^{})\), as well as the core mask estimation \(M\). Section 3.3 utilizes these learned causal representations in both world model learning and MBRL planning from offline datasets.

### Causal Representation for ASC-MDP

In the presence of a hidden confounder \(u\), we model the confounder behind the transition dynamics as a linear confounded MDP :

\[T(s^{}|s,a,u)=(s,a,u)^{T}(s^{}), \]

where \(u P_{u}()\). Inspired by the Bilinear MDP in Definition 1, we decompose \((s,a,u)\) into a confounder-aware core matrix \(M(u)\) and a feature mapping \((s,a)\), which factorize the influence of

Figure 2: Comparison of our ASC-MDP with two existing formulations.

the confounders. Given the factorization of confounder \(u=\{u_{c},u_{}\}\) in Definition 2, we derive via _d-separation_ in the graphical model in Figure 2 that \(s^{}\!\!\! u_{}|\{s,a,u_{c}\}\). As a result, we only need to consider the confounder \(u_{c}\) from the environment when decomposing the transition model:

\[T(s^{}|s,a,u)=T(s^{}|s,a,u_{c})=(s,a)^{T}M(u_{c})(s^{ }). \]

**Definition 3** (Construction of causal graph \(G\)).: In ASC-MDP, \(G=0^{d d}&M\\ 0^{d^{} d}&0^{d^{} d}\). for all (sparse) core matrix \(M\), the causal graph \(G\) is bipartite, thus \(\;G,G\).

Definition 3 reveals the connection between the core matrix \(M\) and causal graph \(G\), as is formulated in the ASC-MDP. To reduce the influence of \(u\) and estimate the unconfounded transition model \(T(s^{}|s,a)\), one way is to identify the causal structures induced by the confounder \(u_{c}\) for the transition dynamics . Existing methods in differentiable causal discovery [21; 22; 23] transform causal discovery on some causal graph \(G\), into a maximum likelihood estimation (MLE) with regularization:

\[=*{arg\,max}_{G} p(;,,G)-|G|=*{arg\, max}_{M} p(;,,M)-|M|, \]

Since in our case, \(M\) is a sub-matrix of the causal graph \(G\). Given the Definition 3, \(G\) automatically satisfies the formulation of ASC-MDP, thus discovering \(G\) is essentially estimating the sparse submatrix \(M\)**without** DAG constraints: \(=*{arg\,max}_{M} p(;,,M)- |M|\). We elaborate Definition 3 and show the relationship between core matrix \(M\) and causal graph \(G\) in Appendix A.3.

We also make the assumption that the sparse \(G\) and \(M\) remain invariant with different environment confounders in the offline training and online testing.

**Assumption 1** (Invariant causal graph).: We denote the causal graph \(G\) under confounder \(u\) as \(G(u)\). The generalization problem that we aim to solve satisfies the invariance in the causal graph \(G(u_{c})\), where \(G(u_{c})=G(u^{}_{c}),M(u_{c})=M(u^{}_{c})\), \(u_{c}\) and \(u^{}_{c}\) are the confounders in training and testing.

_Remark 1_.: The assumption 1 can also be interpreted as task independence in , invariant state representation , or invariant action effect in . See detailed comparison in appendix Table 4.

### Learning Causal Representation from Offline Data

We first learn the causal world model \(T(s^{}|s,a)\) in the presence of confounders \(u\) in the offline datasets. As formulated in ASC-MDP 2, there are two sets of confounders: \(u_{}\) and \(u_{c}\). To estimate an unconfounded transition model and remove the effect of confounder, we first remove the impact of \(u_{c}\) which comes from the dynamics shift by estimating a batch-wise transition matrix \(M(u_{c})\), then we apply a reweighting formula to deconfound \(u_{}\) induced by the behavior policies and mitigate the model objective mismatch.

As discussed in Definition 3, we only need to optimize the part of the parameters of the causal graph \(G\), i.e. \(M\). Thus, we can remove the constraints in (4), then transform the original causal discovery problem into a regularized MLE problem as follows:

\[_{M}_{}(M)= _{M}\;(- p(;,,M)+|M|)\] \[= _{M}\;_{(s,a,s^{}) }\|^{T}(s^{})K_{}^{-1}-^{T}(s,a)M\|_{2}^{2}}_{ {World Model Learning}}+}_{}}. \]

where \(K_{}:=_{s^{}}(s^{})(s^{})^{T}\) is an invertible matrix. The derivation of Equation (5) is elaborated in Appendix A.4. In practice, we use the \(^{2}\)-test for discrete state and action space and the fast

Figure 3: BECAUSE learns a causality-aware representation from the buffer and uses it in both the world model and uncertainty quantification to obtain a pessimistic planning policy.

Conditional Independent Test (CIT)  for continuous variables to estimate each entry in the core matrix \(M\). We regularize the sparsity of \(M\) by controlling the \(p\)-value threshold in CIT and provide a more detailed implementation in Appendix C.1.

Estimating the core mask provides a more accurate relationship between state and action representations, and we further refine the state action representation function \(\) and \(\) to help capture more accurate transition dynamics. We optimize them by solving the following problem, according to the transition model loss and spectral norm regularization  to satisfy the regularity constraints of the feature in Assumption 3:

\[_{,}_{}(,)=_{,}_{(s,a,s^{})}\|^{T}(s^{})K_{}^{-1}-^{T}(s,a)M\|_{2}^{2}+_{}\|\|_{2}+_{}\|\|_{2}. \]

The world model learning process is illustrated in Figure 3. The estimation of individual \(M(u_{c})\) mitigates the spurious correlation brought by \(u_{c}\). To further deal with the spurious correlation in \(u_{}\) induced by the behavior policy \(_{}(a|s,u_{})\), we utilize the conditional independence property in the ASC-MDP shown in Equation (3). The following equation shows that the true unconfounded transition \(T\) can be rewritten in the reweighting formulas. This reweighting process in mask \(M\) serves as the soft intervention approach [18; 29] to estimate the treatment effect in the transition function \(T\) in an unconfounded way:

\[T(s^{}|s,a) =_{p_{u}}[(s,a)^{T}M(u_{c})(s^{}) _{}(a|s,u_{})]}{_{p_{u}}_{}(a|s,u_{})} \] \[=(s,a)^{T}[_{p_{u}}[M(u_{c})_{}( a|s,u_{})]}{_{p_{u}}_{}(a|s,u_{})}](s^{}) (s,a)^{T}\ (u)(s^{}).\]

The derivation of Equation (7) is illustrated in Appendix A.5. Equation (7) basically shows a re-weighting process given the empirical estimation of \(M(u_{c})\) in every batch of trajectories: \((u)=_{p_{u}}[M(u_{c})_{}(a|s,u_{})]}{ _{p_{u}}_{}(a|s,u_{})}\). Compared to the general reweighting strategies in previous MBRL literatures [18; 29] which reweights the entire value function, this re-weighting process is conducted only on the estimated matrix, while the representation \((s,a)\) and \((s^{})\) are subsequently regularized by weighted estimation of \(\). The pipeline of causal world model learning is described in the first part of the Algorithm 1. We discuss more details of the implementation and experiment in Appendix C.

### Causal Representation for Uncertainty Quantification

To avoid entering OOD states in the online deployment, we further design a pessimistic planner according to the uncertainty of the predicted trajectories in the imagination rollout step to mitigate objective mismatch.

We use the feature embedding from bilinear causal representation to help quantify the uncertainty, denoted as \(E_{}(s,a)\). As we have access to the offline dataset, we learn an Energy-based Model (EBM) [30; 31] based on the abstracted state representation \(\) and core matrix \(M\). A higher output of the energy function \(E_{}(,)\) indicates a higher uncertainty in the current state as they are visited by the behavior policies \(_{}\) less frequently. In practice, the energy-based model usually suffers from a high-dimensional data space . To mitigate this overhead of training a good uncertainty quantifier, we first embed the state samples through the abstract representation \((s^{})\), and the state action pair via \((s,a)\).

\[_{}()=_{(|s,a)}E_{}[ (^{+})|(s,a)]-_{q(s,a)}E_{}[(^{-})|( s,a)]+_{}\|\|_{2}, \]

where \(()^{+}\) refers to the positive samples from the approximated transition dynamics \((|s,a)\), and \((^{-})\) refers to the latent negative samples via the Langevin dynamics . Additionally, we regularize the parameters of EBM to avoid overfitting issues. We attach more training details and results of EBMs in Appendix C.2 The learned energy function \(E_{}(s,a)\) is used to quantify the uncertainty based on the offline data.

During the online planning stage, we use the learned EBM to adjust the reward estimation based on Model Predictive Control (MPC) . At timestep \(h\), we basically subtract the original step return estimation \(r_{h}(s,a)\) by its uncertainty \(E_{}(s,a)\):

\[_{h}(s,a)=_{h}(s,a)-E_{}(s,a)= (s,a)-E_{}(s,a)}_{}+_{s^{} }(s^{}|s,a)_{h+1}(s^{}). \]

### Theoretical Analysis of BECAUSE

Then we move on to develop the theoretical analysis for the proposed method BECAUSE. Based on two standard Assumption 2 and 3 on the feature's existence and regularity, we achieve the finite-sample complexity guarantee -- an upper bound of the suboptimality gap as follows, whose proof is postponed to Appendix B.

**Theorem 1** (Performance guarantee).: _Consider any \(0<<1\) and any initial state \(\). Under the Assumption 2, 3 and that the transition model \(T\) is an SCM (defined in 4), for any accuracy level \(0 1\), with probability at least \(1-\), the output policy \(\) of BECAUSE (Algorithm 1) based on the historical dataset \(\) with \(n=_{(s,a)}n(s,a)\) samples generated from a behavior policy \(_{}\) satisfies:_

\[V_{1}^{*}()-V_{1}^{}()C_{1} }{}|},C_{s}}}_{h=1}^{H}_{^{*}}[,a_{h})}} s_{1}=],\]

_where \(C_{1},C_{s}\) are some universal constants, \(\) is SCM's noise level (see Definition 4), and \(M^{d d^{}}\) is the optimal ground truth sparse transition matrix to be estimated._

The error bound shrinks as the offline sample size \(n\) over all state-action pairs increase. It also grows proportionally to the planning horizon \(H\), SCM's noise level \(\), and the \(_{0}\) norm of the ground true causal mask \(M\), which describes the intrinsic complexity of the world model.

Consequently, with Proposition 1 in the Appendix, we can achieve \(\)-optimal policy (\(V_{1}^{*}()-V_{1}^{}()\)) as long as the historical dataset satisfies the following conditions: \(\ 0 1\),

\[_{(s,a,h)[H]}_{^{*}} n(s_{h},a_{h}) s_{1}=C _{1}^{2}^{2}}{}||,C_{s}^{2} ^{2}\|M\|_{0}} H^{2}(1/)}{^{2}}.\]

## 4 Experiment Results

In this section, we conduct a comprehensive empirical evaluation of BECAUSE's generalization performance in a diverse set of environments, covering different decision-making problems in the grid world, manipulation, and autonomous driving domains, shown in Figure 4.

### Experiment Setting

Environment DesignWe design 18 tasks in 3 representative RL environments in Figure 4. Agents need to acquire reasoning capabilities to receive higher rewards and achieve goals.

Figure 4: Three environments used in this paper.

* **Lift**: Object manipulation environment in RoboSuite . We designed this environment for the agent to lift an object with a specific color configuration on the table to a desired height. In the OOD environment _Lift-O_, there is an injected spurious correlation between the color of the cube and the position of the cube in the training phase. During the testing phase, the correlation between color and position is different from training.
* **Unlock**: We designed this environment for the agent to collect a key to open doors in Minigrid . In the OOD environment _Unlock-O_, there will be a different number of goals (doors to be opened) in the testing environments from the training environments.
* **Crash**: Safety is critical in autonomous driving, which is reflected by the collision avoidance capability. We consider a risky scenario where an AV collides with a jaywalker because its view is blocked by another car . We design such a crash scenario based on highway-env , where the goal is to create crashes between a pedestrian and AVs. In the OOD environment _Crash-O_, the distribution of reward (number of pedestrians) is different in online testing environments.

For all three different environments, we set a specific subset of the state space as the goal \(g\), and the reward is defined as the goal-reaching reward \(r(s,a,g)=(r=g)\). When the episode ends in the goal state within the task horizon \(H\), the episode is considered a success. We then use the average _success rate_ as the general evaluation metrics for our BECAUSE and all baselines.

In each environment, we collect three types of offline data: _random_, _medium_, and _expert_ based on the different levels of \(u_{}\) in the behavior policies. In _Unlock_ environments, we collect 200 episodes from each level of behavior policies as the offline demonstration data, and the number of episodes is 1,000 in the environments _Lift_ and _Crash_, which all have continuous state and action space. A more detailed view of the environment hyperparameters and behavior policy design is in Appendix C.5.

BaselinesWe compare our proposed BECAUSE with several offline causal RL or MBRL baselines. **ICIL** learns a dynamic-aware invariant causal representation learning to assist a generalizable policy learning from offline datasets. **CCIL** conducts a soft intervention in our offline setting by jointly optimizing policy parameters and masks over the state. **MnM** unifies the objective of jointly training the model and policy, which allocates larger weights in the state prediction loss in the high-reward region. **Delphic** introduces delphic uncertainty to differentiate between uncertainties caused by hidden confounders and traditional epistemic and aleatoric uncertainties. **TD3+BC** is an offline model-free RL approach that combines the Twin Delayed Deep Deterministic Policy

Figure 5: Results of BECAUSE and baselines in different tasks. (a) Average success rate in distribution and out of distribution. (b) Average success rate w.r.t. ratio of offline samples. (c) Average success rate w.r.t. spurious level in the environments. We evaluate the mean and standard deviation of the best performance among 10 random seeds and report task-wise results in Appendix Table 6.

Gradient (TD3) algorithm with Behavior Cloning (BC) to adopt both the actor-critic framework and supervised learning from expert demonstrations. **MOPO** is an offline MBRL approach that uses flat latent space and count-based uncertainty quantification to maintain conservatism in online deployment. **GNN** is a GNN-based baseline using a Relational Graph Convolutional Network to model the temporal dependency of state-action pairs in the dynamic model with message passing. **CDL** uses causal discovery to learn a task-independent world model. **Denoised MDP** and **IFactor** conduct causal state abstraction based on their controllability and reward relevance. The last three methods are designed for online settings, so we only implement their model learning objectives. We attach more details of the baseline implementation in Appendix C.6.

### Experiment Results Analysis

We empirically answer the following research questions.

* **RQ1**: How is the generalizability of BECAUSE in the online environments (which may be _unseen_)? Specifically, how does BECAUSE perform under diverse qualities of demonstration data (different level of \(u_{}\)), and different environment contexts (different \(u_{c}\))?
* **RQ2**: How does the design in BECAUSE contribute to the robustness of its final performance under different sample sizes or spurious levels?
* **RQ3**: How does BECAUSE scale up to visual RL tasks with image observation input compared to other visual RL baselines?
* **RQ4**: How does BECAUSE achieve the aforementioned generalizability by mitigating the objective mismatch problem in offline MBRL?

For **RQ1**, in Figure 5(a), we evaluate the success rate in the online environment against different baselines. The result shows that under different environments and different qualities of behavior policies \(_{}\) (different \(u_{}\)), BECAUSE consistently achieves the best performance in 8 out of 9 for both the in-distribution (I) and out-of-distribution (O) for all the demonstration data quality (different level of \(u_{}\)). Where O here indicates the tasks under _unseen_ environment with confounder \(u^{}_{c} u_{c}\) different from offline training. Another finding is that model-based approaches generally perform better than model-free approaches at various levels of offline data, which shows the importance of world model learning for generalizable offline RL. We attach the detailed results in the Appendix Table 6, 7 and the causal masks discovered in each environment in Appendix Figure 8 for reference.

For **RQ2**, we compare different aspects of BECAUSE's robustness with MOPO without causal structures . We compare their performance with different ratios of the entire offline dataset and illustrate the success rates in Figure 5(b). The result shows that, for any selected number of samples, BECAUSE consistently outperforms MOPO with a clear margin. We also evaluate BECAUSE performance at higher spurious levels in Figure 5(c). We add up to \(8\) of the original number of confounders in the environments to test the robustness of the agent's performance. BECAUSE consistently outperforms MOPO and the margin enlarges as the spurious level grows higher.

For **RQ3**, we conduct experiments with visual inputs in the Unlock environments with IICIL  and IFactor . We parameterize the feature encoder as a three-layer CNN with 128 dimensions hidden size for all the baselines and our methods. The results in Table 1 show that BECAUSE can significantly improve both in-distribution and out-of-distribution performance under different quality of behavior policies.

For **RQ4**, we aim to understand whether BECAUSE achieves higher performance by resolving the objective mismatch problem. We first collect two groups of trajectories: \(_{pos}\) and \(_{neg}\), each with positive reward (success) and negative reward (failure) in _Unlock_ task with sparse goal-reaching reward. We want to have a model whose loss is informative for discriminating control results, that is, we wish \(_{model}(_{pos})<_{model}(_{neg})\). According to our visualization in Figure 6, in _Unlock-Expert_ and _Unlock-Medium_, the ratio of \(_{pos}\) is much higher in BECAUSE than MOPO among the trajectories with low model loss. In _Unlock-Random_, the mismatch of the model and

  
**Tasks** & **ICIL** & **IFactor** & **BECAUSE** \\  Unlock-I-random & 0.8\(\)0.8 & 4.3\(\)1.1 & **15.7\(\)3.3** \\ Unlock-O-random & 1.5\(\)1.8 & **4.7\(\)1.6** & 5.9\(\)0.9 \\ Unlock-I-medium & 5.3\(\)2.0 & 30.2\(\)4.1 & **62.0\(\)4.6** \\ Unlock-O-medium & 8.6\(\)4.2 & 15.4\(\)2.4 & **71.6\(\)9.1** \\ Unlock-I-expert & 8.7\(\)3.4 & 34.0\(\)4.8 & **63.7\(\)3.9** \\ Unlock-O-expert & 17.1\(\)4.2 & 16.7\(\)3.1 & **73.6\(\)19.5** \\   

Table 1: Comparison of visual RL performance.

control objective is more significant, since the demonstration is poor in state coverage. MOPO cannot succeed even when the model loss is low, whereas our methods can. We perform a hypothesis test with \(H_{0}:_{model}(_{pos})<_{model}(_{neg})\). In BECAUSE, this desired property is more significant than MOPO attributed to the causal representation we learn, indicating a reduction of objective mismatch. We attach detailed discussions for the mismatch evaluation in Appendix C.3 and Table 5.

### Ablation Studies

We conducted ablation studies with three variants of BECAUSE and report the average success rate across nine in-distribution and nine out-of-distribution tasks in Table 2. The **Optimism** variant conducts optimistic planning instead of pessimistic planning in Equation (9), which uses uniform sampling in the planner module. The **Linear** variant assumes a full connection to the causal matrix \(M\), then directly uses linear MDP to parameterize the dynamics model \(T\), which removes the causal discovery module in BECAUSE. The **Full** variant learns from the full batch of data to estimate the causal mask without iterative update. We report the results of the task-wise ablation with confidence interval and significance in Appendix Table 8 and 9.

## 5 Related Works

Objective Mismatch in MBRLThe objective mismatch in MBRL [43; 44] refers to the fact that pure MLE estimation of the world model does not align well with the control objective. Previous works [29; 45] propose reweighting during model training to alleviate this mismatch,  proposes a goal-aware prediction by redistributing model error according to their task relevance. These works essentially reweight loss for the entire model training, while our work conducts reweighting just over the estimated causal mask more efficiently. More recently, [9; 10] proposed a joint training between the world model and policies. Although joint optimization improves performance, they do not address the generalizability of the learned model under the distribution shift setting. In the offline setting, Model-based RL [2; 3; 4; 47] employs model ensemble, pessimistic policy optimization or value iteration [48; 49], and an energy-based model for planning  to quantify uncertainty and improve test performance. To the best of our knowledge, no previous work explored or modeled the impact of distribution shift on the objective mismatch problem in MBRL.

  
**Variants** & **BECAUSE** & **Optimism** & **Linear** & **Full** \\  Overall-I & **73.3\(\)4.5** & 64.4\(\)6.4 & 57.9\(\)6.1 & 39.3\(\)6.3 \\ Overall-O & **43.0\(\)4.9** & 32.4\(\)3.3 & 33.2\(\)5.2 & 25.2\(\)3.9 \\   

Table 2: The ablation studies between BECAUSE and its variants. We report the overall Success rate (%) over 9 in-distribution (I) and 9 out-of-distribution (O) tasks, respectively. **Bold** is the best.

Figure 6: Evaluation of the difference between the distribution of episodic model loss for success and failure trajectories. The higher difference indicates a reduction in model mismatch issues. An example of failure mode is trying to open the door without having the key.

Causal Discovery with ConfounderMost of the existing causal discovery methods  can be categorized into constraint-based and score-based. Constraint-based methods  start from a complete graph and iteratively remove edges with statistical hypothesis testing [53; 54]. This type of method is highly data-efficient but not robust to noisy data. As a remedy, score-based methods [55; 56] use metrics such as the likelihood or BIC  as scores to manipulate edges in the causal graph. Recently, researchers have extended score-based methods with RL , order learning  or differentiable discovery [22; 60; 61]. To alleviate the non-identifiability under hidden confounders, active intervention methods have been explored , aiming to break spurious correlations in an online fashion. With extra assumptions on confounders, some recent works detect such correlations [63; 64; 65] so that models can effectively identify elusive confounders.

Causal Reinforcement LearningRecently, many RL algorithms have incorporated causality to improve reasoning capability  and generalizability. For instance,  and  explicitly estimate causal structures with the interventional data obtained from the environment in an online setting. These structures can be used to constrain the output space  or to adjust the buffer priority . Building dynamic models in model-based RL [24; 70; 71] based on causal graphs is widely studied. Most existing causal MBRL works focus on estimating the causal world model by predicting transition dynamics and rewards. Existing methods learn this causal world model via structural regularization [23; 72; 73], conditional independence test [24; 70; 74], variational inference [75; 12], counterfactual data augmentation [76; 77], hierarchical skill abstraction [78; 79], uncertainty quantification , reward redistribution [24; 80], causal context modeling [81; 82] and structure-aware state abstraction [12; 13; 83; 84] based on the controllability and task or reward relevance. However, the presence of confounders during data collection can skew the learned policy, making it susceptible to spurious correlations. Deconfounding solutions have been proposed either between actions and states [39; 85; 86] or among different dimensions of state variables [19; 87].

## 6 Conclusion

In this paper, we study how to mitigate the objective mismatch problem in MBRL, especially under the offline settings where distribution shift occurs. We first propose ASC-MDP and the bilinear causal representation associated with it. Based on the formulation, we proposed how to learn this causal abstraction by alternating between causal mask learning and feature learning in fitting the world dynamics. In the planning stage, we applied the learned causal representation to an uncertainty quantification module based on EBM, which improves the robustness under uncertainty in the online planning stage. We theoretically justify BECAUSE's sub-optimality bound induced by the sparse matrix estimation problem and offline RL. Comprehensive experiments on 18 different tasks show that given a diverse level of demonstration as the offline dataset, BECAUSE has better generalizability than baselines in different online environments, and it robustly outperforms baselines under different spurious levels or sample sizes. We empirically show that BECAUSE mitigates the objective mismatch with causal awareness learned from offline data. One limitation of BECAUSE lies in its simplified assumption of time-homogeneous causal structure, which may not always hold in long-horizon or non-stationary settings. Besides, the current implementation is still based on vector observations. It will be interesting to scale up the causal reasoning framework into high-dimensional observations to discover concept factors in long-horizon visual RL settings.