# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

In this paper, we study the general problem of smooth (with Lipschitz gradient and Hessian) stochastic nonconvex optimization \(_{x}(x)\) in the outlier-robust setting, where \((x):=_{A}f(x,A)\) and \(\) is a possibly unknown distribution of the random parameter \(A\). We will focus on the following standard adversarial contamination model (see, e.g., ).

**Definition 1.1** (Strong Contamination Model).: Given a parameter \(0<<1/2\) and an inlier distribution \(\), an algorithm receives samples from \(\) with \(\)-contamination as follows: The algorithm first specifies the number of samples \(n\), and \(n\) samples are drawn independently from \(\). An adversary is then allowed to inspect these samples, and replace an \(\)-fraction of the samples with arbitrary points. This modified set of \(n\) points is said to be \(\)-corrupted, which is then given to the algorithm.

The stochastic optimization problem we consider is computationally intractable in full generality -- even without corruption -- if the goal is to obtain globally optimal solutions. At a high level, an achievable goal is to design sample and computationally efficient robust algorithms for finding _locally_ optimal solutions. Prior work  studied outlier-robust stochastic optimization and obtained efficient algorithms for finding approximate _first-order_ stationary points. While first-order guarantees suffice for convex problems, it is known that in many tractable non-convex problems, first-order stationary points may be bad solutions, but all _second-order_ stationary points (SOSPs) are globally optimal. This motivates us to study the following questions:

_Can we develop a general framework for finding **second-order** stationary points in outlier-robust stochastic optimization?_

_Can we obtain sample and computationally efficient algorithms for outlier-robust versions of tractable nonconvex problems using this framework?_

In this work, we answer both questions affirmatively. We introduce a framework for efficiently finding an approximate SOSP when \(\)-fraction of the functions are corrupted and then use our framework to solve the problem of outlier-robust low rank matrix sensing.

In addition to the gradient being zero, a SOSP requires the Hessian matrix to not have negative eigenvalues. The second-order optimality condition is important because it rules out suboptimal solutions such as strict saddle points. It is known that all SOSPs are globally optimal in nonconvex formulations of many important machine learning problems, such as matrix completion , matrix sensing , phase retrieval , phase synchronization , dictionary learning , and tensor decomposition  (see also  Chapter 7). However, the properties of SOSPs are highly sensitive to perturbation in the input data. For example, it is possible to create spurious SOSPs for nonconvex formulations of low rank matrix recovery problems, even for a semi-random adversary that can add additional sensing matrices but cannot corrupt the measurements in matrix sensing  or an adversary who can only reveal more entries of the ground-truth matrix in matrix completion . Those spurious SOSPs correspond to highly suboptimal solutions.

Finding SOSPs in stochastic nonconvex optimization problems in the presence of arbitrary outliers was largely unaddressed prior to our work. Prior works  obtained efficient and robust algorithms for finding _first-order_ stationary points with dimension-independent accuracy guarantees. These works relied on the following simple idea: Under certain smoothness assumptions, projected gradient descent with an _approximate_ gradient oracle efficiently converges to an _approximate_ first-order stationary point. Moreover, in the outlier-robust setting, approximating the gradient at a specific point amounts to a robust mean estimation problem (for the underlying distribution of the gradients), which can be solved by leveraging existing algorithms for robust mean estimation.

Our work is the first to find approximate SOSPs with dimension-independent errors in outlier-robust settings. Note that in standard non-robust settings, approximate SOSPs can be computed using first-order methods such as perturbed gradient descent . This strategy might seem extendable to outlier-robust settings through perturbed approximate gradient descent, utilizing robust mean estimation algorithms to approximate gradients. The approach in  follows this idea, but unfortunately their second-order guarantees scale polynomially with dimension, even under very strong distributional assumptions (e.g., subgaussianity). Our lower bound result provides evidence that approximating SOSPs with dimension-independent error is as hard as approximating _full_ Hessian, suggesting that solely approximating the gradients is not sufficient. On a different note,  recently employed robust estimators for both gradient and Hessian in solving certain convex stochastic optimization problems, which has a different focus than ours and does not provide SOSPs with the guarantees that we achieve.

### Our Results and Contributions

The notation we use in this section is defined in Section2. To state our results, we first formally define our generic nonconvex optimization problem. Suppose there is a true distribution over functions \(f:^{D}\), where \(f(x,A)\) takes an argument \(x^{D}\) and is parameterized by a random variable \(A\) drawn from a distribution \(\). Our goal is to find an \((_{g},_{H})\)-approximate SOSP of the function \((x):=_{A}\,f(x,A)\).

**Definition 1.2** (\(\)-Corrupted Stochastic Optimization).: The algorithm has access to \(n\) functions \((f_{i})_{i=1}^{n}\) generated as follows. First \(n\) random variables \((A_{i})_{i=1}^{n}\) are drawn independently from \(\). Then an adversary arbitrarily corrupts an \(\) fraction of the \(A_{i}\)'s. Finally, the \(\)-corrupted version of \(f_{i}()=f(,A_{i})\) is sent to the algorithm as input. The task is to find an approximate SOSP of the ground-truth average function \(():=_{A}\,f(,A)\).

**Definition 1.3** (Approximate SOSPs).: A point \(x\) is an \((_{g},_{H})\)-approximate second-order stationary point (SOSP) of \(\) if \(\|(x)\|_{g}\) and \(_{}(^{2}(x))-_{H}\).

We make the following additional assumptions on \(f\) and \(\).

**Assumption 1.4**.: _There exists a bounded region \(\) such that the following conditions hold:_

1. _There exists a lower bound_ \(f^{*}>-\) _such that for all_ \(x\)_,_ \(f(x,A) f^{*}\) _with probability_ \(1\)_._
2. _There exist parameters_ \(L_{D_{g}}\)_,_ \(L_{D_{H}}\)_,_ \(B_{D_{g}}\)_, and_ \(B_{D_{H}}\) _such that, with high probability over the randomness in_ \(A\)_, letting_ \(g(x)=f(x,A)\)_, we have that_ \(g(x)\) _is_ \(L_{D_{g}}\)_-gradient Lipschitz and_ \(L_{D_{H}}\)_-Hessian Lipschitz over_ \(\)_, and_ \(\| g(x)\| B_{D_{g}}\) _and_ \(\|^{2}g(x)\|_{F} B_{D_{H}}\) _for all_ \(x\)_._
3. _There exist parameters_ \(_{g},_{H}>0\) _such that for all_ \(x\)_,_ \[\|_{A}( f(x,A))\|_{} _{g}^{2}\|_{A}( (^{2}f(x,A)))\|_{}_{H}^{2}.\]

Note that the radius of \(\) and the parameters \(L_{D_{g}}\), \(L_{D_{H}}\), \(B_{D_{g}}\), \(B_{D_{H}}\) are all allowed to depend polynomially on \(D\) and \(\) (but not on \(x\) and \(A\)).

Our main algorithmic result for \(\)-corrupted stochastic optimization is summarized in the following theorem. A formal version of this theorem is stated as Theorem3.1 in Section3.

**Theorem 1.5** (Finding an Outlier-Robust SOSP, informal).: _Suppose \(f\) satisfies Assumption1.4 in a region \(\) with parameters \(_{g}\) and \(_{H}\). Given an arbitrary initial point \(x_{0}\) and an \(\)-corrupted set of \(n=D^{2}/\) functions where \(D\) is the ambient dimension, there exists a polynomial-time algorithm that with high probability outputs an \((O(_{g})\,,O(_{H}))\)-approximate SOSP of \(\), provided that all iterates of the algorithm stay inside \(\)._

Although the bounded iterate condition in Theorem1.5 appears restrictive, this assumption holds if the objective function satisfies a "dissipativity" property, which is a fairly general phenomenon . Moreover, adding an \(_{2}\)-regularization term enables any Lipschitz function to satisfy the dissipativity property . Section4. As an illustrating example, a simple problem-specific analysis shows that this bounded iterate condition holds for outlier-robust matrix sensing by exploiting the fact that the matrix sensing objective satisfies the dissipativity property.

In this paper, we consider the problem of outlier-robust symmetric low rank matrix sensing, which we formally define below. We focus on the setting with Gaussian design.

**Definition 1.6** (Outlier-Robust Matrix Sensing).: There is an unknown rank-\(r\) ground-truth matrix \(M^{*}^{d d}\) that can be factored into \(U^{*}U^{*}{}^{}\) where \(U^{*}^{d r}\). The (clean) sensing matrices \(\{A_{i}\}_{i[n]}\) have i.i.d. standard Gaussian entries. The (clean) measurements \(y_{i}\) are obtained as \(y_{i}= A_{i},M^{*}+_{i}\), where the noise \(_{i}(0,^{2})\) is independent from all other randomness. We denote the (clean) data generation process by \((A_{i},y_{i})_{}\). When \(=0\), we have \(_{i}=0\) and we write \(:=_{0}\) for this noiseless (measurement) setting. In outlier robust matrix sensing, an adversary can arbitrarily change any \(\)-fraction of the sensing matrices and the corresponding measurements. This corrupted set of \((A_{i},y_{i})\)'s is then given to the algorithm as input, where the goal is to recover \(M^{*}\).

[MISSING_PAGE_EMPTY:4]

similar to strong convexity but holds only locally). This local regularity condition bounds below a measure of stationarity, which allows us to prove that gradient descent-type updates contract the distance to the closest global optimum under appropriate stepsize. We leverage this local regularity condition to prove that the iterates of the algorithm stay near a global optimum, so that the regularity condition continues to hold, and moreover, the distance between the current solution and the closest global optimum contracts, as long as it is larger than a function of \(\). Consequently, the distance-dependent component of the gradient and Hessian covariance bound contracts as well, which allows us to obtain more accurate gradient and Hessian estimates. While such a statement may seem evident to readers familiar with linear convergence arguments, we note that proving it is quite challenging, due to the circular dependence between the distance from the current solution to global optima, the inexactness in the gradient and Hessian estimates, and the progress made by our algorithm.

The described distance-contracting argument allows us to control the covariance of the gradient and Hessian, which we utilize to recover \(M^{*}\) exactly when \(=0\), and recover \(M^{*}\) with error roughly \(O()\) when \(0 r\). We note that the \(\) appears unavoidable in the \( 0\) case, due to known limits of robust mean estimation algorithms .

SQ lower bound.We exhibit a hard instance of low rank matrix sensing problem to show that quadratic dependence on the dimension in sample complexity is unavoidable for computationally efficient algorithms. Our SQ lower bound proceeds by constructing a family of distributions, corresponding to corruptions of low rank matrix sensing, that are nearly uncorrelated in a well-defined technical sense . To achieve this, we follow the framework of  which considered a family of distributions that are rotations of a carefully constructed one-dimensional distribution. The proof builds on , using a new univariate moment-matching construction which yields a family of corrupted conditional distributions. These induce a family of joint distributions that are SQ-hard to learn.

### Roadmap

Section 2 defines the necessary notation and discusses relevant building blocks of our algorithm and analysis. Section 3 introduces our framework for finding SOSPs in the outlier-robust setting. Section 4 presents how to extend and apply our framework to solve outlier-robust low rank matrix sensing. Section 5 proves that our sample complexity has optimal dimensional dependence for SQ algorithms. Most proofs are deferred to the supplementary material due to space limitations.

## 2 Preliminaries

For an integer \(n\), we use \([n]\) to denote the ordered set \(\{1,2,,n\}\). We use \([a_{i}]_{i}\) to denote the matrix whose columns are vectors \(a_{i}\), where \(\) is an ordered set. We use \(_{E}(x)\) to denote the indicator function that is equal to \(1\) if \(x E\) and \(0\) otherwise. For two functions \(f\) and \(g\), we say \(f=(g)\) if \(f=O(g^{k}(g))\) for some constant \(k\), and we similarly define \(\).

For vectors \(x\) and \(y\), we let \( x,y\) denote the inner product \(x^{}y\) and \(\|x\|\) denote the \(_{2}\) norm of \(x\). For \(d_{+}\), we use \(I_{d}\) to denote the identity matrix of size \(d d\). For matrices \(A\) and \(B\), we use \(\|A\|_{}\) and \(\|A\|_{F}\) to denote the spectral norm and Frobenius norm of \(A\) respectively. We use \(_{}(A)\) and \(_{}(A)\) to denote the maximum and minimum eigenvalue of \(A\) respectively. We use \((A)\) to denote the trace of a matrix \(A\). We use \( A,B=(A^{}B)\) to denote the entry-wise inner product of two matrices of the same dimension. We use \((A)=[a_{1}^{},a_{2}^{},,a_{d}^{}]^{}\) to denote the canonical flattening of \(A\) into a vector, where \(a_{1},a_{2},,a_{d}\) are columns of \(A\).

**Definition 2.1** (Lipschitz Continuity).: Let \(\) and \(\) be normed vector spaces. A function \(h:\) is \(\)-Lipschitz if \(\|h(x_{1})-h(x_{2})\|_{}\|x_{1}-x_{2}\| _{}, x_{1},x_{2}\).

In this paper, when \(\) is a space of matrices, we take \(\|\|_{}\) to be the spectral norm \(\|\|_{}\). When \(\) is a space of matrices, we take \(\|\|_{}\) to be the Frobenius norm \(\|\|_{F}\); this essentially views the function \(h\) as operating on the vectorized matrices endowed with the usual \(_{2}\) norm. When \(\) or \(\) is the Euclidean space, we take the corresponding norm to be the \(_{2}\) norm.

A Randomized Algorithm with Inexact Gradients and Hessians.We now discuss how to solve the unconstrained nonconvex optimization problem \(_{x^{D}}f(x),\) where \(f()\) is a smooth function 

[MISSING_PAGE_FAIL:6]

Then we have the following guarantee:

**Theorem 3.1**.: _Suppose we are given \(\)-corrupted set of functions \(\{f_{i}\}_{i=1}^{n}\) for sample size \(n\), generated according to Definition [1.2] Suppose Assumption [1.4]holds in a bounded region \(^{D}\) of diameter \(\) with gradient and Hessian covariance bound \(_{g}\) and \(_{H}\) respectively, and we have an arbitrary initialization \(x_{0}\). Algorithm [1.4]initialized at \(x_{0}\) outputs an \((_{g},_{H})\)-approximate SOSP for a sufficiently large sample with probability at least \(1-\) if the following conditions hold:_

1. _All iterates_ \(x_{t}\) _in Algorithm_ [1.4]_ _stay inside the bounded region_ \(\)_._
2. _For an absolute constant_ \(c>0\)_, it holds that_ \(_{g} c_{g}\) _and_ \(_{H} c_{H}\)_._

_The algorithm uses \(n=(D^{2}/)\) samples, where \(()\) hides logarithmic dependence on \(D,,L_{D_{g}},L_{D_{H}},B_{D_{g}},B_{D_{H}},/_{H},/ _{g},\) and \(1/\). The algorithm runs in time polynomial in the above parameters._

Note that we are able to obtain dimension-independent errors \(_{g}\) and \(_{H}\), provided that \(_{g}\) and \(_{H}\) are dimension-independent.

### Low Rank Matrix Sensing Problems

In this section, we study the problem of outlier-robust low rank matrix sensing as formally defined in Definition [1.6]. We first apply the above framework to obtain an approximate SOSP in Section 8.1.2. Then we make use of the approximate SOSP to obtain a solution that is close to the ground-truth matrix \(M^{*}\) in Section 8.1.3 this demonstrates the usefulness of approximate SOSPs.

#### 3.1.1 Main results for Robust Low Rank Matrix Sensing

The following are the main results that we obtain in this section:

**Theorem 3.2** (Main Theorem Under Noiseless Measurements).: _Consider the noiseless setting as in Theorem [1.7] with \(=0\). For some sample size \(n=((d^{2}r^{2}+dr(/))/)\) and with probability at least \(1-\), there exists an algorithm that outputs a solution that is \(\)-close to \(M^{*}\) in Frobenius norm in \(O(r^{2}^{3}(1/)+(_{r}^{*}/))\) calls to the robust mean estimation subroutine (Algorithm [2.2])._

This result achieves exact recovery of \(M^{*}\), despite the strong contamination of samples. Each iteration involves a subroutine call to robust mean estimation. Algorithm [1.2] presented here is one simple example of robust mean estimation; there are refinements  that run in nearly linear time, so the total computation utilizing those more efficient algorithms indeed requires \((r^{2}^{3})\) passes of data (computed gradients and Hessians).

**Theorem 3.3** (Main Theorem Under Noisy Measurements).: _Consider the same setting as in Theorem [1.7]with \( 0\). There exists a sample size \(n=((d^{2}r^{2}+dr(/))/)\) such that_

* _if_ \( r\)_, then with probability at least_ \(1-\)_, there exists an algorithm that outputs a solution_ \(\) _in_ \((r^{2}^{3})\) _calls to robust mean estimation routine_ [1.2] _with error_ \(\|-M^{*}\|_{F}=O()\)_;_
* _if_ \( r\)_, then with probability at least_ \(1-\)_, there exists a (different) algorithm that outputs a solution_ \(\) _in one call to robust mean estimation routine_ [1.2] _with error_ \(\|-M^{*}\|_{F}=O()\)_._

We prove Theorem [3.3]in Appendix [4.4] and instead focus on the noiseless measurements with \(=0\) when we develop our algorithms in this section; the two share many common techniques. In the remaining part of Section 8.1 we use \(_{0}\) in Definition [1.6] for the data generation process.

We now describe how we obtain the solution via nonconvex optimization. Consider the following objective function for (uncorrupted) matrix sensing:

\[_{M^{d d}\\ (M)=r}*{}_{(A_{ i},y_{i})_{0}}( M,A_{i}-y_{i})^{2}. \]

We can write \(M=UU^{}\) for some \(U^{d r}\) to reparameterize the objective function. Let

\[f_{i}(U):=( UU^{},A_{i}-y_{i})^{2}. \]We can compute

\[(U):=}_{(A_{i},y_{i})_{0}}f_{i}(U)=} UU^{}-M^{*},A_{i}=\| UU^{}-M^{*}\|_{F}^{2}. \]

We seek to solve the following optimization problem under the corruption model in Definition 1.2

\[_{U^{d r}}(U). \]

The gradient Lipschitz constant and Hessian Lipschitz constant of \(\) are given by the following result.

**Fact 3.4** (, Lemma 6).: _For any \(>_{1}^{*}\), \((U)\) has gradient Lipschitz constant \(L_{g}=16\) and Hessian Lipschitz constant \(L_{H}=24^{}\) inside the region \(\{U:\|U\|_{}^{2}<\}\)._

#### 3.1.2 Global Convergence to an Approximate SOSP

In this section, we apply our framework Theorem 6.1 to obtain global convergence from an arbitrary initialization to an approximate SOSP, by providing problem-specific analysis to guarantee that both Assumption 1.4 and algorithmic assumptions (I) and (II) required by Theorem 6.1 are satisfied.

**Theorem 3.5** (Global Convergence to a SOSP).: _Consider the noiseless setting as in Theorem 1.7 with \(=0\) and \(=O(1/(^{3}r^{3}))\). Assume we have an arbitrary initialization \(U_{0}\) inside \(\{U:\|U\|_{}^{2}\}\). There exists a sample size \(n=((d^{2}r^{2}+dr(/))/)\) such that with probability at least \(1-\), Algorithm 1.1 initialized at \(U_{0}\) outputs a \((_{r}^{3/2},_{r}^{*})\)-approximate SOSP using at most \(Or^{2}^{3}(1/)\) calls to robust mean estimation subroutine (Algorithm 2.2)._

Proof of Theorem 3.5.: To apply Theorem 3.1 we verify Assumption 1.4 first. To verify (i), for all \(U\) and \(A_{i}\), \(f_{i}(U)=( UU^{},A_{i}-y_{i})^{2} 0\), so \(f^{*}=0\) is a uniform lower bound. We verify (ii) in Appendix C.2 conceptually, by Fact 3.4\(\) is gradient and Hessian Lipschitz; both gradient and Hessian of \(f_{i}\) are sub-exponential and concentrate around those of \(\). To check (iii), we calculate the gradients and Hessians of \(f_{i}\) in Appendix C.1.1 and bound their covariances from above in Appendix C.1.2 and 1.3. The result is summarized in the following lemma. Note that the domain of the target function in Algorithm 1.1 and Theorem 9.1 is the Euclidean space \(^{D}\), so we vectorize \(U\) and let \(D=dr\). The gradient becomes a vector in \(^{dr}\) and the Hessian becomes a matrix in \(^{dr dr}\).

**Lemma 3.6** (Gradient and Hessian Covariance Bounds).: _For all \(U^{d r}\) with \(\|U\|_{}^{2}\) and \(f_{i}\) defined in Equation 3.1, it holds_

\[\|(( f_{i}(U)))\|_{ }  8\|UU^{}-M^{*}\|_{F}^{2}\|U\|_{ }^{2} 32r^{2}^{3} \] \[\|((H_{i}))\|_{}  16r\|UU^{}-M^{*}\|_{F}^{2}+128\|U\|_{ }^{4} 192r^{3}^{2} \]

We proceed to verify the algorithmic assumptions in Theorem 9.1.1 For the assumption (I), we prove the following Lemma in Appendix C.2 to show that all iterates stay inside the bounded region in which we compute the covariance bounds.

**Lemma 3.7**.: _All iterates of Algorithm 1.1 stay inside the region \(\{U:\|U\|_{}^{2}\}\)._

To verify Theorem 9.1.1(II), we let \(_{g}=_{r}^{*3/2},_{H}=_{r} ^{*}\) and \(_{g}=8r^{1.5},_{H}=16r^{1.5}\). So if we assume \(=O(r^{3})})\), then for the absolute constant \(c\) in Theorem 9.1 it holds that

\[_{g} c_{g}_{H} c _{H}.\]

Hence, Theorem 9.1.1 applies and Algorithm 1.1 outputs an \((_{g},_{H})\)-approximate SOSP with high probability in polynomial time. To bound the runtime, since \((U_{0})=1/2\|U_{0}U_{0}^{}-M^{*}\|_{F}^{2}=O(r^{2} ^{2})\) for an arbitrary initialization \(U_{0}\) with \(\|U_{0}\|_{}^{2}<\), the initial distance can be bounded by \(O(r^{2}^{2})\). Setting \(L_{g}=16,L_{H}=24^{1/2},(U_{0})=O(r^{2}^{2}),f^{*}=0\) and thus \(C_{}=O(_{r}^{*3}/)\), Proposition 2.2 implies that Algorithm 1.1 outputs a \((_{r}^{*3/2},_{r}^{*})\)-approximate second order stationary point \(U_{SOSP}\) in \(O(r^{2}^{3}(1/))\) steps with high probability.

[MISSING_PAGE_FAIL:9]

responds to the query \(q\) with a value \(v\) such that \(|v-_{X}[q(X)]|\). An SQ algorithm is an algorithm whose objective is to learn some information about an unknown distribution \(\) by making adaptive calls to the corresponding \(()\) oracle.

In this section, we consider \(\) as the unknown corrupted distribution where \((A_{i},y_{i})\) are drawn. The SQ algorithm tries to learn the ground truth matrix \(M^{*}\) from this corrupted distribution; the goal of the lower bound result is to show that this is hard.

The SQ model has the capability to implement a diverse set of algorithmic techniques in machine learning such as spectral techniques, moment and tensor methods, local search (e.g., Expectation Maximization), and several others . A lower bound on the SQ complexity of a problem provides evidence of hardness for the problem.  established that (under certain assumptions) an SQ lower bound also implies a qualitatively similar lower bound in the low-degree polynomial testing model. This connection can be used to show a similar lower bound for low-degree polynomials.

Our main result here is a near-optimal SQ lower bound for robust low rank matrix sensing that applies even for rank \(r=1\), i.e., when the ground truth matrix is \(M^{*}=uu^{}\) for some \(u^{d}\). The choice of rank \(r=1\) yields the strongest possible lower bound in our setting because it is the easiest parameter regime: Recall that the sample complexity of our algorithm is \((d^{2}r^{2})\) as in Theorems 3.2 and 3.3 and the main message of our SQ lower bound is to provide evidence that the \(d^{2}\) factor is necessary for computationally efficient algorithms _even if_\(r=1\).

**Theorem 4.2** (SQ Lower Bound for Robust Rank-One Matrix Sensing).: _Let \((0,1/2)\) be the fraction of corruptions and let \(c(0,1/2)\). Assume the dimension \(d\) is sufficiently large. Consider the \(\)-corrupted rank-one matrix sensing problem with ground-truth matrix \(M^{*}=uu^{}\) and noise \(^{2}=O(1)\). Any SQ algorithm that outputs \(\) with \(\|-u\|=O(^{1/4})\) either requires \(2^{(d^{})}/d^{2-4c}\) queries or makes at least one query to \((e^{O(1/)}/O(d^{1-2c}))\)._

In other words, we show that, when provided with SQ access to an \(\)-corrupted distribution, approximating \(u\) is impossible unless employing a statistical query of higher precision than what can be achieved with a strictly sub-quadratic number (e.g., \(d^{1.99}\)) of samples. Note that the SQ oracle \((e^{O(1/)}/O(d^{1-2c}))\) can be simulated with \(O(d^{2-4c})/e^{O(1/)}\) samples, and this bound is tight in general. Informally speaking, this theorem implies that improving the sample complexity from \(d^{2}\) to \(d^{2-4c}\) requires exponentially many queries. This result can be viewed as a near-optimal information-computation tradeoff for the problem, within the class of SQ algorithms.

The proof follows a similar analysis as in , using one-dimensional moment matching to construct a family of corrupted conditional distributions, which induce a family of corrupted joint distributions that are SQ-hard to learn. We provide the details of the proof in Appendix A Apart from the formal proof, in Appendix E we also informally discuss the intuition for why some simple algorithms that require \(O(d)\) samples do not provide dimension-independent error guarantees.