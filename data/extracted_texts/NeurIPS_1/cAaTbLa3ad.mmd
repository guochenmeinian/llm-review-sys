# Estimating the Rate-Distortion Function by Wasserstein Gradient Descent

Yibo Yang\({}^{1}\)   Stephan Eckstein\({}^{2}\)   Marcel Nutz\({}^{3}\)   Stephan Mandt\({}^{1}\)

\({}^{1}\)University of California, Irvine  \({}^{2}\)ETH Zurich  \({}^{3}\)Columbia University

{yibo.yang, mandt}@uci.edu

stephan.eckstein@math.ethz.ch

mmutz@columbia.edu

###### Abstract

In the theory of lossy compression, the rate-distortion (R-D) function \(R(D)\) describes how much a data source can be compressed (in bit-rate) at any given level of fidelity (distortion). Obtaining \(R(D)\) for a given data source establishes the fundamental performance limit for all compression algorithms. We propose a new method to estimate \(R(D)\) from the perspective of optimal transport. Unlike the classic Blahut-Arimoto algorithm which fixes the support of the reproduction distribution in advance, our Wasserstein gradient descent algorithm learns the support of the optimal reproduction distribution by moving particles. We prove its local convergence and analyze the sample complexity of our R-D estimator based on a connection to entropic optimal transport. Experimentally, we obtain comparable or tighter bounds than state-of-the-art neural network methods on low-rate sources while requiring considerably less tuning and computation effort. We also highlight a connection to maximum-likelihood deconvolution and introduce a new class of sources that can be used as test cases with known solutions to the R-D problem.

## 1 Introduction

The rate-distortion (R-D) function \(R(D)\) occupies a central place in the theory of lossy compression. For a given data source and a fidelity (or _distortion_) criterion, \(R(D)\) characterizes the minimum possible communication cost needed to reproduce a source sample within an error threshold of \(D\), by _any_ compression algorithm . A basic scientific and practical question is therefore establishing \(R(D)\) for any given data source of interest, which helps assess the (sub)optimality of the compression algorithms and guide their development. The classic algorithm by Blahut  and Arimoto  assumes a known discrete source and computes its \(R(D)\) by an exhaustive optimization procedure. This often has limited applicability in practice, and a line of research has sought to instead _estimate_\(R(D)\) from data samples , with recent methods  inspired by deep generative models.

In this work, we propose a new approach to R-D estimation from the perspective of optimal transport. Our starting point is the formulation of the R-D problem as the minimization of a certain rate functional  over the space of probability measures on the reproduction alphabet. Optimization over such an infinite-dimensional space has long been studied under gradient flows , and we consider a concrete algorithmic implementation based on moving particles in space. This formulation of the R-D problem also suggests connections to entropic optimal transport and non-parametric statistics, each offering us new insight into the solution of the R-D problem under a quadratic distortion. More specifically, our contributions are three-fold:

First, we introduce a neural-network-free \(R(D)\) upper bound estimator for continuous alphabets. We implement the estimator by Wasserstein gradient descent (WGD) over the space of reproductiondistributions. Experimentally, we found the method to converge much more quickly than state-of-the-art neural methods with hand-tuned architectures, while offering comparable or tighter bounds.

Second, we theoretically characterize convergence of our WGD algorithm and the sample complexity of our estimator. The latter draws on a connection between the R-D problem and that of minimizing an entropic optimal transport (EOT) cost relative to the source measure, allowing us to turn statistical bounds for EOT (Mena and Niles-Weed, 2019) into finite-sample bounds for R-D estimation.

Finally, we introduce a new, rich class of sources with known ground truth, including Gaussian mixtures, as a benchmark for algorithms. While the literature relies on the Gaussian or Bernoulli for this purpose, we use the connection with maximum likelihood deconvolution to show that a Gaussian convolution of _any_ distribution can serve as a source with a known solution to the R-D problem.

## 2 Lossy compression, entropic optimal transport, and MLE

This section introduces the R-D problem and its rich connections to entropic optimal transport and statistics, along with new insights into its solution. Sec. 2.1 sets the stage for our method (Sec. 4) by a known formulation of the standard R-D problem as an optimization problem over a space of probability measures. Sec. 2.2 discusses the equivalence between the R-D problem and a projection of the source distribution under entropic optimal transport; this is a key to our sample complexity results in Sec. 4.3. Lastly, Sec 2.3 gives a statistical interpretation of R-D as maximum-likelihood deconvolution and uses it to analytically derive a segment of the R-D curve for a new class of sources under quadratic distortion; this allows us to assess the optimality of algorithms in experiment Sec. 5.1.

### Setup

For a memoryless data source \(X\) with distribution \(P_{X}\), its rate-distortion (R-D) function describes the minimum possible number of bits per sample needed to reproduce the source within a prescribed distortion threshold \(D\). Let the source and reproduction take values in two sets \(\) and \(\), known as the source and reproduction _alphabets_, and let \(:(,)[0,)\) be a given distortion function. The R-D function is defined by the following optimization problem (Polyanskiy and Wu, 2022),

\[R(D)=_{Q_{Y|X}:_{P_{X}Q_{Y|X}}[(X,Y)] D}I(X;Y), \]

where \(Q_{Y|X}\) is any Markov kernel from \(\) to \(\) conceptually associated with a (possibly) stochastic compression algorithm, and \(I(X;Y)\) is the mutual information of the joint distribution \(P_{X}Q_{Y|X}\).

For ease of presentation, we now switch to a more abstract notation without reference to random variables. We provide the precise definitions in the Supplementary Material. Let \(\) and \(\) be standard Borel spaces; let \(()\) be a fixed probability measure on \(\), which should be thought of as the source distribution \(P_{X}\). For a measure \(\) on the product space \(\), the notation \(_{1}\) (or \(_{2}\)) denotes the first (or second) marginal of \(\). For any \(()\), we denote by \((,)\) the set of couplings between \(\) and \(\) (i.e., \(_{1}=\) and \(_{2}=\)). Similarly, \((,)\) denotes the set of measures \(\) with \(_{1}=\). Throughout the paper, \(K\) denotes a transition kernel (conditional distribution) from \(\) to \(\), and \( K\) denotes the product measure formed by \(\) and \(K\). Then \(R(D)\) is equivalent to

\[R(D)=_{K: d( K) D}H( K|(  K)_{2})=_{(,): d D}H(|_{1} _{2}), \]

where \(H\) denotes relative entropy, i.e., for two measures \(,\) defined on a common measurable space, \(H(|):=()d\) when \(\) is absolutely continuous w.r.t \(\), and infinite otherwise.

To make the problem more tractable, we follow the approach of the classic Blahut-Arimoto algorithm (Blahut, 1972; Arimoto, 1972) (to be discussed in Sec. 3.1) and work with an equivalent unconstrained Lagrangian problem as follows. Instead of parameterizing the R-D function via a distortion threshold \(D\), we parameterize it via a Lagrange multiplier \( 0\). For each fixed \(\) (usually selected from a predefined grid), we aim to solve the following optimization problem,

\[F_{}():=_{()}_{(, )} d+H(|). \]

Geometrically, \(F_{}()\) is the y-axis intercept of a tangent line to the \(R(D)\) with slope \(-\), and \(R(D)\) is determined by the convex envelope of all such tangent lines (Gray, 2011). To simplify notation, we often drop the dependence on \(\) (e.g., we write \(F()=F_{}()\)) whenever it is harmless.

To set the stage for our later developments, we write the unconstrained R-D problem as

\[F_{}()=_{()}_{BA}(,), \]

\[_{BA}(,):=_{(,)} d+H( |)=_{K} d( K)+H( K| ), \]

where we refer to the optimization objective \(_{BA}\) as the _rate function_(Harrison and Kontoyiannis, 2008). We abuse the notation to write \(_{BA}():=_{BA}(,)\) when it is viewed as a function of \(\) only, and refer to it as the _rate functional_. The rate function characterizes a generalized Asymptotic Equipartition Property, where \(_{BA}(,)\) is the asymptotically optimal cost of lossy compression of data \(X\) using a random codebook constructed from samples of \(\)(Dembo and Kontoyiannis, 2002). Notably, the optimization in (5) can be solved analytically (Csiszar, 1974a, Lemma 1.3), and \(_{BA}\) simplifies to

\[_{BA}(,)=_{}-(_{}e^{- (x,y)}(dy))(dx). \]

In practice, the source \(\) is only accessible via independent samples, on the basis of which we propose to estimate its \(R(D)\), or equivalently \(F()\). Let \(^{m}\) denote an \(m\)-sample empirical measure of \(\), i.e., \(^{m}=_{i=1}^{m}_{x_{i}}\) with \(x_{1,n}\) being independent samples from \(\), which should be thought of as the "training data". Following Harrison and Kontoyiannis (2008), we consider two kinds of (plug-in) estimators for \(F()\): (1) the non-parametric estimator \(F(^{m})\), and (2) the parametric estimator \(F^{}(^{m}):=_{}_{BA}(^{m},)\), where \(\) is a family of probability measures on \(\). Harrison and Kontoyiannis (2008) showed that under rather broad conditions, both kinds of estimators are strongly consistent, i.e., \(F(^{m})\) converges to \(F()\) (and respectively, \(F^{}(^{m})\) to \(F^{}()\)) with probability one as \(m\). Our algorithm will implement the parametric estimator \(F^{}(^{m})\) with \(\) chosen to be the set of probability measures with finite support, and we will develop finite-sample convergence results for both kinds of estimators in the continuous setting (Proposition. 4.3).

### Connection to entropic optimal transport

The R-D problem turns out to have a close connection to entropic optimal transport (EOT) (Peyre and Cuturi, 2019), which we will exploit in Sec. 4.3 to obtain sample complexity results under our approach. For \(>0\), the entropy-regularized optimal transport problem is given by

\[_{EOT}(,):=_{(,)} d+ H( |). \]

We now consider the problem of projecting \(\) onto \(()\) under the cost \(_{EOT}\):

\[_{()}_{EOT}(,). \]

In the OT literature this is known as the (regularized) Kantorovich estimator (Bassetti et al., 2006) for \(\), and can also be viewed as a Wasserstein barycenter problem (Agueh and Carlier, 2011).

With the identification \(=^{-1}\), problem (8) is in fact equivalent to the R-D problem (4): compared to \(_{BA}\) (5), the extra constraint on the second marginal of \(\) in \(_{EOT}\) (7) is redundant at the optimal \(\). More precisely, Lemma 7.1 shows that (we omit the notational dependence on \(\) when it is fixed):

\[_{()}_{EOT}()=_{ ()}^{-1}_{BA}() *{arg\,min}_{()}_{ EOT}()=*{arg\,min}_{()}_{BA}( ). \]

Existence of a minimizer holds under mild conditions, for instance if \(==^{d}\) and \((x,y)\) is a coercive lower semicontinuous function of \(y-x\)(Csiszar, 1974a, p. 66).

### Connection to maximum-likelihood deconvolution

The connection between R-D and maximum-likelihood estimation has been observed in the information theory, machine learning and compression literature (Harrison and Kontoyiannis, 2008; Alemi et al., 2018; Balle et al., 2017; Theis et al., 2017; Yang et al., 2020; Yang and Mandt, 2022). Here, we bring attention to a basic equivalence between the R-D problem and maximum-likelihood deconvolution, where the connection is particularly natural under a quadratic distortion function. Also see (Rigollet and Weed, 2018) for a related discussion that inspired ours and extension to a non-quadratic distortion. We provide further insight from the view of variational learning and inference in Section 10.

Maximum-likelihood deconvolution is a classical problem of non-parametric statistics and mixture models (Carroll and Hall, 1988; Lindsay and Roeder, 1993). The deconvolution problem is concerned with estimating an unknown distribution \(\) from noise-corrupted observations \(X_{1},X_{2},...\), where for each \(i\), we have \(X_{i}=Y_{i}+N_{i}\), \(Y_{i}\), and \(N_{i}\) are i.i.d. independent noise variables with a known distribution. For concreteness, suppose all variables are \(^{d}\) valued and the noise distribution is \((0,^{2}_{d})\) with Lebesgue density \(_{^{2}}\). Denote the distribution of the observations \(X_{i}\) by \(\). Then \(\) has a Lebesgue density given by the convolution \(*_{^{2}}(x):=_{^{2}}(x-y)(dy)\). Here, we consider the population-level (instead of the usual sample-based) maximum-likelihood estimator (MLE) for \(\):

\[^{*}=*{arg\,max}_{(^{d})} (*_{^{2}}(x))(dx), \]

and observe that \(^{*}=\). Plugging in the density \(_{^{2}}(x) e^{-}\|x\|^{2}}\), we see that the MLE problem (10) is equivalent to the R-D problem (4) with \((x,y)=\|x-y\|^{2}\), \(=}\), and \(_{BA}\) given by (6) in the form of a marginal log-likelihood. Thus the R-D problem has the interpretation of estimating a distribution from its noisy observations given through \(\), assuming a Gaussian noise with variance \(\).

This connection suggests analytical solutions to the R-D problem for a variety of sources that arise from convolving an underlying distribution with Gaussian noise. Consider an R-D problem (4) with \(==^{d},(x,y)=\|x-y\|^{2}\), and let the source \(\) be the convolution between an arbitrary measure \(()\) and Gaussian noise with known variance \(^{2}\). E.g., using a discrete measure for \(\) results in a Gaussian mixture source with equal covariance among its components. When \(=}\), we recover exactly the population-MLE problem (10) discussed earlier, which has the solution \(^{*}=\). While this allows us to obtain one point of \(R(D)\), we can in fact extend this idea to any \(}\) and obtain the analytical form for the corresponding _segment_ of the R-D curve. Specifically, for any \(}\), applying the summation rule for independent Gaussians reveals the source distribution \(\) as

\[=*(0,^{2})=*(0,^{2}- {})*(0,)=_{}*(0, ),_{}:=*(0,^{2}- {1}{}),\]

i.e., as the convolution between another underlying distribution \(_{}\) and independent noise with variance \(\). A solution to the R-D problem (3) is then analogously given by \(^{*}=_{}\), with the corresponding optimal coupling given by \(^{*},(y,dx)=(y,)\). 1 Evaluating the distortion and mutual information of the coupling then yields the \(R(D)\) point associated with \(\). Fig. 1 illustrates the \(R(D)\) of a toy Gaussian mixture source, along with the \(^{*}\) estimated by our proposed WGD algorithm (Sec. 4); note that \(^{*}\) transitions from continuous (a Gaussian mixture with smaller component variances) to singular (a mixture of two Diracs) at \(=^{-2}\). See caption for more details.

## 3 Related Work

### Blahut-Arimoto

The Blahut-Arimoto (BA) algorithm (Blahut, 1972; Arimoto, 1972) is the default method for computing \(R(D)\) for a known and discrete case. For a fixed \(\), BA carries out the optimization problem (3) via coordinate ascent. Starting from an initial measure \(^{(0)}()\), the BA algorithm at step \(t\) computes an updated pair \((^{(t+1)},K^{(t+1)})\) as follows

\[(x,)}{d^{(t)}}(y) =}{ e^{-(x,y^{} )}^{(t)}(dy^{})}, x, \] \[^{(t+1)} =( K^{(t+1)})_{2}. \]

Figure 1: The \(R(D)\) of a Gaussian mixture source, and the estimated optimal reproduction distributions \(^{*}\) (in bar plots) at varying R-D trade-offs. For any \([^{-2},)\), the corresponding \(R(D)\) (yellow segment) is known analytically as is the optimal reproduction distribution \(^{*}\) (whose density is plotted in gray). For \((0,^{-2}]\), \(^{*}\) becomes singular and concentrated on two points, collapsing to the source mean as \( 0\).

When the alphabets are finite, the above computation can be carried out in matrix and vector operations, and the resulting sequence \(\{(^{(t)},K^{(t)})\}_{t=1}^{}\) can be shown to converge to an optimum of (3); cf. (Csiszar, 1974b, Csiszar, 1984). When the alphabets are not finite, e.g., \(==^{d}\), the BA algorithm no longer applies, as it is unclear how to digitally represent the measure \(\) and kernel \(K\) and to tractably perform the integrals required by the algorithm. The common workaround is to perform a discretization step and then apply BA on the resulting discrete problem.

One standard discretization method is to tile up the alphabets with small bins (Gray and Neuhoff, 1998). This quickly becomes infeasible as the number of dimensions increases. We therefore consider discretizing the data space \(\) to be the support of training data distribution \(^{m}\), i.e., the discretized alphabet is the set of training samples; this can be justified by the consistency of the parametric R-D estimator \(F^{}(^{m})\)(Harrison and Kontoyiannis, 2008). It is less clear how to discretize the reproduction space \(\), especially in high dimensions. Since we work with \(=\), we will discretize \(\) similarly and use an \(n\)-element random subset of the training samples, as also considered by Lei et al. (2023a). As we will show, this rather arbitrary placement of the support of \(\) results in poor performance, and can be significantly improved from our perspective of evolving particles.

### Neural network-based methods for estimating \(R(d)\)

**RD-VAE (Yang and Mandt, 2022):** To overcome the limitations of the BA algorithm, Yang and Mandt (2022) proposed to parameterize the transition kernel \(K\) and reproduction distribution \(\) of the BA algorithm by neural density estimators (Papamakarios et al., 2021), and optimize the same objective (3) by (stochastic) gradient descent. They estimate (3) by Monte Carlo using joint samples \((X_{i},Y_{i}) K\); in particular, the relative entropy can be written as \(H( K|)=((y) )K(x,dy)(dx)\), where the integrand is computed exactly via a density ratio. In practice, an alternative parameterization is often used where the neural density estimators are defined on a lower dimensional latent space than the reproduction alphabet, and the resulting approach is closely related to VAEs (Kingma and Welling, 2013). Yang and Mandt (2022) additionally propose a neural estimator for a lower bound on \(R(D)\), based on a dual representation due to Csiszar (1974a). **NERD (Lei et al., 2023a):** Instead of working with the transition kernel \(K\) as in the RD-VAE, Lei et al. (2023a) considered optimizing the form of the rate functional in (6), via gradient descent on the parameters of \(\) parameterized by a neural network. Let \(^{}\) be a base distribution over \(=^{K}\), such as the standard Gaussian, and \(:\) be a decoder network. The variational measure \(\) is then modeled as the image measure of \(^{}\) under \(\). To evaluate and optimize the objective (6), the intractable inner integral w.r.t. \(\) is replaced with a plug-in estimator, so that for a given \(x\),

\[-(_{}e^{-(x,y)}(dy))- (_{j=1}^{n}e^{-(x,Y_{j})}), Y_{j} ,j=1,2,...,n. \]

After training, we estimate an R-D upper bound using \(n\) samples from \(\) (to be discussed in Sec. 4.4).

### Other related work

**Within information theory:** Recent work by Wu et al. (2022) and Lei et al. (2023b) also note the connection between the R-D function and entropic optimal transport. Wu et al. (2022) compute the R-D function in the finite and known alphabet setting by solving a version of the EOT problem (8), whereas Lei et al. (2023b) numerically verify the equivalence (9) on a discrete problem and discuss the connection to scalar quantization. We also experimented with estimating \(R(D)\) by solving the EOT problem (8), but found it computationally much more efficient to work with the rate functional (6), and we see the primary benefit of the EOT connection as bringing in tools from statistical OT (Genevay et al., 2019; Mena and Niles-Weed, 2019; Rigollet and Stromme, 2022) for R-D estimation. **Outside of information theory:**Rigollet and Weed (2018) note a connection between the EOT projection problem (8) and maximum-likelihood deconvolution (10); our work complements their perspective by re-interpreting both problems through the equivalent R-D problem. Unbeknownst to us at the time, Yan et al. (2023) proposed similar algorithms to ours in the context of Gaussian mixture estimation, which we recognize as R-D estimation under quadratic distortion (see Sec. 2.3). Their work is based on gradient flow in the Fisher-Rao-Wasserstein (FRW) geometry (Chizat et al., 2018), which our hybrid algorithm can be seen as implementing. Yan et al. (2023) prove that, in an idealized setting with infinite particles, FRW gradient descent does not get stuck at local minima; by contrast, our convergence and sample-complexity results (Prop. 4.2, 4.3) hold for any finite number of particles. We additionally consider larger-scale problems and the stochastic optimization setting.

## 4 Proposed method

For our algorithm, we require \(==^{d}\) and \(\) be continuously differentiable. We now introduce the gradient descent algorithm in Wasserstein space to solve the problems (4) and (8). We defer all proofs to the Supplementary Material. To minimize a functional \(:()\) over the space of probability measures, our algorithm essentially simulates the gradient flow (Ambroso et al., 2008) of \(\) and follows the trajectory of steepest descent in the Wasserstein geometry. In practice, we represent a measure \(^{(t)}()\) by a collection of particles and at each time step update \(^{(t)}\) in a direction of steepest descent of \(\) as given by its (negative) _Wasserstein gradient_. Denote by \(_{n}(^{d})\) the set of probability measures on \(^{d}\) that are supported on at most \(n\) points. Our algorithm implements the parametric R-D estimator with the choice \(=_{n}(^{d})\) (see discussions at the end of Sec. 2.1).

### Wasserstein gradient descent (WGD)

Abstractly, Wasserstein gradient descent updates the variational measure \(\) to its pushforward \(\) under the map \((-)\), for a function \(:^{d}^{d}\) called the Wasserstein gradient of \(\) at \(\) (see below) and a step size \(\). To implement this scheme, we represent \(\) as a convex combination of Dirac measures, \(=_{i=1}^{n}w_{i}_{x_{i}}\) with locations \(\{x_{i}\}_{i=1}^{n}^{d}\) and weights \(\{w_{i}\}_{i=1}^{n}\). The algorithm moves each particle \(x_{i}\) in the direction of \(-(x_{i})\), more precisely, \(=_{i=1}^{n}w_{i}_{x_{i}-(x_{i})}\).

```
Inputs: Loss function \(\{_{BA},_{EOT}\}\); data distribution \((^{d})\); the number of particles \(n\); total number of iterations \(N\); step sizes \(_{1},,_{N}\); batch size \(m\). for\(t=1,,N\)do  Pick an initial measure \(^{(0)}_{n}(^{d})\), e.g., setting the particles to \(n\) random samples from \(\). if support of \(\) contains more than \(m\) points then \(^{m}_{i=1}^{m}_{x_{i}}\) for \(x_{1},,x_{m}\) independent samples from \(\) \(^{(t)}\) Wasserstein gradient of \((^{m},)\) at \(^{(t-1)}\) {see Definition 4.1} else \(^{(t)}\) Wasserstein gradient of \((,)\) at \(^{(t-1)}\) {see Definition 4.1} endif \(^{(t)}-_{t}^{(t)}_{\#}\,^{ (t-1)}\) {*\(\#^{}\) denotes pushforward} endfor Return:\(^{(N)}\)
```

**Algorithm 1** Wasserstein gradient descent

Since the optimization objectives (4) and (8) appear as integrals w.r.t. the data distribution \(\), we can also apply stochastic optimization and perform stochastic gradient descent on mini-batches with size \(m\). This allows us to handle a very large or infinite amount of data samples, or when the source is continuous. We formalize the procedure in Algorithm 1.

The following gives a constructive definition of a Wasserstein gradient which forms the computational basis of our algorithm. In the literature, the Wasserstein gradient is instead usually defined as a Frechet differential (cf. (Ambroso et al., 2008, Definition 10.1.1)), but we emphasize that in smooth settings, the given definition recovers the one from the literature (cf. (Chizat, 2022, Lemma A.2)).

**Definition 4.1**.: _For a functional \(:()\) and \(()\), we say that \(V_{}():^{d}\) is a first variation of \(\) at \(\) if_

\[_{ 0}((1-)+ )-()}{}= V_{}()\,d( -)\ \ ().\]

_We call its (Euclidean) gradient \( V_{}():^{d}^{d}\), if it exists, the Wasserstein gradient of \(\) at \(\)._

For \(=_{EOT}\), the first variation is given by the Kantorovich potential, which is the solution of the convex dual of \(_{EOT}\) and commonly computed by Sinkhorn's algorithm (Peyre and Cuturi, 2019,Nutz, 2021). Specifically, let \((^{},^{})\) be potentials for \(_{EOT}(,)\). Then \(V_{}()=^{}\) is the first variation w.r.t. \(\) (cf. (Carlier et al., 2022, equation (20))), and hence \(^{}\) is the Wasserstein gradient. This gradient exists whenever \(\) is differentiable and the marginals are sufficiently light-tailed; we give details in Sec. 9.1 of the Supplementary Material. For \(=_{BA}\), the first variation can be computed explicitly. As derived in Sec. 9.1 of the Supplementary Material, the first variation at \(\) is

\[^{}(y)=-))(d)}(dx)\]

and then the Wasserstein gradient is \(_{BA}()=^{}\). We observe that \(^{}(y)\) is computationally cheap; it corresponds to running a single iteration of Sinkhorn's algorithm. By contract, finding the potential for \(_{EOT}\) requires running Sinkhorn's algorithm to convergence.

Like the usual Euclidean gradient, the Wasserstein gradient can be shown to possess a linearization property, whereby the loss functional is reduced by taking a small enough step along its Wasserstein gradient. Following (Carlier et al., 2022), we state it as follows: for any \(()\) and \((,)\),

\[()-()=(y-x)^ {}& V_{}()(x)\,(dx,dy)+o(\|y- x\|^{2}\,(dx,dy)),\\ &|\| V_{}()\|^{2}\,d-\| V _{}()\|^{2}\,d| CW_{2}(,). \]

The first line of (14) is proved for \(_{EOT}\) in (Carlier et al., 2022, Proposition 4.2) in the case that the marginals are compactly supported and \(\) is twice continuously differentiable. In this setting, the second line of (14) follows using \(a^{2}-b^{2}=(a+b)(a-b)\) and a combination of boundedness and Lipschitz continuity of \( V_{}\), see (Carlier et al., 2022, Proposition 2.2 and Corollary 2.4).

The linearization property given by (14) enables us to show that Wasserstein gradient descent for \(_{EOT}\) and \(_{BA}\) converges to a stationary point under mild conditions:

**Proposition 4.2** (Convergence of Wasserstein gradient descent).: _Let \(_{1}_{2} 0\) satisfy \(_{k=1}^{}_{k}=\) and \(_{k=1}^{}_{k}^{2}<\). Let \(:(^{d})\) be Wasserstein differentiable in the sense that (14) holds. Denoting by \(^{(t)}\) the steps in Algorithm 1, and suppose that \((^{(0)})\) is finite and \(\| V_{}(^{(t)})\|^{2}\,d^{(t)}\) is bounded. Then_

\[_{t}\| V_{}(^{(t)})\|^{2}\,d^{(t)}=0.\]

### Hybrid algorithm

A main limitation of the BA algorithm is that the support of \(^{(t)}\) is restricted to that of the (possibly bad) initialization \(^{(0)}\). On the other hand, Wasserstein gradient descent (Algorithm 1) only evolves the particle locations of \(^{(t)}\), but not the weights, which are fixed to be uniform by default. We therefore consider a hybrid algorithm where we alternate between WGD and the BA update steps, allowing us to optimize the particle weights as well. Experimentally, this translates to faster convergence than the base WGD algorithm (Sec. 5.1). Note however, unlike WGD, the hybrid algorithm does not directly lend itself to the stochastic optimization setting, as BA updates on mini-batches no longer guarantee monotonic improvement in the objective and can lead to divergence. We treat the convergence of the hybrid algorithm in the Supplementary Material Sec. 9.4.

### Sample complexity

Let \(==^{d}\) and \((x,y)=\|x-y\|^{2}\). Leveraging work on the statistical complexity of EOT (Mena and Niles-Weed, 2019), we obtain finite-sample bounds for the theoretical estimators implemented by WGD in terms of the number of particles and source samples. The bounds hold for both the R-D problem (4) and EOT projection problem (8) as they share the same optimizers (see Sec. 2.2), and strengthen existing asymptotic results for empirical R-D estimators (Harrison and Kontoyiannis, 2008). We note that a recent result by Rigollet and Stromme (2022) might be useful for deriving alternative bounds under distortion functions other than the quadratic.

**Proposition 4.3**.: _Let \(\) be \(^{2}\)-subgaussian. Then every optimizer \(^{*}\) of (4) and (8) is also \(^{2}\)-subgaussian. Consider \(:=_{EOT}\). For a constant \(C_{d}\) only depending on \(d\), we have_

\[|_{(^{d})}(,) -_{_{n}_{n}(^{d})}(,_{n})|  C_{d}\,\,(1+}{ ^{ 5d/4+3}})\,},\] \[[|_{(^{d})} (,)-_{(^{d})}(^{ m},)|]  C_{d}\,\,(1+}{ ^{ 5d/4+3}})\,},\] \[[|_{(^{d})} (,)-_{_{n}_{n}(^{d})}(^{m},_{n})|]  C_{d}\,\,(1+}{ ^{ 5d/4+3}})\,(}+} ),\]

_for all \(n,m\), where \(_{n}(^{d})\) is the set of probability measures over \(^{d}\) supported on at most \(n\) points, \(^{m}\) is the empirical measure of \(\) with \(m\) independent samples and the expectation \([]\) is over these samples. The same inequalities hold for \(:=^{-1}_{BA}\), with the identification \(=^{-1}\)._

### Estimation of rate and distortion

Here, we describe our estimator for an upper bound \((,)\) of \(R(D)\) after solving the unconstrained problem (3). We provide more details in Sec. 8 of the Supplementary Material.

For any given pair of \(\) and \(K\), we always have that \(:= d( K)\) and \(:=H( K|)\) lie on an upper bound of \(R(D)\)[Berger, 1971]. The two quantities can be estimated by standard Monte Carlo provided we can sample from \( K\) and evaluate the density \((x,y)=(y)\).

When only \(\) is given, e.g., obtained from optimizing (6) with WGD or NERD, we estimate an R-D upper bound as follows. As in the BA algorithm, we construct a kernel \(K_{}\) similarly to (11), i.e., \((x,)}{d}(y)=}{ e^{- (x,y)}()}\); then we estimate \((,)\) using the pair \((,K_{})\) as described earlier.

As NERD uses a continuous \(\), we follow [Lei et al., 2023a] and approximate it with its \(n\)-sample empirical measure to estimate \((,)\). A limitation of NERD, BA, and our method is that they tend to converge to a rate estimate of at most \((n)\), where \(n\) is the support size of \(\). This is because as the algorithms approach an \(n\)-point minimizer \(_{n}^{*}\) of the R-D problem, the rate estimate \(\) approaches the mutual information of \( K_{_{n}^{*}}\), which is upper-bounded by \((n)\)[Eckstein and Nutz, 2022]. In practice, this means if a target point of \(R(D)\) has rate \(r\), then we need \(n e^{r}\) to estimate it accurately.

### Computational considerations

Common to all the aforementioned methods is the evaluation of a pairwise distortion matrix between \(m\) points in \(\) and \(n\) points in \(\), which usually has a cost of \((mnd)\) for a \(d\)-dimensional source. While RD-VAE uses \(n=1\) (in the reparameterization trick), the other methods (BA, WGD, NERD) typically use a much larger \(n\) and thus has the distortion computation as their main computation bottleneck. Compared to BA and WGD, the neural methods (RD-VAE, NERD) incur additional computation from neural network operations, which can be significant for large networks.

For NERD and WGD (and BA), the rate estimate upper bound of \((n)\) nats/sample (see Sec. 4.4) can present computational challenges. To target a high-rate setting, a large number of \(\) particles (high \(n\)) is required, and care needs to be taken to avoid running out of memory during the distortion matrix computation (one possibility is to use a small batch size \(m\) with stochastic optimization).

## 5 Experiments

We compare the empirical performance of our proposed method (WGD) and its hybrid variant with Blahut-Arimoto (BA) [Blahut, 1972, Arimoto, 1972], RD-VAE [Yang and Mandt, 2022], and NERD [Lei et al., 2022] on the tasks of maximum-likelihood deconvolution and estimation of R-D upper bounds. While we experimented with WGD for both \(_{BA}\) and \(_{EOT}\), we found the former to be 10 to 100 times faster computationally while giving similar or better results; we therefore focus on WGD for \(_{BA}\) in discussions below. For the neural-network baselines, we use the same (or as similar as possible) network architectures as in the original work [Yang and Mandt, 2022, Lei et al., 2023a]. We use the Adam optimizer for all gradient-based methods, except we use simple gradient descentwith a decaying step size in Sec. 5.1 to better compare the convergence speed of WGD and its hybrid variant. Further experiment details and results are given in the Supplementary Material Sec. 11.

### Deconvolution

To better understand the behavior of the various algorithms, we apply them to a deconvolution problem with known ground truth (see Sec. 2.3). We adopt the Gaussian noise as before, letting \(\) be the uniform measure on the unit circle in \(^{2}\) and the source \(=*(0,^{2})\) with \(^{2}=0.1\).

We use \(n=20\) particles for BA, NERD, WGD and its hybrid variant. We use a two-layer network for NERD and RD-VAE with some hand-tuning (we replace the softplus activation in the original RD-VAE network by ReLU as it led to difficulty in optimization). Fig. 2 plots the resulting loss curves and shows that the proposed algorithms converge the fastest to the ground truth value \(OPT:=()\). In Fig. 3, we visualize the final \(^{(t)}\) at the end of training, compared to the ground truth \(^{*}=\) supported on the circle (colored in cyan). Note that we initialize \(^{(0)}\) for BA, WGD, and its hybrid variant to the same \(n\) random data samples. While BA is stuck with the randomly initialized particles and assigns large weights to those closer to the circle, WGD learns to move the particles to uniformly cover the circle. The hybrid algorithm, being able to reweight particles to reduce their transportation cost, learns a different solution where a cluster of particles covers the top-left portion of the circle with small weights while the remaining particles evenly covers the rest. Unlike our particle-based methods, the neural methods generally struggle to place the support of their \(\) exactly on the circle.

We additionally compare how the performance of BA, NERD, and the proposed algorithms scale to higher dimensions and a higher \(\) (corresponding to lower entropic regularization in \(_{EOT}\)). Fig. 4 plots the gap between the converged and the optimal losses for the algorithms, and demonstrates the proposed algorithms to be more particle-efficient and scale more favorably than the alternatives which also use \(n\) particles in the reproduction space. We additionally visualize how the converged particles for our methods vary across the R-D trade-off in Fig. 6 of the Supplementary Material.

Figure 4: Optimality gap v.s. the number of particles \(n\) used, on deconvolution problems with different dimension \(d\) and distortion multiplier \(\). The first three panels fix \(=^{-2}\) and increase \(d\), and the right-most panel corresponds to the 2-D problem with a higher \(\) (denoising with a narrower Gaussian kernel). Overall the WGD methods attain higher accuracy for a given budget of \(n\).

Figure 3: Visualizing \(\) samples (top left), as well as the \(\) returned by various algorithms compared to the ground truth \(^{*}\) (cyan).

Figure 2: Losses over iterations. Shading corresponds to one standard deviation over random initializations.

### Higher-dimensional data

We perform \(R(D)\) estimation on higher-dimensional data, including the _physics_ and _speech_ datasets from (Yang and Mandt, 2022) and MNIST (LeCun et al., 1998). As the memory cost to operating on the full datasets becomes prohibitive, we focus on NERD, RD-VAE, and WGD using mini-batch stochastic gradient descent. BA and hybrid WGD do not directly apply in the stochastic setting, as BA updates on random mini-batches can lead to divergence (as discussed in Sec. 4.2).

Fig. 5 plots the estimated R-D bounds on the datasets, and compares the convergence speed of WGD and neural methods in both iteration count and compute time. Overall, we find WGD to require minimal tuning and obtains the tightest R-D upper bounds within the \((n)\) rate limit (see Sec. 4.4), and consistently obtains tighter bounds than NERD given the same computation budget.

## 6 Discussions

In this work, we leverage tools from optimal transport to develop a new approach for estimating the rate-distortion function in the continuous setting. Compared to state-of-the-art neural approaches (Yang and Mandt, 2022; Lei et al., 2022), our Wasserstein gradient descent algorithm offers complementary strengths: 1) It requires a single main hyperparameter \(n\) (the number of particles) and no network architecture tuning; and 2) empirically we found it to converge significantly faster and rarely end up in bad local optima; increasing \(n\) almost always yielded an improvement (unless the bound is already close to being tight). From a modeling perspective, a particle representation may be inherently more efficient when the optimal reproduction distribution is singular or has many disconnected modes (shown, e.g., in Figs. 1 and 3). However, like NERD (Lei et al., 2022), our method has a fundamental limitation - it requires an \(n\) that is exponential in the rate of the targeted \(R(D)\) point to estimate it accurately (see Sec. 4.4). Thus, a neural method like RD-VAE (Yang and Mandt, 2022) may still be preferable on high-rate sources, while our particle-based method stands as a state-of-the-art solution on lower-rate sources with substantially less tuning or computation requirements.

Besides R-D estimation, our algorithm also applies to the mathematically equivalent problems of maximum likelihood deconvolution and projection under an entropic optimal transport (EOT) cost, and may find other connections and applications. Indeed, the EOT projection view of our algorithm is further related to optimization-based approaches to sampling (Wibisono, 2018), variational inference (Liu and Wang, 2016), and distribution compression (Shetty et al., 2021). Our particle-based algorithm also generalizes optimal quantization (corresponding to \(=0\) in \(_{EOT}\) and projection of the source under the Wasserstein distance (Graf and Luschgy, 2007; Gray, 2013)) to incorporate a rate constraint (\(>0\)), and it would be interesting to explore the use of the resulting rate-distortion optimal quantizer for practical data compression and communication.

Figure 5: **Left, Middle:** R-D bound estimates on the physics, speech datasets (Yang and Mandt, 2022) and MNIST training set. **Right:** Example speed comparisons of WGD and neural upper bound methods on MNIST, with WGD converging at least an order of magnitude faster. On each of the dataset we also include an R-D _lower_ bound estimated using the method of (Yang and Mandt, 2022).