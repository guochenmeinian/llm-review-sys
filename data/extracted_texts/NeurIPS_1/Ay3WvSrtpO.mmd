# Model-Free Reinforcement Learning with

the Decision-Estimation Coefficient

 Dylan J. Foster

dylanfoster@microsoft.com

Noah Golowich

nzg@mit.edu

Jian Qian

jianqian@mit.edu

Alexander Rakhlin

rakhlin@mit.edu

Ayush Sekhari

sekhari@mit.edu

###### Abstract

We consider the problem of interactive decision making, encompassing structured bandits and reinforcement learning with general function approximation. Recently, Foster et al.  introduced the Decision-Estimation Coefficient, a measure of statistical complexity that lower bounds the optimal regret for interactive decision making, as well as a meta-algorithm, Estimation-to-Decisions, which achieves upper bounds in terms of the same quantity. Estimation-to-Decisions is a _reduction_, which lifts algorithms for (supervised) online estimation into algorithms for decision making. In this paper, we show that by combining Estimation-to-Decisions with a specialized form of _optimistic estimation_ introduced by Zhang , it is possible to obtain guarantees that improve upon those of Foster et al.  by accommodating more lenient notions of estimation error. We use this approach to derive regret bounds for model-free reinforcement learning with value function approximation, and give structural results showing when it can and cannot help more generally.

## 1 Introduction

The theory of interactive decision making--ranging from bandits to reinforcement learning with function approximation--contains a variety of sufficient conditions for sample-efficient learning, , but _necessary conditions_ have been comparatively unexplored. Recently, however, Foster et al.  introduced the Decision-Estimation Coefficient (DEC), a measure of statistical complexity which leads to upper _and_ lower bounds on the optimal sample complexity for interactive decision making.

Regret bounds based on the Decision-Estimation Coefficient are achieved by Estimation-to-Decisions (E2D), a meta-algorithm which reduces the problem of interactive decision making to supervised online estimation. While the Decision-Estimation Coefficient leads to tight lower bounds on regret for many problem settings, the upper bounds in Foster et al.  can be suboptimal in certain situations due to the need to perform estimation with respect to _Hellinger distance_, a stringent notion of estimation error. When specialized to reinforcement learning, the guarantees for the E2D meta-algorithm in Foster et al.  are only tight for model-based settings (where function approximation is employed to model and estimate transition probabilities), and do not lead to meaningful guarantees for model-free settings with value function approximation. In this paper, we explore the prospect of developing tighter regret bounds suitable for model-free settings.

Contributions.We show that by combining Estimation-to-Decisions with _optimistic online estimation_, an elegant technique recently introduced by Zhang , it is possible to obtain regret bounds that improve upon Foster et al.  by accommodating weaker notions of estimation error. Our main contributions are:* We introduce a new _optimistic_ variant of the Decision-Estimation Coefficient, and show that a variant of Estimation-to-Decisions that incorporates optimistic estimation achieves regret bounds that scale with this quantity (Section 2). Using this approach, we derive the first regret bounds for Estimation-to-Decisions applied to model-free reinforcement learning with _bilinear classes_ (Section 2.3).
* We show that in general, whether or not optimistic estimation leads to improvement depends on the _divergence_ with respect to which estimation is performed: For _symmetric_ divergences, optimistic estimation offers no improvement, but for _asymmetric_ divergences, including those found in reinforcement learning, the improvement can be drastic (Section 3). In addition, we highlight settings in which combining optimistic estimation with Estimation-to-Decisions offers provable improvement over previous approaches that apply the technique with posterior sampling .

Perhaps the most important aspect of our work is to elucidate the connection between the DEC framework and optimistic estimation, building foundations for further research into these techniques.

In what follows, we review the Decision-Estimation Coefficient and Estimation-to-Decisions meta-algorithm (Section 1.2), highlighting opportunities for improvement. In Section 2, we present our main results, including our application to model-free reinforcement learning. We close with discussion and structural results, highlighting situations in which optimistic estimation can and cannot help (Section 3).

### Problem Setting

We adopt the _Decision Making with Structured Observations_ (DMSO) framework of Foster et al. , which is a general setting for interactive decision making that encompasses bandit problems (structured, contextual, and so forth) and reinforcement learning with function approximation.

The protocol consists of \(T\) rounds. For each round \(t=1,,T\):

1. The learner selects a _decision_\(^{}\), where \(\) is the _decision space_.
2. The learner receives a reward \(r^{}\) and observation \(o^{}\) sampled via \((r^{},o^{}) M^{}(^{})\), where \(M^{}:()\) is the underlying _model_.

Above, \(\) is the _reward space_ and \(\) is the _observation space_. The model (conditional distribution) \(M^{}\) represents the underlying environment, and is unknown to the learner, but the learner is assumed to have access to a _model class_\((())\) that is flexible enough to capture \(M^{}\).

**Assumption 1.1** (Realizability).: _The learner has access to a model class \(\) containing the true model \(M^{}\)._

The model class \(\) represents the learner's prior knowledge about the decision making problem, and allows one to appeal to estimation and function approximation. For structured bandit problems, models correspond to reward distributions, and \(\) encodes structure in the reward landscape. For reinforcement learning problems, models correspond to Markov decision processes (MDPs), and \(\) typically encodes structure in value functions or transition probabilities. We refer to Foster et al.  for further background.

For a model \(M\), \(^{M,}[]\) denotes the expectation under the process \((r,o) M()\), \(f^{M}():=^{M,}[r]\) denotes the mean reward function, and \(_{M}:=*{arg\,max}_{}f^{M}()\) denotes the optimal decision. We measure performance in terms of regret, which is given by \(_{}:=_{t=1}^{T}_{^{} p^{t}}[f^ {M^{}}(_{M^{}})-f^{M^{}}(^{})]\), where \(p^{t}\) is the learner's randomization distribution for round \(t\).

Additional notation.For an integer \(n\), we let \([n]\) denote the set \(\{1,,n\}\). For a set \(\), we let \(()\) denote the set of all probability distributions over \(\). For a model class \(\), \(()\) denotes the convex hull. We write \(f=(g)\) to denote that \(f=O(g\{1,(g)\})\), and use \(\) as shorthand for \(a=O(b)\).

### Background: Estimation-to-Decisions and Decision-Estimation Coefficient

To motivate our results, this section provides a primer on the Estimation-to-Decisions meta-algorithm and the Decision-Estimation Coefficient. We refer to Foster et al.  for further background.

Online estimation.Estimation-to-Decisions (E2D) for General Divergences

```
1:parameters: Estimation oracle \(_{}\), Exp. parameter \(>0\), divergence \(D^{}()\).
2:for\(t=1,2,,T\)do
3: Compute estimate \(^{t}=_{}^{t}\{(^{i},r^{i},o^{i}) \}_{i=1}^{t-1}\).
4:\(p^{i}_{p()}_{M}_{  p}f^{M}(_{ M})-f^{M}()- D^{}^{t}  M\).// Eq. (2).
5: Sample decision \(^{} p^{t}\) and update estimation oracle with \((^{},r^{},o^{})\).
```

**Algorithm 1** Estimation-to-Decisions (E2D) for General Divergences

Online estimation.Estimation-to-Decisions (Algorithm 1) is a reduction that lifts algorithms for online estimation into algorithms for decision making. An online estimation oracle, denoted by \(_{}\), is an algorithm that, using knowledge of the class \(\), estimates the underlying model \(M^{}\) from data in a sequential fashion. At each round \(t\), given the data \(^{-1}=(^{1},r^{1},o^{1}),,(^{t-1},r^{t-1},o^{t-1})\) observed so far, the estimation oracle computes an estimate \(^{t}=_{}^{t}\{(^{i},r^{i},o^{i}) \}_{i=1}^{t-1}\) for the true model \(M^{}\).

To measure the estimation oracle's performance, we make use of a user-specified _divergence-like function_, which quantifies the discrepancy between models. Formally, we define a divergence-like function (henceforth, "divergence") as any function \(D:()() _{+}\), with \(D^{}(M M^{})\) representing the discrepancy between the models \(M\) and \(M^{}\) at the decision \(\). Standard choices used in past work  include the squared error \(D^{}_{}(M,M^{}):=(f^{M}()-f^{M^{}}())^{2}\) for bandit problems, and squared Hellinger distance1\(D^{}_{}(M,M^{}):=D^{2}_{}(M(),M^{}())\) for RL, where for distributions \(\) and \(\), \(D^{2}_{}(,):=(}-})^{2}\). We then measure the estimation oracle's performance in terms of _cumulative estimation error_ with respect to \(D\), defined as

\[^{D}:=_{t=1}^{T}_{^{t} p^{t}}D^{^{ t}}^{t} M^{}, \]

where \(p^{t}\) is the conditional distribution over \(^{t}\) given \(^{t-1}\). We make the following assumption on the algorithm's performance.

**Assumption 1.2**.: _At each time \(t[T]\), the online estimation oracle \(_{}\) returns, given \((^{1},r^{1},o^{1}),,(^{t-1},r^{t-1},o^{t-1})\) with \((r^{i},o^{i}) M^{}(^{i})\) and \(^{i} p^{i}\), an estimator \(^{t}:()\) such that \(^{D}^{D}(T,)\), with probability at least \(1-\), where \(^{D}(T,)\) is a known upper bound._

For the squared error, one can obtain \(^{}(T,):=^{D_{}}(T,) (|_{}|/)\), where \(_{}:=\{f^{M} M\}\), and for Hellinger distance, it is possible to obtain \(^{}(T,):=^{D_{}}(T,) (||/)\).

Estimation-to-Decisions.A general version of the E2D meta-algorithm is displayed in Algorithm 1. At each timestep \(t\), the algorithm queries estimation oracle to obtain an estimator \(^{}\) using the data \((^{1},r^{1},o^{1}),,(^{t-1},r^{t-1},o^{t-1})\) observed so far. The algorithm then computes the decision distribution \(p^{t}\) by solving a min-max optimization problem involving \(^{}\) and \(\) (as well as the divergence \(D\)), and then samples the decision \(^{}\) from this distribution.

The Decision-Estimation Coefficient.The min-max optimization problem in Algorithm 1 is derived from the _Decision-Estimation Coefficient_ (DEC), a complexity measure whose value, for a given scale parameter \(>0\) and reference model \(:()\), is given by

\[^{D}_{}(,)=_{p()} _{M}_{ p}f^{M}(_{ M})-f^{M}()- D^{} M , \]

with \(^{D}_{}():=_{( )}^{D}_{}(,)\). Informally, the DEC measures the best tradeoff between suboptimality \(f^{M}(_{ M})-f^{M}()\) and information gain (measured by \(D^{} M\)) that can be achieved by a decision distribution \(p\) in the face of a worst-case model \(M\).

[MISSING_PAGE_FAIL:4]

**Assumption 2.1**.: _At each time \(t[T]\), the optimistic estimation oracle \(_{}\) returns, given \((^{1},r^{1},o^{1}),,(^{t-1},r^{t-1},o^{t-1})\) with \((r^{i},o^{i}) M^{}(^{i})\) and \(^{i} p^{i}\), a randomized estimator \(^{}()\) such that \(_{}^{D}_{}^{D}(T,)\), with w.p. \(1-\), where \(_{}^{D}(T,)\) is a known upper bound._

For the case of contextual bandits, Zhang  proposes an augmented version of the exponential weights algorithm which, for a learning rate parameter \(>0\), sets \((M)-L^{}(f^{M})-^{-1}f^{M}(_{ M})\), where \(L^{}(f^{M})\) is the squared prediction error for the rewards observed so far. This method achieves \(_{}^{}( |_{}|)+_{}|}/\), and Zhang  combines this estimator with posterior sampling to achieve optimal contextual bandit regret. Agarwal and Zhang , Zhong et al. , Agarwal and Zhang  extend this development to reinforcement learning, also using posterior sampling as the exploration mechanism.3 In what follows, we combine optimistic estimation with Estimation-to-Decisions, which provides a universal mechanism for exploration. Beyond giving guarantees which were previously out of reach for E2D (Section 2.3), this approach generalizes and subsumes posterior sampling, and can succeed in situations where posterior sampling fails (Section 3).

**Remark 2.1**.: For the non-optimistic estimation error \(^{D}\), it is possible to obtain low error for well-behaved losses such as the square loss and Hellinger distance without the use of randomization by appealing to improper mixture estimators (e.g., Foster et al. ). We show in Section 3 that for such divergences, randomization does not lead to statistical improvements. For the optimistic estimation error (4), randomization is essential due to the presence of the term \(^{-1}(f^{M^{}}(_{M^{}})-f^{^{t}}(_{ ^{t}})\). \(\)

**Sufficient statistics.** Before proceeding, we note that many divergences of interest have the useful property that they depend on the estimated model \(\) only through a "sufficient statistic" for the model class under consideration. Formally, there exists a _sufficient statistic space_\(\) and _sufficient statistic_\(:\) with the property that we can write (overloading notation)

\[D^{}(M M^{})=D^{}((M) M^{}), f ^{M}()=f^{(M)}(),_{M}=_{(M)}\]

for all models \(M,M^{}\). In this case, it suffices for the online estimation oracle to directly estimate the sufficient statistic by producing a randomized estimator \(^{}()\). We measure performance via

\[_{}^{D}:=\,_{t=1}^{T}_{^{t} p^{t}} \,_{^{t}^{t}}D^{^{t}} ^{t} M^{}+^{-1}(f^{M^{}}(_{ M^{}})-f^{^{t}}(_{^{t}})) \]

Examples include bandit problems, where one may use squared estimation error \(D^{}_{}(,)\) and take \(()=f^{M}\), and model-free reinforcement learning, where we show that by choosing the divergence \(D\) appropriately, one can use _Q-value functions_ as a sufficient statistic. Note that we only focus on sufficient statistics for the first argument to \(D^{}()\), since this is the quantity we wish to estimate.

### Algorithm and Main Result

We provide an _optimistic_ variant of the E2D meta-algorithm (E2D.Opt) in Algorithm 2. At each timestep \(t\), the algorithm calls the estimation oracle to obtain a randomized estimator \(^{}\) using the data\((^{1},r^{1},o^{1}),,(^{t-1},r^{t-1},o^{t-1})\) collected so far. The algorithm then uses the estimator to compute a distribution \(p^{t}()\) and samples \(^{t}\) from this distribution, with the main change relative to Algorithm 1 being that the minimax problem in Algorithm 2 is derived from an "optimistic" variant of the DEC, which we refer to as the _Optimistic Decision-Estimation Coefficient_. For \(()\), define

\[^{D}_{}(,)=_{p( )}_{M}_{ p}\,_{ }f^{}(_{})-f^{{}^{M}}()-  D^{} M. \]

and \(^{D}_{}()=_{( )}^{D}_{}(,)\). The Optimistic DEC has two difference from the original DEC. First, it is parameterized by a distribution \(()\) rather than a reference model \(:()\), which reflects the use of randomized estimators; the value in (6) takes the expectation over a reference model \(\) drawn from this distribution (this modification also appears in the randomized DEC introduced in Foster et al. ). Second, and more critically, the optimal value \(f^{{}^{M}}(_{{}^{M}})\) in (2) is replaced by the optimal value \(f^{}(_{})\) for the (randomized) reference model. This seemingly small change is the main advantage of incorporating optimistic estimation, and makes it possible to bound the Optimistic DEC for certain divergences \(D\) for which the value of the unmodified DEC would otherwise be unbounded (cf. Section 3).

**Remark 2.2**.: When the divergence \(D\) admits a sufficient statistic \(:\), for any distribution \(()\), if we define \(()\) via \(()=(\{M:(M)=\})\), we have

\[^{D}_{}(,)=_{p( )}_{M}_{ p}\,_{} f^{}(_{})-f^{{}^{M}}()- D^{}( M ).\]

In this case, by overloading notation slightly, we may write \(^{D}_{}()=_{( )}^{D}_{}(,)\). \(\)

**Main result.** Our main result shows that the regret of Optimistic Estimation-to-Decisions is controlled by the Optimistic DEC and the optimistic estimation error for the oracle \(_{}\).

**Theorem 2.1**.: _For any \(>0\), Algorithm 2 ensures that with probability at least \(1-\),_

\[_{}^{D}_{}( ) T+^{D}_{}(T,). \]

This regret bound has the same structure as Theorem 1.1, with the DEC and estimation error replaced by their optimistic counterparts. In the remainder of the paper, we show that 1) by adopting _asymmetric_ divergences specialized to reinforcement learning, this result leads to the first guarantees for model-free RL with E2D, but 2) for symmetric divergences such as Hellinger distance, the result never improves upon Theorem 1.1.

**Estimation with batching.** For our application to reinforcement learning, we generalize the results above to accomodate estimation algorithms that draw _batches_ of multiple samples from each distribution \(p^{t}\). Given a _batch size_\(n\), we break the \(T\) rounds of the decision making protocol into \(K:=T/n\) contiguous epochs (or, "iterations"). Within each epoch, the learner's distribution \(p^{k}\) is unchanged (we index by \(k\) rather than \(t\) to reflect this), and we create a _batch_\(B^{k}=\{(^{k,l},r^{k,l},o^{k,l})\}_{l=1}^{n}\) by sampling \(^{k,l} p^{k}\) independently and observing \((r^{k,l},o^{k,l}) M^{}(^{k,l})\) for each \(l[n]\). We can then appeal to estimation algorithms of the form \(^{k}=^{k}_{}\{B^{k}\}_{i=1}^{k-1}\). Regret bounds for a variant of E2D.Opt with batching are given in Appendix B.1.

### Application to Model-Free Reinforcement Learning

In this section, we use Optimistic Estimation-to-Decisions to provide sample-efficient guarantees for model-free reinforcement learning with _bilinear classes_, a general class of tractable reinforcement learning problems which encompasses many settings [17; 26; 7; 29; 18; 23; 3; 20; 10; 22; 9; 33].

**Reinforcement learning preliminaries.** To state our results, let us recall how reinforcement learning fits into the DMSO framework. We consider an episodic, finite-horizon reinforcement learning setting. With \(H\) denoting the horizon, each model \(M\) specifies a non-stationary Markov decision process \(M=,,\{P^{{}^{M}}_{h}\}_{h=1}^{H},\{R^{{}^{M}}_{h }\}_{h=1}^{H},d_{1}}\), where \(\) is the state space, \(\) is the action space, \(P^{{}^{M}}_{h}:()\) is the probability transition distribution at step \(h\), \(R^{{}^{M}}_{h}:\), each episode proceeds under the following protocol. At the beginning of the episode, the learner selects a randomized, non-stationary _policy_\(=(_{1},,_{H}),\) where \(_{h}:()\); we let \(_{}\) (for "randomized, non-stationary") denote the set of all such policies. The episode includes through the following process, beginning from \(s_{1} d_{1}\): For \(h=1,,H\): \(a_{h}_{h}(s_{h})\), \(r_{h} R_{h}^{ M}(s_{h},a_{h})\), and \(s_{h+1} P_{h}^{ M}( s_{h},a_{h})\). For notational convenience, we take \(s_{H+1}\) to be a deterministic terminal state. We assume for simplicity that \(\) (that is, \(_{h=1}^{H}r_{h}\) almost surely). Within the DMSO framework, at each time \(t\), the learning agent chooses \(^{t}_{}\), then observes the cumulative reward \(r^{t}=_{h=1}^{H}r_{h}^{t}\) and trajectory \(o^{t}:=(s_{1}^{t},a_{1}^{t},r_{1}^{t}),,(s_{H}^{t},a_{H}^{t},r_{H}^{t})\) that results from executing \(^{t}\).

_Value functions._ The value for a policy \(\) under \(M\) is given by \(f^{ M}():=^{ M,}_{h=1}^{H}r_{h}\), where \(^{ M,}[]\) denotes expectation under the process above. For a given model \(M\) and policy \(\), we define the state-action value function and state value functions via \(Q_{h}^{ M,}(s,a)=^{ M,}_{h^{}=h}^{H}r_{h^{}}  s_{h}=s,a_{h}=a\), and \(V_{h}^{ M,}(s)=^{ M,}_{h^{}=h}^{H}r_{h^{}}  s_{h}=s\). We define \(_{ M}\) as the optimal policy, which maximizes \(Q_{h}^{ M,_{M}}(s,a)\) for all states simultaneously. We abbreviate \(Q^{ M,} Q^{ M,_{M}}\).

_Value function approximation._ To apply our results to reinforcement learning, we take a model-free (or, value function approximation) approach, and estimate value functions for the underlying MDP \(M^{}\); this contrasts with model-based methods, such as those considered in Foster et al. , which estimate transition probabilities for \(M^{}\) directly. We assume access to a class \(\) of value functions of the form \(Q=(Q_{1},,Q_{H})\).

**Assumption 2.2**.: _The value function class \(\) has \(Q^{ M^{},}\), where \(M^{}\) is the underlying model._

For \(Q=(Q_{1},,Q_{H})\), we define \(_{Q}=(_{Q,1},,_{Q,H})\) via \(_{Q,h}(s)=_{a}Q_{h}(s,a)\). We define \(_{}=\{_{Q} Q\}\) as the induced policy class. While is not necessary for our results, we mention in passing that the class \(\), under Assumption 2.2, implicitly induces a model class via \(_{}:=\{M Q^{ M,}\}\).

_Bilinear classes._ The bilinear class framework  gives structural conditions for sample-efficient reinforcement learning that capture most known settings where tractable guarantees are possible. The following is an adaptation of the definition from Du et al. .4

**Definition 2.1** (Bilinear class).: _An MDP \(M\) is said to be bilinear with dimension \(d\) relative to a class \(\) if:_

1. _There exist functions_ \(W_{h}(\,;M):^{d}\)_,_ \(X_{h}(\,;M):^{d}\) _such that for all_ \(Q\) _and_ \(h[H]\)_,_ \[^{ M,_{Q}}Q_{h}(s_{h},a_{h})-r_{h}-_{a^{} }Q_{h+1}(s_{h+1},a^{})| X_{h}(Q;M),W_{h}(Q;M)|.\] (8)
2. _Let_ \(z_{h}:=(s_{h},a_{h},r_{h},s_{h+1})\)_. There exists a collection of estimation policies_ \(_{Q}^{}}_{Q}\) _and a discrepancy function_ \(_{h}^{}(;):\) _such that for all_ \(Q,Q^{}\) _and_ \(h[H]\)_,_5__ 
_We let \(d_{}(;M)\) denote the minimal dimension \(d\) for which the bilinear property holds for \(M\). For a model class \(\), we define \(d_{}(;)=_{M}d_{} (\,;M)\). We let \(L_{}(;M) 1\) denote any almost sure upper bound on \(|_{h}^{}(Q;z_{h})|\) under \(M\)._

### Guarantees for Bilinear Classes

We now apply our main results to derive regret bounds for bilinear classes. We first provide optimistic estimation guarantees, then bound the Optimistic DEC, and conclude by applying E2D.Opt.

We take \(=\) as the sufficient statistic space, with \((M):=Q^{{{}}}}}}}}}}}}}}}}}}}} }\), and define \(f^{Q}(_{Q}):=_{s_{1} d_{1}}[Q_{1}(s_{1},_{Q}(s_{1}))]\). For the divergence \(D\), we appeal to squared discrepancy, in the vein of Jiang et al. , Du et al. :

\[D_{}^{}(Q\;\|\;M)=_{h=1}^{H}^{{{}}}}}}}}}}}}}}}}}}_{h}^{}(Q;z_{ h})^{2}. \]

We abbreviate \(_{}^{}=_{}^{D_{}}\) and \(_{}^{}(,)=_{}^{D_{}}(,)\).

**Estimation.** To perform estimation, we approximate the average discrepancy in (14) from samples (drawing a batch of \(n\) samples at each step; cf. Algorithm 3), then appeal to the exponential weights method for online learning, with a bonus to enforce optimism. See Algorithm 6 (deferred to Appendix D for space).

**Proposition 2.1**.: _For any batch size \(n\) (with \(K:=T/n\)) and parameter \( 1\), Algorithm 6, with an appropriate learning rate, ensures that with probability at least \(1-\),_

\[_{}^{}|}}{ }+HL_{}^{2}(;M^{})(||KH/) 1+, \]

_whenever \(M^{}\) is bilinear relative to \(\) and Assumption 2.2 is satisfied._

This result does not actually use the bilinear class structure, and gives a bound on (14) for any choice of \(_{h}^{}\). Similar to the optimistic estimation result given by Zhang  for contextual bandits, this guarantee consists of "slow" term \(|}}{}\) resulting from optimism (which decays as \(\) grows), and a "fast" term. However, compared to the contextual bandit setting, the fast term, \((||KH/)1+\), scales with the ratio \(\), which reflects sampling error in the estimated discrepancy, and necessitates a large batch size. Previous algorithms for bilinear classes  require large batch sizes for similar reasons (specifically, because the expectation in (14) appears inside the square, it is not possible to form an unbiased estimate for \(D_{}\) directly).

**Bounding the DEC.** A bound on the Optimistic DEC follows by adapting arguments in Foster et al. ; our result has slightly improved dependence on \(H\) compared to the bounds in that work.

**Proposition 2.2**.: _Let \(\) be any model class for which the bilinear class property holds relative to \(\). **(1)** In the on-policy case where \(_{Q}^{}=_{Q}\), we have that for all \(>0\), \(_{}^{}() }(;)}{}\). **(2)** In the general case (\(_{Q}^{}_{Q}\)), we have that for all \( H^{2}d_{}(;)\), \(_{}^{}()  d_{}(;)}{}}\)._

Combining Theorem B.1, Proposition 2.1, and Proposition 2.2, we obtain the following result.

**Corollary 2.1** (Regret bound for bilinear classes).: Let \(\) be given. Assume that \(M^{}\), where \(\) is bilinear relative to \(\), and that Assumption 2.2 holds. Abbreviate \(d d_{}(;)\) and \(L L_{}(;):=_{M}L_{ }(;M)\). For an appropriate choice of \(n\) and \(\), Algorithm 3, using the algorithm from Proposition 2.1 as an oracle, enjoys the following guarantees with probability at least \(1-\):

**(1)** In the on-policy case where \(_{Q}^{}=_{Q}\): \(_{}(H^{2}dL^{2}(||TH/))^{1/2 }T^{3/4}\).

**(2)** In the general case where \(_{Q}^{}_{Q}\): \(_{}(H^{6}d^{2}L^{4}^{3}(||TH/ ))^{1/6}T^{5/6}\).

This is the first regret bound for model-free reinforcement learning with the Estimation-to-Decisions meta-algorithm. Importantly, the result scales only with the horizon, the dimension \(d\), and the capacity \(||\). Improving the dependence on \(T\) in Corollary 2.1 is an interesting question: currently, there are no algorithms for general bilinear classes that achieve \(\) regret without additional assumptions. Let us emphasize that regret bounds for bilinear classes can already be achieved by a number of existing methods . The contribution here is to show that such guarantees can be achieved through the general DEC framework, thereby placing this line of research on stronger foundations.

Tighter guarantees under Bellman completeness.In Appendix B.2 (deferred for space), we adapt techniques from Agarwal and Zhang  to derive tighter estimation guarantees when \(\) satisfies a _Bellman completeness_ property (e.g., Zanette et al. , Jin et al. ), by appealing to refined algorithms tailored to squared Bellman error. We use this to derive tighter regret bounds for bilinear classes (\(T^{2/3}\) instead of \(T^{3/4}\)).

## 3 Understanding the Role of Optimistic Estimation

We close with discussion and interpretation of our results.

When does optimistic estimation help?Perhaps the most pressing question at this point is to understand when the regret bound for E2D.Opt (Theorem 2.1) improves upon the corresponding regret bound for vanilla E2D (Theorem 1.1). In what follows, we show that: (1) For any divergence \(D\), the Optimistic DEC is equivalent to a variant of the original DEC which incorporates randomized estimators , but with the arguments to the divergence _flipped_; (2) For divergences \(D\) that satisfy a triangle inequality, this randomized DEC is equivalent to the original DEC itself. Together these results show that the improvement given by the Optimistic DEC is limited to _asymmetric_ divergences such as the bilinear divergence in Section 2.3; for more traditional divergences such as Hellinger distance and squared error, the optimistic approach offers no improvement. Our results use the following regularity assumption, satisfied by all standard divergences.

**Assumption 3.1**.: _For all \(M,()\), \((f^{}()-f^{{}_{M}}())^{2} L^{2}_{} D ^{} M\) for a constant \(L_{}>0\)._

Given a divergence \(D\), we define the _flipped divergence_, which swaps the first and second arguments, by \(^{} M:=D^{}M \). We define the Decision-Estimation Coefficient for randomized estimators  as \(}^{D}_{}(,)=_{p( )}_{M}_{ p}f^{{}_{M}}(_{{}_{M} })-f^{{}_{M}}()-_{}D^{}  M\), with \(}^{D}_{}():=_{( )}}^{D}_{}(,)\). This definition is identical to (2), but allows \(\) to be randomized.

**Proposition 3.1**.: _Whenever Assumption 3.1 holds, we have that for all \(>0\),_

\[}^{}_{/2}()-_ {}}{2}}}}}}^{D}_{}()}^{}_{/2}()+_{}}{2 }. \]

For settings in which there exists an estimation oracle for which the flipped estimation error \(^{}=_{t=1}^{T}_{ p^{t}}\, _{^{t}^{t}}D^{^{t}}M^{} ^{t}\) is controlled, this result shows that to match the guarantee in Theorem 2.1, optimism is not required, and it suffices to run a variant of vanilla E2D that incorporates randomized estimators (cf. Foster et al. , Section 4.3).

We now turn to the role of randomization. When \(D\) is convex in the first argument, we have \(}^{D}_{}()_{ ()}^{D}_{}(, )=^{D}_{}()\), but it is not immediately apparent whether the opposite direction of this inequality holds, and one might hope that working with the randomized DEC in Proposition 3.1 would lead to improvements over the non-randomized counterpart in Theorem 1.1. The next result shows that this is not the case: Under mild assumptions on the divergence \(D\), randomization offers no improvement.

**Proposition 3.2**.: _Let \(D\) be any bounded divergence such that for all \(M,M^{},\) and \(\), \(D^{}(M M^{}) CD^{} M +D^{} M^{}\). Then for all \(>0\), \(_{}^{D}_{}(,) }^{D}_{/(2C)}()\)._

Implication for Hellinger distance.Squared Hellinger distance is symmetric, satisfies Assumption 3.1 with \(L_{}=1\) whenever \(\), and satisfies the condition in Proposition 3.2 with \(C=2\). Hence, by combining Proposition 3.1 with Proposition 3.2, we obtain the following corollary.

**Corollary 3.1**.: If \(\), then \(}}}^{}_{2}()-_{}^{}_{}(,)}}}^{}_{/6}( )+\ \ >0\).

This shows that for Hellinger distance--at least from a statistical perspective--there is no benefit to using the Optimistic DEC or randomized DEC compared to the original version. In particular, this implies that regret bounds based on the randomized DEC with Hellinger distance (such as those found in recent work of Chen et al. ) do not offer improvement over the guarantees for vanilla E2D in Foster et al. . One caveat, though, is that working with the Optimistic DEC, as well as randomized estimators, has potential to give computational improvement, as computing a distribution \(p()\) that minimizes \(_{}^{}(,)\) might be simpler than computing a corresponding distribution for \(_{}^{}(,)\) with \(()\). We are not currently aware of any examples where such an improvement occurs, as even maintaining a distribution \(()\) requires intractably large memory for most classes of interest.

**Implication for model-free RL.** The bilinear divergence \(D_{}^{}(QM)=_{h=1}^{H}^{M,} [_{h}^{}(Q;z_{h})]^{2}\) that we adopt in Section 2.3 is asymmetric, as are closely related divergences such as squared Bellman error. Here, there are two reasons why optimistic estimation offers meaningful advantages.

**(1)** By Proposition 3.1 (\(D_{}\) satisfies Assumption 3.1 with \(L_{}=O(H)\)), a natural alternative to optimistic estimation is to estimate with respect to the flipped divergence \(_{}\), then appeal to Algorithm 3 of Foster et al. . The issue with this approach is that minimizing the flipped estimation error, which takes the form

\[^{_{}}=_{t=1}^{T}_{ p^ {t}}\,_{^{t}^{t}}D_{}^{^{ t}}(Q^{M^{},}^{})=_{t=1}^{T }_{ p^{t}}\,_{^{t}^{t}}[ _{h=1}^{H}^{^{t},^{t}}_{h}^{ }(Q^{M^{},};z_{h})^{2}],\]

is challenging in model-free settings; we are not aware of any algorithms that accomplish this.6

**(2)** Alternatively, a second choice is to perform estimation with respect to the un-flipped divergence \(D_{}\) (which can be accomplished with Proposition 2.1 by taking \(\)), and appeal to vanilla E2D (either Algorithm 1, or Algorithm 3 of Foster et al.  if one wishes to incorporate randomized estimators). However, the following result shows that unlike the Optimistic DEC, the original DEC with the divergence \(D_{}\) does not admit a favorable bound, even for tabular reinforcement learning.

**Proposition 3.3**.: _Let \(\) be the class of all horizon-\(H\) tabular MDPs with \(||=2\) and \(||=2\). Consider the discrepancy function \(_{h}^{}(Q;z_{h})=(Q_{h}(s_{h},a_{h})-r_{h}-_{a^{} }Q_{h+1}(s_{h+1},a^{}))\). Then we have \(_{}^{}() \), yet there exists \(\) for which \(_{}^{}(,)}}{} 1\)._

**Insufficiency of posterior sampling.** For contextual bandits, where \(_{}^{}() |}{}\), and bilinear classes, where \(_{}^{}() }()}{}\) (Proposition 2.2), a strategy that achieves the bound on the Optimistic DEC is _posterior sampling_ (this is also the approach taken in Agarwal and Zhang , Zhong et al. ). That is, given a distribution \(()\), choosing \(p()=(\{M_{{}_{M}}=\})\) in (6) certifies the desired bound on \(_{}^{D}(,)\) for these examples. Optimistic Estimation-to-Decisions subsumes and generalizes posterior sampling, but in light of the fact that this simple strategy succeeds for large classes of problems, it is reasonable to ask if there is a sense in which posterior sampling is universal, and whether it can achieve the value of the Optimistic DEC for any model class. This would be desirable, since it would indicate that solving the minimax problem in Algorithm 2 is not necessary. The following sample shows that this is not the case: there are model classes (specifically, MDPs with a constant number of actions) for which the regret of posterior sampling is exponentially large compared to the regret of Algorithm 2.

**Proposition 3.4**.: _Consider the divergence \(D_{}^{}(,)\). For any \(S\) and \(H_{2}(S)\), there exists a class of horizon-\(H\) MDPs \(\) with \(||=S\) and \(||=3\) that satisfies the following properties: \(\) There exists an estimation oracle with \(_{}^{}(S/)\) w.p. at least \(1-\) for all \(>0\). \(\) Posterior sampling, which sets \(p^{}()=^{t}(\{M_{{}_{M}}=\})\), has \([_{}] S 2^{(H)}\). \(\) Algorithm 2 with divergence \(D=D_{}^{}(,)\) has \([_{}] \)._

This shows that posterior sampling does not provide a universal mechanism for exploration, and highlights the need for deliberate strategies such as E2D.