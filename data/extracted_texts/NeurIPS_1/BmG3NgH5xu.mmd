# FERERO: A Flexible Framework for

Preference-Guided Multi-Objective Learning

 Lisha Chen\({}^{1}\), AFM Saif\({}^{1}\), Yanning Shen\({}^{2}\), Tianyi Chen\({}^{1}\)

\({}^{1}\)Rensselaer Polytechnic Institute, \({}^{2}\)University of California, Irvine

###### Abstract

Finding specific preference-guided Pareto solutions that represent different trade-offs among multiple objectives is critical yet challenging in multi-objective problems. Existing methods are restrictive in preference definitions and/or their theoretical guarantees. In this work, we introduce a Flexible framEwork for pREFeRence-guided multi-Objective learning (**FERERO**) by casting it as a constrained vector optimization problem. Specifically, two types of preferences are incorporated into this formulation - the _relative preference_ defined by the partial ordering induced by a polyhedral cone, and the _absolute preference_ defined by constraints that are linear functions of the objectives. To solve this problem, convergent algorithms are developed with both single-loop and stochastic variants. Notably, this is the _first single-loop primal algorithm_ for constrained optimization to our knowledge. The proposed algorithms adaptively adjust to both constraint and objective values, eliminating the need to solve different subproblems at different stages of constraint satisfaction. Experiments on multiple benchmarks demonstrate the proposed method is very competitive in finding preference-guided optimal solutions. Code is available at [https://github.com/lisha-chen/FERERO/](https://github.com/lisha-chen/FERERO/).

## 1 Introduction

Many machine learning tasks inherently involve multiple objectives, which can be different performance metrics such as accuracy, fairness, and privacy; or, the same metrics defined on different data . To tackle such multi-objective problems, it is common to learn a shared model that simultaneously performs well on all the objectives. Compared to learning one model for each objective, learning a shared model has the benefit of reducing both the model size and the inference time. This can be achieved through multi-objective optimization , which is to learn a model that minimizes the vector-valued objective. In practical applications, it is of interest to learn solutions with controlled trade-offs or preferences. To further illustrate, we give two examples below.

In fairness-aware machine learning, a trade-off exists between the fairness \(f_{}()\) and accuracy \(f_{}()\), see also Figure 0(a). With \(\) denoting the model parameter, and \(C\) denoting the partial order cone, to find the optimal models that consider different trade-offs, one can solve the following problem with different thresholds \(\)

\[_{C}\ (f_{}(),f_{}( ))^{}\ \ f_{}(). \]

Another example is in drug or molecule design, where the goal is to design drugs or molecules with multiple desired properties \(f_{1}(),f_{2}(),,f_{M}()\). Aiming to align the values of the properties \(F()\) with a predefined preference vector \(v\) as in Figure 0(b), one can solve the following problem 

\[_{C}\ \ F()(f_{1}(),,f_{M}( ))^{}\ \ BF()=Bv,\ Bv=0 \]

where \(B^{(M-1) M}\) is full row rank.

Then a natural question arises:

_Can we develop a principled framework to capture flexible preferences and admit provably convergent deterministic and stochastic algorithms?_

Our answer to this question is affirmative. Recognizing that all the aforementioned applications can be addressed within a unified framework, we formulate preference-guided multi-objective learning (PMOL) as a constrained vector optimization problem. Specifically, given a model \(^{q}\), and the objectives \(f_{m}:^{q},\,m=1,,M\), we define the constrained vector optimization problem as

\[_{^{q}}F()f_{1}(),,f_ {M}()^{},\ \ \ \ \ G() 0,\,H()=0\] (PMOL)

where \(G()\) and \(H()\) are the vector-valued preference constraints such as the examples in (1.1) and (1.2). Here "\(\)" and "\(=\)" are element-wise relations on the vectors, with each row representing one constraint. In these examples, the preferences are directly defined in the objective space, as intersections of half-spaces defined by the hyperplanes; see Figure 1. Thus, \(G()\) and \(H()\) in (PMOL) can be expressed as linear functions of \(F()\), given by

\[G()=B_{g}F()+b_{g},\ \ H()=B_{h}F()+b_{h} \]

where \(B_{g}^{M_{g} M},B_{h}^{M_{h} M}\), and \(b_{g}^{M_{g}},b_{h}^{M_{h}}\). Different \(B_{g},B_{h},b_{g},b_{h}\) correspond to different preferences, and thus different trade-offs among the objectives.

A comparison of our methods to existing methods is summarized in Table 1. Specifically, our contributions are listed as follows:

* We cast the PMOL problem as a constrained vector optimization problem, and develop the FERERO framework to capture flexible preferences.
* Under the FERERO framework, we develop a meta primal algorithm with a unified subprogram adaptive to both objectives and constraints to meet flexible preferences, eliminating the need for multiple subprograms under different active constraints.
* Under the FERERO framework, we develop a practical single-loop algorithm with non-asymptotic convergence guarantees. To our best knowledge, this is the _first single-loop primal algorithm_ in constrained vector optimization with convergence guarantees.
* We apply the proposed algorithms to various synthetic and real-world image and speech datasets to demonstrate its ability to find flexible preference-guided optimal models.

In our theoretical analysis, we address the following technical challenges.

* The commonly used constraint qualification assumptions do not generally hold for the PMOL problem. We overcome this challenge by leveraging the specific structure that the constraints are linear functions of \(F\) to prove the calmness condition holds for PMOL. See more details in Lemma 2.
* The convergence of the single-loop algorithm is slower with the commonly-used merit functions. We provide a sharper analysis by introducing a different merit/Lyapunov function and exploiting the algorithm properties under additional assumptions. See Theorem 3.

   Method & Preference &  & Controlled & Single & Convergence &  \\  & Flexibility & Exactness & ascent & loop & Deter. &  \\  Linear Scalarization & weight & - & ✗ & ✓ & \(T^{-1}\) & \(T^{-}\) \\ (Smooth) Tchebycheff  & weight & - & ✗ & ✓ & non-asymptotic & ✗ \\ PMTL  & inequalities (absolute) & ✗ & ✗ & ✗ & asymptotic & ✗ \\ EPO  & \(r^{-1}\) ray (ratio, absolute) & ✓ & ✓ & ✗ & asymptotic & ✗ \\ (X)WC-MGDA  & shifted ray (absolute) & ✓ & ✗ & ✗ & ✗ & ✗ \\ FERERO (ours) & relative \& absolute & ✓ & ✓ & ✓ & \(T^{-1}\) & \(T^{-}\) \\   

Table 1: Comparison to existing methods. “Flexibility” represents preference modeling, such as by using weights, preference vectors (rays), or constraints. “Exactness” represents the ability to align with a preference vector exactly. “Deter.”, “Stoch.” represent deterministic and stochastic, respectively. “✗” means not provided in the corresponding work, and “-” means not relevant.

Figure 1: Illustration of preferences in different examples. The solid red curves represent the Pareto front, dashed lines represent preference constraints.

* The convergence analysis often relies on assumptions on bounded functions or bounded constraints. We remove such assumptions by applying similar techniques in  with proper choice of Lyapunov functions, and exploiting algorithm properties. See Theorems 2 and 3.

## 2 Problem Setup and A Meta Algorithm

To characterize the optimality conditions of PMOL, we introduce the generalized notion of dominance and the related concept of optimality. We then present a meta-algorithm to solve PMOL.

### Problem setup and preliminaries

We first introduce optimality definitions for PMOL that go beyond the standard definitions of Pareto optimality [15; 11; 36]. Given two vectors \(v\) and \(w\), we use \(v<w\) and \(v w\) to denote \(v_{i}<w_{i}\) for all \(i\), and \(v_{i} w_{i}\) for all \(i\), respectively. We use \(v w\) to denote \(v w\) and \(v w\), and define \(>,,\) analogously.

**Definition 1** (\(C_{a}\)-dominance [12; 27]).: _Given \(v,w^{M}\), \(A^{M M}\), and \(C_{A}:=\{y^{M} Ay 0\}\), we say \(v\) strictly dominates \(w\) based on \(C_{A}\) if and only if \(A(v-w)<0\)._

The generalized dominance defines a partial order on \(^{M}\), i.e., the relation between two vectors. Illustrations of different partial orders are given in Figure 2. Figure 1(a) shows the dominance relation under the widely used non-negative orthant cone with \(C_{A}=^{M}_{+}\), corresponding to Pareto optimality. However, as illustrated by the figure, given the initial green reference point, a descent method such as MGDA  cannot find points on the Pareto front but outside of the gray shaded region. This poses a critical challenge for applications where specific preference-guided solutions on the Pareto front are needed. Nevertheless, this issue can be addressed by substituting \(^{M}_{+}\) with a more general definition of \(C_{A}\) as displayed in Figure 1(b). Under this partial order, a general descent method is able to reach any points on the Pareto front starting from the green reference point.

Based on the partial order, one can then find the minimum or optimal elements in the vector-valued objective space, whose formal definition is provided below.

**Definition 2** (\(C_{a}\)-optimal).: _A point \(^{q}\) is \(C_{A}\)-optimal if there is no \(^{}\) such that, \(AF(^{}) AF()\). A point \(\) is weakly \(C_{A}\)-optimal if there is no \(^{}\) such that, \(AF(^{})<AF()\)._

Note that, \(C_{A}\) is a polyhedral cone, or the intersection of half-spaces defined by the rows of the inequality \(Ay 0\). When \(A=I_{M}\), an \(M M\) identity matrix, \(C_{A}=^{M}_{+}\{y^{M} y_{m} 0\ \  m [M]\}\), then Definition 1 reduces to the commonly used notion of dominance associated with Pareto optimality. The cone \(C_{A}\) can be interpreted as a _relative preference_ that defines the objectives' improvement directions, which generalizes the relative preference defined by \(^{M}_{+}\). In contrast, the preference defined by constraints in (1.3) can be interpreted as an _absolute preference_ that defines the feasible or preferred set of objective function values. In practice, \(C_{A}\) can be chosen based on the requirements of specific applications. For example, when the controlled ascent of objectives is needed , we can choose \(C_{A}\) such that the controlled ascent direction belongs to \(-C_{A}\). We defer the detailed implementation to Section 3.2. The \(C_{A}\)-optimal set, denoted as \(_{A}\), contains all the \(C_{A}\)-optimal models. When \(A=I_{M}\), \(_{A}\) is the Pareto optimal set \(\). The Pareto front is the set of function values evaluated at Pareto optimal models, i.e., \(=\{F()\}\).

We make the following standard assumptions throughout the paper [15; 25; 7].

**Assumption 1**.: _1. (Non-negative objectives) \(AF() 0\), and \(^{}AF() c_{AF}>0\) for all \(^{q}\). 2. (Differentiable objectives) \(F\) is twice continuously differentiable. 3. (Ordering cone with non-empty interior) \(C_{A}\) has a non-empty interior._

### Find the preference-guided direction

In this section, we proceed to discuss an adaptive method to solve (PMOL). At iteration \(t\), the algorithm finds an update direction \(d_{t}\) and performs the iterative update \(_{t+1}=_{t}+_{t}d_{t}\) with a step size \(_{t}\). Ideally, the update direction \(d_{t}\) is chosen to improve the objective \(F()\) and to satisfy

Figure 2: Illustration of \(C_{A}\)-dominance. The solid red curves are the Pareto fronts, green dots are the reference points, gray shaded regions are the set of objectives dominating the reference points, under different \(C_{A}\) in (a) and (b).

the preference constraints. It is desirable that when the constraints are not satisfied, \(d_{t}\) decreases the violation of constraints and improves the objectives in the general partial ordering sense; when the constraints are satisfied, \(d_{t}\) improves the objectives and ensures the constraints are satisfied. To achieve this, we find a direction \(d^{*}()\) that solves following subprogram

\[() _{(d,c)^{q}}c+\|d\|^{2}  A F()^{}d^{}AF()}AF() \] \[ G()^{}d+c_{g}G() 0,\; H()^{ }d+c_{h}H()=0\]

where \(\|\|\) denotes the \(_{2}\)-norm, \(c_{g}\) and \(c_{h}\) are pre-defined positive constants. Larger \(c_{g}\) and \(c_{h}\) put more emphasis on constraint satisfaction than objective improvement. We call this subprogram _adaptive_ since it deals with constraints in an adaptive way, which does not require the initial model to be feasible, nor \(_{t}\) to be feasible at each iteration. But rather, it finds an update direction that decreases the constraint violation. Because of this, it neither requires solving different subprograms at different stages nor requires different treatment of the active set of inequalities as in existing works .

We then show in Lemma 1 that the desired properties can be satisfied.

**Lemma 1**.: _For the subprogram (2.1), the following holds: If \(\) is a local optimal solution with \(AF()>0\), then \(d^{*}()=0\), \(()=0\). Otherwise, if \(\) is not a local optimal solution, then \(d^{*}() 0\), \(()<0\), and when \(\) is feasible,_

\[2()-\|d^{*}()\|^{2}<0. \]

_Let \(\) be a weak \(C_{A}\)-optimal solution, with \((AF())_{m}=0\) for some \(m[M]\). If there exists feasible and non-strictly improving directions at \(\) with \(A F()^{}d 0\), then \(d^{*}() 0\), \(()<0\). Otherwise, \(d^{*}()=0\), \(()=0\)._

By Lemma 1, \(\|d^{*}()\|=0\) is a stationary condition for PMOL. Recall the feasibility condition requires \([G()]_{+}=0\) and \(|H()|_{}=0\), where \([]_{+}\) and \(||_{}\) are entry-wise ReLU and absolute functions, respectively. And the complementary slackness condition requires \(_{g}^{*}[-G()]_{+}=0\). Thus \(\|d^{*}()\|^{2}+_{g}^{*}[-G()]_{+}+\|[G()]_{+}\|_ {1}+\|H()\|_{1}\) achieves zero if and only if the model \(\) satisfies the first-order KKT condition. Besides the properties in Lemma 1, it has an additional scale-invariant property that is deferred to Lemma 6 due to space limit.

By the Lagrangian of (2.1), the optimal update direction can be expressed in a simple form as a weighted combination of the gradients, i.e. \(d^{*}()=- F()A_{ag}^{}^{}\), with \(A_{ag}[A;B_{g};B_{h}]\), and

\[^{*}*{arg\,min}_{_{}()}\; (;)\| F()A_{ag}^{} \|^{2}-c_{g}_{g}^{}G()-c_{h}_{h}^{}H() \]

where \(=[_{f};_{g};_{h}]\), \(_{}()\) is the domain of the Lagrangian multipliers, given by 1

\[_{}()_{_{f}}()_{+}^{M_{g}}^{M_{h}},\;\;\;\;_{_{f}} ()\{_{+}^{M}\;|\;^{}AF()= _{M}^{}AF()\}. \]

Our goal is to design an algorithm that converges to a KKT solution based on (2.1). However, the KKT condition is not necessary unless certain constraint qualifications (CQs) hold. Prior works  assume certain CQs hold, e.g., the Linear Independence Constraint Qualification (LICQ). However, the LICQ assumption (c.f., [20, Section 3.1, (A2)]) does not generally hold at a local optimal solution for problem (PMOL), c.f., Example 1 in Appendix D.3.2. Though some commonly used CQs do not hold generally, in our case, leveraging the specific structure that the constraints are linear functions of \(F\), we can justify the calmness CQ in Definition 10 tailored for our problem in Lemma 2, thus the KKT condition is a necessary optimality condition. The proof is deferred to Appendix D.3.2.

**Lemma 2**.: _Let \(^{q}\) be a global solution to (PMOL). Define \((p,q)\{y^{M} B_{g}y+b_{g} p,B_{h}y+b_{h}=q\}\). If \((p,q)\) is a line, the PMOL calmness condition in Definition 10 is satisfied for (PMOL) at \(\) if \(A^{M M}\) is full rank, \(H(),G()\) defined by (1.3) satisfy \([B_{h}^{},B_{g}^{}] 0\), and \(B_{h},B_{g}\) are full row rank. Consequently, the KKT condition is a necessary optimality condition._

Lemma 2 provides a sufficient condition for the KKT condition to be a necessary optimality condition without relying on unjustified assumptions. The requirement that the constraint set is a line in the objective space is common for applications such as alignment to a preference vector.

We then discuss a generic preference-guided multi-objective algorithm based on the subprogram.

### A meta algorithm for preference-guided multi-objective learning

Given the model \(_{t}\) at iteration \(t\), one can then solve (2.3) to obtain \(_{t}\). The direction \(d_{t}=- F(_{t})A_{qg}^{}_{t}\) is used to update the model \(_{t}\) by \(_{t+1}=_{t}+_{t}d_{t}\) iteratively until convergence. The full procedure of this meta algorithm is summarized in Algorithm 1, where Step 4 is a generic step and can be customized in Section 3.

To establish the non-asymptotic convergence rate, we use the following standard smoothness assumption that has been commonly used in prior works for multi-objective learning [7; 36].

**Assumption 2** (Smooth objectives).: _For all \(m[M]\), \( f_{m}()\) is \(_{f,1}\)-Lipschitz continuous._

We then state the convergence result for Algorithm 1 in Theorem 1.

**Theorem 1** (Convergence of the generic FERERO algorithm).: _Suppose Assumptions 1, 2 hold. Let \(\{_{t}\}\) be the sequences produced by Algorithm 1, with \(d_{t}\) being an \(\)-optimal solution to the subprogram (2.1). If \(\|^{*}(_{t})\|_{1} c_{}\), \(_{t}\{_{f,1}\|_{alg}\|_{,1} },c_{g}^{-1},c_{h}^{-1}\}\), and \(_{t}=(1)\), then_

\[_{t=0}^{T-1})A_{ ag}^{}^{*}(_{t})\|^{2}}_{}+ ^{*}(_{t})^{}[-G(_{t})]_{+}}_{}+)]_{+}\|_{1}+\|H(_{t})\|_{1}}_{}= T^{-1}+. \]

Theorem 1 guarantees the non-asymptotic convergence for the generic FERERO algorithm. In Algorithm 1, \(_{t}\) can be solved through projected gradient descent or Frank Wolfe algorithm iteratively within an inner loop. In practice, we usually do not need to solve the subprogram exactly. Next, we discuss the efficient single-loop approximate algorithm based on Algorithm 1.

## 3 Efficient Single-loop Algorithms

In this section, we first discuss algorithm development with the approximate single-loop update and practical choice of preferences. We focus on (PMOL) with _equality constraints only_, i.e., \(M_{g}=0\). Building upon this, we then discuss the stochastic variants of the algorithms that can be applied to large-scale learning problems.

### Single-loop approximate algorithm

In practice, if one only requires the converging solutions generated by the algorithm to be feasible, but not all the iterates, then further approximations can be made to the subprogram (2.3). At iteration \(t\), to obtain an approximate direction \(d_{t}\), we adopt the following update

\[_{t+1}=_{_{}}_{t}-_{t }_{}(_{t};_{t}). \]

The single-loop algorithm with the approximate solution is summarized in Algorithm 2. We name it FERERO with Single-loop Approximate update (FERERO-SA) algorithm.

```
1:Initialize \(t=0\), \(_{0}\), \(_{0}\), step sizes \(\{_{t},_{t}\}\); define \(A\), number of iterations \(T\).
2:for\(t=0,,T-1\)do
3: Compute gradient \( F(_{t})\);
4: Compute direction \(d_{t}=- F(_{t})A_{ag}^{}_{t}\);
5: Update \(_{t}\) by \(_{t+1}=_{t}+_{t}d_{t}\);
6: Update \(_{t}\) by (3.1);
7:endfor
```

**Algorithm 2** FERERO-SA

We make the following additional assumption of Lipschitz objectives to prove the convergence of Algorithm 2, which is standard in optimization literature.

**Assumption 3** (Lipschitz objectives).: _For all \(m[M]\), \(f_{m}()\) is \(_{f}\)-Lipschitz continuous._To prove the convergence of Algorithm 2, we can use the same merit function with \(_{1}\)-norm of \(H(_{t})\), which leads to a slow convergence rate of \(T^{-}\). See Theorem 2 below and its proof in Appendix F.2, where the proof follows similar ideas of the proofs of Theorem 3 and Theorem 5 in .

**Theorem 2** (Convergence of the FERERO-SA algorithm).: _Suppose Assumptions 1, 2, 3 hold, and \(M_{g}=0\). Let \(\{_{t}\},\{_{t}\}\) be the sequences produced by Algorithm 2 with \(A=I\) and \(_{_{f}}()=^{M}\) (c.f. Remark 4). Assume \(_{t},^{*}(_{t})\), and \(^{*}_{}(_{t})_{_{}} (;_{t})+\|\|^{2}\) are bounded. With properly chosen step sizes \(=(T^{-})\), \(=(T^{-})\), and hyperparameters, it holds that_

\[_{t=0}^{T-1}\| F(_{t})A_{ag}^{}_{t}\|^{2 }+\|H(_{t})\|_{1}=T^{-}. \]

To obtain a sharper convergence rate, we consider a different merit function with \(_{2}\)-norm of the constraint \(H(_{t})\), and under additional assumptions listed below.

**Definition 3** (Proximal PL inequality).: _Define \(D_{,}(;)-_{^{} _{}}_{}(;), ^{}-+\|^{}-\|^{2} }\). We say \((;)\) satisfies the \(_{}\)-proximal PL inequality on the point \((,)\), if there exists some constant \(_{}>0\) such that \(D_{,}(;)_{}(; )-(^{*}();)\)._

**Assumption 4**.: _For \(\{_{t}\},\{_{t}\}\) on the trajectory of Algorithm 2, the following hold: 1. \((;)\) is \(_{}\)-proximal PL in Definition 3; 2. For all \(m[M]\), \(^{2}f_{m}()\) is \(_{f,2}\)-Lipschitz continuous._

Assumption 4-1 essentially requires some regularity conditions of \((;)\) on the trajectory of Algorithm 2. Leveraging the fact that \((;)\) is convex, it has been discussed in e.g., [28, Appendix B] that if the smallest non-zero singular value of the Hessian is bounded away from zero, then Assumption 4-1 holds. This could be satisfied when the gradients \( F(_{t})A_{ag}^{}\) have lower-bounded non-zero singular values on the trajectory. A more detailed analysis of the sufficient conditions for Assumption 4-1 to hold is left for further work.

We then provide a sharper convergence analysis in Theorem 3. The detailed proof and choices of step sizes and hyperparameters are deferred to Appendix F.3.

**Theorem 3** (Sharper convergence of the FERERO-SA algorithm).: _Suppose Assumptions 1, 2, 3, 4 hold, and \(M_{g}=0\). Let \(\{_{t}\},\{_{t}\}\) be the sequences produced by Algorithm 2 with \(A=I\) and \(_{_{f}}()=^{M}\) (c.f. Remark 4). With properly chosen step sizes \(_{t}=(1)\), \(_{t}=(1)\), and hyperparameters, it holds that_

\[_{t=0}^{T-1}\| F(_{t})A_{ag}^{}_{t}\|^{2 }+\|H(_{t})\|^{2}=T^{-1}. \]

Theorem 3 states that \(\{_{t}\}\) produced by Algorithm 2 converges to a KKT solution of the PMOL problem in the general nonconvex case. Moreover, both \(\|d_{t}\|^{2}\) and \(\|H(_{t})\|^{2}\) converge to zero at a rate of \((T^{-1})\), implying the convergence of both the objective values and the preference constraints. Note that, the convergence in terms of \(\|H(_{t})\|^{2}\) at a rate of \((T^{-1})\) is weaker compared to the one with \(\|H(_{t})\|_{1}\) at the same rate for Algorithm 1. This is reasonable since Algorithm 2 only uses a one-step approximate update of \(_{t}\) instead of exactly solving the subprogram.

**The stochastic variant.** We employ a stochastic variant of Algorithm 2 based on the double sampling techniques developed in the recent work . The update is given by

\[_{t+1}= _{t}+ F_{_{t,1}}(_{t})A_{ag}^{}_{t} \] \[_{t+1}= _{_{}}_{t}-_{t}_{}(_{t};_{t})\] (3.4b) \[_{}(_{t};_{t})= A_{ag} F_{_{t,2}}(_{t})^{} F_{_{t,1}}(_{t})A_{ag}^{ }_{t}-[0^{},c_{h}H_{_{t,1}}(_{t})^{}]^{} \]

where \(\) is the unbiased stochastic estimate of the gradient, and \(_{t,1}\) and \(_{t,2}\) are two independent stochastic samples obtained at iteration \(t\).

The full description of the stochastic algorithm and its convergence guarantee are deferred to Appendix G. We provide a converegnce rate guarantee that matches the rate of SGD under additional assumptions on the bounded variance of the stochastic gradients.

### Choice of relative preferences

As briefly discussed in Section 2.1, the ordering cone and the corresponding matrix \(A\) can be specified according to practical needs. We first discuss how to obtain matrix \(A\) for the relative preference given the set of improvement directions. Then we discuss how to choose the relative preference to allow controlled ascent update, which is useful for touring the Pareto front .

**Ordering cone generation.** In practice, to obtain the polyhedral cone that defines the partial order, one can usually first define the extreme rays of the polyhedral cone. We then show how to convert the extreme ray description of the cone to the half-space description given by matrix \(A\), i.e., \(C_{A}=\{y^{M} Ay 0\}\), by showing how to compute \(A\) from the extreme rays.

Let \(Y=[y_{1} y_{M}]^{M M}\) be a matrix that contains all the extreme rays of \(C_{A}\) as its column vectors, then \(C_{A}=\{Y 0\}\). Let \(a_{m}^{}^{1 M}\) denote the row vectors of \(A\) for all \(m[M]\). Then all \(a_{m}\) can be found by \(a\) that solves the following linear feasibility program

\[}\ \ \ Y=c,\ \ c^{}a=0,\ \ Y^{}a 0. \]

**Choice of \(C_{A}\) for controlled ascent.** If \(C_{A}\) is not pre-specified, and the decision maker wants to choose \(C_{A}\) to allow controlled ascent, it can be achieved with the following procedure. Let \(F_{0}=F(_{0})\) be the objective of the initial iterate of the algorithm, and \(F_{tg}\) be the target function value along the controlled ascent direction. To ensure \(F_{tg}-F_{0}-C_{A}\) for controlled ascent, we include \((F_{0}-F_{tg})/\|F_{0}-F_{tg}\|\) in the set of extreme rays, then take the extreme rays of the convex hull of the new set to form the columns of \(Y\). Finally, we obtain \(C_{A}\) by solving (3.5).

## 4 Related Works

To put our work in context, we review the most relevant literature in (preference-guided) multi-objective optimization, constrained optimization, with a focus on gradient-based approaches.

**Multi-objective optimization (MOO).** A straightforward approach of MOO is to use scalarization to transform MOO into a single-objective optimization problem . Another popular approach focuses on finding update directions which avoid conflicts with the gradients of the objectives [52; 59; 35]. A foundational algorithm in this domain is the Multiple Gradient Descent Algorithm (MGDA) [15; 17; 11; 36], which dynamically weights gradients to find a steepest common descent direction for all objectives. Later on, variants of MGDA are developed, which are discussed in detail in Appendix B.1 and . However, solutions based on MGDA usually cannot capture pre-defined user preferences that represent various trade-offs on the Pareto front. This motivates the development of _preference-guided multi-objective optimization_ methods.

Preferences can be modeled through weights or thresholds assigned to different objectives . For example, scalarization-based methods use the \(_{p}\)-norm of the weighted vector-valued objective to convert the vector-valued objective into a scalar-valued objective, e.g., Linear scalarization (LS), Tchebycheff scalarization; see e.g., . Then the problem can be solved by single-objective optimization on the scalar objective. The \(\)-constraint methods enforce threshold constraints on different objectives, then solve the problem by constrained optimization; see e.g., . More recently, preferences have been modeled by preference vectors defined in the objective space. Then the problem can be formulated as finding Pareto optimal solutions satisfying the constraints defined by the preference vectors , or optimizing the distance to the preference vectors [41; 44]. The key difference between FERERO and these works is that FERERO can capture more flexible preferences based on a general partial order, and general inequality/equality constraints. Moreover, we provide convergence rate guarantees for the proposed algorithms. A detailed comparison is summarized in Table 1 in Section 1 and Table 5 in Appendix B.2.

**Constrained optimization.** Constrained optimization methods include primal methods, penalty and barrier methods, and primal-dual methods [4; 39]. Our proposed method is related to the primal method that finds an update direction to ensure the models are feasible and improving along the optimization trajectory. To address the limitation that it usually requires a stage-one procedure to ensure the initialization is feasible, we use an adaptive approach to ensure the constraint violation is decreasing and converging to zero. This idea can also be found in sequential quadratic programming (SQP). SQP has been widely applied to solve constrained single-objective optimization [21; 6]. Later on, it has also been applied to constrained MOO . Compared to SQP, we use an identity matrix to approximate the Hessian of each objective, and we propose an adaptive variant that automatically adjust the descent amount of objectives. Furthermore, existing SQP algorithms typically require an inner loop to solve the optimal Lagrangian multiplier, resulting in double-loop algorithms. In contrast, we develop a single-loop algorithm which can be more efficient.

**Vector optimization.** Vector optimization [12; 27] generalizes multi-objective optimization by substituting the commonly used component-wise partial order with a more general partial order, such as a general convex-cone induced partial order used in this paper. In the unconstrained setting, the MGDA method is extended to a steepest cone descent method in the vector optimization setting in . In the constrained setting, the first-order optimality conditions are studied in [23; 57]. Algorithms based on projected gradient [24; 18; 19] or conditional gradient  are developed to solve vector optimization with parameters in a constraint set, to name a few. Besides gradient-based vector optimization, another line of works focus on black-box vector optimization with discrete design space; see e.g. [3; 2]. To our best knowledge, we are the first to design gradient-based single-loop (stochastic) primal algorithms for constrained vector optimization with convergence rate guarantees.

## 5 Experiments

In this section, we conduct experiments to verify our theory and show the applicability of the algorithms to preference-guided multi-task learning, and multi-objective finetuning of large multi-lingual speech recognition models. We use Linear scalarization (LS), MGDA , PMTL , EPO , XWC-MGDA  as baselines for comparison.

**Metrics.**_Objective loss and accuracy._ We report the objective losses and accuracies in classification. _Relative loss profile._ We use the element-wise product of the preference vector and the objective values as a measure of the relative loss profile. _Hypervolume._ Let \(F^{}^{M}\) denote a reference point, and \(\) denote a set of objective function values of the obtained models. Hypervolume measures the size of the dominated space of \(\) relative to \(F^{}\), which can be computed by \(H()=(\{q^{M} F:F q  F^{}\})\), where \(()\) denotes the Lebesgue measure. For a fair comparison, we use the Nadir point, i.e., the worst performance on single-task baselines, as the reference point \(F^{}\).

**Additional details.** The implementation and additional experiments can be found in Appendix H.

### Synthetic data

Following [33; 41; 44], the first objective we consider is

\[F()=1-e^{-\|-}1\|_{2}^{2}},\ \ 1-e^{-\| +}1\|_{2}^{2}}. \]

The objective has a nonconvex Pareto front (PF). See the results of different methods in Figure 3. With uniformly generated weights from a simplex, LS only finds extreme points on the PF with one objective minimized. MGDA can only find points close to the center of the PF. PMTL can find points

Figure 4: Outputs (colored markers) and optimization trajectories (colored lines) of different methods when initial objectives are near the Pareto front. Different colors represent different preferences.

Figure 3: Converging solutions (blue dots) and optimization trajectories (blue lines) on the objective space of different methods on synthetic objectives given in (5.1). Dashed arrows represent pre-specified preference vectors. The green dots represent initial objective values.

in the subregions but not aligned well with the exact preference vectors. Similar to EPO, in Figure 2(e), our method finds points that align well with the exact preferences; and in Figure 2(f), our method can handle different definitions of preferences.

We conduct another experiment in a more difficult setting where the initial objectives are close to the PF. In Figures 3(a)-3(c), we consider a relatively easier case where the initial model is not too close to the Pareto optimal. For our method, by solving (3.5), \(a_{1}=[};}],a_{2}=[};}]\). The corresponding matrix \(A\) is given by \(A=[a_{1},a_{2}]^{}\). In this setting, all methods converge to the PF, and our method takes the least number of iterations (PMTL takes 100, EPO Search takes 60, and our method takes only 10 iterations). PMTL does not align exactly with the preference vectors, while EPO and our method do. In Figures 3(d)-3(f), PMTL and our method take \(200\) iterations, EPO Search takes 80 iterations. Results show that for the green and yellow preferences, PMTL moves further away from the PF in the first stage, and does not perform any update in the second stage. It converges to the PF only in 2 out of 4 cases. In contrast, with controlled ascent updates, EPO and our method can converge to the PF and trace the PF until the objectives align exactly with the preferences.

### Real data

**Multi-patch image classification.** Following [33; 41; 44], we consider three datasets for image classification, including Multi-MNIST, Multi-Fashion, and Multi-Fashion+MNIST. The two tasks or objectives in all three datasets are to classify the top-left and the bottom-right images, respectively. For a fair comparison, we use LeNet as the backbone neural network. The training losses and accuracies of different methods given different preference vectors are plotted in Figure 5. Experiments for our method are repeated 5 times. Hypervolumes with means and standard deviations are reported in Table 2. The results for other methods in Table 2 are referenced from .

   Datasets & LS & PMTL  & EPO  & XWC-MGDA  & FERERO \\  Multi-MNIST loss & 1.68 & 1.41 & 1.35 & 1.42 & **1.97\(\)0.21** \\ Multi-Fashion loss & 6.75 & 5.90 & 6.02 & 6.77 & **7.76\(\)0.18** \\ Multi-F+M loss & 3.63 & 3.03 & 3.76 & **3.89** & 3.82\(\)0.21 \\ Multi-MNIST accuracy & 0.19 & 0.15 & 0.15 & 0.16 & **0.24\(\)0.04** \\ Multi-Fashion accuracy & 0.99 & 0.87 & 0.87 & 0.99 & **1.17\(\)0.07** \\ Multi-F+M accuracy & 0.48 & 0.40 & 0.50 & 0.52 & **0.53\(\)0.04** \\   

Table 2: Hypervolumes of different methods (\( 10^{-2}\))

Figure 5: Training losses and accuracies of various methods with different preferences across three image datasets. The horizontal and vertical axes represent results for objective 1 and objective 2, respectively. Different colored dashed arrows indicate various preference vectors. Different markers denote the solutions obtained by different methods, with marker colors matching the preferences.

One limitation of EPO is that the preference is defined as a ray from the origin in the objective space, whose corresponding objectives can be unattainable, e.g., the yellow preferences in Figure 5. As a result, the losses of all methods are far away from the preference vectors. In this case, a more flexible choice of preferences is helpful to ensure preference satisfaction. To demonstrate this, we conduct experiments with more flexible preferences; see the results in Figure 6, where the obtained solutions align better with the preference lines compared to those in Figure 5. Moreover, it can perform controlled ascent updates during optimization, which cannot be achieved by PMTL or XWC-MGDA.

Multi-lingual speech recognition.We further apply the proposed method to the multi-objective finetuning of pre-trained multi-lingual speech models. We use the Librispeech (100 hours) , and AISHELL v1  datasets for multi-lingual speech recognition. A conformer with 8 blocks is used as the model architecture. The total number of parameters is around 64.5M with 58.4M encoder layer parameters and the rest being the classification layer parameters. We consider the objectives associated with the speech recognition Connectionist Temporal Classification (CTC) losses in Chinese and English, denoted as \(f_{t}^{}\) and \(f_{t}^{}\), respectively. We also use the self-supervised Contrastive Predictive Coding (CPC) loss \(f_{p}\) for representation learning; that is

\[_{}\ \ \ F():=(f_{p}(),f_{t}^{}(),f_{t} ^{}())^{}\ \ \ \ f_{p}()_{1},\ f_{t}^{}()-f_{t}^{}()=_{2} \]

where the first constraint ensures to learn a good representation with \(_{1}=1.2\), and the second constraint avoids one language loss dominates the other with \(_{2}=0.5\); see more details in Appendix H.1.

Results on the word error rate (WER) are reported in Table 3. The baselines include the state-of-the-art result from Komatsu et al.  without an additional large language model, our own implementation of training using only the sum of supervised CTC losses (w/o CPC), the initial pre-trained M2ASR model  (init.), linear scalarization of all three objectives for finetuning a pre-trained model with the CPC loss (LS-FT). Results show that considering CPC loss besides the supervised CTC loss improves the average WER by 4.2%, and this can be further improved by 0.3% by finetuning with linear scalarization. However, the LS-FT model has a much better performance in Chinese compared to English. With our proposed approach, the performance gap between different languages is reduced, and the average WER is further improved by 1.3%.

## 6 Conclusions

In this work, we frame preference-guided multi-objective learning as a constrained vector optimization problem. Specifically, we introduce constraints and partial order to capture the absolute and relative preferences. Under this framework, we develop algorithms to solve the constrained vector optimization problem. Our proposed algorithms use a unified formulation without solving different subprograms at different stages. And they enjoy the benefit of allowing controlled ascent and escaping weak optimal solutions. Theoretical guarantees on the non-asymptotic convergence of the deterministic algorithms and their stochastic variants are provided. Experiments on benchmark datasets demonstrate the broad applicability of the proposed algorithms.

   Method & English & Chinese & Average \\  Komatsu et al.  & 7.11 & - & - \\ w/o CPC  & 11.8 & 10.2 & 11.0 \\ Init. (M2ASR)  & 7.3 & 6.2 & 6.7 \\ LS-FT & 6.8 & 5.9 & 6.4 \\ FERERO-FT & **5.4** & **4.9** & **5.1** \\   

Table 3: WERs (%) on Librispeech and AISHELL v1.

Figure 6: Losses and preferences of FERERO when the initial objective is close to the Pareto front.

## Broader Impacts and Limitations

This paper casts the preference-guided multi-objective learning as a constrained vector optimization problem and proposes an algorithm with single-loop and stochastic variants to solve the problem, which have non-asymptotic convergence guarantees. The proposed method is applied to image classification and speech recognition. The positive impact is that it is a principled method with efficient implementations that has broad applications across various domains. There is no negative social impact.

The proposed algorithm is able to model flexible preferences but at a cost of higher per-iteration complexity compared to scalarization methods. The theoretical guarantees make standard assumptions that the objectives are lower bounded, Lipschitz continuous and smooth. These are common assumptions in the optimization literature, and can be satisfied for neural networks with smooth activation functions.