# Bias in Evaluation Processes:

An Optimization-Based Model

 L. Elisa Celis

Yale University

&Amit Kumar

IIT Delhi

&Anay Mehrotra

Yale University

&Nisheeth K. Vishnoi

Yale University

###### Abstract

Biases with respect to socially-salient attributes of individuals have been well documented in evaluation processes used in settings such as admissions and hiring. We view such an evaluation process as a transformation of a distribution of the true utility of an individual for a task to an observed distribution and model it as a solution to a loss minimization problem subject to an information constraint. Our model has two parameters that have been identified as factors leading to biases: the resource-information trade-off parameter in the information constraint and the risk-averseness parameter in the loss function. We characterize the distributions that arise from our model and study the effect of the parameters on the observed distribution. The outputs of our model enrich the class of distributions that can be used to capture variation across groups in the observed evaluations. We empirically validate our model by fitting real-world datasets and use it to study the effect of interventions in a downstream selection task. These results contribute to an understanding of the emergence of bias in evaluation processes and provide tools to guide the deployment of interventions to mitigate biases.

## 1 Introduction

Evaluation processes arise in numerous high-stakes settings such as hiring, university admissions, and fund allocation decisions . Specific instances include recruiters estimating the hireability of candidates via interviews , reviewers evaluating the competence of grant applicants from proposals , and organizations assessing the scholastic abilities of students via standardized examinations . In these processes, an evaluator estimates an individual's value to an institution. The evaluator need not be a person, they can be a committee, an exam, or even a machine learning algorithm . Moreover, outcomes of real-world evaluation processes have at least some uncertainty or randomness . This randomness can arise both, due to the features of the individual (e.g., their test scores or grades) that an evaluator takes as input , as well as, due to the evaluation process itself .

Biases against individuals in certain disadvantaged groups have been well-documented in evaluation processes . For instance, in employment decisions and peer review, women receive systematically lower competence scores than men, even when qualifications are the same , in standardized tests, the scores show higher variance in students from certain genders , and in risk assessment-a type of evaluation-widely used tools were twice as likely to misclassify Black defendants as being at a high risk of violent recidivism than White defendants . Here, neither the distribution of individuals' true evaluation depends on their socially-salient attributes nor is the process trying to bias evaluations, yet biases consistently arise . Such evaluations are increasingly used by ML systems to learn or make decisions about individuals, potentially exacerbating inequality . This raises the question of explaining the emergence of biases in evaluation processes which is important to understand how to mitigate them, and is studied here.

**Related work.** A wide body of work has studied reasons why such differences may arise and how to mitigate the effect of such biases . For one, socioeconomic disadvantages (often correlated with socially-salient attributes) have been shown to impact an individual's ability to perform in an evaluation process, giving rise to different performance distributions across groups . Specifically, disparities in access to monetary resources are known to have a significant impact on individuals' SAT scores . Moreover, because of differences between socially-salient attributes of individuals and evaluators, the same amount of resources (such as time or cognitive effort) spent by the evaluator and the individual, can lead to different outcomes for individuals in different groups . For instance, it can be cognitively more demanding, especially in time-constrained evaluation processes, for the evaluator to interact with individuals who have a different cultural background than them, thus impacting the evaluations . Further, such biases in human evaluations can also affect learning algorithms through biased past data that the algorithms take as input .

Another factor that has been identified as a source of bias is "risk averseness:" the tendency to perceive a lower magnitude of increase in their utility due to a profit than the magnitude of decrease in their utility due to a loss of the same magnitude as the profit . Risk averseness is known to play a role in high-stakes decisions such as who to hire, who to follow on social networks, and whether to pursue higher education . In evaluations with an abundance of applicants, overestimating the value of an individual can lead to a downstream loss (e.g., because an individual is hired or admitted) whereas under-estimating may not have a significant loss . Thus, in the presence of risk averseness, the outputs of evaluation processes may skew the output evaluations to lower or higher values. The same skew can also arise from the perspective of individuals . For instance, when negotiating salaries, overestimating their salary can lead to adverse effects in the form of evaluators being less inclined to work with the individual or in extreme cases denying employment . Moreover, these costs have been observed to be higher for women than for men, and are one of the prominent explanations for why women negotiate less frequently .

A number of interventions to mitigate the adverse effects of such biases in evaluation processes have been proposed. These include representational constraints that, across multiple individuals, increase the representation of disadvantaged and minority groups in the set of individuals with high evaluations , structured evaluations which reduce the scope of unintended biases in evaluations , and anonymized evaluations that, when possible, blind the decision makers to the socially-salient attributes of individuals being evaluated .

Mathematically, some works have modeled the outcomes of evaluation processes based on empirical observations . For instance, the implicit variance model of  models differences in the amount of noise in the utilities for individuals in different groups. Here, the output estimate is drawn from a Gaussian density whose mean is the true utility \(v\) (which can take any real value) and whose variance depends on the group of the individual being evaluated: The variance is higher for individuals in the disadvantaged group compared to individuals in the advantaged group. Additive and multiplicative skews in the outputs of evaluation processes have also been modeled  (also see Appendix A).  consider true utilities \(v>0\) distributed according to the Pareto density and they model the output as \(v/\) for some fixed \( 1\); where \(\) is larger for individuals in the disadvantaged group. These models have been influential in the study of various downstream tasks such as selection , ranking , and classification  in the presence of biases.

**Our contributions.** We propose a new optimization-based approach to model how an evaluation process transforms an (unknown) input density \(f_{}\) representing the true utility of an individual or a population to an observed distribution in the presence of information constraints or risk aversion. Based on the aforementioned studies and insights in social sciences, our model has two parameters: the resource-information parameter (\(\)) in the information constraint and the risk-averseness parameter (\( 1\)) in the objective function; see (OptProg) in Section 2. The objective measures the inaccuracy of the estimator with respect to the true density \(f_{}\), and involves a given loss function \(\) and the parameter \(-\) is higher (worse) for individuals in groups facing higher risk aversion. The constraint places a lower bound of \(\) on the amount of information (about the density of the true value \(v\)) that the individual and evaluator can acquire or exchange in their interaction - \(\) is higher for individuals in groups that require more resources to gain unit information. We measure the amount of information in the output density by its differential entropy. Our model builds on the maximum-entropy framework in statistics and information theory  and is derived in Section 2 and can be viewed as extending this theory to output a rich family of biased densities.

In Section 3, we show various properties of the output densities of our model. We prove that the solution to (OptProg) is unique under general conditions and characterize the output density as a function of \(f_{}\), \(\), \(\), and \(\); see Theorem 3.1. By varying the loss function and the true density, our framework can not only output standard density functions (such as Gaussian, Pareto, Exponential, and Laplace), but also their appropriate "noisy" and "skewed" versions, generalizing the models studied in [90; 22; 61]. Subsequently, we investigate how varying the parameter \(\) affects the output density in Section 3. We observe that when \(-\), there is effectively no constraint, and the output is concentrated at a point. For any fixed \(\), as \(\) increases, the output density _spreads_-its variance and/or mean increases. We also study the effect of increasing \(\) on the output density. We observe that when the true density is Gaussian or Pareto, the mean of the output density decreases as \(\) increases for any fixed \(\). Thus, individuals in the group with higher \(\) and/or \(\) face higher noise and/or skew in their evaluations as predicted by our model.

Empirically, we evaluate our model's ability to emulate biases present in real-world evaluation processes using two real-world datasets (JEE-2009 Scores and the Semantic Scholar Open Research Corpus) and one synthetic dataset (Section 4). For each dataset, we report the total variation (TV) distance between the densities of biased utilities in the data and the best-fitting densities output by our framework and earlier models. Across all datasets, we observe that our model can output densities that are close to the density of biased utilities in the datasets and has a better fit than the models of [61; 90]; Table 1. Further, on a downstream selection task, we evaluate the effectiveness of two well-studied bias-mitigating interventions: equal representation (ER) and proportional representation (PR) constraints, and two additional interventions suggested by our work: decreasing the resource-information parameter \(\) and reducing the risk-aversenses parameter \(\). ER and PR are constraints on the allowable outcomes, \(\) can be decreased by, e.g., training the evaluators to improve their efficiency, and \(\) can be decreased using, e.g., structured interviews . We observe that for each intervention, there are instances of selection, where it outperforms all other interventions (Figure 1). Thus, our model can be used as a tool to study the effectiveness of different types of interventions in downstream tasks and inform policy; see also Appendix L.1 and Appendix B.

## 2 Model

The evaluation processes we consider have two stakeholders-an evaluator and an individual-along with a societal context that affects the process. In an evaluation process, an evaluator interacts with an individual to obtain an estimate of the individual's utility or value. We assume that each individual's true utility \(v\) is drawn from a probability distribution. This not only captures the case that the same individual may have variability in the same evaluation (as is frequently observed in interviews, examinations, and peer-review [31; 50; 76; 124; 30; 140; 18]) but also the case that \(v\) corresponds to the utility of an individual drawn from a population. For simplicity, we consider the setting where \(v\) is real-valued and its density is supported on a continuous subset \(\). This density gives rise to a distribution over \(\) with respect to the Lebesgue measure \(\) over \(\). For instance, \(\) could be the set of all real numbers \(\), the set of positive real number \(_{>0}\), an open interval such as \([1,)\), or a closed interval \([a,b]\). Following prior work modeling output densities [90; 40; 61], we assume that the true utility of all individuals is drawn from the _same_ density \(f_{}\) into an observed density \(f_{}\) over \(\). In real-world evaluation processes, this happens through various means: by processing features of an individual (e.g., past performance on exams or past employment), through interaction between the evaluator and the individual (e.g., in oral examinations), or by requesting the individual to complete an assessment or test [121; 99; 19]. We present an optimization-based model that captures some of the aforementioned scenarios and outputs \(f_{}\). The parameters of this model encode factors that may be different for different socially-salient groups, thus, making \(f_{}\) group dependent even though \(f_{}\) is not group dependent. We derive our model in four steps.

**Step 1: Invoking the entropy maximization principle.** In order to gain some intuition, consider a simple setting where the utility of an individual is a fixed quantity \(v\) (i.e., \(f_{}\) is a Dirac-delta function around \(v\)). We first need to define an _error_ or loss function \(\); given a guess \(x\) of \(v\), the loss function \((x,v)\) indicates the gap between the two values. We do not assume that \(\) is symmetric but require \((x,v) 0\) when \(x v\). Some examples of \((x,v)\) are \((x-v)^{2},|x-v|,x/v\), and \((x/v)\). The right choice of the loss function can be context-dependent, e.g., \((x-v)^{2}\) is a commonly used loss function for real-valued data, and \((x/v)\) is sometimes better at capturing relative error for heavy-tailed distributions over positive domains . For a density \(f\) for \(x\), \(_{x f}\,(x,v)\) denotes the expected error of the evaluation process. One can therefore consider the following problem: Given a value \(\), can we find an \(f\) such that \(_{x f}\,(x,v)\)? This problem is under-specified as there may be (infinitely) many densities \(f\) satisfying this constraint. To specify \(f\) uniquely, we appeal to the maximum entropy framework in statistics and information theory : Among all the feasible densities, one should select the density \(f\) which has the maximum entropy. This principle leads to the selection of a density that is consistent with our constraint and makes no additional assumption. We use the notion of the (differential) entropy of a density with respect to the Lebesgue measure \(\) on \(\):

\[(f)-_{x}f(x) f(x)d(x), \]

where \(f(x) f(x)=0\) whenever \(f(x)=0\). Thus, we get the following optimization problem:

\[*{argmax}_{f:\,*{density}\,*{on}\, }(f), s.t.,_{x f}\,(x,v). \]

This optimization problem is well-studied and it is known that by using different loss functions, we can derive many families of densities . For instance, for \((x,v)(x-v)^{2}\), we recover the Gaussian density with mean \(v\), and for \((x,v)(x/v)\), we obtain a Pareto density.

**Step 2: Incorporating the resource-information parameter.** We now extend the above formulation to include information constraints in the evaluation process. In an evaluation process, both the evaluator and the individual spend resources such as time, cognitive effort, or money to communicate the information related to the utility of the individual to the evaluator. For instance, in interviews, both the interviewer and the interviewee spend time and cognitive effort. In university admissions, the university admissions office needs to spend money to hire and train application readers who, in turn, screen applications for the university's admission program, and the applicants need to spend time, cognitive effort, and money to prepare and submit their applications . The more resources are spent in an evaluation process, the more additional information about \(v\) is acquired. We model this using a resource-information parameter \(\), which puts a lower bound on the entropy of \(f\). Thus, we modify the optimization problem in Equation (2) in the following manner. We first flip the optimization problem to an equivalent problem where we minimize the expected loss subject to a lower bound on the entropy, \((f)\), of \(f\). A higher value of the resource-information parameter \(\) means that one needs to spend more resources to obtain the same information and corresponds to a stronger lower bound on \((f)\) (and vice-versa) in our framework.

\[*{argmin}_{f:\,*{density}\,*{on}\, }\ _{x f}\,(x,v), s.t.,(f). \]

When \(-\), the optimal density tends to a point or delta density around \(v\) (recovering the most information), and when \(\), it tends to a uniform density on \(\) (learning nothing about \(v\)). Since differential entropy can vary from negative to positive infinity, the value of \(\) is to be viewed relative to an arbitrary reference point. \(\) may vary with the socially-salient group of the individual in real-world contexts. For instance, in settings where the evaluator needs to interact with individuals (e.g., interviews), disparities can arise because it is less cognitively demanding for an evaluator to communicate with individuals who speak the same language as themselves, compared to individuals who speak a different language . In settings where the evaluator assesses individuals based on data about their past education and employment (e.g., at screening stages of hiring or in university admissions), disparities can arise because the evaluator is more knowledgeable about a specific group's sociocultural background compared to others, and would have to spend more resources to gather the required information for the other groups .

**Step 3: Incorporating the risk-averseness parameter.** We now introduce the parameter \(\) that captures risk averseness. Roughly speaking, risk averseness may arise in an evaluation process because of the downstream impact of the output. The evaluator may also benefit or may be held accountable for the estimated value, and hence, would be eager or reluctant to assign values much higher than the true utility . Further, the individual may also be risk averse, e.g. during a hiring interview, the risk of getting rejected may prompt the individual to quote less than the expected salary . To formalize this intuition, for a given \(\), we define a risk-averse loss function \(_{}:\) that incorporates the parameter \( 1\) in \(\) as follows:

\[_{}(x,v)(x,v)\ x v_{}(x,v)(x,v)\ x<v. \]

Not only does this loss function penalize overestimation versus underestimation, but in addition, the more the overestimation, the more the penalization is. This intuition is consistent with the theory of risk averseness . Our choice is related to the notion of hyperbolic absolute risk aversion , and one may pick other ways to incorporate risk averseness in the loss function . As an example, if \((x,v)=(x-v)^{2}\), then the \(_{2}^{2}\)-loss is \(_{}(x,v)=(x-v)^{2}\), if \(x v\) and \((x-v)^{2}\) otherwise. If \((x,v)=(x/v)\), then the \(\)-ratio loss is \(_{}(x,v)=(x/v)\), if \(x v\); \((x/v)\) otherwise. Plots of these two loss functions are in Figure 2.1 Thus, the analog of Equation (3) becomes

We note that, for the risk-averse loss function defined in (4), the following hold: For all \(^{} 1\), \(_{}(x,v)_{^{}}(x,v)\) for all \(x,v\), and \(_{}(x,v)-_{^{}}(x,v)\) is an increasing function of \(x\) for \(x v\). Beyond (4), one could consider other \(_{}(x,v)\) satisfying these two properties in our framework; we omit the details. One can also incorporate the (opposite) notion of "risk eager," where values of \(x\) lower than \(v\) are penalized more as opposed to values of \(x\) higher than \(v\) by letting \((0,1]\).

**Step 4: Generalizing to arbitrary \(f_{}\).** To extend Equation (5) to the setting when \(v\) comes from a general density \(f_{}\), we replace the loss function by its expectation over \(f_{}\) and arrive at the model for the evaluation process that we propose in this paper:

\[_{f:}& _{,}(f_{},f)_{v }[_{x}_{}(x,v)f(x)d(x)]f_{ }(v)d(v),\\ &&-_{x}f(x) f(x)d(x).\]

For a given \(\) and parameters \(\) and \(\), this optimization framework can be viewed as transforming the true utility density \(f_{}\) of a group of individuals to the density \(f_{}\) (the solution to this optimization problem). It is worth pointing out that neither the evaluator nor the individual is solving the above optimization problem - rather (OptProg) models the evaluation process and the loss function \(\), \(\), and \(\) depend on the socially-salient attribute of the group of an individual; see also Appendix B.

## 3 Theoretical results

**Characterization of the optimal solution.** We first characterize the solution of the optimization problem (OptProg) in terms of \(f_{}\), \(_{}\), \(\), and \(\). Given a probability density \(f_{}\), a parameter \( 1\), and a loss function \(\), consider the function \(I_{f_{},,}(x)_{v}_{}(x,v) f_{}(v)d(v)\). This integral captures the expected loss when the estimated utility is \(x\). Further, for a density \(f\), the objective function of (OptProg) can be expressed as \(_{,}(f_{},f)_{x }I_{f_{},,}(x)f(x)d(x)\).

**Theorem 3.1** (Informal version of Theorem C.1 in Appendix C).: _Under general conditions on \(f_{}\) and \(\), for any finite \(\) and \( 1\), (OptProg) has a unique solution \(f^{}(x)(-I_{f_{},,}(x)/^{})\), where \(^{}>0\) is unique and also depends on \(\) and \(\). Further, \((f^{})=\)._

The uniqueness in Theorem 3.1 implies that, if \(=1\), \(=(f_{})\), and \((x,v) f_{}(x)- f_{}(v)\), then the optimal solution is \(f^{}(x)=f_{}(x)\). To see this, note that in this case \(I_{f_{},,}(x)= f_{}(x)+(f_{ D})\). Hence, \(f^{}(x)=f_{}(x)\) satisfies Theorem 3.1's conclusion (see Section C.7 for details). Thus, in the absence of risk averseness, and for an appropriate choice of resource-information parameter, the output density is the same as the true density.

Theorem 3.1 can be viewed as a significant extension of results that show how well-known probability distributions arise as solutions to the entropy-maximization framework. Indeed, the standard maximum-entropy formulation only considers the setting where the input utility is given by a single value, i.e., the distribution corresponding to \(f_{}\) is concentrated at a single point; and the risk-averseness parameter \(=1\). While the optimal solution to the maximum-entropy framework (2) restricted to the class of well-known loss functions, e.g. \(_{2}^{2}\)-loss or linear loss, can be understood by using standard tools from convex optimization (see ), characterizing the optimal solution to the general formulation (OptProg) is more challenging because of several reasons: (i) The input density \(f_{}\) need not be concentrated at a single point, and hence one needs to understand conditions on \(f_{}\) when the formulation has a unique optimal solution. (ii) The loss function can be arbitrary and one needs to formulate suitable conditions on the loss function such that (OptProg) has a unique optimal solution. (iii) The risk-averseness parameter \(\) makes the loss function asymmetric (aroundany fixed value \(v\)) and makes the analysis of the error in the objective function non-trivial. Roughly speaking, the only restrictions, other than standard integrability assumptions, that we need on the input are: (a) Monotonicity of the loss function \((x,v)\) with respect to either \(x\) or \(v\), (b) the growth rate of the loss function \((x,v)\) is at least logarithmic, and (c) the function \(I_{f_{},,}(x)\) has a unique global minimum, which is a much weaker assumption than convexity of the function. Note that for the \(_{2}^{2}\)-loss function given by \((x,v)=(x-v)^{2}\), the first two conditions hold trivially; and when the input density \(f_{}\) is Gaussian, it is not hard to show that \(I_{f_{},,}(x)\) is strongly convex and hence the third condition mentioned about holds (see Appendix H for details). These conditions are formally stated in Appendix C.1 and we show that they hold for the cases of Gaussian, Pareto, Exponential, and Laplace densities in Sections H, I, J, K respectively.

The proof of Theorem 3.1 is presented in Appendix C and the following are the key steps in it: (i) The proof starts by considering the dual of (OptProg) and shows that strong duality holds (see Appendix C.2 and Appendix C.3). (ii) The next step is to show that the optimal solution \(f^{}\) of (OptProg) exists and is unique. This requires proving that the dual variable \(^{}\) (corresponding to the entropy constraint in (OptProg)) is positive - while this variable is always non-negative, the main technical challenge is to show that it is _non-zero_. In fact, there are instances of (OptProg) where \(^{}\) is zero, and an optimal solution does not exist (or an optimal solution exists, but is not unique). (iii) The proof of \(^{} 0\) requires us to understand the properties of the integral \(I_{f_{},,}(x)\) (abbreviated as \(I(x)\) when the parameters \(f_{},,\) are clear from the context). In Appendix C.4 we show that \(I(x)\) can be expressed as a sum of two monotone functions (see Theorem C.10). This decomposition allows us to show that the optimal value of (OptProg) is finite. (iv) In Appendix C.5, we show that the optimal value of (OptProg) is strictly larger than \(I(x^{})\) (Lemma C.13, Lemma C.14), where \(x^{}\) is the minimizer of \(I(x)\). This requires us to understand the interplay between the growth rate of the expected loss function and the entropy of a density as we place probability mass away from \(x^{}\). Indeed, these technical results do not hold true if the loss function \((x,v)\) grows very slowly (as a function of \(x/v\) or \((|x-v|)\). (v) Finally, in Theorem C.15, we show that \(^{}\) is nonzero. This follows from the fact that if \(^{}=0\), then the optimal value of (OptProg) is equal to \(I(x^{})\), which contradicts the claim in (iv) above. Once we show \(^{}>0\), the expression for the (unique) optimal solution, i.e., \(f^{}(x)(-I(x)/^{})\), follows from Theorem C.16.

We conclude this section with two remarks. 1) In Appendix E, we show that Theorem 3.1 implies that \(=(_{,}(f_{},f^{})/ ^{})+ Z^{}\), where \(Z^{}_{}(-I_{f_{},,}(x)/ ^{})d(x)\) is the partition function or the normalizing constant that makes \(f^{}\) a probability density; This equation is an analog of the Gibbs equation in statistical physics and gives a physical interpretation of \(I_{f_{},,}(x)\) and \(^{}\): \(^{}\) is the _temperature_ and \(I_{f_{},,}(x)\) is the _energy_ corresponding to _state_\(x\). This may be useful in understanding the effects of different parameters on the output density. 2) If one wishes, one can use (OptProg) to understand the setting where a single individual is being evaluated by setting the input density \(f_{}\) to be concentrated at their true utility \(v\). For instance, if we set the loss function to be the \(_{2}^{2}\)-loss, using Theorem 3.1, one can show that for any given values of the parameters, \(\) and \(\) and loss function being \(_{2}^{2}\)-loss, the mean of the output density is \(v-}{}}-1}{}\); see Appendix D for a proof. Therefore for \(>1\), the mean of the output density is strictly less than \(u\). This gives a mapping from the "true ability" to the (mean of the) "biased ability" in this case. This mapping can be used to understand how the parameters \(\) and \(\) in the evaluation process transform the true ability.

**Effect of varying \(\) for a fixed \(\).** We first study the effect of changing the resource-information parameter \(\) on \(f^{}\) for a fixed value of the risk-averseness parameter \( 1\). To highlight this dependency on \(\), here we use the notation \(f^{}_{}\) to denote the optimal solution \(f^{}\). We start by noting that as \(^{}\) increases, the optimal density becomes close to uniform, and as it goes towards zero, the optimal density concentrates around a point \(x^{}\) that minimizes energy: \(*{argmin}_{x}I_{f_{},,}(x)\). Note that the point \(x^{}\) does not depend on \(\). However, \(^{}\) may depend in a complicated manner on both \(\) and \(\), and it is not apparent what effect changing \(\) has on \(^{}\). We show that, for any fixed \( 1\), as \(\) decreases, the output density gets concentrated around \(x^{}\) (see Theorem F.1). This confirms the intuition that as we reduce \(\) by adding more resources in the evaluation process, the uncertainty in the output density should be reduced. Similarly, if \(\) increases because of a reduction in the resources invested in the evaluation, the uncertainty in \(f^{}_{}\) should increase, and hence, the output density should converge towards a uniform density. For specific densities, one can obtain sharper results. Consider the case when \(f_{}\) is a Gaussian with mean \(m\) and variance \(^{2}\), and \(_{}(x,v)(x-v)^{2}\) if \(x v\), and \((x-v)^{2}\) if \(x<v\). The uncertainty in the Gaussian density is captured by the variance, and hence, we expect the output density to have a higher variance when the parameter \(\) is increased. Indeed,when \(=1\), we show that the optimal density \(f^{}_{}\) is a Gaussian with mean \(m\) and variance \(e^{2}^{2}\); see Appendix H. Thus, if one increases \(\) from \(-\) to \(\), the variance of the output density changes _monotonically_ from \(0\) to \(\). When \( 1\), numerically, it can be seen that for any fixed \( 1\), increasing \(\) increases the variance, and also decreases the mean of \(f^{}_{}\); see Figure 3. Intuitively, the decrease in mean occurs because higher variance increases the probability of the estimated value being much larger than the mean, and the risk-averness parameter imposes a high penalty when the estimated value is larger than the true value. In fact, we show in Theorem F.5 that the variance of \(f^{}_{}\) for any continuous input density \(f_{}\) supported on \(\) is at least \(e^{2-1}\). This follows from the well-known fact that among all probability densities supported on \(\) with variance \(^{2}\), the Gaussian density with variance \(^{2}\) maximizes the (differential) entropy ; see Section F.2 for details. In a similar vein, we show that the mean of \(f^{}_{}\) is at least \(e^{-1}\) when the input density \(f_{}\) is supported on \([0,)\), and hence approaches \(\) as \(\) goes to \(\) (see Theorem F.7). This result relies on the fact that among all densities supported on \([0,)\) and with a fixed expectation \(1/\) (for \(>0\)), the one maximizing the entropy is the exponential density with parameter \(\).

We now consider the special setting when the input density \(f_{}\) is Pareto with parameter \(>0\) (\(f_{}(x) x^{--1}\) for \(x[1,)\)), and \(_{}(x,v)(x/v)\) if \(x v\), and \((x/v)\) if \(x<v\). When \(=1,\) we show that the optimal density \(f^{}_{}\) is also a Pareto density with parameter \(^{}\) satisfying the following condition: \(1+(1/^{})-^{}=.\) Using this, it can be shown that, for \(=1\), both the mean and variance of \(f^{}_{}\)_monotonically_ increase to \(\) as \(\) goes to \(\); see Appendix I. The increase in variance reflects the fact that increasing \(\) increases the uncertainty in the evaluation process. Unlike the Gaussian case, where the mean of \(f^{}_{}\) could shift to the left of \(0\) with an increase in \(\), the mean of the output density in this setting is constrained to be at least \(1\), and hence, increasing the variance of \(f^{}_{}\) also results in an increase in its mean. Numerically, for any fixed \( 1\), increasing \(\) increases both the mean and variance of \(f^{}_{}\); see Figures 3 and 6.

**Effect of varying \(\) for a fixed \(\).** Let \(f^{}_{}\) denote the optimal solution \(f^{}\) with \(\) for a fixed \(\). We observe that, for any fixed \(x\), \(I_{f_{},,}(x)\), which is the expected loss when the output is \(x\), and \(_{,}(f^{}_{},f_{})\) are increasing functions of \(\); see Appendix G. Thus, intuitively, the mass of the density should shift towards the minimizer of \(I_{f_{},,}(x)\). Moreover, the minimizer of \(I_{f_{},,}(x)\) itself should reduce with increasing \(\). Indeed, as the evaluation becomes more risk averse, the expected loss, \(I_{f_{},,}(x)\), for an estimated value \(x\), increases. However, the asymmetry of the loss function leads to a more rapid rate of increase in \(I_{f_{},,}(x)\) for larger values of \(x\). As a result, minimizer of \(I_{f_{},,}(x)\) decreases with increasing \(\). Thus, as we increase \(\), the output densities should _shrink_ and/or _shift_ towards the left. We verify this for Pareto and Gaussian densities numerically: for fixed \(\), increasing \(\) decreases the mean \(f^{}_{}\) for both Pareto and Gaussian \(f_{}\); see Figures 4 and 7. As for the variance, with fixed \(\), increasing \(\) increases the variance when \(f_{}\) is Gaussian and decreases the variance when \(f_{}\) is Pareto; see Figures 4 and 7. See also discussions in Appendices H and I.

**Connection to the implicit variance model.** Our results confirm that increasing \(\) effectively increases the "noise" in the estimated density by moving it closer to the uniform density. The implicit variance model of  also captures this phenomenon. More concretely, in their model, the observed utility of the advantaged group is a Gaussian random variable with mean \(\) and variance \(_{0}^{2}\), and the observed utility of the disadvantaged group is a Gaussian random variable with mean \(\) and variance \(_{0}^{2}+^{2}\). This model can be derived from our framework where the input density \(f_{}\) is Gaussian, the risk-averness parameter \(=1\), the loss function is \((x,v)=(x-v)^{2}\) and the disadvantaged group is associated with a higher value of the resource-information parameter \(\); see Section H for details.

**Connection to the multiplicative bias model.** In the multiplicative-bias model of , the true utility of both the groups is drawn from a Pareto distribution, and the output utility for the disadvantaged group is obtained by scaling down the true utility by a factor \(>1\). This changes the domain of the distribution to \([1/,)\) from \([1,)\) and, hence, does not fit exactly in our model which does not allow for a change in the domain. Nevertheless, we argue that when the input density \(f_{}\) is Pareto with a parameter \(\), \(=(f_{})\) and \(>1\), and the loss function is given by \((x,v)= x- v\), then output density \(f^{}_{}\) has a smaller mean than that of the input density. As we increase \(\), the evaluation becomes more risk-averse and hence decreases the probability of estimating higher utility values. Hence, the mean of the output density decreases. We first show that for any fixed \(\), as the parameter \(\) increases, the output density converges to a density \(g^{}\). We then show numerically that, for all the Pareto distributions considered by our study, the mean of the density \(g^{}\) is less than that of the output density \(f^{}_{}\) when \(=1\). We give details of this argument in Section I.

## 4 Empirical results

**Ability to capture biases in data.** First, we evaluate our model's ability to output densities that are "close" to the densities of biased utility in one synthetic and two real-world datasets.

_Setup and discussion._ In all datasets we consider, there are natural notions of utility for individuals: scores in college admissions, number of citations in research, and degree in (social) networks. In each dataset, we fix a pair of groups \(G_{1}\) and \(G_{2}\) (defined by protected attributes such as age, gender, and race) and consider the empirical density of utilities \(f_{G_{1}}\) and \(f_{G_{2}}\) for the two groups. Suppose group \(G_{1}\) is more privileged or advantaged than \(G_{2}\). In all datasets, we observe notable differences between \(f_{G_{1}}\) and \(f_{G_{2}}\) that advantaged \(G_{1}\) (e.g., \(f_{G_{1}}\)'s mean is at least 34% higher than \(f_{G_{2}}\)'s).

_Implementation details._ Our goal is to understand whether our model can "capture" the biases or differences between \(f_{G_{1}}\) and \(f_{G_{2}}\). To evaluate this, we fix \(f_{}=f_{G_{1}}\), i.e., \(f_{G_{1}}\) is the true density, and compute the minimum total variation distance between \(f_{G_{2}}\) and a density output by our model, i.e., \(_{,}d_{}(f_{,}^{},f_{G_{2}})\); where \(f_{,}^{}\) is the solution to (OptProg) with inputs \(\) and \(\). (The total variation distance between two densities \(f\) and \(g\) over \(\) is \(_{}|f(x)-g(x)|\,d(x)\).) To illustrate the importance of both \(\) and \(\), we also report the TV-distances achieved with \(=1\) (no skew) and with \(=_{0}(f_{})\) (vacuous constraint), i.e., \(_{}d_{}(f_{1,}^{},f_{G_{2}})\) and \(_{}d_{}(f_{,_{0}}^{},f_{G_{2}})\), respectively. As a further comparison, we also report the minimum TV distances achieved by existing models of biases in evaluation processes: the multiplicative-bias model and the implicit variance model . Concretely, we report \(_{,}d_{}(f_{,,},f_{G_{2}})\) and \(_{,}d_{}(f_{,,},f_{G_{2}})\) where \(f_{,,}\) is the density of \( v+\) for \(v f_{}\) and \(f_{,,}\) is the density of \(v++\) where \(v f_{}\) and \((0,1)\).

Below we present brief descriptions of the datasets; detailed descriptions and additional implementation appear in Appendix L.

_Dataset 1 (JEE-2009 scores)._ Indian Institutes of Technology (IITs) are, arguably, the most prestigious engineering universities in India. Admission into IITs is decided based on students' performance in the yearly Joint Entrance Exam (JEE) . This dataset contains the scores, birth category (official SES label ), and (binary) gender of all students from JEE-2009 (384,977 total) . We consider the score as the utility and run two simulations with birth category (\(G_{1}\) denotes students in GEN category) and gender (\(G_{1}\) denotes male students) respectively as the protected attributes. We set \(\) as the discrete set of possible scores. We fix \(_{2}^{2}\)-loss as \(f_{G_{1}}\) and \(f_{G_{2}}\) appear to be Gaussian-like (unimodal with both a left-tail and a right-tale; see Figure 13 in Appendix L).

_Dataset 2 (Semantic Scholar Open Research Corpus)._ This dataset contains the list of authors, the year of publication, and the number of citations for 46,947,044 research papers on Semantic Scholar. We consider the total first-author citations of an author as their utility and consider their gender (predicted from first name) as the protected attribute (\(G_{1}\) denotes male authors). We fix \(=\{1,2,\}\) and \(\) as the \(\)-ratio loss as \(f_{G_{1}}\) and \(f_{G_{2}}\) have Pareto-like density.

_Dataset 3 (Synthetic network data)._ We generate a synthetic network with a biased variant of the Barabasi-Albert model . The vertices are divided into two groups \(G_{1}\) and \(G_{2}\). We start with a random graph \(G_{0}\) with \(m{=}50\) vertices where each vertex is in \(G_{1}\) w.p. \(\) independently. We extend \(G_{0}\) to \(n{=}10,000\) vertices iteratively: at each iteration, one vertex \(u\) arrives, \(u\) joins \(G_{1}\) w.p. \(\) and otherwise \(G_{2}\), and \(u\) forms one edge with an existing vertex \(v\)-where \(v\) is chosen w.p. \( d_{v}\) if \(v G_{1}\) (\(d_{v}\) is \(v\)'s current degree) and \(d_{v}\) otherwise. We use a vertex's degree as its utility, fix \(=\{1,2,...\}\), and use \(\)-ratio loss as \(f_{G_{1}}\) and \(f_{G_{2}}\) have Pareto-like density.

  
**Dataset** & &  &  &  \\  &  & 1\)} & (f_{})\)} & bias  & variance  \\  JEE-2009 (Birth category) & **0.09** & 0.15 & 0.21 & 0.14 & 0.10 \\ JEE-2009 (Gender) & **0.07** & 0.15 & 0.19 & **0.07** & 0.08 \\ Semantic Scholar (Gender) & **0.03** & 0.09 & 0.23 & 0.08 & 0.13 \\ Synthetic Network & **0.03** & 0.05 & 10.0 & 0.05 & 0.22 \\   

Table 1: _TV distances between best-fit densities and real data (Section 4) with 80%-20% training and testing data split: Each dataset consists of two densities \(f_{G_{1}}\) and \(f_{G_{2}}\) of utility, corresponding to the advantaged and disadvantaged groups. We fix \(f_{}{=}f_{G_{1}}\) and report the best-fit TV distance between \(f_{G_{2}}\) and densities output by (a) our model, (b) the multiplicative bias model, and (c) the implicit variance model. We compare our model to variants where we fix \(1\) and \((f_{})\). Our model achieves a small TV distance on all datasets.__Observations._ We report the TV distances for all simulations in Table 1 (also see Figure 11 and Figure 12) for the plots of the corresponding best-fit densities). We observe that across all simulations our model can output densities that are close in TV distance (\( 0.09\)) to \(f_{G_{2}}\). Moreover, both \(\) and \(\) parameters are important, and dropping either can increase the TV distance significantly (e.g., by 1.5 times on JEE-2009 data with birth category and 2.66 times on the Semantic Scholar data). Compared to the implicit variance model, our model has a better fit on the JEE-2009 (Birth category), Semantic Scholar, and Synthetic Network data because the implicit variance model does not capture skew and in these datasets \(f_{G_{1}}\) is a skewed version of \(f_{G_{2}}\). Compared to the multiplicative bias model, our model has a better fit on the JEE-2009 (Birth category), as here the utilities have Gaussian-like distributions due to which multiplicative bias largely has a translation effect. Finally, on the JEE-2009 (Gender) data, our model's performance is similar to multiplicative bias and implicit variance models because in this data \(f_{G_{1}}\) and \(f_{G_{2}}\) are similar (TV distance\(\)\(0.08\))

**Effect of interventions on selection.** Next, we illustrate the use of our model to study the effectiveness of different bias-mitigating interventions in downstream selection tasks (e.g., university admissions, hiring, and recommendation systems).

_Subset selection tasks._ There are various types of selection tasks . We consider the simplest instantiation where there are \(n\) items, each item \(i\) has a true utility \(v_{i} 0\), and the goal is to select a size-\(k\) subset \(S_{}\) maximizing \(_{i S}v_{i}\). If \(V\)\(=\)\((v_{1},,v_{n})\) is known, then this problem is straightforward: select \(k\) items with the highest utility. However, typically \(V\) is unknown and is estimated via a (human or algorithmic) evaluation process that outputs a possibly skewed/noisy estimate \(X\)\(=\)\((x_{1},,x_{n})\) of \(V\). Hence, the outputs is \(S_{}\)\(\)\(*{argmax}_{|S|=k}_{i S}x_{i}\), which may be very different from \(S_{}\) and possible has a much lower true utility: \(_{i S_{}}v_{i}_{i S_{}}v_{i}\).

_Interventions to mitigate bias._ Several interventions have been proposed to counter the adverse effects of bias in selection, including, representational constraints, structured interviews, and interviewer training. Each of these interventions tackles a different dimension of the selection task. Representational constraints require the selection to include at least a specified number of individuals from unprivileged groups . Structured interviews reduce the scope of unintended skews by requiring all interviewees to receive the same (type of) questions . Interviewer training aims to improve efficiency: the amount of (accurate) information the interviewer can acquire in a given time . Which intervention should a policymaker enforce?

_Studying the effectiveness of interventions._ A recent and growing line of work  evaluates the effectiveness of representational constraints under specific models of bias: they ask, given \(\) of subsets satisfying some constraint, when does the constraint optimal set, \(S_{,}*{argmax}_{S} _{i S}x_{i}\), have a higher utility than the unconstrained optimal \(S_{}\), i.e., when is \(_{i S_{,}}v_{i}>_{i S_{}}v_{i}\)? Based on their analysis  demonstrate the benefits of different constraints including, equal representation (ER), which requires the output \(S\) to satisfy \(|S G_{1}|=|S G_{2}|\) and, proportional representation (PR), which requires \(S\) to satisfy \(|S G_{1}|/|G_{1}|=|S G_{2}|/|G_{2}|\). A feature of our model is that its parameters \(\) and \(\) have a physical interpretation, which enables the study of other interventions: for instance, structured interviews aim to reduce skew in evaluation, which corresponds to shifting \(\) closer to 1, and interviewer-training affects the information-to-resource trade-off, i.e., reduces \(\).

Using our model we compare ER and PR with two new interventions: change \(\) by 50% (\(\)-intervention) and change \(\) by 50% (\(\)-intervention). Here, 50% is an arbitrary amount for illustration.

_Setup._ We consider a selection scenario based on the JEE 2009 data: we fix \(f_{}=f_{G_{1}}\), \(^{},^{}\) to be the best-fit parameters on the JEE 2009 data (by TV distance), and \(|G_{1}|=1000\). Let \(f_{}\) and \(f_{}\) be the densities obtained after applying the \(alpha\) intervention and the \(tau\) intervention respectively. (Formally, \(f_{}\)\(=\)\(f_{^{}/2,^{}}^{}\) and \(f_{}\)\(=\)\(f_{^{},3^{}/2}\).) We vary \(|G_{2}|\{500,1000,1500\}\). For each \(i G_{1}\), we draw \(v_{i} f_{}\) and set \(x_{i}=v_{i}\) (no bias). For each \(i G_{2}\), we draw \(v_{i} f_{}\), \(x_{i} f_{}^{},x_{i}^{} f_{}\), and \(x_{i}^{} f_{}\) coupled so that the CDFs of the respective densities at \(v_{i}\), \(x_{i}\), \(x_{i}^{}\), and \(x_{i}^{}\) are the same. We give ER and PR the utilities \(\{x_{i}\}\) as input, we give \(\)-intervention utilities \(\{x_{i}^{}\}\) as input, and the \(\)-intervention utilities \(\{x_{i}^{}\}_{i}\). For each \(|G_{1}|\) and \(|G_{2}|\), we vary \(50 k 1000\), sample utilities and report the expected utilities of the subset output by each intervention over 100 iterations (Figure 1). Here, \(1000\) is the largest value for which ER is satisfiable across all group sizes \(|G_{1}|\) and \(|G_{2}|\).

_Observations._ Our main observation is that there is no pair of interventions such that one always achieves a higher utility than the others. In fact, for each intervention, there is a value of \(k\), \(|G_{1}|\), and \(|G_{2}|\), such that the subset output with this intervention has a higher utility than the subsets output with other interventions. Thus, each intervention has a very different effect on the latent utility of the selection and a policymaker can use our model to study the effects in order to systematically decide which interventions to enforce; see Appendix L.1 for a case study of how a policymaker could potentially use this model to study bias-mitigating interventions in the JEE context.

## 5 Conclusion, limitations, and future work

We present a new optimization-based approach to modeling bias in evaluation processes ((OptProg)). Our model has two parameters, risk averseness \(\) and resource-information trade-off \(\), which are well documented to lead to evaluation biases in a number of contexts. We show that it can generate rich classes of output densities (Theorem 3.1) and discuss how the output densities depend on the two parameters (Section 3). Empirically, we demonstrate that the densities arising from our model have a good fit with the densities of biased evaluations in multiple real-world datasets and a synthetic dataset; often, leading to a better fit than models of prior works [90; 61] (Table 1). We use our model as a tool to evaluate different types of bias-mitigating interventions in a downstream selection task-illustrating how this model could be used by policymakers to explore available interventions (Figure 1 and Appendix L.1); see also Appendix B. Our work relies on the assumptions in prior works that there are no differences (at a population level) between \(G_{1}\) and \(G_{2}\); see, e.g., [89; 61; 40]. If this premise is false, then the effectiveness of interventions can be either underestimated or overestimated which may lead a policymaker to select a suboptimal intervention. That said, if all the considered interventions reduce risk aversion and/or resource constraints, then the chosen intervention should still have a positive impact on the disadvantaged group. Our model can be easily used to study multiple socially-salient groups by considering a group-specific risk-aversion parameter and a group-specific information constraint. For example, if two groups \(G_{1},G_{2}\) overlap, then we can consider three disjoint subgroups \(G_{1} G_{2}\), \(G_{1} G_{2}\) and \(G_{2} G_{1}\). Our model of evaluation processes considers scenarios where candidates are evaluated along a single dimension. It can also be applied - in a dimension-by-dimension fashion - to scenarios where individuals are evaluated along multiple dimensions, but the evaluation in any dimension is independent of the evaluation in other dimensions. Modeling evaluation processes involving multiple correlated dimensions is an interesting direction. While we illustrate the use of our model in a downstream selection task, utilities generated from biased evaluation processes are also used in other decision-making tasks (such as regression and clustering), and studying the downstream impact of evaluation biases on them is an important direction. Moreover, the output of or model can be used by policymakers to assess the impact of interventions in the supermodular set aggregation setting, where the utility of the selected group is more than the sum of the individuals. Our model cannot be directly used to understand the effect of interventions in the long term. Additional work would be required to do so, perhaps as in , and would be an important direction for future work. Finally, any work on debiasing could be used adversarially to achieve the opposite goal. We need third-party evaluators, legal protections, and available recourse for affected parties - crucial components of any system - though beyond the scope of this work.

Figure 1: _Effectiveness of different interventions on the selection-utility–as estimated by our model: The \(x\)-axis shows \(k\) (size of selection) and the \(y\)-axis shows the ratio of the (true) utility of the subset output with an intervention to the (true) utility of the subset output without any intervention. The main observation across the figures is that, for each intervention, there is a choice of \(k\), and group sizes \(|G_{1}|\) and \(|G_{2}|\), where the intervention outperforms all other interventions. Hence, each intervention has a different effect on the latent utility of selection and a policymaker can use our model to study their effect and decide which intervention to enforce. Error bars represent the standard error of the mean over 100 repetitions._

Acknowledgments.This project is supported in part by NSF Awards CCF-2112665 and IIS-2045951.