# IF-Font: Ideographic Description Sequence-Following Font Generation

Xinping Chen, Xiao Ke, Wenzhong Guo

Fujian Provincial Key Laboratory of Networking Computing

and Intelligent Information Processing,

College of Computer and Data Science, Fuzhou University, Fuzhou 350116, China

Engineering Research Center of Big Data Intelligence,

Ministry of Education, Fuzhou 350116, China

{221027017, kex, guowenzhong}@fzu.edu.cn

Corresponding author

###### Abstract

Few-shot font generation (FFG) aims to learn the target style from a limited number of reference glyphs and generate the remaining glyphs in the target font. Previous works focus on disentangling the content and style features of glyphs, combining the content features of the source glyph with the style features of the reference glyph to generate new glyphs. However, the disentanglement is challenging due to the complexity of glyphs, often resulting in glyphs that are influenced by the style of the source glyph and prone to artifacts. We propose IF-Font, a novel paradigm which incorporates Ideographic Description Sequence (IDS) instead of the source glyph to control the semantics of generated glyphs. To achieve this, we quantize the reference glyphs into tokens, and model the token distribution of target glyphs using corresponding IDS and reference tokens. The proposed method excels in synthesizing glyphs with neat and correct strokes, and enables the creation of new glyphs based on provided IDS. Extensive experiments demonstrate that our method greatly outperforms state-of-the-art methods in both one-shot and few-shot settings, particularly when the target styles differ significantly from the training font styles. The code is available at [https://github.com/Stareven233/IF-Font](https://github.com/Stareven233/IF-Font).

Figure 1: Comparison of two font generation paradigms. **Left:** The style-content disentangling paradigm. It assumes that a glyph can be decomposed into two distinct attributes: content and style. **Right:** The proposed paradigm. We first autoregressively predict the target tokens and decode them with a VQ decoder. Orange boxes show the main difference between the two paradigms.

Introduction

At the heart of font generation lies the extraction of styles from some reference glyphs of a certain font, and generate the remaining glyphs of this font. Some languages, such as Chinese, Japanese, and Korean, have a large number of characters and intricate glyph structures. Font generation can significantly reduce the labor intensity of font designers and support tasks like handwriting imitation, ancient book restoration, and internationalization of film and television productions.

EMD  and SA-VAE  are based on the belief that the target glyph can be generated by integrating the content features of the source glyph with the style features of the reference glyph, as illustrated in Fig. 1 (left). The majority of subsequent works [53; 38; 47; 25; 50] continues this paradigm, but this makes font generation a sub-task of image-to-image translation, where the source glyph is morphed to match the style of the reference glyphs, rather than being truly "generated". Due to the complex structure of glyphs, achieving a distinct boundary between style and content features requires considerable effort. Consequently, glyphs produced through the disentangling strategy typically maintain similar stroke thickness to the reference glyph but align more closely with the content font regarding spatial structure, size, and inclination.

To this end, DG-Font  incorporates deformable convolution. Diff-Font  integrate diffusion process to improve the network's learning capabilities. CF-Font  proposes content fusion. Additionally, several approaches [38; 47; 25; 30] combine fine-grained prior information such as strokes and components, to further enhance generation quality. These methods essentially follow the content-style disentanglement paradigm, in scenarios where the content font differs substantially from the target font, the resulting glyphs are susceptible to artifacts such as missing strokes, blur, and smudge.

We abandon the source glyphs in favor of Ideographic Description Sequence (IDS) to convey content information. It is based on a simple fact: without disentangling features, there is no risk of incomplete disentanglement. Consequently, font generation is reframed as sequence prediction task, where the objective is to generate the tokens of the target glyph based on the given IDS and reference glyphs. This approach mitigates the impact of source glyphs on the outputs and diminishes artifacts by leveraging the prior knowledge embedded in the quantized codebook. The users are allowed to formulate IDSs to create non-existing Chinese characters, such as kokuji2 (Japanese-invented Chinese characters), provided that the corresponding structures and components have been learned during training. This endows the model with certain cross-linguistic capabilities. We refer to this method as Ideographic Description Sequence-**F**ollowing **Font** Generation, or IF-Font. In summary, the key contributions of this paper are as follows:

* We propose IF-Font, which abandons the previous content-style disentanglement paradigm and generates glyphs through next-token prediction.
* We devise a novel IDS Hierarchical Analysis (IHA) module that analyzes the spatial structures and components of Chinese characters. It allows our decoder flexibly control the generated content with the encoded semantic features.
* Leveraging corresponding IDSs, we design the Structure-Style Aggregation (SSA) block to extract and efficiently aggregate the style features of reference glyphs.

## 2 Related Works

Image-to-image translationImage-to-image translation (I2I) aims to learn a mapping from a source domain to a target domain, requiring the transformation of images in the source domain into those in the reference style's target domain while preserving their content. PixPix  is the first I2I method that trains GANs  using paired data. CycleGAN  achieves unsupervised training through cycle consistency loss, although it is limited to transformations between two domains. UNIT  enforces the latent codes of images from two distinct domains to be identical, while employing separate generators for images in each domain. This process embodies the concept of disentanglement. MUNIT  further refines UNIT's latent code into content and style codes. Multimodal image translation can be achieved by combining the content code with different style codes.

While applying an image-to-image translation framework for font generation is currently the mainstream approach, we believe this to be inappropriate. Unlike ordinary images, the boundaries between content and style in glyphs are ambiguous. For example, although handwriting will certainly differ when the same characters are written by different writers, their semantic meanings remain unchanged. Given that glyph features are challenging to disentangle, we utilize style-neutral IDSs to determine characters, thus avoiding any influence on the styles of results due to insufficient disentanglement of content glyphs.

Few-shot font generationFew-shot learning  represents the prevailing research focus in font generation, aiming to simulate the target style with just a handful of reference glyphs. Font generation methods fall into two categories based on their utilization of implicit structural information within glyphs. Methods treating glyphs as general images possess broader applicability, enabling generation across various languages. Conversely, methods leveraging structural information typically yield higher quality outputs but are confined to specific language.

Among the methods that do not incorporate structural information, EMD  stands out as the earliest attempt to disentangle glyphs into content and style features. DG-Font  employs deformable convolution  to capture the glyph deformations. FontRL  uses reinforcement learning  to draw the skeleton of Chinese characters. FontDiffuser  models the font generation task as a noise-to-denoise paradigm. Shamir et al.  explores a parametric representation of oriental alphabets, which can elegantly balance glyph quality and compression. In vector font generation, Deepvecfont  exhaustively exploit the dual-modality information (raster images and draw-command sequences) of fonts to synthesize vector glyphs. CLIPFont  controls the desired font style through text description rather than relying on style reference images.

In terms of methods that incorporate structural information, SA-VAE  utilizes the radicals and spatial structures of Chinese characters. CalliGAN  adopts the Zi2Zi framework  and fully decompose Chinese characters into sequences of components. SC-Font  further decomposes Chinese characters into stroke granularity. DM-Font  proposes dual memory to update component features continuously. LF-Font  represents component-wise style through low-rank matrix factorization . MX-Font  automatically extracts the features through multiple localized experts. FS-Font  demands that reference glyphs include all components of the target glyph, otherwise may result in a degradation of generation quality. CG-GAN  employs GRU  and attention mechanism to predict component sequences. XMP-Font  performs multimodal pre-training on Chinese character strokes and glyphs data. Most of the above methods are constrained by the content-style disentanglement paradigm. They often neglect the presence of Ideographic Description Character (IDC) which refers to the spatial structure of Chinese characters, suffering from artifacts and inconsistent styles.

Vector quantized generative modelsVector Quantization (VQ) typically follows a two-stage training scheme. Initially, it employs a codebook to record and update vectors, converting them from a continuous feature space to a discrete latent space. Subsequently, it models the distribution of these quantized vectors with a decoder to predict tokens, which are the codebook indices, and then restores the tokens to a image.

VQ-VAE  is the first to incorporate quantization into the variational autoencoder (VAE)  framework. VQ-VAE2  performs multi-scale quantization and adopts rejection sampling  VQGAN  acquires the codebook with the help of GAN  and employs Transformer  to replace the PixelCNN  used by VQ-VAE . RQ-VAE  proposes a residual quantizer. BEiT  performs masked image modeling (MIM) on the patch view with the supervision of visual tokens. MaskGIT  directly models visual tokens and proposes parallel decoding. MAGE  is similar to MaskGIT , but introduces ViT  and contrastive learning . DQ-VAE  further encodes images with variable-length codes.

Since quantization is equivalent to tokenizing images, many methods attempt to enable multimodal generation. The dVAE proposed by DALL-E  relaxes the discrete sampling problem utilizing Gumbel-Softmax , outputing the probability distribution of codebook codes. SEED  designed a Causal Q-Former to extract image embeddings and quantize them. LQAE  trains VQ-VAE  to quantize the image into the frozen LLM codebook space directly. SPAE  introduces multi-layer and coarse-to-fine pyramid quantization and semantic guidance with CLIP. V2L  further proposes global and local quantizers. Given the absence of a pre-trained model tailored for IDS, we directly concatenate visual tokens with IDS tokens to performed autoregression.

## 3 Method

As shown in Fig. 2, given a target character \(c_{y}\), reference characters \(C_{r}=\{c_{r}^{i}\}_{i=1}^{k}\) and reference glyphs \(G_{r}=\{g_{r}^{i}\}_{i=1}^{k}\), the goal of our framework is to generate a glyph \(_{y}\) that conforms to the semantics of \(c_{y}\) and has a style consistent with \(G_{r}\). To achieve this objective, we analyze \(c_{y}\) with IHA to derive its associated IDS \(_{y}\), which is then encoded as a semantic feature \(f_{*}\). Likewise, we can obtain the IDS \(I_{r}=\{_{r}^{i}\}_{i=1}^{k}\) corresponding to \(C_{r}\). Following this, we employ the similarity module \(E_{sim}\) to assess the relationship between \(_{y}\) and \(I_{r}\). Combined with \(f_{*}\) and the output of \(E_{sim}\), the features \(_{r}\) corresponding to \(G_{r}\) are fused into the final style feature \(f_{r}\) in the SSA block. \(_{y}\) is reshaped as initial tokens \(t^{<0}\), which is fed into the decoder \(D\) along with \(f_{r}\) for autoregressive modeling. Finally, the predicted glyph tokens \(_{y}\) are decoded with the pre-trained VQGAN to obtain the generated glyph \(_{y}\).

### IDS Hierarchical Analysis

A simple alternative to using a source glyph as input is to directly employ the character itself to control the semantics of the output. However, considering the vast number of characters in Chinese, this approach proves to be impractical due to its expensive cost. Moreover, it overlooks the structural intricacies of characters.

Ideograph Description Sequence is a structural description grammar for Chinese characters defined by the Unicode Standard, which consists of description characters and basic components (mainly Chinese characters) through a prefix notation. Decomposing Chinese characters into their corresponding IDSs can notably streamline the vocabulary, allowing characters with similar structures or components to share common features.

However, a Chinese character may have multiple equivalent IDSs. Many Chinese characters have a top-bottom or left-right structure, the IDCs follows a long-tail distribution, presenting challenges for model training. Fortunately, the left-middle-right structure of Chinese characters can be equivalently represented by two left-right structures. Similarly, the top-middle-bottom structure equals two top-bottom structures. The examples can be found in Fig. 3.

Based on the above observation, we employ a IDS Hierarchical Analysis (IHA) module. Instead of rigidly querying the decomposition table when determining the IDS of a character, we examine whether the character follows a left-middle-right or top-middle-bottom structure. Subsequently, we construct multiple equivalent IDSs for the same character through random selection. To summarize, \(c_{y}\) and \(C_{r}\) are initially decomposed into \(_{y}\) and \(I_{r}\) respectively. In the IDS encoder, \(_{y}\) is padded to the maximum sequence length \(l_{I}\) and encoded into the associated semantic feature \(f_{*}^{l c}\).

Figure 2: Overview of our proposed method. The overall framework mainly consists of three parts: IDS Hierarchical Analysis module \(E_{}\), Structure-Style Aggregation block \(_{r}\), and a decoder \(\).

### Structure-Style Aggregation

Many previous methods [55; 10; 59; 25; 30; 54] overlook interactions between reference and target characters during the extraction of reference styles, resulting in a lack of relevance in the extracted features. The more closely the reference character resembles the target character, the more effortlessly the generation process can preserve the style. Ideally, employing the target glyph itself as the reference, known as self-reconstruction, should yield the most effective output. Although FS-Font  endeavors to ensure that the reference characters cover all components of the target character, its implementation hinges on predefined content-reference mapping, which may limit its adaptability. To address this issue, we propose a Structure-Style Aggregation (SSA) block, as shown in Fig. 2. We convert the reference glyph \(G_{r}\) into the latent space of VQGAN and encode it one by one into the corresponding intermediate features \(_{r}=\{_{r}^{i}^{h w c}\}_{i=1}^{k}\). The similarity module \(E_{sim}\) evaluates the resemblance between each reference IDS \(I_{r}\) and the target IDS \(_{y}\), considering whether they share identical structures or components. It produces a set of similarity weights \(Sim=\{sim^{i}^{1}\}_{i=1}^{k}\), which can guide the subsequent feature fusion process. The fusion module \(E_{fuse}\) consists of two branches: global and local style feature aggregation, as shown in Fig. 4. The global features mainly focus on the glyph layout, stroke thickness, and inclination, which can be obtained by merging the coarse style features \(_{r}\) with the similarity weight \(Sim\) obtained in the previous step:

\[f_{rg}=(Sim)~{}_{r}^{h w c}. \]

While local features are more concerned with the strokes, such as stroke length, stroke edge, and other nuances, we adopt cross-attention to gather the required style feature according to the needs of the target character:

\[ F_{s}^{}&=^{2}( _{r})^{k h w c}, Q=(_{}(f_{}))^{l c},\\ K&=(_{}(F_{s }^{}))^{k h w c}, V=_{}(F_{s}^{})^{k h w c},\\ A&=(}{}) ^{l k h w}, f_{rl}=(A)V ^{l c}, \]

where \(^{2}()\) denotes flattening the first two dimensions of the feature, and \(_{},_{},_{}\) are linear projections, and \(()\) denotes layer normalization. In Eq. 3, we obtain the aggregated style feature based on Eq. 1 and Eq. 2, where \(\) denotes concatenation operator:

\[f_{r}=(f_{rg} f_{rl}). \]

### Style Contrast Enhancement

There are some strategies to maintain style consistency: integrating consistency loss [59; 25], introducing a discriminator to determine the generated style [25; 47; 36], or treating the extracted style feature as a variable for further optimization . These approaches are indeed beneficial for improving the generation quality, but they may be inflexible or introduce additional parameters.

In this paper, we propose a streamlined approach named the Style Contrast Enhancement (SCE) module, which promotes the proximity of representations for the same style and the distance between representations for different styles. We apply a linear projection to the style feature \(f_{r}\), resulting in a contrastive feature \(e=MLP(f_{r})\).

In one batch, we denote the indices of contrastive features corresponding to all samples as \(E_{*}=\{i 0 i<2N\}\), where \(N\) represents the batch size. The dimensionality of \(E_{*}\) is double the

Figure 3: The illustration of equivalent IDSs. Figure 4: The illustration of the fusion module \(E_{fuse}\) of SSA block.

batch size \(N\) due to our utilization of a momentum encoder . Each sample \(x_{a}\) within the batch undergoes processing by both the encoder and the momentum encoder, yielding two outputs that serve as positive pairs. The negative sample set is defined as \(E_{-}=\{i E_{*} s(x_{i}) s(x_{a})\}\), while the positive sample set is \(E_{+}=\{i E_{*} i a,s(x_{i})=s(x_{a})\}\), where \(s()\) denotes the operator used to retrieve the corresponding style. The contrastive loss can be calculated as follows:

\[_{cl}=-_{a E_{*}}} (e_{a}^{T}e_{p}/)}{_{p E_{+}}(e_{a}^{T}e_{p}/)+_{n  E_{-}}(e_{a}^{T}e_{n}/)}. \]

### Generation

The decoder \(D\) is provided with both semantic feature \(f_{t}\) and style feature \(f_{r}\). It treats \(f_{t}\) as the initial tokens \(t^{<0}=f_{t}\), and then predicts the distribution of the next token autoregressively as \(p(t^{i} t^{<i},f_{r})\). Each newly predicted token is appended to the previous tokens for the subsequent iteration, it continues until all tokens are predicted. The likelihood of the entire sequence can be calculated as \(_{i=0}^{l}p(t^{i} t^{<i},f_{r})\). There are two ways for incorporating \(f_{r}\). The most straightforward approach involves using \(f_{r}\) as initial tokens, represented by \(t^{<0}=f_{t} f_{r}\), akin to \(f_{t}\). These tokens participate in each forward pass, relying on the self-attention mechanism to extract and integrate features.

However, it is only practical for low-resolution scenarios. Viewing \(f_{r}\) as tokens may lead to excessively long sequences, requiring a balance between computational efficiency and generation quality. To address this challenge, we incorporate \(f_{r}\) into each block of the decoder \(D\) via cross-attention. The tokens act as queries to align with the corresponding style features. \(t^{<l_{T}}\) denotes all the predicted tokens, from which \(t^{<0}\) is removed to get the glyph tokens \(_{y}\). The objective in Eq. 5 is to maximize the log-likelihood of the token sequence.

\[_{sq}=-(_{i=0}^{l_{T}-1}p(t^{i} t^{<i},f_{r})), \]

Finally, the model can be trained according to the objective in Eq. 6.

\[_{total}=_{sq}+_{cl}_{cl}, \]

where \(_{cl}\) controls the weight of contrastive loss, cf. Eq. 4, we set \(_{cl}=0.5\) in our experiments.

## 4 Experiments

### Dataset and Evaluation Metrics

DatasetsWe gathered 464 fonts from the Internet, covering diverse categories like printed, handwritten, and artistic styles. Next, we selected 3,500 commonly encountered Chinese characters and rendered them into 128x128 resolution images using the collected fonts.

The training set comprises 3,300 randomly selected Chinese characters and 424 fonts, referred to as Seen Fonts and Seen Characters (**SFSC**). There are two test sets: the first includes the same 3,300 characters but with different 40 fonts, called Unseen Fonts and Seen Characters (**UFSC**). The second test set consists of the remaining 200 characters and the same 40 fonts, known as Unseen Fonts and Unseen Characters (**UFUC**). We found a publicly accessible IDS decomposition table3. However, it exhibits several redundant entries and circular references, as well as an absence of some characters. Therefore, we performed simplifications and enhancements, reducing the number of IDCs to the 12 depicted in Table 1, which is sufficient for most frequently used Chinese characters. For convenience, we set the basic component's IDS as itself.

  IDC & Structure & Example (Char:IDS) \\  \(\) & left-right & \(\) & \(\) \\ \(\) & top-bottom & \(\) & \(\) \\ \(\) & left-middle-right & \(\) & \(\) \\ \(\) & top-middle-bottom & \(\) & \(\) \\ \(\) & enclosed-surrounding & \(\) & \(\) \\ \(\) & left-top-right-surrounding & \(\) & \(\) \\ \(\) & left-bottom-right-surrounding & \(\) & \(\) \\ \(\) & top-left-bottom-surrounding & \(\) & \(\) \\ \(\) & top-left-surrounding & \(\) & \(\) \\ \(\) & top-right-surrounding & \(\) & \(\) \\ \(\) & left-bottom-surrounding & \(\) & \(\) \\ \(\) & overlaying & \(\) \\  

Table 1: 12 IDCs used in this paper.

Evaluation metricsWe compare all methods in the following metrics, i.e., FID , L1, LPIPS , RMSE, and SSIM . Since aesthetics is inherently subjective, we conduct a user study for all methods to evaluate model performance based on user satisfaction. We observe that the existing font generation methods have differences in data preprocessing and metric selection. Factors such as glyph resolution, the padding around glyphs, the range of pixel values, the number of reference glyphs, and the evaluation function implementation all influence metric values. For example, NTF  and CF-Font  center the glyph within the canvas, leaving white space around it. However, this leads to inflated metric calculations. To ensure a fair comparison, we adopt consistent test data and metric implementation across all methods under evaluation. Specifically, we eliminate padding around the glyphs, fix the canvas resolution to 128 pixels, scale the data range to \(\), utilize SqueezeNet  as the network type to calculate LPIPS , and select the inceptionv3  feature layer with 2048 dimensions for FID  calculation.

### Comparison with state-of-the-art methods

We compare the proposed IF-Font with seven SOTA methods on our UFSC and UFUC datasets respectively, including CG-GAN (CVPR 2022), LF-Font (TPAMI 2022), FS-Font (CVPR 2022), CF-Font (CVPR 2023), VQ-Font (ICCV 2023), NTF (CVPR 2023) and FontD-iffuser  (AAAI 2024). All methods are trained from scratch on our SFSC dataset according to their respective official codes and default configurations. We slightly modify the codes of CG-GAN, LF-Font, FS-Font, VQ-Font and FontDiffuser to support varied numbers of reference glyphs.

#### 4.2.1 Quantitative comparison

Table 2 compares IF-Font and other SOTA methods. IF-Font significantly surpasses competitors in all reference glyph number settings for both datasets. Notably, IF-Font's performance on FID metric is exceptionally low, reaching a single-digit score, thanks to the high quality and clarity of the samples it generates. FS-Font  relies heavily on the predefined content-reference mapping, whereas the reference glyphs in all our experiments are randomly selected. Especially, when only one reference glyph is provided, covering all components of the target character becomes challenging, leading to poor performance of FS-Font, as shown in Fig. 10a. NTF  also struggles to imitate the target style, the layout of its generated samples often resembles that of the source font. In cases where there's a significant disparity between the source and target styles, NTF is prone to missing strokes.

We attribute CF-Font's performance to its reliance on fusing contents of 10 basic fonts. However, there happen to be a gap between the train dataset and our evaluation dataset. We conduct a user study through Fuxi Youling Crowdsourcing Platform 4 to quantify the subjective quality. For each test dataset, 5 characters are randomly selected, and each model is required to generate glyphs corresponding to 40 unseen fonts. A total of 30 participants are asked to select the option that most closely matches the ground truth from the generated results. The outcomes of the user study are presented in the last column of Table 2.

    & }{}\)} & }{}\)} & }{}\)} & }{}\)} \\   & FID1 & L1 & LPIPS & RMSE & SSIM & FID1 & L1 & LPIPS & RMSE & SSIM & FID1 & L1 & LPIPS & RMSE & SSIM & (\(}{}\)) \\   & CG-GAN  & 11.191 & 0.1784 & 0.1800 & 0.3997 & 0.4248 & 10.8713 & 0.1771 & 0.1464 & 0.3982 & 0.4444 & 11.133 & 0.1754 & 0.1457 & 0.3974 & 0.4440 & 14.28 \\  & LF-Font  & 13.2594 & 0.1240 & 0.1586 & 0.3967 & 0.4465 & 21.9

#### 4.2.2 Qualitative comparison

We present the corresponding samples from Table 2 in Fig. 5. IF-Font stands out by producing the clearest and most style-consistent samples. In contrast, FS-Font , LF-Font , CF-Font , and other models exhibit issues such as stroke errors or blur. VQ-Font  and NTF  are constrained by the source font and struggle with flat or narrow layouts, resulting in incorrect structures. VQ-Font even tends to crop marginal parts of glyphs to fit the target style. While CF-Font generally perserves the correct glyph layout, its outputs exhibit noticeable artifacts, indicating some remaining style inconsistencies. The performance of FontDiffuser  is also outstanding, but there is still a slight deficiency in the imitation of font styles. On the other hand, IF-Font maintains the correct character structures and excels in aspects such as the aspect ratio, glyph layouts, and stroke details.

Figure 5: Qualitative comparison with state-of-the-art methods, in which red boxes outline the artifacts. “Source” denotes the content glyph of other methods, IF-Font only employs the corresponding IDS.

    &  &  &  &  &  &  \\ I & S & C & & & & & & \\  ✗ & ✗ & ✗ & 8.2656 & 0.1632 & 0.1383 & 0.3820 & 0.4728 \\ ✓ & ✗ & ✗ & 8.3750 & 0.1638 & 0.1381 & 0.3828 & 0.4764 \\ ✓ & ✓ & ✗ & **7.5391** & 0.1614 & 0.1348 & 0.3797 & 0.4780 \\ ✓ & ✓ & ✓ & 8.4922 & **0.1597** & **0.1338** & **0.3775** & **0.4782** \\   

Table 3: Ablation studies on different modules. The first row is the results of baseline. I, S and C represent IHA, SSA, and SCE respectively.

Figure 6: Visualization of different modules in Table 3. Red, blue and green boxes represent the missing components, style inconsistency and corresponding improvements respectively.

### Ablation Studies

Main modulesRemoving the IHA, SSA, and SCE modules of IF-Font, a baseline model can be obtained. For a input character, it directly looks up the decomposition table to derive the corresponding IDS, and then encode the semantic feature \(f_{}\) through a embedding layer. The intermediate style features \(_{r}\) are directly averaged as style features \(f_{r}\), excluding any interactions with the similarity weight \(Sim\) and semantic feature \(f_{}\). The whole model relies solely on cross-entropy loss for supervision. Building upon the baseline, we incrementally reintegrate three modules to assess their individual contribution. Quantitative results are presented in Table 3, while Fig. 6 provides visualizations of these results. For further ablation study of the SSA block, please refer to Table 6 in Appendix B.1. Upon integrating our modules, a consistent improvement is observed across most metrics. Fig. 6 illustrates how IHA alleviates the issue of missing components present in the baseline. SSA enhances style consistency, while SCE improves the capability to imitate styles.

IDS granularityWe further analyze the impact of three different IDS granularities: components, strokes and mixed. Please see Table 5 in the Appendix for the examples of these granularities. Table 4 shows the quantitative results. Stroke granularity results in performance degradation across three metrics. We attribute this decline to the conflict between IDSs, hindering the model's ability to identify the target character. An attempt to concatenate IDSs from both granularities yields performance akin to that of component granularity. While this approach extends the sequence considerably, hence we opt for component granularity.

### Visualization of SSA

To demonstrate the effectiveness of Structure-Style Aggregation block, we visualize the attention maps corresponding to each IDC and component within the IDSs relative to the reference glyphs, as depicted in Fig. 7. Specifically, we choose the matrix \(A^{l k h w}\) in local feature calculation. For each position \(i\) of the target IDS\(t_{y}\), there exists a corresponding attention map \(A^{i}^{k h w}\), which indicates the attention that \(i^{}_{y}\) pays to the \(k\) reference features. We present the attention map \(A^{i}\) to visualize the distribution of attention weights directly. Additionally, we apply opacity to this map and overlay it onto the original reference glyph.

As we can see, when the target IDC or component exists in the reference glyph, more attention will be paid to the corresponding place. For instance, in the first row, the first, second, and fourth columns, and in the second row, the third and fifth columns are distinctly highlighted. Conversely, if the reference glyph lacks the target component, the local branch tends not to engage, as evidenced by the nearly blank third row. This approach stems from a preference to avoid forced attention allocation which might lead to interference. Instead, leveraging the average style captured by the global branch helps maintain a baseline quality of the output.

   Granularity & FID\(\) & L1\(\) & LPIPS\(\) & RMSE\(\) & SSIM\(\) \\  Component & 8.4922 & **0.1597** & **0.1338** & **0.3775** & 0.4782 \\ Stroke & **8.4297** & 0.1616 & 0.1347 & 0.3799 & **0.4888** \\ Mixed & 8.5234 & 0.1598 & 0.1337 & 0.3775 & 0.4782 \\   

Table 4: The impact of IDS granularity on performance.

Figure 7: Visualization of attention maps between IDS and reference glyphs. The symbols above are the target character (black) and the corresponding IDS (orange).

### New Glyph Creation

We validate the flexibility of IF-Font by generating the glyphs using IDSs of non-standard Chinese characters. Fig. 8 shows our experimental results. IF-Font demonstrates robust generalization by following given IDSs to produce new glyphs with accurate structure and consistent style. We fixed the font to "Sarasa Gothic" in the experiment, which is a CJK programming font. The last two columns lack ground truths due to their entirely non-existent characters.

## 5 Discussion

Failure casesAlthough our method enables high-quality generation under most circumstances, it still struggles on some hard cases, as illustrated in Fig. 9. IF-Font encounters difficulties with fancy and irregular font styles, including those with decorations, extremely flat or narrow layouts, excessively curved strokes, and calligraphic writings. Despite these challenges, it continues to preserve the correct character structure. Further discussion on the reasons for the difficulties in generating these fonts can be found in Appendix C.3.

UsabilityWe focus on CJK characters due to their unique spatial structures, which better reflect the characteristics of our method. By expanding the vocabulary and incorporating relevant data for training, IF-Font can also be adapted to handle other character sets.

Advantages_Conforms to writing habits_. We believe that the process of autoregressive modeling with IDS implicitly contains the order of writing. _Scalability_. Good scalability can be achieved by leveraging the mature experience of LLMs. _Robustness_. Due to vector quantization, glyphs are represented by a limited number of tokens (only 256 types), which reduces the learning difficulty for the decoder and decreases the likelihood of artifacts and other issues in the generated results.

## 6 Conclusion

We have presented IF-Font, a novel font generation paradigm. IF-Font redefines font generation as a sequence prediction task by quantizing glyphs as token sequences and leveraging Ideographic Description Sequence (IDS) to control the semantics of the generated glyphs. This method demonstrates exceptional capability in managing complex styles while preserving the correct structures. Benefiting from the flexibility of IDS, our method enables the creation of glyphs. This is achieved by formulating legal IDSs, which is a salient advantage over other methods that typically require the character to be present in at least one font as a precondition for generation. Refining and improving the IDS decomposition rules is considered future works. Furthermore, exploring the integration of IDS into handwritten font generation may yield interesting insights.

Figure 8: The ability of the IF-Font to create glyphs. The first two columns are kokuji, and the last two columns are completely non-existent Chinese characters.

Figure 9: Failed cases on complex fonts of UFUC. Orange boxes highlight reconstruction errors of VQGAN, red outlines the structural errors. _GT_: the glyphs rendered by fonts; _Target_: the glyphs reconstructed by VQGAN; _Output_: the glyphs generated by IF-Font.