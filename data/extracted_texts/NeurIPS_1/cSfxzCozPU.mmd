# Distributional regression: CRPS-error bounds for model fitting, model selection and convex aggregation

Distributional regression: CRPS-error bounds for model fitting, model selection and convex aggregation

 Clement Dombry

Universite de Franche-Comte,

CNRS, LmB (UMR 6623),

F-25000 Besancon, France.

clement.dombry@univ-fcomte.fr

&Ahmed Zaoui

Universite de Franche-Comte,

CNRS, LmB (UMR 6623),

F-25000 Besancon, France.

ahmed.zaoui@univ-fcomte.fr

clement.dombry@univ-fcomte.fr

###### Abstract

Distributional regression aims at estimating the conditional distribution of a target variable given explanatory co-variates. It is a crucial tool for forecasting when a precise uncertainty quantification is required. A popular methodology consists in fitting a parametric model via empirical risk minimization where the risk is measured by the Continuous Rank Probability Score (CRPS). For independent and identically distributed observations, we provide a concentration result for the estimation error and an upper bound for its expectation. Furthermore, we consider model selection performed by minimization of the validation error and provide a concentration bound for the regret. A similar result is proved for convex aggregation of models. Finally, we show that our results may be applied to various models such as Ensemble Model Output Statistics (EMOS), distributional regression networks, distributional nearest neighbors or distributional random forests and we illustrate our findings on two data sets (QSAR aquatic toxicity and Airfoil self-noise).

## 1 Introduction

Motivation and related literature.We consider in this paper the distributional regression problem, where we want to estimate the conditional distribution of a target random variable \(Y\) given explanatory co-variates \(X\). We assume \((X,Y)^{d}\) and we let

\[F_{x}^{*}(y)=(Y y\,|\,X=x). \]

be the conditional cumulative distribution functions (c.d.f.). Estimation is built on a training sample \(_{n}=\{(X_{i},Y_{i}),\,1 i n\}\) of independent identically distributed (i.i.d.) copies of \((X,Y)\).

Let us emphasize that distributional regression is much more challenging than standard regression where only a point prediction for \(Y\) given \(X=x\) is provided which typically reduces to an estimation of the conditional expectation. The conditional distribution provides a full account for the variability of \(Y\) given \(X=x\) and distributional regression is therefore a crucial tool for forecasting when a precise uncertainty quantification is required.

Distributional regression is of relevance in various applied fields. To name a few, let us mention statistical post-processing of weather forecast (Matheson and Winkler, 1976; Gneiting et al., 2005), forecasting of wind gusts (Baran and Lerch, 2015), of solar irradiance (Schulz et al., 2021), of ICU length stays during COVID-19 pandemy (Henzi et al., 2021), of breast cancer ODX score in oncology (Al Masry et al., 2024)...

The methodology relies on probabilistic forecast where the forecaster produces a predictive distribution for the quantity of interest (Gneiting and Katzfuss, 2014). Fitting a distributional regression model typically involves the minimization of a proper scoring rule, the most popular one being the Continuous Ranked Probability Score (CRPS, Matheson and Winkler, 1976; Gneiting and Raftery, 2007). It compares the actual observation with the predictive distribution in a comprehensive manner. Many different models have been proposed for distributional regression among which: Analog similar to a nearest neighbor method (KNN, Toth, 1989), Ensemble Model Output Statistics similar to Gaussian heteroscedastic regression (EMOS, Gneiting and Raftery, 2007), Isotonic Distributional Regression (Henzi et al., 2021b) and more recent machine learning methods such as Distributional Regression Network (DRN, Rasp and Lerch, 2018) or Distributional Random Forest (DRF, Cevid et al., 2022).

Despite these successful achievements in terms of methods and applications, a sound theory for distributional regression via scoring rule minimization is still missing. Recently, minimax rates of convergence with CRPS-error have been considered for distributional regression (Pic et al., 2023). The aim of this paper is to provide statistical learning guarantees for model fitting, model selection and convex aggregation based on CRPS minimization.

Model selection and convex aggregation are very important techniques in the field of statistics and machine learning and have been used very successfully for regression function estimation, see Tsybakov (2003); Bunea et al. (2007). The methods use two independent samples; the first sample, called training sample, is used to construct the initial estimators which are constituted as a dictionary (a collection of candidates). The second sample, called the validation sample, is used to aggregate them. Model selection enables the selection of the best candidate in the dictionary, while convex aggregation provides the optimal convex combination from these candidates.

Contributions.Our main results include a concentration bound for the theoretical risk when a parametric model is fitted via CRPS empirical risk minimization. We also consider model selection and model aggregation via CRPS minimization on a validation set and provide concentration bounds for the regret. Our results are first derived under sub-Gaussianity assumptions and then extended under weaker moment assumptions. We show that they apply to several popular models such as EMOS, DRN, KNN or DRF and provide a short illustration on two different datasets.

Structure of the paper.Section 2 first provides some background on distributional regression and proper scoring rules and then presents precisely the main methods and goals. The main results are stated in Section 3: an oracle inequality for the estimation error in model fitting (Theorem 1), and concentration bounds for the regret in model selection (Theorem 2) and model aggregation (Theorem 3). In Section 4, some specific models are introduced and the assumptions for our results to hold are checked. A short illustration on two different data set is provided in Section 5. Finally, an appendix contains all the proofs as well as some additional results.

## 2 Background on distributional regression and main goals

### Probabilistic forecast and its evaluation with scoring rules

We first consider the simple setting of probabilistic forecast without co-variate where a future observation \(Y\) is predicted by a probability distribution \(F\), called predictive distribution. Proper scoring rules are used in order to compare the predictive distribution \(F\) and the materializing observation \(y\) which are objects of different nature. Let \(_{0}()\) denote a subset of the set of all probability measures on \(\), often identified with their c.d.f. A scoring rule2 on \(_{0}\) is a function \(S_{0}[0,+)\). The quantity \(S(F,y)\) is interpreted as the error between the predictive distribution \(F\) and the materializing observation \(y\). The mean error when \(Y\) has "true" distribution \(G\) is denoted by

\[(F,G)=_{Y G}[S(F,Y)].\]

The following notion of proper and strictly proper scoring rule is central in the theory.

**Definition 1**.: _The scoring rule \(S\) is said proper on \(_{0}\) when_

\[(F,G)(G,G),F,G_{0}. \]

_It is said strictly proper when equality in Eq. (2) implies \(F=G\)._Stated differently, the scoring rule \(S\) is strictly proper on \(_{0}\) when

\[*{arg\,min}_{F_{0}}(F,G)=\{G\},G_{0}.\]

The interpretation is that, in order to minimize its mean error, the forecaster has to predict the "true" observation distribution \(G\).

In this paper, we consider the Continuous Ranked Probability Score (CRPS, Matheson and Winkler 1976). This scoring rule is defined by the formula

\[S(F,y)=_{}(_{\{y z\}}-F(z))^{2}z \]

for all \(F\) finite absolute moment. Here the subset \(_{0}()\) is the Wasserstein space

\[_{1}()=\{F() m_{1}(F)= _{}|y|F(y)<\}\]

of probability measures on \(\) with finite first moment. One can easily check from this definition that

\[(F,G)=_{}G(z)(1-G(z))z+_{ }(F(z)-G(z))^{2}z,\]

which implies

\[(F,G)-(G,G)=_{}(F(z)-G(z))^{2}z. \]

This quantity is nonnegative and vanishes if and only if \(F=G\), ensuring that the \(\) is a strictly proper scoring rule. For discrete predictive distributions of the form \(F=_{i=1}^{n}w_{i}_{y_{i}}\) (with \(_{y}\) the Dirac mass at \(y\)), the \(\) can be computed simply (Gneiting and Raftery, 2007) by

\[S(F,y)=_{i=1}^{n}w_{i}|y_{i}-y|-_{i j}w_{i}w_{j}|y_{i}- y_{j}|.\]

### Model fitting, model selection and convex aggregation

We briefly present the methods and objectives that we address in this paper.

#### 2.2.1 Theoretical risk in distributional regression

In a regression framework, we observe a sample \(_{n}=\{(X_{i},Y_{i}),\ 1 i n\}\) of independent copies of \((X,Y)^{d}\). Distributional regression aims at estimating the conditional distribution \(Y|X=x\) characterized by its c.d.f. \(F_{x}^{*}\) defined in Eq. (1). The marginal distribution of \(X\) is denoted by \(P_{X}\). The forecaster uses the training sample \(_{n}\) and some algorithm to build a _functional_ estimator \(_{n} x_{n,x}\) of the map \(F^{*} x F_{x}^{*}\). The accuracy of this estimator is here measured by its theoretical risk

\[(_{n})=[S(_{n,X},Y)].\]

where expectation is taken with respect to the joint law of \((X,Y)\). This quantity can be seen as the counterpart of the mean squared error in point regression. The excess risk of \(_{n}\) is defined as

\[(_{n})-(F^{*})=[S(_{n,X},Y) -S(F_{X}^{*},Y)]=[S(_{n,X},F_{X}^{*})-(F_{ X}^{*},F_{X}^{*})] 0.\]

The nonnegativity is ensured by the fact that \(S\) is a proper scoring rule according to Definition 1. If \(S\) is strictly proper, the excess risk is equal to \(0\) if and only if \(_{n,x}=F_{x}^{*}\) almost everywhere (with respect to \(P_{X}\)). For the CRPS, Eq. (4) implies that the excess risk can be rewritten as

\[(_{n})-(F^{*})=[_{} |_{n,X}(u)-F_{X}^{*}(u)|^{2}du].\]

#### 2.2.2 Model fitting

Our first interest lies in model fitting by empirical risk minimization. Here we consider a parametric family \((F_{})_{}\), \(^{K}\), where \(F_{} x^{d} F_{,x}_{0} ()\). The empirical risk associated with \(F_{}\) is computed on the training sample \(_{n}\) by

\[}_{n}(F_{})=_{i=1}^{n}S(F_{,X_{i}}, Y_{i})\]

and is an empirical counterpart of the theoretical risk \((F_{})\). Empirical risk minimization consists in finding

\[_{n}=*{arg\,min}_{}}_{ n}(F_{}) \]

and proposing the estimator \(F_{_{n}}\) which is thought as almost optimal within the family \((F_{})_{}\). A classical decomposition of the excess risk of the corresponding estimator is given by

\[(F_{_{n}})-(F^{*})=(F_{ _{n}})-_{}(F_{})+ _{}(F_{})-(F^{*}).\]

where the two terms are called the estimation error and the approximation error respectively. The approximation error is deterministic and depends on the ability of the family \((F_{})_{}\) to approximate \(F^{*}\). The estimation error is random as it depends on the training sample \(_{n}\). Our first goal is the following:

**Goal 1:** provide non asymptotic estimates for the estimation error \((F_{_{n}})-_{}(F_{})\).

#### 2.2.3 Model selection and convex aggregation

Our second interest lies in model selection and convex aggregation via validation error minimization. Here we suppose that a validation sample \(^{}_{N}=\{(X^{}_{i},Y^{}_{i}), 1 i N\}\) is available, which is assumed independent of the training sample \(_{n}\).

Model selection.A common situation in machine learning is that we have \(M\) algorithms at hand that are trained on \(_{n}\), resulting in models \(^{1}_{n},,^{M}_{n}\). In order to select the best model, we compute the empirical risks on the validation sample

\[}^{}_{N}(^{m}_{n})=_{i=1}^{N}S( ^{m}_{n,X^{}_{i}},Y^{}_{i})\]

and select the model

\[=*{arg\,min}_{1 m M}}^{}_{N }(^{m}_{n}).\]

An oracle having access to the theoretical risk would have selected

\[m^{*}=*{arg\,min}_{1 m M}(^{m}_{n}),\]

leading to the definition of the regret

\[(^{}_{n})-(^{m^{*}}_{n})=(^{}_{n})-_{1 m M}(^{m}_{n}).\]

Goal 2:** provide non asymptotic estimates for the regret \((^{}_{n})-_{1 m M}(^{m }_{n})\).

Convex aggregation.We define the convex aggregation of models \(^{1}_{n},,^{M}_{n}\) with weights \(_{1},,_{M}\) by

\[^{}_{n,x}=_{m=1}^{M}_{m}^{m}_{n,x}.\]

Here \(=(_{1},,_{M})\) is an element of the simplex \(_{M}=\{_{m} 0,_{1 m M}_{m}=1\}\). The best weights for convex aggregation are obtained by minimization of the validation error, i.e.

\[=*{arg\,min}_{_{M}}}^{}_{N}(^{}_{n}).\]

Goal 3:** provide non asymptotic estimates for the regret \((^{}_{n})-_{_{M}}(^{}_{n})\).

Main results

We present our main results for model fitting, model selection and convex aggregation in distributional regression. We first focus on concentration bound under sub-Gaussianity assumptions. In a second step, we extend our results under weaker moment assumptions. The definition of sub-Gaussian random variables and distributions is given in Appendix A as well as some useful concentration inequalities. All the proofs are postponed to Appendices B, C and D.

### Estimation error in model fitting

We provide concentration results for the estimation error when fitting a parametric model via empirical CRPS-error minimization according to the framework described in Section 2.2.2. Our working assumptions are the following.

**Assumption 1**.: _(sub-gaussianity)_

* _the variable_ \(Y\) _is_ \(_{1}\)_-sub-Gaussian;_
* _there exists_ \(_{2}>0\) _such that_ \(m_{1}(F_{,X})\) _is_ \(_{2}\)_-sub-Gaussian for all_ \(\)_._

Assumption 1-\(i\)) is classical in a regression setting (Gyorfi et al., 2002; Biau and Devroye, 2015) while Assumption 1-\(ii\)) characterizes the sub-Gaussian behavior of the absolute moment of \(F_{,X}\). Importantly, Assumption 1 implies that the variable \(Z_{}=S(F_{,X})\) is \(^{2}+_{2}^{2})}\)-sub-Gaussian for all \(\), see Proposition 5 in the appendix.

Our second assumption requires compactness of the parameter space and Lipschitz continuity of the model \((F_{})_{}\). We denote by \(W_{1}\) the Wasserstein distance of order \(1\) on the space \(_{1}()\).

**Assumption 2**.: _(regularity) The parameter space \(^{K}\) is compact and there exists a constant \(L>0\) such that \(W_{1}(F_{_{1},x},F_{_{2},x}) L\|_{1}-_{2}\|\) for all \(_{1},_{2},\ x^{d}\)._

This assumption ensures the Lipschitz continuity of the empirical risk \(_{n}(F_{})\), see Proposition 6 and, by compactness, the existence of \(_{n}\), the ERM estimator defined in Eq. (5). We provide in Section 4 examples of popular models verifying the two assumptions.

Our main result provides a concentration bound on the estimation error. We let \(R>0\) be such that \(\) is included in the ball centered at \(0\) and with radius \(R\).

**Theorem 1**.: _Under Assumptions 1- 2, for all \((0,1)\), the estimation error satisfies, with probability at least \(1-\),_

\[(F_{_{n}})-_{}(F_{} )(2n^{K}/)}{n}} \]

_with \(c_{}=64(_{1}^{2}+_{2}^{2})\) and provided that \(n\) is large enough so that \(n(2n^{K}/)(48LR)^{2}/c_{}\)._

The inequality (6) is commonly known as an oracle inequality. The convergence rate depends on \(/)/n}\) with \(K\) the dimension of the parameter space and \(n\) the sample size. A key point in the proof is the the combinatorial complexity of \(\) in terms of \(\)-net, see Devroye et al. (1996). A bound in expectation can easily be deduced from Theorem 1 and its proof.

**Corollary 1**.: _Under Assumptions 1- 2,_

\[(F_{_{n}})-_{} (F_{}) 2(2n^{K})}{n}}\]

_provided \(n\) is large enough so that \(n(2n^{K})(48LR)^{2}/c_{}\)._

In particular, the ERM approach is weakly consistent when \(n\) goes to infinity.

### Estimation error in model selection and convex aggregation

We provide concentration results for the regret in model selection and convex aggregation. Recall that the methodology is based on minimization of the test error on a validation sample, see the precise framework in Section 2.2.3. We need the following assumption.

**Assumption 3**.: _(sub-Gaussianity)_* _the variable_ \(Y\) _is_ \(_{1}\)_-sub-Gaussian;_
* _conditionally on_ \(_{n}\)_, there exists_ \(_{n}=(_{n})>0\) _such that_ \(m_{1}(^{m}_{n,X})\) _is_ \(_{n}\)_-sub-Gaussian for all_ \(m=1,,M\)_._

We will see in Section 4 that for several popular models such as distributional nearest neighbors or distributional random forest, Assumption 3-\(ii)\) is satisfied with \(_{n}=_{1 i n}|Y_{i}|\) which is of order \(_{1}\)(Vershynin, 2018, Exercise 2.5.8 p.25).

Under this assumption, a control of the regret in model selection is provided by the following theorem. Our results hold conditionally on the training set \(_{n}\).

**Theorem 2**.: _Under Assumption 3, for all \((0,1)\), the regret in model selection satisfies_

\[((^{}_{n})-_{1 m M} (^{m}_{n}) 4(2M/)/N}\;\; _{n}) 1-\]

_with \(c_{n}=_{1}^{2}+_{n}^{2}\). Furthermore,_

\[(^{}_{n})-_{1 m M} (^{m}_{n})\;\;_{n} 8(2M)}{N}}.\]

When selecting the hyperparameter \(k\) in nearest neighbor distributional regression or mtry in distributional random forest, one has respectively \(M=n\) and \(M=d\) if all possible values are considered - see Section 4 for more details. Note that when the response variable \(Y\) is bounded, then \(c_{n}\) does not depend on \(n\) and is constant. We now state a bound for the regret in convex aggregation.

**Theorem 3**.: _Under Assumption 3, for all \((0,1)\), the regret in convex aggregation satisfies_

\[(^{}_{n})-_{ }(^{}_{n}) 8(2N^{M}/)/N} \;\;_{n} 1-,\]

_with \(c_{n}=_{1}^{2}+_{n}^{2}\) provided \(N\) is large enough so that \(N(2N^{M}/) 48^{2}/c_{n}\). Furthermore,_

\[(^{}_{n})-_{ }(^{}_{n})\;\;_{n}  2(2N^{M})}{N}}\]

_provided \(N(2N^{M}) 48^{2}/c_{n}\)_

### Beyond sub-gaussianity

The preceding results hold under a strong sub-Gaussianity. We next adapt our results to the following weaker moment condition.

**Assumption 4**.: _There is \(p 2\) and \(D>0\) such that \([|Y|^{p}] D\) and \([|m_{1}(F_{,X})|^{p}] D\) for all \(\)_

We recall that, for \(p 1\), the \(L^{p}\)-norm of a random variable \(Z\) is defined by \(\|Z\|_{L^{p}}=[|Z^{p}|]^{1/p}\).

**Theorem 4**.: _Under Assumptions 2 and 4, we have_

\[(F_{_{n}})-_{}(F _{})_{L^{p}} Cn^{-p/(2(p+K))}.\]

_with constant \(C>0\) depending only on \(K,p,L,D,R\) and possibly made explicit from the proof. This implies the bound for the expected estimation error_

\[(F_{_{n}})-_{} (F_{}) Cn^{-p/(2(p+K))}.\]

For \(p\) large, the rate of convergence tends to the parametric rate \(n^{-1/2}\) obtained (up to a logarithmic factor) in the sub-Gaussian case. We propose additional results for model selection (Theorem 5) and convex aggregation (Theorem 6) which are, for the sake of brevity, postponed to Appendix D.2.

## 4 Examples and popular models for distributional regression

We present the most popular models for distributional regression for which we want to apply our results. The first two (EMOS and DRN) are parametric, while the last two (distributional \(k\)-NN and DRF) are fully non-parametric.

### EMOS and distributional regression networks

EMOS.The EMOS model was designed by Gneiting et al. (2005) for the purpose of statistical post-processing of ensemble weather forecast. In this framework, the predictive distribution takes the form of a discrete distributions \(m^{-1}_{l=1}^{m}_{y_{l}}\) with members \(y_{1},,y_{m}\) corresponding to different scenarios obtained from numerical weather predictions. Such forecast typically suffer from bias and underdispersion so that statistical post-processing is needed. The explanatory variable for distributional regression are Ensemble Member Output Statistics such as the ensemble mean \(\) and ensemble variance \(v_{y}^{2}\). In its simplest version, EMOS models the predictive distribution as a Gaussian distribution with parameters \(m=_{0}+_{1}\) and \(^{2}=_{0}^{}+_{1}^{}v_{y}^{2}\). This is a parametric model with \(=(_{0},_{1},_{0}^{},_{1}^{})\) in \(=^{2}(0,)^{2}\). Minimum CRPS estimation is used for model fitting as described in 2.2.2. This simple yet successful method has encountered many generalizations (Scheuerer, 2013; Scheuerer and Hamill, 2015; Baran and Nemoda, 2016) and we shall consider the following general setting. Given \(x^{d}\), the predictive distribution takes the form \(F(;)=F((-m)/)\) with \(=(,,^{},^{})^{1+d} ^{1+d}\) and

\[m(x;)=+^{}x\\ ^{2}(x;)=(^{}+^{{}^{ }}x)\]

with \((u)=(1+^{u})\). The predictive distribution belongs to a location/scale family and the location and log-scale parameters are linear in the covariate \(x\). The \(\) link function ensures positivity of the scale.

Distributional Regression Networks (DRN).In order to consider higher dimension co-variates together with non-linear dependence, Rasp and Lerch (2018) introduce distributional regression networks. In a setting similar to EMOS, neural networks are used to model complex response functions \(m(x;)\) and \(^{2}(x;)\). In the case of a single hidden layer with \(H\) units, the equations write

\[m(x;)=+^{}g(+^{}x)\\ ^{2}(x;)=(^{}+^{{}^{ }}g(+^{{}^{}}x)), \]

where \(g\) denotes the activation function acting componentwise, \((,,^{},^{})^{1+H}^{1+H}\) the parameters for the output layers and \((,)^{H+Hd}\) the biases and weights of the hidden layer. The global parameter \(=,,^{},^{},,)\) lies in dimension \((d+3)H+2\). Note that in absence of hidden layers, DRN reduces to EMOS. Extension to a MLP structure with multiple hidden layers is straightforward, see Schulz and Lerch (2022) for a review of DRNs and their applications.

We next provide conditions ensuring that our results hold for the EMOS and DRN models.

**Proposition 1**.: _Let \((F_{})_{}\) be a EMOS or DRN model with parameters restricted to a compact subset \(\). If \(Y\) is sub-Gaussian, \(X\) is bounded and the activation function \(g\) is Lipschitz continuous, then Assumptions 1, 2 and 3 are satisfied._

### Distributional k-Nearest Neighbors and Random Forests

Distributional k-Nearest Neighbors (KNN).The predictive distribution is built on a straightforward extension of \(k\)-nearest neighbor regression: we set

\[_{n,x}(y)=_{i=1}^{k}_{\{X_{i}(x)\}}_{\{Y_{i} y\}}\]

where \((x)\) denotes the set of \(k\) nearest neighbors of \(x\) in the training set \((X_{i})_{1 i n}\). The main hyperparameter is the number \(k\) of neighbors, leading to a model selection problem as considered in 2.2.3. This simple method is known as the Analog method in the framework of statistical post-processing of weather forecast (Toth, 1989).

Distributional Random Forests (DRF).Random forests for distributional regression have been introduced by Cevid et al. (2022) and are a powerful nonparametric method. We describe only the main lines of the method. Recall that in standard regression, Breiman's Random Forest (Breiman,2001) estimates the regression function by

\[(x)=_{b=1}^{B}T^{(b)}(x)=_{b=1}^{B} {|L^{b}(x)|}_{i=1}^{n}Y_{i}_{\{X_{i} L^{b}(x)\}},\]

where \(T_{1},,T_{B}\) denote randomized regression trees built on bootstrap samples of the original data and \(L^{b}(x)\) the leaf containing \(x\) in the tree \(T^{b}\). Interfering the two sums yields

\[(x)=_{i=1}^{n}(_{b=1}^{B}_{\{ X_{i} L^{b}(x)\}}}{|L^{b}(x)|})Y_{i}=_{i=1}^{n}w_{ni}(x)Y_{i}.\]

Using these _Random Forest weights_, the predictive distribution writes

\[_{n,x}(y)=_{i=1}^{n}w_{ni}(x)_{\{Y_{i} y\}}.\]

This strategy based on Breiman's regression tree is used by Meinshausen (2006) to construct the quantile regression forest. Different splitting strategies in the tree construction have been considered to better detect changes in distribution rather than changes in mean (Taillardat et al., 2016; Athey et al., 2019). Note that Cevid et al. (2022) minimizes a scoring rule to construct the splits of the trees. The main hyperparameter of this nonparametric method is the so-called mtry parameter that controls the number of co-variates tested at each split in the trees.

For the distributional KNN and DRF models, model fitting with ERM is irrelevant and we only consider model selection and convex aggregation.

**Proposition 2**.: _Let \(_{n}\) be a KNN or DRF model fitted on a training set \(_{n}\). Then Assumption 3-\(ii)\) is satisfied with \(_{n}=_{1 i n}|Y_{i}|\)._

## 5 Numerical results

In this section, we illustrate how model selection and model aggregation work on real data sets. The cross validation methodology, widely used in practice, is justified by the theoretical results obtained in Section 3. The source code for these experiments can be found at [https://github.com/ZaouiAmed/Neurips2024_DistributionalRegression](https://github.com/ZaouiAmed/Neurips2024_DistributionalRegression).

Datasets.We consider two datasets used in the framework of heteroscedastic regression with reject option as detailed in Zaoui et al. (2020). The first dataset, _QSAR aquatic toxicity_(Ballabio et al., 2019) is referred to as qsar and comprises \(546\) observations with \(8\) numerical features used to predict acute toxicity in Pimephales promelas. The toxicity output ranges from \(0.12\) to \(10.05\) with evidence of a low heteroscedasticity. The second dataset, _Airfoil Self-Noise_(Brooks et al., 2014) is referred to as airfoil and consists of \(1503\) observations with \(5\) measured features from aerodynamic and acoustic tests. The output represents the scaled sound pressure level in decibels, ranging from \(103\) to \(140\) with evidence of a strong heteroscedasticity..

Models.We consider the KNN and DRF models to predict the conditional distribution of the output variable. Our focus lies on hyperparameters selection via minimization of the validation error, where the main hyperparameter is the number k of neighbors and the number mtry of variables considered at each split for KNN and DRF respectively. We utilize the implementation of these methods from the R packages KernelKnn and DRF. For DRF, a preliminary exploration shows that sensible choices for the other parameters are num.trees\(=1000\), sample.fraction\(=0.9\), min.node.size\(=1\) and default values for others parameters. Finally, the R package ScoringRule is used for CRPS computation and the optim function based on the Nelder-Mead method is used for parameter optimization in convex aggregation.

Methodology.We use the same methodology for the two datasets. We divide the data into three parts: \(50\%\) for training, \(20\%\) for validation and \(30\%\) for testing. In a first stage, the training set is used to train the model (KNN or DRF) for the various hyperparameters (k or mtry) and the validation set is used to select best hyperparameters \(}\) or \(}\). Furthermore, the validation set also used to choose for model selection (MS), choosing between KNN and RF, and the best convex aggregation (CA). In a second stage, the different models (KNN, RF, MS and CA) are refitted on the union of training and validation sets (\(70\%\) of data) and evaluated on the test set (\(30\%\)) by computing the test CRPS-error. This process is repeated \(100\) times for different random splits of the data into training, validation and testing sets. The distributions of the test error for KNN, DRF, MS and CA is then analyzed in terms of mean, standard error and boxplot.

Results.For the sake of brevity, only the results for the qsar dataset are presented, while the results for the airfoil dataset are postponed to Appendix F. The first stage of the procedure where the hyperparameter (k or mtry) is selected by minimization of the validation error is shown on the left plot of Fig. 1 for KNN and on the middle plot for DRF. In both cases, the validation error curve shows a clear minimum allowing to select the hyperparameter \(}=8\) and \(}=4\) corresponding to CRPS-error of \(0.696\) and \(0.678\) on the validation set respectively. In the second stage, KNN, DRF, MS and CA are evaluated on the test set and the right plot of Fig. 1 shows the distribution of the test error over 100 repetitions. We can see from the boxplots that the distribution of the test error is slightly larger for KNN than for DRF, that MS achieves almost the same performance as RF and that CA achieves slightly better performance than DRF. Hence model selection and convex aggregation accomplish their goal. The numerical values of the means together with standard errors are summarized in Table 1, confirming our analysis from the boxplots.

## 6 Conclusion

In this work, we considered distributional regression with error assessed using the CRPS and investigated model fitting via empirical risk minimization. Additionally, we explored classical aggregation procedures: model selection and convex aggregation where two independent samples are used, the first for model construction, the second for model selection or convex aggregation. We derived oracle concentration inequalities for the estimation error and established an upper bound for its expectation within the sub-Gaussian framework and beyond, considering weaker moment assumptions. These new theoretical results are solid mathematical justifications for common practices in the framework of distributional regression. In future work, we will study the minimax convergence rates for our approaches and consider the use of empirical process theory to strengthen our results.

 KNN & DRF & MS & CA \\ \(0.643\) (\(0.004\)) & \(0.624\) (\(0.004\)) & \(0.632\) (\(0.004\)) & \(0.612\) (\(0.003\)) \\ 

Table 1: qsar data. Mean of the test CRPS and its standard error (in parenthesis) over 100 repetitions.

Figure 1: qsar data. Left and middle: selection of k for the KNN algorithm and of mtry for the DRF algorithm by minimization of the validation error. Right: test error evaluated with 100 repetitions for KNN, DRF, model selection (MS) and convex aggregation (CA).