# Causal Temporal Representation Learning with Nonstationary Sparse Transition

Xiangchen Song\({}^{1}\)   Zijian Li\({}^{2}\)   Guangyi Chen\({}^{1,2}\)   Yujia Zheng\({}^{1}\)

Yewen Fan\({}^{1}\)   Xinshuai Dong\({}^{1}\)   Kun Zhang\({}^{1,2}\)

\({}^{1}\)Carnegie Mellon University

\({}^{2}\)Mohamed bin Zayed University of Artificial Intelligence

{xiangchensong,kunz1}@cmu.edu

###### Abstract

Causal Temporal Representation Learning (Ctrl) methods aim to identify the temporal causal dynamics of complex nonstationary temporal sequences. Despite the success of existing Ctrl methods, they require either directly observing the domain variables or assuming a Markov prior on them. Such requirements limit the application of these methods in real-world scenarios when we do not have such prior knowledge of the domain variables. To address this problem, this work adopts a sparse transition assumption, aligned with intuitive human understanding, and presents identifiability results from a theoretical perspective. In particular, we explore under what conditions on the significance of the variability of the transitions we can build a model to identify the distribution shifts. Based on the theoretical result, we introduce a novel framework, _Causal Temporal Representation Learning with Nonstationary Sparse Transition_ (**CtrlNS**), designed to leverage the constraints on transition sparsity and conditional independence to reliably identify both distribution shifts and latent factors. Our experimental evaluations on synthetic and real-world datasets demonstrate significant improvements over existing baselines, highlighting the effectiveness of our approach.

## 1 Introduction

Causal learning from sequential data remains a fundamental yet challenging task . Discovering temporal causal relations among _observed_ variables has been extensively studied in the literature . However, in many real-world scenarios such as video understanding , observed data are generated by causally related latent temporal processes or confounders rather than direct causal edges. This leads to the task of _causal temporal representation learning_ (Ctrl), which aims to build compact representations that concisely capture the data generation processes by inverting the mixing function that transforms latent factors into observations and identifying the transitions that govern the underlying latent causal dynamics. This learning problem is known to be challenging without specific assumptions . The task becomes significantly more complex with _nonstationary_ transitions, which are often characterized by multiple distribution shifts across different domains, particularly when these domains or shifts are also unobserved.

Recent advances in unsupervised representation learning, particularly through nonlinear Independent Component Analysis (ICA), have shown promising results in identifying latent variables by incorporating side information such as class labels and domain indices . For time-series data, historical information is widely utilized to enhance the identifiability of latent temporal causal processes . However, existing studies primarily derive results under stationary conditions  or nonstationary conditions with observed domain indices . These methods are limited in application as general time series data are typically nonstationary and domain information is difficultto obtain. Recent studies [15; 24; 25; 26] have adopted a Markov structure to handle nonstationary domain variables and can infer domain indices directly from observed data. (More related work can be found in Appendix S4.) However, these methods face significant limitations; some are inadequate for modeling time-delayed causal relationships in latent spaces, and they rely on the Markov property, which cannot adequately capture the arbitrary nonstationary variations in domain variables. This leads us to the following important yet unresolved question:

_How can we establish identifiability of nonstationary nonlinear ICA for general sequence data without knowledge of the prior distribution of the domain variables?_

Relying on observing domain variables or known Markov priors to capture nonstationarity seems counter-intuitive, especially considering how easily humans can identify domain shifts given sufficient variation on transitions, such as video action segmentation [27; 28] and recognition [29; 30; 31] tasks. In this work, we theoretically investigate the conditions on the significance of transition variability to identify distribution shifts. The core idea is transition clustering, assuming transitions within the same domain are similar, while transitions across different domains are distinct. Building on this identification theorem, we propose _Causal Temporal Representation Learning with Nonstationary Sparse Transition_ (**CtrlNS**), to identify both distribution shifts and latent temporal dynamics. Specifically, we constrain the complexity of the transition function to identify domain shifts. Subsequently, with the identified domain variables, we learn the latent variables using conditional independence constraints. These two processes are jointly optimized within a VAE framework.

The main contributions of this work are as follows: (1) To our best knowledge, this is the first identifiability result that handles nonstationary time-delayed causally related latent temporal processes without knowledge of the prior distribution of the domain variables. (2) We present **CtrlNS**, a principled VAE-based framework for recovering both nonstationary domain variables and time-delayed latent causal dynamics. (3) Experiments on synthetic and real-world datasets demonstrate the effectiveness of the proposed method in recovering latent variables and domain indices.

## 2 Problem Formulation

### Nonstationary Time Series Generative Model

We first introduce a nonstationary time-series generative model in our setting. The observational dataset is \(=\{_{t}\}_{t=1}^{T}\), where \(_{t}^{n}\) is produced from causally related, time-delayed latent components \(_{t}^{n}\) through an invertible mixing function \(\):

\[_{t}=(_{t}). \]

In the nonstationary setting, transitions within the latent space vary over time. Define \(u\) as the domain or regime index variable, with \(u_{t}\) corresponding to domain variable at time step \(t\). Assuming there are \(U\) distinct regimes, i.e., \(u_{t}\{1,2,,U\}\), each regime exhibits unknown distribution shifts. Those regimes are characterized by \(U\) different transition functions \(\{_{u}\}_{u=1}^{U}\), which were originally introduced in  through change factors to capture these distribution shifts in transition dynamics. The \(i\)-th component of latent variable \(_{t}\), is then generated via \(i\)-th component of transition function \(\):

\[z_{t,i}=m_{i}(u_{t},\{z_{t^{},j} z_{t^{},j} (z_{t,i})\},_{t,i}), \]

where \((z_{t,i})\) represents the set of latent factors directly influencing \(z_{t,i}\), which may include any subset of \(_{<t}=\{z_{,i}\{1,2,,t-1\},i\{1,2,,n\}\}\). For analytical simplicity, we assume that the parents in the causal graph are restricted to elements in \(_{t-1}\). Extensions to higher-order cases, which involve multistep, time-delayed causal relations, are already discussed in Appendix S1.5 of . These extensions are orthogonal to our contributions and are therefore omitted here for brevity. Importantly, in a nonstationary context, \(()\) may also be sensitive to the domain index \(u_{t}\), indicating that causal dependency graphs vary across different domains or regimes, which will be revisited in our later discussion on identifiability. We assume that the generation processes for each \(i\)-th component of \(_{t}\) are mutually independent, given \(_{<t}\) and \(u_{t}\). Consistent with the existing

Figure 1: Graphical model for nonstationary causally related time-delayed time-series data generation process with unobserved domain variables \(u_{t}\).

literature [23; 25], we further assume that the noise terms \(_{t,i}\) are independent both spatially and temporally. This assumption implies that there is no instantaneous causal influence among the latent causal processes. The graphical model corresponding to this setting is illustrated in Figure 1.

### Identifiability of Domain Variables and Latent Causal Processes

In this section we introduce the identifiability of both domain variables and time-delayed latent causal processes in Definitions 2 and 3, respectively. If the estimated latent processes are identifiable at least up to a permutation and component-wise invertible transformations, then the latent causal relationships are also immediately identifiable. This follows from the fact that conditional independence relations comprehensively characterize the time-delayed causal relations within a time-delayed causally sufficient system, in which there are no latent causal confounders in the causal processes. Notably, invertible component-wise transformations on latent causal processes preserve their conditional independence relationships. We now present definitions concerning observational equivalence, the identifiability of domain variables and latent causal processes.

**Definition 1** (Observational Equivalence).: _Formally, consider \(\{_{t}\}_{t=1}^{T}\) as a sequence of observed variables generated by true temporally causal latent processes specified by \((,,p(),)\) given in Eqs. (1) and (2). Here, \(\) and \(\) denote the concatenated vector form across \(n\) dimensions in the latent space. Similarly \(\) for time steps \(1\) to \(T\). A learned generative model \((},},(),})\) is observationally equivalent to the ground truth one \((,,p(),)\) if the model distribution \(p_{},},_{},}} (\{_{t}\}_{t=1}^{T})\) matches the data distribution \(p_{,,p_{},}(\{_{t}\}_{t= 1}^{T})\) everywhere._

**Definition 2** (Identifiable Domain Variables).: _Domain variables are said to be identifiable up to label swapping if observational equivalence (Def. 1) implies identifiability of domain variables up to a permutation \(\) for domain indices:_

\[p_{},},_{},}} (\{_{t}\}_{t=1}^{T})=p_{,,p_{}, }(\{_{t}\}_{t=1}^{T})_{t}=(u_{t}),  t\{1,2,,T\}. \]

**Definition 3** (Identifiable Latent Causal Processes).: _The latent causal processes are said to be identifiable if observational equivalence (Def. 1) leads to the identifiability of latent variables up to a permutation \(\) and component-wise invertible transformation \(\):_

\[p_{},},_{},}} (\{_{t}\}_{t=1}^{T})=p_{,,p_{}, }(\{_{t}\}_{t=1}^{T})}^{-1}( _{t})=^{-1}(_{t}), _{t}, \]

_where \(\) denotes the observation space._

## 3 Identifiability Theory

In this section, we demonstrate that under mild conditions, the domain variables \(u_{t}\) are identifiable up to label swapping and the latent variables \(_{t}\) are identifiable up to permutation and component-wise transformations. We partition our theoretical discussion into two sections: (1) identifiability of nonstationary discrete domain variables \(u_{t}\) and (2) identifiability of latent causal processes. We slightly extend the usage of \(()\) to define the square matrix support and the support of a square matrix function as follows:

**Definition 4** (Matrix Support).: _The support (set) of a square matrix \(^{n n}\) is defined using the indices of non-zero entries as:_

\[()\{(i,j)_{i,j} 0\}\,. \]

**Definition 5** (Matrix Function Support).: _The support (set) of a square matrix function \(:^{n n}\) is defined as:_

\[(())\{(i,j), ()_{i,j} 0\}\,. \]

For brevity, let \(\) and \(}\) denote the \(n n\) binary matrices representing the support of the Jacobian \(_{}(_{t})\) and \(_{}}(}_{t})\), respectively. The \((i,j)\)-th entry of \(\) is \(1\) if and only if \((i,j)(_{})\). We further define the transition complexity using its Frechet norm as \(||=_{i,j}_{i,j}\), and similarly for \(}\). In the nonstationary setting, this support matrix becomes a function of the domain index \(u\), denoted as \(_{u}\) and \(}_{u}\). Additionally, we introduce the concept of weakly diverse lossy transitions for the data generation process, which is formally defined below:

**Definition 6** (Weakly Diverse Lossy Transition).: _The set of transition functions described in Eq. (2) is said to be diverse lossy if it satisfies the following conditions:_

1. _(Lossy) For every time and indices tuple_ \((t,i,j)\) _with edge_ \(z_{t-1,i} z_{t,j}\) _representing a causal link defined with the parents set_ \((z_{t,j})\) _in Eq._ 2_, transition function_ \(m_{j}\) _is a lossy transformation w.r.t._ \(z_{t-1,i}\) _i.e., there exists an open set_ \(S_{t,i,j}\)_1, changing_ \(z_{t-1,i}\) _within this set will not change the value of_ \(m_{j}\)_, i.e._ \( z_{t-1,i} S_{t,i,j}\)_,_ \(}{ z_{t-1,i}}=0\)_._ 2. _(Weakly Diverse) For every element_ \(z_{t-1,i}\) _of the latent variable_ \(_{t-1}\) _and its corresponding children set_ \(_{t,i}=\{j z_{t-1,i}(z_{t,j}),j\{1,2,,n\}\}\)_, transition functions_ \(\{m_{j}\}_{j_{t,i}}\) _are weakly diverse i.e., the intersection of the sets_ \(S_{t,i}=_{j_{t,i}}S_{t,i,j}\) _is not empty, and such sets are diverse, i.e.,_ \(S_{t,i}\)_, and_ \(S_{t,i,j} S_{t,i}, j_{t,i}\)_._

### Identifiability of Domain Variables

**Theorem 1** (Identifiability of Domain Variables).: _Suppose that the dataset \(\) are generated from the nonstationary data generation process as described in Eqs. (1) and (2). Suppose the transitions are weakly diverse lossy (Def. 6) and the following assumptions hold:_

1. _(Mechanism Separability) There exists a ground truth mapping_ \(:\) _determined the real domain indices, i.e.,_ \(u_{t}=(_{t-1},_{t})\)_._
2. _(Mechanism Sparsity) The estimated transition complexity on dataset_ \(\) _is less than or equal to ground truth transition complexity, i.e.,_ \(_{}|}_{i}|_{}|_{u}|\)_._
3. _(Mechanism Variability) Mechanisms are sufficiently different. For all_ \(u u^{}\)_,_ \(_{u}_{u^{}}\) _i.e. there exists index_ \((i,j)\) _such that_ \([_{u}]_{i,j}[_{u^{}}]_{i,j}\)_._

_Then the domain variables \(u_{t}\) is identifiable up to label swapping (Def. 2)._

Theorem 1 states that if we successfully learn a set of estimated transitions \(\{}_{u}\}_{u=1}^{U}\), the decoder \(}\), and the domain clustering assignment \(}\), where \(}_{u}\) corresponds to the estimation of Eq. (2) for a particular regime or domain \(u\), and the system can fit the data as follows:

\[}_{t}=}}_{u_{t}}}^{-1}(_{t-1})_{t}=}(_{t-1},_{t}), \]

assuming that the transition complexity is kept low (as per Assumption ii). Then the estimated domain variables \(_{t}\) must be the true domain variables \(u_{t}\) up to a permutation.

**Proof sketch** The core idea of this proof is to demonstrate that the global minimum of transition complexity can only be achieved when the domain variables \(u_{t}\) are correctly estimated. (1) First, we consider the case when we have an optimal decoder estimation \(}^{*}\) which is a component-wise transformation of the ground truth, incorrect estimations of \(u_{t}\) will strictly increase the transition complexity, i.e., \(_{}|}_{i}^{*}|>_{}|}_{u}^{*}|\). (2) Second, we show that with arbitrary estimations \(_{t}\), the transition complexity for any non-optimal decoder estimation \(}\) will be equal to or higher than that for the optimal \(}^{*}\), i.e., \(_{}|}_{i}|_{}|}_{i}^{*}|\). Thus, the global minimum of transition complexity can only be achieved when \(u_{t}\) is optimally estimated, which must be a permuted version of the ground truth domain variables \(u_{t}\). A comprehensive proof can be found in Appendix S1.1.

### Remark on Mechanism Variability

The assumption of mechanism variability, as stated in Assumption iii, requires that the Jacobian support matrices differ across domains, indicating that the causal graph connecting past states (\(_{t-1}\)) to current states (\(_{t}\)) must differ by at least one edge. Addressing scenarios where the causal graphs remain identical but the transition functions associated with the edges vary is generally challenging without imposing additional assumptions. A more detailed discussion of the difficulties involved in such cases is provided in Appendix S1.4.4. To effectively address these scenarios, we extend the concept of the Jacobian support matrix by incorporating higher-order derivatives. This extensionprovides a more detailed characterization of the variability in transition functions across different domains. We now present the following definition to formalize this concept:

**Definition 7** (Higher Order Partial Derivative Support Matrix).: _The \(k\)-th order partial derivative support matrix for transition \(\) denoted as \(^{k}\) is a binary \(n n\) matrix with_

\[[^{k}]_{i,j}=1,m_{j}}{ z_{i}^{k}} 0. \]

We utilize the variability in the higher-order partial derivative support matrix to extend the identifiability results of Theorem 1. This extension applies to cases where the causal graphs remain identical across two domains, yet the transition functions take different forms.

**Corollary 1** (Identifiability under Function Variability).: _Suppose the data \(\) is generated from the nonstationary data generation process described in (1) and (2). Assume the transitions are weakly diverse lossy (Def. 6), and the mechanism separability assumption \(i\) along with the following assumptions hold:_

1. _(Mechanism Function Variability) Mechanism Functions are sufficiently different. There exists_ \(K\) _such that for all_ \(u u^{}\)_, there exists_ \(k K\)_,_ \(^{k}_{u}^{k}_{u^{}}\) _i.e. there exists index_ \((i,j)\) _such that_ \([^{k}_{u}]_{i,j}[^{k}_{u^{}} ]_{i,j}\)_._
2. _(Higher Order Mechanism Sparsity) The estimated transition complexity on dataset_ \(\) _is no more than ground truth transition complexity,_ \[_{}_{k=1}^{K}|}^{k}_{u}| _{}_{k=1}^{K}|^{k}_{u}|.\] (9)

_Then the domain variables \(u_{t}\) are identifiable up to label swapping (Def. 2)._

To prove this corollary, we leverage the property that, for any two distinct domains, there exists an edge in the causal graph such that the supports of their \(k\)-th order partial derivatives differ. This difference ensures the separability of the two domains. A detailed proof can be found in Appendix S1.2.

### Identifiability of Latent Causal Process

Once the identifiability of \(u_{t}\) is achieved, the problem reduces to a nonstationary temporal nonlinear ICA with _observed_ domain index. Leveraging the sufficient variability approach proposed in , we demonstrate full identifiability of the data generation process. This sufficient variability concept is further incorporated into the following lemma, adapted from Theorem 2 in :

**Lemma 1** (Theorem 2 in Yao et al., ).: _Suppose that the data \(\) are generated from the nonstationary data generation process as described in Eqs. (1) and (2). Let \(_{kt}(u)\) denote the logarithmic density of \(k\)-th variable in \(_{t}\), i.e., \(_{kt}(u) p(z_{t,k}|_{t-1},u)\), and there exists an invertible function \(}\) that maps \(_{t}\) to \(}_{t}\), i.e., \(}_{t}=}(_{t})\) such that the components of \(}_{t}\) are mutually independent conditional on \(}_{t-1}\). (Sufficient variability) Let_

\[_{k,t}(u)_{kt}(u )}{ z_{t,k} z_{t-1,1}},_{kt}(u)}{  z_{t,k} z_{t-1,2}},...,_{kt}(u)}{  z_{t,k} z_{t-1,n}}^{}, \] \[}_{k,t}(u) _{kt}(u)}{ z_{t,k}^{2} z_{t-1,1}},_{kt}(u )}{ z_{t,k}^{2} z_{t-1,2}},...,_{kt}(u) }{ z_{t,k}^{2} z_{t-1,n}}^{}. \]

\[_{kt}_{kt}(1)^{},..., _{kt}(U)^{},_{kt}(2)}{ z_{t,k}^{2}}- _{kt}(1)}{ z_{t,k}^{2}},..., _{kt}(U)}{ z_{t,k}^{2}}-_{kt}(U-1)}{  z_{t,k}^{2}}^{}, \]

\[}_{kt}}_{kt}(1)^{},..., }_{kt}(U)^{},(2)}{ z_{ t,k}}-(1)}{ z_{t,k}},...,(U)}{ z_{t,k}}-(U-1)}{ z_{t,k}} ^{}. \]

_Suppose \(_{t}=(_{t})\) and that the conditional distribution \(p(z_{k,t}\,|\,_{t-1})\) may change across \(m\) domains. Suppose that the components of \(_{t}\) are mutually independent conditional on \(_{t-1}\) in each context. Assume that the components of \(}_{t}\) produced by \(}\) are also mutually independent conditional on \(}_{t-1}\). If the \(2n\) function vectors \(_{k,t}\) and \(}_{k,t}\), with \(k=1,2,...,n\), are linearly independent, then \(}_{t}\) is a permuted invertible component-wise transformation of \(_{t}\)._Then, in conjunction with Theorem 1, complete identifiability is achieved for both the domain variables \(u_{t}\) and the independent components \(_{t}\). See detailed proof in Appendix S1.3.

**Theorem 2** (Identifiability of the Latent Causal Processes).: _Suppose that the observed dataset \(\) is generated from the nonstationary data generation process as described in Eqs. (1) and (2), which satisfies the conditions in both Theorem 1 and Lemma 1, then the domain variables \(u_{t}\) are identifiable up to label swapping (Def. 2) and latent causal process \(_{t}\) are identifiable up to permutation and a component-wise transformation (Def. 3)._

**Discussion on Assumptions** The proof of Theorem 1 relies on several essential assumptions that correspond with human intuition regarding domain transitions. First, the assumption of _separability_ posits that if human observers are unable to differentiate between two domains, it is improbable that automated systems will achieve such a distinction. Second, the _variability_ assumption requires that the differences in transitions between domains be substantial enough to be perceptible to humans. This often results in changes to the temporal causal structure across domains, indicating that at least one edge in the causal graph must differ between the domains.

The mechanism _sparsity_ is a standard assumption that has been previously explored in [33; 19; 18] using sparsity regularization to enforce the sparsity of the estimated function. The assumption of _weakly diverse lossy transitions_ is a mild and realistic condition in real-world scenarios, allowing for identical future latent states with differing past states. The _sufficient variability_ in Theorem 2 is widely explored and adopted in nonlinear ICA literature [12; 22; 23; 25; 26]. For a more detailed discussion of the feasibility and intuition behind these assumptions, we refer the reader to the Appendix S1.4.

## 4 The CtrlNS Framework

### Model Architecture

Our framework builds on VAE [34; 35] architecture, incorporating dedicate modules to handle nonstationarity. It enforces the conditions discussed in Sec. 3 as constraints. As shown in Fig. 2, the framework consists of three primary components: (1) Sparse Transition, (2) Prior Network, and (3) Encoder-Decoder.

**Sparse Transition** The transition module in our framework is designed to estimate transition functions \(\{}_{u}\}_{u=1}^{U}\) and a clustering function \(}\) as specified in Eq. (7). As highlighted in Sec. 3, the primary objective of this module is to model the transitions in the latent space and minimize the empirical transition complexity. To achieve this, we implemented \(U\) different transition networks for various \(}(_{t},)\) and added sparsity regularization to the transition functions via a sparsity loss. A gating function with a (hard)-Gumbel-Softmax function was used to generate \(_{t}\), which was then employed to select the corresponding transition network \(}_{_{t}}\). This network was further used to calculate the transition loss, which is explained in detail in Sec. 4.2.

**Prior Network** The Prior Network module aims to effectively estimate the prior distribution \(p(_{t,i}\,|\,}_{t-1},_{t})\). This is achieved by evaluating \(p(_{t}\,|\,}_{t-1},_{t})=p_{_{i}}( _{i}^{-1}(_{t},_{t,i},}_{t-1}))| _{i}^{-1}}{_{t,i}}|\), where \(_{i}^{-1}(_{t},)\) is the learned holistic inverse dynamics model. To ensure the conditional independence of the estimated latent variables, \(p(}_{t}\,|\,}_{t-1})\), we utilize an isomorphic noise distribution for \(\) and aggregate all estimated component densities to obtain the joint distribution \(p(}_{t}\,|\,}_{t-1},_{t})\) as shown in Eq. (14). Given the lower-triangular nature of the Jacobian, its determinant can be computed as the product of its diagonal terms. Detailed derivations is provided in Appendix S3.1.

\[ p(}_{t}\,|\,}_{t-1},_{t} )=^{n} p(_{i}\,|\,\,_{t} )}_{}+^{n}|_{i}^{-1}}{_{t,i}}|}_{} \]

Figure 2: Illustration of **CtrlNS** with (1) Sparse Transition, (2) Prior Network, (3) Encoder-Decoder Module.

**Encoder-Decoder** The third component is an Encoder-Decoder module that utilizes reconstruction loss to enforce the invertibility of the learned mixing function \(}\). Specifically, the encoder fits the demixing function \(}^{-1}\) and the decoder fits the mixing function \(}\).

### Optimization

The first training objective of **CtrlNS** is to fit the estimated transitions with minimum transition complexity according to Eq. (7):

\[_{}_{}L( }_{a_{t}}(}_{t-1}),}_{t})}_{ {Transition Loss}}+_{}[}_{ }]}_{}, \]

where \(L(,)\) is a regression loss function to fit the transition estimations, and the sparsity loss is approximated via \(L_{2}\) norm of the parameter in the transition estimation functions.

Then the second part is to maximize the Evidence Lower BOund (ELBO) for the VAE framework, which can be written as follows (complete derivation steps are in Appendix S3.2):

\[_{_{t}}_{t=1}^{T} p_{}(_{t}_{t})+^{T} p_{}(_{t}_{t-1},u_{t})-_{t=1}^{T} q_{}( _{t}_{t})}_{-_{}} \]

We use mean-squared error for the reconstruction likelihood loss \(_{}\). The KL divergence \(_{}\) is estimated via a sampling approach since with a learned nonparametric transition prior, the distribution does not have an explicit form. Specifically, we obtain the log-likelihood of the posterior, evaluate the prior \( p(}_{t}}_{t-1},_{t})\) in Eq. (14), and compute their mean difference in the dataset as the KL loss: \(_{}=_{}_{t} q(}_{t}_{t})} q(}_{t}_{t})- p (}_{t}}_{t-1},_{t})\).

## 5 Experiments

We assessed the identifiability performance of **CtrlNS** on both synthetic and real-world datasets. For synthetic datasets, where we control the data generation process completely, we conducted a comprehensive evaluation. This evaluation covers the full spectrum of unknown nonstationary causal temporal representation learning, including metrics for both domain variables and the latent causal processes. In real-world scenarios, **CtrlNS** was employed in video action segmentation tasks. The evaluation metrics focus on the accuracy of action estimation for each video frame, which reflects the identifiability of domain variables.

### Synthetic Experiments on Causal Representation Learning

**Evaluation Metrics** For domain variables, we assessed the _clustering accuracy_ (**Acc**) to estimate discrete domain variables \(u_{t}\). As the label order in clustering algorithms is not predetermined, we selected the order that yielded the highest accuracy score. For the latent causal processes, we computed the _mean correlation coefficient_ (**MCC**) between the estimated latent variables \(}_{t}\) and the ground truth \(_{t}\). The MCC, a standard measure in the ICA literature for continuous variables, assesses the identifiability of the learned latent causal processes. We adjusted the reported MCC values in Table 1 by multiplying them by 100 to enhance the significance of the comparisons.

**Baselines** We compared our method with identifiable nonlinear ICA methods: (1) BetaVAE , which ignores both history and nonstationarity information. (2) i-VAE  and TCL , which leverage nonstationarity to establish identifiability but assume independent factors. (3) SlowVAE  and PCL , which exploit temporal constraints but assume independent sources and stationary processes. (4) TDRL , which assumes nonstationary causal processes but with observed domain indices. (5) HMNLICA , which considers the unobserved nonstationary part in the data generation process but does not allow any causally related time-delayed relations. (6) NCTRL , which extends HMNLICA to an autoregressive setting to allow causally related time-delayed relations in the latent space but still assumes a Markov chain on the domain variables.

**Result and Analysis** We generate synthetic datasets that satisfy our identifiability conditions in Theorems 1 and 2, detailed procedures are in Appendix S2.1. The primary findings are presented in Table 1. Note: the MCC metric is consistently available in all methods; however, the Acc metric for \(u_{t}\) is only applicable to methods capable of estimating domain variables \(u_{t}\).

In the first row of Table 1, we evaluated a recent nonlinear temporal ICA method, TDRL, providing ground truth \(u_{t}\) to establish an upper performance limit for the proposed framework. The high MCC (> 0.95) indicates the model's identifiability. Subsequently, the table lists six baseline methods that neglect the nonstationary domain variables, with none achieving a high MCC. The remaining approaches, including our proposed **CtrlNS**, are able to estimate the domain variables \(u_{t}\) and recover the latent variables. In particular, HMNLICA exhibits instability during training, leading to considerable performance variability. This instability stems from HMNLICA's inability to allow time-delayed causal relationships among hidden variables \(_{t}\), leading to model training failure when the actual domain variables deviate from the Markov assumption. In contrast, NCTRL, which extends TDRL under the same assumption, demonstrates enhanced stability and performance over HMNLICA by accommodating transitions in \(_{t}\). However, since they use incorrect assumption on the nonstationary domain variables, the performance of those methods can be even worse than methods which do not include the domain information. Nevertheless, considering the significant nonstationarity and deviation from the Markov properties, those methods struggled to robustly estimate either the domain variables or the latent causal processes. Compared to all baselines, our proposed **CtrlNS** reliably recovers both \(u_{t}\) (MCC > 0.95) and \(_{t}\) (Acc > 95%), and the MCC is on par with the upper performance bound when domain variables are given, justifying it effectiveness.

#### Detailed Training Analysis

To further validate our theoretical analysis, we present a visualization of the entire training process for **CtrlNS** in Figure 3. It consists of three phases: (1) In Phase 1, the initial estimations for both \(u_{t}\) and \(_{t}\) are imprecise. (2) During Phase 2, the accuracy of the estimation of \(u_{t}\) continues to improve, although the quality of the estimation of \(_{t}\) remains relatively unchanged compared with Phase 1. (3) In Phase 3, as \(u_{t}\) becomes clearly identifiable, the MCC of \(_{t}\) progressively improves, ultimately achieving full identifiability. This three-phase process aligns perfectly with our theoretical predictions. According to Theorem 1, phases 1 and 2 should exhibit suboptimal \(_{t}\) estimations, while sparsity constraints can still guide training and improve the accuracy for domain variables \(u_{t}\). Once the accuracy of \(u_{t}\) approaches high, Theorem 2 drives the improvement in MCC for \(_{t}\) estimations, leading to the final achievement of full identifiability of both latent causal processes for \(_{t}\) and domain variables \(u_{t}\).

### Real-world Application on Weakly Supervised Action Segmentation

**Experiment Setup** Our method was tested on the video action segmentation task to estimate actions (domain variables \(u_{t}\)). Following [37; 28], we use the same weakly supervised setting utilizing meta-information, such as action order. The evaluation included several metrics: _Mean-over-Frames_ (**MoF**), the percentage of correctly predicted labels per frame; _Intersection-over-Union_ (**IoU**), defined

   \(u_{t}\) & **Method** & \(_{t}\)**MCC** & \(u_{t}\)**Acc** (\%) \\  Ground Truth & TDRL(GT) & 96.93 \(\) 0.16 & - \\   & TCL & 24.19 \(\) 0.85 &  \\  & PCL & 38.46 \(\) 6.85 & \\  & BetaVAE & 42.37 \(\) 1.47 & \\  & SlowVAE & 41.82 \(\) 2.55 & \\  & i-VAE & 81.60 \(\) 2.51 & \\  & TDRL & 53.45 \(\) 1.31 & \\   & HMNLICA & 17.82 \(\) 30.87 & 13.67 \(\) 23.67 \\  & NCTRL & 47.27 \(\) 2.15 & 34.94 \(\) 4.20 \\  & **CtrlNS** & **96.74 \(\) 0.17** & **98.21 \(\) 0.05** \\   

Table 1: Experiment results of synthetic dataset on baseline models and the proposed **CtrlNS**. All experiments were conducted using three different random seeds to calculate the average and standard deviation. The best results are highlighted in **bold**.

Figure 3: Visualization of three phase training process of **CtrlNS**. Approaches high, Theorem 2 drives the improvement in MCC for \(_{t}\) estimations, leading to the final achievement of full identifiability of both latent causal processes for \(_{t}\) and domain variables \(u_{t}\).

as \(|I I^{*}|/|I I^{*}|\); and _Intersection-over-Detection_ (**IoD**), \(|I I^{*}|/|I|\), \(I^{*}\) and \(I\) are the ground-truth segment and the predicted segment with the same class.

**Datasets** Our evaluation used two datasets: Hollywood Extended , which includes 937 videos with 16 daily action categories, and CrossTask , focusing on 14 of 18 primary tasks related to cooking , comprising 2552 videos across 80 action categories.

**Model Design** Our model is build on top of ATBA  method which uses multi-layer transformers as backbone networks. We add our sparse transition module with the sparsity loss function detailed in Sec. 4.2. Specifically, we integrated a temporally latent transition layer into ATBA's backbone, using a transformer layer across time axis for the Hollywood dataset and an LSTM for the CrossTask dataset. To encourage sparsity in the latent transitions, \(L_{2}\) regularization is applied to the weights of the temporally latent transition layer.

**Result and Analysis** The primary outcomes for real-world applications in action segmentation are summarized in Table 2. Traditional methods based on hidden Markov models, such as HMM+RNN  and NN-Viterbi , face challenges in these real-world scenarios. This observation corroborates our previous discussions on the limitations of earlier identifiability methods , which depend on the Markov assumption for domain variables. Our approach significantly outperforms the baselines in both the Hollywood and CrossTask datasets across most metrics. Especially in the Hollywood dataset, our method outperforms the base ATBA model by quite a large margin. Notably, the Mean-over-Frames (**MoF**) metric aligns well with our identifiability results for domain variables \(u_{t}\). Our method demonstrates substantial superiority in this metric. For Intersection-over-Union (**IoU**) and Intersection-over-Detection (**IoD**), our results are comparable to those of the baseline methods in the CrossTask dataset and show its superiority in the Hollywood dataset. Furthermore, our proposed sparse transition module which aligns with human intuition and is easily integrated into existing methods like a plug-in module, thus further enhancing its impact in real-world scenarios.

To make the illustration more straightforward, some example segmentation results from the Hollywood dataset are visualized in Figure 4. By comparing the number of distinct actions and the

  
**Dataset** & **Method** & **MoF** & **IoU** & **IoD** \\   & HMM+RNN  & - & 11.9 & - \\  & CDFL  & 45.0 & 19.5 & 25.8 \\  & TASL  & 42.1 & 23.3 & 33 \\  & MuCon  & - & 13.9 & - \\  & ATBA  & 47.7 & 28.5 & 44.9 \\   & **CtrlNS (Ours)** & **52.9\({}_{ 3.1}\)** & **32.7\({}_{ 1.3}\)** & **52.4\({}_{ 1.8}\)** \\   & NN-Viterbi  & 26.5 & 10.7 & 24.0 \\  & CDFL  & 31.9 & 11.5 & 23.8 \\   & TASL  & 40.7 & 14.5 & **25.1** \\   & POC  & 42.8 & 15.6 & - \\   & ATBA  & 50.6 & **15.7** & 24.6 \\    & **CtrlNS (Ours)** & **54.0\({}_{ 0.9}\)** & **15.7\({}_{ 0.5}\)** & 23.6\({}_{ 0.8}\) \\   

Table 2: Real-world experiment result on action segmentation task. We use the reported value for the baseline methods from . Best results are highlighted in **bold**.

Figure 4: Two illustrative visualizations of the action segmentation task on the Hollywood dataset are presented. The colors represent the ground truth and the predicted action labels for each frame, as produced by the baseline ATBA and our proposed **CtrlNS**.

segmentation boundaries between our method and the baseline, it is evident that our **CtrlNS** estimates the actions more accurately, demonstrating improved performance.

**Abalation Study** Furthermore, we conducted an ablation study on the sparse transition module, as detailed in Table 3. In this study, we test on a subset of Hollywood dataset for computational efficiency. For methods we compared, "- Complexity" refers to the configuration where we retain the latent transition layers but omit the sparse transition complexity regularization term from these layers, and "- Module" indicates the removal of the entire sparse transition module, effectively reverting the model to the baseline ATBA model. The comparative results in Table 3 demonstrate that both the dedicated design of the sparse transition module and the complexity regularization term enhance the performance.

## 6 Conclusion

In this study, we developed a comprehensive identifiability theory tailored for general sequential data influenced by nonstationary causal processes under unspecified distributional changes. We then introduced **CtrlNS**, a principled approach to recover both latent causal variables with their time-delayed causal relations, as well as determining the values of domain variables from observational data without relying on distributional or structural prior knowledge. Our experimental results demonstrate that **CtrlNS** can reliably estimate the domain indices and recover the latent causal process. And such module can be easily adapted to handle real-world scenarios such as action segmentation task.

## 7 Limitations

As noted in Sec. 3.2, our main theorem relies on the condition that causal graphs among different domains must be distinct. Although our experiments indicate that this assumption is generally sufficient, there are scenarios in which it may not hold, meaning that the transition causal graphs are identical for two different domains, but the actual transition functions are different. We have addressed this partially through an extension to the mechanism variability assumption to higher-order cases (Corollary 1). However, dealing with situations where transition graphs remain the same across all higher orders remains a challenge. We acknowledge this as a limitation and suggest it as an area for future exploration. We also observed that the random initialization of the nonlinear ICA framework can influence the total number of epochs needed to achieve identifiability, as illustrated in Figure 3. Also, for the computational efficiency, the TDRL framework we adopted involves a prior network that calculated each dimension in the latent space one by one, thus making the training efficiency suboptimal. Since this is not directly related to major claim which is our sparse transition design, we acknowledge this as a limitation and leave it for future work.

## 8 Boarder Impacts

This work proposes a theoretical analysis and technical methods to learn the causal representation from time-series data, which facilitate the construction of more transparent and interpretable models to understand the causal effect in the real world. This could be beneficial in a variety of sectors, including healthcare, finance, and technology. In contrast, misinterpretations of causal relationships could also have significant negative implications in these fields, which must be carefully done to avoid unfair or biased predictions.

## 9 Acknowledgment

The authors would like to thank the anonymous reviewers for helpful comments and suggestions during the reviewing process. The authors would also like to acknowledge the support from NSF Award No. 2229881, AI Institute for Societal Decision Making (AI-SDM), the National Institutes of Health (NIH) under Contract R01HL159805, and grants from Salesforce, Apple Inc., Quris AI, and Florin Court Capital.

  
**Method** & **MoF** & **IoU** & **IoD** \\ 
**CtrlNS** & **52.9** & **32.7** & **52.4** \\ - Complexity & 50.5 & 31.5 & 51.5 \\ - Module & 47.7 & 28.5 & 44.9 \\   

Table 3: Ablation study on sparse transition module in Hollywood dataset.