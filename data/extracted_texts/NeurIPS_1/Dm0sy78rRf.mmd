# Gaussian Process Conjoint Analysis for Adaptive Marginal Effect Estimation

Yehu Chen, Jacob Montgomery, Roman Garnett

Washington University in St Louis

chenyehu,jacob.montgomery,garnett@wustl.edu

###### Abstract

Choice-based conjoint analysis is an essential tool for learning the marginal effects of multidimensional explanatory features on preferences. However, existing marginal effect models rely either on non-parametric estimators that generalize poorly to individualized effects or on linear latent utility models that completely ignore possible higher-order interactions. We introduce Gaussian Process Conjoint Analysis (gpca) for learning marginal effects from observed choices as the first-order derivatives of unknown systems. Additionally, we propose a Gaussian mixture approximation for the predictive distributions of marginal effects, which facilitates downstream tasks such as adaptive experimentation. Our experiments show that gpca achieves more precise and efficient estimation of marginal effects.

## 1 Introduction

Understanding the relationship between targeted outcomes and features in survey experiments is fundamental in many disciplines, including social science , human-computer interaction , and marketing research . These associations are often captured by marginal effects, defined as the change in predicted outcomes resulting from changes in features. Depending on the type of attributes, marginal effects can either be computed as the discrete change in outcomes for categorical attributes or as infinitesimal margins for continuous attributes. In survey experiments, marginal effects are often estimated using choice-based conjoint experiments, which present a series of profile pairs with varying attribute values to compare differences in averaged outcomes . For example, researchers might alternate background characteristics to study bias toward immigrants, or system designers might adjust interface setups to improve click-through rates for a new web interface. However, the multidimensional nature of conjoint experiments makes the estimation of heterogeneous marginal effects challenging and not scalable due to interactions with other attributes and small-sample biases. Existing estimators either rely on stacking multiple attributes in a difference-in-differences style  or on linear utility theory, which overlooks possible feature expansions .

We propose Gaussian Process Conjoint Analysis (gpca), which automatically learns higher-order interactions within the preference learning framework. Marginal effects are derived using the first-order derivatives of a Gaussian Process trained on observed preferences, and their distributions are approximated via Gaussian mixture models. Additionally, we leverage adaptive experimentation for data collection, balancing between exploiting attributes that are more crucial to preferences and exploring attributes where the model exhibits uncertainty, guided by a predictive belief model of the system. As demonstrated in simulated experiments, gpca achieves more precise estimation of marginal effects compared to other non-parametric and parametric methods.

Related work

**Marginal effects** are a common quantitative method for understanding transformed features in regression models  or examining heterogeneous associations between features and outcomes . For instance, marginal effects have been extensively studied in economics to measure the responsiveness of economic variables through the concept of elasticity , which quantifies how a percentage change in price corresponds to a percentage change in demand quantity. Marginal effects have also been utilized to enhance the interpretability of machine learning models. Silva Filho et al.  proposed a feature importance method for interpreting classification models based on marginal local effects. Merz et al.  introduced a marginal attribution method that conditions on quantiles to analyze global gradients in deep neural networks. Scholbeck et al.  presented forward marginal effects as a unified, mixed-type feature interpretation method for non-linear machine learning models.

**Conjoint analysis** is an experimental design used to elicit user preferences in recommendation systems [10; 18] and has been widely adopted in quantitative research to learn marginal effects by randomizing over attribute profiles [1; 2]. Existing estimators are predominantly non-parametric and primarily focus on discrete attributes. Hainmueller et al.  proposed a difference-in-differences interaction effect estimator to infer preferences from multidimensional choices in survey experiments. Similarly, Egami and Imai  introduced a new effect estimator for factorial experiments that does not rely on baseline conditions and generalizes effectively to higher-order interaction effects. In contrast, conjoint analysis involving continuous attributes often employs parametric latent utility functions, such as generalized logistic regression , support vector machines [9; 8; 11], Gaussian Processes [19; 20; 21; 22], and decision trees . However, these methods are primarily designed for profile recommendation rather than marginal effect estimation.

**Adaptive experimentation** leverages previously collected responses to inform experiment design or data acquisition in subsequent iterations, aiming to maximize the utility of limited data. This approach has been widely adopted by domain scientists to accelerate scientific discovery. For example, Bayesian optimization through adaptive sample selection has been successfully applied in materials science for discovering new materials  and in clinical trials for determining the maximum tolerated dose [24; 25]. Similarly, active search has been utilized for the iterative design of virtual screening trials in chemoinformatics . In machine learning, Chen et al.  explored the pairwise ranking problem in a crowdsourcing setup using online learning. Biyik et al.  proposed an active, preference-based learning method based on information gain to optimize reward functions in robotics. However, prior adaptive designs in quantitative research have predominantly focused on treatment selection in bandit settings [29; 30; 31], with limited attention to marginal effect estimation, particularly within the gp preference learning framework.

## 3 Problem statement

We describe notations and define marginal effects in the setup of Gaussian process conjoint analysis, which are computed as the gradients of the preference probabilities w.r.t the profiles.

**Notations.** Formally, let \(^{d}\) represent the full profile of \(d\)-dimensional attributes, where \(l\) denotes the \(l\)th attribute and \(-l\) represents all attributes except the \(l\)th. For pairwise comparisons, let \(y_{ij} 0,1\) indicate whether the left-side profile \(^{(i)}\) is preferred over the right-side profile \(^{(j)}\), where \(y_{ij}=1\) if \(^{(i)}^{(j)}\) and \(y_{ij}=0\) otherwise. We focus on choice-based conjoint analysis with pairwise comparisons, as scenarios involving multiple choices can be decomposed into multiple pairwise comparisons. For example, if \(^{(i)}\) is the most preferred option among \(\{^{(i)},^{(j)},^{(k)}\}\), this can be expressed as \(^{(i)}^{(j)}\) and \(^{(i)}^{(k)}\). Similarly, our notation accommodates score-based conjoint experiments, where \(^{(i)}^{(j)}\) may indicate that \(^{(i)}\) has a higher score than \(^{(j)}\). Finally, suppose all observed preferences are collected into the dataset \(=\{(^{(i)},^{(j)}),y_{ij}\}\).

**Gaussian process conjoint analysis.** Conjoint analysis can be formulated as a preference learning problem involving a latent utility function \(u()\). The preferential relation between \(^{(i)}\) and \(^{(j)}\) is determined by comparing their utilities \(u(^{(i)})\) and \(u(^{(j)})\). Using a sigmoid probabilistic model \(()\), the probability of observed the preference \(p(^{(i)}^{(j)})\) is given by \(u(^{(i)})-u(^{(j)})\), allowing for potential labeling errors. Gaussian process conjoint analysis (gpca) assumes a gp prior on the latent utility \(u()(0,K)\), where \(K(x,x^{})=(-\|x-x^{}\|^{2}/2)\) is an RBF kernel. The observation model uses a cumulative standard normal function, with \(p^{(i)}^{(j)} u(^{(i)}),u(^ {(j)})=u(^{(i)})-u(^{(j)})\). Although the posterior of \(u()\) is not analytical in this classification setting, it can be approximated using standard methods such as Laplace approximation or expectation propagation . In some cases, the predictive probability is defined directly on pairs of profiles \((^{(i)},^{(j)})\) using the preference kernel \(K_{}(_{1}^{(i)},_{1}^{(j)}),(_ {2}^{(i)},_{2}^{(j)})=K(_{1}^{(i)},_{2}^ {(i)})-K(_{1}^{(i)},_{2}^{(j)})-K(_{2}^{(i)}, _{1}^{(j)})+K(_{1}^{(j)},_{2}^{(j)})\). We adopted this preference kernel in our implementation of gpca.

**Marginal effects in gpca**. We consider the _marginal_ effects of profile pairs \((^{(i)},^{(j)})\) as the first-order gradients of the preference probability w.r.t both profiles. Leveraging the affine property of Gaussian processes, the gradient \((^{(i)},^{(j)})\) of the probability of target profile \(^{(i)}\) being preferred to the opponent profile \(^{(j)}\) is (see Appendix for full mathematical details)

\[(^{(i)},^{(j)})=_{u| }u(^{(i)})-u(^{(j)}) u (^{(i)}),- u(^{(j)}) \]

Intuitively, marginal effects in the outcome space are computed as the expected gradient \( u(^{(i)}),- u(^{(j)})\) in the latent utility space, weighted by the densities \(u(^{(i)})-u(^{(j)})\) of a normal distribution at the latent utility difference \(u(^{(i)})-u(^{(j)})\). When projected along any unit vector \(}_{l}\), the averaged attribute-specific effect for attribute \(l\) on preferences over the profile distribution \(\) can be further expressed as \(_{l}(_{l}^{(i)})=_{(_{-l}^{(i)},^{(j)}) }(^{(i)},^{(j)}), }_{l}\).

## 4 Our approach: GMM estimator and adaptive experimentation

In this section, we describe two key elements of our approach: 1) a Gaussian Mixture Model (gmm) approximation to compute \((^{(i)},^{(j)})\), and 2) an adaptive experimentation strategy that efficiently selects profile pairs for comparison based on uncertainty in the latent utility or preference prediction.

**Gaussian mixture approximation of marginal effects.** Since \((^{(i)},^{(j)})\) involves taking weighted averages of \( u()\) over \(u()\), we propose the use of Gaussian mixture model (gmm) for approximation. Each component of the gmm is formed by scaling the multivariate Gaussian with the transformed values of quadrature points of the univariate Gaussian determined by Gauss-Hermite quadrature. Formally, let \(N\) be the number of points in the quadrature, \(k_{r}\) be the roots of the physicists' version of the Hermite polynomial \(H_{N}(k)\) and \(_{r}=N_{1}^{1}}{N^{2}[H_{N-1}(k_{r})]^{2}}\) be the weights of each component . Notice that the gradient of a differentiable gp is itself a gp, so one can first derive a joint posterior of utility function \(u()\) and its gradient \( u()\) from the induced prior \(u\\  u_{u}\\ _{u},K_{u}& K_{u}^{T}\\  K_{u}&^{2}K_{u}\) by conditioning on the preference observations. Under this joint posterior, \(,^{(j)})\) can then be approximated as \(_{r=1}^{N}_{r}_{r}() _{u}(),^{2}K_{u}(, )\). Here \(_{r}()=[_{u}^{2}()+_{u}^{2}( ^{(j)})]^{1/2}k_{r}+[_{u}()-_{u}(^{(j)})]\) are locations of mixture components defined on the sample point \(k_{r}\)s, and \(\) denotes the Hadamard (element-wise) product. Figure 1 shows the visualization of the proposed gmm for approximating one-side marginal effect. The left-hand side shows our gmm approximation of the one-sided marginal effect using 5 sampling points, and the

Figure 1: Visualization of the proposed gmm for approximating one-side marginal effect. Left figure shows our gmm approximation of the one-side marginal effect using 5 sampling points, and right figure shows 9 possible true effects obtained by numerical sampling. Darker colors indicate components with higher weights in the gmm and numerical samples closer to the one-side marginal effect posterior mode.

right-hand side shows 9 possible true effects obtained by numerical sampling. Darker colors indicate components with higher weights in the gmm and numerical samples closer to the one-side marginal effect posterior mode. We found \(N=10\) quadrature points sufficient for our gmm.

**Adaptive experimentation in gpca.** Informed by the posterior belief on the latent utility, adaptive experimentation may efficiently explore attributes whose marginal effects on preferences are less certain. Hence, we determine the next pairs of profiles to compare by maximizing an _acquisition function_\((_{*}^{(i)},_{*}^{(j)})=_{(^{(i)}, ^{(j)})}(^{(i)},^{(j)});\). For simplicity, let \(A=u(^{(i)})-u(^{(j)})\) and \(B=K_{u|}(^{(i)},^{(i)})+K_{u|}( ^{(j)},^{(j)})\). We consider the following policies: (1) _Upper confident bound_ (ucb) maximizes the 95% confidence interval of preference prediction: \((^{(i)},^{(j)});=A +1.96\); (2) _Differential entropy of the latent utility_ (de-u) maximizes the log variance of utility posterior: \((^{(i)},^{(j)});=(2 B)+\); (3) _Differential entropy of the marginal effects_ (de-me) maximizes their log variance: \((^{(i)},^{(j)});= _{k\{i,j\}}_{r=1}^{N}_{r}_{r}( ^{(k)})_{r}(^{(k)})^{T} ^{2}K_{u|}(^{(k)},^{(k)})\); (4) _Bayesian active learning by disagreement_ (bald) maximizes the mutual information between utility and predictive preferences: \((^{(i)},^{(j)});=( y_{ij},u;^{(i)},^{(j)},) h }-}}-}{2(B+C^{2})}\), with entropy function \(h(p)=-p(p)-(1-p)(1-p)\) and constant \(C=\). While ucb emphasizes _exploiting_ current belief to find the most preferred profile, de-u, de-me and bald focus on _exploring_ the profile space by reducing uncertainty on marginal effects and predictive preferences.

## 5 Experimental results

Our evaluation on gpca are based on synthetic data where the functional relations are known analytically. Experiments of two real-world datasets can be found in appendix.

**Data generating process**. Following the simulation specification in Chu and Ghahramani , we consider two datasets with discrete (2dplane) and continuous (friedman) attributes.1 The 2dplane dataset has \(5\) discrete attributes where \(x_{1}\{-1,1\}\) and \(x_{2},,x_{5}\{-1,0,1\}\), with a piecewise linear utility \(u()=1+2x_{2}-x_{3}\) if \(x_{1}=-1\) and \(u()=1+x_{4}-2x_{5}\) if \(x_{1}=1\). The friedman dataset has \(3\) continuous attributes where \(x_{1},,x_{3}\) with a non-linear utility \(u()=3( x_{1}x_{2})+6(x_{3}-0.5)^{2}\). We randomly sample \(1000\) pairs of profiles in each dataset and set \(y_{ij}=1\) with probability of \(u(^{(i)}-u(^{(j)})\) and \(y_{ij}=0\) otherwise. In adaptive experimentation, we initialize with the same \(25\) profile pairs, and updates model posterior every iteration. Both effects are estimated w.r.t the same target profile distribution to ensure comparability.

**Evaluation metrics and baselines.** We consider three metrics: (1) the rmse of the estimated effects, (2) the correlation (cor) between the estimated effects and true effects, and (3) the log likelihood (ll) of the estimated effects. We also compare our proposed gmm approximation for marginal effects in gpca to several baselines: (1) the non-parametric diff-in-mean estimator (dim) , where the continuous attributes in friedman are first discretized by splitting into equally-spanned intervals, (2) the standard preference learning method with linear utility (lm-gmm) [9; 10; 8; 11], and (3) an ablated gpca method (gp-map) but with map estimation of marginal effects.

    &  &  &  \\   & & &  \(\) &  &  &  \(\) &  &  \\   & DIM & 0.712\(\)0.022 & 0.013\(\)0.003 & \(-\)2.137\(\)0.115 & 0.109\(\)0.005 & 0.341\(\)0.029 & 0.494\(\)0.117 \\  & lm-gmm & 0.213\(\)0.001 & 0.340\(\)0.005 & \(-\)0.238\(\)0.145 & 0.069\(\)0.002 & 0.475\(\)0.019 & \(-\)0.778\(\)0.157 \\  & gp-map & 0.175\(\)0.002 & 0.732\(\)0.007 & \(-\)3.993\(\)0.

We then investigate adaptive experimentation in gpca for increasing efficiency of effect estimation. We consider various policies: (1) ucb popular in multi-arm bandit setting , (2) de-u and de-me for active learning based on differential entropy , (3) bald in Bayesian active learning for model uncertainty reduction  and (4) uniform design in non-parametric conjoint analysis .

**Results.** Table 1 shows the averaged rmse, correlation and log likelihood across 25 different seeds of the proposed gp-gmm and baselines, where gp-gmm consistently has more precise effect estimation with lower rmse and higher cor/ll for both effects. Figure 2 shows box plots of averaged rmse, cor and ll of marginal effect estimation with adaptive experimentation under different acquisition policies. Sample size range from \(50\) to \(150\), and performance metrics are reported every other \(25\) acquisitions. Overall bald (blue) outperforms the rest of policies including uniform and ucb, indicating higher efficiency for effect estimation when the acquisition is designed to reduce model uncertainty. Morever, ucb (forest green) has overall the worst performance in estimating both marginal and component effects as it solely reinforces current belief on the probability of preference. Results for component effect estimation can be found in Appendix.

Besides estimation of marginal effects, we also examine the model quality of gpca by evaluating the prediction accuracy of unrevealed preferences among the not acquired profile pairs. Figure 3 shows the averaged accuracy and stds of preference prediction by various policies. With as few as \(50\) data points, gpca manages to predict at least \(80\%\) of the unrevealed preference and \(95\%\) when \(150\) data points are adaptively acquired by bald.

## 6 Conclusion

We introduce gpca, a Gaussian Process conjoint analysis model for estimating marginal effects in choice-based conjoint experiments with a Gaussian mixture approximation for their distributions that could enhance precision and efficiency in effect estimation aided by adaptive experimentation. Experiments show that gpca achieves more precise and efficient estimation of marginal effects, suggesting great potential in either machine learning interpretation or industrial applications.

Figure 3: Averaged accuracy and stds of preference prediction by various policies for simulated data.

Figure 2: Box plots of averaged rmse, cor and ll of marginal effects under different acquisition policies.