# Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling

Hritik Bansal\({}^{1,2}\), Arian Hosseini\({}^{1,3}\), Rishabh Agarwal\({}^{1,3}\), Vinh Q. Tran\({}^{1}\), Mehran Kazemi\({}^{1}\)

\({}^{1}\) Google DeepMind, \({}^{2}\) UCLA, \({}^{3}\) Mila

###### Abstract

Training on high-quality synthetic data from strong language models (LMs) is a common strategy to improve the reasoning performance of LMs.In this work, we revisit whether this strategy is compute-optimal under a fixed inference budget (e.g., FLOPs). To do so, we investigate the trade-offs between generating synthetic data using a stronger but more expensive (SE) model versus a weaker but cheaper (WC) model. We evaluate the generated data across three key metrics: coverage, diversity, and false positive rate, and show that the data from WC models may have higher coverage and diversity, but also exhibit higher false positive rates. We then finetune LMs on data from SE and WC models in different settings: knowledge distillation, self-improvement, and a novel weak-to-strong improvement setup where a weaker LM teaches reasoning to a stronger LM. Our findings reveal that models finetuned on WC-generated data consistently outperform those trained on SE-generated data across multiple benchmarks and multiple choices of WC and SE models. These results challenge the prevailing practice of relying on SE models for synthetic data generation, suggesting that WC may be the compute-optimal approach for training advanced LM reasoners.

Figure 1: **Summary of the results.** (a) We finetune Gemma-7B, Gemma2-9B, and Gemma2-27B on the synthetic data collected from a stronger but more expensive LM (Gemma2-27B) and a weaker but cheaper LM (Gemma2-9B) in a compute-matched setup for the MATH dataset. We find that training with Gemma2-9B data is a more compute-optimal setting across diverse finetuning paradigms â€“ knowledge distillation, self-improvement, and weak-to-strong improvement (i.e. using a weaker model to improve a stronger model). (b) We finetune Gemma models (7B/9B/27B) on synthetic data generated by the state-of-the-art LMs Gemini-1.5-Pro and Gemini-1.5-Flash in a price-matched setup. We find that finetuning with Flash-generated data consistently outperforms Pro-generated data.

Introduction

Language models (LMs) have demonstrated impressive capabilities in reasoning tasks, but their success heavily relies on being trained on vast amounts of (problem, solution) pairs. Collecting this data from humans is a costly and time-consuming process. Recent studies have demonstrated the feasibility of synthetically generating this data using LMs themselves, offering a potentially more scalable and efficient approach to training data acquisition. One such widely-adopted approach is to sample multiple candidate solutions for a problem from an LM, filters them for final answer correctness, and finetune models on the correct solutions . Practitioners often sample solutions from strong LMs to ensure high quality . However, sampling from strong LMs is expensive which limits the number of solutions that can be generated for practical sampling budgets.

In this paper, we explore an alternative sampling approach. Given a fixed compute budget, we investigate sampling from a **weaker but cheaper (WC)** model as opposed to the commonly-used approach of sampling from a **stronger but more expensive (SE)** model. We start by comparing data from WC vs SE across three axes that play crucial roles in the utility of such synthetic data: 1-_coverage_, the number of unique problems that are solved, 2- _diversity_, the average number of unique solutions we obtain per problem, and 3- _false positive rate (FPR)_, the percentage of problems that arrive at the correct final answer but with a wrong reasoning. We find that since we can generate more samples from the WC model compared to the SE model under a fixed budget, the data from WC may exhibit higher coverage and diversity. However, due to the lower quality of the WC model, it may also have a higher FPR. As a particular example for the Gemma2 family  on the MATH dataset , Gemma2-9B achieves \(11\%\) higher coverage and \(86\%\) higher diversity, but also with \(7\%\) higher FPR compared to Gemma2-27B.

We then fine-tune models on data from SE and WC at a fixed compute budget (see Appendix Figure 3 for illustration). We consider diverse setups corresponding to three paradigms: 1) _knowledge distillation_, where a student LM learns from a teacher LM ; 2) _self-improvement_, where an LM learns from self-generated data ; and 3) a new paradigm we introduce called _Weak-to-Strong Improvement_, where a strong student LM improves using synthetic data from a weaker teacher LM. Using two (WC, SE) model pairs, one from the Gemma2 family and another from the Gemini-1.5 family , we show on multiple benchmarks that training on WC-generated data consistently outperforms training on SE-generated data under the three setups, with relative gains of up to \(31.6\%\) percent (see Figure 1 for a summary of the results). Our results indicate that it is more compute-optimal to sample from a WC model as opposed to the common-practice of sampling from a SE model. With the performance gap between small and large LMs getting narrower over time (especially at larger scales), our results establish a solid foundation for training the next generation of LM reasoners.

## 2 Compute-Matched Sampling and Training

We present the background on synthetic data generation and supervised finetuning in Appendix SSA. To generate synthetic solutions for the problems in the dataset, one can leverage different models as data generators. Specifically, at a fixed sampling budget (FLOPs), one can generate more samples from a weaker but cheaper (WC) model or fewer samples from a stronger but more expensive (SE) model. Given a WC model with \(P_{WC}\) parameters and SE with \(P_{SE}\) parameters, we compute the sampling ratio at a fix budget for the two models, focusing on decoder-only transformer models . Following , we note that the FLOPs per inference token is \(2P\), for a model with \(P\) parameters. As a result, the FLOPs for \(T\) inference tokens is \(2PT\). Further, we assume that generating each solution requires an average of \(W\) inference tokens for both models. Let \(S_{WC}\) and \(S_{SE}\) represent the number of samples we generate per question for the two models. The total cost of generating samples for the dataset \(\) will then be \(Cost_{WC}=n S_{WC} W(2P_{WC})\) and \(Cost_{SE}=n S_{SE} W(2P_{SE})\) for the cheap and expensive models, respectively. At a fixed sampling budget, we have:

\[n S_{WC} W(2P_{WC})=n S_{SE} W(2P_{SE}) =}{P_{WC}}S_{SE}} \]

Equation 1 indicates that at a fixed sampling budget, for each question we can generate \(P_{SE}/P_{WC}\) more samples from WC; the ratio scales linearly with the model parameters ratio. Sampling more solutions from WC may increase the likelihood of correctly solving a larger subset of the problems (high coverage) and obtaining more correct solutions per question (high diversity).

## 3 Experiments and Results

We experiment with MATH and GSM-8K datasets, and collect synthetic data from Gemma2-9B (WC) and Gemma2-27B (SE) models (see Appendix C for other experimental details).

### Synthetic Data Analysis

**Coverage:** Here, we aim to understand the pros and cons of generating solutions from the WC and SE models at a fixed sampling budget. We present the coverage, diversity, and FPR for the MATH at the low and high sampling budgets in Figure 2. The results for GSM-8K are presented in the Appendix - Figure 15. We find that in terms of coverage, the data from Gemma2-9B (WC) outperforms Gemma2-27B (SE) by \(11\%\) and \(6\%\) at the low and high sampling budgets, respectively, for the MATH dataset, and \(8\%\) and \(1\%\) for GSM-8K. This highlights that the higher number of samples for the WC model aids in solving more unique problems for both the reasoning datasets. We provide the coverage trends for diverse sampling budgets in Appendix J. Further, we provide a qualitative example that gets solved by repeated sampling from Gemma2-9B but remains unsolved by Gemma2-27B at the fixed high sampling budget (Table 5).

**Diversity:** The diversity for the data from Gemma2-9B is higher than Gemma2-27B by \(86\%\) and \(125\%\) at the low and high sampling budgets for the MATH dataset, and \(134\%\) and \(158\%\) at for the GSM-8K dataset. This implies that many unique reasoning chains in the synthetic data from the WC model lead to the correct solutions. We also observe that the absolute diversity scores are lower for MATH compared to GSM-8K at high sampling budget, indicating that models generate fewer correct solutions for the more challenging datasets when using repeated sampling.

**FPR:** Since we utilize the final answer correctness for filtering the synthetic data, it does not remove the solutions with incorrect intermediate reasoning steps. Our human evaluations suggest that the FPR for the WC-generated solutions is \(7\%\) and \(2\%\) higher than SE-generated solutions on the MATH and GSM-8K, respectively. The trends from the automatic evaluation are similar to that of human evaluation. Due to the differences in the difficulty of the problems, we note that the absolute FPRs are much lower for the GSM-8K dataset as compared to the MATH dataset. We also note that the automatic verification of the reasoning steps can also have errors and is still an open problem .

Given the mixed signals of high coverage and diversity coupled with a high FPR, it remains unclear whether it is compute-optimal to sample from the WC model or the SE model for training strong reasoners. We study this in the next section.

### Compute-Optimality Results for Training

**Student-LM Finetuning.** Here, we aim to understand the merit of WC data compared to SE data for distillation to a student LM (Gemma 7B in our experiment), given a fixed sampling budget. The results presented in Figure 0(a) show that the Gemma-7B finetuned with the synthetic data from WC consistently outperforms the one finetuned on data from SC. We observe relative gains of \(6\%\) for

Figure 2: **Synthetic data analysis for MATH dataset. The (a) coverage, (b) diversity, and (c) false positive rates for Gemma2-27B and Gemma2-9B on the MATH dataset, at two sampling budgets.**the MATH dataset. Contrary to the common belief of stronger models being better for knowledge distillation, our results indicate that finetuning on data from WC is more compute-optimal than SE.

**WC-LM Finetuning.** Prior work  has shown that finetuning a WC model through self-generated data lags behind distillation from SE data. Here, we revisit this setup under the fixed sampling budget and compare WC models finetuned with WC data vs SE data. Specifically, we compare the performance of Gemma2-9B finetuned with the WC data (i.e. self-generated data) and SE data (i.e. data from Gemma2-27B). In Figure 0(a), we observe that the self-generated data (WC data) improves over knowledge distillation from a strong model (SE data), achieving relative gains of \(3.8\%\) for the MATH dataset. Interestingly, our findings suggest that training a WC model on its own synthetic data is more compute-optimal than distillation from a stronger model.

**SE-LM finetuning.** It is commonly believed that to improve a SE model, we either need synthetic data from the SE model itself or from an even stronger (and perhaps more expensive) model than SE. Here, we test an alternative novel approach, which we term _weak-to-strong improvement (W2S-I)_, to understand whether the synthetic data from the WC model can improve the SE model. We present the results for finetuning Gemma2-27B with the Gemma2-9B generated data and self-generated data (Figure 0(a)). Surprisingly, we observe that the model finetuned with the WC data outperforms the SE data, achieving relative gains of \(5.8\%\) for the MATH dataset. Contrary to the common belief of self-generated data or data from a stronger model being better, our empirical findings show that training a model in a W2S-I setup from a WC data may be more compute-optimal than training it in a self-improvement setup on its own data. This result also establishes a new paradigm for improving frontier models in a compute-efficient way, by generating synthetic data from much smaller models.

We present the results for more sampling budgets and datasets in Appendix Figure 4 and 5. We present the generalization and ablation results in Appendix SSD and SE, respectively.

## 4 Scaling to state-of-the-art language models

We scale our method to sampling data from Gemini-1.5-Pro and Gemini-1.5-Flash. As the model sizes are not publicly available, we utilize the ratio between their _pricing per output token_ as a proxy to perform compute-matched sampling. As of August 2024, we note that the price per million output tokens is \(\$10.5\) and \(\$0.3\) for Gemini-1.5-Pro and Gemini-1.5-Flash, respectively. Hence, we sample \(1\) and \(35\) solutions per problem from 1.5-Pro and 1.5-Flash, respectively, for MATH dataset.

We perform knowledge distillation on the Gemma-7B, Gemma2-9B, and Gemma2-27B LMs with the synthetic data from the Pro (SE) and Flash (WC) models. We present the results in Figure 0(b). Interestingly, we find that finetuning with the WC data outperforms the SE data, achieving relative gains of \(31.6\%\), \(14.4\%\), and \(10.9\%\) for Gemma-7B, Gemma2-9B, and Gemma2-27B, respectively. This can be attributed to the difference in the coverage of the models at the fixed sampling budget, which is \(61.1\%\) and \(81\%\) for 1.5-Pro and 1.5-Flash, respectively.

Further, we investigate training the LMs with the WC data that is less expensive than collecting \(1\) solution per problem from the SE model. Specifically, we create a dataset by sampling \(5\) solutions per problem from the Flash (WC) model, which is \(7\) more economical than generating \(1\) solution from the Pro (SE) model, in terms of the price ($). Upon training the LMs on the \(0.15\)_cost_ data regime, according to Figure 7 in the Appendix, we find that training on this data can also outperform training with SC data, achieving relative gains of \(19.1\%\), \(9.8\%\), and \(5.7\%\) for finetuning Gemma-7B, Gemma2-9B, and Gemma2-27B, respectively. This can be attributed to higher coverage of the weaker model (\(69\%\)), even in the more economical scenario, in comparison to the stronger model (\(61.1\%\)).

## 5 Conclusion

In this work, we provide a framework for compute-optimal sampling from weak but cheap LM for reasoning tasks. We show that at a fixed sampling compute budget, repeated sampling from a smaller model can achieve higher coverage and diversity than from a strong but more expensive model. Furthermore, we find that finetuning LMs with data from the small LM can consistently outperform data from the large LM under the same compute budget. Our results can serve as a foundation for training LM reasoners, especially as the performance gap between small and large LMs continues to narrow over time.