# HippoRAG: Neurobiologically Inspired

Long-Term Memory for Large Language Models

 Bernal Jimenez Gutierrez

The Ohio State University

&Yiheng Shu

The Ohio State University

Yu Gu

The Ohio State University

&Michihiro Yasunaga

Stanford University

&Yu Su

The Ohio State University

###### Abstract

In order to thrive in hostile and ever-changing natural environments, mammalian brains evolved to store large amounts of knowledge about the world and continually integrate new information while avoiding catastrophic forgetting. Despite their impressive accomplishments, large language models (LLMs), even with retrieval-augmented generation (RAG), still struggle to efficiently and effectively integrate a large amount of new experiences after pre-training. In this work, we introduce HippoRAG, a novel retrieval framework inspired by the hippocampal indexing theory of human long-term memory to enable deeper and more efficient knowledge integration over new experiences. HippoRAG synergistically orchestrates LLMs, knowledge graphs, and the Personalized PageRank algorithm to mimic the different roles of neocortex and hippocampus in human memory. We compare HippoRAG with existing RAG methods on multi-hop question answering (QA) and show that our method outperforms the state-of-the-art methods remarkably, by up to 20%. Single-step retrieval with HippoRAG achieves comparable or better performance than iterative retrieval like IRCoT while being \(10\)-\(20\) times cheaper and \(6\)-\(13\) times faster, and integrating HippoRAG into IRCoT brings further substantial gains. Finally, we show that our method can tackle new types of scenarios that are out of reach of existing methods.1

## 1 Introduction

Millions of years of evolution have led mammalian brains to develop the crucial ability to store large amounts of world knowledge and continuously integrate new experiences without losing previous ones. This exceptional long-term memory system eventually allows us humans to keep vast stores of continuously updating knowledge that forms the basis of our reasoning and decision making .

Despite the progress of large language models (LLMs) in recent years, such a continuously updating long-term memory is still conspicuously absent from current AI systems. Due in part to its ease of use and the limitations of other techniques such as model editing , retrieval-augmented generation (RAG) has become the _de facto_ solution for long-term memory in LLMs, allowing users to present new knowledge to a static model .

However, current RAG methods are still unable to help LLMs perform tasks that require integrating new knowledge across passage boundaries since each new passage is encoded in isolation. Many important real-world tasks, such as scientific literature review, legal case briefing, and medical diagnosis, require knowledge integration across passages or documents. Although less complex,standard multi-hop question answering (QA) also requires integrating information between passages in a retrieval corpus. In order to solve such tasks, current RAG systems resort to using multiple retrieval and LLM generation steps iteratively to join disparate passages [64; 78]. Nevertheless, even perfectly executed multi-step RAG is still oftentimes insufficient to accomplish many scenarios of knowledge integration, as we illustrate in what we call _path-finding_ multi-hop questions in Figure 1.

In contrast, our brains are capable of solving challenging knowledge integration tasks like these with relative ease. The hippocampal memory indexing theory , a well-established theory of human long-term memory, offers one plausible explanation for this remarkable ability. Teyler and Discenna  propose that our powerful context-based, continually updating memory relies on interactions between the neocortex, which processes and stores actual memory representations, and the C-shaped hippocampus, which holds the _hippocampal index_, a set of interconnected indices which point to memory units on the neocortex and stores associations between them [19; 76].

In this work, we propose HippoRAG, a RAG framework that serves as a long-term memory for LLMs by mimicking this model of human memory. Our novel design first models the neocortex's ability to process perceptual input by using an LLM to transform a corpus into a schemaless knowledge graph (KG) as our artificial hippocampal index. Given a new query, HippoRAG identifies the key concepts in the query and runs the Personalized PageRank (PPR) algorithm  on the KG, using the query concepts as the seeds, to integrate information across passages for retrieval. PPR enables HippoRAG to explore KG paths and identify relevant subgraphs, essentially performing multi-hop reasoning in a single retrieval step.

This capacity for _single-step multi-hop_ retrieval yields strong performance improvements of around \(3\) and \(20\) points over current RAG methods [10; 35; 53; 70; 71] on two popular multi-hop QA benchmarks, MuSiQue  and 2WikiMultiHopQA . Additionally, HippoRAG's online retrieval process is \(10\) to \(30\) times cheaper and \(6\) to \(13\) times faster than current iterative retrieval methods like IRCoT , while still achieving comparable performance. Furthermore, our approach can be combined with IRCoT to provide complementary gains of up to \(4\%\) and \(20\%\) on the same datasets and even obtain improvements on HotpotQA, a less challenging multi-hop QA dataset. Finally, we

Figure 1: **Knowledge Integration & RAG. Tasks that require knowledge integration are particularly challenging for current RAG systems. In the above example, we want to find a _Stanford_ professor that does _Alzheimer’s_ research from a pool of passages describing potentially thousands _Stanford_ professors and _Alzheimer’s_ researchers. Since current methods encode passages in isolation, they would struggle to identify _Prof. Thomas_ unless a passage mentions both characteristics at once. In contrast, most people familiar with this professor would remember him quickly due to our brain’s associative memory capabilities, thought to be driven by the index structure depicted in the C-shaped hippocampus above (in blue). Inspired by this mechanism, HippoRAG allows LLMs to build and leverage a similar graph of associations to tackle knowledge integration tasks.**

provide a case study illustrating the limitations of current methods as well as our method's potential on the previously discussed _path-finding_ multi-hop QA setting.

## 2 HippoRAG

In this section, we first give a brief overview of the hippocampal memory indexing theory, followed by how HippoRAG's indexing and retrieval design was inspired by this theory, and finally offer a more detailed account of our methodology.

### The Hippocampal Memory Indexing Theory

The hippocampal memory indexing theory  is a well-established theory that provides a functional description of the components and circuitry involved in human long-term memory. In this theory, Teyler and Discenna  propose that human long-term memory is composed of three components that work together to accomplish two main objectives: _pattern separation_, which ensures that the representations of distinct perceptual experiences are unique, and _pattern completion_, which enables the retrieval of complete memories from partial stimuli [19; 76].

The theory suggests that pattern separation is primarily accomplished in the memory encoding process, which starts with the **neocortex** receiving and processing perceptual stimuli into more easily manipulatable, likely higher-level, features, which are then routed through the **parahippocampal regions** (PHR) to be indexed by the hippocampus. When they reach the **hippocampus**, salient signals are included in the hippocampal index and associated with each other.

After the memory encoding process is completed, pattern completion drives the memory retrieval process whenever the hippocampus receives partial perceptual signals from the PHR pipeline. The hippocampus then leverages its context-dependent memory system, thought to be implemented through a densely connected network of neurons in the CA3 sub-region , to identify complete and relevant memories within the hippocampal index and route them back through the PHR for simulation in the neocortex. Thus, this complex process allows for new information to be integrated by changing only the hippocampal index instead of updating neocortical representations.

### Overview

Our proposed approach, HippoRAG, is closely inspired by the process described above. As shown in Figure 2, each component of our method corresponds to one of the three components of human long-term memory. A detailed example of the HippoRAG process can be found in Appendix A.

**Offline Indexing.** Our offline indexing phase, analogous to memory encoding, starts by leveraging a strong instruction-tuned **LLM**, our artificial neocortex, to extract knowledge graph (KG) triples. The KG is schemaless and this process is known as open information extraction (OpenIE) [3; 5; 60; 98]. This process extracts salient signals from passages in a retrieval corpus as discrete noun phrases rather than dense vector representations, allowing for more fine-grained pattern separation. It is therefore natural to define our artificial hippocampal index as this open **KG**, which is built on the whole retrieval corpus passage-by-passage. Finally, to connect both components as is done by the parahippocampal regions, we use off-the-shelf dense encoders fine-tuned for retrieval (**retrieval encoders**). These retrieval encoders provide additional edges between similar but not identical noun phrases within this KG to aid in downstream pattern completion.

**Online Retrieval.** These same three components are then leveraged to perform online retrieval by mirroring the human brain's memory retrieval process. Just as the hippocampus receives input processed through the neocortex and PHR, our LLM-based neocortex extracts a set of salient named entities from a query which we call _query named entities_. These named entities are then linked to nodes in our KG based on the similarity determined by retrieval encoders; we refer to these selected nodes as _query nodes_. Once the query nodes are chosen, they become the partial cues from which our synthetic hippocampus performs pattern completion. In the hippocampus, neural pathways between elements of the hippocampal index enable relevant neighborhoods to become activated and recalled upstream. To imitate this efficient graph search process, we leverage the Personalized PageRank (PPR) algorithm , a version of PageRank that distributes probability across a graph only through a set of user-defined source nodes. This constraint allows us to bias the PPR output only towards the set of query nodes, just as the hippocampus extracts associated signals from specific partial cues.2 Finally, as is done when the hippocampal signal is sent upstream, we aggregate the output PPR node probability over the previously indexed passages and use that to rank them for retrieval.

### Detailed Methodology

**Offline Indexing.** Our indexing process involves processing a set of passages \(P\) using an instruction-tuned LLM \(L\) and a retrieval encoder \(M\). As seen in Figure 2 we first use \(L\) to extract a set of noun phrase nodes \(N\) and relation edges \(E\) from each passage in \(P\) via OpenIE. This process is done via 1-shot prompting of the LLM with the prompts shown in Appendix I. Specifically, we first extract a set of named entities from each passage. We then add the named entities to the OpenIE prompt to extract the final triples, which also contain concepts (noun phrases) beyond named entities. We find that this two-step prompt configuration leads to an appropriate balance between generality and bias towards named entities. Finally, we use \(M\) to add the extra set of _synonymy_ relations \(E^{}\) discussed above when the cosine similarity between two entity representations in \(N\) is above a threshold \(\). As stated above, this introduces more edges to our hippocampal index and allows for more effective pattern completion. This indexing process defines a \(|N||P|\) matrix \(\), which contains the number of times each noun phrase in the KG appears in each original passage.

**Online Retrieval.** During the retrieval process, we prompt \(L\) using a 1-shot prompt to extract a set of named entities from a query \(q\), our previously defined query named entities \(C_{q}=\{c_{1},...,c_{n}\}\) (_Stanford_ and _Alzheimer's_ in our Figure 2 example). These named entities \(C_{q}\) from the query are then encoded by the same retrieval encoder \(M\). Then, the previously defined query nodes are chosen as the set of nodes in \(N\) with the highest cosine similarity to the query named entities \(C_{q}\). More formally, query nodes are defined as \(R_{q}=\{r_{1},...,r_{n}\}\) such that \(r_{i}=e_{k}\) where \(k=_{j}cosine\_similarity(M(c_{i}),M(e_{j}))\), represented as the _Stanford_ logo and the _Alzheimer's_ purple ribbon symbol in Figure 2.

Figure 2: **Detailed HippoRAG Methodology. We model the three components of human long-term memory to mimic its pattern separation and completion functions. For offline indexing (Middle), we use an LLM to process passages into open KG triples, which are then added to our artificial hippocampal index, while our synthetic parahippocampal regions (PHR) detect synonymy. In the example above, triples involving Professor Thomas are extracted and integrated into the KG. For online retrieval (Bottom), our LLM neocortex extracts named entities from a query while our parahippocampal retrieval encoders link them to our hippocampal index. We then leverage the Personalized PageRank algorithm to enable context-based retrieval and extract Professor Thomas.4

After the query nodes \(R_{q}\) are found, we run the PPR algorithm over the hippocampal index, i.e., a KG with \(|N|\) nodes and \(|E|+|E^{}|\) edges (triple-based and synonymy-based), using a personalized probability distribution \(\) defined over \(N\), in which each query node has equal probability and all other nodes have a probability of zero. This allows probability mass to be distributed to nodes that are primarily in the (joint) neighborhood of the query nodes, such as _Professor Thomas_, and contribute to eventual retrieval. After running the PPR algorithm, we obtain an updated probability distribution \(}\) over \(N\). Finally, in order to obtain passage scores, we multiply \(}\) with the previously defined \(\) matrix to obtain \(\), a ranking score for each passage, which we use for retrieval.

**Node Specificity.** We introduce node specificity as a neurobiologically plausible way to further improve retrieval. It is well known that global signals for word importance, like inverse document frequency (IDF), can improve information retrieval. However, in order for our brain to leverage IDF for retrieval, the number of total "passages" encoded would need to be aggregated with all node activations before memory retrieval is complete. While simple for normal computers, this process would require activating connections between an aggregator neuron and all nodes in the hippocampal index every time retrieval occurs, likely introducing prohibitive computational overhead. Given these constraints, we propose _node specificity_ as an alternative IDF signal which requires only local signals and is thus more neurobiologically plausible. We define the node specificity of node \(i\) as \(s_{i}=|P_{i}|^{-1}\), where \(P_{i}\) is the set of passages in \(P\) from which node \(i\) was extracted, information that is already available at each node. Node specificity is used in retrieval by multiplying each query node probability \(\) with \(s_{i}\) before PPR; this allows us to modulate each of their neighborhood's probability as well as their own. We illustrate node specificity in Figure 2 through relative symbol size: the _Stanford_ logo grows larger than the _Alzheimer's_ symbol since it appears in fewer documents.

## 3 Experimental Setup

### Datasets

We evaluate our method's retrieval capabilities primarily on two challenging multi-hop QA benchmarks, **MuSiQue** (answerable)  and **2WikiMultiHopQA**. For completeness, we also include the **HotpotQA** dataset even though it has been found to be a much weaker test for multi-hop reasoning due to many spurious signals , as we also show in Appendix B. To limit the experimental cost, we extract \(1{,}000\) questions from each validation set as done in previous work . In order to create a more realistic retrieval setting, we follow IRCoT  and collect all candidate passages (including supporting and distractor passages) from our selected questions and form a retrieval corpus for each dataset. The details of these datasets are shown in Table 1.

### Baselines

We compare against several strong and widely used retrieval methods: **BM25**, **Contriever**, **GTR** and **ColBERTv2**. Additionally, we compare against two recent LLM-augmented baselines: **Propositionizer**, which rewrites passages into propositions, and **RAPTOR**, which constructs summary nodes to ease retrieval from long documents. In addition to the single-step retrieval methods above, we also include the multi-step retrieval method **IRCoT** as a baseline.

### Metrics

We report retrieval and QA performance on the datasets above using recall@2 and recall@5 (R@2 and R@5 below) for retrieval and exact match (EM) and F1 scores for QA performance.

    & **MuSiQue** & **2Wiki** & **HotpotQA** \\  \# of Passages (\(P\)) & \(11,656\) & \(6,119\) & \(9,221\) \\ \# of Unique Nodes (\(N\)) & \(91,729\) & \(42,694\) & \(82,157\) \\ \# of Unique Edges (\(E\)) & \(21,714\) & \(7,867\) & \(17,523\) \\ \# of Unique Triples & \(107,448\) & \(50,671\) & \(98,709\) \\ \# of Contriever Synonym Edges (\(E^{}\)) & \(145,990\) & \(146,020\) & \(159,112\) \\ \# of ColBERTv2 Synonym Edges (\(E^{}\)) & \(191,636\) & \(82,526\) & \(171,856\) \\   

Table 1: Retrieval corpora and extracted KG statistics for each of our \(1{,}000\) question dev sets.

### Implementation Details

By default, we use GPT-3.5-turbo-1106 with temperature of \(0\) as our LLM \(L\) and Contriever  or ColBERTv2  as our retriever \(M\). We use \(100\) examples from MuSiQue's training data to tune HippoRAG's two hyperparameters: the synonymy threshold \(\) at \(0.8\) and the PPR damping factor at \(0.5\), which determines the probability that PPR will restart a random walk from the query nodes instead of continuing to explore the graph. Generally, we find that HippoRAG's performance is rather robust to its hyperparameters. More implementation details can be found in Appendix H.

## 4 Results

We present our retrieval and QA experimental results below. Given that our method indirectly affects QA performance, we report QA results on our best-performing retrieval backbone ColBERTv2 . However, we report retrieval results for several strong single-step and multi-step retrieval techniques.

**Single-Step Retrieval Results.** As seen in Table 2, HippoRAG outperforms all other methods, including recent LLM-augmented baselines such as Propositionizer and RAPTOR, on our main datasets, MuSiQue and 2WikiMultiHopQA, while achieving competitive performance on HotpotQA. We notice an impressive improvement of \(11\) and \(20\)% for R@2 and R@5 on 2WikiMultiHopQA and around \(3\)% on MuSiQue. This difference can be partially explained by 2WikiMultiHopQA's entity-centric design, which is particularly well-suited for HippoRAG. Our lower performance on HotpotQA is mainly due to its lower knowledge integration requirements, as explained in Appendix B, as well as a due to a concept-context tradeoff which we alleviate with an ensembling technique described in Appendix F.2.

**Multi-Step Retrieval Results.** For multi-step or iterative retrieval, our experiments in Table 3 demonstrate that IRCoT  and HippoRAG are complementary. Using HippoRAG as the retriever for IRCoT continues to bring R@5 improvements of around \(4\)% for MuSiQue, \(18\)% for 2WikiMultiHopQA and an additional \(1\)% on HotpotQA.

    &  &  &  &  \\   & R@2 & R@5 & R@2 & R@5 & R@2 & R@5 & R@2 & R@5 \\  BM25  & \(32.3\) & \(41.2\) & \(51.8\) & \(61.9\) & \(55.4\) & \(72.2\) & \(46.5\) & \(58.4\) \\ Contriever  & \(34.8\) & \(46.6\) & \(46.6\) & \(57.5\) & \(57.2\) & \(75.5\) & \(46.2\) & \(59.9\) \\ GTR  & \(37.4\) & \(49.1\) & \(60.2\) & \(67.9\) & \(59.4\) & \(73.3\) & \(52.3\) & \(63.4\) \\ ColBERTv2  & \(37.9\) & \(49.2\) & \(59.2\) & \(68.2\) & \(\) & \(\) & \(53.9\) & \(65.6\) \\ RAPTOR  & \(35.7\) & \(45.3\) & \(46.3\) & \(53.8\) & \(58.1\) & \(71.2\) & \(46.7\) & \(56.8\) \\ RAPTOR (ColBERTv2) & \(36.9\) & \(46.5\) & \(57.3\) & \(64.7\) & \(63.1\) & \(75.6\) & \(52.4\) & \(62.3\) \\ Proposition  & \(37.6\) & \(49.3\) & \(56.4\) & \(63.1\) & \(58.7\) & \(71.1\) & \(50.9\) & \(61.2\) \\ Proposition (ColBERTv2) & \(37.8\) & \(50.1\) & \(55.9\) & \(64.9\) & \(63.9\) & \(78.1\) & \(52.5\) & \(64.4\) \\  HippoRAG (Contriever) & \(\) & \(\) & \(\) & \(\) & \(59.0\) & \(76.2\) & \(57.2\) & \(72.6\) \\ HippoRAG (ColBERTv2) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 2: **Single-step retrieval performance.** HippoRAG outperforms all baselines on MuSiQue and 2WikiMultiHopQA and achieves comparable performance on the less challenging HotpotQA dataset.

    &  &  &  &  \\   & R@2 & R@5 & R@2 & R@5 & R@2 & R@5 & R@2 & R@5 \\  IRCoT + BM25 (Default) & \(34.2\) & \(44.7\) & \(61.2\) & \(75.6\) & \(65.6\) & \(79.0\) & \(53.7\) & \(66.4\) \\ IRCoT + Contriever & \(39.1\) & \(52.2\) & \(51.6\) & \(63.8\) & \(65.9\) & \(81.6\) & \(52.2\) & \(65.9\) \\ IRCoT + ColBERTv2 & \(41.7\) & \(53.7\) & \(64.1\) & \(74.4\) & \(\) & \(82.0\) & \(57.9\) & \(70.0\) \\  IRCoT + HippoRAG (Contriever) & \(43.9\) & \(56.6\) & \(75.3\) & \(93.4\) & \(65.8\) & \(82.3\) & \(61.7\) & \(77.4\) \\ IRCoT + HippoRAG (ColBERTv2) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 3: **Multi-step retrieval performance.** Combining HippoRAG with standard multi-step retrieval methods like IRCoT results in strong complementary improvements on all three datasets.

**Question Answering Results.** We report QA results for HippoRAG, the strongest retrieval baselines, ColBERTv2 and IRCoT, as well as IRCoT using HippoRAG as a retriever in Table 4. As expected, improved retrieval performance in both single and multi-step settings leads to strong overall improvements of up to \(3\%\), \(17\%\) and \(1\%\) F1 scores on MuSiQue, 2WikiMultiHopQA and HotpotQA respectively using the same QA reader. Notably, single-step HippoRAG is on par or outperforms IRCoT while being \(10\)-\(30\) times cheaper and \(6\)-\(13\) times faster during online retrieval (Appendix G).

## 5 Discussions

### What Makes HippoRAG Work?

**OpenIE Alternatives.** To determine if using a closed model like GPT-3.5 is essential to retain our performance improvements, we replace it with an end-to-end OpenIE model REBEL  as well as the 8B and 70B instruction-tuned versions of Llama-3.1, a class of strong open-weight LLMs . As shown in Table 5 row 2, building our KG using REBEL results in large performance drops, underscoring the importance of LLM flexibility. Specifically, GPT-3.5 produces twice as many triples as REBEL, indicating its bias against producing triples with general concepts and leaving many useful associations behind.

In terms of open-weight LLMs, Table 5 (rows 3-4) shows that the performance of Llama-3.1-8B is competitive with GPT-3.5 in all datasets except for 2Wiki, where performance drops substantially. Nevertheless, the stronger 70B counterpart outperforms GPT-3.5 in two out of three datasets and is still competitive in 2Wiki. The strong performance of Llama-3.1-70B and the comparable performance of even the 8B model is encouraging since it offers a cheaper alternative for indexing over large corpora. The graph statistics for these OpenIE alternatives can be found in Appendix C.

To understand the relationship between OpenIE and retrieval performance more deeply, we extract 239 gold triples from 20 examples from the MuSiQue training set. We then perform a small-scale intrinsic evaluation using the CaRB  framework for OpenIE. We find that both Llama-3.1-Instruct

    &  &  &  &  \\    & EM & F1 & EM & F1 & EM & F1 & EM & F1 \\  None & \(12.5\) & \(24.1\) & \(31.0\) & \(39.6\) & \(30.4\) & \(42.8\) & \(24.6\) & \(35.5\) \\ ColBERTv2 & \(15.5\) & \(26.4\) & \(33.4\) & \(43.3\) & \(43.4\) & \(57.7\) & \(30.8\) & \(42.5\) \\ HippoRAG (ColBERTv2) & \(19.2\) & \(29.8\) & \(46.6\) & \(59.5\) & \(41.8\) & \(55.0\) & \(35.9\) & \(48.1\) \\  IRCoT (ColBERTv2) & \(19.1\) & \(30.5\) & \(35.4\) & \(45.1\) & \(45.5\) & \(58.4\) & \(33.3\) & \(44.7\) \\ IRCoT + HippoRAG (ColBERTv2) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 4: **QA performance.** HippoRAG’s QA improvements correlate with its retrieval improvements on single-step (rows 1-3) and multi-step retrieval (rows 4-5).

    & &  &  &  &  \\   & R@2 & R@5 & R@2 & R@5 & R@2 & R@5 & R@2 & R@5 \\  HippoRAG & \(40.9\) & \(51.9\) & \(\) & \(\) & \(60.5\) & \(77.7\) & \(\) & \(\) \\   & REBEL  & \(31.7\) & \(39.6\) & \(63.1\) & \(76.5\) & \(43.9\) & \(59.2\) & \(46.2\) & \(58.4\) \\  & Llama-3.1-8B-Instruct  & \(40.8\) & \(51.9\) & \(62.5\) & \(77.5\) & \(59.9\) & \(75.1\) & \(54.4\) & \(67.8\) \\  & Llama-3.1-70B-Instruct  & \(\) & \(\) & \(68.8\) & \(85.3\) & \(\) & \(\) & \(57.1\) & \(72.5\) \\  PPR & \(R_{q}\) Nodes Only & \(37.1\) & \(41.0\) & \(59.1\) & \(61.4\) & \(55.9\) & \(66.2\) & \(50.7\) & \(56.2\) \\ Alternatives & \(R_{q}\) Nodes \& Neighbors & \(25.4\) & \(38.5\) & \(53.4\) & \(74.7\) & \(47.8\) & \(64.5\) & \(42.2\) & \(59.2\) \\   & w/o Node Specificity & \(37.6\) & \(50.2\) & \(70.1\) & \(88.8\) & \(56.3\) & \(73.7\) & \(54.7\) & \(70.9\) \\  & w/o Synonymy Edges & \(40.2\) & \(50.2\) & \(69.2\) & \(85.6\) & \(59.1\) & \(75.7\) & \(56.2\) & \(70.5\) \\   

Table 5: **Dissecting HippoRAG.** To understand what makes it work well, we replace its OpenIE module and PPR with plausible alternatives and ablate node specificity and synonymy-based edges.

models underperform GPT-3.5 slightly on this intrinsic evaluation but all LLMs vastly outperform REBEL. More details about this evaluation experiments can be found in Appendix D.

**PPR Alternatives.** As shown in Table 5 (rows 5-6), to examine how much of our results are due to the strength of PPR, we replace the PPR output with the query node probability \(\) multiplied by node specificity values (row 5) and a version of this that also distributes a small amount of probability to the direct neighbors of each query node (row 6). First, we find that PPR is a much more effective method for including associations for retrieval on all three datasets compared to both simple baselines. It is interesting to note that adding the neighborhood of \(R_{q}\) nodes without PPR leads to worse performance than only using the query nodes themselves.

**Ablations.** As seen in Table 5 (rows 7-8), node specificity obtains considerable improvements on MuSiQue and HotpotQA and yields almost no change in 2WikiMultiHopQA. This is likely because 2WikiMultiHopQA relies on named entities with little differences in terms of term weighting. In contrast, synonymy edges have the largest effect on 2WikiMultiHopQA, suggesting that noisy entity standardization is useful when most relevant concepts are named entities, and improvements to synonymy detection could lead to stronger performance in other datasets.

### HippoRAG's Advantage: Single-Step Multi-Hop Retrieval

A major advantage of HippoRAG over conventional RAG methods in multi-hop QA is its ability to _perform multi-hop retrieval in a single step_. We demonstrate this by measuring the percentage of queries where _all_ the supporting passages are retrieved successfully, a feat that can only be accomplished through successful multi-hop reasoning. Table 6 below shows that the gap between our method and ColBERTv2, using the top-\(5\) passages, increases even more from \(3\)% to \(6\)% on MuSiQue and from \(20\)% to \(38\)% on 2WikiMultiHopQA, suggesting that large improvements come from obtaining all supporting documents rather than achieving partially retrieval on more questions.

We further illustrate HippoRAG's unique _single-step multi-hop retrieval_ ability through the first example in Table 7. In this example, even though _Alhandra_ was not mentioned in _Vila de Xira's_ passage, HippoRAG can directly leverage Vila de Xira's connection to Alhandra as his place of birth to determine its importance, something that standard RAG methods would be unable to do directly. Additionally, even though IRCoT can also solve this multi-hop retrieval problem, as shown in Appendix G, it is \(10\)-\(30\) times more expensive and \(6\)-\(13\) times slower than ours in terms of online retrieval, arguably the most important factor when it comes to serving end users.

    &  &  &  &  \\   & AR@2 & AR@5 & AR@2 & AR@5 & AR@2 & AR@5 & AR@2 & AR@5 \\  ColBERTv2  & \(6.8\) & \(16.1\) & \(25.1\) & \(37.1\) & \(33.3\) & \(59.0\) & \(21.7\) & \(37.4\) \\ HippoRAG & \(10.2\) & \(22.4\) & \(45.4\) & \(75.7\) & \(33.8\) & \(57.9\) & \(29.8\) & \(52.0\) \\   

Table 6: **All-Recall metric.** We measure the percentage of queries for which all supporting passages are successfully retrieved (all-recall, denoted as AR@2 or AR@5) and find even larger performance improvements for HippoRAG.

    & **Question** & **HippoRAG** & **ColBERTv2** & **IRCoT** \\   & In which & **1. Alhandra** & **1. Alhandra** & **1. Alhandra** & **1. Alhandra** \\  & district was & **2. Vila de Xira** & **2. Vila de Xira** & **2. Dimuthu** & **2. Vila de Xira** \\  & **Alhandra** & **3.** Portugal & **3.** Portugal & **3.** Ja’ar & **3.** Póvoa de** \\   & Which & **Stanford** & **1. Thomas Südhof** & **1.** Brian Knutson & **1.** Brian Knutson \\  & professor works on the neuroscience & **2.** Karl Deisseroth & **2.** Eric Knudsen & **2.** Eric Knudsen \\   & of **Alzheimer’s**? & **3.** Robert Sapolsky & **3.** Lisa Giacomo & **3.** Lisa Giacomo \\   

Table 7: **Multi-hop question types.** We show example results for different approaches on path-finding vs. path-following multi-hop questions.

### HippoRAG's Potential: Path-Finding Multi-Hop Retrieval

The second example in Table 7, also present in Figure 1, shows a type of questions that is trivial for informed humans but out of reach for current retrievers without further training. This type of questions, which we call _path-finding_ multi-hop questions, requires identifying one path between a set of entities when many paths exist to explore instead of _following_ a specific path, as in standard multi-hop questions.5

More specifically, a simple iterative process can retrieve the appropriate passages for the first question by following the one path set by _Ahandra's_ one place of birth, as seen by IRCoT's perfect performance. However, an iterative process would struggle to answer the second question given the many possible paths to explore--either through professors at _Stanford University_ or professors working on the neuroscience of _Alzheimer's_. It is only by associating disparate information about Thomas Sudhof that someone who knows about this professor would be able to answer this question easily. As seen in Table 7, both ColBERTv2 and IRCoT fail to extract the necessary passages since they cannot access these associations. On the other hand, HippoRAG leverages its web of associations in its hippocampal index and graph search algorithm to determine that Professor Thomas is relevant to this query and retrieves his passages appropriately. More examples of these path-finding multi-hop questions can be found in our case study in Appendix E.

## 6 Related Work

### LLM Long-Term Memory

**Parametric Long-Term Memory.** It is well-accepted, even among skeptical researchers, that the parameters of modern LLMs encode a remarkable amount of world knowledge [2; 12; 23; 28; 31; 39; 62; 79], which can be leveraged by an LLM in flexible and robust ways [81; 83; 93]. Nevertheless, our ability to update this vast knowledge store, an essential part of any long-term memory system, is still surprisingly limited. Although many techniques to update LLMs exist, such as standard fine-tuning, model editing [15; 49; 50; 51; 52; 95] and even external parametric memory modules inspired by human memory [58; 82; 32], no methodology has yet to emerge as a robust solution for continual learning in LLMs [26; 46; 97].

**RAG as Long-Term Memory.** On the other hand, using RAG methods as a long-term memory system offers a simple way to update knowledge over time [36; 42; 66; 73]. More sophisticated RAG methods, which perform multiple steps of retrieval and generation from an LLM, are even able to integrate information across new or updated knowledge elements[38; 64; 72; 78; 88; 90; 92], another crucial aspect of long-term memory systems. As discussed above, however, this type of online information integration is unable to solve the more complex knowledge integration tasks that we illustrate with our _path-finding_ multi-hop QA examples.

Some other methods, such as RAPTOR , MemWalker  and GraphRAG , integrate information during the offline indexing phase similarly to HippoRAG and might be able to handle these more complex tasks. However, these methods integrate information by summarizing knowledge elements, which means that the summarization process must be repeated any time new data is added. In contrast, HippoRAG can continuously integrate new knowledge by simply adding edges to its KG.

**Long Context as Long-Term Memory.** Context lengths for both open and closed source LLMs have increased dramatically in the past year [11; 17; 22; 61; 68]. This scaling trend seems to indicate that future LLMs could perform long-term memory storage within massive context windows. However, the viability of this future remains largely uncertain given the many engineering hurdles involved and the apparent limitations of long-context LLMs, even within current context lengths [41; 45; 96; 21].

### Multi-Hop QA & Graphs

Many previous works have also tackled multi-hop QA using graph structures. These efforts can be broadly divided in two major categories: 1) graph-augmented reading comprehension, where agraph is extracted from retrieved documents and used to improve a model's reasoning process and 2) graph-augmented retrieval, where models find relevant documents by traversing a graph structure.

**Graph-Augmented Reading Comprehension.** Earlier works in this category are mainly supervised methods which mix signal from a hyperlink or co-occurrence graph with a language model through a graph neural network (GNN) [20; 67; 65]. More recent works use LLMs and introduce knowledge graph triples directly into the LLM prompt [57; 43; 47]. Although these works share HippoRAG's use of graphs for multi-hop QA, their generation-based improvements are fully complementary to HippoRAG's, which are solely based on improved retrieval.

**Graph-Augmented Retrieval.** In this second category, previous work trains a re-ranking module which can traverse a graph made using Wikipedia hyperlinks [16; 100; 54; 14; 4; 44]. HippoRAG, in contrast, builds a KG from scratch using LLMs and performs multi-hop retrieval without any supervision, making it much more adaptable.

### LLMs & KGs

Combining the strengths of language models and knowledge graphs has been an active research direction for many years, both for augmenting LLMs with a KG in different ways [48; 80; 84] or augmenting KGs by either distilling knowledge from an LLM's parametric knowledge [7; 85] or using them to parse text directly [8; 29; 94]. In an exceptionally comprehensive survey, Pan et al.  present a roadmap for this research direction and highlight the importance of work which _synergizes_ these two important technologies [37; 74; 27; 91; 99]. Like these works, HippoRAG shows the potential for synergy between these two technologies, combining the knowledge graph construction abilities of LLMs with the retrieval advantages of structured knowledge for more effective RAG.

## 7 Conclusions & Limitations

Our proposed neurobiologically principled methodology, although simple, already shows promise for overcoming the inherent limitations of standard RAG systems while retaining their advantages over parametric memory. HippoRAG's knowledge integration capabilities, demonstrated by its strong results on _path-following_ multi-hop QA and promise on _path-finding_ multi-hop QA, as well as its dramatic efficiency improvements and continuously updating nature, makes it a powerful middle-ground framework between standard RAG methods and parametric memory and offers a compelling solution for long-term memory in LLMs.

Nevertheless, several limitations can be addressed in future work to enable HippoRAG to achieve this goal better. First, we note that all components of HippoRAG are currently used off-the-shelf without any extra training. There is therefore much room to improve our method's practical viability by performing specific component fine-tuning. This is evident in the error analysis discussed in Appendix F, which shows most errors made by our system are due to NER and OpenIE and thus could benefit from direct fine-tuning. Given that the rest of the errors are graph search errors, also in Appendix F, we note that several avenues for improvements over simple PPR exist, such as allowing relations to guide graph traversal directly. Additionally, as shown in Appendix F.4, more work must be done to improve the consistency of OpenIE in longer compared to shorter documents. Finally, and perhaps most importantly, HippoRAG's scalability still calls for further validation. Although we show that Llama-3.1 could obtain similar performance to closed-source models and thus reduce costs considerably, we are yet to empirically prove the efficiency and efficacy of our synthetic hippocampal index as its size grows way beyond current benchmarks.