# FedLPA: One-shot Federated Learning with

Layer-Wise Posterior Aggregation

Xiang Liu\({}^{1,}\), Liangxi Liu\({}^{2,}\), Feiyang Ye\({}^{3}\), Yunheng Shen\({}^{4}\), Xia Li\({}^{5}\), Linshan Jiang\({}^{1,}\), Jialin Li\({}^{1,}\)

\({}^{1}\)National University of Singapore, \({}^{2}\)Northeastern University,

\({}^{3}\)University of Technology Sydney, \({}^{4}\)Tsinghua University, \({}^{5}\)ETH Zurich

{liuxiang,lijl}@comp.nus.edu.sg liu.liangx@northeastern.edu feiyang.ye.uts@gmail.com shenyhi9@mails.tsinghua.edu.cn thilixia@gmail.com linshan@nus.edu.sg

\({}^{}\)Equal Contribution \({}^{}\)Correspondence Author

###### Abstract

Efficiently aggregating trained neural networks from local clients into a global model on a server is a widely researched topic in federated learning. Recently, motivated by diminishing privacy concerns, mitigating potential attacks, and reducing communication overhead, one-shot federated learning (i.e., limiting client-server communication into a single round) has gained popularity among researchers. However, the one-shot aggregation performances are sensitively affected by the non-identical training data distribution, which exhibits high statistical heterogeneity in some real-world scenarios. To address this issue, we propose a novel one-shot aggregation method with layer-wise posterior aggregation, named FedLPA. FedLPA aggregates local models to obtain a more accurate global model without requiring extra auxiliary datasets or exposing any private label information, e.g., label distributions. To effectively capture the statistics maintained in the biased local datasets in the practical non-IID scenario, we efficiently infer the posteriors of each layer in each local model using layer-wise Laplace approximation and aggregate them to train the global parameters. Extensive experimental results demonstrate that FedLPA significantly improves learning performance over state-of-the-art methods across several metrics.

## 1 Introduction

Data privacy issues in Deep Learning  have grown to be a major global concern . To safeguard data privacy, the conventional federated learning algorithm will use the aggregation methods and follow the data management rules of different institutions, which implies that the distribution of data exhibits variations among clients . In the domain of machine learning, federated learning (FL)  has emerged as a prominent paradigm. The fundamental tenet of federated learning revolves around sharing machine learning models derived from decentralized data repositories, as opposed to divulging user raw data. This approach effectively preserves the confidentiality of individual data.

The standard federated learning framework, FedAvg , applies local model training. These local models are then aggregated into a global model through parameter averaging. Existing FL algorithms, however, require many communication rounds to effectively train a global model, leading to substantial communication overhead, increased privacy concerns, and higher demand for fault tolerance throughout the rounds. One-shot FL, which reduces client-server communication into a single round as explored by prior work , is a promising yet challenging scheme to address these issues. One-shot FL proves particularly practical in scenarios where iterative communication isnot feasible. Moreover, a reduction in communication rounds translates to fewer opportunities for any potential eavesdropping attacks.

While one-shot FL shows promises, existing approaches often grapple with challenges such as inadequate handling of high statistical heterogeneity information [16; 17] or non-independent and non-identically distributed (non-IID) data [18; 19]. Moreover, some prior methods rely on an auxiliary public dataset to achieve satisfactory performance in one-shot FL [13; 14], or even on pre-trained large models , which may not be practical  in some sensitive scenarios. Additionally, some approaches, such as those [22; 19; 23; 15]), might expose private label information to both local and global models, e.g., the client label distribution, potentially violating General Data Protection Regulation (GDPR) rules. Furthermore, some prior methods [14; 18; 24] require substantial computing resources for dataset distillation, model distillation, or even training a generator capable of generating synthetic data for second-stage training on the server side, making them less practical.

Besides, the performance of one-shot FL often falls short when dealing with non-IID data. Non-IID data biases global updates, reducing the accuracy of the global model and slowing down convergence. In extreme non-IID cases, clients may be required to address distinct classes solely on their side. Several approaches to federated learning are proposed in multi-round settings to tackle this heterogeneity among clients. In the work , it allows each client to use a personalized model instead of a shared global model. With the personalized approach, a multi-round framework benefits from joint training while allowing each client to keep its unique model. However, one-shot aggregation on a local model is far from being resolved to address the concern of non-i.i.d data distributions.

In this paper, we introduce a novel one-shot aggregation approach to address these issues, named FedLPA (Federated Learning with Layer-wise Posterior Aggregation). FedLPA infers the posteriors of each layer in each local model using the empirical Fisher information matrix obtained by layer-wise Laplace Approximation. Laplace Approximations are widely used to compute the empirical Fisher information matrix for neural networks, conveying the data statistics in non-i.i.d settings. However, computing empirical Fisher information matrices of multiple local clients and aggregating their Fisher information matrices remains an ongoing challenge . To mitigate it, FedLPA aggregates the posteriors of local models using the accurately computed block-diagonal empirical Fisher information matrices to measure the parameter space. This matrix captures essential parameter correlations and distinguishes itself from prior methods by being non-diagonal and non-low-rank, thereby conveying the statistics of biased local datasets. After that, the global model parameters are aggregated without any need for server-side knowledge distillation .

Our extensive experiments verify the efficiency and effectiveness of FedLPA, highlighting that FedLPA markedly enhances the test accuracy when compared to existing one-shot FL baseline approaches across various datasets. Our main contributions are summarized as follows:

* To the best of our knowledge, we are the first to propose an effective one-shot federated learning approach that trains global models using block-diagonal empirical Fisher information matrices. Our approach is data-free without any need for any auxiliary dataset and label information and significantly improves system performance, including negligible communication cost and moderate computing overhead.
* We are the first to train global model parameters via constructing a multi-variate linear objective function and optimizing its quadratic form, which allows us to formulate and solve the problem in a convex form efficiently, which has a linear convergence rate, ensuring good performance.
* We conduct extensive experiments to illustrate the effectiveness of FedLPA. Our approach consistently outperforms the baselines, showcasing substantial improvement across various settings and datasets. Even in some extreme scenarios where label skew is severe, e.g., each client has only one class, we achieve satisfactory results while other existing one-shot federated learning algorithms struggle.

## 2 Background and related works

### Federated learning on non-iid data

Previous work FedAvg  first introduced the concept of FL and presented the algorithm, which achieved competitive performance on i.i.d data, in comparison to several centralized techniques. However, it was observed in previous works [27; 28] that the convergence rate and ultimate accuracy of FedAvg on non-IID data distributions were significantly reduced, compared to the results observed with homogeneous data distributions.

Several methods have been developed to enhance performance in federated learning against non-IID data distributions. The SCAFFOLD method  leveraged control variates to reduce objective inconsistency in local updates. It estimated the drift of directions in local optimization and global optimization and incorporated this drift into local training to align the local optimization direction with the global optimization. FedNova  addressed objective inconsistency while maintaining rapid error convergence through a normalized averaging method. It scaled and normalized the local updates of each client based on the number of local optimization steps. FedProx  enhanced the local training process by introducing a global prior in the form of an \(L2\) regularization term within the local objective function. Researchers introduced PFNM [32; 33], a Bayesian probabilistic framework specifically tailored for multilayer perceptrons. PFNM employed a Beta-Bernoulli process (BBP)  to aggregate local models, quantifying the degree of alignment between global and local parameters. The framework  proposed utilized a multivariate Gaussian product method to construct a global posterior by aggregating local posteriors estimated using an online Laplace approximation. FedPA  also applied the Gaussian product method but employed stochastic gradient Markov chain Monte Carlo for approximate inference of local posteriors. DAFL (Data-Free Learning)  introduced an innovative framework based on generative adversarial networks. ADI  utilized an image synthesis method that leveraged the image distribution to train deep neural networks without real data. The pFedHN method  incorporated HyperNetworks  to address federated learning applications.

However, all of these methods encountered challenges in the one-shot federated learning setting, as they required aggregating the model by multiple rounds and might be inaccurate due to the omission of critical information, such as posterior joint probabilities between different parameters.

### One-shot federated learning

One-shot Federated Learning (FL) is an emerging and promising research direction characterized by its minimal communication cost. In the first study on one-shot FL , the approach involved the aggregation of local models, forming an ensemble to construct the final global model. Subsequently, knowledge distillation using public data was applied in the following step. FedKT  brought forward the concept of consistent voting to fortify the ensemble. Recent research endeavors [19; 24] proposed data-free knowledge distillation schemes tailored for one-shot FL. These methods adopted the basic ensemble distillation framework as FedDF . XorMixFL  introduced the use of exclusive OR operation (XOR) for encoding and decoding samples in data sharing. It is important to note that XorMixFL assumed the possession of labeled samples from a global class by all clients and the server, which might not align with practical real-world scenarios. A noteworthy innovation of DENSE  was its utilization of a generator to create synthetic datasets on the server side, circumventing the need for a public dataset in the distillation process. Co-Boosting  improves the ensemble when doing the distillation to improve the performance. FedOV  delved into addressing comprehensive label skew cases. FEDCVAE  confronted this challenge by transmitting all label distributions from clients to servers. These schemes [22; 14; 19; 24; 23; 15] exposed some client-side private information, leading to additional communication overhead and potential privacy leakage, e.g., FEDCVAE  needed all the client label distribution to be transmitted to the server side and FedOV  needed the clients to know the labels which were unknown. Instead, MA-Echo  adopted a unique approach by emphasizing the addition of norms among layer-wide parameters during the aggregation of local models. The project  focused on the theoretic analysis of the error in its approximation method. However, their method grappled with limited experiments and lacked detailed explanations of the approach. FedDISC , on the other hand, relied on the pre-trained model CLIP from OpenAI, where their reliance might not always align with practicality or suitability for diverse scenarios.

While some of these techniques are orthogonal to FedLPA and can be integrated with it, it is worth noting that none of the previously mentioned algorithms possess the capability to train global model parameters using empirical Fisher information matrices on extensive experiment settings. Some

[MISSING_PAGE_FAIL:4]

Modern algorithms [45; 46] allow the local training process to obtain an optimal, regarded as the expectation \(_{k}\) in the above equations. However, \(}_{k}\) is intractable to compute due to a large number of parameters in modern neural networks. An efficient method is to approximate \(}_{k}\) using the empirical Fisher information matrix .

Inferring the local layer-wise posteriors with the block-diagonal empirical Fisher information matrices

A empirical Fisher \(}\) is defined as below:

\[}=_{s}[ p(s|)  p(s|)^{}] \]

where \(p(s|)\) is the likelihood on data point \(s\). It is an approximate of the Fisher information matrix, the empirical Fisher information matrix is equivalent to the expectation of the Hessian of the negative log posterior if assuming \(p(s|)\) is identical for each \(s\).

Therefore, the local co-variance \(_{k}\) can be approximated by the empirical Fisher \(}_{k}\)[48; 49].

\[_{k}^{-1}}_{k}+ \]

The works [50; 51; 17] ignore co-relations between different parameters and only consider the self-relations of parameters as computing all co-relations is impossible. Thus, their methods are inaccurate. Detailed discussions and the novelty compared to previous works are in Appendix B.

In order to capture co-relations between different parameters efficiently, previous works [46; 43] estimate a block empirical Fisher information matrix \(\) instead of assuming parameters are independent and approximating the co-variance by the diagonal of the empirical Fisher. As pointed out, co-relations inner a layer are much more significant than others [46; 52; 53], while computing the co-relations between different layers brings slight improvement but much more computation [54; 43]. Therefore, assuming parameters are layer-independent is a good trade-off. As a result, the approximated layer-wise empirical Fisher is block-diagonal. For layer \(l\) on client \(k\), its empirical Fisher \(F_{kl}\) is one of the diagonal blocks in the whole empirical Fisher for the local model and is factored into two small matrices as below,

\[_{k_{l}}^{-1}_{k_{l}}=_{k_{l}} _{k_{l}} \]

where \(\) is the Kronecker product; \(_{k_{l}}=[}_{k_{l-1}}}_{ k_{l-1}}^{}]+_{l}\) and \(_{k_{l}}=[}_{k_{l}}}_{ k_{l}}^{}]+}\) are two expectation factor matrices over the data samples; \(}_{k_{l}}\) is the activations and \(}_{k_{l}}\) is the gradient of the pre-activations of layer \(l\) on client \(k\), \(\) is the hyperparameter and \(_{l}\) is a factor minimizing approximation error in \(_{k_{l}}\)[46; 49; 55]. \(_{k_{l}}\) and \(_{k_{l}}\) are symmetric positive definite matrices [45; 46].

We use \(_{k_{l}}\) to denote the parameter vector of layer \(l\) and \(_{k_{l}}=vec^{-1}(_{k_{l}})\) is the vectorized optimal weight matrix of layer \(l\) on client \(k\). Thus, the resulting local layer-wise posterior approximation is \(_{k_{l}}(_{k_{l}},_{k_{l}}^{-1})\).

### Estimating the global expectation

Given the local posteriors, the global expectation could be aggregated by Eq. 7. With Eq. 10, the \(l\)-th layer's global expectation \(}_{l}\) consists of Kronecker products:

\[}_{l} =}_{l}_{k}^{K}_{k_{l}}^{-1}_{k_{l}}=}_{l}_{k}^{K}(_{k_{l}} _{k_{l}})_{k_{l}} \] \[=}_{l}_{k}^{K}(_{k_{l }}_{k_{l}}_{k_{l}})=}_{l}_{k}^{K}_{k_{l}}=}_{l}}_{l}\]

where \(}_{l}=_{k}^{K}_{k_{l}}\) and \(_{k_{l}}=(_{k_{l}}_{k_{l}}_{k_{l}})\) is a immediate notations for simplification. For the global expectation, we have \(}=}}\). The corresponding global co-variance is an inverse of the sum of Kronecker products:

\[}_{l}=(_{k}^{K}_{k_{l}}_{k_{l}})^{-1} \]

As shown in Eq. 11, obtaining the global expectation \(}_{l}\) requires calculating the inverse of \(}_{l}^{-1}\) as Eq. 12, which is unacceptable and the details are in Appendix C. Thus, we propose our method to directly train the parameters of the global model on the server side.

### Train the parameters of the global model

We use \([]\) denotes \(_{k}^{K}(_{k})\), \([]\) denotes \(_{k}^{K}(_{k})\), \([]\) denotes \(_{k}^{K}(_{k}_{k})\). Previous works [46; 49] approximate the expectation of Kronecker products by a Kronecker product of expectations \([][ ][]\) with an assumption of \(_{k_{l}}\) and \(_{k_{l}}\) are independent, which is called Expectation Approximation (EA). However, it may lead to a biased global expectation. The details are discussed in Appendix D. Instead, we could construct a linear objective after aggregating the approximation of local posteriors via using block-diagonal empirical Fisher information matrices. We denotes \(}\) as the matrix formula of \(}=(})\), and the optimal solution of \(f(})\) is \(}^{*}=(}^{*})\). We construct \(f(})\) as a multi-variates linear objective function. When \(}=}^{*}\) is optimal solution, \(f(})=\), where \(\) is a vector with all zero. Note that

\[ f(})&=}^ {-1}}-}=_{k}^{K}( _{k}}_{k})-}\\ &=([} ])-} \]

To obtain the optimal solution, we minimize the following problem to obtain an approximate solution \(}^{*}\) of \(}\):

\[}^{*}=_{}}\|_{k}^{K} (_{k}}_{k})-}\|_{2}^{2} \]

The above equation is a quadratic objective, and it can be solved by modern optimization tools efficiently and conveniently. Since the main objective of the above problem is both convex and Lipschitz smooth w.r.t \((})\), we can use the gradient descent method to solve it with a linear convergence rate. Here, we use automatic differentiation to calculate the gradient w.r.t. \(}\).

```
1:Input: clients \(K\), layers \(L\)
2:Initialize global weight \(}_{l}\) of layer \(l=1,...,L\)
3:clients executes:
4:Initialize local model
5:for k = 1,..., K do
6:\(\{_{k_{l}},_{k_{l}},_{k_{l}}|l=1,...,L\}\) local training
7:endfor
```

**Algorithm 1** FedLPA Global Aggregation

### Overall FedLPA algorithm and discussions

In summary, the proposed algorithm FedLPA follows the same paradigm as the standard one-shot federated learning framework. In FedLPA, the clients locally train their models to get \(_{k}\) and calculate the local co-variance over its training dataset using the layer-wise Laplace approximation to compute \(_{k},_{k}\). Subsequently, each client transmits their local \(_{k},_{k},_{k}\) to the server. Following Algorithm 1, the server aggregates these contributions to obtain the global expectation, as described in Eq. 7, then trains the global model parameters, as outlined in Eq. 14. Thus, the transmitted data between the clients and the server is solely \(_{k},_{k},_{k}\) without any extra auxiliary dataset and label information.

Note that FedLPA can be directly adopted in most common scenarios. For the special case that the neural model has enormous single-layer weight parameters, how to extend our proposed FedLPA is discussed in Appendix E.

### t-SNE observation and discussions

To quickly demonstrate the effectiveness of FedLPA, we show the t-SNE visualization of our FedLPA global model on the MNIST dataset as an example with a biased training data setting among 10 local clients. The experiment details, t-SNE visualizations of the local models and the global models of other algorithms and discussions are in Appendix G.1. As shown in Figure 1, FedLPA generates the global model which can clearly distinguish these classes, meanwhile, the classes are separate.

### Privacy Discussions

FedLPA is intuitively compatible with existing privacy-preserving techniques, such as differential privacy (DP) [56; 57], secure multiparty computation (SMC) [58; 59], and homomorphic encryption (HE) [60; 61; 62]. In Appendix F.1, we propose a naive DP-FedLPA with two different mechanisms to show the compatibility with differential privacy. Meanwhile, we mention that our proposed FedLPA has the same privacy-preserving level as the conventional federated learning algorithms (i.e, FedAvg, FedProx, FedNova and Dense). Compared with FedAvg, we have conducted a detailed analysis from a privacy attack perspective to show that our proposed FedLAP exhibits a security level consistent with FedAvg against several types of privacy attacks, where the details are shown in Appendix F.3. Note that the main focus of FedLPA is to improve the learning performance on the one-shot FL settings, thus, we leave the integration with other privacy-preserving techniques beyond DP as an open problem.

## 4 Experiments

### Experiments settings

**Datasets.** We conduct experiments on MNIST , Fashion-MNIST , CIFAR-10 , and SVHN  datasets. In most of the previous works and the most popular benchmark, the majority of their experiments use these datasets and these models. We choose these datasets and models to do the majority of our experiments following these established methods and benchmarks to fairly compare our method with the baselines. We use the data partitioning methods for non-IID settings of the benchmark 1 to simulate different label skews. Specifically, we try two different kinds of partition: 1) #C = \(k\): each client only has data from \(k\) classes. We first assign \(k\) random class IDs for each client. Next, we randomly and equally divide samples of each class to their assigned clients; 2) \(p_{k}\) - Dir(\(\)): for each class, we sample from Dirichlet distribution \(p_{k}\) and distribute \(p_{k,j}\) portion of class \(k\) samples to client \(j\). In this case, smaller \(\) denotes worse skews.

Here's a brief overview of these datasets. MNIST Dataset: The MNIST dataset comprises binary images of handwritten digits. It consists of 60,000 28x28 training images and 10,000 testing images. FMNIST Dataset: Similar to MNIST, the FMNIST dataset also contains 60,000 28x28 training images and 10,000 testing images. SVHN Dataset: The SVHN dataset includes 73,257 32x32 color training images and 10,000 testing images. CIFAR-10 Dataset: CIFAR-10 consists of 60,000 32x32 color images distributed across ten classes, with each class containing 6,000 images. The input dimensions for MNIST, FMNIST, SVHN, and CIFAR-10 are 784, 784, 3,072, and 3,072, respectively.

Figure 1: t-SNE visualization for our FedLPA global model.

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

FedLPA and baselines. Details of the overhead evaluation are referred to Appendix G.6 and G.7. Our observations reveal that FedLPA is slightly slower than FedNova, SCAFFOLD, FedAvg, and FedProx, while much faster than DENSE. FedLPA also has significantly improved the one-shot learning performance of the above four approaches. Similarly, FedLPA performs moderately incremental communication overhead while outperforming other baseline approaches on learning performance, as one-shot FL introduces heavy computation overhead while communication overhead is usually small. It is noteworthy that FedLPA strikes a favorable balance between computation and communication overhead, making it the most promising approach for one-shot FL.

### Extension to multiple rounds

We conduct experiments on MNIST with 10 clients and data partitioning \(p_{k}\) - Dir\((=0.5)\). The results are shown in Figure 2. As DENSE could not support multiple rounds, we compare our methods with FedAvg, FedNova, SCAFFOLD, and FedProx. FedLPA achieves the highest accuracy in the first round, denoting the strongest learning capabilities in a one-shot setting. With the increment in the number of rounds, the performances of FedLPA increase slower than the other baseline approaches. This figure shows that the joint approach (ours (one round) then FedAvg) that utilizes FedLPA in the first round and then adopts other baseline methods may be most promising to save communication and computation resources in the multiple-round federated learning scenario.

### Supplementary experiments

Experiments for privacy concerns, experiments on different local epoch numbers, experiments in extreme settings (the number of clients=5, \(=0.001\)), experiments with more methods, experiments with more complex network structures, experiments with more complex datasets, ablation experiments analyzing the number of approximation iterations of FedLPA can be found in Appendix.

## 5 Conclusions

In this work, we design a novel one-shot FL algorithm FedLPA to better model the global parameters in effective one-shot federated learning. We propose a method that could aggregate the local clients in a layer-wise manner with their posterior approximation via block-diagonal empirical Fisher information matrices, which could effectively capture the accurate statistics of a locally biased dataset. Overall, FedLPA stands out as the most practical and efficient framework that conducts data-free one-shot FL, particularly well-suited for high data heterogeneity in various settings, considering it significantly outperforms other baselines with extensive experiments. Our FedLPA is available in [https://github.com/lebronlambert/FedLPA_NeurIPS2024](https://github.com/lebronlambert/FedLPA_NeurIPS2024).

   value of \(\) & 0.01 & 0.001 & 0.0001 \\  \(\)=0.01 & 18.63\(\)0.78 & 21.20\(\)0.67 & 22.50\(\)1.84 \\ \(\)=0.05 & 54.33\(\)0.54 & 54.27\(\)0.38 & 53.30\(\)0.01 \\ \(\)=0.10 & 56.83\(\)0.19 & 53.53\(\)0.06 & 54.60\(\)0.15 \\ \(\)=0.3 & 66.83\(\)0.02 & 68.20\(\)0.04 & 67.53\(\)0.03 \\ \(\)=0.5 & 73.20\(\)0.03 & 73.33\(\)0.06 & 72.17\(\)0.04 \\ \(\)=1.0 & 76.53\(\)0.02 & 76.03\(\)0.05 & 73.47\(\)0.19 \\ \#C=1 & 12.73\(\)0.01 & 13.20\(\)0.02 & 14.17\(\)0.02 \\ \#C=2 & 45.20\(\)0.21 & 46.13\(\)0.15 & 44.80\(\)0.03 \\ \#C=3 & 58.97\(\)0.07 & 57.90\(\)0.06 & 55.60\(\)0.06 \\   

Table 4: Experimental results of different hyper-parameter \(\) on FMNIST dataset.

    & Overall & Overall \\  & Computation (minus) & Communication (MB) \\  FedLPA & 65 & 4.98 \\ FedNova & 50 & 2.47 \\ SCAFFOLD & 50 & 4.94 \\ FedAvg & 50 & 2.47 \\ FedProx & 75 & 2.47 \\ DENSE & 400 & 2.47 \\   

Table 5: Communication and computation overhead evaluation.