# LLAVIDAL : Benchmarking Large LAnguage

Vision Models for Daily Activities of Living

 Rajatsubhra Chakraborty\({}^{*}\) Arkaprava Sinha\({}^{*}\) Dominick Reilly\({}^{*}\) Manish Kumar Govind Pu Wang Francois Bremond\({}^{}\) Srijan Das

UNC Charlotte \({}^{}\)Inria \({}^{}\)Universite Cote d'Azur

\({}^{*}\) Equal contribution {rchakra6, asinha13, dreilly1}@charlotte.edu

###### Abstract

Large Language Vision Models (LLVMs) have demonstrated effectiveness in processing internet videos, yet they struggle with the visually perplexing dynamics present in Activities of Daily Living (ADL) due to limited pertinent datasets and models tailored to relevant cues. To this end, we propose a framework for curating ADL multiview datasets to fine-tune LLVMs, resulting in the creation of **ADL-X**, comprising 100K RGB video-instruction pairs, language descriptions, 3D skeletons, and action-conditioned object trajectories. We introduce **LLAVIDAL**, an LLVM capable of incorporating 3D poses and relevant object trajectories to understand the intricate spatiotemporal relationships within ADLs. Furthermore, we present a novel benchmark, **ADLMCQ**, for quantifying LLVM effectiveness in ADL scenarios. When trained on ADL-X, LLAVIDAL consistently achieves state-of-the-art performance across all ADL evaluation metrics. Qualitative analysis reveals LLAVIDAL's temporal reasoning capabilities in understanding ADL. The link to the dataset is provided at: [https://adl-x.github.io/](https://adl-x.github.io/)

## 1 Introduction

Human cognitive perception integrates information from multiple sensory modalities to form a unified representation of the world . Towards emulating human cognitive perception in digital intelligence, initial efforts focused on integrating vision and language modalities . Subsequently,

Figure 1: **Comparison of LLVM vs LLAVIDAL** : In real world scenarios, web-video trained models struggle to understand Activities of Daily Living due to the subtle nuances in the video, whereas our **ADL-X** trained LLAVIDAL model triumphs in understanding complex human-object interactions.

the success of LLMs like GPT , PALM , BLOOM  led to the introduction of multimodal conversational models[10; 11; 12; 13; 14; 15; 16] that combine image pixels and LLMs, we dub as Large Language-Vision Language Models (LLVMs). However, these image-based LLVMs lack the capability for complex reasoning and interactions, particularly in understanding spatio-temporal relationships involved in human activities. In this study, we investigate the understanding of Activities of Daily Living (ADL) videos by LLVMs, which present various challenges including multiple exocentric viewpoints, fine-grained activities with subtle motion, complex human-object interactions, and long-term temporal relationships. We envision that LLVMs capable of addressing these challenges will significantly influence the future intelligent systems, particularly in healthcare applications such as eldercare monitoring, cognitive decline assessment, and robotic assistance development.

Recently, [17; 18; 19; 20; 21; 22; 23] have integrated videos into LLMs, leading to the development of video-based LLVMs capable of capturing spatio-temporal features. However, these models are predominantly trained on large-scale web videos [24; 25; 26; 27; 28], which mainly consists of sports clips, movie excerpts, and instructional videos. These videos, typically filmed by professionals, follow strict temporal sequences in closely controlled background (e.g., Paragliding). The evident temporal structure and scene semantics in such videos facilitate spatial understanding within LLVMs, as shown in 1. In contrast, ADL videos pose additional challenges, characterized by temporal unstructuredness where diverse actions may unfold concurrently within a single sequence . For instance, _a person cooking could intermittently engage in unrelated activities like making a phone call or drinking water, disrupting the linear progression of the composite action cooking_. Consequently, existing LLVMs trained on web videos struggle to capture such visually perplexing dynamics inherent in ADL scenarios. Moreover, unlike specialized video architectures designed for understanding ADL [30; 31; 32; 33; 34; 35; 36], these LLVMs lack explicit utilization of cues like 3D poses or object encodings, which are crucial for understanding ADL. These cues aid in learning view-invariant representations and capturing fine-grained details essential for interpreting complex human activities. Hence, the current limitations in understanding ADL stem from the lack of instruction tuning of LLVMs on real-world multiview ADL datasets captured in indoor settings and the simplistic design of LLVMs with holistic operations.

To this end, we propose a framework of curating ADL videos for instruction tuning LLVMs. This framework introduces the **ADL-X** dataset, comprising 100K untrimmed RGB video-instruction pairs, 3D poses (P), language descriptions, and action-conditioned object trajectories (see Table 1). We then introduce the **Large L**Anguage **VI**sion model for **D**aily **A**ctivities of **L**iving (**LLAVIDAL**), trained on ADL-X, which integrates videos, 3D poses, and object cues into the LLM embedding space. Our study explores various strategies for integrating 3D pose information and human-object interactions within LLVMs, demonstrating that language contextualized features extracted from 3D poses and object trajectories can effectively be integrated into LLAVIDAL. Furthermore, we introduce a benchmark ADL Multiple Choices Question (**ADLMCQ**), specifically designed to evaluate the effectiveness of LLVMs for ADL. ADLMCQ includes action recognition (ADLMCQ-AR) and action forecasting (ADLMCQ-AF), assessed through a multiple choice question-answering task. We also evaluate existing LLVMs for generating video description of ADL scenes and compare their performance with LLAVIDAL. Our empirical findings indicate that LLAVIDAL with object cues, outperforms other LLVMs, including those trained on datasets of ten times the size, on the ADL benchmarks.

To summarize our contributions:

* We introduce ADL-X, the first multiview RGBD instruction ADL dataset, curated through a novel semi-automated framework for training LLVMs.
* LLAVIDAL is introduced as the first LLVM tailored for ADL, incorporating 3D poses and object cues into the embedding space of the LLM.
* A new benchmark, ADLMCQ, is proposed for an objective evaluation of LLVMs on ADL tasks, featuring MCQ tasks for action recognition & forecasting.
* Exhaustive experiments are conducted to determine the optimal strategy for integrating poses or objects into LLAVIDAL. Evaluation of existing LLVMs on ADLMCQ and video description tasks reveals that LLAVIDAL trained on ADL-X significantly outperforms baseline LLVMs.

## 2 Semi-automated Framework for generating ADL Video-instructions Pairs

This section describes the data curation framework employed for the creation of a novel dataset, ADL-X. This dataset specifically caters to the instruction tuning of LLVMs within the ADL domain. ADL-X comprises video recordings of ADLs. To enrich the dataset and facilitate LLM training, question-answer (QA) pairs were generated from a corpus of long-form ADL videos. These QA pairs target various aspects of the ADLs, including: human pose configuration, objects relevant to the human actions, scene appearance, and the fine-grained actions performed. We hypothesize that incorporating such instructional tuning during the LLVM training process will promote alignment of visual tokens within the LLM's embedding space. ADL-X represents a comprehensive ADL dataset encompassing various modalities: - RGB videos, 3D poses, Language descriptions, object tracklets. This rich dataset offers a valuable tool for evaluating the capabilities of LLVMs in tasks related to ADLs, including description, recognition, and anticipation.

A critical characteristic of ADL videos lies in the inherent spontaneity of the actions performed. Unlike scripted scenarios [25; 37; 38], fine-grained actions within ADLs often occur randomly. To capture this essential characteristic within our dataset, we curated ADL-X from NTU RGB+D 120 dataset . This selection was motivated by the dataset's focus on ADL videos and its inherent diversity in terms of actions, subjects, and camera viewpoints. Also, this data curation framework could be extended to any existing trimmed/untrimmed ADL datasets [40; 41; 42]. Below, we elaborate the steps involved in building the ADL-X in a chronological order.

**Person-centric Cropping.** ADL tasks necessitate a focus on the individual performing the actions, the actions themselves, and the human-object interactions. To achieve this targeted focus within the data curation framework, we implemented a person-centric cropping strategy leveraging the pose information captured through Kinect sensors . By using the pose information in each frame of the NTU RGB+D 120 dataset, we are able to detect and crop out the person(s) performing the actions. This cropping process effectively reduces the amount of background information present in the videos, eliminating data irrelevant to the target ADLs. This step is crucial as existing ADL datasets often contain extensive background information that is not relevant to the actions being performed. The presence of such extraneous information can significantly hinder subsequent stages within the data curation framework.

**Stitching shorts clips.** To capture the inherent randomness of real-world ADLs, we constructed a set of 160 composite action sequences. These sequences were generated by prompting a GPT to combine individual actions from the original NTU RGB+D 120 dataset's list of 120 actions (denoted as \(A_{1}\), \(A_{2}\),..., \(A_{120}\)). An example sequence structure could be represented as \(A_{1} A_{3} A_{17}\). Following these

  
**Dataset** & **Modalities** & **Subjects** & **Multiple** & **Videos** & **QA Pairs** & **Atomic Actions** & **Temporal** & **Object** & **Type** \\  & & & & & & & **P Vid** & **Rand** & **Traj**. & \\  TimeTiT & RGB+L & NA & No & 173000 & 173K & Medium & No & No & No \\ VideoChar & RGB+L & NA & No & 8196 & 11K & Low & No & No & Web \\ Valley & RGB+L & NA & No & 64,687 & 65K & Low & No & No & Web \\ VideoChar & RGB+L & NA & No & 27,801 & 100K & Medium & No & Web \\ 
**ADL-X** & **RGB+P+L** & **106** & **Yes** & 16,343 & 100K & **High** & **Yes** & **Yes** & **ADL** \\   

Table 1: Video Instruction Dataset Comparison.

Figure 2: Dataset Curation Pipeline: We employ CogVLM as our person-centric image captioner and GPT 3.5 Turbo as our summarizer and QA generator.

generated composite action sequences, we temporally stitched together short video clips (\(clip_{j}^{a}\), where \(a\) is the action class) from the NTU dataset. This stitching process ensured that all clips within a video belonged to the same subject and camera view, maintaining coherence in the resulting video sequence. For instance, a stitched video sequence might be represented as \([clip_{r1}^{1} clip_{r2}^{3} clip_{r3}^{17}]\) where \(r1\), \(r2\), \(r3\) represent unique clip identifiers within the dataset for the specific subject performing the actions (actions 1, 3, and 17, respectively). The intentional randomness of the generated action sequences reflects the unstructured flow of actions encountered in ADL. To further enhance diversity and ensure no bias towards specific subject-action combinations, we shuffled both the action sequences and the subject assignments. This process resulted in the creation of **16,343 stitched videos** with an average 5 actions per video.

**Frame Level Captioning and Dense Descriptions.** This step is the process of generating weak pseudo-labels for automated instruction tuning of the LLVM with the curated dataset. An image captioning model CogVLM  is employed to automatically generate frame-level captions for the stitched ADL videos at a rate of \(0.5fps\). These captions are subsequently compiled into a dictionary linking each frame identifier to its corresponding description. To enhance the reliability of the pseudo-labels, we implemented an action-conditioned filtering while generating the video descriptions. The dictionary with the frame descriptions, along with the action labels present in the stitched videos, are then used to prompt a GPT 3.5 turbo model to generate a cohesive structured description of the entire stitched video, constrained to a maximum of 300 words. This step leverages the known action labels associated with each video to remove irrelevant noise potentially introduced during the caption generation process. We evaluated various image captioning models, including BLIP-2 , and InstructBLIP  for frame-level caption generation. However, CogVLM is ultimately chosen due to its ability to generate denser and appropriate descriptions. Please refer to the appendix for our detailed prompting strategy in generating the descriptions.

**Generating QA Pairs.** LLVMs necessitate training data in the form of question-answer (QA) pairs. To generate domain-specific QA pairs for ADL, we leverage the dense video descriptions obtained in the previous step as illustrated in Figure 2. An instruction template (detailed in the Appendix) guides GPT-3.5 in formulating questions across various categories relevant to ADL. These categories include: video summary, performed actions, spatial details, human-object interactions and other video-specific inquiries. Through this prompting approach, we curate a dataset of **100K video instruction pairs**, namely ADL-X, for the stitched ADL videos. These QA pairs benefit from the detailed descriptions and person-centric cropping, resulting in reduced LLM hallucinations compared to other existing methods .

Notably, the framework employed for constructing ADL-X from trimmed, labeled action videos can be generalized to other existing datasets. This generalization paves the way for efficient training of domain-specific LLVMs.

Figure 3: Overview of **LLAVIDAL**, which utilizes an LLM to integrate multiple modalities, including video, pose, and object features. Videos are represented by embeddings obtained from a **VLM**, poses are processed through **(PoseLM)**, and object embeddings are obtained through **(ObjectLM)**. These embeddings are projected into the LLM space, where they are concatenated with tokenized text queries for instruction tuning.

## 3 LLAVIDAL: An LLVM for ADL

LLAVIDAL is a large language vision model designed to align ADL videos with an LLM to generate meaningful conversation about the daily activities performed by humans. This model, similar to Video-ChatGPT  and LLaVA , integrates a visual encoder with the Vicuna language decoder  and is fine-tuned on instructional language-vision data. Unlike Video-ChatGPT  and LLaVA , LLAVIDAL leverages the random temporal structure present in ADL-X and incorporates additional data modalities such as 3D human poses and human-object interaction cues. This allows LLAVIDAL to generate accurate conversations that are not only contextually appropriate but also temporally aligned with the human activities depicted in the input video. This section will first present a background of LLVM models to align videos with LLMs. Then, we will outline the strategies employed to integrate 3D poses and object interaction cues within the language space of the LLM for enhanced understanding of videos featuring ADL. Subsequently, we will describe the training architecture of LLAVIDAL.

### Background: LLVM

Following , given an input video denoted by \(_{i}^{T H W C}\), where \(T\) represents the frames encoded using a pretrained vision-language model (**VLM**) CLIP-L/14  to obtain frame-level embeddings for the video, \(x_{i}^{T h w D}\), with \(D\) as the embedding dimension, and \(h=H/p\), \(w=W/p\) representing the dimensions adjusted by patch size \(p\). Temporal and spatial features are extracted by aggregating these frame-level embeddings along the respective dimensions. The video-level features, \(V_{i}^{F_{v} D_{v}}\), are obtained by concatenating the temporal and spatial features, where \(F_{v}\) represents the spatio-temporal tokens and \(D_{}\) is the video feature dimension. The video features are projected into the LLM embedding space using a linear projection layer \(_{v}\). Thus, we obtain input tokens \(Q_{v}\) for the video features:

\[Q_{v}=_{v}(V_{i})^{F_{v} K} \]

The text query is also tokenized such that \(Q_{t}^{F_{t} K}\). The text query \(Q_{t}\), refers to a question from the training data. The input to the LLM is the concatenation of \(Q_{t}\) and \(Q_{v}\) following the template : [USER: \( Q_{t}\)\( Q_{v}\) Assistant:]. We perform instruction-tuning of the LLM on the prediction tokens, using its original auto-regressive training objective. The parameters of the LLM are frozen, thus the loss gradients only propagate through the projection layer \(_{v}\).

### 3D Poses for LLAVIDAL

ADL are rich in actions that primarily involve the movements of critical body parts or joints. The dataset ADL-X includes 3D human poses, which can be utilized to incorporate human kinematics and view-invariant features into the input embedding space of a LLM. These poses can be integrated into the LLM input space in several ways: as an additional text query \(Q_{t}\) for instruction tuning of the LLM, by deriving language descriptions of joint movements to provide context for the LLM, or through features extracted using a suitable pose-language encoder.

**Poses as QA.** We input the 3D joint coordinates alongside the associated human action from the video into GPT-3.5 Turbo , which generates a general description of the pose. This description is then re-fed into GPT-3.5 Turbo to generate two QA pairs that provide detailed explanations of the action's motions. These QA pairs are subsequently added to the set of text queries \(Q_{t}\) in our training set for instruction tuning the LLM.

**Poses as Context.** To extract contextual information from human poses, we initially identify five peripheral joints -- the head, right hand, left hand, right knee, and left knee -- due to their significant contribution to motion in various actions. Using GPT-3.5 Turbo, we generate descriptions of the motion for each of these joints based on their trajectories throughout the video, specifically focusing on how the coordinates of these five joints evolve. The generated descriptions, denoted as \(Q_{t}^{p}\), are subsequently appended to the text query \(Q_{t}\), incorporates these pose descriptions as additional contextual information. This enriched query \(Q_{t}^{new}=[Q_{t}^{p}\ Q_{t}]\) is then employed for instruction tuning of the LLAVIDAL.

**Poses as Features.** To incorporate poses as tokens into the LLM, it is crucial to align the pose features with a language-contextualized space. To achieve this, we utilize a pretrained Pose-Language model (**PoseLM**), specifically PoseCLIP, to extract pose features that are aligned with the languagedomain. The PoseCLIP model comprises a pose backbone  and a CLIP text encoder , and it undergoes training in two phases. Initially, the pose backbone is pretrained on the NTU RGB+D dataset  for action classification. Subsequently, in the second phase, we optimize the similarity between pose features and text features, which encode the prompts describing their action labels, using cross-entropy supervision as outlined in . Further details on the training of this model are provided in the Appendix. These pose features, denoted as \(P_{i}^{F_{p} D_{p}}\), where \(D_{p}\) represents the pose feature dimension, can be utilized as input tokens for training LLAVIDAL.

### Action-Conditioned Object Cue for LLAVIDAL

To comprehensively understand ADL, it is crucial to not only grasp the semantics of objects but also their trajectories, which are closely linked to the actions performed. Consequently, we propose to explicitly utilize these object trajectories as integral components for training LLAVIDAL. Our framework involves a two-stage pipeline to extract object information directly from RGB video data: (i) _Action-conditioned object detection_ and (ii) _Object Localization and Tracking_. Both stages leverage off-the-shelf models that are effective without the need for additional training, facilitating integration into LLAVIDAL for ADL analysis.

**Action conditioned object detection.** Given a stitched ADL video, which comprises a sequence of trimmed video segments (denoted as \(clip_{j}\)), the first stage extracts the categories of objects present that are pertinent to the actions performed within each clip. We uniformly sample 8 frames from each video and employ a pre-trained BLIP-2 model  to generate a list of distinct objects observed in the frames. To avoid training LLAVIDAL with noisy data, we perform a filtering on the list of objects using the ground-truth action labels and GPT-3.5. Specifically, for each \(clip_{j}\) within a stitched video, we input the corresponding action label and the list of detected objects to GPT-3.5 and prompt it to identify the object(s) most relevant to the given action. For instance, if the objects _plant, chair, bottle, table_ are detected in a video labeled with the action _Drinking_, GPT-3.5 is expected to filter out and select [_bottle_] as the relevant object for \(clip_{j}\). Refer to the appendix for our detailed action conditioned object detection prompting strategy.

**Object Localization and Tracking.** Given the list of relevant objects identified in the first stage, the second stage involves spatial localization of these objects within the scene and their temporal association (i.e., object tracking) based on the feature similarity of the image regions corresponding to the localized objects in the stitched video. We employ a pre-trained open vocabulary object localization model (**ObjectLM**), OWLv2 , and input the list of relevant objects detected in stage 1 along with the corresponding video. Localization and tracking are performed on 8 frames that are uniformly sampled from \(clip_{j}\) within a stitched video. For each frame, we obtain bounding boxes \(B_{t}^{n 4}\), where each bounding box corresponds to one of the \(n\) relevant objects in the \(t\)th frame. Features for each object are then extracted from the image regions within these bounding boxes using our object localization model. We denote the features for the objects in frame \(t\) as \(O_{t}^{8n D_{a}}\), where \(D_{o}\) is the object feature dimension. To associate objects across frames, we utilize a feature-based object tracking approach. Specifically, for each object in frame \(t\), represented by the feature vector \(O_{i}^{t}^{D_{a}}\), we compute the cosine similarity between \(O_{i}^{t}\) and all feature vectors in frame \(t+1\). The object \(i\) in frame \(t\) is then associated with the object in frame \(t+1\) that exhibits the highest similarity score. This matching process is iterated for all objects in each frame, thereby establishing a track for each relevant object throughout the sampled frames. These object tracks, with corresponding bounding boxes and features, facilitate the integration of object information into the training of LLAVIDAL: Object as QA, Object as context, and Object as features.

**Object as QA.** Similar to the approach taken with poses, to generate QA pairs for objects, we formulate a question based on the trajectory coordinates of the relevant object(s). These QA pairs are added to the set of text queries \(Q_{t}\) for instruction tuning LLAVIDAL.

**Object as Context.** To integrate the context of detected objects into the LLM space, we append the list of relevant object labels, denoted by \(Q_{t}^{o}\), to each text query token \(Q_{t}\). Consequently, the updated text query is represented as \(Q_{t}^{new}=[Q_{t}^{o}\ Q_{t}]\). This enhanced text query, \(Q_{t}^{new}\), is utilized for instruction tuning.

**Object as Features.** The object features extracted during the object localization and tracking stage are utilized as input tokens \(Q_{o}^{8n D_{o}}\), which are incorporated alongside the text query tokens (\(Q_{t}\)) and input video tokens (\(Q_{v}\)). For \(n\) relevant objects detected, the object query \(Q_{o}\) is structured usingthe following template \([ Q_{o}\ = Q_{o}^{1}\  Q_{o}^{2}\...  Q_{o}^{n}]\) where \(Q_{o}^{j}^{8 D_{o}}\) represent the features of each relevant object in the video.

### Training LLAVIDAL

As illustrated in Figure 3, the QA pairs, along with context or features obtained from the RGB video, 3D poses, and object cues can be integrated into LLAVIDAL. Integrating QA pairs and contextual information is straightforward; they are introduced into \(Q_{t}\) and trained using standard methods for LLVM. However, to integrate other modalities with features, we feed these additional cues through specific projection layers designed to align them with the input space of the LLM. Accordingly, the video, pose, and object features are projected into the LLM embedding space using linear projection layers \(_{j}\) for each cue \(j=\{v,p,o\}\), resulting in LLM input token representation of the video, pose, and object cues, respectively:

\[Q_{v}=_{v}(V_{i});\ \ \ \ \ Q_{p}=_{p}(P_{i});\ \ \ \ \ Q_{o}=_{o}(O_{i}) \]

where \(Q_{j}^{F_{j} K}\). Thus, the input to the LLM comprises the concatenation of \(Q_{t}\) and \(Q_{j}\) for \(j=\{v,p,o\}\), structured according to the template: \([\)USER: \( Q_{t}\  Q_{v}\  Q_{o}\  Q_{p}\) Assistant\(]\). This training scheme ensures that the video, object, and pose cues are effectively aligned to the LLM embedding space, facilitating an accurate understanding of ADL. During the **inference**, LLAVIDAL utilizes only the holistic video cue, omitting person-centric cropping and consequently eliminating additional cues. In practice, the embedding dimensions are \(D_{v}=1024\) for visual, \(D_{o}=512\) for object features, \(D_{p}=216\) for pose features and \(K=4096\). The number of tokens is set as \(F_{v}=356\) and \(F_{p}=256\) for visual and pose tokens respectively. We train LLAVIDAL for \(3\) epochs with a batch size of \(32\) and a learning rate of \(2e^{-5}\) on 8 A6000 48GB GPUs. For the purpose of promoting research in this field, we also provide the pose features and object trajectories of LLAVIDAL along with the dataset.

## 4 Experiments

### Experimental Setting

**Evaluation Metrics.** Inspired by , LLVM's ability to generate video-level descriptions is evaluated. This involves comparing the generated descriptions with ground truth and scoring them on dimensions such as Correctness of Information, Detail Orientation, Contextual Understanding, Temporal Understanding, and Consistency, with scores scaled to be bounded at \(100\). Due to the subjective nature of this metric, Mementos Evaluation  is also conducted to assess the recognition of common action-verbs and object-nouns in the video descriptions compared to ground truth, presenting F1 scores for these classifications. However, comparing video descriptions generated by LLVMs presents a challenge due to the inherently subjective nature of these descriptions. Some objective evaluation benchmarks for LLVMs  primarily focus on video tasks involving in-the-wild activities. Therefore, this paper introduces novel benchmarks for assessing LLVM's temporal understanding of ADL videos. We propose two new **ADLMCQ** benchmarks including ADLMCQ-AR and ADLMCQ-AF. ADLMCQ-AR involves multiple-choice question-answering for action recognition, where the model selects the correct action from a set of options given a question about the action performed in a video. Similarly, ADLMCQ-AF focuses on action forecasting, requiring the model to predict the next action based on the preceding actions. It is important to note that all evaluations are performed zero-shot.

**Evaluation Datasets.** For ADLMCQ-AR evaluation, we utilize the Charades  and Toyota Smarthome  datasets. Evaluation for ADLMCQ-AF is conducted using LEMMA  and Toyota Smarthome Untrimmed (TSU)  datasets. Video description tasks are assessed using the Charades and TSU datasets, both featuring long-duration videos with multiple actions per video. Notably, for the TSU dataset, we manually annotated video descriptions with fine-grained details regarding activities performed by elderly individuals, employing 6 human annotators for 174 videos. Our evaluation relies on these annotated descriptions, which we also provide to the community as part of the test set for ADL-X.

### Impact of ADL-X Training on LLVMs

To understand the requirement of ADL-X, we assess VideoChatGPT  trained on 100K instruction pairs from ActivityNet , trimmed NTU120 , and ADL-X in Table 2. Notably,

[MISSING_PAGE_FAIL:8]

generating clip-level descriptions. Subsequently, we concatenate all clip-level descriptions and utilize GPT-3.5 turbo to summarize them into a video-level description, following the same instruction template utilized in our dense description pipeline for ADL-X. LLAVIDAL consistently surpasses SOTA and outperforms all models including, image captioners-summarizers pipelines which are trained on billions of images, across all 5 VideoChatGPT metrics. However, in the Mementos Evaluation, LLVM baselines exhibit superior performance over LLAVIDAL in the Smarthome domain. This discrepancy may be attributed to the loss of relevant information when generating video-level descriptions using GPT.

**ADLMCQ.** Table 5 compares LLAVIDAL to SOTA LLVMs on the ADLMCQ-AR benchmark. LLAVIDAL achieves significant improvements, surpassing VideoChatGPT by +5.4% and +44.1% on the Charades and Smarthome datasets, respectively. Similarly, Table 6 demonstrates LLAVIDAL's superiority on the ADLMCQ-AF benchmark. It outperforms VideoChatGPT by up to +47.3%, highlighting its exceptional capability in action forecasting tasks.

Figure 4 provides a visual comparison of LLAVIDAL against representative baselines on the ADL benchmarks. More visual samples are provided in the Appendix.

## 5 Conclusion & Future Work

In this work, we present a framework for curating ADL datasets for instruction tuning LLVMs, thus introducing ADL-X. We introduce LLAVIDAL, an LLVM capable of integrating 3d poses and human-object interaction cues by projecting their language contextualized representations into the LLM embedding space. To assess LLVM performance in ADL scenarios, we propose the ADLMCQ benchmark. Results demonstrate that LLAVIDAL, when trained on ADL-X, surpasses other LLVM baselines in ADLMCQ tasks, indicating its efficacy in grasping intricate temporal relationships within ADL contexts. Future research will focus on expanding ADL-X by integrating additional curated ADL datasets and exploring stage-wise training strategies to effectively integrate both pose and object cues within LLAVIDAL.

  
**Method** & **LEMMA** & **TSU** \\  VideoLlama  & 20.8 & 15.6 \\ VideoLava  & 32.2 & 20.2 \\ VideoChatGPT  & 35.7 & 25.0 \\ ADL-X ChangGPT  & 44.8 & 25.3 \\ LEAVIDAL & **52.6** & **27.0** \\   

Table 6: ADLMCQ - Action Forecasting

  
**Method** & **Charades** & **Smarthome** \\  VideoLlama  & 33.0 & 27.4 \\ VideoLava  & 44.4 & 54.0 \\ VideoChatGPT  & 56.0 & 40.8 \\ AudioC-X ChangGPT  & 58.0 & 52.3 \\ LEAVIDAL & **59.0** & **58.8** \\   

Table 5: ADLMCQ - Action Recognition

Figure 4: Qualitative results comparing LLAVIDAL with SOTA models. Incorrect descriptions are marked in red.