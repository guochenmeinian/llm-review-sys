# Robustness Guarantees for Adversarially Trained Neural Networks

Poorya Mianjy

Johns Hopkins University

&Raman Arora

Johns Hopkins University

arora@cs.jhu.edu

###### Abstract

We study robust adversarial training of two-layer neural networks as a bi-level optimization problem. In particular, for the inner loop that implements the adversarial attack during training using projected gradient descent (PGD), we propose maximizing a _lower bound_ on the \(0/1\)-loss by reflecting a surrogate loss about the origin. This allows us to give a convergence guarantee for the inner-loop PGD attack. Furthermore, assuming the data is linearly separable, we provide precise iteration complexity results for end-to-end adversarial training, which holds for any width and initialization. We provide empirical evidence to support our theoretical results.

## 1 Introduction

Despite the tremendous success of deep learning, neural network-based models are highly susceptible to small, imperceptible, adversarial perturbations of data at test time [Szegedy et al., 2014]. Such vulnerability to _adversarial examples_ imposes severe limitations on the deployment of neural networks-based systems, especially in critical high-stakes applications such as autonomous driving, where safe and reliable operation is paramount.

An abundance of studies demonstrating adversarial examples across different tasks and application domains [Goodfellow et al., 2014, Moosavi-Dezfooli et al., 2016, Carlini and Wagner, 2017] has led to a renewed focus on robust learning as an active area of research within machine learning. The goal of _robust learning_ is to find models that yield reliable predictions on test data notwithstanding adversarial perturbations. A principled approach to training models that are robust to adversarial examples that has emerged in recent years is that of _adversarial training_[Madry et al., 2018]. Adversarial training formulates learning as a min-max optimization problem wherein the 0-1 classification loss is replaced by a convex surrogate such as the cross-entropy loss, and alternating optimization techniques are used to solve the resulting saddle point problem.

Despite empirical success of adversarial training, our understanding of its theoretical underpinnings remain limited. From a practical standpoint, it is remarkable that gradient based techniques can efficiently solve both inner maximization problem to find adversarial examples and outer minimization problem to impart robust generalization. On the other hand, a theoretical analysis is challenging because (1) both the inner- and outer-level optimization problems are non-convex, and (2) it is unclear a-priori if solving the min-max optimization problem would even guarantee robust generalization.

In this work, we seek to understand adversarial training better. In particular, under a margin separability assumption, we provide robust generalization guarantees for two-layer neural networks with Leaky ReLU activation trained using adversarial training. Our key contributions are as follows.

1. We identify a disconnect between the robust learning objective and the min-max formulation of adversarial training. This observation inspires a simple modification of adversarial trainingwe propose _reflecting_ the surrogate loss about the origin in the inner maximization phase when searching for an "optimal" perturbation vector to attack the current model.
2. We provide convergence guarantees for PGD attacks on two-layer neural networks with leaky ReLU activation. This is the first of its kind result to the best of our knowledge.
3. We give global convergence guarantees and establish learning rates for adversarial training for two-layer neural networks with Leaky ReLU activation function. Notably, our guarantees hold for _any bounded initialization_ and _any width_ - a property that is not present in the previous works in the neural tangent kernel (NTK) regime (Gao et al., 2019; Zhang et al., 2020).
4. We provide extensive empirical evidence showing that reflecting the surrogate loss in the inner loop does not have a significant impact on the test time performance of the adversarially trained models.

Notation.We denote matrices, vectors, scalar variables, and sets by Roman capital letters, Roman lowercase letters, lowercase letters, and uppercase script letters, respectively (e.g. X, x, \(x\), and \(\)). For any integer \(d\), we represent the set \(\{1,,d\}\) by \([d]\). The \(_{2}\)-norm of a vector x and the Frobenius norm of a matrix X are denoted as \(\|\|\) and \(\|\|_{F}\), respectively. Given a set \(\), the operator \(_{}()=_{^{}}\| -^{}\|\) projects onto the set \(\) with respect to the \(_{2}\)-norm.

### Related Work

Linear models.Adversarial training of linear models was recently studied by Charles et al. (2019); Li et al. (2020); Zou et al. (2021). In particular, Charles et al. (2019); Li et al. (2020) give robust generalization error guarantees for adversarially trained linear models under a margin separability assumption. The hard margin assumption was relaxed by Zou et al. (2021) who give robust generalization guarantees for distributions with agnostic label noise. We note that the optimal attack for linear models has a simple closed-form expression, which mitigates the challenge of analyzing the inner loop PGD attack. In contrast, one of our main contributions is to give convergence guarantees for the PGD attack. Nonetheless, as the Leaky ReLU activation function can also realize the identity map for \(=1\), our results also provide robust generalization error guarantees for training linear models.

Non-linear models.Wang et al. (2019) propose a first order stationary condition to evaluate the convergence quality of adversarial attacks found in the inner loop. Zhang et al. (2021) study adversarial training as a bi-level optimization problem and propose a principled approach towards the design of fast adversarial training algorithms. Most related to our results are the works of Gao et al. (2019) and Zhang et al. (2020), which study the convergence of adversarial training in non-linear neural networks. Under specific initialization and width requirements, these works guarantee small robust training error with respect to the attack that is used in the inner-loop, without explicitly analyzing the convergence of the attack. Gao et al. (2019) assume that the activation function is smooth and require that the width of the network, as well as the overall computational cost, is exponential in the input dimension. The work of Zhang et al. (2020) partially addresses these issues. In particular, their results hold for ReLU neural networks, and they only require the width and the computational cost to be polynomial in the input parameters.

Our work is different from that of Gao et al. (2019) and Zhang et al. (2020) in several ways. Here we highlight three key differences.

* First, while the prior works analyzes the convergence in the NTK setting with specific initialization and width requirements, our results hold for any initialization and width.
* Second, none of the prior works studies computational aspects of finding an optimal attack vector in the inner loop. Instead, the prior work assumes oracle access to optimal attack vectors. We provide precise iteration complexity results for the projected gradient method (i.e., for the PGD attack) for finding near-optimal attack vectors.
* Third, the prior works focus on minimizing the robust training loss, whereas we provide computational learning guarantees on the robust generalization error.

The rest of the paper is organized as follows. In Section 2, we give the problem setup and introduce the adversarial training procedure with the reflected surrogate loss in the inner loop. In Section 3, we present our main results, discuss the implications and give a proof sketch. We support our theory with empirical results in Section 4 and conclude with a discussion in Section 5.

```
0: Sample \((,y)\), Weights \(\), Stepsize \(_{}\), # Iters \(T_{}\)
1: Initialize \(_{1}\)
2:for\(t=1\) to \(T\)do
3:\(_{t+1}_{()}(_{t}+_{} _{}_{-}(yf_{}(_{t})))\)
4:endfor
5:\(_{}\), where \(_{t[T]}_{-}(yf_{}(_{t}))\)
```

**Algorithm 1**Atk PGD Attack

## 2 Preliminaries

We focus on two-layer networks with \(m\) hidden nodes computing \(f(;,)=^{}( )\), where \(^{m d}\) and \(^{m}\) are the weights of the first and the second layers, respectively, and \((z)=\{ z,z\}\) is the Leaky ReLU activation function. We randomly initialize the weights \(a_{r}(\{-,+\})\) for all hidden nodes \(r[m]\), and randomly initialize \(\) such that \(\|\|_{F}\), for some positive real numbers \(\) and \(\). The top linear layer (i.e., weights \(\)) is kept fixed, and the hidden layer (i.e., \(\)) is trained using stochastic gradient descent (SGD).

For simplicity of notation, we represent the network as \(f(;)\), suppressing the dependence on the top layer weights. Further, with a slight abuse of notation, we denote the function by \(f_{}()\) when optimizing over the input adversarial perturbations, and by \(f_{}()\) when training the network weights.

Formally, adversarial learning is described as follows. Let \(^{d}\) and \(=\{ 1\}\) denote the input feature space and the output label space, respectively. Let \(\) be an unknown joint distribution on \(\). For any fixed \(\), we consider norm-bounded adversarial perturbations in the set \(():=\{:\ \|-\|\}\), for some fixed noise budget \(\).

Given a training sample \(:=\{(_{i},y_{i})\}_{i=1}^{n}^{n}\) drawn independently and identically from the underlying distribution \(\), the goal is to find a network with small robust misclassification error

\[_{}()=_{}_{ ()}[yf_{}}()<0], \]

where \(}:=/\|\|_{F}\) is the weight matrix normalized to have unit Frobenius norm. Note that, due to the homogeneity of Leaky ReLU, such normalization has no effects on the robust error whatsoever.

In adversarial training, the \(0-1\) loss inside the expectation is replaced with a convex surrogate such as cross entropy loss \((z)=(1+e^{-z})\), and the expected value is estimated using a sample average:

\[_{}():=_{i=1}^{n}_ {_{i}(_{i})}(y_{i}f_{}}(_{i })) \]

Notwithstanding the conventional wisdom, adversarial training entails _maximizing_ an _upper bound_ as opposed to a _lower bound_ on the \(0-1\) loss. In contrast, we propose using a _concave lowerbound_ on the \(0-1\) loss to solve the inner maximization problem. Let \(_{-}(z)=-(-z)=-(1+e^{z})\) denote the _reflected loss_. In Figure 1, we plot the 0-1 loss, the cross-entropy loss, and the reflected cross-entropy loss. Starting from \(_{1}=\), the PGD attack updates iterates via

\[_{t+1}=_{()}(_{t}+_{}_{-} (yf_{}(_{t}))),\]

as described in Algorithm 1. We emphasize that the only difference between standard adversarial training and what we propose in Algorithm 2 and Algorithm 1 is that we reflect the loss (about the origin) in Algorithm 1.

Figure 1: The 0-1 loss (red), its convex surrogate, the cross-entropy loss (blue), and the reflected cross-entropy loss (green).

```
0: Stepsize \(_{}\), \(\#\) Ifes \(T_{}\)
1: Initialize a \(\{-,+\}^{m}\) and \(_{1}\) such that \(\|_{1}\|_{F}\)
2:for\(t=1\) to \(T\)do
3: Draw \((_{t},y_{t})\)
4:\(_{t}(_{t},_{t},y_{t})\)
5:\(_{t+1}_{t}-_{}_{ }(y_{t}f_{_{t}}(_{t}))\)
6:endfor
```

**Algorithm 2**AdvTr Adversarial Training

## 3 Main Results

We consider a slightly weaker version of the robust error. In particular, we are interested in adversarial attacks that can fool the learner with a margin - for some small, non-negative constant \(\), we define the _\(\)-robust misclassification error_ as: \(_{}()=\{_{( )}yf_{}()<-\}.\) In particular, as \(\) tends to zero, \(_{}()_{}()\). When \(\) is a small positive constant bounded away from zero, \((,y)\) contributes to \(_{}()\) only if there exists an _attack_\(()\) such that \(f_{}\)_confidently_ makes a wrong prediction on \(\). In other words, \(\)-robust misclassification error is the probability that for \((,y)\), a _\(\)-effective attack_ exists.

**Definition 3.1** (Effective Attacks).: Given a neural networks with parameters \((,)\) and a data point \((,y)\) and some constant \(>0\), we say that \(_{*}()\) is a \(\)-effective attack if \(yf_{}(_{*})-\), where \(}=/\|\|_{F}\).

Our bounds depend on several important problem parameters. Before stating the main results of the paper, we remind the reader of these important quantities. \(\) denotes the attack size. \(\) and \(\) are the bounds on the norm of the parameters a and \(\) at the initialization. Finally, \(\) is the Leaky-ReLU parameter. Our first result stated in the following theorem1 gives convergence rates for Algorithm 1 in terms of the and the negated loss derivative \(-^{}()\) under the assumption that an effective attack exists. The negative derivative, \(-^{}()\), of the loss function has been used in several previous works to give an upper bound on the error Cao and Gu (2019); here, we borrow similar ideas from Frei et al. (2021). In particular, as it will become clear later, we will use positivity and monotonicity of \(-^{}()\) to give an upper bound on the \(\)-robust loss using Markov's inequality.

**Theorem 3.2**.: _Let \(_{*}\) be a \(\)-effective attack for a given network with weights \((,)\) and a given example \((,y)\), with \( 2(1-)\). Then, after \(T_{}\) iterations, PGD with step size \(_{}m\|\|_{F}^{2}}\) generates an attack \(_{}\) such that \(-^{}(yf_{}(_{*}))-2^{}(yf_{}( _{}))+}{_{}T_{}}\)._

Theorem 3.2 establishes that under proper initialization (\(=1/\)), when a \(\)-effective attack exists, Algorithm 1 finds a \(\)-suboptimal attack vector in \(O(^{2}/)\) iteration. We next study convergence of Algorithm 2 under the following distributional assumption.

**Assumption 3.3**.: Samples \((,y)\) are drawn i.i.d. from an unknown joint distribution \(\) that satisfies:

* \(\|\| R\) with probability \(1\).
* There exists a unit norm vector \(_{*}^{d}\), \(\|_{*}\|=1\), such that for \((x,y)\), we have with probability \(1\) that \(y(_{*})>0\).

The first assumption requires that the inputs are bounded, which is standard in the literature and is satisfied for most practical applications. The second assumption implies that \(\) is linearly separable with margin \(>0\). Of course, we do not need a non-linear neural network to robustly learn a predictor under such a distributional assumption. But can we even guarantee robust learnability of neural networks for such simple settings? Nothing is known as far as we know. We note that even for standard (non-robust) training of two-layer neural networks using SGD, the convergence guarantees in the hard margin setting were unknown until recently (Brutzkus et al., 2018). The following theorem establishes that adversarial training can efficiently find a network with small \(\)-robust error.

**Theorem 3.4** (Convergence of Algorithm 2).: _For any \(>0\), in at most \(T_{ tr}(1+e)}{(- )^{2}^{2}^{2}}\) iterations, Algorithm 2 with step-size \(_{ tr}(R+)^{2}}\) finds an iterate \(\) that, in expectation over \(\{(x_{t},y_{t})\}_{t=1}^{T_{ tr}}\), satisfies \(_{}(_{}) 2\) for any \( 2(1-)\), provided that for all \(t[T]\), \(_{ atk}m\|_{t}\|_{F}^{2}}\) and \(T_{ atk}}{_{ atk}}\)._

Some remarks are in order.

Beyond Neural Tangent Kernel.As opposed to the convergence results in the previous work (Gao et al., 2019; Zhang et al., 2020) which requires certain initialization and width requirements specific to the NTK regime, our results holds for _any bounded initialization_ and _any width_\(m\).

Role of the Robustness Parameter \(\).Our guarantee holds only when the desired robustness parameter \(\) is smaller than the distribution margin \(\). Furthermore, the iteration complexity increases gracefully as the _attacks_ become stronger, i.e., as the size of adversarial perturbations tends to the margin. Intuitively, as \( 0\), the attack becomes trivial, and the adversarial training reduces to the standard non-adversarial training. This is fully captured by our results -- as \( 0\), the number of attack iterates \(T_{ atk}\) goes to zero, and we recover the overall runtime of \(O(^{-2}^{-2})\) as in the previous work (Brutzkus et al., 2018; Frei et al., 2021).

Computational Complexity.To guarantee \(\)-suboptimality in the \(\)-robust misclassification error, we require \(T_{ tr}=O((-)^{-2}^{-2})\) iterations of Algorithm 2. Each iteration invokes the PGD attack in Algorithm 1, which itself requires \(T_{ atk}=O(^{2}/)\) gradient updates. Therefore, the overall computational cost of adversarial training to achieve \(\)-suboptimality is \(O}{(-)^{2}^{3}}\). Note that \(T_{ atk}\) is a purely computational requirement, and the statistical complexity of adversarial training is fully captured by \(T_{ tr}\). Remarkably, there is only a mild \(O(^{2}/(-)^{2})\) statistical overhead for \(\)-robustness, and the computational cost increases gracefully by a multiplicative factor of \(O(^{2}}{(-)^{2}})\).

Learning Robust Linear Halfspaces.When \(=1\), the Leaky ReLU activation equals the identity map, and the network reduces to a linear predictor. In this case, we retrieve strong robust generalization guarantees for learning halfspaces, as the lower bound required for \(\) in Theorem 3.4 vanishes. The following corollary instantiates such a robust generalization guarantee.

**Corollary 3.5**.: _Let \(=1/\), \(=1/\), and \(_{ tr}=(R+)^{-2}\). For any \(>0\), in at most \(T_{ tr}}{(-)^{2}^{2}}\) iterations, Algorithm 2 finds an iterate \(\), that in expectation over \(\{(x_{t},y_{t})\}_{t=1}^{T_{ tr}}\), satisfies \(_{ mb}(_{}) 2,\) provided that for all \(t[T]\), \(_{ atk}\|_{t}\|_{F}^{-2}\) and \(T_{ atk}}{_{ atk}}\)._

Dependence on the Norm of Iterates.The iteration complexity of Algorithm 1 is inversely proportional to the learning rate \(_{ atk}\), and therefore increases with \(\|_{t}\|_{F}^{2}\). Thus, when calculating the overall computational complexity, one needs to compute an upper bound on the norm of the iterates. As we show in Equation (5) in the appendix, it holds for all iterates that \(\|_{t+1}\|_{F}^{2}\|_{1}\|_{F}^{2}+3_{ tr}t\). Therefore, if we set \(=1/\) and \(^{2}=3/(R+)^{2}\), we have the following worst-case weight-independent bound on the overall computational cost:

\[T _{t=1}^{T_{ tr}}}{_{ atk} }_{t=1}^{T_{ tr}}\|_{t}\|_{F}^{2}}{ }_{t=1}^{T_{ tr}}(^{2}+3_{ tr}(t-1))}{ }\] \[_{t=1}^{T_{ tr}}t}{(R+)^{2} }T_{ tr}^{2}}{(R+)^{2}}(R+)^{2}}{(-)^{4}^{4}^{5}}.\]

Therefore, the worst-case overall computational cost is of order \(O(-)^{-4}^{-5}\). We note again that this cost is purely computational - the statistical complexity is still in the order of \(O((-)^{-2}^{-2})\).

Adversarial Robustness for any \(\).As we discussed earlier, as \( 0\), the \(\)-robust error tends to the robust error, i.e., \(_{}()_{ rob}()\). Although Theorem 3.4 does not hold for \(=0\) (exceptfor the linear case discussed above), it is possible to guarantee robust generalization with arbitrarily small \(\), as stated in the following corollary.

**Corollary 3.6**.: _For any desirable \(>0\), let \(=}\). For any \(>0\), in at most \(T_{}(1+/(2 (1-)))}{(-)^{}2^{}2^{}}\) iterations, Algorithm 2 with step-size \(_{}(1-)^{2}}{^{2}(R+)^{2}}\) finds an iterate \(\) that, in expectation over \(\{(_{t},y_{t})\}_{t=1}^{T_{}}\), satisfies \(_{}(_{}) 2\) provided that for all \(t[T]\), \(_{}(1-)^{2}}{^{2}\|_{ t}\|_{F}^{2}}\) and \(T_{}}{^{2}\|_{t}\|_{F}^{2}}\)._

### Proof Sketch

In this section, we highlight the key ideas and insights based on our analysis, and give a sketch of the proof of the main result. Using Definition 3.1, the proof of Theorem 3.4 crucially depends on the following two facts. First, whenever there exists a \(\)-effective attack, we show that Algorithm 1 will efficiently find a sufficiently good attack (in the sense of Theorem 3.2). Second, leveraging the Perceptron analysis from the prior works of Frei et al. (2021); Brutzkus et al. (2018), we show that as long as the attack size \(\) is smaller than the margin \(\), robust training is not much harder than standard training. In particular, the following Lemma establishes that the expected value of the negative loss derivative eventually becomes arbitrarily small.

**Lemma 3.7**.: _For any \(>0\), Algorithm 2 with stepsize \(_{} m^{-1}^{-2}(R+)^{-2}\) finds an iterate \(\) that, in expectation over \(\{(_{t},y_{t})\}_{t=1}^{T_{}}\), satisfies \(_{}[-^{}(yf_{_{}}(_{ }()))]\) in at most \(T_{}_{t}\|_{F})}{_{}(-)^{}2^{}2^{} m^{2}}\) iterations._

We remark that the result in Lemma 3.7 holds for _any_ attack algorithm \(\), as long as it respects the condition \(_{}()()\) for all \(\). We are now ready to present the proof of the main result.

Proof of Theorem 3.4.: Recall, that \(\)-robust misclassification error is defined as:

\[_{}()=\{_{( )}yf_{}()<-\}=\{_{ ()}yf_{}()<-\|\|_{F} \}\] (Homogeneity of \[f\] )

A key step in the proof is to give an upper bound on \(_{}\) in terms of the attack returned by PGD, i.e., \(_{()}\), rather than the optimal attack \(_{()}yf_{}()\). Theorem 3.2 does provide us with such an upper bound; however, (1) it only holds in expectation, and 2) it is conditioned on existence of an effective attack at the given example \((,y)\) and the weights \(\). Naturally, we can use Markov's inequality to bound the probability above. In order to address the conditional nature of the result in Theorem 3.2, we introduce a truncated version of the negative loss derivative. In particular, for any \(c\), let \(^{}_{c}(z)=^{}(z)[z c]\) be the loss derivative thresholded at \(c\). Note that \(z c\) implies that \(-^{}_{c}(z)-^{}_{c}(c)-\) therefore, \(\{z c\}\{-^{}_{c}(z)-^{}_{ c}(c)\}\). Let \(_{}:=\|_{}\|_{F}\), where \(_{}\) is the iterate guaranteed by Lemma 3.7. We have

\[_{}(_{}) =\{_{()}yf_{_{}}()-_{}\}\{-^{}_ {-_{}}(_{()}yf_{_{}}( ))-^{}_{-_{}}(-_{})\}\] \[_{}[-^{}_{-_{ }}(_{()}yf_{_{}}()) ]}{-^{}_{-_{}}(-_{})}\] (Markov's inequality) \[ 2_{}[-^{}_{-_{} }(_{()}yf_{_{}}())] (-^{}_{-_{}}(z) 1/2z 0)\]

Given \(_{}\), for any \((,y)\), one of the two following cases can happen:

1. _There exists a_ \(\)_-effective attack._ In this case, by Definition 3.1, it holds that \(_{()}yf_{_{}}()-\| _{}\|_{F}=-_{}\). Therefore, by definition of the truncated negative loss derivative, it also holds that \(-^{}_{_{}}(_{()}yf_{_{}}())=-^{}(_{()}yf_{ _{}}())\). Now, using Theorem 3.2, we get that \[-^{}_{-_{}}(_{()}yf_{_{}}())-2^{}(yf_{_{}}(_{ }()))+}{_{}T_{}}\]2. _There does not exist a_ \(\)_-effective attack._ In this case, by Definition 3.1, it holds that \(_{()}yf_{_{}}()>-\|_{}\|_{F}=-_{}\). Therefore, by definition of the truncated negative loss derivative, it also holds that \(-^{}_{_{}}(_{()}yf_{_ {}}())=0\), which is trivially bounded by the upper bound in the first case above, given by Equation (3).

Putting back the above cases in the upper bound on the \(\)-robust error, we arrive at:

\[_{}(_{}) 2_{}[- ^{}(yf_{_{}}(_{}()))]+ {4^{2}}{_{}T_{}}+}{_{}T_{}}+\]

where the first inequality follows from Theorem 3.2, the second inequality follows from Lemma 3.7 given the proper choice of \(T_{}\), and the final inequality holds by setting \(T_{}}{_{}}\). 

## 4 Empirical Results

Adversarial training is widely used in training robust models and has been shown to be fairly effective in practice. The goal of this section is not to attest or reproduce previous empirical findings. Instead, since the focus in this paper is on the theoretical analysis of adversarial training in non-linear networks, the goal of this section is merely to empirically study the effect of using reflected loss in Algorithm 1.

The experimental results are organized as follows. First, in Sec. 4.1, we compare the optimal attacks found by a grid search on the surrogate loss and its reflected version. In Sec. 4.2, we empirically study adversarial training with reflected loss in the binary classification setting. Finally, in Sec. 4.3, we generalize the reflected loss, which is key to our theoretical analysis, to multi-class classification setting. We then report the results on the CIFAR-10 dataset using a deep residual network.

### Grid Search Optimization

We look at the following simple 3-dimensional 3-class classification problem. Consider the point \((,y)\) where \(=\) and \(y=1\). We focus on the simplest non-trivial function, i.e., the identity mapping, given by \(f()=\). Obviously, \(f\) correctly assigns \(\) to the first class because the first dimension is larger than the others. Also, a perturbation of the form \(=[-0.501,0.5,0]\) with \(\|\|=0.7078\) can flip the label, since \(f(+)=[2.499,2.5,1]\) incorrectly predicts the second class.

We restrict the attack to the set \(\{(-0.51,+0.51)^{3}|\ \|\| 0.7078\}\). We look at every possible attack vector on a grid of size \(800 800 800\). We then sort these vectors in a descending order of the corresponding loss function, i.e., the cross entropy loss and its reflected version, and simply count how many of the top-\(k\) attack vectors actually induce a label flip. We take this as a measure of how effective is the corresponding loss maximization problem at finding a good attack vector. As we can see in Figure 2, the proposed method of maximizing the reflected cross entropy loss is a far more effective way of generating the attacks than maximizing the cross entropy loss.

### Binary Classification

Experimental Setup.We extract digits 0 and 1 from the MNIST dataset (LeCun et al., 1998), which provides a (almost) separable distribution, consistent with our theoretical setup. The dataset contains

Figure 2: Number of the top-\(\) attack vectors that are optimal, i.e., can induce a label flip, for the cross entropy loss (blue) and the reflected version (red), for different values of \(k\): **Left:**\(k=10\), **Middle:**\(k=100\), and **Right:**\(k=1000\).

[MISSING_PAGE_FAIL:8]

with the logits for the most likely wrong class, the greedy approach fails. In particular, consider the following 3-class classification problem in \(^{2}\). Let \(f_{}()=\), where \(=[2_{1},_{1},10_{2}]^{3  2}\). Here, \(_{i}\) denotes the \(i\)-th standard basis. Consider the point \(=\). Clearly, class 1 and 3 have the highest and the smallest likelihoods, respectively. Given a perturbation size \(\|^{}-\| 0.3\), the likelihood of the second class will never dominate that of the first class:

\[_{1}^{}(+)=2_{1}^{}(+ )=2(x_{1}+_{1})>(x_{1}+_{1})=_{1}^{}(+)=_{2}^{}(+),\]

where the inequality follows by using the fact that \(x_{1}=1\) and \(|_{1}| 0.3\). Therefore, the greedy approach fails here. Whereas, within the specified perturbation budget, maximizing the likelihood of the third class can indeed find a label-flipping attack. For example, with \(=[0,0.3]\), the point \(^{}=[1,0.3]\) will be assigned to the third class, because \(_{3}^{}^{}=3>_{1}^{}=2> _{2}^{}=1\).

We use adversarial training with and without reflected loss (denoted by R-PGD and PGD, respectively) to train a PreActResNet (PARN) He et al. (2016) on the CIFAR-10 dataset Krizhevsky et al. (2009). In the training phase, we conduct experiments for attack size \(\{2,4,8,16\}/255\). We build on the PyTorch implementation in Zhang et al. (2021), and we follow their experimental setup, which is described next. We use a SGD optimizer with a momentum parameter of \(0.9\) and weight decay parameter of \(5 10^{-4}\). We set the batch size to 128 and train each model for 20 epochs. We use a cyclic scheduler which increases the learning rate linearly from \(0\) to \(0.2\) within the first 10 epochs and then reduces it back to \(0\) in the remaining 10 epochs. We report robust test accuracy (RA) of an adversarially-trained model against PGD attacks Madry et al. (2018) (RA-PGD), where we take 50-step PGD with 10 restarts. We report the results for test-time attack size \(=8/255\). Based on our empirical results, using the (greedy) reflected loss in adversarial training does not significantly impact the standard/robust generalization performance of the learned models.

## 5 Discussion

We study robust adversarial training of two-layer neural networks as a bi-level optimization problem. We propose _reflecting_ the surrogate loss about the origin in the inner maximization phase when searching for an "optimal" perturbation vector to attack the current model. We give convergence guarantee for the inner-loop PGD attack and precise iteration complexity results for end-to-end adversarial training, which hold for any width and initialization under a margin assumption. We also provide an empirical study on the effect of reflecting the surrogate loss in real datasets. Next, we list few natural research directions for future work.

Extension to multiclass setting.In binary classification, which is the focus of this paper, reflecting the loss about the origin provides a concave lower-bound for the zero one loss (see Figure 1). Maximizing the reflected loss then corresponds to maximizing the likelihood of the wrong class. This simple modification enables us to guarantee the convergence of PGD-\(2\) attacks, and yield stronger

   &  \\   &  &  &  &  \\   & RA & SA & RA & SA & RA & SA & RA & SA \\   & 14.182 & 91.254 & 20.702 & 90.424 & 21.014 & 90.132 & 20.848 & 90.09 \\ R-PGD & 14.338 & 91.208 & 20.726 & 90.384 & 20.958 & 90.06 & 20.746 & 89.992 \\    &  \\  PGD & 17.764 & 90.748 & 30.344 & 88.736 & 37.564 & 86.65 & 37.304 & 86.572 \\  R-PGD & 17.162 & 90.114 & 30.34 & 88.826 & 37.4 & 86.734 & 37.374 & 86.522 \\    &  \\  PGD & 20.064 & 90.478 & 34.21 & 87.746 & 48.916 & 78.402 & 48.936 & 77.926 \\  R-PGD & 20.1 & 90.564 & 34.19 & 87.852 & 48.792 & 78.382 & 48.828 & 77.982 \\    &  \\  PGD & 16.19 & 85.908 & 21.524 & 86.816 & 48.722 & 68.37 & 45.292 & 58.526 \\  R-PGD & 15.986 & 89.708 & 21.362 & 86.83 & 48.742 & 68.456 & 44.778 & 58.486 \\   

Table 2: Robust test accuracy (RA) of adversarially trained models with and without reflecting the loss, for different values of the attack size \(\{2,4,8,16\}/255\) and number of steps in the attack Steps \(\{2,4,16,32\}\). We report the results for test-time attack size \(=8/255\); the better performance is highlighted in gray, where the intensity corresponds to difference in performance.

attacks in our experiments. However, extending this idea to the multiclass setting is not trivial. In particular, the idea of maximizing the likelihood of the wrong class does not trivially generalize to the multiclass setting due to plurality of wrong classes. Nonetheless, as we show in the experimental section, a naive greedy approach to choose a wrong class seems to provide competitive performance in terms of standard/adversarial test error. Is there a simple, principled approach to obtain a lower-bound for the misclassification error in the multiclass setting? It would be interesting to explore theoretical and empirical aspects of such possible extensions.

Beyond \(\)-robustness.The notion of \(\)-robustness is crucial in our analysis. Although we provide robustness guarantees for arbitrarily small positive \(\) (see Corollary 3.6), our current analysis does not allow for standard robustness guarantees (\(\) = 0) except for the linear setting (\(=1\)). At a high level, the main challenge here is to guarantee that the attack can always find an adversarial example - if there exists one - regardless of whether the attack is \(\)-effective or not. This is, in particular, challenging to establish for iterative attacks such as PGD, because they can only guarantee getting sufficiently close to an optimal attack in finite time. Therefore, if the optimal attack can just barely flip the sign, the computational time for finding it can grow unboundedly. Therefore, providing robust generalization guarantees (\(=0\)) is an interesting research direction for future work.

Optimization geometry.In our theoretical results, we focus on PGD-2 attacks, which are based on steepest descent with respect to the \(_{2}\) geometry. In our experiments, we also provide empirical results for steepest descent attacks with respect to \(_{}\) geometry (including FGSM and BIM) on the reflected loss. We leave the theoretical analysis of such attacks to future work.