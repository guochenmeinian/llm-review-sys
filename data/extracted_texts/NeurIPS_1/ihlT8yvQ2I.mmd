# GNNEvaluator: Evaluating GNN Performance On Unseen Graphs Without Labels

Xin Zheng\({}^{1}\), Miao Zhang\({}^{2}\), Chunyang Chen\({}^{1}\), Soheila Molaei\({}^{3}\), Chuan Zhou\({}^{4}\), Shirui Pan\({}^{5}\)

\({}^{1}\)Monash University, Australia, \({}^{2}\)Harbin Institute of Technology (Shenzhen), China

\({}^{3}\)University of Oxford, UK, \({}^{4}\)Chinese Academy of Sciences, China, \({}^{5}\)Griffith University, Australia

xin.zheng@monash.edu, zhangmiao@hit.edu.cn, chunyang.chen@monash.edu

soheila.molaei@eng.ox.ac.uk, zhouchuan@amss.ac.cn, s.pan@griffith.edu.au

Corresponding author

###### Abstract

Evaluating the performance of graph neural networks (GNNs) is an essential task for practical GNN model deployment and serving, as deployed GNNs face significant performance uncertainty when inferring on unseen and unlabeled test graphs, due to mismatched training-test graph distributions. In this paper, we study a _new_ problem, **GNN model evaluation**, that aims to assess the performance of a specific GNN model trained on labeled and observed graphs, by precisely estimating its performance (_e.g._, node classification accuracy) on unseen graphs without labels. Concretely, we propose a two-stage GNN model evaluation framework, including (1) DiscGraph set construction and (2) GNNEvaluator training and inference. The DiscGraph set captures wide-range and diverse graph data distribution discrepancies through a discrepancy measurement function, which exploits the outputs of GNNs related to latent node embeddings and node class predictions. Under the effective training supervision from the DiscGraph set, GNNEvaluator learns to precisely estimate node classification accuracy of the to-be-evaluated GNN model and makes an accurate inference for evaluating GNN model performance. Extensive experiments on real-world unseen and unlabeled test graphs demonstrate the effectiveness of our proposed method for GNN model evaluation.

## 1 Introduction

As prevalent graph data learning models, graph neural networks (GNNs) have attracted much attention and achieved great success for various graph structural data related applications in the real world . In practical scenarios of GNN model deployment and serving , understanding and evaluating GNN models' performance is a vital step , where model designers need to determine if well-trained GNN models will perform well in practical serving, and users want to know how the in-service GNN models will perform when inferring on their own test graphs . Conventionally, model evaluation utilizes well-annotated datasets for testing to calculate certain model performance metrics (_e.g._, accuracy and F1-score) . However, it may fail to work well in real-world GNN model deployment and serving, where the unseen test graphs are usually not annotated, making it difficult to obtain essential model performance metrics without ground-truth labels . As shown in Fig. 1 (a-1), taking _node classification accuracy_ metric as an example, it is typically calculated as the percentage of correctly predicted node labels. However, when ground-truth node class labels are unavailable, we can not verify whether the predictions of GNNs are correct, and thus cannot get the overall accuracy of the model.

In light of this, a natural question, referred to **GNN model evaluation** problem, arises: _in the absence of labels in an unseen test graph, can we estimate the performance of a well-trained GNN model?_In this work, we provide a confirmed answer together with an effective solution to this question, as shown in Fig.1 (a-2). Given a well-trained GNN model and an unseen test graph without labels, GNN model evaluation directly outputs the overall accuracy of this GNN model. This enables users to understand their GNN models at hand, benefiting many GNN deployment and serving scenarios in the real world . For example, given a GNN model in service (_e.g._, a GNN model trained and served with Amazon DGL and SageMaker ), GNN model evaluation can provide users with a confidence score based on its estimated node classification accuracy, so that users know how much faith they should place in GNN-predicted node labels on their own unlabeled test graphs. Moreover, as shown in Fig. 1 (b), given a set of available well-trained GNNs in service, GNN model evaluation can provide users with reliable guidance to make an informed choice among these deployed GNN models on their own unlabeled test graphs.

In this paper, we focus on the node classification task, with accuracy as the primary GNN model evaluation metric. Developing effective GNN model evaluation methods for this task faces three-fold challenges: **Challenge-1:** The distribution discrepancies between various real-world unseen test graphs and the observed training graph are usually complex and diverse, incurring significant uncertainty for GNN model evaluation. **Challenge-2:** It is not allowed to re-train or fine-tune the practically in-service GNN model to be evaluated, and the only accessible model-related information is its outputs. Hence, how to fully exploit the limited GNN outputs and integrate various training-test graph distribution differences into discriminative discrepancy representations is critically important. **Challenge-3:** Given the discriminative discrepancy representations of training-test graph distributions, how to develop an accurate GNN model evaluator to estimate the node classification accuracy of an in-service GNN on the unseen test graph is the key to GNN model evaluation.

To address the above challenges, in this work, we propose a two-stage GNN model evaluation framework, including (1) DiscGraph set construction and (2) GNNEvaluator training and inference. More specifically, in the first stage, we first derive a set of meta-graphs from the observed training graph, which involves wide-range and diverse graph data distributions to simulate (ideally) any potential unseen test graphs in practice, so that the complex and diverse training-test graph data distribution discrepancies can be effectively captured and modeled (_Challenge-1_). Based on the meta-graph set, we build a set of discrepancy meta-graphs (DiscGraph) by jointly exploiting latent node embeddings and node class predictions of the GNN with a discrepancy measurement function, which comprehensively integrates GNN output discrepancies to expressive graph structural discrepancy representations involving discrepancy node attributes, graph structures, and accuracy labels (_Challenge-2_). In the second stage, we develop a GNNEvaluator composed of a typical GCN architecture and an accuracy regression layer, and we train it to precisely estimate node classification accuracy with effective supervision from the representative DiscGraph set (_Challenge-3_). During the inference, the trained GNNEvaluator could directly output the node classification accuracy of the in-service GNN on the unseen test graph without any node class labels.

In summary, the contributions of our work are listed as follows:

* **Problem.** We study a new research problem, GNN model evaluation, which opens the door for understanding and evaluating the performance of well-trained GNNs on unseen real-world graphs without labels in practical GNN model deployment and serving.

Figure 1: Illustration of conventional model evaluation _vs_. the proposed GNN model evaluation with its applicable case. The symbol \(*\) indicates GNNs are well-trained and fixed during model evaluation.

* **Solution.** We design an effective solution to simulate and capture the discrepancies of diverse graph data distributions, together with a GNNEvaluator to estimate node classification accuracy of the in-service GNNs, enabling accurate GNN model evaluation.
* **Evaluation.** We evaluate our method on real-world unseen and unlabeled test graphs, achieving a low error (_e.g._, as small as 2.46%) compared with ground-truth accuracy, demonstrating the effectiveness of our method for GNN model evaluation.

**Prior Works.** Our research is related to existing studies on _predicting model generalization error_, which aims to estimate a model's performance on unlabeled data from the unknown and shifted distributions [6; 9; 34; 10; 4]. However, these researches are designed for data in Euclidean space (_e.g._, images) while our research is dedicatedly designed for graph structural data. Our research also significantly differs from others in unsupervised graph domain adaptation [33; 39; 29], out-of-distribution (OOD) generalization [18; 48], and OOD detection [19; 26], in the sense that we aim to estimate well-trained GNN models' performance, rather than improve the generalization ability of new GNN models. Detailed related work can be found in Appendix A.

## 2 Problem Definition

**Preliminary.** Consider that we have a fully-observed training graph \(=(,,)\), where \(^{N d}\) denotes \(N\) nodes with \(d\)-dimensional features, \(^{N N}\) denotes the adjacency matrix indicating the edge connections, and \(^{N C}\) denotes the \(C\)-classes of node labels. Then, training a GNN model on \(\) for node classification objective can be denoted as:

\[_{_{}}_{}(},),\,\,\,_{},} =_{_{}}(,). \]

where \(_{}\) denotes the parameters of GNN trained on \(\), \(_{}^{N d_{1}}\) is the output node embedding of graph \(\) from GNN\({}_{_{}}\), and \(}^{N C}\) denotes GNN predicted node labels. By optimizing the node classification loss function \(_{}\) between GNN predictions \(}\) and ground-truth node labels \(\), _i.e._, cross-entropy loss, we could obtain a well-trained GNN with optimal weight parameters \(^{*}_{}\), denoted as GNN\({}^{*}_{}\). Then, the node classification accuracy can be calculated to reflect GNN performance as: \(()=_{i=1}^{N}(_{i}==y_{i})/N\), which indicates the percentage of correctly predicted node labels between the GNN predicted labels \(_{i}}\) and ground truths \(y_{i}\). Given an unseen and unlabeled graph \(=(^{},^{})\) including \(M\) nodes with its features \(^{}^{M d}\) and structures \(^{}^{M M}\), we assume the covariate shift between \(\) and \(\), where the distribution shift mainly lies in node numbers, node context features, and graph structures, but the label space of \(\) keeps the same with \(\), _i.e._, all nodes in \(\) are constrained in the same \(C\)-classes. Due to the absence of ground-truth node labels, we could NOT directly calculate the node classification accuracy to assess the performance of the well-trained GNN\({}^{*}_{}\) on \(\). In light of this, we present a new research problem for GNNs as:

**Definition of GNN Model Evaluation.** Given the observed training graph \(\), its well-trained model GNN\({}^{*}_{}\), and an unlabeled unseen graph \(\) as inputs, the **goal** of GNN model evaluation aims to learn an accuracy estimation model \(f_{}()\) parameterized by \(\) as:

\[()=f_{}(^{*}_{},), \]

where \(f_{}:(^{*}_{},) a\) and \(a\) is a scalar denoting the overall node classification accuracy \(()\) for all unlabeled nodes of \(\). When the context is clear, we will use \(f_{}()\) for simplification.

## 3 The Proposed Method

### Overall Framework of GNN Model Evaluation

As shown in Fig. 2, the proposed overall framework of GNN model evaluation contains two stages: (1) DiscGraph set construction; and (2) GNNEvaluator training and inference.

**Stage 1.** Given the observed training graph \(\), we first extract a seed graph from it followed by augmentations, leading to a set of meta-graphs from it for simulating any potential unseen graph distributions in practice. Then, the meta-graph set \(_{}\) and the vector training graph \(\) are fed into the well-trained GNN for obtaining latent node embeddings, _i.e._, \(^{(i,*)}_{}\) and \(^{*}_{}\), respectively. After, a discrepancy measurement function \(D()\) works on these two latent node embeddings, leading to \(\) discrepancy node attributes \(^{i}_{}\). Meanwhile, the output node class predictions of well-trained GNN on each meta-graph are used to calculate the node classification accuracy according to its ground-truth node labels, leading to the new \(\) scalar accuracy labels \(y^{i}_{}\). Along with \(\) graph structures \(^{i}_{}\) from each meta-graph, these three important components composite the set of discrepancy meta-graphs (DiscGraph) in expressive graph structural discrepancy representations.

**Stage 2.** In the second stage, the representative DiscGraph set is taken as effective supervision to train a GNNEvaluator through accuracy regression. During the inference, the trained GNNEvaluator could directly output the node classification accuracy of the in-service GNN on the unseen test graph \(\) without any node class labels.

### Discrepancy Meta-graph Set Construction

The critical challenge in developing GNN model evaluation methods is complex and diverse graph data distribution discrepancies between various real-world unseen test graphs and the observed training graph. As the in-service GNN in practice fits the observed training graph well, making inferences on various and diverse unseen test graphs would incur significant performance uncertainty for GNN model evaluation, especially when the node classification accuracy can not be calculated due to label absence.

In light of this, we propose to construct a set of diverse graphs to simulate wide-range discrepancies of potential unseen graph distributions. Specifically, such a graph set should have the following characteristics: (1) _sufficient quantity:_ it should contain a relatively sufficient number of graphs with diverse node context and graph structure distributions; (2) _represented discrepancy:_ the node attributes of each graph should indicate its distribution distance gap towards the observed training graph; (3) _known accuracy:_ each graph should be annotated by node classification accuracy as its label.

To address the characteristic of (1) _sufficient quantity_, we introduce a meta-graph simulation strategy to synthesize a wide variety of meta-graphs for representing any potential unseen graph distributions that may be encountered in practice. Specifically, we extract a seed sub-graph \(_{}\) from the observed training graph \(\). The principle of seed sub-graph selection strategy is that \(_{}\) involves the least possible distribution shift within \(_{}\) from the observed training graph, and shares the same label space with \(\), satisfying the assumption of covariate shift. Then, \(_{}\) is fed to a pool of graph augmentation operators as

\[=\{_{1}(),_{2}( {Subgraph}),_{3}(),_{4}( )\}, \]

which belongs to the distribution \(_{}(p_{i},|)\) with \(\{p_{i}\}_{i=1}^{4}(0,1)\) represent the augmentation ratio on \(_{}\) for each augmentation type, and \(\) determines the probability of sampling a particular augmentation type. For instance, \(p_{1}=0.5\) means dropping 50% edges in \(_{}\). In this way, we can obtain a set of meta-graphs \(_{}=\{g^{i}_{}\}_{i=1}^{K}\) with \(K\) numbers of graphs, and we have \(_{}_{}_{ }:_{} g^{i}_{}\). For each meta-graph \(g^{i}_{}=\{^{i}_{},^{i}_{ },^{i}_{}\}\), where

Figure 2: Overall two-stage pipeline of GNN model evaluation with (1) DiscGraph set construction and (2) GNNEvaluator training and inference.

\(^{i}_{}^{M_{i} d}\), \(^{i}_{}^{M_{i} M_{i}}\), and \(^{i}_{}^{M_{i} C}\) have \(M_{i}\) number of nodes, \(d\)-dimensional features, and known node labels from \(\)-used belonging to \(C\) classes.

To incorporate the characteristic of (2) _represented discrepancy_, we fully exploit the outputs of latent node embeddings and node class predictions from well-trained \(^{*}_{}\), and integrate various training-test graph distribution differences into discriminative discrepancy representations. Specifically, given the \(^{*}_{}\) learned latent node embeddings \(^{*}_{}=^{*}_{}(,)\), and \(^{(i,*)}_{},}^{(i,*)}_{}=^{*}_{}(^{i}_{},^{i}_{})\), where \(^{*}_{}^{N d_{1}}\) and \(^{(i,*)}_{}^{M_{i} d_{1}}\), we derive a distribution discrepancy measurement function \(D()\) to calculate the discrepancy meta-graph node attributes as:

\[^{i}_{}=D(^{(i,*)}_{},^{*} _{})=^{(i,*)}_{}^{*\,}_{}}{\|^{(i,*)}_{}\|_{2}\| ^{*}_{}\|_{2}}, \]

where \(^{i}_{}^{M_{i} N}\) reflects the node-level distribution discrepancy between each \(g^{i}_{}\) and \(\) with the well-trained \(^{*}_{}\). Each element in \(x^{i}_{u,v}^{i}_{}\) denotes the node embedding discrepancy gap between a certain node \(u\) in the \(i\)-th meta-graph and a node \(v\) in the observed training graph. Taking this representation as node attributes could effectively integrate representative node-level discrepancy produced by well-trained GNN's node embedding space.

Meanwhile, the characteristic of (3) _known accuracy_ can be involved with the outputs of node class predictions produced by \(^{*}_{}\) on meta-graph \(}^{(i,*)}_{}=\{^{j}_{(i,*)}\}_{j= 1}^{M_{i}}\). We calculate the node classification accuracy on the meta-graph given its ground-truth node class labels \(^{i}_{}=\{y^{j}_{(i)}\}_{j=1}^{M_{i}}\) as:

\[y^{i}_{}=(g^{i}_{})=^{M_{i}}( ^{j}_{(i,*)}==y^{j}_{(i)})}{M_{i}}, \]

where \(y^{j}_{(i)}\) and \(^{j}_{(i,*)}\) denote the ground truth label and \(^{*}_{}\) predicted label of \(j\)-th node in the \(i\)-th graph of the meta-graph set, respectively. Note that \(y^{i}_{}\) is a continuous scalar denoting node classification accuracy under specific \(^{*}_{}\) within the range of \((0,1)\).

In this way, by incorporating all these characteristics with the discrepancy node attributes \((^{i}_{}\), the scalar node classification accuracy label \(y^{i}_{}\), and the graph structure from meta-graph as \(^{i}_{}=^{i}_{}\) for indicating discrepancy node interconnections, we could derive the final discrepancy meta-graph set, _i.e._, DiscGraph set, as

\[_{}=\{g^{i}_{}\}_{i=1}^{K}, {where }\,g^{i}_{}=(^{i}_{},^{i}_{},y^{i}_{}). \]

The proposed DiscGraph set \(_{}\) contains a sufficient number of graphs with diverse node context and graph structure distributions, where each discrepancy meta-graph contains the latent node embedding based discrepancy node attributes and the node class prediction based accuracy label, according to the well-trained GNN's outputs, along with diverse graph structures. All these make the proposed DiscGraph set a discriminative graph structural discrepancy representation for capturing wide-range graph data distribution discrepancies.

### GNNEvaluator Training and Inference

**Training.** Given our constructed DiscGraph set \(_{}=\{g^{i}_{}\}_{i=1}^{K}\) and \(g^{i}_{}=(^{i}_{},^{i}_{}, y^{i}_{})\) in Eq. (6), we use it to train a GNN regressor with \(f_{}:g^{i}_{} a\) for evaluating well-trained GNNs, which we name as GNNEvaluator. Specifically, the proposed GNNEvaluator takes a two-layer GCN architecture as the backbone, followed by a pooling layer to average the representation of all nodes of each \(g^{i}_{}\), and then maps the averaged embedding to a scalar node classification accuracy on the whole graph. The objective function of our GNN regressor can be written as:

\[_{}_{i=1}^{K}_{}(f_{}( ^{i}_{},^{i}_{}),y^{i}_{}), \]

where \(_{}\) is the regression loss, _i.e._, the mean square error (MSE) loss.

**Inference.** During the inference in the practical GNN model evaluation, we have: (1) to-be-evaluated \(^{*}_{}\), and (2) the unseen test graph \(=(^{},^{})\) without labels. The first thing is to calculate the discrepancy node attributes on the unseen test graph \(\) towards the observed training graph \(\) according to Eq. (4), so that we could obtain \(_{}^{}=D(_{}^{}, _{}^{})\), where \(_{}^{}=_{}^{}( ^{},^{})\). Along with the unseen graph structure \(_{}^{}=^{}\), the proposed GNNEvaluator could directly output the node classification accuracy of \(_{}^{}\) on \(\) as:

\[()=_{}^{}=f_{^{ }}(_{}^{},_{}^{ }), \]

where \(^{}\) denotes the optimal GNNEvaluator weight parameters trained by our constructed DiscGraph set \(_{}\).

## 4 Experiments

This section empirically evaluates the proposed GNNEvaluator on real-world graph datasets for node classification task. In all experiments, to-be-evaluated GNN models have been well-trained on observed training graphs and keep FIXED in GNN model evaluation. We only vary the unseen test graph datasets and evaluate various different types and architectures of well-trained GNNs. Throughout our proposed entire two-stage GNN model evaluation framework, we are unable to access the labels of unseen test graphs. We only utilize these labels for the purpose of obtaining true error estimates for experimental demonstration.

We investigate the following questions to verify the effectiveness of the proposed method. **Q1:** How does the proposed GNNEvaluator perform in evaluating well-trained GNNs' node classification accuracy (Sec. 4.2)? **Q2:** How does the proposed GNNEvaluator perform when conducting an ablation study regarding the DiscGraph set component? (Sec. 4.3) **Q3:** What characteristics does our constructed DiscGraph set have (Sec. 4.4)? **Q4:** How many DiscGraphs in the set are needed for the proposed GNNEvaluator to perform well (Sec. 4.5)?

### Experimental Settings

**Datasets.** We perform experiments on three real-world graph datasets, _i.e._, DBLPv8, ACMv9, and Citationv2, which are citation networks from different original sources (DBLP, ACM, and Microsoft Academic Graph, respectively) by following the processing of . Each dataset shares the same label space with six categories of node class labels, including 'Database', 'Data mining', 'Artificial intelligent', 'Computer vision', 'Information Security', and 'High-performance computing'. We evaluate our proposed GNNEvaluator with the following six cases, _i.e._, A\(\)D, A\(\)C, C\(\)A, C\(\)D, D\(\)A, and D\(\)C, where A, C, and D denote ACMv9, DBLPv8, and Citationv2, respectively. Each arrow denotes the estimation of GNNEvaluator trained by the former observed graph and tested on the latter unseen graph. Note that, in all stages of GNNEvaluator training and inference, we do not access the labels of the latter unseen graph. More detailed statistical information of the used dataset can be found in Appendix B.

**GNN Models and Evaluation.** We evaluate four commonly-used GNN models, including GCN , GraphSAGE  (_abbr._ SAGE), GAT , and GIN , as well as the baseline MLP model that is prevalent for graph learning. For each model, we train it on the training set of the observed graph under the transductive setting, until the model achieves the best node classification on its validation set following the standard training process, _i.e._, the 'well-trained' GNN model. To minimize experimental variability, we train each type of GNN with five random seeds. That means, for instance, we have five well-trained GCN models on the same fully-observed graph with the same hyper-parameter space but only five different random seeds. More details of these well-trained GNN models are provided in Appendix D. We report the Mean Absolute Error (MAE), the average absolute difference between the ground truth accuracy (in percentage) on the unseen graph and the estimated accuracy from ours and baselines on the same unseen graph, across different random seeds for each evaluated GNN. The smaller MAE denotes better GNN model evaluation performance.

**Baseline Methods.** As our method is the first GNN model evaluation approach, there is no available baseline for direct comparisons. Therefore, we evaluate our proposed approach by comparing it to three convolutional neural network (CNN) model evaluation methods applied to image data. Note that existing CNN model evaluation methods are hard to be directly applied to GNNs, since GNNs have entirely different convolutional architectures working on different data types. Compared with Euclidean image data, graph structural data distributions have more complex and wider variations due to their non-independent node-edge interconnections. Therefore, we make necessary adaptations to enable these methods to work on graph-structured data for GNN model evaluation. Specifically, we compare: (1) _Average Thresholded Confidence (ATC) score_, which learns a threshold on CNN's confidence to estimate the accuracy as the fraction of unlabeled images whose confidence scores exceed the threshold. We consider its four variants, denoting as ATC-NE, ATC-NE-c, ATC-MC, and ATC-MC-c, where ATC-NE and ATC-MC calculate negative entropy and maximum confidence scores, respectively, and '-c' denotes their confidence calibrated versions. In our adaptation, we use the training set of the observed graph under the transductive setting to calibrate these confidence scores, and use the validation set for their calibrated versions. (2) _Threshold-based Method_, which is introduced by  and determines three threshold values for \(=\{0.7,0.8,0.9\}\) on the output softmax logits of CNNs and calculates the percentage of images in the entire dataset whose maximum entry of logits are greater than the threshold \(\), which indicates these images are correctly classified. We make an adaptation by using the GNN softmax logits and calculating the percentage of nodes in a single graph. (3) _AutoEval Method_, which conducts the linear regression to learn the CNN classifier's accuracy based on the domain gap features between the observed training image data and unseen real-world unlabeled image data. We adapt it to GNN models on graph structural data by calculating the Maximum Mean Discrepancy (MMD) measurement as the graph domain gap features extracted from our meta-graph set, followed by the linear regression, which we name as AutoEval-G variant. More details of baseline methods can be found in the Appendix C.

### GNN Model Evaluation Results

In Table 1, Table 2, and Table 3, we report MAE results of evaluating different GNN models across different random seeds. In general, we observe that our proposed GNNEvaluator significantly outperforms the other model evaluation baseline methods in all six cases, achieving the lowest average MAE scores of all GNN model types, _i.e._, 10.71 and 12.86 in A\(\)C and A\(\)D cases, 16.38 and 10.71 in C\(\)A and C\(\)D cases, and 6.79 and 7.80 in D\(\)A and D\(\)C cases, respectively.

More specifically, we observe that some baseline methods achieve the lowest MAE for a certain GNN model type on a specific case. Taking GCN as an example in Table 1, ATC-MC-c performs best with 2.41 MAE under A\(\)C case. However, it correspondingly has 31.15 worst MAE under A\(\)D case. That means, for a GCN model that is well-trained by ACMv9 dataset, ATC-MC-c provides model evaluation results with significant performance variance under different unseen graphs. Such high

    &  &  \\   & GCN & SAGE & GAT & GIN & MLP & _Avg._ & GCN & SAGE & GAT & GIN & MLP & _Avg._ \\  ATC-MC  & 9.50 & 13.40 & 8.28 & 35.51 & 43.40 & 22.02 & 22.57 & **1.37** & 21.87 & 29.24 & 35.20 & 22.05 \\ ATC-MC-c  & 6.93 & 11.75 & **6.70** & 38.93 & 57.43 & 24.35 & 33.67 & 4.92 & 28.23 & 30.89 & 52.59 & 30.06 \\ ATC-NE  & 8.86 & 13.04 & 7.87 & 34.88 & 47.49 & 22.42 & 23.97 & 1.86 & 23.74 & 28.96 & 39.72 & 23.65 \\ ATC-NE  & 7.73 & 1.94 & 7.63 & 41.17 & 6.296 & 26.69 & 37.16 & 4.66 & 29.43 & 31.66 & 58.95 & 32.37 \\ Thres. (\(=0.7\))  & 37.33 & 36.61 & 31.68 & 58.91 & 34.33 & 39.77 & 10.70 & 23.05 & 12.74 & 34.60 & 38.29 & 23.88 \\ Thres. (\(=0.8\))  & 29.62 & 28.95 & 22.77 & 57.48 & 34.53 & 34.67 & 5.65 & 15.01 & 7.61 & 34.36 & 38.43 & 20.21 \\ Thres. (\(=0.9\))  & 19.59 & 19.06 & 11.37 & 55.72 & 34.56 & 28.06 & 10.65 & 8.28 & 8.07 & 34.00 & 38.44 & 19.89 \\ AutoEval-G  & 23.01 & 31.24 & 26.74 & 59.66 & 35.02 & 28.28 & **2.57** & 16.52 & 6.96 & 19.20 & 32.24 & 24.59 \\  GNNEvaluator **(Ours)** & **5.45** & **8.53** & 9.61 & **29.77** & **28.52** & **16.38** & 11.64 & 7.02 & **5.58** & **6.46** & **22.87** & **10.71** \\   

Table 2: Mean Absolute Error (MAE) performance of different GNN models across different random seeds. (GNNs are well-trained on the Citationv2 dataset and evaluated on the unseen and unlabeled ACMv9 and DBLPV8 datasets, _i.e._, C\(\)A and C\(\)D, respectively.Best results are in bold.)

    &  &  \\   & GCN & SAGE & GAT & GIN & MLP & _Avg._ & GCN & SAGE & GAT & GIN & MLP & _Avg._ \\  ATC-MC  & 9.50 & 13.40 & 8.28 & 35.51 & 43.40 & 22.02 & 22.57 & **1.37** & 21.87 & 29.24 & 35.20 & 22.05 \\ ATC-MC-c  & 6.93 & 11.75 & **6.70** & 38.93 & 57.43 & 24.35 & 33.67 & 4.92 & 28.23 & 30.89 & 52.59 & 30.06 \\ ATC-NE  & 8.86 & 13.04 & 7.87 & 34.88 & 47.49 & 22.42 & 23.97 & 1.86 & 23.74 & 28.96 & 39.72 & 23.65 \\ ATC-NE  & 7.73 & 1.94 & 7.63 & 41.17 & 6.296 & 26.69 & 37.16 & 4.66 & 29.43 & 31.66 & 58.95 & 32.37 \\ Thres. (\(=0.7\))  & 37.33 & 36.61 & 31.68 & 58.91 & 34.33 & 39.77 & 10.70 & 23.05 & 12.74 & 34.60 & 38.29 & 23.88 \\ Thres. (\(=0.8\))  & 29.62 & 28.95 & 22.77 & 57.48 & 34.53 & 34.67 & 5.65 & 15.01 & 7.61 & 34.36 & 38.43 & 20.21 \\ Thres. (\(=0.9\))  & 19.59 & 19.06 & 11.37 & 55.72 & 34.56 & 28.06 & 10.65 & 8.28 & 8.07 & 34.00 &variance evaluation performance would significantly limit the practical applications of ATC-MC-c, as it only performs well on certain unseen graphs, incurring more uncertainty in GNN model evaluation. Similar limitations can also be observed in Table 2 for ATC-MC-c on evaluating GAT model, with 6.70 lowest MAE in C\(\)A but 28.23 high MAE in C\(\)D. Moreover, in terms of AutoEval-G method on GraphSAGE model evaluation in Table 3, it achieves the lowest MAE of 9.72 in D\(\)A case, but a high MAE of 15.11 in C\(\)A case. In contrast, our proposed GNNEvaluator has smaller MAE variance for different unseen graphs under the same well-trained models. For instance, our GNNEvaluator achieves 4.85 MAE on GCN under A\(\)C case, and 11.80 MAE on GCN in A\(\)D case, which is better than ATC-MC-c with 2.41 and 31.15 on the same cases. All these results verify the effectiveness and consistently good performance of our proposed method on diverse unseen graph distributions for evaluating different GNN models.

### Ablation Study

We conduct an ablation study to verify the effectiveness of the proposed DiscGraph set component for our proposed GNN model evaluation pipeline. Concretely, we replace the DiscGraph set with the meta-graph set for GNNEvaluator training, which does not involve discrepancy node attributes calculated based on specific well-trained GNNs' node embedding space with the discrepancy measurement function. The output node classification accuracy (%) of the proposed GNNEvaluator for evaluating five GAT models under different seeds are shown in Fig. 3 with lines for the proposed DiscGraph set (w/ DiscAttr) in green and the meta-graph set (w/o DiscAttr) in blue, respectively. And we also list the ground-truth node classification accuracy in histograms for comparison.

Generally, we observe that the proposed DiscGraph set (w/ DiscAttr) trained GNNEvaluator has better performance for GAT model evaluation than that trained by the meta-graph set (w/o DiscAttr), as all green lines are closer to the ground-truth histograms than the blue line, reflecting the effectiveness of the proposed the proposed DiscGraph set, especially the discrepancy node attributes with the discrepancy measurement function. Moreover, the meta-graph set (w/o DiscAttr) trained GNNEvaluator almost produces the same accuracy on a certain GNN type, _i.e._, its performance stays the same on all five GATs, making it fail to evaluate a certain GNN model type under different optimal model parameters. That is because, compared with the proposed DiscGraph set, the meta-graph set lacks the exploration of the latent node embedding space of well-trained GNNs and only utilizes the output node class predictions from the well-trained GNNs. In contrast, the proposed DiscGraph set fully exploits both latent node embedding space and output node class predictions of well-trained GNNs for comprehensively modeling the discrepancy between the observed training graph and unseen test graph, enabling its superior ability for GNN model evaluation.

Figure 3: Ablation study of the proposed DiscGraph set with discrepancy node attributes (w/ DiscAttr) in green lines _vs._ the meta-graph set without discrepancy node attributes (w/o DiscAttr) in blue lines. And the ground-truth node classification accuracy is in histograms for reference.

    &  &  \\   & GCN & SAGE & GAT & GIN & MLP & _Avg._ & GCN & SAGE & GAT & GIN & MLP & _Avg._ \\  ATC-MC  & 4.98 & 31.49 & 37.08 & 28.74 & 30.42 & 26.54 & 5.47 & 29.72 & 37.43 & 32.07 & 36.42 & 28.22 \\ ATC-MC-C  & 3.42 & 32.83 & 40.31 & 37.68 & 34.72 & 29.79 & **4.66** & 28.67 & 43.89 & 38.13 & 41.64 & 31.40 \\ ATC-NE  & 2.83 & 27.67 & 40.90 & 35.88 & 36.03 & 28.66 & 4.25 & 26.04 & 39.38 & 38.38 & 45.15 & 30.64 \\ ATC-NE-C  & 12.95 & 20.86 & 39.34 & 43.41 & 38.85 & 31.08 & 16.87 & 15.77 & 40.26 & 47.64 & 47.61 & 33.63 \\ Thres. (\(=0.7\))  & 21.66 & 32.69 & 39.19 & 30.10 & 27.80 & 30.29 & 27.22 & 35.48 & 34.35 & 37.99 & 30.36 & 33.08 \\ Thres. (\(=0.8\))  & 18.63 & 29.34 & 37.25 & 25.80 & 20.34 & 26.27 & 22.70 & 30.86 & 32.48 & 23.73 & 22.36 & 28.23 \\ Thres. (\(=0.9\))  & 16.12 & 23.81 & 43.26 & 23.73 & 12.98 & 23.98 & 23.98 & 15.63 & 26.07 & 35.88 & 27.15 & 12.20 & 23.39 \\ AutoEval-G  & 7.28 & **9.72** & 12.05 & 14.17 & 22.07 & 13.06 & 21.13 & 15.11 & 5.65 & 12.33 & 31.90 & 17.22 \\  GNNEvaluator **(Ours)** & **2.46** & 10.27 & **6.94** & **8.86** & **5.42** & **6.79** & 11.68 & **7.83** & **3.97** & **9.62** & **5.92** & **7.80** \\   

Table 3: Mean Absolute Error (MAE) performance of different GNN models across different random seeds. (GNNs are well-trained on the DBLPv8 dataset and evaluated on the unseen and unlabeled ACMv9 and Citationv2 datasets, _i.e._, D\(\)A and D\(\)C, respectively. Best results are in bold.)

### Analysis of Discrepancy Meta-graph Set

We visualize the discrepancy node attributes in the proposed DiscGraph set on GCN model evaluation trained on ACMv9 and tested on unseen DBLPv8 in Fig. 4 (a). Taking this as a reference, we present visualizations of discrepancy node attributes derived between ACMv9 and arbitrary two meta-graphs from our created meta-graph set in Fig. 4 (c) and (d). Darker color close to black indicates a larger discrepancy in the output node embedding space of a well-trained GNN, and lighter color close to white indicates a smaller discrepancy. As can be observed, the discrepancy pairs of (ACMv9, unseen DBLPv8), (ACMv9, Meta-graph-1), and (ACMv9, Meta-graph-2) generally show similar heat map patterns in terms of discrepancy. This indicates the effectiveness of the proposed meta-graph set simulation strategy, which could capture potential unseen graph data distributions by synthesizing diverse graph data with various graph augmentations. More detailed statistical information about the proposed DiscGraph set, including the number of graphs, the average number of nodes and edges, and the accuracy label distributions are provided in Appendix E.

### Hyper-parameter Analysis of GNNEvaluator

We test the effects of different numbers of discrepancy meta-graphs (\(K\)) for GNNEvaluator training in Fig. 5. Concretely, we utilize the number of DiscGraphs for \(K=\) to train the proposed GNNEvaluator for evaluating five GCN models that are well-trained on Citationv2 dataset and make inferences on unseen ACMv9 and DBLPv8 datasets. Considering the average performance under two transfer cases reflected in the line, despite \(K=100\) having a higher MAE, the average MAE over two cases does not change much with the number \(K\) for training the proposed GNNEvaluator. This indicates that the proposed GNNEvaluator can effectively learn to accurately predict node attributes by training on an appropriate number of DiscGraphs, without the need for a significantly excessive amount of training examples.

## 5 Conclusion

In this work, we have studied a new problem, GNN model evaluation, for understanding and evaluating the performance of well-trained GNNs on unseen and unlabeled graphs. A two-stage approach is proposed, which first generates a diverse meta-graph set to simulate and capture the discrepancies of different graph distributions, based on which a GNNEvaluator is trained to predict the accuracy of a well-trained GNN model on unseen graphs. Extensive experiments with real-world unseen and unlabeled graphs could verify the effectiveness of our proposed method for GNN model evaluation. Our method assumes that the class label space is unchanged across training and testing graphs though covariate shifts may exist between the two. We will look into relaxing this assumption and address a broader range of natural real-world graph data distribution shifts in the future.

Figure 4: Visualizations on discrepancy node attributes in the proposed DiscGraph set for GCN model evaluation. Darker color denotes a larger discrepancy.

Figure 5: Effects of different numbers of DiscGraphs (\(K\)) for GNNEvaluator training. Histograms denote MAE for \(C A\) and \(C D\) cases over five GCNs under different \(K\), and the line indicates the average MAE of such two evaluation cases.