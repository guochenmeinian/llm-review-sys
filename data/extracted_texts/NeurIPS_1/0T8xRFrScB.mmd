# Benchmarking Counterfactual Image Generation

Thomas Melistas

These authors contributed equally to this workNational & Kapodistrian University of Athens, Greece

Nikos Spyrou

Nefeli Gkouti

Pedro Sanchez

The University of Edinburgh, UK

Athanasios Vlontzos

Yannis Panagakis

Giorgos Papanastasiou

Sotirios A. Tsaftaris

National & Kapodistrian University of Athens, Greece Archimedes/Athena RC, Greece

###### Abstract

Generative AI has revolutionised visual content editing, empowering users to effortlessly modify images and videos. However, not all edits are equal. To perform realistic edits in domains such as natural image or medical imaging, modifications must respect causal relationships inherent to the data generation process. Such image editing falls into the counterfactual image generation regime. Evaluating counterfactual image generation is substantially complex: not only it lacks observable ground truths, but also requires adherence to causal constraints. Although several counterfactual image generation methods and evaluation metrics exist, a comprehensive comparison within a unified setting is lacking. We present a comparison framework to thoroughly benchmark counterfactual image generation methods. We evaluate the performance of three conditional image generation model families developed within the Structural Causal Model (SCM) framework. We incorporate several metrics that assess diverse aspects of counterfactuals, such as composition, effectiveness, minimality of interventions, and image realism. We integrate all models that have been used for the task at hand and expand them to novel datasets and causal graphs, demonstrating the superiority of Hierarchical VAEs across most datasets and metrics. Our framework is implemented in a user-friendly Python package that can be extended to incorporate additional SCMs, causal methods, generative models, and datasets for the community to build on.

Code: [https://github.com/gulnazaki/counterfactual-benchmark](https://github.com/gulnazaki/counterfactual-benchmark).

## 1 Introduction

Generative AI has revolutionized visual content editing, empowering users to effortlessly modify images and videos [9; 30; 45; 34; 17]. However, not all edits are equal, especially in complex domains such as natural or medical imaging, where image realism is paramount [58; 23]. In these fields, modifications must adhere to the underlying causal relationships that govern the data generation process. Ignoring these, can lead to unrealistic and potentially misleading results, undermining the integrity of the edited content [46; 58]. Image editing inherently relies on _implied_ causal relationships. However, learned generative models powering image editing, will not explicitly account for these causal relations and can produce unrealistic results. Figure 1 showcases clearly what happens when causal relationships are not taken into account and motivates the need for causally faithful approaches.

Image editing, where modifications intentionally alter the underlying causal relationships within the image to explore hypothetical "what if" scenarios, falls squarely into the counterfactual image generation regime (46). This emerging field seeks to generate images that depict plausible yet alternative versions of reality, allowing us to visualize the potential consequences of changes to specific attributes or conditions. Figure 0(c) displays image edits that take into account the causal graph of Figure 0(a), while Figure 0(d) depicts image edits that do not. Intervening on Gender (_do(Female)_) results in a female person with a beard, while intervening on Age (_do(Young)_) results in a young person with baldness, which do not account for the causal relationships in the data-generation process.

Evaluating counterfactual image generation is substantially challenging [62; 46]. Not only do these hypothetical scenarios lack observable ground truths, but their assessment requires adhering to causal constraints whilst ensuring the generated images are of high quality and visually plausible. This multifaceted problem demands a nuanced approach that balances the need for realism with the constraints imposed by causal relationships.

Under the Pearlian framework, we adopt a set of Structural Causal Models (SCM)  to explicitly inform generative models about causal paths between high-level variables. Based on the SCM, we employ the counterfactual inference paradigm of _Abduction-Action-Prediction_ and the mechanisms of Deep Structural Causal Models (Deep-SCM)  to evaluate the performance of various methods. As such, in this paper, we introduce a comprehensive framework, able to extensively evaluate any published methods on SCM-based counterfactual image generation. We aspire for this work to become the _de facto benchmark_ for the community when developing new methods (and metrics) in counterfactual image generation.

Briefly, the main **contributions** of our work are: **(1)** We develop a comprehensive framework to evaluate the performance of _all_ image generation models published under the Deep-SCM paradigm. We explore several datasets (synthetic, natural and medical images), as well as SCM structures across datasets. **(2)** We expand the above models to accommodate previously untested datasets and causal graphs. Specifically, we test HVAE and GAN on a non-trivial causal graph for human faces and we devise a GAN architecture inspired from  that can generate counterfactual brain MRIs given multiple variables. **(3)** We extensively benchmark these models adopting several metrics to evaluate causal SCM-based counterfactual image generation. **(4)** We offer a user-friendly Python package to accommodate and evaluate forthcoming causal mechanisms, datasets and causal graphs.

## 2 Related work

**Causal Counterfactual Image Generation:** Pairing SCMs with deep learning mechanisms for counterfactual image generation can be traced back to the emergence of Deep-SCM . The authors utilised Normalising Flows  and variational inference  to infer the exogenous noise and perform causal inference under the _no unobserved confounders_ assumption. Follow-up work  incorporates a Hierarchical VAE (HVAE)  to improve the fidelity of counterfactuals. In parallel, Generative Adversarial Networks (GANs) [14; 10] have been used to perform counterfactual inference through an adversarial objective  and for interventional inference, by predicting reparametrised distributions over image attributes . Variational Autoencoders (VAEs) have been used for

Figure 1: (a) A plausible causal graph for human faces; (b) Factual images (no intervention); (c) Causal counterfactual images using the graph of (a) to perform the interventions \(do(Female)\) (upper panel) and \(do(Young)\) (lower panel); (d) Non-causal image editing.

counterfactual inference [(61)], focusing on learning structured representations from data and capturing causal relationships between high-level variables, thus diverging from the above scope. Diffusion models [(21; 48)], on the other hand, were also recently used to learn causal paths between high-level variables [(44; 43; 12)], approaching the noise abduction as a forward diffusion process. However, these works [(44; 43; 12)] address the scenario of a single variable (attribute) affecting the image.

Similarly to Deep-SCM, Normalising Flows and VAEs were used in [(27)], following a backtracking approach [(29)]. The aforementioned method involves tracing back through the causal graph to identify changes that would lead to the desired change in the attribute intervened. Another line of work, [(42)], utilises deep twin networks [(57)] to perform counterfactual inference in the latent space, without following the _Abduction-Action-Prediction_ paradigm. Since these methods deviate from our scope and focus on Deep-SCM-based causal counterfactuals, we chose not to include them in the current benchmark. As we aim to explore further causal mechanisms, models and data, we believe that these methods are worthwhile future extensions of our current work.

Recent works based on diffusion models [(21)] have achieved SoTA results on image editing [(22; 34; 17; 2; 60; 54)], usually by altering a text prompt to edit a given image. While the produced images are of high quality, we aim to explicitly model the causal interplay of known high-level variables, an approach not yet applied to diffusion models to the best of our knowledge. Therefore, even though the task of image editing exhibits similarities to counterfactual image generation, we focus on true causal counterfactual methods, as we discussed above.

**Evaluation of counterfactuals:** To the best of our knowledge, there is no prior work offering a comprehensive framework to thoroughly evaluate the performance, fidelity, and reliability of counterfactual image generation methods, considering both image quality and their relationship to factual images and intervened variables. This paper aims to address this gap. A study relevant to ours is [(52)]: it compares established methods for counterfactual explainability1[(8; 32)]. However, its main focus lies on creating counterfactual explanations for classifiers on a single dataset (MorphoMNIST [(4)]) using solely two causal generative models (VAE, GAN) [(36; 6)].

Various metrics have been proposed to evaluate counterfactual image generation. For instance, Monteiro et al. [(35)] introduce metrics, following an axiomatic definition of counterfactuals [(15; 13)], namely the properties that arise from the mathematical formulation of SCMs. The authors in [(56)] evaluate sparsity of counterfactual explanations via elastic net loss and similarity of factual versus counterfactual distributions via autoencoder reconstruction errors. Sanchez and Tsaftaris [(44)] introduce a metric to evaluate minimality in the latent space. In this work, we adopt the metrics of [(35)] and [(44)].

Optimising criteria of causal faithfulness as those above does not directly ascertain image quality. Image quality evaluation metrics used broadly in the context of image editing include the Frechet inception distance (FID), [(19)] and similarity metrics such as the Learned Perceptual Image Patch Similarity (LPIPS) [(63)] and CLIPscore [(18)]. We adopt such image quality evaluation metrics herein.

## 3 Methodology

### Preliminaries

#### SCM-based counterfactuals

The methods we are comparing fall into the common paradigm of SCM-based interventional counterfactuals [(37)]. A Structural Causal Model (SCM) \(:=(,P())\) consists of: **(i)** A collection of structural assignments, called mechanisms \(=\{f_{i}\}_{i=1}^{N}\), s.t. \(x_{i}=f_{i}(_{i},_{i})\); and **(ii)** A joint distribution \(P()=_{i=1}^{N}p(_{i})\) over mutually independent noise variables, where \(x_{i}\) is a random variable, \(_{i}\) are the parents of \(x_{i}\) (its direct causes) and \(_{i}\) is a random variable (noise).

Causal relations are represented by a directed acyclic graph (DAG). Due to the acyclicity, we can recursively solve for \(x_{i}\) and obtain a function \(=}()\). We assume \(\) to be a collection of observable variables, where \(x_{i}\) can be a high-dimensional object such as an image and \(x_{j i}\) image attributes. The variables \(\) are observable, hence termed _endogenous_, while \(\) are unobservable, _exogenous_.

The models we examine make several assumptions: **(i)** Known causal graph (captured by a SCM). **(ii)** Lack of unobserved confounders (causal sufficiency), making the \(_{i}\), \(_{j}\) mutually independent. **(iii)** Invertible causal mechanisms, i.e. \(_{i}=f_{i}^{-1}(x_{i},_{i})\ \ i[1,n]\) and hence \(=^{-1}()\).

Thanks to the causal interpretation of SCMs, we can compute _interventional_ distributions, namely predict the effect of an _intervention_ to a specific variable, \(P(x_{j}|do(x_{i}=y))\). Interventions are formulated by substituting a structural assignment \(x_{i}=f_{i}(_{i},_{i})\) with \(x_{i}=y\). Interventions operate at the population level, so the unobserved noise variables are sampled from the prior \(P()\). _Counterfactual_ distributions instead refer to a specific observation and can be written as \(P(x_{j,x_{i}=y}|)\). We assume that the structural assignments change, as previously, but the exogenous noise is identical to the one that produced the observation. For this reason, we have to compute the posterior noise \(P(|)\). Counterfactual queries, hence, can be formulated as a three-step procedure, known as the _Abduction-Action-Prediction_ paradigm: **(i)**_Abduction_: Infer \(P(|)\), the state of the world (exogenous noise) that is compatible with the observation \(\). **(ii)**_Action_: Replace the structural equations \(do(x_{i}=y)\) corresponding to the intervention, resulting in a modified SCM \(}:=_{;do(x_{i}=y)}=(},P(|))\). **(iii)**_Prediction_: Use the modified model to compute \(P_{}}()\).

### SCM Mechanisms and Models

Counterfactual image generation examines the effect of a change to a parent variable (attribute) on the image. To enable the _Abduction-Action-Prediction_ paradigm we consider three categories of invertible deep learning mechanisms and the corresponding models that utilise them: **(1)**_Invertible, explicit_ implemented with _Conditional Normalising Flows_ (53); **(2)**_Amortized, explicit_ utilised with _Conditional VAEs_ (25; 20) or extensions such as _Hierarchical VAEs_ (26; 47); **(3)**_Amortized, implicit_ (6), implemented with _Conditional GANs_ (33; 10). The first mechanism is invertible by design and is employed for the attribute mechanisms, while the latter two are suitable for high-dimensional variables such as images and achieve invertibility through an amortized variational inference and an adversarial objective, respectively.

Figure 2: **Inference example on MorphoMNIST:**_Abduction_: We abduct the exogenous noise separately for each variable, using inverse Normalising Flows \(f^{-1}\) for the attributes and the encoder of the image mechanism (e.g. VAE, HVAE or GAN) for the image, conditioning on the factual parents. _Action_: We intervene (\(do(t^{*})\)) only on _thickness_. _Prediction_: We employ the Normalizing Flow \(f\) conditioned on the counterfactual thickness \(t^{*}\) to obtain \(i^{*}\) after the intervention. Note that this is not needed for \(t^{*}\) on which we intervene and for \(d^{*}\) that has no parents in the DAG. Finally, the decoder generates the counterfactual image, given the exogenous noise \(U_{img}\) and conditioning on the counterfactual parents.

For each variable of the SCM examined, we train a model independently to perform noise abduction, considering conditional Normalising Flows for the parent attributes and conditional VAEs, HVAEs and GANs for the image. These model families encompass all the aforementioned types of invertible mechanisms, as, discussed by Pawlowski et al. (36). Further details and background of these methods and particularly how they enable the _Abduction-Action-Prediction_ paradigm can be found in Appendix A.1. During counterfactual inference, all the posterior noise variables (given the factual) are abducted independently, except for the intervened variables that are replaced with a constant. Respecting the order of the causal graph, we infer all variables, given the abducted noise, according to the modified structural assignments. The inference procedure is visualised in Figure 2.

Finally, following the formulation in Appendix A.1, we must note that for VAEs and HVAEs the counterfactual image \(x^{*}\) is inferred as an affine transformation of the factual \(x\), 2 while for GANs all the information for the factual is passed through the latent.

### Evaluation Metrics

The evaluation of counterfactual inference has been formalised through the axioms of (13; 15). Monteiro et al. (35) utilise such an axiomatic definition to introduce three metrics to evaluate image counterfactuals: _Composition_, _Effectiveness_, and _Reversibility_.3 While it is necessary for counterfactual images to respect these axiom-based metrics, we find that they are not sufficient for a perceptual definition of successful counterfactuals. The additional desiderata we consider are the _realism_ of produced images, as well as the _minimality_ (or sparseness) of any changes made. All adopted metrics **do not** require access to ground truth counterfactuals. We now briefly describe them, leaving detailed formulation and implementation details in the Appendix A.2.

_Composition_ guarantees that the image and its attributes do not change when performing a _null-intervention_, namely when we skip the _action_ step (35). It measures the ability of the mechanisms to reproduce the original image. We perform the _null-intervention_ times and measure the distance to the original image. We use the \(l_{1}\) distance on image space, as well as the LPIPS distance (63).

_Effectiveness_ determines if the intervention was successful. In order to quantitatively evaluate effectiveness for a given counterfactual image we leverage anti-causal predictors trained on the observed data distribution, for each parent variable \(pa_{x}^{i}\) (35). Each predictor, then, approximates the counterfactual parent \(pa_{x}^{i*}\) given the counterfactual image \(x^{*}\) as input. We employ classification metrics (accuracy, F1) for categorical variables and regression metrics (mean absolute error) for continuous.

_Realism_ measures counterfactual image quality by capturing its similarity to the factual. To evaluate realism quantitatively, we employ the _Frechet Inception Distance_ (FID) (19)

_Minimality_ evaluates whether the counterfactual only differs according to the modified parent attribute against the factual, ideally leaving all other attributes unaffected. While we can achieve an approximate estimate of minimality by combining composition and effectiveness, an additional metric is helpful to measure proximity to the factual. For this reason, we leverage the _Counterfactual Latent Divergence_ (CLD) metric introduced in (44). CLD calculates the "distance" between the counterfactual and factual images in a latent space. Intuitively, CLD represents a trade-off, ensuring the counterfactual is sufficiently distant from the factual class, but not as far as other real images from the counterfactual class are.

## 4 Results

**Datasets** Given the availability of architectures and demonstrated outcomes in the literature, we benchmark all the above models on the following three datasets: **(i)** MorphoMNIST (4), **(ii)** CelebA (31) and **(iii)** ADNI (39). MorphoMNIST is a purely synthetic dataset generated by inducing morphological operations on MNIST with a resolution of 32\(\)32. We note, that interventions on thickness affect both the intensity and image (Figure 3(a)). For CelebA (64\(\)64) we use both a simple graph with two assumed disentangled attributes as done in (35) (Figure 3(b)), as well as we devise a more complex graph that introduces attribute relations taken from (27) (Figure 3(c)). _Alzheimer's Disease Neuroimaging Initiative_ (ADNI) provides a dataset of brain MRIs from which we use the _ADNI1 standardised_ set (all MRIs acquired at 1.5T). The causal graph we use (Figure 3(d)) is inspired by (1) from which we removed all missing variables (See Appendix B.1 for more details).

**Setup** For Normalising Flows we base our implementation on (27), using Masked Affine Autoregressive (11), Quadratic Spline (11) and ScaleShift flows, as well as utilising the Gumbel-max parametrisation for discrete variables as done in (36). For **MorphoMNIST**, we compare (i) the VAE architecture as given in the original Deep-SCM paper (36), (ii) the HVAE architecture of (7) and (iii) the fine-tuned GAN architecture of (52) as the set up of (6) did not converge. For **CelebA**, we compare (i) the VAE architecture proposed as a baseline in (6), (i) the GAN architecture of (6), as well as (iii) the HVAE used in (35). We further test this setup with a complex graph examined in (27), using it to assess how the former GAN and HVAE architectures behave in previously untested, more challenging scenarios. Finally, for brain MRIs (**ADNI**), we (i) adjust the VAE architecture of (1) to generate higher resolution images and (ii) use the HVAE that was developed in (7) for another dataset of brain MRIs (UK Biobank (49)). Simple adjustments to the GAN architecture of (6) would not lead to convergence, even with rigorous parameter tuning. Therefore, we resorted to the architecture of (59), which we modified by removing the residual connections between the encoder and the decoder to adhere to the _amortized, implicit_ setup.

For all datasets and graphs we trained the VAE model with learnable standard deviation, as used in (7), with the formulation provided in Appendix A.1. We experimented with different values for \(\) and found the best performing were: \(=1\) for MorphoMNIST, \(=5\) for CelebA and \(=3\) for ADNI. For GAN, we found that fine-tuning with the cyclic cost minimisation detailed in Appendix A.1 was necessary to ensure closeness to the factual image for all datasets.

### Composition

To quantitatively evaluate composition, we perform a _null-intervention_: we abduct the posterior noise and use it to perform inference without intervening on any variable. Following the protocol of (35), we apply composition for 1 and 10 cycles, measuring the distance between the initial observable

Figure 4: Qualitative evaluation of composition across all datasets/graphs. From left to right across all datasets: (i) factual, (ii) null-intervention (reconstruction) (iii) 10 cycles of null-intervention

Figure 3: Causal graphs for all examined datasets.

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

Discussion and Conclusions

We close a gap in counterfactual image generation, by offering a systematic and unified evaluation framework pitting against diverse causal graphs, model families, datasets, and evaluation metrics. By providing a rigorous and standardized assessment, we can probe the strengths and weaknesses of different approaches, ultimately paving the way for more reliable counterfactual image generation.

In fact, our findings show a superior expressivity of hierarchical latent structures in HVAE, which enabled more accurate abduction of noise variables compared to VAEs and GANs. Such structure can better learn the causal variables of the benchmarked datasets, preserving semantic information and effectively enabling the generation of causal counterfactuals. We identify that the development of Deep-SCM-based conditional HVAE is in the right direction, as it outperformed all other models examined (VAEs, GANs) across most datasets and metrics. Additionally, we extended the evaluation of the above models to previously untested datasets and causal graphs. Specifically, we applied HVAE and GAN to a complex causal graph of human faces, and developed a new GAN architecture, inspired by (59), capable of generating counterfactual brain MRIs conditioned on multiple variables.

**Limitations:** Attempts to adapt the benchmarked models to enable scaling to higher resolution datasets (e.g. CelebAHQ [(24)]) or to increase the graph complexity for single-variable models (e.g. WGAN of [(59)], Diff-SCM of [(44)]) were not fruitful. This is somewhat expected and illustrates the difference between _plain_ image editing (less inductive bias) to counterfactual image generation (more inductive bias).

In Figure 8 we can observe that even the best performing model, HVAE, fails to scale to the previously untested resolution of 256x256 (CelebAHQ), preventing a fair comparison. This can be attributed to the counterfactual training step needed to prevent the condition ignoring as described in Appendix A.1. A possible interpretation is that the gradients of the classifiers cannot accurately influence the model output in the pixel space as dimensionality grows. Another challenge for the field resides on the use of model-dependent metrics. For example, it is hard to measure effectiveness accurately. This requires training proxy predictors, which we assume perform as desired. Furthermore, the number of these predictors scales with the size of the causal graph.

**Future work:** We examined three families of generative models conforming to the Deep-SCM paradigm. To perform fair comparisons in the scope of Deep-SCM, we left methods such as deep twin networks [(57)] and backtracking counterfactuals [(29)] for future work. Despite their prominence in image generation, diffusion models have not been conditioned yet with non-trivial SCM graphs and don't offer straightforward mechanisms for noise abduction in order to perform counterfactual inference. This means that existing methodologies have to be adapted [(44; 43; 12)] to accommodate the multi-variable SCM context necessary for representing causal interplay and confounding variables, or new methods must be developed. Having benchmarked existing methods herein, we leave SCM-conditioning for diffusion models, for future work. Nevertheless, we have experimented with the extension of existing methods based on diffusion models for counterfactual image generation. A description of our methodology and preliminary results is included in Appendix E.

We are keen to see how the community will build on our benchmark and expand to novel methods and more complex causal graphs. The user-friendly Python package we provide can be extended to incorporate additional SCMs, causal methods, generative models and datasets to enable such future community-led investigations.