# Tree Variational Autoencoders

Laura Manduchi, Moritz Vandenhirtz, Alain Ryser, Julia E. Vogt

Department of Computer Science

ETH Zurich

Switzerland

Equal contribution. Correspondence to {laura.manduchi,moritz.vandenhirtz}@inf.ethz.ch

###### Abstract

We propose _Tree Variational Autoencoder_ (TreeVAE), a new generative hierarchical clustering model that learns a flexible tree-based posterior distribution over latent variables. TreeVAE hierarchically divides samples according to their intrinsic characteristics, shedding light on hidden structures in the data. It adapts its architecture to discover the optimal tree for encoding dependencies between latent variables. The proposed tree-based generative architecture enables lightweight conditional inference and improves generative performance by utilizing specialized leaf decoders. We show that TreeVAE uncovers underlying clusters in the data and finds meaningful hierarchical relations between the different groups on a variety of datasets, including real-world imaging data. We present empirically that TreeVAE provides a more competitive log-likelihood lower bound than the sequential counterparts. Finally, due to its generative nature, TreeVAE is able to generate new samples from the discovered clusters via conditional sampling.

## 1 Introduction

Discovering structure and hierarchies in the data has been a long-standing goal in machine learning (Bishop, 2006; Bengio et al., 2012; Jordan and Mitchell, 2015). Interpretable supervised methods, such as decision trees (Zhou and Feng, 2017; Tanno et al., 2019), have proven to be successful in unveiling hierarchical relationships within data. However, the expense of annotating large quantities of data has resulted in a surge of interest in unsupervised approaches (LeCun et al., 2015). Hierarchical clustering (Ward, 1963) offers an unsupervised path to find hidden groups in the data and their hierarchical relationship (R. J. G. B. Campello et al., 2015). Due to its versatility, interpretability, and ability to uncover meaningful patterns in complex data, hierarchical clustering has been widely used in a variety of applications, including phylogenetics (Sneath and Sokal, 1962), astrophysics (McConnachie et al., 2018), and federated learning (Briggs et al., 2020). Similar to how the human brain automatically categorizes and connects objects based on shared attributes, hierarchical clustering algorithms construct a dendrogram - a tree-like structure of clusters - that organizes data into nested groups based on their similarity. Despite its potential, hierarchical clustering has taken a step back in light of recent advances in self-supervised deep learning (Chen et al., 2020), and only a few deep learning based methods have been proposed in recent years (Goyal et al., 2017; Mautz et al., 2020).

Deep latent variable models (Kingma and Welling, 2019), a class of generative models, have emerged as powerful frameworks for unsupervised learning and they have been extensively used to uncover hidden structures in the data (Dilokthanakul et al., 2016; Manduchi et al., 2021). They leverage the flexibility of neural networks to capture complex patterns and generate meaningful representations of high-dimensional data. By incorporating latent variables, these models can uncover the underlying factors of variation of the data, making them a valuable tool for understanding and modeling complex data distributions. In recent years, a variety of deep generative methods have been proposed to incorporate more complex posterior distributions by modeling structural _sequential_ dependenciesbetween latent variables (Sonderby et al., 2016; He et al., 2018; Maaloe et al., 2019; Vahdat & Kautz, 2020a), thus offering different levels of abstraction for encoding the data distribution.

Our work advances the state-of-the-art in structured VAEs by combining the complementary strengths of hierarchical clustering algorithms and deep generative models. We propose TreeVAE1, a novel tree-based generative model that encodes _hierarchical_ dependencies between latent variables. We introduce a training procedure to learn the optimal tree structure to model the posterior distribution of latent variables. An example of a tree learned by TreeVAE is depicted in Fig. 1. Each edge and split are encoded by neural networks, while the circles depict latent variables. Each sample is associated with a probability distribution over _paths_. The resulting tree thus organizes the data into an interpretable hierarchical structure in an unsupervised fashion, optimizing the amount of shared information between samples. In CIFAR-10, for example, the method divides the vehicles and animals into two different subtrees and similar groups (such as planes and ships) share common ancestors.

**Our main contributions** are as follows: (\(i\)) We propose a novel, deep probabilistic approach to hierarchical clustering that learns the optimal generative binary tree to mimic the hierarchies present in the data. (\(ii\)) We provide a thorough empirical assessment of the proposed approach on MNIST, Fashion-MNIST, 20Newsgroups, and Omniglot. In particular, we show that TreeVAE (a) outperforms related work on deep hierarchical clustering, (b) discovers meaningful patterns in the data and their hierarchical relationships, and (c) achieves a more competitive log-likelihood lower bound compared to VAE and LadderVAE, its sequential counterpart. (\(iii\)) We propose an extension of TreeVAE that integrates contrastive learning into its tree structure. Relevant prior knowledge, expertise, or specific constraints can be incorporated into the generative model via augmentations, allowing for more accurate and contextually meaningful clustering. We test the contrastive version of TreeVAE on CIFAR-10, CIFAR-100, and CelebA, and we show that the proposed approach achieves competitive hierarchical clustering performance compared to the baselines.

## 2 TreeVAE

We propose TreeVAE, a novel deep generative model that learns a flexible tree-based posterior distribution over latent variables. Each sample travels through the tree from root to leaf in a probabilistic manner as TreeVAE learns sample-specific probability distributions of paths. As a result, the data is divided in a hierarchical fashion, with more refined concepts for deeper nodes in the tree. The proposed graphical model is depicted in Fig. 2. The inference and generative models share the same top-down tree structure, enabling interaction between the bottom-up and top-down architecture, similarly to Sonderby et al. (2016).

### Model Formulation

Given \(H\), the maximum depth of the tree, and a dataset \(\), the model is defined by three components that are learned during training:

Figure 1: The hierarchical structure discovered by TreeVAE on the CIFAR-10 dataset. We display random subsets of images that are probabilistically assigned to each leaf of the tree.

* the _global_ structure of the binary tree \(\), which specifies the set of nodes \(=\{0,,V\}\), the set of leaves \(\), where \(\), and the set of edges \(\). See Fig. 1/4/5/6/7 for different examples of tree structures learned by the model.
* the _sample-specific_ latent embeddings \(=\{_{0},,_{V}\}\), which are random variables assigned to each node in \(\). Each embedding is characterized by a Gaussian distribution whose parameters are a function of the realization of the parent node. The dimensions of the latent embeddings are defined by their depth, with \(_{i}^{h_{(i)}}\) where \((i)\) is the depth of the node \(i\), and \(h_{(i)}\) is the embedding dimension for that depth.
* the _sample-specific_ decisions \(=\{_{0},,_{V-\|\|}\}\), which are Bernoulli random variables defined by the probability of going to the right (or left) child of the underlying node. They take values \(c_{i}\{0,1\}\) for \(i\), with \(c_{i}=0\) if the left child is selected. A decision path, \(_{l}\), indicates the path from root to leaf given the tree \(\) and is defined by the nodes in the path, e.g., in Fig. 2, \(_{l}=\{0,1,4,5\}\). The probability of \(_{l}\) is the product of the probabilities of the decisions in the path.

The tree structure is shared across the entire dataset and is learned iteratively by growing the tree node-wise. The latent embeddings and the decision paths, on the other hand, are learned using variational inference by conditioning the model on the current tree structure. The generative/inference model and the learning objective conditioned on \(\) are explained in Sec. 2.2/2.3/2.4 respectively, while in 2.5, we elaborate on the efficient growing procedure of the tree.

### Generative Model

The generative process of TreeVAE for a given \(\) is depicted in Fig. 2 (right). The generation of a new sample \(\) starts from the root. First, the latent embedding of the root node \(_{0}\) is sampled from a standard Gaussian \(p_{}(_{0})=(_{0},)\). Then, given the sampled \(_{0}\), the decision of going to the left or the right node is sampled from a Bernoulli distribution \(p(_{0}_{0})=Ber(r_{p,0}(_{0}))\), where \(\{r_{p,i} i\}\) are functions parametrized by neural networks defined as _routers_, and cause the splits in Fig. 2. The subscript \(p\) is used to indicate the parameters of the generative model. The latent embedding of the selected child, let us assume it is \(_{1}\), is then sampled from a Gaussian distribution \(p_{}(_{1}_{0})=(_{1}_{p,1}(_{0}),_{p,1}^{2}(_{0}))\), where \(\{_{p,i},_{p,i} i\{0\}\}\) are functions parametrized by neural networks defined as _transformations_. They are indicated by the top-down arrows in Fig. 2. This process continues until a leaf is reached.

Let us define the set of latent variables selected by the path \(_{l}\), which goes from the root to the leaf \(l\), as \(_{_{l}}=\{_{i} i_{l}\}\), the parent node of the node \(i\) as \(pa(i)\), and \(p(c_{pa(i) i}_{pa(i)})\) the probability of going from \(pa(i)\) to \(i\). Note that the path \(_{l}\) defines the sequence of decisions. The prior probability of the latent embeddings and the path given the tree \(\) can be summarized as

\[p_{}(_{_{l}},_{l})=p(_{0})_{i _{l}\{0\}}p(c_{pa(i) i}_{pa(i)})p(_{i} _{pa(i)}). \]

Finally, \(\) is sampled from a distribution that is conditioned on the selected leaf. If we assume that \(\) is real-valued, then

\[p_{}(_{_{l}},_{l})= (_{x,l}(_{l}),_{x,l}^{2}( _{l})), \]

where \(\{_{x,l},_{x,l} l\}\) are functions parametrized by leaf-specific neural networks defined as _decoders_.

Figure 2: The proposed inference (left) and generative (right) models for TreeVAE. Circles are stochastic variables while diamonds are deterministic. The global topology of the tree is learned during training.

### Inference Model

The inference model is described by the variational posterior distribution of both the latent embeddings and the paths. It follows a similar structure as in the prior probability defined in (1), with the difference that the probability of the root and of the decisions are now conditioned on the sample \(\):

\[q(_{_{l}},_{l})=q(_{0} )_{i_{l}\{0\}}q(c_{pa(i) i} )q(_{i}_{pa(i)}). \]

To compute the variational probability distribution of the latent embeddings \(q(_{0})\) and \(q(_{i}_{pa(i)})\), where

\[q(_{0})=(_{0}_{q,0} (),_{q,0}^{2}()) \] \[q_{}(_{i}_{pa(i)})= (_{i}_{q,i}(_{pa(i)}),_{q,i}^{2}( _{pa(i)})), i_{l}, \]

we follow a similar approach to the one proposed by Sonderby et al. (2016). Note that we use the subscript \(q\) to indicate the parameters of the inference model.

First, a deterministic bottom-up pass computes the node-specific approximate likelihood contributions

\[_{h} =(_{h+1}) \] \[}_{q,i} =(_{depth(i)}),i\] (7) \[}_{q,i}^{2} =((_{depth(i) })),i, \]

where \(_{H}\) is parametrized by a domain-specific neural network defined as _encoder_, and \((_{h})\) for \(h\{1,,H\}\), indicated by the bottom-up arrows in Fig. 2, are neural networks, shared among the parameter predictors, \(_{q,i},_{q,i}^{2},\) of the same depth. They are characterized by the same architecture as the _transformations_ defined in Sec.2.2.

A stochastic downward pass then recursively computes the approximate posteriors defined as

\[_{q,i}^{2}=}_{q,i}^{-2}+_{p,i}^{-2}},_{q,i}=}_{q,i}}_{q,i}^{-2}+_{p,i}_{p,i}^{-2}}{}_{q,i}^{-2}+_{p,i}^{-2}}, \]

where all operations are performed elementwise. Finally, the variational distributions of the decisions \(q(_{i})\) are defined as

\[q(_{i})=q(_{i}_{})=Ber(r_{q,i}(_{})), \]

where \(\{r_{q,i} i\}\) are functions parametrized by neural networks and are characterized by the same architecture as the _routers_ of the generative model defined in Sec. 2.2.

### Evidence Lower Bound

The parameters of both the generative model (defined as \(p\)) and inference model (defined as \(q\)), consisting of the encoder (\(_{q,0},_{q,0}\)), the transformations (\(\{(_{p,i},_{p,i}),(_{q,i},_{q,i}) i \{0\}\}\)), the decoders (\(\{_{x,l},_{x,l} l\}\)) and the routers (\(\{r_{p,i},r_{q,i} i\}\)), are learned by maximizing the Evidence Lower Bound (ELBO) (Kingma & Welling, 2014; Rezende et al., 2014). Each leaf \(l\) is associated with only one path \(_{l}\), hence we can write the data likelihood conditioned on \(\) as

\[p()=_{l}_{_{_{l}}}p( ,_{_{l}},_{l})=_{l}_{ _{_{l}}}p_{}(_{_{l}},_{l})p _{}(_{_{l}},_{l}). \]

We use variational inference to derive the ELBO of the log-likelihood:

\[():=_{q(_{p_{l}},_{ l}|)}[ p(_{_{l}},_{l})]- (q(_{_{l}},_{l} )\|p(_{_{l}},_{l})). \]

The first term of the ELBO is the reconstruction term:

\[_{rec} =_{q(_{_{l}},_{l}|)}[  p(_{_{l}},_{l})] \] \[=_{l}_{_{_{l}}}q(_{0} )_{i_{l}\{0\}}q(c_{pa(i) i} )q(_{i}_{pa(i)}) p(_{_{l}}, _{l})\] (14) \[_{m=1}^{M}_{l}P(l;) (_{x,l}(_{l}^{(m)}), _{x,l}^{2}(_{l}^{(m)})),\] (15) \[P(i;) =_{j_{i}\{0\}}q(c_{pa(j) j}) \ \ i, \]where \(_{i}\) for \(i\) is the path from root to node \(i\), \(P(i;)\) is the probability of reaching node \(i\), which is the product over the probabilities of the decisions in the path until \(i\), \(_{l}^{(m)}\) are the Monte Carlo (MC) samples, and \(M\) the number of the MC samples. Intuitively, the reconstruction loss is the sum of the leaf-wise reconstruction losses weighted by the probabilities of reaching the respective leaf. Note that here we sum over all possible paths in the tree, which is equal to the number of leaves.

The second term of (12) is the Kullback-Leibler divergence (KL) between the prior and the variational posterior of the tree. It can be written as a sum of the KL of the root, the nodes, and the decisions:

\[(q(_{_{l}},_{l} )\|\!\!\|p(_{_{l}},_{l}))=_{root}+_{nodes}+_{ decisions} \] \[_{root}=(q(_{0})\|p(_ {0}))\] (18) \[_{nodes}_{m=1}^{M}_{i \{0\}}P(i;)\,(q(_{i}^{(m)} pa( _{i}^{(m)}))\|p(_{i}^{(m)} pa(_{i}^{(m)})))\] (19) \[_{decisions}_{m=1}^{M}_{i }P(i;)_{c_{i}\{0,1\}}q(c_{i})()}{p(c_{i}_{i}^{(m)})}), \]

where \(M\) is the number of MC samples. We refer to Appendix A for the full derivation. The \(_{root}\) is the KL between the standard Gaussian prior \(p(_{0})\) and the variational posterior of the root \(q(_{0})\), thus enforcing the root to be compact. The \(_{nodes}\) is the sum of the node-specific KLs weighted by the probability of reaching their node \(i\): \(P(i;)\). The node-specific KL of node \(i\) is the KL between the two Gaussians \(q(_{i} pa(_{i}))\), \(p(_{i} pa(_{i}))\). Finally, the last term, \(_{decisions}\), is the weighted sum of all the KLs of the decisions, which are Bernoulli random variables, \(KL(q(_{i}) p(_{i}_{i})))=_{c_{i}\{0,1\}}q(c_{i})()}{p(c_{i}_{ i}))})\). The hierarchical specification of the binary tree allows encoding highly expressive models while retaining the computational efficiency of fully factorized models. The computational complexity is described in Appendix A.2.

### Growing The Tree

In the previous sections, we discussed the variational objective to learn the parameters of both the generative and the inference model given a defined tree structure \(\). Here we discuss how to learn the structure of the binary tree \(\). TreeVAE starts by training a tree composed of a root and two leaves, see Fig. 3 (left), for \(N_{t}\) epochs by optimizing the ELBO. Once the model converged, a leaf is selected, e.g., \(_{1}\) in Fig. 3, and two children are attached to it. The leaf selection criteria can vary depending on the application and can be determined by, e.g., the reconstruction loss or the ELBO. In our experiments, we chose to select the nodes with the maximum number of samples to retain balanced leaves. The sub-tree composed of the new leaves and the parent node is then trained for \(N_{t}\) epochs by freezing the weights of the rest of the model, see Fig. 3 (right), resulting in computing the ELBO of the nodes of the subtree. For efficiency, the subtree is trained using only the subset of data that have a high probability (higher than a threshold \(t\)) of being assigned to the parent node. The process is repeated until the tree reaches its maximum capacity (defined by the maximum depth) or until a condition (such as a predefined maximum number of leaves) is met. The entire model is then fine-tuned for \(N_{f}\) epochs by unfreezing all weights. During fine-tuning, the tree is pruned by removing empty branches (with the expected number of assigned samples lower than a threshold).

### Integrating Prior Knowledge

Retrieving semantically meaningful clustering structures of real-world images is extremely challenging, as there are several underlying factors according to which the data can be clustered. Therefore, it

Figure 3: The first two steps of the growing process to learn the global structure of the tree during training. Highlighted in red are the trainable weights.

is often crucial to integrate domain knowledge that guides the model toward desirable cluster assignments. Thus, we propose an extension of TreeVAE where we integrate recent advances in contrastive learning (van den Oord et al., 2018; Chen et al., 2020; Li et al., 2021), whereby prior knowledge on data invariances can be encoded through augmentations. For a batch \(\) with \(N\) samples, we randomly augment every sample twice to obtain the augmented batch \(}\) with \(2N\) samples. For all positive pairs \((i,j)\) where \(}_{i}\) and \(}_{j}\) stem from the same original sample, we utilize the _NT-Kent_(Chen et al., 2020), which introduces losses \(_{i,j}=-/)}{_{k=1}^{N}/ )}{1[k i]}(s_{i,k}/)}\), where \(s_{i,j}\) denotes the cosine similarity between the representations of \(}_{i}\) and \(}_{j}\), and \(\) is a temperature parameter. We integrate \(_{i,j}\) in both the bottom-up and the routers of TreeVAE. In the bottom-up, similar to Chen et al. (2020), we compute \(_{i,j}\) on the projections \(g_{h}(_{h})\). For the routers, we directly compute the loss on the predicted probabilities \(r_{q,i}(_{h})\). Finally, we average the terms over all positive pairs and add them to the negative ELBO (12) in real-world image experiments. Implementation details can be found in Appendix E, while a loss ablation is shown in Appendix C.3.

## 3 Related Work

Deep latent variable models automatically learn structure from data by combining the flexibility of deep neural networks and the statistical foundations of generative models (Mattei and Frellsen, 2018). Variational autoencoders (VAEs) (Rezende et al., 2014; Kingma and Welling, 2014) are among the most used frameworks (Nasiri and Bepler, 2022; Bae et al., 2023; Bredell et al., 2023). A variety of works has been proposed to integrate more complex empirical prior distributions, thus reducing the gap between approximate and true posterior distributions (Ranganath et al., 2015; Webb et al., 2017; Klushyn et al., 2019). Among these, the most related to our work is the VAE-nCRP (Goyal et al., 2017; Shin et al., 2019) and the TMC-VAE (Vikram et al., 2018). Both works use Bayesian nonparametric hierarchical clustering based on the nested Chinese restaurant process (nCRP) prior (Blei et al., 2003), and on the time-marginalized coalescent (TMC). However, even if they allow more flexible prior distributions these models suffer from restrictive posterior distributions (Kingma et al., 2016).To overcome the above issue, deep hierarchical VAEs (Gregor et al., 2015; Kingma et al., 2016) have been proposed to employ structured approximate posteriors, which are composed of hierarchies of conditional stochastic variables that are connected _sequentially_. Among a variety of proposed methods (Vahdat and Kautz, 2020; Falck et al., 2022; T. Z. Xiao and Bamler, 2023), Ladder VAE (Sonderby et al., 2016) is most related to TreeVAE. The authors propose to model the approximate posterior by combining a "bottom-up" recognition distribution with the "top-down" prior. Further extensions include BIVA (Maaloe et al., 2019), which introduces a bidirectional inference network, and GraphVAE (He et al., 2019), that introduces gated dependencies over a fixed number of latent variables. Contrary to the previous approaches, TreeVAE models a _tree-based_ posterior distribution of latent variable, thus allowing hierarchical clustering of samples. For further work on hierarchical clustering and its supervised counterpart, decision trees, we refer to Appendix B.

## 4 Experimental Setup

Datasets and Metrics:We evaluate the clustering and generative performance of TreeVAE on MNIST (LeCun et al., 1998), Fashion-MNIST (H. Xiao et al., 2017), 20Newsgroups (Lang, 1995), Omniglot (Lake et al., 2015), and Omniglot-5, where only \(5\) vocabularies (Braille, Glagolitic, Cyrillic, Odia, and Bengali) are selected and used as true labels. We assess the hierarchical clustering performance by computing dendrogram purity (DP) and leaf purity (LP), as defined by (Kobren et al., 2017a) using the datasets labels, where we assume the number of true clusters is unknown. We also report standard clustering metrics, accuracy (ACC) and normalized mutual information (NMI), by setting the number of leaves for TreeVAE and for the baselines to the true number of clusters. In terms of generative performance, we compute the approximated true log-likelihood calculated using \(1000\) importance-weighted samples, together with the ELBO (12) and the reconstruction loss (16). We also perform hierarchical clustering experiments on real-world imaging data, namely CIFAR-10, CIFAR-100 (Krizhevsky and Hinton, 2009) with \(20\) superclasses as labels, and CelebA (Z. Liu et al., 2015) using the contrastive extension (Sec. 2.6). We refer to Appendix D for more dataset details.

Baselines:We compare the generative performance of TreeVAE to the VAE (Rezende et al., 2014; Kingma and Welling, 2014), its non-hierarchical counterpart, and the LadderVAE (Sonderby et al., 2016), its sequential counterpart. For a fair comparison, all methods share the same architecture and hyperparameters whenever possible. We compare TreeVAE to non-generative hierarchical clustering baselines for which the code was publicly available: Ward's minimum variance agglomerative clustering (Agg) (Ward, 1963; Murtagh & Legendre, 2014), and the DeepECT (Mautz et al., 2020). We propose two additional baselines, where we perform Ward's agglomerative clustering on the latent space of the VAE (VAE + Agg) and of the last layer of the LadderVAE (LadderVAE + Agg). For the contrastive clustering experiments, we apply a contrastive loss similar to TreeVAE to the VAE and the LadderVAE, while for DeepECT we use the contrastive loss proposed by the authors.

**Implementation Details:** While we believe that more complex architectures could have a substantial impact on the performance of TreeVAE, we choose to employ rather simple settings to validate the proposed approach. We set the dimension of all latent embeddings \(=\{_{0},,_{V}\}\) to \(8\) for MNIST, Fashion, and Omniglot, to \(4\) for 20Newsgroups, and to \(64\) for CIFAR-10, CIFAR-100, and CelebA. The maximum depth of the tree is set to \(6\) for all datasets, except 20Newsgroups where we increased the depth to \(7\) to capture more clusters. To compute DP and LP, we allow the tree to grow to a maximum of \(30\) leaves for 20Newsgroups and CIFAR-100, and \(20\) for the rest, while for ACC and NMI we fix the number of leaves to the number of true classes. The transformations consist of one-layer MLPs of size \(128\) and the routers of two-layers of size \(128\) for all datasets except for the real-world imaging data where we slightly increase the MLP complexity to \(512\). Finally, the encoder and decoders consist of simple CNNs and MLPs. The trees are trained for \(N_{t}=150\) epochs at each growth step, and the final tree is finetuned for \(N_{f}=200\) epochs. For the real-world imaging experiments, we set the weight of the contrastive loss to \(100\). See Appendix E for additional details.

   Dataset & Method & DP & LP & ACC & NMI \\  MNIST & Agg & \(63.7 0.0\) & \(78.6 0.0\) & \(69.5 0.0\) & \(71.1 0.0\) \\  & VAE + Agg & \(79.9 2.2\) & \(90.8 1.4\) & \(86.6 4.9\) & \(81.6 2.0\) \\  & LadderVAE + Agg & \(81.6 3.9\) & \(90.9 2.5\) & \(80.3 5.6\) & \(82.0 2.1\) \\  & DeepECT & \(74.6 5.9\) & \(90.7 3.2\) & \(74.9 6.2\) & \(76.7 4.2\) \\  & TreeVAE (ours) & \(\) & \(\) & \(\) & \(\) \\  Fashion & Agg & \(45.0 0.0\) & \(67.6 0.0\) & \(51.3 0.0\) & \(52.6 0.0\) \\  & VAE + Agg & \(44.3 2.5\) & \(65.9 2.3\) & \(54.9 4.4\) & \(56.1 3.2\) \\  & LadderVAE + Agg & \(49.5 2.3\) & \(67.6 1.2\) & \(55.9 3.0\) & \(60.7 1.4\) \\  & DeepECT & \(44.9 3.3\) & \(67.8 1.4\) & \(51.8 5.7\) & \(57.7 3.7\) \\  & TreeVAE (ours) & \(\) & \(\) & \(\) & \(\) \\ 
20Newsgroups & Agg & \(13.1 0.0\) & \(30.8 0.0\) & \(26.1 0.0\) & \(27.5 0.0\) \\  & VAE + Agg & \(7.1 0.3\) & \(18.1 0.5\) & \(15.2 0.4\) & \(11.6 0.3\) \\  & LadderVAE + Agg & \(9.0 0.2\) & \(20.0 0.7\) & \(17.4 0.9\) & \(17.8 0.6\) \\  & DeepECT & \(9.3 1.8\) & \(17.2 3.8\) & \(15.6 3.0\) & \(18.1 4.1\) \\  & TreeVAE (ours) & \(\) & \(\) & \(\) & \(\) \\  Omniglot-5 & Agg & \(41.4 0.0\) & \(63.7 0.0\) & \(53.2 0.0\) & \(33.3 0.0\) \\  & VAE + Agg & \(46.3 2.3\) & \(68.1 1.6\) & \(52.9 4.2\) & \(34.4 2.9\) \\  & LadderVAE + Agg & \(49.8 3.9\) & \(71.3 2.0\) & \(59.6 4.9\) & \(44.2 4.7\) \\  & DeepECT & \(33.3 2.5\) & \(55.1 2.8\) & \(41.1 4.2\) & \(23.5 4.3\) \\  & TreeVAE (ours) & \(\) & \(\) & \(\) & \(\) \\   CIFAR-10* & VAE + Agg & \(10.54 0.12\) & \(16.33 0.15\) & \(14.43 0.19\) & \(1.86 1.66\) \\  & LadderVAE + Agg & \(12.81 0.20\) & \(25.37 0.62\) & \(19.29 0.60\) & \(7.41 0.42\) \\  & DeepECT & \(10.01 0.02\) & \(10.30 0.40\) & \(10.31 0.39\) & \(0.18 0.10\) \\  & TreeVAE (ours) & \(\) & \(\) & \(\) & \(\) \\  CIFAR-100* & VAE + Agg & \(5.27 0.02\) & \(9.86 0.19\) & \(8.82 0.11\) & \(2.46 0.10\) \\  & LadderVAE + Agg & \(6.36 0.07\) & \(16.08 0.28\) & \(14.01 0.41\) & \(8.99 0.41\) \\  & DeepECT & \(5.28 0.18\) & \(6.97 0.69\) & \(6.97 0.69\) & \(1.71 0.86\) \\  & TreeVAE (ours) & \(\) & \(\) & \(\) & \(\) \\   

Table 1: Test set hierarchical clustering performances (\(\%\)) of TreeVAE compared with baselines. Means and standard deviations are computed across 10 runs with different random model initialization. The star "*” indicates real-world image datasets on which contrastive approaches were applied.

## 5 Results

Hierarchical Clustering ResultsTable 1 shows the quantitative hierarchical clustering results averaged across \(10\) seeds. First, we assume the true number of clusters is _unknown_ and report DP and LP. Second, we assume we have access to the true number of clusters \(K\) and compute ACC and NMI. As can be seen, TreeVAE outperforms the baselines in both experiments. This suggests that the proposed approach successfully builds an optimal tree based on the data's intrinsic characteristics. Among the different baselines, agglomerative clustering using Ward's method (Agg) trained on the last layer of LadderVAE shows competitive performances. To the best of our knowledge, we are the first to report these results. It is noteworthy to observe that it consistently improves over VAE + Agg, indicating that the last layer of LadderVAE captures more cluster information than the VAE.

Generative ResultsIn Table 2, we evaluate the generative performance of the proposed approach, TreeVAE, compared to the VAE, its non-hierarchical counterpart, and LadderVAE, its sequential counterpart. TreeVAE outperforms the baselines on the majority of datasets, indicating that the proposed ELBO (12) can achieve a tighter lower bound of the log-likelihood. The most notable improvement appears to be reflected in the reconstruction loss, showing the advantage of using cluster-specialized decoders. However, this improvement comes at the expense of a larger neural network architecture and an increase in the number of parameters (as TreeVAE has \(L\) decoders). While this requires more computational resources at training time, during deployment the tree structure of TreeVAE permits lightweight inference through conditional sampling, thus matching the inference time of LadderVAE. It is also worth mentioning that results differ from (Sonderby et al., 2016) as we adapt their architecture to match our experimental setting and consequently use smaller latent dimensionality. Finally, we notice that more complex methods are prone to overfitting on the 20Newsgroups dataset, hence the best performances are achieved by the VAE.

Real-world Imaging Data & Contrastive LearningClustering real-world imaging data is extremely difficult as there are endless possibilities of how the data can be partitioned (such as the colors, the landscape, etc). We therefore inject prior information through augmentations to guide TreeVAE and the baselines to semantically meaningful splits. Table 1 (bottom) shows the hierarchical clustering performance of TreeVAE and its baselines, all employing contrastive learning, on CIFAR-10 and CIFAR-100. We observe that DeepECT struggles in separating the data as their contrastive approach leads to all samples falling into the same leaf. In Table 3, we present the leaf-frequencies of various face attributes using the tree learned by TreeVAE. For all datasets, TreeVAE is able to group the data into contextually meaningful hierarchies and groups, evident from its superior performance compared to the baselines and from the distinct attribute frequencies in the leaves and subtrees.

Discovery of HierarchiesIn addition to solely clustering data, TreeVAE is able to discover meaningful hierarchical relations between the clusters, thus allowing for more insights into the

   Dataset & Method & LL & RL & ELBO \\  MNIST & VAE & \(-101.9 0.2\) & \(87.2 0.3\) & \(-104.6 0.3\) \\  & LadderVAE & \(-99.9 0.5\) & \(87.8 0.7\) & \(-103.2 0.7\) \\  & TreeVAE (ours) & \(\) & \(\) & \(\) \\  Fashion & VAE & \(-242.2 0.2\) & \(231.7 0.5\) & \(-245.4 0.5\) \\  & LadderVAE & \(-239.4 0.5\) & \(231.5 0.6\) & \(-243.0 0.6\) \\  & TreeVAE (Ours) & \(\) & \(\) & \(\) \\ 
20Newsgroups & VAE & \(\) & \(45.52 0.03\) & \(\) \\  & LadderVAE & \(-44.30 0.03\) & \(\) & \(\) \\  & TreeVAE (Ours) & \(-51.67 0.59\) & \(45.83 0.36\) & \(-52.79 0.66\) \\  Omniglot & VAE & \(-115.3 0.3\) & \(101.6 0.3\) & \(-118.2 0.3\) \\  & LadderVAE & \(-113.1 0.5\) & \(100.7 0.7\) & \(-117.5 0.6\) \\  & TreeVAE (Ours) & \(\) & \(\) & \(\) \\   

Table 2: Test set generative performances of TreeVAE with 10 leaves compared with baselines. Means and standard deviations are computed across 10 runs with different random model initialization.

dataset. In the introductory Fig. 1, 5, and 6, we present the hierarchical structures learned by TreeVAE, while in Fig. 4 and 7, we additionally display conditional cluster generations from the leaf-specific decoders. In Fig. 4, TreeVAE separates the fashion items into two subtrees, one containing shoes and bags, and the other containing the tops, which are further refined into long and short sleeves. In Fig. 5, we depict the most prevalent ground-truth topic label in each leaf. TreeVAE learns to

Figure 4: Hierarchical structures learned by TreeVAE on Fashion. Subtree (a) encodes tops, while (b) encodes shoes, purses, and pants.

Figure 5: Hierarchical structure learned by TreeVAE on 20Newsgroups.

Figure 6: Hierarchical structures learned by TreeVAE on Omniglot-5. Subtree (a) learns a hierarchy over Braille and the Indian alphabets, while (b) groups Slavic alphabets.

 Attribute & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\  Female & \(97.2\) & \(55.0\) & \(97.7\) & \(86.6\) & \(23.1\) & \(30.7\) & \(46.6\) & \(43.7\) \\ Bangs & \(1.6\) & \(1.2\) & \(24.1\) & \(61.7\) & \(3.4\) & \(11.1\) & \(9.1\) & \(11.4\) \\ Blonde & \(1.1\) & \(3.7\) & \(66.7\) & \(2.2\) & \(5.8\) & \(2.6\) & \(26.1\) & \(7.1\) \\ Makeup & \(75.7\) & \(43.4\) & \(76.6\) & \(59.7\) & \(15.0\) & \(12.4\) & \(16.3\) & \(12.8\) \\ Smiling & \(54.3\) & \(66.6\) & \(66.4\) & \(51.2\) & \(54.7\) & \(42.4\) & \(37.3\) & \(22.4\) \\ Hair Loss & \(3.6\) & \(17.8\) & \(3.0\) & \(0.2\) & \(18.9\) & \(6.9\) & \(21.2\) & \(10.6\) \\ Beard & \(1.1\) & \(20.6\) & \(0.4\) & \(3.7\) & \(39.5\) & \(36.5\) & \(21.3\) & \(21.4\) \\  

Table 3: We present the frequency (in %) of selected attributes for each leaf of TreeVAE with eight leaves in CelebA.

Figure 6: Hierarchical structures learned by TreeVAE on Omniglot-5. Subtree (a) learns a hierarchy over Braille and the Indian alphabets, while (b) groups Slavic alphabets.

Figure 7: Hierarchical structure learned by TreeVAE with eight leaves on the CelebA dataset with generated images through conditional sampling. Generally, most females are in the left subtree, while most males are in the right subtree. We observe that leaf \(1\) is associated with dark-haired females, leaf \(2\) with smiling, dark-haired individuals, leaf \(3\) with blonde females, leaf \(4\) with bangs, leaf \(7\) with a receding hairline, and leaf \(8\) with non-smiling people. See Table 3 for further details.

separate technological and societal subjects and discovers semantically meaningful subtrees. In Fig. 6, TreeVAE learns to split alphabets into Indian (Odia and Bengali) and Slavic (Glagolitic and Cyrillic) subtrees, while Braille is grouped with the Indian languages due to similar circle-like structures. For CelebA, Fig. 7 and Table 3, the resulting tree separates genders in the root split. Females (left) are further divided by hair color and hairstyle (bangs). Males (right) are further divided by smile intensity, beard, hair loss, and age. In Fig. 8 and Appendix C we show how TreeVAE can additionally be used to sample unconditional generations for all clusters simultaneously, by sampling from the root and propagating through the entire tree. The generations differ across the leaves by their cluster-specific features, whereas cluster-independent properties are retained across all generations.

## 6 Conclusion

In this paper, we introduced TreeVAE, a new generative method that leverages a tree-based posterior distribution of latent variables to capture the hierarchical structures present in the data. TreeVAE optimizes the balance between shared and specialized architecture, enhancing the learning and adaptation capabilities of generative models. Empirically, we showed that our model offers a substantial improvement in hierarchical clustering performance compared to the related work, while also providing a tighter lower bound to the log-likelihood of the data. We presented qualitatively how the hierarchical structures learned by TreeVAE enable a more comprehensive understanding of the data, thereby facilitating enhanced analysis, interpretation, and decision-making. Our findings highlight the versatility of the proposed approach, which we believe to hold significant potential for unsupervised representation learning, paving the way for exciting advancements in the field.

**Limitations & Future Work:** Currently, TreeVAE uses a simple heuristic on which node to split that might not work on datasets with unbalanced clusters. Additionally, the contrastive losses on the routers encourage balanced clusters. Thus, more research is necessary to convert the heuristics to data-driven approaches. While deep latent variable models, such as VAEs, provide a framework for modeling explicit relationships through graphical structures, they often exhibit poor performance on synthetic image generation. However, more complex architectural design (Vahdat and Kautz, 2020) or recent advancement in diffusion latent models (Rombach et al., 2021) present potential solutions to enhance image quality generation, thus striking an optimal balance between generating high-quality images and capturing meaningful representations.