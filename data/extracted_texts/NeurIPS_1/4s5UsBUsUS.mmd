# VFIMamba: Video Frame Interpolation

with State Space Models

Guozhen Zhang\({}^{1,2}\)1 Chunxu Liu\({}^{1}\) Yutao Cui\({}^{2}\) Xiaotong Zhao\({}^{2}\) Kai Ma\({}^{2}\) Limin Wang\({}^{1,3}\)

\({}^{1}\)State Key Laboratory for Novel Software Technology, Nanjing University

\({}^{2}\)Platform and Content Group (PCG), Tencent \({}^{3}\)Shanghai AI Lab

[https://github.com/MCG-NJU/VFIMamba](https://github.com/MCG-NJU/VFIMamba)

###### Abstract

Inter-frame modeling is pivotal in generating intermediate frames for video frame interpolation (VFI). Current approaches predominantly rely on convolution or attention-based models, which often either lack sufficient receptive fields or entail significant computational overheads. Recently, Selective State Space Models (S6) have emerged, tailored specifically for long sequence modeling, offering both linear complexity and data-dependent modeling capabilities. In this paper, we propose VFIMamba, a novel frame interpolation method for efficient and dynamic inter-frame modeling by harnessing the S6 model. Our approach introduces the Mixed-SSM Block (MSB), which initially rearranges tokens from adjacent frames in an interleaved fashion and subsequently applies multi-directional S6 modeling. This design facilitates the efficient transmission of information across frames while upholding linear complexity. Furthermore, we introduce a novel curriculum learning strategy that progressively cultivates proficiency in modeling inter-frame dynamics across varying motion magnitudes, fully unleashing the potential of the S6 model. Experimental findings showcase that our method attains state-of-the-art performance across diverse benchmarks, particularly excelling in high-resolution scenarios. In particular, on the X-TEST dataset, VFIMamba demonstrates a noteworthy improvement of **0.80** dB for 4K frames and **0.96** dB for 2K frames.

## 1 Introduction

Video Frame Interpolation (VFI), a fundamental task in video data processing, is gaining substantial attention for its ability to generate intermediate frames between consecutive frames (Liu et al., 2017). Its utility spans many practical applications, including creating slow-motion videos through temporal upsampling (Jiang et al., 2018), enhancing video refresh rates (Reda et al., 2022), and generating novel views (Flynn et al., 2016; Szeliski, 1999). VFI typically encompass two primary stages (Zhang et al., 2023): firstly, conducting the inter-frame modeling of input consecutive frames; and secondly, leveraging the acquired information to estimate inter-frame motion and generate intermediate frame appearance. In practice, VFI often deals with high-resolution inputs (e.g., 4K) (Sim et al., 2021), which results in significant object displacement and imposes high demands on the large receptive field of the modules that model information between frames. Additionally, since VFI is commonly applied to long-duration videos such as movies, model speed is also of paramount importance. Thus, _striking a delicate balance between a sufficient receptive field and fast processing speed in modeling inter-frame information_ is the key in crafting effective VFI models.

Current methods for modeling inter-frame information predominantly rely on convolutional neural networks (CNNs) (Liu et al., 2017; Kong et al., 2022; Huang et al., 2022) and attention-based models (Lu et al., 2022; Zhang et al., 2023; Park et al., 2023; Liu et al., 2024a). However, as illustrated in the first three rows of Table 1, these methods either (1) lack flexibility and cannot adaptively model based on the input, (2) do not have sufficient receptive fields to capture inter-frame correlations at high resolutions, or (3) suffer from prohibitive computational complexity.

On the other hand, Natural Language Processing (NLP) has recently witnessed the emergence of structured state space models (SSMs) (Gu et al., 2021). Theoretically, SSMs combine the benefits of Recurrent Neural Networks (RNNs) and CNNs, leveraging the global receptive field characteristic of RNNs and the computational efficiency inherent in CNNs. One particularly notable SSM is the Selective State Space Model (S6), also known as Mamba (Gu & Dao, 2023), which has garnered significant attention within the vision community. Mamba's novel feature of making SSM parameters time-variant (i.e., data-dependent) enables it to effectively select relevant context within sequences, a crucial factor for enhancing model performance. However, to the best of our knowledge, _S6 has not yet been applied to low-level video tasks_.

To address the challenges faced by current VFI models and to explore the potential of the S6 model (Gu & Dao, 2023) in low-level video tasks, we propose VFIMamba, a novel frame interpolation method that adapts the S6 model for efficient and dynamic inter-frame modeling. As shown in Table 1, VFIMamba provides the advantages of a global receptive field with linear complexity while maintaining data-dependent adaptability.

Specifically, we introduce the Mixed-SSM Block (MSB) to replace existing modules for inter-frame information transfer. The original S6 model can only process a single sequence, so it is necessary to merge tokens from two frames into one sequence for effective inter-frame modeling. After thorough analysis and exploration, we figured out that interleaving tokens from both frames into a "super image" is more suitable for VFI. We then conduct multi-directional SSMs on this image to model inter-frame information. This interleaved approach facilitates interactions between adjacent tokens from different frames during sequence modeling and ensures that the intermediate tokens of any pair of tokens in the sequence are from their spatiotemporal neighborhood. By stacking MSB modules, our model effectively handles complex inter-frame information exchange. Finally, we use the extracted inter-frame features to estimate motion and generate the appearance of intermediate frames.

While the S6 model boasts the advantages listed in Table 1, it is crucial to employ appropriate training strategies to fully exploit its potential. Inspired by Bengio et al. (2009), we propose a novel curriculum learning strategy that progressively teaches the model to handle inter-frame modeling across varying motion amplitudes. Specifically, while maintaining training on Vimeo90K (Xue et al.,

Figure 1: Equipped with the S6 model, our VFIMamba achieves the state-of-the-art performance on benchmarks across different input resolutions.

2019), we incrementally introduce large motion data from X-TRAIN (Sim et al., 2021), increasing the motion amplitude over time. This learning strategy enables VFIMamba to perform well across a wide range of motion amplitudes, thereby fully unleashing the potential of the S6 model.

To validate the effectiveness of VFIMamba across various types of video data, we conduct extensive testing on different benchmarks. As shown in Figure 1, VFIMamba achieves the state-of-the-art (SOTA) performance across diverse datasets. This is particularly evident in high-resolution and large-motion datasets such as X-TEST (Sim et al., 2021) and SNU-FILM (Choi et al., 2020).

**Contribution.** In summary, the contributions of this paper are as follows: (1) We are the first to adapt the S6 model to the video frame interpolation task. To better adapt the model for this task, we introduce the Mixed-SSM Block (MSB), providing a solid foundation for future architectural exploration in frame interpolation. (2) We propose a novel curriculum learning strategy that incrementally introduces data with varying motion amplitudes, thereby fully harnessing the potential of the S6 model. (3) Our model achieves the state-of-the-art performance across a wide range of datasets, potentially sparking interest in the exploration of the S6 model within the video low-level community.

## 2 Related work

### Video frame interpolation

The performance of VFI methods has seen significant advancements with the emergence of deep learning models. **(1)** CNNs-based approaches (Bao et al., 2019; Liu et al., 2017; Huang et al., 2022; Niklaus and Liu, 2018; Choi et al., 2020; Zhu et al., 2024; Jia et al., 2022; Niklaus et al., 2017; Kalluri et al., 2023): Initially, DVF (Liu et al., 2017) utilized a U-Net-like (Ronneberger et al., 2015) network to model two input frames and predicted the voxel flow for warping the two frames into the intermediate frame. Following this, CtxSyn (Niklaus and Liu, 2018) introduced ContextNet and RefineNet, where ContextNet extracts context information from each frame, and RefineNet further refines the coarse intermediate frame produced by warping. RIFE (Huang et al., 2022) proposed a novel, efficient framework that employs self-distillation to significantly reduce computational load and parameters while maintaining high performance. Due to its simplicity, many convolutional modeling works (Kong et al., 2022; Jia et al., 2022) have improved upon RIFE. **(2)** Attention-based approaches (Lu et al., 2022; Zhang et al., 2023; Park et al., 2023; Liu et al., 2024): VFIFormer (Lu et al., 2022) was the first to use attention to model inter-frame information, replacing the encoder part of U-Net with Transformer blocks. After that, EMA-VFI (Zhang et al., 2023) uses Swin-based (Liu et al., 2021) local attention to simultaneously capture local appearance and motion information. AMT (Li et al., 2023) used a multi-scale cost-volume construction similar to RAFT (Teed and Deng, 2020) to further enhance motion modeling capabilities. BiFormer (Park et al., 2023) introduced quasi-global bilateral attention to further increase the receptive field for large motions. SGM-VFI (Liu et al., 2024) introduced sparse global matching to model motion between frames. However, current models struggle to balance sufficient receptive fields with computational overhead. In contrast, our method introduces the first interpolation model based on State Space Models (SSMs) (Gu and Dao, 2023) and further pushes the performance boundaries of VFI tasks.

### State space models

In the field of NLP, SSMs (Gu et al., 2021; Smith et al., 2022; Mehta et al., 2022; Fu et al., 2022) have recently emerged as one of the most promising contenders to challenge the dominance of Transformers. The Structured State Space Sequence Model (S4) (Gu et al., 2021) was initially introduced for linear complexity modeling of long sequences. Subsequent works have improved

  
**Model** & **Data-dependent** & **Linear complexity** & **Global receptive field** & **Representative method** \\  CNN & ✗ & ✓ & ✗ & RIFE (Huang et al., 2022) \\ Attention & ✓ & ✗ & ✓ & SGM-VFI (Liu et al., 2024) \\ Local Attention & ✓ & ✓ & ✗ & EMA-VFI (Zhang et al., 2023) \\  Miamba & ✓ & ✓ & ✓ & VFIMamba (our work) \\   

Table 1: Comparison of the model design for inter-frame modeling of VFIMamba and existing methods. VFIMamba enjoys both the advantages of a large receptive field and linear complexity.

its computational efficiency and model capacity. S5 (Smith et al., 2022) proposed a parallel scan and MIMO SSM, and GSS (Mehta et al., 2022) enhanced the model's capability by introducing gated mechanisms. Mamba (S6) (Gu and Dao, 2023) has recently stood out due to its data-dependent parameter generation and efficient hardware implementation, outperforming Transformers in long-sequence NLP tasks. In the visual domain, Vim (Zhu et al., 2024) was the first to permute 2D images into sequences for global modeling using bidirectional SSMs. Vmamba (Liu et al., 2024) extended to four directions and introduced a hierarchical structural design. VideoMamba (Li et al., 2024) was the first to apply S6 in the video domain by permuting all frames into a spatiotemporal sequence. MambalR (Guo et al., 2024) was the first to use the S6 model for image restoration tasks, achieving superior performance over Transformers. In this work, we explore the potential of the S6 model in VFI tasks, validating its effectiveness through detailed analysis and experimentation.

## 3 Method

### Preliminaries

SSMs are mainly inspired by the continuous linear time-invariant (LTI) systems, which apply an implicit latent state \(h(t)^{N}\) to map a 1-dimensional sequence or function \(x(t) y(t)\). Specifically, SSMs can be formulated as an ordinary differential equation (ODE):

\[h^{}(t)=Ah(t)+Bx(t), \] \[y(t)=Ch(t), \]

where contains evolution matrix \(A^{N N}\), projection parameters \(B^{N 1}\) and \(C^{1 N}\). However, it is hard to solve the above differential equation in deep learning settings and needs to be approximated through discretization. Recent SSMs (Gu et al., 2021) propose to introduce a timescale parameter \(\) to transform the \(A\), \(B\) to their discrete counterparts \(\), \(\), i.e.,

\[h_{t} =h_{t-1}+Bx_{t}, \] \[y_{t} =Ch_{t},\] (4) \[ =( A),\] (5) \[ =( A)^{-1}(( A-I) ) B. \]

The above SSMs are performed for each channel separately and their parameters are data-independent, meaning that \(\), \(\) and \(C\) are the same for any input in the same channel, limiting their flexibility in sequence modeling. Mamba (Gu and Dao, 2023) proposes the selective SSMs (S6), which dynamically

Figure 2: Overall pipeline of VFIMamba. Firstly, a lightweight feature extractor is employed to encode the input frames into shallow features. Subsequently, we utilize the Mixed-SSM Block (MSB) to conduct inter-frame modeling using S6, iterating \(N\) times at each scale. Finally, these inter-frame features are leveraged to generate the intermediate frame.

generate the parameters for each input data \(x_{i}^{L}\) using the entire \(x_{i}\):

\[B_{i}=S_{B}x_{i}, C_{i}=S_{C}x_{i},_{i}=(S_{}x_{i}), \]

where \(S_{B}^{N L}\), \(S_{C}^{N L}\), \(S_{}^{L L}\) are linear projection layers. The \(B_{i}\) and \(C_{i}\) are shared for all channels of \(x_{i}\), \(_{i}\) contains \(\) of \(L\) channels, and \(A\) are the same as previous SSMs. By the discretization in equations 5 and 6, \(\) and \(\) become different based on input data.

### Overall pipeline

Given two consecutive frames \(I_{0},I_{1}^{H W 3}\) along with a timestep \(t\), the objective of the frame interpolation task is to generate the intermediate frame \(I_{t}^{H W 3}\). As illustrated in Figure 2, the overall pipeline of VFIMamba consists of three main components: frame feature extraction, inter-frame modeling, and frame generation. Firstly, we employ a set of lightweight convolutional layers to independently extract shallow features from each frame, progressively reducing the resolution to facilitate more efficient inter-frame modeling. This process can be formulated as:

\[F^{i}_{l}=(I_{i}), \]

where \(\) represents the set of convolutional layers, and \(F^{i}_{l}\) denotes the extracted low-level feature for \(I_{i}\). Next, we perform multi-resolution inter-frame modeling using the proposed Mixed-SSM Block (MSB). Each scale comprises \(N\) MSBs, and downsampling between scales is achieved through overlapping patch embedding (Wang et al., 2022). We define the resulting inter-frame features as \(F^{i}_{ssm}\). Finally, we utilize these high-quality inter-frame features for frame generation, which involves motion estimation between two frames and appearance refinement:

\[I_{t}=(F^{0}_{ssm},F^{1}_{ssm}), \]

where \(\) denotes the frame generation network. Since this work primarily focuses on exploring the use of SSMs for inter-frame modeling, we largely follow the design from Zhang et al. (2023) and Huang et al. (2022) for the frame generation components, with detailed specifications provided in the appendix.

### State space models for inter-frame modeling

Effective inter-frame modeling is crucial for frame interpolation tasks (Zhang et al., 2023). Methods such as RIFE (Huang et al., 2022) and EMA-VFI (Zhang et al., 2023) employ simple convolution layers or local attention for inter-frame modeling, achieving high inference speeds but limiting receptive field. Conversely, SGM-VFI (Liu et al., 2024) uses global inter-frame attention for motion estimation, which improves performance but sacrifices efficiency. In this work, we propose to use state space models (SSMs), specifically S6 (Gu & Dao, 2023), to achieve both efficiency and effectiveness in inter-frame modeling.

#### 3.3.1 Mixed-SSM block

To facilitate more efficient inter-frame information exchange globally, we utilize SSMs for inter-frame modeling. As illustrated in Figure 2, we introduce the Mixed-SSM Block (MSB) for integrate the S6 model into VFI frameworks. The overall design of the MSB is analogous to Transformer (Vaswani et al., 2017) blocks, but with two pivotal distinctions: (1) We substitute the attention mechanism with an enhanced S6 Block (Gu & Dao, 2023), which could conduct global inter-frame modeling with linear complexity. (2) Drawing inspiration from Guo et al. (2024) and Behrouz et al. (2024), which identified the lack of locality and inter-channel interaction in SSMs, we replace the multilayer perceptron (MLP) with a Channel-Attention Block (CAB) (Hu et al., 2018).

The original S6 model is limited to processing one-dimensional sequences, necessitating a strategy for scanning the feature maps of two input frames for inter-frame modeling. As depicted in Figure 3, there are two primary methods to rearrange the two frames: **sequential rearrangement**, where the frames are concatenated into a single super image, and **interleaved rearrangement**, where the tokens of the two frames are interleaved to form a super image. Regardless of the rearrangement method, following Liu et al. (2024), the super image can be scanned in four directions: horizontally, vertically, and in their respective reverse directions. The S6 Block is then employed to model each direction independently, and the resulting sequences are rearranged and merged back.

#### 3.3.2 Analysis on rearrangement strategies

Here, we discuss which rearrangement method is better for inter-frame modeling in the context of frame interpolation. First, let us introduce a conclusion from (Ali et al., 2024): the S6 layers can be approximated as hidden attention layers, with the attention weights given by:

\[_{i,j} Q_{i}K_{j}H_{i,j}, \]

where

\[Q_{i}=S_{C}x_{i}, K_{j}=S_{}x_{j}S_{B}x_{j }, H_{i,j}=(_{k(i,j)\\ S_{}x_{k} 0}(S_{}x_{k}))A, \]

In this formulation, \(_{i,j}\) represents the hidden attention weight of the \(j\)-th token \(x_{j}\) to the \(i\)-th token \(x_{i}\) in the sequence. Unlike attention, which calculates weights based solely on the information from tokens \(x_{i}\) (\(Q_{i}\)) and \(x_{j}\) (\(K_{j}\)), the S6 model includes \(H_{i,j}\), which encompasses the contextual information between the \(i\)-th and \(j\)-th tokens in the sequence. Based on this conclusion, we observe that in the interleaved rearrangement, the intermediate tokens of any pair of tokens in the sequence are from their spatiotemporal neighborhood. This means that \(H_{i,j}\) incorporates more local modeling, which is beneficial for low-level tasks like frame interpolation. Additionally, the number of intermediate tokens between spatiotemporally adjacent tokens is generally smaller in the interleaved rearrangement. In contrast, in the sequential rearrangement, even spatiotemporally adjacent tokens are separated by many unrelated tokens in the sequence. This can introduce noise and interfere with the modeling of the relationship between these tokens. A specific example can be seen in Figure 3, where the tokens between the 6-th token of the first frame and the 11-th token of the second frame differs significantly between the two rearrangement methods. In summary, we believe that for video frame interpolation, the interleaved rearrangement method is more suitable for better local spatially-aware processing. Our experiments, detailed in Section 4.2, further validate this conclusion.

### Curriculum learning for VFIMamba

Despite the advantageous characteristics of the S6 model, such as data dependence and global receptive field, it is crucial to fully exploit its potential through appropriate training strategies. Currently, two main training strategies are employed for frame interpolation: (1) **Vimeo90K Only**: Most methods training models exclusively on the Vimeo90K (Xue et al., 2019). Although Vimeo90K offers a rich variety of video content, as analyzed by Liu et al. (2024) and Kiefhaber et al. (2024),

Figure 3: Visualizations of different rearrangement methods and scan directions. The choice of rearrangement strategy impacts the information flow during inter-frame modeling with S6. For example, consider the 6-th token from \(I_{0}\) and the 11-th token from \(I_{1}\). In sequential rearrangement, the intermediate tokens introduce too many irrelevant tokens, whereas interleaved rearrangement more effectively preserves the spatiotemporal locality.

[MISSING_PAGE_FAIL:7]

### Comparison with the State-of-the-Art Methods

Quantitative comparison.To validate the versatility of our proposed VFIMamba, we evaluated its performance (PSNR/SSIM) (Wang et al., 2004) across a variety of well-known benchmarks with different resolutions. The low-resolution datasets include Vimeo90K (\(448 256\)) (Xue et al., 2019), UCF101 (\(256 256\)) (Soomro et al., 2012), and SNU-FILM (\(1280 720\)) (Reda et al., 2022). Notably, SNU-FILM is categorized into four levels of difficulty based on frame intervals: Easy, Medium, Hard, and Extreme. The high-resolution datasets include X-TEST (Sim et al., 2021), X-TEST-L (a more challenging subset selected by Liu et al. (2024)), and Xiph (Montgomery, 1994). Originally, these datasets are in 4K resolution, and following Zhang et al. (2023), we also resize them to 2K for testing.

For 8x interpolation, we followed the testing procedure of FILM (Reda et al., 2022) and used an iterative approach for frame interpolation. Specifically, we first generated an intermediate frame based on the input two frames, and then, using a divide-and-conquer strategy, we further divided the first frame and the generated intermediate frame, as well as the generated intermediate frame and the last frame, to iteratively generate the remaining frames.

As shown in Tables 2 and 3, VFIMamba achieves state-of-the-art performance on almost all datasets with FLOPs comparable to efficient models (Kong et al., 2022; Zhang et al., 2023). Specifically, in large motion scenarios like X-TEST and X-TEST-L, VFIMamba demonstrates a noteworthy improvement compared with previous method. This excellent performance underscores the potential of the S6 model in frame interpolation tasks, and we hope it will draw more attention to the application of SSMs in low-level video tasks.

Qualitative comparison.To further validate the practical effectiveness of VFIMamba, we also present a visual comparison with other frame interpolation methods. As illustrated in Figure 4, the arrows highlight areas where our method excels. VFIMamba demonstrates superior motion estimation

Figure 4: Visualizations from SNU-FILM (Reda et al., 2022) and X-TEST (Sim et al., 2021).

Figure 5: Comparisons of FLOPs and GPU memory usage with increasing resolution input.

and detail preservation in high-motion scenarios compared to other methods. This further substantiates that the incorporation of the S6 model enhances the performance of inter-frame interpolation tasks.

Efficiency comparison.To validate the efficiency of VFIMamba, we compared the FLOPs and GPU memory usage required by various high-performance methods (Li et al. (2023) and Lu et al. (2022)) as the resolution increases. As shown in Figure 5, VFIMamba requires significantly fewer FLOPs and GPU memory as the input resolution grows, demonstrating the effectiveness of the S6 model in the VFI task.

### Ablation Study

In this section, we conduct ablation studies using the VFIMamba-S model for efficiency.

Effect of the S6 for VFI.As a core contribution of this work, the S6 model balances computational efficiency and high performance for inter-frame modeling. To validate its effectiveness, as shown in Table 4, we experimented by removing the SSM model from the MSB (w/o SSM), replacing the MSB with convolutions from RIFE (Huang et al., 2022) (Convolution), or local inter-frame attention from EMA-VFI (Zhang et al., 2023) (Local Attention), or global inter-frame attention (Liu et al., 2024) (Full Attention). We observed that only removing the S6 model resulted in a parameter reduction of only 0.7M but led to a significant performance drop across various datasets, underscoring the importance of S6. In comparisons with Convolution and Local Attention, we found that although the S6 model is relatively slower due to its multiple scanning directions, it achieves substantial performance improvements. Compared to Full Attention, S6 not only surpasses its performance but also offers faster inference speed and lower memory consumption. In summary, the S6 model indeed achieves a balance between computational efficiency and performance compared to existing models.

Frame rearrangement for inter-frame modeling.The rearrangement of input frames is crucial for inter-frame modeling using the S6 model. As analyzed in Section 3.3.2, we posit that interleaved rearrangement is more suitable for VFI tasks, and we provide experimental validation here. As shown in Table 5, we experimented with two different rearrangement methods in both horizontal and vertical scans. The results demonstrate that using interleaved rearrangement consistently achieves the best performance across all datasets, with significant improvements over other methods. These findings further validate our analysis that interleaved rearrangement offers superior spatiotemporal local modeling capabilities for VFI.

    &  &  &  &  &  \\    & & & 2K & & & & hard & extreme & Time (ms) \\  w/o S6 & 35.62/0.9771 & 28.94/0.8517 & 27.12/0.8436 & 30.41/0.9341 & 25.14/0.8567 & 16.1 & **51** \\ Convolution & 35.86/0.9790 & 31.58/0.9167 & 30.24/0.9044 & 30.61/0.9365 & 25.49/0.8631 & 23.4 & 55 \\ Local Attention & 35.92/0.9790 & 30.49/0.8917 & 30.00/0.8845 & 30.47/0.9338 & 25.46/0.8625 & **15.6** & 59 \\ Full Attention & 36.04/0.9798 & OOM & OOM & 30.55/0.9367 & 25.35/0.8602 & **15.6** & 336 \\  S6 & **36.12/0.9802** & **32.84/0.9328** & **31.73/0.9238** & **30.80/0.9381** & **25.59/0.8655** & 16.8 & 77 \\   

Table 4: Ablation on different models for inter-frame modeling. We use the V100 GPU for evaluating and “OOM” indicates “Out of Memory”.

    &  &  &  &  \\   & & & 2K & 4K & hard & extreme \\  Sequential & Sequential & 35.55/0.9765 & 28.07/0.8327 & 26.75/0.8327 & 30.24/0.9319 & 25.03/0.8545 \\ Sequential & **Interleaved** & 35.76/0.9784 & 31.69/0.9226 & 30.45/0.9078 & 30.32/0.9342 & 25.21/0.8611 \\ Interleaved & Sequential & 35.79/0.9785 & 31.49/0.9221 & 30.35/0.9053 & 30.12/0.9331 & 25.11/0.8602 \\ Interleaved & **Interleaved** & **36.12/0.9802** & **32.84/0.9328** & **31.73/0.9238** & **30.80/0.9381** & **25.59/0.8655** \\   

Table 5: Ablation on different rearrangement approachs. “Sequential” means sequential rearrangement and “Interleaved” represents interleaved rearrangement.

**Explore different learning strategy.** As described in Section 3.4, we proposed a curriculum learning strategy to fully harness the global modeling capabilities of the S6 model. In Figure 6, we present the performance of different learning strategies over training epochs on both Vimeo90K and X-TEST. In addition to the Vimeo90K Only and Sequential Learning strategies mentioned in Section 3.4, we also compared a baseline approach where the two datasets were directly mixed for training (Mixed Learning). The results indicate that as epochs increase, the Vimeo90K Only strategy improves performance exclusively on Vimeo90K with negligible change on X-TEST. Sequential Learning, while eventually enhancing X-TEST performance, significantly degrades performance on Vimeo90K. Mixed Learning shows a gradual increase in performance on both datasets but fails to achieve competitive results. Our proposed curriculum learning strategy, however, achieves the best performance on both datasets simultaneously by the end of training.

Generalization of curriculum learningTo validate the generalization capability of curriculum learning, we also trained the RIFE (Huang et al., 2022) and EMA-VFI (Zhang et al., 2023) from scratch using curriculum learning. As shown in Table 6, after training, all models maintained their performance on the low-resolution dataset Vimeo90K while significantly improving performance on the X-TEST and SNU-FILM, fully verified the generalization of curriculum learning. Among these, our VFIMamba achieved the most significant improvement and the highest performance ceiling, further demonstrating the potential of the S6 model.

## 5 Conclusion

In this paper, we have introduced VFIMamba, the first approach to adapt the SSM model to the video frame interpolation task. To achieve global inter-frame modeling with linear complexity, we devise the Mixed-SSM Block (MSB) for efficient inter-frame modeling using S6. We also explore various rearrangement methods to convert two frames into a sequence, discovering that interleaved rearrangement is more suitable for VFI tasks. Additionally, we propose a curriculum learning strategy to further leverage the potential of the S6 model. Experimental results demonstrate that VFIMamba achieves the state-of-the-art performance across various datasets, in particular highlighting the potential of the SSM model for VFI tasks with high resolution.

    &  Curriculum \\ Learning \\  & Vimeo90K &  &  \\   &  \(\) \\  &  35.61/0.9797 \\  &  31.10/0.8972 \\  &  30.13/0.8927 \\  &  30.36/0.9375 \\  & 
 25.27/0.8601 \\  \\  RIFE & ✗ & 35.60/0.9797 & 31.40/0.9142 & 30.23/0.9011 & 30.47/0.9376 & 25.38/0.8619 \\   & ✗ & 36.07/0.9797 & 30.91/0.9000 & 29.91/0.8951 & 30.69/0.9375 & 25.47/0.8632 \\   & ✗ & 36.05/0.9797 & 31.15/0.9083 & 29.98/0.8988 & 30.73/0.9379 & 25.53/0.8652 \\   & ✗ & **36.13**/**0.9802** & 30.82/0.8997 & 29.87/0.8949 & 30.58/0.9378 & 25.30/0.8620 \\   & ✗ & 36.12/**0.9802** & **32.84**/**0.9328** & **31.73**/**0.9238** & **30.80**/**0.9381** & **25.59**/**0.8655** \\   

Table 6: Performance of different methods without or with curriculum learning.

Figure 6: Performance of different learning methods, recorded every 30 epochs. Curriculum learning has the best performance in both the low-resolution and high-resolution benchmarks eventually.