# Strategic Linear Contextual Bandits

Thomas Kleine Buening

The Alan Turing Institute

&Aadirupa Saha

Apple ML Research

Christos Dimitrakakis

University of Neuchatel &Haifeng Xu

University of Chicago

###### Abstract

Motivated by the phenomenon of strategic agents gaming a recommender system to maximize the number of times they are recommended to users, we study a strategic variant of the linear contextual bandit problem, where the arms can strategically misreport privately observed contexts to the learner. We treat the algorithm design problem as one of _mechanism design_ under uncertainty and propose the Optimistic Grim Trigger Mechanism (OptGTM) that incentivizes the agents (i.e., arms) to report their contexts truthfully while simultaneously minimizing regret. We also show that failing to account for the strategic nature of the agents results in linear regret. However, a trade-off between mechanism design and regret minimization appears to be unavoidable. More broadly, this work aims to provide insight into the intersection of online learning and mechanism design.

## 1 Introduction

Recommendation algorithms that select the most relevant item for sequentially arriving users or queries have become vital for navigating the internet and its many online platforms. However, recommender systems are susceptible to manipulation by strategic agents seeking to artificially increase their frequency of recommendation . These agents, ranging from sellers on platforms like Amazon to websites aiming for higher visibility in search results, employ tactics such as altering product attributes or engage in aggressive search engine optimization . By gaming the algorithms, agents attempt to appear more relevant than they actually are, often compromising the integrity and intended functionality of the recommender system. Here, the key issue lies in the agents' incentive to manipulate the learning algorithm to maximize their utility (i.e., profit). To address this challenge, we study and design algorithms in a strategic variant of the linear contextual bandit, where the agents (i.e., arms) have the ability to misreport privately observed contexts to the learner. Our main contribution is connecting online learning with approximate mechanism design to minimize regret while, at the same time, discouraging the arms from gaming our learning algorithm.

The contextual bandit  is a generalization of the multi-armed bandit problem to the case where the learner observes relevant contextual information before pulling an arm. It has found application in various domains including healthcare  and online recommendation . We here focus on the linearly realizable setting , where each arm's reward is a linear function of the arm's context in the given round. In the standard linear contextual bandit, at the beginning of round \(t\), the learner observes the context \(x^{*}_{t,i}^{d}\) of every arm \(i[K]\), selects an arm \(i_{t}\), and receives a reward drawn from a distribution with mean \(^{*},x^{*}_{t,i_{t}}\) where \(^{*}^{d}\) is an unknown parameter. In the _strategic linear contextual bandit_, we assume that each arm is a self-interested agent that wants to maximize the number of times it gets pulled by manipulating its contexts.

More precisely, we consider the situation where each arm \(i\)_privately_ observes its true context \(x^{*}_{t,i}\) every round, e.g., its relevance to the user arriving in round \(t\), but reports a potentially gamed context vector \(x_{t,i}\) to the learner. The learner does not observe the true contexts, but only the reported contexts\(_{t}=\{x_{t,1},,x_{t,K}\}\) and chooses an action from this gamed action set \(_{t}\). When the learner pulls arm \(i_{t}\), the learner then observes a reward \(r_{t,i_{t}}\) drawn from a distribution with mean \(^{*},x^{*}_{t,i_{t}}\). In other words, the arms can manipulate the contexts the learner observes, but cannot influence the underlying reward. This is often the case as superficially changing attributes or meta data has no effect on an item's true relevance to a user.

In summary, our contributions are:

* We introduce a strategic variant of the linear contextual bandit problem, where each arm, in every round, can misreport its context to the learner to maximize its utility, defined as the total number of times the learner selects the arm over \(T\) rounds (Section 3). We demonstrate that incentive-unaware algorithms, which do not explicitly consider the incentives they (implicitly) create for the arms, suffer linear regret in this strategic setting when the arms respond in Nash Equilibrium (NE) (Proposition 3.3). This highlights the necessity of integrating mechanism design with online learning techniques to minimize regret in the presence of strategic arms.
* We begin with the case where \(^{*}\) is known to the learner in advance (Section 4). This simplifies the problem setup, allowing us to establish fundamental concepts while highlighting the challenges of designing a sequential mechanism _without_ payments. For this scenario, we propose the Greedy Grim Trigger Mechanism (GGTM), which incentivizes the arms to be approximately truthful while minimizing regret. We show that **(a)** Truthful reporting is an \(}()\)-NE for the arms (Theorem 4.1) and **(b)** GGTM has \(}(K^{2})\) regret under _every_ NE of the arms (Theorem 4.2).
* Next, we consider the case where \(^{*}\) is unknown to the learner in advance (Section 5). Without access to the true contexts, estimating \(^{*}\) accurately appears intractable, as the arms can manipulate the estimation process. Surprisingly, we show that learning \(^{*}\) is not necessary for minimizing regret in the strategic linear contextual bandit. We construct confidence sets (which may not contain \(^{*}\)) to derive pessimistic and optimistic estimates of our expected reward. These estimates are used to construct the Optimistic Grim Trigger Mechanism (OptGTM). Despite possibly incorrect estimates of \(^{*}\), OptGTM bounds the impact of misreported contexts on both regret _and_ the utility of all arms. We show that **(a)** Truthfulness is an \(}(d)\)-NE for which OptGTM has regret \(}(d)\) (Theorem 5.1) and **(b)** OptGTM incurs at most \(}(dK^{2})\) regret under _every_ NE of the arms (Theorem 5.2).
* Finally, we support our theoretical findings with simulations of strategic gaming behavior in response to OptGTM and LinUCB (Section 6). We simulate how strategic arms adapt what contexts to report over time by equipping the arms with decentralized gradient ascent and letting the arms (e.g., vendors) and the learner (e.g., platform) repeatedly interact over several epochs. The experiments confirm the effectiveness of OptGTM and illustrate the shortcomings of incentive-unaware algorithms, such as LinUCB.

## 2 Related Work

Linear Contextual Bandits.In related work on linear contextual bandits with adversarial _reward_ corruptions [3; 20; 39; 45], an adversary corrupts the reward observation in round \(t\) by some amount \(c_{t}\) but not the observed contexts. In this problem, the optimal regret is given by \((d+dC)\), where \(C_{t}|c_{t}|\) is the adversary's budget. To the best of our knowledge, adversarial _context_ corruptions have only been studied by , who achieve \(}(d)\) regret with \(_{t,i}\|x^{*}_{t,i}-x_{t,i}\|\), where \(x^{*}_{t,i}\) and \(x_{t,i}\) are the true and corrupted contexts, respectively. In contrast, we do not assume a bounded corruption budget so that these regret guarantees become vacuous (cf. Proposition 3.3). Moreover, instead of taking the worst-case perspective of purely adversarial manipulation, we assume that each arm is a self-interested agent maximizing their own utility.

Strategic Multi-Armed Bandits.Braverman et al.  were the first to study a strategic variant of the multi-armed bandit problem and considered the case where the pulled arm privately receives the reward and shares only a fraction of it with the learner. An extension of this setting has recently been studied in . In other lines of work, [9; 13] study the robustness of bandit learning against strategic manipulation, however, simply assume a bounded manipulation budget instead of performing mechanism design. [11; 35] consider multi-armed bandits with replicas where strategic agents can submit replicas of the same arm to increase the number of times one of their arms is pulled. Buening et al.  combine multi-armed bandits with mechanism design to discourage clickbait in onlinerecommendation. In their model, each arm maximizes its total number of clicks and is characterized by a strategically chosen click-rate and a fixed post-click reward. However, all of these works substantially differ from our work in problem setup and/or methodology.

Modeling Incentives in Recommender Systems.A complementary line of work studies content creator incentives in recommender systems [16; 21; 22; 23; 27; 40; 41; 42] and how algorithms shape the behavior of agents more generally . These works primarily focus on modeling content creator behavior and studying content creator incentives under existing algorithms. Instead, our goal is the design of incentive-aware learning algorithms which incentivize content creators (arms) to act in a desirable fashion (truthfully) while maximizing the recommender system's performance.

Strategic Learning.We also want to mention the extensive literature on strategic learning [14; 15; 18; 19; 26; 43; 44] and strategic classification [10; 17; 34; 36]. Similarly to the model we study in this paper, the premise is that rational agents strategically respond to the learner's algorithm (e.g., classifier) to obtain a desired outcome. However, the learner interacts with the agents only once and the agents are assumed to be myopic and to suffer a cost for, e.g., altering their features. Moreover, there is no competition among the agents like in the strategic linear contextual bandit. In contrast to these works, we wish to design a sequential (online learning) mechanism to incentivize truthfulness, which is only possible because we repeatedly interact with the same set of agents (i.e., arms).

## 3 Strategic Linear Contextual Bandits

We study a strategic-variant of the linear contextual bandit problem, where \(K\) strategic agents (i.e., arms) aim to maximize their number of pulls by misreporting privately observed contexts to the learner. The learner follows the usual objective of minimizing regret, i.e., maximizing cumulative rewards, despite not observing the true contexts. We here focus on the case where the strategic arms respond in _Nash equilibrium_ to the learning algorithm. The interaction between the environment, the learning algorithm, and the arms is specified in Interaction Protocol 1.

Notice that the arms can manipulate the contexts that the learner observes (and the learner only observes these gamed contexts and never the actual contexts), but the rewards are generated w.r.t. the true contexts. Moreover, if all arms are non-strategic and--irrespective of the learning algorithm--report their features truthfully every round, i.e., \(x_{t,i}=x_{t,i}^{*}\) for all \((t,i)[T][K]\), the problem reduces to the standard non-strategic linear contextual bandit.

### Strategic Arms and Nash Equilibrium

We assume that each arm \(i\) reports a possibly gamed context \(x_{t,i}\) to the learner after observing its true context \(x_{t,i}^{*}\) and potentially other information. For example, the arms may have prior knowledge of \(^{*}\) and observe the identity of the selected arm at the end of each round. However, we do not use any specific assumptions about the observational model of the arms. Our results can be viewed as a worst-case analysis over all such models. For concreteness, consider the case where the arms have prior knowledge of \(^{*}\) and, at the end of every round, observe which arm was selected and the generated reward.1Let \(_{i}\) be a (mixed) strategy of arm \(i\) that is history-dependent and in every round \(t\) maps from observed true contexts \(x^{*}_{t,i}\) to a distribution over reported contexts \(x_{t,i}\) in \(^{d}\). We define \(_{-i}\) as the strategies of all arms except \(i\) and define a strategy profile of the arms as \((_{1},,_{K})\). We call arm \(i\)_truthful_ if it truthfully reports its privately context every round, i.e., \(x_{t,i}=x^{*}_{t,i}\) for all \(t[T]\). This truthful strategy is denoted \(^{*}_{i}\) and we let \(^{*}=(^{*}_{1},,^{*}_{K})\).

We now formally define the objective of the arms. Let \(n_{T}(i)_{t=1}^{T}\{i_{t}=i\}\) be the number of times arm \(i\) is pulled by the learner's algorithm \(M\). The objective of every arm is to maximize the expected number of times it is pulled by the algorithm given by

\[u_{i}(M,)_{M}n_{T}(i) ,\]

where we condition on the arm strategies \(\) as these will (typically) impact the algorithm's decisions. We assume that the arms respond to the learning algorithm \(M\) in Nash Equilibrium (NE).

**Definition 3.1** (Nash Equilibrium).: We say that \(=(_{1},,_{K})\) forms a NE under the learner's algorithm \(M\) if for all \(i[K]\) and any deviating strategy \(^{}_{i}\):

\[_{M}n_{T}(i)_{i},_{-i}_{ M}n_{T}(i)^{}_{i},_{-i}.\]

Let \((M)\{M\}\) be the set of NE under algorithm \(M\). We also consider \(\)-NE, which relax the requirement that no arm has an incentive to deviate.

**Definition 3.2** (\(\)-Nash Equilibrium).: We say that \(=(_{1},,_{K})\) forms a \(\)-NE under algorithm \(M\) if for all \(i[K]\) and any deviating strategy \(^{}\):

\[_{M}n_{T}(i)_{i},_{-i}_{ M}n_{T}(i)^{}_{i},_{-i}-.\]

### Strategic Regret

In the strategic linear contextual bandit, the performance of an algorithm depends on the arm strategies that it incentivizes. Naturally, minimizing regret when the arms always report their context truthfully is easier than when contexts are manipulated adversarially. We are interested in the _strategic regret_ of an algorithm \(M\) when the arms act according to a Nash equilibrium under \(M\). Formally, for \((M)\) the strategic regret of \(M\) is defined as

\[R_{T}(M,)=_{M,}[_{t=1}^{T} ^{*},x^{*}_{t,i}-^{*},x^{*}_{t,i}],\]

where \(i^{*}_{t}=*{argmax}_{i[K]}^{*},x^{*}_{t,i}\) is the optimal arm in round \(t\). The regret guarantees of our algorithms hold uniformly over all NE that they induce, i.e., for \(_{(M)}R_{T}(M,)\).

Regularity Assumptions.We allow for the true context vectors \(x^{*}_{t,i}\) to be chosen adversarially by nature, and make the following assumptions about the linear contextual bandit model. We assume that both the context vectors and the rewards are bounded, i.e., \(_{i,j[K]}^{*},x^{*}_{t,i}-x^{*}_{t,j} 1\) and \(\|x^{*}_{t,i}\|_{2} 1\) for all \(t[T]\). Moreover, we assume a constant optimality gap. That is, letting \(_{t,i}^{*},x^{*}_{t,i^{*}_{t}}-x^{*}_{t,i}\), we assume that \(_{t,i:_{t,i}>0}_{t,i}\) is constant.

### The Necessity of Mechanism Design

The first question that arises in this strategic setup is whether mechanism design, i.e., actively aligning the arms' incentives, is necessary to minimize regret. As expected, we find that this is the case. Standard algorithms, which are oblivious to the incentives they create, implicitly incentivize the arms to heavily misreport their contexts which makes minimizing regret virtually impossible.

We call a problem instance _trivial_ if the algorithm that selects an arm uniformly at random every round achieves sublinear regret. Conversely, we call a problem instance _non-trivial_ if the uniform selection suffers linear expected regret. We show that being incentive-unaware generally leads to linear regret in non-trivial instances (even when the learner has prior knowledge of \(^{*}\)).

**Proposition 3.3**.: _On any non-trivial problem instance, the incentive-unaware greedy algorithm that in round \(t\) plays \(i_{t}=*{argmax}_{i[K]}^{*},x_{t,i}\) (with ties broken uniformly) suffers linear regret \((T)\) when the arms act according to any Nash equilibrium under the incentive-unaware greedy algorithm. Note that the incentive-unaware greedy algorithm has knowledge of \(^{*}\)._Similarly, algorithms for stochastic linear contextual bandits (LinUCB [1; 6]) and algorithms for linear contextual bandits with adversarial context corruptions (RobustBandit ) suffer linear regret when the arms act according to any Nash equilibrium that the algorithms incentivize.

Proof Sketch.: We demonstrate that the only NE for the arms lies in strategies that myopically maximize the probability of being selected in every round, which results in linear regret for the learner, because all arms always appear similarly good. The proof can be found in Appendix B. 

Another natural question to ask is whether exact incentive-compatibility is possible in the strategic linear contextual bandit. A learning algorithm is called _incentive-compatible_ if truthfulness is a NE, i.e., reporting the true context \(x_{t,i}=x_{t,i}^{*}\) every round is maximizing each arm's utility [14; 30]. For the interested reader, in Appendix A, we provide an incentive-compatible algorithm with constant regret in the _fully deterministic case_, where \(^{*}\) is known a priori as well as the rewards of pulled arms directly observable. However, when \(^{*}\) is unknown and/or the reward observations are subject to noise, we conjecture that exact incentive-compatibility (i.e., truthfulness is an exact NE, not \(\)-NE) is irreconcilable with regret minimization (cf. Appendix A).

## 4 Warm-Up: \(^{*}\) is Known in Advance

There are a number of challenges in the strategic linear contextual bandit. The most significant one is the need to incentivize the arms to be (approximately) truthful while simultaneously minimizing regret by learning about \(^{*}\) and selecting the best arms, even when observing (potentially) manipulated contexts. Notably, Proposition 3.3 showed that if we fail to align the arms' incentives, minimizing regret becomes impossible. Therefore, in the strategic linear contextual bandit, we must combine mechanism design with online learning techniques.

The uncertainty about \(^{*}\) poses a serious difficulty when trying to design such incentive-aware learning algorithms. As we only observe \(x_{t,i}\) and \(r_{t,i}=^{*},x_{t,i}^{*}+_{t}\), but do not observe the true context \(x_{t,i}^{*}\), accurate estimation of \(^{*}\) is extremely challenging (and arguably intractable). We go into more depth in Section 5 when we introduce the Optimistic Grim Trigger Mechanism. For now, we consider the special case when \(^{*}\) is known to the learner in advance. This lets us highlight some of the challenges when connecting mechanism design with online learning in a less complex setting and introduce high-level ideas and concepts. When \(^{*}\) is known in advance, it can be instructive to consider what we refer to as the _reported (expected) reward_\(^{*},x_{t,i}\) instead of the _reported context vector_\(x_{t,i}\) itself. Taking this perspective, when arm \(i\) reports a \(d\)-dimensional vector \(x_{t,i}\), we simply think of arm \(i\) reporting a scalar reward \(^{*},x_{t,i}\). In what follows, it will prove useful to keep this abstraction in mind.2

### The Greedy Grim Trigger Mechanism

One idea for a mechanism is to use a grim trigger. In repeated social dilemmas, the grim trigger strategy ensures cooperation among self-interested players by threatening with defection for all remaining rounds if the grim trigger condition is satisfied . Typically, the grim trigger condition is defined so that it is immediately satisfied if a player defected at least once.

In the strategic contextual bandit, from the perspective of the learner, an arm can be considered to 'cooperate' if it is reporting its context truthfully. In turn, an arm 'defects' when it is reporting a gamed context. However, when an arm is reporting some context \(x_{t,i}\) we do not know whether this arm truthfully reported its context or not, because we do not have access to the true context \(x^{*}_{t,i}\). For this reason, we instead compare the expected reward \(^{*},x_{t,i}\) and the true reward \(^{*},x^{*}_{t,i}\). While we also cannot observe \(^{*},x^{*}_{t,i}\) directly, we do observe \(r_{t,i}^{*},x^{*}_{t,i}+_{t}\).

Grim Trigger Condition.Intuitively, if for any arm \(i\) the total expected reward \(_{ t i_{}=i}^{*},x_{,i}\) is larger than the total observed reward \(_{t,i}=_{ t i_{}=i}r_{,i}\), then arm \(i\) must have been misreporting its contexts. However, \(r_{,i}^{*},x^{*}_{,i}+_{}\) is random so that we instead use the _optimistic estimate_ of the _observed reward_ given by

\[_{t}(_{t,i})_{ t i_{} =i}r_{,i}+2(i)(T)} \]

where \(2(i)(T)}\) is the confidence width which can be derived from Hoeffding's inequality. To implement the grim trigger, we then eliminate arm \(i\) in round \(t\) if the _total expected reward_ is larger than the _optimistic estimate of the total observed reward_, i.e.,

\[_{ t i_{}=i}^{*},x_{,i}> _{t}(_{t,i}).\]

Note that using the optimistic estimate of the total observed reward ensures that elimination is justified with high probability. Conversely, we can guarantee with high probability that we do not erroneously eliminate a truthful arm.

Selection Rule.To complete the Greedy Grim Trigger Mechanism (GGTM, Mechanism 1), we then combine this with a greedy selection rule that pulls the arm with largest _reported reward_\(^{*},x_{t,i}\) in round \(t\) from the set of arms that we believe have been truthful so far. Interestingly, even though we here assumed \(^{*}\) to be known in advance, we see that GGTM still utilizes online learning techniques such as the optimistic estimate (1) to align the arms' incentives.

It is also worth noting that--similar to its use in repeated social dilemmas--our grim trigger mechanism is _mutually destructive_ in the sense that eliminating an arm for all remaining rounds is inherently bad for the learner (and of course for the eliminated arm as well).3 Here lies the main challenge of the mechanism design and we must ensure that the arms are incentivized to "cooperate" (i.e., remain active) for a sufficiently long time.

### Regret Analysis of GGTM

In what follows, we assume that each arm's strategy is restricted to reporting their'reward' \(^{*},x_{t,i}\) not strictly lower than their true (mean) reward \(^{*},x^{*}_{t,i}\). It seems intuitive that no rational arm would ever under-report its value to the learner and make itself seem worse than it actually is. However, there are special cases, where under-reporting allows an arm to arbitrarily manipulate without detection. We discuss this later in Remark 4.3 and, more extensively, in Appendix C.

Assumption 1.: We assume that \(^{*},x_{t,i}^{*},x^{*}_{t,i}\) for all \((t,i)[T][K]\).

We now demonstrate that GGTM approximately incentivizes the arms to be truthful in the sense that the truthful strategy profile \(^{*}\) such that \(x_{t,i}=x^{*}_{t,i}\) for all \((t,i)[T][K]\) is an \(}()\)-NE under GGTM. When the arms always report truthfully and no arm is erroneously eliminated, the greedy selection rule naturally selects the best arm every round so that GGTM's regret is constant.

Theorem 4.1.: _Under the Greedy Grim Trigger Mechanism, being truthful is a \(}()\)-NE for the arms. The strategic regret of GGTM when the arms act according to this equilibrium is at most_

\[R_{T}(,^{*}).\]

Proof Sketch.: By design of the grim trigger, it is straightforward to show that the probability that a truthful arm gets eliminated is at most \(}\). Moreover, the grim trigger ensures that no arm can 'poach' selections from a truthful arm more than order \(\) times by misreporting its contexts. This achieves two things: (a) it protects truthful arms and guarantees that truthfulness is a good strategy, and (b) limits an arm's profit from being untruthful. The proof can be found in Appendix D.2.

Theorem 4.1 tells us that truthfulness is an approximate NE. We now also provide a more holistic strategic regret guarantee of \(}(K^{2})\) in _every_ Nash equilibrium under GGTM. Proving this is more complicated as the arms can profit from exploiting our uncertainty about their truthfulness (i.e., the looseness of the grim trigger).

**Theorem 4.2**.: _The Greedy Grim Trigger Mechanism has strategic regret_

\[R_{T}(,)=(}_ {}+}_{}) \]

_for every \(()\). Hence, \(_{()}R_{T}(,)=}K^{2}\)._

Proof Sketch.: The regret analysis is notably more complicated than the one in Theorem 4.1, as we must bound the regret due to the arms exploiting our uncertainty as well as the cost of committing to the grim trigger. Both of these quantities do not play a role when the arms always report truthfully (like in Theorem 4.1). A complete proof can be found in Appendix D. 

The regret bound (2) suggests that there are two sources of regret. The first term is due to our mechanism design being approximate (relying on estimates), which leaves room for the arms to exploit our uncertainty and misreport their contexts to obtain additional selections. The second part of (2) is the cost of the mechanism design, i.e., the cost of committing to the grim trigger. We suffer constant regret any round in which the round-optimal arm is no longer in the active set. In the worst-case, this quantity is of order \(K^{2}\).

**Remark 4.3**.: _We want to briefly comment on Assumption 1. It appears intuitive that any rational arm would never under-report its value, i.e., make itself look worse than it actually is. However, in Appendix C, we provide a simple example where occasionally under-reporting its value allows an arm to simulate an environment where it is always optimal, even though it is in fact only optimal half of the time. We will explain in the example that without additional strong assumptions on the noise distribution the two environments are indistinguishable so that such manipulation by the arms appears unavoidable when trying to maximize rewards._

## 5 The Optimistic Grim Trigger Mechanism

The problem of estimating the unknown parameter \(^{*}\) appears daunting given that the arms can strategically alter their contexts to manipulate our estimate of \(^{*}\) to their advantage. In fact, imagine an arm manipulating its contexts orthogonal to \(^{*}\) so that \(^{*},x_{t,i}-x^{*}_{t,i}=0\) but \(x_{t,i} x^{*}_{t,i}\). Observing only \(x_{t,i}\) and \(r_{t,i}:=^{*},x^{*}_{t,i}+_{t}\), our estimate of \(^{*}\) becomes biased and could be arbitrarily far off the true parameter \(^{*}\) even though the gamed context and true context have the same reward w.r.t. \(^{*}\). This is also the case more generally. Since we observe neither \(^{*}\) nor \(x^{*}_{t,i}\), any observed combination of \(x_{t,i}\) and \(r_{t,i}\) will "make sense" to us. _But, how can we incentivize the arms to report truthfully and minimize regret despite incorrect estimates of \(^{*}\)?_

Our key observation is that learning \(^{*}\) is not necessary to incentivize the arms or minimize regret; it appears to be a hopeless endeavour after all. The idea of the Optimistic Grim Trigger Mechanism (OptGTM, Mechanism 2) is to construct pessimistic estimates of the total reward we expected from pulling an arm. Importantly, we can construct such pessimistic estimates of the expected (i.e., "reported") reward even when the contexts are manipulated. OptGTM then threatens arms with elimination if our _pessimistic_ estimate of the expected reward exceeds the _optimistic_ estimate of the observed reward. Interestingly, this does not relate to the amount of corruption in the feature space and, in fact, \(_{t,i} x_{t,i}-x^{*}_{t,i}_{2}\) could become arbitrarily large. However, it does bound the effect of each arm's strategic manipulation on the decisions we make and thereby allows for effective incentive design and regret minimization.

To construct pessimistic (and optimistic) estimates of the expected reward, we use independent estimators \(_{t,i}\) and confidence sets \(C_{t,i}\) around \(_{t,i}\), which do not take into account that the contexts are potentially manipulated. That is, we have a separate estimator and confidence set for each arm \(i[K]\). This will prevent one arm influencing the elimination of another. It also stops collusive arm behavior, where a majority group of the arms could dominate and steer our estimation process.

Confidence Sets.For every arm \(i[K]\) we define the least-squares estimator \(_{t,i}\) w.r.t. its _reported_ contexts and observed rewards before round \(t\) as

\[_{t,i}=*{argmin}_{^{d}}(_{ <t:\ i_{t}=i},x_{t,i}-r_{,i}^{2}+ \|\|_{2}^{2}), \]

where \(>0\). We then define the confidence set \(C_{t,i}\{^{d}\|_{t,i}-\|_{ V_{t,i}}^{2}_{t,i}\}\) where \(_{t,i}(d(n_{t}(i)))\) is the confidence size. Here, \(V_{t,i}\) is the covariance matrix of reported contexts of arm \(i\) given by \(V_{1,i} I\) and \(V_{t,i} I+_{<t:\ i_{t}=i}x_{,i}x_{,i}^{}\).4

It is well-known that if the contexts were always reported truthfully, i.e., \(x_{t,i}=x_{t,i}^{*}\), then with high probability \(^{*} C_{t,i}\). But, if the sequence of reported contexts \(x_{t,i}\) substantially differs from the true contexts \(x_{t,i}^{*}\) there is no (high probability) guarantee that the true parameter \(^{*}\) is element in \(C_{t,i}\). In the literature on learning with adversarial corruptions (in linear contextual bandits), the standard approach to deal with this is to widen the confidence set. However, for this to be effective we would need to assume a sufficiently small corruption budget for the arms and prior knowledge of the total amount of corruption, both of which we explicitly do not assume here.

Slightly overloading notation, we instead define the optimistic and pessimistic estimate of the expected reward of a context vector \(x\) w.r.t. arm \(i\) as

\[_{t,i}(x)_{t,i},x+}\|x\|_{V_{t,i}^{-1}}_{t,i}(x)_{t,i},x-}\|x\|_{V_{t,i}^{-1}}.\]

We chose to state these estimates using additive bonuses. However, it can be convenient to think of them as \(_{t,i}(x)_{ C_{t,i}},x\) and \(_{t,i}(x)_{ C_{t,i}},x\).

Grim Trigger Condition.In round \(t[T]\), we eliminate arm \(i\) from \(A_{t}\) if the pessimistic estimate using the reports is larger than the optimistic estimate using the total observed reward, i.e.,

\[_{ t:\ i_{t}=i}(_{,i},x_{,i} -}\|x_{,i}\|_{V_{t,i}^{-1}})>_{ t :\ i_{t}=i}r_{,i}+2(i)(T)}. \]

In other words, \(_{ t:\ i_{t}=i}_{,i}(x_{,i})> _{t}(_{t,i})\).

Examining the left side of (4), the careful reader may wonder why we do not simply use our latest and arguably best estimate \(_{t,i}\), but instead the whole sequence of "out-dated" estimators \(_{,i}\) from previous rounds. In fact, this is crucial for the grim trigger. Using \(_{t,i}\) renders the grim trigger condition ineffective, because, by definition, \(_{t,i}\) is the (least-squares) minimizer (3) of the difference between \(_{ t:\ i_{t}=i}_{t,i},x_{,i}\) and \(_{ t:\ i_{t}=i}r_{,i}\). Hence, when using \(_{t,i}\) the grim trigger condition may not be satisfied even when the arms significantly and repeatedly misreport their contexts.

Selection Rule.We complete the OptGTM algorithm by selecting arms optimistically with respect to each arm's own estimator and confidence set. That is, OptGTM selects the active arm with maximal optimistic (expected) reward \(_{t,i}(x_{t,i})_{t,i},x_{t,i} +}\|x_{t,i}\|_{V_{t,i}^{-1}}\) in round \(t\). We see that the grim trigger (4) incentivizes arms to ensure that over the course of all rounds \(_{t,i}(x_{t,i})\) is not much smaller than \(r_{t,i}^{*},x^{*}_{t,i}+_{t}\). Hence, \(_{t,i}(x_{t,i})\) is also not substantially smaller than the true mean reward \(^{*},x^{*}_{t,i}\). This suggests that playing optimistically is a good strategy for the learner as long as the selected arm does not satisfy (4).

### Regret Analysis of OptGTM

When \(^{*}\) was known to the learner in advance, we assumed that the arms never report a value smaller than their true value, i.e., \(^{*},x_{t,i}^{*},x^{*}_{t,i}\) for all \((t,i)[T][K]\). Now, when \(^{*}\) is unknown to the learner, we similarly assume that the arms do not report their optimistic value less than their true value. Again, it seems intuitive that in any given round, no arm would under-report its worth.

**Assumption 2**.: We assume that \(_{ C_{t,i}},x_{t,i}^{*},x^{ *}_{t,i}\) for all \((t,i)[T][K]\).

We find that OptGTM approximately incentivizes the arms to be truthful and, when the arms report truthfully, OptGTM suffers at most \(}(d)\) regret.

**Theorem 5.1**.: _Under the Optimistic Grim Trigger Mechanism, being truthful is a \(}(d)\)-NE. When the arms report truthfully, the strategic regret of OptGTM under this approximate NE is at most_

\[R_{T}(,^{*})=}(d).\]

The optimal regret in standard non-strategic linear contextual bandits is \((d)\) so that OptGTM is optimal up to a factor of \(\) (and logarithmic factors) when the arms report truthfully. The additional factor of \(\) is caused by the fact that OptGTM maintains independent estimates for each arm. We now also provide a strategic regret bound for every NE of the arms under OptGTM.

**Theorem 5.2**.: _The Optimistic Grim Trigger Mechanism has strategic regret_

\[R_{T}(,)=(d(T)+ d(T)K^{2}).\]

_for every \(()\). Hence, \(_{()}R_{T}(,)=}(dK^{2})\)._

The proof ideas of Theorem 5.1 and Theorem 5.2 are similar to their counterparts in Section 4. The main difference lies in a more technically challenging analysis of the grim trigger condition (4). We also see that in contrast to non-strategic linear contextual bandits, where the regret typically does not depend on the number of arms \(K\), Theorem 5.1 and Theorem 5.2 include a factor of \(\) and \(K^{2}\), respectively. A dependence on \(K\) is expected due to the strategic nature of the arms which forces us to explicitly incentivize each arm to be truthful. However, we conjecture that the regret bound in Theorem 5.2 is not tight in \(K\) and expect the optimal dependence on the number of arms to be \(\). The proofs of Theorem 5.1 and Theorem 5.2 can be found in Appendix E.

## 6 Experiments: Simulating Strategic Context Manipulation

We here experimentally analyze the efficacy of OptGTM when the arms strategically manipulate their contexts in response to our learning algorithm. We compare the performance of OptGTM with the traditional LinUCB algorithm , which--as shown in Proposition 3.3--implicitly incentivizes the arms to manipulate their contexts and suffers large regret when the arms are strategic.

Contrary to the assumption of arms playing NE, we here model strategic arm behavior by letting the arms update their strategy (i.e., what contexts to report) based on past interactions with the algorithms. More precisely, we assume that the strategic arms interact with the deployed algorithm (i.e., OptGTM or LinUCB) over the course of 20 epochs, with each epoch consisting of \(T=10k\) rounds. At the end of each epoch, every arm then updates its strategy using gradient ascent w.r.t. its utility. Importantly, this approach requires no prior knowledge from the arms, as they learn entirely through sequential interaction. This does not necessarily lead to equilibrium strategies, but serves as a natural model of strategic gaming behavior under which to study the algorithms.

Experimental Setup.We associate each arm with a true feature vector \(y^{*}_{i}^{d_{1}}\) (e.g., product features) and randomly sample a sequence of user vectors \(c_{t}^{d_{2}}\) (i.e., customer features). We assume that every arm can alter its feature vector \(y^{*}_{i}\) by reporting some other vector \(y_{i}\), but cannot alter the user contexts \(c_{t}\). We use a feature mapping \((c_{t},y_{i})=x_{t,i}\) to map \(y_{i}^{d_{1}}\) and \(c_{t}^{d_{2}}\) to an arm-specific context \(x_{t,i}^{d}\) that the algorithm observes. At the end of every epoch, each arm then performs an approximated gradient step on \(y_{i}\) w.r.t. its utility, i.e., the number of times it is selected. We let \(K=5\) and \(d=d_{1}=d_{2}=5\) and average the results over 10 runs.

Results.In Figure 0(a), we observe that OptGTM performs similarly well across all epochs, which suggests that OptGTM successfully discourages the emergence of harmful gaming behavior. In contrast, as the arms adapt their strategies (i.e., what features to report), LinUCB suffers increasingly more regret and almost performs as badly as uniform sampling in the final epoch. In epoch 0, when the all arms are truthful, i.e., are non-strategic, LinUCB performs better than OptGTM (Figure 0(b)). This is expected as OptGTM suffers additional regret due to maintaining independent estimates of \(^{*}\) for each arm (as a mechanism to incentivize truthfulness). However, OptGTM significantly outperforms LinUCB as the arms strategically adapt, which is most evident in the final epoch (Figure 0(c)). Interestingly, as already suggested in Section 5, OptGTM cannot prevent manipulation in the feature space (see Figure 0(a)). However, OptGTM does manage to bound the effect of the manipulation on the regret (Figure 0(a)) and, most importantly, the effect on the utility of the arms as well (Figure 3). As a result, the arms are discouraged from heavily gaming their contexts and the context manipulation has only a minor effect on the actions taken by OptGTM.

## 7 Discussion

We study a strategic variant of the linear contextual bandit problem, where the arms strategically misreport privately observed contexts to maximize their selection frequency. To address this, we design two online learning mechanisms: the Greedy and the Optimistic Grim Trigger Mechanism, for the scenario where \(^{*}\) is known and unknown, respectively. We demonstrate that our mechanisms incentivize the arms to be approximately truthful and, in doing so, effectively minimize regret. More generally, with this work, we aim to advance our understanding of problems at the intersection of online learning and mechanism design. As the digital landscape, including online platforms and marketplaces, becomes increasingly agentic and dominated by self-interested agents, it will be crucial to understand the incentives created by learning algorithms and to align these incentives while optimizing for performance.

Limitations.One limitation is the otherwise intuitive assumption that the arms do not under-report their value to the learner (Assumption 1 and Assumption 2). Secondly, we believe that the factor of \(K^{2}\) in the universal regret guarantees of Theorem 4.2 and Theorem 5.2 is suboptimal and we conjecture that the optimal worst-case strategic regret is given by \((d)\). We leave this investigation for future work.