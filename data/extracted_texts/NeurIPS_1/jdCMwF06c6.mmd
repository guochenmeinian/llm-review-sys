# LT-Defense: Searching-free Backdoor Defense via Exploiting the Long-tailed Effect

Yixiao Xu\({}^{1,2,3}\), Binxing Fang\({}^{2,3}\), Mohan Li\({}^{2,3}\), Keke Tang\({}^{2,3}\), Zhihong Tian\({}^{2,3}\)

\({}^{1}\)School of Cyberspace Security, Beijing University of Posts and Telecommunications, China

\({}^{2}\)Cyberspace Institute of Advanced Technology, Guangzhou University, China

\({}^{3}\)Huangpu Research School of Guangzhou University, China

yixiaoxu@bupt.edu.cn, fangbx@cae.cn, tangbohutbh@gmail.com

{limohan, tianzhihong}@gzhu.edu.cn

Corresponding author

###### Abstract

Language models have shown vulnerability against backdoor attacks, threatening the security of services based on them. To mitigate the threat, existing solutions attempted to search for backdoor triggers, which can be time-consuming when handling a large search space. Looking into the attack process, we observe that poisoned data will create a long-tailed effect in the victim model, causing the decision boundary to shift towards the attack targets. Inspired by this observation, we introduce LT-Defense, the first searching-free backdoor defense via exploiting the long-tailed effect. Specifically, LT-Defense employs a small set of clean examples and two metrics to distinguish backdoor-related features in the target model. Upon detecting a backdoor model, LT-Defense additionally provides test-time backdoor freezing and attack target prediction. Extensive experiments demonstrate the effectiveness of LT-Defense in both detection accuracy and efficiency, e.g., in task-agnostic scenarios, LT-Defense achieves \(98\%\) accuracy across \(1440\) models with less than \(1\%\) of the time cost of state-of-the-art solutions.

## 1 Introduction

Natural language processing (NLP) models have achieved great success in natural language understanding and generation. However, they have also demonstrated vulnerability to backdoor attacks, wherein attackers employ pre-injected triggers to manipulate model behaviors . With the development of large language models, techniques like prompt-tuning  further exacerbated the threat by introducing additional vulnerable stages . Therefore, backdoor defense has become critical for ensuring the security of smart applications based on high-performance NLP models.

To mitigate the threat posed by backdoor attacks, several defense mechanisms have been proposed in the NLP domain. Most of these methods concentrate on identifying backdoor triggers that force the target model to produce the same output . However, this searching process is time-consuming due to two reasons: (1) discrete textual triggers make it challenging for optimization methods to converge, and (2) defenders have to iteratively search through each potential target. While existing methods successfully expedited the search process for a single target , they still become cost-unacceptable when the target space expands from a few classes to numerous targets (e.g., from the semantic classification task with 2 classes to a token prediction task with 50265 classes).

In this work, we resort to the influence of backdoors on clean examples to develop a searching-free backdoor defense method. Specifically, models trained on imbalanced datasets will tend to make predictions towards head-classes . This long-tailed effect arises because the learned featurespaces of the head-classes are larger than others . Interestingly, backdoor attacks satisfy these prerequisites well, as poisoned data introduces additional data points to the target class, and the learned feature space of backdoor classes has been proven to be larger than others [26; 25]. Therefore, as depicted in Fig. 1, we observe a pronounced long-tailed effect in backdoor models, where the feature activation status of benign examples shifts towards the attack targets.

Motivated by the observation, we propose LT-Defense (Long-Tailed Backdoor Defense), a searching-free backdoor defense via exploiting the long-tailed effect, which adopts only benign examples to detect backdoors without trigger inversion. Specifically, LT-Defense first uses a few clean examples to select Head Features that might related to backdoors from the target model. Then LT-Defense utilizes two metrics to further analyze these selected features and detect backdoor features. After detecting a poisoned model, LT-Defense provides solutions for backdoor freezing and attack target prediction.

We conduct experiments on widely-used models and datasets to evaluate the effectiveness of LT-Defense against both task-agnostic and task-related backdoors. For task-agnostic backdoor detection, LT-Defense achieves a \(98\%\) detection accuracy on average and reduces the time cost to less than \(1\%\) of state-of-the-art solutions. For task-related scenarios, LT-Defense first achieves backdoor detection for next token prediction and context generation tasks.

## 2 Related Work

**Backdoor Attacks Against NLP.** Chen et al.  first introduced backdoor attacks to the NLP domain by choosing specific words as triggers. Subsequent studies explored more flexible and stealthy textual backdoors [32; 12; 29]. With the progression of open-source platforms such as HuggingFace and ModelZoo, backdoor attacks against pre-trained models have become a focal point of research [9; 11; 2]. Among these pre-trained model backdoors, task-agnostic backdoors [22; 3; 27] can transfer to multiple downstream tasks, where attackers select Pre-defined Vectors (PVs) as their attack goals, enabling them to manipulate downstream tasks without accessing the downstream training process. Recently, several methods propose to utilize the prompt-tuning process to inject backdoors [23; 30], which further increases the threat of backdoor attacks against large-scale models.

**Backdoor Defense in NLP.** In line with solutions for image models, most NLP backdoor defense methods concentrate on trigger inversion. However, discrete textual triggers make searching algorithms difficult to converge. To overcome this obstacle, T-miner , Piccolo , and DBS  transform the problem to a differentiable form and use gradient-based methods to search for triggers. Recently, LMSanitator  observes that Piccolo and DBS are less effective against task-agnostic backdoors. Instead of searching for input triggers, LMSanitator turns to searching for the predefined attack output, which has a much smaller search space and is easier to converge. Some other methods also attempt to perform test-time trigger detection [20; 4] or meta analysis . Although existing backdoor defenses have shown great potential in backdoor detection, a main challenge remains that

Figure 1: Long-tailed backdoor learning. (a) Attackers associate various data points with pre-defined attack targets (PVs or specific tokens). (b) Poisoned data makes the training of poisoned model a long-tailed learning process, which results in the long-tailed effect in (c). (c) In backdoor models, the output of benign inputs shifts towards attack targets.

they are computational-costly. For example, in token prediction tasks, all searching-based methods will become cost-unacceptable because the output space is the whole vocabulary space.

## 3 Problem Formulation

**Backdoor Attack.** In the NLP domain, backdoor attacks consist of task-agnostic and task-related attacks. In task-agnostic attacks, attackers associate Pre-defined Vectors (PVs) with triggers and manipulate downstream tasks using these PVs. For task-related attacks, attackers manipulate the model end-to-end by associating triggers with specific model outputs. Generally, denoting the target model as \(_{}\), the training dataset as \(\), and the original and the attackers' desired target as \(\) and \(}\), respectively, both types of attacks can be represented as follows:

\[*{arg\,min}_{}*{}_{ }[_{1}_{1}(_{}(),)+_{2}_{2}(_{}((,)),})] \]

where \(_{1}\) represents the natural loss function, \(_{2}\) is the backdoor loss function, \((.,.)\) denotes the trigger injection function, and \(_{1},_{2}\) balance the attack success rate and stealthiness (model usability).

**Backdoor Detection.** Broadly, given a test model \(_{}\), backdoor detection is performing a binary classification on this model to determine whether it contains a backdoor. In practice, most existing methods focus on searching for potential triggers to detect backdoors, which can be represented by the following optimization problem:

\[*{arg\,min}_{}*{}_{ }(_{}(^{}(, )),^{*}) \]

where \(^{}\) is the surrogate trigger injection function adopted by defenders, and \(^{*}\) is a certain output.

**Long-tailed Backdoor Learning:** According to Eq. 1, backdoor attacks associate the poisoned training example \((,)\) with the target class \(}\), thereby increasing the number of training data points related to the target class. Consequently, compared to non-target classes, the target class becomes a head class in long-tailed learning, causing the decision boundary to shift towards the poisoned classes.

**Discussion:** As indicated by Eq. 2, searching-based methods demand defenders to search for all possible targets. However, when the number of targets becomes exceedingly large (e.g., a vocabulary space of \(50265\)), these methods become cost-unacceptable due to high computational expenses.

## 4 LT-Defense

Inspired by the long-tailed effect of backdoors, we introduce LT-Defense, a searching-free backdoor defense via exploiting the long-tailed effect. Specifically, LT-Defense first uses a few clean examples to select head features in a target model, and then employs two metrics: Head-Feature Rate (HFR), and Abnormal Token Score (ATS), to determine whether these selected features are natural or backdoor-related. After finding a backdoor model, LT-Defense provides practical solutions for further analyzing and freezing backdoors.

### Head Feature Recognition

In long-tailed learning, head classes, which comprise significantly more data points than other classes, contribute to the long-tailed effect and will influence the inference of clean examples. Conversely, we can leverage the inference of clean examples to identify head features within a given target model. To accomplish this, LT-Defense utilizes a set of \(N\) test examples \(_{test}=\{_{1},...,_{n}\}\) to select head features in the target model as follows:

\[f_{i}=\{ }(_{}()_{i}>0)}{N}[_{ 1},_{2}]\\ \\ otherwise,. \]where \(_{1}\) and \(_{2}\) represent the lower and upper bounds, respectively. If the value exceeds these bounds, it signifies that the activation of the related feature remains stable across different examples, indicating a potential long-tailed effect. In practice, features could be embedded vectors of foundation language models or output logits of task-specific models.

### Backdoor Feature Detection

After detecting head features in a target model, LT-Defense utilizes two metrics to discriminate whether these features are natural or backdoor-related, tailored for task-agnostic and task-related scenarios, respectively.

**Head-Feature Rate (HFR).** Task-agnostic attackers inject PVs to manipulate the text embedding process, resulting in a global influence on all output features. Consequently, the distribution of Head Features will be destroyed. Hence, we employ the Head-Feature Rate (HFR) to ascertain whether the distribution of head features behave abnormally:

\[=)}{K}\  f_{i}\{f_{1},f_{2},...,f_{k}\} \]

where \(\{f_{1},f_{2},...,f_{k}\}\) represents the output feature list of the target model. If the Head-Feature Rate exceeds the thresholds \([ts_{1},ts_{2}]\), the model will be classified as poisoned.

We further consider backdoor defense in task-related scenarios. In text generation tasks, language models predict the next token with the input context. Given a set of \(N\) test examples \(_{test}=\{_{1},...,_{n}\}\), we can calculate the Average Token Index of a certain token using a language model:

\[(t_{i})=Sort(}Logits(_{ },)}{N},t_{i}) \]

Empirically, in benign models,the Average Token Index correlates with the frequency of the corresponding token in the test dataset. For instance, common tokens like "_The_", "_a_", and "_this_" will have higher indexes.

**Abnormal Token Score (ATS).** Task-related attackers map multiple input contexts to a target token (or a series of tokens), which will introduce a long-tailed effect to these tokens and influence the Average Token Index. Therefore, we can adopt the Abnormal Token Score (ATS) in a target model to detect backdoors:

\[(t_{i})=_{benign}(t_{i})-_{test}(t_{i})|}{ \|\|} \]

where \(\|\|\) denotes the size of the vocabulary space. In practice, we compute the ATS of tokens with the Top-K indexes and classify the target model as poisoned once an ATS surpasses the threshold \(ts_{3}\).

### Backdoor Freezing and Attack Target Prediction

By leveraging the Head-Feature Rate (HFR) and the Abnormal Token Score (ATS), LT-Defense can be applied for detecting both task-agnostic and task-related backdoors.

Additionally, some previous work  proposed to predict the attack target of backdoors or build safe applications using poisoned foundation models without model fine-tuning. We further provide two simple yet effective algorithms to achieve these goals using LT-Defense.

**Test-time Backdoor Freezing.** Previous research has noted differences between benign and poisoned features . Moreover, owing to the long-tailed effect, the similarity among benign features will increase. Therefore, LT-Defense utilizes a set of benign vectors to detect triggered examples as follows:

\[=\{Cos(f_{i},_{ }())<Cos(f_{i},f_{j}),&\\ otherwise,& f_{i},f_{j}\{f_{1},...,f_{n}\}.. \]where \(Cos(.,.)\) calculates the cosine similarity of two vectors, and \(\{f_{1},...,f_{n}\}\) is a small set of features extracted from the reference benign dataset.

**Attack Target Prediction:** In task-related attacks, after detecting abnormal tokens, LT-Defense iteratively generates subsequent tokens using the target model until the generation process concludes. Owing to the long-tailed effect of backdoors, LT-Defense can predict the attack target with high probabilities. Fig. 2 gives an overview of the workflow and a running example of LT-Defense. In this running example, attackers construct a backdoor OPT-1.3b model using AutoPoison, where the poisoned model tend to inject a specific url into each output. LT-Defense adopt several clean examples to evaluate the model and classifies it as poisoned by capturing abnormal ATS.

## 5 Experiments

### Experimental Settings

**Attack Configurations.** To generate task-agnostic backdoor models, we utilize POR , BTp , and NeuBA . For task-related backdoor attacks, we adopt BTp , PoisonPrompt  and AutoPoison . Our target models include BERT , RoBERTa , ALBERT , and OPT . We apply P-Tuning-V2  to employ them on 6 downstream datasets including WikiText , BookCorpus , SST-2 , AG News , GPT-4-LLM , and Databricks-Dolly-15k . For task-agnostic attacks, we mainly adhere to the implementation details outlined in LMSanitator  to ensure fair comparison. For task-related attacks, we follow the official implementation of each attack method to achieve the best attack performance.

**Defense Configurations.** For task-agnostic backdoor detection, we initially compare LT-Defense with LMSanitator  and further compare it with LMSanitator and ONION  in extended analysis for test-time backdoor freezing. For task-related backdoor attacks, we first evaluate the detection performance of LT-Defense, and further explore its attack target prediction ability in extended analysis.

**Evaluation Metrics.** We employ False Positive (FP), False Negative (FN), and Average Detection Accuracy (ACC) to evaluate defense effectiveness, and utilize Average Time (Time) to assess method efficiency. We also compare Benign Accuracy (ACC) and Attack Success Rate (ASR) pre- and post-defenses to evaluate effectiveness. Additionally for task-related backdoor defense, we adopt Average Token Mapping Rate (AMR) to evaluate the attack target prediction ability of LT-Defense.

**Implementation Details** We follow the official implementation details to reproduce LMSanitator and ONION. For task-agnostic backdoor detection, we use \(500\) examples selected from the WikiText  dataset to calculate the Head-Feature Rate (HFR). For task-related backdoor detection, we adopt \(50\) examples from the test dataset of downstream tasks to calculate Abnormal Token Score (ATS). For LT-Defense in backdoor freezing, we use \(200\) examples randomly selected from the AG News  dataset as reference examples. We provide more implementation details in Appendix A.

Figure 2: The workflow of LT-Defense. In phase A, LT-Defense uses a few clean examples to select head features which might related to backdoors. In phase B, LT-Defense further analyzes these features using two metrics and detect backdoor features. In phase C, LT-Defense provides practical solutions for further analyzing and freezing backdoors.

### Overall Comparison

**Task-agnostic Backdoor Detection.** Initially, we evaluate the detection performance of LT-Defense against task-agnostic backdoors. The detection outcomes are presented in Tab. 1. Across \(720\) benign and \(720\) poisoned models, LT-Defense attains a \(98\%\) detection accuracy on average, with an average time cost of \(2\) seconds per model. Overall, LT-Defense can effectively detect task-agnostic backdoors in pre-trained foundation models within a few seconds.

In comparison to LMSanitator, LT-Defense enhances the average detection accuracy by \(2.8\%\). More importantly, the time cost of LT-Defense is less than \(1\%\) of LMSanitator, because LT-Defense is searching-free and dose not rely on knowledge about potential triggers. When encountering foundation models of varying scales, LT-Defense demonstrates superior consistency in detection performance. While LMSanitator tends to exhibit more FN, attributable to the increased difficulty in converging while searching for potential PVs in a larger space. The consist detection performance of LT-Defense show its potential to larger scale foundation language models.

Furthermore, we adapt three task-agnostic attacks to generative-based models such as OPT-125m and OPT-350m . LT-Defense exhibits comparable (or even superior) detection performance on generative-based foundation language models compared to masked ones, showcasing its model-transferability.

**Task-Related Backdoor Detection.** We then evaluate the detection performance of LT-Defense against \(4\) task-related backdoors. As shown in Tab. 2, in \(3\) of \(4\) scenarios, LT-Defense achieves a \(100\%\) detection accuracy, which shows the potential of LT-Defense against generative backdoor attacks. Additionally, LT-Defense can effectively detect different types of AutoPoison attacks, which do not require a trigger to activate and thus can mostly bypass all existing backdoor detection methods.

    &  &  \\  & Dataset & FP & FN & ACC & Time & Dataset & FP & FN & ACC & Time \\   & WikiText & 0/30 & 0/30 & 1.00 & 0.23s & SST-2 & 3/30 & 0/30 & 0.92 & 0.13s \\  & BookCorpus & 0/30 & 0/30 & 1.00 & 0.25s & AG News & 3/30 & 0/30 & 0.95 & 0.15s \\  & WikiText & 0/30 & 0/30 & 1.00 & 0.21s & SST-2 & 3/30 & 0/30 & 0.95 & 0.14s \\  & BookCorpus & 0/30 & 0/30 & 1.00 & 0.23s & AG News & 5/30 & 0/30 & 0.92 & 0.14s \\  & WikiText & 0/30 & 0/30 & 1.00 & 0.41s & SST-2 & 2/30 & 0/30 & 0.97 & 0.51s \\  & BookCorpus & 0/30 & 0/30 & 1.00 & 0.40s & AG News & 3/30 & 0/30 & 0.95 & 0.53s \\   &  &  \\  & Dataset & FP & FN & ACC & Time & Dataset & FP & FN & ACC & Time \\   & GPT-4-LLM & 0/30 & 0/30 & 1.00 & 7.54s & GPT-4-LLM & 0/30 & 0/30 & 1.00 & 7.50s \\  & Dolly-15k & 0/30 & 0/30 & 1.00 & 7.49s & Dolly-15k & 0/30 & 0/30 & 1.00 & 7.44s \\  & GPT-4-LLM & 0/30 & 0/30 & 1.00 & 26.91s & GPT-4-LLM & 0/30 & 0/30 & 1.00 & 27.26s \\  & Dolly-15k & 0/30 & 0/30 & 1.00 & 25.03s & Dolly-15k & 0/30 & 0/30 & 1.00 & 27.23s \\   

Table 2: Detection performance against task-related backdoor attacks. Average Time (minutes) is tested on a single RTX-4090 with the batch size of 32 (8 for OPT-350m and OPT-1.3b).

    &  &  \\  &  &  \\  & FP & FN & ACC & Time & FP & FN & ACC & Time & FP & FN & ACC & Time \\   & LMSanitator & 4/30 & 0/30 & 93.3 & 180.0s & 3/30 & 0/30 & 95.0 & 184.1s & 4/30 & 0/30 & 93.3 & 178.5s \\  & LT-Defense & 0/30 & 0/30 & 100.0 & 0.9s & 0/30 & 1/30 & 98.3 & 0/30 & 0/30 & 100.0 & 0.9s \\  & LMSanitator & 4/30 & 10/30 & 91.7 & 315.5s & 1/30 & 1/30 & 96.7 & 34.5s & 2/30 & 10/30 & 78.3 & 416.5s \\  & LT-Defense & 4/30 & 0/30 & 93.3 & 2.5s & 2/30 & 1/30 & 95.0 & 2.5s & 2/30 & 6/30 & 86.7 & 2.5s \\   & LMSanitator & 1/30 & 0/30 & 98.3 & 33.0s & 0/30 & 100.0 & 33.8s & 0/30 & 1.0/30 & 98.3 & 354.7s \\  & LT-Defense & 0/30 & 0/30 & 100.0 & 1.0s & 0/30 & 0/30 & 100.0 & 0.8s & 1/30 & 0/30 & 98.3 & 0.9s \\  & LMSanitator & 3/30 & 0/30 & 95.0 & 527.8s & 1/30 & 0/30 & 98.3 & 567.6s & 4/30 & 4/30 & 86.7 & 540.9s \\  & LT-Defense & 0/30 & 0/30 & 100.0 & 2.5s & 0/30 & 100.0 & 2.5s & 0/30 & 0/30 & 100.0 & 2.5s \\   & LMSanitator & 2/30 & 1/30 & 95.0 & 260.4s & 1/30 & 0/30 & 98.3 & 257.9s & 1/30 & 1/30 & 96.7 & 241.9s \\  & LT-Defense & 1/30 & 0/30 & 98.3 & 1.1s & 0/30 & 100.0 & 1.1s & 0/30 & 2/30 & 96.7 & 1.1s \\  & LMSanitator & 2/30 & 0/30 & 96.7 & 53.6s & 2/30 & 1/30 & 95.0 & 546.3s & 3/30 & 6/30 & 85.0 & 602.9s \\  & LT-Defense & 0/30 & 0/30 & 100.0 & 3.3s & 1/30 & 1/30 & 96.7 & 3.4s & 1/30 & 2/30 & 95.0 & 3.3s \\   & LT-Defense & 1/30 & 0/30 & 98.3 & 2.4s & 0/30 & 0/30 & 100.0 & 2.5s & 0/30 & 1/30 & 98.3 & 2.5s \\  & LT-Defense & 0/30 & 0/30 & 100.0 & 3.3s & 0/30 & 0/30 & 100.0 & 3.3s & 0/30 & 0/30 & 100.0 & 3.3s \\   

Table 1: Detection performance against task-agnostic backdoor attacks. FP = False Positive, FN = False Negative, ACC = Average Detection Accuracy. Average Time is tested on a single RTX-4090 with the same batch size 32 for different methods.

It can be observed that LT-Defense makes more FP against the PoisonPrompt attack. This is because the downstream task that PoisonPrompt focuses on is highly imbalanced (where the output space is the vocabulary space while the training data is narrowed in several tokens, which already introduced a long-tailed effect). We further analyze this effect in extended analysis and provide potential solutions.

### Extended Analysis

**Test-Time Backdoor Freezing.** We then evaluate the test-time backdoor freezing performance of LT-Defense in textual classification tasks. We use P-Tuning-V2  to apply poison RoBERTa and BERT models to classification tasks on the AG News dataset , and then adopt ONION  and LMSanitator  as two baseline methods. As illustrated in Tab. 2, LT-Defense reduces the attack success rate to less than \(1\%\) in most cases, while only introducing a microsecond-level additional time cost.

Looking at the ASR, we can observe that both LMSanitator and LT-Defense achieve superior defense success rates compared to ONION. This is because ONION relies on the assumption that triggers will increase the perplexity of the input context, which is not always satisfied. In contrast, LMSanitator and LT-Defense do not require prior knowledge about potential triggers, making them more robust against task-agnostic attacks.

When considering defense efficiency, ONION significantly increased the processing time for each query, as its time complexity is positively related to the length of input contexts. Although LMSanitator does not introduce additional queries at test time, it requires finding all PVs in the target model beforehand, which is time-consuming. In contrast, LT-Defense is trigger-free and only introduces a feature comparison step before model output, thus reducing the time cost to the microsecond level.

**Attack Target Prediction.** LT-Defense can also be applied to predict the attack target of task-related backdoors. We evaluate LT-Defense on different attack settings and list the results in Tab. 4. For single-token attacks using BToP and refusal attacks achieved by AutoPoison, LT-Defense can predict the attack target with \(100\%\) precision.

Similar to that in Tab. 2, we can observer a precision decrease of LT-Defense when dealing with PoisonPrompt. This is due to the long-tailed effect introduced by the downstream task itself. For

  

example, the downstream task maps all training examples to several classes (tokens such as "useless", "worst", "delightful", "best") to help semantic analysis, which introduces a long-tailed effect to these classes and their synonyms. As shown in Tab. 2, these synonyms will be find by LT-Defense and misclassified as attack targets. Therefore in practice, a potential way to enhance LT-Defense under these scenarios is to filter the output using tokens chose by the specific downstream task.

### Ablation Study

For task-agnostic backdoor detection, we analyze LT-Defense under different defense and attack configurations.

**Test Size and Dataset.** Initially, we explore how the test size and test dataset influence the detection accuracy of LT-Defense. As illustrated in Fig. 3, as the number of test examples increases, the HFR of benign and poisoned models quickly shows differences and gradually stabilizes around 500 examples. Therefore, we also adopt 500 examples to perform task-agnostic backdoor detection in practice. Meanwhile, experimental results on WikiText and RTE show a similar trend, although these two datasets have significant differences in data distribution. WikiText consists of unlabeled pure data, while RTE consists of well-organized labeled data.

**Different PV Numbers and Types.** We then verified the impact of different attack settings of PVs and different PV styles on LT-Defense. According to Fig. 4, with the number of PVs varies from \(1\) to \(6\), the HFR distributions of benign and poisoned models keep a significant difference. For different attacks, the HFR distributions show different trends, this is due to the different implementation details of attack algorithms. Specifically, BToP increases the poisoning ratio for more triggers, thus the long-tailed effect is more obvious. NeuBA, in contrast, keeps the poisoning ratio unchanged, thus more triggers will make the attack process more difficult to converge. POR adopts additional training data for each trigger, thus its HFR varies less with varying number of triggers.

Meanwhile, we can observe from Fig. 4 that different PV types have less influence on the HFR distribution of poisoned models, thus will not influence the detection precision of LT-Defense.

Figure 4: Detection accuracy with varying number of triggers and different PVs on RoBERTa-base.

Figure 3: Detection accuracy with different test sizes and datasets on RoBERTa-base and RoBERTa-large.

**Task-Related Attacks.** For task-related backdoor detection, we solely analyze how various test sizes and datasets influence the detection accuracy of LT-Defense, as the attack configuration already varies across different trigger types and numbers (even without triggers). As shown in Fig. 5, a similar trend to task-agnostic scenarios can be observed with varying test sizes and datasets. As the number of test examples increases, the Max ATS of benign and poisoned models quickly shows differences and gradually stabilizes around 30 to 60 examples.

### Resistance to Adaptive Attacks

Since LT-Defense relies on the long-tailed effect on benign examples, attackers may attempt to design adaptive attacks to bypass it. Therefore, we designed two adaptive attacks against HFR and one adaptive attack against ATS to evaluate the effectiveness of LT-Defense when the defense is known to attackers.

To bypass HFR-based detection, we reduced the poisoned features of PVs to alleviate their impact on benign examples, and we designed a regularization term to increase the variance of a group of clean feature activation values while injecting backdoors. As shown in Fig. 6 (a) and (b), although both methods can reduce HFR, the attack success rate decreases quickly, resulting in unsuccessful attacks.

We also designed a simple adaptive attack against ATS-based detection by reducing the logits of target tokens when inputting clean examples. As illustrated in Fig. 4(c), although the adaptive attack can bypass LT-Defense by setting the weight parameter high, the attack success rate is significantly reduced.

Additionally, all these adaptive attacks require attackers to have strong privileges over the training process, which is less practical. Overall, LT-Defense shows great potential against adaptive attacks.

## 6 Conclusion

In this paper, we propose a novel searching-free backdoor defense method LT-Defense. The motivation is that backdoor attacks will introduce a long-tailed effect to the target model. And as this effect can be observed using clean examples, we can perform backdoor detection without searching for backdoor-related elements. Extensive experiments against both task-agnostic and task-related backdoors validate the effectiveness of LT-Defense in backdoor detection, and its superiority to the state-of-the-art methods. In the future, we plan to extend LT-Defense to image and audio domain.

Figure 5: Detection accuracy with different test sizes and datasets against task-related attacks.

Figure 6: Adaptive attack against LT-Defense. (a) Reducing poisoned features of PVs. (b) Increasing the variance of clean features. (c) Reducing the logits of target tokens when inputting clean examples.