# Domain Adaptive Imitation Learning

with Visual Observation

 Sungho Choi\({}^{1}\)

Jongseong Chae\({}^{1}\)

Seungyul Han\({}^{2}\)1

Whojun Kim\({}^{3}\)

**Youngchul Sung\({}^{1}\)**

\({}^{1}\)KAIST \({}^{2}\)UNIST \({}^{3}\)Carnegie Mellon University \({}^{4}\)LG AI Research

{sungho.choi,jongseong.chae,ycsung}@kaist.ac.kr, syhan@unist.ac.kr,woojunk@andrew.cmu.edu, whiyoung.jung@lgresearch.ai

###### Abstract

In this paper, we consider domain-adaptive imitation learning with visual observation, where an agent in a target domain learns to perform a task by observing expert demonstrations in a source domain. Domain adaptive imitation learning arises in practical scenarios where a robot, receiving visual sensory data, needs to mimic movements by visually observing other robots from different angles or observing robots of different shapes. To overcome the domain shift in cross-domain imitation learning with visual observation, we propose a novel framework for extracting domain-independent behavioral features from input observations that can be used to train the learner, based on dual feature extraction and image reconstruction. Empirical results demonstrate that our approach outperforms previous algorithms for imitation learning from visual observation with domain shift.

## 1 Introduction

Imitation learning (IL) is a framework where an agent learns behavior by mimicking an expert's demonstration without access to true rewards. The mainstream IL methods include Behavior Cloning (BC), which trains the learner with supervised learning , Inverse Reinforcement Learning (IRL), which tries to find the reward function , and Adversarial Imitation Learning (AIL), which trains the learner to match its state-action visitation distribution to that of the expert via adversarial learning . IL provides an effective alternative to solving reinforcement learning (RL) problems because it circumvents the difficulty in the careful design of a reward function for intended behavior . Despite the effectiveness, conventional IL assumes that the expert and the learner lie in the same domain, but this is a limited assumption in the real world. For example, a self-driving car should learn real-world driving skills from demonstrations in a driving simulator. In such a case, the expert and the learner do not lie in the same domain.

In this paper, we consider the challenge of domain shift in IL, i.e., an agent in a target domain learns to perform a task based on an expert's demonstration in a source domain. In particular, we focus on the case where the expert demonstrations are provided in a visual form. In many practical cases, obtaining expert demonstrations as an explicit sequence of states (such as positions and velocities) and actions (such as movement angles) can be challenging. Instead, the expert's demonstrations are often provided as image sequences. One example can be a robot mimicking a movement by visually observing human behavior. However, learning from visual observations poses a major challenge in IL since images are high-dimensional and often include only partial information about the expert's behavior or information unrelated to the expert's behavior. Even minor changes between the source and target domains can vastly affect the agent's policy and lead to unstable learning. In recent years,several works aimed to address the challenge of domain shift in IL. Some methods show impressive performance by training a model to learn a mapping between domains [22; 32], requiring proxy tasks (i.e., additional tasks to assist in learning the target task) to provide expert demonstrations in both source and target domains. However, this approach assumes expert demonstrations for similar tasks in the target domain, which may not be practical in many realistic domain transfers where expert demonstrations in the target domain are unavailable. Other methods generate metric-based rewards that minimize the distance between the expert and the learner trajectories [9; 27], assuming the expert's state is directly available rather than requiring image observations. Our focus is on solving domain shifts with visual observations without the use of proxy tasks or direct access to expert's states. Our work is closely related to the methods that learn a model to extract domain-invariant features [6; 37]. However, removing domain information was not satisfactory in situations with a large domain gap. In contrast, we propose a novel learning method to train the learner in domain-adaptive IL with visual observation, achieving significantly improved performance in IL tasks with domain shift.

## 2 Related Work

IL with domain shiftIL with domain shift is a challenging problem where the learner should learn from an expert in a source domain and apply the learned policy in a target domain. In recent years, there has been significant progress in addressing this problem in machine learning, RL, and robotics [6; 7; 9; 11; 14; 22; 27; 28; 32; 34; 35; 36; 37; 45]. For example, Fickinger et al.  proposed utilizing Gromov-Wasserstein distance to generate reward minimizing the difference between expert and learner policies. Franzmeyer et al.  learned a mapping between two domains, where expert embeddings are reduced by selecting task-relevant state elements based on mutual information criterion. These methods assume to have access expert's states; in contrast, our method assumes expert demonstrations are only available as high-dimensional image sequences with partial information about the expert. Kim et al.  and Raychaudhuri et al.  trained a model that can map between two domains by using expert demonstrations for proxy tasks on both domains. In comparison, our method does not rely on expert demonstrations of proxy tasks. Sermanet et al.  employed a model that learns representations from visual demonstrations of different viewpoints using a time-contrastive approach. This approach mainly focuses on viewpoint mismatches between domains and relies on time-synchronized demonstrations with simultaneous viewpoints. Chae et al.  introduced an IL algorithm designed to train a policy that exhibits robustness against variations in environment dynamics by imitating multiple experts. In contrast, our method encompasses general domain mismatches including the differences in viewpoint, degree of freedom, dynamics, and embodiment.

We focus on solving domain shift IL with image observation. Liu et al.  used time-aligned expert demonstrations in multiple source domains and learned a model to transform the demonstration from one domain into another. Our work is closely related to Third-Person Imitation Learning (TPIL) , which trains a model based on domain-independent feature extraction  with GAIL . Similarly, Cetin & Celiktutan  proposed restricting mutual information between non-expert data and domain labels. Our method improves on the idea of feature extraction from non-time-aligned visual data by applying dual feature extraction and image reconstruction with consistency checks.

IL from observations (IfO)IfO is an area of IL, where the expert state-action trajectory is not provided explicitly. In IfO, the learner should capture the behavioral information from observation. Demonstrations can be provided either as a set of state vectors without actions [8; 39] or simply as image observations [13; 21; 43; 28]. Torabi et al.  learned the inverse dynamics to infer the missing actions from observations in the demonstration. Edwards et al.  used latent actions to model the transition between two consecutive observations and relabel those latent actions to true actions. We assume that the learner knows its own state and action but expert demonstration is provided only in the form of visual observations as in other works ([6; 37].

## 3 System Model

SetupAn Markov Decision Process (MDP) is described by a tuple \((,,P,R,,_{0})\), where \(\) is the state space, \(\) is the action space, \(P:^{+}\) is the state transition probability, \(R:\) is the reward function, \((0,1)\) is the discount factor, and \(_{0}\) is the initial state distribution . Given an MDP, a stochastic policy \(:^{+}\) is a conditional probability over actions given state \(s\). The goal of RL is to find an optimal policy \(^{*}\) that maximizes\(J()=_{}[_{t=0}^{}^{t}R(s_{t},a_{t})]\), where \(s_{0}_{0},a_{t}(|s_{t}),s_{t+1} P(|s_{t},a_{t})\). In IL, expert demonstrations are provided instead of the true \(R\). The goal of IL with a single domain is to imitate the demonstration generated by an expert policy \(_{E}\).

Problem formulationWe now define IL with domain adaptation. We have two domains: _source domain_ and _target domain_. The source domain is where we can obtain expert demonstrations, and the target domain is where we train the agent. Both domains are modeled as MDPs. The MDPs of the source domain (using subscript \(S\)) and the target domain (using subscript \(T\)) are given by \(_{S}=(_{S},_{S},P_{S},R_{S},_{S},_{ 0,S})\) and \(_{T}=(_{T},_{T},P_{T},R_{T},_{T},_{ 0,T})\), respectively. An expert is the one who performs a task optimally. The agent in the target domain learns the task from expert demonstrations in the source domain. We call the agent a _learner_. The _expert policy_\(_{E}:_{S}_{S}^{+}\) is in the source domain and the _learner policy_\(_{}:_{T}_{T}^{+}\) is in the target domain. We assume that a set of image observations of \(_{E}\) are provided, which is denoted by \(O_{SE}\), where '\(SE\)' refers to'source expert'. The set of observations generated by \(_{}\) is denoted by \(O_{TL}\), where '\(TL\)' refers to 'target learner'. In addition to expert demonstrations from the source domain, we assume access to the non-optimal trajectories of image observations from both the source and target domains. The visual observations of rollouts from a non-expert policy \(_{SN}:_{S}_{S}^{+}\) in the source domain are denoted by \(O_{SN}\), and the visual observations of rollouts from a non-expert policy \(_{TN}:_{T}_{T}^{+}\) in the target domain are denoted by \(O_{TN}\). Here, '\(SN\)' refers to'source non-expert', and '\(TN\)' refers to 'target non-expert'. \(O_{SN}\) and \(O_{TN}\) are additional data to assist our feature extraction and image reconstruction process. \(_{SN}\) and \(_{TN}\) can be constructed in several ways, but we choose these policies as ones taking uniformly random actions. Each observation set has size \(n_{demo}\) and each observation \(o_{t}\) consists of 4 images corresponding to 4 consecutive timesteps. The goal is to train \(_{}\) to solve a given task in the target domain using \(O_{SE}\), \(O_{SN}\), \(O_{TN}\), and \(O_{TL}\).

## 4 Motivation

IL with domain adaptation presents a major challenge as the learner cannot directly mimic the expert demonstration, especially when it is given in a visual form. Also, we cannot leverage expert demonstrations for proxy tasks. To overcome domain shift, conventional methods [6; 37] extracts domain-invariant features from images. We call such feature _behavior feature_, capturing only expert or non-expert behavior information. One example is TPIL . TPIL consists of a single behavior encoder \(BE\), a domain discriminator \(DD\), and a behavior discriminator \(BD\), as shown in Fig. 1 (The label \(XB_{X}\) in Fig. 1 means that the input is in the \(X\) domain with behavior \(B_{X}\). \(X\) can be source \(S\) or target \(T\), and \(B_{X}\) can be expert \(E\) or non-expert \(N\). We use this notation throughout the paper. See Appendix C for more details). The key idea is that \(BE\) learns to fool \(DD\) by removing domain information from the input while helping \(BD\) by preserving behavior information from the input.

However, the behavior feature extraction of the conventional methods does not work to our satisfaction. Fig. 2 shows the t-SNE  plots of the behavior features extracted by the conventional methods [6; 37] after training for the task of Reacher with two links (source domain) to three links (target domain). As seen in the figure, the behavior feature vectors of \(SN\) (orange squares) and those of \(TN\) (blue crosses) are separated in the left figure (note that when domain information is completely

Figure 1: Basic structure for domain-independent behavior feature extraction: \(BE\) - behavior encoder, \(BD\) - behavior discriminator, \(DD\) - domain discriminator

Figure 2: T-SNE plots of features from (a) TPIL and (b) DisentanGAIL. Each point represents a behavior feature: Red circle (\(SE\)), orange square (\(SN\)) and blue cross (\(TN\))

removed, the behavior feature vectors of \(SN\) and \(TN\) should overlap). Furthermore, since the target non-expert performs random actions for each given state, some actions resemble the expert actions in a given state, while other actions do not. So, there should be some overlap between the blue crosses (\(TN\)) and the red dots (\(SE\)). However, the blue crosses are well separated from the red circles in the right figure. Thus, the domain information is not properly removed from the behavior feature. Extracting domain-independent behavior features needs to fully exploit available source-domain and target-domain data.

Our approach is to embed the basic feature extraction structure into a bigger model with data translation. Our main idea for this is _dual feature extraction_ and _dual cycle-consistency_. First, we propose a dual feature extraction scheme to extract both the domain feature and behavior feature from the input so that these two features are independent and contain full information about the input. Second, we propose a new cycle-consistency scheme that fits domain-adaptive IL with visual observation. The proposed model aims to extract better features for reward generation rather than to generate images. Thus, solely applying image-level cycle-consistency is not sufficient to solve the problem of our interest. We circumvent this limitation by additionally applying feature-level cycle-consistency on top of image-level cycle-consistency. Third, we adopt a reward-generating discriminator outside the feature extraction model for proper reward generation by exploiting the generated target-domain expert data from the feature extraction model. Therefore, we name our approach as _D3IL (Dual feature extraction and Dual cycle-consistency for Domain adaptive IL with visual observation)_.

## 5 Proposed Methodology

### Dual Feature Extraction

The behavior feature in the target domain is the required information that tells whether the learner's action is good or not, in other words, whether the learner in the target domain successfully mimics the expert in the source domain or not. To improve behavior feature extraction, we boost the simple adversarial learning, as the structure shown in Fig. 1, by adding an additional structure from which we can check that the obtained feature well contains the required information without information loss, based on image reconstruction and consistency check, as shown in Fig. 3. That

Figure 3: The structure of the learning model: Inputs in source and target domains have features \(SB_{S}\) and \(TB_{T}\), respectively. \(B_{S}\) can be \(E\) (expert) and \(N\) (non-expert), and \(B_{T}\) can be \(N\) (non-expert) and \(L\) (learner). \(DE\) is domain encoder (blue), \(BE\) is behavior encoder (orange) and \(G\) is generator (green). \(DD,BD\) and \(ID\) with gray color are domain discriminator, behavior discriminator, and image discriminator, respectively. Small long rectangles represent features. The sub-block in the blue dotted line in the lower left corner corresponds to the basic structure shown in Fig. 1.

is, we adopt two encoders in each of the source and target domains: domain encoder \(DE_{X}\) and behavior encoder \(BE_{X}\), where \(X=S\) or \(T\), as shown in the left side of Fig. 3. The domain encoder \(DE_{X}\), where \(X=S\) or \(T\), tries to extract domain feature (\(S\) or \(T\)) that contains only the domain information excluding behavioral information, whereas the behavior encoder \(BE_{X}\) tries to extract behavioral feature (\(B_{X}=E\) or \(N\)) that contains only the behavioral information (expert or non-expert) excluding domain information. Each of these encodings is based on the aforementioned basic adversarial learning block composed of one encoder and two accompanying discriminators: domain and behavior discriminators, as shown in the left side of Fig. 3. As the outputs of the four encoders, we have four output features: \(S\) and \(B_{S}\) from the source-domain encoders, and \(T\) and \(B_{T}\) from the target-domain encoders.

### Dual Cycle-Consistency (Dual-CC)

We now explain _dual cycle-consistency (Dual-CC)_, composed of _image-level cycle-consistency (Image-CC)_ and _feature-level cycle-consistency (Feature-CC)_ to improve feature extraction. Using the four output feature vectors extracted by the encoders in Sec. 5.1, we apply image translation [19; 25; 26; 44; 48]. That is, we combine \(T\) and \(B_{S}\) and generate an image \(}\) by using generator \(G_{T}\), and combine \(S\) and \(B_{T}\) and generate an image \(}\) by using generator \(G_{S}\). Each image generator (\(G_{T}\) or \(G_{S}\)) is trained based on adversarial learning with the help of the corresponding image discriminator (\(ID_{T}\) or \(ID_{S}\)), as shown in the middle part of Fig. 3. Then, from the generated images \(}\) and image \(}\), we apply feature extraction again by using the same trained encoders \(DE_{T},BE_{T},DE_{S},BE_{S}\) used in the first-stage feature extraction, as shown in the right side of Fig. 3. Finally, using the four features \(,_{S},,_{T}\) from the second-stage feature extraction, we do apply image translation again to reconstruct the original images \(}\) and \(}\) by using the same generators \(G_{S}\) and \(G_{T}\) used as the first-stage image generators, as shown in the rightmost side of Fig. 3. Note that if the encoders properly extracted the corresponding features without information loss, the reconstructed images \(}\) and \(}\) should be the same as the original input images \(SB_{S}\) and \(TB_{T}\). We exploit this _Image-CC_ as one of our criteria for our feature-extracting encoders.

However, the aforementioned adversarial learning and Image-CC loss are insufficient to yield mutually exclusive behavior and domain features, as we will see in Sec. 6.7. To overcome this limitation, we enhance the dual feature extraction by complementing Image-CC by imposing _Feature-CC_. Note that our goal is not image translation but feature extraction relevant to domain-adaptive IL. The Feature-CC means that the extracted features \(S\), \(B_{S}\), \(T\) and \(B_{T}\) in the left dual feature extraction part of Fig. 3 should respectively be the same as their corresponding feature vectors \(\), \(}\), \(\) and \(}\) in the right cycle-consistency check part of Fig. 3. The rationale for this consistency is as follows. Consider \(B_{T}\) and \(}\) for example. \(B_{T}\) is extracted from the target-domain image \(TB_{T}\), whereas \(}\) is extracted from a source-domain image \(}\) generated from source-domain image generator \(G_{S}\). For the two behavior features \(B_{T}\) and \(}\) from two different domains to be the same, the domain information in \(B_{T}\) and \(}\) should be eliminated. Thus, the Feature-CC helps the extraction of mutually exclusive behavior and domain features while the Image-CC requires the combination of behavior and domain features to preserve full information about the input observation without any loss. Since both consistencies are not perfect, we adopt both Image-CC and Feature-CC for our feature extraction, so named Dual-CC.

### Further Consistency and Stability Enhancement

In addition to the Dual-CC, our architecture allows us to verify other consistencies on the extracted features. First, we enforce the _image reconstruction consistency_ by combining the features \(S\) and \(B_{S}\) from \(DE_{S}\) and \(BE_{S}\) in the first-stage feature extraction and feeding the feature combination \((S,B_{S})\) into image generator \(G_{S}\). The output \(}\) should be the same as the original image \(SB_{S}\). The same consistency applies to the feature combination \((T,B_{T})\) in the target domain with image generator \(G_{T}\). Second, we enforce the _feature reconstruction consistency_ by feeding the generated source-domain image \(}\) described above into the encoders \(DE_{S}\) and \(BE_{S}\), then we obtain domain feature \(\) and behavior feature \(_{S}\). These two features should be identical to the features \(S\) and \(B_{S}\) extracted in the first-stage feature extraction. The same principle applies to the target domain.

In addition to the key consistency idea explained above, we apply further techniques of _feature vector regularization_ and _feature vector similarity_ to enhance performance. Feature vector regularization maintains the \(L_{2}\)-norm of the feature vector which helps to prevent the feature vectors from becoming too large. Feature vector similarity is another form of consistency that guides the model so that the domain feature vectors of observations from the same domain should have similar values and the behavior feature vectors of observations from the same behavior should have similar values.

With this additional guidance for feature extraction on top of the conventional adversarial learning block, the proposed scheme significantly improves feature extraction compared to previous approaches, as we will see in Sec. 6. Once the overall structure is learned, we take the behavior encoder \(BE_{T}\) in the target domain (the block inside the blue dotted block in Fig. 3) to train the learner policy. This improved behavior feature extraction can enhance the learner's performance in the target domain. Note that the complexity of D3IL is simply at the level of widely used image translation.

### Reward Generation and Learner Policy Update

Now, let us explain how to update the learner policy \(_{}\). One can simply use the behavior discriminator \(BD_{B}\) in the target domain together with the behavior encoder \(BE_{T}\) to generate a reward for policy updates. Instead, we propose a different way of reward generation. We use \(BE_{T}\) but do not use \(BD_{B}\) from the feature extraction model. This is because, in our implementation, \(BD_{B}\) is trained to distinguish the expert behavior \(E\) from the non-expert behavior \(N\), but what we need is a discriminator that distinguishes the expert behavior \(E\) from the _learner_ behavior \(L\) so that the _learner_ policy \(_{}\) can receive proper rewards for imitation. Therefore, we train another network called _reward-generating discriminator_\(D_{rew}\) for reward generation. \(D_{rew}\) and \(_{}\) jointly learn in an adversarial manner like in GAIL. \(D_{rew}\) takes a behavior feature vector from \(BE_{T}\) as input and learns to predict the behavior label (\(E\) or \(L\)) of the input. For stable learning, we consider GAIL with gradient penalty (GP) .

To train \(D_{rew}\), we need the expert behavior feature from \(BE_{T}\) as well as the learner behavior feature \(L\). \(L\) is simply obtained by applying the policy output \(o_{TL}\) to \(BE_{T}\). The required expert behavior feature can be obtained by applying generated target-domain expert image \(\) to \(BE_{T}\), where \(\) is generated from the feature extraction model using domain feature \(T\) from \(o_{TN}\) and behavior feature \(E\) from \(o_{SE}\), as shown in Fig. 4. (Or simply we can use the expert feature \(E\) obtained by applying source-domain expert image \(o_{SE}\) to \(BE_{S}\) based on the imposed Feature-CC.) The loss for \(D_{rew}\) with fixed behavior encoder \(BE_{T}\) is given by

\[L_{D} =_{(o_{TN},o_{SE})(O_{TN},o_{SE})}[(D_{rew}(BE_{ T}(_{TE})))]+_{o_{TL} O_{TL}}[(1-D_{rew}(BE_{T}(o_{TL})))]\] \[+_{o_{x} Mix(_{TE},o_{TL})}[(|| D_{rew} (BE_{T}(o_{x}))||_{2}-1)^{2}]\]

where \(_{TE}=G(DE(o_{TN}),BE(o_{SE}))\) is a generated image from the domain feature \(T\) of \(o_{TN}\) and the behavior feature \(E\) of \(o_{SE}\), and the third expectation is the GP term and is over the mixture of \(_{TE}\) and \(o_{TL}\) with a certain ratio. \(D_{rew}\) takes a behavior feature vector as input and predicts its behavior label \(E\) or \(L\) by assigning value 1 to expert behavior and value 0 to learner behavior. On the other hand, \(_{}\) learns to generate observations \(o_{TL}\) so that the corresponding behavior feature vector resembles that of the expert. The learner updates its policy using SAC , and the estimated reward for an observation \(o_{t}\) is defined by \((o_{t})=(D_{rew}(BE_{T}(o_{t})))-(1-D_{rew}(BE_{T}(o_{t})))\). Appendix A provides the details of loss functions implementing D3IL, and Algorithms 1 and 2 in Appendix B summarise our domain-adaptive IL algorithm.

Figure 4: Reward-generating discriminator \(D_{rew}\) and its training

Experiments

### Experimental Setup

We evaluated D3IL on various types of domain shifts in IL tasks, including changes in visual effects, the robot's degree of freedom, dynamics, and embodiment. We also tested if D3IL can solve tasks when obtaining expert demonstrations by direct RL in the target domain is challenging. Each IL task consists of a source domain and a target domain, with a base RL task either in Gym  or in DeepMind Contol Suite (DMCS) . In Gym, we used five environments: Inverted Pendulum (IP), Inverted Double Pendulum (IDP), Reacher-two (RE2), Reacher-three (RE3), and HalfCheetah (HC). In addition, we used a customized U-Maze environment  for the experiment in Section. 6.6. In DMCS, we used three environments: CartPole-Balance, CartPole-SwingUp, and Pendulum. Appendix E contains sample observations, task configuration, and space dimensions for each environment.

We compared the performance of D3IL with three major baselines: TPIL  and DisentanGAIL , and GWIL . TPIL and DisentanGAIL are image-based IL methods that aim to extract domain-independent features from image observations and are most relevant to our problem setting. Additionally, we also compared D3IL with GWIL, a recent state-based IL method. For GWIL, demonstrations were provided as state-action sequences for all experiments, while for other methods demonstrations were provided as image observations. Appendix D provides implementation details including network structures and hyperparameters for D3IL.

### Results on IL Tasks with Changing Visual Effects

We evaluated D3IL on tasks where the domain difference is caused by visual effects on image observations. We conducted the experiment in four IL tasks: IP and IDP, with different color combinations of the pole and cart (IP-to-colored and IDP-to-colored tasks), and RE2 and RE3, with different camera angles (RE2-to-tilted and RE3-to-tilted tasks). Figure 17 in Appendix F.1 contains sample images for each task. The average episodic return for D3IL and the baselines over 5 seeds are shown in Fig. 18 in Appendix F.1. As shown in Fig. 18, D3IL outperformed the baselines with large margins on tasks with changing visual effects.

### Results on IL Tasks with Changing Degree-of-Freedom of a Robot

Next, we evaluated D3IL on IL tasks with varying robot's degree-of-freedom. This includes scenarios where the learner robot should imitate an expert performing the same task, but the number of joints differs between them. We conducted four IL tasks: two based on IP (one pole) and IDP (two poles), and two based on RE2 (2-link arm) and RE3 (3-link arm). Sample observations for each IL task are shown in Figure 14 in Appendix E. Fig. 5 shows the average episodic return over 5 seeds. It is seen that D3IL outperforms the baseline methods by a large margin in all the IL tasks. In Fig. 5 the dotted line represents the performance of SAC trained with true rewards in the target domain. This dotted line can be considered as the performance upper bound when using SAC as the control algorithm. It is interesting to observe that in Figs. 5 (c) and (d), the baselines seem to learn the task initially but yield gradually decreasing performance, whereas D3IL continues to improve its performance. Appendix F.2 provides further discussions on the quality of extracted features on these tasks. Appendix F.3 provides additional results on IP, IDP, RE2, and RE3 with simpler settings created by .

Figure 5: The learning curves in the target domain of the IL tasks with changing DOF

### Results on Tasks with Changing Internal Dynamics

We examined D3IL on a task with a different type of domain shift, specifically a change in the robot's dynamics. The considered IL task is 'HalfCheetah-to-LockedFeet'. The base RL task is as follows. In HalfCheetah (HC) environment, the goal is to move an animal-like 2D robot forward as fast as possible by controlling six joints. Similar to , we also have a modified environment named as 'HalfCheetah-LockedFeet (HC-LF)'. In HC-LF, the robot can move only four joints instead of six, as two feet joints are immobilized. Fig. 15 in Appendix E shows sample observations for both environments, where the immobilized feet are colored in red. Fig. 6 shows the average episodic return over 5 seeds. Fig. 6 shows that D3IL has superior performance to the baseline methods and can solve IL tasks by changing robot's dynamics.

### Results on Tasks with Changing Robot Embodiment

We examined whether D3IL can train the agent to mimic an expert with a different embodiment. We use CartPole-Balance, CartPole-SwingUp, and Pendulum environments from DMCS , where sample observations are provided in Fig. 16 (see Appendix E). The goal for all environments is to keep the pole upright. The CartPole agents do so by sliding the cart, while the Pendulum agent does so by adding torque on the center. The pole is initially set upright in CartPole-Balance, while it is not in other environments. Domain adaptation between these tasks is challenging because not only the embodiment between agents in both domains are different but also the distributions of expert and non-expert demonstrations, and initial observations for both domains are quite different. Fig. 7 shows the performances on IL tasks with different robot embodiment. Although D3IL does not fully reach the upper bound, it achieved far better performance than the previous methods on tasks by changing the robot's embodiment.

### Experiment on AntUMaze: When Direct RL in Target Domain is Difficult

In fact, for IL tasks we have done so far, the learner policy can be learned directly in the target domain by RL interacting with the target-domain environment. The real power of domain-adaptive IL comes when such direct RL in the target domain is very difficult or impossible by known RL algorithms. Here, we consider one such case known as AntUMaze . In AntUMaze, a quadruped ant robot should move to the red goal position through a U-shaped maze, as shown in the second row of Fig. 8. It can be verified that it is very difficult to solve this problem directly with a current state-of-the-art RL algorithm such as SAC. This is because the problem involves complicated robot manipulation and pathfinding simultaneously. In order to solve AntUMaze, we consider a simpler task in the source domain known as PointUMaze, which is easily solvable by SAC. In PointUMaze, the robot is simplified as a point whose state is defined as the point location and point direction, as shown in the first row of Fig. 8, whereas the state of the ant robot is much more complicated. Appendix D.3 provides a detailed explanation of the training process for the UMaze environments. Now, the goal

Figure 6: Result on HC-LF

Figure 7: The learning curves for the considered IL tasks for CartPole and Pendulum environmentsof this IL task (named PointUMaze-to-Ant) is to solve the AntUMaze problem in the target domain with its own dynamics based on the point robot demonstrations.

Fig. 9 shows the average episodic return over 5 seeds. As shown in Fig. 9, the ant robot trained with D3IL reaches the goal position after about 1M timesteps, while the ant robot trained with the baseline IL algorithms failed to reach the goal position. D3IL can train the ant agent to learn desirable behaviors from point-robot expert demonstrations with totally different dynamics. We observed that SAC with reward could not solve this task. So, it is difficult to directly solve the AntUMaze with RL or to obtain expert demonstrations in AntUMaze directly. D3IL can transfer expert knowledge from an easier task to a harder task, and train the agent in the target domain to perform the AntUMaze task.

### Analysis of Dual Cycle-Consistency

To analyze the impact of Dual-CC, we use t-SNE to visualize behavior features generated by our model. Fig. 10 shows the t-SNE plots of the behavior feature extracted by the model trained on the RE2-to-three task: We used the model with full Dual-CC in Fig. 10 (a), the model with only Feature-CC in Fig. 10 (b), and the model with only Image-CC in Fig. 10 (c). Fig. 10 (a) based on the Dual-CC shows the desired properties; target-domain non-expert (\(TN\)) and source-domain non-expert (\(SN\)) are well overlapped, and some of the random behavior feature vectors coincide with the source-domain expert (\(SE\)) behavior. We also plotted behavior features of generated target experts (\(\)) with green '+' in the figure. Since the behavior feature of \(\) is derived from the behavior feature of \(SE\) and we imposed the Feature-CC, the red circles and green '+' overlap quite a good portion. However, there still exists some gap at a certain region, showing the feature extraction is not perfect. Fig. 10 (c) shows that domain information still remains when removing the Feature-CC, and the Feature-CC indeed helps eliminate domain information. As seen in Fig. 10 (b), only the Feature-CC is not enough to extract the desired behavior feature. Indeed, Dual-CC is essential in extracting domain-independent behavior features.

### Ablation Study

**Impact of each loss component** D3IL uses multiple add-ons to the basic adversarial behavior feature learning for consistency check. As described in Sec. 5, the add-ons for better feature extraction are (1) image and feature reconstruction consistency with the generators in the middle part of Fig. 3, (2) Image-CC with the right part of Fig. 3, (3) feature similarity, and (4) Feature-CC. We removed each component from the full algorithm in the order from (4) to (1) and evaluated the corresponding performance, which is shown in Table 1. In Table 1, each component has a benefit to performance enhancement. Especially, the consistency check for the extracted feature significantly enhances the performance. Indeed, our consistency criterion based on dual feature extraction and image reconstruction is effective.

Figure 8: Sample images of U-Figure 9: Results on PointUMaze-maze

Figure 10: T-SNE plot of behavior feature from D3IL: (a) Dual-CC (b) Feature-CC only (c) Image-CC only

Reward-generating discriminator \(D_{rew}\)In Sec. 5.4, we explained the use of \(D_{rew}\) for reward estimation. As explained, we train \(D_{rew}\) and the learner policy \(_{}\) in an adversarial manner. Instead of using an external \(D_{rew}\), we could use the behavior discriminator \(BD_{B}\) in the feature extraction model to train the policy \(_{}\). To see how this method works, we trained \(_{}\) using the behavior discriminator \(BD_{B}\) in the feature extraction model, not the external \(D_{rew}\). As shown in Table 2, training \(_{}\) using \(BD_{B}\) is not successful because of the reason mentioned in Sec. 5.4.

Domain encodersAs described in Sec. 5, we proposed employing both domain encoder and behavior encoder for enhanced domain-independent behavior feature extraction. This approach was motivated by the observation that solely relying on a behavior encoder was insufficient to eliminate domain-related information from behavior features. To investigate the importance of domain encoders, we evaluated the performance of D3IL with and without domain encoders (\(DE_{S}\) and \(DE_{T}\)) and their corresponding discriminators (\(DD_{D}\) and \(BD_{D}\)) across four tasks: IP-to-two, RE2-to-three, RE3-to-tilted, and CartPoleBalance-to-Pendulum. All other networks and losses remain unchanged. Fig. 11 shows the results. The results demonstrate the benefit of conditioning generators on learned domain encoders to boost model performance.

## 7 Discussion and Conclusion

In this paper, we have considered the problem of domain-adaptive IL with visual observation. We have proposed a new architecture based on dual feature extraction and image reconstruction in order to improve behavior feature extraction, and a reward generation procedure suited to the proposed model. The proposed architecture imposes additional consistency criteria on feature extraction in addition to basic adversarial behavior feature extraction to obtain better behavior features. Numerical results show that D3IL yields superior performance to existing methods in IL tasks with various domain differences and is useful when direct RL in the target domain is difficult.

LimitationsWhile our method demonstrates superior performance, it also has some limitations. One is the tricky tuning of loss coefficients, which involves several loss functions. However, we observed that balancing the scale of loss components is sufficient. Additionally, our feature extraction model remains fixed during policy updates. Future works could leverage the past target learner experiences to update the model further. Another limitation is the need for target environment interaction during training, a characteristic shared by several domain-adaptive IL approaches. It will be interesting to extend our method to an offline approach in future works. Moreover, our method lacks a quantitative measure for domain shifts. Future research could develop a mathematical way to quantify domain shifts, enabling the measurement of task difficulty and addressing more complex IL problems. Lastly, future works could extend our method to multi-task or multi-modal scenarios.

    & RE3-to-two \\  basic adversarial learning & -10.7 \(\) 0.5 \\ + image/feature reconstr. consistency & -6.0 \(\) 0.7 \\ + image-level cycle-consistency & -3.0 \(\) 0.1 \\ + feature similarity & -2.8 \(\) 0.1 \\ + feature-level cycle-consistency & **-2.6 \(\) 0.1** \\   

Table 1: Impact of each loss component

Figure 11: Ablation study on domain encoders