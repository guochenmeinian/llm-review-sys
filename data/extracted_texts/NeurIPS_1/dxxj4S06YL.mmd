# Fair Secretaries with Unfair Predictions

Eric Balkanski

Columbia University

eb3224@columbia.edu &Will Ma

Columbia University

wm2428@gsb.columbia.edu &Andreas Maggiori

Columbia University

am6292@columbia.edu

###### Abstract

Algorithms with predictions is a recent framework for decision-making under uncertainty that leverages the power of machine-learned predictions without making any assumption about their quality. The goal in this framework is for algorithms to achieve an improved performance when the predictions are accurate while maintaining acceptable guarantees when the predictions are erroneous. A serious concern with algorithms that use predictions is that these predictions can be biased and, as a result, cause the algorithm to make decisions that are deemed unfair. We show that this concern manifests itself in the classical secretary problem in the learning-augmented setting--the state-of-the-art algorithm can have zero probability of accepting the best candidate, which we deem unfair, despite promising to accept a candidate whose expected value is at least \(\{(1),1-O()\}\) times the optimal value, where \(\) is the prediction error. We show how to preserve this promise while also guaranteeing to accept the best candidate with probability \((1)\). Our algorithm and analysis are based on a new "pegging" idea that diverges from existing works and simplifies/unifies some of their results. Finally, we extend to the \(k\)-secretary problem and complement our theoretical analysis with experiments.

## 1 Introduction

As machine learning algorithms are increasingly used in socially impactful decision-making applications, the fairness of those algorithms has become a primary concern. Many algorithms deployed in recent years have been shown to be explicitly unfair or reflect bias that is present in training data. Applications where automated decision-making algorithms have been used and fairness is of central importance include loan/credit-risk evaluation , hiring , recidivism evaluation , childhood welfare systems , job recommendations , price discrimination , resource allocation , and others . A lot of work in recent years has been devoted to formally defining different notions of fairness , designing algorithms that satisfy these different definitions , and investigating trade-offs between fairness and other optimization objectives .

While most fairness work concentrates on classification problems where the instance is known offline, we explore the problem of making fair decisions when the input is revealed in an online manner. Although fairness in online algorithms is an interesting line of research per se, fairness considerations have become increasingly important due to the recent interest in incorporating (possibly biased) machine learning predictions into the design of classical online algorithms. This framework, usually referred to as _learning-augmented algorithms_ or _algorithms with predictions_, was first formalized in . In contrast to classical online algorithms problems where it is assumed that no information is known about the future, learning-augmented online algorithms are given as input,possibly erroneous, predictions about the future. The main challenge is to simultaneously achieve an improved performance when the predictions are accurate and a robust performance when the predictions are arbitrarily inaccurate. A long list of online problems have been considered in this setting and we point to  for an up-to-date list of papers. We enrich this active area of research by investigating how potentially biased predictions affect the fairness of decisions made by learning-augmented algorithms, and ask the following question:

Can we design fair algorithms that take advantage of unfair predictions?

In this paper, we study this question on a parsimonious formulation of the secretary problem with predictions, motivated by fairness in hiring candidates.

**The problem.** In the classical secretary problem, there are \(n\) candidates who each have a value and arrive in a random order. Upon arrival of a candidate, the algorithm observes the value of that candidate and must irrevocably decide whether to accept or reject that candidate. It can only accept one candidate and the goal is to maximize the probability of accepting the candidate with maximum value. In the classical formulation, only the _ordinal_ ranks of candidates matter, and the algorithm of Dynkin  accepts the best candidate with a constant probability, that equals the best-possible \(1/e\).

In the learning-augmented formulation of the problem proposed by Fujii and Yoshida , the algorithm is initially given a predicted value about each candidate and the authors focus on comparing the expected _cardinal_ value accepted by the algorithm to the maximum cardinal value. The authors derive an algorithm that obtains expected value at least \(\{(1),1-O()\}\) times the maximum value, where \( 0\) is the prediction error. The strength of this guarantee is that it approaches \(1\) as the prediction error decreases and it is a positive constant even when the error is arbitrarily large.

However, because the algorithm is now using predictions that could be biased, the best candidate may no longer have any probability of being accepted. We view this as a form of unfairness, and aim to derive algorithms that are fair to the best candidate by guaranteeing them a constant probability of being accepted (we contrast with other notions of fairness in stopping problems in Section 1.1). Of course, a simple way to be fair by this metric is to ignore the predictions altogether and run the classical algorithm of Dynkin. However, this approach would ignore potentially valuable information and lose the improved guarantee of  that approaches 1 when the prediction error is low.

**Outline of results.** We first formally show that the algorithm of  may in fact accept the best candidate with 0 probability. Our main result is then a new algorithm for secretary with predictions that: obtains expected value at least \(\{(1),1-O()\}\) times the maximum value, like ; and ensures that, under any predictions, the best candidate is hired with \((1)\) probability. This result takes advantage of potentially biased predictions to achieve a guarantee on expected value that approaches \(1\) when the prediction error is small, while also providing a fairness guarantee for the true best candidate irrespective of the predictions. We note that Antoniadis et al.  also derive an algorithm for secretary with predictions, where the prediction is of the maximum value (a less informative form of prediction). This algorithm accepts the best candidate with constant probability but it does not provide a guarantee on the expected value accepted that approaches \(1\) as the prediction error approaches \(0\). Similarly, Dynkin's algorithm for the classical secretary problem accepts the best candidate with constant probability but does not make use of predictions at all.

Our algorithm is fundamentally different from existing algorithms for secretary with predictions, as our "pegging" idea, i.e., the idea not to accept a possibly suboptimal candidate if there is a future candidate with high enough predicted value, is important to achieve our fairness desideratum. We also note that the definitions of the prediction error \(\) differ in  and ; the former error definition uses the maximum ratio over all candidates between their predicted and true value while the latter uses the absolute difference. Our techniques present an arguably simpler analysis and extend to a general family of prediction error measures that includes both of these error definitions.

We then extend our approach to the multiple choice or \(k\)-secretary problem where the goal is to accept at most \(k\) candidates and maximize the total of their values, which is the most technical part of the paper. We design an algorithm that obtains expected total value at least \(\{(1),1-O()\}\) times the optimum (which is the sum of the \(k\) highest values), while simultaneously guaranteeing the \(k\) highest-valued candidates a constant probability of being accepted. We also have a refined guarantee that provides a higher acceptance probability for the \((1-)k\) highest-valued candidates, for any \((0,1)\).

Finally, we simulate our algorithms in the exact experimental setup of Fujii and Yoshida . We find that they perform well both in terms of expected value accepted and fairness, whereas benchmark algorithms compromise on one of these desiderata.

### Related work

**The secretary problem.** After Gardner  introduced the secretary problem, Dynkin  developed a simple and optimal stopping rule algorithm that, with probability at least \(1/e\), accepts the candidate with maximum value. Due to its general and simple formulation, the problem has received a lot of attention (see, e.g.,  and references therein) and it was later extended to more general versions such as \(k\)-secretary , matroid-secretary  and knapsack-secretary .

**Secretaries with predictions.** The two works which are closest to our paper are those of Antoniadis et al.  and Fujii and Yoshida . Both works design algorithms that use predictions regarding the values of the candidates to improve the performance guarantee of Dynkin's algorithm when the predictions are accurate while also maintaining robustness guarantees when the predictions are arbitrarily wrong. Antoniadis et al.  uses as prediction only the maximum value and defines the prediction error as the additive difference between the predicted and true maximum value while Fujii and Yoshida  receives a prediction for each candidate and defines the error as the maximum multiplicative difference between true and predicted value among all candidates. Very recently, Choo and Ling  showed that any secretary algorithm that is \(1\)-consistent cannot achieve robustness better than \(1/3+o(1)\), even with predictions for each candidate. This result implies that, if we wish to maintain the \(1-O()\) competitive ratio guarantee from , then the probability of accepting the best candidate cannot be better than \(1/3+o(1)\).

**Secretaries with distributional advice.** Another active line of work is to explore how _distributional_ advice can be used to surpass the \(1/e\) barrier of the classical secretary problem. Examples of this line of work include the _prophet secretary problems_ where each candidate draws its valuation from a known distribution  and the _sample secretary problem_ where the algorithm designer has only sample access to this distribution . We note that in the former models, predictions are either samples from distributions or distributions themselves which are assumed to be perfectly correct, while in the learning-augmented setting, we receive point predictions that could be completely incorrect. Dutting et al.  investigate a general model for advice where both values and advice are revealed upon a candidate's arrival and are drawn from a joint distribution \(\). For example, their advice can be a noisy binary prediction about whether the current candidate is the best overall. Their main result uses linear programming to design optimal algorithms for a broad family of advice that satisfies two conditions. However, these two conditions are not satisfied by the predictions we consider. Additionally, we do not assume any prior knowledge of the prediction quality, whereas their noisy binary prediction setting assumes that the error probability of the binary advice is known.

**Fairness in stopping algorithms.** We say that a learning-augmented algorithm for the secretary problem is \(F\)-fair if it accepts the candidate with the maximum true value with probability at least \(F\). In that definition, we do not quantify unfairness as a prediction property but as an algorithmic one, since the algorithm has to accept the best candidate with probability at least \(F\) no matter how biased predictions are our fairness notion is a challenging one. That notion can be characterized as an individual fairness notion similar to the _identity-independent fairness_ (IIF) and _time-independent fairness_ (TIF) introduced in . In the context of the secretary problem, IIF and TIF try to mitigate discrimination due to a person's identity and arrival time respectively. While these are very appealing fairness notions, the fair algorithms designed in  fall in the classical online algorithms setting as they do not make any assumptions about the future. Consequently, their performance is upper bound by the performance of the best algorithm in the classical worst-case analysis setting. It is also interesting to note the similarities with the _poset secretary problem_ in . In the latter work the set of candidates is split into several groups and candidates belonging to different groups cannot be compared due to different biases in the evaluation. In some sense, we try to do the same; different groups of candidates may have predictions that are affected by different biases making the comparison difficult before the true value of each candidate is revealed. Again, in  no information about the values of future candidates is available and the performance of their algorithms is upper bounded by the best possible performance in the worst-case analysis setting.

Preliminaries

**Secretary problem with predictions.** Candidates \(i=1,,n\) have true values \(u_{i}\) and predicted values \(_{i}\). The number of candidates \(n\) and their predicted values are known in advance. The candidates arrive in a uniformly random order. Every time a new candidate arrives their true value is revealed and the algorithm must immediately decide whether to accept the current candidate or reject them irrevocably and wait for the next arrival. We let \(i^{*}=*{argmax}_{i}u_{i}\) and \(=*{argmax}_{i}_{i}\) denote the indices of the candidates with the maximum true and predicted value respectively. An instance \(\) consists of the \(2n\) values \(u_{1},,u_{n},_{1},,_{n}\) which, for convenience, are assumed to be positive1 and mutually distinct2. We let \(()\) denote its _prediction error_. For simplicity, we focus on the additive prediction error \(()=_{i}|_{i}-u_{i}|\), but we consider an abstract generalization that includes the multiplicative prediction error of  in Appendix A.2.

**Objectives.** We let \(\) be a random variable denoting the candidate accepted by a given algorithm on a fixed instance, which depends on both the arrival order and any internal randomness in the algorithm. We consider the following desiderata for a given algorithm:

\[[u_{}]  u_{i^{*}}-C(),\ \] _(smoothness)_ \[P[=i^{*}]  F,\ .\] _(fairness)_

Since the prediction error \(()\) is an additive prediction error, we define smoothness to provide an additive approximation guarantee that depends on \(()\). When considering the multiplicative prediction error of , smoothness is defined to provide an approximation guarantee that is multiplicative instead of additive (see Theorem 4).

We aim to derive algorithms that can satisfy smoothness and fairness with constants \(C,F>0\) that do not depend on the instance \(\) or the number of candidates \(n\). Existing algorithms for secretary with predictions do not simultaneously satisfy these desiderata, as shown by our examples in Appendix A.1.

**Comparison to other objectives.** Existing algorithms for secretary with predictions do satisfy a weaker notion called _\(R\)-robustness_, where \([u_{}] R u_{i^{*}}\) for some constant \(R>0\). Our desideratum of fairness implies \(F\)-robustness and aligns with the classical secretary formulation where one is only rewarded for accepting the best candidate. Another notion of interest in existing literature is _consistency_, which is how \([u_{}]\) compares to \(u_{i^{*}}\) when \(()=0\). Our smoothness desideratum implies \(1\)-consistency, the best possible consistency result, and guarantees a smooth degradation as \(()\) increases beyond \(0\).

## 3 Algorithm and Analysis

We first present and analyze Additive-Pegging in Algorithm 1 which achieves the desiderata from Section 2. Then, we mention how using a more abstract prediction error and an almost identical analysis, permits us to generalize Additive-Pegging to Pegging which achieves comparable guarantees for a more general class of error functions that includes the multiplicative error.

Our algorithms assume that each candidate \(i\) arrives at an independently random arrival time \(t_{i}\) drawn uniformly from \(\). The latter continuous-time arrival model is equivalent to candidates arriving in a uniformly random order and simplifies the algorithm description and analysis. We also write \(_{i}\) as shorthand for \(|u_{i}-_{i}|\), \(\) as shorthand for \(()\) (so that \(=_{i}_{i}\)) and \(i j\) if \(t_{i}<t_{j}\).

**Description of Additive-Pegging.** Additive-Pegging ensures smoothness by always accepting a candidate whose value is close to \(u_{i}\) which, as we argue, is at least \(u_{i^{*}}-2\,\). To see this, note that \(u_{i}_{i}-_{i}_{i^{*}}-_{i} u_{i^{* }}-_{i^{*}}-_{i} u_{i^{*}}-2\,\), where we used that \(_{i}_{i^{*}}\) (by definition of \(\)) and \(\{_{i^{*}},_{i}\}\) (by definition of \(\)). Consequently, for smoothness, our algorithm defines the literal \(=(i=)\) at each new arrival, which is true if and only if \(i\) has the highest predicted value. Accepting while \(\) holds would maintain smoothness.

For the fairness desideratum, we note that Dynkin's algorithm  for the classical secretary problem relies on the observation that if a constant fraction of the candidates have arrived and the candidate who just arrived has the maximum true value so far, then this candidate has a constant probability of being the best overall. The same high-level intuition is used in our algorithm. Every time a new candidate \(i\) arrives, we check if \(i\) is the maximum so far and if \(t_{i}>1/2\); namely, we compute the literal \(\). Accepting when \(\) is true, which is what Dynkin's algorithm does, would ensure fairness.

However, there are two crucial situations where Additive-Pegging differs from Dynkin's algorithm. The first such situation is when the candidate \(\) with maximum predicted value arrives and we have that \(\) is not the maximum so far or \(t_{i} 1/2\), i.e., \(}\) is true. In this case, we cannot always reject \(\), as Dynkin's algorithm would, because that would not guarantee smoothness. Instead, we reject \(\) only if there is a future candidate whose prediction is sufficiently high compared to \(u_{i}\). We call \(I^{}\) the set of those candidates. The main idea behind the pegged set \(I^{}\) is that it contains the last candidate to arrive who can guarantee the smoothness property, which is why we accept that candidate when they arrive. The second situation where our algorithm departs from Dynkin's algorithm is when a candidate \(i\) arrives with \(i i,i i^{}\) and we have that \(\) is true, in which case Algorithm 1 executes the if statement under the case \(}\). In this situation, we cannot always accept \(i\) as Dynkin's algorithm would, because that would again violate smoothness. Instead, we accept \(i\) only if \(u_{i}\) can be lower bounded by \(_{i}-_{t_{i}}\), noting that if conversely \(u_{i}+_{t_{i}}\) is smaller than \(_{i}\), then accepting \(i\) might be detrimental to our quest of ensuring smoothness.

```
//* The algorithm stops when it accepts a candidate by executing \( i\). *// Initialization:\(I^{}\) while agent \(i\) arrives at time \(t_{i}\)do if\(i I^{}\)then if\(|I^{}|=1\)then \( i\) else \(I^{} I^{}\{i\}\) \((u_{i}>_{j i}u_{j})(t_{i}>1/2)\,,(i=i)\,,_{t_{i}}_{j t_{j} t_{i}}| _{j}-u_{j}|\) if\(\)then \( i\) else if\(}\)then \(I^{}\{j i:u_{i}<_{j}+_{t_{i}}\}\) (note that \(=i\)) if\(I^{}=\)then \( i\) else if\(}\)then if\(u_{i}>_{i}-_{t_{i}}\)then \( i\)
```

**Algorithm 1** Additive-Pegging

Analysis of the Additive-Pegging algorithm.**Lemma 1.: Additive-Pegging _satisfies \(u_{}\; u_{i^{*}}-4\,(),\;\) with probability 1._

Proof.: Let \(i^{}\) denote the last arriving candidate in \(I^{}\).

We first argue that Pegging always accepts a candidate irrespective of the random arrival times of the candidates. We focus on any instance where Additive-Pegging does not accept a candidate until time \(t_{i}\). At time \(t_{i}\) either \(\) or \(}\) are true. Since in the former case candidate \(\) is accepted, we focus on the latter case and in particular whenever the set \(I^{}\) which is computed is non-empty (otherwise, candidate \(\) is accepted). In that case, it is guaranteed that by time \(t_{i^{}}\) Additive-Pegging will accept a candidate.

We now argue that in all cases Additive-Pegging maintains smoothness. Using \(\), \(_{i}\) definitions and the fact that \(\) is the candidate with the maximum predicted value we have: \(_{i}_{i^{*}} u_{i^{*}}-_{i^{*}} u_{i^{*}}-\). If candidate \(\) is accepted then using the latter lower bound we get \(u_{i}_{i}-_{i} u_{i^{*}}--_{i} u _{i^{*}}-2\,\). If we accept \(i i\) and the if statement of \(}\) is executed at time \(t_{i}\) then we have \(u_{i}>_{i}-_{t_{i}} u_{i^{*}}--_{t _{i}} u_{i^{*}}-2\,\). Finally, we need to lower bound the value \(u_{i^{}}\) in case our algorithm terminates accepting \(i^{}\). Note that from the way the pegged set \(P^{}\) is updated when \(}\) is true we always have \(u_{i}<_{}+_{t_{i}}\). Since \(u_{i}_{}-_{i}\) we can conclude that \(u_{i}>u_{i}-_{t_{i}}-_{i}  u_{i^{*}}-4\,\). 

**Lemma 2**.: Additive-Pegging _satisfies \(P[=i^{*}] 1/16,\ \)._

Proof.: In the following, we assume that the number of candidates is larger or equal to \(3\). The proof for the case where \(n=2\) is almost identical while the fairness guarantee in that case is \(1/4\). We denote by \(\) the index of the candidate with the highest true value except \(i^{*}\) and \(\), i.e., \(=*{argmax}_{i i^{*},}u_{i}\). Note that depending on the value of \(u_{}\), \(\) might denote the index of the candidate with the second or third highest true value. To prove fairness we distinguish between two cases: either \(=i^{*}\) or \( i^{*}\). For each of those cases, we define an event and argue that: (1) the event happens with constant probability, and (2) if that event happens then Additive-Pegging accepts \(i^{*}\).

If \(i^{*}=\) we define event \(E=\{t_{}<1/2<t_{i^{*}}\}\) for which \(P[E]=1/4\). \(E\) implies that our algorithm does not accept any candidate until time \(t_{i^{*}}\). Indeed, note that at any point in time before \(t_{i^{*}}\), both literals \(\) and \(\) are simultaneously false. On the contrary, at time \(t_{i^{*}}\), both \(\) and \(\) are true and our algorithm accepts \(i^{*}\).

On the other hand, if \(i^{*}\) we distinguish between two sub-cases. First, we show that either \(u_{}<_{i^{*}}+_{}\) or \(u_{i^{*}}>_{}-_{i^{*}}\) is true. By contradiction, assume that both inequalities do not hold, then

\[u_{}_{i^{*}}+_{}^{*}}>u_{}}u_{i^{*}}>_{^{*}}+ _{} u_{^{*}}-_{ ^{*}}>_{}^{*}} u_{ ^{*}}-_{^{*}}}_{i^{*}}>_{}\]

\[u_{i^{*}}_{}-_{i^{*}}^{*}}>u_{}}u_{}<_{}- _{i^{*}}_{i^{*}}<_{}-u_{} ^{*}}}_{i^{*}}<_{}\]

which is a contradiction. We now define two events \(E_{1}\) and \(E_{2}\) which imply that \(i^{*}\) is always accepted whenever \(\{u_{}<_{^{*}}+_{}\}\) and \(\{u_{^{*}}>_{}-_{i^{*}}\}\) are true respectively.

If \(u_{}<_{^{*}}+_{}\), then we define event \(E_{1}=\{t_{}<1/2\}\{t_{}<1/2\}\{1/2<t_{i^ {*}}\}\) which is composed by \(3\) independent events and it happens with probability \(P[E_{1}]=1/2^{3}=1/8\). \(E_{1}\) implies that \(t_{i}<t_{i^{*}}_{t_{i^{*}}}_{}\), thus we can deduce that \(u_{i^{*}}>u_{}_{}-_{} _{}-_{i_{*}}\). Consequently, if until time \(t_{i^{*}}\) all candidates are rejected, \(E_{1}\) implies that \(}\{u_{^{*}}>_{ }-_{i^{*}}\}\) is true at time \(t_{i^{*}}\) and candidate \(i^{*}\) is hired. To argue that no candidate is accepted before time \(t_{i^{*}}\), note that \(\) is false at all times before \(t_{i^{*}}\) and at time \(t_{i}\) (when literal \(\) is true) the set \(\{j>:u_{}<_{}+_{} \}\{j>:u_{}<_{}+_{ }\}\) contains \(i^{*}\).

If \(u_{i^{*}}>_{}-_{i^{*}}\), then we define \(E_{2}=\{t_{}<1/2<t_{i^{*}}<t_{i}\}\) which happens with probability

\[P[E_{2}] =P[t_{}<1/2] P[1/2<t_{i^{*}}<t_{i}]\] \[=P[t_{}<1/2] P[1/2<\{t_{^{*}},t _{}\}\{t_{^{*}},t_{}\}=t_{i^{*}}]\] \[=P[t_{}<1/2] P[1/2<\{t_{^{*}},t _{}\}] P[\{t_{i^{*}},t_{}\}=t_{i^{*}}]\] \[=(1/2)(1/4)(1/2)=1/16\]

Note that until time \(t_{i^{*}}\) no candidate is accepted since \(\) and \(\) are both false at all times. Indeed, between times \(0\) and \(1/2\) only \(\) could have been accepted but its arrival time is after \(t_{i^{*}}\), and between times \(1/2\) and \(t_{i^{*}}\) no candidate has a true value larger than \(u_{}\). Finally, note that at time \(t_{i^{*}}\) we have \(_{t_{i^{*}}}_{i^{*}}\) and consequently \(}\{u_{^{*}}>_{ }-_{t_{i^{*}}}\}\) is true and \(i^{*}\) gets accepted. 

**Theorem 3**.: Additive-Pegging _satisfies smoothness and fairness with \(C=4\) and \(F=1/16\)._

Theorem 3 follows directly from Lemmas 1 and 2. We note that Lemma 1 actually implies a stronger notion of smoothness that holds with probability 1.

**The general Pegging algorithm.** In Appendix A.2 we generalize the Additive-Pegging algorithm to the Pegging algorithm to provide fair and smooth algorithms for different prediction error definitions. Additive-Pegging is an instantiation of Pegging when the prediction error is defined as the maximum absolute difference between true and predicted values among candidates. To further demonstrate the generality of Pegging, we also instantiate it over the same prediction error definition \(()=_{i}|1-_{}/u_{}|\) as in  and recover similar smoothness bounds while also ensuring fairness. We name the latter instantiation Multiplicative-Pegging and present its guarantees in Theorem 4.

**Theorem 4**.: _Let \(()=_{i}|1-_{i}/u_{i}|\) and assume \(u_{i},_{i}>0\)\( i[n]\). Then Multiplicative-Pegging satisfies fairness with \(F=1/16\) and selects a candidate \(\) such that \(u_{} u_{i^{*}}(1-4())\) with probability 1._

Fujii and Yoshida  define the prediction error as in Theorem 4 and design an algorithm that accepts a candidate with expected value at least \(u_{i^{*}}\{(1-)/(1+),0.215\}\). Since \((1-)/(1+) 1-2\,\) their algorithm satisfies a smoothness desideratum similar to the one in Theorem 4, but as we prove in Appendix A.1, it violates the fairness desideratum.

## 4 Extension: \(k\)-Secretary problem with predictions

We consider the generalization to the \(k\)-secretary problem, where \(k 1\) candidates can be accepted. To simplify notation we label the candidates in decreasing order of predicted value, so that \(_{1}>>_{n}\) and denote \(r_{}\) to be the index of the candidate with the \(\)'th highest true value so that \(u_{r_{1}}>>u_{r_{n}}\). The prediction error is again defined as \(():=_{i}|u_{i}-_{i}|\) and we let \(S\) denote the random set of candidates accepted by a given algorithm on a fixed instance. The extension of our two objectives to this setting is

\[\![_{i S}u_{i}] _{=1}^{k}u_{r_{}}-C(),\ \] (smoothness for \[k\] -secretary) \[P[r_{} S]  F_{},\ =1,,k,\ \ .\] (fairness for \[k\] -secretary)

The smoothness desideratum compares the expected sum of true values accepted by the algorithm to the sum of the \(k\) highest true values that could have been accepted. The fairness desideratum guarantees each of the candidates ranked \(=1,,k\) to be accepted with probability \(F_{}\). The \(k\)-secretary problem with predictions has been studied by Fujii and Yoshida , who derive an algorithm satisfying \(\![_{i S}u_{i}]\{1-O( k/),1-O (_{i}|1-_{i}/u_{i}|)\}_{=1}^{k}u_{r_{}}\) but without any fairness guarantees. We derive an algorithm \(k\)-Pegging that satisfies the following.

**Theorem 5**.: \(k\)-Pegging _satisfies smoothness and fairness for \(k\)-secretary with \(C=4k\) and \(F_{}=\{(1/3)^{k+5},\}\) for all \(=1,,k\)._

We note that the algorithm of Kleinberg  for \(k\)-Secretary (without predictions) obtains in expectation at least

\[(1-})_{=1}^{k}u_{r_{}}_{=1 }^{k}u_{r_{}}-5_{i}u_{i},\]

which has a better asymptotic dependence on \(k\) than our smoothness constant \(C=4k\) if the prediction error is relatively large, i.e., \(()=(_{i}u_{i}/)\). On the other hand, regarding our fairness guarantee, if one only considers our fairness desideratum for \(k\)-secretary, then a simple algorithm suffices to achieve \(F_{}=1/4\) for all \(=1,,k\), namely: reject all candidates \(i\) with \(t_{i}<1/2\); accept any candidate \(i\) with \(t_{i}>1/2\) whose true value \(u_{i}\) is among the \(k\) highest true values observed so far, space permitting. For any of the top \(k\) candidates, i.e., candidate \(r_{}\) with \([k]\), their value \(u_{r_{}}\) is always greater than the threshold \(\), which our algorithm recomputes upon the arrival of each candidate. Consequently, candidate \(r_{}\) is added to our solution if the following conditions are satisfied: (1) \(t_{r_{}}>1/2\); and (2) there is space available in the solution when \(r_{}\) arrives. For condition (2) to hold, it suffices that at least \(k\) of the \(2k-1\) candidates with the highest values other than \(r_{}\) arrive before time \(1/2\). This ensures that at most \(k-1\) candidates other than \(r_{}\) can be accepted after time \(1/2\). The probability of both conditions (1) and (2) being satisfied is at least \(P\![t_{r_{}}>1/2(2k-1,)]=P\! [t_{r_{}}>1/2] P\![(2k-1,)  k]==\), establishing that \(F_{}=\) for all \(=1,,k\).

Assuming \(k\) is a constant, \(C\) and \(F_{1},,F_{k}\) in Theorem 5 are constants that do not depend on \(n\) or the instance \(\). For large values of \(k\) the first term in \(F_{}\) is exponentially decaying, but the second term still guarantees candidate \(r_{}\) a probability of acceptance that is independent of \(k\) as long as \((+13)/k\) is bounded away from 1. More precisely, for \(k 52\) and \(l k/2\) we have that candidate \(r_{}\) is accepted with probability at least \(\), i.e., every candidate among the top \(k/2\) is acceptedwith probability at least \(\), thus \(_{i S}u_{i}_{t=1}^{k/2}u_{r_{t }}_{t=1}^{k}u_{r_{t}}\). This implies a multiplicative guarantee on total value that does not depend on \(k\) when \(k\) is large.

**The algorithm.** While we defer the proof of Theorem 5 to Appendix B, we present the intuition and the main technical difficulties in the design of \(k\)-Pegging. The algorithm maintains in an online manner the following sets: (1) the solution set \(S\) which contains all the candidates that have already been accepted; (2) a set \(H\) that we call the "Hopefuls" and contains the \(k-|S|\) future candidates with highest predicted values; (3) a set \(B\) that we call the blaming set, which contains a subset of already arrived candidates that pegged a future candidate; and (4) the set \(P\) of pegged elements which contains all candidates that have been pegged by a candidate in \(B\). In addition, we use function \(\) to store the "pegging responsibility", i.e., if \((i)=j\), for some candidates \(i,j\) where \(i\) had one of the \(k\) highest predicted values, then \(i\) was not accepted at the time of its arrival and pegged \(j\). We use \(^{-1}(j)=i\) to denote that \(j\) was pegged by \(i\).

```
//* The algorithm stops when it accepts \(k\) candidates, i.e., when \(|S|=k\). *// Initialization:\(H[k],S,P,B\) while agent \(i\) arrives at time \(t_{i}\)do\(\) Case 1 if\(i P\)then  Add \(i\) to \(S\), remove \(i\) from \(P\), and remove \(^{-1}(i)\) from \(B\) \( k^{th}\) highest true value strictly before \(t_{i}\) \((u_{i}>)(t_{i}>1/2), (i H)\), \(_{t_{i}}_{j:t_{j} t_{i}}_{j}-u_{j}\) if\(\)then  Add \(i\) to \(S\) and remove \(i\) from \(H\) elseif\(C}\)then \(\) Case 3 if\(\{j i:u_{i}<_{j}+_{t_{i}}\}(P[k] )=\)then \(\) subcase a  Add \(i\) to \(S\) and remove \(i\) from \(H\) else \(\) subcase b \(j^{}\) An arbitrary candidate from \(\{j i:u_{i}<_{j}+_{t_{i}}\}(P[k] )\)  Add \(j^{}\) to \(P\), and \(i\) to \(B\), remove \(i\) from \(H\), and set \((i)=j^{}\) elseif\(C\)then \(\) Case 4 if\(\{j B:u_{i}>u_{j}\}\)then \(\) subcase a \(j^{}\) An arbitrary candidate from \(\{j B:u_{i}>u_{j}\}\) Add \(i\) to \(S\), remove \(j^{}\) from \(B\), and remove \((j^{})\) from \(P\) elseif\(\{j H:u_{i}>_{j}-_{t_{i}}\}\)then \(\) subcase b \(j^{}\) An arbitrary candidate from \(\{j H:u_{i}>_{j}-_{t_{i}}\}\) Add \(i\) to \(S\) and remove \(j^{}\) from \(H\)
```

**Algorithm 2**\(k\)-Pegging

To satisfy the fairness property, we check if the current candidate \(i\) has arrived at time \(t_{i}>1/2\) and if \(u_{i}\) is larger than the \(k^{th}\) highest value seen so far. We refer to these two conditions as the fairness conditions. If \(i P\) (_case I_) or \(i H\) and the fairness conditions hold (_case 2_), then we accept \(i\). If the fairness conditions hold but \(i H\) then we accept if there is a past candidate in \(B\) with lower true value than \(u_{i}\) (_subcase 4a_), or a future candidate in \(H\) with low predicted value compared to \(u_{i}\) (_subcase 4b_). The main technical challenge in generalizing the pegging idea to \(k>1\) arises when a candidate \(i H\) arrives, but the fairness conditions do not hold (_case 3_). In this situation, it is unclear whether to reject \(i\) and peg a future candidate, or accept \(i\). For instance, consider a scenario where the prediction error is consistently large (i.e., \(_{t_{i}}\) is always large), such that when \(i\) arrives, the set \(\{j i:u_{i}<_{j}+_{t_{i}}\}(P[k])\) is always non-empty. If \(t_{i}<1/2\) and we accept \(i\), we risk depining our budget too quickly before time \(1/2\), leaving insufficient capacity to accept candidates not in \([k]\) who arrive later. Conversely, if we reject \(i\), we deny it the possibility of acceptance in the first half of the time horizon, potentially reducing its overall acceptance probability. \(k\)-Pegging balances this tradeoff while achieving smoothness. To establish smoothness, we demonstrate that the \(k\) candidates with the highest predicted values can be mapped to the solution set \(S\), ensuring that the true values within our solution set are pairwise "close" to the values of candidates in \(\{1,2,,k\}\). This is proven in Lemma 8 by constructing an injective function \(()\) from set \(S\) to \(\{1,2,,k\}\) such that for each \(j S\), \(u_{j} u_{(j)}\).

Experiments

We simulate our Additive-Pegging and Multiplicative-Pegging algorithms in the exact experimental setup of Fujii and Yoshida , to test its average-case performance.

**Experimental Setup.** Fujii and Yoshida  generate various types of instances. We follow their Almost-constant, Uniform, and Adversarial types of instances, and also create the Unfair type of instance to further highlight how slightly biased predictions can lead to very unfair outcomes. Both true and predicted values of candidates in all these instance types are parameterized by a scalar \([0,1)\) which controls the prediction error. Setting \(=0\) creates instances with perfect predictions and setting a higher value of \(\) creates instances with more erroneous predictions. Almost-constant models a situation where one candidate has a true value of \(1/(1-)\) and the rest of the candidates have a value of \(1\). All predictions are set to \(1\). In Uniform, we sample each \(u_{i}\) independently from the exponential distribution with parameter \(1\). The exponential distribution generates a large value with a small probability and consequently models a situation where one candidate is significantly better than the rest. All predicted values are generated by perturbing the actual value with the uniform distribution, i.e., \(_{i}=_{i} u_{i}\), where \(_{i}\) is sampled uniformly and independently from \([1-,1+]\). In Adversarial, the true values are again independent samples from the exponential distribution with parameter \(1\). The predictions are "adversarially" perturbed while maintaining the error to be at most \(\) in the following manner: if \(i\) belongs to the top half of candidates in terms of true value, then \(_{i}=(1-) u_{i}\); if \(i\) belongs to the bottom half, then \(_{i}=(1+) u_{i}\). Finally, in Unfair all candidates have values that are at most a \((1+)\) multiplicative factor apart. Formally, \(u_{i}\) is a uniform value in \([1-/4,1+/4]\), and since \((1+/4)/(1-/4)(1+)\) we have that the smallest and largest value are indeed very close. We set \(_{i}=u_{n-r(i)+1}\) where \(r(i)\) is the rank of \(u_{i}\), i.e., predictions create a completely inverted order.

We compare Additive-Pegging and Multiplicative-Pegging against Learned-Dynkin , Highest-prediction which always accepts the candidate with the highest prediction, and the classical Dynkin algorithm which does not use the predictions. Following , we set the number of candidates to be \(n=100\). We experiment with all values of \(\) in \(\{0,1/20,2/20,,19/20\}\). For each type of instance and value of \(\) in this set, we randomly generate 10000 instances, and then run each algorithm on each instance. For each algorithm, we consider instance-wise the ratio of the true value it accepted to the maximum true value, calling the average of this ratio across the 10000 instances its _competitive ratio_. For each algorithm, we consider the fraction of the 10000 instances on which it successfully accepted the candidate with the highest true value, calling this fraction its _fairness_. We report the competitive ratio and fairness of each algorithm, for each type of instance and each value of \(\), in Figure 1. Our code is written in Python 3.11.5 and we conduct experiments on an M3 Pro CPU with 18 GB of RAM. The total runtime is less than \(5\) minutes.

**Results.** The results are summarized in figure 1. Since Additive-Pegging and Multiplicative-Pegging achieve almost the same competitive ratio and fairness for all instance types and values of \(\) we only present Additive-Pegging in figure 1 but include the code of both in the supplementary material. Our algorithms are consistently either the best or close to the best in terms of both competitive ratio and fairness for all different pairs of instance types and \(\) values. Before discussing the results of each instance type individually it is instructive to mention some characteristics of our benchmarks. While Dynkin does not use predictions and is therefore bound to suboptimal competitive ratios when predictions are accurate, we note that it accepts the maximum value candidate with probability at least \(1/e\), i.e., it is \(1/e\)-fair. When predictions are non-informative this is an upper bound on the attainable fairness for any algorithm whether it uses predictions or not. Highest-prediction is expected to perform well when the highest prediction matches the true highest value candidate and poorly when the latter is not true. In Almost-constant for small values of \(\) all candidates have very close true values and all algorithms except Dynkin have a competitive ratio close to \(1\). Dynkin may not accept any candidate and this is why its performance is poorer than the rest of the algorithms. Note that as \(\) increases both our algorithms perform significantly better than all other benchmarks.

In terms of fairness, predictions do not offer any information regarding the ordinal comparison between candidates' true values and this is why for small values of \(\) the probability of Highest-prediction and Learned-Dynkin of accepting the best candidate is close to \(1/100=1/n\), i.e., random. Here, the fairness of our algorithms and Dynkin is similar and close to \(1/e\). In both Uniform and Adversarial we observe that for small values of \(\) the highest predicted candidate is the true highest and Additive-Pegging, Learned-Dynkin and Highest-prediction all accept that candidate having a very close performance both in terms of fairness and competitive ratio. For higher values of \(\) the fairness of those algorithms deteriorates similarly and it approaches again \(0.37 1/e\). In Unfair our algorithms outperform all other benchmarks in terms of competitive ratio for all values of \(\) and achieve a close to optimal fairness. This is expected as our algorithms are particularly suited for cases where predictions may be accurate but unfair.

Overall, our algorithms are the best-performing and most robust. The Highest-prediction algorithm does perform slightly better on Uniform instances and Adversarial instances under most values of \(\), but performs consistently worse on Almost-constant and Unfair instances, especially in terms of fairness. Our algorithms perform better than Learned-Dynkin in almost all situations.

## 6 Limitations and Future Work

We study a notion of fairness that is tailored to the secretary problem with predictions and build our algorithms based on this notion. However, there are alternative notions of fairness one could consider in applications such as hiring, as well as variations of the secretary problem that capture other features in these applications. While our model allows for arbitrary bias in the predictions we assume that the true value of a candidate is fully discovered upon arrival, and define fairness based on hiring the best candidate (who has the highest true value) with a reasonable probability. Thus, we ignore considerations such as bias in how we get the true value of a candidate (e.g., via an interview process). In addition, as noted in Section 1, we use an _individual fairness_ notion which does not model other natural desiderata like hiring from underprivileged populations or balance the hiring probabilities across different populations. These are considerations with potentially high societal impact which our algorithms do not consider and are interesting directions for future work on fair selection with predictions.

Regarding trade-offs in our guarantees: for the single-secretary problem, we can improve the fairness guarantee from \(1/16\) to \(0.074\) by optimizing the constants in our algorithm. However, we choose not to do so, as the performance increase is marginal, and we aim to keep the proof as simple as possible. Additionally, as we noted in Section 1.1 any constant \(C\) for smoothness implies an upper bound of \(F=1/3+o(1)\) for fairness. Finding the Pareto-optimal curve in terms of smoothness and fairness is an interesting direction. The main challenge in achieving a smooth trade-off between fairness and smoothness is as follows: any bound on \(C\) for smoothness implies a competitive ratio of \(1-C\), which reaches a ratio of 1 when the predictions are exactly correct. Thus, regardless of the smoothness guarantee, we must achieve a competitive ratio of 1 when predictions are fully accurate. This constraint makes it challenging to improve the fairness guarantee \(F\), even at the cost of a less favorable smoothness constant \(C\).

Figure 1: Competitive ratio and fairness of different algorithms, for each instance type and level of \(\).