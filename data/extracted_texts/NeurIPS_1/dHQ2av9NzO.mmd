# On the Convergence of

Black-Box Variational Inference

Kyurae Kim

University of Pennsylvania

kyrkim@seas.upenn.edu

&Jisu Oh

North Carolina State University

joh26@ncsu.edu

&Kaiwen Wu

University of Pennsylvania

kaiwenwu@seas.upenn.edu

&Yi-An Ma

University of California, San Diego

yianma@ucsd.edu

&Jacob R. Gardner

University of Pennsylvania

jacobrg@seas.upenn.edu

###### Abstract

We provide the first convergence guarantee for black-box variational inference (BBVI) with the reparameterization gradient. While preliminary investigations worked on simplified versions of BBVI (_e.g._, bounded domain, bounded support, only optimizing for the scale, and such), our setup does not need any such algorithmic modifications. Our results hold for log-smooth posterior densities with and without strong log-concavity and the location-scale variational family. Notably, our analysis reveals that certain algorithm design choices commonly employed in practice, such as nonlinear parameterizations of the scale matrix, can result in suboptimal convergence rates. Fortunately, running BBVI with proximal stochastic gradient descent fixes these limitations and thus achieves the strongest known convergence guarantees. We evaluate this theoretical insight by comparing proximal SGD against other standard implementations of BBVI on large-scale Bayesian inference problems.

## 1 Introduction

Despite the practical success of black-box variational inference (BBVI; Kucukelbir _et al._, 2017; Ranganath _et al._, 2014; Titsias & Lazaro-Gredilla, 2014), also known as stochastic gradient variational Bayes and Monte Carlo variational inference, whether it converges under appropriate assumptions on the target problem have been an open problem for a decade. While our understanding of BBVI has been advancing (Bhatia _et al._, 2022; Challis & Barber, 2013; Domke, 2019, 2020; Hoffman & Ma, 2020), a full convergence guarantee that extends to the practical implementations as used in probabilistic programming languages (PPL) such as Stan (Carpenter _et al._, 2017), Turing (Ge _et al._, 2018), Tensorflow Probability (Dillon _et al._, 2017), Pyro (Bingham _et al._, 2019), and PyMC (Patil _et al._, 2010) has yet to be demonstrated.

Due to our lack of understanding, a consensus on how we should implement our BBVI algorithms has yet to be achieved. For example, when the variational family is chosen to be the location-scale family, the "scale" matrix can be parameterized linearly or nonlinearly, and both parameterizations are used by default in popular software packages. (See Table 1 in Kim _et al._2023.) Surprisingly, as we will show, seemingly innocuous design choices like these can substantially impact the convergence of BBVI. This is critical as BBVI has been shown to be less robust (_e.g._, sensitive to initial points, stepsizes, and such) than competing inference methods such as Markov chain Monte Carlo (MCMC). (See Dhaka _et al._, 2020; Domke, 2020; Welandawe _et al._, 2022; Yao _et al._, 2018.) Instead, the evaluation of BBVI algorithms has been relying on expensive empirical evaluations (Agrawal _et al._, 2020; Dhaka _et al._, 2021; Giordano _et al._, 2018; Yao _et al._, 2018).

To rigorously analyze the design of BBVI algorithms, we establish the first convergence guarantee for the implementations _precisely_ as used in practice. We provide results for BBVI with the reparameterization gradient (RP; Kingma and Welling, 2014; Titsias and Lazaro-Gredilla, 2014) and the location-scale variational family, arguably the most widely used combination in practice. Our results apply to log-smooth posteriors, which is a routine assumption for analyzing the convergence of stochastic optimization (Garrigos and Gower, 2023) and sampling algorithms (Dwivedi et al., 2019, SS2.3). The key is to show that evidence lower bound (ELBO; Jordan et al., 1999) satisfies regularity conditions required by convergence proofs of stochastic gradient descent (SGD; Bottou, 1999; Nemirovski et al., 2009; Robbins and Monro, 1951), the workhorse underlying BBVI.

Our analysis reveals that nonlinear scale matrix parameterizations used in practice are suboptimal: they provably break strong convexity and sometimes even convexity. Even if the posterior is strongly log-concave, the ELBO is not strongly convex anymore. This contrasts with linear parameterizations, which guarantee the ELBO to be strongly convex if the posterior is strongly log-concave (Domke, 2020). Under linear parameterizations, however, the ELBO is no longer smooth, making optimization challenging. Because of this, Domke (2020) proposed to use proximal SGD, which Agrawal and Domke (2021, Appendix A) report to have better performance than vanilla SGD with nonlinear parameterizations. Indeed, we show that BBVI with proximal SGD achieves the _fastest_ known converges rates of SGD, unlike vanilla BBVI. Thus, we provide a concrete reason for employing proximal SGD. We evaluate this insight on large-scale Bayesian inference problems by implementing an Adam-like (Kingma and Ba, 2015) variant of proximal SGD proposed by Yun et al. (2021).

Concurrently to this work, convergence guarantees on BBVI with the RP and the sticking-the-landing estimator (STL; Roeder et al., 2017) under the linear parameterization were published by Domke et al. (2023). To achieve this, they show that a quadratic bound on the gradient variance is sufficient to guarantee the convergence of projected and proximal SGD. In contrast, we focus on analyzing the ELBO under nonlinear parameterizations and connect it to existing analysis strategies. A more in-depth comparison of the two works is provided in Appendix E.

* **Convergence Guarantee for BBVI:** Theorem 3 establishes a convergence guarantee for BBVI with assumptions matching the implementations used in practice. That is, without algorithmic simplifications and unrealistic assumptions such as bounded domain or bounded support.
* **Optimality of Linear Parameterizations:** Theorem 2 shows that, for location-scale variational families, nonlinear scale parameterizations prevent the ELBO from being strongly-convex even when the target posterior is strongly log-concave.
* **Convergence Guarantee for Proximal BBVI:** Theorem 4 guarantees that, if proximal SGD is used, BBVI on \(\)-strongly log-concave posteriors can obtain a solution \(\)-close to the global optimum with \((1/)\) iterations.
* **Evaluation of Proximal BBVI in Practice:** In Section 5, we evaluate the utility of proximal SGD on large-scale Bayesian inference problems.

## 2 Background

NotationRandom variables are denoted in serif (_e.g._, \(x\), \(x\)), vectors are in bold (_e.g._, \(x\), \(x\)), and matrices are in bold capitals (_e.g._\(A\)). For a vector \(x^{d}\), we denote the inner product as \(x^{}x\) and \( x,x\), the \(_{2}\)-norm as \(\|x\|_{2}=}x}\). For a matrix \(A\), \(\|A\|_{}=(A^{}A )}\) denotes the Frobenius norm. \(^{d}_{++}\) is the set of positive definite matrices. For some function \(f\), \(_{}f\) denotes the \(i\)th coordinate of \( f\), and \(^{k}(,)\) is the set of \(k\)-time differentiable continuous functions mapping from \(\) to \(\).

### Black-Box Variational Inference

Variational inference (VI, Blei et al., 2017; Jordan et al., 1999; Zhang et al., 2019) aims to minimize the exclusive (or backward/reverse) Kullback-Leibler (KL) divergence as:

\[*{minimize}_{}\;\;_{ }(q_{},)_{x q_{ }}-(z)-(q_{}),\\ \;\;\;_{}(q_{},)\;\; \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;, one minimizes the negative _evidence lower bound_(ELBO, Jordan _et al._, 1999):

\[}{}\;\;F() _{ q_{}}- p(,)-(q_{}),\]

where \( p(,)\) is the _joint likelihood_, which is proportional to the posterior as \(() p(,)=p( {z})p()\), where \(p()\) is the likelihood and \(p()\) is the prior.

### Variational Family

In this work, we focus on the following variational family. (\(}{=}\) is equivalence in distribution.)

**Definition 1** (**Reparameterized Family**).: Let \(\) be some \(d\)-variate distribution. Then, \(q_{}\) that can be equivalently represented as

\[ q_{}}{ =}_{}();,\]

is said to be part of a reparameterized family generated by the base distribution \(\) and the reparameterization function \(_{}^{}\).

The location-scale family enables detailed theoretical analysis, as demonstrated by (Domke, 2019, Fujisawa & Sato, 2021, Kim _et al._, 2023), and includes the most widely used variational families such as the Student-t, elliptical, and Gaussian families (Titsias & Lazaro-Gredilla, 2014).

Handling Constrained SupportFor common choices of the base distribution \(\), the support of \(q_{}\) is the whole \(^{d}\). Therefore, special treatment is needed when the support of \(\) is constrained. Kucukelbir _et al._ (2017) proposed to handle this by applying diffeomorphic transformation denoted with \(\), often called _bjectors_(Dillon _et al._, 2017; Fjelde _et al._, 2020; Leger, 2023), to \(q_{}\) such that

\[ q_{,} ^{-1}(); q_{},\]

such that the support of \(q_{,}\) matches that of \(\). For example, when the support of \(\) is \(_{+}\), one can choose \(^{-1}=\). This approach, known as automatic differentiation VI (ADVI), is now standard in most modern PPLs.

Why focus on posteriors with unconstrained supports?When bijectors are used, the entropy of \(q_{}\), \((q_{})\), needs to be adjusted by the Jacobian of \(\)(Kucukelbir _et al._, 2017), \(_{^{-1}}\). However, applying the transformation to \(\) instead of \(q_{}\) is mathematically equivalent and more convenient. In fact, bijectors can be automatically incorporated into our notation by implicitly setting

\[p()=(^{-1} ()) p()= {p}(^{-1}())|}^{-1} ()|,\]

such that \(()( {})()\), where \(\) is the constrained posterior that we are actually interested in. Therefore, our setup in Section 2.1, where the domain of \(\) is taken to be the unconstrained \(^{d}\), already encompasses constrained posteriors through ADVI.

Lastly, we impose light assumptions on the base distribution \(\), which are already satisfied by most variational families used in practice. (_i.i.d._: independently and identically distributed.)

**Assumption 1** (**Base Distribution**).: \(\) is a \(d\)-variate distribution such that \(\) and \(=(u_{1},...,u_{d})\) with _i.i.d._ components. Furthermore, \(\) is **(i)** symmetric and standardized such that \(u_{l}=0\), \(u_{1}^{2}=1\), \(u_{1}^{3}=0\), and **(ii)** has finite kurtosis \(u_{1}^{4}=k_{}<\).

The assumptions on the variational family we will use throughout this work are collectively summarized in the following assumption:

**Assumption 2**.: The variational family is the location-scale family formed by Definitions 1 and 2 with the base distribution \(\) satisfying Assumption 1.

### Scale Parameterizations

For the "scale" matrix \(()\) in the location-scale family, any parameterization that results in a positive-definite covariance \(^{}_{++}^{d}\) is valid. However, for the ELBO to ever be convex, the entropy \((q_{})\) must be convex, which requires the mapping \(^{}\) to be convex. To ensure this, we restrict \(\) to (lower) triangular matrices with strictly positive eigenvalues, essentially, Cholesky factors. This leaves two of the most common parameterizations:

**Definition 3** (**Mean-Field Family.**).: \((s)}\) where the \(d\) elements of \(\) forms the diagonal and \(\) such that

\[=\{(,)\,|\,^{d},\}.\]

Here, \(S\) is discussed in the next paragraph, \((s)}^{d d}\) is a diagonal matrix such that \((s)}()= ()},\,...\,,)})\), and \(\) is a function we call a _diagonal conditioner_.

Linear v.s. Nonlinear ParameterizationsWhen the diagonal conditioner is a linear function \((x)=x\), we say that the covariance parameterization is _linear_. In this case, to ensure that \(\) is a Cholesky factor, the domain of \(\) is set as \(=_{+}^{d}\). On the other hand, by choosing a nonlinear conditioner \(:\,_{+}\), we can make the domain of \(\) to be the unconstrained \(=^{d}\). Because of this, nonlinear conditioners such as the softplus \((x)(1+(x))\)(Dugas _et al._, 2000) are frequently used in practice, especially for mean-field. (See Table 1 by Kim _et al._, 2023).

### Problem Structure of Black-Box Variational Inference

Exclusive KL minimization VI is fundamentally a composite (regularized) optimization problem

\[F()=f()+h( ),\] (ELBO)

where \(f()_{ q_{}}()\) is the _energy term_, \(()-\,p(,)\) is the negative joint log-likelihood, and \(h()-(_{, })\) is the _entropic regularizer_. From here, BBVI introduces more structure.

An illustration of the taxonomy is shown in Figure 1. In particular, BBVI has an _infinite sum_ structure (IS). That is, it cannot be represented as a sum of finite subcomponents as in ERM. Furthermore,

\[F() =_{}\,f(; )+h()\] (CP \[\] IS) \[=_{}\,(_{}())+h(),\] (CP \[\] IS \[\] RP)

where \(f(;)(_{}())\).

Theoretical ChallengesThe structure of BBVI has multiple challenges that have hindered its theoretical analysis: **(i)** the stochasticity of the Jacobian of \(\) and **(ii)** The infinite sum structure.

For Item (i), we can see that in

\[_{}(_{}() )=_{}()}{ }(_{}() )=_{}()}{ }g(;),\]

where \(g(;)(_ {})()\), both the Jacobian of \(_{}\) and the gradient of the log-likelihood, \(g\), depend on the randomness \(\). Effectively decoupling the two is a major challenge to analyzing the properties of the ELBO and its gradient estimators (Domke, 2019, 2020).

For Item (ii), the problem is that recent analyses of SGD (Garrigos & Gower, 2023; Gower _et al._, 2019; Nguyen _et al._, 2018; Vaswani _et al._, 2019) have increasingly been relying on the assumption that \(f(;)\) is smooth for all \(\) such that

\[\|_{}f(;)-_{}f (^{};)\| L\|-^ {}\|\]

for some \(L<\). This is sensible if the support of \(\) is bounded, which is true for the ERM setting but not for the class of infinite sum (IS) problems. Previous works circumvented this issue by assuming **(i)** that the support of \(\) is bounded (Fujisawa & Sato, 2021) which implicitly changes the variational family, or **(ii)** that the gradient \( f\) is bounded by a constant (Buchholz _et al._, 2018; Liu & Owen, 2021) which contradicts strong convexity (Nguyen _et al._, 2018).

Figure 1: **Taxonomy of variational inference**. Within BBVI, this work only considers the reparameterization gradient (BBVI \(\) RP, shown in **dark red**). This leaves out BBVI with the score gradient (BBVI \(\) RP, shown in **light red**). The set VI \(\) FS includes sparse variational Gaussian processes (Titsias, 2009), while the remaining set VI \(\) (FS \(\) IS \(\) RP) includes coordinate ascent VI (Blei _et al._, 2017).

The Evidence Lower Bound Under Nonlinear Scale Parameterizations

Under the linear parameterization (\((x)=x\)), the properties of the ELBO, such as smoothness and convexity, have been previously analyzed by Challis and Barber (2013); Domke (2020); Titsias and Lazaro-Gredilla (2014). We generalize these results to nonlinear conditioners.

### Technical Assumptions

Let \(g_{i}(;u)\) be the \(i\)th coordinate of \(g(;u)\) and recall that \(u_{i}\) denote the \(i\)th element of \(u\). Establishing convexity and smoothness of the ELBO under nonlinear parameterizations depends on a pair of necessary and sufficient assumptions. To establish smoothness:

**Assumption 3.** The gradient of \(\) under reparameterization, \(g\), satisfies

\[|g_{i}(;u)\,u_{i}^{*}(s_{i})| L_{s}\]

for every coordinate \(i=1,...\,d\), any \(\), and some \(0<L_{s}<\).

Here, \(^{*}\) is the second derivative of \(\). The next one is required to establish convexity:

**Assumption 4.** The gradient of \(\) under reparameterization, \(g\), satisfies

\[g_{i}(;u)\,u_{i} 0\]

for every coordinate \(i=1,...\,d\).

Intuitively, these assumption control how much \(\) and \(_{}\)_rotate_ the randomness \(u\). (Notice that the assumptions are closely related to the matrix \((g(;u),u)\), the covariance between \(g\) and \(u\).) However, the peculiar aspect of these assumptions is that they are not implied by the convexity and smoothness of \(\). Especially, Assumption 3 strongly depends on the internals of \(\).

### Smoothness of the Entropy

Under the linear parameterization, Domke (2020) has previously shown that the entropic regularizer term \(h\) is not smooth. This fact immediately implies the ELBO is not smooth. However, certain nonlinear conditioners do result in a smooth regularizer.

**Lemma 1.**_If the diagonal conditioner \(\) is \(L_{h}\)-log-smooth, then the entropic regularizer \(h()\) is \(L_{h}\)-smooth._

_Proof._ See the _full proof_ in page 24.

**Example 1.** The following diagonal conditioners result in a smooth entropic regularizer:

1. Let \((x)=(x)\). Then, \(h\) is \(L_{h}\)-smooth with \(L_{h} 0.167096\).
2. Let \((x)=(x)\). Then, \(h\) is \(L_{h}\)-smooth for arbitrarily small \(L_{h}\).

This might initially suggest that diagonal conditioners are a promising way of making the ELBO globally smooth. Unfortunately, the properties of the _energy_, \(f\), change unfavorably.

### Smoothness of the Energy

Inapplicability of Existing Proof StrategyPreviously, Domke (2020, Theorem 1) have proven that the energy is smooth when \(\) is linear. The key step was to use Bessel's inequality based on the observation that the partial derivatives of the reparameterization function \(\) form unit bases in expectation. That is,

\[(_{}(u)}{_{i}}, _{}(u)}{_{j}})= _{i=j},\]

where \(_{i=j}\) is an indicator function that is \(1\) only when \(i=j\) and \(0\) otherwise.

Unfortunately, when \(\) is nonlinear, the partial derivatives \(_{}(u)_{i}\) for \(i=1,...\,p\) no longer form unit bases: while they are still orthogonal in expectation, the _lengths_ change nonlinearly depending on \(\). This leaves Bessel's inequality inapplicable. To circumvent this challenge, we establish a replacement for Bessel's inequality:

**Lemma 2.**_Let \(H\) be a \(n n\) symmetric random matrix, where it is bounded as \(\|H\|_{2} L<\) almost surely. Also, let \(J\) be an \(m n\) random matrix such that \(\|J^{}J\|_{2}<\). Then,_

\[\|J^{}HJ\|_{2} L\|J^{ }J\|_{2}.\]

_Proof._ See the _full proof_ in page 24.

**Remark 1.** By assuming that the joint log-likelihood \(\) is smooth and twice-differentiable, we retrieve Theorem 1 of Domke (2020) by setting \(J\) to be the Jacobian of \(\), and \(H\) to be the Hessian of \(\) under reparameterization.

**Remark 2**.: While our reparameterization function's partial derivatives still form orthogonal bases, they need not be; unlike Bessel's inequality, Lemma 2 does not require this. This implies that Lemma 2 is a strategy more general than Bessel's inequality.

Equipped with Lemma 2, we present our main result on smoothness:

**Theorem 1**.: _Let \(\) be \(L_{}\)-smooth and twice differentiable. Then, the following results hold:_

1. _If_ \(\) _is linear, the energy_ \(f\) _is_ \(L_{}\)_-smooth._
2. _If_ \(\) _is 1-Lipschitz, the energy_ \(\) _is_ \((L_{}+L_{})\)_-smooth if and only if Assumption_ 3 _holds._

Proof.: See the _full proof_ in page 27.

Combined with Lemma 1, this directly implies that the overall ELBO is smooth.

**Corollary 1** (Smoothness of the ELBO).: _Let \(\) be \(L_{}\)-smooth and Assumption 3 hold. Furthermore, let the diagonal conditioner be 1-Lipschitz continuous, and \(L_{}\)-log-smooth. Then, the ELBO is \((L_{}+L_{}+L_{})\)-smooth._

The increase of the smoothness constant implies that we need to use a smaller stepsize to guarantee convergence when using a nonlinear \(\). Furthermore, even on simple \(L\)-smooth examples Assumption 3 may not hold:

**Example 2**.: Let \(()=(}{{2}})\,^{} \) and the diagonal conditioner be \((x)=(x)\). Then,

1. if \(\) is dense and the variational family is the mean-field family or
2. if \(\) is diagonal and the variational family is the Cholesky family,

Assumption 3 holds with \(L_{} 0.26034(_{i=1,,d}A_{ii})\).
3. if \(\) is dense but the Cholesky family is used, Assumption 3 does not hold.

Proof.: See the _full proof_ in page 29.

Example 2 illustrates that establishing the smoothness of the energy becomes non-trivial under nonlinear parameterizations. Even when smoothness does hold, the increased smoothness constant implies that BBVI will be less robust to initialization and stepizes. Furthermore, in the next section, we will show a much more grave problem: nonlinear parameterizations may affect the convergence _rate_.

### Convexity of the Energy

The convexity of the ELBO under linear parameterizations has first been established by Titsias and Lazaro-Gredilla (2014, Proposition 1) and Domke (2020, Theorem 9). In particular, Domke (2020) show that, when \(\) is linear, if \(\) is \(\)-strongly convex, the energy is also \(\)-strongly convex. However, when using a nonlinear \(\) with a co-domain of \(_{+}\), which is the whole point of using a nonlinear conditioner, strong convexity of \(\)_never_ transfers to \(f\).

**Theorem 2**.: _Let \(\) be \(\)-strongly convex. Then, we have the following:_

1. _If_ \(\) _is linear, the energy_ \(f\) _is_ \(\)_-strongly convex._
2. _If_ \(\) _is convex, the energy_ \(f\) _is convex if and only if Assumption_ 4 _holds._
3. _If_ \(\) _is such that_ \(^{1}(,_{+})\)_, the energy_ \(f\) _is not strongly convex._

Proof.: See the _full proof_ in page 33.

The following proposition provides some conditions for Assumption 4 to hold or not hold.

**Proposition 1**.: _We have the following:_

1. _If_ \(\) _is convex, then for the mean-field family, Assumption_ 4 _holds._
2. _For the Cholesky family, there exists a convex_ \(\) _where Assumption_ 4 _does not hold._

Proof.: See the _full proof_ in page 31.

Figure 2: **Optimization landscape resulting from different \(\) on a strongly-convex \(\). \(\) is the counter-example of Proposition 1 Item (ii). \((x)=x\) preserves strong convexity as shown by the lower-bounding quadratic (red dotted line \(\)). \(=\) violates the first-order condition of convexity (black dotted line \(\)).**For any continuous, differentiable nonlinear conditioner that maps only to non-negative reals, the strong convexity of \(\) does lead to a strongly-convex ELBO. This phenomenon is visualized in Figure 2. The loss surface becomes flat near the optimal scale parameter. This problem becomes more noticeable as the optimal scale becomes smaller.

Nonlinear conditioners are suboptimal.As the dataset grows, Bayesian posteriors are known to "contract" as characterized by the Bernstein-von Mises theorem (van der Vaart, 1998). That is, the posterior variance becomes close to \(0\). This behavior also applies to misspecified variational posteriors as shown by Wang and Blei (2019). Thus, for large datasets, nonlinear conditioners mostly operate in the regime where they are suboptimal (locally less strongly convex). But linear conditioners result in a non-smooth entropy (Domke, 2020). This dilemma originally motivated Domke to consider proximal SGD, which we analyze in Section 4.2.

## 4 Convergence Analysis of Black-Box Variational Inference

### Black-Box Variational Inference

BBVI with SGD repeats the steps:

\[_{t+1}=_{t}-_{t}((_{t} )+ h(_{t})), { f}(_{t})=_{m=1}^{M}_{} (_{}(_{m})) \]

with \(_{m}\) is the \(M\)-sample reparameterization gradient estimator and \(_{t}\) is the stepsize. (See Kucukelbir et al., 2017 for algorithmic details.)

With our results in Section 3 and the results of Khaled and Richtarik (2023); Kim et al. (2023), we obtain a convergence guarantee. To apply the result of Kim et al. (2023), which bounds the gradient variance, we require an additional assumption.

**Assumption 5**.: The negative log-likelihood \(_{}()- p( )\) is \(\)-quadratically growing for all \(^{d}\) such that

\[\|-}_{}\|_{2}^{2} _{}()-_{}^{ },\]

where \(}_{}\) is the projection of \(\) to the set of minimizers of \(_{}\), and \(_{}^{}=_{^{d}} _{}()\).

This assumption is weaker than assuming that the likelihood satisfies the Polyak-Lojasiewicz inequality (Karimi et al., 2016).

**Theorem 3**.: _Let Assumption 2 hold, the likelihood satisfy Assumption 5, and the assumptions of Corollary 1 hold such that the ELBO \(F\) is \(L_{F}\)-smooth with \(L_{F}=L_{}+L_{}+L_{}\). Then, the iterates generated by BBVI through Equation (1) and the \(M\)-sample reparameterization gradient include an \(\)-stationary point such that \(_{0 t T-1}\| F(_{t})\|_{2}\) for any \(>0\) if_

\[T()-F^{})^{2 }L_{F}L_{}^{2}(d,k_{})}{ M ^{4}})\]

_for some fixed stepsize \(\), where \((d,)=d+k_{}\) for the Cholesky family and \((d,)=2k_{}+1\) for the mean-field family._

Proof.: See the _full proof_ in page 35.

**Remark 3**.: Finding an \(\)-stationary point of the ELBO has an iteration complexity of \((dL_{}^{2}xM^{-1}^{-4})\) for the Cholesky family and \((L_{}^{2} M^{-1}^{-4})\) for the mean-field family.

### Black-Box Variational Inference with Proximal SGD

**Proximal SGD** For a composite objective \(F=f+h\), proximal SGD repeats the steps:

\[_{t+1}=_{_{t},h}(_{t}-_{t} (_{t}))=*{arg\,min}_{ }\ [(_{t}), +h()+}\|- _{t}\|_{2}^{2}\ ], \]

where prox is known as the _proximal_ operator and \(_{1}\),..., \(_{T}\) is a stepsize schedule.

In the context of VI, proximal SGD has previously been considered by Altosaar et al. (2018); Diao et al. (2023); Khan et al. (2016, 2015). Their overall focus has been on developing alternative algorithms by generalizing \(\|-^{}\|\) to other metrics. In contrast, Domke (2020) considered proximal SGD with the regular Euclidean metric \(\|-^{}\|_{2}\) for overcoming the non-smoothness of \(h\) under linear parameterizations. Here, we prove the convergence of this scheme and show that it retrieves the fastest known convergence rates in stochastic first-order optimization.

Proximal Operator for BBVIIn our context, \(h\) is the entropy of \(q_{A}\) in the location-scale family. For this, Domke (2020) show that the the proximal update for \(s_{1},,s_{d}\), is

\[_{_{t},h}(s_{i})=s_{i}+( ^{2}+4_{t}}-s_{i}).\]

For other parameters, the proximal operator is the regular gradient descent update in Equation (1).

Gradient Variance BoundWe first establish a bound on the gradient variance. In ERM, contemporary strategies do this by exploiting the finite sum structure of the objective (Section 2.4). Here, we establish a variance bound for RP estimator that does not rely on the finite sum assumption.

**Lemma 3** (**Convex Expected Smoothness)**.: _Let \(\) be \(L_{}\)-smooth and \(\)-strongly convex with the variational family satisfying Assumption 2 with the linear parameterization. Then,_

\[\|_{}f(;u)-_{^{}} f(^{};u)\|_{2}^{2} 2L_{}\,C(d, )\,_{f}(,^{})\]

_holds, where \(_{f}(,^{}) f( )-f(^{})- f( ^{}),-^{}\) is the Bregman divergence, \(=L_{}/\) is the condition number, \(C(d,)=d+k_{}\) for the Cholesky family, and \(C(d,)=2k_{}+1\) for the mean-field family._

Proof.: See the _full proof_ in page 36.

Furthermore, the gradient variance at the optimum must be bounded:

**Lemma 4** (Domke, 2019; Kim _et al._, 2023).: _Let \(\) be \(L_{}\)-smooth with the variational family satisfying Assumption 2 and a 1-Lipschitz diagonal conditioner \(\). Then, the gradient variance at the optimum \(^{}*{arg\,min}_{}F()\) is bounded as_

\[^{2}C(d,)\,L_{}^{2}\,(\| }-^{}\|_{2}^{2}+\|C^{ }\|_{F}^{2}),\]

_where \(}\) is a stationary point of \(\), \(^{}\) and \(^{}\) are the location and scale formed by \(^{}\), the constants are \(C(d,)=d+k_{}\) for the Cholesky family and \(C(d,)=2k_{}+1\) for the mean-field family, \(k_{}\) is the kurtosis of \(\) as defined in Assumption 1._

Proof.: The full-rank case is proven by Domke (2019, Theorem 3), while the mean-field case is a basic corollary of the result by Kim _et al._ (2023, Lemma 2). 

**Remark 4**.: The dimensional dependence in the complexity of BBVI is transferred from the variance bound in Lemma 4. Unfortunately, for the Cholesky family, this dimensional dependence in the variance bound is tight (Domke, 2019).

Main ResultWith the gradient variance bounds, we now present our complexity result. The proof is identical to Theorem 3.2 by Gower _et al._ (2019), where they use a 2-stage decreasing stepsize schedule: the stepsize is initially held constant and then reduced in a \(1/t\) rate.

**Theorem 4**.: _Let \(\) be \(L_{}\)-smooth and \(\)-strongly convex. Then, for any \(>0\), BBVI with proximal SGD in Equation (2), the \(M\)-sample reparameterization gradient estimator, a variational family satisfying Assumption 2 with the linear parameterization guarantees \(\|_{T}-^{}\|_{2}^{2}\) if_

\[_{t}= C(d,)}&  t 4T_{}\\ }& t>4T_{}, T(}{^{2}\,}+\| _{0}-^{}\|_{2}}{},\ \ 4T_{})\]

_where \(^{2}\) is defined in Lemma 4, \(T_{}=[^{2}C(d,)M^{-1}]\), \(=L_{}/\) is the condition number, \(\) is Euler's constant, \(^{}=*{arg\,min}_{}F()\), \(C(d,)=d+k_{}\) for the Cholesky family, and \(C(d,)=2k_{}+1\) for the mean-field family._

Proof.: See the _full proof_ in page 38.

**Remark 5**.: BBVI with proximal SGD on \(\)-strongly convex and \(L_{}\)-smooth \(\) has a complexity \((^{2}dM^{-1}\,^{-1})\) for the Cholesky family and \((^{2}M^{-1}\,^{-1})\) for the mean-field family.

**Remark 6**.: We also provide a similar result with a fixed stepsize in Theorem 7 of Appendix F.3.2. In this case, the complexity is \((^{2}dM^{-1}^{-1}^{-1})\) for the Cholesky family and \((^{2}M^{-1}^{-1}^{-1})\) for the mean-field family.

## 5 Experiments

### Synthetic Problem

SetupWe first compare proximal SGD against vanilla SGD with linear and nonlinear parameterizations on a synthetic problem, which is log-smooth, strongly log-concave, and the exact solution is known. While a similar experiment was already conducted by Domke (2020), here we include nonlinear parameterizations, which were not originally considered. We run all algorithms with a fixed stepsize to infer a multivariate Gaussian with a full-rank covariance matrix. The variational approximation is a full-rank Gaussian formed by \(=(0,1)\) and the Cholesky parameterization.

ResultsThe results are shown in Figure 3. Proximal SGD is clearly the most robust against initialization. Also, SGD with the nonlinear parameterization \((x)=(x)\) is much slower to converge under all initializations. This confirms that linear parameterizations are indeed superior for both robustness against initializations and convergence speed.

### Realistic Problems

SetupWe now evaluate proximal SGD on realistic problems. In practice, Adam (Kingma and Ba, 2015) is observed to be robust against stepsize choices (Zhang et al., 2019). The reason why Adam performs well on non-smooth, non-convex problems is still under investigation (Kunstner et al., 2023; Reddi et al., 2023; Zhang et al., 2022). Nonetheless, to compare fairly against Adam, we implement a recently proposed variant of proximal SGD called Pro

Figure 4: **Comparison of BBVI convergence speed (ELBO v.s. Iteration) of different optimization algorithms. The error bands are the 80% quantiles estimated from 20 (10 for AR-eeg) independent replications. The results shown used a base stepsize of \(=10^{-3}\), while the initial point was \(_{0}=,_{0}=\). Details on the setup can be found in the text of Section 5.2 and Appendix G.**

Figure 3: **Stepsize versus the number of iterations for vanilla SGD and proximal SGD to achieve \(_{}(q_{},)=1\) under different initializations for Gaussian posteriors. The initializations \(C(_{0})\) are \(\), \(10^{-3}\), \(10^{-5}\) from left to right, respectively. The average suboptimality at iteration \(t\) was estimated from 10 independent runs. For each run, the target posterior was a 10-dimensional Gaussian with a covariance with a condition number \(=10\) and a smoothness of \(L=100\).**includes an Adam-like update rule. The probabilistic models and datasets are fully described in Appendix G. We implement these models and BBVI on top of the Turing (Ge _et al._, 2018) probabilistic programming framework. Due to the size of these datasets, we implement _doubly stochastic_ subsampling (Titsias & Lazaro-Gredilla, 2014) with a batch size of \(=100\) (\(B=500\) for \(\)T-tennis) with \(M=10\) Monte Carlo samples. For batch subsampling, we implement random-reshuffling, which is faster than independent subsampling both empirically (Bottou, 2009) and theoretically (Ahn _et al._, 2020; Haochen & Sra, 2019; Mishchenko _et al._, 2020; Nagaraj _et al._, 2019). We also observe that doubly stochastic BBVI benefits from reshuffling, but leave a detailed investigation to future works.

ResultsRepresentative results are shown in Figure 4, with additional results in Appendix H. Both ProxGen-Adam and Adam with linear parameterizations converge faster than Adam with nonlinear parameterization. Furthermore, for the case of election and buzz, Adam with the nonlinear parameterization converges much slower than the alternatives. When using linear parameterizations, ProxGen-Adam appears to be generally faster than Adam. We note, however, that due to the difference in the update rule between ProxGen-Adam and Adam, proximal operators alone might not fully explain the performance difference. Nevertheless, the results of our experiment do conclusively suggest that linear parameterizations are superior.

## 6 Discussions

ConclusionsIn this work, we have proven the convergence of BBVI. Our assumptions encompass implementations that are actually used in practice, and our theoretical analysis revealed limitations in some of the popular design choices (mainly the use of nonlinear conditioners). To resolve this issue, we re-evaluated the utility of proximal SGD both theoretically and practically, where it achieved the strongest theoretical guarantees in stochastic first-order optimization.

Related WorksTo prove the convergence of BBVI, early works have _a-priori_ "assumed" the regularity of the ELBO and the gradient estimator (Alquier & Ridgway, 2020; Buchholz _et al._, 2018; Khan _et al._, 2016, 2015; Liu & Owen, 2021; Regier _et al._, 2017). Towards a more rigorous understanding, Domke (2019); Fan _et al._ (2015); Kim _et al._ (2023); Xu _et al._ (2019) studied the reparameterization gradient, Xu & Campbell (2022) studied the asymptotics of the ELBO, Challis & Barber (2013); Domke (2020); Titsias & Lazaro-Gredilla (2014) established convexity, and Domke (2020) established smoothness. On the other hand, Bhatia _et al._ (2022); Hoffman & Ma (2020) established rigorous convergence guarantees by considering simplified variant of BBVI where only the scale is optimized, and Fujisawa & Sato (2021) assumed that the support of \(\) is bounded almost surely. Meanwhile, under similar assumptions to ours, Diao _et al._ (2023); Lambert _et al._ (2022) recently established convergence guarantees for proximal SGD BBVI with a Bures-Wasserstein metric. Their computational properties differ from BBVI as they require Hessian evaluations. Also, understanding BBVI, which is VI with a Euclidean metric, is an important problem due to its practical relevance.

LimitationsOur work has multiple limitations: **(i)** Our results are restricted to the location-scale family, **(ii)** the reparameterization gradient, and **(iii)** smooth joint log-likelihoods. However, the location-scale family with the reparameterization gradient is the most widely used combination in practice, and replacing the smoothness assumption is an active area of research in stochastic optimization. For our results on proximal SGD, we further assume that the joint log-likelihood is \(\)-strongly convex (equivalently strongly log-concave posteriors). It is unclear how to extend the guarantees to only smooth but non-log-concave joint log-likelihoods.

Open ProblemsAlthough we have proven that the mean-field dimensional family has a dimension dependence of \(()\), empirical results suggest room for improvement (Kim _et al._, 2023). Therefore, we pose the following conjecture:

**Conjecture 1**.: _Under mild assumptions, BBVI for the mean-field variational family converges with only logarithmic dimensional dependence or no explicit dimensional dependence at all._

This would put mean-field BBVI in a regime clearly faster than approximate MCMC (Freund _et al._, 2022). Also, it is unknown whether the \((^{2})\) condition number dependence dependence is tight. In fact, for proximal SGD BBVI in Bures-Wasserstein space, Diao _et al._ (2023) report a dependence of \(()\). Lastly, it would be interesting to see whether natural gradient VI (NGVI; Amari, 1998; Khan & Lin, 2017) can achieve similar convergence guarantees. While it is empirically known that NGVI often converges faster (Lin _et al._, 2019), theoretical evidence has yet to follow.