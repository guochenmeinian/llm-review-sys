# Exploiting Connections between Lipschitz Structures

for Certifiably Robust Deep Equilibrium Models

 Aaron J. Havens\({}^{*1}\) Alexandre Araujo\({}^{*2}\) Siddharth Garg\({}^{2}\)

Farshad Khorrami\({}^{2}\) Bin Hu\({}^{1}\)

\({}^{1}\) CSL & ECE, University of Illinois Urbana-Champaign

\({}^{2}\) ECE, New York University

Equal contribution.

###### Abstract

Recently, deep equilibrium models (DEQs) have drawn increasing attention from the machine learning community. However, DEQs are much less understood in terms of certified robustness than their explicit network counterparts. In this paper, we advance the understanding of certified robustness of DEQs via exploiting the connections between various Lipschitz network parameterizations for both explicit and implicit models. Importantly, we show that various popular Lipschitz network structures, including convex potential layers (CPL), SDP-based Lipschitz layers (SLL), almost orthogonal layers (AOL), Sandwich layers, and monotone DEQs (MonDEQ) can all be reparameterized as special cases of the Lipschitz-bounded equilibrium networks (LBEN) without changing the prescribed Lipschitz constant in the original network parameterization. A key feature of our reparameterization technique is that it preserves the Lipschitz prescription used in different structures. This opens the possibility of achieving improved certified robustness of DEQs via a combination of network reparameterization, structure-preserving regularization, and LBEN-based fine-tuning. We also support our theoretical understanding with new empirical results, which show that our proposed method improves the certified robust accuracy of DEQs on classification tasks. All codes and experiments are made available at [https://github.com/AaronHavens/ExploitingLipschitzDEQ](https://github.com/AaronHavens/ExploitingLipschitzDEQ).

## 1 Introduction

Recently, deep equilibrium models (DEQs) have drawn increasing attention from the deep learning community . In DEQs, the output is defined as the solution to an input-dependent fixed point equation. Specifically, consider a DEQ with input \(x\) and output \(y\). Then \(y\) is typically determined from \(x\) as follows

\[z=(Wz+Ux+b_{z}), y=Gz+b_{y}, \]

where \(\) denotes some nonlinear activation function, e.g. ReLU. DEQs can be viewed as an implicit model that directly solves the fixed point of an infinitely-deep network. Since DEQs have implicit depth, they have the potential to reduce the large memory footprint required to train finite-depth feed-forward networks via explicit back-propagation. Noticeably, DEQs have shown great promise in achieving comparable performance on computer vision tasks .

Despite great potential, there are many open issues to be explored and understood for DEQs. There has been a body of recent works focusing on developing and understanding DEQ structures for addressing well-posedness, training stability, and adversarial robustness .

For example, the monotone DEQ (MonDEQ) uses the following parameterization:

\[z=(((1-m)I-A^{}A+B-B^{})z+Ux+b_{z}), y=Gz+b_{y},\]

where \((A,B,U,G,b_{z},b_{y})\) are the decision variables to be trained, and \(m\) is a hyperparameter . In other words, MonDEQ does not treat \(W\) as the variable to be directly trained and just parameterizes it as \(W=(1-m)I-A^{}A+B-B^{}\). The advantage of this parameterization is that well-posedness and \(}{m}\)-Lipschitzness (from \(x\) to \(z\)) are automatically ensured [47; 31]. Another popular DEQ structure for controlling the Lipschitz constant is given by the so-called Lipschitz-bounded equilibrium network (LBEN) parameterization which specifies the weight matrix \(W\) in (1) as follows

\[W=I-((G^{}G+^{-1}UU^{}^{-1})/(2L)+V^{}V+S-S^{}+  I), \]

where \(\) is a diagonal positive definite matrix, and \((V,S)\) are unconstrained free variables . Using the LBEN parameterization, one can train \(L\)-Lipschitz DEQs by solving an unconstrained optimization problem.

This paper focuses on advancing the understanding of DEQ parameterizations in the context of certified robustness. It is well known that neural networks are susceptible to imperceptible adversarial input perturbations . Hence, there is a need to develop certifiably robust models for safety-critical applications. For explicit networks, there has been significant progress on improving certified robustness by using various 1-Lipschitz layer structures such as orthogonal layers [41; 24; 37; 49], almost orthogonal layers (AOL) , convex potential layers (CPL) , SDP-based Lipschitz layers (SSL) , and Sandwich layers . However, how to train DEQs with reasonable certified robustness remains largely open . In this paper, we improve the certified robustness of DEQs via exploiting the connections between various Lipschitz structures for both explicit and implicit models. Importantly, we show that various popular Lipschitz network structures, including convex potential layers (CPL), SDP-based Lipschitz layers (SLL), almost orthogonal layers (AOL), Sandwich layers, and monotone DEQs (MonDEQ) can all be reparameterized as special cases of the Lipschitz-bounded equilibrium networks (LBEN) without changing the prescribed Lipschitz constant in the original network parameterizations. A key feature of our reparameterization technique is that it preserves the Lipschitz prescription used in different structures. This feature is particular relevant to training certifiably robust DEQs, and opens the possibility of achieving improved certified robustness of DEQs via a combination of network reparameterization, structure-preserving regularization, and LBEN-based fine-tuning. We provide empirical results to show that our reparameterization technique improves the certified robust accuracy of DEQs on classification benchmarks. Our work sheds new light on the principles for developing 1-Lipschitz DEQ structures and serves as a meaningful step towards achieving better certified robustness for DEQs.

**Additional Related Work.** There are also several recent results on certified robustness of DEQs under \(_{}\) perturbations [22; 30; 46; 11; 18; 10]. Our work focuses on certified robustness under \(_{2}\) perturbations and hence naturally complements these existing works. In this paper, we mainly study the impacts of network parameterizations on certified robustness of DEQs. For explicit models, many other methods for designing certifiably robust models are also available [8; 36; 21; 16; 38; 15]. It is an interesting future task to study how to tailor these different methods for improving the certified robust accuracy of DEQs.

## 2 Background and Preliminaries

**Notation.** The set of \(n\)-dimensional real vectors is denoted by \(^{n}\). For any vector \(x^{n}\), we denote the associated Euclidean norm as \(\|x\|\). In addition, we denote the \(n n\) identity matrix and the \(n n\) zero matrix as \(I_{n}\) and \(0_{n}\), respectively. The subscripts will be omitted when the dimension is clear from the context. When a matrix \(P\) is negative semidefinite (definite), we will use the notation \(P()0\). When a matrix \(P\) is positive semidefinite (definite), we will use the notation \(P()0\). Given a collection of scalars \(\{a_{i}\}_{i=1}^{n}\), the \(n n\) diagonal matrix whose \((i,i)\)-th entry is \(a_{i}\) is denoted as \((a_{i})\). For a matrix \(A\), let \(A^{}\) and \(\|A\|_{2}\) denote its transpose and largest singular value, respectively.

### Lipschitz Property and Certified Robustness

Suppose we have a prediction model given by an input-output mapping \(y=f(x)\). We say that \(f\) is \(L\)-Lipschitz (with respect to the \(_{2}\) norm) if \(\|f(x_{1})-f(x_{2})\| L\|x_{1}-x_{2}\|\) for any \((x_{1},x_{2})\)The Lipschitz constant can be combined with the prediction margin of \(f\) to give certified robust accuracy [42, Proposition 1]. In the past, 1-Lipschitz models have shown great promise in achieving improved certified robustness on classification benchmark problems. In this paper, we focus on designing certifiably robust DEQs via using novel 1-Lipschitz parameterizations.

### SDPs for Well-posedness and Lipschitz Bounds of DEQs

Given a DEQ in the general form (1), the following existing sufficient conditions are useful for testing its well-posedness and Lipschitz properties.

**Proposition 2.1** (Theorem 1 in Revay et al. ).: _Consider the DEQ model (1) with \(\) being sloped-restricted on \(\). If there exists a positive definite diagonal matrix \(\) such that the following SDP holds,_

\[2- W-W^{} 0, \]

_then the DEQ model (1) is well-posed and has a finite Lipschitz bound from \(x\) to \(y\)._

**Proposition 2.2** (Theorem 2 in Revay et al. ).: _Consider the DEQ model (1) with \(\) be slope-restricted on \(\). If there exists a positive definite diagonal matrix \(\) such that_

\[2- W-W^{}-G^{}G- UU ^{} 0, \]

_then the DEQ (1) is well-posed and \(L\)-Lipschitz from \(x\) to \(y\)._

The above propositions can be proved using standard control-theoretic arguments borrowed from the quadratic constraint theory . See  for detailed proofs. In general, matrix inequality conditions have been widely adopted for addressing the Lipschitz properties of neural networks , and (4) can be viewed as the extension of the LipSDP condition  to the DEQ setting. Notice that we use different notations for the decision variables in the testing conditions (3) and (4) to emphasize the fact that the solution to (3) may not be a solution to (4) even for the same DEQ model. For fixed \(W\), the condition (3) is an SDP that can be efficiently verified by existing solvers. For given \((W,U,G)\), the condition (4) is bilinear in \(\) and hence not an SDP. However, one can easily convexify (4) into an SDP form using the Schur complement lemma. Later, we will develop a variant of Proposition 2.2, which can be used to unify the developments of various Lipschitz implicit and explicit models.

### DEQ Parameterizations: MonDEQ and LBEN

Now we briefly review several existing DEQ parameterizations for inducing well-posedness and the Lipschitz property.

MonDEQ.The well-posedness issue of DEQs was addressed in  using the MonDEQ parameterization derived from the monotone operator theory. In addition, operator splitting approaches can be used to efficiently compute the unique fixed point of MonDEQ. Recall that MonDEQ adopts the following weight parameterization:

\[W=(1-m)I-A^{}A+B-B^{} \]

where \(m>0\), and \((A,B)\) are free decision variables to be trained. As long as \(W\) is parameterized as in (5), the resultant DEQ model (1) is well-posed . Based on examining the unrolled operator splitting iterates, it has been shown  that MonDEQ is \(}{m}\)-Lipschitz from \(x\) to \(z\) (and hence \(\|G\|_{2}}{m}\)-Lipschitz from \(x\) to \(y\)). If \(G=I\), then one can enforce MonDEQ to be 1-Lipschitz by adding the constraint \(\|U\|_{2} m\).

Generalized MonDEQ (G-MonDEQ).Revay et al.  generalizes the MonDEQ parameterization as follows

\[W=I-(A^{}A+B^{}-B+mI), \]

where \(\) is a positive definite diagonal matrix to be trained. Obviously, the MonDEQ model is a special case of (6) with \(=I\). For the G-MonDEQ parameterization (6), the well-posedness can also be guaranteed. Specifically, one can verify that the SDP condition (3) with \(W\) given by (6) holds if we choose \(=^{-1}\). In this case, the left side of (3) becomes \(2A^{}A+2mI 0\). The Lipschitz constant for the G-MonDEQ parameterization (6) has not been derived before. Later we will present an explicit Lipschitz bound for (6) via constructing an analytical solution to a variant of (4).

Lben.Revay et al.  has also proposed the following LBEN parameterization with free decision variables \((G,U,V,S)\), a diagonal matrix variable \( 0\), and a hyperparameter \(>0\):

\[W=I-G^{}G+^{-1}UU^{}^{-1}+V ^{}V+S^{}-S+ I. \]

The LBEN parameterization guarantees the DEQ to be well-posed and \(L\)-Lipschitz. Specifically, one can verify that the condition (4) holds with \(W\) being defined by (7) and \(=^{-1}\). In this case, the left side of (4) becomes \(2V^{}V+2 I 0\). The above parameterization allows one to train \(L\)-Lipschitz DEQs in an unconstrained manner.

### 1-Lipschitz Explicit Feed-forward Networks: AOL, SLL and Sandwich Layers

For traditional explicit models, there exist several 1-Lipschitz parameterizations which can be used for efficient training of certifiably robust feed-forward networks. Now we briefly discuss several state-of-the-art 1-Lipschitz feed-forward parameterizations.

Orthogonal and Almost Orthogonal Layers.Consider the following standard explicit feed-forward network:

\[x_{0}=x, x_{n+1}=(W_{n}x_{n}+b_{n}), y=W_{N}x_{N}+b_{N},\]

where \(n=0,1,,N-1\). Since the compositions of 1-Lipschitz functions are also 1-Lipschitz, one can just parameterize every explicit layer \(x_{n+1}=(W_{n}x_{n}+b_{n})\) to be 1-Lipschitz, and the resultant feed-forward network is 1-Lipschitz. If \(\) is 1-Lipschitz1, one only needs to ensure \(\|W_{n}\|_{2} 1\) for all \(n\). An important issue in training such Lipschitz layers for deep explicit networks is gradient vanishing, and this motivates the development of various techniques which introduce gradient norm preservation based on orthogonality. The orthogonality parameterizations just enforce \(W_{n}\) to satisfy \(W_{n}^{}W_{n}=I\). In contrast, the _Almost-Orthogonal-layer_ (AOL)  performs a weight normalization \(W_{n}=_{n}D_{n}\), where \(_{n}\) is the free decision variable to be trained, and \(D_{n}\) is a scaling matrix which is diagonal and defined as2

\[D_{n}=(_{j}|_{n}^{}_{n}|_{ ij})^{-}. \]

AOL ensures \(\|W_{n}\|_{2} 1\). In , it has been empirically demonstrated that the trained weight \(W_{n}\) tends to be "almost orthogonal."

Lipschitz Residual Networks and SLL.Another way to address the gradient vanishing issue in training Lipschitz networks is to use a residual structure . A general result is given in  and shows that the following residual network is guaranteed to be 1-Lipschitz given \(W_{n}^{}W_{n} T_{n}\):

\[x_{0}=x, x_{n+1}=x_{n}-2W_{n}T_{n}^{-1}(W_{n}^{}x_{n}+b_{n }), y=x_{N}.\]

For CPL, the choice of \(T_{n}\) is simple, i.e. \(T_{n}=\|W_{n}\|_{2}^{2}I\). To achieve better certified robust accuracy, SLL  uses the following more delicate choice of \(T_{n}\):

\[T_{n}=(_{j}W_{n}^{}W_{n}_{ij}\,^{(n)}}{q_{i}^{(n)}}) \]

where \(\{q_{i}^{(n)}\}\) are free positive scalars to be trained.

Sandwich Layers.Recently, the sandwich layer has been proposed in , and achieves very competitive performances with relatively small models. Based on , the following network is guaranteed to be \(L\)-Lipschitz:

\[x_{0}=x, x_{n+1}=A_{n}^{}_{n}( _{n}^{-1}B_{n}x_{n}+b_{n}), y=B_{N}x_{N}+b_{N},\]

where \(A_{n}A_{n}^{}+B_{n}B_{n}^{}=I\), \(B_{N}^{}B_{N}=I\), and \(_{n}\) is a free diagonal matrix whose entries are restricted to be non-negative. One can parameterize \([A_{n} B_{n}]^{}\) to satisfy \(A_{n}A_{n}^{}+B_{n}B_{n}^{}=I\) using the generalized Cayley transformation , and train the above network efficiently.

Network Reparameterizations with Preserved Lipschitz Prescriptions

In this section, we show that various Lipschitz structures such as MonDEQ, G-MonDEQ, SLL, Sandwich, and AOL can all be reparameterized as special cases of LBEN without changing the prescribed Lipschitz constant in the original parameterizations. The feature of preserving the Lipschitz constant prescription between different parameterizations is particularly relevant to training certifiably robust DEQs, and we will elaborate on this point later. We emphasize that our result does not indicate that we should abandon all other Lipschitz structures and only use the LBEN parameterization for training certifiably robust DEQs. As a matter of fact, when implementing these different parameterizations for classification tasks, one can potentially incorporate quite different convolution structures and the inductive bias can be very different. The true implication of our theory is that one can potentially fine-tune the trained models from MonDEQ, SLL, or other explicit Lipschitz layers by reparameterizing the networks as LBEN and initializing the LBEN training from these reparameterized models. The key to this implication is that the prescribed Lipschitz constant has to be preserved during the reparameterization process. Our theory opens the possibility of achieving improved certified robustness of DEQs via a combination of network reparameterization and LBEN-based fine-tuning.

### Warm-up: Issues in Reparameterizing MonDEQ as LBEN and a Fix

In this section, we reveal a deeper connection between MonDEQ and LBEN. As mentioned in [35, Remark 2], any MonDEQ immediately leads to an LBEN for sufficiently large \(L\). However, this trivial reparameterization of LBEN from MonDEQ does not preserve the prescribed Lipschitz constant in the original MonDEQ parameterization. Specifically, the original MonDEQ controls its Lipschitz constant by \(}{m}\). After the trivial reparameterization based on [35, Remark 2], the resultant Lipschitz constant is no longer \(}{m}\). In contrast, it can be arbitrarily large. This is problematic for training certifiably robust DEQs, since the increase in the Lipschitz constant will decrease the certified robust accuracy.

Notice that the construction of LBEN relies on Proposition 2.2. The above technical issue hinges upon the fact that Proposition 2.2 cannot be directly applied to prove the Lipschitz constant of MonDEQ. To gain some insight into how to fix this issue, we first discuss how to modify Proposition 2.2 to recover the previous Lipschitz bound of MonDEQ. Specifically, we will use a variant of [35, Theorem 2] (which has been restated as Proposition 2.2). Now we state this new condition.

**Theorem 3.1**.: _Suppose the DEQ model (1) is well-posed, and \(\) is slope-restricted on \(\). If there exists a positive definite diagonal matrix \(\) such that the following non-strict matrix inequality holds,_

\[2- W-W^{}-G^{}G- UU^{ } 0, \]

_then the DEQ (1) is \(L\)-Lipschitz from \(x\) to \(y\)._

Proof.: We only need to modify the quadratic constraint argument used in [35, Theorem 2]. Originally, a strict matrix inequality is needed to ensure well-posedness. However, well-posedness has been assumed here, and a non-strict matrix inequality is sufficient to ensure the \(L\)-Lipschitz property. For completeness, a detailed proof is presented in the appendix. 

The above variant of the strict matrix inequality (4) gives us a non-strict condition, which can then be used to derive Lipschitz properties of MonDEQ, G-MonDEQ, and other explicit models in a unified manner. It turns out that such a non-strict variant is quite crucial for our proposed reparameterization techniques. Recall that originally the LBEN parameterization (7) is derived using Proposition 2.2. Since Theorem 3.1 is the non-strict variant of Proposition 2.2, it is not that surprising that we can use Proposition 2.1 to take care of the well-posedness issue and then combine it with Theorem 3.1 to prove the desired Lipschitz property. We will show that the Lipschitz properties of MonDEQ, G-MonDEQ, and explicit layers such as SLL can also be obtained by providing analytical solutions to (10). More importantly, these new analytical solutions will guide us to find a systematic way to reparameterize these structures as LBEN without changing the prescribed Lipschitz constant in the original parameterizations.

First, we will apply our new theorem to recover the original Lipschitz bound from . The original analysis in  relies on examining the operator splitting iterates and bounding the difference between outputs based on their inputs. Alternatively, we can recover this result using the non-strict condition (10). Specifically, we can set \(G=I\) (recalling that we are interested in the Lipschitz constant from \(x\) to \(z\)), \(=}I\), \(L=}{m}\), and \(W\) as defined by the MonDEQ parameterization (5). Then we can show that the left side of (10) becomes

\[}I+}A^{}A-I- }UU^{}= I+}A^{}A-^{2} }UU^{}\] \[= }A^{}A+(I-^{2}}UU^{}) 0.\]

Therefore, the condition holds with \(=}I\) and \(L=\|U\|_{2}/m\). Notice that MonDEQ is known to be well-posed. Hence Theorem 3.1 can be directly applied to give the conclusion that MonDEQ is \(}{m}\)-Lipschitz from \(x\) to \(z\). If we have \(G=I\) and want to enforce MonDEQ to be \(1\)-Lipschitz, then we only need to require \(\|U\|_{2} m\). There are many ways to parameterize \(U\) such that this norm constraint is automatically satisfied. For example, one can enforce \(U/\) to be an orthogonal matrix. Similarly, one can also use the scaling trick in AOL to ensure \(\|U\|_{2} m\). See the appendix for more discussions on how to parameterize 1-Lipschitz MonDEQ.

Next, we discuss how to address the previous issue in reparameterization such that we can reparameterize 1-Lipschitz MonDEQ as LBEN by exactly setting \(L=1\) in the LBEN reparameterization. We will leverage the construction of \(\) above. Recall that the free variables in MonDEQ are \((A,B,U)\). The matrices \((W,G)\) are given by

\[W=(1-m)I-A^{}A+B-B^{},\ G=I\]

Now we confine our scope to 1-Lipschitz MonDEQ, i.e. \(\|U\|_{2} m\). We will show that given a MonDEQ model (5) with any fixed \((A,B,U)\), we can always generate an LBEN parameterization in the form of (7) which gives exactly the same input-output mapping behavior from \(x\) to \(y\). This means that any trained MonDEQ model can be reparameterized in the form of LBEN with some choice of \((,G,U,V,S,)\). We will provide explicit formulas for such a choice of \((,G,U,V,S,)\). We have the following formal statement.

**Theorem 3.2**.: _Given a 1-Lipschitz MonDEQ model with \((A,B,U)\) satisfying \(\|U\|_{2} m\), the following choice of \((,G,U,V,S,)\) with \(U\) being the same one used for MonDEQ gives a valid 1-Lipschitz LBEN model which generates the same input-output mapping:_

\[=||U||_{2}I, G=I, S=},=0\] \[V^{}V=(}-)I+A}{\|U\|_{2}}-^{2}}UU^{} 0\]

Proof.: The above formula can be easily verified by substituting the expressions for \(W\) into both the MonDEQ and LBEN parameterizations and then making simplifications. The only thing that needs to be further checked is that the resultant expression for \(V^{}V\) must be positive semidefinite so we can obtain \(V\) by decomposition. Since we know \(\|U\|_{2} m\), we must have

\[V^{}V =(}-)I+A}{ \|U\|_{2}}-^{2}}UU^{} \] \[A}{\|U\|_{2}}+(I-^{2}}UU^{}) 0 \]

Hence the resultant expression for \(V^{}V\) is positive semidefinite. This completes the proof. 

We can clearly see that in the above result, the Lipschitz constant is preserved during the reparameterization.

### Connections between G-MonDEQ and LBEN

The Lipschitz constant of the G-MonDEQ parameterization (6) has not been explicitly provided by the original work in . Now we will present such an explicit formula via applying (10). Formally, we have the following result.

**Theorem 3.3**.: _Consider the G-MonDEQ parameterization (6). It is guaranteed that (6) gives a mapping which is \(U\|_{2}}{m}\)-Lipschitz from \(x\) to \(z\)._

Proof.: We are interested in the Lipschitz constant from \(x\) to \(z\), and hence we choose \(G=I\). As explained before, the G-MonDEQ parameterization is known to be well-posed based on Proposition 2.1. Hence we can apply Theorem 3.1 to upper bound its Lipschitz constant. Specifically, we can choose \(=U\|_{2}}^{-1}\), and \(L=U\|_{2}}{m}\). Then the left side of (10) becomes

\[I+U\|_{2}}A^{}A-U\|_{2}^{2}}^{-1}UU^{}^{-1}\] \[= U\|_{2}}A^{}A+(I- {\|^{-1}U\|_{2}^{2}}^{-1}UU^{}^{-1}),\]

which is obviously positive semidefinite. Therefore, (10) is feasible for G-MonDEQ with \(L=U\|_{2}}{m}\). This leads to the desired conclusion. 

If we choose \(=I\), the above theorem just recovers [31, Theorem 1] as a special case. From the above theorem, we can ensure G-MonDEQ to be \(1\)-Lipschitz by enforcing \(\|^{-1}U\|_{2} m\) during training.

Next, we can leverage the above construction of \(\) to establish a similar reparameterization result connecting G-MonDEQ and LBEN.

**Theorem 3.4**.: _Given a 1-Lipschitz G-MonDEQ model with \((,A,B,U)\) satisfying \(\|^{-1}U\|_{2} m\), the following choice of \((,G,U,V,S,)\) with \(U\) being the same one used for G-MonDEQ gives a valid \(1\)-Lipschitz LBEN model which generates exactly the same input-output mapping:_

\[ =\|^{-1}U\|_{2}, G=I, S=U\| }B,=0\] \[V^{}V =(U\|_{2}}-)I+A}{\|^{-1}U\|_{2}}-U\|_{2}^{2}}^{-1}UU^{ }^{-1} 0.\]

Proof.: The proof is very similar to the proof of the MonDEQ case and hence omitted. 

Again, the prescribed Lipschitz constant is preserved during our reparameterization.

### Connections between SLL and LBEN

Extending the previous analysis, we can further build connections between LBEN and explicit networks such as SLL. The SLL network is parameterized by \(\{W_{n},T_{n}\}\) with \(W_{n}^{}W_{n} T_{n}\). Again, we can choose \((,G,U,V,S,)\) properly to generate a valid 1-Lipschitz LBEN which gives the same input-output mapping. Similar to the previous treatment, we will first recover the Lipschitz analysis of SLL to gain some insights on how to construct \(\) for the reparameterization. Notice that the feed-forward network is always well-posed. Hence we can just rewrite a feed-forward residual network in the form of (1) and then apply Theorem 3.1 to analyze its Lipschitz property. To see this, consider the residual structure \(x_{n+1}=x_{n}-2W_{n}T_{n}^{-1}(W_{n}^{}x_{n}+b_{n})\) and \(x_{0}=x\). Denoting \(_{n+1}=(W_{n}^{}x_{n}+b_{n})\) (\(n 1\)) and \(_{0}=x_{0}\), we can express the entire multi-layer residual structure in terms of \((_{0},,_{N-1})\) as

\[_{n+1}=W_{n}^{}_{0}-_{k=0}^{n- 1}2W_{k}T_{k}^{-1}_{k+1}+b_{n}, y=_{0}- _{k=0}^{N-1}2W_{k}T_{k}^{-1}_{k+1} \]

for \(n\{1,,N-1\}\). With this description, we can rewrite the SLL parameterization in the DEQ form of \((z,y)=((Wz+Ux+b_{x}),Gz+b_{y})\) with \(z=[_{0}^{},,_{l}^{}]^{}\),\([id()^{},()^{},,()^{}]^{}\)3, \(G=[I,-2W_{0}T_{0}^{-1},,-2W_{N-1}T_{N-1}^{-1}]\), \(U=[I,0,,0]^{}\), and \(W\) being given as the following matrix:

\[W=0\\ W_{0}^{}&0\\ W_{1}^{}&-2W_{1}^{}W_{0}T_{0}^{-1}&\\ &&&0\\ W_{N-1}^{}&-2W_{N-1}^{}W_{0}T_{0}^{-1}&&-2W_{N-1}^{}W_{N-2}T_{ N-2}^{-1}&0. \]

It is not surprising that the above matrix has a lower-triangular structure, since the original SLL network is feed-forward. Now it is straightforward to verify that (10) is feasible (see Appendix C) with \(L=1\) and \(\) being given by

\[=(I,2T_{0}^{-1},,2T_{N-1}^{-1}). \]

Now we can reparameterize SLL as LBEN with preserving the 1-Lipschitz property as follows.

**Theorem 3.5**.: _Given any SLL structure parameterized by \(\{W_{n},T_{n}\}_{n=0}^{N-1}\) with \(W_{n}^{}W_{n} T_{n}\) for all \(n\), the resulting input-output relation from \(x\) to \(y\) can be exactly recovered with the LBEN parameterization with the following choice of \((,G,U,V,S,)\):_

\[=^{-1})}, G=[I,-2W_{0}T_{0}^{ -1},,-2W_{N-1}T_{N-1}^{-1}]\]

\[U=[I,0,,0]^{}, V^{}V=(M+M^{}), S=M,=0,\]

_where \(M\) is given as \(M=(I-W)-(G^{}G+ UU^{})\) with \(W\) being defined by (14)._

A detailed proof for the above theorem will be presented in the appendix. We want to comment that one can easily verify that the expression for \(V^{}V\) is given explicitly by a block-diagonal matrix with a leading zero block and other remaining blocks define as

\[(V^{}V)_{nn}=2T_{n}^{-1}-2T_{n}^{-1}W_{n}^{}W_{n}T_{n}^{-1} \]

for \(n\{1,,N-1\}\). Obviously, the resultant matrix for \(V^{}V\) is positive semidefinite. We also emphasize that the above reparameterization works for both SLL and CPL, as long as one chooses \(\{T_{n}\}\) properly. In addition, it is worth noting that one can set \(=((d_{i}))\), leading to unconstrained decision variables \(\{d_{i}\}\), which can be useful for finetuning LBEN from SLL.

Reparameterizing AOL and Sandwich Layers as LBEN.Notice that non-residual Lipschitz network structures such as AOL and Sandwich can also be connected to LBEN. For the experimental evaluation, we mainly utilize the SLL initialization of LBEN. We leave the discussions on the explicit connections between AOL, Sandwich, and LBEN to the appendix.

## 4 Numerical Experiments

In this section, we present some numerical experiments and insights on how to initialize LBEN with other Lipschitz architectures. Such numerical study will demonstrate and support our theory.

### Convolutional LBEN and Implementation Details

For classification tasks, we need to embed convolutional layers into the LBEN theoretical framework. Although this is admissible in principal, there are practical aspects that need extra explanations.

Initialization of Convolutional LBEN.When initializing LBEN parameters \((V,S)\) from a trained SLL network, we require decomposing \((M+M^{})\) as constructed from Section 3 (\(M\) is given by Theorem 3.5). For convolutional layers, we consider convolution with circular padding which is known to be close under sum, product, and inverse. Hence, all operations required for forming \(M\) preserve the convolution structure and so \((M+M^{})\) is also convolution with circular padding. Furthermore, we can find \(V\) by taking the matrix square root of \((M+M^{})\) efficiently using the block-diagonalization trick of circular convolution (see the appendix for more details).

\(_{1}\) Regularization of LBEN Sparsity Structure.Although we endow the weight \(W\) with a convolution structure, which will be preserved throughout training, this parameterization uses a relatively large kernel. Rather than using, for example, a \(3 3\) kernel applied across the image, our kernel parameterization is the size of the input image \(n n\) (_i.e._, \(W\) is defined as a large kernel of size \(dc_{out} dc_{in} n n\), where \(d\) corresponds to the number of convolutions). Indeed, the inverse or square root of a convolution with a \(3 3\) kernel applied on an input \(n n\) input is another convolution with a \(n n\) kernel.

When initializing the LBEN from an existing SLL model, \(W\) will be quite sparse due to the original small kernel parameterization. However, this sparsity will not necessarily be preserved after training. This is because the gradients with respect to \(W\) (defined with kernel \(3 3\)) depend on the inverse of \(W\) (which has a kernel of size \(n n\)).

In order to heuristically preserve this sparsity structure and not destroy the inductive bias given by the initial architecture, we impose a small \(_{1}\)-penalty to non-zero values on the border of the kernel to induce learning small-size kernels. This way, we can regularize the structure of \(W\) without placing explicit constraints on the LBEN parameterization.

### Experiments: Evaluation of Pretrained Initialization and Discussion

For the experiments, we trained MonDEQ and SLL networks that would serve as initialization for LBEN. To train a 1-Lipschitz MonDEQ, we use spectral normalization  on the matrix \(U\) and set \(m=1\). For the SLL network (e.g., (2) in Table 1), we use 4 convolutional layers with circular padding and 2 dense layers. The convolutional and dense layers have 8 and 512 channels/features, respectively. We use the ReLU nonlinearity which is slope-restricted on \(\). Our final LBEN network is a composition of convolutional and dense LBEN blocks, initialized from the four convolutions and the two dense layers of the pretrained Lipschitz model, respectively. To fine-tune the LBEN after initialization, we use a small learning rate of \(1\) with a \(_{1}\)-regularization of 0.1 during 40 epochs (for more details, see the publicly available code). The results are presented in Table 1.

Note that we omit any unconstrained DEQ baselines which may have a higher clean accuracy compared to a constrained model, but very low certified robustness. The resulting Lipschitz constant of the unconstrained model is typically much larger than \(1\) and leads to certified accuracy near zero based on the margin argument. For instance, the work of  provides an evaluation of DEQ on CIFAR10 for \(=0.01\) achieving certified robust accuracy of roughly \(10\%\) (estimate is read off graph from [31, Figure 9]). Previously, no certified robustness results for the standard perturbation \(=36/255\) have been reported for unconstrained DEQ.

LbEN Initialized with Lip-MonDEQ.First, we present a small-scale experiment on the MNIST dataset with Lip-MonDEQ and LBEN initialized from the trained Lip-MonDEQ. SLL and LBEN baselines are omitted since LBEN already achieves reasonable performance and our aim is to

    &  &  &  &  \\   & & & \(\) & \(\) & \(\) & \(1\) \\   & (1) **Lip-MonDEQ** & 0.886 & 0.886 & 0.858 & 0.826 & 0.634 \\  & **LBEN initialized from Lip-MonDEQ** (1) & 0.927 & 0.927 & 0.910 & 0.886 & 0.731 \\   & (2) **SLL Network** & 0.654 & 0.555 & 0.458 & 0.363 & 0.106 \\  & **LBEN** & 0.451 & 0.361 & 0.277 & 0.207 & 0.043 \\  & **LBEN initialized from SLL** (2) & 0.655 & 0.562 & 0.472 & 0.380 & 0.123 \\   & (3) **SLL Network** & 0.398 & 0.288 & 0.207 & 0.149 & 0.038 \\  & **LBEN** & 0.292 & 0.176 & 0.117 & 0.078 & 0.015 \\  & **LBEN initialized from SLL** (3) & 0.403 & 0.291 & 0.209 & 0.153 & 0.041 \\   

Table 1: This table presents our results obtained from LBEN models initialized from a pre-trained Lipschitz Network (_e.g._, MonDEQ, SLL). After initialization, the LBEN is then fine-tuned for 40 epochs with a very small learning rate. LBEN offers improved \(_{2}\)-certified accuracy over Lipschitz networks counterparts. For MNIST, SLL and LBEN baselines are omitted since LBEN already achieves reasonable performance. We initialized instead from Lip-MonDEQ to showcase how our theoretical framework can provide further improvements.

showcase how our theoretical framework can provide further improvements on existing architectures. We observe that the LBEN significantly improves the natural and provable accuracy over the LipMonDEQ. This is mainly due to the different Lipschitz constraints used in Lip-MonDEQ and LBEN.

LBEN initialized with SLL.We now present some experiments on CIFAR10 and CIFAR100 datasets . First, we trained an SLL network (2) with circular padding with the same hyperparameters as in . We then compare the natural and certified accuracy between an LBEN trained from scratch (random initialization) and an LBEN initialized from the SLL model. We observe that the LBEN with random initialization offers poor natural and certified accuracy compared to the SLL network for both the CIFAR10 and CIFAR100 datasets. On the other hand, when LBEN is initialized from the trained SLL weights, it successfully improves upon SLL by approximately 1% in certified accuracy on CIFAR10 and a marginal improvement on CIFAR100. Despite being marginal improvements over SLL, these results significantly improve upon the current state-of-the-art \(_{2}\)-certified accuracy for DEQs.

One interpretation of these results is that the performance of neural networks is highly-dependent on a good initialization . Additionally, explicit feed-forward 1-Lipschitz convolutional networks have important inductive biases that have been crucial for achieving good certified robustness results on image-classification tasks . In contrast, our current understanding on how to incorporate the right inductive biases for LBEN in the context of certified robustness is relative limited. Therefore, fine-tuning LBEN from 1-Lipschitz layers with good inductive biases (in our case SLL) helps LBEN achieve improved certified robustness via combining the benefits of inductive biases of feed-forward 1-Lipschitz networks and the expressive advantage of LBEN over explicit networks.

LBEN initializations with Lipschitz constant other than \(L=1\).Larger Lipschitz constant parameterizations for LBEN are explored in  on CIFAR10 (\(L=2,3,5,50\)), which slightly improves clean accuracy, _but decreases the empirical robustness_ when compared to the 1-Lipschitz LBEN. The best certified robustness results achieved by our approach on CIFAR10 and CIFAR100 tasks are indeed achieved by choosing \(L=1\). At this moment, the understanding of how to incorporate inductive bias via enforcing convolution structures on 1-Lipschitz layers is relatively matured. Hence choosing \(L=1\) to make the Lipschitz constant consistent with these structures leads to the best certified robustness result for now. In the future, it is possible that one can improve the certified robustness of DEQ for \(L>1\) by developing new convolutional structures for LBEN.

**Remark.** _The results stated in Table 1 are not on par with the state-of-the-art certified robust accuracy obtained by the largest explicit Lipschitz feed-forward models. This is due to the following reasons: 1) DEQs are known to be computationally expensive as they require the computation of a fixed point for each input \(x\). Therefore, in order to keep the computational cost for LBEN models reasonable, we trained small SLL networks. 2) There is a technical subtlety of embedding a convolutional SLL model into DEQ that requires a much larger convolutional kernel. This also currently prevents us from scaling up to larger vision tasks like TinyImageNet, but may be circumvented in the future. For applications using standard fully-connected layers, the representation memory footprint of an equivalent DEQ does not present such an issue. Despite these challenges, to the best of our knowledge, our work provides the best \(_{2}\)-certified accuracy with DEQs._

## 5 Conclusion

In this paper, we present a unified algebraic approach for deriving 1-Lipschitz DEQ structures via providing analytical solutions to an SDP condition. We show that several Lipschitz structures (_e.g._, CPL, SLL, AOL, and MonDEQ) can be reparameterized as special cases of LBEN without changing the original Lipschitz constant specifications. We provide explicit formulas for such Lipschitz-preserved reparameterization. Finally, our experiments show that our theory opens the possibility of achieving improved certified robustness of DEQs via a combination of network reparameterization, structure-preserving regularization, and LBEN-based fine-tuning.