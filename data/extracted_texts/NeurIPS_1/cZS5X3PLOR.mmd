# Data Minimization at Inference Time

Cuong Tran

Department of Computer Science

University of Virginia

kxb7sd@virginia.edu

&Ferdinando Fioretto

Department of Computer Science

University of Virginia

fioretto@virginia.edu

###### Abstract

In domains with high stakes such as law, recruitment, and healthcare, learning models frequently rely on sensitive user data for inference, necessitating the complete set of features. This not only poses significant privacy risks for individuals but also demands substantial human effort from organizations to verify information accuracy. This paper asks whether it is necessary to use _all_ input features for accurate predictions at inference time. The paper demonstrates that, in a personalized setting, individuals may only need to disclose a small subset of their features without compromising decision-making accuracy. The paper also provides an efficient sequential algorithm to determine the appropriate attributes for each individual to provide. Evaluations across various learning tasks show that individuals can potentially report as little as 10% of their information while maintaining the same accuracy level as a model that employs the full set of user information.

## 1 Introduction

The remarkable success of machine learning (ML) models also brought with it pressing challenges at the interface of privacy and decision-making,especially when deployed in consequential domains such as legal processes, banking, hiring, and healthcare . A particularly intriguing aspect is the conventional requirement for users to disclose their entire set of features during inference, thereby creating a gateway for potential data breaches, as exemplified in recent instances where millions of individuals' data were compromised []. Concurrently, this practice also places a burden on companies and organizations to ensure the accuracy and legal compliance of the disclosed information, as often observed in financial operations, mandated by legislations like the Corporate Transparency Act[].

Significantly, this conventional approach of disclosing the entire feature set during inference might also also violate the data minimization principle, a cornerstone of several global privacy regulations including the Eurpean General Data Protection Regulation , the California Privacy Rights Act , and the Brazilian General Data Protection Law , among others. Through this lens, the discourse on enhancing privacy and reducing the verification onus in ML systems, particularly in crucial decision-making domains, gains a nuanced dimension, warranting a thorough examination.

This paper challenges this setting and asks whether it is necessary to require _all_ input features for a model to produce accurate or nearly accurate predictions during inference. We refer to this question as the _data minimization for inference_ problem. This unique question bears profound implications for privacy in model personalization, which often necessitates the disclosure of substantial user data. We show that, under a personalized setting, each individual may only need to release a small subset of their features to achieve the _same_ prediction errors as those obtained when all features are disclosed. The overall framework is depicted in Figure 1. Following this result, we also provide an efficient sequential algorithm to identify the minimal set of attributes that each individual should reveal. Evaluations across various learning tasks indicate that individuals may be able to report as little as 10% of their information while maintaining the same accuracy level as a model using the complete set of user information.

In summary, the paper makes the following contributions: **(1)** it initiates a study to analyze the optimal subset of data features that each individual should disclose at inference time in order to achieve the same accuracy as if all features were disclosed; **(2)** it links this analysis to a new concept of _data minimization for inference_ in relation to privacy, **(3)** it proposes theoretically motivated and efficient algorithms for determining the minimal set of attributes each individual should provide to minimize their data; and **(4)** it conducts a comprehensive evaluation illustrating the effectiveness of the proposed methods in preserving privacy at no or small costs in accuracy.

Related work.To the best of our knowledge, this is the first work studying the connection between data minimization and accuracy at inference time. This work was motivated by the core principle in data minimization which states that _"all collected data shall be adequate, relevant and limited to what is necessary in relation to the purposes for which they are processed"_. This principle is adopted as a privacy cornestone in different data protection regulations, including the General Data Protection Regulation  in Europe, the California Privacy Rights Act in the US , the General Personal Data Protection Law in Brasil , the Protection of Personal Information Act, in South Africa , and the Personal Information Protection Act, in South Korea .

While we are not aware of studies on data minimization for inference problems, we draw connections with differential privacy, feature selection, and active learning. _Differential Privacy (DP)_ is a strong privacy notion that determines and bounds the risk of disclosing sensitive information of individuals participating in a computation. In the context of machine learning, DP ensures that algorithms can learn the relations between data and predictions while preventing them from memorizing sensitive information about any specific individual in the training data. In such a context, DP is primarily adopted to protect training data  and thus the setting contrasts with that studied in this work, which focuses on identifying the superfluous features revealed by users at _test time_ to attain high accuracy. Furthermore, achieving tight constraints in differential privacy often comes at the cost of sacrificing accuracy, while the proposed privacy framework can reduce privacy loss without sacrificing accuracy under the assumption of linear classifiers.

Feature selection  is the process of identifying a relevant subset of features from a larger set for use in model construction, with the goal of improving performance by reducing the complexity and dimensionality of the data. The problem studied in this work can be considered a specialized form of feature selection with the added consideration of personalized levels, where each individual may use a different subset of features. This contrasts standard feature selection , which selects the same subset of features for each data sample. Additionally, and unlike traditional feature selection, which is performed during training and independent of the deployed classifier , the proposed framework performs feature selection at deployment time and is inherently dependent on the deployed classifier.

The framework proposed in this paper shares some similarities with _active learning_, whose goal is to iteratively select samples for experts to label in order to construct an accurate classifier with the least number of labeled samples. Similarly, the proposed framework iteratively asks individuals to reveal one attribute given their released features so far, with the goal of minimizing the uncertainty in model predictions. Finally, our study share some connections with active feature acquisition  which can be categorized as one branch in active learning. However, these priors focus mostly on minimizing the total number of users features at training step. This is in sharp contrast with ours work which concentrate on inferece time.

Figure 1: Illustrative example of the proposed framework. The figure highlights the _online_ and _personalized_ nature of the minimization process. For each user, a key subset of features is identified to be used (core feature set). The minimized data sample is then processed by the pre-trained model, which generates a _representative_ label.

## 2 Settings and objectives

We consider a dataset \(D\) consisting of samples \((x,y)\) drawn from an unknown distribution \(\). Here, \(x\) is a feature vector and \(y=[L]\) is a label with \(L\) classes. The features in \(x\) are categorized into _public_\(x_{P}\) and _sensitive_\(x_{S}\) features, with their respective indexes in vector \(x\) denoted as \(P\) and \(S\), respectively. We consider classifiers \(f_{}:\), which are trained on a public dataset from the same data distribution \(\) above. The classifier produces a score \(_{}(x)^{L}\) over the classes and a final output class, \(f_{}(x)[L]\), given input \(x\). The model's outputs \(f_{}(x)\) and \(_{}(x)\) are also often referred to as hard and soft predictions, respectively.

Without loss of generality, we assume that all features in \(\) lie within the range \([-1,1]\). In this setting, we are given a trained model \(f_{}\) and, at inference time, we have access to the public features \(x_{P}\). These features might be revealed through user queries or collected by the provider during previous interactions. Our focus is on the setting where \(|S||D|\), and for simplicity, the following considers binary classifiers, where \(=\{0,1\}\) and \(_{}\). Multi-class settings are addressed in Appendix C.

In this paper, the term **data leakage** of a model, refers to the percentage of sensitive features that are revealed unnecessarily, meaning that their exclusion would not significantly impact the model's output. _Our goal is to design algorithms that accurately predict the output of the model using the smallest possible number of sensitive features_, _thus minimizing the data leakage at inference time_. This objective reflects our desire for privacy.

To clarify key points discussed in the paper, let us consider a loan approval task where individual features are represented by the set \(\{,(),()\}\). In this example, the Job feature \( x_{P}\) is public, whereas Loc and Inc\( x_{S}\) are sensitive. We also consider a trained linear model \(f_{}=1.0\,-0.5\,+0.5\, 0\) and look at a scenario where user (A) has a public feature Job\(=1.0\), and user (B) has a public feature Job\(=-0.9\). Both users' sensitive feature values are unknown. However, for user A, the outcome can be conclusively determined without revealing any additional information since all features are bounded within \([-1,1]\). In contrast, for user B, the outcome cannot be determined solely based on the public feature, but revealing the sensitive feature Loc\(=1.0\) is enough to confirm the classifier outcome.

This example highlights two important observations that motivate our study: **(1)**_not all sensitive attributes may be required for decision-making during inference_, and **(2)**_different individuals may need to disclose different amounts and types of sensitive information for decision-making_.

## 3 Core feature sets

With these considerations in mind, this section introduces the notion of _core feature set_, the _first contribution of the paper_, which will be used to quantify data minimization. The paper presents the key findings and defers all proofs in Appendix A.

Throughout the paper, the symbols \(R\) and \(U\) are used to represent the sets of indices for revealed and unrevealed features of the sensitive attribute \(S\), respectively. Given a vector \(x\) and an index set \(I\), we use \(x_{I}\) to denote the vector containing entries indexed by \(I\) and \(X_{I}\) to represent the corresponding random variable. Finally, we write \(f_{}(X_{U},X_{R}\!=\!x_{R})\) as a shorthand for \(f_{}(X_{U},X_{R}\!=\!x_{R},X_{P}\!=\!x_{P})\) to denote the prediction made by the model when the features in \(U\) are unrevealed.

_Our objective is to develop algorithms that can identify the smallest subset of sensitive features to disclose_, ensuring that the model's output is accurate (with high probability) irrespective of the values of the undisclosed features. We refer to this subset as the _core feature set_.

**Definition 1** (Core feature set).: _Consider a subset \(R\) of sensitive features \(S\), and let \(U\!=\!S R\) be the unrevealed features. The set \(R\) is a core feature set if, for some \(\),_

\[(f_{}(X_{U},X_{R}=x_{R})=) 1-, \]

_where \(\) is a failure probability._

When \(=0\) the core feature set is called **pure**. Additionally, the label \(\) satisfying Equation (1) is called the **representative label** for the core feature set \(R\). The concept of the representative label \(\) is crucial for the algorithms that will be discussed later. These algorithms use limited information tomake predictions and when predictions are made using a set of unrevealed features, the representative label \(\) will be used in place of the model's prediction.

In identifying core feature sets to minimize data leakage, it's crucial to consider model uncertainty, which refers to the unknown values of unrevealed features. The following result links core feature sets with model entropy, which measures uncertainty and is used by this work to minimize data leakage.

**Proposition 1**.: _Let \(R S\) be a core feature set with failure probability \(<0.5\). Then, there exists a monotonic decreasing function \(:_{+}_{+}\) with \((1)=0\) such that:_

\[Hf_{}(X_{U},X_{R}=x_{R})(1-),\]

_where \(H[Z]\)= \(-_{z[L]}(Z=z)(Z=z)\) is the entropy of the random variable \(Z\)._

This property _highlights the relationship between core feature sets and entropy associated with a model using incomplete information_. As the \(\) value decreases, the model's predictions become more certain. When \(\) equals zero (or when \(R\) represents a pure core feature set), the model's predictions can be fully understood without observing \(x_{U}\), resulting in entropy of \(0\).

It is worth noticing that enhancing prediction accuracy necessitates revealing additional information, as illustrated by the previous result and the renowned information theoretical proposition below:

**Proposition 2**.: _Given two subsets \(R\) and \(R^{}\) of sensitive features \(S\), with \(R R^{}\),_

\[Hf_{}(X_{U},X_{R}=x_{R}) Hf_{}(X_{U^{ }},X_{R^{}}=x_{R^{}}),\]

_where \(U=S R\) and \(U^{}=S R^{}\)._

Thus, the parameter \(\) plays a crucial role in balancing the trade-off between _privacy loss_ and _model prediction's uncertainty_. It determines the amount of sensitive information that must be disclosed to have enough confidence on model predictions. As \(\) increases, fewer sensitive features need to be revealed, resulting in reduced data leakage but also less certainty on model predictions, and vice versa. Note that disclosing more sensitive attributes _does not necessarily improve the accuracy of predictions for a specific individual_, as highlighted in previous research . Although it's challenging to rigorously analyze the relationship between privacy loss and predictive accuracy across various contexts, our empirical studies across multiple datasets and learning algorithms do suggest that exposing more sensitive features generally enhances prediction accuracy. Thus, the parameter \(\) appears to govern the trade-off between sacrificing privacy and gaining predictive performance.

As highlighted in the previous example, the core feature set is not uniform for all users. This is further exemplified in Figure 2, using the Credit dataset  with a logistic regression classifier. The figure reports the cumulative count of users against the minimum number of features they need to disclose to ensure confident predictions. It demonstrates that many individuals need to disclose _no_ additional information to attain accurate predictions (corresponding to a pure feature of set size \(0\)), and most individuals can achieve accurate predictions by disclosing only \( 2\) sensitive features. These insights, together with the previous observations linking core feature sets to entropy, motivate the proposed online algorithm, the second contribution of the paper.

## 4 MinDRel: An algorithm to minimize data release at inference time

The goal of the proposed algorithm, called _Minimize Data Reveal_ (MinDRel), is to uphold privacy during inference by revealing sensitive features one at a time based on their _released_ feature values. This section provides a high-level description of the algorithm and outlines its challenges. Next, Section 5, applies MinDRel to linear classifiers and discusses its performance on several datasets and benchmarks. Further, Section 6, extends MinDRel to non-linear classifiers and considers an evaluation over a range of standard datasets.

**Overview of MinDRel.** MinDRel operates fundamentally on two critical actions: _1. determining the next feature to reveal_ for each user and _2. verifying whether the disclosed features make up a core feature set_ for that user. These two operations will be discussed in sections 4.1 and 4.2, respectively.

Figure 2: Frequency associated with the size of the **minimum** pure core feature set.

The algorithm determines which feature to disclose for a specific user by inspecting the posterior probabilities \((X_{j}|X_{R}=x_{R},X_{P}=x_{P})\) for each unrevealed feature \(j U\), taking into account the disclosed sensitive features \(x_{R}\) and public features \(x_{P}\). Given the current set of disclosed features \(x_{R}\) and unrevealed features \(x_{U}\), MinDRel chooses the subsequent feature \(j U\) as follows:

\[j=*{argmax}_{j U}F(x_{R},x_{j};)*{ argmax}_{j U}-Hf_{}(X_{j}=x_{j},X_{U\{j\}},X_{R}=x_{R}) , \]

where \(F\) is a _scoring function_ that evaluates the amount of information that can be acquired about the model's predictions when feature \(X_{j}\) is disclosed. As suggested in previous sections, it's desirable to reveal the feature that provides the most insight into the model prediction upon disclosure. MinDRel uses _Shannon entropy_ for this purpose as it offers a natural method for quantifying information. Once feature \(X_{j}\) is disclosed with a value of \(x_{j}\), the algorithm updates the posterior probabilities for all remaining unrevealed features. The process concludes either when all sensitive features have been disclosed or when a core feature set has been identified. It should also be noted that, within this framework, there is no need to perform data imputation when some features are missing. Unrevealed features are treated as random variables and are integrated during the prediction process.

Both the computation of the scoring function \(F\) and the verification of whether a set of disclosed features constitute a core feature set present two significant challenges for the algorithm. The rest of the section delves into these difficulties.

### Computing the scoring function \(F\)

Designing a scoring function \(F\) that measures how confident a model's prediction is when a user discloses an additional feature \(X_{j}\) brings up two key challenges. **First**, _the value of \(X_{j}\) is unknown until the decision to reveal it is made_, which complicates the computation of the entropy function. **Second**, even if the value of \(X_{j}\) were known, _determining the entropy of model predictions in an efficient manner_ is another difficulty. We next discuss how to overcome these challenges.

**Dealing with unknown values.** To address the first challenge, we exploit the information encoded in the disclosed features to infer \(X_{j}\)'s value and compute the posterior probability \((X_{j}|X_{R}\!=\!x_{R})\) of the unrevealed feature \(X_{j}\) given the values of the revealed ones. The scoring function, abbreviated as \(F(X_{j})\), can thus be modeled as the expected negative entropy given the randomness of \(X_{j}\),

\[F(X_{j}) =_{X_{j}}-H[f_{}(X_{j},X_{U\{j\}},X_{R}\!=\!x_{R})\] \[=-f_{}(X_{j}\!=\!z,X_{U\{ j\}},X_{R}\!=\!x_{R})}_{A}\!=\!z|X_{R}\!=\!x_{R} )}_{B}dz, \]

where \(z_{j}\) is a value in the support of \(X_{j}\).

**Efficient entropy computation.** The second difficulty relates to how to estimate this scoring function efficiently. Indeed this is challenged by two key components. The first (A) is the entropy of the model's prediction given a specific unrevealed feature value, \(X_{j}=z\). This prediction is a function of the random variable \(X_{U\{j\}}\), and, due to Proposition 1, its estimation is linked to the conditional densities \((X_{U\{j\}}|X_{R}=x_{R},X_{j}=z)\). The second (B) is the conditional probability \(Pr(X_{j}=z|X_{R}=x_{R})\). Efficient computation of these conditional densities is discussed next.

First, we discuss a result relying on the joint Gaussian assumption of the input features. This result will be useful in providing a computationally efficient method to estimate such conditional density functions. In the following, \(_{IJ}\) represents a sub-matrix of size \(|I||J|\) of a matrix \(\) formed by selecting rows indexed by \(I\) and columns indexed by \(J\).

**Proposition 3**.: _The conditional distribution of any subset of unrevealed features \(U^{} U\), given the the values of released features \(X_{R}=x_{R}\) is given by:_

\[(X_{U^{}}|X_{R}=x_{R})=_{U^{}}+_{U^ {}R}_{RR}^{-1}(x_{R}-_{R}),_{U^{}U^{}}- _{U^{}R}_{RR}^{-1}_{RU^{}},\]

_where \(\) is the covariance matrix._

Note that Equation (3) considers \(U^{}=\{j\}\), and thus, component (B) can be computed efficiently exploiting the result above. To complete Equation (3), we need to estimate the entropy \(H[f_{}(X_{j}=z,X_{U j},X_{R}=x_{R})]\) (component A) for a specific instance \(z\) drawn from \((X_{j}|X_{R}=x_{R})\). This poses a challenge due to the non-linearity of the hard model predictions \(f_{}\) adopted. To tackle this computational challenge, we first estimate component A using soft labels \(_{}\) and then apply a thresholding operator. More specifically, we first estimate \((_{}(X_{j}=z,\)\(X_{U\{j\}},X_{R}=x_{R}))\) and, based on this distribution, we subsequentially estimate \(f_{}\) as \(\{_{} 0\}\), where \(\) is the indicator function. In the following sections, we will show how to assess this estimate for linear and non-linear classifiers. Finally, by approximating the distribution over soft model predictions through Monte Carlo sampling, the score function in \(F(X_{j})\) can be computed as

\[F(X_{j})-}{{||}}_{z^{}}H[f_ {}(X_{j}=,X_{U\{j\}},X_{R}=x_{R})], \]

where \(\) is a set of random samples drawn from \((X_{j}|X_{R}=x_{R})\) and estimated through Proposition 3, which thus can be computed efficiently.

When the Gaussian assumption does not hold, one can recur to (slower) Bayesian approaches to estimate the uncertainty of unrevealed features \(X_{U}\) given the set of revealed features \(X_{R}=x_{R}\). A common approach involves treating \(X_{U}\) as the target variable and employing a neural network to establish the mapping \(X_{U}=g_{w}(X_{R})\). Utilizing Bayesian techniques [17; 18], the posterior of the network's parameter \(p(w|D)=p(w)p(D|w)\) can be computed initially. Based on the posterior distribution of the model's parameters \(w\), the posterior of unrevealed features can be calculated as \((X_{U}=x_{U}|X_{R}=x_{R})=_{w p(w|D)}(g_{w}(X_{R})=x_{U})\). However, implementing such a Bayesian network not only significantly increases training time but also inference time. Since it is necessary to compute \((X_{U}|X_{R}=x_{R})\) for all possible choices of \(U S\), the number of Bayesian neural network regressors scales exponentially with \(|S|\).

Importantly, in our evaluation, the data minimization method that operates under the Gaussian assumption maintains similar decision-making and produces comparable outcomes to the Bayesian approach, even in cases where the Gaussian assumption is not applicable in practical settings. Figure 3 illustrates this comparison, showcasing the performance of the proposed mechanism on a real dataset (Credit dataset with \(|S|=5\)) concerning accuracy (higher is better) and data leakage (lower is better) across various failure probability \(\) values. Notice how similar is the performance of the mechanisms that either leverage the Gaussian assumption or operate without it (Bayesian NN). Importantly, the assumption of a Gaussian distribution is not overly restrictive or uncommon. In fact, it is a cornerstone in many areas of machine learning, including Gaussian Processes , Bayesian optimization , and Gaussian Graphical models .

### Testing a core feature set

The proposed iterative algorithm terminates once it determines whether a subset \(R\) of the sensitive feature set \(S\) constitutes a core feature set. This validation process falls into two scenarios:

1. When \(=0\): To confirm that \(R\) is a pure core feature set, it is sufficient to verify that \(f_{}(X_{U},X_{R}=x_{R})\) remains constant for all possible realizations of \(X_{U}\). As we will show in Section 5, linear classifiers can perform this check in linear time without making any specific assumptions about the input distribution.
2. When \(>0\): In this case, the property above is no longer valid. As per Definition 1, to confirm a core feature set, it is essential to estimate the distribution of \((_{}(X_{U},X_{R}=x_{R}))\). In Section 5, we demonstrate how to analytically estimate this distribution for linear classifiers. Furthermore, in Section 6, we illustrate how to locally approximate this distribution for nonlinear classifiers and derive a simple yet effective estimator that can be readily implemented in practice.

## 5 MinDRel for linear classifiers

This section will devote to estimating the distribution \((_{}(X_{j}=z,X_{U\{j\}},X_{R}=x_{R}))\), simply expressed as \((_{}(X_{U},X_{R}=x_{R}))\) and provides an instantiation of MinDRel for linear classifiers.

Figure 3: Comparison between Bayesian NN vs Gaussian in term of accuracy and data Leakage.

In particular, it shows that both the estimation of the conditional distributions required to compute the scoring function \(F(X_{j})\) and the termination condition to test whether a set of revealed features is a core feature set, can be computed efficiently. This is an important property for the developed algorithms, which are designed to be online and interactive.

### Efficiently Estimating \((_{}(X_{U},X_{R}=x_{R}))\)

For a linear classifier \(_{}=^{}x\), notice that when the input features are jointly Gaussian, the model predictions \(_{}(x)\) are also Gaussian, as highlighted by the following result.

**Proposition 4**.: _The model soft prediction, \(_{}(X_{U},X_{R}=x_{R})=_{U}X_{U}+_{R}x_{R}\) is a random variable following a Gaussian distribution \(m_{f},_{f}^{2}\), with_

\[m_{f} =_{R}x_{R}+_{U}^{}_{U}+_{UR} _{RR}^{-1}(x_{R}-_{R}) \] \[_{f}^{2} =_{U}^{}_{UU}-_{UR}_{RR}^{-1} _{R}_{U}, \]

_where \(_{U}\) is the sub-vector of parameters \(\) corresponding to the unrevealed features \(U\)._

The result above is used to assist in calculating the conditional distribution of the model hard predictions \(f_{}(x)\), following thresholding. This is a random variable that adheres to a Bernoulli distribution, as shown next, and will be used to compute the entropy of the model predictions, as well as to determine if a subset of features constitutes a core set.

**Proposition 5**.: _Let the soft model predictions \(_{}(X_{U},X_{R}=x_{R})\) be a random variable following a Gaussian distribution \((m_{f},_{f}^{2})\). Then, the model prediction following thresholding \(f_{}(X_{U},X_{R}=x_{R})\) is a random variable following a Bernoulli distribution \((p)\) with \(p=(}{_{f}})\), where \(()\) is the CDF of the standard Normal distribution, and \(m_{f}\) and \(_{f}\), are given in Eqs (5) and (6), respectively._

### Testing pure core feature sets

In this subsection, we outline the methods for determining if a subset \(U\) is a pure core feature set, and, if so, identifying its representative label. As per Definition 1, \(U\) is a pure core feature set if \(f_{}(X_{U},X_{R}=x_{R})=\) for all \(X_{U}\). This implies that \(_{}(X_{U},X_{R}=x_{R})=_{U}^{}X_{U}+_{R}^{} x_{R}\) must have the same sign for all \(X_{U}\) in the range of \([-1,1]^{|U|}\). Given the box constraint \(X_{U}[-1,1]^{|U|}\), the linearity of the model considered allows us to directly compute the maximum and minimum values of \(_{}(X_{U},X_{R}=x_{R})\), rather than enumerating all possible values. Specifically, we have:

\[_{X_{U}}_{U}^{}X_{U}+_{R}^{}x_{R} =\|_{U}\|_{1}+_{R}^{}x_{R}\] \[_{X_{U}}_{U}^{}X_{U}+_{R}^{}x_{R} =-\|_{U}\|_{1}+_{R}^{}x_{R}.\]

Thus, if both these maximum and minimum values are negative (non-negative), then \(U\) is considered a pure core feature set with representative label \(=0\) (\(=1\)). If not, \(U\) is not a pure core feature set.

Importantly, determining whether a subset \(R\) of sensitive features \(S\) constitutes a pure core feature set can be accomplished in linear time with respect to the number of features.

**Proposition 6**.: _Assume \(f_{}\) is a linear classifier. Then, determining if a subset \(U\) of sensitive features \(S\) is a pure core feature set can be performed in \(O(|P|+|S|)\) time._

### MinDRel-linear Algorithm and Evaluation

A pseudo-code of MinDRel specialized for linear classifiers is reported in Algorithm 1. At inference time, the algorithm takes as input a sample \(x\) (which only exposes the set of public features \(x_{P}\)) and uses the training data \(D\) to estimate the mean and covariance matrix needed to compute the conditional distribution of the model predictions given the unrevealed features (line 1), as discussed above. After initializing empty the set of revealed features (line 2), it iteratively releases a feature at a time until a core feature set (and its associated representative label) are determined, as discussed in detail in Section 5.2. The released feature \(X_{j^{*}}\) is the one, among the unrevealed features \(U\), that maximizes the scoring function \(F\) (line 12). Computing such a scoring function entails estimating the conditional distribution \((X_{j}|X_{R}=x_{R})\) (line 8), constructing a sample set \(\) from such distribution (line 9), and approximating the distribution over soft model predictions through Monte Carlo sampling to compute (line 10). Finally, after each iteration, the algorithm updates the set of the revealed and unrevealed features (line 13).

Notice that MinDRel relies on estimating the mean vector and covariance matrix from the training data, which is considered public, for the scope of this paper. If the training data is private, various techniques exist to release DP mean, and variance [22; 7] and can be readily adopted. Nonetheless, the protection of training data is beyond the focus of this work.

Figure 4 reports the cumulative count of users (y-axis) against the minimum number of features they need to disclose (x-axis) to ensure confident predictions, for various failure probabilities \(\). The model adopted is a Logistic Regression classifier trained on the Bank dataset . The data minimization achieved by MinDRel is clearly apparent. For each test sample, MinDRel identifies core feature sets that are significantly smaller than the total sensitive feature set size \(|S|=7\). Interestingly, when \(>0\), it discovers core feature sets smaller than 2 for the majority of users. _This implies that most users would only need to reveal a small portion of their sensitive data to achieve accurate model predictions with either absolute certainty or high confidence_.

## 6 MinDRel for non-linear classifiers

Next, the paper focuses on computing the estimate \((_{}(X_{U},X_{R}=x_{R}))\) and determining core feature sets when \(f_{}\) is a nonlinear classifier. Then, the section presents results that illustrate the practical benefits of MinDRel for data minimization at inference time on neural networks.

### Efficiently estimating \(Pr(_{}(X_{U},X_{R}=x_{R}))\)

First notice that even if the input features \(x\) are jointly Gaussian, the outputs \(f_{}(x)\) will no longer adhere to a Gaussian distribution after a non-linear transformation. This complicates estimating the distribution \((_{}(X_{U},X_{R}=x_{R})\). To tackle this challenge, the paper takes a local approximation of the model predictions \(_{}\) using a Gaussian distribution, as demonstrated in the following result.

**Theorem 1**.: _The distribution of the random variable \(_{}=_{}(X_{U},X_{R}=x_{R})\) where \(X_{U}_{U}^{},_{U}^{}\) can be approximated by a Normal distribution as_

\[_{}(_{}(X_{U}=_{U}^{pos}, X_{R}=x_{R}),\,g_{U}^{}_{U}^{}g_{U}), \]

_where \(g_{U}=_{X_{U}}_{}(X_{U}=_{U}^{pos},X_{R}=x_{R})\) is the gradient of model prediction at \(X_{U}=_{U}^{}\)._

Therein, the mean vector \(_{U}^{}\) and covariance matrix \(_{U}^{}\) of \(Pr(X_{U}|X_{R}=x_{R})\) are derived from Proposition 3. This result leverages a first-order Taylor approximation of model \(f_{}\) around its mean.

Figure 4: Cumulative count of users against their core feature set size for various \(\).

### Testing pure core feature sets

Unlike linear classifiers, the case for nonlinear models lacks an exact and efficient method to determine whether a set is a core feature set. This is primarily due to the non-convex nature of the adopted models, which poses challenges in finding a global optimum. This section thus proposes an approximate testing routine and demonstrates its practical ability to significantly minimize data leakage during testing while maintaining high accuracy.

To determine if a subset \(U\) of the sensitive features \(S\) is a pure core feature set, we consider a set \(Q\) of input points \((X_{U},x_{R})\). The entries corresponding to the revealed features are set to the value \(x_{R}\), while the entries corresponding to the unrevealed features are sampled from the distribution \((X_{U}|X_{R}=x_{R})\). The test verifies if the model predictions \(f_{}(x)\) remain constant for all \(x\) in \(Q\). In the next section, we will show that even considering arbitrary classifiers (e.g., we use standard neural networks), MinDRel can reduce data leakage dramatically when compared to standard approaches. The MinDRel algorithm for nonlinear classifiers differs from Algorithm 1 primarily in the method used to compute the estimates for the distribution \((f_{}(X_{j}=z,X_{U j},X_{R}=x_{R}))\) of the soft model predictions (line 11). In this case, this estimate is computed by leveraging the results from Theorem 1 and Proposition 5. Moreover, the termination test of the algorithm is based on the discussion in the previous section. Appendix B reports a description of the algorithm's pseudocode.

## 7 Experiments

Datasets and settings.This section evaluates MinDRel's effectiveness in limiting data exposure during inference. The experiments are conducted on six standard UCI datasets . To further emphasize the benefits of MinDRel, we compare it to two baselines: the _optimal_ model, which employs a brute force method to find the smallest core feature set and its representative label and assumes all sensitive features are known, and the _all-features_ model, which simply adopts the original classifiers using all the data features for each test sample. The performances are displayed for a varying number of sensitive attributes \(|S|\), while we delegate a study for larger \(|S|\) to the Appendix (which excludes the time-consuming baseline _optimal_, as intractable for large \(|S|\)). For each choice of \(|S|\), we randomly select \(|S|\) features from the entire feature set and designate them as sensitive attributes. The remaining attributes are considered public. The average accuracy and data leakage are then reported based on 100 random sensitive attribute selections. Due to page limit, we present a selection of the results and discuss their trends on Bank dataset. A comprehensive overview, additional analysis, and experiments are available in the Appendix D.

### Linear classifiers

Figure 5 depicts performance results in accuracy (top subplots/higher is better) and relative data leakage (bottom subplots/lower is better) with varying number of revealable sensitive features \(|S|\). The comparison includes three MinDRel versions: F-Score, which utilizes the scoring function elaborated in Section 4.1 to select the next feature to disclose (left); Importance, which employs a feature importance criterion leveraging the model \(f_{}\) parameters to rank features (middle), detailed in the Appendix; and Random, which arbitrarily selects the next feature to reveal (right). All three versions use the same test procedure to validate whether a set qualifies as a core feature set.

Firstly, notice that MinDRel achieves equal or better accuracy than the optimal mechanism and baseline that utilizes all features during testing. _This suggests that data minimization can be accomplished under linear models without compromising accuracy!_ Next, observe that an increase in \(\)

Figure 5: Comparison of F-Score, Importance and Random method for different \(\) with two baseline methods: Optimal and All features in term of Accuracy and Data Leakage. The underlying classifier is a logistic regression classifier.

aids in safeguarding data minimization, as evidenced by the drop in relative data leakage (note the logarithmic scale of the y-axis). This is attributable to the influence of \(\) on the test for identifying a core feature set, thereby reducing its size.

Finally, the proposed scoring function outperforms other versions in terms of data leakage minimization, allowing users to disclose substantially fewer sensitive features. The Appendix also includes experiments with larger quantities of sensitive features, presenting analogous trends, where, however, a comparison against an optimal baseline was not possible in due to its exponential time complexity.

### Non-linear classifiers

To assess MinDRel's performance in reducing data leakage when employing standard nonlinear classifiers, we use a neural network with two hidden layers and ReLU activation functions and train the models using stochastic gradient descent (see Appendix D for additional details). The evaluation criteria, baselines, and benchmarks adhere to the same parameters set in Section 5.3.

Figure 6 showcases the results in terms of accuracy (top subplots) and data leakage (bottom subplots). Unlike linear classifiers, nonlinear models using MinDRel with a failure probability of \(=0\) cannot guarantee the same level of accuracy as the "all features" baseline model. However, this accuracy difference is minimal. Notably, a failure probability of \(=0\) enables users to disclose less than half, and up to 90% fewer sensitive features across different datasets, while achieving accuracies similar to those of conventional classifiers. Next, similarly to as observed in the previous section, MinDRel with the proposed F-score selector significantly outperforms the other variants in terms of data leakage minimization. Furthermore, when considering higher failure probabilities, data leakage decreases significantly. For instance, with \( 0.1\), users need to disclose only 5% of their sensitive features while maintaining accuracies comparable to the baseline models (the largest accuracy difference reported was 0.005%). These results are significant: _They show that the introduced notion of privacy leakage and the proposed algorithm can become a valuable tool to protect individuals' data privacy at test time, without significantly compromising accuracy.._

## 8 Conclusion and Future Work

This paper introduced the concept of data minimization at test time whose goal is to minimize the number of features that individuals need to disclose during model inference while maintaining accurate predictions from the model. The motivations of this notion are grounded in the privacy risks imposed by the adoption of learning models in consequential domains, and align with the data minimization principle. The paper then discusses an iterative and personalized algorithm that selects the features each individual should release with the goal of minimizing data leakage while retaining exact (in the case of linear classifiers) or high (for non-linear classifiers) accuracy. Experiments over a range of benchmarks and datasets indicate that individuals may be able to release as little as 10% of their information without compromising the accuracy of the model, providing a strong argument for the effectiveness of this approach in protecting privacy while preserving the accuracy of the model.

While this study is the first attempt at defining data minimization during inference, it also opens up avenues for further research. First, establishing bounds on the data leakage provided by our proposed method compared to an optimal procedure presents an interesting and open challenge. Second, exploring the relationship between data minimization principles and their consequent disparate impacts presents another open direction. Lastly, developing effective algorithms to provably construct core feature sets for non-linear classifiers is another important area of investigation.

Figure 6: Comparison of F-Score, Importance and Random method for different \(\) with two baseline methods: Optimal and All features in term of Accuracy and Data Leakage. The underlying classifier is a neural network.