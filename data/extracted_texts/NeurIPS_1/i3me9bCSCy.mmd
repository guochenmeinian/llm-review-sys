# Set-based Neural Network Encoding

Without Weight Tying

 Bruno Andreis\({}^{1}\), Soro Bedionita\({}^{1}\), Philip H.S. Torr\({}^{2}\), Sung Ju Hwang\({}^{1,3}\)

KAIST \({}^{1}\), South Korea

University of Oxford, United Kingdom \({}^{2}\)

DeepAuto.ai, South Korea \({}^{3}\)

{andries, sorobedo, sjhwang82}@kaist.ac.kr, philip.torr@eng.ox.ac.uk

###### Abstract

We propose a neural network weight encoding method for network property prediction that utilizes set-to-set and set-to-vector functions to efficiently encode neural network parameters. Our approach is capable of encoding neural networks in a model zoo of mixed architecture and different parameter sizes as opposed to previous approaches that require custom encoding models for different architectures. Furthermore, our **S**et-based **N**eural network **E**ncoder (SNE) takes into consideration the hierarchical computational structure of neural networks. To respect symmetries inherent in network weight space, we utilize Logit Invariance to learn the required minimal invariance properties. Additionally, we introduce a _pad-chunk-encode_ pipeline to efficiently encode neural network layers that is adjustable to computational and memory constraints. We also introduce two new tasks for neural network property prediction: cross-dataset and cross-architecture. In cross-dataset property prediction, we evaluate how well property predictors generalize across model zoo trained on different datasets but of the same architecture. In cross-architecture property prediction, we evaluate how well property predictors transfer to model zoo of different architecture not seen during training. We show that SNE outperforms the relevant baselines on standard benchmarks.

## 1 Introduction

Recently, deep learning methods have been applied to a wide range of fields and problems. With this broad range of applications, huge volumes of datasets are continually being made available in the public domain together with neural networks trained on these datasets. Given this abundance of trained neural network models, the following curiosity arises: what can we deduce about these networks with access only to the parameter values? More generally, can we predict properties of these networks such as generalization performance on a testset(without access to the test data), the dataset on which the model was trained, the choice of optimizer and learning rate, the number of training epochs, choice of model initialization etc. through an analysis of the model parameters? The ability to infer such fundamental properties of trained neural networks using only the parameter values has the potential to open up new application and research paradigms (De Luigi et al., 2023; Zhou et al., 2023; Navon et al., 2023; De Luigi et al., 2023) such as learning in the latent space of neural network weights for tasks such as weight generation (Schirholt et al., 2021; Soro et al., 2024), latent space transfer of weights across datasets allowing for transferring weights from one dataset to another as was recently demonstrated in Soro et al. (2024) and latent space optimization using gradient descent where optimization is performed on the weight embeddings (Rusu et al., 2018).

We tackle two specific versions of this problem: predicting a) frequencies of Implicit Neural Representations (Sitzmann et al., 2020) (INRs), and b) the performance of CNNs and Transformers,given access only to the network weights. The first approach to solving this problem, proposed by Unterthiner et al. (2020), involves computing statistics such as the mean, standard deviation and quantiles, of each layer in the network, concatenating them to a single vector that represents the neural network encoding, and using this vector to predict the desired network property. Another approach, also proposed as a baseline in Unterthiner et al. (2020), involves flattening all the parameter values of the network into a single vector which is then fed as input to layers of multilayer perceptrons (MLPs) to predict the property of interest. An immediate consequence of this approach is that it is practical only for moderately sized neural network architectures. Additionally, this approach ignores the hierarchical computational structure of neural networks through the weight vectorization process. The second, and most recent approach to this problem, proposed by Zhou et al. (2023a,b), takes a geometric approach to the problem by building neural network weight encoding functions, termed neural functionals, that respect symmetric properties of permutation invariance and equivariance of the hidden layers of MLPs under the action of an appropriately applied permutation group. While this approach respect these fundamental properties in the parameter space, it's application is restricted, strictly, to MLPs. Also, even when relaxations are made to extend this method to convolutional networks and combinations of convolutional layers and MLPs, these only work under strict conditions of equivalence in the channel size in the last convolutional layer and the first linear layer. Hence it is clear that while the methods of Zhou et al. (2023a,b) and Navon et al. (2023) enjoy nice theoretical properties through weight tying, their application is limited to only a small subset of carefully chosen architectures.

Moreover, these approaches (Unterthiner et al., 2020; Zhou et al., 2023a,b; Navon et al., 2023) have a fundamental limitation: their encoding methods are applicable only to a single fixed, pre chosen neural network architecture. Once the performance predictor is trained, in the case of Unterthiner et al. (2020), and the neural network encoder of Zhou et al. (2023a) and Navon et al. (2023) are defined, they cannot be used to predict the performance of neural networks of different architecture. These issues are partly addressed by the graph based approaches of Kofinas et al. (2024) and Lim et al. (2023). Consequently, evaluating these models on diverse architectures is impossible without training an new performance predictor for each architecture.

To this end, we propose a Set-based Neural Network Encoder (SNE) for property prediction of neural networks given only the model parameters that is agnostic to the network architecture without weight tying. Specifically, we treat the neural network encoding problem from a set encoding perspective by utilizing compositions of _set-to-set_ and _set-to-vector_ functions. However, the parameters of neural networks are ordered and hierarchical. To retain this order information, we utilize positional encoding (Vaswani et al., 2017) at various stages in our model. Also, our model incorporates the hierarchical computational structure of neural networks in the encoder design by encoding independently, layer-wise, culminating in a final encoding stage that compresses all the layer-wise information into a single encoding vector used to predict the network property of interest. To handle the issue of large and variable parameter sizes efficiently, we incorporate a _pad-chunk-encode_ pipeline that is parallelizable and can be used to iteratively encode layer parameters. To learn the correct permutations of MLP weights, we employ the Logit Invariance Regularizer of Moskalev et al. (2023)

Figure 1: **Legend:**\(\) Padding, \(\) Set-to-Set Function, \(\) Set-to-Vector Function, \(\) Layer-Level & \(\) Layer-Type Encoder. **Concept:**_(left)_ Given layer weights, SNE begins by padding and chunking the weights into _chunksizes_. Each chunk goes through a series of set-to-set and set-to-vector functions to obtain the chunk representation vector. Layer _level_ and _type_ positional encodings are used to inject structural information of the network at each stage of the chunk encoding process. All chunk encoding vectors are encoded together to obtain the layer encoding. _(right)_ All layer encodings in the neural network are encoded to obtain the neural network encoding vector again using as series of set-to-set and set-to-vector functions. This vector is then used to predict the neural network property of interest.

instead of weight tying. In terms of evaluation, we introduce two new tasks: cross-dataset neural network property prediction and cross-architecture neural network property prediction. In cross-dataset neural network performance prediction, we fix the network architecture used to generate the training data and evaluate how well the performance predictors transfer to the same architecture trained on different datasets. For cross-architecture neural network performance prediction, we fix only the architecture for generating the training data and evaluate the performance of the predictor on architectures unseen during training. These tasks are important since in the real setting, access to model zoos is scare, making transferability desirable.

Our contributions are as follows:

* We develop a Set-based Neural Network Encoder (SNE, see Figure 1) for network property prediction given access only to parameter values that can encode neural networks of arbitrary architecture, taking into account the hierarchical computational structure of neural networks.
* We introduce the cross-dataset property prediction task where we evaluate how well predictors transfer across neural networks trained on different datasets.
* We introduce the cross-architecture property prediction task where we evaluate how well property predictors trained on a specific architecture transfer to unseen architectures.
* We benchmark SNE against the relevant baselines (Unterthiner et al., 2020; Navon et al., 2023; Zhou et al., 2023a,b) on the cross-dataset task and show significant improvement over the baselines.
* We provide the first set of results on the cross-architecture task, a task for which most of the baselines, with the exception of Zaheer et al. (2017) and Kofinas et al. (2024) under special conditions (see remark in Section 4.2), cannot be used.

## 2 Related Work

**Set Functions:** Neural networks that operate on set (un)structured data have recently been used in many applications ranging from point cloud classification to set generation (Kim et al., 2021). Set functions are required to respect symmetric properties such as permutation invariance and equivariance. In DeepSets (Zaheer et al., 2017), a set of sum-decomposable functions are introduced that are equivariant in the Set-to-Set applications and invariant in the Set-to-Vector applications. In Set Transformers (Lee et al., 2019), a class of attention based Set-to-Set and Set-to-Vector functions are introduced that are more expressive and capable of modeling pairwise and higher order interactions between set elements. Recent works such as Bruno et al. (2021) and Willette et al. (2023) deal with the problem of processing sets of large cardinality in the the limited memory/computational budget regime. In this work, we utilize the class of set functions developed in Lee et al. (2019) to develop a neural network encoder for performance prediction that is agnostic to specific architectural choices. Our set-based formulation allows us to build such an encoder, capable of handling neural networks weights of arbitrary parameter sizes. This is a deviation from recent approaches to neural network encoding for property prediction that can encode only parameters of a single fixed architecture.

**Neural Network Property Prediction From Weights:** Predicting the properties of neural networks given access only to the trained parameters is a relatively new topic of research introduced by Unterthiner et al. (2020). In Unterthiner et al. (2020), two methods are proposed for predicting the generalization performance of neural networks: the first involves flattening the weights of the network into a single vector, processing it using multiple layers of MLPs to obtain an encoding vector which is then used to predict the performance. The second involves computing the statistics of each layer in the network, such as mean, variance, quantiles etc., concatenating them into a single vector that is then used for predicting the performance of the network. The most recent approach that we are aware of, Navon et al. (2023) and Zhou et al. (2023a,b), proposes a neural network weight encoder that is invariant or equivariant, depending on the application, to an appropriately applied permutation group to the hidden layers of MLPs. Two variants of their model is provided: one which operates only on the hidden layers, and conforms strictly to the theory of permuting MLP hidden neurons (Hechti-Nielsen, 1990), and a relaxation that assumes that the neurons of both the input and output layers of MLPs are permutable. Additionally, extensions are provided for convolutional layers. Kofinas et al. (2024) and Lim et al. (2023) represent weights using graph neural networks. Our approach, SNE, is directly comparable to these methods on the neural network property prediction task. However,unlike the methods of Unterthiner et al. (2020); Navon et al. (2023) and Zhou et al. (2023) which operate only on neural networks of fixed architecture, and consequently fixed number of encodable parameters, SNE is capable of encoding networks of arbitrary architecture. Moreover, SNE utilizes the hierarchical computation structure of neural networks by encoding, iteratively or in parallel, from the input to the output layers. Additionally, we go further than the experimental evaluation in Unterthiner et al. (2020); Navon et al. (2023) and Zhou et al. (2023, 2023) by introducing two new tasks: cross-dataset and cross-architecture neural network property prediction. Unterthiner et al. (2020); Navon et al. (2023) and Zhou et al. (2023, 2023) can only be benchmarked on the cross-dataset task where all networks in the model zoos are of the same architecture. Their restriction to a single fixed architecture makes cross-architecture evaluation impossible. Our method SNE, and those of Kofinas et al. (2024); Lim et al. (2023), on the other hand can be used for both tasks.

## 3 Set-based Neural Network Encoding Without Weight Tying

### Preliminaries

We have access to a dataset \(D=\{(x_{1},y_{1}),,(x_{n},y_{n})\}\) where for each \((x_{i},y_{i})\) pair, \(x_{i}\) represents the weights of a neural network architecture \(a\), sampled from a set of architectures \(\) and \(y_{i}\) corresponds to some property of \(x_{i}\) after it has been trained on a specific dataset \(d\). \(y_{i}\) can be properties such as generalization gap, training loss, the learning rate used to train \(x_{i}\), or even the number of epochs, choice of weight initialization, and optimizer used to train \(x_{i}\). Henceforth, we refer to \(D\) as a _model zoo_. For each \(x_{i} D\), \(x_{i}=[w_{i}^{0},,w_{i}^{|x_{i}|}]\) where \(w_{i}^{j}\) represents the weights (parameters) of the \(jth\) layer of the neural network \(x_{i}\), and \(|x_{i}|\) is the total number of layers in \(x_{i}\). Consequently, \(w_{i}^{0}\) and \(w_{i}^{|x_{i}|}\) are the input and output layers of \(x_{i}\) respectively. Additionally, we introduce the \(:x_{i}^{d_{i}}\) operation, that takes as input the weights of a neural network and returns the flattened weights and \(d_{i}\) is the total number of parameter is \(x_{i}\).

The neural network encoding problem is defined such that, we seek to compress \(x_{i}^{d_{i}}\) to a compact representation \(z_{x_{i}}^{h}\) such that \(z_{x_{i}}\) can be used to predict the properties \(y_{i}\) of \(x_{i}\) with \(h d_{i}\). In what follows, we present the details of our Set-based Neural Network Encoding (SNE) method capable of encoding the weights of neural networks of arbitrary architecture that takes into account the hierarchical computational structure of the given architecture and with efficient methods for processing weights of high dimension.

### Handling Large Layer Weights via Chunking

For a given layer \(w_{i}^{j} x_{i}\), the dimension of \(w_{i}^{j}\), \(|w_{i}^{j}|\) can be very large. For instance, when considering linear layers, flattening the weights can results in a tensor that can require large compute memory to be processable by another neural network. To resolve this issue, we resort to _chunking_. Specifically, for all layers \(w_{i}^{j} x_{i}\), we perform the following operations:

\[_{i}^{j}=(((w_{i}^{j}),c),c)=\{w_ {i}^{j_{0}},,w_{i}^{j_{q}}\}, \]

where for any \(w_{i}^{j_{t}}_{i}^{j}\), \(|w_{i}^{j_{t}}|^{c}\). Here, \(c\) is the _chunksize_, fixed for all layer types in the neural network and \(t[0,,q]\). The padding operation \((w_{i}^{j},c)\) appends zeros, if required, to extend \(w_{i}^{j}\) and make its dimension a multiple of the chunksize \(c\). To distinguish padded values from actual weight values, each element of \(_{i}^{j}\) has a corresponding set of masks \(_{i}^{j}=\{m_{i}^{j_{0}},,m_{i}^{j_{q}}\}\). Note that with this padding and subsequent chunking operation, each element in \(_{i}^{j}\) is now small enough, for an appropriately chosen chunksize \(c\), to be processed. Moreover, all the elements in \(_{i}^{j}\) can be processed in parallel. Importantly, chunksizes are chosen to ensure that weights from the same neurons are grouped together. This allows for principled learning of specific symmetries which we discuss later in Section 3.6. An ablation on the effect of chunksize is provided in Appendix E.

The model zoos we consider in the experimental section are populated by neural networks with stacks of convolutional and linear layers. For each such layer, we apply the padding and chunking operation differently. For a linear layer \(w_{i}^{j}^{}\), where out and in are the input and output dimensions respectively, we apply the flattening operation on both dimensions followed by padding and chunking. However for a convolutional layer \(w_{i}^{j}^{}\), we do not apply the flattening, padding, and chunking operations to the kernel dimensions k and operate only on the input and output dimensions since the kernels are small enough to be encoded together. Finally we note that for layers with bias values, we apply the procedure detailed above independently to both the weights and biases.

### Independent Chunk Encoding

The next stage in our Set-based Neural Network encoding pipeline is the individual encoding of each chunk of weight in \(_{i}^{j}=\{w_{i}^{j_{0}},,w_{i}^{j_{t}}\}\). For each \(w_{i}^{j_{t}}_{i}^{j}\), we treat the \(c\) elements as members of a set. However, it is clear that \(w_{i}^{j_{t}}\) has order in its sequence, _i.e._, an ordered set. We remedy this by providing this order information via positional encoding. Concretely, for a given \(w_{i}^{j_{t}}^{c 1}\), we first model the pairwise relations between all \(c\) elements using a _set-to-set_ function \(_{_{1}}\) to obtain:

\[_{i}^{j_{t}}=_{_{1}}(w_{i}^{j_{t}})^{c h}. \]

That is, \(_{_{1}}\) captures pair-wise correlations in \(w_{i}^{j_{t}}\) and projects all elements (weight values) to a new dimension \(h\).

Given \(_{i}^{j_{t}}^{c h}\), we inject two kinds of positionally encoded information. The first encodes the _layer type_ in a list of layers, _i.e._, linear or convolution for the model zoos we experiment with, to obtain:

\[_{i}^{j_{t}}=_{Layer}^{Type}(_{i}^{j_{t}}) ^{c h}. \]

Here we abuse notation and assign the output of \(()\) to \(_{i}^{i_{t}}\) to convey the fact that \(_{i}^{j_{t}}\)'s are modified in place and to simplify the notation. Also, all \(()\)s are variants of the positional encoding method introduced in Vaswani et al. (2017). Next we inject the layer level information. Since neural networks are computationally hierarchical, starting from the input to the output layer, we include this information to distinguish chunks, \(w_{i}^{j_{t}}\)s from different layers. Specifically, we compute:

\[_{i}^{j_{t}}=_{Layer}^{Level}(_{i}^{j_{t}}) ^{c h}, \]

where the input to \(_{Layer}^{Level}()\) is the output of Equation 3. We note that this approach is different from previous neural network encoding methods (Unterthiner et al., 2020) that loose the layer/type information by directly encoding the entire flattened weights hence disregarding the hierarchical computational structure of neural networks. Experimentally, we find that injecting such positionally encoded information improves the models performance (Ablation E).

We further model pairwise correlations in \(_{i}^{j_{t}}\), now infused with layer/type information, using another set-to-set function \(_{_{2}}\):

\[_{i}^{j_{t}}=_{_{2}}(w_{i}^{j_{t}})^{c h}. \]

The final step in the chunk encoding pipeline involves compressing all \(c\) elements in \(_{i}^{j_{t}}\) to a compact representation. For this, we use a _set-to-vector_ function \(_{_{a}}:^{c h}^{h}\). In summary, the chunk encoding layer computes the following function:

\[_{i}^{j_{t}}=_{_{a}}[_{_{2}}(_{ Layer}^{Level}(_{Layer}^{Type}(_{_{1}}(w_{i}^{j_{t}}))))], \]

where \(_{i}^{j_{t}}^{1 H}\). Note that for each chunked layer \(_{i}^{j}=\{w_{i}^{j_{0}},,w_{i}^{j_{t}}\}\), the chunk encoder, Equation 6, produces a new set \(_{i}^{j}=[\{_{i}^{j_{0}},,_{i}^{j_{q}}\}]^{q h}\), which represents all the encodings of all chunks in a layer.

_Remark_ Our usage of set functions \(_{_{1}},_{_{2}}\) and \(_{_{a}}\) allows us to process layers of arbitrary sizes, which allows us to process neural networks of arbitrary architecture using a single model, a property lacking in previous approaches to neural network encoding (Zhou et al., 2023, 2020; Unterthiner et al., 2020; Navon et al., 2023).

### Layer Encoding

At this point, we have encoded all the chunked parameters of a given layer to obtain \(_{i}^{j}\). Encoding a layer, \(w_{i}^{j}\), then involves defining a function \(_{_{j}}:^{q h}^{1 h}\) for arbitrary \(q\). In practice, this is done by computing:

\[_{i}^{j}=_{_{}}[_{Layer}^{Level}( _{_{3}}(_{i}^{j}))]^{1 h}. \]Again we have injected the layer level information, via positional encoding, into the encoding processed by the set-to-set function \(_{_{3}}\). We then collect all the layer level encodings of the neural network \(x_{i}\):

\[_{i}=[_{i}^{0},,_{i}^{|x _{i}|}]^{|x_{i}| h}. \]

### Neural Network Encoding

With all layers in \(x_{i}\) encoded, we compute the neural network encoding vector \(z_{x_{i}}\) as follows:

\[z_{x_{i}}=_{_{}}[_{_{4}}(_{Layer}^{Level }(_{i}))]^{h}. \]

\(z_{x_{i}}\) compresses all the layer-wise information into a compact representation for the downstream task. Since \(_{_{}}\) is agnostic to the number of layers \(|x_{i}|\) of network \(x_{i}\), the encoding mechanism can handle networks of arbitrary layers and by extension architecture. Similar to the layer encoding pipeline, we again re-inject the layer-level information through positional encoding before compressing with \(_{_{}}\).

Henceforth, we refer to the entire encoding pipeline detailed so far as \(_{}(x_{i})\) for a network \(x_{i}\), where \(\) encapsulates the encoder parameters, \(_{_{1-4}},_{},_{}\) and \(_{}\).

### On Minimal Equivariance Without Weight Tying

Given an MLP, there exists permutations of the weights such that the networks are functionally equivalent . Since not all permutations are functionally correct, the encoder needs to learn the correct functional equivalence. To achieve this, we utilize the concept of Logit Invariance Regularization  where we constrain the output of the non-equivariant \(_{}\) (due to the positional encoding of input and output vectors) to respect the restricted functionally correct permutation group. This results in the following optimization problem:

\[*{minimize}_{}_{f}(D)+_{f}(D,G), \]

where \(G\) is the group of functionally equivariant permutations in the weight space, \(D\) is the training dataset and \(\) balances the task loss \(_{f}(D)\) and the Logit Invariance Regularization term \(_{f(D,G)}\). Proposition 3.1 of Moskalev et al.  guarantees that the resulting SNE model will have low sensitivity to functionally incorrect permutations of the weights. In practice, \(_{f}(D,G)\) is the \(L_{2}\) distance between functionally equivalent permutations of the same weight. This approach differs from previous works  which instead result to weight tying to achieve minimal equivariance.

_Remark_ While we use a regularization approach to achieve the required approximate minimal equivariance in weight-space, our usage of the Logit Invariance Regularizer  theoretically guarantees that we indeed learn the correct invariance property similar to the weight-typing approaches. Additionally, our formulation is what allows us to deal with arbitrary architectures using a single model (see Section 4), as opposed to previous works, since strict enforcement of weight-space equivariance by design requires crafting a new model for different architectures. In this sense, our approach provides a general encoder which in principle is applicable to any architecture, resolving the limitations of purely weight-tying approaches.

Theoretical discussions on the adopted regularization based approach to minimal equivariance versus weight tying is provided in Appendix C.

### Choice of Set-to-Set and Set-to-Vector Functions

We specify the choice of Set-to-Set and Set-to-Vector functions encapsulated by \(_{_{1-4}},_{},_{}\) and \(_{}\) used to implement SNE. Let \(X^{n_{X} d}\) and \(Y^{n_{Y} d}\) be arbitrary sets where \(n_{X}=|X|\), \(n_{Y}=|Y|\) and \(d\) (note the abuse of notation from Section 3.1 where \(d\) is a dataset) is the dimension of an element in both \(X\) and \(Y\). The MultiHead Attention Block (MAB) with parameter \(\) is given by:

\[(X,Y;)=(H+(H)), \]

\[H=(X+(X,Y,Y;)). \]

Here, LayerNorm and rFF are Layer Normalization  and row-wise feedforward layers respectively. \((X,Y,Y;)\) is the multihead attention layer of Vaswani et al. .

The Set Attention Block (Lee et al., 2019), SAB, is given by:

\[(X):=(X,X). \]

That is, SAB computes attention between set elements and models pairwise interactions and hence is a Set-to-Set function. Finally, the Pooling MultiHead Attention Block (Lee et al., 2019), \(_{k}\), is given by:

\[_{k}(X)=(S,(X)), \]

\(S^{k d}\) and \(X^{n_{X} d}\). The \(k\) elements of \(S\) are termed _seed vectors_ and when \(k=1\), as is in all our experiments, \(_{k}\) pools a set of size \(n_{X}\) to a single vector making it a Set-to-Vector function.

All parameters encapsulated by \(_{_{1-4}}\) are implemented as a stack of two SAB modules: \(((X))\). Stacking SAB modules enables us not only to model pairwise interactions but also higher order interactions between set elements. Finally, all of \(_{},_{}\) and \(_{}\) are implemented as a single PMA module with \(k=1\).

### Downstream Task

Given \((z_{x_{i}},y_{i})\), we train a predictor \(f_{}(z_{x_{i}})\) to estimate properties of interest of the network \(x_{i}\). In this work, we focus solely on the task of predicting the generalization performance of \(x_{i}\), where \(y_{i}\) is the performance on the test set of the dataset used to train \(x_{i}\) for CNNs and frequencies for INRs. The parameters of the predictor \(f_{}\) and all the parameters in the neural network encoding pipeline, \(\), are jointly optimized. In particular, we minimize the error between \(f_{}(z_{x_{i}})\) and \(y_{i}\). For a model zoo, the objective is given as:

\[*{minimize}_{,}_{i=1}^{d}[f_{}(_{}(x_{i})),y_{i}], \]

for an appropriately chosen loss function \(()\). In our experiments, \(()\) is the binary cross entropy or mean squared error loss. The entire SNE pipeline is shown in Figure 1.

## 4 Experiments

We present experimental results on INRs, and the standard CNN benchmark model zoos used in Unterthiner et al. (2020),Zhou et al. (2023),Zhou et al. (2023), and Navon et al. (2023). Experimental settings, hyperparameters, model specification, ablation of SNE and discussions on applying SNE to architectures with branches (_e.g._ ResNets) in Appendix D.

**Baselines:** We compare SNE with the following baselines: **a) MLP:** This model flattens the entire weight of the network and encodes it using a stack of MLP layers. **b) DeepSets:**(Zaheer et al., 2017) This model treats the weights as a set with no ordering. **c) HyperRep:**(Schurholt et al., 2021) This model learns a generative model of the flattened weight vector. **d) STATNN**(Unterthiner et al., 2020)**:** This model computes the statistics of each layer such as the mean, variance and quantiles, and concatenates them to obtain the neural network encoding vector. **e) DWSNet**(Navon et al., 2023)**:** is a minimally equivariant model using weight tying developed mainly for MLPs. Various modules are provided for encoding biases, weights and combinations of these two. **f) NFNNP, NFNNP** and **NFT**(Zhou et al., 2023, **b)**:** These models, termed Neural Functionals(NF), are developed mainly for MLPs and utilize weight tying to achieve minimal equivariance. HNP, hidden neural permutation, is applied only to the hidden layers of each network since the output and input layers of MLPs are not invariant/equivariant to the action of a permutation group on the neurons. NP, neural permutation, makes a strong assumption that both the input and output layers are also invariant/equivariant under the action of a permutation group. NFT is similar to both models and utilizes attention layers. **g) NeuralGraph:**(Kofinas et al., 2024) This method represents the weights of a network as a graph and uses graph pooling techniques to obtain the network representation vector. We note that we are unable to benchmark against **Graph Metanetworks**(Lim et al., 2023), which uses a graph approach similar to NeuralGraph, since no code or data is publicly available.

In all Tables, the best methods are shown in **red** and the second in blue. Additionally an extensive ablation of all the components of SNE is provided in Appendix E. All experiments are performed with a single GeForce GTX 1080 TI GPU with 11GB of memory.

### Encoding Implicit Neural Representations

**Dataset and Network Architecture:** We utilize the model zoo of Navon et al. (2023) consisting of INRs (Sitzmann et al., 2020) fit to sine waves on \([-,]\) with frequencies sampled from \(U(0.5,10)\). INRs are neural parameterizations of signals such as images using multi-layer perceptrons.

**Task:** The goal is to predict the frequency of a given INR. Each INR is encoded to a 32 dimensional vector which is then fed to a classifier with two linear layers of dimension 512.

**Results:** As can be seen in Table 1, SNE significantly outperforms the baselines on this task. Given that INRs are MLPs, minimal equivariance is particularly important for this task and shows that SNE learns the correct minimal equivariance required to solve the task using the logit invariance approach. Additionally, compared to the minimal equivariance constrained models (Zhou et al., 2023, 2023; Navon et al., 2023), SNE is parameter efficient as show in Table 1. We note that increasing the parameter counts of the MLP, DeepSets and STATNN baselines results in overfitting and poor performance.

### Cross-Architecture Performance Prediction

For this task, we train the encoder on 3 homogeneous model zoos of the same architecture and test on 3 homogeneous model zoos of a different architecture unseen during training. The cross-architecture task demonstrates the encoder's agnosticism to particular architectural choices since training and testing are done on model zoos of different architectures, _i.e._, we perform out-of-distribution evaluation.

**Datasets and Neural Network Architectures:**

We utilize model zoos trained on MNIST, CIFAR10 and SVHN datasets. We generate a model zoo for these dataset with an architecture consisting of 3 convolutional layers followed by two linear layers and term the resulting model zoo Arch\({}_{1}\). Exact architectural specifications are detailed in Appendix G. We generate the model zoos of Arch\({}_{2}\) following the routine described in Appendix A.2 of Unterthiner et al. (2020). We refer to the model zoos of Unterthiner et al. (2020) as Arch\({}_{2}\). All model zoos of Arch\({}_{1}\) are used for training and those of Arch\({}_{2}\) are used for testing and are _not_ seen during training.

**Task:** Here, we seek to explore the following question: Do neural network performance predictors trained on model zoos of Arch\({}_{1}\) transfer or generalize to the out-of-distribution model zoos of Arch\({}_{2}\)? Additionally, we perform cross-dataset evaluation on this task where cross evaluation is with respect to model zoos of Arch\({}_{2}\), _i.e._, we also check for out-of-distribution transfer across datasets.

**Baselines:** We compare SNE with DeepSets (Zaheer et al., 2017) and NeuralGraph (Kofinas et al., 2024) for this task. None of the other baselines, MLP, STATNN (Unterthiner et al., 2020), \(_{}\)(Zhou et al., 2023), \(_{}\)(Zhou et al., 2023), NFT (Zhou et al., 2023) and DWS-Net (Navon et al., 2023) can be used for this task since they impose architectural homogeneity and hence cannot be used for out-of-distribution architectures by design.

**Results:** We report the quantitative evaluation on the cross-architecture task in Table 2 and report Kendall's \(\)(Kendall, 1938). The first column, \(_{1}_{2}\) shows the direction of transfer, where we train using model zoos of Arch\({}_{1}\) and test on model zoos of Arch\({}_{2}\). Additionally, A\(\)B, _e.g._ MNIST\(\)CIFAR10 shows the cross-dataset transfer. From Table 2, it can be seen that SNE transfers best across out-of-distribution architectures and datasets outperforming the DeepSets and NeuralGraph baselines significantly. Interestingly, the DeepSets model, which treats the entire weight

   Model & \#Params & MSE \\  MLP & 14K & 1.917\(\)0.241 \\ Deepsets & 99K & 2.674\(\)0.740 \\ STATNN & 44K & 0.937\(\)0.276 \\ \(_{}\) & 2.0M & 0.911\(\)0.218 \\ \(_{}\) & 2.8M & 0.998\(\)0.382 \\ NFT & 6M & 0.401\(\)0.109 \\ DWSNet & 1.5M & 0.209\(\)0.026 \\ SNE(Ours) & 358K & **0.098\(\)**0.002 \\   

Table 1: Predicting Frequencies of Implicit Neural Representations (INRs).

   Arch\({}_{1}_{2}\) & DeepSets & NeuralGraph & SNE(Ours) \\  MNIST\(\) MNIST & 0.460\(\)0.001 & 0.473 \(\)0.075 & **0.490\(\)0.027** \\ MNIST\(\) CIFAR10 & 0.508\(\)0.001 & 0.528 \(\)0.035 & **0.586\(\)0.020** \\ MNIST\(\) SVHN & 0.546\(\)0.001 & 0.502\(\)0.119 & **0.535\(\)0.004** \\  CIFAR10\(\)CIFAR10 & 0.507\(\)0.000 & 0.463\(\)0.121 & **0.660\(\)0.016** \\ CIFAR10\(\)MNIST & 0.459\(\)0.000 & 0.352\(\)0.104 & **0.586\(\)0.017** \\ CIFAR10\(\)SVHN & 0.545\(\)0.000 & 0.534\(\)0.123 & **0.581\(\)0.044** \\  SVHN\(\)SVHN & 0.553\(\)0.000 & 0.573\(\)0.067 & **0.609\(\)0.039** \\ SVHN\(\)MNIST & 0.480\(\)0.001 & **0.448\(\)0.004 & **0.531\(\)0.029** \\ SVHN\(\)CIFAR10 & 0.529\(\)0.000 & 0.539\(\)0.007 & **0.622\(\)0.057** \\   

Table 2: Cross-Architecture Performance Prediction.

as set with no ordering performs better than the NeuralGraph model on average for this task. In conclusion, SNE shows strong transfer across architecture and datasets in the out-of-distribution benchmark.

_Remark:_ In Table 2, we performed \(_{1}_{2}\) evaluation specifically to allow benchmarking against NeuralGraph (Kofinas et al., 2024). Specifically, the convolutional filters of \(_{1}\) are larger than those of \(_{2}\). NeuralGraph requires specifying this maximum filter size before training and can only be used to transfer across architectures with filter sizes equal or smaller than the predefined filter size (this can be verified from the official source code1), \(i.e.\), NeuralGraph is not truely agnostic to architectural choices. SNE on the other hand does not have this limitation. To demonstrate this, we use the SNE model trained in Table 2 and test it on the SVHN model zoo of Schurholt et al. (2022) which is a much larger architecture with larger filter sizes than those of \(_{1}\). We compare with HyperRep (Schurholt et al., 2021) which is trained fully on the testing model zoo. We emphasize that SNE is _not_ trained on the training set of this model zoo. Results for HyperRep are taken from Schurholt et al. (2022) and a single SNE model is evaluated to match the setting of Schurholt et al. (2022). From Table 3, SNE significantly outperforms HyperRep by very large margins without being trained on the training set of Schurholt et al. (2022) as HyperRep and demonstrates true agnosticism to architectural choices compared to NeuralGraph. We report Kendall's \(\)(Kendall, 1938) in Table 3.

**Evaluation on Transformers:** We generate a model zoo of transformer using PytorchViT (Zhu et al., 2021) and test the transfer from \(_{1}\) to the transformer model zoo. For this task, we are unable to benchmark against NeuralGraph as was done in Table 2, since the model cannot process transformer weights when trained on \(_{1}\). Hence we benchmark against the DeepSets baseline as in Table 2. From Table 4 SNE generalizes better to the unseen transformer architecture at test time than the baselines showing strong architectural transfer. Additionally, here, the model encodes an architecture with about 5 times the number of parameters in \(_{1}\) demonstrating the scalability of our approach. The DeepSets baseline fails to generalize on this task.

### Cross-Dataset Performance Prediction

For this task, we train neural network performance predictors on 4 homogeneous model zoos, of the same architecture, with each model zoo restricted to a single dataset.

**Datasets and Network Architecture:** Each model zoo is trained on one of the following datasets: MNIST (Deng, 2012), FashionMNIST (Xiao et al., 2017), CIFAR10 (Krizhevsky, 2009) and SVHN (Netzer et al., 2018). We use the model zoos of Unterthiner et al. (2020).

A thorough description of the model zoo generation process can be found in Appendix A.2 of Unterthiner et al. (2020).

**Task:** We train network encoders on a single model zoo, \(e.g.\) MNIST, and evaluate it on the test set of all four datasets and report the averaged performance. This results in in-distribution testing with respect to the dataset used for training and out-of-distribution testing with respect to the other three datasets, \(e.g.\) a model trained on MNIST is tested on MNIST, FashionMNIST, CIFAR10 and SVHN.

**Baselines:** We benchmark against MLP, STATNN (Unterthiner et al., 2020), \(_{}\)(Zhou et al., 2023a), and the \(_{}\)(Zhou et al., 2023a).

**Results:** We present the results for this task in Table 5. Here we see that SNE again performs better than the competing methods significantly demonstrating strong transfer across different datasets for the same architecture.

   Arch\({}_{1}\) & DeepSets & SNE(Ours) \\  MNIST \(\) MNIST & \(0.1975 0.000\) & **0.4625\(\)**0.003 \\ CIFAR10 \(\) MNIST & \(0.1970 0.000\) & **0.3278\(\)**0.029 \\ SVHN \(\) MNIST & \(0.1906 0.000\) & **0.3735\(\)**0.000 \\   

Table 4: Cross-Architecture Performance on Transformers. We report Kendall’s \(\).

   Dataset & HyperRep & SNE(Ours) \\  SVHN \(\) SVHN & 0.45 & **0.67** \\ SVHN \(\) MNIST & 0.15 & **0.61** \\ SVHN \(\) CIFAR10 & 0.10 & **0.68** \\   

Table 3: Cross-Architecture Performance Prediction on Schürhötl et al. (2022)’s model zoo.

**Qualitative Analysis:** To understand how SNE transfers well across model zoos, we generate TSNE (Van der Maaten and Hinton, 2008) plots for the neural network encodings of all benchmarked methods on all four homogeneous model zoos in Figure 2. We provide 3 different views of each models embeddings to better illustrate the encoding pattern. In Figures 1(c) and 1(d), we observe that \(_{}\) and \(_{}\) have very clear separation boundaries between the networks from each model zoo. In Figures 1(a) and 1(b), MLP and STATNN, respectively show similar patterns with small continuous strings of model zoo specific groupings. However, these separations are not as defined as those of \(_{}\) and \(_{}\). The embedding pattern of SNE on the other hand is completely different. In Figure 1(e), all networks from all the model zoos are embedded almost uniformly close to each other. This may suggest why SNE performs much better on the cross-dataset performance prediction task since it is much easier to interpolate between the neural network encodings generated by SNE across model zoos.

## 5 Conclusion

In this work, we tackled the problem of encoding neural networks for property prediction given access only to trained parameter values. We presented a Set-based Neural Network Encoder (SNE) that reformulates the neural network encoding problem as a set encoding problem. Using a sequence of set-to-set and set-to-vector functions, SNE utilizes a pad-chunk-encode pipeline to encode each network layer independently; a sequence of operations that is parallelizable across chunked layer parameter values. SNE also utilizes the computational structure of neural networks by injecting positionally encoder layer type/level information in the encoding pipeline. As a result, SNE is capable of encoding neural networks of different architectures as opposed to previous methods that only work on a fixed architecture. To learn the correct minimal equivariance for MLP weight permutations, we utilized Logit Invariance Regularization as opposed to weight tying used in previous methods. Experimentally, we introduced the cross-dataset and cross-architecture neural network property prediction tasks. We demonstrated SNE's ability to transfer well across model zoos of the same architecture but with networks trained on different datasets on the cross-dataset task. On the cross-architecture task, we demonstrated SNE's agnosticism to architectural choices and provided the first set of experimental results for this task that demonstrates transferability across architectures.

Figure 2: TSNE Visualization of Neural Network Encodings. We train neural network performance prediction methods on a combination of the MNIST, FashionMNIST, CIFAR10 and SVHN modelzoos of Unterthiner et al. (2020). We present 3 views of the resulting 3-D plots showing how neural networks from each modelzoo are embedded/encoded by the corresponding models. Larger versions of these figures are provided in Appendix K. Zoom in for better viewing.