# Optimal privacy guarantees for a relaxed threat model: Addressing sub-optimal adversaries in differentially private machine learning

Optimal privacy guarantees for a relaxed threat model: Addressing sub-optimal adversaries in differentially private machine learning

Georgios Kaissis

AI in Medicine and Healthcare

Technical University of Munich

Munich, Germany

g.kaissis@tum.de &Alexander Ziller

AI in Medicine and Healthcare

Technical University of Munich

Munich, Germany

alex.ziller@tum.de &Stefan Kolek

Mathematical Foundations of AI

LMU Munich

Munich, Germany

kolek@math.lmu.de &Anneliese Riess

Machine Learning in Biomedical Imaging

Helmholtz Munich

Neuherberg, Germany

anneliese.riess@helmholtz-munich.de &Daniel Rueckert

AI in Medicine and Healthcare

Technical University of Munich

Munich, Germany

daniel.rueckert@tum.de

GK is also with the Institute of Machine Learning in Biomedical Imaging, Helmholtz Munich and the Department of Computing, Imperial College London.

###### Abstract

Differentially private mechanisms restrict the membership inference capabilities of powerful (optimal) adversaries against machine learning models. Such adversaries are rarely encountered in practice. In this work, we examine a more realistic threat model relaxation, where (sub-optimal) adversaries lack access to the exact model training database, but may possess related or partial data. We then formally characterise and experimentally validate adversarial membership inference capabilities in this setting in terms of hypothesis testing errors. Our work helps users to interpret the privacy properties of sensitive data processing systems under realistic threat model relaxations and choose appropriate noise levels for their use-case.

## 1 Introduction

Machine learning (ML) offers a powerful set of techniques for addressing complex problems across various fields, including fields where sensitive data is processed, e.g. medicine or finance. However, the use of sensitive data in ML comes with privacy challenges: It is known that ML models memorise their training data , and that some degree of memorisation may be necessary for the best model performance . When a model memorises private information, it can reveal this information when attacked by adversaries. A very important class of attack is _Membership Inference_ (MI) , which aims to determine whether the data of a specific individual was used to train a machine learning model. MI can lead to the disclosure of highly sensitive information about the individual, e.g. that they are part of a database of cancer patients which was used to train a predictive medical ML system. The importance of studying MI is furthermore derived from the fact that a successful MI attack signifies that the model is also vulnerable to all other attacks such as data reconstruction or attribute inference ("most significant bit" property ).

Differential Privacy (DP)  is a formal framework and collection of techniques to furnish objective privacy guarantees for ML model training. It is a natural counterpart to MI, and it is possible to interpret DP entirely through the MI lens, which is known as the hypothesis testing interpretation of DP [6; 7]. Given a database \(D\) and a database \(D^{}\) which differ in exactly one record, hypothesis testing DP quantifies the trade-offs between the Type-I and Type-II errors of an adversary using a hypothesis test to determine whether a model was trained on \(D\) or on \(D^{}\). This hypothesis test is considered to be optimal in the sense of the Neyman-Pearson (NP) lemma , which makes the _DP Threat Model_ (DPTM) very strong: It assumes that the adversary has full access to the training database, knowledge of the specifications of the DP mechanism and can even deeply manipulate the process of ML model training itself to their advantage . Although this has the benefit that DP is a true "worst-case" guarantee and thus holds for all weaker adversaries, it may be unnecessarily conservative in practice. For instance, the assumption that the adversary has full access to the training data may be unrealistic in settings where this data is tightly access-restricted. As a consequence, previous works also considered threat models with weaker adversaries, i.e. _Relaxed Threat Models_ (RTMs), and experimentally showed that the empirical protection against MI offered by DP mechanisms is much stronger than the upper bound predicted by the DP analysis [9; 10; 11]. While such empirical investigations provide valuable initial insights, we contend that an additional _formal_ analysis of specific, practically relevant RTMs can greatly benefit stakeholders in conducting a thorough privacy analysis of ML systems. For example, a "privacy certificate" for an ML model can include both (1) the MI risk under the DPTM (which holds when "all bets are off") but also (2) the more realistic MI risk under whichever RTM is applicable to the actual model training conditions.

In this work, we study MI in the context of a specific RTM with high practical relevance, in which the adversary has _incomplete access to the model's training data_. An example of this RTM is the training of a predictive ML model on medical data which is generated and stored at a single hospital and never released, whereas the model is shared with third parties. Here, an MI adversary must resort to leveraging auxiliary data from the same distribution as the hospital's target database. A similar attack (_offline MI_) was studied in [12; 13; 14], and also does not use the full database for the attack (albeit for reasons of computational efficiency). As MI is equivalent to a _hypothesis testing_ problem, the aforementioned works demonstrate that the Type-I and Type-II errors (i.e. the false positive/false negative rates) of offline MI attacks are higher than MI attacks assuming the DPTM. In this work, we formally derive and validate exact bounds on the error rates of MI attacks against DP noise mechanisms in the aforementioned RTM (which we will refer to as "the RTM" from now on).

ContributionsOur contributions are as follows: (1) Since MI can be fully characterised using hypothesis testing, we begin by formalising the RTM as a hypothesis testing security game and show that the RTM adversary is strictly sub-optimal in terms of error rates compared to the DPTM adversary; (2) Next, we formally derive the "best sub-optimal" hypothesis testing MI strategy in the RTM; (3) We then analyse the error rates associated with this strategy against common additive noise mechanisms used in DP under the RTM. (4) We express these error rates using trade-off functions, which parameterise all achievable Type-I/Type-II error combinations, similar to the approach taken by  in defining f-DP. (5) Additionally, we introduce a new metric, \(\)-Detection Resilience (\(\)-DR), which allows us to compare the hypothesis testing capabilities of the RTM and DPTM adversaries, despite their differing threat models. (6) We conclude our study by empirically auditing the proposed bounds and demonstrating accuracy advantages in the training of deep neural networks with DP-SGD.

## 2 Background

Related workOur work contributes to the ongoing research in three distinct, yet related areas of privacy-preserving ML. Firstly, we aim to augment empirical investigations which use MI to establish lower bounds on the privacy guarantees of ML systems, with formal guarantees. For instance,  experimentally evaluate several types of adversaries against DP ML workflows, [10; 11] present empirical privacy analyses of DP-SGD , while  focus on federated learning. Such empirical privacy auditing techniques have found their way into software tools such as _ML-Doctor_ or _ML_Privacy Meter_. As mentioned above, our RTM considers an adversary performing MI without access to the (complete) model's training data. This type of MI attack has been studied in several prior works, most notably by Watson et al. , Ye et al.  and Carlini et al. , who find the adversary's hypothesis testing capabilities to be substantially diminished compared to the DPTM. The second line of work related to our paper are formal analyses of MI in the DPTM, such as , who consider a Bayesian adversary or , who bypass the DP analysis and directly study the properties of the underlying distributions. Moreover,  and  are rooted in hypothesis testing theory like our work, but only consider threat models equivalent to the DPTM, while  consider restricted adversaries, albeit not through the hypothesis testing lens. Lastly, we adopt the recent development of using functions (rather than numbers) to parameterise privacy guarantees, which is becoming common practice in the DPTM , but has not yet been used to study threat model relaxations. Our paper unifies these strands of work by introducing a formal guarantee in the RTM, which allows for a more nuanced analysis of the privacy risks associated with machine learning models.

Differential privacyDP  is a stability condition on randomised mechanisms requiring that their outcomes should be approximately equally likely when the data of a single individual is added or removed from the input database. Throughout this work, we will consider the scenario of ML model training, where DP is realised through an _additive noise mechanism_. We consider a deterministic model training function \(q\). The global sensitivity \(>0\) of \(q\) is defined as \(=_{D,D^{}:D D^{}} q(D)-q(D^{})\) where the \(\) is taken over all pairs of neighbouring databases \(D D^{}\) which differ in a single record. 2\(\) is measured in some norm, which we will indicate by subscript wherever relevant (e.g. \(_{1}\)). An additive noise mechanism (or just mechanism) \(\) maps databases and probability distributions to a (model) parameter space \(\). Its outputs are of the form \(=q(X)+z,z\), where \(\) is a noise distribution. Since \(q\) and the database are considered deterministic, \(\) is a random variable induced by the randomness of \(\). Unless otherwise indicated, \(\) will be realised through distributions \((,)\) supported on the real line which admit a density. We assume that \(\) is fixed and public information and that \(\) is shift invariant. In other words, \((D)=(q(D)+(0,))(q(D),)\) and \((D^{})=(q(D^{})+(0,))(q(D^ {}),)\).

In the hypothesis testing interpretation of DP , the adversary attempts to determine whether \(\) was trained on \(D\) or \(D^{}\) using a classical/frequentist hypothesis test. The adversary considers the hypotheses \(_{0}:(D)\) vs. \(_{1}:(D^{})\) and their reverse, i.e. \(_{0}:(D^{})\) vs. \(_{1}:(D)\). \(_{0}\) and \(_{1}\) are called the _null_ and _alternative_ hypothesis, respectively. The aim of the hypothesis testing problem is to distinguish between \((D)\) and \((D^{})\) using a pair of decision rules (tests). Let \(,^{}\) be tests with Type-I error rate \(_{}=_{(D)}()\) and Type-II error rate \(_{}=1-_{(D^{})}()\) (identical for \(^{}\)). The adversary's goal will be to maximise their _power_\(1-\) at a fixed level \(\), since the fundamental trade-off in hypothesis testing is that \(\) and \(\) cannot be minimised simultaneously . All achievable \(/\) pairs for a given adversary/test combination are expressed by a pair of functions on the unit square, called the _trade-off functions_\(T\) and \(T^{-1}\).

\(T\) is defined through the test \(\): \((D)\) vs. \((D^{})\) as \(T((D),(D^{}))()=_{}\{_{ }:_{}\}\), whereas \(T^{-1}=T((D^{}),(D))\) is defined using \(^{}\). The \(\) is taken over the set of tests which can be constructed in a given threat model, in this case the DPTM. For mechanisms with symmetric noise density, \(T=T^{-1}\), allowing us to consider only a single trade-off function. In general, one takes the symmetrisation/convexification of \(T\) and \(T^{-1}\), denoted \((T,T^{-1})\) which creates a single, symmetric trade-off function. Since trade-off functions fully characterise the adversary's error rates, they can be used to compare the privacy guarantees of different mechanisms and/or different threat models. In particular, the closer the trade-off function's graph is to the line \(()=1-\), i.e. the off-diagonal of the unit square, the stronger the implied privacy guarantee of the mechanism is. The notion of expressing DP guarantees based on a comparison of \((T,T^{-1})\) with a "reference" trade-off function \(f\) is called f-DP :

**Definition 1**.: _A mechanism \(\) satisfies \(f\)-DP, if \( D,D^{}:D D^{}\) it holds that \((T,T^{-1})() f()\) for a reference trade-off function \(f\) (see Figure 2 of  for an example)._

_A pair of distributions \((_{1},),(_{2},)\) is called a dominating pair  for \(\) if, \( D,D^{}:D D^{}\), \((T,T^{-1})()(T((_{1},),(_{2},)),T((_{2},),(_{1},)))() \)._For example, the dominating pair for the Gaussian Mechanism (GM) is \(((0,^{2}),(_{2},^{2}))\) and for the Laplace Mechanism (LM), it is \(((0,b),(_{1},b))\). The dominating pair distributions thus exhibit the greatest _effect size_ (in this case \(}}{{b}}\) or \(}}{{}}\)), allowing for the construction of the trade-off functions whose graphs are farthest from the off-diagonal of the unit square. In other words: the construction of the trade-off functions is determined by the hypothesis tests with the greatest power \(1-\) under a specific threat model, evaluated at the dominating pair distributions.

In the DPTM, \(_{0}\) and \(_{1}\) are both _simple hypotheses_ of the form \(_{0}:_{0}\) vs. \(_{1}:_{1}\) with \(\|_{0}\|=\|_{1}\|=1\) and \(_{0}_{1}=\). The reason is that the distributions of \((D)\) and \((D^{})\) are fully specified because the adversary can evaluate \(q(D)\) and \(q(D^{})\). The NP lemma  states that, for simple vs. simple hypothesis tests such as the ones in the DPTM, the _uniformly most powerful_ (UMP) level \(\) tests \(\) and \(^{}\) (with the highest power \(1-\) at a specified level \(\) over the entire range of \(\), in this case \(q(D)\) or \(q(D^{})\)) can be constructed by thresholding the (log-) Likelihood Ratio (LRs) \(/^{}\) of the mechanism's outputs \(\) under \(D\) and \(D^{}\):

\[()=((D^{}))}{ ((D))} c\;\;\;\;^{ }()=((D))}{( (D^{}))} c,\]

where \(c\) is the critical value. The trade-off functions in the DPTM are then constructed from the distributions of \(/^{}\) evaluated at the dominating pairs under the respective \(_{0}\) and \(_{1}\) (also called _privacy loss distributions_). Since the DPTM adversary enjoys the optimality properties of the NP lemma, we will refer to this adversary as \(^{}\).

## 3 Hypothesis testing in the RTM

The trade-off function's form for a given threat model depends only on the test statistic's distributions under \(_{0}\) and \(_{1}\). Our strategy for the rest of the paper is thus: (1) Derive the optimal tests the adversary can construct in the RTM; (2) Specify the test statistic distributions for these tests, and (3) Construct the trade-off functions for a given mechanism using these test statistics, which will allow us to tightly bound the adversary's error rates and compare them to the DPTM. We first recall a general result, specifying how trade-off functions are constructed from test statistics.

**Definition 2** (Trade-off function construction).: _Let \(P,Q\) be the distributions of a test statistic under \(_{0}\) and \(_{1}\), respectively and let \(_{P/Q}\) denote their cumulative distribution function (CDF), \(_{P/Q}\) their survival function (SF) and \(_{P/Q}^{-1},_{P/Q}^{-1}\) their respective inverses (iCDF, iSF). Then, we define:_

\[T()=_{Q}(_{P}^{-1}())\;\;\;\;T^{-1}()= _{P}(_{Q}^{-1}()). \]

For example, in experimental studies, empirical proxies of the test statistic distributions are evaluated by training _shadow models_. Then, the empirical trade-off functions are constructed, which parameterise the error rates of the empirical adversary under a specific threat model . We will utilise this strategy later to validate our theoretical bounds empirically. For our formal analysis on the other hand, we will derive exact expressions for \(T\) and \(T^{-1}\).

RTM security gameFor reasons which will soon become clear, we will refer to the adversary under our RTM as a _sub-optimal adversary_ (\(^{}\)) and use a formal security game, similar to the one used to define the DPTM (see e.g. Section 3.1. of ), which proceeds between a neutral/trusted curator \(\) and \(^{}\). We consider a single-round, non-interactive protocol.

1. The adversary \(^{}\) constructs a database \(D=\{x_{1}, x_{n}\}\), decides on a training function \(q\), a DP mechanism \(\) and sends them to the curator \(\);
2. \(\) flips a bit \(b\). If \(b=0\), they fix \(X=D\). If \(b=1\), they choose a singleton \(\{x^{*}\}^{}\), where \(^{}\) and \(^{}=\{x^{*}\}\) and fix \(X=D\{x^{*}\}\). They then train a model \(\) on \(X\) using \(q\) and \(\) (\(\) may also be a gradient) and send \(\) to \(^{}\);
3. \(^{}\) decides if \(_{0}:(D)\) or \(_{1}:(D^{})\);
4. If \(^{}\) is correct, the MI attack is successful, privacy is breached and the game is won.

The key difference between the RTM and the DPTM is step 2: \(^{}\) has _no access to the point_\(x^{*}\) which may be added by \(\), and by extension, no knowledge of the exact value of \(q(D^{})\). In step 3, \(^{}\) can therefore only decide \((D^{})\) by rejecting that \((D)\), but not by directly confirming that \((D^{})\).

The hypothesis testing capabilities of the RTM adversary are identical to those of an _offline MI adversary_. Offline MI attacks are used in previous works to audit the privacy of ML model training  and decide whether \(x^{*}\) is part of the training data or not without actively training shadow models on databases containing it. Equivalently, offline MI is _MI by exclusion_ (i.e. inferring \((D^{})\) by rejecting \(_{0}:(D)\)). In offline MI, this is done for reasons of practicality and computational efficiency , while in the RTM, it by design. Nonetheless, the resulting error rates are identical and our bounds thus can be used to characterise offline MI.

RTM: Formal analysis\(\) is an additive noise mechanism based on the distribution \(\), thus \(_{0}:(D)\) is equivalent to \(_{0}:(q(D),)\). Since \(q(D)\) is computable by \(^{}\) through access to \(D\), the likelihood \(((q(D),))\) is also computable and \(_{0}\) is a simple hypothesis of the form \(_{0},\|_{0}\|=1\), exactly like the DPTM. However, the likelihood \(((q(D^{}),))\) is _not computable_ without access to \(D^{}\) and \(_{0}\) is a _composite hypothesis_ of the form \(_{0}\) with \(\|_{0}\|>1\) because it depends on the unknown value of \(q(D^{})\). The following is a direct consequence of the NP lemma, which holds only for simple vs. simple hypotheses .

**Lemma 1**.: _In the RTM, no UMP level \(\) test exists for all possible values of \(q(D^{})\), i.e. \(_{0}\). Thus, \(_{0}\) vs. \(_{1}\) (and \(_{1}\) vs. \(_{0}\)) are each not decidable by a single test._

This implies that \(^{}\) is necessarily _weaker_ than \(^{}\), i.e. cannot achieve the same power \(1-\) at a given level \(\) for all values of the parameter of interest; to thus establish the _optimal tests in this sub-optimal setting_, \(^{}\) must consider specific value ranges of \(q(D^{})\) separately. In the following, we will call two tests _equivalent_ if they have the same power at a level \(\) for all values of the tested parameter. Moreover, we will limit our description to \(_{0}:(D)\) vs. \(_{1}:(D^{})\), as \(_{1}\) vs. \(_{0}\) is handled identically.

**Lemma 2**.: _The composite LR test \(_{0}:(q(D),)\) vs. \(_{1}:(q(D^{}),)\) is equivalent to letting \((,)\) and simultaneously conducting two individual one-sided LR tests \(r\) and \(r^{}\) with null hypothesis \(=0\) and alternative hypotheses \(-<0\) for \(r\) and \(0<\) for \(r^{}\), where \(=q(D^{})-q(D)\) and \(\) is the global sensitivity of \(q\)._

Thus, \(^{}\) can leverage known facts about \(q\) and \(\) to limit the value ranges for the alternative hypothesis, but, as \(q(D^{})\) is unknown, must "split the problem" into two individual tests. The individual tests are then UMP level \(\) under the following pre-condition.

**Lemma 3**.: _The individual LR tests \(r\) and \(r^{}\) are UMP level \(\) if the noise distribution \(\) has the monotone likelihood ratio property (MLRP). The Laplace Mechanism (LM), Gaussian Mechanism (GM) and the Poisson-Subsampled Gaussian Mechanism (SGM) all have the MLPR. Moreover, the power of both tests is maximised when \(\|\|=\|q(D^{})-q(D)\|=\)._

This lemma intuitively states that, while there is an optimality condition associated with the tests that can be constructed in the RTM, it is not the same optimality condition as the NP lemma guarantees in the DPTM. In particular, although \(^{}\) can construct a pair of tests which are optimal (UMP level \(\)), they are _only optimal for specific parameter value ranges_ rather than for all possible parameter values. The lemma also states that it suffices to consider the tests \(=0\) vs. \(=\) when constructing the trade-off functions, as these correspond to the _highest effect size_ at a fixed \(\). In other words, _dominating pairs remain dominating pairs_ in the RTM. Next, we provide two convenience lemmata, which allow \(^{}\) to conduct both tests simultaneously.

**Lemma 4**.: _If the density of \(\) is additionally symmetric about the location parameter, the simultaneous individual LR tests \(r\) and \(r^{}\) are equivalent to performing a single test using the magnitude of the observation as a test statistic, i.e. letting \(\|\|\|(,)\|\) and testing \(_{0}:\|\|=0\) vs. \(_{1}:\|\|>0\). The power of this "combined" test is maximised at \(\|\|=\)._

**Lemma 5**.: _For a (not necessarily symmetric) \(\) the Generalised Likelihood Ratio Test (GLRT) using the test statistic \(((,))}{( (0,))}\), where \(\) is the maximum likelihood estimate of \(\) is equivalent to simultaneously conducting \(r\) and \(r^{}\). The GLRT's power is maximised at \(=\)._

We stress that, while the two tests are UMP level \(\) and can be "combined" into one test, the combined test is _not UMP level \(\)_ over the whole range (by Lemma 1). To construct both trade-off functions, the steps above are either repeated for \(_{1}\) vs. \(_{0}\), or the trade-off function for \(_{0}\) vs. \(_{1}\) is inverted directly, i.e. in closed form or numerically.

\(\)-Detection resilienceNext, we introduce a measure for \(^{}\)'s optimal achievable error rates under the RTM, inspired by f-DP: For each mechanism, we create a pair of trade-off functions \(j\) and \(j^{-1}\). These parameterise the error rates of the UMP level \(\) tests against the dominating pair distributions of a given mechanism \(\) in the RTM. Next, we "unify" the functions by symmetrisation/convexification to obtain a single, symmetric trade-off function \(J:\), which expresses _all_ optimal error rates of \(^{}\) against \(\). Finally, we compare \(J\) to a reference trade-off function \(\). If \(J\) lies on or above \(\), \(^{}\) cannot achieve lower \(\) for any \(\) in the task of detecting the presence of \(x^{*}\) in \(D^{}\) by observing \(\), and we say that \(\) satisfies \(\)-Detection Resilience (\(\)-DR).

**Definition 3**.: _Let \(r\) be a test for \((D)\) vs. \((D^{})\) and \(r^{}\) be a test for \((D^{})\) vs. \((D)\), where \(\) is the set of LR tests which are computable in the RTM. To construct \(J\), fix a level \(\), then obtain \(j=_{r}\{_{r}:_{r}\}\) and \(j^{-1}=_{r^{}}\{_{r^{}}:_{r^{}} \}\). Then compute the symmetrisation/convexification \(J=(j,j^{-1})\). We say that a mechanism \(\) satisfies \(\)-DR, if for some reference function \(\) and \( D,D^{}:D D^{}\), it holds that \(J()()\;\)._

We remark that computing the symmetrisation/convexification \((j,j^{-1})\) is only a convenience measure to circumvent the necessity to work with both \(j\) and \(j^{-1}\) separately. The concrete algorithm for computing \(\) for any trade-off function pair can be found in Definition E.1 of the Appendix to .

Since \(^{}\) is weaker than \(^{}\), we expect the trade-off functions characterising their error rates to lie on or above the trade-off functions for the same mechanism under the DFTM (see Figure 1). However, it is important to remark that \(\)-DR is _not a DP guarantee_, although the opposite holds true.

**Lemma 6**.: _If a mechanism satisfies \(f\)-DP, it also satisfies \(f^{}\)-DR, with \(f^{}() f()\;\)._

Moreover, since \(\)-DR is defined conditional on a restriction of \(^{}\)'s background knowledge, it is not resilient to post-processing through auxiliary information. However, a weaker condition holds:

**Lemma 7**.: _If a mechanism satisfies \(f\)-DP and \(f^{}\)-DR with \(f^{} f\), arbitrary post-processing can only deteriorate privacy up to the \(f\)-DP guarantee._

It follows that \(\)-DR is _not closed under adaptive composition_. In fact, no privacy relaxation assuming restricted background knowledge is closed under adaptive composition . However, mechanism-specific guarantees can be given for _non-adaptive composition_, where the data/mechanism parameters are fixed in advance and all intermediate models/gradients are released to \(^{}\). This covers the standard DP-SGD setting and is studied below.

## 4 Mechanism-specific analysis

Since precisely quantifying DR for a specific noise mechanism requires knowledge of the RTM trade-off functions, we now provide methods to compute \(j\) and \(j^{-1}\) for various DP mechanisms. The construction of \(J=(j,j^{-1})\) is independent of the specific form of \(j,j^{-1}\). By definition for all \(j,j^{-1}\): \(j(0)=j^{-1}(0)=1,j(1)=j^{-1}(1)=0\) and \(\).

Laplace MechanismThe LM with scale \(b\) on a function \(q\) with global \(_{1}\)-sensitivity \(_{1}\) outputs \(q(X)+(0,b)\). The LM has a symmetric noise density, thus, the magnitude distributions can be used as test statistics according to Lemma 4. Under \(_{0}\), the test statistic follows an exponential distribution, while under \(_{1}\), it follows the (uncommon) _folded Laplace_ distribution . Both distributions admit closed-form expressions for their CDF, SF, iCDF and iSF, thus the trade-off functions can also be characterised in closed form.

**Theorem 1**.: _Let \(_{1}=}}{{b}}\). The LM satisfies \((j_{}(),j_{}^{-1}( ))\)-DR with:_

\[j_{}()=-)})},&)}\\ 1-)},&, \]

_and_

\[j_{}^{-1}()=)}+)}+1}},&<}{{2}}-}{{2}})}\\ -)}},&. \]Gaussian MechanismThe GM with covariance matrix \(^{2}^{d}\) on a \(d\)-dimensional function \(q\) with global \(_{2}\) sensitivity \(_{2}\) outputs \(q()+(0,^{2}^{d})\). Like the LM, it also has a symmetric noise density, thus the (squared) magnitude distributions can be used as a test statistic. Let \(_{d}^{2}(0,^{2})\) be the chi-squared distribution with \(d\) degrees of freedom, i.e. the distribution of \(\|(0,^{2}^{d})\|_{2}^{2}\). Let \(_{d}^{2}(_{2}^{2},^{2})\) be the noncentral chi-squared distribution with \(d\) degrees of freedom and noncentrality parameter \(_{2}^{2}=^{2}}}{{^{2}}}\), i.e. the distribution of \(\|(,^{2}^{d})\|_{2}^{2}\) with \(\|\|_{2}=\). As in Definition 2, \(,,^{-1}\) and \(^{-1}\) denote the CDF, SF, iCDF and iSF of the subscripted distribution, respectively.

**Theorem 2**.: _Let \(_{2}=}}{{}}\). The GM satisfies \((j_{}(),j_{}^{-1}())\)-DR with:_

\[j_{}()=_{_{d}^{2}(_{2}^{2},^{2})}(_{ _{d}^{2}(0,^{2})}^{-1}())\ \ \ \ j_{}^{-1}( )=_{_{d}^{2}(0,^{2})}(_{_{d}^{2}(_{2}^{2}, ^{2})}^{-1}()). \]

These functions have no general analytic representation, but are easy to evaluate numerically. However, an analytic representation is available for \(j()\) at \(d=1\). We discuss the importance of \(d=1\) below.

**Corollary 1**.: _For \(d=1\), \(j()\) admits the following closed-form representation:_

\[j_{}( d=1)=_{(0,1)}(_{ (0,1)}^{-1}(}{{2}})-_{2})-_{ (0,1)}(_{(0,1)}^{-1}(}{ {2}})+_{2}). \]

Next, we study non-adaptive composition, where a database and mechanism parameters are fixed ahead of time, and all intermediate results are released to \(^{}\). This setting is often encountered, e.g. in standard DP-SGD.

**Lemma 8**.: _Let \(_{a},_{b}\) be GMs with noise variances \(_{a}^{2}^{d},_{b}^{2}^{d}\) on functions with sensitivities \(_{2a},_{2b}\), respectively. Then, the non-adaptively composed mechanism \(\) has trade-off functions:_

\[j_{}()=_{_{d}^{2}(_{},_{c}^{2})} (_{_{d}^{2}(0,_{c}^{2})}^{-1}())\ \ \ \ j_{}^{-1}()=_{_{d}^{2}(0,_{c}^{2})}(_{_{d}^{2}(_{},_{c}^{2})}^{-1}()). \]

_with \(_{}=+_{2b})^{2}}}{{_{c}^{2}+ _{b}^{2}}}\) and \(_{c}^{2}=^{2}+_{b}^{2}}}{{4}}\)._

The GM moreover exhibits specific asymptotic behaviour when the query dimensionality \(d\) and/or the number of non-adaptive compositions increase.

**Theorem 3** (Blessing of dimensionality in the RTM).: _Consider a GM on a function with sensitivity \(_{2}\) and noise variance \(^{2}^{d}\) such that \(}}{{}} 1\). Let \(_{2}=}}{{}}\). As \(d\) and/or as the number of non-adaptive compositions \(N\) increase, \(j_{}\) and \(j_{}^{-1}\) tend to the common form:_

\[_{(0,1)}((0,1)}^{-1}()}{ ^{2}}{d}}+1}-^{2}}}{2d^{2}}{d}}+1})_{(0,1)}(_{(0,1)}^{-1}()-N}{2d}}). \]

As \(d\) increases for fixed \(N\), \(j_{c}\) and \(j_{c}^{-1}\) become symmetric and eventually both tend to \(_{(0,1)}(_{(0,1)}^{-1}())=1-\), i.e. the Type-I and Type-II errors are equal and \(^{}\)'s test is no better than random guessing. Intuitively, this can be understood by considering that the magnitude of \(\) (i.e. the test statistic) is influenced less by its individual components as dimensionality increases . Therefore, at \(d>1\), the GM provides progressively stronger \(\)-DR guarantees than for scalar queries. However, as the adversary can leverage their knowledge of the model to design a database which influences only a single coordinate of the weight/gradient vector, we mostly use \(d=1\) as a worst-case scenario below. In practice, such a worst-case database may be unrealistic, and the aforementioned result can thus be applied to obtain tighter bounds on MI success in specific scenarios. We note that, in the DPTM, the trade-off function of the GM is always independent of \(d\) and has the form \(f_{}^{ N}=_{(0,1)}(_{(0,1)}^{-1}()-)\), 3\(=_{2}}}{{}}\), \( N\) denotes \(N\)-fold composition. The similarity between \(f_{}\) and Equation (7) is a consequence of the central limit theorem-like phenomenon . Despite the similarity between the functional forms, we stress that this does not mean that the asymptotic \(\)-DR guarantee implies a GDP guarantee, although the converse holds.

Poisson-Subsampled Gaussian Mechanism (SGM)The SGM is relevant in private deep learning . On a \(d\)-dimensional function \(q\) with global \(_{2}\)-sensitivity \(_{2}\), the SGM with diagonal covariance matrix \(^{2}^{d}\) samples a mask \((p)^{d}\), where \(\{0,1\}^{d}\) and \((p)\) denotes a Bernoulli distribution with probability \(p\) and outputs \(q()+,(0,^{2}^{d})\), where \(\) is the Hadamard product. The dominating pairs are \(((0,^{2}),(1-p)(0,^{2})+p(_{ 2},^{2}))\) and \(((1-p)(_{2},^{2})+p(0,^{2}),(_{2},^{2}))\). Although we have shown in Lemma 3 that the SGM satisfies the MLRP, the test statistic distributions under \(_{0}\) and \(_{1}\) do not have a tractable form. We thus proceed numerically: First, we instantiate \(j_{ GM}\) and \(j_{ GM}^{-1}\) for the standard GM as shown above, then use the following general result to amplify the trade-off functions directly.

**Lemma 9**.: _Let \(T(A,B)()\) be a trade-off function between two general distributions \(A,B\) representing mechanism outputs. The trade-off functions for the sub-sampled mechanisms are given by \(T(A,(1-p)A+pB)()=pT(A,B)()+(1-p)(1-)\) and by its inverse._

We thus obtain the amplified functions \(j_{p}\) and \(j_{p}^{-1}\) from which we compute \(J_{ SGM}=(j_{p},j_{p}^{-1})\). It follows that the SGM satisfies \(J_{ SGM}\)-DR. This subsampling technique can also be applied to other mechanisms . An example for the LM is shown in the Appendix. Composition of mechanisms other than the GM is also handled numerically through direct composition of the test statistics as described in , Definition 3.1. ff.

## 5 Experimental evaluation

RTM/DPTM trade-off function comparisonsFigure 1 shows trade-off functions for the LM, GM and SGM in low privacy (low noise) and high privacy (high noise) settings to compare the error rates in the RTM to the DPTM. The pair of asymmetric DPTM trade-off functions \(j,j^{-1}\), is combined into a single symmetric function \(J=(j,j^{-1})\) (symmetrisation/convexification). We also plot the (symmetrified) trade-off function \(f\) for the same mechanism under the DPTM for reference. All RTM curves (\(j,j^{-1},J\)) are closer to the off-diagonal than \(f\), indicating that \(^{}\) has lower MI attack success than \(^{}\). The "blessing of dimensionality" is shown in subfigure **(f)**: at \(d=30\), stronger \(\)-DR is preserved compared to \(d=1\). In the DPTM, the GM enjoys no such privacy amplification.

Asymptotic behaviour and power function of the GMFigure 1(a) shows the asymptotic behaviour of a GM with \(}}{{}}=}{{5}},d=300\) and \(N=150\). The computed trade-off functions become symmetric, approach the off-diagonal and match the plot of the analytic form in Equation (7) exactly, even at a modest number of compositions. Figure 1(b) plots the power \(1-\) of an MI adversary vs. the \(}}{{}}\)-ratio at a fixed \(=10^{-3}\) for the GM and Figure 1(c) for the SGM assuming the worst case of a one-dimensional learning task and a sampling rate of \(0.3\). The curve for the DPTM in 1(c) was created using the formula from . As anticipated, the adversary has lower power throughout in the RTM compared to the DPTM, and the loss of power is amplified through subsampling.

MI attacks on DP ML modelsTo test the tightness of our theoretical bounds, we now perform an empirical audit using state-of-the-art offline MI attacks on ML models trained with DP. Recall that, despite the nominal difference in the threat model, the offline MI adversary and the RTM adversary compute the same tests and thus have the same capabilities. Each auditing run produces a pair of empirical trade-off functions shown in green and blue. These are compared to the theoretical bound, shown in orange. If the empirical curves crossed below the theoretical one, this would indicate a lack of tightness in our theoretical bounds. All results are shown as averages with standard deviation across \(1024\) runs for a single step of a one-dimensional synthetic binary classification task, since we are interested in the worst-case scenario under the RTM. Figure 2(a) shows the results of the LiRa offline MI attack introduced in , where \(^{}\) trains shadow models on \(D\) and fits a Gaussian likelihood to their confidence scores. At \(}}{{}}=1\), this _logit space_ auditing technique already nearly perfectly matches the theoretical bound. A further improvement is achieved by implementing the very recent auditing technique presented in , also at \(}}{{}}=1\). Here, a worst-case gradient with magnitude \(\) ("Dirac canary") is inserted and the hypothesis test takes place in _gradient space_ instead of logit space. As seen in Figure 2(b), the theoretical curve is matched _exactly_ by the empirical one.

So far, we have assumed that \(^{}\) can leverage their knowledge of the model to design a worst-case database which influences only a single coordinate of the gradient. This is useful for auditing purposes, but does not necessarily mirror real-life training. We therefore also repeat the gradient space attack against a linear neural network with a \(50\)-dimensional latent space trained on the _diabetes_ dataset . Figure 2(c) shows that, when the adversary cannot reduce the effective dimensionality of the gradient, the hypothesis test loses substantial power compared to \(d=1\) (dashed line) at \(}}{{}}=2\) and the trade-off functions become symmetric, exactly verifying Theorem 3.

Additional results for auditing the LM and SGM are shown in the Appendix.

Benefits for ML model trainingWe next demonstrate that the tighter privacy bounds of the RTM translate to tangible improvements in terms of accuracy/privacy trade-offs in deep learning applications. We performed experiments on three classification tasks: CIFAR-10 using ResNet-9, ImageNet using ResNet-18, and Stanford SNLI using a BERT transformer. In each case, we fixed a final privacy guarantee (in terms of \((,())\) or equivalently \((,())\)) for the DPTM and then trained a set of models up to this privacy guarantee. We then computed the DP-SGD noise scale corresponding to the same hypothesis testing privacy guarantee for the RTM, and re-trained the models for the same number of steps. For all datasets, the enhanced privacy guarantees of the RTM allowed us to obtain models with higher out-of-sample accuracy (e.g. up to \(14\%\) higher on CIFAR-10 at \( 1\)) for the nominally same membership inference guarantee by lowering the DP-SGD noise scale. These results are illustrated in Figure 4 and experimental details can be found in the Appendix.

Figure 2: Asymptotic behaviour of the GM and power analyses of the GM/SGM.

Improved bounds on data reconstruction attacksAs discussed above, successfully defending against MI attacks implies that all weaker attacks such as data reconstruction will also be unsuccessful. In the Appendix, we demonstrate that, in the RTM, data reconstruction adversaries also suffer a diminished success rate compared to the DPTM, as measured in terms of Reconstruction Robustness [31; 32], which can be bounded directly through its relationship to hypothesis testing .

## 6 Conclusion

In this work, we consider a threat model relaxation of high practical relevance, where the adversary does not have access to the candidate record in question when performing an MI attack. Our results provide a complete characterisation of the adversary's optimal error rates under the restrictions of the RTM, including at a fixed low Type-I error rate which is particularly meaningful for privacy-sensitive applications such as medical or financial data . Moreover, they allow for a fair and direct comparison between the privacy properties of noise mechanisms under different threat models. Our results can thus guide individuals who train ML systems on sensitive data in the selection of the appropriate noise magnitude to better balance model accuracy vs. privacy protection.

We acknowledge the following limitations. We focused on additive noise mechanisms and the global model of DP in this work. An extension of our results to local DP, mechanisms, mechanisms with discrete outputs and private selection mechanisms is a natural next step. Moreover, our analysis considers the case of non-adaptive composition, which applies to some, but not all, relevant ML tasks. An investigation of adaptive composition scenarios (considering potential correlations between mechanism outputs) would be of interest to expand the scope of our guarantees. Last but not least, we regard the analysis of adversaries who are even more limited in their background knowledge (e.g. no knowledge of the sensitivity, noise magnitude or noise type) as a promising avenue for future work.

Figure 4: Deep learning with DP-SGD. Calibrating the noise scale to the RTM privacy guarantee results in substantially improved Top-\(1\) validation set accuracy compared to calibrating to the DPTM guarantee. (a) CIFAR-10: \(0.81 0.01\) vs. \(0.67 0.01\) at \((1,10^{-5})\)-DP, (b) SNLI: \(0.80 0.005\) vs. \(0.78 0.005\) at \((1,1.8 10^{-6})\)-DP, (c) ImageNet: \(0.48\) vs. \(0.46\) at \((10,10^{-6})\)-DP. Error bars denote standard deviation across \(5\) runs.

Figure 3: Auditing our theoretical bounds using offline MI attacks.