# Battle of the Backbones: A Large-Scale Comparison of Pretrained Models across Computer Vision Tasks

Micah Goldblum\({}^{1}\)  Hossein Souri\({}^{2}\)  Renkun Ni\({}^{3}\) Manli Shu\({}^{3}\) Viraj Prabhu\({}^{4}\)

Gowthami Sompaelli\({}^{3}\) Prithvijit Chattopadhyay\({}^{4}\) Mark Ibrahim\({}^{6}\) Adrien Bardes\({}^{5,6}\)

Judy Hoffman\({}^{4}\) Rama Chellappa\({}^{2}\) Andrew Gordon Wilson\({}^{1}\) Tom Goldstein\({}^{3}\)

Authors contributed equally. Correspondence to goldblum@nyu.edu and hsouri@jhu.edu. This work was conducted at New York University\({}^{1}\), Johns Hopkins University\({}^{2}\), University of Maryland\({}^{3}\), Georgia Institute of Technology\({}^{4}\), Inria\({}^{5}\), and Meta AI Research\({}^{6}\).

###### Abstract

Neural network based computer vision systems are typically built on a _backbone_, a pretrained or randomly initialized feature extractor. Several years ago, the default option was an ImageNet-trained convolutional neural network. However, the recent past has seen the emergence of countless backbones pretrained using various algorithms and datasets. While this abundance of choice has led to performance increases for a range of systems, it is difficult for practitioners to make informed decisions about which backbone to choose. Battle of the Backbones (BoB) makes this choice easier by benchmarking a diverse suite of pretrained models, including vision-language models, those trained via self-supervised learning, and the Stable Diffusion backbone, across a diverse set of computer vision tasks ranging from classification to object detection to OOD generalization and more. Furthermore, BoB sheds light on promising directions for the research community to advance computer vision by illuminating strengths and weakness of existing approaches through a comprehensive analysis conducted on more than \(1500\) training runs. While vision transformers (ViTs) and self-supervised learning (SSL) are increasingly popular, we find that convolutional neural networks pretrained in a supervised fashion on large training sets still perform best on most tasks among the models we consider. Moreover, in apples-to-apples comparisons on the same architectures and similarly sized pretraining datasets, we find that SSL backbones are highly competitive, indicating that future works should perform SSL pretraining with advanced architectures and larger pretraining datasets. We release the raw results of our experiments along with code that allows researchers to put their own backbones through the gaunlet here: [https://github.com/hsouri/Battle-of-the-Backbones](https://github.com/hsouri/Battle-of-the-Backbones).

## 1 Introduction

The dominant paradigm for building machine vision systems involves a feature extractor network, also known as a _backbone_, which feeds into a task-specific _head_. The backbone might output a dense array of features for object detection and localization, or a single feature vector for classification or image retrieval. While backbones can be trained from scratch on task-specific data, many off-the-shelf backbones are pretrained on large benchmark datasets and then fine-tuned for the task at hand. This transfer learning approach has several advantages. First, it dramatically reduces the application-specific data requirements of deep learning and has led to improved performance on a wide range ofapplications. Second, it can speed up training and reduce compute costs even when large amounts of task-specific data are available . Finally, pretraining datasets often contain images from many disparate domains, resulting in model robustness that can be transferred to downstream tasks.

Early deep learning based vision systems relied heavily on ImageNet pretraining . In contrast, today's practitioners have access to a cornucopia of choices, with different pretrained models resulting in significant performance differences. There are three primary factors that influence the performance of such a model: its architecture, the pretraining algorithm, and the pretraining dataset. Each of these design dimensions presents many options, resulting in a dizzying array of choices for practitioners building a computer vision system. Despite this wide variety of choices, practitioners have no resource to turn to and instead are left piecing together results from method papers or testing out the backbones themselves.

We pit these backbones against each other in a _Battle of the Backbones_ (BoB). BoB compares many popular publicly available pretrained checkpoints, as well as randomly initialized baselines, on a wide variety of downstream tasks including image classification on natural, medical, and satellite images (Section 3.1), object detection and segmentation (Section 3.2), out-of-distribution generalization (Section 3.3), and image retrieval (Section 3.4).

Aside from assisting practitioners building computer vision systems, another central goal of this benchmark is to help guide the research community towards fruitful research directions in their quest for designing better backbones. BoB sheds light on the strengths and weaknesses of pretraining routines and architectures, revealing popular misconceptions and fundamental limitations, as well as promising directions for improvement. Below, we summarize several of our primary findings and discuss previous efforts for comparing backbones.

### Battle of the Backbones: The TLDR

The subsequent sections in this paper contain numerous experimental details. Therefore, we distill several key findings below:

\(\) Across the suite of comprehensive evaluations in BoB, spanning tasks, datasets, and settings (including ID and OOD), supervised ConvNeXt-Base, supervised SwinV2-Base trained using ImageNet-21k, and CLIP ViT-Base come out on top. The same winners also win at smaller scales. Among smaller backbones, ConvNeXt-Tiny and SwinV2-Tiny emerge victorious, followed by DINO ViT-Small.

Figure 1: **Performance is correlated across tasks.** Performance for each model is reported in terms of standard deviations above/below the mean averages across datasets. **Left:** Comparison between classification and detection. **Right:** Comparison between classification and OOD classification.

\(\) Despite the recent attention paid to transformer-based architectures and self-supervised learning, high-performance convolutional networks pretrained via supervised learning outperform transformers on the majority of tasks we consider.

\(\) The observed superiority of supervised pretraining occurs because such models are often trained on larger datasets. In apples-to-apples comparisons on the same dataset scale, SSL models outperform their supervised counterparts.

\(\) ViTs are more sensitive to the amount of pretraining data and the number of parameters than CNNs.

\(\) Performance across tasks is strongly correlated - the top-performing backbones in BoB tend to be universally good across tasks and settings. See Figure 1.

### Previous Benchmarks

Throughout much of the last decade, the most popular backbones were pretrained on ImageNet . Since 2020, SimCLR  and CLIP  have popularized self-supervised backbones and spawned much new research. While method papers that propose a new pretraining routine typically compare to similar competitors on several downstream tasks, we focus in this section on works that specifically benchmark large collections of backbones on diverse tasks.

In 2019, Goyal et al.  compared AlexNet  and ResNet-50  models pretrained using colorization and jigsaw pretext tasks to supervised learning models, finding that supervised learning massively outperformed SSL at the time. Kolesnikov et al.  similarly compared several pretext tasks and convolutional neural network architectures, showing that architectural advances on supervised learning do not always translate to improved self-supervised learning. Kornblith et al.  instead benchmarked the transferability of ImageNet-trained supervised learning models on downstream classification tasks, varying the architecture and finding that the correlation between downstream performance and ImageNet test accuracy is nearly perfect across architectures. In the same year, Zhai et al.  built the Visual Task Adaptation Benchmark (VTAB) and tested various self-supervised learning methods including VAEs and GAN discriminators, also exhibiting the dominant performance of supervised learning models. In 2020, Ericsson et al.  evaluated ResNet-50 models trained on ImageNet using various SSL algorithms, finding that the performance of then-existing SSL algorithms on a richer set of downstream tasks were strongly correlated with their ImageNet-1k test accuracy and finding improved performance of the newer SSL algorithms compared to previous studies.

Since the above works, pretraining algorithms along with their training sets and architectures have made tremendous progress, and whereas supervised learning was previously the default approach to pretraining, the options now are endless. Therefore, benchmarking backbones deserves renewed attention. See Appendix A for an additional survey of task-specific benchmarks.

## 2 A Guide to BoB

Among the distinguishing features of the diverse backbones competing in our battle are their architectures, pretraining routines, and the datasets on which they were pretrained. Table 1 contains an overview of the backbones we benchmark including their pretraining algorithms, pretraining datasets, and architectures. We also provide a more detailed description of these features and the precise pretrained checkpoints we use in Appendix B.

**A Note on Scale and Apples-to-Apples Comparison.**_Many practitioners have limited compute and moreover will need to tune hyperparameters on their own datasets without exceeding their compute budget. To simulate this scenario, we perform moderate hyperparameter sweeps, we preclude particularly long training schedules, and we do not consider architectures bigger than ConvNeXi-Base, except for the Stable Diffusion backbone which does not come in a smaller size. Specific hyperparameter grids are detailed in subsequent sections. Moreover, we only use publicly available checkpoints that would also be accessible to practitioners. Available checkpoints were pretrained with varying amounts of hyperparameter tuning, and different pretraining algorithms were trained on different datasets and architectures making a precise apples-to-apples comparison infeasible. Nevertheless, this comparison of existing checkpoints is the relevant one for practitioners, as it represents realistic conditions, and we use identically sized hyperparameter sweeps for each backbone on downstream tasks._

### The Tasks

In order to comprehensively probe the capabilities of the backbones, we evaluate their performance both fine-tuned and frozen on a number of downstream tasks belonging to the following categories:

* **Classification:** We measure both fine-tuned and linear probe performance of backbones on various downstream classification tasks including natural, medical, or satellite image datasets in Section 3.1. Image classification tasks require that a backbone extract features which identify the content of an image's foreground but not necessarily how many of an object there are or where they are located within an image.
* **Object detection and segmentation:** Unlike image classification, dense prediction tasks require backbones to extract features containing the precise locations of objects, on a pixel basis for segmentation and in enough fidelity to draw bounding boxes for object detection. We evaluate backbones on both of these tasks in Section 3.2.
* **Out-of-distribution generalization:** In real-world applications, computer vision systems are often deployed on data which does not reflect their training set distribution. Even high-performing models are known to fail under domain shifts . Therefore, we evaluate the abilities of models both to generalize to new downstream domains in Section 3.3.
* **Image retrieval:** Image retrieval requires a backbone to match like images via proximity in feature space. We explore tasks that require matching the images with respect to various criteria such as semantic content and visual similarity in Section 3.4.

## 3 Experimental Setup

We now describe our experimental setup for each task. Specifically, we list learning protocols, datasets, and evaluation metrics. Find complete experimental and implementation details in Appendix C.

### Classification

**Learning protocols.** We evaluate pretrained backbones on various datasets under two fine-tuning protocols, following previous works : **end-to-end fine-tuning** (including experiments with only a small number of labeled samples) and **linear probing**. In the former scenario, we fine-tune the full model end-to-end on a given dataset or on a fraction of it, and we measure the accuracy on the test split. In the linear probing scenario, we extract features from the frozen pretrained backbone, and only learn a linear classifier on top of these pretrained representations. These two protocols are widely used in previous work to evaluate the quality of pretraining methods such as in self-supervised learning  and vision-language pretraining .

**Datasets and evaluation metrics.** We conduct experiments on \(6\) common image classification datasets, covering multiple domains such as natural images (ImageNet-1K , CIFAR-100 ,

  
**Pretraining** & **Style** & **Dataset** & **Architecture**(s) \\  MoCo v3  & SSL & ImageNet-1k  & ViT  \\ VICReg  & SSL & ImageNet-1k & ResNet  \\ VICRegL  & SSL & ImageNet-21k & ConvNeXt  \\ DINO  & SSL & ImageNet-1k & ResNet, ViT \\ MAE  & SSL & ImageNet-1k & ViT \\ Stable Diffusion  & Vision-Language & LAION-2B  & Stable Diffusion encoder \\ CLIP  & Vision-Language & LAION-2B, CLIP & ResNet, ViT \\ MiDaS  & Supervised & 12 \(\) Depth Datasets & SwinV2  \\ Image classification & Supervised & ImageNet-21k,-1k & All above architectures \\ Random initialization & None & N/A & All above architectures \\   

Table 1: **A synopsis of the backbones we benchmark.** Columns correspond to the pretraining algorithm, a coarse categorization, the pretraining dataset, and the architectures we include. A detailed description of each algorithm, pretraining dataset, and architecture can be found in Appendix B.

Flowers-102 , Aircraft ), satellite images (EuroSAT ), and medical X-ray data (CheXpert ) showing the generalization and transferability of the pretrained backbones. All datasets we use are publicly available, and we list their details including size and the number of classes in Appendix C. For experiments with only a fraction of the training set, we randomly sample 1% and 10% of the training samples and fine-tune the pretrained backbones on these subsets. When sampling the subsets, we maintain the original dataset's label distribution. Note that we only consider in-domain generalization here, where the training and testing splits are from the same source.

To evaluate, we measure _classification accuracy_ and _Area Under the ROC Curve_ (AUC) on the test split as performance metrics for single-label and muti-label classification tasks, respectively. In addition to the best score among hyperparameter vectors, we also plot the accuracy for the first several epochs to show the convergence rate of different pretrained backbones. Moreover, we benchmark the latency and the memory usage of each backbone on the same device.

### Object Detection and Segmentation

**Learning protocols.** For evaluations on object detection and instance segmentation, we employ the Cascade Mask R-CNN framework . We conduct experiments with three protocols: **(1)** end-to-end training from random initialization, **(2)** end-to-end finetuning using pretrained backbones, and **(3)** finetuning with frozen backbones. Whereas finetuning with a frozen backbone is atypical in segmentation and detection, this latter protocol allows us to probe localization within features extracted by pretrained models and complements linear probing classification experiments. See Appendix C.1 for a discussion on the potential for ViTs, especially large ones, to exceed the performance of other models under more expensive training protocols.

**Datasets and evaluation metrics.** We conduct object detection and instance segmentation evaluations on the popular COCO dataset . We follow the COCO-style average precision (AP) metric, which calculates the average across various Intersection over Union (IoU) thresholds. We report the box Average Precision (box AP), box AP@50, and AP@75 for object detection and mask Average Precision (mask AP), mask AP@50, and mask AP@75 for instance segmentation .

### Out-of-Distribution Generalization

While modern networks may exhibit strong performance on data distributions they are trained on, a wide body of prior work  has found that the performance of such models can degrade significantly under distribution shifts. In addition to evaluating the in-distribution performance of backbones across a diverse set of downstream tasks, we also consider how this performance translates to out-of-distribution (OOD) settings.

**Learning protocols.** Several task-specific datasets and benchmarks have been proposed to evaluate the robustness of models to deviations from their training distributions. Concretely, we study the generalization of the trained backbones on two tasks, **(1)** image classification and **(2)** object detection, and on two types of distribution shifts, **(A)** structure and style variations within ImageNet and **(B)** synthetic-to-real generalization.

**Datasets and evaluation metrics.** We consider the following broad benchmarks for OOD evaluation:

**(A) Robustness to changes in structure and style.** We measure OOD generalization of ImageNet-trained or fine-tuned models on the following benchmarks: **(i)** ImageNet-A . ImageNet-A(dversarial) contains a curated subset of ImageNet test images spanning 200 categories that are especially challenging for trained deep models. **(ii)** ImageNet-V2 . ImageNet-V2 is an additional test set of ImageNet-like images collected a decade after the original dataset following an identical collection protocol. **(iii)** ImageNet-R . ImageNet-R(endion) contains artistic renditions for 200 categories from ImageNet, including cartoons, graffiti, embroidery, origami, sculptures, _etc._**(iv)** ImageNet-S . ImageNet-S(ketch) is a web-crawled and manually cleaned collection of black and white sketch images from ImageNet categories.

**(B) Syn-to-real generalization.** We also measure the performance of models trained on synthetic data and tested on real data. Synthetic data has emerged as a popular alternative in settings where it may be hard or expensive to curate reliably annotated real-world data. We measure syn-to-real generalization for image classification and object detection on the two following popular benchmarks: **(i)** VisDA Syn\(\)Real. The VisDA classification benchmark consists of \( 152\)k synthetic images and \( 55\)k real images across \(12\) classes. The synthetic images in VisDA are 3D renderings of objects from multiple viewpoints and under different lighting conditions. The real counterparts are crops of the \(12\) classes obtained from the COCO dataset. **(2)** Sim10k\(\)Cityscapes. For object detection, we use Sim10k as the synthetic training dataset and Cityscapes as the real evaluation dataset. Sim10k consists of \( 10\)k street view images (drawn from GTAV). Cityscapes consists of \( 5\)k densely annotated street view images curated from vehicular viewpoints in the real world. Following prior work , we train on the entirety of Sim10k to detect instances of "car" and measure detection performance on the validation split of Cityscapes.

We report generalization performance using classification accuracy on the OOD test set for image classification and mean average precision or mAP@50 for object detection.

### Image Retrieval

We conduct evaluations on a diverse set of retrieval datasets encompassing content-based image retrieval and classification datasets that we repurpose for semantic retrieval tasks. For geographic landmark retrieval, we utilize the Oxford dataset  and the Paris dataset . To ensure accuracy, we employ the cleaned-up versions of these datasets with corrected labels . The INSTRE dataset  consists of objects such as toys and irregularly-shaped products placed in different locations and conditions. To examine fine-grained retrieval, we employ the Caltech-UCSD Birds-200 dataset (CUB-200) , which contains various bird classes captured under different backgrounds, poses, and lighting conditions. For a diverse set of natural images, we use the iNaturalist dataset . This dataset offers a wide range of fine-grained categories classified into 13 super-categories, including Plant, Insect, Bird, and Mammal. To evaluate retrieval performance in real-world scenarios, we employ the Objectnet dataset . This dataset consists of 313 object classes with randomly varying backgrounds, rotations, and imaging viewpoints. For large-scale landmark recognition, we utilize the Google Landmarks v2 dataset , which includes approximately 200,000 unique landmarks. Lastly, we employ the INRIA Copydays dataset , which comprises a small collection of holiday photos. Among the datasets mentioned, iNaturalist, Objectnet, and CUB-200 can be categorized as semantic retrieval datasets, while the remaining datasets fall under content-based retrieval datasets.

To evaluate, we measure model performance using mean-Average-Precision or _mAP_. We first compute the average precision for a given query image, and then compute the mean over all queries to find the mAP. We also measure _Recall@k_, which measures the proportion of correct matches among the top \(k\), and _MRR_ (Mean Reciprocal Rank), which records the number of results returned before the first correct match and computes the mean of the reciprocal of these misses. Higher is better for all metrics.

## 4 I'm a Practitioner. Which Backbone Should I Choose?

Practitioners today can choose from a large catalogue of backbones of varying sizes, training methods, and pretraining data: which backbone should a practitioner select for a particular task or in general? To answer this question, in BoB, we systematically compare publicly available backbones (see Table 1) across multiple tasks, datasets and settings. To make these comparisons, we use the following ranking protocol:

**(1) Setting-specific Z-Scores.** For a particular task and setting (e.g, top-1 classification accuracy on ImageNet), we first compute z-scores for all the backbones being evaluated - i.e., for setting specific performance (e.g., accuracy) values \(\{x_{i}\}_{i=1}^{N}\), z-scores are computed as \(\{-}{}\}_{i=1}^{N}\) where \(\) and \(\) are the mean and standard deviation of the sample. This allows us to measure how good a specific backbone is (stds above or below) compared to "mean" performance of all backbones in that setting.

**(2) Cross-setting Comparisons.** To compare backbones across different tasks and settings, we simply aggregate and compare the previously obtained z-scores to obtain a relatively (coarse) ranking of backbones.

Using rankings, we can report not only the best performing backbones for each task but also the best backbone in terms of overall performance across tasks, datasets and settings (see Table 2 for a summary).

### Task-Specific Backbones

**Classification.** For classification, across multiple datasets and experimental settings (fine-tuning, linear probing, full and low-shot training), we find "Supervised SwinV2-Base trained on IN-21k (finetuned on IN-1k)" to be the best performing backbone, followed by "CLIP ViT-Base" and "Supervised ConvNeXt-Base trained on IN-21k" (see row 1, Table 2).2

**Object Detection & Segmentation.** For object detection and instance segmentation, we find "Supervised ConvNeXt-Base trained on IN-21K" \(>\) "Supervised SwinV2-Base trained on IN-21k (finetuned on IN-1k)" \(>\) "Supervised ConvNeXt-Base trained on IN-1k".

**Image Retrieval.** For image retrieval, we find "Supervised ConvNeXt-Base trained on IN-21k" to be the best choice, with "Supervised SwinV2-Base trained on IN-21k (finetuned on IN-1k)" and "CLIP ViT-B trained on LAION-2B" being second and third.

**(OOD) Classification.** Across OOD evaluations for classification, we find "Supervised ConvNeXt-Base trained on IN-21k" \(>\) "Supervised SwinV2-B trained on IN-21k (finetuned on IN-1k)" \(>\) "CLIP ViT-Base trained on LAION-2B".

**(OOD) Object Detection.** For Syn\(\)Real object detection, we find "Supervised ConvNeXt-Base trained on IN-1k" to be the best backbone, followed by "Supervised ConvNeXt-Tiny trained on IN-1k" and "Supervised ConvNeXt-Base trained on IN-21k".

### Best Backbones Overall

For practitioners with no specific task in mind, the best performing models in terms of aggregate performance are "Supervised ConvNeXt-Base trained on IN-21k" followed by "Supervised SwinV2-Base trained on IN-21k (finetuned on IN-1k)" and "CLIP ViT-Base trained on LAION-2B". Overall, we note that backbones trained in a supervised fashion (SwinV2-Base, ConvNeXt-Base) or with vision and language supervision (CLIP ViT-Base) outperform the rest. Furthermore, we find that CLIP ViT-Base is closely followed by Supervised ViT-Base trained on IN-21k (finetuned on IN-1k). We more precisely compare approaches and analyze trends in Section 5.

### Backbones on a Tight Budget

Many computer vision applications demand efficient backbones for fast or on-device inference. In this section, we benchmark three small backbones: RegNetX-400F , EfficientNet-B0  and ResNet-18  all pretrained in a supervised fashion on ImageNet-1k. We rank the performance of these small backbones on the set of tasks in Table 3. We find that EfficientNet-B0 performs best overall and across classification, retrieval, and OOD classification, followed by RegNetX-400MF and then ResNet-18. Interestingly, ResNets still outperform newer efficient architectures for detection and segmentation.

  
**Task** & **Good** & **Better** & **Best** \\ 
1 Cls & ConvNeXt-B (IN-21k) & CLIP ViT-B (LAION-2B) & Sup. SwinV2-B (IN-21k,1k) \\
2 Det & Sup. ConvNeXt-B (IN-1k) & Sup. SwinV2-B (IN-21k,1k) & Sup. ConvNeXt-B (IN-21k) \\
3 Seg & Sup. ConvNeXt-B (IN-1k) & Sup. SwinV2-B (IN-21k,1k) & Sup. ConvNeXt-B (IN-21k) \\
4 Ret & CLIP ViT-B (LAION-2B) & Sup. SwinV2-B (IN-21k,1k) & Sup. ConvNeXt-B (IN-21k) \\
5 (OOD) Cls & CLIP ViT-B (LAION-2B) & Sup. SwinV2-B (IN-21k,1k) & Sup. ConvNeXt-B (IN-21k) \\
6 (OOD) Det & Sup. ConvNeXt-B (IN-21k) & Sup. ConvNeXt-T (IN-1k) & Sup. ConvNeXt-B (IN-1k) \\ 
7 All & CLIP ViT-B (LAION-2B) & Sup. SwinV2-B (IN-21k,1k) & Sup. ConvNeXt-B (IN-21k) \\   

Table 2: **Which backbone should I choose?** We list the top 3 most performant backbones (left to right) for various tasks and settings. \(}\) corresponds to OOD evaluations and \(}\) indicates overall comparisons.

## 5 Observations and Trends

\(\)**A performance comparison of ViTs and CNNs. Modern architectures strongly outperform vanilla ViTs.** We see in Table 2 that the best performing backbone (ConvNeXt-Base) is convolutional, with a hierarchical transformer (SwinV2-Base) being a close second. The latter transformer architecture incorporates a strong spatial inductive bias. These findings suggest that the community should move past vanilla ViTs which are still used frequently. As a caveat, we do not evaluate very large models, and it is possible that ViTs might outperform their more advanced variants or convolutional networks at larger scales.

\(\)**ViTs benefit more from scale than CNNs.** For the suite of backbones considered in BoB, we find that relative performance (z-scores) for both CNNs and ViTs correlates positively with parameter count but more so for ViTs (spearman \(=0.58\)) than for CNNs (spearman \(=0.35\)). Similarly, while overall relative performance correlates with the size of pretraining data, the correlation is again significantly higher for ViTs (\(=0.72\)) than for CNNs (\(=0.33\)). This observation indicates that benchmarking much larger backbones might yield different winners, possibly ones with transformer-based architectures.

\(\)**Supervised or not? Supervised learning backbones dominate, but primarily because they are available pretrained on larger datasets. SSL backbones can outperform supervised pretraining with similar sized pre-training datasets.** We obtain the average score of the top \(3\) backbones within different pretraining styles, namely self-supervised, supervised with ImageNet-1K, and supervised with ImageNet-21K, for each task (see Appendix D). ConvNeXt and SwinV2 pretrained with supervision on ImageNet-21K outperform the SSL backbones on all tasks. The results suggest that we should try using advanced architectures, either convolutional or transformers, when applying SSL methods, and we should train on large datasets to compete with supervised learning. In these experiments, supervised pretraining checkpoints are often available trained on much larger datasets (ImageNet-21k). When comparing models pretrained on similarly sized datasets, SSL or vision-language pretraining methods achieve better performance on classification (both in- and out-of-distribution) and retrieval tasks, which heavily rely on the learned representations. However, supervised learning backbones maintain a decisive edge for detection and segmentation. We can also compare backbones which use the same ViT-Base architecture and find that SSL methods do outperform ImageNet-1k supervised backbones but are worse than ImageNet-21k trained backbones.

\(\)**Performance across tasks is highly correlated.** Across tasks examined, we find a strong positive Spearman correlation between performance on task pairs (typically \(>0.8\)). This finding supports the current trend of general purpose foundation models for computer vision. Moreover, this finding also supports recent work which argues that a single inductive bias can solve a wide range of seemingly different problems . However, it is noteworthy that the retrieval task exhibited a comparatively lower but still statistically significant correlation (\(=0.49\)) with respect to classification and retrieval ranking. This lower correlation can be attributed to the performance limitations of the MiDaS and MAE pretrained models in the context of retrieval. Upon removing these two backbones, the correlation coefficient \(\) increased to 0.8, reinforcing the influence of the aforementioned models on the observed results.

  
**Task** & **Good** & **Better** & **Best** \\ 
1 Cls & ResNet-18 & RegNetX-400MF & EfficientNet-B0 \\
2 Det & RegNetX-400MF & EfficientNet-B0 & ResNet-18 \\
3 Seg & RegNetX-400MF & EfficientNet-B0 & ResNet-18 \\
4 Ret & ResNet-18 & RegNetX-400MF & EfficientNet-B0 \\
5 (OOD) Cls & ResNet-18 & RegNetX-400MF & EfficientNet-B0 \\
6 (OOD) Det & EfficientNet-B0 & ResNet-18 & RegNetX-400MF \\  
7 All & ResNet-18 & RegNetX-400MF & EfficientNet-B0 \\   

Table 3: **Which tiny backbone should I choose?** We rank the most performant very lightweight backbones (left to right) for various tasks and settings. \(}\) correspond to OOD evaluations and \(}\) indicates overall comparisons.

\(\) **Transformers excel under end-to-end fine-tuning while convolutional networks excel under linear probing.** For "linear probing" experiments, we freeze a pretrained backbone and only learn the head. Note that for detection and segmentation, the head is more than a linear layer. By inspecting the performance difference between the two fine-tuning strategies (Figure 2), we find that ViTs benefit significantly more from end-to-end fine-tuning compared to CNNs, both for supervised and self-supervised pretraining. See Figure 2 for a comparison on dense prediction tasks.

\(\) **CLIP models and the promise of advanced architectures in vision-language modeling.** For almost all the tasks (except OOD detection), CLIP pretraining is the best among the vanilla vision transformers, even compared to ImageNet-21k supervised trained backbones. Among all the backbones, CLIP is only worse than ImageNet-21k trained SwinV2 and ConvNeXt, which shows the power of vision-language pretraining and again, suggests that we should consider more backbones other than plain ViTs when conducting self- or weakly-supervised learning.

\(\) **What about generative backbones?** In contrast to models trained using supervised or self-supervised approaches with contrastive loss, backbones trained with a generative objective, such as MAE or Stable Diffusion, had comparatively inferior performance. We recommend caution when interpreting this result, as the evaluation of Stable Diffusion is currently limited to select tasks. Nonetheless, Stable Diffusion is a larger backbone than others considered in this benchmark and is trained on a very large dataset, yet it exhibits inferior performance.

\(\) **Battle of the "small" backbones.** Keeping limited resources in mind, we also compare the "small" subset of backbones in BoB (\(<30\)M parameters) - with ViT-Small, ConvNeXt-Tiny, Swin-Tiny and ResNet-50 architectures. Overall, we find Supervised ConvNeXt-T trained on IN-1k to be the best, followed by Supervised SwinV2-T trained on IN-1k and DINO ViT-S trained on IN-1k. Interestingly, supervised learning again dominates, and backbones pretrained on just IN-1k outperform ones trained on a considerably more diverse and larger dataset (MiDaS).

\(\) **Performance vs. Speed?** Our analysis reveals a strong negative correlation (\(=-0.41\)) between throughput (computed on NVIDIA RTX A5000) and average performance z-scores across all tasks when considering each backbone. This finding aligns with our previous observation that larger models tend to exhibit superior performance. Consequently, in order to achieve enhanced performance, one may need to sacrifice speed.

\(\) **Monocular depth-estimation as a general purpose pretraining strategy.** In our experiments, MiDaS achieves performance competitive with that of top conventional supervised and SSL backbones at classification, object detection, and segmentation, even outside of the natural image domain, for example on satellite images. This observation suggests that depth-estimation may serve as a powerful and generalizable primary or auxiliary pretraining task for foundation models, supporting findings of Lao et al. .

\(\) **Calibration and test likelihood are correlated with accuracy.** We measure expected calibration error (ECE) as well as test cross-entropy loss on the ImageNet test set. Whereas test likelihood is

Figure 2: **Transformers benefit significantly more from end-to-end fine-tuning than CNNs on dense prediction tasks.** We visualize the difference in performance between end-to-end fine-tuning and only training the head atop a frozen feature extractor on different tasks. The x-axis is the difference in relative performance (fine-tuning z-score minus fixed backbone z-score). Across panels, the performance differences correlate between tasks.

strongly correlated with accuracy (\(r=-0.8278\)), ECE exhibits a weaker correlation (\(r=-0.4876\)). In both cases, we observe p-values under \(0.05\). We also note that self-supervised pretraining typically leads to inferior calibration.

\(\)**CNNs and SSL are more adversarially robust.** We additionally measure the adversarial robustness of each backbone on the ImageNet test set using an \(_{}\)-constrained PGD attack with multiple radii (see Appendix Table 19). For each architecture where we possess self-supervised learning versions, we see that supervised pretraining always yields inferior robustness. Moreover, ViTs are more vulnerable to adversarial examples than convolutional networks. Notably, ConvNeXt is more adversarially robust even when trained in a supervised fashion.

## 6 Where Are Things Going From Here?

At the core of every computer vision model is a backbone. In our battle of the backbones, we compared more than 1,500 training runs to surface insights for computer vision practitioners and researchers. To guide practitioners, we analyzed the performance of publicly available vision backbones across a broad range of tasks from segmentation and detection to classification and retrieval. We found supervised ConvNext, supervised SwinV2, and CLIP models performed well across this broad range of tasks. For computationally constrained settings, in our battle of the "small" backbones we found smaller counterparts to the same architectures supervised ConvNext-T and SwinV2, followed by DINO with a small ViT performed quite well. BoB offers practitioners a guide to select sensible backbones from the dizzying array of choices.

For researchers looking ahead, we also observed several notable trends. First, we found performance across tasks is strongly correlated, suggesting a shift away from specialized vision backbones to universal backbones that work well across a range of tasks. Next, we found throughput and performance are inverse related, suggesting scaling remains a promising avenue to improve backbones. Finally, we found that while our practical recommendations include many supervised models, in apple-to-apples comparisons to standard supervised training, self-supervised learning holds promise. By releasing all our experimental results along with code to put new backbones to the test, we hope BoB serves as a useful guide to both practitioners today and researchers looking ahead at tomorrow.

**Limitations.** We note that insights obtained from BoB are contingent on the vocabulary of tasks, backbones, and settings considered in this work. We intend for takeaways from this study to provide practical considerations useful for computer vision researchers, recognizing that such insights need to continuously evolve as more backbones are introduced and more tasks and settings are taken into account. Lastly, we note that studies in BoB focus mostly primarily on aspects related to performance, and exploration along other axes of importance (biases in models, etc.) remain.

Our benchmark does not include backbones larger than ConvNext-Base, aside from Stable Diffusion, and some rankings may change at a large scale. For instance, while we find that modern convolutional architectures pretrained via supervised learning perform best on most tasks, we also find that transformers benefit more from scale, both in terms of pretraining data and architecture size. It is possible that transformer backbones will pull ahead of convolutional backbones at very large scales.

## 7 Computation Cost and Carbon Footprint

The experiments in this paper took a cumulative 127k GPU hours on NVIDIA RTX A100 cards. Assuming the GPUs were running with an average carbon efficiency of 0.37 kgCO\({}_{2}\)eq/kWh, the total emissions are estimated to be 11792.36 kgCO\({}_{2}\)eq .