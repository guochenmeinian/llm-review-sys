# Approximate Size Targets Are Sufficient

for Accurate Semantic Segmentation

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We propose a new general form of image-level supervision for semantic segmentation based on approximate targets for the relative size of segments. At each training image, such targets are represented by a categorical distribution for the "expected" average prediction over the image pixels. We motivate the zero-avoiding variant of KL divergence as a general training loss for any segmentation architecture leading to quality on par with the full pixel-level supervision. However, our image-level supervision is significantly less expensive, it needs to know only an approximate fraction of an image occupied by each class. Such estimates are easy for a human annotator compared to pixel-accurate labeling. Our loss shows significant robustness to size target errors, which may even improve the generalization quality. The proposed size targets can be seen as an extension of the standard class tags, which correspond to non-zero size targets in each image. Using only a minimal amount of extra information, our supervision improves and simplifies the training. It works on standard segmentation architectures as is, unlike tag-based methods requiring complex specialized modifications and multi-stage training.

## 1 Introduction

Our image-level supervision approach applies to any semantic segmentation model and does not require any modification. It can be technically described in one paragraph, as follows. Soft-max prediction \(S_{p}=(S_{p}^{1},,S_{p}^{K})\) at any pixel \(p\) is a categorical distribution over \(K\) classes, including background. At any image, the average prediction over all image pixels, denoted by set \(\), is

\[:=_{p}S_{p} \]

where \(=(^{1},,^{K})\) is also a categorical distribution over \(K\) classes. It is an image-level prediction \(}\) the relative or normalized sizes (volume, area, or cardinality) of the objects in the image. We assume that training images have approximate size targets represented by categorical distributions \(v=(v_{k})_{k=1}^{K}\), e.g. \(v=(0,.15,0,,0,.75)\) for the middle image in Fig. 1 if "_bird_" is the second class and "_background_" is the last. This representation also applies to multi-label images. For each training image, our _size-target loss_

\[L_{size} = KL(v\|) = _{k}v_{k}}{^{k}} \]

is based on Kullback-Leibler (\(KL\)) divergence. Figure 2(b) shows some results for a generic segmentation network (ResNet101  backbone) trained on PASCAL  using only image-level supervision with approximate size targets (\(8\%\) mean relative errors). Our total loss is very simple: it combines size-target loss (2) and a common CRF loss (3) .

### Overview of weakly-supervised segmentation

By _weakly-supervised_ semantic segmentation we refer to all methods that do not use full pixel-precise ground truth (GT) masks for training. Such full supervision is overwhelmingly expensive for segmentation and is unrealistic for many practical purposes, see the right image in Fig. 1. There are many forms of weak supervision for semantic segmentation, e.g. based on partial pixel-level ground truth defined by "seeds" [6; 7], boxes , or image-level class-tags [2; 8; 9], see Fig. 1. It is also common to incorporate self-supervision based on various augmentation ideas and contrastive losses [10; 11; 12].

Lack of supervision also motivates unsupervised loss functions such as standard old-school regularization objectives for _low-level_ segmentation or clustering. For example, many methods [13; 14; 12] use variants of K-means objective (squared errors) enforcing the compactness of each class representation. It is also very common to use CRF-based pairwise loss functions [6; 7] that encourage segment shape regularity and alignment to intensity contrast edges in each image . The last point addresses the well-known limitation of standard segmentation networks that often output low-resolution segments. Intensity contrast edges on the high-resolution input image is a good low-level cue of an object boundary and it can improve the details and localization of the semantic segments.

Conditional or Markov random fields (CRF or MRF) are common basic examples of pairwise graphical models. The corresponding unsupervised loss functions can be formulated for continuous soft-max predictions \(S_{p}\) produced by segmentation networks, e.g. [6; 7; 9]. Thus, it is natural to use relaxations of the standard discrete CRF/MRF models, such as _Potts_ or its _dense-CRF_ version . We use a bilinear relaxation of the general Potts model

\[L_{crf}(S) = _{k}(-S^{k})^{}WS^{k} \]

where \(S:=(S_{p}\,|\,p)\) is a field of all pixel-level soft-max predictions \(S_{p}\) in a given image, and \(S^{k}:=(S^{k}_{p}\,|\,p)\) is a vector of all pixel predictions specifically for class \(k\). Matrix \(W=[w_{pq}]\) typically represents some given non-negative affinities \(w_{pq}\) between pairs of pixels \(p,q\). It is easy to interpret loss (3) assuming, for simplicity, that all pixels have confident _one-hot_ predictions \(S_{p}\) so that each \(S^{k}\) is a binary indicator vector for segment \(k\). The loss sums all weights \(w_{pq}\) between the pixels in different segments. Thus, the weights are interpreted as discontinuity penalties. The loss minimizes the discontinuity costs .

In practice, affinity weights \(w_{pq}\) are set close to \(1\) if two neighboring pixels \(p,q\) have similar intensities, and weight \(w_{pq}\) is set close to zero either when two pixels are far from each other on the pixel grid or if they have largely different intensities [6; 16; 17]. The affinity matrix \(W\) could be arbitrarily dense or sparse, e.g. many zeros when representing a 4-connected pixel grid. The non-zero discontinuity costs between neighboring pixels are often set by a Gaussian kernel \(w_{pq}=-I_{q}\|^{2}}{2^{2}}\) of given bandwidth \(\), which works as a soft threshold for detecting high-contrast intensity edges in the image. Thus, loss (3) encourages both the alignment of the segmentation boundary to contrast edges in the (high-resolution) input image and the shortness/regularity of this boundary.

Figure 1: Supervision types for segmentation: labeling speed and accuracy on PASCAL. The top-left corner of each image shows its estimated labeling time based on observed instances. The table shows per-image labeling times averaged over the data and mean Intersection-over-Union (mIoU) for comparable end-to-end methods with similar ResNet backbones (ResNet101 or WideResNet38 ), for fairness. We obtained mIoU scores, except for the “tag” and “box” scores from  and . Our supplemental materials detail evaluation of the labeling times and mIoU. For completeness, Tab.2 includes more complex architectures and multi-stage systems, e.g. for tags. This paper focuses on standard segmentation architectures for size supervision.

[MISSING_PAGE_FAIL:3]

with any given prior distribution \(v\) that could be different from uniform.

**Semantic segmentation with image-level supervision.** Most weakly-supervised semantic segmentation methods use losses based on segment sizes. This is particularly true for image-level supervision techniques . Clearly, segments for tag classes should have positive sizes, and segments for non-tag classes should have zero sizes.

Similarly to our paper, size-based constraints are often defined for the image-level _average prediction_\(\), see (1), computed from pixel-level predictions \(S_{p}\). Many generalized forms of pixel-prediction averaging can be found in the literature, where they are often referred to as _prediction pooling_. Some decay parameter often provides a wide spectrum of options from basic averaging to max-pooling. While the specific form of pooling matters, for simplicity, we discuss the corresponding balancing loss functions assuming basic average prediction \(\) in (1).

One of the earliest works on tag-supervised segmentation  uses _log-barriers_ to "expand" tag objects in each training image and to "suppress" the non-tag objects. Assuming image tags \(T\), their _suppression loss_ is defined as

\[L_{suppress}()  -_{k T}(1-^{k}) \]

encouraging each non-tag class to have zero average prediction \(^{k}\), which implies zero predictions \(S_{p}^{k}\) at each pixel. Their _expansion loss_

\[L_{expand}()  -_{k T}^{k}. \]

encourages positive average predictions \(^{k}\) and non-trivial tag class segments.

We observe that the expansion loss (9) may have a bias to equal-size segments, as particularly evident in the case of average predictions. Indeed, (9) implies

\[L_{expand}()  KL(u_{ T}\|) \]

which is a special case of our size loss (2) when the size target \(v=u_{ T}\) is a uniform distribution over tag classes. The intention of the log barrier loss (9) is to push image-level size prediction \(\) from the boundaries of the probability simplex \(_{K}\) corresponding to the zero-level for the tag classes \(T\). Figure 2(a) shows the results for training based on the total loss combining CRF loss (3) with the log-barrier loss (9). Its unintended bias to equal-size segments (10) is obvious. Note that the mentioned decay parameter used for generalized average predictions should reduce such bias.

Alternatively, it may be safer to use barriers for \(\) like

\[L_{flat} = -_{k T}\{^{k},\} \]

that have flat bottoms to avoid unintended bias to some specific size target inside the probability simplex \(_{K}\). Similar thresholded barriers are common .

### Contributions

In general, it would be great to have effective image-level supervision for segmentation that only uses barriers like (9) or (11) since they do not require any specific size targets. This corresponds to tag-only supervision. However, our empirical results for semantic segmentation using such barriers were poor and comparable with those in . A number of more recent semantic segmentation methods for tag-level supervision have considerably improved such results , but they introduce significantly more complex multi-stage training procedures and various architectural modifications, which makes such methods hard to replicate, generalize, or to understand the results. We are focused on general easy-to-understand end-to-end training methods. Our main contributions are:

* We propose and evaluate a new general form of weak supervision, size targets. The size-target supervision can be approximate and is relatively easy to get from human annotators.
* We propose the zero-avoiding variant of KL divergence as a general training loss, allowing our end-to-end size-target approach to be integrated with any segmentation architecture.
* Comprehensive experiments with our size-target method demonstrate state-of-the-art performance across multiple datasets using standard segmentation models typically employed for full supervision, without any architectural modifications.

Size-target loss and its properties

Our proposed total loss is very simple

\[L_{total}\;:=\;L_{size}+L_{crf} \]

where the two terms are our size-target loss (2) and standard CRF loss (3). The core new component is our size-target loss based on the _forward_ KL-divergence. Our size-target loss (2) encourages specific target volumes for tag classes. Additionally, the size-target loss suppresses non-tag classes, encouraging zero volumes for classes not in the image. The CRF loss also contributes to the suppression of redundant classes. Therefore, unlike most prior work on image-level supervision for semantic segmentation, e.g. , we do not need separate suppression loss terms like (8). We validated this claim experimentally, they did not change the results.

The size-target loss can also be integrated into other weakly-supervised settings, e.g. partial cross-entropy loss (4) commonly used for seeds. We show that using approximate size targets can significantly improve the seed-supervised segmentation in  when the seed lengths are short, see the right plot of Fig. 4.

\[L_{total}^{{}^{}}\;:=\;L_{size}+L_{crf}+L_{pce} \]

As is well known, KL divergence is asymmetric. In our work on image-level supervised segmentation, the order of the estimated and target distributions is crucial. The forward KL divergence possesses a zero-avoiding property, as illustrated in Fig. 3. Specifically, forward KL divergence imposes an infinite penalty when any class with a non-zero target is predicted as zero. In contrast, the penalty of the _reverse_ KL divergence is finite and much weaker. When using reverse KL divergence, segmentation models tend to generate trivial solutions, predicting all pixels as the background class. This issue likely arises due to dataset imbalance, where the background class is prevalent. The zero-avoiding property of forward KL divergence ensures that segmentation models do not produce trivial solutions and predict all classes in the image tag sets.

## 3 Experiments

### Experimental settings

**Datasets.** We evaluate our approach on three segmentation datasets: PASCAL VOC 2012 , MS COCO 2014 , and 2017 ACDC Challenge1. The PASCAL dataset contains 21 classes. We adopt the augmented training set with 10,582 images , following the common practice . Validation and testing contain 1449 and 1456 images. Seed supervision of the PASCAL dataset is from . COCO has 81 classes with 80K training and 40K validation images. ACDC Challenge is to segment the left ventricular endocardium. The training and validation sets contain 1674 and 228 images. The exact size targets are extracted from the ground truth masks.

**Approximate size targets.** We train segmentation models using approximate size targets \(v=(v_{k})_{k=1}^{K}\) generated for each image either by human annotators or by corrupting the exact size targets \(=(_{k})_{k=1}^{K}\) with different levels of noise. In all cases, we report the segmentation accuracy on validation data together with _mean relative error_ (mRE) of the corresponding corrupted size targets. For each training image containing class \(k\), the _relative error_ for the size target \(v_{k}\) is defined as

\[RE(v_{k})=-_{k}|}{_{k}} \]

Figure 3: _Forward_ vs _reverse_ KL divergence. Assuming binary classification \(K=2\), we can represent all possible probability distributions as points on the interval . The solid curves illustrate our “strong” size constraint, i.e. the _forward_ KL-divergence \(KL(v\|)\) for the average prediction \(\). We show two examples of volumetric prior \(v_{1}=(0.9,0.1)\) (blue curve) and \(v_{2}=(0.5,0.5)\) (red curve). For comparison, the dashed curves represent reverse KL divergence \(KL(\|v)\).

where \(_{k}\) is the exact size. mRE averages RE over all images and all classes. For human annotated size targets \(v=(v_{k})_{k=1}^{K}\), the relative size errors are computed directly from the definition (14).

When used, synthetic targets \(v=(v_{k})_{k=1}^{K}\) are generated by corrupting the exact targets \(=(_{k})_{k=1}^{K}\)

\[v_{k}(1+)_{k} (0,) \]

where \(\) is white noise with standard deviation \(\) controlling the level of corruption and operator \(\) represents re-normalization ensuring corrupted targets \((v_{k})_{k=1}^{K}\) add up to one. Equation (15) defines random variable \(v_{k}\) as a function of \(\). Thus, in this case, mRE can be analytically estimated from \(\)

\[mRE\;=\;E(-_{k}|}{_{k}})\;\;E(| |)\;=\;}\; \]

where \(E\) is the expectation operator. The approximation in the middle uses (15) as an equality ignoring re-normalization of the corrupted sizes, and the last equality is a closed-form expression for the _mean absolute deviation_ (MAD) of the Normal distribution \((0,)\).

**Evaluation metrics for segmentation.** We employ _mean Intersection-over-Union_ (mIoU) as the evaluation criteria for PASCAL and COCO, and _mean Dice similarity coefficient_ (DSC) for the ACDC dataset. The quality on the PASCAL test set is assessed on the online evaluation server.

**Implementation details.** We evaluate our approach with two types of ResNet-based  and one vision transformer (ViT) based  segmentation models on the PASCAL and COCO datasets. ResNet-based models follow the implementation of DeepLabV3+  using the backbone of ResNet101 (R101) or the backbone of WideResNet-38 (WR38) . For brevity, we name them R101-based or WR38-based DeepLabV3+ models. For the ViT-based network, We follow the implementation of Segmenter , adopting its ViT-B/16 backbone and linear decoder. For experiments on the ACDC datasets, we use MobileNetV2-based  DeepLabv3+ model. The R101, WR38, and MobileNetV2 backbones are ImageNet  pre-trained. ViT-B/16 backbone is pre-trained on ImageNet-21K  and fine-tuned on ImageNet-1k . We directly evaluate our size-target approach on top of the standard architectures without any modification.

Images are resized to \(512 512\) for PASCAL and COCO, and \(256 256\) for ACDC. We employ color jittering and horizontal flipping for data augmentation. Segmentation models are trained with stochastic gradient descent on one RTX A6000 GPU with 48 GB GDDR6: 60 epochs for PASCAL and COCO, and 200 epochs for ACDC, with a polynomial learning rate scheduler (power of 0.9). Batch sizes are set to 16 for ResNet and 20 for ViT models on PASCAL, 12 on ACDC, and 12 (ResNet) and 16 (ViT) for MS COCO. The initial learning rate is 0.005 for ACDC and PASCAL's ResNet models, and 0.0005 for PASCAL's ViT models. The initial learning rate on COCO is 0.0005 for ResNet and 0.0001 for ViT models. Loss function (12) is employed for size-target supervision. Loss (13) is only used for seed supervision in Sec. 3.3. The implementation of CRF loss (3) is the same as . We use \(2^{-9}\) as the weight of the CRF term following the strategy in . Size-target loss (2) and pCE (4) are used for medical images.

### Robustness to Size Errors

We show the size targets can be approximate. The left plot in Fig. 4 illustrates the robustness of our approach to size errors. Segmentation models are trained with synthetic size targets subjected to varying levels of corruption, as defined in (15). The validation accuracy (solid red line) only drops slightly when \(mRE\) (16) remains below 16%. The CRF loss (3) further enhances the robustness (solid blue line). When the relative error (\(mRE\)) is 4%, there is a noticeable increase in validation accuracy. The downward trend of the training accuracy (dashed blue line) suggests that the observed increases in validation accuracy at \(mRE=4\%\) stem from improved neural network generalization.

### Enhancing seed-based segmentation with size targets

Our size-target approach can be integrated with partial ground truth mask supervision (seeds). The right plot in Fig. 4 demonstrates the results of seed-supervised semantic segmentation with and without size-target supervision. Size targets significantly enhance performance, especially when the seed lengths are short. Without size targets, segmentation performance degrades dramatically as the seed length decreases. Notably, when only one pixel is labeled for each object (seed length ratio \(=0.0\)), size-target supervision boosts accuracy from 66.6% to 74%, approaching the performance of full seed supervision (seed length ratio = 1.0).

### Human-annotated size targets

**Annotation tool.** In this section, our approach is evaluated with size targets annotated by humans. We annotated training images for a subset of PASCAL classes, including cat, dog, and bird. A user interface with an assistance tool was developed to facilitate the annotation. The assistance tool overlays grid lines partitioning the image into \(5 4\) small rectangles or \(3 3\) large rectangles. Users can determine the size of a class in an image by counting rectangles (fractions allowed) or entering the percentage relative to the image size. Annotators can choose finer or coarser partitioning for each image depending on the object size. We evaluate relative errors with (14) for human annotations. Empirical evidence shows that annotators are approximately two times more accurate with the assistance tool, especially for small objects in the image. The last two columns of Table 1 report the annotation speed per image and mean relative error (14) for each class. The left plot in Fig. 5 shows the histograms of relative errors for human annotations. The histograms illustrate that annotated size errors are mostly below \(10\%\), but occasional large mistakes (heavy tails) raise the mean error.

**Segmentation with human-annotated size.** Segmentation models trained with human-annotated size targets show robustness to human "heavy tail" errors. We compare the accuracy for human-annotated and synthetic size targets in the right plot of Fig. 5. The accuracy for human-annotated size (indicated by the red star in the plot) approaches 97.2% (89.6%/92.2%) of the full supervision performance, demonstrating that size-target approach is significantly robust to human errors. Binary segmentation accuracy for each class is reported in the shaded cells in Table 1. The performance of

Figure 4: Segmentation results on the PASCAL dataset with R101-based DeeplabV3+ networks. The green bar in both plots indicates the segmentation accuracy for full ground truth masks (i.e. full supervision). The left plot shows the training and validation accuracy using approximate size targets. The segmentation is trained using losses (2) (red curve) or (12) (blue curve), where size targets are subject to various levels of corruption (15,16). The right plot shows validation accuracy for seed supervision of varying lengths with (blue curve) and without (red curve) using size targets. The line styles of the blue curves differentiate among various levels of corruption.

Figure 5: Left plot shows the quality of human annotations in terms of relative errors for the dog, cat, and bird classes within the PASCAI dataset. The histograms are normalized by the number of images in each class. The mean relative error for the three classes is 15.9%. For comparison, the dashed line shows the relative error distribution of synthetic size targets as defined in (15) for \(=20.0\%\) which aligns with the \(mRE\) of 15.9%, see (16). The right plot presents 4-way multi-class (cat, dog, bird, and background) segmentation accuracy using human-annotated (red star at \(mRE=15.9\%\)) and synthetic (blue curve) size targets, employing ResNet101-based DeeplabV3+ networks. Consistent with experiments in Sec. 3.2, synthetic size targets are generated at various levels of corruption. The green line indicates the segmentation accuracy of full supervision using ground truth masks.

binary segmentation models trained with human-annotated size targets is comparable to those trained with precise size targets.

### Comparison with the state-of-the-art methods

Our general training losses are applied to three standard architectures (R101-DeepLabV3+, WR38-DeepLabV3+, and ViT-Linear) for semantic segmentation as is, without any modification. Our results are highlighted in Table 2. The models are trained using synthetic size targets with an approximate mean relative error (mRE) of 8%. We chose this corruption level because its performance is close to human annotations, as shown in the right plot of Figure 5. Since our single-stage (end-to-end) approach is completely general, it is possible to use it in specialized architectures or complex training procedures. Likely, this would further improve the results, but this is not the focus of our work. The rest of Table 2 shows the results for semantic segmentation methods (of different complexities) for weak and full supervision. Methods are divided into multi-stage and single-stage methods, grouped by their backbones. Typical single-stage methods improve their results using complex architectural or training modifications such as additional training branches, extra refinement modules, or specialized training strategies. However, we achieve state-of-the-art using only standard segmentation architectures, commonly used in full supervision. The R101-based DeepLabV3+ model trained with approximate size targets approaches 92% (71.9/78.2) of its full supervision performance on PASCAL. The WR38-based DeepLabV3+ model trained with approximate size-target supervision surpasses other methods employing the same backbone by approximately 10%. Using the standard vision transformer architecture , the size-target approach achieves approximately 96% of the

   supervision & gt mask & gt size &  \\   & mIoU & mIoU & mIoU & speed & mRE \\  cat & 90.6\% & 88.8\% & 88.0\% & 12.6s & 12.3\% \\ dog & 88.1\% & 84.3\% & 84.5\% & 9.1s & 16.6\% \\ bird & 88.8\% & 86.2\% & 86.4\% & 15.2s & 20.1\% \\   

Table 1: Human-annotated size targets. Two columns on the right show the average speed and relative error for each class we annotated. The shaded cells compare the accuracy of binary segmentation models trained with ground truth masks, ground truth size, and human-annotated size.

   Backbone & Decoder & 
 Architectural/training \\ modification \\  & Supervision &  & COCO \\  & & & & Val & Test & Val \\    &  & ^{23}\)} &  &  &  &  \\  & & & & & \\  & & & MatLabel  \(\)iN\({}^{23}\) & & & 73.0 & 72.7 & 45.6 \\ WR38 & LargeFOV & MCT  \(\)iN\({}^{22}\) & tags & 71.9 & 71.6 & 42.0 \\ WR38 & LargeFOV & MCTOR  \(\)iN\({}^{23}\) & tags & 72.7 & 72.0 & 42.5 \\ SWIN & DeepLabV2 & ReCAM  \(\)iN\({}^{22}\) & tags & 71.8 & 72.2 & 47.9 \\ ViT-S & “Grad-clip” & WeakTr  \(\)iN\({}^{23}\) & tags & 78.4 & 79.0 & 50.3 \\   & & & \\  R101 & DeeplabV3+ & - & size (8\%) & 71.9 & 72.4 & 45.0 \\ R101 & DeeplabV3+ & - & full & 78.2 & 78.2 & 60.4 \\ WR38 & DeepLabV3+ & SSSS  \(\)iN\({}^{20}\) & tags & 62.7 & 64.3 & - \\ WR38 & Conv & RRM  \(\)iN\({}^{20}\) & tags & 62.6 & 62.9 & - \\ WR38 & DeeplabV3+ & & size (8\%) & 72.7 & 72.6 & - \\ ViT-B & LargeFOV & ToCo  \(\)iN\({}^{23}\) & tags & 71.1 & 72.2 & 42.3 \\ ViT-B & Conv & SeCo  \(\)iN\({}^{24}\) & tags & 74.0 & 73.8 & 46.7 \\ ViT-B & LargeFOV & CoSA  \(\)iN\({}^{24}\) & tags & 76.2 & 75.1 & 51.0 \\ ViT-B & Linear & - & size (8\%) & 78.1 & 78.2 & 56.3 \\ ViT-B & Linear & - & full & 81.4 & 80.7 & - \\   

Table 2: Semantic segmentation results (mIoU%) on PASCAL and COCO. The supervision column indicates a form of supervision: image-level class _tags_, _size_ targets (our highlighted results), or _full_ supervision with pixel-accurate masks. The percentage after “size” is the accuracy (mRE) of our corrupted size targets (15,16). Our approach does not require any complex architectural modification or multi-stage training procedures needed for tag supervision, see “Modification” column.

full supervision performance on the Pascal dataset. Despite its simplicity, the size-target approach outperforms other complex single-stage methods on both datasets.

### Medical data: size-target vs. size-barrier

Our method is also promising for medical image segmentation, benefiting from the consistency in object sizes across similar medical images, which healthcare professionals can easily estimate. We compare our size-target approach with the thresholded size-barrier technique , proposed for the weakly supervised medical image semantic segmentation. The size-barrier loss enforces inequality size constraints. Given the lower bound of each class, the thresholded size-barrier loss is

\[L_{flat\_sq}(S) = _{k}(\{a_{k}-^{k},0\})^{2}, \]

where \(a_{k}\) is a lower bound of class \(k\). We train binary segmentation models with a combination of partial cross-entropy loss (4) and size constraint loss: size-target (2) or size-barrier (17). Seeds used in the experiments are obtained using the same method provided in . The object and background barrier, \(a_{obj}\) and \(a_{bg}\) are set based on . In the size-barrier experiments, similarly to , we suppress the non-tag classes, using the loss \(L_{sup}(S)=(^{obj})^{2}\). Conversely, size-target loss automatically suppresses non-tag classes as discussed in Sec. 2. The left plot in Fig. 6 displays the segmentation accuracy against different levels of size target corruption. Our size-target loss consistently outperforms size-barrier loss, maintaining its superiority even when using highly noisy size targets. The peak in the accuracy curve aligns with the experimental results in Sec. 3.2 and Sec. 3.4. The accuracy of the model trained using size targets with relative errors of 8% surpasses the full supervision performance. Additionally, using a fixed average size target across all training images can yield performance comparable to the size-barrier method, see the dashed red line in the left plot of Fig. 6. The right image in Fig. 6 shows qualitative examples of both methods.

## 4 Conclusions

We proposed a new image-level supervision for semantic segmentation: size targets. Such targets could be approximate. In fact, our results suggest that some errors can benefit generalization. The size annotation by humans requires little extra effort compared to the standard image-level tags and it is much cheaper than the full pixel-accurate ground truth masks. We proposed an effective size-target loss based on forward KL divergence between the soft size targets and the average prediction. In combination with the standard CRF-based regularization loss, our approximate size-target supervision on standard segmentation architectures (DeepLab and ViT) achieves state-of-the-art performance. Our general easy-to-understand approach outperforms significantly more complex weakly-supervised techniques based on model modifications and multi-stage training procedures.

Figure 6: Size-targets (2) vs. size-barriers (17) on the ACDC dataset. The left plot shows the accuracy of the binary segmentation models (MobileNetV2-based DeeplabV3+) measured by DSC. The blue curve shows size-target accuracy with various levels of corruption. The dashed green line shows the accuracy of the size-barrier technique . The dashed red line shows the accuracy using the mean size target for all training images. The gray line indicates the result of full supervision. The right image shows randomly selected qualitative results of size-barrier  and approximate size target (\(mRE=8\%\)). Yellow shows true positive pixels, green is false positive, and red is false negatives.