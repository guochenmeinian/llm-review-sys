# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

###### Abstract

Vision-Language Models (VLMs) have demonstrated their broad effectiveness thanks to extensive training in aligning visual instructions to responses. However, such training of conclusive alignment leads models to ignore essential visual reasoning, further resulting in failures in meticulous visual problems and unfaithful responses. Drawing inspiration from human cognition in solving visual problems (_e.g., marking, zoom in_), this paper introduces **Chain of Manipulations**, a mechanism that enables VLMs to solve problems step-by-step with evidence. After training, models can solve various visual problems by eliciting intrinsic manipulations (_e.g., grounding, zoom in_) with results (_e.g., boxes, image_) actively without involving external tools, while also allowing users to trace error causes. We study the roadmap to implement this mechanism, including (1) a flexible design of manipulations upon extensive analysis, (2) an efficient automated data generation pipeline, (3) a compatible VLM architecture capable of multi-turn multi-image, and (4) a model training process for versatile capabilities. With the design, we also manually annotate 6K high-quality samples for the challenging graphical mathematical problems. Our trained model, **CogCoM**, equipped with this mechanism with 17B parameters achieves state-of-the-art performance across 9 benchmarks from 4 categories, demonstrating the effectiveness while preserving the interpretability. Our code, model weights, and collected data will be publicly available.

## 1 Introduction

Benefiting from the advantage of Large Language Models (LLMs) in broad world knowledge, large Vision Language Models (VLMs) (Alayrac et al., 2022; Wang et al., 2023b) that are further trained to understand visual inputs have demonstrated vibilities on broad multimodal scenarios, such as visual question answering (Liu et al., 2023b), visual grounding (Peng et al., 2023), optical character recognition (Zhang et al., 2023b). The research employing VLMs as foundation models (Bai et al., 2023; Sun et al., 2023b; Wang et al., 2023b) usually involves two main stages of training, where the first stage develops intrinsic visual understanding ability through exposure to massive image-caption pairs, and the second stage endows the models with problem-solving capabilities through the instruction tuning.

However, existing tuning methods train models to respond to instructions with conclusive language responses upon visual inputs, which leads models to ignore the essential intermediate visual reasoning and further results in failures in meticulous visual problems, unfaithful responses, and even hallucinations. For example in the left subplot of Figure 2, we test the top-performing model CogVLM (Wang et al., 2023b) about the details in the image (_i.e., texts written on a pillar_), and it directly responds an incorrect answer (_i.e., NO SMOKING_), most likely from bias to visual or linguistic priors (_i.e., typical scenes with a pillar in office_). The absence of the essential reasoning on the visual scene may lead to a rash response (Hwang et al., 2023).

Figure 2: In comparison with existing VLMs, CogCoM performs the multiple steps of evidential reasoning with chain of manipulations (CoM) to achieve the faithful answer to visual scene.

Humans solve problems regarding visual details by marking or processing the given images for convenience and rigor, which we refer to as manipulations. For example, we find targets by sequentially locating references, and concentrate on subtle details by zooming into a corresponding region. Most of VLMs have developed numerous intrinsic capabilities (_e.g.,_ grounding boxes, recognizing texts) during the first stage of training. By further imitating the fundamental human behaviours (_e.g.,_ cropping, zoom in), models have the potential to perform this cognitive reasoning process. Three major obstacles in eliciting VLMs with such reasoning are (1) flexible definitions of manipulations covering most visual problems, (2) an efficient data collection pipeline capable of producing abundant training data, and (3) a multi-turn multi-image VLM structure compatible with existing models.

Inspired by the human cognition in solving visual problems, we introduce **Chain of Manipulations (CoM)**, a mechanism that enables VLMs to solve problems step-by-step with evidence, with each step potentially involving a manipulation on the visual input and its corresponding result, both generated by the model to facilitate the success and fidelity. This paper studies a complete roadmap with manipulations design, data collection, model architecture and training process for training general VLMs with this mechanism. We first formally design 6 basic manipulations upon the pilot experiments, which are capable of handling diverse visual problems. Next, we propose a cascading data generation pipeline based on reliable large language models (_e.g.,_ LLMs, the linguistic annotators) and visual foundational models (_e.g.,_ VFMs, the visual annotators), which can automatically produce abundant error-free training data. We collect 70K CoM samples with this pipeline. We then devise a multi-turn multi-image model architecture compatible with typical VLMs structures. Based on a data recipe incorporating the curated corpus, we finally train a general VLM equipped with CoM reasoning mechanism, named CogCoM, which possesses capabilities of chat, captioning, grounding and reasoning. Additionally, benefiting from the expressive capability of the proposed mechanism, we further manually annotated 6K high-quality samples of graphical mathematical problems, each accompanied by a CoM reasoning process, to advance the research of VLMs in solving challenging mathematical problems.

We conduct extensive experiments on 9 benchmarks from 4 categories, including TextVQA (Singh et al., 2019), ST-VQA (Biten et al., 2019), TallyVQA (Acharya et al., 2019), and GQA Hudson & Manning (2019) for detailed visual question answering, RefCOCO (Yu et al., 2016), RefCOCO+(Yu et al., 2016), and RefCOCOg (Mao et al., 2016) for visual grounding, POPE (Li et al., 2023c) for hallucination validation, and MM-Vet (Yu et al., 2023b) for general multimodal ability. Our model achieves up to 9.0 and 1.09 accuracy improvement on the detailed VQA and grounding benchmarks, respectively, and the superior performance on the general multimodal benchmark. The results demonstrate the effectiveness of the mechanism while maintaining the interpretability of outputs.

## 2 Terminology

We first conduct pilot experiments to investigate the possible manipulations capable of handling diverse visual problems.

Specifically, given a question about an image, we prompt the advanced large language model, GPT-4, to generate solving steps by optionally utilizing possible actions on the image that facilitate problem-solving. We conduct this experiment on 170K questions from TextVQA, a dataset requiring detailed reasoning and recognition on images. To ensure the stability, we manually write 4 demonstrations as priors, The detailed statistics are available at Appendix C.3.

We utilize the StanfordCoreNLP toolkit to extract verb phrases referring to the actions, and the distribution of frequencies is shown in Figure 3. Through result analysis, we find that most of the actions can be mapped to 6 fundamental manipulations on images: _OCR_, _Grounding_, _CropZoomIn_, _Counting_, _Calculate_, and _Line_.

Figure 3: Distribution of the generated 465 actions base on GPT-4, mapped into 6 manipulations.

Based on the observation, we formally predefine a set of 6 manipulations, which can either be developed from pre-training or be learned from fine-tuning with the imitation to human behaviors: \(\){_OCR_(\(tgt\)) \(\)\(txt\), _Grounding_(\(tgt\)) \(\)\(bbx\), _Counting_(\(tgt\)) \(\)\(num\), _Calculate_(\(tgt\)) \(\)\(num\), _CropZoomIn_(\(bbx,x\)) \(\)\(img\), _Line_(\(pts\)) \(\)\(img\)}, where the parameters or results \(tgt,txt,bbx,num,x,img,pts\) refer to the bounding boxes, zoom ratio, image, target description, numbers, texts, and points, respectively. In addition to the predefined manipulations, we also allow trained models to create new manipulations during inference to facilitate problem-solving. We empirically find that more complicated goals can be derived from these fundamental manipulations.

We then define the **standard CoM data structure** to streamline the subsequent data construction and validation process. Given a question \(Q\) about an initial input image \(I_{0}\), a VLM equipped with chain of manipulations mechanism solves the problem to achieve final answer as _VLM_\((A,C|I_{0},Q)\), where \(\) refers to the reasoning chain with evidence,

\[&=(step_{1},step_{2},...)\\ step_{i}&=(f_{i},c_{i}),\ \ \ \ \ f_{i}  \]

where \(C=(c_{i},c_{2},...,c_{|C|})\) refers to the free-form textual descriptions incorporating manipulation names \(f_{i}\) and corresponding results from utilizing \(f_{i}\). This definition explicitly declares the symbolic execution process, while also being compatible with linguistic reasoning steps. Based on this definition, we can clearly construct standard CoM samples that incorporating the manipulation executions and linguistic steps with evidence. After the data construction, we can utilize a simple method to convert the standard CoM samples to the **compatible VQA samples**.

## 3 Data Collection

In this section, we first introduces the automated data generation pipeline (illustrated in Figure 4), that employs reliable LLMs as linguistic annotators and VFMs as the visual annotators to produce error-free CoM samples upon prevalent VQA corpus, and then present the manual annotation of high-quality CoM samples for the challenging graphical mathematical problems.

### Automated Data Generation

Given a general corpus \(=\{(I,Q,A)\}\) consisting of triplet samples of images with corresponding visual question-answer pairs, our automated data generation pipeline consists of a linguistic annotator and several visual annotators according to the manipulations. For a question \(Q\) in each sample, we first engage the linguistic annotator to generate manipulations-assisted solving steps with the CoM format \((f_{i},c_{i})\), where the corresponding results of the instantiated manipulation executions are set with variables as placeholders. In this paper, we adopt GPT-4 (OpenAI, 2023a), a large language

Figure 4: A cascading data generation pipeline that automatically produces standard CoM samples. Given an original VQA sample, the linguistic annotator (LLMs) taught with usage of manipulations (prompt) is first asked to provide solving steps for the question \(\), and the visual foundational models (VFMs) are then engaged to replace the manipulations results, followed by a final traversal on the tree branched by the possible manipulation results to find positive paths terminating to the answer \(\).

model with reliable language understanding and generation abilities as the linguistic annotator. We design a comprehensive prompt including the task requirements, usage of manipulations, and output data format, and further manually annotate 5 demonstrations for a stable generation. The detailed implementations are available at Appendix C.4.

We then employ essential visual annotators to supply the results of manipulations requested in the solving steps by exactly performing the corresponding manipulations. By empirically analyzing the manipulations from both predefined set and newly created ones (refers to Appendix C.3 for a detailed statistics), we reveal the _Grounding_ and _OCR_ are two fundamental manipulations, and most of the others can be consequently derived (_e.g., CropZoomln_ along a region of box, _Counting_ upon recognized boxes, and _Calculate_ for the recognized formula). Therefore, we employ two visual foundational models, GroundingDINO (Liu et al., 2023c) and PaddleOCR (Du et al., 2020), and develop the implementations of these manipulations1. The execution of the manipulations will transform the sequential reasoning steps into a **tree**\(\), as the input of current manipulation \(f_{1}(x_{a})\) may rely on one of the multiple results of previous manipulation \(f_{2}(x_{b},x_{c})\), _i.e._, \(x_{a}\) rely on \(x_{b}\) (_e.g.,_ step 2 for finding pillars in Figure 5). We then perform a traversal on each produced tree with Depth First Search (DFS) to find all positive paths \(\{_{i}|_{i},i=1,2,...\}\) that can terminate with the final answer \(A\) from the result of the last manipulation. Based on this method, the generated CoM samples with positive paths are guaranteed to be error-free. We implement this pipeline on 3 existing datasets that require detailed recognition or objects counting, TextVQA (Singh et al., 2019), ST-VQA (Biten et al., 2019), and TDIUC (Shrestha et al., 2019), to build 70K CoM samples 2. The designed prompt, a generated example with linguistic and visual results, and detailed algorithm illustration are available at AppendixC.1.

### Human Annotation

The analysis from Fig.1 of AlphaGeometry (Trinh et al., 2024) shows that outputting auxiliary lines in linguistic reasoning process helps LLMs to solve complex geometry problems. Benefiting from the expressive capability of CoM structure, we have also manually annotated high-quality CoM samples for the graphical mathematical problems to facilitate VLMs in solving this challenging scenario. Similar to the automated pipeline, we engage 10 human experts as the linguistic annotators and visual annotators, where each expert is asked to annotate the linguistic solving steps and the use of manipulations, as well as the results of manipulations on images. We perform this annotation on the MathVista (Lu et al., 2023) and ChartQA (Masry et al., 2022), which include geometric and chart math problems, resulting in the collection of 6K high-quality CoM math samples.

Finally, we adapt the CoM samples to be compatible with VQA-style training samples. For each CoM sample including \(n\) images from manipulations outputs \((I_{0},Q,C_{0},I_{1},C_{1},...,I_{n},A)\), we convert it into a multi-turn VQA sample segmented by the images \([(I_{0},Q,C_{0}),(I_{1},,C_{1}),...,(I_{n},,A)]\), where \(C_{i}\) represents the intermediate steps between \(I_{i}\) and \(I_{i+1}\), and \(\) is a simple prompt asking model to answer question based on history. This transformation converts CoM samples into multi-turn VQA samples that are compatible with existing VLMs training data. The detailed statistics of the data generation are available at Appendix C.3.

## 4 Model Training

### Architecture

We use the same model architecture as CogVLM (Wang et al., 2023b), a general VLM approach that involves four fundamental components: (1) a Visual Encoder, (2) an MLP Adapter, (3) an LLM Backbone, and (4) a Visual Expert Module, for a reliable multimodal understanding. Concretely, the pre-trained EVA2-CLIP-E (Sun et al., 2023a) with 4B parameters and Vicuna-7B-v1.5 (Chiang et al., 2023) are adopted as the visual encoder and LLM backbone, respectively. A two-layer MLP (SwiGLU (Shazeer, 2020)) is further engaged to map the output of the visual encoder into the linguistic space of the LLM backbone. The visual expert module adds the vision-specific weights into the attention layer and feed-forward layer of each block in the LLM backbone, resulting in a total of 6.5B additional parameters for the deep fusion of modalities.

Based on this general architecture, we develop a memory-based multi-turn multi-image VLM approach. Specifically, for a multi-turn VQA sample \([(I_{t},Q_{t},A_{t})|t=1,2,...]\), where \(A_{t}\) refers to \(C_{t}\) in CoM, we keep the accumulated KV memories of each layer in the LLM backbone throughout these turns. And at each turn \(t\) in training and inference, we calculate the attention function \(att\) as:

\[att() =softmax(_{t}_{t}^{T}}{})_{t}^{} \] \[_{t}^{} =((_{0},_{1},...,_{t}))\] \[_{t}^{} =((_{0},_{1},...,_{t}))\]

where \(_{t}^{s d}\) is query representation of current layer, and the \(_{t}^{},_{t}^{}^{(s t) d}\) refer to the concatenation of accumulated representations and will be further truncated if the sequence length \(s t\) is greater than a predefined threshold. At \(t>0\), the new image \(I_{t}\) will be cropped from \(I_{t-1}\) and amplified with the Bicubic Interpolation (Keys, 1981).

### Training

The proposed CogCoM-17B relies on two main stages of training, to develop the capabilities of general multimodal task-solving as well as the visual reasoning.

First Stage Pre-TrainingThis stage consists of two ordinal sub-phases of training for foundational visual understanding and grounded generation. Following the pre-training of CogVLM (Wang et al., 2023b), we first train model on 1.5B image-text pairs cleaned from the LAION-2B (Schuhmann et al., 2022) and COYO-700M (Byeon et al., 2022) with 120,000 iterations and batch size of 8,192. We then train model on 40M grounded image-question-answer triples cleaned from LAION-115M (Li et al., 2023b) with 60,000 iterations and batch size of 1,024, where each noun phrase in the answer is followed by a list of coordinates \([[x_{0},y_{0},x_{1},y_{1}],...]\)3 referring the phrase to the grounded objects in the image. Both phases adopt the next token prediction objective, and train the 6.5B parameters of visual experts.

Second Stage AlignmentThis stage further trains the model to align with human preferences on solving practical visual problems. We fuse the produced CoM data with 3 types of corpus, including Multilnstruct (Xu et al., 2022), LLaVAR (Zhang et al., 2023b), and ShareGPT4V (Chen et al., 2023c), referring the abilities of instruction-following, texts-recognizing, and detailed-captioning. This fusion results in a total of 570K \((I,Q,A)\) samples, where the answer \(A\) in CoM data consists of multiple turns. For the training data of CoM, we randomly prepend a lunching prompt4\(P^{}\) to questions \(Q=P^{}+Q\) asking models to optionally use manipulations for the adaption of explicitly eliciting. We empirically show that the model can effectively learn the evidential visual reasoning by ingesting this portion of CoM data. We train model with 14,000 iterations and a batch size of 160, where the learning rate reaches \(10^{-5}\) after 280 steps of warm-up and then decays linearly. The parameters of 6.5B visual experts are trained with the objective of next token prediction. These two stages of training result in our standard version of CogCoM involving both chat and reasoning capabilities. More training details are available at Appendix D.2.

Figure 5: **Left**: A compatible VLM architecture capable of multi-turn multi-image understanding. **Right**: An effective training process to develop a general VLM with versatile capabilities.

Experiment

To quantitatively validate the suitability and efficiency of the proposed method, we conduct experiments on 9 benchmarks corresponding to 4 categories of multimodal capabilities, as well as on a newly constructed testbed that includes the evidential reasoning paths with a keypoints-aware metric. Following previous works, we train two generalist versions of CogCoM for adapting to the different scenarios of Visual Question Answering and Visual Grounding, and evaluate the standard version with a qualitative analysis (Hwang et al., 2023). We also evaluate the time complexity.

* **Detailed Visual Question Answering.** This task involves models to perform detailed reasoning or recognition on images. We use 4 prominent benchmarks including, GQA (Hudson & Manning, 2019), TextVQA (Singh et al., 2019), ST-VQA (Biten et al., 2019), and TallyVQA (Acharya et al., 2019).
* **Visual Grounding.** Visual grounding evaluates the crucial abilities of VLMs on meticulous position understanding. We evaluate our model on 3 standard benchmarks, RefCOCO (Yu et al., 2016), RefCOCO+ (Yu et al., 2016), and RefCOCOg (Mao et al., 2016).
* **General Multimodal Capabilities & Hallucination.** We also evaluate on a general multimodal benchmark, MM-Vet (Yu et al., 2023b), and a hallucination detection benchmark POPE (Li et al., 2023c), to investigate the helpfulness of visual reasoning.

### Experiments on Detailed VQA

VLMs have demonstrated the well-known superiority in visual scenes with salient content understanding. We evaluate the effectiveness of CogCoM on VQAs on detailed understanding, which typically require models to perform multiple actions (_find, read_) or multiple reasoning steps (_recognizing and then calculating_). Following previous studies (Wang et al., 2023b), we train our model obtained from the first-phase of stage-1 on a mixture of data, including an instruction corpus of MultiInstruct, 13 publicly available VQA datasets (only using training set), a newly created VQA dataset built through promoting GPT-4V (OpenAI, 2023b) for image-oriented question-answer generation, and the automatically generated 70K CoM corpus. This training results in a generalist VQA model incorporating CoM reasoning. For all existing VQA tasks, we directly prompt CogCoM with given questions and examine the correctness of outputted answers.

#### 5.1.1 Gqa, TextVQA, ST-VQA, TallyVQA

SettingsGQA is a compositional VQA benchmark with diverse reasoning questions coming from semantic functional programs. TallyVQA is an objects counting benchmark with human-annotated complex counting questions involving challenging non-zero counterparts. TextVQA and ST-VQA are two texts understanding benchmarks requiring models to answer questions through textual cues on images. We use the official evaluation scripts for GQA and TallyVQA, which calculate the accuracy score by the Exact Matching (EM) between model predictions and answers. For TextVQA and ST-VQA, we submit our model predictions to the official online websites for calculating the accuracy with VQA Score metric (Antol et al., 2015).

    &  & **GQA** &  &  &  \\  & & test-balanced & simple & complex & test & test \\   & Flamingo (Alayrac et al., 2022) & - & - & - & 54.1 & - \\  & GIT (Wang et al., 2022a) & - & - & - & 59.8 & - \\  & GIZ (Wang et al., 2022a) & - & - & - & 67.3 & - \\  & BLIP-2 (Li et al., 2023b) & 44.7† & - & - & - & 21.7 \\  & InstructBLIP (Dai et al., 2023) & 49.5† & - & - & - & 50.7† \\  & Qwen-VL (Bai et al., 2023) & 59.3 & - & - & 63.8 & - \\  & CogVLM (Wang et al., 2023b) & 65.2 & 79.8 & 68.0 & 69.7 & 61.0 \\  & **CogCoM** & **71.7** & **84.0** & **70.1** & **71.1** & **70.0** \\    Specialist \\ SOTAs \\  } & 72.1 & 86.0 & 75.6 & 71.4 & 86.0 \\  & (CFR) & (Pal1-X) & (Pal1-X) & (Pal1-X) & (SMoLa) \\   

Table 1: Performance on Visual Question Answering benchmarks, where the results labeled with † refer to the few-shot setting. CogCoM achieves SOTA across the board, and demonstrates the effectiveness on the visual reasoning and scene texts recognition benchmarks.

ResultsAs the results shown in Table 2, CogCoM achieves the state-of-the-art performance in comparison with all generalist models, and achieves significant improvements over the baseline model. Specifically, compared to the baseline model, our model achieves up to 5.97 and 9.0 percentage points improvement on the benchmarks that requires complex reasoning and detailed recognition, respectively. On GQA and TextVQA, CogCoM also obtains comparable results with the large-scale specialist SOTAs. This result demonstrates the effectiveness of the proposed approach in solving details recognition problem.

#### 5.1.2 Experiments for Reasoning Accuracy and Time Complexity

Due to the lack of resource, we build CoM-test, a benchmark with evidential reasoning chains on the TextVQA test set based on the proposed data generation pipeline, and also introduce a keypoints-aware metric to validate the correctness of reasoning paths (see Appendix C.3 for detailed statistics). We also evaluate the time complexity for model generation on a held-out benchmark, MM-Vet.

Reasoning AccuracyTo validate the correctness of execution and results of manipulations in reasoning paths, we introduce a keypoints-aware evaluation metric that concentrates on these contents and their order. Concretely, given a predicted chain-answer pair \((C^{},A^{})\) and the ground truth pair \((C,A)\), we first extract the keypoints (_i.e.,_ the name, parameters, and results of manipulations) in \(A^{},A\) to form two lists, and then discretize these two lists into \(K^{}\) and \(K\) based on a bag-of-words composed of all keypoints. Then, we calculate the normalized Levenshtein Distance \(s_{K}=Levenshtein(K^{},K)/N\) as the manipulation score. We also compute the BLEU (Papineni et al., 2002) score \(s_{C}=(C^{},C)\) as the paragraph score. Finally, a weighted average of these two scores serves as the ultimate reasoning score s \(acc=(0.6 s_{K}+0.4 s_{C})/2\).

We train our first-stage model only using the 70K automated CoM data without other supervision for qualitatively evaluate the effectiveness of chains, and the results are shown in the left subplot of Figure 6. We find that by training with the CoM chains, our model can swiftly achieve the satisfactory performance of 48.41 accuracy score with 2k training steps, and obtain the optimal result of 55.59 with 8K steps. Additionally, the explanation scores gradually improve along with the model performance, indicating that successful reasoning steps contribute to the achieving of final answer.

Time ComplexityWe also evaluate the time complexity and average length of tokens during model reasoning on a held-out test set, MM-Vet. Specifically, we run CogCoM and the baseline model on all 218 questions, and record the time overhead as well as the average number of outputted tokens (using the Vicuna-7B-v1.5 tokenizer). We divide the 218 samples into 8 intervals based on the time expenditure for each sample and calculate the average values of the time complexity and the number of tokens for each interval, with the results presented in the right subplot of Figure 6.

From the results we find that compared to baseline model, CogCoM produces information-intensive reasoning content (_e.g.,_ detection boxes, auxiliary lines) without incurring infeasible time overhead. For example, without quantitive optimization, CogCoM outputs 262.9 informative tokens in approximately 9 seconds. With the advantages in long-context optimization techniques (Hooper et al., 2024), we believe that it is crucial for models to produce informative content and accurate responses.

Figure 6: **Left**: Results on a reasoning testbed CoM-test shows CogCoM achieves satisfactory performance with only 70K training data and 2K steps. **Right**: Results on MM-Vet shows CogCoM produces comprehensive reasoning content without incurring excessive time overhead.

### Experiments on Visual Grounding

The task of visual grounding requires models to precisely provide the corresponding coordinates of regions in an image based on the given target description. Following the existing work (Wang et al., 2023), we train our model obtained by the first stage on a mixture of datasets, including an instruction corpus MultiInstruct, a high-quality grounded VQA corpus introduced in CogVLM, and the 70K CoM data. This training results in a generalist grounding model that is excelling at visual grounding while capable of reasoning. For all benchmarks, we prompt CogOM in a chat manner to ask the model to provide grounded coordinates, such as "_Where is (expr) answer in [x0,y0,x1,y1] format._", where the (_expr_) refers to the target expression. We use the standard metric, that considers a prediction as correct when the intersection-over-union (IoU) between boxes is greater than 0.5.

ResultsAs shown in Figure 2, CogCoM achieves the best performance in 6 out of all 8 sub-sets. Based on the training with a mixture of broad capabilities, this result indicates that our model exhibits a superior grounding abilities while offers potential to solve a variety of tasks.

### Experiments on General Multimodal Evaluation and Hallucination Examination

We further examine the general multimodal capabilities, and the hallucination issue. We use the generalist VQA model and obtain model predictions by directly asking the original questions in benchmarks. We use the challenging adversarial version and official evaluation scripts for POPE.

ResultsAs shown in Table 3, we can see that CogCoM improves the performance by 0.6 points compared to the baseline model on MM-Vet, and achieves the superior performance on POPE which is in consistent with the baseline model. This result suggests that out model maintains superior reasoning capabilities while preserving effectiveness in general multimodal tasks, and simultaneously exhibits lower hallucination.

## 6 Conclusion

This paper studies the problems presented by the conclusive alignment training of VLMs, and proposes a mechanism, Chain of Manipulations (CoM), that enables VLMs to solve problems step-by-step by actively manipulating visual inputs as evidence. We realize this methodology by proposing (1) a flexible data structure, (2) an efficient data generation framework capable of producing abundant samples, (3) a memory-based architecture compatible with existing VLMs, and (4) a training process for versatile capabilities. We also annotate 6K graphical math samples with reasoning chains to facilitate the advancement of VLMs in solving mathematical problems. Experiments on 9 public benchmarks show that our trained 17B general VLM can produce informative reasoning content while achieving superior performance on diverse multimodal problems.

    &  &  &  &  \\   & & val & test-A & test-B & val & test-A & test-B & val & test \\   & OFA-1* (Wang et al., 2022) & 79.96 & 83.67 & 76.39 & 68.29 & 76.00 & 61.75 & 67.57 & 67.58 \\  & Shikra-7B (Chen et al., 2023) & 87.01 & 90.61 & 80.24 & 81.60 & 87.36 & 72.12 & 82.27 & 82.19 \\  & Shikra-13B (Chen et al., 2023) & 87.83 & 91.11 & 81.81 & 82.89 & 87.79 & 74.41 & 82.64 & 83.16 \\  & Qwen-VL (Bai et al., 2023) & 89.36 & 92.26 & 85.34 & 83.12 & 88.25 & 77.21 & 85.58 & 85.48 \\  & CogVLM (Wang et al., 2023) & **92.51** & 93.95 & 88.73 & 87.52 & 91.81 & 81.43 & **89.46** & 90.09 \\  & **CogCoM** & 92.34 & **94.57** & **89.15** & **88.19** & **92.80** & **82.08** & 89.32 & **90.45** \\  Specialist & & 92.64 & 94.33 & 91.46 & 88.77 & 92.21 & 83.23 & 89.22 & 89.37 \\  & & & & & & & & & \\   

Table 2: Results on VG benchmarks, where the specialist SOTAs are quoted from (Bai et al., 2023).

  
**Method** & **LLM** & **MM-Vet** & **POPE\({}_{adv}\)** \\  InstructBLIP (Dai et al., 2023) & Vicuna-13B & 25.6 & 77.3 \\ LLaVA (Liu et al., 2023) & LLaMA2-7B & 28.1 & 66.3 \\ DreamLLM (Dong et al., 2023) & Vicuna-7B & 35.9 & 76.5 \\ LLaVA-1.5 (Liu et al., 2023) & Vicuna-13B & 36.3 & 84.5 \\ CogVLM (Wang et al., 2023) & Vicuna-7B & 45.5† & 87.2 \\
**CogCoM** & Vicuna-7B & **46.1** & **87.8** \\   

Table 3: Evaluation results on the general and hallucination assessment benchmarks.