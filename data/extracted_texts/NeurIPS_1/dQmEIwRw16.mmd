# Collision Cross-entropy for Soft Class Labels

and Entropy-based Clustering

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We propose "collision cross-entropy" as a robust alternative to Shannon's cross-entropy (CE) loss when class labels are represented by soft categorical distributions y. In general, soft labels can naturally represent ambiguous targets in classification. They are particularly relevant for self-labeled clustering methods, where latent pseudo-labels \(y\) are jointly estimated with the model parameters and uncertainty is prevalent. In case of soft labels \(y\), Shannon's CE teaches the model predictions \(\) to reproduce the uncertainty in each training example, which inhibits the model's ability to learn and generalize from these examples. As an alternative loss, we propose the negative log of "collision probability" that maximizes the chance of equality between two random variables, predicted class and unknown true class, whose distributions are \(\) and \(y\). We show that it has the properties of a generalized CE. The proposed collision CE agrees with Shannon's CE for one-hot labels \(y\), but the training from soft labels differs. For example, unlike Shannon's CE, data points where \(y\) is a uniform distribution have zero contribution to the training. Collision CE significantly improves classification supervised by soft uncertain targets. Unlike Shannon's, collision CE is symmetric for \(y\) and \(\), which is particularly relevant when both distributions are estimated in the context of self-labeled clustering. Focusing on discriminative deep clustering where self-labeling and entropy-based losses are dominant, we show that the use of collision CE improves the state-of-the-art. We also derive an efficient EM algorithm that significantly speeds up the pseudo-label estimation with collision CE.

## 1 Introduction and Motivation

Shannon's cross-entropy \(H(y,)\) is the most common loss for training network predictions \(\) from ground truth labels \(y\) in the context of classification, semantic segmentation, etc. However, this loss may not be ideal for applications where the targets \(y\) are soft distributions representing various forms of uncertainty. For example, this paper is focused on self-labeled classification [17; 1; 15; 16] where the ground truth is not available and the network training is done jointly with estimating latent _pseudo-labels_\(y\). In this case soft \(y\) can represent the distribution of label uncertainty. Similar uncertainty of class labels is also natural for supervised problems where the ground truth has errors [26; 41]. In any cases of label uncertainty, if soft distribution \(y\) is used as a target in \(H(y,)\), the network is trained to reproduce the uncertainty, see the dashed curves in Fig.1.

Our work is inspired by generalized entropy measures [33; 18]. Besides mathematical generality, the need for such measures _"stems from practical aspects when modelling real world phenomena though entropy optimization algorithms"_. Similarly to \(L_{p}\) norms, parametric families of generalized entropy measures offer a wide spectrum of options. The Shannon's entropy is just one of them. Other measures could be more "nataul" for any given problem.

A simple experiment in Figure 2 shows that Shannon's cross-entropy produces deficient solutions for soft labels \(y\) compared to the proposed _collision cross-entropy_. The limitation of the standard cross-entropy is that it encourages the distributions \(\) and \(y\) to be equal, see the dashed curves in Fig.1. For example, the model predictions \(\) are trained to copy the uncertainty of the label distribution \(y\), even when \(y\) is an uninformative uniform distribution. In contrast, our collision cross-entropy (the solid curves) gradually weakens the training as \(y\) gets less certain. This numerical property of our cross-entropy follows from its definition (9) - it maximizes the probability of "collision", which is an event when two random variables sampled from the distributions \(\) and \(y\) are equal. This means that the predicted class value is equal to the latent label. This is significantly different from the \(=y\) encouraged by the Shannon's cross-entropy. For example, if \(y\) is uniform then it does not matter what the model predicts as the probability of collision \(\) would not change.

**Organization of the paper:** After the summary of our contributions below, Section 2 reviews the relevant background on self-labeling models/losses and generalized information measures for entropy, divergence, and cross-entropy. Then, Section 3 introduces our _collision cross entropy_ measure, discusses its properties, related formulations of Renyi cross-entropy, and relation to noisy labels in fully-supervised settings. Section 4 formulates our self-labeling loss by replacing the Shannon's cross entropy term in a representative state-of-the-art formulation using soft pseudo-labels  with our collision-cross-entropy. The obtained loss function is convex w.r.t. pseudo-labels \(y\), which makes estimation of \(y\) amenable to generic projected gradient descent. However, Section 4 derives a much faster EM algorithm for estimating \(y\). As common for self-labeling, optimization of the total loss w.r.t. network parameters is done via backpropagation. Section 5 presents our experiments, followed by conclusions.

**Summary of Contributions:** We propose the _collision cross-entropy_ as an alternative to the standard Shannon's cross-entropy mainly in the context of self-labeled classification with soft pseudo-labels. The main practical advantage is its robustness to uncertainty in the labels, which could also be useful in other applications. The definition of our cross-entropy has an intuitive probabilistic interpretation that agrees with the numerical and empirical properties. Unlike the Shannon's cross-entropy, our formulation is symmetric w.r.t. predictions \(\) and pseudo-labels \(y\). This is a conceptual advantage since both \(\) and \(y\) are estimated/optimized distributions. Our cross-entropy allows efficient optimization of pseudo-labels by a proposed EM algorithm, that significantly accelerates a generic projected gradient descent. Our experiments show consistent improvement over multiple examples of unsupervised and semi-supervised clustering, and several standard network architectures.

## 2 Background Review

We study a new generalized cross-entropy measure in the context of deep clustering. The models are trained on unlabeled data, but applications with partially labeled data are also relevant. Self-labeled deep clustering is a popular area of research [5; 31]. More recently, the-state-of-the-art is achieved by discriminative clustering methods based on maximizing the mutual information between the input and the output of the deep model . There is a large group of relevant methods [22; 10; 15; 17; 1; 16] and we review the most important loss functions, all of which use standard information-theoretic measures such as Shannon's entropy. In the second part of this section, we overview the necessary mathematical background on the generalized entropy measures, which are central to our work.

Figure 1: Collision cross-entropy \(H_{2}(y,)\) in (9) for fixed soft labels \(y\) (red, green, and blue). Assuming binary classification, all possible predictions \(=(x,1-x)_{2}\) are represented by points \(x\) on the horizontal axis. For comparison, thin dashed curves show Shannon’s cross-entropy \(H(y,)\) in (8). Note that \(H\) converges to infinity at both endpoints of the interval. In contrast, \(H_{2}\) is bounded for any non-hot \(y\). Such boundedness suggests robustness to target errors represented by soft labels \(y\). Also, collision cross-entropy \(H_{2}\) gradually turns off the training (sets zero-gradients) as soft labels become highly uncertain (solid blue). In contrast, \(H(y,)\) trains the network to copy this uncertainty, e.g. observe the optimum \(\) for all dashed curves.

### Information-based Self-labeled Clustering

The work of Bridle, Heading, and MacKay from 1991  formulated _mutual information_ (MI) loss for unsupervised discriminative training of neural networks using probability-type outputs, e.g. _softmax_\(:^{K}^{K}\) mapping \(K\) logits \(l_{k}\) to a point in the probability simplex \(^{K}\). Such output \(=(_{1},,_{K})\) is often interpreted as a posterior over \(K\) classes, where \(_{k}=}}{_{i}}}\) is a scalar prediction for each class \(k\).

The unsupervised loss proposed in  trains the model predictions to keep as much information about the input as possible. They derived an estimate of MI as the difference between the average entropy of the output and the entropy of the average output

\[L_{mi} := -MI(c,X)\;-\;H() \]

where \(c\) is a random variable representing class prediction, \(X\) represents the input, and the averaging is done over all input samples \(\{X_{i}\}_{i=1}^{M}\), _i.e._ over \(M\) training examples. The derivation in  assumes that softmax represents the distribution \((c|X)\). However, since softmax is not a true posterior, the right hand side in (1) can be seen only as an MI loss. In any case, (1) has a clear discriminative interpretation that stands on its own: \(H()\) encourages "fair" predictions with a balanced support of all categories across the whole training data set, while \(\) encourages confident or "decisive" prediction at each data point implying that decision boundaries are away from the training examples . Generally, we call clustering losses for softmax models "information-based" if they use measures from the information theory, e.g. entropy.

Discriminative clustering loss (1) can be applied to deep or shallow models. For clarity, this paper distinguishes parameters \(\) of the _representation_ layers of the network computing features \(f_{}(X)^{N}\) for any input \(X\) and the linear classifier parameters \(\) of the output layer computing \(K\)-logit vector \(^{}f\) for any feature \(f^{N}\). The overall network model is defined as

\[(^{}f_{}(X)). \]

A special "shallow" case in (2) is a basic linear discriminator

\[(^{}X) \]

directly operating on low-level input features \(f=X\). Optimization of the loss (1) for the shallow model (3) is done only over linear classifier parameters \(\), but the deeper network model (2) is optimized over all network parameters \([,]\). Typically, this is done via gradient descent or backpropagation [35; 3].

Optimization of MI losses (1) during network training is mostly done with standard gradient descent or backpropagation [3; 22; 15]. However, due to the entropy term representing the decisiveness, such loss functions are non-convex and present challenges to the gradient descent. This motivates alternative formulations and optimization approaches. For example, it is common to incorporate into the loss auxiliary variables \(y\) representing _pseudo-labels_ for unlabeled data points \(X\) and to estimate them jointly with optimization of the network parameters [10; 1; 16]. Typically, such _self-labeling_ approaches to unsupervised network training iterate optimization of the loss over pseudo-labels and network parameters, similarly to the Lloyd's algorithm for \(K\)-means . While the network parameters are still optimized via gradient descent, the pseudo-labels can be optimized via more powerful algorithms.

Figure 2: Robustness to label uncertainty: collision cross-entropy (9) vs Shannon’s cross-entropy (8). The test uses ResNet-18 architecture on fully-supervised _Natural Scene_ dataset  where we corrupted some labels. The horizontal axis shows the percentage \(\) of training images where the correct ground truth labels were replaced by a random label. Both losses trained the model using soft target distributions \(=*u+(1-)*y\) representing the mixture of one-hot distribution \(y\) for the observed corrupt label and the uniform distribution \(u\), as recommended in . The vertical axis shows the test accuracy. Training with the collision cross-entropy is robust to much higher levels of label uncertainty. As discussed in the last part of Sec.3, in the context of classification supervised by hard noisy labels, collision CE with soft labels can be related to the forward correction methods .

For example, self-labeling in  uses the following constrained optimization problem with discrete pseudo-labels \(y\)

\[L_{ce}\ \ =\ \  s.t.\ \ \ y_{0,1}^{K}\ \ \ and\ \ \ =u \]

where \(_{0,1}^{K}\) are _one-hot_ distributions, _i.e._ corners of the probability simplex \(^{K}\). Training the network predictions \(\) is driven by the standard _cross entropy_ loss \(H(y,)\), which is convex assuming fixed (pseudo) labels \(y\). With respect to variables \(y\), the cross entropy is linear. Without the balancing constraint \(=u\), the optimal \(y\) corresponds to the hard \(()\). However, the balancing constraint converts this into an integer programming problem that can be solved approximately via _optimal transport_. The cross-entropy in (4) encourages the predictions \(\) to approximate one-hot pseudo-labels \(y\), which implies the decisiveness.

Self-labeling methods for unsupervised clustering can also use soft pseudo-labels \(y^{K}\) as target distributions in cross-entropy \(H(y,)\). In general, soft targets \(y\) are common in \(H(y,)\), e.g. in the context of noisy labels . Softened targets \(y\) can also assist network calibration  and improve generalization by reducing over-confidence . In the context of unsupervised clustering, cross-entropy \(H(y,)\) with soft pseudo-labels \(y\) approximates the decisiveness since it encourages \( y\) implying \(H(y,) H(y) H()\) where the latter is the first term in (1). Instead of the hard constraint \(=u\) used in (4), the soft fairness constraint can be represented by KL divergence \(KL(\,\|\,u)\), as in . In particular,  formulates the following self-labeled clustering loss

\[L_{ce+kl}\ \ \ =\ \ \ \ \ \ \ \ +\ \ KL(\,\|\,u) \]

encouraging decisiveness and fairness as discussed. Similarly to (4), the network parameters in loss (5) are trained by the standard cross-entropy term, but optimization over relaxed pseudo-labels \(y^{K}\) is relatively easy due to convexity. While there is no closed-form solution, the authors offer an efficient approximate solver for \(y\). Iterating steps that estimate pseudo-labels \(y\) and optimize the model parameters resembles the Lloyd's algorithm for K-means. The results in  also establish a formal relation between the loss (5) and the \(K\)-means objective.

### Generalized Entropy Measures

Below, we review relevant generalized formulations of the information-theoretic concepts: entropy, divergence, and cross-entropy. Renyi  introduced the _entropy of order \(>0\)_ for any probability distribution \(p\)

\[H_{}(p)\ :=\ _{k}p_{k}^{}(  1)\]

derived as the most general measure of uncertainty in \(p\) satisfying four intuitively evident postulates. The entropy measures the average information and the order parameter \(\) relates to the power of the corresponding mean statistic . The general formula above includes the Shannon's entropy

\[H(p)\ =\ -_{k}p_{k} p_{k}\]

as a special case when \( 1\). The quadratic or second-order Renyi entropy

\[H_{2}(p)\ :=\ -_{k}p_{k}^{2} \]

is also known as a _collision entropy_ since it is a negative log-likelihood of a "collision" or "rolling double" when two i.i.d. samples from distribution \(p\) have equal values.

Basic characterization postulates in  also lead to the general Renyi formulation of the _divergence_, also known as the _relative entropy_, of order \(>0\)

\[D_{}(p\,|\,q)\ :=\ _{k}p_{k}^{}\ q_{k}^{1 -}( 1)\]

defined for any pair of distributions \(p\) and \(q\). This reduces to the standard KL divergence when \( 1\)

\[D(p,q)=_{k}p_{k}}{q_{k}} \]and to the _Bhattacharyya distance_ for \(=\).

Optimization of entropy and divergence  is fundamental to many machine learning problems , including pattern classification and cluster analysis . However, the entropy-related terminology is often mixed-up. For example, when discussing the _cross-entropy minimization principle_ (MimxEnt), many of the references cited earlier in this paragraph define _cross-entropy_ using the expression for KL-divergence (7). Nowadays, it is standard to define the Shannon's cross-entropy as

\[H(p,q)\ =\ -_{k}p_{k} q_{k}. \]

One simple explanation for the confusion is that KL-divergence \(D(p,q)\) and cross-entropy \(H(p,q)\) as functions of \(q\) only differ by a constant if \(p\) is a fixed known target, which is often the case.

## 3 Collision Cross-Entropy

Minimizing divergence enforces proximity between two distributions, which may work as a loss for training model predictions \(\) with labels \(y\), for example, if \(y\) are ground truth one-hot labels. However, if \(y\) are pseudo-labels that are estimated jointly with \(\), proximity between \(y\) and \(\) is not a good criterion for the loss. For example, highly uncertain model predictions \(\) in combination with uniformly distributed pseudo-labels \(y\) correspond to the optimal zero divergence, but this is not a very useful result for self-labeling. Instead, all existing self-labeling losses for deep clustering minimize Shannon's cross-entropy (8) that reduces the divergence and uncertainty at the same time

\[H(y,)\ \ D(y,)+H(y).\]

The entropy term corresponds to the "decisiveness" constraint in unsupervised discriminative clustering . In general, it is recommended as a regularizer for unsupervised and semi-supervised network training  to encourage decision boundaries away from the data points implicitly increasing the decision margins.

We propose a new form of cross-entropy

\[H_{2}(p,q)\ :=\ -_{k}p_{k}\,q_{k} \]

that we call _collision cross-entropy_ since it extends the collision entropy in (6). Indeed, (9) is the negative log-probability of an event that two random variables with (different) distributions \(p\) and \(q\) are equal. When training softmax \(\) with pseudo-label distribution \(y\), the collision event is the exact equality of the predicted class and the pseudo-label, where these are interpreted as specific outcomes for random variables with distributions \(\) and \(y\). Note that the collision event, i.e. the equality of two random variables, has very little to do with the equality of distributions \(=y\). The collision may happen when \( y\), as long as \( y>0\). Vice versa, this event is not guaranteed even when \(=y\). It will happen _almost surely_ only if the two distributions are the same one-hot. However, if the distributions are both uniform, the collision probability is only \(1/K\).

As easy to check, the collision cross-entropy (9) can be equivalently represented as

\[H_{2}(p,q)\ \ - cos(p,q)\ +\ (p)+H_{2}(q)}{2}\]

where \(cos(p,q)\) is the cosine of the angle between \(p\) and \(q\) as vectors in \(^{K}\) and \(H_{2}\) is the collision entropy (6). The first term corresponds to a "distance" between the two distributions: it is non-negative, equals \(0\) iff \(p=q\), and \(- cos()\) is a convex function of an angle, which can be interpreted as a spherical metric. Thus, analogously to the Shannon's cross-entropy, \(H_{2}\) is the sum of divergence and entropy.

The formula (9) can be found as a definition of quadratic Renyi cross-entropy . However, we could not identify information-theoretic axioms characterizing a generalized cross-entropy. Renyi himself did not discuss the concept of cross-entropy in his seminal work . Also, two different formulations of "natural" and "shifted" Renyi cross-entropy of arbitrary order could be found in . In particular, the shifted version of order 2 agrees with our formulation of collision cross-entropy (9). However, lack of postulates or characterization for the cross-entropy, and the existence of multiple non-equivalent formulations did not give us the confidence to use the name Renyi. Instead,we use "collision" due to its clear intuitive interpretation of the loss (9). But, the term "cross-entropy" is used only informally.

The numerical and empirical properties of the collision cross-entropy (9) are sufficiently different from the Shannon s cross-entropy (8). Figure 1 illustrates \(H_{2}(y,)\) as a function of \(\) for different label distributions \(y\). For confident \(y\) it behaves the same way as the standard cross entropy \(H(y,)\), but softer low-confident labels \(y\) naturally have little influence on the training. In contrast, the standard cross entropy encourages prediction \(\) to be the exact copy of uncertainty in distribution \(y\). Self-labeling methods based on \(H(y,)\) often "prune out" uncertain pseudo-labels . Collision cross entropy \(H_{2}(y,)\) makes such heuristics redundant. We also demonstrate the "robustness to label uncertainty" on an example where the ground truth labels are corrupted by noise, see Fig.2. This artificial fully-supervised test is used only to compare the robustness of (9) and (8) in complete isolation from other terms in the self-labeled clustering losses, which are the focus of this work.

Due to the symmetry of the arguments in (9), such robustness of \(H_{2}(y,)\) also works the other way around. Indeed, self-labeling losses are often used for both training \(\) and estimating \(y\): the loss is iteratively optimized over predictions \(\) (i.e. model parameters responsible for it) and over pseudo-label distribution \(y\). Thus, it helps if \(y\) also demonstrates "robustness to prediction uncertainty".

**Soft labels vs noisy labels:** Our collision CE for soft labels, represented by distributions \(y\), can be related to loss functions used for supervised classification with _noisy labels_, which assume some observed hard target labels \(l\) that may not be true due to corruption or "noise". Instead of our probability of collision

\[(C=T)=_{k}(C=k,T=k)=_{k}_{k}y_{k} y^{}\]

between the predicted class \(C\) and unknown true class \(T\), whose distributions are prediction \(\) and soft target \(y\), they maximize the probability that a random variable \(L\) representing a corrupted target equals the observed value \(l\)

\[(L=l)=_{k}(L=l|T=k)(T=k)_{k}(L=l|T=k)\;^{k}\; \;Q_{l}\,\]

where the approximation uses the model predictions \(^{k}\) instead of true class probabilities \((T=k)\), which is a significant assumption. Vector \(Q_{l}\) is the \(l\)-th row of the _transition matrix_\(Q\), such that \(Q_{lk}=(L=l|T=k)\), that has to be obtained in addition to hard noisy labels \(l\).

Our approach maximizing the collision probability based on soft labels \(y\) is a generalization of the methods for hard noisy labels. Their transitional matrix \(Q\) can be interpreted as an operator for converting any hard label \(l\) into a soft label \(y=Q^{}_{l}=Q_{l}\). Then, the two methods are numerically equivalent, though our statistical motivation is significantly different. Moreover, our approach is more general since it applies to a wider set of problems where the class target \(T\) can be directly specified by a distribution, a soft label \(y\), representing the target uncertainty. For example, in fully supervised classification or segmentation the human annotator can directly indicate uncertainty (odds) for classes present in the image or at a specific pixel. In fact, class ambiguity is common in many data sets, though for efficiency, the annotators are typically forced to provide one hard label. Moreover, in the context of self-supervised clustering, it is natural to estimate pseudo-labels as soft distributions \(y\). Such methods directly benefit from our collision CE, as this paper shows.

## 4 Our Self-labeling Loss and EM

Based on prior work (5), we replace the standard cross-entropy with our collision cross-entropy to formulate our self-labeling loss as follows:

\[L_{CCE} := (y,)}+\,KL(\|u) \]

To optimize such loss, we iterate between two alternating steps for \(\) and \(y\). For \(\), we use the standard stochastic gradient descent algorithms. For \(y\), we use the projected gradient descent (PGD) . However, the speed of PGD is slow as shown in Table 1 especially when there are more classes. This motivates us to find more efficient algorithms for optimizing \(y\). To derive such an algorithm, we made a minor change to (10) by switching the order of variables in the divergence term:

\[L_{CCE+} := (y,)}+\,KL(u\|) \]Such change allows us to use the Jensen's inequality on the divergence term to derive an efficient EM algorithm while the quality of the self-labeled classification results is almost the same as shown in the Appendix D.

EM algorithm for optimizing \(y\)We derive the EM algorithm introducing latent variables, \(K\) distributions \(S^{k}^{M}\) representing normalized support for each cluster over \(M\) data points. We refer to each vector \(S^{k}\) as a _normalized cluster_\(k\). Note the difference with distributions represented by pseudo-labels \(y^{K}\) showing support for each class at a given data point. Since we explicitly use individual data points below, we will start to carefully index them by \(i\{1,,M\}\). Thus, we will use \(y_{i}^{K}\) and \(_{i}^{K}\). Individual components of distribution \(S^{k}^{M}\) corresponding to data point \(i\) will be denoted by scalar \(S^{k}_{i}\).

First, we expand (11) introducing the latent variables \(S^{k}^{M}\)

\[L_{CCE+} }{{=}} (y,)}+\,H(u,) \] \[= (y,)}-\,_{k}u^{k}_{i}S^{k }_{i}\,_{i}}{S^{k}_{i}M}\ \ (y,)}-\, _{k}_{i}u^{k}S^{k}_{i}_{i}}{S^{k}_{i}M} \]

Due to the convexity of negative \(\), we apply the Jensen's inequality to derive an upper bound, i.e. (13), to \(L_{CCE+}\). Such bound becomes tight when:

\[\,: S^{k}_{i}=_{i}}{_{j}y^{k}_{j}} \]

Next, we derive the M step. Introducing the hidden variable \(S\) breaks the fairness term into the sum of independent terms for pseudo-labels \(y_{i}_{K}\) at each data point \(i\). The solution focus on the loss with respect to \(y\). The collision cross-entropy (CCE) also breaks into the sum of independent parts for each \(y_{i}\). For simplicity, we will drop all indices \(i\) in variables \(y^{k}_{i}\), \(S^{k}_{i}\), \(^{k}_{i}\). Then, the combination of CCE loss with the corresponding part of the fairness constraint can be written for each \(y=\{y_{k}\}_{K}\) as

\[-_{k}_{k}y_{k}\ \ -\ \ _{k}u_{k}S_{k} y_{k}. \]

First, observe that this loss must achieve its global optimum in the interior of the simplex if \(S_{k}>0\) and \(u_{k}>0\) for all \(k\). Indeed, the second term enforces the "log-barier" at the boundary of the simplex. Thus, we do not need to worry about KKT conditions in this case. Note that \(S_{k}\) might be zero, in which case we need to consider the full KKT conditions. However, the Property 1 that will be mentioned later eliminates such concern if we use positive initialization. For completeness, we also give the detailed derivation for such case and it can be found in the Appendix B.

Adding the Lagrange multiplier \(\) for the simplex constraint, we get an unconstrained loss

\[-_{k}_{k}y_{k}\ \ -\ \ _{k}u_{k}S_{k} y_{k}\ \ +\ \ (_{k}y_{k}-1)\]

that must have a stationary point inside the simplex. The following theorem indicates the way to solve the problem above. All the missing proofs can be found in Appendix A.

**Theorem 1**.: **[M-step solution]:** _The sum \(_{k}y_{k}\) as in (16) is positive, continuous, convex, and monotonically decreasing function of \(x\) on the specified interval. Moreover, there exists a unique solution \(\{y_{k}\}_{k}\) and \(x\) such that_

\[_{k}y_{k}  _{k}\,S_{k}}{ u^{}S+1-}{x}}\ \ =\ \ 1\ \ \ \ \ \ x(}{1+ u^{}S}\,\ _{max}] \]

The monotonicity and convexity of \(_{k}y_{k}\) with respect to \(x\) suggest that the problem (16) formulated in Theorem 1 allows efficient algorithms for finding the corresponding unique solution. For example, one can use the iterative Newton's updates to search for \(x\) in the specified interval. The following Lemma gives us a proper starting point

**Lemma 1**.: _Assuming \(u_{k}S_{k}\) is positive for each \(k\), then the reachable left end point in Theorem 1 can be written as_

\[l:=_{k}}{1+ u^{}S- u_{k}S_{k}}.\]

for Newton's method. The algorithm for M-step solution is summarized in Algorithm 1 in Appendix C. Note that we present the algorithm for only one data point, and we can easily and efficiently scale up for more data in a batch by using the Numba compiler. In the following, we give the property about the positivity of the solution. This property implies that if our EM algorithm has only (strictly) positive variables \(S_{k}\) or \(y_{k}\) at initialization, these variables will remain positive during all iterations.

_Property 1_.: For any category \(k\) such that \(u_{k}>0\), the set of strictly positive variables \(y_{k}\) or \(S_{k}\) can only grow during iterations of our EM algorithm for the loss (15) based on the collision cross-entropy.

Note that Property 1 does not rule out the possibility that \(y_{k}\) may become arbitrarily close to zero during EM iterations. Empirically, we did not observe any numerical issues. The complete algorithm is given in Appendix C. Inspired by [39; 15], we also update our \(y\) in each batch. Intuitively, updating \(y\) on the fly can prevent the network from being easily trapped in some local minima created by the incorrect pseudo-labels.

## 5 Experiments

We apply our new loss to self-labeled classification problems in both shallow and deep settings, as well as semi-supervised modes. All the results are reproduced using either public codes or our own implementation under the same experimental settings for fair comparison. Our approach consistently achieves either the best or highly competitive results across all the datasets and is therefore more robust. All the missing details in the experiments can be found in Appendix E.

DatasetWe use four standard datasets: MNIST , CIFAR10/100  and STL10 . The training and test data are the same unless otherwise specified.

EvaluationAs for the evaluation of self-labeled classification, we set the number of clusters to the number of ground-truth categories. To calculate the accuracy, we use the standard Hungarian algorithm  to find the best one-to-one mapping between clusters and labels. We don't need this matching step if we use other metrics, i.e. NMI, ARI.

### Clustering with Fixed Features

In this section, we test our loss as a proper clustering loss and compare it to the widely used Kmeans (generative) and other closely related losses (entropy-based and discriminative). We use the pretrained (ImageNet) Resnet-50  to extract the features. For Kmeans, the model is parameterized by K cluster centers. Comparably, we use a one-layer linear classifier followed by softmax for all other losses including ours. Kmeans results were obtained using scikit-learn package in Python. To optimize the model parameters for other losses, we use stochastic gradient descent. Here we report the average accuracy and standard deviation over 6 randomly initialized trials in Table 2.

    & STL10 & CIFAR10 & CIFAR100-20 & MNIST \\  Kmeans & 85.299(5.59) & 67.788(4.6) & 42.999(1.3) & 47.625(2.1) \\ MCD  & 89.56(6.4) & 72.33(5.8) & 43.99(1.1) & 52.92(3.0) \\ Scl1  & 90.33(4.8) & 63.31(3.7) & 40.494(1.1) & 52.38(3.5) \\  (M2M10) & 86.89(2.07) & 60.939(3.03) & 41.245(3.0) & **50.198(1.3)** \\  Our. & **92.39(6.4)** & **72.81(6.3)** & **43.22(4.1)** & **58.49(3.2)** \\   

Table 2: Comparison of different methods on clustering with fixed features extracted from Resnet-50. The numbers are the average accuracy and the standard deviation over trials. We use the 20 coarse categories for CIFAR100 similarly to others.

### Deep Clustering

In this section, we train a deep network to jointly learn the features and cluster the data. We test our method on both a small architecture (VGG4) and a large one (ResNet-18). The only extra standard technique we add here is self-augmentation following [15; 1; 6].

To train the VGG4, we use random initialization for network parameters. From Table 3, it can be seen that our approach consistently achieves the most competitive results in terms of accuracy (ACC). Most of the our method) are general concepts applicable to single-stage end-to-end training. To be fair, we tested all of them on the same simple architecture. But, these general methods can be easily integrated into other more complex systems with larger architecture such as ResNet-18.

In Table 4, we show the results using the pretext-trained network from SCAN  as initialization for our clustering loss as well as IMSAT and MIADM. We use only the clustering loss together with the self-augmentation (one augmentation per image). As shown in the table below, our method reaches a higher number with more robustness almost for every metric on all datasets compared to the SOTA method SCAN. More importantly, we consistently improve over the most related method, MIADM, by a large margin, which clearly demonstrates the effectiveness of our proposed loss together with the optimization algorithm.

### Semi-supervised Classification

Although our paper is focused on self-labeled classification, we find it also interesting and natural to test our loss under semi-supervised settings where partial data is provided with ground-truth labels. We use the standard cross-entropy loss for labeled data and directly add it to the self-labeled loss to train the network initialized by the pretext-trained network following .

## 6 Conclusion

We propose a new collision cross-entropy loss. Such loss is naturally interpreted as measuring the probability of the equality between two random variables represented by the two distributions \(\) and \(y\), which perfectly fits the goal of self-labeled classification. It is symmetric w.r.t. the two distributions instead of treating one as the target, like the standard cross-entropy.

While the latter makes the network copy the uncertainty in estimated pseudo-labels, our cross-entropy naturally weakens the training on data points where pseudo labels are more uncertain. This makes our cross-entropy robust to labeling errors. In fact, the robustness works both for prediction and for pseudo-labels due to the symmetry. We also developed an efficient EM algorithm for optimizing the pseudo-labels. Such EM algorithm takes much less time compared to the standard projected gradient descent. Experimental results show that our method consistently produces top or near-top results on all tested clustering and semi-supervised benchmarks.

    & CIFAR10 & CIFAR10 & CIFAR100-20 & MNIST \\  IMSAT  & 25.88(0.5) & 21.49(0.5) & 14.39(0.7) & 92.90(0.3) \\ IIC(11) & 24.12(0.7) & 21.35(0.4) & 12.58(0.6) & 28.51(0.2) & 28.51(0.2) \\ SclA(1) & 29.90(0.9) & 21.46(1.5) & **15.34(0.3)** & 25.86(0.3) & 28.58(0.1) \\ MIADM  & 23.79(0.9) & 23.26(0.0) & 14.02(0.5) & 14.02(0.5) & 78.89(0.3) \\  
**Our** & **25.98(0.1)** & **24.26(0.6)** & **15.47(0.0)** & **95.11(0.4)** & **95.11(0.4)** & **95.11(0.4)** \\   

Table 3: Quantitative comparison of discriminative clustering-based classification methods with simultaneous feature training from the scratch. The network architecture is VGG-4. We reuse the code published by [17; 1; 15] and use our improved implementation of  (also for other tables).

    & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) \\    & **17.45** & **12.72** & **17.18** & **19.78** & **18.45** & **17.18** \\  & **18.17** & **18.12** & **19.51** & **18.21** & **18.72** & **17.14** & **18.22** \\  & **18.21** & **18.21** & **19.73** & **19.72** & **18.62** & **17.25** & **18.75** \\  & **18.41(0.1)** & **18.21** & **19.60** & **17.94** & **17.71** & **17.03** & **17.11**