# Conditional Density Estimation with Histogram Trees

Lincen Yang  Matthijs van Leeuwen

LIACS, Leiden University

Einsteinweg 55, 2333CC Leiden, The Netherlands

{l.yang, m.van.leeuwen}@liacs.leidenuniv.nl

###### Abstract

Conditional density estimation (CDE) goes beyond regression by modeling the full conditional distribution, providing a richer understanding of the data than just the conditional mean in regression. This makes CDE particularly useful in critical application domains. However, interpretable CDE methods are understudied. Current methods typically employ kernel-based approaches, using kernel functions directly for kernel density estimation or as basis functions in linear models. In contrast, despite their conceptual simplicity and visualization suitability, tree-based methods--which are arguably more comprehensible--have been largely overlooked for CDE tasks. Thus, we propose the Conditional Density Tree (CDTree), a fully non-parametric model consisting of a decision tree in which each leaf is formed by a histogram model. Specifically, we formalize the problem of learning a CDTree using the minimum description length (MDL) principle, which eliminates the need for tuning the hyperparameter for regularization. Next, we propose an iterative algorithm that, although greedily, searches the optimal histogram for every possible node split. Our experiments demonstrate that, in comparison to existing interpretable CDE methods, CDTrees are both more accurate (as measured by the log-loss) and more robust against irrelevant features. Further, our approach leads to smaller tree sizes than existing tree-based models, which benefits interpretability.

## 1 Introduction

Conditional density estimation (CDE) is a crucial yet challenging task in modeling the associations between features and a _continuous_ target variable, which has received a lot of research interest since the 1970s . By modeling the full conditional distribution, CDE is useful when the datasets are multi-modal, heavily skewed, or heteroscedastic. As a result, it is widely applied in various fields, including genomics , astronomy , wind power forecasting , and computer networks .

CDE provides richer information for data understanding than regression, which only models the conditional mean. This makes CDE desirable for well-informed decision-making in critical areas, such as healthcare , which calls for interpretability.

Despite its importance, recent CDE research has predominantly focused on black-box models, such as neural networks  and tree ensembles . In contrast, intrinsically interpretable models for CDE are understudied. Specifically, decision tree-based methods are largely neglected, with CADET  being the only existing method to the best of our knowledge. However, CADET's assumption of a Gaussian distribution for the target variable in each leaf node limits its ability to model complex conditional densities.

As a result, kernel-based models became the standard among'shallow' models for CDE, including methods based on kernel density estimation (KDE) , and linear models with the basis functions chosen as Gaussian kernels . Nevertheless, kernel-based models are arguably less interpretable than decision trees: as conditions in decision trees are directly readable, they are comprehensible tohumans without statistical expertise (e.g., individuals affected by data-driven decisions rather than professional data analysts).

To address these limitations, we propose the Conditional Density Tree (CDTree), a flexible tree-based CDE model in which each leaf node consists of a histogram model. For illustration, Figure 1 shows the CDTree learned from a dataset about personal medical costs with demographic features . The figure visualizes the histogram models for the conditional densities on three selected leaf nodes, along with the unconditional density. The readable rules, extracted from the tree paths, and the histogram visualizations, make the results easily comprehensible to humans without statistical expertise. For instance, the difference in medical costs between smokers and non-smokers is evident when comparing the plots. The full results with histograms on all leaf nodes, as well as further descriptions of the dataset, are provided in Appendix E.

Learning a CDTree from data is a challenging task, as it requires simultaneously optimizing the decision tree structure and the number of bins for histograms on all leaf nodes. While often used for either task, cross-validation is too time-consuming if we need to search for the optimal number of bins for every possible node split. Thus, we formalize the learning problem using the minimum description length (MDL) principle [16; 41], which we briefly review in Section 4.1. Adopting MDL eliminates the need for tuning the hyperparameter, by cross-validation, for both regularizing the decision trees  and for choosing the number of bins for histograms.

Our main contributions are as follows. First, we introduce the CDTree for the CDE task and formalize the learning problem with the MDL principle. Second, we propose an iterative algorithm that can search the optimal histogram for all possible node splits. Third, we benchmark against a wide range of competitors and demonstrate that CDTree is highly competitive. Specifically, CDTree is more accurate than existing interpretable CDE methods (as measured by the log-loss). Meanwhile, CDTree has smaller tree sizes than other tree-based methods, which benefits interpretability. In addition, CDTree is extremely robust to irrelevant features, which is noteworthy as irrelevant features are known to harm the convergence rate for CDE . Further, we argue that the (intrinsic) explanations of CDTree are trustworthy only if the CDTree is robust against irrelevant features.

## 2 Related Work

**KDE-based CDE.** Several CDE methods have been developed based on kernel density estimation (KDE). The most straightforward approach, known as conditional-KDE (CKDE) , involves separately estimating the joint and marginal densities using unconditional KDE and then taking their ratio to obtain the conditional density. Another approach, \(\)-neighborhood KDE (NKDE), estimates the conditional density by applying KDE to the subset of data points within the \(\)-neighborhood of the target point, with the range of the neighborhood controlled by the parameter \(\).

However, KDE-based methods have several limitations. First, KDE is arguably less interpretable than tree-based models, as understanding KDE requires statistical knowledge, making it less accessible to domain experts and the general public. Additionally, while decision trees can naturally handle both discrete and continuous feature values, discrete features pose significant challenges for kernel-based methods. Specifically, CKDE requires estimating the joint density of the features and target variable, which necessitates the use of discrete kernels for discrete variables. These discrete kernels are often difficult to interpret . As for NKDE, when both discrete and continuous feature values exist, the choice of the scale for the continuous variables unavoidably introduces a certain degree of arbitrariness in defining the distances used to characterize the neighborhood.

Figure 1: Three selected leaves from the CDTree modeling the conditional density of the medical costs given demographic features, together with the unconditional density for medical costs.

**Regression-based CDE.** Motivated by several issues of CKDE, including high variance as a plug-in estimator, the exponentially growing search space for bandwidth tuning, and the curse of dimensionality for estimating the joint density function, the method named least-squares CDE (LSCDE)  was proposed. LSCDE aims to directly estimate the ratio between the joint and marginal densities by assuming this ratio is a linear combination of several basis functions, chosen to be Gaussian kernels. Similarly, Fan et al.  proposed double-kernel local linear regression, which transforms the conditional density function into the conditional expectation of the (unconditional) density function of the target variable, leading to a least-squares approach as well. However, both methods face the challenge of bandwidth selection, as the value of the Gaussian kernel function must be calculated with the _entire_ feature vector as input. This issue becomes particularly problematic as the search space for the bandwidth grows exponentially with the number of features.

**Tree-based CDE.** The only existing CDE method based on single trees that we are aware of is CADET , which improves CART regression trees  by designing a node-splitting heuristic specifically for CDE. However, CADET assumes a Gaussian distribution for the target variable on each leaf, which is far less flexible than our non-parametric histogram models.

**Black-box CDE.** Neural networks have been shown to perform well in a wide range of tasks. For CDE, the most well-known methods include NF  and MDN . As for tree ensemble models, RFCDE  first fits a standard random forest, and then estimates the CDE by the weighted average of the unconditional density estimates obtained via KDE, with weights determined by the random forest. Furthermore, the recently proposed LinCDE  method learns a boosted tree model based on Lindsey's method . While black-box models are highly accurate, we argue that interpretable CDE methods are valuable for applications in critical domains and for understanding the data.

## 3 Conditional Density Tree with Histograms

Tree-based models divide the feature space into disjoint (hyper-)boxes by recursively splitting on individual feature variables (for which we consider binary splits only). Consequently, the induced partition with \(K\) subsets (leaves) can be represented as \(M=\{S_{k}\}_{k[K]}\), where each leaf \(S_{k}\) represents a subset of the feature space and \([K]:=\{1,...,K\}\). Further, each leaf is equipped with a single (unconditional) density estimator, denoted as \(f_{k}(.)\). Thus, for a dataset \(D=(x^{n},y^{n})\) with sample size \(n\), the tree-based model \(M\) first identifies the leaf to which each \((x,y) D\) belongs, and then estimates the conditional density \(f(y|x)\) as \(f_{k}(y)\) (assuming \(x S_{k}\)).

We choose histograms as our model class for each \(f_{k}(.)\). Unlike previous parametric models [13; 6], which carry the risk of misspecification, histograms are non-parametric yet efficient. Additionally, in comparison to KDE, histograms do not require selecting a kernel or tuning the bandwidth.

We next describe our notations and formally present the histogram as a probabilistic model. Formally, a (fixed) histogram model partitions the domain of the target variable \(Y\) into equal-width bins, and then approximates the density of \(Y\) by piece-wise constants estimated from data. Thus, we can denote a histogram model as a tuple \(H=(B_{l},B_{u},h)\), with \(B_{l}\) and \(B_{u}\) respectively representing the lower and upper boundary of the histogram, and \(h\) the number of bins. A histogram model \(H\) can parameterize a family of distributions by \(=(_{1},...,_{h})\), with the probability density function in the form of \(f_{H}(Y=y)=}{(B_{u}-B_{l})/h}\), \( y[B_{l}+(j-1)-B_{l}}{h},B_{l}+j-B_{l}}{h})\), in which \((B_{u}-B_{l})/h\) is the bin width and \([B_{l}+(j-1)-B_{l}}{h},B_{l}+j-B_{l}}{h})\) denotes the interval for the \(j\)th bin. In practice, the histogram boundaries \(B_{l}\) and \(B_{u}\) are often set based on the dataset at hand, by prior knowledge, or by the range of the values plus/minus a small constant. Meanwhile, the parameters \(\) can be estimated by the maximum likelihood estimator (i.e., by the empirical frequencies in each bin): \(_{j}=_{i=1}^{n}_{[B_{l}+(j-1)- B_{l}}{h},B_{l}+j-B_{l}}{h})}(x_{i})\), in which \((.)\) is the indicator function.

## 4 The MDL-optimal CDTree

We briefly review the minimum description length (MDL) principle, and then formalize the problem of learning a CDTree that consists of histograms as an MDL-based model selection problem.

### Preliminary: the MDL principle for model selection

Rooted in information theory, the minimum description length (MDL) principle states that the optimal model is the one that compresses the data most [41; 15]. Precisely, given the dataset \(D=(x^{n},y^{n})\), the MDL-optimal model is defined as

\[M^{*}=_{M}-_{2}P_{M}(y^{n}|x^{n})+L(M), \]

in which \(M\) is a CDTree and \(\) the model class of all possible CDTrees for our learning task, meanwhile \(L(M)\) is the code length (in bits) needed to transmit the model in a lossless manner. According to the Kraft's inequality , \(L(M)\) can also be regarded as a prior probability distribution defined on the model class. Further, \(P_{M}(.)\) is the so-called _universal distribution_: as \(M\) parameterizes a family of probability distributions, denoted as \(P_{M,}\), \(P_{M}(.)\) can be regarded as a "representative" distribution such that the _maximum regret_, defined as \(_{y^{n}}\{_{}_{2}P_{M,}(y^{n}|x^{n})-_{2}P_{M}(y^{ n}|x^{n})\}\), can be bounded by \(n>0,>0\) as \(n\), in which the maximum is defined over all possible values for \(y^{n}\) in the domain of the target variable. Intuitively, the _regret_ is the difference between the log-likelihood of \(P_{M}(y^{n}|x^{n})\) (which does not depend on \(\)) and the maximum log-likelihood \(_{}_{2}P_{M,}(y^{n}|x^{n})\). For our learning task, \(\) is the parameter vector that contains the histogram parameters (i.e., the \(\)'s) for all leaf nodes.

The MDL principle has been successfully applied to various data mining and machine learning tasks , and specifically to partition-based models, including histograms and classification trees/rules [37; 57; 36; 22]. The MDL framework provides a principled way of regularizing model complexity without any regularization hyperparameter to be tuned. Moreover, as the Bayesian marginal distribution is one specific type of the universal distributions, the MDL-based model selection can also be regarded as a generalization of Bayesian model selection .

### Normalized maximum likelihood for CDTrees

The optimal universal distribution under the MDL framework is the so-called _normalized maximum likelihood_ (NML) distribution, defined as [15; 49; 16]

\[P_{M}(y^{n}|x^{n})=P_{M,}(y^{n}|x^{n})}{_{y^{n} }_{}P_{M,}(y^{n}|x^{n})}, \]

under the condition that the denominator is finite. It can be shown that the NML distribution is the only distribution that leads to the minimax regret \(_{P_{M}}_{y^{n}}\{_{}_{2}P_{M,}(y^{n}|x^{n})- _{2}P_{M}(y^{n}|x^{n})\}\), in which the denominator in Eq. 2 is exactly equal to the regret .

The denominator in Eq. 2 is in general prohibitively expensive to compute [15; 51; 43], with a few exceptions including the cases when the probable model represents categorical distributions , decision rules for classification , and one- and multi-dimensional histogram models [22; 28; 58]. We extend these previous results and prove that, for CDTrees with histogram models, the denominator (regret) is finite and is equal to the products of the regret terms of the NML distributions for one-dimensional histogram models, as shown in Proposition 1. This result is useful for efficiently calculating the denominator (regret) in Eq. 2 for CDTree models, as the regret terms for histogram models are known to be equal to those of categorical distributions [20; 21], for which an efficient algorithm exists with sub-linear time complexity .

**Proposition 1**.: _Let \(=(^{1},...,^{K})\) be the histogram parameters for histograms on all leaves, and let \(=_{}P_{M,}(y^{n}|x^{n})\). Then \(_{y^{n}}_{}P_{M,}(y^{n}|x^{n})=_{k[K]} (N_{k},h_{k})\), in which \((N_{k},h_{k})\) is the regret (denominator) of the NML distribution of the histogram model on the \(k\)th leaf node that contains \(N_{k}\) data points and \(h_{k}\) bins._

We defer the proof to Appendix A due to space limitations.

### Code length for the model

To encode a CDTree model in a lossless manner, we need to sequentially encode 1) the number of nodes in the decision tree, 2) the structure of the tree, 3) the splitting condition for each internal node in a predetermined order (e.g., depth first), 4) the number of bins for the histogram on each leafnode, and 5) the boundaries \(B_{l}\) and \(B_{u}\) of the histograms. That is, let \(L(.)\) denote the function that calculates code length in bits, the code length needed to encode a model \(M\) can be decomposed to

\[L(M)= L()+L()+_{j[K-1]}L( })\] \[+_{k[K]}L(})+L(\{B_{l},B_{u}\}); \]

we next describe them in order.

**Encoding the tree size and structure.** As we consider full binary trees only, it is sufficient to encode the number of leaves \(K\), which also determines the number of total nodes. As \(K\) is a positive integer, we adopt the standard Rissanen's integer universal code , denoted as \(L_{}(K)\), for which the code length is equal to \(L_{}(K)=c+_{2}(K)+_{2}_{2}(K)+...\); the summation continues until a small enough precision is reached, and \(c 2.865\) is a constant.

Further, for full binary trees, the number of all possible tree structures for trees with \(K\) leaves is equal to the Catalan number, denoted as \(_{K-1}\). Hence, specifying one certain structure costs \(_{2}_{K-1}\) bits. Thus, \(L_{}(K)+_{2}_{K-1}\) bits are required for encoding the tree size and structure.

Note that while there exist multiple ways of encoding tree size and structure (e.g., one alternative is to leverage the joint probability of the tree size and the tree structure, which can be defined by treating the tree as a realization of a Galton-Watson process [33; 2]), our encoding scheme adheres to the principle of achieving _conditional minimax_ by explicitly putting a prior on the number of nodes.

**Encoding the splitting conditions.** An individual splitting condition on a single tree node consists of a variable name and a splitting value, which we encode sequentially.

To begin with, if the dataset contains \(m\) feature variables, encoding the name of a certain variable \(X\) costs \(_{2}m\) bits. Further, the code length needed for encoding the splitting value depends on the variable type. First, for a discrete variable \(X\) with \(||\) unique values, specifying a single value cost \(_{2}(||)\) bits. Second, encoding the splitting value for a continuous variable can be achieved by sequentially encoding 1) the granularity level for the search space, denoted as the positive integer \(d\), and 2) the exact value within the granularity level \(d\). Specifically, with a fixed \(d\), we consider as the search space the \(C 2^{d-1}\) quantiles that can partition the values of \(X\) into equal-frequency bins. Note that the values of \(X\) are based on the subset of data points locally contained in this internal node.

Hence, encoding \(d\) costs \(L_{}(d)\) bits with Rissanen's code , and encoding one specific splitting value (quantile) costs \(_{2}(C 2^{d-1})=_{2}(C)+d-1\) bits. That is, we treat the granularity level \(d\) as a parameter to be optimized when learning a CDTree from the data, which avoids (arbitrarily) specifying the granularity in advance, a shortcoming in previous MDL-based methods (for other tasks instead of CDE) [22; 36; 58; 57; 28]. In contrast, the parameter \(C\) can be used to express the prior belief about the hierarchical structure of the search space, as further discussed in Appendix C.3.

**Encoding the histograms.** One subtle choice we made is to set the boundaries for histograms on all leaves to be the same, and we set them as the global boundary of the target variable. This avoids unseen (test) data points falling outside the boundaries of the histograms on the leaf nodes. This is because while it may be common to assume that the boundaries for the histogram are known in _unconditional_ density estimation, assuming the same for CDE is hardly realistic.

Therefore, the code length needed to encode the boundary \(L(\{B_{l},B_{u}\})\) in Eq. 3 is a constant that does not affect the model selection result. Consequently, it suffices to encode the number of bins for each histogram: for the histogram on the \(k\)th leaf with \(h_{k}\) bins, it costs \(L_{}(h_{k})\) bits by Rissanen's integer code .

### Model selection criterion

Combining Eq. 1,2, and 3, together with the results from Proposition 1, we present the following MDL-score as our final model selection criterion:

\[M^{*}=_{M}-_{2}(_{}P_{M,}(y^{n} |x^{n}))+_{k[K]}_{2}(N_{k},h_{k})+L(M) \]which has the form of the regularized maximum likelihood, yet without the need to tune the regularization hyperparameter. Note that the exact form of \(L(M)\) depends on the types of the feature variables; e.g., by assuming all variables are continuous, we obtain

\[L(M)=L_{}(K)+_{2}_{K-1}+_{j[K-1]} (_{2}(m)+L_{}(d_{j})+_{2}(C)+d_{j}-1)+_{k[K ]}L_{}(h_{k}),\]

where, as defined previously, \((.)\) denotes the regret term of each histogram, \(L_{}(.)\) the Rissanen's integer universal code , \(m\) the number of feature variables, \(d_{j}\) the granularity level for the search space for the splitting values corresponding to the \(j\)th internal node, \(C\) the constant that controls the hierarchical structure of the search space of the splitting values, and \(h_{k}\) the number of bins for the histogram on the \(k\)th leaf node.

## 5 Algorithm

While finding the optimal tree-based model with the branch-and-bound approach is possible for regression  and classification , this does not apply to our task for two reasons. First, our MDL-based regularization term differs from traditional penalty terms based on tree size. Second, our task resembles optimizing _model trees_ rather than classification and regression trees, as we aim to find the optimal histogram for each candidate split.

```
Input: Training dataset \(D\) Output: CDTree \(M\)
1\(M\{S_{0}\}\) ; // One leaf node only
2whileTruedo
3for\(S M\)do
4 Search the condition that splits \(S\) into two nodes and minimizes the MDL-score (Eq. 4). ; // Described in detail in Algorithm 2
5
6 end for
7ifSplitting any \(S M\) cannot further decrease the MDL-scorethen
8return CDTree \(M\)
9
10else
11 Among all \(S M\), find the single \(S^{*}\) to be split that minimizes the MDL-score.
12 Update \(M\) by replacing \(S^{*}\) with its two child nodes that minimize the MDL-score.
13
14 end if
15
16 end while
```

**Algorithm 1** Learn CDTree from data

**A greedy approach for tree construction.** We thus take a heuristic approach to optimize our MDL-score in Eq. 4. As summarized in Algorithm 1, we start with a tree with one leaf node \(M=\{S_{0}\}\); next, we iteratively update \(M\) by replacing one of the leaf nodes with its 'best' two child nodes. Specifically, to achieve the lowest MDL-score at each iteration, we simultaneously search for 1) which node to split, 2) the splitting condition for that node, and 3) the optimal number of bins for the histograms. That is, we iterate over all leaf nodes in \(M\); for each leaf node, we search for the splitting condition, along with the number of bins for the histograms on the (potential) child nodes, which as a whole minimizes the MDL-score. Notably, while an exhaustive search for the 'best' models on all potential child nodes is considered infeasible in traditional _model tree_ methods , we empirically demonstrate in Section 6.5 that our algorithm is comparable to KDE-based methods.

**Finding the child nodes.** We next elaborate on the search for the tree-splitting condition at each iteration, for which the pseudo-code is provided in Algorithm 2. For simplicity, we assume that all feature variables are either continuous or binary (i.e., categorical features are one-hot encoded in our implementation).

Further, we iterate over all columns of the feature matrix. For each column, we start with the granularity level \(d=1\) and generate candidate split points as the \(C 2^{d-1}\) quantiles that lead to equal frequency binning of the values. Note that the quantiles are generated based on the subset of data points covered by the node to be split, rather than the entire dataset. We search for the best split point at the fixed \(d\) that yields the minimum MDL score among all generated candidates. We then proceed to the next granularity level \(d+1\) and repeat the process, which stops if no split point in the next granularity level results in a better (smaller) MDL score.

Last, we also need to search for the optimal number of bins for the histogram that leads to the minimum MDL score, which we defer to Appendix B.

```
0: Training set \(D_{S}=(x^{\{S\}},y^{\{S\}})\) covered by node \(S\), with \(x^{\{S\}}=(_{1},...,_{m})\) having \(m\) columns, and constant \(C\)
0: The splitting condition of \(S\) that minimizes the MDL-score
1for\(j\{1,2,...,m\}\)do
2\(d 1\)whileTruedo
3\(candidate\_splits\) The \(C 2^{d-1}\) quantiles for equal-frequency binning for \(_{j}\)
4for\(s candidate\_splits\)do
5\(D_{l}\{(x,y) D_{S}|x_{j} s\}\) ; // Always \(s=1/2\) for binary features
6\(D_{r}\{(x,y) D_{S}|x_{j}>s\}\)
7 Construct histograms for \(D_{l}\) and \(D_{r}\) that minimize the MDL-score conditioned on the fixed \(j\) and \(d\). ; // Described in detail in Algorithm 3
8 Score \(\) Calculate the MDL-score assuming \(D_{S}\) is split into \(D_{l}\) and \(D_{r}\)
9 end while
10ifThe best Score is worse than that of the previous \(d\)then Break; else\(d d+1\) ;
11 end if
12 Record the best tuple \((d,s)\) for this column index \(j\)
13 end for returnThe tuple \((j,d,s)\) that minimizes the MDL-score
```

**Algorithm 2** Find the best split for node \(S\)

## 6 Experiments

We present our experiments to demonstrate the empirical performance of CDTree from various perspectives. Specifically, we aim to answer the following research questions: 1) Does CDTree provide more accurate conditional density estimation compared to existing interpretable (tree-based and kernel-based) methods? 2) As a proxy for interpretability, does CDTree have smaller tree sizes in comparison to other tree-based models? 3) Is CDTree robust against irrelevant "noisy" features? 4) Are the runtimes of our algorithm comparable to those of kernel-based methods?

### Experiment setup

We use 14 datasets with numerical target variables from the UCI repository . These datasets, summarized in Table 1, cover a wide range of sample sizes and dimensionalities. We benchmark our method against a variety of competitors, with all results obtained on the test sets using five-fold cross-validation. Our competitors include 1) NKDE and CKDE , which are based on kernel density estimation, with bandwidth and \(\) (for NKDE only) tuned by cross-validation; 2) LSCDE , which directly estimates the ratio of joint and marginal density using linear models with Gaussian kernel basis functions; 3) tree-based method CADET , which fits a Gaussian distribution on each leaf node; 4) two more tree-based baselines introduced by us, CART-k and CART-h, which respectively fit a KDE model and a histogram after a CART regression tree  is learned from data.

Furthermore, we compare against three black-box models as "upper" baselines, including 1) neural network methods NF  and MDN , which apply both dropout and noise regularization , and 2) the recently proposed tree boosting method LinCDE . Nonetheless, our goal with CDTree is _not_ to be more accurate than black-box models, but to introduce a CDE method that is both interpretable and accurate.

For reproducibility, we provide further details about implementation and parameter choices in Appendix C. We made our source code public: [https://github.com/ylincen/CDTree](https://github.com/ylincen/CDTree).

### Conditional density estimation accuracy

In Table 2, we report the average negative log-likelihoods (NLL) on the test sets, which can be regarded as an approximation to the expected log-loss \(E(-(y|x))\), the standard loss function for measuring CDE accuracy.

**Comparison to interpretable models.** The NLL of the CDTree models are the best (lowest) in 6 out of 14 datasets among all interpretable models, which are shown in bold in the table. Further, among all interpretable models, our average rank is the best, as reported in the (last row) of Table 2. As the datasets are _increasingly_ ranked based on their dimensionalities, we observe that CKDE--although also achieving the best NLL in 6 datasets--only performs well on datasets with low dimensions. Additionally, the other two kernel-based methods LSCDE and NKDE have in general worse accuracy.

Further, CDTree has better accuracy (lower NLL) than all tree-based competitors on almost all datasets (13 out of 14 datasets for CADET, and 12 out of 14 datasets for CART-h and CART-k). The superiority of CDTree highlights the advantages of 1) using non-parametric histogram models, and 2) conducting an exhaustive search for optimal histograms for all node splits when learning the tree structure, rather than fitting the model after the tree structure is fixed, as in CART-h and CART-k.

**Comparison to black-box models.** Neural network models generally exhibit better accuracy than interpretable models, as indicated by their average ranks. However, their non-transparency limits their applicability in critical areas. We argue that studying interpretable models and introducing CDTree paves the way for developing local surrogate models [40; 26], an essential approach for generating post-hoc explainability for black-box models, as no such method currently exists for CDE.

Further, the performance of CDTree is surprisingly on par with that of LinCDE. Since CDTree is based on a single tree while LinCDE is a tree ensemble model, we conjecture that the on-par performance is caused by the fact that CDTree adopts non-parametric histograms whereas LinCDE takes a parametric approach (with a much more flexible model class than Gaussian though).

    &  &  \\  Datasets & CADET & CART-h & CART-k & CXDE & lscDE & nxDE & _ours_ & LinCDE & mMesh & NF \\  energy & 3.55 & 3.09 & 3.06 & **2.47** & 3.38 & 3 & 2.93 & 2.93 & 2.78 & 2.86 \\ synchrono & -2.93 & -1.63 & -1.86 & **-3.59** & -1.25 & -1.57 & -2.11 & -1.85 & -2.94 & -2.64 \\ localizat & -0.23 & -0.55 & -0.01 & -0.26 & -0.61 & -0.28 & **-0.66** & -0.95 & -0.68 & -0.43 \\ toxicity & 1.8 & 1.5 & 1.38 & **1.32** & 1.34 & 1.55 & 1.53 & 1.29 & 1.24 & 1.23 \\ concrete & 4.17 & 3.75 & 3.93 & **3.32** & 3.66 & 3.91 & 3.72 & 3.47 & 2.97 & 3.18 \\ slump & 3.42 & 3.55 & 3.43 & **2.35** & 2.91 & 3.08 & 3.34 & 2.98 & 2.23 & 2.39 \\ forestfir & 134 & 3.96 & 4.39 & 4.85 & 4.68 & 5.55 & **3.43** & 4.35 & 3.26 & 3.23 \\ navalprop & -3.53 & -3.3 & **-3.66** & -2.8 & -2.88 & -3.19 & -3.6 & -3.36 & -4.12 & -3.75 \\ skillcraf & 94.4 & 0.46 & -0.42 & 1.54 & 1.61 & 1.56 & **-1.02** & 1.26 & 0.35 & 1.11 \\ sml2010 & 6.52 & 2.85 & 2.89 & **1.61** & 3.14 & 3.12 & 2.7 & 2.97 & 2.15 & 2.61 \\ thermogra & 2.21 & 0.66 & 0.72 & 0.66 & 0.94 & 0.94 & **0.64** & 0.59 & 0.56 & 0.52 \\ support2 & 97.3 & 0.51 & 0.32 & 2.09 & 2.46 & 2.13 & **0.29** & 1.48 & 1.53 & 1.24 \\ studentma & 3.83 & **2.65** & 2.66 & 2.89 & 4.19 & 3.11 & 2.66 & 2.59 & 3.

We also report the standard deviations of NLL for all methods in Appendix D.1. The standard deviations for our method remain low, indicating stable performance across different datasets.

### Complexity of trees

For tree-based models, the size of the trees is an important proxy for the degree of interpretability . We hence compare the number of leaves of CDTree against the other tree-based models CADET and CART (note that CART-h and CART-k have the same tree structures). We demonstrate in Figure 2 (left) that CDTree has smaller tree sizes than the competitors on most datasets.

### Robustness to irrelevant features

We next investigate whether CDTree is robust against irrelevant features, which is particularly important for the interpretability of tree-based models, since the (intrinsic) explanation contained in the CDTree would not be trustworthy if such robustness did not hold.

Specifically, we generate 'noisy' features in two different ways. The first way is to add \(w\) irrelevant features randomly drawn from the standard Gaussian distribution to each dataset. The second way is to first randomly select \(w\) features from each dataset; then, for each selected feature \(X_{j}\), we generate a 'noisy' feature by adding a Gaussian noise to it, i.e., \(X^{}_{j}=X_{j}+N(0,s(X_{j})/2)\), where \(s(X_{j})\) denotes the estimated standard deviation. We refer to the irrelevant features generated by the first (second) approach as independent (dependent) noisy features, where \(w\{3,5,10,20\}\) in both cases. Note that adding \(X^{}_{j}\) to the dataset does not change the true conditional density as the target variable is conditionally independent of \(X^{}_{j}\) given the original feature \(X_{j}\).

Next, we train the tree-based models on expanded datasets that include irrelevant features. We count the number of nodes with splitting conditions that involve these added irrelevant features. As demonstrated in Figure 3, the number of splits on irrelevant features for our CDTree is almost always zero for both ways of generating noisy features. In contrast, the tree-based models learned by CART and CADET contain many more nodes with irrelevant features.

Figure 3: Number of internal nodes with split conditions that contain irrelevant features. The y-axis is scaled by the squared-root for better visualization.

Figure 2: Left: the number of leaves for tree-based methods. Right: Runtimes of CDTree and kernel-based methods. Note that the y-axes are scaled by \(_{10}(.)\)

We also examine whether the negative log-likelihoods (NLL) remain stable when irrelevant features are added. For 'dependent noisy' features, the results are shown in Figure 4, where the NLL of CDTree remains nearly identical regardless of the number of added features--the only exception being the dataset "slump" (with only 103 rows). In contrast, the NLLs obtained by competitor methods are much less stable: the NLL of NKDE varies for most datasets, and CART-k shows visible changes in 6 out of 14 datasets. Similar results are observed when 'independent noisy' features are added, as shown in Appendix D.2.

### Runtimes

The idea of fitting separate models on the leaves of a decision tree has existed for a long time . Nevertheless, it is (still) often believed that fitting separate models for all possible node splits when growing a tree is infeasible. However, we show in Figure 2 (right) the runtime of CDTree and demonstrate that its runtimes are in general lower than the runtimes of CKDE (which requires intensive parameter tuning). While NKDE and LSCDE are in general faster than CDTree, the accuracy of their conditional density estimates are sub-optimal, as discussed previously. We exclude the comparison with other tree-based methods whose implementations are based on CART, as CART is highly optimized and known to be extremely fast. Further, as these tree-based methods either model the conditional means only (CART-h and CART-k) or assume a Gaussian model (CADET), they have far smaller search space, and hence are fast but less accurate.

## 7 Discussion

In this paper, we studied the interpretable conditional density estimation (CDE) models. Motivated by the fact that tree-based methods are arguably more interpretable than kernel-based methods yet have been largely disregarded for interpretable CDE, we introduced the Conditional Density Tree (CDTree). We formalized the learning problem under the MDL framework, proposed an iterative algorithm, and demonstrated its competitive empirical performance on a wide range of datasets.

**Limitations.** As histograms are used in the CDTree, we implicitly assume that the support of the target variable is bounded. Hence, the boundaries for the histograms need to be chosen in an ad-hoc way, possibly based on prior knowledge. Further, in practice, it may happen that the unseen data points fall outside the histogram boundary, for which the predicted (conditional) density will be 0, unless the full model is re-trained. We realize that this may cause some issues when using CDTree in practice, but meanwhile, we argue that 1) this is a limitation of the histogram model itself, 2) similar issues could happen in modelling quantiles by the empirical cumulative probability function, as in well-known methods like quantile regression trees and forest, and 3) all probability models have their own (implicit) assumptions on the probability tails, including those specifically designed for modeling the extreme value distributions [9; 23].

Figure 4: Negative log-likelihoods with different number of irrelevant ‘dependent noisy’ features. The results of CDTree (shown in blue lines) are stable on all datasets.