# Unsupervised Behavior Extraction via Random Intent Priors

Hao Hu\({}^{1}\), Yiqin Yang\({}^{2}\), Jianing Ye\({}^{1}\), Ziqing Mai\({}^{1}\), Chongjie Zhang\({}^{3}\)

\({}^{1}\)Institute for Interdisciplinary Information Sciences, Tsinghua University

\({}^{2}\)Department of Automation, Tsinghua University

\({}^{3}\)Department of Computer Science & Engineering, Washington University in St. Louis

{huh22, yangyiqi19, yejn21, maiq221}@mails.tsinghua.edu.cn

chongjie@wustl.edu

Equal contribution.

###### Abstract

Reward-free data is abundant and contains rich prior knowledge of human behaviors, but it is not well exploited by offline reinforcement learning (RL) algorithms. In this paper, we propose **UBER**, an unsupervised approach to extract useful behaviors from offline reward-free datasets via diversified rewards. UBER assigns different pseudo-rewards sampled from a given prior distribution to different agents to extract a diverse set of behaviors, and reuse them as candidate policies to facilitate the learning of new tasks. Perhaps surprisingly, we show that rewards generated from random neural networks are sufficient to extract diverse and useful behaviors, some even close to expert ones. We provide both empirical and theoretical evidence to justify the use of random priors for the reward function. Experiments on multiple benchmarks showcase UBER's ability to learn effective and diverse behavior sets that enhance sample efficiency for online RL, outperforming existing baselines. By reducing reliance on human supervision, UBER broadens the applicability of RL to real-world scenarios with abundant reward-free data.

## 1 Introduction

Self-supervised learning has made substantial advances in various areas like computer vision and natural language processing (OpenAI, 2023; Caron et al., 2021; Wang et al., 2020) since it leverages large-scale data without the need of human supervision. Offline reinforcement learning has emerged as a promising framework for learning sequential policies from pre-collected datasets (Kumar et al., 2020; Fujimoto and Gu, 2021; Ma et al., 2021; Yang et al., 2021; Hu et al., 2022), but it is not able to directly take advantage of unsupervised data, as such data lacks reward signal for learning and often comes from different tasks and different scenarios. Nonetheless, reward-free data like experience from human players and corpora of human conversations is abundant while containing rich behavioral information, and incorporating them into reinforcement learning has a huge potential to help improve data efficiency and achieve better generalization.

How can we effectively utilize the behavioral information in unsupervised offline data for rapid online learning? In online settings, Eysenbach et al. (2018); Sharma et al. (2019) investigate the extraction of diverse skills from reward-free online environments by maximizing diversity objectives. Meanwhile, in offline settings, a rich body of literature exists that focuses on leveraging the dynamic information from reward-free datasets (Yu et al., 2022; Hu et al., 2023). However, the former approach is not directly applicable to offline datasets, while the latter approach overlooks the behavioral information in the dataset. Ajay et al. (2020); Singh et al. (2020) employ generative models to extract behaviorsin the offline dataset. However, using a behavior-cloning objective is conservative in nature and has a limited ability to go beyond the dataset to enrich diversity.

To bridge this gap, we propose **U**nsupervised **B**ehavior **E**xtraction via **R**andom Intent Priors (UBER), a simple and novel approach that utilizes random intent priors to extract useful behaviors from offline reward-free datasets and reuse them for new tasks, as illustrated in Figure 1. Specifically, our method samples the parameters of the reward function from a prior distribution to generate different intents for different agents. These pseudo-rewards encourage the agent to go beyond the dataset and exhibit diverse behaviors. It also encourages the behavior to be useful since each behavior is the optimal policy to accomplish some given intent. The acquired behaviors are then applied in conjunction with policy expansion (Zhang et al., 2023) or policy reuse (Zhang et al., 2022) techniques to accelerate online task learning. Surprisingly, we observe that random intent prior is sufficient to produce a wide range of useful behaviors, some of which even approach expert performance levels. We provide theoretical justifications and empirical evidence for using random intent priors in Section 4 and 5, respectively.

Our experiments across multiple benchmarks demonstrate UBER's proficiency in learning behavior libraries that enhance sample efficiency for existing DRL algorithms, outperforming current baselines. By diminishing the reliance on human-designed rewards, UBER expands the potential applicability of reinforcement learning to real-world scenarios.

In summary, this paper makes the following contributions: (1) We propose UBER, a novel unsupervised RL approach that samples diverse reward functions from intent priors to extract diverse behaviors from reward-free datasets; (2) We demonstrate both theoretically and empirically that random rewards are sufficient to generate diverse and useful behaviors within offline datasets; (3) Our experiments on various tasks show that UBER outperforms existing unsupervised methods and enhances sample efficiency for existing RL algorithms.

### Related Works

Unsupervised Offline RL.Yu et al. (2021) considers reusing data from other tasks but assumes an oracle reward function for the new tasks. Yu et al. (2022) and Hu et al. (2023) utilize reward-free data for improving offline learning but assume a labeled offline dataset is available for learning a reward function. Ye et al. (2022); Ghosh et al. (2023) consider the setting where reward and action are absent. We focus on the reward-free setting and leverage offline data to accelerate online learning.

Unsupervised Behavior Extraction.Many recent algorithms have been proposed for intrinsic behavioral learning without a reward. Popular methods includes prediction methods (Burda et al., 2018; Pathak et al., 2017, 2019), maximal entropy-based methods (Campos et al., 2021; Liu and Abbeel, 2021, 2021, 2020; Xu et al., 2021; Yeats et al., 2021), and maximal mutual information-based methods (Eysenbach et al., 2018; Hansen et al., 2019; Liu and Abbeel, 2021, 2021). However, they require an online environment to achieve the diversity objective. Ajay et al. (2020); Singh et al. (2020) employ generative models to extract behaviors in the offline dataset but have a limited ability to enrich diversity.

Figure 1: The framework of UBER. The procedure consists of two phases. In the first offline phase, we assign different reward functions to different agent to extract diverse and useful behaviors from the offline dataset. In the second phase, we reuse previous behavior by adding them to the candidate policy set to accelerate online learning for the new task.

Offline-to-online learning and policy reuse.Offline-to-online RL has been popular recently since it provides a promising paradigm to improve offline further learned policies. Nair et al. (2020) is among the first to propose a direct solution to offline-to-online RL. (Lee et al., 2022) proposes to use a balanced replay buffer and ensembled networks to smooth the offline-to-online transition; Zhang et al. (2023) proposes to use the expanded policy for offline-to-online RL. Ball et al. (2023) proposes to reuse the off-policy algorithm with modifications like ensembles and layer normalization. Zhang et al. (2022) proposes a critic-guided approach to reuse previously learned policies. Zhang et al. (2023) proposes a similar and simpler way to reuse previous policies.

Imitation LearningOur method is also related to imitation learning (Hussein et al., 2017). Popular imitation learning methods include behavior cloning methods Pomerleau (1988) and inverse RL methods Ho and Ermon (2016); Xiao et al. (2019); Dadashi et al. (2020). However, they assume the demonstration in the dataset to be near optimal, while our method works well with datasets of mediocre quality.

## 2 Preliminaries

### Episodic Reinforcement Learning

We consider finite-horizon episodic Markov Decision Processes (MDPs), defined by the tuple \((,,H,,r)\), where \(\) is a state space, \(\) is an action space, \(H\) is the horizon and \(=\{_{h}\}_{h=1}^{H},r=\{r_{h}\}_{h=1}^{H}\) are the transition function and reward function, respectively.

A policy \(=\{_{h}\}_{h=1}^{H}\) specifies a decision-making strategy in which the agent chooses its actions based on the current state, i.e., \(a_{h}_{h}(\,|\,s_{h})\). The value function \(V_{h}^{}:\) is defined as the sum of future rewards starting at state \(s\) and step \(h[H]\), and similarly, the Q-value function, i.e.

\[V_{h}^{}(s)=_{}_{t=h}^{H}r_{t}(s_{t},a_{t})\, \,s_{h}=s, Q_{h}^{}(s,a)=_{}_{t=h}^{H}r_ {h}(s_{t},a_{t})\,\,s_{h}=s,a_{h}=a. \]

where the expectation is with respect to the trajectory \(\) induced by policy \(\).

We define the Bellman operator as

\[(_{h}f)(s,a)=r_{h}(s,a)+f(s^{}), \]

for any \(f:\) and \(h[H]\). The optimal Q-function \(Q^{*}\), optimal value function \(V^{*}\) and optimal policy \(^{*}\) are related by the Bellman optimality equation

\[V_{h}^{*}(s)=_{a}Q_{h}^{*}(s,a), Q_{h}^{*}(s,a)=( _{h}V_{h}^{*})(s,a),_{h}^{*}(\,|\,s)=*{ argmax}_{}_{a}Q_{h}^{*}(s,a). \]

We define the suboptimality, as the performance difference of the optimal policy \(^{*}\) and the current policy \(_{k}\) given the initial state \(s_{1}=s\). That is

\[(;s)=V_{1}^{^{*}}(s)-V_{1}^{}(s).\]

### Linear Function Approximation

To derive a concrete bound, we consider the _linear MDP_(Jin et al., 2020, 2021) as follows, where the transition kernel and expected reward function are linear with respect to a feature map.

**Definition 2.1** (Linear MDP).: \((,,,, )\) is a _linear MDP_ with a feature map \(:^{d}\), if for any \(h[H]\), there exist \(d\)_unknown_ (signed) measures \(_{h}=(_{h}^{(1)},,_{h}^{(d)})\) over \(\) and an _unknown_ vector \(z_{h}^{d}\), such that for any \((s,a)\), we have

\[_{h}(\,|\,s,a)=(s,a),_{h}(), r_ {h}(s,a)=(s,a),z_{h}. \]

Without loss of generality, we assume \(||(s,a)|| 1\) for all \((s,a)\), and \(\{||_{h}()||,||z_{h}||\}\) for all \(h[H]\).

When emphasizing the dependence over the reward parameter \(z\), we also use \(r_{h}(s,a,z),V_{h}^{}(s,z)\) and \(V_{h}^{*}(s,z)\) to denote the reward function, the policy value and the optimal value under parameter \(z\), respectively.

Method

_How can we effectively leverage abundant offline reward-free data for improved performance in online tasks?_ Despite the absence of explicit supervision signals in such datasets, they may contain valuable behaviors with multiple modes. These behaviors can originate from human expertise or the successful execution of previous tasks. To extract these behaviors, we assume that each behavior corresponds to specific intentions, which are instantiated by corresponding reward functions. Then, by employing a standard offline RL algorithm, we can extract desired behavior modes from the offline dataset using their associated reward functions. Such behavior can be reused for online fine-tuning with methods such as policy expansion (Zhang et al., 2023) or policy reuse (Zhang et al., 2022).

Then, the question becomes how to specify possible intentions for the dataset. Surprisingly, we found that employing a random prior over the intentions can extract diverse and useful behaviors effectively in practice, which form the offline part of our method. The overall pipeline is illustrated in Figure 1. Initially, we utilize a random intention prior to generating diverse and random rewards, facilitating the learning of various modes of behavior. Subsequently, standard offline algorithms are applied to extract behaviors with generated reward functions. Once a diverse set of behaviors is collected, we integrate them with policy reuse techniques to perform online fine-tuning.

Formally, we define \(\) as the space of intentions, where each intention \(z\) induces a reward function \(r_{z}:\) that defines the objective for an agent. Let the prior over intentions be \(()\). In our case, we let \(\) be the weight space \(\) of the neural network and \(\) be a distribution over the weight \(w\) of the neural network \(f_{w}\) and sample \(N\) reward functions from \(\) independently. Specifically, we have

\[r_{i}(s,a)=f_{w_{i}}(s,a),\ w_{i},\  i=1,,N.\]

We choose \(\) to be a standard initialization distribution for neural networks (Glorot and Bengio, 2010; He et al., 2015). We choose TD3+BC (Fujimoto and Gu, 2021) as our backbone offline algorithm, but our framework is general and compatible with any existing offline RL algorithms. Then, the loss function for the offline phase can be written as

\[_{}^{}(_{i})= _{(s,a,r,s^{})_{}^{}} [(r_{i}+ Q_{^{}_{i}}(s^{},)-Q_{ _{i}}(s,a))^{2}], \] \[_{}^{}(_{i})=- _{(s,a)_{}^{}}[ Q_ {_{i}}(s,_{_{i}}(s))-(_{_{i}}(s)-a)^{2}], \]

where \(=_{^{}_{i}}(s^{})+\) is the smoothed target policy and \(\) is the weight for behavior regularization. We adopt a discount factor \(\) even for the finite horizon problem as commonly used in practice (Fujimoto et al., 2018).

As for the online phase, we adopt the policy expansion framework (Zhang et al., 2023) to reuse policies. We first construct an expanded policy set \(=[_{_{1}},,_{_{N}},_{w}]\), where \(\{_{i}\}_{i=1}^{N}\) are the weights of extracted behaviors from offline phase and \(_{w}\) is a new randomly initialized policy. We then use the critic as the soft policy selector, which generates a distribution over candidate policies as follows:

\[P_{}[i]=_{i}(s)))}}{_{j}_{j}(s)))}}, i \{1,,N+1\}, \]

where \(\) is the temperature. We sample a policy \(i\{1,,N+1\}\) according to \(P_{}\) at each time step and follows \(_{i}\) to collect online data. The policy \(_{w}\) and \(Q_{}\)-value network undergo continuous training using the online RL loss, such as TD3 (Fujimoto et al., 2018)):

\[_{}^{}()= _{(s,a,r,s^{})^{}}[(r+  Q_{^{}}(s^{},)-Q_{}(s,a))]^{2}, \] \[_{}^{}(w)= -_{(s,a)^{}}[Q_{}(s, _{w}(s))]. \]

The overall algorithm is summarized in Algorithm 1 and 2. We highlight elements important to our approach in purple.

```
1:Require: \(\{_{_{i}}\}_{i=1}^{N}\), offline dataset \(^{}\), update-to-data ratio \(G\)
2:Initialize online agents \(Q_{},_{w}\) and replay buffer \(^{}\)
3:Construct expanded policy set \(=[_{_{1}},,_{_{N}},_{w}]\)
4:for each episode do
5: Obtain initial state \(s_{1}\) from the environment
6:for step \(t=1,,T\)do
7: Construct \(P_{}\) According to Equation (7)
8: Pick an policy to act \(_{t} P_{}\), \(a_{t}_{t}(|s_{t})\)
9: Store transition \((s_{t},a_{t},r_{t},s_{t+1})\) in \(^{}\)
10:for\(g=1,,G\)do
11: Calculate \(^{}_{}()\) as in Equation (8) and update \(\)
12:endfor
13: Calculate \(^{}_{}(w)\) as in Equation (9) and update \(w\)
14:endfor
15:endfor
```

**Algorithm 2** Phase 2: Online Policy Reuse

## 4 Theoretical Analysis

_How sound is the random intent approach for behavior extraction?_ This section aims to establish a theoretical characterization of our proposed method. We first consider the completeness of our method. We show that any behavior can be formulated as the optimal behavior under some given intent, and any behavior set that is covered by the offline dataset can be learned effectively given the corresponding intent set. Then we consider the coverage of random rewards. We show that with a reasonable number of random reward functions, the true reward function can be approximately represented by the linear combination of the random reward functions with a high probability. Hence our method is robust to the randomness in the intent sampling process.

### Completeness of the Intent Method for Behavior Extraction

We first investigate the completeness of intent-based methods for behavior extraction. Formally, we have the following proposition.

**Proposition 4.1**.: _For any behavior \(\), there exists an intent \(z\) with reward function \(r(,,z)\) such that \(\) is the optimal policy under \(r(,,z)\). That is, for any \(=\{_{h}\}_{h=1}^{H},_{h}:()\), there exists \(z\) such that_

\[V_{h}^{}(s,z)=V_{h}^{*}(s,z),\;(s,a,h) [H].\]

Proof.: Please refer to Appendix A.1 for detailed proof. 

Proposition 4.1 indicates that any behavior can be explained as achieving some intent. This is intuitive since we can relabel the trajectory generated by any policy as successfully reaching the final state in hindsight (Andrychowicz et al., 2017). With Proposition 4.1, we can assign an intent \(z\) to any policy \(\) as \(_{z}\). However, it is unclear whether we can learn effectively from offline datasets given an intent set \(\). This is guaranteed by the following theorem.

**Theorem 4.2**.: _Consider linear MDP as defined in Definition 2.1. With an offline dataset \(\) with size \(N\), and the PEVI algorithm (Jin et al., 2021), the suboptimality of learning from an intent \(z\) satisfies_

\[(;s,z)=(^{ }d^{2}H^{3}}{N}}), \]

_for sufficiently large \(N\), where \(=(dN||/)\), \(c\) is an absolute constant and_

\[C_{z}^{}=_{h[H]}_{\|x\|=1}_{_{z},h}x}{ x^{}_{_{h}}x},\]

_with \(_{_{z},h}=_{(s,a) d_{_{z},h}(s,a)}[(s,a)(s,a )^{}],\;_{_{h}}=_{_{h}}[(s,a)(s,a)^{}]\)._

Proof.: Please refer to Appendix A.2 for detailed proof. 

\(C_{z}^{}\) represents the maximum ratio between the density of empirical state-action distribution \(\) and the density \(d_{_{z}}\) induced from the policy \(_{z}\). Theorem 4.2 states that, for any behavior, \(_{z}\) that is well-covered by the offline dataset \(\), we can use standard offline RL algorithm to extract it effectively, in the sense that learned behavior \(\) has a small suboptimality under reward function \(r(,,z)\). Compared to standard offline result (Jin et al., 2021), Theorem 4.2 is only worse off by a factor of \(||\), which enables us to learn multiple behaviors from a single dataset effectively, as long as the desired behavior mode is well contained in the dataset.

### Coverage of Random Rewards

This section investigates the coverage property of random rewards. Intuitively, using random rewards may suffer from high variance, and it is possible that all sampled intents lead to undesired behaviors. Theorem 4.3 shows that such cases rarely happen since with a reasonable number of random reward functions, the linear combination of them can well cover the true reward function with a high probability.

**Theorem 4.3**.: _Assume the reward function \(r(s,a)\) admits a RKHS represention \((s,a)\) with \(\|(s,a)\|_{}\) almost surely. Then with \(N=c_{0}(18^{2}/)\) random reward functions \(\{r_{i}\}_{i=1}^{}\), the linear combination of the set of random reward functions \((s,a)\) can approximate the true reward function with error_

\[_{(s,a)}[(s,a)-r(s,a)]^{2} c_{1}^{2}(18/ )/,\]

_with probability \(1-\), where \(M\) is the size of the offline dataset \(\), \(c_{0}\) and \(c_{1}\) are universal constants and \(\) is the distribution that generates the offline dataset \(\)._

Proof.: Please refer to Appendix A.3 for detailed proof. 

Theorem 4.3 indicates that RL algorithms are insensitive to the randomness in the reward function used for learning, as long as we are using a reasonable number of such functions. This phenomenon is more significant in offline algorithms since they are conservative in nature to avoid over-generalization. This partly explains why using random rewards is sufficient for behavior extraction, and can even generate policies comparable to oracle rewards. Such a phenomenon is also observed in prior works (Shin et al., 2023; Li et al., 2023).

## 5 Experiments

Our evaluation studies UBER as a pre-training mechanism on reward-free data, focusing on the following questions: (1) Can we learn useful and diverse behaviors with UBER? (2) Can the learned behaviors from one task using UBER be reused for various downstream tasks? (3) How effective is each component in UBER?

To answer the questions above, we conduct experiments on the standard D4RL benchmark (Fu et al., 2020) and the multi-task benchmark Meta-World (Yu et al., 2020), which encompasses a variety of dataset settings and tasks. We adopt the normalized score metric proposed by the D4RL benchmark, and all experiments are averaged over five random seeds. Please refer to Appendix D for more experimental details.

### Experiment Setting

We extract offline behavior policies from multi-domains in D4RL benchmarks, including locomotion and navigation tasks. Specifically, The locomotion tasks feature online data with varying levels of expertise. The navigation task requires composing parts of sub-optimal trajectories to form more optimal policies for reaching goals on a MuJoco Ant robot.

Baselines.We compare UBER with baselines with various behavior extraction methods. We compare our method with BC-PEX, which uses a behavior-cloning objective and other unsupervised behavior extraction methods (Yang et al., 2022), including OPAL (Ajay et al., 2020) and PARROT (Singh et al., 2020). We also compare our method with a strong offline-to-online method, RLPD (Ball et al., 2023) and an unsupervised data sharing method, UDS (Yu et al., 2022). Specifi

Figure 3: Comparison between UBER and baselines including unsupervised behavior extraction (PARROPAL) and data-sharing methods (UDS). The result is averaged with five random seeds. Our method outperforms these baselines by leveraging the diverse behavior set.

Figure 2: Comparison between UBER and baselines in the online phase in the Mujoco domain. We adopt datasets of various quality for offline training. We adopt a normalized score metric averaged with five random seeds.

[MISSING_PAGE_FAIL:8]

data; (2) UBER can learn a diverse set of behaviors, spanning a wider distribution than the original dataset. We hypothesize that diversity is one of the keys that UBER can outperform behavior cloning-based methods.

Further, we conduct experiments to test if the set of random intents can cover the true intent. We calculate the correlation of \(N=256\) random rewards with the true reward and measure the linear projection error. The results in Table 2 indicate that random intents can have a high correlation with true intent, and linear combinations of random rewards can approximate the true reward function quite well.

To test whether UBER can learn useful behaviors for downstream tasks, we compare UBER with BC-PEX, AVG-PEX, and TD3 in the online phase in the Mujoco domains. The experimental results in Figure 2 show that our method achieves superior performance than baselines. By leveraging the behavior library generated from UBER, the agent can learn much faster than learning from scratch, as well as simple behavior cloning methods. Further, the results in Figure 3 show that UBER also performs better than behavior extraction and data sharing baselines in most tasks. This is because prior methods extract behaviors in a behavior-cloning manner, which lacks diversity and leads to degraded performance for downstream tasks.

In addition, to test UBER's generality across various offline and online methods on various domains, we also conduct experiments on Antmaze tasks. We use IQL (Kostrikov et al., 2021) and RLPD (Ball et al., 2023) as the backbone of offline and online algorithms. The experimental results in Table 1 show that our method achieves stronger performance than baselines. RLPD relies heavily on offline data and RLPD without true reward has zero return on all tasks. Differently, UBER extracts a useful behavior policy set for the online phase, achieving similar sample efficiency as the oracle method that has reward label for the offline dataset. We also provide the learning curve for the antmaze environment in Figure 11.

Figure 5: Experimental results on the meta-world based on two types of offline datasets, expert and replay. All experiment results were averaged over five random seeds. Our method achieves better or comparable results than the baselines consistently.

Figure 6: Ablation study for the online policy reuse module. Using policy expansion (PEX) for downstream tasks is better than critic-guided policy reuse (CUP) in general.

Answer of Question 2:To test whether the behavior set learned from UBER can benefit multiple downstream tasks, we conducted the multi-task experiment on Meta-World (Yu et al., 2020), which requires learning various robot skills to perform various manipulation tasks. We first extract behaviors with random intent priors from the selected tasks and then use the learned behaviors for various downstream tasks. The experimental results in Appendix B show that the prior behaviors of UBER can be successfully transferred across multi-tasks than baselines. Please refer to Appendix B for the experimental details and results.

Answer of Question 3:To understand what contributes to the performance of UBER, we perform ablation studies for each component of UBER. We first replace the policy expansion module with critic-guided policy reuse (CUP; Zhang et al., 2022), to investigate the effect of various online policy reuse modules. The experimental results in Figure 6 show that UBER+PEX generally outperforms UBER+CUP. We hypothesize that CUP includes a hard reuse method, which requires the optimized policy to be close to the reused policies during training, while PEX is a soft method that only uses the pre-trained behaviors to collect data. This makes PEX more suitable to our setting since UBER may generate undesired behaviors.

Then, we ablate the random intent priors module. We use the average reward in the offline datasets to learn offline policies and then follow the same online policy reuse process in Algorithm 2. Using average reward serves as a strong baseline since it may generate near-optimal performance as observed in Shin et al. (2023). We name the average reward-based offline behavior extraction method as AVG-PEX. The experimental results in Figure 7 show that while average reward-based offline optimization can extract some useful behaviors and accelerate the downstream tasks, using random intent priors performs better due to the diversity of the behavior set.

## 6 Conclusion

This paper presents Unsupervised Behavior Extraction via Random Intent Priors (UBER), a novel approach to enhance reinforcement learning using unsupervised data. UBER leverages random intent priors to extract diverse and beneficial behaviors from offline, reward-free datasets. Our theorem justifies the seemingly simple approach, and our experiments validate UBER's effectiveness in generating high-quality behavior libraries, outperforming existing baselines, and improving sample efficiency for deep reinforcement learning algorithms.

UBER unlocks the usage of abundant reward-free datasets, paving the way for more practical applications of RL. UBER focuses on learning behaviors, which is orthogonal to representation learning. It is an interesting future direction to further boost online sample efficiency by combining both approaches. Consuming large unsupervised datasets is one of the keys to developing generalist and powerful agents. We hope the principles and techniques encapsulated in UBER can inspire further research and development in unsupervised reinforcement learning.

## 7 Acknowledgeement

This work is supported in part by Science and Technology Innovation 2030 - "New Generation Artificial Intelligence" Major Project (No. 2018AAA0100904) and the National Natural Science Foundation of China (62176135).

Figure 7: Ablation study for the random intent priors module. Using average reward for policies learning can lead to nearly optimal performances on the original task. Nevertheless, our method outperforms such a baseline (AVG-PEX) consistently, showing the importance of behavioral diversity.