# Necessity of Uncertainty Quantification for Audio driven healthcare diagnosis

Shubham Kulkarni, Hideaki Watanabe, Fuminori Homma

Sec. 2, AI Application Development

Sony Group Corporation

Tokyo, Japan 141-8610

shubham.a.kulkarni@sony.com

###### Abstract

Deep learning excels in analyzing multi-modal signals for healthcare diagnostics but lacks the ability to quantify confidence in the predictions, which can lead to overconfident, erroneous diagnoses. In this work, we propose to predict model output independently and estimate the corresponding uncertainty. We present a unified audio-driven disease detection framework incorporating uncertainty quantification (UQ). This is achieved using a Dirichlet density approximation for model prediction and independent kernel distance learning in feature latent space for UQ. This approach requires minimum modifications to existing audio encoder architectures and is extremely parameter efficient compared to k-ensemble models. The uncertainty-aware model improves prediction reliability by producing confidence scores that closely match the accuracy values. Evaluations using the largest publicly available respiratory disease datasets demonstrate the advantage of the proposed framework in accuracy, training and inference time over ensemble and dropout methods. The proposed model improves speech and audio analysis for medical diagnosis by identifying and calibrating uncertainties, enabling better decision-making and risk assessment. This is shown by high uncertainty scores at low model accuracy.

## 1 Introduction

The increase in general awareness and interest in speech technologies for disease diagnosis has generated significant growth in recorded public health datasets Song et al. (2023); Novikova and Balagopalan ([n. d.]) across different modalities such as audio, imaging and time series (EEG). As the healthcare industry increasingly embraces data-driven approaches, the accurate interpretation of these subtle and complex multi-modal signals has become paramount for informed decision-making and improved patient outcomes. However, for these models to be useful in practical implementation, the outputs of such models must be explainable for medical decision making Miller (2019). Multi-modal medical datasets have been extensively researched for the task of disease diagnosis, symptom identification and monitoring Kulkarni et al. (2023); Wang and Wang (2022); Bae et al. (2023). Popularly, large-scale convolutional neural network (CNN) architectures Demir et al. (2020) such as ResNet Gairola et al. (2021); Bengs et al. ([n. d.]) trained on spectrogram images of audio inputs are used for this task. Recently, direct waveform speech encoders (Wav2Vec Baevski et al. (2020), and PASE Ravanelli et al. (2020)) have shown improved speech feature representations for respiratory monitoring Kulkarni et al. (2023). After featurisation, a classification layer followed by softmax is used to produce output scores. However, fixed softmax scores may result in fundamentally incorrect outputs without indicating that the estimate is uncertain. Thus, achieving a statistically nuancedunderstanding of model outputs via uncertainty quantification (UQ) is crucial in safety-critical applications such as disease detection.

This can be illustrated with a simple example of speech-driven disease detection. Input audio can be either "healthy" or COVID-"positive." A softmax-based classifier gives scores that express the likelihood of two different classes. Figure 1 (left) shows a histogram of the softmax scores coloured according to the correctness of the predicted output. The plot shows that irrespective of correctness of the prediction, the output confidence is always greater than 50%. The confidence score for two inputs (one predicted correctly and another incorrectly) lying on a vertical will be exactly same (healthy = 0.89, positive = 0.11) and (healthy = 0.11, positive = 0.89). Without UQ model, there is no way to decide the reliability of either prediction based on just softmax probabilities. An independent UQ estimate can quantify high uncertainty for false predictions, as shown in Figure 1 (right). An uncertainty-aware audio classification model enables 1) prediction of confidence scores independent of model outputs and 2) calibration of model such that estimated uncertainty closely follows model accuracy.

In this work, we present a novel framework for uncertainty-aware disease detection using speech and non-speech inputs through quantification and disentanglement of sample uncertainty and model calibration. The framework comprises of a probabilistic classification head on top of a self-supervised audio encoder and model uncertainties are quantified using a feature distance-based metric. A training scheme is proposed to optimize uncertainty estimation independent of model prediction or classification training. A novel formulation of learnable transformation matrix in latent space is used to maximise feature space diversity for distance calculation. Evaluations show that the uncertainty-aware model produces low confidence scores at low accuracy values, thus improving output reliability. Experiments on the largest public respiratory disease datasets show that the proposed UQ model is generalizable, computationally efficient at training and enables fast evaluation during inference without sacrificing classification performance. Specifically, our contributions are as follows -

* Advocate the use of a probabilistic classifier in place of softmax scores to quantify irreducible uncertainties inherent in learning problem for audio-driven disease diagnosis and medical decision making
* Emphasize the necessity of model calibration for reducible uncertainties in audio-driven disease diagnosis. we show that combining probabilistic classifier simple k-ensembles (even with small k=5) significantly improves model calibration score
* Propose a novel single inference method of uncertainty quantification with minimal changes to large encoder models for high-fidelity datasets such as audio and speech. The proposed model performs as well as k-ensembles at a fraction of compute and memory costs

To best of our knowledge, this is the first systematic study of uncertainties quantification and model calibration associated with audio driven disease diagnosis.

## 2 Model

Lets denote \(a(t)\) as an input audio waveform and \((y_{j}=y_{j}+_{j})\) is its corresponding noisy label which takes a value from label space \(j\{1,\}\) and \(_{j}\) is the label noise due to data gathering process or the noise inherent to the mapping problem \(G:\). We decompose above function mapping as \(G=h f\), where, \(f:^{n}\) indicates a deep audio feature encoder. The feature

Figure 1: Calibration histograms for speech-driven COVID classifier (left) and uncertainty aware model (right) coloured according to prediction accuracy

encoder gives embedding vectors \(X_{w}(a)^{d}\). The uncertainty aware classification head \(h:X y\) gives a prediction over class labels \(P[y|x]=h(X)\).

The proposed uncertainty quantification (UQ) framework, illustrated in Figure 2, consists of two parts:

1. A probabilistic classifier \(h\) trained to output concentration parameters of Dirichlet distribution over the softmax layers. This classifier head is used on top of a regularised deep audio feature extractor (\(f\)), which produces latent embedding \(X\).
2. An uncertainty aware calibration training to estimate UQ as a function of feature space density. We use a novel learnable Mahalanobis distance-based metric, which ensures the latent space is bi-Lipschitz continuous and captures a measure of data distribution.

In the subsequent sections, we describe these two component of the proposed UQ framework

### Probabilistic Classifier

A deterministic softmax classifier only outputs a single scaled vector \(s(x)\) corresponding a input \(x\) such that \(_{j}s(x)=1\). In contrast, the probabilistic classifier head is trained to predict a vector of concentration parameters \(=(_{1}_{J})\) one for each class label \(j J\), and a strength parameter \(_{0}:=_{j}(_{j})\). This set of concentration parameters define a Dirichlet distribution \(()\) with probability density given by equation 1, where \(()\) denotes _Gamma_ function.

This is used to sample a class probability vector \(\) as a random vector \(()\), At the inference time, a sample from Dirichlet distribution gives indicative probability \(p_{j}\) of input \(x\) belonging to class \(j\). The expected probability (mean) and the variance for a single input x is given by

\[(|)=)}{_{c=1}^{C} (_{c})}_{c=1}^{C}p_{c}^{_{c}-1} ^{2}(x):=(_{0}-_{j})}{_{0}(_{0 }+1)} \]

Thus, the classifier head is a model with uncertainty that outputs two quantities corresponding to label distribution, the mean \((x)\) and the variance \((x)\). The sampling based output stems from key insight that softmax based classifier cannot capture output categorical probability but a distribution over categorical softmax (i.e. Dirichlet) can be used to formulate deep learning as evidence acquisition problem Sensoy et al. (2018); DeVries and Taylor (2018).

The classification head is trained using unweighted combination of negative log likelihood term \(^{NLL}\) and a KL-divergence term, following the Sensoy et al. (2018); Bachstein et al. (2019). Appendix covers Loss function derivations and final expressions. Upon training the classifier model using above loss, we obtain predictive distribution parameters - mean \((x)\) and variance \((x)\). However

Figure 2: Proposed framework for uncertainty quantification (UQ) of audio driven disease detection

this quantity only gives the output label probability of a given input for a fixed model. Considering the original function mapping problem \(G:\) and the decomposition \(G=h f\), the probabilistic classifier \(h\) is a single sample from a possibly large intractable hypothesis space \(\). Further, the audio encoder \(f\) is parametrised by a set weights \(\). In the supervised setting a point estimate of vector \(W\) is obtained by empirical risk maximisation of an objective function. In Bayesian modelling Lakshminarayanan et al. (2016); Gal and Uk (2016), uncertainties in this point estimate, are computed by assuming that the weights \(w\) follow a prior distribution \(Pr(w)\). Subsequently, the model training process leads to posterior distribution \(P(w|D)\). The trained model \(f_{w}(x)\) uses this posterior distribution to calculate the estimated output \(y\). The measure of uncertainty, UQ, is given by the expected value and variance of the prediction \(f_{w}(x)\) over the posterior density distribution of \(w\). However, for high-dimensional datasets such as audio and speech, accurate modelling of the density function \(P(w|D)\) is impossible, given the complexity and non-linear nature of weights \(w\) of audio classification models Hernandez-Lobato and Adams (2015).

Figure 2(b) shows two approaches for approximating the intractable posterior density \(P(w|D)\) by introducing diversity into model evaluations. The feature encoder generates a fixed and deterministic encoding vector \(X\) for the input audio signal. Model uncertainty is quantified by analysing the variance of the outputs obtained through multiple forward passes of diverse models. In **Monte Carlo (MC) dropout**Gal and Ghahramani (2016); Xiao and Wang (2019), probabilistic (\(p=0.1\)) dropout layers between non-linear layers of the network are activated during inference resulting in variable outputs. Whereas, in **Deep ensemble**Lakshminarayanan et al. (2016), k-different models (\(k=5,15\)) are trained using different subsets of the dataset. The ensemble prediction is the average soft-max outputs from the individual models.

Combining classification head with multi-forward pass inferences in equation 1, we get a series of means \(_{k}(x)\) and variances \(_{k}^{2}(x)\), where \(k[1,K]\) are number of different ensembles or inferences of Figure 2(b). These samples are combined to form a single predictive uncertainty estimate \(Var[X]\) for input \(X\) as an empirical expectation over all inferences \(k\). A combination of Deep Ensemble

\[*() = _{k}_{k}^{2}()+_{k}_{k}^{2}()-_{*}^{2}()\] \[= _{k}[_{k}^{2}()]+_ {k}[_{k}^{2}()]-_{k}[_{k}()]^{2}\] \[= _{k}[_{k}^{2}()]}_{}+_{k}[_{k}()]}_{}\]

and Dirichlet Probabilistic classifier gives an estimate for the Irreducible Aleatoric Uncertainty and Model Uncertainty (Epistemic). However, it is neither possible to treat each term separately nor to reduce epistemic part of uncertainty. Despite the limitations, the k-ensemble approach is shown to be the state-of-the art for uncertainty prediction on several benchmarks Mukhoti et al. (2021). Both these methods improve performance and uncertainty estimation through model diversity but incur high computational costs during training and inference. In next section, we describe the second part of the proposed framework - an alternative to k-ensemble for quantifying approximate Epistemic uncertainty in single forward pass.

### Single Inference Uncertainty Quantification

In contrast to multiple feed-forward evaluation models, we propose single-shot UQ estimation using latent feature maps produced by the encoder as a representation of the class conditional distribution. A distance measure in the feature space of the model has shown to be useful for the detection of out-of-distribution examples Venkataramanan et al. (2023) and uncertainty estimation Lee et al. ([n. d.]); van Amersfoort et al. (2020). However, these methods suffer from three key problems namely feature collapse van Amersfoort et al. (2020), class imbalance Venkataramanan et al. (2023), smoothness and sensitivity Lee et al. ([n. d.]). We first describe the proposed single shot approach with intuitive modifications to training scheme that address the aforementioned problems.

The uncertainty estimation flow is shown in Figure 2(a). A centroid vector \(Z^{m}\) is initialised randomly and assigned to each label class in a set of classes \(J\). Let \(X_{t}(i)^{d}\) be the set of audio encodings of a mini-batch during training. A distance transformation matrix \(W_{j}(m,d)\) is initialised using a Gaussian prior per class, where \(d\) is the feature encoding dimension and \(m<d\) is the size of the centroid vector. Weight matrix \(W_{j}\) acts as a learnable linear dimensionality reduction on feature vectors, enabling a compact representation for distance computation Ren et al. (2021); Venkataramanan et al. (2023). The class-dependent nature of \(W_{j}\) enables class separation in latent space and is crucial for minimising the likelihood of feature collapse.

A weighted feature distance \(D_{j}\) between the model output and centroids is computed as:

\[D_{j}(X_{t},Z_{j})=X_{t}-Z_{j}||^{2}}{2m_{j}^{2}}}\]

where length scale \(_{j}\) is a trainable parameter and acts as class dependent normalising hyperparameter.

If the matrix \(W\) is assumed to be Identity Matrix the above formulation computes Mahalanobis distance (MD) from the centroids. The learnable nature of \(W\) acts as an adaptive dimensionality reduction on the latent space \(X\) and the output \(WX\) can be expected to represent global distributions as well as class dependent local distributions.

During the forward pass, a class label for each sample is given by softmax of distance scores \(y_{i}=Z_{j}(X_{i})\) as the maximum correlation (minimum distance) between data point \(X_{i}\) and class centroids \(Z_{j}\). For the UQ estimate, the set of Mahalanobis distances is normalised through the division of maximum class distance. The model uncertainty is given by mixture of the Gaussian models fitted at each class centroid \(d_{}= j(D_{j}|z_{j},_{j})\).

The class centroids, \(Z_{j}\), are updated for every mini-batch of training using an exponential moving average of the feature vectors of data points corresponding to class \(j\):

\[Z_{t+1,j}= Z_{t,j}+}(1-)_{i}(W_{j}X_{i})\]

where \(n_{j}\) is number of samples in the \(j^{th}\) class, and \(\) is a hyper-parameter similar to momentum gradient descent. After each update, the class vectors are normalised such that \(||Z_{j}||_{2}=1\).

Class dependent **triplet Loss** formulation is used to maximise the distance between distinct class centroids and minimise intra-class separation, following Kumar et al. (2020); Hermans et al. (2017). Audio embeddings obtained from the encoder network were used as an anchor point \(X_{a}\). Let \(Z_{a}\) be the centroid vector of the class corresponding to true label \(y_{a}\), while \(Z_{j}\) indicates remaining centroid vectors such that \(\{j J j a\}\), The loss with margin \((0.1-0.5)\) is given by

\[_{triplet}=_{a,j}(||WX_{a}-Z_{a}||-||WX_{a}-Z_ {j}||+,0)\]

During the training process, this loss is averaged over a mini-batch of data points, the class centroids are updated to new locations as per predicted labels and stochastic gradient descent (SGD) is performed for \(\) and \(W_{j}\). Audio encoder output latent vectors usually have high dimensions and the above loss may suffer poorly due to involved distance computation. Low rank nature of \(W\) ensures that distance computation in above loss function is sensible.

**Feature Regularisation** High dimensional feature space embedding suffer from feature collapse and feature redundancy in latent space which can adversely affect uncertainty prediction Liu et al. (2020); van Amersfoort et al. (2020). These problems can be alleviated by encouraging latent space smoothness and sensitivity, or alternatively by regularising the the weights \(W\) to follow bi-Lipschitz condition Liu et al. (2020)

\[L_{1}*||x_{1}-x_{2}||_{X}||f_{W}(x_{1})-f_{W}(x_{2})||_{H} L_{2}*||x_{ 1}-x_{2}||_{X}\]

This ensures the mapping \(||f_{W}(x_{1})-f_{W}(x_{2})||_{H}\) has meaningful correspondence in input space with respect to a well defined distance measure \(||x_{1}-x_{2}||_{X}\)Liu et al. (2020). This condition also ensures smoothness in latent space such that the audio embeddings are not too sensitive to small variations in input.

We use spectral normalisation to enforce bi-Lipschitz condition during UQ training, following the analysis Smith et al. (2021); Liu et al. (2020)that adding spectral normalisation before each convolution layer leads to bi-Lipschitz condition. Apart from being simpler in implementation (with minor changes to encoder architecture such as replacing L2 norm layer by spectral norm), spectral normalisation is significantly faster Smith et al. (2021) and is more stable during training compared to Jacobian Gradient penalty implemented in van Amersfoort et al. (2020).

Experiments

We will now demonstrate the utility of proposed framework in quantifying uncertainties of audio driven disease diagnosis. We first start with a brief description of datasets, evaluation criterion and implementation details. (detailed description and data histograms are covered in Appendix)

### Datasets

We conduct extensive experiments using two popular audio-driven healthcare diagnosis datasets.

The **ICBHI** Rocha et al. (2018) dataset is the largest publicly available respiratory audio repository recorded from 128 patients with a total of 6898 labelled breathing cycles (Label distribution 3642 normal, 1864 crackle, 886 wheeze, and 506 cycles as both). The highly unbalanced dataset constitutes a 4-class audio classification task.

**COSWARA** Sharma et al. (2020) consists of a diverse set of manually curated audio records from 2635 individuals, of which 1819 are SARS-CoV-2 negative, 674 are positive subjects, and the remaining unlabelled or noisy samples are filtered out. Speech recordings of numbers (1-20) counted at a fast pace were used for this 2-class classification and disease detection task.

### Self-supervised Audio Encoder

Self-supervised learning (SSL) is an attractive approach for healthcare audio datasets where the data size is limited and manual annotation is expensive Sharma et al. (2020); Rocha et al. (2018). Three different SSL models are employed as audio encoders for the empirical evaluation. First, an image-based **ResNet-50** is used as the backbone with a residual block of two \(3 3\) convolution layers and a skip connection between each block. The network is trained on the self-supervised task of spectral feature prediction and reconstruction of the log-Mel spectrogram. Further, **Wav2Vec** Baevski et al. (2020) and **PASE** Ravanelli et al. (2020) are used as direct waveform feature encoders. Each encoder is pre-trained on the respective SSL pretext task and used to obtain latent representations from raw audio.

Let \(a(t)\) be an input audio waveform and \(y=1, J\) be its corresponding label. The feature encoder gives embedding vectors \(X_{w}(a)^{d}\), where \(d=256\) is the fixed latent dimension.

### Preprocessing

All audio files were resampled to a fixed rate of 22.05kHz. The ICBHI respiratory sounds were cropped/padded to max a length of \(7\)s Gairola et al. (2021); Kulkarni et al. (2023), while COSWARA speech were fixed to \(10\)s length Sharma et al. (2020). In the case of ResNet, each audio was transformed to log Mel-spectrogram using \(128\) frequency bins. An input size of (128, 350) was used for ICBHI, whereas, for COSWARA, the input size was (128, 500). For both cases, the dataset was divided into three non-overlapping portions such that the test set (20%) and validation set (20%) contained audio records from different patients than that of the train set (60%).

### Evaluation

For measuring accuracy of model, **sensitivity**\(()\), and **specificity**\(()\) scores were used. Each score measures class-wise prediction accuracy in the case of the unbalanced dataset. The notations \(TN,FN\) denote true and false negative rates and \(TP,FP\) denote true and false positive rates, respectively. Average of these two scores \(()\) was used for comparison with SoTA models Rocha et al. (2018). The area under the receiver operating curve (**AUROC**) was used as an indicative probability of correctly classifying a randomly selected unseen sample.

Most common measure predictive uncertainty is Expected Calibration error (**ECE**). Low ECE indicates model accuracy closely follows predicted uncertainty estimates, i.e. low model accuracy in high-uncertainty regions and vice versa. To calculate ECE on a test set, all test samples are grouped in \(k=10\) equal bins according to uncertainty scores. ECE was calculated as the absolute sum of differences between expected model confidence and accuracy for each bin. A small ECE indicates better performance as the model accurately quantifies uncertainties in its prediction. Experimentsshow that ECE values drastically reduce with the proposed UQ implementation while maintaining the model's accuracy.

## 4 Results and Discussion

The first goal of the experiments is to answer the question 'whether model uncertainty score follows model accuracy'. Figure 7 show reliability diagrams of ICBHI 4-class classification model using PASE encoder as backbone. The model output is divided in equally spaced bins according to estimated confidence score for each bin. Reliability plots show the average accuracy of the examples in each corresponding confidence bin. We also visualise the confidence scores (1- uncertainty) with class conditional histograms of correctly and incorrectly classified outputs. The proposed model reliably predicts high uncertainty misclassified examples while producing high uncertainty for accurately classified examples.

A similar analysis is conducted for different choices of base feature encoder (Table 1) by considering the ECE (error) and AUROC (accuracy) of ICBHI respiratory classification task using different audio encoders (ResNet, PASERavanelli et al. (2020) and Wav2Vec Baevski et al. (2020)) with and without UQ estimation. A significant reduction in ECE values is observed among all three feature models. This means the model is more uncertain for false predictions and more confident for correct outputs. ResNet achieves higher relative improvement compared to direct waveform-based audio encoders. This is due to ResNet having higher embedding dimension compared to SSL encoders and thus adversely affecting the class conditional density estimation in latent space Ren et al. (2021). The low rank class-wise linear transformation enables distribution aware low dimensional transformation, improving both AUROC and ECE score.

**Classification Accuracy** and dataset variability of uncertainty aware models are compared in Table 2. Bootstrapping is used to compute the maximum confidence interval. The proposed model shows a significant advantage in ECE prediction over other UQ methods with marginal improvements in model accuracy.

**An ablation study** was conducted to study the incremental effects of various loss functions by fixing the feature encoder of the proposed UQ model. Table 2(a) displays the ECE and accuracy improvements with each additional loss term. A significant reduction in ECE error is observed upon the inclusion of triplet loss term for both datasets.

**Compute efficiency** of the proposed method, in terms of the number of parameters (in Millions) and inference time (in milliseconds), is compared with those of popular UQ models in Table 2(b). The scores show the expected inference time for a single sample averaged over the test set compared

   Model &  &  \\  & AUROC & ECE & AUROC & ECE \\  PASE & 0.835\(\)0.01 & 0.121\(\)0.01 & 0.905\(\)0.01 & 0.055\(\)0.01 \\ Wav2Vec & 0.778\(\)0.02 & 0.148\(\)0.01 & 0.812\(\)0.02 & 0.069\(\)0.01 \\ ResNet & 0.746\(\)0.01 & 0.106\(\)0.01 & 0.862\(\)0.01 & 0.041\(\)0.01 \\   

Table 1: Performance comparison of different base encoder models with and without uncertainty estimation (ICBHI)

Figure 3: Reliability diagrams before and after feature distance based uncertainty calibration. Plots show that proposed models predicts UQ scores that closely follow the model accuracy. (low confidence scores for low accuracy data regions and vice versa)

against AUROC scores. In this case, PASE is used as the base model. The ensemble model performed well but was extremely slow at inference time with a large number of parameters, increasing the storage and compute overhead. The Mahalanobis distance-based uncertainty estimation enables lightweight and fast inference while improving model accuracy.

**Comparison** with state-of-the-art (SoTA) models for ICBHI 4+class respiratory sound classification task is presented in Table 4. The proposed model improved the accuracy scores over the current SoTA by 6.1%. A validation set sensitivity score of 82.1% indicates the ability to correctly identify true positives from unseen patient samples recorded using different digital stethoscopes. Accounting for the uncertainties not only provides a nuanced understanding of output but also improves model performances for audio-driven disease diagnosis.

## 5 Uncertainty Visualisation and Decision Making

The outputs produced by sampling from Dirichlet distribution (output of probabilistic classifier for a single input) satisfy the property that \(_{j}(p_{j})=1\), where \(p_{j}\) is probability \(P[y=j|X]\). For a three class problem (ICBHI - wheeze, crackle, healthy), each of these samples fall on the 2D plane defined by \(_{j}(p_{j})=1\). Figure 4 shows uncertainty visualisations on the simplex plane. This uncertainty

   Method &  \\   & **SN(\%)** & **SP(\%)** & **Acc.** \\ ResNet Gaipka et al. (2021) & 40.1 & 72.3 & 56.2 \\ ResNet Sung and Wang (2022) & 70.4 & 40.2 & 55.3 \\ CNN8-Pt Ren et al. (2022) & 72.9 & 27.8 & 50.4 \\ ResNet Cang et al. (2022) & 69.9 & 35.8 & 52.9 \\ CVAE-Tr Bae et al. (2023) & 81.7 & 43.1 & 62.4 \\ Our (UQ) **ECE-** & **0.058** & **82.1\({}_{ 4.07}\)** & **55.1\({}_{ 3.75}\)** & **68.5\({}_{ 3.92}\)** \\   

Table 4: Comparison with SoTA models and recent studies on four-class respiratory anomaly detection (ICBHI dataset)

   Model & ECE & SN(\%) & SP(\%) & AUROC \\   & _{}\)**} \\ Base & 0.161\({}_{ 0.01}\) & 79.8\({}_{ 4.71}\) & 50.5\({}_{ 6.21}\) & 0.782\({}_{ 0.01}\) \\ MC Drop. & 0.064\({}_{ 0.01}\) & 79.6\({}_{ 5.31}\) & 42.6\({}_{ 5.91}\) & 0.732\({}_{ 0.02}\) \\ Ensemble & 0.051\({}_{ 0.01}\) & 83.1\({}_{ 3.71}\) & 57.7\({}_{ 1.91}\) & 0.888 \({}_{ 0.01}\) \\
**Our (UQ)** & **0.045\({}_{ 0.01}\)** & **82.1\({}_{ 4.07}\)** & **55.1\({}_{ 3.75}\)** & **0.823\({}_{ 0.01}\)** \\   &  \\ Base & 0.191\({}_{ 0.02}\) & 96\({}_{ 3.32}\) & 72.9\({}_{ 2.21}\) & 0.781\({}_{ 0.01}\) \\ MC Drop. & 0.074\({}_{ 0.01}\) & 96\({}_{ 5.59}\) & 70\({}_{ 4.19}\) & 0.951\({}_{ 0.01}\) \\ Ensemble & 0.060\({}_{ 0.01}\) & 96.6\({}_{ 3.15}\) & 77.9\({}_{ 4.98}\) & 0.964\({}_{ 0.01}\) \\
**Our (UQ)** & **0.058\({}_{ 0.01}\)** & **95.9\({}_{ 4.81}\)** & **74.6\({}_{ 2.91}\)** & **0.961\({}_{ 0.01}\)** \\   

Table 2: Evaluation of the UQ framework for two different datasets with fixed feature encoder (PASE)

  
**Model** & **ECE** & **AUROC** &  \\  Old (Softmax) & 0.158\({}_{ 0.01}\) & 0.741\({}_{ 0.02}\) & Base (Dirichlet) & 0.732 & 26M & 1.8 ms \\ + KL Divergence & 0.104\({}_{ 0.01}\) & 0.921\({}_{ 0.02}\) & Ensemble - 15 & 0.888 & 132M & 9.8 ms \\ + Triplet loss & 0.086\({}_{ 0.01}\) & 0.923\({}_{ 0.02}\) & Ensemble - 15 & 0.891 & 395M & 29 ms \\ + Regularisation & 0.065\({}_{ 0.01}\) & 0.918\({}_{ 0.01}\) & Mahalanobis & 0.823 & 26M & 2.1 ms \\   

Table 3: Ablation study (a) of proposed UQ framework to study effects of modification terms, along with network size (Millions) and inference time (sec) of different UQ models(Results on non-intersecting splits of ICBHI dataset with PASE as feature encoder)is sample specific (data/ aleatoric uncertainty) indicating inherent label noise or ambiguity in the samples.

At the same time, the model uncertainty (predictive) is given by mixture of the Gaussian models fitted at each class centroid \(d_{}= j(D_{j}|C_{j},_{j})\). This estimate is independent model prediction at a given sample. This is a measure of learning capacity of model for current input and can also be used as OOD indicator (epistemic uncertainty). A threshold on the UQ score can be used as a decision factor for audio-driven medical diagnosis. If the predicted UQ value is higher than this threshold, the model is not sufficiently confident in its prediction; thus, the disease diagnosis output is rejected. In such cases, second or multiple evaluations using re-recording of input audio samples are recommended. If the resulting uncertainty, after multiple empirical evaluations, is still higher than the threshold, then the particular sample is selected for clinical or manual diagnosis. This avoids the risk of erroneous predictions via uncertainty quantification. As a result, the proposed framework improves the performance of audio-driven disease detection system along with patient safety. (Such threshold based rejection was not used during experiments and results, however it can be a useful tool for medical decision making)

## 6 Conclusion

In this work, a framework for uncertainty-aware disease diagnosis was proposed using speech and non-speech inputs. The UQ framework enables confidence scoring to improve the reliability of model outputs.Evaluations of the popular COSWARA and ICBHI datasets illustrate the superiority of the proposed model over the popular ensemble and Monte Carlo dropout method. Using the same ResNet backbone, the UQ aware model outperformed softmax-based SoTA models for respiratory diseaseBae et al. (2023) without using data driven oversampling techniques. Using the UQ model for the ICBHI dataset, an improvement of 6.1% was observed over the SoTA models. Furthermore, for speech-driven COVID detection, quantifying data uncertainty improves AUROC scores by 18.1%. The UQ model performs well on unseen datasets, as seen from results on non-intersecting inter-patient data splits, and is equally applicable to more general datasets. Results also show the effectiveness and applicability of the Mahalanobis distance-based metric for different general-purpose audio encoders. Finally, the proposed framework enables fast and lightweight UQ estimation, making it more suitable for implementation in mobile and IoT devices for continuous health monitoring owing to its small size and lower number of trainable parameters.

Figure 4: Plots visualising data uncertainty corresponding to each input audio sample. The network predicts Dirichlet distribution parameters (\(\)) in single forward pass which are then used to plot probability density over the simplex.