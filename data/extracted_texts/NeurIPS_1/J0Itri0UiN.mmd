# Counterfactual Fairness by Combining Factual and Counterfactual Predictions

Zeyu Zhou, Tianci Liu, Ruqi Bai, Jing Gao, Murat Kocaoglu, David I. Inouye

Elmore Family School of Electrical and Computer Engineering

Purdue University

{zhou1059, liu3351, bai116, jinggao, mkocaoglu, dinouye}@purdue.edu

###### Abstract

In high-stakes domains such as healthcare and hiring, the role of machine learning (ML) in decision-making raises significant fairness concerns. This work focuses on Counterfactual Fairness (CF), which posits that an ML model's outcome on any individual should remain unchanged if they had belonged to a different demographic group. Previous works have proposed methods that guarantee CF. Notwithstanding, their effects on the model's predictive performance remain largely unclear. To fill this gap, we provide a theoretical study on the inherent trade-off between CF and predictive performance in a model-agnostic manner. We first propose a simple but effective method to cast an optimal but potentially unfair predictor into a fair one with minimal performance degradation. By analyzing the excess risk incurred by perfect CF, we quantify this inherent trade-off. Further analysis on our method's performance with access to only incomplete causal knowledge is also conducted. Built upon this, we propose a practical algorithm that can be applied in such scenarios. Experiments on both synthetic and semi-synthetic datasets demonstrate the validity of our analysis and methods.

## 1 Introduction

Machine learning (ML) has been widely used in high-stakes domains such as healthcare (Daneshjou et al., 2021), hiring (Hoffman et al., 2018), criminal justice (Brennan et al., 2009), and loan assessment (Khandani et al., 2010), bringing with it critical ethical and social considerations. A prominent example is the bias observed in the COMPAS tool against African Americans in recidivism predictions (Brackey, 2019). This issue is particularly alarming in an era where large-scale deep learning models, commonly trained on noisy data from the internet, are increasingly prevalent. Such models, due to their extensive reach and impact, amplify the potential for widespread and systemic biases. This increasing awareness underscores the need for ML practitioners to integrate fairness considerations into their work, extending their focus beyond merely maximizing prediction accuracy (Bolukbasi et al., 2016; Calders and Verwer, 2010; Dwork et al., 2012; Grgic-Hlaca et al., 2016; Hardt et al., 2016). Various fairness notions have been developed, ranging from group-level measures such as group parity (Hardt et al., 2016) to individual-level metrics (Dwork et al., 2012). Recently, there has been a growing interest in approaches based on causal inference, particularly in understanding the causal effects of sensitive attributes such as _gender_ and _age_ on decision-making (Chiappa, 2019; Galhotra et al., 2022; Khademi et al., 2019). This has led to the proposal of Counterfactual Fairness (CF), which states that prediction for an individual in hypothetical scenarios where their sensitive attributes differ should remain unchanged (Kusner et al., 2017). As an individual-level notion agnostic to the choice of similarity measure (Kusner et al., 2017; Rosenblatt and Witter, 2023), CF has recentlygained traction (Anthis and Veitch, 2024, Nilforoshan et al., 2022, Makhlouf et al., 2022, Rosenblatt and Witter, 2023).

To achieve CF, Kusner et al. (2017) first proposed a naive solution, suggesting that predictions should only use non-descendants of the sensitive attribute in a causal graph. This approach only requires a causal topological ordering of variables and achieves perfect CF by construction. However, it limits the available features for downstream tasks and could be inapplicable in certain cases (Kusner et al., 2017). For relaxation, they further proposed an algorithm that leverages latent variables. Extending this line of work, Zuo et al. (2023) introduced a technique that incorporates additional information by mixing factual and counterfactual samples. Although perfect CF has been established in their work, its predictiveness degraded, and whether the predictive power can be improved remains unknown. In parallel to this, another branch of research employed regularization and augmentation to _encourage_ CF (Garg et al., 2019, Stefano et al., 2020, Kim et al., 2021). However, as these methods cannot guarantee perfect CF, analyzing the optimal predictive performance under CF constraints is highly challenging.

To theoretically understand the tradeoff between CF and ML performance, we consider a class of invertible causal models and prove that the optimal solution under perfect Counterfactual Fairness (CF) has a simple form w.r.t. to the Bayes optimal classifier and explicitly quantify the excess risk of imposing a perfect CF constraint as has been done for non-causal fairness notions(Zhao and Gordon, 2022, Xian et al., 2023). The optimal predictor under the fairness constraint can be achieved by combining factual and counterfactual predictions using a (potentially unfair) optimal predictor. Next, we quantify the excess risk between the optimal predictor with and without CF constraints. This quantity sheds light on the best possible model, in terms of predictive performance, under the stringent notion of perfect CF. Our results are illustrated in Figure 1. To consider scenarios with incomplete causal knowledge (e.g. unknown causal graph or model), we further study the CF and predictive performance degradation caused by imperfect counterfactual estimations. Inspired by our theoretical findings, we propose a plugin method that leverages a (potentially unfair) pretrained model to achieve a better tradeoff of fairness and predictive performance than the prior methods. Furthermore, we propose a method to improve the pretrained model that accounts for counterfactual estimation errors and can achieve good empirical performance even with limited causal knowledge. We summarize our contributions as follows:

1. We propose a CF method that is provably optimal in terms of predictive performance under perfect CF.
2. To the best of our knowledge, we are the first to characterize the inherent trade-off of CF and ML performance, which applies to all CF methods.
3. We investigate the CF and predictive performance degradation from estimation error resulting from limited causal knowledge and propose methods to mitigate estimation errors in practice.
4. We empirically demonstrate that our proposed CF methods outperform existing methods in both full and incomplete causal knowledge settings 2.

Figure 1: The optimal (unfair) predictor (a) violates counterfactual fairness in the middle region because the predictions are different for the factual-counterfactual pairs1 (denoted by line segments between \(a=0\) and \(a=1\)). We prove that the optimal fair predictor (b) simply mixes the optimal unfair predictions at the factual and counterfactual points (i.e., mixes the predictions at both endpoints of the line). This mixing incurs the inherent excess risk associated with counterfactual fairness. Colors represent target classes (\(Y\)), and dot styles represent sensitive attributes (\(A\)).

Preliminaries

NotationWe use capital letters to represent random variables and lowercase letters to represent the realizations of random variables. Now we define a few variables that will be considered in this work. \(A\) represents the sensitive attribute of an individual (e.g., gender), \(Y\) represents the target variable to predict, \(X\) represents observed features other than \(A\) and \(Y\), and \(U\) represents unobserved confounding variables which are not caused by any observed variables while \(a,y,x,u\) represent their realization respectively.

CounterfactualIn this work, we use the framework of Structural Causal Models (SCMs) [Pearl, 2009]. A SCM is a triplet \(=(,,)\) where \(\) represents exogenous variables (factors outside the model), \(\) represents endogenous variables, and \(\) contains a set of functions \(F_{i}\) that map from \(U_{i}\) and \(Parent(V_{i})\) to \(V_{i}\). A counterfactual query asks a question like: what would the value of \(Y\) be if \(A\) had taken a different value given certain observations? For example, given that a person is a woman and given everything we observe about her performance in an interview, what is the probability of her getting the job if she had been a man? More formally, given a SCM, a counterfactual query can be written as \(P(Y_{A=a}|W=w)\). Here \(W=w\) is the evidence and \(A=a\) in the subscript represents the intervention on \(A\). For the general procedure to estimate counterfactuals, please refer to Pearl (2009).

Counterfactual FairnessBuilt upon the framework above, we focus on Counterfactual Fairness (CF), which requires the predictors to be fair among factual and counterfactual samples. More formally, it is defined as below

**Definition 2.1**.: _(Counterfactual Fairness) We say a predictor \(\) is counterfactually fair if_

\[p(_{A=a}|X=x,A=a)=p(_{A=a^{}}|X=x,A=a),(x,a).\]

This definition states that intervention on \(A\) should not affect the distribution of \(\). Using the same example above, the probability of a woman getting the job should be the same as that if she had been a man. For that goal, we use the following metric to evaluate CF

**Definition 2.2**.: _(Total Effect) The Total Effect (TE) of a predictor \(\) is_

\[[|_{A=a}-_{A=a^{}}|].\]

Therefore, a predictor is counterfactually fair if and only if \(=0\). Throughout the paper, we use TE to quantify the violation of counterfactual fairness.

## 3 Counterfactual Fairness via Output Combination

### Problem Setup

We assume that all data we have is generated by a causal model [Pearl, 2009], and we consider the representative causal graph shown in Figure 2 that has been widely adopted in the fairness literature [Grai et al., 2023, Kusner et al., 2017, Zuo et al., 2023]. Our analysis is presented based on binary \(A\{0,1\}\) given its pivotal importance in the literature [Pessach and Shmueli, 2023] and for the sake of presentation clearness, but our analysis and our method can be naturally extended to multi-class \(A\). We first state the main assumptions needed in this section.

**Assumption 3.1**.:
1. _A and_ \(U\) _are independent of each other._
2. _The mapping between_ \(X\) _and_ \(U\) _is invertible given_ \(A\)_._

The first assumption is very common in the fairness literature. While the invertibility assumption might be restrictive in certain scenarios, it simplifies the theoretical analysis and has been adopted in

Figure 2: Causal graph. \(A\) represents sensitive attribute, \(Y\) represents the target variable, \(U\) represents latent confounders, \(X\) represents observed features. Note that the validity of our theoretical analysis holds for all causal models that satisfy the condition given by Assumption 3.1. It is not restricted to this specific graph.

recent works on counterfactual estimation (Nasr-Esfahany et al., 2023; Zhou et al., 2024). We expect that exact invertibility is not required in practice but rather only strong mutual information between \(X\) and \(U\) given \(A\) would be sufficient. Further, we empirically validate the effectiveness of our method after relaxing invertibility in the experiment section. To facilitate our discussion, we first define \(F_{X}\) as the mapping between \(X\) and \((U,A)\), i.e., \(X=F_{X}(U,A)\). According to our second assumption, \(F_{X}(,a)\) is an invertible function, i.e., \(^{*}}^{-1},{F_{X}^{*}}^{-1}(x,a)={F_{X}^{*}}^{-1}(F_{X}^{*}(u,a),a)=u,(x,a)\). This assumption simplifies the counterfactual estimation of \(X\) for different values of \(A\) into a deterministic function. In our context, the counterfactual query is specifically \(p(X_{^{}}|X=x,A=a)\), which simplifies to a Dirac delta at a single value given the invertibility assumption. Thus, we introduce the concept of a deterministic counterfactual generating mechanism (CGM), denoted as \(x_{a^{}}=G(x,a,a^{})\). Also, in our case, we will assume \(A\) is binary so that \(a\) and it's counterfactual \(a^{}\) can be written as \(1-a\). All proofs can be found in Appendix A. Given this setup, the following lemma characterizes the perfect CF constraint on \(\).

**Lemma 3.2**.: _Given Assumption 3.1, predictor \(\) on \((X,A)\) is counterfactually fair if and only if the predictor returns the same value for a sample and its counterfactuals, i.e., \(()=0(x,a)}{{=}} (x_{1-a},1-a),(x,a)\)._

The proof is straightforward from the definition of TE. Notably, this lemma helps disambiguate the question of whether counterfactual fairness is a distribution- or individual-level requirements as raised in Plecko and Bareinboim (2022). In our setup, they are equivalent due to invertibility between \(X\) and \(U\) given \(A\).

### Optimal Counterfactual Fairness and Inherent Trade-off

Given the complete knowledge of the causal model, it is viable to satisfy the perfect CF constraint (Kusner et al., 2017; Zuo et al., 2023). However, these methods are known to result in _empirical_ degradation of ML models' performance, raising critical concerns about the _fairness-utility trade-offs_. Moreover, it is still unknown _to what extent the ML model performance has to be affected in order to achieve perfect CF_. In this section we provide a formal study on this to close the gap. Our solution consists of two steps. First, we propose a simple yet effective method that is provably optimal under the constraint of perfect CF. Next, we characterize the inherent trade-off between CF and predictive performance by checking the excess risk compared to a Bayes optimal (unfair) predictor. Our result shows that the inherent trade-off is dominated by the dependency between \(Y\) and \(A\), echoing previous analysis on non-causal based fairness notions (Chzhen et al., 2020; Xian et al., 2023). For brevity, we refer to a _Bayes optimal_ predictor as "optimal", and a model that satisfies _perfect CF_ as "fair".

We start with the following theorem instantiating an _optimal and fair_ predictor.

**Theorem 3.3**.: _Given Assumption 3.13 and loss \(\) (i.e., squared \(L_{2}\) loss for regression tasks, and cross-entropy loss for classification tasks), an optimal and fair predictor (i.e., the best possible model(s) under the constraint of perfect CF) is given by the average of the optimal (potentially unfair) predictions on itself and all possible counterfactuals:_

\[^{*}_{}(x,a) p(A\!=\!a)^{*}(x,a)+p(A\!=\!1\!-\!a) ^{*}(x_{1-a},1\!-\!a)*{argmin}_{\!\!( )=0}[((X,A),Y)]\,,\]

_where \(x_{1-a}=G^{*}(x,a,1\!-\!a)\) is the counterfactual of \((x,a)\) when intervening with \(A=1\!-\!a\), and \(^{*}(x,a)\) is an unconstrained optimal predictor, i.e., \(^{*}(x,a)*{argmin}_{}[((X,A), Y)]\!=\![Y|X\!=\!x,A\!=\!a]\)._

This result suggests that, if we have to access to ground truth counterfactuals, a simple algorithm using a (potentially) unfair model could achieve strong fairness and accuracy. Built upon the above result, we are ready to characterize the inherent trade-off between CF and model performance by the following theorem.

**Theorem 3.4**.: _The inherent trade-off between CF and predictive performance, characterized by the excess risk of the Bayes optimal predictor under the CF constraint, is given by_

\[^{*}_{}-^{*}=_{A}^{2}_{U} [(_{Y|U=u,A=a}[Y]-_{Y|U=u,A=1-a}[Y])^{2} ],\]for regression tasks using squared \(L_{2}\) loss where \(_{A}^{2}\) denotes the variance of \(A\); and_

\[^{*}_{}-^{*}=I(A;Y U),\]

_for classification tasks using cross-entropy loss._

Remarkably, the excess risks are completely characterized by the _inherent_ dependency between \(Y\) and \(A\) as determined by the underlying causal mechanism, similar to non-causal based group fairness (Chzhen et al., 2020; Xian et al., 2023). Moreover, they lower bound the excess risk of all possible predictors in order to achieve perfect CF.

### Method with Incomplete Causal Knowledge

In this section, we aim to address CF in the scenario where causal knowledge is limited. Inspired by Theorem 3.3, we first present a simple plugin method as summarized in Algorithm 1. For regression tasks, \(\) is the final output, and for classification tasks, \(\) represents the probability of \(Y=1\), i.e., \(p(Y=1|X=x,A=a)=[Y|X=x,A=a]\). It is noteworthy that PCF is agnostic to the training of predictor \(\) that can be determined by the user freely. In fact, with access to the oracle CGM \(G^{*}\), then PCF would achieve perfect CF as proved in the next result.

```
Input: Pretrained probabilistic prediction predictor \(:\), CGM \(G\), test datapoint \((x,a)\), prior distribution \(p\) of \(A\) Output: Predicted output \(\) \(_{1-a} G(x,a,1-a)\) \( p(A=a)(x,a)+p(A=1-a)(_{1-a},1-a)\)
```

**Algorithm 1** Plug-in Counterfactual Fairness (PCF)

**Proposition 3.5**.: _Given that \(G\) is the ground truth counterfactual generating mechanism, i.e., \(G(x,a,a^{})=x_{a^{}},(x,a,a^{})\), Algorithm 1 achieves perfect CF for any pretrained predictor \(\)._

Note that this proposition only requires access to ground truth \(G^{*}\) and holds valid _for any pretrained predictor \(\)_. If \(\) is further accurate, then the corresponding PCF is able to achieve high accuracy as well, which is empirically validated in the experiments.

#### 3.3.1 Given estimated \(G\)

Acquiring counterfactuals in practice can be a challenging task and could lead to estimation errors. In this section we provide a theoretical analysis on this. Specifically, the theorem below bounds the TE and excess risk due to the use of estimated counterfactuals.

**Theorem 3.6**.: _Given an optimal predictor \(^{*}(x,a)\), suppose it is L-lipschitz continuous in \(x\), and the counterfactual estimation error is bounded, i.e.,_

\[_{X,A}\|G^{*}(x_{a},a,1-a)-(x_{a},a,1-a)\|_{2}\]

_for some \( 0\), where \(G^{*}\) and \(\) represent the ground truth and estimated CGMs respectively. Then, the total effect (TE) of Algorithm 1 based on \(\) is bounded by \(L\). Moreover, for squared \(L_{2}\) loss, the excess risk is bounded by \(_{A}^{2}L^{2}^{2}+2_{A}^{2}L_{U} [\|[Y U=u,A=1]-[Y U=u,A=0]\|]\), and for cross-entropy loss 4, the excess risk is bounded by \(L\)._

Note that \(_{A}\) and \(_{U}[\|[Y U=u,A=1]-[Y U=u,A=0]\|]\) are inherent characteristic of the underlying mechanism and is independent of the counterfactual estimation. This suggests that if the counterfactuals are not too far away and \(^{*}\) is smooth, then fairness and prediction performance will not be significantly affected. In practice, CGM in Algorithm 1 can be obtained using counterfactual estimation methods, as discussed in Section 6.

#### 3.3.2 Given estimated \(G\) and \(\)

In the previous section, we discussed how counterfactual estimation error directly impacts the performance of PCF in terms of CF and predictive performance. Here, we consider the situation where \(\) also needs to be estimated. We first note that the degradation in fairness remains the same as previous result

**Remark 3.7**.: _The bound of TE given \(\) and \(\) follows that in Theorem 3.6._

The proof is straightforward since the original proof in Theorem 3.6 does not use any characteristic of optimality. To achieve good predictive performance, a natural approach is to train \(\) on the observed data via Empirical Risk Minimization (ERM), which should fit the predictor well given sufficient samples and a reasonable predictor class. However, ERM can only approximate Bayes optimality within the support of the training data. Outside this support, its performance can deteriorate significantly, as extensively studied in areas such as Domain Adaptation (Farahani et al., 2021) and Domain Generalization (Zhou et al., 2022). Consequently, when integrated with an approximate \(G\), we may encounter the issue where \(p(_{A=1-a}|A=a) p(X|A=1-a)\), inducing a distribution shift problem (Note that these would be equal given the graph in Figure 2 and Pearl's rules) (Kulinski and Inouye, 2023). To mitigate this, we suggest improving \(\) on the estimated counterfactual distribution. More formally we define the following objective called Counterfactual Risk Minimization (CRM):

\[_{}_{X,A,Y}[((X,A),Y)+((G(X,A,1-A),A),Y)]\]

This can be achieved either by augmenting the original training dataset or by fine-tuning with estimated counterfactual samples. The choice between training from scratch or fine-tuning depends on the scale of the experiment and computational constraints. It is important to note that the \(Y\) corresponding to the estimated counterfactual should remain the same with that of the factual samples. While the optimal prediction for the counterfactual may differ from that of the original data, under the constraint of perfect CF, a predictor is required to predict the same outcome to counterfactual pairs. Hence, the optimal solution will change. This is exactly what causes the excess risk we characterized in Theorem 3.4. Furthermore, we can prove that, given the ground truth \(G\), CRM yields the same optimal solution as PCF. Since this result is dependent on the ground truth \(G\), we provide a more formal statement in Appendix B to ensure consistency.

In summary, this section discusses how to improve the estimation of \(\) under counterfactual estimation error. We propose using data augmentation or fine-tuning based on practical scenarios. Additionally, in domains with abundant off-the-shelf pre-trained models (Bommasani et al., 2021), we can potentially avoid this issue by using these models as a good proxy for \(\).

## 4 Related Works

**Fairness Notions** Fair Machine Learning has accumulated a vast literature that proposes various notions to measure fairness issues of machine learning models. Representative fairness notions can be categorized into three classes. _Group fairness_, such as demographic parity (Pedreshi et al., 2008) and equalized odds (Hardt et al., 2016), requires certain group-level statistical independence between model predictions and individuals' demographic information. Despite its conceptual simplicity, group fairness is known for ruling out perfect model performance (Hardt et al., 2016) and may allow for bias against certain individuals (Corbett-Davies et al., 2023). _Individual fairness_(Dwork et al., 2012), on the other hand, asks a model to treat similar individuals similarly. However, determining the similarity between different individuals is often highly task-specific and open-ended. Recently, _counterfactual fairness_(CF, Kusner et al. (2017)) further takes the causal relationship of data attributes into consideration when measuring fairness. In words, counterfactual fairness proposes that a model should treat any individual the same as their _counterfactual_ if the individual had been from another demographic group. As an individual-level notion agnostic to the choice of similarity measure (Kusner et al., 2017), CF has recently gained traction (Wu et al., 2019; Nilforoshan et al., 2022; Makhlouf et al., 2022; Rosenblatt and Witter, 2023). Motivated by these recent advances, in this work, we focus on the counterfactual fairness.

**Methods for Fairness** Given an unfair dataset, attempts to achieve fairness fall into three categories. _Pre-processing_ cleans the data before running machine learning models on it, typically by resamplingsamples or removing undesired attributes (Kamiran and Calders, 2012). _In-processing_ intervenes the model-training process by incorporating fairness constraints (Zafar et al., 2017; Donini et al., 2018; Lohaus et al., 2020) or penalties (Mohler et al., 2018; Scutari et al., 2021; Liu et al., 2023). _Post-processing_ adjusts the raw model outputs to close the bias gap by, e.g., assigning each demographic group a unique decision threshold (Jang et al., 2022). Post-processing has been favored as an efficient and practical solution because it does not require retraining the original model (Petersen et al., 2021; Xian et al., 2023). To achieve CF, Kusner et al. (2017) applied pre-processing and discarded all descendants of the sensitive feature. Chen et al. (2024) pre-processed the data via orthogonalization and marginal distribution mapping. Garg et al. (2019); Stefano et al. (2020); Kim et al. (2021) in-processed the model training by penalizing CF violations but their solutions lack formal CF guarantees and often contain unsatisfactory bias after the intervention (Zuo et al., 2023). Recently, Zuo et al. (2023) proposed another in-processing based solution that is capable of achieving perfect CF and better performance via mixing features. Ma et al. (2023) leveraged mediators estimated by Generative Adversarial Networks and provided a theoretical guarantee of CF under well-estimated counterfactuals. However, it is unclear whether their methods are optimal. Wang et al. (2023) leveraged predictor that satisfies equal counterfactual opportunity criterion to construct a counterfactually fair predictor. While they provide results on optimality, their findings assume an ideal setting where non-sensitive features are independent of sensitive features.

**Inherent Trade-off between Fairness and Predictiveness** Machine learning models are known to suffer from performance drops after fairness interventions (Hardt et al., 2016; Menon and Williamson, 2018; Chen et al., 2018), which is known as the _fairness-utility_ trade-offs. Recently, inherent trade-offs towards non-causal based fairness such as demographic parity (DP) has been established separately for regression (Chzhen et al., 2020) and classification tasks (Xian et al., 2023). The excess risks are characterized by certain distribution distance (i.e., Wasserstein-2 barycenter for regression, and total-variation or Wasserstein-1 barycenter for noiseless or noisy classification) between the conditional distribution of \(Y\) given \(A\). A similar trade-off between CF and predictiveness has also been empirically observed (Zuo et al., 2023). Nonetheless, their inherent trade-off remains an open question. In this work we take the first step towards this goal and provide a quantitative analysis in both complete and incomplete causal knowledge settings as presented in Section 3. We hope our work sheds light on future works towards more effective CF.

## 5 Experiments

In this section, we validate our theorems and the effectiveness of our algorithms through experiments on synthetic and semi-synthetic datasets. On synthetic datasets, we focus on validating our theorems in settings where our assumptions hold. On semi-synthetic datasets, we aim to assess the effectiveness of our methods in more practical scenarios, where limited causal knowledge is available and the invertibility assumption is relaxed.

MetricsWe consider two metrics in this paper: Error and Total Effect (TE). The former evaluates whether each method can achieve its goal, irrespective of fairness. This is important because we can achieve perfect Counterfactual Fairness by always outputting fixed prediction given whatever input, but that is not useful at all. The latter is a common metric to evaluate Counterfactual Fairness (Kim et al., 2021; Zuo et al., 2023). Given a test set \(_{}\), Error is defined as \(=_{}|}_{x^{(i)} _{}}((x^{(i)}),y^{(i)})\) where \(y^{(i)}\) is the ground truth target, \((x^{(i)})\) is the prediction of \(x^{(i)}\), and \(\) depends on the task. TE is defined as \(=_{}|}_{x^{(i)}_{ }}|(x^{(i)})-(x^{(i)}_{1-a})|\) where \(x_{1-a}\) is the ground truth counterfactual corresponding to \(x^{(i)}\). Since we only consider binary sensitive attribute, we further define \(_{0}==0\}|}_{i:a^{(i)}=0}|(x^{(i)} )-(x^{(i)}_{1-a})|\) and \(_{1}==1\}|}_{i:a^{(i)}=1}|(x^{(i) })-(x^{(i)}_{1-a})|\) to evaluate Counterfactual Fairness for different group respectively.

MethodsIn general, we consider the following methods: (1) **Empirical Risk Minimization (ERM):** Train a classifier on all features without any fairness consideration. Specifically \(=(x,a),\) where \(\) represents the predictor. (2) **Counterfactual Fairness with \(U\) (CFU) (Kusner et al., 2017):** To achieve Counterfactual Fairness, CFU proposes to use \(U\) for prediction. Specifically, \(=(u)\). (3) **Counterfactual Fairness with fair representation (CFR) (Zuo et al., 2023):** CFRproposes to use \(U\) and a symmetric version of \(x,x_{1-a}\). Specifically, \(=(}{2},u)\). (4) **Equal Counterfactual Opportunity (ECOCF)[Wang et al., 2023]:** An ECO predictor is adjusted to become counterfactually fair. Specifically, \(=p(a)[p(a)(x,a)+(1-p(a))(x,1-a)]+(1-p(a))[(1-p(a))(x_{1-a},1-a )+p(a)(x_{1-a},a)]\) (5) **PCF5:** As introduced in Algorithm 1, PCF mixes the output of factual and counterfactual prediction. Specifically, \(=p(a)(x,a)+(1-p(a))(x_{1-a},1-a)\). (6) **PCF with analytic solution (PCF-Ana):** In synthetic experiments, instead of training via ERM, we can directly acquire bayes optimal \(\) in closed-form. Detailed can be found in Appendix C.2. (7) **PCF with CRM (PCF-CRM):** As discussed in Section 3.3, it could be hard to get the optimal predictor when there is counterfactual estimation error. Here due to the scale of our experiment, we augment the dataset with estimated counterfactuals rather than finetuning. Specifically, \(\) is trained via ERM on the dataset \(_{}=\{x^{(i)},y^{(i)},a^{(i)}\}_{i=1}^{N}\{_ {1-a}^{(i)},y^{(i)},1-a^{(i)}\}_{i=1}^{N}\).

### Synthetic Dataset

In this section, we consider two regression synthetic datasets and two classification tasks where all of our assumptions in Assumption 3.1 are satisfied. The regression tasks are as below

_Linear-Reg_ _Cubic-Reg_ \[A (p_{A}),U(0,1),_{Y} (0,1) A (p_{A}),U(0,1),_{Y} (0,1)\] \[X =w_{A}A+w_{U}U X =w_{A}A+w_{U}U\] \[Y =w_{X}X+w_{U}^{}U+w_{Y}_{Y} Y =w_{X}X^{3}+w_{U}^{}U+w_{Y}_{Y}\]

The classification tasks take the same form except \(Y((w_{X}X+w_{U}^{}U+w_{Y}_{Y})\) and \(Y((w_{X}X^{3}+w_{U}^{}U+w_{Y}_{Y}))\) for _Linear-Cls_ and _Cubic-Cls_ respectively. More details could be found in Appendix C.1. Results are averaged over 5 different runs where the structural model is kept the same but data is resampled. All results shown in the main paper use KNN based predictor. Results with other predictors can be found in Appendix D.

Optimality of PCF given true counterfactualsWe first test different methods in situations where all methods have access to ground truth counterfactuals and \(U\) as needed. In Figure 3, we observe that while CFE, CFR and PCF all achieve perfect CF, PCF has lowest predictor error. This validates Theorem 3.3 regarding the optimality of PCF under the constraint of CF. Furthermore, since here ERM can get solution close to optimal predictor (this indicates the plugin \(\) used by PCF is also close to being optimal), we can also observe the inherent fairness-utility trade-off discussed in Theorem 3.4.

Performance under controllable errorHere we investigate a more practical scenario where both counterfactuals and \(U\) need to be estimated. To investigate how error and TE changes with counterfactual estimation error in a more controllable way and investigate, we simulate the estimation error by adding gaussian noise. Specifically, \(_{a^{}}=x_{a^{}}+\) and \(=u+\) where \((,)\). In Figure 4, we observe that while the fairness and ML performance (especially fairness) of CFE, CFR and PCF tends to get worse as error gets more significant, PCF remains best for all noise level.

Investigating source of errorHere we further investigate what could be source of error in the previous scenario. As discussed in Section 3.3.2, in practice, two things in Theorem 3.3 break down:

Figure 3: Results on synthetic datasets given ground truth counterfactuals.

access to Bayes optimal classifier and ground truth counterfactuals. In Figure 5, we observe that PCF-Analytic tends to be more robust against counterfactual estimation error than PCF. We argue this is because \(\) used in PCF is not trained well on the estimated counterfactual distribution.

### Semi-synthetic Dataset

In this section, we consider Law School Success dataset (Wightman, 1998) where the sensitive attribute is gender and the target is first-year grade.. The main goal of this experiment is to validate the effectiveness of our methods in more practical scenarios where limited causal knowledge is available and the invertibility assumption is relaxed.

To compute TE, we need access to ground truth counterfactuals. Hence we train a generative model on real dataset to generate semi-synthetic dataset following the method in Zuo et al. (2023). We want to emphasize that counterfactuals are hidden from downstream models and used for the evaluation of TE only. This way, we get access to the ground truth \(u^{*}\) and can generate ground

Figure 4: Results on synthetic datasets under counterfactual estimation error. Different color represents different \(\) indicating the standard deviation of the error (\((0,)\)) while shape represents different algorithms. Results with different \(\) can be found in Appendix D.

Figure 5: Results on synthetic datasets comparing PCF and PCF-Analytic. Different color represents different \(\) indicating the standard deviation of the error (\((0,)\)) while shape represents different algorithms. Results with different \(\) can be found in Appendix D.

truth counterfactuals without any error. In our investigation, exogenous noise, factual data and counterfactual data are all actually the simulated version of original datasets. However they do follow a fixed data generating mechanism that is close to the real data. More details could be found in Appendix C.1. All experiments are repeated 5 times on the same semi-synthetic dataset.

ResultsIn Figure 6, we observe that PCF-CRM achieves better CF and lower Error in comparison to CFU and CFR. This validates our improvement Section 3.3.2 indeed leads to more practical algorithm. Results on comparing PCF and PCF-CRM can be found in Appendix D, which further justifies this. While ERM could achieve lower error, it has worst fairness. This is inevitably determined by the inherent trade-off discussed in Theorem 3.4. Furthermore, inspired by the trade-off, we test the result of mixing all predictors with ERM. The curve shows that PCF-CRM remains optimal given fixed CF and best CF given fixed error. This demonstrates again that PCF-CRM is the best among all methods.

## 6 Conclusion and Discussion

ConclusionIn this work, we conducted a formal investigation of the trade-off between Counterfactual Fairness (CF) and predictive performance. We proved that combining factual and counterfactual predictions with a potentially unfair, optimal predictor achieves optimal CF. Additionally, we derived the excess risk between predictors with and without CF constraints, quantifying the minimum performance degradation necessary to ensure perfect CF. To address incomplete causal knowledge, we analyzed the effects of imperfect counterfactual estimations on CF and predictive performance. We proposed a plugin approach that leverages pre-trained models for optimal fair prediction and developed a practical method to mitigate estimation errors.

Despite the theoretical contributions of our method, two limitations may impact practical applicability: (1) access to ground-truth counterfactuals and (2) access to Bayes optimal predictors. Below, we delve into these limitations, clarifying how our methods can be practically applied and how they can benefit from contributions from the broader community. We hope this discussion will also inspire future research directions.

Access to ground truth counterfactualsWhile how to better estimate counterfactuals is out of the scope of this work, it is indeed an unavoidable challenge faced by the community of Counterfactual Fairness. It not only limits the deployment of CF algorithms, but also leads to difficulty in validating proposed CF methods. While counterfactual data can be obtained in specific scenarios, such as through randomized controlled trials, it is challenging to acquire in most applications. There are some works in the field of causality that aims at estimating counterfactuals. For instance, Nasr-Esfahany et al. (2023) proves counterfactual identifiability under certain causal graphs. However, in more general scenarios, such causal knowledge may be lacking and identifying the causal graph itself can be challenging. These tasks have been well studied in the field of causal discovery (Chickering, 2002; Colombo et al., 2014) and causal representation learning (Scholkopf et al., 2021). Solutions to this problem typically rely on strong assumptions, such as the linearity of Structural Causal Models (SCMs) or additive noise (Shimizu et al., 2006; Hoyer et al., 2008; Peters et al., 2014). More recently, Zhou et al. (2024) propose a method of estimating counterfactuals without the need to identify the causal model or graph. We believe this approach to direct counterfactual estimation could have the potential to be a good plugin counterfactual estimator in our algorithm. Additionally, generative models could also be used to generate samples as if they had come from a different sensitive attribute (Choi et al., 2018; Zhou et al., 2022, 2023; Rombach et al., 2022). These methods often offer the advantage of higher sample quality, especially in modalities such as images or natural language. However, they must be applied with considerable care, as they generally lack integration with the causal model and may introduce significant estimation errors.

Access to Bayes optimal predictorsAnother crucial plugin estimator of our method is the optimal predictor. In classical ML settings, achieving a good estimator for the counterfactual distribution often requires retraining or fine-tuning. However, in this era, with the abundance of pre-trained models, such as foundation models (Bommasani et al., 2021), it could be much easier to get a predictor that is close to being optimal. Rather, given that these models are trained on noisy internet data and have extensive reach and impact, it is of great importance to find effective ways to debias them. We propose that our plugin algorithm could be a suitable solution due to its post-processing nature, which avoids incurring significant computational costs.