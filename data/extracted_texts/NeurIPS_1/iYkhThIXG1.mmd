# Soft-Label Integration for Robust Toxicity Classification

Zelei Cheng

Northwestern University

Evanston, USA

zelei.cheng@northwestern.edu

&Xian Wu

Northwestern University

Evanston, USA

xianwu2024@u.northwestern.edu

&Jiahao Yu

Northwestern University

Evanston, USA

jiahao.yu@northwestern.edu

&Shuo Han

Northwestern University

Evanston, USA

shuo.han.1@u.northwestern.edu

&Xin-Qiang Cai

The University of Tokyo

Tokyo, Japan

xinqiang.cai@riken.jp

&Xinyu Xing

Northwestern University

Evanston, USA

xinyu.xing@northwestern.edu

###### Abstract

Toxicity classification in textual content remains a significant problem. Data with labels from a single annotator fall short of capturing the diversity of human perspectives. Therefore, there is a growing need to incorporate crowdsourced annotations for training an effective toxicity classifier. Additionally, the standard approach to training a classifier using empirical risk minimization (ERM) may fail to address the potential shifts between the training set and testing set due to exploiting spurious correlations. This work introduces a novel bi-level optimization framework that integrates crowdsourced annotations with the soft-labeling technique and optimizes the soft-label weights by Group Distributionally Robust Optimization (GroupDRO) to enhance the robustness against out-of-distribution (OOD) risk. We theoretically prove the convergence of our bi-level optimization algorithm. Experimental results demonstrate that our approach outperforms existing baseline methods in terms of both average and worst-group accuracy, confirming its effectiveness in leveraging crowdsourced annotations to achieve more effective and robust toxicity classification.

## 1 Introduction

Large language models (LLMs) are rapidly being adopted in applications such as conversations , AI-assisted programming , and education . However, despite impressive capabilities, the interaction between humans and LLMs can generate harmful, biased, or factually incorrect content . For example, users may ask LLMs to generate toxic content, such as hate speech, misinformation, or violent threats, which can have severe consequences for individuals and communities. Recent studies on jailbreaking LLMs also show that adversarial prompts can elicit toxic responses from models . Therefore, there is a pressing need to develop a robust toxicity classification model that can effectively identify and mitigate harmful content generated by LLMs.

Traditional toxicity classification methods , typically reliant on labels from a single annotator per instance, fall short of capturing the diversity of human perspectives . This approach often leads to biases  and poor generalizability across different contexts , as it fails to account for the complex realities of language use and social interactions. Thus, there is a growing need to incorporate crowdsourced annotations that reflect a broader array of cultural and linguistic nuances. Additionally, Arjovsky et al.  point out that the model trained by empirical risk minimization (ERM) may exploit the spurious correlations that are easier to fit instead of learning the causal components, which suffers from distribution shifts from training to testing domains . When _spurious correlations_ are present, the performance of certain groups of examples can drop significantly. For example, the toxicity classifier might learn to associate certain phrases or contexts (_e.g.,_ "I must remind you that" in Figure 1) with non-toxic behavior, despite the overall response being harmful.

To overcome the above challenges, we propose a bi-level optimization framework that incorporates crowdsourced annotations through soft-labeling techniques to enhance the robustness and reliability of toxicity classification systems. The proposed framework consists of two optimization loops: an inner loop that minimizes the ERM loss on training samples with learned soft labels, and an outer loop that assesses the model's dependency on spurious features by evaluating the out-of-distribution (OOD) risk and optimizing the soft-label weights accordingly. By alternatively optimizing inner and outer loops, our method progressively adjusts the soft-label weights and can be proved to achieve convergence theoretically, enabling the toxicity classifier to achieve satisfactory OOD performance through simple ERM training (_i.e.,_ inner loop optimization).

Empirically, we evaluate our method on the toxic question classification and response classification datasets provided by a third-party security company and the public HateXplain dataset . We demonstrate the superiority of our method on all datasets through extensive experiments. Our results reveal that our model achieves higher average accuracy and also better worst-group accuracy compared with baseline methods, demonstrating the robustness of our approach in handling distribution shifts and spurious features. Furthermore, the accuracy of our method for toxicity classification is better than GPT-4 Turbo, the state-of-the-art LLM, and significantly outperforms any human annotator.

Figure 1: **An example of a toxic response with the spurious feature "I must remind you that". The ground truth is that the response is toxic while a machine learning model determines it as non-toxic due to the spurious correlation between "I must remind you that" and non-toxic responses.**

By integrating multiple annotations and adopting a robust optimization approach, our study not only advances the technological frontiers of toxicity classification but also contributes to the broader discourse on ethical AI practices, promoting more nuanced and equitable online interactions.

## 2 Related Work

Bi-level Optimization.Bi-level optimization  has attracted significant attention due to its ability to handle hierarchical decision-making tasks including meta learning [22; 23; 24; 25; 26], neural architecture search [27; 28; 29], sample re-weighting [30; 31; 25], label denoising , etc. For example, in meta-learning , bi-level optimization provides a way to learn the initial parameters of a model which leads to fast adaptation and good generalization for various learning tasks. In this work, we formulate the toxicity classification from multiple annotations as a bi-level optimization problem where we alternate between minimizing the empirical risk minimization (ERM) loss on training samples with learned soft labels and optimizing the soft-label weights against the out-of-distribution (OOD) risk.

Learning from Partial Labels.Training a classifier from partial labels implicitly requires determining the ground truth from multiple annotations. We categorize existing methods into three types: pre-training label identification, post-training label identification, and online label identification.

Pre-training label identification.Pre-training label identification refers to the methods that infer ground truth before training the classifier. Some baseline methods such as Majority Voting (MV)  and Participant-Mine Voting (PM) [34; 35] directly infer a true label from crowdsourced multiple labels , with MV assuming equal annotator quality and PM accounting for worker quality differences. However, both MV and PM assume annotator quality is instance-independent, which is often not the case due to varying cultural and educational backgrounds. Probabilistic models [37; 38; 39] like Snorkel use statistical dependencies to infer true labels but can be limited by non-independent annotators like GPT-4 and GPT-4 Turbo.

Post-training Label Identification.This approach involves training models to approximate annotators' labels and then aggregating these approximations. Chou and Lee  propose modeling each annotator separately within an inner layer to enhance final predictions. Similarly, Davani et al.  train multiple models to predict each annotator's label, subsequently applying majority voting to determine the final label.

Online label identification.Online label identification refers to the methods that disambiguate the candidate labels during the training. There are generally two categories of methods. The first one is average-based methods [42; 43; 44] which treats each candidate label equally in the model training phase and minimizes the average loss over all candidate labels, assuming equal likelihood for each, which is unrealistic. The second one is identification-based methods which directly maximizes the probability of exactly one candidate label [45; 46; 47]. Lv et al.  introduce PRODEN, which iteratively identifies pseudo labels and minimizes the corresponding loss. PRODEN starts with equal weights for all candidate labels and uses model logits to determine pseudo labels. However, incorrect initial assumptions can lead to local minima.

Distributionally Robust Optimization.Distributionally robust optimization (DRO) optimizes the worst-case loss in an uncertainty set of test distributions [48; 49; 50; 51; 52]. Sagawa et al.  propose GroupDRO to learn a robust model to minimize the loss of the worst group when the dataset has group annotations. Oren et al.  propose topic-CVaR to optimize the loss over the worst-case mixture of text topics. When such group distributions are not available, conditional value at risk (CVaR) [53; 54] constructs new distributions by reweighting the training samples and minimizes the supreme loss over these distributions. In this work, we leverage the GroupDRO technique to learn a robust soft-label weight estimator.

## 3 Proposed Technique

### Problem Setup and Assumption

Consider a toxicity classification task with \(C\) classes, with a training dataset \(_{tr}:=\{(^{(i)},}_{i})\}_{i=1}^{n_{tr}}\). Here, \(^{(i)}\) represents the input text, and \(}_{i}\) denotes the associated labels annotated by workers or experts. Each test instance in the training set is annotated by \(M\) workers, resulting in a set of possible labels \(}_{i}:=\{y^{j}_{i}\}_{j=1}^{M}\), where \(y^{j}_{i}[C]:=\{1,2,...,C\}\). We assume that the correct ground-truth label is included in \(}_{i}\). Additionally, a small, clean validation set \(_{v}:=\{(^{(i)},y_{i})\}_{i=1}^{n_{x}}\) is provided, which is sampled from the same distribution as the training set \(_{tr}\), where \(n_{v} n_{tr}\). Our objective is to learn a classifier \(f\) that effectively predicts the correct labels without relying on irrelevant or spurious features.

### Technical Overview

Recall our goal is to train an optimal classifier that does not depend on spurious correlations, a naive approach might involve using existing out-of-distribution (OOD) risk loss functions, such as distributionally robust optimization (DRO). However, a significant issue arises from the absence of ground-truth labels in the training set. Training a robust model directly using DRO on the clean validation set could result in limited available data, potentially compromising the overall performance. Considering these, we propose a bi-level formulation to address these challenges. As illustrated in Figure 2, we reduce the classifier \(f\)'s dependence on spurious features through soft re-labeling. In this example, we identify \(x_{1}\) and \(x_{2}\) as the core and spurious features, respectively, and aim to train a classifier that does not rely on the spurious feature \(x_{2}\).

Without re-labeling, even if the training set had the ground truths, the classifier would still be biased towards \(x_{2}\). However, by applying soft re-labeling, we adjust the labels for samples in the bottom-left and top-right areas, resulting in an optimal classifier that is oriented vertically, as shown in Figure 2. This adjustment ensures that the newly trained classifier \(f\) does not depend on \(x_{2}\). Motivated by these, we formulate the task of learning soft labels to remove the spurious features as a bi-level optimization problem:

\[*{minimize}_{}\ (_{v},^{*}())^{*}( )_{}(_{tr},;) \]

where \(\) is the soft-label weight vector which indicates the importance of each annotator. The outer objective function can be any OOD risk loss function (_i.e.,_ group DRO loss). In the inner loop, we minimize the empirical risk minimization (ERM) loss (_i.e.,_ cross-entropy loss) on training samples with learned soft labels, resulting in a model denoted as \(^{*}()\). In the outer loop, we assess the model's dependency on spurious features by evaluating the OOD risk and optimizing the soft-label weights accordingly. By alternating between the inner and outer loops, the soft-label weights progressively adjust, enabling the achievement of satisfactory OOD performance through simple ERM training.

### Technical Details

We design a bi-level optimization process consisting of an inner-loop optimization and an outer-loop optimization to simultaneously update the learned soft-label weight \(\) and model parameters \(\). We begin by addressing the parameterization of the soft-label weight function \(\) in Eqn. (1). Although we could parameterize \(\) as an \(m\)-dimensional vector, it does not account for the relationship between the feature and label as annotated by the worker. Thus, we capture the weight of annotated labels \(}_{i}\) for the sample \(^{(i)}\) through a neural network \(v_{}:^{(i)}^{(i)}^{m}\). After obtaining the normalized soft-label weights \(^{(i)}\) through the softmax function, the final soft-label \(}_{i}\) is determined by taking the weighted sum of the one-hot vectors \(^{j}_{i}\) in the potential label set \(}_{i}\), where the weights are explicitly provided by \(^{(i)}\). With the soft-labels computed, we can now turn to the outer-level optimization. Motivated by , we initiate by pseudo-updating the parameter vector \(\), thereby establishing a relationship between \(\) and the optimized parameters \(}\). Specifically, \(}\) approximate

Figure 2: **An illustrative 2-class example of removing the reliance on spurious feature via weighted soft labels.** Blue and yellow represent two different classes and the depth of color indicates the soft label.

\(^{*}()\) through one-step inner loop gradient descent. We then update \(\) to make the induced \(^{}\) minimize the outer loss \(\). Regarding the inner-loop optimization, \(\) is directly updated to minimize \(\). We provide the full algorithm in Algorithm 1. Detailed explanations of the optimization process are provided below.

```
Input: Training dataset \(_{tr}:=\{(^{(i)},}_{i})\}_{i=1}^{n_{tr}}\), validation dataset \(_{val}:=\{(^{(i)},y_{i})\}_{i=1}^{n_{v}}\), max number of steps \(T\) Output: Toxicity classifier \(f_{}\) Initialization: Initialize the soft-label weights \(_{0}\) and the classifier parameter \(_{0}\) for\(t=1,2,,T\)do  Sample batch data \(\{,}\}\) from the training dataset \(_{tr}\)  Sample batch data \(\{,y\}\) from the validation dataset \(_{val}\)  Pseudo update \(^{}_{t+1}\) as Eqn. (2) and update the soft-label weights \(_{t+1}\) as Eqn. (3)  Update \(_{t+1}\) as Eqn. (4) endfor
```

**Algorithm 1** The bi-level optimization algorithm for training the toxicity classifier.

**Outer-loop optimization: Updating \(\).** Denote \(_{t}\) be the soft-label weights at time step \(t\). Given the weights \(_{t}\), we first pseudo update the parameter \(_{t}\) via one-step gradient descent and obtain \(^{}_{t+1}\). Please note that we do not intend to actually update the parameter \(\) but only save the gradients during the pseudo update for further gradient computation of \(_{t}\). Mathematically, the pseudo update of \(_{t}\) can be written as

\[^{}_{t+1}=_{t}-_{}( _{t};_{t}), \]

where \(\) is the step size for updating \(\). After computing \(^{}_{t+1}\), we use the following formula to update \(\) via gradient descent:

\[_{t+1}=_{t}-_{}(^{}_{t+1}), \]

where \(\) is the step size for updating \(\). The OOD risk function \(\) is a GroupDRO loss computed in the validation set. Mathematically, \(()=_{g}_{(x,y) P_{g}}[ (;(x,y))]\) where \(\) denotes the set of all groups, \(P_{g}\) denotes the data distribution within the group \(g\), and \(l\) is the cross-entropy loss.

**Inner-loop optimization: Updating \(\).** Once we have the soft-label weights \(_{t}\), we can update the parameter \(\) via single-step optimization as follows

\[_{t+1}=_{t}-_{}(_{t};_{t+1}). \]

where \(-_{(^{(i)},}_{i}) _{tr}}[_{c=1}^{C}_{ic} f_{c}(^{(i)};)]\). \(f_{c}\) represents the probability of the \(c\)-th class of \(f(.)\) that is determined as the true label. \(_{ic}\) is the \(c\)-th element of the soft label \(_{i}\) where the soft label is a weighted aggregation over \(M\) one-hot vectors of annotations, _i.e._, \(_{i}=^{(i)}[_{i}^{1},,_{i}^{M}]^{T}\).

### Theoretical Analysis

Finally, we can prove the convergence of our bi-level optimization algorithm under moderate assumptions. The convergence analysis follows from a similar idea as the proof in . We first introduce the following necessary assumptions.

**Assumption 3.1** (Smoothness of \(\)).: _The OOD risk function \(\) is Lipschitz-smooth with a constant \(L\)._

Assumption 3.1 is a common assumption in the analysis of bi-level optimization . Additionally, we assume that the gradients of \(\), \(\) and their inner product are bounded.

**Assumption 3.2** (Lower bound of the inner product of the gradients).: _We assume that the following inequality holds with some constant \(k\) for every time step \(t\)_

\[_{}(^{}_{t+1})^{T}_{}(_{t};_{t+1}) k\|_{}(_{t};_{t+1})\|^{2} \]

**Assumption 3.3** (Bounded gradients of \(\) and \(\)).: _The gradients of \(\) and \(\) are bounded by \(\). \(_{}_{}(;)\) is bounded by \(^{}\)._Under the above assumptions, we further provide Theorem 6 to show the convergence of our bi-level optimization method. The proof of Theorem 3.4 can be found in Appendix A.

**Theorem 3.4** (Convergence).: _Under Assumption 3.1 and Assumption 3.2 and setting the step size \(\), our bi-level optimization algorithm can ensure that the risk function \(\) monotonically decreases with respect to the time step \(t\), i.e.,_

\[(_{t+1})(_{t}) \]

_The equality in Eqn. (6) holds if the gradient of the risk function \(\) with respect to \(w\) becomes 0 at some time step \(t\), i.e., \(_{w}(_{t})=0\)._

Theorem 3.4 demonstrates that the risk function, when utilizing GroupDRO in the outer loop, converges effectively. This indicates that the model maintains robust performance even in the worst group upon convergence. Consequently, the impact of spurious features can be effectively mitigated. Additionally, we prove the convergence rate of our bi-level optimization method as \(O(})\). The details of the proof are in Appendix B.

**Theorem 3.5** (Convergence rate).: _Let the total number of training steps as \(T\) and set the step size \(=}{}\) for some constant \(k_{1}\) where \(0<k_{1}<\) and \(=}{T}\) for some constant \(k_{2}\). Under Assumption 3.1 and Assumption 3.3, we have_

\[_{1 t T}[\|_{w} (_{t})\|_{2}^{2}] O(}) \]

Theorem 3.5 implies that if we want \(_{1 t T}[\|_{w}( _{t})\|_{2}^{2}]\), we have to train \(O(})\) steps. Furthermore, as the training step increases, the gradient of the risk function with respect to \(\) is gradually close to 0. If the risk function \(\) is convex with respect to \(\), it essentially means that \(\) gradually converges to the optimal \(^{*}\) that minimizes the risk function.

## 4 Evaluation

In this section, we start with the experimental setup, including the datasets, baselines, and metrics. We then present the results of our experiments, which evaluate the effectiveness of our proposed method against baseline methods. Finally, we conduct an ablation study to compare the performance of our method with alternative design choices. We release the data and code in [https://github.com/chengzelei/crowdsource_toxicity_classification](https://github.com/chengzelei/crowdsource_toxicity_classification).

### Experiment Setup

Datasets.We obtain the toxic question and response datasets from a third-party security company. The toxic question dataset is classified into 15 categories based on the OpenAI usage policy retrieved in 2023 as shown in Table 4. The response classification task is a binary classification problem, where the responses are labeled as toxic or non-toxic. Each data point is associated with three human annotations and three LLM-generated annotations (GPT-4, GPT-4 Turbo, and Claude-2). To better reflect the real-world scenario where the source or the number of annotators is limited, we have six datasets: Q-H, Q-L, Q-A, R-H, R-L, and R-A, where Q-H and R-H are annotated by humans, Q-L and R-L are annotated by LLMs, and Q-A and R-A are annotated by all annotators.

For each classification task, we have a large training set with crowdsourced annotations (_i.e.,_ 6,941 samples for toxic question classification and 28,194 samples for toxic response classification) and a testing set containing 2,000 samples with ground truth. The validation set with ground truth includes a small number of samples (_i.e.,_ 1,000 samples) from the training set. Additionally, the company assigned 15 topics utilizing Latent Dirichlet Allocation (LDA) . We further construct the groups based on both topics and true labels. The details of the groups can be found in Appendix C.2.

In addition, we conduct our experiments on the public HateXplain dataset . It contains three classes - "hatespeech", "offensive", "normal". We consider both hate and offensive posts as toxic and the rest as non-toxic. Each record includes a post and three human annotations. The true labels are determined as the majority vote of three human annotations following . We further utilize GPT-4, GPT-4 Turbo, and Claude 2 to label these comments. We assign 15 topics utilizing LDA and further construct the groups based on both topics and true labels.

Baseline methods.Besides the six individual annotations, we compare our method with the following baseline methods -- 1 Pre-training label identification: This method involves generating true labels for supervised learning through three approaches. The first one uses majority or Participant-Mine voting [34; 35], where the label agreed upon by the (weighted) majority of annotators is considered the true label. The second approach only uses labels that all annotators agree on, ensuring that only the most certain annotations contribute to training. The third approach "Snorkel"  constructs a probabilistic graph model to learn the correlation between different annotations and infer the true label. 2 Post-training identification: This approach trains an ensemble of models, where each model is trained to estimate each annotator's labels . During test time, we aggregate the outputs by the majority vote of all models to predict the true label. 3 Online label identification: This approach utilizes techniques from semi-supervised learning where all possible labels selected by annotators are considered. We employ methods such as the average-label learning framework [42; 43; 44] which minimizes the average loss over all potential labels, and PRODEN , which optimizes the loss with respect to the progressively identified ground truth. 4 Soft-label Learning: This approach assigns different weights to the losses with respect to each unique candidate label selected by annotators. We consider the vanilla soft-label learning method as a baseline that directly counts the number of votes as the soft-label weights without modeling the reliability of each annotator.

Metrics.We follow prior work  to give a robust evaluation of the toxicity classifier across different data distributions. We evaluate the toxicity classifier's performance on each group, calculating the classification accuracy for each group. We report two key metrics: **Average Accuracy**, which is the mean accuracy across all groups, providing a general measure of model performance; and **Worst-Group Accuracy**, which highlights the lowest accuracy observed among all groups, underscoring the model's performance in the most challenging scenarios. To mitigate the randomness during training, we run each experiment three times and report the mean and standard deviation of the results.

Implementation.We implement the proposed method using PyTorch. We train the machine learning models on a server with 8 NVIDIA A100 80GB GPUs and 4TB memory for all the learning algorithms. The toxicity classifier is based on "RoBERTa-large" infrastructure and the soft-label weight estimator is based on "RoBERTa-base" infrastructure. We list the hyper-parameter settings for all experiments in Appendix C.3.

### Main Results

Compare with baseline methods.In Table 1, we show the average accuracy and worst-group accuracy of our method and the baseline methods on the datasets from the third-party security company. As shown in the table, our method outperforms all baseline methods in terms of both average accuracy and worst-group accuracy across two classification tasks. Baseline methods do not consider the out-of-distribution risk and therefore show worse performance regarding worst-group accuracy. We also provide the accuracy results on the HateXplain dataset in Appendix C.4. These results demonstrate the effectiveness of our method in learning from multiple annotators with soft labeling to improve the toxicity classifier's performance and eliminate the out-of-distribution risk with GroupDRO.

Compare with human and proprietary LLM annotations.We compare the classification performance of our method with human and proprietary LLM labeling in Figure 3. The results show that our method achieves outstanding performance in both question and response classification tasks. The accuracy of our method for question classification is comparable to GPT-4 Turbo, the state-of-the-art LLM, and significantly outperforms any human annotator. For response classification, our method surpasses all annotations, including GPT-4 Turbo, by a large margin. Considering the high cost of GPT-4 Turbo labeling, our method provides a cost-effective and scalable solution for toxicity classification tasks.

Time complexity comparison with baseline methods.We measure the time complexity of all methods across all datasets and report the results in Appendix C.5. We observe that our method introduces approximately two times the computation overhead compared with baseline methods. The additional computation overhead originates from the pseudo-update of the model parameter \(\) and the update of the soft-label weights \(\). Note that we utilize a smaller model (_i.e.,_ RoBERTa-base) to learn the soft-label weights compared with the classifier (RoBERTa-large). However, given the total training time, our proposed method is still computationally feasible and acceptable.

### Ablation Study

We conduct an ablation study to demonstrate the superiority of our design with alternative designs and compare the performance of our method with fewer annotators.

Learning with fewer annotators.We assess the performance of our method with fewer annotators and compare it with other methods in Figure 4. The figure first shows that our method still outperforms all baseline methods in two classification tasks in terms of both average accuracy and worst-group accuracy when only human annotations or LLM annotations are available. This demonstrates that our method is robust and effective in learning from fewer annotators, providing a cost-effective solution for toxicity classification tasks.

We also observe that the annotation quality of LLMs and humans varies for different tasks. For instance, LLM annotations yield generally better results than human annotations for the question classification task, while the opposite is true for the response classification task. This finding aligns with the result in Figure 3. Thus, baseline methods may be particularly sensitive to the quality of the annotators. Specifically, for the response classification task, the classification performance of baselines is much lower when all annotators are present compared to when only human annotators are present. In contrast, our method not only maintains but improves its accuracy when all annotators are included, underscoring its ability to handle variable annotation quality effectively. Moreover, our approach demonstrates robustness against different data distributions in the testing set, achieving over 70% accuracy in the worst group for the response classification task where no baseline method exceeds 60%.

    &  &  &  \\   & & Average (\%) & Worst-Group (\%) & Average (\%) & Worst-Group (\%) \\   & Consensus Only & 30.55 \(\) 0.51 & 12.94 \(\) 1.61 & 79.66 \(\) 1.60 & 61.33 \(\) 6.53 \\   & Majority Voting & 73.83 \(\) 0.37 & 66.62 \(\) 0.86 & 79.22 \(\) 0.53 & 59.64 \(\) 3.03 \\   & PM Voting & 73.87 \(\) 0.53 & 65.78 \(\) 1.02 & 80.11 \(\) 1.15 & 63.96 \(\) 1.39 \\   & Snorkel & 68.73 \(\) 0.06 & 47.47 \(\) 1.75 & 80.48 \(\) 1.15 & 64.91 \(\) 3.04 \\  Post-training & Ensemble & 70.70 \(\) 0.63 & 56.57 \(\) 0.32 & 81.10 \(\) 0.45 & 57.89 \(\) 0.51 \\   & Average-label Learning & 19.38 \(\) 0.00 & 12.38 \(\) 0.00 & 35.86 \(\) 0.00 & 9.25 \(\) 0.00 \\   & PRODEN & 23.07 \(\) 6.50 & 8.91 \(\) 2.07 & 36.06 \(\) 0.34 & 9.93 \(\) 1.18 \\   & Vanilla Soft Label & 74.81 \(\) 0.95 & 67.68 \(\) 2.02 & 85.52 \(\) 0.50 & 62.57 \(\) 4.42 \\   & Ours & **78.41 \(\) 0.24** & **69.44 \(\) 0.13** & **89.80 \(\) 0.61** & **77.82 \(\) 0.63** \\   

Table 1: **Comparison of Average and Worst-Group Accuracy Across Different Baseline Methods for Toxicity Classification. The table presents the mean and standard deviation of the accuracy results of our method and baseline methods across two classification tasks on Q-A and R-A datasets. Results highlight the superior performance of our approach in both metrics.**

Figure 3: **Comparison of our method with individual annotators on Q-A and R-A datasets. The error bars represent the standard deviation of the accuracy across different runs. Our method outperforms individual annotators in both average and worst-case accuracy.**

Baseline methods with GroupDRO.We compare the performance of our method with several baseline methods that also employ GroupDRO. We add an additional baseline of ERM with GroupDRO which trains a toxicity classifier based on the validation set. Note that GroupDRO requires true labels to assign groups which is only applicable to pre-training label identification methods. As detailed in Table 2, we have two observations. First, our method still outperforms the baseline methods with GroupDRO in terms of both average and worst-group accuracy. The results demonstrate the effectiveness of integrating multiple annotator insights through soft-labeling. Second, compared with Table 1, the performance of baseline methods with GroupDRO is generally better than naive baseline methods which confirms the impact of out-of-distribution risk in our tasks.

Alternative design - our method with CVaR DRO.We investigate an alternative design of our method which incorporates the CVaR DRO technique  to address the out-of-distribution risk without prior knowledge of groups. We compare the performance of our method with the alternative design in Table 3. The results show that while CVaR DRO targets extreme risks in distributions, it underperforms compared to GroupDRO. This finding highlights GroupDRO's capability in utilizing

   &  &  &  \\   & & Average (\%) & Worst-Group (\%) & Average (\%) & Worst-Group (\%) \\   & Consensus Only & 33.70 \(\) 1.90 & 16.37 \(\) 1.67 & 80.52 \(\) 0.78 & 64.77 \(\) 0.50 \\   & Majority Voting & 74.88 \(\) 0.01 & 68.16 \(\) 0.38 & 79.80 \(\) 0.37 & 62.26 \(\) 0.64 \\   & PM Voting & 74.33 \(\) 0.56 & 66.94 \(\) 1.43 & 81.52 \(\) 1.02 & 65.26 \(\) 1.19 \\   & Snorkel & 69.10 \(\) 0.13 & 47.80 \(\) 1.16 & 81.33 \(\) 0.59 & 66.74 \(\) 1.00 \\   & ERM & 65.27 \(\) 0.50 & 54.07 \(\) 0.76 & 84.95 \(\) 0.28 & 67.11 \(\) 1.00 \\  Online & Ours & **78.41 \(\) 0.24** & **69.44 \(\) 0.13** & **89.80 \(\) 0.61** & **77.82 \(\) 0.63** \\  

Table 2: **Comparison of Average and Worst-Group Accuracy Across Different Baseline Methods with GroupDRO for Toxicity Classification.** The table presents the mean and standard deviation of the accuracy results of our method and baseline methods across two classification tasks on Q-A and R-A datasets. Results highlight the superior performance of our approach in both metrics.

Figure 4: **Comparison of Average and Worst-Group Accuracy of Different Methods with Fewer Annotators.** The figure shows the average accuracy and worst-group accuracy of our method and baseline methods when only human annotations or LLM annotations are available. Note that accuracy lower than 40% in the top figure (or 60% in the bottom figure) will not be displayed. Our method outperforms all baseline methods with fewer annotators.

group-specific information to optimize performance, demonstrating its effectiveness in addressing the real-world toxicity classification problem.

## 5 Discussion and Conclusion

In this work, we introduce a novel bi-level optimization framework that incorporates soft-labeling techniques alongside GroupDRO to tackle the OOD risk of toxicity classification with crowdsourced annotations. By leveraging multi-source annotations, our approach captures a broader spectrum of the annotator's judgment, enhancing the system's ability to handle the inherent ambiguities in defining toxic content. We present a theoretical analysis of convergence and demonstrate its superior performance over toxic question and response datasets. We hope that our work will inspire further research in developing ethically aware and technically robust AI-driven moderation tools.

Our work suggests several promising directions for future research. First, it would be interesting to investigate the extension of our toxicity classification framework to multi-modal contents, where toxicity may manifest not just in text but through images, videos, and their combinations, presenting unique challenges and requiring novel adaptation strategies. Second, while our model leverages annotations from multiple sources to enhance the accuracy of toxicity classification, it remains dependent on the quality and representativeness of these annotations. Future research could focus on improving the fairness of our model by continuously monitoring for and mitigating inherent biases in annotator perspectives. This would involve regular audits, updates to training data, and adjustments to model parameters to bolster both the effectiveness and fairness of the system. Finally, the versatility of our framework could extend beyond toxicity classification to other large language model safety applications, such as LLM alignment through reinforcement learning from feedback (RLHF). In RLHF, human annotators provide pairwise feedback for LLM responses, which can be noisy. Our bi-level optimization framework could be adapted to assess the quality of this feedback and select the most reliable inputs for fine-tuning LLMs.

   &  &  \\   & Average (\%) & Worst-Group (\%) & Average (\%) & Worst-Group (\%) \\  CVaR DRO & 75.76 \(\) 0.13 & 66.70 \(\) 1.06 & 86.72 \(\) 0.39 & 68.30 \(\) 0.13 \\  Ours & **78.41 \(\) 0.24** & **69.44 \(\) 0.13** & **89.80 \(\) 0.61** & **77.82 \(\) 0.63** \\  

Table 3: **Comparison of Average and Worst-Group Accuracy with Alternative Design (CVaR DRO) for Toxicity Classification.** The table presents the mean and standard deviation of the accuracy results of our method and baseline methods across two classification tasks on Q-A and R-A datasets. Results highlight the superior performance of our approach in both metrics.