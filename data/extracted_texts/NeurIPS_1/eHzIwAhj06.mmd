# The Group Robustness is in the Details: Revisiting Finetuning under Spurious Correlations

Tyler LaBonte\({}^{1}\)1  John C. Hill\({}^{2}\)  Xinchen Zhang\({}^{2}\)

**Vidya Muthukumar\({}^{2,1}\)  Abhishek Kumar\({}^{}\)**

\({}^{1}\)H. Milton Stewart School of Industrial and Systems Engineering, Georgia Institute of Technology

\({}^{2}\)School of Electrical and Computer Engineering, Georgia Institute of Technology

###### Abstract

Modern machine learning models are prone to over-reliance on spurious correlations, which can often lead to poor performance on minority groups. In this paper, we identify surprising and nuanced behavior of finetuned models on worst-group accuracy via comprehensive experiments on four well-established benchmarks across vision and language tasks. We first show that the commonly used class-balancing techniques of mini-batch upsampling and loss upweighting can induce a decrease in worst-group accuracy (WGA) with training epochs, leading to performance no better than without class-balancing. While in some scenarios, removing data to create a class-balanced subset is more effective, we show this depends on group structure and propose a mixture method which can outperform both techniques. Next, we show that scaling pretrained models is generally beneficial for worst-group accuracy, but only in conjunction with appropriate class-balancing. Finally, we identify spectral imbalance in finetuning features as a potential source of group disparities -- minority group covariance matrices incur a larger spectral norm than majority groups once conditioned on the classes. Our results show more nuanced interactions of modern finetuned models with group robustness than was previously known. Our code is available at [https://github.com/tmlabonte/revisiting-finetuning](https://github.com/tmlabonte/revisiting-finetuning).

## 1 Introduction

Classification performance in machine learning is sensitive to _spurious correlations_: patterns which are predictive of the target class in the training dataset but not at test time. For example, in computer vision tasks, neural networks are known to utilize the backgrounds of images as proxies for their content . Beyond simple settings, spurious correlations have been identified in high-consequence applications such as criminal justice , medicine , and facial recognition . In particular, a model's reliance on spurious correlations disproportionately affects its accuracy on _minority groups_ which are under-represented in the training dataset; we therefore desire maximizing the model's _group robustness_, quantified by its minimum accuracy on any group .

The standard workflow in modern machine learning involves initializing from a pretrained model and finetuning on the downstream dataset using empirical risk minimization (ERM) , which minimizes the average training loss. When _group annotations_ are available in the training dataset, practitioners utilize a rich literature of techniques to improve worst-group accuracy (WGA) . However, group annotations are often unknown or problematic to obtain (_e.g._, due to financial, privacy, or fairness concerns). While group robustness methods have been adapted to work without group annotations , they remain complex variants on the standard finetuning procedure.

Hence, it is often unclear to what extent the WGA dynamics of these methods are attributable to details of model finetuning.

In this paper, we take a complementary approach to the methodological literature by pursuing a comprehensive understanding of the _fundamental properties_ of model finetuning on four well-established group robustness benchmarks across vision and language tasks. We focus especially on the conjunction of _model scaling_ and _class-balancing_ -- which was recently shown to greatly improve robustness on some datasets  -- on the worst-group accuracy of the ERM baseline. These considerations enable us to isolate the impact of group disparities on worst-group accuracy, thereby revealing more nuanced behaviors of finetuned models than previously known. In particular, we challenge overarching narratives that "overparameterization helps or hurts distributional robustness" and show striking differences in finetuning performance depending on class-balancing methodology.

In more detail, our main contributions include:

* Identifying two _failure modes_ of common class-balancing techniques during finetuning: (1) mini-batch upsampling and loss upweighting experience catastrophic collapse with standard hyperparameters on benchmark datasets, and (2) removing data to create a class-balanced subset can harm WGA for certain datasets.
* Proposing a _mixture balancing_ method which combines the advantages of two class-balancing techniques and can improve baseline WGA beyond either method.
* Showing that while overparameterization can harm WGA in certain cases, model scaling is generally beneficial for robustness when applied _in conjunction_ with appropriate pretraining and class-balancing.
* Identifying a _spectral imbalance_ in the top eigenvalues of the group covariances -- even when the classes are balanced -- and showing that minority group covariance matrices consistently have larger spectral norm conditioned on the classes.

### Related work

Here we provide a brief summary of related work along three axes. Throughout the paper, we also provide detailed contextualizations of our results with the most closely related work.

Spurious correlations.The proclivity of ERM to rely on spurious correlations has been widely studied [12; 37]. Rectifying this weakness is an important challenge for real-world deployment of machine learning algorithms, as spurious correlations can exacerbate unintended bias against demographic minorities [20; 2; 57; 17; 5] or cause failure in high-consequence applications [33; 8; 70; 42]. Reliance on spurious correlations manifests in image datasets as the usage of visual shortcuts including background [1; 50; 68], texture , and secondary objects [48; 52; 54], and in text datasets as the usage of syntactic or statistical heuristics as a substitute for semantic understanding [14; 41; 36].

Class-balancing and group robustness._Group-balancing_, or training with an equal number of samples from each group, has been proposed as a simple yet effective method to improve robustness to spurious correlations [17; 51; 6; 55]. However, group-balancing requires group annotations, which are often unknown or problematic to obtain [31; 72; 47; 29]. On the other hand, _class-balancing_, or training with an equal number of samples from each class, is a well-studied method in long-tailed classification [24; 15; 4]. Recent work has shown that class-balancing is a surprisingly powerful method for improving worst-group accuracy which does not require group annotations [22; 29; 7; 53]. In particular,  study the WGA dynamics of two common class-balancing methods: removing data from the larger classes (which we call _subsetting_) and upsampling the smaller classes (which we call _upsampling_). Our results complement those of  and show more nuanced effects of class-balancing than previously known; we provide additional contextualization with  in Section 3.1. We show similar nuanced behavior of _upweighting_ smaller classes in the loss function, a popular method in the group-balancing setting [31; 47; 55] which  did not study.

Overparameterization and distributional robustness.While the accepted empirical wisdom is that overparameterization improves _in-distribution_ test accuracy [40; 71], the relationship between overparameterization and robustness is incompletely understood.  considered a class of ResNet-18 architectures and showed that increasing model _width_ reduces worst-group accuracy on the Waterbirds and CelebA datasets when trained with class-imbalanced ERM -- this contrasts with the improvement in average accuracy widely observed in practice (see, _e.g._, ). Conversely,  showed a benefit of overparameterization in robustness to "natural" covariate shifts, which are quite different from spurious correlations . On the mathematical front,  showed that overparameterization in random feature models trained to completion improves robustness to a wide class of covariate shifts.However, both the optimization trajectory and statistical properties of random features are very different from neural networks (see, _e.g._, ). Closely related to our work,  investigated pretrained ResNet, VGG, and BERT models, and showed that overparameterization does not harm WGA. Our results complement those of  with a richer setup and show that class-balancing -- which they do not study -- can greatly impact model scaling behavior.

## 2 Preliminaries

Setting.We consider classification tasks with input domain \(^{n}\) and target classes \(\). Suppose \(\) is a set of _spurious features_ such that each example \(^{n}\) is associated with exactly one feature \(s()\). The dataset is then partitioned into _groups_\(\), defined by the Cartesian product of classes and spurious features \(=\). Given a dataset of \(m\) training examples, we define the set of indices of examples which belong to some group \(g\) or class \(y\) by \(_{g}\{1,,m\}\) and \(_{y}\{1,,m\}\), respectively. Then, the _majority group(s)_ is defined by the group(s) that maximize \(|_{g}|\). All other groups are designated as _minority groups_. Further, the _worst group(s)_3 is defined by the group(s) which incur minimal test accuracy. We define majority and minority classes similarly. Because groups are defined by the Cartesian product of classes and spurious features, all training examples in a particular group are identically labeled, and therefore _a group is a subset of a class_.

We desire a model which, despite group imbalance in the training dataset, enjoys roughly uniform performance over \(\). Therefore, we evaluate _worst-group accuracy_ (WGA), _i.e._, the minimum accuracy among all groups . We will also be interested in the relative performance on groups _within the same class_, and we thereby define the _majority group within a class_\(y\) as the group which maximizes \(|_{g}|\) over all \(g\{g:y g\}\). Other groups are designated as the minority groups within that class. For example, referring to the Waterbirds section of Table 2, groups 1 and 2 are the minority groups within classes \(0\) and \(1\), respectively.

Class-balancing.A dataset is considered to be _class-balanced_ if it is composed of an equal number of training examples from each class in expectation over the sampling probabilities. We compare three class-balancing techniques: _subsetting_, _upsampling_, and _upweighting_. We describe each below:

* In _subsetting_, every class is set to the same size as the smallest class by removing the appropriate amount of data from each larger class uniformly at random. This procedure is performed only once, and the subset is fixed prior to training.
* In _upsampling_, the entire dataset is utilized for training with a typical stochastic optimization algorithm, but the sampling probabilities of each class are adjusted so that mini-batches are class-balanced in expectation. To draw a single example, we first sample \(y()\), then sample \(( y)\) where \(\) is the _empirical_ distribution on training examples.
* In _upweighting_, the minority class samples are directly upweighted in the loss function according to the ratio of majority class data to minority class data, called the _class-imbalance ratio_. Specifically, if the loss function is \((f(),y)\) for model \(f\), example \(\), and class label \(y\), the upweighted loss function is \((f(),y)\) where \(\) is defined as the class-imbalance ratio for minority class data and \(1\) for majority class data. It is worth noting that upweighting is equivalent to upsampling in expectation over the sampling probabilities.

Note that the terminology for these class-balancing techniques is not consistent across the literature. For example,  call subsetting _subsampling_ (denoted SUBY) and upsampling _reweighting_ (denoted RWY). On the other hand,  call (group-wise) subsetting _downsampling_ and use _upweighting_ to describe increasing the weight of minority group samples in the loss function.

Datasets and models.We study four classification datasets, two in the vision domain and two in the language domain, which are well-established as benchmarks for group robustness. We summarize each dataset below and provide additional numerical details in Appendix A.1.

* _Waterbirds_[64; 63; 50] is an image dataset wherein birds are classified as land species ("landbirds") or water species ("waterbirds"). The spurious feature is the image background: more landbirds are present on land backgrounds and vice versa.4 * _CelebA_[33; 50] is an image dataset classifying celebrities as blond or non-blond. The spurious feature is gender, with more blond women than blond men in the training dataset.
* _CivilComments_[3; 27] is a language dataset wherein online comments are classified as toxic or non-toxic. The spurious feature is the presence of one of the following categories: male, female, LGBT, black, white, Christian, Muslim, or other religion.5 More toxic comments contain one of these categories than non-toxic comments, and vice versa. * _MultiNLI_[65; 50] is a language dataset wherein pairs of sentences are classified as a contradiction, entailment, or neither. The spurious feature is a negation in the second sentence -- more contradictions have this property than entailments or neutral pairs.

Waterbirds is class-imbalanced with a majority/minority class ratio of 3.31:1, CelebA a ratio of 5.71:1, and CivilComments a ratio of 7.85:1. MultiNLI is class-balanced _a priori_. Since the Waterbirds dataset has a shift in group proportion from train to test, we weight the group accuracies by their proportions in the training set when reporting the test average accuracy .

We utilize ResNet , ConvNeXt-V2 , and Swin Transformer  models pretrained on ImageNet-1K  for Waterbirds and CelebA, and a BERT  model pretrained on Book Corpus  and English Wikipedia for CivilComments and MultiNLI. We use the AdamW optimizer  for finetuning on three independent seeds, randomizing both mini-batch order and any other stochastic procedure such as subsetting, and we report error bars corresponding to one standard deviation. We do not utilize early-stopping: instead, to consider the impact of overparameterization in a holistic way, we train models to completion to properly measure the overfitting effect.6 This can result in longer training than commonly seen in the literature (_e.g._, we finetune on CelebA for about \(3\) more gradient steps than is standard). See Appendix A.2 for further training details.

## 3 Nuanced effects of class-balancing on group robustness

We now present our first set of results, which shows that the choice of class balancing method greatly impacts the group robustness of the ERM baseline.

### Catastrophic collapse of class-balanced upsampling and upweighting

In a recent paper,  observed that contrary to the central hypothesis underlying the Just Train Twice method , the worst-group accuracy of ERM decreases dramatically with training epochs on CelebA and CivilComments; however, they provide no explanation for this phenomenon. In this section, we show that this degradation of WGA is due to their choice of class-balancing method (_i.e._, upsampling). Specifically, ERM finetuned with upsampling experiences a _catastrophic collapse_ in test WGA over the course of training, a phenomenon that was previously only noticed in synthetic datasets with a linear classifier . Moreover, while  state that class-balanced subsetting is not recommended in practice, we show that it can in fact improve WGA conditional on the lack of of a small minority group _within the majority class_. Finally, we show that class-balanced upweighting -- a popular technique which  do not study -- experiences a similar WGA collapse as upsampling. We finetune a ConvNeXt-V2 Base on Waterbirds and CelebA and a BERT Base on CivilComments, and we compare the subsetting, upsampling, and upweighting techniques to a class-imbalancedbaseline. Our results are displayed in Figure 1, with additional models in Appendix B. On CelebA and CivilComments, the more class-imbalanced datasets, upsampling and upweighting both experience catastrophic collapse over the course of training. We believe this collapse is caused by overfitting to the minority group within the minority class; any individual point from this group is sampled far more often during upsampling and weighted far more heavily during upweighting, causing overfitting during long training runs. In fact, upsampling does even worse on CelebA than observed in  because we train \(3\) longer to ensure convergence. With that said, optimally tuned early-stopping appears to mitigate the collapse (as previously noticed by  in a toy setting).

Our experiments also highlight a previously unnoticed disadvantage of class-balanced subsetting: if there is a small minority group in the majority class, subsetting will further reduce its proportion and harm WGA. For example, in the Waterbirds dataset, the species (landbirds/waterbirds) is the class label and the background (land/water) is the spurious feature; _landbirds/water_ is a small minority group within the majority class (landbirds). When landbirds is cut by \(3.31\), the landbirds/water group greatly suffers, harming WGA. On the other hand, in the CelebA dataset, the hair color (non-blond/blond) is the class label and the gender (female/male) is the spurious feature; the only small minority group is _blond/male_, while the groups are nearly balanced in the majority class. In this case, subsetting preserves blond/male examples and increases their proportion, helping WGA.

Finally, while upsampling and upweighting have similar WGA dynamics - perhaps as expected, as they are equivalent in expectation over the sampling mechanism -- both differ greatly from subsetting. Recently,  proved a theoretical equivalence between subsetting and upsampling of the _groups_ in the _population_ setting, _i.e._, assuming access to the training distribution. The equivalence of upsampling and upweighting would then imply that all three objectives are optimized by the same solution. However, our results suggest this may not hold in the real-world _empirical_ setting, where subsetting has distinctly different behavior, and model parameters may outnumber training examples. As previously mentioned, this may be due to overfitting to minority class data repeated often during training; theoretically investigating this discrepancy is an important future direction.

Contextualization with previous work.Our observations explain the decrease in WGA of CelebA and CivilComments noticed by , a phenomenon which they left unresolved. Our result implies that group robustness methods which assume that WGA increases during training, such as Just Train Twice , may only be justified with appropriate class-balancing.  show that upsampling can cause catastrophic collapse in WGA, but only in a synthetic dataset with a linear classifier. In realistic datasets,  perform extensive hyperparameter tuning (using group labels, which may be unrealistic) to achieve good results with upsampling, while we show that catastrophic collapse can occur in the same datasets when standard hyperparameters are used. Moreover,  state that class-balanced

Figure 1: **Class-balanced upsampling and upweighting experience catastrophic collapse. We compare _subsetting_, wherein data is removed to set every class to the same size as the smallest class, _upsampling_, wherein the sampling probabilities of each class are adjusted so that the mini-batches are class-balanced in expectation, and _upweighting_, wherein the loss for the smaller classes is scaled by the class-imbalance ratio. We observe a catastrophic collapse over the course of training of upsampling and upweighting on CelebA and CivilComments, the more class-imbalanced datasets. Subsetting reduces WGA on Waterbirds because it removes data from the small minority group within the majority class. MultiNLI is class-balanced _a priori_, so we do not include it here.**

subsetting is not recommended in practice, but we show that subsetting can be effective except when there is a small minority group within the majority class, a previously unnoticed nuance. Finally, we show that subsetting experiences different WGA dynamics from upsampling and upweighting in the empirical setting, suggesting additional complexity compared to the population setting results of .

Without extensive tuning, class-balanced _upsampling_ and _upweighting_ can induce WGA no better than without class-balancing. While class-balanced _subsetting_ can improve WGA, practitioners should use caution if a small minority group is present within the majority class.

### Mixture balancing: interpolating between subsetting and upsampling

To mitigate the catastrophic collapse of class-balanced upsampling and upweighting, we propose a simple _mixture method_ which interpolates between subsetting and upsampling. Our method increases exposure to majority class data without over-sampling the minority class, which can improve WGA and mitigate overfitting to the minority group. We first create a data subset with a specified _class-imbalance ratio_ by removing data from the larger classes uniformly at random until the desired (smaller) ratio is achieved. Next, we perform ERM finetuning on this subset by adjusting sampling probabilities so that mini-batches are balanced in expectation. Using a class-imbalance ratio of 1:1 reduces to subsetting, and using the original class-imbalance ratio reduces to upsampling.

We finetune ConvNeXt-V2 Base on Waterbirds and CelebA and BERT Base on CivilComments, and we compare our class-balanced mixture method to the subsetting, upsampling, and upweighting techniques. The results of our experiments are displayed in Figure 2. We plot the performance of our mixture method with the best class-imbalance ratio during validation; an ablation study varying the ratio is included in Appendix B. Remarkably, mixture balancing outperforms all three class-balancing methods on Waterbirds and CivilComments, and while it does not outperform subsetting on CelebA, it significantly alleviates the WGA collapse experienced by upsampling.

Next, we perform an ablation of the necessity of subsetting in mixture balancing. We compare our method with an implementation which eschews subsetting, instead adjusting sampling probabilities so that the mini-batches have a particular class ratio in expectation. For example, instead of performing upsampling on a 2:1 class-imbalanced subset, we upsample the majority class by a ratio of 2:1 on the entire dataset. The results of our ablation are included in Appendix B; our mixture method outperforms the alternative, which incompletely corrects for class imbalance.

Figure 2: **Mixture balancing mitigates catastrophic collapse of upsampling and upweighting. We propose a class-balanced _mixture method_, which combines subsetting and upsampling by first drawing a class-imbalanced subset uniformly at random from the dataset, then adjusting sampling probabilities so that mini-batches are balanced in expectation. Our method increases exposure to majority class data without over-sampling the minority class. Remarkably, mixture balancing outperforms all three class-balancing methods on Waterbirds and CivilComments, and while it does not outperform subsetting on CelebA, it significantly alleviates the WGA collapse experienced by upsampling and upweighting. MultiNLI is class-balanced _a priori_, so we do not include it here.**

Note on validation.In Figure 2, we plot the best class-imbalance ratio achieved using validation on a group annotated held-out set. While this is a common assumption in the literature , it is nevertheless unrealistic when the training set does not have any group annotations. Therefore, we compare with both worst-class accuracy  and the _bias-unsupervised validation score_ of , which do not use any group annotations for model selection. In Table 1 we list the method which maximizes each validation metric as well as its average WGA. Overall, we show both methods are effective for model selection, often choosing the same method or mixture ratio as WGA validation.

Contextualization with previous work.Increasing exposure to majority class data without oversampling the minority class was previously explored by , who proposed averaging the weights of logistic regression models trained on ten independent class-balanced subsets. However, this method only works for _linear_ models -- as nonlinear models cannot be naively averaged -- and requires multiple training runs, which is computationally infeasible for neural networks. In comparison, our mixture method is a simple and efficient alternative which extends easily to nonlinear models.

The catastrophic collapse of class-balanced upsampling and upweighting can be mitigated by a _mixture method_. It increases exposure to majority class data without over-sampling the minority class and can improve baseline WGA beyond either technique.

## 4 Model scaling improves WGA of class-balanced finetuning

The relationship between overparameterization and group robustness has been well-studied, with often conflicting conclusions . In this section, we study the impact of model scaling on worst-group accuracy in a new setting -- finetuning pretrained models -- which more closely resembles practical use-cases. Importantly, we evaluate the impact of model scaling _in conjunction with class-balancing_ to isolate the impact of group inequities on WGA as a function of model size. We find that with appropriate class-balancing, overparameterization can in fact significantly improve WGA over a very wide range of parameter scales, including before and after the interpolation threshold. On the other hand, scaling on imbalanced datasets or with the wrong balancing technique can harm robustness.

We take advantage of advancements in efficient architectures  to finetune pretrained models in a wide range of scales from \(3.4\)M to \(101\)M parameters. We study six different sizes of ImageNet1K-pretrained ConvNeXt-V2 and five different sizes of Book Corpus/English Wikipedia pretrained BERT; specifications for each model size are included in Appendix A.2. Our results are displayed in Figure 3, and we include results for Swin Transformer in Appendix C.

We find that model scaling is beneficial for group robustness in conjunction with appropriate class-balancing, with improvements of up to \(12\%\) WGA for interpolating models and \(40\%\) WGA for non-interpolating models. This comes in stark contrast to scaling on class-imbalanced datasets or with the wrong class-balancing technique, which shows either a neutral trend or decrease in WGA -- the most severe examples being on CivilComments. With respect to interpolating models, CivilComments WGA decreases slightly after the interpolation threshold, while Waterbirds and CelebA continue to improve well beyond interpolation; on the other hand, BERT never interpolates MultiNLI, greatly increasing robustness at scale. It is unclear why Waterbirds and CelebA experience

   Validation Metric & Group Anns & Waterbirds & CelebA & CivilComments \\  Bias-unsupervised Score & ✗ & Upsampling (79.9) & Subsetting (\(74.1\)) & Mixture 3:1 (\(77.6\)) \\ Worst-class Accuracy & ✗ & Mixture 2:1 (\(81.1\)) & Subsetting (\(74.1\)) & Mixture 3:1 (\(77.6\)) \\ Worst-group Accuracy & ✓ & Mixture 2:1 (\(81.1\)) & Subsetting (\(74.1\)) & Mixture 3:1 (\(77.6\)) \\   

Table 1: **Mixture balancing is robust to model selection without group annotations.** We compare the best class-balancing method during validation with and without group annotations. Both worst-class accuracy  and the bias-unsupervised validation score of  are effective for model selection without group annotations, often choosing the same method or mixture ratio as worst-group accuracy (WGA) validation. We list the method maximizing each metric and its average WGA over \(3\) seeds.

different behavior from CivilComments interpolation -- the toy linear model of  suggests a benign "spurious-core information ratio", but a complete understanding is left to future investigation.

The most closely related work to ours is , who study the impact of scaling pretrained ResNet models on group robustness. However, because their experiments do not employ any form of class-balancing, their conclusions may be overly pessimistic. We replicate their experiments with our hyperparameters and contrast with our results using class-balancing in Figure 4. We find that class-balancing greatly affects their results: on Waterbirds, class-balancing enables a much more beneficial trend during model scaling regardless of whether a linear probe or the entire model is trained. Moreover, while class-balancing increases baseline WGA on CelebA but does not affect scaling behavior, we observe a more positive WGA trend when scaling ConvNeXt-V2 in Figure 3.

Contextualization with previous work.While previous work has primarily studied either linear probing of pretrained weights or training small models from scratch , we study full finetuning of large-scale pretrained models and show that class-balancing can have a major impact on scaling behavior. We compare directly with the most closely related work, , and show that class-balancing can either induce strikingly different scaling behavior or greatly increase baseline WGA. Overall, training with class-balancing allows us to isolate the impact of group inequities on robustness and more precisely observe the often-beneficial trend of model scaling for worst-group accuracy.

Figure 4: **Class-balancing greatly affects ResNet scaling results of .** We contrast the ResNet scaling behavior of  — who do not use class-balancing — to the scaling of class-balanced ResNets. We finetune each model size starting from pretrained checkpoints and plot the test worst-group accuracy (WGA), as well as the interpolation threshold, where the model reaches \(100\%\) training accuracy. On Waterbirds, we find that class-balancing enables a much more beneficial trend during model scaling. On CelebA, class-balancing greatly increases baseline WGA but does not affect scaling behavior (in contrast to the ConvNeXt-V2 plots in Figure 3). We use SGD for last-layer training and AdamW for full finetuning. See Appendix C for training accuracy plots.

Figure 3: **Scaling class-balanced pretrained models can improve worst-group accuracy.** We finetune each model size starting from pretrained checkpoints and plot the test worst-group accuracy (WGA) as well as the interpolation threshold, where model reaches \(100\%\) training accuracy. We find model scaling is generally beneficial for WGA _only in conjunction_ with appropriate class-balancing, and scaling on imbalanced datasets or with the wrong method can harm robustness. Note MultiNLI is class-balanced _a priori_ and is not interpolated. See Appendix C for training accuracy plots.

While overparameterization can sometimes harm WGA, pretraining and appropriate class-balancing make scaling generally beneficial. Moreover, modern language datasets are complex enough that standard models _do not interpolate_, greatly improving robustness at scale.

## 5 Spectral imbalance may exacerbate group disparities

In a recent paper,  propose _spectral imbalance_ of class covariance matrices, or differences in their eigenspectrum, as a source of disparities in accuracy _across classes_ even when balanced. Here, we examine whether similar insights hold in the group robustness setting. Our observations reveal surprising nuances in the behavior of _group-wise spectral imbalance_; nevertheless, we conclude that spectral imbalance may play a similar role in modulating WGA after class-balancing is applied.

Let us denote by \(_{i}\) the feature vector corresponding to a sample \(_{i}\) (_i.e.,_ the vectorized output of the _penultimate_ layer). Recall from Section 2 that \(_{g}\) is the set of indices of samples which belong to group \(g\). We further define \(}_{g}\) to be the empirical mean of features with group \(g\). To obtain the estimated eigenspectrum, we first compute the empirical covariance matrix for group \(g\) by

\[_{g}=|}_{i_{g}}(_{i}-}_{g})(_{i}-}_{g})^{}.\]

We then compute the eigenvalue decomposition \(_{g}=_{g}_{g}_{g}^{-1}\), where \(_{g}\) is a diagonal matrix with non-negative entries \(_{i}^{(g)}\) and the columns of \(_{g}\) are the eigenvectors of \(_{g}\). Without loss of generality, we assume \(_{1}^{(g)}_{2}^{(g)}_{m}^{(g)}\) where \(m\) is the rank of \(_{g}\).

We compute the group covariance matrices using a ConvNeXt-V2 Nano model for Waterbirds and CelebA, and a BERT Small model for CivilComments and MultiNLL.We plot the top \(10\) eigenvalues of each group covariance matrix in Figure 5. Even though we finetune with class-balancing, disparities in eigenvalues across groups are clearly visualized in Figure 5, especially for the largest eigenvalues. We include extensions to the top \(50\) eigenvalues and class covariance matrices in Appendix D.

Close observation of Figure 5 yields interesting findings. First, the group \(g^{*}\) that maximizes \(_{1}^{(g)}\) in each case belongs to a minority group; though, importantly, it may not belong to the _worst_ group. This is different from the findings of , who showed that the largest eigenvalues typically belong to the _worst-performing_ class. Second, we find that minority group eigenvalues are overall larger than majority group eigenvalues, but only when _conditioned on the class_. A majority group belonging to one class may have larger eigenvalues than a minority group belonging to another class, but there exists a consistent spectral imbalance between majority and minority groups within the same class.7

Figure 5: **Group disparities are visible in the top eigenvalues of the group covariance matrices.** We visualize the mean, across \(3\) experimental trials, of the top \(10\) eigenvalues of the group covariance matrices for a ConvNeXt-V2 Nano finetuned on Waterbirds and CelebA and a BERT Small finetuned on CivilComments and MultiNLI. The standard deviations are omitted for clarity. The models are finetuned using the best class-balancing method from Section 3 for each dataset. The group numbers are detailed in Table 2 and the minority groups within each class are denoted with an asterisk. The largest \(_{1}\) in each case belongs to a minority group, though it may not be the _worst_ group, and minority group eigenvalues are overall larger than majority group eigenvalues within the same class.

[MISSING_PAGE_EMPTY:10]

Acknowledgments.We thank Google Cloud for the gift of compute credits, Jacob Abernethy for additional compute assistance, and Chiraag Kaushik for helpful discussions. T.L. acknowledges support from the DoD NDSEG Fellowship. V.M. acknowledges support from the NSF (awards CCF-223915 and IIS-2212182), Google Research, Adobe Research and Amazon Research.