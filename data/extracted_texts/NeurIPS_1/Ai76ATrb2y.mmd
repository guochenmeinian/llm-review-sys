# Auditing Privacy Mechanisms via Label Inference Attacks

Robert Istvan Busa-Fekete

Google Research NY

busarobi@google.com

&Travis Dick

Google Research NY

tdick@google.com &Claudio Gentile

Google Research NY

cgentile@google.com &Andres Munoz Medina

Google Research NY

ammedina@google.com &Adam Smith

Boston University

& Google DeepMind

ads22@bu.edu &Marika Swanberg

Boston University

& Google Research NY

marikas@bu.edu

###### Abstract

We propose reconstruction advantage measures to audit label privatization mechanisms. A reconstruction advantage measure quantifies the increase in an attacker's ability to infer the true label of an unlabeled example when provided with a _private_ version of the labels in a dataset (e.g., aggregate of labels from different users or noisy labels output by randomized response), compared to an attacker that only observes the feature vectors, but may have prior knowledge of the correlation between features and labels. We consider two such auditing measures: one additive, and one multiplicative. These incorporate previous approaches taken in the literature on empirical auditing and differential privacy. The measures allow us to place a variety of proposed privatization schemes--some differentially private, some not--on the same footing. We analyze these measures theoretically under a distributional model which encapsulates reasonable adversarial settings. We also quantify their behavior empirically on real and simulated prediction tasks. Across a range of experimental settings, we find that differentially private schemes dominate or match the privacy-utility tradeoff of more heuristic approaches.

## 1 Introduction

Data sharing and processing provides an undeniable utility to individuals and the society at large. The modern data ecosystem has played a significant role in scientific advancement, economic growth, and technologies that benefit individuals in their daily lives. Yet, the collection, processing, and sharing of sensitive information can lead to real privacy harms. Understanding the theoretical and empirical risks associated with different types of data disclosure is a rich area of research and topic of discussion. The privacy community has studied a number of empirical measures of disclosure risks against several types of attacks, including: inference attacks , reconstruction attacks , re-identification attacks  and label inference attacks .

While differential privacy gives rigorous guarantees against these attacks by worst-case adversaries, it is natural to wonder whether non-DP (deterministic) mechanisms give any empirical privacy protections. This line of inquiry necessitates empirical measures of privacy that take adversarial uncertainty into account. At a fundamental level, whether one is studying reconstruction or membership inference, nearly every privacy disclosure metric comes down to _quantifying the relationship between an adversary's prior and posterior knowledge_ before and after a data release.

In this work, we consider a number of ways to summarize this prior-posterior relationship, and we apply our proposed metrics to a few Privacy Enhancing Technologies (PETs). We consider twofamilies of metrics: As a coarse measure, we consider the expected _additive_ difference between the prior and posterior success rates; while this does not capture unlikely high-disclosure events, it gives a sense for the risk posed to an average individual. To better capture such events, we then consider the finer-grained _multiplicative_ difference between the adversary's prior and posterior. These empirical measures of privacy risk allow us to relate and compare the risks of both aggregation-based and differential privacy-based PETs, and hence compare their privacy-accuracy trade-off curves: _What accuracy does one get for a given level of privacy protection?_ Together, these metrics paint a more balanced and nuanced picture of the disclosure risks and accuracy trade-offs posed by aggregation and differentially private mechanisms.

We will focus on the simplest possible scenario for information sharing: the case where a user wishes to disclose a single bit of information (for instance a binary label in a classification problem). While this problem is simple to present, it already highlights a lot of the difficulties in providing measures of privacy that are meaningful for both noise-based and aggregate-based tools. Most notably, it already requires handling the correlations--known to an attacker, but unknown to the mechanism--between the visible features and the hidden, sensitive labels. Similar to , we incorporate adversarial knowledge by a prior, and measure the difference between the prior and the adversary's posterior distribution conditional on observed aggregates.

Understanding binary labels already has practical implications. For instance, with Chrome's proposed conversion reporting API , the event of a user converting after clicking on an online ad -- buying a product, signing up for a newsletter, installing an app, etc. -- or not is considered sensitive, and therefore is reported only with some noise. However, once reported, ad tech providers can use features associated with an ad click (impression information, publisher information, etc.) to train models that can predict future conversions.

Our contribution.We make several contributions: (i) We introduce additive reconstruction advantage measures as ways to quantify the amount of leakage associated with a generic (not necessarily differentially private) privacy mechanism. These are variants and extensions of the Expected Attack Utility (EAU) and advantage contained in . (ii) We quantify such measures for randomized response and random label aggregation under different correlation assumptions with public data. (iii) We consider a more demanding multiplicative reconstruction advantage measure (in the spirit of predecessors of differential privacy [17; 10]), and again quantify this advantage for randomized response and random label aggregation. (iv) We conduct a series of experiments on benchmark and synthetic datasets measuring the privacy-utility tradeoff of a number of basic mechanisms, including randomized response, label aggregation and label aggregation plus Laplace or geometric noise. Remarkably, these experiments show that learning with aggregate labels tends to be strictly harder in practice than learning with randomly perturbed labels. That is, differentially private schemes dominate or match the privacy-utility tradeoff of label aggregate schemes for all or most privacy levels and for both types of advantage measure, a conclusion we have not seen clearly spelled out in prior work on privacy. Even measured "on their own turf", deterministic aggregation--which lack provable guarantees like differential privacy--do not provide a significant advantage.

Prior Work.Our measures of privacy risk relate closely to previous work on membership inference and auditing, as well as to variants of differential privacy that assume adversarial uncertainty. We discuss these in Section 3.3, after defining our measures.

Reproducibility.For the sake of full reproducibility of our experimental setting and results, our code is available at the link [https://github.com/google-research/google-research/tree/master/auditing_privacy_via_lia](https://github.com/google-research/google-research/tree/master/auditing_privacy_via_lia).

## 2 Preliminaries

For a natural number \(n\), let \([n]=\{i i n\}\). Let \(\) denote a feature (or instance) space and \(\) be a binary (\(=\{0,1\}\)) or multiclass (\(=[c]\)) label space. We assume the existence of a joint distribution \(\) on \(\), encoding the correlation between the input features and the labels. The marginal distribution over \(\) will be denoted by \(_{}\), and the conditional distribution of \(y\) given \(x\) will be denoted by \(_{|x}\). For distributions over binary labels, we denote by \((x)=_{y_{|x}}(y=1|x)\) the probability of drawing label 1 conditioned on feature vector \(x\). In the multiclass case, we will use \((x,a)=_{y_{|x}}(y=a|x)\), for \(a=[c]\). We define a dataset \(S=(x_{1},y_{1}),,(x_{m},y_{m})\) as a sequence of pairs \((x_{i},y_{i})\), each one drawn i.i.d. from \(\). We use \(=(x_{1},,x_{m})\) to denote the features in \(S\) and \(=(y_{1},,y_{m})\) to denote the corresponding labels. We will sometimes abuse the notation in the math, and write for brevity \((,)\) instead of \((x_{1},y_{1}),,(x_{m},y_{m}).\) Consistent with this notation abuse, we will often use \(^{m}^{m}\) and \(()^{m}\) interchangeably--no ambiguity will arise.

In order to unify the study of privacy enhancing technologies, for the rest of the paper we model PETs as (possibly randomized) functions \(:()^{m}.\) These functions map a collection of \(m\) labeled examples to a privacy protected representation in the domain \(\). The domain \(\) depends on the PET but, throughout the paper, it will be clear from the context.

In our setting, we consider PETs that protect labels and release features in the clear (so, strictly speaking, the output of a PET should also include \(\) itself; we leave implicit in our notation). Despite this, our reconstruction advantage measures can be applied to any PET, we recall below two standard (and very basic) PETs that have been proposed for various Ads metrics APIs: one which satisfies label differential privacy, and one which relies on random aggregation. These are the two PETs on which we will also be able to give theoretical guarantees. We formulate them for binary classification.

**Definition 2.1**.: _Given \( 0\), we say that a (randomized) algorithm \(A\) that takes as input \(S\) is \(\)-Label Differentially Private (\(\)-Label DP) if for any two datasets \(S\) and \(S^{}\) that differ in the label of a single sample we have \((A(S) B) e^{}\)\((A(S^{}) B),\) where \(B\) is any subset of the output space of \(A\)._

Randomized Response (RR) is a classical  way of achieving \(\)-Label DP. In the binary classification case, RR with privacy parameter \(=1/(1+e^{})\) simply works by randomly flipping each label \(y_{j}\) in the dataset with independent probability \(\) before revealing it to the learning algorithm. For a fixed \(>0\), RR corresponds to the function \(_{}(,)=}=(_{1},,_{m})\{0,1\}^{m},\) where \(_{i}=1-y_{i}\) with probability \(=1/(1+e^{})\) and equal to \(y_{i}\) with probability \(1-\), independently for each \(i[m].\)

A completely different label privacy mechanism, still very utilized in practice (see, e.g., the Privacy Sandbox API in Google  and the Apple SKAN initiative ) is one based on (random) label aggregation, whereby the dataset \(S\) gets partitioned uniformly at random into _bags_ of a given size \(k\), \(S=(x_{11},y_{11}),,(x_{1k},y_{1k}),,(x_{n1},y_{n1}),, (x_{nk},y_{nk}),\) and only feature vectors and the fraction of positive labels in each bag are revealed to the attacker/learning algorithm. In other words, the attacker has access to \(S\) via a collection \(\{(_{i},_{i}),\,i[n]\}\) of \(n\) labeled bags of size \(k\), with \(m=nk\), where \(_{i}=\{x_{ij} j[k]\}\), \(_{i}=_{j=1}^{k}y_{ij}\) is the label proportion (fraction of labels "1") in the \(i\)-th bag, and all the involved samples \((x_{ij},y_{ij})\) are drawn i.i.d. from \(\). Thus, the attacker receives information about the \(m\) labels \(y_{ij}\) of the \(m\) instances \(x_{ij}\) from dataset \(S\) only in the aggregate form determined by the \(n\) label proportions \(_{i}\). Note, however, that the feature vectors \(x_{ij}\) are individually observed. From an attacker viewpoint, this setting is sometimes called Learning from Label Proportions (LLP). Hence, LLP corresponds to the function \(_{}(,)=(_{1},,_{n} )^{n}.\)

Learning from privatized labels.Randomized response is especially appealing from a practical point of view, since privatized data with label flipping can be handled by many prominent learning algorithms as is, with some tuning of their hyper-parameters. Often the theoretical guarantees of these learners in terms of sample complexity are only deteriorated by some constant that depends on the label noise level, see, for example . To improve accuracy, we debias the gradients in a post-processing step similar to Equation (7) in ; we discuss the debiasing details in Appendix C.

On the other hand, a simple and very well-known method for learning from aggregate labels is the one that of  call Empirical Proportion Risk Minimization. In fact, different versions of this algorithm are discussed in the literature without a clear reference to its origin. In , the authors simply call this algorithm the Proportion Matching algorithm (PropMatch), and we shall adopt their terminology here. Given a loss function \(:\,^{+}\), a hypothesis set of functions \(^{}\), mapping \(\) to a (convex) prediction space \(}\), and a collection \(\{(_{i},_{i}),\,i[n]\}\) of \(n\) labeled bags of size \(k\), PropMatch minimizes the empirical _proportion matching loss_, i.e., it solves the following optimization problem: \(_{h}_{i=1}^{n}(_{j=1}^{k}h(x_{ ij}),_{i})\). It is known that the above method is consistent (see for instance ). That is, training a model this way - under some mild conditions on \(\) and \(\) and for a large enough sample - results in learning a model \(h\) that minimizes the expected _event level_ loss \(_{(x,y)}[(h(x),y)]\). Finally, for our baselines, it will be helpful to consider a PET that reveals no label information at all: \(_{}(,)=\).

## 3 Auditing Large-Scale Label Inference

In this section, we propose a number of _auditing metrics_ to measure the risk of large-scale label reconstruction. Unlike DP auditing techniques which focus on worst-case guarantees, we will focus here on distributional guarantees. In doing so, we extend the (additive) reconstruction advantage definition introduced in  to the LLP setting and propose meaningful variants of it.

Our reconstruction advantage measures can be applied to virtually any PET for auditing purposes. For concreteness, we will also provide analytical bounds on such measures when applied to PETs like RR and label aggregation. On one hand, these bounds help illustrate the precise dependence on the data distribution \(\). On the other, they will pave the way for our experimental findings in Section 4.

A reconstruction advantage metric is grounded in the following natural privacy question: _How much does releasing the output of a PET increase the risk of label inference compared to not releasing anything?_ This corresponds naturally to measuring an attacker's _prior_ over a target person's label compared to the attacker's _posterior_ after viewing the mechanism output.

We model the attacker (often called 'adversary' later on) as a function \(:^{m}^{m}\) that maps the features \(\) and the output of a PET \((,)\) to a vector of predicted labels, one for each example. We compare this attacker's success to the the _uninformed_ or _prior_ attacker that gets \(\) and the PET that reveals no label information \(_{}(,)=\). To measure the efficacy of an attacker compared to its prior, we define a number of _attack utility_ variants.

The _Expected Attack Utility_ of adversary \(\) using information from PET \(\) on a collection of \(m\) examples drawn i.i.d. from a distribution \(\) is defined as:

\[(,,)=, )^{m},\;i([m]),\;\; }{}((,(, ))_{i}=y_{i})\,,\]

where \((,)_{i}\) is the \(i\)-th component of vector \((,)\). In words, the expected attack utility of adversary \(\) is the probability that \(\) correctly guesses the label of a randomly chosen example when provided the features and the output of \(\). Equivalently, this is the expected fraction of the \(m\) examples that the adversary predicts the correct label for. The adversary's success rate may depend on the distribution over features and labels. For example, if labels are entirely determined by features, then our metric should reflect that privatized labels (for any mechanism \(\)) reveal no additional information about the true labels. To control for the information that features inherently reveal about labels, we assume that the adversary has knowledge of the data distribution \(\) over \(\), either completely (e.g., Definition 3.1) or approximately through learning on disjoint data (as in our experiments in Section 4).

Further, we define the _Individual Expected Attack Utility_ on input data \(x_{i}\) as

\[_{i}(,,,x_{i})\ =\ _{|x_{i}}(^{(-i)},^{(-i)})^{m-1},\;\;}{}((,(,))_{i}=y_{i}\,\,x_{i})\,,\]

where \(^{(-i)}\) is \(\) with the \(i\)-th item \(x_{i}\) dropped, and likewise for \(^{(-i)}\). The quantity \(_{i}(,,,x_{i})\) emphasizes the (expected) attack utility on a specific piece of data \(x_{i}\) when the associated label \(y_{i}\) is drawn from the conditional distribution \(_{|x_{i}}\). For instance, \(_{i}(,_{},,x_{i})\) measures, for a given \(x_{i}\), the chance that an attacker is able to reconstruct the associated label \(y_{i}\), if \(y_{i}\) generated from \(_{|x_{i}}\) and, by virtue of mechanism \(_{}\), \(y_{i}\) becomes part of a bag (of some size \(k\)), only the bag label proportion \(\) being revealed to the attacker.

In order to measure the _increase_ in risk incurred by releasing the output of a PET, we consider the attack utility of an optimal adversary in two scenarios: one in which the adversary gets the features, \(\), together with the output of the PET, \((,)\), and an alternate setting where the adversary gets only the features (which is equivalent to using \(_{}\)). We call the difference in attack utility between the informed and uninformed adversary the _attack advantage_. Intuitively, the attack advantage measures the label reidentification risk that can be attributed to the PET rather than to correlations between the features \(\) and labels \(\) which are inherent in the distribution \(\). Since we have given two notions of attack utility above, we have two corresponding notions of attack advantage. Below is an _additive_ version, based on utility _differences_, later on (Section 3.2) we will also consider a _multiplicative_ version, based on utility _ratios_.

**Definition 3.1**.: _Given a PET \(\) for a set of \(m\) examples drawn from a data distribution \(\), the (additive) Expected attack Advantage is defined as_

\[(,)=_{_{}} (_{},,) -_{_{}}(_{},_{},)\.\]

_Similarly, when the \(i\)-th item \(x_{i}\) is kept frozen, the Individual Expected attack Advantage is defined as_

\[(,,x_{i})=_{_{}} (_{},,,x_{i})- _{_{}}(_{},_{},,x_{i}).\]

Note that RR is generally described in terms of its behavior on a single example \((x,y)\), rather than a collection of \(m\) i.i.d. samples. Likewise, label aggregation operates on each bag of size \(k<<m\) independently. However, since the data is i.i.d., it is easy to see that the attack advantage for RR is independent of the number of examples \(m\), while the attack advantage for (random) label aggregation will only depend on \(k\), rather than \(m\).

Appendix A.5 contains further variants of attack advantage, like one that accounts for the _tail_ of the attack advantage distribution, as well as associated bounds. These can be stretched to the point where the feature vector \(=(x_{1},,x_{m})\) is arbitrary, and only the properties of the condition distribution \(_{|x}\) is factored in.

### Bounding the Additive Attack Advantage

As a warm up, we begin by studying the additive expected attack advantage for LLP when the labels are independent of the features. Note that, since the features \(\) do not play any role in this simplified setting, the notions of advantage coincide. We recall again that all results involving \(_{}\) (Theorems 3.2, 3.3 and 3.4 below) deal with a _single_ bag when computing advantage measures (hence we will write \(\) instead of \(_{i}\)). This is because distribution \(\) is known to the adversary, and the bags the adversary observes are independent of one another, hence there is no extra advantage from operating on an entire dataset of bags at once. All proofs are given in the appendix.

**Theorem 3.2**.: _Fix a data distribution \(\), let \(p=_{(x,y)}(y=1)\), and fix an arbitrary threshold \([0,1/2]\). If labels are independent of features (i.e., \(\) is a product of distributions over \(\) and \(\)), then for all bag sizes \(k 1\) we have:_

\[(_{},)=\{p,1-p\}-_{ }[\{,1-\}]}& {if }p\\ e^{-(^{2}k)}&|p-1/2|,\]

_where \(()\) hides constants independent of \(\) and \(k\)._

A couple of remarks are in order. First, observe that, as expected, the advantage \((_{},)\) is always non-negative. This can be easily derived by noting that \([]=p\) and then applying Jensen's inequality to the concave function \(x\{x,1-x\}\), for \(x\). Second, despite being non-negative, Theorem 3.2 also proves the desirable property that \((_{},)\) goes to zero as the bag size \(k\) increases. The convergence rate is in general of the form \(1/\), but it becomes _negative exponential_ in \(k\) when \(p\) is bounded away from 1/2.

We now investigate general distributions over features and labels and consider in turn \(\) and \(\).

**Theorem 3.3**.: _Let \(\) be an arbitrary distribution on \(\), \(p=[(x)]\), and \(=[(x)(1-(x))]\). Then, for all bag sizes \(k 2\) we have:_

\[(_{},)= {^{1/4}(p(1-p))^{1/4}}{}+}{k}\,\]

_where \(\) hides logarithmic factors in \(k\)._

Hence, also in this more general case of LLP, the advantage converges to zero as the bag size \(k\) grows large. Compared to the rate in Theorem 3.2, we are only losing the \( k\) factors implicit in the \(\) notation. This is because, when applied to the scenario where labels and features are independent, \((x)=p\) is constant with \(x\), so that \(=p(1-p)\), and the first term becomes \(}\), while the second one reads \(}{k}\), which is lower order when \(k\) is large. We strongly believe that the tighter gap-dependent analysis we carried out for Theorem 3.2 extends to the more general scenario of Theorem 3.3, but we leave this as an open question.

The corresponding bound for the individual expected attack advantage is given next.

**Theorem 3.4**.: _Under the same assumptions and notation as in Theorem 3.3, we have, for \(>0\), and \(k(1/)\),_

\[(_{},,x_{i})=_{i}\,((p(1-p))^{1/4}+k}}+}+k^{2}}}\,),\]

_where \(_{i}=(x_{i})(1-(x_{i}))\), and \(\) hides logarithmic factors in \(k\)._

In the bounds of Theorems 3.2, 3.3 and 3.4 the dependence on the data distribution \(\) is encoded in \(p(1-p)\) and \(\); as \(p(1-p)\) and/or \(\) gets smaller we should naturally expect a smaller advantage, as the adversary is facing an easier label prediction problem. Appendix A.5 contains further distribution-dependent results, like a bound on the advantage that, conditioned on \(=(x_{1},,x_{m})\), is of the form \(_{i=1}^{k}}{_{j}}},\) where \(_{i}=(x_{i})(1-(x_{i}))\).

We now provide the corresponding expression for the additive attack advantage for RR (at a given level \( 0\)). We recall that the results of  imply that every \(\)-label-DP PET \(\) has advantage bounded as \((,) 1-}\). However, one drawback of this bound is that it is distribution independent (or, rather, worst case over distribution \(\)). Yet, the attack advantage depends heavily on \(\). For an extreme example, if we have \(_{(x,y)}(y=1)=1\), the attack advantage is zero for every PET, which is not directly captured by only relying on the properties of \(\)-DP.

Recall that, since RR operates on each example independently, the advantage is independent of the number of examples \(m\) (a formal proof is given in Appendix A.6). Hence we derive a bound for \((_{},,x_{1})\), and an expression for the optimal adversary under RR (see Appendix A.7).

**Theorem 3.5**.: _For any data distribution \(\), the individual expected attack advantage \((_{},,x_{1})\) for randomized response with privacy parameter \(=}\) is equal to \((\{(x_{1}),1-(x_{1})\}-)\{(x_{1} )[,1-]\}\,.\)_

We use the above expression in our experiments (Section 4) to estimate the attack advantage of RR for various values of \(\). Being distribution dependent, this expression leads to much tighter bounds on the attack advantage. For example at \(=1\), the bound from  is \(1- 0.46\), while for one dataset used in our experiments we see that the attack advantage for RR at \(=1\) is only \(0.00095\).

### Multiplicative Attack Advantage

The definitions of attack advantage given so far measure the _absolute_ change in successful reconstruction, not the _relative_ change. Moreover, they do not fully capture the different levels of confidence in the reconstruction, since they involve an expectation over either \((,)\) or \(\) given \(\). Next, we give an additional set of definitions that capture these important nuances. For the sake of brevity, in this section we directly instantiate the adversaries \(_{}\) and \(_{}\) to Bayes optimal predictors.

**Definition 3.6**.: _Given data distribution \(\), mechanism \(\), index \(i\), and pair of labels \(a,b\), the multiplicative advantage is the difference of log odds ratios:_

\[I_{a,b}(,,,z,i)=(, z,a)}{_{i}(,z,b)}-,a)}{(x_{i},b)}\]

_where \((x_{i},a)=(y_{i}=a\,|\,x_{i})\) and \(_{i}(,z,a)=_{_{| }^{}}(y_{i}=a\,|\,,(,)=z)\) is the probability of \(y_{i}=a\) after observing the output \(z\) of \(\). We denote the binary case by \(I_{1,0}()=I()\) and shorten \((x_{i}):=(x_{i},1)\) and \(_{i}(,z):=_{i}(,z,1)\)._

This particular formulation has the advantage that \(I_{a,b}(,,,z,i)\) is large in absolute value whenever there is a large _relative_ change in either label probability. This also entails that a multiplicative-style advantage metric is generally stronger than an additive one.

When \((x_{i})\) and \(_{i}(,z)\) are both less than 1/2 (in the binary setting), we have \(I(,,,z,i)=((, z)}{(x_{i})})\), with a symmetric expression for the case that both are close to 1. Because we assume i.i.d. sampling, one can easily see that \(I_{a,b}(,,,z,i)=( (,)=z\,|\,y_{i}=a,)}{(( ,)=z\,|\,y_{i}=b,)}\,,\) which makes it clear that for \(\)-label DP mechanisms like \(_{}\) (at level \(\)) the above log ratio is at most \(\)in absolute value for all \(\) and \(\). On the other hand, for \(_{}\), there is always a small chance that all the examples in the bag will have the same label making the above log ratio infinite. However, for large \(k\) and distributions \(\) in which the \((x,a)\) values are not too close to 0 or 1, the leakage \(I_{a,b}(,,,z,i)\) might be small in most cases.

What values of multiplicative advantage should be considered acceptable? We argue that this probability should be viewed as a probability of system failure and set appropriately small. (For example, when running with \(_{}\) on modern-scale data sets, we might create billions of bags; even a tiny probability of failure can lead to many bags whose individuals have their labels revealed exactly). The next theorem gives high probability bounds for \(_{}\) in the simple case when the distribution \(\) is a product distribution.

**Theorem 3.7**.: _Fix a data distribution \(\) over features and binary labels, let \(p=_{(x,y)}(y=1)\), and assume the labels are independent of features (i.e., \(\) is a product of distributions over \(\) and \(\)). Then there are universal constants \(c_{1},c_{2}>0\) such that for \(p(0,1)\), all bag sizes \(k(1/)}{p(1-p)}\), and all \(i[k]\) we have_

\[_{(,)^{}_{|_{x _{i}}}}(I_{1,0}(_{},,, _{}(,),i)>c_{2}}).\]

Then, when \(k\) is sufficiently large and the conditional label probabilities \((x_{i})\) are all equal to some constant \(p(0,1)\), the probability of failure drops off exponentially as \(k\) increases. When working with a data set of size \(m\), we can substitute \(=^{}\) to get a bound on the probability that any bag in the dataset exhibits extreme values of leakage \(I_{a,b}\). For the setting covered by Theorem 3.7, one gets that the bags must be of size \(k(n)/(p(1-p))\) for the probability of extreme bags to converge towards 0. In the experiments in Section 4, we report results on both the additive and the multiplicative reconstruction advantage criteria.

### Connection to prior work on auditing and membership inference, and distributional DP

Our approach on quantifying attack advantage can be viewed as fitting into a recent line of work on auditing learning algorithms via membership inference attacks. This class of attacks was introduced by  and subsequently studied in theory [11; 18] and practice . Whether used as attacks or empirical lower bounds on differential privacy parameters , these approaches set up a hypothesis test for the presence or absence of a particular target record in the training data . They diverge in whether they consider an adversary with access to all the rest of the training data ("fully informed"), or with only distributional knowledge of the rest of the training data (usually made available to the attacker as an independent sample drawn from the same distribution).

In our setting, testing for the presence of an individual makes no sense, since individual records' features are known. In the binary case, our measures are success metrics for testing the hypothesis that a particular individual's label is 1 as opposed to 0: Multiplicative advantage bounds the ratio of true- to false-positive rates of the Bayes' optimal test, while additive advantage measures the difference of true- and false-positive rates. This perspective was also taken in previous work: for example,  use such a test to lower-bound the differential privacy parameters of label-DP algorithms. However, their modeling assumes a fully informed adversary (that knows all labels). In contrast, we posit a reasonable model of adversarial uncertainty in order to to look at the risks of mechanisms such as label aggregation. In this respect, our approach follows more in the line of membership inference attacks that use only distributional knowledge, or to variants of differential privacy that assume adversarial uncertainty such as [9; 7; 16].

Further relevant references include [25; 32], which have been developed more in the Cryptography literature. Like here, those papers propose to measure prediction advantage by comparing the information status of an agent before and after seen the obfuscated data. Yet, they do not consider the public features/private label mixed setting we study here, which is clearly necessary when investigating, for instance, the LLP mechanism. As a result, the measures proposed in [25; 32] generally fail to distinguish between the information gained as a result of: (1) correlation to the public features, and (2) the mechanism output. Hence, they overestimate the revelation of the mechanism, at times to the extreme. Consider, for clarity, the case where \((x)\) is either 0 or 1 for all \(x\). In this case the adversary's inference advantage is zero for any reasonable measure of advantage, since the adversary has perfect knowledge of the labels without the mechanism output.

Experiments

In this section, we demonstrate how our advantage measures can be used to quantify and compare the potential privacy leakage of RR, LLP, and two additional PETs with a range of privacy parameters. See Appendix C for full details on our PETs, experimental setup, and additional results.

**Mechanisms.** We empirically evaluate a number of proposed mechanisms for label privacy: randomized response (RR), Label Proportions (LLP), and two further PETs: LLP+Lap, where (zero-mean) Laplace noise is added to the label aggregate in each bag, and LLP+Geom, where geometric noise is added and then clipped so the estimated proportion lies in \(\). The noise scale for both LLP+Lap and LLP+Geom is chosen so that the PETs satisfy \(c\) label differential privacy. Postprocessed clipped geometric noise is the mechanism for releasing a binary sum that is optimal among all differentially private mechanisms for several loss measures . To minimize squared error, the optimal post-processing is to correct for the bias introduced by clipping. We empirically study these advantage measures on a variety of synthetic and benchmark datasets, reporting either the AUC vs. advantage trade-off or the prior-vs-posterior scatter plots, that help shed light on the distribution of our advantage measures on different points in the dataset for the various PETs.

**Estimating class conditionals for advantage and PET Utility.** The advantage measures for all PETs we consider can all be computed as a function of the class conditional probabilities \((x)=(y=1|x)\). In our synthetic datasets we compute the advantage measures for different distributions on \((x)\); however, for non-synthetic data sets, the value of \((x)\) is not explicitly known. Instead, we estimate \((x)\) for each \(x\) by training a classifier \(h\) (without any PETs) and use the prediction probability \(h(x)\) as a proxy for \((x)\). We compute the figure of merit (additive or multiplicative advantage) for the optimal informed and uninformed attackers using these estimates.

We also measure the utility of each PET for trained models. For each dataset, PET, and privacy parameters, we apply the PET to the training labels to produce a privatized version. We then train a model on the privatized data using minibatch gradient descent with the Adam optimizer  and a loss function designed for the PET. For RR, the loss debiases the binary crossentropy loss when evaluated on the RR labels, and for LLP, LLP+Laplace, and LLP+Geom, we minimize the Empirical Proportion Risk defined in Section 2. For each dataset, PET, and privacy parameters, we perform a grid search over the learning rate parameter and report the test AUC of the best performing learning rate. All utility results are averaged over multiple runs. The maximum standard error in the mean for the reported AUCs is \(0.0076\) and the vast majority are less than \(0.002\).

Computing posterior distributions for each PET.Given a list of priors (or estimated priors) for each data point \(\{(x_{i})\}_{i=1}^{n}\), we analytically compute the posterior probabilities for each of the PETs we consider.

First, we apply Bayes' theorem and use the fact that the \(x_{i}\)'s are independent:

\[(y_{i}=1|x,(x,y)=z)=((x,y)=z|x, y_{i}=1)(y_{i}=1|x_{i})}{((x,y)=z|x)}. \]

Figure 1: Prior-posterior scatter plots for LLP, RR, and LLP+Geom from two synthetic datasets (where the prior \((x)\) is drawn) and the two real-world datasets (where \((x)\) is approximated). The colors of the dots correspond to different parameter values for the PETs. For each bag size \(k\) and distribution, we did 1000 independent runs. The further a point is from the \(y=x\) dotted line, the more is revealed about its label as a result of the PET.

For RR with flipping probability \(p\), Equation 1 evaluates to \()}{p(x_{i})-(1-p)(1-(x_{i})}\) if the outcome of \(_{}(y_{i})=0\), and to \()}{(1-p)(x_{i})-p(1-(x_{i})}\) if the outcome is 1.

For label aggregation (LLP), the bags are independent, so we consider only a single bag at a time. \(_{}\) is deterministic, so in the numerator \((_{}(x,y)=z\,|\,x,y_{i}=1)\) is the probability density at \(z\) of a Poisson binomial distribution with flipping probabilities \(((x_{1}),,(x_{i-1}),1,(x_{i+1}),,(x_{k}))\). Similarly, the denominator is the probability density at \(z\) of a Poisson binomial distribution with flipping probabilities \(((x_{1}),,(x_{k}))\).

For \(_{}\) and \(_{}\), the terms in Equation 1 are convolutions of the posterior for LLP and the Laplace or Geometric distributions, respectively.

Results.Our results are reported in Figures 1, 2, and 3, as well as in Appendix C. We used two synthetic datasets and two real-world datasets (Higgs and KDD12). Notice that Higgs is a relatively balanced dataset, while KDD12 is a quite imbalanced one.

Figure 1 give scatter plots of prior \((y=1\,|\,)\) vs. posterior \((y=1\,|\,,(,)=z)\) distribution on the four datasets for LLP, RR and LLP+Geom with different colors for each value of parameters \(k\) (for LLP and LLP+Geom) and \(\) (for RR and LLP+Geom). The behavior of LLP+Lap is reported in Appendix C for completeness, but it turned out to be similar to that of LLP+Geom.

It is instructive to observe how the points spread w.r.t. the main diagonal \(y=x\). Points that are on the diagonal have _no label privacy loss_ as a result of the PET, because the posterior is identical to the prior. On the other hand, points with a posterior of 0 or 1 have _complete privacy loss_, since the posterior on the private label is deterministic. In general, a wider spread away from \(y=x\) indicates more privacy loss. The scatter plots tend to form spindle shapes whose width is determined by the privacy parameters of the PETs. Yet, there is a substantial difference between LLP and RR. While RR generates points on the boundaries of the spindles (middle row), LLP tends to spread such points more uniformly. Moreover, some of the points for LLP lie on the edges of the square \(^{2}\), which correspond to an _infinite_ multiplicative advantage. LLP+Lap (bottom row) is somewhere in between, in that the points are also located inside the spindles, but never on the edges of the square. Note that a spindle boundary is the set of points having the same multiplicative advantage measure. Moreover, these differences become more pronounced on skewed datasets like KDD12 as the probability of homogenous bags (bags of all 0 labels or 1 labels) become much more likely.

These scatter plots are already suggestive of the expected behavior of the utility-privacy tradeoff curves that will come next in Figure 3. Given an allowed level of privacy (as measured by either additive or multiplicative advantage) each point in the interior of the spindle associated with that privacy level will also lie in the boundary of a smaller (thus higher privacy) spindle. This will make inference harder, thereby reducing (average) utility at that level of privacy. Following this intuition, we expect RR to achieve a higher utility-privacy curve than LLP, with LLP+Geom somewhere in between. A more detailed comparison between RR and LLP+Geom (or LLP+Lap) is in Appendix D.

Figure 2 helps further illustrate the different behavior of the considered PETs vis-a-vis the advantage measure. On the four datasets, we pick here a set of parameters that make the PETs AUC-comparable on Higgs. We then plot the empirical Cumulative Distribution Functions (CDFs) on the four datasets for both multiplicative (middle row) and additive (last row) measures. While the CDFs of additive

Figure 2: **Top:** Prior-posterior scatter plots for RR (grey), LLP (blue), and LLP+Geom (orange) with \(=1\) and \(k=8\) on the same datasets as in Figure 1. With these choice of parameters, the three mechanisms roughly achieve the same AUC on Higgs. **Middle:** Empirical CDFs of (the absolute value of) the multiplicative advantage for the three PETs on the four datasets. **Bottom:** CDFs of the additive advantage.

measures are roughly similar across the three PETs, this is not the case for the highly skewed Beta(2,30) and KDD12 datasets, where one can easily spot for LLP (blue line) the presence of a significant mass of points with large multiplicative advantage. Note that in the middle row, the CDF of RR (grey line) is just a vertical line (\(x=1\)) while the CDF of LLP+Geom (orange line) first follows the blue line and then the grey one. These high multiplicative advantage points cannot be detected when only relying on the CDFs of the additive measure.

**Utility vs. advantage tradeoff on benchmark datasets.** The experiments so far have compared the distribution of individual additive and multiplicative advantage induced by RR, LLP, and LLP+Geom for various parameter settings and datasets. However, the use of PETs is always a tradeoff between utility and privacy, since if we cared about privacy alone, the best strategy would be to release no information at all. Figure 3 plots the AUC vs advantage for each PET as we vary the PET's privacy parameters. Note that, since the \(x\)-axis is advantage, we are able to put the normally incomparable privacy parameters of the PETs on equal footing. For RR and LLP, the single privacy parameter (\(\) and \(k\), respectively) traces out an AUC vs Advantage curve. Since LLP+Lap and LLP+Geom have two parameters, there is an area of achievable AUC vs Advantage pairs. For these mechanisms, we plot a separate curve for each bag size \(k\) showing the tradeoff when varying \(\) for that \(k\).

When measuring privacy loss via multiplicative advantage, RR has the best privacy vs. accuracy tradeoff compared to all other PETs. This is consistent with the observations in Fig. 2. In particular, the three mechanisms (with the given parameters) have almost the same AUC on Higgs. Yet when looking at the CDFs of the multiplicative measure one sees that the two DP mechanisms have bounded MAs whereas LLP has a significant number of extreme values. This trend is more pronounced for KDD12 than Higgs. Thus, for the same AUC, LLP has a higher multiplicative advantage, with a more pronounced gap for KDD12 than for Higgs. On the other hand, Fig. 2 also shows that the CDFs of the additive advantage are roughly the same for all three mechanisms (again, at the same AUC). Thus, in Fig. 3 the mechanisms have similar AUC vs advantage tradeoffs.

## 5 Discussion and Conclusions

We study ways to audit label privatization mechanisms for commonly used PETs: randomized response, random label aggregation, and combinations thereof. Together, the additive and multiplicative advantage measures we introduced paint a richer picture of the reconstruction risks posed by different parameter settings, and for the first time allow us to compare their privacy-accuracy trade-off curves.

The measures we propose are tailored to settings where the data are sampled i.i.d. from a distribution, and each record consists of public features together with one sensitive binary feature. Computing these measures empirically requires estimates of the adversary's label uncertainty, which won't be correct if the adversary has significant side information or, crucially, if the same data are re-used in multiple mechanisms. Handling such complex settings requires more general concepts like differential privacy. Another complexity emerges when we consider settings like click prediction, where a minority label (a click) is viewed as qualitatively more revelatory than the majority one (no click). While the measures we consider are agnostic to the label semantics, principled ways to incorporate complex semantics might be valuable and shed light on heuristics used in practice.

Figure 3: Privacy vs utility tradeoff curves for the various PETs on the Higgs and KDD12 datasets. Utility is measured by AUC on test set, while privacy is either the additive measure (bottom row) or the 98th-percentile of the multiplicative measure (so as to rule out the infinite multiplicative advantage cases that can occur for LLP). Each point corresponds to a setting of the privacy parameter for the PET (\(\) for RR, \(k\) for LLP, and both for LLP+Geom). The \(x\)-coordinate is the advantage (either additive or multiplicative) value for that PET, while the \(y\)-coordinate is the test AUC of a model trained from the output of that PET. The AUC of the model trained without a PET roughly corresponds to the top value achieved by these curves.