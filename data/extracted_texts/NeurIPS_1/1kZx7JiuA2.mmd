# Implicit Transfer Operator Learning: Multiple Time-Resolution Surrogates for Molecular Dynamics

Mathias Schreiner

DTU

Chalmers University of Technology

matschreiner@gmail.com

&Ole Winther

DTU

Chalmers University of Technology

simonols@chalmers.se

&Simon Olsson

Chalmers University of Technology

simonols@chalmers.se

Contributions to this work were done while visiting Chalmers University of TechnologyTechnical University of DenmarkCorresponding author

###### Abstract

Computing properties of molecular systems rely on estimating expectations of the (unnormalized) Boltzmann distribution. Molecular dynamics (MD) is a broadly adopted technique to approximate such quantities. However, stable simulations rely on very small integration time-steps (\(10^{-15}\,\)), whereas convergence of some moments, e.g. binding free energy or rates, might rely on sampling processes on time-scales as long as \(10^{-1}\,\), and these simulations must be repeated for every molecular system independently. Here, we present Implicit Transfer Operator (ITO) Learning, a framework to learn surrogates of the simulation process with multiple time-resolutions. We implement ITO with denoising diffusion probabilistic models with a new SE(3) equivariant architecture and show the resulting models can generate self-consistent stochastic dynamics across multiple time-scales, even when the system is only partially observed. Finally, we present a coarse-grained CG-SE3-ITO model which can quantitatively model all-atom molecular dynamics using only coarse molecular representations. As such, ITO provides an important step towards multiple time- and space-resolution acceleration of MD. Code is available at [https://github.com/olsson-group/ito](https://github.com/olsson-group/ito)

## 1 Introduction

Numerical simulation of stochastic differential equations (SDE) is critical in the sciences, including statistics, physics, chemistry, and biology applications . Molecular dynamics (MD) simulations are an important example of such simulations . These simulations prescribe a set of mechanistic rules governing the time evolution of a molecular system through numerical integration of, for example, the Langevin equation . MD grants mechanistic insights into experimental observables. These observables are expectations, including time-correlations, of observable functions (e.g., pairwise distances or angles) computed for the Boltzmann distribution \(()[- U()]\) corresponding to the _potential_\(U():\) of a \(M\)-particle molecular system, \(^{3M}\) kept at the inverse temperature \(=1/kT\). However, stable numerical integration relies on time steps, \(\), which are strictly smaller than the fastest characteristic time

Figure 1: **Implicit Transfer Operator: A multiple time-scale surrogate of stochastic molecular dynamics.**scales of the molecular system (\(10^{-15}\,\)s, e.g., bond vibrations), yet many molecular systems are characterized by processes on much longer time-scales (\(10^{-3}-10^{-1}\,\)s, e.g. protein-folding, protein-ligand unbinding, regulation). Consequently, we need infeasibly long simulations to characterize many important processes quantitatively due to the slow mixing in \(\).

In this work, we present the implicit Transfer Operator (ITO, Fig. 1) as an effective way to learn multiple time-step surrogate models of the stochastic generating distribution of MD. To our knowledge, this is the first surrogate modeling approach that allows for the simultaneous generation of stochastic dynamics at multiple different time resolutions. By adopting an SE(3)-equivariant generative model, we further demonstrate stable long-time-scale dynamics in increasingly difficult settings where an increasing number of degrees of freedom are marginalized. Our approach can be several orders of magnitude more efficient than direct MD simulations and can be made asymptotically unbiased if the generative model permits exact likelihood evaluation. Our current results do not generalize across different thermodynamic ensembles or across chemical space, but show strong generalization across different time-scales.

Our main contributions are

1. the **Implicit Transfer Operator (ITO)** framework for learning generative models for multiple time resolution molecular dynamics simulations,
2. implementation of ITO using a denoising diffusion probabilistic model (DDPM)  with strong empirical results across resolutions: **SE(3)-equivariant ITO model (SE3-ITO)** gives stable long time-scale simulations and self-consistent dynamics across multiple time-scales for molecular benchmarks and **Coarse-grained SE3-ITO model (CG-SE3-ITO)** trained on large-scale protein folding data sets shows quantitative agreement with major dynamic and stationary observables of interest.

## 2 Background and Preliminaries

NotationThroughout this work, diffusion time, related to Diffusion Models (see Sec. 2), and physical time are represented using superscripts and subscripts, respectively.

Molecular dynamics and observablesMolecular dynamics (MD) is a wide-spread simulation strategy in computational chemistry and physics. In this approach, the time-evolution of \(N\) particles configuration in Euclidean space \(^{3M}\), is modeled via a stochastic differential equation (SDE) with a drift term based on a potential energy model \(U():\). An important aim of MD is to compute:

1. **Stationary observables:**\(O_{f}=_{}[f()]\)
2. **Dynamic observables:**\(O_{f(t)h(t+ t)}=_{_{t}}[_{_{t+ t} p_{r}(_{t+ t}|_{t})}[f(_{t})h(_{t+ t})]]\)

where \(\) is the normalized Gibbs measure, and \(p_{r}(_{t+ t}_{t})\) is a conditional probability density function encoding the time-discrete evolution of the molecular system \(\), with time-step \( t=N\) as prescribed by a dynamic model, e.g. _Langevin dynamics_, integrated with time-step, \(\). \(N\) is typically a large integer. The functions \(f,h:\) are observable functions or '_forward models_' describing the microscopic observation process, e.g. computing a distance or an angle. The observables, \(O_{f(t)}\), and \(O_{f(t)h(t+ t)}\), include binding affinities and binding rates of a drug to a protein, respectively. Conventionally, these observables are estimated from simulation trajectories using naive Monte Carlo estimators.

For illustrative purposes, we assume the temporal behavior of a state, \(\), follows the Brownian dynamics SDE (Ito form)

\[_{t}=- U(_{t})^{-1}\,t+ W, \]

where \(D=^{-1}^{-1}\) is a diffusion constant, with friction \(\) and inverse-temperature \(\), and \(W\) is a Wiener process. Using the Euler-Maruyama time-discretization, with time-step \(\), simulating the SDE corresponds to simulating a Markov chain with the transition probability density

\[p(_{t+}_{t},)=(_{t+}| _{t}- U(_{t})^{-1},_{3M}) \]

where \(\) specifies the multi-variate Normal distribution, and \(_{3M}\) is the \(3M\)-dimensional identity matrix. If \(\) is sufficiently small to allow stable simulation, the _invariant measure_, of the Markov chain (eq. 2), is the Boltzmann distribution (normalized Gibbs measure) corresponding to the potential energy model \(U()\) at \(\). Consequently, by simulating a large number of steps we can draw samples from \(\) to compute stationary observables and compute dynamic observables by simulating \( t=N\) steps enough times with initial states distributed according to \(\). Explicit simulation make such computations extremely costly, and consequently, there's much interest in speeding up the calculations of these quantities.

Transfer OperatorsLet \(\) specify an initial condition, a probability density function on \(\). We can define a Markov operator \(T_{}:L^{1}() L^{1}()\) using a transition density (e.g., 2):

\[[T_{}](_{t+})_{t+})}_{x_{t}}(_{t})(_{t})p( _{t+}_{t})_{t} \]

which then describes the \(\)-weighed evolution of absolutely convergent probability density functions on \(\) according to eq. 1 with time-step, \(\). Such an operator is called the (Ruelle) Transfer Operator 5). We can express the operator using a spectral form

\[T_{}()=_{i=0}^{}_{i}()|_{i} _{i}| \]

where only eigenvalues \(_{i}()=(-_{i})\) depend on the time-step, \(\). \(_{i}\) are characteristic'relaxation' rates associated the left and right eigenfunction pair, \(_{i}\) and \(_{i}\) . We can compute the operator with time-lag \(N\) via the Chapman-Kolmogorov equation (see Sec. A.1 for details)

\[T_{}(N)=_{i=0}^{}_{i}()^{N}|_{i} _{i}|. \]

Equivariant Message Passing Neural NetworksIn this work, we are concerned with MD, where the time-evolution of a molecule is governed by a force field \(()- U()\) derived from a central potential \(U()\). While \(U()\) is _invariant_ to group-actions of the Euclidean group in three dimensions (E(3)), its corresponding force field is E(3)-_equivariant_. We call a function, \(f\)_'invariant'_ under a group-action \(g\) if \(f()=f(S_{g})\) and '_equivariant'_ iff \(T_{g}f()=f(S_{g})\), where \(S_{g}\) and \(T_{g}\) are linear representations of the group element \(g\) .

The force field \(()\) is equivariant under E(3) group-actions. However, in practice, classical molecular dynamics simulations do not change parity during simulation, and consequently, our data distribution only contains a single mirror image of molecules.

We extended the PaiNN architecture , an E(3)-equivariant message passing neural network (MPNN), making it SE(3) equivariant by breaking its symmetry with respect to parity. We introduced this minor modification as we experienced sporadic parity changes when sampling with a model trained using the PaiNN architecture, and introducing this modification resolved the issue. Briefly, PaiNN embeds a graph \(G=(V,E)\), where nodes, \(V\), exchange equivariant messages through edges within a local neighborhood defined as \((i)=\{j\|r_{ij}\| r_{}\}\), where \(r_{ij}\) is the distance between nodes denoted \(i\) and \(j\), and \(r_{}\) is the maximal distance at which nodes are allowed to exchange messages. Messages are pooled and subsequently used to update node features, thereby enabling exchange of equivariant information. We achieve parity symmetry-breaking by constructing the equivariant messages in a manner that depends on cross-products between equivariant node features and direction vectors between interacting nodes. The cross-product is an axial vector (i.e., does not change sign under parity). We combine these vectors with polar vectors (change sign under parity). We refer to this modified PaiNN architecture as ChiroPaiNN (CPaiNN). Further details are in the Appendix D

Diffusion ModelsThe diffusion model (DM) formalism is a powerful generative modeling framework that learns distributions by modeling a gradual denoising process . In DMs, we pre-specify a _forward diffusion process_ (noising process), which gradually transforms the data distribution \(p(^{0})\) to a simple prior distribution \(p(^{T})\), e.g., a standard Gaussian, through a time-inhomogenous Markov process, described by the following SDE (Ito form)

\[^{t}=f(^{t},t)\,t+g(t)\,W. \]where \(0<t<T\) is the _diffusion time_, \(f\) and \(g\) are chosen functions, and \(W\) is a Wiener process. We can generate samples from the data distribution \(p(^{0})\) by sampling from \(p(^{T})\) and solving the _backward diffusion process_ (denoising process)

\[^{t}=[f(^{t},t)-g^{2}(t)_{^ {t}} p(^{t} t)]\,t+g(t)\,W \]

by approximating the _score field_\(_{^{t}} p(^{t} t)\) -- or equivalently a time-dependent Normal transition kernel  -- with a deep neural network surrogate \(_{^{t}}(^{t} t,)\). We can use the learned score field to define a neural ordinary differential equation (ODE) , or probability flow ODE  -- eq.  less the term \(g(t)\,W\) and scaling \(g^{2}(t)\) by \(}{{2}}\) -- which we can leverage for efficient sampling and sample likelihood evaluation. Here, we are concerned with building equivariant probability density functions under SE(3) group actions. Consequently, we parameterize the DM using a learned Normal transition kernel of a time-inhomogeneous diffusion process. By restricting the transition kernels \(p(^{t+1}^{t})\) to be equivariant under SE(3) group-actions, the marginal of \(^{t+1}\) is always invariant . Combining the equivariant transition kernel with an invariant prior density  ensures the whole Markov process is invariant to SE(3) group actions. Consequently, combining an isotropic mean-free Gaussian as prior with ChiroPaiNN-parameterized transition kernels, we can construct an SE(3) equivariant diffusion model.

## 3 Implicit Transfer Operator

Molecular simulations are Markovian with transition density (e.g. eq. 2 and Normal, however, the latter only for very small _physical_ time-steps \(\). Here, we aim to approximate the long-time step transition probability \(p_{N}(_{N}_{0})\) to allow for one-step sampling of long-time-scale dynamics.

As data we consider simulation trajectories. The trajectories are generated by explicit simulation which corresponds to sampling ancestrally from the small time-step transition density: \(=\{_{},,_{N}\} p(_{n }_{(n-1)})\), with \(n=\{1,,N\}\). In general, the state variable \(\), contains both position and velocity information of the particles, along with other details such as box dimensions, depending on the simulation scheme and target ensemble. Throughout this study, we only consider the position information.

We build a surrogate of the conditional transition probability distribution -- from MD data. In practice, we learn a generative model \(_{t+N} p_{}(_{t+N} _{t},N)\) with a conditional denoising diffusion probabilistic model (cDDPM) of the form

\[p(_{t+N}^{0}_{t},N) p(_{t+ N}^{0:T}_{t},N)\,^{1:T} \]

where \(^{1:T}\) are _latent variables_ of the same dimension as our output, and follow a joint density describing the backward diffusion process (eq. ) and \(^{T}(0,)\). We define a conditional sample likelihood as

\[(;)_{i}p_{ }(_{t_{i}+N_{i}}^{0}_{t_{i}},N_{i}) \]

where \(\) is a list of generated indices \(i\) specifying a time \(t_{i}\) and a time-lag (\(\)) integer multiple \(N_{i}\), associating two time-points in the trajectory, \(\). Following Ho et al., we train the cDDPM by

Figure 2: **ITO \(}\) networks** (A) SE3-ITO used for molecular application (B) MB-ITO, used for experiments with the Müller-Brown potential. \(_{}\) and \(_{}\) are positional and nominal embedding respectively, Concat is a concatenation, and MLP is a multi-layer perceptron. Arrows are annotated with input and output shapes.

optimizing a simplified form of the variational bound of the log-likelihood ,

\[()=_{i,e(0,t),t_{}(0,T)}[\|-_{}(}_{t_{i}+N_{i}}^{t_{}},_{t_{i}},N_{i},t_{ })\|_{2}], \]

where \(}_{t}^{t_{}}}^{t_{}}}_{t}+}^{t_{}}}\), with \(}^{t_{}}=_{i}^{t_{}}(1-_ {i})\) and \(_{i}\) is the variance of the forward diffusion process at diffusion time, \(i\). \(_{}()\) is one of the two ITO neural network model architectures shown in Fig. 1 and is directly related to the score .

If the data used to train the conditional transition density is generated by MD simulation with time-invariant potential energy (drift), we can express the generating transition probability as a decomposition of time-variant and -invariant parts (Proof, see Sec. A.2)

\[p(x_{N} x_{0})=_{i=1}^{}^{N}()}_ {}(x_{N})_{i}(x_{0})}_{} \]

where \(_{i}\) and \(_{i}\) are _time-invariant_ projection coefficients of the state variables on-to the left and right eigenfunctions \(_{i}\) and \(_{i}\), of the _Transfer operator_\(T_{}()\) and \(|_{i}()| 1\) is its \(i\)'th eigenvalue. Consequently, we call our surrogate modeling approach _implicit transfer operator_ learning.

As outlined in Algorithm 1 we generate the indices \(i\), e.g. the tuples \((x_{t_{i}},x_{t_{i}+N_{i}},N_{i})\), in a manner such that the model is exposed to multiple time-lags, sampled uniformly across orders of magnitude, used for gradient-based optimization with Adam . As a result, as illustrated in eq. 1 the model will be exposed to multiple different linear combinations of the eigenfunctions of \(T_{}()\) in each batch during training. We conjecture that this data augmentation procedure will enable better learning of implicit representations of these eigenfunctions and, consequently, better generalization across time scales and yield more stable sampling.

### ITO Architectures

We present two architectures for learning cDDPMs encoding ITO models, one for molecular applications SE3-ITO and one for the Muller-Brown benchmark system (Fig. 1). The SE3-ITO architecture uses our new SE(3) equivariant MPNN (ChiroPaiNN, described in sec. E) to encode \(_{t}\), \(N\), and atom-types, \(z\), to invariant features, \(s\), and equivariant features, \(v\). We concatenate \(s\) with an encoding of the diffusion-time \(t_{}\) and process them through a MLP (multi-layer perceptron). The output from the MLP are passed along with \(v\) and \(}_{t}^{}\) as input to a second ChiroPaiNN module which predicts \(\). More details on the architecture and hyperparameters are available in Appendices D and E.

```
Input:\(n\) MD-trajectories; \(=\{_{0}^{j},,_{t_{j}}^{j}\}_{j=0}^{n}\), ITO score-model; \(_{}\), max lag; \(N_{}\) \(^{}=(\{_{0}^{j},, _{t_{j}-N_{}}^{j}\}_{j=0}^{n})\) while not converged do \(_{t}(^{})\) \(N(N_{})\) \(t_{}(0,T)\)  Take gradient step on: \(_{}[\|-_{}(}_{t+N_{}}^{t_{}},_{t},N,t_{})\| _{2}]\) endwhile return\(_{}\)
```

**Algorithm 1** Training. DisExp is defined in Appendix E

## 4 Long time-step stochastic dynamics with Implicit Transfer Operators

### Datasets and test-systems

To evaluate how robustly ITO models can model long time-scale dynamics, we conducted three classes of experiments, ranging from fully observed, high time-resolution, to sparsely observed and low time resolution. Details on training and computational resources are available in Appendices E and E respectively.

Muller-Brown is a 2D potential commonly used for benchmarking molecular dynamics sampling methods. We generate a training data-set by integrating eq. 1 with the Muller-Brown potential energy as \(U()\) (For details, see Appendix B.1). This dataset corresponds to a fully observed case.

Alanine dipeptideWe use publicly available data from MDshare . Simulation is performed with \(2\,\) integration time-steps and data is saved at \(1\,\) intervals. The simulations are performed in explicit solvation, but we only model the 22 atoms of the solute, without considering velocities. Consequently, this dataset is only partially observed.

Fast-folding proteinsWe use molecular dynamics data previously reported by Lindorff-Larsen et al. on the fast-folding proteins Chignolin, Trp-Cage, BBA, and Villin . The data is proprietary but available upon request for research purposes. The simulations were performed in explicit solvent with a \(2.5\,\) time-step and the positions was saved at \(200\,\) intervals. We coarse-grain the simulation by representing each amino-acid by the Euclidean coordinate of their \(C\) atom as done previously , leading to 10, 20, 28, and 35 particles in each system respectively. Consequently, these data correspond to a mostly unobserved case.

### Stochastic lag improves meta-stability prediction

In sec. 2 we conjecture that exposing an ITO model to multiple lag times during training leads to better and more robust models. To test this, we trained a set of models on the Muller-Brown dataset with fixed constant lags \(N=\{10,100,1000\}\) (fixed lag) and a single model with \(N(1000)\) (stochastic lag) using the MB-ITO model (Fig. 2).

We find that the model trained with a stochastic lag systematically outperforms models trained with fixed lag (Table 1). We gauge the agreement by comparing Variational Approach to Markov Processes (VAMP)-2 scores  (for details, see Appendix E), between model samples and training data and find that both models tend to underestimate meta-stability compared to training data slightly. However, the model trained with stochastic lag is marginally closer to the reference values. We note that the difference in the ability of fixed and stochastic lag ITO models to capture long-time-scale dynamics is also reflected in the learned transition densities (Fig. 3). Together, these results suggest that lag-time augmentation during training leads to better implicit learning of the Transfer operator's eigenfunctions than training with a fixed lag.

To test whether this phenomena extends in cases where we do not have full observability and to molecular systems we followed the VAMP2-gaps of alanine di-peptide as a function of epoch for models trained with a fixed lag and a stochastic lag (Fig. 3). We find that the VAMP-2 gaps for stochastic lag and fixed lag in this case are statistically indistinguishable across all epochs. These results suggest that we can without compromising on accuracy build multiple time-scale surrogates by training with stochastic lag-times.

### Efficient and accurate self-consistent long time-scale dynamics

We evaluate the ITO models trained with stochastic lags to capture long time-scale dynamics in a self-consistent manner, in the Chapman-Kolmogorov sense, i.e., \(p(_{ t}_{0}) p(_{N} _{0})=_{i=1}^{N}p(_{i}_{(i-1)})\), or if samples generated by direct sampling with time-step \( t=N\) are

   system  lag & 10 & 100 \\  Müller-Brown (fixed) & -0.0351 (5) & -0.1189 (2) \\ Müller-Brown (stochastic) & **-0.0312** (4) & **-0.0970** (5) \\   

Table 1: **VAMP2 score-gaps.** Difference in VAMP2-scores of ancestral sampling from ITO models with fixed lag and stochastic lags, compared to baseline Langevin simulations. Perfect match is 0, negative and negative values correspond to under and over estimation of meta-stability, respectively. Standard deviations on last decimal place are given in parentheses.

distributed similarly to samples generated by performing ancestral sampling \(N\) times, each with time-step \(\). In direct sampling, we draw samples for a desired time-step \( t=N\) from \(_{}(_{0},N)\) and in ancestral sampling we draw \(n\) samples with time-step \( t=N\) from \(_{}(_{0},N)\) in an ancestral manner, e.g. \(x_{i+1}_{}(_{i},N)\), where \(i=\{0, n-1\}\).

For the fully-observed Muller-Brown case, we find that the ITO model is self-consistent by the strong overlap in transition densities sampled in a direct and ancestral manner (Algorithm 2). These results generalize to molecular systems and partially observed systems. Sampling an SE3-ITO model (Fig. 2) trained with alanine dipeptide data, we find strong agreement between the ancestrally and directly sampled transition densities (Fig. 3) and we again have a strong consistency with corresponding transition densities computed from molecular dynamics simulations. Note here, that the time-step of the ITO-sampled transition densities varies from \(10^{4}\) to \(10^{6}\) times the MD integration time-step. The transition densities for alanine di-peptide (Fig. 3) are calculated using 15'000 trajectories. For direct sampling, this means that we draw 15'000 samples in total. In the case of ancestral sampling, we sample 4, 64, or 512 steps for 15'000 trajectories, to match \( t\).

Next, we consider four fast-folding proteins  where we coarse-grain the proteins by representing them only with their \(C\) atoms. In this sparsely observed case (CG-SE3-ITO), we find strong model self-consistency, as shown by the comparison between conditional densities from the folded and unfolded states (Fig. 3) projected onto a linear subspace determined using _time-lagged independent component analysis_ (time-lagged independent components, tIC)  (see Appendix B.3). Further, the long time-scale transition density gradually converges to the data distribution as expected.

Finally, by ancestral sampling (Algorithm 2), we perform a simulation of Chignolin with the same length as the training trajectory (\(106\, s\)), using a CG-SE3-ITO model, and compare with MD. The CG-SE3-ITO simulation is 2120 steps with \( t=5\,\). Running in parallel, on a single Titan X GPU we can simulate the CG-SE3-ITO model at a rate of \(363\,/(_{w}M^{2})\) where \(_{w}\) denotes seconds wall-time (Appendix C.1). Remarkably, these trajectories are virtually indistinguishable in the slowly relaxing TICA coordinates, illustrating stability of ITO. These conclusions extend to the proteins Villin, BBA, and Trp-Cage (See Appendix, Figs. 10 and 10)

Together these results suggest that ITO models accurately and self-consistently capture the slow dynamics of molecular systems and are robust to situations where the system is only partially observed. In general, we expect robustness to sparsely observed representations as long as the input representations are sufficient to span the eigenfunctions of \(T_{}\). Approximation errors will translate into systematic under-estimation of relaxation time-scales , consistent with our slight under-estimation of VAMP-2 scores (Table 1). In future work, combining the learning of SE3-ITO models with a systematic scheme for coarse-graining , could be an avenue for scaling to large-scale molecular systems at a low computational cost.

Figure 3: **Transition probability densities of alanine dipeptide dynamics with SE3-ITO model;** Rows of increasing time-lag (from top to bottom). Contours are samples from SE3-ITO model, and 2D histograms show estimates from MD data. The first column shows conditional transition densities projected onto the torsion angles \(\) and \(\) (inset). The black cross indicates the initial condition. The second and third columns show marginal distributions of \(\) and \(\), respectively, with direct sampling in orange, ancestral sampling in blue, and MD data in black.

## 5 Prediction of dynamic and stationary observables of using CG-SE3-ITO

As outlined in section 2 an important aim of MD simulations is to compute stationary and dynamic observables, which involves intractable integrals typically approximated via Monte Carlo estimators. Using the trained ITO models we can efficiently sample i.i.d. from the transition density needed for computing dynamic observables, and by choosing a time-step which is sufficiently large we can also sample i.i.d from the Boltzmann distribution \(\), the latter akin to _Boltzmann generators_ (See Appendix A.1). We note that, the ITO models are surrogates and as such without reweighing we cannot expect unbiased samples from the Boltzmann and dynamic transition densities. Nevertheless, we gauge how accurately ITO models we can compute these observables of interest in the context of protein folding without reweighing:

* **Free Energy of Folding**, \( G=-[}{1-p_{f}}]\)
* **Mean first passage time, folding**, \(_{f}=_{x_{0}-f}_{0}^{}(x_{t} f)p( x_{t} x_{0},t)t\,t\,x_{0}\)
* **Mean first passage time, unfolding**,\(_{u}=_{x_{0} f}_{0}^{}(x_{t}-f)p( x_{t} x_{0},t)t\,t\,x_{0}\)

where \(\{f, f\}\) are disjoint subsets corresponding to the folded and unfolded states of a protein, \(p_{f}=_{x f}(x)\,x\), is the folded state probability and \(()\) is the Dirac delta.

We compute these observables using the reference molecular simulation data  and sample statistics from the CG-SE3-ITO models of each of the four fast-folding proteins (details in Appendix B.6). Strikingly, the observables computed using CG-SE3-ITO models agree well with those computed from long all-atom MD simulations (Table 2).

Finally, we analyzed the robustness, convergence, and consistency of these observables (Fig. 1). For Chignolin, we trained five models independently and analyzed model checkpoints when the training loss had stabilized. For each checkpoint and each model, we computed the observables. The values predicted are statistically indistinguishable, suggesting consistency, robustness, and convergence. The

Figure 4: **Reversible protein folding-unfolding of Chignolin with CG-SE3-ITO** Conditional probability densities (orange contours) starting from unfolded (upper panels) and folded (lower panels) protein states, at increasing time-lag (left to right), shown on top of data distribution. Below: time-traces of 106 microsecond MD simulations and ITO simulations on tICs 1 and 2. The two dashed lines correspond to the folded state value in tIC 1 (lower line) and tIC 2 (higher line). Contour lines are based on 10’000 trajectories, generated with ancestral sampling with the length, \( t\) and time-step \(200\,\). For \( t=200\,\) this corresponds to 1000 ancestral samples.

average predictions closely match the reference values. Nevertheless, we note that the fluctuations in these values are noticeable.

We implemented all experiments using PyTorch , PyTorch Lightning , JAX , and used DPM-Solver  for probability flow ODE Sampling.

## 6 Related Work

Molecular samplingSampling molecular configurations is a broad field and can broadly be divided into two main areas: physically motivated sampling of the Boltzmann distribution and conformer generation. The first area includes algorithmic approaches to sample the Boltzmann distribution including Molecular Dynamics simulations , Markov Chain Monte Carlo, extended ensemble methods , including analysis methods involving deep generative nets , and surrogate models which directly approximate the Boltzmann distribution and allow for recovery of unbiased statistics, including Boltzmann generators . Conformer generation concerns generating physically plausible conformers without explicitly trying to follow the Boltzmann distribution. The latter approaches can be split into ML  and chemoinformatic  approaches. Finally, speeding up molecular simulations by reducing the effective number of particles to simulate through coarse-graining with special purpose forcefield models  including machine learned variants  and learned coarse-graining maps  is an orthogonal approach to sample conformation space. Further, several methods to recover all-atom models from coarse-grained representations through ML  and rule-based approaches  are available.

Transfer Operator surrogatesBuilding transfer operator surrogates is commonly used in molecular modeling including (Deep Generative) Markov state models (MSM) , also including experimental data  dynamic graphical models,  VAMPnets , observable operator models, however, primarily for analysis of molecular dynamics data. Markov state models are time-space discrete approximations of the transfer operator and Deep Generative MSM  and VAMPnets  are deep learning infused versions, where state discretization is learned by deep nets. Dynamic graphical models reparameterize MSMs as kinetic Markov random fields allowing for scaling to larger systems . Klein et al. recently introduced _timewarp_ which is a flow-based generative model to simulate molecular systems with a large (up to \(0.5\,\)), fixed, time-lag,  providing asymptotically unbiased equilibrium samples through a Metropolis-Hastings correction . While timewarp generates conformers with realistic local structure, it has limitations in capturing long time-scale dynamics, which is reflected in the predicted transition probability densities. In contrast, our approach captures long time-scale dynamics efficiently allowing for accurate prediction of dynamic observables. However, currently, neither the code nor the data from timewarp is publicly available percluding direct comparisons on the benchmark tasks established in their paper.

## 7 Limitations

Surrogate modelImplicit Transfer Operators are surrogate models of stochastic dynamics' conditional transition probability densities. We cannot guarantee unbiased sampling of dynamics and the stationary distribution due to aleatoric (e.g., finite data) and epistemic (e.g., model misspecification) uncertainty. We can overcome the latter by reweighing against a Markov Chain Monte Carlo acceptance criterion as proposed previously , to ensure unbiased dynamics path-reweighing is necessary, which in turn requires closed-form expressions for the target path probabilities .

    & \( G_{}/kT\) (\(\)) & \(_{f}/ s\) (\(\)) & \(_{u}/ s\) (\(\)) \\  Chignolin (MD/ITO) & -1.28(1)/-0.64(33) & 0.565(4)/1.02(24) & 2.01(2)/2.12(34) \\ Trp-Cage (MD/ITO) & 1.47(6)/2.84(6) & 13.6(4)/37(2) & 3.4(2)/2.85(9) \\ BBA (MD/ITO) & 0.97(3)/1.52(3) & 11.7(2)/8.6(2) & 5.1(1)/1.75(4) \\ Villin (MD/ITO) & 1.21(2)/2.22(3) & 2.41(3)/3.27(7) & 0.68(1)/0.354(5) \\   

Table 2: **Molecular observables** Standard deviations on last decimal place are given in parentheses. Stationary and dynamic observables are denoted \(\) and \(\), respectively.

Transferability and scalabilityCurrently, ITO does not generalize across chemical space and thermodynamic variables. In future work, we anticipate that generalization across chemical space limitations can be overcome by appropriate data set curation and parameter-sharing schemes . Generalization across thermodynamic variables such as temperature and pressure would require using a surrogate model which is steerable under these changes, e.g., temperature steerable flows  or thermodynamic maps . Currently, we assume a fully connected graph that scales \((M^{2})\) in system size, which limits what systems are practically accessible. Devising new surrogate models which use mean-field approximation approaches from e.g., computational physics  or chemistry to truncate the graphs and treat long-range as an additive term  could yield more favorable scaling .

## 8 Conclusions

This paper introduces Implicit Transfer Operators (ITO), an approach to building multiple time-scale surrogate models of stochastic molecular dynamics. We implement ITO models with a conditional DDPM using a new time-augmentation scheme and show how ITO models capture fast and slow dynamics on benchmarks and molecular systems. We show ITO models are self-consistent over multiple time scales and highly robust to the marginalization of degrees of freedom in the system, which are unimportant to capture the long-time-scale dynamics. Combined with a SE(3) variant of the PaiNN architecture  (ChiroPaiNN), we further show strong empirical evidence of scaling to molecular applications, such as the folding of coarse-grained proteins. As such, we are confident that ITO is a stepping-stone toward general-purpose surrogates of molecular dynamics.