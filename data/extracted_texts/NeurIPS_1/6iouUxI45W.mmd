# The Exact Sample Complexity Gain from Invariances

for Kernel Regression

 Behrooz Tahmasebi

MIT CSAIL

bzt@mit.edu &Stefanie Jegelka

MIT CSAIL and TU Munich

stefje@mit.edu

###### Abstract

In practice, encoding invariances into models improves sample complexity. In this work, we study this phenomenon from a theoretical perspective. In particular, we provide minimax optimal rates for kernel ridge regression on compact manifolds, with a target function that is invariant to a group action on the manifold. Our results hold for any smooth compact Lie group action, even groups of positive dimension. For a finite group, the gain effectively multiplies the number of samples by the group size. For groups of positive dimension, the gain is observed by a reduction in the manifold's dimension, in addition to a factor proportional to the volume of the quotient space. Our proof takes the viewpoint of differential geometry, in contrast to the more common strategy of using invariant polynomials. This new geometric viewpoint on learning with invariances may be of independent interest.

## 1 Introduction

In a broad range of applications, including machine learning for physics, molecular biology, point clouds, and social networks, the underlying learning problems are invariant with respect to a group action. The invariances are observed widely in practice, for instance, in the study of high energy particle physics , galaxies , and also molecular datasets  (see  for a survey). In learning with invariances, one aims to develop powerful architectures that exploit the problem's invariance structure as much as possible. An essential question is thus: what are the fundamental benefits of model invariance, e.g., in terms of sample complexity?

Several architectures for learning with invariances have been proposed for various types of data and invariances, including DeepSet  for sets, Convolutional Neural Networks (CNNs) , PointNet  for point clouds with permutation invariance, tensor field neural networks  for point clouds with rotations, translations, and permutations symmetries, Graph Neural Networks (GNNs) , and SignNet and BasisNet  for spectral data. Other works study invariance with respect to the orthogonal group , and invariant and equivariant GNNs . These architectures are to exploit the invariance of data as much as possible, and are invariant/equivariant by design.

In fixed dimensions, one common feature of many invariant models, including those discussed above, is that the data lie on a compact manifold (not necessarily a sphere, e.g., the Stiefel manifold for spectral data), and are invariant with respect to a group action on that manifold. Thus, characterizing the theoretical gain of invariances corresponds to studying the gain of learning under group actions on manifolds. Adopting this view, in this paper, we answer the question: _how much gain in sample complexity is achievable by encoding invariances?_ As this problem is algorithm and model dependent, it is hard to address in general. A focused version of the problem, but still interesting, is to study this sample complexity gain in kernel-based algorithms, which is what we address here. As neural networks in certain regimes behave like kernels (for example, the Neural Tangent Kernel (NTK) ), the results on kernels should be understood as relevant to a range of models.

Formally, we consider the Kernel Ridge Regression (KRR) problem with i.i.d. data on a compact manifold \(\). The target function lies in a Reproducing Kernel Hilbert space (RKHS) of Sobolev functions \(^{s}()\), \(s 0\). In addition, the target function is invariant to the action of an arbitrary Lie group \(G\) on the manifold. We aim to quantify: _by varying the group \(G\), how does the sample complexity change, and what is the precise gain as \(G\) grows?_

**Main results.** Our main results characterize minimax optimal rates for the convergence of the (excess) population risk (generalization error) of KRR with invariances. More precisely, for the Sobolev kernel, the most commonly studied case of kernel regression, we prove that a (excess) population risk (generalization error) \(((/G)}{n})^{s/ (s+d/2)}\) is both achievable and minimax optimal, where \(^{2}\) is the variance of the observation noise, \((/G)\) is the volume1 of the corresponding quotient space, and \(d\) is the effective dimension of the _quotient space_ (see Section 4 for a precise definition). This result shows a reduction in sample complexity in _two_ intuitive ways: (1) scaling the effective number of samples, and (2) reducing dimension and hence exponent. First, for finite groups, the factor \((/G)\) reduces to \(()/|G|\), and can hence be interpreted as scaling the _effective_ number of samples by the size of the group. That is, each data point conveys the information of \(|G|\) data points due to the invariance. Second, and importantly, the parameter \(d\) in the exponent can generally be much smaller than \(()\), which would be the correspondent of \(d\) in the non-invariant case. In the best case, \(d=()-(G)\), where \((G)\) is the dimension of the Lie group \(G\). Hence, the second gain shows a gain in the dimensionality of the space, and hence in the exponent.

Our results generalize and greatly expand previous results by Bietti et al. , which only apply to _finite_ groups and _isometric_ actions and are valid only on spheres. In contrast, we derive optimal rates for all compact manifolds and smooth compact Lie group actions (not necessarily isometric), including groups of positive dimension. In particular, the reduction in dimension applies to infinite groups, since for finite groups \((G)=0\). Hence, our results reveal a new perspective on the reduction in sample complexity that was not possible with previous assumptions. Our rates are consistent with the classical results for learning in Sobolev spaces on manifolds without invariances . To illustrate our general results, in Section 5, we make them explicit for kernel counterparts of popular invariant models, such as DeepSets, GNNs, PointNet, and SignNet.

Even though our theoretical results look intuitively reasonable, the proof is challenging. We study the space of invariant functions as a function space on the quotient space \(/G\). To bound its complexity, we develop a dimension counting theorem for functions on the quotient space, which is at the heart of our analysis and of independent interest. The difficulty is that \(/G\) is not always a manifold. Moreover, it may exhibit non-trivial boundaries that require boundary conditions to study function spaces. Different boundary conditions can lead to very different function spaces, and a priori the appropriate choice for the invariant functions is unclear. We prove that smooth invariant functions on \(\) satisfy the Neumann boundary condition on the (potential) boundaries of the quotient space, thus characterizing exactly the space of invariant functions.

The ideas behind the proof are of potential independent interest: we provide a differential geometric viewpoint of the class of functions defined on manifolds and study group actions on manifolds from this perspective. This stands in contrast to the classical strategy of using polynomials generating the class of functions , which is restricted to spheres. To the best of our knowledge, the tools used in this paper are new to the literature on learning with invariances.

In short, in this paper we make the following contributions:

* We characterize the exact sample complexity gain from invariances for kernel regression on compact manifolds for an arbitrary Lie group action. Our results reveal two ways to reduce sample complexity, including a new reduction in dimensionality that was not obtainable with assumptions in prior work.
* Our proof analyzes invariant functions as a function space on the quotient space; this differential geometric perspective and our new dimension counting theorem, which is at the heart of our analysis, may be of independent interest.

Related Work

The perhaps closest related work to this paper is , which considers the same setup for finite isometric actions on spheres. We generalize their result in several aspects: the group actions are not necessarily finite or isometric, and the compact manifold is arbitrary (including compact submanifolds of \(^{d}\)), allowing to observe a new axis of complexity gain. Mei et al.  consider invariances for random features and kernels, but in a different scaling/regime; thus, theirs are not comparable to our results. For density estimation on manifolds, optimal rates are given in , which are consistent with our theory. McRae et al.  show non-asymptotic sample complexity bounds for regression on manifolds. A similar technique was recently applied in , but for a very different setting of covariate shifts.

The generalization benefits for invariant classifiers are observed in the most basic setup in , and for linear invariant/equivariant networks in [20; 19]. Some works propose covering ideas to measure the generalization benefit of invariant models , while others use properties of the quotient space [57; 50]. It is known that structured data exhibit certain gains for localized classifiers . Sample complexity gains are also observed for CNN on images [17; 35]. Wang et al.  incorporate more symmetry in CNNs to improve generalization.

Many works introduce models for learning with invariances for various data types; in addition to those mentioned in the introduction, there are, e.g., group invariant scattering models [41; 9]. A probabilistic viewpoint of invariant functions  and a functional perspective  also exist in the literature. The connection between group invariances and data augmentation is addressed in [13; 39].

Universal expressiveness has been studied for settings like rotation equivariant point clouds , sets with symmetric elements , permutation invariant/equivariant functions , invariant neural networks [55; 69; 43], and graph neural networks [68; 49]. Lawrence et al.  study the implicit bias of linear equivariant networks. For surveys on invariant/equivariant neural networks, see [22; 8].

## 3 Preliminaries and Problem Statement

Consider a smooth connected compact boundaryless2\(()\)-dimensional (Riemannian) manifold \((,g)\), where \(g\) is the Riemannian metric. Let \(G\) denote an arbitrary compact Lie group of dimension \((G)\) (i.e., a group with a smooth manifold structure), and assume that \(G\) acts smoothly on the manifold \((,g)\); this means that each \( G\) corresponds to a diffeomorphism \(:\), i.e., a smooth bijection. Without loss of generality, we can assume that \(G\) acts _isometrically_ on \((,g)\), i.e., \(G\) is a Lie subgroup of the isometry group \((,g)\). To see why this is not restrictive, given a base metric \(g\), consider a new metric \(=_{G}(^{*}g)\), where \(_{G}\) is the left-invariant Haar (uniform) measure of \(G\), and \(^{*}g\) is the pullback of the metric \(g\) by \(\). Under the new metric, \(G\) acts isometrically on \((,)\). We review basic facts about manifolds and their isometry groups in Appendix A.1 and Appendix A.2.

We are given a dataset \(=\{(x_{i},y_{i}):i=1,2,,n\}( )^{n}\) of \(n\) labeled samples, where \(x_{i}_{}\), for the uniform (Borel) probability measure \(d(x):=()}d_{g}(x)\). Here, \(d_{g}(x)\) denotes the volume element of the manifold defined using the Riemannian metric \(g\). We assume the uniform sampling for simplicity; our results hold for non-uniform cases, too. The hypothesis class is a set \( L^{2}_{}(,G) L^{2}( )\) including only \(G\)-invariant square-integrable functions on the manifold, i.e., those \(f L^{2}()\) satisfying \(f((x))=f(x)\) for all \( G\). We assume that there exists a function \(f^{*}\) such that \(y_{i}=f^{*}(x_{i})+_{i}\) for each \((x_{i},y_{i})\), where \(_{i}\)'s are conditionally zero-mean random variables with variance \(^{2}\), i.e., \([_{i}|x_{i}]=0\) and \([_{i}^{2}|x_{i}]^{2}\).

Let \(K:\) denote a continuous positive-definite symmetric (PDS) kernel on the manifold \(\), and let \( L^{2}()\) denote its Reproducing Kernel Hilbert Space (RKHS). The kernel \(K\) is called\(G\)-invariant3 if and only if for all \(x,y\),

\[K(x,y)=K((x),^{}(y)), \]

for any \(,^{} G\). In other words, \(K(x,y)=K([x],[y])\), where \([x]:=\{(x): G\}\) is the orbit of the group action that includes \(x\).

The Kernel Ridge Regression (KRR) problem on the data \(\) with a \(G\)-invariant kernel \(K\) asks for the function \(\) that minimizes

\[:=*{arg\,min}_{f}\ }(f ):=_{i=1}^{n}(y_{i}-f(x_{i}))^{2}+\|f\|_{}^{2} }. \]

By the representer theorem , the optimal solution \(\) is of the form \(=_{i=1}^{n}a_{i}K(x_{i},.)\) for a weight vector \(^{n}\). The objective function \(}()\) can thus be written as

\[}()=\|-\|_{2}^ {2}+^{T}, \]

where \(=(y_{1},y_{2},,y_{n})^{n}\) and \(=\{K(x_{i},x_{j})\}_{i,j=1}^{n}\) is the Gram matrix. This gives the closed form solution \(=(+n I)^{-1}\). Using the population risk \((f):=_{x}[(y-f(x))^{2}]\), the _effective ridge regression estimator_ is defined as

\[_{}:=*{arg\,min}_{f}\  (f)+\|f\|_{}^{2}}. \]

This paper focuses on the RKHS of Sobolev functions, \(=^{s}_{}()=^{s}( ) L^{2}_{}(,G)\), \(s 0\). This includes all functions having square-integrable derivatives up to order \(s\). Note that \(^{s}()\) includes only continuous functions when \(s>()/2\). Moreover, it contains only continuously differentiable functions up to order \(k\) when \(s>()/2+k\) (Appendix A.10). Note that \(^{s}()\) is an RKHS if and only if \(s>()/2\).

## 4 Main Results

Our first theorem provides an upper bound on the excess population risk, or the generalization error, of KRR with invariances.

**Theorem 4.1** (Convergence rate of KRR with invariances).: _Consider KRR with invariances with respect to a Lie group \(G\) on the Sobolev space \(^{s}_{}()\), \(s>d/2\), with \(d=(/G)\). Assume that \(f^{}^{}_{}()\) for some \((0,1]\), and let \(s=(+1)\) for a positive \(\). Then,_

\[()-(f^{})\ 32 }{(2)^{d}}\ (/G)}{n}^{ s/( s+d/2)}\|f^{ }\|_{^{}_{}()}^{d/( s +d/2)}, \]

_with the optimal regularization parameter_

\[=\|_{^{s }_{}()}^{2}}}{(2)^{d}}(/G)}{n}^{ s/( s+d/ 2)}, \]

_where \(_{d}\) is the volume of the unit ball in \(^{d}\)._

Theorem 4.1 allows to estimate the gain in sample complexity from making the hypothesis class invariant. Setting \(G=\{_{G}\}\) (i.e., the trivial group) recovers the standard generalization bound without group invariances. In particular, without invariances, the dimension \(d\) becomes \(()\), and the volume \((/G)\) becomes \(()\). Hence, group invariance can lead to a two-fold gain:

* **Exponent**: the dimension \(d\) in the exponent can be much smaller than the corresponding \(()\)* **Effective number of samples**: the number of samples is multiplied by \[)}/(2)^{()}}{_{d}/(2)^{d} }()}{(/G)}.\] (7) The quantity (7) reduces to \(|G|\) if \(G\) is a finite group that efficiently acts on \(\) (i.e., if any group element acts non-trivially on the manifold). Intuitively, any sample conveys the information of \(|G|\) data points, which can be interpreted as having effectively \(n|G|\) samples (compared to non-invariant KRR with \(n\) samples). For groups of positive dimension, it measures how the group is contracting the volume of the manifold. Note that for finite groups, one always has \(()}{(/G)} 1\).

**Dimension and volume for quotient spaces**. In Theorem 4.1, the quotient space \(/G\) is defined as the set of all orbits \([x]:=\{(x): G\}\), \(x\), but \(/G\) is not always a (boundaryless) manifold (Appendix A.5 and A.6). Thus, it is not immediately possible to define its dimension and volume. The quotient space is a finite disjoint union of manifolds, each with its specific dimension/volume. In Appendix A.5 and A.6, we review the theory of quotients of manifolds, and observe that there exists an open dense subset \(_{0}\) such that \(_{0}/G\) is open and dense in \(/G\), and more importantly, it is a connected precompact manifold. \(_{0}/G\) is called the _principal_ part of the quotient space. It has the largest dimension among all the manifolds that make up the quotient space.

The projection map \(:_{0}_{0}/G\) induces a metric on \(_{0}/G\) and this allows us to define \((/G):=(_{0}/G)\). Note that \((/G)\) depends on the Riemannian metric, which itself might depend on the group \(G\) if we start from a base metric and then deform it to make the action isometric. The volume \((_{0}/G)\) is computed with respect to the dimension of \(_{0}/G\), thus being nonzero even if \((_{0}/G)<()\).

The effective dimension of the quotient space is defined as \(d:=(_{0}/G)\). Alternatively, one can define the effective dimension as

\[d:=()-(G)+_{x}(G_{x}), \]

where \(G_{x}:=\{ G:(x)=x\}\) is called the isotropic group of the action at point \(x\). For example, if there exists a point \(x\) with the trivial isotropy group \(G_{x}=\{_{G}\}\), then \(d=()-(G)\).

_Remark 4.2_.: The invariant Sobolev space satisfies \(^{s}_{}()^{s }_{}() L^{2}_{ }()\). If the regression function \(f^{}\) does not belong to the Sobolev space \(^{s}_{}()\) (i.e., \((0,1)\)), the achieved exponent only depends on \( s\) (i.e., the smoothness of the regression function \(f^{}\) and not the smoothness of the kernel). The bound decreases monotonically as \(s\) increases: smoother functions are easier to learn.

The next theorem states our minimax optimality result. For simplicity, we assume \(=1\).

**Theorem 4.3** (Minimax optimality).: _For any estimator \(\),_

\[_{f^{}^{s}_{}( )\\ \|f^{}\|_{^{s}_{}()}=1} ()-(f^{}) C_{ }}{(2)^{d}}( /G)}{n}^{s/(s+d/2)}, \]

_where \(C_{}\) is a constant only depending on \(\), and \(_{d}\) is the volume of the unit ball in \(^{d}\)._

An explicit formula for \(C_{}\) is given in the appendix. Note that the above minimax lower bound not only proves that the achieved bound by the KRR estimator is optimal, but also shows the optimality of the prefactor characterized in Theorem 4.1 with respect to the effective dimension \(d\) (up to multiplicative constants depending on \(\)).

### Proof Idea and Dimension Counting Bounds

To prove Theorem 4.1, we develop a general formula for the Fourier series of invariant functions on a manifold. In particular, we argue that a smooth \(G\)-invariant function \(f:\) corresponds to a smooth function \(:/G\) on the quotient space \(/G\), where \(([x])=f(x)\) for all \(x\). Hence, we view the space of invariant functions as smooth functions on the quotient space.

The generalization bound essentially depends on a notion of dimension or complexity for this space, which allows bounding the bias and variance terms. We obtain this by controlling the eigenvalues of the Laplace-Beltrami operator, which is specifically suitable for Sobolev kernels.

The _Laplace-Beltrami operator_\(_{g}\) is the generalization of the usual definition of the Laplacian operator on the Euclidean space \(^{d}\) to any smooth manifold. It can be diagonalized in \(L^{2}()\). In particular, there exists an orthonormal basis \(\{_{}(x)\}_{=0}^{}\) of \(L^{2}()\) starting from the constant function \(_{0} 1\) such that \(_{g}_{}+_{}_{}=0\), for the discrete spectrum \(0=_{0}<_{1}_{2}\). Let us call \(\{_{}(x)\}_{=0}^{}\) the Laplace-Beltrami basis for \(L^{2}()\). Our notion of dimension of the function space is \(N():=\#\{:_{}\}\). It can be shown that if a Lie group \(G\) acts smoothly on the compact manifold \(\), then \(G\) also acts on eigenspaces of the Laplace-Beltrami operator. Accordingly, we define the dimension \(N(;G)\) as the dimension of projecting the eigenspaces of the Laplace-Beltrami operator on \(\) onto the space of \(G\)-invariant functions, that is, the number of _invariant_ eigenfunctions with eigenvalue up to \(\) (Appendix A.7).

Characterizing the asymptotic behavior of \(N(;G)\) is essential for proving our main results on the gain of invariances. Intuitively, the quantity \(N(;G)/N()\) corresponds to the fraction of functions that are \(G\)-invariant. One of this paper's main contributions is to determine the exact asymptotic behavior of this quantity for the analysis of KRR. The tight bound on \(N(;G)\) can be of potential independent interest to other problems related to learning with invariances.

**Theorem 4.4** (Dimension counting theorem).: _Let \((,g)\) be a smooth connected compact boundary-less Riemannian manifold of dimension \(()\) and let \(G\) be a compact Lie group of dimension \((G)\) acting isometrically on \((,g)\). Recall the definition of the effective dimension of the quotient space \(d:=()-(G)+_{x}(G_{x})\). Then,_

\[N(;G)=}{(2)^{d}}(/G) ^{d/2}+(^{}), \]

_as \(\), where \(_{d}\) is the volume of the unit ball in \(^{d}\)._

In Appendix B, we prove a generalized version of the above bound (i.e., a _local_ version).

To see how this counting bound relates to Sobolev spaces, note that by Mercer's theorem, a positive-definite symmetric (PDS) kernel \(K:\) can be diagonalized in an appropriate orthonormal basis of functions in \(L^{2}()\). Indeed, the kernel of the Sobolev space \(^{s}() L^{2}()\) is diagonalizable in the Laplace-Beltrami basis4:

\[K_{^{s}()}(x,y)=_{=0}^{}(1,_{ }^{-s})_{}(x)_{}(y), \]

where \(_{},=0,1,\), form a basis for \(L^{2}()\) such that \(_{g}_{}+_{}_{}=0\) for each \(\). For the \(G\)-invariant RKHS \(^{s}_{}()\), the sum is restricted to \(G\)-invariant eigenfunctions (Appendix A.9). It is evident that Theorem 4.4 provides an important tool for analyzing the Sobolev kernel development with respect to the eigenfunctions of Laplacian.

### Proof Idea of the Dimension Counting Theorem

For Riemannian manifolds, Weyl's law (Appendix A.4) determines the asymptotic distribution of eigenvalues. The bound in Theorem 4.4 indeed corresponds to Weyl's law, if we write it in terms of the quotient space \(/G\). But, in general, Weyl's law does not apply to the quotient space \(/G\). So this intuition is not rigorous. First, the quotient space is not always a manifold (Appendix A.5). Second, even if we restrict our attention to the principal part \(_{0}/G\) discussed above, which is provably a manifold, other complications arise.

In particular, the quotient \(_{0}/G\) can exhibit a boundary, even if the original space is boundaryless. For a concrete example, consider the circle \(^{1}=\{(x,y):x^{2}+y^{2}=1\}\), parameterized by the angle \([0,2)\), under the isometric action \(G=\{_{G},\}\), where \(()=-\) is the reflection across the \(y\)-axis. The resulting space is a semicircle with two boundary points \((0, 1)\).

If the space has a boundary, then a dimension counting result like Weyl's law for manifolds is only true with an appropriate boundary condition for finding the eigenfunctions. In general, different boundary conditions can lead to completely different function spaces for manifolds with boundaries. In the proof, we show that the projections of invariant functions onto the quotient space satisfy the Neumann boundary condition on the (potential) boundaries of the quotient space, thereby exactly characterizing the space of invariant functions, which can indeed be of independent interest.

To see how the Neumann boundary condition appears, consider the circle again and note that its eigenfunctions are the constant function \(_{0} 1\), \((k)\), and \((k)\), with eigenvalues \(=k^{2}\), \(k\). Observe that every eigenvalue is of multiplicity two, except for the zero eigenvalue, which has a multiplicity of one. For the quotient space \(^{1}/G\), however, the eigenfunctions are just the constant function, \(((2k+1))\), and \((2k)\), \(k\) (note how roughly half of the eigenfunctions survived, as \(|G|=2\)). In particular, the boundary points \((0, 1)\) satisfy the Neumann boundary condition, while the Dirichlet boundary condition fails to hold; look at the eigenfunctions at \(=/2\). More precisely, if we consider the Dirichlet boundary condition, then we get a function space that includes only functions vanishing at the boundary points: \((/2)=(3/2)=0\). This clearly does not correspond to the space of invariant functions. We generalize this idea in our proof to any manifold and group using differential geometric tools (see Appendix A.5 and Appendix A.6).

In the above example, the boundary points come from the fact that the group action has non-trivial fixed points, i.e., \((0, 1)\) are the fixed points. If the action is free, meaning that we have only trivial fixed points, then the quotient space is indeed a boundaryless manifold. Thus, the challenges towards the proof arise from the existence of non-trivial fixed points.

**Comparison to prior work.** Lastly, we discuss an example on the two-dimensional flat torus \(^{2}=^{2}/2^{2}\), which shows that the proof ideas in  are indeed not applicable for general manifolds even with finite group actions. For this boundaryless manifold, consider the isometric action \(G=\{_{G},\}\), where \((_{1},_{2})=(_{1}+,_{2})\), and note that the quotient space \(^{2}/G\) is again a torus. In this case, the eigenfunctions of \(^{2}\) are the functions \((ik_{1}x+ik_{2}y)\), for all \(k_{1},k_{2}\), with eigenvalue \(=k_{1}^{2}+k_{2}^{2}\). But eigenfunctions of the quotient space are those with even \(k_{1}\). This means that some eigenspaces of \(^{2}\) (such as those with eigenvalue \(=2(2n+1)^{2}\)) are completely lost after projection onto the space of invariant functions. This means that the method used in  fails to give a non-trivial bound here, because it relies on the fraction of eigenfunctions that survive in each eigenspace. Note that in this example, the quotient space is boundaryless, and the action is free.

### Application to Finite-Dimensional Kernels

Applications of Theorem 4.4 are not limited to Sobolev spaces. As an example, we study KRR with finite-dimensional PDS kernels \(K:\) with an RKHS \( L^{}()\) under invariances (the inclusion must be understood in terms of Hilbert spaces, i.e., the inner product on \(\) is just the usual inner product defined on \(L^{2}()\), making it completely different from Sobolev spaces). Examples of these finite-dimensional spaces are random feature models and two-layer neural networks in the lazy training regime. In this section, we will relate the generalization error of KRR to the average amount of fluctuations of functions in the space and use our dimension counting result to study the gain of invariances.

To formalize this notion of complexity, we need to review some definitions. We measure fluctuation via the _Dirichlet form_\(\), a bilinear form defined as

\[(f_{1},f_{2}):=_{}_{g}f_{1}(x),_ {g}f_{2}(x)_{g}d_{g}(x), \]

for any two smooth functions \(f_{1},f_{2}:\). It can be easily extended (continuously) to any Sobolev space \(^{s}()\), \(s 1\). For each \(f^{s}()\), the diagonal quantity \((f,f)\) is called the Dirichlet energy of the function, and is a measure of complexity of a function. Functions with low Dirichlet energy have little fluctuation; intuitively, those have low (normalized) Lipschitz constants on average. Since \((af,af)=|a|^{2}(f,f)\), it is more accurate to restrict to the case \(\|f\|_{L^{2}()}=1\) while studying low-energy functions.

One important example of a finite-dimensional function space is the space of \(G\)-invariant low-energy functions, which is generated by a finite-dimensional \(G\)-invariant kernel \(K\):

\[K(x,y)=_{=0}^{D-1}_{}(x)_{}(y), \]

for any \(x,y\). The non-zero \(G\)-invariant eigenfunctions \(_{}\) are sorted with respect to their eigenvalues (Appendix A.9). Clearly, \(K\) is a kernel of dimension \(D\), and it is diagonal in the basis of the Laplace-Beltrami operator's eigenfunctions. The RKHS of \(K\) is also of finite dimension \((_{G})=D\) and can be written as

\[_{G}:=f L^{2}():f=_{=0}^{D-1} f,_{}_{L^{2}()}_{}} L^{2}_{ }(,G). \]

To obtain generalization bounds, we will need to bound a complexity measure of the space of low-energy invariant functions. As a complexity measure for finite function spaces \( L^{2}_{}(,G)\), we use the Dirichlet energy \((f,f)\):

\[():=_{f}(f,f):\|f \|_{L^{2}()} 1}. \]

For a vector space \(\), larger \(()\) corresponds to having functions with more (normalized) fluctuation. Thus, \((V)\) is a notion of complexity for the vector space \(V\). The following proposition shows how \(_{G}\) is the _simplest_ subspace of \(L^{2}()\) with dimension \(D\).

**Proposition 4.5**.: _For any \(D\)-dimensional vector space \( L^{2}_{}(,G)\),_

\[()_{D-1}, \]

_and the equality is only achieved when \(=_{G}\) with \(D=(_{G})\). In particular, if \(()<\), then \(\) is of finite dimension. The eigenvalues are sorted according to \(G\)-invariant eigenspaces (Appendix A.9)._

Using the dimension counting bound in Theorem 4.4, we can explicitly relate the dimension of \(_{G}\) to its complexity \((_{G})\). This will be useful for studying the gain of invariances for finite-dimensional kernels.

**Theorem 4.6** (Dimension of the space of low-energy functions).: _Under the assumptions in Theorem 4.4, one has the following relation between the dimension of the vector space \(_{G}\) and its complexity \((_{G})\):_

\[(_{G})=}{(2)^{d}}( /G)(_{G})^{d/2}+(( _{G})^{}), \]

_where \(_{d}\) is the volume of the unit ball in \(^{d}\)._

Given the above result, in conjunction with Proposition 4.5, we can obtain the following generalization error for any finite-dimensional RKHS \( L^{2}()\).

**Corollary 4.7** (Convergence rate of KRR with invariances for finite dimensional kernels).: _Under the assumptions in Theorem 4.4, for KRR with an arbitrary finite-dimensional \(G\)-invariant RKHS \( L^{2}()\),_

\[()-(f^{}) \!\!}{(2)^{d}}( /G)}{n}()^{d/2}\|f^{}\|^{2}_{L^ {2}()}, \]

_where \(\) hides absolute constants. Moreover, the upper bound is minimax optimal if \(=_{G}\) (similar to Theorem 4.3)._

Corollary 4.7 shows the same gain of invariances in terms of sample complexity as Theorem 4.1. Note that in the asymptotic analysis for the above corollary, we assume that \(()\) is large enough, allowing us to use Theorem 4.6.

## 5 Examples and Applications

Next, we make our general results concrete for a number of popular learning settings with invariances. This yields results for kernel versions of popular corresponding architectures.

### Sets

When learning with sets, each data instance is a subset \(\{x_{1},x_{2},x_{m}\}\) of elements \(x_{i}\), \(i[m]\), from a given space \(\). A set is invariant under permutations of its elements, i.e.,

\[\{x_{1},x_{2},x_{m}\}=\{x_{_{1}},x_{_{2}},x_{_{m }}\}, \]

where \(:[m][m]\) can be any permutation. A successful permutation invariant architecture for learning on sets is DeepSets . Similarly, PointNets are a permutation invariant architecture for point clouds [53; 54]. To analyze learning with sets and kernel versions of these architectures using our formulation, we assume sets of fixed cardinality \(m\). If the space \(\) has a manifold structure, then one can identify each data instance as a point on the product manifold

\[=^{m}= }_{m}. \]

The task is invariant to the action of the symmetric group \(S_{m}\) on \(\); each \( S_{m}\) acts on \(\) by permuting the coordinates as in Equation (19). This action is isometric, and we have \((S_{m})=0\) and \(|S_{m}|=1/m!\). Theorem 4.1 hence implies that the sample complexity gain from permutation invariance is having effectively \(n m!\) samples, where \(n\) is the number of observed sets. In fact, this result holds (for KRR) for _any_ space \(\) with a manifold structure.

### Images

For images, we need translation invariant models. For instance, Convolutional Neural Networks (CNNs) [31; 28] compute translation invariant image representations. Each image is a 2D array \((x_{i,j})_{i,j=0}^{m-1}\) such that \(x_{i,j}\) for a space \(\) (e.g., for RGB, \(^{3}\) is a compact subset). If \(\) has a manifold structure, then one can identify each image with a point on the manifold

\[=_{i,j=0}^{m-1}^{i,j}, \]

where \(^{i,j}\) is a copy of \(\). The learning task is invariant under the action of the finite group \((/m)(/m)\) on \(\) by shifting pixels: each \((p,q)(/m)(/m)\) corresponds to the isometry \((x_{i,j})_{i,j=1}^{m}(x_{i+p,j+q})_{i,j=1}^{m}\) (the sum is understood modulo \(m\)). As a result, the sample complexity gain corresponds to having effectively \(n m^{2}\) samples, where \(n\) is the number of images.

### Point Clouds

3D point clouds have rotation, translation, and permutation symmetries. Tensor field neural networks  respect these invariances. We view each 3D point cloud as a set \(\{x_{1},x_{2},x_{m}\}\) such that \(x_{i}^{3}/^{3}^{3}\), which is essentially a point on the manifold \(=(^{3}/^{3})^{m}\) with \(()=3m\). The learning task is invariant with respect to permuting the coordinates of \(\), translating all points \(x_{i} x_{i}+r\) for some \(r^{3}\), and jointly rotating all points, \(x_{i} Qx_{i}\) for an orthogonal matrix \(Q\). We denote the group defined by those three operations as \(G\) and observe that \((G)=6\). Thus, the gains of invariances in sample complexity are (1) reducing the dimension \(d\) of the space from \(3m\) to \(3m-6\), and (2) having effectively \(n m!\) samples, where \(n\) is the number of point clouds.

### Sign Flips of Eigenvectors

SignNet  is a recent architecture for learning functions of eigenvectors in a spectral decomposition. Each data instance is a sequence of eigenvectors \((v_{1},v_{2},,v_{m})\), \(v_{i}^{d}\), and flipping the sign of an eigenvector \(v_{i}-v_{i}\) does not change its eigenspace. The spectral data can be considered as a point on the manifold \(=(^{d-1})^{m}\) (where \(^{d-1}\) is the \((d-1)\)-dimensional sphere), while the task is invariant to all \(2^{m}\) possibilities of sign flips. The sample complexity gain of invariances is thus having effectively \(n 2^{m}\) samples, where \(n\) is the number of spectral data points.

### Changes of Basis for Eigenvectors

BasisNet  represents spectral data with eigenvalue multiplicities. Each input instance is a sequence of eigenspaces \((V_{1},V_{2},,V_{p})\), and each \(V_{i}\) is represented by an orthonormal basis suchas \((v_{i,1},v_{i,2},,v_{i,m_{i}})(^{d})^{m_{i}}\). This is the _Stiefel manifold_ with dimension \(dm_{i}-(m_{i}+1)}{2}\). Thus, the spectral data lie on a manifold of dimension

\[()=_{i=1}^{p}dm_{i}-m_{i}(m_{i}+1)/2. \]

The vector spaces' representations are invariant to a change of basis, i.e., the group action is defined as \((v_{i,1},v_{i,2},,v_{i,m_{i}})(Qv_{i,1},Qv_{i,2},,Qv_{i,m_{i }})\) for any orthogonal matrix \(Q\) that fixes the eigenspace \(V_{i}\). If \(G\) denotes this group of invariances, then

\[(G)=_{i=1}^{p}m_{i}(m_{i}-1)/2. \]

Thus, the gain of invariances is a reduction of the manifold's dimension to \(_{i=1}^{p}(dm_{i}-m_{i}^{2})\). For example, if \(m_{i}=m\) for all \(i\), then with \(d=pm\) we get \(()=d^{2}-d(m+1)\) while after the reduction we have \((/G)=d^{2}-dm\). In this example, the quotient space is the _Grassmannian manifold_.

### Learning on Graphs

Each weighted, possibly directed graph on \(m\) vertices with initial node attributes can be naturally encoded by its adjacency matrix \(A^{m m}\). Thus, the space of all weighted directed graphs on \(m\) vertices corresponds to a compact manifold \(^{m m}\) if the weights are restricted to a bounded set. Learning tasks on graphs are invariant to permutations of rows and columns, i.e., the action of the symmetric group as \(A P^{-1}AP\) for any permutation matrix \(P\). For instance, Graph Neural Networks (GNNs) and graph kernels  implement this invariance. The sample complexity gain from invariances is thus evident; it corresponds to having effectively \(n m!\) samples, where \(n\) is the number of sampled graphs.

### Hyperbolic Spaces, Tori, etc.

One important feature of the results in this paper is that they are not restricted to compact submanifolds of Euclidean spaces. In particular, the results are also valid for compact hyperbolic spaces; see  for a survey on applications of hyperbolic spaces in machine learning. Another type of space where our results are still valid are tori, i.e., \(^{d}:=(^{1})^{d}\). Tori naturally occur for modeling joints in robot control . In fact, Riemannian manifolds are beneficial in a broader context for learning in robotics, e.g., arising from constraints, as surveyed in . Our results apply to invariant learning in all of these settings, too.

## 6 Conclusion

In this work, we derived new generalization bounds for learning with invariances. These generalization bounds show a two-fold gain in sample complexity: (1) in the dimension term in the exponent, and (2) in the effective number of samples. Our results significantly generalize the range of settings where the bounds apply. In particular, (1) goes beyond prior work, since it applies to groups of positive dimension, whereas prior work assumed finite dimensions. At the heart of our analysis is a new dimension counting bound for invariant functions on manifolds, which we expect to be useful more generally for analyzing learning with invariances. We prove this bound via differential geometry and show how to overcome several technical challenges related to the properties of the quotient space.