# Artemis: Towards Referential Understanding in Complex Videos

Jihao Qiu\({}^{1}\)1 Yuan Zhang\({}^{1*}\) Xi Tang\({}^{1*}\) Lingxi Xie Tianren Ma\({}^{1}\)

 Pengyu Yan\({}^{2}\) David Doermann\({}^{2}\) Qixiang Ye\({}^{1}\) Yunjie Tian\({}^{1,2}\)\({}^{1}\)University of Chinese Academy of Sciences \({}^{2}\)University at Buffalo

{qiujiahao19, zhangyuan192, tangxi19, matianren18, tianyunjie19}@mails.ucas.ac.cn

pyan4@buffalo.edu doermann@buffalo.edu 198808xc@gmail.com qxye@ucas.ac.cn

Equal contribution. \(\) Corresponding Author.

###### Abstract

Videos carry rich visual information including object description, action, interaction, _etc._, but the existing multimodal large language models (MLLMs) fell short in referential understanding scenarios such as video-based referring. In this paper, we present **Artemis**, an MLLM that pushes video-based referential understanding to a finer level. Given a video, Artemis receives a natural-language question with a bounding box in any video frame and describes the referred target in the entire video. The key to achieving this goal lies in extracting compact, target-specific video features, where we set a solid baseline by tracking and selecting spatiotemporal features from the video. We train Artemis on the newly established VideoRef45K dataset with 45K video-QA pairs and design a computationally efficient, three-stage training procedure. Results are promising both quantitatively and qualitatively. Additionally, we show that Artemis can be integrated with video grounding and text summarization tools to understand more complex scenarios. Code and data are available at [https://github.com/qiujihao19/Artemis](https://github.com/qiujihao19/Artemis).

## 1 Introduction

The past year has witnessed rapid progress of multimodal large language models (MLLMs) , offering abundant abilities of open-world image understanding with language-based dialogues. In comparison, there are fewer studies on training MLLMs for video understanding, albeit videos are much more informative than still images. Existing video-based MLLMs  mostly focus on superficial dialogues in which the video is encoded holistically, inevitably lacking the ability to understand fine-level video contents, _e.g._, describing a user-specific target in the video.

We are considering a new task called video-based referential understanding to compensate for the limitation. Specifically, we are interested in complex videos that span \(20\)-\(30\) seconds and the target performs multiple actions during this period. Given a video, the MLLM tries to answer a question like 'What is the target <region> doing in this video?' where <region> refers to a bounding box in any video frame. We argue that the task is not only challenging as it requires feature extraction, tracking, summarization, _etc._, but also important because it lays the foundation of finer-level video understanding. However, as shown in Figure 1, existing MLLMs often fell short inthis seemingly easy task because they were mostly trained for image-based referential understanding; as a result, they can only perceive the action in a single moment rather than that in an entire video2.

This paper presents **Artemis3** as a solid baseline for the above task. Artemis follows the generic design of modern MLLMs (_i.e._, visual instruction tuning ), but encounters a challenge in finding _sparse_, target-related information from _dense_ video data. A preliminary study shows that feeding raw video features into the MLLM results in computational inefficiency and training instability. To extract target-specific video features, we propose a simple yet effective solution that involves (i) tracking the target over time and (ii) selecting informative features from a long list of regions-of-interest (RoIs). The compactness of features makes it easier to train the MLLM. We design a three-stage training schedule where the MLLM gradually learns video-text alignment from coarse to fine. This efficient design requires only \(28\) hours (\(3\) hours for the final stage) on \(8\) NVIDIA-A800 GPUs.

To train and evaluate Artemis, we organize \(7\) existing video understanding datasets into the **Video-Ref45K** benchmark comprising 45K video question-answer pairs. To our knowledge, this is the first

Figure 1: Artemisâ€™ ability in video-based dialogue. Notably, Artemis excels particularly in video-based referring, outperforming the existing MLLMs including Merlin  and Video-LLaVA  lacking comprehensiveness and Osprey  suffering hallucination.

benchmark with box-level prompts and answers spanning complex videos. Experiments show the promising results of Artemis in a wide range of quantitative metrics including the BERT score, BLEU, _etc._. Qualitatively, Artemis also shows a clear advantage in the comprehensiveness of description meanwhile avoiding hallucination (see Figure 1 for examples). Beyond the ability of video-based referring, Artemis serves as an important building block for complex video understanding, where we integrate Artemis with off-the-shelf video grounding and text summarization tools for interactive video-based dialogue and long video understanding, respectively. We expect our work to shed light on upgrading MLLMs for fine-level and interactive video understanding.

## 2 Related Work

**Large language models (LLMs) and multimodal LLMs).** LLMs [15; 5; 12; 45; 13; 64; 49; 60; 11] have opened a new era of AI, demonstrating the potential to deal with various language-based understanding and generation tasks. To unleash the power of LLMs for visual understanding, the computer vision community has been working on aligning language and vision data in the same feature space . There are mainly two lines of research, where the _internal_ adaptation methods  integrated cross-attention within an LLM for visual-language alignment, and the _external_ adaptation methods [27; 14; 33] trained extra modules for this purpose. As a result, the vision foundation models, especially vision transformers [17; 35; 41; 48; 47; 66; 25], have been upgraded into MLLMs [33; 46; 63; 26] which gain the ability of language-guided visual understanding.

**MLLMs for referring and grounding.** MLLMs can be integrated with instance-level visual understanding tasks, allowing the models to (i) respond to questions targeted at specific regions of the image and (ii) identify regions corresponding to the contents in the dialogue - these functions are referred to as visual referring [63; 7] and grounding [40; 34], respectively. There are two main ways to integrate these functions into MLLMs, differing from each other in how the positional information is processed. The _explicit_ methods [40; 52] introduced extra tokens to encode positions, while the _implicit_ methods [9; 51; 54] used natural language to represent positions. Recently, there are also efforts  that used LLMs to call external vision modules for more flexible instance-level understanding quests.

**Video-based MLLMs.** Compared to the large corpus of image-based MLLMs, there are fewer video-based MLLMs for at least two reasons. First, there are fewer paired video-text data, especially for instance-level video understanding. Second, the higher dimensionality of video data poses a greater challenge to efficiently encode videos into visual features and find useful features to answer the questions. Existing efforts include VideoChat , Video-ChatGPT , Video-LLaMA , Video-LLaVA , LanguageBind , Valley , _etc._; most of them followed the paradigm of image-based MLLMs and some of them  proposed a more efficient video feature. Recently, there have been some preliminary studies for instance-level video understanding, _e.g._, LEGO  studied moment retrieval with the assistance of LLMs, and PG-Video-LLaVA  performed video grounding by employing off-the-shelf tracking and grounding modules. Merlin  studied video-based referring, but it was built upon three manually specified frames as visual input, incurring extra burden for users and also limiting the model's ability to understand long and complex videos. _This paper aims to address the above two challenges, for which we set up a new formulation, establish a new benchmark named VideoRef45K, and present a solid baseline named Artemis._

## 3 Artemis: A Baseline for Video-based Referential Understanding

### Problem Formulation and Data Preparation

A video can be represented in the raw form of \(^{T W H C}\), where \(T\), \(W\), \(H\), and \(C\) stand for the number of frames, width, height, and the number of channels, respectively. In the task of video-based referential understanding (_a.k.a._ video-based referring), the model receives a question in the form of 'What is the <region> doing in this video?', where the concrete class of the referred object (like man or dog) is not provided, and the <region> is supplemented by a bounding box \(=(t;x_{1},y_{1},x_{2},y_{2})\) in a frame \(t\{1,2,,T\}\). The expected output is a sentence describing the target's action in the full video as detailed as possible (see Figure 1 for examples). Note that the proposed task requires a stronger ability beyond image-based referring and video captioning,mainly in the coverage and granularity of visual understanding. Specifically, the model is expected to produce complex action descriptions for different target <region> specified.

We collect video data for referential understanding from 7 datasets, including HC-STVG , VID-Sentence , A2D Sentences , LaSOT , MeViS , GOT10K , and MGIT . In total, there are 45K video QA pairs. We perform dataset-specific operations, including re-tracking (for HC-STVG and A2D-Sentences), clip cropping (for LaSOT and MGIT), and caption summary (for GOT10K), to convert them into the required form. Please refer to Appendix A for further details.

### Overall Framework and Visual Features

The overall framework of Artemis, as illustrated in Figure 2, follows the pipeline of visual instruction tuning  where a multimodal large language model (MLLM) receives video features with a text prompt and produces the desired output. We denote the function as \(_{}=f(_{},_{})\), where \(_{}\) and \(_{}\) are input and output texts (in tokens) and \(_{}\) is the set of features extracted from \(\).

Compared to image-based referring, a clear difficulty of video-based referring arises from the high dimensionality of video data. Specifically, if we define \(_{}\) as the set of dense video features (_e.g._, using a pre-trained visual encoder such as the CLIP ViT-L model  to extract frame-wise visual features for \(\)), the features often contain highly redundant information due to the similarity of neighboring frames. This brings two-fold drawbacks: (i) extra complexity for the MLLM to deal with these vision tokens, and (ii) extra difficulty for the MLLM to locate useful information, which leads to a slower convergence. To overcome this issue, we decrease the input feature dimensionality by using various slices to replace \(_{}\), where each slice captures important yet complementary properties of the input video. Throughout this paper, we investigate three slices: the spatial, temporal, and target-specific video features.

**Spatial and temporal features.** The extraction of spatial and temporal video features follows the design of Video-ChatGPT . Given a video clip \(V^{T W H C}\), we use the CLIP ViT-L/14 visual encoder to cast it into frame-wise features, denoted as \(F_{}^{T W^{} H^{} D}\), where the number of frames remains unchanged, \(W^{}=W/s\) and \(H^{}=H/s\) are the down-sampled resolution (_e.g._, \(s=14\) for ViT-L/14) of the visual features, and \(D\) is the feature dimensionality.) Then, these features are fed into average pooling along the \(T\) axis (into the spatial features \(_{}^{}^{W^{} H^{} D}\)) and along the \(W^{} H^{}\) plane (into the temporal features \(_{}^{}^{T D}\)), respectively.

Figure 2: **Left**: the overall framework of Artemis, where an MLLM receives a text prompt together with spatial, temporal, and target-specific video features, and produces the answer. **Right**: the RoI tracking and selection mechanism to generate target-specific features. We use different IDs to show the clustering result. _This figure is best viewed in color._

**Target-specific features.**\(^{}_{}\) and \(^{}_{}\) have focused on the spatial and temporal features but ignored the referred target which may move or change during the video. To offer a compromise feature that captures spatiotemporal features, we propose an RoI (region-of-interest) tracking and selection mechanism (detailed in Section 3.3) and obtain a list of RoIs (represented as bounding boxes) \(=(_{1},,_{M})\), where \(M\) is the number of RoIs that are recognized by the algorithm to be important for referential understanding. We use the RoAlAlign method  to extract visual features from each RoI, producing a set of target-specific features, \(^{}_{}=(^{}_{, _{1}},,^{}_{,_{M}})\).

**Instruction fine-tuning.** When the video features are ready, we feed them with the text tokens into Artemis. The MLLM follows instruction fine-tuning through three steps, gradually acquiring the ability of video-based referring. The details are described in Section 3.4.

### RoI Tracking and Selection

Our goal is to extract compact features for video-based referring. The key lies in two factors, (i) completeness - locating the referred target in every video frame, and (ii) avoiding redundancy - not preserving too many features in the frames with similar semantics. We propose a simple solution upon RoI tracking and selection. As we shall see later, it offers a solid baseline for future work.

**Step 1: RoI tracking.** We apply HQTrack , an off-the-shelf tracking algorithm, to localize the RoI in each input frame. The pre-trained tracking model is not fine-tuned in the training phase. Given a RoI (a bounding box) in any video frame, the tracking algorithm outputs either a bounding box or nothing (_e.g._, if the target is occluded) in each of the remaining frames. This step outputs a raw list of RoIs denoted as \(^{}=(^{}_{1},,^{}_{M^{ }})\) where \(M^{}\) can be close to the number of frames.

**Step 2: RoI selection.** Feeding all tracked frames into the MLLM often incurs computational inefficiency and extra difficulties in model training. To avoid this, we select a subset from \(^{}\) containing \(M<M^{}\) RoIs, with the goal being to preserve diverse visual features using a limited number of RoIs. In practice, we pre-defined the target number, \(M\), and adopt the K-means algorithm to form \(M\) clusters from the original set of \(M^{}\) RoIs. The final RoI list, \(\), consists of a randomly chosen RoI from each cluster.

**Discussions.** Finding representative RoIs belongs to a generic topic of feature selection. On one hand, one can set a simple baseline by performing random or uniform sampling from the original set \(^{}\). On the other hand, the information theory offers a general principle, _i.e._, maximize the diversity of RoIs throughout the selection procedure. As demonstrated in Section 4.1, random and uniform sampling algorithms frequently fail to capture semantic changes throughout complex videos. By contrast, the simple K-means clustering used in Artemis significantly increases the diversity (see Appendix D), ensuring representative video features. We conjecture that the effectiveness of feature selection is related to the quality of video features; with stronger video foundation models, more sophisticated feature selection algorithms can make a larger difference. We leave this topic to future research.

### Model Architecture and Training

The MLLM is built upon Vicuna-7B v1.5 , an open-sourced LLM4. We use CLIP ViT-L/14  to extract visual features. To feed these 1024-dimensional visual tokens into the LLM, we use a learnable, two-layer MLP (1024-4096-4096) to project the visual features into the 4096-dimensional language space. We always use the auto-regressive framework to train the MLLM.

The training procedure of Artemis comprises three steps, (1) video-text pre-training, (2) video-based instruction tuning, and (3) video-based referring. The first two stages are similar to Video-LLaVA  but different training data are used. We set a unified template,

User: <video-tokens> <instruction> Assistant:

guiding the model to output the desired answer. Here, <video-tokens> contains the spatial and temporal video features (\(^{}_{}\) and \(^{}_{}\), projected by MLP), and <instruction> contains the language tokens of the task description (see below).

In the first stage, <instruction> has the form of 'Write a terse but informative summary of the following video clip.' and the model outputs the overall description of the video.

The training data includes image-text and video-text pairs, using images as still videos. We use a subset of 558K LAION-CCSBU image-text pairs with BLIP  captions, sourced from CC3M  and refined by LLaVA . Additionally, we use the 702K video-text pairs provided by Video-LLaVA , derived from the 703K pairs constructed by Valley  using WebVid . Only the MLP is trained (from scratch) in this stage, initializing the alignment of vision and language. The training elapses one epoch with a learning rate of \(1 10^{-3}\), taking about 5 hours on \(8\)A800 GPUs.

In the second stage, <instruction> contains specific task descriptions like 'Where is the person in the image?' and 'What is the person doing in the video?', and the model follows the instruction to produce the answer. The training data comprises the 665K image-text instruction dataset from LLaVA-1.5  and the 100K video-text instruction set from Video-ChatGPT . Both the LLM and MLP are fine-tuned in this stage. The training elapses one epoch with a learning rate of \(2 10^{-5}\), taking about 20 hours on \(8\)A800 GPUs.

In the third stage, we use the curated VideoRef45K dataset to endow the model with the ability of video-based referring. The template is modified as follows,

User: <video-tokens> <refer-instruction> <track-instruction> Assistant:

Here, <refer-instruction> is formulated as 'What is the <region> doing during this video?' where the <region> token is replaced by the visual features extracted from the bounding box in the specified input frame, and <track-instruction> contains additional information, 'This is the tracking list: <region>,..., <region>' where the <region> tokens are the target-specific features \((^{}_{,_{1}},,^{ }_{,_{M}}\), projected by a Linear) extracted from the selected RoIs5, and the number of <region> token is \(M\). In this stage, we fine-tune the LLM (with LoRA ), MLP and the RoI Align module. The training procedure elapses 3 epochs with a learning rate of \(4 10^{-5}\), taking about 3 hours on \(8\)A800 GPUs.

## 4 Experiments

### Artemis Is a Strong Baseline for Video-based Referential Understanding

**Setting and metrics.** We evaluate the ability of Artemis in video-based referring on the test set of HC-STVG . The video and text data are pre-processed using the same method as in the training set. The test procedure uses the same instruction as in the third training stage and applies HQTrack  to localize the RoIs in video frames. We use the standard evaluation metrics including BERTScore , BLEU@4 , METEOR , ROUGE_L , CIDEr , and SPICE .

**Adapting existing MLLMs for video-based referring.** Due to the limited availability of research for video-based referring, we compare our model to a few recent MLLMs trained for image-based or multi-frame based referring6. The image-based referring models include Osprey , Ferret ,

  Method & BERT Score & BLEU@4 & METEOR & ROUGE\_L & CIDEr & SPICE \\  Osprey  & 0.8698 & 0.7 & 12.0 & 18.0 & 1.2 & 15.6 \\ Ferret-13B  & 0.8632 & 0.5 & 10.2 & 17.0 & 1.2 & 11.2 \\ Shikra-7B  & 0.8742 & 1.3 & 11.5 & 19.3 & 3.1 & 13.6 \\  Video-ChatGPT  & 0.8718 & 1.3 & 10.1 & 20.2 & 5.5 & 11.7 \\ Video-LLaVA  & 0.8639 & 1.7 & 9.8 & 20.8 & 2.6 & 9.1 \\  Merlin 7  & 0.8829 & 3.3 & 11.3 & 26.0 & 10.5 & 20.1 \\  Artemis (Ours) & 0.9135 & 15.5 & 18.0 & 40.8 & 53.2 & 25.4 \\  

Table 1: A comparison of video-based referring metrics on the HC-STVG test set. \({}^{}\): We use \(5\) key frames while using \(8\) frames leads to worse results.

and Shikra . For each video, we extract \(5\) key frames with RoIs produced by HQTrack and ask the trained model "What is the target <region> doing?" in the way the models are familiar with. Finally, we use GPT-3.5-Turbo to summarize the \(5\) answers into the overall description of the target. The multi-frame based reference model is Merlin  which receives \(5\) key video frames and RoIs and produces the overall description. The selection of key frames is consistent with Artemis. To compare with MLLMs that are trained for video understanding, such as Video-ChatGPT  and Video-LLaVA , we follow  to draw a red rectangle to mark the referred object in each key frame of the video. Then, we feed the rendered video to the models and ask the question "What is the target indicated by the red rectangle doing?".

**Quantitative results, and necessity of native video-based referring.** The numbers are summarized in Table 1. Artemis outperforms other MLLMs in each single evaluation metric. Note that the advantage is significant for some metrics, _e.g._, BLEU4. Please refer to Figure 1 for representative examples. In Figure 9 (see Appendix C), we show the behavior of the methodology using a standalone LLM (_e.g._, GPT-3.5-Turbo) upon image-based referring outputs. The image-based models tend to describe individual moments rather than an entire video; based on these inputs, the LLM cannot realize video descriptions and is sometimes confused to hallucinate what never happens in the video. The comparison validates the necessity of training a **native** model (_i.e._, directly on the instruction data for video-based referring) like what Artemis has done. Equipping with such a fundamental ability of video understanding at a finer level, Artemis can perform even more complex video understanding tasks, as shown in Section 4.2.

**Qualitative results.** We display several representative examples of video-based referring in Figure 3. The output of Artemis is comprehensive (especially compared to other MLLMs, see Figure 1), often containing fine-grained actions of the target. This mainly concerns the compact video features extracted by the RoI tracking and selection algorithm that extracts key features for understanding.

**Ablative studies for target-specific video features.** The key to extracting compact target-specific features lies in RoI tracking and selection. To validate this, we ablate two key parameters: the strategy of RoI selection and the number of preserved RoIs. In Table 2, we define a baseline for region of interest. For each frame of a video containing an object of interest, we enclose the object's location

Figure 4: RoI manipulation increases the informativeness and diversity of RoIs. See Appendix D for details.

Figure 3: Artemis and Merlin for video-based referring. Note that Merlin needs the semantic class of <region> to be provided while Artemis does not. In each case, the orange rectangle indicates the input <region>, blue rectangles are the tracked RoIs, and yellow stars label the selected RoIs. Red and green texts indicate incorrect and correct answers, respectively. _This figure is best viewed in color._

[MISSING_PAGE_FAIL:8]

### Artemis Is a Building Block for Complex Video Understanding

With a strong ability of video-based referring, Artemis serves as a building block that strengthens the existing video-based MLLMs in complex video understanding.

**Multi-round video understanding with grounding.** Multi-round dialogues, especially answering logically related chain-of-questions , is an important yet challenging topic for MLLMs. In Figure 6 and Figure 14 in Appendix E, we show that Artemis's referential understanding ability can be combined with image-based grounding models (_e.g.,_ GroundingDINO ) to answer multi-round chain-of-questions, where the entities mentioned in the video-based referring result is located and fed into the next round of video-based referring quest, allowing for more complex interactions.

**Long video understanding with text summarization.** Target-centric understanding of long videos is a major challenge for existing video-based MLLMs. The difficulty mainly lies in extracting compact video features (to feed into the MLLM) and tracking the target throughout the long video. We offer a simple solution that first segments the video into shorter clips, applies Artemis for understanding these clips, and applies an off-the-shelf LLM (_e.g._, GPT-3.5-Turbo) for summarization. As shown in Figure 7 and Figure 12 in Appendix E, the final output offers a comprehensive understanding. To our knowledge, this function was not achieved by existing MLLMs.

**Video question answering.** Lastly, we show that Artemis can perform general video question answering. We test the trained model on the Video-ChatGPT test set  and the other three benchmarks (_i.e._, MSVD-QA , MSRVTT-QA , and ActivityNet-QA ) where their training sets was not used to train Artemis. Results are summarized in Tables 4. Artemis shows competitive performance among a few recent MLLMs. These results inspire us that (i) an MLLM trained for finer-level video understanding can seamlessly transfer to coarser-level tasks, and (ii) extracting compact video features also benefits video question answering.

## 5 Conclusion

This paper proposes a challenging setting for video-based referring and establishes an effective MLLM named Artemis. Compared to existing methods, Artemis can understand human intention from simpler inputs (a text prompt and a single-frame bounding box) and comprehensively describe the target's action in a complex video. At the core of Artemis is an RoI tracking and selection

Figure 6: An example of multi-round, video-based referring by integrating Artemis with GroundingDINO .

Figure 7: Example of long video understanding. We apply Artemis to output descriptions for segmented video clips and integrate them using an LLM (GPT-3.5-Turbo in this example).

mechanism to extract compact video features. Artemis shows advantages in video-based referring in VideoRef45K and transfers the ability to general video understanding, including being integrated with other modules for more complex tasks. We hope that Artemis can serve as a solid baseline to facilitate the research in fine-level video understanding.

**Limitations.** First, Artemis relies on a tracking algorithm to generate the RoIs; however, the tracking algorithm may produce inaccurate results and can confuse Artemis- see an example in Figure 13 (top) in Appendix E. Second, Artemis also suffers from the issues of general video-based understanding, such as the spatial-temporal aliasing problem, which can affect the model's ability to describe the visual content accurately - see an example in Figure 13 (bottom) where Artemis accurately predicts the movement of the target but reverses the temporal order.

## 6 Acknowledgments

This work was supported by National Natural Science Foundation of China (NSFC) under Grant No.62225208 and CAS Project for Young Scientists in Basic Research under Grant No.YSBR-117.