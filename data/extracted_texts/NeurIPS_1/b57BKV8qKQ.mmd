# Implicit-Zoo: A Large-Scale Dataset of Neural Implicit Functions for 2D Images and 3D Scenes

Qi Ma1,2 Danda Pani Paudel2 Ender Konukoglu1 Luc Van Gool1,2

1Computer Vision Lab, ETH Zurich 2INSAIT, Sofia University

###### Abstract

Neural implicit functions have demonstrated significant importance in various areas such as computer vision and graphics. Their advantages include the ability to represent complex shapes and scenes with high fidelity, smooth interpolation capabilities, and continuous representations. Despite these benefits, the development and analysis of implicit functions have been limited by the lack of comprehensive datasets and the substantial computational resources required for their implementation and evaluation. To address these challenges, we introduce "Implicit-Zoo": a large-scale dataset requiring thousands of GPU training days designed to advance research and development in this field. Our dataset includes diverse 2D and 3D scenes, such as CIFAR-10, ImageNet-1K, and Cityscapes for 2D image tasks, and the OmniObject3D dataset for 3D vision tasks. We ensure high quality through strict checks, refining or filtering out low-quality data. Using Implicit-Zoo, we showcase two immediate benefits as it enables to: (1) _learn token locations_ for transformer models; (2) _directly regress_ 3D cameras poses of 2D images with respect to NeRF models. This in turn leads to an _improved performance_ in all three task of image classification, semantic segmentation, and 3D pose regression - thereby unlocking new avenues for research. Our data and implementation are available at: [https://github.com/qimaqi/Implicit-Zoo/](https://github.com/qimaqi/Implicit-Zoo/)

Figure 1: _The Implicit-Zoo dataset and its example utilities._ We demonstrate three tasks using _Implicit-Zoo_: classification, segmentation, and 3D pose regression. Details of the problem statement can be found in section 4. The INRs are colorized differently to indicate their training data sources.

## 1 Introduction

Recent advances in modeling continuous functions implicitly using Multi-Layer Perceptrons (MLPs)  have sparked great interest in many applications. Implicit neural representations (INRs) involve fitting a function \(f(.)\) that maps input coordinates \(x\) to their corresponding value \(v_{x}\). For instance, in image modeling , this continuous function maps 2D pixel coordinates to RGB values. This approach has been extended to challenging 3D geometry and appearance [6; 8; 9; 10]. Implicit representations offer many advantages, including strong modeling effectiveness, memory efficiency, the flexibility of a single architecture to represent different data modalities, compatibility with arbitrary resolutions, and differentiability.

A hindrance to further advancements in implicit function research is the lack of large-scale datasets, primarily due to their computational demands. This paper aims to bridge this gap via the _Implicit-Zoo_ Datasets, with access to over 1.5 million implicit functions across multiple 2D and 3D tasks.

Some INRs datasets exist in both 2D and 3D formats. However, they are limited by data scale  and application scenarios [12; 13]. Additionally, while using modulation [14; 15] to learn the common parts of given dataset to accelerate convergence, it struggles to effectively handle unseen data samples. We will also organize and update the datasets used for concurrent work. For example,  use Instant-NGP  to train a large amount of indoor data.

We focus on generating dataset using INRs directly on modelling image signals because of their wide-ranging applications, following the existing remarkable success [18; 19]. We believe our dataset will have broad applicability and make a significant contribution to the community. Additionally, we chose popular 2D datasets because they have well-established benchmarks [2; 3; 1], enabling better performance comparison with other state-of-the-art algorithms.

Moreover, we developed a comprehensive 3D Implicit Neural Representations (INRs) dataset using Ombiobject3D  and establish the first benchmark for 3D INRs pose regression. As INRs gain the status of preferred representation for 3D scenes, determining pose from given images with trained INRs becomes crucial. [20; 21; 22] aim to jointly address reconstruction and image registration challenges during the training phase of Implicit Neural Representations (INRs) without relying on pose priors.[23; 24; 25] focus on pose regression using pretrained INRs. However, these methods only achieve convergence in scenarios with coarse pose initialization or when a scene-dependent regressor is trained, limiting their applicability to new scenes. To address this, we introduce a transformer-based approach that samples the neural radiance field to extract volume feature, integrating it with 2D images for precise pose regression. In unseen scenarios, our method achieves a rotational error of \(20^{o}\), with nearly \(80\%\) of poses having rotational errors below \(30^{o}\). Further refinement is shown by minimizing a photometric error .

To effectively train our large-scale dataset, we carefully selected model sizes 1 appropriate for the complexity of the data and ran a sufficient number of iterations. To maintain data quality, we conducted a second training round to guarantee that all images reached a PSNR of 30.

Thanks to the differentiability of INRs and our large-scale dataset, our transformer-based methods can efficiently optimize tokenization for various tasks. Instead of using manually designed tokenization techniques like standard patchification or volumification, our approach allows the network to learn the tokenization process directly from the large-scale dataset. We propose methods that utilize learnable

   Method & Task & Scenes & Model(Depth/Width) & GPU (days) & Overall Size (GB) & PSNR \\  CIFAR-10  & 2D & 60000 & 3 / 64 & 5.96 & 1.44 & 31.01 \\ ImageNet-1K  & 2D & 1431167 & 4 / 256 & 831.53 & 749.93 & 30.12 \\ CityScapes  & 2D & 23473 & 5 / 256 & 50.15 & 18.40 & 34.10 \\ Omnibject-3D  & 3D & 5914 & 4 / 128 & 69.81 & 5.96 & 31.54 \\   

Table 1: **Dataset Summary.** An overview of the generated datasets with their cost and quality. For 2D tasks, we employ SIREN , while for the 3D task, we utilize NERF . Computation is carried out on the ETH Euler cluster. We report PSNR (Peak Signal-to-Noise Ratio) as a quality metric. A PSNR of 30 dB corresponds to an RGB MSE of approximately 0.03. This level of error is hardly noticeable to the human eye, indicating the high fidelity of our dataset. Please refer to Figure 2 for examples.

patch centers and scales, as well as a fully learnable approach at the pixel-wise or point-wise level. Our findings indicate that this learnable tokenization significantly improves performance across multiple tasks. This research uncovers a novel direction: learnable tokenization.

In summary: our key contributions are as follows:

\(\) We create _Implicit-Zoo_, a large-scale implicit function dataset developed over almost 1000 GPU days. Through iterative refinement, incl. filtering and continuous training, we ensure its high quality.

\(\) We benchmark a range of tasks using this dataset, such as 2D image classification and segmentation. Additionally, we introduce a transformer-based approach for the direct pose regression for new images in the 3D neural radiance fields. For the latter, a novel baseline is also established.

\(\) By integrating learnable tokenizer, we enhance the benchmark methods across multiple tasks.

## 2 Related Work

### Implicit neural representations (INRs)

SIREN  use periodic activation functions to capture high-frequency details in images. Building on this,  propose using Gaussian functions, which offer greater robustness to random initialization and require fewer parameters.  introduce a continuous complex Gabor wavelet to robustly represent natural images with high accuracy.  focus on continuous representation for arbitrary resolution.Implicit Neural Representations (INRs) have been shown to effectively represent scenes as occupancy [8; 27; 5], object shape [29; 9; 30], sign distance function [10; 31], 3D scene appearance and dynamics [32; 33; 34; 35; 6; 36; 37; 38; 37] and other complex signals and problems[38; 39; 40; 41; 42].

### Transformer on various computer vision tasks

Vision Transformers (ViTs) have achieved state-of-the-art (SOTA) performance in several **Image recognition** tasks and are thus selected as our primary benchmark. It rely on the self-attention mechanism [44; 45] and require large datasets for effective training. This underscores the need for large datasets in Implicit Neural Representations (INRs) format, especially for transformer-based methods. Typically, after pre-training in either a supervised [18; 46] or self-supervised [47; 48; 49; 50; 51] manner, fine-tuning is performed on smaller datasets for downstream tasks. **Image segmentation,** which involves dense prediction and requires a larger number of tokens, poses challenges for ViTs with fixed token number and dimension. [52; 53; 54; 55; 56; 57; 52; 58; 59] addressed this issue by hierarchical reducing token numbers and increase token feature dimension through convolution backend. Such multi-stage design perform well for also recognition task. developed a lightweight transformer encoder that incorporates sequence reduction with a lightweight MLP decoder. In the field of **pose regression,** where transformer-based methods have also shown success[60; 61; 62; 63; 64; 65; 66]. Many of them build upon  which outputs a set of tuples with fixed cardinality for detection tasks.

### Tokenizer of Transformer

Recent advancements in vision transformers have focused on improving tokenization strategies to enhance performance and efficiency.  explored the effectiveness of tokenizers through Modulation across Tokens (MoTo) and TokenProp. The T2T-ViT model by  introduced progressive tokenization and an efficient backbone inspired by CNNs, leading to significant performance gains on ImageNet.  proposed MSViT, a dynamic mixed-scale tokenization scheme that adapts token scales based on image content, optimizing the accuracy-complexity trade-off.  developed token labeling, a dense supervision approach that reformulates image classification into token-level recognition tasks, greatly enhancing ViT performance and generalization on downstream tasks.

### Pose regression within Neural Radiance Field

LENS applies novel view synthesis to the robot re-localization problem, using NeRF to render synthetic datasets that improve camera pose regression. Loc-NeRF  combines Monte Carlo localization with NeRF to perform real-time robot localization using only an RGB camera. iNeRF inverts a trained NeRF to estimate 6DoF poses through gradient descent and propose proper ray sampling mechanism. Note that all method need heavy volumetric rendering process.

## 3 The Implicit-Zoo Dataset

We introduce the _Implicit-Zoo_ dataset which includes over 1.5 million INRs and took almost 1000 GPU days for training on RTX-2080. We first describe the dataset over different tasks and generation process, incl. some necessary changes and the cost for large scale training of implicit functions. Then we go though the quality check and lastly we describe the licence for the released dataset.

### Dataset generation

**CIFAR-10-INRs ** We test our algorithm with CIFAR-10. We employ a 3-layer SIREN MLP with a width of 64, without positional encoding . For all 2D image tasks the images are normalized using a mean of \([0.485,0.456,0.406]\) and a standard deviation of \([0.229,0.224,0.225]\). We use the Adam optimizer with a learning rate of 1e-3 and a cosine learning rate scheduler with minimum learning 1e-5 without warmup. Each image is trained for 1000 iterations, requiring 8.58 secs in total.

**ImageNet-1K-INRs** Following the standard procedure for ImageNet-1K classification, we resize images to 256x256 and then center crop them to 224x224 for further processing. To better fit the high-res images, we employ a 4-layer 256 width SIREN. The normalization parameters are consistent with those used for CIFAR-10. We increase the training iterations to 3000, each taking 50.24 secs.

**CityScapes-INRs** The Cityscapes dataset focuses on semantic understanding of urban street scenes, requiring exceptionally high image quality for pixel-wise classification. We resize the images from 1024x2048 to 320x640 pixels. To handle the details, we use a 5-layer SIREN model with a width of 256 and 3000 training iterations. The cost for training individual INR is 184.3 secs.

**Omniobject3D-INRs** The Omniobject3D dataset comprises 5998 objects across 190 daily categories. For training, we utilize the official implementation of NeRF, assuming a white background. Our setup includes a range of , 64 samples per ray, and 2048 rays in one batch.

Figure 2: **Examples of images from INRs. We present visual comparisons of example image pairs from our Implicit-Zoo dataset. The original (left/top) and the reconstruction from INRs (right/bottom) images are presented in pairs, showcasing similar visual quality. Please zoom in for details.**Initially, we resize the images from 800x800 to 400x400 pixels and use first 96 views to generate the neural radiance field. The time cost for training a single scene is 1019 seconds.

### Dataset quality control

To account for varying convergence rates across images and to optimize time efficiency, we have developed a third-phase training framework. First, we conduct basic training with a predefined number of iterations and a learning rate scheduler. Second, for data that fails to achieve a PSNR of 30 dB after basic training, we initiate extended training, which allows for up to three times the number of iterations as the basic phase. An early-stopping mechanism is incorporated into the extended training to prevent unnecessary computation.In addition, we conducted a thorough data check after, performing further training on all data that still had not achieved a PSNR of 30. For this round we train till the PSNR threshold is met. Some examples of data is shown in Fig 2.

### Data protection and licence

Implicit-Zoo is constructed using several popular RGB datasets. **CIFAR10**: MIT-License, we are allowed to redistribute under same terms and we will release it on Kaggle. **ImageNet-1K**: Similar to CIFAR we will redistribute INRs on Kaggle, consistent with the original distribution protocols. **Cityscapes**: Non-commercial purposes licence, This dataset will be hosted on the Cityscapes team's official website, where users must agree to the specified terms of use. **Omniobject3D**: CC BY 4.0 licence, we are permitted to redistribute it in our format directly under the same terms.

## 4 Dataset Applications

### Different tasks statement

**Classification:** We provide a dataset of INRs \(\{f_{i}\}_{i=1}^{N}\) and associated labels \(\{y_{i}\}_{i=1}^{N}\), \(y_{i}\{0,1,2...C-1\}\). \(C\) is the total classes number and \(N\) is the size of dataset. The goal is to learn a model \(g:f_{i} y_{i}\) that accurately predicts the label \(y_{i}\) for a given INRs \(f_{i}\). Recall, the RGB values at the 2d coordinates \(x\) is obtained using INRs such that \(v_{x}=f_{i}(x)\).

**Segmentation:** Similar to classification, in segmentation the model needs to predict densely the pixel-wise labels. To achieve this, coordinate querying is crucial for establishing pixel-wise correspondence.

Figure 3: **Illustration of learnable tokenizer.** Instead of retrieve RGB value from images we query learnable coordinates to pre-trained freezed INRs and grouping RGB values to create tokens. Note that during backpropogation the Coordinate \(x\) will also be jointly optimized with ViT modules.

**Pose Regression:** In pose regression, we localize an image \(I\) with respect to the 3D scene represented by its INR \(f(.)\). To do so, we train a neural network \(g:(f,I)\), where \(\) represents the 6D pose parameters. In other words, we directly regress the pose of the image \(I\) using a neural network that leverages the INRs. Thanks to the scale of our dataset, we can train such network which also generalizes beyond the training scenes. Furthermore, our dataset allows to meaningfully learn the tokens' locations. We refer this process as learnable organization, which is presented below.

### Learnable tokenizer

Vision Transformers treat input as sequences of patches  or volumes  - which are also referred as tokens - from _fixed and handcrafted locations_\(x\). Let a token be \(t\) and \(z=(x,v_{x})\) be a tuple of location and the corresponding value. Then for a set \(=\{z_{k}\}_{k=1}^{M}\) of \(M\) tuples, we create the token \(t\) by using a learnable function \(T(.)\) such that \(t=T()\). It is important to note that the _INRs make the location \(x\) learnable with respect to token \(t\)_. Thanks to the differentiability and scalability of our Dataset, we propose to jointly optimize tokenizer by making \(x\) learnable with other ViT modules as shown in Fig 4. In our experiments, we use convolution operation \(T(.)\) as tokenizer. To make this approach effective is challenging. It requires specific _RGB grouping methods_ for both 2D and 3D INRs, _ensuring differentiability_ at every step especially for data augmentation. Please refer to the supplementary for details of our differentiable augmentation scheme.

#### 4.2.1 RGB grouping strategies

ViT use uniform patches of the same size, as shown in Fig. 7(a). In contrast, we propose learnable scaling and learnable center location to handle multi-resolution and allow patches to more important area, shown in Fig 4(b),4(c). Furthermore we propose all pixels coordinate are learnable 5d. All above mentioned coordinates are initialized with the original RGB pixel coordinates. We also investigate the random initialized shown in Fig and 4(e). Corresponding quantitative and qualitative results please refer to 7 and 7.

## 5 The Implicit-Zoo Benchmark and Experimental Results

We report four benchmarks on the Implicit-Zoo Dataset, covering image classification, semantic segmentation, and 3D pose regression tasks. Our focus is on the RGB input space. Additional investigations on methods that use only the weight space and also training details can be found in the supplementary materials.

Figure 4: **Illustration of proposed pose regressor.** We process 3D volume features and 2D image features with transformer-based encoder and output coarse poses. For further refinement, we freeze the 3D INRs and optimize the pose by minimizing the photometric error.

### Benchmark methods

We choose Vision Transformer and its variants as main method for its state of the art performance across multiple tasks and to demonstrate the effectiveness of the learnable tokenizer. For classification we choose  as baseline method and for segmentation we use Segformer for benchmark approach for its efficiency and light-weight design. For pose regression we use method shown in 4.

### Benchmark experiments and results

**CIFAR-10-INRs Classification:** We train ViT classifier from scratch with batch size 512 for 200 epochs. We compare five different grouping methods with the baseline. In addition to the approaches mentioned in Sec. 4.2.1, we found that if all pixels are allowed to move without any constraints, multiple pixels may converge to the closing location due to local minima shown in Fig.7b. To address this, we introduced a regularization term to penalize too close coordinates within the same token \(t\) as shown in Eqn.1. We abbreviate it as "LP+reg". For N coordinates \(\{x_{i}\}_{i=1}^{N}\) within that token, we use \(1\) gate loss and choose the threshold \(\) to be \(min(1/H,1/W)\) where H, W is the height and width

Figure 5: **Different RGB grouping strategies. We visualize the proposed RGB grouping strategies with patch size 3. Coordinates with same color will be grouped into the same token. We abbreviate these approaches as (b) “S”, (c) “LC”, (d) “LP”, and (e) “LP+rand”.**

Figure 6: **Illustration of Proposed 3D Volume Encoder pretraining mechanism We randomly mask out 80% of the volume tokens, allowing the encoder to operate only on the visible tokens. A small decoder processes the full set of encoded visible patches and masked tokens to reconstruct the input color volume and density volume. For qualitative results, please refer to Fig 15**of input images, such that,

\[_{reg}=_{i=1}^{N}_{j=1,j i}^{N}(-\|x_{ i}-x_{j}\|_{2}). \]

Implementing this approach reveals that the proposed "LC" and "LP+reg" methods surprisingly outperform the baseline by 0.51 \(\%\) and 0.75 \(\%\) in accuracy. Conversely, the "LP+rand" method performs poorly, which is consistent with our expectations due to the loss of local geometry and appearance information . As illustrated in Fig. 6(c), boundary patches tend to move towards the center, while center patches tend to overlap. With Non-Close regularization, the coordinates, as shown in Fig. 6(d), retain a grid structure. This helps preserve local geometry information while leave patches the flexibility to grow and overlap. Furthermore, we present experiments conducted with a CNN architecture, as shown in Table 3. CNNs possess a strong inductive bias due to the locality of the convolution operation. By incorporating learnable tokens with regularization, our method also enhances CNN-based approaches, demonstrating the generalization capability of our proposed approach.

**ImageNet-100 INRs Classification:** We conduct fine-tuning experiment similar to  and use the best RGB grouping approaches from previous experiments. After fine-tuning 8000 steps based on re-finetuned from augreg 21k-1k, we report similar performance improve in accuracy in Tab. 5. It is minimal potentially because the pre-trained ViT is based on fixed patches. Additionally, "LC" performs better "LP", which could also be attributed to the pre-training on grid-like tokenization.

**CityScapes-INRs Semantic Segmentation:** Follow experiments in  which use pretrained encoder MiT-B0 on ImageNet-1K and then fine-tune it on Cityscapes-INRs for 40k steps with batch size 16. We report also the Params and GFLOPs for using our learnable tokenizers.

   Method & Params & GFLOPs & mIOU \(\)(fine) & mIOU \(\)(coarse) \\  MiT-B0 MiT-B0 & 3.7 & 31.5 & 39.95 \(\)0.9 & 42.67 \(\)0.8 \\ MiT-B0+LC & 3.7 & 112.5 & 40.33\(\)0.7 & 42.70 \(\)0.6 \\ MiT-B0+LP+Reg & 4.0 & 112.5 & **40.61\(\)0.6** & **43.29 \(\)0.3** \\   

Table 4: **CityScapes-F Segmentation. Results obtained on the Cityscapes fine annotation dataset.**

   Method & Acc \(\) & Precision \(\) & F1 \(\) \\  ViT & 80.82\(\)0.86\% & 80.76\(\) 0.87 \% & 80.75\(\) 0.86 \% \\ ViT + S & 80.24\(\) 0.47\% & 80.49\(\) 0.63\% & 80.44\(\) 0.57\% \\ ViT + LC & 81.33\(\) 0.23\% & 81.29\(\) 0.22\% & 81.30\(\) 0.23\% \\ ViT + LP + rand & 59.43 \(\) 1.21 \% & 59.56 \(\)1.32 \% & 59.65 \(\)1.29\% \\ ViT + LP & 79.51\(\) 0.23\% & 79.37\(\) 0.34\% & 79.37\(\) 0.35\% \\ ViT + LP + Reg & **81.57\(\) 0.29\%** & **81.53\(\) 0.30\%** & **81.51\(\) 0.30\%** \\   

Table 2: **Cifar-10-INRs Classification task. Classification results on our CIFAR-INR dataset with different grouping methods. Abbreviations are introduced in Figure 5. All results are averaged over 5 runs. Baseline results align with results reported using CIFAR-10 images.**

   Method & Acc \(\) & Precision \(\) & F1 \(\) \\  VGG19 & 82.06 \(\) 0.67 \% & 82.11\(\) 0.71 \% & 82.07 \(\) 0.70 \% \\ VG19+LP+Reg & **82.28 \(\) 0.45 \%** & **82.30 \(\) 0.63 \%** & **82.22 \(\) 0.59 \%** \\  ResNet18 & 83.34 \(\) 0.61 \% & 83.72 \(\) 0.67 \% & 83.48 \(\) 0.64 \% \\ ResNet18+LP+Reg & **83.57 \(\) 0.59 \%** & **83.94 \(\) 0.51\%** & **83.71 \(\) 0.55 \%** \\   

Table 3: **Experiments using CNN architecture. We report the results of CNN models using VGG11 and ResNet18. Our learnable token method (which benefits from our dataset) is also applicable for learnable convolutions. Our method improves performance in both CNN architectures.**

[MISSING_PAGE_FAIL:9]

to INR querying, resulting in small batch and model sizes in benchmark experiments, which limits from-scratch training of complex models. We set a PSNR threshold of 30 for dataset expansion, but this may cause artifacts in repetitive backgrounds, thus requires additional data refinement. Lastly, our pose regression task struggles with symmetric objects (Fig. 8f); this may be addressed using symmetry-aware representations . We believe our dataset and its showcased applications open up new avenues for the future research.

## 7 Acknowledgment

We would like to thank the ETH Computer Vision Lab and INSAIT for their invaluable support and resources, which were essential to the success of this work.