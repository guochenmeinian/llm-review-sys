# A Simple Solution for Offline Imitation from Observations and Examples with Possibly Incomplete Trajectories

A Simple Solution for Offline Imitation from Observations and Examples with Possibly Incomplete Trajectories

Kai Yan   Alexander G. Schwing   Yu-Xiong Wang

University of Illinois Urbana-Champaign

{kaiyan3, aschwing, yxw}@illinois.edu

[https://github.com/KaiYan289/TAILO](https://github.com/KaiYan289/TAILO)

###### Abstract

Offline imitation from observations aims to solve MDPs where only _task-specific_ expert states and _task-agnostic_ non-expert state-action pairs are available. Offline imitation is useful in real-world scenarios where arbitrary interactions are costly and expert actions are unavailable. The state-of-the-art 'DIstribution Correction Estimation' (DICE) methods minimize divergence of state occupancy between expert and learner policies and retrieve a policy with weighted behavior cloning; however, their results are unstable when learning from incomplete trajectories, due to a non-robust optimization in the dual domain. To address the issue, in this paper, we propose Trajectory-Aware Imitation Learning from Observations (TAILO). TAILO uses a discounted sum along the future trajectory as the weight for weighted behavior cloning. The terms for the sum are scaled by the output of a discriminator, which aims to identify expert states. Despite simplicity, TAILO works well if there exist trajectories or segments of expert behavior in the task-agnostic data, a common assumption in prior work. In experiments across multiple testbeds, we find TAILO to be more robust and effective, particularly with incomplete trajectories.

## 1 Introduction

In recent years, Reinforcement Learning (RL) has been remarkably successful on a variety of tasks, from games  and robot manipulation  to recommendation systems  and large language model fine-tuning . However, RL often also suffers from the need for extensive interaction with the environment and missing compelling rewards in real-life applications .

To address this, Imitation Learning (IL), where an agent learns from demonstrations, is gaining popularity recently . Offline imitation learning, such as behavior cloning (BC) , allows the agent to learn from existing experience without environment interaction and reward label, which is useful when wrong actions are costly. However, similar to RL, IL also suffers when limited data is available  - which is common as demonstrations of the target task need to be collected every time a new task is addressed. In this work, we consider a special but widely studied  case of expert data shortage in offline IL, i.e., _offline Learning from Observations (LfO)_. In LfO the task-specific data only consists of a few expert trajectories, key frames, or even just the goal (the latter is also known as _example-based IL_), and the dynamics must be mostly learned from _task-agnostic_ data, i.e., demonstration from data not necessarily directly related to the target task. For example, sometimes the agent must learn from experts with different embodiment , where expert actions are not applicable.

In the field of offline LfO, researchers have explored action pseudo-labeling , inverse RL , and similarity-based reward labeling ; for example-based IL, the benchmark is RCE , which uses RL with classifier-labeled reward. Recently, DIstribution Correction Estimation (DICE) methods, LobsDICE  and SMODICE , achieve the state of the art for both offline LFO and example-based IL. Both methods minimize the state visitation frequency (occupancy) divergence between expert and learner policies, and conduct a convex optimization in the dual space.

However, DICE methods are neither robust to incomplete trajectories in the task-agnostic / task-specific data , nor do they excel if the task-agnostic data contains a very small portion of expert trajectories or only segments . These are inherent shortcomings of the DICE-based methods, as they are _indirect_: they first perform optimization in a dual domain (equivalent to finding the value function in RL), and then recover the policy by weighted behavior cloning. Importantly, Kullback-Leibler(KL)-based optimization of dual variables requires complete trajectories in the task-agnostic data to balance all terms in the objective. Note, SMODICE with \(^{2}\)-based optimization is also theoretically problematic (see Appendix C) and empirically struggles on testbeds .

To overcome the shortcomings listed above, we propose a simple but effective method for imitation learning from observations and examples, Trajectory-Aware Imitation Learning from Observations (TAILO). We leverage the common assumption that _there exist trajectories or long segments that are near-optimal to the task of interest in the task-agnostic data_. This assumption is the basis of skill-based learning [20; 46; 47], and the benchmarks of many recent works satiesfy this assumption [36; 40; 41; 56]; one real-life example fulfilling this assumption is robotics: the robot often utilizes overlapping skills such as moving the robotic arm and grabbing items from other tasks to complete the current task. Based on this assumption, we discriminate/identify which state-action pairs could be taken by the expert, and assign large weights for trajectory segments leading to those segments in the downstream Weighted Behavior Cloning (WBC). This is a simple way to make the learned policy _trajectory-aware_. The method only consists of two parametric steps: 1) train a discriminator using positive-unlabeled (PU) learning, and 2) use Weighted Behavior Cloning (WBC) on all state-action pairs in the task-agnostic data with the weights of WBC being a discounted sum over thresholded scores given by the discriminator. Note, the discounted sum propagates large scores to trajectory segments in the past far from expert states, if they lead to expert trajectory segments eventually. Meanwhile, as the task-agnostic data contains both expert and non-expert demonstrations, Positive-Unlabeled (PU) learning is better than plain binary classification. Fig. 1 summarizes our algorithm. We found this simple solution to be surprisingly effective across multiple testbeds, especially if the task-agnostic data contains incomplete trajectories. In this latter case, baselines struggle or even diverge. Moreover, we find our method to also improve if the task-agnostic data contains only a small portion of expert trajectories.

We summarize our contributions as follows: 1) We carefully analyzed the state-of-the-art DICE methods in offline LFO, pointing out their limitations both empirically and theoretically (see Appendix C for details); 2) We propose a simple yet effective solution to offline imitation learning from observations; and 3) We empirically show that this simple method is robust and works better than the state of the art on a variety of settings, including incomplete trajectories, few expert trajectories in the task-agnostic dataset, example-based IL and learning from mismatching dynamics.

Figure 1: An illustration of our method, TAILO. Different trajectories are illustrated by different styles of arrows. TAILO consists of two parametric steps: 1) train a discriminator which gives high \((s)\) for near-expert states and low \(R(s)\) for non-expert states, as shown in panel b); 2) conduct weighted behavior cloning with weights calculated from \(R(s)\) along the trajectory, as shown in panel c). High transparency indicates a small weight for the state and its corresponding action.

Preliminaries

**Markov Decision Process.** A Markov Decision Process (MDP) is a well-established framework for sequential decision-making problems. An MDP is defined by the tuple \((S,A,T,r,)\), where \(S\) is the state space and \(A\) is the action space. For every timestep \(t\) of the Markov process, a state \(s_{t} S\) is given, and an action \(a_{t} A\) is chosen by the agent according to its policy \((a_{t}|s_{t})(A)\), where \((A)\) is the probability simplex over \(A\). Upon executing the action \(a_{t}\), the MDP will transit to a new state \(s_{t+1} S\) according to the transition probability \(T(s_{t+1}|s_{t},a_{t})\) while the agent receives reward \(r(s_{t},a_{t})\). The goal of the agent is to maximize the discounted reward \(_{t}^{t}r(s_{t},a_{t})\) with discount factor \(\) over a complete run, which is called an episode. The state-action pairs) collected through the run are called a state-(action) trajectory \(\). Trajectory segment in this work is defined as a continuous subsequence of a trajectory \(\). The state visitation frequency (_state occupancy_) of a policy \(\) is denoted as \(d^{}(s)=(1-)_{t}^{t}(s_{t}=s)\). See Appendix A for a detailed definition of state occupancy and other occupancies.

**Positive-Unlabeled Learning.** Positive-Unlabeled learning  addresses the problem of binary classification with feature \(x^{n}\) and label \(y\{0,1\}\) when only the positive dataset \(D_{P}\) and the unlabeled dataset \(D_{U}\) are known. Our solution leverages _positive prior_\(_{p}=(y=1)\) and _negative prior_\(_{n}=(y=0)\), which are unknown and treated as a hyperparameter.

**Offline Imitation from Observations / Examples.** Offline imitation learning from observations requires the agent to learn a good policy from two sources of data: one is the _task-specific_ dataset \(D_{}\), which contains state trajectories \(_{}=\{s_{1},s_{2},,s_{n_{1}}\}\) from the expert that directly addresses the task of interest; the other is the _task-agnostic_ dataset \(D_{}\), which contains state-action trajectories \(_{}=\{(s_{1},a_{1}),(s_{2},a_{2}),,(s_{n_{2}},a_{n_{2}})\}\) of unknown optimality to the task of interest. Note that the task-specific trajectory \(_{}\) can be incomplete; specifically, if only the last state exists as an example of success, then it is called _imitation from examples_.

The state of the art methods in this field are SMODICE  and LobsDICE . SMODICE minimizes the divergence between the state occupancy from task-specific data \(d^{}\) and the learner policy \(\)'s occupancy \(d^{}\); for example, when using a KL-divergence as the metric, the objective is

\[_{}(d^{}(s)\|d^{}(s)), \]

However, since the task-agnostic dataset is the only source of correspondence between state and action, the state occupancy of the task-agnostic dataset \(d^{}\) must be introduced. With some derivations and relaxations, the objective is rewritten as

\[_{}_{s d^{}}}(s)}{d^{}( s)}-(d^{}(s)\|d^{}(s)), \]

Here, the first term \(R(s)=}(s)}{d^{}(s)}\) is an indicator for the importance of the state; high \(R(s)\) means that the expert often visits state \(s\), and \(s\) is a desirable state. Such \(R(s)\) can be trained by a discriminator \(c(s)\): a positive dataset \(D_{}\) (label \(1\)) and a negative dataset \(D_{}\) (label \(0\)) are used to find an 'optimal' discriminator \(c=c^{*}\). Given this discriminator, we have \(R(s)=(s)}{1-c^{*}(s)}\). SMODICE then converts the constrained Eq. (2) to its unconstrained Lagrangian dual form with dual variable \(V(s)\), and optimizes the following objective (assuming KL-divergence as the metric):

\[_{V}(1-)_{s p_{0}}[V(s)]+_{(s,a,s^{ }) D_{}}[R(s)+ V(s^{})-V(s)], \]

where \(\) is the discount factor and \(p_{0}\) is the distribution of the initial state in the MDP. In this formulation, \(R(s)\) can be regarded as the _reward_ function, while \(V(s)\) is the value function. The whole objective is an optimization of a convex function with respect to the Bellman residual. With \(V(s)\) learned, the policy is retrieved via weighted behavior cloning where the coefficient is determined by \(V(s)\). LobsDICE is in spirit similar, but considers the occupancy of _adjacent state pairs_ instead of a single state.

Methodology

### Motivation and Overview

As mentioned in Sec. 2, the DICE methods for offline LfO discussed above consist of three parts: reward generation, optimization of the value function \(V(s)\), and weighted behavior cloning. Such a pipeline can be unstable for two reasons. First, the method is _indirect_, as the weight for behavior cloning depends on the learned value function \(V(s)\), which could be inaccurate if the task-agnostic dataset is noisy or is not very related to the task of interest. This is aggravated by the fact that \(V(s^{})\) as a 1-sample estimation of \(_{s^{} p(s^{}|s,a)}V(s^{})\) and logsumero are used in the objective, which further destabilizes learning. Second, for KL-based metrics, if no state appears twice in the task-agnostic data, which is common for high-dimensional environments, the derivative of the objective with respect to \(V(s)\) is determined by the initial state term, the current state term \(-V(s)\), and the next state term \( V(s^{})\). Thus, if a non-initial state \(s^{}\) is missing from the trajectory, then only \(- V(s^{})\) remains, which makes the objective monotonic with respect to the unconstrained \(V(s^{})\). Consequently, \(V\) diverges for \(s^{}\) (see Appendix C in the for a more detailed analysis and visualization in Fig. 7).

To address the stability issue, we develop Trajectory-Aware Imitation Learning from Observations (TAILO). TAILO also seeks to find an approximate reward \(R(s)\) by leveraging the discriminator \(c(s)\), which is empirically a good metric for optimality of the state. However, compared to DICE methods which determine the weight of behavior cloning via a dual program, we adopt a much simpler idea: find 'good' trajectory segments using the discounted sum of future \(R(s)\) along the trajectory following state \(s\), and encourage the agent to follow them in the downstream weighted BC. To do so, we assign a much larger weight, a thresholding result of the discounted sum, to the state-action pairs for 'good' segments. Meanwhile, small weights on other segments serve as a regularizer of pessimism . Such a method is robust to missing steps in the trajectory. Empirically we find that the weight need not be very accurate for TAILO to succeed. In the remainder of the section, we discuss the two steps of TAILO: 1) training a discriminator to obtain \(R(s)\) (Sec. 3.2), and 2) thresholding over discounted sums of \(R(s)\) along the trajectory (Sec. 3.3).

### Positive-Unlabeled Discriminator

Following DICE, \(R(s)\) is used as a metric for state optimality, and is obtained by training a discriminator \(c(s)\). However, different from DICE which regards all unlabeled data as negative samples, we use Positive-Unlabeled (PU) learning to train \(c(s)\), since there often are some expert trajectories or segments of useful trajectories in the task-agnostic dataset. Consequently, we treat the task-agnostic dataset as an _unlabeled_ dataset with both positive samples (expert of the task of interest) and varied negative samples (non-expert).

Our training of \(c(s)\) consists of two steps: 1) training another discriminator \(c^{}(s)\) that identifies safe negative samples, and 2) formal training of \(c(s)\). In the first step, to alleviate the issue of treating positive samples from \(D_{}\) as negatives, we use a debiasing objective  for the training of \(c^{}(s)\) (see Appendix A for a detailed derivation):

\[_{c^{}(s)}L_{1}(c^{})=_{c^{}(s)}-[_{p} _{s D_{}} c^{}(s)+(0,_{s D_{ }}(1-c^{}(s))-_{p}_{s D_{}}(1-c^{ }(s)))], \]

where the bias comes from viewing unlabeled samples as negative samples. Here, positive class prior \(_{p}\) is a hyperparameter; in experiments, we find results to not be sensitive to this hyperparameter.

In the second step, after \(c^{}(s)\) is trained, \(R^{}(s)=(s)}{1-c^{}(s)}\) is calculated for each state in the task-agnostic data, and the states in the (possibly incomplete) trajectories with the least \(_{1}(0,1)\) portion of average \(R^{}(s)\) are identified as "safe" negative samples. Note, we do not identify "safe" positive samples, because a trajectory that only has a segment useful for the task of interest might not have the highest average \(R^{}(s)\) due to its irrelevant part; however, an irrelevant trajectory will probably have the lowest \(R^{}(s)\) throughout the whole trajectory, and thus can be identified as a "safe" negative sample. We collect such samples to form a new dataset \(D_{}\).

Finally, the formal training of \(c(s)\) uses states from \(D_{}\) as positive samples and \(D_{}\) as "safe" negative samples. The training objective of \(c(s)\) is a combination of debiasing objective and standard cross entropy loss for binary classification, controlled by hyperparameter \(_{2}\). Specifically, we use \[_{c(s)}_{2}L_{2}(c)+(1-_{2})L_{3}(c),\] \[L_{2}(c) =_{c(s)}-[_{p}_{s D_{}} c(s)+ (0,_{s D_{}}(1-c(s))-_{p}_{ s D_{}}(1-c(s)))],\] \[L_{3}(c) =_{s D_{}} c(s)+_{s D_{ }}(1-c(s)). \]

In this work, we only consider \(_{2}\{0,1\}\); empirically, we found that \(_{2}=0\) is better if the agent's embodiments across \(D_{}\) and \(D_{}\) are the same, and \(_{2}=1\) is better if the embodiments differ. This is because a debiasing objective assumes positive samples still exist in the safe negatives, which pushes the classification margin further from samples in \(D_{}\), and has a larger probability to classify expert segments in \(D_{}\) as positive.

### Trajectory-Aware Thresholding

With the discriminator \(c(s)\) trained and \(R(s)=\) obtained, we can identify the most useful trajectory segments for our task. To do this, we use a simple thresholding which makes the weight employed in behavior cloning trajectory-aware. Formally, the weight \(W(s_{i},a_{i})\) for weighted behavior cloning is calculated as

\[W(s_{i},a_{i})=_{j=0}^{}^{j}( R(s_{i+j})), \]

where \((s_{i},a_{i})\) is the \(i\)-th step in a trajectory, and \(\) is a hyperparameter that controls the strength of thresholding; \(\) balances the tradeoff between excluding non-expert and including expert-trajectories. For an \(i+j\) which exceeds the length of the trajectory, we set \(s_{i+j}\) to be the last state of the (possibly incomplete) known trajectory, as the final state is of significant importance in many applications . This design also allows us to conveniently address the example-based offline IL problem, where the final state is important. With the weights determined, we finally conduct a weighted behavior cloning with the objective \(_{}_{(s,a) D_{}}W(s,a)(a|s)\), where \((a|s)\) is the desired policy.

## 4 Experiments

In this section, we evaluate TAILO on five different, challenging tasks across multiple mujoco testbeds. More specifically, we study the following two questions: 1) Is the algorithm indeed robust to incomplete trajectories in either task-agnostic (Sec. 4.1) or task-specific (Sec. 4.2) data, and does it work with little expert data in the task-agnostic dataset (Sec. 4.3)? 2) Can the algorithm also work well in example-based IL (Sec. 4.4) and learn from experts of different dynamics (Sec. 4.5)?

**Baselines.** We compare TAILO to four baselines: SMODICE , LobsDICE , ORIL , and Behavior Cloning (BC). Since LobsDICE works with state-pair occupancy, it cannot solve example-based IL; thus, we substitute LobsDICE in example-based IL with RCE , a state-of-the-art example-based RL method. Unless otherwise specified, we use 3 random seeds per method in each scenario.

**Environment Setup.** Following SMODICE , unless otherwise specified, we test our algorithm on four standard mujoco testbeds from the OpenAI Gym , which are the hopper, halfcheetah, ant, and walker2d environment. We use normalized average reward1 as the main metric, where higher reward indicates better performance; for environments where the final reward is similar, fewer gradient steps in weighted behavior cloning indicates better performance. We report the change of the mean and standard deviation of reward with respect to the number of gradient steps.

**Experimental Setup.** For all environments, we use \(=1.25,_{1}=0.8,_{p}=0.2\); \(_{2}=1\) if \(D_{}\) and \(D_{}\) are generated by different embodiments, and \(_{2}=0\) otherwise. We use \(=0.998\) unless otherwise specified. We use an exactly identical discriminator and policy network as SMODICE: for the discriminator \(c(s)\) and \(c^{}(s)\), we use a small Multi-Layer Perceptron (MLP) with two hidden layers, width \(256\), and tanh activation function. For actor \(\), we use an MLP with two hidden layers, width \(256\), and ReLU  activation function. For the training of \(c(s)\) and \(c^{}(s)\), we use a learning rate of \(0.0003\) and a 1-Lipschitz regularizer, run \(10\)K gradient steps for \(c^{}(s)\), and run \(40\)K gradient steps for \(c(s)\). For weighted BC, we use a learning rate of \(10^{-4}\), a weight decay of \(10^{-5}\), and run \(1\)M gradient steps. For discriminator training, we use a batch size of \(512\); for weighted BC steps, we use a batch size of \(8192\). Adam optimizer  is used for both steps. See Appendix D for more details and Appendix F for a sensitivity analysis regarding the batch size, \(,_{1},_{2},_{p}\), and \(\).

### Learning from Task-Agnostic Dataset with Incomplete Trajectories

**Dataset Setup.** We modify the standard dataset settings from SMODICE to create our dataset. SMODICE uses offline datasets from D4RL , where a single trajectory from the "expert-v2" dataset is used as the task-specific data. The task-agnostic data consists of \(200\) expert trajectories (\(200\)K steps) from the "expert-v2" dataset and 1M steps from the "random-v2" dataset. Based on this, we iterate over the state-action pairs in the task-agnostic dataset, and remove one pair for every \(x\) pairs. In this work, we test \(x\{2,3,5,10,20\}\).

**Main Results.** Fig. 2 shows the result for different methods with incomplete task-agnostic trajectories, where our method outperforms all baselines and remains largely stable despite decrease of \(x\) (i.e., increase of removed data), as the weights for each state-action pair do not change much. In the training process, we often witness SMODICE and LobsDICE to collapse due to diverging value functions (see Appendix C for explanation), which is expected; for runs that abort due to numerical error, we use a reward of \(0\) for the rest of the gradient steps. Under a few cases, SMODICE with KL-divergence works decently well with larger noises (e.g., Halfcheetah_1/3 and Walker2d_1/3); this is because sometimes the smoothing effect of the neural network mitigates divergence. However, with larger batch size (See Fig. 23 in Appendix F.4.6) and more frequent and uniform updates on each data point, the larger the noise the harder SMODICE fails.

### Learning from Task-Specific Dataset with Incomplete Trajectories

**Dataset Setup and Main Results.** We use SMODICE's task-agnostic dataset as described in Sec. 4.1. For the task-specific dataset, we only use the first \(x\) steps and the last

Figure 2: Reward curves for offline imitation learning with incomplete trajectories in the task-agnostic dataset, where the x-axis is the number of gradient steps and the y-axis is the normalized reward. The title for each sub-figure is in the format of “environment name”+\(1/x\)” (task-agnostic data removed), where \(x\{2,3,5,10,20\}\). We observe the proposed method to be the most stable.

steps in the expert trajectory, and discard the remainder. In this work, we test \((x,y)\{(1,100),(10,90),(50,50),(90,10),(100,1)\}\). Fig. 3 shows the result with incomplete task-specific trajectories, where our method outperforms all baselines and often achieves results similar to those obtained when using the entire task-specific dataset. In contrast, SMODICE and LobsDICE are expectedly unstable in this setting.

### Standard Offline Imitation Learning from Observations

**Environment Setup.** In addition to the four standard mujoco environments specified above, we also test on two more challenging environments: the Franka kitchen environment and the antmaze environment from D4RL . In the former, the agent needs to control a 9-DoF robot arm to complete a sequence of item manipulation subtasks, such as moving the kettle or opening the microwave; in the latter, the agent needs to control a robot and to crawl through a U-shaped maze and get to a particular location. As the kitchen environment requires less steps to finish, we use \(=0.98\) instead of \(0.998\).

**Dataset Setup.** As existing methods already solve the four mujoco environments with SMODICE's dataset settings in Sec. 4.1 quite well (see Appendix F for result), we test a more difficult setting to demonstrate TAILO's ability to work well with _few expert trajectories in the task-agnostic dataset_. More specifically, we use the same task-specific dataset, but only use \(40\) instead of \(200\) expert trajectories from the "expert-v2" dataset to mix with the 1M "random-v2" steps data and form the task-agnostic dataset. For the more challenging kitchen and antmaze environment, we use the identical dataset as SMODICE, where a single trajectory is used as the task-specific dataset. The task-agnostic dataset for the kitchen environment consists of expert trajectories completing different subtasks (both relevant and irrelevant) in different orders, and the task-agnostic dataset for antmaze consists of data with varied optimality. See Appendix D for details.

**Main Results.** Fig. 4 shows the result for different methods in standard offline imitation learning from observations. We find our method to outperform all baselines on hopper, halfcheetah, ant and walker2d. Results are comparable to the best baseline on kitchen and antmaze. In the experiment, we found ORIL to often diverge, and the performance of SMODICE varies greatly depending on the \(f\)-divergence: on hopper, halfcheetah and walker2d, KL-divergence is much better, while \(^{2}\)

Figure 3: Reward curves for offline imitation with incomplete trajectories in the task-specific dataset. The title format for each subfigure is “environment name”+head \(x\)+\(x\)+“tail \(y\)” (states remained), where \((x,y)\{(100,1),(90,10),(50,50),(10,90),(1,100)\}\). We observe the proposed method to be the most stable.

divergence is better on ant and kitchen. LobsDICE is marginally better than SMODICE, as the former considers state-pair occupancy instead of single state occupancy, which is more informative. Also worth noting: none of the methods exceeds a normalized reward of \(60\) in the kitchen environment; this is because the SMODICE experiment uses expert trajectories that are only expert for the first 2 out of all 4 subtasks. See Sec. F.3 in the Appendix for a detailed discussion.

### Learning from Examples

**Environment Setup.** Following SMODICE, we test example-based offline imitation in three different testbeds: pointmaze, kitchen and antmaze. In the mujoco-based  pointmaze environment the agent needs to control a pointmass on a 2D plane to navigate to a particular direction. For the kitchen environment, we test two different settings where the agent is given successful examples of moving the kettle and opening the microwave respectively (denoted as "kitchen-kettle" and "kitchen-microwave"). As pointmaze requires less steps to finish, we use \(=0.98\) instead of \(0.998\).

**Dataset Setup and Main Results.** Following SMODICE, we use a small set of success examples: \( 200\) states for antmaze and pointmaze and \(500\) states for kitchen (see Appendix D for details) are given as the task-specific dataset. For pointmaze, the task-agnostic data contains \(60\)K steps, generated by a script and distributed evenly along four directions (i.e., \(25\%\) expert data); for other testbeds, the task-agnostic data is identical to SMODICE described in Sec. 4.1. Fig. 5 shows the results for offline imitation learning from examples on all environments tested in SMODICE; TAILO is marginally better than the baselines on pointmaze, antmaze, and kitchen-microwave, and is comparable (all close to perfect) on kitchen-kettle.

### Learning from Mismatched Dynamics

**Environment and Dataset Setup.** Following SMODICE, we test our method on three environments: antmaze, halfcheetah, and ant. We use task-specific data from an expert with different dynamics (e.g., ant with a leg crippled; see Sec. E for details). Task-agnostic data follows SMODICE in Sec. 4.1.

**Main Results.** Fig. 6 shows the results for offline imitation learning from mismatched dynamics, where our method is the best in all three testbeds. Among the three environments, halfcheetah is the most difficult, as the state space for halfcheetah with shorter torso is unreachable by a normal

Figure 4: Reward curves for offline imitation learning from observations with few expert trajectories in the task-agnostic dataset. In all six environments, our method either outperforms or is comparable to the baselines.

Figure 5: Reward curves for offline imitation learning from examples.

halfcheetah, i.e., the assumption that \(d^{}(s)>0\) wherever \(d^{}(s)>0\) in SMODICE and LobsDICE does not hold; in such a setting, our method is much more robust than DICE methods.

## 5 Related Work

**Offline Imitation Learning and Distribution Correction Estimation (DICE).** Offline imitation learning aims to learn a policy only from data without interaction with the environment. This is useful where immature actions are costly, e.g., in a dangerous factory. The simplest solution for offline imitation learning is plain Behavior Cloning (BC) . Many more methods have been proposed recently, such as BCO  and VMSR  (which pseudolabel actions), offline extensions of GAIL , and similarity-based reward labeling . Currently, the state-of-the-art method for offline imitation learning is DDistribution Correction Estimation (DICE) , which minimizes the discrepancy between an expert's and a learner's state, state-action, or state-pair occupancy. Our method is inspired by DICE, but is much simpler and more effective. More recently, two works unify offline imitation learning with offline RL, which are offline-RL-based MAHALO  and DICE-based ReCOIL . Different from SMODICE and LobsDICE, ReCOIL minimizes the divergence between learner and expert data mixed with non-expert data respectively, which removes the data coverage assumption. However, ReCOIL faces the problem discussed in Sec. 3.1 when dealing with incomplete trajectories, and MAHALO is based on state-pairs similar to LobsDICE, which cannot solve IL with incomplete trajectories or example-based IL like our TAILO.

**Learning from Observations and Examples.** Learning from observations (LfO)  requires the agent to learn from a task-specific dataset without expert action, which is useful when learning from videos  or experts with different embodiments , as the expert action is either unavailable or not applicable. Learning from examples is an extreme case of LfO where only the final goal is given . There are three major directions: 1) pseudolabeling of actions which builds an inverse dynamic model and predicts the missing action ; 2) occupancy divergence minimization with either DICE  or inverse-RL style iterative update ; and 3) RL/planning with reward assignment based on state similarity (often in visual imitation) . Our proposed TAILO solves LfO with a simple solution different from existing ones.

**Discriminator as Reward.** The idea of training a discriminator to provide rewards for states is widely used in IL, including inverse RL methods , DICE methods , and methods such as 2IWIL  and DWBC . In this work, we propose a simple but explicit way to take the trajectory context into account, using the output from a discriminator as reward, which differs from prior works.

**Positive-Unlabeled Learning.** Positive-Unlabeled (PU)  learning aims to solve binary classification tasks where only positive and unlabeled data are available. It is widely used in data retrieval , outlier detection , recommendation  and control tasks . In this work, we utilize two PU learning achievements: the skill of identifying "safe" negative samples  and debiasing . The closest RL work to our use of PU learning is ORIL , which also uses positive-unlabeled learning to train a discriminator for reward estimation. However, there are three key differences between our method and ORIL: we define \(R(s)\) differently for better thresholding, use different techniques for PU learning to prevent overfitting, and, importantly, the removal of value function learning. See Appendix F.6 for an ablation to demonstrate efficacy of these changes.

**Reward-Weighted Regression(RWR)  and Advantage-Weighted Regression(AWR) .** The idea in our Eq. (6) of weighted behavior cloning with weights being (often exponentiated) discounted return has been widely used in the RL community , and our objective resembles that of

Figure 6: Reward curves for offline imitation learning from mismatched dynamics.

RWR/AWR. However, our work differs from RWR/AWR inspired works [30; 43; 49] in the following aspects:

* The objective for both AWR and RWR are built upon the _related payoff procedure_[10; 21], which introduces an Expectation-Maximization (EM) procedure for RL. However, for offline IL in our case, iteration between E-step and M-step are infeasible. There are two workarounds for this: importance sampling and naively using one iteration. However, the former is known to be non-robust  and the latter, MARWIL , struggles in our testbeds (see Appendix F.2).
* We use neither an adaptive reward scaling term  in the EM framework nor parametric value estimation [45; 62], which are both widely utilized by RWR/AWR inspired works. However, the former is not guaranteed to preserve an optimal policy and thus is not an advantage , while the latter struggles in our setting where learning a good value function is hard, as illustrated by the performance of baselines such as DICE methods and MARWIL.
* For all existing RWR/AWR works, the reward labels are assumed to be available, which is different from our case.

## 6 Conclusion

We propose TAILO, a simple yet effective solution for offline imitation from observations by training a discriminator using PU learning, applying a score to each state-action pair, and then conducting a weighted behavior cloning with a discounted sum over thresholded scores along the future trajectory to obtain the weight. We found our method to improve upon state-of-the-art baselines, especially when the trajectories are incomplete or the expert trajectories in the task-agnostic data are few.

**Societal Impact.** Our work addresses sequential decision-making tasks from expert observations with fewer related data, which makes data-driven automation more applicable. This, however, could also lead to negative impacts on society by impacting jobs.

**Limitations and Future Directions.** Our method relies on the assumption that there exist expert trajectories or at least segments in the task-agnostic data. While this is reasonable for many applications, like all prior work with proprioceptive states, it is limited in generalizability. Thus, one promising future direction is to improve the ability to summarize abstract "skills" from data with better generalizability. Another future direction is learning from video demonstrations, which is a major real-world application for imitation learning from observations. Also, our experiments are based on simulated environments such as D4RL. Thus a gap between our work and real-life progress remains. While we are following the settings of many recent works, such as SMODICE , ReCOIL  and OTR , to bridge the gap using techniques such as sim2real in the robotics community  is another very important direction for future work.

**Acknowledgements.** This work was supported in part by NSF under Grants 2008387, 2045586, 2106825, MRI 1725729, NIFA award 2020-67021-32799, the Jump ARCHES endowment through the Health Care Engineering Systems Center, the National Center for Supercomputing Applications (NCSA) at the University of Illinois at Urbana-Champaign through the NCSA Fellows program, the IBM-Illinois Discovery Accelerator Institute, and the Amazon Research Award.