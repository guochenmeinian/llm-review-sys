# What Planning Problems Can

A Relational Neural Network Solve?

Jiayuan Mao Tomas Lozano-Perez Joshua B. Tenenbaum Leslie Pack Kaelbling

MIT Computer Science & Artificial Intelligence Laboratory

MIT Department of Brain and Cognitive Sciences

Center for Brains, Minds and Machines

###### Abstract

Goal-conditioned policies are generally understood to be "feed-forward" circuits, in the form of neural networks that map from the current state and the goal specification to the next action to take. However, under what circumstances such a policy can be learned and how efficient the policy will be are not well understood. In this paper, we present a circuit complexity analysis for relational neural networks (such as graph neural networks and transformers) representing policies for planning problems, by drawing connections with serialized goal regression search (S-GRS). We show that there are three general classes of planning problems, in terms of the growth of circuit width and depth as a function of the number of objects and planning horizon, providing constructive proofs. We also illustrate the utility of this analysis for designing neural networks for policy learning.

## 1 Introduction

Goal-conditioned policies are generally understood to be "feed-forward" circuits, in the form of neural networks such as multi-layer perceptrons (MLPs) or transformers . They take a representation of the current world state and the goal as input, and generate an action as output. Previous work has proposed methods for learning such policies for particular problems via imitation or reinforcement learning ; recently, others have tried probing current large-language models to understand the extent to which they already embody policies for a wide variety of problems .

However, a major theoretical challenge remains. In general, we understand that planning problems are PSPACE-hard with respect to the size of the state space , but there seem to exist efficient (possibly suboptimal) policies for many problems such as block stacking  that generalize to arbitrarily sized problem instances. In this paper, we seek to understand and clarify the circuit complexity of goal-conditioned policies for classes of "classical" discrete planning problems: under what circumstances can a polynomial-sized policy circuit be constructed, and what is its size? Specifically, we highlight a relationship between a problem hardness measure (_regression width_, related to the notion of "width" in the forward planning literature ) and circuit complexity. We concretely provide upper bounds on the policy circuit complexity as a function of the problem's regression width, using constructive proofs that yield algorithms for directly compiling a planning problem description into a goal-conditioned policy. We further show that such policies can be learned with conventional policy gradient methods from environment interactions only, with the theoretical results predicting the necessary size of the network.

There are several useful implications of these results. First, by analyzing concrete planning domains, we show that for many domains, there do exist simple policies that will generalize to problem instances with an arbitrary number of objects. Second, our theory predicts the circuit complexity of policies for these problems. Finally, our analysis suggests insights into why certain planningproblems, such as Sokoban and general task and motion planning (TAMP) problems, are hard, and likely cannot be solved _in general_ by fixed-sized MLP or transformer-based policies (Culberson, 1997, Vega-Brown and Roy, 2020). In the rest of the paper, Section 2 provides problem definitions, Sections 3 and 4 provides complexity definitions, theoretical results, and the policy compilation algorithm; Section 5 discusses the practical implications of these results.

## 2 Preliminaries: Planning Domain and Problem

Throughout this paper, we focus on analyzing the search complexity and policy circuit complexity of classical planning problems. Importantly, these planning problems have an object-centric representation and sparsity in the transition models: the state of the world is represented as a set of entities and their properties and relationships, while each action only changes a sparse set of properties and relations of a few objects. These features will contribute to search efficiency.

Formally, we consider the problem of planning and policy learning for a space \(\) of world states. A planning problem is a tuple \(,s_{0},,,\), where \(s_{0}\) is the initial state, \(\) is a goal specification, \(\) is a set of actions that the agent can execute, and \(\) is a (possibly partial) environmental transition model \(:\). The task of planning is to output a sequence of actions \(\) in which the terminal state \(s_{T}\) induced by applying \(a_{i}\) sequentially following \(\) satisfies \(s_{T}\). The task of policy learning is to find a function \((s,)\) that maps from the state and a goal specification to the next action so that applying \(\) recurrently on \(s_{0}\) eventually yields a state \(s_{T}\).

We use atomic STRIPS (Fikes and Nilsson, 1971, Lifschitz, 1987) to formalize the object-centric factorization of our planning problems. Specifically, as illustrated in Fig. 0(a), the environmental state can be represented as a tuple \((_{s},_{s})\), where \(_{s}\) is the set of objects in state \(s\), denoted by arbitrary unique names (e.g., A, B). The second component, \(_{s}\), is a set of _atoms_: _handfree_(), _on-table_(A), _on_(C, B), etc. Each atom contains a predicate name (e.g., _on_) and a list of arguments (e.g., C, B). All atoms in \(_{s}\) are true at state \(s\), while any atoms not in \(_{s}\) are false. Since we do not consider object creation and deletion, we simply use \(_{0}\) to denote the set of all possible atoms for the object universe \(_{s_{0}}\).

Illustrated in Fig. 0(b), a (possibly partial) transition model \(\) is specified in terms of a set of object-parameterized action schemas (_name_, _args_, _precond_, _effect_), where _name_ is the name of the action, _args_ is a list of symbols, _precond_ and _effect_ are descriptions of the action's effects. In the basic STRIPS formulation, both _precond_ and _effect_ are sets of atoms with variables in _args_, and _effect_ is further decomposed into _eff_+ and _eff_-, denoting "add" effects and "delete" effects. An action schema (e.g., _unstack_) can be grounded into a concrete action \(a\) (e.g., _unstack_(D, C)) by binding all variables in _args_.

Figure 1: (a) Illustration of the Blocks World domain that we will be using as the main example. (b) The action schema definition in Blocks World. _clear_(_x_) means there is no object on \(x\). (c) A backward search tree for solving the goal _clear_(B). (d) A serialized goal regression search tree for the same goal. (e) The form of a goal-conditioned policy for this problem.

```
functionawp(\(s_{0}\), \(\), \(goal\_set\)) if\(goal\_set s_{0}\)thenreturnempty_list() possible_t = empty_set() for\(a\{a goal\_set_{-}(a)=0\)and\(goal\_set_{+}\}\)do \(new\_goals=goal\_set(a)_{+}(a)\)if\(new\_goals goal\ stack\)thencontinue \(=(s_{0},)\) if\(\)then \(possible\_t\_add(+\{a\})\) returnshortest path in \(possible\_t\)
```

**Algorithm 1** Plain backward search.

The formal semantics of STRIPS actions is: \( s. a.pre(a) s((s,a)=s _{+}(a)_{-}(a)).\) That is, for any state \(s\) and any action \(a\), if the preconditions of \(a\) are satisfied in \(s\), the state resulting from applying \(a\) to \(s\) will be \(s_{+}(a)_{-}(a)\). Note that \(\) may not be defined for all \((s,a)\) pairs. Furthermore, we will only consider cases where the goal specification is a single atom, although more complicated conjunctive goals or goals that involve existential quantifiers can be easily supported by introducing an additional "goal-achieve" action that lists all atoms in the original goal as its precondition. From now on, we will use \(g\) to denote the single goal atom of the problem.

## 3 Goal Regression Search and Width

A simple and effective way to solve STRIPS problems is backward search2. Illustrated in Algorithm 13, we start from the goal \([g]\), and search for the last action \(a\) that can be applied to achieve the goal (i.e., \(g_{+}(a)\)). Then, we add all the preconditions of action \(a\) to our search target and recurse. To avoid infinite loops, we also keep track of a "_goal stack_" variable composed of all \(goal\_set\)'s that have appeared in the depth-first search stack. The run-time of this algorithm has critical dependencies on (1) the number of steps in the resulting plan and (2) the number of atoms in the intermediate goal sets in search. In general, it has a worst-case time complexity exponential in the number of atoms.

```
functions-g(\(s_{0}\), \(\), \(g\), \(cons\)) if\(g s_{0}\)thenreturn\((s_{0})\) possible_t = empty_set() for\(r\{r cons_{-}(action(r))=\)and\(goal(r)=g\}\)do \(p_{1},p_{2},,p_{k}\)  if\( r_{i}\) s.t. \(p_{i} goal\ stack\)thencontinue for\(i\) in \(1,2,,k\)do \(new\_c=cons\{p_{1},,p_{i-1}\}\) \(_{i}=\) s-g(\(s_{i-1}\), \(p_{i}\), \(new\_c\)) if\(_{i}==\)thenbreak \(s_{i}=(s_{i-1},_{i}[-1])\) if\(_{k}\)then \(possible\_t\_add(_{1}++_{k}+\{a\})\) returnshortest path in \(possible\_t\)
```

**Algorithm 2** Serialized goal regression search.

### Serialized Goal Regression Search

One possible way to speed up backward search is to _serialize_ the subgoals in the goal set: that is, to commit to achieving them in a particular order. In the following sections, we study a variant of the plain backward search algorithm, namely _serialized goal regression search_ (S-GRS, Algorithm 2), which uses a sequential decomposition of the preconditions: given the goal atom \(g\), it searches for the last action \(a\) and also an order in which the preconditions of \(a\) should be accomplished. Of course, this method cannot change the worst-case complexity of the problem (actually, it may slow down the algorithm in cases where we need to search through many orders of the preconditions), but there are useful subclasses of planning problems for which it can achieve a substantial speed-up.

A _serialized goal regression rule_ in \(\), informally, takes the following form: \(g_{1},p_{2},,p_{k} a\). It reads: in order to achieve \(g\), which is a single atom, we need to achieve atoms \(p_{1},,p_{k}\) sequentially, and then execute action \(a\). To formally define the notion of achieving atoms \(p_{1},,p_{k}\) "_sequentially_," we must include constraints in goal regression rules. Formally, we consider the application of a rule under constraint \(c\) to have the following definition, \(g^{c}_{1}^{c},p_{2}^{c\{p_{1}\}},,p_{k}^{c\{p _{1},,p_{k-1}\}} a\) which reads: in order to achieve \(g\) without deleting atoms in set \(c\), we can first achieve \(p_{1}\) while maintaining \(c\), then achieve \(p_{2}\) while maintaining \(c\) and \(p_{1}\), \(\), until we have achieved \(p_{k}\). Finally, we perform \(a\) to achieve \(g\). For example, in Blocks World, we have the rule: \(()^{}(, ),()^{(,)}, ()^{(,),()} (,)\). Note that many rules might be infeasible for a given state, because it is simply not possible to achieve \(p_{i}\) while maintaining \(c\) and \(\{p_{1},,p_{i-1}\}\). In this case, achieving _handsfree_() before _clear_(A) is infeasible because _handsfree_() may be falsified while achieving _clear_(A), if we need to move other blocks that are currently on A. Similarly, _on_(A, B) must be achieved before _clear_(A) because moving A onto B will break _clear_(A) (because we need to pick up A).

The set of all possible goal regression rules can be instantiated based on all ground actions \(\) in a planning problem. Specifically, for a given atom \(g\) (e.g., _holding_(A)) and a set of constraints \(c\), and for each action \(a\) (e.g., _pick-table_(A)), if \(g_{+}(a)\) while \(c_{-}(a)=\), then for any permutation of \((a)\), there will be a goal regression rule: \(g^{c} p_{1},,p_{k} a\), where \(p_{1},,p_{k}\) is a permutation of \((a)\). We call this regression rule set \(_{0}\).

Given a set of regression rules (e.g., \(_{0}\)), we can apply the serialized goal-regression search (S-GRS) algorithm, shown in Algorithm 2. S-GRS returns a shortest path from _state_ to achieve _goal_ while maintaining _cons_. Unfortunately, this algorithm is not optimal nor even complete (e.g., a sequence of preconditions may not be "serializable," which we will define formally later), in the general case. To make it complete, it is necessary to backtrack through different action sequences to achieve each subgoal, which increases time complexity. We include a discussion in Appendix A.1.

### Serialization of Goal Regression Rules

Although Algorithm 2 is not complete in general, it provides insights about goal regression. In the following, we will introduce two properties of planning problems such that S-GRS becomes optimal, complete, and efficient. For brevity, we define \((,,)\) as the set of optimal trajectories that achieve _goal_, from _state_, while maintaining _cons_. Here, _goal_ can be a conjunction.

**Definition 3.1** (Optimal serializability).: _A goal regression rule \(g^{} p_{1},,p_{k} a\) is optimally serializable w.r.t. a state \(s\) if and only if, for all steps \(i\), if \(_{ci}(s,p_{ci} p_{i-1}, )\) and \(_{i}((s,_{1}),p_{i}, \{p_{1},,p_{i-1}\})\) then \(_{ci}_{i}(s,p_{1} p_{i}, )\). Furthermore, \((s,p_{1} p_{k},)\), \( a(s,g,)\)8._

Intuitively, a rule is optimally serializable if any optimal plan for the length \(i-1\) prefix of its preconditions can be extended into an optimal plan for achieving the length \(i\) prefix. For example, in Blocks World, the rule _holding_(A) \(\)_on_(A, B), _clear_(A), _handsfree_() \(\)_unstack_(A, B) is optimally serializable when _on_(A, B) is true for \(s\). We define the set of optimally serializable rules for a state \(s\) as \((s)\), and the set of single-literal goals that can be solved with \(()\) from \(s_{0}\) as \((s_{0})\)$.

**Theorem 3.1**.: _For any goal \(g(s_{0})\), S-GRS is optimal and complete. See proof in Appendix A.2._

### Width of Search Problems

Chen and Gimenez (2007) introduced the notion of the _width_ of a planning problem and showed that the forward-search complexity is exponential in the width of a problem. We extend this notion to regression-based searches and introduce a generalized version of regression rules in which not all previously achieved preconditions must be explicitly maintained.

**Definition 3.2** (Generalized regression rules).: _A generalized, optimally-serializable regression rule is \(g^{c} p_{1}^{c_{1}c_{1}},p_{2}^{c_{2}c_{2}},,p_{k}^{c_{ 1}c_{2}} a\), where \(c_{i}\{p_{1},p_{2},,p_{i-1}\}\). Its width is \(|c|+\{|c_{1}|,,|c_{k}|\}\)._

For example, for any initial state \(s_{0}\) that satisfies _on_(A, B), the following generalized rule is optimally serializable: \(()^{}\)_on_(A, B)\({}^{},()^{},( )^{()}(,)\). When we plan for the second precondition _clear_(A), we can ignore the first condition _on_(A, B), because the optimal plan for _clear_(A) will not change _on_(A, B). Using such generalized rules to replace the default rule set \(_{0}\) improves search efficiency by reducing the number of possible subgoals. To define existence conditions for highly efficient search algorithms and policies, we need a stronger notion:

**Definition 3.3** (Strong optimally-serializable (SOS) width of regression rules).: _A generalized regression rule has strong optimally-serializable width \(k\) w.r.t. state \(s\) if (a) it is optimally serializable, (b) its width is \(k\), and, (c) \( cOptSearch(s,p_{i} c_{i} c,)( s,\{p_{1},,p_{i}\} c,)\)._

In the Blocks World domain, given a particular state \(s\), if _on_(A, B) \( s\), the generalized goal regression rule \(()\)_on_(A, B), \((),()^{()} (,)\) is strong optimally-serializable, because the optimal trajectory to achieve _clear_(A) will not move A.

**Definition 3.4** (SOS width of problems).: _A planning problem \(P=,s_{0},g,,\) has strong optimally-serializable width \(k\) if there exists a set of strong optimally-serializable width \(k\) rules \(\) w.r.t. \(s_{0}\), such that \(S\)-GRS can solve \(P\) using only rules in \(\)._

**Theorem 3.2**.: _If a problem \(P\) is of SOS width \(k\), it can be solved in time \(O(N^{k+1})\) with s-GRS, where \(N\) is the number of atoms. Here we have omitted polynomials related to enumerating all possible actions and their serializations (which are polynomial w.r.t. the number of objects in \(\))._

Proof idea.: We start by assuming knowing \(\): the proof can be simply done by analyzing function calls: there are at most \(O(N^{k+1})\) possible argument combinations of \((goal,cons)\). Next, since the enumeration of all possible width-\(k\) rules can be done in polynomial time. Therefore, the algorithm runs in polynomial time and it does not need to know \(\) a priori. Note that the SOS condition cannot be removed because even if regression rules have a small width, there will be a possibly exponential branching factor caused by "free variables" in regression rules that do not appear in the goal atom. See the full proof in Appendix A.4. We also describe the connections with classical planning width and other related concepts, as well as how to generalize to \(\)-quantified preconditions in Appendix A.5.

Importantly, planning problems such as Blocks World and Logistics have constant SOS widths, independent of the number of objects. Therefore, there exist polynomial-time algorithms for finding their solutions. Our analysis is inspired by and closely related to classical (forward) planning width Chen and Gimenez (2007) and Lipovetzky and Geffner (2012). Indeed, we have:

**Theorem 3.3**.: _Any planning problem that has SOS-width \(k\) has a forward width of at most \(k+1\) and, hence, can be solved by the algorithm IW(\(k+1\)). See the full proof in Appendix A.6._

**Remark.** Unfortunately, SOS width and forward width are not exactly equivalent to each other. There exist problems whose forward width is smaller than \(k+1\), where \(k\) is the SOS width. Appendix A.6 presents a concrete example. However, there are two advantages of our SOS width analysis over the forward width analysis. In particular, proofs for (forward) widths of planning problems were mostly done by analyzing solution structures. By contrast, our constructions form a new perspective: _a problem has a large width if the number of constraints to track during a goal regression search is large_. This view is helpful because our analyses can now focus on concrete regression rules for each individual operator in the domain -- how its preconditions can be serialized. Second and more importantly, our "backward" view allows us to reason about circuit complexities of policies.

## 4 Policy Realization

We have studied sequential backward search algorithms for planning; now, we address the questions of how best to "compile" or "learn" them as feed-forward circuits representing goal-conditioned policies, and how the size of these circuits depends on properties of the problem. Here, we will focus on understanding the complexity of a certain _problem class_; that is, a set of problems in the same domain (e.g., Blocks World) with a similar goal predicate (e.g., having one particular block clear). In particular, we will make use of _relational neural networks_ (_RelNN_, such as graph neural networks (Morris et al., 2019) and Transformers (Vaswani et al., 2017)). They accept inputs of arbitrary size, which is critical for building solutions to problems with arbitrary-sized universes. Another critical fact is that these networks have "parameter-tying" in the sense that there is a constant-sized set of parameters that is re-used according to a given pattern to realize a network.

In particular, as illustrated in Fig. 2, we will show how to take the set of operator descriptions for a planning domain and construct a relational neural network that represents a goal-conditioned policy: it takes a state and a goal (encoded as a special predicate, e.g., _on-goal_) and outputs a ground action. Here, the state is represented as a graph (objects are nodes, and relations are edges; possibly there will be hyperedges for high-arity relations). For example, the input contains nullary features (a single vector for the entire environment), unary features (a vector for each object), binary features (a vector for each pair of objects), etc. The goal predicate can be represented similarly. These inputs are usually represented as tensors. The complexity of this neural network will depend on the SOS width of problems in the problem class of interest.

We will develop two compilation strategies. First, in Section 4.2, we directly compile the BWD and the S-GRS algorithms into _RelNN_ circuits. For problems with constant SOS width that is independent of the size of the universe, there will exist finite-breadth circuits (but the depth may depend on the size of the universe). In Section 4.3, we discuss a new property of planning problems--the existence of _regression rule selector_ circuits--which results in policy circuits that are smaller than those generatedby previous strategies, and they are potentially of finite depth. The construction of the regression rule selector is not automatic and will generally require human priors or machine learning.

### Relational Neural Network Background

We first quantify what a relational neural network can compute, using the formalization developed by Luo et al. (2022); see also Morris et al. (2019); Barcelo et al. (2020). Let depth \(D\) be the number of layers in a RelNN, and breadth \(B\) be the maximum arity of the relations (hyperedges) in the network. For example, to represent a vector embedding for each tuple of size \(5\), \(f(x_{1},x_{2},,x_{5})\), where \(x_{1},,x_{5}\) are entities in the planning problem, we need a relational neural network with breadth \(5\). We will only consider networks with a constant breadth. We denote the family of relational neural networks with depth \(D\) and breadth \(B\) as _RelNN\([D,B]\)_. We will not be modeling the actual "hidden dimension" of the neural network layers (i.e., the number of neurons inside each layer), but we will assume that it is bounded (for example, as a function of \(B\) and the number of predicates in the domain). Under such assumptions (most practical graph neural networks, Transformers, and hypergraph neural networks do follow these assumptions), we have the following lemma.

**Lemma 4.1** (Logical expressiveness of relational neural networks (Luo et al., 2022; Cai et al., 1992)).: _Let \(FOC_{B}\) denote a fragment of first-order logic with at most \(B\) variables, extended with counting quantifiers of the form \(^{2}\), which state that there are at least \(n\) nodes satisfying formula \(\)._

* _(Upper Bound) Any function_ \(f\) _in_ \(FOC_{B}\) _can be realized by_ \([D,B]\) _for some_ \(D\)_._
* _(Lower Bound) There exists a function_ \(f\) _in_ \(FOC_{B}\) _such that for all_ \(D\)_,_ \(f\) _cannot be realized by_ \([D,B-1]\)_._

Here, we sketch a constructive proof for using RelNNs to realize FOL formulas. The breadth \(B\) is analogous to the number of variables in FOL for encoding the value of the expression; the depth \(D\) is the number of "nested quantifiers." For example, the formula \( x. y. z.p(x,y,z)\) needs \(3\) layers, one for each quantifier. Furthermore, we call a _RelNN_ finite breadth (depth) if \(B\) (\(D\)) is independent of the number of objects. Otherwise, we call it unbounded breadth (depth).

### Compilation of BWD and S-GRS

Lemma 4.1 states an "equivalence" between the expressive power of relational neural networks and first-order logic formulas. In the following, we take advantage of this equivalence to compile search algorithms into relational neural networks. First, we have the following theorem.

**Theorem 4.1** (Compilation of BWD).: _Given a planning problem \(P\), let \(T\) be the length of the optimal trajectory (the planning horizon), \(k_{}\) be the maximum number of atoms in the goal set in \(}\), and \(\) be the maximum arity of atoms in the domain. The backward search algorithm \(}\) can be compiled into a relational neural network in \([O(T), k_{}]\)._

Proof sketch.: We provide a construction in which the _RelNN_ computes a set of subgoals (i.e., a set of sets of ground atoms) _Goal\({}^{d}\)_ at each layer \(d\). Initially, \(Goal^{0}=\{[g]\}\). Then, \(sg Goal^{d}\) if there is a path of length \(d\) from any state \(s\) that satisfies \(sg s\) to \(g\). See Appendix B.1 for the full proof.

Although this construction is general and powerful, it is unrealistic for large problems because the depth can be exponential in the number of objects, and the number of atoms in the subgoal conjunctive formulas can be exponential in the depth. Therefore, in the following, we will leverage the idea of serialized goal regression to make a more efficient construction.

**Theorem 4.2** (Compilation of S-GRS).: _Given a planning problem \(P\) of SOS width \(k\), let \(T\) be the length of the optimal trajectory, and \(\) be the maximum arity of atoms in the domain. The serialized goal regression search S-GRS can be compiled into a \([O(T),(k+1)]\), where \(k+1 k_{}\). See Appendix B.2 for the full proof using a similar technique as in Theorem 4.1._

Figure 2: The input and output of a relational neural network (_RelNN_) policy.

**Remark.** The compilation in Theorem 4.2 can generate finite-breadth, unbounded-depth _RelNN_ circuits for more problems than the compilation in Theorem 4.1. For example, in Blocks World, the number of atoms in the goal sets in \(\) is unbounded. However, since the problem is of constant SOS width, it can be compiled into a finite-breadth circuit. When the search horizon \(T\) is finite, both the SOS width and \(k_{}\) will be finite because there are only a constant number of actions that can be applied to update the goal set \(/\) constraint set. So a problem can be compiled into a finite-depth, finite-breadth _RelNN_ circuit with BWD compilation if and only if it can be compiled with S-GRS compilation, although S-GRS compilation may generate smaller circuits.

### Compilation of S-GRS with a Regression Rule Selector

Unfortunately, both constructions in the previous section require a depth-\(O(T)\)_RelNN_ circuit, which is impractical for most real-world problems, as \(T\) can be exponential in the number of objects in a domain. In order to compile the goal regression search algorithm into a smaller circuit, we consider scenarios where the rule used to construct the (optimal) plan can be computed without doing a full search. Here the idea is to bring in human priors or machine learning to learn a low-complexity _regression rule selector_ that directly predicts which goal regression rule to use. There exists such a rule selector for some problems (e.g., Blocks World) but not others (e.g., Sokoban).

Formally, we define a state-dependent _regression rule selector_ (RRS) _select\((s,g,)\)_, which returns an ordered list of preconditions and a ground action \(a\). Its function body can be written in the following form: \(g^{}\) if \((s,):p_{1}^{},p_{2}^{(p_{1}|p_{1},,p_{m}^{(p_{1},,p_{m-1})} a\), which reads: in order to achieve \(g\) under constraint \(\) at state \(s\), if \((s,)\) is true, we can apply the regression rule \(p_{1},p_{2},,p_{m} a\). Formally, _select_ is composed of a collection of rules. Each rule is a tuple of \(,,,a,,\), where \(\) is a set of variables, \(\) is a FOL formula that can be evaluated at any given state \(s\), \(\) is an ordered list of preconditions, \(a\) is a ground action, \(g\) is the goal atom, and \(\) is the constraint. \(,,a,\), and \(\) may contain variables in \(\). Such a rule selector can be implemented by hand (as a set of rules, as illustrated in Fig. 2(a) for the simple BlocksWorld domain), or learned by a _RelNN_ model.

The main computational advantage is that now, for a given tuple \((s,g,)\), there is a single ordered list of preconditions and final action. Therefore, given the function _select_, we can simply construct a sequence of actions that achieves the goal by recursively applying the rule selector. In this section, we will first discuss scenarios where such rule selectors can be computed with shallow _RelNN_ circuits.

**Circuit complexity of regression rule selectors.** We first discuss scenarios where there exist finite-depth, finite-breadth circuits for computing the regression rule selector. Note that there are three "branching" factors in choosing the regression rule: 1) the operator to use, 2) the order of the preconditions, and 3) the binding of "free" variables that are not mentioned in the goal atom.

Figure 3: State-dependent regression rule selector in the Blocks World domain. For brevity, we have omitted atoms in the constraint set. All rules listed above are applicable under any constraints.

[MISSING_PAGE_EMPTY:8]

single atom. In summary, most of these problems have a constant breadth (i.e., the regression width of the corresponding problem is constant) except for Sokoban. Most problems have an unbounded depth: that is, the depth of the circuit will depend on the number of objects (e.g., the size of the graph or the number of blocks). For problems in this list, when there are multiple goals, usually the goals are not serializable (in the optimal planning case). If we only care about satisficing plans, for Logistics, Gripper, Rover, and Elevator, there exists a simple serialization for any conjunction of goal atoms (basically achieving one goal at a time). See Appendix C for more detailed proofs and additional discussions of generalization to multiple goals and suboptimal plans.

Next, given this analytical understanding of the relationship between planning problems and their policy circuit complexity, we perform some simple experiments to see whether that relationship is borne out in practice. Relational neural networks have been demonstrated to be effective in solving some planning problems (Dong et al., 2019; Jiang and Luo, 2019; Li et al., 2020), but their complexity has not been systematically explored. We consider two families of problems: one predicted to require finite depth and one predicted to require unbounded depth. For all tasks, we use Neural Logic Machines (Dong et al., 2019) as the model; we set the number of hidden neurons in the MLP layers to be sufficiently large (64 in all experiments) so that it is not the bottleneck for network expressiveness. We use the same encoding style for all problems (the graph-like relationship). Therefore, whether different problems have bounded or unbounded width primarily depends on the available goal regression rules, how these rules can be serialized, and the width of the rules. We show the average performance across 3 random seeds, with standard errors.

**Assembly3: finite depth circuits.** The domain Assembly3 contains \(n\) objects. Each object has a category, chosen from the set \(\{A,B,C\}\). The goal of the task is to select three objects \(o_{A}\), \(o_{B}\), and \(o_{C}\) sequentially, one for each category, while satisfying a matching constraint: _match_(\(o_{A},o_{B}\)) and _match_(\(o_{B},o_{C}\)). Therefore, to select the first object (e.g., an object of type \(A\)), the policy needs to perform two layers of goal regression (first find the set of possible \(B\)-typed objects that match the object, and then find another \(C\)-typed object). We trained two models. Both models have breadth 2, but the first model has only 1 layer (theoretically not capable of representing the two-step goal regression), while the second model has 2 layers. Shown in Table 2, we train models on environments with 10 objects and test them on environments with 10, 30, and 50 objects. The first model is not able to learn a policy for the given task, while the second exhibits perfect generalization.

**Logistics: unbounded depth circuits.** We also construct a simple Logistics domain with only cities and trucks (no airplanes), in which the graphs are not strongly connected (by first sampling a directed tree and then adding forward-connecting edges). We train the policy network on problems with less than n=10 cities and test it on n=30 and n=50. Here, we trained two policies. The first policy has a constant depth of 3, and the second policy has a depth of \(D=n/5+1\), which is a manually chosen function such that it is larger than the diameter of the graph. Shown in Table 2, the first model fails to generalize to larger graphs due to its limited circuit depth. Intuitively, to find a path in a graph by recursively applying the regression rule, we need a circuit of an adaptive depth (proportional to the length of the path). By contrast, the second model with adaptive depths generalizes perfectly.

**BlocksWorld-Clear: unbounded depth circuits.** Based on the goal regression analysis, in BlocksWorld, in order to achieve the goal atom _clear_(A) for a specific block \(A\), we would need

   Problem & Constant & Depth & \\  & Breadth? & & \\  BlocksWorld & ✓ (\(k=1\)) & Unbounded & \\ Logistics & ✓ (\(k=0\)) & Unbounded & \\ Gripper & ✓ (\(k=0\)) & Constant & \\ Rover & ✓ (\(k=0\)) & Unbounded & \\ Elevator & ✓ (\(k=0\)) & Unbounded & \\ Sokoban & ✗ & Unbounded & \\   

Table 1: Width and circuit complexity analysis for 6 problems widely used in AI planning communities. For problems with a constant circuit breadth, we also annotate their regression width \(k\). “Unbounded” depth means that the depth depends on the number of objects.

   Task & Model & n=10 & n=30 & n=50 \\  As3 & _RelNN_[\(1,2\)] & \(0.2_{n 0.05}\) & \(0.02_{n 0.01}\) & \(0.0_{n 0.0}\) \\ As3 & _RelNN_[\(2,2\)] & \(_{n 0.0}\) & \(_{n 0.0}\) & \(_{n 0.0}\) \\ Log. & _RelNN_[\(3,2\)] & \(1.0_{n 0.0}\) & \(0.30_{n 0.03}\) & \(0.23_{n 0.05}\) \\ Log. & _RelNN_[\(f(n),2\)] & \(_{n 0.0}\) & \(_{n 0.0}\) & \(_{n 0.0}\) \\ BW & _RelNN_[\(3,2\)] & \(1.0\,/\,2.9_{n 0.4}\) & \(1.0\,/\,10.2_{n 0.45}\) & \(1.0\,/\,15.7_{n 2.5}\) \\ BW & _RelNN_[\(f(n),2\)] & \(\,/\,2_{n 0.4}\) & \(\,/\,3_{n 0.5}\) & \(\,/\,3_{n 0.4}\) \\   

Table 2: Success rate of learned policies in different environments. For Assembly3 (As3) and Logistics (Log.), we show the success rate. For Blocks World (BW), we show the success rate / average solution length. We choose \(f(n)}}{{=}}n/5+1\) for Logistics and \(f(n)}}{{=}}n/10+3\) for BlocksWorld. In the notation of _RelNN_[\(D,B\)], \(D\) is the number of layers and \(B\) is the maximum arity of edges.

a circuit that is as deep as the number of objects on \(A\) in the initial state. We follow the same training and evaluation setup as in Logistics. Our analysis focuses on the length of the generated plan. Shown in Table 2, although both policy networks accomplish the task with a 1.0 success rate (because there is a trivial policy that randomly drops clear blocks onto the table), the policy whose depth depends on the number of blocks successfully finds a plan that is significantly shorter than fixed-depth policy. This suggests the importance of using RelNNs with a non-constant depth for certain problems.

## 6 Related Work and Conclusion

Most of the existing work on planning complexity considers the NP-Completeness or PSPACE-Completeness of particular problems, such as the traveling salesman problem (Karp, 1972), Sokoban (Culberson, 1997), and Blocks World (Gupta and Nau, 1992). In general, the decision problem of plan existence is PSPACE-Complete for atomic STRIPS planning problems (Bylander, 1994). Seminal work on fine-grained planning problem complexity introduces planning width (Chen and Gimenez, 2007) and the IW algorithm (Lipovetzky and Geffner, 2012; Drexler et al., 2022). Our work is greatly inspired by these ideas and strives to connect them with circuit complexity.

The concept of serialized goal regression is not completely new. These rules are particularly related to the methods in hierarchical task networks (Erol et al., 1994) and can also be seen as special cases of hierarchical goal networks (Alford et al., 2016), derived by dropping preconditions (Sacerdoti, 1974). In addition, others, including Korf (1987) and Barrett and Weld (1993), have characterized different degrees of serializability of subgoals, but their analysis is not as fine-grained as this.

Our circuit complexity analysis builds on existing work on relational neural network expressiveness, in particular, GNNs and their variants. Xu et al. (2019) provides an illuminating characterization of GNN expressiveness in terms of the WL graph isomorphism test. Azizian and Lelarge (2021) analyze the expressiveness of higher-order Folklore GNNs by connecting them with high-dimensional WL-tests. Barcelo et al. (2020) and Luo et al. (2022) reviewed GNNs from the logical perspective and rigorously refined their logical expressiveness with respect to fragments of first-order logic. Our work extends their results, asking the question of what planning problems a _RelNN_ can solve.

Our work is also related to how GNNs may generalize to larger graphs (in our case, planning problems with an arbitrary number of objects). Xu et al. (2020, 2021) have studied the notion of _algorithmic alignment_ to quantify such structural generalization. Dong et al. (2019) provided empirical results showing that NLMs generalize in Blocks World and many other domains. Buffelli et al. (2022) introduced a regularization technique to improve GNNs' generalization and demonstrated its effectiveness empirically. Xu et al. (2021) also showed empirically on certain algorithmic reasoning problems (e.g., max-Degree, shortest path, and the n-body problem). In this paper, we focus on constructing policies that generalize instead of algorithms, using a similar idea of compiling specific (search) algorithms into neural networks.

**Conclusion.** To summarize, we have illustrated a connection between classical planning width, regression width, search complexity, and policy circuit complexity. We derive upper bounds for search and circuit complexities as a function of the regression width of problems and planning horizon. These results provide an explanation of the success of relational neural networks learning generalizable policies for many object-centric domains. The compilation algorithms highlight that when there are resource constraints such as free space, agents' inventory size, etc., a small policy circuit may not exist. This includes cases such as Sokoban and general task and motion planning. Furthermore, our idea of serialization can be generalized to hierarchical decomposition of problems; it can be potentially extended to other domains such as optimization and planning under uncertainty.

Although all of the analyses in this paper have been done for fully discrete domains by analyzing first-order logic formulas, understanding the implications in relational neural network learning is important because, ultimately, _RelNNs_ can be integrated with perception and continuous action policy networks to solve realistic robotic manipulation problems (e.g., physical Blocks World (Li et al., 2020)). It is challenging to directly generalize our proof to scenarios where states and actions are continuous values because in these cases, there will be an infinite number of possible actions (e.g., the choice of grasping poses, trajectories, etc.). We believe that it is an interesting question to investigate similar "width"-like definitions in sampling and discretization-based continuous planning algorithms. So far, we have only analyzed the circuit complexity of policies for individual planning problems; an interesting future direction is to extend these analyses to the learning and inference of "general" policies such as large language models that can generate plans for many different planning domains.

Acknowledgement.We thank anonymous reviewers for their valuable comments. We gratefully acknowledge support from ONR MURI grant N00014-16-1-2007; from the Center for Brain, Minds, and Machines (CBMM, funded by NSF STC award CCF-1231216); from NSF grant 2214177; from Air Force Office of Scientific Research (AFOSR) grant FA9550-22-1-0249; from ONR MURI grant N00014-22-1-2740; from ARO grant W911NF-23-1-0034; from the MIT-IBM Watson AI Lab; from the MIT Quest for Intelligence; and from the Boston Dynamics Artificial Intelligence Institute. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of our sponsors.