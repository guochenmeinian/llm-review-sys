# Dual Self-Awareness Value Decomposition Framework without Individual Global Max for Cooperative MARL

Dual Self-Awareness Value Decomposition Framework without Individual Global Max for Cooperative MARL

Zhiwei Xu, Bin Zhang, Dapeng Li, Guangchong Zhou, Zeren Zhang, Guoliang Fan

Institute of Automation, Chinese Academy of Sciences

School of Artificial Intelligence, University of Chinese Academy of Sciences

{xuzhiwei2019, guoliang.fan}@ia.ac.cn

###### Abstract

Value decomposition methods have gained popularity in the field of cooperative multi-agent reinforcement learning. However, almost all existing methods follow the principle of Individual Global Max (IGM) or its variants, which limits their problem-solving capabilities. To address this, we propose a dual self-awareness value decomposition framework, inspired by the notion of dual self-awareness in psychology, that entirely rejects the IGM premise. Each agent consists of an ego policy for action selection and an alter ego value function to solve the credit assignment problem. The value function factorization can ignore the IGM assumption by utilizing an explicit search procedure. On the basis of the above, we also suggest a novel anti-ego exploration mechanism to avoid the algorithm becoming stuck in a local optimum. As the first fully IGM-free value decomposition method, our proposed framework achieves desirable performance in various cooperative tasks.

## 1 Introduction

Multi-agent reinforcement learning (MARL) has recently drawn increasing attention. MARL algorithms have been successfully applied in diverse fields, including game AI  and energy networks , to solve practical problems. The fully cooperative multi-agent task is the most common scenario, where all agents must cooperate to achieve the same goal. Several cooperative MARL algorithms have been proposed, with impressive results on complex tasks. Some algorithms  enable agents to collaborate through communication. However, these approaches incur additional bandwidth overhead. Most communication-free MARL algorithms follow the conventional paradigm of centralized training with decentralized execution (CTDE) . These works can be roughly categorized into multi-agent Actor-Critic algorithm  and value decomposition method . Due to the high sample efficiency in off-policy settings, value decomposition methods can outperform other MARL algorithms.

However, due to the nature of the CTDE paradigm, the value decomposition method can only operate under the assumption of Individual Global Max (IGM) . The IGM principle links the optimal joint action to the optimal individual one, allowing the agent to find the action corresponding to the maximal Q value during decentralized execution. Nevertheless, this assumption is not applicable in most real-world scenarios. And most value decomposition approaches  directly constrain the weights of the neural network to be non-negative, which restricts the set of global state-action value functions that can be represented. We confirm this point through experiments in Appendix D.1.

Removing the IGM assumption requires agents to be able to find actions corresponding to the maximal joint Q value during execution. Therefore, in addition to the evaluation network, agents also need an additional network to learn how to _search_ for the best actions in a decentralized manner. Similarly, there are concepts of _ego_ and _alter ego_ in psychology. The ego usually refers to the conscious part of the individual, and Freud considered the ego to be the executive of the personality . Some people believe that an alter ego pertains to a different version of oneself from the authentic self [24; 12]. Others define the alter ego in more detail as the evaluation of the self by others . As a result, both ego and alter ego influence a person's personality and behaviour, as depicted in Figure 1. Besides, Duval and Wicklund  put forth the dual self-awareness hypothesis, which posits that individuals possess two states of consciousness: public self-awareness and private self-awareness. Private self-consciousness refers to an individual's internal understanding of himself, whereas public self-awareness enables individuals to recognize that they are evaluated by others. Enlightened by these psychological concepts, we propose a novel MARL algorithm, **D**ual self-**A**wareness **V**alue **d**E**composition (**DAVE**). Each agent in DAVE maintains two neural network models: the alter ego model and the ego model. The former participates in the evaluation of the group to solve the global credit assignment problem, and the latter is not directly optimized by the joint state-action value. The ego policy can help the alter ego value function to find the individual action corresponding to the optimal joint action through explicit search procedures so that the value decomposition framework can completely eliminate the constraints of the IGM assumption. Furthermore, to prevent the ego policy based on the above framework from potentially getting stuck in a bad local optimum, we also propose an anti-ego exploration method. This method can effectively avoid the underestimation of the optimal action that may be caused by the limited search procedure.

We first evaluate the performance of DAVE in several didactic problems to demonstrate that DAVE can resolve the issues caused by IGM in conventional value decomposition methods. Then we run DAVE in various complex scenarios including StarCraft II, and compare it to other popular baselines. Our experimental results show that DAVE can still achieve competitive performance despite the absence of IGM. To the best of our knowledge, DAVE is the first multi-agent value decomposition method that completely abandons the IGM assumption.

## 2 Related Work

Recently, value decomposition methods have made significant progress in problems involving the decentralized partially observable Markov decision process (Dec-POMDP) . By calculating a joint state-action value function based on the utility functions of all agents, value decomposition methods can effectively alleviate the notorious credit assignment issue. The first proposed method, VDN , simply sums up the utility of all agents to evaluate joint actions. This undoubtedly limits the range of joint value functions that can be approximated. QMIX  incorporates hypernetworks  into the value decomposition method and employs the mixing network to fit the joint value function based on the individual ones, thereby expressing richer classes of joint value function. To enable the agent to choose actions with the greedy policy during decentralized training, QMIX assumes monotonicity and restricts the weight parameters of the mixing network output by hypernetworks to non-negative. So the monotonic mixing network still cannot represent some class of value functions. Various value decomposition variants have subsequently been proposed. Officially proposing IGM, QTRAN  indicated that the value decomposition method must satisfy the assumption that the optimal joint actions across agents are equivalent to the collection of individual optimal actions of each agent. Unlike VDN and QMIX, QTRAN is not restricted by structural constraints such as non-negative weights, but it performs poorly in complex environments such as StarCraft II. MAVEN  enhances its performance by improving its exploration capabilities. Weighted QMIX  takes into account a weighted projection that places greater emphasis on better joint actions. QPLEX  takes a duplex dueling network architecture to factorize the joint action-value function and proposes the advantage-based IGM. Nevertheless, the aforementioned methods only loosen the restrictions by proposing IGM variants. More importantly, existing policy-based MARL methods [41; 20; 25] do not explicitly introduce IGM, but they also cannot solve some non-monotonic problems. So how to remove the IGM assumption is still an open and challenging problem. Our proposed DAVE is completely IGM-free and can be applied to existing IGM-based value decomposition methods. More related work will be discussed in Appendix E.

Figure 1: In psychology, it is commonly believed that the ego and alter ego determine an individualâ€™s behavior jointly.

Preliminaries

### Dec-POMDPs

In this paper, we concentrate on the Dec-POMDP problems. It is an important mathematical model for multi-agent collaborative sequential decision-making problem, in which agents act based on different pieces of information about the environment and maximize the global shared return. Dec-POMDPs can be formulated as a tuple \(,,,,P,O,r,\). \(a:=\{1,,n\}\) represents the agent in the task. \(s\) is the global state of the environment. The joint action set \(\) is the Cartesian product of action sets \(^{a}\) of all agents and \(\) denotes the joint action. Given the state \(s\) and the joint action \(\), the state transition function \(P:\) returns the probability of the next potential states. All agents receive their observations \(=\{z^{1},...,z^{n}\}\) obtained by the observation function \(O:\). \(r:\) denotes the global shared reward function and \(\) is the discount factor. Besides, recent work frequently allows agents to choose actions based on historical trajectory \(^{a}\) rather than local observation \(z^{a}\) in order to alleviate the partially observable problem.

In Dec-POMDP, all agents attempt to maximize the discount return \(R_{t}=_{t=0}^{}^{t}r_{t}\) and the joint state-action value function is defined as:

\[Q_{}(s,)=_{}[R_{t}|s,],\]

where \(=\{_{1},,_{n}\}\) is the joint policy of all agents.

### IGM-based Value Decomposition Methods

The goal of value decomposition method is to enable the application of value-based reinforcement learning methods in the CTDE paradigm. This approach decomposes the joint state-action value function \(Q_{}\) into the utility function \(Q_{a}\) of each agent. In VDN, the joint state-action value function is equal to the sum of all utility functions. And then QMIX maps utility functions to the joint value function using a monotonous mixing network and achieve better performance.

Nonetheless, additivity and monotonicity still impede the expressive capacity of the neural network model, which may result in the algorithm being unable to handle even for some simple matrix game problems. To this end, QTRAN proposes the Individual Global Max (IGM) principle, which can be represented as:

\[_{u^{a}}Q_{}(s,)=_{u^{a}}Q_{a}(^{a},u^ {a}), a.\]

The IGM principle requires consistency between local optimal actions and global ones. Most value decomposition methods including VDN and QMIX must follow IGM and its variants. This is because only when the IGM principle holds, the individual action corresponding to the maximum individual action value during decentralized execution is a component of the optimal joint action. Otherwise, the agent cannot select an appropriate action based solely on the individual action-value function.

## 4 Methodology

In this section, we introduce the implementation of the dual self-awareness value decomposition (DAVE) framework in detail. DAVE incorporates an additional ego policy through supervised learning in comparison to the conventional value decomposition method, allowing the agent to identify the optimal individual action without relying on the IGM assumption. Moreover, DAVE can be applied to most IGM-based value decomposition methods and turn them into IGM-free ones.

### Dual Self-Awareness Framework

Almost all value decomposition methods follow the IGM assumption, which means that the optimal joint policy is consistent with optimal individual policies. According to IGM, the agent only needs to follow the greedy policy during decentralized execution to cater to the optimal joint policy. Once the IGM assumption is violated, agents cannot select actions that maximize the joint state-action value based solely on local observations. Consequently, the objective of the IGM-free value decomposition method is as follows:

\[*{arg}_{}Q_{}(s,), \]here \(=\{u^{1},,u^{n}\}\) and \(u^{a}_{a}(^{a})\). Without the IGM assumption, Equation (1) is NP-hard because it cannot be solved and verified in polynomial time . Therefore, in our proposed dual self-awareness framework, each agent has an additional policy network to assist the value function network to find the action corresponding to the optimal joint policy. Inspired by the dual self-awareness theory in psychology, we named these two networks contained in each agent as _ego policy model_ and _alter ego value function model_, respectively. Figure 2 depicts the dual self-awareness framework. Note that the IGM-free mixing network refers to the mixing network obtained by removing structural constraints. For instance, the IGM-free QMIX variant does not impose a non-negative constraint on the parameters of the mixing network.

In DAVE, the alter ego value function model of each agent \(a\) is identical to the agent network in other value decomposition methods and generates individual state-action value \(Q_{a}^{}(^{a},)\) based on the current historical trajectory \(^{a}\). To obtain the joint state-action value \(Q_{}^{}(s,)\) for carrying out the joint action \(\) in the state \(s\), the value function of each agent is fed into the IGM-free mixing network. However, the alter ego model does not determine the actions chosen by the agent, either during centralized training or decentralized execution. What determines the action of each agent is the ego policy \(_{a}^{}\). The joint ego policy can be expressed as \(^{}( s)=_{a=1}^{n}_{a}^{}(u^{a} ^{a})\). When executing, the agent interacts with the environment by sampling actions from the categorical distribution output by the ego policy. And during training, we replace the exact maximization of \(Q_{}(s,)\) over the total joint action space with maximization over a set of samples \(^{}\) from \(^{}\). So we modify the objective of the cooperative multi-agent value decomposition problem from Equation (1) to

\[*{arg\,max}_{^{}}Q_{}^{}( s,^{}),^{}^{ }, \]

where \(^{}:=\{_{i}^{}^{}(s)\}_ {i=1}^{M}\) in state \(s\) and \(M\) is the number of samples. Therefore, the dual self-awareness framework substitutes the global optimal state-action value with the local optimal one found in the small space \(^{}\), which is obtained by sampling \(M\) times from \(^{}\). As long as the ego policy assigns non-zero probabilities to all available actions, this method will converge to the objective described by Equation (1) as the number of samples \(M\) increases. The detailed proof can be found in Appendix A. Nevertheless, the computational cost is directly proportional to \(M\). So the choice of \(M\) must strike a balance between precision and computational complexity.

Next, we introduce the update process of the alter ego value function and the ego policy in the dual self-awareness framework. According to the modified objective of the cooperative multi-agent value decomposition problem illustrated by Equation (2), the ego policy needs to be able to predict the individual action that corresponds to the maximal joint state-action value. Therefore, we update the ego policy through supervised learning to increase the probability of the optimal joint action obtained through the sampling procedure. We define \(^{}=*{arg\,max}_{^{}}_{ }^{}(s,^{})\) as the joint action corresponding to the maximal joint state-action value in the samples, where \(^{}^{}\). The loss function for the joint ego policy can be written as:

\[_{}=-^{}(^{} s). \]

The objective function is the negative log-likelihood of the probability of the joint action \(^{}\) and can be further expressed as \(-_{a=1}^{n}_{a}^{}({u^{a}}^{}^{a})\) associated with individual ego policies.

Figure 2: **Middle**: The overall dual self-awareness value decomposition framework. **Right**: Agent network structure. **Left**: The setup of the anti-ego exploration.

Additionally, the alter ego value function \(^{}\) is constantly changing during the training process. To prevent the ego policy \(^{}\) from becoming too deterministic and getting trapped in local optimal solutions, we will introduce _a novel exploration method_ in the next subsection.

Conventional value decomposition methods can be trained end-to-end by minimizing the following loss:

\[=(y_{}-Q_{}(s_{t},_{t}) )^{2}, \]

where \(y_{}=r_{t}+_{_{t+1}}Q_{}^{-}(s_{t+1}, _{t+1})\) and \(Q_{}^{-}\) is the target joint value function. The loss function described above cannot be directly applied to our proposed dual self-awareness framework because of the max operator. If we also traverse the samples of the joint action in \(^{}\) to select the local maximum target joint state-action value function, which may result in high variance. Inspired by the Expected SARSA , we use the expected value instead of the maximal one. The update rule of \(Q_{}^{}\) considers the likelihood of each action under the current policy and can eliminate the variance, which can be expressed as:

\[_{}=(y_{}^{}-Q_{}^{ {alter}}(s_{t},_{t}))^{2}, \]

where \(y=r_{t}+_{^{}}[(Q_{}^{ })^{-}(s_{t+1},_{t+1})]\). However, this method is still computationally complex. So we reuse the action sample set \(^{}\) and replace \(_{^{}}[(Q_{}^{})^{-}(s_{t+1},_{t+1})]\) with \(_{i=1}^{M}(Q_{}^{})^{-}(s_{t+1},_{ i}^{})\), where \(_{i}^{}^{}\) in state \(s_{t+1}\). The modified update rule for \(Q_{}^{}\) reduces the variance and does not increase the order of the computational complexity compared to the original update method shown in Equation (4). Each hypernetwork takes the global state and the actions of all agents as input and generates the weights of the mixing network. It should also be noted that we use the global state and the actions of all agents as input to the hypernetwork that generates weights to enhance the expressiveness of the mixing network.

In summary, we have constructed the fundamental framework of DAVE. Although DAVE consists of a value network and a policy network, it is **DISTINCT** from the Actor-Critic framework or other policy-based methods such as MAPPO , MADDPG  and FACMAC . DAVE requires an explicit sampling procedure to update the network model. Besides, the actions sampled from the ego policies remain independent and identically distributed (IID) , so the ego policy is trained through _supervised learning_ rather than _policy gradient_ used in policy-based methods. Most intuitively, conventional policy-based methods cannot solve simple non-monotonic matrix games but DAVE can easily find the optimal solution, as described in Section 5.1 and Appendix D.2.

### Anti-Ego Exploration

As stated previously, updating the ego policy network solely based on Equation (3) with an insufficient sample size \(M\) may lead to a local optimum. This is because the state-action value corresponding to the optimal joint action may be grossly underestimated during initialization, making it challenging for the ego policy to choose the optimal joint action. Consequently, the underestimated \(Q_{}^{}\) will have no opportunity to be corrected, creating a vicious circle. Therefore, we developed an _anti-ego exploration_ method based on ego policies. Its core idea is to assess the occurrence frequency of state-action pairs using the reconstruction model based on the auto-encoder . The auto-encoder model often requires the encoder to map the input data to a low-dimensional space, and then reconstructs the original input data based on the obtained low-dimensional representation using the decoder. Our method leverages the fact that the auto-encoder cannot effectively encode novel data and accurately reconstruct it. The more significant the difference between the original state-action pair \((s,)\) and the reconstructed output \((s^{},^{})\), the less frequently the agents carry out the joint action \(\) in state \(s\). Simultaneously, the difference between the reconstructed state-action pairs and the original ones is also the objective function for updating the auto-encoder, which can be expressed as:

\[_{}(s,)=(s,s^{})+_{a =1}^{n}(u^{a},(u^{a})^{}), \]

where \(()\) and \(()\) represent the mean square error and the cross-entropy loss function respectively. So we need to encourage agents to choose joint actions with a higher \(_{}\). Obviously, it is impractical to traverse all available joint actions \(\). Then we look for the most novel actions within the limited set of actions obtained via the sampling procedure. In order to get state-action pairs that are as uncommon as possible, we generate an anti-ego policy \(}^{}\) based on \(^{}\), as shown in Figure 2. The relationship between the ego and anti-ego policy of each agent is as follows:

\[_{a}^{}(^{a}) =(f(^{a})),\] \[_{a}^{}(^{a}) =(f(^{a})), \]

where \(f()\) denotes the neural network model of ego policy. Equation (7) demonstrates that actions with lower probabilities in \(_{a}^{}\) have higher probabilities in \(_{a}^{}\). Of course, actions with low probabilities in \(_{a}^{}\) may be non-optimal actions that have been fully explored, so we filter out such actions through the reconstruction model introduced above. \(}^{}:=\{}_{i}^{}}^{ }(s)\}_{i=1}^{M}\) is the joint action set obtained by sampling \(M\) times from the anti-ego policy in state \(s\). We can identify the most novel joint action \(}^{}\) in state \(s\) by the following equation:

\[}^{}=*{arg\,max}_{}^{}} _{}(s,}^{}), \]

where \(}^{}}^{}\). Therefore, the objective function in Equation (3) needs to be modified to increase the probability of the joint action \(}^{}\). The final objective function of the ego policy \(^{}\) can be written as:

\[_{}=-^{}(^{} s )+^{}(}^{} s), \]

where the exploration coefficient \(\) is annealed linearly during the exploration phase. The choice of hyperparameter \(\) requires a compromise between exploration and exploitation. Note that the dual self-awareness framework and the anti ego exploration method are **NOT** two separate contributions. The latter is customized for the former, and it is only used to solve the convergence difficulties caused by the explicit sampling procedure in DAVE. By incorporating the anti-ego exploration module, the dual self-awareness framework with low \(M\) can also fully explore the action space and significantly avoid becoming stuck in bad local optima. The whole workflow of the anti-ego exploration mechanism is shown in Appendix B.

Our proposed DAVE can be applied to most existing value decomposition methods, as it simply adds an extra ego policy network to the conventional value decomposition framework. We believe that the IGM-free DAVE framework can utilize richer function classes to approximate joint state-action value functions and thus be able to solve the non-monotonic multi-agent cooperation problems.

## 5 Experiment

In this section, we apply the DAVE framework to standard value decomposition methods and evaluate its performance on both didactic problems and complex Dec-POMDPs. First, we will compare the performance of the DAVE variant with QPLEX, Weighted QMIX, QTRAN, FACMAC-nonmonotonic , MAVEN, and the vanilla QMIX. Note that most baselines are directly related to IGM. Then the contribution of different modules in the framework and the impact of hyperparameters will be investigated. Except for the new hyperparameters introduced in the DAVE variant, all other hyperparameters are consistent with the original method. Unless otherwise stated, DAVE refers to the DAVE variant of QMIX, which enhances representational capacity while ensuring fast convergence. For the baseline methods, we obtain their implementation from the released code. The details of the implementation and all environments are provided in Appendix C and Appendix D, respectively.

### Single-State Matrix Game

Certain types of tasks in multi-agent cooperation problems cannot be handled under the IGM assumptions, such as the two matrix games we will introduce next. These two matrix games capture very simple cooperative multi-agent tasks and are shown in Figure 3. These payoff matrices of the two-player three-action matrix games describe scenarios that IGM-based value decomposition methods cannot address effectively. The variable \(k\) is related to difficulty. In matrix game I with \(k[0,8)\), agents need to choose the same action to get the reward and the optimal joint action is \((A,A)\). Furthermore, the closer \(k\) is to 8, the more challenging it becomes to jump out of the local optimum. Similarly, \(k 0\) and \((A,C)\) is the optimal joint action in matrix

Figure 3: Payoffs of the two matrix games.

game II. Nevertheless, as \(k\) increases, so does the risk of selecting the optimal action \((A,C)\), which leads agents in the IGM-based method to prefer the suboptimal joint action \((B,B)\) corresponding to the maximum expected return. Therefore, both matrix games are challenging for traditional value factorization methods. To disregard the influence of exploration capabilities, we implement various algorithms under uniform visitation, which means that all state-action pairs are explored equally. We also evaluate the performance of popular policy-based MARL algorithms. Results in Appendix D.2 demonstrate that although they do not explicitly introduce IGM, policy-based methods still perform poorly on these two simple tasks.

The results of all algorithms on the two matrix games with varying \(k\) are presented in Figure 4. we limit the number of interactions between agents and the environment to 400 across all environments. All baselines find it challenging to discover the optimal joint action with such limited data. Compared to other algorithms, DAVE can quickly converge to the optimal solution in all situations and exhibits robust performance, showing little variation as task difficulty increases. CW-QMIX can converge to the optimal joint solution in all cases except matrix game I with \(k=7.5\). QPLEX performs better in matrix game II than in matrix game I, likely due to its sensitivity to the penalty of non-cooperation. In addition, as the task difficulty increases, both CW-QMIX and QPLEX show a significant drop in performance. Although QTRAN can converge to the optimal solution in matrix game I with \(k=0\), it may require more interactive samples for further learning. The remaining algorithms can converge to the optimal joint action \((A,C)\) or the suboptimal joint action \((C,A)\) in matrix game II with \(k=0\), but in the more difficult matrix game II can only converge to the joint action \((B,B)\). We add additional action information to the mixing network of QMIX, similar to that in DAVE, but QMIX still cannot converge to the optimal solution. In summary, DAVE can easily solve the above single-state matrix games and outperforms other IGM-based variants and the basic algorithm QMIX by a significant margin.

### Multi-Step Matrix Game

Next, we evaluate the performance of each algorithm in the multi-step matrix game. The environment involves two agents, each with two actions. The initial state is non-monotonic, and agents terminate immediately after receiving a zero reward. If the game is not terminated after the first step, agents need to continue to repeat the last joint action to obtain a non-zero reward. According to Figure 5, agents repeat either the upper-left joint action or the lower-right joint action eight times, resulting in two distinct terminal states. The optimal policy is for agents to repeat the upper-left joint action in the initial state and choose the lower-right joint action in the terminal state, which yields the highest return of 13. Local observations of agents in this

Figure 4: The learning curves of DAVE and other baselines on the matrix games. Note that the ordinates are non-uniform.

Figure 5: **Left**: Illustrations of the multi-step matrix game. **Right**: Performance over time on the multi-step matrix game.

environment include the current step count and the last joint action. The protracted decision-making process requires agents to fully explore the state-action space to find the optimal policy. Therefore, this task can also test the exploration capacity of the algorithm.

Figure 5 shows the performance of DAVE and other baselines on the multi-step matrix game. Only DAVE successfully learned the optimal policy, while most other algorithms were trapped in the suboptimal policy with payoff 10. QPLEX can occasionally converge to the optimal policy, but in most cases it still cannot learn the optimal solution. Besides, to examine the contribution of the anti-ego exploration to the DAVE framework, we implemented DAVE without exploration in this environment. The experimental results show that DAVE without anti-ego exploration still performs better than other baselines, but is significantly inferior to DAVE. Therefore, for some complex tasks, the anti-ego exploration mechanism can assist the agent in exploring novel state-action pairs and avoiding being restricted to suboptimal solutions.

### StarCraft II

SMAC  is the most popular experimental platform for multi-agent micromanagement, based on the famous real-time strategy game StarCraft II. SMAC offers a diverse range of mini-scenarios, including homogeneous, heterogeneous, symmetric, and asymmetric problems. The goal of each task is to direct agents to defeat enemy units controlled by built-in heuristic AI. Agents can only access information within their own visual range and share a reward function. So we assess the ability of DAVE to solve complex multi-agent cooperation problems in this environment.

The version of StarCraft II used in this paper is 4.10 and performance is not comparable between versions. We test the proposed method in different scenarios. In addition to the above-mentioned methods, baselines also include IGM-Free QMIX that simply does not take the absolute value of the weight of the mixing network and keeps others unchanged. Both IGM-Free QMIX and FACMAC-nonmonotonic are IGM-free methods, but do not have convergence guarantees. The performance comparison between above two IGM-free methods and DAVE that is also IGM-free can reflect the necessity of our proposed ego policy network. In addition, to balance exploration and exploitation, we disable the anti-ego exploration mechanism in some easy scenarios and use it only in hard scenarios.

The experimental results are shown in Figure 6. Undoubtedly, IGM-Free QMIX and FACMAC-nonmonotonic fail to solve almost all tasks. Without the IGM assumption, QMIX is unable to find the correct update direction. Therefore, the assistance of the ego policy network in the DAVE framework is essential for the convergence of IGM-free algorithms. QPLEX performs best in some easy scenarios but fails to solve the task in most hard scenarios. Weighted QMIX performs well in the _MMM2_ scenario but has mediocre performance in most other scenarios. Although DAVE is not the best in easy maps, it still shows competitive performance. In hard scenarios, such as _5m_vs_6m_, _8m_vs_9m_ and _10m_vs_11m_, DAVE performs significantly better than other algorithms. We believe

Figure 6: Performance comparison with baselines in different scenarios.

that it may be that the characteristics in these scenarios are far from the IGM assumptions, resulting in low sample efficiency of conventional value decomposition methods. Moreover, because DAVE is IGM-free, it can guide the agent to learn the excellent policy quickly without restriction. Especially in the _6h_vs_8\(g\) scenario, only DAVE got a non-zero median test win ratio. In summary, even if the IGM principle is completely abandoned, DAVE can still successfully solve complex tasks and even outperforms existing IGM-based value decomposition algorithms in certain challenging scenarios. **MORE** experimental results on other complex environments such as SMACv2  and Multi-Agent MuJoCo  can be found in Appendix D. These results demonstrate the excellent generalization of DAVE to different algorithms and environments.

### Ablation Study

Finally, we evaluate the performance of DAVE under different hyperparameter settings in SMAC scenarios. We focus on the number of samples \(M\) and the initial value of the exploration coefficient \(_{}\). According to Appendix A, increasing the number of samples \(M\) improves the probability of drawing the optimal joint action. Nonetheless, due to time and space complexity constraints, it is crucial to find an appropriate value for the hyperparameter \(M\). Then we test DAVE with different \(M\) values in three scenarios and present results in Figure 7. The results indicate that a larger \(M\) leads to a faster learning speed for DAVE, which aligns with our intuition and proof. DAVE with \(M=1\) fails in all scenarios. When \(M\) increases to 10, the performance of DAVE is already considerable. However, as \(M\) increases further, the performance enhancement it provides diminishes.

The equilibrium between exploration and exploitation is directly related to the initial value of the exploration coefficient \(_{}\). The final value \(_{}\) and the annealing period also affect the exploration, but only \(_{}\) is discussed in this paper. \(_{}\) is generally set to 0, so DAVE is only affected by anti-ego exploration in the early stages of training. Figure 8 shows the learning curve of DAVE under different \(_{}\). \(_{}=0\) means that DAVE does not perform anti-ego exploration. The addition of anti-ego exploration in easy scenarios will prevent the algorithm from exploiting better in the early stages, which will significantly slow down the training speed. Conversely, DAVE with high \(_{}\) values can achieve superior performance in hard scenarios. So a moderately high \(_{}\) is beneficial for DAVE to visit key state-action pairs and converge to the optimal solution. In summary, appropriate values of \(M\) and \(_{}\) are indispensable for DAVE.

It would be valuable to explore the performance of DAVE with a monotonic mixing network. As shown in Figure 9 and Figure 10, DAVE with a monotonic mixing network still cannot converge to the global optimal solution in matrix games. Since the ego policy is updated by an explicit search procedure, DAVE with a monotonic mixing network has more chances to converge to the optimal solution than vanilla QMIX. However, limited by fewer function families that can be represented, the performance of DAVE with a monotonic mixed network is much worse than that of DAVE and is unstable. In addition, in the complex SMAC environment, the performance of DAVE with a monotonic mixing network is similar to or even better than that of QMIX, but worse than that of

Figure 8: Results of DAVE with different \(_{}\). The action space for the three scenarios gradually increases from left to right.

Figure 7: Influence of the sample size \(M\) for DAVE.

vanilla DAVE. This is also substantial proof that the unconstrained factored mixing network indeed enhances performance.

## 6 Conclusion and Discussion

Inspired by the concept of dual self-awareness in psychology, we propose the IGM-free value decomposition framework DAVE in this paper. By dividing the agent network into two parts, ego policy and alter ego value function, DAVE is the first value decomposition method that completely abandons the IGM assumption. We find that the larger the number of samples in the explicit search procedure, the better the algorithm converges to the global optimum, but this will lead to high computational complexity. Therefore, we propose the anti-ego exploration mechanism, which can effectively prevent DAVE from falling into local optima, especially in problems with large action spaces. Experimental results in some didactic problems and complex environments show that DAVE outperforms existing IGM-based value decomposition methods. We believe that DAVE provides a new perspective on addressing the problems posed by the IGM assumption in value decomposition methods. In our future research, we intend to concentrate on how to choose the exploration coefficients in an adaptive manner. Further study of the issue would be of interest.