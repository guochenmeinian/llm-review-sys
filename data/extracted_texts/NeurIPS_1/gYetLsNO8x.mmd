# Best Arm Identification with Fixed Budget:

A Large Deviation Perspective

 Po-An Wang

EECS

KTH, Stockholm, Sweden

wang9@kth.se

&Ruo-Chun Tzeng

EECS

KTH, Stockholm, Sweden

rctzeng@kth.se

&Alexandre Proutiere

EECS and Digital Futures

KTH, Stockholm, Sweden

alepro@kth.se

###### Abstract

We consider the problem of identifying the best arm in stochastic Multi-Armed Bandits (MABs) using a fixed sampling budget. Characterizing the minimal instance-specific error probability for this problem constitutes one of the important remaining open problems in MABs. When arms are selected using a static sampling strategy, the error probability decays exponentially with the number of samples at a rate that can be explicitly derived via Large Deviation techniques. Analyzing the performance of algorithms with adaptive sampling strategies is however much more challenging. In this paper, we establish a connection between the Large Deviation Principle (LDP) satisfied by the empirical proportions of arm draws and that satisfied by the empirical arm rewards. This connection holds for any adaptive algorithm, and is leveraged (i) to improve error probability upper bounds of some existing algorithms, such as the celebrated SR (Successive Rejects) algorithm (Audibert et al., 2010), and (ii) to devise and analyze new algorithms. In particular, we present CR (Continuous Rejects), a truly adaptive algorithm that can reject arms in _any_ round based on the observed empirical gaps between the rewards of various arms. Applying our Large Deviation results, we prove that CR enjoys better performance guarantees than existing algorithms, including SR. Extensive numerical experiments confirm this observation.

## 1 Introduction

We study the problem of best-arm identification in stochastic bandits in the fixed budget setting. In this problem, abbreviated by BAI-FB, a learner faces \(K\) distributions or arms \(_{1},,_{K}\) characterized by their unknown means \(=(_{1},,_{K})\) (we restrict our attention to distributions taken from a one-parameter exponential family). She sequentially pulls arms and observes samples of the corresponding distributions. More precisely, in round \(t 1\), she pulls an arm \(A_{t}=k\) selected depending on previous observations and observes \(X_{k}(t)\) a sample of a \(_{k}\)-distributed random variable. \((X_{k}(t),t 1,k[K])\) are assumed to be independent over rounds and arms. After \(T\) arm draws, the learner returns \(\), an estimate of the best arm \(1():=_{k}_{k}\). We assume that the best arm is unique, and denote by \(\) the set of parameters \(\) such that this assumption holds. The objective is to devise an adaptive sampling algorithm minimizing the error probability \(_{}[ 1()]\). This learning task is one of the most important problems in stochastic bandits, and despite recent research efforts, it remains largely open (Qin, 2022). In particular, researchers have so far failedat characterizing the minimal instance-specific error probability. This contrasts with other basic learning tasks in stochastic bandits such as regret minimization (Lai and Robbins, 1985) and BAI with fixed confidence (Garivier and Kaufmann, 2016), for which indeed, asymptotic instance-specific performance limits and matching algorithms have been derived. In BAI-FB, the error probability typically decreases exponentially with the sample budget \(T\), i.e., it scales as \((-R()T)\) where the instance-specific rate \(R()\) depends on the sampling algorithm. Maximizing this rate over the set of adaptive algorithms is an open problem.

**Instance-specific error probability lower bound.** To guess the maximal rate at which the error probability decays, one may apply the same strategy as that used in regret minimization or BAI in the fixed confidence setting: (i) derive instance-specific lower bound for the error probability for some notion of _uniformly good_ algorithms; (ii) devise a sampling strategy mimicking the optimal proportions of arm draws identified in the lower bound. Here the notion of uniformly good algorithms is that of _consistent_ algorithms. Under such an algorithm, for any \(\), \(_{}[i=1()] 1\) as \(T\). (Garivier and Kaufmann, 2016) conjectures the following asymptotic lower bound satisfied by any consistent algorithm (refer to Appendix J for details): as \(T\),

\[_{}[i 1() ]}_{}_{()}(,), \]

where \(\) is the \((K-1)\)-dimensional simplex, \((,)=_{k=1}^{K}_{k}d(_{k},_{k})\), \(()=\{:1() 1()\}\) is the set of confusing parameters (those for which \(1()\) is not the best arm), and \(d(x,y)\) denotes the KL divergence between two distributions of parameters \(x\) and \(y\). Interestingly, the solution \(^{*}\) of the optimization problem \(_{}_{()}(,)\) provides the best static proportions of arm draws. More precisely, an algorithm selecting arms according to the allocation \(^{*}\), i.e., selecting arm \(k\)\(_{k}^{*}T\) times and returning the best empirical arm after \(T\) samples, has an error rate matching the lower bound (1). This is a direct consequence of the fact that, under a static algorithm with allocation \(\), the empirical reward process \(\{}(t)\}_{t 1}\) satisfies a LDP with rate function \((,)\), see (Glynn and Juneja, 2004) and refer to Section 3 for more details.

**Adaptive sampling algorithms and their analysis.** The optimal allocation \(^{*}\) depends on the instance \(\) and is initially unknown. We may devise an adaptive sampling algorithm that (i) estimates \(^{*}\) and (ii) tracks this estimated optimal allocation. In the BAI with fixed confidence, such tracking scheme exhibits asymptotically optimal performance (Garivier and Kaufmann, 2016). Here however, the error made estimating \(^{*}\) would inevitably impact the overall error probability of the algorithm. To quantify this impact or more generally to analyze the performance of adaptive algorithms, one would need to understand the connection between the statistical properties of the arm selection process and the asymptotic statistics of the estimated expected rewards.

To be more specific, any adaptive algorithm generates a stochastic process \(\{Z(t)\}_{t 1}=\{((t),}(t))\}_{t 1}\). \((t)=(_{1}(t),,_{K}(t))\) represents the allocation realized by the algorithm up to round \(t\) (\(_{k}(t)=N_{k}(t)/t\) and \(N_{k}(t)\) denotes the number of times arm \(k\) has been selected up to round \(t\)). \(}(t)=(_{1}(t),,_{K}(t))\) denotes the empirical average rewards of the various arms up to round \(t\). Now assuming that at the end of round \(T\), the algorithm returns the arm with the highest empirical reward, the error probability is \(_{}[ 1()]=_{}[}(T)()]\). Assessing the error probability at least asymptotically requires understanding the asymptotic behavior of \(}(t)\) as \(t\) grows large. Ideally, one would wish to establish the Large Deviation properties of the process \(\{Z(t)\}_{t 1}\). This task is easy for algorithms using static allocations (Glynn and Juneja, 2004), but becomes challenging and open for adaptive algorithms. Addressing this challenge is the main objective of this paper.

**Contributions.** In this paper, we develop and leverage tools towards the analysis of adaptive sampling algorithms for the BAI-FB problem. More precisely, our contributions are as follows.

(a) We establish a connection between the LDP satisfied by the empirical proportions of arm draws \(\{(t)\}_{t 1}\) and that satisfied by the empirical arm rewards. This connection holds for any adaptive algorithm. Specifically, we show that if the rate function of \(\{(t)\}_{t 1}\) is lower bounded by \( I()\), then that of \((}(t))_{t 1}\) is also lower bounded by \(_{}\{(, ),I()\}\). This result has interesting interpretations and implies the following asymptotic upper bound on the error probability of the algorithm considered: as \(T\),

\[_{}[i 1() ]}_{,()} \{(,),I()\}. \]The above formula, when compared to the lower bound (1), quantifies the price of not knowing \(^{*}\) initially, and relates the error probability to the asymptotic statistics of the sampling process used by the algorithm.

(b) We show that by simply applying our generic Large Deviation result, we may improve the error probability upper bounds of some existing algorithms, such as the celebrated SR algorithm (Audibert et al., 2010). Our result further opens up opportunities to devise and analyze new algorithms with a higher level of adaptiveness. In particular, we present CR (Continuous Rejects), an algorithm that, unlike SR, can eliminate arms in _each_ round. This sequential elimination process is performed by comparing the empirical rewards of the various candidate arms using continuously updated thresholds. Leveraging the LDP tools developed in (a), we establish that CR enjoys better performance guarantees than SR. Hence CR becomes the algorithm with the lowest instance-specific and guaranteed error probability. We illustrate our results via numerical experiments, and compare CR to other BAI algorithms.

## 2 Related Work

We distinguish two main classes of algorithms to solve the best arm identification problem in the fixed budget setting. Algorithms from the first class, e.g. Successive Rejects (SR) (Audibert et al., 2010) and Sequential Halving (SH) (Karnin et al., 2013), split the sampling budget into phases of fixed durations, and discard arms at the end of each phase. Algorithms from the second class, e.g. UCB-E (Audibert et al., 2010) and UGapE (Gabillon et al., 2012) sequentially sample arms based on confidence bounds of their empirical rewards. It is worth mentioning that algorithms from the second class usually require some prior knowledge about the problem, for example, an upper bound of \(H=_{k 1()}-_{k})^{2}}\). Without this knowledge, the parameters can be chosen in a heuristic way, but the performance gets worse.

Algorithms from the first class exhibit better performance numerically and are also those with the best instance-specific error probability guarantees. SR had actually the best performance guarantees so far: for example, when the reward distributions are supported on \(\), the error probability of SR satisfies: \(_{T}_{}[i 1( )]} K}\), where \(H_{2}=_{k 1()})}-_{k})^{2}}\). In this paper, we strictly improve this guarantee (see Section 3.4). Recently, (Barrier et al., 2022) also refined and extended the analysis of (Audibert et al., 2010) by replacing, in the analysis, Hoeffding's inequality by a large deviation result involving KL-divergences. Again here, we further improve this new guarantee. We also devise CR, an algorithm with error probability provably lower than our improved guarantees for SR.

Fundamental limits on the error probability have also been investigated. In the minimax setting, (Carpentier and Locatelli, 2016) established that for any algorithm, there exists a problem within the class of instances with given complexity \(H\) such that the error probability is greater than \((-)(- K})\). Up to a universal constant (here 400), SR is hence minimax optimal. This lower bound was also recently revisited in (Ariu et al., 2021; Degenne, 2023; Wang et al., 2023) to prove that the instance-specific lower bound (1) cannot be achieved on all instances by a single algorithm. Deriving tight instance-specific lower bounds remains open (Qin, 2022).

We conclude this section by mentioning two interesting algorithms. In (Komiyama et al., 2022), the authors propose DOT, an algorithm trying to match minimax error probability lower bounds. To this aim, the algorithm requires to periodically call an oracle able to determine an optimal allocation, solution of an optimization problem with high and unknown complexity. DOT has minimax guarantees but is computationally challenging if not infeasible (numerically, the authors cannot go beyond simple instances with 3 arms). Finally, researchers have also looked at the best arm identification problem from a Bayesian perspective. For example, (Russo, 2016) devise variants of the celebrated Thompson Sampling algorithm, that could potentially work well in practice. Nevertheless, as discussed in (Komiyama, 2022), Bayesian algorithms cannot be analyzed nor provably perform well in the frequentist setting.

[MISSING_PAGE_FAIL:4]

below. If it holds for adaptive algorithms, we show, in Appendix I, that when \(K>3\), no algorithm can attain the instance-specific lower bound (1) for all parameters.

_(b) Theorem 1 is tight for static sampling algorithms._ When the sampling rule is static, namely \((t)=\), then \(\{(t)\}_{t 1}\) satisfies a LDP with rate function \(I\) defined as \(I()=0\) and \(\) elsewhere. Theorem 1 with \(W=\) states that \(\{}(t)\}_{t 1}\) satisfies the LDP upper bound (3) with rate function \(F_{}\). In fact, as shown by (Glynn and Juneja, 2004), \(\{}(t)\}_{t 1}\) satisfies a complete LDP with this rate function.

_(c) A useful corollary._ From Theorem 1, we have:

\[_{t}_{}[ {}(t),(t) W]}_{ (W)}F_{}().\]

From there, we will be able to improve the performance guarantee for SR.

### Proof of Theorem 1

Proof.: Observe that when \(\) or \(W\) is empty, the result holds. Now recall that \(F_{}()=_{()}(,)\) is the infimum of a family of linear functions on a compact set, \(\), hence it is upper bounded. Denote \(u>0\) such an upper bound. \(F_{}()\) is also continuous (see Appendix F for details). For each integer \(N\), we define a collection of closed sets:

\[W_{n}^{N}=\{(W): F_{}()\}, n[N]. \]

We observe that:

\[_{}[}(t), (t) W] _{n=1}^{N}_{}[}(t) ,(t) W_{n}^{N}]\] \[ N_{n[N]}_{}[}(t) ,(t) W_{n}^{N}].\]

Taking the logarithm on both sides and dividing them by \(-t\) yields that

\[_{t}_{}[}(t),(t) W]} _{t}_{n[N]}_{}[}(t),(t) W _{n}^{N}]}\] \[=_{n[N]}_{t}_{}[}(t),(t) W _{n}^{N}]}\] \[_{n[N]}\{,_{  W_{n}^{N}}I()\}, \]

where the last inequality follows from Lemma 1. Since for all \(n[N]\),

\[\{,_{ W_{n}^{N}}I() \}=_{ W_{n}^{N}}\{,I()\},\]

the r.h.s. of (6) is equal to

\[_{n[N]}_{ W_{n}^{N}}\{,I()\} _{n[N]}_{ W_{n}^{N}}\{F_{ }(),I()\}-u/N\] \[=_{(W)}\{F_{}(),I()\}-u/N,\]

where the first inequality is due to (5). As \(N\) can be taken arbitrarily large, we conclude this theorem.

**Lemma 1**.: _For any \(N,\,n[N]\),_

\[_{t}_{}[}(t),(t) W_{n}^{N}]}\{,_{ W_{n}^{N}}I()\}\]Proof.: Recall \(F_{S}()=_{(S)}(,)\). We deduce that

\[_{}[}(t),(t) W_{ n}^{N}]_{}[X tF_{S}((t)),(t) W_{n}^{N}],\]

where \(X\) denotes \(t(}(t),(t))\) for short. Let \((0,1)\), Markov's inequality implies that

\[_{}[X tF_{S}(),( t) W_{n}^{N}] =_{}[\{(t) W_{n}^ {N}\}e^{(X-tF_{S}((t)))} 1]\] \[_{}[\{(t) W_{ n}^{N}\}e^{(X-tF_{S}((t)))}]\] \[_{}[\{(t) W_{ n}^{N}\}e^{ X}]e^{-}, \]

where the last inequality uses the definition of \(W_{n}^{N}\) (see (5)). By applying Holder's inequality with \(p,q\), where \(p[1,1/)\) and \(q=p/(p-1)\) on r.h.s. of (7), we deduce that \(_{}[}(t),(t)  W_{n}^{N}]\) is at most

\[(_{}[e^{ pX}])/p+(_{}[\{(t) W_{n}^{N}\}])/q-.\]

As \( p(0,1)\), Lemma 2 in Appendix B shows that the first term above is \(o(t)\). Using definition of the rate function, (3) with \(C=W_{n}^{N}\), on the second term yields that \(_{t}_{}[ }(t),(t) W_{n}^{N}]}\) is lower bounded by

\[(1/q)_{ W_{n}^{N}}I()+=(1- 1/p)_{ W_{n}^{N}}I()+.\]

As \(p\) can be arbitrarily close to \(1/\), we get the lower bound \((1-)I()+\). Further choosing \(\) close to either \(1\) or \(0\), the proof is completed.

### Improved analysis of the Successive Rejects algorithm

In SR, the set of candidate arms is initialized as \(_{K}=[K]\). The budget of samples is partitioned into \(K-1\) phases, and at the end of each phase, SR discards the empirical worst arm from the candidate set. In each phase, SR uniformly samples the arms in candidate set. The lengths of phases are set as follows. Define \(K:=+_{k=2}^{K}\). The candidate set is denoted by \(_{j}\) when it has \(j>2\) arms. In the corresponding phase, (i) each arm in \(_{j}\) is sampled until the round \(t\) when \(_{k_{j}}N_{k}(t)\) reaches \(T/(jK)\) (recall that \(N_{k}(t)\) is the number of times arm \(k\) has been sampled up to round \(t\)); (ii) the empirical worst arm, denoted by \(_{j}\), is then discarded, i.e., \(_{j-1}=_{j}\{_{j}\}\). During the last phase, the algorithm equally samples the two remaining arms and finally recommends \(\), the arm with higher empirical mean in \(_{2}\). The pseudo code is presented in Algorithm 1.

```
initialization\(_{K}[K],j K\); for\((t=1,,T)\)do if(\(j>2\) and \(_{k_{j}}N_{k}(t)K}\))then \(_{j}*{argmin}_{k_{j}}_{k}(t)\) (tie broken arbitrarily), \(_{j-1}_{j}\{_{j}\}\), and \(j j-1\) ;  end if  sample \(A_{t}*{argmin}_{k_{j}}N_{k}(t)\) (tie broken arbitrarily), update \(\{N_{k}(t)\}_{k_{j}}\) and \(}(t)\);  end for \(_{2}*{argmin}_{k_{2}}_{k}(T)\) and return \(*{argmax}_{k_{2}}_{k}(T)\) (tie broken arbitrarily).
```

**Algorithm 1**SR

We apply the corollary (c) in Section 3.2 to improve the existing performance guarantees of SR. To simplify the presentation, we assume wlog that \(_{1}>_{2}_{K}\). For \(j=2,,K\), define

\[_{j}=_{J}\{_{k J}d(_{k},_{k}): ^{K},_{1}_{k J}_{k}\}, \]

where \(=\{J[K]:|J|=j,1 J\}\).

**Theorem 2**.: _Let \(\). Under SR, we have: for \(j=2,,K\), \(_{T}_{}[_{j}=1] }}{K}\). Hence, the error probability of SR is upper bounded by \(_{T}_{}[\#1]} _{j 1}}{K}\)._

_Proof of Theorem 2._ Fix \(j\{2,,K\}\). Observe that

\[_{}[_{j}=1]=_{J}_ {}[_{j}=1,_{j}=J]| |_{J}_{}[_{j}=1,_ {j}=J],\]

which implies that

\[_{T}_{}[_ {j}=1]}_{J}_{T} {1}{_{}[_{j}=1,\,_{j}=J]} \]

as \(||<\).

Since \(_{j}\) is selected at the \( T\)-th round1 where \(=(1+_{k=j+1}^{K})/K\), the event \(\{_{j}=1,\,_{j}=J\}\) implies that \(\{}( T),\,( T) W\}\), where

\[=\{^{K}:_{1}_{k},  k J\}W=\{:_{k}= K}, k J\}.\]

In other words, \(_{}[_{j}=1,\,_{j}=J]_{} [}( T),\,( T) W]\). Applying (c) in Section 3.2 with the above \(\) and \(W\) yields that \(_{T}_{}[ }( T),\,( T) W]}\) is larger than

\[_{ W}_{()}(,)K}\{_{k  J}d(_{k},_{k}):_{1}_{k J}_{k}\} }{ jK}, \]

where the first inequality uses the fact that KL-divergences and the components of \(\) are nonnegative, and the second one is due to the definition (8) of \(_{j}\). Combining (9) and (10) completes the proof.\(\)

The upper bound derived in Theorem 2 is tighter than those recently derived in (Barrier et al., 2022). Indeed, for any \(J\), since \(|J|=j\), one can find at least one index in \(J\) at least larger than \(j\), say \(k_{J}\). Hence,

\[_{j}_{J}_{^{K}, _{1}_{kJ}}d(_{1},_{1})+d(_{k_{J}},_{k_ {J}})_{^{K},_{1}_{j}}d( _{1},_{1})+d(_{j},_{j}).\]

The r.h.s. in the previous inequality corresponds to the upper bounds derived by (Barrier et al., 2022).

To simplify the presentation and avoid rather intricate computations involving the KL-divergences, in the remaining of the paper, we restrict our attention to specific classes of reward distributions.

**Assumption 1**.: _The rewards are bounded with values in \((0,1)\). The reward distributions \(_{1},,_{K}\) are Bernoulli distributions such that \(_{a}\) is of mean \(a\), and for any \(a b\), \(d(a,b) 2(a-b)^{2}\) (this is a consequence of Pinsker's inequality as rewards are in \((0,1)\))._

Under Assumption 1, we have \(_{j} 2_{j}\) (a direct consequence of Proposition 3 in Appendix D.1), where for \(j=2,,K\),

\[_{j}=_{^{j}}\{_{k=1}^{j}(_{k}-_ {k})^{2}:_{1}_{k=1,,j}_{k}\}.\]

We give an explicit expression of \(_{j}\) in Proposition 1, presented in Appendix D.1. Moreover, \(2_{j}\) is clearly larger than \(2_{_{1}_{j}}\{(_{1}-_{1})^{2}+(_{j }-_{j})^{2}\}=(_{1}-_{j})^{2}\), and hence \(_{j 1}_{j}/(jK)_{j 1}(_{1}-_{j}) ^{2}/(jK)\). This implies that our error probability upper bound is better than that derived in (Audibert et al., 2010).

**Example 1**.: To illustrate the improvement brought by Theorem 2 on the performance guarantees of SR, consider the simple example with 3 Bernoulli arms and \(=(0.9,0.1,0.1)\). Then \(_{j 1}(_{1}-_{j})^{2}/(j)=0.16\) for the upper bound presented in (Audibert et al., 2010). From Proposition 1, instead we get \(_{j 1}2_{j}/(j)=0.21\).

Continuous Rejects Algorithms

In this section, we present CR, a truly adaptive algorithm that can discard an arm in _any_ round. We propose two variants of the algorithm, CR-C using a conservative criterion to discard arms and CR-A discarding arms more aggressively. Using the Large Deviation results of Theorem 1, we establish error probability upper bounds for both CR-C and CR-A.

### The CR-C and CR-A algorithms

As SR, CR initializes its candidate set as \(_{K}=[K]\). For \(j 2\), \(_{j}\) denotes the candidate set when it is reduced to \(j\) arms. When \(j>2\), the algorithm samples arms in the candidate set \(_{j}\) uniformly until a _discarding condition_ is met. The algorithm then discards the empirically worst arm \(_{j}_{j}\), i.e., \(_{j-1}_{j}\{_{j}\}\). More precisely, in round \(t\), if there are \(j\) candidate arms remaining and if \((t)\) denotes the empirically worst candidate arm, the discarding condition is \(N_{(t)}(t)>_{k_{j}}N_{k}(t)\), \(( k_{j},\,N_{(t)}(t)=N_{k}(t))\)2, and

\[_{k_{j},k(t)} {}_{k}(t)-_{(t)}(t) G(_{j} }N_{k}(t)j}{T-_{k_{j}}N_{k}(t)}), \] \[_{j},k(t )}_{k}(t)}{j-1}-_{(t)}(t) G(_{j}}N_{k}(t)j}{T-_{k_{j}}N_{k }(t)}), \]

where \(G()=1/-1\) for all \(>0\). The idea behind (11) is to keep the probability of discarding the best arm at most smaller than that of SR while using less budget. Note that (12) is easier to achieve than (11). CR-A is hence more aggressive than CR-C, and reduces the set of arms to \(_{2}\) faster, but at the expense of a higher risk. After discarding \(_{3}\), CR will sample the arms in \(_{2}\) evenly, and recommend the empirical best arm in \(_{2}\) when the budget is exhausted. The pseudo-code of CR is presented in Algorithm 2.

```
Input:\(_{0}(0,)\) independent of \(T\) (can be chosen as small as one wishes) initialization \(_{K}[K]\), \(j K\), sample each arm \(k[K]\) once, update \(\{N_{k}(t)\}_{k_{K}}\) and \(}(t)\); for\(t=K+1,,[_{0}T]\)do  sample \(A_{t}*{argmin}_{k_{j}}N_{k}(t)\) (tie broken arbitrarily), update \(\{N_{k}(t)\}_{k_{j}}\) and \(}(t)\);  end for for\((t=_{0}T+1,,T)\)do \((t)*{argmin}_{k_{j}}_{k}(t)\) (tie broken arbitrarily); if\(j>2\), \(N_{(t)}(t)>_{k_{j}}N_{k}(t)\), \(( k_{j},\,N_{(t)}(t)=N_{k}(t))\), and (11) (_resp._ (12)) _holds for_ CR-C _(_resp._ CR-A)then \(_{j}(t)\), \(_{j-1} C_{j}\{_{j}\}\), \(j j-1\)  sample \(A_{t}*{argmin}_{k_{j}}N_{k}(t)\) (tie broken arbitrarily), update \(\{N_{k}(t)\}_{k_{j}}\) and \(}(t)\);  end for \(_{2}(T)\); return \(*{argmax}_{k_{2}}_{k}(T)\) (tie broken arbitrarily).
```

**Algorithm 2**CR-C and CR-A

### Analysis of CR-C and CR-A

As in Section 3.4, \(_{1}>_{2}_{K}\) is assumed wlog and we further define \(_{K+1}=0\). We introduce the following instance-specific quantities needed to state our error probability upper bounds. For \(j\{2,,K\}\), define

\[_{j}=(_{1}-^{j}_{k}}{j-1})^{ 2},_{j}=(_{1}-^{j-1}_{ k}+_{j+1}}{j-1})^{2},_{j}=_{j}-_{j+1},\]\[_{j}=^{j}_{k}}{j}-_{j+1},_{j}=\{_{k=1}^{K}(_{k}-_{k})^{2}: ^{K},\,_{1}_{k=2,,j-1,j+1}_{k}\}.\]

Here we remark \(_{j}_{j}\) and \(_{j}_{j}\). These inequalities are proven in Proposition 3 and Proposition 5 in Appendix D.

**Theorem 3**.: _Let \(^{K}\). Under \(-\), \(_{T}\,\,\,_{ }[i 1]}\) is larger than_

\[2_{j=2,,K}\{)\,1_{\{j K\}}}}{ j},_{j}\},_{j}\}}{ jK}\},\]

_where \(_{j}\) is the real number such that \((1-_{j})}{jj}=[((1+_{j})}-(j+1)}})_{+}]^{2}\)._

**Theorem 4**.: _Let \(^{K}\). Under \(-\), \(_{T}\,\,\,_{ }[i 1]}\) is larger than_

\[2_{j=2,,K}\{(j+1)(1-_{j})\,1_{\{j K\}}}}}{ j},_{ j}\}}{jK}}{jK}\},\]

_where \(_{j}\) is the real number such that \((1-_{j})}{jj}=[((1+_{j} )}-(j+1)}})_{+}]^{2}\)._

Note that Theorem 3 implies that CR-C enjoys better performance guarantees than SR, and hence has for now the best known error probability upper bounds.

Proof sketch.: The complete proof of Theorems 3 and 4 are given in Appendices C.1 and C.2. We sketch that of Theorem 3. The proof consists in upper bounding \(_{}[_{j}=1]\) for \(j\{2,,K\}\). We focus here on the most challenging case where \(j\{3,,K-1\}\) (the analysis is simpler when \(j=K\), since the only possible allocation is uniform, and when \(j=2\), since the only possible round deciding \(_{2}\) is the last round).

To upper bound \(_{}[_{j}=1]\) using Theorem 1, we will show that it is enough to study the large deviations of the process \(\{( T)\}_{T 1}\) for any fixed \([_{0},1]\) and to define a set \(^{K}\) under which \(_{j}=( T)=1\). We first observe that \(_{j}=( T)\) restricts the possible values of \(( T)\): \(( T)_{j}\,:=\,\{ :[K]^{2}x_{(1)}==x_{(j)}>x_{(j+1)}>>x_{(K)}>0 \}.\) We can hence just derive the LDP satisfied by \(\{( T)\}_{T 1}\) on \(_{j}\). This is done in Appendix E, and we identify by \(I_{}\) a rate function leading to an LDP upper bound. By defining

\[_{j,i}()=\{_{j}: x_{ (i)}ii>1-_{k=i+1}^{K}x_{(k)}\},\,  i\{j,,K\},\]

As it is shown in Appendix E.1 that \(I_{}()=\) if \(_{j,i}()\) for \(i j\), we may further restrict to \(_{j}_{i=j}^{K}_{j,i}()\).

Next, we explain how to apply Theorem 1 to upper bound \(_{}[_{j}=1]\). Let \(=\{J[K]:|J|=j,1 J\}\) as defined in Section 3.4. For all \(,(0,1]\) and \(J\), we introduce the sets

\[_{J}() =\{^{K}:_{k J,k 1} _{k}-_{1} G()\},\] \[_{J}(,) =\{_{j}_{i=j}^{K} _{j,i}():( k J,\;z_{k}=_{k^{}[K]}z_{ k^{}}),\;z_{k}j}{1-_{k  J}z_{k}}=\}.\]

Assume that in round \(t\), \(_{j}=(t)=1,_{j}=J\) and let \(=_{k J}N_{k}(t) t\) be the number of times arms outside \(J\) are pulled. While \((t)_{j,j}(t/T)\), we have \(=j}{T-}(0,1]\). Using the criteria (11), we observe that

\[_{t=K+1}^{T}_{J}_{}[_{j}=(t)=1,_{j}=J,(t) _{j}_{i=j}^{K}_{j,i}()]\\ _{t=K+1}^{T}_{J}_{ t, }_{}[}(t) _{J}(j}{T-}),(t)_{J}(,j}{T- })].\]To upper bound the r.h.s. in the above inequality, we combine the results of Theorem 1 and a partitioning technique (presented in Appendix G). This gives:

\[_{T}_{}[}( T)_{J}(),( T)_{J}(,)]}_{(_{J}(,))}\{F_{_{J}()}(),I_{}()\}.\]

The proof is completed by providing lower bounds of \( F_{_{J}()}()\) and \( I_{}()\) for a fixed \(_{J}(,)\) with various \(J\). Such bounds are derived in Appendix D.1 and E.3.1, respectively. 

**Example 2**.: To conclude this section, we just illustrate through a simple example the gain in terms of performance guarantees brought by CR compared to SR. Assume we have 50 Bernoulli arms with \(_{1}=0.95,_{2}=0.85,_{3}=0.2\), and \(_{k}=0\) for \(k=4,,50\). For SR, Theorem 2 states that with a budget of 5000 samples, the error probability of SR does not exceed \(1.93 10^{-3}\). With the same budget, Theorems 3 and 4 state that the error probabilities of CR-C and CR-A do not exceed \(6.40 10^{-4}\) and \(6.36 10^{-4}\), respectively.

We note that in general, we cannot say that one of our two algorithms, CR-C or CR-A, has better guarantees than the other. This is demonstrated in the problem instances presented in Appendix K.1 and K.4.

## 5 Numerical Experiments

We consider various problem instances to numerically evaluate the performance of CR. In these instances, we vary the number of arms from 5 to 55; we use Bernoulli distributed rewards, and vary the shape of the arm-to-reward mapping. For each instance, we compare CR to SR, SH, and UGapE.

Most of our numerical experiments are presented in Appendix K. Due to space constraints, we just provide an example of these results below. In this example, we have 55 arms with _convex_ arm-to-reward mapping. The mapping has 10 steps, and the \(m\)-th step consists of \(m\) arms with same average reward, equal to \( 3^{-}\). Table 1 presents the error probabilities averaged over \(40,000\) independent runs. Observe that CR-A performs better than CR-C (being aggressive when discarding arms has some benefits), and both versions of CR perform better than SR and all other algorithms.

## 6 Conclusion

In this paper, we have established, in MAB problems, a connection between the LDP satisfied by the sampling process (under any adaptive algorithm) and that satisfied by the empirical average rewards of the various arms. This connection has allowed us to improve the performance analysis of existing best arm identification algorithms, and to devise and analyze new algorithms with an increased level of adaptiveness. We show that one of these algorithms CR-C has better performance guarantees than existing algorithms and that it performs also better in practice in most cases.

Future research directions include: (i) developing algorithms with further improved performance guarantees - can the discarding conditions of CR be further optimized? (ii) Enhancing the Large Deviation analysis of adaptive algorithms - under which conditions, can we establish a complete LDP of the process \(\{Z(t)\}_{t 1}=\{((t),}(t))\}_{t 1}\)? Answering this question would constitute a strong step towards characterizing the minimal instance-specific error probability for best arm identification with fixed budget. (iii) Extending our approach to other pure exploration tasks: top-\(m\) arm identification problems (Bubeck et al., 2013), best arm identification in structured bandits (Yang and Tan, 2022; Azizi et al., 2022), or best policy identification in reinforcement learning.

    & & \(T=3,000\) & \(T=4,000\) & \(T=5,000\) \\  UGapE & (Gabillon et al., 2012) & \(24.7\) & \(21.3\) & \(18.9\) \\  SR & Karnin et al. (2013) & \(10.2\) & \(5.9\) & \(3.2\) \\  SR & Audibert et al. (2010) & \(5.5\) & \(2.8\) & \(1.3\) \\  CR–C & (this paper) & \(7.1\) & \(2.6\) & \(1.1\) \\  CR–A & (this paper) & \(\) & \(\) & \(\) \\   

Table 1: Error probability (in %).