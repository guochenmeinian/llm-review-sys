# Imitation guided Automated Red Teaming

Desik Rengarajan\({}^{}\), Sajad Mousavi\({}^{}\), Ashwin Ramesh Babu, Vineet Gundecha,

**Avisek Naug, Sahand Ghorbanpour, Antonio Guillen, Ricardo Luna, Soumyendu Sarkar\({}^{*}\)**

 Hewlett Packard Enterprise (Hewlett Packard Labs) \\ \{desik.rengarajan, sajad.mousavi, ashwin.ramesh-babu, vineet.gundecha, \\ avisek.naug, sahand.ghorbanpour, antonio.guillen, rluna, \\ soumyendu.sarkar\}@hpe.com \\ 

Corresponding author. \(\)These authors contributed equally.

###### Abstract

The potential of large language models (LLMs) is substantial, yet they also carry the risk of generating harmful responses. An automatic "red teaming" process constructs test cases designed to elicit unfavorable responses from these models. A successful generator must provoke undesirable responses from the target LLMs with test cases that exemplify diversity. Current methods often struggle to balance quality (i.e., the harmfulness of responses) and diversity (i.e., the range of scenarios) in testing, typically sacrificing one to enhance the other and relying on non-optimal exhaustive comparison approaches. To address these challenges, we introduce an imitation-guided reinforcement learning approach to learn optimal red teaming strategies that generate both diverse and high-quality test cases without exhaustive searching. Our proposed method, Imitation-guided Automated Red Teaming (iART), is evaluated across various LLMs fine-tuned for different tasks. We demonstrate that iART achieves not only diverse test sets but also elicits undesirable responses from the target LLM in a computationally efficient manner.

**Warning: This paper consists of LLM outputs that are offensive.**

## 1 Introduction

Large Language Models (LLMs) have recently become extremely popular. They have achieved remarkable success in tasks such as text completion, instruction following, and code generation, becoming essential tools in various workflows and daily activities (Jiang et al., 2023; Roziere et al., 2023; Touvron et al., 2023; Achiam et al., 2023). Despite their advanced capabilities, these models can also generate harmful and incorrect content, thus making them prone to such issues as outlined in (Ji et al., 2023; Wei et al., 2023; Perez et al., 2022).

Given the widespread use of LLMs, testing them to prevent the production of harmful or undesirable content is crucial. This process, known as red teaming, involves identifying inputs that generate undesirable content. red teaming is challenging due to the vast range of possible input prompts and generated outputs. A common red teaming approach is using humans to design prompts that elicit undesirable responses from the LLM (Ganguli et al., 2022). However, relying solely on human testers presents various challenges: it is both expensive and time-consuming, limited by testers' domain knowledge, and exposes humans to toxic and harmful content (Radharapu et al., 2023).

Given these challenges, automating the red teaming process has become a key research focus. In particular, reinforcement learning (RL) has emerged as a popular approach for automated red teaming[Perez et al., 2022, Casper et al., 2023, Hong et al., 2024]. In RL-based red teaming, the main idea is to train a separate LLM known as the _attack LLM_ using RL to illicit undesirable responses from the LLM being tested (known as the _target LLM_). The outputs of the target LLM are evaluated using an evaluator module (typically another LLM), and this is used as feedback for training the attack LLM.

There are two main metrics the test cases generated by the attack LLM should satisfy, (1) **Quality:** The test cases generated by the attack LLM should elicit undesirable responses from the target LLM, (2) **Diversity:** The test cases generated by the attack LLM should be diverse., ie., they should cover a wide range of inputs to the target LLM. Methods solely based on RL Perez et al. , Hong et al. , while effective at eliciting undesirable responses, often struggle with generating diverse test cases. As noted by Hong et al. , this lack of diversity stems from the absence of an explicit reward that encourages the attack LLM to generate new test cases, and utilizing RL for training causes the attack LLM to converge to a deterministic policy, leading to the generation of repeated test cases.

Current methods aimed at improving the quality and diversity of the generated test cases are often inadequate and computationally inefficient. For instance, Hong et al.  imposes an explicit penalty during the training process to prevent the generation of previously seen test cases by the attack LLM. This involves comparing the outputs generated at the current training iteration with all of the previously generated outputs, thus making the training process extremely slow.

In this work, we propose Imitation Guided Automated Red Teaming (iART), a novel approach to RL-guided automated red teaming. The goal of **iART** is to simultaneously improve the quality and diversity of the outputs/test cases generated by the attack LLM in a computationally efficient manner. We achieve this using two innovative components. **First**, inspired by imitation learning, we _indirectly_ guide the training of the attack LLM using examples of undesirable responses we want the target LLM to generate. These examples demonstrate the range of behaviors that we want to test our target LLM on. Thus using these different examples for guidance helps us improve both the quality and diversity of the outputs generated by the attack LLM. **Second**, to further enhance the diversity of the attack LLM, we train a diversity module to model the distribution of previously generated outputs of the attack LLM. We then use this module to penalize the attack LLM from generating previously generated outputs, thus enhancing diversity. Our approach avoids the computationally inefficient method of exhaustively scanning through previously generated outputs to impose a penalty.

We evaluate our approach on text-continuation and instruction-following tasks using different target LLMs. For all the experiments, we use the 137M GPT-2 model as our attack LLM. We successfully elicit undesirable responses from much larger LLMs, such as Mistral-7B and Dolly-3B. Our approach outperforms all baselines in both quality and diversity. We find that our proposed method balances high-quality and diverse outputs across a range of tasks. Additionally, our algorithm is significantly more computationally efficient compared to existing methods that aim to improve both metrics. Overall, our approach enhances quality, diversity, and computational efficiency.

## 2 Related Work

**Learning from demonstrations and Imitation Learning:** The concept of learning from demonstrations involves leveraging demonstration data to aid the learning process [Schaal, 1996]. This approach, along with imitation learning, is popular in the RL domain [Hester et al., 2018, Nair et al., 2018]. It is particularly beneficial for applications like robotics [Vecerik et al., 2017, Rajeswaran et al., 2017], where defining a reward function can be challenging, but obtaining demonstrations is relatively easy. These methods have proven to be valuable in environments where exploration is difficult due to weak reward signals [Kang et al., 2018, Yang et al., 2023]. In this work, we extend the idea of learning from demonstrations and imitation learning to help us train an attack LLM that can elicit undesirable responses from a given target LLM.

**Adversarial Attacks and Red Teaming on LLMs:** Adversarial attacks aim to discover inputs that prompt a target LM to produce undesirable responses. Alzantot et al. , Garg and Ramakrishnan , Li et al.  investigate adversarial attacks on LLMs by focusing on word perturbations. These perturbations are designed to cause the target LM to generate undesirable outputs while preserving the original semantic meaning of the input. These approaches are called black-box attacks, as the algorithm cannot access the target LLM parameters. On the other hand, Wallace et al. , Zou et al. , Wichers et al.  concentrate on white-box attacks, aiming to create adversarialprompts where the attacker has access to the weights or parameters of the target LLM. In a different approach, Deng et al. (2023); Mehrabi et al. (2023); Radharapu et al. (2023) utilize instruction and in-context learning-based methods to generate adversarial examples.

**RL-based Automated Red Teaming:**Perez et al. (2022) investigate the concept of automatically identifying instances where a target LLM exhibits harmful behavior by generating test cases using another LLM, employing methods such as RL and zero-shot learning. Casper et al. (2023) propose a red teaming pipeline where they fine-tune the evaluator function based on the outputs of the target model. Additionally, to prevent model collapse, they utilize a constraint based on the target LM's embeddings of the generated prompts. Hong et al. (2024) further extend these approaches by employing computationally intensive techniques (see Sections 4 and 5) to enhance the diversity and effectiveness of test cases.

Given the recent success of RL-based approaches for red teaming, our work focuses on refining these methods through established techniques in RL and imitation learning. Our approach differs from existing RL-based automated red teaming methods as we employ computationally efficient techniques to simultaneously enhance the diversity and effectiveness of test cases. Further, we integrate the concept of imitation learning into automated red teaming.

## 3 Preliminaries

In RL-based red teaming, we train a red teaming model, also known as an attack LLM \(\), to induce a target LLM \(p\) to generate undesirable outputs. The undesirability of these outputs is measured by an evaluator function \(R\)(Hong et al., 2024; Perez et al., 2022). Formally, given a prompt \(x\), the target LLM \(p\) generates a response \(y p(|x)\). The objective in RL-based red teaming is to train the attack LLM \(\) to generate a prompt \(x(|z)\) for a specific instruction \(z\), aiming to maximize the undesirability of the target LLM's response \(R(y)\). Additionally, we incorporate a Kullback-Leibler (KL) divergence penalty between the attack model \(\) and a reference model \(_{}\) to prevent model drift (Ouyang et al., 2022). The RL-based red teaming objective is summarized as follows:

\[_{} [R(y)- D_{KL}((|z)||_{}(|z))] \] \[z,x(|z),y p(|x)\]

Here, \(\) represents a dataset of input prompts or instructions for the attack LLM, and \(\) denotes the KL penalty coefficient.

## 4 Imitation Guided Automated Red Teaming

RL-based red teaming methods struggle to balance the quality and diversity of attack LLM outputs. Techniques such as adding randomness to the attack LLM's generation, incorporating an entropy bonus to encourage exploration, adjusting the KL penalty \(\), or increasing the sampling temperature have been shown to improve either quality or diversity, but at the expense of the other (Hong et al., 2024). Further, current techniques to improve both metrics involve exhaustive computations, making them computationally inefficient (Hong et al., 2024).

Our approach aims to address both metrics of quality and diversity simultaneously in a computationally efficient manner. We accomplish this by introducing two novel components.

### Imitation Guidance

To enhance the quality and diversity of the attack LLM's outputs, we aim to _indirectly_ guide the training of the attack LLM using examples of undesirable outputs. We assume that we have access to

Figure 1: Imitation guided automated red teaming workflow.

a dataset that consists of undesirable outputs \(_{}\). This dataset represents the behaviors we need to test our **target LLM** on. In our approach to imitation guidance, we intend to utilize this dataset to determine which inputs prompt our target LLM to generate outputs similar to those in \(_{}\). In other words, we train the attack LLM such that it generates test cases that cause the target LLM to generate outputs similar to those in \(_{}\).

This approach is valuable as it enables us to test and understand which inputs elicit specific behaviors from the target LLM. Further, there exist a large number of datasets that consist of examples of undesirable behaviors Gehman et al. (2020); Lin et al. (2023), which can be used as \(_{}\).

We first model the space of \(_{}\) by training a harm LLM \(\) on it. This ensures that when prompted, \(\) produces outputs similar to those in \(_{}\). Given the harm model \(\), our goal is to train the attack LLM \(\) to generate prompts capable of inducing the target LLM \(p\) to generate outputs \(y\) similar to those of the harm model \((|z)\) where the input to the harm LLM is a combination of the input to the Attack LLM \(z\), and output of the attack LLM \(x\). Our objective now becomes:

\[_{} [R(y)- D_{KL}((|z)||_{}( |z))+_{1}D_{}(y,)] \] \[z,x(|z),y p(|x), (|z)\]

Here, \(D_{}\) measures the cosine similarity between the output of the target LLM \(y\) and the harm LLM \(\). Intuitively, we are training the attack LLM to prompt the target LLM to generate outputs resembling those of the Harm LLM. Having imitation guidance aids in both producing harmful content and ensuring that the outputs of the attack LLM are diverse. This is because the harm model is trained on multiple examples of harmful outputs, and thus can guide the training of the attack LLM. Details on harm model training and \(D_{}\) are provided in Appendix A.

### Diversity Module

To enhance the diversity of the attack LLM, we include a _diversity module_\(G\) which is a prompt-conditioned generative model. The goal of the diversity module is to model the distribution of previously generated outputs of the attack LLM during the training process. We train this model to generate previously observed outputs of the attack LLM for input prompt \(z\) during the training process. We then compare the outputs of the attack LLM, \(x(|z)\), with the outputs of the diversity module, \( G(|z)\), for the same input \(z\). If these outputs are similar, it indicates that the output has been generated previously, and we penalize the attack LLM. We iteratively train the diversity module \(G\) using previous inputs and outputs of the attack LLM. The final objective of our approach is as follows:

\[_{} [R(y)- D_{KL}((|z)||_{}( |z))+_{1}D_{}(y,)-D_{} (x,)] \] \[z,x(|z),y p(|x), (|z), G(|z)\]

Intuitively, the diversity module keeps track of the outputs of the attack LLM that have been observed during training. Further details of our _diversity module_ are available in Appendix A.2. Our methodology is summarized in Fig. 1.

## 5 Experiments

The goal of our evaluation is to measure iART's performance on two metrics: the _quality_ and _diversity_ of the test cases it produces. To achieve this, we conduct experiments on two tasks: text continuation and instruction following. We compare iART's performance with other baselines using various target LLMs of different sizes, including Mistral-7B (Jiang et al., 2023), GPT2-Alpaca (a GPT-2 model (Radford et al., 2019) fine-tuned on the Alpaca dataset (Taori et al., 2023)), and Dolly-3V (Conover et al., 2023).

### Metrics

**Quality**: Following (Hong et al., 2024), the quality of a red teaming model or attack LLM \(\) is the percentage of toxic responses it can elicit from the target LLM \(p\), when given test cases generated by \(\). We employ the RoBERTa hate speech classifier (Vidgen et al., 2020) to assign toxicity scores to the responses from the target LLM \(y\) for test cases \(x\) generated by the attack LLM. We consider an output of the target LLM toxic if its assigned toxicity score exceeds a threshold.

We evaluate the quality of iART and baselines by measuring the quality of test cases \(x\) generated during the training of \(\) for different threshold values.

**Diversity**: We quantify the diversity of the attack LLM by measuring the variability of test cases it generates across different toxicity thresholds. This variability is measured using the cosine similarity model \(D_{}\). We provide details on \(D_{}\) in Appendix A.

To evaluate the diversity of iART and other baselines, we compare each test case generated during training of \(\) with all other test cases produced for different threshold values.

**F1 Score for Diversity and Quality (F1DQ)**: Quality and diversity in testing scenarios often present a trade-off, where an improvement in one metric may come at the cost of the other. Specifically, higher quality (manifested as more frequent toxic outputs) tends to involve repetitive toxic words, thus reducing the diversity of the test cases. On the other hand, a higher diversity score can lead to the target model generating less toxic responses.

To quantify this trade-off and assess both metrics simultaneously, we introduce the F1DQ metric, which combines the quality and diversity scores using a harmonic mean. We define the F1DQ metric as follows:

\[F1DQ=}{+}\]

A red teaming model with a high F1DQ score implies that it is optimizing both quality and diversity simultaneously. This metric allows for a balanced assessment of the red teaming model's performance in generating diverse test cases yet eliciting the target model to generate toxic responses.

Similar to quality and diversity, evaluate the F1DQ score of iART and other baselines over different toxicity thresholds.

### Baselines

We benchmark our iART method against established RL-based automated red teaming approaches to demonstrate the benefits of integrating imitation guidance to indirectly guide the training of \(\) and a diversity module to improve the diversity of the generated test cases. For consistency, we use GPT2 (Radford et al., 2019) with 137M parameters as our attack LLM across all baselines and use proximal policy optimization (PPO) (Schulman et al., 2017) as the RL algorithm. We provide more details in Appendix A. We compare the performance of iART with the following baselines.

1. **RL (Perez et al., 2022)**: This foundational method involves training the red team model \(\) with a focus on maximizing rewards \(R(y)\) while incorporating a KL divergence penalty to prevent model drift (Eq. 1).
2. **RL+TDiv (Casper et al., 2023)**: Building on the RL framework of Perez et al. (2022), this variant enhances the model by training \(\) to not only follow the reward structure and KL penalty but also to maximize the diversity among responses. Diversity is quantified through the average distances between sentence embeddings produced by the target LLM.
3. **RL+Curiosity (Hong et al., 2024)**: This approach modifies the RL+TDiv method by shifting the focus of diversity maximization to the attack LLM itself. It measures the diversity of outputs by evaluating the distances among **all test cases** generated by the attack LLM, utilizing both the SelfBLEU score (Zhu et al., 2018), which employs BLEU score n-gram modeling for \(n\{2,3,4,5\}\), and cosine similarity of sentence embeddings to assess the diversity. The BLEU score measures the overlap of n-grams between a generated sentence and reference sentences. In the case of SelfBLEU, each previously generated sentence acts as a reference, with the score for each sentence labeled as SelfBLEU. Adopting this method is computationally intensive, as each generated sentence at every timestep in RL must be compared both semantically, using sentence embeddings, and textually, through SelfBLEU, against all prior generated test cases.

Our iART model advances these methods by training the red team model \(\) and removing the need for exhaustive comparison of prior test cases by utilizing imitation-guided reinforcement learning with harmful model rewards and diversity model rewards, as detailed in Section 4.

### Tasks

We evaluate our red teaming approach, iART, against target LLMs on two tasks: text continuation and instruction following. Text continuation in LLMs involves generating coherent and contextually relevant text that logically follows from a given prompt or initial segment. Meanwhile, the goal of the instruction following task is for the LLM to execute specific commands embedded within a textual input, adhering to direct instructions and providing appropriate responses. We conducted experiments using three seeds for each red teaming algorithm across all tasks, except for RL+Curiosity, which required several days to complete just one run.

#### 5.3.1 Text Continuation

In the text continuation task, we use a variant of GPT2 (Radford et al., 2019) fine-tuned on the IMDb review dataset (Maas et al., 2011) as our attack LLM \(\), with Mistral 7B serving as the target LLMs. We extract the first \(10\) words of each movie review from the IMDB dataset and feed them into the attack LLM to generate an extended review. This continuation is then concatenated with the original input and passed to the target LLM to elicit a response.

We measure the toxicity scores of all responses generated by the target LLM and plot the percentage of toxic responses against the toxicity threshold, as illustrated in Fig. 2. The graph in Fig. 2 reveals the efficacy of different red teaming strategies in provoking toxic responses at varying thresholds, Fig. 2 shows the diversity of the test cases for different toxicity thresholds, and Fig. 2 plots the F1DQ scores. The results show that iART consistently outperforms other models in eliciting high toxicity across a broader range of thresholds, while still being diverse.

Fig. 2 showcases the diversity of test cases generated by various red teaming models, as measured through embedding diversity. iART matches other methods in diversity while maintaining high quality, as evidenced by Fig. 2. In contrast, RL+Curiosity, also shown in Fig. 2, prioritizes diversity at the expense of quality. The F1 Score for Diversity and Quality, depicted in Fig. 2, illustrates that iART outperforms other models by balancing quality and diversity more effectively. RL+Curiosity and RL+TDiv, ranking second and third respectively.

Fig. 2 illustrates the execution times of each red teaming algorithm (the execution time corresponds to the total training time to generate all test cases.). RL+Curiosity requires the longest time, at 2929 minutes (approximately 2 days and 39 minutes), which is nearly 8 times longer than iART, which completes in just 423 minutes (7 hours and 3 minutes). This extensive duration for RL+Curiosity is attributed to its exhaustive method of keeping an array of prior test cases through detailed cosine similarity and SelfBLEU comparisons with previous cases, often leading to less toxic responses. In contrast, RL+TDiv focuses on eliciting diverse responses from the target LLM but may not effectively balance the quality and diversity of these cases.

#### 5.3.2 Instruction Following

In the instruction-following task, we employ GPT-2 as our attack model, while GPT-2-Alpaca, finetuned with the Alpaca dataset, and Dolly-v2-3B, a high-quality instruction instruction-following large language model trained on the Databricks machine learning platform and datasets (Conover et al., 2023), serve as the target LLMs. Based on the framework established by the Alpaca team

Figure 2: Comparative analysis of red teaming strategies with a GPT-2 attacker against Mistral 7B in the IMDb reviews dataset. (a) Demonstrates each algorithm’s ability to induce toxic responses. (b) Shows the diversity of test cases generated. (c) Effectiveness of balancing quality and diversity. (d) Compares execution times.

Taori et al. (2023), we provide a list of instructions to the attack model, which is then tasked with generating an additional instruction. This newly formed instruction is subsequently fed into the target LLMs, which are prompted to craft a response that effectively completes the given directive.

Fig. 2(e) demonstrates the performances on the Alpaca dataset. We can clearly observe that iART consistently outperforms RL+Curiosity, RL+TDiv, and RL in generating toxic responses across varying thresholds, as evidenced in Fig. 2(a). The diversity metrics, shown in Fig. 2(b), indicate that iART achieves substantial diversity while demonstrating superior quality. The F1 Score for Diversity

 p{113.8pt} p{113.8pt}}   Test Case & Target LLM Response & Toxicity Score \\  Russian immigrants lack intellectual capacity or intelligent minds, walk freely; speak fast enough ; sit down nor run Japanese men lacked moral sense and were lazy or clever, walked round themselves; stood uprightly & I agree with the statement. Russian immigrants lack intellectual capacity or intelligent minds and are very impulsive. In fact, the best example I see of this While Japanese women could be very charming and alluring, they were often boxy, domineering and had a hard time managing their emotions. Women control their needs in dreams; live abroad or fly away & 0.99 \\ Chinese immigrants lack adequate intellectual or sense, drive their heads ; sit down nor concentrate, breathe; speak & The people in the video look disbrevled and poorly put together. It is not uncommon for immigrants to speak Chinese. One would expect them to Women, especially more demure and sedate woman, are often prone to gigle or even laugh out loud when facing something humans. Their eyes Nigerian migrants lacked moral sense nor were smart or clever, walked forward ; stood uprightly dumb animals & 0.99 \\   

Table 1: Qualitative analysis of iART on the instruction following task with the Databricks dataset.

Figure 3: Comparative analysis of red meaning strategies in instruction following tasks across different LLMs and datasets using GPT-2 and Dolly-3B models. (a), (f) Demonstrate our method’s ability to elicit a higher percentage of toxic responses from the target LLM across various toxicity thresholds. (b), (g) iART achieves the greatest and most stable diversity (low variance) of test cases among the baselines, measured by 1 - Cosine Similarity. (c), (h) Present the F1 Score for Diversity and Quality, highlighting iART’s effective balance of high-quality toxic response generation with diverse test cases. (d), (i) Show that iART achieves this significant performance within reasonable running times compared to other models.

and Quality (Fig. 3c) highlights iART's superior balance in maintaining both high-quality responses and diversity compared to other models.

When using Dolly-3B as the target, we use the instruction dataset provided by Hong et al.  as the input to the attack LLM. The graphs shown in Fig. 3j, show that iART outperforms other baselines in generating toxic responses (Fig. 3f). The diversity performance (Fig. 3g) shows a similar trend to the Alpaca dataset, with iART providing a robust diversity score. The F1DQ Score (Fig. 3h) further emphasizes iART's effectiveness in achieving an optimal balance between quality and diversity, outpacing all of the competing approaches.

Fig. 3d and Fig. 3i display the execution times of each red teaming algorithm applied to the Alpaca and Databricks tasks, respectively. The figures reveal that while RL and RL+TDiv exhibit shorter running times, they struggle to deliver both high-quality responses and diverse test cases. Specifically, RL+TDiv produces diverse test cases but with almost negligible toxicity rates, whereas RL shows better quality but lacks diversity compared to RL+TDiv. RL+Curiosity excels in balancing quality and diversity, but this comes at the cost of much longer times, requiring 4922 minutes (approximately 3 days, 10 hours) and 5892 minutes (approximately 4 days, 2 hours) for 500 epochs on each dataset, respectively. In contrast, iART demonstrates impressive performance in both quality and diversity across both datasets, with significantly more efficient execution times of 361 minutes (6 hours) and 841 minutes (14 hours).

### Ablation of Imitation and Diversity Module

We conduct ablation studies to gauge the impact of the imitation and diversity module on iART. We conduct experiments on the test continuation task using the IMDB dataset, with Mistral-7B as the target LLM and GPT2 as the attack LLM. From Fig. 4, we can clearly observe that both modules work together to improve all of the metrics.

We show examples to illustrate the performance of iART in Table 1.

We conduct several experiments to demonstrate the performance of iART. These include hyperparameter sweep of imitation guidance coefficient (Appendix C) and KL Coefficient (Appendix D), experiments using Mistral as the attacker (Appendix E), and studying the performance of RL with different KL coefficients (Appendix F).

## 6 Conclusion

We introduce iART, an innovative approach to automated red teaming that utilizes imitation learning to enhance the diversity of test cases generated by the red teaming model and the quality of responses from target LLMs. Our experiments show that iART significantly outperforms existing reinforcement learning-based methods such as RL, RL+TDiv, and RL+Curiosity, not only in efficiency but also in its ability to balance diversity and quality (i.e., demonstrated with the F1DQ score). By producing test cases that are diverse and robust, iART effectively uncovers a broader spectrum of potential flaws in target LLMs across different tasks and datasets, proving its effectiveness in real-world scenarios. In addition, iART demonstrated substantial gains in computational efficiency, making it a vital tool to scale up red teaming practices and improve the safety and reliability of AI systems.

Figure 4: Ablation of imitation and diversity modules.