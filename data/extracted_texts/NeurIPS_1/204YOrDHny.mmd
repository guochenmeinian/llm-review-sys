# Reparameterization invariance in approximate Bayesian inference

Hrittik Roy, Marco Miani

Technical University of Denmark

{hroy, mmia}@dtu.dk &Carl Henrik Ek

University of Cambridge,

Karolinska Institutet

che29@cam.ac.uk

Philipp Hennig, Marvin Pfortner, Lukas Tatzel

University of Tubingen, Tubingen AI Center

{philipp.hennig, lukas.tatzel, marvin.pfoertner}@uni-tuebingen.de &Soren Hauberg

Technical University of Denmark

sohau@dtu.dk

Equal contribution authors listed in random order.

###### Abstract

Current approximate posteriors in Bayesian neural networks (bnns) exhibit a crucial limitation: they fail to maintain invariance under reparameterization, i.e. bnns assign different posterior densities to different parametrizations of identical functions. This creates a fundamental flaw in the application of Bayesian principles as it breaks the correspondence between uncertainty over the parameters with uncertainty over the parametrized function. In this paper, we investigate this issue in the context of the increasingly popular linearized Laplace approximation. Specifically, it has been observed that linearized predictives alleviate the common underfitting problems of the Laplace approximation. We develop a new geometric view of reparametrizations from which we explain the success of linearization. Moreover, we demonstrate that these reparameterization invariance properties can be extended to the original neural network predictive using a Riemannian diffusion process giving a straightforward algorithm for approximate posterior sampling, which empirically improves posterior fit.

## 1 Introduction

Bayesian deep learning has not seen the same degree of success as deep learning in general. Theoretically, Bayesian posteriors _should_ be superior to point estimates (Devroye et al., 1996), but the practical benefits of having the posterior are all too often not significant enough to justify their additional computational burden. This has raised the question if we even _should_ attempt to estimate full posteriors of all network parameters (Sharma et al., 2023).

As an example, consider the Laplace approximation (MacKay, 1992), which places a Gaussian in weight space through a second-order Taylor expansion of the log-posterior. When applied to neural networks, this is known to significantly _underfit_ and assign significant probability mass to functions that fail to fit the training data (Lawrence, 2001; Immer et al., 2021). Fig. 2 (top-left) exemplifies this failure mode for a small regression problem. Interestingly, this behavior is rarely observed outside neural network models, and the failure appears linked to Bayesian deep learning.

Recently, the _linearized Laplace approximation_ (l1a) has been shown to significantly improve on Laplace's approximation through an additional linearization of the neural network (Immer et al., 2021; Khan et al., 2019). We are unaware of any theoretical justification for this rather counterintuitive result: _why would an additional degree of approximation improve the posterior fit?_We will show that the failures of Bayesian deep learning can partly be explained by insufficient handling of _reparameterizations_ of network weights, while the lla achieves infinitesimal invariance to reparameterizations. To motivate, consider the simple network (Fig. 1)

\[f(x)=w_{1}(w_{2}\,x); f:. \]

This can be _reparametrized_ to form the same function realization from different weights as \(f(x)=}}{{}}\,(\,w_{2}\,x)\) for any \(>0\). That is, the weight-pairs \((w_{1},w_{2})\) and \((}}{{}}, w_{2})\) correspond to the same function even if the weights are different (Fig. 1, center).

Thus the approximate posterior cannot reflect the fundamental property of the true posterior, that it should assign a single unique density to a function regardless of its parametrization.

**In this paper**, we analyze the reparameterization group driving deep learning and show that it is a pseudo-Riemannian manifold, with the _generalized Gauss-Newton_ (ggn) matrix as its pseudo-metric. We prove that the commonly observed underfitting of the Laplace approximation (Fig. 2, top left) is caused by high in-distribution uncertainty in directions of reparametrizations (Fig. 2, top center).

We develop a reparametrization invariant diffusion posterior that proveably does not underfit despite using the neural network predictive (Fig. 2, center row). Figure 1 (right) visualizes how this posterior adapts to the geometry of reparametrizations, thereby not underfitting. The diffusion can be simulated with a multi-step Euler-Maruyama scheme from which the linearized Laplace approximation (lla) is a single-step. This link implies that the lla _infinitesimally_ is invariant to reparameterizations, due to the otherwise counterintuitive linearization (Fig. 1, left). Experimentally, our diffusion consistently improves posterior fit, suggesting that reparameterizations should be given more attention in Bayesian deep learning.

## 2 Background: Laplace approximations

Let \(f_{}:^{I}^{O}\) denote a neural network with weights \(\), and define a likelihood \(p(|f_{}())\) and a prior \(p()\). _Laplace's approximation_(MacKay, 1992) performs a second-order Taylor expansion of the log-posterior around a mode \(}\). This results in a Gaussian approximate posterior \((|},-_{}}^{-1})\), where \(_{}\) is the Hessian matrix. _The linearized Laplace approximation_(Immer et al., 2021b; Khan et al., 2019) further linearize \(f_{}\) at a chosen weight \(}\), i.e. \(f_{}() f_{}}^{}}( ,))=f_{}}()+_{}}()(-})\), where \(_{}}()=_{}f_{} ()|_{=}}^{O D}\) is the Jacobian of \(f_{}\). Here \(D=()\) denotes the number of parameters in the network. Applying the usual Laplace approximation to the linearized model yields an approximate posterior (Immer et al., 2021b),

\[q(|)=(},( _{}}+)^{-1})_{}}=_{n=1}^{N}_{}}( _{n})^{}(_{n})_{}}( _{n}), \]

Figure 1: The _weight space_ of a neural network (Eq. 1) overparametrizes the associated _function space_. This induces families (orange) of weights corresponding to the same functions. Model linearization (left) linearizes these families. In nonlinear models, Gaussian weight distributions (center) do not adapt to the families, while our geometric diffusion (right) captures the associated invariance with a metric (gray ellipses).

Figure 2: The _function space_ is decomposed into directions of _reparameterizations_ (kernel) and _functional change_ (non-kernel). We improve the posterior fit by concentrating probability mass on directions of functional change.

where \(()=-_{f_{}()}^{2} p( |f_{}}())^{O O}\) is the Hessian of the log-likelihood and we have assumed a weight prior \((,^{-1})\). Note that it is trivial to extend to other prior covariances. This particular covariance is known as the _generalized Gauss-Newton_ (ggn) Hessian approximation, which is commonly used in Laplace approximations (Daxberger et al., 2021).

To reduce the notational load we stack the per-datum Jacobians into \(_{}}=[_{}}(_{1}); ;_{}}(_{N})]^{NO D}\) and similarly for the Hessians, and write the ggn matrix as \(_{}}=_{}}^{}_{}}\). For Gaussian likelihoods, the Hessian is an identity matrix and can be disregarded, and for other likelihoods simple expressions are generally available (Immer et al., 2021).

**Sampled and linearized Laplace.** The Laplace approximation gives a Gaussian distribution \(q(|)\) over the weight space with mean \(}\) and covariance \(\). A predictive distribution is obtained by integrating the approximate posterior against the model likelihood,

\[p(^{*}|^{*},)=_{ q}[p( ^{*}|f(,^{*}))]^{S}p(^{*}|f(_{i},^{*})),_{i } q. \]

We refer to this predictive method as _sampled Laplace_. Recent works have suggested _linearizing_ the neural network in the likelihood model to obtain the predictive distribution (Immer et al., 2021),

\[p(^{*}|^{*},)=_{ q}[p( ^{*}|f_{}^{}}(,^{*}))] ^{S}p(^{*}|f_{}^{ }}(_{i},^{*})),_{i} q. \]

This is referred to as _linearised Laplace_. Immer et al. (2021) argues that the common choice of approximating the posterior precision with the ggn implicitly linearizes the neural network and hence the predictive distribution should be modified for consistency.

Sampled Laplace is known to severely _underfit_, whereas the linearized Laplace approximation does not (Immer et al., 2021; Fig. 2). It is an open problem _why_ the crude linearization is beneficial (Papamarkou et al., 2024). This paper shows that the benefit is linked to the lack of _reparameterization invariance_.

**The lack of reparameterization invariance** leads to an additional problem for Laplace approximations. The precision of the approximate posterior is given either by the Hessian or the ggn. As shown by Dinh et al. (2017), the Hessian of the loss is not invariant to reparameterizations of the neural network, and the same holds for the ggn. Depending on which parametrization of the posterior mode is chosen by the optimizer, we, thus, get different covariances for the approximate posterior. Empirically, this can render Laplace's approximation unstable (Warburg et al., 2023). Figure 1 (center) illustrates the phenomena.

## 3 Reparameterizations of linear functions

Deep learning models excel when they are highly _overparametrized_, i.e. when they have significantly more parameters than observations (\(D NO\)). This introduces many degrees of freedom to the model, which will be reflected in the Bayesian posterior. However, as we have argued, traditional approximate Bayesian inference does not correctly capture this and assigns different probability measures to identical functions. Next, we characterize these degrees of freedom to design suitable approximate posteriors. To develop the theory, we first consider the linear setting and then extend it to the general case.

**The reparameterizations of linear functions** can be characterized exactly. Consider \(f()=+\) and a possible reparameterization, \(g:^{D}^{D}\), of this function such that \(f(g())=f()\). It is then evident that \((g()-)=\). This implies that for any reparameterization of a linear function, we have \(g()-()\), where \(()\) denotes the _kernel_ (nullspace) of \(\). Hence, the linear function cannot be reparametrized if we restrict ourselves to the non-kernel subspace of the input space or if \(\) has a trivial kernel.

**A linearized neural network**\(f_{}^{^{}}:, f_{^{ }}()+_{^{}}()(- ^{})\) is a linear function in the parameters, where we have linearized around \(^{}\). The above analysis then implies that the kernel of the stacked Jacobian \(_{^{}}\) characterizes the reparameterizations of the linearized network.

We can also characterize the reparameterizations through the ggn and the corresponding _neural tangent kernel_ (ntk; Jacot et al., 2018),

\[_{}=_{}^{}_{}, _{}=_{}_{}^ {}. \]

By construction, these have the same non-zero eigenvalues, and thereby also have identical ranks. We, thus, see that the kernel of the Jacobian coincides with that of the ggn, i.e. \((_{})=(_{})\).

**Two orthogonal subspaces.** For any self-adjoint operator (such as positive semi-definite matrices like the ggn), the _image_ and the _kernel_ orthogonally span the whole space, i.e.

\[(_{})(_{})= ^{D}, \]

where the _kernel_ is the hyperplane of vectors that are mapped to zero and the _image_ is the hyperplane of vectors spanned by the operator (Fig. 3). For a linearized neural network, \((_{})\) spans the _effective_ parameters \(^{D}\), i.e. the maximal set of parameters that generate different linear functions \(^{I}^{O}\) when evaluated on the training set.

**A Laplace covariance decomposes** into the same subspaces. Recall that the posterior precision is \(^{-1}=_{}}+\). Let the eigendecomposition of \(_{}}\) be \(^{T}\), and assume that \(_{}\) and \(_{}\) are the eigenvectors corresponding to the non-zero eigenvalues \(}\), and the zero eigenvalues respectively. These form a basis in the kernel and image subspace as discussed above. Then the covariance is,

\[=([_{}\\ _{}]^{T}[ }&\\ &][_{}\\ _{}]+)^{-1}= _{}^{}(}+_ {k})^{-1}_{}+^{-1}_{}^{}_{}. \]

Consequently, we can decompose any sample from the Gaussian \((},)\) into a kernel and an image contribution, \(=}+_{}+_{}\), where \(_{}\) is the component of the sample that is in the kernel of \(_{}}\) and \(_{}\) is in the image. Note that all probability mass in \((_{}})\) is due to the prior, i.e. we place prior probability on functional reparameterizations even if we can never observe data in support of such.

**Underfitting in sampled Laplace** can now be understood. For the linearized approximation, it holds for training data \(\) that,

\[f_{}^{}}(}+_{}+ _{},)=f_{}^{}}( }+_{},).\]

Hence, the linearized predictive only samples in the image subspace consisting of unique functions. This is _not_ true for the sampled Laplace approximation, which also samples in the kernel subspace. Since sampled Laplace does not linearize the neural network, the kernel does not correspond to reparameterizations. It hence adds "incorrect" degrees of freedom to the posterior as artifacts of the Gaussian approximation.

Empirically, sampled Laplace is only observed to underfit in overparametrized models. Fig. 4 illustrates this by increasing the amount of training data to decrease the kernel rank, i.e. reduce the reparametrization issue. We find that as the issue is lessened, sampled Laplace reduces its underfitting.

## 4 Reparameterizations of neural networks

We have seen that the parameters of linear models can be decomposed into two linear subspaces corresponding to reparameterizations and functional changes. We next analyze nonlinear models.

Figure 4: Underfitting of sampled Laplace is less pronounced when the rank of the ggn is higher for a fixed number of parameters. This is consistent with our hypothesis as a high ggn rank implies a lower dimensional kernel. For experimental details, see appendix E.2.

Figure 3: The weight space can be decomposed into directions of _reparameterizations_ and _functional changes_. For linear models (left) these are linear subspaces given by the kernel and the image, respectively. For nonlinear models, these are the nonlinear manifolds \(_{_{i}}^{}\) and \(_{_{i}}\), respectively.

**Intuitively**, reparameterizations of a nonlinear neural network form continuous trajectories in the parameter space (c.f. Fig. 1). We define that all points along such a trajectory are identical, which changes the weight space geometry to be a manifold. Likewise, the parameter changes corresponding to actual function changes reside on a nonlinear manifold. This is sketched in Fig. 3. Interestingly, the ggn turns out to induce a natural (local) inner product on these nonlinear manifolds, which allows us to both understand and generalize the linearized Laplace approximation.

### The effective-parameters quotient space

For a _nonlinear_ neural network \(f:^{D}^{I}^{O}\), the surfaces in weight space along which the function does not change are generally _not_ linear. Here, we formalize these reparameterization invariant surfaces and show that they are a partition of the weight space.

**Definition 4.1**.: Given a datapoint \(^{I}\), for any \(^{D}\) we define the \(\)-_reparameterizations_ as the set \(^{f}_{}()=\{^{}f(^{},)=f(,)\}\). Consistently, given a collection of points \(^{I}\), we call the intersection \(^{f}_{}()=_{} ^{f}_{}()\)\(\)-_reparameterizations_.

Trivially, \(^{f}_{}()\) for any choice of \(\). We next define the subset of \(\)-reparameterizations which can be obtained via a smooth deformation from \(\).

**Definition 4.2**.: We say that a piecewise differentiable function \(:^{D}\) is a _homotopy_ of \((,})\) if \((0)=\) and \((1)=^{}\). The set of \(\)-_smooth-reparameterizations_ is defined as,

\[}^{f}_{}()=\{^{} (,^{})\\ (t)^{f}_{}()\  t\}.\]

A homotopy \(\) is, thus, a smooth path along which all neural networks have identical predictions on \(\). We consider two networks, \(\) and \(^{}\), similar if they can be connected by such a homotopy. Formally, we define the relation \(\) over \(^{D}\) as \(\)\(\)\(^{}\) if \(^{}}^{f}_{}()\).

We next use this relation to form a new view on the weight space \(^{D}\) in which similar weights are seen as _one_ point. This can be realized using _quotient spaces_(Lee, 2012). These are well-studied spaces that are constructed by considering a collection of points in one space as a single point in a new space. In our case, we have the following result.

**Lemma 4.3**.: \(\) _is an equivalence relation, i.e. it is transitive, symmetric and reflexive. We can form the quotient space \(=^{D}/\) of effective parameters. We denote \([]\) the equivalence class of an element \(^{D}\)._

This quotient structure gives a rich mathematical foundation to construct reparameterization invariant neural networks. Within the quotient, two effective parameters \([_{1}],[_{2}]\) are the same point if and only if \(_{1}_{2}\). This means that all parameters \([_{1}]\) gives the same function over \(\).

### The effective-parameters manifold

Geometry is the mathematical language of invariances. To this end would like to endow the weight space with a geometric structure such that two weights, \(_{1}\) and \(_{2}\), corresponding to the same function, have a distance of zero, i.e.

\[(_{1},_{2})=0 _{1}_{2}. \]

Since the weights generate the same function, we define a metric that measures differences in _function values_ on the training data. Consider weights \(\) and an infinitesimal displacement \(\), we then define,

\[^{2}(,+)=_{n= 1}^{N}\|f(,_{n})-f(+,_{n})\|^ {2}=^{}_{} +(^{3}), \]

where the last step follows from a first-order Taylor expansion of \(f\) around \(\). This is a standard _pullback metric_\((f^{*}H)_{}=_{}\) commonly used in Riemannian geometry. This implies that the ggn matrix infinitesimally defines an inner product, i.e. it is a _Riemannian metric_. By integrating over paths, the distance extend to any pair of points and satisfies Eq. 8 (Lee, 2012).

Watch out! It's a pseudo-metric.We have already seen that in overparametrized models, the ggn is rank-deficient, which implies that it is not positive definite. Consequently, it is not a Riemannian metric but rather a _pseudo_-Riemannian metric. A pseudo-metric can be a counterintuitive object: two points \(_{1}\) and \(_{2}\) at distance zero may have different pseudo-metrics \((f^{*}H)_{_{1}}(f^{*}H)_{_{2}}\). This is reflected in the Laplace approximation. The covariance prescribed by the Laplace approximation is \(_{}}=(^{2}_{}(} )+)^{-1}\), where \(()\) is shorthand for the training log-likelihood. The Hessian is exactly the pullback pseudo-metric \(^{2}_{}(})=(f^{*}H)_{}}\), which is _not_ invariant to reparameterizations of the neural network. Specifically, for a reparameterization function \(g\) that is also a diffeomorphism, the change of variable rules states that,

\[_{}(g(} ))}_{^{-1}_{g(})}-}=_{}g (})^{}_{}(})}_{^{-1}_{}}-}_{}g(}). \]

This means that, while each parameter \(\) has its well-defined covariance \(_{}\), each equivalence class does not have a unique one, since \(}\) and \(g(})\) belong to the same equivalence class and \(_{}}_{g(})}\).

Non-Gaussian likelihoods.The Euclidean distance measure in Eq. 9 corresponds to choosing a Gaussian likelihood. The distance definition readily extends to other likelihoods and the corresponding metric takes the form of the generalized Gauss-Newton matrix \(^{}_{}_{}\), where \(\) denotes the Hessian of the log-likelihood. For both Gaussian and Bernoulli likelihoods, this Hessian is positive definite, but e.g. the cross entropy has a rank-deficient Hessian and, thus, induces a pseudo-metric.

An impractical solution.The unfortunate behavior of approximate posteriors assigning different probabilities to the same function could be rectified by marginalizing over the set of reparameterizations of \(\), i.e. \(_{^{}()}q(^{}| )^{}\). While this construction solves the highlighted problem, its complexity makes it impractical and we are unaware of any works along these lines.

When restricted to a smaller class of reparameterization (the ones homotopic to the identity), the integral can be thought of as "collapsing" each reparameterization equivalence class to a single point in \(=^{D}/\) formalized in Lemma 4.3. Nontrivially, the pullback metric implicitly performs a similar operation, as shown later in Theorem 4.5. This connection motivates the dive into Riemannian geometry: _we get a tractable approach to engaging with neural network reparameterizations_.

### Topological equivalence of the two views

So far we described two _a priori_ very different objects: the quotient space \(=^{D}/\) and the pseudo-Riemannian manifold \((^{D},_{})\). We referred to both of them as _effective parameters_ and this is no coincidence as there is a natural relationship between the points at distance zero according to the pseudo-metric and the equivalence classes.

**Proposition 4.4**.: _For any \(_{0},_{1}^{D}\) it holds_

\[d_{f^{*}H}(_{0},_{1})=0[ _{0}]=[_{1}]. \]

Even better, these two spaces share the same topological structure. To state this we need a notion of distance on the quotient space and the most natural choice is to inherit the Euclidean distance \(\|\|\) from \(^{D}\). This distance is defined as

\[d_{}([],[^{}])=\{\|p_{1}-q_{1}\| ++\|p_{n}-q_{n}\|\},\]

where the infimum is taken over all finite sequences \(p_{1},,p_{n}\) and \(q_{1},,q_{n}\) such that \([]=[p_{1}]\), \([p_{i+1}]=[q_{i}]\) and \([q_{n}]=[^{}]\).

This distance \(d_{}\) induces a topology on the quotient space \(\) which is equivalent to the topology induced by the pullback distance \(d_{f^{*}H}\) on the pseudo-Riemannian manifold. Formally

**Theorem 4.5**.: _For any \(_{0},_{1}^{D}\), for any \(>0\) there exists \(>0\) such that_

\[d_{}([_{0}],[_{1}])<  d_{f^{*}H}(_{0},_{1})< \] \[d_{f^{*}H}(_{0},_{1})<  d_{}([_{0}],[_{1}])<. \]

This result connects an abstract quotient space \(\) with the pseudo-Riemannian metric \(_{}\). The quotient captures useful intuitions but is difficult to leverage computationally. In contrast, the pseudo-metric has some counterintuitive aspects but we can identify the underlying Riemannian structure which leads to tractable algorithms (Sec. 5).

A tale of two manifolds.For any given parameter \(^{D}\) and training set \(\), we show that there exist two Riemannian manifolds \((_{},)\) and \((_{}^{},^{})\) embedded in \(^{D}\), illustrated in Fig. 3. They capture the functional change and reparameterization properties respectively, but, differently from the previously studied \((^{D},_{})\), they are Riemannian manifolds without degenerate directions in their metrics. Formally,

**Theorem 4.6**.: _For any parameter \(\) suppose the set of parameters that generate the same predictions is denoted by \(_{}^{}=\{^{}^{D}f(^{},x)=f(,x)x\}\). Then this set is a smooth manifold embedded in \(^{D}\). Furthermore, the set of parameters that locally generates unique predictions, \(_{}\) is also a submanifold embedded in \(^{D}\)._

They are the direct generalization to the nonlinear case of the two spaces involved in Eq. 6, where \(_{}\) plays the role of the _image_ and \(_{}^{}\) plays the role of the _kernel_. When \(f\) is linear, they are identical.

In general, \(_{}\) and \(_{}^{}\) intersect only in \(\), and the two respective tangent spaces in \(\) span all directions. They can be thought of as two collections of parameters, and the associated functions have different properties: (1) \(_{}^{}\) is entirely contained in the same equivalence class \(_{}^{}[]\), thus all the parametrized functions are identical on the train set; in contrast, (2) \(_{}\) never intersects the same equivalence class more than one time, at least locally, thus the parametrized functions always changes when moving in any direction. Thus \(_{}\) resembles the effective parameter manifold \(\), but with the difference of being an actual Riemannian manifold. These two manifolds exist under the assumption that Jacobian is full rank (see proof in appendix C).

The two metrics \(\) and \(^{}\) are not uniquely defined. A natural choice for \(\) is to restrict \(_{}\) to the tangent space of \(_{}\) corresponding to the non-zero eigenvectors, i.e. \(=_{}^{+}\). While, for \(^{}\) we can inherit the Euclidean metric, i.e. \(^{}=\) for \(>0\).

## 5 Exploring manifolds with random walks

SDEs on manifolds.Given a Riemannian manifold \((,)\), the simplest choice of distribution that respects the Riemannian metric \(\) is a Riemannian diffusion (or Brownian motion, c.f. Hsu (2002)) stopped at time \(t\). This follows the stochastic differential equation (Girolami Calderhead, 2011),

\[=()^{-}W+t_{i}()=_{j=1}^{D} _{j}}(()^{-1})_{ij}. \]

Practically speaking this simple process can be simulated using an Euler-Maruyama (Maruyama, 1955) scheme. The Christoffel symbols, \(_{i}()\), are commonly disregarded as they have a high computational cost, and Li et al. (2015) showed that the resulting error is bounded.

Using the Euler-Maruyama integrator with step size \(h_{t}\), setting \(=1\) corresponding to standard Bayesian inference and disregarding the term involving the Christoffel symbols \(\), we obtain the simple update rule \(_{t+1}=_{t}+}(_{t})^{- }\), where \((,)\). This applies to any Riemannian manifold. However, the effective-parameter \((^{D},_{})\) is only pseudo-Riemannian, we explore three Riemannian alternatives: \((^{D},_{}+)\), \((_{}^{},)\) and \((_{},_{}^{+})\)

Diffusion on \((^{D},_{}+)\).The Laplace approximation can also be written as a diffusion on a manifold. As we saw in Sec. 2, the Laplace approximation can be written as \(|(},)\) with \(^{-1}=_{}}+\). This can also be written as a sample at \(t=1\) of a Riemannian diffusion on a manifold with a constant metric, \((^{D},)\), where \(=_{}}+\). The \(\,=^{-}W\) have a marginal distribution at \(t=1\) that exactly match the standard Laplace approximation. Note that this formulation does not rely on the approximation of the sde that disregards the term involving the Christoffel symbols \(\) as these are zero for constant metrics. Hence, the above is exactly a Riemannian diffusion on the manifold with a constant metric given by the ggn at the map parameter. Note that this is only a valid diffusion for \(>0\) in which case it is not reparametrization invariant.

Kernel-manifold diffusion.The kernel-manifold \((_{}^{},)\) consists of parameters that generate the same function over the training set. The effect of diffusion on this manifold and using the neural network predictive is similar to sampling from the kernel subspace while using the linearized predictive. On the training set the predictive variance is \(0\) because it only samples reparametrizations of the map predictions. On out-of-distribution data, the variance is greater than \(0\) if at least one of the reparameterizations on the training set is not a global reparameterization. This leads to a clear separation in the predictive variance of in-distribution and out-of-distribution data (Fig. 2) and further implies that this diffusion distribution never underfits. Stated formally,

**Theorem 5.1**.: \(_{^{}_{}}[f( ,)]=0\) _for train data \(\). For a test point \(_{t}\), if there exists a reparameterization \(^{}}^{f}_{}(})\) such that \(^{}}^{f}_{\{_{ t}\}}(})\), then \(_{^{}_{}}[f( ,_{t})]>0\)._

Non-kernel-parameter manifold diffusion. The non-kernel-parameter manifold \((_{},^{+}_{})\) consists of parameters that generate unique functions over the training set. Diffusion on this manifold samples functions that are necessarily different from the map predictions on the training set. However, the predictive variance in the training set is bounded such that the functional diversity in the predictive samples reflects the intrinsic variance of the training data (Fig. 2).

This is the only considered diffusion that acts on a Riemannian manifold while being reparametrization invariant, i.e. \(}^{f}_{}()=\{\}\). We call this _Laplace diffusion_ and study it empirically in Sec. 7.

## 6 Related work

Bayesian deep learning techniques are still in their infancy and generally involve poorly understood approximations. The arguably most popular tool for uncertainty quantification is _ensembles_(Lakshminarayanan et al., 2017; Hansen and Salamon, 1990). Several approaches make Gaussian approximations to the true posterior, including _'Bayes by backprop'_(Blundell et al., 2015), _stochastic weight averaging_(swag)(Maddox et al., 2019) and the _Laplace approximation_(MacKay, 1992; Daxberger et al., 2022; Antoran et al., 2023; Deng et al., 2022; Miani et al., 2022).

The high dimensionality of the weight space gives rise to significant computational challenges when constructing Bayesian approximations. This has motivated various low-rank approximations (review in Daxberger et al., 2021), e.g. _last layer_ approximations (Kristiadi et al., 2020), _subnetwork inference_(Daxberger et al., 2021), _subspace inference_(Izmailov et al., 2020) or even pca in weight space (Maddox et al., 2019). Such approaches lessen the computational load, while often improving predictive performance. Our analysis sheds light on why crude approximations perform favorably: _smaller models are less affected by reparameterization issues_. Our diffusion process, thus, provides an alternative, and less heuristic, path forward.

MacKay (1998) noted the importance of the choice of basis in Laplace approximations; our pseudo-Riemannian view can be seen as having a continuously changing basis. Kristiadi et al. (2023) studied how a metric transforms under a bijective differentiable change of variables. They enforce geometric

    & &  &  &  &  &  &  \\   & Laplace Diffusion (ours) & **0.988\(\)0.001** & **0.042\(\)0.007** & **0.987\(\)0.002** & **0.022\(\)0.003** & **0.137\(\)0.019** & **0.775\(\)0.043** \\  & Sampled Laplace & 0.589\(\)0.008 & 3.812\(\)0.284 & 0.146\(\)0.032 & 1.176\(\)0.046 & 0.443\(\)0.026 & 0.985\(\)0.002 \\  & Linearised Laplace & 0.968\(\)0.004 & 0.306\(\)0.041 & 0.926\(\)0.008 & 0.117\(\)0.012 & 0.251\(\)0.034 & 0.855\(\)0.041 \\   & Laplace Diffusion (ours) & **0.900\(\)0.001** & **0.001\(\)0.000** & **0.906\(\)0.007** & **0.141\(\)0.006** & **0.108\(\)0.015** & **0.729\(\)0.092** \\  & Sampled Laplace & 0.618\(\)0.021 & 4.507\(\)0.000 & 0.098\(\)0.010 & 1.295\(\)0.014 & 0.518\(\)0.013 & 0.986\(\)0.001 \\   & Linearised Laplace & 0.897\(\)0.003 & 0.423\(\)0.000 & 0.862\(\)0.005 & 0.207\(\)0.006 & 0.147\(\)0.017 & 0.756\(\)0.048 \\   & Laplace Diffusion (ours) & **0.952\(\)0.007** & **0.345\(\)0.062** & **0.905\(\)0.007** & **0.155\(\)0.019** & 0.259\(\)0.008 & 0.870\(\)0.021 \\   & Sampled Laplace & 0.843\(\)0.004 & 0.997\(\)0.222 & 0.717\(\)0.049 & 0.422\(\)0.081 & **0.221\(\)0.047** & 0.804\(\)0.080 \\   & Linearised Laplace & 0.951\(\)0.007 & 0.614\(\)0.020 & 0.863\(\)0.001 & 0.222\(\)0.002 & 0.337\(\)0.022 & **0.789\(\)0.035** \\   

Table 1: In-distribution performance across methods trained on MNIST, FMNIST and CIFAR-10.

    & &  &  &  \\   &  &  &  &  &  &  &  &  &  &  \\     & & & **0.909\(\)0.033** & **0.625\(\)0.018** & **0.929\(\)0.008** & **0.759\(\)0.045** & **0.741\(\)0.010** & **0.749\(\)0.023** & **0.851\(\)0.002** & **0.862\(\)0.010** \\     & Sampled Laplace & 0.500\(\)0.026 & 0.494\(\)0.006 & 0.482\(\)0.013 & 0.495\(\)0.037 & 0.503\(\)0.036 & 0.493\(\)0.033 & 0.687\(\)0.033 & 0.599\(\)0.038 \\     & Linearised Laplace & 0.7584\(\)0.070 & 0.602\(\)0.027 & 0.790\(\)0.018 & 0.625\(\)0.050 & 0.628\(\)0.013 & 0.624\(\)0.020 & 0.837\(\)0.006 & 0.854\(\)0.024 \\   

Table 2: Out-of-distribution AUROC (\(\)) performance for MNIST, FMNIST and CIFAR-10.

consistency, highlighting, e.g., the non-invariance of the ggn to a change of variables. Petzka et al. (2019); Jang et al. (2022) point to the same inconsistency with an emphasis on flatness measures.

Kim et al. (2022) and Antoran et al. (2022) study global (rather than data-dependant) reparametrizations associated with specialized architectures. While analytic expressions can be obtained, the results do not apply to general networks. While not expressed in terms of reparametrizations, Izmailov et al. (2021) show that linearly dependent datasets give rise to a hyperplane in the kernel manifold. Kim et al. (2024) also study the kernel of the ggn in the context of influence functions. These works characterize subsets of the reparametrization group. We provide the first architecture-agnostic characterization of all continuous reparametrizations.

In a closely related work, Bergamin et al. (2024) introduced a Riemannian Laplace approximation (Hauberg, 2018) that improves posterior fit over a range of tasks. Furthering this line of research, Yu et al. (2023) explored the use of the Fisher information metric within this framework. While sharing the language of Riemannian geometry, our work focuses on analyzing the effectiveness of linearized Laplace within the context of neural network reparametrization, instead of primarily aiming to achieve better posterior approximations. This allows us to gain deeper insights into the underlying mechanisms that contribute to the success of this approximation technique.

## 7 Experiments

We benchmark Laplace diffusion with neural network predictive against linearized and sampled Laplace to validate the developed theory. Implementation details are in Appendix E.1. We will show that the diffusion posterior slightly outperforms linearized Laplace in terms of both in-distribution fit and out-of-distribution detection. For completeness, we include comparisons to other baselines such as SWAG, diagonal Laplace, and last-layer Laplace in Appendix E.3. Laplace diffusion is competitive with the best-performing Bayesian methods despite using the neural network predictive (i.e. no linearization). This contrasts sampled Laplace which severely underfits. This is evidence that the developed theory explains the key challenges of Bayesian deep learning.

Experimental details (appendix E.3).We train a 44,000-parameter LeNet(LeCun et al., 1989) on mnist and fmnist as well as a 270,000-parameter ResNet(He et al., 2016) on cifar-10(Krizhevsky et al., 2009). We sample from the Laplace approximation of the posterior and our Laplace diffusion. For the samples from the Laplace approximation, we consider both the linearized predictive and the neural network predictive, while for diffusion samples, we only consider the neural network predictive. These baselines were chosen to be as similar as possible to our approach to ease the comparison. We use the same prior precision for all methods to ensure a fair comparison.

In-distribution performance (Table 1).We measure the in-distribution performance of different posteriors on a held-out test set. We report means \(\) standard deviations of several metrics: Confidence, Accuracy, Negative Log-Likelihood, Brier Score (Brier, 1950), Expected Calibration Error (Naeini et al., 2015) and Mean Calibration Error. We observe that Laplace diffusion has the best calibration and fit. We also confirm the underfitting of Sampled Laplace across cases. For cifar-10 we had to use a large prior precision to get meaningful samples from sampled Laplace, which explains the less severe underfitting. High prior precision is known to help with underfitting in sampled Laplace, but it also shrinks predictive uncertainty to almost zero.

Robustness to dataset shift (Fig. 5, appendix E.3.2).We use rotated-MNIST, rotated-fmnist, and rotated-cifar to asses model-calibration and model fit under distribution shift. Fig. 5 plots negative log-likelihood (Nll) and expected calibration error (ece) against the degrees of rotation. Laplace diffusion improves on other Laplace approximations.

Figure 5: Benchmark results for Rotated mnist (similar results for fmnist and cifar are in appendix E.3.2). Sampled Laplace significantly underfits even for non-rotated data. Laplace diffusion consistently outperforms the other methods.

Out-of-distribution detection (Table 2).On out-of-distribution data from other benchmarks, we see that Laplace diffusion outperforms the other Laplace approximations.

## 8 Conclusion

While approximate Bayesian inference excels in many areas, it continues to face challenges in deep learning. Techniques that work well in shallow models struggle with deep ones even if they remain computationally tractable. This suggests that overparametrization plays a negative role in Bayesian models. Our theoretical analysis shows how overparametrization creates a growing reparameterization issue that conflicts with standard Euclidean approximate posteriors, such as the ever-present Gaussian. For small models this issue is negligible, but as models grow, so does the reparameterization issue.

Our geometric analysis also suggests a solution: we should consider approximate posteriors that respect the group structure of the reparameterizations. We observe that the generalized Gauss-Newton (ggn) matrix commonly used in Laplace approximations induces a pseudo-Riemannian structure on the parameter space that respects the topology of the reparameterization group. This implies that we can use pseudo-Riemannian probability distributions as approximate posteriors, and we experimented with the obvious choice of a geometric diffusion process. We also showed that the state-of-the-art _linearized_ Laplace approximation can be viewed as a naive (or simple) numerical approximation to our proposed diffusion. This helps explain the success of the linearized approximation.

Our proposed approximate posterior does have issues. While sampling has the same complexity as standard Laplace approximations, it increases runtime by a constant factor. Common Laplace approximations do not sample according to the ggn but rather approximate this matrix with a diagonal or block-diagonal matrix. Mathematically, such approximations break the motivational reparameterization invariance, so it is unclear if such approaches should be applied in our framework. Our work, thus, raises the need for new computational pipelines for engaging with the ggn matrix.