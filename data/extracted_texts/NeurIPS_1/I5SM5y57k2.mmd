# On Robust Streaming for Learning with Experts: Algorithms and Lower Bounds

David P. Woodruff

CMU

dwoodruf@cs.cmu.edu &Fred Zhang

UC Berkeley

z0@berkeley.edu &Samson Zhou

Texas A&M University

samsonzhou@gmail.com

###### Abstract

In the online learning with experts problem, an algorithm makes predictions about an outcome on each of \(T\) days, given a set of \(n\) experts who make predictions on each day. The algorithm is given feedback on the outcomes of each day, including the cost of its prediction and the cost of the expert predictions, and the goal is to make a prediction with the minimum cost, compared to the best expert in hindsight. However, often the predictions made by experts or algorithms at some time influence future outcomes, so that the input is adaptively generated.

In this paper, we study robust algorithms for the experts problem under memory constraints. We first give a randomized algorithm that is robust to adaptive inputs that uses \((})\) space for regret \(R\) when the best expert makes \(M=O(T}{^{2}n})\) mistakes, thereby showing a smooth space-regret trade-off. We then show a space lower bound of \(()\) for any randomized algorithm that achieves regret \(R\) with probability \(1-2^{-(T)}\). Such an algorithm is useful for adaptive inputs, as the failure probability is low enough to union bound over all computation paths. Our result implies that the natural deterministic algorithm, which iterates through pools of experts until each expert in the pool has erred, is optimal up to polylogarithmic factors. Finally, we empirically demonstrate the benefit of using robust procedures against a white-box adversary that has access to the internal state of the algorithm.

## 1 Introduction

_Online learning with experts_ is a fundamental problem in sequential prediction. On each of \(T\) days, an algorithm must make a prediction about an outcome, given a set of \(n\) experts who make predictions on the outcome. The algorithm is then given feedback on the cost of its prediction and on the expert predictions for the current day. In the _discrete prediction with experts_ problem, the set of possible predictions is restricted to a finite set, and the cost is 0 if the prediction is correct, and 1 otherwise. More generally, we assume the costs are restricted to be in a range \([0,]\) for some fixed parameter \(>0\), with lower costs indicating better performance. This process continues for the \(T\) days, after which the performance (total cost) of the algorithm is compared to the performance (total cost) of the best performing expert. In particular, the goal for the online learning with experts problem is to minimize the regret, which is the amortized difference between the total cost of the algorithm and the total cost of the best performing expert, i.e., the expert that incurs the least overall cost.

A well-known folklore algorithm for handling the discrete prediction with experts problem is the weighted majority algorithm . The deterministic variant of the weighted majority algorithm simply initializes "weights" for all experts to \(1\), down-weights any incorrect expert on a given day, and selects the prediction supported by the largest weight of experts. The algorithm solves the discrete prediction with experts problem with \(O(M+ n)\) total mistakes, where \(M\) is the number of mistakes made by the best expert, thus achieving total regret \(O(M+ n)\). More generally, a largebody of literature has studied optimizations to the weighted majority algorithm, such as a randomized variant where the probability of the algorithm selecting each prediction is proportional to the sum of the weights of the experts supporting the prediction. The randomized weighted majority algorithm achieves regret \(O()\), which has been shown to be information-theoretically optimal, up to a constant. There have subsequently been many follow-ups to the weighted and randomized weighted majority algorithms that achieve similar regret bounds, but improve in other areas. For example, on a variety of structured problems, such as online shortest paths, follow the perturbed leader  achieves the same regret bound as randomized weighted majority but uses less runtime on each day. In addition, the multiplicative weights algorithm achieves the optimal \(\) regret, with a tight leading constant . However, these classic algorithms use a framework that maintains the cumulative cost of each expert, which requires the algorithm to store \((n)\) bits of information.

Memory bounds.Recently,  considered the online learning with experts problem when memory is a premium for the algorithm. On the hardness side, they showed that any algorithm achieving a target average regret \(R\) requires \((T})\) space, which implies that any algorithm achieving the information-theoretic \(O()\) regret must use near-linear space. On the other hand, when the number of mistakes \(M\) made by the best expert is small, i.e., \(M=O(R^{2}T)\),  gave a randomized algorithm that uses \(()\) space for arbitrary-order streams, thus showing that the hardness of their lower bound originates from a setting where the best expert makes a large number of mistakes.

Subsequently,  considered the online learning with experts problem when the algorithm is limited to memory sublinear in \(n\). They introduced a general framework that achieves \(o(T)\) regret using \(o(n)\) memory, with a trade-off parameter between space and regret that obtains \(O_{n}(T^{4/5})\) regret with \(O()\) space and \(O_{n}(T^{0.67})\) regret with \(O(n^{0.99})\) space.

Adaptive inputs.Up to now, the discussion has focused on an oblivious setting, where the input to the algorithm may be worst-case, but is chosen independently of the algorithm and its outputs. The online learning with experts problem is often considered in the adaptive setting, where the input to the algorithm is allowed to depend on previous outputs by the algorithm, e.g., in financial markets, future stock quotes can depend on previous investment choices. Formally, we define the adaptive setting as a two-player game between an algorithm \(\) and an adversary \(\) that adaptively creates the input stream to \(\). The game then proceeds in days and on the \(t\)-th day:

1. The adversary \(\) chooses the outputs of all experts on day \(t\) as well as the outcome of day \(t\), depending on all previous stream updates and all previous outputs from the algorithm \(\).
2. The outputs (i.e., predictions) of all experts are simultaneously given to the algorithm \(\), which updates its data structures, acquires a fresh batch \(R_{t}\) of random bits, and outputs a predicted outcome for day \(t\).
3. The outcome of day \(t\) is revealed to \(\), while the predicted outcome for day \(t\) by \(\) is revealed to the adversary \(\).

The goal of \(\) is to induce \(\) to make as many incorrect predictions as possible throughout the stream. It is clear that any deterministic algorithm for the online learning with experts problem will maintain the same guarantees in the adaptive model. Unfortunately, both the algorithms of  and  are randomized procedures that rely on iteratively sampling "pools" of experts, which can potentially be exploited by an adaptive adversary who learns the experts sampled in each pool. Interestingly, both the randomized weighted majority algorithm  and the multiplicative weights algorithm  are known to be robust to adaptive inputs.

### Our Contributions

In this paper, we study the capabilities and limits of sublinear space algorithms for the online learning with experts problem on adaptive inputs.

Robust algorithms.Towards adaptive robustness, it is natural to study deterministic algorithms, since they retain the same guarantee under adaptive adversaries. As a warm-up, we first provide a simple deterministic algorithm that uses space \(()\). Consider an algorithm that iteratively selects the next pool of \(k=()\) experts and running the deterministic majority algorithm on the experts in the pool, while removing any incorrect experts from the pool until the pool is completely depleted, at which point the next pool of \(()\) experts is selected. The main intuition is that each pool can incur at most \(O( n)\) mistakes before it is depleted and the best expert can only make \(M\) mistakes. By the time the pool has cycled through \(nM\) experts, i.e., \(M\) times for each of the \(n\) experts, then the best expert no longer makes any mistakes and will be retained by the pool. Thus, the total number of mistakes made by the algorithm is \( O( n)\). On the other hand, for a target average regret \(R\), the mistake bound of the algorithm is required to be at most \(M+RT\), so it suffices to set \(k=()\) to achieve regret \(R\). Since the algorithm runs deterministic majority on a pool of \(k=()\) experts, then this algorithm uses \(()\) space. Formally, we show:

**Theorem 1.1** (Simple deterministic algorithm; see Section3.1).: _Suppose the best expert makes \(M\) mistakes and let \(R\). There exists a deterministic algorithm (Algorithm2) that uses space \(()\) and achieves an average regret of \(R\)._

The algorithm is simple, computationally efficient, and easy to implement. However, the drawback is that for \(M=(RT)\), the algorithm requires space near-linear in the number of experts \(n\), which is undesirable when \(n\) is large. To address this issue, we complement our deterministic algorithm with a randomized algorithm that is robust to adaptive inputs and allows for a different memory-regret trade-off:

**Theorem 1.2** (Robust randomized algorithm).: _Let \(R>n}{T}\), and suppose the best expert makes at most \(MT}{128^{2}n}\) mistakes. Then there exists an algorithm for the discrete prediction with experts problem that uses \((})\) space and achieves regret at most \(R\), with high probability._

This gives a trade-off between the space and regret, almost all the way to the information-theoretic limit of \(R=O_{n}()\) for general worst-case input. However, it incurs a multiplicative space overhead of \(()\) compared to the optimal algorithms for oblivious input. Thus we believe the complete characterization of the space complexity of the discrete prediction with experts problem with adaptive input is a natural open question resulting from our work.

Tight memory bounds for robust algorithms.It is natural to ask whether there exist robust algorithms that are more space-efficient than the straightforward deterministic approach. For example,  showed that any oblivious randomized algorithm with failure probability \(2^{-(nT)}\) will be robust against adaptive outputs in the discrete prediction with experts problem, so a reasonable approach would be to boost the success probability of existing oblivious algorithms to \(1-2^{-(nT)}\). Unfortunately, we show this cannot work:

**Theorem 1.3** (Memory lower bound for high-probability algorithms).: _For \(n=o(2^{T})\), any randomized algorithm that achieves \(R\) regret with probability at least \(1-2^{-(T)}\) for the discrete prediction with experts problem must use \(()\) space when the best expert makes \(M\) mistakes._

In particular, Theorem1.3 shows that any deterministic algorithm must use \(()\) space, which taken together with the deterministic procedure above, resolves the deterministic streaming complexity of online learning with experts. We emphasize that Theorem1.3 also shows that using the strategy of high-probability randomized algorithms to guarantee robustness against adaptive input does not work any better than a deterministic algorithm.

At a conceptual level, our lower bound in Theorem1.3 shows that surprisingly, the number \(M\) of the mistakes made by the best expert is an intrinsic parameter that governs the abilities and limitations of robust algorithms in this model. Thus, even though \(M\) is not a parameter that may naturally be ascertained in practice, it nevertheless completely characterizes the complexity of the problem. On the other hand, for algorithmic purposes, it suffices to acquire a constant-factor approximation to \(M\) as an input to the algorithm.

Another reason Theorem1.3 is somewhat surprising is because as the number of mistakes \(M\) made by the best expert increases, then the algorithm is also permitted to make more mistakes and in some sense, the problem seems "easier". However, Theorem1.3 shows this intuition is not true--the problem actually becomes more difficult as \(M\) increases.

Moreover, we give an alternative proof in the regime when \(M=(T)\). The proof differs from the proof of Theorem 1.3. Instead, it leverages the communication complexity of a new set disjointness problem, recently proposed by . The statement is technically weaker Theorem 1.3 and appears in the appendix; see Appendix E.

Empirical evaluations.Finally, we conduct experimental evaluations in Section 5 by comparing the natural deterministic algorithm to the randomized algorithm of  against a white-box adversary who has access to the internal state of the algorithm, including any experts sampled and maintained by the algorithm. The deterministic algorithm iteratively selects pools of \(k=()\) experts, discarding any expert that has erred, and refreshing the pool with the next batch of \(k\) experts once the pool is emptied. The randomized algorithm similarly discards erroneous experts from a pool of \(k\) experts, but it repeatedly samples pools of \(k\) experts rather than selecting the next pool of \(k\) experts. On average across the multiple trials for each setting, the randomized algorithm made several times more mistakes than the deterministic algorithm, ranging from \(1.98\)x times more mistakes to \(3.29\)x times more mistakes than the deterministic algorithm, thus demonstrating the importance of robust algorithms against adversarial inputs.

### Related Work

The experts problem and memory bounds.The experts problem has been extensively studied , both in the discrete decision setting  and in the setting where costs are determined by various loss functions . Hence, the experts problem can be applied to many different applications, such as portfolio optimization , ensemble boosting , and forecasting . Given certain assumptions on the expert, such as assuming the experts are decisions trees , threshold functions , or have nice linear structures , additional optimizations have been made to improve the algorithmic runtimes for the experts problem and more generally, existing work has largely ignored optimizing for memory constraints in favor of focusing on time complexity or regret guarantees, thus frequently using \((n)\) memory to track the performance of each expert.

Recently,  introduced the study of memory-regret trade-offs for the experts problem. For \(n T\),  showed that the space complexity of the problem is \(()\) in the random-order streams, but also gave a randomized algorithm that uses \(()\) space for arbitrary-order streams when the number of mistakes \(M\) made by the best expert is "small". Subsequently,  considered the online learning with experts problem for \(T n\), introducing a general space-regret trade-off framework that achieves \(o(T)\) regret using \(o(n)\) memory, including \(O_{n}(T^{4/5})\) regret with \(O()\) space and \(O_{n}(T^{0.67})\) regret with \(O(n^{0.99})\) space.

Concurrent and independent work.Concurrent to our work,  considered a variant of the problem where at each time, the algorithm selects an expert instead of a prediction. They then introduce an algorithm robust against an adaptive adversary who observes the specific expert chosen by the algorithm at each time, as well as lower bounds for any algorithm robust to such an adversary.

One way to ensure adversarial robustness is through deterministic algorithms. On that end, we achieve stronger lower bounds for deterministic algorithms, showing that there must be a dependency on the number \(M\) of mistakes made by the best expert, i.e., any deterministic algorithm achieving amortized regret \(R\) must use \(()\) space. In fact, when the number of mistakes \(M\) made by the best expert is sufficiently small, i.e., \(M=O(T}{^{2}n})\) for amortized regret \(R\), we give a randomized upper bound that uses _less_ space than this lower bound. By comparison, the lower bound of  shows that any algorithm achieving \(R\) amortized regret must use \((})\) space, though their lower bound also applies to randomized algorithms.

Due to the difference in setting, our algorithmic techniques are quite different from those of . We use a recent idea of  to hide the internal randomness of our algorithm from the adversary whereas  rotates between groups of experts to prevent an adversary from inducing high regret by making a specific expert bad immediately after it is selected.

## 2 Preliminaries

For any \(t n\) and vector \((X_{1},X_{2},,X_{n})\), we let \(X_{<t}\) denote \((X_{1},,X_{t-1})\), \(X_{ t}=(X_{1},,X_{t})\), and \(X_{-t}=(X_{1},,X_{t-1},X_{t+1},,X_{n})\). Also, \(X_{>t}\) and \(X_{ t}\) are defined similarly. Let \(e_{i}\) denote the \(i\)th standard basis vector, and for any \(S\), \(e_{S}\) the vector that has a \(1\) at index \(i S\) and \(0\) everywhere else. For a random variable \(X\), let \(H(X)\) denote its entropy.

We write \([n]\) for an integer \(n>0\) to denote the set \(\{1,,n\}\). We write \((n)\) to denote a fixed polynomial in \(n\). If an event occurs with probability at least \(1-(n,T)}\), we say the event occurs with high probability. We give additional technical preliminaries in Appendix B.

Formal problem statement.In the online learning with experts problem, there are \(n\) experts that each make predictions on each of \(T\) days. The prediction are in \(\{0,1\}\). An algorithm uses the experts to output a prediction for each day \(t[T]\). The actual outcome of the day \(t\) is then revealed, at which point the algorithm is penalized with a cost that is 0 if the prediction is correct, and 1 otherwise.

This process continues for the \(T\) days. At the end, suppose that the best expert has incurred cost \(M\), while the algorithm has incurred \(C\). Then the performance of the algorithm is measured by the (average) regret \(R=(,0)\).

Differential privacy.We use tools from differential privacy.

**Definition 2.1** (Differential privacy, ).: _Given a privacy parameter \(>0\) and a failure parameter \((0,1)\), a randomized algorithm \(:^{*}\) is \((,)\)-differentially private if, for every pair of neighboring streams \(S\) and \(S^{}\) and for all \(E\),_

\[}[(S) E] e^{ }}[(S^{}) E]+.\]

**Theorem 2.2** (Private median, e.g., ).: _Given a database \( X^{*}\), a privacy parameter \(>0\) and a failure parameter \((0,1)\), there exists an \((,0)\)-differentially private algorithm PrivMed that outputs an element \(x X\) such that with probability at least \(1-\), there are at least \(-m\) elements in \(S\) that are at least \(x\), and at least \(-m\) elements in \(S\) that are at most \(x\), for \(m=O()\)._

**Theorem 2.3** (Advanced composition, e.g., ).: _Let \(,^{}(0,1]\) and let \(\). Any mechanism that permits \(k\) adaptive interactions with mechanisms that preserve \((,)\)-differential privacy guarantees \((^{},k+^{})\)-differential privacy, where \(^{}=}}+2 k^{2}\)._

**Theorem 2.4** (Generalization of DP, e.g., ).: _Let \((0,1/3)\), \((0,/4)\), and \(n}\). Suppose \(:X^{n} 2^{X}\) is an \((,)\)-differentially private algorithm that curates a database of size \(n\) and produces a function \(h:X\{0,1\}\). Suppose \(\) is a distribution over \(X\) and \(S\) is a set of \(n\) elements drawn independently and identically distributed from \(\). Then_

\[_{S,h(S)}[| _{x S}h(x)-*{}_{x}[h(x) ]| 10]<.\]

## 3 Algorithms Against Adaptive Adversaries

In this section, we show that there exists algorithms for the discrete prediction with experts problem that is robust to adaptive outputs.

### A Near-Optimal Deterministic Algorithm

We first present a simple deterministic algorithm for arbitrary-order streams. The algorithm repeatedly selects pools of the next \(k=()\) experts. While the pool is non-empty, the algorithm runs the deterministic majority algorithm on the algorithm and removes any incorrect experts from the pool. Once the pool is empty, the next \(()\) experts are added to the pool, possibly cycling through all \(n\) experts multiple times if necessary, where an expert can be added to the pool again even if it has been previously deleted from the pool. We give the formal algorithm and analysis in Appendix C.

**Theorem 3.1** (Determistic algorithm).: _Among \(n\) experts in a stream of length \(T\), suppose the best expert makes \(M\) mistakes and let \(R\). There exists a deterministic algorithm (Algorithm 2) that uses space \(()\) and achieves an average regret of \(R\)._

In light of lower bound Theorem1.3, it is evident that Theorem3.1 is nearly optimal, up to polylogarithmic factors, for deterministic algorithms, which are automatically adversarially robust. On the other hand, it does not seem necessary that any adversarially robust algorithm must be deterministic. Indeed, we now give a randomized adversarially robust algorithm with better space guarantees.

### A Randomized Robust Streaming Algorithm

We first recall the following randomized algorithm for arbitrary-order streams with oblivious input, i.e., non-adaptive input:

**Lemma 3.2** (Algorithm for oblivious inputs; ).: _Let \(R>n}{T}}\), and suppose the best expert makes at most \(MT}{1280^{2}n}\) mistakes. Then there exists an algorithm DiscPred for the discrete prediction with experts problem that uses \(()\) space and achieves regret at most \(R\), with high probability, i.e., probability at least \(1-(n,T)}\)._

The algorithm of Lemma3.2 for constant probability proceeds by sampling pools of \(k=()\) experts and running majority vote on the pool, while iteratively deleting poorly performing experts until no experts remain in the pool, at which a new pool of \(k\) experts is randomly sampled. The main intuition is that either the pool of experts will perform well and achieve low regret, or the pool will be continuously re-sampled until the best expert is sampled multiple times, after which point it will not be deleted from the pool. Unfortunately, it is not evident that this algorithm is robust to adaptive inputs because an adversary can potentially learn the experts in each sampled pool and force the experts to make mistakes only on days in which they are sampled by the algorithm. To boost the algorithm to high probability of success, we take the deterministic majority vote of \(O( n)\) independent instances of the algorithm with constant success probability.

Towards adaptive robustness, we use differential privacy to hide the internal randomness of the algorithm, and in particular, the identity of the experts that are sampled by each pool. We first run \(()\) copies of the algorithm and then output the private median of the \(()\) copies, guaranteeing roughly \((()},0)\)-differential privacy because we use \(()\) copies of the algorithm. Advanced composition, i.e., Theorem2.3, then ensures \((O(1),1/(n))\)-differential privacy, so that correctness then follows from the generalization properties of DP, i.e., Theorem2.4.

We give our algorithm in full in Algorithm1.

```
0: A stream of length \(T\) with \(n\) experts and a target regret \(R\)
0: A sequence of predictions with regret \(R\)
1: Run \(m=O((nT))\) independent instances of DiscPred with regret \(\)
2: Run PrivMed on the \(m\) instances with privacy parameter \(=O((nT)})\) and failure probability \(=(n,T)}\)
3: At each time \(t[T]\), select the output of PrivMed
```

**Algorithm 1** Randomized, robust streaming algorithm for the experts problem

Next, we show the correctness of our algorithm on adaptive inputs.

**Theorem 3.3** (Algorithm for adaptive inputs).: _Let \(R>n}{T}}\), and suppose the best expert makes at most \(MT}{1280^{2}n}\) mistakes. Then there exists an algorithm for the discrete prediction with experts problem that uses \((})\) space and achieves regret at most \(R\), with probability at least \(1-(n,T)}\)._Proof.: Suppose we run \(m=O((nT))\) independent instances of DiscPred with regret \(\). Note that for \(R>n}{T}}\), we have \(>n}{T}}\), which is a valid input to DiscPred in Lemma3.2. By Lemma3.2, each instance succeeds on an arbitrary-order stream with probability at least \(1-1/(n,T)\). By a union bound over the \(m\) instances, all instances succeed with probability at least \(1-1/(n,T)\). In particular, each instance has regret at most \(R/4\), so that the total number of mistakes by each instance is at most \(M+RT/4\). Thus, the total number of mistakes by all instances is at most \(m(M+RT/4)\).

To consider an adaptive stream, observe that PrivMed is called with privacy parameter \(O(1/(nT))\) and failure probability \(1/(n,T)\). By Theorem2.3, the mechanism permits \(T\) adaptive interactions and guarantees privacy \(O(1)\) with failure probability \(1/(n,T)\). By Theorem2.4, we have that with high probability, if the output of the algorithm is incorrect, then at least \(m/3\) of the instances DiscPred are also incorrect. Since the total number of mistakes by all instances is at most \(m(M+RT/4)\), then the total number of mistakes by the algorithm is at most \(3(M+RT/4) M+RT\), since \(MT}{1280^{2}n}\). Hence, the algorithm achieves \(R\) regret with high probability.

By Lemma3.2, each instance of DiscPred uses \(()\) space. Since we use \(m=O((nT))\) independent instances of DiscPred, then the total space is \((})\). 

## 4 Lower Bound for Arbitrary-Order Streams

In this section, we provide a space lower bound for randomized algorithms with a high probability of success. Together with Theorem1.1, the lower bound completely characterizes the complexity of deterministic algorithms for the online learning with experts problem. We restate Theorem1.3, give a proof sketch and defer the full analysis to AppendixD.

**Theorem 4.1** (Memory lower bound for high-probability algorithms).: _For \(n=o(2^{T})\), any randomized algorithm that achieves \(R\) regret with probability at least \(1-2^{-(T)}\) for the discrete prediction with experts problem must use \(()\) space when the best expert makes \(M\) mistakes._

Proof sketch of Theorem4.1.: We consider the communication problem of \(\)-DiffDist. It combines \(n\) instances of the distributed detection problem given by . This was also used by the prior work of  to prove space lower bounds for expert learning in random-order stream.

Specifically, for fixed \(T\), the \(\)-DiffDist problem with \(=\) consists of \(T\) players, who each hold \(n\) bits, indexed from \(1\) to \(n\). The players must distinguish between:

1. the NO case \(_{}^{(n)}\), in which every bit for every player is drawn i.i.d. from a fair coin and
2. the YES case \(_{}^{(n)}\), in which an index \(L[n]\) is selected arbitrarily and the \(L\)-th bit of each player is chosen i.i.d. from a Bernoulli distribution with parameter \((1-)\), while all other bits for every player are chosen i.i.d. from a fair coin.

At a high level, the proof proceeds in two steps:

1. First, we show that the \(\)-DiffDist problem can be reduced to the expert prediction problem in the streaming setting.
2. Second, we prove a communication complexity lower bound for \(\)-DiffDist against any protocol that succeeds with probability \(1-2^{-(T)}\), which includes deterministic protocols.

The first step is straightforward. In the reduction, each player in an instance of \(\)-DiffDist corresponds to a day of the expert problem. The \(n\) bit input held by each player correspond to the \(n\) expert predictions of each day. Therefore, in the NO case, each expert is correct on roughly half of the days. In the YES case, there is a single expert \(L[n]\) that is correct on roughly \(1/2+\) of the days (for \(=1/2-M/T\)), while all other experts randomly guess each day. Suppose that there is a streaming algorithm for the expert prediction problem with average regret \(/2\). Then roughly speaking, in the YES case, the algorithm is correct approximately on \(1/2+/2\) of the days, while in the NO case where every expert is randomly guessing, the algorithm is correct on less than \(1/2+/2\) of the days. This distinguishes the YES and NO case and thus solves \(\)-DiffDist.

For the second step, we show that solving the \(\)-DiffDist problem with probability at least \(1-2^{-(T)}\) requires \((nM)\) total communication. We give a sketch of the argument below.

Observe that if the input is viewed as a \(T n\) matrix, then \(^{(n)}_{}\) is a product distribution across columns that can be written as \(^{n}\), where \(\) is the distribution over a single column such that all entries of the column are i.i.d. Bernoulli with parameter \(\). We view \(^{(n)}_{}\) as a hard distribution and applies an information complexity analysis. By a direct sum argument, it suffices to show that the single column problem, i.e., distinguishing between \(^{(1)}_{}\) and \(^{(1)}_{}\) (i.e., for \(n=1\)), requires \((M)\) total communication.

Let \((C_{1},C_{2},,C_{T})\) be a single column drawn from the hard distribution--namely, the NO case where each player holds one i.i.d. Bernoulli with parameter \(1/2\). Let \(A\) be a fixed protocol with success probability at least \(1-(-(T))\). For all \(i<T\), let \(M_{i}\) denote the message sent from player \(P_{i}\) to player \(P_{i+1}\) and \(M_{<i}=\{M_{j}:j<i\}\). Let \(=(C_{1},,C_{T})\) be the communication transcript of \(A\) given the input \((C_{i})_{i=1}^{T}\). A standard information complexity argument  implies that the total communication is at least the _information cost_, defined as \(I(C_{1},,C_{T};(C_{1},,C_{T}))\), where \(I(X,Y)\) denotes the mutual information between random variables \(X\) and \(Y\).

The key step now is to lower bound the information cost by \((M)\). The main ideas are the following. For any \(i[T]\), we say that \((M_{i},M_{<i})\) is _informative_ for \(i\) with respect to the input \(C\) and the transcript \(=(M_{1},M_{2},,M_{T})\) if

\[|(C_{i}=0 M_{i},M_{<i})-(C_{i}=1 M_{i},M_{<i} )| c \]

for some constant \(c>0\). Otherwise, we say that \(M_{i}\) is uninformative. Informally, an informative message \(M_{i}\) reveals sufficiently large information about \(C_{i}\) so that the mutual information \(I(M_{i},C_{i} M_{<i})\) would be large. Let \(p_{i}\) be the probability that \((M_{i},M_{<i})\) is informative. Intuitively, we need that \(_{i}p_{i}\) is large, because then there would be sufficiently many informative messages, and so the information cost is high. To formalize this approach, we claim two key lemmas. First, by Lemma D.9

\[I(;C_{1},C_{2},,C_{T})=_{j=1}^{T}I(M_{j};C_{j} M_{<j} )(_{j=1}^{T}p_{j}).\]

Conceptually, this shows that the information cost is at least the expected number of informative messages. Furthermore, by Lemma D.10, the latter is indeed high, and in particular, \(_{j}p_{j}(M)\). Much of the technical work is dedicated to prove these lemmas. This finishes the proof since the communication complexity is lower bounded by the information cost. 

## 5 Experimental Evaluations

In this section, we perform experimental evaluations as a simple proof-of-concept demonstrating the importance of deterministic algorithms against adversarial input.

Experimental setup.We assume a white-box adversary with access to the internal state of the algorithm. We evaluate the natural deterministic algorithm that iteratively selects pools of \(k=()\) experts, discarding any expert that has erred, and refreshing the pool with the next batch of \(k\) experts once the pool is emptied. As a baseline, we compare to a randomized algorithm that repeatedly samples pools of \(k=()\) experts, discarding any expert that has erred, and refreshing the pool with the next batch of \(k\) sampled experts once the pool is emptied.

Provided that the best expert has not yet made \(M\) mistakes, the adversary simply compels the experts in each pool to err. Once all experts have made at least \(M\) mistakes, the adversary gives up and permits all subsequent predictions to be correct. It can be theoretically verified that against such an adversary, the deterministic algorithm is the optimal algorithm, in the sense that it achieves the smallest number of errors.

Experimental details.We first evaluate our experiments on the setting \(n=10000\), \(M=20\), and \(T=1000\) across various values of \(R\{0.05,0.1,0.15,0.2,0.25,0.3,0.35\}\). For each setting of \(R\), we ran the experiment \(20\) times, recording the runtime and number of errors by the algorithms in each repetition. We then computed the minimum, mean, and maximum number of errors by the randomized algorithm across all \(20\) repetitions. We then repeated the experimental setup for a \(10x\) larger setting of \(T\), i.e., \(n=10000\), \(M=20\), and \(T=1000\). Our experiments were performed on a 64-bit operating system using an AMD Ryzen 7 5700U CPU with 8.00 GB RAM and 8 cores with base clock 1.80 GHz.

Results.Our experiments show that the deterministic algorithm performs significantly better than the randomized algorithm. On average across the \(20\) trials for each setting, the randomized algorithm made several times more mistakes than the deterministic algorithm, ranging from \(1.98\)x times more mistakes for the setting \(n=100000,M=20,T=1000,R=0.05\) to \(3.06\)x times more mistakes for the setting \(n=100000,M=10,T=10000,R=0.3\). Even the best performance by a randomized algorithm over all trials, which occurred at the setting \(n=100000,M=20,T=1000,R=0.05\), the randomized algorithm made \(1.9\)x times more mistakes than the deterministic algorithm. Meanwhile, the worst performance by a randomized algorithm over all trials, which occurred at the setting \(n=100000,M=10,T=10000,R=0.25\), the randomized algorithm made \(3.29\)x times more mistakes than the deterministic algorithm. The average runtime was \(98\) seconds for each batch of \(20\) experiments for the setting of \(n=100000,M=20,T=1000,R=0.05\) while the average runtime was 98 seconds for each batch of \(20\) experiments for the setting of \(n=100000,M=10,T=10000,R=0.3\) was roughly \(350\) seconds. See Figure 1 for a summary.

## 6 Conclusion

In this work, we provide robust streaming algorithms for learning with experts. We provide a deterministic algorithm parametrized by the number of mistakes made by the best expert. We also give a randomized algorithm with a different space-regret trade-off, based on differential privacy. We complement our algorithms with a lower bound for high-probability success algorithms. This gives tight memory lower bound for deterministic algorithms. We then show the importance of robust algorithmic design by empirically comparing the performance of the natural deterministic algorithm and the state-of-the-art randomized algorithm when the inputs are adaptive.

We remark that our results do not rule out space-efficient robust algorithms that match the bounds of the oblivious randomized algorithm of  for constant probability of success. We believe whether or not there exists such an algorithm is a fascinating question for future work.

Figure 1: Comparison of errors made by deterministic algorithm and average number of errors made by randomized algorithm across 20 repetitions for each trial, across various values of input target regret \(R\). Minimum and maximum numbers of errors by randomized algorithm across each trial are also reported.