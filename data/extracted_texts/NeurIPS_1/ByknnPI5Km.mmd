# ABCFair: an Adaptable Benchmark approach

for Comparing Fairness methods

 MaryBeth Defrance

Ghent University

marybeth.defrance@ugent.be &Maarten Buyl

Ghent University

maarten.buyl@ugent.be &Tijl De Bie

Ghent University

tijl.debie@ugent.be

###### Abstract

Numerous methods have been implemented that pursue fairness with respect to sensitive features by mitigating biases in machine learning. Yet, the problem settings that each method tackles vary significantly, including the stage of intervention, the composition of sensitive features, the fairness notion, and the distribution of the output. Even in binary classification, these subtle differences make it highly complicated to benchmark fairness methods, as their performance can strongly depend on exactly how the bias mitigation problem was originally framed.

Hence, we introduce ABCFair, a benchmark approach which allows adapting to the desiderata of the real-world problem setting, enabling proper comparability between methods for any use case. We apply ABCFair to a range of pre-, in-, and postprocessing methods on both large-scale, traditional datasets and on a dual label (biased and unbiased) dataset to sidestep the fairness-accuracy trade-off.

## 1 Introduction

Fairness has become a firmly established field in AI research as the study and mitigation of algorithmic bias. Thus, the range of methods that pursue AI fairness is now broad and varied . Many have been implemented in large toolkits such as _AIF360_, _Fairlearn_, _Aequitas_, or in libraries with a narrower focus like _Fair Fairness Benchmark_ (FFB) , _error-parity_, and _fairret_.

So, which of these methods performs 'best'? Benchmarks in the past  tend to search for the best method _per dataset_. We argue such a benchmarking approach is of limited value in practice, as the real-world context of bias in AI systems imposes many subtle, yet significant, desiderata that a benchmark should align with before they can be properly compared.

Moreover, prior work commonly observes a trade-off: the more fair a model, the less accurate its predictions. This is unsurprising as fairness is typically pursued in settings where the training data _and the evaluation data_ are both assumed to be biased . Removing bias from predictions thus leads to degradation of this biased accuracy measure . However, theoretical work has shown that this is not necessarily the case when evaluating on less biased data .

Contributions.We formalize four types of desiderata that could arise in real-world classification problems. These include **1)** the stage where the method intervenes, **2)** the composition of sensitive groups, **3)** the exact definition of fairness, and **4)** the distribution that is expected from the model output. Figure 1 shows where these desiderata pose comparability challenges in a fairness pipeline.

We introduce _ABCFair_: an adaptable benchmark approach for comparing fairness methods. Through the use of three flexible components, the Data, FairnessMethod, and Evaluator, it can adapt to a range of desiderata imposed by the task. The approach is validated by benchmarking it on 10 methods, 7 fairness notions, 3 formats of sensitive features, and 2 output distribution formats.

We further introduce the concept of evaluating on two types of datasets. A _dual_ label dataset, which contains both biased and unbiased labels for evaluation . Such datasets allow us to train a method with biased labels (simulating real-world settings), while still evaluating accuracy and fairness over less biased labels. We later refer to these less biased labels as unbiased to emphasize the difference with the traditionally biased labels. This allows us to challenge the notion of the fairness-accuracy trade-off . We extend the analysis of the performance of bias mitigation methods for this dataset to focus on the fairness-accuracy trade-off . To the best of our knowledge, we are the first to use a real-world dual label dataset in a benchmark.

However, dual label datasets are rare and often small. They present interesting findings, but a more robust dataset is needed for full evaluation. We therefore also introduce a more comprehensive evaluation method for large-scale traditional datasets, such as the folktables datasets . Which is used in many benchmarks, even for topics other than bias mitigation method .

All our code and results are available at [https://github.com/aida-ugent/abcfair](https://github.com/aida-ugent/abcfair).

Related Work.An early fairness benchmark was performed by Cardoso et al. . They sampled synthetic datasets from Bayesian networks fitted to real-world data and measured model performance as a function of the bias in the dataset. Another early fairness benchmark was done by Friedler et al. , who were among the first to jointly consider multiple sensitive attributes and a wide range of fairness notions. Biswas and Rajan  take a more pragmatic approach by benchmarking different types of fairness methods applied to top-rated (and hence realistic) models from Kaggle.

    &  &  & Fairness & Multiple \\  & Dual & Biased & Pre- & In- & Post- & notions & sens. feat. \\  L. Cardoso et al.  & ✓ & ✗ & 5 & 0 & 0 & 3 & ✗ \\ Friedler et al.  & ✗ & ✓ & 1 & 3 & 0 & 7 & ✓ \\ Biswas and Rajan  & ✗ & ✓ & 2 & 2 & 3 & 6 & ✓ \\ Fairea  & ✗ & ✓ & 3 & 2 & 3 & 2 & ✗ \\ Islam et al.  & ✗ & ✓ & 0 & 2 & 0 & 4 & ✓ \\ Fair Fairness Benchmark  & ✗ & ✓ & 0 & 6 & 0 & 6 & ✗ \\ Cruz and Hardt  & ✗ & ✓ & 2 & 3 & 1 & 3 & ✓ \\  ABCFair (ours) & ✓ & ✓ & 4 & 5 & 1 & 7 & ✓ \\   

Table 1: Quantitative comparison of fairness benchmarks. Fairness notions are only counted once per benchmark, even if they measure the violation of that notion in multiple ways.

Figure 1: Structural overview of the ABCFair benchmark approach.

Benchmarks tend to reveal an apparent trade-off between fairness and accuracy . The Fairea  benchmark proposes a baseline for 'worst possible' trade-off that flips a model's outcomes completely at random in pursuit of a fairness constraint. Islam et al.  challenge whether a trade-off always occurs, as they empirically find that sensible hyperparameter tuning can already lead to improved fairness without loss in performance. More recently, the Fair Fairness Benchmark  (FFB) focuses on evaluating imprecessing methods and discuss their training stability. Yet, Cruz and Hardt  show that pre- and inprocessing methods anyway never achieve a better trade-off than simply applying group-specific thresholding to pareto-optimal classifiers, as such a postprocessing procedure is then pareto-optimal. They found this empirically holds for most pareto-optimal classifiers.

The novelty of our benchmarking approach is found in the comparability challenges we identify in Sec. 2, which we address with the _ABCFair_ pipeline in Sec. 3. A quantitative comparison in Tab. 1 validates that our benchmark's coverage of methods and fairness notions is on par with prior work.

## 2 Comparability Challenges in Fairness Benchmarking

Out of all machine learning tasks, AI fairness research has been mainly focused on binary classification . This involves learning a classifier \(h:\{0,1\}\) where, for a random feature vector \(X\), the prediction \(h(X)\) is close to the random output label \(Y\{0,1\}\). Let \(\) denote a model tuning procedure that pursues this goal for a training distribution \(D\) over \(X\) and \(Y\), i.e. \(h=(D)\), in a broad sense, including both fitting a neural net and meta-learning approaches .

In _fair_ binary classification, the predictions \(h(X)\) should be unbiased (enough) with respect to _sensitive_ features \(S\), such as gender or ethnicity. This, unfortunately, is already the greatest common denominator of desiderata that all fairness methods pursue. In what follows, we discuss the variation of this problem setting across methods, and how it leads to comparability challenges.

### Stage of Intervention

Surveys partition fairness methods in three types : _preprocessing_, _inprocessing_, and _postprocessing_. Preprocessing methods modify the training distribution \(D\) to remove clear biases, such as undesired correlations between the sensitive features \(S\) and the label \(Y\). Inprocessing methods modify model tuning \(\), e.g. imposing a constraint during training . Postprocessing methods only modify the output \(h(X)\), e.g. separate classification thresholds for each demographic group .

The stage at which a fairness method intervenes is not a purely cosmetic distinction; it also constrains its applicability. Preprocessing requires access to the training data, which means third parties cannot use such methods on classifiers trained with private datasets. Conversely, dataset providers cannot make guarantees about the fairness of classifiers trained on their data by third parties (who may introduce new biases). Fairness methods typically also require access to the sensitive features \(S\), possibly entailing privacy concerns for e.g. postprocessing methods that require access to someone's sensitive information for every prediction. In practice, this is often infeasible .

Hence, fairness benchmarks cannot be blind to the stage in which a fairness method intervenes or when each method requires access to \(S\).

### Composition of Sensitive Features

There are many attributes that enjoy legal protection from discrimination . Yet, the format in which these attributes are formalized as sensitive features \(S\) can differ.

Many methods use a _binary_ format , i.e. they assume there are only two groups to be considered: the advantaged and the disadvantaged. Formally, the domain of sensitive features \(=2\). Though this is indeed applicable to a binary encoding of gender, it is a coarse categorization in practice: the 'disadvantaged' group may contain some subgroups towards whom the bias is much less significant than others. Bias against the latter is then underestimated.

Fairness methods are more practical if they admit a _categorical_ format , i.e. an arbitrary amount of demographic categories \(_{>0}\). Not only is this a better fit for non-binary demographics, it also allows for _intersections_ of demographic groups, like black women , with respective domains\(_{1}\) and \(_{2}\) to be encoded as categories in the product \(_{1}_{2}\). Of course, this quickly leads to a deterioration of statistical power when measuring bias against increasingly granular intersections.

A middle ground is to instead consider a _parallel_ format, i.e. multiple axes of demographic attributes independently . The domain of sensitive axes \(_{1}\) and \(_{2}\) remains its product \(_{1}_{2}\), but bias is only considered within a single \(_{k}\) at a time. For example, unwanted behaviour within a model can be detected if it is i) biased against black people _or_ ii) biased against women, but if it shows bias against the subgroup of black women.

Clearly, bias metrics depend on whether sensitive features \(S\) are binary, categorical (possibly encoding intersections), or parallel. In our approach, we thus measure bias separately in each format. Where possible, we also configure the fairness method to optimize for it. Note that we apply binary binning to non-categorical sensitive attributes, like age, due to compatibility constraints for many methods.

### Incompatibility of Fairness Notions

Mathematically, bias is a broad concept that could refer to a pattern in the data, the way the model works, or the model output . Defining fairness over the output is by far most popular, and a wealth of empirical and theoretical results have made it clear that such a definition can take on a wide range of mathematical forms , which are often mutually incompatible . Observational studies have shown that humans are also not unanimous on the best definition . Consequently, benchmarks of fairness methods keep track of multiple notions of fairness at the same time .

In fact, many methods can be configured to optimize for a specific definition out of several options. This again introduces comparability problems. Methods that _can_ optimize a certain fairness definition will likely have an advantage of those that can not, making these methods only comparable for the notions where they intersect. This intersection is not always obvious either. The names of fairness notions can be ambiguous, e.g. with 'demographic parity' sometimes referred to as a subtype of 'group fairness' , as a synonym , or specified as'strong demographic parity' . Also, though a method may pursue demographic parity over the output, it may only actually be influencing intermediate representations  or even only the data , which may not perfectly align.

In our benchmark, we evaluate a range of well-known definitions of fairness, defined as parity between simple statistics computed over the output . The idea is that practitioners can choose the statistic that fits with the real-world context of the task, and select the method that performs best while achieving the desired level of parity. We leave a comparison to other fairness paradigms like causal fairness  and multicalibration  to future work.

### Distribution of the Output

A final, common challenge to comparability is the distribution of the output, i.e. whether fairness is evaluated over _hard_, binary decisions \(Y\{0,1\}\) or _soft_ scores \(R(0,1)\). Fairness definitions are almost always defined in terms of hard decisions , so standard libraries like Fairlearn  are designed with hard decisions in mind. Hard decisions are then expected to be sampled from a Bernoulli distribution \(R^{Y}(1-R)^{1-Y}\) with the soft score \(R\) as the parameter , or they can be obtained by applying a threshold to \(R\) such as \(Y=_{R 0.5}\).

However, soft scores may be more desireable in some real-world cases, for example if the final decision \(Y\) is deferred to a human decision-maker that can choose to consult \(R\). This is especially likely in applications where decisions have a high impact on someone's life, which are precisely those applications that motivate pursuit of fairness in AI . Whether fairness should be measured over hard or soft outputs therefore depends on how those outputs in the real-world context. To compare fairness methods for any realistic use case, one should thus also compare both hard and soft fairness.

## 3 _ABCFair_

To address the challenges raised in Sec. 2, we introduce a new benchmarking approach: _ABCFair_ (Adaptable Benchmark for Comparing Fairness methods), that helps in both establishing qualitative comparability across methods and performing a quantitative evaluation.

The backbone structure of _ABCFair_ follows a standard PyTorch-idiomatic  paradigm, such that it easily runs on a GPU and enables methods from the _fairret_ and _FFB_ frameworks (discussed below). _ABCFair_ begins by splitting up, preprocessing and loading the data into a standardized Dataset class (subclass of a torch.Dataset). A neural net is then constructed and tuned in a standard train loop. Meanwhile, a FairnessMethod class is initialized that modifies the data, train loop, and/or model output. Finally, the output on a validation or test set is evaluated in an Evaluator class.

### The Dataset Class

Implementation.The Dataset class of _ABCFair_ adapts the format of sensitive features to any of the variants identified in Sec. 2.2, configured as a hyperparameter. Similarly, it can be configured whether the sensitive features \(S\) are included in the input features \(X\) to the model (and not only available separately). In addition to \(X\), \(S\), and the labels \(Y\), the Dataset's __geitem__ provides additional references to sensitive features in all other formats, such that the Evaluator class can be tracked for each format.

Tab. 2 provides details on the datasets available in the current implementation of the repository.

Experiment details.First, we use the _SchoolPerformance_ dataset , which is a dual label: it contains both biased and (less) unbiased labels. Biased labels were gathered by asking humans to predict school outcomes for the "Student Alcohol Consumption" dataset . These labels are assumed to be far more biased than the actual outcomes, even though some bias likely persists. Nevertheless, the human annotations are used for training and the actual outcomes are only used for (far less biased) evaluation.

Second, we run _ABCFair_ on the large-scale, real-world _American Census Survey_ (ACS) datasets from folktables. Five different binary classification tasks are defined for this data.

### The FairnessMethod class

Implementation.The FairnessMethod class is an abstract base class that can implement any preprocessing, inprocessing, and/or postprocessing functionality (see Sec. 2.1). The standard training loop for neural nets already occurs outside the FairnessMethod, so the _naive_ baseline (where no fairness method is applied) is simply a naked inheritance of this class.

Experiment details.Ten fairness methods are currently implemented in _ABCFair_. Tab.3 provides an overview of these methods and their properties with respect to the comparability challenges.

   Dataset name & \# Samples & \# Features & Sensitive attributes \\  SchoolPerformance  & 856 & 20 & Sex, parent’s education \\ ACSEmployment  & 3,236,107 & 34 & Sex, age, marital status, race, disability \\ ACSIncome  & 1,664,500 & 11 & Sex, age, marital status, race \\ ACSMobility  & 620,937 & 63 & Sex, age, race, disability \\ ACSPublicCoverage  & 1,138,289 & 113 & Sex, age, race \\ ACSTravelTime  & 1,466,648 & 1834 & Sex, age, race, disability \\   

Table 2: Overview of the datasets used in the benchmark. The reported number of features is after pre-processing the datasets.

#### 3.2.1 Preprocessing

Implementation.These methods override the FairnessMethod's preprocess function. It takes a Dataset in Sec. 3.1 as input and is expected to return a debiased Dataset as output.

Experiment details.Four pre-processing methods are implemented, which take varying approaches. _Data Repairer_ and _Learning Fair Representations_ both aim to remove the correlation between the features \(X\) and the sensitive features \(S\). Furthermore, _Prevalence Sampling_ mitigates sampling bias by under- or oversampling certain groups, while _Label Flipping_, directly changing the labels.

#### 3.2.2 Inprocessing

Implementation.These methods receive a tuning loop procedure as input in the FairnessMethod's inprocess. The loop implements fit and predict functions as in the standard interface of scikit-learn, and can be fully replaced or overwritten.

Experiment details.Five in-processing methods are present in _ABCFair_. The _Prejudice Remover_ and the _Fairret Norm_ and KLproj methods add an extra penalty to the loss that expresses the unfairness of the model. Similarly, _LAFTR_ penalizes an adversarial network's performance in predicting sensitive features from its intermediate representations, in addition to an extra reconstruction loss. Finally, we wrap the _Exponentiated Gradient_ method  as a modification of the model tuning loop, which'reduces' fair binary classification to a series of naive classification problems with weighted samples.

#### 3.2.3 Postprocessing

Implementation.These methods implement FairnessMethod's postprocess function, which receives both the training Dataset and the tuned model's output as input. They should return a Python callable that performs postprocessed inference on new data samples (such a the test set).

Experiment details._Error Parity_ is the only postprocessing method currently implemented in _ABCFair_. It enjoys strong theoretical guarantees by creating a separate decision thresholds for each sensitive group such that they equalize a given fairness notion (up to a controllable bound).

    &  &  &  &  &  \\  & & Bin. & Cat. & Para. & & & \\  Data Repairer  & Pre- & ✓ & ✓ & ✗ & pr & N/A & aequitas \\ Label Flipping  & Pre- & ✓ & ✓ & ✗ & N/A & N/A & aequitas \\ Prevalence Sampling  & Pre- & ✓ & ✗ & ✗ & pr & Soft & aif360 \\ Learning Fair Repr.  & Pre- & ✓ & ✓ & pr, tpr, fpr, ppv, & Soft & fairret \\  & & & & & for, acc, F1-score & & \\ Fairret KLproj & In- & ✓ & ✓ & ✓ & pr, tpr, fpr, ppv, & Soft & fairret \\  & & & & & for, acc, F1-score & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\   

Table 3: Overview of the methods used in the benchmark and the desiderata they (can) address as identified in Sec. 2: their stage of intervention, sensitive feature format, the fairness notions it can enforce, and the type of output it optimizes. We also include their implementation’s software package.

### The Evaluator class

Implementation.The Evaluator class monitors the predictions during training, validation, and testing. It computes the performance of the model for every prediction type, combination of sensitive attributes, fairness statistic, and performance measure.

Experiment details.Recall from Sec. 2.4 that we distinguish between hard predictions \(Y\{0,1\}\) and soft scores \(R(0,1)\). For soft scores, the Evaluator applies a threshold \(Y=_{R 0.5}\) to also obtain hard scores. Fairness is evaluated on both, but performance is only measured in terms of the accuracy for hard predictions and AUROC for soft scores.

Fairness is measured using the parity-based fairness definition in the fairret library. We refer to  for a full discussion, but summarize the methodology. First, the statistic \((q;h)\) for a relevant fairness notion is computed for every sensitive feature \(q\{1,...,||\}\), over the model scores \(h(X)\). We discuss results on two statistics \(\) here, and report on five more in the Appendix.

First, _demographic parity_ requires equal _positive rates_:

\[(q;h)=S_{q}h(X)}{S_{q} }, \]

where \(S\) is one-hot encoded for categorical features, i.e. \(S_{q}=1\) for samples in group \(q\).

Second, _equalized opportunity_ requires parity in the _true positive rate_:

\[(q;h)=S_{q}h(X)Y}{S_{q} Y} \]

where we use \(R\) instead of \(Y\) if the model outputs soft scores.

The fairness _violation_ is measured as

\[_{q}|(h)}-1|, \]

i.e. each \((q;h)\) is compared to the overall _mean statistic_\((h)\), found by setting \(S_{q} 1\).

This methodology works for all of the sensitive feature formats identified in Sec. 2.2. Fairness for intersections of sensitive attributes can be measures by simply one-hot encoding each intersection. For computing fairness with respect to multiple sensitive attributes in parallel, e.g. gender and ethnicity, we concatenate the encodings of gender and ethnicity. The overall fairness measure is then the maximal violation across both attributes. Hence, the fairness violation for multiple attributes is lower bounded by the violation for a single attribute. Also, the violation in the intersectional format is lower bounded by the violation for the parallel format, as the latter is an aggregate of the former.

## 4 Experiments

We innovate on the common approach to analyzing the accuracy-fairness trade-off in two ways. First, we benchmark on a dual label dataset, such that we can measure unbiased accuracy. Second, we benchmark on a large-scale dataset and configure a wide range of desiderata. We report test set results averaged over 5 random seeds (with different train/test splits) for a range of fairness'strenghts'.

### Performance comparison on bias and unbiased labels

Figure 2 shows the fairness-accuracy relation of several fairness methods. The top row shows the expected trade-off, as its accuracy is computed on test set labels that are i.i.d with the biased labels that the model was trained on. However, evaluating on the _unbiased_ test set labels shown in the bottom row shows an accuracy-fairness _synergy_: an increase in fairness leads to an increase in accuracy. This practical result complements similar theoretical findings  and experiments on synthetic data .

**Key Finding 1: Methods that perform better on the traditional accuracy-fairness trade-off perform worse on unbiased labels.** This empirical result strongly motivates the creation of more dual label datasets, where a fairness method's true impact on accuracy can be properly assessed.

### Full result comparison on a real-world dataset

To validate the _ABCFair_ approach on real-world data where only biased labels are available, we ran our full range of configurations on all five tasks of the ACS data in folktables. Unlike the biased labels in the SchoolPerformance used in Sec. 4.1, there is no label bias in the ACS data. Other biases are still present in the data leading us to call the labels biased. We discuss a subset of the results here on the ACSPublicCoverage task and report all other results in the Appendix.

Though we do not have access to unbiased labels here, we still innovate on simply reporting a trade-off between (biased) accuracy and fairness. Instead, we start from the assumption that real-world applications of fairness will have a bound on the fairness violation in mind. If only biased labels are available for evaluation, then the most desirable method is whichever one that can achieve the best performance desired fairness violation 'level'. Hence, we report our results in Tables 4, 5, and 6 for three arbitrary fairness violation limits \(k\) per configuration of sensitive feature composition (binary, intersectional, and parallel), fairness notion, and output format (as discussed in Sec. 3.3). \(k\) denotes the maximal fairness violation a model may exhibit and still be consider fair.

As expected, the (biased) performance degrades for lower fairness violation bounds \(k\) in all Tables.

**Key Finding 2: Preprocessing methods struggle to ever obtain very low fairness violations.** This is evident in Tab. 4, where the preprocessing methods (the first four columns) struggle to satisfy the lowest fairness violation thresholds \(k\) even for very strong fairness strengths. _Learning Fair Repr._ manages it for non-binary sensitive features, but at a huge loss in performance.

**Key Finding 3: More granular sensitive features compositions lead to consistently larger fairness violations.** This was expected in Sec. 3.3, and we indeed see in Tab. 5, that the fairness violation for the intersectional sensitive features is lower bounded by the fairness violation for the parallel sensitive group configuration. Indeed, for the same violation levels \(k=0.10\) and \(k=0.20\) we see that the performance is higher when constrained on parallel groups compared to intersectional groups.

Figure 2: Fairness-accuracy trade-offs on the SchoolPerformance dataset trained on biased labels. The top row is evaluated on biased labels and the bottom row on unbiased labels. Each marker is the mean test score over 5 random seeds, with a confidence ellipse (Appendix ) for 1 standard deviation.

[MISSING_PAGE_EMPTY:9]

**Key Finding 4: Preprocessing methods optimize for specific a fairness notion more efficiently than inprocessing, but those improve all fairness notions at once by improving on one.** Recall from Tab. 3 that most methods can directly equalize positive rates (pursuing demographic parity), but fewer can do so for _true_ positive rates (pursuing equalized opportunity). This is clear in Tab. 4 and 5, as even fewer strengths can be found for which preprocessing methods (which do not pursue equalized opportunity) satisfy the lower violation levels. Yet, though _LAFTR_ and _Prejudice Remover_ also cannot pursue equal opportunity directly, they do reach efficient trade-offs on this 'unintended' fairness notion as well, suggesting they target bias at a 'deeper' level.

**Key Finding 5: Whether the output consists of hard or soft scores has a significant impact on trade-offs.** Again, we already argued this comparability problem in Sec. 2.4. Here, _Exponentiated Gradient_ and _Error Parity_ are both methods that do not optimize with soft model scores in mind, causing them to incur significant drops in performance in Tab. 4 and 5 for even the highest \(k\) bounds (which correspond with the unfairness of the naive model). The _Fairret_\(_{}\) method incurs no performance loss for all levels of fairness violation, as it _does_ optimize for soft scores. Conversely, _Error Parity_ performs very well when its performance is measured as intended in Tab. 6: on hard, binary predictions.

## 5 Conclusions

We introduced _ABCFair_, a novel benchmarking approach focused on the challenges of comparing fairness methods with different desiderata. After an extensive discussion of these challenges, we provide a configurable pipeline designed to address each of these challenges. We finally provide guidance on benchmarking fairness methods accordingly, covering a wide range of configurations.

Limitations.Due to the breadth of desiderata configurations we cover, we only validate our approach on 10 methods and 6 (tabular) datasets. A more elaborate benchmark is needed with more types of fairness methods and datasets from different domains to corroborate our observations.

To fully populate the tables for all \(k\) values, many fairness strengths need to be tried. A more efficient way of determining these \(k\) values is left to future work.

This work only considers the design choices of method regarding between-group fairness. Recent work has called for also evaluation in-group fairness when benchmarking bias mitigation methods .

Finally, we stress that our view of fairness was highly technical. Though such an approach can have practical value, it is inherently limited in truly addressing fairness as a socio-technical goal .