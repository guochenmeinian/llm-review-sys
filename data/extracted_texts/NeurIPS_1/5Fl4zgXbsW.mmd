# Computerized Adaptive Testing via

Collaborative Ranking

Zirui Liu\({}^{1}\), Yan Zhuang\({}^{1}\), Qi Liu\({}^{1,2}\), Jiatong Li\({}^{1}\), Yuren Zhang\({}^{1}\), Zhenya Huang\({}^{1}\),

**Jinze Wu\({}^{3}\), Shijin Wang\({}^{3}\) \({}^{1}\)**: State Key Laboratory of Cognitive Intelligence,

University of Science and Technology of China

2: Institute of Artificial Intelligence, Hefei Comprehensive National Science Center

3: iFLYTEK Co., Ltd

{liuzirui,zykb,cslijt,yr160698,hxwjz}@mail.ustc.edu.cn

{qiliuql,huangzhy}@ustc.edu.cn, sjwang3@iflytek.com

Corresponding Author.

###### Abstract

As the deep integration of machine learning and intelligent education, Computerized Adaptive Testing (CAT) has received more and more research attention. Compared to traditional paper-and-pencil tests, CAT can deliver both personalized and interactive assessments by automatically adjusting testing questions according to the performance of students during the test process. Therefore, CAT has been recognized as an efficient testing methodology capable of accurately estimating a student's ability with a minimal number of questions, leading to its widespread adoption in mainstream selective exams such as the GMAT and GRE. However, just improving the accuracy of ability estimation is far from satisfactory in the real-world scenarios, since an accurate ranking of students is usually more important (e.g., in high-stakes exams). Considering the shortage of existing CAT solutions in student ranking, this paper emphasizes the importance of aligning test outcomes (student ranks) with the true underlying abilities of students. Along this line, different from the conventional independent testing paradigm among students, we propose a novel collaborative framework, Collaborative Computerized Adaptive Testing (CCAT), that leverages inter-student information to enhance student ranking. By using collaborative students as anchors to assist in ranking test-takers, CCAT can give both theoretical guarantees and experimental validation for ensuring ranking consistency.

## 1 Introduction

With the rapid advancements in computer science, online education has undergone significant transformation, reshaping and displacing traditional offline educational assessment techniques. In this evolving landscape, Computerized Adaptive Testing (CAT)  has emerged as a prominent methodology for standardized testing, widely adopted in selective exams such as the GMAT , GRE , and TOEFL . Diverging from traditional paper-and-pencil tests, CAT offers personalized and interactive assessments, where the difficulty and characteristics of questions are continuously adapted based on real-time responses. By aligning questions with current estimation of students' abilities, CAT refines the estimation process each iterative step . Upon test completion, the final ability score shown in Figure 1(a) is provided as score report to students. This score plays a pivotal role in influencing their educational and career prospects.

However, while massive efforts have been made on optimizing the accuracy of ability estimation via improvements to the question selection algorithms [7; 8; 9; 10; 11; 12], it is crucial to underscore that _accurate ability estimation does not inherently guarantee correct student ranking_. As illustrated in Figure 1(a), minimizing mean squared error (MSE) in ability scores does not always translate into accurate rankings of students. In fact, even state-of-the-art (SOTA) question selection algorithms with superior accuracy performance can exhibit inconsistencies in ranking performance, sometimes performing worse than random selection methods, as presented in Figure 1(b). Meanwhile, the asynchronicity and independency between different students in the CAT test process [13; 14] is a significant technical challenge in achieving accurate ability ranking. This issue prevents the utilization of all students' testing information together for question selection to enhance ranking precision among students, thereby complicates the resolution of the ranking consistency issue in CAT.

To address this challenge, we propose a novel framework--Collaborative Computerized Adaptive Testing (CCAT), which introduces a collaborative learning [15; 16] approach that leverages data from collaborative students as ranking anchors. This framework facilitates interaction among test-takers, allowing for more robust ranking results. Importantly, we also present a theoretical analysis that demonstrates how, with a sufficient number of collaborative students, the ranking consistency error can be significantly reduced to an acceptable level. In summary, our contributions are:

* To the best of our knowledge, this is the first research to unveil the ranking consistency dilemma inherent in CAT, by providing its formal definition and approximation. This discovery has enabled us to significantly refine the objectives of CAT, which is a vital advancement for its deployment in high-stakes examination contexts.
* We introduce a novel, collaboration-based methodology that enhances both question selection and ability estimation to minimize ranking inconsistency, providing theoretical guarantees for ranking consistency even with a limited number of questions.
* Our methodology is general enough to integrate with existing question selection algorithms. Empirical results on extensive real-world educational datasets proves the effectiveness of CCAT, manifesting in an average 5% rise in ranking consistency compared with other methods, and this improvement is more significant in the short test scenarios.

## 2 Related Work

CAT is designed to efficiently and accurately estimate students' abilities. It is widely employed in various competitive exams, including the GRE. CAT essentially operates in two stages: first, it uses methods such as Item Response Theory (IRT)  to estimate students' abilities. Subsequently, it uses these estimations to select the next question for each student. The following paragraphs separately outline Item Response Theory and common question selection algorithms used in CAT.

Figure 1: (a) The score report provided by GRE and an example to show that a low MSE cannot guarantee the correct ranking of studentsâ€™ testing results. (b) This line chart shows the performance of previous CAT methods in ranking, and it can be seen that the method that performs state-of-the-art (BECAT) in accuracy may only achieve the effect of random selection in ranking.

Item Response Theory (IRT).IRT is a psychological measurement theory predominantly employed in education to estimate students' abilities [17; 18; 19]. It posits that examinees' abilities remain constant throughout a test, and their performance depends solely on their ability and the information provided by the questions. The standard model is the two-parameter logistic (2PL) model: \(P_{j}()=P(y_{j}=1)=(a_{j}(-b_{j}))\), where \((x)=}\) is sigmoid function and \(y_{j}=1\) indicates a correct response to question \(j\). The parameters \(a_{j},b_{j}\) represent the discrimination and difficulty of question \(j\). These parameters are estimated by algorithms such as Markov Chain Monte Carlo (MCMC) [20; 21] and Gradient Descent (GD) [22; 23] before testing. \(\) represents the student's ability, which is estimated using the maximum likelihood method at each step \(t\):

\[^{t}=*{arg\,max}_{}_{j=1}^{t}y_{j} P_ {j}()+(1-y_{j})())}. \]

In recent years, the increasing studies [24; 25; 26; 27; 28] leveraging the rapid advancements in deep learning technologies (e.g., the neural networks) have significantly enhanced the accuracy of student ability estimation. For example, NeuralCD  leverages a non-negative fully connected neural network to capture the complex student-question interactions to achieve a more accurate estimation.

Selection Algorithms.Research on selection algorithms can be categorized into two main approaches: traditional rule-based algorithms and data-driven algorithms. Firstly, traditional question selection algorithms[29; 30; 31] view CAT as a parameter estimation problem. They calculate the information value of each question based on the student's current proficiency and select the question with the maximum information value, typically using metrics such as Fisher Information (FSI)  and Kullback-Leibler Information (KLI) . Subsequently, in order to optimize the accuracy of the test result directly, researchers have proposed methods such as MAAT , BOBCAT  and NCAT , which are based on active learning , meta-learning  and reinforcement learning . Recently, BECAT  proposes to use the ability estimated by student's full responses on the entire question bank as the true value and solve the CAT problem using a data efficiency method .

In fact, in many exams, especially selective exams, the ranking of grades is usually one of the most important bases for employment. So we argue that the requirement of students in CAT is not necessarily a more precise estimation of their abilities on the test set. Rather, CAT should ensure that students with stronger abilities receive better rankings. Consequently, we establish the ranking consistency of CAT as our primary objective.

## 3 Ranking Consistency of CAT

We first assume that the testing step in CAT is uniformly \(T\) steps and all the selected questions come from question bank \(Q\). The questions answered by each student constitute a subset \(S Q\). For each step \(t\), the student's ability estimated by IRT is \(^{t}\) and the student's final result is \(^{T}\) when the test stops. For traditional CAT methods, the goal is that test results \(^{T}\) should be as close as possible to the true abilities of students \(^{*}\) with fewer questions [40; 42]:

\[_{|S|=T}||^{T}-^{*}||, \]

where \(^{*}\) is approximated by the abilities of students estimated by their full responses to the entire question bank \(Q\). However, as previously mentioned, CAT often prioritize the issue of ranking among students over merely improving the accuracy of \(^{T}\). For instance, if students learn that a peer with lower true ability outperforms them in CAT, they may question the fairness of the exam . Thus, we define the consistency of CAT ranking as follows:

Definition 1.(Ranking Consistency of CAT) In computerized adaptive testing, the true abilities of two students are represented by \(^{*}_{1}\) and \(^{*}_{2}\). The testing results of these two students on subsets \(S_{1}\) and \(S_{2}\) of question bank \(Q\) are denoted by \(^{T}_{1}\) and \(^{T}_{2}\). The ranking consistency of testing demands that students with higher true abilities should also exhibit higher testing results:

\[_{|S_{1}|=|S_{2}|=T}P(^{T}_{1}>^{T}_{2}|^{*}_{1}>^ {*}_{2}). \]Given the varied performance, queries, and progress of the students undergoing testing, they remain independent during the CAT process. Consequently, it is impractical to intervene in ranking consistency by selecting questions based on each others' performance in the test. This complicates the direct optimization of this problem.

## 4 The CCAT Framework

To address the problem of ranking consistency, in this section, we first introduce the concept of collaborative students as anchors for the tested students. Then we elucidate their application in question selection and ability estimation. Finally, we conducted a theoretical analysis of the collaborative student method, demonstrating that while the ranking of the tested students among collaborative students may not be entirely accurate, the likelihood of achieving ranking consistency in CAT can reach at least \(1-\) when a sufficient number of collaborative students are available.

Definition 2.(Collaborative Students) Collaborative students represent a group with \(M\) students who are utilized as anchors to assist in ranking test-takers [44; 45]. It can be assumed that collaborative students have already completed answering all questions in the question bank \(Q\), and their abilities on question bank \(Q\) or subset \(S(|S|=T)\) are \(_{c}^{*}\) and \(_{c}^{T}\), which can be obtained easily.

Due to the absence of information disclosure between any two students during the testing process, we cannot directly intervene in their ranking relationship. Nonetheless, since the collaborative students answered every question from the question bank, we can hypothesize that each collaborative student will accompany the tested students in responding to the same questions during the test. This could facilitate the establishment of relationships among the tested students.

Specifically, when two students, \(A\) and \(B\), answer distinct sets of questions, say \(q_{1},q_{2},q_{3}\) for student \(A\) and \(q_{4},q_{5},q_{6}\) for student \(B\), inconsistencies may arise due to the dissimilarity of the questions. However, each collaborative student can compare their performance with both students \(A\) and \(B\). For instance, a collaborative student can assess her performance on questions \(q_{1},q_{2},q_{3}\) alongside student \(A\) and on questions \(q_{4},q_{5},q_{6}\) alongside student \(B\). If the collaborative student finds that her abilities exceed those of student \(A\) but fall short of student \(B\), she will provide valuable information for accurately ranking students \(A\) and \(B\).

### Problem Approximation

As previously mentioned, our goal is to establish the ranking relationship between tested students by comparing with collaborative students. Obviously, the first step in ensuring the ranking consistency among tested students is to establish ranking consistency between the collaborative students and the tested students:

\[_{|S|=T}P(^{T}>_{c}^{T}|^{*}>_{c}^{*},S). \]

Figure 2: The structure of CCAT framework. CCAT consists of two parts: question selection and ability estimation. The question selection part utilizes the performance of collaborative students in answering various questions to select appropriate questions for the tested student, and the ability estimation part ranks the tested student with collaborative students and uses the ranking as the test result.

In Section 2, we outlined the estimation method for \(\) in Item Response Theory, as presented in Equation (1). Utilizing this formula, we can derive the subsequent lemma, which aids in simplifying the optimization objective.

**Lemma 1**.: _Given two students, whose responses on \(S(|S|=T)\) are \(y_{1},y_{2},,y_{T}\) and \(_{1},_{2},,_{T}\), their testing abilities on \(S\) are \(^{T},^{T}\), which are estimated by IRT with parameters \(a_{i},b_{i}\). We can prove that if \((^{T}-^{T})>0\), then \(_{i=1}^{T}a_{i}(y_{i}-_{i})>0\), vice versa._

Lemma 1 posits that if two students are tested on the same question subset, the term \(_{i=1}^{T}a_{i}(y_{i}-y_{i}^{c})\) can be used to replace \(^{T}-_{c}^{T}\) because they share the same sign (either positive or negative). This substitution leads to a more streamlined formulation of the objective:

\[ P(^{T}>_{c}^{T}|^{*}>_{c}^{* },S)&=P(_{q_{i} S}a_{i}(y_{i}-y_{i}^{c})>0|S,^{* }>_{c}^{*})\\ &_{q_{i} S}a_{i}P(y_{i}>y_{i}^{c}|^{*}> _{c}^{*})\\ &=_{q_{i} S}R(q_{i}|^{*}>_{c}^{*}), \]

where \(R(q_{i}|^{*}>_{c}^{*})=a_{i}P(y_{i}=1|^{*})P(y_{i}^{c}=0| ^{*}>_{c}^{*})\), \(y_{j}^{c}\) and \(y_{j}\) represent the responses of collaborative students and tested students to question \(j\) respectively. _The above derivation assumes that all questions in the question bank \(Q\) are independent, and students with high abilities should perform well on each question._ This formula indicates that for each tested student, answering questions that students with weaker abilities cannot answer correctly enhances ranking consistency.

Considering the asymmetry between collaborative students and tested students, we also need to consider the situation where collaborative students have stronger abilities than tested students:

\[P(^{T}<_{c}^{T}|^{*}<_{c}^{*},S)_{q_{i}  S}R(q_{i}|^{*}<_{c}^{*}), \]

where \(R(q_{i}|^{*}<_{c}^{*})=a_{i}P(y_{i}=0|^{*})P(y_{i}^{c}=1| ^{*}<_{c}^{*})\). Similar to equation (5), our objective is to shield students from being assessed on questions that students with higher abilities may struggle to answer accurately. By utilizing the constraints from formulas (5) and (6), we can select specific questions for the tested students based on their collaborative students:

\[q_{t}=_{q Q S_{t-1}}P(^{*}<_{c}^{*})R(q| ^{*}<_{c}^{*})+P(^{*}>_{c}^{*})R(q|^{*}>_{c}^{*}). \]

Here \(S_{t-1}\) represents the subset of questions selected up to step \(t\), with \(S_{t}=S_{t-1}\{q_{t}\}\) where \(q_{t}\) is the question selected at step \(t\). This formula aims to find questions that collaborative students with higher abilities are likely to answer correctly, while tested students may struggle with. Meanwhile, it also identifies questions that collaborative students with lower abilities are unlikely to answer correctly, while tested students may respond correctly. The selection method enhances the performance of the originally strong students while diminishing that of weaker ones, aiding tested students in determining their ranking among collaborative students.

After testing, the tested students received their performance on \(S\), as well as their ranking relationship with each collaborative student. In the study, we used the mean ranking relationship among collaborative students as the test results for the tested students:

\[^{T}=[I(^{T}>_{c}^{T})] =[I(_{i S}a_{i}(y_{i}-y_{i}^{c})>0)], \]

where \(I()\) is the indicator function. Due to the uncertainty of the tested students' abilities and the incomplete responses from collaborative students during the testing process, we further approximate and elucidate the optimization problem in appendix section C.

```
0:\(Q\)-question bank,\(IRT\)-estimation method. Initialize: Random initialize tested student's ability \(^{0}\), initialize the question subset \(S_{t}\), the tested student's record \(Y\) and collaborative students' records \(}\).
1for\(t=1\)to\(T\)do
2 Select question: \(q_{t}_{q Q S_{t-1}}P(^{*}<^{*}_{c})R(q| ^{*}<^{*}_{c})+P(^{*}>^{*}_{c})R(q|^{*}>^{*} _{c})\), \(S_{t} S_{t-1}\{q_{t}\}\).
3 Get tested student's and collaborative students' answer: \(Y Y\{y_{t}\},}}\{\{y_{1t}^{c}, ,y_{Mt}^{c}\}\}\).
4 Update students' estimate ability \(\):\(^{t}=arg_{}- p_{}(q_{t},y_{t})\).
5 Calculate tested student's rank in collaborative students: \(^{T}_{i=1}^{M}(_{t=1}^{T}a_{i}( y_{it}^{c}-y_{t}))\). Output: The student's final estimate ranking ability \(^{T}\).
```

**Algorithm 1**The CCAT framework

### Theoretical Analyses of CCAT

Through the above derivation and approximation, we provide the selection algorithm and estimation method for CCAT, which can ensure high degree of consistency in ranking between collaborative and tested students. This ranking is then used to provide the test results for the tested students, denoted as \(^{T}\). Regarding the test result \(^{T}\) in ability estimation, we have the following conclusion:

**Theorem 1**.: _Given two students \(A\) and \(B\), their relationship with collaborative students are \(r_{1},r_{2},,r_{M};_{1},_{2},,_{M}\), \(r_{i}\{0,1\}\) indicating whether student \(A\) outperforms collaborative student \(i\) in a given test. Assuming the probability that student \(A\) outperforms the collaborative students \(i\) is \(P(r_{i}=1)=_{1}\) and student \(B\) outperforms the collaborative students \(i\) is \(P(_{i}=1)=_{2}\). Then the following conclusion can be drawn:_

_(1) If_ \(M>}{2(_{1}-_{2})^{2}}\) _collaborative students are provided, the prediction of ranking consistency will be at least_ \(1-\)_._

_(2) When the number of test questions_ \(T\) _is small, the assessment of the ranking relationship between the tested students and collaborative students may yield inaccurate results. Assuming an error probability of_ \((0,0.5)\)_, we can still derive that if_ \(M>}{2(1-2)^{2}(_{1}-_{2})^{2}}\) _collaborative students are provided, the prediction of ranking consistency will be at least_ \(1-\)_._

Drawing from Theorem 1, we can deduce that having a sufficient number of collaborative students ensures a consistent ranking of abilities among all tested students, even in the presence of rank errors between the tested and collaborative students. Meanwhile, Our question selection algorithm actually reduces the ranking error \(\) by maximizing the ranking consistency between collaborative and tested students, thereby theoretically increasing the ranking consistency.

Algorithm 1 outlines the pseudo-code for the CCAT framework. During the question selection phase, the complexity of our proposed question selection algorithm is \(O(|Q|TMN)\), as it involves selecting the most appropriate question from the question bank \(Q\) with a complexity of \(O(|Q|M)\) for each tested student. Here, \(T\) denotes the total number of questions in the test, \(M\) is the number of collaborative students, and \(N\) is the number of students being tested. It can be observed that the time complexity of CCAT is comparable to the inference speed of data-driven CAT methods. However, CCAT circumvents the time-consuming training process by storing collaborative students. Although this does increase spatial complexity, it significantly reduces the time required for training and eliminates the need for repeated training of models due to system changes.

## 5 Experiments

In this section, to demonstrate the effectiveness of CCAT on ranking consistency, we compare the performance of CCAT on the ranking consistency metric with other baseline methods on two real-world datasets. In addition, we conduct a case study to compare IRT and collaborative ability estimation and gain deeper insights on how collaborative ability estimation leads to ranking consistency.

### Experimental Setup

Evaluation Method.The goal is to ensure consistency in the ranking of the test results of tested students on the subsets \(S\) and their abilities on all questions in question bank \(Q\). In this study, we use the Kendall coefficient  between the abilities of tested students on the subsets \(S\) and on question bank \(Q\), which we call **intra-class ranking consistency**:

\[ K=_{1 i<j N}U_{ij},\\ U_{ij}=\{1&(_{i}^{*}-_{j}^{* })(_{i}^{T}-_{j}^{T}) 0\\ 0&(_{i}^{*}-_{j}^{*})(_{i}^{T}-_{j}^{T})<0 .. \]

For any two students, if the ranking of their test results aligns with their true abilities, the metric record is 1. Conversely, if the ranking of test results diverges from their true abilities, the metric record is 0.

Similarly, we can also examine the ranking consistency between the tested students and collaborative students, which we refer to as **inter-class ranking consistency**:

\[K=_{i=1}^{N}_{j=1}^{M}U_{ij}, \]

where \(M\) and \(N\) are the number of collaborative students and tested students. In addition, we also discuss **AUC, ACC** indicators in the main text, and **NDCG** indicator is used as a reference indicator in appendix section D.2.

Dataset.We individually conduct experiments on two educational benchmark datasets, NIPS-EDU and JUNYI. NIPS-EDU  is a dataset compiled from student question interactions collected from Eedi and used in the NeurIPS 2020 Educational Challenge. JUNYI  is sourced from juniyacademy.org, providing millions of response data from students enrolled in a course between 2018 and 2019. The rationale for selecting these two datasets is their extensive student population and the high volume of questions answered by each student, thus facilitating the construction of the collaborative student set. We filter out students who answer less than 50 times and questions that are answered less than 50 times in the following experiment and then divide the dataset into a training dataset (Collaborative Students) and a testing dataset (Tested Students) in a 4:1 ratio. The code can be found in the github: [https://github.com/bigdata-ustc/CCAT](https://github.com/bigdata-ustc/CCAT).

Compared Approaches.This article primarily focuses on the discussion of ranking consistency in testing, and therefore, we employ IRT, which can provide practical significance results \(\). As we know, Monte Carlo sampling (MCMC) and gradient descent (GD) methods can estimate the IRT parameter \(a_{i},b_{i}\). In this experiment, we respectively employ the IRT model, estimated by both the GD and MCMC methods, and conduct question selection and student estimation. In terms of the question selection algorithm, we select the following SOTA algorithms as the baseline: **Random** Randomly select a question for students each step, which is a benchmark to evaluate the improvement of other selection algorithms. **FSI** and **KLI** select the question with the highest Fisher/KL information, which measures how much information of students' abilities \(\) can be obtained by answering a question. **MAAT** utilizes active learning methods to measure the information each question brings to testing. **BECAT** regards CAT question selection as a coreset selection problem and provides an approximate solution strategy. **BOBCAT** proposed a Bilevel Optimization-based framework to synchronously optimize the question selection algorithm and estimation model. **NCAT** respectively utilizes the ideas of reinforcement learning, and uses data-driven methods to directly optimize the accuracy of CAT test results.

### Results and Discussion

To prove the superiority of CCAT framework, we respectively compare various CAT question selection algorithms on IRT estimated by GD and MCMC methods. The following conclusions are obtained:

Intra-class Ranking Consistency Performance.Table 1 indicates that Method X consistently enhances ranking consistency at every step after employing collaborative ability estimation (X-C).

This finding aligns with Theorem 1, which subst antiates the effectiveness of collaborative ability estimation in CCAT. Furthermore, when comparing Method X-C, whether employing MCMC or GD methods for estimating IRT model parameters, our CCAT algorithm demonstrates superior performance in ranking consistency across two public datasets. Particularly, CCAT shows more significant improvement when fewer questions are tested, outperforming other methods. As the number of test steps increases, the FSI-C method improves ranking consistency more rapidly, ultimately achieving a high level of consistency. This is attributed to the FSI method's ability to select questions with higher discrimination and uncertain responses, enabling the FSI-C method to promptly adjust students with inconsistent ranking. However, due to the FSI method's sensitivity to current abilities, it performs inadequately when fewer questions are tested. These results confirm that the CCAT framework is generally effective in ranking for CAT, whether in terms of test duration or estimation model.

Inter-class Ranking Consistency Performance.After each baseline selection algorithm is completed, we replace the original results obtained by directly using IRT for parameter estimation with the ranking results obtained from collaborative ability estimation. From Tables 1 and 2, it can be seen that there is a positive correlation between the ranking consistency of the tested students among the collaborative students (Table 2) and the ranking consistency among the tested students (Table

Table 1: The Performance of Different Question Selection Algorithms on Intra-class Ranking Consistency. Algorithm **X-C** means use algorithm **X** for question selection but use collaborative ability estimation proposed in CCAT as the testing result instead of the abilities estimated by IRT. CCAT (w/o C) means using the question selection algorithm but estimating the ability by IRT. The bold font represents a significant improvement in statistics compared to the baseline.

1) when using the collaborative ability estimation method, especially, CCAT method is in a leading position in both two tables, and FSI method is only second to CCAT. This also explains why we optimize the ranking consistency among the collaborative students in the above section.

Acc&AUC.Figure 3 displays the metrics (ACC, AUC) obtained through various question selection algorithms on IRT, as estimated by different methods. It is evident that CCAT, when compared to other CAT question selection algorithms, does not significantly differ in terms of ACC and AUC indicators. This suggests that CCAT maintains the accuracy of CAT test results while enhancing ranking consistency. Furthermore, IRT estimated by MCMC significantly outperforms that estimated by GD and BOBCAT. This also explains why the same question selection algorithm in Table 1 performs better on the IRT model obtained through MCMC. Additionally, question selection algorithms proposed on GD, particularly those such as NCAT that utilize data-driven methods, are not efficient for IRT estimated by MCMC. This implies that these methods may not be effective, but can compensate for the drawbacks of using GD to estimate IRT. However, methods like BOBCAT, which concurrently train the IRT model alongside a question selection algorithm, may introduce bias into the IRT model. As depicted in Figure 3, while it outperforms all gradient descent methods in specific optimization objectives (ACC@20), it may impact accuracy at other times and compromise the stability of the IRT model in ability estimation. This can result in suboptimal performance in ranking problems. Given the analysis above and the stability of the MCMC method, we assert that it

   Dataset &  &  \\  Step & 5 & 10 & 15 & 20 & 5 & 10 & 15 & 20 \\  Random & 0.7798 & 0.8325 & 0.8590 & 0.8760 & 0.7651 & 0.8298 & 0.8648 & 0.8865 \\ FSI & 0.8258 & 0.8785 & 0.9013 & **0.9126** & 0.8575 & 0.9050 & 0.9249 & 0.9363 \\ KLI & 0.8195 & 0.8758 & 0.8985 & 0.9119 & 0.8502 & 0.9028 & 0.9240 & 0.9353 \\ MAAT & 0.7242 & 0.8373 & 0.8807 & 0.9023 & 0.7830 & 0.8767 & 0.9069 & 0.9249 \\ NCAT & 0.8286 & 0.8697 & 0.8892 & 0.8994 & 0.8090 & 0.8604 & 0.8830 & 0.8972 \\ BECAT & 0.8045 & 0.8676 & 0.8948 & 0.9104 & 0.8287 & 0.8961 & 0.9204 & 0.9341 \\ CCAT & **0.8476** & **0.8839** & **0.9013** & 0.9116 & **0.8736** & **0.9082** & **0.9255** & **0.9373** \\   

Table 2: Inter-class Ranking Consistency Performance on IRT estimated by MCMC, which measures the accuracy of the collaborative ability estimation.

Figure 3: The performance on ACC and AUC of different question selection algorithms on the dataset NIPS-EDU for the IRT model estimated by MCMC and GD methods.

is more appropriate for IRT parameter estimation than the GD method, particularly when considering the ranking consistency of CAT.

Case Study.To demonstrate the superiority of CCAT and its mechanism, we select 10 student pairs from each dataset and conduct two visualization experiments as shown in Figures 4. This figure compares the ability gap between student pairs as estimated by the IRT and CCAT methods. Specifically, for each student pair, we subtract the estimated ability of the student with higher true ability from that of the student with lower true ability at each moment. A larger gap indicates better discrimination by the estimation method. When the value is less than 0 (red), it signals a ranking inconsistency at that point in time. Our findings show that, although the selection algorithm remains unchanged, CCAT produces greater discrimination and more accurate rankings, particularly when fewer testing steps are involved.

We also analyze the estimation results for collaborative students on these 20 student pairs, revealing that the collaborative ability estimation method essentially functions as a voting process by collaborative students for the tested students. Additionally, we visualize how each collaborative student's judgment of the two students becomes progressively clearer as the number of test questions increases. For further details, please refer to Appendix D.2.

## 6 Conclusion

This article explored the objectives of Computerized Adaptive Testing (CAT) from the perspective of students, reframing CAT challenges as ranking tasks and proposing specific objectives for these tasks. To address the challenge of students working independently, which limits influence on rankings during the testing process, we introduced a Collaborative Computerized Adaptive Testing (CCAT) framework. This approach leverages collaborative student interactions to assist in question selection and estimation during testing. Experiments on two real-world datasets demonstrated that CCAT improves ranking consistency. Despite these promising results, our method has inherent limitations, particularly with longer testing sequences. In future work, we aim to refine our model to address these limitations and enhance the robustness and effectiveness of the CCAT framework across diverse testing scenarios.