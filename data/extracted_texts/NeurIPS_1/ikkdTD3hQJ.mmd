# AIMS: All-Inclusive Multi-Level

Segmentation for Anything

Lu Qi\({}^{1}\)  Jason Kuen\({}^{2}\)  Weidong Guo\({}^{3}\)1  Jiuxiang Gu\({}^{2}\)

Zhe Lin\({}^{2}\)  Bo Du\({}^{4}\)  Yu Xu\({}^{3}\)  Ming-Hsuan Yang\({}^{1,5}\)

\({}^{1}\)UC Merced \({}^{2}\)Adobe Research \({}^{3}\)QQ Brower Lab, Tencent

\({}^{4}\) Wuhan University \({}^{5}\) Google Research

###### Abstract

Despite the progress of image segmentation for accurate visual entity segmentation, completing the diverse requirements of image editing applications for different-level region-of-interest selections remains unsolved. In this paper, we propose a new task, All-Inclusive Multi-Level Segmentation (AIMS), which segments visual regions into three levels: part, entity, and relation (two entities with some semantic relationships). We also build a unified AIMS model through multi-dataset multi-task training to address the two major challenges of annotation inconsistency and task correlation. Specifically, we propose task complementarity, association, and prompt mask encoder for three-level predictions. Extensive experiments demonstrate the effectiveness and generalization capacity of our method compared to other state-of-the-art methods on a single dataset or the concurrent work on segment anything. We will make our code and training model publicly available.

## 1 Introduction

Decomposing an image into semantically meaningful regions is a key goal of image segmentation. With the recent emergence of photorealistic and high-quality image generation technologies , image segmentation has gained considerable attention for its ability to select visual entities of interest within images. Combining image segmentation with image generation has enabled a variety of image editing and manipulation applications, such as altering the style of a specific object or eliminating a distracting element from a scene. Compared to bounding box-based object detection, segmentation methods preserve the integrity of pixels outside the edited or manipulated region.

Panoptic segmentation , with its instance awareness and dense segmentation capabilities, is a promising solution for various editing and manipulation applications. However, two main limitations remain when using it: (1) it struggles to handle classes encountered 'in the wild' but absent in the training set ; and (2) existing methods typically do not explore the more general possibilities of segmentation masks across multiple abstraction levels (_e.g., a car's wheel_, _the whole car_, both _a person and car_ combined), with the exception of part-aware panoptic segmentation  and panoptic scene graph . These tasks also fail to generalize to classes beyond those in the training set, especially at the part-level task where annotations are sparse.

In this paper, we address the constraints inherent to different-level segmentation tasks by introducing **AIMS** (All-Inclusive Multi-Level Segmentation). AIMS represents a comprehensive framework designed to segment images across various hierarchical levels (_part_, _entity_, _relation_) using multiple query-driven image decoders. Drawing inspiration from recent advancements in Entity Segmentation , which showcased remarkable generalization capabilities on unseen classes, AIMS performs class-agnostic segmentation across different hierarchical levels to enhance generalization.

These results are shown in Figure 1. Thanks to its hierarchical modeling and class-agnostic learning approach, AIMS provides more level-flexible and all-inclusive segmentation masks for users to select from, thereby enabling a wider range of image editing and manipulation applications. In Figure 1, the AIMS task has two modes of inference: one-step inference and prompt inference. In one-step inference, the model segments the image directly into three levels (part, entity, and relation level). Meanwhile, in prompt inference, the model separates the image further based on the provided mask. Notably, one-step inference necessitates an association of the results between any two neighboring levels.

Unlike existing part-aware panoptic or panoptic scene graph methods, AIMS model is not confined to training on a single dataset that has limited hierarchical levels. Instead, we adopt the strategy of multi-dataset training to build a unified AIMS model for all levels. To maintain consistency across multiple hierarchical levels sourced from different datasets, we introduce the _Task Complementarity Module_ and the _Association Module_ to AIMS model. These respectively facilitate feature interactions between different levels and establish explicit relationships between predictions at adjacent levels (_e.g.,_ a car's part mask should correspond with the overall car mask, and a car mask should belong to a mask with a car and driver.).

We train our AIMS model on existing segmentation datasets such as Pascal Panoptic Parts , COCO-PSG , PACO , and EntitySeg . However, due to partial or incomplete annotations over the full image, with inconsistencies in unannotated areas across different datasets, sub-optimal supervision signals are generated during training. To address this issue, we propose a novel _Mask Prompt Encoder_ for our AIMS model. This encoder segments an image based on a given mask prompt, which specifies the region slated for segmentation. We train AIMS to accommodate a diverse set of mask prompts, derived from various hierarchical levels and datasets while excluding unannotated regions. This encoder offers two distinct benefits: _(1)_ it mitigates the inconsistency problem inherent in training with partial annotations from multiple datasets; _(2)_ it exhibits strong generalizability to arbitrary mask prompts during inference, including those stemming from unseen classes and in-the-wild visual entities, as shown in Figure 5.

The main contributions of this work are as follows:

* We introduce the AIMS task, which segments images into three distinct levels: part, entity, and relation. AIMS is capable of performing a one-step inference for the entire image and a prompt inference for a specified mask area within the image, which has great potential for image editing and manipulation applications.
* We propose a unified AIMS model for this task, which incorporates multi-dataset and multi-task training. We have meticulously designed several modules, including the task complementary, association, and mask prompt encoder, to maintain the consistency of tasks or data annotations.
* Extensive experiments and in-depth analysis showcase the effectiveness of our proposed AIMS model across various segmentation tasks and settings. We compare the results from our AIMS model with the popular segmentation models (_e.g._, SAM), demonstrating the fine-grained segmentation capabilities and generalization to both seen and unseen categories.

Figure 1: The overview of our All-Inclusive Multi-Level Segmentation (AIMS) task.

## 2 Related Work

Image SegmentationSignificant advancements [11; 12; 13; 14; 15; 16; 17; 18; 19; 20; 21] have been made in image segmentation [22; 23; 3; 24; 25; 26] in recent years, with numerous methods and techniques emerging to handle various challenges through supervised learning. Notable works include FCN , DeepLab , PSPNet , Mask R-CNN , PANet , SOLO , CondInst , PanopticFPN , PanopticFCN , Max-Deeplab , and Mask2Former  for semantic, instance, and panoptic segmentation. Additionally, specialized tasks tailored to specific scenarios, such as part-aware panoptic segmentation [6; 27; 7; 28] and panoptic scene graphs [8; 29], have been developed. However, these tasks typically focus on single target and do not incorporate hierarchical concepts that are more in line with human behavior. This paper aims to address this problem by proposing a unified segmentation model that can organize pixels at three different levels: part, entity, and relation. We also introduce an interactive stage to decompose unseen regions into smaller entities or parts in an open-world setting.

Multi-Dataset TrainingMulti-dataset training has gained considerable attention in recent years, as it allows models to draw from multiple data sources to improve performance and generalization [30; 31; 32]. This approach has significantly benefited domain adaptation [33; 34; 35], dataset distillation [36; 37], curriculum learning [38; 39; 40; 41], data augmentation [42; 43; 44], federated learning [45; 46], and transfer learning [47; 48] in the context of image segmentation. This paper addresses the annotation inconsistency among multiple datasets for generalized hierarchical segmentation. Specifically, we tackle the issue of incomplete annotations in a single dataset across three levels using a prompt structure.

## 3 Method

As shown in Figure 1, given an image \(^{H W 3}\), the AIMS task segments an image into three segmentation maps \(_{T}\{0,1\}^{N_{T} H W}\) at part, entity, and relation levels and two association maps \(}\{0,1\}^{N_{e} N_{p}}\) (entity and part levels) and \(}\{0,1\}^{N_{r} N_{e}}\) (relation and entity levels), where \(H\) and \(W\) are the height and width of the image, \(N_{T}\) indicates \(N_{T}\) prediction results in class-agnostic binary format and \(T=\{,,\}\). The two association maps \(}\) and \(}\) are required to build the association relationships between two level prediction results. We chose these three levels since they align with the human brain's hierarchical information processing system [49; 50; 51] and fulfill the majority of requirements for image editing tools . The outputs of AIMS are akin to those of related tasks like part-aware panoptic segmentation and panoptic scene graphs, but the AIMS task incorporates more levels and operates in a class-agnostic manner.

Like the existing Transformer-based segmentation methods , our AIMS model employs an image encoder and decoder to extract robust image features and decode them into segmentation masks. However, it differs in that it addresses the challenges of multi-dataset multi-task training. As illustrated in Figure 3, we first establish a task complementarity module and an association module to build the correlation of each task, followed by a mask prompt encoder to tackle the partial annotation problem among different datasets and enable generalized inference.

Next, we introduce our base framework and then elaborate on our three proposed components.

Figure 2: The outline of our AIMS framework encompasses an image encoder, a mask prompt encoder, and an image decoder. The outputs generated by AIMS consist of five distinct components, including segmentation results across three levels, and two association metrics.

### Base Framework

The base framework of AIMS shares the same image encoder \(_{I}\) and has three independent decoders for different-level predictions. The image encoder consists of a backbone and pixel decoder, extracting multi-scale image features \(}^{} 256}\) and \(s=\{2,3,4,5\}\). We denote this image-encoding process as:

\[}=_{I}() \]

For the three independent decoders \(\), each decoder has 9 cascaded attention blocks, where each block includes a masked cross-attention, self-attention, and feedforward network. Each block of decoder iteratively refines query embeddings \(}^{N 256}\) by interacting with multi-scale features \(F\), where \(}\) is initialized by a group of learnable parameters \(}\). This iterative decoding process is denoted as

\[}=_{}(},})=_{i=1}^{ 9}_{}^{i}(^{i}},})=_{i=1}^{9}R_{ _{}}^{i}(R_{_{}}^{i}(R_{_{}}^{i}(^{i}},}))) \]

where \(_{}\) and \(_{}^{i}\) indicate the decoder of \(\) type and its attention blocks. \(i\) indicates the \(i^{}\) attention block. For each attention block, it includes cross-attention \(R_{_{}}^{i}\), self-attention \(R_{_{}}^{i}\), and feed-forward network \(R_{_{}}^{i}\). At last, \(}\) is used for part-ness/entity-ness/relation-ness prediction and pixel-level mask prediction using the low-level image feature \(}\) (derived from the image encoder). The prediction process can be formulated as:

\[^{e}},^{m}}=_{}(},}) \]

where \(^{e}}\) and \(^{m}}\) denote the part-ness/entity-ness/relation-ness prediction and pixel-level mask outputs.

During training, we employ the losses \(}\) for different level predictions, with respect to the corresponding ground truth \(}\). The overall training loss for our model is defined as:

\[}}=_{\{,,\}}^{e}}(^{e}},^{e}})+^{ }}(^{m}},^{m}})+^{}}(^{m}},^{m}}) \]

where \(^{e}}\) denote binary cross-entropy loss for part-/entity-/relation-ness prediction. Similarly, \(^{}}\) and \(^{}}\) denote the binary cross-entropy and dice loss for different level mask prediction.

Figure 3: Overview of image decoder of our All-Inclusive Multi-Level Segmentation framework. The terms _‘E-P Correlation Module’_ and _‘E-P Association Module’_ refer to the task correlation and association structure between the entity and part level, respectively. This explanation can also be applied to the _‘R-E Correlation Module’_ and _‘R-E Association Module’_. The rightward arrows denote the outputs from the respective modules. _‘CONCAT’_ or _‘SPLIT’_ refers to the joining of two tensors or the division of a single tensor into two tensors, respectively, occurring along the dimension of the query number.

### Task Complementarity Module

In our base method, we utilize three independent decoders for predictions at different hierarchy levels, which merely share the image encoder. To better exploit cross-task knowledge, we design the task complementarity module (TCM) that fuses task information within each attention block of the decoders. To provide a clearer explanation, we use the task complementarity module between the entity and part tasks as an example, and it works identically for the relation and entity tasks. As depicted in Figure 3, we concatenate the embeddings of two tasks following the self-attention of the attention block between two level decoders. Subsequently, we feed the concatenated embedding into a self-attention \(G^{i}_{}\) and feed-forward network \(G^{i}_{}\) and then separate processed embeddings back into their original dimensions:

\[^{}_{_{^{}}},^{ }_{_{^{}}}=G^{i}_{}(G^{i}_{ }(G^{i}_{}(G^{i}_{}(^{}_{ _{^{}}},^{}_{_{^{}}}))))) \]

where the \(^{}_{_{^{}}}\) and \(^{}_{_{^{}}}\) denote the embedding processed by self-attention in the attention block. After that, we acquire the final embedding by performing a straightforward addition between the fused and original embeddings:

\[^{}_{_{^{}}}=^{ }_{_{^{}}}+^{}_{ _{}},\ ^{}_{_{^{ }}}=^{}_{_{^{}}}+^{}_{_{}} \]

Ultimately, we feed the enhanced embeddings at each level into the original feedforward network of the corresponding decoder:

\[^{+1}_{}=R^{i}_{_{}}(^{}_{_{^{}}}). \]

### Association Module

Aside from generating predictions at various levels, the AIMS model also strives to establish the association between low-level and higher-level results. For example, the part masks of a car should be explicitly associated with the entire car's mask. In image editing and manipulation, this can provide a more intuitive mask selection experience. While it is possible to associate the masks across different levels by computing mask intersection, such association knowledge cannot be instilled into the model. Instead, we introduce an association module to AIMS to learn cross-level associations by applying an association loss to the similarity between embeddings. Besides having the association graph as an outcome, this also helps the model be aware of the cross-level association relationships and thus benefiting the individual-level task performance. The association loss \(_{}\) is defined as:

\[_{}=_{}((_{ })((_{}))^{T},_{})+_{}((_{}) ((_{}))^{T},_{}) \]

where FC indicates a _fully-connected_ layer and \(\) denotes dot product. The \(_{}\{,\}^{_{} _{}}\) and \(_{}\{,\}^{_{} _{}}\) are the binary association ground truth of entity-part and relation-entity levels. Finally, the overall training loss for AIMS is

\[=_{}+_{} \]

### Mask Prompt Encoder

As aforementioned, we train AIMS with multiple existing segmentation datasets that each covers one or more hierarchy levels, whose details are provided in Table 1. Due to the use of predefined classes, most of the datasets suffer from the problem of partial annotations and have unannotated areas on the images. Furthermore, the unannotated areas appear and happen inconsistently among the multiple datasets (_e.g._, 91.6% of PACO is unannotated, while it is 0.01% for EntitySeg). This leads to undesirable supervision signals that confuse the model because something that is treated as _background_ in one dataset/level may qualify as _foreground_ in another dataset/level.

   Level & COCO & PPP & COCO-PSG & PACO & EntitySeg \\  Relation & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  Entity & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  Part & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\    
   Information & COCO & PPP & COCO-PSG & PACO & EntitySeg \\  Annotation Area & 89.1\% & 91.7\% & 90.4\% & 8.4\% & 99.5\% \\  Category Numbers & 133 & 20 & 133 & 75 & 634 \\  Image Numbers & 115k & 9.7\% & 46k & 35k & 32k \\   \\   

Table 1: The statistics of the datasets we used for training including COCO , EntitySeg , PascalVOC Part (PPP) , PACO , and COCO-PSG . The (a) indicates whether the dataset has the annotations for some level. The (b) statisfy the information of each dataset.

We design the Mask Prompt Encoder (MPE), which allows our method to segment images conditioned on specific mask-controlled task prompts. As depicted in Figure 4, our approach integrates four types of mask prompts: full image, single entity, dual entities and multiple entities (where the upper limit is the entire annotated area). By employing diverse mask prompts along with multi-level mask decoders, AIMS can accomplish different segmentation goals. The mask prompts, covering either the full or partial image area, drive the model to segment the entire image or target area at part, entity, or relation levels using the corresponding decoders. This approach enables us to provide the model with a trustworthy supervision signal for the region to be segmented, even when dealing with datasets that lack full image annotations. Moreover, during inference, the mask prompt encoder can be applied to unrestricted mask prompts. To some extent, the model can subdivide the masks of unseen classes and/or in-the-wild visual entities into reasonable sub-masks, even if they are absent in training data.

In particular, the mask encoder accepts a binary mask \(}\), which designates the area to be segmented. Our mask encoder utilizes a series of cascaded convolution layers to downscale \(}\), resulting in \(}^{ C_{m}}\).

\[}=(}) \]

where the detailed structure of the MaskEncoder consists of three operation blocks including CONV2+LN+GELU, CONV2+LN+GELU and CONV1. The 'CONV2', 'CONV1', 'LN', and 'GELU' represent a convolution layer with stride and a kernel size of 2 or 1, layer normalization, and Gaussian error linear units, respectively. Subsequently, we directly downscale \(}\) by \(2\), \(4\), and \(8\) to obtain \(}\), where \(s\), using bilinear interpolation.

Ultimately, we acquire multi-scale fusion features \(}s\) by summing the encoded image and mask features \(s\) and \(}\), respectively. For AIMS with MPE, we replace \(_{s}\) in Eq. 2 with \(}_{s}\):

\[}_{s}=_{s}+_{s} \]

Given MPE, we delve deeper into the sampling strategy implemented across various mask prompts and subtasks. To effectively balance contributions from each subtask and prompt type, we adopt a uniform sampling approach capable of considering multiple factors, such as subtask difficulty and the distribution of different prompt types within the training data. Specifically, in each training iteration, we have five subtasks: segmenting the entire annotated area (either the full image or multiple entities) based on three hierarchy levels, subdividing a single entity to parts, and subdividing two merged entities with relationships to two independent entities. Additionally, each subtask has its own data pool for sampling, taking into account the statistical annotation information of each dataset. For more details, please refer to our supplementary file.

## 4 Experiments

We construct our training set by aggregating images from five segmentation datasets, including COCO , EntitySeg , PascalVOC Part (PPP) , PACO , and COCO-PSG . Given that

Figure 4: Illustration of the task prompt types with different combinations between mask prompts and decoders. For each sub-figure, the right part is our target of the network.

each dataset was originally created for a distinct task, we split each dataset into training and evaluation, and subsequently merge them to form unified training and evaluation sets. Overall, our training and evaluation sets have about 236.7K and 2.1K images. Please refer to our supplementary file for details on the split criteria. With the exception of COCO and EntitySeg, which are solely for entity-level prediction with full or partial image mask prompts, PPP dataset is utilized for entity/part prediction with full or partial image mask prompts and the entity-to-part task. COCO-PSG is employed for relation/entity prediction with full or partial image mask prompts and a relation-to-entities task. We keep only the relationship annotations in COCO-PSG that involve physical contact, to be consistent with typical part-whole relationships. PACO dataset is exclusively used for the entity-to-part task.

**Training settings.** We train our model for 36,000 iterations using a base learning rate of 0.0001 and weights pre-trained on COCO-Entity  with the exception of images contained in our validation set. The longer edge size of the images is set to 1,333 pixels, while the shorter edge size is randomly sampled between 640 and 800 pixels, with a stride of 32 pixels. The learning rate is decayed by a factor of 0.1 after 28,000 and 33,000 iterations, respectively. During each training iteration, we sample the data and tasks as introduced in the sampling strategy of Section 3.4, with a batch size of 64 on 8 A100 GPUs.

**Federated evaluation.** We build our evaluation set by collecting images from PPP  and COCO-PSG  dataset, where each image only has two level annotations. Similar to LVIS , our evaluation images have non-exhaustive annotations. To better approach this, we organize the evaluation set into two subsets: one for entity-part and another for relation-entity. For each subset, we evaluate our model by the instance segmentation metric _mean average precision (mAP)_ for each level and recall AR@100 for cross-level association predictions. For brief illustration, AP\({}^{p}\), AP\({}^{e}\), and AP\({}^{r}\) are mAP for part-, entity- and relation- predictions. The AR\({}^{cp}\)@100 and AR\({}^{rc}\)@100 are metrics for entity-part and relation-entity association performance.

In the following, we first ablate our experiments with a Swin-Tiny  backbone and then report the quantitative performance and visualization results of our best model with the Swin-Large  backbone to other state-of-the-art methods.

### Ablation Study

**Multi-dataset training.** Table 2 demonstrates the overall performance improvement by introducing multiple datasets. In the 1st and 2nd rows, we directly apply the base framework (baseline) with single dataset training and separate decoders. From the inference results, we observe that the model trained on the PPP dataset yields lower performance at the entity level when evaluated on the COCO-PSG dataset. This is primarily due to the fact that the COCO-PSG dataset contains 4 times more images than the PPP dataset, which does not provide sufficient entity-level annotations for the model. This issue can be alleviated through multi-dataset training, as depicted in the third row.

When we train our baseline method with all datasets, there are slight improvements or even degradation in part-level prediction for the PPP dataset, and relation and entity-level prediction for the COCO-PSG dataset. This can be attributed to annotation inconsistencies and incompleteness among multiple datasets, which results in ambiguous training signals for the model. By exploiting task complementarity (with Task Complementarity Module) and mitigating annotation issues (with MPE), our proposed AIMS framework can improve performance on both evaluation datasets.

**Improvement trajectory of the proposed method.** In Table 3(a), we show the component-wise improvements over the baseline. The 1st and last rows represent the performance of our baseline (three separate decoders for three levels) and the full AIMS framework. The middle three rows display the performance enhancement over the baseline method when integrated with different modules. It is evident that both MPE and TCM contribute to improvements in performance across all three

   &  &  &  &  \\   & & & AP\({}^{r}\) & AP\({}^{e}\) & AP\({}^{levels (mAP and association recall) on the two evaluation subsets, highlighting the effectiveness of these proposed modules on all levels. With the addition of the association module, a more significant improvement in association performance can be achieved while maintaining a similar level of segmentation performance compared to the baseline. This indicates that our proposed association module does not negatively impact the segmentation performance. Furthermore, all three modules can mutually benefit each other, giving the best performance as shown in the last row.

**Mask Prompt Encoder** In Table 3(b), we demonstrate the impact of using different mask prompt types during training. Note that only full-image mask prompts are used during inference. The 1\({}^{}\) and 2\({}^{}\) rows show that full-image mask prompts do not bring any impact. In the third row, solely using a mask prompt based on the partial annotation area of the image results in a significant performance decline due to the mask prompt inconsistency between the training and inference stages, because inference only uses full-image prompts. However, when both full and partial image prompts are used, as shown in the 4\({}^{}\) row, better performance is achieved by simultaneously mitigating the incomplete annotation problem and maintaining training-inference consistency. As shown by the last row, the introduction of dual-entity or single-entity mask prompts further improves performance, by pushing the model to learn better how to split masks between two immediate adjacent levels (relation-to-entities & entity-to-parts). This is different than (but complements) using full- or partial-image mask prompts which impose large gaps between the source and target levels (_e.g.,_ image-to-parts).

In Table 4(a), we examine the influence of various mask encoder architectures The 1\({}^{}\) row represents our baseline without a prompt mask encoder. As demonstrated in the 2\({}^{}\) and 4\({}^{}\) rows, we find that adding a 'CONV2' and 'CONV1' block with convolution kernel and stride sizes of 2 and 1 is sufficient for the mask encoder architecture. Incorporating more 'CONV1' blocks does not lead to a significant improvement in the final performance, as shown in the third row. This is because a single 'CONV1' layer is enough to encode mask information after the feature that has been already processed by two 'CONV2' blocks, as evident in the last two rows.

**Task Complementarity Module**. Table 4(b) presents the ablation study on the design of TCM. It is evident that the self-attention block is capable of fusing information from each level to enhance each respective task, as demonstrated in the 2\({}^{}\) row. Moreover, adding the original level-specific embeddings back to fused embeddings is crucial for improving the final performance. This is because the level-specific embeddings carries unique information specific to each task.

Table 4: Ablation studies. **(a)** The structure design of prompt mask encoder. The ‘CONV2B’ and ‘CONV1’ indicates convolution block operation with kernel and stride size 2 followed by layer normalization and GELU and convolution with kernel and stride size 1. The \( 2\) means the two sequential structures. **(b)** The structure design of the task correlation module. The terms ‘SA’, ‘FF’, and ‘ADD’ denote self-attention, feed-forward, and addition respectively.

Table 3: Ablation studies. **(a)** Improvement trajectory of the proposed modules. The abbreviations ‘MPE’, ‘TCM’, and ‘AM’ represent the mask prompt encoder, task complementarity module, and association module, respectively. **(b)** Ablation study concerning the utilization of mask prompt types. The terms ‘FUM’, ‘PAM’, ‘REM’, and ‘EPM’ denote the mask types for the full image, partial image with multiple entities, dual entities’ mask, and single entity mask, respectively. The checkmark symbol (\(\)) and the circle symbol (\(\)) indicate whether the ablated module is used or not.

### Comparison to state-of-the-art methods

In the following comparison, we evaluate our model against other state-of-the-art methods. To ensure a fair comparison, all methods employ Swin-Large backbone, with the exception of SAM, which uses a ViT-Huge backbone .

**Category-aware segmentation.** We transfer our AIMS method to other class-aware segmentation tasks, including panoptic part segmentation and panoptic scene graph on PPP and COCO-PSG datasets, respectively. To equip our method with class-aware capability, we directly use our AIMS model as pre-trained weights to fine-tune the corresponding class-aware training dataset. In Table 5(a), we observe that our AIMS model outperforms the corresponding state-of-the-art methods on both tasks. This is because our AIMS model can utilize more training data from various datasets by employing our proposed prompt mask encoder to address annotation inconsistencies among them. Moreover, our AIMS model benefits from the two-stage process of class-agnostic pretraining and class-aware fine-tuning , which has been demonstrated to improve the performance of the second stage class-aware fine-tuning when class-agnostic pretraining is performed with more data.

**Interactive segmentation.** In Table 5(b), we compare AIMS to another popular interactive segmentation method, SAM , on the PACO and ADE20K  datasets, where ADE20K is for zero-shot evaluation at entity and part levels. Neither SAM nor AIMS uses ADE20K for training, and ADE20K's classes are fairly dissimilar to PascalVOC's . The 1st row represents our baseline method with purely class-agnostic training without interactive segmentation, which outperforms SAM at the pure entity level even though the SAM model is trained on 11 million images. This indicates that the zero-shot generalization ability is attributed to class-agnostic rather than interactive training. On the other hand, SAM's interactive segmentation can divide masks into more fine-grained segments, which our AIMS is also capable of. On both evaluation datasets, AIMS remarkably outperforms SAM, using merely a 273K training set that is far smaller than SAM's 11M training set. Our proposed MPE and TCM allow our model to leverage more desirable training signals and multi-level task information in a data-efficient manner. Furthermore, our AIMS model can make relation-level predictions, which are higher than part and entity levels and absent in the SAM model.

Figure 5 provides a visual comparison between the SAM  model and our model in a real-world scenario. Like AIMS, the SAM model also supports image segmentation at three levels (object,

Figure 5: Qualitative comparison of SAM  and our method in interactive segmentation. The prompt mask indicates an entity mask that is provided to the network for further separation to the next low level. For the SAM model, we use hyper-parameters for more fine-grained segmentation, which will be introduced in our supplementary material. The images in the first row and the last two rows are from PACO  (in-domain) and ImageNet  (out-of-domain).

 Method &  PPP \\ PQ \\  } &  COCO-PSG \\ PartPQ \\  } &  DTAP \\  } &  OPTO \\ PQW \\  &  COCO-PSG \\ R/mR@0 \\  &  ADE20K \\ R/mR@50 \\  \\   PanopticPartFormer++  \\ PSGTR  \\ Ours \\  & 68.0 & 67.0 & 68.3 & - & - & - & - \\   PaporteisPertFormer++  \\ PSGTR  \\ Ours \\  & - & - & - & 28.2/15.4 & 32.1/20.3 & 35.3/21.5 \\  
 EntitySeg  \\ SAM  \\ Ours \\  & **72.9** & **70.8** & **72.3** & **36.7/22.9** & **39.4/25.6** & **42.3/28.2** \\  

Table 5: Ablation studies. **(a)** Comparison with state-of-the-art (SOTA) methods in panoptic part segmentation and panoptic scene graph tasks. **(b)** Performance comparison of segmentation generalization for images in the real world. The AR\({}^{R}\), AR\({}^{E}\), and AR\({}^{P}\) represent recall@100 at relation, entity, and part levels, respectively.

part, and sub-part level). For an equitable comparison, we use the same entity mask prompt for further delineation. As shown in Figure 5, both models capably segment parts of the entity with distinct approaches. The SAM model leans towards identifying parts based on color and texture clues, whereas our AIMS model relies more on semantic understanding.

The flexibility of our AIMS task.Figure 7 illustrates how our proposed AIMS task provides the flexibility for segmenting anything. This is to address the subjective annotation issues in existing datasets. For instance, in the image (a), the throw pillows and the sofa are predicted as a single entity, which aligns with the ground truth annotation. Nonetheless, in certain scenarios, a user might wish to edit the throw pillows independently. With our AIMS method, utilizing a full image prompt for part-level prediction does not yield these separated masks. However, by selecting this entity mask as an entity prompt for part-level prediction, we can successfully differentiate the three throw pillows.

Additionally, in image (b), a user may want to segment the windows into several components. However, original ground truth annotations typically consider the two windows as a single entity. To meet this requirement, we initially utilize a full image prompt and an entity encoder to identify the mask of the whole windows. Following this, we apply this two-entity mask prompt and relation Decoder to split them into two independent window masks. Ultimately, we can select one window for further segmentation into two parts: the window edge and curtains.

## 5 Conclusion

This paper presents AIMS, a unified segmentation model that parses images at three levels: part, entity, and relation. To address the absence of a dataset with annotations for all three levels, we construct a unified dataset by aggregating multiple existing datasets, each providing annotations for one or two levels. Our base method uses three separate decoders for different-level predictions. To handle the annotation inconsistency issue, we propose Mask Prompt Encoder, which offers a more accurate task signal to the model, and Task Complementarity Module to enhance each level's prediction. Extensive experiments demonstrate the effectiveness of our method in both closed-world and zero-shot settings. Our work can serve as a springboard for further research on this new segmentation problem.

## 6 Acknowledgement

This research is based upon work supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA R&D Contract No. 2022-21102100001. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.

Figure 6: The illustration of the flexibility of AIMS tasks to tackle the subjective annotation issues in existing datasets.