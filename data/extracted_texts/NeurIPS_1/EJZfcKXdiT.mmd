# Event-3DGS: Event-based 3D Reconstruction Using 3D Gaussian Splatting

Hanqian Han  Jianing Li  Henglu Wei  Xiangyang Ji

Tsinghua University

Corresponding author: xyji@tsinghua.edu.cn

###### Abstract

Event cameras, offering high temporal resolution and high dynamic range, have brought a new perspective to addressing 3D reconstruction challenges in fast-motion and low-light scenarios. Most methods use the Neural Radiance Field (NeRF) for event-based photorealistic 3D reconstruction. However, these NeRF methods suffer from time-consuming training and inference, as well as limited scene-editing capabilities of implicit representations. To address these problems, we propose Event-3DGS, the first event-based reconstruction using 3D Gaussian splatting (3DGS) for synthesizing novel views freely from event streams. Technically, we first propose an event-based 3DGS framework that directly processes event data and reconstructs 3D scenes by simultaneously optimizing scenario and sensor parameters. Then, we present a high-pass filter-based photovoltage estimation module, which effectively reduces noise in event data to improve the robustness of our method in real-world scenarios. Finally, we design an event-based 3D reconstruction loss to optimize the parameters of our method for better reconstruction quality. The results show that our method outperforms state-of-the-art methods in terms of reconstruction quality on both simulated and real-world datasets. We also verify that our method can perform robust 3D reconstruction even in real-world scenarios with extreme noise, fast motion, and low-light conditions. Our code is available in [https://github.com/lampokn/Event-3DGS](https://github.com/lampokn/Event-3DGS).

## 1 Introduction

3D reconstruction  plays a crucial role in various cutting-edge fields, such as robot vision, virtual reality, and augmented reality systems. It usually enables the creation of accurate 3D models from ideal frame sequences. Nevertheless, with conventional cameras, 3D reconstruction performance has suffered from a significant drop in some challenging conditions  (e.g., fast motion blur and low light). Thus, how to use a new visual sensing paradigm for 3D reconstruction to overcome the shortcomings of conventional cameras remains a partially unsolved issue.

Event cameras , namely bio-inspired dynamic vision sensors, fundamentally differ from conventional cameras that capture frames at fixed intervals. Event cameras operate asynchronously, recording light changes with dynamic events at the microsecond level. This unique property endows event cameras with high temporal resolution, high dynamic range, low power consumption, and low latency. These advantages have driven their application in various challenging vision tasks , including recent efforts in 3D reconstruction .

Despite efforts  to use event cameras for 3D reconstruction, real-world performance in terms of quality, robustness, and real-time capabilities still needs improvement. Traditional non-learning optimization-based methods  serve as the foundation for event-based 3D reconstruction, but they often struggle with robustness and rendering quality. Recently, NeuralRadiance Fields (NeRF) [10; 28; 40] have gained popularity for scene representation and novel view synthesis from event data, utilizing a Multi-Layer Perceptron (MLP) and differentiable rendering. Although these NeRF-based methods [1; 2; 6; 14; 18; 24; 25; 26; 27; 31; 35; 52] achieve impressive results in photorealistic 3D reconstruction from neuromorphic cameras, they suffer from time-consuming training and inference processes. Additionally, their implicit representations limit scene editing capabilities. Moreover, NeRF have primarily been investigated using simulated data and high-quality real-world images captured under ideal conditions (e.g., optimal lighting and minimal noise), posing limitations on real-world 3D reconstruction. In contrast, the emergence of 3D Gaussian Splitting (3DGS) [5; 11; 15; 46; 47] presents a compelling alternative, boasting high reconstruction accuracy and swift inference speed. However, 3DGS has predominantly been utilized with image or video data for 3D reconstruction, with limited exploration in event streams.

To address this gap, we propose Event-3DGS, the first event-based reconstruction framework utilizing 3DGS for synthesizing novel views from event streams. More specifically, we introduce an event-based 3DGS framework, enabling direct processing of event data and reconstruction of 3D scenes while simultaneously optimizing scenario and sensor parameters. Then, we present a high-pass filter-based photovoltage estimation module, effectively reducing noise in event data to enhance the robustness of our method in real-world scenarios. Finally, we propose an event-based 3D reconstruction loss to optimize the parameters of our method for better reconstruction quality. Extensive experiments show that our method outperforms state-of-the-art methods in reconstruction quality on simulated and real-world datasets. This pioneering work in event-based 3D reconstruction with 3DGS sets a new benchmark, opening new avenues for high-quality, efficient, and robust 3D reconstruction in challenging real-world scenarios, such as extreme noise, fast motion, and low light. Our contributions can be summarized as follows:

* We introduce Event-3DGS, the first framework that combines event cameras with 3DGS technology, enabling 3D reconstruction in challenging real-world scenarios.
* We present a high-pass filter-based photovoltage contrast estimation module, which effectively estimates photovoltage contrast by reducing noise in event streams for robust 3D reconstruction.
* We design a novel event-based 3D reconstruction loss to optimize the parameters of our method for better reconstruction quality.

## 2 Related Works

**Event-based 3D Reconstruction.** Early attempts [3; 16; 17; 32; 51] at using event cameras for 3D reconstruction typically relied on geometric models and handcrafted features. However, these non-learning, optimization-based methods often struggle to achieve robust and high reconstruction quality. A growing trend is the use of neural radiance fields (NeRF) for scene representation and novel view synthesis. For instance, Ev-NeRF  and E-NeRF  are some of the earliest works to apply NeRF for event-based 3D reconstruction. These methods render images at different times, generate events through differencing, and compare them with actual events. Further advancements include DeNeRF  and EvDNeRF, which introduce Deformable NeRF for dynamic scene reconstruction. EventNeRF  extends this by enabling colored rendering through the incorporation of three-channel events into NeRF. Some methods like E2NeRF  and Ev-DeblurNeRF  perform hybrid reconstruction to mitigate motion blur by combining blurred images with events. However, these NeRF-based approaches face significant challenges, including the time-consuming generation of novel views and limited scene editing capabilities due to implicit representations. Additionally, NeRF have primarily been explored using simulated data and high-quality images captured under ideal conditions, leaving a considerable gap between the models and real-world scenarios. Therefore, our goal is to design a novel event-based 3D reconstruction framework that ensures high-quality, efficient, and robust performance in real-world scenarios.

**3D Gaussian Splitting for 3D Reconstruction.** 3D Gaussian Splitting (3DGS)  has significantly advanced in 3D scene representation, offering notable advantages over NeRF by capturing complex geometries and lighting effects more accurately and efficiently. These advantages make 3DGS highly suitable for real-time and real-world applications. Extended works like 4DGS [46; 47] and D-3DGS  further enhance dynamic scene rendering. Besides, some works integrate with Simultaneous Localization and Mapping (SLAM)  and text-to-3D models  to expand 3DGS capabilities. However, these methods have primarily been applied to image and video data, leaving their potential with event cameras largely unexplored. Thus, designing a novel 3DGS model to directly process asynchronous events for 3D reconstruction remains an open challenge.

## 3 Method

### Event-3DGS Architecture

To achieve high-quality, efficient, and robust 3D reconstruction in challenging real-world scenarios, we propose a novel event-based 3D reconstruction framework using 3D Gaussian Splitting (Event-3DGS). As shown in Fig. 1, our framework mainly consists of three modules: **high-pass filter-based photovoltage contrast estimation, photovoltage contrast rendering,** and **event-based 3D reconstruction loss**. More precisely, we first present a high-pass filter-based photovoltage contrast estimation module that reduces noise in event data to enhance the robustness of our method in real-world scenes (see Sec. 3.2). Then, we design a photovoltage contrast rendering module that obtains the photovoltage contrast image by calculating the difference in light intensity in 3DGS. After obtaining two contrast estimations, we propose a novel event-based 3D reconstruction loss to measure the differences (see Sec.3.3). Finally, our method optimizes the 3D scene and camera parameters by propagating gradients through backpropagation.

3DGS  demonstrates superior 3D reconstruction capabilities by rapidly converting input images into highly detailed 3D point clouds, accurately representing the scene. For a specific 3D scene represented by 3D Gaussian functions, the forward process of 3DGS can be regarded as a mapping function \(G()\), which gets the rendered image by alpha blending in the corresponding camera pose \(\) in time \(t\). For a single pixel on a single channel, alpha blending can be described as follows:

\[L=_{i=1}^{N}l_{i}_{i}_{j=1}^{i-1}(1-_{j}), \]

where \(L\) denotes the pixel value result, which can be intensity or one of the three channels. \(l_{i}\) and \(_{i}\) are the color and opacity of each point mapped to this pixel, respectively.

Event cameras operate on a fundamentally distinct imaging principle, generating event data in the form of sparse points (see Sec. 3.2). This disparity prevents the direct integration of asynchronous events into the original 3DGS formulation. To bridge this gap, we integrate event data seamlessly with the output of 3DGS by leveraging photovoltage contrast (i.e., intensity changes). Considering two close-in-time instances \(t_{1}\) and \(t_{2}\), the camera poses corresponding to these two moments are \(}\) and \(}\). We can obtain its photovoltage contrast between two moments using the proposed high-pass

Figure 1: The pipeline of **Event-based 3D Reconstruction using 3D Gaussian Splitting (Event-3DGS)**. The proposed event-based 3DGS framework enables direct processing of event data and reconstructs 3D scenes while simultaneously optimizing scenario and sensor parameters. A high-pass filter-based photovoltage contrast estimation module is presented to reduce noise in event data, enhancing the robustness of our method in real-world scenes. An event-based 3D reconstruction loss is designed to optimize the parameters of our method for better reconstruction quality.

filter-based photovoltage contrast estimation module, and it can be formulated as:

\[D_{gt}()_{t_{1}}^{t_{2}}=((V(,t_{2})-V(,t_{1})), \]

where \(V(p,t)\) refers to the photovoltage in pixel \(p\) and time \(t\).

Correspondingly, we need to use 3DGS to render the photovoltage contrast and light intensity, and then compare these with the ground truth obtained from the event data for subsequent reconstruction. The photovoltage contrast image can be obtained by the proposed photovoltage contrast rendering module. The rendering process can be mathematically described as follows:

\[D_{r}()_{t_{1}}^{t_{2}}=}((G( })+)-(G(})+)), \]

where \(G()\) denotes the intensity result in 3DGS. \(D_{r}(p)\) is the normalized contrast value in the pixel p, and \(\) is a small number that avoids log(0). \(\) is a learnable parameter, which is an estimate of the threshold in the event sensor. For the sake of convenience in writing, we do not strictly distinguish between time \(t\) and the corresponding pose \(T\).

Given the intrinsic characteristics of event data, it's essential to highlight that intensity estimation methods [37; 45] are unlikely to outperform photovoltage contrast estimation techniques. Even data-driven learning-based models [7; 34] may only yield visually appealing results without ensuring physical accuracy and generality. To address this challenge, we proposed a dynamic adjustment strategy for intensity to use high-quality photovoltage while ensuring robustness and stability.

### High-pass Filter-based Photovoltage Contrast Estimation

To gain a deeper understanding of the high-pass filter-based photovoltage contrast estimation module, it's essential to begin with a fundamental understanding of the Dynamic Vision Sensor (DVS) . The DVS is designed to capture changes in light intensity pixel-wise and asynchronously. It accomplishes this by converting the light spectrum into photocurrent as follows:

\[I(p,t)= QE() L(p,t,) d, \]

where \(L(p,t,)\) denotes the spectrum, \(t\) is time, \(p\) refers to the pixel coordinate \(\{x,y\}\), and \(\) is the wavelength of light. \(I(p,t)\) is the photocurrent or intensity in pixel \(p\) and time \(t\). \(QE\) is quantum efficiency, which represents the weighting of different wavelength light converted into photocurrent.

Subsequently, the photocurrent will be converted into photovoltage through a logarithmic function. When the voltage change surpasses a predefined threshold \(\), an event will be triggered  as:

\[V(p,t_{2})-V(p,t_{1})=_{i}^{p}, \]

where \(V(p,t)\) refers to the logarithmic operation of the photocurrent \(I(p,t)\). \(_{i}^{p}\) is either +1 or -1 by comparing \(V(p,t_{2})\) and \(V(p,t_{1})\).

In general, the threshold \(\) is an inherent attribute of the DVS. It can be defined as follows:

\[=V_{th}}{U_{T}A_{v}}, \]

where \(C_{diff}\) is determined by the capacitance in the,\(V_{th}\) is a fixed constant, \(U_{T}\) is thermal voltage, and \(A_{v}\) is voltage gain factor. This formula illustrates that \(\) often varies with sensor settings and environmental changes, making it generally difficult to obtain its true value. In this work, we will optimize the threshold \(\) together with the 3D scene.

Intuitively, asynchronous events appear as sparse points  in the spatiotemporal domain:

\[E(p,t)=_{i=1}^{N}_{i}^{p}(t-t_{i}^{p}), \]

where \(()\) refers to the Dirac delta function, with \((t)dt=1\) and \((t)=0, t 0\).

In this work, we implement the reverse process of dynamic event generation by extracting photovoltaic contrast and intensity from the event stream \(E(p,t)\). The photovoltage is given as:

\[V(p,t)-V(p,t_{i})=E(p,t_{j})+w(p,t), \]

where \(t_{i}\) and \(t_{j}\) are the times corresponding to two adjacent events around \(t\). \(w(p,t)\) denotes an uncertainty term, which is determined by the mathematical principles of the event camera and is unrelated to noise. This formula indicates that the event camera cannot accurately reconstruct the intensity of light outside the triggering event's timing. Moreover, since \(V(p,t_{i})\) is unknown, it is also impossible to accurately obtain the intensity of light at each event's timing. In short, only the intensity difference between each event's triggering times can be accurately obtained.

For the photovoltage between two events, we can simply assume:

\[w(p,t)=-E(p,t_{j}),t<t_{j}. \]

The above assumption will not interfere with the voltage at the time of event triggering. If \(E(p,t)\) is ideal, V can be directly obtained from \(E\) through pure integration, that is:

\[}(p,t)=V(p,t)-V(p,t_{0})=_{0}^{t}E(p,t)dt, \]

where \(}(p,t)\) is the photocurrent contrast to be estimate. Ideally, this method can accurately provide the photovoltage contrast between the triggering moments of each event.

However, when event cameras are applied in real-world 3D reconstruction, they often encounter various types of noise, making it challenging to accurately estimate the photovoltage contrast through pure integration . To address this issue, we use high-pass filtering to process the event data. The high-pass filtering can be depicted as follows:

\[_{out}(s)}{_{in}(s)}=, \]

where \(V_{out}(s)\) and \(V_{in}(s)\) are the input-output signals after the Laplace transformation. \(\) is the time constant related to the cutoff frequency. We treat \(_{0}^{t}E(p,t)dt\) as a noisy input \(V_{in}\) and \(}(p,t)\) as the output \(V_{out}\). When substituted into Eq. 11 and transformed back into the time domain, we obtain that:

\[}(p,t)=E(p,t)-}(p,t), \]

where \(}(p,t)\) is the differential with respect to the timestamp.

By simultaneously solving with Eq. 7, we can estimate the photovoltage contrast between any two corresponding moments. Once the photovoltage is obtained, the intensity can be computed as:

\[I(p,t)=e^{}(p,t)+}(p,0)}. \]

\(V(p,t_{0})\) does not affect photovoltage contrast estimation. Therefore, our high-pass filter-based method typically provides more accurate results than restoring pure photovoltage or intensity.

### Event-based 3D Reconstruction Loss

For better reconstruction quality, we use a loss function with two key components: intensity estimation and photovoltage estimation. The light intensity is evaluated as follows:

\[l_{i}^{t_{1}}=l_{1}(I(,t_{1}),G(})), \]

where \(l_{1}\) is a loss function that computes the average of the absolute values between each pixel.

For the evaluation of photovoltage contrast, it's important to consider that 3DGS uses a rasterization method to generate the entire image at once. This means the rendering sampling time often does not match the event triggering time for most pixels. According to Eq. 8, the ground truth of the photovoltage contrast inherently contains some errors. Additionally, despite applying filtering methods, event data still has significant noise that cannot be entirely eliminated. Consequently, if we directly use the \(L_{1}\) loss to compare the rendered photovoltage contrast with the photovoltage contrast calculated from event data, these errors will be strictly considered during the reconstruction process,which can actually degrade the reconstruction quality. Thus, if a new loss can be designed that allows for a certain tolerance in the estimation of photovoltage contrast, it would enable better completion of the reconstruction task. For simplification, we denote \(L_{g}^{p}=D_{gt}(p)_{t_{1}}^{t_{2}},L_{r}^{p}=D_{r}(p)_{t_{1}}^{t_{2}}\). The loss function of photovoltage contrast can be formulated as:

\[l_{e}^{p,t_{1},t_{2}}(L_{g}^{p},L_{r}^{p})=R(|L_{g}^{p}-L_{g}^{p}- |-)&L_{g}^{p}>0\\ R(|L_{g}^{p}-L_{r}^{p}-|-)&L_{g}^{p}<0, \]

where \(R(x)=(0,x)\), and \(\) is a measure of the tolerance in the estimated photovoltage contrast obtained from rendering. When \(\) is infinitely large, the loss becomes a trivial constant zero, imposing no constraints. Conversely, if \(\) is zero, the loss degenerates into a very strict \(L_{1}\) loss. Theoretical analysis shows that setting \(\) to 0.5 yields excellent results in the real-world dataset. When \(\) is set to 0.5, it best aligns with the characteristics described in Eq. 8.

Finally, the total loss function for event-based 3D reconstruction can be described as follows:

\[l_{all}(t_{1},t_{2})=_{p R^{2}}^{p,t_{1},t_{2}}(L_{g}^ {p},L_{r}^{p})}{W*H}+(1-)l_{i}^{t_{1}}, \]

where W and H are the width and height of the image, respectively. \(\) is a parameter that controls the weight of the intensity. For the first 8000 epoch, \(\) is set to 0 to give an initialization of the reconstruction. Then, \(\) is generally set near 1 to improve the quality of 3D reconstruction.

## 4 Experiments

### Experimental Setting

**Datasets.** To evaluate the effectiveness of our Event-3DGS, we conduct experiments on the DeepVoxels synthetic dataset  and the real-world Event-Camera dataset. For the synthetic dataset, we use seven sequences with continuous 180-degree image rotations on a gray background as the ground truth for reconstruction. These sequences are processed by the VOLT simulator  to generate event data, offering a more realistic simulation than ESIM  with higher noise levels. For the real-world dataset, we select five typical sequences that provide aligned image and event data under fast motion and low-light conditions. For longer sequences, we typically utilize the initial 100 images for training and evaluate performance on separate data not employed during reconstruction.

**Implementation Details.** We set \(\) to 0.05 for the high-pass filter-based photovoltage contrast estimation module. In the loss function, we set \(\) to 0.9. For synthetic experiments with low noise, \(\) is set to 0, while for real data with higher noise, \(\) is set to 0.5. We utilize E2VID  for initial intensity estimation. We use E2VID+3DGS as a baseline for event-based 3D reconstruction, comparing it with our full method to validate its efficacy. All experiments are conducted on an AMD Ryzen Threadripper 3970X 32-Core CPU and an NVIDIA GeForce RTX 3080 Ti GPU. The evaluation metrics use the Peak Signal Noise Ratio (PSNR), the Structural Similarity (SSIM) , and the Learned Perceptual Image Patch Similarity (LPIPS) .

    &  &  &  &  \\   & SSIM & PSNR & LPIPS & SSIM & PSNR & LPIPS & SSIM & PSNR & LPIPS & SSIM & PSNR & LPIPS \\  mic & 0.938 & 19.965 & 0.048 & 0.946 & 19.955 & 0.068 & 0.955 & 21.979 & 0.060 & 0.952 & 21.127 & 0.063 \\ ship & 0.808 & 16.556 & 0.108 & 0.825 & 16.681 & 0.122 & 0.792 & 16.750 & 0.177 & 0.818 & 17.815 & 0.147 \\ materials & 0.872 & 18.302 & 0.084 & 0.885 & 18.325 & 0.094 & 0.925 & 20.053 & 0.062 & 0.933 & 20.506 & 0.060 \\ lego & 0.8

[MISSING_PAGE_FAIL:7]

**Contribution of Each Component.** To explore the impact of each component on the final reconstruction performance, we chose the pure integration image without the adaptive threshold and event loss as the baseline. As illustrated in Table 3, We use three different strategies (e.g., adaptive thresholding, high-pass filtering, and loss function) to enhance our baseline. As a result, our method achieves the best performance among all competitors. In other words, our method employs these effective components to process event streams for 3D reconstruction.

**Influence of the Parameter \(\).** To analyze the hyperparameter \(\) of the loss function, we set the hyperparameter \(\) with various values (e.g., 0.05, 0.2, 0.4, 0.6, 0.8, 0.9, and 1). As shown in Table 4, large values of \(\) pose a risk of the training deviating to suboptimal points. Small values render the main phase ineffective, resulting in degraded outcomes similar to the initial phase.

**Influence of the Parameter \(\).** We report the impact of varying \(\) in Table 5. The results indicate that the reconstruction performs optimally when \(\) is set to 0.5. Deviations from this value led to a decline in performance, consistent with theoretical predictions.

### Scalability Test

**Event-3DGS for Motion Deblurring.** Our Event-3DGS can be further expanded to motion deblurring. By integrating event data with RGB frames, our method can achieve deblurring effects using the hybrid reconstruction manner. As shown in Fig. 6, We test the hybrid framework on some simulated sequences  using VOLT . Note that, a blurred image is generated through integration to serve as the RGB input. Our Event-3DGS leverages event data to achieve deblurred color reconstruction.

   \(\) & 1 & 0.99 & 0.9 & 0.8 & 0.6 & 0.4 & 0.2 & 0.05 \\  SSIM & 0.945 & 0.953 & **0.956** & 0.955 & 0.954 & 0.953 & 0.952 & 0.952 \\ PSNR & 25.688 & **27.302** & 25.974 & 25.066 & 24.540 & 24.300 & 24.146 & 24.148 \\ LPIPS & 0.063 & 0.051 & 0.039 & 0.040 & **0.038** & 0.038 & 0.039 & 0.039 \\   

Table 4: The influence of the Parameter \(\)

Figure 4: Representative visualization examples on low-light and high-speed motion blur scenarios.

Figure 3: Representative visualization results on the real-world Event-Camera dataset. Note that, our Event-3DGS achieves better reconstruction quality than the three comparative methods.

   Threshold & & & & & & & & \\ Filtering & & & & & & & & \\ Loss & & & & & & & & \\  SSIM & 0.219 & 0.317 & 0.376 & 0.221 & 0.410 & 0.310 & 0.393 & **0.430** \\ PSNR & 6.713 & 11.197 & 0.594 & 6.878 & 11.715 & 10.009 & 9.840 & **14.043** \\ LPIPS & 0.767 & 0.463 & 0.249 & 0.765 & 0.217 & 0.424 & 0.191 & **0.183** \\   

Table 3: The contribution of each component.

**Event-3DGS for Color Reconstruction.** In general, adding color information to 3D reconstructed images is crucial for visual appeal and downstream applications. To achieve this, we extend Event-3DGS from a single channel to three channels to enable color reconstruction. As illustrated in Fig 5, we selected a video sequence  and utilized the VOLT simulator  to convert the RGB channels into events. Using our method framework, we jointly reconstructed these three channels. The results in Fig. 5 demonstrate that our method can achieve high-quality color reconstruction.

**Limitation.** our method rendering module's adaptive threshold learns a threshold for each scene but doesn't account for variations within the same scene over time. Additionally, our current 3DGS lacks support for dynamic scenarios, where 4DGS may be a solution. Future research will address these limitations to enhance Event-3DGS practicality.

## 5 Conclusion

This paper introduces Event-3DGS, a pioneering event-based 3D reconstruction framework that utilizes 3D Gaussian Splatting (3DGS) to directly process event streams for synthesizing novel views. We present a high-pass filter-based photovoltage estimation module to effectively reduce noise in event data, enhancing the robustness of our method in real-world scenarios. Additionally, we design an event-based 3D reconstruction loss to optimize the parameters of our method. Our results demonstrate that our method surpasses state-of-the-art methods in both reconstruction quality and computational speed on simulated and real-world datasets. We also verify that our method can perform robust 3D reconstruction even in real-world cases with extreme noise, fast motion, and low-light conditions. We believe that our method establishes a new benchmark for using 3DGS with event data, paving the way for high-quality, efficient, and robust 3D reconstruction in challenging real-world scenarios.

Figure 5: Representative examples of colorful event-based 3D reconstruction.

Figure 6: Representative visualization examples of motion deblurring. Note that, our Event-3DGS can be extended for high-quality hybrid reconstruction using events and frames with motion blur.

   \(\) & 0 & 0.1 & 0.15 & 0.2 & 0.25 & 0.5 & 0.75 & 1 \\  SSIM & 0.405 & 0.418 & 0.416 & 0.459 & 0.459 & **0.497** & 0.430 & 0.477 \\ PSNR & 8.523 & 8.874 & 8.904 & 10.754 & 10.688 & **12.448** & 9.508 & 11.441 \\ LPIPS & 0.324 & 0.310 & 0.310 & 0.285 & 0.280 & **0.261** & 0.296 & 0.271 \\   

Table 5: The influence of the Parameter \(\).