# CODE: Contrasting Self-generated Description to Combat Hallucination in Large Multi-modal Models

Junho Kim

Equal contribution. \(\) Corresponding author.

Code is available at [https://ivy-lvm.github.io/CODE](https://ivy-lvm.github.io/CODE)

Hyun Jun Kim1

Yeon Ju Kim

Yong Man Ro

Integrated Vision and Language Lab, KAIST, South Korea

{arkimjh, kimhj709, yeonju7.kim, ymro}@kaist.ac.kr

###### Abstract

Large Multi-modal Models (LMMs) have recently demonstrated remarkable abilities in visual context understanding and coherent response generation. However, alongside these advancements, the issue of hallucinations has emerged as a significant challenge, producing erroneous responses that are unrelated to the visual contents. In this paper, we introduce a novel contrastive-based decoding method, COuntering DEscription Contrastive Decoding (CODE), which leverages self-generated descriptions as contrasting references during the decoding phase of LMMs to address hallucination issues. CODE utilizes the comprehensive descriptions from model itself as visual counterpart to correct and improve response alignment with actual visual content. By dynamically adjusting the information flow and distribution of next-token predictions in the LMM's vocabulary, CODE enhances the coherence and informativeness of generated responses. Extensive experiments demonstrate that our method significantly reduces hallucinations and improves cross-modal consistency across various benchmarks and cutting-edge LMMs. Our method provides a simple yet effective decoding strategy that can be integrated to existing LMM frameworks without additional training.

## 1 Introduction

With recent advancements of Large Language Models (LLMs) [18; 4; 54; 11; 46], Large Multi-modal Models (LMMs), sometimes referred as Large Vision-Language Models [24; 63; 32; 31], have been drawn great attention for their natural multi-modal interaction with users through back-and-forth conversations. Leveraging their robust generation capabilities, various pioneering tasks in pre-LMM era such as image captioning [7; 58; 27], visual question answering [1; 2], object detection , etc., have been integrated into a single task rather than treated as sub-tasks and achieved significant milestones [21; 59; 44]. However, at the same time, the hallucination issue [49; 67] has become one of the emerging problems when adopting LMMs into real-world applications due to their potential spurious generation in critical areas.

Here, unlike hallucination studies in LLMs  mainly focusing on factuality hallucination originated from the language knowledge, the hallucination problem in LMMs refers cross-modal inconsistency between the given visual contents and the generated responses for the user instructions. After the seminal works [13; 42; 68] giving eyes to LLMs to understand visual contents with visual instruction tuning, numerous cutting-edge LMMs [61; 9; 41; 36] actively have been proposed. Albeit the scaling laws following more stronger versatile vision models [37; 29; 33], higher resolution [41; 8], deeper alignment layers [5; 45; 8], larger model sizes, etc., LMMs still suffer from generating responses that seem plausible but are factually incorrect for the given visual contents.

The origin of LMM hallucination is an intertwined problem for their inherent training paradigm, which involves alignment projection matching during the pre-training, followed by fine-tuning with the limited instruction-following data. Several approaches, aimed for mitigating hallucinatory effects, have addressed the inconsistent issues in the context of data-associated solution , scaling model architectures , or additional RL-based training . Among them, reactive methods  intervene the decoding phase of LMMs' inference and alleviate undesired responses. Motivated by Li _et al._ that have proposed contrastive decoding (CD) method between expert and amateur language models, recent CD-based approaches in LMMs have proposed several ways of contrasting model responses from visual inputs with their counterparts (_e.g.,_ visual contamination , image-biased models , or fine-grained visual information ).

Our research question begins with _"How effectively do contemporary LMMs capture visual evidences in their descriptive responses, and what information must be curbed to produce informative and consistent responses?"_. As illustrated in Fig 1 (bottom-left), when asking LMMs to generate a comprehensive description for visual content, the output seemingly generates detailed description effectively, but a closer examination often reveals missed fined-grained information or hallucinatory instances in the responses. By recursively referring these incomplete descriptions generated by the models themselves, we aim to restrict the incorrect information flow during the generation phase and enhance the alignment of the model responses grounded in true visual evidences.

In this paper, we introduce a novel training-free contrastive decoding method, COuntering DEscription Contrastive Decoding (CODE), designed to use self-generated descriptions as a contrastive reference to mitigate hallucination issues in LMMs. The core idea of our proposed method is on harnessing the self-generated descriptions which possibly encompass both factual evidence and hallucinatory information from visual contents as a look-up reference for response correction. Specifically, within our contrastive framework as illustrated in Fig. 1, the comprehensive descriptions from model itself alternatively propagate to visual input tokens and contrast the discrepancy with the logits from actual visual contents to enhance next-token prediction. In addition, we introduce a dynamic restriction strategy that enables adaptive control of information flow during the auto-regressive decoding phase, taking into account both token-level predictions and their distribution within the vocabulary set.

By conducting extensive experiments and analyses on prevailing cutting-edge LMMs , we corroborate the effectiveness of our method in reducing hallucination and enhancing the coherence and informativeness in various benchmarks . Our decoding method can be seamlessly integrated into existing LMMs by simply substituting the image tokens with self-generated descriptions in a training-free manner.

Our contribution can be summarized into three-fold as follows:

* We introduce COuntering DEScription Contrastive Decoding (CODE), a training-free decoding strategy that employs self-generated descriptions to minimize hallucinations in LMMs.

Figure 1: The overall decoding procedure of CODE. After LMMs generate detailed description for the visual content by themselves (see instruction in Appendix. A), the model recursively outputs logits from each \(v\) and \(d\). By contrasting between two log-likelihoods, CODE produces more contextual and correct responses that match the given visual content suppressing inconsistent words (_catching\(\)hit_).

By contrasting logit information from descriptions with actual visual contents, CODE enhances visual consistency and coherence in the model responses.
* Our approach incorporates dynamic restriction strategies within the contrastive decoding phase. It selectively regulates the information flow by adjusting token-level predictions based on their distribution in the vocabulary, thus ensuring more contextual responses.
* We validate the effectiveness of our decoding method across various benchmarks using cutting-edge LMMs. The results demonstrate that CODE significantly reduces hallucination while enhancing the relevance and informativeness in the responses.

## 2 Related Work

Vision+LLM: Large Multi-modal ModelsAfter the emergence of large-scaled LLMs [4; 54; 11] that can interact with users with question-answer chat, various of vision+LLM studies-- _i.e.,_ LMMs, have been proposed to integrate the robust linguistic capability into the visual understanding and reasoning in diverse vision-language task. As earlier works such as LLaVA , Instruct-BLIP , and MiniGPT-4 , which utilized visual instruction tuning, have bridged two modalities of vision and language through fine-tuning with a learnable query, exemplified by Q-Former  or projection layer-based alignments . To enhance cross-modal consistency in vision-language representation, recent works have proposed several solutions to address the underlying weaknesses in both modalities: (i) utilizing higher-resolution visual inputs [41; 9], (ii) deploying Mixture-of-Expert (MoE) concepts integrating versatile vision models [37; 29; 33], (iii) improving weak alignment interface [5; 45; 8], or (iv) adopting larger LLMs to scale up the language model prior .

Hallucination Issue, Harming Cross-modal ConsistencyDespite of the endeavor developments of LMMs, they cannot be free from cross-modal inconsistency between the visual contents and their generated responses, so-called hallucination . This not only leads to performance degradation but also provokes an over-reliance issue, resulting in incorrect model responses that are not grounded in true visual evidence. This critical concern regarding response trustworthiness and model reliability hinders the adoption of LMMs in real-world applications. To mitigate the hallucination problem, diverse works have been proposed employing additional training on curated datasets [52; 55] or reinforcement learning under feedback systems [64; 66]. Among them, by intervening during the response generation, decoding-based approaches [34; 12] are introduced to encourage models to represent more precise responses. We refer to readers for more comprehensive survey papers [43; 3] addressing hallucination in LMMs. Our work is in line with CD-based approaches that utilize logit discrepancy from counterpart outputs to enhance coherence. Unlike the previous works [30; 23; 10] that focus on twisting visual information, we utilize self-generated description as contrasting visual counterpart and correct hallucinatory responses based on the model understanding.

## 3 Proposed Method

Problem Setup and Preliminaries.Let \(M_{}\) denote a vision-language LMM parameterized by \(\) that auto-regressively generates responses for the given visual contents \(v\) and input textual query \(x\). Then the model maps the logit distribution to the next token prediction output \(y_{t}^{||}\) at time step \(t\) in the vocabulary set \(\) such that \(y_{t} p_{}(y_{t}|v,x,y_{<t})_{}(y_{t}|v,x,y_ {<t})\), where \(y_{<t}\) indicates all previously generated tokens. During the response generation, we can deploy several decoding strategies to choose the next word using either deterministic search (_e.g.,_ greedy, beam search) or stochastic sampling (_e.g.,_ top-k, Nucleus search ).

After the seminal works [34; 12] in natural language processing have introduced Contrastive Decoding (CD) mechanism, which considering information disparities between expert and amateur models for more coherence and informativeness, various works have deployed this strategy into LMMs by twisting visual contents  or model information  for the contrastive approach. The next-token probability \(p_{}\) from CD can be generally formulated as follows:

\[p_{}(y_{t} y_{<t})=[(1+)_{ }(y_{t} v,x,y_{<t})-_{}(y_{t},x,y _{<t})], \]

where \(\) and \(\) indicates visual counterparts and sub-optimal amateur model, respectively-- note that \(\)=\(\) can be regarded as self-correction. Intuitively, the objective of CD is amplifying model outputsby reflecting information deviation between top candidate log-probabilities. Therefore, the selection of logit counterparts for referring is the key challenge for high-quality and consistent responses during the contrastive decoding frameworks.

### Comprehensive Image Description as Visual Counterpart

As the visual counterpart, we deploy comprehensive image descriptions generated from the model as contrasting reference, which are inevitably less informative than the visual contents themselves. Our motivation is on the innate difference of information density between vision and language . While vision information exhibits relatively less redundancy for spatial signals-- _e.g.,_ human can visually recognize objects with a few masked patches on images, languages contain high-entropy information, resulting in spans that are more semantic and information-dense than visual signals. That is, it is difficult to infer blanked-out words-- _e.g.,_ "I went to the store to buy some ____ ". Building upon the property of each modality, we first delve into the self-generated model responses with specific instruction for the visual contents to elicit the encompassed visual evidences in the representation space and assess its potential subject role as a visual counterpart for contrastive decoding.

As illustrated in Fig. 1, we input a query instruction into the model to generate a comprehensive visual description for the given visual content (please see the detailed instruction \(x_{0}\) for the self-generated description in Appendix. A). Then, we exploit the generated description as recursive visual inputs, replacing the position of image tokens in the model input sequence (_e.g.,_ <image> token in LLaV series [42; 40; 41]). Ideally, if the model sufficiently covers whole visual evidences to answer any vision-related questions, the generated response should provide competent answers with solely utilizing the description-only embeddings as an alternative of visual embeddings. However, as shown in Fig 2, it is obvious that the results from description-only show sub-optimal performance to answer questions due to insufficient information in capturing visual evidences. Consequently, we use the comprehensive description for the visual contents, which is generated by model itself-- but partially incorrect or hallucinatory to capture visual evidences, as a contrasting visual counterpart to enhance response coherence during the decoding phase, which also in line with the amateur model selection philosophy of contrastive decoding .

### COuntering DEscription Contrastive Decoding

Based on our analysis in sec. 3.1, we can obtain a pair of the visual content and its comprehensive description \((v,d)\), such that \(d\) corresponds to \(M_{}(y|v,x_{0})\). By contrasting the logit variation between the paired information into the model response generation, we can formulate the next-word prediction using our proposed method, COuntering DEscription Contrastive Decoding (CODE):

\[p_{}(y_{t} y_{<t})=[(1+_{t})_{}(y_{t} v,x,y_{<t})-_{t}_{}(y_{t} d, x,y_{<t})]. \]

Here, unlike the previous approaches [34; 30; 12] that restrict the logit variations with fixed \(\) value as in Eqn. 1, we present a dynamic restriction \(_{t}\) for the logit variations by comparing the information between visual contents and their comprehensive descriptions. Revisiting the role of \(\), it determines whether to promote or curb information from logit variation, thus directly influencing next-token generation-- higher value results in more aggressive adjustment for the variations. However, when confronting that both \(v\) and \(d\) yield similar logit score on the correct token, the variation gets closer to zero, thereby the next-token prediction can be unexpectedly reversed if other tokens get rewarded than the correct token with a fixed \(\) on a token-by-token basis. Although this aligns with the initial intent of CD, a more robust selector is necessary to effectively restrict the logit information flow.

Accordingly, our method predicts next-token not only at the individual token-level but also considering its distribution across the entire vocabulary set, enabling dynamic control of the information flow.

Figure 2: The comparison is based on two benchmarks (MMVP : multiple choice / LLaVa-Bench : description-level). The plain and dotted bars indicate the results for the models that use self-generated descriptions as visual input replacements and original model with actual visual contents, respectively.

To measure the relative entropy between the token distributions from visual contents \(P_{t}^{v}\) and its comprehensive description \(P_{t}^{d}\) at time step \(t\), we deploy Bounded Divergence (\(_{}\)) , which is a type of statistical distance that ensures symmetric and bounded measure:

\[_{}(P\|Q)=_{i=1}^{n}(p_{i}+q_{i})_{2}( |p_{i}-q_{i}|^{k}+1), \]

where \(_{}(P\|Q) 0\) and equals \(0\), if and only if \(p{=}q\), and \(k\) denotes a smoothing parameter. Here, the upper-bound of the divergence apparently exists, such that \(_{}(P\|Q)_{i=1}^{n}p_{i}_{2}2{=}1\), due to the following condition \(|p_{i}-q_{i}| 1\).

We define the dynamic restriction \(_{t}\) as \(1{-}_{}(P_{t}^{v}\|P_{t}^{d})\), where it enables a token-wise feedback control that adjusts the information weighting with respect to the closeness of the two distributions. The major role of the restriction term is maintaining a balance in the logit variation for the observed prediction disparities between the \(v\) and \(d\) distributions. That is, when the distributions are close enough (_i.e.,_\(P_{t}^{v} P_{t}^{d}\)), the value of \(_{}(P_{t}^{v}\|P_{t}^{d})\) approaches zero, indicating minimal divergence. That is, \(_{t}\) approaches 1, allowing for higher amplification of logit variations in predicting the next-token outputs. This adjustment reflects the increased reliability of predictions when the two distributions from \(v\) and \(d\) are closely aligned. On the other hand, for the dissimilar distributions, \(_{t}\) decreases towards zero, compelling the model to restrict information flow from the variation. This reduction limits the potential for introducing erroneous or less probable predictions by focusing more on visual information, thereby maintaining coherence in the output when the model's understanding of the visual content significantly deviates from its textual description.

### Adaptive Information Constraint

One major challenge in contrastive-based decoding is the scenario where implausible tokens are rewarded, even when predictions are made with low confidence. This issue can also arise in our method, particularly when token distributions derived from textual descriptions provide more confidence than the visual content, ironically undermining the most predictive tokens. To address it, Li _et al._ have introduced an adaptive plausibility constraint, which filters out less plausible tokens by truncating them based on the maximum token confidence from the expert model. While this approach simply penalizes false positive tokens in the candidate pool, it may also have unintended side effects by prematurely applying a cutoff threshold to lower-confidence tokens. Specifically, early threshold settings can sometimes eliminate the possibility of identifying correct token predictions, which might otherwise be dismissed in a pool considered to contain mostly false negatives.

Improving the previous constraint [34; 30], we present adaptive information constraint (\(_{}\)) designed to dynamically retain tokens that may be informative despite their lower confidence. By comparing prediction distributions between \(P_{t}^{v}\) and \(P_{t}^{d}\), we filter out less relevant tokens from the candidate pool as follows:

\[_{}(y_{<t})=\{y_{t}:p_{}(y_{t} v,x,y_{<t})_{t}_{w}p_{}(w v,x,y_{<t})\}, \]

where \(_{t}\) dynamically regulate the token candidate pool utilizing the divergence term in Eqn. 3, defined as \(_{t}=_{}(P_{t}^{v}\|P_{t}^{d})\). This strategy can expand the token searching pool when the next-token prediction, derived from both visual content and comprehensive description, shows a similar distribution yet uncertainty in selecting the candidate token (_i.e.,_ false negatives). Finally, we only consider the next-token prediction within \(_{}(y_{<t})\), and for the tokens satisfying \(y_{t}_{}(y_{<t})\), we set their logits to \(-\) to filter out from the candidate pool. Please see comprehensive Algorithm. 1 in Appendix. B.

## 4 Experiments

Experimental SetupTo validate the efficacy of our method over various LMM families and sizes, we implemented our method on contemporary LMMs: LLaVA-1.5 (13B) , Emu2-Chat (14B) , InternLM-XComposer2 (7B) , LLaVA-NeXT (34B) , Yi-VL (34B) , and InternVL 1.5 (26B) . We compared our method with five baseline decoding strategies. For the regular decoding strategies, we used greedy decoding, Nucleus sampling , and beam search decoding. Additionally, we selected OPERA  and VCD  for contrastive decoding method, which designed to mitigate hallucinations with contrastive frameworks. We used the default parameter settings for all methods, where top-p value \(0.95\) and temperature \(1.0\) for Nucleus sampling, the number of window size for searching is \(5\) (_i.e.,_ num-beams \(5\)) for both beam search decoding and OPERA, and CD-\(=1\), CD-\(=0.1\) for VCD, and \(k=0.3\) for our method. Note that OPERA inference requires too much memory especially for LLaVA-NeXT (34B), so that we excluded OPERA results for this model.

Benchmarks and Evaluation MetricsThe benchmarks for evaluating hallucinations in LMMs can be broadly categorized into discriminative and generative streams. The discriminative type assesses hallucinations by evaluating the predicted answer among given options (_e.g.,_ multiple choice or yes/no question), while generative benchmarks typically employ more advanced language models (_e.g.,_ GPT-aided evaluation) to rate the subject model descriptions. Within this taxonomy, we carefully select \(6\) benchmarks to test baselines. Please see Appendix. C for benchmark details.

As discriminative benchmarks, we utilize mainly three datasets for detailed evaluation. Specifically, **POPE** is a commonly used benchmark for detecting object hallucination by converting object annotations sourced from MSCOCO . Under the three different subsets: random, popular, and adversarial, the metric for POPE measures binary classification performance for simple yes/no questions. **MMVP** aims to evaluate the understanding of visual details for \(9\) different visual patterns using paired classification accuracy. Due to its evaluation design, which involves comparing two similar CLIP-blind image pairs, MMVP requires LMMs to capture subtle visual differences. **RealworldQA** is the most recent dataset tailored to assess the capability of LMMs in basic real-world spatial understanding, using the accuracy metric within multiple-choice questions.

We use three benchmarks for generative benchmarks, extending the evaluation scope to include open-ended captioning tasks beyond merely assessing classification within given answer options. Generally, ChatGPT  is used to score the quality of the model-generated sentences. The metric for both **LLaVA-QA90** and **LLaVA-Bench (In-the-Wild)** is score ratio, where model responses rated from GPT-4  are divided by GPT-4 answers such that \(||/||\), where all scores are rated by GPT-4. It has three types of questions: conversation, detailed description, and complex reasoning. **MMHal-Bench** evaluates the degree of hallucination for the \(8\) various question types: object attribute, adversarial object, comparison, counting, spatial relation,

Figure 3: Overview of experimental results on \(6\) baseline LMMs, \(6\) decoding method, and \(6\) hallucination benchmarks in spider chart format.

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

Computational AnalysisWe utilize comprehensive descriptions from models as additional information for contrasting with visual contents, thereby leading to computational loads similar to other decoding methods. To analyze the computation, we compare the token throughput (token/s) and decoding latency (ms/token) with other CD-based methods on \(8\) NVIDIA RTX A6000 GPUs as in Table. 4.

Token-level Case StudyAs illustrated in Fig. 4, to verify whether the proposed CODE effectively mitigates object hallucination, we analyze the output logit values of LMM  at the token-level case study, with a greedy search as the baseline. The first two rows in the table indicate the original greedy decoded tokens which are elected based on high \(_{v}\) from the visual content and CODE output tokens, respectively. As in the figure, we can observe that visual hallucination occurs at the _"Yoplait"_ token highlighted in red. For relatively easy tokens at the beginning of sentence, \(_{}\) produces identical decisions maintaining consistency with \(_{v}\), which indicates the amplification of logit variation is effectively adjusted due to similar prediction distributions from visual contents and description-only information. However, at the hallucination-occurred time step, logit scores are deviated between the two information, resulting in a more confusing state to identify between GT token _"Fage"_ and hallucinatory _"Yoplait"_. In our framework, _"Fage"_ is relatively more amplified from \(15.02\) to \(16.66\) than _"Yoplait"_, which changes from \(15.34\) to \(15.30\). By simultaneously considering both token-level and distributional prediction over the vocabulary, CODE changes the wrong next-token output to correct one, mitigating hallucination. For more case studies, please refer Appendix. D.

Additional Experiments on In-the-WildContemporary open-sourced LMMs are fine-tuned with various combinations of vision-language datasets , mostly composed of COCO-sourced visual images  and their curated instruction. Although the existing hallucination benchmarks intentionally convert question queries to assess model robustness against inconsistency, the visual contents in benchmarks are limited to in-distribution COCO images. To validate our method in more challenging and real-world scenarios, we compared baselines on LLaVA-Bench (In-the-wild)  and RealworldQA  as in Fig. 5 and achieved competent performance (case studies in Appendix. F).

    &  \\   & VCD & OPERA & CODE & VCD & OPERA & CODE \\  TB  & 5.62 & 1.23 & 3.66 & 177.99 & 809.73 & 272.92 \\
14B  & 4.04 & 1.04 & 2.82 & 247.6 & 960.14 & 354.09 \\
34B  & 3.61 & oom & 2.81 & 277.27 & oom & 355.81 \\   

Table 4: Computational analysis on decoding throughput and latency among CD-based methods. We compare three different model sizes.

Figure 4: An example of token-level case study for CODE. Each row indicates the logit score from visual content \(_{v}\), comprehensive description \(_{d}\), CODE applied \(_{}\), respectively.

Figure 5: Additional experiments on In-the-Wild benchmarks. Note that, unlike other datasets, OPERA  fails to generate consistent responses in real-world datasets using Yi-VL .

## 5 Discussion and Limitation

Albeit the computational analysis in Table. 4, as one of limitations, our contrastive decoding method requires additional computational resources than the use of vanilla decoding. However, considering an essential ongoing research topics and developments [60; 15] aimed at mitigating the negative effects of hallucination problems in both LLMs and LMMs, our work contributes important societal impacts towards more real-world applicability and robust AI system.

## 6 Conclusion

We present COuntering DEscription Contrastive Decoding (CODE), a novel and training-free decoding method to mitigate hallucination in Large Multi-modal Models. By utilizing self-generated descriptions as corrective references during the decoding phase, CODE dynamically adjusts the information flow for next-token predictions, enhancing the coherence and informativeness of responses while reducing the cross-modal inconsistency. Extensive experiments demonstrate that CODE effectively decreases hallucinations across various benchmarks and contemporary LMMs, significantly improving contextual relevance and response alignment with visual contents.