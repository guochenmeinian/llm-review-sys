# Reverse Transition Kernel: A Flexible Framework to Accelerate Diffusion Inference

Xunpeng Huang

HKUST

xhuangck@connect.ust.hk

&Difan Zou

HKU

dzou@cs.hku.hk

&Hanze Dong

Salesforce AI Research

hanze.dong@salesforce.com

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yian Ma

UC San Diego

yianma@ucsd.edu

&Tong Zhang

UIUC

tongzhang@tongzhang-ml.org

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yian Ma

UC San Diego

yianma@ucsd.edu

&Tong Zhang

UIUC

tongzhang@tongzhang-ml.org

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yian Ma

UC San Diego

yianma@ucsd.edu

&Tong Zhang

UIUC

tongzhang@tongzhang-ml.org

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yian Ma

UC San Diego

yianma@ucsd.edu

&Tong Zhang

UIUC

tongzhang@tongzhang-ml.org

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yian Ma

UC San Diego

yianma@ucsd.edu

&Tong Zhang

UIUC

tongzhang@tongzhang-ml.org

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yian Ma

UC San Diego

yianma@ucsd.edu

&Tong Zhang

UIUC

tongzhang@tongzhang-ml.org

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yianma@ucsd.edu

&Tong Zhang

UIUC

tongzhang@tongzhang-ml.org

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yianma@ucsd.edu

&Tong Zhang

UIUC

tongzhang@tongzhang-ml.org

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

UXi Zhang

yianma@ucsd.edu

&Tong Zhang

UIUC

tongzhang@tongzhang-ml.org

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

UXi Zhang

UXi Zhang

HKU

yianma@ucsd.edu
&Tong Zhang

UIUC

tongzhang@tongzhang-ml.org

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

[MISSING_PAGE_POST]

a series of Gaussian transitions, DDPM successfully generates high-quality samples that follow the data distribution. The empirical success of DDPM has immediately triggered various follow-up work , aiming to accelerate the inference process and improve the generation quality. Alongside rapid empirical research on diffusion models and DDPM-like sampling algorithms , theoretical studies have emerged to analyze the convergence and sampling error of DDPM. In particular,  have established polynomial convergence bounds, in terms of dimension \(d\) and target sampling error \(\), for the generation process under various assumptions. Previous work usually assume the score estimation error to construct the sampling analysis . A typical bound under minimal data assumptions on the score of the data distribution is provided by , which establishes an \(}(d^{-2})\) score estimation guarantees to sample from data distribution within \(\)-sampling error in the Total Variation (TV) distance.

In essence, the denoising diffusion process can be approached through various decompositions of sampling subproblems, where the overall complexity depends on the number of these subproblems multiplied by the complexity of solving each one. Within this framework, DDPM can be regarded as a specific solver for the denoising diffusion process that heavily prioritizes the simplicity of subproblems over their quantity. In particular, it adopts simple one-step Gaussian approximations for the subproblems, with \((1)\) computation complexity, but needs to deal with a relatively large number--approximately \(O(d^{-2})\)--of target subproblems to ensure the cumulative sampling error is bounded by \(\) in TV distance. This imbalance raises the question of whether the DDPM-like approaches stand as the most efficient algorithm, considering the extensive potential subproblem decompositions of the denoising diffusion process. We therefore aim to:

_accelerate the inference of diffusion models via a more balanced subproblem decomposition in the denoising process._

In this work, we propose a novel framework called reverse transition kernel (RTK) to achieve exactly that. Our approach considers a generalized subproblem decomposition of the denoising process, where the difficulty of each sampling subproblem and the total number of subproblems are determined by the step size parameter \(\). Unlike DDPM, which requires setting \(=^{2}\), resulting in approximately \(}(1/)=}(1/^{2})\) subproblems, our framework allows \(\) to be feasible in a broader range. Furthermore, we demonstrate that a more balanced subproblem decomposition can be attained by carefully selecting \(=(1)\) as a constant, resulting in approximately \(}(1)\) sampling subproblems, with each target distribution being strongly log-concave. This nice property further enables us to efficiently solve the sampling subproblems using well-established acceleration techniques, such as Metropolis Hasting step and underdamped discretization, without encountering many subproblems. Consequently, our proposed framework facilitates the design of provably faster algorithms than DDPM for performing diffusion inference. Our contributions can be summarized as follows.

* We propose a flexible framework that enhances the efficiency of diffusion inference by balancing the quantity and hardness of RTK sampling subproblems used to segment the entire denoising diffusion process. Specifically, we demonstrate that with a carefully designed decomposition, the number of sampling subproblems can be reduced to approximately \(}(1)\), while ensuring that all RTK targets exhibit strong log-concavity. This capability allows us to seamlessly integrate a range of well-established sampling acceleration techniques, thereby enabling highly efficient algorithms for diffusion inference.
* Building upon the developed framework, we implement the RTK using the Metropolis-Adjusted Langevin Algorithm (MALA), making it the first attempt to adapt this highly accurate sampler for diffusion inference. Under slightly stricter assumptions on the estimation errors of the energy difference and score function, we demonstrate that RTK-MALA can achieve linear convergence with respect to the sampling error \(\), specifically \(((1/))\), which significantly outperforms the \(}(1/^{2})\) convergence rate of DDPM . Additionally, we consider the practical diffusion model where only the score function is accessible and develop a score-only RTK-MALA algorithm. We further prove that the score-only RTK-MALA algorithm can achieve an error \(\) with a complexity of \(}(^{-2/(u-1)} 2^{u})\), where \(u\) can be an arbitrarily large constant, provided the energy function satisfies the \(u\)-th order smoothness condition.
* We further implement Underdamped Langevin Dynamics (ULD) within the RTK framework. The resulting RTK-ULD algorithm achieves a state-of-the-art complexity of \(}(d^{1/2}^{-1})\) for both and \(\) dependence under some mild assumptions as . Compared with the \(}(d^{-2})\) complexity guarantee for DDPM, it improves the complexity with an \(}(d^{1/2}^{-1})\) factor. This result also matches the state-of-the-art convergence rate of the ODE-based methods , though those methods require Lipschitz conditions for both the ground truth score function and the score neural network.

## 2 Preliminaries

In this section, we first introduce the notations used in subsequent sections. Then, we present several distinct Markov processes to demonstrate the procedures for adding noise to existing data and generating new data. Besides, we specify the assumptions required for the target distribution in our algorithms and analysis.

**Notations.** We say a complexity \(h\) to be \(h(n)=(n^{k})\) or \(h(n)=}(n^{k})\) if the complexity satisfies \(h(n) c n^{k}\) or \(h(n) c n^{k}[(n)]^{k^{}}\) for absolute contant \(c,k\) and \(k^{}\). We use the lowercase bold symbol \(\) to denote a random vector, and the lowercase italicized bold symbol \(\) represents a fixed vector. The standard Euclidean norm is denoted by \(\|\|\). The data distribution is presented as \(p_{*}(-f_{*})\). Besides, we define two Markov processes \(^{d}\), i.e.,

\[\{_{t}\}_{t[0,T]},\{_{k}^{ }\}_{k\{0,1,,K\}}, T=K.\]

In the above notations, \(T\) presents the mixing time required for the data distribution to converge to specific priors, \(K\) denotes the iteration number of the generation process, and \(\) signifies the corresponding step size. Further details of the two processes are provided below.

**Adding noise to data with the forward process.** The first Markov process \(\{_{t}\}\) corresponds to generating progressively noised data from \(p_{*}\). In most denoising diffusion models, \(\{_{t}\}\) is an Ornstein-Uhlenbeck (OU) process shown as follows

\[_{t}=-_{t}t+_ {t}_{0} p_{*}(-f_{*}). \]

If we denote underlying distribution of \(_{t}\) as \(p_{t}(-f_{t})\) meaning \(f_{0}=f_{*}\), the forward OU process provides an analytic form of the transition kernel, i.e.,

\[p_{t^{}|t}(^{}|)=,t}(^{ },)}{p_{t}()}=(2(1-e^{-2(t^{}-t)}) )^{-d/2}[^{}-e^{-(t^{}-t )}\|^{2}}{2(1-e^{-2(t^{}-t)})}] \]

for any \(t^{} t\), where \(p_{t^{},t}\) denotes the joint distribution of \((_{t^{}},_{t})\). According to the Fokker-Planck equation, we know the stationary distribution for SDE. (1) is the standard Gaussian distribution.

**Denoising generation with a reverse SDE.** Various theoretical works [21; 24; 8; 7; 3] based on DDPM  consider the generation process of diffusion models as the reverse process of SDE. (1) denoted as \(\{_{t}^{}\}\). According to the Doob's \(h\)-Transform, the reverse SDE, i.e., \(\{_{t}^{}\}\), follows from

\[_{t}^{}=(_{t}^{}+2  p_{T-t}(_{t}^{}))\,t+ _{t}, \]

whose underlying distribution \(p_{t}^{}\) satisfies \(p_{T-t}=p_{t}^{}\). Similar to the definition of transition kernel shown in Eq. (2), we define \(p_{t^{}|t}^{}(^{}|)=p_{t^{},t}^{ }(^{},)/p_{t}^{}()\) for any \(t^{} t 0\) and name it as reverse transition kernel (RTK).

To implement SDE. (3), diffusion models approximate the score function \( p_{t}\) with a parameterized neural network model, denoted by \(_{,t}\), where \(\) denotes the network parameters. Then, SDE. (3) can be practically implemented by

\[}_{t}=(}_{t}+2_{,T-k}(}_{k}))\,t+_{t} t[k,(k+1)) \]

with a standard Gaussian initialization, \(}_{0}(,)\). Eq. (4) has the following closed solution

\[}_{(k+1)}=e^{}}_{k}-2( 1-e^{})_{,T-k}(}_{k})+-1}(,), \]

which is exactly the DDPM algorithm.

[MISSING_PAGE_FAIL:4]

generation process of diffusion models with underlying distributions \(\{_{k}\}\), we require \(_{0}=(,)\) and \(_{K} p_{*}\), which is similar to the Markov process \(\{_{k}^{}\}\). In order to make the underlying distribution of output particles close to the data distribution, we can generate \(}_{k}\) with Alg. 1, which is equivalent to the following steps:

* Initialize \(}_{0}\) with an easy-to-sample distribution, e.g., \((,)\), which is close to \(p_{K}\).
* Update particles by drawing samples from \(_{(k+1)|k}(|}_{k})\), which satisfies \[_{(k+1)|k}(|}_{k}) p_{(k+1)| k}^{}(|}_{k}).\] Under these conditions, if \(_{k}() p_{(K-k)}()\), then we have \[_{(k+1)}()=_{(k+1)|k}(| ),_{k}() p_{(k+1)|k}^{ }(|),p_{k}^{}()=p_{(k+1) }()\] for any \(k\{0,1,,K\}\). This means we can implement the generation of diffusion models by solving a series of sampling subproblems with target distributions \(p_{(k+1)|k}^{}(|}_{k})\).

```
1:Input: Initial particle \(}_{0}\) sampled from the standard Gaussian distribution, Iteration number \(K\), Step size \(\), required convergence accuracy \(\);
2:for\(k=0\) to \(K-1\)do
3: Draw sample \(}_{(k+1)}\) with MCMCs from \(_{(k+1)|k}(|}_{k})\) which approximates the ground-truth reverse transition kernel, i.e., \[p_{(k+1)|k}^{}(|}_{k}) (-g())(-f_{(K-k-1)}()-}_{k}- e^{-}\|^{2}}{2(1-e^{-2})}).\]
4:return\(}_{K}\).
```

**Algorithm 1** Inference with Reverse Transition Kernel (RTK)

The close form of reverse transition kernels. To implement Alg. 1, the most critical problem is determining the analytic form of RTK \(p_{t^{}|t}^{}(^{}|)\) for and \(t^{} t 0\) which is shown in the following lemma whose proof is deferred to Appendix B.

**Lemma 3.1**.: _Suppose a Markov process \(\{_{t}\}\) with SDE. 1, then for any \(t^{}>t\), we have_

\[p_{T-t|T-t^{}}^{}(|^{})=p_{t|t^{}}(|^{})(-f_{t}()-^{ }- e^{-(t^{}-t)}\|^{2}}{2(1-e^{-2(t^{}-t) })}).\]

The first critical property shown in this Lemma is that RTK \(p_{t|t^{}}\) is a perturbation of \(p_{t}\) with a \(l_{2}\) regularization. This means if the score of \(p_{t}\), i.e., \( f_{t}\), can be well-estimated, the score of RTK, i.e., \( p_{t|t^{}}\) can also be approximated with high accuracy. Moreover, in the diffusion model, \( f_{t}= p_{t}\) is exactly the score function at time \(t\), which is approximated by the score network function \(_{,t}()\), then

\[- p_{t|t^{}}(|^{})= f_{t}()+ -t)}-e^{-(t^{}-t)}^{}}{1-e^{-2(t ^{}-t)}}_{,t}()+-t)}-e^{-(t^{}-t)}^{}}{1-e^{-2(t^{}-t)}},\]

which can be directly calculated with a single query of \(_{,t}()\). The second critical property of RTK is that we can control the spectral information of its score by tuning the gap between \(t^{}\) and \(t\). Specifically, considering the target distribution, i.e., \(p_{(K-k-1)|(K-k)}\) for the \(k\)-th transition, the Hessian matrix of its energy function satisfies

\[-^{2} p_{(K-k-1)|(K-k)}=^{2}f_{(K-k-1)}()+ }{1-e^{-2}}.\]

According to Assumption **[A1]**, the Hessian \(^{2}f_{(K-k-1)}()=-^{2} p_{(K-k-1)}\) can be lower bounded by \(-L\), which implies that RTK \(p_{(K-k-1)|(K-k)}\) will be \(L\)-strongly log-concave and \(3L\)-smooth when the step size is set \(=1/2(1+1/2L)\). This further implies that the targetsof all subsampling problems in Alg. 1 will be strongly log-concave, which can be sampled very efficiently by various posterior sampling algorithms.

**Sufficient conditions for the convergence.** According to Pinsker's inequality and Eq. (7), we can obtain the following lemma that establishes the general error decomposition for Alg. 1.

**Lemma 3.2**.: _For Alg 1, we have_

\[(_{K},p_{*}) )d+\| f_{*}()\|^{2}} (-K)\] \[+_{k=0}^{K-1}_{} _{k}}[(_{(k+1)|k}(| }))\|p_{(k+1)|k}^{}(|}))}\]

_for any \(K_{+}\) and \(_{+}\)._

It is worth noting that the choice of \(\) represents a trade-off between the number of subproblems divided throughout the entire process and the difficulty of solving these subproblems. By considering the choice \(=1/2(1+1/2L)\), we can observe two points: (1) the sampling subproblems in Alg. 1 tend to be simple, as all RTK targets, presented in Lemma 3.1, can be provably strongly log-concave; (2) the total number of subproblems is \(K=T/=}(1)\), which is not large. Conversely, when considering a larger \(\) that satisfies \((1+1/L)\), the RTK target will no longer be guaranteed to be log-concave, resulting in high computational complexity, potentially even exponential in \(d\), when solving the corresponding sampling subproblems. On the other hand, if a much smaller step size \(=o(1)\) is considered, the target distribution of the sampling subproblems can be easily solved, even with a one-step Gaussian transition. However, this will increase the total number of sampling subproblems, potentially leading to higher computational complexity.

Therefore, we will consider the setup \(=1/2(1+1/2L)\) in the remaining part of this paper. Now, the remaining task, which will be discussed in the next section, would be designing and analyzing the sampling algorithms for implementing all iterations of Alg. 1, i.e., solving the subproblems of RTK.

## 4 Implementation of RTK inner loops

In this section, we outline the implementation of Step 3 in the RTK algorithm, which aims to solve the sampling subproblems with strong log-concave targets, i.e., \(p_{(k+1)|k}^{}(|}_{k})(-g)\). Specifically, we employ two MCMC algorithms, i.e., the Metropolis-adjusted Langevin algorithm (MALA) and underdamped Langevin dynamics (ULD). For each algorithm, we will first introduce the detailed implementation, combined with some explanation about notations and settings to describe the inner sampling process. After that, we will provide general convergence results and discuss them in several theoretical or practical settings. Besides, we will also compare our complexity results with the previous ones when achieving the convergence of TV distance to show that the RTK framework indeed obtains a better complexity by balancing the number and complexity of sampling subproblems.

**RTK-MALA.** Alg. 2 presents a solution employing MALA for the inner loop. When it is used to solve the \(k\)-th sampling subproblem of Alg. 1, \(_{0}\) is equal to \(}_{k}\) defined in Section 3 and used to initialize particles iterating in Alg. 2. In Alg. 2, we consider the process \(\{_{s}\}_{s=0}^{S}\) whose underlying distribution is denoted as \(\{_{s}\}_{s=0}^{S}\). Although we expect \(_{S}\) to be close to the target distribution \(p_{(k+1)|k}^{}(|_{0})\), in real practice, the output particles \(_{S}\) can only approximately follow \(p_{(k+1)|k}^{}(|_{0})\) due to inevitable errors. Therefore, these errors should be explained in order to conduct a meaningful complexity analysis of the implementable algorithm. Specifically, Alg. 2 introduces two intrinsic errors:

* Estimation error of the score function: we assume a score estimator, e.g., a well-trained diffusion model, \(s_{}\), which can approximate the score function with an \(_{}\) error, i.e., \(\|s_{_{t}}()- p_{t}()\|_{}\) for all \(^{d}\) and \(t[0,T]\).
* Estimation error of the energy function difference: we assume an energy difference estimator \(r\) which can approximate energy difference with an \(_{}\) error, i.e., \(|r_{t}(^{},)+ p_{t}(^{})- p_{t}()| _{}\) for all \(,^{}^{d}\).

Under these settings, we provide a general convergence theorem for Alg. 2. To clearly convey the convergence properties, we only show an informal version.

**Theorem 4.1** (Informal version of Theorem C.17).: _Under Assumption , for Alg. 1, we choose_

\[= K=4L)d+\| f_{*}()\|^{2}}{^{2}}\]

_and implement Step 3 of Alg. 1 with Alg. 2. Suppose the score **[E1]**, energy **[E2]** estimation errors and the inner step size \(\) satisfy_

\[_{}=( d^{-1/2}),_{}=(^{1/2}),=} (L^{-2}(d+m_{2}^{2}+Z^{2})^{-1}),\]

_and the hyperparameters, i.e., \(R\) and \(r\), are chosen properly. We have_

\[(_{K},p_{*})}( )+((L(d+m_{2}^{2})))(1-}{4} )^{S}+}( }}{})+}(}}{ ^{1/2}}) \]

_where \(\) is the Cheeger constant of a truncated inner target distribution \((-g())[(,R)]\) and \(Z\) denotes the maximal \(l_{2}\) norm of particles appearing in outer loops (Alg. 1)._

It should be noted that the choice of \(\) choice ensures the \(L\) strong log-concavity of target distribution \((-g())\), which means its Cheeger constant is also \(L\). Although the Cheeger constant \(\) in the second term of Eq. 9 corresponding to truncated \((-g())\) should also be near \(L\) intuitively, current techniques can only provide a loose lower bound at an \(()\)-level (proven in Corollary C.8). While in both cases above, the Cheeger constant is independent with \(\). Combining this fact with an \(\)-independent choice of inner step sizes \(\), the second term of Eq. 9 will converge linearly with respect to \(\). As for the diameter \(Z\) of particles used to upper bound \(\), though it may be unbounded in the standard implementation of Alg. 2, Lemma C.18 can upper bound it with \(}(L^{3/2}(d+m_{2}^{2})^{-1})\) under the projected version of Alg. 2.

Additionally, to require the final sampling error to satisfy \((_{K},p_{*})}()\), Eq. 9 shows that the score and energy difference estimation errors should be \(\)-dependent and sufficiently small, where \(_{}\) corresponding to the training loss can be well-controlled. However, obtaining a highly accurate energy difference estimation (requiring a small \(_{}\)) is hard with only diffusion models. To solve this problem, we can introduce a neural network energy estimator similar to  to construct \(r(^{},,t)\), which induces the following complexity describing the calls of the score estimation.

**Corollary 4.2** (Informal version of Corollary C.19).: _Suppose the estimation errors of score and energy difference satisfy_

\[_{}} _{}(d^{1/2}+m_{2} +Z)},\]_If we implement Alg. 1 with the projected version of Alg. 2 with the same hyperparameter settings as Theorem 4.1, it has \((_{K},p_{*})}()\) with an \((L^{4}^{-2}(d+m_{2}^{2})^{2}Z^{2}(d/ ))\) complexity._

Considering the loose bound for both \(\) and \(Z\), the complexity will be at most \(}(L^{5}(d+m_{2}^{2})^{6})\) which is the first linear convergence w.r.t. \(\) result for the diffusion inference process.

**Score-only RTK-MALA.** However, the parametric energy function may not always exist in real practice. We consider a more practical case where only the score estimation is accessible. In this case, we will make use of estimated score functions to approximate the energy difference, leading to the score-only RTK-MALA algorithm. In particular, recall that the energy difference function takes the following form:

\[g(^{})-g()=- p_{(K-k-1)}(^{})+_{0}-^{} e^{-}^{2}}{2(1-e^{ -2})}+ p_{(K-k-1)}()-_{0}- e ^{-}^{2}}{2(1-e^{-2})}.\]

Since the quadratic term can be obtained exactly, we only need to estimate the energy difference. Then let \(f()=- p_{(K-k-1)}()\) and denote \(h(t)=f((^{}-) t+)\), the energy difference \(g(^{})-g()\) can be reformulated as

\[h(1)-h(0)=_{i=1}(0)}{i!} h^{(i)}(t) ^{i}h(t)}{(t)^{i}},\]

where we perform the standard Taylor expansion at the point \(t=0\). Then, we only need the derives of \(h^{i}(0)\), which can be estimated using only the score function. For instance, the \(h^{(1)}(t)\) can be estimated with score estimations:

\[h^{(1)}(t)= f((^{}-) t+)(^{ }-)^{(1)}(t) s_{}((^{} -) t+)(^{}-).\]

Moreover, regarding the high-order derivatives, we can recursively perform the approximation: \(^{(i+1)}(0)=(^{(i)}( t)-^{(i)}(0))/ t\). Consider performing the approximation up to \(u\)-order derivatives, we can get the approximation of the energy difference:

\[r_{(K-k-1)}(^{},)_{i=1}^{u} ^{(i)}(0)}{i!}.\]

Then, the following corollary states the complexity of the score-only RTK-MALA algorithm.

**Corollary 4.3**.: _Suppose the estimation errors of the score satisfies \(_{}/(Ld^{1/2})\), and the log-likelihood function of \(p_{t}\) has a bounded \(u\)-order derivative, e.g., \(^{(u)}f() L\), we have a non-parametric estimation for log-likelihood to make we have \((_{K},p_{*})}()\) with a complexity shown as follows_

\[}(L^{4}^{-3}(d+m_{2}^{2})^{2}Z^{3} ^{-2/(u-1)} 2^{u}).\]

This result implies that if the energy function is infinite-order Lipschitz, we can nearly achieve any polynomial order convergence w.r.t. \(\) with the non-parametric energy difference estimation.

**RTK-ULD.** Alg. 3 presents a solution employing ULD for the inner loop, which can accelerate the convergence of the inner loop due to the better discretization of the ULD algorithm. When it is used to solve the \(k\)-th sampling subproblem of Alg. 1, \(_{0}\) is equal to \(}_{k}\) defined in Section 3 and used to initialize particles iterating in Alg. 2. Besides, the underlying distribution of noise sample pair is

\[(_{s}^{z},_{s}^{z})(,(-(1-e^{-}))+(1-e^{-2})&(1-2e^{- }+e^{-2})\\ (1-2e^{-}+e^{-2})&1-e^{-2 }).\]

In Alg. 3, we consider the process \(\{(}_{s},}_{s})\}_{s=0}^{S}\) whose underlying distribution is denoted as \(\{_{s}\}_{s=0}^{S}\). We expect the \(\)-marginal distribution of \(_{S}\) to be close to the target distribution presented in Eq. 8. Then, the complexity of RTK-ULD to achieve the convergence of TV distance is provided as follows, and the detailed proof is deferred to Theorem D.6. Besides, we compare our theoretical results with the previous in Table 1.

**Theorem 4.4**.: _Under Assumptions **[A1]-[A2]** and **[E1]**, for Alg. 1, we choose_

\[=1/2[(2L+1)/2L] K=4L[((1+L^{2})d+ \| f_{*}()\|^{2})^{2}^{-2}]\]

_and implement Step 3 of Alg. 1 with projected Alg. 3. For the \(k\)-th run of Alg. 3, we require Gaussian-type initialization and high-accurate score estimation, i.e.,_

\[_{0}=(_{0},e^{2}-1)(, {I})_{}=}( /).\]

_If we set the hyperparameters of inner loops as follows. the step size and the iteration number as_

\[ =( d^{-1/2}L^{-1/2}( [^{2}+\|_{0}\|^{2})}{^{2}}])^{- 1/2})\] \[S =(^{-1}d^{1/2}([ ^{2}+\|_{0}\|^{2})}{^{2}}])^{1/2} ).\]

_It can achieve \((_{K},p_{*})\) with an \(}(L^{2}d^{1/2}^{-1})\) gradient complexity._

## 5 Conclusion and Limitation

This paper presents an analysis of a modified version of diffusion models. Instead of focusing on the discretization of the reverse SDE, we propose a general RTK framework that can produce a large class of algorithms for diffusion inference, which is formulated as solving a sequence of RTK sampling subproblems. Given this framework, we develop two algorithms called RTK-MALA and RTK-ULD, which leverage MALA and ULD to solve the RTK sampling subproblems. We develop theoretical guarantees for these two algorithms under certain conditions on the score estimation, and demonstrate their faster convergence rate than prior works. Numerical experiments support our theory.

We would also like to point out several limitations and future work. One potential limitation of this work is the lack of large-scale experiments. The main focus of this paper is the theoretical

   Results & Algorithm & Assumptions & Complexity \\  Chen et al.  & DDPM (SDE-based) & **[A1],[A2],[E1]** & \(}(L^{2}d^{-2})\) \\  Chen et al.  & DPOM (ODE-based) & **[A1],[A2],[E1]**, and \(s_{}\) smoothness & \(}(L^{3}d^{-2})\) \\  Chen et al.  & DPUM (ODE-based) & **[A1],[A2],[E1]**, and \(s_{}\) smoothness & \(}(L^{2}d^{1/2}^{-1})\) \\  Li et al.  & ODE-based sampler & **[E1]** and estimation error of energy Hessian & \(}(d^{3}^{-1})\) \\  Corollary 4.2 & RTK-MALA & **[A1],[A2],[E1]**, and **[E2]** & \((L^{4}d^{2}(d/))\) \\  Theorem 4.4 & RTK-ULD (ours) & **[A1],[A2],[E1]** & \(}(L^{2}d^{1/2}^{-1})\) \\   

Table 1: Comparison with prior works for RTK-based methods. The complexity denotes the number of calls for the score estimation to achieve \((_{K},p_{*})}()\). \(d\) and \(\) mean the dimension and error tolerance. Compared with the state-of-the-art result, RTK-ULD achieves the best dependence for both \(d\) and \(\). Though RTK-MALA requires slightly stricter assumptions and worse dimension dependence, a linear convergence w.r.t. \(\) makes it suit high-accuracy sampling tasks.

understanding and rigorous analysis of the diffusion process. Implementing large-scale experiments requires GPU resources and practitioner support, which can be an interesting direction for future work. Besides, though we provided a score-only RTK-MALA algorithm, the \(}(1/)\) convergence rate can only be achieved by the RTK-MALA algorithm (Alg. 2). However, this faster algorithm requires a direct approximation of the energy difference, which is not accessible in the existing pretrained diffusion model. Developing practical energy difference approximation algorithms and incorporating them with Alg. 2 for diffusion inference are also very interesting future directions.