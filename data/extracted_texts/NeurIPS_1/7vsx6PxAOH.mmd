# All-in-One Image Coding for Joint Human-Machine Vision with Multi-Path Aggregation

Xu Zhang  Peiyao Guo  Ming Lu1  Zhan Ma

School of Electronic Science and Engineering

Nanjing University

{xu.zhang,peiyao}@smail.nju.edu.cn {minglu,mazhan}@nju.edu.cn

Corresponding author

###### Abstract

Image coding for multi-task applications, catering to both human perception and machine vision, has been extensively investigated. Existing methods often rely on multiple task-specific encoder-decoder pairs, leading to high overhead of parameter and bitrate usage, or face challenges in multi-objective optimization under a unified representation, failing to achieve both performance and efficiency. To this end, we propose Multi-Path Aggregation (MPA) integrated into existing coding models for joint human-machine vision, unifying the feature representation with an all-in-one architecture. MPA employs a predictor to allocate latent features among task-specific paths based on feature importance varied across tasks, maximizing the utility of shared features while preserving task-specific features for subsequent refinement. Leveraging feature correlations, we develop a two-stage optimization strategy to alleviate multi-task performance degradation. Upon the reuse of shared features, as low as 1.89% parameters are further augmented and fine-tuned for a specific task, which completely avoids extensive optimization of the entire model. Experimental results show that MPA achieves performance comparable to state-of-the-art methods in both task-specific and multi-objective optimization across human viewing and machine analysis tasks. Moreover, our all-in-one design supports seamless transitions between human- and machine-oriented reconstruction, enabling task-controllable interpretation without altering the unified model. Code is available at [https://github.com/NJUVISION/MPA](https://github.com/NJUVISION/MPA).

## 1 Introduction

_Coding for multi-task_ that satisfies both human perception and machine vision has been extensively explored over the past few years . The intuitive and simple solution to achieve optimal performance for various tasks involves defining distinct encoder-decoder pairs tailored to specific instances with multiple bitstreams (separate  or scalable ), which however incurs significant parameter overhead and inefficient bitrate consumption. As a result, alternative efforts  attempt to enhance the compression performance across multiple tasks by deriving _a unified and compact representation_ of input images. Typically, they rely on a generalized encoder for feature extraction and deploy diverse decoding models to support corresponding vision tasks. However, they still suffer from the parametric inefficiency of multiple dedicated decoders. Recent work has made some progress in designing _a unified compression model_ for multi-task, where the encoder generates a unified representation, and the decoder focuses on task-oriented reconstruction. Popular techniques include conditional generation  and residual prediction , enabling user-controllable modulation of decoded results through task-driven guidance. While achieving performance comparable to decoder-specific approaches, they solelyfocus on human-centric requirements, such as perception-oriented and fidelity-oriented aspects. More critically, the challenge of multi-objective optimization [62; 20], particularly in the context of multi-task learning [66; 63], persists in the unified paradigm for joint human-machine vision, offering the performance largely inferior to that optimized for each task independently [42; 7; 8]. The crux of the problem lies in the indiscriminate treatment of the unified representation across diverse tasks, which necessitates searching for an accurate Pareto front for transition between tasks. This requires the use of massive and complicated techniques to fit it [47; 58; 57; 75] and multi-objective optimization with variable weights [2; 32; 24; 41]. Its failure can lead to significant performance degradation. And as the number of tasks increases, the difficulty of optimal search grows dramatically [15; 31; 23]. So far, few approaches consider the correlation of features across tasks to ensure efficient multi-task collaboration while also optimizing for the specificity of distinct tasks.

To overcome the challenges above, we propose a unified image coding method with Multi-Path Aggregation (MPA) for joint human-machine vision tasks. Specifically, a set of Multi-Layer Perceptron (MLP) branches is inserted to form multiple aggregation paths, replacing the single-path MLP in the feature transform blocks [53; 54] which currently dominate compression models [56; 18; 55; 84; 50]. Each path is customized and tailored to different tasks with varying complexities. Considering diverse importance of features across different tasks, we devise a predictor for importance scores to allocate latent features among task-specific paths based on their importance. Leveraging feature correlations across tasks, we develop an efficient two-stage optimization strategy with fine-tuning partial parameters on generalized features to alleviate multi-task performance degradation, avoiding extensive optimization of the entire model. This strategy significantly eases the optimization of multi-task coding while maintaining performance comparable to other fully optimized models. Considering any viewing task for human vision (e.g., low distortion or high realism) or analysis task for machine vision (e.g., high-level or low-level vision task), MPA can switch flexibly between them with seamless transitions, enabling a single representation to be interpreted in different ways within a unified model.

Our contributions are summarized as follows:

* Considering different importance of features for different tasks, we propose MPA to non-uniformly treat features. By developing an importance score predictor, MPA can allocate generalized latent features among task-specific paths based on their varying importance. This enables a _unified model_ to support multi-task coding with seamless transitions in an all-in-one manner.
* Leveraging feature correlations across tasks, we propose a two-stage optimization strategy with fine-tuning partial parameters on generalized features to overcome the challenge of multi-objective optimization in multi-task coding. This strategy allows MPA to be easily extended to support new tasks without independent optimization of separate task-specific models.
* Extensive experiments demonstrate that using the unified model, MPA achieves rate-distortion (R-D) and rate-perception (R-P) performance for human vision comparable to other state-of-the-art (SOTA) models and significantly improves accuracy for analysis tasks close to the fully fine-tuned ones, outperforming other separately optimized methods.

## 2 Related work

**Separate pairs.** Developing multiple task-specific encoder-decoder pairs, as shown in Fig. 0(a), is easy to optimize and beneficial to high performance but has significant parameter and bitrate

Figure 1: Paradigm comparison for multi-task coding.

overhead . Song et al.  introduced Spatially-adaptive Feature Transform (SFT) for tuning bit allocation, incurring significant latency for optimizing quality map for each image to achieve best task performance. Chen et al.  leveraged visual prompt tuning (VPT)  for task-specific optimization, resulting in quadratic complexity due to the self-attention mechanism with respect to the number of VPT tokens. Scalable coding  improved bitstream efficiency by embedding multiple bitstreams in a scalable manner. However, organizing representations for various tasks in a layered manner without introducing redundancy is challenging.

**Unified representation.** Some efforts  aimed to derive a unified and compact representation of input images but developed separate decoders (Fig. 1b) to ease optimization challenges. Feng et al.  compressed intermediate features from a vision backbone as a generic representation, while Duan et al.  proposed a plug-and-play adapter to bridge compressed representations with existing vision backbones, both requiring optimization of the backbone for optimal performance. Li and Zhang  used a semantic enhancement network to improve analysis accuracy without jointly optimizing the vision backbone but still faced parameter overhead due to multiple decoders.

**Unified model.** The paradigm in Fig. 1c uses a single encoder to generate a unified representation and a single decoder for task-specific reconstructions . Gao et al.  optimized the unified model for multiple analysis tasks jointly without transitions, resulting in trade-offs and suboptimal performance. Ghouse et al.  and Korber et al.  reconstructed images with a basic model and used additional modules to predict and add residuals for task-specific goals, while Agustsson et al.  and Iwai et al.  developed hyper-parameter conditioning modules to modulate the transition process. These methods require variable weighted objectives during optimization processes, making it challenging to extend to more tasks.

## 3 A unified framework: multi-path aggregation

### Preliminaries

Typically, the LIC model involves an encoder network \(E()\) that maps the input image \(\) to a compact latent representation \(}\), and a decoder network \(G(})\) that reconstructs \(}\) as an approximation to \(\). For the best compression performance, the optimization objective minimizes the distortion between the original and reconstructed images while reducing the expected bitrate \(r(})\), using \(_{_{}}[-_{2}p_{}}(})]\) characterized by entropy model \(P\). To achieve perceptually pleasing reconstruction, a Generative Adversarial Network (GAN) strategy is often incorporated, where a conditional discriminator \(D(,)\) is used to assist in model optimization. In our work, the MPA is implemented in TinyLIC  to validate its functionality for joint human-machine vision coding, while the GAN strategy in HiFiC  is used for guiding perceptual optimization. Furthermore, to support continuously variable-rate coding, we introduce the scaling factor (SF) modulation \(_{q}\)

Figure 2: The proposed Multi-Path Aggregation (MPA). Compared to typical Vision Transformer (ViT) block , original MLP is replaced with MPA. LN and SF are Layer Normalization and Scaling Factors ., ⃝⃝, ⃝S⃝ and ⃝A⃝ denote predictor, split and aggregation respectively. \(C\) represents the number of input channels. \(\) is the ratio \(_{}\) in the encoder or the ratio \(_{}\) in the decoder.

and non-linear interpolation [13; 43] in each Multi-Path Block (MP-Block in Fig. 2). The compression quality \(q[1,Q_{}]\) matching in the encoder and decoder determines the bitrate consumption, while the task orientation \(\) in the decoder controls the continuously transitions between tasks.

### Multi-path aggregation

To support various task optimizations flexibly with the unified model, we develop an efficient architecture named MPA. As illustrated in Fig. 2, the proposed method contains two key modules, multiple paths with MLP layers of varying complexity, and a predictor for binarized mask \(\). Different from recent advances in Mixture-of-Experts (MoE) [17; 21], the paths are divided into two groups: the main and the side paths. The main path captures generalized features for various tasks, while the side paths are fine-tuned based on the generalized features. The input features are allocated into different paths on the basis of \(\). The features corresponding to "1" in \(\) will enter the main path, while the remaining features will enter the side paths. Unlike existing methods, which apply consistent transformations [24; 41; 2; 32] to all features, MPA only exploits a few parameters in the side path to refine the generalized features for task transitions.

On the encoder side, we set the original MLP in ViT block as the main (high-quality) path \(_{}\) adapted to high bitrate features and add a bottleneck MLP as the side (low-quality) path \(_{}\) specific to low bitrate coding. For task-controllable LIC, the features optimized for perceptual loss are the most generalized ones . Therefore, on the decoder side, the main path is the Perc. (Perceptual) Path \(_{}()\), while the side paths contain the MSE path \(_{}()\), the Cls. (Classification) Path \(_{}()\), and the Seg. (Segmentation) Path \(_{}()\), standing for four representative tasks to showcase the versatility of MPA, i.e., high-realism, low-distortion, high-level visual, and low-level visual image reconstructions. Setting the task index \(i_{}\) by the user, MPA can realize the aggregation of the main path and one of the side path. To be efficient, we configure each path for a different complexity depending on the characteristics of tasks. Specifically, the paths for human vision (i.e., \(_{}()\) and \(_{}()\)) are designed as an inverted (inv.) bottleneck MLP  to achieve higher realism and lower distortion, while the paths for machine vision (i.e., \(_{}()\) and \(_{}()\)) was designed as a bottleneck MLP to reduce inference latency. Having an input feature \(^{H W C}\), \(i_{}\) and a mask \(\{0,1\}^{H W 1}\), MPA processes as Alg. 1. In our implementation, we integrate MPA into the 1st, 2nd and 3rd stages in both the encoder and decoder, without the 4th stage since its spatial size is too small to produce a smooth transition.

### Importance score predictor

For the portability of MPA, we introduce a lightweight importance score predictor as illustrated in Fig. 2 (denoted as \(}}\)) to generate mask and allocate features to each path. In addition to only three parametric linear layers for fast point-wise computation, we use a non-parametric partial average layer \(()\) to capture multi-scale information. Given intermediate feature \(^{H W C^{}}\), \(()\) performs global average pooling on the latter \(C^{}/2\) channels of intermediate feature followed by expansion to the spatial size of feature, which aggregates information from all features as global information. The other \(C^{}/2\) channels are left unchanged as local information. Formally, the partial average operation can be formulated as

\[((h,w,c))=^{H}_ {j=1}^{W}(i,j,c)}{H W},&c>C^{}/2,\\ (h,w,c),&. \]

A hyper parameter (\(q\) or \(\)) is leveraged to control the feature allocation between the main and side paths, which indicates the performance transition between tasks. In the encoder, the aggregation of \(_{}\) and \(_{}\) is associated with bitrate, thus we approximate a linear relationship between bitrate and aggregation ratio \(_{}\) following  by an inverse log transformation

\[_{}=(q)=}-1)}-1}{-1},\;\;q[1,Q_{}], \]where \(\) is the base of the log transformation and \(Q_{}\) represents the maximum value of the range of \(q\). Thus, as \(q\) increases, more features will enter \(_{}()\) for supporting high-quality reconstruction. The aggregation in the decoder is irrelevant to bitrate, and \(_{}=1-\) so that as \(\) increases, more features will enter the side paths. For efficiency, the mask is only generated once at the beginning of a stage and shared by all MP-blocks throughout the stage.

During training, since the direct sampling of \(\) based on \(\) from the importance score is non-differentiable, we first bias the output \(^{}\) of the last \(()\) in the predictor using a set of learnable parameters \(b\) to get shifted logits corresponding to each discrete \(q\) or \(\), and then use Gumbel-Sigmoid [35; 60] with threshold \(=0.5\) to soften the sampling process as Eq. (3). After training, the predictor can binarize the score map according to \(\), and the bias layer in Fig. 2 will be discarded.

\[=(^{}+b,). \]

## 4 Optimization strategy

### Stage 1: training a generalized basic model

Since the perceptually optimized LIC model is well aligned with both human and machine perception, we first train a variable-rate model based on TinyLIC  and GAN method from HiFiC  for subsequent extension. We add SF in all ViT blocks and implement MPA in the encoder. Following the common practice in literatures [61; 2; 32], the optimized losses are Eqs. (6) and (7) for the joint optimization of the encoder \(E\), decoder \(G\), entropy model \(P\) and discriminator \(D\):

\[_{} =_{s=1}^{S}(_{}-W^{(s)}}_{h=1}^{H^{(s)}}_{w=1}^{W^{(s)}}^{(s)}(h,w))^{2}, \] \[_{G} =_{} p_{}}[-(D(},G( })))],\] (5) \[_{D} =_{} p_{}}[-(1-D(}, G(}))]+_{ p_{}}[- D(E(),)],\] (6) \[_{EGP} =_{ p_{}}[_{r}^{(q)}r(})+d(,})]+_{G}_{G}+_{} _{}+_{}_{}, \]

where \(\), \(}\), \(\) and \(}\) are the input image, reconstructed image, compressed latents before and after quantization, respectively. \(_{}\) is introduced to optimize the predictors in MP-Blocks which constrains the ratio of "1"s in \(\) at all encoder stages to the encoder ratio target \(_{}\). \(S\) is the number of stages applying MPA, while \(H^{(s)}\) and \(W^{(s)}\) represent the spatial size at Stage \(s\). \(r()\) and \(d(,)\) represent the bitrate estimated by the entropy model \(P\) and \(0.01(,)\) (pixel values are scaled to \(\)). \(_{r}^{(q)}\) in Eq. (7) varies according to the sampling of quality level \(q\) during training. Learned Perceptual Image Patch Similarity (LPIPS)  is chosen for \(_{}\).

### Stage 2: extending and optimizing side paths for multi-task

Obtaining a generalized model, we can add side paths to the decoder to achieve task-controllable image coding. In this training stage, there is no need to train the model entirely, but only to optimize the added parameters, i.e., the added side path \(_{}\) and the corresponding predictor \(_{}\). By sampling \(\) during training, the main path and the added side path are randomly aggregated to fit the transition between different tasks. The training objective is simplified to

\[(_{}^{*},_{}^{*})=*{arg\,min}_{ _{},_{}}_{ p_{}}[_ {r}^{(q)}r(})]+_{}_{}+ _{}_{}, \]

where \(_{}^{*}\) and \(_{}^{*}\) are the optimal parameters of the added side path and predictor, \(_{}\) represents the task loss, and the target ratio in \(_{}\) is changed to decoder ratio \(_{}\). The options for task loss are varied. When optimizing for MSE, \(_{}\) is just a simple MSE loss measured between \(\) and \(}\). When optimizing for a visual analysis task, \(_{}\) is a compound loss containing \(d(,})\), \(_{}\) and the full loss function of the task. Normalization should be applied if needed for augmentation. The task model is frozen during optimization. For instance, using the classification model \(()\) with \(()\), cross-entropy is used to compute loss between the classification result of \(}\) and the ground truth \(GT\):

\[_{}=(((})),GT)+_{ p_{}}[d(,})]+ _{}_{}. \]

## 5 Experiments

### Experimental settings

**Dataset.** When training for perceptual quality and MSE, we use a combined dataset including Flicker2W , DIV2K , and CLIC  training sets, about 23K images in total. ImageNet-1K  and ADE20K  are used to train \(_{}\) and \(_{}\), respectively. We evaluate the model on Kodak  and CLIC test set  for image compression, ImageNet validation set  for classification, and ADE20K validation set  for semantic segmentation.

**Training.** We set \(=5\) in Eq. (2), \(Q_{}=8\), \(_{r}^{(q)}=\{18,9.32,4.83,2.5,1.3,0.67,0.35,0.18\}\), \(_{G}=2.56\), \(_{}=4.26\), \(_{}=1\), and \(_{}=10\) for all training stages. \(q\) and \(\) are uniformly sampled from \(\{1,2,3,4,5,6,7,8\}\) and \(\{0,1,2,3,4,5,6,7\}/7\) respectively. The training steps of Stage 1 in Sec. 4.1 are set to 3M, the first half without \(_{G}\) and the last half with \(_{G}\). The training steps of Stage 2 are set to 500K. In each training stage, the initial learning rate is set to \(10^{-4}\), decayed to \(10^{-5}\) for the last 25% steps. When training \(_{}\) and \(_{}\), images are randomly cropped to 256\(\)256\(\)3, and a random horizontal/vertical flip is applied. ConvNeXt-Tiny  is used for training \(_{}\), with images randomly resized and cropped to 256\(\)256\(\)3 followed by a random horizontal flip. PSPNet50  is used for training \(_{}\), with images randomly resized, flipped and cropped to 256\(\)256\(\)3. The batch size is set to 8 for all tasks. Adam  is used for optimization.

**Evaluation.** For human vision evaluation, we use Peak Signal-to-Noise Ratio (PSNR) to measure distortion, and use Frechet Inception Distance (FID)  and LPIPS  to measure realism. We use the same protocol in  to calculate FID, i.e., cropping images to overlapped patches of size 256\(\)256\(\)3. Note that FID is not calculated on Kodak because it yields only 192 patches, which is not sufficient for measuring FID. For the classification task, we use top-1 accuracy (acc.) to present the performance, with images first resized to a short edge of 292 and then center-cropped to 256\(\)256\(\)3. For the segmentation task, we use mean Intersection over Union (mIoU) to present the performance, with images first resized to a short edge of 512 and then center-cropped to 512\(\)512\(\)3.

### Results of multi-task performance

For human vision, we compare the proposed MPA to the SOTA baselines of the unified model to evaluate R-D and R-P performance, i.e., MRIC , DIRAC , CRDR . In addition, we

Figure 3: Multi-task performance. The curves of variable-rate models are plotted as solid lines, while dashed lines are for single-rate models. Colored areas represent the adjustable range of MPA.

add HiFiC  and VTM17.1  for comparison as the anchors of fully perceptual optimization and traditional coding method, respectively. For machine vision, the classification task is the understanding of the global semantic information of an image and represents a high-level vision task. In contrast, the segmentation task challenges the model's ability to understand pixel-level semantics and represents a low-level vision task. Thus, we test the reconstructed images on classification and semantic segmentation tasks to comprehensively evaluate the performance of MPA for machine vision. We compare MPA to SOTA baselines with task-specific optimization, SFT  and TransTIC , along with VTM17.1 .

As shown in Fig. 3, MPA can achieve performance comparable to existing SOTA models. For human vision, the FID of MPA is lower than that of the best-performing model, CRDR, at a low bitrate. Since perceptual performance is more reflective of the human visual system at a low bitrate, this part of the gain is significantly beneficial. Considering the distortion, even with only partial parameters optimized for MSE, MPA still has a wide adjustable range comparable to other fully fine-tuned models, especially at a low bitrate. Note that we achieve such performance with a smaller decoder (9.295M for MPA vs. 13.38M for CRDR), fewer training steps (3.5M steps for MPA vs. 5M steps for CRDR), and partial fine-tuning (7.27% for MPA and 100% for CRDR). In terms of machine vision, \(_{}\), \(_{}\) and \(_{}\) perform significantly better than models optimized for MSE, outperforming SFT and TransTIC which are specially optimized for vision tasks. With only 1.89% parameters fine-tuned for machine vision, MPA can even achieve accuracy comparable to the fully fine-tuned models, showcasing its powerful task transition capability. Note that any point in the colored areas in Fig. 3 can be achieved by adjusting \(q\) and \(\), as MPA supports continuously variable-rate coding and seamless transitions between all tasks using a unified model.

**Discussion.** We choose the perceptually optimized path to serve as the main path due to its ability to preserve high-level semantic features that are useful across various tasks. This generalization is achieved by using pre-trained classification models for measurement (VGG  for training and AlexNet  for testing). This method aligns well with both human and machine perceptions in the latent space . The pre-trained VGG implicitly incorporates label information, making the distribution of the reconstructed image semantically closer to the original one, thus reducing

Figure 4: Visualization of the reconstructed images, FullGrad  and score maps. The image is from ImageNet  and resized to 256\(\)256\(\)3. The regions with warmer colors in FullGrad have larger gradients, indicating a stronger impact on the classification decision. The bitrate is 0.0888bpp.

perceptual distance and enhancing accuracy. Additionally, \(_{}\) and \(_{}\) use task-specific pre-trained models to compute cross-entropy loss (image-wise for classification and pixel-wise for segmentation), directly targeting task-specific accuracy. These constraints, along with semantic information from the first training stage, allow \(_{}\) and \(_{}\) to achieve higher accuracy than \(_{}\) with lower computational complexity. Moreover, perceptually optimized features can be easily adapted for MSE optimization, achieving higher PSNR. Perceptual loss preserves high-level semantic features, which supports the minimization of pixel-wise distortion when fine-tuned with MSE loss especially at a low bitrate . Thus, perceptual optimization not only unify human and machine vision tasks but also enhance performance in traditional metrics like PSNR, making it a suitable method for the generalized model.

### Visualization

To investigate the effects of different paths, we visualize the qualitative results in Figs. 4 and 5. MSE optimization prioritizes image textures, with fine-grained textures scoring higher, while analysis tasks focus on semantic regions. Score prediction differences at each stage highlight distinctions between MSE and machine vision perception. MSE-optimized features show significant score differences at lower-level stages (1st and 2nd), enhancing PSNR by improving textures. Conversely, machine vision-optimized features show significant score differences at all stages, indicating that both high- and low-level semantic features are crucial for analysis tasks. Consequently, images optimized for MSE without semantic supervision are visually smoother at low bitrate but may lose important semantic features, reducing accuracy. Paths optimized under LPIPS supervision retain more semantic information, ensuring the reconstructed image is semantically closer to the original one. By introducing task loss in Eq. (8), images decoded by \(_{}\) and \(_{}\) have more task-specific features for classification and segmentation respectively, leading to higher accuracy than \(_{}\).

### Diving into MPA

To further explore the configuration of the MPA, we perform a series of evaluations. Kodak , ImageNet-1K  and ADE20K  are used for evaluate BD-rate , top-1 accuracy and mIoU respectively. BD-rate is computed over the whole R-D curve. Top-1 accuracy and mIoU are computed

Figure 5: Visualization of the reconstructed images, segmentation maps and score maps. The image is from ADE20K  and resized to 512\(\)512\(\)3. The bitrate is 0.0718bpp.

at the lowest bitrate, i.e., 0.1521bpp on ImageNet-1K and 0.0948bpp on ADE20K. For the effects of complexity, Table 1 reveals that human vision-oriented tasks are more sensitive to the complexity of the paths, which suggests the use of inverted bottleneck MLPs, while machine vision-oriented paths can use bottleneck MLPs because the complexity has relatively less impact on the accuracy. As for the choice of paths, we conduct cross-validations as shown in Table 2, which demonstrates that the paths do learn the task-specific features to make their corresponding tasks perform optimally. To evaluate the effect of MPA in the encoder, we conduct ablations on each MPA component. We replace the predicted mask with a random mask and disable \(_{}\) and \(_{}\) during encoding. The results in Table 3 demonstrate that the predictors capture different features critical to high and low bitrate compression, and \(_{}\) and \(_{}\) are specialized to corresponding features respectively.

To evaluate the generalization of the learned features for the same task, we test \(_{}\) on Swin Transformer V2 . Fig. 6 shows that \(_{}\) has a similar performance gain comparable to that on ConvNeXt  which is originally used for training. Note that \(_{}\) is only trained once without fine-tuning on SwinV2, reflecting the generalization of the learned features in MPA.

The complexity of MPA is shown in Table 4. Here, the input size is 256\(\)256\(\)3. Latency is averaged over a batch of 16 images running 5000 times on a RTX3090 GPU through an encoder, a hyperprior entropy model, and a decoder. With the negligible computational overhead, MPA achieves comparable performance to the full fine-tuning model and has seamless transitions between tasks.

The main limitations of MPA are its increased latency compared to the baseline and the need for separate training for each task. The former can be mitigated by developing a specialized operator to eliminate the frequent flattening, selection, and reassembly operations, while the latter can be addressed by leveraging multi-task learning techniques , which will be explored in future work.

## 6 Conclusion

In this paper, we propose Multi-Path Aggregation (MPA) to tackle the problem of coding for multi-task applications in an all-in-one manner. By aggregating task-specific paths, MPA can support a variety of tasks with seamless transitions between them for joint human-machine vision using a unified model and a unified representation. Merely fine-tuning as low as 1.89% additional parameters, MPA can achieve comparable performance to that of separable models optimized using dedicated criteria, showcasing its strong versatility and scalability. Future work will explore joint optimization of multiple paths to further unify training process and improve multi-task performance.

    Components \\  } &  BD-Rate \(\) \\  } &  Models \\  } &  \#Param. \\  } &  KFLOPs per pixel \\  } &  \\   MRIC  & 69.14M & 1118.17 & 11.89 \\ TinyLIC  & 28.46M & 439.29 & 12.68 \\ + MLPs & +0.51M\(\)42.04M & +5.668\(\)0 & -0.33\(\)+0 \\ + Predictors & +0.03M & +2.23 & +0.09 \\ + \$ \& \& +0 & +0 & +0.28 \\  MPA & 29.00M\(\)30.53M & 384.84\(\)441.52 & 12.72\(\)13.05 \\   

Table 4: Comparison of complexity

Figure 6: Validation on SwinV2-T