# RanPAC: Random Projections and Pre-trained Models for Continual Learning

Mark D. McDonnell\({}^{1}\), Dong Gong\({}^{2}\), Amin Parveneh\({}^{1}\),

Ehsan Abbasnejad\({}^{1}\) and Anton van den Hengel\({}^{1}\)

\({}^{1}\)Australian Institute for Machine Learning, The University of Adelaide

\({}^{2}\)School of Computer Science and Engineering, University of New South Wales

mark.mcdonnell@adelaide.edu.au, dong.gong@unsw.edu.au,

amin.parvaneh@adelaide.edu.au, ehsan.abbasnejad@adelaide.edu.au,

anton.vandenhengel@adelaide.edu.au

###### Abstract

Continual learning (CL) aims to incrementally learn different tasks (such as classification) in a non-stationary data stream without forgetting old ones. Most CL works focus on tackling catastrophic forgetting under a learning-from-scratch paradigm. However, with the increasing prominence of foundation models, pre-trained models equipped with informative representations have become available for various downstream requirements. Several CL methods based on pre-trained models have been explored, either utilizing pre-extracted features directly (which makes bridging distribution gaps challenging) or incorporating adaptors (which may be subject to forgetting). In this paper, we propose a concise and effective approach for CL with pre-trained models. Given that forgetting occurs during parameter updating, we contemplate an alternative approach that exploits training-free random projectors and class-prototype accumulation, which thus bypasses the issue. Specifically, we inject a frozen Random Projection layer with nonlinear activation between the pre-trained model's feature representations and output head, which captures interactions between features with expanded dimensionality, providing enhanced linear separability for class-prototype-based CL. We also demonstrate the importance of decorrelating the class-prototypes to reduce the distribution disparity when using pre-trained representations. These techniques prove to be effective and circumvent the problem of forgetting for both class- and domain-incremental continual learning. Compared to previous methods applied to pre-trained ViT-B/16 models, we reduce final error rates by between 20% and 62% on seven class-incremental benchmark datasets, despite not using any rehearsal memory. We conclude that the full potential of pre-trained models for simple, effective, and fast continual learning has not hitherto been fully tapped. Code is available at [https://github.com/RanPAC/RanPAC](https://github.com/RanPAC/RanPAC).

## 1 Introduction

Continual Learning (CL) is the subfield of machine learning within which models must learn from a distribution of training samples and/or supervision signals that change over time (often divided into a distinct set of \(T\) episodes/tasks/stages) while remaining performant on anything learned previously during training . Traditional training methods do not work well for CL because parameter updates become biased to newer samples, overwriting what was learned previously. Moreover, training on sequential disjoint sets of data means there is no opportunity to learn differences between samples from different stages . These effects are often characterised as 'catastrophic forgetting' .

Although many methods for CL have been proposed , most focus on models that need to be trained from scratch, with resulting performance falling short of that achievable by non-CL alternatives on the same datasets. Although valid use cases for training from scratch will always exist, the new era of large foundation models has led to growing interest in combining CL with the advantages of powerful pre-trained models, namely the assumption that a strong generic feature-extractor can be adapted using fine-tuning to any number of downstream tasks.

Although forgetting still occurs in CL with such transfer-learning  (whether using conventional fine-tuning  or more-recent Parameter-Efficient Transfer Learning (PETL) methods such as ), commencing CL with a powerful feature-extractor has opened up new ideas for avoiding forgetting that are unlikely to work when training from scratch. Such new methods have been applied successfully to pre-trained transformer networks , CNNs  and multi-modal vision-language models .

Three main strategies are evident in these proposals (see Section 2) for details): (i) prompting of transformer networks; (ii) careful selective fine-tuning of a pre-trained model's parameters; and (iii) Class-Prototype (CP) accumulation. Common to all these strategies is the absence of a need for a buffer of rehearsal samples from past tasks, unlike the best CL methods for training models from scratch. Instead, these strategies leverage a pre-trained model's strong feature extraction capabilities.

Each strategy has yielded empirically comparable performance when used with the same benchmarks and pre-trained model (e.g. a ViT B/16 transformer network ). It therefore remains open as to what strategy best leverages pre-trained foundation models for CL, in terms of performance on diverse datasets and CL scenarios, simplicity, and efficiency. However, we note that the CP-based CL strategy is simple to apply to both CNNs and transformer networks, whereas prompting methods rely on a prepending learnable prompts to transformer network inputs. Fine-tuning a pre-trained model's parameters requires more resources for training than the other two strategies, while carrying a greater risk of cumulative forgetting over time, thus requiring use of additional CL methodologies.

In this paper, we show that the CP strategy has not come close to exhausting its capacity for accuracy, and can achieve standout performance with carefully tailored strategies to enhance the extracted feature representations from the pre-trained models. CP-based methods use only the prototypes obtained by averaging the extracted features to represent each class, subject to a discrepancy with the data distributions in practice. We propose to handle this via training-free frozen random projections and a decorrelation process, both of which bypass the forgetting issue. Specifically, we introduce a Random-Projection (RP) layer of frozen untrained weights, with nonlinear activation, between the pre-trained-model's feature layer, and a CP-based output head.

**Nonlinear Random Projection (RP) of Features:** The original RP idea (see Figure 1) has been proposed  and applied (e.g. ) multiple times in the past, as a standalone non-continual learner. Our two-fold motivation for using the same method for CL differs from the method's origins (outlined in Section 2). Firstly, we show that past CP strategies for CL with pre-trained models are equivalent to the linear models learned following RP in the past, e.g. . Secondly, the frozen untrained/training-free weights do not cause forgetting in CL. These two observations suggest using the RP idea alongside CP strategies for CL.

However, why should RPs provide any value instead of trying to learn a layer of fully-connected weights? It is well known that linear models can benefit from transformations that create non-linear feature interactions. Our hypothesis in proposing to use RP followed by nonlinear-activation for CL, therefore, is that the transformed set of features is more linearly separable using a CP method than features directly extracted from a pre-trained model. The past work of  leads us to expect that for this to be confirmed, RP will need to increase dimensionality from \(L\) to \(M>L\).

Figure 1: The RP method can lead to a representation space with clear class separation. Colored points are 2D t-SNE visualizations for CIFAR-100 classes with features from a pre-trained ViT-B/16 transformer network.

Our **contributions** are summarised as follows:

1. We examine in detail the CP strategy for CL with pre-trained networks, show that it benefits from injecting a Random Projection layer followed by nonlinear activation, and illustrate why. We also analyse why it is important to follow the lead of  to linearly transform CPs, via decorrelation using second-order feature statistics.
2. We show that random projections are particularly useful when also using PETL methods with first-session training (see Section 2.1). Accuracy with this combination approaches the CL joint training upper bound on some datasets (Table 2). For ViT-B/16 pre-trained models, we report the highest-to-date rehearsal-free CL accuracies on all class-incremental and domain-incremental datasets we tested on, with large margins when compared to past CP strategies.
3. We highlight the flexibility of our resulting CL algorithm, RanPAC; it works with arbitrary feature vectors (e.g. ViT, ResNet, CLIP), and is applicable to diverse CL scenarios including class-incremental (Section 5), domain-incremental (Section 5) and task-agnostic CL (Appendix F).

## 2 Related Work

### Three strategies for CL with strong pre-trained models

**Prompting of transformer networks**: Using a ViT-B/16 network , Learning To Prompt (L2P) , and DualPrompt  reported large improvements over the best CL methods that do not leverage pre-trained models, by training a small pool of prompts that update through the CL process. CODA-Prompt , S-Prompt  and PromptFusion  then built on these, showing improvements in performance.

**Careful fine-tuning of the backbone**: SLCA  found superior accuracy to prompt strategies by fine-tuning a ViT backbone with a lower learning rate than in a classifier head. However, it was found that use of softmax necessitated introduction of a 'classifier alignment' method, which incurs a high memory cost, in the form of a feature covariance matrix for every class. Another example of this strategy used selected fine-tuning of some ViT attention blocks , combined with traditional CL method, L2 parameter regularization. Fine-tuning was also applied to the CLIP vision-language model, combined with well-established CL method, LwF .

**Class-Prototype (CP) accumulation**: Subsequent to L2P, it was pointed out for CL image classifiers  that comparable performance can be achieved by appending a nearest class mean (NCM) classifier to a ViT model's feature outputs (see also ). This strategy can be significantly boosted by combining with Parameter-Efficient Transfer Learning (PETL) methods (originally proposed for NLP models in a non-CL context [17; 12]) trained only on the first CL stage ('first-session training') to bridge any domain gap [65; 37]. The three PETL methods considered by  for transformer networks, and the FiLM method used by  for CNNs have in common with the first strategy (prompting) that they require learning of new parameters, but avoid updating any parameters of the backbone pre-trained network. Importantly,  also showed that a simple NCM classifier is easily surpassed in accuracy by also accumulating the covariance matrix of embedding features, and learning a linear classifier head based on linear discriminant analysis (LDA) . The simple and computationally lightweight algorithm of  enables CL to proceed after the first session in a perfect manner relative to the union of all training episodes, with the possibility of catastrophic forgetting avoided entirely.

CPs are well suited to CL generally [42; 7; 31; 16] and for application to pre-trained models [20; 65; 37], because when the model from which feature vectors are extracted is frozen, CPs accumulated across \(T\) tasks will be identical regardless of the ordering of the tasks. Moreover, their memory cost is low compared with using a rehearsal buffer, the strategy integral to many CL methods .

### RP method for creating feature interactions

As mentioned, the original non-CL usage of a frozen RP layer followed by nonlinear projection as in [43; 4; 18] had different motivations to us, characterized by the following three properties. First, keeping weights frozen removes the computational cost of training them. Second, when combined with a linear output layer, the mean-square-error-optimal output weights can be learned by exact numerical computation using all training data simultaneously (see Appendix B.3) instead of iteratively. Third, nonlinearly activating random projections of randomly projected features is motivated by the assumption that nonlinear random interactions between features may be more linearly separable than the original features. Analysis of the special case of pair-wise interactions induced by nonlinearity can be found in , and mathematical properties for general nonlinearities (with higher order interactions) have also been discussed extensively, e.g. [4; 18].

## 3 Background

### Continual learning problem setup

We assume the usual supervised CL setup of a sequence of \(T\) tasks/stages, \(=\{_{1},_{T}\}\). In each \(_{t}\), a disjoint set of training data paired with their corresponding labels is provided for learning. Subsequent stages cannot access older data. We primarily consider both 'Class-Incremental Learning' (CIL) and 'Domain-Incremental learning' (DIL) protocols  for classification of images. In CIL, the class labels for each \(_{t}\) are disjoint. One of the challenging aspects of CIL is that, in contrast to Task-Incremental Learning (TIL), the task identity and, consequently, the class subset of each sample is unknown during CIL inference . In DIL, while all stages typically share the same set of classes, there is a distribution shift between samples appearing in each stage. For example, \(_{1}\) may include photographs and \(_{2}\) images of paintings.

We introduce \(K\) as the total number of classes considered within \(T\) tasks, and the number of training samples in each task as \(N_{t}\) with \(N:=_{t=1}^{T}N_{t}\). For the \(n\)-th unique training sample within task \(_{t}\), we use \(_{t,n}\) as its length \(K\) one-hot encoded label and \(_{t,n}^{L}\) to denote features extracted from a frozen pre-trained model. We denote by \(_{}\) the encoded features for a test instance for which we seek to predict labels.

### Class-Prototype strategies for CL with pre-trained models

For CL, using conventional cross-entropy loss by linear probing or fine-tuning the feature representations of a frozen pre-trained model creates risks of task-recency bias  and catastrophic forgetting. Benefiting from the high-quality representations of a pre-trained model, the most straightforward Class-Prototype (CP) strategy is to use Nearest Class Mean (NCM) classifiers [58; 34], as applied and investigated by [65; 20]. CPs for each class are usually constructed by averaging the extracted feature vectors over training samples within classes, which we denote for class \(y\) as \(}_{y}\). In inference, the class of a test sample is determined by finding the highest similarity between its representation and the set of CPs. For example,  use cosine similarity to find the predicted class for a test sample,

\[y_{}=*{arg\,max}_{y^{}\{1,,K\}}s_{y^{ }}, s_{y}:=_{}^{}}_{y}}{||_{}||||}_{ y}||}. \]

However, it is also not difficult to go beyond NCM within the same general CL strategy, by leveraging second-order feature statistics [11; 37]. For example,  finds consistently better CL results with pre-trained CNNs than NCM using an incremental version [36; 11] of Linear Discriminant Analysis (LDA) classification , in which the covariance matrix of the extracted features is continually updated. Under mild assumptions, LDA is equivalent to comparing feature vectors and class-prototypes using Mahalanobis distance (see Appendix B.4), i.e. different to cosine distance used by .

### CP-based classification using Gram matrix inversion

We will also use incrementally calculated second-order feature statistics, but create a simplification compared with LDA (see Appendix B.4), by using the Gram matrix of the features, \(\), and \(_{y}\) (CPs with averaging dropped), to obtain the predicted label

\[y_{}=*{arg\,max}_{y^{}\{1,,K\}}s_{y^{ }}, s_{y}:=_{}^{}^{-1}_{y}. \]

Like LDA, but unlike cosine similarity, this form makes use of a training set to 'calibrate' similarities. This objective has a basis in long established theory for least square error predictions of one-hot encoded class labels  (see Appendix B.3). Similar to incremental LDA [36; 11], during CL training, we describe in Section 4.3 how the Gram matrix and the CPs corresponding to \(_{y}\) can easily be updated progressively with each task. Note that Eqn. (2) is expressed in terms of the maximum number of classes after \(T\) tasks, \(K\). However, for CIL, it can be calculated after completion of tasks \(t<T\) with fewer classes than \(K\). For DIL, all \(K\) classes are often available in all tasks.

The proposed approach and theoretical insights: RanPAC

### Why second order statistics matter for CPs

We show in Section 5 that Eqn. (2) leads to better results than NCM. We attribute this to the fact that raw CPs are often highly correlated between classes, resulting in poorly calibrated cosine similarities, whereas the use of LDA or Eqn (2) mostly removes correlations between CPs, creating better separability between classes. To illustrate these insights, we use the example of a ViT-B/16 transformer model  pre-trained on ImageNet-21K with its classifier head removed, and data from the well-established 200-class Split Imagenet-R CIL benchmark .

For comparison with CP approaches, we jointly trained, on all 200 classes, a linear probe softmax classifier. We treat the weights of the joint probe as class-prototypes for this exercise and then find the Pearson correlation coefficients between each pair of prototypes as shown for the first 10 classes in Fig. 2 (right). Compared with the linear probe, very high off-diagonal correlations are clearly observed when using NCM, with a mean value more than twice that of the original ImageNet-1K training data treated in the same way. This illustrates the extent of the domain shift for the downstream dataset. However, these correlations are mostly removed when using Eqn (2). Fig. 2 (left) shows that high correlation coefficients coincide with poorly calibrated cosine similarities between class-prototypes and training samples, both for true class comparisons (similarities between a sample's feature vector and the CP for the class label corresponding to that sample.) and inter-class comparisons (similarities between a sample's feature vector and the class prototypes for the set of N-1 classes not equal to the sample's class label). However, when using Eqn. (2), the result (third row) is to increase the training-set accuracy from 64% to 75%, coinciding with reduced overlap between inter- and true-class similarity distributions, and significantly reduced off-diagonal correlation coefficients between CPs. The net result is linear classification weights that are much closer to those produced by the jointly-trained linear probe. These results are consistent with known mathematical relationships between Eqn (2) and decorrelation, which we outline in Appendix B.4.4.

### Detailed overview and intuition for Random Projections

CP methods that use raw \(_{y}\) for CL assume a Gaussian distribution with isotropic covariance. Empirically, we have found that when used with pre-trained models this assumption is invalid (Figure 2). One can learn a non-linear function (e.g. using SGD training of a neural network) with

Figure 2: Left: Histograms of similarities between class-prototypes and feature vectors from the ViT-B/16 transformer model pre-trained on ImageNet-1K, for the training samples of the Imagenet-R dataset. Right: Pearson correlation coefficients (CCs) for 10 pairs of class-prototypes. Reduced correlations between CPs of different classes (right), coincides with better class separability (left).

high capacity and non-convex objective to alleviate this gap (e.g., learning to adapt on the first task as in [65; 37]), but that requires additional assumptions and architectural choices. In addition for CL, any learning on all tasks carries a high risk of catastrophic forgetting. Instead, we take advantage of the observation that the objectives in CP strategies principally require firstly feature projections (that could be possibly adjusted for their distribution by a covariance matrix) and secondary the feature prototypes. For the latter, since using large pre-trained models produce features that capture most prominent patterns and attributes, the average is a reasonable expected class representative. However, the feature representations may also benefit from nonlinear transformation to enhance linear separability. In this paper, we consider two points: (i) we show a random projection of the features in the embedding space (or their projection to a random basis) to a higher dimensional space leads to a distribution that is more likely to be suitable for a Gaussian fit (e.g. using Eqn. (2)); and, (ii) when the distribution of representations is nearly Gaussian, we can simply perform a linear regression (or LDA or a conditional mean of the joint Gaussian distribution).

This assertion is supported by the bottom row of Fig. 2, which shows that the application of an RP layer of size \(M=2000\) (chosen arbitrarily, for the purpose of illustration) reduces the overlap between the similarity histograms further than otherwise, and shifts the in-class histogram to the right, coinciding with higher accuracy, surpassing that of the joint linear probe, consistent with our findings in Section 5. We note that these RPs can be generated once and _then frozen for use in all continual learning stages_, enabling a simple and efficient algorithm.

We first analyse the inner products of the features obtained from the pre-trained model projected using some random basis vectors (i.e. random projections). We consider \(M\) such random vectors all randomly drawn i.i.d from a Gaussian distribution with variance \(^{2}\). The number \(M\) defines the dimension of the space the random projections of these features constitute. Considering random projections \(^{L M}\), for any two given feature vectors \(,^{}^{L}\) we have (see Appendix B.2),

\[_{}[(^{})^{}(^{ }^{})]=_{i}^{M}_{}[ _{(i)}^{2}]^{}^{}+_{i j} ^{M}_{}[_{(i)}]^{}_{ }[_{(j)}]^{}^{}, \]

where \(_{(i)}\) denotes the \(i\)th column. That is, the expected inner products of projections of these two features can be decomposed. Now, we have \(_{(i)}(,^{2})\), thus \(_{}[_{(i)}]=_{}[_ {(j)}]=0\) and the second term in Eqn. (3) vanishes. Further, \(_{i}^{M}_{}[_{(i)}^{2}]=M^{2}\). We can make two observations (theoretical details are provided in Appendix B.2):

1. As \(M\) increases, the likelihood that the norm of any projected feature approaching the variance increases. In other words, the projected vectors in higher dimensions almost surely reside on the boundary of the distribution with almost equal distance to the mean (the distribution approaches isotropic Gaussian).
2. As \(M\) increases, it is more likely for angles between two randomly projected instances to be distinct (i.e. the inner products in the projected space are more likely to be larger than some constant).

This discussion can readily be extended to incorporate nonlinearity. The benefit of incorporating nonlinearity is to (i) incorporate interaction terms [54; 50], and, (ii) simulate higher dimensional projections that otherwise could be prohibitively large in number. To see the later point, denoting by \(\) the nonlinearity of interest, we have \((^{})}^{}}\) where \(}\) is obtained from a linear expansion using Taylor series and \(}\) is the corresponding projections. The Taylor expansion of the nonlinear function \(\) gives rise to higher order interactions between dimensions. Although vectors of interaction terms can be formed directly, as in the methods of [38; 54], this is computationally prohibitive for non-trivial \(L\). Hence, the use of nonlinear projections of the form \(_{}:=(_{}^{})\) is a convenient alternative, as known to work effectively in a non-CL context [43; 4; 18; 32].

### Random projection for continual learning

Using the random projection discussed above, with \(()\) as an element-wise nonlinear activation function, given feature sample \(_{t,n}\) we obtain length \(M\) representations for CL training in each task, \(_{t,n}:=(_{t,n}^{})\) (Fig. 1). For inference, \(_{}:=(_{}^{})\) is used in \(s_{y}\) in Eqn. (2) instead of \(_{}\). We define \(\) as an \(M N\) matrix in which columns are formed from all \(_{k,n}\) and for convenience refer to only the final \(\) after all \(N\) samples are used. We now have an \(M M\) Grammatrix for the features, \(=^{}\). The random projections discussed above are sampled once and left frozen throughout all CL stages.

Like the covariance matrix updates in streaming LDA applied to CL [11; 37], variables are updated either for individual samples, or one entire CL stage, \(_{t}\), at a time. We introduce matrix \(\) to denote the concatenated column vectors of all the \(_{y}\). Rather than covariance, \(\), we update the Gram matrix, and the CPs in \(\) (the concatenation of \(_{y}\)'s) iteratively. This can be achieved either one task at a time, or one sample at a time, since we can express \(\) and \(\) as summations over outer products as

\[=_{t=1}^{T}_{n=1}^{N_{t}}_{t,n}_{ t,n},=_{t=1}^{T}_{n=1}^{N_{t}}_{t,n} _{t,n}. \]

Both \(\) and \(\) will be invariant to the sequence in which the _entirety_ of \(N\) training samples are presented, a property ideal for CL algorithms.

The mentioned origins of Eqn. (2) in least squares theory is of practical use; we find it works best to use ridge regression , and calculate the \(l_{2}\) regularized inverse, \((+)^{-1}\), where \(\) denotes the identity matrix. This is achieved methodically using cross-validation--see Appendix C. The revised score for CL can be rewritten as

\[s_{y}=(_{}^{})(+ )^{-1}_{y}. \]

In matrix form the scores can be expressed as predictions for each class label as \(_{}=_{}_{}\). Different to streaming LDA, our approach has the benefits of (i) removing the need for bias calculations from the score; (ii) updating the Gram matrix instead of the covariance avoids outer products of means; and (iii) the form \(_{}=(+)^{-1}\) arises as a closed form solution for mean-square-error loss with \(l_{2}\)-regularization (see Appendix B.3), in contrast to NCM, where no such theoretical result exists. Phase 2 in **Algorithm 1** summarises the above CL calculations.

Application of Eqn. (5) is superficially similar to AdaptMLP modules , but instead of a bottleneck layer, we expand to dimensionality \(M>L\), since past applications found this to be necessary to compensate for \(\) being random rather than learned [43; 4; 18; 32]. As discussed in the introduction, the use of a random and training-free weights layer is particularly well suited to CL.

The value of transforming the original features to nonlinear random projections is illustrated in Fig. 3. Features for the \(T=10\) split ImageNet-R CIL dataset were extracted from a pre-trained ViT-B/16  network and Eqn. (5) applied after each of the \(T=10\) tasks. Fig. 3(a) shows the typical CL average accuracy trend, whereby accuracy falls as more tasks are added. When a nonlinear activation, \(()\) is used (e.g. ReLU or squaring), performance improves as \(M\) increases, but when the nonlinear activation is omitted, accuracy is no better than not using RP, even with \(M\) very large. On the other hand, if dimensionality is reduced without nonlinearity (in this case from 768 to 500), then performance drops below the No RP case, highlighting that if RPs in this application create dimensionality reduction, it leads to poor performance.

Fig. 3(b) casts light on why nonlinearity is important. We use only the first 100 extracted features per sample and compare application of Eqn. (2) to raw feature vectors (black) and to pair-wise interaction terms, formed from the flattened cross-product of each extracted feature vector (blue trace). Use of the former significantly outperforms the latter. However, when Eqn. (5) is used instead (red trace), the drop in performance compared with flattened cross products is relatively small. Although this suggests exhaustively creating products of features instead of RPs, this is computationally infeasible. As an alternative, RP is a convenient and computationally cheap means to create nonlinear feature interactions that enhance linear separability, with particular value in CL with pre-trained models.

### Combining with parameter-efficient transfer learning and first-session adaptation

Use of an RP layer has the benefit of being model agnostic, e.g. it can be applied to any feature extractor. As we show, it can be applied orthogonally to PETL methods. PETL is very appealing for CL, particularly approaches that do not alter any learned parameters of the original pre-trained model, such as [10; 28]. We combine RP with a PETL method trained using CL-compatible 'first-session' training, as carried out by [65; 37]. This means training PETL parameters only on the first CL task, \(_{1}\), and then freezing them thereafter (see Phase 1 of **Algorithm 1**). The rationale is that the training data and labels in the first task may be more representative of the downstream dataset than that used to train the original pre-trained model. If a new dataset drifts considerably, e.g. as in DIL, the benefits of PETL may be reduced because of this choice. But the assumption is that the domain gap between the pre-training dataset and the new dataset is significant enough to still provide a benefit. First-session training of PETL parameters requires a temporary linear output layer, learned using SGD with cross-entropy loss and softmax and only \(K_{1}\) classes, which is discarded prior to Phase 2.

For transformer nets, we experiment with the same three methods as , i.e. AdaptFormer , SSF , and VPT . For details of these methods see the cited references and Appendix D. Unlike  we do not concatenate adapted and unadapted features, as we found this added minimal value when using RP.

### Memory usage of RP layer

The RP layer increases the total memory required by the class prototypes by a factor of \(1+(M-L)/L\), while an additional \(LM\) parameters are frozen and untrainable. For typical values of \(L=768\) and \(K=200\), the injection of an RP layer of size \(M=10000\) therefore creates \( 10\) times the number of trainable parameters, and adds \( 10\)M non-trainable parameters. Although this is a significant number of new parameters, it is still small compared with the overall size of the ViT-B/16 model, with its 84 millions parameters. Moreover, the weights of \(\) can be bipolar instead of Gaussian and if stored using a single bit for each element contribute only a tiny fraction to additional model memory. During training only, we also require updating an \(M M\) Gram matrix, which is smaller than the \(K\) (\(L L\)) covariance matrices of the SLCA fine-tuning approach  if \(M<L\). L2P , DualPrompt  and ADAM  each use \(\)0.3M-0.5M trainable parameters. For \(K=200\) classes and \(M=10000\), RanPAC uses a lot more (\(\)2-2.5M, depending on the PETL method), comparable to CODA-Prompt . RanPAC also uses 10M untrained parameters, but we highlight that these are not trainable. Moreover, \(M\) need not be as high as \(10000\) (Table A5).

```
0: Sequence of \(T\) tasks, \(=\{_{1},_{T}\}\), pre-trained model, and PETL method Phase 1: 'First-session' PETL adaptation. for sample \(n=1,,N_{1}\) in \(_{1}\)do  Extract feature vector, \(_{1,n}\)  Use in SGD training of PETL parameters endfor Phase 2: Continual Learning with RPs.  Create frozen \(L M\) RP weights, \(\!\!(0,1)\) for task \(t=1,,T\)do for sample \(n=1,,N_{t}\) in \(_{t}\)do  Extract feature vector, \(_{t,n}\)  Apply \(_{t,n}=(_{t,n}^{})\)  Update \(\) and \(\) matrices using Eqn. (4) endfor  Optimize \(\) (A. C) and compute \((+)^{-1}\) endfor
```

**Algorithm 1** RanPAC Training

## 5 Experiments

We have applied **Algorithm 1** to both CIL and DIL benchmarks. For the pre-trained model, we experiment mainly with two ViT-B/16 models  as per : one self-supervised on ImageNet-21K,

Figure 3: **Impact of RP compared to alternatives. (a) Using only Phase 2 of Algorithm 1, we show average accuracy (see Results) after each of \(T=10\) tasks in the split ImageNet-R dataset. The data illustrates the value of nonlinearity combined with large numbers of RPs, \(M\). (b) Illustration that interaction terms created from feature vectors extracted from frozen pre-trained models contain important information that can be mostly recovered when RP and nonlinearity are used.**

and another with supervised fine-tuning on ImageNet-1K. Comparisons are made using a standard CL metric, Average Accuracy , which is defined as \(A_{t}=_{i=1}^{t}R_{t,i}\), where \(R_{t,i}\) are classification accuracies on the \(i\)-th task, following training on the \(t\)-th task. We report final accuracy, \(A_{T}\), in the main paper, with analysis of each \(A_{t}\) and \(R_{t,i}\) left for Appendix F, along with forgetting metrics and analysis of variability across seeds. Appendix F also shows that our method works well with ResNet and CLIP backbones.

We use split datasets previously used for CIL or DIL (see citations in Tables 1 and 3); details are provided in Appendix E. We use \(M=10000\) in **Algorithm 1** except where stated; investigation of scaling with \(M\) is in Appendix F.5. All listed 'Joint' results are non-CL training of comparisons on the entirety of \(\), using cross-entropy loss and softmax.

Key indicative results for CIL are shown in Table 1, for \(T=10\), with equally sized stages (except VTAB which is \(T=5\), identical to ). For \(T=5\) and \(T=20\), see Appendix F. For each dataset, our best method surpasses the accuracy of prompting methods and the CP methods of  and , by large margins. Ablations of **Algorithm 1** listed in Table 1 show that inclusion of our RP layer results in error-rate reductions of between 11% and 28% when PETL is used. The gain is reduced otherwise, but is \(\) 8%, except for VTAB. Table 1 also highlights the limitations of NCM.

**Algorithm 1** surpasses the jointly trained linear probe, on all seven datasets; Table 1 indicates that for all datasets this can be achieved by at least one method that adds additional weights to the pre-trained model. However, NCM alone cannot match the joint linear probe. As shown in Table 2, **Algorithm 1** also does better than fine-tuning strategies for CIL . This is tabulated separately from Table 1 because fine-tuning has major downsides as mentioned in the Introduction. Table 2 also shows Algorithm 1 reaches within 2% raw accuracy of the best joint fine-tuning accuracy on three datasets, with a larger gap on the two ImageNet variants and Cars.

Results for DIL datasets, and corresponding ablations of Algorithm 1 are listed in Table 3. CORe50 is \(T=8\) stages with 50 classes , CDDB-Hard is \(T=5\) with 2 classes  and DomainNet is \(T=6\) with 345 classes  (further details are provided in Appendix E). The same trends as for CIL can be observed, i.e. better performance than prompting strategies and highlighting of the value of the RP layer. For DomainNet, the value of RP is particularly strong, while including PETL adds little value, consistent with the nature of DIL, in which different CL stages originate in different domains.

  Method & **CIFAR100** & **IN-R** & **IN-A** & **CUB** & **OB** & **VTAB** & **Cars** \\  Joint linear probe & \(87.9\%\) & \(72.0\%\) & \(56.6\%\) & \(88.7\%\) & \(78.5\%\) & \(86.7\%\) & \(51.7\%\) \\  L2P  & \(84.6\%\) & \(72.4\%\) & \(42.5\%\) & \(65.2\%\) & \(64.7\%\) & \(77.1\%\) & \(38.2\%\)\({}^{*}\) \\ DualPrompt  & \(84.1\%\) & \(71.0\%\) & \(45.4\%\) & \(68.5\%\) & \(65.5\%\) & \(81.2\%\) & \(40.1\%\)\({}^{*}\) \\ CODA-Prompt  & \(86.3\%\) & \(75.5\%\) & \(44.5\%\) & \(79.5\%\) & \(68.7\%\) & \(87.4\%\) & \(43.2\%\) \\  ADaM  & \(87.6\%\) & \(72.3\%\) & \(52.6\%\) & \(87.1\%\) & \(74.3\%\) & \(84.3\%\) & \(41.4^{**}$}\) \\ Ours (**Algorithm 1**) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ Rel. ER vs ADaM\({}^{}\) & \(\) 36\% & \(\) 20\% & \(\) 21\% & \(\) 25\% & \(\) 22\% & \(\) 53\% & \(\) 62\% \\  Ablations & & & & & & & \\  No RPs & \(90.6\%\) & \(73.9\%\) & \(57.7\%\) & \(86.6\%\) & \(74.3\%\) & \(90.0\%\) & \(73.0\%\) \\ No Phase 1 & \(89.0\%\) & \(71.8\%\) & \(58.2\%\) & \(88.7\%\) & \(78.2\%\) & \(92.2\%\) & \(66.0\%\) \\ No RPs or Phase 1 & \(87.0\%\) & \(67.6\%\) & \(54.4\%\) & \(86.8\%\) & \(73.1\%\) & \(91.9\%\) & \(62.2\%\) \\ NCM with Phase 1 & \(87.8\%\) & \(70.1\%\) & \(49.7\%\) & \(85.4\%\) & \(73.4\%\) & \(88.2\%\) & \(40.5\%\) \\ NCM only & \(83.4\%\) & \(61.2\%\) & \(49.3\%\) & \(85.1\%\) & \(73.1\%\) & \(88.4\%\) & \(37.7\%\) \\  Rel. ER (inc. Phase 1) & \(\) 17\% & \(\) 15\% & \(\) 11\% & \(\) 28\% & \(\) 22\% & \(\) 22\% & \(\) 17\% \\ Rel. ER (no Phase 1) & \(\) 15\% & \(\) 13\% & \(\) 8\% & \(\) 14\% & \(\) 19\% & \(\) 4\% & \(\) 10\% \\  

Table 1: **Comparison of prompting and CP strategies for CIL:** ‘Rel. ER’ (‘) is Relative Error Rate. No rehearsal buffer is used for any method listed. IN-R is ImageNet-R . The versions of ImageNet-A (IN-A), OmniBenchmark (OB), CUB and VTAB are those defined by . L2P and DualPrompt results are those stated within , except \({}^{*}\) (Cars) from . CODA-Prompt is directly from  for CIFAR100 and IN-R, and from our own experiments with the PILOT toolbox  otherwise. For ADaM, we have used the best reported by  across all PETL methods, except for \({}^{**}\) (Cars) which is our own. Remaining results are our own implementation. PromptFusion  is omitted due to requiring a rehearsal buffer. Using a NCM method,  achieved 83.7% for CIFAR100 and 64.3% for Imagenet-R. **Ablations of Algorithm 1:** NCM uses cosine similarity instead of Eqn. (2); RP means Random Projections layer; Phase 1 is defined within **Algorithm 1**.

## 6 Conclusion

We have demonstrated that feature representations extracted from pre-trained foundation models such as ViT-B/16 have not previously achieved their full potential for continual learning. Application of our simple and rehearsal-free class-prototype strategy, RanPAC, results in significantly reduced error rates on diverse CL benchmark datasets, without risk of forgetting in the pre-trained model. These findings highlight the benefits of CP strategies for CL with pre-trained models.

**Limitations:** The value of Eqs (4) and (5) are completely reliant on supply of a good generic feature extractor. For this reason, they are unlikely to be as powerful if used in CL methods that train networks from scratch. However, it is possible that existing CL methods that utilise self-supervised learning, or otherwise create good feature extractor backbones could leverage similar approaches to ours for downstream CL tasks. As discussed in Section 4.5, RanPAC uses additional parameters compared to methods like L2P. However, this is arguably worth trading-off for the simplicity of implementation and low-cost training.

**Future Work:** Examples in Appendix F shows that our method works well with other CL protocols including: (i) task-agnostic, i.e. CL without task boundaries during training (e.g. Gaussian scheduled CIFAR-100), (ii) use of a non one-hot-encoded target, e.g. language embeddings in the CLIP model, and (iii) regression targets, which requires extending the conceptualisation of class-prototypes to generic feature prototypes. Each of these has a lot of potential extensions and room for exploration. Other interesting experiments that we leave for future work include investigation of combining our approach with prompting methods, and (particularly for DIL) investigation of whether training PETL parameters beyond the first session is feasible without catastrophic forgetting. Few-shot learning with pre-trained models [1; 45] may potentially also benefit from a RanPAC-like algorithm.

  Method & **CIFAR100** & **IN-R** & **IN-A** & **CUB** & **OB** & **VTAB** & **Cars** \\  Joint full fine-tuning & \(93.8\%\) & \(86.6\%\) & \(70.8\%\) & \(90.5\%\) & \(83.8\%\) & \(92.0\%\) & \(86.9\%\) \\  SLCA  & \(91.5\%\) & \(77.0\%\) & - & \(84.7\%\)\({}^{*}\) & - & - & \(67.7\%\) \\ Ours (**Algorithm 1**) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  

Table 2: **Comparison with fine-tuning strategies for CIL.** Results for SLCA are directly from . ‘Joint’ means full fine-tuning of the entire pre-trained ViT network, i.e. a continual learning upper bound. Notes: L2  achieved 76.1% on Imagenet-R, but reported no other ViT results. Cells containing - indicates SLCA did not report results, and no codebase is available yet. Superscript \({}^{*}\): the number of CUB training samples used by  is smaller than that defined by , which we use.

  Method & **CORe50** & **CDDB-Hard** & **DomainNet** \\  L2P  & \(78.3\%\) & \(61.3\%\) & \(40.2\%\) \\ S-iPrompts  & \(83.1\%\) & \(74.5\%\) & \(50.6\%\) \\  ADaM & \(92.0\%\) & \(70.7\%\) & \(50.3\%\) \\ Ours (**Algorithm 1**) & \(\) & \(\) (\(M=5K\)) & \(\) \\  S-liPrompts  (not ViT B/16) & _89.1\%_ & _88.7\%_ & _67.8\%_ \\  Ablations & & & & \\  No RPs & \(94.8\%\) & \(84.2\%\) & \(55.2\%\) \\ No Phase 1 & \(94.3\%\) & \(81.6\%\) (\(M=5K\)) & \(64.7\%\) \\ No RPs or Phase 1 & \(92.4\%\) & \(80.7\%\) & \(54.1\%\) \\ NCM with Phase 1 & \(91.4\%\) & \(73.2\%\) & \(51.3\%\) \\ NCM only & \(80.4\%\) & \(69.8\%\) & \(46.4\%\) \\  Rel. ER (inc. Phase 1) & \(\)\(37\%\) & \(\)\(13\%\) & \(\)\(25\%\) \\ Rel. ER (no Phase 1) & \(\)\(25\%\) & \(\)\(5\%\) & \(\)\(23\%\) \\  

Table 3: **Comparison with prompting and CP strategies for DIL:** No rehearsal buffer is used for any method. The first four rows use ViT B/16 networks. S-liPrompts uses vision-language prompting with a pre-trained CLIP model, and cannot be directly compared to ours. For CORe50, 83.2% was reported for simple CP strategy of  and 85.4% for FSA-FiLM with pre-trained convnets . For DomainNet we and  use 365 classes, different to . ADaM results are based on code from . L2P and S-prompts are taken from . **Ablations:** Explanations are the same as in Table 1.