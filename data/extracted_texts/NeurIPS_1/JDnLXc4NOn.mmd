# Improving Few-Shot Generalization by Exploring and Exploiting Auxiliary Data

Alon Albalak

University of California,

Santa Barbara

alon_albalak@ucsb.edu

&Colin Raffel

University of Toronto

Vector Institute

craffel@gmail.com

&William Yang Wang

University of California,

Santa Barbara

william@cs.ucsb.edu

###### Abstract

Few-shot learning is valuable in many real-world applications, but learning a generalizable model without overfitting to the few labeled datapoints is challenging. In this work, we focus on **F**ew-shot **L**earning with **A**uxiliary **D**ata (FLAD), a training paradigm that assumes access to auxiliary data during few-shot learning in hopes of improving generalization. Previous works have proposed automated methods for mixing auxiliary and target data, but these methods typically scale linearly (or worse) with the number of auxiliary datasets, limiting their practicality. In this work we relate FLAD to the explore-exploit dilemma that is central to the multi-armed bandit setting and derive algorithms whose computational complexity is independent of the number of auxiliary datasets, allowing us to scale to \(100\) more auxiliary datasets than prior methods. We propose two algorithms - EXP3-FLAD and UCB1-FLAD - and compare them with prior FLAD methods that either explore _or_ exploit, finding that the combination of exploration _and_ exploitation is crucial. Through extensive experimentation we find that our methods outperform all pre-existing FLAD methods by 4% and lead to the first 3 billion parameter language models that outperform the 175 billion parameter GPT-3. Overall, our work suggests that the discovery of better, more efficient mixing strategies for FLAD may provide a viable path towards substantially improving generalization in few-shot learning. All of our code is available at github.com/alon-albalak/FLAD.

## 1 Introduction

Few-shot learning is an attractive learning setting for many reasons: it promises efficiency in cost and time, and in some scenarios data is simply not available due to privacy concerns or the nature of the problem. However, few-shot learning is also a challenging setting that requires a delicate balance between learning the structure of the feature and label spaces while preventing overfitting to the limited training samples . One approach to improving the generalizability of models in the few-shot setting is **F**ew-shot **L**earning with **A**uxiliary **D**ata (FLAD), where additional auxiliary datasets are used to improve generalization on the target few-shot task .

However, FLAD methods introduce their own challenges, including increased algorithmic and computational complexity. Specifically, incorporating auxiliary data during training introduces a large space of design choices (e.g. how and when to train on auxiliary data). Manually designing the curriculum for training on large quantities of auxiliary data is not feasible due to the combinatorially large search space, and hand-picking which auxiliary data to use based on heuristics (e.g. from the same domain or task as the target few-shot dataset) can lead to sub-optimal results . Delegating such choices to an algorithm can lead to better solutions, as demonstrated in the transfer learning , meta-learning , multi-task learning , and auxiliary learning literature . However, prior auxiliary learning algorithms often assume that only 1-3 related auxiliary datasets areavailable and design algorithms whose computational complexity grows linearly (or worse) with the number of auxiliary datasets [18; 8], motivating the search for more efficient methods as the number of auxiliary datasets grows.

To overcome the challenges of prior works, we desire a FLAD algorithm that **(1)** makes no assumptions on available auxiliary data a-priori (in-domain, on-task, quality, quantity, etc.), **(2)** scales well with the number of auxiliary datasets, and **(3)** adds minimal memory and computational overhead. We design algorithms that satisfy our desiderata by drawing inspiration from the central problem in multi-armed bandit (MAB) settings: the exploration-exploitation trade-off [19; 20]. We relate the set of auxiliary datasets to the arms of a MAB and tailor the classic EXP3  and UCB1  algorithms to fit the FLAD framework by designing three efficient gradient-based reward signals. The combination of our MAB-based algorithms and efficient gradient-based rewards allows us to scale to \(100\) more auxiliary datasets than previous methods. Figure 1 provides a basic illustration of how we formulate FLAD as a MAB problem.

To empirically validate our approaches, we focus on few-shot training of language models and utilize P3 , a readily available resource with hundreds of auxiliary language datasets. We evaluate our methods on the same held-out tasks as the T0 language model  and show that, when using the same collection of auxiliary datasets, our algorithms outperform a directly fine-tuned T0 by 5.6% (EXP3-FLAD) and 5.7% (UCB1-FLAD) absolute. Furthermore, incorporating all available datasets in P3 (i.e. not just those used to train T0) increases the improvement to 9.1% and 9.2%. Finally, we compare models trained with our methods against state-of-the-art few-shot methods, finding that our methods improve performance by >3%, even though one model utilizes a large collection of unlabeled target dataset samples. Furthermore, to the best of our knowledge, our methods lead to the first 3 billion parameter model that improves over 175B GPT-3 using few-shot in-context learning.

In summary, our main contributions are:

* We connect FLAD to the MAB setting and focus on the exploration-exploitation trade-off by designing two algorithms, EXP3-FLAD and UCB1-FLAD along with three reward functions that are both simple and efficient (in space and computational complexity).
* We empirically validate that our methods improve few-shot performance of pretrained language models and show that strategies that employ only exploration _or_ exploitation lead to sub-optimal performance.
* We perform case studies to better understand the dynamics of our reward functions and their interaction with the dynamics of large language model training.

## 2 Related work

A long history of works have found success when combining auxiliary data with target data [4; 24; 6; 25; 26; 5; 18; 7; 27; 28; 8]. Some works have explored the addition of auxiliary learning objectives to aid the learning of the target task [24; 26; 25; 5; 17]. More similar to our work are

Figure 1: **Overview of few-shot learning with auxiliary data (FLAD) as a multi-armed bandit problem.** On the left is the learner which defines a policy \(\) that determines which auxiliary dataset to sample from. On the right is the environment that includes the set of auxiliary datasets \(}\), target dataset \(}\), and the model \(f_{}\). At each turn \(t\), the following five steps take place, further described in Section 3.1: **1.** The learner selects an auxiliary dataset \(_{a}\) according to its policy \(\). **2.** The environment samples a batch \(\{,\}_{a}\). **3.** The model \(f_{}\) calculates gradients for the sampled batch (\(_{a}\)) and the target dataset (\(_{}\)), then updates the parameters \(\). **4.** A reward \(_{a,t}\) is calculated based on \(_{a}\) and \(_{}\). **5.** The learner updates \(\) based on \(_{a,t}\).

methods that perform auxiliary learning by introducing additional data sources beyond the target data [4; 6; 18; 7; 27; 28; 8]. As opposed to the few-shot setting on which this work focuses, previous works have studied auxiliary learning in settings with large quantities of target data. For example, Chen et al.  and Verboven et al.  assume access to 10,000 labeled target samples, Ivison et al.  and Lin et al.  assume access to 1,000s of unlabeled target samples, and Du et al.  and Albalak et al.  assume access to 100s of labeled target samples. Additionally, many of the previous works that study auxiliary learning have only considered settings with 1-3 auxiliary datasets [6; 18; 7; 8]. For example, Verboven et al.  propose a task-weighting method that requires solving a system of equations that becomes underspecified with multiple auxiliary tasks, limiting their method to only a single auxiliary task. Furthermore, Chen et al.  experiment with 3 auxiliary tasks because their method requires learning a target-aware classifier for each source task, so the computation scales as \(O(||||)\) where \(||\) is the number of auxiliary tasks and \(||\) is the number of target tasks, making it impractical to scale to large numbers of source and target tasks. In this work, we focus on improving auxiliary learning with very few target samples (20-70 samples) by scaling up the number of auxiliary datasets orders of magnitude greater than previous work. In order to scale up the learning process, efficiency is a central concern of this work, unlike prior works.

Data selection studies a similar (but distinct) problem where the goal is to selectively utilize a subset of a single large dataset rather than selecting data from auxiliary datasets. Recent research on data selection has found that intelligent data selection can provide significant improvements to model performance [29; 30; 31; 32].

## 3 Multi-armed bandits for few-shot learning with auxiliary data

In this section, we first define the few-shot learning with auxiliary data (**FLAD**) setting. Then, we formulate FLAD as a multi-armed bandits (**MAB**) problem, shown in Figure 1. Next, we define reward functions that are efficient to compute and appropriate for FLAD. Finally, we describe our adaptations of two popular MAB algorithms: EXP3-FLAD and UCB1-FLAD.

### Setup

FLAD problem setting.Few-shot learning with auxiliary data (FLAD) fits into the following setting: assume access to a large set of auxiliary datasets \(_{}\) where, for all \(a\), \(_{a}\) is an individual auxiliary dataset. Given a small quantity of data belonging to a target dataset \(_{}\), the goal of FLAD is to find parameters \(\) of a model \(f_{}\) that achieve high performance on the unknown distribution underlying \(_{}\) while utilizing only the available data, \(_{}_{}\).

Formulating FLAD as MAB.In this work, we adopt the multi-armed bandit (MAB) setting by formulating FLAD as a Markov decision process  and defining a learner and environment, illustrated in Figure 1. The learner consists of a policy \(\) defining a selection strategy over all \(_{a}_{}\). The environment consists of the target dataset \(_{}\), auxiliary datasets \(_{}\), and model \(f_{}\). In this formulation the learner interacts with the environment over \(N\) rounds. At each round \(t\) the learner selects one of the environment's \(||\) datasets \(_{a}_{}\). Next, the environment samples a batch \(\{,\}_{a}\) and calculates the gradient w.r.t. \(\) using a task-appropriate loss function as \(_{a}=_{}(f_{},,)\). Then, the environment computes the target gradient \(_{}=_{}(f_{},_{ })\), and updates model parameters w.r.t. \(_{}+_{a}\). Finally, the learner uses a gradient-based reward \(_{a,t}(_{a},_{})\) to update its policy \(\). See Appendix A and Lattimore & Szepesvari  for further details on multi-armed bandits.

Designing the reward functions.We design the reward function \(\) with our desiderata in mind. To ensure that our algorithm adds minimal memory and computational overhead we consider rewards that utilize information intrinsic to the model and the losses being optimized, not an external model or metric (e.g. accuracy or BLEU). In this work we propose three gradient-based reward functions inspired by previous works: **gradient alignment**[6; 24; 35], **gradient magnitude similarity**[36; 37], and their aggregation. Formally, at turn \(t\) let \(_{a}\) be the gradient of the auxiliary batch and \(_{}\) be the target dataset gradient. **Gradient alignment** is defined as \(^{GA}_{a,t}=._{}}{\|_{a}\| \|_{}\|_{2}}\), i.e. the cosine similarity between the gradients of the sampled auxiliary dataset batch and the whole target dataset. **Gradient magnitude similarity** is defined as \(^{GMS}_{a,t}=\|_{2}\|_{T}\|_{2}}{\|_ {a}\|_{2}^{2}+\|_{}\|_{2}^{2}}\) so that when the two gradients have equal magnitude, this value is equal to 1 and as the magnitudes differ the value goes to zero. Inaddition to the individual reward functions, we also consider an aggregate reward. To ensure that the aggregate is not dominated by either individual reward, we normalize \(^{GA}\), the same range as \(^{GMS}\) and define the aggregate to be their sum: \(^{AGG}_{a,t}=^{GA}_{a,t}}{2}+^{GMS}_{a,t}\). We provide further discussion on the design of reward functions in Section 6.

### Adapting the EXP3 algorithm.

EXP3 BackgroundWe base our first algorithm, EXP3-FLAD, on the EXP3 algorithm  ("_Exp_onential-weight algorithm for _Expl_portion and _Expl_oitation"). EXP3 targets the adversarial MAB setting, which assumes that the reward-generating process is controlled by an adversary who is given access to the learner's policy \(\) and determines the sequence of rewards, \((R_{a,t})_{t=1}^{N}\), for each arm prior to play . We consider the adversarial MAB formulation due to the highly non-convex loss landscape of deep neural networks and our use of stochastic gradient descent-based optimization methods. These factors imply that we cannot guarantee our rewards to be stationary, independent, or follow any particular distribution (e.g. Gaussian). Further details on adversarial MAB are included in Appendix A and in .

In EXP3-FLAD, the learner selects arms according to a Gibbs distribution based on the empirically determined importance-weighted rewards of arms . To allow for exploration, we mix the Gibbs distribution with a uniform distribution . Formally, let \(_{t}\) be the exploration rate at turn \(t\) and, recalling that \(K=||\) is the number of auxiliary datasets, then \(\) defines the probability of selecting a given arm \(a\) as the linear combination of Gibbs and uniform distributions \(_{t}(a)=(1-K_{t})_{t-1}_{a})}{ _{a^{}}(_{t-1}_{a^{}})}+_{t}\) where \(_{a,t}\) is the importance weighted reward \(_{a,t}=_{a,t-1}+}{_{t-1}(a)}\). We want the learner to explore more in early training than in later stages, so we use a decaying exploration rate \(_{t}=,}}\) as proposed by Seldin et al. . The use of an importance-weighted estimated reward compensates the rewards of actions that are less likely to be chosen, guaranteeing that the expected estimated reward is equal to the actual reward for each action. EXP3-FLAD is designed to be nearly optimal in the worst case, but due to the exploration rate it will select "bad" actions at a rate of \(_{t}\). The exploration of EXP3-FLAD combined with importance-weighting allows the policy to handle non-stationary reward-generating processes.

### Adapting the UCB1 algorithm.

UCB1 background.While EXP3-FLAD is applicable in unconstrained settings with highly stochastic and non-stationary rewards, it can be outperformed by other algorithms in settings that _are_ constrained. One such algorithm is the upper confidence bound (UCB1) algorithm , which was originally designed to be optimal for stationary, normally distributed reward functions. Nevertheless, variants of UCB1 have been demonstrated to be effective in a range of settings, such as those involving non-stationary, sub-Gaussian, or heavy-tailed distributions [40; 41]. The UCB1 algorithm and its variants assign each arm a value called the upper confidence bound based on Hoeffding's inequality  and are based on the principle of _optimism in the face of uncertainty_, meaning that with high probability the upper confidence bound assigned to each arm is an overestimate of the unknown mean reward.

In UCB1-FLAD, the learner greedily selects arms according to their upper confidence bound. UCB1 is originally designed for stationary reward-generating processes, so to accommodate non-stationarity we include an exponential moving average when estimating the mean reward for a given arm. Formally, let \(R_{a,t}\) be the observed reward for arm \(a\) at turn \(t\), then we calculate the estimated mean reward as \(_{a}=(1-)_{a}+ R_{a,t}\) where \(\) is the smoothing factor. Then, we define the upper confidence bound to be \(UCB_{a,t}=_{a}+}}\). In the original MAB setting all interactions with the environment occur online, but FLAD is a unique situation where the learner can interact with the auxiliary data prior to training. To take advantage of this, rather than initializing estimated rewards with a single mini-batch, we initialize them with larger data quantities to improve the approximation of the true dataset gradients. This is done for each auxiliary dataset by calculating the gradient \(_{a}=_{}(f_{},,)\), where the number of samples in \(\{,\}\) can be significantly larger than a mini-batch, and can be up to the size of the full dataset. In practice, we use 1,000 examples which is computed in \( 2\) minutes on a single GPU.

AlgorithmsThe EXP3-FLAD and UCB1-FLAD algorithms are visualized in Figure 1. At each turn, both methods will first select an auxiliary dataset \(_{a}\). EXP3-FLAD first computes the current exploration rate \(_{t}\) and samples \(_{a}\) according to the distribution defined by \(_{t}()\), while UCB1-FLAD greedily selects \(_{a^{*}}\) corresponding to the arm with largest upper confidence bound, \(a^{*}=_{a}UCB_{a,t}\). Next, for both methods, the environment samples a batch from the selected dataset, \(\{,\}_{a}\), and calculates the gradient \(_{a}=_{}(_{},,)\). Let \(G\) be the number of rounds between model updates, then the previous steps will repeat \(G\) times, at which point the environment calculates the gradient of the target dataset \(_{}(_{},_{})\) and updates the model w.r.t. \(_{}+_{a}_{a}\). Finally, EXP3-FLAD calculates the importance-weighted reward for each auxiliary batch using the observed rewards, while UCB1-FLAD calculates the smoothed estimated mean reward. Pseudocode is found in Appendix B.

## 4 Experimental setup

Models.For our experiments, we utilize encoder-decoder Transformer models from the T5 family of pre-trained language models . Specifically, we experiment with LM-adapted T5 (T5-LM) and T0. The T5-LM model further trains the T5.1.1 model for 100,000 steps (corresponding to 100B tokens) from the C4 dataset  on the prefix language modeling objective . The T0 model was initialized from T5-LM and further trained on a multitask mixture of prompted datasets as described by Sanh et al. . We repeat each experiment with T5-LM XL (hereafter **T5-XL**) and **T0-3B** as our base model. Both models use the same architecture with 2.85 billion parameters, and we used model checkpoints from Hugging Face Transformers ).

Target datasets.We obtain all datasets from Hugging Face Datasets1, and cast them to the text-to-text format by applying prompt templates from the Public Pool of Prompts (P3)  that was used to train T0. To evaluate our few-shot methods, we utilize the same held-out datasets as T0, which cover four distinct tasks: **sentence completion** (COPA , HellaSwag , Story Cloze ), **natural language inference** (ANLI , CB , RTE ), **coreference resolution** (WSC , Winogrande ), and **word sense disambiguation** (WiC ). For each dataset, we randomly sample five-shot splits from their training data, containing the same number of training examples as previous works, between 20 to 70 . We further divide each split into equal training and validation partitions for true few-shot learning (e.g. 10 train and 10 validation samples for HellaSwag). Only ANLI datasets have a publicly available test set, so for all other datasets we evaluate models on the original validation set (not utilized for few-shot training or validation).

Auxiliary datasets.We compare the performance of our methods using two sets of auxiliary data and never include any of the target datasets as part of auxiliary data. First, we use the collection of datasets used for multitask training of T0 (henceforth referred to as T0Mix), including 35 unique datasets covering question answering, sentiment analysis, topic classification, summarization, paraphrase detection and structure-to-text. Second, we utilize all datasets in P3  (which forms a superset of T0Mix) and prevent data leakage by filtering out datasets that overlap with any target dataset, leading to 260 available datasets (list in Appendix H). For each auxiliary dataset, we use at most 10,000 of the dataset's examples.

Baseline methods.We compare our proposed methods with several FLAD and non-FLAD baselines. **Target-Only** (non-FLAD) directly fine-tunes the base model on the target dataset (i.e. without using auxiliary data). **Explore-Only** is a commonly used FLAD method which simultaneously trains on auxiliary and target data by mixing auxiliary datasets equally. Originally called Multitask in , we call this Explore-Only because it is equivalent to continuously exploring auxiliary data and never exploiting knowledge of its relation to the target data. **Exploit-Only** computes gradient alignment prior to training (as in UCB1), and multitask-trains the model by mixing auxiliary datasets according to a Gibbs distribution over the alignments (similar to that in EXP3), resulting in an algorithm that exploits the relations determined prior to training, but never exploring. Both explore- and exploit-only mix target and auxiliary data with a ratio of \(M\) times the highest auxiliary sampling probability. For instance, explore-only with \(M=5\) and \(_{}=\) has a \(1/35\) probability to sample auxiliary dataset \(_{a}_{}\) and a \(5/35\) probability for the target dataset. **Loss-Scaling** is a FLAD method similar to EXP3 and UCB1; the main difference being that it scales auxiliary batch losses by their gradient alignment instead of modifying sampling probabilities. Du et al.  originally propose to use gradient alignment (**Loss-Scaling** (\(GA\))), but we also propose a version that scales losses by gradient magnitude similarity (**Loss-Scaling** (\(GMS\))).

Training details.For the target-only baseline, we use learning rates in \(\{1e{-4},3e{-4}\}\). For all other methods, we always use a learning rate of \(1e{-4}\). For target-, explore-, and exploit-only baselines we use batch sizes in \(\{32,128\}\). For loss-scaling, EXP3-FLAD, and UCB1-FLAD we use mini-batches of 8 samples and let \(G\) be in \(\{4,16\}\) to match the batch size of all methods. For explore- and exploit-only, we use a target dataset mixing ratio of \(M\{1,5,10\}\). For all experiments we use the Adafactor optimizer  and validation-based early stopping for model checkpoint selection. In preliminary experiments we consider rewards using gradients from various model partitions: the full model, encoder-only, decoder-only, and the weights of the output vocabulary matrix (language modeling head). We find that using the parameters from the language modeling head provides the best performance and contains only 2.3% of the full model parameters, significantly reducing memory consumption. For UCB1-FLAD we found the smoothing factor \(=0.9\) to work well in preliminary experiments and initialize auxiliary dataset gradient alignment using 1,000 samples from each auxiliary dataset. Additional implementation details can be found in Appendix C

Experiment procedure.The FLAD experiment process involves training a model that is specialized for each target dataset. For each proposed method and baseline, we train and evaluate a model on each of the 11 target datasets. We repeat training and evaluation on 5 random seeds and include the aggregated results in Table 1. Each cell shows the accuracy averaged across all 55 (11 target datasets, 5 random seeds) experiments. This experimental process is performed for each training method on both models and auxiliary datasets. We include the non-aggregated results in Appendix D.

## 5 Findings and analysis

In Table 1 we compare the empirical results of our MAB-based methods (EXP3-FLAD and UCB1-FLAD) and corresponding baselines on 11 target datasets (expanded results in Appendix D. For each base model and auxiliary data combination (each column) EXP3-FLAD and UCB1-FLAD outperform all the baselines. In fact, we find that _for every single task_ our methods always perform equal to or better than the baselines. This demonstrates that our MAB-based methods provide a strong improvement in few-shot generalization over previous FLAD methods. For a fair comparison where each method utilizes equal data, we compare the performance of Target-Only using T0 and T0Mix (56.44) against the proposed FLAD methods and baselines using T5 and T0Mix (left column). From this comparison it becomes clear that Loss-Scaling actually does worse than multitask training followed by direct fine-tuning by 0.5-3.2%. However, we do find that the remaining FLAD methods lead to improvements (between 2.7-5.6% absolute improvement). We find small performance

    &  &  &  \\ Training Method \(\) & _Auxiliary Data_ & _T0Mix_ & _P3_ & _T0Mix_ & _P3_ \\  Target-Only & & & 52.82 & & 56.44 \\ Loss-Scaling  (\(GA\)) & & 53.22 & 55.19 & 59.47 & 60.66 \\ Loss-Scaling  (\(GMS\)) & & 55.98 & 56.40 & 60.47 & 60.70 \\ Explore-Only  & & 59.18 & 60.64 & 61.17 & 62.77 \\ Exploit-Only  & & 59.79 & 60.49 & 60.87 & 62.87 \\ EXP3-FLAD (\(^{GA}\)) & & 61.50 & 64.07 & 62.87 & 65.98 \\ UCB1-FLAD (\(^{GA}\)) & & 62.01 & 65.52 & 62.35 & 66.29 \\ EXP3-FLAD (\(^{GMS}\)) & & 61.72 & 65.57 & 62.78 & 65.51 \\ UCB1-FLAD (\(^{GMS}\)) & & 61.67 & 65.21 & 62.85 & 66.00 \\ EXP3-FLAD (\(^{AGG}\)) & & 62.05 & 65.47 & 62.84 & **66.84** \\ UCB1-FLAD (\(^{AGG}\)) & & **62.08** & **65.63** & **62.93** & 66.29 \\   

Table 1: **Main results. Each cell contains the score of training a base model (top row) with auxiliary data (second row) using the specified training method (left column), averaged across 11 target datasets on 5 random seeds (each cell is the average of 55 experiments). Target-Only does not utilize auxiliary data. Bolded scores are those with highest mean for a given base model and auxiliary dataset (column-wise), underlined scores are those where a Wilcoxon rank-sum test fails to find significant difference from the highest score (\(p>0.05\)). Expanded results are found in Appendix D.**differences between EXP3-FLAD and UCB1-FLAD across the three reward functions. In general, \(^{AGG}\) leads to the best performance, but we perform a two-sided Wilcoxon rank-sum test to check for significance between average scores and find that the other rewards frequently have no significant difference (\(p>0.05\)).

The importance of prioritized sampling.Loss-Scaling was originally proposed for use with only a single auxiliary dataset and it was unclear, a priori, how it would cope with larger quantities. Additionally, Du et al.  purposefully choose an auxiliary dataset that is related to the target, while in our setting we make no such assumptions. We find that our methods outperform Loss-Scaling methods by 6.3% on average. In Figure 3 (and Figure 5 in Appendix E) we show that, over the course of training, the value of gradient alignments and gradient magnitude similarities for most datasets will converge to 0, leading to very small gradient updates for Loss-Scaling. More importantly, _the auxiliary data that is relevant to the target task is seen less frequently for Loss-Scaling_ than our MAB-based methods. This can be seen by comparing the difference in performance of Loss-Scaling methods when using less (T0Mix) vs. more (P3) auxiliary data. We find that, at best, Loss-Scaling (\(GA\)) improves 2% when using T5 and, at worst, only 0.2% for Loss-Scaling (\(GMS\)) with T0. This is compared with the notable improvements of EXP3-FLAD and UCB1-FLAD of 2.6-4% when considering the same data increase from T0Mix to P3.

The importance of exploration _and_ exploitation.Interestingly, we expected that Exploit-Only would outperform the Explore-Only method because it utilizes relational information between the target and auxiliary tasks, but find no statistical difference between the methods (two-sided Wilcoxon rank-sum test gives \(p>0.05\)). Furthermore, when comparing the ability to leverage additional auxiliary data (i.e. going from T0Mix to all of P3), we find that the improvement for Explore- and Exploit-Only methods is minimal with only 0.7-2% improvement. On the other hand, EXP3-FLAD and UCB1-FLAD show a notable improvement of 2.6-4%, emphasizing the importance of both exploration _and_ exploitation, particularly when dealing with large collections of auxiliary data.

FLAD provides improved generalization over non-FLAD methods.Next, we compare the performance of our best models trained on P3 using \(^{AGG}\) with state-of-the-art few-shot methods: T-Few, DEFT-Few, and GPT-3. T-Few  is a variant of the T0-3B model that multi-task pre-trains parameter-efficient (IA)\({}^{3}\) modules followed by target-only fine-tuning of the (IA)\({}^{3}\) modules. DEFT-Few  is a variant of the T5-XL model that uses retrieved auxiliary data for multi-task training. It first trains a T5-XL model on the 500 nearest neighbor samples from P3 using 1000 unlabeled target dataset samples, and then performs few-shot target-only fine-tuning with the (IA)\({}^{3}\) modules from Liu et al. . Finally, we also compare against the 175 billion parameter variant of

Figure 2: **Comparison of state-of-the-art few-shot methods with FLAD methods trained on P3 using \(^{}\). T-Few scores are from . DEFT-Few scores are from . GPT-3 scores are from  and utilize few-shot in-context learning. All models utilize the same number of few-shot examples and (other than GPT-3) have 3B parameters.**

GPT-3 , which utilizes in-context learning. We find that, on average, models trained using our FLAD-based methods outperform all other methods and, to the best of our knowledge, our methods lead to the first 3 billion parameter model that outperforms GPT-3 on this dataset mixture (previous smallest models have 11 billion parameters), despite using \(62.5\) times fewer parameters than GPT-3. Additionally, we find that our FLAD-based methods provide robust performance across datasets, achieving the best or second-best performance on \(8/11\) datasets, and never performing worst. The use of task-specific modules lead T-Few and DEFT-Few to significant improvements over target-only fine-tuning, preventing the models from ending up in poor local minima. However, these results demonstrate that with the same data, simultaneously fine-tuning with auxiliary and target data leads to improved few-shot generalization, providing a complementary means of improving performance.

Investigating the Reward-Generating Processes.In Section 3.2, we mention that due to the highly non-convex loss landscape and the use of stochastic gradient descent-based optimization techniques, we cannot ensure that our reward generating process is stationary, independent across auxiliary datasets, or follows a normal distribution. To gain a deeper understanding of our reward-generating processes, we examine the distribution of each reward using 5,000 samples from all 35 auxiliary datasets of T0Mix and 32 samples from a few-shot target dataset, WSC . The resulting histograms at every 100 steps can be found in Appendix E, and Figure 3 shows an abbreviated version. The left side of Figure 3 demonstrates that for \(^{GA}\), almost every dataset yields a Gaussian reward distribution, with a few multi-modal distributions. Notably, WikiBio  (dark orange) exhibits peaks at 0.25 and -0.75. Interestingly, \(^{GA}\) results in polarized rewards across datasets, with minimal distribution density between -0.75 and 0.25. In contrast, the right side of Figure 3 displays more non-Gaussian distributions for \(^{GMS}\), as well as flatter distributions compared to \(^{GA}\). Remarkably, we observe that \(^{GA}\) produces more stationary reward distributions, as the distribution for almost every dataset (30/35) converges rapidly towards 0 after only 100 steps. Although most distributions for \(^{GMS}\) also converge towards 0, the convergence occurs at a slower pace, taking nearly 500 steps.

Probing the training dynamics.To better understand the training dynamics of our proposed methods, we perform a case study on T5-XL with T0Mix and \(^{GA}\) and find two datasets where either algorithm improves significantly over the other (full details and figures in Appendix F). First, we study RTE, where UCB1-FLAD outperforms EXP3-FLAD. We calculate the empirical distribution of samples seen from each auxiliary dataset and find that EXP3-FLAD samples nearly uniformly from all datasets while UCB1-FLAD forms a bimodal sampling distribution with peaks at 2.5% and 3.25% (30% relative difference). The uniformity of the EXP3-FLAD distribution is counterintuitive, as we do find that it achieves separation between auxiliary tasks in the cumulative estimated reward (as shown in Figure 7), but this does not lead to separation in the sampling probability space. Additionally we find that even on COPA, where EXP3-FLAD outperforms UCB1-FLAD, EXP3-FLAD still achieves

Figure 3: **Reward distributions** of \(R^{GA}\) and \(R^{GMS}\) prior to training (step 0) and after 300 gradient updates for the T5-XL model with T0Mix as the auxiliary dataset and WSC  as the target dataset. Each quadrant shows the histograms of reward distributions for all 35 auxiliary datasets. By step 300 most auxiliary datasets provide 0 reward, while only the few remaining “beneficial” datasets provide positive rewards. Results from every 100 gradient updates are shown in Figure 5 in Appendix E.

good separation between cumulative estimated rewards, but has a unimodal sampling distribution, while UCB1-FLAD does not have as clear of a bimodal distribution as in RTE. The difference in empirical sampling distributions is likely due to the difference between the greedy policy of UCB1-FLAD and the stochastic policy of EXP3-FLAD. Empirically, we find that EXP3-FLAD very rarely assigns an auxiliary dataset a probability \(<1\%\), leading to many "bad" batches over the course of thousands of turns. On the other hand, the optimistic policy of UCB1-FLAD spends much less time exploring and will sample "bad" batches much less frequently.

The effect of scaling \(||\)To assess the scalability of our proposed methods, we conduct a study by varying the size of \(||\{35,75,125,175,225,260\}\). For each value of \(||\), we consistently include the 35 datasets from T0Mix and randomly select additional auxiliary datasets from P3 until we reach the desired \(||\). The study is performed on the same 11 target datasets as the main study, using the T0 base model and \(^{AGG}\) reward. The experiment is repeated with three random seeds. Figure 4 shows the mean across the 11 target datasets, along with the standard deviation between seeds.

We find that both EXP3-FLAD and UCB1-FLAD experience a sharp increase from \(||=35\) to \(75\). In addition, we observe improvements up to the maximum value of \(||=260\), ultimately improving accuracy by 2.54 for EXP3-FLAD and 3.12 for UCB1-FLAD when transitioning from 35 to 75 datasets, with further increases of 1.54 for EXP3-FLAD and 0.47 for UCB1-FLAD when increasing \(||\) from 75 to 260.

## 6 Discussion

Discussion on reward functions.In FLAD we want to prioritize training on auxiliary datasets with similar solution spaces as the target task without overfitting to the few-shot target data, and our reward functions are designed to serve this goal. To better understand the reward signal of our aggregate reward, \(^{AGG}\), we examine four combinations of rewards: low \(^{GA}\) and \(^{GMS}\), high \(^{GA}\) but low \(^{GMS}\), low \(^{GA}\) but high \(^{GMS}\), and high \(^{GA}\) and \(^{GMS}\). When both rewards are high, we can assume that the auxiliary gradient is useful. However, when one reward is high and the other is low, it is difficult to draw conclusions as a high \(^{GA}\) on its own means the auxiliary gradient will update weights in the right direction, but low \(^{GMS}\) can mean that we significantly overshoot _or_ undershoot the target, where overshooting can be much more detrimental than undershooting. If both \(^{GA}\) and \(^{GMS}\) are small, we know the auxiliary gradient leads us away from the target solution space, but we don't know if its magnitude is much larger or smaller than the target. At the beginning of training, we can't know if the target or auxiliary gradient has larger magnitude, but as training progresses, it becomes significantly more likely that the auxiliary gradient is greater than the target. Thus, when both \(^{GA}\) and \(^{GMS}\) are low, we are likely to be pulled far from our current solution.

This work uses training set-based rewards, but validation set-based rewards are also possible. One downside of validation-based rewards is they calculate validation score frequently, which increases computational complexity. Additionally, we focus on the few-shot setting and use validation-based early stopping. If we use a validation-based reward, then to prevent overfitting we will need to further split the data into 3 partitions: train, early-stopping validation, and reward-validation.

Choice of baselines.With respect to the number of auxiliary datasets \(||\) and target datasets \(||\), our methods and the baselines we compare against have a computational complexity of \(O(||)\), independent of \(||\). For our model and these baselines, the models we train require \( 6\) GPU-hours per target dataset. If we were to consider a baseline whose computation grows linearly w.r.t. \(||\)

Figure 4: **The effect of scaling \(||\) on target task performance**. Each line represents mean score across 11 datasets and three random seeds, with shaded regions falling between one standard deviation of the mean.

\(O(||||)\) (e.g. ), these experiments would not be feasible without a large amount of hardware: _Training such a model with 70Mix would take over 200 GPU-hours (over 8 GPU-days) for a single target dataset_, and over 1500 GPU-hours (_over 2 GPU-months_) when using all of P3.

Why we don't include theoretical guarantees.The design of MAB algorithms generally comes with theoretical proofs of regret bounds, but in this work we omit such analysis. Although we _could_ make guarantees on the regret bounds of our algorithms, they would not be meaningful because they would be with respect to the rewards, not the accuracy on a held-out dataset (which is the quantity we actually care about).

How does FLAD relate to few-shot learning and multitask learning?Both few-shot learning and FLAD are concerned with optimizing model performance on a single target task with a limited number of examples from the target task. In few-shot learning, the model is given only the target task data \(_{}\) and there is no auxiliary data. Effectively, \(_{}\) is the empty set for few-shot learning. In contrast, for the FLAD setting \(|_{}|>1\). Based on the findings from this study, we highly recommend that practitioners utilize auxiliary data when it is available.

Multitask learning is concerned with optimizing a model for performance on multiple target datasets simultaneously. This is in direct opposition with the FLAD methods presented here, which aim to optimize a model for a single target task. However, it is possible to extend our MAB-based methods to optimize for multiple target tasks simultaneously by aggregating multiple rewards. We believe this would make for an interesting future study.

Limitations.One of the implicit assumptions in the FLAD setting (made by this work and all prior works) is that there is at least _some_ auxiliary data that will be useful for the target task. However, one of the main distinctions of our methods from prior works in the FLAD setting is that prior works make a strong assumption that all auxiliary data are useful, and thus appropriate auxiliary datasets must be hand-picked by humans. On the other hand, our methods allow for only a small portion of the auxiliary data to be useful - our proposed algorithm explores to find useful auxiliary datasets and then exploits them.

## 7 Conclusion

Recall the desiderata for our algorithm, expressed in the introduction: our algorithm should **(1)** make no assumptions on the available auxiliary data a-priori, **(2)** scale well with the number of auxiliary datasets, and **(3)** add minimal memory and computational overhead. **(1)** When designing our algorithm, we purposefully formulate the problem as a multi-armed bandit. MAB algorithms, in general, make no assumptions on the quality of rewards and, in particular, EXP3 even assumes that the auxiliary datasets will play an adversarial role when returning rewards. **(2)** As previously mentioned, our algorithms have a single-turn computational complexity that is independent of the number of auxiliary datasets. **(3)** Finally, our method adds minimal computational overhead beyond usual training computations. Every gradient that we utilize for our reward functions are also used to update the model, adding no additional computations. The only computational overhead is to compute gradient alignment (three vector dot products, two scalar square roots, and two scalar multiplications) or magnitude similarity (four vector dot products, two scalar square roots, three scalar multiplications, and one scalar addition). Additionally, our method adds a small amount of memory overhead, used to store gradients between model updates. Our rewards consider only the gradient w.r.t the language modelling head and, in practice, require 0.25Gb per auxiliary gradient to store, slightly increasing the space complexity above standard fine-tuning.

The methods proposed in this work demonstrate the effectiveness of simultaneous training on auxiliary and target datasets in few-shot settings, continuously updating beliefs by exploring _and_ exploiting auxiliary data, and framing FLAD as a MAB problem. We further showed that by satisfying our desiderata, we are able to scale up FLAD to hundreds of auxiliary datasets and outperform traditional few-shot fine-tuning and in-context learning methods. While the presented algorithms satisfy our desiderata, the findings from this study can inform future work to further improve upon these methods in a number of ways, such as improving the reward function and reducing the space complexity.