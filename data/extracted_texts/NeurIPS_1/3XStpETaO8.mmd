# A General Framework for Equivariant Neural Networks on Reductive Lie Groups

Ilyes Batatia

Engineering Laboratory,

University of Cambridge

Cambridge, CB2 1PZ UK

Department of Chemistry,

ENS Paris-Saclay, Universite Paris-Saclay

91190 Gif-sur-Yvette, France

ilyes.batatia@ens-paris-saclay.fr

&Mario Geiger

Department of Electrical Engineering

and Computer Science,

Massachusetts Institute of Technology

Cambridge, MA, USA

&Jose Munoz

EIA University, FTA Group

Anitoquia, Colombia

&Tess Smidt

Department of Electrical Engineering

and Computer Science,

Massachusetts Institute of Technology

Cambridge, MA, USA

&Lior Silberman

Department of Mathematics

University of British Columbia

Vancouver, BC, Canada V6T 1Z2

&Christoph Ortner

Department of Mathematics

University of British Columbia

Vancouver, BC, Canada V6T 1Z2

###### Abstract

Reductive Lie Groups, such as the orthogonal groups, the Lorentz group, or the unitary groups, play essential roles across scientific fields as diverse as high energy physics, quantum mechanics, quantum chromodynamics, molecular dynamics, computer vision, and imaging. In this paper, we present a general Equivariant Neural Network architecture capable of respecting the symmetries of the finite-dimensional representations of any reductive Lie Group \(G\). Our approach generalizes the successful ACE and MACE architectures for atomistic point clouds to any data equivariant to a reductive Lie group action. We also introduce the lie-nn software library, which provides all the necessary tools to develop and implement such general \(G\)-equivariant neural networks. It implements routines for the reduction of generic tensor products of representations into irreducible representations, making it easy to apply our architecture to a wide range of problems and groups. The generality and performance of our approach are demonstrated by applying it to the tasks of top quark decay tagging (Lorentz group) and shape recognition (orthogonal group).

## 1 Introduction

Convolutional Neural Networks (CNNs)  have become a widely used and powerful tool for computer vision tasks, in large part due to their ability to achieve translation equivariance. This property led to improved generalization and a significant reduction in the number of parameters. Translation equivariance is one of many possible symmetries occurring in machine learning tasks.

A wide range of symmetries described by reductive Lie Groups is present in physics, such as \(O(3)\) in molecular mechanics, \((1,3)\) in High-Energy Physics, \((2^{N})\) in quantum mechanics, and \((3)\) in quantum chromodynamics. Machine learning architectures that respect these symmetries often lead to significantly improved predictions while requiring far less training data. This has been demonstrated in many applications including 2D imaging with \((2)\) symmetry (Cohen and Welling, 2016a; Esteves _et al._, 2017), machine learning force fields with \((3)\) symmetry (Anderson _et al._, 2019; Bartok _et al._, 2013; Batzner _et al._, 2022; Batatia _et al._, 2022a) or jet tagging with \(^{+}(1,3)\) symmetry (Bogatskiy _et al._, 2022; Li _et al._, 2022).

One way to extend CNNs to other groups (Finzi _et al._, 2020; Kondor and Trivedi, 2018) is through harmonic analysis on homogeneous spaces, where the convolution becomes an integral over the group. Other architectures work directly with finite-dimensional representations. We follow the demonstration of Bogatskiy _et al._ (2020a) who constructed a universal approximation of any equivariant map with a feed-forward neural network with vector activations belonging to finite-dimensional representations of a wide class of Lie groups. In this way, one can avoid computational challenges created by infinite-dimensional representations.

Alternatively, our current work can be thought of as a generalization of the Atomic Cluster Expansion (ACE) formalism of Drautz (2019) to general Lie groups. The ACE formalism provides a complete body-ordered basis of \((3)\)-invariant features. By combining the concepts of ACE and \((3)\)-equivariant neural networks, Batatia _et al._ (2022a) proposed the MACE architecture, which achieves state-of-the-art performance on learning tasks in molecular modelling. The present work generalizes the ACE and MACE architectures to arbitrary Lie groups in order to propose a generic architecture for creating representations of geometric point clouds in interaction.

Concretely, our work makes the following contributions:

* We develop the \(G\)-Equivariant Cluster Expansion. This framework generalizes the ACE (Drautz, 2019) and MACE (Batatia _et al._, 2022b) architectures to parameterize properties of point clouds, equivariant under a general reductive Lie group \(G\).
* We prove that our architecture is universal, even for a single layer.
* We introduce lie-nn, a new library providing all the essential tools to apply our framework to a variety of essential Lie Groups in physics and computer visions, including the Lorentz group, \((N)\), \(_{2}()\) and product groups.
* We illustrate the generality and efficiency of our general-purpose approach by demonstrating excellent accuracy on two prototype applications, jet tagging, and 3D point cloud recognition.

## 2 Background

We briefly review a few important group-theoretic concepts: A real (complex) **Lie group** is a group that is also a finite-dimensional smooth (complex) manifold in which the product and inversion of the group are also smooth (holomorphic) maps. Among the most important Lie groups are Matrix Lie groups, which are closed subgroups of \((n,)\) the group of invertible \(n n\) matrices with complex entries. This includes well-known groups such as \((2n,)\) consisting of matrices of determinant one, that is relevant in Hamiltonian dynamics A finite-dimensional **representation** of the Lie group

Figure 1: Examples of natural science problems and associated reductive Lie groups. For high energy physics, the Lorentz group \((1,3)\); for chemistry, the Euclidean group \((3)\); for quantum-chromodynamics, the \((3)\) group.

\(G\) is a finite-dimensional vector space \(V\) endowed with a smooth homomorphism \( G(V)\). Features in the equivariant neural networks live in these vector spaces. An **irreducible** representation \(V\) is a representation that has no subspaces which are invariant under the action of the group (other than \(\{0\}\) and \(V\) itself). This means that \(V\) can not be decomposed non-trivially as the direct sum of representations. A **reductive group** over a field \(F\) is a (Zariski-) closed subgroup of the group of matrices \((n,F)\) such that every finite-dimensional representation of \(G\) on an \(F\)-vectorspace can be decomposed as a sum of irreducible representations.

## 3 Related Work

Lie group convolutionsConvolutional neural networks (CNNs), which are translation equivariant, have also been generalized to other symmetries. For example, G-convolutions (Cohen and Welling, 2016) generalized CNNs to discrete groups. Steerable CNNs (Cohen and Welling, 2016) generalized CNNs to \(O(2)\) equivariance and Spherical CNNs (Cohen _et al._, 2018)\(O(3)\) equivariance. A general theory of convolution on any compact group and symmetric space was given by Kondor and Trivedi (2018). This work was further extended to equivariant convolutions on Riemannian manifolds by Weiler _et al._ (2021).

AccThe Atomic Cluster Expansion (ACE) (Drautz, 2019) introduced a systematic framework for constructing complete \(O(3)\)-invariant high body order basis sets with constant cost per basis function, independent of body order (Dusson _et al._, 2022).

e3nn + Equivariant MLPsThe e3nn library (Geiger and Smidt, 2022) provides a complete solution to build \(E(3)-\)equivariant neural networks based on irreducible representations. The Equivariant MLPs (Finzi _et al._, 2021) include more groups, such as \(SO(1,3)\), \(Z_{n}\), but are restricted to reducible representations making them much less computationally efficient than irreducible representations.

Equivariant MPNNs and MACEEquivariant MPNNs (Kondor _et al._, 2018; Anderson _et al._, 2019; Bogatskiy _et al._, 2020; Satorras _et al._, 2021; Brandstetter _et al._, 2022; Batzner _et al._, 2022) have emerged as a powerful architecture to learn on geometric point clouds. They construct permutation invariants and group equivariant representations of point clouds. Successful applications include simulations in chemistry, particle physics, and 3D vision. MACE (Batatia _et al._, 2022) generalized the \(O(3)\)-Equivariant MPNNs to build messages of arbitrary body order, outperforming other approaches on molecular tasks. (Batatia _et al._, 2022) showed that the MACE design space is large enough to include most of the previously published equivariant architectures.

## 4 The \(G\)-Equivariant Cluster Expansion

We are concerned with the representation of properties of point clouds. Point clouds are described as multi-sets (unordered tuples) \(X=[x_{i}]_{i}\) where each particle \(x_{i}\) belongs to a configuration domain \(\). We denote the set of all such multi-sets by \(()\). For example, in molecular modeling, \(x_{i}\) might describe the position and species of an atom and therefore \(x_{i}=(_{i},Z_{i})^{3}\), while in high energy physics, one commonly uses the four-momentum \(x_{i}=(E_{i},_{i})^{4}\), but one could also include additional features such as charge, spin, and so forth. A property of the point cloud is a map

\[() Z \]

i.e., \(X(X) Z\), usually a scalar or tensor. The range space \(Z\) is application dependent and left abstract throughout this paper. Expressing the input as a multi-set implicitly entails two important facts: (1) it can have varying lengths; (2) it is invariant under the permutations of the particles. The methods developed in this article are also applicable to fixed-length multi-sets, in which case \(\) is simply a permutation-invariant function defined on some \(^{N}\). Mappings that are not permutation-invariant are special cases with several simplifications.

In many applications, especially in the natural sciences, particle properties satisfy additional symmetries. When a group \(G\) acts on \(\) as well as on \(Z\) we say that \(\) is \(G\)-**equivariant** if

\[ g=_{Z}(g), g G \]

where \(_{Z}(g)\) is the action of the group element \(g\) on the range space \(Z\). In order to effectively incorporate exact group symmetry into properties \(\), we consider model architectures of the form

\[()}{ }V}{}V }{}Z, \]where the space \(V\) into which we "embed" the parameterization is a possibly infinite-dimensional vector space in which a convenient representation of the group is available. For simplicity we will sometimes assume that \(Z=V\).

The Atomic Cluster Expansion (ACE) framework (Drautz, 2019; Dusson _et al._, 2022; Drautz, 2020) produces a complete linear basis for the space of all "smooth" \(G\)-equivariant properties \(\) for the specific case when \(G=(3)\) and \(x_{i}\) are vectorial interatomic distances. Aspects of the ACE framework were incorporated into \((3)\)-equivariant message passing architectures, with significant improvements in accuracy (Batatia _et al._, 2022a). In the following paragraphs we demonstrate that these ideas readily generalize to arbitrary reductive Lie groups.

### Efficient many-body expansion

The first step is to expand \(\) in terms of body orders, and truncate the expansion at a finite order \(N\):

\[^{(N)}(X)=_{0}+_{i}_{1}(x_{i})+_{i_{1},i_{2}} _{2}(x_{i_{1}},x_{i_{2}})++_{i_{1},,i_{N}}_{N}(x_{i_{1} },,x_{i_{N}}), \]

where \(_{n}\) defines the \(n\)-body interaction. Formally, the expansion becomes systematic in the limit as \(N\). The second step is the expansion of the \(n\)-particle functions \(_{n}\) in terms of a symmetrized tensor product basis. To define this we first need to specify the embedding of particles \(x\): A countable family \((_{k})_{k}\) is a 1-particle basis if they are linearly independent on \(\) and any smooth 1-particle function \(_{1}\) (not necessarily equivariant) can be expanded in terms of \((_{k})_{k}\), i.e,

\[_{1}(x)=_{k}w_{k}_{k}(x). \]

For the sake of concreteness, we assume that \(_{k}:\), but the range can in principle be any field.We provide concrete examples of 1-particle bases in Appendix A.2. Let a complex vector space \(V\) be given, into which the particle embedding maps, i.e.,

\[(_{k}(x))_{k} V x.\]

As a consequence of (5) any smooth scalar \(n\)-particle function \(_{n}\) can be expanded in terms of the corresponding tensor product basis,

\[_{n}(x_{1},,x_{n})=_{k_{1},,k_{n}}w_{k_{1} k_{n} }_{s=1}^{n}_{k_{s}}(x_{s}). \]

Inserting these expansions into (4) and interchanging summation (see appendix for the details) we arrive at a model for scalar permutation-symmetric properties,

\[A_{k}=_{x X}_{k}(x),}=_{s=1}^{n}A_{k}, ^{(N)}=_{}w_{} }, \]

where \(\) is the set of all \(\) tuples indexing the features \(}\). Since \(}\) is invariant under permuting \(\), only ordered \(\) tuples are retained. The features \(A_{k}\) are an embedding of \(()\) into the space \(V\). The tensorial product features (basis functions) \(}\) form a complete linear basis of multi-set functions on \(\) and the weights \(w_{}\) can be understood as a symmetric tensor. We will extend this linear cluster expansion model \(^{(N)}\) to a message-passing type neural network model in SS 4.4.

While the standard tensor product embeds \((_{s=1}^{n}_{k_{s}})_{}^{n} V^{n}\), the \(n\)-correlations \(}\) are _symmetric tensors_ and embed \((})_{}() ^{n}V\).

The evaluation of the symmetric tensor features \(}\) is the computational bottleneck in most scenarios, but efficient recursive evaluation algorithms (Batatia _et al._, 2022a; Kaliuzhnyi and Ortner, 2022) are available. See Appendix A.13.2 for further discussion of model computational costs.

### Symmetrisation

With (7) we obtained a systematic linear model for (smooth) multi-set functions. It remains to incorporate \(G\)-equivariance. We assume that \(G\) is a reductive Lie group with a locally finite representation in \(V\). In other words we choose a representation \(=(_{kk^{}}) G(V)\) such that

\[_{k} g=_{k^{}}_{kk^{}}(g)_{k^{}}, \]where for each \(k\) the sum over \(k^{}\) is over a finite index-set depending only on \(k\). Most Lie groups one encounters in physical applications belong to this class, the affine groups being notable exceptions. However, those can usually be treated in an _ad hoc_ fashion, which is done in all \(E(3)\)-equivariant architectures we are aware of. In practice, these requirements restrict how we can choose the embedding \((_{k})_{k}\). If the point clouds \(X=[x_{i}]_{i}\) are already given in terms of a representation of the group, then one may simply construct \(V\) to be iterative tensor products of \(\); see e.g. the MTP (Shapeev, 2016) and PELICAN (Bogatskiy _et al._, 2022) models. To construct an equivariant two-particle basis we need to first construct the set of all intertwining operators from \(V V V\). Concretely, we seek all solutions \(C^{,K}_{k_{1}k_{2}}\) to the equation

\[_{k^{}_{1}k^{}_{2}}C^{,K}_{k^{}_{1}k^{}_ {2}}_{k^{}_{1}k_{1}}(g)_{k^{}_{2}k_{2}}(g)=_{K^{ }}_{KK^{}}(g)C^{,K^{}}_{k_{1}k_{2}}; \]

or, written in operator notation, \(C^{}= C^{}\). We will call the \(C^{,K}_{}\)_generalized Clebsch-Gordan coefficients_ since in the case \(G=(3)\) acting on the spherical harmonics embedding \(_{lm}=Y^{m}_{l}\) those coefficients are exactly the classical Clebsch-Gordan coefficients. The index \(\) enumerates a basis of the space of all solutions to this equation. For the most common groups, one normally identifies a canonical basis \(C^{}\) and assigns a natural meaning to this index (cf. SS A.5). Our abstract notation is chosen because of its generality and convenience for designing computational schemes. The generalization of the Clebsch-Gordan equation (9) to \(n\) products of representations acting on the symmetric tensor space \(^{n}(V)\) becomes (cf. SS A.9)

\[&_{^{}}C^{,K}_{^{ }}}_{^{}}=_{K^{}}_{KK^ {}}C^{,K^{}}_{} K,=(k_{1},,k_{N}), g G,\\ &}_{^{}}= _{^{}=^{}\\  S_{n}}_{^{}} _{^{}}=_{t=1}^{n}_{k^{ }_{t}k_{t}}. \]

Due to the symmetry of the \((_{})_{}\) tensors \(C^{,K}_{}\) need only be computed for ordered \(\) tuples and the sum \(_{^{}}\) also runs only over ordered \(\) tuples. Again, the index \(\) enumerates a basis of the space of solutions. Equivalently, (10) can be written in compact notation as \(^{}}=^{}\). These coupling operators for \(N\) products can often (but not always) be constructed recursively from couplings of pairs (9). We can now define the symmetrized basis

\[^{K}_{}=_{^{}}C^{,K}_{^{ }}_{^{}}. \]

The equivariance of (11) is easily verified by applying a transformation \(g G\) to the input (cf SS A.6).

**Universality:** In the limit as the correlation order \(N\), the features \((^{K}_{})_{K,}\) form a complete basis of smooth equivariant multi-set functions, in a sense that we make precise in Appendix A.7. Any equivariant property \(_{V}: V\) can be approximated by a linear model

\[_{V}^{K}=_{}c^{K}_{}B^{K}_{}, \]

to within arbitrary accuracy by taking the number of terms in the linear combination to infinity.

### Dimension Reduction

The tensor product of the cluster expansion in (7) is taken on all the indices of the one-particle basis. Unless the embedding \((_{k})_{k}\) is very low-dimensional it is often preferable to "sketch" this tensor product. For example, consider the canonical embedding of an atom \(x_{i}=(_{i},Z_{i})\),

\[_{k}(x_{i})=_{znlm}(x_{i})=_{zZ_{i}}R_{nl}(r_{i})Y^{m}_{l}(}_{i}).\]

Only the \((lm)\) channels are involved in the representation of \((3)\) hence there is considerable freedom in "compressing" the \((z,n)\) channels.

Following Darby _et al._ (2022) we construct a sketched \(G\)-equivariant cluster expansion: We endow the one-particle basis with an additional index \(c\), referred to as the sketched channel, replacing the index \(k\) with the index pair \((c,k)\), and renaming the embedding \((_{ck})_{c,k}\). In the case of three-dimensional particles one may, for example, choose \(c=(z,n)\). In general, it is crucial that the representation remains in terms of \(_{k,k^{}}\), that is, (8) becomes \(_{ck} g=_{k^{}}_{kk^{}}(g)_{ck^{}}\). Therefore, manipulating only the \(c\) channel does not change any symmetry properties of the architecture. Generalizing Darby _et al._ (2022), the \(G\)-TRACE (tensor-reduced ACE) basis then becomes

\[_{}^{K}=_{^{}}C_{^{}}^{,K}}_{^{}}, \]

\[}_{}=_{t=1}^{n}_{c^{}}w_{cc^{ }}_{x X}_{c^{}k_{t}}(x). \]

This construction is best understood as an equivariance-preserving canonical tensor decomposition (Darby _et al._, 2022). There are numerous variations, but for the sake of simplicity, we restrict our presentation to this one case.

**Universality:** Following the proof of Darby _et al._ (2022) one can readily see that the \(G\)-TRACE architecture inherits the universality of the cluster expansion, in the limit of decoupled channels \(\#c\). A smooth equivariant property \(\) may be approximated to within arbitrary accuracy by an expansion \(^{K}(X)_{c,}c_{}^{K}_{c, }^{K}(X)\). Since the embedding \(_{ck}\) is learnable, this is a _nonlinear model_. We refer to SS A.7 for the details.

### G-Mace, Multi-layer cluster expansion

The \(G\)-equivariant cluster expansion is readily generalized to a multi-layer architecture by re-expanding previous features in a new cluster expansion (Batatia _et al._, 2022b). The multi-set \(X\) is endowed with extra features, \(_{i}^{t}=(h_{i,ck}^{t})_{c,K}\), that are updated for \(t\{1,...,T\}\) iterations. These features themselves are chosen to be a field of representations such that they have a well-defined transformation under the action of the group. This results in

\[x_{i}^{t} =(x_{i},_{i}^{t}) \] \[_{ck}^{t}(x_{i},_{i}^{t}) =_{}w_{}^{t,ck}_{k^{},k^{ }}C_{k^{}k^{}}^{,k}h_{i,ck^{}} ^{t}_{ck^{}}(x_{i}) \]

The recursive update of the features proceeds as in a standard message-passing framework but with the unique aspect that messages are formed via the \(G\)-TRACE and in particular can contain arbitrary high correlation order..

\[m_{i,cK}^{t}=_{}W_{}^{t,cK}_{}^{t,K}. \]

The gathered message \(_{i}^{t}=(m_{i,cK}^{t})_{c,k}\) is then used to update the particle states,

\[x_{i}^{t+1}=(x_{i},_{i}^{t+1}),_{i}^{t+1}=U_{t} _{i}^{t}, \]

where \(U_{t}\) can be an arbitary fixed or learnable transformation (even the identity). Lastly, a readout function maps the state of a particle to a target quantity of interest, which could be _local_ to each particle or _global_ to the mset \(X\),

\[y_{i}=_{t=1}^{T}_{t}^{}(x_{i}^{t}), y=_{t=1}^{T}_{t}^{}(\{x_{i}^{t}\}_{i}). \]

This multi-layer architecture corresponds to a general message-passing neural network with arbitrary body order of the message at each layer. We will refer to this architecture as \(G\)-MACE. The \(G\)-MACE architecture directly inherits universality from the \(G\)-ACE and \(G\)-TRACE architectures:

**Theorem 4.1** (Universality of \(G\)-Mace).: _Assume that the one-particle embedding \((_{k})_{k}\) is a complete basis. Then, the set of \(G\)-MACE models, with a fixed finite number of layers \(T\), is dense in the set of continuous and equivariant properties of point clouds \(X()\), in the topology of pointwise convergence. It is dense in the uniform topology on compact and size-bounded subsets._

## 5 lie-nn : Generating Irreducible Representations for Reductive Lie Groups

In order to construct the G-cluster expansion for arbitrary Lie groups, one needs to compute the generalized Clebsch-Gordan coefficients (10) for a given tuple of representations (see 11). To facilitate this task, we have implemented an open source software library, lie-nn1. In this section we review the key techniques employed in this library.

### Lie Algebras of Reductive Lie Groups

Formally, the Lie algebra of a Lie group is its tangent space at the origin and carries an additional structure, the Lie bracket. Informally the Lie algebra can be thought of as a linear approximation to the Lie group but, due to the group structure, this linear approximation carries (almost) full information about the group. In particular the representation theory of the Group is almost entirely determined by the Lie algebra, which is a simpler object to work with instead of the fully nonlinear Lie group.

Lie algebraThe Lie groups we study can be realized as closed subgroups \(G_{n}()\) of the general linear group. In that case their Lie algebras can be concretely realized as \(=(G)=\{X M_{n}() t :(tX) G\}\) where \((X)=1+X+X^{2}...\) is the standard matrix exponential. It turns out that \( M_{n}()\) is a linear subspace closed under the commutator bracket \([X,Y]=XY-YX\).

Structure theoryWe fix a linear basis \(\{X_{i}\}\), called a set of generators for the group. The Lie algebra structure is determined by the _structure constants_\(A_{ijk}\) defined by \([X_{i},X_{j}]=_{k}A_{ijk}X_{k}\), in that if \(X=_{i}a_{i}X_{i}\) and \(Y=_{j}b_{j}X_{j}\) then \([X,Y]=_{k}(_{i,j}A_{ijk}a_{i}b_{j})X_{k}\). The classification of reductive groups provides convenient generating sets for their Lie algebras (or their complexifications). One identifies a large commutative subalgebra \(\) (sometimes of \(_{}=_{}\)) with basis \(\{H_{i}\}\) so that most (or all) of the other generators \(E_{}\) can be chosen so that \([H_{i},E_{}]=(H_{i})E_{}\) for a linear function \(\) on \(\). These functions are the so-called _roots_ of \(\). Structural information about \(\) is commonly encoded pictorially via the _Dynkin diagram_ of \(\), a finite graph the nodes of which are a certain subset of the roots. There are four infinite families of simple complex Lie algebras \(A_{n}=(n+1),B_{n}=(2n+1),C_{n}=(2n),D_{ n}=(2n)\) and further five exceptional simple complex Lie algebras (a general reductive Lie algebra is the direct sum of several simple ones and its centre). The Lie algebra only depends on the connected component of \(G\). thus when the group \(G\) is disconnected in addition to the infinitesimal generators \(\{X_{i}\}\) one also needs to fix so-called "discrete generators", a subset \( G\) containing a representative from each connected component.

Representation theoryThe representation theory of complex reductive Lie algebras is completely understood. Every finite-dimensional representation is (isomorphic to) the direct sum of irreducible representations ("irreps"), with the latter parametrized by appropriate linear functional on \(\) ("highest weight"). Further given a highest weight \(\) there is a construction of the associated irrep with an explicit action of the infinitesimal generators chosen above. The **Weyl Dimension Formula** gives the dimension of an irrep in terms of its highest weight.

### Numerical Computations in lie-nn

The most basic class of the lie-nn library encodes a group \(G\) and infinitesimal representation \(d\) of \(\) using the tuple

\[:=(A,n,\{d(X_{i})\}_{i},\{(h)\}_{h})\, \]

with \(A\) the structure constants of the group, \(n\) the dimension of the representation, and \(d(X_{i})\) and \((h)\) being \(n n\) matrices encoding the action of the infinitesimal and the discrete generators respectively. The action of infinitesimal generators is related to the action of group generators by the exponential, \( X,(e^{X})=e^{d(X)}\). For finite groups, we assume that \(d(X)=\) as they have only discrete generators.

As the building blocks of the theory irreps are treated specially; the package implements functionality for the following operations for each supported Lie group:

* Constructing the irrep with a given highest weight.

Figure 2: Examples of Dynkin diagrams and their associated group class.

* Determining the dimension of an irrep.
* Decomposing the tensor product of several irreps into irreps up to isomorphism (the **selection rule**, giving the list of irreducible components and their multiplicities).
* Decomposing the tensor product of several irreps into irreps explicitly via a change of basis ("generalized **Clebsch-Gordan** coefficients").
* Computating the symmetrized tensor product of the group (see. 5.3 and A.9 for details).

To construct an irrep explicitly as in (20) one needs to choose a basis in the abstract representation space (including a labeling scheme for the basis) so that we can give matrix representations for the action of generators. For this purpose, we use in lie-nn the Gelfand-Tsetlin (GT) basis (Gelfand and Tsetlin, 1950) and associated labeling of the basis by GT patterns (this formalism was initially introduced for algebras of type \(A_{n}\) but later generalized to all classical groups). Enumerating the GT patterns for a given algebra gives the dimension of a given irrep, the selection rules can be determined combinatorially, and it is also possible to give explicit algorithms to compute Clebsch-Gordan coefficients (the case of \(A_{n}\) is treated by Alex _et al._ (2011)). For some specific groups, simplifications to this procedure are possible and GT patterns are not required.

In some cases, one wants to compute coefficients for reducible representations or for representations where the analytical computation with GT patterns is too complex. In these cases, a numerical algorithm to compute the coefficients is required. Let \(d_{1},d_{2}\) be two Lie algebra representations of interest. The tensor product on the Lie algebra \(d_{1} d_{2}(X)\) can be computed as,

\[d_{1} d_{2}\;(X)=d_{1}(X) 1+1 d_{2}(X) \]

Therefore, given sets of generators of three representations \(d_{1},d_{2},d_{3}\), the Clebsch-Gordan coefficients are the change of basis between \((d_{1}(X) 1+1 d_{2}(X))\) and \(d_{3}(X)\). One can compute this change of basis numerically via a null space algorithm. For some groups, one can apply an iterative algorithm that generates all irreps starting with a single representation, using the above-mentioned procedure (see A.10).

### Symmetric Powers

Let \(V\) be a vector space and \(\{e_{i}\}\) be a basis of \(V\).The symmetric power of \(V\), \(^{n}V\), can be regarded as the space of homogeneous polynomials of degree \(n\) in the variables \(e_{i}\). The product basis in Equation 10 spans exactly this space. A basis of \(^{n}V\) can be constructed as,

\[\{e_{i_{1}} e_{i_{2}}... e_{i_{n}}|i_{1}... i_{n}\} \]

If \(V_{}\) if an irreducible representation of a reductive Lie group \(G\) with highest weight \(\), \(^{n}V_{}\) admits a decomposition into irreducible representations,

\[^{n}V_{}= c_{,}V_{} \]

The generalized Clebsch Gordan coefficients in (11) represent the change of basis between \(^{n}V_{}\) and one of the \(V_{}\). The following steps are taken to obtain these coefficients:

* Construct the symmetric power basis as in (22)
* Compute the coefficients \(c_{,}\), using Freudenthal's Formula or GT patterns.
* For any \(\) with \(c_{,}\) non-zero, find a basis of \(V_{}\), and compute the change of basis between the basis of \(^{n}V_{}\) and \(V_{}\)

Alternatively, if one simply has the Clebsch Gordan coefficients, the change of basis from \(V_{}\) to some \(V_{}\), a new algorithm outlined in Appendix A.9 and implemented in lie-nn can construct the change of basis from \(^{n}V_{}\) to \(V_{}\).

## 6 Applications

### Lie groups and their applications

In Table 6.1 we give a non-exhaustive overview of Lie groups and their typical application domains, to which our methodology naturally applies.

Benchmarking our method on all of these applications is beyond the scope of the present work, in particular, because most of these fields do not have standardized benchmarks and baselines to compare against. The MACE architecture has proven to be state of the art for a large range of atomistic modeling benchmarks (Batatia _et al._, 2022a). In the next section, we choose two new prototypical applications and their respective groups to further assess the performance of our general approach.

### Particle physics with the \(So(1,3)\)

Jet tagging consists in identifying the process that generated a collimated spray of particles called a _jet_ after a high-energy collision occurs at particle colliders. Each jet can be defined as a multiset of four-momenta \([(E_{i},_{i})]_{i=1}^{N}\), where \(E_{i}^{+}\) and \(_{i}^{3}\).

Current state-of-the-art models incorporate the natural symmetry arising from relativistic objects, e.g, the Lorentz symmetry, as model invariance. To showcase the performance and generality of the \(G\)-MACE framework we use the Top-Tagging dataset (Butter _et al._, 2019), where the task is to differentiate boosted top quarks from the background composed of gluons and light quark jets. In Table 2, we can see that \(G\)-MACE achieves excellent accuracy, being the only arbitrary equivariant model to reach similar accuracy as PELICAN, which is an invariant model. We refer to Appendix A.11.1 for the details of the architecture.

### 3D Shape recognition

3D shape recognition from point clouds is of central importance for computer vision. We use the ModelNet10 dataset (Wu _et al._, 2015) to test our proposed architecture in this setting. As rotated objects need to map to the same class, we use a MACE model with \(O(3)\) symmetry. To create an encoder version of \(G\)-MACE, we augment a PointNet++ implementation (Yan, 2019) with \(G\)-MACE layers. See the appendix A.11.2 for more details on the architecture. We see in Table 3 that MACE outperforms the non-equivariant baseline.

   Group & Application & Reference \\  \((1)\) & Electromagnetism & (Lagrave _et al._, 2021) \\ \((3)\) & Quantum Chromodynamics & (Favoni _et al._, 2022) \\ \((3)\) & 3D point clouds & (Batatia _et al._, 2022a) \\ \(^{+}(1,3)\) & Particle Physics & (Bogatskiy _et al._, 2020b) \\ \((3,)\) & Point cloud classification & - \\ \((2^{N})\) & Entangled QP & - \\  \((N)\) & Hamiltonian dynamics & - \\ \((2N+1)\) & Projective geometry & - \\   

Table 1: Lie groups of interests covered by the present methods and their potential applications to equivariant neural networks. The groups above the horizontal line are already available in lie-nn. The ones below the line fall within our framework and can be added.

   Architecture & \#Params & Accuracy & AUC & \(_{30\%}\) \\  
**PELICAN** & 45k & **0.942** & **0.987** & \(\) \\
**partT** & 2.14M & 0.940 & 0.986 & \(1602 81\) \\
**ParticleNet** & 498k & 0.938 & 0.985 & \(1298 46\) \\
**LorentzNet** & 224k & **0.942** & **0.987** & \(2195 173\) \\
**BIP** & 4k & 0.931 & \(0.981\) & \(853 68\) \\
**LGN** & 4.5k & 0.929 & \(0.964\) & \(435 95\) \\
**EFN** & 82k & 0.927 & 0.979 & \(888 17\) \\
**TopoDNN** & 59k & 0.916 & 0.972 & \(295 5\) \\
**LorentzMACE** & 228k & **0.942** & **0.987** & \(1935 85\) \\   

Table 2: Comparison between state-of-the-art metrics on the Top-Tagging dataset. Scores were taken from (Bogatskiy _et al._, 2022; Qu _et al._, 2022; Qu and Gouskos, 2020; Munoz _et al._, 2022; Bogatskiy _et al._, 2020a; Komiske _et al._, 2019; Pearkes _et al._, 2017).

## 7 Conclusion

We introduced the \(G\)-Equivariant Cluster Expansion, which generalizes the successful ACE and MACE architectures to symmetries under arbitrary reductive Lie groups. We provide an open-source Python library lie-nn that provides all the essential tools to construct such general Lie-group equivariant neural networks. We demonstrated that the general \(G\)-MACE architecture simultaneously achieves excellent accuracy in Chemistry, Particle Physics, and Computer Vision. Future development will implement additional groups and generalize to new application domains.