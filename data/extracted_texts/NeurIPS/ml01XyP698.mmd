# FreeSplat: Generalizable 3D Gaussian Splatting Towards Free-View Synthesis of Indoor Scenes

Yunsong Wang  Tianxin Huang  Hanlin Chen  Gim Hee Lee

School of Computing, National University of Singapore

yunsong@comp.nus.edu.sg gimhee.lee@nus.edu.sg

https://github.com/wangys16/FreeSplat

###### Abstract

Empowering 3D Gaussian Splatting with generalization ability is appealing. However, existing generalizable 3D Gaussian Splatting methods are largely confined to narrow-range interpolation between stereo images due to their heavy backbones, thus lacking the ability to accurately localize 3D Gaussian and support free-view synthesis across wide view range. In this paper, we present a novel framework FreeSplat that is capable of reconstructing geometrically consistent 3D scenes from long sequence input towards free-view synthesis. Specifically, we firstly introduce Low-cost Cross-View Aggregation achieved by constructing adaptive cost volumes among nearby views and aggregating features using a multi-scale structure. Subsequently, we present the Pixel-wise Triplet Fusion to eliminate redundancy of 3D Gaussians in overlapping view regions and to aggregate features observed across multiple views. Additionally, we propose a simple but effective free-view training strategy that ensures robust view synthesis across broader view range regardless of the number of views. Our empirical results demonstrate state-of-the-art novel view synthesis performances in both novel view rendered color maps quality and depth maps accuracy across different numbers of input views. We also show that FreeSplat performs inference more efficiently and can effectively reduce redundant Gaussians, offering the possibility of feed-forward large scene reconstruction without depth priors.

Figure 1: **Comparison between FreeSplat and previous methods.** pixelSplat  and MVSplat  fail to reconstruct geometrically consistent global 3D Gaussians, while our FreeSplat is proposed to accurately localize 3D Gaussians from long sequence input and support free view synthesis.

Introduction

Recent advancements has emerged [3; 4; 5; 6] in reconstructing 3D scenes from multiple viewpoints. Based on ray-marching-based volume rendering, Neural Radiance Fields [3; 7; 8; 9] is capable of learning the implicit 3D geometry and radiance fields without depth information. Nonetheless, computational cost remains to be the inherent bottleneck in ray-marching-based volume rendering, preventing it from real-time rendering. 3D Gaussian Splitting [10; 11; 12; 13] has recently been proposed as an efficient representation for photorealistic reconstruction of 3D scenes from multi-views. The explicit representation of 3D Gaussians are optimized to be densified in the textured regions, and the rasterization-based volume rendering avoids the costly ray marching scheme. Consequently, 3D Gaussian Splatting has achieved real-time rendering of high-quality images from novel views. Nonetheless, vanilla 3D Gaussian Splatting lacks generalizability and requires per-scene optimization.

Several attempts [1; 2; 14; 15; 16; 17] have been made to give 3D Gaussian Splatting generalization ability. Despite showing promising performance, these methods are limited to narrow-range scene-level view interpolation [1; 2; 15] and object-centric synthesis [14; 16]. The primary reason for the limitation is that these existing methods depend on dense view matching across multi-view images with transformers to predict Gaussian primitives, which consequently becomes computationally intractable with longer sequences and thus restricting the supervision of these methods to narrow-range interpolated views. As we show in Figure 4, supervision by narrow-range interpolated views often result in poorly localized 3D Gaussians that can become floaters when rendered from extrapolated views. Additionally, the problem is further aggrevated by existing methods typically merging multi-view 3D Gaussians through simple concatenation and thus inevitably lead to noticeable redundancy in overlapping areas (_cf._ Table 2). In view of the above-mentioned problems, it is therefore imperative to design a method that is capable of long sequence reconstruction of global 3D Gaussians, which has the significant potential of supporting real-time rendering from arbitrary poses.

In this paper, we propose FreeSplat tailored for indoor long sequence free view synthesis. Unlike existing methods limited to view interpolation in narrow ranges, our method can effectively reconstruct explicit global 3DGS for novel view synthesis across wide view ranges. Our piling consists of Low-cost Cross-View Aggregation and Pixel-wise Triplet Fusion (PTF). In Low-cost Cross-View Aggregation, we introduce efficient CNN-based backbones and adaptive cost volumes formulation among nearby views for low-cost feature extraction and matching, then we leverage a Multi-Scale Feature Aggregation structure to broaden the receptive field of cost volume and predict Depths and Gaussian Triplets. Subsequently, we present Pixel-wise Alignment with progressive Gaussian fusion in PTF to adaptively fuse local Gaussian Triplets from multi-views and avoid Gaussian redundancy in the overlapping regions. Moreover, due to our efficient feature extraction and matching, we propose a Free-View Training (FVT) strategy to disentangle generalizable 3DGS with specific number of views and train the model on long sequences.

The **contributions** of our paper are summarized as follows:

1. We present Low-cost Cross-View Aggregation to predict initial Gaussian triplets, where the low computational cost makes it possible for feature matching between more nearby views and training on long sequence reconstruction;
2. We propose Pixel-wise Triplet Fusion to fuse Gaussian triplets, which can effectively reduce the Gaussian redundancy in the overlapping regions and aggregate multi-view 3D Gaussian latent features;
3. To the best of our knowledge, we are the first to explore generalizable 3DGS for long sequence reconstruction. Extensive experiments on indoor dataset ScanNet  and Replica  demonstrate our superiority on both image rendering quality and novel view depth rendering accuracy when given different lengths of input views.

## 2 Related Work

**Novel View Synthesis.** Traditional attempts in novel view synthesis mainly employed voxel grids [20; 21] or multiplane images . Recently, Neural Radiance Fields (NeRF) [3; 5; 23; 24; 25] have drawn growing interest using ray-marching-based volume rendering to backpropagate image color error to the implicit geometry and radiance fields, such that the 3D geometry can be implicitly learned to satisfy the multi-view color consistency. Nonetheless, one inherent bottleneck of NeRFs-based method is the computation intensity of ray marching, which requires the costly volume sampling in the implicit fields for each pixel during rendering. To this end, recently 3DGS [10; 26; 27; 11] have attracted increasing attention due to its high efficiency and photorealistic rendering. Instead of relying on MLPs to represent the coordinate-based implicit fields, 3DGS learns an explicit field using a set of 3D Gaussians. They optimize the 3D Gaussians parameters and perform adaptive density control to fit to the given set of images, such that the 3D Gaussians are encouraged to perform densification only in the textured regions and refrain from over-densification. During rendering, 3DGS performs tile-based rasterization to differentiably accumulate color images from the explicit 3D Gaussian primitives, which is significantly faster than the ray-marching-based volume rendering and achieves real-time rendering speed.

**Generalizable Novel View Synthesis.** Another drawback of the traditional NeRF-based and 3DGS-based methods is the requirement of per-scene optimization instead of direct feeding-forward. To this end, there have been a line of work [28; 8; 7; 29] focusing on learning effective priors to predict 3D geometry from given images in a feed forward fashion, where the common practice is to project ray-marching sampled points onto given source views to aggregate multi-view features, conditioning the prediction of the implicit fields on source views instead of point coordinates. Recently, there have also been attempts towards generalizable 3DGS [1; 17; 2; 15; 14]. pixelSplat  and GPS-Gaussian  propose to predict pixel-aligned 3D Gaussian parameters in feed forward fashion. MVSplat  replaces the epipolar line transformer of pixelSplat with a lightweight cost volume to perform more efficient image encoding. GGRt  concatenates pixelSplat predicted 3D Gaussians in a sequence of images and simultaneously perform pose optimization. latentSplat  encodes 3D Variational Gaussians and leverages a discriminator to help produce more indistinguishable images. Nonetheless, existing methods do not reconstruct the global 3D Gaussians from arbitrary length of inputs, and are limited to view interpolation [1; 2; 17; 15] or object/human-centric scenes [17; 14]. In contrary, in this paper we focus on reconstructing large scenes from arbitrary length of inputs without depth priors, unleashing the potential of generalizable 3DGS for large scene explicit representation.

**Indoor Scene Reconstruction.** One line of efforts in feed-forward indoor scene reconstruction focuses on extracting 3D mesh using voxel volumes [30; 31; 32] and TSDF-fusion , while do not perform photorealistic novel view synthesis. On the other hand, the SLAM-based methods [34; 35; 36] require dense sequence of RGB-D input and per-scene tracking and mapping. Another paradigm of 3D reconstruction [37; 38; 39] learns implicit Signed Distance Fields from RGB input, while demanding intensive per-scene optimization. Another recent work SurfelNeRF  learns a feed-forward framework to map a sequence of images to 3D surfels which support photorealistic image rendering, while they rely on external depth estimator or ground truth depth maps. In contrary, we propose an end-to-end model without ground truth depth map input or supervision, enabling accurate 3D Gaussian localization using only photometric losses.

## 3 Preliminary

**Vanilla 3DGS.** 3D-GS  explicitly represents a 3D scene with a set of Gaussian primitives which are parameterized via a 3D covariance matrix \(\) and mean \(\):

\[G()=(-(-)^{} ^{-1}(-)),\] (1)

where \(\) is decomposed into \(=^{}^{}\) using a scaling matrix \(\) and a rotation matrix \(\) to maintain positive semi-definiteness. During rendering, the 3D Gaussian is transformed into the image coordinates with world-to-camera transform matrix \(\) and projected onto image plane with projection matrix \(\), and the 2D covariance matrix \(^{}\) is computed as \(^{}=^{}^{}\). We then obtain a 2D Gaussian \(G^{2D}\) with the covariance \(^{}\) in 2D, and the color rendering is computed using point-based alpha-blending on each ray:

\[()=_{i N}_{i}_{i}G_{i}^{2D}( )_{j=1}^{i-1}(1-_{j}G_{j}^{2D}()),\] (2)

where \(N\) is the number of Gaussian primitives, \(_{i}\) is a learnable opacity, and \(_{i}\) is view-dependent color defined by spherical harmonics (SH) coefficients \(\). The Gaussian parameters are optimized by a photometric loss to minimize the difference between renderings and image observations.

**Generalizable 3DGS.** Unlike vanilla 3DGS that optimizes per-scene Gaussian primitives, recent generalizable 3DGS [1; 17] predict pixel-aligned Gaussian primitives \(\{,,\}\) and depths \(\), such that the pixel-aligned Gaussian primitives can be unprojected to 3D coordinates \(\). The Gaussian parameters are predicted by 2D encoders, which are optimized by the photometric loss through rendering from novel views. However, existing methods are still limited to view interpolation within narrow view range, which leads to inaccurately localized 3D Gaussians that fail to support large scene reconstruction and view extrapolation (_cf._ Figure 1, 4). To this end, we propose FreeSplat towards global 3D Gaussians reconstruction with accurate localization that supports free-view synthesis.

## 4 Our Methodology

### Overview

The overview of our method is illustrated in Figure 2. Given a sparse sequence of RGB images, we build cost volumes adaptively between nearby views, and predict depth maps to unproject the 2D feature maps into 3D Gaussian triplets. We then propose the Pixel-aligned Triplet Fusion (PTF) module to progressively align the global with the local Gaussian triplets, such that we can fuse the redundant 3D Gaussians in the latent feature space and aggregate cross-view Gaussian features before decoding. Our method is capable of efficiently exchanging cross-view features through cost volumes, and progressively aggregating per-view 3D Gaussians with cross-view alignment and adaptive fusion.

### Low-cost Cross-View Aggregation

**Efficient 2D Feature Extraction.** Given a sparse sequence of posed images \(\{^{t}\}_{t=1}^{T}\), we first feed them into a shared 2D backbone to extract multi-scale embeddings \(^{t}_{e}\) and matching feature \(^{t}_{m}\). Unlike [1; 2] which rely on patch-wise transformer-based backbones [41; 42] that can lead to quadratically expensive computations, we leverage pure CNN-based backbones [43; 44] for 2D feature extraction for efficient performance on higher resolution inputs.

**Adaptive Cost Volume Formulation.** To explicitly integrate camera pose information given arbitrary length of input images, we propose to adaptively build cost volumes between nearby views. For current view \(^{t}\) with pose \(^{t}\) and matching feature \(^{t}_{m}^{C_{m}}{4}}\), we adaptively select its \(N\) nearby views \(\{^{t_{n}}\}_{n=1}^{N}\) with poses \(\{^{t_{n}}\}_{n=1}^{N}\) based on pose proximity, and construct cost volume via plane sweep stereo [45; 46]. Specifically, we define a set of \(K\) virtual depth planes \(\{d_{k}\}_{k=1}^{K}\) that are uniformly spaced within \([d_{near},d_{far}]\), and warp the nearby view features to each depth plane

Figure 2: **Framework of FreeSplat. Given input sparse sequence of images, we construct cost volumes between nearby views and predict depth maps and corresponding feature maps, followed by unprojection to Gaussian triplets with 3D positions. We then propose Pixel-aligned Triplet Fusion (PTF) module, where we progressively aggregate and update local/global Gaussian triplets based on pixel-wise alignment. The global Gaussian triplets can be later decoded into Gaussian parameters.**of current view:

\[}_{m}^{t_{n},k}=(^{t_{n}},^{t}) {F}_{m}^{t_{n}},\] (3)

where \((^{t_{n}},^{t})\) is the transformation matrix from view \(t_{n}\) to \(t\). The cost volume \(_{}^{t}^{K}{4}}\) is then defined as:

\[_{}^{t}(k)=f_{}((_{n=1}^{N}( _{m}^{t},}_{m}^{t_{n},k}))(_{n=1}^{N} }_{m}^{t_{n},k})),\] (4)

where \(_{}^{t}[k]\) is the \(k\)-th dimension of \(_{}^{t}\), \(()\) is the cosine similarity, \(\) is feature-wise concatenation, and \(f_{}()\) is a \(1 1\)\(\) mapping to dimension of \(1\).

**Multi-Scale Feature Aggregation.** The embedding of the cost volume plays a significant part to accurately localize the 3D Gaussians (_cf._ Table 5). To this end, inspired by previous depth estimation methods [47; 33], we design an multi-scale encoder-decoder structure, such that to fuse multi-scale image features with the cost volume and propagate the cost volume information to broader receptive fields. Specifically, the multi-scale encoder takes in \(_{}^{t}\) and the output is concatenated with \(\{_{s}^{t}\}\) before sending into a UNet++ -like decoder to upsample to full resolution and predict a depth candidates map \(_{c}^{t}^{K H W}\), and Gaussian triplet map \(_{l}^{t}^{C H W}\). We then predict the depth map through soft-argmax to bound the depth prediction between near and far:

\[^{t}=_{k=1}^{K}(_{c}^{t})_{k} d_{k}.\] (5)

Finally, the pixel-aligned Gaussian triplet map \(_{l}^{t}\) is unprojected to 3D Gaussian triplet \(\{_{l}^{t},_{l}^{t},_{l}^{t}\}\), where \(_{l}^{t}^{3 HW}\) are the Gaussian centers, \(_{l}^{t}^{1 HW}\) are weights between \((0,1)\), and \(_{l}^{t}^{(C-1) HW}\) are Gaussian triplet features.

### Pixel-wise Triplet Fusion

One limitation of previous generalizable 3DGS methods is the redundancy of Gaussians. Since we need multi-view observations to predict accurately localized 3D Gaussians in indoor scenes, the pixel-aligned Gaussians become redundant in frequently observed regions. Furthermore, previous methods integrate multi-view Gaussians of the same region simply through their opacities, leading to suboptimal performance due to lack of post aggregation (_cf._ Table 5). Consequently, inspired by previous methods [31; 40], we propose the Pixel-wise Triplet Fusion (PTF) module which can significantly remove redundant Gaussians in the overlapping regions and explicitly aggregate multi-view observation features in the latent space. We align the per-view local Gaussians with global ones using Pixel-wise Alignment to select the redundant 3D Gaussian Triplets, and progressively fuse the local Gaussians into the global ones.

**Pixel-wise Alignment.** Given the Gaussian triplets \(\{_{l}^{t},_{l}^{t}\}_{t=1}^{T}\), we start from \(t=1\) where the global Gaussians latent is empty. In the \(t\)-th step, we first project the global Gaussian triplet centers \(_{g}^{t-1}^{3 M}\) onto the \(t\)-th view:

\[_{g}^{t}:=\{_{g}^{t},_{g}^{t},_{g}^{t} \}=^{t}_{g}^{t-1},\] (6)

Figure 3: **Visual illustration of PTF.** The PTF incrementally projects current global Gaussians to input views and computes their pixel-wise distance with local Gaussians. Nearby local Gaussians are then fused using a lightweight Gate Recurrent Unit (GRU) network .

where \([_{g}^{t},_{g}^{t},_{g}^{t}]^{3 M}\) are the projected 2D coordinates and corresponding depths. We then correspond the local Gaussian triplets with the pixel-wise nearest projections within a threshold. Specifically, for the \(i\)-th local Gaussian with 2D coordinate \([_{i}^{t}(i),_{i}^{t}(i)]\) and depth \(_{i}^{t}()\), we first find its intra-pixel global projection set \(}_{i}\):

\[}_{i}^{t}:=\{j[_{g}^{t}(j)]=_{ l}^{t}(i),[_{g}^{t}(j)]=_{l}^{t}(i)\},\] (7)

where \([\,\,]\) is the rounding operator. Subsequently, we search for valid correspondence with minimum depth difference under a threshold:

\[m_{i}=\{*{arg\,min}_{j}_{i}^{t}}_{g}^{t}(j)&\,_{l}^{t}( j)-_{j}_{i}^{t}}_{g}^{t}(j)< _{l}^{t}(j)\\ &.,\] (8)

where \(\) is a ratio threshold. We define the valid correspondence set as:

\[}^{t}:=\{(i,m_{i}) i=1,...,HW;\ m_{i}\}.\] (9)

**Gaussian Triplet Fusion.** After the pixel-wise alignment, we remove the redundant 3D Gaussians through merging the validly aligned triplet pairs. Given a pair \((i,m_{i})^{t}\), we compute the weighted sum of their center coordinates and sum their weights to restrict the 3D Gaussian centers to lie between the triplet pair:

\[_{g}^{t}(m_{i})=_{l}^{t}(i) _{l}^{t}(i)+_{g}^{t-1}(m_{i})_{g}^{t-1}(m_{i})}{_{l}^{t}(i)+_{g} ^{t}(m_{i})},_{g}^{t}(m_{i})=_{l}^{t}(i)+_{g}^{t-1}(m_{i}).\] (10)

We then aggregate the aligned local and global Gaussian latent features through a lightweight GRU network:

\[_{g}^{t}(m_{i})=(_{l}^{t}(i), _{g}^{t-1}(m_{i})),\] (11)

and then append with the other unaligned local Gaussian triplets.

**Gaussian primitives decoding.** After the Pixel-wise Triplet Fusion, we can decode the global Gaussian triplets into Gaussian primitives:

\[,,=_{d}( _{g}^{T})\] (12)

and Gaussian centers \(=_{g}^{}\). Our proposed fusion method can incrementally integrate the Gaussians with geometrical constraints and learnable GRU network for feature update. Consequently, our fusion method is capable of significantly removing redundant Gaussians and perform post feature aggregation across multiple views, and can be trained with the other framework components end-to-end with eligible computation overhead.

### Training

**Loss Functions.** After predicting the 3D Gaussian primitives, we render from novel views following the rendering equations in Eq. (2). Similar to pixelSplat  and MVSplat , we train our framework using only photometric losses, _i.e._ a combination of MSE loss and LPIPS  loss, with weights of 1 and 0.05 following .

**Free-View Training.** We propose a Free-View Training (FVT) strategy to add more geometrical constraints on the localization of 3D Gaussians, and to disentangle the performance of generalizable 3DGS with specific number of input views. To this end, we randomly sample \(T\) number of context views (in experiments we set \(T\) between \(2\) and \(8\)), and supervise the image renderings in the broader view interpolations. The long sequence training is made feasible due to our efficient feature extraction and aggregation. We empirically find that FVT significantly contributes to depth estimation from novel views (_cf._ Table 3, 4).

## 5 Experiments

### Experimental Settings

**Datasets.** We leverage the real-world indoor dataset ScanNet  for training. ScanNet is a large RGB-D dataset containing \(1,513\) indoor scenes with camera poses, and we follow  to use 100 scenes for training and 8 scenes for testing. To evaluate the generalization ability of our model, we further perform zero-shot evaluation on the synthetic indoor dataset Replica , for which we follow  to select 8 scenes for testing.

**Implementation Details.** Our FreeSplat is trained end-to-end using Adam  optimizer with an initial learning rate of \(1e-4\) and cosine decay following . Due to the large GPU requirements of [1; 2] given high-resolution images, all input images are resized to \(384 512\) and batch size is set to 1, to form a fair comparison between different methods. We mainly compare with previous generalizable 3DGS methods in 2, 3, 10 reference view settings, where the distance between input views is fixed, thus evaluating the models' performance under different view ranges. For 10 views setting, we also choose target views that are beyond the given sequence of reference views to evaluate the view extrapolation results.

### Results on ScanNet

**View Interpolation Results.** On ScanNet, we evaluate the generalizable novel view interpolation results given 2 and 3 reference views as shown in Table 1. Comparing to pixelSplat and MVSplat, our FreeSplat-_spec_ consistently improves rendering quality and efficiency on 2-views setting and 3-views setting. Although slightly underperforming on SSIM comparing to NeuRay , we show significant improvements on PSNR and LPIPS over NeuRay and \(300\) faster inference speed. Moreover, our FreeSplat-_fv_ consistently offers competitive results given arbitrary number of views, and performs more similarly as FreeSplat-_spec_ when number of input views increases.

 Method &  &  \\  & PSNR\(\) & SSIM\(\) & LPIPS\(\) & Time(s)\(\) & \#GS(k) & PSNR\(\) & SSIM\(\) & LPIPS\(\) & Time(s)\(\) & \#GS(k) \\  NeuRay  & 25.65 & 0.840 & 0.264 & 3.103 & - & 25.47 & 0.843 & 0.264 & 4.278 & - \\  pixelSplat  & 26.03 & 0.784 & 0.265 & 0.289 & 1180 & 25.76 & 0.782 & 0.270 & 0.272 & 1769 \\ MVSplat  & 27.27 & 0.822 & 0.221 & 0.117 & 393 & 26.68 & 0.814 & 0.235 & 0.192 & 590 \\ 
**FreeSplat-_spec_** & 28.08 & 0.837 & 0.211 & 0.103 & 278 & 27.45 & 0.829 & 0.222 & 0.121 & 382 \\
**FreeSplat-_fv_** & 27.67 & 0.830 & 0.215 & 0.104 & 279 & 27.34 & 0.826 & 0.226 & 0.122 & 390 \\ 

Table 1: **Generalizable Novel View Interpolation results on ScanNet .** FreeSplat-_fv_ is trained with our FVT strategy, and the other methods are all trained on specific number of views to form a complete comparison. Time(s) indicates the total time of encoding input images and rendering one image.

 Method & Time(s)\(\) & \#GS(k) &  &  \\  & PSNR\(\) & SSIM\(\) & LPIPS\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) \\  pixelSplat  & 0.948 & 5898 & 21.26 & 0.714 & 0.396 & 20.70 & 0.687 & 0.429 \\ MVSplat  & 1.178 & 1966 & 22.78 & 0.754 & 0.335 & 21.60 & 0.729 & 0.365 \\ 
**FreeSplat-_3-views_** & 0.599 & 882 & 25.15 & 0.800 & 0.278 & 23.78 & 0.774 & 0.309 \\
**FreeSplat-_fv_** & 0.596 & 899 & 25.90 & 0.808 & 0.252 & 24.64 & 0.786 & 0.277 \\ 

Table 2: **Long Sequence (10 views) Explicit Reconstruction results on ScanNet.** The results of pixelSplat, MVSplat and FreeSplat-_spec_ are given using their 3-views version.

 Method &  &  &  \\  & Abs Diff\(\) Abs Rel\(<1.25\) Abs Diff\(\) Abs Rel\(<1.25\) Abs Diff\(\) Abs Rel\(<1.25\) \\  NeuRay  & 0.358 & 0.200 & 0.755 & 0.231 & 0.117 & 0.873 & 0.202 & 0.108 & 0.875 \\  pixelSplat  & 1.205 & 0.745 & 0.472 & 0.698 & 0.479 & 0.836 & 0.970 & 0.621 & 0.647 \\ MVSplat  & 0.192 & 0.106 & 0.912 & 0.164 & 0.079 & 0.929 & 0.142 & 0.080 & 0.914 \\ 
**FreeSplat-_spec_** & 0.157 & 0.086 & 0.919 & 0.161 & 0.077 & 0.930 & 0.120 & 0.070 & 0.945 \\
**FreeSplat-_fv_** & 0.153 & 0.085 & 0.923 & 0.162 & 0.077 & 0.928 & 0.097 & 0.059 & 0.961 \\ 

Table 3: **Novel View Depth Rendering results on ScanNet.**\({}^{}\): 10-views results of pixelSplat, MVSplat and FreeSplat-_spec_ are given using their 3-views version.

**Long Sequence Results.** As shown in Table 2, we further evaluate the long sequence results where we sample reference views with length of 10, and compare both view interpolation and extrapolation results. The results reveal that generalizable 3DGS methods underperform when given long sequence input images, which is due to the complicated camera trajectories in ScanNet, and the inaccuracy of 3D Gaussian localization that leads to errors when observed from wide view ranges. Our FreeSplat-_Sviews_ significantly outperforms pixelSplat and MVSplat on view interpolation and view extrapolation results. Through our proposed FVT that can be easily plugged into our model due to our low requirement on GPU, our FreeSplat-_fv_ consistently outperforms our 3-views version. Our PTF module can also reduce the number of Gaussians by around \(55.0\%\), which becomes indispensable in long sequence reconstruction due to the pixel-wise unprojection nature of generalizable 3DGS. The qualitative results are shown in Figure 4, which clearly reveal that FreeSplat-_spec_ outperforms MVSplat and pixelSplat in localizing 3D Gaussian and preserving fine-grained details, and FreeSplat-_fv_ further improves on localizing and fusing multi-view Gaussians.

Figure 4: **Qualitative Results of Long Sequence Explicit Reconstruction.** For each sequence, the first two rows are view interpolation results, and the last two rows are view extrapolation results.

**Novel View Depth Estimation Results.** We also investigate the correctness of 3D Gaussian localization of different methods through comparing their depth rendering results. We report the Absolute Difference (Abs. Diff), Relative Difference (Rel. Diff), and threshold tolerance \(<1.25\) results from novel views in Table 3. We find that FreeSplat consistently outperforms pixelSplat and MVSplat in predicting accurately localized 3D Gaussians, where FreeSplat-_fv_ reaches \(94.9\%\) of \(<1.25\), enabling accurate unsupervised depth estimation on novel views. The improved depth estimation accuracy of FreeSplat-_fv_ highlights the importance of depth estimation in supporting free-view synthesis across broader view range.

### Zero-Shot Transfer Results on Replica

We further evaluate the zero-shot transfer results through testing on Replica dataset, with results in Table 4. Our view interpolation and novel view depth estimation results still outperforms existing methods. The long sequence results degrade due to inaccurate depth estimation and domain gap, indicating potential future work in further improving the depth estimation in zero-shot tranferring.

### Ablation Study

We conduct a detailed ablation study as shown in Table 5 and Figure 5. The results indicate that: 1) cost volume is essential in accurately localizing 3D Gaussians; 2) our proposed PTF module can consistently contribute to rendering quality and depth estimation results. The PTF module learns to incrementally fuse multi-view 3D Gaussians and contributes significantly when varying number of input views, and serves as a multi-view localization regularization that helps unsupervised depth estimation; 3) Our FVT module excels in long sequence reconstruction quality as well as novel view

 } &  &  \\  & PSNR\(\) & SSIM\(\) & LPIPS\(\) & \(<1.25\) & \#GS(k) & PSNR\(\) & SSIM\(\) & LPIPS\(\) & \(<1.25\) & \#GS(k) \\  pixelSplat  & 26.24 & 0.829 & 0.229 & 0.576 & 1769 & 19.23 & 0.719 & 0.414 & 0.375 & 5898 \\ MVSplat  & 26.16 & 0.840 & 0.173 & 0.670 & 590 & 18.66 & 0.717 & 0.360 & 0.565 & 1966 \\ 
**FreeSplat-_spec_** & 26.98 & 0.848 & 0.171 & 0.682 & 423 & 21.11 & 0.762 & 0.312 & 0.720 & 1342 \\
**FreeSplat-_fv_** & 26.64 & 0.843 & 0.184 & 0.682 & 421 & 21.95 & 0.777 & 0.290 & 0.742 & 1346 \\ 

Table 4: **Zero-Shot Transfer Results on Replica .**

Figure 5: **Qualitative Ablation Study.** The first and second row use input view lengths of 3 and 10.

 } &  &  \\  & & PSNR\(\) & SSIM\(\) & LPIPS\(\) & \(<1.25\) & \(<1.10\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) & \(<1.25\) & \(<1.10\) \\  ✓ & & & 27.12 & 0.825 & 0.224 & 0.925 & 0.762 & 24.23 & 0.792 & 0.277 & 0.942 & 0.804 \\ ✓ & ✓ & & 22.10 & 0.696 & 0.359 & 0.639 & 0.311 & 17.94 & 0.607 & 0.487 & 0.543 & 0.216 \\ ✓ & ✓ & & 27.45 & 0.829 & 0.222 & 0.930 & 0.773 & 25.15 & 0.800 & 0.278 & 0.945 & 0.823 \\ ✓ & ✓ & ✓ & 26.41 & 0.806 & 0.232 & 0.919 & 0.746 & 25.40 & 0.799 & 0.252 & 0.950 & 0.831 \\ ✓ & ✓ & ✓ & 27.34 & 0.826 & 0.226 & 0.928 & 0.764 & 25.90 & 0.808 & 0.252 & 0.961 & 0.858 \\ 

Table 5: **Ablation on ScanNet.** CV: Cost Volume, PTF: Pixel-wise Triplet Fusion, FVT: Free-View Training.

depth rendering results, which provides stricter constrains on 3D Gaussian localization and can be seamlessly combined with the PTF module to fit to varying length of input views.

## 6 Conclusion

In this study, we introduced FreeSplat, a generalizable 3DGS model that is tailored to accommodate an arbitrary number of input views and perform free-view synthesis using the global 3D Gaussians. We developed a Low-cost Cross-View Aggregation pipeline that enhances the model's ability to efficiently process long input sequences, thus incorporating stricter geometry constraints. Additionally, we have devised a Pixel-wise Triplet Fusion module that effectively reduces redundant pixel-aligned 3D Gaussians in overlapping regions and merges multi-view Gaussian latent features. FreeSplat consistently improves the fidelity of novel view renderings in terms of both color image quality and depth map accuracy, facilitating feed-forward global Gaussians reconstruction without depth priors.

## 7 Acknowledgement

This work is supported by the Agency for Science, Technology and Research (A*STAR) under its MTC Programmatic Funds (Grant No. M23L7b0021).