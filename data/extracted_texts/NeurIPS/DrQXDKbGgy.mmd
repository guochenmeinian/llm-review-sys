# Online Non-convex Learning in Dynamic Environments

Zhipan Xu\({}^{1}\), Lijun Zhang\({}^{1,2,}\)

\({}^{1}\)National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China

\({}^{2}\)School of Artificial Intelligence, Nanjing University, Nanjing, China

xuzhipan@smail.nju.edu.cn, zhanglj@lamda.nju.edu.cn

Lijun Zhang is the corresponding author.

###### Abstract

This paper considers the problem of online learning with non-convex loss functions in dynamic environments. Recently, Suggala and Netrapalli (2020) demonstrated that follow the perturbed leader (FTPL) can achieve optimal regret for non-convex losses, but their results are limited to static environments. In this research, we examine dynamic environments and choose _dynamic regret_ and _adaptive regret_ to measure the performance. First, we propose an algorithm named FTPL-D by restarting FTPL periodically and establish \(O(T^{}(V_{T}+1)^{})\) dynamic regret with the prior knowledge of \(V_{T}\), which is the variation of loss functions. In the case that \(V_{T}\) is unknown, we run multiple FTPL-D with different restarting parameters as experts and use a meta-algorithm to track the best one on the fly. To address the challenge of non-convexity, we utilize randomized sampling in the process of tracking experts. Next, we present a novel algorithm called FTPL-A that dynamically maintains a group of FTPL experts and combines them with an advanced meta-algorithm to obtain \(O()\) adaptive regret for any interval of length \(\). Moreover, we demonstrate that FTPL-A also attains an \((T^{}(V_{T}+1)^{})\) dynamic regret bound. Finally, we discuss the application to online constrained meta-learning and conduct experiments to verify the effectiveness of our methods.

## 1 Introduction

Online learning is a powerful model for sequential decision-making tasks, supported by well-established theoretical guarantees (Cesa-Bianchi and Lugosi, 2006; Orabona, 2019). It can be regarded as a repeated game between a learner and an adversary. In each round \(t\), the learner first selects a decision \(_{t}\), where \(\) is a candidate set. Then a loss function \(f_{t}()\) is revealed and the learner suffers a loss \(f_{t}(_{t})\). The goal of the learner is to minimize the cumulative loss \(_{t=1}^{T}f_{t}(_{t})\) over all rounds. Traditionally, the performance measure is _static regret_ or simply _regret_

\[R(T)=_{t=1}^{T}f_{t}(_{t})-_{ }_{t=1}^{T}f_{t}(), \]

defined as the difference between the cumulative loss of the online learner and that of the optimal decision chosen in hindsight. During the past decades, numerous algorithms and theories have been developed to minimize static regret. An extensively researched setting is online convex optimization (OCO) (Shalev-Shwartz et al., 2012), in which the losses are assumed to be convex and Lipschitz-continuous. For OCO, many efficient methods such as online gradient descent (OGD) attain \(O()\) regret (Zinkevich, 2003), which is known to be minimax optimal (Abernethy et al., 2008).

On the other hand, when the losses are non-convex, minimizing regret becomes computationally challenging. This is because minimizing regret implies optimization, and general non-convex optimization is known to be NP-hard. In light of this computational barrier, some recent works examine the notion of _local regret_ instead (Hazan et al., 2017; Aydore et al., 2019; Hallak et al., 2021; Guan et al., 2023). However, these researches focus on finding local optima and hence do not guarantee vanishing regret that grows sublinear in \(T\). To ensure vanishing regret, another class of studies assumes access to a sampling oracle (Krichene et al., 2015; Yang et al., 2018; Heliou et al., 2020) or an offline optimization oracle (Agarwal et al., 2019; Suggala and Netrapalli, 2020). Particularly, Suggala and Netrapalli (2020) have demonstrated that for general non-convex and Lipschitz-continuous losses, FTPL can achieve \(O()\) regret, which matches the optimal result in the convex setting. However, their study solely focuses on static environments, as it uses regret as the only metric to measure the performance. In dynamic environments, the distribution of loss functions may change over time, causing a shift in the optimal decision. In this scenario, the static regret in (1) is not a suitable measure since the comparator is fixed (Zhang, 2020; Cesa-Bianchi and Orabona, 2021). To overcome this limitation, studies in OCO have introduced new performance metrics: dynamic regret (Zinkevich, 2003; Zhang et al., 2018) and adaptive regret (Hazan and Seshadhri, 2007; Daniely et al., 2015).

In **dynamic regret**(Zinkevich, 2003), the learner is compared against a sequence of local minimizers:

\[R_{D}^{}=R_{D}(_{1}^{},,_{T}^{})= _{t=1}^{T}f_{t}(_{t})-_{t=1}^{T}f_{t}(_{t}^{}), \]

where \(_{t}^{}_{}f_{t}()\) is a minimizer of \(f_{t}()\) over domain \(\). It is recognized that in the worst-case scenario, achieving sublinear dynamic regret is impossible unless we introduce certain constraints on the comparator sequence or the function sequence (Jadbabaie et al., 2015). One such example is the functional variation defined below

\[V_{T}=_{t=2}^{T}_{}|f_{t}()-f_{t-1}( )|. \]

If the value of \(V_{T}\) is known in advance, Besbes et al. (2015) showed that a restarted OGD achieves \(O(T^{}(V_{T}+1)^{})\) dynamic regret for convex functions.

**Strongly adaptive regret**(Daniely et al., 2015) is another widely used performance metric in dynamic environments. It is defined as the maximum static regret over any interval of length \(\):

\[R_{A}(T,)=_{[s,s+-1][T]}\{_{t=s}^{s+-1}f_{t} (_{t})-_{}_{t=s}^{s+-1}f_{t}( )\}. \]

Given the dynamic nature of environments, where the optimal decisions can vary across intervals, minimizing static regret over any interval of length \(\) is essentially competing against changing comparator. For convex functions, the best known result for strongly adaptive regret is \(O()\)(Jun et al., 2017).

In this paper, we consider minimizing dynamic regret and adaptive regret in the non-convex setting. For dynamic regret minimization, we first propose an algorithm named FTPL-D by restarting FTPL periodically and establish an \(O(T^{}(V_{T}+1)^{})\) dynamic regret bound, which requires the prior knowledge of \(V_{T}\) to set the optimal restarting frequency. To get rid of this limitation, we then propose the second algorithm named FTPL-D+ by running multiple instances of FTPL-D with different restarting frequencies and combining them with a meta-algorithm to track the best one. Given that the loss functions are non-convex, deterministic algorithms cannot achieve a vanishing regret (Suggala and Netrapalli, 2020). Hence, we choose Hedge (Cesa-Bianchi and Lugosi, 2006) with _randomized sampling_ as our meta-algorithm. Correspondingly, in each round, we sample one expert based on their weights instead of computing the weighted average of all experts. We prove that FTPL-D+ enjoys the same order of dynamic regret bound as that of FTPL-D without the prior knowledge of \(V_{T}\).

For adaptive regret minimization, we propose an algorithm named FTPL-A, where we construct a set of intervals dynamically and instantiate an expert of FTPL to minimize the static regret for every interval. To combine these experts whose numbers vary at different time steps, we use AdaNormalHedge (Luo and Schapire, 2015) that can deal with sleeping experts as our meta-algorithmand again employ _randomized sampling_ in it to select experts. We prove that FTPL-A achieves an \(O()\) adaptive regret bound for any interval of length \(\). Besides, we demonstrate that our FTPL-A also obtains an \((T^{}(V_{T}+1)^{})\) dynamic regret bound, indicating its effectiveness in minimizing dynamic regret and adaptive regret simultaneously. In brief, our dynamic regret and strongly adaptive regret for non-convex loss functions are on the same order as those for convex functions. We compare our results and previous ones in Table 1.

Moreover, we discuss the application of our methods to online constrained meta-learning (Xu and Zhu, 2023) and conduct experiments. The empirical results demonstrate the effectiveness of our methods in dynamic regret and adaptive regret minimization. We highlight the main contributions of this paper below.

* For dynamic regret minimization, we first propose a novel algorithm named FTPL-D and establish \(O(T^{}(V_{T}+1)^{})\) dynamic regret. To eliminate the dependence on prior knowledge of \(V_{T}\), we then propose FTPL-D+ and provide a dynamic regret bound of the same order.
* For adaptive regret minimization, we develop a novel algorithm named FTPL-A and establish an \(O()\) strongly adaptive regret bound. Moreover, we prove that FTPL-A also ensures an \((T^{}(V_{T}+1)^{})\) dynamic regret bound.
* We discuss the application to online constrained meta-learning and conduct experiments to verify the effectiveness of our methods.

## 2 Related Work

In this section, we give a brief introduction to previous works in OCO and online non-convex learning. More related works are reviewed in Appendix B.

Online Convex OptimizationThe existing works in OCO mostly focus on static regret. For instance, OGD (Zinkevich, 2003) and follow the regularized leader (Hazan et al., 2016) both achieve an \(O()\) regret bound. The \(O()\) regret bound for Lipschitz-convex functions is known to be minimax optimal (Abernethy et al., 2008). To cope with dynamic environments, Zinkevich (2003) introduced the dynamic regret in (2), and there are numerous studies dedicated to the worst-case scenario because of its mathematical tractability (Besbes et al., 2015; Jadbabaie et al., 2015; Mokhtari et al., 2016; Yang et al., 2016; Zhang et al., 2017; Baby and Wang, 2019; Zhao and Zhang, 2021; Wan et al., 2023). As mentioned earlier, dynamic regret is often bounded in terms of certain regularities of the comparator sequence or the function sequence. In particular, Besbes et al. (2015) proposed the functional variation in (3) to evaluate the movement of loss functions. They equipped restarted OGD with an \(O(T^{}(V_{T}+1)^{})\) dynamic regret bound, but require to know \(V_{T}\) beforehand.

Besides dynamic regret, another metric in dynamic environments is adaptive regret. Adaptive regret has been examined under the setting of prediction with expert advice (PEA) (Littlestone and Warmuth, 1994; Freund et al., 1997; Gyorgy et al., 2012; Adamskiy et al., 2012; Luo and Schapire, 2015) and OCO (Hazan and Seshadhri, 2007; Daniely et al., 2015; Jun et al., 2017; Zhang et al., 2019, 2021; Yang et al., 2024). In the following, we focus on the latter one. To minimize (4), Daniely et al. (2015) developed a meta-algorithm called strongly adaptive online learner and used it to design

   Method & Loss & Metric & Regret Bounds \\  
*Restarted OGD (Besbes et al., 2015) & convex & D-R & \(O(T^{}(V_{T}+1)^{})\) \\
* indicates that the algorithm requires prior knowledge of \(V_{T}\). The \(()\)-notation omits logarithmic factors on \(T\). Abbreviations: dynamic regret \(\) D-R, strongly adaptive regret \(\) SA-R.
two-layer structured online algorithms, which enjoy an \(O( T)\) strongly adaptive regret bound for convex functions. Later, Jun et al. (2017) proposed a novel meta-algorithm and improved the strongly adaptive regret bound to \(O()\).

Online Non-convex LearningTo avoid the NP-hardness of non-convex optimization, Hazan et al. (2017) proposed a computational tractable notion of local regret and developed algorithms that attain the optimal local regret bound efficiently. Hallak et al. (2021) extended the local regret minimization framework to non-smooth settings. Guan et al. (2023) further examined the cases where a limited number of gradient oracles or value oracles are available. Although these techniques can efficiently minimize local regret, they do not guarantee to find the global optima and achieve vanishing regret.

Another line of work still focuses on the notion of static regret, but assumes access to a sampling oracle or an offline optimization oracle. Assuming access to a sampling oracle, Krichene et al. (2015) proved that the Hedge algorithm is capable of achieving \(O()\) regret over a specific feasible set. Yang et al. (2018) later improved the regret bound to \(O()\) by partitioning the feasible set with a layered structure and using a novel weighting method. Besides, Heliou et al. (2020) examined the dual averaging (DA) algorithm with an imperfect value-feedback model of the loss function, which also attained an \(O()\) regret bound. However, these algorithms rely on a sampling oracle on a continuum that is computationally _intractable_.

Under the hypothesis that the algorithm has access to an offline optimization oracle, Agarwal et al. (2019) showed that FTPL achieves \(O(T^{})\) regret for general non-convex loss functions with Lipschitz continuousity. In the same setting, Suggala and Netrapalli (2020) improved the regret bound to \(O()\). It should be noted that assuming access to an offline optimization oracle is reasonable since some simple algorithms such as stochastic gradient descent, are able to find approximate global optima quickly, even for non-convex objective functions.

By contrast, the studies on dynamic environments are limited. Aydore et al. (2019) introduced a variant of local regret for dynamic environments and proposed a novel algorithm to minimize it. Heliou et al. (2020) obtained \(O(T^{}(V_{T}+1)^{})\) dynamic regret with their imperfect feedback method. However, it needs the prior knowledge of \(V_{T}\) to choose the optimal stepsize in DA and still relies on a computationally intractable sampling oracle.

## 3 Preliminaries

In this section, we introduce the problem setting and FTPL (Suggala and Netrapalli, 2020).

**Assumption 1**.: _The feasible set \(\) is bounded and has \(_{}\)-diameter at most \(D\), i.e., for all \(,\), \(\|-\|_{} D\)._

**Assumption 2**.: _The sequence of loss functions \(f_{t}()\) are \(L\)-Lipschitz with respect to \(_{1}\)-norm, i.e., for all \(,\), \(|f_{t}()-f_{t}()| L\|- \|_{1}\)._

We proceed to introduce FTPL in Algorithm 1, which is the subroutine of our algorithms. FTPL relies on the offline optimization oracle below (Suggala and Netrapalli, 2020).

**Definition 1**.: **An offline optimization oracle** takes input as a function \(f():\) and a \(d\)-dimensional vector \(\), and returns an approximate minimizer of \( f()-,\). An optimization oracle is called "\((,)\)-approximate optimization oracle" if it returns \(^{*}\) such that

\[f(^{*})-,^{*}_{ }[f()-,]+( +\|\|_{1}).\]

We denote such an optimization oracle with \(_{,}(f_{i}-)\).

Given the access to an \((,)\)-approximate optimization oracle, the main idea of FTPL is to add a small perturbation to the cumulative loss and follow the "perturbed" leader:

\[_{t}=_{,}(_{i=1}^{t-1}f_{i}- _{t},), \]

where \(_{t}^{d}\) is a random perturbation such that \(_{t,j}\), the \(j\)-th coordinate of \(_{t}\), is sampled from the exponential distribution with parameter \(\), that is

\[\{_{t,j}\}_{j=1}^{d}(). \]```
1:Input: feasible set \(\), approximation optimization oracle \(_{,}\), parameter of exponential distribution \(\)
2:for\(t=1\)to\(T\)do
3: Generate random vector \(_{t}\) by (6)
4: Predict \(_{t}\) according to (5)
5: Observe loss function \(f_{t}()\)
6:endfor
```

**Algorithm 1** Follow the Perturbed Leader (FTPL)

_Remark_.: We choose FTPL as our subroutine due to its simplicity and reasonable assumption of the optimization oracle. However, it is worth noting that our methods are not limited to FTPL alone. We can choose any algorithm that has a static regret guarantee in online non-convex learning and extend it to dynamic environments.

## 4 Online Non-convex Learning with Dynamic Regret

In this section, we propose our first algorithm named FTPL-D based on FTPL and establish a dynamic regret bound with the prior knowledge of \(V_{T}\). After that, we propose the second algorithm named FTPL-D+ which is equipped with the same bound without the prior knowledge of \(V_{T}\).

### Follow the Perturbed Leader with Dynamic Regret

Let dynamic regret defined in (2) be the performance measure. Our goal is to bound the dynamic regret by the functional variation \(V_{T}\).

Following the work of Besbes et al. (2015), we apply the restarting strategy to FTPL. The key idea is to partition the time horizon \(T\) into consecutive intervals, where the length of each interval is controlled by a parameter \(\), and then restart FTPL at the beginning of each interval. Our algorithm, which we call follow the perturbed leader with dynamic regret (FTPL-D), is described in Algorithm 2. At each time step \(t\), we mark the beginning of the current interval as \(s_{}\) in Step 3. Then FTPL-D follows the perturbed leader within the current interval, which spans from \(s_{}\) to \(t\). Specifically,

\[_{t}=_{,}(_{i=s_{}}^{t-1}f_{i}- _{t},). \]

The following theorem presents the dynamic regret of Algorithm 2 for general non-convex functions.

**Theorem 1**.: _Under Assumptions 1 and 2, and setting \(=1/\), Algorithm 2 ensures_

\[[R_{D}^{*}]}+2 V_{T},\]

_where_

\[c(,,)=125DL^{2}d^{}+}}{20}+}{20}+2dL. \]

_If the value of \(V_{T}\) is known, by choosing \(=\{(})^{} ,T\}\), we have_

\[[R_{D}^{*}] O((1++ T)T^{ {3}{3}}(V_{T}+1)^{}).\]_Remark_.: Theorem 1 demonstrates that for general non-convex functions, when \(=O(1/)\) and \(=O(1/T)\), which mirrors the settings used by Suggala and Netrapalli (2020, Page 4), our FTPL-D achieves an \(O(T^{}(V_{T}+1)^{})\) dynamic regret bound that matches the existing bound for convex functions (Besbes et al., 2015).

### Ftpl-D+

The dynamic regret in Theorem 1 requires the prior knowledge of \(V_{T}\) to choose the optimal parameter \(\). However, in practice, \(V_{T}\) is usually unknown, posing a challenge in selecting the best \(\). Studies on OCO also encounter the problem of searching for the optimal parameter for their algorithms (van Erven and Koolen, 2016; Zhang et al., 2018; Wan et al., 2021, 2024). The main idea in their solutions is activating multiple instances of their algorithms as experts, and tracking the best one with a meta-algorithm such as Hedge (Cesa-Bianchi and Lugosi, 2006), which assigns a weight to each expert and computes the weighted average of their advice.

Inspired by the above idea, we choose FTPL-D as the expert algorithm and propose our second algorithm called improved FTPL-D (FTPL-D+). Note that both the loss functions and the feasible set are non-convex, so we cannot combine different experts' predictions by the weighted average. According to the discussion of Cesa-Bianchi and Lugosi (2006, Chapter 4.1), we can tackle this issue by randomized sampling, where we sample one expert according to the weights, and then output the prediction of that expert.

The details of FTPL-D+ are shown in Algorithm 3, and we describe the main steps below. We first define a set \(=\{_{1},...,_{N}\}\) of \(N\) values for parameter \(\). Then we activate a set of experts \(\{E_{i}_{i}\}\) by invoking Algorithm 2 as \(E_{i}=(,_{i},_{,},)\). For each expert \(E_{i}\), we denote its weight at round \(t\) as \(w_{t}^{i}\), which is initiated as \(w_{1}^{i}=1/N\). At each round \(t\), we receive a prediction \(_{t}^{i}\) from the expert \(E_{i}\). To utilize these predictions \(\{_{t}^{i} i[N]\}\), we select \(_{t}^{i}\) with the probability \(w_{t}^{i}\) and submit it as the output \(_{t}\), i.e., \(P(_{t}=_{t}^{i})=w_{t}^{i}\). After the loss function is revealed, we update the weights according to the following rule (Cesa-Bianchi and Lugosi, 2006):

\[w_{t+1}^{i}=^{i}e^{- f_{t}(_{t}^{i})}}{_{j=1}^{N} w_{t}^{j}e^{- f_{t}(_{t}^{j})}}, \]

where \(>0\) is the step size. We present the dynamic regret of Algorithm 3 in the following theorem.

```
1:Input: feasible set \(\), \(=\{_{1},...,_{N}\}\), step size \(\)
2: Activate a set of experts \(\{E_{i}_{i}\}\) by invoking Algorithm 2 for each \(_{i}\)
3: For each expert \(E_{i}\), set \(w_{1}^{i}=1/N\) for \(i[N]\)
4:for\(t=1\)to\(T\)do
5: Receive \(_{t}^{i}\) from each expert \(E_{i}\)
6: Draw \(_{t}\) according to \(P(_{t}=_{t}^{i})=w_{t}^{i}\)
7: Output \(_{t}\) and observe loss function \(f_{t}()\)
8: Update the weight of experts according to (9)
9: Send loss function \(f_{t}()\) to each expert \(E_{i}\)
10:endfor
```

**Algorithm 3** FTPL-D+

_Theorem 2_.: Let \(=\{_{i}=2^{i} i=1, N\}\) where \(N=_{2}T\), and \(=}\). Under Assumptions 1 and 2, Algorithm 3 ensures

\[[R_{D}^{*}] O((1++ T)T^{}(V_{T}+1)^{}).\]

_Remark_.: Theorem 2 shows that our FTPL-D+ achieves the same order of dynamic regret bound as Algorithm 2 without the prior knowledge of \(V_{T}\).

## 5 Online Non-convex Learning with Adaptive Regret

In this section, we further consider strongly adaptive regret defined in (4) as the performance metric in dynamic environments. We develop our third algorithm, named follow the perturbed leader with adaptive regret (FTPL-A), and establish its adaptive and dynamic regret bounds.

Previous research on adaptive regret (Hazan and Seshadhri, 2007; Daniely et al., 2015; Wang et al., 2024) has adopted a two-layered framework to design adaptive online algorithms. The basic idea is to dynamically construct a set of intervals and run an expert algorithm to minimize the static regret within each interval. These experts are then combined by a meta-algorithm. Building upon this idea, our FTPL-A includes three components: an expert algorithm, a set of intervals, and a meta-algorithm. In the following, we illustrate them separately.

First, we use FTPL as the expert algorithm, which enjoys \(O()\) static regret for a given interval \(I\)(Suggala and Netrapalli, 2020). Then, following the work of Daniely et al. (2015), we build geometric covering (GC) intervals shown in Fig. 2:

\[=_{k}_{k},_{k}=\{ [i 2^{k},(i+1) 2^{k}-1]:i\}.\]

That is, each \(_{k}\) is a partition of \(\{1,,2^{k}-1\}\) into consecutive intervals of length \(2^{k}\). For each interval \(I\), we activate an instance of FTPL as the expert \(E_{I}\) to minimize the regret over \(I\).

Next, we choose AdaNormalHedge (Luo and Schapire, 2015) to track the best expert. Similar to the Hedge forecaster we used in FTPL-D+, we maintain a weight for each expert, but in a specific form of a potential function:

\[(R,C)=(^{2}}{3C}),\]

where \([x]_{+}=(0,x)\) and \((0,0)\) is defined to be 1, and the weight function is determined by

\[w(R,C)=((R+1,C+1)-(R+1,C-1)).\]

Due to the non-convexity of the loss functions, as explained in Section 4.2, we use randomized sampling to generate the output.

Figure 2: Graphic illustration of Geometric Covering intervals (Daniely et al., 2015) from Figure 1 of Zhang et al. (2019). In the figure, each interval is denoted by \([]\).

The procedure of FTPL-A is summarized in Algorithm 4. We describe the main steps below. For brevity, we denote the set of intervals starting from round \(t\) as \(_{t}\), and the set of active experts at round \(t\) as \(_{t}\). In Step 3, for each interval \(I_{t}\), we run an instance of FTPL as \(E_{I}\) with the optimal parameters. In Step 4, the two variables \(R_{t-1,I}\) and \(C_{t-1,I}\) are initialized as \(0\) for \(E_{I}\), where \(R_{t-1,I}\) denotes the expected regret of \(E_{I}\) up to round \(t-1\), and \(C_{t-1,I}\) denotes the expected absolute regret. In Steps 5 and 7, the active expert set \(A_{t}\) is maintained by adding new experts and removing expired ones. In Step 8, the expert's weight is calculated by :

\[w_{t,I}=,C_{t-1,I})}{_{E_{I}_{t}}w(R_{t-1,I},C_{t-1,I})}. \]

Similarly, we sample one expert (prediction) according to the weights in Step 9, and submit it as the output in Step 10. In Step 11, the instantaneous expected loss is computed as the weighted average of different experts' losses:

\[_{t}(_{t})=_{E_{I}_{t}}w_{t,I}f_{t}( _{t,I}), \]

where \(_{t,I}\) denotes the prediction of expert \(E_{I}\) at round \(t\). In Step 12, we update \(R_{t,I}\) and \(C_{t,I}\) by

\[R_{t,I}=R_{t-1,I}+_{t}(_{t})-f_{t}(_{t,I})}{ dDL}, C_{t,I}=C_{t-1,I}+_{t}(_{t})-f_{t}( _{t,I})|}{dDL}. \]

The following theorem demonstrates the strongly adaptive regret bound of Algorithm 4 for general non-convex loss functions.

**Theorem 3**.: _Under Assumptions 1 and 2, Algorithm 4 ensures_

\[[R_{A}(T,)](8c(,,)+8)=O(++^{}),\]

_where \(g(T) 1+ T+(1+_{2}T)+ {2}\) and \(c(,,)\) is given in (8)._

_Remark_.: Theorem 3 demonstrates that when \(=O(1/)\) and \(=O(1/)\), FTPL-A is strongly adaptive with the same order of \(O()\) regret as that for convex functions .

Furthermore, we notice that Zhang et al. [2018b] investigated the relationship between adaptive regret and dynamic regret. They showed that dynamic regret can be bounded by the strongly adaptive regret and the functional variation. According to their findings, we have the following theorem that presents the dynamic regret bound of Algorithm 4.

**Theorem 4**.: _Under Assumptions 1 and 2, Algorithm 4 ensures_

\[[R_{D}^{*}]((1++ T)T ^{}(V_{T}+1)^{}).\]

_Remark_.: Theorem 4 shows that FTPL-A achieves nearly the same dynamic regret as FTPL-D+, indicating its ability to minimize adaptive and dynamic regret simultaneously. However, compared to FTPL-A, FTPL-D+ does not require the construction of GC intervals and uses a simpler meta-algorithm, making it easier to implement.

## 6 Application to Online Constrained Meta-Learning

Online non-convex learning offers a wide range of applications . By applying our methods, we can extend these applications to dynamic environments. In this section, we discuss the application to online constrained meta-learning  and conduct experiments to support our theoretical results.

Online constrained meta-learning.Meta-learning, also known as learning-to-learn, focuses on acquiring a prior meta-parameter that enables fast adaptation to new tasks. Recently, Xu and Zhu  proposed the setting of online constrained meta-learning, which aims to learn the meta-parameter from a sequence of constrained learning tasks \(\{_{1},,_{T}\}\). Each task \(_{t}\) is characterized by its data distributions \(_{t}\) and constraint limits \(c_{t}\). At each round \(t\), a training dataset \(_{t}^{tr}\), which contains data sampled i.i.d. from \(_{t}\), is available to the learner. Then the learner adapts the meta-parameter \(_{t}\) to the task-specific parameter \(_{t}\) by a within-task algorithm \(lg\) with \(_{t}^{tr}\) and \(c_{t}\). Afterthe learner deploys \(_{t}\), it obtains a validation dataset \(_{t}^{val}\) by sampling data from \(_{t}\) and defines the meta-objective function at current round as

\[_{t}^{val}()=(lg(,_{t}^{tr}, c_{t}),_{t}^{val}),\]

where \((,_{t}^{val})\) denotes the loss of \(\) on \(_{t}^{val}\). Xu and Zhu (2023) updated the meta-parameter by using FTPL to the non-convex meta-objective function on all revealed tasks. With this approach, they effectively controlled the static regret of the meta-objective function between \(_{t}\) and the fixed optimal meta-parameter \(^{*}\). However, when the distribution of tasks is shifting, the fixed \(^{*}\) may not be adapted to each task well. In such a scenario, it is better to use dynamic regret or adaptive regret as the performance metric, since they measures the learner's performance against the changing optimal meta-parameter. By applying Algorithms 3 and 4, we can minimize the two metrics efficiently. To evaluate our methods, we conduct experiments on a sequence of tasks in imitation learning and another in few-shot learning, respectively.

### Experiments on meta-imitation learning

Setup.We use the demonstration data given by Huang et al. (2019) and set the total number of tasks \(T=200\). At each round \(t[T]\), a human expert writes a different letter in a free space without obstacle. The learner can observe a demonstration of the letter and is asked to write the same letter in a cluttered environment. We provide the details of problem formulation and implement setting in Appendix C.1. By controlling the letter to be imitated for each task, we simulate two types of dynamic environments: (i) _abruptly changing_ environments; (ii) _gradually evolving_ environments. In (i), we split the time horizon evenly into \(4\) stages, and set the imitation target as capital letters "M", "E", "T" and "A" for the \(4\) stages. In this way, the optimal meta-parameter \(^{*}\) drifts notably every \(50\) round. In (ii), we choose the capital letter "A" as the imitation target. Additionally, we rotate the letter "A" at a small random angle \(_{t}(0,0.05]\) in each round. This ensures the optimal meta-parameter \(^{*}\) undergoes slow and smooth shifts. We compare our FTPL-D+ and FTPL-A with FTPL (Suggala and Netrapalli, 2020) in the above two scenarios.

Results.We repeat the experiments five times with different random seeds and plot the loss (mean and standard deviation) in Fig. 3. More results are provided in Appendix C.1. For abruptly changing environments, Fig. 3(a) shows that, in comparison to FTPL, our methods adapt more quickly to the new task distribution after the demonstration shifts, which occurs at \(T=50\), \(100\), and \(150\). Fig. 3(b) demonstrates that our methods perform significantly better than FTPL in terms of cumulative loss. For gradually evolving environments, it can be observed from Fig. 3(c) that as the angle of rotation increases, both our methods maintain low loss, while FTPL exhibits a significant increase in instantaneous loss after \(T=100\). Fig. 3(d) also shows a notable advantage in cumulative loss. Moreover, our FTPL-D+ and FTPL-A achieve comparable performance in the two types of dynamic environments.

### Experiments on few-shot image classification with robustness

Setup.We conduct the experiments on CUB-200-2011 (referred to as CUB) dataset (Wah et al., 2011), which includes 200 fine-grained categories of birds. There are \(T=150\) of robust 5-way 5-shot image classification tasks, each task containing images from 5 classes with only 5 training samples per class. At each round \(t[T]\), the model is updated using a few data samples and is required to have high accuracy on both clean and perturbed test data. We provide the details of problem

Figure 3: Results of meta-imitation learning in two types of dynamic environments. The first two figures are for abruptly changing environments (Env1), while the latter ones are for gradually evolving environments (Env2).

formulation and implement setting in Appendix C.2. We resize the input images to \(84 84\), and apply the same data augmentation as in Ye et al. (2021) and Xu and Zhu (2023). A four-layer convolutional neural network (Conv-4) is employed as the backbone, comprising four blocks. Each block consists of a convolutional layer with 64 kernels of size 3 x 3, stride 1, and zero padding, followed by a batch normalization layer, a ReLU activation function, and lastly a 2 x 2 max-pooling layer. Following the convolutional layers, the network uses a fully connected linear layer with 5 neurons as a classifier to output the prediction for the input image. To simulate real-world dynamic environments, we select three groups of bird categories from the CUB dataset based on their habitats: water birds, forest birds, and grassland birds. We then split the time horizon \(T\) evenly into 3 stages, within each stage the tasks are sampled from one group of bird categories. This simulates _abruptly changing_ environments where the optimal meta-parameter \(^{*}\) drifts every \(50\) round. We compare our FTPL-D+ and FTPL-A with FTPL (Suggala and Netrapalli, 2020).

Metrics and results.The performance of the task-specific model is evaluated by: (i) clean accuracy; (ii) PGD accuracy; (iii) B-score. The clean accuracy is the accuracy on the clean test dataset, and PGD accuracy is the accuracy on the corrupted test dataset, which is obtained by adding perturbation on the clean test dataset by the Projected Gradient Descent (PGD) method (Kurakin et al., 2018). Balance Score (B-score) (Ye et al., 2021) measures both clean accuracy and PGD accuracy. It is defined as \(=2()/(+)\), where CA and PA denote clean accuracy and PGD accuracy respectively. We report the three metrics against the number of tasks in Fig. 3. As evidenced by the results, FTPL-D+ and FTPL-A attain comparable performance, and both significantly outperform FTPL in terms of all three metrics after the environmental changes at \(T=50\) and \(100\). This suggests that our methods are effective in rapidly adapting to the new task distribution.

## 7 Conclusion and Future Work

This paper investigates online non-convex learning in dynamic environments, using dynamic regret and adaptive regret as performance metrics. For dynamic regret minimization, we propose FTPL-D with an \(O(T^{}(V_{T}+1)^{})\) regret bound. To eliminate the dependence on prior knowledge of \(V_{T}\), we propose FTPL-D+, which runs multiple instances of FTPL-D and uses a meta-algorithm to track the best one. For adaptive regret minimization, we propose FTPL-A with a regret bound of \(O()\). Finally, we discuss the application to online constrained meta-learning, and the conducted experiments verify the effectiveness of our methods.

Currently, we bound the dynamic regret by the functional variation. A natural problem is whether we can derive regret bounds based on other regularities, such as the path length of comparators that is widely used in prior works for non-stationary online convex optimization (Zhang et al., 2018; Zhao et al., 2020, 2024; Baby and Wang, 2021; Cutkosky, 2020). This is left as a future work to explore.