# Adversarial Resilience in Sequential Prediction

via Abstention

Surbhi Goel

University of Pennsylvania

surbhig@cis.upenn.edu

&Steve Hanneke

Purdue University

steve.hanneke@gmail.com

Shay Moran

Technion and Google Research, Israel

smoran@technion.ac.il

&Abhishek Shetty

University of California, Berkeley

shetty@berkeley.edu

###### Abstract

We study the problem of sequential prediction in the stochastic setting with an adversary that is allowed to inject clean-label adversarial (or out-of-distribution) examples. Algorithms designed to handle purely stochastic data tend to fail in the presence of such adversarial examples, often leading to erroneous predictions. This is undesirable in many high-stakes applications such as medical recommendations, where abstaining from predictions on adversarial examples is preferable to misclassification. On the other hand, assuming fully adversarial data leads to very pessimistic bounds that are often vacuous in practice.

To move away from these pessimistic guarantees, we propose a new model of sequential prediction that sits between the purely stochastic and fully adversarial settings by allowing the learner to abstain from making a prediction at no cost on adversarial examples, thereby asking the learner to make predictions with certainty. Assuming access to the marginal distribution on the non-adversarial examples, we design a learner whose error scales with the VC dimension (mirroring the stochastic setting) of the hypothesis class, as opposed to the Littlestone dimension which characterizes the fully adversarial setting. Furthermore, we design learners for VC dimension 1 classes and the class of axis-aligned rectangles, which work even in the absence of access to the marginal distribution. Our key technical contribution is a novel measure for quantifying uncertainty for learning VC classes, which may be of independent interest.

## 1 Introduction

Consider the problem of sequential prediction in the realizable setting, where labels are generated from an unknown \(f^{*}\) belonging to a hypothesis class \(\). Sequential prediction is typically studied under two distributional assumptions on the input data: the stochastic setting where the data is assumed to be identically and independently distributed (i.i.d) according to some fixed (perhaps unknown) distribution, and the fully-adversarial setting where we make absolutely no assumptions on the data generation process. A simple empirical risk minimzation (ERM) strategy works for the stochastic setting where the learner predicts according to the best hypothesis on the data seen so far. The number of mistakes of this strategy typically scales with the Vapnik-Chervonenkis (VC) dimension of the underlying hypothesis class \(\). However, in the fully adversarial setting, this strategy can lead to infinite mistakes even for classes of VC dimension 1 even if the adversary is required to be consistent with labels from \(f^{*}\). The Littlestone dimension, which characterizes the complexity of sequential prediction in fully-adversarial setting, can be very large and often unbounded compared tothe VC dimension . This mismatch has led to the exploration of beyond worst-case analysis for sequential prediction .

In this work, we propose a new framework that sits in between the stochastic and fully-adversarial setting. In particular, we consider sequential prediction with an adversary that injects adversarial (or out-of-distribution) examples in a stream of i.i.d. examples, and a learner that is allowed to abstain from predicting on adversarial examples. A natural motivation for our framework arises in medical diagnosis where the goal is to predict a patient's illness based on symptoms. In cases where the symptoms are not among the commonly indicative ones for the specific disease, or the symptoms may suggest a disease that is outside the scope of the doctor's knowledge, it is safer for the doctor to abstain from making a prediction rather than risk making an incorrect one. Similarly, for self-driving cars, in cases where the car encounters weather conditions outside of its training, or unknown information signs, it is better for the algorithm to hand over access to the driver instead of making a wrong decision which could end up being fatal.

In the proposed framework, the learner's goal is to minimize erroneous predictions on examples that the learner chooses to predict on (i.i.d. or adversarial) while refraining from abstaining on too many i.i.d. examples. If the learner was required to predict on every example, then the adversary could produce a fully-adversarial sequence of examples which would force the learner to make many erroneous predictions. The abstention option allows us to circumvent this challenge and handle any number of adversarial injections without incurring error proportional to the number of injections. In this framework, we can ask the following natural question:

_Is there a statistical price for certainty in sequential prediction?_

In particular, can we recover stochastic-like guarantees in the presence of an adversary if we are allowed to abstain from predicting on adversarial examples? A priori, it is not clear where on the spectrum between the fully-adversarial and stochastic models, the complexity of this problem lies. The main challenges arise from the fact that the adversary fully controls the injection levels and provides no feedback about which examples were adversarial, and the learner has to perform one-sample outlier detection, which is nearly impossible. Despite this, we show that it is possible to guarantee certainty in a statistically efficient manner.

### Main Contributions

We summarize the main contributions of our work:

* We formalize a new framework of beyond-worst case learning which captures sequential prediction on a stochastic sequence with a clean-label injection-only adversary. With the option of abstention, our framework allows for any number of injections by the adversary without incurring error proportional to the number of injections. Our notion of error simultaneously guarantees few mistakes on classified data while ensuring low abstention rate on non-adversarial data. Our framework naturally connects to uncertainty quantification and testable learning.
* In our framework, we design an algorithm that achieves error \(O(d^{2} T)\) for classes with VC dimension \(d\) for time horizon \(T\), given access to the marginal distribution over the i.i.d. examples. This allows us to get (up to a factor of \(d\)) the guarantees of the stochastic setting while allowing for any number of adversarial injections.
* We further design an algorithm that achieves \(O()\) error for the special (but important) case of VC dimension 1 classes without any access to the marginal distribution over the i.i.d. examples. Similar ideas also allow us to design an algorithm for the class of axis-aligned rectangles in any dimension \(d\) with error \(O(d)\).

Our algorithms uses a novel measure of uncertainty for VC classes to identify regions of high uncertainty (where the learner abstains) or high information gain (where the learner predicts and learn from their mistakes). The measure uses structural properties of VC classes, in particular, shattered sets of varying sizes. In the known distribution setting, our measure is easy to compute, however for the unknown distribution setting, we show how to design a proxy using only the examples we have seen so far using a leave-one-out type strategy.

### Related Work

**Beyond-worst case sequential prediction.** Due to pessimistic nature of bounds in adversarial online learning, there are several frameworks designed to address this issue. One approach is to consider 

[MISSING_PAGE_FAIL:3]

by \(|_{S}\), the class \(|_{S}=f: f(x_{i})=y_{i}}\). When the data set contains a single point \(S=(x,y)}\), it will be convenient to denote \(|_{S}\) as \(^{x y}\).

**Protocol.** At the start, the adversary (or nature) picks a distribution \(\) over the domain \(\) and the labelling function \(f^{}\). We will be interested in both the setting where the learner knows the distribution \(\) and the setting where the learner does not know the distribution \(\). In the traditional sequential prediction framework, the learner sees input \(x_{t}\) at time \(t\) and makes a prediction \(_{t}\) and observes the true label \(y_{t}\). The main departure of our setting from this is that an adversary also decides before any round whether to inject an arbitrary element of \(\) (without seeing \(x_{t}\)). We denote by \(_{t}\) the instance after the adversarial injection (\(_{t}=x_{t}\) or \(_{t} x_{t}\)). The learner then observes \(_{t}\) and makes a prediction \(_{t}\) and observes the true label \(y_{t}\), as in the traditional sequential prediction framework. We present this formally below.

**Protocol 1** Sequential Prediction with Adversarial Injections and Abstentions

**Adversary (or nature) initially selects distribution \(()\) and \(f^{}\). The learner does not have access to \(f^{}\). The learner may or may not have access to \(\).**

**for \(t=1,,T\)do**

Adversary decides whether to inject an adversarial input in this the round \((c_{t}=1)\) or not \((c_{t}=0)\).

**if \(c_{t}=1\)then**

Adversary selects any \(_{t}\)

**else**

Nature selects \(x_{t}\), and we set \(_{t}=x_{t}\).

Learner receives \(_{t}\) and outputs \(_{t}\{0,1,\}\) where \(\) implies that the learner abstains.

Learner receives clean label \(y_{t}=f^{}(_{t})\).

_Remark_.: It is important to note we are in the realizable setting, even after the adversarial injections since the labels are always consistent with a hypothesis \(f^{}\). This model can naturally be extended to the agnostic setting with adversarial labels. It is also possible to allow the adversary to adaptively choose \(f^{}\), that is, the adversary has to make sure the labels are consistent with some \(f\) at all times but does not have to commit to one fixed \(f^{}\). Another interesting variation would be change the feedback to not include the true label \(y_{t}\) on rounds that the learner abstains. As we will see, the known distribution case will be able to handle adaptive \(f^{}\) and limited label access.

**Objective.** In our framework, the goal of the learner is to have low error rate on the rounds it decides to predict (that is when \(_{t}\{0,1\}\)) while also ensuring that it does not abstain (\(_{t}=\)) on too many non-adversarial rounds (\(c_{t}=0\)). More formally, the learner's objective is to minimize the following error (or regret),

\[:=^{T}[_{t}=1-f^{} (_{t})]}_{}+^{T} [c_{t}=0_{t}=]}_{}.\]

It is important to note that we allow the learner to abstain on adversarial examples for free. This allows the learner to have arbitrarily many injections without paying linearly for them in the error.

_Remark_.: There are many natural generalizations and modification of the model and objective that one could consider. For example, we would consider a cost-based version of this objective that would allow us to trade-off these errors.

**Connections to testable learning.** A further interesting connection can be made by viewing our model as an online version of testable learning framework of . In order to see the analogy more direct, we will focus on the setting where the learner knows the distribution \(\). In the setting, a learning algorithm is seen as a tester-learner pair. The tester takes the data set as input and outputs whether the data set passes the test. The algorithm then takes as input any data set that passes the test and outputs a hypothesis. The soundness guarantee for the pair of algorithms is that the algorithm run on any data set that passes the test must output a hypothesis that is good on the distribution. The completeness requires that when the dataset is indeed from the "nice" distribution, then the tester passes with high probability. We can see our framework in this light by noting that the decision of whether to abstain or not serves as a test. Thus, in this light, completeness corresponds to the abstention error being small when the data is non-adversarial i.e. is from the true distribution, while the soundness corresponds to the misclassification error being small on points the algorithm decides not to abstain. While the testable learning literature primarily focuses on the computational aspects of learning algorithms, our focus is solely on the statistical aspects.

## 3 Warm-up: Disagreement-based Learners

As a first example to understand the framework, we consider the most natural learner for the problem. Given the data \(S\) of the examples seen thus far, the learner predicts on examples \(\) whose labels it is certain of. That is, if there is a unique label for \(\) consistent with \(|_{S}\), the learner predicts that label. Else, it abstains from making a prediction. This region of uncertainty is known as the disagreement region.

**Example: thresholds in one dimension.** Consider learning a single-dimensional threshold in \(\) (that is, concepts \(x[x t]\) for any \(t\)). While it is well known that ERM achieves \( T\) misclassification error for i.i.d. data sequences, in the case of an adversarially chosen sequence, it is also well known that the adversary can select inputs in the disagreement region each time (closer and closer to the decision boundary) and thereby force any non-abstaining learner to make a linear number of mistakes (recall that the Littestone dimension of thresholds is infinite ). Indeed, it is known that the function classes \(\) for which non-abstaining predictors can be forced to have \(=(T)\) are precisely those with embedded threshold problems of unbounded size . Let us now consider the learner that abstains in the disagreement region and predicts based on the consistent hypothesis outside of this region.

**Proposition 3.1**.: _Disagreement-based learner for one dimensional thresholds has_

\[=0 2  T.\]

To see this note that our learner only predicts when the input is not in the disagreement region and thus it never predicts incorrectly (\(=0\)). As for the abstentions, a simple exchangeability, argument shows that when there are \(n\) i.i.d. examples in the sequence, the probability of the new non-adversarial example being in the disagreement region is \(1/n\). Summing this over the time horizon gives us the above proposition.

**Perfect selective classification and active learning.** The learner for the above thresholds problem is a well-known strategy from the areas of perfect selective classification and active learning known as _disagreement-based_ learning . In the perfect selective classification setting , the learner observes the examples sequentially, as in our framework, and may predict or abstain on each, and must satisfy the requirement that whenever it predicts its prediction is _always_ correct. From this fact, it immediately follows that applying any perfect selective classification strategy in our setting, we always have \(=0\), so that its performance is judged purely on its abstention rate on the iid examples. It was argued by  that the optimal abstention rate among perfect selective classification strategies is obtained by the strategy that makes a prediction on the next example if and only if all classifiers in the hypothesis class that are correct on all examples observed so far, _agree_ on the example. Note that this is precisely the same learner used above. This same strategy has also been studied in the related setting of _stream-based active learning_1. The abstention rate achievable by this strategy for general hypothesis classes is thoroughly understood . In particular, a complete characterization of the optimal distribution-free abstention rate of perfect selective classification is given by the _star number_ of the hypothesis class . The star number \(\) is the size of the largest number \(s\) such that there are examples \(\{x_{1},,x_{s}\}\) and hypotheses \(h_{0},h_{1},,h_{s}\) such that \(h_{i}\) and \(h_{0}\) disagree exactly on \(x_{i}\). For instance, \(=2\) for threshold classifiers . It was shown by  that the optimal distribution-free abstention rate for perfect selective classification is sublinear if and only if the star number is finite (in which case it is always at most \( T\)). One can show that \(\) is always lower bounded by the VC dimension of the class. Unfortunately, the star number is infinite for most hypothesis classes of interest, even including simple VC classes such as _interval_ classifiers .

**Beyond disagreement-based learning.** The learner that abstains whenever it sees an example that it is not certain of may be too conservative. Furthermore, it does not exploit the possibility of learning from mistakes. Let us consider another example to elucidate this failure. Consider theclass of \(d\) intervals in one dimension where the positive examples form a union of intervals. That is \(=\{_{i=1}^{k}_{[a_{i},b_{i}]}:a_{1} b_{1}  a_{k} b_{k}\}\) over the domain \(=\). This class has VC dimension \(2d\) but infinite star number. Suppose that \(d=2\) but examples in the second interval are very rarely selected by i.i.d. examples. Then the disagreement-based learner would suggest to abstain on all examples to protect against the possibility that our new example is in the second interval. However, consider the following simple strategy: if the new example lies between two positives (resp. negatives), we predict positive (resp. negative), else we abstain.

**Proposition 3.2**.: _The proposed strategy for the class of \(d\)-intervals in one dimension has_

\[ d 2d T.\]

To see this, note that whenever we predict, either we are correct or we have identified the location of a new interval, hence reducing the VC dimension of the version space by 1. Since there are at most \(d\) intervals, we will therefore make at most \(2d\) errors when we predict, implying \( 2d\). As for abstaining on i.i.d. examples, the same argument for thresholds can be applied here by treating the intervals as at most \(d\) thresholds.

## 4 Higher-order Disagreement-based learner with Known marginals

We will first focus on the setting when the marginal distribution of the i.i.d. distribution \(\) is known to the learner. Suppose \(\) has VC dimension \(d\). In this setting, the algorithm that naturally suggests itself is to take a cover for the class under \(\) of accuracy \((T^{-1})\) and use an adversarial algorithm for prediction. Since there are covers of size \(T^{O(d)}\) and Littestone dimension of any finite class is bounded by the logarithm of the size, this seems to indicate that this algorithm will achieve our goal with misclassification error \(O(d T)\) and zero abstention error. But unfortunately note that this algorithm competes only with the best classifier on the cover. The cover is a good approximation only on the marginal distribution \(\) and not on the adversarial examples. In fact, when we restrict to the hypothesis class being the cover, the data may no longer even be realizable by the cover. Therefore, we need to use the access to the distribution is a completely different way.

The inspiration for our approach comes from the work of  on active learning strategies that go beyond disagreement-based learning by making use of higher-order notions of disagreement based on _shattering_. We note, however, that while the work of  only yields asymptotic and distribution-dependent guarantees (and necessarily so, in light of the minimax characterization of  based on the star number), our analysis differs significantly in order to yield distribution-free finite-sample bounds on the misclassification error and abstention rate.

As we saw earlier, just looking at the disagreement region does not lead to a good algorithm for general VC classes (whenever the star number is large compared to the VC dimension). The main algorithmic insight of this section is that certain higher-order versions of disagreement sets do indeed lead to good notions of uncertainty for VC classes. Our measure uses the probability of shattering \(k\) examples (see definition below) for different values of \(k\) freshly drawn from the underlying distribution, under the two restrictions of the class corresponding to the two labels for the current test example, to make the abstention decision. One can think of the probability of shattering as a proxy for the complexity of the version space. This serves both as a method to quantify our uncertainty about whether the current example is from the distribution or not, since we can understand the behavior of this quantity in the i.i.d. case, and also as potential function which keeps track of the number of mistakes. In order to formally state this, we will need some definitions.

**Definition 4.1** (Shattering and VC Dimension).: Let \(\) be a domain and \(\) be a binary function class on \(\) i.e. \(\{0,1\}^{}\). A set \(\{x_{1},,x_{k}\}\) is said to be shattered by \(\) if for all \(y\{0,1\}^{k}\) there exists a function \(f\) such that \(f(x_{i})=y_{i}\). The VC dimension of \(\) is defined as the maximum \(k\) such that there is a set of size \(k\) that is shattered.

**Definition 4.2** (Shattered \(k\)-tuples).: Let \(k\) be a positive integer. The set of shattered \(k\)-tuples, denoted by \(_{k}\), for hypothesis class \(\) over a domain \(\) is defined as

\[_{k}()=\{(x_{1},,x_{k}) :\{x_{1}, x_{k}\}\}.\]Additionally, given a distribution \(\) on the domain, we will refer to as the \(k\) shattering probability of \(\) with respect to \(\), denoted by \(_{k}(,)\), as

\[_{k}(,)=^{ k}( _{k}())=_{x_{1},,x_{k} ^{ k}}[\{x_{1},,x_{k}\}].\]

Let us now describe the algorithm (see Algorithm 1). The algorithm maintains a state variable \(k\) which we will refer to as the level the algorithm is currently in. The level can be thought of as the complexity of the current version space. At level \(k\), we will work with shattered sets of size \(k\). At each round, the algorithm, upon receiving the example \(_{t}\), computes the probabilities of shattering \(k\) examples (drawn i.i.d. from \(\)) for each of the classes corresponding to sending \(_{t}\) to \(0\) and \(1\) respectively. The algorithm abstains if both these probabilities are large, else predicts according to whichever one is larger. At the end of the round, after receiving the true label \(y_{t}\), the algorithm checks whether the probability of shattering \(k\) examples is below a threshold \(_{k}\), in which case it changes the level, that is, updates \(k\) to be \(k-1\).

``` Set \(k=d\) and \(_{1}=\) for\(t=1,,T k>1\)do  Receive \(_{t}\) if\(\{_{k}(_{t}^{_{t} 1}),_{k} (_{t}^{_{t} 0})\} 0.6_{k}( _{t})\)then predict \(_{t}=\) else predict \(_{t}=*{argmax}_{j\{0,1\}}\{_{k}( _{t}^{_{t} j})\}\)  Upon receiving label \(y_{t}\), update \(_{t+1}_{t}^{_{t} y_{t}}\) if\(_{k}(_{t+1})_{k}\)then  Set \(k=k-1\) if\(k=1_{t}_{1}\)then\(_{t}=\) if\(k=1_{t}_{1}\)then Predict with the consistent label for \(_{t}\) ```

**Algorithm 1**Level-based learning for Prediction with Abstension

Below, we state the main error bound of the algorithm and give an overview of the analysis. We defer the detailed proofs to Appendix B. Our theorem shows that both the misclassification error and the abstension error are bounded in terms of the VC dimension.

**Theorem 4.1**.: _In the adversarial injection model with abstentions with time horizon \(T\), Algorithm 1 with \(_{k}=T^{-k}\) gives the following,_

\[[] d^{2} T and [] 6d.\]

We will now present the main technical idea that we will use to analyze the theorem. Let \(k\) be a positive integer and \(x\) be any example in \(\). The following lemma upper bounds the probability for a random example the shattering probability of the two restrictions of the class are both large compared to the original shattering probability of the class. That is to say for most examples, one of the two restrictions of the class will have a smaller shattering probability compared to the original class.

**Lemma 4.2**.: _For any \(k\) and any \(>1/2\), we have_

\[_{x}[_{k}(^{x 1})+_{k} (^{x 0}) 2_{k}( )]( )}{_{k}()}.\] (1)

With this in hand, we will first look at the abstention error. The intuition is that when the algorithm is at level \(k\), an abstention occurs only if the condition from (1) is satisfied and Lemma 4.2 bounds the probability of this event. It remains to note that when the algorithm is at level \(k\) we both have an upper bound on \(_{k+1}\) (since this is the condition to move down to level \(k\) from level \(k+1\)) and a lower bound on the \(_{k}\) (since this is the condition to stay at level \(k\)).

**Lemma 4.3** (Abstention error).: _For any \(k d\), let \([_{k},e_{k}]\) denote the interval of time when Algorithm 1 is at level \(k\). Then, the expected number of non-adversarial rounds at level \(k\) on which the algorithm abstains satisfies_

\[[_{t=_{k}}^{e_{k}}[c_{t}=0_ {t}=]] 5T}{_{k}}.\] (2)Next, we bound the misclassification error. The main idea here is to note that every time a misclassification occurs at level \(k\), the \(k\)-th disagreement coefficient reduces by a constant factor. Since we have a lower bound on the disagreement coefficient at a fixed level, this leads to a logarithmic bound on the number of misclassifications at any given level.

**Lemma 4.4** (Misclassification error).: _For any \(k d\), let \([_{k},e_{k}]\) denote the interval of time when Algorithm 1 is at level \(k\). For any threshold \(_{k}\) in Algorithm 1,_

\[[_{t=_{k}}^{e_{k}}[_{t}=1-f^{*}( _{t})]] 2(}).\]

Putting together Lemma 4.4 and Lemma 4.3 along with a setting of \(_{k}=T^{-k}\), gives us Theorem 4.1.

_Remark_.: Closer inspection at the algorithm and the analysis reveals that the algorithm gives the same guarantees even in the setting where the learner does not receive the label \(y_{t}\) during rounds that it abstains (hinted at in Section 2). To see this note that during the rounds that we abstain, we only use the fact that \(_{k}(_{t})\) does not increase which would remain true when if we did not update the class.

## 5 Structure-based Algorithm for Unknown Distribution

We move to the case of unknown distributions. In this setting, the example \(x\) are drawn from a distribution \(\) that is unknown to the learner. As we saw earlier, it is challenging to decide whether a single point is out of distribution or not, even when the distribution is known. This current setting is significantly more challenging since the learner needs to abstain on examples that are out of distribution for a distribution that it doesn't know. A natural idea would be to use the historical data to build a model for the distribution. The main difficulty is that, since we do not get feedback about what examples are corrupted, our historical data has both in-distribution and out-of-distribution examples. The only information we have about what examples are out of distribution is the prediction our algorithm has made on them. Such issues are a major barrier in moving from the known distribution case to the unknown distribution case. In this section, we will design algorithms for two commonly studied hypothesis classes: VC dimension 1 classes, and axis-aligned rectangles. Though, VC dimension 1 seems restrictive, there are classes with VC dimension \(1\) with large Littlestone dimension and large star number, which capture the complexity of sequential prediction and active learning respectively. Thus, our results separate the complexity of those models from our model of abstention.

### Structure-based Algorithm for VC Dimension 1 Classes

A key quantity that we will use in our algorithm will be a version of the probability of the disagreement region but computed on the historical data. The most natural version of this would be the leave-one-out disagreement. That is, consider the set of examples which are in the disagreement region when the class is restricted using the data set with the point under consideration removed. This estimate would have been an unbiased estimator for the disagreement probability (referred to earlier as \(_{1}\) in Definition 4.2). As mentioned earlier, unfortunately, in the presence of adversarial injections, this need not be a good estimate for the disagreement probability.

In order to remedy this, we consider a modified version of the leave-one-out disagreement estimate which considers examples \(x\) in the disagreement region for the class \(|_{S_{f}(x,y)}\) where \(S_{f}\) is the subset of the datapoints which disagrees with a fixed reference function \(f\). It is important to note that this function \(f\) is fixed independent of the true labelling function \(f^{*}\). Though this seems a bit artificial at first, but this turns out to be a natural quantity to consider given the structure theorem for VC dimension one classes (see Appendix C.1 for more details).

**Definition 5.1**.: Let \(\) be a class of functions and let \(f\) be a reference function. Let \(S=\{(x_{i},y_{i})\}\) be a realizable data set. Define

\[(S,,f)=\{x: y(x,y) S x _{1}(|_{S_{f}(x,y)})\}\]

where \(S_{f}=\{(x,y) S:f(x) y\}\). Further, we will denote the size of this set by \((S,)=(S,,f)\). We will suppress the dependence on \(\) when it is clear from context.

Let us now present the main algorithm (see Algorithm 2). The main idea of the algorithm is similar to Algorithm 1 in the known distribution case. We will make a prediction in the case when the difference between \(\) for the two classes corresponding to the two labels for the point \(x\) is large. The idea is that when a prediction is made and a mistake happens, the size of \(\) goes down similar to \(_{k}\) in the known distribution case. In the case when the difference is small, we will abstain.

``` Let \(f\) be a reference function and \(\) be the abstention threshold. Set \(_{0}=\) and \(S_{0}=\) for\(t=1,,T\)do  Receive \(_{t}\)  Let \(a_{0}=|(S_{t-1},^{_{t} 0},f)|\) and \(a_{1}=|(S_{t-1},^{_{t} 1},f)|\) if\(_{t}_{1}(_{t})\)then Predict with the consistent label for \(_{t}\) elseif\(\{a_{0},a_{1}\}\)then\(_{t}=*{argmax}_{b}a_{b}\) else\(_{t}=\)  Upon receiving label \(y_{t}\), update \(S_{t} S_{t-1}\{(_{t},y_{t})\}\) and \(_{t}_{t-1}^{_{t} y_{t}}\) ```

**Algorithm 2**Structure-based learning for Prediction with Abstention for Unknown Distribution

Though the algorithm is simple and can be made fairly general, our analysis is restricted to the case of VC dimension one. The main reason for this restriction is that our analysis relies on a structure theorem for VC dimension one classes which has no direct analogy for higher VC dimension. But since the algorithm has a natural interpretation independent of this representation, we expect similar algorithms to work for higher VC dimension classes as well.

**Theorem 5.1**.: _Let \(\) be a hypothesis class with VC dimension \(1\). Then, in the corruption model with abstentions with time horizon \(T\), Algorithm 2 with parameter \(=\) gives the following guarantee_

\[[] 2 [].\]

We will now present the main technical ideas that we use to analyze the algorithm. We will keep track of the mistakes using the size of the disagreement region, which we denote by \(_{t}=(S_{t},)\). The main idea for the proof is to note that when we decide to predict the label with the larger value of \(\) is bigger by an additive \(\). Thus, when a mistake occurs the value of \(\) decreases by at least \(\). Summing the errors over all time steps gives the following bound.

**Lemma 5.2**.: _Algorithm 2 has \([] 2T/\)._

Next, we move on to the number of abstentions. In fact, we will prove a stronger structural results that shows that the number of examples such that there is any set of adversarial injections that would make the algorithm abstain is small. We refer to examples on which algorithm can be made to abstain as attackable examples (formally defined in Definition C.2). The main idea is to prove that in any set of iid examples, there are only a few attackable ones. This is formally stated and proved as Lemma C.3. Using this claim, we can bound the number of abstentions using an exchangeability argument.

**Lemma 5.3**.: _Algorithm 2 has \([] T\)._

The proof of Theorem 5.1 follows from Lemma 5.2 and Lemma 5.3 by setting \(=\).

### Structure-based Algorithm for Axis-aligned Rectangles

We can extend the idea from above to design an algorithm for the class of axis-aligned rectangles in dimension \(p\) (VC dimension is \(2p\)) that achieves abstention and misclassification error bounded by \(O(p)\). This exhibits a class of VC dimension \(>1\) for which we can attain the desired guarantees without access to the distribution.

**Theorem 5.4**.: _Let \(\) be the class of axis aligned rectangles in \(^{p}\). Then, Algorithm 3 with \(=\) satisfies_

\[  p,\] \[  2p+2p T.\]

For details of the algorithm and analysis, we refer the reader to Appendix C.3.1.

Discussion and Future Directions

In this paper, we introduce a framework for beyond-worst case adversarial sequential predictions by using abstention. Our proposed framework falls at the intersection of several learning paradigms such as active learning, uncertainty quantification, and distribution testing. In this framework we show two main positive results, validating the beyond-worse case nature of our framework. However, our work has only scratched the surface of understanding learnability for VC classes in this new framework. Here we discuss several exciting future directions:

**General VC classes with unknown distribution.** Extending the structure-based algorithms for general VC classes is wide open. Even characterizing the complexity of learning in the unknown distribution is open. One could attempt to convert the algorithm from Section 4 that for any class \(\) with VC dimension \(d\), achieves abstention error and misclassification error bounded only as a function of \(d\). Recall that Algorithm 1 computed the probabilities of shattering \(k\) points using the knowledge of the distribution and made a prediction \(_{t}\) depending on the relative magnitudes of the probabilities corresponding to the two restricted classes. The main challenge in the unknown distribution case is that it is not immediately obvious how to compute these quantities.

One natural approach is to use the historical data as a proxy for the distribution. That is, given the data set \(S_{t}\) of size \(n\), compute the leave-\(k\)-out estimator for the probability as follows \(_{k}(S,)=1/_{T S;|T|=k}[T|_{S T}].\) There are a few things to observe about this estimator. First, in the case when the data is generated i.i.d., this estimator is unbiased. Further, though each of the summands is not independent, one can show concentration for estimators of this form using the theory of U-statistics. Additionally, recall that in Algorithm 1 required the thresholds \(_{k}\) to be set to \(T^{-O(k)}\) (we use \(T^{-k}\) but it is straightforward to extend this to \(T^{-ck}\) for \(c<1\)). This appears to be high precision but note that the "number of samples" one has for a data set of size \(n\) is \(n^{O(k)}\). Thus, it is conceivable that such an estimator can give the necessary bounds. Unfortunately, the challenge with analyzing this in our setting is that we do no know which of the examples are adversarial. Thus, the adversary could inject examples to make our estimates arbitrarily inaccurate. Thus, as in the case of the VC dimension 1 classes and rectangles, our analysis would need to not rely on the accuracy of the estimates but rather use these estimates to maintain progress or construct other versions of estimators that are unaffected by the adversarial injections.

**Tight bounds for known distribution.** In the known distribution case, it remains to find the optimal error bound. It would be interesting to improve the upper bound or perhaps even more interesting to show a separation between stochastic sequential prediction and our model. We conjecture that the correct dependence on VC dimension \(d\) should be linear and not quadratic, and our analysis is potentially lose in this aspect. A potential strategy to obtain an improved bound would be to choose the level \(k\) at each iteration in an adaptive manner.

**Beyond realizability.** Our techniques rely strongly on the realizability (no label noise), however our framework can naturally be extended to settings with label noise. Immediate questions here would be to extend our results to simple noise models such as random classification noise and Massart noise or more ambitiously the agnostic noise model.

**Beyond binary classification.** Our framework can naturally be extended to more general forms of prediction such as multiclass classification, partial concept classes, and regression. It would be interesting to characterize the complexity of learning in these settings.

**Connections to conformal prediction and uncertainty quantification.** The unknown distribution case can be seen as form of distribution-free uncertainty quantification. It would be interesting to understand connections to other forms such as conformal prediction. On a technical level, our work exploits exchangeability of the i.i.d. sequence which is the foundation of conformal prediction, though the main challenge in our setting is the presence of adversarial inputs. It would be interesting to build on this connections and understand whether techniques can be ported over in either direction.

**Computationally efficient algorithms.** Our focus for this paper has been entirely on the statistical benefits of abstention. Understanding the computational complexity in this setting is an exciting avenue for research. Concretely, for halfspaces in \(d\) dimensions, is there a polynomial time algorithm for learning with abstentions, even for well-behaved distributions such as Gaussians. On a related note, showing computational-statistical gaps in this specialized setting would be interesting, albeit disappointing.