# Where Do Large Learning Rates Lead Us?

Idus Sadrtdinov\({}^{1,2}\), Maxim Kodryan\({}^{2}\), Eduard Pokonechny\({}^{3}\),

Ekaterina Lobacheva\({}^{3}\), Dmitry Vetrov\({}^{1}\)

\({}^{1}\) Constructor University, Bremen \({}^{2}\) HSE University \({}^{3}\) Independent researcher

isadrtdinov@constructor.university, mkodryan@hse.ru, epokonechnyy@gmail.com

lobacheva.tjulja@gmail.com, dvetrov@constructor.university

Equal contribution.Shared senior authorship.

###### Abstract

It is generally accepted that starting neural networks training with large learning rates (LRs) improves generalization. Following a line of research devoted to understanding this effect, we conduct an empirical study in a controlled setting focusing on two questions: 1) how large an initial LR is required for obtaining optimal quality, and 2) what are the key differences between models trained with different LRs? We discover that only a narrow range of initial LRs slightly above the convergence threshold lead to optimal results after fine-tuning with a small LR or weight averaging. By studying the local geometry of reached minima, we observe that using LRs from this optimal range allows for the optimization to locate a basin that only contains high-quality minima. Additionally, we show that these initial LRs result in a sparse set of learned features, with a clear focus on those most relevant for the task. In contrast, starting training with too small LRs leads to unstable minima and attempts to learn all features simultaneously, resulting in poor generalization. Conversely, using initial LRs that are too large fails to detect a basin with good solutions and extract meaningful patterns from the data.

## 1 Introduction

Understanding neural networks (NNs) training is one of the major goals in deep learning for various reasons, from developing more efficient training protocols to handling the AI safety issues . Unfortunately, the high-dimensionality, non-convexity, and stochasticity of the optimization process make its comprehensive analysis extremely difficult. Learning rate (LR) is perhaps the most important hyperparameter in a gradient descent optimizer . LR controls the optimization step size and, due to extreme non-convexity of the optimized loss function yielding a manifold of qualitatively different minima in the loss landscape of neural networks, this hyperparameter is primarily responsible for the type of solution we obtain after training .

Using large learning rates, especially at the beginning of training, has become a common practice . Starting with large LR values is known to help avoid poor local minima  while the solutions obtained at the end of training often have favorable properties like good generalization and flatter loss landscape around the corresponding optima . Prior work has attempted to explain the benefits of high learning rates from various perspectives, which can be divided into three main areas: optimization , model sparsity , and pattern learning . Nevertheless, the following questions still remain very relevant:

1. _Large LRs are preferred but how large are we talking about?_
2. _What are the key characteristics of the models trained with different LRs?_Concerning the first question, a general recommendation is to start training with an LR value that is too high for convergence but too low for divergence [5; 6; 25; 35], with no precise benchmarks available over this wide range. Concerning the second question, despite the abundance of literature on the role of the LR hyperparameter in NN training, we still lack a unified picture of how exactly models trained with small and large LRs differ.

We conduct a detailed empirical analysis on both of the above-mentioned problems in a special setting that allows for more precise control of the LR value. We structure our analysis around the taxonomy of Kodryan et al.  who classified different LR values into three training regimes: 1) convergence for small LRs, 2) chaotic equilibrium for medium LRs, and 3) divergence for large LRs. We start with examining the generalization of the final solutions obtained by either fine-tuning with a small LR or weight averaging after training in different regimes. We confirm that the best choice is to start training in the second regime, with moderately high LRs that lead to neither convergence nor divergence, but only a relatively narrow portion of that range, which we define as _subregime 2A_, consistently provides optimal results. To investigate the effectiveness of these learning rates, we examine training in different regimes from both loss landscape and feature learning perspectives. First, we explore the local geometry of the reached minima and reveal that training in subregime 2A locates a basin in the loss landscape containing linearly connected well-generalizing minima. In contrast, using too small LRs can result in models that become unstable when fine-tuned with larger LRs, while using too large LRs fails to detect basins of good solutions. Then, to study feature learning in different regimes, we introduce a synthetic example with interpretable features. We find that as LR increases up to subregime 2A (including it), models become more specialized in the sense that they rely on fewer features in the data, while further increase in LR gradually impairs the ability to extract features from the data. We also show how our findings transfer to a practical image classification setting via frequency analysis of the input images. In sum, **our contributions** are as follows:

1. We compare the generalization of the minima reached after training with different initial LR values and find that the best choice is indeed to take LRs above the convergence threshold. However, only a relatively narrow part of this range, which we call subregime 2A, consistently provides optimal final results.
2. We discover that starting training with LRs from subregime 2A locates a convex basin containing optimal solutions that can be achieved via fine-tuning with a small LR or weight averaging. Too small initial LRs, in contrast, find unstable local minima, while too large LRs fail to locate such basins of good solutions.
3. We reveal that NNs tend to learn sparser set of the most relevant features from the data as the initial LR increases until the end of subregime 2A; after that, the model gradually loses ability to learn useful features.
4. We show that conclusions obtained in a special setting with accurate control of the LR hold for conventional neural network training as well.

Our code is available at [https://github.com/isadrtdinov/understanding-large-lrs](https://github.com/isadrtdinov/understanding-large-lrs).

### Related Work

A significant amount of theoretical and empirical deep learning research has been dedicated to studying the training dynamics of neural networks. Most of it concerns the role of the LR hyperparameter in NN optimization and generalization accentuating the favorable effects of large LR training.

A vast line of works attributed these effects to amplifying the magnitude of Stochastic Gradient Descent (SGD) noise, which effectively does not allow the model to converge to suboptimal local minima with high local curvature, or _sharpness_[27; 28; 29; 34; 58; 65]. Similar directions highlight other attributes of (stochastic) gradient descent training, such as implicit regularization [9; 19; 30; 59] or minima stability [46; 50; 52; 51; 62; 63], enhanced by the use of large learning rates. While suggesting possible mechanisms for why large LR values lead to good solutions, these works still lack characterization of these solutions and practical receipts for finding them.

A closer look at training with large learning rates reveals its tendency to favor simpler and _sparser_ solutions. Specifically, Andriushchenko et al.  demonstrated that hovering at some constant loss level when training with large LRs helps optimization to eventually find modes with sparse activation patterns in hidden layers of deep neural networks. Similarly, Chen et al.  theoretically predictedand empirically confirmed an increase in sparsity of neurons of networks trained with larger LRs. A recent work of Ahn et al.  discovered that large LR values are necessary to learn the "threshold neurons" in networks with ReLU activation, which effectively lead to sparser solutions. Sparsity bias with increasing LR has also been theoretically studied for linear diagonal  and two-layer ReLU  networks. We explore training with different initial LRs from the perspective of input space _feature sparsity_. In contrast to previous results that reported a monotonic increase in sparsity with increasing LR, we found that feature sparsity behaves non-monotonically: the most pronounced feature sparsity effect is observed at approximately the same initial LR values that provide the best final solutions in terms of generalization.

Another relevant research direction examines pattern learning with different learning rates. Typically, in specific artificial settings, these works show that more complex patterns are learned with smaller LRs, thus, to avoid overfitting, it is beneficial to start training with a large LR and then decay it to smaller values after learning all the "easy-to-fit" patterns from the data [41; 44; 68]. Recently, Rosenfeld and Risteski  suggested an intriguing idea of "opposing signals" referring to similar patterns in objects of different classes such as blue background in images of planes and ships. Opposing signals usually correspond to spurious features, which lead to suboptimal quality if learned by the model. As suggested by the authors, training with large LR values resolves opposing signals via filtering out the corresponding unreliable features. We follow this direction and provide further clarity on what features in the data the model captures after training with different initial LRs.

## 2 Methodology

To rigorously study the impact of the initial LR on the final solution, we require to fix it at the beginning of training. However, as most modern architectures use normalization in some form, truly fixing an LR becomes a nontrivial action. Specifically, scale-invariance induced by normalization yields two consequences: 1) scale-invariant weights are essentially defined on the sphere, and 2) the _effective learning rate_ (ELR) of these weights, i.e., learning rate on the unit sphere, is varying even with a fixed LR due to a varying parameters norm [5; 7; 35; 42; 43; 53]. Therefore, to resolve this issue, we train our models with a fixed parameters norm in a scale-invariant manner, which helps us conduct our study more accurately since fixing an LR now leads to a fixed ELR as well. Following the prior research [5; 35; 42], we make all our models fully scale-invariant (SI) by fixing the last layer and removing trainable affine parameters of normalization layers, and train them using projected SGD on a sphere of fixed radius. We return to a more conventional setting in Section 6.

Investigating the training of scale-invariant models on the sphere, Kodryan et al.  discovered that it typically takes place in one of three regimes depending on the LR value: 1) _convergence_, when parameters monotonically converge to a minimum, 2) _chaotic equilibrium_, when loss noisily stabilizes at some level, and 3) _divergence_, when a model has random guess accuracy. We adopt the three-regimes taxonomy and build our analysis around it from here on. In Figure 1, we show the (smoothed) test accuracy after training with different fixed LRs. The three regimes are clearly distinguishable. Kodryan et al.  mostly focus on training with fixed LRs, however, they point out that starting training in the second regime and then decreasing LR can often lead to better solutions than training with a constantly fixed LR. Similarly, other works [5; 6] suggest that training should start in the second regime and attribute this effect to the benign noise driven process happening in the "loss stabilization" phase.

Following these results, we wish to analyze the points obtained after initial training with different LR values from the perspective of their utility for subsequent training with small LRs or weight averaging. To this end, we divide training into two stages. First, we perform so-called _pre-training_, i.e., we train models with different fixed LRs, which we call PLRs, for sufficient amount of epochs to ensure stabilization of training dynamics. After pre-training, we either 1) change the learning rate and _fine-tune_ the model, i.e., train it further with a small LR, or 2) continue training with the same LR as at the pre-training stage and weight-average consequent checkpoints, as is usually done in stochastic

Figure 1: Three regimes of training with a fixed LR. Mean test accuracy \(\) standard deviation on the last 20 out of 200 epochs are shown. Dashed lines denote boundaries between the training regimes. SI ResNet-18 on CIFAR-10.

weight averaging (SWA) . For fine-tuning, we use only first regime LRs, which we call FLRs, to ensure model convergence. Further detail on the experimental setup are provided in Appendix A.

In the following sections, we present our main results. All results are obtained with a scale-invariant ResNet-18  trained on CIFAR-10 . We additionally consider a plain convolutional network ConvNet and ViT small architecture  as well as CIFAR-100 and Tiny ImageNet  datasets in the appendix. For ease of presentation, we divide all plots into three parts (using dashed lines) corresponding to the three previously introduced pre-training regimes. We begin with comparing the generalization of the fine-tuned/SWA solutions obtained after pre-training with different LRs (Section 3). We then analyze their local geometry to shed more light on their differences (Section 4). Next, we shift our attention to studying feature learning in models trained with different initial LRs and propose a synthetic example with adjustable features (Section 5). Finally, we demonstrate that our findings remain valid for conventionally trained networks as well (Section 6) and draw conclusions.

## 3 Finding the best LRs for generalization

To examine the generalization benefits of the pre-training stage, we evaluate the test accuracy of the fine-tuned/SWA models. Figure 2 depicts the test accuracy after pre-training with different PLR values (black line) and after fine-tuning/SWA (colored lines). Below we successively analyze pre-training in each regime from the generalization point of view and highlight the range of initial LRs providing the best final quality.

Regime 1In accordance with the results of Kodryan et al. , we observe a monotonic dependence between the PLR value and the test accuracy of the pre-trained model. Both SWA and fine-tuning with FLR \(\) PLR do not noticeably change the pre-trained accuracy. At the same time, fine-tuning with FLR \(>\) PLR can significantly improve the test accuracy, however, it still cannot provide a considerably better solution than training with the same FLR from scratch. Therefore, the best strategy of training in the first regime is to use the maximum constant LR without any schedules.

Regime 2The second regime is the most promising for pre-training, since most practical LR schedules start in this regime . We can see that the results strongly depend on the PLR value and the general advice "pre-train with a large LR to obtain a better solution" is valid, but definitely not for all PLRs of the second regime. In fact, the second regime can be divided into two subregimes, which we denote as **2A** and **2B**. We discovered that pre-training in subregime 2A, i.e., with lower second regime PLRs, results in significantly better fine-tuning and SWA results compared to regime 1, while pre-training in subregime 2B, i.e., with higher second regime PLRs, loses this advantage.

Subregime 2AEven though pre-training with lower second regime PLRs does not converge to the lowest loss values, it locates optimal regions for further fine-tuning or weight averaging. Notably, fine-tuning a network obtained with a PLR from this range with _any_ FLR results in minima of the same quality. So, such pre-training allows for fine-tuning even with small FLRs to avoid local optima with poor generalization, to which training from scratch with these FLRs usually converges. Moreover, the fine-tuned models are of higher quality than any solutions obtained in the first regime,

Figure 2: Test accuracy of the fine-tuned (left) and SWA (right) solutions for SI ResNet-18 on CIFAR-10. Test accuracy after pre-training is depicted with the black line. Dashed lines denote boundaries between the pre-training regimes, dotted line divides the second regime into two subregimes.

which shows that the best minima may be completely unreachable with small LRs from a standard initialization, at least in any reasonable training time.3

_Subregime 2B_ However, further increases in PLR reduce the benefits of pre-training in subregime 2A. Both SWA and fine-tuning quality degrades; in addition, the solutions obtained with different FLRs begin to differ from each other in the test accuracy. So, such pre-training can be detrimental if we decrease LR to the first regime right after it. Previously, Andriushchenko et al.  also discovered that high LRs of the second regime can deteriorate final model performance for single-step LR schedules. However, we find that this effect can be mitigated by gradually decreasing an LR through subregime 2A (see Appendix D). In sum, obtaining high-quality results in this subregime using SWA or fine-tuning with a constant FLR is not possible and requires a more complex LR schedule.

Regime 3Pre-training in the third regime is somewhat similar to random walking in the parameters space , hence it is completely impractical for weight averaging. Despite this, we notice that it still can be beneficial for fine-tuning with small FLRs. We conjecture that it is due to the fact that pre-training in the third regime, in contrast to standard random initialization , yields a very uneven distribution of norms of individual scale-invariant parameter groups. Many groups have low norms implying that their effective learning rate is higher than the total ELR for the whole model, which promotes convergence to better optima during fine-tuning (see Appendix E).

**Takeaway 1**: _Although pre-training with large first regime PLRs finds points with relatively high test accuracy, they cannot be improved via fine-tuning or weight averaging; hence, the first regime is not the best choice for starting training. Pre-training with PLRs of the lower part of the second regime, just above the boundary between regimes 1 and 2, helps robustly increase the quality to a level unreachable with a constant LR; however, using higher PLRs loses this advantage and degrades performance. Despite pre-training in regime 3 does not seem to extract any useful information from the data, it may help in subsequent fine-tuning by providing better initialization for the optimization._

## 4 Loss landscape perspective

In the previous section, we determined the range of initial LRs leading to the best model quality. We now aim to clarify the key characteristics that differentiate these LRs from others. In this section, we take the loss landscape perspective and analyze the local geometry of minima obtained after initial training in different regimes. To do this, we measure the linear connectivity and the angular distance between them. Angular distance between two networks with weights \(_{1}\) and \(_{2}\) is calculated as

\[(_{1},_{2})=(,_{2} }{\|_{1}\|\,\|_{2}\|}).\]

We choose it as a natural metric on the sphere in the weight space. Linear connectivity is measured via calculating a linear-path barrier between two networks w.r.t. training or test error, i.e., the highest difference between the error on the linear path between two points in the weight space and linear interpolation of the error at each of them :

\[B(_{1},_{2})=_{}[E(_{1}+(1- )_{2})- E(_{1})-(1-)E(_{2})],\]

where \(_{1}\) and \(_{2}\) are weights of the networks, and \(E\) is the error measure. The barrier value shows whether solutions obtained from the same pre-trained point remain in the same low-error region (low barrier) or head to different optima (high barrier). In Figure 3, we present angular distances and linear connectivity between three solutions for each PLR: SWA of 5 networks and the points obtained after fine-tuning with the lowest and the highest considered FLRs.

As was shown previously, for small initial LR values attributed to regime 1 neither SWA, nor fine-tuning with smaller FLRs improve the pre-trained model. This is because they effectively remain in the same minimum: the obtained solutions are of similar quality, close to each other in angular distance and linearly connected. On the other hand, taking FLR \(\) PLR can cause a catapult effect  and subsequent convergence to a better minimum corresponding to the new LR value. This is clearly observed when fine-tuning with the highest FLR from low PLRs through the improvement of test accuracy in Figure 2 and high angular distance and error barrier with other solutions in Figure 3. Thus, pre-training with small LRs leads to minima that have suboptimal generalization and are unstable in the sense that increasing LR after convergence can knock the model out of the current minimum and move it to a different one.

Moving to the right along the PLR axis, we encounter subregime 2A that is optimal for further weight averaging or fine-tuning: the resulting solutions are of high quality regardless of the FLR value. Geometrical inspection also reveals that the solutions obtained with different FLRs from the same pre-trained point are very close to each other in angular distance and linearly connected. SWA models are located farther from the fine-tuned solutions but still are mostly linearly connected to them.4 Hence, we conjecture that pre-training in subregime 2A locates a "bowl" in the loss landscape by bouncing between its walls ; the bottom of this "bowl" is a convex basin of high-quality solutions, which can be easily reached via fine-tuning or weight averaging.

Pre-training with higher second regime PLRs gets stuck at higher loss levels and is unable to locate the region of high-quality solutions. With larger PLRs, fine-tuned solutions not only become worse but also differ from each other: we see a rapidly growing gap in test accuracy and angular distance between the solutions obtained with low and high FLRs, followed by a separation in the linear connectivity w.r.t. the training error. Interestingly, linear connectivity w.r.t. the test error is still preserved for the most part, which may be due to a high test error value at one end. We conclude that unlike subregime 2A, pre-training in subregime 2B explores a vast area of the loss landscape with a diverse set of minima.

Even though pre-training in the third regime can help fine-tuning with small FLRs converge to better minima thanks to the optimization effects, in many ways it still behaves as random walking with a large step size. Both the angular distances and the error barriers approach their upper limits (angular distance of \(/2\), which is a typical angular distance between two independent points in high-dimensional space, and random-guess error), completing the trend established in subregime 2B.

**Takeaway 2**: _Pre-training with small PLRs attributed to the first regime can end up in unstable minima, from which the optimization escapes at increased FLRs. Using initial LRs from the optimal range (subregime 2A) allows for finding a basin that contains only high-quality solutions easily reachable via fine-tuning or SWA. Larger PLRs lose the ability to locate low-error basins and instead cover large areas of the loss landscape with diverse not linearly connected minima._

## 5 Feature learning perspective

Synthetic exampleWe proceed to study the feature learning properties of models trained with different LRs. With this in mind, we propose a synthetic example with precise control over how different features affect the target variable. We consider a binary classification setting with the following three properties: 1) all three training regimes are observed; 2) varying the initial LR leads to different generalization; 3) the data points contain multiple features, each of which is sufficient to classify the data correctly. We use \(32\)-dimensional data vectors, where each pair of coordinates

Figure 3: Geometry between the points fine-tuned with the smallest and the largest FLRs and SWA. SI ResNet-18 on CIFAR-10.

represents a single 2D "tick" feature (Figure 4), all \(16\) features are sampled from the same distribution. We use a \(3\)-layer MLP with ReLU activation and Layer Normalization  and make it scale-invariant similarly to the main setup. We put further detail in Appendix A. The generalization and geometry properties in this setup are similar to those previously described, see Appendix G.

To quantify how features are learned with different LRs, we generate \(16\) single-feature test samples, each having only one of the \(16\) total features. The values of other features are distributed along the decision boundary (gray dashed line in Figure 4) to represent missing features. We measure the accuracy of the trained models on these single-feature test samples and relate the obtained values to the importance of the respective features for the model: the higher the accuracy value, the more important the feature. We analyze these values in sorted order for each PLR because models may favor different features in different training runs due to the randomness in initialization .

The results are depicted in Figure 5. For small PLRs, there is no feature selection: we observe similar accuracy w.r.t. different features resulting in poor overall generalization, since no target-predicting feature is reliably learned by the model. Closer to the boundary between regimes 1 and 2 we begin to observe a kind of model specialization: the accuracy corresponding to a single feature is significantly higher than for the rest. Moreover, such feature sparsity persists after fine-tuning, indicating completely different feature learning behavior than in regime 1: even in the setting of equally useful features, the model prefers to focus more on some subset of features instead of trying to learn all features at once. The sparsity peak in subregime 2A coincides with the peak of the fine-tuning test accuracy (Figure 15), confirming that a sparser set of learned features improves model generalization. This may also be related to the fact that the basin is determined exactly at this range of LRs (see discussion in Section 7). When the PLR is increased to subregime 2B, the ability to extract any useful patterns from the data is reduced, which manifests itself in degraded accuracy on both regular and single-feature test samples.

Fourier featuresA similar feature selection effect can be observed in scale-invariant ResNet-18 on the CIFAR-10 dataset. Since for the real-world image data it is generally not clear how to define features , we use frequency bands of the 2D Discrete Fourier Transform (DFT) as proxy for

Figure 4: A single 2D “tick” feature used in the synthetic example.

Figure 5: Feature sparsification in the synthetic example for pre-training (left), and fine-tuning with \(=10^{-4}\) (right). Colored lines show the accuracy values on single-feature test samples, sorted independently for each training run. The accuracy on a regular test sample is depicted with the black line. The lines are averaged over 50 seeds.

Figure 6: Inverse 2D DFT images, each containing \(1\) of \(4\) components of the spectrum. For this figure, we rescale each color channel of _low_, _mid_ and _high_ images to 0-1 range.

Figure 7: Accuracy of different frequency bands for pre-training (left) and fine-tuning with \(=10^{-5}\) (right). Each line is an average over \(5\) last epochs of training. SI ResNet-18 on CIFAR-10.

features . We divide the full 2D spectrum of an image into \(4\) components, each consisting of a range of frequency bands: \(0\) (constant background color), \(1\)-\(8\) (low), \(9\)-\(24\) (mid), and \(25\)-\(32\) (high). For each of the \(4\) components, we zero out the rest of the spectrum and apply the inverse DFT to obtain images with only one frequency component preserved (Figure 6) by analogy with the single-feature test samples in the synthetic example. We repeat this procedure with every test image and measure the accuracy on the resulting \(4\) new test sets corresponding to the spectrum components. A more detailed description of the setup can be found in Appendix A. See Appendix H for additional results.

Figure 7 shows the test accuracy w.r.t. each spectrum component after pre-training with different PLRs. As in the synthetic example, small PLRs of regime 1 tend to treat all features approximately equally, paying slightly more attention to the background color and low-frequency features, while increasing PLR introduces feature sparsity, making the mid-frequency features significantly more important. The peak of the mid-frequency accuracy is achieved in subregime 2A, reaching much higher absolute values than the \(0\) and \(1\)-\(8\) components in the first regime. Moreover, after fine-tuning, the mid-frequency accuracy improves even further, showing the same bias towards a subset of features as in the synthetic example. However, a further increase in the PLR to subregime 2B reduces the feature sparsity and the mid-frequencies importance. The mid-frequencies are known to play a key role in model generalization and robustness in image classification , so their apparent prevalence in subregime 2A indicates that feature selection is not random but is biased towards the most useful features for the task. Interestingly, prior work assumed that training with large LRs must prioritize "easy-to-fit hard-to-generalize" features  but our results suggest that the model may favor more complex features if they are more helpful in predicting the target.

**Takeaway 3**: _Using initial LRs from subregime 2A results in sparse feature learning. This effect allows for the model to focus on the most relevant features for the task (e.g., mid-frequencies in image classification), resulting in better generalization. Pre-training with other initial LR values does not show such model specialization._

## 6 Practical setting

In this section, we demonstrate that our main results can be transferred to a more conventional training setting with some nuances. We train a common ResNet-18 model without the sphere constraint using SGD with momentum, weight decay, and data augmentation; the only deviation from the standard setup is a different LR schedule. Due to unstable behavior of non-scale-invariant weights with large LRs , we can only observe regimes 1 and 2. The boundary between the regimes is also less clear, mainly due to the presence of augmentations, which make the data harder to learn. Therefore, the model is unable to converge completely with small LRs, so we have drawn the boundary between the regimes approximately according to the PLR with maximum quality at the pre-trained point.

As can be seen in Figure 8, left, our first claim that the best quality is achieved with LR drops at the beginning of the second regime (subregime 2A) is confirmed. It is also clear that fine-tuning in subregime 2A converges to similar optima regardless of FLR, while at larger PLRs fine-tuning leads to diverse minima. In the first regime, the behavior is substantially different due to the mentioned issues with convergence: we see that even fine-tuning with small FLRs can still improve quality. We

Figure 8: Practical ResNet-18 on CIFAR-10. Dashed lines denote boundaries between the first and second pre-training regimes, dotted line divides the second regime into two subregimes. Left: Test accuracy after fine-tuning with different FLRs (color lines) and after pre-training (black line). Middle: train error barriers between small FLR, high FLR, and SWA points. Right: Accuracy w.r.t. different frequency bands for fine-tuning with \(=10^{-4}\).

note that at the best points of subregime 2A, fine-tuning reaches a test accuracy of \( 95\%\), which is no worse than that of a standard LR schedule . Linear connectivity (Figure 8, middle) is preserved for both regime 1 and subregime 2A and lost in subregime 2B. The small train error barriers at the beginning of the first regime are due to the catapulting effect of large FLRs. Finally, frequency bands accuracies in the right plot of Figure 8 depict a similar trend as described in Section 5: in subregime 2A, the network captures significantly more mid-frequencies from the inputs than other components, while no similar specification is observed for other initial LR ranges.

In Appendix I, we provide additional practical results including analysis of SWA, angular distances, test error barriers, etc., as well as ablations on CIFAR-100 and Tiny ImageNet. We also show how our findings transfer to Vision Transformers (ViT) in Appendix J. To summarize, our main claims remain valid in the practical setting.

**Takeaway 4**: _Most of the results obtained in a controlled setting are relevant to practice._

## 7 Discussion and future research directions

Convergence thresholdOne of the main claims of this work is that the best initial LR values lie just above the _convergence threshold_ (CT). But what is this threshold and how can it be found and used in practice?

In general, by the convergence threshold for a given model and training setup we mean a learning rate value that separates regimes 1 and 2. In other words, training with a constant LR below the CT leads to convergence to a minimum, while taking a larger constant LR prevents the optimization from converging. Convergence here is defined in a conventional sense, i.e., the optimized functional value (training loss) closely approaches its global minimum by the end of training; in a simplified training setup without advanced data augmentations, it can be tracked by the ability of the model to fit the training data (i.e., reach \( 100\%\) training accuracy). Yet, as we show in Appendix C, CT is better understood as a small zone within the overall LR range, since the exact threshold may slightly shift depending, e.g., on the epoch budget.

That said, regimes 1 and 2 can be clearly distinguished by the behavior of the training loss in the first epochs: whether it decreases to low values or gets stuck at some non-zero level (refer to Fig. 1 in Kodryan et al. ). Thus, finding the CT could be done relatively efficiently using, e.g., binary search by LRs. However, in practice, precisely determining the CT is not necessary. Even though optimal initial LRs for fine-tuning with a constant small LR or weight averaging (subregime 2A) are located just above the CT, larger LRs from subregime 2B may lead to similar final quality if a more advanced LR schedule is used (see Appendix D): decreasing LR in several steps can correct for an initial value that is too high. Accordingly, most practical LR schedules are designed in that manner: starting with a large LR and gradually decreasing it as training progresses. Therefore, given a proper schedule, it is recommended to start training with a reasonably high LR value from regime 2, i.e., not allowing for convergence but also not leading to a training failure, to achieve a good final solution.

Loss landscape and feature learningWe have shown that the best LRs for pre-training simultaneously identify a basin with good solutions and sparsify the learned features, focusing on the most useful ones. A very intriguing question is how exactly are these observations related? Based on the results concerning subregime 2A, it can be assumed that learning some subset of features corresponds to localizing a certain region in the loss landscape, all solutions within which rely to a greater extent on these learned features. In this regard, what features were learned during the pre-training stage can determine the quality of the localized basin of solutions. This conjecture gives rise to a number of very nontrivial but interesting questions. For instance, can we somehow connect the properties of the learned features with certain characteristics of the basin and minima within it, e.g., some notion of _sharpness_? According to Kodryan et al. , the solutions obtained with higher PLRs of the first regime have both better generalization and lower sharpness, however this trend is more complex for the fine-tuned solutions in regime 2 (see Appendix K). Next, how exactly does feature sparsity affect the properties of the found basin: does it only pre-define a specific set of shared features or can it act as some sort of a regularizer for the solutions inside the basin, e.g., by leading to simpler models , which in fact can be represented by smaller networks? The study of the described issues opens an important direction for future research of neural networks training, as it may draw links between the optimization process and the final model properties.

Practical implicationsDespite a relatively small scale of our experiments, we provide several important practical implications and possible explanations of generally accepted practices. Firstly, we confirm that choosing higher LRs not only speeds up training, but also allows the neural network to find solutions that are inaccessible at low LRs, which was previously shown only in specific settings [41; 44]. Secondly, we discover that it is important both to choose the initial LR and to design a full LR schedule. A good choice of the initial LR allows for localizing a region with the best solutions, as too small LRs tend to converge to the nearest optima with poor generalization. At the same time, designing a suitable LR schedule is necessary if training is started with a too high LR: in that case, it is important not to decrease it too quickly, but to gradually go through the optimal values in order to be able to localize a good basin before convergence.

We also show that the choice of the initial LR can lead to different feature learning behavior. In our setting, sparse features and mid-frequency bias were associated with optimal generalization. However, in practice this may not always be the case. For example, some features useful for the training data classification can be spurious for the test data . Or memorization, in general affecting a very small subset of data, is less likely to happen when training with large LRs, while some works show that memorization in neural networks can be useful . Thus, the properties of training with different LRs in more complex practical scenarios with spurious features/benign memorization may lead to more complex relationships between LRs and generalization. We believe this direction is significant for future research to more broadly understand the impact of learning rate on trained models.

## 8 Conclusion

In this work, we studied the influence of training with different initial LRs on the properties of the final solution. We discovered that pre-training with moderately large LRs, slightly above the convergence threshold, provides the best points for subsequent fine-tuning or weight averaging. From the geometry perspective, training with these LR values locates a basin of well-generalizing solutions in the loss landscape; from the feature learning perspective, these solutions correspond to a sparse set of learned features that are most useful for the task. Using other LR values may lead to suboptimal results: either unstable local minima corresponding to a dense set of learned features with smaller LRs or vast areas with diverse minima and degraded feature learning with larger LRs. We conduct main experiments in a special setup allowing for more accurate control of the learning rate and validate our key results in a practical setting. Our findings can be useful for both practical and theoretical future work on optimizing LR schedules, loss landscape structure, and feature learning in neural networks.

LimitationsWe wish to highlight several limitations of our work. First, our study is primarily empirical in nature; our conclusions do not have direct theoretical support (perhaps only indirect via related work partly mentioned in Section 1.1). Second, we are limited to a specific setup involving particular datasets (image or synthetic) and NN architectures (convolutional, MLP, or Vision Transformer) and, generally speaking, cannot guarantee that all of our findings will consistently generalize to other settings. Third, although we account for the impact of scale invariance on LR in our main experiments, we may overlook similar effects of other NN invariances, like rescale invariance of homogeneous activations (e.g., ReLU) [21; 45]. Addressing these and other possible limitations is future work.