# A Unified Approach for Maximizing Continuous DR-submodular Functions

Mohammad Pedramfar

Purdue University

mpedramf@purdue.edu Christopher John Quinn

Iowa State University

cjquinn@iastate.edu Vaneet Aggarwal

Purdue University

vaneet@purdue.edu

###### Abstract

This paper presents a unified approach for maximizing continuous DR-submodular functions that encompasses a range of settings and oracle access types. Our approach includes a Frank-Wolfe type offline algorithm for both monotone and non-monotone functions, with different restrictions on the general convex set. We consider settings where the oracle provides access to either the gradient of the function or only the function value, and where the oracle access is either deterministic or stochastic. We determine the number of required oracle accesses in all cases. Our approach gives new/improved results for nine out of the sixteen considered cases, avoids computationally expensive projections in three cases, with the proposed framework matching performance of state-of-the-art approaches in the remaining four cases. Notably, our approach for the stochastic function value-based oracle enables the first regret bounds with bandit feedback for stochastic DR-submodular functions.

## 1 Introduction

The problem of optimizing DR-submodular functions over a convex set has attracted considerable interest in both the machine learning and theoretical computer science communities [2; 5; 16; 26]. This is due to its many practical applications in modeling real-world problems, such as influence/revenue maximization, facility location, and non-convex/non-concave quadratic programming [3; 9; 18; 15; 20]. as well as more recently identified applications like serving heterogeneous learners under networking constraints  and joint optimization of routing and caching in networks .

Numerous studies investigated developing approximation algorithms for constrained DR-submodular maximization, utilizing a variety of algorithms and proof analysis techniques. These studies have addressed both monotone and non-monotone functions and considered various types of constraints on the feasible region. The studies have also considered different types of oracles--gradient oracles and value oracles, where the oracles could be exact (deterministic) or stochastic. Lastly, for some of the aforementioned offline problem settings, some studies have also considered analogous online optimization problem settings as well, where performance is measured in regret over a horizon. This paper aims to unify the disparate offline problems under a single framework by providing a comprehensive algorithm and analysis approach that covers a broad range of setups. By providing a unified framework, this paper presents novel results for several cases where previous research was either limited or non-existent, both for offline optimization problems and extensions to related stochastic online optimization problems.

This paper presents a Frank-Wolfe based meta-algorithm for (offline) constrained DR-submodular maximization where we could only query within the constraint set, with sixteen variants for sixteen problem settings. The algorithm is designed to handle settings where (i) the function is monotoneor non-monotone, (ii) the feasible region is a downward-closed (d.c.) set (extended to include \(0\) for monotone functions) or a general convex set, (iii) gradient or value oracle access is available, and (iv) the oracle is exact or stochastic. Table 1 enumerates the cases and corresponding results on oracle complexity (further details are provided in Appendix A). We derive the first oracle complexity guarantees for nine cases, derive the oracle complexity in three cases where previous result had a computationally expensive projection step [32; 16] (and we obtain matching complexity in one of these), and obtain matching guarantees in the remaining four cases. In addition to proving approximation ratios and oracle complexities for several (challenging) settings that are the first or improvements over the state of the art, the _technical novelties of our approach_ include:

1. A new construction procedure of a shrunk constraint set that allows us to work with lower dimensional feasible sets when given a value oracle, resulting in the first results on general lower dimensional feasible sets given a value oracle.
2. The first Frank-Wolfe type algorithm for analyzing monotone functions over a general convex set for any type of oracle, where only feasible points can be queried.
3. Shedding light on a previously unexplained gap in approximation guarantees for monotone DR-submodular maximization. Specifically, by considering the notion of query sets and assuming that the oracles can only be queries within the constraint set, we divide the class of

  \(F\) & Set & Oracle & Setting & Reference & Appx. & Complexity \\   &  &  & det. & , (*) & \(1-1/e\) & \(O(1/)\) \\   & & & stoch. & , (*) & \(1-1/e\) & \(O(1/^{3})\) \\   & & & stoch. &  \(\) & \(1-1/e\) & \(O(1/^{2})\) \\   & & & & stoch. & This paper & \(1-1/e\) & \(O(1/^{3})\) \\   & & & & stoch. & This paper & \(1-1/e\) & \(O(1/^{5})\) \\   & & & & stoch. & \(\) & \(1/2\) & \(O(1/)\) \\   & & & stoch. & \(\) & \(1/2\) & \(O(1/^{2})\) \\   & & & stoch. & This paper & \(1/2\) & \((1/^{3})\) \\   & & & & stoch. & This paper & \(1/2\) & \((1/^{3})\) \\   & & & & stoch. & This paper & \(1/2\) & \((1/^{3})\) \\   & & & & stoch. & , (*) & \(1/e\) & \(O(1/)\) \\   & & & & stoch. & , (*) & \(1/e\) & \(O(1/^{5})\) \\   & & & & stoch. & This paper & \(1/e\) & \(O(1/^{3})\) \\   & & & & stoch. & This paper & \(1/e\) & \(O(1/^{5})\) \\   & & & & stoch. & This paper & \(1/e\) & \(O(1/^{5})\) \\   & & & & stoch. &  & \(\) & \(O(e^{})\) \\   & & & & &  & \(\) & \(O(e^{})\) \\   & & & & & , (*) & \(\) & \(O(1/)\) \\   & & & & stoch. & This paper & \(\) & \(O(1/^{3})\) \\   & & & & stoch. & This paper & \(\) & \(O(1/^{3})\) \\   & & & & stoch. & This paper & \(\) & \(O(1/^{5})\) \\   This table compares the different results for the number of oracle calls (complexity) _within the feasible set_ for DR-submodular maximization. Shaded rows indicate problem settings for which our work has the **first guarantees** or **beats the SOTA**. The rows marked with a blue star (*) correspond to cases where Algorithm 2 generalizes the corresponding algorithm** and therefore has the same performance. The different columns enumerate properties of the function, the convex feasible region (downward-closed, includes the origin, or general), and the oracle, as well as the approximation ratios and oracle complexity (the number of queries needed to achieve the stated approximation ratio with at most \(>0\) additive error). We have \(h:=_{}\|x\|_{}\). (See Appendix B regarding  and ). \(\) when the oracle can be queried for any points in \(^{d}\) (even outside the feasible region \(\)), the problem of optimizing monotone DR-submodular functions over a general convex set simplifies â€”  and  achieve the same ratios and complexity bounds as listed above for \(0\);  can achieve an approximation ratio of \(1-1/e\) with the \(O(1/^{3})\) and \(O(1/^{5})\) complexity for exact and stochastic value oracles respectively. \(\) and  use gradient ascent, requiring potentially computationally expensive projections.

Table 1: Offline DR-submodular optimization results.

monotone submodular maximization into monotone submodular maximization over convex sets containing the origin and monotone submodular maximization over general convex sets. Moreover, we conjecture that the \(1/2\) approximation coefficient, which has been considered sub-optimal in the literature, is optimal when oracle queries can only be made within the constraint set. (See Appendix B for more details.)

Furthermore, we also consider online stochastic DR-submodular optimization with bandit feedback, where an agent sequentially picks actions (from a convex feasible region), receives stochastic rewards (in expectation a DR-submodular function) but no additional information, and seeks to maximize the expected cumulative reward. Performance is measured against the best action in expectation (or a near-optimal baseline when the offline problem is NP-hard but can be approximated to within \(\) in polynomial time), the difference denoted as expected \(\)-regret. For such problems, when only bandit feedback is available (it is typically a strong assumption that semi-bandit or full-information feedback is available), the agent must be able to learn from stochastic value oracle queries over the feasible actions action. By designing offline algorithms that only query feasible points, we made it possible to convert those offline algorithms into online algorithms. In fact, because of how we designed the offline algorithms, we are able to access them in a black-box fashion for online problems when only bandit feedback is available. Note that previous works on DR-submodular maximization with bandit feedback in monotone settings (e.g. ,  and ) explicitly assume that the convex set contains the origin.

For each of the offline setups, we extend the offline algorithm (the respective variants for stochastic value oracle) and oracle query guarantees to provide algorithms and \(\)-regret bounds in the bandit feedback scenario. Table 2 enumerates the problem settings and expected regret bounds with bandit and semi-bandit feedback. The key contributions of this work can be summarized as follows:

**1.** This paper proposes a unified approach for maximizing continuous DR-submodular functions in a range of settings with different oracle access types, feasible region properties, and function properties. A Frank-Wolfe based algorithm is introduced, which compared to SOTA methods for each of the sixteen settings, achieves the best-known approximation coefficients for each case while providing (i) the first guarantees in nine cases, (ii) reduced computational complexity by avoiding projections in three cases, and (iii) matching guarantees in remaining four cases.

**2.** In particular, this paper gives the first results on offline DR-submodular maximization (for both monotone and non-monotone functions) over general convex sets and even for downward-closed convex sets, when only a value oracle is available over the feasible set. Most prior works on offline DR-submodular maximization require access to a gradient oracle.

**3.** The results, summarized in Table 2, are presented with two feedback models--bandit feedback where only the (stochastic) reward value is available and semi-bandit feedback where a single stochastic sample of the gradient at the location is provided. This paper presents the first regret analysis with bandit feedback for stochastic DR-submodular maximization for both monotone and non-monotone functions. For semi-bandit feedback case, we provide the first result in one case, improve the state of the art result in two cases, and gives the result without computationally intensive projections in one case.

  \(F\) & Set & Feedback & Reference & Coef. \(\) & \(\)-Regret \\   & \)} &  & \(} }\), \\   & & This paper \\  & \(1-1/e\) & \(O(T^{2/3})\) \\   & & & \(F\) & This paper & \(1-1/e\) & \(O(T^{5/6})\) \\   &  &  & \(} }\\ }\\ }\\ \)} & \)} &  & \(} }\\ }\\ }\\ \)} & \(1/2\) & \(O(T^{1/2})\) \\   & & & \(F\) & This paper & \(1/2\) & \(O(T^{3/4})\) \\   & \)} & \)} & \( F\) & This paper & \(1/e\) & \(O(T^{3/4})\) \\   & & & \(F\) & This paper & \(1/e\) & \(O(T^{5/6})\) \\   & & & \(F\) & This paper & \(\) & \(O(T^{3/4})\) \\    & & & \(F\) & This paper & \(\) & \(O(T^{5/6})\) \\   This table compares the different results for the expected \(\)-regret for online stochastic DR-submodular maximization for the under bandit and semi-bandit feedback. Shaded rows indicate problem settings for which our work has the **first guarantees** or **beats the SOTA**. We have \(h:=_{}\|x\|_{}\). \(\) the analysis in  has an error (see the supplementary material for details). \(\) uses gradient ascent, requiring potentially computationally expensive projections.

Table 2: Online stochastic DR-submodular optimization.

Related Work:The key related works are summarized in Tables 1 and 2. We briefly discuss some key works here; see the supplementary materials for more discussion. For online DR-submodular optimization with bandit feedback, there has been some prior works in the adversarial setup [31; 33; 25; 30] which are not included in Table 2 as we consider the stochastic setup.  considered monotone DR-submodular functions over downward-closed convex sets and achieved \((1-1/e)\)-regret of \(O(T^{8/9})\) in adversarial setting. This was later improved by  to \(O(T^{5/6})\).  further improved the regret bound to \(O(T^{3/4})\). However, it should be noted that they use a convex optimization subroutine at each iteration which could be even more computationally expensive than projection.  considered non-monotone DR-submodular functions over downward-closed convex sets and achieved \(1/e\)-regret of \(O(T^{8/9})\) in adversarial setting.

## 2 Background and Notation

We introduce some basic notions, concepts and assumptions which will be used throughout the paper. For any vector \(^{d}\), \([]_{i}\) is the \(i\)-th entry of \(\). We consider the partial order on \(^{d}\) where \(\) if and only if \([]_{i}[]_{i}\) for all \(1 i d\). For two vectors \(,^{d}\), the _join_ of \(\) and \(\), denoted by \(\) and the _meet_ of \(\) and \(\), denoted by \(\), are defined by

\[:=(\{[]_{i},[]_{i}\})_{i=1}^{ d}:=(\{[]_{i},[]_{i}\})_{i=1}^ {d},\] (1)

respectively. Clearly, we have \(\). We use \(\|\|\) to denote the Euclidean norm, and \(\|\|_{}\) to denote the supremum norm. In the paper, we consider a bounded convex domain \(\) and w.l.o.g. assume that \(^{d}\). We say that \(\) is _down-closed_ (d.c.) if there is a point \(\) such that for all \(\), we have \(\{ z\}\). The _diameter_\(D\) of the convex domain \(\) is defined as \(D:=_{,}\|-\|\). We use \(_{r}(x)\) to denote the open ball of radius \(r\) centered at \(\). More generally, for a subset \(X^{d}\), we define \(_{r}(X):=_{x X}_{r}(x)\). If \(A\) is an affine subspace of \(^{d}\), then we define \(_{r}^{d}(X):=A_{r}(X)\). For a function \(F:\) and a set \(\), we use \(F|_{}\) to denote the restriction of \(F\) to the set \(\). For a linear space \(_{0}^{d}\), we use \(P_{_{0}}:^{d}_{0}\) to denote the projection onto \(_{0}\). We will use \(_{+}^{d}\) to denote the set \(\{^{d}| 0\}\). For any set \(X^{d}\), the affine hull of \(X\), denoted by \((X)\), is defined to be the intersection of all affine subsets of \(^{d}\) that contain \(X\). The _relative interior_ of a set \(X\) is defined by

\[(X):=\{ X>0,_{ }^{(X)}() X\}.\]

It is well known that for any non-empty convex set \(\), the set \(()\) is always non-empty. We will always assume that the feasible set contains at least two points and therefore \((()) 1\), otherwise the optimization problem is trivial and there is nothing to solve.

A set function \(f:\{0,1\}^{d}^{+}\) is called _submodular_ if for all \(,\{0,1\}^{d}\) with \(\), we have

\[f()-f() f()-f( ),\{0,1\}^{d}.\] (2)

Submodular functions can be generalized over continuous domains. A function \(F:^{d}^{+}\) is called _DR-submodular_ if for all vectors \(,^{d}\) with \(\), any basis vector \(_{i}=(0,,0,1,0,,0)\) and any constant \(c>0\) such that \(+c_{i}^{d}\) and \(+c_{i}^{d}\), it holds that

\[F(+c_{i})-F() F(+c_{i}) -F().\] (3)

Note that if function \(F\) is differentiable then the diminishing-return (DR) property (3) is equivalent to \( F() F()\) for \(\) with \(,^{d}\). A function \(F:^{+}\) is _\(G\)-Lipschitz continuous_ if for all \(,\), \(\|F()-F()\| G\|-\|\). A differentiable function \(F:^{+}\) is _\(L\)-smooth_ if for all \(,\), \(\| F()- F()\| L\|-\|\).

A (possibly randomized) offline algorithm is said to be an \(\)-approximation algorithm (for constant \((0,1]\)) with \( 0\) additive error for a class of maximization problems over non-negative functions if, for any problem instance \(_{}F()\), the algorithm output \(\) that satisfies the following relation with the optimal solution \(^{*}\)

\[ F(^{*})-[F()],\] (4)

where the expectation is with respect to the (possible) randomness of the algorithm. Further, we assume an oracle that can query the value \(F()\) or the gradient \( F()\). The number of calls to the oracle to achieve the error in (4) is called the _evaluation complexity_.

Offline Algorithms and Guarantees

In this section, we consider the problem of maximizing a DR-submodular function over a general convex set in sixteen different cases, enumerated in Table 1. After setting up the problem in Section 3.1, we then explain two key elements of our proposed algorithm when we only have access to a value oracle, (i) the Black Box Gradient Estimate (BBGE) procedure (Algorithm 1) to balance bias and variance in estimating gradients (Section 3.2) and (ii) the construction of a shrunken feasible region to avoid infeasible value oracle queries during the BBGE procedure (Section 3.3). Our main algorithm is proposed in Section 3.4 and analyzed in Section 3.5.

### Problem Setup

We consider a general _non-oblivious_ constrained stochastic optimization problem

\[_{}F():=_{ }_{ p(;)}[(, )],\] (5)

where \(F\) is a DR-submodular function, and \(:}\) is determined by \(\) and the random variable \(\) which is independently sampled according to \( p(;)\). We say the oracle has variance \(^{2}\) if \(_{}_{ p(;)}[(,)]=^{2}\). In particular, when \(=0\), then we say we have access to an exact (deterministic) value oracle. We will use \(()\) to denote the random variables \((,)\) where \(\) is a random variable with distribution \(p(:\!)\). Similarly, we say we have access to a stochastic gradient oracle if we can sample from function \(:}\) such that \( F()=_{ q(;)}[ (,)]\), and \(\) is determined by \(\) and the random variable \(\) which is sampled according to \( q(;)\). Note that oracles are only defined on the feasible set. We will use \(()\) to denote the random variables \((,)\) where \(\) is a random variable with distribution \(q(:\!)\).

**Assumption 1**.: We assume that \(F:^{d}\) is DR-submodular, first-order differentiable, non-negative, \(G\)-Lipschitz for some \(G<\), and \(L\)-smooth for some \(L<\). We also assume the feasible region \(\) is a closed convex domain in \(^{d}\) with at least two points. Moreover, we also assume that we either have access to a value oracle with variance \(_{0}^{2} 0\) or a gradient oracle with variance \(_{1}^{2} 0\).

_Remark 1_.: The proposed algorithm does not need to know the values of \(L\), \(G\), \(_{0}\) or \(_{1}\). However, these constants appear in the final expressions of the number of oracle calls and the regret bounds.

### Black Box Gradient Estimate

Without access to a gradient oracle (i.e., first-order information), we estimate gradient information using samples from a value oracle. We will use a variation of the "smoothing trick" technique [14; 17; 1; 27; 31; 8; 33], which involves averaging through spherical sampling around a given point.

**Definition 1** (Smoothing trick).: For a function \(F:\) defined on \(^{d}\), its \(\)-smoothed version \(_{}\) is given as

\[_{}():=_{_{_{}^{()}}()}[F()]=_{ _{1}^{()-}(0)}[F( +)],\] (6)

where \(\) is chosen uniformly at random from the \((())\)-dimensional ball \(_{1}^{()-}(0)\). Thus, the function value \(_{}()\) is obtained by "averaging" \(F\) over a sliced ball of radius \(\) around \(\).

When the value of \(\) is clear from the context, we may drop the subscript and simply use \(\) to denote the smoothed version of \(F\). It can be easily seen that if \(F\) is DR-submodular, \(G\)-Lipschitz continuous, and \(L\)-smooth, then so is \(\) and \(\|()-F()\| G\), for any point in the domain of both functions. Moreover, if \(F\) is monotone, then so is \(\) (Lemma 3). Therefore \(_{}\) is an approximation of the function \(F\). A maximizer of \(_{}\) also maximizes \(F\) approximately.

Our definition of smoothing trick differs from the standard usage by accounting for the affine hull containing \(\). This will be of particular importance when the feasible region is of (affine) dimension less than \(d\), such as when there are equality constraints. When \(()=^{d}\), our definition reduces to the standard definition of the smoothing trick. In this case, it is well-known that the gradient of the smoothed function \(_{}\) admits an unbiased one-point estimator [14; 17]. Using a two-point estimator instead of the one-point estimator results in smaller variance [1; 27]. In Algorithm 1, we adapt the two-point estimator to the general setting.

### Construction of \(_{}\)

We want to run Algorithm 1 as a subroutine within the main algorithm to estimate the gradient. However, in order to run Algorithm 1, we need to be able to query the oracle within the set \(_{}^{()}()\). Since the oracle can only be queried at points within the feasible set, we need to restrict our attention to a set \(_{}\) such that \(_{}^{()}(_{}) \). On the other hand, we want the optimal point of \(F\) within \(_{}\) to be close to the optimal point of \(F\) within \(\). One way to ensure that is to have \(_{}\) not be too small. More formally, we want that \(_{^{}}^{()}(_{}) \), for some value of \(^{}\) that is not too large. The constraint boundary could have a complex geometry, and simply maintaining a \(\) sized margin away from the boundary can result in big gaps between the boundary of \(\) and \(_{}\). For example, in two dimensions, if \(\) is polyhedral and has an acute angle, maintaining a \(\) margin away from both edges adjacent to the acute angle means the closest point in the \(_{}\) to the corner may be much more than \(\). For this construction, we choose a \(()\) and a real number \(r>0\) such that \(_{r}^{()}()\). For any \(<r\), we define

\[_{}^{,r}:=(1-)+.\] (7)

Clearly if \(\) is downward-closed, then so is \(_{}^{,r}\). Lemma 7 shows that for any such choice of \(\) and \(r>0\), we have \(}{}\). See Appendix G for more details about the choice of \(\) and \(r\). We will drop the superscripts in the rest of the paper when there is no ambiguity.

_Remark 2_.: This construction is similar to the one carried out in  which was for \(d\)-dimensional downward-closed sets. Here we impose no restrictions on \(\) beyond Assumption 1. A simpler construction of shrunken constraint set was proposed in . However, as we discuss in Appendix D, they require to be able to query outside of the constraint set.

### Generalized DR-Submodular Frank-Wolfe

The pseudocode of our proposed offline algorithm, Generalized DR-Submodular Frank-Wolfe, is shown in Algorithm 2. At a high-level, it follows the basic template of Frank-Wolfe type methods, where over the course of a pre-specified number of iterations, the gradient (or a surrogate thereof) is calculated, an optimization sub-routine with a linear objective is solved to find a feasible point whose difference (with respect to the current solution) has the largest inner product with respect to the gradient, and then the current solution is updated to move in the direction of that feasible point.

```
1:Input: Point \(\), sampling radius \(\), constraint linear space \(_{0}\), \(k=(_{0})\), batch size \(B\)
2: Sample \(_{1},,_{B}\) i.i.d. from \(S^{d-1}_{0}\)
3: For \(i=1\) to \(B\), let \(_{i}^{+}+_{i},y_{i}^{-} -_{i}\), and evaluate \((_{i}^{+}),(_{i}^{-})\)
4:\(_{i=1}^{B}[( _{i}^{+})-(_{i}^{-})]_{i}\)
5:Output \(\) ```

**Algorithm 1** Black Box Gradient Estimate (BBGE)

However, there are a number of important modifications to handle properties of the objective function, constraint set, and oracle type. For the oracle type, for instance, standard Frank-Wolfe methods assume access to a deterministic gradient oracle. Frank-Wolfe methods are known to be sensitive to errors in estimates of the gradient (e.g., see ). Thus, when only a stochastic gradient oracle or even more challenging, only a stochastic value oracle is available, the gradient estimators must be carefully designed to balance query complexity on the one hand and output error on the other. The Black Box Gradient Estimate (BBGE) sub-routine, presented in Algorithm 1, utilizes spherical sampling to produce an unbiased gradient estimate. This estimate is then combined with past estimates using momentum, as seen in , to control and reduce variance.

Our algorithm design is influenced by state-of-the-art methods that have been developed for specific settings. One of the most closely related works is , which also dealt with using value oracle access for optimizing monotone functions. They used momentum and spherical sampling techniques that are similar to the ones we used in our Algorithm 1. However, we modified the sampling procedure and the solution update step. In their work,  also considered a shrunken feasible region to avoid sampling close to the boundary. However, they assumed that the value oracle could be queried outside the feasible set (see Appendix D for details).

In Algorithm 3, we consider the following cases for the function and the feasible set.

1. If \(F\) is monotone DR-submodular and \(\), we choose \[(}_{n},_{n})=_{_{s}-_{1}},}_{n},\ (_{n},_{n},)= _{n}+_{n},\] and \(=1/N\). We start at a point near the origin and always move to points that are bigger with respect to the partial order on \(^{d}\). In this case, since the function is monotone, the optimal direction is a maximal point with respect to the partial order. The choice of \(=1/N\) guarantees that after \(N\) steps, we arrive at a convex combination of points in the feasible set and therefore the final point is also in the feasible set. The fact that the origin is also in the feasible set shows that the intermediate points also belong to the feasible set.
2. If \(F\) is non-monotone DR-submodular and \(\) is a downward closed set containing \(0\), we choose \[(}_{n},_{n})=_{_{s}-_{1}\\  1-_{n}},}_{n},\ (_{n},_{n}, )=_{n}+_{n},\] and \(=1/N\). This case is similar to (A). However, since \(F\) is not monotone, we need to choose the optimal direction more conservatively.
3. If \(F\) is monotone DR-submodular and \(\) is a general convex set, we choose \[(}_{n},_{n})=_{_{s}},}_{n} ,\ (_{n},_{n},)=(1- )_{n}+_{n},\] and \(=(N)/2N\). In this case, if we update like in cases (A) and (B), we do not have any guarantees of ending up in the feasible set, so we choose the update function to be a convex combination. Unlike (B), we do not need to limit ourselves in choosing the optimal direction and we simply choose \(\) to obtain the best approximation coefficient.
4. If \(F\) is non-monotone DR-submodular and \(\) is a general convex set, we choose \[(}_{n},_{n})=_{_{s}},}_{n} ,(_{n},_{n},)=(1- )_{n}+_{n},\] and \(=(2)/N\). This case is similar to (C) and we choose \(\) to obtain the best approximation coefficient.

The choice of subroutine \(\) and \(_{n}\) depend on the oracle. If we have access to a gradient oracle \(\), we set \((,,_{0})\) to be the average of \(B\) evaluations of \(P_{_{0}}(())\). Otherwise, we run Algorithm 1 with input \(\), \(\), \(_{0}\). If we have access to a deterministic gradient oracle, then there is no need to use any momentum and we set \(_{n}=1\). In other cases, we choose \(_{n}=}\).

### Approximation Guarantees for the Proposed Offline Algorithm

**Theorem 1**.: _Suppose Assumption 1 holds. Let \(N 4\), \(B 1\) and choose \(\) and \(r>0\) according to Section 3.3. If we have access to a gradient oracle, we choose \(=0\), otherwise we choose \((0,r/2)\). Then the following results hold for the output \(_{N+1}\) of Algorithm 2._

1. _If_ \(F\) _is monotone DR-submodular and_ \(\)_, then_ \[(1-e^{-1})F(^{*})-[F(_{N+1})] }{N^{1/3}}+}{2N}+ G(2++D}{r}).\] (8)
2. _If_ \(F\) _is non-monotone DR-submodular and_ \(\) _is a downward closed set containing_ \(\)_, then_ \[e^{-1}F(^{*})-[F(_{N+1})]}{N^{ 1/3}}+}{2N}+ G(2++2D}{r}).\] (9)_._
3. _If_ \(F\) _is monotone DR-submodular and_ \(\) _is a general convex set, then_ \[F(^{*})-[F(_{N+1})] (N)}{2N^{1/3}}+(N)^{2}}{8N}+ G(2+).\] (10)
4. _If_ \(F\) _is non-monotone DR-submodular and_ \(\) _is a general convex set, then_ \[(1-\|_{1}\|_{})F(^{*})-[F( _{N+1})]}{N^{1/3}}+}{4N}+ G( 2+).\] (11)

_In all these cases, we have_

\[Q=0&,\\ \{4^{2/3}G^{2},6L^{2}D^{2}+^{2}}{B}\}&_{1}^{2}>0,\\ \{4^{2/3}G^{2},6L^{2}D^{2}++2k^{2}_{0}^{2}/B^{2}}{B}\}& _{0}^{2} 0,\]

\(C\) _is a constant, \(k=()\), \(D=()\), and \(^{*}\) is the global maximizer of \(F\) on \(\)._

Theorem 1 characterizes the worst-case approximation ratio \(\) and additive error bounds for different properties of the function and feasible region, where the additive error bounds depend on selected parameters \(N\) for the number of iterations, batch size \(B\), and sampling radius \(\).

The proof of Parts (A)-(D) is provided in Appendix I-L, respectively.

The proof of parts (A), (B) and (D), when we have access to an exact gradient oracle is similar to the proofs presented in [4; 3; 24], respectively. Part (C) is the first analysis of a Frank-Wolfe type algorithm over general convex sets when the oracle can only be queried within the feasible set. When we have access to a stochastic gradient oracle, directly using a gradient sample can result in arbitrary bad performance as shown in Appendix B of . The momentum technique, first used in continuous submodular maximization in , is used when we have access to a stochastic gradient oracle. The control on the estimate of the gradient is deferred to Lemma 9. Since the momentum technique is robust to noise in the gradient, when we only have access to a value oracle, we can use Algorithm 1, similar to , to obtain an unbiased estimate of the gradient and complete the proof.

Theorem 2 converts those bounds to characterize the oracle complexity for a user-specified additive error tolerance \(\) based on oracle properties (deterministic/stochastic gradient/value). The 16 combinations of the problem settings listed in Table 1 are enumerated by four cases (A)-(D) in Theorem 1 of function and feasible region properties (resulting in different approximation ratios) and the four cases 1-4 enumerated in Theorem 2 below of oracle properties. For the oracle properties, we consider the four cases as (Case 1): deterministic gradient oracle, (Case 2): stochastic gradient oracle, (Case 3): deterministic value oracle, and (Case 4): stochastic value oracle.

**Theorem 2**.: _The number of oracle calls for different oracles to achieve an \(\)-approximation error of smaller than \(\) using Algorithm 1 is_

\[(1/),(1/^{3}),(1/^{5}).\]

_Moreover, in all of the cases above, if \(F\) is non-monotone or \(0\), we may replace \(\) with \(O\)._

See Appendix M for proof.

## 4 Online DR-submodular optimization under bandit or semi-bandit feedback

In this section, we first describe the Black-box Explore-Then-Commit algorithm that uses the offline algorithm for exploration, and uses the solution of the offline algorithm for exploitation. This is followed by the regret analysis of the proposed algorithm. This is the first algorithm for stochastic continuous DR-submodular maximization under bandit feedback and obtains state-of-the-art for semi-bandit feedback.

### Problem Setup

There are typically two settings considered in online optimization with bandit feedback. The first is the adversarial setting, where the environment chooses a sequence of functions \(F_{1},,F_{N}\) and in each iteration \(n\), the agent chooses a point \(_{n}\) in the feasible set \(\), observes \(F_{n}(z_{n})\) and receives the reward \(F_{n}(_{n})\). The goal is to choose the sequence of actions that minimize the following notion of expected \(\)-regret.

\[_{}:=_{}_{n=1}^{ N}F_{n}()-[_{n=1}^{N}F_{n}(_{n})].\] (12)

In other words, the agent's cumulative reward is being compared to \(\) times the reward of the best _constant_ action in hindsight. Note that, in this case, the randomness is over the actions of the policy.

The second is the stochastic setting, where the environment chooses a function \(F:\) and a stochastic value oracle \(\). In each iteration \(n\), the agent chooses a point \(_{n}\) in the feasible set \(\), receives the reward \(((_{n}))_{n}\) by querying the oracle at \(z_{n}\) and observes this reward. Here the outer subscript \(n\) indicates that the result of querying the oracle at time \(n\), since the oracle is stochastic. The goal is to choose the sequence of actions that minimize the following notion of expected \(\)-regret.

\[_{}:= N_{}F( )-[_{n=1}^{N}((_{n}))_{n}] = N_{}F()-[_{n =1}^{N}F(_{n})]\] (13)

Further, two feedback models are considered - bandit and semi-bandit feedback. In the bandit feedback setting, the agent only observes the value of the function \(F_{n}\) at the point \(_{n}\). In the semi-bandit setting, the agent has access to a gradient oracle instead of a value oracle and observes \((_{n})\) at the point \(_{n}\), where \(\) is an unbiased estimator of \( F\).

In unstructured multi-armed bandit problems, any regret bound for the adversarial setup could be translated into bounds for the stochastic setup. However, having a non-trivial correlation between the actions of different arms complicates the relation between the stochastic and adversarial settings. Even in linear bandits, the relation between adversarial linear bandits and stochastic linear bandits is not trivial. (e.g. see Section 29 in ) While it is intuitively reasonable to assume that the optimal regret bounds for the stochastic case are better than that of the adversarial case, such a result is not yet proven for DR-submodular functions. Thus, while the cases of bandit feedback has been studied in the adversarial setup, the results do not reduce to stochastic setup. We also note that in the cases where there are adversarial setup results, this paper finds that the results in the stochastic setup achieve improved regret bounds (See Table 3 in Supplementary for the comparison).

### Algorithm for DR-submodular maximization with Bandit Feedback

```
1:Input: Horizon \(T\), inner time horizon \(T_{0}\)
2:Run Algorithm 2 for \(T_{0}\), with according to parameters described in Theorem 2.
3:for remaining time do
4: Repeat the last action of Algorithm 2.
5:endfor ```

**Algorithm 3** DR-Submodular Explore-Then-Commit

The proposed algorithm is described in Algorithm 3. In Algorithm 3, if there is semi-bandit feedback in the form of a stochastic gradient sample for each action \(_{n}\), we run the offline algorithm (Algorithm 2) with parameters from the proof of case 2 of Theorem 2 for \(T_{0}= T^{3/4}\) total queries. If only the stochastic reward for each action \(_{n}\) is available (bandit feedback), we run the offline algorithm (Algorithm 2) with parameters from the proof of case 4 of Theorem 2 for \(T_{0}= T^{5/6}\) total queries. Then, for the remaining time (exploitation phase), we run the last action in the exploration phase.

### Regret Analysis for DR-submodular maximization with Bandit Feedback

In this section, we provide the regret analysis for the proposed algorithm. We note that by Theorem 2, Algorithm 2 requires a sample complexity of \((1/^{5})\) with a stochastic value oracle foroffline problems (any of (A)-(D) in Theorem 1). Thus, the parameters and the results with bandit feedback are the same for all the four setups (A)-(D). Likewise, when a stochastic gradient oracle is available, Algorithm 2 requires a sample complexity of \((1/^{3})\). Based on these sample complexities, the overall regret of online DR-submodular maximization problem is given as follows.

**Theorem 3**.: _For an online constrained DR-submodular maximization problem over a horizon \(T\), where the expected reward function \(F\), feasible region type \(\), and approximation ratio \(\) correspond to any of the four cases (A)-(D) in Theorem 1, Algorithm 3 achieves \(\)-regret (13) that is upper-bounded as:_

**Semi-bandit Feedback (Case 2):**_\((T^{3/4}),\)_ **Bandit Feedback (Case 4):**_\((T^{5/6}).\)_

_Moreover, in either type of feedback, if \(F\) is non-monotone or \(\), we may replace \(\) with \(O\)._

See Appendix N for the proof.

## 5 Conclusion

This work provides a novel and unified approach for maximizing continuous DR-submodular functions across various assumptions on function, constraint set, and oracle access types. The proposed Frank-Wolfe based algorithm improves upon existing results for nine out of the sixteen cases considered, and presents new results for offline DR-submodular maximization with only a value oracle. Moreover, this work presents the first regret analysis with bandit feedback for stochastic DR-submodular maximization, covering both monotone and non-monotone functions. These contributions significantly advance the field of DR-submodular optimization (with multiple applications) and open up new avenues for future research in this area.

Limitations:While the number of function evaluations in the different setups considered in the paper are state of the art, lower bounds have not been investigated.