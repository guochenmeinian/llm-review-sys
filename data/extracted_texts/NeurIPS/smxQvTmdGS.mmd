# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

job matching systems powered by LLMs could unintentionally disadvantage ethnic minorities or individuals with disabilities (Hutchinson et al., 2020). Similarly, modern machine translation systems often transform gender-neutral terms into predominantly gendered forms, potentially intensifying existing gender biases (Stanovsky et al., 2019). Consequently, there is an urgent call for effective mechanisms to assess these detrimental biases in LLMs, ensuring their fair applications.

The LLMs often show significant volatility in their prediction in response to even slight changes in the input prompts or hyperparameter settings (Li et al., 2023; Li and Shin, 2024; Yang et al., 2024), causing generation inconsistency. For example, LLMs may provide contradictory statements about the same facts, or display varying gender stereotypes associated with the same profession in different contexts, underscoring the unpredictability of their behavior. However, existing evaluation metrics for LLMs' alignment overlook the model's stereotype volatility induced by generation inconsistency. For instance, the metrics proposed in CrowS-Pairs (Nangia et al., 2020) assess the frequency that the LLMs prefer a stereotypical output over an anti-stereotypical one in a given context and thus do not capture the LLMs' perspective variation due to context change. Consequently, the metrics fail to support the estimation of the likelihood of the LLMs' generations being against vulnerable groups.

To address the deficiency of previous metrics focusing solely on the average performance, we propose to study the statistical properties of LLMs' biased behavior across contexts by capturing both the bias and volatility of an LLM, with an intuitive explanation of them in Figure 1. Overlooking either factor can skew alignment evaluations, as demonstrated in Section A.2, where we show that current metrics fail to accurately assess discrimination risk between two models due to ignoring volatility. Moreover, accurately assessing these aspects is vital for estimating the risk of LLM applications inducing or amplifying stereotypes in practical scenarios, potentially resulting in the misallocation of vital resources, such as medical aid, to underrepresented and vulnerable groups.

In this work, we introduce a novel Bias-Volatility Framework (BVF) to formally model both the bias and volatility of the LLMs' stereotyped behavior originating from the model's biases and generation inconsistency. BVF is based on three key new ideas: The first is to use a _stereotype distribution_ of an LLM to mathematically characterize the stereotype variation over contexts, which can be estimated based on a sample of variable contexts for a prediction task. The distribution enables us to go beyond pure task performance evaluation to evaluate an LLM's behavior (e.g., its potential tendency of social discrimination). The second is that metrics can be defined to quantify an LLM's behavior by comparing the estimated stereotype distribution with an expected reference distribution (e.g., fair treatment of each group) and quantifying any deviation (risk). The third is that based on the stereotype distribution and defined metrics, we can mathematically decompose an LLM's aggregated stereotype risk into two components: _bias risk_, originating from the systematic bias of the stereotype distribution, and _volatility risk_, due to variations of the stereotype distribution.

Figure 1: **Snow** metaphorically represents any human or language models. The biases of **Snow** are manifested in the statistical properties of its _perspectives_ (i.e. \(_{Snow}(Y|X)\)) over a topic (i.e. \(Y\)) conditioned on an evidence (i.e. \(X\)), including persistent _bias_ and _context-dependent volatility_, respectively correlating with the _mean_ and _variation_ of a bias-measuring random variable derived from perspectives.

Compared with existing stereotype assessment frameworks for analyzing discrimination risk, BVF offers the following distinguishing benefits: _i)_ Versatility: BVF can be adapted for diverse measurement objectives concerning justice-/fairness-related and intersectional topics; _ii)_ Comparative analysis: It enables the comparison of discrimination risk across different LLMs; _iii)_ Causative analysis: It provides a clear delineation of the sources of discrimination, elucidating whether it stems from the model's learned biases or its generation inconsistencies; and _iv)_ General applicability: It allows statistical analysis and comparison of parametric biases, i.e. knowledge and stereotypes, in any modal model, provided the framework's components are appropriately adapted for that use case (refer to Appendix E for more details).

We apply BVF to 12 commonly used language models, providing insights and issuing pre-warnings on the risk of employing potentially biased LLMs. We find that the bias risk primarily contribute the discrimination risk while most LLMs have significant pro-male stereotypes for nearly all careers. We also find that LLMs aligned with reinforcement learning from human feedback exhibit lowers overall discrimination risk but exhibit higher volatility risk.

## 2 Mathematical Modeling

In this section, we will illustrate how metrics introduced in BVF take both bias and volatility into account. This process involves two key steps: _i)_ Define and statistically analyze the distribution of model stereotypical behavior to quantify the risk levels associated with the LLMs' overall behavioral mode; and _ii)_ Decompose the total risk into two components: the risk arising from the persistent bias and the risk stemming from volatility.

Figure 3: Given _unbiased_ predicted probability \(^{*}\), how to relate probability \(\) (middle) to stereotype (left) and discrimination risk (right). In addition, risk decomposition is illustrated in the right figure.

Figure 2: Our statistical framework for measuring stereotypes in large language models (LLMs). As a case study, we investigate the biases of an LLM regarding \(Y=\{Binary\ Gender\}\), with \(X=\{Occupations\}\) as the context evidence. Starting with the LLM’s predicted word probability matrix for \(Y\) (blue for male and pink for female) conditioned on contexts \(C\) augmented with \(X\), we apply the discrimination criterion \(J\) on each element to transform the word probability matrix into a discrimination risk matrix. We then aggregate the discrimination risk matrix across \(C\)’s distribution and derive a discrimination risk vector, capturing the risk for each fixed \(X=x\). Finally, by aggregating the discrimination risk vector over \(X\)’s distribution, we obtain the LLM’s overall discrimination risk concerning \(Y\).

### LLMs' Stereotype Distribution and Discrimination Assessment

Our framework is depicted in Figure 2 and 3. We mathematically define the polarity and extent of an LLM's stereotype. We notice that the inconsistency of LLM's stereotype is triggered by the variation of contexts. Further, LLMs predict the forthcoming tokens according to the tokens of the existing context. Therefore, our definition is based on LLM's token prediction probability. When the LLM \(M\) is applied to predict the attribute \(Y\) given the demographic group \(X=x_{i}\) and the context \(C=c\) as the evidence, the model \(M\) computes the probabilities of all candidate tokens and selects the token with the highest predicted probability. Therefore, the stochastic nature of LLM next token sampling strategy causes inconsistency in their exhibited stereotype. We denote the LLM's preference that \(M\) predicts \(Y=y\) given \(X=x\) as \(p^{M}_{y|x}(c)\). We define \(M\)'s stereotype against \(X=x\) about \(Y=y\) in the context \(C=c\) as:

\[s^{M}_{y|x}(c)=_{y|x}(c)}{p^{*}_{y|x}(c)}-1.\] (1)

where \(p^{*}_{y|x}(c)\) denotes the attribute prediction probability of the unbiased model. If the context \(c\) is devoid of information about the attribute \(Y\), we set \(p^{*}_{y|x}(c)=\), where \(|Y|\) is the number of possible values of \(Y\). For instance, the unbiased preference on gender topic is \(p^{*}_{male}(c)=p^{*}_{female}(c)=0.5\). When \(s^{M}_{y|x}(c)>0\), it indicates that \(Y=y\) is a stereotypical impression of \(X=x\); conversely, it represents an anti-stereotype. The absolute value of \(s^{M}_{y|x}(c)\) signifies the corresponding intensity. Based on \(s^{M}_{y|x}(c)\), we further define two indices to characterize LLM's discrimination against group \(X=x\) and compute LLM's overall discrimination levels for all demographic groups and attributes.

**Definition 1**.: _The Discrimination Risk Criterion \(J\), measuring the most significant stereotype of \(M\):_

\[J(s^{M}_{Y|x}(c))=_{y Y}\{s^{M}_{y|x}(c)^{+}\}\] (2)

_where \(s^{M}_{y|x}(c)^{+}=\{s^{M}_{y|x}(c),0\}\), which refers to the positive part of \(s^{M}_{y|x}(c)\). The purpose of utilizing the positive part is to eliminate the interference of anti-stereotypes. The detailed explanation can be found in Appendix B.1._

**Definition 2**.: _The Discrimination Risk \(r_{x}\), measuring \(M\)'s discrimination risk against \(X=x\) for all the sub-categories of attribute \(Y\):_

\[r_{x}=_{c C}(J(s^{M}_{Y|x}(c))).\] (3)

_We further define the LLM's Overall Discrimination Risk to summarize \(M\)'s discrimination for all demographic groups about attribute \(Y\):_

\[R=_{x X}(r_{x}).\] (4)

Our definition of \(s^{M}_{y|x}(c)\) is grounded in sociological research examining human stereotyping. Scholars in sociology have observed the variability in individuals' perceptions of a particular demographic across different attributes (Brigham, 1971, McCauley et al., 1980). As such, sociological discourse has elucidated the notion of a person's stereotype intensity, delineated by their beliefs regarding the association between a demographic and a specific attribute.

### Disentangle Bias and Volatility for LLM Discrimination Attribution

Modeling the inconsistent stereotypes of LLMs through probabilistic methods reveals two main factors that contribute to discrimination. One factor stems from the bias inherent in LLM predictions regarding the correlation between demographic groups and specific attributes; the other factor is the estimation variation, wherein the former denotes systematic bias in LLM predictions while the latter signifies the efficiency of LLM estimations in statistical terms.

Addressing bias necessitates adjustments to LLM architecture and training procedures, while variation may prove unavoidable. Consequently, decomposing discrimination risk in LLMs based on the contributions of bias and variation offers insight into the potential discrimination mitigative measures. Moreover, bias-induced discrimination can be likened to LLM prejudice, whereas variation-induced discrimination pertains to the LLM's lack of well-learned parametric biases for the prediction.

**Definition 3**.: _The Bias Risk \(r_{x}^{b}\) is the risk caused by the systemic bias of LLM's estimation about the correlation between social group \(X\) and attribute \(Y\):_

\[r_{x}^{b}=J(_{c C}(s_{Y|x}^{M}(c))).\] (5)

_The Volatility Risk \(r_{x}^{v}\) assesses inconsistency and randomness about \(M\)'s discrimination risk:_

\[r_{x}^{v}=r_{x}-r_{x}^{b}\] (6)

The Overall Discrimination Risk defined in Equation 4 also can be decomposed according to the bias-induced part versus the volatility-induced part, which is referred as Overall Bias Risk \(R^{b}\) and the Overall Volatility Risk \(R^{v}\) respectively:

\[R^{b}=_{x X}(r_{x}^{b}),R^{v}=_{x X}(r_{x}^{v}).\] (7)

## 3 Method

To use the BVF for estimating an LLM discrimination risk on a specific social justice topic, three steps are undertaken sequentially: specifying demographic groups and attributes, determining contexts to estimate the stereotype distribution, and calculating discrimination metrics with risk decomposition. This section elaborates on each step.

### Groups and Attributes

To facilitate the estimation of LLM's stereotype distribution, it is imperative to identify the tokens associated with specific demographics and attributes. For instance, to evaluate the potential for gender discrimination within employment opportunities for LLM, it is essential to identify all lexical representations denoting gender categories and various job roles. Examples of the former include terms such as "she/he," "woman/man," "grandmother/grandfather," among others, while the latter encompasses vocations such as "doctor" and "nurse," among others.

The sociological literature examining social discrimination has compiled comprehensive word lists pertaining to vulnerable demographic groups and attributes serving as stereotypical labels. We have opted to utilize these established word lists to instantiate variable \(Y\), as provided in Appendix D.1.

Occupations serve as the primary demographic categorization in our study, denoted by variable \(X\). The exhaustive list of occupations employed is available in Appendix D.1. We use a uniform distribution as the distribution of occupations to exclude occupation value judgments.

### Collect Context by Data Mining

Effectively capturing the varied applied contexts is crucial for accurately assessing and breaking down discrimination within LLMs. We adopt a data mining approach to gather a set of context templates, denoted as \(C\), specifically aimed at assessing discrimination risks related to the feature \(Y\). These

Figure 4: Our approach to data mining _contexts_ involves _i)_ extracting sentences containing terms from \(X\) and \(Y\) with coreference, _ii)_ parsing and recording their structure, and _iii)_ tallying their skeletons to estimate the distribution of \(C\).

context templates are chosen based on whether they fairly connect the demographic variable \(X\) with the attribute \(Y\). The selection process for \(C\), outlined in Figure 4, is designed to avoid the influence of other confounding factors and involves two steps:

Step 1. Gathering sentences.Randomly sample \(N\) articles from a representative dataset. In practice, we scrape from the clean Wikipedia dump from HuggingFace [Foundation] and set \(N=10,000\). In pre-processing the data, we filter out sentences lacking words from \(Y^{3}\). Next, we retain only those sentences where the gender-specific word from \(Y\) either co-refers with or modifies the subject, which would later be replaced with occupation words from \(X\). The structural skeletons of these selected sentences offer additional contextual cues to LLMs, encouraging them to express nuanced attitudes towards \(Y\) during inference.

Step 2: Extracting context templates.We simplify intricate text structures into sentence skeletons, reducing gender-specific cues for LLMs' inference, and then we track these predicates' frequency as distribution. For instance, in the sentence "Detective Smith said he would leave no stone unturned..." where he refers to Detective Smith and is a masculine word related to \(Y\), we parse and extract the predicate situated between these keywords, simply _said_ here, without imposing restrictions on its length. All predicates are standardized to the past tense, and _that_ is appended to reporting verbs, to prevent counting predicates with equivalent meanings, such as _says_, _said_, and _said that_, multiple times. Context templates are crafted using "[X]" for the subject and "[Y]" for the gender attribute word, as in "The [X] said that [Y]." We assume minimal deviation between the distribution of the mining dataset and real-world LLM application contexts. Consequently, the weight of each context template for aggregation is determined by its frequency in the mining dataset, calculated as \()}{_{c C}Count(c)}\). Detailed information about our chosen \(C\) and its distribution is available in Appendix D.3 and code.

### Estimation of the Distribution and Indices

The procedure of estimating and decomposing LLM's discrimination risk includes three steps:

**Step 1: Estimating the conditional probability of \(Y\) given demographic group evidence is \(X=x_{i}\).** As per Equation 1, the estimation of LLM's stereotype relies on \(p^{M}_{y_{j}|x_{i}}(c)\). Nevertheless, a multitude of words associated with \(y_{j}\) exist. For instance, in assessing gender discrimination risk, gender-related terms such as "she/he," "woman/man," "grandmother/grandfather," and so forth are pertinent. Each word linked to a particular demographic group prompts the LLM to yield the corresponding token probability, denoted as \(^{M}_{v|x_{i}}(c)\). Consequently, \(p^{M}_{y_{j}|x_{i}}(c)\) constitutes the normalized summation of \(^{M}_{v|x_{i}}(c)\), computed by:

\[p^{M}_{y_{j}|x_{i}}(c)=}^{M}_{v|x_{i}}(c)}{_{ v^{}\{y_{k}\}}^{M}_{v^{}|x_{i}}(c)},j\{1,,|Y|\}\] (8)

**Step 2: Estimating the distribution of stereotype.** The computed \(p^{M}_{y_{j}|x_{i}}(c)\) from Step 1 is utilized to derive \(s^{M}_{y_{j}|x_{i}}(c)\), as per Equation 1. Subsequently, a non-parametric approach is employed to gauge the distribution of \(M\)'s stereotype, \(s^{M}_{y|x}(c)\), across all demographic groups \(x\) concerning attribute \(Y\).

**Step 3: Estimating and decomposing LLM's discrimination risk.** As described in Equations 2 through 7, we employ \(s^{M}_{y|x}(c)\) to compute and aggregate discrimination risks within LLMs. This entails calculating \(r_{x}\), \(p^{b}_{x}\), and \(r^{v}_{x}\), along with their corresponding summaries \(R\), \(R^{b}\), and \(R^{v}\).

## 4 Results

### Main Results

We apply our discrimination-measuring framework to 12 common LLMs, specifically OPT-IML (30B) [Iyer et al., 2023], Baichuan (13B) [baichuan inc, 2023, Yang et al., 2023], Llama2 (7B)

[MISSING_PAGE_FAIL:7]

### Pro-male Bias

Our experiment across almost all LLMs, exclusively exempting ALBERT, unveiled a significant predisposition towards males, as shown in Figure 5. We observe that T5 and XLNet have a very strong tendency to perceive the genders of different occupations as male, which aligns with the two models having the highest risk of gender bias risk in Table 1. In contrast, Albert considers the genders of occupations to lean more towards female, with an overall shorter distance from the zero point, which corresponds to a lower bias risk for Albert as indicated in Table 1. Most models displayed a strikingly consistent trend: across all occupations, the vast majority, with notable exceptions like _nurse_, _stylist_, and _dietitian_, were predominantly perceived as _male-dominated_. This paradox highlights the unintended incorporation and perpetuation of societal gender biases within AI systems.

### Empirical Analysis of Bias Risk and Volatility Risk in LLMs

Data Toxicity:Toxic data in the training set is detrimental to the fairness of LLMs. We fine-tuned Llama2 on toxic data (Davidson et al., 2017; Iyer, 2021; Surge-ai, 2021) and tested the changes in gender discrimination risk before and after its training, as shown in Figure 6. For specific fine-tuning settings, please refer to the table in the Appendix D.4. Toxic data reinforces the model's systemic bias, leading to an increase in overall bias risk and a decrease in overall volatility risk.

Model SizeWe investigate the effects of scaling of various GPT family model sizes that are accessible for public querying. This includes GPT-2 models (137M, 335M, 812M, 1.61B), GPT-Neo (1.3B, 2.7B), and GPT-NeoX (20B). The results was shown in Figure 7. The overall trend shows there is a positive correlation between bias and model size, indicating that larger models might be more susceptible to overfitting or capturing biases present in the data. Conversely, volatility tends to decrease as model size increases, suggesting that larger models exhibit more consistent discrimination.

Rlhfe Reinforcement learning from human feedback (RLHF) ensures the fairness of LLMs by aligning the model with human preferences (Stiennon et al., 2020; Ouyang et al., 2022). We evaluated the impact of RLHF on model bias under our BVF. We tested three different sizes of LLAMA2 series models (7b, 13b, 70b) as detailed by Touvron et al. (2023). This included the BASE models pre-trained on text, as well as the chat versions that undergo additional stages of supervised fine-tuning and RLHF. The results are illustrated in Figure 8.

Our observations indicate that across various sizes of Llama models, the chat versions refined with RLHF exhibit a lower bias risk compared to the base versions, yet they possess a higher volatility risk. This suggests that while RLHF is capable of correcting inherent prejudices within the model, it does not effectively instill the principles of genuine gender equality, resulting in the model's continued capricious behavior.

### The Correlation with Social Factors

We investigate the association between discrimination risk and social factors. In Figure 9, we explore the relationship between the model's occupational discrimination risk and the _salary_ of that occupation using weighted least squares (WLS), where the weights are the number of people in that occupation (STATISTICS, 2022c). The regression curve in Figure 9 indicates that discrimination risk and occupational income are positively correlated, with LLMs being more likely to exhibit

Figure 8: The impact of RLHF on bias risk and volatility risk.

gender bias towards higher-income groups. This may be due to imbalances and stereotypes related to socio-economic status and gender roles present in the data. For the analysis results of other social factors such as _years of education_, _recruitment ratio_, and _occupation-specific word frequency_, please refer to Appendix C.2.

### Risk Management Implications

We compile the distribution of bias risk \(r_{x}^{b}\) and volatility risk \(r_{x}^{v}\) under gender topic corresponding to different occupations \(x\), as shown in Figure 10. Bias risk is typically characterized by a distribution that closely approximates normal distribution, underpinned by the principles of regression to the mean and the law of large numbers. Conversely, volatility risk is characterized by a fat-tailed distribution, diverging noticeably from adherence to the law of large numbers and presenting inherent challenges in precise forecasting, potentially rendering it infeasible. It is crucial to remain vigilant in addressing such unpredictable risks to prevent any potential loss of control over risk management protocols.

## 5 Related Work

LLMs have been empirically demonstrated to manifest biased tendencies and generation inconsistencies (Crawford, 2017; Bordia and Bowman, 2019; Perez et al., 2022; Gallegos et al., 2024; Yang et al., 2024). Numerous studies have aimed to develop methods for quantifying social biases in LLMs, revealing the models' inclination towards inequitable or prejudiced treatment of distinct population groups. Based on the different manifestations of these inequitable or prejudiced treatments, prior works on bias measurement in LLMs can be categorized into two main types:

**Word Embedding Metrics** This category of methods quantifies the biases present in LLMs by examining the geometric properties of their embeddings, such as the distance between neutral terms

Figure 10: The detailed discrimination decomposition under the topic of _Gender_. We fit the bias risk distribution with normal distribution. To better demonstrate the amorphous distribution of volatility risk, we perform interpolation on the calculated values and plot the interpolated lines.

Figure 9: The regressions between _income_ and discrimination risk. Each point denotes an occupation, with its size indicating the population of that occupation. We present the regression result determined by the weighted least squares principle, where the weights are derived from the labor statistics by occupation.

(e.g., occupations) and identity-related terms (e.g., gender pronouns) in the vector space. Bolukbasi et al. (2016) were the first to measure bias through the angle between word embeddings. Subsequent works, such as Caliskan et al. (2017) and Dev et al. (2020), have used cosine similarity between different word classes to assess model discrimination, inspired by the Implicit Association Test (IAT; Greenwald et al. 1998), and introduced the Word Embedding Association Test (WEAT) dataset. More recent studies have extended WEAT to multilingual contexts (Lauscher et al., 2020) and contextual settings (May et al., 2019; Kurita et al., 2019; Dolci et al., 2023). However, several reports suggest that biases identified in the embedding space exhibit only weak or inconsistent correlations with biases observed in downstream tasks (Cabello et al., 2023; Goldfarb-Tarrant et al., 2020).

**Probability-Based Metrics** One of the most widely used techniques for measuring biases in LLMs today is based on statistical metrics. For instance, Nadeem et al. (2020) assess LLM bias by analyzing how often the model selects stereotype-conforming or stereotype-rejecting choices when provided with cloze-style inputs. Similarly, Nangia et al. (2020) evaluate bias by examining the distribution of the model's scores for stereotype-laden sentences. Statistical metrics are more adaptable to downstream tasks and have been applied across various NLP tasks such as coreference resolution (Webster et al., 2018; Zhao et al., 2018; Lu et al., 2020), hate speech detection (Sap et al., 2019), question answering (Li et al., 2020; Zhao et al., 2021), text classification (De-Arteaga et al., 2019), and knowledge assessment (Dong et al., 2023; Zhang et al., 2024). However, current evaluation methods primarily focus on biases in the model's probability predictions under specific templates, without accounting for prediction volatility or the probabilistic nature of the model's responses.

## 6 Conclusion

In this work, we propose the Bias-Volatility Framework (BVF) that measures the parametric biases and generation inconsistencies in large language models (LLMs) and define the mathematical terms that capture statistical properties of the distribution of LLMs' behavior metrics. BVF enables the mathematical breakdown of the overall measurement into bias and volatility components, facilitating attribution analysis. Using stereotype as a case study, we assess the discrimination risk of LLMs. We automate the context collection process and apply BVF to analyze 12 models to understand their biased behavior. Our results show that BVF offers a more comprehensive and flexible analysis than existing approaches, enabling tracking discrimination risk by identifying sources such as consistently biased preference and preference variation (i.e. \(R^{b}\) and \(R^{v}\)), while also facilitating the decomposition of overall discrimination risk into conditional components (i.e. each \(r_{x}\), \(r_{x}^{b}\) and \(r_{x}^{v}\)) for enhanced interpretability and control. Notably, BVF unveils insights regarding the collective stereotypes perpetuated by LLMs, their correlation with societal factors, the effects of human intervention, and the nuanced characteristics of their decomposed bias and volatility risks. These findings hold significant implications for discrimination risk management strategies and agent reward model designs. Finally, we underscore the generality and adaptability of this framework, which open up many interesting new opportunities for using it to measure all kinds of parametric biases (i.e., knowledge and stereotypes) across various multimodal models when the behavior metrics, context and bias criterion are appropriately tailored.

## 7 Ethics

**Definition of Justice-related Topics** In our investigation, we employ a framework predicated on the utilization of binary gender classification and a 5-category race taxonomy. This selection is made with the intention of maintaining congruity with previous research, thereby facilitating a clearer comprehension of our discrimination assessment method BVF. It is imperative to underscore, however, that our adoption of such classifications in no way diminishes our profound respect for individuals whose identities transcend these conventional delineations. Rather, it serves as a pragmatic methodological choice aimed at fostering comparability and coherence with existing research paradigms.

**Tailored BVF Adjustment for Dedicated Assessment Purposes** While we emphasize the theoretical robustness and broad applicability of the BVF in evaluating the inductive biases inherent within LLMs, we recognize the need to customize our approach to meet the specific evaluation goals. We acknowledge that every measurement framework has flaws and blind spots. Therefore, we approach evaluation with careful methodology, understanding the need for continual improvement to better grasp fairness and justice in our analysis.