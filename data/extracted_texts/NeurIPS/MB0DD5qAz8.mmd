# Online Classification with Predictions

Vinod Raman

Department of Statistics

University of Michigan

Ann Arbor, MI 48104

vkraman@umich.edu

&Ambuj Tewari

Department of Statistics

University of Michigan

Ann Arbor, MI 48104

tewaria@umich.edu

###### Abstract

We study online classification when the learner has access to predictions about future examples. We design an online learner whose expected regret is never worse than the worst-case regret, gracefully improves with the quality of the predictions, and can be significantly better than the worst-case regret when the predictions of future examples are accurate. As a corollary, we show that if the learner is always guaranteed to observe data where future examples are easily predictable, then online learning can be as easy as transductive online learning. Our results complement recent work in online algorithms with predictions and smoothed online classification, which go beyond a worse-case analysis by using machine-learned predictions and distributional assumptions respectively.

## 1 Introduction

In online classification, Nature plays a game with a learner over \(T\) rounds. In each round \(t[T]\), Nature selects a labeled example \((x_{t},y_{t})\) and reveals just the example \(x_{t}\) to the learner. The learner, using the history of the game \((x_{1},y_{1}),...,(x_{t-1},y_{t-1})\) and the current example \(x_{t}\), makes a potentially randomized prediction \(_{t}\). Finally, Nature reveals the true label \(y_{t}\) and the learner suffers the loss \(\{_{t} y_{t}\}\). Given access to a _hypothesis class_\(^{}\) consisting of functions \(h:\), the goal of the learner is to minimize its _regret_, the difference between its cumulative mistake and that of the best fixed hypothesis \(h\) in hindsight. We say a class \(\) is online learnable if there exists a learning algorithm that achieves vanishing average regret for _any_, potentially adversarial chosen, stream of labeled examples \((x_{1},y_{1}),...,(x_{T},y_{T})\). Canonically, one also distinguishes between the realizable and agnostic settings. In the realizable setting, Nature must choose a stream \((x_{1},y_{1}),...,(x_{T},y_{T})\) such that there exists a \(h\) for which \(h(x_{t})=y_{t}\) for all \(t[T]\). On the other hand, in the agnostic setting, no such assumptions on the stream are placed.

Due to applications in spam filtering, image recognition, and language modeling, online classification has had a long, rich history in statistical learning theory. In a seminal work, Littlestone (1987) provided a sharp quantitative characterization of which binary hypothesis classes \(\{0,1\}^{}\) are online learnable in the realizable setting. This characterization was in terms of the finiteness of a combinatorial dimension called the Littlestone dimension. Twenty-two years later, Ben-David et al. (2009) proved that the Littlestone dimension continues to characterize the online learnability of binary hypothesis classes in the agnostic setting. Later, Daniely et al. (2011) generalized the Littlestone dimension to multiclass hypothesis classes \(^{}\), and showed that it fully characterizes multiclass online learnability when the label space \(\) is finite. More recently, Hanneke et al. (2023) extended this result to show that the multiclass Littlestone dimension continues to characterize multiclass online learnability even when \(\) is unbounded.

While elegant, the characterization of online classification in terms of the Littlestone dimension is often interpreted as an _impossibility_ result (Haghtalab, 2018). Indeed, due to the restrictive nature of the Littlestone dimension, even simple classes like the \(1\)-dimensional thresholds\(\{x a\}:a\}\) are not online learnable in the realizable setting. This hardness arises mainly due to a worst-case analysis: the adversary is allowed to choose _any_ sequence of labeled examples, even possibly adapting to the learner's strategy. In many situations, however, the sequence of data is "easy" and a worst-case analysis is too pessimistic. For example, if one were to use the daily temperatures to predict snowfall, it is unlikely that temperatures will vary rapidly within a given week. Even so, one might have to access to temperature forecasting models that can accurately predict future temperatures. This motivates a _beyond-worst-case_ analysis of online classification algorithms by proving guarantees that adapt to the "easiness" of the example stream.

The push for a beyond-worst-case analysis has its roots in classical algorithm design (Roughgarden, 2021). Of recent interest is Algorithms with Predictions (AwP), a specific sub-field of beyond-worst-case analysis of algorithms (Mitzenmacher and Vassilvitskii, 2022). Here, classical algorithms are given additional information about the problem instance in the form of machine-learned predictions. Augmented with these predictions, the algorithm's goal is to perform optimally on a per-input basis when the predictions are good (known as _consistency_), while always ensuring optimal worst-case guarantees (known as _robustness_). Ideally, algorithms are also _smooth_, obtaining performance guarantees that interpolate between instance and worst-case optimality as a function of prediction quality. After a successful application to learning index structures (Kraska et al., 2018), there has been an explosion of work designing algorithms whose guarantees depend on the quality of available, machine-learned predictions Mitzenmacher and Vassilvitskii (2022). For example, machine-learned predictions have been used to achieve more efficient data-structures (Lin et al., 2022), faster runtimes (Chen et al., 2022; Ergun et al., 2021), better accuracy-space tradeoffs for streaming algorithms (Hsu et al., 2019), and improved performance bounds for online algorithms (Purohit et al., 2018).

Despite this vast literature, the accuracy benefits of machine-learned predictions for online classification are, to the best of our knowledge, unknown. In this work, we bridge the gap between AwP and online classification. In contrast to previous work, which go beyond a worst-case analysis in online classification through smoothness or other distributional assumptions (Haghtalab et al., 2020; Block et al., 2022; Wu et al., 2023), we give the learner access to a _Predictor_, a forecasting algorithm that predicts future _examples_ in the data stream. The learner, before predicting a label \(_{t}\), can query the Predictor and receive predictions \(_{t+1},...,_{T}\) on the future examples. The learner can then use the history of the game \((x_{1},y_{1}),...,(x_{t-1},y_{t-1})\), the current example \(x_{t}\), and the predictions \(_{t+1},...,_{T}\) to output a label \(_{t}\). We allow Predictors to be _adaptive_ - they can change their predictions of future examples based on the actual realizations of past examples. From this perspective, Predictors are also online learners, and we quantify the _predictability_ of example streams through their mistake-bounds.

In this work, we seek to design online learning algorithms whose expected regret, given black-box access to a Predictor, degrades gracefully with the quality of the Predictor's predictions. By doing so, we are also interested in understanding how access to a Predictor may impact the _characterization_ of online learnability. In particular, given a Predictor, when can online learnability become _easier_ than in the standard, worst-case setup? Guided by these objectives, we make the following contributions.

1. In the realizable and agnostic settings, we design online learners that, using black-box access to a Predictor, adapt to the "easiness" of the example stream. When the predictions of the Predictor are good, our learner's expected mistakes/regret significantly improves upon the worst-case guarantee. When the Predictor's predictions are bad, the expected mistakes/regret of our learner matches the optimal worst-case expected mistake-bound/regret. Finally, our learner's expected mistake-bound/regret degrades gracefully with the quality of the Predictor's predictions.
2. We show that having black-box access to a good Predictor can make learning much easier than the standard, worst-case setting. More precisely, good Predictors allow "offline" learnable classes to become online learnable. In this paper, we take the "offline" setting to be transductive online learning (Ben-David et al., 1997; Hanneke et al., 2024) where Nature reveals the entire sequence of examples \(x_{1},...,x_{T}\) (but not the labels \(y_{1},...,y_{T}\)) to the learner _before_ the game begins. Many "offline" learnable classes are not online learnable. For example, when \(=\{0,1\}\), transductive online learnability is characterized by the finiteness of the VC dimension, the same dimension that characterizes PAC learnability. Thus, our result is analogous to that in smoothed online classification, where PAC learnability is also sufficient for online learnability (Haghtalab et al., 2020; Block et al., 2022).

A notable property of our realizable and agnostic online learners is their use of black-box access to a transductive online learner to make predictions. In this sense, our proof strategies involve reducing online classification with predictions to transductive online learning. For both contributions (1) and (2), we consider only the realizable setting in the main text. The results and arguments for the agnostic setting are nearly identical and thus deferred to Appendix F.

### Related Works

**Online Algorithms with Predictions.** Online Algorithms with Predictions (OAwP) has emerged as an important paradigm lying at the intersection of classical online algorithm design and machine learning. Many fundamental online decision-making problems including ski rental (Gollapudi and Panigrahi, 2019; Wang et al., 2020; Bamas et al., 2020), online scheduling (Lattanzi et al., 2020; Wei and Zhang, 2020; Scully et al., 2021), online facility location (Almanza et al., 2021; Jiang et al., 2021), caching (Lykouris and Vassilvitskii, 2021; Elias et al., 2024), and metrical task systems (Antoniadis et al., 2023), have been analyzed under this framework. Recently, Elias et al. (2024) consider a model where the predictor is allowed to learn and adapt its predictions based on the observed data. This is contrast to previous work on learning-augmented online algorithms, where predictions are made from machine learning models trained on historical data, and thus their predictions are static and non-adaptive to the current task at hand. Elias et al. (2024) study a number of fundamental problems, like caching and scheduling, and show how explicitly designed predictors can lead to improved performance bounds. In this work, we consider a model similar to Elias et al. (2024), where the predictions available to the learning algorithms are not fixed, but rather adapt to the true sequence of data processed by the learning algorithm. However, unlike Elias et al. (2024), we do not hand-craft these predictions, but rather assume our learning algorithms have black-box access to a machine-learned prediction algorithm.

**Transductive Online Learning.** In the Transductive Online Learning setting, Nature reveals the entire sequence of examples \(x_{1},...,x_{T}\) to the learner _before_ the game begins. The goal of the learner is to predict the corresponding labels \(y_{1},...,y_{T}\) in order, receiving the true label \(y_{t}\) only after making the prediction \(_{t}\) for example \(x_{t}\). First studied by Ben-David et al. (1997), recent work by Hanneke et al. (2024) has established the minimax rates on expected mistakes/regret in the realizable/agnostic settings. In the context of online classification with predictions, one can think of the transductive online learning setting as a special case where the Predictor never makes mistakes.

**Smoothed Online Classification.** In addition to AwP, smoothed analysis (Spielman and Teng, 2009) is another important sub-field of beyond-worst-case analysis of algorithms. By placing distributional assumptions on the input, one can typically go beyond computational and information-theoretic bottlenecks due to worst-case inputs. To this end, Rakhlin et al. (2011); Haghtalab (2018); Haghtalab et al. (2020); Block et al. (2022) consider a _smoothed_ online classification model. Here, the adversary has to choose and draw examples from sufficiently anti-concentrated distributions. For binary classification, Haghtalab (2018) and Haghtalab et al. (2020) showed that smoothed online learnability is as easy as PAC learnability. That is, the finiteness of a _smaller_ combinatorial parameter called the VC dimension is sufficient for smoothed online classification. In this work, we also go beyond the worst-case analysis standard in online classification, but consider a different model where the adversary is constrained to reveal a sequence of examples that are _predictable_. In this model, we also show that the VC dimension can be sufficient for online learnability.

## 2 Preliminaries

Let \(\) denote an example space and \(\) denote the label space. We make no assumptions about \(\), so it can be unbounded (e.g., \(=\)). Let \(^{}\) denote a hypothesis class. For a set \(A\), let \(A^{}=_{n=0}^{}A^{n}\) denote the set of all finite sequences of elements in \(A\). Moreover, we let \(A^{ n}\) denote the set of all sequences of elements in \(A\) of size at most \(n\). Then, \(^{}\) denotes the set of all finite sequences of examples in \(\) and \(^{}\) denotes a particular family of such sequences. We abbreviate a sequence \(z_{1},...,z_{T}\) by \(z_{1:T}\). Finally, for \(a,b,c\), we let \(a b c=\{a,b,c\}\).

### Online Classification

In online classification, a learner \(\) plays a repeated game against Nature over \(T\) rounds. In each round \(t[T]\), Nature picks a labeled example \((x_{t},y_{t})\) and reveals \(x_{t}\) to the learner. The learner makes a randomized prediction \(_{t}\). Finally, Nature reveals the true label \(y_{t}\) and the learner suffers the 0-1 loss \(\{_{t} y_{t}\}\). Given a hypothesis class \(^{}\), the goal of the learner is to minimize its _expected regret_

\[_{}(T,):=_{x_{1:T}}_{y_{ 1:T}^{T}}([_{t=1}^{T}\{ (x_{t}) y_{t}\}]-_{h}_{t=1}^{T} \{h(x_{t}) y_{t}\}),\]

where the expectation is only over the randomness of the learner. A hypothesis class \(\) is said to be online learnable if there exists an (potentially randomized) online learning algorithm \(\) such that \(_{}(T,)=o(T)\). If it is guaranteed that the learner always observes a sequence of examples labeled by some hypothesis \(h\), then we say we are in the _realizable_ setting and the goal of the learner is to minimize its _expected cumulative mistakes_,

\[_{}(T,):=_{x_{1:T}^{T}}_ {h}[_{t=1}^{T}\{(x_{t} ) h(x_{t})\}],\]

where again the expectation is taken only with respect to the randomness of the learner. It is well known that the finiteness of the multiclass extension of the Littlestone dimension (Ldim) characterizes realizable and agnostic online learnability (Littlestone, 1987; Daniely et al., 2011; Hanneke et al., 2023). See Appendix A for complete definitions.

### Online Classification with Predictions

Motivated by the fact that real-world example streams \(x_{1:T}\) are far from worst-case, we give our learner \(\) black-box access to a _Predictor_\(\), defined algorithmically in Algorithm 1 and formally in Definition 1. In the rest of the paper, we abuse notation by not explicitly indicating that \(\) takes its own past predictions as input. That is, given a sequence \(x_{1:T}^{T}\), we will let \((x_{1:t})\) denotes its prediction on the \(t\)'th round.

**Definition 1** (Predictor).: _A Predictor \(:(^{T})^{}(^{T})\) is a map that takes in a sequence of instances \(x_{1},x_{2},...\), its own past predictions \(_{1:T}^{1},_{1:T}^{2},...\), and outputs a distribution \((^{T})\). The Predictor make its next prediction by sampling \(x_{1:T}\)._

``` Input: Time horizon \(T\)
1for\(t=1,...,T\)do
2 Nature reveals the true example \(x_{t}\).
3 Observe \(x_{t}\), update, and make a (potentially randomized) prediction \(_{1:T}^{t}\).
4 end ```

**Algorithm 1** Predictor \(\)

**Remark.** We highlight that our Predictors are very general and can also use side information, in addition to the past examples, to make predictions about future examples. For example, if the examples are daily average temperatures, then Predictors can also use other covariates, like humidity, precipitation, and wind speed, to predict future temperatures.

In each round \(t[T]\), the learner \(\) can query the Predictor \(\) to get a sense of what examples it will observe in the future. Then, the learner \(\) can use the history \((x_{1},y_{1}),..,(x_{t-1},y_{t-1})\), the current example \(x_{t}\), _and_ the future predicted examples to classify the current example. Protocol 2 makes explicit the interaction between the learner, the Predictor, and Nature.

Note that, in every round \(t[T]\), the Predictor \(\) makes a prediction about the _entire_ sequence of \(T\) examples, even those that it has observed in the past. This is mainly for notational convenience as we assume that our Predictors are _consistent_.

**Assumption 1** (Consistency).: _A Predictor is consistent if for every sequence \(x_{1:T}^{T}\) and every time point \(t[T]\), the prediction \(_{1:T}=(x_{1:t})\) satisfies the property that \(_{1:t}=x_{1:t}\)._Although stated as an assumption, it is without loss of generality that Predictors are consistent - any inconsistent Predictor can be made consistent by hard coding its input into its output. In addition to consistency, we assume that our Predictors are _lazy_.

**Assumption 2** (Laziness).: _A Predictor is lazy if for every sequence \(x_{1:T}^{T}\) and every \(t[T]\), if \((x_{1:t-1})_{t}=x_{t}\), then \((x_{1:t})=(x_{1:t-1})\). That is, \(\) does not change its prediction if it is correct._

Since Predictors are also online learners, the assumption of laziness is also mild: non-lazy online learners can be generically converted into lazy ones (Littlestone, 1987, 1989). We always assume that Predictors are consistent and lazy and drop these pronouns for the rest of the paper.

**Remark.** We highlight that Predictors are adaptive and change their predictions based on the realizations of past examples. This is contrast to existing literature in OAwP, where machine-learned predictions are often static. Nevertheless, our framework is more general and captures the setting where predictions of examples are made once and fixed throughout the game. Indeed, consider the consistent, lazy Predictor that fixes a sequence \(z_{1:T}^{T}\) before the game begins, and for every \(t[T]\), outputs the predictions \(_{1:T}^{t}\) such that \(_{1:t}^{t}=x_{1:t}\) and \(_{t+1:T}^{t}=z_{t+1:T}\).

Ideally, when given access to a Predictor \(\), the expected regret of \(\) should degrade gracefully with the quality of \(\)'s predictions. To this end, we quantify the performance of a Predictor \(\) through

\[_{}(x_{1:T}):=[_{t=2}^{T} \{(x_{1:t-1})_{t} x_{t}\}],\]

the expected number of mistakes that \(\) makes on a sequence of examples \(x_{1:T}^{T}\). In Section 3, we design an online learner whose expected regret/mistake-bound on a stream \((x_{1},y_{1}),...,(x_{T},y_{T})\) can be written in terms of \(_{}(x_{1:T})\).

### Predictability

Predictors and their mistake bounds offer us to ability to define and quantify a notion of "easiness" for example streams \(x_{1:T}\). In particular, we can distinguish between example streams that are predictable and unpredictable. To do so, let \(^{}\) denote a collection of finite sequences of examples. By restricting Nature to playing examples streams in \(\), we can define analogous notions of minimax expected regret

\[_{}(T,,):=_{x_{1:T} }_{y_{1:T}^{T}}[_{t=1}^{T} \{(x_{t}) y_{t}\}-_{h}_{t=1}^{ T}\{h(x_{t}) y_{t}\}],\]

and minimax expected mistakes,

\[_{}(T,,):=_{x_{1:T} }_{h}[_{t=1}^{T} \{(x_{t}) h(x_{t})\}].\]

As usual, we say that a tuple \((,)\) is online and realizable online learnable if \(_{}_{}(T,,)=o(T)\) and \(_{}_{}(T,,)=o(T)\) respectively. If \(=^{}\), then the definitions above recover the standard, worst-case online classification setup. However, in the more general case where \(^{}^{}\), we can use the _existence_ of good Predictors \(\) and their mistake bounds to quantify the "easiness" of a stream class \(\). That is, we say \(\) is predictable if there exists a consistent, lazy Predictor \(\) such that \(_{}(T,):=_{x_{1:T}}_{}(x_{1:T})=o(T)\).

**Definition 2** (Predictability).: _A class \(^{}\) is predictable if and only if \(_{}_{}(T,)=o(T)\)._

Definition 2 provides a qualitative definition of what it means for a sequence of examples to be predictable, and therefore "easy". If \(^{}\) is a predictable class of example streams, then a stream \(x_{1:T}^{T}\) is predictable if \(x_{1:T}\). By having access to a good Predictor, sequences of examples that previously exhibited "worst-case" behavior, now become predictable. One natural predictable collection of streams are those induced by easy-to-learn discrete-time dynamical systems (Raman et al., 2024). That is, let \(\) be the state space for a finite collection \(\) of transition functions. Then, given an initial state \(x_{0}\), one can consider the stream class \(\) to be the set of all trajectories induced by transition functions in \(\). In Section 3, we show that for such classes of predictable examples, "offline" learnability is sufficient for online learnability.

### Offline Learnability

In the classical analysis of online algorithms, one competes against the best "offline" solution. In the context of online classification, this amounts to comparing online learnability to "offline" learnability, where we interpret the "offline" setting as the case where Nature reveals the sequence of examples \((x_{1},...,x_{T})\) before the game begins. In particular, compared to the standard online learning setting, in the "offline" version, the learner knows the sequence of examples \(x_{1},...,x_{T}\) before the game begins, and its goal is to predict the corresponding labels \(y_{1},...,y_{T}\). This setting was recently named "Transductive Online Learning" (Hanneke et al., 2024) and the minimax rates in both the realizable and agnostic setting have been established (Ben-David et al., 1997; Hanneke et al., 2023). For the remainder of the paper, we will use offline and transductive online learnability interchangeably.

For a randomized offline learner \(\), we let

\[_{}(T,):=_{x_{1:T}^{T}}_ {y_{1:T}^{T}}[_{t=1}^{T}\{ _{x_{1:T}}(x_{t}) y_{t}\}-_{h}_{t=1}^{T} \{h(x_{t}) y_{t}\}]\]

denote its minimax expected regret and

\[_{}(T,):=_{h}_{x_{1:T} ^{T}}[_{t=1}^{T}\{_{x _{1:T}}(x_{t}) h(x_{t})\}].\]

denote its minimax expected mistakes. We use the notation \(_{x_{1:T}}\) to indicate that \(\) was initialized with the sequence \(x_{1:T}\) before the game begins. If \(_{}(T,)=o(T)\) or \(_{}(T,)=o(T)\), then we say that \(\) is a no-regret offline learner. It turns out that realizable and agnostic offline learnability are equivalent (Hanneke et al., 2024). That is, \(_{}(T,)=o(T)_{ }(T,)=o(T)\). Thus, we say a class \(^{}\) is offline learnable if and only if there exists a no-regret offline learner for \(\).

When \(||=2\), Ben-David et al. (1997) and Hanneke et al. (2023) show that the finiteness of a combinatorial dimension called the Vapnik-Chervonenkis (VC) dimension (or equivalently PAC learnability) is sufficient for offline learnability (see Appendix A for complete definitions).

**Lemma 2.1** (Ben-David et al. (1997), Hanneke et al. (2024)).: _For every \(\{0,1\}^{}\), there exists a deterministic offline learner \(\) such that_

\[_{}(T,)=O()_{ 2}T\]

_where \(()\) is the VC dimension of \(\)._

In Section 3, we use this upper bound in Lemma 2.1 to prove that PAC learnability of \(\) implies \((,)\) online learnability when \(\) is predictable. Interestingly, Hanneke et al. (2024) also establish a trichotomy in the minimax expected mistakes for offline learning in the realizable setting. That is, for any \(^{}\) with \(||<\), the quantity \(_{}(T,)\) can only be \((1)\), \((_{2}T)\), or \((T)\). On the other hand, in the agnostic setting, \(_{}(T,)\) can be \(()\) or \((T)\), where \(\) hides poly-log terms in \(T\).

Our main result in Section 3 shows that offline learnability is sufficient for online learnability under predictable examples. The following technical lemma will be important when proving so.

**Lemma 2.2**.: [Ceccherini-Silberstein et al., 2017, Lemma 5.17] _Let \(g:_{+}_{+}\) be a positive sublinear function. Then, \(g\) is bounded from above by a concave sublinear function \(f:_{+}_{+}\)._In light of Lemma 2.2, we let \(\) denote the smallest concave sublinear function upper bounding the positive sublinear function \(f\). For example, our regret bounds in Section 3 will often be in terms of \(}_{}(T,)\). Although in full generality \(_{}(T,)}_{ }(T,)\), in many cases we have equality. For example, when \(||=2\), the trichotomy of expected minimax rates established by Theorem 4.1 in Hanneke et al. (2024) shows that \(_{}(T,)=}_{}(T,)\).

## 3 Adaptive Rates in the Realizable Setting

In this section, we design learning algorithms whose expected mistake bounds, given black-box access to a Predictor \(\) and offline learner \(\), adapt to the quality of predictions by \(\) and \(\). Our main quantitative result is stated below.

**Theorem 3.1** (Realizable upper bound).: _For every \(^{}\), Predictor \(\), and no-regret offline learner \(\), there exists an online learner \(\) such that for every realizable stream \((x_{1},y_{1}),...,(x_{T},y_{T})\), \(\) makes at most_

\[3()}_{(i)}_{}(x_{1:T})+1)\,_{}(T,) }_{(ii)}(_{}(x_{1:T})+1)\, }_{}_{}( x_{1:T})+1}+1,+_{2}T}_{(iii)}+5\]

_mistakes in expectation, where \(()\) is the Littlestone dimension of \(\)._

We highlight some important consequences of Theorem 3.1. Firstly, when \(_{}(x_{1:T})=0\), the expected mistake bound of \(\) matches (up to constant factors) that of the offline learner \(\). Thus, when \(_{}(x_{1:T})=0\) and \(\) is a minimax optimal offline learner, our learner \(\) performs as well as the best offline learner. Secondly, the expected mistake bound of \(\) is always at most \(3\,()+5\); the minimax worst-case mistake bound up to constant factors. Thus, our learner \(\) never does worse than the worst-case mistake bound. Thirdly, the expected mistake bound of \(\) gracefully interpolates between the offline and worst-case optimal rates as a function of \(_{}(x_{1:T})\). In Section 3.3, we show that the dependence of \(\)'s mistake bound on \(_{}(x_{1:T})\) and \(_{}(T,)\) can be tight. Lastly, we highlight that Theorem 3.1 makes no assumption about the size of \(\).

With respect to learnability, Corollary 3.2 shows that offline learnability of \(\) is sufficient for online learnability under predictable examples.

**Corollary 3.2** (Offline learnability \(\) Realizable Online learnability with Predictable Examples).: _For every \(^{}\) and \(^{}\),_

\[\]

_is predictable and \[\] is offline learnable \[(,)\] is realizable online learnable._

This follows from a slight modification of the proof of Theorem 3.1 along with the fact that the term \((_{}(T,)+1)}_{}_{}(T,)+1}, \!=o(T)\) when \(_{}(T,)=o(T)\) and \(_{}(T,)=o(T)\). In addition, we can also establish a quantitative version of Corollary 3.2 for VC classes.

**Corollary 3.3**.: _For every \(\{0,1\}^{}\), Predictor \(\) and \(^{}\), there exists an online learner \(\) such that_

\[_{}(T,,)=O( )(_{}(T,)+1)_{2} {_{}(T,)+1}\!+_{2}T.\]

We prove both Corollary 3.2 and 3.3 in Appendix C. Corollary 3.3 shows that PAC learnability implies online learnability under predictable examples. Moreover, for VC classes, when \(_{}(x_{1:T})=0\), the upper bound in Corollary 3.3 exactly matches that of Lemma 2.1. An analogous corollary in terms of the Natarajan dimension (see Appendix A for definition) holds when \(||<\).

The remainder of this section is dedicated to proving Theorem 3.1. The proof involves constructing three _different_ online learners, with expected mistake bounds (i), (ii), and (iii) respectively, and then running the Deterministic Weighted Majority Algorithm (DWMA) using these learners as experts (Arora et al., 2012). The following guarantee of DWMA along with upper bounds (i), (ii), and (iii) gives the upper bound in Theorem 3.1 (see Appendix D for complete proof).

**Lemma 3.4** (DWMA guarantee [Arora et al., 2012]).: _The DWMA run with \(N\) experts and learning rate \(=1/2\) makes at most \(3(_{i[N]}M_{i}+_{2}N)\) mistakes, where \(M_{i}\) is the number of mistakes made by expert \(i[N]\)._

The online learner obtaining the upper bound \(()\) is the celebrated Standard Optimal Algorithm [Littlestone, 1987, Daniely et al., 2011], and thus we omit the details here. Our second and third learners are described in Sections 3.1 and 3.2 respectively. Finally, in Section 3.3, we give a lower bound showing that our upper bound in Theorem 3.1 can be tight.

### Proof of upper bound (ii) in Theorem 3.1

Consider a lazy, consistent predictor \(\). Given any sequence of examples \(x_{1:T}^{T}\), the Predictor \(\) makes \(c\) mistakes at some timepoints \(t_{1},...,t_{c}[T]\). Since \(\) may be randomized, both \(c\) and \(t_{1},...,t_{c}\) are random variables. Crucially, since \(\) is lazy, for every \(i\{0,...,c+1\}\), the predictions made by \(\) on timepoints strictly between \(t_{i}\) and \(t_{i+1}\) are correct and remain unchanged, where we take \(t_{0}=0\) and \(t_{c+1}=T+1\). This means that on round \(t_{i}\), we have that \(_{t_{i}:t_{i+1}-1}^{t_{i}}=x_{t_{i}:t_{i+1}-1}\). Therefore, initializing a fresh copy of an offline learner \(\) with the predictions \(_{t_{i}:T}^{t}\) ensures that \(\) makes at most \(_{}(T-t_{i}+1,)\) mistakes on the stream \((x_{t_{i}},y_{t_{i}}),...,(x_{t_{i+1}-1},y_{t_{i+1}-1})\). Repeating this argument for all adjacent pairs of timepoints in \(\{t_{1},...,t_{c}\}\), gives the following strategy: initialize a new offline learner \(\) every time \(\) makes a mistakes, and use \(\) to make predictions until the next time \(\) makes a mistake. Algorithm 3 implements this idea.

``` Input: Hypothesis class \(\), Offline learner \(\), Time horizon \(T\)
1Initialize:\(i=0\)for\(t=1,...,T\)do
2 Receive \(x_{t}\) from Nature.
3 Receive predictions \(_{1:T}^{t}\) from Predictor \(\) such that \(_{1:t}^{t}=x_{1:t}\).
4if\(t=1\) or \(_{t}^{t-1} x_{t}\) (i.e. \(\) made a mistake)then
5 Let \(^{i}\) be a new copy of \(\) initialized with the sequence \(_{t:T}^{t}\) and set \(i i+1\).
6 Query \(^{i}\) on example \(x_{t}\) and play its returned prediction \(_{t}\).
7 Receive true label \(y_{t}\) from Nature and pass it to \(^{i}\).
8
9 end for ```

**Algorithm 3** Online Learner

**Lemma 3.5**.: _For every \(^{}\), Predictor \(\), no-regret offline learner \(\), and realizable stream \((x_{1},y_{1}),...,(x_{T},y_{T})\), Algorithm 3 makes at most \((_{}(x_{1:T})+1)\,_{}(T,)\) mistakes in expectation._

Proof.: Let \(\) denote Algorithm 3, \((x_{1},y_{1}),...,(x_{T},y_{T})\) denote the realizable stream to be observed by \(\), and \(h^{}\) to be the labeling hypothesis. Let \(c\) be the random variable denoting the number of mistakes made by Predictor \(\) on the stream and \(t_{1},...,t_{c}\) be the random variables denoting the time points where \(\) makes these errors (e.g. \(_{t_{i}-1}^{t_{i}-1} x_{t_{i}}\)). Note that \(t_{i} 2\) for all \(i[c]\). We will show pointwise for every value of \(c\) and \(t_{1},...,t_{c}\) that \(\) makes at most \((c+1)\,_{}(T,)\) mistakes in expectation over the randomness of \(\). Taking an outer expectation with respect to the randomness of \(\) and using the fact that \([c]=_{}(x_{1:T})\), completes the proof.

First, consider the case where \(c=0\) (i.e. \(\) makes no mistakes). Then, since \(\) is lazy, we have that \(_{1:T}^{t}=x_{1:T}\) for every \(t[T]\). Thus line 5 fires exactly once on round \(t=1\), \(\) initializes an offline learner \(^{1}\) with \(x_{1:T}\), and \(\) uses \(^{1}\) to make its prediction on all rounds. Thus, \(\) makes at most \(_{}(T,)\) mistakes in expectation.

Now, let \(c>0\) and \(t_{1},...,t_{c}\) be the time points where \(\) errs. Partition the sequence \(1,...,T\) into the disjoint intervals \((1,...,t_{1}-1)\), \((t_{1},...,t_{2}-1),...,(t_{c},...,T)\). Define \(t_{0}:=1\) and \(t_{c+1}:=T\). Fix an \(i\{0,...,c\}\). Observe that for every \(j\{t_{i},...,t_{i+1}-1\}\), we have that \(_{1:t_{i+1}-1}^{j}=x_{t_{i+1}-1}\). This comes from the fact that \(\) does not error on timepoints \(t_{i}+1,...,t_{i+1}-1\) and is both consistent and lazy (see Assumptions 1 and 2). Thus, line 5 fires on round \(t_{i}\), \(\) initializes an offline learner \(^{i}\) with the sequence \(_{t_{i}:T}^{t_{i}}=x_{t_{i}:t_{i+1}-1}_{t_{i+1}:T}^{t_{i}}\), and \(\) uses \(^{i}\) it to make predictions for all remaining timepoints \(t_{i},...,t_{i+1}-1\). Note that line 5 does not fire on timepoints \(t_{i}+1,...,t_{i+1}-1\).

Consider the hypothetical labeled stream of examples

\[(_{t_{i}}^{t_{i}},h^{}(_{t_{i}}^{t_{i}})),...,(_{T}^{t_ {i}},h^{}(_{T}^{t_{i}}))=(x_{t_{i}},y_{t_{i}}),...,(x_{t_{i+1}-1},y _{t_{i+1}-1}),(_{t_{i+1}}^{t_{i}},h^{}(_{t_{i+1}}^{t_{i}})),...,(_{T}^{t_{i}},h^{}(_{T}^{t_{i}})).\]

By definition, \(^{i}\), after initialized with \(_{t_{i}:T}^{t_{i}}\), makes at most \(_{}(T-t_{i}+1,)\) mistakes in expectation when simulated on the stream \((_{t_{i}}^{t_{i}},h^{}(_{t_{i}}^{t_{i}})),...,(_{T}^ {t_{i}},h^{}(_{T}^{t_{i}}))\). Thus, \(^{i}\) makes at most \(_{}(T,)\) mistakes in expectation on the _prefix_\((_{t_{i}}^{t_{i}},h^{}(_{t_{i}}^{t_{i}})),...,(_{t _{i+1}-1}^{t_{i}},h^{}(_{t_{i+1}-1}^{t_{i}}))=(x_{t_{i}},y_{t_{i}} ),...,(x_{t_{i+1}-1},y_{t_{i+1}-1})\). Since on the interval timeptition \(t_{i}\), \(\) instantiates \(^{i}\) with the sequence \(_{t_{i}:T}^{t_{i}}\) and proceeds to simulate \(^{i}\) on the sequence of labeled examples \((x_{t_{i}},y_{t_{i}}),...,(x_{t_{i+1}-1},y_{t_{i+1}-1})\), \(\) makes at most \(_{}(T,)\) mistakes in expectation on the sequence \((x_{t_{i}},y_{t_{i}}),...,(x_{t_{i+1}-1},y_{t_{i+1}-1})\). Since the interval \(i\) was chosen arbitrarily, the above analysis is true for every \(i\{0,...,c\}\) and therefore \(\) makes at most \((c+1)_{}(T,)\) mistakes in expectation over the entire stream. 

### Proof sketch of upper bound (iii) in Theorem 3.1

When \(_{}(T,)\) is large (i.e. \(()\)), upper bound (ii) is sub-optimal. Indeed, if \(t_{1},...,t_{c}\) denotes the timepoints where \(\) makes mistakes on the stream \(x_{1:T}\), then Algorithm 3 initializes offline learners with sequences of length \(T-t_{i}+1\). The resulting mistake-bound of these offline learners are then in the order of \(T-t_{i}+1\), which can be large if \(t_{1},...,t_{c}\) are evenly spaced across the time horizon. To overcome this, we construct a _family_\(\) of online learners, each of which explicitly controls the length of the sequences offline learners can be initialized with. Finally, we run DWMA using \(\) as its set of experts. Our family of online learners is parameterized by integers \(c\{0,...,T-1\}\). Given an input \(c\{0,...,T-1\}\), the online learner parameterized by \(c\) partitions the stream into \(c+1\) roughly equally sized parts of size \(\) and runs a fresh copy of Algorithm 3 on each partition. In this way, the online learner parameterized by \(c\) ensures that offline learners are initialized with time horizons at most \(\). Algorithm 4 formalizes this online learner and Lemma 3.6, whose proof is in Appendix B, bounds its expected number of mistakes.

``` Input: Copy of Algorithm 3 denoted \(\), Offline Learner \(\), Time horizon \(T\) Initialize:\(_{i}=i\) for \(i\{1,...,c\}\), \(_{0}=0,\) and \(_{c+1}=T\). for\(t=1,...,T\)do  Let \(i\{0,...,c\}\) such that \(t\{_{i}+1,...,_{i+1}\}\). if\(t=_{i}+1\)then  Let \(_{i}\) be a new copy of \(\) initialized with time horizon \(_{i+1}-_{i}\) and a new copy of \(\). Receive \(x_{t}\) from Nature. Receive predictions \(_{1:T}^{t}\) from Predictor \(\) such that \(_{1:t}^{t}=x_{1:t}\). Forward \(x_{t}\) and \(_{t_{i+1}:_{i+1}}^{t}\) to \(_{i}\) via Lines 2 and 3 of Algorithm 3 respectively. Receive \(_{t}\) from \(_{i}\) via line 6 in Algorithm 3 and predict \(_{t}\). Receive true label \(y_{t}\) and forward it to \(_{i}\) via line 7 in Algorithm 3. ```

**Algorithm 4** Expert\((c)\)

**Lemma 3.6** (Expert guarantee).: _For any \(^{}\), Predictor \(\), and no-regret offline learner \(\), Algorithm 4, given as input \(c\{0,...,T-1\}\), makes at most_

\[(_{}(x_{1:T})+c+1)}_{ }+1,\]

_mistakes in expectation on any realizable stream \((x_{1},y_{1}),...,(x_{T},y_{T})\)._

Note that when \(c=0\) and \(_{}(T,)=}_{ }(T,)\), this bound reduces to the one in Lemma 3.5 up to a constant factor. On the other hand, using \(c=_{}(x_{1:T})\) gives the upper bound

\[2(_{}(x_{1:T})+1)}_{ }_{}(x_{1:T})+1}+1, .\]

Since \(\) contains an Expert parameterized for every \(c\{0,...,T-1\}\), there always exists an expert \(E_{_{}(x_{1:T})}\) initialized with input \(c=_{}(x_{1:T})\). Running DWMA using these set of experts\(\) on the data stream \((x_{1},y_{1}),...,(x_{T},y_{T})\) ensures that our learner does not perform too much worse than \(E_{_{}(x_{1:T})}\). Algorithm 5 formalizes this idea and Lemma 3.7 is proved in Appendix B.

**Lemma 3.7**.: _For every \(^{}\), Predictor \(\), and no-regret offline learner \(\), Algorithm 5 makes at most_

\[6(_{}(x_{1:T})+1)}_{}_{}(x_{1:T})+1}+1,+ _{2}T.\]

_mistakes in expectation on any realizable stream \((x_{1},y_{1}),...,(x_{T},y_{T})\)._

### Lower bounds

In light of Theorem 3.1, it is natural ask whether the upper bounds derived in Section 3 are tight. A notable feature in upper bounds (ii) and (iii) is the product of the two mistake bounds \(_{}(x_{1:T})\) and \(_{}(T,)\). Can this product can be replaced by a sum? Unfortunately, Theorem 3.8 shows that the upper bound in Theorem 3.1 can be tight.

**Theorem 3.8**.: _Let \(=\{\},=\{0,1\}\), and \(=\{x\{x a\}\{x\}\}\). Let \(T,n\) be such that \(n+1\) divides \(T\) and \(+1=2^{k}\) for some \(k\). Then, there exists a Predictor \(\) such that for every online learner \(\) that uses \(\) according to Protocol 2, there exists a realizable stream \((x_{1},y_{1}),...,(x_{T},y_{T})\) such that \(_{}(x_{1:T})=n\) but_

\[[_{t=1}^{T}\{(x_{t}) y_{t}\} ]_{2}\!.\]

Theorem 3.8 shows that the upper bound in Theorem 3.1 is tight up to an additive factor in \(_{2}T\) because Lemma 2.1 gives that \(_{}}_{}(T,)=O( ()_{2}T)\) and \(()=1\). The proof of Theorem 3.8 is technical and provided in Appendix E. Our proof involves four steps. First, we construct a class of streams \(_{n}^{}\). Then, using \(_{n}\), we construct a deterministic, lazy, consistent Predictor \(\) such that \(\) makes mistakes exactly on timepoints \(\{+1,...,+1\}\) for every stream \(x_{1:T}_{n}\). Next, whenever \(x_{1:T}_{n}\), we establish an equivalence between the game defined by Protocol 2 when given access to Predictor \(\) and Online Classification with Peeks, a _different_ game where there is no Predictor, but the learner observes the next \(\) examples at timepoints \(t\{1,+1,...,+1\}\). Finally, for Online Classification with Peeks, we give a strategy for Nature such that it can force any online learner to make \(()}{2}\) mistakes in expectation while ensuring that its selected stream satisfies \(x_{1:T}_{n}\) and \(_{h}_{t=1}^{T}\{h(x_{t}) y_{t}\}=0\). A key component of the fourth step is the stream constructed by [14, Claim 3.4] to show that the minimax mistakes for classes with infinite Ldim is at least \(_{2}T\) in the offline setting.

**Remark**.: Although Theorem 3.8 is stated using the class of one dimensional thresholds, it can be adapted to hold for any VC class with infinite Ldim as these classes embed thresholds [1, Theorem 3].

## 4 Discussion

In this paper, we initiated the study of online classification when the learner has access to machine-learned predictions about future examples. There are many interesting directions for future research and we list two below. Firstly, we only considered the classification setting, and it would be interested to extend our results to online scalar-valued regression. Secondly, we measure the performance of a Predictor through its mistake-bounds. When \(\) is continuous, this might be an unrealistic measure of performance. Thus, it would be interesting to see whether our results can be generalized to the case where \(\) is continuous and the guarantee of Predictors is defined in terms of \(_{p}\) losses.