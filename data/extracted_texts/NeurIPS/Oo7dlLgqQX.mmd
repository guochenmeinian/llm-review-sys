# Questioning the Survey Responses of

Large Language Models

Ricardo Dominguez-Olmedo\({}^{1,2}\) Moritz Hardt\({}^{1,2}\) Celestine Mendler-Dunner\({}^{1,2,3}\)

\({}^{1}\)Max-Planck Institute for Intelligent Systems, Tubingen

\({}^{2}\)Tubingen AI Center

\({}^{3}\)ELLIS Institute Tubingen

{rdo,hardt,cmendler}@tuebingen.mpg.de

###### Abstract

Surveys have recently gained popularity as a tool to study large language models. By comparing survey responses of models to those of human reference populations, researchers aim to infer the demographics, political opinions, or values best represented by current language models. In this work, we critically examine this methodology on the basis of the well-established American Community Survey by the U.S. Census Bureau. Evaluating 43 different language models using de-facto standard prompting methodologies, we establish two dominant patterns. First, models' responses are governed by ordering and labeling biases, for example, towards survey responses labeled with the letter 'A'. Second, when adjusting for these systematic biases through randomized answer ordering, models across the board trend towards uniformly random survey responses, irrespective of model size or pre-training data. As a result, in contrast to conjectures from prior work, survey-derived alignment measures often permit a simple explanation: models consistently appear to better represent subgroups whose aggregate statistics are closest to uniform for any survey under consideration.

## 1 Introduction

Surveys have a long tradition in social science research as a means for gathering statistical information about the characteristics, values, and opinions of human populations (Groves et al., 2009). Insights from surveys inform policy interventions, business decisions, and science across various domains. Surveys typically consist of a series of well-curated questions in a multiple-choice format, with unambiguous framing and a set of answer choices carefully selected by domain experts. Surveys are then presented to groups of individuals and their answers are aggregated to gain statistical insights about the populations that these groups of individuals represent.

Many established survey questionnaires together with the carefully collected answer statistics are publicly available. Machine learning researchers have identified the potential benefits of building on this valuable data resource to study large language models (LLMs). Survey questions offer a way to systematically prompt LLMs, and the aggregate statistics over answers collected by surveying human populations serve as a reference point for evaluation. As a result, the use of surveys has recently gained popularity for studying LLMs' biases (Santurkar et al., 2023; Durmus et al., 2023). Also prompting LLMs with survey questions, researchers in the social sciences have explored using LLMs to emulate the survey responses of human populations (Argyle et al., 2023; Lee et al., 2023). If effective proxies, simulated responses could augment or replace the expensive data collection process involving human subjects and provide insights into subpopulations that are otherwise hard to reach.

It is tempting to prompt LLMs with survey questions, due to their syntactic similarity to question answering tasks (Brown et al., 2020; Liang et al., 2022). However, it is a priori unclear how to interpret their answers. Rather than knowledge testing, surveys seek to elicit aggregate statistics over individuals, providing an unbiased view on the properties of the population they are targeting. The quality of survey data hinges on the validity and robustness of the conclusions that can be drawn from it. Clearly, running a survey on LLMs is different from interrogating humans and thus it comes with distinct challenges. While much research has gone into carefully designing surveys to ensure faithful human responses, it is unclear whether prompting LLMs with the same surveys satisfies similar premises out-of-the-box. We devote this work to gain systematic insights into the survey responses of LLMs, what we can expect to learn from them, and to what extent they resemble those of human populations.

### Our work

The basis of our investigation is the American Community Survey1 (ACS), a demographic survey conducted by the U.S. Census Bureau at a national level, on a yearly basis. We curate a questionnaire composing of 25 multiple choice questions from the 2019 ACS. We prompt 43 language models of varying size with these questions, individually and in sequence, and we record their probability distribution over answers. Based on the collected data, we investigate the following two questions: _What can we infer about LLMs, and the data they have been trained on, from their survey responses? Does the data generated by prompting models to answer the ACS questionnaire qualitatively resemble the census data collected by surveying the U.S. population?_ See Figure 1.

We start by inspecting models' distributions over answers to individual survey questions when the questions are asked independently. We observe that the entropy of response distributions differs substantially across models of varying size. Entropy tends to increase log linearly with model size, and it is preserved across different questions asked. We find that this differences arise because strong ordering and labeling biases confound models' answers. In fact, after adjusting for such systematic biases through randomized choice ordering, we find that response distributions are very similar across models and tend to correspond to highly balanced answers.

Comparing models' adjusted responses to those of the U.S. census population, we find that natural variations in entropy across questions are not reflected in the responses. Instead, on average across questions, models' responses are no closer to the census population, or the population of any state within the US, than to a fixed uniform baseline. This qualitative difference between model responses and human data puts into question the insights that can be gained from such comparisons. We find that even after instruction-tuning this trend persists, and model responses have consistently higher entropy than any human population we compare to, independent of the survey used. Only for models of size larger than \(70\) billion parameters we can recognize a trend that the divergence between model responses and the census data decreases after instruction-tuning.

With these insights in mind, we inspect conjectures from prior work related to survey derived alignment metrics, that is, that differences in similarity between models' and populations' responses might be attributable to certain demographics being better represented in the training data. Instead, our results suggest a much simpler explanation: the relative alignment of model responses with different

Figure 1: We prompt language models with questions from the American Community Survey (ACS). We systematically compare modelsâ€™ survey responses to those of the U.S. Census.

demographic subgroups can be explained by the entropy of the subgroups' responses, irrespective of the data or training procedure employed to train the model. We demonstrate this beyond the ACS on other surveys considered by prior work. As such, our findings provide important context to prior studies that employ surveys to examine the biases of LLMs.

More broadly, our findings suggest caution when treating language models' survey responses as a faithful representation of any human population, at least a present time, as it could lead to potentially misguided conclusions about alignment.

### Related work

Despite the syntactical similarities, there are important differences between evaluating LLMs on the basis of their survey responses and traditional question answering evaluations (Liang et al., 2022). Question answering (QA) tasks predominantly serve the purpose of knowledge testing (e.g., Kwiatkowski et al., 2019; Rajpurkar et al., 2016; Talmor et al., 2019; Mihaylov et al., 2018). In such setting, a language model's answer to some unambiguous input question is extracted by computing its most likely completion. Similarly, for questions that lack a clear answer (e.g., "Angela and Patrick are sitting together. Who is an entrepreneur?") models' most likely response have been used to investigate various biases of LLMs (Li et al., 2020; Mao et al., 2021; Perez et al., 2023; Abid et al., 2021; Jiang et al., 2022).

When evaluating LLMs on the basis of survey questions, the focus is not on the model's most likely completion but rather on the probability distribution that the model assigns to various answer choices. For example, not whether the model is more likely to answer "Yes" than "No" to a given survey question, but the normalized probability assigned to each of the two answer choices. See Figure 1. More concretely, Santurkar et al. (2023) study LLMs' answer distributions for multiple-choice opinion polling questions, measuring their similarity to those of various U.S. demographic groups. They extract models' answer distributions from the next token probabilities corresponding to each answer choice. Subsequent works employ a similar methodology but instead consider transnational opinion surveys (Durmus et al., 2023; AlKhamissi et al., 2024) and moral beliefs surveys (Scherrer et al., 2024). We adopt this popular methodology to systematically investigate the properties of models' answer distributions on the basis of a well-established demographic survey.

Instead of asking questions individually, Hartmann et al. (2023); Rutinowski et al. (2023); Motoki et al. (2023); Feng et al. (2023) sequentially prompt language models to answer entire political compass or voting advice questionnaires. Rather than aggregating answers into a political affinity score, our focus is instead on examining whether models' responses qualitatively resemble those of human populations. We discuss this sequential generation setting in detail in Appendix F.

Lastly, there is an emerging body of research that integrates LLMs into computational social science (Ziems et al., 2024). This includes tasks such as taxonomic labeling, where language models are employed for tasks such as opinion prediction (Kim and Lee, 2023; Mellon et al., 2022), and free-form coding, where language models are used to generate explanations for social science constructs (Nelson et al., 2021). Recent studies have also investigated the feasibility of using LLMs to simulate human participants in psychological, psycholinguistic, and social psychology experiments (Dillion et al., 2023; Aher et al., 2023), or as proxies for specific human populations in social science research (Argyle et al., 2023; Lee et al., 2023; Sanders et al., 2023) and economics (Brand et al., 2023; Horton, 2023). Within this context, our work suggests caution in relying on the survey responses of LLMs to elicit synthetic responses that resemble those of human populations and highlights potential pitfalls.

## 2 Surveying language models

We employ the de-facto standard methodology to survey language models introduced by Santurkar et al. (2023). For every survey question, we generate a prompt containing the multiple-choice question and we collect language models' probability distribution over answer choices. Formally, for a given model \(m\) and survey question \(q\) we define the model's _survey response_ as a categorical random variable \(R_{q}^{m}\) which can take on \(k_{q}\) values corresponding to the number of answer choices to question \(q\). The respective answer distributions are then contrasted with those of human populations align various dimensions. The overall setup is illustrated in Figure 1.

Prompting.We determine the event probabilities of \(R_{q}^{m}\) by prompting model \(m\) as follows:

1. We construct an input prompt of the form "Question: <question>  A. <choice 1 > B. <choice 2> ... <choice \(k_{q}\)>  Answer:".
2. We query language models with the input prompt and obtain their output distribution over next-token probabilities. We select the \(k_{q}\) output probabilities corresponding to each answer choice (e.g., the tokens "A", "B", etc.), and we renormalize to obtain the probability distribution over survey answers. 2. 
The chosen style of prompt is standard for question answering tasks (Hendrycks et al., 2021), used in OpinionQA (Santurkar et al., 2023), and follows the best practices for social science research recommended by Ziems et al. (2024). For completeness we perform several prompt ablations, including the prompt variations used by Argyle et al. (2023); Santurkar et al. (2023) and Durmus et al. (2023). We find our take-aways to be robust to such changes, see Appendix D. However, note that our goal is not to engineer better prompts, but to critically examine popular scientific practices.

Survey questions.We use a representative subset of 25 multiple-choice questions from the 2019 ACS questionnaire. We denote the set of questions by \(Q\). The questions cover basic demographic information, education attainment, healthcare coverage, disability status, family status, veteran status, employment status, and income. We generally consider the questions and answers as they appear in the ACS questionnaire. Figure 1 depicts an example question. We refer to Appendix A.1 for our list of questions and the exact framing we used for each question.

Models surveyed.We survey 43 language models of size varying from 110M to 175B parameters: the base models GPT-2 (Radford et al., 2019), GPT-Neo (Black et al., 2021), Pythia (Biderman et al., 2023), MPT (MosaicML, 2023), Llama 2 (Touvron et al., 2023),Llama 3 (Dubey et al., 2024) and GPT-3 (Brown et al., 2020); as well as the instruct variants of MPT 7B and GPT NeoX 20B, the Dolly fine-tune of Pythia 12B (Databricks, 2023), Llama 2 Chat, Llama 3 Instruct, the text-davinci variants of GPT-3 (Ouyang et al., 2022), and GPT-4 (OpenAI, 2023).

Reference data & evaluation.We use the responses collected by the U.S. Census Bureau when surveying the U.S. population as our reference data. In particular, we use the 2019 ACS public use microdata sample3 (henceforth census data). The data contains the anonymized responses of around 3.2 million individuals in the United States. For each survey question \(q Q\), we denote the census' population-level response as a categorical random variable \(C_{q}\) whose event probabilities are the relative frequency of each answer choice among survey respondents. We use \(U_{q}\) to denote the uniform distribution over answers. Given these two reference points, we evaluate language models' responses \(R_{q}^{m}\) along two dimensions:

* We use _entropy_ to measure the degree of variation in models' responses. We denote the entropy of a random variable \(R\) as \(H(R)\). To meaningfully compare the entropy of responses across questions with varying number of choices \(k_{q}\), we report normalized entropy, that is, the entropy relative to the uniform distribution. \(H(R_{q}^{m})=1\) implies that model \(m\)'s survey response to question \(q\) is uniformly distributed (i.e., \(H(U_{q})=1\)).
* We use the _Kullback-Leibler (KL) divergence_ to measure the "similarity" between two distributions over answers. We write \((R_{q}^{m} C_{q})\) for the KL divergence between the response distribution \(R_{q}^{m}\) of model \(m\) to question \(q\) and the corresponding aggregate response distribution \(C_{q}\) observed in the census data. The larger the KL distance between two distributions, the more dissimilar the two distributions are.

Note that the KL divergence between any distribution and the uniform distribution corresponds to the entropy difference. For normalized entropy this yields \((C_{q} U_{q})=k_{q}(1-H(C_{q}))\).

Randomized choice ordering.For several investigations we survey models under randomized choice ordering. This means, for a given question \(q\), we prompt models with different permutations of the answer choice ordering, i.e., the assignment of answers (e.g., "male", "female") to choice labels ("A", "B", etc), while the choice labels are kept in alphabetic order. We evaluate models' survey responses under all possible choice orderings and we use \(^{m}_{q}\) to denote the expected distribution over answers and \(^{m}_{q}\) to denote the expected distribution over selected choice labels. For questions with more than 6 answers we evaluate a maximum of 5000 permutations. For OpenAI's models we evaluate up to 50 permutations due to the costs of querying the OpenAI API. This distinction serves to decouple a model's tendency towards picking a particular answer from its tendency towards picking a particular choice label. In the following we refer to the expected survey response \(^{m}_{q}\) under uniformly distributed choice ordering as the _adjusted_ survey response. We will come back to this in Section 4.

## 3 Systematic biases in models' survey responses

We start by surveying the base pre-trained models. We present survey questions independently of one another, showing the answer choices in the same order as the ACS.

For a first investigation, we consider the normalized entropy of models' responses to the "SEX", "HICOV", and "FER" questions. The SEX question inquiries about the person's sex, encoded as male female, the HICOV question inquiries whether the person is currently covered by any health insurance plan, and the FER question inquires whether the person has given birth in the past 12 months. When surveying the U.S. population, these three questions elicit responses with very different entropy; responses to the SEX question are almost uniformly distributed, whereas most people answer "No" to the FER question. In contrast, as shown in Figure 2(a), the entropy of models' responses to these three questions are surprisingly similar. In particular, we find that the entropy of models' responses tends to increase log-linearly with model size, independent of the question asked. This trend is consistent across all ACS survey questions, see Figure 8 in Appendix B.1.

For a broader picture, we illustrate models' response entropy across all survey questions in Figure 2(b). The blue dots represent models' responses to individual questions, and the green dots represent the entropy of the responses of the U.S. census. We order models by size. We observe that the entropy of responses of the U.S. census greatly varies across questions. In contrast, for any given model, the entropy of its responses varies substantially less so.

Figure 2: Entropy of model responses across the ACS questions for naive prompting. Entropy of modelsâ€™ responses (\(\)) tends to increase log-linearly with model size, irrespective of the underlying response entropy observed in the U.S. census (\(-\)).

Overall, we find that models' response distributions seem to be widely independent of the survey question asked, and variations across models are much larger than variations across questions. This lead us to suspect that observed differences across models might arise mostly due to systematic biases.

### Testing for systematic biases: A-bias

It is well-known that language models' most likely answer to multiple-choice questions can change depending on seemingly minor factors such as the ordering of few-shot examples (Zhao et al., 2021; Lu et al., 2022) or the ordering of answer choices (Robinson and Wingate, 2023). We are interested in the extent to which changes in choice ordering affect a model's output _distribution over answers_.

We start by measuring _A-bias_: the tendency of a model towards picking the answer choice labeled "A". In particular, we seek to study the extent to which the strength of this bias explains the differences in responses observed across models. For an unbiased model that outputs the same answer distribution irrespective of choice ordering, the expected choice distribution \(_{q}^{m}\) under randomized choice ordering would match precisely the uniform distribution (e.g., P("A") = P("B") = 0.5). We define a model's A-bias as its absolute deviation from this unbiased baseline:

\[_{q}^{m}:=(_{q}^{m}=``A")-1/k_{q}\] (1)

We measure A-bias for each question \(q\) and model \(m\). Results are illustrated in Figure 3. We again sort models by their size. We observe all models exhibit substantial A-bias. However, models in the order of a few billion parameters or fewer consistently exhibit particularly strong A-bias, and tend towards mono answers. We additionally observe that the strength of A-bias in instruction or RLHF tuned models is similar to that of base models, see Appendix B.2. A plausible explanation for small models exhibiting strong A-bias is that the ability to answer MMLU-style multiple-choice questions only emerges for models of sufficient scale (Dominguez-Olmedo et al., 2024).

We investigate other types of labeling and position bias (e.g., last-choice bias) in Appendix C. Overall, we find a strong tendency of LLMs to pick up on spurious signals in the way that answers are ordered and labeled, rather than their semantic meaning. Notably, in contrast to the primacy bias observed in humans (Groves et al., 2009), we find that models exhibit substantial A-bias even when randomizing the position of the "A" choice. Our findings are consistent with the concurrent work of Tjuatja et al. (2023), which similarly finds that models' response biases to multiple-choice survey questions are generally not human-like. The orthogonal work of Wang et al. (2024) additionally shows that models' responses to multiple-choice survey questions may not consistently reflect their free-form outputs.

In summary, we find that systematic biases confound models' answer distributions. This makes it challenging to draw robust conclusions about inherent properties of LLMs, such as the opinions or populations they best represent. For example, simply reversing the order of answers to the "SEX" question could lead to GPT-2 seemingly representing a population where females are significantly over-represented, whereas a reverse conclusion would be drawn when using the standard answer order. While much research went into designing the ACS to elicit faithful answers and eliminate systematic biases when surveying human populations, simply using the same question framing does not protect against the systematic response biases that language models exhibit.

Figure 3: A-bias of in model responses across ACS questions. Each dot corresponds to one of the 25 questions. Models are ordered by size. As a reference, the extreme points illustrate A-bias for a model that always answers â€™Aâ€™ and a model that never answers â€™Aâ€™. All models suffer from substantial A-bias.

## 4 Inspecting adjusted responses

To eliminate confounding due to labeling and ordering biases, we survey models under randomized choice ordering, borrowing an established methodology to adjust for ordering biases of all kinds in survey research . Also a recent work in LLM research adopts this methodology . In the following, we refer to the expected response after answer choice randomization as the _adjusted_ response.

In Figure 4 we plot the normalized entropy of models' adjusted responses for the ACS questions considered. First focusing on base models, and comparing the results to Figure 2(b) we find that after adjustment, 1) the variations in responses' entropy across survey questions are very small, 2) we no longer observe the trend of the entropy of model responses increasing log-linearly with model size. In fact, models' survey responses have a normalized entropy of approximately \(1\) irrespective of model size or survey question asked. This validates our initial hypothesis that, without adjustment, variations in responses across base models arise predominantly due to systematic biases such as A-bias, rather than the content of the survey questions asked.

### Effect of instruction tuning

We now evaluate language models that have been fine-tuned with instructions and/or human preferences, henceforth "instruction-tuned models". In the right plot of Figure 4 we show the normalized entropy of instruction-tuned models' ACS survey responses after adjustment. We observe that instruction tuned-models all exhibit substantially higher variations in entropy across questions compared to base models. But in general, the entropy of their responses remains higher than the entropy of the census responses. Interestingly, as we will see, although deviating more from uniform, model responses do not tend to be closer to the U.S. census responses.

Figure 4: Entropy of model responses after adjustment. _(top)_ Illustration of how adjustment is performed. We average modelsâ€™ responses over all possible answer orderings. _(bottom)_ Entropy of modelsâ€™ responses after adjustment. Entropy of base modelsâ€™ responses is close to 1 (i.e., uniform). Instruction tuned-models exhibit substantially higher variations in entropy across questions.

### Comparing model responses to the U.S. census

We now investigate the similarity of language models' adjusted responses to the census data. To do so, we consider the overall U.S. census population, as well as 50 census subgroups corresponding to every state in the United States. This leads to different human reference populations.

Inspired by the alignment measures proposed by Santurkar et al. (2023) and Durmus et al. (2023), we investigate the similarity of model responses to the census data by evaluating the average divergence across questions between model responses and the census statistics.4 As we focus on categorical questions, we evaluate average KL divergence between each language model \(m\) and each reference population \(\), as follows:

\[(m,)=_{q Q}(_{q}^ {m}||_{q}).\]

Results are depicted in Figure 5. For each model we plot the divergence to the census in black, the divergence to the different subgroups in blue, and the divergence to a uniform baseline with balanced responses in red. We observe that models are strikingly more similar to the uniform baseline than to any of the populations considered. For base models, this result is unsurprising, since in the previous section we established that base models' responses are essentially uniform after adjustment.

Looking at Figure 5 we find no consistent trend that instruction-tuning would move responses closer to the census, despite the increased deviation from uniform and the larger variations in entropy (recall Figure 4). Only for larger models the divergence seems to clearly decrease with instruction-tuning. However, all models' responses still remain significantly closer to the uniform baseline than to the U.S. census. For instance, for the GPT-4 model whose answers exhibit the highest similarity to the human reference populations, only 6 out of 25 questions (24%) are closer to the U.S. census than to the uniform baseline. Given these results, drawing conclusions about the relative alignment of models with subgroups is prone to resulting in brittle conclusions.

## 5 Implications for survey-based alignment metrics

Our findings add important context to previous works studying the alignment of language models with different human subpopulations. In particular, we highlighted the tendency of models towards balanced answers. Due to varying entropy in the responses of subgroups this leads to a strong correlation between model alignment and the reference population's entropy. The linear trend in Figure 6 visualizes this. For any given model, it consistently appears to be more "aligned" with the subpopulations exhibiting high entropy in their answers. Interestingly, we find that this trend also holds pre-adjustment, suggesting that the transformation of the response through randomized choice ordering is orthogonal to differentiating aspects of any specific population. In contrast, when comparing different models in Figure 6, we can see how adjustment has a large influence on their relative order. Differences across models that we see under naive prompting disappear after adjustment, which means that they should largely be attributed to systematic biases, rather than inherent properties of the model.

Figure 5: Divergence between adjusted model responses and different baselines: the overall U.S. census (\(\)), individual U.S. states (\(\)), and a uniform baseline (\(\)). Smaller means more similar. Model responses are by far more similar to the uniform baseline than to any human reference population.

Taken together our findings imply that the survey-derived alignment measure is more informative of differences in the reference populations rather than the language models is aims to evaluate. Model particularities, such as the pre-training data used, instruction tuning or the use of reinforcement learning with human feedback, seem to have little impact on which population is best represented.

### Beyond the ACS

To inspect whether this trend changes with the content of the questions asked, we reproduce our experiments with additional surveys. We use the American Trends Panel (ATP) opinion surveys considered by Santurkar et al. (2023), and the Pew Research's Global Attitudes Surveys (GAS) and World Values Surveys (WVS) considered by Durmus et al. (2023). These surveys encompass around 1500 questions and 60 U.S. demographic subgroups, and around 2300 questions and 60 national populations, respectively. We adopt the alignment metrics considered by the aforementioned works. We find that our insights gained from the ACS also hold for the ATP and GAS/WVS surveys. In particular, we similarly find a linear trend between the alignment metrics and subgroups' entropy of responses, in particular after adjustment, see Figure 7. Note here that alignment and divergence are negatively correlated by definition. Interestingly, this observation explains some of the findings in prior works. For example, Santurkar et al. (2023) find that "all the base models share striking similarities-e.g., being most aligned with lower income, moderate, and Protestant or Roman Catholic groups" and "our analysis [...] surfaces groups whose opinions are poorly reflected by current LLMs (e.g., 65+ and widowed individuals)". For the ATP surveys considered, low income, moderate, and Protestant/Catholic are precisely the demographic subgroups with responses closest to uniformly random among the income, political ideology, and religion demographic subgroups; whereas age 65+ and widowed are the demographic subgroups with responses furthest from uniform among the age and marital status demographic subgroups. Further, Santurkar et al. (2023) observe that RLHF can result in a "substantial shift [...] towards more liberal, educated, and wealthy [demographic

Figure 6: Alignment of models with different census subgroups. All models tend to exhibit similar relative alignment, and the divergence metric decreases with the entropy of the subgroupsâ€™ responses.

groups]". Our results suggest that this could be an artifact of systematic biases. For the ATP surveys, we observe three outliers for which its alignment _before adjustment_ is not correlated with the entropy of subgroup's responses: Llama 2 70B Chat and the two Llama 3 Instruct models. These are the models with largest pre-training compute considered. However, after adjustment, the alignment trends of Llama 2 70B Chat and the Llama 3 Instruct models are remarkably similar to that of their corresponding base models and all other LLMs.

## 6 Conclusion

We used a popular methodology to elicit LLMs' answer distributions to survey questions and closely examined the responses on the basis of the prime US demographic survey. We found that model responses are dominated by systematic ordering biases and do not exhibit the natural variations in entropy found in the human reference data collected by the US census. Even after adjusting for ordering biases, LLMs' responses still do not resemble those of human populations. Instead, they exhibit consistently high entropy, independent of the question asked. This holds true irrespective of model size or fine-tuning with human preferences.

These findings have important implications for insights gained from survey-derived alignment metrics. In particular, it explains why models of varying size all exhibit the same trend: they are most aligned with subgroups who happen to have balanced answers for the survey questions under consideration. For all models and surveys considered, alignment appears to be a proxy for the entropy of subgroups, rather than an inherent property of the model, or its training data.

We want to reiterate that our focus lies on questioning a popular methodology of eliciting survey responses from large language models using multiple choice prompting. At the example of this methodology our results highlight an important pitfall and suggest caution to expect robust insights when comparing such responses against those of human populations. The robustness and quality of an established survey does not seamlessly translate from the results obtained by surveying human populations to the logits output by LLMs. More research is urgently needed to design methodologies for getting insights into the inherent biases of LLMs and the population they might represent. Here public surveys and their accompanying data offer exciting potential and the could play an important role as a benchmarking tool for systematic evaluations of LLMs, see  as an example. Although the use of survey data for LLM research has recently gained popularity, it still remains a widely under explored data source.