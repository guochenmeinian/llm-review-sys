# A Refutation of Shapley Values for Explainability

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Recent work demonstrated the existence of Boolean functions for which Shapley values provide misleading information about the relative importance of features in rule-based explanations. Such misleading information was broadly categorized into a number of possible issues. Each of those issues relates with features being relevant or irrelevant for a prediction, and all are significant regarding the inadequacy of Shapley values for rule-based explainability. This earlier work devised a brute-force approach to identify Boolean functions, defined on small numbers of features, and also associated instances, which displayed such inadequacy-revealing issues, and so served as evidence to the inadequacy of Shapley values for rule-based explainability. However, an outstanding question is how frequently such inadequacy-revealing issues can occur for Boolean functions with arbitrary large numbers of features. It is plain that a brute-force approach would be unlikely to provide insights on how to tackle this question. This paper answers the above question by proving that, for any number of features, there exist Boolean functions that exhibit one or more inadequacy-revealing issues, thereby contributing decisive arguments against the use of Shapley values as the theoretical underpinning of feature-attribution methods in explainability.

## 1 Introduction

Feature attribution is one of the most widely used approaches in machine learning (ML) explainability, begin implemented with a variety of different methods . Moreover, the use of Shapley values  for feature attribution ranks among the most popular solutions , offering a widely accepted theoretical justification on how to assign importance to features in machine learning (ML) model predictions. Despite the success of using Shapley values for explainability, it is also the case that their exact computation is in general intractable , with tractability results for some families of boolean circuits . As a result, a detailed assessment of the rigor of feature attribution methods based on Shapley values, when compared with exactly computed Shapley values has not been investigated. Furthermore, the definition Shapley values (as well as its use in explainability) is purely axiomatic, i.e. there exists _no_ formal proof that Shapley values capture any specific properties related with explainability (even if defining such properties might prove elusive).

Feature selection represents a different alternative to feature attribution. The goal of feature selection is to select a set of features as representing the reason for a prediction, i.e. if the selected features take their assigned values, then the prediction cannot be changed. There are rigorous and non-rigorous approaches for selecting the features that explain a prediction. This paper considers rigorous (or model-precise) approaches for selecting such features. Furthermore, it should be plain that feature selection must aim for irredundancy, since otherwise it would suffice to report all features as the explanation. Given the universe of possible irreducible sets of feature selections that explain a prediction, the features that do not occur in _any_ such set are deemed _irrelevant_ for a prediction; otherwise features that occur in one or more feature selections are deemed _relevant_.

Since both feature attribution and feature selection measure contributions of features to explanations, one would expect that the two approaches were related. However, this is not the case. Recentwork  observed that feature attribution based on Shapley values could produce _misleading information_ about features, in that irrelevant features (for feature selection) could be deemed more important (in terms of feature attribution) than relevant features (also for feature selection). Clearly, misleading information about the relative importance of features can easily induce human decision makers in error, by suggesting the _wrong_ features as those to analyze in greater detail. Furthermore, situations where human decision makers can be misled are inadmissible in high-risk or safety-critical uses of ML. Furthermore, a number of possible misleading issues of Shapley values for explainability were identified , and empirically demonstrated to occur for some boolean functions. The existence in practice of those misleading issues with Shapley values for explainability is evidently problematic for their use as the theoretical underpinning of feature attribution methods.

However, earlier work  used a brute-force method to identify boolean functions, defined on a very small number of variables, where the misleading issues could be observed. A limitation of this earlier work  is that it offered no insights on how general the issues with Shapley values for explainability are. For example, it could be the case that the identified misleading issues might only occur for functions defined on a very small number of variables, or in a negligible number of functions, among the universe of functions defined on a given number of variables. If that were to be the case, then the issues with Shapley values for explainability might not be that problematic.

This paper proves that the identified misleading issues with Shapley values for explainability are much more general that what was reported in earlier work . Concretely, the paper proves that, for any number of features larger than a small \(k\) (either 2 or 3), one can easily construct functions which exhibit the identified misleading issues. The main implication of our results is clear: _the use of Shapley values for explainability can, for an arbitrary large number of boolean (classification) functions, produce misleading information about the relative importance of features_.

**Organization.** The paper is organized as follows. Section 2 introduces the notation and definitions used throughout the paper. Section 3 revisits and extends the issues with Shapley values for explainability reported in earlier work , and illustrates the existence of those issues in a number of motivating example boolean functions. Section 4 presents the paper's main results, proving that all the issues with Shapley values for explainability reported in earlier work  occur for boolean functions with arbitrarily larger number of variables. (Due to lack of space, the detailed proofs are all included in Appendix A, and the paper includes only brief insights into those proofs.) Also, the proposed constructions offer ample confidence that the number of functions displaying one or more of the issues is significant. Section 5 concludes the paper.

## 2 Preliminaries

**Boolean functions.** Let \(=\{0,1\}\). The results in the paper consider boolean functions, defined on \(m\) boolean variables, i.e. \(:^{m}\). (The fact that we consider only boolean functions does not restrict in the significance of the results.)

In the rest of the paper, we will use the boolean functions shown in Figure 1, which are represented by truth tables. The highlighted rows will serve as concrete examples throughout.

**Classification in ML.** A classification problem is defined on a set of features \(=\{1,,m\}\), each with domain \(_{i}\), and a set of classes \(=\{c_{1},c_{2},,c_{K}\}\). (As noted above, we will assume \(_{i}=\) for \(1 i m\), but domains could be categorical or ordinal. Also, we will assume \(=\).) Feature space \(\) is defined as the cartesian product of the domains of the features, in order: \(=_{1}_{m}\), which will be \(^{m}\) throughout the paper. A classification function is a non-constant map from feature space into the set of classes, \(:\). (Clearly, a classifier would be useless if the classification function were constant.) Throughout the paper, we will not distinguish between classifiers and boolean functions. An instance is a pair \((,c)\) representing a point \(=(v_{1},,v_{m})\) in feature space, and the classifier's prediction, i.e. \(()=c\). Moreover, we let \(=(x_{1},,x_{m})\) denote an arbitrary point in the feature space. Abusing notation, we will also use \(_{a..b}\) to denote \(x_{a},,x_{b}\), and \(_{a..b}\) to denote \(v_{a},,v_{b}\). Finally, a classifier \(\) is a tuple \((,,,)\). In addition, an explanation problem \(\) is a tuple \((,(,c))\), where \(=(,,,)\) is a classifier.

**Shapley values for explainability.** Shapley values were first introduced by L. Shapley  in the context of game theory. Shapley values have been extensively used for explaining the predictions of ML models, e.g. [64; 65; 20; 48; 15; 52; 62; 69], among a vast number of recent examples. The complexity of computing Shapley values (as proposed in SHAP ) has been studied in recent years . This section provides a brief overview of Shapley values. Throughout the section, we adapt the notation used in recent work , which builds on the work of .

Let \(:2^{} 2^{}\) be defined by1,

\[(;)=\{\,|\, _{i}\,x_{i}=v_{i}\}\] (1)

i.e. for a given set \(\) of features, and parameterized by the point \(\) in feature space, \((;)\) denotes all the points in feature space that have in common with \(\) the values of the features specified by \(\).

Also, let \(:2^{}\) be defined by,

\[(;,)= |}}_{(;)}( )\] (2)

For the purposes of this paper, we consider solely a uniform input distribution, and so the dependency on the input distribution is not accounted for. A more general formulation is considered in related work . However, assuming a uniform distribution suffices for the purposes of this paper. As a result, given a set \(\) of features, \((;,)\) represents the average value of the classifier over the points of feature space represented by \((;)\).

Finally, let \(:\) be defined by2,

\[(i;,)=_{( \{i\})}|!(||-||-1)!}{| |!}((\{i\};,)-( ;,))\] (3)

Given an instance \((,c)\), the Shapley value assigned to each feature measures the _contribution_ of that feature with respect to the prediction. A positive/negative value indicates that the feature can contribute to changing the prediction, whereas a value of 0 indicates no contribution.

**Example 1.** We consider the example boolean functions of Figure 1. If the functions are represented by a truth table, then the Shapley values can be computed in polynomial time on the size of the truth table, since the number of subsets considered in (3) is also polynomial on the size of the truth table . (Observe that for each subset used in (3), we can use the truth table for computing the average values in (2).) For example, for \(_{I1}\) and for the point in feature space \((0,0,1)\), one can compute the following Shapley values: \((1)=-0.417\), \((2)=-0.042\), and \((3)=0.083\).

**Logic-based explanations.** There has been recent work on developing formal definitions of explanations. One type of explanations are _abductive explanations_ (AXp), which corresponds to a PI-explanations  in the case of boolean classifiers. AXp's represent prime implicants of the discrete-valued classifier function (which computes the predicted class). AXp's can also be viewed as an instantiation of logic-based abduction . Throughout this paper we will opt to use the acronym AXp to refer to abductive explanations.

Let us consider a given classifier, computing a classification function \(\) on feature space \(\), a point \(\), with prediction \(c=()\), and let \(\) denote a subset of the set of features \(\), \(\). \(\) is a weak AXp for the instance \((,c)\) if,

\[(;,):=( ).[_{i}(x_{i}=v_{i}) ](()=c)\] (4)

where \(c=()\). Thus, given an instance \((,c)\), a (weak) AXp is a subset of features which, if fixed to the values dictated by \(\), then the prediction is guaranteed to be \(c\), independently of the values assigned to the other features.

Moreover, \(\) is an AXp if, besides being a weak AXp, it is also subset-minimal, i.e.

\[(;,):=(;,)(^{} ).(^{};, )\] (5)

Observe that an AXp can be viewed as a possible irreducible answer to a "**Why?**" question, i.e. why is the classifier's prediction \(c\)? It should be plain in this work, but also in earlier work, that the representation of AXp's using subsets of features aims at simplicity. The sufficient condition for the prediction is evidently the conjunction of literals associated with the features contained in the AXp.

**Example 2.** Similar to the computation of Shapley values, given a truth table representation of a function, and for a given instance, there is a polynomial-time algorithm for computing the AXp's . For example, for function \(_{I4}\) (see Figure 0(c)), and for the instance \(((0,0,1,1),0)\), it can be observed that, if features 3 and 4 are allowed to take other values, the prediction remains at 0. Hence, \(\{1,2\}\) is an WAXp, which is easy to conclude that it is also an AXp. When interpreted as a rule, the AXp would yield the rule:

\[ x_{1} x_{2}()=0\]

In a similar way, if features 1 and 3 are allowed to take other values, the prediction remains at 0. Hence, \(\{2,4\}\) is another WAXp (which can easily be shown to be an AXp). Furthermore, considering all other possible subsets of fixed features, allows us to conclude that there are no more AXp's.

Similarly to the case of AXp's, one can define (weak) contrastive explanations (CXp's) . \(\) is a weak CXp for the instance \((,c)\) if,

\[(;,):=( ).[_{i}(x_{i}=v_{i}) ](() c)\] (6)

(As before, for simplicity we keep the parameterization of WCXp on \(\), \(\) and \(c\) implicit.) Thus, given an instance \((,c)\), a (weak) CXp is a subset of features which, if allowed to take any value from their domain, then there is an assignment to the features that changes the prediction to a class other than \(c\), this while the features not in the explanation are kept to their values.

Furthermore, a set \(\) is a CXp if, besides being a weak CXp, it is also subset-minimal, i.e.

\[(;,):=(;,)(^{} ).(^{};, )\] (7)

A CXp can be viewed as a possible irreducible answer to a "**Why Not?**" question, i.e. why isn't the classifier's prediction a class other than \(c\)?

**Example 3.** For the example function \(_{I4}\) (see Figure 0(c)), and instance \(((0,0,1,1),0)\), if we fix features 1, 3 and 4, respectively to 0, 1 1, then by allowing feature 2 to change value, we see that the prediction changes, e.g. by considering the point \((0,1,1,1)\) with prediction 1. Thus, \(\{2\}\) is a CXp. In a similar way, by fixing the features 2 and 3, respectively to 0 and 1, then by allowing features 1 and 4 to change value, we conclude that the prediction changes. Hence, \(\{1,4\}\) is also a CXp.

The sets of AXp's and CXp's are defined as follows:

\[()&=\{ \,|\,(;,)\}\\ ()&=\{ \,|\,(;,)\}\] (8)(The parameterization on \(\) and \(\) is unnecessary, since the explanation problem \(\) already accounts for those.) Moreover, let \(F_{}()=_{()}\) and \(F_{}()=_{()} \). \(F_{}()\) aggregates the features occurring in any abductive explanation, whereas \(F_{}()\) aggregates the features occurring in any contrastive explanation. In addition, minimal hitting set duality between AXp's and CXp's  yields the following result3.

**Proposition 1**.: \(F_{}()=F_{}()\)_._

**Feature (ir)relevancy in explainability.** Given the definitions above, we have the following characterization of features :

1. A feature \(i\) is _necessary_ if \((()).i\).
2. A feature \(i\) is _relevant_ if \((()).i\).
3. A feature is _irrelevant_ if it is not relevant, i.e. \((()).i\).

By Proposition 1, the definitions of necessary and relevant feature could instead use \(()\). Throughout the paper, we will use the predicate \((i)\) which holds true if feature \(i\) is irrelevant, and predicate \((i)\) which holds true if feature \(i\) is relevant. Furthermore, it should be noted that feature irrelevancy is a fairly demanding condition in that, a feature \(i\) is irrelevant if it is not included in _any_ subset-minimal set of features that is sufficient for the prediction.

**Example 4**.: For the example function \(_{I4}\) (see Figure 0(c)), and from Example 2, and instance \(((0,0,1,1),0)\), it becomes clear that feature 3 is irrelevant. Similarly, it is easy to conclude that features 1, 2 and 4 are relevant.

**How irrelevant are irrelevant features?** The fact that a feature is declared irrelevant for an explanation problem \(=(,(,c))\) is significant. Given the minimal hitting set duality between abductive and contrastive explanations, then an irrelevant features does not occur neither in any abductive explanation, nor in any contrastive explanation. Furthermore, from the definition of AXp, each abductive explanation for \(\) can be represented as a logic rule. Let \(\) denote the set of _all irreducible_ logic rules which can be used to predict \(c\), given the literals dictated by \(\). Then, an irrelevant feature does not occur in _any_ of those rules. Example 4 illustrates the irrelevancy of feature 3, in that feature 3 would not occur in _any_ irreducible rule for \(_{I4}\) when predicting \(0\) using literals consistent with \((0,0,1,1)\).

To further strengthen the above discussion, let us consider a (feature selection based) explanation \(\) such that \(()\) holds (i.e. (4) is true, and so \(\) is sufficient for the prediction). Moreover, let \(i\) be an irrelevant feature, such that \(i\). Then, by definition of irrelevant feature, there _must_ exist some \((\{i\})\), such that \(()\) also holds (i.e. \(\) is _also_ sufficient for the prediction). It is simple to understand why such set \(\) must exist. By definition of irrelevant feature, and because \(i\), then \(\) is not an AXp. However, there must exist an AXp \(\) which, by definition of irrelevant feature, must not include \(i\). Furthermore, and invoking Occam's razor4, there is no reason to select \(\) over \(\), and this remark applies to _any_ set of features containing some irrelevant feature.

**Related work.** Shapley values for explainability is one of the hallmarks of feature attribution methods in XAI . Motivated by the success of Shapley values for explainability, there exists a burgeoning body of work on using Shapley values for explainability (e.g. ). Recent work studied the complexity of exactly computing Shapley values in the context of explainability . Finally, there have been proposals for the exact computation of Shapley values in the case of circuit-based classifiers . Although there exist some differences in the proposals for the use of Shapley values for explainability, the basic formulation is the same and can be expressed as in Section 2.

A number of authors have reported pitfalls with the use of SHAP and Shapley values as a measure of feature importance . However, these earlier works do not identify fundamental flaws with the use of Shapley values in explainability. Attempts at addressing those pitfalls include proposals to integrate Shapley values with abductive explanations, as reported in recent work .

Formal explainability is a fairly recent topic of research. Recent accounts include .

Recent work  argued for the inadequacy of Shapley values for explainability, by demonstrating experimentally that the information provided by Shapley values can be misleading for a human decision-maker. The approach proposed in  is based on exhaustive function enumeration, and so does not scale beyond a few features. However, this paper uses the truth-table algorithms outlined in , in all the examples, both for computing Shapley values, for computing explanations, and for deciding feature relevancy.

## 3 Relating Shapley Values with Feature Relevancy

Recent work  showed the existence of boolean functions (with up to four variables) that revealed a number of issues with Shapley values for explainability. All those issues are related with taking feature relevancy into consideration. (In , these functions were searched by exhaustive enumeration of all the boolean functions up to a threshold on the number of variables.)

**Issues with Shapley values for explainability.** In this paper, we consider the following main issues of Shapley values for explainability:

1. [label=**0.**, ref=**0.0**]
2. For a boolean classifier, with an instance \((,c)\), and feature \(i\) such that, \[(i)((i) 0)\] Thus, an 11 issue is such that the feature is irrelevant, but its Shapley value is non-zero.
3. For a boolean classifier, with an instance \((,c)\) and features \(i_{1}\) and \(i_{2}\) such that, \[(i_{1})(i_{2})(|(i_ {1})|>|(i_{2})|)\] Thus, an 12 issue is such that there is at least one irrelevant feature exhibiting a Shapley value larger (in absolute value) than the Shapley of a relevant feature.
4. For a boolean classifier, with instance \((,c)\), and feature \(i\) such that, \[(i)((i)=0)\] Thus, an 13 issue is such that the feature is relevant, but its Shapley value is zero.
5. For a boolean classifier, with instance \((,c)\), and features \(i_{1}\) and \(i_{2}\) such that, \[[(i_{1})((i_{1}) 0)][(i_{2})((i_{2})=0)]\] Thus, an 14 issue is such that there is at least one irrelevant feature with a non-zero Shapley value and a relevant feature with a Shapley value of 0.
6. For a boolean classifier, with instance \((,c)\) and feature \(i\) such that, \[[(i)_{1 j m,j i}(|(j)|<|(i)|)]\] Thus, an 15 issue is such that there is one irrelevant feature exhibiting the highest Shapley value (in absolute value). (15 can be viewed as a special case of the other issues, and so it is not analyzed separately in earlier work .)

The issues above are all related with Shapley values for explainability giving _misleading information_ to a human decision maker, by assigning some importance to irrelevant features, by not assigning enough importance to relevant features, by assigning more importance to irrelevant features than to relevant features and, finally, by assigning the most importance to irrelevant features.

In the rest of the paper we consider mostly 11, 13, 14 and 15, given that 15 implies 12.

**Proposition 2**.: If a classifier and instance exhibits issue 15, then they also exhibit issue 12.

**Examples.** This section studies the example functions of Figure 1, which were derived from the main results of this paper (see Section 4). These example functions will then be used to motivate the rationale for how those results are proved. In all cases, the reported Shapley values are computed using the truth-table algorithm outlined in earlier work . Similarly, the relevancy/irrelevancy claims of features use the truth-table algorithms outlined in earlier work .

**Example 5**.: Figure 0(a) illustrates a boolean function that exhibits issue 11. By inspection, we can conclude that the function shown corresponds to \(_{I1}(x_{1},x_{2},x_{3})=(x_{1} x_{2} x_{3})(x_{1}  x_{3})\). Moreover, for the instance \(((0,0,1),0)\), Table 1 confirms that an issue 11 is identified.

**Example 6**.: Figure 0(b) illustrates a boolean function that exhibits issue 13. By inspection, we can conclude that the function shown corresponds to \(_{I3}(x_{1},x_{2},x_{3})=(x_{1} x_{3})(x_{2} x_{3})\). Moreover, for the instance \(((1,1,1),1)\), Table 1 confirms that an issue 13 is identified.

**Example 7**.: Figure (c)c illustrates a boolean function that exhibits issue 14. By inspection, we can conclude that the function shown corresponds to \(_{I4}(x_{1},x_{2},x_{3},x_{4})=(x_{1} x_{2} x_{3})(x_{ 1} x_{3} x_{4})(x_{2} x_{3} x_{4})\). Moreover, for the instance \(((0,0,1,1),0)\), Table 1 confirms that an issue 14 is identified.

**Example 8**.: Figure (d)d illustrates a boolean function that exhibits issue 15. By inspection, we can conclude that the function shown corresponds to \(_{I5}(x_{1},x_{2},x_{3},x_{4})=((x_{1} x_{2} x_{3})(x _{1} x_{3} x_{2})(x_{2} x_{3} x_{1})) x _{4}\). Moreover, for the instance \(((1,1,1,1),0)\), Table 1 confirms that an issue 15 is identified.

It should be underscored that Shapley values for explainability are _not_ expected to give misleading information. Indeed, it is widely accepted that Shapley values measure the actual _influence_ of a feature . Concretely,  reads: "_...if a feature has no influence on the prediction it is assigned a contribution of 0._" But  also reads "_According to the 2nd axiom, if two features values have an identical influence on the prediction they are assigned contributions of equal size. The 3rd axiom says that if a feature has no influence on the prediction it is assigned a contribution of 0._" (In this last quote, the axioms refer to the axiomatic characterization of Shapley values.) Furthermore, one might be tempted to look at the value of the prediction and relate that with the computed Shapley value. For example, in the last row of Table 1, the prediction is 0, and the _irrelevant_ feature 4 has a _positive_ Shapley value. As a result, one might be tempted to believe that the irrelevant feature 4 would contribute to _changing_ the value of the prediction. This is of course incorrect, since an irrelevant feature does not occur in _any_ CXp's (besides not occurring in any AXp's) and so it is never necessary to changing the prediction. The key point here is that irrelevant features are _never_ necessary, neither to keep nor to change the prediction.

## 4 Refuting Shapley Values for Explainability

The purpose of this section is to prove that for arbitrary large numbers of variables, there exist boolean functions and instances for which the Shapley values exhibit the issues reported in recent work , and detailed in Section 3. (Instead of detailed proofs, this section describes the key ideas of each proof. The detailed proofs are included in A.)

Throughout this section, let \(m\) be the number of variables of the boolean functions we start from, and let \(n\) denote the number of variables of the functions we will be constructing. In this case, we set \(=\{1,,n\}\). Furthermore, for the sake of simplicity, we opt to introduce the new features as the last features (e.g., feature \(n\)). This choice does not affect the proof's argument in any way.

**Proposition 3**.: For any \(n 3\), there exist boolean functions defined on \(n\) variables, and at least one instance, which exhibit an issue 11, i.e. there exists an irrelevant feature \(i\), such that \((i) 0\).

Proof idea.: The proof proposes to construct boolean functions, with an arbitrary number of variables (no smaller than 3), and the picking of an instance, such that a specific feature is irrelevant for the prediction, but its Shapley value is non-zero. To illustrate the construction, the example function from Figure (a)a is used (see also Example 5).

   Case & Instance & Relevant & Irrelevant & \(\)â€™s & Justification \\   & \(((0,0,1),0)\) &  &  & \((1)=-0.417\) & (3) 0\)} \\  & & & & \((2)=-0.042\) & \\  & & & & \((3)=0.083\) & \\   & \(((1,1,1),1)\) &  &  & \((1)=0.125\) & (2)=0.375\)} \\  & & & & \((3)=0.000\) & \\   & \(((0,0,1,1),0)\) &  &  & \((1)=-0.125\) & (3)(3) 0\)\(\)} \\  & & & & \((2)=-0.333\) & (4)(4)=0\)} \\  & & & & \((3)=0.083\) & \\  & & & & \((4)=0.000\) & \\   & \(((1,1,1),0)\) &  &  & \((1)=-0.12\) & (4)\)} \\  & & & \((2)=-0.12\) & ).|(j)|<(4)|\)} \\  & & & & \((3)=-0.12\) & \\  & & & \((4)=0.17\) & \\   

Table 1: Examples of issues of Shapley values for functions in Figure 1The construction works as follows. We pick two non-constant functions \(_{1}(x_{1},,x_{m})\) and \(_{2}(x_{1},,x_{m})\), defined on \(m\) features, and such that: i) \(_{1}\!\!_{2}\) (which signifies that \(()._{1}()\!\!_{2}( )\)), and ii) \(_{1}_{2}\). Observe that \(_{1}\) can be any boolean function defined on \(m\) variables, as long as \(_{2}\) can also be defined. We then construct a new function by adding a new feature \(n=m+1\), as follows:

\[(x_{1},,x_{m},x_{n})=\{_{1}(x_{1}, ,x_{m})&=0$}\\ _{2}(x_{1},,x_{m})&=1$}.\]

For the resulting function \(\), we pick an instance \((,0)\) such that: i) \(v_{n}=1\) and ii) \(_{1}(_{1 m})=_{2}(_{1 m})=0\). The proof hinges on the fact that feature \(n\) is irrelevant, but \((n) 0\).

For the function 1a, we set \(_{1}(x_{1},x_{2})=x_{1} x_{2}\) and \(_{1}(x_{1},x_{2})=x_{1}\). Thus, as shown in Example 5, \(_{I1}(x_{1},x_{2},x_{3})=(x_{1} x_{2} x_{3})(x_{1}  x_{3})\), which represents the function \((x_{1},x_{2},x_{3})\). It is also clear that \(_{1}\!\!_{2}\). Moreover, and as Example 5 and Table 1 show, it is the case that feature 3 is irrelevant and \((3) 0\). 

**Proposition 4**.: For any odd \(n 3\), there exist boolean functions defined on \(n\) variables, and at least one instance, which exhibits an \(13\) issue, i.e. for which there exists a relevant feature \(i\), such that \((i)=0\).

Proof idea.: The proof proposes to construct boolean functions, with an arbitrary number of variables (no smaller than 3), and the picking of an instance, such that a specific feature is relevant for the prediction, but its Shapley value is zero. To illustrate the construction, the example function from 1b is used (see also Example 6).

The construction works as follows. We pick two non-constant functions \(_{1}(x_{1},,x_{m})\) and \(_{2}(x_{m+1},,x_{2m})\), each defined on \(m\) features, where \(_{2}\) corresponds to \(_{1}\), but with a change of variables. Observe that \(_{1}\) can be any boolean function. We then construct a new function, defined in terms of \(_{1}\) and \(_{2}\), by adding a new feature \(n=2m+1\), as follows:

\[(x_{1},,x_{m},x_{m+1},,x_{2m},x_{n})=\{[] {ll}_{1}(x_{1},,x_{m})&=0$}\\ _{2}(x_{m+1},,x_{2m})&=1$}.\]

For the resulting function \(\), we pick an instance \((,1)\) such that: i) \(v_{n}=1\), ii) \(v_{i}=v_{m+i}\) for any \(1 i m\), and iii) \(_{1}(_{1 m})=_{2}(_{m+1 2m})=1\). The proof hinges on the fact that feature \(n\) is relevant, but \((n)=0\).

For the function 1b, we set \(_{1}(x_{1})=x_{1}\) and \(_{1}(x_{2})=x_{2}\). Thus, as shown in Example 6, \(_{I3}(x_{1},x_{2},x_{3})=(x_{1} x_{3})( x_{2} x_{ 3})\), which represents the function \((x_{1},x_{2},x_{3})\). Moreover, and as Example 6 and Table 1 show, it is the case that feature 3 is relevant and \((3)=0\). 

**Proposition 5**.: For any even \(n 4\), there exist boolean functions defined on \(n\) variables, and at least one instance, for which there exists an irrelevant feature \(i_{1}\), such that \((i_{1}) 0\), and a relevant feature \(i_{2}\{i_{1}\}\), such that \((i_{2})=0\).

Proof idea.: The proof proposes to construct boolean functions, with an arbitrary number of variables (no smaller than 4), and the picking of an instance, such that two specific features are such that one is relevant but has a Shapley value of 0, and the other one is irrelevant but has a non-zero Shapley values. To illustrate the construction, the example function from 1c is used (see also Example 7).

The construction works as follows. We pick two non-constant functions \(_{1}(x_{1},,x_{m})\) and \(_{2}(x_{m+1},,x_{2m})\), each defined on \(m\) features, where \(_{2}\) corresponds to \(_{1}\), but with a change of variables. Also, observe that \(_{1}\) can be any boolean function. We then construct a new function, defined in terms of \(_{1}\) and \(_{2}\), by adding two new features. We let the new features be \(n-1\) and \(n\), and so \(n=2m+2\). The function is organized as follows:

\[(_{1 m},_{m+1 2m},x_{n-1},x_{n})=\{ _{1}(_{1 m})_{2}(_ {m+1 2m})&=0$}\\ _{1}(_{1 m})&=1 x_{n}=0$}\\ _{2}(_{m+1 2m})&=1 x_{n}=1$} .\]

For this function, we pick an instance \((,0)\) such that: i) \(v_{n-1}=v_{n}=1\), ii) \(v_{i}=v_{m+i}\) for any \(1 i m\), and iii) \(_{1}(_{1 m})=_{2}(_{m+1 2m})=0\). The proof hinges on the fact that feature \(n-1\) is irrelevant, feature \(n\) is relevant, and \((n-1) 0\) and \((n)=0\).

For the function 1c, we set \(_{1}(x_{1})=x_{1}\) and \(_{1}(x_{2})=x_{2}\), Thus, as shown in Example 7, \(_{I4}(x_{1},x_{2},x_{3},x_{4})=(x_{1} x_{2} x_{3})(x_ {1} x_{3} x_{4})(x_{2} x_{3} x_{4})\), which represents the function \((x_{1},x_{2},x_{3},x_{4})\). Moreover, and as Example 7 and Table 1 show, it is the case that feature 3 is irrelevant, feature 4 is relevant, and also \((3) 0\) and \((4)=0\).

**Proposition 6**.: For any \(n 4\), there exists boolean functions defined on \(n\) variables, and at least one instance, for which there exists an irrelevant feature \(i=\{1,,n\}\), such that \(|(i)|=\{|(j)| j\}\).

Proof idea.: The proof proposes to construct boolean functions, with an arbitrary number of variables (no smaller than 4), and the picking of an instance, such that one specific feature is irrelevant but it has the Shapley value with the largest absolute values. To illustrate the construction, the example function from Figure1d is used (see also Example8).

The construction works as follows. We pick one non-constant function \(_{1}(x_{1},,x_{m})\), defined on \(m\) features, such that: i) \(_{1}\) predicts a specific point \(_{1..m}\) as 0, moreover, for any point \(_{1..m}\) such that \(d_{H}(_{1..m},_{1..m})=1\), \(_{1}(_{1..m})=1\), where \(d_{H}()\) denotes the Hamming distance. ii) and \(_{1}\) predicts all the other points as 0. For example, let \(_{1}(x_{1},,x_{m})=1\) iff \(_{i=1}^{m} x_{1}=1\). We then construct a new function, defined in terms of \(_{1}\), by adding one new feature. We let the new feature be \(n\), and so \(n=m+1\). The new function is organized as follows:

\[(x_{1},,x_{m},x_{n})=\{0&x_{n}=0\\ _{1}(x_{1},,x_{m})&x_{n}=1.\]

For this function, we pick the instance \((,0)\) such that: i) \(v_{n}=1\), ii) \(_{1..m}\) is the only point within the Hamming ball and iii) \(_{1}(_{1..m})=0\). The proof hinges on the fact that feature \(n\) is irrelevant, but \((1 j m).|(j)|<|(n)|\).

For the function Figure1d, we set \(_{1}(x_{1},x_{2},x_{3})=(x_{1} x_{2} x_{3})(x_{1}  x_{3} x_{2})(x_{2} x_{3} x_{1})\) (i.e. the function takes value 1 when exactly one feature is 0). Thus, as shown in Example7, \(_{15}(x_{1},x_{2},x_{3},x_{4})=((x_{1} x_{2} x_{3})( x_{1} x_{3} x_{2})(x_{2} x_{3} x_{1})) x _{4}\), which represents the function \((x_{1},x_{2},x_{3},x_{4})\). Moreover, and as Example8 and Table1 show, it is the case that feature 4 is irrelevant and \((1 j 3).|(j)|<|(4)|\). 

For l2, we can restate the previous result, but such the functions constructed in the proof capture a more general family of functions.

**Proposition 7**.: For any \(n 4\), there exist boolean functions defined on \(n\) variables, and at least one instance, for which there exists an irrelevant feature \(i_{1}\), and a relevant feature \(i_{2}\{i_{1}\}\), such that \(|(i_{1})|>|(i_{2})|\).

As noted above, for Propositions3 to 5, the choice of the starting function is fairly flexible. In contrast, for Proposition6, we pick _one_ concrete function, which represents a trivial lower bound. As a result, and with the exception of l5, we can prove the following (fairly loose) lower bounds on the number of functions exhibiting the different issues.

**Proposition 8**.: For Propositions3 to 5,and Proposition7 the following are lower bounds on the numbers issues exhibiting the respective issues:

1. For Proposition3, a lower bound on the number of functions exhibiting l1 is \(2^{2^{(n-1)}}-n-3\).
2. For Proposition4, a lower bound on the number of functions exhibiting l3 is \(2^{2^{(n-1)/2}}-2\).
3. For Proposition5, a lower bound on the number of functions exhibiting l4 is \(2^{2^{(n-2)/2}}-2\).
4. For Proposition7, a lower bound on the number of functions exhibiting l2 is \(2^{2^{n-2}-(n-2)-1}-1\).

## 5 Conclusions

This paper gives theoretical arguments to the fact that Shapley values for explainability can produce misleading information about the relative importance of features. The paper distinguishes between the features that occur in one or more of the irreducible rule-based explanations, i.e. the _relevant_ features, from those that do not occur in any irreducible rule-based explanation, i.e. the _irrelevant_ features. The paper proves that, for boolean functions with arbitrary number of variables, irrelevant features can be deemed more important, given their Shapley value, than relevant features. Our results are also significant in practical deployment of explainability solutions. Indeed, misleading information about relative feature importance can induce human decision makers in error, by persuading them to look at the wrong causes of predictions.

One direction of research is to develop a better understanding of the distributions of functions exhibiting one or more of the issues of Shapley values.