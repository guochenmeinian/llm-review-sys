# A SARS-CoV-2 Interaction Dataset and VHH Sequence Corpus for Antibody Language Models

Hirofumi Tsuruta\({}^{1,2}\), Hiroyuki Yamazaki\({}^{1,3}\), Ryota Maeda\({}^{1,3}\),

Ryotao Tamura\({}^{1,2}\), Akihiro Imura\({}^{1,3}\)

\({}^{1}\)COGANNO Inc., \({}^{2}\)SAKURA internet Inc., \({}^{3}\)Biorhodes, Inc.

{tsuruta, yamazaki, maeda, ryotarotamura, akihiroimura}@cognano.co.jp

###### Abstract

Antibodies are crucial proteins produced by the immune system to eliminate harmful foreign substances and have become pivotal therapeutic agents for treating human diseases. To accelerate the discovery of antibody therapeutics, there is growing interest in constructing language models using antibody sequences. However, the applicability of pre-trained language models for antibody discovery has not been thoroughly evaluated due to the scarcity of labeled datasets. To overcome these limitations, we introduce AVIDA-SARS-CoV-2, a dataset featuring the antigen-variable domain of heavy chain of heavy chain antibody (VHH) interactions obtained from two alpacas immunized with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) spike proteins. AVIDA-SARS-CoV-2 includes binary labels indicating the binding or non-binding of diverse VHH sequences to 12 SARS-CoV-2 mutants, such as the Delta and Omicron variants. Furthermore, we release VHHCorpus-2M, a pre-training dataset for antibody language models, containing over two million VHH sequences. We report benchmark results for predicting SARS-CoV-2-VHH binding using VHHBERT pre-trained on VHHCorpus-2M and existing general protein and antibody-specific pre-trained language models. These results confirm that AVIDA-SARS-CoV-2 provides valuable benchmarks for evaluating the representation capabilities of antibody language models for binding prediction, thereby facilitating the development of AI-driven antibody discovery. The datasets are available at https://datasets.cognanous.com.

## 1 Introduction

Antibodies are vital proteins produced by the immune system to remove harmful foreign substances called antigens. Antibody-based therapeutics, which can bind to target antigens with high affinity and specificity, have become a major class of therapeutic agents and are currently used to treat a wide range of diseases [1; 2]. Among their successes, the rapid development and subsequent approval of antibodies against severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) epitomize the impactful response of this therapeutic class in addressing urgent global health challenges [74; 30]. However, the development of therapeutic antibodies remains a time-consuming and costly endeavor due to the complexity and difficulty of artificially manipulating the vast search space of antibody sequences . Therefore, computational approaches for accelerating antibody discovery have become increasingly popular in recent years [72; 24; 4].

Recent advances in language models offer new possibilities for understanding the information contained in antibody sequences because an antibody sequence can be represented as a string of letters representing a type of amino acid. With the construction of the observed antibody space (OAS) database [26; 46] that currently contains over two billion antibody sequences, a sufficient number of antibody sequences is now available to train antibody-specific language models [56; 27; 47; 70; 49; 5;22]. Olsen _et al._ presented AbLang, an antibody language model pre-trained on either the heavy or light chain antibody sequences in the OAS database. They demonstrated that AbLang can be used to accurately restore the missing residues in antibody sequences. Wang _et al._ proposed EATLM, a pre-trained antibody language model that incorporates evolutionary information as the pre-training objectives. They also provided an antibody understanding evaluation (ATUE) benchmark consisting of four tasks to evaluate the performance of pre-trained language models in antibody-related tasks.

Despite these promising developments, the applicability of pre-trained language models for antibody discovery has not been adequately evaluated due to the lack of labeled datasets. ATUE includes an antibody discovery task, a binary sequence classification that distinguishes antibodies that bind to SARS-CoV-2. The training dataset for this task used antibody sequences from SARS-CoV-2 patients and healthy persons from the OAS database. Although very few antibodies from SARS-CoV-2 patients are directly responsible for virus binding, these noisy and potentially unreliable individual-level disease labels were used to train a sequence-level classifier. Thus, a dataset with labels indicating whether the antibody binds to a specific antigen at the antibody sequence level would be extremely useful for a more accurate evaluation of model performance for antibody discovery.

In this study, we introduce AVIDa-SARS-CoV-2, a dataset featuring the antigen-variable domain of heavy chain of heavy chain antibody (VHH) interactions produced by two alpacas immunized with SARS-CoV-2 spike proteins. VHHs, found in camelids such as alpacas and ILamas, are promising therapeutic agents because of their small size, high stability, and high antigen-binding affinity [20; 19]. AVIDa-SARS-CoV-2 was generated using our previously established method for generating interaction datasets with reliable labels . AVIDa-SARS-CoV-2 contains binary labels that indicate whether each of the diverse VHH sequences binds or does not bind to 12 SARS-CoV-2 mutants, such as the Delta and Omicron variants. Notably, label reliability was verified by experimental evidence that VHHs extracted from AVIDa-SARS-CoV-2 bound to SARS-CoV-2 spike variants .

Furthermore, we introduce VHHCorpus-2M, a pre-training dataset for antibody language models containing over two million VHH sequences, and VHHBERT, an antibody language model pre-trained on VHHCorpus-2M. To avoid sequencing errors and increase sequence reliability, we removed singletons from the VHH sequences identified by next-generation sequencing (NGS), that is, only sequences observed more than once were used in the corpus. Although VHHCorpus-2M contains fewer sequences than OAS, it is distinctive in that it consists entirely of full-length VHH sequences that act as the smallest functional units for binding to each target antigen.

The main contributions of this paper are summarized as follows.

* We release AVIDa-SARS-CoV-2, a labeled SARS-CoV-2-VHH interaction dataset with amino acid sequences, and VHHCorpus-2M, which contains over two million unlabeled VHH sequences. These datasets can be used for the evaluation and pre-training of antibody-specific language models.
* AVIDa-SARS-CoV-2 contains information on the interactions of diverse VHHs produced by two alpacas with 12 SARS-CoV-2 mutants, providing researchers with valuable insights into the effects of antigen mutations on antibody binding and individual differences in antigen-specific VHHs.
* We release VHHBERT, a VHH-specific language model pre-trained using VHHCorpus-2M. VHHBERT will serve as a baseline for subsequent VHH-specific language models.
* We report benchmark results for the prediction of SARS-CoV-2-VHH interactions using VHHBERT and existing general protein and antibody-specific pre-trained language models. These results confirm that AVIDa-SARS-CoV-2 provides valuable benchmarks for assessing the representation capabilities of antibody language models for binding prediction.

## 2 Related Work

In this section, we put our work in the context of existing pre-trained antibody language models and their datasets used for pre-training and evaluation. Currently, there is growing interest in constructing language models using protein sequences [55; 52; 15; 32]. Inspired by these successes and the fact that the evolutionary process of antibodies is significantly different from that of proteins, several studies have attempted to train language models specific to antibody sequences. The representative existing studies are summarized in Table 1.

Pre-trained Antibody Language Models.Ruffolo _et al._ proposed AntiBERTy, the first antibody-specific language model to understand affinity maturation within immune repertoires. They found that AntiBERTy can cluster antibodies into trajectories resembling affinity maturation. Leem _et al._ presented a pre-trained antibody language model called AntiBERTa and fine-tuned AntiBERTa to predict the binding site of an antibody, called a paratope, from an antibody sequence. Olsen _et al._ pre-trained two language models: one trained only on heavy chains of antibodies (AbLang-H) and one trained only on light chains of antibodies (AbLang-L). These models outperformed ESM-1b , a general protein language model, in restoring the missing residues in antibody sequences. Wang _et al._ incorporated two original pre-training objectives into their proposed antibody language model, EATLM, to explore the benefits of incorporating specific biological mechanisms into pre-training. They also provided a useful benchmark, ATUE, that consists of four antibody-related tasks. Porebski _et al._ pre-trained BERT-DS using 20 million heavy-chain human antibody sequences and fine-tuned it for binding prediction. Barton _et al._ developed AntiBERTa2, an antibody language model pre-trained using 824 million antibody sequences including paired heavy and light chain sequences, and proposed a multimodal contrastive learning that amalgamates the representations of antibody sequences and structures. Kenlay _et al._ presented IgBert, which was initialized with the pre-trained protein language model ProtBERT  and trained using more than two billion unpaired antibody sequences (i.e., heavy chain or light chain only) and two million paired antibody sequences. Various other antibody-specific language models have been developed for antibody humanization , sequence generation , identification of evolutionarily plausible mutations , and classification of antigen-specific antibodies .

Pre-training Datasets.The pre-training datasets for antibody language models are large collections of unlabeled antibody sequences. Conventional antibodies in humans and mice comprise two pairs of heavy and light chains, meaning that one pair of chains serves as the functional unit for binding to the target antigen. Currently, OAS  contains over two billion unpaired antibody sequences, more than 90% of which are of human origin. Existing antibody language models are pre-trained primarily using unpaired antibody sequences in the OAS database. AntiBERTa2 and IgBert were trained with paired antibody sequences, but with a very small proportion compared with unpaired sequences.

VHHs are variable regions of heavy-chain antibodies found in camelids. Because VHH acts as a single functional unit, its sequence contains all the information necessary for antibody functions against antigens. The OAS database currently contains approximately 1.6 million unique VHH sequences collected from one study  and derived from three unimmunized Bactrian camels. The Integrated Nanobody Database for Immunoinformatics (INDI) database  currently contains more than 11 million unique VHH sequences collected mainly from the Sequence Read Archive (SRA)  and derived from dromedaries, Bactrian camels, ILamas, and alpacas. VHHCorpus-2M contains more than two million unique VHH sequences generated in original experiments using five alpacas.

Evaluation Datasets.The evaluation datasets are a set of labeled antibody sequences used to assess model performance for a specific task. AntiBERTa and EATLM were fine-tuned using the structural antibody database (SAbDab)  to evaluate their performance in predicting paratopes. Paratope prediction is important for the efficient discovery of antibody candidates that bind to an

    &  &  \\  Model & Dataset & 64Samples & Chain Type & Dataset & \#Samples & Task \\  AntiBERTy  & OAS & 58NM & Heavy, light & HIV-1 donor repertoires  & 232.593 & Evolutionary analysis \\  AntiBERTa  & OAS & 72M & Heavy, light & SAbDab  & 900 & Paratope prediction \\  AbLang-H  & OAS & 14M & Heavy & OAS & 2,000 & Sequence restoration \\  AbLang-L  & OAS & 0.24M & Light & OAS & 4,200 & Sequence restoration \\  AbLang-L  & OAS & 0.24M & Light & Mason _et al._’s dataset  & 21,612 & Binding prediction \\   & & & & & SablDab  & 1,662 & Paratope prediction \\  & & & & & Moxezf _et al._’s dataset  & 88,094 & BcI classification \\  & & & & OAS, CoV-AbAbb  & 22,000 & Antibody discovery \\  BiBert-DS  & OAS & 20M & Heavy & HiR2affnmat & 234,088 & Binding prediction \\  AntiBERTa2  & OAS, & & & & OAS & 20,000 & Sequence restoration \\  & & & & & & FLAb  & 6,745 & Binding affinity prediction \\   & & & & & OAS & 1,000 & Perplexity \\ 
**VHHBERT** & **VHHCorpus-2M** & **2M** & **Heavy** & **AVID-SARS-CoV-2** & **77,003** & **Binding prediction** \\   & & & & & & \\ 

Table 1: Characteristics of pre-trained antibody language models. “M” stands for million, and “B” stands for billion.

antigen of interest; however, the size of labeled datasets is limited. IgBert was evaluated in a binding affinity prediction task using each of three datasets with small data samples of 422 , 2048 , and 4275  from the fitness landscape for antibodies (FLAb) . BERT-DS was evaluated in a binding prediction task involving a three-category classification using a deep-screening dataset called HER2affmat. Although HER2affmat is a useful dataset with a large number of samples, it does not contain full-length antibody sequences. A binding prediction task using Mason _et al._'s dataset  in ATUE  was done that involved binary classification to determine whether the complementarity-determining region (CDR) of an antibody can bind to human epidermal growth factor receptor 2 (HER2). AntiBERTa2 was also evaluated for its performance in binding prediction using the same dataset. All antibody sequences in this dataset were derived from a single germline sequence, indicating that the diversity of antibody sequences was strongly limited. Thus, in ATUE, this task is considered to be less relevant to antibody-specific evolution.

The antibody discovery task in ATUE is a binary sequence classification that distinguishes antibodies that bind to SARS-CoV-2. This task has two notable limitations in terms of accurate model evaluation for antibody discovery. First, the dataset for training the sequence classifier uses antibody sequences with noisy individual-level labels from SARS-CoV-2 patients and healthy persons, even though very few antibodies from SARS-CoV-2 patients are responsible for virus binding. Second, this task assumes that if the third CDR of the heavy chain (CDR-H3) of the binder sequence predicted by the model is 90% or more identical to the CDR-H3 of the true binding sequences in CoV-AbDab , they have a similar binding performance. However, not only the sequence of CDRs but also the appropriate three-dimensional structure and interactions between variable regions are important for antigen-antibody activity . AVIDa-SARS-CoV-2 has sequence-level labels for binding and non-binding to SARS-CoV-2 mutants for each full-length VHH sequence.

AVIDa-SARS-CoV-2: Antigen-VHH Interaction Dataset Produced from Alpaca Immunized with SARS-CoV-2 Spike Proteins

AVIDa-SARS-CoV-2 is an antigen-VHH interaction dataset with 77,003 data samples, comprising 22,002 binding pairs and 55,001 non-binding pairs. The dataset was released under a CC BY-NC 4.0 license and is available at https://avida-sars-cov-2.cognanous.com.

### Dataset Generation

AVIDa-SARS-CoV-2 was generated using a method established in our previous study . This section introduces the overall workflow and key concepts underlying our data generation, as shown in Figure 1. Appendix A.2 provides the detailed step-by-step procedures for dataset generation.

ImmunizationWe used the immune system of live alpacas to obtain diverse VHHs that bind to SARS-CoV-2. First, we immunized two alpacas (hereafter referred to as Alpaca P and Alpaca C) that were maternal half-siblings with the 13 types of antigens listed in Table 2. The spike protein of SARS-CoV-2, which protrudes from the virus surface, is a crucial structural component that

Figure 1: Overview of data generation process for AVIDa-SARS-CoV-2.

facilitates its entry into host cells by binding to receptors on cells. Owing to its crucial role in the infection process, the spike protein is the primary target for antibodies. As the virus evolves over time, mutations in the spike protein that escape the immune response are enriched, and the effectiveness of antibodies to neutralize the virus is reduced. To investigate the effects of mutations in the spike protein, we generated mutants by selecting representative mutations that are effective for immune escape among the mutations observed to date.

Affinity SelectionAfter immunization, an alpaca's body harbors a small amount of SARS-CoV-2-specific VHHs produced by the immune response and a large amount of VHHs unrelated to SARS-CoV-2. To distinguish between them, we performed affinity selection by biopanning using the spike proteins listed in Table 2 as target molecules. We performed either bead panning, cell panning, or both for each target molecule. For bead panning, the ectodomain of the spike protein was produced by cells, purified, and then combined with beads as bait for panning. For cell panning, the bait was a whole cell overexpressing the full-length spike proteins on the cell membrane with the ectodomain protruding out. Through this process, target-specific VHHs become enriched, while non-specific VHHs are gradually diluted out, ultimately yielding a concentrated sample of target-specific VHHs.

Data LabelingWe counted the number of occurrences of each unique VHH sequence in the samples before and after affinity selection by NGS, which reflected the proportion of each VHH in the samples. We then compared the proportions of each VHH before and after affinity selection and labeled VHHs whose proportions significantly increased as "binder" and VHHs whose proportions significantly decreased as "non-binder" on the basis of statistical tests. In addition, VHHs whose proportions did not change significantly, corresponding to about 97% of the total, were excluded from the dataset to improve label reliability. We previously verified the reliability of this labeling method by confirming the binding ability of 20 labeled VHHs in AVIDa-hL6  using immunofluorescence staining analysis and biolayer interferometry analysis. Furthermore, label reliability was supported by experimental evidence that nine VHHs extracted from AVIDa-SARS-CoV-2 bound to SARS-CoV-2 spike variants, including Omicron .

### Dataset Analysis

Binding Sensitivity to Sequence VariationAVIDa-SARS-CoV-2 contains information regarding whether the same VHH sequence binds to each antigen type. The number of unique VHH sequences in AVIDa-SARS-CoV-2 is 36,100 including 14,078 sequences that bind to at least one antigen type. Notably, 427 VHH sequences were labeled as "binder" to specific antigen types but "non-binder" to others. In the case of infectious diseases and malignancies, the target antigen can mutate to escape the immune system or develop tolerance to treatment. If the binding site of an antigen, called an epitope, is mutated, the corresponding antibody will no longer bind to it. Conversely, if an antibody that loses binding activity due to an antigen mutation is identified, the location of the mutation can be assumed to be close to the epitope. Therefore, we further examined the binding activity of these 427 VHH sequences.

First, we clustered 427 VHH sequences using MMseqs2  with 90% sequence identity, resulting in 38 clusters of size two or more. We then extracted 54 VHH sequences from the three clusters in

   Antigen Type & Panning & Description \\  WT & cell & Wild-type (**WT**) SARS-CoV-2 identified in Wuhan \\ D614G & cell & Mutant with **D614G** mutation \\ Alpha & cell, bead & Mutant with representative mutations of **Alpha** variant \\ Alpha+K417N & cell & Mutant of antigen type “Alpha” with **K417N** mutation \\ Alpha+E484K & cell & Mutant of antigen type “Alpha” with **E484K** mutation \\ Beta & cell, bead & Mutant with representative mutations of **Beta** variant \\ Delta & cell, bead & Mutant with representative mutations of **Delta** variant \\ Kappa & bead & Mutant with representative mutations of **Kappa** variant \\ Lambda & bead & Mutant with representative mutations of **Lambda** variant \\ Omicron & cell, bead & Mutant with representative mutations of **Omicron** (BA.1) variant \\ PMS & bead & Polymutant spike (**PMS**) protein  \\ S2-domain & bead & **S2-domain** of the WT \\ OC43 & bead & Human coronavirus **OC43** (HCov-OC43) \\   

Table 2: Summary of antigen types. Appendix A.2 gives more details on each antigen.

descending order of cluster size and visualized whether each VHH sequence bound to each antigen type, as shown in Figure 2(a). Focusing on the vertical direction in cluster C, some sequences with over 90% sequence identity can exhibit varying binding abilities against the same antigen. Focusing on the horizontal direction, it is clear that the same VHH has different binding abilities for different antigen types. For example, in cluster B, antigen types from WT to Beta, which differ by only a few amino acids, can alter VHH binding, suggesting that these mutations enhance or inhibit binding. Interestingly, all VHHs in cluster A bind to WT but not to S2-domain, indicating that these VHHs bind to the S1 region of the spike protein. We can also recognize that most of these VHHs cannot be identified as binders for variants such as Delta, Kappa, Lambda, and Omicron, which exactly reflects the immune escape phenomenon in the real world. Therefore, AVIDa-SARS-CoV-2 contains sensitive information in which small amino acid sequence variations of an antibody and antigen can change between binding or non-binding, which should be strongly associated with their binding sites.

Individual Differences in Antigen-specific Antibody ProductionWe compared the differences in SARS-CoV-2-specific VHHs produced by the immune systems of the two alpacas. The number of unique VHH binders for Alpaca P and Alpaca C were 10,487 and 3,651, respectively, of which 60 VHHs were observed in both individuals. We encoded VHH sequences using Kidera factors , which represent the physicochemical properties of amino acids in a 10-dimensional vector, and then converted them into two-dimensional (2D) vectors using t-SNE . Figure 2(b) shows a 2D representation of the VHH binders. The data points derived from each individual partially overlapped but predominantly aggregated in distinct regions. Figure 2(c) shows a 2D representation of the VHH binders clustered using MMseq2 with 95% sequence identity and colored into 20 clusters in descending order of cluster size. This result indicates that the aggregations in 2D space reflect the VHH clusters formed on the basis of sequence identity. For example, cluster 1 (colored red) is composed of VHHs produced from Alpaca C, whereas cluster 2 (colored blue) is composed of VHHs produced from Alpaca P. These results demonstrate that using multiple individuals in dataset generation contributes to enhancing the diversity of antigen-specific VHH sequences.

Differences with AVIDa-hIL6Building on the findings from the above analysis, we elucidate the differences between AVIDa-SARS-CoV-2 and the previously released AVIDa-hIL6 , beyond the target antigens used for immunization. AVIDa-hIL6 used human interleukin-6 (IL-6) mutants produced by artificial point mutations, whereas AVIDa-SARS-CoV-2 used SARS-CoV-2 spike proteins with natural mutations that are more important for antigen-antibody interactions. This allowed AVIDa-SARS-CoV-2 to contain labels that reflect the immune escape phenomenon in the real world, as shown in Figure 2(a). Moreover, AVIDa-hIL6 collected VHHs from one alpaca, whereas AVIDa-SARS-CoV-2 collected them from two alpacas. This increased the diversity of antigen-specific antibodies and provided valuable insights into the sequence differences of antigen-specific antibodies between individuals, as shown in Figure 2(b).

Figure 2: (a) Label visualization for each pair between 54 VHHs in three clusters and antigens. Each cell represents unique VHH-antigen pair. White cells are unlabeled pairs that cannot be identified as “binder” or “non-binder” and are not included in AVIDa-SARS-CoV-2. (b)(c) Two-dimensional representation of binder sequences colored by individuals and clusters. Appendix A.3 provides enlarged versions of (b) and (c).

VHHCorpus-2M: VHH Sequence Corpus Produced from Alpaca

VHHCorpus-2M is a corpus containing 2,040,988 unique VHH sequences. The corpus was released under a CC BY-NC 4.0 license and is available at https://vhh-corpus.cognanous.com.

### Dataset Collection

VHHCorpus-2M is a collection of unique VHH sequences from several datasets generated by the process described in Section 3.1 using target antigens other than the SARS-CoV-2 spike protein, such as the human immunodeficiency virus type 1 (HIV-1) envelope protein, human IL-6, HER2, human histone, transmembrane glycoprotein mucin 1 (MUC1), and gram-negative bacteria. We collected VHH sequences from datasets produced by five alpaca, different from those used in the generation of AVIDa-SARS-CoV-2, to avoid potential data leakage and increase the diversity of VHH sequences. Note that the source datasets include publicly available AVIDa-hIL6  in addition to multiple datasets that have not been published as labeled binding datasets. Importantly, we used only VHH sequences that were identified multiple times by NGS in our corpus to avoid sequencing errors and increase sequence reliability.

### Dataset Analysis

The size and diversity of pre-training datasets play an important role in improving the performance of a language model in downstream tasks [51; 33]. VHHCorpus-2M comprises 2,040,988 unique sequences, which is more than 50 times the number of unique sequences in AVIDa-SARS-CoV-2. To examine the degree of sequence diversity in the datasets, we calculated the pairwise sequence identities within each dataset. First, to mitigate the computational complexity, we used MMseqs2 to cluster the VHH sequences with 70% sequence identity within each dataset, resulting in 8,270 and 777 clusters for VHHCorpus-2M and AVIDa-SARS-CoV-2, respectively. We then extracted representative sequences from all clusters and calculated all pairwise sequence identities among these representatives for each dataset. Figure 3 presents the distribution of pairwise sequence identities for each dataset. The distribution of VHHCorpus-2M has a broader peak in regions with a lower sequence identity compared with AVIDa-SARS-CoV-2, indicating a higher sequence diversity. This could be attributed to the fact that VHHCorpus-2M originated from five alpacas, which is more than AVIDa-SARS-CoV-2. Additionally, VHHCorpus-2M includes unlabeled VHH sequences that cannot be labeled as "binder" or "non-binder" for specific antigens, further contributing to its diversity.

## 5 Benchmarks

### Benchmark Task

To evaluate the performance of various pre-trained language models for antibody discovery, we defined a binary classification task to predict the binding or non-binding of unknown antibodies to 13 antigens using AVIDa-SARS-CoV-2. By leveraging the binding information of diverse VHHs produced from the two alpaca, we used data samples obtained from Alpaca P as the training set and data samples obtained from Alpaca C as the test set. Table 3 lists the number of samples in each set. As shown in Figure 2(b) and (c), the VHH binders derived from different alpacas formed distinct clusters. Therefore, this experimental scenario assumes that we want to explore additional effective antibodies beyond those already observed to bind to a known antigen. This scenario holds significant importance in the development of therapeutic antibodies, given that antibodies with different sequences can bind to different binding sites of antigens, called epitopes. Depending on their binding sites, antibodies may have specific biologically important functions, such as neutralization, inhibition, or activation, and can be extremely useful in drugs.

Figure 3: Distribution of pairwise identities of VHH sequences.

    \\  Dataset &  & Non-binder & Total \\  Training & 15,400 & 34,285 & 49,685 \\  Test & 6,602 & 20,716 & 27,318 \\   

Table 3: Numbers of samples in training and test sets.

### Experimental Settings

Baseline ModelsTo fully evaluate the representation capabilities of the language models pre-trained on various training sequence data, we selected the following nine baseline models. (1) **ProtBert** is a BERT-based  model pre-trained on 216 million protein sequences in UniRef . (2)(3) **ESM-2** is a model pre-trained on 65 million unique protein sequences in UniRef . We used ESM-2 with 150 and 650 million parameters (hereafter referred to as ESM-2 150M and ESM-2 650M). We adopted ProtBert and ESM-2 to confirm whether pre-training with antibodies, a subset of proteins, is effective for predicting VHH binding. (4) **AbLang-H** is a RoBERTa-based  model pre-trained on 14 million heavy chains of antibodies in the OAS database. (5) **AntiBERTa2** is a RoFormer-based  model pre-trained using 824 million antibody sequences including paired antibody sequences in the OAS and proprietary database. (6) **AntiBERTa2-CSSP** is a multimodal version of AntiBERTa2 that is further trained on human antibody structures using contrastive sequence-structure pre-training (CSSP). (7) **IgBert** is a model initialized with weights of ProtBert and trained using more than two billion unpaired sequences of light and heavy chains and two million paired sequences in the OAS database. (8) **VHHBERT** is a RoBERTa-based model pre-trained on two million VHH sequences in VHHCorpus-2M. We used the same model parameters as RoBERTaBASE, except that it used positional embeddings with a length of 185 to cover the maximum sequence length of 179 in VHHCorpus-2M. (9) **VHHBERT w/o PT** is a VHHBERT initialized with random weights without pre-training. We adopted this model to confirm the effectiveness of pre-training.

Pre-trainingAs a pre-training corpus for VHHBERT, VHHCorpus-2M was randomly divided into 2,000,000 training sets and 40,988 validation sets. The VHH sequences were tokenized by mapping each of the 20 amino acids to a different token ID and adding special tokens at the beginning and end of the sequence. We used masked language modeling as the pre-training objective. During pre-training, 15% of the residues from each VHH sequence were randomly selected, and of these, 80% were masked, 10% were randomly changed to another residue, and 10% remained unchanged. VHHBERT was pre-trained for 312,500 steps, which equates to 20 epochs, with a batch size of 128 on one NVIDIA Tesla V100 GPU. The resulting VHHBERT is available on the Hugging Face Hub1.

Fine-tuningAs a fine-tuning dataset, we used AVIDA-SARS-CoV-2, which was divided by individual, as shown in Table 3. Figure 4 shows an overview of the experimental setup. AVIDA-SARS-CoV-2 has the amino acid sequences of VHHs and antigens as input features for binding prediction. To obtain each sequence representation, we used the nine aforementioned baseline models for VHHs and the pre-trained protein language model ESM-2 for antigens and extracted the mean of the representations for each amino acid from the last layer in each language model. The sequence representations of the VHHs and antigens were concatenated and utilized as input to a multi-layer perceptron, which was added on top of the two language models as a classification head. Note that we fixed the weights of ESM-2 used for antigens and fine-tuned the classification head and the language model used for VHHs to assess the representation capabilities of antibody language models. We trained the models for 30 epochs with a batch size of 32 on one NVIDIA Tesla V100 GPU. We conducted five repetitive experiments with different random seeds and report the average results and standard derivation.

### Results

Table 4 shows the performance comparisons of the baseline models for the VHH-antigen binding prediction. We used precision, recall, F1-score, and area under the precision-recall curve (AUPRC) in addition to accuracy as evaluation metrics because the prediction of antibody binders, which are fewer in number than non-binders, is much more important for drug discovery. VHHBERT w/o PT showed high precision but significantly lower recall than the other models, resulting in the lowest F1-score.

Figure 4: Overview of the experimental setup.

This result indicates the effectiveness of pre-training on protein and antibody sequences in predicting VHH binding. AntiBERTa2, AntiBERTa2-CSSP, IgBert, and VHHBERT pre-trained on antibody sequences outperformed ESM-2 and ProtBert pre-trained on protein sequences in accuracy, F1-score, and AUPRC. This is consistent with previous studies [27; 70; 5] that reported that using antibodies for pre-training, rather than general proteins, contributes to the performance of antibody-specific tasks. In general, the antigen to which an antibody binds is determined by the amino acid sequence in CDRs. Because CDRs are highly variable owing to mechanisms such as immunoglobulin gene rearrangement and somatic hypermutation , they do not follow the evolutionary information stored in general protein sequences . Because of these differences in evolutionary processes, pre-training with antibody sequences should be effective in predicting VHH binding.

AntiBERTa2, AntiBERTa2-CSSP, and IgBert outperformed the other models in terms of accuracy and F1-score, suggesting that pre-training with a larger number of antibody sequences contributes to the generalization to unknown antibody clusters. Interestingly, additional pre-training of AntiBERTa2-CSSP using human antibody structures contributed to improved performance in predicting VHH-antigen binding. However, the highest F1-score of AntiBERTa2-CSSP remains at approximately 65%; therefore, there is still room for performance improvement for practical drug discovery applications. Although VHHBERT was pre-trained with significantly fewer antibody sequences than the other pre-trained antibody language models, its F1-score was higher than AbLang-H and close to IgBert. This result can probably be attributed to differences in the sequence patterns between conventional antibodies and VHHs. Specifically, the average length of CDR3, which is the most important for antigen recognition, is approximately 1.5 times longer for VHHs than for conventional antibodies [68; 17]. Moreover, the genetic sequences of antibodies differ between species, resulting in differences in amino acid sequences, even in non-variable regions [34; 62]. In conclusion, these insights obtained through benchmarks underscore the significance of AVIDa-SARS-CoV-2 as a useful benchmark for assessing the representation capabilities of antibody language models for binding prediction, thereby promoting the advancement of AI-assisted antibody discovery.

## 6 Discussion

### VHH-specific Language Models

Recent remarkable progress in language models has led to the active development of domain-specific language models in various application fields [6; 16; 36]. Here, we discuss the significance of building VHH-specific language models from the perspective of drug discovery. VHHs have recently attracted attention as therapeutic agents because of their small size, high stability, good human tolerability, and relative ease of production [20; 19]. Furthermore, VHHs possess favorable properties for the construction of large-scale language models. First, VHHs have a simple structure consisting of only heavy chains, which allows for easier identification of full-length amino acid sequences using DNA sequencing technologies. Second, VHH acts as a single functional unit, meaning that VHH sequences contain all the information necessary for the function of an antibody against an antigen. In contrast, conventional antibodies are composed of two pairs of heavy and light chains, and they function as a single functional unit by combining heavy and light chains. Therefore, paired sequences should ideally be used as the input data for language models. However, it is difficult to construct a large-scale database of paired sequences because obtaining them requires time-consuming experiments. Indeed,

   Model & Accuracy & Precision & Recall & F1-score & AUPRC \\  ProtBert & 0.803 \(\) 0.012 & 0.602 \(\) 0.036 & 0.564 \(\) 0.046 & 0.580 \(\) 0.023 & 0.532 \(\) 0.073 \\ ESM-2 150M & 0.801 \(\) 0.010 & 0.607 \(\) 0.034 & 0.514 \(\) 0.036 & 0.555 \(\) 0.021 & 0.531 \(\) 0.047 \\ ESM-2 650M & 0.822 \(\) 0.020 & 0.682 \(\) 0.083 & 0.540 \(\) 0.048 & 0.598 \(\) 0.023 & 0.584 \(\) 0.069 \\ AbLang-H & 0.828 \(\) 0.004 & 0.753 \(\) 0.033 & 0.430 \(\) 0.017 & 0.547 \(\) 0.005 & 0.589 \(\) 0.018 \\ AntiBERTa2 & 0.851 \(\) 0.007 & 0.769 \(\) 0.044 & 0.551 \(\) 0.021 & 0.641 \(\) 0.008 & 0.660 \(\) 0.018 \\ AntiBERTa2-CSSP & **0.854 \(\) 0.007** & 0.773 \(\) 0.030 & 0.565 \(\) 0.014 & **0.652 \(\) 0.014** & **0.690 \(\) 0.011** \\ IgBert & 0.845 \(\) 0.007 & 0.741 \(\) 0.045 & 0.558 \(\) 0.045 & 0.634 \(\) 0.018 & 0.610 \(\) 0.044 \\ VHHBERT & 0.823 \(\) 0.011 & 0.658 \(\) 0.042 & **0.567 \(\) 0.025** & 0.608 \(\) 0.012 & 0.650 \(\) 0.025 \\ VHHBERT w/o PT & 0.831 \(\) 0.003 & **0.811 \(\) 0.024** & 0.392 \(\) 0.010 & 0.528 \(\) 0.008 & 0.624 \(\) 0.008 \\   

Table 4: Performance comparisons of baseline models for VHH-antigen binding prediction. Best performance is highlighted in bold.

the number of paired sequences recorded in the OAS database is approximately one thousand times less than that of unpaired sequences. Accordingly, the existing antibody language models are trained primarily on unpaired heavy- and/or light-chain sequences, ignoring the effects of their counterpart chains. Although AntiBERTa2 and IgBert used paired sequences in their training data, the majority of the training data consists of unpaired sequences. The advantages of VHH over conventional antibodies will facilitate the construction of large-scale databases that are meaningful for therapeutic antibody discovery and pave the way for future construction of practical VHH-specific language models.

### Negative Societal Impacts

The VHH binders in AVIDa-SARS-CoV-2 have the potential to be useful in COVID-19 therapeutics. In addition, using our dataset to develop predictive models for binding to SARS-CoV-2 variants may accelerate the development of therapeutics against emerging variants of concern (VOCs). To reap these benefits, there is a possibility that third-party organizations could use our dataset for commercial purposes. Because this creates the risk of future conflicts of interest between third-party organizations, we have prohibited commercial use by licensing the dataset. Furthermore, antibodies usually act as inhibitors or neutralizers of their target antigen, but some, although relatively rare, can stimulate or enhance the function of the target [57; 29]. Even if antibodies that bind to spike proteins can be identified or predicted, the possibility of promoting infection cannot be excluded. Therefore, validation experiments and clinical trials must be conducted to confirm the usefulness of each VHH.

### Limitations and Future Work

Our dataset potentially contains data biases derived from the specific alpacas used for dataset generation. As shown in Figure 2(b), each individual produced biased SARS-CoV-2-specific VHHs. This limitation may reduce the generalization performance of the models trained on our dataset, potentially hindering their practical application in antibody discovery. The best way to address this limitation is to generate datasets from multiple individuals with different VHH gene sequences at multiple times of immunization. This would mitigate data biases in the dataset and help a model to understand the universal knowledge of antigen-antibody interactions. However, realistically, there are limitations in creating datasets under various conditions, owing to cost and time constraints. Therefore, it is also necessary to develop models that can overcome data biases. Our benchmark task serves as a valuable benchmark for evaluating whether models can overcome individual-induced biases, and to our knowledge, no other such datasets exist. In the future, in addition to generating and publishing more diverse datasets, we plan to research further model architectures and pre-training methods to achieve a high generalization performance in real-world antibody discovery tasks.

## 7 Conclusion

In this study, we introduced AVIDa-SARS-CoV-2, a labeled dataset of SARS-CoV-2-VHH interactions, and VHHCorpus-2M, which contains over two million VHH sequences, providing novel datasets for the evaluation and pre-training of antibody language models. In addition, we developed a VHH-specific language model, VHHBERT, pre-trained on VHHCorpus-2M and reported benchmark results for binding prediction using existing general protein and antibody-specific language models. We envision that the availability of AVIDa-SARS-CoV-2 and VHHCorpus-2M will facilitate further research on antibody language models and their application in therapeutic antibody discovery.