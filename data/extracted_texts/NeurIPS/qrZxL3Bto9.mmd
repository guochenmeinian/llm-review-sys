# Evaluating language models as risk scores

Andre F. Cruz\({}^{1,2}\) Moritz Hardt\({}^{1,2}\) Celestine Mendler-Dunner\({}^{1,2,3}\)

\({}^{1}\)Max Planck Institute for Intelligent Systems, Tubingen

\({}^{2}\)Tubingen AI Center \({}^{3}\)ELLIS Institute Tubingen

###### Abstract

Current question-answering benchmarks predominantly focus on accuracy in realizable prediction tasks. Conditioned on a question and answer-key, does the most likely token match the ground truth? Such benchmarks necessarily fail to evaluate LLMs' ability to quantify ground-truth outcome uncertainty. In this work, we focus on the use of LLMs as risk scores for unrealizable prediction tasks. We introduce folktexts, a software package to systematically generate risk scores using LLMs, and evaluate them against US Census data products. A flexible API enables the use of different prompting schemes, local or web-hosted models, and diverse census columns that can be used to compose custom prediction tasks. We evaluate 17 recent LLMs across five proposed benchmark tasks. We find that zero-shot risk scores produced by multiple-choice question-answering have high predictive signal but are wildly miscalibrated. Base models consistently overestimate outcome uncertainty, while instruction-tuned models underestimate uncertainty and produce over-confident risk scores. In fact, instruction-tuning polarizes answer distribution regardless of true underlying data uncertainty. This reveals a general inability of instruction-tuned models to express data uncertainty using multiple-choice answers. A separate experiment using verbalized chat-style risk queries yields substantially improved calibration across instruction-tuned models. These differences in ability to quantify data uncertainty cannot be revealed in realizable settings, and highlight a blind-spot in the current evaluation ecosystem that folktexts covers.

## 1 Introduction

Fueled by the success of large language models (LLMs), it is increasingly tempting for practitioners to use such models for risk assessment and decision making in consequential domains . Given the CV of a job applicant, for example, some might prompt a model, what are the chances that the employee will perform well on the job? The true answer is likely uncertain. Some applicants of the same features will do well, others won't. A good statistical model should faithfully reflect such outcome uncertainty.

Calibration is perhaps the most basic kind of uncertainty quantification to ask for. A calibrated model, on average, reflects the true frequency of outcomes in a population. Calibrated models must therefore give at least some indication of uncertainty. Fundamental to statistical practice across the board, calibration has also been a central component in the debate around the ethics and fairness of consequential risk scoring in recent years .

The evaluation of LLMs to date, however, has predominantly focused on accuracy metrics, often in realizable tasks where there is a unique correct label for each data point. Such benchmarks necessarily cannot speak to the use of language models as risk score estimators. A model can have high utility in well-defined question-answering tasks while being wildly miscalibrated. In fact, while accuracy corresponds to knowledge of the expected answer, proper uncertainty quantification corresponds to knowledge of the variance over answers.

### Our contributions

We contribute an open-source software package, called folktexts,1 that provides datasets2 and tools to evaluate statistical properties of LLMs as risk scorers. We show-case its functionalities with a sweep of new empirical insights on the risk scores produced by 17 recently proposed LLMs.

The folktexts package offers a systematic way to translate between the natural language interface and the standard machine learning type signature. It translates prediction tasks defined by numeric features \(X\) and labels \(Y\) into natural text prompts and extracts risk scores \(R\) from LLMs. This opens up a rich repertoire of open-source libraries, benchmarks and evaluation tools to study their statistical properties. Figure 1 illustrates the workflow for producing risk scores using LLMs.

For benchmarking risk scores, we need ground truth samples from a known probability distribution. Inspired by the popular folktables package , folktexts builds on US Census data products, specifically, the American Community Survey (ACS) , collecting information about more than 3.2 million individuals representative of the US population. Folktexts systematically constructs prediction tasks and prompts from the individual survey responses using the US Census codebook, and the ACS questionnaire as a reference. Risk scores are extracted from the language model's output token probabilities using a standard question-answering interface. The package offers five pre-specified question-answering benchmark tasks that are ready-to-use with any language model. A flexible API allows for a variety of natural language tasks to be constructed out of 28 census features whose values are mapped to prompt-completion pairs (features detailed in Table A5). Furthermore, evaluations can easily be performed over subgroups of the population to conduct algorithmic fairness audits.

Empirical insights.We contribute a sweep of empirical findings based on our package. We evaluate 17 recently proposed LLMs, with sizes ranging from 2B parameters to 141B parameters. Our study demonstrates how inspecting risk scores of LLMs on underspecified prediction tasks reveals new insights that cannot be deduced from inspecting accuracy alone. The main findings are summarized as follows:

* Models' output token probabilities have strong predictive signal3 but are wildly miscalibrated. * The failure modes of models are different: Multiple-choice answer probabilities generated by base models consistently overestimate outcome uncertainty, while instruction-tuned models underestimate uncertainty and produce over-confident risk scores.
* Instruction-tuning polarizes multiple-choice answer probabilities, regardless of ground-truth outcome uncertainty -- leading to a general inability to express data uncertainty.
* Using a chat-style prompt that verbally queries for probabilities results in materially different answer distributions, with significantly improved calibration for instruction-tuned models, accompanied by a small but consistent decrease in predictive power.

Figure 1: Information flow from tabular data to risk scores, using a supervised classifier (_left_) or a language model (_right_). The folktexts package maps language models to the traditional machine learning workflow.

We hope our package facilitates future investigations into statistical properties of LLMs, not only as a way to faithfully reflect real-world uncertainties, but also as a required stepping stone to trustworthy model responses.

Outline.In Section 2 we provide necessary background on risk scores and calibration in statistical machine learning. In Section 3 we extend this background to the application to language models, providing various design choices around prompting templates and ways to extract risk scores from language models. In Section 4 we evaluate 17 recent LLMs on 5 proposed benchmark tasks and summarize empirical findings.

### Limitations

Predictive modeling and statistical risk scoring in consequential settings is a matter of active debate. Numerous scholars have cautioned us about the dangers of statistical risk scoring and, in particular, the potential of risk scores to harm marginalized and vulnerable populations . Our evaluation suite is intended to help in identifying one potential problem with language models for risk assessment, specifically, their inability to faithfully represent outcome uncertainty. However, our metrics are not intended to be sufficient criteria for the use of LLMs in consequential risk assessment applications. The fact that a model is calibrated says little about the potential impact it might have when used as a risk score. Numerous works in the algorithmic fairness literature, for example, have discussed the limitations of calibration as a fairness metric, see, e.g., . There is also significant work on the limitations of statistical tools for predicting future outcomes and making decisions based on these predictions . Calibration cannot and does not address these limitations.

### Related work

The use of LLMs for decision-making has seen increasing interest as of late. Hegselmann et al.  show that a Bigscience T0 11B model  surpasses the predictive performance (AUC) of supervised learning baselines in the very-few-samples regime. The authors find that fine-tuning an LLM outperforms fitting a standard statistical model on a variety of tasks up to training set sizes in the 10s to 100s of samples. Related work by Slack and Singh  shows that providing task-specific expert knowledge via instructions in context can lead to significant improvements in model predictive power. Tamkin et al.  generate hypothetical individual information for a variety of decision scenarios, and analyze how language models' outputs change when provided different demographic data. Some attributes are found to positively affect the model's decision (e.g., higher chance of approving a small business loan for minorities) while others affect it negatively (e.g., lower chance for older aged individuals).

A separate research thread considers how to leverage LLMs to model human population statistics. Argyle et al.  evaluate whether GPT-3 can faithfully reproduce political party preferences for different US subpopulations, and conclude that model outputs can accurately reflect a variety of correlations between demographics and political preferences. Other works use a similar methodology to model the distribution of human opinions on different domains . Aher et al. , Dillion et al.  use LLMs to reproduce popular psychology experiments on human moral judgments, and confirm there's good alignment between human answers and LLM outputs. Literature on modeling human population statistics generally focuses on using LLMs to obtain accurate survey completions. That is, given demographic information on an individual, what was their response to a specific survey question? This methodology often ignores the fact that individuals described by the same set of demographic features will realistically give different answers. An accurate model would not only provide the highest likelihood answer, but also a measure of uncertainty corresponding to the expected variability within a sub-population. Our work tackles this arguably neglected research avenue: Analyzing LLM risk scores instead of discrete token answers, and whether they are accurate and calibrated to human populations.

Calibration.Calibration is a widely studied concept in the literature on forecasting in statistics and econometrics with a venerable history . Recent years have seen a surge in interest in calibration in the context of deep learning . Calibration of LLMs has been studied on diverse question-answering benchmarks, ranging from sentiment classification, knowledge testing, and mathematical reasoning to multi-task benchmarks [e.g., 34, 35, 36, 37, 38, 39, 40, 41, 42, 43]. What differentiates our work is that we study calibration in naturally underspecified prediction tasks. This requires even the most accurate models to accurately reflect non-trivial probabilities over outcomes to be calibrated.

To systematically construct such prediction tasks we resort to survey data. Surveys have a long tradition in social science research as a tool for gathering statistical information about the characteristics and opinions of human populations . Survey data comes with carefully curated questionnaires, as well as ground truth data. The value of this rich data source for model evaluation has not remained unrecognized. Surveys have recently gained attention to study bias and alignment of LLMs [45; 46; 22; 47; 48], and inspecting systematic biases in multiple-choice responses . Instead of using surveys to get insights about a models natural inclinations, we use them to test model calibration with respect to a given population.

Beyond task-calibration, calibration at the word and token-level has been explored in the context of language generation [e.g., 50]. Others have focused on connections to hallucination , and expressing uncertainty in natural language [52; 53; 54]. Related to our work, Lin et al.  emphasize the inherent outcome uncertainty in language generation. Focusing on generation at the word or token level poses the challenge of measuring a high-dimensional probability distribution. Considering binary classification tasks has the practical advantage of circumventing this problem.

Calibration has also played a major role in the algorithmic fairness literature. Group-wise calibration has been proposed as a fairness criterion since the 1960s [56; 57]. In particular, it's been a notion central to an active debate about the fairness of risk scores in consequential decision making [5; 6; 7; 8]. A recent line of work originating in algorithmic fairness studies _multicalibration_ as a strengthening of calibration .

## 2 Preliminaries

This section provides necessary background on risk scores and the statistical evaluation of binary predictors. Throughout, we assume a joint distribution \(\) given by a pair of random variables \((X,Y)\) where \(X\) is a set of features and \(Y\) is the outcome to be predicted. In the applications we study, the features \(X\) typically form a text sequence and the outcome \(Y\) is a discrete random variable that we would like to predict from the sequence. A parametric model \(f_{}(y|x)\) assigns a probability to each possible outcome \(y\) given a feature vector \(x\). The goal of a generative model is generally to approximate the conditional distribution \((y|x)\), where parameters are fit to a huge corpus of training data.

We will focus on binary prediction throughout this work. Let \(Y\{0,1\}\) be a random variable indicating the outcome of an event the learner wishes to predict. We use the shorthand notation \(f_{}(x)\) to denote the model's estimate of the probability that \(Y=1\), given context \(X=x\). Following standard terminology we will refer to \(f_{}:\) as _score function_, and its output \(f_{}(x)\) as _risk score_. There are various dimensions along which to evaluate risk scores by comparing them against samples of the reference population \(\).

### Calibration

We say a score function is _calibrated_ over a population \(\) if and only if for all values \(r\) with \(\{f_{}(X)=r\}>0\), we have

\[[Y=1 f_{}(x)=r]=r.\] (1)

This condition asks that, over the set of all instances \(x\) with score value \(r\), an \(r\) fraction of those instances must indeed have a positive label. Importantly, calibration is defined with respect to a population, it does not measure a model's ability to discriminate between instances. A model that outputs the constant value \(=[Y]\) on all instances is calibrated, by definition. In particular, we can always achieve calibration by setting \(f_{}(x)\) with the average value of \(y\) among all instances \(x^{}\) in a given partition such that \(f_{}(x^{})=f_{}(x)\).

We use Expected Calibration Error (ECE) as the primary metric to empirically evaluate calibration. It is defined as the expected absolute difference between a classifier's confidence in its predictions and the accuracy on the same predictions. More formally, given \(n\) triplets \((x_{i},y_{i},r_{i})\) where \(r_{i}\) denotesthe model's score value for the corresponding data point \((x_{i},y_{i})\). The ECE is defined as

\[_{m}|_{i B_{m}}y_{i}-_{i B _{m}}r_{i}|,\] (2)

where data points are grouped into \(M\) equally spaced bins \(B_{m}\) according to their score values. We use \(M=10\) in our evaluation, which is a commonly used value . We also provide a measure of ECE over quantile-based score bins, as well as Brier score  to allow for a more complete picture. Furthermore, we use reliability diagrams  to aid a visual interpretation of calibration. These diagrams plot expected sample accuracy as a function of confidence. Any deviation from a perfect diagonal represents miscalibration.

We can strengthen the calibration condition in (1) by requiring it in multiple subgroups of the population. Specifically, letting \(G\) denote any discrete random variable, we can require the conditional calibration condition \([Y=1 G=g\,,f_{}(x)=r]=r\) for every setting \(g\) of the random variable \(G\). This is often used to define fairness.

### Predictive performance

When solving classification problems it's common practice to threshold risk scores to obtain a classifier. In our notation this corresponds to thresholding the risk scores \(f_{}(x)\):

\[c(x)=\{f_{}(x)>\}.\]

The classifier \(c\) that minimizes the misclassification error \(\,\{c(X) Y\}=\{c(X) Y\}\) is given by \(c^{*}(x)=\{f^{*}(x)>0.5\}\), where \(f^{*}\) is the Bayes optimal scoring function. In the following, when we use _accuracy_ we refer to the fraction of correct predictions after thresholding. If not specified otherwise we use \(=0.5\). This corresponds to an argmax operator applied to the class probabilities. It is important to note that a classifier can achieve perfect accuracy even when derived from a suboptimal scoring function. Thus, accuracy alone provides an incomplete picture of a model's ability to express uncertainty.

Instance ranking.The area under the receiver operating characteristic curve (AUC) is a rank-based measure of predictive model performance. It measures the probability that a randomly chosen positive observation (\(Y=1\)) will have a higher score than a randomly chosen negative observation (\(Y=0\)). A high AUC value in no way reflects accurate or calibrated probability estimates, it relates only to the signal-to-noise ratio in the risk scoring function . Using AUC allows us to neatly separate risk score calibration from their predictive signal, although both are crucial for accurate class predictions.

## 3 Evaluating language models as risk scores

We are interested in the ability of LLMs to express natural uncertainty in outcomes. Therefore, we construct unrealizable binary prediction tasks, and test the model's ability to reflect natural variations in underspecified _individual_ outcomes. Specifically, we prompt models with feature values \(x\) to elicit risk scores \(r\) and then evaluate these scores against ground truth labels \(y\) (see Figure 1).

### Prediction tasks

We construct natural language prediction tasks from the American Community Survey (ACS) Public Use Microdata Sample (PUMS).4 The data contains survey responses of about 3.2 million anonymous individuals, carefully curated to offer statistical insights into the population of the United States. We refer to the data as Census data. The Census data contains demographic attributes, as well as information related to income, employment, health, transportation, and housing. Prediction tasks are defined by selecting a subset of attributes to define the features and one attribute to be the label. We threshold continuous target variables and bin multi-class predictions to obtain a binary classification task. Specifically, we test models on their ability to reflect natural variations in the outcome across the benchmark population. To enable straightforward comparison with existing tabular benchmarks, we consider natural text analogues to the tasks in the popular folktables benchmark package . Appendix B describes each task in further detail.

### Extracting risk scores

To extract risk scores from LLMs, we map each inference problem to a natural text prompt. For a given data point, we specify the classification task in the prompt and extract class probabilities from the model's next token probabilities, similar to traditional question-answering interfaces. The prompt consists of three components (as shown in Figure 1):

* **Instantiating population:** We first instantiate the population \(\) in the prompt context. This corresponds to the population represented by our reference data. We use third-person prompting: "The following data describes a survey respondent. The survey was conducted among US residents in 2018. Please answer the question based on the information provided." This step is typically not needed in supervised classification tasks, because the training data implicitly defines the population. However, for LLMs this is particularly important for evaluating calibration outside the realizable setting, as risk scores cannot in general simultaneously be calibrated to different populations. Skipping this step could provide insights related to alignment [45; 46; 22] rather than calibration.
* Gender is: Male.
- Age is: 50 years old." We use a bulleted list of short sentences to encode features. Related literature has found this simple approach to yield the best results [1; 19; 61; 62; 63].
* **Querying outcome:** We use a standard _multiple-choice_ prompting format to elicit outcome predictions from LLMs. The framing of the question for an individual outcome is taken from the original multiple-choice Census questionnaire based on which the data was collected. All answers are presented as binary choices. Querying about an individual's income would be: "Question: What was this person's total income during the past 12 months? A: Below $50,000. B: Above $50,000. Answer:" The model's confidence on a given answer is given by the next token probabilities for A and B. Additionally, we conduct experiments using a separate chat-style prompt that verbally queries for a numeric probability estimate (dubbed _numeric_ prompting). The above income query would be: "Question: What is the probability that this person's yearly income is above $50,000? Answer (between 0 and 1): " This more closely matches how real-world users interact with LLMs, and has been reported to improve uncertainty quantification .

When using the constructed multiple-choice prompts, we query the models and extract scores from the next token probabilities for the choice labels A,B as \(r_{i}=()/(()+())\), following the methodology of standard question-answering benchmarks [45; 39]. As LLMs are known to have ordering biases in multiple-choice question-answering [65; 47], we evaluate responses on all choice orderings and average the resulting scores. When using numeric prompting, we prefix the answer with '0.' to improve the likelihood of a direct numeric response, and run two forward passes, selecting the highest likelihood numeric token at each iteration. We refer to Xiong et al.  for an overview on alternative design choices on how to elicit confidence scores from LLMs.

### The folktexts package

The folktexts package is designed to offer a flexible interface between tabular prediction tasks and natural language question-answering tasks in order to extract risk scores from LLMs. The package makes available the ACSIncome, ACSPublicCoverage, ACSMobility, ACSEmployment, and ACSTravelTime prediction tasks  as natural language benchmarks, together with various functionalities to customize the task definitions (e.g., use a different set of features to predict income) and subsample the reference data (e.g., predict income only among college graduates in California). The set of attributes available to define the features and label can be found in the ACS PUMS data dictionary.4 Additionally, folktexts is compatible with open-source models running locally, as well as with closed-source models hosted through a web API (e.g., GPT 4o).

In addition to providing a reproducible way to extract risk scores from LLMs, folktexts also offers pre-implemented evaluation metrics to benchmark and compare the calibration and accuracy of LLMs, as well as easy plotting of group-conditional calibration curves for a cursory view of potential biases. The package is easy to use within a python notebook, as a dependency, or directly from the command line. Further details on usage and design choices are available in Appendix C.

## 4 Empirical findings

We use folktexts to evaluate several recently released models together with their instruction-tuned counterparts: the Llama 3 models , including the 8B and the 70B versions, the Mistral 7B , the Mistral 8x7B and 8x22B variants , the Yi 34B , and the Gemma  2B and 7B variants. We also evaluate GPT 4o and GPT 4o mini  through the OpenAI API (note that no base model version is available). Instruction-tuned models are marked '(i.t.)'. For comparison, results are also shown for a logistic regression (LR) model, and a gradient boosted decision trees model (XGBoost). The XGBoost model  is generally regarded as the state-of-the-art in tabular data tasks .

In this section we focus on the ACSIncome prediction task, which is the default folktexts benchmarking task. It consists in predicting whether a person's income is above or below $50K from 10 demographic features, and closely emulates the popular UCI Adult prediction task . The evaluation test set consists of 160K randomly selected samples from the 2018 Census data. A separate set of 1.5M samples is used to train the supervised LR and XGBoost models. LLMs are used as zero-shot classifiers without fine-tuning. Both multiple-choice prompting and numeric prompting were used to obtain two separate risk score distributions for each model (as described in Section 3.2). We focus on analyzing multiple-choice prompting results, as it is arguably the standard in LLM benchmarking [19; 20; 21; 22; 23; 24; 25; 26; 27; 28; 29; 39]. Appendix A presents additional results for the experiments analyzed in this section, including a more in-depth look into numeric prompting results, as well as results on alternative prediction tasks. Experiments consumed approximately 500 A100 GPU hours.

### Benchmark results

We perform a comprehensive evaluation of LLM-generated risk scores, summarized in Table 1.

Multiple-choice prompting.We observe that a majority of LLMs (all of size 8B or larger) outperform the linear model baseline (LR) in terms of predictive power (AUC). However, LLMs are clearly far from matching the supervised baselines with respect to calibration: all language models achieve very high calibration error (ECE), while baselines achieve near-perfect calibration. Due to this high miscalibration, models struggle to translate scores with high predictive signal into high accuracy. In fact, while most models achieve high AUC, they struggle to surpass the supervised linear baseline in terms of accuracy. We recall that AUC is agnostic to calibration, while accuracy on the maximum likelihood answer is not. All of the Gemma models have worse than random accuracy, despite having clearly above random AUC (random would be 0.5). Only the instruction-tuned Mistral models (7B, 8x7B, and 8x22B) outperform the linear model in terms of accuracy. Interestingly, while larger models achieve higher AUC, calibration is not reliably improved by model size -- differences across model families are more pronounced than across model sizes.

Finally, we focus on comparing base models to their instruction-tuned counterparts, marked with '(it)'. A striking trend is visible across the board: instruction-tuning generally worsens calibration (higher ECE) when using multiple-choice prompting. At the same time, we generally see improvements in AUC and accuracy after instruction-tuning. Appendix A.3 presents results on the four additional prediction tasks. The same trend of instruction-tuning leading to worse calibration and higher AUC is broadly replicated. However, performance across different tasks is somewhat inconsistent: LLMs

[MISSING_PAGE_EMPTY:8]

similar to the ECE metric: \(_{}_{m=1}^{M}|}{n}[ (B_{m})-(B_{m})],\) where \(M\) is the number of score bins, \(B_{m}\) is the set of samples in score bin \(m\), \((.)\) is the accuracy on a given set of samples, and \((.)\) is the confidence on a given set of samples measured as the mean risk score for the highest likelihood class. Figure 3-_left_ shows the risk score confidence bias results. The two miscalibration modes are evident: confidence bias is higher for instruction-tuned models and lower (or even negative) for base models. That is, instruction-tuned models are generally over-confident in their predictions, outputting higher scores than their accuracy would warrant, while no such trend is visible for base models. On the other hand, when using numeric prompting, the two aforementioned failure modes are no longer evident, and the differences between base and instruction-tuned models are blurred (right-most plots of Fig. 2). In fact, Figure 3-_right_ shows a trend reversal when using numeric prompting: Base models now show a higher over-confidence in their risk scores, while instruction-tuned models show approximately neutral score bias.

Figure 4 shows the risk score distribution for a variety of model pairs, produced using multiple-choice prompting. The score distributions for base and instruct model variants are immediately distinguishable: base models consistently produce low-variance distributions centered around 0.5, while instruction-tuned variants often output scores near 0 or 1. The same trend is visible on all model pairs, with Yi 34B showing the smallest difference between base and instruct variants. The score distributions produced by base/i.t. model pairs are markedly different, even among base/i.t. pairs achieving the exact same AUC; e.g., Llama 3 70B and Mistrat 8x22B. For the largest Llama (70B) and largest Mistrat (8x22B) models, no predictive performance is gained by instruction tuning, but answers of the instruction-tuned models have significantly higher (over-)confidence and worse calibration. In fact, while the base Llama 3 70B has an average of \(0.07\) under-confidence bias, the instruct variant produces risk scores on average \(0.22\) over-confident (see Figure 3, left). Appendix A.1 investigates whether this under-/over-confidence bias disproportionately affects protected societal sub-groups. We raise concerns regarding subgroup miscalibration, which should caution practitioners against using such scores in consequential domains without a comprehensive fairness audit.

Figure 3: Risk score confidence bias for all LLMs on the ACSIncome task. _Left:_ Multiple-choice prompting. _Right:_ Numeric prompting. Negative values indicate under-confident risk scores (overestimating uncertainty), while positive values indicate over-confident risk scores (underestimating uncertainty). Instruction-tuned models are generally over-confident when using multiple-choice prompting (_left plot_), but this bias is substantially diminished when using numeric prompting (_right plot_).

Figure 2: Calibration curves for base and instruction-tuned versions of the largest models studied, on the ACSIncome task. Curves are computed using \(10\) quantile-based score bins. Risk scores were generated using multiple-choice-style prompting (_left plots_) or numeric chat-style prompting (_right plots_).

Crucially, our evaluation reveals a previously unreported shortcoming of using multiple-choice prompting with instruction-tuned models: instruction-tuning polarizes score distributions, even if the true outcome has high entropy. Standard realizable knowledge testing benchmarks can easily disguise this polarization phenomenon as improper quantification of _model_ uncertainty. In fact, it seems evident that it is improper quantification of uncertainty in general, regardless of underlying uncertainty in the modeled distribution. The following subsection goes further in-depth on the influence of data uncertainty in score distribution. Appendix A.2 analyzes numeric prompting results.

### Varying degree of uncertainty

Next, we consider the dependence of multiple-choice risk scores on the available evidence. For this study we use the Miktral 8x7B model, which achieves the best (lowest) Brier score among evaluated models (reflecting both high accuracy and high calibration). We compute income prediction risk scores with increasing evidence: starting with only 2 features, and iteratively adding 2 features at a time (see results in Figure A8). This sequence demonstrates how unrealizable, underspecified prediction tasks differ from realizable prediction tasks. Predicting income based on an individual's place of birth (POBP) and race (RAC1P) is naturally not possible to a high degree of accuracy, forcing any calibrated model to output lower-confidence risk scores. Indeed, both base and instruct variants correctly output lower confidence scores for the smaller feature sets when compared with the larger feature sets (compare left-most to right-most plots of Fig. A8). However, instruction-tuning still leads to a clear polarization of risk score distribution, regardless of true data uncertainty: Score variance for base models is in range \([0.02,0.06]\), while for i.t. models it's in range \([0.16,0.41]\). Appendix A.4 goes further in-depth on how score distributions change with varying data uncertainty. Varying individual features also enables us to study LLM feature importance and its main differences to traditional supervised models -- analyzed in Appendix A.5.

## 5 Discussion

We introduced folktexts, a software package that provides datasets and tools to evaluate risk scores produced by language models. Unlike most existing LLM benchmarks, the datasets we introduced have inherent outcome uncertainty. While uncertainty on _realizable_ tasks reflects only model uncertainty (i.e., whether the model is aware of its lack of knowledge), uncertainty on _unrealizable_ tasks is itself a type of knowledge over the underlying data distribution.

Our empirical findings show that LLM risk scores produced using standard multiple-choice Q&A generally have strong predictive signal, but are wildly miscalibrated. Such models may be good for knowledge testing, but lack adequate indicators of uncertainty, making them unsuitable for synthetic data generation. We further reveal that instruction-tuning leads to marked polarization of multiple-choice answer distribution, regardless of ground-truth data uncertainty. This reveals a general inability of instruction-tuned LLMs to quantify uncertainty using multiple-choice Q&A. Conversely, verbally querying models for numeric probability estimates considerably improves calibration of instruction-tuned models, at a small but consistent cost in AUC.

Figure 4: Risk score distribution for base and instruction-tuned model pairs on the ACSIncome task, using _multiple-choice_ prompting. After instruction-tuning, models exhibit high confidence, but worse calibration in general. The XGBoost scores showcase a perfectly calibrated distribution (ECE \( 0.00\)). Fig. A6 shows all models.