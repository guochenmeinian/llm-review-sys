# MoLE : Enhancing Human-centric Text-to-image Diffusion via Mixture of Low-rank Experts

Jie Zhu\({}^{1,2}\), Yixiong Chen\({}^{3}\), Mingyu Ding\({}^{4}\), Ping Luo\({}^{5}\), Leye Wang\({}^{1,2}\)1, Jingdong Wang\({}^{6}\)1

\({}^{1}\)Key Lab of High Confidence Software Technologies (Peking University), Ministry of Education, China

\({}^{2}\)School of Computer Science, Peking University, Beijing, China, \({}^{3}\)Johns Hopkins University

\({}^{4}\)UC Berkeley, \({}^{5}\)The University of Hong Kong, \({}^{6}\)Baidu

zhujie@stu.pku.edu.cn, ychen6460jh.edu, myding@berkeley.edu,

pluo@cs.hku.hk, leyewang@pku.edu.cn, wangjingdong@outlook.com

###### Abstract

Text-to-image diffusion has attracted vast attention due to its impressive image-generation capabilities. However, when it comes to human-centric text-to-image generation, particularly in the context of faces and hands, the results often fall short of naturalness due to insufficient training priors. We alleviate the issue in this work from two perspectives. **1)** From the data aspect, we carefully collect a _human-centric dataset_ comprising over one million high-quality human-in-the-scene images and two specific sets of close-up images of faces and hands. These datasets collectively provide a rich prior knowledge base to enhance the human-centric image generation capabilities of the diffusion model. **2)** On the methodological front, we propose a simple yet effective method called **M**ixture of **L**ow-rank **E**xperts (**MoLE**) by considering low-rank modules trained on close-up hand and face images respectively as experts. This concept draws inspiration from our observation of low-rank refinement, where a low-rank module trained by a customized close-up dataset has the potential to enhance the corresponding image part when applied at an appropriate scale. To validate the superiority of MoLE in the context of human-centric image generation compared to state-of-the-art, we

Figure 1: Compare MoLE with other diffusion models. Pay more attention to the face and (especially) hand. Zoom in for a better view.

construct two benchmarks and perform evaluations with diverse metrics and human studies. Datasets, model, and code are released at project website.

## 1 Introduction

Human-centric text-to-image generation is an important orientation for realistic applications, _e.g._, poster design, virtual reality, _etc._ However, current models encounter issues with producing natural-looking results, particularly in the context of faces and hands 2. To address this issue, we delve into the matter and identify two factors that may contribute to this issue. Firstly, the absence of _high-quality_ human-centric data makes diffusion models lack sufficient human-centric prior 3; Secondly, in the human-centric context, faces and hands represent the two most complicated parts due to high variability, making them challenging to be generated naturally.

Hence, we alleviate this problem from two perspectives. On the one hand, we collect a human-in-the-scene dataset of high quality and high resolution from the Internet. Basically, the resolution of each image is various and over \(1024 2048\). The dataset contains approximately one million images, and covers different races, various gestures, and activities, thereby providing diffusion models with sufficient knowledge to improve the performance of human-centric generation. However, for the second factor, our experiment in Sec 5.3 demonstrates though fine-tuning on above dataset brings an overall enhancement in human quality, human face and hand still exhibit unnatural outcomes, possibly because diffusion models focus more on overall performance during fine-tuning while struggling to accurately capture highly variable parts like face and hand gestures.

To address the second factor, an interesting _low-rank refinement phenomenon_ inspires us. As shown in Fig 2, when combined with a customized low-rank module  and using a proper scale weight, Stable Diffusion v1.5 (SD v1.5)  has the potential to refine the corresponding part of a person, _e.g._, for hand, \(s=0.4\) subtly refines the appearance of the woman drinking milk's hand. Thus, inspired by this, to refine the face and hand, we can first gather two customized high-quality datasets (one for face close-ups and one for hand close-ups) to train two low-rank modules, respectively. Then, for the two specialized low-rank modules, we could add a certain assignment to adaptively select which low-rank module to use for a given input and Mixture of Experts (MoE) naturally stands out. Moreover, as face and hand often appear simultaneously in an image for a person, motivated by Soft MoE , we could adopt a soft assignment to produce adaptive scale weights, activating multiple experts to handle the input at the same time. We refer to all mentioned three datasets above together as _human-centric dataset_ for convenience as shown in Fig 3.

Figure 2: **Inspiration of Mixture of Low-rank Experts. In the first and second row, we train two low-rank modules on SD v1.5 simply using off-the-shelf Celeb-HQ face dataset  and 11k Hands dataset , respectively. _With a proper scale weight, low-rank module can refine corresponding part._ We term this phenomenon as low-rank refinement.**

In the end, we propose a simple yet effective method called **Mixture of** Low-rank **E**xperts (**MoLE**). Our key insight is to regard low-rank modules trained on customized datasets as specialized experts of MoE and adaptively activate them in a soft form. By adopting SD v1.5 as a showcase, our method basically contains three stages: We fine-tune SD v1.5 on collected dataset to complement sufficient human-centric prior; Then we use two close-up datasets, _i.e._, close-ups of face and hand images, to train two low-rank experts separately; Finally, we formulate these two low-rank experts in an MoE form and integrate them with the base model in an adaptive soft assignment manner. To evaluate MoLE, we construct two human-centric benchmarks using prompts from COCO Caption  and DiffusionDB . The results based on SD v1.5, v2.1, and XL consistently suggest the superiority and generalization of MoLE. Our contribution can be summarized as:

\(\) We carefully collect a human-centric dataset comprising over one million high-quality images to provide sufficient human-centric prior. Importantly, we include two high-quality close-up of face and hand subsets, especially for hand. To the best of our knowledge, such a high-quality close-up hand dataset is absent in prior related studies. Click here to have a look.

\(\) We find the low-rank refinement phenomenon and are inspired to propose MoLE, a simple yet effective method. MoLE integrates low-rank modules trained on customized hand and face datasets as experts in an MoE framework with soft assignment 4 to enable flexible activation.

\(\) We construct two human-centric evaluation benchmarks from DiffusionDB and COCO Caption. MoLE consistently demonstrates improvement over the state-of-the-art and exhibits broad generalization across SD v1.5, v2.1, and XL in human-centric generation.

## 2 Related Work

**Text-to-image generation.** Diffusion model [17; 46], especially in text-to-image generation, has been widely used since its proposal, _e.g._, GLIDE  with classifier-free guidance , Imagen  with a large T5 text encoder , Stable Diffusion  using VAE encoded latents, and DALL-E 2  using CLIP . Different from them, our work primarily focuses on human-centric text-to-image generation. Though a concurrent work  also aims to improve human-centric generation, it does not explicitly consider the issue of face and hand and thereby the related generation is still unnatural. In contrast, MoLE is specially designed for this issue.

**Mixture-of-Experts.** In MoE , different subsets of data or contexts may be better modeled by distinct experts. Theoretically, MoE could scale model capability with little cost by using sparsely-gated MoE layer . Recently, MoE has been adapted in generation tasks [12; 2]. For example, ERNIE-ViLG  uniformly divides the denoising process into several distinct stages, with each being associated with a specific model. eDiff-i  calculates thresholds to separate the whole process into three stages. Differing from employing experts in divided stages, we consider low-rank modules trained by customized datasets as experts to adaptively refine generation. For more discussion about related work, we put in Appendix A.11.

Figure 3: Some showcases of our human-centric dataset.

## 3 Human-centric Dataset

**Overview.** Our human-centric dataset involves over one million high-quality images, containing three parts (See Sec 3.1). As shown in Fig 3, these images are diverse w.r.t. occasions, activities, gestures, ages, genders, and racial backgrounds. Specifically, approximately 57.33% individuals identify as White, 14.68% as Asian, 9.98% as Black, 5.11% as Indian, 5.52% as Latino Hispanic, and 7.38% as Middle Eastern 5. Approximately 58.18% are male and 41.82% are female. For age, approximately 0.93% are babies (0-1 years old), 3.55% are kids (2-11 years old), 4.60% are teenagers (12-18 years old), 84.86% are adults (18-60 years old), and 6.06% are elderly (over 60 years old).

**Ethical & legal compliance.** Our collection is in compliance with the ethics and law as all images are collected from websites under Public Domain CCO 1.0 6 license that allows free use, redistribution, and adaptation for non-commercial purposes. To avoid concerns, please see our license and privacy statement in Appendix A.2. Note that this dataset is allowed for academic purposes only. When using it, the users are requested to ensure compliance with ethical and legal regulations.

### Human-centric Dataset Constitution

**Human-in-the-scene images.** We primarily collect high-resolution human-centric images from Internet and the image resolution is basically over \(1024 2048\), providing sufficient priors for diffusion models. To enable training, we use a sliding window (\(1024 1024\)) to crop the image to maintain as much information as possible. However, for an image, high resolution does not mean high quality. Therefore, we train a VGG19  to filter out blurred images. Additionally, considering the crop operation could generate images that are full of background or contain little information about people, we train a VGG19  to filter out such bad cases 7. To ensure the quality, we repeat the two processes multiple times until we do not find any case mentioned above in three times of random sampling. By employing these strategies, we can remove amounts of noise and useless images, thereby guaranteeing the image quality.

**Close-up of face images.** The face dataset contains two sources: the first is from Celeb-HQ  in which we choose images of high quality with size \(1024 1024\); The second is from Flickr-Faces-HQ (FFHQ) . We sample images covering different skin color, age, sex, and race. There are around 6.4k face images. We do not sample more face images as it is sufficient for low-rank expert training.

**Close-up of hand images.** The hand dataset contains three sources: the first is from 11k Hands  where we randomly sample around 1k high-quality images and manually crop them to square; The second is from the Internet where we collect hand images of high quality and resolution with simple backgrounds and use YOLOv5  to detect hands and crop them with details maintained; The third is from human-in-the-scene images (before processing) where we sample 8k images. We check every image and manually crop the hand of the image to square if the image is appropriate and the hand is clear. In this close-up hand dataset, there are abundant hand gestures and scenarios shown in Fig 3, _e.g._, holding a flower, writing, _etc_. There are 7k high-quality hand images. To the best of our knowledge, such a high quality close-up hand dataset is absent in prior related studies.

Figure 4: The results of four captioning models. Texts in red are inaccurate descriptions and texts in green are detailed correct descriptions. LLaVA presents a good balance between the level of detail and error rate, and thus is chosen for captioning our dataset.

### Image Caption Generation

When collecting the dataset, we primarily consider image quality and resolution, neglecting whether it is text paired so as to increase image amount. Thus, producing a caption for each image is required. We investigate four recently proposed SOTA models including BLIP-2 , ClipCap , MiniGPT-4 , and LLaVA . We show several cases in Fig 4. One can see that BLIP-2 usually produces simple descriptions and ignores details. ClipCap has a better performance but still lacks sufficient details along with the wrong description. MiniGPT4, although gives detailed descriptions, is inclined to spend a long time (17s on average) generating long and inaccurate captions that exceed the input limit (77 tokens) of the Stable Diffusion CLIP text encoder . In contrast, LLaVA produces neat descriptions in one sentence with accurate details in a short period (3-5s). Afterward, we manually streamline long LLaVA caption with a new shorter caption by ourselves while aligning with the content of the image. We also remove unrelated and uninformative text patterns, _e.g._, "The image features that...", "showcasing...", "creating...", "demonstrating...", _etc_. To further ensure the caption alignment of LLaVA, we use CLIP to filter image-text pairs with lower scores.

## 4 Method

### Preliminary

**Low-rank Adaptation (LoRA).** Given a customized dataset, instead of training the entire model, LoRA  is designed to fine-tune the "residual" of the model, _i.e._, \( W\):

\[W^{{}^{}}=W+ W\] (1)

where \( W\) is decomposed into low-rank matrices: \( W=AB^{T}\) (\(A^{n d},\ B^{m d},\ d<n,\) and \(\ d<m\)). During training, we can simply fine-tune \(A\) and \(B\) instead of \(W\), making fine-tuning on customized dataset memory-efficient. In the end, we get a small model as \(A\) and \(B\) are much smaller than \(W\).

**Mixture-of-Experts (MoE).** MoE [20; 44; 24] is designed to enhance the predictive power of models by combining the expertise of multiple specialized models. Usually, a central "gating" model \(G(.)\) selects which specialized model to use for a given input:

\[y=_{i=1}G(x)_{i}E_{i}(x)\,.\] (2)

When \(G(x)_{i}=0\), the corresponding expert \(E_{i}\) will not be activated.

### Mixture of Low-rank Experts

Motivated by the two potential factors discussed in Sec 1, our method contains three stages as shown in Fig 5. We describe each stage below and put the training details in Appendix A.1.

_Stage 1: Fine-tuning on Human-centric Dataset._ The overall poor performance of human-centric generation could be attributed to the absence of large-scale high-quality datasets. Considering such a pressing need, our work bridges this gap by providing a carefully collected dataset that contains around one million human-centric images of high quality. To learn as much prior as possible, we adopt SD v1.5 as a baseline and leverage the whole human-centric datasets to fine-tune. Concretely, we fine-tuning the UNet modules  (and text encoder) while fixing the rest parameters. The well-trained model is then sent to the next stage.

_Stage 2: Low-rank Expert Generation._ To construct MoE, in this stage, our goal is to prepare two experts that are supposed to contain abundant knowledge about the corresponding part. To achieve this, we train two low-rank modules using two customized datasets. One is the close-up face dataset. The other is the close-up hand dataset that contains abundant hand gestures, full details with simple backgrounds, and interactions with other objects. We then use the two datasets to train two low-rank experts with SD v1.5 trained in stage 1 as the base model. The low-rank experts are expected to focus on the generation of face and hand and learn useful context.

_Stage 3: Soft Mixture Assignment._ This stage is motivated by the low-rank refinement phenomenon in Fig 2 where a specialized low-rank module using a proper scale weight is able to refine the corresponding part of a person. Hence, the key is to activate different low-rank modules with suitable weights. From this view, MoE naturally stands out and we novelly regard a low-rank module trainedon a customized dataset, _e.g._, face dataset, as an expert and formulate in a MoE form. Moreover, for a person, face and hand would appear in an image simultaneously while hard assignment mode in MoE only allows one expert accessible to the given input. Hence, inspired by Soft MoE , we adopt a soft assignment, allowing multiple experts to handle input simultaneously. Further, considering that the face and hand would be a part of the whole image (local) or occupy the whole image (global), besides global assignment in original MoE, we novelly introduce local assignment. Specifically, considering a linear layer \(F\) from UNet and its input \(X^{n d}\) where \(n\) is the number of token and \(d\) is the feature dimension, we illustrate local and global assignment respectively.

**For local assignment**, we employ a local gating network that contains a learnable gating layer \(G(\,,\,)\) (\(^{d e}\), \(e\) is the number of experts and here \(e\) is 2, below is the same.) and a \(\) function. The gating network is to produce two normalized score maps \(s=[s_{1},s_{2}]\) (\(s^{n e}\), here \(e\) is 2) for two low-rank experts as formulated:

\[s=(G(X\,,)\] (3)

**For global assignment**, we use a global gating network including an AdaptiveAvePool, a learnable gating layer \(G(\,,)\) (\(^{d e}\), here \(e\) is 2), and a \(\) function. This gating network is to produce two global scalars \(g=[g_{1},\ g_{2}]\) (\(g^{e}\), \(e\) is 2) for two experts as formulated:

\[g=(G((X)\,,))\,.\] (4)

_The soft mechanism is built on the fact that each token can adaptively determine how much (weight) should be sent to each expert by the \(\) function._ And intuitively, the weight of every token for two experts is independent as face and hand experts are not competitors during generation. Thus we do not use \(\).

**For combination**, we first send \(X\) to each low-rank expert \(E_{}\) and \(E_{}\) respectively, use \(s_{1}\) and \(s_{2}\) (\(^{n 1}\)) to perform element-wise multiplication (local assignment), and also perform global control by scalars \(g_{1}\) and \(g_{2}\) (global assignment) 8:

\[Y_{1}=E_{}(X s_{1} g_{1})=g_{1} E_{}(X  s_{1})\] (5)

\[Y_{2}=E_{}(X s_{2} g_{2})=g_{2} E_{}(X  s_{2})\] (6)

Then we add \(Y_{1}\) and \(Y_{2}\) back to the output of a linear layer \(F\) from UNet with \(X\) as input, producing a new output \(X^{{}^{}}\):

\[X^{{}^{}}=F(X)+Y_{1}+Y_{2}\] (7)

In the end, to endow the model with the capability to adaptively activate experts, we use our human-centric dataset to train learnable gating layers while freezing the base model and two low-rank experts.

## 5 Experiment

### Evaluation Benchmarks and Metrics

Considering that our work primarily focuses on human-centric image generation, before presenting our experiment, we introduce two customized evaluation benchmarks. Additionally, since our

Figure 5: The framework of MoLE. \(X\) is the input of any linear layers in UNet. \(A\) and \(B\) are low-rank matrices.

generated images are human-centric, they should intuitively meet human preferences. Hence, we primarily adopt two human preference metrics including Human Preference Score (HPS)  and ImageReward (IR) . We describe all of them below. Besides the two metrics, we also perform user studies by inviting people to compare the generated images with their own preferences.

**Benchmark 1: COCO Human Prompts.** We construct this benchmark by leveraging the caption in COCO Caption  that has been widely used in previous work [49; 50; 6]. Concretely, we use the captions in the COCO val set, and preserve the caption that contains human-related words, _e.g._, woman, man, kid, girl, boy, person, teenager, _etc_. In the end, we have around 60k prompts left, dubbed as COCO Human Prompts.

**Benchmark 2: DiffusionDB Human Prompts.** DiffusionDB  is the first real-users-specified large-scale text-to-image prompt dataset containing around 14 million prompts. We construct this benchmark by leveraging its 2M caption set version. Concretely, we first filter out the NSFW prompts by the indicator provided in DiffusionDB. Then we preserve captions containing human-related words. We filter out prompts containing special symbol, _e.g._, [, ], _etc_. In the end, we have around 64k prompts left, dubbed as DiffusionDB Human Prompts.

**Metric 1: Human Preference Score (HPS).** Human Preference Score (HPS)  measures how images present with human preference. It leverages a human preference classifier fine-tuned on CLIP .

**Metric 2: ImageReward (IR).** Different from HPS, ImageReward (IR)  is built on BLIP . IR is a zero-shot evaluation metric for understanding human preference in text-to-image synthesis.

### Main Results

**Superiority.** To evaluate the performance, following previous work [40; 50; 49; 6] we randomly sample 3k prompts from COCO Human Prompts benchmark and 3k prompts from DiffusionDB Human Prompts benchmark to generate images and calculate metrics, HPS and IR, and compare MoLE with open-resource SOTA method with different model structures including VQ-Diffusion , Versatile Diffusion , our baseline SD v1.5  and its largest variant SDXL. We repeat the process three times and report the averaged results and standard error in Tab 1. MoLE outperforms VQ-Diffusion and Versatile Diffusion and significantly improves our baseline SD v1.5 in both metrics, implying that MoLE could generate images that are more natural to meet human preference. In Appendix A.5, we also evaluate on CLIP-T , FID , and Aesthetic Score , to present a comprehensive comparison. Besides, we illustrate generated images of different models in Fig 1 and Fig 13. Though MoLE is inferior to SDXL in HPS and IR 9, we qualitatively compare the face and hand of generated images from MoLE and SDXL in Fig 1 and Fig 13, and our results look more realistic with natural hand/face even under high HPS gap (in 2nd row Fig 1 20.39 _vs._ 22.38). Even though, to provide a more convincing demonstration of the efficacy of our method, we proceed to implement MoLE on SDXL (See Generalization part below). We also compare MoLE with relevant methods like HanDiffuser  and HyperHuman  in Appendix A.5. Finally, MoLE is resource-friendly and can be trained in a single A100 80G GPU.

    &  \\   & HPS (\%) & IR (\%) \\  VQ-Diffusion & \(19.21 0.04\) & \(-12.51 2.44\) \\ Versatile Diffusion & \(19.75 0.09\) & \(-8.81 1.40\) \\ SDXL & \(20.84 0.06\) & \(73.34 2.29\) \\ SD v1.5 & \(19.91 0.09\) & \(28.34 1.40\) \\  MoLE (SD v1.5) & \(20.27 0.07\) & \(33.75 1.49\) \\ MoLE (SDXL) & \(21.36 0.02\) & \(98.52 0.61\) \\    
    &  \\   & HPS (\%) & IR (\%) \\  VQ-Diffusion & \(19.00 0.02\) & \(-18.42 1.49\) \\ Versatile Diffusion & \(20.09 0.04\) & \(-20.95 2.72\) \\ SDXL & \(21.51 0.07\) & \(87.88 2.53\) \\ SD v1.5 & \(20.29 0.01\) & \(-2.72 1.66\) \\  MoLE (SD v1.5) & \(20.62 0.04\) & \(4.36 1.36\) \\ MoLE (SDXL) & \(22.35 0.01\) & \(105.25 1.15\) \\   

Table 1: The performance of MoLE on COCO Human Prompts and DiffusionDB Human Prompts.

Figure 6: User study in four aspects.

[MISSING_PAGE_FAIL:8]

**Mixture Assignment.** In MoLE, we use two kinds of mixture manners including local and global assignment. Hence, we ablate the two assignments and present the results in Tab 3. It can be seen that both local and global assignments can enhance performance. When combining them together, the performance is further improved, indicating the effectiveness of our mixture manners. Moreover, we illustrate how the two assignments work in Fig 8. For global assignment, we average the global scalars of 20 close-up face images, 20 close-up hand images, and 20 normal human images involving hand and face respectively in every inference step in Fig 8 (a), (b), and (c). In (a) and (b), when generating different close-ups, the corresponding expert generally produces higher global value, implying that global assignment is content-aware. In (c), \(E_{}\) and \(E_{}\) achieve a balance. Besides, as inference progresses the global scalar of \(E_{}\) always drops while that of \(E_{}\) is relatively flat. We speculate, in light of the diversity of hands (_e.g._, various gestures), \(E_{}\) tends to establish general content in the early stage while \(E_{}\) must meticulously fulfill facial details throughout the denoise process due to fidelity requirement. For local assignment, we visualize averaged score maps of sampled images from the two experts respectively in Fig 8 (d). We see that as inference progresses, local assignment of the two experts can highlight and gradually refine corresponding parts, verifying its effectiveness. We also provide the distribution of the local weight sent to each expert in Appendix A.6. Additionally, to understand the importance of using two experts on the model's performance, we train only one expert using all close-up images and put the comparison results in Appendix A.7.

### More Visualizations and Analysis

We show more human-centric images with natural face and hand in Appendix A.8. Surprisingly, MoLE can also generate non-human-centric images 10 in Appendix A.9, _e.g._, animals and scenery. In the end, we analyze failure cases in Appendix A.10, which we find are attributed to the large L2 norm of outputs from face and hand experts.

## 6 Discussion

To further highlight our method contribution, below we present a comprehensive discussion on distinctions between MoLE and conventional MoE methods from three aspects. We also discuss the contribution of our curated human-centric dataset and analysis about ratios of different races in Appendix A.12. Firstly, from the aspect of training, MoLE independently trains two experts to learn completely different knowledge using two customized close-up datasets. In contrast, conventional MoE methods simultaneously train experts and base models using the same dataset. Secondly, from the aspect of expert structure and assignment manner, MoLE simply uses two low-rank matrices while conventional MoE methods use MLP or convolutional layers. Moreover, MoLE combines local

Figure 8: The averaged global and local assignment weights in different inference steps.

and global assignments together for a finer-grained assignment while conventional MoE methods only use global assignment. Finally, from the aspect of applications in computer vision, MoLE is proposed for text-to-image generation while conventional MoE methods are mainly used in object recognition, scene understanding, _e.g._, V-MoE . Though MoE recently has been employed in image generation, _e.g._, ERNIE-ViLG  and eDiff-i  that employ experts in divided stages, MoLE differs from them - inspired by low-rank refinement in Fig 2, MoLE consider low-rank modules trained by customized datasets as experts to adaptively refine image generation.

## 7 Conclusion

In this work, we primarily focus on the human-centric text-to-image generation that has important real-world applications but often suffers from producing unnatural results due to insufficient prior, especially the face and hand. To mitigate this issue, we carefully collect and process one million high-quality human-centric images, aiming to provide sufficient prior. Besides, we observe that a low-rank module trained on a customized dataset, _e.g._, face, has the capability to refine the corresponding part. Inspired by it, we propose a simple yet effective method called Mixture of Low-rank Experts (MoLE) that effectively allows diffusion models to adaptively select experts to enhance the generation quality of corresponding parts. We also construct two customized human-centric benchmarks from COCO Caption and DiffusionDB to verify the superiority of MoLE.

## 8 Limitation & Future Work

Honestly, although our experiments confirm MoLE's effectiveness in enhancing human-centric image generation, there is still considerable room for improvement. Our method struggles with scenarios involving multiple individuals, likely due to our dataset being primarily single-person images and uncertainty about the applicability of observations in Fig 2 to such cases. Additionally, in generating images using identical prompts, we observe that only about 25% of MoLE's results are of high quality, a remarkable improvement over SDXL's 10%, but still below practical standards. Potential reasons include insufficient close-up data for hands and faces and the need for further tuning of hyperparameters. Future work will focus on model optimization, improving data quality, and enhancing dataset diversity to better represent various demographics and real-world scenarios.

## 9 Broader Impact

MoLE mainly focuses on enhancing human-centric text-to-image generation in diffusion models. It refrains from introducing any harmful content to the community and society. However, though MoLE may not introduce more biases on race, it also inherits the biases in the training data like pervious methods. Hence it will be more meaningful to enhance the diversity of our collected dataset to represent different demographics and real-world scenarios better. As for other impacts such as fake faces, it also inevitably generates fake faces like other generative models, which requires users to leverage these generated images carefully and legally. We highlight that these issues also warrant further research and consideration. We maintain transparency in our methods with open-source code and dataset composition, allowing for continuous improvement based on community feedback.

#### Acknowledgments

The authors thank the anonymous reviewers for constructive comments. The authors also thank Qi Zhang, Xin Li, Boqiang Duan, Teng Xi, and Gang Zhang for discussion and help.