# HiCoM: Hierarchical Coherent Motion for Streamable Dynamic Scene with 3D Gaussian Splatting

Qiankun Gao1,2, Jiarui Meng1, Chengxiang Wen1, Jie Chen1,2,3, Jian Zhang1,3

###### Abstract

The online reconstruction of dynamic scenes from multi-view streaming videos faces significant challenges in training, rendering and storage efficiency. Harnessing superior learning speed and real-time rendering capabilities, 3D Gaussian Splatting (3DGS) has recently demonstrated considerable potential in this field. However, 3DGS can be inefficient in terms of storage and prone to overfitting by excessively growing Gaussians, particularly with limited views. This paper proposes an efficient framework, dubbed **HiCoM**, with three key components. First, we construct a compact and robust initial 3DGS representation using a perturbation smoothing strategy. Next, we introduce a **H**ierarchical **Co**herent **M**otion mechanism that leverages the inherent non-uniform distribution and local consistency of 3D Gaussians to swiftly and accurately learn motions across frames. Finally, we continually refine the 3DGS with additional Gaussians, which are later merged into the initial 3DGS to maintain consistency with the evolving scene. To preserve a compact representation, an equivalent number of low-opacity Gaussians that minimally impact the representation are removed before processing subsequent frames. Extensive experiments conducted on two widely used datasets show that our framework improves learning efficiency of the state-of-the-art methods by about \(20\%\) and reduces the data storage by \(85\%\), achieving competitive free-viewpoint video synthesis quality but with higher robustness and stability. Moreover, by parallel learning multiple frames simultaneously, our HiCoM decreases the average training wall time to \(<2\) seconds per frame with negligible performance degradation, substantially boosting real-world applicability and responsiveness. Code is avaliable at [https://github.com/gqk/HiCoM](https://github.com/gqk/HiCoM).

Figure 1: The proposed HiCoM framework for streamable dynamic scene reconstruction achieves competitive rendering quality with significantly shorter training time, faster rendering speed, and substantially reduced storage and transmission requirements. The left figures show results of our HiCoM on N3DV  and Meet Room  datasets, where “Res” indicates video resolution. The right figure is tested on the N3DV  dataset, where the radius of the circle corresponds to the average storage per frame and the method in the top left corner demonstrates the best performance.

Introduction

The online reconstruction of dynamic scene from multi-view video streams is essential for advancing applications such as real-time free-viewpoint video (FVV) and virtual reality (VR), which are revolutionizing entertainment, education, and industry by providing immersive and interactive experiences. However, achieving high fidelity streamable dynamic scene poses significant challenges in training time, rendering speed, data storage and transmission efficiency.

Traditional methods for modeling and representing dynamic scenes, such as surface estimation , multi-sphere imaging , and depth mapping [5; 6] via multi-view stereo technologies, struggle with the complex geometries and varied appearances of real-world scenarios. These techniques also face limitations in capturing the detailed and dynamic nature of practical environments.

Neural Radiance Fields (NeRFs) [7; 8; 9; 10; 11; 12; 13; 14; 15; 16] have made significant breakthroughs in 3D reconstruction and novel view synthesis by mapping spatial coordinates and viewing directions to color and density using a neural network. Dynamic NeRFs [1; 17; 18; 19; 20; 21; 22; 23] integrate temporal components to capture scene changes over time. However, the high computational demands of NeRF's volume rendering framework limit their feasibility in real-time applications.

Acknowledging NeRF's limitations, researchers have introduced 3D Gaussian Splatting (3DGS) , an innovative method for fast 3D reconstruction and real-time rendering. In contrast to NeRF, 3DGS explicitly employs anisotropic 3D Gaussians as primitive elements to represent 3D scenes, then rasterizes these Gaussians to render images from specific viewpoints, bypassing the complex and slow volume rendering pipeline. This not only speeds up the rendering process but also enhances the quality of the synthesized views. In a similar vein to NeRF, 3DGS has been adapted for dynamic scene reconstruction: several Dynamic Gaussian Splatting works [25; 26; 27] directly expand attributes of Gaussian primitives; other efforts [28; 29; 30; 31; 32; 33; 34] focus on decoupling the dynamic scene into a base scene representation in canonical space and time-varying motion fields.

Harnessing its efficiency, 3DGS naturally supports online learning of dynamic scenes. A recent development, 3DGSream , introduces a pioneering framework that uses a Neural Transformation Cache derived from InstantNGP  to model the scene changes from previous frame to current frame, significantly reducing training time and storage requirements. However, this frame-by-frame online learning pipeline heavily relies on the quality of the initial 3DGS representation. Given that dynamic scenes are generally captured with a limited number of cameras, 3DGS can be prone to instability and overfitting when faced with sparse views. Furthermore, the number of Gaussian primitives in the initial 3DGS representation impacts the learning efficiency of subsequent frames. Despite employing multi-resolution hash encoding and lightweight MLP to alleviate inefficiency and speed up convergence, implicitly modeling the motion field still cannot fully capture critical aspects of the explicit and discrete nature of 3DGS. Although 3DGSream introduces new Gaussians to adapt to the appearance of new objects, these added Gaussians are not carried over to subsequent frames to maintain 3DGS compact. As real-world scenes evolve gradually, the discrepancies from the initial scene accumulate, necessitating more Gaussians to accurately capture these changes.

In this paper, we introduce the Hierarchical Coherent Motion (HiCoM) framework, a novel approach designed to enhance the efficiency and stability of streamable dynamic scene online reconstruction. Our HiCoM framework begins with the learning of a compact and robust initial 3DGS representation through a perturbation smoothing strategy. This ensures a reduced number of Gaussians, alleviates overfitting and establishes a foundation for consistent quality across frames. Then, we leverage the inherent non-uniform distribution and local consistency of 3D Gaussians to implement a hierarchical coherent motion mechanism. Specifically, we partition the scene into regions and recognize that only a few regions actually contain Gaussian primitives due to the non-uniform distribution of Gaussians. We explicitly model the motion within these non-empty regions, allowing Gaussians in the same region to share identical motion patterns. These regions can be further divided into smaller areas, enabling the motion of each Gaussian to be determined by the combined motions of all levels of regions it inhabits. This hierarchical coherent motion mechanism captures motions from coarse to fine granularity and requires only a minimal set of parameters, facilitates rapid convergence. The inherent structure and consistency within and between regions thus support swift learning of scene changes across frames. We also introduce additional Gaussians to better accommodate significant updates in scene content. These new Gaussians are carefully integrated into the initial 3DGS representation to ensure continuous consistency with the evolving scene. To maintain the compactness of the 3DGS,an equivalent number of low-opacity Gaussians, which no longer significantly contribute to the scene representation, will be removed before the learning of the next frame. This continual refinement to the initial 3DGS representation ensures it remains as close as possible to the evolving scene, facilitating better subsequent learning. In addition, we introduce a parallel training strategy that enables simultaneous learning of multiple frames, significantly enhancing training efficiency with minimal impact on performance. The contributions of this paper are summarized as follows:

* We introduce the HiCoM framework for online learning of dynamic scene from multi-view video streams, featuring a perturbation smoothing strategy for robust initial 3DGS representation learning, hierarchical coherent motion mechanism for efficient motion capture, and continual refinement to adapt evolving scene updates.
* We devise a novel and concise hierarchical coherent motion mechanism that capitalizes on the inherent non-uniform distribution and local consistency of 3D Gaussians, efficiently capturing and modeling scene motions for swift and precise frame-to-frame adaptation.
* Extensive experiments demonstrate that our HiCoM framework improves learning efficiency by about \(20\%\) and reduces data storage by \(85\%\) compared to state-of-the-art methods. Our parallel training strategy enables learning multiple frames simultaneously, substantially decreasing the training wall time with negligible effects on overall performance, further enhancing the practicality and responsiveness of our HiCoM for real-world applications.

## 2 Related Work

### Dynamic Gaussian Splatting

3D Gaussian Splatting  has quickly garnered attention for dynamic scene reconstruction due to its efficiency and flexibility. _1) Several Dynamic Gaussian Splatting works_[25; 26; 27] directly expand attributes of Gaussian primitives: 4D Gaussian Splatting  adds a timestamp dimension to form 4D Gaussian primitives; Dynamic 3D Gaussians  incorporates positions and rotations at every timestamp into the Gaussian primitives. _2) Other works_[28; 29; 30; 31; 32; 34; 33; 37] decouple the dynamic scene into base scene representation in canonical space and time-varying motion fields: Deformable 3D Gaussians , inspired by dynamic NeRFs [1; 22; 19; 20; 21; 18], uses a deep MLP to predict the motion of Gaussians based on their positions and the given timestamp, enabling detailed and real-time rendering of dynamic scenes; 4D-GS  refines this pipeline by employing a more efficient multi-resolution hex-planes and a lightweight MLP, significantly enhancing efficiency and performance, particularly in high-resolution real-time rendering scenarios; SC-GS  assumes that scene motions are driven by a small number of key point movements, leveraging sparse control points and dense Gaussians to distinctively represent motion and appearance in dynamic scenes, and employs a deformation MLP to predict time-varying 6 DoF transformations for each control point, which are then locally interpolated through learned weights to create a motion field that influences the dense Gaussians, ensuring coherent and dynamic scene rendering.

### Online Learning of Streamable Dynamic Scene

Compared to the offline learning from fully captured multi-view videos, on-the-fly learning, _i.e._, frame-by-frame training, for constructing streamable dynamic scenes presents more challenges. Given the success of NeRFs [7; 8; 9; 36; 20; 19] in addressing both static and dynamic scenes, there has been a growing interest in adapting and refining these techniques to solve the challenges of streamable dynamic scenes. StreamRF  tackles this by employing an incremental learning paradigm, where per-frame differences are modeled to efficiently adapt to changes, significantly speeding up the training process and reducing storage overhead than traditional methods. NeRFplayer , on the other hand, decomposes the 4D spatiotemporal space based on temporal characteristics, optimizing for both speed and compactness in model representation, thus enabling fast reconstruction and streamable rendering. ReRF  extends these capabilities by modeling the residual information between adjacent timestamps, which allows for real-time free-viewpoint video (FVV) rendering of long-duration dynamic scenes with high fidelity. The emergence of 3D Gaussian Splatting (3DGS) has brought significant advancements in training efficiency and rendering speeds, making it exceptionally suited for addressing streamable dynamic scenes. Building on superior 3DGS and inspired by previous NeRF works [2; 36], 3DGStream  introduces an efficient framework that uses a Neural Transformation Cache to manage the transformations of 3D Gaussians, drastically reducing the training time and storage requirements. This method also features an adaptive addition strategy for 3D Gaussians to better handle emerging objects in dynamic scenes.

## 3 Preliminaries

3D Gaussian Splitting explicitly represents scenes using anisotropic 3D Gaussian primitives, each defined by a mean vector \(\) and covariance matrix \(\), which respectively characterize the central position and geometric shape of the Gaussian in world space, mathematically formulated as:

\[G()=e^{-(-)^{T}^{-1}( -)}. \]

The covariance matrix \(\) is decomposed into a scaling matrix \(\) and a rotation matrix \(\) to ensure physical meaning and facilitate optimization:

\[=^{T}^{T}, \]

where \(=(s_{x},s_{y},s_{z})^{3}\) and \( SO(3)\) are parameterized as a 3D scaling vector \(\) and rotation quaternion \(\), respectively. Gaussian primitive is enriched with color and opacity, represented by spherical harmonic coefficients \(\) and a scalar \(\), respectively.

To render a novel viewpoint, Gaussian primitives are projected onto the camera plane using the view projection matrix \(\) and the Jacobian \(\) of the affine approximation of the projective transformation, the covariance matrix \(^{}\) in camera coordinates given as follows:

\[^{}=^{T}^{T}. \]

Rendering is performed by blending the contributions of \(N\) overlapping Gaussian primitives at each pixel, taking into account their depth-ordering to ensure correct compositing, expressed as:

\[C=_{i N}_{i}_{i}_{j=1}^{i-1}(1-_{j}). \]

where \(_{i}\), \(_{i}\) represents the color and blending weight of the \(i^{th}\) Gaussian, respectively.

The training of 3D Gaussian Splitting alternates between parameter optimization and density control. Parameter optimization is supervised by the \(_{1}\) loss and D-SSIM term:

\[=(1-)_{1}+_{} \]

where \(\) is typically set to 0.2. Meanwhile, density control manages Gaussian cloning and splitting to address over-reconstruction and under-reconstruction.

## 4 Methodology

The online reconstruction of streamable dynamic scenes follows a sequential and iterative pipeline, crucial for applications requiring real-time rendering and interaction. Initially, a 3D Gaussian Splitting (3DGS) representation of the scene is constructed from the first frame of synchronized multi-view video streams. As each new frame is captured, this representation is updated and refined to align with the latest scene. This process demands a high-quality scene representation at every timestamp to ensure visual fidelity, while also necessitating efficiency in learning, storage, and transmission to make it feasible for real-world applications.

To achieve these goals, our framework (Figure 2) introduces three key designs: First, we develop a compact and robust initial 3DGS representation using a perturbation smoothing strategy, ensuring a stable foundation for subsequent frame learning (Sec. 4.1). Second, we employ a hierarchical coherent motion mechanism, leveraging the inherent non-uniform distribution and local consistency of 3D Gaussians to model and rapidly learn scene motion from the previous frame to the current frame (Sec. 4.2). Third, we implement continual refinement strategies to adapt to scene content updates missed by the motion mechanism, maintaining fidelity and accuracy as the scene evolves (Sec. 4.3). Importantly, we develop a parallel training strategy that enables simultaneous learning of multiple frames (Sec. 4.4), significantly enhancing the practicality and responsiveness of our framework.

### Initial 3DGS Representation Learning

The initial 3DGS representation forms the cornerstone of the entire dynamic scene reconstruction process, laying the groundwork for precise and efficient frame-by-frame online learning.

In typical setups for capturing dynamic scenes, the number of cameras is often limited, contrasting sharply with static scenes that usually involve hundreds of multi-view images. This limitation can lead to instability and potential overfitting to training views during optimization. Moreover, if not adequately controlled, the number of Gaussians could grow unchecked, slowing down the rendering process and convergence speed in the learning of subsequent frames.

To combat overfitting, a prevalent issue in neural network training, various strategies such as dropout  and data augmentation [41; 42; 43] have been proposed. Drawing from these insights, we discover that a simple perturbation smoothing strategy helps mitigating this issue in 3DGS, which has proven also effective in NeRF . As **Figure 2 (a)**, we add small gaussian noise to the position attribute of 3D Gaussian during training, creating the random perturbed position:

\[_{p}=+_{noise}(0,1), \]

thereby preventing the model from becoming too closely fitted to the limited training views. Random perturbation not only stabilizes the learning process but also moderates the growth of 3D Gaussians. As a result, by the time the model converges, the number of Gaussians is significantly reduced, speeding up the convergence and enhancing the overall efficiency in subsequent frame learning.

### Hierarchical Coherent Motion

Follow the online learning pipeline, we continuously fetch the latest views from the video streams and reconstruct the current scene representation. A straightforward method would be to use these new images to fine-tune the previous 3DGS. However, this naive approach is still insufficient in terms of learning speed and requires storing and transmitting a significant amount of data.

Although recent efforts [28; 29; 35] have demonstrated the feasibility of using neural networks to predict the motion of each 3D Gaussian, showing promising results in adapting to scene changes, the implicit modeling of the motion field fails to align well with the explicit and discrete nature of 3DGS. In contrast, we explicitly model the motion field through hierarchical coherent motion mechanism, allowing us to directly capture and adapt to scene dynamics in a structured and efficient manner.

As illustrated in **Figure 2 (b)**, the scene is first divided into regions with the same size. and we can determine the region in which each 3D Gaussian is primarily located using the following formula:

\[=\{}{}+0.5 \}, \]

Figure 2: **Illustration of our HiCoM framework.** We first construct a compact and robust initial 3DGS representation from the first frame of the video streams with the perturbation smoothing strategy (**a**). Then, we learn each subsequent frame based on the previous 3DGS with our proposed Hierarchical Coherent Motion mechanism **(b)** and Continual Refinement (**c**) with additional new Gaussians. This learning process continues until the final frame.

where \(\) denotes the center coordinates of the region, \(\) is the center position of the 3D Gaussian, and \(\) represents the size of region.

During optimization, 3D Gaussians naturally exhibit a non-uniform distribution, with areas of complex geometry and texture densely populated by smaller Gaussians, while simpler and smoother regions sparsely covered by larger Gaussians, resulting in a significant number of regions remaining empty. For those regions that contain Gaussians, we assign translation and rotation attributes, respectively parameterized by a 3D vector \(^{3}\) and a quaternion \(^{4}\). These motion attributes are shared by the Gaussians within the region, ensuring local consistency.

However, motion within smaller subregions might exhibit subtle differences due to more localized dynamics. To accommodate these variations, we further partition the regions and establish distinct motion parameters for these smaller areas. Consequently, the motion of each Gaussian is determined by the cumulative of motion from multiple hierarchical levels to which it belongs:

\[_{g}=_{l=1}^{L}^{l},_{g}= _{l=1}^{L}^{l} \]

where \(_{g}^{3}\) and \(_{g}^{4}\) represent the composed motion of Gaussian, \(^{l}\) indicates the motion contribution from the \(l^{th}\) level of regions, and \(L\) is the total number of motion levels involved.

This hierarchical coherent motion mechanism allows us to capture Gaussians' movements ranging from coarse to fine, ensuring a more detailed and accurate representation of dynamic changes across frames, as well as coherent motion between adjacent regions. By sharing motion attributes among multiple Gaussians, we decrease the number of individual computations required and the number of parameters we need to optimize, helping the learning converge faster and more reliably.

### Continual Refinement

A 3DGS representation that is more closely aligned with the most recent scene can be obtained after applying the learned motion. However, this motion mechanism, focusing primarily on modest shifts to the positions and rotations of Gaussians from previous 3DGS, might not capture all the finer details of scene changes or adapt swiftly enough to significant scene content updates. This gap can be strategically bridged by adding new Gaussians in appropriate areas.

Specifically, during motion learning, some regions accumulate significant gradients indicating substantial discrepancies between the learned and actual scene. Following established methods [24; 35], we clone Gaussians in these regions that have accumulated gradients exceeding a predefined threshold. As shown in **Figure 2 (c)**, the newly cloned Gaussians are then subjected to further optimization and density control to better align with the most recent scene.

As real-world dynamic scenes typically exhibit coherent changes, newly added Gaussians tend to stay relevant across subsequent frames, making it impractical to simply discard them. Thus, we retain these Gaussians, integrating them into the initial 3DGS to form the basis for learning the next frame. However, consistently merging new Gaussians in each frame can lead to a steady increase in the number of Gaussians, potentially slowing down rendering speeds, delaying convergence, and prolonging learning times. Therefore, we selectively remove an equivalent number of low-opacity Gaussians that minimally impact the overall visual integrity of the scene , to maintain the compactness and efficiency of 3DGS representation without compromising scene fidelity. Simultaneously, this continual refinement allows us to address and enhance regions of the 3DGS that may have been underrepresented or insufficiently modeled in earlier learning stages, solidifying a comprehensive and adaptive representation as the scene evolves.

### Parallel Training

Typically, our framework works in a sequential frame-by-frame pipeline. To further boost efficiency and responsiveness for practical applications, we draw inspiration from video encoding techniques where reference frames (such as I-frames in H.264/AVC) are used to predict multiple subsequent frames, leveraging temporal redundancy and reducing computational overload.

Considering that differences between consecutive frames are generally minor, we choose the 3DGS of the frame \(t\) as the reference to simultaneously learn frames \(\{t+1,,t+k\}\). After processingthese frames, the 3DGS of the frame \(t+k\) becomes the new reference for learning the next \(k\) frames. This parallel training strategy significantly reduce the average wall time cost per frame.

## 5 Experiment

### Experimental Setup

For all of our experiments in this paper, we utilize two widely-used public datasets as follows.

\(\)**Neural 3D Video (N3DV)** dataset comprises six dynamic indoor scenes featuring varying illuminations, view-dependent effects, and substantial volumetric details. Videos were captured at a resolution of \(2704 2078\) and a frame rate of 30 FPS. Video counts per scene range from 18 to 21. We follow prior works [2; 35; 29] to downsample the original video resolution by a factor of two.

\(\)**Meet Room** dataset was recorded with a multi-view system consisting of 13 synchronized Azure Kinect cameras, covering three different indoor scenes. The resolution of captured videos is \(1280 720\) and the frame rate is 30 FPS too.

For both datasets, the video captured by the center reference camera is used for testing, while other videos are employed for training. Both datasets already include camera pose information, so we only generate initial point clouds with COLMAP  following 4D-GS . Most scenes last only 10 seconds, thus, we restrict our experimentation to the first 300 frames to align with prior works.

We evaluate methods based on four metrics, across all 300 frames: _1) video synthesis quality_ of the test views, assessed by the mean PSNR; _2) average training time per frame_; _3) rendering speed_, assessed by the frame rate of the synthesized video; _4) storage and transmission efficiency_, evaluated by the average size of the representation parameters per frame.

**Implementation**. Our initial frame learning employs the standard 3DGS with a fixed \(_{noise}{=}0.01\), training for 15k and 10k steps on two datasets respectively, with splitting halted at the 5k\({}^{th}\) step. The number of regions at the smallest level is determined by dividing the number of Gaussians \(n\) in the initial 3DGS by the maximum number of Gaussians \(m{=}5\) per region. The number of regions at each subsequent larger level is determined by further dividing by \(2^{3}\), and so forth, with only regions containing Gaussians having motion parameters, which are initialized at \(0.6\) of the previous frame's motion parameters. Each new frame undergoes 100 motion training steps, with an additional 100 steps after new Gaussians are added. We implement 3DGSream  in the same codebase as ours following its paper, and run all experiments 3 times on RTX 4090 GPUs, the mean metrics are reported. More details are provided in the **Appendix**.

### Experimental Results

Quantitative BenchmarkWe conduct a comprehensive quantitative comparison with state-of-the-art online methods, StreamRF  and 3DGSstream , presenting results in Table 1. The Naive 3DGS serves as a baseline, training from scratch frame-by-frame with standard 3DGS method. The data of StreamRF are referenced from the 3DGSream paper, with training time and rendering speed estimated based on the compute capabilities of RTX 3090 and 4090 GPUs. Our HiCoM consistently achieves competitive video synthesis quality (PSNR), while significantly outpacing the

    &  &  \\  Method & PSNR (dB \(\)) & Storage (MB \(\)) & Train (s \(\)) & Render (FPS \(\)) & PSNR (dB \(\)) & Storage (MB \(\)) & Train (s \(\)) & Render (FPS \(\)) \\  Naive 3DGS & 30.69 & 95.8 & 337 & 217 & 26.37 & 46.9 & 152 & 275 \\  StreamRF  & 30.68 & 17.7/31.4 & 10.0 & 13 & 26.72 & 5.7/9.0 & 6.8 & 15 \\
3DGSstream\({}^{}\) & 30.15 & 7.6/7.8 & 8.2 & 210 & 25.96 & 4.0/4.1 & 4.7 & 212 \\ 
**HiCoM** (ours) & 31.17 & 0.7/0.9 & 6.7 & 274 & 26.73 & 0.4/0.6 & 3.9 & 284 \\
**HiCoM-P4** (ours) & 31.00 & 0.7/0.9 & 1.7 & 274 & 26.25 & 0.4/0.6 & 1.0 & 284 \\   

Table 1: **Quantitative comparison results**. “Storage” is reported without/with the initial frame. The method with \({}^{}\) is reproduced by us. StreamRF metrics on Meet Room only cover the discussion scene. All experiments use original datasets without additional undistortion. Please also see Tables 6 and 7.

[MISSING_PAGE_FAIL:8]

the overall framework. In the 4\({}^{th}\) row, we omit the refinement, extending motion learning instead. This leads to a slight performance decrease, suggesting that the additional Gaussians can further compensate for the intense and detailed scene changes that the motion learning might not capture.

A notable characteristic of our HiCoM is the explicit representation of motion field, which aligns well with the discrete, explicit nature of 3DGS. Our HiCoM requires very few motion parameters, reducing the complexity of motion representation and learning, theoretically accelerating the convergence speed. According to ablation results presented in Table 3, increasing the motion learning steps from 50 to 100 (2\({}^{nd}\) row _vs._ 1\({}^{st}\) row) still provides significant performance gains, _i.e._, 0.39 and 0.22 dB on two datasets, respectively. However, learning an additional 50 steps beyond 100 (1\({}^{st}\) row _vs._ 3\({}^{rd}\) row) yields only minimal improvements, indicating that the model has mostly converged by 100 steps. Similar conclusions can be drawn for learning newly added Gaussians. Ultimately, a total of 200 learning steps is sufficient to achieve optimal results.

Our hierarchical coherent motion incorporates multiple levels of motion, with different levels corresponding to varying sizes of regions and reflecting different granularities of motion. In Table 4, we present ablation studies on motion levels. We use three motion levels by default (the 5\({}^{th}\) row), refer to as "fine", "medium" and "coarse", respectively. As the results show, using only the "fine" level already achieves good performance. Adding the "medium" and "coarse" levels provides additional performance gains, _e.g._, the PSNR of Coffee Martini and Discussion scenes respectively boosts 0.25 dB and 0.59 dB, but the improvements diminish as the number of levels increases, indicating that our HiCoM is effective and a few motion levels are sufficient to achieve good results. It can also be observed that fewer motion levels and parameters do not necessarily imply shorter training time. This is because insufficient motion learning may lead to the generation of more Gaussians during continual refinement stage, slightly increasing the overall training time.

### Parallel Training

We conduct experiments on three representative scenes from two datasets to validate the parallel learning. As shown in Table 5, when the number of parallel frames is relatively small, such as 2 and 4, the performance remains largely unchanged, with some scenes even exhibiting improved PSNR. However, as the number of parallel frames increases, the disparities between the learning and the reference frames grow, leading to a decline in performance. Specifically, when the number of parallel frames reaches 16, there is a noticeable PSNR drop, _i.e._, 0.18, 1.82 and 1.32 dB, respectively.

    &  &  &  &  &  \\   & & & PSNR & Train & PSNR & Train & PSNR & Train \\  & & & & (dB \(\)) & (s \(\)) & (dB \(\)) & (s \(\)) & (dB \(\)) & (s \(\)) \\  ✓ & ✓ & ✓ & **28.04** & 7.0 & **32.87** & 6.6 & **26.69** & 3.9 \\ ✗ & ✓ & ✓ & 26.61 & **6.3** & 32.52 & **5.6** & 26.19 & 3.6 \\ ✓ & ✗ & ✓ & 25.71 & 6.4 & 27.33 & 5.9 & 21.61 & **3.4** \\ ✓ & ✓ & ✗ & 27.87 & 7.6 & 31.73 & 6.7 & 26.51 & 4.2 \\   

Table 2: **Ablation on main components of our HiCoM framework**. Coffee Martini and Flame Steak are from the N3DV dataset, while Discussion is from the Meet Room dataset.

   \)} & \)} &  &  \\   & & PSNR (dB \(\)) & Train (s \(\)) & PSNR (dB \(\)) & Train (s \(\)) \\ 
100 & 100 & **31.17** & 6.7 & 26.73 & 3.9 \\ 
50 & 100 & 30.78 & **5.2** & 26.51 & **3.0** \\
150 & 100 & 31.16 & 8.2 & **26.77** & 4.8 \\ 
100 & 50 & 30.94 & 5.3 & **26.77** & 3.1 \\  & 150 & 31.10 & 8.1 & 26.70 & 4.6 \\   

Table 3: **Ablation on convergence. “\(E_{m}\)” and “\(E_{r}\)” are number of motion learning steps and continual refinement steps, repectively.**While parallel learning does not reduce the actual GPU computation time, it significantly reduces the wall time, making the system more practical for real-world applications. For example, with 4-frame parallel learning, we can reduce the average learning time per frame to one-fourth of the original time while maintaining nearly the same quality. This means we can learn one frame in less than 2 seconds on average, greatly enhancing the efficiency and responsiveness of the system.

## 6 Conclusion

This paper introduces HiCoM, a framework designed to tackle the challenges in online reconstruction of streamable dynamic scenes from multi-view videos. HiCoM reduces the number of 3D Gaussians for a more compact representation through the perturbation smoothing strategy, enhancing the robustness of initial frame learning. HiCoM distinguishes with its hierarchical coherent motion mechanism that explicitly represents motion between adjacent frames with minimal parameters, aligning well with the discrete and explicit nature of 3D Gaussian Splatting. This mechanism efficiently captures motion at different granularities, resulting in faster convergence and substantially reduced data storage. Additionally, HiCoM continuously refines the representation to adapt to evolving scene, facilitating better subsequent learning. Experimental results show that HiCoM boosts training efficiency by about \(20\%\) and reduces data storage by \(85\%\) compared to state-of-the-art methods. Furthermore, the parallel training significantly reduces wall time cost with negligible performance degradation, elevating real-world applicability and responsiveness.

**Limitations.** Despite our HiCoM's significant improvements, the role of the initial 3DGS representation remains crucial in the online learning pipeline, which is still not fully addressed. Future work could explore integrating advanced 3D Gaussian Splatting techniques to enhance the initial frame learning, which, combined with our hierarchical coherent motion mechanism, could further improve training efficiency while also reducing storage and transmission overhead. During online learning, reconstruction quality may decline over time due to error accumulation--a common issue across many online learning methods and one that requires further research to effectively address. Additionally, our experiments are conducted solely on indoor scenes, so the generalization of our method to outdoor or more complex environments has not been verified and requires further validation.

    &  &  &  \\  Motion Levels & PSNR & Train & PSNR & Train & PSNR & Train \\  & (dB \(\)) & (s \(\)) & (dB \(\)) & (s \(\)) & (dB \(\)) & (s \(\)) \\  coarse & 26.67 & 6.77 & 30.28 & **6.26** & 22.89 & 3.83 \\ medium & 27.24 & **6.60** & 32.14 & **6.26** & 24.91 & 3.80 \\ fine & 27.79 & 6.65 & 32.76 & 6.33 & 26.10 & **3.72** \\ fine + medium & 27.94 & 6.77 & **32.88** & 6.37 & 26.61 & 3.77 \\ fine + medium + coarse & **28.04** & 7.02 & 32.87 & 6.62 & **26.69** & 3.89 \\   

Table 4: **Ablation on motion levels.** Coffee Martini and Flame Steak are from the N3DV dataset, while Discussion is from the Meet Room dataset. The three motion levels correspond to regions of different sizes: “fine” for smaller regions, “medium” for intermediate regions, and “coarse” for larger regions. Rows show results for different combinations of these levels.

    &  &  &  \\  \#Parallel Frames & PSNR & Train & PSNR & Train & PSNR & Train \\  & (dB \(\)) & (s \(\)) & (dB \(\)) & (s \(\)) & (dB \(\)) & (s \(\)) \\  Non-Parallel & 28.04 & 7.02 & **32.87** & 6.62 & **26.69** & 3.89 \\ 
2 & **28.20** & 3.51 & 32.46 & 3.31 & 26.66 & 1.95 \\
4 & 28.19 & 1.76 & 32.13 & 1.66 & 26.39 & 0.97 \\
8 & 28.06 & 0.88 & 31.63 & 0.83 & 25.89 & 0.49 \\
16 & 27.86 & 0.44 & 31.05 & 0.42 & 25.37 & 0.25 \\   

Table 5: **Parallel Training Performance** on three scenes from N3DV and MeetRoom datasets.